0,1,label2,summary_sentences
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2832–2838 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Bidirectional Long Short-Term Memory (BLSTM) based models (Graves and Schmidhuber, 2005), along with word embeddings and character embeddings, have shown competitive performance on Part-of-Speech (POS) tagging given sufficient amount of training examples (Ling et al., 2015; Lample et al., 2016; Plank et al., 2016; Yang et al., 2017).
",1 Introduction,[0],[0]
"Given insufficient training examples, we can improve the POS tagging performance by cross-
lingual POS tagging, which exploits affluent POS tagging corpora from other source languages.",1 Introduction,[0],[0]
"This approach usually requires linguistic knowledge or resources about the relation between the source language and the target language such as parallel corpora (Täckström et al., 2013; Duong et al., 2013; Kim et al., 2015a; Zhang et al., 2016), morphological analyses (Hana et al., 2004), dictionaries (Wisniewski et al., 2014), and gaze features (Barrett et al., 2016).
",1 Introduction,[0],[0]
"Given no linguistic resources between the source language and the target language, transfer learning methods can be utilized instead.",1 Introduction,[0],[0]
"Transfer learning for cross-lingual cases is a type of transductive transfer learning, where the input domains of the source and the target are different (Pan and Yang, 2010) since each language has its own vocabulary space.",1 Introduction,[0],[0]
"When the input space is the same, lower layers of hierarchical models can be shared for knowledge transfer (Collobert et al., 2011; Kim et al., 2015b; Yang et al., 2017), but that approach is not directly applicable when the input spaces differ.
",1 Introduction,[0],[0]
Yang et al. (2017) used shared character embeddings for different languages as a cross-lingual transfer method while using different word embeddings for different languages.,1 Introduction,[0],[0]
"Although the approach showed improved performance on Named Entity Recognition, it is limited to character-level representation transfer and it is not applicable for knowledge transfer between languages without overlapped alphabets.
",1 Introduction,[0],[0]
"In this work, we introduce a cross-lingual transfer learning model for POS tagging requiring no cross-lingual resources, where knowledge transfer is made in the BLSTM layers on top of word embeddings and character embeddings.",1 Introduction,[0],[0]
"Inspired by Kim et al. (2016)’s multi-task slot-filling model, our model utilizes a common BLSTM for representing language-generic information, which al-
2832
lows knowledge transfer from other languages, and private BLSTMs for representing languagespecific information.",1 Introduction,[0],[0]
"The common BLSTM is additionally encouraged to be language-agnostic with language-adversarial training (Chen et al., 2016) so that the language-general representations to be more compatible among different languages.
",1 Introduction,[0],[0]
"Evaluating on POS datasets from 14 different target languages with English as the source language in the Universal Dependencies corpus 1.4 (Nivre et al., 2016), the proposed model showed significantly better performance when the source language and the target language are in the same language family, and competitive performance when the language families are different.",1 Introduction,[0],[0]
Cross-Lingual Training Figure 1 shows the overall architecture of the proposed model.,2 Model,[0],[0]
"The baseline POS tagging model is similar to Plank et al. (2016)’s model, and it corresponds to having only word+char embeddings, common BLSTM, and Softmax Output in Figure 1.",2 Model,[0],[0]
"Given an input
word sequence, a BLSTM is used for the character sequence of each word, where the outputs of the ends of the character sequences from the forward LSTM and the backward LSTM are concatenated to the word vector of the current word to supplement the word representation.",2 Model,[0],[0]
"These serve as an input to a BLSTM, and an output layer are used for POS tag prediction.
",2 Model,[0],[0]
"For the cross-lingual transfer learning, the character embedding, the BLSTM with the character embedding (Yang et al., 2017),1 and the common BLSTM are shared for all the given languages while word embeddings and private BLSTMs have different parameters for different languages.
",2 Model,[0],[0]
The outputs of the common BLSTM and the private BLSTM of the current language are summed to be used as the input to the softmax layer to predict the POS tags of given word sequences.,2 Model,[0],[0]
"The loss function of the POS tagging can be formulate as:
Lp = − S∑
i=1",2 Model,[0],[0]
"N∑ j=1 pi,j log (p̂i,j) , (1)
where S is the number of sentences in the current minibatch,N is the number of words in the current sentence, pi,j is the label of the j-th tag of the i-th sentence in the minibatch, and p̂i,j is the predicted tag.",2 Model,[0],[0]
"In addition to this main objective, two more objectives for improving the transfer learning are described in the following subsections.
",2 Model,[0],[0]
"Language-Adversarial Training We encourage the outputs of the common BLSTM to be language-agnostic by using language-adversarial training (Chen et al., 2016) inspired by domainadversarial training (Ganin et al., 2016; Bousmalis et al., 2016).",2 Model,[0],[0]
"First, we encode a BLSTM output sequence as a single vector using a CNN/MaxPool encoder, which is implemented the same as a CNN for text classification (Kim, 2014).",2 Model,[0],[0]
"The encoder is with three convolution filters whose sizes are 3, 4, and 5.",2 Model,[0],[0]
"For each filter, we pass the BLSTM output sequence as the input sequence and obtain a single vector from the filter output by using max pooling, and then tanh activation function is used for transforming the vector.",2 Model,[0],[0]
"Then, the vector outputs of the three filters are concatenated and forwarded to the language discriminator through the gradient reversal layer.",2 Model,[0],[0]
"The discriminator is implemented
1We also tried isolated character-level modules but the overall performance was worse.
as a fully-connected neural network with a single hidden layer, whose activation function is Leaky ReLU (Maas et al., 2013), where we multiply 0.2 to negative input values as the outputs.
",2 Model,[0],[0]
"Since the gradient reversal layer is below the language classifier, the gradients minimizing language classification errors are passed back with opposed sign to the sentence encoder, which adversarially encourages the sentence encoder to be language-agnostic.",2 Model,[0],[0]
"The loss function of the language classifier is formulated as:
La = − S∑
i=1
li log l̂i, (2)
where S is the number of sentences, li is the language of the i-th sentence, and l̂i is the softmax output of the tagging.",2 Model,[0],[0]
"Note that though the language classifier is optimized to minimize the language classification error, the gradient from the language classifier is negated so that the bottom layers are trained to be language-agnostic.
",2 Model,[0],[0]
"Bidirectional Language Modeling Rei (2017) showed the effectiveness of the bidirectional language modeling objective, where each time step of the forward LSTM outputs predicts the word of the next time step, and each of the backward LSTM outputs predicts the previous word.",2 Model,[0],[0]
"For example, if the current sentence is “I am happy”, the forward LSTM predicts “am happy <eos>” and the backward LSTM predicts “<bos> I am”.",2 Model,[0],[0]
"This objective encourages the BLSTM layers and the embedding layers to learn linguistically general-purpose representations, which are also useful for specific downstream tasks (Rei, 2017).",2 Model,[0],[0]
"We adopted the bidirectional language modeling objective, where the sum of the common BLSTM and the private BLSTM is used as the input to the language modeling module.",2 Model,[0],[0]
"It can be formulated as:
Ll = − S∑
i=1",2 Model,[0],[0]
N∑ j=1 log (P (wj+1|fj)),2 Model,[0],[0]
"+
log (P (wj−1|bj)) , (3)
where fj and bj represent the j-th outputs of the forward direction and the backward direction, respectively, given the output sum of the common BLSTM and the private BLSTM.
",2 Model,[0],[0]
"All the three loss functions are added to be optimized altogether as:
L = ws",2 Model,[0],[0]
"(Lp + λLa + λLl) , (4)
where λ is gradually increased from 0 to 1 as epoch increases so that the model is stably trained with auxiliary objectives (Ganin et al., 2016).",2 Model,[0],[0]
ws is used to give different weights to the source language and the target language.,2 Model,[0],[0]
"Since the source language has a larger train set and we are focusing on improving the performance of the target language, ws is set to 1 when training the target language.",2 Model,[0],[0]
"For the source language, instead, it is set as the size of the target train set divided by the size of the source train set.",2 Model,[0],[0]
"For the evaluation, we used the POS datasets from 14 different languages in Universal Dependencies corpus 1.4 (Nivre et al., 2016).",3 Experiments,[0],[0]
"We used English as the source language, which is with 12,543 training sentences.2",3 Experiments,[0],[0]
We chose datasets with 1k to 14k training sentences.,3 Experiments,[0],[0]
"The number of tag labels differs for each language from 15 to 18 though most of them are overlapped within the languages.
",3 Experiments,[0],[0]
"Table 1 shows the POS tagging accuracies of different transfer learning models when we limited the number of training sentences of the target languages to be the same as 1,280 for fair comparison among different languages.",3 Experiments,[0],[0]
The remainder training examples of the target languages are still used for both language-adversarial training and bidirectional language modeling since the objectives do not require tag labels.,3 Experiments,[0],[0]
Training with only the train sets in the target languages (c) showed 91.61% on average.,3 Experiments,[0],[0]
"When bidirectional language modeling objective is used (c, l), the accuracies were significantly increased to 92.82% on average.",3 Experiments,[0],[0]
"Therefore, we used the bidirectional language modeling for all the transfer learning evaluations.
",3 Experiments,[0],[0]
"With transfer learning, the three cases of using only the common BLSTM (c), using only the private BLSTMs (p), and using both (c, p) were evaluated.",3 Experiments,[0],[0]
"They showed better average accuracies than target only cases, but they showed mixed results.",3 Experiments,[0],[0]
"However, our proposed model (c, p, l + a), which utilizes both the common BLSTM with language-adversarial training and the private BLSTMs, showed the highest average score, 93.26%.",3 Experiments,[0],[0]
"For all the Germanic languages, where the source language also belongs to, the accuracies are significantly higher than those of
2The accuracies of English POS tagging are 94.01 and 94.33 for models without the bidirectional language modeling and with it, respectively.
",3 Experiments,[0],[0]
other transfer learning models.,3 Experiments,[0],[0]
"For the languages belonging to Slavic, Romance, or Indo-Iranian, our model shows competitive performance with the highest average accuracies among the compared models.",3 Experiments,[0],[0]
"Since languages in the same family are more likely to be similar and compatible, it is expected that the gain from the knowledge transfer to the languages in the same family to be higher than transferring to the languages in different families, which was shown in the results.",3 Experiments,[0],[0]
"This shows that utilizing both language-general representations that are encouraged to be more language-agnostic and language-specific representations effectively helps improve the POS tagging performance with transfer learning.
",3 Experiments,[0],[0]
Table 2 shows the results when using 320 taglabeled training sentences.,3 Experiments,[0],[0]
"In this case, transfer learning methods still show better accuracies than target-only approaches on average.",3 Experiments,[0],[0]
"However, the performance gain is weakened compared to using 1,280 labeled training sentences and there are some mixed results.",3 Experiments,[0],[0]
"In several cases, just utilizing private BLSTMs without the common BLSTM showed better accuracies than utilizing the common BLSTM.
",3 Experiments,[0],[0]
"When training with only 32 tag-labeled sentences, which is an extremely low-resourced setting, transfer learning methods still showed better accuracies than target-only methods on average.",3 Experiments,[0],[0]
"However, not using the common BLSTM
in transfer learning models showed better performance than using it on average.3",3 Experiments,[0],[0]
The main reason would be that we are not given a sufficient number of labeled training sentences to train both the common BLSTM and the private BLSTMs.,3 Experiments,[0],[0]
"In this case, just having private BLSTMs without the common BLSTM can show better performance.",3 Experiments,[0],[0]
"We also evaluated the opposite cases, which use all the tag-labeled training sentences in the target languages, and they showed mixed results.",3 Experiments,[0],[0]
"For example, the accuracy of German with the target only model is 93.31% while that of the proposed model is 93.04%.",3 Experiments,[0],[0]
"This is expected since transfer learning is effective when the target train set is small.
",3 Experiments,[0],[0]
An extension of this work is utilizing multiple languages as the source languages.,3 Experiments,[0],[0]
"Since we have four languages for each of Germanic, Slavic, and Romance language families, we evaluated the performance of those languages using the other languages in the same families as the source languages expecting that languages in the same language family are more likely to be helpful each other.",3 Experiments,[0],[0]
"For the efficiency, we performed multi-task learning for multiple languages rather than differentiating the targets from sources.",3 Experiments,[0],[0]
"When we tried to use 1,280, 320, and 32 tag-labeled training sentences for each language in the multi-source settings, the results showed noticeably better per-
3The results in detail are shown in the first authors dissertation Kim (2017).
formance than the results of using English as a single source language.",3 Experiments,[0],[0]
"Considering that utilizing 1,280*3=3,840, 320*3=960, or 32*3=96 tag labels from three other languages showed better results than using 12,543 English tag labels as the source, we can see that the knowledge transfer from multiple languages can be more helpful than that from single resource-rich source language.",3 Experiments,[0],[0]
"We also tried to use Wasserstein distance (Arjovsky et al., 2017) for the adversarial training in the multi-source settings, but there were no significant differences on average.4
Implementation Details All the models were optimized using ADAM (Kingma and Ba, 2015)5 with minibatch size 32 for total 100 epochs and we picked the parameters showing the best accuracy on the development set to report the score on the test set.",3 Experiments,[0],[0]
The dimensionalites of all the BLSTM related layers follow Plank et al. (2016)’s model.,3 Experiments,[0],[0]
Each word vector is 128 dimensional and each character vector is 100 dimensional.,3 Experiments,[0],[0]
"They are randomly initialized with Xavier initialization (Glorot and Bengio, 2010).",3 Experiments,[0],[0]
"For stable training, we use gradient clipping, where the threshold is set to 5.",3 Experiments,[0],[0]
"The dimensionality of each hidden output of LSTMs is 100, and the hidden outputs of both forward LSTM and backward LSTM are concatenated, thereby the output of each BLSTM for each time step is 200.",3 Experiments,[0],[0]
"Therefore, the input to the common BLSTM and the private BLSTM is 128+200=328
4The extended work in detail are shown in Kim (2017).",3 Experiments,[0],[0]
"5learning rate=0.001, β1 = 0.9, β2 = 0.999, = 1e− 8.
dimensional.",3 Experiments,[0],[0]
"The inputs and the outputs of the BLSTMs are regularized with dropout rate 0.5 (Pham et al., 2014).",3 Experiments,[0],[0]
"For the consistent dropout usages, we let the dropout masks to be identical for all the time steps of each sentence (Gal and Ghahramani, 2016).",3 Experiments,[0],[0]
"For all the BLSTMs, forget biases are initialized with 1 (Jozefowicz et al., 2015) and the other biases are initialized with 0.",3 Experiments,[0],[0]
"Each convolution filter output for the sentence encoding is 64 dimensional, and the three filter outputs are concatenated to represent each sentence with a 192 dimensional vector.",3 Experiments,[0],[0]
We introduced a cross-lingual transfer learning model for POS tagging which uses separate BLSTMs for language-general and languagespecific representations.,4 Conclusion,[0],[0]
"Evaluating on 14 different languages, including the source language improved tagging accuracies in almost all the cases.",4 Conclusion,[0],[0]
"Specifically, our model showed noticeably better performance when the source language and the target languages belong to the same language family, and competitively performed with the highest average accuracies for target languages in different families.",4 Conclusion,[0],[0]
We thank the anonymous reviewers for their helpful comments.,Acknowledgments,[0],[0]
All the experiments in this work were conducted with machines at Ohio Supercomputer Center (1987).,Acknowledgments,[0],[0]
Training a POS tagging model with crosslingual transfer learning usually requires linguistic knowledge and resources about the relation between the source language and the target language.,abstractText,[0],[0]
"In this paper, we introduce a cross-lingual transfer learning model for POS tagging without ancillary resources such as parallel corpora.",abstractText,[0],[0]
"The proposed cross-lingual model utilizes a common BLSTM that enables knowledge transfer from other languages, and private BLSTMs for language-specific representations.",abstractText,[0],[0]
The cross-lingual model is trained with language-adversarial training and bidirectional language modeling as auxiliary objectives to better represent language-general information while not losing the information about a specific target language.,abstractText,[0],[0]
"Evaluating on POS datasets from 14 languages in the Universal Dependencies corpus, we show that the proposed transfer learning model improves the POS tagging performance of the target languages without exploiting any linguistic knowledge between the source language and the target language.",abstractText,[0],[0]
Cross-Lingual Transfer Learning for POS Tagging without Cross-Lingual Resources,title,[0],[0]
"Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 998–1008, Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics",text,[0],[0]
"Discourse structure, logical flow of sentences, and context play a large part in ordering medical events based on temporal relations within a clinical narrative.",1 Introduction,[0],[0]
"However, cross-narrative temporal relation ordering is a challenging task as it is difficult to learn temporal relations among medical events which are not part of the logically coherent discourse of a single narrative.",1 Introduction,[0],[0]
"Resolving crossnarrative temporal relationships between medical events is essential to the task of generating an event timeline from across unstructured clinical narratives such as admission notes, radiology reports, history and physical reports and discharge summaries.",1 Introduction,[0],[0]
"Such a timeline has multiple applications in clinical trial recruitment (Luo et al., 2011), medical document summarization (Bramsen et al.,
2006, Reichert et al., 2010) and clinical decision making (Demner-Fushman et al., 2009).
",1 Introduction,[0],[0]
"Given multiple temporally ordered medical event sequences generated from each clinical narrative in a patient record, how can we combine the events to create a timeline across all the narratives?",1 Introduction,[0],[0]
"The tendency to copy-paste text and summarize past information in newly generated clinical narratives leads to multiple mentions of the same medical event across narratives (Cohen et al., 2013).",1 Introduction,[0],[0]
These cross-narrative coreferences act as important anchors for reasoning with information across narratives.,1 Introduction,[0],[0]
We leverage crossnarrative coreference information along with confident cross-narrative temporal relation predictions and learn to align and temporally order medical event sequences across longitudinal clinical narratives.,1 Introduction,[0],[0]
We model the problem as a sequence alignment task and propose solving this using two approaches.,1 Introduction,[0],[0]
"First, we use weighted finite state machines to represent medical events sequences, thus enabling composition and search to obtain the most probable combined sequence of medical events.",1 Introduction,[0],[0]
"As a contrast, we adapt dynamic programming algorithms (Needleman et al., 1970, Smith and Waterman, 1981) used to produce global and local alignments for aligning sequences of medical events across narratives.",1 Introduction,[0],[0]
"We also compare the proposed methods with an Integer Linear Programming (ILP) based method for timeline construction (Do et al., 2012).",1 Introduction,[0],[0]
"The cross-narrative coreference and temporal relation scores used in both these approaches are learned from a corpus of patient narratives from The Ohio State University Wexner Medical Center.
",1 Introduction,[0],[0]
The main contribution of this paper is a general framework that allows aligning multiple event sequences using cascaded weighted finite state transducers (WFSTs) with the help of efficient composition and decoding.,1 Introduction,[0],[0]
"Moreover, we demonstrate that this method can be used for more accurate multiple sequence alignment when compared to
998
dynamic programming or other ILP-based methods proposed in literature.",1 Introduction,[0],[0]
"In the areas of summarization and text-to-text generation, there has been prior work on several ordering strategies to order pieces of information extracted from different input documents (Barzilay et al., 2002, Lapata, 2003, Bollegala et al., 2010).",2 Related Work,[0],[0]
"In this paper, we focus on temporal ordering of information, as discussed next.
",2 Related Work,[0],[0]
"Recent state-of-the art research has focused on the problem of temporal relation learning within the same document, and in many cases within the same sentence (Mani et al., 2006, Verhagen et al., 2009, Lapata and Lascarides, 2011).",2 Related Work,[0],[0]
"Chambers and Jurafsky (2009) describe a process to induce a partially ordered set of events related by a common protagonist by using an unsupervised distributional method to learn relations between events sharing coreferring arguments, followed by temporal classification to induce partial order.",2 Related Work,[0],[0]
"The task was carried out on the Timebank newswire corpus, but was limited to an intra-document setting.",2 Related Work,[0],[0]
"More recently, (Do et al., 2012) proposed an ILP-based method to combine the outputs of an event-interval and an event-event classifier for timeline construction on the ACE 2005 corpus.",2 Related Work,[0],[0]
"However, this approach is also restricted to events within documents and requires annotations for event intervals.",2 Related Work,[0],[0]
We empirically compare our methods for timeline creation from longitudinal clinical narratives to such an ILP-based approach in Section 7.,2 Related Work,[0],[0]
"While a lot of this work has been done in the news domain, there is also some recent work in rule-based algorithms (Zhou et al., 2006) and machine learning (Roberts et al., 2008) applied to temporal relations between medical events in clinical text.",2 Related Work,[0],[0]
"Clinical narratives are written in a distinct sub-language with domain specific terminology and temporal characteristics, making them markedly different from newswire text.
",2 Related Work,[0],[0]
There is limited prior work in learning relations across documents.,2 Related Work,[0],[0]
"Ji and Grishman (2008) extended the one sense per discourse idea (Yarowsky, 1995) to multiple topically related documents and propagate consistent event arguments across sentences and documents.",2 Related Work,[0],[0]
Barzilay and McKeown (2005) propose a text-to-text generation technique for synthesizing common information across documents using sentence fusion.,2 Related Work,[0],[0]
"This involves multisequence dependency tree alignment to identify phrases conveying sim-
ilar information and statistical generation to combine common phrases into a sentence.",2 Related Work,[0],[0]
"Along with syntactic features, they combine knowledge from resources like WordNet to find similar sentences.",2 Related Work,[0],[0]
"In case of clinical narratives and medical event alignment, the objective is to identify a unique sequence of temporally ordered medical events from across longitudinal clinical data.
",2 Related Work,[0],[0]
"To the best of our knowledge, there is no prior work on cross-document alignment of event sequences.",2 Related Work,[0],[0]
"Multiple sequence alignment is a problem that arises in a variety of domains including gene/protein alignments in bioinformatics (Notredame, 2002), word alignments in machine translation (Kumar and Byrne, 2003), and sentence alignments for summarization (Lacatusu et al., 2004).",2 Related Work,[0],[0]
"Dynamic programming algorithms have been popularly leveraged to produce pairwise and global genetic alignments, where edit distance based metrics are used to compute the cost of insertions, deletions and substitutions.",2 Related Work,[0],[0]
"We use dynamic programming to compute the best alignment, given the temporal and coreference information between medical events across these sequences.",2 Related Work,[0],[0]
"More importantly, we propose a cascaded WFST-based framework for crossdocument temporal ordering of medical event sequences.",2 Related Work,[0],[0]
"Composition and search operations can be used to build a single transducer that integrates these components, directly mapping from input states to desired outputs, and obtain the best alignment (Mohri et al., 2000).",2 Related Work,[0],[0]
"In natural language processing, WFSTs have seen varied applications in machine translation (Kumar and Byrne, 2003), morphology (Sproat, 2006), named entity recognition (Krstev et al., 2011) and biological sequence alignment / generation (Whelan et al., 2010) among others.",2 Related Work,[0],[0]
We demonstrate that the WFST-based approach outperforms popularly used dynamic programming algorithms for multiple sequence alignment.,2 Related Work,[0],[0]
"Medical events are temporally-associated concepts in clinical text that describe a medical condition affecting the patient’s health, or procedures performed on a patient.",3 Problem Description,[0],[0]
We represent medical events by splitting each event into a start and a stop.,3 Problem Description,[0],[0]
"When there is insufficient information to discern the start or stop of an event, it is represented as a single concept.",3 Problem Description,[0],[0]
"If only the start is known then the stop is set to +∞, whereas when only the stop is known , the start is set to the date of birth of the
patient.1 Often, for chronic ailments like hypertension, we would only associate a start with the medical event and set the stop to +∞. The start of hypertension may be associated with the temporal expression history of in the narrative.",3 Problem Description,[0],[0]
"This, when considered along with the admission date, allows us to relatively order hypertension with respect to other medical events.",3 Problem Description,[0],[0]
"A medical event occurrence like chest pain may be associated with a start and a stop, where the start may be determined by the mention of “patient was complaining of chest pain yesterday” in the narrative text.",3 Problem Description,[0],[0]
"Further, the narrative may state that “he continued to have chest pain on admission, but currently he is chest pain free”; this may be used to infer the relative stop of chest pain.",3 Problem Description,[0],[0]
"Medical events may also be instantaneous, for e.g., injected with antibiotic.",3 Problem Description,[0],[0]
Such events are represented with the start and stop as being the same.,3 Problem Description,[0],[0]
Temporal relations exist between the start and stop of events as shown in Figure 1.,3 Problem Description,[0],[0]
"Learning temporal relations before, after and simultaneous between the medical event starts and stops corresponds to learning all of Allen’s temporal relations (Allen, 1981) between the medical events.",3 Problem Description,[0],[0]
"Following our previous work (Raghavan et al., 2012c), such a representation allows us to temporally order the event starts and stops within each clinical narrative by learning to rank them in relative order of time.",3 Problem Description,[0],[0]
"The problem definition is as follows:
1Patient date of birth, admission/ discharge date are usually available in the metadata associated with a clinical narrative.
",3 Problem Description,[0],[0]
Input: Sequences of temporally ordered medical event starts and stops.,3 Problem Description,[0],[0]
"This corresponds to N1, N2, and N3 in Figure 2.",3 Problem Description,[0],[0]
Each sequence corresponds to a clinical narrative.,3 Problem Description,[0],[0]
"The total number of sequences correspond to the number of clinical narratives for a patient.
",3 Problem Description,[0],[0]
Problem:,3 Problem Description,[0],[0]
"Combine medical events across these sequences to generate a timeline i.e., a single comprehensive sequence of medical events over all clinical narratives of the patient.
",3 Problem Description,[0],[0]
Expected Output:,3 Problem Description,[0],[0]
"In the example shown in Figure 2, the output would be as follows:",3 Problem Description,[0],[0]
"Timeline (N1, N2, N3)= {cocaine usestart < hypertensionstart = hypertensionstart < admission1 < chest painstart ∼ palpitationsstart < chest painstop < heart attackstart = myocardial infarctionstart <",3 Problem Description,[0],[0]
"admission2 < infectionstart < MRSAstart < admission3 < woundsstart}.
",3 Problem Description,[0],[0]
The goal of multiple sequence alignment is to find an alignment that maximizes some overall alignment score.,3 Problem Description,[0],[0]
"Thus, in order to align event sequences, we need to compute scores corresponding to cross-narrative medical event coreference resolution and cross-narrative temporal relations.",3 Problem Description,[0],[0]
"The first approach to learning a temporal ordering of medical events across all clinical narratives is to consider all pairs of events across all narratives and learn to classify them as sharing one of Allen’s temporal relations (Allen, 1981) using a single learning model.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"Alternatively, a ranking ap-
proach, similar to the one used to generate intranarrative temporal ordering, can also be extended to the cross-narrative case.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"However, the features related to narrative structure and relative and implicit temporal expressions used for temporal ordering within a clinical narrative may not be applicable across narratives.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"For instance, a history and physical report may have sections like “past medical history”, “history of present illness”, “assessment and plan”, and a certain logical pattern to the flow of text within and across these sections.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"Further, temporal cues like “thereafter”, “subsequently”, follow from the context around an event mention.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"The absence of such features in the cross-narrative case does not allow such a model to generate accurate temporal relation predictions.
",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"Thus, for use in our sequence alignment models, we learn two independent classifiers for medical event coreference and temporal relation learning across narratives.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
We train a classifier to resolve cross-narrative coreferences by extracting semantic and temporal relatedness feature sets for each pair of medical concepts.,4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"Extracting these feature sets helps us train a classifier to predict medical event coreferences (Raghavan et al., 2012a).",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"Another classifier is then trained to classify pairs of medical event starts and stops across narratives as sharing temporal relations {before, after, overlaps}.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
The learned cross-narrative coreference predictions can then be used along with confident temporal relation predictions to derive a joint probability to enable cross-narrative temporal ordering.,4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
Sequence alignment algorithms have been developed and popularly used in bioinformatics.,5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
"However, multiple sequence alignment (MSA) has been shown to be NP complete (Wang and Jiang, 1994) and various heuristic algorithms have been proposed to solve this problem (Notredame, 2002).",5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
"We propose a novel WFST-based representation that enables accurate decoding for MSA when compared to popularly used dynamic programming algorithms (Needleman et al., 1970, Smith and Waterman, 1981) or other state of the art methods (Do et al., 2012).
",5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
"In the problem of aligning events across multiple narrative sequences, we want to align temporally ordered medical events corresponding to clinical narratives of a patient.",5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
"Unlike problems in biological sequence alignment where the sym-
bols to be aligned across sequences are restricted to a fixed set, our symbol set is not fixed or certain because the symbols correspond to medical events in clinical narratives.",5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
"Moreover, we cannot have fixed scores for symbol transformations since our transformations correspond to coreference and temporal relations between the medical events across sequences.",5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
The computation of these scores is described next.,5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
"Let us assume a, b are medical events in the first clinical narrative and have been temporally ordered so a < b.",5.1 Scoring Scheme,[0],[0]
"Similarly, x, y are medical events in the second clinical narrative such that x",5.1 Scoring Scheme,[0],[0]
< y.,5.1 Scoring Scheme,[0],[0]
"There exists a match or an alignment between a pair of medical events, across the sequences, in the following cases:
1.",5.1 Scoring Scheme,[0],[0]
"If the medical events are simultaneous and coreferring, denoted as a = x.
2.",5.1 Scoring Scheme,[0],[0]
"If the medical events are simultaneous and non-coreferring, denoted as a ∼ x.
3.",5.1 Scoring Scheme,[0],[0]
"If the a medical event from one sequence is before a medical event from another sequence, denoted as a < x.
4.",5.1 Scoring Scheme,[0],[0]
"If the a medical event from one sequence is after a medical event from another sequence, denoted as a > x.
We now illustrate how the scores for candidate aligned sequences are computed using the learned cross-narrative coreference and temporal probabilities for the following three scenarios:
• The medical events across sequences are simultaneous and corefer as illustrated in Figure 3.",5.1 Scoring Scheme,[0],[0]
"The joint score considers the probability of event temporal relations simultaneous conditioned on coreference.
",5.1 Scoring Scheme,[0],[0]
•,5.1 Scoring Scheme,[0],[0]
Some medical events across sequences are simultaneous but do not corefer as illustrated in Figure 4.,5.1 Scoring Scheme,[0],[0]
"Here, the joint score considers the joint probability of temporal relations simultaneous or before and no-coreference.
",5.1 Scoring Scheme,[0],[0]
•,5.1 Scoring Scheme,[0],[0]
The medical events across sequences are not simultaneous and do not corefer as illustrated in Figure 5.,5.1 Scoring Scheme,[0],[0]
"In this case, the joint score considers the probability of the temporal relation before and no coreference.
",5.1 Scoring Scheme,[0],[0]
"Thus, the coreference and temporal relation scores can be leveraged for aligning sequences of medical events.",5.1 Scoring Scheme,[0],[0]
"These scores are used in both the WFSTbased representation and decoding, as well as for dynamic programming.",5.1 Scoring Scheme,[0],[0]
"A weighted finite-state transducer (WFST) is an automaton in which each transition between states
is associated with an input symbol, an output symbol, and a weight (Mohri et al., 2005).",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
WFSTs can be used to efficiently represent and combine sequences of medical events based coreference and temporal relation information.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
The WFST representation gives us the ability to talk about the global joint probability derived from coreference and temporal relation scores described in Section 5.1.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
It allows us to build a weighted lattice of sequences that can be searched for the most probable sequence of medical events from across all clinical narratives of a patient.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"We use unweighted FSAs to represent the input described in Section 3, i.e. temporally ordered sequences of medical events corresponding to clinical narratives.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"This corresponds to N1 and N2 in Figure 6.
",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Based on whether we want to align the sequences purely based on coreference scores or both coreference and temporal relation scores, the arc weights for the WFST can be determined.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
M c12 is a WFST that maps input symbols from N1 to output symbols inN2 and is weighted by the probability of coreference or no-coreference between medical events across N1 and N2.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
The representation in WFST M c+t12 shown in Figure 7 allows us to align N1 and N2 based on both coreference as well as temporal relation probabilities.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
The WFST has transitions to accommodate insertion and deletion of medical events when combining the sequences.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
Deletions correspond to the case when an event in the first sequence does not map to any event in the second sequence; similarly insertions correspond to the case where an event in the second sequence does not map to any event in the first sequence.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
The WFST composition operation allows the outputs of one WFST to be fed to the inputs of a second WFST or FSA.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Thus, we build our final machine by composing the three sub-machines as,
D = N1 ◦M i12 ◦N2.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
(1) where i = c or i = c + t.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
This gives us a combined weighted graph by mapping the output symbols of the first medical event sequence to the input symbols of the second medical event sequence.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"The scores on the decoding graph are derived from only the coreference probabilities if i = c and both coreference and temporal relation probabilities if i = c+ t.
In the medical event sequence alignment problem, we want to align multiple sequences of medical events that correspond to multiple clinical narratives of a patient.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Since we want to now combine
all narrative chains belonging to the same patient, the composition cascade to build the final combined sequence will be as,
Df = N1◦M i12◦N2◦M i23◦N3◦M i34...◦",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Nn (2)
where i = c",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
or i = c + t and n is the number of medical event sequences corresponding to clinical narratives for a patient.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"During composition we retain intermediate paths like M i23 utilizing the ability to do lazy composition (Mohri and Pereira, 1998) in order to facilitate beam search through the multi-alignment.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
The best hypothesis corresponds to the highest scoring path which can be obtained using shortest path algorithms like Djikstra’s algorithm.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"The best path corresponds to the best alignment across all medical event sequences based on the joint probability of cross-narrative medical event coreferences and temporal relations across the narrative sequences.
",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"The complexity of decoding increases exponentially with the number of narrative sequences in
the composition, and exact decoding becomes infeasible.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"One solution to this problem is to do the alignment greedily pairwise, starting from the most recent medical event sequences, finding the best path, and iteratively moving on to the next sequence, and proceeding until the oldest medical event sequence.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"The disadvantage of such a method is that it does not take into account constraints between medical events across multiple event sequences and may lead to a less accurate solution.
",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
An alternative method is to use lazy composition to perform more efficient composition as it allows practical memory usage.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"We also use beam search to make for an efficient approximation to the best-path computation (Mohri et al., 2005).",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
This allows accommodating constraints from across multiple sequences and generates a more accurate best path.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Thus, this method generates more accurate alignments when we have more than two sequences to be aligned.
",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"For instance, instance say a, b ∈ N1, x, y ∈ N2, and m,",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"n ∈ N3 are temporally medical event sequences corresponding to narratives N1, N2 and N3.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Based on the learned pairwise temporal relations, if we have the following constraints a < x, m > x, m < a.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Aligning N1 and N2 greedily pairwise may give us the best combined sequence as a, x, b, y ∈ N12.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Now in aligning N12 with N3, we won’t be able to accommodate m > x",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
and m < a.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"However, performing a beam search over the composed WFST in equation 2 allows us to accommodate such constraints across multiple sequences.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"The complexity of composing two transducers is O(V1V2D1(logD2 + M2)) where each edge from the first sequence matches every edge in the second sequence and Vi is the number of states, Di is the maximum out-degree and Mi maximum multiplicity for the ith FST (Mohri et al., 2005).
",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"We also use popular dynamic programming algorithms (Needleman et al., 1970, Smith and Waterman, 1981) for sequence alignment of medical events across narratives and compare it to the WFST-based representation and decoding.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"As a contrast, we adapt two dynamic programming algorithms for sequence alignment: global alignment using the Needleman Wunsch algorithm (NW) (Needleman et al., 1970) and local alignment using the Smith-Waterman algorithm (SW) (Smith and Waterman, 1981).",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
NW allows us to align all events in one sequence with all events in another sequence.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
A drawback of NW is that short and highly similar sequences maybe missed because they get overweighted by the rest of the sequence.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
NW is suitable when the two sequences are of similar length with significant degree of similarity throughout.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"On the other hand, SW gives the longest sub-sequence pair that yields maximum degree of similarity between the two original sequences.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
It does not force all events in a sequence to align with another sequence.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
SW is useful in aligning sequences that differ in length and have short patches of similarity.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"The time complexity of these methods for sequences of length m and n are O(mn).
",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
The scoring scheme described earlier is used to update the scoring matrix for dynamic programming.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"In order to accommodate the temporal relations before and after, we insert a null symbol after every medical event in each sequence in the scoring matrix.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"A vertical or horizontal gap arises when cases 1, 2, 3 and 4 in Section 5.1 mentioned
above are not true.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"If the medical events are not simultaneous, not before or not after, the medical events will not align.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"Thus, the value of each cell in the scoring matrix is determined by computing the maximum score at each position C(i, j) as,
max{(C(i−1, j−1)+Sij), (C(i, j−1)+w), (C(i− 1, j) + w)} (3)
where, Sij = max{P (i = j), P (i < j), P (i > j)}, and w = max{(1",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"− P (i = j)), (1 − P (i < j)), (1 − P (i > j))}.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"Here, C(i − 1, j − 1) corresponds to a match, whereas C(i, j − 1) and C(i − 1, j) correspond to a gaps in sequence one and two.
",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"In case of the SW algorithm, the negative scoring matrix cells are set to zero, thus making the positively scoring local alignments visible.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"Backtracking starts at the highest scoring matrix cell and proceeds until a cell with score zero is encountered, yielding the highest scoring local alignment.
",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
The time and space complexity grows exponentially with the number of sequences to be aligned and finding the global optimum has been shown to be a NP-complete problem.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"The time complexity of aligning N sequences of length L is O(2NLN ) (Wang and Jiang, 1994).",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"Thus, for MSA using dynamic programming, we use a heuristic method where we combine pairwise alignments iteratively starting with the latest narrative and progressing towards the oldest narrative.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
Corpus Description.,6 Experiments and Evaluation,[0],[0]
The corpus consists of a dataset of clinical narratives obtained from the [redacted] medical center.,6 Experiments and Evaluation,[0],[0]
"The corpus has a total of 2060 patients, and 100704 clinical narratives.",6 Experiments and Evaluation,[0],[0]
"We gathered a gold standard set of seven patients (80 clinical narratives overall) with manual annotation of all medical events mentioned in the narratives, coreferences, and medical event sequence information.",6 Experiments and Evaluation,[0],[0]
"The annotation agreement across annotators is high, with 89.5% agreement corresponding to inter-annotator Cohen’s kappa statistic of 0.86 (Raghavan et al., 2012b).",6 Experiments and Evaluation,[0],[0]
"The types of clinical narratives included 27 discharge summaries, 30 history and physical reports, 15 radiology reports and 8 pathology reports.",6 Experiments and Evaluation,[0],[0]
The distribution of the number of medical event sequences and unique medical events across patients is shown in Table 1.,6 Experiments and Evaluation,[0],[0]
"The annotated dataset is used to crossvalidate and train our coreference and temporal relation learning models and to evaluate our crossnarrative medical event timeline.
",6 Experiments and Evaluation,[0],[0]
Evaluation Metric.,6 Experiments and Evaluation,[0],[0]
"For each patient and each method (WFST or dynamic programming), the output timeline to evaluate is the highest scoring candidate hypothesis derived as described above.",6 Experiments and Evaluation,[0],[0]
Accuracy of the timeline is calculated as the number of transformations required to obtain the reference sequence in the annotated gold-standard from the one generated by our system.,6 Experiments and Evaluation,[0],[0]
"Transformations are measured in terms of the minimum edit distance, insertions, deletions, and substitutions of medical events.
",6 Experiments and Evaluation,[0],[0]
Experiments and Results.,6 Experiments and Evaluation,[0],[0]
"We first temporally order medical events within each clinical narrative by learning to rank them in relative order of occurence as described in our previous work (Raghavan et al., 2012c).",6 Experiments and Evaluation,[0],[0]
The overall accuracy of ranking medical events using leave-one-out cross validation is 82.1%.,6 Experiments and Evaluation,[0],[0]
"The resulting medical event sequences serve as the input to the problem of crossnarrative sequence alignment.
",6 Experiments and Evaluation,[0],[0]
The cross-narrative coreference and temporal relation pairwise classification models described in Section 4 are trained using a Maximum entropy classifier.,6 Experiments and Evaluation,[0],[0]
The coreference resolution performs with 71.5% precision and 82.3% recall.,6 Experiments and Evaluation,[0],[0]
The temporal relation classifier performs with 60.2% precision and 76.3% recall.,6 Experiments and Evaluation,[0],[0]
"The learned pairwise coreference and temporal relation probabilities are now used to derive the score for the WFST and dynamic programming approaches.
WFST representation and decoding.",6 Experiments and Evaluation,[0],[0]
We build finite-state machines using the open source OpenFST,6 Experiments and Evaluation,[0],[0]
library.2,6 Experiments and Evaluation,[0],[0]
We use a tropical semi-ring weighted using the negative log-likelihood of the computed scores.,6 Experiments and Evaluation,[0],[0]
"OpenFST provides tools that can search for the highest scoring sequences accepted by the machine, and can sample from highscoring sequences probabilistically, by treating the
2www.openfst.org
scores of each transition within the machine as a negative log probability.",6 Experiments and Evaluation,[0],[0]
The decoding process to compute the most likely combined medical event sequence can be defined as searching for the best path in the combined graph representation (Equation 2).,6 Experiments and Evaluation,[0],[0]
The best path is the one that minimizes the total weight on a path (since the arcs are negative log probabilities).,6 Experiments and Evaluation,[0],[0]
"In searching for the best path, the beam size is set to 5.",6 Experiments and Evaluation,[0],[0]
"The accuracy of the WFST-based representation and beam search across all sequences using the coreference and temporal relation scores to obtain the combined aligned sequence is 78.9%.
",6 Experiments and Evaluation,[0],[0]
Dynamic Programming.,6 Experiments and Evaluation,[0],[0]
We use the NW and SW algorithms described in Section 5.3 to produce local and global alignments respectively.,6 Experiments and Evaluation,[0],[0]
We use the scoring scheme described in Section 5.1 to update the cost matrix for dynamic programming and implement the algorithms as described in Section 5.3.,6 Experiments and Evaluation,[0],[0]
The overall accuracy of sequence alignment with both coreference and temporal relation scores using NW is 68.7% whereas SW gives an accuracy of 72.1%.,6 Experiments and Evaluation,[0],[0]
"In case of aligning just two sequences, both methods yield the same results.",6 Experiments and Evaluation,[0],[0]
"The accuracy of cross-narrative MSA for each patient, for each method, using cross validation, is shown in Table 1.",6 Experiments and Evaluation,[0],[0]
Results indicate that the WFSTbased method outperforms the dynamic programming approach for multi-sequence alignment (statistical significance p<0.05).,6 Experiments and Evaluation,[0],[0]
"Morever, the results using both coreference and temporal realtion scores for alignment outperform using only coreference scores for alignment using all approaches.",6 Experiments and Evaluation,[0],[0]
This indicates that cross-narrative temporal relations are important for accurately aligning medical event sequences across narratives.,6 Experiments and Evaluation,[0],[0]
"We propose and evaluate different approaches to multiple sequence alignment of medical events.
",7 Discussion,[0],[0]
Approaches to multi-alignment.,7 Discussion,[0],[0]
We address the problem of aligning medical event sequences using a novel WFST-based framework and empirically demonstrate that it outperforms pairwise progressive alignment using dynamic programming.,7 Discussion,[0],[0]
"This is mainly because the WFST-based allows us to consider temporal constraints from across multiple sequences when performing the alignment.
",7 Discussion,[0],[0]
"Moreover, it also outperforms the integer linear programming (ILP) method for timeline construction proposed in (Do et al., 2012).",7 Discussion,[0],[0]
We implemented the proposed method that also allows combining the output of classifiers subject to some constraints.,7 Discussion,[0],[0]
We derive intervals from event starts and stops and learn two perceptron classifiers for classifying the temporal relations between events and assigning events to intervals.,7 Discussion,[0],[0]
The classifier probabilities are then used to solve the optimization problem using the lpsolve solver.3,7 Discussion,[0],[0]
We also use intra-document coreference information to resolve coreference before performing the global optimization.,7 Discussion,[0],[0]
"We observe that in case of MSA, the optimal solution using ILP is still intractable as the number of constraints increases exponentially with the number of sequences.",7 Discussion,[0],[0]
Aligning pairwise iteratively gives us an overall average accuracy of 68.2% similar to dynamic programming.,7 Discussion,[0],[0]
"While this is comparable to the dynamic programming performance, the WFST-based method significantly outperforms this in case of multialignments for cross-narrative temporal ordering.
",7 Discussion,[0],[0]
Performance and error analysis.,7 Discussion,[0],[0]
"We perform multi-alignments over medical event sequences for a patient, where each sequence corresponds to temporally ordered medical events in a clinical narrative generated using the ranking model described in (Raghavan et al., 2012c).",7 Discussion,[0],[0]
The accuracy of intra-narrative temporal ordering is 82.1%.,7 Discussion,[0],[0]
The errors in performing this intra-narrative ordering may propagate to the cross-narrative model resulting in reduced accuracy.,7 Discussion,[0],[0]
"This may be addressed by considering n-best temporally ordered medical event sequences, generated by the ranking process, and aligning the n-best sequences using the WFST-based framework.",7 Discussion,[0],[0]
"This could be feasible as, practically, the WFST-based method for multialignment takes only a few secs to align a pair of medical event sequences with average length 40.
",7 Discussion,[0],[0]
The accuracy of alignments across multiple medical event sequences is also affected by the error induced by the coreference and temporal relation scores.,7 Discussion,[0],[0]
"Often, insufficient temporal cues leads
3http://lpsolve.sourceforge.net/5.5/
to misclassification of events incorrectly as sharing the “simultaneous” temporal relation and often as coreferring.",7 Discussion,[0],[0]
This induces errors in the score calculation and hence the alignments.,7 Discussion,[0],[0]
"Better methods to address the challenging problem of crossdocument temporal relation learning, perhaps with the help of structured data from the patient record, could improve the accuracy of alignments.
",7 Discussion,[0],[0]
"There is no clear trend with respect to the number of medical events and narratives for a patient (Table 1.), and the alignment accuracy.",7 Discussion,[0],[0]
"In future work, it would be interesting to examine any such correlation and also study the scalability of the WFST-based method for sequence alignment on longer medical event sequences and a larger dataset of patients.",7 Discussion,[0],[0]
"Further, the WFST-based method may be used to model multi-alignment tasks in other speech and language problems as well.",7 Discussion,[0],[0]
We propose a novel framework for aligning medical event sequences across clinical narratives based on coreference and temporal relation information using cascaded WFSTs.,8 Conclusion,[0],[0]
FSTs provide a convenient and flexible framework to model sequences of temporally ordered medical events and compose them into a combined graph representation.,8 Conclusion,[0],[0]
Decoding this graph allows us to jointly maximize coreference as well as temporal relation probabilities to derive a timeline of the most likely temporal ordering of medical events.,8 Conclusion,[0],[0]
This approach to aligning multiple sequences of medical events significantly outperforms other approaches such as dynamic programming.,8 Conclusion,[0],[0]
"Moreover, we demonstrate the importance of learning temporal relations for the task timeline generation from across multiple clinical narratives by empirically proving that decoding using both coreference and temporal relation scores is far more accurate than decoding with only coreference scores.",8 Conclusion,[0],[0]
The project was supported by Award Number Grant R01LM011116 from the National Library of Medicine.,Acknowledgments,[0],[0]
The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Library of Medicine or the National Institutes of Health.,Acknowledgments,[0],[0]
The authors would like to thank Yanzhang He for his input on the WFST-based model.,Acknowledgments,[0],[0]
Cross-narrative temporal ordering of medical events is essential to the task of generating a comprehensive timeline over a patient’s history.,abstractText,[0],[0]
"We address the problem of aligning multiple medical event sequences, corresponding to different clinical narratives, comparing the following approaches: (1) A novel weighted finite state transducer representation of medical event sequences that enables composition and search for decoding, and (2) Dynamic programming with iterative pairwise alignment of multiple sequences using global and local alignment algorithms.",abstractText,[0],[0]
The cross-narrative coreference and temporal relation weights used in both these approaches are learned from a corpus of clinical narratives.,abstractText,[0],[0]
We present results using both approaches and observe that the finite state transducer approach performs performs significantly better than the dynamic programming one by 6.8% for the problem of multiple-sequence alignment.,abstractText,[0],[0]
Cross-narrative temporal ordering of medical events,title,[0],[0]
Relation extraction has made great strides in newswire and Web domains.,1 Introduction,[0],[0]
"Recently, there has
∗",1 Introduction,[0],[0]
"This research was conducted when the authors were at Microsoft Research.
been increasing interest in applying relation extraction to high-value domains such as biomedicine.",1 Introduction,[0],[0]
"The advent of $1000 human genome1 heralds the dawn of precision medicine, but progress in personalized cancer treatment has been hindered by the arduous task of interpreting genomic data using prior knowledge.",1 Introduction,[0],[0]
"For example, given a tumor sequence, a molecular tumor board needs to determine which genes and mutations are important, and what drugs are available to treat them.",1 Introduction,[0],[0]
"Already the research literature has a wealth of relevant knowledge, and it is growing at an astonishing rate.",1 Introduction,[0],[0]
"PubMed2, the online repository of biomedical articles, adds two new papers per minute, or one million each year.",1 Introduction,[0],[0]
"It is thus imperative to advance relation extraction for machine reading.
",1 Introduction,[0],[0]
"In the vast literature on relation extraction, past work focused primarily on binary relations in single sentences, limiting the available information.",1 Introduction,[0],[0]
"Consider the following example: “The deletion mutation on exon-19 of EGFR gene was present in 16 patients, while the L858E point mutation on exon-21 was noted in 10.",1 Introduction,[0],[0]
All patients were treated with gefitinib and showed a partial response.”.,1 Introduction,[0],[0]
"Collectively, the two sentences convey the fact that there is a ternary interaction between the three entities in bold, which is not expressed in either sentence alone.",1 Introduction,[0],[0]
"Namely, tumors with L858E mutation in EGFR gene can be treated with gefitinib.",1 Introduction,[0],[0]
Extracting such knowledge clearly requires moving beyond binary relations and single sentences.,1 Introduction,[0],[0]
N -ary relations and cross-sentence extraction have received relatively little attention in the past.,1 Introduction,[0],[0]
"Prior
1http://www.illumina.com/systems/ hiseq-x-sequencing-system.html
2https://www.ncbi.nlm.nih.gov/pubmed
ar X
iv :1
70 8.
03 74
3v 1
[ cs
.C",1 Introduction,[0],[0]
"L
] 1
2 A
ug 2
work on n-ary relation extraction focused on single sentences (Palmer et al., 2005; McDonald et al., 2005) or entity-centric attributes that can be extracted largely independently (Chinchor, 1998; Surdeanu and Heng, 2014).",1 Introduction,[0],[0]
"Prior work on cross-sentence extraction often used coreference to gain access to arguments in a different sentence (Gerber and Chai, 2010; Yoshikawa et al., 2011), without truly modeling inter-sentential relational patterns.",1 Introduction,[0],[0]
(See Section 7 for a more detailed discussion.),1 Introduction,[0],[0]
"A notable exception is Quirk and Poon (2017), which applied distant supervision to general cross-sentence relation extraction, but was limited to binary relations.
",1 Introduction,[0],[0]
"In this paper, we explore a general framework for cross-sentence n-ary relation extraction, based on graph long short-term memory networks (graph LSTMs).",1 Introduction,[0],[0]
"By adopting the graph formulation, our framework subsumes prior approaches based on chain or tree LSTMs, and can incorporate a rich set of linguistic analyses to aid relation extraction.",1 Introduction,[0],[0]
"Relation classification takes as input the entity representations learned from the entire text, and can be easily extended for arbitrary relation arity n. This approach also facilitates joint learning with kindred relations where the supervision signal is more abundant.
",1 Introduction,[0],[0]
We conducted extensive experiments on two important domains in precision medicine.,1 Introduction,[0],[0]
"In both distant supervision and supervised learning settings, graph LSTMs that encode rich linguistic knowledge outperformed other neural network variants, as well as a well-engineered feature-based classifier.",1 Introduction,[0],[0]
Multitask learning with sub-relations led to further improvement.,1 Introduction,[0],[0]
"Syntactic analysis conferred a significant benefit to the performance of graph LSTMs, especially when syntax accuracy was high.
",1 Introduction,[0],[0]
"In the molecular tumor board domain, PubMedscale extraction using distant supervision from a
small set of known interactions produced orders of magnitude more knowledge, and cross-sentence extraction tripled the yield compared to single-sentence extraction.",1 Introduction,[0],[0]
Manual evaluation verified that the accuracy is high despite the lack of annotated examples.,1 Introduction,[0],[0]
"Let e1, · · · , em be entity mentions in text T .",2 Cross-sentence n-ary relation extraction,[0],[0]
"Relation extraction can be formulated as a classification problem of determining whether a relation R holds for e1, · · · , em in T .",2 Cross-sentence n-ary relation extraction,[0],[0]
"For example, given a cancer patient with mutation v in gene g, a molecular tumor board seeks to find if this type of cancer would respond to drug d. Literature with such knowledge has been growing rapidly; we can help the tumor board by checking if the Respond relation holds for the (d, g, v) triple.
",2 Cross-sentence n-ary relation extraction,[0],[0]
"Traditional relation extraction methods focus on binary relations where all entities occur in the same sentence (i.e., m = 2 and T is a sentence), and cannot handle the aforementioned ternary relations.",2 Cross-sentence n-ary relation extraction,[0],[0]
"Moreover, as we focus on more complex relations and n increases, it becomes increasingly rare that the related entities will be contained entirely in a single sentence.",2 Cross-sentence n-ary relation extraction,[0],[0]
"In this paper, we generalize extraction to cross-sentence, n-ary relations, where m > 2 and T can contain multiple sentences.",2 Cross-sentence n-ary relation extraction,[0],[0]
"As will be shown in our experiments section, n-ary relations are crucial for high-value domains such as biomedicine, and expanding beyond the sentence boundary enables the extraction of more knowledge.
",2 Cross-sentence n-ary relation extraction,[0],[0]
"In the standard binary-relation setting, the dominant approaches are generally defined in terms of the shortest dependency path between the two entities in question, either by deriving rich features from the path or by modeling it using deep neural
networks.",2 Cross-sentence n-ary relation extraction,[0],[0]
"Generalizing this paradigm to the n-ary setting is challenging, as there are ( n 2 ) paths.",2 Cross-sentence n-ary relation extraction,[0],[0]
"One apparent solution is inspired by Davidsonian semantics: first, identify a single trigger phrase that signifies the whole relation, then reduce the n-ary relation to n binary relations between the trigger and an argument.",2 Cross-sentence n-ary relation extraction,[0],[0]
"However, challenges remain.",2 Cross-sentence n-ary relation extraction,[0],[0]
"It is often hard to specify a single trigger, as the relation is manifested by several words, often not contiguous.",2 Cross-sentence n-ary relation extraction,[0],[0]
"Moreover, it is expensive and time-consuming to annotate training examples, especially if triggers are required, as is evident in prior annotation efforts such as GENIA (Kim et al., 2009).",2 Cross-sentence n-ary relation extraction,[0],[0]
"The realistic and widely adopted paradigm is to leverage indirect supervision, such as distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009), where triggers are not available.
",2 Cross-sentence n-ary relation extraction,[0],[0]
"Additionally, lexical and syntactic patterns signifying the relation will be sparse.",2 Cross-sentence n-ary relation extraction,[0],[0]
"To handle such sparsity, traditional feature-based approaches require extensive engineering and large data.",2 Cross-sentence n-ary relation extraction,[0],[0]
"Unfortunately, this challenge becomes much more severe in crosssentence extraction when the text spans multiple sentences.
",2 Cross-sentence n-ary relation extraction,[0],[0]
"To overcome these challenges, we explore a general relation extraction framework based on graph LSTMs.",2 Cross-sentence n-ary relation extraction,[0],[0]
"By learning a continuous representation for words and entities, LSTMs can handle sparsity effectively without requiring intense feature engineering.",2 Cross-sentence n-ary relation extraction,[0],[0]
"The graph formulation subsumes prior LSTM approaches based on chains or trees, and can incorporate rich linguistic analyses.
",2 Cross-sentence n-ary relation extraction,[0],[0]
This approach also opens up opportunities for joint learning with related relations.,2 Cross-sentence n-ary relation extraction,[0],[0]
"For example, the Response relation over d, g, v also implies a binary sub-relation over drug d and mutation v, with the gene underspecified.",2 Cross-sentence n-ary relation extraction,[0],[0]
"Even with distant supervision, the supervision signal for n-ary relations will likely be sparser than their binary sub-relations.",2 Cross-sentence n-ary relation extraction,[0],[0]
Our approach makes it very easy to use multi-task learning over both the n-ary relations and their sub-relations.,2 Cross-sentence n-ary relation extraction,[0],[0]
Learning a continuous representation can be effective for dealing with lexical and syntactic sparsity.,3 Graph LSTMs,[0],[0]
"For sequential data such as text, recurrent neural networks (RNNs) are quite popular.",3 Graph LSTMs,[0],[0]
"They resemble hidden
Markov models (HMMs), except that discrete hidden states are replaced with continuous vectors, and emission and transition probabilities with neural networks.",3 Graph LSTMs,[0],[0]
"Conventional RNNs with sigmoid units suffer from gradient diffusion or explosion, making training very difficult (Bengio et al., 1994; Pascanu et al., 2013).",3 Graph LSTMs,[0],[0]
"Long short-term memory (LSTMs) (Hochreiter and Schmidhuber, 1997) combats these problems by using a series of gates (input, forget and output) to avoid amplifying or suppressing gradients during backpropagation.",3 Graph LSTMs,[0],[0]
"Consequently, LSTMs are much more effective in capturing long-distance dependencies, and have been applied to a variety of NLP tasks.",3 Graph LSTMs,[0],[0]
"However, most approaches are based on linear chains and only explicitly model the linear context, which ignores a variety of linguistic analyses, such as syntactic and discourse dependencies.
",3 Graph LSTMs,[0],[0]
"In this section, we propose a general framework that generalizes LSTMs to graphs.",3 Graph LSTMs,[0],[0]
"While there is some prior work on learning tree LSTMs (Tai et al., 2015; Miwa and Bansal, 2016), to the best of our knowledge, graph LSTMs have not been applied to any NLP task yet.",3 Graph LSTMs,[0],[0]
Figure 2 shows the architecture of this approach.,3 Graph LSTMs,[0],[0]
The input layer is the word embedding of input text.,3 Graph LSTMs,[0],[0]
Next is the graph LSTM which learns a contextual representation for each word.,3 Graph LSTMs,[0],[0]
"For the entities in question, their contextual representations are concatenated and become the input to the relation classifiers.",3 Graph LSTMs,[0],[0]
"For a multi-word entity, we simply used the average of its word representations and leave the exploration of more sophisticated aggregation approaches to future work.",3 Graph LSTMs,[0],[0]
The layers are trained jointly with backpropagation.,3 Graph LSTMs,[0],[0]
"This framework is
agnostic to the choice of classifiers.",3 Graph LSTMs,[0],[0]
"Jointly designing classifiers with graph LSTMs would be interesting future work.
",3 Graph LSTMs,[0],[0]
At the core of the graph LSTM is a document graph that captures various dependencies among the input words.,3 Graph LSTMs,[0],[0]
"By choosing what dependencies to include in the document graph, graph LSTMs naturally subsumes linear-chain or tree LSTMs.
",3 Graph LSTMs,[0],[0]
"Compared to conventional LSTMs, the graph formulation presents new challenges.",3 Graph LSTMs,[0],[0]
"Due to potential cycles in the graph, a straightforward implementation of backpropagation might require many iterations to reach a fixed point.",3 Graph LSTMs,[0],[0]
"Moreover, in the presence of a potentially large number of edge types (adjacent-word, syntactic dependency, etc.), parametrization becomes a key problem.
",3 Graph LSTMs,[0],[0]
"In the remainder of this section, we first introduce the document graph and show how to conduct backpropagation in graph LSTMs.",3 Graph LSTMs,[0],[0]
We then discuss two strategies for parametrizing the recurrent units.,3 Graph LSTMs,[0],[0]
"Finally, we show how to conduct multi-task learning with this framework.",3 Graph LSTMs,[0],[0]
"To model various dependencies from linguistic analysis at our disposal, we follow Quirk and Poon (2017) and introduce a document graph to capture intra- and inter-sentential dependencies.",3.1 Document Graph,[0],[0]
"A document graph consists of nodes that represent words and edges that represent various dependencies such as linear context (adjacent words), syntactic dependencies, and discourse relations (Lee et al., 2013; Xue et al., 2015).",3.1 Document Graph,[0],[0]
"Figure 1 shows the document graph for our running example; this instance suggests that tumors with L858E mutation in EGFR gene responds to the drug gefitinib.
",3.1 Document Graph,[0],[0]
This document graph acts as the backbone upon which a graph LSTM is constructed.,3.1 Document Graph,[0],[0]
"If it con-
tains only edges between adjacent words, we recover linear-chain LSTMs.",3.1 Document Graph,[0],[0]
"Similarly, other prior LSTM approaches can be captured in this framework by restricting edges to those in the shortest dependency path or the parse tree.",3.1 Document Graph,[0],[0]
Conventional LSTMs are essentially very deep feedforward neural networks.,3.2 Backpropagation in Graph LSTMs,[0],[0]
"For example, a left-to-right linear LSTM has one hidden vector for each word.",3.2 Backpropagation in Graph LSTMs,[0],[0]
This vector is generated by a neural network (recurrent unit) that takes as input the embedding of the given word and the hidden vector of the previous word.,3.2 Backpropagation in Graph LSTMs,[0],[0]
"In discriminative learning, these hidden vectors then serve as input for the end classifiers, from which gradients are backpropagated through the whole network.
",3.2 Backpropagation in Graph LSTMs,[0],[0]
"Generalizing such a strategy to graphs with cycles typically requires unrolling recurrence for a number of steps (Scarselli et al., 2009; Li et al., 2016; Liang et al., 2016).",3.2 Backpropagation in Graph LSTMs,[0],[0]
"Essentially, a copy of the graph is created for each step that serves as input for the next.",3.2 Backpropagation in Graph LSTMs,[0],[0]
"The result is a feed-forward neural network through time, and backpropagation is conducted accordingly.
",3.2 Backpropagation in Graph LSTMs,[0],[0]
"In principle, we could adopt the same strategy.",3.2 Backpropagation in Graph LSTMs,[0],[0]
"Effectively, gradients are backpropagated in a manner similar to loopy belief propagation (LBP).",3.2 Backpropagation in Graph LSTMs,[0],[0]
"However, this makes learning much more expensive as each update step requires multiple iterations of backpropagation.",3.2 Backpropagation in Graph LSTMs,[0],[0]
"Moreover, loopy backpropagation could suffer from the same problems encountered to in LBP, such as oscillation or failure to converge.
",3.2 Backpropagation in Graph LSTMs,[0],[0]
"We observe that dependencies such as coreference and discourse relations are generally sparse, so the backbone of a document graph consists of the linear chain and the syntactic dependency tree.",3.2 Backpropagation in Graph LSTMs,[0],[0]
"As in belief propagation, such structures can be leveraged to make backpropagation more efficient by replac-
ing synchronous updates, as in the unrolling strategy, with asynchronous updates, as in linear-chain LSTMs.",3.2 Backpropagation in Graph LSTMs,[0],[0]
"This opens up opportunities for a variety of strategies in ordering backpropagation updates.
",3.2 Backpropagation in Graph LSTMs,[0],[0]
"In this paper, we adopt a simple strategy that performed quite well in preliminary experiments, and leave further exploration to future work.",3.2 Backpropagation in Graph LSTMs,[0],[0]
"Specifically, we partition the document graph into two directed acyclic graphs (DAGs).",3.2 Backpropagation in Graph LSTMs,[0],[0]
"One DAG contains the left-to-right linear chain, as well as other forwardpointing dependencies.",3.2 Backpropagation in Graph LSTMs,[0],[0]
The other DAG covers the right-to-left linear chain and the backward-pointing dependencies.,3.2 Backpropagation in Graph LSTMs,[0],[0]
Figure 3 illustrates this strategy.,3.2 Backpropagation in Graph LSTMs,[0],[0]
"Effectively, we partition the original graph into the forward pass (left-to-right), followed by the backward pass (right-to-left), and construct the LSTMs accordingly.",3.2 Backpropagation in Graph LSTMs,[0],[0]
"When the document graph only contains linear chain edges, the graph LSTMs is exactly a bi-directional LSTMs (BiLSTMs).",3.2 Backpropagation in Graph LSTMs,[0],[0]
"A standard LSTM unit consists of an input vector (word embedding), a memory cell and an output vector (contextual representation), as well as several gates.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"The input gate and output gate control the information flowing into and out of the cell, whereas the forget gate can optionally remove information from the recurrent connection to a precedent unit.
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"In linear-chain LSTMs, each unit contains only one forget gate, as it has only one direct precedent (i.e., the adjacent-word edge pointing to the previous word).",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"In graph LSTMs, however, a unit may have several precedents, including connections to the same word via different edges.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"We thus introduce a forget gate for each precedent, similar to the approach taken by Tai et al. (2015) for tree LSTMs.
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"Encoding rich linguistic analysis introduces many distinct edge types besides word adjacency, such as syntactic dependencies, which opens up many possibilities for parametrization.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"This was not considered in prior syntax-aware LSTM approaches (Tai et al., 2015; Miwa and Bansal, 2016).",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"In this paper, we explore two schemes that introduce more fined-grained parameters based on the edge types.
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
Full Parametrization,3.3 The Basic Recurrent Propagation Unit,[0],[0]
"Our first proposal simply introduces a different set of parameters for each edge type, with computation specified below.
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"it = σ(Wixt + ∑
j∈P (t) U
m(t,j) i hj + bi)
ot = σ(Woxt + ∑
j∈P (t) Um(t,j)o hj + bo) c̃t = tanh(Wcxt + ∑
j∈P (t) Um(t,j)c hj + bc)
ftj = σ(Wfxt",3.3 The Basic Recurrent Propagation Unit,[0],[0]
+,3.3 The Basic Recurrent Propagation Unit,[0],[0]
"U m(t,j) f hj + bf )",3.3 The Basic Recurrent Propagation Unit,[0],[0]
ct = it c̃t,3.3 The Basic Recurrent Propagation Unit,[0],[0]
"+ ∑
j∈P (t) ftj",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"cj
ht = ot tanh(ct)
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"As in standard chain LSTMs, xt is the input word vector for node t, ht is the hidden state vector for node t, W ’s are the input weight matrices, and b’s are the bias vectors.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"σ, tanh, and represent the sigmoid function, the hyperbolic tangent function, and the Hadamard product (pointwise multiplication), respectively.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
The main differences lie in the recurrence terms.,3.3 The Basic Recurrent Propagation Unit,[0],[0]
"In graph LSTMs, a unit might have multiple predecessors (P (t)), for each of which (j) there is a forget gate ftj , and a typed weight matrix Um(t,j), where m(t, j) signifies the connection type between t and j. The input and output gates (it, ot) depend on all predecessors, whereas the forget gate (ftj) only depends on the predecessor with which the gate is associated.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"ct and c̃t represent intermediate computation results within the memory cell, which take into account the input and forget gates, and will be combined with output gate to produce the hidden representation ht.
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"Full parameterization is straightforward, but it requires a large number of parameters when there are many edge types.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"For example, there are dozens of syntactic edge types, each corresponding to a Stanford dependency label.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"As a result, in our experiments we resort to using only the coarse-grained types: word adjacency, syntactic dependency, etc.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"Next, we will consider a more fine-grained approach by learning an edge-type embedding.
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"Edge-Type Embedding To reduce the number of parameters and leverage potential correlation among fine-grained edge types, we learned a lowdimensional embedding of the edge types, and conducted an outer product of the predecessor’s hidden vector and the edge-type embedding to generate a “typed hidden representation”, which is a matrix.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"The new computation is as follows:
it = σ(Wixt + ∑
j∈P (t) Ui ×T (hj ⊗ ej) + bi)
ftj = σ(Wfxt +",3.3 The Basic Recurrent Propagation Unit,[0],[0]
Uf ×T (hj ⊗ ej) + bf ),3.3 The Basic Recurrent Propagation Unit,[0],[0]
"ot = σ(Woxt + ∑
j∈P (t)",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"Uo ×T (hj ⊗ ej) + bo) c̃t = tanh(Wcxt + ∑
j∈P (t) Uc ×T (hj ⊗ ej) + bc) ct",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"= it c̃t + ∑
j∈P (t) ftj cj
ht = ot tanh(ct)
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
U ’s are now l ×,3.3 The Basic Recurrent Propagation Unit,[0],[0]
"l × d tensors (l is the dimension of the hidden vector and d is the dimension for edgetype embedding), and hj ⊗ ej is a tensor product that produces an l × d matrix.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
×T denotes a tensor dot product defined as T ×T,3.3 The Basic Recurrent Propagation Unit,[0],[0]
"A = ∑ d(T:,:,d · A:,d), which produces an l-dimensional vector.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
The edgetype embedding ej is jointly trained with the other parameters.,3.3 The Basic Recurrent Propagation Unit,[0],[0]
The main advantages of a graph formulation are its generality and flexibility.,3.4 Comparison with Prior LSTM Approaches,[0],[0]
"As seen in Section 3.1, linear-chain LSTMs are a special case when the document graph is the linear chain of adjacent words.",3.4 Comparison with Prior LSTM Approaches,[0],[0]
"Similarly, Tree LSTMs (Tai et al., 2015) are a special case when the document graph is the parse tree.
",3.4 Comparison with Prior LSTM Approaches,[0],[0]
"In graph LSTMs, the encoding of linguistic knowledge is factored from the backpropagation strategy (Section 3.2), making it much more flexible, including introducing cycles.",3.4 Comparison with Prior LSTM Approaches,[0],[0]
"For example, Miwa and Bansal (2016) conducted joint entity and binary relation extraction by stacking a LSTM for relation extraction on top of another LSTM for entity recognition.",3.4 Comparison with Prior LSTM Approaches,[0],[0]
"In graph LSTMs, the two can be combined seamlessly using a document graph comprising both the word-adjacency chain and the dependency path between the two entities.
",3.4 Comparison with Prior LSTM Approaches,[0],[0]
The document graph can also incorporate other linguistic information.,3.4 Comparison with Prior LSTM Approaches,[0],[0]
"For example, coreference and discourse parsing are intuitively relevant for cross-sentence relation extraction.",3.4 Comparison with Prior LSTM Approaches,[0],[0]
"Although existing systems have not yet been shown to improve crosssentence relation extraction (Quirk and Poon, 2017), it remains an important future direction to explore incorporating such analyses, especially after adapting them to the biomedical domains (Bell et al., 2016).",3.4 Comparison with Prior LSTM Approaches,[0],[0]
"Multi-task learning has been shown to be beneficial in training neural networks (Caruana, 1998; Collobert and Weston, 2008; Peng and Dredze, 2016).",3.5 Multi-task Learning with Sub-relations,[0],[0]
"By learning contextual entity representations, our framework makes it straightforward to conduct multi-task learning.",3.5 Multi-task Learning with Sub-relations,[0],[0]
The only change is to add a separate classifier for each related auxiliary relation.,3.5 Multi-task Learning with Sub-relations,[0],[0]
"All classifiers share the same graph LSTMs representation learner and word embeddings, and can potentially help each other by pooling their supervision signals.
",3.5 Multi-task Learning with Sub-relations,[0],[0]
"In the molecular tumor board domain, we applied this paradigm to joint learning of both the ternary relation (drug-gene-mutation) and its binary sub-relation (drug-mutation).",3.5 Multi-task Learning with Sub-relations,[0],[0]
Experiment results show that this provides significant gains in both tasks.,3.5 Multi-task Learning with Sub-relations,[0],[0]
"We implemented our methods using the Theano library (Theano Development Team, 2016).",4 Implementation Details,[0],[0]
We used logistic regression for our relation classifiers.,4 Implementation Details,[0],[0]
Hyper parameters were set based on preliminary experiments on a small development dataset.,4 Implementation Details,[0],[0]
Training was done using mini-batched stochastic gradient descent (SGD) with batch size 8.,4 Implementation Details,[0],[0]
"We used a learning rate of 0.02 and trained for at most 30 epochs, with early stopping based on development data (Caruana et al., 2001; Graves et al., 2013).",4 Implementation Details,[0],[0]
"The dimension for the hidden vectors in LSTM units was set to 150, and the dimension for the edge-type embedding was set to 3.",4 Implementation Details,[0],[0]
"The word embeddings were initialized with the publicly available 100-dimensional GloVe word vectors trained on 6 billion words from Wikipedia and web text3 (Pennington et al., 2014).",4 Implementation Details,[0],[0]
"Other model parameters were initialized with random samples drawn uniformly from the range [−1, 1].
",4 Implementation Details,[0],[0]
"In multi-task training, we alternated among all tasks, each time passing through all data for one task4, and updating the parameters accordingly.",4 Implementation Details,[0],[0]
"This was repeated for 30 epochs.
3http://nlp.stanford.edu/projects/glove/ 4However, drug-gene pairs have much more data, so we subsampled the instances down to the same size as the main n-ary relation task.",4 Implementation Details,[0],[0]
"Our main experiments focus on extracting ternary interactions over drugs, genes and mutations, which is important for molecular tumor boards.",5 Domain: Molecular Tumor Boards,[0],[0]
A druggene-mutation interaction is broadly construed as an association between the drug efficacy and the mutation in the given gene.,5 Domain: Molecular Tumor Boards,[0],[0]
There is no annotated dataset for this problem.,5 Domain: Molecular Tumor Boards,[0],[0]
"However, due to the importance of such knowledge, oncologists have been painstakingly curating known relations from reading papers.",5 Domain: Molecular Tumor Boards,[0],[0]
"Such a manual approach cannot keep up with the rapid growth of the research literature, and the coverage is generally sparse and not up to date.",5 Domain: Molecular Tumor Boards,[0],[0]
"However, the curated knowledge can be used for distant supervision.",5 Domain: Molecular Tumor Boards,[0],[0]
"We obtained biomedical literature from PubMed Central5, consisting of approximately one million fulltext articles as of 2015.",5.1 Datasets,[0],[0]
Note that only a fraction of papers contain knowledge about drug-gene-mutation interactions.,5.1 Datasets,[0],[0]
Extracting such knowledge from the vast body of biomedical papers is exactly the challenge.,5.1 Datasets,[0],[0]
"As we will see in later subsections, distant supervision enables us to generate a sizable training set from a small number of manually curated facts, and the learned model was able to extract orders of magnitude more facts.",5.1 Datasets,[0],[0]
"In future work, we will explore incorporating more known facts for distant supervision and extracting from more full-text articles.
",5.1 Datasets,[0],[0]
"We conducted tokenization, part-of-speech tagging, and syntactic parsing using SPLAT (Quirk et al., 2012), and obtained Stanford dependencies (de Marneffe et al., 2006) using Stanford CoreNLP",5.1 Datasets,[0],[0]
"(Manning et al., 2014).",5.1 Datasets,[0],[0]
"We used the entity taggers from Literome (Poon et al., 2014) to identify drug, gene and mutation mentions.
",5.1 Datasets,[0],[0]
"We used the Gene Drug Knowledge Database (GDKD) (Dienstmann et al., 2015) and the Clinical Interpretations of Variants In Cancer (CIVIC) knowledge base6 for distant supervision.",5.1 Datasets,[0],[0]
"The knowledge bases distinguish fine-grained interaction types, which we do not use in this paper.
",5.1 Datasets,[0],[0]
5http://www.ncbi.nlm.nih.gov/pmc/ 6http://civic.genome.wustl.edu,5.1 Datasets,[0],[0]
"After identifying drug, gene and mutation mentions in the text, co-occurring triples with known interactions were chosen as positive examples.",5.2 Distant Supervision,[0],[0]
"However, unlike the single-sentence setting in standard distant supervision, care must be taken in selecting the candidates.",5.2 Distant Supervision,[0],[0]
"Since the triples can reside in different sentences, an unrestricted selection of text spans would risk introducing many obviously wrong examples.",5.2 Distant Supervision,[0],[0]
"We thus followed Quirk and Poon (2017) in restricting the candidates to those occurring in a minimal span, i.e., we retain a candidate only if is no other co-occurrence of the same entities in an overlapping text span with a smaller number of consecutive sentences.",5.2 Distant Supervision,[0],[0]
"Furthermore, we avoid picking unlikely candidates where the triples are far apart in the document.",5.2 Distant Supervision,[0],[0]
"Specifically, we considered entity triples within K consecutive sentences, ignoring paragraph boundaries.",5.2 Distant Supervision,[0],[0]
K = 1 corresponds to the baseline of extraction within single sentences.,5.2 Distant Supervision,[0],[0]
"We explored K ≤ 3, which captured a large fraction of candidates without introducing many unlikely ones.
",5.2 Distant Supervision,[0],[0]
Only 59 distinct drug-gene-mutation triples from the knowledge bases were matched in the text.,5.2 Distant Supervision,[0],[0]
"Even from such a small set of unique triples, we obtained 3,462 ternary relation instances that can serve as positive examples.",5.2 Distant Supervision,[0],[0]
"For multi-task learning, we also considered drug-gene and drug-mutation sub-relations, which yielded 137,469 drug-gene and 3,192 drugmutation relation instances as positive examples.
",5.2 Distant Supervision,[0],[0]
"We generate negative examples by randomly sampling co-occurring entity triples without known interactions, subject to the same restrictions above.",5.2 Distant Supervision,[0],[0]
We sampled the same number as positive examples to obtain a balanced dataset7.,5.2 Distant Supervision,[0],[0]
"To compare the various models in our proposed framework, we conducted five-fold cross-validation, treating the positive and negative examples from distant supervision as gold annotation.",5.3 Automatic Evaluation,[0],[0]
"To avoid traintest contamination, all examples from a document were assigned to the same fold.",5.3 Automatic Evaluation,[0],[0]
"Since our datasets are balanced by construction, we simply report average test accuracy on held-out folds.",5.3 Automatic Evaluation,[0],[0]
"Obviously, the
7We will release the dataset at http://hanover.azurewebsites.net.
results could be noisy (e.g., entity triples not known to have an interaction might actually have one), but this evaluation is automatic and can quickly evaluate the impact of various design choices.
",5.3 Automatic Evaluation,[0],[0]
We evaluated two variants of graph LSTMs: “Graph LSTM-FULL” with full parametrization and “Graph LSTM-EMBED” with edge-type embedding.,5.3 Automatic Evaluation,[0],[0]
"We compared graph LSTMs with three strong baseline systems: a well-engineered feature-based classifier (Quirk and Poon, 2017), a convolutional neural network (CNN)",5.3 Automatic Evaluation,[0],[0]
"(Zeng et al., 2014; Santos et al., 2015; Wang et al., 2016), and a bi-directional LSTM (BiLSTM).",5.3 Automatic Evaluation,[0],[0]
"Following Wang et al. (2016), we used input attention for the CNN and a input window size of 5.",5.3 Automatic Evaluation,[0],[0]
Quirk and Poon (2017) only extracted binary relations.,5.3 Automatic Evaluation,[0],[0]
"We extended it to ternary relations by deriving features for each entity pair (with added annotation to signify the two entity types), and pooling the features
from all pairs.",5.3 Automatic Evaluation,[0],[0]
"For binary relation extraction, prior syntax-aware approaches are directly applicable.",5.3 Automatic Evaluation,[0],[0]
"So we also compared with a state-of-the-art tree LSTM system (Miwa and Bansal, 2016) and a BiLSTM on the shortest dependency path between the two entities (BiLSTM-Shortest-Path) (Xu et al., 2015b).
",5.3 Automatic Evaluation,[0],[0]
"Table 1 shows the results for cross-sentence, ternary relation extraction.",5.3 Automatic Evaluation,[0],[0]
"All neural-network based models outperformed the feature-based classifier, illustrating their advantage in handling sparse linguistic patterns without requiring intense feature engineering.",5.3 Automatic Evaluation,[0],[0]
"All LSTMs significantly outperformed CNN in the cross-sentence setting, verifying the importance in capturing long-distance dependencies.
",5.3 Automatic Evaluation,[0],[0]
"The two variants of graph LSTMs perform on par with each other, though Graph LSTM-FULL has a small advantage, suggesting that further exploration of parametrization schemes could be beneficial.",5.3 Automatic Evaluation,[0],[0]
"In particular, the edge-type embedding might improve by pretraining on unlabeled text with syntactic parses.
",5.3 Automatic Evaluation,[0],[0]
"Both graph variants significantly outperformed BiLSTMs (p < 0.05 by McNemar’s chi-square test), though the difference is small.",5.3 Automatic Evaluation,[0],[0]
This result is intriguing.,5.3 Automatic Evaluation,[0],[0]
"In Quirk and Poon (2017), the best system incorporated syntactic dependencies and outperformed the linear-chain variant (Base) by a large margin.",5.3 Automatic Evaluation,[0],[0]
"So why didn’t graph LSTMs make an equally substantial gain by modeling syntactic dependencies?
",5.3 Automatic Evaluation,[0],[0]
One reason is that linear-chain LSTMs can already captured some of the long-distance dependencies available in syntactic parses.,5.3 Automatic Evaluation,[0],[0]
"BiLSTMs substantially outperformed the feature-based classifier, even without explicit modeling of syntactic dependencies.",5.3 Automatic Evaluation,[0],[0]
"The gain cannot be entirely attributed to word embedding as LSTMs also outperformed CNNs.
",5.3 Automatic Evaluation,[0],[0]
Another reason is that syntactic parsing is less accurate in the biomedical domain.,5.3 Automatic Evaluation,[0],[0]
"Parse errors confuse the graph LSM learner, limiting the potential for gain.",5.3 Automatic Evaluation,[0],[0]
"In Section 6, we show supporting evidence in a domain when gold parses are available.
",5.3 Automatic Evaluation,[0],[0]
"We also reported accuracy on instances within single sentences, which exhibited a broadly similar set of trends.",5.3 Automatic Evaluation,[0],[0]
"Note that single-sentence and crosssentence accuracies are not directly comparable, as the test sets are different (one subsumes the other).
",5.3 Automatic Evaluation,[0],[0]
We conducted the same experiments on the binary sub-relation between drug-mutation pairs.,5.3 Automatic Evaluation,[0],[0]
"Table 2
shows the results, which are similar to the ternary case: Graph LSTM-FULL consistently performed the best for both single sentence and cross-sentence instances.",5.3 Automatic Evaluation,[0],[0]
"BiLSTMs on the shortest path substantially underperformed BiLSTMs or graph LSTMs, losing between 4-5 absolute points in accuracy, which could be attributed to the lower parsing quality in the biomedical domain.",5.3 Automatic Evaluation,[0],[0]
"Interestingly, the state-of-the-art tree LSTMs (Miwa and Bansal, 2016) also underperformed graph LSTMs, even though they encoded essentially the same linguistic structures (word adjacency and syntactic dependency).",5.3 Automatic Evaluation,[0],[0]
"We attributed the gain to the fact that Miwa and Bansal (2016) used separate LSTMs for the linear chain and the dependency tree, whereas graph LSTMs learned a single representation for both.
",5.3 Automatic Evaluation,[0],[0]
"To evaluate whether joint learning with subrelations can help, we conducted multi-task learning using Graph LSTM-FULL to jointly train extractors for both the ternary interaction and the drug-mutation, drug-gene sub-relations.",5.3 Automatic Evaluation,[0],[0]
Table 3 shows the results.,5.3 Automatic Evaluation,[0],[0]
Multi-task learning resulted in a significant gain for both the ternary interaction and the drug-mutation interaction.,5.3 Automatic Evaluation,[0],[0]
"Interestingly, the advantage of graph LSTMs over BiLSTMs is reduced with multi-task learning, suggesting that with more supervision signal, even linear-chain LSTMs can learn to capture long-range dependencies that are were made evident by parse features in graph LSTMs.",5.3 Automatic Evaluation,[0],[0]
"Note that there are many more instances for drug-gene interaction than others, so we only sampled a subset of comparable size.",5.3 Automatic Evaluation,[0],[0]
"Therefore, we do not evaluate the performance gain for drug-gene interaction, as in practice, one would simply learn from all available data, and the sub-sampled results are not competitive.
",5.3 Automatic Evaluation,[0],[0]
We included coreference and discourse relations in our document graph.,5.3 Automatic Evaluation,[0],[0]
"However, we didn’t observe any significant gains, similar to the observation in
Quirk and Poon (2017).",5.3 Automatic Evaluation,[0],[0]
We leave further exploration to future work.,5.3 Automatic Evaluation,[0],[0]
Our ultimate goal is to extract all knowledge from available text.,5.4 PubMed-Scale Extraction,[0],[0]
"We thus retrained our model using the best system from automatic evaluation (i.e., Graph LSTM-FULL) on all available data.",5.4 PubMed-Scale Extraction,[0],[0]
"The resulting model was then used to extract relations from all PubMed Central articles.
",5.4 PubMed-Scale Extraction,[0],[0]
Table 4 shows the number of candidates and extracted interactions.,5.4 PubMed-Scale Extraction,[0],[0]
"With as little as 59 unique druggene-mutation triples from the two databases8, we learned to extract orders of magnitude more unique interactions.",5.4 PubMed-Scale Extraction,[0],[0]
"The results also highlight the benefit of cross-sentence extraction, which yields 3 to 5 times more relations than single-sentence extraction.
",5.4 PubMed-Scale Extraction,[0],[0]
"Table 5 conducts a similar comparison on unique number of drugs, genes, and mutations.",5.4 PubMed-Scale Extraction,[0],[0]
"Again, machine reading covers far more unique entities, especially with cross-sentence extraction.",5.4 PubMed-Scale Extraction,[0],[0]
"Our automatic evaluations are useful for comparing competing approaches, but may not reflect the true classifier precision as the labels are noisy.",5.5 Manual Evaluation,[0],[0]
"Therefore, we randomly sampled extracted relation instances and asked three researchers knowledgeable in precision medicine to evaluate their correctness.",5.5 Manual Evaluation,[0],[0]
"For each instance, the annotators were presented with the provenance: sentences with the drug, gene, and mutation highlighted.",5.5 Manual Evaluation,[0],[0]
"The annotators determined in
8There are more in the databases, but these are the only ones for which we found matching instances in the text.",5.5 Manual Evaluation,[0],[0]
"In future work, we will explore various ways to increase the number, e.g., by matching underspecified drug classes to specific drugs.
",5.5 Manual Evaluation,[0],[0]
each case whether this instance implied that the given entities were related.,5.5 Manual Evaluation,[0],[0]
"Note that evaluation does not attempt to identify whether the relationships are true or replicated in follow-up papers; rather, it focuses on whether the relationships are entailed by the text.
",5.5 Manual Evaluation,[0],[0]
We focused our evaluation efforts on the crosssentence ternary-relation setting.,5.5 Manual Evaluation,[0],[0]
"We considered three probability thresholds: 0.9 for a high-precision but potentially low-recall setting, 0.5, and a random sample of all candidates.",5.5 Manual Evaluation,[0],[0]
"In each case, 150 instances were selected for a total of 450 annotations.",5.5 Manual Evaluation,[0],[0]
"A subset of 150 instances were reviewed by two annotators, and the inter-annotator agreement was 88%.
",5.5 Manual Evaluation,[0],[0]
"Table 6 shows that the classifier indeed filters out a large portion of potential candidates, with estimated instance accuracy of 64% at the threshold of 0.5, and 75% at 0.9.",5.5 Manual Evaluation,[0],[0]
"Interestingly, LSTMs are effective at screening out many entity mention errors, presumably because they include broad contextual features.",5.5 Manual Evaluation,[0],[0]
"We also conducted experiments on extracting genetic pathway interactions using the GENIA Event Extraction dataset (Kim et al., 2009).",6 Domain: Genetic Pathways,[0],[0]
"This dataset contains gold syntactic parses for the sentences, which offered a unique opportunity to investigate the impact of syntactic analysis on graph LSTMs.",6 Domain: Genetic Pathways,[0],[0]
"It also allowed us to test our framework in supervised learning.
",6 Domain: Genetic Pathways,[0],[0]
"The original shared task evaluated on complex, nested events for nine event types, many of which are unary relations (Kim et al., 2009).",6 Domain: Genetic Pathways,[0],[0]
"Following Poon et al. (2015), we focused on gene regulation and reduced it to binary-relation classification for headto-head comparison.",6 Domain: Genetic Pathways,[0],[0]
"We followed their experimental protocol by sub-sampling negative examples to be about three times of positive examples.
",6 Domain: Genetic Pathways,[0],[0]
"Since the dataset is not entirely balanced, we reported precision, recall, and F1.",6 Domain: Genetic Pathways,[0],[0]
We used our best performing graph LSTM from the previous experiments.,6 Domain: Genetic Pathways,[0],[0]
"By default, automatic parses were used in the document graphs, whereas in Graph LSTM (GOLD), gold parses were used instead.",6 Domain: Genetic Pathways,[0],[0]
Table 7 shows the results.,6 Domain: Genetic Pathways,[0],[0]
"Once again, despite the lack of intense feature engineering, linear-chain LSTMs performed on par with the feature-based classifier (Poon et al., 2015).",6 Domain: Genetic Pathways,[0],[0]
"Graph LSTMs exhibited a more commanding advantage over linear-chain LSTMs in this domain, substantially outperforming the latter (p < 0.01 by McNemar’s chi-square test).",6 Domain: Genetic Pathways,[0],[0]
"Most interestingly, graph LSTMs using gold parses significantly outperformed that using automatic parses, suggesting that encoding high-quality analysis is particularly beneficial.",6 Domain: Genetic Pathways,[0],[0]
Most work on relation extraction has been applied to binary relations of entities in a single sentence.,7 Related Work,[0],[0]
"We first review relevant work on the single-sentence bi-
nary relation extraction task, and then review related work on n-ary and cross-sentence relation extraction.
",7 Related Work,[0],[0]
"Binary relation extraction The traditional featurebased methods rely on carefully designed features to learn good models, and often integrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014).",7 Related Work,[0],[0]
"The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information.",7 Related Work,[0],[0]
"Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016).
",7 Related Work,[0],[0]
"Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations.",7 Related Work,[0],[0]
Such representations are then taken by relation classifiers to produce the final predictions.,7 Related Work,[0],[0]
"Effectively representing sequences of words, both convolutional (Zeng et al., 2014; Wang et al., 2016; Santos et al., 2015) and RNN-based architectures (Zhang et al., 2015; Socher et al., 2012; Cai et al., 2016) have been successful.",7 Related Work,[0],[0]
Most of these have focused on modeling either the surface word sequences or the hierarchical syntactic structure.,7 Related Work,[0],[0]
"Miwa and Bansal (2016) proposed an architecture that benefits from both types of information, using a surface sequence layer, followed by a dependency-tree sequence layer.
",7 Related Work,[0],[0]
"N -ary relation extraction Early work on extracting relations between more than two arguments has been done in MUC-7, with a focus on fact/event extraction from news articles (Chinchor, 1998).",7 Related Work,[0],[0]
"Semantic role labeling in the Propbank (Palmer et al., 2005) or FrameNet (Baker et al., 1998) style are also instances of n-ary relation extraction, with extraction of events expressed in a single sentence.",7 Related Work,[0],[0]
"McDonald et al. (2005) extract n-ary relations in a biomedical domain, by first factoring the n-ary relation into pair-wise relations between all entity pairs, and then constructing maximal cliques of related entities.",7 Related Work,[0],[0]
"Recently, neural models have been applied to semantic role labeling (FitzGerald et al., 2015; Roth
and Lapata, 2016).",7 Related Work,[0],[0]
"These works learned neural representations by effectively decomposing the n-ary relation into binary relations between the predicate and each argument, by embedding the dependency path between each pair, or by combining features of the two using a feed-forward network.",7 Related Work,[0],[0]
"Although some re-ranking or joint inference models have been employed, the representations of the individual arguments do not influence each other.",7 Related Work,[0],[0]
"In contrast, we propose a neural architecture that jointly represents n entity mentions, taking into account long-distance dependencies and inter-sentential information.
",7 Related Work,[0],[0]
"Cross-sentence relation extraction Several relation extraction tasks have benefited from crosssentence extraction, including MUC fact and event extraction (Swampillai and Stevenson, 2011), record extraction from web pages (Wick et al., 2006), extraction of facts for biomedical domains (Yoshikawa et al., 2011), and extensions of semantic role labeling to cover implicit inter-sentential arguments (Gerber and Chai, 2010).",7 Related Work,[0],[0]
"These prior works have either relied on explicit co-reference annotation, or on the assumption that the whole document refers to a single coherent event, to simplify the problem and reduce the need for powerful representations of multi-sentential contexts of entity mentions.",7 Related Work,[0],[0]
"Recently, cross-sentence relation extraction models have been learned with distant supervision, and used integrated contextual evidence of diverse types without reliance on these assumptions (Quirk and Poon, 2017), but that work focused on binary relations only and explicitly engineered sparse indicator features.
",7 Related Work,[0],[0]
"Relation extraction using distant supervision Distant supervision has been applied to extraction of binary (Mintz et al., 2009; Poon et al., 2015) and n-ary (Reschke et al., 2014;",7 Related Work,[0],[0]
"Li et al., 2015) relations, traditionally using hand-engineered features.",7 Related Work,[0],[0]
"Neural architectures have recently been applied to distantly supervised extraction of binary relations (Zeng et al., 2015).",7 Related Work,[0],[0]
"Our work is the first to propose a neural architecture for n-ary relation extraction, where the representation of a tuple of entities is not decomposable into independent representations of the individual entities or entity pairs, and which integrates diverse information from multi-sentential context.",7 Related Work,[0],[0]
"To utilize training data more effectively, we show how multitask learning for component binary sub-relations can
improve performance.",7 Related Work,[0],[0]
"Our learned representation combines information sources within a single sentence in a more integrated and generalizable fashion than prior approaches, and can also improve performance on single-sentence binary relation extraction.",7 Related Work,[0],[0]
We explore a general framework for cross-sentence nary relation extraction based on graph LSTMs.,8 Conclusion,[0],[0]
The graph formulation subsumes linear-chain and tree LSTMs and makes it easy to incorporate rich linguistic analysis.,8 Conclusion,[0],[0]
"Experiments on biomedical domains showed that extraction beyond the sentence boundary produced far more knowledge, and encoding rich linguistic knowledge provided consistent gain.
",8 Conclusion,[0],[0]
"While there is much room to improve in both recall and precision, our results indicate that machine reading can already be useful in precision medicine.",8 Conclusion,[0],[0]
"In particular, automatically extracted facts (Section 5.4) can serve as candidates for manual curation.",8 Conclusion,[0],[0]
"Instead of scanning millions of articles to curate from scratch, human curators would just quickly vet thousands of extractions.",8 Conclusion,[0],[0]
The errors identified by curators offer direct supervision to the machine reading system for continuous improvement.,8 Conclusion,[0],[0]
"Therefore, the most important goal is to attain high recall and reasonable precision.",8 Conclusion,[0],[0]
"Our current models are already quite capable.
",8 Conclusion,[0],[0]
Future directions include: interactive learning with user feedback; improving discourse modeling in graph LSTMs; exploring other backpropagation strategies; joint learning with entity linking; applications to other domains.,8 Conclusion,[0],[0]
"We thank Daniel Fried and Ming-Wei Chang for useful discussions, as well as the anonymous reviewers and editor-in-chief Mark Johnson for their helpful comments.",Acknowledgements,[0],[0]
Past work in relation extraction has focused on binary relations in single sentences.,abstractText,[0],[0]
Recent NLP inroads in high-value domains have sparked interest in the more general setting of extracting n-ary relations that span multiple sentences.,abstractText,[0],[0]
"In this paper, we explore a general relation extraction framework based on graph long short-term memory networks (graph LSTMs) that can be easily extended to cross-sentence n-ary relation extraction.",abstractText,[0],[0]
"The graph formulation provides a unified way of exploring different LSTM approaches and incorporating various intra-sentential and intersentential dependencies, such as sequential, syntactic, and discourse relations.",abstractText,[0],[0]
"A robust contextual representation is learned for the entities, which serves as input to the relation classifier.",abstractText,[0],[0]
"This simplifies handling of relations with arbitrary arity, and enables multi-task learning with related relations.",abstractText,[0],[0]
"We evaluate this framework in two important precision medicine settings, demonstrating its effectiveness with both conventional supervised learning and distant supervision.",abstractText,[0],[0]
Cross-sentence extraction produced larger knowledge bases.,abstractText,[0],[0]
and multi-task learning significantly improved extraction accuracy.,abstractText,[0],[0]
A thorough analysis of various LSTM approaches yielded useful insight the impact of linguistic analysis on extraction accuracy.,abstractText,[0],[0]
Cross-Sentence N -ary Relation Extraction with Graph LSTMs,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 778–783 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
778",text,[0],[0]
"Stance classification is the task of automatically identifying users’ positions about a specific target from text (Mohammad et al., 2017).",1 Introduction,[0],[0]
"Table 1 shows an example of this task, where the stance of the sentence is recognized as favorable on the target climate change is concern.",1 Introduction,[0],[0]
"Traditionally, this task is approached by learning a target-specific classifier that is trained for prediction on the same target of interest (Hasan and Ng, 2013; Mohammad et al., 2016; Ebrahimi et al., 2016).",1 Introduction,[0],[0]
"This implies that a new classifier has to be built from scratch on a well-prepared set of ground-truth data whenever predictions are needed for an unseen target.
",1 Introduction,[0],[0]
"An alternative to this approach is to conduct a cross-target classification, where the classifier is adapted from different but related targets (Augenstein et al., 2016), which allows benefiting from the knowledge of existing targets.",1 Introduction,[0],[0]
"For example, in our project we are interested in online users’ stances on the approvals of particular mining projects in the country.",1 Introduction,[0],[0]
"It might be useful to start with a classifier that is adapted from a related target such as climate change is concern (presumably available and annotated), as in both cases
users could discuss the impacts from the targets to some common issues, such as the environment or communities.
",1 Introduction,[0],[0]
Cross-target stance classification is a more challenging task simply because the language models may not be compatible between different targets.,1 Introduction,[0],[0]
"However, for some targets that can be recognized as being related to the same and more general domains, it could be possible to generalize through certain aspects of the domains that reflect users’ major concerns.",1 Introduction,[0],[0]
"For example, from the following sentence, whose stance is against the approval of a mining project, “Environmentalists warn the $16 billion coal facility will damage the Great Barrier Reef”, it can be seen that both this sentence and the one in Table 1 mention the same aspect “reef destruction/damage”, which is closely related to the “environment” domain.
",1 Introduction,[0],[0]
"In this paper, we focus on cross-target stance classification and explore the limits of generalizing models between different but domain-related targets1.",1 Introduction,[0],[0]
"The basic idea is to learn a set of domainspecific aspects from a source target, and then apply them to prediction on a destination target.",1 Introduction,[0],[0]
"To this end, we propose CrossNet, a novel neural model that implements the above idea based on the self-attention mechanism.",1 Introduction,[0],[0]
"Our preliminary analysis shows that the proposed model can find useful domain-specific information from a stancebearing sentence and that the classification performance is improved in certain domains.
",1 Introduction,[0],[0]
"1In this work, the source target is chosen based on common sense.",1 Introduction,[0],[0]
Exploring more sophisticated source target selection methods will be our future work.,1 Introduction,[0],[0]
"In this section, we introduce the proposed model, CrossNet, for cross-target stance classification.",2 Model,[0],[0]
Figure 1 shows the architecture of CrossNet.,2 Model,[0],[0]
It consists of four layers from the Embedding Layer (bottom) to the Prediction Layer (top).,2 Model,[0],[0]
It works by taking a stance-bearing sentence and a target as input and yielding the predicted stance label as output.,2 Model,[0],[0]
"In the following, we present the implementation of each layer in CrossNet.",2 Model,[0],[0]
"There are two inputs in CrossNet: a stance-bearing sentence P and a descriptive target T (e.g, climate change is concern in Table 1).",2.1 Embedding Layer,[0],[0]
"We use word embeddings (Mikolov et al., 2013) to represent each word in the input as a dense vector.",2.1 Embedding Layer,[0],[0]
"The output of this layer are two sequences of vectors P = {p1, ...,p|P |} and T = {t1, ..., t|T |}, where p, t are word vectors.",2.1 Embedding Layer,[0],[0]
"In this layer, we encode the contextual information in the input sentence and target.",2.2 Context Encoding Layer,[0],[0]
"We use a bi-directional Long Short-Term Memory Network (BiLSTM) (Hochreiter and Schmidhuber, 1997) to capture the left and right contexts of each word in the input.",2.2 Context Encoding Layer,[0],[0]
"Moreover, to account for the impact of the target on stance inference, we borrow the idea of conditional encoding (Augenstein et al., 2016) to model the dependency of the sentence on the target.",2.2 Context Encoding Layer,[0],[0]
"Formally, we first use a BiLSTMT to encode the target:
",2.2 Context Encoding Layer,[0],[0]
[ −→ h Ti −→c,2.2 Context Encoding Layer,[0],[0]
"Ti ] = −−−−→ LSTMT (ti, −→ h Ti−1, −→c",2.2 Context Encoding Layer,[0],[0]
Ti−1),2.2 Context Encoding Layer,[0],[0]
"[ ←− h Ti ←−c Ti ] = ←−−−− LSTMT (ti, ←− h Ti+1, ←−c Ti+1)",2.2 Context Encoding Layer,[0],[0]
"(1)
where h ∈ Rh and c ∈",2.2 Context Encoding Layer,[0],[0]
Rh are the hidden state and cell state of LSTM.,2.2 Context Encoding Layer,[0],[0]
The symbol −→(←−) indicates the forward (backward) pass.,2.2 Context Encoding Layer,[0],[0]
"ti is the input word vector at time step i.
",2.2 Context Encoding Layer,[0],[0]
"Then, we learn a conditional encoding of the sentence P , by initializing BiLSTMP (a different BiLSTM) with the final states of BiLSTMT :
",2.2 Context Encoding Layer,[0],[0]
"[ −→ h P1 −→c P1 ] = −−−−→ LSTMP (p1, −→ h",2.2 Context Encoding Layer,[0],[0]
T|T,2.2 Context Encoding Layer,[0],[0]
"|, −→c T|T",2.2 Context Encoding Layer,[0],[0]
"|)
",2.2 Context Encoding Layer,[0],[0]
"[ ←− h P|P | ←−c P|P |] = ←−−−− LSTMP (p|P |, ←− h T1 , ←−c T1 )
(2)
It can be seen that the initialization is done by aligning the forward (backward) pass of the two BiLSTMs.",2.2 Context Encoding Layer,[0],[0]
"The output is a contextually-encoded sequence, HP = {hP1 , ...,hP|P |}, where h =",2.2 Context Encoding Layer,[0],[0]
[ −→ h ; ←− h ] ∈ R2h with [; ] as the vector concatenation operation.,2.2 Context Encoding Layer,[0],[0]
"In this layer, we implement the idea of discovering domain-specific aspects for cross-target stance inference.",2.3 Aspect Attention Layer,[0],[0]
"In particular, the key observation we make is that the domain aspects that reflect users’ major concerns are usually the core of understanding their stances, and could be mentioned by multiple users in a discussion.",2.3 Aspect Attention Layer,[0],[0]
"For example, we find that many users in our corpus mention the aspect “reef” to express their concerns about the impact of a mining project on the Great Barrier Reef.",2.3 Aspect Attention Layer,[0],[0]
"Based on this observation, the perception of the domain aspects can be boiled down to finding the sentence parts that not only carry the core idea of a stance-bearing sentence but also tend to be recurring in the corpus.
",2.3 Aspect Attention Layer,[0],[0]
"First, to capture the recurrences of the domain aspects, a simple way is to make every input sentence be consumed by this layer (see Figure 1), so that the layer parameters are shared across the corpus for being stimulated by all appearances of the domain aspects.
",2.3 Aspect Attention Layer,[0],[0]
"Then, we utilize self-attention to signal the core parts of a stance-bearing sentence.",2.3 Aspect Attention Layer,[0],[0]
"Self-attention is an attention mechanism for selecting specific parts of a sequence by relating its elements at different positions (Vaswani et al., 2017; Cheng et al., 2016).",2.3 Aspect Attention Layer,[0],[0]
"In our case, the self-attention process is based on the assumption that the core parts of a sentence are those that are compatible with the semantics of the entire sentence.",2.3 Aspect Attention Layer,[0],[0]
"To this end, we introduce a compatibility function to score the semantic compatibility between the encoded se-
quence HP and each of its hidden states hP :
ci = w > 2 σ(W1h P i + b1) + b2 (3)
where W1 ∈ Rd×2h, w2 ∈ Rd, b1 ∈ Rd, and b2 ∈ R are trainable parameters, and σ is the activation function.",2.3 Aspect Attention Layer,[0],[0]
Note that all the above parameters are shared by every hidden state in HP .,2.3 Aspect Attention Layer,[0],[0]
"Next, we compute the attention weight ai for each hPi based on its compatibility score via softmax operation:
ai = exp(ci)∑|P | j=1 exp(cj)
(4)
",2.3 Aspect Attention Layer,[0],[0]
"Finally, we can obtain the domain aspect encoded representation based on the attention weights:
AP = |P |∑ i=1",2.3 Aspect Attention Layer,[0],[0]
"aih P i (5)
where AP ∈ R2h is the domain aspect encoding for sentence P and also the output of this layer.",2.3 Aspect Attention Layer,[0],[0]
"We predict the stance label of the sentence based on its domain aspect encoding:
ŷ = softmax(MLP(AP ))",2.4 Prediction Layer,[0],[0]
"(6)
where we use a multilayer perceptron (MLP) to consume the domain aspect encoding AP and apply the softmax to get the predicted probability for each of the C classes, ŷ = {y1, ..., yC}.",2.4 Prediction Layer,[0],[0]
"For model training, we use multi-class crossentropy loss,
J (θ) =",2.5 Model Training,[0],[0]
− N∑ i C∑ j y,2.5 Model Training,[0],[0]
(i) j log ŷ,2.5 Model Training,[0],[0]
(i) j,2.5 Model Training,[0],[0]
"+ λ‖Θ‖ (7)
",2.5 Model Training,[0],[0]
whereN is the size of training set.,2.5 Model Training,[0],[0]
"y is the groundtruth label indicator for each class, and ŷ is the predicted probability.",2.5 Model Training,[0],[0]
λ is the coefficient for L2regularization.,2.5 Model Training,[0],[0]
Θ denotes the set of all trainable parameters in our model.,2.5 Model Training,[0],[0]
This section reports the results of quantitative and qualitative evaluations of the proposed model.,3 Experiments,[0],[0]
SemEval-2016:,3.1 Datasets,[0],[0]
the first dataset is from SemEval2016,3.1 Datasets,[0],[0]
"Task 6 on Twitter stance detection, which contains stance-bearing tweets on different targets.",3.1 Datasets,[0],[0]
"We use the following five targets for our experiments: Climate Change is Concern (CC), Feminist Movement (FM), Hillary Clinton (HC), Legalization of Abortion (LA), and Donald Trump (DT).",3.1 Datasets,[0],[0]
"The class labels are favor, against, and neither, and their distributions are shown in Table 2.",3.1 Datasets,[0],[0]
Tweets on an Australian mining project (AM): the second is our collection of tweets on a mining project in Australia obtained using Twitter API.,3.1 Datasets,[0],[0]
"It includes 220,067 tweets posted from January 2016 to June 2017 that contain the project name in the text.",3.1 Datasets,[0],[0]
"We remove all URL-only tweets and duplicate tweets, and obtain a set of 40,852 (unlabeled) tweets.",3.1 Datasets,[0],[0]
"Due to the lack of annotation, this dataset is only used for our qualitative evaluation.
",3.1 Datasets,[0],[0]
"To align with our scenario, the above targets can be categorized into three different domains: Women’s Rights (FM, LA), American Politics (HC, DT), and Environments (CC, AM).",3.1 Datasets,[0],[0]
We use F1-score to measure the classification performance.,3.2 Metric,[0],[0]
"Due to the imbalanced class distributions of the SemEval dataset, we compute both micro-averaged (large classes dominate) and macro-averaged (small classes dominate) F1scores (Manning et al., 2008), and use their average as the metric, i.e., F = 12(Fmicro + Fmacro).
",3.2 Metric,[0],[0]
"To evaluate the effectiveness of target adaptation, we use the metric transfer ratio (Glorot et al., 2011) to compare the cross-target and in-target performance of a model: Q = F (S,D)Fb(D,D) , where F (S,D) is the cross-target F1-score of a model trained on the source target S and tested on the destination target D, and Fb(D,D) is the in-target F1-score of a baseline model trained and tested on the same target D, which serves as the performance calibration for target adaptation.",3.2 Metric,[0],[0]
"The word embeddings are initialized with the pretrained 200d GloVe word vectors on the 27B Twitter corpus (Pennington et al., 2014), and fixed during training.",3.3 Training setup,[0],[0]
"The model is trained (90%) and validated (10%) on a source target, and tested on a destination target.",3.3 Training setup,[0],[0]
"The following model settings are selected based on a small grid search on the validation set: the LSTM hidden size of 60, the MLP layer size of 60, and dropout 0.1.",3.3 Training setup,[0],[0]
The L2-regularization coefficient λ in the loss is 0.01.,3.3 Training setup,[0],[0]
"ADAM (Kingma and Ba, 2014) is used as the optimizer, with a learning rate of 10−3.",3.3 Training setup,[0],[0]
Stratified 10-fold cross-validation is conducted to produce averaged results.,3.3 Training setup,[0],[0]
This section reports the results of our model and two baseline approaches on cross-target stance classification.,3.4 Classification Performance,[0],[0]
BiLSTM:,3.4 Classification Performance,[0],[0]
this is a base model for our task.,3.4 Classification Performance,[0],[0]
It has two BiLSTMs for encoding the sentence and target separately.,3.4 Classification Performance,[0],[0]
"Then, the concatenation of the resulting encodings is fed into the final Prediction Layer to generate predicted stance labels.",3.4 Classification Performance,[0],[0]
"In our evaluation, this model is treated as the baseline model for deriving the in-target performance calibration Fb(D,D).",3.4 Classification Performance,[0],[0]
"MITRE (Augenstein et al., 2016):",3.4 Classification Performance,[0],[0]
"this is the
best system in SemEval-2016 Task 6.",3.4 Classification Performance,[0],[0]
It utilizes the conditional encoding to learn a targetdependent representation for the input sentence.,3.4 Classification Performance,[0],[0]
"The conditional encoding is realized in the same way as the Context Encoding Layer does in our model, namely by using the hidden states of the target-encoding BiLSTM to initialize the sentence-encoding BiLSTM.
Table 3 shows the results (in-target and crosstarget) on the two domains: Women’s Rights and American Politics.",3.4 Classification Performance,[0],[0]
"First, it is observed that MITRE outperforms BiLSTM over all target configurations, suggesting that, compared to simple concatenation, the conditional encoding of the target information could be more helpful to capture the dependency of the sentence on the target.
",3.4 Classification Performance,[0],[0]
"Second, our model is shown to achieve better results than the two baselines in almost all cases (only slightly worse than MITRE on LA under the in-target setting, and the difference is not statistically significant), which implies that the aspect attention mechanism adopted in our model could benefit target-level generalization while it does not hurt the in-target performance.",3.4 Classification Performance,[0],[0]
"Moreover, by comparing the performance of our model under different target configurations, we see that the improvements brought by our model are more significant on the cross-target task than they are on the intarget task, with an average improvement of 6.6% (cross-target) vs. 3.0% (in-target) over MITRE in F1-score, which demonstrates a greater advantage of our model in the cross-target task.
",3.4 Classification Performance,[0],[0]
"Finally, according to the transfer ratio results, the general drop from the in-target to cross-target performance (26% averaged over all cases) could imply that while the target-independent information (i.e., the domain-specific aspects) is shown to benefit generalization, it could be important to also consider the information that is specific to the destination target for model building (which has not yet been explored in this work).",3.4 Classification Performance,[0],[0]
"To show that our model can select sentence parts that are related to domain aspects, we visualize the self-attention results on some tweet examples that are correctly classified by our model in Table 4.
",3.5 Visualization of Attention,[0],[0]
We can see that the most highlighted parts in each example are relevant to the respective domain.,3.5 Visualization of Attention,[0],[0]
"For example, “feminist”, “rights”, and “equality” are commonly used when talking about women’s rights, and “president” and “dreams” of-
ten appear in text about politics.",3.5 Visualization of Attention,[0],[0]
"It is also interesting to note that words that are specific to the destination target may not be captured by the model learned from the source target, such as “abortion” in sentence 1 and “trumps” in sentence 3.",3.5 Visualization of Attention,[0],[0]
"This makes sense because those words are rare in the source target corpus and thus not well noticed by the model.
",3.5 Visualization of Attention,[0],[0]
"Finally, for our project, we can see from the last two sentences that the model learned from climate change is concern is able to concentrate on words that are central to understanding the authors’ stances on the approval of the mining project, such as “reef”, “destroy”, “environmental”, and “disaster”.",3.5 Visualization of Attention,[0],[0]
"Overall, the above visualization demonstrates that our model could benefit stance inference across related targets through capturing domain-specific information.",3.5 Visualization of Attention,[0],[0]
"Finally, it is also possible to show the learned domain aspects by extracting all sentence parts in a corpus that are highly attended by our model.",3.6 Learned Domain-Specific Aspects,[0],[0]
Table 5 presents a number of samples from the intersections between the sets of highly-attended words on the respective targets in the three domains.,3.6 Learned Domain-Specific Aspects,[0],[0]
"Again, we see that these highly-attended words are specific to the respective domains.",3.6 Learned Domain-Specific Aspects,[0],[0]
"We
also notice that besides the domain-aspect words, our model can find words that carry sentiments as well, such as “great”, “crazy”, and “beautiful”, which contribute to stance prediction.",3.6 Learned Domain-Specific Aspects,[0],[0]
"In this work, we study cross-target stance classification and propose a novel self-attention neural model that can extract target-independent information for model generalization.",4 Conclusion and Future Work,[0],[0]
Experimental results show that the proposed model can perceive high-level domain-specific information in a sentence and achieves superior results over a number of baselines in certain domains.,4 Conclusion and Future Work,[0],[0]
"In the future, there are several ways of extending our model.
",4 Conclusion and Future Work,[0],[0]
"First, selecting the effective source targets to generalize from is crucial for achieving satisfying results on the destination targets.",4 Conclusion and Future Work,[0],[0]
"One possibility could be to learn certain correlations between target closeness and generalization performance, which could further be used for guiding the target selection process.",4 Conclusion and Future Work,[0],[0]
"Second, our current model for identifying users’ stances on mining projects only generalizes from one source target (i.e., Climate Change is Concern).",4 Conclusion and Future Work,[0],[0]
"However, a mining project in general could affect other aspects of our society such as community and economics.",4 Conclusion and Future Work,[0],[0]
It could be useful to also consider other related sources for knowledge transfer.,4 Conclusion and Future Work,[0],[0]
"Finally, it would be interesting to evaluate our model in a multilingual scenario (Taulé et al., 2017), in order to examine its generalization ability (whether it can attend to useful domain-specific information in a new language) and multilingual scope.",4 Conclusion and Future Work,[0],[0]
We thank all anonymous reviewers for their valuable comments.,Acknowledgments,[0],[0]
We would also like to thank Keith Vander Linden for his helpful comments on drafts of this paper.,Acknowledgments,[0],[0]
"In stance classification, the target on which the stance is made defines the boundary of the task, and a classifier is usually trained for prediction on the same target.",abstractText,[0],[0]
"In this work, we explore the potential for generalizing classifiers between different targets, and propose a neural model that can apply what has been learned from a source target to a destination target.",abstractText,[0],[0]
We show that our model can find useful information shared between relevant targets which improves generalization in certain scenarios.,abstractText,[0],[0]
Cross-Target Stance Classification with Self-Attention Networks,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3664–3674 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3664",text,[0],[0]
Information retrieval and question answering are by now mature technologies that excel at answering factual queries on noncontroversial topics.,1 Introduction,[0],[0]
"However, they provide no specialized support for queries where there is no single canonical answer, as with topics that are controversial or opinion-based.",1 Introduction,[0],[0]
"For such queries, the user may need to carefully assess the stance, source, and supportability for each of the answers.",1 Introduction,[0],[0]
"These processes can be supported by argument mining (AM), a nascent area of natural language processing concerned with the automatic recognition and interpretation of arguments.",1 Introduction,[0],[0]
"In this paper, we apply AM to the task of argument search—that is, searching a large document collection for arguments relevant to a given topic.",1 Introduction,[0],[0]
"Searching for and classifying relevant arguments plays an important role in decision making (Svenson, 1979), legal reasoning (Wyner et al., 2010), and
the critical reading, writing, and summarization of persuasive texts (Kobayashi, 2009; Wingate, 2012).",1 Introduction,[0],[0]
"Automating the argument search process could ease much of the manual effort involved in these tasks, particularly if it can be made to robustly handle arguments from different text types and topics.",1 Introduction,[0],[0]
"But despite its obvious usefulness, this sort of argument search has attracted little attention in the research community.",1 Introduction,[0],[0]
"This may be due in part to the limitations of the underlying models and training resources, particularly as they relate to heterogeneous sources.",1 Introduction,[0],[0]
"That is, most current approaches to AM are designed for use with particular text types, faring poorly when applied to new data (Daxenberger et al., 2017).",1 Introduction,[0],[0]
"Indeed, as Habernal et al. (2014) observe, while there is a great diversity of perspectives on how arguments can be best characterized and modelled, there is no “one-size-fits-all” argumentation theory that applies to the variety of text sources found on the Web.",1 Introduction,[0],[0]
"To approach these challenges, we propose the novel task of topic-based sentential argument mining.",1 Introduction,[0],[0]
Our contributions are as follows: (1) We propose a new argument annotation scheme applicable to the information-seeking perspective of argument search.,1 Introduction,[0],[0]
"We show it to be general enough for use on heterogeneous data sources, and simple enough to be applied manually by untrained annotators at a reasonable cost.",1 Introduction,[0],[0]
"(2) We introduce a novel corpus of heterogeneous text types annotated with topic-based arguments.1 The corpus includes over 25,000 instances covering eight controversial topics.",1 Introduction,[0],[0]
This is the first known resource that can be used to evaluate the performance of argument mining methods across topics in heterogeneous sources.,1 Introduction,[0],[0]
"(3) We investigate different approaches for incorporating topic information into neural networks and
1https://www.ukp.tu-darmstadt.de/sent_am
show that including the topic vector into the i- and c-gates of the LSTM cell outperforms common attention-based approaches in two- and three-label cross-topic experiments.",1 Introduction,[0],[0]
(4) We further improve the performance of the modified LSTM cell by leveraging additional data for topic relevance in a multi-task learning setup.,1 Introduction,[0],[0]
"(5) In the more challenging setup of cross-topic experiments, we show that our models yield considerably better performance than common BiLSTM models when little data of the target topic is available.",1 Introduction,[0],[0]
"Most existing approaches treat argument mining at the discourse level, focusing on tasks such as segmenting argumentative discourse units (Ajjour et al., 2017; Goudas et al., 2014), classifying the function of argumentative discourse units (for example, as claims or premises) (Mochales-Palau and Moens, 2009; Stab and Gurevych, 2014), and recognizing argumentative discourse relations (Eger et al., 2017; Stab and Gurevych, 2017; Nguyen and Litman, 2016).",2 Related work,[0],[0]
"These discourse-level approaches address the identification of argumentative structures within a single document but do not consider relevance to externally defined topics.
",2 Related work,[0],[0]
"To date, there has been little research on the identification of topic-relevant arguments for argument search.",2 Related work,[0],[0]
Wachsmuth et al. (2017) present a generic argument search framework.,2 Related work,[0],[0]
"However, it relies on already-structured arguments from debate portals and is not yet able to retrieve arguments from arbitrary texts.",2 Related work,[0],[0]
"Levy et al. (2014) investigate the identification of topic-relevant claims, an approach that was later extended with evidence extraction to mine supporting statements for claims (Rinott et al., 2015).",2 Related work,[0],[0]
"However, both approaches are designed to mine arguments from Wikipedia articles; it is unclear whether their annotation scheme is applicable to other text types.",2 Related work,[0],[0]
"It is also uncertain that it can be easily and accurately applied by untrained annotators, since it requires unitizing (i.e., finding the boundaries of argument components at the token level).",2 Related work,[0],[0]
Hua and Wang (2017) identify sentences in cited documents that have been used by an editor to formulate an argument.,2 Related work,[0],[0]
"By contrast, we do not limit our approach to the identification of sentences related to a given argument, but rather focus on the retrieval of any argument relevant to a given topic.",2 Related work,[0],[0]
"The fact that we are concerned with retrieval of arguments also sets our work apart from
the discourse-agnostic stance detection task of Mohammad et al. (2016), which is concerned with the identification of sentences expressing support or opposition to a given topic, irrespective of whether those sentences contain supporting evidence (as opposed to mere statements of opinion).
",2 Related work,[0],[0]
"Cross-domain AM experiments have so far been conducted only for discourse-level tasks such as claim identification (Daxenberger et al., 2017), argumentative segment identification (Al-Khatib et al., 2016), and argumentative unit segmentation (Ajjour et al., 2017).",2 Related work,[0],[0]
"However, the discourse-level argumentation models these studies employ seem to be highly dependent on the text types for which they were designed; they do not work well when applied to other text types (Daxenberger et al., 2017).",2 Related work,[0],[0]
The crucial difference between our own work and prior cross-domain experiments is that we investigate AM from heterogeneous texts across different topics instead of studying specific discourse-level AM tasks across restricted text types of existing corpora.,2 Related work,[0],[0]
"There exists a great diversity in models of argumentation, which differ in their perspective, complexity, terminology, and intended applications (Bentahar et al., 2010).",3 Corpus creation,[0],[0]
"For the present study, we propose a model which, though simplistic, is nonetheless well-suited to the argument search scenario.",3 Corpus creation,[0],[0]
We define an argument as a span of text expressing evidence or reasoning that can be used to either support or oppose a given topic.,3 Corpus creation,[0],[0]
"An argument need not be “direct” or self-contained—it may presuppose some common or domain knowledge, or the application of commonsense reasoning—but it must be unambiguous in its orientation to the topic.",3 Corpus creation,[0],[0]
"A topic, in turn, is some matter of controversy for which there is an obvious polarity to the possible outcomes—that is, a question of being either for or against the use or adoption of something, the commitment to some course of action, etc.",3 Corpus creation,[0],[0]
"In some graph-based models of argumentation (Stab, 2017, Ch. 2), what we refer to as a topic would be part of a (major) claim expressing a positive or negative stance, and our arguments would be premises with supporting/attacking consequence relations to the claim.",3 Corpus creation,[0],[0]
"However, unlike these models, which are typically used to represent (potentially deep or complex) argument structures at the discourse level, ours is a flat model that considers arguments in isolation from their surrounding context.",3 Corpus creation,[0],[0]
"A great
advantage of this approach is that it allows annotators to classify text spans without having to read large amounts of context and without having to consider relations to other topics or arguments.",3 Corpus creation,[0],[0]
"In this work, we consider only those topics that can be concisely and implicitly expressed through keywords, and those arguments that consist of individual sentences.",3 Corpus creation,[0],[0]
"Some examples, drawn from our dataset, are shown in Table 1.",3 Corpus creation,[0],[0]
"Note that while the fourth example expresses opposition to the topic, under our definition it is properly classified as a non-argument because it is a mere statement of stance that provides no evidence or reasoning.
",3 Corpus creation,[0],[0]
Data.,3 Corpus creation,[0],[0]
For our experiments we gathered a large collection of manually annotated arguments that cover a variety of topics and that come from a variety of text types.,3 Corpus creation,[0],[0]
"We started by randomly selecting eight topics (see Table 2) from online lists of controversial topics.2 For each topic, we made a Google query for the topic name, removed results not archived by the Wayback Machine,3 and truncated the list to the top 50 results.",3 Corpus creation,[0],[0]
"This resulted in a set of persistent, topic-relevant, largely polemical Web documents representing a range of genres and text types, including news reports, editorials, blogs, debate forums, and encyclopedia articles.",3 Corpus creation,[0],[0]
"We preprocessed each document with Apache Tika (Mattmann and Zitting, 2011) to remove boilerplate text.",3 Corpus creation,[0],[0]
"We then used the Stanford CoreNLP tools (Manning et al., 2014) to perform tokenization, sentence segmentation, and part-ofspeech tagging on the remaining text, and removed all sentences without verbs or with less than three tokens.",3 Corpus creation,[0],[0]
"This left us with a raw dataset of 27,520 sentences (about 2,700 to 4,400 per topic).",3 Corpus creation,[0],[0]
"Annotators classified the sentences using a browser-based interface that presents a set of in2https://www.questia.com/library/ controversial-topics, https://www.procon.org/ 3https://web.archive.org/
structions, a topic, a list of sentences, and amultiplechoice form for specifying whether each sentence is a supporting argument, an opposing argument, or not an argument with respect to the topic.",3 Corpus creation,[0],[0]
"(In preliminary experiments, we presented annotators with a fourth option for sentences that are ambiguous or incomprehensible.",3 Corpus creation,[0],[0]
"However, we found that these constituted less than 1% of the distribution and so mapped all such answers to the “no argument” class.)
",3 Corpus creation,[0],[0]
Annotation experiments.,3 Corpus creation,[0],[0]
"We tested the applicability of our annotation scheme by untrained annotators by performing an experiment where we had a group of “expert” annotators and a group of untrained annotators classify the same set of sentences, and then compared the two groups’ classifications.",3 Corpus creation,[0],[0]
The data for this experiment consisted of 200 sentences randomly selected from each of our eight topics.,3 Corpus creation,[0],[0]
Our expert annotators were two graduatelevel language technology researchers who were fully briefed on the nature and purpose of the argument model.,3 Corpus creation,[0],[0]
Our untrained annotators were anonymous American workers from the Amazon Mechanical Turk (AMT) crowdsourcing platform.,3 Corpus creation,[0],[0]
"Each sentence was independently annotated by the two expert annotators and ten crowd workers.
",3 Corpus creation,[0],[0]
"Inter-annotator agreement for our two experts, as measured by Cohen’s κ, was 0.721; this exceeds the commonly used threshold of 0.7 for assuming the results are reliable (Carletta, 1996).",3 Corpus creation,[0],[0]
"We proceeded by having the two experts resolve their disagreements, resulting in a set of “expert” gold-standard annotations.",3 Corpus creation,[0],[0]
"Similar gold standards were produced for the crowd annotations by applying the MACE denoising tool (Hovy et al., 2013); we tested various thresholds (1.0, 0.9, and 0.8) to discard instances that could be confidently assigned a gold label.",3 Corpus creation,[0],[0]
We then calculated κ between the remaining instances in the expert and crowd gold standards.,3 Corpus creation,[0],[0]
"In order to
determine the relationship between inter-annotator agreement and the number of crowd workers, we performed this procedure with successively lower numbers of crowd workers, going from the original ten annotators per instance down to two.",3 Corpus creation,[0],[0]
The results are visualized in Fig. 1.,3 Corpus creation,[0],[0]
We found that using seven annotators and a MACE threshold of 0.9 results in κ = 0.723; this gives us similar reliability as with the expert annotators without sacrificing much coverage.,3 Corpus creation,[0],[0]
"Table 3 shows the κ and percentage agreement for this setup, as well as the agreement between our expert annotators, broken down by topic.",3 Corpus creation,[0],[0]
"We proceeded with annotating the remaining instances in our dataset using seven crowd workers each, paying a rate corresponding to the US federal minimum wage of $7.25/hour.",3 Corpus creation,[0],[0]
"Our total expenditure, including AMT processing fees, was $2,774.02.",3 Corpus creation,[0],[0]
"After MACE denoising, we were left with 25,492 gold-standard annotations.",3 Corpus creation,[0],[0]
Table 2 provides statistics on the size and class distribution of the final corpus.,3 Corpus creation,[0],[0]
"We are releasing the gold-standard annotations for this dataset, and code for retrieving
the original sentences from the Wayback Machine, under a Creative Commons licence.",3 Corpus creation,[0],[0]
We model the identification of arguments as a sentence-level classification task.,4 Approaches for identifying arguments,[0],[0]
"In particular, given a sentence ς with words u1, . . .",4 Approaches for identifying arguments,[0],[0]
",unς and a topic τ of words v1, . . .",4 Approaches for identifying arguments,[0],[0]
", vnτ (e.g., “gun control” or “school uniforms”), we aim to classify ς",4 Approaches for identifying arguments,[0],[0]
"as a “supporting argument” or “opposing argument” if it includes a relevant reason for supporting or opposing the τ , or as a “non-argument” if it does not include a reason or is not relevant to τ .",4 Approaches for identifying arguments,[0],[0]
We also investigate a two-label classification where we combine supporting and opposing arguments into a single category; this allows us to evaluate argument classification independent of stance.,4 Approaches for identifying arguments,[0],[0]
"We focus on the challenging task of cross-topic experiments, where one topic is withheld from the training data and used for testing.",4 Approaches for identifying arguments,[0],[0]
"Here, we denote scalars by italic lowercase letters (e.g., t), vector representations by italic bold lowercase letters (e.g., c), and matrices as italic bold uppercase letters (e.g.,W ).",4 Approaches for identifying arguments,[0],[0]
"Since arguments need to be relevant to the given topic, we posit that providing topic information to the learner results in a more robust prediction capability in cross-topic setups.",4.1 Integrating topic information,[0],[0]
"Below, we present two models that integrate the topic, one that uses an attention mechanism and another that includes the topic vector directly in the LSTM cell.
",4.1 Integrating topic information,[0],[0]
Outer-attention BiLSTM (outer-att).,4.1 Integrating topic information,[0],[0]
"To let the model learn which parts of the sentence are relevant (or irrelevant) to the given topic, we use an attentionbased neural network (Bahdanau et al., 2014) that learns an importance weighting of the input words depending on the given topic.",4.1 Integrating topic information,[0],[0]
"In particular, we adopt an outer-attention mechanism similar to the one proposed by Hermann et al. (2015), which has achieved state-of-the-art results in related tasks such as natural language inference and recognizing textual entailment (Rocktäschel et al., 2015; Wang and Jiang, 2016).",4.1 Integrating topic information,[0],[0]
"We combine the attention mechanism with a common BiLSTM model and, at time step t, determine the importance weighting for each hidden state h(t) as
m(t) = tanh(W hh(t)",4.1 Integrating topic information,[0],[0]
"+W pp) (1)
fattention(h(t),p) = exp(wTmm(t))∑ t exp(wTmm(t))
",4.1 Integrating topic information,[0],[0]
"(2)
whereW h,W p, andwm are trainable parameters of the attention mechanism and p is the average of all word embeddings of topic words v1, . .",4.1 Integrating topic information,[0],[0]
.,4.1 Integrating topic information,[0],[0]
", vnτ .",4.1 Integrating topic information,[0],[0]
"Using the importance weighting, we determine the final, weighted hidden output state s as
αt ∝ fattention(h(t),p) (3) s = n∑ t=1",4.1 Integrating topic information,[0],[0]
h(t)αt .,4.1 Integrating topic information,[0],[0]
"(4)
Finally, we feed s into a dense layer with a softmax activation function to get predictions for our twoor three-label setups.
",4.1 Integrating topic information,[0],[0]
Contextual BiLSTM (biclstm).,4.1 Integrating topic information,[0],[0]
"A more direct approach to integrating an argument’s topic is the contextual LSTM (CLSTM) architecture (Ghosh et al., 2016), where topic information is added as another term to all four gates of an LSTM cell.",4.1 Integrating topic information,[0],[0]
"We, however, hypothesize that topic information is more relevant at the i- and c-gates, the former because it has the biggest impact on how a new token is processed and the latter because it is closely linked
to how the sequence seen so far is to be interpreted and stored.",4.1 Integrating topic information,[0],[0]
"To this end, we experimented with severalmodifications to the original CLSTMsuch as removing peepholes—i.e., removing gates’ access to the cell state c",4.1 Integrating topic information,[0],[0]
"(Gers and Schmidhuber, 2000)— and removing topic information from one or more gates.",4.1 Integrating topic information,[0],[0]
"Empirical results on the validation set show that topic integration at the i- and c-gates only, and removal of all peephole connections, does indeed outperform the original CLSTM on our task by 1 percentage point.",4.1 Integrating topic information,[0],[0]
"Our modified CLSTM (Fig. 2) is defined as
i t = σ(W xi x t",4.1 Integrating topic information,[0],[0]
+,4.1 Integrating topic information,[0],[0]
"W hiht−1 + bi + Wpi p ) (5)
f t = σ(W x f x t",4.1 Integrating topic information,[0],[0]
+W h f ht−1 + b f ),4.1 Integrating topic information,[0],[0]
"(6) ct = f tct−1 + i tσc(W xc x t +W hcht−1
+bc + Wpc p ) (7)
ot = σ(W xox t",4.1 Integrating topic information,[0],[0]
+,4.1 Integrating topic information,[0],[0]
W hoht−1 + bo) (8) ht = otσc(ct ).,4.1 Integrating topic information,[0],[0]
"(9)
Here i , f , and o represent the input, forget, and output gates; c the cell memory; x t the embedded token of a sentence at timestep t; ht−1 the previous hidden state; and b the bias.",4.1 Integrating topic information,[0],[0]
σ,4.1 Integrating topic information,[0],[0]
"and σc are the activation and recurrent activation functions, respectively.",4.1 Integrating topic information,[0],[0]
The novel terms for topic integration are outlined.,4.1 Integrating topic information,[0],[0]
"We use this model bidirectionally, as we did with our BiLSTM network, and hence refer to it as biclstm.",4.1 Integrating topic information,[0],[0]
"As we want to classify arguments related to specific topics, leveraging information that supports the classifier in the decision of topic-relation is crucial.",4.2 Leveraging additional data,[0],[0]
The multi-task learning (mtl) and transfer learning (trl) models are able to make use of auxiliary data that can potentially improve the results on the main task.,4.2 Leveraging additional data,[0],[0]
"Thus, we extend our previously described models by integrating them into mtl and trl setups.",4.2 Leveraging additional data,[0],[0]
"We also choose to integrate two corpora
from which we expect to learn (a) topic-relevance and (b) the capability to distinguish between supporting and attacking arguments.",4.2 Leveraging additional data,[0],[0]
"The first corpus, DIP2016 (Habernal et al., 2016), consists of 49 queries from the educational domain and 100 documents for each query.",4.2 Leveraging additional data,[0],[0]
Each document has its sentences annotated for relevance (true/false) to the query.4,4.2 Leveraging additional data,[0],[0]
"The second corpus, from SemEval-2016 Task 6 (Mohammad et al., 2016), consists of around 5000 multi-sentence tweets, a corresponding topic (e.g., “atheism”), and the author’s stance on the topic (for/against/neither).
",4.2 Leveraging additional data,[0],[0]
"For our mtl and trl approaches, we consider every possible pairing of a model (biclstm, outer-att, and the bilstm baseline we introduce in §5) with an auxiliary corpus (DIP2016, SemEval).",4.2 Leveraging additional data,[0],[0]
"We formalize our datasets as Sk = {(xki ,pki , yki )|i = 0, . . .",4.2 Leveraging additional data,[0],[0]
", |Sk |}, where k can be either our main dataset or an auxiliary dataset, xki denotes a single sentence as a sequence of word embeddings and yki its corresponding label in k, and pki represents the corresponding averaged topic vector.
",4.2 Leveraging additional data,[0],[0]
Transfer learning (trl).,4.2 Leveraging additional data,[0],[0]
"For trl, we use the approach of parameter transfer (Pan andYang, 2010)— i.e., we do not modify the model used.",4.2 Leveraging additional data,[0],[0]
"Instead, we train the model twice: the first time, we train the model on the chosen auxiliary corpus, and the second time, we keep the trained model’s weights and train it with our own corpus.",4.2 Leveraging additional data,[0],[0]
"For the threelabel setting, we have to modify the transfer model slightly for the DIP2016 corpus, since it provides only two labels for each training sample.",4.2 Leveraging additional data,[0],[0]
"In this case, we simply add a layer with two neurons on top of the layer with three neurons for training with the DIP2016 corpus and remove it afterwards for training with our corpus.
4We only use 300K of the corpus’s 600K samples to ease hyperparameter tuning for our computation-heavy models.
",4.2 Leveraging additional data,[0],[0]
Multi-task learning (mtl).,4.2 Leveraging additional data,[0],[0]
"For mtl, we use a shared–private model (Liu et al., 2017), which showed promising results for text classification and word segmentation (Chen et al., 2017).",4.2 Leveraging additional data,[0],[0]
"(We also experimented with their adversarial approach to learn topic-invariant features, but abandoned this due to low scores.)",4.2 Leveraging additional data,[0],[0]
"The mtl base model consists of a private recurrent neural network (RNN) for both the auxiliary dataset and our dataset, plus a shared RNN that both datasets use (Fig. 3).",4.2 Leveraging additional data,[0],[0]
The last hidden states of the RNNs are concatenated and fed through a dense layer and a softmax activation function.,4.2 Leveraging additional data,[0],[0]
"The model is trained in an alternating fashion—i.e., after each epoch the loss for the other dataset is minimized until each dataset has run for the set number of epochs, where the last epoch is always executed on our dataset.",4.2 Leveraging additional data,[0],[0]
"At prediction time, only the private RNN trained on our dataset and the shared RNN are used.",4.2 Leveraging additional data,[0],[0]
"The core idea is that the shared RNN learns what is relevant for both tasks, while the private ones learn only the task-specific knowledge.
",4.2 Leveraging additional data,[0],[0]
"For the cases of mtl+bilstm+corpus, mtl+biclstm+ corpus, and mtl+outer-att+corpus, we simply switch the RNN with our bilstm, biclstm, and outer-att, respectively.",4.2 Leveraging additional data,[0],[0]
"For mtl+outer-att+corpus, we add the outer attention mechanism (see §4.1), modified for use with the mtl model, after each of the private RNNs, while additionally feeding it a second topic vector—the last hidden state of the shared RNN:
m(t) =",4.2 Leveraging additional data,[0],[0]
tanh(W rhr (t) +,4.2 Leveraging additional data,[0],[0]
W shs,4.2 Leveraging additional data,[0],[0]
"+W pp) (10) fattention(hr (t),hs,p) = exp(wTmm(t))∑ t exp(wTmm(t))",4.2 Leveraging additional data,[0],[0]
"(11)
",4.2 Leveraging additional data,[0],[0]
αt ∝,4.2 Leveraging additional data,[0],[0]
"fattention(hr (t),hs,p) (12)
s = n∑ t=1 hr (t)αt (13)
",4.2 Leveraging additional data,[0],[0]
"whereW r ,W s, andW p are trainable weight matrices, hr (t) is the hidden state of the private bilstm at timestep t, hs is the last hidden state of the shared model, and p is the average of all word embeddings of topic words v1, . . .",4.2 Leveraging additional data,[0],[0]
", vnτ .",4.2 Leveraging additional data,[0],[0]
"To evaluate the robustness of the models, we conduct cross-topic experiments to evaluate how well the models generalize to an unknown topic.",5 Evaluation,[0],[0]
"To this end, we combine training (70%) and validation
data (10%) of seven topics for training and parameter tuning, and use the test data (20%) of the eighth topic for testing.",5 Evaluation,[0],[0]
"For encoding the words of sentence ς and topic τ , we use 300-dimensional word embeddings trained on the Google News dataset by Mikolov et al. (2013).",5 Evaluation,[0],[0]
"To handle out-of-vocabulary words, we create separate random word vectors for each.5 Since reporting single performance scores is insufficient to compare non-deterministic learning approaches like neural networks (Reimers and Gurevych, 2017), we report all results as averages over ten runs with different random seeds.",5 Evaluation,[0],[0]
"As evaluation measures, we report the average macro F1, as well as the precision and the recall for the argument class (Parg, Rarg).",5 Evaluation,[0],[0]
"For the three-label approach, we split the precision and recall for predicting supporting (Parg+, Rarg+) and attacking arguments (Parg−, Rarg−).",5 Evaluation,[0],[0]
"As baselines, we use a simple bidirectional LSTM (Hochreiter and Schmidhuber, 1997), as well as a logistic regression model with lowercased unigram features, which has been shown to be a strong baseline for various other AM tasks (Daxenberger et al., 2017; Stab and Gurevych, 2017).",5 Evaluation,[0],[0]
"We refer to these models as bilstm and lr-uni, respectively.",5 Evaluation,[0],[0]
"All neural networks are trained using the Adam optimizer (Kingma and Ba, 2015) and cross-entropy loss function.",5 Evaluation,[0],[0]
"For finding the best model, we run each for ten epochs and take the best model based on the lowest validation loss.",5 Evaluation,[0],[0]
"In addition to that, we tune the hyperparameters of all
5Each dimension is set to a random number between −0.01 and 0.01.",5 Evaluation,[0],[0]
"Digits are mapped to the same random word vector.
neural networks (see Appendix A).",5 Evaluation,[0],[0]
"To accelerate training, we truncate sentences at 60 words.6",5 Evaluation,[0],[0]
Two-label setup.,5.1 Results,[0],[0]
The results in Table 4 show that all our models outperform the baselines for two-label prediction.7 F1 for biclstm improves by 3.5 percentage points over the bilstm baseline and by 5.6 over lr-uni.,5.1 Results,[0],[0]
A main reason for this proves to be the substantial increase in recall for our topic-integrating models—outer-att and especially biclstm—in comparison to our baselines.,5.1 Results,[0],[0]
These results show that knowledge of the argument’s topic has a strong impact on argument prediction capability.,5.1 Results,[0],[0]
"Further, we observe that integrating biclstm in a multi-task learning setup in order to draw knowledge about topic relevance from the DIP2016 corpus (mtl+biclstm+dip2016) improves F1 by an additional 2.5 percentage points.",5.1 Results,[0],[0]
"It achieves an F1 of 0.6662, which is 19.48 percentage points less than the human upper bound of 0.861.",5.1 Results,[0],[0]
"When using the SemEval corpus, which holds less task-relevant knowledge for our two-label approach, we are able to gain only 1 percentage point when integrating it into mtl+biclstm+corpus.
",5.1 Results,[0],[0]
"For the transfer learning models that integrate the topic (tr+biclstm+corpus and tr+outer-att+corpus), the parameter transfer is mostly ineffective.",5.1 Results,[0],[0]
"If no topic is provided (tr+bilstm+corpus), the transfer learning models are able to improve over the baseline bilstm.",5.1 Results,[0],[0]
"This shows that the parameter transfer
6Only 244 of our sentences (<1%) exceed this length.",5.1 Results,[0],[0]
"7Detailed results per topic are given in Appendix B.
itself can be of use, but confuses the model when combined with topic integration.
",5.1 Results,[0],[0]
"In general, we observe an overall lower score for trl models that use the DIP2016 corpus compared to those using the SemEval corpus.",5.1 Results,[0],[0]
"In contrast to the mtl model, for trl models all parameters are transferred to the main task, not just parameters that represent shared knowledge.",5.1 Results,[0],[0]
"Thus, we suspect the lower scores of the trl models with DIP2016 are due to overfitting on the vast number of samples which shape the parameters much more than the comparatively small SemEval corpus could.
",5.1 Results,[0],[0]
Three-label setup.,5.1 Results,[0],[0]
"For the three-label approach, we observe overall lower scores due to the additional difficulty in distinguishing supporting from opposing arguments.",5.1 Results,[0],[0]
"As already observed in the two-label setup, biclstm outperforms both the bilstm and lr-uni baselines; here, the former by 4.5 and the latter by 4.2 percentage points in F1.",5.1 Results,[0],[0]
"Again, this is caused by a substantial increase in recall and shows the impact that the available topic information has on the classifier’s predictive power.",5.1 Results,[0],[0]
"For transfer learning, we see similar results as for the two-label approach; both the DIP2016 and SemEval corpora have a generally negative impact when compared to the respective base models.",5.1 Results,[0],[0]
The SemEval corpus does not provide the knowledge required to distinguish supporting from attacking arguments.,5.1 Results,[0],[0]
"We conclude that the original purpose of the SemEval task, stance recognition, is too different from our own.",5.1 Results,[0],[0]
"But in multi-task learning, where only the shared parameters are taken, we observe slight improvements when using biclstm with DIP2016; this correlates with the same model in the two-label setup.",5.1 Results,[0],[0]
"To understand the errors of our best model, mtlbiclstm-dip, and the nature of this task, we manually analyzed 100 sentences randomly sampled from the false positive and false negative arguments of the three-label experiments (combining supporting and attacking arguments).",5.2 Error analysis,[0],[0]
"Among the false positives, we found 48 off-topic sentences that were wrongly classified as arguments.",5.2 Error analysis,[0],[0]
The 52 on-topic false positives consist of non-argumentative background information or mere opinions without evidence (as with the first and fourth examples of Table 1) and questions about the topic.,5.2 Error analysis,[0],[0]
"Among the false negatives, we found 65 arguments that did not explicitly refer to the topic but to related aspects that
depend on background knowledge.",5.2 Error analysis,[0],[0]
"For instance, the model fails to establish an argumentative link between the topic “gun control” and the Second Amendment to the US Constitution.",5.2 Error analysis,[0],[0]
"Lastly, we inspected arguments that are incorrectly classified as supporting and/or opposing a topic.",5.2 Error analysis,[0],[0]
We found several samples in which the term “against” is not correctly interpreted and the argument is classified as supporting a topic.,5.2 Error analysis,[0],[0]
"Similarly, for arguments incorrectly classified as attacking, we find various samples where the word “oppose” is used not to oppose the topic but to strengthen a supporting argument, as in “There is reason even for people who oppose the use of marijuana to support its legalization. . . ”",5.2 Error analysis,[0],[0]
"To evaluate the performance of the models in datascarce scenarios, we gradually add target topic data to the training data and analyze the model performance on the target test set.",5.3 Adapting to new topics,[0],[0]
"Figure 4 shows model performance (F1, Parg, and Rarg) on the “marijuana legalization” topicwhen adding different amounts of randomly sampled topic-specific data to the training data (x-axes).8",5.3 Adapting to new topics,[0],[0]
"As the results show, the models that integrate the topic achieve higher recall when adding target topic data to the training data.",5.3 Adapting to new topics,[0],[0]
"For bilstm, we observe a drastic difference when compared to the other models; the recall for arguments stays at around 30% and rises only when integrating more than 60% target topic data.",5.3 Adapting to new topics,[0],[0]
"In strong contrast, topic-integrating models retrieve a much higher number of actual arguments at target topic augmentation levels as low as 20%.",5.3 Adapting to new topics,[0],[0]
"Further, and equally important, this does not come at the cost of precision; on the contrary, the precision is mostly steady and slowly rising after around 20% of target topic integration, leading to an overall higher F1 for these models.",5.3 Adapting to new topics,[0],[0]
"Finally, in comparing F1 between topic-integrating models and bilstm, we conclude that the former need much less target topic data to substantially improve their score, making them more robust in situations of data scarcity.",5.3 Adapting to new topics,[0],[0]
We have presented a new approach for searching a document collection for arguments relevant to a given topic.,6 Conclusion,[0],[0]
"First, we introduced an annotation scheme suited to the information-seeking perspec-
8Each data point in the plot is the average score of ten runs with different random samples of target topic data.
",6 Conclusion,[0],[0]
tive of argument search and showed that it is cheaply but reliably applicable by untrained annotators to arbitrary Web texts.,6 Conclusion,[0],[0]
"Second, we presented a new corpus, including over 25,000 instances over eight topics, that allows for cross-topic experiments using heterogeneous text types.",6 Conclusion,[0],[0]
"Third, we conducted cross-topic experiments and showed that integrating topic information of arguments with our contextual BiLSTM leads to better generalization to unknown topics.",6 Conclusion,[0],[0]
"Fourth, by leveraging knowledge from similar datasets and integrating our contextual BiLSTM into a multi-task learning setup, we were able to gain an improvement over our strongest baseline of 5.9 percentage points in F1 in the two-label setup and 4.6 in the three-label setup.",6 Conclusion,[0],[0]
"Finally, by gradually adding target topic data to our training set, we showed that, when available, even small amounts of target topic data (20%) have a strong positive influence on the recall of arguments.
",6 Conclusion,[0],[0]
"In a separate, simultaneously written paper (Stab et al., 2018) we evaluate our models in real-world application scenarios by applying them to a large document collection and comparing the results to a manually produced gold standard.",6 Conclusion,[0],[0]
"An online argument search engine implementing our approach is now available for noncommercial use at https://www.argumentsearch.com/. Furthermore, we are experimenting with language adaptation and plan to extend the tool to the German language.",6 Conclusion,[0],[0]
Preliminary results are presented in Stahlhut (2018).,6 Conclusion,[0],[0]
We also intend to investigate methods for grouping similar arguments.,6 Conclusion,[0],[0]
This work has been supported by the German Federal Ministry of Education and Research (BMBF) under the promotional reference 03VP02540,Acknowledgements,[0],[0]
"(Ar-
gumenText) and the DFG-funded research training group “Adaptive Preparation of Information form Heterogeneous Sources” (AIPHES, GRK 1994/1).",Acknowledgements,[0],[0]
Argument mining is a core technology for automating argument search in large document collections.,abstractText,[0],[0]
"Despite its usefulness for this task, most current approaches are designed for use onlywith specific text types and fall short when applied to heterogeneous texts.",abstractText,[0],[0]
"In this paper, we propose a new sentential annotation scheme that is reliably applicable by crowd workers to arbitrary Web texts.",abstractText,[0],[0]
"We source annotations for over 25,000 instances covering eight controversial topics.",abstractText,[0],[0]
We show that integrating topic information into bidirectional long short-termmemory networks outperforms vanilla BiLSTMs by more than 3 percentage points in F1 in twoand three-label cross-topic settings.,abstractText,[0],[0]
We also show that these results can be further improved by leveraging additional data for topic relevance using multi-task learning.,abstractText,[0],[0]
Cross-topic Argument Mining from Heterogeneous Sources,title,[0],[0]
"Crowdsourcing is an omnipresent phenomenon: it has emerged as an integral part of the machine learning pipeline in recent years, and one reason for the great advances in deep learning is the presence of large data sets that have been labeled by the crowd (e.g., Deng et al., 2009; Krizhevsky, 2009).",1. Introduction,[0],[0]
"Crowdsourcing is also at the heart of peer grading systems (e.g., Alfaro & Shavlovsky, 2014), which help with rising enrollment at universities, and online rating systems (e.g., Liao et al., 2014), which many of us rely on when choosing the next restaurant, to provide just a few examples.
",1. Introduction,[0],[0]
A crowdsourcing scenario consists of a set of workers and a set of tasks that need to be solved.,1. Introduction,[0],[0]
A data curator utilizing crowdsourcing can aim at estimating various quantities of interest.,1. Introduction,[0],[0]
The first goal might be to estimate the true labels or answers for the tasks at hand.,1. Introduction,[0],[0]
"Typically, additional constraints are involved here such as a worker not being willing
1Department of Computer Science, Rutgers University, Piscataway Township, New Jersey, USA.",1. Introduction,[0],[0]
Correspondence to: Matthäus,1. Introduction,[0],[0]
"Kleindessner <matthaeus.kleindessner@rutgers.edu>, Pranjal Awasthi <pranjal.awasthi@rutgers.edu>.
Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
to solve too many tasks and the data curator wanting to get high-quality labels at a low price.",1. Introduction,[0],[0]
The canonical example of this case is the Amazon Mechanical TurkTM.,1. Introduction,[0],[0]
There one cannot track specific workers as they are fleeting.,1. Introduction,[0],[0]
"However, in scenarios such as peer grading or online rating systems, a second goal might be to estimate worker qualities, especially if workers can be reused at a later time.
",1. Introduction,[0],[0]
"In a seminal paper, Dawid & Skene (1979) proposed a formal model that involves worker quality parameters for crowdsourcing scenarios in the context of classification.",1. Introduction,[0],[0]
"The Dawid-Skene model has become a standard theoretical framework and has led to a flurry of research over the past few years (Liu et al., 2012; Raykar & Yu, 2012; Li et al., 2013; Gao et al., 2016; Zhang et al., 2016; Khetan et al., 2017), in particular in its special symmetric form usually referred to as one-coin model (Ghosh et al., 2011; Karger et al., 2011a;b; Dalvi et al., 2013; Gao & Zhou, 2013; Karger et al., 2014; Bonald & Combes, 2017; Ma et al., 2017).",1. Introduction,[0],[0]
"In its general form for binary classification problems, the DawidSkene model assumes that for each worker, the probability of providing the wrong label only depends on the true label of the task, but not on the task itself.",1. Introduction,[0],[0]
"Moreover, given the true label, the responses provided by different workers are independent.",1. Introduction,[0],[0]
"The one-coin model additionally assumes that for each worker, the probability of providing the wrong label is the same for both classes.",1. Introduction,[0],[0]
We will formally introduce the one-coin model in Section 2.,1. Introduction,[0],[0]
"A discussion of prior work work is provided in Section 5 and Appendix A.
The crucial limitation of the Dawid-Skene and one-coin model is the assumption that workers’ error probabilities are task-independent.",1. Introduction,[0],[0]
"In particular, this excludes the possibility of colluding adversaries (other than those that provide the wrong label all of the time), which might make these models a poor approximation of the real world encountered in such applications as peer grading or online rating.",1. Introduction,[0],[0]
"In this paper, we study a significant extension of the one-coin model that allows for arbitrary, highly colluding adversaries.",1. Introduction,[0],[0]
We provide an algorithm for estimating the workers’ error probabilities and prove that it asymptotically recovers the true error probabilities.,1. Introduction,[0],[0]
"Using our estimates of the error probabilities in weighted majority votes, we also provide strategies to estimate ground-truth labels of the tasks.",1. Introduction,[0],[0]
Experiments on both synthetic and real data show that our approach clearly outperforms existing methods in the presence of adversaries.,1. Introduction,[0],[0]
"We first describe a general model for crowdsourcing with non-adaptive workers and binary classification tasks: there are n workers w1, . . .",2. Setup and problem formulation,[0],[0]
", wn and an i.i.d. sample of m tasklabel pairs ((xi, yi))mi=1 ∼",2. Setup and problem formulation,[0],[0]
"Dm, where D is a joint probability distribution over tasks x ∈ X and corresponding labels y ∈ {−1,+1}.",2. Setup and problem formulation,[0],[0]
"There is a variable gij ∈ {0, 1},",2. Setup and problem formulation,[0],[0]
i ∈,2. Setup and problem formulation,[0],[0]
"[m], j ∈",2. Setup and problem formulation,[0],[0]
"[n], indicating whether worker wj is presented with task xi (for k ∈ N, we use [k] to denote the set {1, . . .",2. Setup and problem formulation,[0],[0]
", k}).",2. Setup and problem formulation,[0],[0]
"If wj is presented with xi, that is gij = 1, wj provides an estimate wj(xi) ∈ {−1,+1} of the ground-truth label yi.",2. Setup and problem formulation,[0],[0]
"Let A ∈ {−1, 0,+1}m×n be a matrix that stores all the responses collected from the workers: Aij = wj(xi) if gij = 1 and Aij = 0",2. Setup and problem formulation,[0],[0]
"if gij = 0.
",2. Setup and problem formulation,[0],[0]
We assume that each worker wj follows some (probabilistic or deterministic) strategy such that wj(xi) only depends on xi.,2. Setup and problem formulation,[0],[0]
"In particular, given xi, any two different workers’ responses wj(xi) and wk(xi) and the ground-truth label yi are independent.",2. Setup and problem formulation,[0],[0]
"Let εwj (x, y) ∈",2. Setup and problem formulation,[0],[0]
"[0, 1] be the conditional error probability that, given x and y, wj(x) does not equal y, that is
εwj (x, y)",2. Setup and problem formulation,[0],[0]
":= Prwj |(x,y)[wj(x) 6=",2. Setup and problem formulation,[0],[0]
"y | (x, y)].",2. Setup and problem formulation,[0],[0]
"(1)
Note that the unconditional probability of wj(x) being incorrect, before seeing x and y, is given by
Pr(x,y)∼D,wj [wj(x) 6=",2. Setup and problem formulation,[0],[0]
"y] = E(x,y)∼D[εwj (x, y)]",2. Setup and problem formulation,[0],[0]
"=: εwj .
",2. Setup and problem formulation,[0],[0]
"Now one may study the following questions:
(i)",2. Setup and problem formulation,[0],[0]
"Given only the matrix A, how can we estimate the ground-truth labels y1, . . .",2. Setup and problem formulation,[0],[0]
", ym?
(ii)",2. Setup and problem formulation,[0],[0]
"Given only the matrix A, how can we estimate the workers’ unconditional error probabilities εw1 , . . .",2. Setup and problem formulation,[0],[0]
", εwn?
(iii)",2. Setup and problem formulation,[0],[0]
"If we can choose gij (either in advance of collecting workers’ responses or adaptively while doing so), how should we choose it such that we can achieve (i) or (ii) with a minimum number of collected responses?
",2. Setup and problem formulation,[0],[0]
"In case of εwj (x, y) as defined in (1) being constant on X × {−1,+1}, that is εwj (x, y) ≡ εwj , for all j ∈",2. Setup and problem formulation,[0],[0]
"[n], our model boils down to what is usually referred to as the one-coin model (e.g., Szepesvari, 2015), for which (i) to (iii) have been studied extensively (see Section 5 and Appendix A for references and a detailed discussion).",2. Setup and problem formulation,[0],[0]
With this paper we initiate the study of a significant extension of the one-coin model.,2. Setup and problem formulation,[0],[0]
"We will allow almost half of the workers to deviate from the one-coin model and for such a worker wj , the conditional error probability εwj (x, y) to be a completely arbitrary random variable.",2. Setup and problem formulation,[0],[0]
"In other words, we will allow for arbitrary adversaries, for which not only error
probabilities can be high, but for which error probabilities can be arbitrarily correlated.",2. Setup and problem formulation,[0],[0]
We mainly study (ii) in this scenario.,2. Setup and problem formulation,[0],[0]
We then make use of existing results for the onecoin model to answer (i) satisfactorily for our purposes.,2. Setup and problem formulation,[0],[0]
"We do not deal with (iii), but instead assume that gij has been specified in advance.",2. Setup and problem formulation,[0],[0]
In this section we want to present the general outline of our approach.,3. General outline of our approach,[0],[0]
"A key insight is that the unconditional probability of workers wj and wk being agreeing is given by
Pr(x,y)∼D,wj ,wk [wj(x) = wk(x)]",3. General outline of our approach,[0],[0]
"= 1− εwj − εwk+ 2εwjεwk + 2 Cov(x,y)∼D[εwj (x, y), εwk(x, y)].
(2)
Cov(x,y)∼D[εwj",3. General outline of our approach,[0],[0]
"(x, y), εwk(x, y)] denotes the covariance between random variables εwj",3. General outline of our approach,[0],[0]
"(x, y) and εwk(x, y), that is
Cov(x,y)∼D[εwj",3. General outline of our approach,[0],[0]
"(x, y), εwk(x, y)]",3. General outline of our approach,[0],[0]
"= E(x,y)∼D[(εwj (x, y)− εwj ) · (εwk(x, y)− εwk)].
",3. General outline of our approach,[0],[0]
"A proof of (2) can be found in Appendix B. The probability on the left-hand side of (2) can be easily estimated from A by the ratio of the number of tasks that wj and wk agreed on to the number of tasks they were both presented with:
Pr[wj(x) = wk(x)]",3. General outline of our approach,[0],[0]
≈ ∑m i=1,3. General outline of our approach,[0],[0]
"gijgik1{Aij = Aik}∑m
i=1",3. General outline of our approach,[0],[0]
"gijgik =: pjk.
(3)
",3. General outline of our approach,[0],[0]
"This suggests to solve the system of equations
1− εj",3. General outline of our approach,[0],[0]
"− εk + 2εjεk + 2cjk = pjk, 1 ≤ j < k ≤ n, (4)
in the unknowns εl, l ∈",3. General outline of our approach,[0],[0]
"[n], and cjk, 1 ≤ j",3. General outline of our approach,[0],[0]
"< k ≤ n, in order to obtain estimates of the workers’ unconditional error probabilities εw1 , . . .",3. General outline of our approach,[0],[0]
", εwn .",3. General outline of our approach,[0],[0]
"However, there is a catch: in general, the system (4) is not identifiable and has several solutions.",3. General outline of our approach,[0],[0]
We will assume that at least n2 + 2 of the workers follow the one-coin model and have error probabilities smaller than one half.,3. General outline of our approach,[0],[0]
"A worker wj following the one-coin model implies
Cov(x,y)∼D[εwj",3. General outline of our approach,[0],[0]
"(x, y), εwk(x, y)] = 0, ∀k 6=",3. General outline of our approach,[0],[0]
"j, (5)
and hence under this assumption we can restrict the search for solutions of (4) to εl, l ∈",3. General outline of our approach,[0],[0]
"[n], and cjk, 1 ≤ j",3. General outline of our approach,[0],[0]
"< k ≤ n, with the property that1
∃L ⊆",3. General outline of our approach,[0],[0]
[n] with |L| ≥ n/2 + 2 such that ∀j ∈ L : (εj < 1/2,3. General outline of our approach,[0],[0]
∧,3. General outline of our approach,[0],[0]
[∀k 6= j : cjk = 0]) .,3. General outline of our approach,[0],[0]
"(6)
1Throughout the paper, we set cjk = ckj if",3. General outline of our approach,[0],[0]
j > k.,3. General outline of our approach,[0],[0]
"We also assume pjk = pkj .
",3. General outline of our approach,[0],[0]
"Note that we never assume to know which workers follow the one-coin model, which corresponds to using the existential quantifier for the set L in (6) rather than considering a “fixed” L. We can show that the system (4) has at most one solution with property (6).",3. General outline of our approach,[0],[0]
We also provide evidence that our assumption of n2 + 2 of the workers following the one-coin model and having error probabilities smaller than one half is a necessary condition for guaranteeing the identifiability of system (4).,3. General outline of our approach,[0],[0]
"If the workers satisfy our assumption and pjk on the right-hand side of (4) are actually true agreement probabilities, then εl = εwl and cjk = Cov[εwj",3. General outline of our approach,[0],[0]
"(x, y), εwk(x, y)] is the unique solution of (4) that satisfies (6).",3. General outline of our approach,[0],[0]
"But if pjk are not exactly true agreement probabilities, there might be no solution of (4) with property (6) at all.",3. General outline of our approach,[0],[0]
"We prove that if estimates pjk are not too bad, we can solve (4) together with (6) approximately, and our approximate solution is guaranteed to be close to true error probabilities εw1 , . . .",3. General outline of our approach,[0],[0]
", εwn and covariances Cov[εwj",3. General outline of our approach,[0],[0]
"(x, y), εwk(x, y)], j < k.",3. General outline of our approach,[0],[0]
This answers (ii) from Section 2 and is the main contribution of our paper: Main result.,3. General outline of our approach,[0],[0]
Assume that at least n2 + 2 of the workers follow the one-coin model and have error probabilities not greater than γTR < 12 .,3. General outline of our approach,[0],[0]
If |Pr[wj(x) = wk(x)],3. General outline of our approach,[0],[0]
"− pjk| ≤ β for all j 6= k and β sufficiently small, we can compute estimates ε̂w1 , . . .",3. General outline of our approach,[0],[0]
", ε̂wn of εw1 , . .",3. General outline of our approach,[0],[0]
.,3. General outline of our approach,[0],[0]
", εwn such that
|εwi",3. General outline of our approach,[0],[0]
"− ε̂wi | ≤ C(γTR) · β1/4.
",3. General outline of our approach,[0],[0]
"We answer (i) from Section 2 and provide two ways to predict ground-truth labels y1, . . .",3. General outline of our approach,[0],[0]
", ym by taking weighted majority votes over the responses provided by the workers.",3. General outline of our approach,[0],[0]
"In these majority votes, the weights depend on our estimates of true error probabilities εw1 , . . .",3. General outline of our approach,[0],[0]
", εwn .",3. General outline of our approach,[0],[0]
"If gij has been specified in advance, we have the following guarantee on the quality of the estimates pjk (see (3)):",4.1. Estimating agreement probabilities,[0],[0]
Lemma 1.,4.1. Estimating agreement probabilities,[0],[0]
Assume ∑m i=1,4.1. Estimating agreement probabilities,[0],[0]
"gijgik > 0, j 6= k.",4.1. Estimating agreement probabilities,[0],[0]
Let δ > 0,4.1. Estimating agreement probabilities,[0],[0]
"and
βjk",4.1. Estimating agreement probabilities,[0],[0]
"= min
{ 1, [ ln(2n2/δ)/ ( 2 ∑m
i=1",4.1. Estimating agreement probabilities,[0],[0]
"gijgik
)]1/2} .
",4.1. Estimating agreement probabilities,[0],[0]
"Then we have with probability at least 1− δ over the sample ((xi, yi))",4.1. Estimating agreement probabilities,[0],[0]
m i=1,4.1. Estimating agreement probabilities,[0],[0]
"and the randomness in workers’ strategies that
|Pr[wj(x) = wk(x)]− pjk| ≤ βjk, 1 ≤ j < k ≤",4.1. Estimating agreement probabilities,[0],[0]
"n.
Proof.",4.1. Estimating agreement probabilities,[0],[0]
A straightforward application of Hoeffding’s inequality and the union bound yields the result.,4.1. Estimating agreement probabilities,[0],[0]
"If all workers follow the one-coin model, that is εwj (x, y) ≡",4.2. Identifiability and approximate solution,[0],[0]
εwj for all j ∈,4.2. Identifiability and approximate solution,[0],[0]
"[n], we have
Cov(x,y)∼D[εwj",4.2. Identifiability and approximate solution,[0],[0]
"(x, y), εwk(x, y)]",4.2. Identifiability and approximate solution,[0],[0]
"= 0, 1 ≤ j < k ≤ n, and system (4) reduces to
1− εj − εk + 2εjεk = pjk, 1 ≤ j < k ≤ n, (7)
in the unknowns εl, l ∈",4.2. Identifiability and approximate solution,[0],[0]
[n].,4.2. Identifiability and approximate solution,[0],[0]
"It is well known that, in general, even (7) is not identifiable.",4.2. Identifiability and approximate solution,[0],[0]
"For example, if pjk = 1 for all 1 ≤ j < k ≤ n, there are the two solutions εl = 0, l ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n], and εl = 1, l ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n], corresponding to either all perfect or all completely erroneous workers.",4.2. Identifiability and approximate solution,[0],[0]
"On the other hand, the system (7) is identifiable if we assume that on average workers are better than random guessing, that is 1 n ∑n j=1 εwj < 1 2 , and there are at least three informative workers with εwj",4.2. Identifiability and approximate solution,[0],[0]
6= 12,4.2. Identifiability and approximate solution,[0],[0]
(,4.2. Identifiability and approximate solution,[0],[0]
"Bonald & Combes, 2017).
",4.2. Identifiability and approximate solution,[0],[0]
"Clearly, these two conditions do not guarantee identifiability of the general system (4).",4.2. Identifiability and approximate solution,[0],[0]
"The next lemma shows that even if we additionally assume half of the workers to follow the one-coin model, the system (4) is not identifiable.",4.2. Identifiability and approximate solution,[0],[0]
Here we only state an informal version of the lemma.,4.2. Identifiability and approximate solution,[0],[0]
"A detailed version and its proof can be found in Appendix B.
Lemma 2.",4.2. Identifiability and approximate solution,[0],[0]
"There exists an instance of the system (4), where n is even, that has two different solutions.",4.2. Identifiability and approximate solution,[0],[0]
"In both solutions, it holds that εl < 12 , l ∈",4.2. Identifiability and approximate solution,[0],[0]
[n].,4.2. Identifiability and approximate solution,[0],[0]
"Furthermore:
(a) In the first solution, cjk = 0 for all j ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n2 ] and k 6= j, and εl is small",4.2. Identifiability and approximate solution,[0],[0]
if l ∈,4.2. Identifiability and approximate solution,[0],[0]
[n2 ] and,4.2. Identifiability and approximate solution,[0],[0]
big if l ∈,4.2. Identifiability and approximate solution,[0],[0]
[n] \,4.2. Identifiability and approximate solution,[0],[0]
[ n 2 ].,4.2. Identifiability and approximate solution,[0],[0]
"(b) In the second solution, cjk = 0 for all j ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n]\ [n2 ] and k 6= j, and εl is small if l ∈",4.2. Identifiability and approximate solution,[0],[0]
[n] \,4.2. Identifiability and approximate solution,[0],[0]
"[n2 ] and big if l ∈ [ n 2 ].
We want to mention that a solution of (4) does not necessarily correspond to actual workers, that is given εl, l ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n], and cjk, 1 ≤ j",4.2. Identifiability and approximate solution,[0],[0]
"< k ≤ n, there might be no collection of workers w1, . . .",4.2. Identifiability and approximate solution,[0],[0]
", wn such that εwl = εl and Cov[εwj",4.2. Identifiability and approximate solution,[0],[0]
"(x, y), εwk(x, y)] = cjk.",4.2. Identifiability and approximate solution,[0],[0]
"By the BhatiaDavis inequality (Bhatia & Davis, 2010) it holds that Var[εwj (x, y)] ≤ εwj − ε 2wj .",4.2. Identifiability and approximate solution,[0],[0]
"Hence, a necessary condition for a solution to correspond to actual workers is that |cjk| ≤ (εj−ε 2j )1/2(εk−ε 2k )1/2",4.2. Identifiability and approximate solution,[0],[0]
(in addition to εl ∈,4.2. Identifiability and approximate solution,[0],[0]
"[0, 1]).",4.2. Identifiability and approximate solution,[0],[0]
"The two solutions in Lemma 2 correspond to actual workers.
",4.2. Identifiability and approximate solution,[0],[0]
"From now on we assume that at least n2 + 2 workers follow the one-coin model and have error probabilities smaller than one half:2
Assumption A.",4.2. Identifiability and approximate solution,[0],[0]
There exists L ⊆,4.2. Identifiability and approximate solution,[0],[0]
"[n] with |L| ≥ n/2 + 2 such that for all j ∈ L, the worker wj follows the one-coin model with error probability εwj < 1/2.
",4.2. Identifiability and approximate solution,[0],[0]
This corresponds to considering (4) together with the constraint (6).,4.2. Identifiability and approximate solution,[0],[0]
"The system (4) together with (6) is identifiable:
Proposition 1.",4.2. Identifiability and approximate solution,[0],[0]
"There exists at most one solution of system (4) that has property (6).
",4.2. Identifiability and approximate solution,[0],[0]
"2All results of Section 4.2 hold true if we assume, more generally, the existence of L ⊆",4.2. Identifiability and approximate solution,[0],[0]
"[n] with |L| ≥ n
2 + 2 such that (5)
together with εwj < 1 2 holds for all j ∈ L.
Proof.",4.2. Identifiability and approximate solution,[0],[0]
"Assuming there are two solutions (εS1l )l∈[n], (c S1jk )",4.2. Identifiability and approximate solution,[0],[0]
"1≤j<k≤n and (ε S2 l )l∈[n], (c S2 jk )1≤j<k≤n with L1 and L2 satisfying (6), there have to be pairwise different i1, i2, i3 ∈ L1 ∩ L2.",4.2. Identifiability and approximate solution,[0],[0]
"It is easy to see that (εS1i1 , ε S1 i2 , εS1i3 ) and (εS2i1 , ε S2 i2 , εS2i3 ) and consequently also all the other components of the two solutions have to coincide.",4.2. Identifiability and approximate solution,[0],[0]
"Details can be found in Appendix B.
If pjk at the right-hand side of (4) are true agreement probabilities, the true error probabilities εw1 , . . .",4.2. Identifiability and approximate solution,[0],[0]
", εwn and covariances Cov[εwj",4.2. Identifiability and approximate solution,[0],[0]
"(x, y), εwk(x, y)], j < k, make up the unique solution of (4) that satisfies (6), but if pjk are not exactly true agreement probabilities, there might be no solution of (4) that satisfies (6) at all.",4.2. Identifiability and approximate solution,[0],[0]
"Our goal is then to find a solution of (4) that satisfies (6) approximately and to show that our approximate solution has to be close to εw1 , . . .",4.2. Identifiability and approximate solution,[0],[0]
", εwn and Cov[εwj",4.2. Identifiability and approximate solution,[0],[0]
"(x, y), εwk(x, y)], j < k.",4.2. Identifiability and approximate solution,[0],[0]
"As a first step towards this goal we need a generalization of Proposition 1:
Proposition 2.",4.2. Identifiability and approximate solution,[0],[0]
Let γ < 1/2 and ν < 1/8− γ/2 +,4.2. Identifiability and approximate solution,[0],[0]
γ2/2.,4.2. Identifiability and approximate solution,[0],[0]
"If there exist two solutions (εSil )l∈[n], (c Si jk )1≤j<k≤n, i ∈ {1, 2}, of system (4) (where pjk ∈",4.2. Identifiability and approximate solution,[0],[0]
"[0, 1]) with the property that εSil ∈",4.2. Identifiability and approximate solution,[0],[0]
"[0, 1], l ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n], and
∃Li ⊆",4.2. Identifiability and approximate solution,[0],[0]
[n] with |Li| ≥ n/2 + 2 such that ∀j ∈ Li : ( εSij ≤,4.2. Identifiability and approximate solution,[0],[0]
γ,4.2. Identifiability and approximate solution,[0],[0]
∧,4.2. Identifiability and approximate solution,[0],[0]
[ ∀k 6=,4.2. Identifiability and approximate solution,[0],[0]
"j : |c Sijk | ≤ ν ]) , (8)
then∣∣εS1l − εS2l ∣∣ ≤ G(γ, ν)√ν, ∣∣c S1jk − c S2jk ∣∣ ≤ 3G(γ, ν)√ν for l ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n], j < k, where G(γ, ν)→ G(γ)",4.2. Identifiability and approximate solution,[0],[0]
> 0,4.2. Identifiability and approximate solution,[0],[0]
"as ν → 0.
",4.2. Identifiability and approximate solution,[0],[0]
"The proof of Proposition 2, which provides an explicit expression for G(γ, ν), can be found in Appendix B.
In a next step, we assume that we are given pairwise different i1, i2, i3 ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n] such that wi1 , wi2 , wi3 follow the onecoin model with εwi1 , εwi2 , εwi3 < 1/2.",4.2. Identifiability and approximate solution,[0],[0]
"In this case, assuming that estimates pjk are close to true agreement probabilities, we can construct a solution of (4) that is guaranteed to be close to the true error probabilities and covariances (and hence approximately satisfies (6)).",4.2. Identifiability and approximate solution,[0],[0]
"This is made precise in the next lemma (its proof can be found in Appendix B).
",4.2. Identifiability and approximate solution,[0],[0]
Lemma 3.,4.2. Identifiability and approximate solution,[0],[0]
Let γTR < 1/2 and consider the system (4) with p TRjk ∈,4.2. Identifiability and approximate solution,[0],[0]
"[0, 1] as right-hand side.",4.2. Identifiability and approximate solution,[0],[0]
"Assume there exists a solution3 (εTRl )l∈[n], (c TR jk )1≤j<k≤n with the property that",4.2. Identifiability and approximate solution,[0],[0]
εTRl ∈,4.2. Identifiability and approximate solution,[0],[0]
"[0, 1] and
∃LTR ⊆",4.2. Identifiability and approximate solution,[0],[0]
[n] with |LTR| ≥ n/2 + 2 such that ∀j ∈ LTR : ( εTRj ≤ γTR,4.2. Identifiability and approximate solution,[0],[0]
∧,4.2. Identifiability and approximate solution,[0],[0]
[ ∀k 6=,4.2. Identifiability and approximate solution,[0],[0]
j :,4.2. Identifiability and approximate solution,[0],[0]
c TRjk = 0 ]) .,4.2. Identifiability and approximate solution,[0],[0]
"(9)
Now consider the system (4) with pjk ∈",4.2. Identifiability and approximate solution,[0],[0]
"[0, 1] as right-hand side.",4.2. Identifiability and approximate solution,[0],[0]
"Assume that |p TRjk − pjk| ≤ β for all j 6= k, where
3By Proposition 1, this solution is unique.
",4.2. Identifiability and approximate solution,[0],[0]
β satisfies β < 1/2− 2γTR + 2γ2TR.,4.2. Identifiability and approximate solution,[0],[0]
"Let i1, i2, i3 ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n] be pairwise different and set
B := −2",4.2. Identifiability and approximate solution,[0],[0]
"+ 4pi1i3 , C := 1 + 2pi1i2pi2i3",4.2. Identifiability and approximate solution,[0],[0]
− pi1i2,4.2. Identifiability and approximate solution,[0],[0]
"− pi1i3 − pi2i3 ,
εRi2 := 1 2 − √ B + 4C 2 √ B , εSi2 := min(γTR,max(0, ε R i2))
(10)
and for all l 6= i2 and for all 1 ≤ j < k ≤ n
εRl := pi2l − 1 + εSi2
2εSi2 − 1 ,
εSl :=
{ min(γTR,max(0, ε R l ))",4.2. Identifiability and approximate solution,[0],[0]
"if l ∈ {i1, i3}
min(1,max(0, εRl ))",4.2. Identifiability and approximate solution,[0],[0]
"if l /∈ {i1, i3} ,
c Sjk := pjk − (1− εSj − εSk + 2εSj εSk )
2 .
(11)
",4.2. Identifiability and approximate solution,[0],[0]
"If all expressions are defined (i.e., B > 0, B + 4C ≥ 0 and εSi2 6= 1 2 ), then (ε S l )l∈[n], (c S jk)1≤j<k≤n is a solution of (4) with pjk as right-hand side.",4.2. Identifiability and approximate solution,[0],[0]
"If i1, i2, i3 ∈ LTR, then all expressions are defined and∣∣εTRl",4.2. Identifiability and approximate solution,[0],[0]
"− εSl ∣∣ ≤ H(γTR, β)√β, l ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n],∣∣c TRjk − c Sjk∣∣ ≤",4.2. Identifiability and approximate solution,[0],[0]
"3H(γTR, β)√β + β/2, j < k, (12) where H(γTR, β)→ H(γTR)",4.2. Identifiability and approximate solution,[0],[0]
> 0,4.2. Identifiability and approximate solution,[0],[0]
"as β → 0.
",4.2. Identifiability and approximate solution,[0],[0]
"In Lemma 3, for constructing the solution (εSl )l∈[n], (c Sjk)1≤j<k≤n as defined in (10) and (11) we need to know γTR < 1/2, which is an upper bound on the error probabilities of at least n2 + 2 workers that follow the one-coin model.",4.2. Identifiability and approximate solution,[0],[0]
"In practice, we might choose γTR depending on the difficulty of the tasks or simply set it conservatively, for example as γTR = 0.45.",4.2. Identifiability and approximate solution,[0],[0]
"If i1, i2, i3 ∈ LTR, then (12) implies that (εSl )l∈[n], (c S jk)1≤j<k≤n satisfies (8) with
γ = γTR +H(γTR, β) √ β, ν = 3H(γTR, β) √ β + β/2.
",4.2. Identifiability and approximate solution,[0],[0]
"(13)
If we know the value of β (using Lemma 1, we easily obtain an upper bound β that holds with high probability), we can compute these quantities.",4.2. Identifiability and approximate solution,[0],[0]
"This suggests the following strategy for obtaining estimates of εw1 , . . .",4.2. Identifiability and approximate solution,[0],[0]
", εwn and Cov[εwj",4.2. Identifiability and approximate solution,[0],[0]
"(x, y), εwk(x, y)], j < k: we sample pairwise different i1, i2, i3 ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n] uniformly at random and construct (εSl )l∈[n], (c S jk)1≤j<k≤n as defined in (10) and (11).",4.2. Identifiability and approximate solution,[0],[0]
"If one of the expressions is not defined, we can immediately discard (i1, i2, i3).",4.2. Identifiability and approximate solution,[0],[0]
"Otherwise, we check whether (εSl )l∈[n], (c Sjk)1≤j<k≤n satisfies (8) with γ and ν as specified in (13).",4.2. Identifiability and approximate solution,[0],[0]
"If it does, since (εTRl )l∈[n], (c TR jk + (pjk− p TRjk )/2)1≤j<k≤n is a solution of (4) with pjk as right-hand side that satisfies
property (8) too, Proposition 2 guarantees that ∣∣εTRl",4.2. Identifiability and approximate solution,[0],[0]
"− εSl ∣∣ ≤√3H(γTR, β)√β",4.2. Identifiability and approximate solution,[0],[0]
"+ β2 · G ( γTR +H(γTR, β)",4.2. Identifiability and approximate solution,[0],[0]
"√ β, 3H(γTR, β)",4.2. Identifiability and approximate solution,[0],[0]
"√ β + β
2
) ∼ β1/4
(14)
for all l ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n] and a similar bound on |c TRjk − c Sjk|, j < k.",4.2. Identifiability and approximate solution,[0],[0]
"If (εSl )l∈[n], (c S jk)1≤j<k≤n does not satisfy (8), we discard (i1, i2, i3) and start anew.",4.2. Identifiability and approximate solution,[0],[0]
"Note that under our Assumption A, the probability of choosing i1, i2, i3 such that i1, i2, i3 ∈ LTR is greater than 1/8.",4.2. Identifiability and approximate solution,[0],[0]
"In expectation we have to discard (i1, i2, i3) for not more than eight times before finding a solution that satisfies (8) and hence (14).
",4.2. Identifiability and approximate solution,[0],[0]
"Assuming that every worker is presented with every task, that is gij",4.2. Identifiability and approximate solution,[0],[0]
= 1,4.2. Identifiability and approximate solution,[0],[0]
for all i ∈,4.2. Identifiability and approximate solution,[0],[0]
[m] and j ∈,4.2. Identifiability and approximate solution,[0],[0]
"[n], it follows from Lemma 1 and (14) that m has to scale as ln(n2/δ)/ρ8 in order that the described strategy is guaranteed to yield, with probability at least 1 − δ, estimates εS1 , . . .",4.2. Identifiability and approximate solution,[0],[0]
", εSn satisfying |εTRl − εSl
∣∣ ≤ ρ,",4.2. Identifiability and approximate solution,[0],[0]
l ∈,4.2. Identifiability and approximate solution,[0],[0]
[n].,4.2. Identifiability and approximate solution,[0],[0]
"This is significantly larger than the rate m ∼ ln(n2/δ)/ρ2 required by the TE algorithm, which solves the estimation problem for the error probabilities in the one-coin model and is claimed to be minimax optimal (Bonald & Combes, 2017).",4.2. Identifiability and approximate solution,[0],[0]
"We suspect that our rate with its dependence on ρ−8 is not optimal and consider it to be an interesting follow-up question to study the minimax rate for our extension of the one-coin model.
",4.2. Identifiability and approximate solution,[0],[0]
"Although the convergence rate that we can guarantee for the described strategy is slow, we might still hope that the strategy performs better in practice.",4.2. Identifiability and approximate solution,[0],[0]
"However, there is an issue that we have to overcome.",4.2. Identifiability and approximate solution,[0],[0]
"Unless β is very small, γ and ν as specified in (13) are too big for being meaningful, that is any solution (εSl )l∈[n], (c S jk)1≤j<k≤n as defined in (10) and (11) will satisfy (8) with these values.",4.2. Identifiability and approximate solution,[0],[0]
"We will not discard any (i1, i2, i3), regardless of whether i1, i2, i3 ∈ LTR holds or not.",4.2. Identifiability and approximate solution,[0],[0]
"We deal with this issue by adapting the strategy as follows: let P ⊆ {(i1, i2, i3) : i1, i2, i3 ∈",4.2. Identifiability and approximate solution,[0],[0]
[n] pairwise different}.,4.2. Identifiability and approximate solution,[0],[0]
"For every p = (i1, i2, i3) ∈ P , we construct (εSl (p))l∈[n], (c Sjk(p))1≤j<k≤n as defined in (10) and (11).",4.2. Identifiability and approximate solution,[0],[0]
We set Qp =,4.2. Identifiability and approximate solution,[0],[0]
"[n] unless γ as specified in (13) is smaller than one, in which case we set Qp = {l ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n] : εSl (p) ≤ γ} and discard any solution (εSl (p))l∈[n], (c S jk(p))1≤j<k≤n for which |Qp| < n2 + 2.",4.2. Identifiability and approximate solution,[0],[0]
Let ν,4.2. Identifiability and approximate solution,[0],[0]
p be the dn2 + 2e-th smallest element of {maxk∈[n]\{l} |c Slk (p)| : l ∈ Qp}.,4.2. Identifiability and approximate solution,[0],[0]
"Then we finally return the solution (εSl (p0))l∈[n], (c S jk(p0)))1≤j<k≤n for which νp is smallest, that is p0 = argminp ν",4.2. Identifiability and approximate solution,[0],[0]
"p.
",4.2. Identifiability and approximate solution,[0],[0]
"If γ is small enough, it follows from Proposition 2 that∣∣εTRl",4.2. Identifiability and approximate solution,[0],[0]
"− εSl (p0)∣∣ ≤√max{νp0 , β/2} · G ( γTR +H(γTR, β) √ β,max{νp0 , β/2} ) .",4.2. Identifiability and approximate solution,[0],[0]
"(15)
Note that if P contains at least one triple of indices i1, i2, i3 ∈ LTR, then νp0 ≤ 3H(γTR, β) √ β + β2 , so that the guarantee (15) is at least as good as (14).",4.2. Identifiability and approximate solution,[0],[0]
We also expect νp0 to be smaller the larger P is.,4.2. Identifiability and approximate solution,[0],[0]
"Hence, we should choose P as large as we can afford due to computational reasons, but in practice, there is one more aspect that we have to consider.",4.2. Identifiability and approximate solution,[0],[0]
"Depending on how gij has been chosen, there might be workers wj and wk that were presented with only a few common tasks or no common tasks at all.",4.2. Identifiability and approximate solution,[0],[0]
"In this case, the estimate pjk of the agreement probability between wj and wk is only poor and there is no uniform bound β on |p TRjk −pjk| (where p TRjk are true agreement probabilities).",4.2. Identifiability and approximate solution,[0],[0]
"We can deal with this aspect by choosing P in a way such that for all p ∈ P , all estimates pjk that are involved in the computation of (εSl (p))l∈[n] are somewhat reliable.",4.2. Identifiability and approximate solution,[0],[0]
We present a concrete implementation of this in Algorithm 1 below.,4.2. Identifiability and approximate solution,[0],[0]
"Once we have estimates ε̂w1 , . . .",4.3. Predicting ground-truth labels,[0],[0]
", ε̂wn of the true error probabilities εw1 , . . .",4.3. Predicting ground-truth labels,[0],[0]
", εwn , we predict ground-truth labels yi by taking a weighted majority vote over the responses collected for the task xi.",4.3. Predicting ground-truth labels,[0],[0]
"Our estimate for yi is given by
ŷi = sign {∑n
l=1 f(ε̂wl) ·Ail
} , (16)
where f :",4.3. Predicting ground-truth labels,[0],[0]
"[0, 1]→",4.3. Predicting ground-truth labels,[0],[0]
"[−∞,+∞].",4.3. Predicting ground-truth labels,[0],[0]
Ties are broken uniformly at random.,4.3. Predicting ground-truth labels,[0],[0]
"We consider two choices for the function f .
",4.3. Predicting ground-truth labels,[0],[0]
"It is well-known that if all workers follow the one-coin model with known error probabilities εw1 , . . .",4.3. Predicting ground-truth labels,[0],[0]
", εwn , groundtruth labels are balanced, that is Pr(x,y)∼D[y = +1] = Pr(x,y)∼D[y = −1], and gij are independent Bernoulli random variables with common success probability α > 0, then the optimal estimator for the ground-truth label yi is given by the weighted majority vote (16) with f(ε̂wl) replaced by f(εwl) =",4.3. Predicting ground-truth labels,[0],[0]
ln,4.3. Predicting ground-truth labels,[0],[0]
"((1− εwl)/εwl) (Nitzan & Paroush, 1982; Berend & Kontorovich, 2015; Bonald & Combes, 2017).",4.3. Predicting ground-truth labels,[0],[0]
"Hence, a common approach for the one-coin model is to first estimate the true error probabilities and then to estimate ground-truth labels by using the majority vote (16) with f(ε̂wl) = ln",4.3. Predicting ground-truth labels,[0],[0]
"((1− ε̂wl)/ε̂wl) (Bonald & Combes, 2017; Ma et al., 2017).",4.3. Predicting ground-truth labels,[0],[0]
"We propose to use the same majority vote, but restricted to answers from workers that we believe to follow the one-coin model.",4.3. Predicting ground-truth labels,[0],[0]
"Using the notation from Section 4.2, this means that we set f(ε̂wl) = ln ((1− ε̂wl)/ε̂wl) for l ∈ Qp0 with maxk∈[n]\{l} |c Slk (p0)| ≤ νp0 and f(ε̂wl) = 0 otherwise.
",4.3. Predicting ground-truth labels,[0],[0]
"Alternatively, we suggest to set f(ε̂wl) = 1 − 2ε̂wl for l ∈",4.3. Predicting ground-truth labels,[0],[0]
[n].,4.3. Predicting ground-truth labels,[0],[0]
With this choice of f we make use of the responses provided by all workers.,4.3. Predicting ground-truth labels,[0],[0]
"The same choice has been used for the one-coin model too (Dalvi et al., 2013).",4.3. Predicting ground-truth labels,[0],[0]
A third option would be to set f(ε̂wl) = 1− 2ε̂wl for l ∈ Qp0 with maxk∈[n]\{l} |c Slk (p0)| ≤ νp0 and f(ε̂wl) = 0,4.3. Predicting ground-truth labels,[0],[0]
"otherwise, but we do not consider this choice any further.",4.3. Predicting ground-truth labels,[0],[0]
"In the interests of clarity, we present our approach as self contained Algorithm 1.",4.4. Algorithm,[0],[0]
Choosing P as the set of triples such that involved pairs of workers have been provided with at least ten or three common tasks might seem somewhat arbitrary here.,4.4. Algorithm,[0],[0]
"Indeed, one could introduce two parameters to the algorithm instead.",4.4. Algorithm,[0],[0]
"Without optimizing for these parameters, we chose them as ten and three in all our experiments on real data, and hence we state Algorithm 1 as is.
",4.4. Algorithm,[0],[0]
"Our analysis best applies to the setting of a full matrix A (or variables gij that are independent Bernoulli random variables with common success probability, as it is assumed by Bonald & Combes, 2017, for example).",4.4. Algorithm,[0],[0]
"In this case, which we consider in our experiments on synthetic data, choosing P as stated in Algorithm 1 reduces to choosing P as the set of all triples of pairwise different indices.",4.4. Algorithm,[0],[0]
"If the number of workers n is small, this is the best one can do.",4.4. Algorithm,[0],[0]
"If n is large, it is infeasible to choose P as the set of all triples though since the running time of Algorithm 1 is in O(n2(m + |P",4.4. Algorithm,[0],[0]
|)).,4.4. Algorithm,[0],[0]
"If n is large and A full, one should sample P uniformly at random.",4.4. Algorithm,[0],[0]
For |P | ≥ ln δ/ ln(7/8),4.4. Algorithm,[0],[0]
our error guarantee (14) holds with probability at least 1− δ then (compare with Section 4.2).,4.4. Algorithm,[0],[0]
We briefly survey related work here.,5. Related work,[0],[0]
A complete discussion can be found in Appendix A.,5. Related work,[0],[0]
"As discussed in Sections 1 and 2, in crowdsourcing one might be interested in estimating ground-truth labels and/or worker qualities given the response matrix A, but also in optimal task assignment.",5. Related work,[0],[0]
"In their seminal paper, Dawid & Skene (1979) proposed an EM based algorithm to address the first two goals.",5. Related work,[0],[0]
"Since then numerous works have followed addressing all three goals for the Dawid-Skene and one-coin model (Ghosh et al., 2011; Karger et al., 2011a;b; 2013; 2014; Dalvi et al., 2013; Gao & Zhou, 2013; Gao et al., 2016; Zhang et al., 2016; Bonald & Combes, 2017; Ma et al., 2017).",5. Related work,[0],[0]
"There have also been efforts to study generalizations of the Dawid-Skene model (Jaffe et al., 2016; Khetan & Oh, 2016; Shah et al., 2016) as well as to explicitly deal with adversaries (Raykar & Yu, 2012; Jagabathula et al., 2017).",5. Related work,[0],[0]
"However, none of the prior work can handle a number of arbitrary adversaries almost as large as the number of reliable workers as we do.",5. Related work,[0],[0]
"On both synthetic and real data, we compared our proposed Algorithm 1 to straightforward majority voting for predicting labels (referred to as Maj) and the following methods from the literature: the spectral algorithms by Ghosh et al. (2011) (GKM), Dalvi et al. (2013) (RoE and EoR) and Karger et al. (2013) (KOS), the two-stage procedure by
Algorithm 1",6. Experiments,[0],[0]
"Input: crowdsourced labels stored in A ∈ {−1, 0,+1}m×n, upper bound 0 < γTR < 12 on the error probabilities of dn2 + 2e workers that follow the one-coin model, confidence parameter 0",6. Experiments,[0],[0]
< δ < 1,6. Experiments,[0],[0]
"Output: estimates (εFl )l∈[n], (c Fjk )j<k, (ŷi)i∈[m] of error probabilities, covariances and ground-truth labels
I Estimating agreement probabilities set gij = 1{Aij 6= 0}, i ∈",6. Experiments,[0],[0]
"[m], j ∈",6. Experiments,[0],[0]
[n] set qjk = ∑m i=1,6. Experiments,[0],[0]
"gijgik, j, k ∈",6. Experiments,[0],[0]
"[n] set pjk as in (3), j, k ∈",6. Experiments,[0],[0]
"[n] (pjk = NaN if qjk = 0)
",6. Experiments,[0],[0]
I Estimating error probabilities and covariances set β =,6. Experiments,[0],[0]
"[ ln(2n2/δ)/ ( 2 minj,k∈[n] qjk )]1/2 ∈ (0,+Inf ] set γ as in (13)",6. Experiments,[0],[0]
if γ /∈,6. Experiments,[0],[0]
"[0, 1] then
set γ = 1 end if set P = { (i1, i2, i3) :",6. Experiments,[0],[0]
"i1, i2, i3 ∈",6. Experiments,[0],[0]
"[n] pairwise different
and qjk ≥ 10, j, k ∈ {i1, i2, i3}, and qi2j ≥ 3, j 6= i2 }
set νold = Inf, (εFl )l∈[n] = 0, (c F jk )1≤j<k≤n = 0, L = ∅ for (i1, i2,",6. Experiments,[0],[0]
"i3) ∈ P do if not all expressions in (10) or (11) are defined then
break end if compute (εSl )l∈[n], (c S jk)1≤j<k≤n as in (10) and (11) set Q = {l ∈",6. Experiments,[0],[0]
[n] : εSl ≤ γ} set ν = dn2 +,6. Experiments,[0],[0]
2e-th smallest element of {maxk∈[n]\{l} |c Slk,6. Experiments,[0],[0]
| : l ∈ Q} (ν = NaN ifQ = ∅) if |Q| ≥ n2 + 2 AND ν <,6. Experiments,[0],[0]
νold then set (εFl )l∈[n] =,6. Experiments,[0],[0]
"(ε S l )l∈[n], (c F jk )",6. Experiments,[0],[0]
j,6. Experiments,[0],[0]
"<k = (c S jk)j<k
set L = {l ∈ Q :",6. Experiments,[0],[0]
"maxk∈[n]\{l} |c Slk | ≤ ν} set νold = ν
end if end for
I Estimating ground-truth labels set f(ε̂wl) =",6. Experiments,[0],[0]
ln ((1− ε̂wl)/ε̂wl) ∈,6. Experiments,[0],[0]
"[−Inf,+Inf], l ∈ L,
and f(ε̂wl) = 0, l ∈",6. Experiments,[0],[0]
"[n] \ L (alternatively set f(ε̂wl) = 1− 2ε̂wl , l ∈",6. Experiments,[0],[0]
"[n]) set ŷi as in (16), i ∈",6. Experiments,[0],[0]
"[m]
Zhang et al. (2016) (S-EM1 and S-EM10, where we run one or ten iterations of the EM algorithm) and the recent method by Bonald & Combes (2017) (TE).",6. Experiments,[0],[0]
"We used the Matlab implementation of KOS, S-EM1 and S-EM10 made available by Zhang et al. (2016).",6. Experiments,[0],[0]
"In our implementations of the other methods, we adapted GKM, RoE and EoR as to assume that the average error of the workers is smaller than one half rather than assuming that the error of the first worker is.",6. Experiments,[0],[0]
"We always called Algorithm 1 with parameters γTR = 0.4 and δ = 0.1, which resulted in γ being set to 1
in the execution of the algorithm in all our experiments.",6. Experiments,[0],[0]
We refer to Algorithm 1 with the logarithmic weights in (16) as Alg. 1 and and with the linear weights as Alt-Alg. 1.,6. Experiments,[0],[0]
"In the following, all results are average results obtained from running an experiment for 100 times.",6. Experiments,[0],[0]
"In our first experiment, we consider n = 50 workers and m = 5000 tasks with balanced ground-truth labels.",6.1. Synthetic data,[0],[0]
Every worker is presented with every task.,6.1. Synthetic data,[0],[0]
"For 0 ≤ t ≤ 25, we choose t workers at random.",6.1. Synthetic data,[0],[0]
"These workers are corrupted workers that all provide the same random response to every task, which is incorrect with error probability 0.5.",6.1. Synthetic data,[0],[0]
"The remaining n − t workers provide responses according to the one-coin model, where the error probability of each of these workers is 0.4.",6.1. Synthetic data,[0],[0]
Figure 1 shows the prediction error for estimating ground-truth labels and the estimation error for estimating error probabilities in both the maximum norm and the normalized 1-norm for the various methods as a function of t.,6.1. Synthetic data,[0],[0]
The prediction error is given by 1m ∑m i=1,6.1. Synthetic data,[0],[0]
1{yi 6= ŷi} for ground-truth labels yi and estimates ŷi and the estimation error is given by maxl∈[n] |εwl,6.1. Synthetic data,[0],[0]
− ε̂wl | or 1n,6.1. Synthetic data,[0],[0]
∑n l=1 |εwl − ε̂wl | for true error probabilities εwl and estimates ε̂wl .,6.1. Synthetic data,[0],[0]
"The methods Maj and KOS, by default, do not provide estimates of the workers’ error probabilities.",6.1. Synthetic data,[0],[0]
"We adapt these two methods in order to return estimates of the error probabilities too as follows: if the method returns label estimates ŷ1, . . .",6.1. Synthetic data,[0],[0]
", ŷm and worker wl provides responses A1l, . . .",6.1. Synthetic data,[0],[0]
", Aml 6= 0, then the method
returns 1m ∑m i=1",6.1. Synthetic data,[0],[0]
1{ŷi 6=,6.1. Synthetic data,[0],[0]
"Ail} as estimate ε̂wl of εwl .
",6.1. Synthetic data,[0],[0]
Our Algorithm 1 is the only method that can handle up to 23 = n2,6.1. Synthetic data,[0],[0]
− 2 corrupted workers (in accordance with our theoretical results).,6.1. Synthetic data,[0],[0]
Its estimation error is constant as the number of corrupted workers increases from 0 to 23.,6.1. Synthetic data,[0],[0]
"Its prediction error depends on which weights we use in (16): the prediction error of Alg. 1 is constant in this range too, the one of Alt-Alg. 1 is slightly increasing.",6.1. Synthetic data,[0],[0]
"If only a few workers are corrupted, Alt-Alg.1 performs better than Alg. 1, while it is the other way round if more than 13 workers are corrupted.",6.1. Synthetic data,[0],[0]
The methods from the literature predict ground-truth labels as badly as random guessing already in the presence of only six corrupted workers.,6.1. Synthetic data,[0],[0]
All these methods are outperformed by majority voting.,6.1. Synthetic data,[0],[0]
We do not have an explanation for the non-monotonic behavior of the estimation error of SEM10 in the maximum norm.,6.1. Synthetic data,[0],[0]
"In Appendix C we present similar experiments, in which the error probability of the workers following the one-coin model is smaller or the error probabilities of the corrupted workers are less correlated.",6.1. Synthetic data,[0],[0]
"Still, the overall picture there is the same.
",6.1. Synthetic data,[0],[0]
One might wonder whether one can combine the considered methods from the literature with one of the algorithms by Jagabathula et al. (2017) in order to first sort the corrupted workers out and then apply the method only to the remaining workers and their responses.,6.1. Synthetic data,[0],[0]
"However, those algorithms cannot deal with the corrupted workers considered in this experiment, which are perfectly colluding, at all.",6.1. Synthetic data,[0],[0]
"Even though provided with the correct number t of corrupted workers as input, when t ≥ 3, the soft-penalty algorithm by Jagabathula et al. (2017) was not able to identify any of the corrupted workers in any of the 100 runs of the experiment.
",6.1. Synthetic data,[0],[0]
"In our next experiment, we study the convergence rate of Algorithm 1.",6.1. Synthetic data,[0],[0]
"We consider n = 50 workers, out of which t = 23 are corrupted in the same way as above.",6.1. Synthetic data,[0],[0]
Figure 2 shows the prediction and estimation error of Algorithm 1 as a function of the number of tasks m varying from 5000 to 20000.,6.1. Synthetic data,[0],[0]
"The prediction error of Alg. 1 decreases only slightly as m increases, the prediction error of Alt-Alg. 1 decreases more significantly.",6.1. Synthetic data,[0],[0]
Most interesting is the decay of the estimation error.,6.1. Synthetic data,[0],[0]
"Apparently, in this experiment it
decreases at a rate ofm−1/2 rather than at a rate ofm−1/8 as suggested by our upper bound (compare with Section 4.2).",6.1. Synthetic data,[0],[0]
We performed experiments on six publicly available data sets that are are commonly used in the literature (cf.,6.2. Real data,[0],[0]
"Snow et al., 2008, Zhang et al., 2016, and Bonald & Combes, 2017).",6.2. Real data,[0],[0]
All six data sets come with ground truth labels for each task.,6.2. Real data,[0],[0]
"For most of the data sets the matrix A, which stores the collected responses, is highly sparse.",6.2. Real data,[0],[0]
"In order to reduce sparseness, we removed workers that provided fewer than 50 labels.",6.2. Real data,[0],[0]
"For two of the data sets, we merged classes in order to end up with binary classification problems in the same way as Bonald & Combes (2017) did (Dog: {0, 2} vs {1, 3}; Web: {0, 1, 2} vs {3, 4}).",6.2. Real data,[0],[0]
Table 2 in Appendix C provides the characteristic values of the data sets.,6.2. Real data,[0],[0]
Note that only for the Bird data set every worker provided a label for every task whereas for the other ones A is still rather sparse.,6.2. Real data,[0],[0]
Figure 5 in Appendix C shows for each data set a histogram of the error probabilities of the workers (computed over those tasks that a worker was presented with).,6.2. Real data,[0],[0]
"Figure 6 shows a heat map of the matrix (|Cov[εwj (x, y), εwk(x, y)]|)nj,k=1 (computed over those tasks that two workers were jointly presented with).
",6.2. Real data,[0],[0]
Table 1 shows the prediction error for the various methods and data sets.,6.2. Real data,[0],[0]
There is no method that performs best on all data sets.,6.2. Real data,[0],[0]
"Overall, S-EM10 seems to be the method of choice.",6.2. Real data,[0],[0]
"Our Algorithm 1 can compete with the other methods, and on four out of the six data sets, the prediction error of Alt-Alg. 1 is smaller or larger only by 0.01 than the prediction error of S-EM10.",6.2. Real data,[0],[0]
Alg. 1 performs slightly worse than Alt-Alg. 1.,6.2. Real data,[0],[0]
"The poor performance of our method on
the Bird data set might be explained by the fact that there the workers clearly deviate from our model: as Figure 6 shows, there are no n2 + 2 workers that follow the one-coin model.
",6.2. Real data,[0],[0]
We performed another experiments on these data sets by corrupting some of the workers (chosen at random).,6.2. Real data,[0],[0]
"Like in the experiments of Section 6.1, the corrupted workers provide the same random response to every task.",6.2. Real data,[0],[0]
Figure 3 shows the prediction errors for the various methods and the first three data sets as functions of the number of corrupted workers.,6.2. Real data,[0],[0]
"Similar plots for the other data sets are shown in Figure 7 in Appendix C. On none of the data sets, any method can handle more corrupted workers than Alt-Alg. 1.",6.2. Real data,[0],[0]
"In this work, we studied an extension of the well-known one-coin model for crowdsourcing that allows for colluding adversaries.",7. Discussion,[0],[0]
"Our results show that even if almost half of the workers are adversarial, one can consistently estimate the workers’ error probabilities with an efficient algorithm.
",7. Discussion,[0],[0]
"For future work, it would be interesting to relax the assumption that the reliable workers follow the one-coin model and to allow for task-dependent error probabilities also for them.",7. Discussion,[0],[0]
It would also be interesting to see whether our approach can be extended to multiclass classification problems.,7. Discussion,[0],[0]
"Another direction concerns improving the sufficient rate m ∼ ρ−8 , which we obtained for our algorithm for recovering worker qualities up to error ρ.",7. Discussion,[0],[0]
"In the absence of adversaries one can achieve a rate m ∼ ρ−2, and we would like to understand whether this gap is inherent or an artifact of our algorithm/proof.",7. Discussion,[0],[0]
"Finally, we wonder about the role of adaptive task assignment in our extension of the one-coin model.",7. Discussion,[0],[0]
This research is supported by a Rutgers Research Council Grant and a Center for Discrete Mathematics and Theoretical Computer Science (DIMACS) postdoctoral fellowship.,Acknowledgements,[0],[0]
"Most existing works on crowdsourcing assume that the workers follow the Dawid-Skene model, or the one-coin model as its special case, where every worker makes mistakes independently of other workers and with the same error probability for every task.",abstractText,[0],[0]
We study a significant extension of this restricted model.,abstractText,[0],[0]
"We allow almost half of the workers to deviate from the one-coin model and for those workers, their probabilities of making an error to be task-dependent and to be arbitrarily correlated.",abstractText,[0],[0]
"In other words, we allow for arbitrary adversaries, for which not only error probabilities can be high, but which can also perfectly collude.",abstractText,[0],[0]
"In this adversarial scenario, we design an efficient algorithm to consistently estimate the workers’ error probabilities.",abstractText,[0],[0]
Crowdsourcing with Arbitrary Adversaries,title,[0],[0]
Reinforcement learning algorithms aim at learning policies for achieving target tasks by maximizing rewards provided by the environment.,1. Introduction,[0],[0]
"In some scenarios, these rewards are supplied to the agent continuously, e.g. the running score in an Atari game (Mnih et al., 2015), or the distance between a robot arm and an object in a reaching task (Lillicrap et al., 2016).",1. Introduction,[0],[0]
"However, in many real-world scenarios, rewards extrinsic to the agent are extremely sparse or miss-
1University of California, Berkeley.",1. Introduction,[0],[0]
"Correspondence to: Deepak Pathak <pathak@berkeley.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, 2017.",1. Introduction,[0],[0]
JMLR: W&CP.,1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"(a) learn to explore in Level-1 (b) explore faster in Level-2
Figure 1.",1. Introduction,[0],[0]
Discovering how to play Super Mario Bros without rewards.,1. Introduction,[0],[0]
"(a) Using only curiosity-driven exploration, the agent makes significant progress in Level-1.",1. Introduction,[0],[0]
(b) The gained knowledge helps the agent explore subsequent levels much faster than when starting from scratch.,1. Introduction,[0],[0]
"Watch the video at http://pathak22. github.io/noreward-rl/
ing altogether, and it is not possible to construct a shaped reward function.",1. Introduction,[0],[0]
This is a problem as the agent receives reinforcement for updating its policy only if it succeeds in reaching a pre-specified goal state.,1. Introduction,[0],[0]
"Hoping to stumble into a goal state by chance (i.e. random exploration) is likely to be futile for all but the simplest of environments.
",1. Introduction,[0],[0]
"As human agents, we are accustomed to operating with rewards that are so sparse that we only experience them once or twice in a lifetime, if at all.",1. Introduction,[0],[0]
"To a three-year-old enjoying a sunny Sunday afternoon on a playground, most trappings of modern life – college, good job, a house, a family – are so far into the future, they provide no useful reinforcement signal.",1. Introduction,[0],[0]
"Yet, the three-year-old has no trouble entertaining herself in that playground using what psychologists call intrinsic motivation (Ryan, 2000) or curiosity (Silvia, 2012).",1. Introduction,[0],[0]
Motivation/curiosity have been used to explain the need to explore the environment and discover novel states.,1. Introduction,[0],[0]
"The French word flâneur perfectly captures the notion of a curiosity-driven observer, the “deliberately aimless pedestrian, unencumbered by any obligation or sense of urgency” (Cornelia Otis Skinner).",1. Introduction,[0],[0]
"More generally, curiosity is a way of learning new skills which might come handy for pursuing rewards in the future.
",1. Introduction,[0],[0]
"Similarly, in reinforcement learning, intrinsic motivation/rewards become critical whenever extrinsic rewards are sparse.",1. Introduction,[0],[0]
"Most formulations of intrinsic reward can be grouped into two broad classes: 1) encourage the agent to explore “novel” states (Bellemare et al., 2016; Lopes
ar X
iv :1
70 5.
05 36
3v 1
[ cs
.L",1. Introduction,[0],[0]
"G
] 1
5 M
ay 2
01 7
et al., 2012; Poupart et al., 2006) or, 2) encourage the agent to perform actions that reduce the error/uncertainty in the agent’s ability to predict the consequence of its own actions (i.e. its knowledge about the environment) (Houthooft et al., 2016; Mohamed & Rezende, 2015; Schmidhuber, 1991; 2010; Singh et al., 2005; Stadie et al., 2015).
",1. Introduction,[0],[0]
"Measuring “novelty” requires a statistical model of the distribution of the environmental states, whereas measuring prediction error/uncertainty requires building a model of environmental dynamics that predicts the next state (st+1) given the current state (st) and the action (at) executed at time t. Both these models are hard to build in highdimensional continuous state spaces such as images.",1. Introduction,[0],[0]
"An additional challenge lies in dealing with the stochasticity of the agent-environment system, both due to the noise in the agent’s actuation, which causes its end-effectors to move in a stochastic manner, and, more fundamentally, due to the inherent stochasticity in the environment.",1. Introduction,[0],[0]
"To give the example from (Schmidhuber, 2010), if the agent receiving images as state inputs is observing a television screen displaying white noise, every state will be novel and it would be impossible to predict the value of any pixel in the future.",1. Introduction,[0],[0]
"Other examples of such stochasticity include appearance changes due to shadows from other moving entities, presence of distractor objects, or other agents in the environment whose motion is not only hard to predict but is also irrelevant to the agent’s goals.",1. Introduction,[0],[0]
"Somewhat different, but related, is the challenge of generalization across physically (and perhaps also visually) distinct but functionally similar parts of an environment, which is crucial for largescale problems.",1. Introduction,[0],[0]
"One proposed solution to all these problems is to only reward the agent when it encounters states that are hard to predict but are “learnable” (Schmidhuber, 1991).",1. Introduction,[0],[0]
"However, estimating learnability is a non-trivial problem (Lopes et al., 2012).
",1. Introduction,[0],[0]
"This work belongs to the broad category of methods that generate an intrinsic reward signal based on how hard it is for the agent to predict the consequences of its own actions, i.e. predict the next state given the current state and the executed action.",1. Introduction,[0],[0]
"However, we manage to escape most pitfalls of previous prediction approaches with the following key insight: we only predict those changes in the environment that could possibly be due to the actions of our agent or affect the agent, and ignore the rest.",1. Introduction,[0],[0]
"That is, instead of making predictions in the raw sensory space (e.g. pixels), we transform the sensory input into a feature space where only the information relevant to the action performed by the agent is represented.",1. Introduction,[0],[0]
We learn this feature space using self-supervision – training a neural network on a proxy inverse dynamics task of predicting the agent’s action given its current and next states.,1. Introduction,[0],[0]
"Since the neural network is only required to predict the action, it has no incentive to represent within its feature embedding space the factors of vari-
ation in the environment that do not affect the agent itself.",1. Introduction,[0],[0]
"We then use this feature space to train a forward dynamics model that predicts the feature representation of the next state, given the feature representation of the current state and the action.",1. Introduction,[0],[0]
"We provide the prediction error of the forward dynamics model to the agent as an intrinsic reward to encourage its curiosity.
",1. Introduction,[0],[0]
The role of curiosity has been widely studied in the context of solving tasks with sparse rewards.,1. Introduction,[0],[0]
"In our opinion, curiosity has two other fundamental uses.",1. Introduction,[0],[0]
Curiosity helps an agent explore its environment in the quest for new knowledge (a desirable characteristic of exploratory behavior is that it should improve as the agent gains more knowledge).,1. Introduction,[0],[0]
"Further, curiosity is a mechanism for an agent to learn skills that might be helpful in future scenarios.",1. Introduction,[0],[0]
"In this paper, we evaluate the effectiveness of our curiosity formulation in all three of these roles.
",1. Introduction,[0],[0]
"We first compare the performance of an A3C agent (Mnih et al., 2016) with and without the curiosity signal on 3-D navigation tasks with sparse extrinsic reward in the VizDoom environment.",1. Introduction,[0],[0]
We show that a curiosity-driven intrinsic reward is crucial in accomplishing these tasks (see Section 4.1).,1. Introduction,[0],[0]
"Next, we show that even in the absence of any extrinsic rewards, a curious agent learns good exploration policies.",1. Introduction,[0],[0]
"For instance, an agent trained only with curiosity as its reward is able to cross a significant portion of Level-1 in Super Mario Bros. Similarly in VizDoom, the agent learns to walk intelligently along the corridors instead of bumping into walls or getting stuck in corners (see Section 4.2).",1. Introduction,[0],[0]
"A question that naturally follows is whether the learned exploratory behavior is specific to the physical space that the agent trained itself on, or if it enables the agent to perform better in unseen scenarios too?",1. Introduction,[0],[0]
"We show that the exploration policy learned in the first level of Mario helps the agent explore subsequent levels faster (shown in Figure 1), while the intelligent walking behavior learned by the curious VizDoom agent transfers to a completely new map with new textures (see Section 4.3).",1. Introduction,[0],[0]
These results suggest that the proposed method enables an agent to learn generalizable skills even in the absence of an explicit goal.,1. Introduction,[0],[0]
Our agent is composed of two subsystems: a reward generator that outputs a curiosity-driven intrinsic reward signal and a policy that outputs a sequence of actions to maximize that reward signal.,2. Curiosity-Driven Exploration,[0],[0]
"In addition to intrinsic rewards, the agent optionally may also receive some extrinsic reward from the environment.",2. Curiosity-Driven Exploration,[0],[0]
Let the intrinsic curiosity reward generated by the agent at time t be rit and the extrinsic reward be ret .,2. Curiosity-Driven Exploration,[0],[0]
"The policy sub-system is trained to maximize the sum of these two rewards rt = rit + r e t , with r e t mostly (if not always) zero.
",2. Curiosity-Driven Exploration,[0],[0]
We represent the policy π(st; θP ) by a deep neural network with parameters θP .,2. Curiosity-Driven Exploration,[0],[0]
"Given the agent in state st, it executes the action at ∼ π(st; θP ) sampled from the policy.",2. Curiosity-Driven Exploration,[0],[0]
"θP is optimized to maximize the expected sum of rewards,
max θP Eπ(st;θP )",2. Curiosity-Driven Exploration,[0],[0]
"[Σtrt] (1)
Unless specified otherwise, we use the notation π(s) to denote the parameterized policy π(s; θP ).",2. Curiosity-Driven Exploration,[0],[0]
"Our curiosity reward model can potentially be used with a range of policy learning methods; in the experiments discussed here, we use the asynchronous advantage actor critic policy gradient (A3C) (Mnih et al., 2016) for policy learning.",2. Curiosity-Driven Exploration,[0],[0]
"Our main contribution is in designing an intrinsic reward signal based on prediction error of the agent’s knowledge about its environment that scales to high-dimensional continuous state spaces like images, bypasses the hard problem of predicting pixels and is unaffected by the unpredictable aspects of the environment that do not affect the agent.",2. Curiosity-Driven Exploration,[0],[0]
"Making predictions in the raw sensory space (e.g. when st corresponds to images) is undesirable not only because it is hard to predict pixels directly, but also because it is unclear if predicting pixels is even the right objective to optimize.",2.1. Prediction error as curiosity reward,[0],[0]
"To see why, consider using prediction error in the pixel space as the curiosity reward.",2.1. Prediction error as curiosity reward,[0],[0]
Imagine a scenario where the agent is observing the movement of tree leaves in a breeze.,2.1. Prediction error as curiosity reward,[0],[0]
"Since it is inherently hard to model breeze, it is even harder to predict the pixel location of each leaf.
",2.1. Prediction error as curiosity reward,[0],[0]
This implies that the pixel prediction error will remain high and the agent will always remain curious about the leaves.,2.1. Prediction error as curiosity reward,[0],[0]
But the motion of the leaves is inconsequential to the agent and therefore its continued curiosity about them is undesirable.,2.1. Prediction error as curiosity reward,[0],[0]
The underlying problem is that the agent is unaware that some parts of the state space simply cannot be modeled and thus the agent can fall into an artificial curiosity trap and stall its exploration.,2.1. Prediction error as curiosity reward,[0],[0]
Novelty-seeking exploration schemes that record the counts of visited states in a tabular form (or their extensions to continuous state spaces) also suffer from this issue.,2.1. Prediction error as curiosity reward,[0],[0]
"Measuring learning progress instead of prediction error has been proposed in the past as one solution (Schmidhuber, 1991).",2.1. Prediction error as curiosity reward,[0],[0]
"Unfortunately, there are currently no known computationally feasible mechanisms for measuring learning progress.
",2.1. Prediction error as curiosity reward,[0],[0]
"If not the raw observation space, then what is the right feature space for making predictions so that the prediction error provides a good measure of curiosity?",2.1. Prediction error as curiosity reward,[0],[0]
"To answer this question, let us divide all sources that can modify the agent’s observations into three cases: (1) things that can be controlled by the agent; (2) things that the agent cannot control but that can affect the agent (e.g. a vehicle driven by another agent), and (3) things out of the agent’s control and not affecting the agent (e.g. moving leaves).",2.1. Prediction error as curiosity reward,[0],[0]
A good feature space for curiosity should model (1) and (2) and be unaffected by (3).,2.1. Prediction error as curiosity reward,[0],[0]
"This latter is because, if there is a source of variation that is inconsequential for the agent, then the agent has no incentive to know about it.",2.1. Prediction error as curiosity reward,[0],[0]
"Instead of hand-designing a feature representation for every environment, our aim is to come up with a general mechanism for learning feature representations such that the prediction error in the learned feature space provides a good intrinsic reward signal.",2.2. Self-supervised prediction for exploration,[0],[0]
"We propose that such a feature space can be learned by training a deep neural network with two sub-modules: the first sub-module encodes the raw state (st) into a feature vector φ(st) and the second submodule takes as inputs the feature encoding φ(st), φ(st+1) of two consequent states and predicts the action (at) taken by the agent to move from state st to st+1.",2.2. Self-supervised prediction for exploration,[0],[0]
"Training this neural network amounts to learning function g defined as:
ât = g ( st, st+1; θI )",2.2. Self-supervised prediction for exploration,[0],[0]
"(2)
where, ât is the predicted estimate of the action at and the the neural network parameters θI are trained to optimize,
min θI",2.2. Self-supervised prediction for exploration,[0],[0]
"LI(ât, at) (3)
where, LI is the loss function that measures the discrepancy between the predicted and actual actions.",2.2. Self-supervised prediction for exploration,[0],[0]
"In case at is discrete, the output of g is a soft-max distribution across all possible actions and minimizing LI amounts to maximum likelihood estimation of θI under a multinomial distribution.",2.2. Self-supervised prediction for exploration,[0],[0]
"The learned function g is also known as the inverse dynamics model and the tuple (st, at, st+1) required to learn g is obtained while the agent interacts with the environment using its current policy π(s).
",2.2. Self-supervised prediction for exploration,[0],[0]
"In addition to inverse dynamics model, we train another neural network that takes as inputs at and φ(st) and predicts the feature encoding of the state at time step t+ 1,
φ̂(st+1)",2.2. Self-supervised prediction for exploration,[0],[0]
"= f ( φ(st), at; θF ) (4)
where φ̂(st+1) is the predicted estimate of φ(st+1) and the neural network parameters θF are optimized by minimizing the loss function LF :
LF ( φ(st), φ̂(st+1) )",2.2. Self-supervised prediction for exploration,[0],[0]
"= 1
2 ‖φ̂(st+1)− φ(st+1)‖22 (5)
The learned function f is also known as the forward dynamics model.",2.2. Self-supervised prediction for exploration,[0],[0]
"The intrinsic reward signal rit is computed as,
rit = η
2 ‖φ̂(st+1)− φ(st+1)‖22 (6)
where η > 0 is a scaling factor.",2.2. Self-supervised prediction for exploration,[0],[0]
"In order to generate the curiosity based intrinsic reward signal, we jointly optimize the forward and inverse dynamics loss described in equations 3 and 5 respectively.",2.2. Self-supervised prediction for exploration,[0],[0]
The inverse model learns a feature space that encodes information relevant for predicting the agent’s actions only and the forward model makes predictions in this feature space.,2.2. Self-supervised prediction for exploration,[0],[0]
"We refer to this proposed
curiosity formulation as Intrinsic Curiosity Module (ICM).",2.2. Self-supervised prediction for exploration,[0],[0]
"As there is no incentive for this feature space to encode any environmental features that are not influenced by the agent’s actions, our agent will receive no rewards for reaching environmental states that are inherently unpredictable and its exploration strategy will be robust to the presence of distractor objects, changes in illumination, or other nuisance sources of variation in the environment.",2.2. Self-supervised prediction for exploration,[0],[0]
"See Figure 2 for illustration of the formulation.
",2.2. Self-supervised prediction for exploration,[0],[0]
"The use of inverse models has been investigated to learn features for recognition tasks (Agrawal et al., 2015; Jayaraman & Grauman, 2015).",2.2. Self-supervised prediction for exploration,[0],[0]
Agrawal et al. (2016) constructed a joint inverse-forward model to learn feature representation for the task of pushing objects.,2.2. Self-supervised prediction for exploration,[0],[0]
"However, they only used the forward model as a regularizer for training the inverse model features, while we make use of the error in the forward model predictions as the curiosity reward for training our agent’s policy.
",2.2. Self-supervised prediction for exploration,[0],[0]
"The overall optimization problem that is solved for learning the agent is a composition of equations 1, 3 and 5 and can be written as,
min θP ,θI ,θF
[ − λEπ(st;θP )",2.2. Self-supervised prediction for exploration,[0],[0]
"[Σtrt] + (1− β)LI + βLF ] (7)
where 0 ≤ β ≤ 1 is a scalar that weighs the inverse model loss against the forward model loss and λ > 0 is a scalar that weighs the importance of the policy gradient loss against the importance of learning the intrinsic reward signal.",2.2. Self-supervised prediction for exploration,[0],[0]
"To evaluate our curiosity module on its ability to improve exploration and provide generalization to novel scenarios, we will use two simulated environments.",3. Experimental Setup,[0],[0]
"This section describes the details of the environments and the experimental setup.
",3. Experimental Setup,[0],[0]
"Environments The first environment we evaluate on is the VizDoom (Kempka et al., 2016) game.",3. Experimental Setup,[0],[0]
"We consider the Doom 3-D navigation task where the action space of the agent consists of four discrete actions – move forward, move left, move right and no-action.",3. Experimental Setup,[0],[0]
"Our testing setup in all the experiments is the ‘DoomMyWayHome-v0’ environment which is available as part of OpenAI Gym (Brockman et al., 2016).",3. Experimental Setup,[0],[0]
Episodes are terminated either when the agent finds the vest or if the agent exceeds a maximum of 2100 time steps.,3. Experimental Setup,[0],[0]
The map consists of 9 rooms connected by corridors and the agent is tasked to reach some fixed goal location from its spawning location.,3. Experimental Setup,[0],[0]
The agent is only provided a sparse terminal reward of +1 if it finds the vest and zero otherwise.,3. Experimental Setup,[0],[0]
"For generalization experiments, we pre-train on
a different map with different random textures from (Dosovitskiy & Koltun, 2016) and each episode lasts for 2100 time steps.",3. Experimental Setup,[0],[0]
"Sample frames from VizDoom are shown in Figure 3a, and maps are explained in Figure 4.",3. Experimental Setup,[0],[0]
"It takes approximately 350 steps for an optimal policy to reach the vest location from the farthest room in this map (sparse reward).
",3. Experimental Setup,[0],[0]
"Our second environment is the classic Nintendo game Super Mario Bros (Paquette, 2016).",3. Experimental Setup,[0],[0]
We consider four levels of the game: pre-training on the first level and showing generalization on the subsequent levels.,3. Experimental Setup,[0],[0]
"In this setup, we reparametrize the action space of the agent into 14 unique actions following (Paquette, 2016).",3. Experimental Setup,[0],[0]
"This game is played using a joystick allowing for multiple simultaneous button presses, where the duration of the press affects what action is being taken.",3. Experimental Setup,[0],[0]
"This property makes the game particularly hard, e.g. to make a long jump over tall pipes or wide gaps, the agent needs to predict the same action up to 12 times in a row, introducing long-range dependencies.",3. Experimental Setup,[0],[0]
"All our experiments on Mario are trained using curiosity signal only, without any reward from the game.
",3. Experimental Setup,[0],[0]
Training details,3. Experimental Setup,[0],[0]
"All agents in this work are trained using visual inputs that are pre-processed in manner similar to (Mnih et al., 2016).",3. Experimental Setup,[0],[0]
The input RGB images are converted into gray-scale and re-sized to 42 × 42.,3. Experimental Setup,[0],[0]
"In order to model temporal dependencies, the state representation (st) of the environment is constructed by concatenating the current frame with the three previous frames.",3. Experimental Setup,[0],[0]
"Closely following (Mnih et al., 2015; 2016), we use action repeat of four during training time in VizDoom and action repeat of six in Mario.",3. Experimental Setup,[0],[0]
"However, we sample the policy without any action repeat during inference.",3. Experimental Setup,[0],[0]
"Following the asynchronous training protocol in A3C, all the agents were trained asynchronously with twenty workers using stochastic gradient descent.",3. Experimental Setup,[0],[0]
"We used ADAM optimizer with its parameters not shared across the workers.
A3C architecture The input state st is passed through a sequence of four convolution layers with 32 filters each,
kernel size of 3x3, stride of 2 and padding of 1.",3. Experimental Setup,[0],[0]
"An exponential linear unit (ELU; (Clevert et al., 2015)) is used after each convolution layer.",3. Experimental Setup,[0],[0]
The output of the last convolution layer is fed into a LSTM with 256 units.,3. Experimental Setup,[0],[0]
"Two seperate fully connected layers are used to predict the value function and the action from the LSTM feature representation.
",3. Experimental Setup,[0],[0]
Intrinsic Curiosity Module (ICM) architecture The intrinsic curiosity module consists of the forward and the inverse model.,3. Experimental Setup,[0],[0]
"The inverse model first maps the input state (st) into a feature vector φ(st) using a series of four convolution layers, each with 32 filters, kernel size 3x3, stride of 2 and padding of 1.",3. Experimental Setup,[0],[0]
ELU non-linearity is used after each convolution layer.,3. Experimental Setup,[0],[0]
The dimensionality of φ(st) (i.e. the output of the fourth convolution layer) is 288.,3. Experimental Setup,[0],[0]
"For the inverse model, φ(st) and φ(st+1) are concatenated into a single feature vector and passed as inputs into a fully connected layer of 256 units followed by an output fully connected layer with 4 units to predict one of the four possible actions.",3. Experimental Setup,[0],[0]
The forward model is constructed by concatenating φ(st) with at and passing it into a sequence of two fully connected layers with 256 and 288 units respectively.,3. Experimental Setup,[0],[0]
"The value of β is 0.2, and λ is 0.1.",3. Experimental Setup,[0],[0]
"The Equation (7) is minimized with learning rate of 1e− 3.
",3. Experimental Setup,[0],[0]
"Baseline Methods ‘ICM + A3C’ denotes our full algorithm which combines intrinsic curiosity model with A3C. Across different experiments, we compare our approach with three baselines.",3. Experimental Setup,[0],[0]
First is the vanilla ‘A3C’ algorithm with -greedy exploration.,3. Experimental Setup,[0],[0]
"Second is ‘ICM-pixels + A3C’, which is a variant of our ICM without the inverse model, and has curiosity reward dependent only on the forward model loss in predicting next observation in pixel space.",3. Experimental Setup,[0],[0]
"To design this, we remove the inverse model layers and append
deconvolution layers to the forward model.",3. Experimental Setup,[0],[0]
ICM-pixels is close to ICM in architecture but incapable of learning embedding that is invariant to the uncontrollable part of environment.,3. Experimental Setup,[0],[0]
"Note that ICM-pixels is representative of previous methods which compute information gain by directly using the observation space (Schmidhuber, 2010; Stadie et al., 2015).",3. Experimental Setup,[0],[0]
We show that directly using observation space for computing curiosity is significantly worse than learning an embedding as in ICM.,3. Experimental Setup,[0],[0]
"Finally, we include comparison with state-of-the-art exploration methods based on variational information maximization (VIME) (Houthooft et al., 2016) which is trained with TRPO.",3. Experimental Setup,[0],[0]
"We qualitatively and quantitatively evaluate the performance of the learned policy with and without the proposed intrinsic curiosity signal in two environments, VizDoom and Super Mario Bros. Three broad settings are evaluated: a) sparse extrinsic reward on reaching a goal (Section 4.1); b) exploration with no extrinsic reward (Section 4.2); and c) generalization to novel scenarios (Section 4.3).",4. Experiments,[0],[0]
"In VizDoom generalization is evaluated on a novel map with novel textures, while in Mario it is evaluated on subsequent game levels.",4. Experiments,[0],[0]
We perform extrinsic reward experiments on VizDoom using ‘DoomMyWayHome-v0’ setup described in Section 3.,4.1. Sparse Extrinsic Reward Setting,[0],[0]
The extrinsic reward is sparse and only provided when the agent finds the goal (a vest) located at a fixed location in the map.,4.1. Sparse Extrinsic Reward Setting,[0],[0]
"We systematically varied the difficulty of this goaldirected exploration task by varying the distance between
the initial spawning location of the agent and the location of the goal.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"A larger distance means that the chances of reaching the goal location by random exploration is lower and consequently the reward is said to be sparser.
",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"Varying the degree of reward sparsity: We consider three setups with “dense”, “sparse” and “very-sparse” rewards (see Figure 4b).",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"In these settings, the reward is always terminal and the episode terminates upon reaching goal or after a maximum of 2100 steps.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"In the “dense” reward case, the agent is randomly spawned in any of the 17 possible spawning locations uniformly distributed across the map.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
This is not a hard exploration task because sometimes the agent is randomly initialized close to the goal and therefore by random -greedy exploration it can reach the goal with reasonably high probability.,4.1. Sparse Extrinsic Reward Setting,[0],[0]
"In the “sparse” and “very sparse” reward cases, the agent is always spawned in Room-13 and Room-17 respectively which are 270 and 350 steps away from the goal under an optimal policy.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"A long sequence of directed actions is required to reach the goals from these rooms, making these settings hard goal directed exploration problems.
",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"Results shown in Figure 5 indicate that while the performance of the baseline A3C degrades with sparser rewards, curious A3C agents are superior in all cases.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"In the “dense” reward case, curious agents learn much faster indicating more efficient exploration of the environment as compared to -greedy exploration of the baseline agent.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
One possible explanation of the inferior performance of ICM-pixels in comparison to ICM is that in every episode the agent is spawned in one out of seventeen rooms with different textures.,4.1. Sparse Extrinsic Reward Setting,[0],[0]
"It is hard to learn a pixel-prediction model as the number of textures increases.
",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"In the “sparse” reward case, as expected, the baseline A3C agent fails to solve the task, while the curious A3C agent is able to learn the task quickly.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"Note that ICM-pixels and ICM have similar convergence because, with a fixed spawning location of the agent, the ICM-pixels encounters the same textures at the starting of each episode which makes learning the pixel-prediction model easier as compared to the “dense” reward case.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"Finally, in the “very sparse” reward case, both the A3C agent and ICM-pixels never succeed, while the ICM agent achieves a perfect score in 66% of the random runs.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"This indicates that ICM is better suited than ICM-pixels and vanilla A3C for hard goal directed exploration tasks.
",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"Robustness to uncontrollable dynamics For testing the robustness of the proposed ICM formulation to changes in the environment that do not affect the agent, we augmented the agent’s observation with a fixed region of white noise which made up 40% of the image (see Figure 3b).",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"In VizDoom 3-D navigation, ideally the agent should be unaffected by this noise as the noise does not affect the agent in anyway and is merely a nuisance.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
Figure 6 compares the performance of ICM against some baseline methods on the “sparse” reward setup described above.,4.1. Sparse Extrinsic Reward Setting,[0],[0]
"While, the proposed ICM agent achieves a perfect score, ICM-pixels suffers significantly despite having succeeded at the “sparse reward” task when the inputs were not augmented with any noise (see Figure 5b).",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"This indicates that in contrast to ICM-pixels, ICM is insensitive to nuisance changes in the environment.
",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"Comparison to TRPO-VIME We now compare our curious agent against variational information maximization agent trained with TRPO (Houthooft et al., 2016) for the
VizDoom “sparse” reward setup described above.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
TRPO is in general more sample efficient than A3C but takes a lot more wall-clock time.,4.1. Sparse Extrinsic Reward Setting,[0],[0]
We do not show these results in plots because TRPO and A3C have different setups.,4.1. Sparse Extrinsic Reward Setting,[0],[0]
"The hyperparameters and accuracy for the TRPO and VIME results follow from the concurrent work (Fu et al., 2017).",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"Despite the sample efficiency of TRPO, we see that our ICM agents work significantly better than TRPO and TRPO-VIME, both in terms of convergence rate and accuracy.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"Results are shown in the Table below:
Method Mean (Median) Score (“sparse” reward setup) (at convergence)
TRPO 26.0 % ( 0.0 %) A3C 0.0 % ( 0.0 %) VIME + TRPO 46.1 % ( 27.1 %)
",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"ICM + A3C 100.0 % (100.0 %)
",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"As a sanity check, we replaced the curiosity network with random noise sources and used them as the curiosity reward.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"We performed systematic sweep across different distribution parameters in the “sparse” reward case: uniform, Gaussian and Laplacian.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
We found that none of these agents were able to reach the goal showing that our curiosity module does not learn degenerate solutions.,4.1. Sparse Extrinsic Reward Setting,[0],[0]
A good exploration policy is one which allows the agent to visit as many states as possible even without any goals.,4.2. No Reward Setting,[0],[0]
"In the case of 3-D navigation, we expect a good exploration policy to cover as much of the map as possible; in the case of playing a game, we expect it to visit as many game states as possible.",4.2. No Reward Setting,[0],[0]
"In order to test if our agent can learn a good exploration policy, we trained it on VizDoom and Mario without any rewards from the environment.",4.2. No Reward Setting,[0],[0]
"We then evaluated what portion of the map was explore (for VizDoom), and how much progress it made (for Mario) in this setting.",4.2. No Reward Setting,[0],[0]
"To our surprise, we have found that in both cases, the noreward agent was able to perform quote well (see video at http://pathak22.github.io/noreward_rl/).
",4.2. No Reward Setting,[0],[0]
VizDoom: Coverage during Exploration.,4.2. No Reward Setting,[0],[0]
"An agent trained with no extrinsic rewards was able to learn to navigate corridors, walk between rooms and explore many rooms in the 3-D Doom environment.",4.2. No Reward Setting,[0],[0]
On many occasions the agent traversed the entire map and reached rooms that were farthest away from the room it was initialized in.,4.2. No Reward Setting,[0],[0]
"Given that the episode terminates in 2100 steps and farthest rooms are over 250 steps away (for an optimally-moving agent), this result is quite remarkable, demonstrating that it is possible to learn useful skills without the requirement of any external supervision of rewards.",4.2. No Reward Setting,[0],[0]
Example explorations are shown in Figure 7.,4.2. No Reward Setting,[0],[0]
"The first 3 maps show our agent ex-
plore a much larger state space without any extrinsic signal, compared to a random exploration agent (last two maps), which often has hard time getting around local minima of state spaces, e.g. getting stuck against a wall and not able to move (see video).
",4.2. No Reward Setting,[0],[0]
Mario: Learning to play with no rewards.,4.2. No Reward Setting,[0],[0]
We train our agent in the Super Mario World using only curiosity based signal.,4.2. No Reward Setting,[0],[0]
"Without any extrinsic reward from environment, our Mario agent can learn to cross over 30% of Level-1.",4.2. No Reward Setting,[0],[0]
"The agent received no reward for killing or dodging enemies or avoiding fatal events, yet it automatically discovered these behaviors (see video).",4.2. No Reward Setting,[0],[0]
"One possible reason is because getting killed by the enemy will result in only seeing a small part of the game space, making its curiosity saturate.",4.2. No Reward Setting,[0],[0]
"In order to remain curious, it is in the agent’s interest to learn how to kill and dodge enemies so that it can reach new parts of the game space.",4.2. No Reward Setting,[0],[0]
"This suggests that curiosity provides indirect supervision for learning interesting behaviors.
",4.2. No Reward Setting,[0],[0]
"To the best of our knowledge, this is the first demonstration where the agent learns to navigate in a 3D environment and discovers how to play a game by making use of relatively complex visual imagery directly from pixels, without any extrinsic rewards.",4.2. No Reward Setting,[0],[0]
"There are several prior works that use reinforcement learning to navigate in 3D environments from pixel inputs or playing ATARI games such as (Mirowski et al., 2017; Mnih et al., 2015; 2016), but they rely on intermediate external rewards provided by the environment.",4.2. No Reward Setting,[0],[0]
In the previous section we showed that our agent learns to explore large parts of the space where its curiosity-driven exploration policy was trained.,4.3. Generalization to Novel Scenarios,[0],[0]
"However, it remains unclear whether the agent has done this by learning “generalized skills” for efficiently exploring its environment, or if it simply memorized the training set.",4.3. Generalization to Novel Scenarios,[0],[0]
"In other words we would like to know, when exploring a space, how much of the learned behavior is specific to that particular space and how much is general enough to be useful in novel scenar-
ios?",4.3. Generalization to Novel Scenarios,[0],[0]
"To investigate this question, we train a no reward exploratory behavior in one scenario (e.g. Level-1 of Mario) and then evaluate the resulting exploration policy in three different ways: a) apply the learned policy “as is” to a new scenario; b) adapt the policy by fine-tuning with curiosity reward only; c) adapt the policy to maximize some extrinsic reward.",4.3. Generalization to Novel Scenarios,[0],[0]
"Happily, in all three cases, we observe some promising generalization results:
Evaluate “as is”: We evaluate the policy trained by maximizing curiosity on Level-1 of Mario on subsequent levels without adapting the learned policy in any way.",4.3. Generalization to Novel Scenarios,[0],[0]
"We measure the distance covered by the agent as a result of executing this policy on Levels 1, 2, and 3, as shown in Table 1.",4.3. Generalization to Novel Scenarios,[0],[0]
"We note that the policy performs surprisingly well on Level 3, suggesting good generalization, despite the fact that Level-3 has different structures and enemies compared to Level-1.",4.3. Generalization to Novel Scenarios,[0],[0]
"However, note that the running “as is” on Level2 does not do well.",4.3. Generalization to Novel Scenarios,[0],[0]
"At first, this seems to contradict the generalization results on Level-3.",4.3. Generalization to Novel Scenarios,[0],[0]
"However, note that Level-3 has similar global visual appearance (day world with sunlight) to Level-1, whereas Level-2 is significantly different (night world).",4.3. Generalization to Novel Scenarios,[0],[0]
"If this is indeed the issue, then it should be possible to quickly adapt the exploration policy to Level-2 with a little bit of “fine-tuning”.",4.3. Generalization to Novel Scenarios,[0],[0]
"We will explore this below.
",4.3. Generalization to Novel Scenarios,[0],[0]
Fine-tuning with curiosity only: From Table 1 we see that when the agent pre-trained (using only curiosity as reward) on Level-1 is fine-tuned (using only curiosity as reward) on Level-2 it quickly overcomes the mismatch in global visual appearance and achieves a higher score than training from scratch with the same number of iterations.,4.3. Generalization to Novel Scenarios,[0],[0]
"Interestingly, training “from scratch” on Level-2 is worse than the fine-tuned policy, even when training for more iterations than pre-training + fine-tuning combined.",4.3. Generalization to Novel Scenarios,[0],[0]
"One possible reason is that Level-2 is more difficult than Level1, so learning the basic skills such as moving, jumping, and killing enemies from scratch is much more dangerous than in the relative “safety” of Level-1.",4.3. Generalization to Novel Scenarios,[0],[0]
"This result, therefore might suggest that first pre-training on an earlier level
and then fine-tuning on a later one produces a form of curriculum which aids learning and generalization.",4.3. Generalization to Novel Scenarios,[0],[0]
"In other words, the agent is able to use the knowledge it acquired by playing Level-1 to better explore the subsequent levels.",4.3. Generalization to Novel Scenarios,[0],[0]
"Of course, the game designers do this on purpose to allow the human players to gradually learn to play the game.
",4.3. Generalization to Novel Scenarios,[0],[0]
"However, interestingly, fine-tuning the exploration policy pre-trained on Level-1 to Level-3 deteriorates the performance, compared to running “as is”.",4.3. Generalization to Novel Scenarios,[0],[0]
This is because Level3 is very hard for the agent to cross beyond a certain point – the agent hits a curiosity blockade and is unable to make any progress.,4.3. Generalization to Novel Scenarios,[0],[0]
"As the agent has already learned about parts of the environment before the hard point, it receives almost no curiosity reward and as a result it attempts to update its policy with almost zero intrinsic rewards and the policy slowly degenerates.",4.3. Generalization to Novel Scenarios,[0],[0]
"This behavior is vaguely analogous to boredom, where if the agent is unable to make progress it gets bored and stops exploring.
",4.3. Generalization to Novel Scenarios,[0],[0]
"Fine-tuning with extrinsic rewards: If it is the case that the agent has actually learned useful exploratory behavior, then it should be able to learn quicker than starting from scratch even when external rewards are provided by environment.",4.3. Generalization to Novel Scenarios,[0],[0]
We perform this evaluation on VizDoom where we pre-train the agent using curiosity reward on a map showed in Figure 4a.,4.3. Generalization to Novel Scenarios,[0],[0]
"We then test on the “very sparse” reward setting of ‘DoomMyWayHome-v0’ environment which uses a different map with novel textures (see Figure 4b) as described earlier in Section 4.1.
",4.3. Generalization to Novel Scenarios,[0],[0]
Results in Figure 8 show that the ICM agent pre-trained only with curiosity and then fine-tuned with external reward learns faster and achieves higher reward than an ICM agent trained from scratch to jointly maximize curiosity and the external rewards.,4.3. Generalization to Novel Scenarios,[0],[0]
This result confirms that the learned exploratory behavior is also useful when the agent is required to achieve goals specified by the environment.,4.3. Generalization to Novel Scenarios,[0],[0]
It is also worth noting that ICM-pixels does not generalize to this test environment.,4.3. Generalization to Novel Scenarios,[0],[0]
This indicates that the proposed mechanism of measuring curiosity is significantly better for learning skills that generalize as compared to measuring curiosity in the raw sensory space.,4.3. Generalization to Novel Scenarios,[0],[0]
"Curiosity-driven exploration is a well studied topic in the reinforcement learning literature and a good summary can be found in (Oudeyer & Kaplan, 2009; Oudeyer et al., 2007).",5. Related Work,[0],[0]
Schmidhuber (1991; 2010) and Sun et al. (2011) use surprise and compression progress as intrinsic rewards.,5. Related Work,[0],[0]
Classic work of Kearns et al. (1999) and Brafman et al. (2002) propose exploration algorithms polynomial in the number of state space parameters.,5. Related Work,[0],[0]
"Others have used empowerment, which is the information gain based on entropy of actions, as intrinsic rewards (Klyubin et al., 2005; Mohamed & Rezende, 2015).",5. Related Work,[0],[0]
Stadie et al. (2015) use prediction error in the feature space of an auto-encoder as a measure of interesting states to explore.,5. Related Work,[0],[0]
"State visitation counts have also been investigated for exploration (Bellemare et al., 2016; Oh et al., 2015; Tang et al., 2016).",5. Related Work,[0],[0]
"Osband et al. (2016) train multiple value functions and makes
use of bootstrapping and Thompson sampling for exploration.",5. Related Work,[0],[0]
"Many approaches measure information gain for exploration (Little & Sommer, 2014; Still & Precup, 2012; Storck et al., 1995).",5. Related Work,[0],[0]
Houthooft et al. (2016) use an exploration strategy that maximizes information gain about the agent’s belief of the environment’s dynamics.,5. Related Work,[0],[0]
"Our approach of jointly training forward and inverse models for learning a feature space has similarities to (Agrawal et al., 2016; Jordan & Rumelhart, 1992; Wolpert et al., 1995), but these works use the learned models of dynamics for planning a sequence of actions instead of exploration.",5. Related Work,[0],[0]
"The idea of using a proxy task to learn a semantic feature embedding has been used in a number of works on self-supervised learning in computer vision (Agrawal et al., 2015; Doersch et al., 2015; Goroshin et al., 2015; Jayaraman & Grauman, 2015; Pathak et al., 2016; Wang & Gupta, 2015).
",5. Related Work,[0],[0]
Concurrent work: A number of interesting related papers have appeared on Arxiv while the present work was in submission.,5. Related Work,[0],[0]
Sukhbaatar et al. (2017) generates supervision for pre-training via asymmetric self-play between two agents to improve data efficiency during fine-tuning.,5. Related Work,[0],[0]
"Several methods propose improving data efficiency of RL algorithms using self-supervised prediction based auxiliary tasks (Jaderberg et al., 2017; Shelhamer et al., 2017).",5. Related Work,[0],[0]
"Fu et al. (2017) learn discriminative models, and Gregor et al. (2017) use empowerment based measure to tackle exploration in sparse reward setups.",5. Related Work,[0],[0]
"In this work we propose a mechanism for generating curiosity-driven intrinsic reward signal that scales to high dimensional visual inputs, bypasses the difficult problem of predicting pixels and ensures that the exploration strategy of the agent is unaffected by nuisance factors in the environment.",6. Discussion,[0],[0]
"We demonstrate that our agent significantly outperforms the baseline A3C with no curiosity, a recently proposed VIME (Houthooft et al., 2016) formulation for exploration, and a baseline pixel-predicting formulation.
",6. Discussion,[0],[0]
In VizDoom our agent learns the exploration behavior of moving along corridors and across rooms without any rewards from the environment.,6. Discussion,[0],[0]
In Mario our agent crosses more than 30% of Level-1 without any rewards from the game.,6. Discussion,[0],[0]
One reason why our agent is unable to go beyond this limit is the presence of a pit at 38% of the game that requires a very specific sequence of 15-20 key presses in order to jump across it.,6. Discussion,[0],[0]
"If the agent is unable to execute this sequence, it falls in the pit and dies, receiving no further rewards from the environment.",6. Discussion,[0],[0]
Therefore it receives no gradient information indicating that there is a world beyond the pit that could potentially be explored.,6. Discussion,[0],[0]
"This issue is somewhat orthogonal to developing models of curiosity, but presents a challenging problem for policy learning.
",6. Discussion,[0],[0]
It is common practice to evaluate reinforcement learning approaches in the same environment that was used for training.,6. Discussion,[0],[0]
"However, we feel that it is also important to evaluate on a separate “testing set” as well.",6. Discussion,[0],[0]
"This allows us to gauge how much of what has been learned is specific to the training environment (i.e. memorized), and how much might constitute “generalizable skills” that could be applied to new settings.",6. Discussion,[0],[0]
"In this paper, we evaluate generalization in two ways: 1) by applying the learned policy to a new scenario “as is” (no further learning), and 2) by finetuning the learned policy on a new scenario (we borrow the pre-training/fine-tuning nomenclature from the deep feature learning literature).",6. Discussion,[0],[0]
We believe that evaluating generalization is a valuable tool and will allow the community to better understand the performance of various reinforcement learning algorithms.,6. Discussion,[0],[0]
"To further aid in this effort, we will make the code for our algorithm, as well as testing and environment setups freely available online.
",6. Discussion,[0],[0]
"An interesting direction of future research is to use the learned exploration behavior/skill as a motor primitive/lowlevel policy in a more complex, hierarchical system.",6. Discussion,[0],[0]
"For example, our VizDoom agent learns to walk along corridors instead of bumping into walls.",6. Discussion,[0],[0]
"This could be a useful primitive for a navigation system.
",6. Discussion,[0],[0]
"While the rich and diverse real world provides ample opportunities for interaction, reward signals are sparse.",6. Discussion,[0],[0]
Our approach excels in this setting and converts unexpected interactions that affect the agent into intrinsic rewards.,6. Discussion,[0],[0]
However our approach does not directly extend to the scenarios where “opportunities for interactions” are also rare.,6. Discussion,[0],[0]
"In theory, one could save such events in a replay memory and use them to guide exploration.",6. Discussion,[0],[0]
"However, we leave this extension for future work.
",6. Discussion,[0],[0]
"Acknowledgements: We would like to thank Sergey Levine, Evan Shelhamer, Saurabh Gupta, Phillip Isola and other members of the BAIR lab for fruitful discussions and comments.",6. Discussion,[0],[0]
We thank Jacob Huh for help with Figure 2 and Alexey Dosovitskiy for VizDoom maps.,6. Discussion,[0],[0]
"This work was supported in part by NSF IIS-1212798, IIS-1427425, IIS1536003, IIS-1633310, ONR MURI N00014-14-1-0671, Berkeley DeepDrive, equipment grant from Nvidia, and the Valrhona Reinforcement Learning Fellowship.",6. Discussion,[0],[0]
"In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether.",abstractText,[0],[0]
"In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life.",abstractText,[0],[0]
We formulate curiosity as the error in an agent’s ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model.,abstractText,[0],[0]
"Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent.",abstractText,[0],[0]
"The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.",abstractText,[0],[0]
Curiosity-driven Exploration by Self-supervised Prediction,title,[0],[0]
"Biological organisms can learn to perform tasks (and often do) by observing a sequence of labeled events, just like supervised machine learning.",1. Introduction,[0],[0]
"But unlike machine learning, in human learning supervision is often accompanied by a curriculum.",1. Introduction,[0],[0]
Thus the order of presented examples is rarely random when a human teacher teaches another human.,1. Introduction,[0],[0]
"Likewise, the task may be divided by the teacher into smaller sub-tasks, a process sometimes called shaping (Krueger & Dayan, 2009) and typically studied in the context of rein-
1School of Computer Science and Engineering, The Hebrew University of Jerusalem, Jerusalem 91904, Israel.",1. Introduction,[0],[0]
"Correspondence to: Daphna Weinshall <daphna@mail.huji.ac.il>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"forcement learning (e.g. Graves et al., 2017).",1. Introduction,[0],[0]
"Although it remained for the most part in the fringes of machine learning research, curriculum learning has been identified as a key challenge for machine learning throughout (e.g., Mitchell, 1980; 2006; Wang & Cottrell, 2015).
",1. Introduction,[0],[0]
"We focus here on curriculum learning based on ranking (or weighting as in (Bengio et al., 2009)) of the training examples, which is used to guide the order of presentation of examples to the learner.",1. Introduction,[0],[0]
"Risking over simplification, the idea is to first present the learner primarily with examples of higher weight or rank, later to be followed by examples with lower weight or rank.",1. Introduction,[0],[0]
"Ranking may be based on the difficulty of each training example as evaluated by the teacher, from easiest to the most difficult.
",1. Introduction,[0],[0]
"In Section 2 we investigate this strict definition of curriculum learning theoretically, in the context of stochastic gradient descent used to optimize the convex linear regression loss function.",1. Introduction,[0],[0]
We first define the (ideal) difficulty of a training point as its loss with respect to the optimal classifier.,1. Introduction,[0],[0]
"We then prove that curriculum learning, when given the ranking of training points by their difficulty thus defined, is expected (probabilistically) to significantly speed up learning especially at the beginning of training.",1. Introduction,[0],[0]
"This theoretical result is supported by empirical evidence obtained in the deep learning scenario of curriculum learning described in Section 3, where similar behavior is observed.",1. Introduction,[0],[0]
"We also show that when the difficulty of the sampled training points is fixed, convergence is faster when sampling points that incur higher loss with respect to the current hypothesis as suggested in (Shrivastava et al., 2016).",1. Introduction,[0],[0]
"This result is not always true when the difficulty of the sampled training points is not fixed.
",1. Introduction,[0],[0]
But such ideal ranking is rarely available.,1. Introduction,[0],[0]
"In fact, the need for such supervision has rendered curriculum learning less useful in machine learning, since ranking by difficulty is hard to obtain.",1. Introduction,[0],[0]
"Moreover, even when it is provided by a human teacher, it may not reflect the true difficulty as it affects the machine learner.",1. Introduction,[0],[0]
"For example, in visual object recognition it has been demonstrated that what makes an image difficult to a neural network classifier may not always match whatever makes it difficult to a human observer, an observation that has been taken advantage of in the recent
work on adversarial examples (Szegedy et al., 2013).",1. Introduction,[0],[0]
"Possibly, this is one of the reasons why curriculum learning is rarely used in practice (but see, e.g., Zaremba & Sutskever, 2014; Amodei et al., 2016; Jesson et al., 2017).
",1. Introduction,[0],[0]
In the second part of this paper we focus on this question - how to rank (or weight) the training examples without the aid of a human teacher.,1. Introduction,[0],[0]
"This is paramount when a human teacher cannot provide a reliable difficulty score for the task at hand, or when obtaining such a score by human teachers is too costly.",1. Introduction,[0],[0]
This question is also closely related to transfer learning: here we investigate the use of another classifier to provide the ranking of the training examples by their presumed difficulty.,1. Introduction,[0],[0]
"This form of transfer should not be confused with the notion of transfer discussed in (Bengio et al., 2009) in the context of multi-task and lifelong learning (Thrun & Pratt, 2012), where knowledge is transferred from earlier tasks (e.g. the discrimination of easy examples) to later tasks (e.g. the discrimination of difficult examples).",1. Introduction,[0],[0]
"Rather, we investigate the transfer of knowledge from one classifier to another, as in teacher classifier to student classifier.",1. Introduction,[0],[0]
"In this form curriculum learning has not been studied in the context of deep learning, and hardly ever in the context of other classification paradigms.
",1. Introduction,[0],[0]
"Differently from previous work, it is not the instance representation which is being transferred but rather the ranking of training examples.",1. Introduction,[0],[0]
Why is this a good idea?,1. Introduction,[0],[0]
"This kind of transfer assumes that a powerful pre-trained network is only available at train time, and cannot be used at test time even for the computation of a test point’s representation.",1. Introduction,[0],[0]
"This may be the case, for example, when the powerful network is too big to run on the target device.",1. Introduction,[0],[0]
"One can no longer expect to have access to the transferred representation at test time, while ranking can be used at train time in order to improve the learning of the target smaller network (see related discussion of network compression in (Chen et al., 2015; Kim et al., 2015), for example).
",1. Introduction,[0],[0]
"In Section 3 we describe our method, an algorithm which uses the ranking to construct a schedule for the order of presentation of training examples.",1. Introduction,[0],[0]
"In subsequent empirical evaluations we compare the performance of the method when using a curriculum which is based on different scheduling options, including 2 control conditions where difficult examples are presented first or when using arbitrary scheduling.",1. Introduction,[0],[0]
"The main results of this empirical study can be summarized as follows: (i) Learning rate is always faster with curriculum learning, especially at the beginning of training.",1. Introduction,[0],[0]
"(ii) Final generalization is sometimes improved with curriculum learning, especially when the conditions for learning are hard: the task is difficult, the network is small, or when strong regularization is enforced.",1. Introduction,[0],[0]
"These results are consistent with prior art (see e.g. Bengio et al., 2009).",1. Introduction,[0],[0]
"We start with some notations in Section 2.1, followed in Sections 2.2 by the rigorous analysis of curriculum learning when used to optimize the linear regression loss.",2. Theoretical analysis,[0],[0]
"In Section 2.3 we report supporting empirical evidence for the main theoretical results, obtained using the deep learning setup described later in Section 3.",2. Theoretical analysis,[0],[0]
"Let X = {(xi, yi)}ni=1 denote the training data, where xi ∈ Rd denotes the i-th data point and yi its corresponding label.",2.1. Notations and definitions,[0],[0]
"In general, the goal is to find a hypothesis h̄(x) ∈ H that minimizes the risk function (the expected loss).",2.1. Notations and definitions,[0],[0]
"In order to minimize this objective, Stochastic Gradient Descent (SGD) is often used with various extensions and regularization.
",2.1. Notations and definitions,[0],[0]
"We start with two definitions:
Definition 1 (Ideal Difficulty Score).",2.1. Notations and definitions,[0],[0]
"The difficulty of point x is measured by its minimal loss with respect to the set of optimal hypotheses {L(h̄(xi), yi)}.",2.1. Notations and definitions,[0],[0]
Definition 2 (Stochastic Curriculum Learning).,2.1. Notations and definitions,[0],[0]
"SCL is a variation on Stochastic Gradient Descent (SGD), where the learner is exposed to the data gradually based on the difficulty score of the training points.
",2.1. Notations and definitions,[0],[0]
"In vanilla SGD training, at each iteration the learner is presented with a new datapoint (or mini-batch) sampled from the training data based on some probability function D(X).",2.1. Notations and definitions,[0],[0]
"In SCL, the sampling is biased to favor easier examples at the beginning of the training.",2.1. Notations and definitions,[0],[0]
This bias is decreased following some scheduling procedure.,2.1. Notations and definitions,[0],[0]
"At the end of training, points are sampled according to D(X) as in vanilla SGD.
",2.1. Notations and definitions,[0],[0]
"In practice, an SCL algorithm should solve two problems: (i) Score the training points by difficulty; in prior art this score was typically provided by the teacher in a supervised manner.",2.1. Notations and definitions,[0],[0]
(ii) Define the scheduling procedure.,2.1. Notations and definitions,[0],[0]
Here we analyze SCL when used to minimize the linear regression model.,2.2. The linear regression loss,[0],[0]
"Specifically, we investigate the differential effect of a point’s Difficulty Score on convergence towards the global minimum of the expected least squares loss, when the family of hypotheses H includes the linear functions h(x) = atx + b and y ∈ R.
",2.2. The linear regression loss,[0],[0]
"The risk function of the regression model is the following
R(X,w) = ED(X)L(hw(x), y) L(hw(xi), yi) =",2.2. The linear regression loss,[0],[0]
"(h(xi)− yi)2 = (atxi + b− yi)2
, (xtiw",2.2. The linear regression loss,[0],[0]
"− yi)2 , L(Xi,w) (1)
",2.2. The linear regression loss,[0],[0]
"In the last transition above, w =",2.2. The linear regression loss,[0],[0]
"[
a b
] ∈ Rd+1.",2.2. The linear regression loss,[0],[0]
"With some
abuse of notation, xi denotes the vector [ xi 1 ] .",2.2. The linear regression loss,[0],[0]
Xi denotes the vector,2.2. The linear regression loss,[0],[0]
"[xi, yi], with Difficulty Score L(Xi, w̄).
",2.2. The linear regression loss,[0],[0]
"In general the output hypothesis hw(x) = xtiw is determined by minimizing R(X,w) with respect to w. The global minimum w̄ of the empirical loss can be computed in closed form from the training data.",2.2. The linear regression loss,[0],[0]
"However, gradient descent can be used to find w̄ with guaranteed convergence, which is efficient when n is very large.
",2.2. The linear regression loss,[0],[0]
Recall that SCL computes a sequence of estimators {wt}Tt=1 for the parameters of the optimal hypothesis w̄.,2.2. The linear regression loss,[0],[0]
"This is based on a sequence of training points {Xt = [xt, yt]}Tt=1, sampled from the training data while favoring easy points at the beginning of training.",2.2. The linear regression loss,[0],[0]
"Other than sampling probability, the update step at time t follows SGD:
wt+1 =",2.2. The linear regression loss,[0],[0]
"wt − η ∂L(Xt,w)
∂w",2.2. The linear regression loss,[0],[0]
|w,2.2. The linear regression loss,[0],[0]
=wt (2),2.2. The linear regression loss,[0],[0]
The main theorem in this sub-section states that the expected rate of convergence of gradient descent is monotonically decreasing with the Difficulty Score of the sample,CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
Xt.,CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
We prove it below for the gradient step as defined in (2).,CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
"If the size of the gradient step is fixed at η, a somewhat stronger theorem can be obtained where the constraint on the step size being small is not required.
",CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
"We first derive the gradient step at time t:
s = −η ∂L(Xi,w) ∂w",CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
= −2η(xtiw,CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
"− yi)xi (3)
",CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
"Let Ωi denote the hyperplane on which this gradient vanishes ∂L(Xi,w)∂w = 0.",CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
"This hyperplane is defined by x t iw = yi, namely, xi defines its normal direction.",CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
Thus (3) implies that the gradient step at time t is perpendicular to Ωi.,CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
Let z̄ denote the projection of w̄ on Ωi.,CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
"Let Ψ2 = L(Xi, w̄) denote the Difficulty Score of Xi.
",CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
Lemma 1.,CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
Fix the training point Xi.,CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
The Difficulty Score of Xi is Ψ2 = r2‖w̄,CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
− z̄‖2.,CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
"Ψ2 = L(Xi, w̄) = L(Xi, z̄ + (w̄ − z̄))",Proof.,[0],[0]
=,Proof.,[0],[0]
[xtiz̄ + x t i(w̄,Proof.,[0],[0]
"− z̄)− yi]2
=",Proof.,[0],[0]
"[xti(w̄ − z̄)]2 = ‖xi‖2‖w̄ − z̄‖2 (4)
",Proof.,[0],[0]
"Recall that xi,w ∈ Rd+1.",Proof.,[0],[0]
"We continue the analysis in the parameter space w ∈ Rd+1, where parameter vector w corresponds to a point, and data vector xi describes a hyperplane.",Proof.,[0],[0]
"In this space we represent each vector
xi in a hyperspherical coordinate system [r, ϑ,Φ], with pole (origin) fixed at w̄ and polar axis (zenith direction) ~O = w̄",Proof.,[0],[0]
− wt (see Fig. 1).,Proof.,[0],[0]
"r denotes the vector’s length, while 0 ≤ ϑ ≤ π denotes the polar angle with respect to ~O.",Proof.,[0],[0]
Let Φ = [ϕ1 . . .,Proof.,[0],[0]
", ϕd−1] denote the remaining polar angles.
",Proof.,[0],[0]
"To illustrate, Fig. 1 shows a planar section of the parameter space, the 2D plane formed by the two intersecting lines ~O and z̄− w̄.",Proof.,[0],[0]
The gradient step s points from wt towards Ωi.,Proof.,[0],[0]
"Ωi is perpendicular to xi, which is parallel to z̄− w̄ and to s, and therefore Ωi is projected onto a line in this plane.",Proof.,[0],[0]
"We introduce the notation λ = ‖w̄ −wt‖.
Let sO denote the projection of the gradient vector s on the polar axis ~O, and let s⊥ denote the perpendicular component.",Proof.,[0],[0]
"From (3) and the definition of Ψ
s = −2ηxi(xtiwt",Proof.,[0],[0]
− yi) =,Proof.,[0],[0]
−2ηxi[xti(wt − w̄)±Ψ] sO = s · w̄,Proof.,[0],[0]
"−wt
λ = 2
η λ",Proof.,[0],[0]
[r2λ2 cos2 ϑ∓Ψrλ,Proof.,[0],[0]
"cosϑ]
(5)
",Proof.,[0],[0]
"Let x = (r, ϑ,Φ).",Proof.,[0],[0]
"Let fD(X) = f(r, ϑ,Φ)fY (|y − xtw̄|) denote the density function of the data X.",Proof.,[0],[0]
"This choice assumes that the density of the label y only depends on the absolute error |y − xtw̄|.
",Proof.,[0],[0]
"For the subsequent derivations we need the conditional distribution of the data X given difficulty score Ψ. Fixing the difficulty score determines one of two labels y(x,Ψ) = xtw̄",Proof.,[0],[0]
± Ψ.,Proof.,[0],[0]
"We further assume that both labels are equally likely1, and therefore fD(X)/Ψ ([x, y]) = 12f(r, ϑ,Φ).
",Proof.,[0],[0]
"Let ∆(Ψ) denote the expected convergence rate at time t, given fixed difficulty score Ψ.
∆(Ψ) =",Proof.,[0],[0]
E[‖wt − w̄‖2 − ‖wt+1,Proof.,[0],[0]
"− w̄‖2/Ψ] (6)
Lemma 2.
∆(Ψ) = 2λE[sO/Ψ]− E[s 2/Ψ] (7) 1This assumption can be somewhat relaxed, but the strict form is used to simplify the exposition.
",Proof.,[0],[0]
Proof.,Proof.,[0],[0]
"From (6)
E[∆] = (−λ)2 − E[(−λ+ sO)2 + s2⊥] = λ2 − (λ2 − 2λE[sO] + E[s2O])− E[s2⊥] = 2λE[sO]− E[s2]
From Lemma 2 and (5)2
1 4 ∆(Ψ) = ηE[r2λ2 cos2 ϑ]− η2E[r4λ2 cos2 ϑ]
−η2Ψ2E[r2] (8) −ηE[(±Ψ)rλ cosϑ]− 2η2E[(±Ψ)r3λ cosϑ]
Lemma 3.
E[(±Ψ)rλ cosϑ] = E[(±Ψ)r3λ cosϑ] = 0
Proof.",Proof.,[0],[0]
"The lemma follows from the assumed symmetry of D(X) with respect to the sign of yi − xtiw̄.
",Proof.,[0],[0]
"It follows from Lemma 3 that 1
4 ∆(Ψ) =ηE[r2λ2 cos2 ϑ]− η2E[r4λ2 cos2 ϑ]
− η2Ψ2E[r2] (9)
We can now state the main theorem of this section.",Proof.,[0],[0]
Theorem 1.,Proof.,[0],[0]
At time t the expected convergence rate for training point x is monotonically decreasing with the Difficulty Score Ψ of x.,Proof.,[0],[0]
If the step size coefficient is sufficiently small so that η ≤,Proof.,[0],[0]
"E[r
2 cos2 ϑ] E[r4 cos2 ϑ] , it is likewise monotonically
increasing with the distance λ between the current estimate of the hypothesis wt and the optimal hypothesis w̄.
Proof.",Proof.,[0],[0]
"From (9)
∂∆(Ψ)
∂Ψ = −8η2E[r2]Ψ ≤ 0
which proves the first statement.",Proof.,[0],[0]
"In addition,
∂∆(Ψ)
∂λ = 8ηλ
( E[r2 cos2 ϑ]− ηE[r4 cos2 ϑ] )",Proof.,[0],[0]
"If η ≤ E[r
2 cos2 ϑ] E[r4 cos2 ϑ] then ∂∆(Ψ) ∂λ ≥ 0, and the second statement
follows.
",Proof.,[0],[0]
Corollary 1.,Proof.,[0],[0]
"Although E[∆(Ψ)] may be negative, wt always converges faster to w̄ when the training points are sampled from easier examples with smaller Ψ. Corollary 2.",Proof.,[0],[0]
If the step size coefficient η is small enough so that η ≤,Proof.,[0],[0]
"E[r
2 cos2 ϑ] E[r4 cos2 ϑ] , we should expect faster convergence
at the beginning of SCL.",Proof.,[0],[0]
"2Below the short-hand notation E[(±Ψ)] implies that the 2 cases of y(x,Ψ) = xtw̄",Proof.,[0],[0]
"± Ψ should be considered, with equal probability 1
2 by assumption.",Proof.,[0],[0]
"The main theorem in this sub-section states that for a fixed difficulty score Ψ, when the gradient step is small enough, convergence is monotonically increasing with the loss of the point with respect to the current hypothesis.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
This is not true in general.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"The second theorem in this section shows that when the difficulty score is not fixed, there exist hypotheses w ∈ H for which the convergence rate is decreasing with the current loss.
",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"Let Υ2 = L(Xi,wt) denote the loss of Xi with respect to the current hypothesis wt.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
Define the angle β ∈,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"[0, π2 ) as follows (see Fig. 1)
β = β(r,Ψ, λ) = arccos(min",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"( Ψ
λr , 1))",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"(10)
Lemma 4.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"The relation between Υ,Ψ, r, ϑ can be written separately in 4 regions as follows (see Fig. 1):
A1 0 ≤ ϑ ≤",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"π−β, yi = xtiw̄+ Ψ =⇒ yi",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"= xtiwt+ Υ, λr cosϑ = xti(w̄ −wt) = −Ψ",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"+ Υ
A2 π−β ≤ ϑ ≤ π, yi = xtiw̄+Ψ =⇒ yi = xtiwt−Υ, λr cosϑ = −Ψ−Υ
A3 0 ≤ ϑ ≤ β, yi",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
= xtiw̄ −Ψ =⇒ yi = xtiwt,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"+ Υ, λr cosϑ = Ψ + Υ
A4 β ≤ ϑ ≤ π,",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"yi = xtiw̄ −Ψ =⇒ yi = xtiwt −Υ, λr cosϑ = Ψ−Υ
Proof.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"We keep in mind that ∀xi and Ψ, there are 2 possible yi with equal probability.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
Recall that z̄ denotes the projection of w̄ on Ωi.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"In the planar section shown in Fig. 1,
z̄ lies in the upper half space ⇐⇒ yi =",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"xtiw̄ + Ψ
z̄ lies in the lower half space ⇐⇒ yi =",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"xtiw̄ −Ψ
",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"This follows from 3 observations: x̄i lies in the upper half space by the definition of the polar coordinate system, xtiw̄",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"− yi = ±Ψ, and
0 = xtiz̄− yi = xti(z̄− w̄) +",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
xtiw̄,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"− yi
Next, let zt denote the projection of wt on Ωi.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"Then
0 = xtizt",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
− yi = xti(zt −wt) +,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
xtiwt,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"− yi
When z̄ lies in the upper half space, the following can be verified geometrically from Fig. 1:
0 ≤ ϑ ≤ π−β ⇒ xti(zt−wt)",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
≥ 0 ⇒ yi = xtiwt,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
+,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"Υ
π−β ≤ ϑ ≤ π ⇒",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"xti(zt−wt) ≤ 0 ⇒ yi = xtiwt−Υ
When z̄ lies in the lower half space
0 ≤ ϑ ≤ β =⇒ xti(zt −wt) ≥ 0",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"=⇒ yi = xtiwt + Υ
β ≤ ϑ ≤ π =⇒ xti(zt −wt) ≤ 0",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"=⇒ yi = xtiwt −Υ
Next we analyze how the convergence rate at xi changes with Υ. Let ∆(Ψ,Υ) denote the expected convergence rate at time t, given fixed difficulty score Ψ and fixed loss Υ. From (9) ∆(Ψ,Υ)",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"= 4ηE[r2λ2 cos2 ϑ/Υ] +O(η 2).
",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"It is easier to analyze ∆(Ψ,Υ) when using the Cartesian coordinates, rather than polar, in the 2D plane defined by the vectors ~O = w̄−wt and z̄− w̄",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"(see Fig. 1); thus we define u = r cosϑ, v = r sinϑ.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"The 4 cases listed in Lemma 4 can be readily transformed to this coordinate system as follows {0 ≤ ϑ ≤ β} ⇔ {λu ≥ Ψ}, {β ≤ ϑ ≤ π",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"− β} ⇔ {−Ψ ≤ λu ≤ Ψ}, and {π − β ≤ ϑ ≤ π} ⇔ {λu ≤ −Ψ}:
A1 λu ≥ −Ψ : λu = −Ψ + Υ
A2 λu ≤ −Ψ",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
": λu = −Ψ−Υ
A3 λu ≥ Ψ : λu = Ψ + Υ
A4 λu ≤ Ψ : λu = Ψ−Υ
Define
∇ = f(ψ+Υλ )",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
− f( ψ−Υ λ ),CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"− f( −ψ+Υ λ ) + f( −ψ−Υ λ )
f(ψ+Υλ )",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
+,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
f( ψ−Υ λ ) +,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
f( −ψ+Υ λ ) +,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"f( −ψ−Υ λ )
",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
Clearly −1 ≤ ∇ ≤ 1.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
Theorem 2.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"Assume that the gradient step size is small enough so that we can neglect second order terms O(η2), and",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
that ∂∇∂Υ ≥ ψ Υ,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"− Υ ψ ∀Υ. Fix the difficulty score at Ψ. At time t the expected convergence rate is monotonically increasing with the loss Υ of the training point x.
Proof.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"In the coordinate system defined above ∆(Ψ,Υ) = 4ηE[λ2u2/Υ]",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"+ O(η
2)",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"We compute ∆(Ψ,Υ) separately in each region, marginalizing out v based on the following∫ ∫ ∞
0
λ2u2vd−1f(u, v)dvdu = ∫",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"λ2u2f(u)du
where f(u) denotes the marginal distribution of u.
Let ui denote the value of u corresponding to loss Υ in each region A1-A4, and 12f(ui) its density.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"∆(Ψ,Υ) takes 4 discrete values, one in each region, and its expected value is therefore ∆(Ψ,Υ) =",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
4η ∑4 i=1,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
λ 2u2i f(ui)∑4 i=1,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"f(ui)
.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"It can readily be shown that
1
4η ∆(ψ,Υ) = ψ2 +",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"Υ2 + 2ψΥ∇ (11)
and subsequently
1
4η
∂∆(ψ,Υ)
∂Υ",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
= 2Υ + 2ψΥ ∂∇ ∂Υ,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"+ 2ψ ∇
≥ 2Υ + 2ψΥ ∂∇ ∂Υ",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"− 2ψ
(12)
",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
Using the assumption that ∂∇∂Υ ≥ ψ Υ,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"− Υ ψ ∀Υ, we have that
1
8η
∂∆(ψ,Υ) ∂Υ",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
≥ Υ + ψΥ ψ,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
−Υ ψΥ,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
− ψ,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"= 0
Corollary 3.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"For any c ∈ R+, if∇ is (c− 1c )-lipschitz",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"then ∂∆(ψ,Υ) ∂Υ",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
≥ 0 for any Υ ≥ c ψ.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
Corollary 4.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"If D(X/Ψ) = k(Ψ) over a compact region and η small enough, then ∂∆(ψ,Υ)∂Υ ≥ 0 for all Υ excluding the boundaries of the compact region.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"If in addition Υ > Ψ, then ∂∆(ψ,Υ)∂Υ ≥ 0 almost surely.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
Theorem 3.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
Assume that D(X) is continuous and that w̄ is realizable.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"Then there are always hypotheses w ∈ H for which the expected convergence rate under D(X) is monotonically decreasing with the loss Υ of the sampled points.
",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
Proof.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"We shift to a hyperspherical coordinate system in Rd+1 similar as before, but now the pole (origin) is fixed at wt.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"For the gradient step s, it can be shown that:
s = − sgn (xtiwt",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"− yi)2ηxiΥ
sO = s · w̄ −wt",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"λ = ±2η λ rλ cosϑ Υ
(13)
Let ∆(Υ) denote the expected convergence rate at time t, given a fixed loss Υ. From Lemma 2
∆(Υ) = 2ηΥ",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"( E[r cosϑ/xtiwt − yi = −Υ ]−
E[r cosϑ/xtiwt − yi = Υ ]
)",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"− E[(2ηrΥ)2]
, 2ηΥQ(r, ϑ,wt)− 4η2Υ2E[r2]
If w = w̄, then Q(r, ϑ,w) = 0 from the symmetry of D(X) with respect to Ψ. From the continuity of D(X), there exists δ > 0",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"such that if ‖w − w̄‖2 < δ, then ‖Q(r, ϑ,w)−Q(r, ϑ, w̄)‖2 < ηΥE[r2], which implies that ∆(Υ) <",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
−2η2Υ2E[r2] < 0.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"While the corollaries above apply to a rather simple situation, when using the Difficulty Score to guide SGD while
minimizing the convex regression loss, their predictions can be empirically tested with the deep learning architecture and loss which are described in Section 3.",2.3. Deep learning: simulation results,[0],[0]
"There an additional challenge is posed by the fact that the empirical ranking is not based on the ideal definition given in Def. 1, but rather on an estimate derived from another classifier.
",2.3. Deep learning: simulation results,[0],[0]
"Still, the empirical results as shown in Fig. 2 demonstrate agreement with the theoretical analysis of the linear regression loss.",2.3. Deep learning: simulation results,[0],[0]
"Specifically, in epoch 0",2.3. Deep learning: simulation results,[0],[0]
"there is a big difference between the average errors in estimating the gradient direction, which is smallest for the easiest examples and highest for the most difficult examples as predicted by Corollary 1.",2.3. Deep learning: simulation results,[0],[0]
"This difference in significantly reduced after 10 epochs, and becomes insignificant after 20 epochs, in agreement with Corollary 2.
",2.3. Deep learning: simulation results,[0],[0]
Discussion.,2.3. Deep learning: simulation results,[0],[0]
"Fig. 2 shows that the variance in the direction of the gradient step defined by easier points is significantly smaller than that defined by difficult points, especially at the beginning of training.",2.3. Deep learning: simulation results,[0],[0]
"This is advantageous when the initial point w0 does not lie in the basin of attraction of the desired global minimum w̄, and if, in agreement with Lemma 1, the pronounced shared component of the easy gradient steps points in the direction of the global minimum, or a more favorable local minimum; then the likelihood of escaping the local minimum decreases with a point’s Difficulty Score.",2.3. Deep learning: simulation results,[0],[0]
This scenario suggests another possible advantage for curriculum learning at the initial stages of training.,2.3. Deep learning: simulation results,[0],[0]
"As discussed in the introduction, a practical curriculum learning method should address two main questions: how to rank the training examples, and how to modify the sampling procedure based on this ranking.",3. Curriculum learning in deep networks,[0],[0]
Solutions to these issues are discussed in Section 3.1.,3. Curriculum learning in deep networks,[0],[0]
In Section 3.2 we discuss the empirical evaluation of our method.,3. Curriculum learning in deep networks,[0],[0]
"The main novelty of our proposed method lies in this step, where we rank the training examples by estimated difficulty in the absence of human supervision.",RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
Difficulty is estimated based on knowledge transfer from another classifier.,RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
"Here we investigate transfer from a more powerful learner.
",RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
It is a common practice now to treat one of the upstream layers of a pre-trained network as a representation (or embedding) layer.,RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
"This layer activation is then used for representing similar objects and train a simpler classifier (such as SVM, or shallower NNs) to perform a different task, related but not identical to the original task the network had been trained on.",RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
"In computer vision such embeddings are commonly obtained by training a deep network on the recognition of a very large database such as ImageNet (Deng et al., 2009).",RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
"These embeddings have been shown to provide better semantic representations of images (as compared to more traditional image features) in a number of related tasks, including the classification of small datasets (Sharif Razavian et al., 2014), image annotation (Donahue et al., 2015) and structured predictions (Hu et al., 2016).
",RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
"Following this practice, the activation in the penultimate layer of a large and powerful pre-trained network is the loci of knowledge transfer from one network to another.",RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
"Repeatedly, as in (Sharif Razavian et al., 2014), it has been shown that competitive performance can be obtained by training a shallow classifier on this representation in a new related task.",RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
"Here we propose to use the confidence of such a classifier, e.g. the margin of an SVM classifier, as the estimator for the difficulty of each training example.",RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
This measure is then used to sort the training data.,RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
"We note that unlike the traditional practice of reusing a pre-trained network, here we only transfer information from one learner to another.",RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
"The goal is to achieve a smaller classifier that can conceivably be used with simpler hardware, without depending on access to the powerful learner at test time.",RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
"In agreement with prior art, e.g. the definition of curriculum in (Bengio et al., 2009), we investigate curriculum learning
where the scheduling of examples changes with time, giving priority to easier examples at the beginning of training.",SCHEDULING THE APPEARANCE OF TRAINING EXAMPLES,[0],[0]
"We explored two variants of the basic scheduling idea:
Fixed.",SCHEDULING THE APPEARANCE OF TRAINING EXAMPLES,[0],[0]
The distribution used to sample examples from the training data is gradually changed in fixed steps.,SCHEDULING THE APPEARANCE OF TRAINING EXAMPLES,[0],[0]
Initially all the weight is put on the easiest examples.,SCHEDULING THE APPEARANCE OF TRAINING EXAMPLES,[0],[0]
"In subsequent steps the weight of more difficult examples is gradually increased, until the final step in which the training data is sampled uniformly (or based on some prior distribution on the training set).
",SCHEDULING THE APPEARANCE OF TRAINING EXAMPLES,[0],[0]
Adaptive.,SCHEDULING THE APPEARANCE OF TRAINING EXAMPLES,[0],[0]
"Similar to the previous mode, but where the length of each step is not fixed, but is being determined adaptively based on the current loss of the training data.",SCHEDULING THE APPEARANCE OF TRAINING EXAMPLES,[0],[0]
Datasets.,EXPERIMENTAL SETUP,[0],[0]
"For evaluation we used 2 data sets: CIFAR-100 (Krizhevsky & Hinton, 2009) and STL-10 (Coates et al., 2010).",EXPERIMENTAL SETUP,[0],[0]
"In all cases, as is commonly done, the data was pre-processed using global contrast normalization; cropping and flipping were used for STL-10.
Network architecture.",EXPERIMENTAL SETUP,[0],[0]
We used convolutional Neural Networks (CNN) which excel at image classification tasks.,EXPERIMENTAL SETUP,[0],[0]
"Specifically, we used two architectures which are henceforth denoted Large and Small, in accordance with the number of parameters.",EXPERIMENTAL SETUP,[0],[0]
"The Large network is comprised of four blocks, each with two convolutional layers, ELU activation, and max-pooling.",EXPERIMENTAL SETUP,[0],[0]
"This is followed by a fully connected layer, for a total of 1,208,101 parameters.",EXPERIMENTAL SETUP,[0],[0]
"The Small network consists of only three hidden layers, for a total of 4,557 parameters.",EXPERIMENTAL SETUP,[0],[0]
"During training, we applied dropout and l2 regularization on the weights, and used either SGD or ADAM to optimize the cross-entropy loss.
",EXPERIMENTAL SETUP,[0],[0]
Scheduling mechanisms: control.,EXPERIMENTAL SETUP,[0],[0]
"As described above, our method is based on a scheduling design which favors the presentation of easier examples at the beginning of training.",EXPERIMENTAL SETUP,[0],[0]
"In order to isolate the contribution of scheduling by increasing level of difficulty as against other spurious consequences of data scheduling, we compared performance with the following control conditions: control-curriculum, identical scheduling mechanism but where the underlying ranking of the training examples is random and unrelated to estimated difficulty; and anti-curriculum, identical scheduling mechanism but favoring the more difficult examples at the beginning of training.",EXPERIMENTAL SETUP,[0],[0]
"Evidence from prior art is conflicting regarding where the benefits of curriculum learning lie, which is to be expected given the variability in the unknown sources of the curricu-
lum supervision information and its quality.",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
We observed in our empirical study that the benefits depended to a large extent on the difficulty of the task.,CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"We always saw faster learning at the beginning of the training process, while lower generalization error was seen only when the task was relatively difficult.",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"We therefore employed controls for the following 3 sources of task difficulty:
Inherent task difficulty.",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"To investigate this factor, we take advantage of the fact that CIFAR-100 is a hierarchical dataset with 100 classes and 20 super-classes, each including 5 member classes.",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
We therefore trained a network to discriminate the 5 member classes of 2 super-classes as 2 separate tasks: ‘small mammals’ (task 1) and ‘aquatic mammals’ (task 2).,CONTROLLING FOR TASK DIFFICULTY,[0],[0]
These are expected to be relatively hard learning tasks.,CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"We also trained a network to discriminate 5 random well separated classes: ‘camel’, ‘clock’, ‘bus’, ‘dolphin’ and ‘orchid’ (task 3).",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"This task is expected to be relatively easy.
",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
Size of classification network.,CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"For a given task, classification performance is significantly affected by the size of the network and its architecture.",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"We assume, of course, that we operate in the domain where the number of model parameters is smaller than can be justified by the training data (i.e., there is no overfit).",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
We therefore used networks of different sizes in order to evaluate how curriculum learning is affected by task difficulty as determined by the network’s strength (see Fig. 3a-b).,CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"In this comparative evaluation, the smaller the network is, the more difficult the task is likely to be (clearly, many other factors participate in the determination of task difficulty).
",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
Regularization and optimization.,CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"Regularization is used to constrain the family of hypotheses, or models, so that they possess such desirable properties as smoothness.",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
Regularization effectively decreases the number of degrees of freedom in the model.,CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"In fact, most optimization methods, other then vanilla stochastic gradient descent, incorporate some form of regularization and smoothing, among other inherent properties.",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
Therefore the selection of optimization method also plays a role in determining the effective size of the final network.,CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"Fig. 3a shows typical results when training the Large CNN (see network’s details above) to classify a subset of 5 CIFAR100 images (task 1 as defined above), using slow learning rate and Adam optimization.",RESULTS,[0],[0]
"In this setup we see that curriculum learning speeds up the learning rate at the beginning of the training, but converges to the same performance as regular training.",RESULTS,[0],[0]
"When we make learning more difficulty by using the Small network, performance naturally decreases, but now we see that curriculum learning also improves the final generalization performance (Fig. 3b).
",RESULTS,[0],[0]
"Similar results are shown for the STL-10 dataset (Fig. 3c).
",RESULTS,[0],[0]
"Fig. 4 shows comparative results when controlling for inherent task difficulty in the 3 tasks described above, using faster learning rate and SGD optimization.",RESULTS,[0],[0]
Task difficulty can be evaluated in retrospect from the final performance seen in each plot.,RESULTS,[0],[0]
"As can be clearly seen in the figure, the improvement in final accuracy with curriculum learning is larger when the task is more difficult.",RESULTS,[0],[0]
"When manipulating the level of regularization, we see that while too much regularization always harms performance, curriculum learning is least affected by this degradation (results are omitted).",RESULTS,[0],[0]
"We investigated curriculum learning, an extension of stochastic gradient descent in which easy examples are more frequently sampled at the beginning of training.",4. Summary and Discussion,[0],[0]
"We started
with the theoretical investigation of this strict definition in the context of linear regression, showing that curriculum learning accelerates the learning rate in agreement with prior empirical evidence.",4. Summary and Discussion,[0],[0]
"While not shedding light on its affect on the classifier’s final performance, our analysis suggests that the direction of a gradient step based on ”easy” examples may be more effective in traversing the input space towards the ideal minimum of the loss function.",4. Summary and Discussion,[0],[0]
"Specifically, we have empirically shown that the variance in the gradient direction of points increases with their difficulty when optimizing a non-convex loss function.",4. Summary and Discussion,[0],[0]
"Over-sampling the more coherent easier examples may therefore increase the likelihood to escape the basin of attraction of a low quality local minimum in favor of higher quality local minima even in the general non-convex case.
",4. Summary and Discussion,[0],[0]
"We also showed theoretically that when the difficulty score of the training points is fixed, convergence is faster if the loss with respect to the current hypothesis is higher.",4. Summary and Discussion,[0],[0]
"This seems to be a very intuitive result, an intuition that underlies the boosting method for example.",4. Summary and Discussion,[0],[0]
"However, as intuitive as it might be, this is not always true when the prior data density is assumed to be continuous and when the optimal hypothesis is realizable.",4. Summary and Discussion,[0],[0]
"Thus the requirement that the difficulty score is fixed is necessary.
",4. Summary and Discussion,[0],[0]
In the second part of this paper we described a curriculum learning method for deep networks.,4. Summary and Discussion,[0],[0]
The method relies on knowledge transfer from other (pre-trained) networks in order to rank the training examples by difficulty.,4. Summary and Discussion,[0],[0]
We described extensive experiments where we evaluated our proposed method under different task difficulty conditions and against a variety of control conditions.,4. Summary and Discussion,[0],[0]
"In all cases curriculum learning has been shown to increase the rate of convergence at the beginning of training, in agreement with the theoretical results.",4. Summary and Discussion,[0],[0]
"With more difficult tasks, curriculum learning improved generalization performance.",4. Summary and Discussion,[0],[0]
This work was supported in part by a grant from the Israel Science Foundation (ISF) and by the Gatsby Charitable Foundations.,Acknowledgements,[0],[0]
We provide theoretical investigation of curriculum learning in the context of stochastic gradient descent when optimizing the convex linear regression loss.,abstractText,[0],[0]
We prove that the rate of convergence of an ideal curriculum learning method is monotonically increasing with the difficulty of the examples.,abstractText,[0],[0]
"Moreover, among all equally difficult points, convergence is faster when using points which incur higher loss with respect to the current hypothesis.",abstractText,[0],[0]
We then analyze curriculum learning in the context of training a CNN.,abstractText,[0],[0]
"We describe a method which infers the curriculum by way of transfer learning from another network, pre-trained on a different task.",abstractText,[0],[0]
"While this approach can only approximate the ideal curriculum, we observe empirically similar behavior to the one predicted by the theory, namely, a significant boost in convergence speed at the beginning of training.",abstractText,[0],[0]
"When the task is made more difficult, improvement in generalization performance is also observed.",abstractText,[0],[0]
"Finally, curriculum learning exhibits robustness against unfavorable conditions such as excessive regularization.",abstractText,[0],[0]
Curriculum Learning by Transfer Learning: Theory and Experiments with Deep Networks,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 570–575 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
570",text,[0],[0]
Building Artificial Intelligence (AI) algorithms to teach machines to read and to comprehend text is a long-standing challenge in Natural Language Processing (NLP).,1 Introduction,[0],[0]
A common strategy for assessing these AI algorithms is by treating them as RC tasks.,1 Introduction,[0],[0]
This can be formulated as finding an answer to a question given the document(s) as evidence.,1 Introduction,[0],[0]
"Recently, many deep-learning based models (Seo et al., 2017; Xiong et al., 2017; Wang et al., 2017; Shen et al., 2017; Clark and Gardner, 2017) have been proposed to solve RC tasks based on the SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017) datasets, reaching human level performance.",1 Introduction,[0],[0]
"A common approach in these models is to score and/or extract candidate spans conditioned on a given question-document pair.
",1 Introduction,[0],[0]
Most of these models have limited applicability to real problems for the following reasons.,1 Introduction,[0],[0]
"They do not generalize well to scenarios where the answer is not present as a span, or where several discontinuous parts of the document are required to
∗ To whom correspondence should be addressed.
",1 Introduction,[0],[0]
form the answer.,1 Introduction,[0],[0]
"In addition, unlike humans, they can not easily skip through irrelevant parts to comprehend long documents (Masson, 1983).
",1 Introduction,[0],[0]
"To address the issues above we develop a novel context zoom-in network (ConZNet) for RC tasks, which can skip through irrelevant parts of a document and generate an answer using only the relevant regions of text.",1 Introduction,[0],[0]
The ConZNet architecture consists of two phases.,1 Introduction,[0],[0]
In the first phase we identify the relevant regions of text by employing a reinforcement learning algorithm.,1 Introduction,[0],[0]
"These relevant regions are not only useful to generate the answer, but can also be presented to the user as supporting information along with the answer.",1 Introduction,[0],[0]
"The second phase is based on an encoder-decoder architecture, which comprehends the identified regions of text and generates the answer by using a residual self-attention network as encoder and a RNNbased sequence generator along with a pointer network (Vinyals et al., 2015) as the decoder.",1 Introduction,[0],[0]
"It has the ability to generate better well-formed answers not verbatim present in the document than span prediction models.
",1 Introduction,[0],[0]
"Recently, there have been several attempts to adopt condensing documents in RC tasks.",1 Introduction,[0],[0]
Wang et al. (2018) retrieve a relevant paragraph based on the question and predict the answer span.,1 Introduction,[0],[0]
Choi et al. (2017) select sentence(s) to make a summary of the entire document with a feed-forward network and generate an answer based on the summary.,1 Introduction,[0],[0]
"Unlike existing approaches, our method has the ability to select relevant regions of text not just based on the question but also on how well regions are related to each other.",1 Introduction,[0],[0]
"Moreover, our decoder combines span prediction and sequence generation.",1 Introduction,[0],[0]
"This allows the decoder to copy words from the relevant regions of text as well as to generate words from a fixed vocabulary.
",1 Introduction,[0],[0]
"We evaluate our model using one of the challenging RC datasets, called ‘NarrativeQA’, which
was released recently by Kočiskỳ et al. (2017).",1 Introduction,[0],[0]
Experimental results show the usefulness of our framework for RC tasks and we outperform stateof-the-art results on this dataset.,1 Introduction,[0],[0]
"An overview of our architecture is shown in Figure 1, which consists of two phases.",2 Proposed Architecture,[0],[0]
"First, the identification of relevant regions of text is computed by the Co-attention and Context Zoom layers as explained in Sections 2.1 and 2.2.",2 Proposed Architecture,[0],[0]
"Second, the comprehension of identified regions of text and output generation is computed by Answer Generation block as explained in Section 2.3.",2 Proposed Architecture,[0],[0]
"The words in the document, question and answer are represented using pre-trained word embeddings (Pennington et al., 2014).",2.1 Co-attention layer,[0],[0]
These wordbased embeddings are concatenated with their corresponding char embeddings.,2.1 Co-attention layer,[0],[0]
"The char embeddings are learned by feeding all the characters of a word into a Convolutional Neural Network (CNN) (Kim, 2014).",2.1 Co-attention layer,[0],[0]
"We further encode the document and question embeddings using a shared bi-directional GRU (Cho et al., 2014) to get context-aware representations.
",2.1 Co-attention layer,[0],[0]
We compute the co-attention between document and question to get question-aware representations for the document by using tri-linear attention as proposed by Seo et al. (2017).,2.1 Co-attention layer,[0],[0]
"Let di be the vector representation for the document word i, qj be the vector for the question word j, and ld and lq be the lengths of the document and question respectively.",2.1 Co-attention layer,[0],[0]
"The tri-linear attention is calculated as
aij = wddi + wqqj + wdq(di qj), (1)
where wd, wq, and wdq are learnable parameters and denotes the element-wise multiplication.
",2.1 Co-attention layer,[0],[0]
We compute the attended document word d̃i by first computing λi = softmax(ai:) and followed by d̃i = ∑lq j=1 λijqj .,2.1 Co-attention layer,[0],[0]
"Similarly, we compute a question to document attention vector q̃ by first computing b = softmax(max(ai:)) and followed by q̃",2.1 Co-attention layer,[0],[0]
= ∑ld i=i dibi.,2.1 Co-attention layer,[0],[0]
"Finally, di, d̃i, di d̃i, d̃i q̃ are concatenated to yield a query-aware contextual representation for each word in the document.",2.1 Co-attention layer,[0],[0]
This layer finds relevant regions of text.,2.2 Context Zoom Layer,[0],[0]
"We use reinforcement learning to do that, with the goal of improving answer generation accuracy – see Section 2.4.
",2.2 Context Zoom Layer,[0],[0]
The Split Context operation splits the attended document vectors into sentences or fixed size chunks (useful when sentence tokenization is not available for a particular language).,2.2 Context Zoom Layer,[0],[0]
"This results in n text regions with each having length lk,",2.2 Context Zoom Layer,[0],[0]
where ld = ∑n k=1 lk.,2.2 Context Zoom Layer,[0],[0]
"We then get the representations, denoted as zk, for each text region by running a BiGRU and concatenating the last states of the forward and backward GRUs.
",2.2 Context Zoom Layer,[0],[0]
"The text region representations, zk, encode how well they are related to the question, and their surrounding context.",2.2 Context Zoom Layer,[0],[0]
"Generating an answer may depend on multiple regions, and it is important for
each text region to collect cues from other regions which are outside of their surroundings.",2.2 Context Zoom Layer,[0],[0]
We can compute this by using a Self-Attention layer.,2.2 Context Zoom Layer,[0],[0]
"It is a special case of co-attention where both operands (di and qj) are the text fragment itself, computed by setting aij = −∞ when i = j in Eq. 1.
",2.2 Context Zoom Layer,[0],[0]
"These further self-attended text region representations, z̃k, are passed through a linear layer with tanh activation and softmax layer as follows:
u = tanh(Wc[z̃1, · · · , z̃n] + bc), (2) ψ = softmax(u), (3)
where ψ is the probability distribution of text regions, which is the evidence used to generate the answer.",2.2 Context Zoom Layer,[0],[0]
"The policy of the reinforcement learner is defined as π(r|u; θz) = ψr, where ψr is the probability of a text region r (agent’s action) being selected, u is the environment state as defined in Eq. 2, and θz are the learnable parameters.",2.2 Context Zoom Layer,[0],[0]
"During the training time we sample text regions using ψ, in inference time we follow greedy evaluation by selecting most probable region(s).",2.2 Context Zoom Layer,[0],[0]
"This component is implemented based on the encoder-decoder architecture of (Sutskever et al., 2014).",2.3 Answer Generation,[0],[0]
"The selected text regions from the Context Zoom layer are given as input to the encoder, where its output is given to the decoder in order to generate the answer.
",2.3 Answer Generation,[0],[0]
The encoder block uses residual connected selfattention layer followed by a BiGRU.,2.3 Answer Generation,[0],[0]
"The selected relevant text regions (∈ ψr) are first passed through a separate BiGRU, then we apply a selfattention mechanism similar to the Context Zoom layer followed by a linear layer with ReLU activations.",2.3 Answer Generation,[0],[0]
"The encoder’s output consists of representations of the relevant text regions, denoted by ei.
",2.3 Answer Generation,[0],[0]
"The decoder block is based on an attention mechanism (Bahdanau et al., 2015) and a copy mechanism by using a pointer network similar to (See et al., 2017).",2.3 Answer Generation,[0],[0]
This allows the decoder to predict words from the relevant regions as well as from the fixed vocabulary.,2.3 Answer Generation,[0],[0]
"At time step t, the decoder predicts the next word in the answer using the attention distribution, context vector and current word embedding.",2.3 Answer Generation,[0],[0]
"The attention distribution and context vector are obtained as follows:
oti = v T tanh(Weei +Whht + bo), (4)
γt = softmax(oti), (5)
where ht is hidden state of the decoder, v, We, Wh, bo are learnable parameters.",2.3 Answer Generation,[0],[0]
The γt represents a probability distribution over words of relevant regions ei.,2.3 Answer Generation,[0],[0]
"The context vector is given by ct = ∑ i γ t iei.
",2.3 Answer Generation,[0],[0]
"The probability distribution to predict word wt from the fixed vocabulary (Pfv) is computed by passing state ht and context vector ct to a linear layer followed by a softmax function denoted as
Pfv = softmax(Wv(Xv[ht, ct] + bp)+ bq).",2.3 Answer Generation,[0],[0]
"(6)
To allow decoder to copy words from the encoder sequence, we compute a soft gate (Pcopy), which helps the decoder to generate a word by sampling from the fixed vocabulary or by copying from a selected text regions (ψr).",2.3 Answer Generation,[0],[0]
"The soft gate is calculated as
Pcopy = σ(w T p ct + v T h",2.3 Answer Generation,[0],[0]
"ht + w T x xt + bc), (7)
where xt is current word embedding, ht is hidden state of the decoder, ct is the context vector, and wp, vh, wx, and bc are learnable parameters.",2.3 Answer Generation,[0],[0]
We maintain a list of out-of-vocabulary (OOV) words for each document.,2.3 Answer Generation,[0],[0]
The fixed vocabulary along with this OOV list acts as an extended vocabulary for each document.,2.3 Answer Generation,[0],[0]
"The final probability distribution (unnormalized) over this extended vocabulary (Pev) is given by
Pev(wt) =",2.3 Answer Generation,[0],[0]
"(1−Pcopy)Pfv(wt)+Pcopy ∑
i:wi=wt
γti .
(8)",2.3 Answer Generation,[0],[0]
"We jointly estimate the parameters of our model coming from the Co-attention, Context Zoom, and Answer Generation layers, which are denoted as θa, θz , and θg respectively.",2.4 Training,[0],[0]
"Estimating θa and θg is straight-forward by using the cross-entropy objective J1({θa, θg}) and the backpropagation algorithm.",2.4 Training,[0],[0]
"However, selecting text regions in the Context Zoom layer makes it difficult to estimate θz
given their discrete nature.",2.4 Training,[0],[0]
We therefore formulate the estimation of θz as a reinforcement learning problem via a policy gradient method.,2.4 Training,[0],[0]
"Specifically, we design a reward function over θz .
",2.4 Training,[0],[0]
"We use mean F-score of ROUGE-1, ROUGE-2, and ROUGE-L (Lin and Hovy, 2003) as our reward function R.",2.4 Training,[0],[0]
"The objective function to maximize is the expected reward under the probability distribution of current text regions ψr, i.e., J2(θz) = Ep(r|θz)[R].",2.4 Training,[0],[0]
"We approximate the gradient ∇θzJ2(θz) by following the REINFORCE (Williams, 1992) algorithm.",2.4 Training,[0],[0]
To reduce the high variance in estimating∇θzJ2(θz) one widely used mechanism is to subtract a baseline value from the reward.,2.4 Training,[0],[0]
"It is shown that any number will reduce the variance (Williams, 1992; Zaremba and Sutskever, 2015), here we used the mean of the mini-batch reward b as our baseline.",2.4 Training,[0],[0]
"The final objective is to minimize the following equation:
J(θ) = J1({θa, θg})−J2(θz)+ B∑ i=1",2.4 Training,[0],[0]
"(Ri−b), (9)
where, B is the size of mini-batch, and Ri is the reward of example i ∈",2.4 Training,[0],[0]
B. J(θ) is now fully differentiable and we use backpropagation to estimate θ.,2.4 Training,[0],[0]
"The NarrativeQA dataset (Kočiskỳ et al., 2017) consists of fictional stories gathered from books and movie scripts, where corresponding summaries and question-answer pairs are generated with the help of human experts and Wikipedia articles.",3.1 Dataset,[0],[0]
The summaries in NarrativeQA are 4-5 times longer than documents in the SQuAD dataset.,3.1 Dataset,[0],[0]
"Moreover, answers are well-formed by human experts and are not verbatim in the story, thus making this dataset ideal for testing our model.",3.1 Dataset,[0],[0]
The statistics of NarrativeQA are available in Table 11.,3.1 Dataset,[0],[0]
"We compare our model against reported models in Kočiskỳ et al. (2017) (Seq2Seq, ASR, BiDAF) and the Multi-range Reasoning Unit (MRU) in Tay et al. (2018).",3.2 Baselines,[0],[0]
"We implemented two baseline models (Baseline 1, Baseline 2) with Context Zoom layer similar to Wang et al. (2018).",3.2 Baselines,[0],[0]
In both baselines we replace the span prediction layer with an answer generation layer.,3.2 Baselines,[0],[0]
"In Baseline 1 we use an
1please refer Kočiskỳ et al. (2017) for more details
attention based seq2seq layer without using copy mechanism in the answer generation unit similar to Choi et al. (2017).",3.2 Baselines,[0],[0]
In Baseline 2 the answer generation unit is similar to our ConZNet architecture.,3.2 Baselines,[0],[0]
"We split each document into sentences using the sentence tokenizer of the NLTK toolkit (Bird and Loper, 2004).",3.3 Implementation Details,[0],[0]
"Similarly, we further tokenize each sentence, corresponding question and answer using the word tokenizer of NLTK.",3.3 Implementation Details,[0],[0]
"The model is implemented using Python and Tensorflow (Abadi et al., 2015).",3.3 Implementation Details,[0],[0]
"All the weights of the model are initialized by Glorot Initialization (Glorot et al., 2011) and biases are initialized with zeros.",3.3 Implementation Details,[0],[0]
"We use a 300 dimensional word vectors from GloVe (Pennington et al., 2014) (with 840 billion pre-trained vectors) to initialize the word embeddings, which we kept constant during training.",3.3 Implementation Details,[0],[0]
"All the words that do not appear in Glove are initialized by sampling from a uniform random distribution between [-0.05, 0.05].",3.3 Implementation Details,[0],[0]
"We apply dropout (Srivastava et al., 2014) between the layers with keep probability of 0.8 (i.e dropout=0.2).",3.3 Implementation Details,[0],[0]
The number of hidden units are set to 100.,3.3 Implementation Details,[0],[0]
"We trained our model with the AdaDelta (Zeiler, 2012) optimizer for 50 epochs, an initial learning rate of 0.1, and a minibatch size of 32.",3.3 Implementation Details,[0],[0]
The hyperparameter ‘sample size’ (number of relevant sentences) is chosen based on the model performance on the devset.,3.3 Implementation Details,[0],[0]
Table 2 shows the performance of various models on NarrativeQA.,3.4 Results,[0],[0]
It can be noted that our model with sample size 5 (choosing 5 relevant sentences) outperforms the best ROUGE-L score available so far by 12.62% compared to Tay et al. (2018).,3.4 Results,[0],[0]
"The low performance of Baseline 1 shows that the hybrid approach (ConZNet) for generating words from a fixed vocabulary as well as copying words from the document is better suited than span prediction models (Seq2Seq, ASR, BiDAF, MRU).
",3.4 Results,[0],[0]
"To validate the importance of finding relevant sentences in contrast to using an entire document for answer generation, we experimented with sample sizes beyond 5.",3.4 Results,[0],[0]
The performance of our model gradually dropped from sample size 7 onwards.,3.4 Results,[0],[0]
"This result shows evidence that only a few relevant sentences are sufficient to answer a question.
",3.4 Results,[0],[0]
"We also experimented with various sample sizes to see the effect of intra sentence relations for an-
swer generation.",3.4 Results,[0],[0]
The performance of the model improved dramatically with sample sizes 3 and 5 compared to the sample size of 1.,3.4 Results,[0],[0]
These results show that the importance of selecting multiple relevant sentences for generating an answer.,3.4 Results,[0],[0]
"In addition, the low performance of Baseline 2 indicates that just selecting multiple sentences is not enough, they should also be related to each other.",3.4 Results,[0],[0]
This result points out that the self-attention mechanism in the Context zoom layer is an important component to identify related relevant sentences.,3.4 Results,[0],[0]
We have proposed a new neural-based architecture which condenses an original document to facilitate fast comprehension in order to generate better well-formed answers than span based prediction models.,4 Conclusion,[0],[0]
Our model achieved the best performance on the challenging NarrativeQA dataset.,4 Conclusion,[0],[0]
"Future work can focus for example on designing an inexpensive preprocess layer, and other strategies for improved performance on answer generation.",4 Conclusion,[0],[0]
In recent years many deep neural networks have been proposed to solve Reading Comprehension (RC) tasks.,abstractText,[0],[0]
Most of these models suffer from reasoning over long documents and do not trivially generalize to cases where the answer is not present as a span in a given document.,abstractText,[0],[0]
We present a novel neural-based architecture that is capable of extracting relevant regions based on a given question-document pair and generating a well-formed answer.,abstractText,[0],[0]
"To show the effectiveness of our architecture, we conducted several experiments on the recently proposed and challenging RC dataset ‘NarrativeQA’.",abstractText,[0],[0]
"The proposed architecture outperforms state-of-the-art results (Tay et al., 2018) by 12.62% (ROUGE-L) relative improvement.",abstractText,[0],[0]
Cut to the Chase: A Context Zoom-in Network for Reading Comprehension,title,[0],[0]
"1 INTRODUCTION
Dance Dance Revolution (DDR) is a popular rhythm-based video game with millions of players worldwide (Hoysniemi, 2006). Players perform steps atop a dance platform, containing four buttons, each labeled with an arrow. An on-screen step chart prompts players to step on the buttons at specific, musically salient points in time. Scores depend upon both hitting the right buttons and hitting them at the right time. Step charts vary in difficulty with harder charts containing more steps and more complex sequences.
Despite the game’s popularity, players have some reasonable complaints: For one, packs are limited to songs with favorable licenses, meaning players may be unable to dance to their favorite songs. Even when charts are available, players may tire of repeatedly performing the same charts. Although players can produce their own charts, the process is painstaking and requires significant expertise.
This paper introduces learning to choreograph, the task of producing a step chart from raw audio. We break the problem into two subtasks: First, step placement consists of identifying a set of timestamps in the song at which to place steps. This process can be conditioned on a user-specified difficulty level. Second, step selection consists of choosing which steps to place at each timestamp. Running these two steps in sequence yields a playable step chart (Figure 1). 1
For both prediction stages of learning to choreograph, we demonstrate the superior performance of neural networks over strong alternatives. Our best model for step placement jointly learns convolutional neural network (CNN) representations and a recurrent neural network (RNN), which
1 Demonstration video showing human choreography and the output of Dance Dance Convolution side-byside: https://youtu.be/yUc3O237p9M
integrates information across consecutive time slices. Our best model for step selection consists of a conditional LSTM generative model which receives high-level rhythm features as auxiliary information.",text,[0],[0]
"Before applying our step placement algorithms, we transform raw audio samples into perceptuallyinformed representations.",2 METHODS,[0],[0]
Music files arrive as lossy encodings at 44.1kHz .,2 METHODS,[0],[0]
We decode the audio files into stereo PCM and average the two channels to produce a monophonic representation.,2 METHODS,[0],[0]
"We then compute a multiple-timescale short-time Fourier transform (STFT) using window lengths of 23ms , 46ms , and 93ms and a stride of 10ms .",2 METHODS,[0],[0]
We reduce the dimensionality of the STFT magnitude spectrum by applying a Mel-scale filterbank yielding 80 frequency bands.,2 METHODS,[0],[0]
Then we scale the filter outputs logarithmically in accordance with human perception of loudness.,2 METHODS,[0],[0]
"Finally, we prepend and append seven frames of past and future context to each frame.
",2 METHODS,[0],[0]
"2.1 STEP PLACEMENT
We consider several models to address the step placement task.",2 METHODS,[0],[0]
Each model’s output consists of a single sigmoid unit which estimates the probability that a step is placed.,2 METHODS,[0],[0]
"For all models, we augment the audio features with a one-hot representation of difficulty.
",2 METHODS,[0],[0]
"Following state-of-the-art work on musical onset detection (Schlüter & Böck, 2014), we adopt a convolutional neural network (CNN) architecture.",2 METHODS,[0],[0]
This model consists of two convolutional layers followed by two fully connected layers.,2 METHODS,[0],[0]
Our first convolutional layer has 10 filter kernels that are 7-wide in time and 3-wide in frequency.,2 METHODS,[0],[0]
The second layer has 20 filter kernels that are 3-wide in time and 3-wide in frequency.,2 METHODS,[0],[0]
"We apply 1D max-pooling after each convolutional layer, only in the frequency dimension, with a width and stride of 3.",2 METHODS,[0],[0]
Both convolutional layers use rectified linear units (ReLU) (Glorot et al.).,2 METHODS,[0],[0]
"Following the convolutional layers, we add two fully connected layers with ReLU activation functions and 256 and 128 nodes respectively.
",2 METHODS,[0],[0]
"To improve upon the CNN, we propose a C-LSTM model (Figure 2), combining a convolutional encoding with an LSTM-RNN (Hochreiter & Schmidhuber, 1997) that integrates information across longer windows of time.",2 METHODS,[0],[0]
Our C-LSTM contains two convolutional layers (of the same shape as the CNN) applied across the full unrolling length.,2 METHODS,[0],[0]
"The output of the second convolutional layer is a 3D tensor, which we flatten along the channel and frequency axes (preserving the temporal dimension).",2 METHODS,[0],[0]
The flattened features at each time step then become the inputs to a two-layer LSTM with 200 nodes per layer.,2 METHODS,[0],[0]
We train this model using 100 unrollings for backpropagation through time.,2 METHODS,[0],[0]
We treat the step selection task as a sequence generation problem.,2.2 STEP SELECTION,[0],[0]
"Our approach follows related work in language modeling where RNNs are well-known to produce coherent text that captures long-range relationships (Mikolov et al., 2010; Sutskever et al., 2011; Sundermeyer et al., 2012).
",2.2 STEP SELECTION,[0],[0]
Our LSTM model passes over the ground truth step placements and predicts the next token given the previous sequence of tokens.,2.2 STEP SELECTION,[0],[0]
The output is a softmax distribution over the game’s 256 possible steps.,2.2 STEP SELECTION,[0],[0]
"As inputs, we use a more compact bag-of-arrows representation containing 16 features (4 per
arrow) to depict the previous step.",2.2 STEP SELECTION,[0],[0]
"For each arrow, the 4 corresponding features represent the states on, off, hold, and release.",2.2 STEP SELECTION,[0],[0]
We add an additional feature that functions as a start token to denote the first step of a chart.,2.2 STEP SELECTION,[0],[0]
"For this task, our LSTM consists of 2 layers of 128 cells each.",2.2 STEP SELECTION,[0],[0]
"We use 64 steps of unrolling, an average of 100 seconds for the easiest charts and 9 seconds for the hardest.
To inform our LSTM of the non-uniform rhythmic spacing of the step placements, we provide the following two pieces of auxiliary information: (1) ∆-beat adds two features representing the number of beats since the previous and until the next step; (2) beat phase adds four features representing which sixteenth note subdivision of the beat the current step most closely aligns to.",2.2 STEP SELECTION,[0],[0]
"We collected a dataset consisting of 203 songs, labeled by 9 annotators.",3 EXPERIMENTS,[0],[0]
"One particularly prolific annotator, Fraxtil, annotated 90 of these songs for all five difficulty levels.",3 EXPERIMENTS,[0],[0]
The remaining songs are from a large multi-author collection called In The Groove (ITG).,3 EXPERIMENTS,[0],[0]
"In total, across all five difficulty settings, we obtain around 35 hours of annotated audio and 350, 000 steps.",3 EXPERIMENTS,[0],[0]
"2 We augment our dataset for step selection by synthesizing mirror images of each chart (i.e., interchanging left and right) which we found to improve performance for all models
For step placement, we compare the performance of our proposed CNN and C-LSTM models against a logistic regressor (LogReg) and a 2-layer MLP.",3 EXPERIMENTS,[0],[0]
"For step selection, we compare our proposed LSTM model against a fixed-window MLP and an n-gram model using modified Kneser-Ney smoothing (Chen & Goodman, 1998) with backoff.",3 EXPERIMENTS,[0],[0]
Both the MLP (MLP5) and n-gram model (KN5) predict the next step from four steps of history and the MLP received the same auxiliary information as the LSTM.,3 EXPERIMENTS,[0],[0]
"We also show the performance of an LSTM model trained with only 5 steps of unrolling (LSTM5) to demonstrate the advantage of longer context.
",3 EXPERIMENTS,[0],[0]
"Model Dataset PPL AUC F-score
LogReg Fraxtil 1.205 0.601 0.609 MLP Fraxtil 1.097 0.659 0.665 CNN Fraxtil 1.082 0.671 0.678 C-LSTM Fraxtil 1.070 0.682 0.681
LogReg ITG 1.123 0.599 0.634 MLP ITG 1.090 0.637 0.671 CNN ITG 1.083 0.677 0.689 C-LSTM ITG 1.072 0.680 0.697
Table 1: Perplexity, area under curve and Fscore assessed for the step placement task.
",3 EXPERIMENTS,[0],[0]
"Model Dataset PPL Acc.
",3 EXPERIMENTS,[0],[0]
KN5 Fraxtil 3.681 0.528 MLP5 Fraxtil 3.428 0.557 LSTM5 Fraxtil 3.185 0.581 LSTM64,3 EXPERIMENTS,[0],[0]
"Fraxtil 3.011 0.613
KN5 ITG 5.847 0.356",3 EXPERIMENTS,[0],[0]
"MLP5 ITG 4.786 0.401 LSTM5 ITG 4.447 0.441 LSTM64 ITG 4.342 0.444
Table 2: Perplexity and per-token accuracy assessed for the step selection task.
",3 EXPERIMENTS,[0],[0]
"Our experiments demonstrate that on both the step placement (Table 1) and the step selection (Table 2) tasks, deep neural network models outperform traditional baselines.",3 EXPERIMENTS,[0],[0]
"For the step placement task, the best performing method by all metrics is the C-LSTM.",3 EXPERIMENTS,[0],[0]
"For the step selection task, LSTMs outperform other models.
",3 EXPERIMENTS,[0],[0]
Data augmentation and the inclusion of ∆-beat and beat phase side information give a significant increase in performance to both the MLP and LSTMs for step selection.,3 EXPERIMENTS,[0],[0]
"For example, the LSTM64 model trained on the Fraxtil dataset without side information or data augmentation only achieves a PPL of 3.526 and accuracy of 0.562.",3 EXPERIMENTS,[0],[0]
Step selection models perform better on the single-author Fraxtil dataset in comparison to the multi-author ITG.,3 EXPERIMENTS,[0],[0]
Author style tends to be distinctive and thus a collection of single-author sequences is more predictable.,3 EXPERIMENTS,[0],[0]
"A few prior systems attempt automatic synthesis of step charts (O’Keeffe, 2003; Nogaj, 2005), however neither establishes a reproducible evaluation methodology or learns the semantics of steps from data.",4 RELATED WORK,[0],[0]
"The most closely related work to our step placement task is concerned with onset detection
2All data shall be made available at publication time
(Bello et al., 2005; Dixon, 2006), which has previously been attempted with deep neural networks (Eyben et al., 2010; Schlüter & Böck, 2014).",4 RELATED WORK,[0],[0]
Our step selection task most closely resembles conditional language modeling.,4 RELATED WORK,[0],[0]
"A recent wave of work in RNNs for language modeling began with (Mikolov et al., 2010; Sutskever et al., 2011).",4 RELATED WORK,[0],[0]
"Inspired by this work, several recent papers extend the methods to polyphonic music generation and transcription (Boulanger-Lewandowski et al., 2012; Chu et al., 2016; Sigtia et al., 2016).",4 RELATED WORK,[0],[0]
"To our knowledge, ours is the first paper to attempt end-to-end DDR choreography from raw audio with deep learning.",4 RELATED WORK,[0],[0]
Dance Dance Revolution (DDR) is a popular rhythm-based video game.,abstractText,[0],[0]
Players perform steps on a dance platform in synchronization with music as directed by on-screen step charts.,abstractText,[0],[0]
"While many step charts are available in standardized packs, users may grow tired of existing charts, or wish to dance to a song for which no chart exists.",abstractText,[0],[0]
We introduce the task of learning to choreograph.,abstractText,[0],[0]
"Given a raw audio track, the goal is to produce a new step chart.",abstractText,[0],[0]
This task decomposes naturally into two subtasks: deciding when to place steps and deciding which steps to select.,abstractText,[0],[0]
We demonstrate deep learning solutions for both tasks and establish strong benchmarks for future work.,abstractText,[0],[0]
"lem in deep reinforcement learning (RL). In many scenarios of interest data is hard to obtain, so agents may learn a source policy in a setting where data is readily available, with the hope that it generalises well to the target domain. We propose a new multi-stage RL agent, DARLA (DisentAngled Representation Learning Agent), which learns to see before learning to act. DARLA’s vision is based on learning a disentangled representation of the observed environment. Once DARLA can see, it is able to acquire source policies that are robust to many domain shifts - even with no access to the target domain. DARLA significantly outperforms conventional baselines in zero-shot domain adaptation scenarios, an effect that holds across a variety of RL environments (Jaco arm, DeepMind Lab) and base RL algorithms (DQN, A3C and EC).",text,[0],[0]
Autonomous agents can learn how to maximise future expected rewards by choosing how to act based on incoming sensory observations via reinforcement learning (RL).,1. Introduction,[0],[0]
"Early RL approaches did not scale well to environments with large state spaces and high-dimensional raw observations (Sutton & Barto, 1998).",1. Introduction,[0],[0]
"A commonly used workaround was to embed the observations in a lower-dimensional space, typically via hand-crafted and/or privileged-information features.",1. Introduction,[0],[0]
"Recently, the advent of deep learning and its successful combination with RL has enabled end-to-end learning of such embeddings directly from raw inputs, sparking success in a wide variety of previously challenging RL domains (Mnih et al., 2015; 2016; Jaderberg et al., 2017).",1. Introduction,[0],[0]
"Despite the seemingly universal
*
Equal contribution
1
DeepMind, 6 Pancras Square, Kings
Cross, London, N1C 4AG, UK.",1. Introduction,[0],[0]
"Correspondence to: Irina Higgins <irinah@google.com>, Arka Pal <arkap@google.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
efficacy of deep RL, however, fundamental issues remain.",1. Introduction,[0],[0]
"These include data inefficiency, the reactive nature and general brittleness of learnt policies to changes in input data distribution, and lack of model interpretability (Garnelo et al., 2016; Lake et al., 2016).",1. Introduction,[0],[0]
"This paper focuses on one of these outstanding issues: the ability of RL agents to deal with changes to the input distribution, a form of transfer learning known as domain adaptation (Bengio et al., 2013).",1. Introduction,[0],[0]
"In domain adaptation scenarios, an agent trained on a particular input distribution with a specified reward structure (termed the source domain) is placed in a setting where the input distribution is modified but the reward structure remains largely intact (the target domain).",1. Introduction,[0],[0]
We aim to develop an agent that can learn a robust policy using observations and rewards obtained exclusively within the source domain.,1. Introduction,[0],[0]
"Here, a policy is considered as robust if it generalises with minimal drop in performance to the target domain without extra fine-tuning.
",1. Introduction,[0],[0]
"Past attempts to build RL agents with strong domain adaptation performance highlighted the importance of learning good internal representations of raw observations (Finn et al., 2015; Raffin et al., 2017; Pan & Yang, 2009; Barreto et al., 2016; Littman et al., 2001).",1. Introduction,[0],[0]
"Typically, these approaches tried to align the source and target domain representations by utilising observation and reward signals from both domains (Tzeng et al., 2016; Daftry et al., 2016; Parisotto et al., 2015; Guez et al., 2012; Talvitie & Singh, 2007; Niekum et al., 2013; Gupta et al., 2017; Finn et al., 2017; Rajendran et al., 2017).",1. Introduction,[0],[0]
"In many scenarios, such as robotics, this reliance on target domain information can be problematic, as the data may be expensive or difficult to obtain (Finn et al., 2017; Rusu et al., 2016).",1. Introduction,[0],[0]
"Furthermore, the target domain may simply not be known in advance.",1. Introduction,[0],[0]
"On the other hand, policies learnt exclusively on the source domain using existing deep RL approaches that have few constraints on the nature of the learnt representations often overfit to the source input distribution, resulting in poor domain adaptation performance (Lake et al., 2016; Rusu et al., 2016).
",1. Introduction,[0],[0]
We propose tackling both of these issues by focusing instead on learning representations which capture an underlying low-dimensional factorised representation of the world and are therefore not task or domain specific.,1. Introduction,[0],[0]
"Many nat-
uralistic domains such as video game environments, simulations and our own world are well described in terms of such a structure.",1. Introduction,[0],[0]
"Examples of such factors of variation are object properties like colour, scale, or position; other examples correspond to general environmental factors, such as geometry and lighting.",1. Introduction,[0],[0]
"We think of these factors as a set of high-level parameters that can be used by a world graphics engine to generate a particular natural visual scene (Kulkarni et al., 2015).",1. Introduction,[0],[0]
"Learning how to project raw observations into such a factorised description of the world is addressed by the large body of literature on disentangled representation learning (Schmidhuber, 1992; Desjardins et al., 2012; Cohen & Welling, 2014; 2015; Kulkarni et al., 2015; Hinton et al., 2011; Rippel & Adams, 2013; Reed et al., 2014; Yang et al., 2015; Goroshin et al., 2015; Kulkarni et al., 2015; Cheung et al., 2015; Whitney et al., 2016; Karaletsos et al., 2016; Chen et al., 2016; Higgins et al., 2017).",1. Introduction,[0],[0]
"Disentangled representations are defined as interpretable, factorised latent representations where either a single latent or a group of latent units are sensitive to changes in single ground truth factors of variation used to generate the visual world, while being invariant to changes in other factors (Bengio et al., 2013).",1. Introduction,[0],[0]
"The theoretical utility of disentangled representations for supervised and reinforcement learning has been described before (Bengio et al., 2013; Higgins et al., 2017; Ridgeway, 2016); however, to our knowledge, it has not been empirically validated to date.
",1. Introduction,[0],[0]
"We demonstrate how disentangled representations can improve the robustness of RL algorithms in domain adaptation scenarios by introducing DARLA (DisentAngled Representation Learning Agent), a new RL agent capable of learning a robust policy on the source domain that achieves significantly better out-of-the-box performance in domain adaptation scenarios compared to various baselines.",1. Introduction,[0],[0]
"DARLA relies on learning a latent state representation that is shared between the source and target domains, by learning a disentangled representation of the environment’s generative factors.",1. Introduction,[0],[0]
"Crucially, DARLA does not require target domain data to form its representations.",1. Introduction,[0],[0]
"Our approach utilises a three stage pipeline: 1) learning to see, 2) learning to act, 3) transfer.",1. Introduction,[0],[0]
"During the first stage,
DARLA develops its vision, learning to parse the world in terms of basic visual concepts, such as objects, positions, colours, etc. by utilising a stream of raw unlabelled observations – not unlike human babies in their first few months of life (Leat et al., 2009; Candy et al., 2009).",1. Introduction,[0],[0]
"In the second stage, the agent utilises this disentangled visual representation to learn a robust source policy.",1. Introduction,[0],[0]
"In stage three, we demonstrate that the DARLA source policy is more robust to domain shifts, leading to a significantly smaller drop in performance in the target domain even when no further policy finetuning is allowed (median 270.3% improvement).",1. Introduction,[0],[0]
"These effects hold consistently across a number of different RL environments (DeepMind Lab and Jaco/MuJoCo: Beattie et al., 2016; Todorov et al., 2012) and algorithms (DQN, A3C and Episodic Control: Mnih et al., 2015; 2016; Blundell et al., 2016).",1. Introduction,[0],[0]
We now formalise domain adaptation scenarios in a reinforcement learning (RL) setting.,2.1. Domain adaptation in Reinforcement Learning,[0],[0]
"We denote the source and target domains as DS and DT , respectively.",2.1. Domain adaptation in Reinforcement Learning,[0],[0]
Each domain corresponds to an MDP defined as a tuple DS ⌘,2.1. Domain adaptation in Reinforcement Learning,[0],[0]
"(SS ,AS , TS , RS) or DT ⌘ (ST ,AT , TT , RT ) (we assume a shared fixed discount factor ), each with its own state space S , action space A, transition function T and reward function R.1",2.1. Domain adaptation in Reinforcement Learning,[0],[0]
"In domain adaptation scenarios the states S of the source and the target domains can be quite different, while the action spaces A are shared and the transitions T and reward functions R have structural similarity.",2.1. Domain adaptation in Reinforcement Learning,[0],[0]
"For example, consider a domain adaptation scenario for the Jaco robotic arm, where the MuJoCo (Todorov et al., 2012) simulation of the arm is the source domain, and the real world setting is the target domain.",2.1. Domain adaptation in Reinforcement Learning,[0],[0]
"The state spaces (raw pixels) of the source and the target domains differ significantly due to the perceptual-reality gap (Rusu et al., 2016); that is to say, SS 6= ST .",2.1. Domain adaptation in Reinforcement Learning,[0],[0]
"Both domains, however, share action spaces (AS = AT ), since the policy learns to control the same set of actuators within the arm.",2.1. Domain adaptation in Reinforcement Learning,[0],[0]
"Finally, the source and target domain transition and reward functions share structural similarity (TS ⇡ TT and RS ⇡ RT ), since in both domains transitions between states are governed by the physics of the world and the performance on the task depends on the relative position of the arm’s end effectors (i.e. fingertips) with respect to an object of interest.",2.1. Domain adaptation in Reinforcement Learning,[0],[0]
"In order to describe our proposed DARLA framework, we assume that there exists a set M of MDPs that is the set 1
For further background on the notation relating to the RL
paradigm, see Section A.1 in the Supplementary Materials.
of all natural world MDPs, and each MDP Di is sampled from M. We define M in terms of the state space ˆS that contains all possible conjunctions of high-level factors of variation necessary to generate any naturalistic observation in any Di 2 M. A natural world MDP Di is then one whose state space S corresponds to some subset of ˆS .",2.2. DARLA,[0],[0]
"In simple terms, we assume that there exists some shared underlying structure between the MDPs Di sampled from M. We contend that this is a reasonable assumption that permits inclusion of many interesting problems, including being able to characterise our own reality (Lake et al., 2016).
",2.2. DARLA,[0],[0]
"We now introduce notation for two state space variables that may in principle be used interchangeably within the source and target domain MDPs DS and DT – the agent observation state space So, and the agent’s internal latent state space Sz .2",2.2. DARLA,[0],[0]
"Soi in Di consists of raw (pixel) observations soi generated by the true world simulator from a sampled set of data generative factors ŝi, i.e. soi ⇠ Sim(̂si).",2.2. DARLA,[0],[0]
"ŝi is sampled by some distribution or process Gi on ˆS , ŝi ⇠ Gi( ˆS).
",2.2. DARLA,[0],[0]
"Using the newly introduced notation, domain adaptation scenarios can be described as having different sampling processes GS and GT such that ŝS ⇠ GS( ˆS) and ŝT ⇠ GT ( ˆS) for the source and target domains respectively, and then using these to generate different agent observation states soS ⇠ Sim(̂sS) and soT ⇠ Sim(̂sT).",2.2. DARLA,[0],[0]
"Intuitively, consider a source domain where oranges appear in blue rooms and apples appear in red rooms, and a target domain where the object/room conjunctions are reversed and oranges appear in red rooms and apples appear in blue rooms.",2.2. DARLA,[0],[0]
"While the true data generative factors of variation
ˆS remain the same - room colour (blue or red) and object type (apples and oranges) - the particular source and target distributions GS and GT differ.
",2.2. DARLA,[0],[0]
"Typically deep RL agents (e.g. Mnih et al., 2015; 2016) operating in an MDP Di 2 M learn an end-to-end mapping from raw (pixel) observations soi 2 Soi to actions ai 2 Ai (either directly or via a value function Qi(soi , ai) from which actions can be derived).",2.2. DARLA,[0],[0]
"In the process of doing so, the agent implicitly learns a function F : Soi !",2.2. DARLA,[0],[0]
Szi that maps the typically high-dimensional raw observations soi to typically low-dimensional latent states s z,2.2. DARLA,[0],[0]
i ; followed by a policy function ⇡i : Szi !,2.2. DARLA,[0],[0]
Ai that maps the latent states szi to actions ai 2 Ai.,2.2. DARLA,[0],[0]
"In the context of domain adaptation, if the agent learns a naive latent state mapping function FS : SoS !",2.2. DARLA,[0],[0]
"SzS on the source domain using reward signals to shape the representation learning, it is likely that FS will overfit to the source domain and will not generalise well to the target domain.",2.2. DARLA,[0],[0]
"Returning to our
2
Note that we do not assume these to be Markovian i.e. it is not necessarily the case that p(so t+1|sot )",2.2. DARLA,[0],[0]
"= p(sot+1|sot , sot 1, . . .",2.2. DARLA,[0],[0]
", so1), and similarly for sz .",2.2. DARLA,[0],[0]
"Note the index t here corresponds to time.
",2.2. DARLA,[0],[0]
"intuitive example, imagine an agent that has learnt a policy to pick up oranges and avoid apples on the source domain.",2.2. DARLA,[0],[0]
Such a source policy ⇡S is likely to be based on an entangled latent state space SzS of object/room conjunctions: oranges/blue !,2.2. DARLA,[0],[0]
"good, apples/red !",2.2. DARLA,[0],[0]
"bad, since this is arguably the most efficient representation for maximising expected rewards on the source task in the absence of extra supervision signals suggesting otherwise.",2.2. DARLA,[0],[0]
"A source policy ⇡S(a|szS ; ✓) based on such an entangled latent representation szS will not generalise well to the target domain without further fine-tuning, since FS(soS) 6= FS(soT ) and therefore crucially SzS 6=",2.2. DARLA,[0],[0]
SzT .,2.2. DARLA,[0],[0]
"On the other hand, since both ŝS ⇠ GS( ˆS) and ŝT ⇠ GT ( ˆS) are sampled from the same natural world state space
ˆS for the source and target domains respectively, it should be possible to learn a latent state mapping function ˆF :",2.2. DARLA,[0],[0]
"So ! SzŜ , which projects the agent observation state space So to a latent state space SzŜ expressed in terms of factorised data generative factors that are representative of the natural world i.e. Sz Ŝ ⇡ ˆS. Consider again our intuitive example, where
ˆF maps agent observations (soS : orange in a blue room) to a factorised or disentangled representation expressed in terms of the data generative factors (szŜ : room type = blue; object type = orange).",2.2. DARLA,[0],[0]
"Such a disentangled latent state mapping function should then directly generalise to both the source and the target domains, so that ˆF(soS) = ˆF(soT ) = szŜ .",2.2. DARLA,[0],[0]
"Since S z Ŝ is a disentangled representation of object and room attributes, the source policy ⇡S can learn a decision boundary that ignores the irrelevant room attributes: oranges !",2.2. DARLA,[0],[0]
"good, apples !",2.2. DARLA,[0],[0]
bad.,2.2. DARLA,[0],[0]
"Such a policy would then generalise well to the target domain out of the box, since ⇡S(a| ˆF(soS); ✓) = ⇡T (a| ˆF(soT ); ✓) = ⇡T (a|szŜ ; ✓).",2.2. DARLA,[0],[0]
"Hence, DARLA is based on the idea that a good quality
ˆF learnt exclusively on the source domain DS 2 M will zero-shot-generalise to all target domains Di 2 M, and therefore the source policy ⇡(a|SzŜ ; ✓) will also generalise to all target domains Di 2 M out of the box.
",2.2. DARLA,[0],[0]
"Next we describe each of the stages of the DARLA pipeline that allow it to learn source policies ⇡S that are robust to domain adaptation scenarios, despite being trained with no knowledge of the target domains (see Fig. 1 for a graphical representation of these steps): 1) Learn to see (unsupervised learning of FU ) – the task of inferring a factorised set of generative factors SzŜ =
ˆS from observations So is the goal of the extensive disentangled factor learning literature (e.g. Chen et al., 2016; Higgins et al., 2017).",2.2. DARLA,[0],[0]
"Hence, in stage one we learn a mapping FU : SoU ! SzU , where SzU ⇡ SzŜ (U stands for ‘unsupervised’) using an unsupervised model for learning disentangled factors that utilises observations collected by an agent with a random policy ⇡U from a visual pre-training
MDP DU 2 M. Note that we require sufficient variability of factors and their conjunctions in DU in order to have SzU ⇡ SzŜ ; 2) Learn to act (reinforcement learning of ⇡S in the source domain DS utilising previously learned FU ) – an agent that has learnt to see the world in stage one in terms of the natural data generative factors is now exposed to a source domain DS 2 M. The agent is tasked with learning the source policy ⇡S(a|szS ; ✓), where szS = FU (soS) ⇡ szŜ , via a standard reinforcement learning algorithm.",2.2. DARLA,[0],[0]
"Crucially, we do not allow FU to be modified (e.g. by gradient updates) during this phase; 3) Transfer (to a target domain DT ) – in the final step, we test how well the policy ⇡S learnt on the source domain generalises to the target domain DT 2 M in a zero-shot domain adaptation setting, i.e. the agent is evaluated on the target domain without retraining.",2.2. DARLA,[0],[0]
We compare the performance of policies learnt with a disentangled latent state SzŜ to various baselines where the latent state mapping function FU projects agent observations so to entangled latent state representations sz .,2.2. DARLA,[0],[0]
"In order to learn FU , DARLA utilises -VAE (Higgins et al., 2017), a state-of-the-art unsupervised model for automated discovery of factorised latent representations from raw image data.",2.3. Learning disentangled representations,[0],[0]
"-VAE is a modification of the variational autoencoder framework (Kingma & Welling, 2014; Rezende et al., 2014) that controls the nature of the learnt latent representations by introducing an adjustable hyperparameter to balance reconstruction accuracy with latent channel capacity and independence constraints.",2.3. Learning disentangled representations,[0],[0]
"It maximises the objective:
L(✓, ;x, z, ) =",2.3. Learning disentangled representations,[0],[0]
"Eq (z|x)[log p✓(x|z)] DKL(q (z|x)||p(z)) (1)
where , ✓ parametrise the distributions of the encoder and the decoder respectively.",2.3. Learning disentangled representations,[0],[0]
"Well-chosen values of - usually larger than one ( > 1) - typically result in more disentangled latent representations z by limiting the capacity of the latent information channel, and hence encouraging a more efficient factorised encoding through the increased pressure to match the isotropic unit Gaussian prior p(z) (Higgins et al., 2017).",2.3. Learning disentangled representations,[0],[0]
"The cost of increasing is that crucial information about the scene may be discarded in the latent representation z, particularly if that information takes up a small proportion of the observations x in pixel space.",2.3.1. PERCEPTUAL SIMILARITY LOSS,[0],[0]
"We encountered this issue in some of our tasks, as discussed in Section 3.1.",2.3.1. PERCEPTUAL SIMILARITY LOSS,[0],[0]
"The shortcomings of calculating the log-likelihood term
Eq (z|x)[log p✓(x|z)] on a per-pixel basis are known and have been addressed in the past by calculating the reconstruction cost in an abstract, high-level feature space given by another neural network model, such as a GAN (Goodfellow et al., 2014) or a pre-trained AlexNet (Krizhevsky et al., 2012; Larsen et al., 2016; Dosovitskiy & Brox, 2016; Warde-Farley & Bengio, 2017).",2.3.1. PERCEPTUAL SIMILARITY LOSS,[0],[0]
"In practice we found that pre-training a denoising autoencoder (Vincent et al., 2010) on data from the visual pre-training MDP DU 2 M worked best as the reconstruction targets for -VAE to match (see Fig. 1 for model architecture and Sec.",2.3.1. PERCEPTUAL SIMILARITY LOSS,[0],[0]
A.3.1 in Supplementary Materials for implementation details).,2.3.1. PERCEPTUAL SIMILARITY LOSS,[0],[0]
"The new -VAEDAE model was trained according to Eq. 2:
L(✓, ;x, z, )",2.3.1. PERCEPTUAL SIMILARITY LOSS,[0],[0]
"=Eq (z|x) kJ(ˆx) J(x)k 2 2
DKL(q (z|x)||p(z))",2.3.1. PERCEPTUAL SIMILARITY LOSS,[0],[0]
"(2)
where
ˆ x ⇠ p✓(x|z) and J : RW⇥H⇥C !",2.3.1. PERCEPTUAL SIMILARITY LOSS,[0],[0]
RN is the function that maps images from pixel space with dimensionality W ⇥H ⇥ C to a high-level feature space with dimensionality N given by a stack of pre-trained DAE layers up to a certain layer depth.,2.3.1. PERCEPTUAL SIMILARITY LOSS,[0],[0]
"Note that by replacing the pixel based reconstruction loss in Eq. 1 with high-level feature reconstruction loss in Eq. 2 we are no longer optimising the variational lower bound, and -VAEDAE with = 1 loses its equivalence to the Variational Autoencoder (VAE) framework as proposed by (Kingma & Welling, 2014; Rezende et al., 2014).",2.3.1. PERCEPTUAL SIMILARITY LOSS,[0],[0]
"In this setting, the only way to interpret is as a mixing coefficient that balances the capacity of the latent channel z of -VAEDAE against the pressure to match the high-level features within the pre-trained DAE.",2.3.1. PERCEPTUAL SIMILARITY LOSS,[0],[0]
"We used various RL algorithms (DQN, A3C and Episodic Control: Mnih et al., 2015; 2016; Blundell et al., 2016) to learn the source policy ⇡S during stage two of the pipeline using the latent states sz acquired by -VAE based models during stage one of the DARLA pipeline.",2.4. Reinforcement Learning Algorithms,[0],[0]
"Deep Q Network (DQN) (Mnih et al., 2015) is a variant of the Q-learning algorithm (Watkins, 1989) that utilises deep learning.",2.4. Reinforcement Learning Algorithms,[0],[0]
"It uses a neural network to parametrise an approximation for the action-value function Q(s, a; ✓) using parameters ✓.",2.4. Reinforcement Learning Algorithms,[0],[0]
"Asynchronous Advantage Actor-Critic (A3C) (Mnih et al., 2016) is an asynchronous implementation of the advantage actor-critic paradigm (Sutton & Barto, 1998; Degris & Sutton, 2012), where separate threads run in parallel and perform updates to shared parameters.",2.4. Reinforcement Learning Algorithms,[0],[0]
"The different threads each hold their own instance of the environment and have different exploration policies, thereby decorrelating parameter updates without the need for experience replay.",2.4. Reinforcement Learning Algorithms,[0],[0]
"Therefore, A3C is an online algorithm, whereas DQN learns its policy offline, resulting in different learning dynamics be-
tween the two algorithms.",2.4. Reinforcement Learning Algorithms,[0],[0]
"Model-Free Episodic Control (EC) (Blundell et al., 2016) was proposed as a complementary learning system to the other RL algorithms described above.",2.4. Reinforcement Learning Algorithms,[0],[0]
"The EC algorithm relies on near-determinism of state transitions and rewards in RL environments; in settings where this holds, it can exploit these properties to memorise which action led to high returns in similar situations in the past.",2.4. Reinforcement Learning Algorithms,[0],[0]
"Since in its simplest form EC relies on a lookup table, it learns good policies much faster than value-function-approximation based deep RL algorithms like DQN trained via gradient descent - at the cost of generality (i.e. potentially poor performance in non-deterministic environments).",2.4. Reinforcement Learning Algorithms,[0],[0]
"We also compared our approach to that of UNREAL (Jaderberg et al., 2017), a recently proposed RL algorithm which also attempts to utilise unsupervised data in the environment.",2.4. Reinforcement Learning Algorithms,[0],[0]
"The UNREAL agent takes as a base an LSTM A3C agent (Mnih et al., 2016) and augments it with a number of unsupervised auxiliary tasks that make use of the rich perceptual data available to the agent besides the (sometimes very sparse) extrinsic reward signals.",2.4. Reinforcement Learning Algorithms,[0],[0]
This auxiliary learning tends to improve the representation learnt by the agent.,2.4. Reinforcement Learning Algorithms,[0],[0]
See Sec.,2.4. Reinforcement Learning Algorithms,[0],[0]
A.6 in Supplementary Materials for further details of the algorithms above.,2.4. Reinforcement Learning Algorithms,[0],[0]
We evaluate the performance of DARLA on different task and environment setups that probe subtly different aspects of domain adaptation.,3. Tasks,[0],[0]
"As a reminder, in Sec. 2.2 we defined ˆS as a state space that contains all possible conjunctions of high-level factors of variation necessary to generate any naturalistic observation in any Di 2 M. During domain adaptation scenarios agent observation states are generated according to soS ⇠ SimS(̂sS) and soT ⇠ SimT(̂sT) for the source and target domains respectively, where ŝS and ŝT are sampled by some distributions or processes GS and GT according to ŝS ⇠ GS( ˆS)",3. Tasks,[0],[0]
"and ŝT ⇠ GT ( ˆS).
",3. Tasks,[0],[0]
"We use DeepMind Lab (Beattie et al., 2016) to test a version of domain adaptation setup where the source and target domain observation simulators are equal (Sim
S
= Sim
T
),
but the processes used to sample ŝS and ŝT are different (GS 6= GT ).",3. Tasks,[0],[0]
"We use the Jaco arm with a matching MuJoCo simulation environment (Todorov et al., 2012) in two domain adaptation scenarios: simulation to simulation (sim2sim) and simulation to reality (sim2real).",3. Tasks,[0],[0]
The sim2sim domain adaptation setup is relatively similar to DeepMind Lab i.e. the source and target domains differ in terms of processes GS and GT .,3. Tasks,[0],[0]
"However, there is a significant point of difference.",3. Tasks,[0],[0]
"In DeepMind Lab, all values of factors in the target domain, ŝT , are previously seen in the source domain; however, ŝS 6=",3. Tasks,[0],[0]
"ŝT as the conjunctions of
these factor values are different.",3. Tasks,[0],[0]
"In sim2sim, by contrast, novel factor values are experienced in the target domain (this accordingly also leads to novel factor conjunctions).",3. Tasks,[0],[0]
"Hence, DeepMind Lab may be considered to be assessing domain interpolation performance, whereas sim2sim tests domain extrapolation.
",3. Tasks,[0],[0]
"The sim2real setup, on the other hand, is based on identical processes GS = GT , but different observation simulators Sim
S 6=",3. Tasks,[0],[0]
"Sim T corresponding to the MuJoCo simulation and the real world, which results in the so-called ‘perceptual reality gap’ (Rusu et al., 2016).",3. Tasks,[0],[0]
More details of the tasks are given below.,3. Tasks,[0],[0]
DeepMind Lab is a first person 3D game environment with rich visuals and realistic physics.,3.1. DeepMind Lab,[0],[0]
"We used a standard seekavoid object gathering setup, where a room is initialised with an equal number of randomly placed objects of two different types.",3.1. DeepMind Lab,[0],[0]
"One of the object varieties is ‘good’ (its collection is rewarded +1), while the other is ‘bad’ (its collection is punished -1).",3.1. DeepMind Lab,[0],[0]
"The full state space
ˆS consisted of all conjunctions of two room types (pink and green based on the colour of the walls) and four object types (hat, can, cake and balloon) (see Fig. 2A).",3.1. DeepMind Lab,[0],[0]
"The source domain DS con-
tained environments with hats/cans presented in the green room, and balloons/cakes presented in either the green or the pink room.",3.1. DeepMind Lab,[0],[0]
The target domain DT contained hats/cans presented in the pink room.,3.1. DeepMind Lab,[0],[0]
In both domains cans and balloons were the rewarded objects.,3.1. DeepMind Lab,[0],[0]
1) Learn to see: we used -VAEDAE to learn the disentangled latent state representation sz that includes both the room and the object generative factors of variation within DeepMind Lab.,3.1. DeepMind Lab,[0],[0]
"We had to use the high-level feature space of a pre-trained DAE within the -VAEDAE framework (see Section 2.3.1), instead of the pixel space of vanilla - VAE , because we found that objects failed to reconstruct when using the values of necessary to disentangle the generative factors of variation within DeepMind Lab (see Fig. 2B).
",3.1. DeepMind Lab,[0],[0]
-VAEDAE was trained on observations soU collected by an RL agent with a simple wall-avoiding policy ⇡U (otherwise the training data was dominated by close up images of walls).,3.1. DeepMind Lab,[0],[0]
"In order to enable the model to learn F(soU ) ⇡ ˆS , it is important to expose the agent to at least a minimal set of environments that span the range of values for each factor, and where no extraneous correlations are added between different factors 3 (see Fig.",3.1. DeepMind Lab,[0],[0]
"2A, yellow).",3.1. DeepMind Lab,[0],[0]
See Section A.3.1 in Supplementary Materials for details of -VAEDAE training.,3.1. DeepMind Lab,[0],[0]
2) Learn to act: the agent was trained with the algorithms detailed in Section 2.4 on a seek-avoid task using the source domain (DS) conjunctions of object/room shown in Fig.,3.1. DeepMind Lab,[0],[0]
2A (green).,3.1. DeepMind Lab,[0],[0]
"Pre-trained -VAEDAE from stage one was used as the ‘vision’ part of various RL algorithms (DQN, A3C and Episodic Control: Mnih et al., 2015; 2016; Blundell et al., 2016) to learn a source policy ⇡S that picks up balloons and avoids cakes in both the green and the pink rooms, and picks up cans and avoids hats in the green rooms.",3.1. DeepMind Lab,[0],[0]
"See Section A.3.1 in Supplementary Materials for more details of the various versions of DARLA we have tried, each based on a different base RL algorithm.",3.1. DeepMind Lab,[0],[0]
3) Transfer: we tested the ability of DARLA to transfer the seek-avoid policy ⇡S it had learnt on the source domain in stage two using the domain adaptation condition DT illustrated in Figure 2A (red).,3.1. DeepMind Lab,[0],[0]
"The agent had to continue picking up cans and avoid hats in the pink room, even though these objects had only been seen in the green room during source policy training.",3.1. DeepMind Lab,[0],[0]
The optimal policy ⇡T is one that maintains the reward polarity from the source domain (cans are good and hats are bad).,3.1. DeepMind Lab,[0],[0]
"For further details, see Appendix A.2.1.
3
In our setup of DeepMind Lab domain adaptation task, the object and environment factors are supposed to be independent.",3.1. DeepMind Lab,[0],[0]
"In order to ensure that -VAE DAE learns a factorised representation that reflects this ground truth independence, we present observations of all possible conjunctions of room and individual object types.",3.1. DeepMind Lab,[0],[0]
"We used frames from an RGB camera facing a robotic Jaco arm, or a matching rendered camera view from a MuJoCo physics simulation environment (Todorov et al., 2012) to investigate the performance of DARLA in two domain adaptation scenarios: 1) simulation to simulation (sim2sim), and 2) simulation to reality (sim2real).",3.2. Jaco Arm and MuJoCo,[0],[0]
"The sim2real setup is of particular importance, since the progress that deep RL has brought to control tasks in simulation (Schulman et al., 2015; Mnih et al., 2016; Levine & Abbeel, 2014; Heess et al., 2015; Lillicrap et al., 2015; Schulman et al., 2016) has not yet translated as well to reality, despite various attempts (Tobin et al., 2017; Tzeng et al., 2016; Daftry et al., 2016; Finn et al., 2015; Rusu et al., 2016).",3.2. Jaco Arm and MuJoCo,[0],[0]
"Solving control problems in reality is hard due to sparse reward signals, expensive data acquisition and the attendant danger of breaking the robot (or its human minders) during exploration.
",3.2. Jaco Arm and MuJoCo,[0],[0]
"In both sim2sim and sim2real, we trained the agent to perform an object reaching policy where the goal is to place the end effector as close to the object as possible.",3.2. Jaco Arm and MuJoCo,[0],[0]
"While conceptually the reaching task is simple, it is a hard control problem since it requires correct inference of the arm and object positions and velocities from raw visual inputs.",3.2. Jaco Arm and MuJoCo,[0],[0]
1) Learn to see: -VAE was trained on observations collected in MuJoCo simulations with the same factors of variation as in DS .,3.2. Jaco Arm and MuJoCo,[0],[0]
"In order to enable the model to learn F(soU ) ⇡ ŝ, a reaching policy was applied to phantom objects placed in random positions - therefore ensuring that the agent learnt the independent nature of the arm position and object position (see Fig. 2C, left); 2) Learn to act: a feedforward-A3C based agent with the vision module pre-trained in stage one was taught a source reaching policy ⇡S towards the real object in simulation (see Fig. 2C (left) for an example frame, and Sec.",3.2. Jaco Arm and MuJoCo,[0],[0]
A.4 in Supplementary Materials for a fuller description of the agent),3.2. Jaco Arm and MuJoCo,[0],[0]
.,3.2. Jaco Arm and MuJoCo,[0],[0]
In the source domain DS the agent was trained on a distribution of camera angles and positions.,3.2. Jaco Arm and MuJoCo,[0],[0]
The colour of the tabletop on which the arm rests and the object colour were both sampled anew every episode.,3.2. Jaco Arm and MuJoCo,[0],[0]
3) Transfer:,3.2. Jaco Arm and MuJoCo,[0],[0]
"sim2sim: in the target domain, DT , the agent was faced with a new distribution of camera angles and positions with little overlap with the source domain distributions, as well as a completely held out set of object colours (see Fig. 2C, middle).",3.2. Jaco Arm and MuJoCo,[0],[0]
"sim2real: in the target domain DT the camera position and angle as well as the tabletop colour and object colour were sampled from the same distributions as seen in the source domain DS , but the target domain DT was now the real world.",3.2. Jaco Arm and MuJoCo,[0],[0]
"Many details present in the real world such as shadows, specularity, multiple light sources and so on are not modelled in the simulation;
3
-3
0
z1 z2 z3 z4 z5 z6
Object Arm close/far left/right close/far right/leftup/downwrist turn
3
-3
0
z1 z2 z3 z4
Disentangled Entangled
Figure 4.",3.2. Jaco Arm and MuJoCo,[0],[0]
Plot of traversals of -VAE on MuJoCo.,3.2. Jaco Arm and MuJoCo,[0],[0]
"Using a disentangled -VAE model, single latents directly control for the factors responsible for the object or arm placements.
",3.2. Jaco Arm and MuJoCo,[0],[0]
the physics engine is also not a perfect model of reality.,3.2. Jaco Arm and MuJoCo,[0],[0]
"Thus sim2real tests the ability of the agent to cross the perceptual-reality gap and generalise its source policy ⇡S to the real world (see Fig. 2C, right).",3.2. Jaco Arm and MuJoCo,[0],[0]
"For further details, see Appendix A.2.2.",3.2. Jaco Arm and MuJoCo,[0],[0]
We evaluated the robustness of DARLA’s policy ⇡S learnt on the source domain to various shifts in the input data distribution.,4. Results,[0],[0]
"In particular, we used domain adaptation scenarios based on the DeepMind Lab seek-avoid task and the Jaco arm reaching task described in Sec. 3.",4. Results,[0],[0]
On each task we compared DARLA’s performance to that of various baselines.,4. Results,[0],[0]
"We evaluated the importance of learning ‘good’ vision during stage one of the pipeline, i.e one that maps the input observations so to disentangled representations sz ⇡ ŝ. In order to do this, we ran the DARLA pipeline with different vision models: the encoders of a
disentangled -VAE 4 (the original DARLA), an entangled -VAE (DARLA
ENT
), and a denoising autoencoder
(DARLA
DAE
).",4. Results,[0],[0]
"Apart from the nature of the learnt rep-
resentations sz , DARLA and all versions of its baselines were equivalent throughout the three stages of our proposed pipeline in terms of architecture and the observed data distribution (see Sec. A.3 in Supplementary Materials for more details).
",4. Results,[0],[0]
"Figs. 3-4 display the degree of disentanglement learnt by the vision modules of DARLA and DARLA
ENT
on Deep-
Mind Lab and MuJoCo.",4. Results,[0],[0]
"DARLA’s vision learnt to independently represent environment variables (such as room colour-scheme and geometry) and object-related variables (change of object type, size, rotation) on DeepMind Lab (Fig. 3, left).",4. Results,[0],[0]
Disentangling was also evident in MuJoCo.,4. Results,[0],[0]
"Fig. 4, left, shows that DARLA’s single latent units zi learnt to represent different aspects of the Jaco arm, the object, and the camera.",4. Results,[0],[0]
"By contrast, in the representations learnt by DARLA
ENT
, each latent is responsible for changes to
both the environment and objects (Fig. 3, right) in DeepMind Lab, or a mixture of camera, object and/or arm movements (Fig. 4, right) in MuJoCo.
",4. Results,[0],[0]
The table in Fig. 5 shows the average performance (across different seeds) in terms of rewards per episode of the various agents on the target domain with no fine-tuning of the source policy ⇡S .,4. Results,[0],[0]
"It can be seen that DARLA is able to zero-shot-generalise significantly better than DARLA
ENT
or DARLA
DAE
, highlighting the importance of learning a
disentangled representation sz = szŜ during the unsupervised stage one of the DARLA pipeline.",4. Results,[0],[0]
"In particular, this also demonstrates that the improved domain transfer performance is not simply a function of increased exposure to training observations, as both DARLA
ENT
and DARLA
DAE
were exposed to the same data.",4. Results,[0],[0]
The results are mostly consistent across target domains and in most cases DARLA is significantly better than the second-best-performing agent.,4. Results,[0],[0]
"This holds in the sim2real task 5 , where being able to perform zero-shot policy transfer is highly valuable due to the particular difficulties of gathering data in the real world.
",4. Results,[0],[0]
"DARLA’s performance is particularly surprising as it actually preserves less information about the raw observations so than DARLA
ENT
and DARLA
DAE
.",4. Results,[0],[0]
"This is due to the
nature of the -VAE and how it achieves disentangling; the disentangled model utilised a significantly higher value of the hyperparameter than the entangled model (see Appendix A.3 for further details), which constrains the ca-
4
In this section of the paper, we use the term -VAE to refer to a standard -VAE for the MuJoCo experiments, and a -VAE
DAE
for the DeepMind Lab experiments (as described in
stage 1 of Sec. 3.1).
",4. Results,[0],[0]
"5
See https://youtu.be/sZqrWFl0wQ4 for example sim2sim and sim2real zero-shot transfer policies of DARLA and baseline A3C agent.
165 166
Figure 5.",4. Results,[0],[0]
Table: Zero-shot performance (avg. reward per episode) of the source policy ⇡ S in target domains within DeepMind Lab and Jaco/MuJoCo environments.,4. Results,[0],[0]
Baseline agent refers to vanilla DQN/A3C/EC (DeepMind Lab) or A3C (Jaco) agents.,4. Results,[0],[0]
See main text for more detailed model descriptions.,4. Results,[0],[0]
Figure:,4. Results,[0],[0]
"Correlation between zero-shot performance transfer performance on the DeepMind Lab task obtained by EC based DARLA and the level of disentanglement as measured by the transfer/disentanglement score (r = 0.6, p < 0.001)
pacity of the latent channel.",4. Results,[0],[0]
"Indeed, DARLA’s -VAE only utilises 8 of its possible 32 Gaussian latents to store observation-specific information for MuJoCo/Jaco (and 20 in DeepMind Lab), whereas DARLA
ENT
utilises all 32 for
both environments (as does DARLA
DAE
).
",4. Results,[0],[0]
"Furthermore, we examined what happens if DARLA’s vision (i.e. the encoder of the disentangled -VAE ) is allowed to be fine-tuned via gradient updates while learning the source policy during stage two of the pipeline.",4. Results,[0],[0]
"This is denoted by DARLA
FT
in the table in Fig. 5.",4. Results,[0],[0]
"We see
that it exhibits significantly worse performance than that of DARLA in zero-shot domain adaptation using an A3Cbased agent in all tasks.",4. Results,[0],[0]
"This suggests that a favourable initialisation does not make up for subsequent overfitting to the source domain for the on-policy A3C. However, the off-policy DQN-based fine-tuned agent performs very well.",4. Results,[0],[0]
"We leave further investigation of this curious effect for future work.
",4. Results,[0],[0]
"Finally, we compared the performance of DARLA to an UNREAL (Jaderberg et al., 2017) agent with the same architecture.",4. Results,[0],[0]
"Despite also exploiting the unsupervised data available in the source domain, UNREAL performed worse than baseline A3C on the DeepMind Lab domain adaptation task.",4. Results,[0],[0]
"This further demonstrates that use of unsupervised data in itself is not a panacea for transfer performance; it must be utilised in a careful and structured manner conducive to learning disentangled latent states sz = szŜ .
",4. Results,[0],[0]
"In order to quantitatively evaluate our hypothesis that disentangled representations are essential for DARLA’s performance in domain adaptation scenarios, we trained various DARLAs with different degrees of learnt disentanglement in sz by varying (of -VAE) during stage one of the pipeline.",4. Results,[0],[0]
"We then calculated the correlation between the performance of the EC-based DARLA on the DeepMind Lab domain adaptation task and the transfer metric, which approximately measures the quality of disentanglement of DARLA’s latent representations sz (see Sec. A.5.2
in Supplementary Materials).",4. Results,[0],[0]
"This is shown in the chart in Fig. 5; as can be seen, there is a strong positive correlation between the level of disentanglement and DARLA’s zeroshot domain transfer performance (r = 0.6, p < 0.001).
",4. Results,[0],[0]
"Having shown the robust utility of disentangled representations in agents for domain adaptation, we note that there is evidence that they can provide an important additional benefit.",4. Results,[0],[0]
"We found significantly improved speed of learning of ⇡S on the source domain itself, as a function of how disentangled the model was.",4. Results,[0],[0]
"The gain in data efficiency from disentangled representations for source policy learning is not the main focus of this paper so we leave it out of the main text; however, we provide results and discussion in Section A.7 in Supplementary Materials.",4. Results,[0],[0]
We have demonstrated the benefits of using disentangled representations in a deep RL setting for domain adaptation.,5. Conclusion,[0],[0]
"In particular, we have proposed DARLA, a multi-stage RL agent.",5. Conclusion,[0],[0]
"DARLA first learns a visual system that encodes the observations it receives from the environment as disentangled representations, in a completely unsupervised manner.",5. Conclusion,[0],[0]
"It then uses these representations to learn a robust source policy that is capable of zero-shot domain adaptation.
",5. Conclusion,[0],[0]
"We have demonstrated the efficacy of this approach in a range of domains and task setups: a 3D naturalistic firstperson environment (DeepMind Lab), a simulated graphics and physics engine (MuJoCo), and crossing the simulation to reality gap (MuJoCo to Jaco sim2real).",5. Conclusion,[0],[0]
"We have also shown that the effect of disentangling is consistent across very different RL algorithms (DQN, A3C, EC), achieving significant improvements over the baseline algorithms (median 2.7 times improvement in zero-shot transfer across tasks and algorithms).",5. Conclusion,[0],[0]
"To the best of our knowledge, this is the first comprehensive empirical demonstration of the strength of disentangled representations for domain adaptation in a deep RL setting.",5. Conclusion,[0],[0]
Domain adaptation is an important open problem in deep reinforcement learning (RL).,abstractText,[0],[0]
"In many scenarios of interest data is hard to obtain, so agents may learn a source policy in a setting where data is readily available, with the hope that it generalises well to the target domain.",abstractText,[0],[0]
"We propose a new multi-stage RL agent, DARLA (DisentAngled Representation Learning Agent), which learns to see before learning to act.",abstractText,[0],[0]
DARLA’s vision is based on learning a disentangled representation of the observed environment.,abstractText,[0],[0]
"Once DARLA can see, it is able to acquire source policies that are robust to many domain shifts even with no access to the target domain.",abstractText,[0],[0]
"DARLA significantly outperforms conventional baselines in zero-shot domain adaptation scenarios, an effect that holds across a variety of RL environments (Jaco arm, DeepMind Lab) and base RL algorithms (DQN, A3C and EC).",abstractText,[0],[0]
DARLA: Improving Zero-Shot Transfer in Reinforcement Learning,title,[0],[0]
"In the context of machine learning, it is not uncommon to have to repeatedly optimize a set of functions that are fundamentally related to each other.",1. Introduction,[0],[0]
"In this paper, we focus on a class of functions called submodular functions.",1. Introduction,[0],[0]
These functions exhibit a mathematical diminishing returns property that allows us to find nearly-optimal solutions in linear time.,1. Introduction,[0],[0]
"However, modern datasets are growing so large that even linear time solutions can be computationally expensive.",1. Introduction,[0],[0]
"Ideally, we want to find a sublinear summary of the given dataset so that optimizing these related functions over this reduced subset is nearly as effective, but not nearly as expensive, as optimizing them over the full dataset.
",1. Introduction,[0],[0]
"As a concrete example, suppose Uber is trying to give their
*Equal contribution 1Department of Computer Science, Yale University, New Haven, Connecticut, USA 2Google Research, Zurich, Switzerland.",1. Introduction,[0],[0]
"Correspondence to: Ehsan Kazemi <ehsan.kazemi@yale.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
drivers suggested waiting locations across New York City based on historical rider pick-ups.,1. Introduction,[0],[0]
"Even if they discretize the potential waiting locations to just include points at which pick-ups have occurred in the past, there are still hundreds of thousands, if not millions, of locations to consider.",1. Introduction,[0],[0]
"If they wish to update these ideal waiting locations every day (or at any routine interval), it would be invaluable to be able to drastically reduce the number of locations that need to be evaluated, and still achieve nearly optimal results.
",1. Introduction,[0],[0]
"In this scenario, each day would have a different function that quantifies the value of a set of locations for that particular day.",1. Introduction,[0],[0]
"For example, in the winter months, spots near ice skating rinks would be highly valuable, while in the summer months, waterfront venues might be more prominent.",1. Introduction,[0],[0]
"On the other hand, major tourist destinations like Times Square will probably be busy year-round.
",1. Introduction,[0],[0]
"In other words, although the most popular pick-up locations undoubtedly vary over time, there is also some underlying distribution of the user behavior that remains relatively constant and ties the various days together.",1. Introduction,[0],[0]
"This means that even though the functions for future days are technically unknown, if we can select a good reduced subset of candidate locations based on the functions derived from historical data, then this same reduced subset should perform well on future functions that we cannot explicitly see yet.
",1. Introduction,[0],[0]
"In more mathematical terms, consider some unknown distribution of functions D and a ground set Ω of n elements to pick from.",1. Introduction,[0],[0]
We want to select a subset S of ` elements (with ` n) such that optimizing functions (drawn from this distribution D) over the reduced subset S is comparable to optimizing them over the entire ground set,1. Introduction,[0],[0]
"Ω.
This problem was first introduced by Balkanski et al. (2016) as two-stage submodular maximization.",1. Introduction,[0],[0]
This name comes from the idea that the overall framework can be viewed as two separate stages.,1. Introduction,[0],[0]
"First, we want to use the given functions to select a representative subset S, that is ideally sublinear in size of the entire ground set Ω.",1. Introduction,[0],[0]
"In the second stage, for any functions drawn from this same distribution, we can optimize over S, which will be much faster than optimizing over Ω.
Our Contributions.",1. Introduction,[0],[0]
"In today’s era of massive data, an algorithm is rarely practical if it is not scalable.",1. Introduction,[0],[0]
"In this
paper, we build on existing work to provide solutions for two-stage submodular maximization in both the streaming and distributed settings.",1. Introduction,[0],[0]
Table 1 summarizes the theoretical results of this paper and compares them with the previous state of the art.,1. Introduction,[0],[0]
The proofs of all the theoretical results are deferred to the Supplementary Material.,1. Introduction,[0],[0]
Data summarization is one of the most natural applications that falls under the umbrella of submodularity.,2. Related Work,[0],[0]
"As such, there are many existing works applying submodular theory to a variety of important summarization settings.",2. Related Work,[0],[0]
"For example, Mirzasoleiman et al. (2013) used an exemplar-based clustering approach to select representative images from the Tiny Images dataset (Torralba et al., 2008).",2. Related Work,[0],[0]
"Kirchhoff & Bilmes (2014) and Feldman et al. (2018) also worked on submodular image summarization, while Lin & Bilmes (2011) and Wei et al. (2013) focused on document summarization.
",2. Related Work,[0],[0]
"In addition to data summarization, submodularity appears in a wide variety of other machine learning applications including variable selection (Krause & Guestrin, 2005), recommender systems (Gabillon et al., 2013), crowd teaching (Singla et al., 2014), neural network interpretability (Elenberg et al., 2017), robust optimization (Kazemi et al., 2017), network monitoring (Gomez Rodriguez et al., 2010), and influence maximization in social networks (Kempe et al., 2003).
",2. Related Work,[0],[0]
There have also been many successful efforts in scalable submodular optimization.,2. Related Work,[0],[0]
For our distributed implementation we will primarily build on the framework developed by Barbosa et al. (2015).,2. Related Work,[0],[0]
"Other similar algorithms include works by Mirzasoleiman et al. (2013) and Mirrokni & Zadimoghaddam (2015), as well as Kumar et al. (2015).",2. Related Work,[0],[0]
"In terms of the streaming setting, there are two existing works we will focus on: Badanidiyuru et al. (2014) and Buchbinder et al. (2015).",2. Related Work,[0],[0]
"The key difference between the two is that Badanidiyuru et al. (2014) relies on thresholding and will terminate as soon as k elements are selected from the stream, while Buchbinder et al. (2015) will continue through the end of the stream, swapping elements in and out when required.
",2. Related Work,[0],[0]
"Repeated optimization of related submodular functions has
been a well-studied problem with works on structured prediction (Lin & Bilmes, 2012), submodular bandits (Yue & Guestrin, 2011; Chen et al., 2017), and online submodular optimization (Jegelka & Bilmes, 2011).",2. Related Work,[0],[0]
"However, unlike our work, these approaches are not concerned with data summarization as a key pre-processing step.
",2. Related Work,[0],[0]
The problem of two-stage submodular maximization was first introduced by Balkanski et al. (2016).,2. Related Work,[0],[0]
"They present two algorithms with strong approximation guarantees, but both runtimes are prohibitively expensive.",2. Related Work,[0],[0]
"Recently, Stan et al. (2017) presented a new algorithm known as REPLACEMENT-GREEDY that improved the approximation guarantee from 12 (1 − 1e ) to 12 (1 − 1e2 ) and the run time from O(km`n2 log(n)) to O(km`n).",2. Related Work,[0],[0]
"They also show that, under mild conditions over the functions, maximizing over the sublinear summary can be arbitrarily close to maximizing over the entire ground set.",2. Related Work,[0],[0]
"In a nutshell, their method indirectly constructs the summary S by greedily building up solutions Ti for each given function fi simultaneously over ` rounds.
",2. Related Work,[0],[0]
"Although Balkanski et al. (2016) and Stan et al. (2017) presented centralized algorithms with constant factor approximation guarantees, there is a dire need for scalable solutions in order for the algorithm to be practically useful.",2. Related Work,[0],[0]
"In particular, the primary purpose of two-stage submodular maximization is to tackle problems where the dataset is too large to be repeatedly optimized by simple greedy-based approaches.",2. Related Work,[0],[0]
"As a result, in many cases, the datasets can be so large that existing algorithms cannot even be run once.",2. Related Work,[0],[0]
"The greedy approach requires that the entire data must fit into main memory, which may not be possible, thus requiring a streaming-based solution.",2. Related Work,[0],[0]
"Furthermore, even if we have enough memory, the problem may simply be so large that it requires a distributed approach in order to run in any reasonable amount of time.",2. Related Work,[0],[0]
"In general, if we want to optimally choose ` out of n items, we need to consider every single one of the exponentially many possibilities.",3. Problem Definition,[0],[0]
"This makes the problem intractable for any reasonable number of elements, let alone the billions of elements that are common in modern datasets.",3. Problem Definition,[0],[0]
"Fortunately,
many data summarization formulations satisfy an intuitive diminishing returns property known as submodularity.
",3. Problem Definition,[0],[0]
"More formally, a set function f : 2V → R is submodular (Fujishige, 2005; Krause & Golovin, 2012)",3. Problem Definition,[0],[0]
"if, for all sets A ⊆ B ⊆ V and every element v ∈ V \ B, we have f(A + v)",3. Problem Definition,[0],[0]
− f(A) ≥ f(B + v),3. Problem Definition,[0],[0]
"− f(B).1 That is, the marginal contribution of any element v to the value of f(A) diminishes as the set A grows.
",3. Problem Definition,[0],[0]
"Moreover, a submodular function f is said to be monotone if f(A) ≤ f(B) for all sets A ⊆ B ⊆ V .",3. Problem Definition,[0],[0]
"That is, adding elements to a set cannot decrease its value.",3. Problem Definition,[0],[0]
"Thanks to a celebrated result by Nemhauser et al. (1978), we know that if our function f is monotone submodular, then the classical greedy algorithm will obtain a (1 − 1/e)-approximation to the optimal value.",3. Problem Definition,[0],[0]
"Therefore, we can nearly-optimize monotone submodular functions in linear time.
",3. Problem Definition,[0],[0]
"Now we formally re-state the problem we are going to solve.
",3. Problem Definition,[0],[0]
Problem Statement.,3. Problem Definition,[0],[0]
Consider some unknown distribution D of monotone submodular functions and a ground set Ω of n elements to choose from.,3. Problem Definition,[0],[0]
"We want to select a set S of at most ` items that maximizes the following function:
G(S) =",3. Problem Definition,[0],[0]
"Ef∼D[ max T⊆S,|T |≤k f(T )].",3. Problem Definition,[0],[0]
"(1)
That is, the set S we choose should be optimal in expectation over all functions in this distribution D. However, in general, the distribution D is unknown and we only have access to a small set of functions F = (f1, . . .",3. Problem Definition,[0],[0]
", fm) drawn from D. Therefore, the best approximation we have is to optimize the following related function:
Gm(S) = 1
",3. Problem Definition,[0],[0]
m m∑ i=1,3. Problem Definition,[0],[0]
"max T∗i ⊆S,|T∗i |≤k fi(T ∗",3. Problem Definition,[0],[0]
i ).,3. Problem Definition,[0],[0]
"(2)
Stan et al. (2017, Theorem 1) shows that with enough sample functions, Gm(S) becomes an arbitrarily good approximation to G(S).
",3. Problem Definition,[0],[0]
"To be clear, each T ∗i ⊂ S is the corresponding size k optimal solution for fi.",3. Problem Definition,[0],[0]
"However, in general we cannot find the true optimal T ∗i , so throughout the paper we will use Ti to denote the approximately-optimal size k solution we select for each fi.",3. Problem Definition,[0],[0]
"Table 2 (Appendix A) summarizes the important terminology and can be used as a reference, if needed.
",3. Problem Definition,[0],[0]
"It is very important to note that although each function fi is monotone submodular, G(S) is not submodular (Balkanski et al., 2016), and thus using the regular greedy algorithm to directly build up S will give no theoretical guarantees.",3. Problem Definition,[0],[0]
"We also note that although G(S) is an instance of an XOS function (Feige, 2009), existing methods that use the XOS property would require an exponential number of evaluations in this scenario (Stan et al., 2017).
",3. Problem Definition,[0],[0]
"1For notational convenience, we use A+ v = A ∪ {v}.",3. Problem Definition,[0],[0]
"In many applications, the data naturally arrives in a streaming fashion.",4. Streaming Algorithm,[0],[0]
"This may be because the data is too large to fit in memory, or simply because the data is arriving faster than we can store it.",4. Streaming Algorithm,[0],[0]
"Therefore, in the streaming setting we are shown one element at a time and we must immediately decide whether or not to keep this element.",4. Streaming Algorithm,[0],[0]
"There is a limited number of elements we can hold at any one time and once an element is rejected it cannot be brought back.
",4. Streaming Algorithm,[0],[0]
There are two general approaches for submodular maximization (under the cardinality constraint k) in the streaming setting: (i) Badanidiyuru et al. (2014) introduced a thresholding-based framework where each element from the stream is added only if its marginal value is at least 12k of the optimum value.,4. Streaming Algorithm,[0],[0]
"The optimum is usually not known a priori, but they showed that with only a logarithmic increase in memory requirement, it is possible to efficiently guess the optimum value.",4. Streaming Algorithm,[0],[0]
(ii) Buchbinder et al. (2015) introduced streaming submodular maximization with preemption.,4. Streaming Algorithm,[0],[0]
"At each step, they keep a solution A of size k with value f(A).",4. Streaming Algorithm,[0],[0]
Each incoming element is added if and only if it can be exchanged with a current element of A for a net gain of at least f(A)k .,4. Streaming Algorithm,[0],[0]
"In this paper, we combine these two approaches in a novel and non trivial way in order to design a streaming algorithm (called REPLACEMENT-STREAMING) for the two-stage submodular maximization problem.
",4. Streaming Algorithm,[0],[0]
"The goal of REPLACEMENT-STREAMING is to pick a set S of at most ` elements from the data stream, where we keep sets Ti ⊆ S, 1 ≤",4. Streaming Algorithm,[0],[0]
i ≤ m as the solutions for functions fi.,4. Streaming Algorithm,[0],[0]
"We continue to process elements until one of the two following conditions holds: (i) ` elements are chosen, or (ii) the data stream ends.",4. Streaming Algorithm,[0],[0]
This algorithm starts from empty sets S and {Ti}.,4. Streaming Algorithm,[0],[0]
"For every incoming element ut, we use the subroutine EXCHANGE to determine whether we should keep that element or not.",4. Streaming Algorithm,[0],[0]
"To formally describe EXCHANGE, we first need to define a few notations.
",4. Streaming Algorithm,[0],[0]
We define the marginal gain of adding an element x to a set A as follows: fi(x|A) = fi(x + A) − fi(A).,4. Streaming Algorithm,[0],[0]
"For an element x and set A, REPi(x,A) is an element of A such that removing it from A and replacing it with x results in the largest gain for function fi, i.e.,
REPi(x,A) = arg max y∈A fi(A+ x− y)− fi(A).",4. Streaming Algorithm,[0],[0]
"(3)
The value of this largest gain is represented by ∆i(x,A) = fi(A+",4. Streaming Algorithm,[0],[0]
x−,4. Streaming Algorithm,[0],[0]
"REPi(x,A))− fi(A).",4. Streaming Algorithm,[0],[0]
"(4)
We define the gain of an element x with respect to a set A as follows:
∇i(x,A)",4. Streaming Algorithm,[0],[0]
=,4. Streaming Algorithm,[0],[0]
"{
1{fi(x|A)≥(α/k)·fi(A)}fi(x|A)",4. Streaming Algorithm,[0],[0]
"if |A| < k, 1{∆i(x,A)≥(α/k)·fi(A)}∆i(x,A) o.w.,
where 1 is the indicator function",4. Streaming Algorithm,[0],[0]
.,4. Streaming Algorithm,[0],[0]
"That is, ∇i(x,A) tells us how much we can increase the value of fi(A) by either
Algorithm 1 EXCHANGE 1:",4. Streaming Algorithm,[0],[0]
"Input: u, S, {Ti}, τ and α {∇i terms use α} 2: if |S| < ` then 3: if 1m ∑m i=1∇i(u, Ti) ≥ τ then
4: S ← S + u 5: for 1 ≤",4. Streaming Algorithm,[0],[0]
"i ≤ m do 6: if∇i(u, Ti) > 0",4. Streaming Algorithm,[0],[0]
"then 7: if |Ti| < k then 8: Ti ← Ti + u 9: else
10: Ti ← Ti + u− REP(u, Ti)
adding x to A (if |A| < k) or optimally swapping it in (if |A| = k).",4. Streaming Algorithm,[0],[0]
"However, if this potential increase is less than α k · fi(A), then ∇i(x,A) = 0.",4. Streaming Algorithm,[0],[0]
"In other words, if the gain of an element does not pass a threshold of αk · fi(A), we consider its contribution to be 0.
",4. Streaming Algorithm,[0],[0]
An incoming element is picked if the average of the ∇i terms is larger than or equal to a threshold τ .,4. Streaming Algorithm,[0],[0]
"Indeed, for ut, the EXCHANGE routine computes the average gain 1 m ∑m i=1∇i(ut, Ti).",4. Streaming Algorithm,[0],[0]
"If this average gain is at least τ , then ut is added to S; ut is also added to all sets Ti with ∇i(ut, Ti) > 0.",4. Streaming Algorithm,[0],[0]
Algorithm 1 explains EXCHANGE in detail.,4. Streaming Algorithm,[0],[0]
Now we define the optimum solution to Eq.,4. Streaming Algorithm,[0],[0]
"(2) by
Sm,` = arg max S⊆Ω,|S|≤`
1
m m∑ i=1 max |T |≤k,T⊆S fi(T ),
where the optimum solution to each function is defined by
Sm,`i = arg max S⊆Sm,`,|S|≤k fi(S).
",4. Streaming Algorithm,[0],[0]
"We define OPT = 1m ∑m i=1 fi(S m,` i ).
",4. Streaming Algorithm,[0],[0]
"In Section 4.1, we assume that the value of OPT is known a priori.",4. Streaming Algorithm,[0],[0]
"This allows us to design REPLACEMENTSTREAMING-KNOW-OPT, which has a constant factor approximation guarantee.",4. Streaming Algorithm,[0],[0]
"Furthermore, in Section 4.2, we show how we can efficiently guess the value of OPT by a moderate increase in the memory requirement.",4. Streaming Algorithm,[0],[0]
This enables us to finally explain REPLACEMENT-STREAMING.,4. Streaming Algorithm,[0],[0]
"If OPT is somehow known a priori, we can use REPLACEMENT-STREAMING-KNOW-OPT.",4.1. Knowing OPT,[0],[0]
"As shown in Algorithm 2, we begin with empty sets S and {Ti}.",4.1. Knowing OPT,[0],[0]
"For each incoming element ut, it uses EXCHANGE to update sets S and {Ti}.",4.1. Knowing OPT,[0],[0]
The threshold parameter τ in EXCHANGE is set to OPTβ` for a constant value of β.,4.1. Knowing OPT,[0],[0]
"This threshold guarantees that if an element is added to S, then the average of functions fi over Ti’s is increased by a value of at least OPTβ` .",4.1. Knowing OPT,[0],[0]
"Therefore, if we end up with ` elements in S, we guarantee that 1m ∑m i=1",4.1. Knowing OPT,[0],[0]
fi(Ti),4.1. Knowing OPT,[0],[0]
≥ OPTβ .,4.1. Knowing OPT,[0],[0]
"On the other hand, if |S| < `, we are still able to prove that our algorithm has picked good
Algorithm 2 REPLACEMENT-STREAMING-KNOW-OPT 1: Input: OPT, α and β 2: Output: Sets S and {Ti}1≤i≤m, where Ti ⊂ S 3: S ← ∅ and 4: Ti ← ∅ for all 1 ≤",4.1. Knowing OPT,[0],[0]
"i ≤ m 5: for every arriving element ut do 6: EXCHANGE(ut, S, {Ti}, OPTβ` , α) 7: Return: S and {Ti}1≤i≤m
enough elements such that 1m ∑m i=1",4.1. Knowing OPT,[0],[0]
fi(Ti) ≥ α·(β−1)·OPT β·((α+1)2+α) .,4.1. Knowing OPT,[0],[0]
The pseudocode of REPLACEMENT-STREAMING-KNOWOPT is provided in Algorithm 2.,4.1. Knowing OPT,[0],[0]
Theorem 1.,4.1. Knowing OPT,[0],[0]
"The approximation factor of REPLACEMENT-STREAMING-KNOW-OPT is at least min{ α(β−1)β·((α+1)2+α) , 1β }.",4.1. Knowing OPT,[0],[0]
"Hence, for α = 1 and β = 6 the competitive ratio is at least 1/6.",4.1. Knowing OPT,[0],[0]
"In this section, we discuss ideas on how to efficiently guess the value of OPT, which is generally not known a priori.",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"First consider Lemma 1, which provides bounds on OPT.",4.2. Guessing OPT in the Streaming Setting,[0],[0]
Lemma 1.,4.2. Guessing OPT in the Streaming Setting,[0],[0]
Assume δ,4.2. Guessing OPT in the Streaming Setting,[0],[0]
= 1m maxu∈Ω ∑m i=1,4.2. Guessing OPT in the Streaming Setting,[0],[0]
fi(u).,4.2. Guessing OPT in the Streaming Setting,[0],[0]
"Then we have δ ≤ OPT ≤ ` · δ.
",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"Now consider the following set:
Γ = {(1 + )",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"l | l ∈ Z, δ 1 + ≤",4.2. Guessing OPT in the Streaming Setting,[0],[0]
(1 + ),4.2. Guessing OPT in the Streaming Setting,[0],[0]
l ≤,4.2. Guessing OPT in the Streaming Setting,[0],[0]
"` · δ}
We define τl = (1 + )",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"l. From Lemma 1, we know that one of the τl ∈ Γ is a good estimate of OPT.",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"More formally, there exists a τl ∈ Γ such that OPT1+ ≤ τl ≤ OPT.",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"For this reason, we should run parallel instances of Algorithm 2, one for each τl ∈ Γ.",4.2. Guessing OPT in the Streaming Setting,[0],[0]
The number of such thresholds is O( log ` ).,4.2. Guessing OPT in the Streaming Setting,[0],[0]
"The final answer is the best solution obtained among all the instances.
",4.2. Guessing OPT in the Streaming Setting,[0],[0]
Note that we do not know the value of δ in advance.,4.2. Guessing OPT in the Streaming Setting,[0],[0]
"So we would need to make one pass over the data to learn δ, which is not possible in the streaming setting.",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"The question is, can we get a good enough estimate of δ within a single pass over the data?",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"Let’s define δt = 1m maxut′ ,t′≤t ∑m i=1",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"fi(u
t′)",4.2. Guessing OPT in the Streaming Setting,[0],[0]
as our current guess for the maximum value of δ,4.2. Guessing OPT in the Streaming Setting,[0],[0]
.,4.2. Guessing OPT in the Streaming Setting,[0],[0]
"Unfortunately, getting δt as an estimate of δ does not resolve the problem.",4.2. Guessing OPT in the Streaming Setting,[0],[0]
This is due to the fact that a newly instantiated threshold τ could potentially have already seen elements with additive value of τ/(β`).,4.2. Guessing OPT in the Streaming Setting,[0],[0]
"For this reason, we instantiate thresholds for an increased range of δt/(1 + )",4.2. Guessing OPT in the Streaming Setting,[0],[0]
≤ τl ≤ ` · β · δt.,4.2. Guessing OPT in the Streaming Setting,[0],[0]
"To show that this new range would solve the problem, first consider the next lemma.",4.2. Guessing OPT in the Streaming Setting,[0],[0]
Lemma 2.,4.2. Guessing OPT in the Streaming Setting,[0],[0]
"For the maximum gain of an incoming element ut, we have 1m ∑m i=1∇i(ut, T t−1i ) ≤ δt.
",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"We need to show that for a newly instantiated threshold τ at time t+ 1, the gain of all elements which arrived before
Algorithm 3 REPLACEMENT-STREAMING
1: Γ0 = {(1 + )l|l ∈ Z} 2: For each τ ∈ Γ0 set Sτ ← ∅ and Tτ,i ← ∅ for all
1 ≤ i ≤ m {Maintain the sets lazily} 3: δ0 ← 0 4: for every arriving element ut do 5: δt = max{δt−1, 1m ∑m i=1",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"fi(u
t)}",4.2. Guessing OPT in the Streaming Setting,[0],[0]
6:,4.2. Guessing OPT in the Streaming Setting,[0],[0]
Γt = {(1 + ),4.2. Guessing OPT in the Streaming Setting,[0],[0]
"l | l ∈ Z, δt(1+ )·",4.2. Guessing OPT in the Streaming Setting,[0],[0]
β·` ≤ (1 + ),4.2. Guessing OPT in the Streaming Setting,[0],[0]
"l ≤ δt} 7: Delete all Sτ and Tτ,i such that τ /∈",4.2. Guessing OPT in the Streaming Setting,[0],[0]
Γt 8: for all τ ∈,4.2. Guessing OPT in the Streaming Setting,[0],[0]
"Γt do 9: EXCHANGE(ut, Sτ , {Tτ,i}1≤i≤m, τ, α)
10: Return: arg maxτ∈Γn{ 1m ∑m i=1",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"fi(Tτ,i)}
time t+ 1 is less than τ ; therefore this new instance of the algorithm would not have picked them if it was instantiated from the beginning.",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"To prove this, note that since τ is a new threshold at time t + 1, we have τ >",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"`·β·δ t
β·` = δ t.
From Lemma 2 we conclude that the marginal gain of all the ut ′ , t′ ≤ t is less than τ and EXCHANGE would not have picked them.",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"The REPLACEMENT-STREAMING algorithm is shown pictorially in Figure 1 and the pseudocode is given in Algorithm 3.
",4.2. Guessing OPT in the Streaming Setting,[0],[0]
Theorem 2.,4.2. Guessing OPT in the Streaming Setting,[0],[0]
"Algorithm 3 satisfies the following properties:
• It outputs sets S and {Ti} ⊂ S for 1 ≤",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"i ≤ m, such that |S| ≤",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"`, |Ti| ≤ k and 1m ∑m i=1",4.2. Guessing OPT in the Streaming Setting,[0],[0]
fi(Ti),4.2. Guessing OPT in the Streaming Setting,[0],[0]
"≥
min{ α(β−1)β((α+1)2+α) , 1β(1+ )} · OPT.
",4.2. Guessing OPT in the Streaming Setting,[0],[0]
• For α = 1 and β = 6+ 1+ the approximation factor is at least 16+ .,4.2. Guessing OPT in the Streaming Setting,[0],[0]
"For = 1.0 the approximation factor is 1/7.
",4.2. Guessing OPT in the Streaming Setting,[0],[0]
• It makes one pass over the dataset and stores at most O( ` log ` ) elements.,4.2. Guessing OPT in the Streaming Setting,[0],[0]
"The update time per each element is O(km log ` ).
",4.2. Guessing OPT in the Streaming Setting,[0],[0]
Algorithm 4 REPLACEMENT-DISTRIBUTED 1: for e ∈ Ω do 2: Assign e to a machine chosen uniformly at random 3:,4.2. Guessing OPT in the Streaming Setting,[0],[0]
"Run REPLACEMENT-GREEDY on each machine l to
obtain Sl and {T li } for 1 ≤",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"i ≤ m 4: S, {Ti} ← arg maxSl,{T li } 1 m ∑m i=1",4.2. Guessing OPT in the Streaming Setting,[0],[0]
fi(T,4.2. Guessing OPT in the Streaming Setting,[0],[0]
"l i )
5: S′, {T ′i} ← REPLACEMENT-GREEDY( ⋃ l S
l) 6:",4.2. Guessing OPT in the Streaming Setting,[0],[0]
Return: arg max{ 1m ∑m i=1,4.2. Guessing OPT in the Streaming Setting,[0],[0]
"fi(Ti), 1 m ∑m i=1",4.2. Guessing OPT in the Streaming Setting,[0],[0]
fi(T ′,4.2. Guessing OPT in the Streaming Setting,[0],[0]
i )},4.2. Guessing OPT in the Streaming Setting,[0],[0]
"In recent years, there have been several successful approaches to the problem of distributed submodular maximization (Kumar et al., 2015; Mirzasoleiman et al., 2013; Mirrokni & Zadimoghaddam, 2015; Barbosa et al., 2015).",5. Distributed Algorithm,[0],[0]
"Specifically, Barbosa et al. (2015) proved that the following simple procedure results in a distributed algorithm with a constant factor approximation guarantee: (i) randomly split the data amongst M machines, (ii) run the classical greedy on each machine and pass outputs to a central machine, (iii) run another instance of the greedy algorithm over the union of all the collected outputs from all M machines, and (iv) output the maximizing set amongst all the collected solutions.",5. Distributed Algorithm,[0],[0]
"Although our objective function G(S) is not submodular, we use a similar framework and still manage to prove that our algorithms achieve constant factor approximations to the optimal solution.
",5. Distributed Algorithm,[0],[0]
"In REPLACEMENT-DISTRIBUTED (Algorithm 4), a central machine first randomly partitions data among M machines.",5. Distributed Algorithm,[0],[0]
"Next, each machine runs REPLACEMENT-GREEDY (Stan et al., 2017) on its assigned data.",5. Distributed Algorithm,[0],[0]
"The outputs Sl, {T li } of all the machines are sent to the central machine, which runs another instance of REPLACEMENT-GREEDY over the union of all the received answers.",5. Distributed Algorithm,[0],[0]
"Finally, the highest value set amongst all collected solutions is returned as the final answer.",5. Distributed Algorithm,[0],[0]
"See Appendix F for a detailed explanation of REPLACEMENT-GREEDY.
Theorem 3.",5. Distributed Algorithm,[0],[0]
"The REPLACEMENT-DISTRIBUTED algorithm outputs sets S∗, {T ∗i } ⊂ S, with |S∗| ≤ `, |T ∗i | ≤ k, such that
E[ 1
m m∑ i=1 fi(T ∗",5. Distributed Algorithm,[0],[0]
i )],5. Distributed Algorithm,[0],[0]
"≥ α 2 · OPT,
where α = 12 (1− 1e2 ).",5. Distributed Algorithm,[0],[0]
"The time complexity of algorithm is O(km`n/M + Mkm`2).
",5. Distributed Algorithm,[0],[0]
"Unfortunately, for very large datasets, the time complexity of REPLACEMENT-GREEDY could be still prohibitive.",5. Distributed Algorithm,[0],[0]
"For this reason, we can use a modified version of REPLACEMENT-STREAMING (called REPLACEMENTPSEUDO-STREAMING) to design an even more scalable distributed algorithm.",5. Distributed Algorithm,[0],[0]
"This algorithm receives all elements in a centralized way, but it uses a predefined order to generate a (pseudo) stream before processing the data.",5. Distributed Algorithm,[0],[0]
"This
consistent ordering is used to ensure that the output of REPLACEMENT-PSEUDO-STREAMING is independent of the random ordering of the elements.",5. Distributed Algorithm,[0],[0]
"The only other difference between REPLACEMENT-PSEUDO-STREAMING and REPLACEMENT-STREAMING is that it outputs all sets Sτ , {Tτ,i} for all τ ∈ Γn (instead of just the maximum).",5. Distributed Algorithm,[0],[0]
"We use this modified algorithm as one of the main building blocks for DISTRIBUTED-FAST (outlined in Appendix E).
",5. Distributed Algorithm,[0],[0]
Theorem 4.,5. Distributed Algorithm,[0],[0]
"The DISTRIBUTED-FAST algorithm outputs sets S∗, {T ∗i } ⊂ S, with |S∗| ≤ `, |T ∗i | ≤ k, such that
E[ 1
m m∑ i=1 fi(T ∗",5. Distributed Algorithm,[0],[0]
i )],5. Distributed Algorithm,[0],[0]
"≥ α · γ α+ γ · OPT,
where α = 12 (1 − 1e2 ) and γ = 16+ .",5. Distributed Algorithm,[0],[0]
The time complexity of algorithm is O(kmn log /̀M,5. Distributed Algorithm,[0],[0]
"+ Mkm`2 log `).
",5. Distributed Algorithm,[0],[0]
"From Theorems 3 and 4, we conclude that the optimum number of machines M for REPLACEMENT-DISTRIBUTED and DISTRIBUTED-FAST isO( √ n/`)",5. Distributed Algorithm,[0],[0]
"andO( √ n/`), respectively.",5. Distributed Algorithm,[0],[0]
"Therefore, DISTRIBUTED-FAST is a factor of O( √ n/log `) and O( √ /̀log `) faster than REPLACEMENT-GREEDY and REPLACEMENT-DISTRIBUTED, respectively.",5. Distributed Algorithm,[0],[0]
"In this section, we evaluate the performance of our algorithms in both the streaming and distributed settings.",6. Applications,[0],[0]
We compare our work against several different baselines.,6. Applications,[0],[0]
"In this experiment, we will use a subset of the VOC2012 dataset (Everingham et al.).",6.1. Streaming Image Summarization,[0],[0]
"This dataset has images containing objects from 20 different classes, ranging from birds to boats.",6.1. Streaming Image Summarization,[0],[0]
"For the purposes of this application, we will use n = 756 different images and we will consider all m = 20 classes that are available.",6.1. Streaming Image Summarization,[0],[0]
"Our goal is to choose a small subset S of images that provides a good summary of the entire ground set Ω. In general, it can be difficult to even define what a good summary of a set of images should look like.",6.1. Streaming Image Summarization,[0],[0]
"Fortunately, each image in this dataset comes with a human-labelled annotation that lists the number of objects from each class that appear in that image.
",6.1. Streaming Image Summarization,[0],[0]
"Using the exemplar-based clustering approach (Mirzasoleiman et al., 2013), for each image we generate an mdimensional vector x such that xi represents the number of objects from class i that appear in the image (an example is given in Appendix G).",6.1. Streaming Image Summarization,[0],[0]
"We define Ωi to be the set of all images that contain objects from class i, and correspondingly Si = Ωi ∩ S (i.e. the images we have selected that contain objects from class i).
",6.1. Streaming Image Summarization,[0],[0]
We want to optimize the following monotone submodular functions: fi(S) = Li({e0}),6.1. Streaming Image Summarization,[0],[0]
"− Li(S ∪ {e0}), where Li(S) =
1 |Ωi| ∑ x∈Ωi miny∈Si d(x, y).",6.1. Streaming Image Summarization,[0],[0]
"We use d(x, y) to
denote the “distance” between two images x and y. More accurately, we measure the distance between two images as the `2 norm between their characteristic vectors.",6.1. Streaming Image Summarization,[0],[0]
"We also use e0 to denote some auxiliary element, which in our case is the all-zero vector.
",6.1. Streaming Image Summarization,[0],[0]
"Since image data is generally quite storage-intensive, streaming algorithms can be particularly desirable.",6.1. Streaming Image Summarization,[0],[0]
"With this in mind, we will compare our streaming algorithm REPLACEMENT-STREAMING against the non-streaming baseline of REPLACEMENT-GREEDY.",6.1. Streaming Image Summarization,[0],[0]
We also compare against a heuristic streaming baseline that we call STREAMSUM.,6.1. Streaming Image Summarization,[0],[0]
This baseline first greedily optimizes the submodular function F (S) = ∑m i=1 fi(S) using the streaming algorithm developed by Buchbinder et al. (2015).,6.1. Streaming Image Summarization,[0],[0]
"Having selected ` elements from the stream, it then constructs each Ti by greedily selecting k of these elements for each fi.
To evaluate the various algorithms, we consider two primary metrics: the objective value, which we define as∑m i=1",6.1. Streaming Image Summarization,[0],[0]
"fi(Ti), and the wall-clock running time.",6.1. Streaming Image Summarization,[0],[0]
We note that the trials were run using Python 2.7 on a quad-core Linux machine with 3.3 GHz Intel Core i5 processors and 8 GB of RAM.,6.1. Streaming Image Summarization,[0],[0]
"Figure 2 shows our results.
",6.1. Streaming Image Summarization,[0],[0]
"The graphs are organized so that each column shows the effects of varying a particular parameter, with the objective value being shown in the top row and the running time in the bottom row.",6.1. Streaming Image Summarization,[0],[0]
"The primary observation across all the graphs is that our streaming algorithm REPLACEMENT-STREAMING not only achieves an objective value that is similar to that of the non-streaming baseline REPLACEMENT-GREEDY, but it also speeds up the running time by a full order of magnitude.",6.1. Streaming Image Summarization,[0],[0]
"We also see that REPLACEMENT-STREAMING outperforms the streaming baseline STREAM-SUM in both objective value and running time.
",6.1. Streaming Image Summarization,[0],[0]
Another noteworthy observation from Figure 2(c) is that can be increased all the way up to = 0.5 before we start to see loss in the objective value.,6.1. Streaming Image Summarization,[0],[0]
Recall that is the parameter that trades off the accuracy of REPLACEMENT-STREAMING with the running time by changing the granularity of our guesses for OPT.,6.1. Streaming Image Summarization,[0],[0]
"As seen Figure 2(f), increasing up to 0.5 also covers the majority of running time speed-up, with diminishing returns kicking in as we get close to = 1.
",6.1. Streaming Image Summarization,[0],[0]
"Also in the context of running time, we see in Figure 2(e) that REPLACEMENT-STREAMING actually speeds up as k increases.",6.1. Streaming Image Summarization,[0],[0]
"This seems counter-intuitive at first glance, but one possible reason is that the majority of the time cost for these replacement-based algorithms comes from the swapping that must be done when the Ti’s fill up.",6.1. Streaming Image Summarization,[0],[0]
"Therefore, the longer each Ti is not completely full, the faster the overall algorithm will run.
",6.1. Streaming Image Summarization,[0],[0]
"Figure 3 shows some sample images selected by REPLACEMENT-GREEDY (top) and REPLACEMENT-
STREAMING (bottom).",6.1. Streaming Image Summarization,[0],[0]
"Although the two summaries contain only one image that is exactly the same, we see that the different images still have a similar theme.",6.1. Streaming Image Summarization,[0],[0]
"For example, both images in the second column contain bikes and people; while in the third column, both images contain sheep.",6.1. Streaming Image Summarization,[0],[0]
In this application we want to use past Uber data to select optimal waiting locations for idle drivers.,6.2. Distributed Ride-Share Optimization,[0],[0]
"Towards this end, we analyze a dataset of 100,000 Uber pick-ups in Manhattan from September 2014 (UberDataset), where each entry in the dataset is given as a (latitude, longitude) coordinate pair.",6.2. Distributed Ride-Share Optimization,[0],[0]
"We model this problem as a classical facility location problem, which is known to be monotone submodular.
",6.2. Distributed Ride-Share Optimization,[0],[0]
"Given a set of potential waiting locations for drivers, we want to pick a subset of these locations so that the distance from each customer to his closest driver is minimized.",6.2. Distributed Ride-Share Optimization,[0],[0]
"In
particular, given a customer location a = (xa, ya), and a waiting driver location b = (xb, yb), we define a “convenience score” c(a, b) as follows: c(a, b) = 2− 2
1+e−200d(a,b) ,
where d(a, b) = |xa",6.2. Distributed Ride-Share Optimization,[0],[0]
− xb|+ |ya,6.2. Distributed Ride-Share Optimization,[0],[0]
"− yb| is the Manhattan distance between the two points.
",6.2. Distributed Ride-Share Optimization,[0],[0]
"Next, we need to introduce some functions we want to maximize.",6.2. Distributed Ride-Share Optimization,[0],[0]
"For this experiment, we can think about different functions corresponding to different (possibly overlapping) regions around Manhattan.",6.2. Distributed Ride-Share Optimization,[0],[0]
"The overlap means that there will still be some inherent connection between the functions, but they are still relatively distinct from each other.",6.2. Distributed Ride-Share Optimization,[0],[0]
"More specifically, we construct regions R1, . . .",6.2. Distributed Ride-Share Optimization,[0],[0]
", Rm by randomly picking m points across Manhattan.",6.2. Distributed Ride-Share Optimization,[0],[0]
"Then, for each point pi, we want to define the corresponding region Ri by all the pick-ups that have occurred within one kilometer of pi.",6.2. Distributed Ride-Share Optimization,[0],[0]
"However, to keep the problem computationally tractable, we instead randomly select only ten pick-up locations within that same radius.",6.2. Distributed Ride-Share Optimization,[0],[0]
"Figure 4(a) shows the center points of the m = 20 randomly selected regions, overlaid on top of a heat map of all the customer pick-up locations.
",6.2. Distributed Ride-Share Optimization,[0],[0]
"Given any set of driver waiting locations Ti, we define fi(Ti) as follows: fi(Ti) = ∑ a∈Ri maxb∈Ti c(a, b).",6.2. Distributed Ride-Share Optimization,[0],[0]
"For this application, we will use every customer pick-up location as a potential waiting location for a driver, meaning we have 100,000 elements in our ground set Ω. This large number of elements, combined with the fact that each single function evaluation is computationally intensive, means running the regular REPLACEMENT-GREEDY will be prohibitively ex-
pensive.",6.2. Distributed Ride-Share Optimization,[0],[0]
"Hence, we will use this setup to evaluate the two distributed algorithms we presented in Section 5.",6.2. Distributed Ride-Share Optimization,[0],[0]
We will also compare our algorithms against a heuristic baseline that we call DISTRIBUTED-GREEDY.,6.2. Distributed Ride-Share Optimization,[0],[0]
"This baseline will first select ` elements using the greedy distributed framework introduced by Mirzasoleiman et al. (2013), and then greedily optimize each fi over these ` elements.
",6.2. Distributed Ride-Share Optimization,[0],[0]
"Each algorithm produces two outputs: a small subset S of potential waiting locations (with size ` = 30), as well as a solution Ti (of size k = 3) for each function fi.",6.2. Distributed Ride-Share Optimization,[0],[0]
"In other words, each algorithm will reduce the number of potential waiting locations from 100,000 to 30, and then choose 3 different waiting locations for drivers in each region.
",6.2. Distributed Ride-Share Optimization,[0],[0]
"In Figure 4(b), we graph the average distance from each customer to his closest driver, which we will refer to as the cost.",6.2. Distributed Ride-Share Optimization,[0],[0]
"One interesting observation is that while the cost of DISTRIBUTED-FAST decreases with the number of machines, the costs of the other two algorithms stay relatively constant, with REPLACEMENT-DISTRIBUTED marginally outperforming DISTRIBUTED-GREEDY.",6.2. Distributed Ride-Share Optimization,[0],[0]
"In Figure 4(c), we graph the run time of each algorithm.",6.2. Distributed Ride-Share Optimization,[0],[0]
"We see that the algorithms achieve their optimal speeds at different values of M , verifying the theory at the end of Section 5.",6.2. Distributed Ride-Share Optimization,[0],[0]
"Overall, we see that while all three algorithms have very comparable costs, DISTRIBUTED-FAST is significantly faster than the others.
",6.2. Distributed Ride-Share Optimization,[0],[0]
"While in the previous application we only looked at the
objective value for the given functions f1, . .",6.2. Distributed Ride-Share Optimization,[0],[0]
.,6.2. Distributed Ride-Share Optimization,[0],[0]
", fm, in this experiment we also evaluate the utility of our summary on new functions drawn from the same distribution.",6.2. Distributed Ride-Share Optimization,[0],[0]
"That is, using the regions shown in Figure 4(a), each algorithm will select a subset S of potential waiting locations.",6.2. Distributed Ride-Share Optimization,[0],[0]
"Using only these reduced subsets, we then greedily select k waiting locations for each of the twenty new regions shown in 4(d).
",6.2. Distributed Ride-Share Optimization,[0],[0]
"In Figure 4(e), we see that the summaries from all three algorithms achieve a similar cost, which is significantly better than RANDOM.",6.2. Distributed Ride-Share Optimization,[0],[0]
"In this scenario, RANDOM is defined as the cost achieved when optimizing over a random size ` subset and OPTIMAL is defined as the cost that is achieved when optimizing the functions over the entire ground set rather than a reduced subset.",6.2. Distributed Ride-Share Optimization,[0],[0]
"In Figure 4(f), we confirm that DISTRIBUTED-FAST is indeed the fastest algorithm for constructing each summary.",6.2. Distributed Ride-Share Optimization,[0],[0]
"Note that 4(f) is demonstrating how long each algorithm takes to construct a size ` summary, not how long it is taking to optimize over this summary.",6.2. Distributed Ride-Share Optimization,[0],[0]
"To satisfy the need for scalable data summarization algorithms, this paper focused on the two-stage submodular maximization framework and provided the first streaming and distributed solutions to this problem.",7. Conclusion,[0],[0]
"In addition to constant factor theoretical guarantees, we demonstrated the effectiveness of our algorithms on real world applications in image summarization and ride-share optimization.",7. Conclusion,[0],[0]
Amin Karbasi was supported by a DARPA Young Faculty Award (D16AP00046) and a AFOSR Young Investigator Award (FA9550-18-1-0160).,Acknowledgements,[0],[0]
Ehsan Kazemi was supported by the Swiss National Science Foundation (Early Postdoc.,Acknowledgements,[0],[0]
Mobility) under grant number 168574.,Acknowledgements,[0],[0]
The sheer scale of modern datasets has resulted in a dire need for summarization techniques that can identify representative elements in a dataset.,abstractText,[0],[0]
"Fortunately, the vast majority of data summarization tasks satisfy an intuitive diminishing returns condition known as submodularity, which allows us to find nearly-optimal solutions in linear time.",abstractText,[0],[0]
We focus on a two-stage submodular framework where the goal is to use some given training functions to reduce the ground set so that optimizing new functions (drawn from the same distribution) over the reduced set provides almost as much value as optimizing them over the entire ground set.,abstractText,[0],[0]
"In this paper, we develop the first streaming and distributed solutions to this problem.",abstractText,[0],[0]
"In addition to providing strong theoretical guarantees, we demonstrate both the utility and efficiency of our algorithms on real-world tasks including image summarization and ride-share optimization.",abstractText,[0],[0]
Data Summarization at Scale:A Two-Stage Submodular Approach,title,[0],[0]
Stochastic gradient descent (SGD) has become one of the workhorses of modern machine learning.,1. Introduction,[0],[0]
"In particular, it is the optimization method of choice for training highly complex and non-convex models, such as neural networks.",1. Introduction,[0],[0]
"When it was observed that these models generalize better (suffer less from overfitting) than classical machine learning theory suggests, a large theoretical interest emerged to explain this phenomenon.",1. Introduction,[0],[0]
"Given that SGD at best finds a local minimum of the non-convex objective function, it has been argued that all such minima might be equally good.",1. Introduction,[0],[0]
"However, at the same time, a large body of empirical work and tricks of trade, such as early stopping, suggests that in prac-
1University of Milan, Italy 2IST Austria.",1. Introduction,[0],[0]
"Correspondence to: Ilja Kuzborskij <ilja.kuzborskij@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"tice one might not even reach a minimum, yet nevertheless observes excellent performance.
",1. Introduction,[0],[0]
In this work we follow an alternative route that aims to directly analyze the generalization ability of SGD by studying how sensitive it is to small perturbations in the training set.,1. Introduction,[0],[0]
"This is known as algorithmic stability approach (Bousquet & Elisseeff, 2002) and was used recently (Hardt et al., 2016) to establish generalization bounds for both convex and non-convex learning settings.",1. Introduction,[0],[0]
"To do so they employed a rather restrictive notion of stability that does not depend on the data, but captures only intrinsic characteristics of the learning algorithm and global properties of the objective function.",1. Introduction,[0],[0]
"Consequently, their analysis results in worst-case guarantees that in some cases tend to be too pessimistic.",1. Introduction,[0],[0]
"As recently pointed out in (Zhang et al., 2017), deep learning might indeed be such a case, as this notion of stability is insufficient to give deeper theoretical insights, and a less restrictive one is desirable.
",1. Introduction,[0],[0]
"As our main contribution in this work we establish that a data-dependent notion of algorithmic stability, very similar to the On-Average Stability (Shalev-Shwartz et al., 2010), holds for SGD when applied to convex as well as nonconvex learning problems.",1. Introduction,[0],[0]
As a consequence we obtain new generalization bounds that depend on the data-generating distribution and the initialization point of an algorithm.,1. Introduction,[0],[0]
"For convex loss functions, the bound on the generalization error is essentially multiplicative in the risk at the initialization point when noise of stochastic gradient is not too high.",1. Introduction,[0],[0]
"For the non-convex loss functions, besides the risk, it is also critically controlled by the expected second-order information about the objective function at the initialization point.",1. Introduction,[0],[0]
"We further corroborate our findings empirically and show that, indeed, the data-dependent generalization bound is tighter than the worst-case counterpart on non-convex objective functions.",1. Introduction,[0],[0]
"Finally, the nature of the data-dependent bounds allows us to state optimistic bounds that switch to the faster rate of convergence subject to the vanishing empirical risk.
",1. Introduction,[0],[0]
"In particular, our findings justify the intuition that SGD is more stable in less curved areas of the objective function and link it to the generalization ability.",1. Introduction,[0],[0]
This also backs up numerous empirical findings in the deep learning literature that solutions with low generalization error occur in less curved regions.,1. Introduction,[0],[0]
"At the same time, in pessimistic scenarios,
our bounds are no worse than those of (Hardt et al., 2016).
",1. Introduction,[0],[0]
"Finally, we exemplify an application of our bounds, and propose a simple yet principled transfer learning scheme for the convex and non-convex case, which is guaranteed to transfer from the best source of information.",1. Introduction,[0],[0]
"In addition, this approach can also be used to select a good initialization given a number of random starting positions.",1. Introduction,[0],[0]
"This is a theoretically sound alternative to the purely random commonly used in non-convex learning.
",1. Introduction,[0],[0]
The rest of the paper is organized as follows.,1. Introduction,[0],[0]
We revisit the connection between stability and generalization of SGD in Section 3 and introduce a data-dependent notion of stability in Section 4.,1. Introduction,[0],[0]
"We state the main results in Section 5, in particular, Theorem 3 for the convex case, and Theorem 4 for the non-convex one.",1. Introduction,[0],[0]
Next we demonstrate empirically that the bound shown in Theorem 4 is tighter than the worstcase one in Section 5.2.1.,1. Introduction,[0],[0]
"Finally, we suggest application of these bounds by showcasing principled transfer learning approaches in Section 5.3, and we conclude in Section 7.",1. Introduction,[0],[0]
"Algorithmic stability has been a topic of interest in learning theory for a long time, however, the modern approach on the relationship between stability and generalization goes back to the milestone work of (Bousquet & Elisseeff, 2002).",2. Related Work,[0],[0]
"They analyzed several notions of stability, which fall into two categories: distribution-free and distribution-dependent ones.",2. Related Work,[0],[0]
The first category is usually called uniform stability and focuses on the intrinsic stability properties of an algorithm without regard to the data-generating distribution.,2. Related Work,[0],[0]
"Uniform stability was used to analyze many algorithms, including regularized Empirical Risk Minimization (ERM) (Bousquet & Elisseeff, 2002), randomized aggregation schemes (Elisseeff et al., 2005), and recently SGD by (Hardt et al., 2016; London, 2016), and (Poggio et al., 2011).",2. Related Work,[0],[0]
"Despite the fact that uniform stability has been shown to be sufficient to guarantee learnability, it can be too pessimistic, resulting in worst-case rates.
",2. Related Work,[0],[0]
"In this work we are interested in the data-dependent behavior of SGD, thus the emphasis will fall on the distributiondependent notion of stability, known as on-average stability, explored throughly in (Shalev-Shwartz et al., 2010).",2. Related Work,[0],[0]
The attractive quality of this less restrictive stability type is that the resulting bounds are controlled by how stable the algorithm is under the data-generating distribution.,2. Related Work,[0],[0]
"For instance, in (Bousquet & Elisseeff, 2002) and (Devroye & Wagner, 1979), the on-average stability is related to the variance of an estimator.",2. Related Work,[0],[0]
"In (Shalev-Shwartz & Ben-David, 2014, Sec. 13), the authors show risk bounds that depend on the expected empirical risk of a solution to the regularized ERM.",2. Related Work,[0],[0]
"In turn, one can exploit this fact to state improved optimistic risk
bounds, for instance, ones that exhibit fast-rate regimes (Koren & Levy, 2015; Gonen & Shalev-Shwartz, 2017), or even to design enhanced algorithms that minimize these bounds in a data-driven way, e.g. by exploiting side information as in transfer (Kuzborskij & Orabona, 2013; Ben-David & Urner, 2013) and metric learning (Perrot & Habrard, 2015).",2. Related Work,[0],[0]
"Here, we mainly focus on the later direction in the context of SGD: how stable is SGD under the data-generating distribution given an initialization point?",2. Related Work,[0],[0]
"We also touch the former direction by taking advantage of our data-driven analysis and show optimistic bounds as a corollary.
",2. Related Work,[0],[0]
We will study the on-average stability of SGD for both convex and non-convex loss functions.,2. Related Work,[0],[0]
"In the convex setting, we will relate stability to the risk at the initialization point, while previous data-driven stability arguments usually consider minimizers of convex ERM rather than a stochastic approximation (Shalev-Shwartz & Ben-David, 2014; Koren & Levy, 2015).",2. Related Work,[0],[0]
"Beside convex problems, our work also covers the generalization ability of SGD on non-convex problems.",2. Related Work,[0],[0]
"Here, we borrow techniques of (Hardt et al., 2016) and extend them to the distribution-dependent setting.",2. Related Work,[0],[0]
"That said, while bounds of (Hardt et al., 2016) are stated in terms of worst-case quantities, ours reveal new connections to the data-dependent second-order information.",2. Related Work,[0],[0]
"These new insights also partially justify empirical observations in deep learning about the link between the curvature and the generalization error (Hochreiter & Schmidhuber, 1997; Keskar et al., 2017; Chaudhari et al., 2017).",2. Related Work,[0],[0]
"At the same time, our work is an alternative to the theoretical studies of neural network objective functions (Choromanska et al., 2015; Kawaguchi, 2016), as we focus on the direct connection between the generalization and the curvature.
",2. Related Work,[0],[0]
"In this light, our work is also related to non-convex optimization by SGD.",2. Related Work,[0],[0]
"Literature on this subject typically studies rates of convergence to the stationary points (Ghadimi & Lan, 2013; Allen-Zhu & Hazan, 2016; Reddi et al., 2016), and ways to avoid saddles (Ge et al., 2015; Lee et al., 2016).",2. Related Work,[0],[0]
"However, unlike these works, and similarly to (Hardt et al., 2016), we are interested in the generalization ability of SGD, and thanks to the stability approach, involvement of stationary points in our analysis is not necessary.
",2. Related Work,[0],[0]
"Finally, we propose an example application of our findings in Transfer Learning (TL).",2. Related Work,[0],[0]
"For instance, by controlling the stability bound in a data-driven way, one can choose an initialization that leads to improved generalization.",2. Related Work,[0],[0]
"This is related to TL where one transfers from pre-trained models (Kuzborskij & Orabona, 2016; Tommasi et al., 2014; Pentina & Lampert, 2014; Ben-David & Urner, 2013), especially popular in deep learning due to its data-demanding nature (Galanti et al., 2016).",2. Related Work,[0],[0]
"Literature on this topic is mostly focused on the ERM setting and PAC-bounds, while our analysis of SGD yields such guarantees as a corollary.",2. Related Work,[0],[0]
"First, we introduce definitions used in the rest of the paper.",3. Stability of Stochastic Gradient Descent,[0],[0]
"We will denote with small and capital bold letters respectively column vectors and matrices, e.g., a “ ra1, a2, . . .",3.1. Definitions,[0],[0]
", adsT P Rd and A P Rd1ˆd2 , }a} is understood as a Euclidean norm and }A}2 as the spectral norm.",3.1. Definitions,[0],[0]
"We denote enumeration by rns “ t1, . . .",3.1. Definitions,[0],[0]
", nu for n P N.
",3.1. Definitions,[0],[0]
We indicate an example space by Z and its member by z P Z .,3.1. Definitions,[0],[0]
"For instance, in a supervised setting Z “ X ˆ Y , such that X is the input and Y is the output space of a learning problem.",3.1. Definitions,[0],[0]
We assume that training and testing examples are drawn iid from a probability distribution D over Z .,3.1. Definitions,[0],[0]
"In particular, we will denote the training set as S “ tziumi“1 „ Dm.
",3.1. Definitions,[0],[0]
"For a parameter space H, we define a learning algorithm as a map A : Zm ÞÑ H and for brevity we will use the notation AS “ ApSq.",3.1. Definitions,[0],[0]
In the following we assume that H Ď Rd.,3.1. Definitions,[0],[0]
To measure the accuracy of a learning algorithm,3.1. Definitions,[0],[0]
"A, we have a loss function fpw, zq, which measures the cost incurred by predicting with parameters w P H on an example z.",3.1. Definitions,[0],[0]
"The risk of w, with respect to the distribution D, and the empirical risk given a training set S are defined as
Rpwq :“ E z„D
rfpw, zqs, and pRSpwq :“ 1
m
m ÿ i“1 fpw, ziq .
",3.1. Definitions,[0],[0]
"Finally, define R‹ :“ infwPHRpwq.",3.1. Definitions,[0],[0]
"On an intuitive level, a learning algorithm is said to be stable whenever a small perturbation in the training set does not affect its outcome too much.",3.2. Uniform Stability and Generalization,[0],[0]
"Of course, there is a number of ways to formalize the perturbation and the extent of the change in the outcome, and we will discuss some of them below.",3.2. Uniform Stability and Generalization,[0],[0]
The most important consequence of a stable algorithm is that it generalizes from the training set to the unseen data sampled from the same distribution.,3.2. Uniform Stability and Generalization,[0],[0]
"In other words, the difference between the risk RpASq and the empirical risk pRSpASq of the algorithm’s output is controlled by the quantity that captures how stable the algorithm is.",3.2. Uniform Stability and Generalization,[0],[0]
"So, to observe good performance, or a decreasing true risk, we must have a stable algorithm and decreasing empirical risk (training error), which usually comes by design of the algorithm.",3.2. Uniform Stability and Generalization,[0],[0]
"In this work we focus on the stability of the Stochastic Gradient Descent (SGD) algorithm, and thus, as a consequence, we study its generalization ability.
",3.2. Uniform Stability and Generalization,[0],[0]
"Recently, (Hardt et al., 2016) used a stability argument to prove generalization bounds for learning with SGD.",3.2. Uniform Stability and Generalization,[0],[0]
"Specifically, the authors extended the notion of the uniform stability
originally proposed by (Bousquet & Elisseeff, 2002), to accommodate randomized algorithms.
",3.2. Uniform Stability and Generalization,[0],[0]
Definition 1 (Uniform stability).,3.2. Uniform Stability and Generalization,[0],[0]
"A randomized algorithm A is -uniformly stable if for all datasets S,",3.2. Uniform Stability and Generalization,[0],[0]
"Spiq P Zm such that S and Spiq differ in the i-th example, we have
sup zPZ,iPrms
!
",3.2. Uniform Stability and Generalization,[0],[0]
"E A rfpAS , zq ´ fpASpiq , zqs
)
ď .
",3.2. Uniform Stability and Generalization,[0],[0]
"Since SGD is a randomized algorithm, we have to cope with two sources of randomness: the data-generating process and the randomization of the algorithm A itself, hence we have statements in expectation.",3.2. Uniform Stability and Generalization,[0],[0]
"The following theorem of (Hardt et al., 2016) shows that the uniform stability implies generalization in expectation.
",3.2. Uniform Stability and Generalization,[0],[0]
Theorem 1.,3.2. Uniform Stability and Generalization,[0],[0]
Let A be -uniformly stable.,3.2. Uniform Stability and Generalization,[0],[0]
"Then, ˇ
ˇ ˇ ˇ E S,A
” pRSpASq ´RpASq ı
ˇ ˇ ˇ ˇ ď .
",3.2. Uniform Stability and Generalization,[0],[0]
Thus it suffices to characterize the uniform stability of an algorithm to state a generalization bound.,3.2. Uniform Stability and Generalization,[0],[0]
"In particular, (Hardt et al., 2016) showed generalization bounds for SGD under different assumptions on the loss function f .",3.2. Uniform Stability and Generalization,[0],[0]
"Despite that these results hold in expectation, other forms of generalization bounds, such as high-probability ones, can be derived from the above (Shalev-Shwartz et al., 2010).
",3.2. Uniform Stability and Generalization,[0],[0]
"Apart from SGD, uniform stability has been used before to prove generalization bounds for many learning algorithms (Bousquet & Elisseeff, 2002).",3.2. Uniform Stability and Generalization,[0],[0]
"However, these bounds typically suggest worst-case generalization rates, and rather reflect intrinsic stability properties of an algorithm.",3.2. Uniform Stability and Generalization,[0],[0]
"In other words, uniform stability is oblivious to the data-generating process and any other side information, which might reveal scenarios where generalization occurs at a faster rate.",3.2. Uniform Stability and Generalization,[0],[0]
"In turn, these insights could motivate the design of improved learning algorithms.",3.2. Uniform Stability and Generalization,[0],[0]
In the following we address some limitations of analysis through uniform stability by using a less restrictive notion of stability.,3.2. Uniform Stability and Generalization,[0],[0]
"We extend the setting of (Hardt et al., 2016) by proving data-dependent stability bounds for convex and non-convex loss functions.",3.2. Uniform Stability and Generalization,[0],[0]
"In addition, we also take into account the initialization point of an algorithm as a form of supplementary information, and we dedicate special attention to its interplay with the data-generating distribution.",3.2. Uniform Stability and Generalization,[0],[0]
"Finally, we discuss situations where one can explicitly control the stability of SGD in a data-dependent way.",3.2. Uniform Stability and Generalization,[0],[0]
"In this section we describe a notion of data-dependent algorithmic stability, that allows us to state generalization bounds which depend not only on the properties of the learning algorithm, but also on the additional parameters of the
algorithm.",4. Data-dependent Stability Bounds for SGD,[0],[0]
"We indicate such additional parameters by θ, and therefore we denote stability as a function pθq.",4. Data-dependent Stability Bounds for SGD,[0],[0]
"In particular, in the following we will be interested in scenarios where θ describes the data-generating distribution and the initialization point of SGD.
",4. Data-dependent Stability Bounds for SGD,[0],[0]
Definition 2 (On-Average stability).,4. Data-dependent Stability Bounds for SGD,[0],[0]
"A randomized algorithm A is pθq-on-average stable if it is true that
sup iPrms
""
E A E S,z rfpAS , zq ´ fpASpiq , zqs
*
ď pθq ,
where S iid„ Dm and Spiq is its copy with i-th example replaced by z iid„D.
Our definition of on-average stability resembles the notion introduced by (Shalev-Shwartz et al., 2010).",4. Data-dependent Stability Bounds for SGD,[0],[0]
The difference lies in the fact that we take supremum over index of replaced example.,4. Data-dependent Stability Bounds for SGD,[0],[0]
"A similar notion was also used by (Bousquet & Elisseeff, 2002) and later by (Elisseeff et al., 2005) for analysis of a randomized aggregation schemes, however their definition involves absolute difference of losses.",4. Data-dependent Stability Bounds for SGD,[0],[0]
"The dependence on θ also bears similarity to recent work of (London, 2016), however, there, it is used in the context of uniform stability.",4. Data-dependent Stability Bounds for SGD,[0],[0]
"The following theorem shows that on-average -stable random algorithm is guaranteed to generalize in expectation.
",4. Data-dependent Stability Bounds for SGD,[0],[0]
Theorem 2.,4. Data-dependent Stability Bounds for SGD,[0],[0]
Let an algorithm A be pθq-on-average stable.,4. Data-dependent Stability Bounds for SGD,[0],[0]
"Then,
E S E A
” RpASq ´ pRSpASq ı ď pθq .",4. Data-dependent Stability Bounds for SGD,[0],[0]
"Before presenting our main results in this section, we discuss algorithmic details and assumptions.",5. Main Results,[0],[0]
"We will study the following variant of SGD: given a training set S “ tziumi“1
iid„ Dm, step sizes tαtuTt“1, random indices I “ tjtuTt“1, and an initialization point w1, perform updates
wt`1 “ wt ´ αt∇fpwt, zjtq
for T ď m steps.",5. Main Results,[0],[0]
"Moreover we will use the notation wS,t to indicate the output of SGD ran on a training set S, at step t. We assume that the indices in I are sampled from the uniform distribution over rms without replacement, and that this is the only source of randomness for SGD.",5. Main Results,[0],[0]
"In practice this corresponds to permuting the training set before making a pass through it, as it is commonly done in practical applications.",5. Main Results,[0],[0]
"We also assume that the variance of stochastic gradients obeys
E S,z
” }∇fpwS,t, zq ´∇RpwS,tq}2 ı ď σ2 @t P rT s .
",5. Main Results,[0],[0]
"Next, we introduce statements about the loss functions f used in the following.
",5. Main Results,[0],[0]
Definition 3 (Lipschitz f ).,5. Main Results,[0],[0]
"A loss function f is L-Lipschitz if }∇fpw, zq} ď L, @w P H and @z P Z .",5. Main Results,[0],[0]
"Note that this also implies that |fpw, zq ´ fpv, zq| ď L}w ´ v} .",5. Main Results,[0],[0]
Definition 4 (Smooth f ).,5. Main Results,[0],[0]
"A loss function is β-smooth if @w,v P H and @z P Z , }∇fpw, zq ´ ∇fpv, zq} ď β}w ´ v} , which also implies fpw, zq ´ fpv, zq ď ∇fpv, zqJpw ´ vq ` β2 }w ´ v} 2 .",5. Main Results,[0],[0]
Definition 5 (Lipschitz Hessians).,5. Main Results,[0],[0]
"A loss function f has a ρLipschitz Hessian if @w,v P H and @z P Z , }∇2fpw, zq´ ∇2fpv, zq}2 ď ρ}w ´ v} .
",5. Main Results,[0],[0]
"The last condition is occasionally used in analysis of SGD (Ge et al., 2015) and holds whenever f has a bounded third derivative.",5. Main Results,[0],[0]
"All presented theorems assume that the loss function is non-negative, Lipschitz, and β-smooth.",5. Main Results,[0],[0]
Examples of such commonly used loss functions are the logistic/softmax losses and neural networks with sigmoid activations.,5. Main Results,[0],[0]
"Convexity of loss functions or Lipschitzness of Hessians will only be required for some results, and we will denote it when necessary.",5. Main Results,[0],[0]
Proofs for all the statements in this section are given in the arXiv version of the paper1.,5. Main Results,[0],[0]
"First, we present a new and data-dependent stability result for convex losses.
",5.1. Convex Losses,[0],[0]
Theorem 3.,5.1. Convex Losses,[0],[0]
"Assume that f is convex, and that SGD’s step sizes satisfy αt “ c?t ď 1 β ,",5.1. Convex Losses,[0],[0]
@t P rT s.,5.1. Convex Losses,[0],[0]
"Then SGD is
pD,w1q-on-average stable with
pD,w1q “ O ˜ a c pRpw1q ´R‹q ¨ 4 ?",5.1. Convex Losses,[0],[0]
"T
m ` cσ
?",5.1. Convex Losses,[0],[0]
"T
m
¸
.
",5.1. Convex Losses,[0],[0]
"Under the same assumptions, taking step size of order Op1{ ?",5.1. Convex Losses,[0],[0]
"tq, Theorem 3.7 of (Hardt et al., 2016) implies a uniform stability bound “ Op ?",5.1. Convex Losses,[0],[0]
T {mq.,5.1. Convex Losses,[0],[0]
Our bound differs since it involves a multiplicative risk at the initialization point.,5.1. Convex Losses,[0],[0]
"Thus, our bound corroborates the intuition that whenever we start at a good location of the objective function, the algorithm is more stable and thus generalizes better.",5.1. Convex Losses,[0],[0]
"However, this is only the case, whenever the variance of stochastic gradient σ2 is not too large.",5.1. Convex Losses,[0],[0]
"In the deterministic case, and of Rpw1q “ R‹, the theorem confirms that SGD, in expectation, does not need to make any updates and is therefore perfectly stable.",5.1. Convex Losses,[0],[0]
"On the other hand, when the variance σ2 is large enough to make the second summand in Theorem 3 dominant, the bound does not offer improvement compared to (Hardt et al., 2016).",5.1. Convex Losses,[0],[0]
"Note, that a result of this type cannot be obtained through the more restrictive uniform stability, precisely because such bounds on the stability must hold even for a worst-case choice of data distribution and initialization.",5.1. Convex Losses,[0],[0]
"In contrast, the notion of stability we
1https://arxiv.org/abs/1703.01678
employ depends on the data-generating distribution, which allowed us to introduce dependency on the risk.
",5.1. Convex Losses,[0],[0]
"Furthermore, consider that we start at arbitrary location w1: assuming that the loss function is bounded for a concrete H and Z , the rate of our bound up to a constant is no worse than that of (Hardt et al., 2016).",5.1. Convex Losses,[0],[0]
"Finally, one can always tighten this result by taking the minimum of two bounds.",5.1. Convex Losses,[0],[0]
"Now we state a new stability result for non-convex losses.
",5.2. Non-convex Losses,[0],[0]
Theorem 4.,5.2. Non-convex Losses,[0],[0]
"Assume that fp¨, zq P r0, 1s and has a ρLipschitz Hessian, and that step sizes of a form αt “ ct satisfy c ď",5.2. Non-convex Losses,[0],[0]
"min !
",5.2. Non-convex Losses,[0],[0]
"1 β , 1 4p2β lnpT qq2
)
.",5.2. Non-convex Losses,[0],[0]
"Then SGD is pD,w1qon-average stable with
pD,w1q ď 1` 1cγ m ` 2cL2 ˘ 1 1`cγ ˆ E S,A rRpASqs ¨ T ˙ cγ 1`cγ ,
(1)
where
γ :“ Õ ´ min !
",5.2. Non-convex Losses,[0],[0]
"β, E z
“ › ›∇2fpw1, zq › ›
2
‰ `∆‹1,σ2 )¯",5.2. Non-convex Losses,[0],[0]
", (2)
∆‹1,σ2 :“ ρ ´ cσ ` a c pRpw1q ´R‹q ¯ .
",5.2. Non-convex Losses,[0],[0]
"In particular, γ characterizes how the curvature at the initialization point affects stability, and hence the generalization error of SGD.",5.2. Non-convex Losses,[0],[0]
"Since γ heavily affects the rate of convergence in (1), and in most situations smaller γ yields higher stability, we now look at a few cases of its behavior.",5.2. Non-convex Losses,[0],[0]
"Consider a regime such that γ is of the order Θ̃ ´ Er}∇2fpw1, zq}2s ` a Rpw1q ` σ ¯
, or in other words, that stability is controlled by the curvature, the risk of the initialization point w1, and the variance of the stochastic gradient σ2.",5.2. Non-convex Losses,[0],[0]
"This suggests that starting from a point in a less curved region with low risk should yield higher stability, and therefore as predicted by our theory, allow for faster generalization.",5.2. Non-convex Losses,[0],[0]
"In addition, we observe that the considered stability regime offers a principled way to pre-screen a good initialization point in practice, by choosing the one that minimizes spectral norm of the Hessian and the risk.
",5.2. Non-convex Losses,[0],[0]
"Next, we focus on a more specific case.",5.2. Non-convex Losses,[0],[0]
"Suppose that we choose a step size αt “ ct such that γ “ Θ̃ ` Er}∇2fpw1, zq}2s ˘
, yet not too small, so that the empirical risk can still be decreased.",5.2. Non-convex Losses,[0],[0]
"Then, stability is dominated by the curvature around w1.",5.2. Non-convex Losses,[0],[0]
"Indeed, lower generalization errors on non-convex problems, such as training deep neural networks, have been observed empirically when SGD is actively guided (Hochreiter & Schmidhuber, 1997; Goodfellow et al., 2016; Chaudhari et al., 2017) or converges to solutions with low curvature (Keskar et al., 2017).",5.2. Non-convex Losses,[0],[0]
"However, to the best of our knowledge, Theorem 4 is the first
to establish a theoretical link between the curvature of the loss function and the generalization ability of SGD in a data-dependent sense.
",5.2. Non-convex Losses,[0],[0]
"Theorem 4 immediately implies the following statement that further reinforces the effect of the initialization point on the generalization error, assuming that ESrRpASqs ď Rpw1q.",5.2. Non-convex Losses,[0],[0]
Corollary 1.,5.2. Non-convex Losses,[0],[0]
"Under conditions of Theorem 4 we have that SGD is pD,w1q-on-average stable with
pD,w1q “ O ˜ 1` 1cγ m pRpw1q ¨ T q cγ 1`cγ ¸ .",5.2. Non-convex Losses,[0],[0]
"(3)
We take a moment to discuss the role of the risk term in pRpw1q ¨ T q cγ 1`cγ .",5.2. Non-convex Losses,[0],[0]
"Observe that pD,w1q Ñ 0",5.2. Non-convex Losses,[0],[0]
"as Rpw1q Ñ 0, in other words, the generalization error approaches zero as the risk of the initialization point vanishes.",5.2. Non-convex Losses,[0],[0]
"This is an intuitive behavior, however, uniform stability does not capture this due to its distribution-free nature.",5.2. Non-convex Losses,[0],[0]
"Finally, we note that (Hardt et al., 2016, Theorem 3.8) showed a bound similar to (1), however, in place of γ their bound has a Lipschitz constant of the gradient.",5.2. Non-convex Losses,[0],[0]
"The crucial difference lies in term γ which is now not merely a Lipschitz constant, but rather depends on the data-generating distribution and initialization point of SGD.",5.2. Non-convex Losses,[0],[0]
"We compare to their bound by considering the worst case scenario, namely, that SGD is initialized in a point with high curvature, or altogether, that the objective function is highly curved everywhere.",5.2. Non-convex Losses,[0],[0]
"Then, at least our bound is no worse than the one of (Hardt et al., 2016), since γ ď β.",5.2. Non-convex Losses,[0],[0]
"Finally, it should be noted that our bound can be compared to the one of (Hardt et al., 2016) only in a setting of a single pass.",5.2. Non-convex Losses,[0],[0]
"In a multiple-pass case, data-dependent analysis in a current form would not hold, since the output of SGD would not be independent from a newly observed example after the first pass.",5.2. Non-convex Losses,[0],[0]
"On the other hand, our results focus on the gains due to data-dependent initialization.
",5.2. Non-convex Losses,[0],[0]
Theorem 4 also allows us to prove an optimistic generalization bound for learning with SGD on non-convex objectives.,5.2. Non-convex Losses,[0],[0]
Corollary 2.,5.2. Non-convex Losses,[0],[0]
"Under conditions of Theorem 4 we have that the output of SGD obeys
E S,A
” RpASq ´ pRSpASq ı “
O
˜
1` 1 cγ
m ¨max
#
ˆ
E S,A
” pRSpASq ı ¨ T ˙
cγ 1`cγ
,
ˆ
T
m
˙cγ +¸
.
",5.2. Non-convex Losses,[0],[0]
"An important consequence of Corollary 2, is that for a vanishing expected empirical risk, in particular for ES,Ar pRSpASqs “ O ` T cγ
m1`cγ
˘
, the generalization error behaves as O ` T cγ
m1`cγ
˘
.",5.2. Non-convex Losses,[0],[0]
"Considering the full pass, that is m “ OpT q, we have an optimistic generalization error of order O p1{mq instead of Opm´ 1 1`cγ q.",5.2. Non-convex Losses,[0],[0]
"We note that
PAC bounds with similar optimistic message (although not directly comparable), but without curvature information can also be obtained through empirical Bernstein bounds as in (Maurer & Pontil, 2009).",5.2. Non-convex Losses,[0],[0]
"However, a PAC bound does not suggest a way to minimize non-convex empirical risk in general, where SGD is known to work reasonably well.",5.2. Non-convex Losses,[0],[0]
Next we empirically assess the tightness of our non-convex generalization bounds on real data.,5.2.1. TIGHTNESS OF NON-CONVEX BOUNDS,[0],[0]
"In the following experiment we train a neural network with three convolutional layers interlaced with max-pooling, followed by the fully connected layer with 16 units, on the MNIST dataset.",5.2.1. TIGHTNESS OF NON-CONVEX BOUNDS,[0],[0]
This totals in a model with 18K parameters.,5.2.1. TIGHTNESS OF NON-CONVEX BOUNDS,[0],[0]
"Figure 1 compares our data-dependent bound (1) to the distribution-free one of (Hardt et al., 2016, Theorem 3.8).",5.2.1. TIGHTNESS OF NON-CONVEX BOUNDS,[0],[0]
As as a reference we also include an empirical estimate of the generalization error taken as an absolute difference of the validation and training average losses.,5.2.1. TIGHTNESS OF NON-CONVEX BOUNDS,[0],[0]
"Since our bound also depends on the initialization point, we plot (1) for multiple “warm-starts”, ie.with SGD initialized from a pre-trained position.",5.2.1. TIGHTNESS OF NON-CONVEX BOUNDS,[0],[0]
"We consider 7 such warm-starts at every 200 steps, and report data-dependent quantities used to compute (1) just beneath the graph.",5.2.1. TIGHTNESS OF NON-CONVEX BOUNDS,[0],[0]
"Our first observation is that, clearly, the datadependent bound gives tighter estimate, by roughly one order of magnitude.",5.2.1. TIGHTNESS OF NON-CONVEX BOUNDS,[0],[0]
"Second, simulating start from a pretrained position suggests even tighter estimates: we suspect that this is due to decreasing validation error which is used as an empirical estimate for Rpw1q which affects bound (1).
",5.2.1. TIGHTNESS OF NON-CONVEX BOUNDS,[0],[0]
"We compute an empirical estimate of the expected Hessian spectral norm by the power iteration method using an efficient Hessian-vector multiplication method (Pearlmutter, 1994).",5.2.1. TIGHTNESS OF NON-CONVEX BOUNDS,[0],[0]
"Since bounds depend on constants L, β, and ρ, we estimate them by tracking maximal values of the
gradient and Hessian norms throughout optimization.",5.2.1. TIGHTNESS OF NON-CONVEX BOUNDS,[0],[0]
"We compute bounds with estimates pL “ 78.72, pβ “ 1692.28, pρ “ 3823.73, and c “ 10´3.",5.2.1. TIGHTNESS OF NON-CONVEX BOUNDS,[0],[0]
"One example application of data-dependent bounds presented before lies in Transfer Learning (TL), where we are interested in achieving faster generalization on a target task by exploiting side information that originates from different but related source tasks.",5.3. Application to Transfer Learning,[0],[0]
"The literature on TL explored many ways to do so, and here we will focus on the one that is most compatible with our bounds.",5.3. Application to Transfer Learning,[0],[0]
"More formally, suppose that the target task at hand is characterized by a joint probability distribution D, and as before we have a training set S iid„Dm.",5.3. Application to Transfer Learning,[0],[0]
Some TL approaches also assume access to the data sampled from the distributions associated with the source tasks.,5.3. Application to Transfer Learning,[0],[0]
"Here we follow a conservative approach – instead of the source data, we receive a set of source hypotheses twsrck u K k“1 Ă H, trained on the source tasks.",5.3. Application to Transfer Learning,[0],[0]
"The goal of a learner is to come up with a target hypothesis, which in the optimistic scenario generalizes better by relying on source hypotheses.",5.3. Application to Transfer Learning,[0],[0]
"In the TL literature this is known as Hypothesis Transfer Learning (HTL) (Kuzborskij & Orabona, 2016), that is, we transfer from the source hypotheses which act as a proxy to the source tasks and the risk Rpwsrck q quantifies how much source and target tasks are related.",5.3. Application to Transfer Learning,[0],[0]
"In the following we will consider SGD for HTL, where the source hypotheses act as initialization points.",5.3. Application to Transfer Learning,[0],[0]
"First, consider learning with convex losses: Theorem 3 depends on Rpw1q, thus it immediately quantifies the relatedness of source and target tasks.",5.3. Application to Transfer Learning,[0],[0]
So it is enough to pick the point that minimizes the stability bound to transfer from the most related source.,5.3. Application to Transfer Learning,[0],[0]
"Then, bounding Rpwsrck q by pRSpwsrck q through Hoeffding bound along with union bound gives with high probability that
min kPrKs pD,wsrck q ď min kPrKs O
˜
pRSpwsrck q",5.3. Application to Transfer Learning,[0],[0]
"` c logpKq m
¸
.
",5.3. Application to Transfer Learning,[0],[0]
"Hence, the most related source is the one that simply minimizes empirical risk.",5.3. Application to Transfer Learning,[0],[0]
"Similar conclusions where drawn in HTL literature, albeit in the context of ERM.",5.3. Application to Transfer Learning,[0],[0]
Matters are slightly more complicated in the non-convex case.,5.3. Application to Transfer Learning,[0],[0]
"We take a similar approach, however, now we minimize stability bound (3), and for the sake of simplicity assume that we make a full pass over the data, so T “ m. Minimizing the following empirical upper bound select the best source.",5.3. Application to Transfer Learning,[0],[0]
Proposition 1.,5.3. Application to Transfer Learning,[0],[0]
"Let pγ˘k “ Θ ´ 1 m řm i“1 }∇2fpwsrck , ziq}2 ` b
pRSpwsrck q ˘ 4 a
logpKq{m ¯
.",5.3. Application to Transfer Learning,[0],[0]
"Then with high probability the generalization error of wsrck is bounded by
min kPrKs
O
¨
˝
ˆ
1` 1 cpγ´k
˙ pRSpwsrck q cpγ",5.3. Application to Transfer Learning,[0],[0]
"` k 1`cpγ` k ¨ a logpKq
m 1 1`cpγ` k
˛
‚ .
",5.3. Application to Transfer Learning,[0],[0]
"Note that pγ˘k involves estimation of the spectral norm of the Hessian, which is computationally cheaper to evaluate compared to the complete Hessian matrix (Pearlmutter, 1994).",5.3. Application to Transfer Learning,[0],[0]
"This is particularly relevant for deep learning, where computation of the Hessian matrix can be prohibitively expensive.",5.3. Application to Transfer Learning,[0],[0]
"In this section we discuss the proof of Theorem 4, which bounds data-dependent stability for non-convex losses.
",6. Proof Outline of Theorem 4,[0],[0]
"We say that the SGD gradient update rule is an operator Gt : H ÞÑ H, such that
Gtpwq :“ w ´ αt∇fpw, zitq ,
and it is also a function of the training set S and a random index set I .",6. Proof Outline of Theorem 4,[0],[0]
"Then, wt`1 “ Gtpwtq, throughout t “ 1, . . .",6. Proof Outline of Theorem 4,[0],[0]
", T .",6. Proof Outline of Theorem 4,[0],[0]
"Recall the use of notation wS,t to indicate the output of SGD ran on a training set S, at step t, and define
δtpS, zq :“ }wS,t ´wSpiq,t} .
",6. Proof Outline of Theorem 4,[0],[0]
The following propoperty of Gt will be central to our proof.,6. Proof Outline of Theorem 4,[0],[0]
Definition 6 (Expansiveness).,6. Proof Outline of Theorem 4,[0],[0]
"A gradient update rule is η-expansive if for all w,v, }Gtpwq´Gtpvq} ď η}w´v} .
",6. Proof Outline of Theorem 4,[0],[0]
The following lemma characterizes expansiveness for the gradient update rule under different assumptions on f .,6. Proof Outline of Theorem 4,[0],[0]
"Lemma 1 ((Hardt et al., 2016)).",6. Proof Outline of Theorem 4,[0],[0]
Assume that f is β-smooth.,6. Proof Outline of Theorem 4,[0],[0]
"Then, we have that Gt is p1` αtβq-expansive.
",6. Proof Outline of Theorem 4,[0],[0]
"The following lemma is similar to Lemma 3.11 of (Hardt et al., 2016), and is instrumental in bounding the stability of SGD.",6. Proof Outline of Theorem 4,[0],[0]
"However, we make an adjustment and state it in expectation over the data.",6. Proof Outline of Theorem 4,[0],[0]
Note that it does not require convexity of the loss function.,6. Proof Outline of Theorem 4,[0],[0]
Lemma 2.,6. Proof Outline of Theorem 4,[0],[0]
"Assume that the loss function fp¨, zq P r0, 1s is L-Lipschitz for all z. Then, for every t0 P t0, 1, 2, . . .mu",6. Proof Outline of Theorem 4,[0],[0]
"we have that,
E S,z E A
“ fpwS,T , zq ´ fpwSpiq,T , zq ‰
(4)
ď L E S,z
”
E A rδT pS, zq | δt0pS, zq “ 0s
ı
` E S,A rRpASqs t0 m .
",6. Proof Outline of Theorem 4,[0],[0]
"(5)
We spend a moment to highlight the role of conditional expectation.",6. Proof Outline of Theorem 4,[0],[0]
"Observe that we could naively bound (4) by the Lipschitzness of f , but Lemma 2 follows a more careful argument.",6. Proof Outline of Theorem 4,[0],[0]
First note that t0 is a free parameter.,6. Proof Outline of Theorem 4,[0],[0]
"The expected distance in (5) between SGD outputs wS,t and wSpiq,t is conditioned on the fact that at step t0 outputs of SGD are still the same.",6. Proof Outline of Theorem 4,[0],[0]
This means that the perturbed point is encountered after t0.,6. Proof Outline of Theorem 4,[0],[0]
"Then, the conditional expectation should be a decreasing function of t0: the later the perturbation occurs, the smaller deviation between wS,t and wSpiq,t we should expect.",6. Proof Outline of Theorem 4,[0],[0]
Eventually we minimize (5) over t0.,6. Proof Outline of Theorem 4,[0],[0]
"Our proof of a stability bound for non-convex loss functions, Theorem 4, follows a general outline of (Hardt et al., 2016, Theorem 3.8).",6.1. Non-convex Losses,[0],[0]
"Namely, the outputs of SGD run on a training set S and its perturbed version Spiq will not differ too much, because by the time a perturbation is encountered, the step size has already decayed enough.",6.1. Non-convex Losses,[0],[0]
"So, on the one hand, stabilization is enforced by the diminishing the step size, and on the other hand, by how much updates expand the distance between the gradients after the perturbation.",6.1. Non-convex Losses,[0],[0]
"Since (Hardt et al., 2016) work with uniform stability, they capture the expansiveness of post-perturbation update by the Lipschitzness of the gradient.",6.1. Non-convex Losses,[0],[0]
"In combination with a recursive argument, their bound has exponential dependency on the Lipschitz constant of the gradient.",6.1. Non-convex Losses,[0],[0]
We argue that the Lipschitz continuity of the gradient can be too pessimistic in general.,6.1. Non-convex Losses,[0],[0]
"Instead, we rely on a local data-driven argument: considering that we initialize SGD at point w1, how much do updates expand the gradient under the distribution of interest?",6.1. Non-convex Losses,[0],[0]
"The following crucial lemma characterizes such behavior in terms of the curvature at w1.
",6.1. Non-convex Losses,[0],[0]
Lemma 3.,6.1. Non-convex Losses,[0],[0]
"Assume that the loss function fp¨, zq is β-smooth and that its Hessian is ρ-Lipschitz.",6.1. Non-convex Losses,[0],[0]
"Then, ›
›GtpwS,tq ´GtpwSpiq,tq › › ď p1` αtξtpS, zqq δtpS, zq
Furthermore, for any t P rT s,
E S,z rξtpS, zqs “ Õ
ˆ
E S,z
“ › ›∇2fpw1, ztq › ›
2
‰ `∆‹1,σ2 ˙ ,
∆‹1,σ2 :“ ρ ´ cσ ` a c pRpw1q ´R‹q ¯ .
",6.1. Non-convex Losses,[0],[0]
"Next, we need the following statement.
",6.1. Non-convex Losses,[0],[0]
Proposition 2 (Bernstein-type inequality).,6.1. Non-convex Losses,[0],[0]
"Let Z be a zeromean real-valued r.v., such that |Z| ď b and ErZ2s ď σ2.",6.1. Non-convex Losses,[0],[0]
"Then for all |c| ď 12b , we have that E “ ecZ ‰ ď ec2σ2 .
",6.1. Non-convex Losses,[0],[0]
"Now we are ready to prove Theorem 4.
",6.1. Non-convex Losses,[0],[0]
Sketch proof of Theorem 4.,6.1. Non-convex Losses,[0],[0]
We start from Lemma 2.,6.1. Non-convex Losses,[0],[0]
Most of the proof is dedicated to bounding the first term in (5).,6.1. Non-convex Losses,[0],[0]
"We deal with this similarly as in (Hardt et al., 2016).",6.1. Non-convex Losses,[0],[0]
"Specifically, we state the bound on EA rδT pS, zq|δt0pS, zq “ 0s by using a recursion.",6.1. Non-convex Losses,[0],[0]
"In our case, however, we also have an expectation w.r.t.",6.1. Non-convex Losses,[0],[0]
"the data, and to avoid complications with dependencies, we first unroll the recursion for the random quantities, and only then take the expectation.",6.1. Non-convex Losses,[0],[0]
"At this point the proof crucially relies on the product of exponentials arising from the recursion, and all relevant random quantities end up inside of them.",6.1. Non-convex Losses,[0],[0]
We alleviate this by Proposition 2.,6.1. Non-convex Losses,[0],[0]
"Finally, we conclude by minimizing (5) w.r.t. t0.",6.1. Non-convex Losses,[0],[0]
"Thus we have three steps: 1) recursion, 2) bounding Erexpp¨",6.1. Non-convex Losses,[0],[0]
"¨ ¨ qs, and 3) tuning of t0.
1) Recursion.",6.1. Non-convex Losses,[0],[0]
"We begin by stating the bound on EA rδT pS, zq|δt0pS, zq “ 0s by recursion.",6.1. Non-convex Losses,[0],[0]
"Thus we will first state the bound on EA rδt`1pS, zq|δt0pS, zq “ 0s in terms of EA rδtpS, zq|δt0pS, zq “ 0s, and other relevant quantities and then unravel the recursion.",6.1. Non-convex Losses,[0],[0]
"We distinguish two cases: 1) SGD encounters the perturbed point at step t, that is t “ i, and 2)",6.1. Non-convex Losses,[0],[0]
"the current point is the same in S and Spiq, so t ‰ i.",6.1. Non-convex Losses,[0],[0]
"For the first case, we will use worst-case boundedness of Gt, that is, we observe that }GtpwS,tq ´ GtpwSpiq,tq} ď δtpS, zq ` 2αtL. To handle the second case we will use Lemma 3, namely, ›
›GtpwS,tq ´GtpwSpiq,tq › › ď p1` αtξtpS, zqq δtpS, zq .
",6.1. Non-convex Losses,[0],[0]
"In addition, as a safety measure we will also take into account that the gradient update rule is at most p1 ` αtβqexpansive by Lemma 1.",6.1. Non-convex Losses,[0],[0]
"So we will work with the function ψtpS, zq :“ min tξtpS, zq, βu instead of ξtpS, zq.",6.1. Non-convex Losses,[0],[0]
"Now, introduce ∆tpS, zq :“ EArδtpS, zq | δt0pS, zq “ 0s, and decompose the expectation w.r.t.",6.1. Non-convex Losses,[0],[0]
A for a step t.,6.1. Non-convex Losses,[0],[0]
"Noting that SGD encounters the perturbed example with probability 1m ,
∆t`1pS, zq ď ˆ 1´ 1 m ˙ p1` αtψtpS, zqq∆tpS, zq
` 1 m p2αtL`∆tpS, zqq “ ˆ 1` ˆ
1´ 1 m
˙ αtψtpS, zq ˙ ∆tpS, zq ` 2αtL
m
ď exp pαtψtpS, zqq∆tpS, zq ` 2αtL
m , (6)
where the last inequality follows from 1`x ď exppxq.",6.1. Non-convex Losses,[0],[0]
"This inequality is not overly loose for x P r0, 1s, and, in our case it becomes instrumental in handling the recursion.
",6.1. Non-convex Losses,[0],[0]
"Now, observe that relation xt`1 ď atxt ` bt with xt0 “ 0 unwinds from T to t0 as xT ď řT t“t0`1 bt śT k“t`1 ak.",6.1. Non-convex Losses,[0],[0]
"Consequently, having ∆t0pS, zq “ 0, we unwind (6) to get
∆T pS, zq ď T ÿ
t“t0`1
˜
T ź
k“t`1 exp
ˆ
cψkpS, zq k
˙
¸
2cL
mt
“ T ÿ
t“t0`1 exp
˜
c T ÿ
k“t`1
ψkpS, zq k
¸
2cL mt .",6.1. Non-convex Losses,[0],[0]
"(7)
2) Bounding Erexpp¨",6.1. Non-convex Losses,[0],[0]
¨ ¨ qs.,6.1. Non-convex Losses,[0],[0]
We take expectation w.r.t.,6.1. Non-convex Losses,[0],[0]
S and z on both sides and focus on the expectation of the exponential in (7).,6.1. Non-convex Losses,[0],[0]
"First, introduce µk :“ ES,zrψkpS, zqs, and observe that the zero-mean version of ψkpS, zq is bounded as
řT k“t`1 1 k |ψkpS, zq ´ µk| ď 2β lnpT q.",6.1. Non-convex Losses,[0],[0]
"Assuming the
setting of c ď 12p2β lnpT qq2 , we apply Proposition 2 and get
E S,z
«
exp
˜
c T ÿ
k“t`1
ψkpS, zq k
¸ff ď exp ˜",6.1. Non-convex Losses,[0],[0]
"c T ÿ
k“t`1
2µk",6.1. Non-convex Losses,[0],[0]
"k
¸
,
(8)
where we bounded variance by µk thanks to the setting of c. Next, we give an upper-bound on µk, that is µk ď min tβ,ES,zrξkpS, zqsu.",6.1. Non-convex Losses,[0],[0]
"Finally, we bound ES,zrξkpS, zqs using the second result of Lemma 3, which holds for any k P rT s, to get that µk ď γ, with γ defined in (2).
3) Tuning of t0.",6.1. Non-convex Losses,[0],[0]
Now we turn our attention back to (7).,6.1. Non-convex Losses,[0],[0]
Considering that we took an expectation w.r.t.,6.1. Non-convex Losses,[0],[0]
"the data, we use (8) and the fact that µk ď γ to get that
E S,z r∆T pS, zqs ď
T ÿ
t“t0`1 exp
˜
2cγ T ÿ
k“t`1
1
k
¸
2cL
mt
ď T ÿ
t“t0`1 exp
ˆ
2cγ",6.1. Non-convex Losses,[0],[0]
"ln
ˆ
T
t
˙˙
2cL mt ď L γm
ˆ
T
t0
˙2cγ
.
",6.1. Non-convex Losses,[0],[0]
"Plug the above into (5) to get
E S,z E A
“ fpwS,T , zq ´ fpwSpiq,T , zq ‰
ď L 2
γm
ˆ
T
t0
˙2cγ
` t0 m .",6.1. Non-convex Losses,[0],[0]
"(9)
Let q “ 2cγ.",6.1. Non-convex Losses,[0],[0]
"Then, setting t0 “ ` 2cL2 ˘ 1 1`q T q 1`q minimizes (9).",6.1. Non-convex Losses,[0],[0]
Plugging t0 back we get that (9) equals to 1` 1q m ` 2cL2 ˘ 1 1`q T q 1`q .,6.1. Non-convex Losses,[0],[0]
This completes the proof.,6.1. Non-convex Losses,[0],[0]
In this work we proved data-dependent stability bounds for SGD and revisited its generalization ability.,7. Conclusions and Future Work,[0],[0]
"We presented novel bounds for convex and non-convex smooth loss functions, partially controlled by data-dependent quantities, while previous stability bounds for SGD were derived through the worst-case analysis.",7. Conclusions and Future Work,[0],[0]
"In particular, for non-convex learning, we demonstrated theoretically that generalization of SGD is heavily affected by the expected curvature around the initialization point.",7. Conclusions and Future Work,[0],[0]
We demonstrated empirically that our bound is indeed tighter compared to the uniform one.,7. Conclusions and Future Work,[0],[0]
"In addition, our data-dependent analysis also allowed us to show optimistic bounds on the generalization error of SGD, which exhibit fast rates subject to the vanishing empirical risk of the algorithm’s output.
",7. Conclusions and Future Work,[0],[0]
In future work we further intend to explore our theoretical findings experimentally and evaluate the feasibility of the transfer learning based on the second-order information.,7. Conclusions and Future Work,[0],[0]
Another direction lies in making our bounds adaptive.,7. Conclusions and Future Work,[0],[0]
"So far we have presented bounds that have data-dependent components, however the step size cannot be adjusted depending on the data, e.g. as in (Zhao & Zhang, 2015).",7. Conclusions and Future Work,[0],[0]
"This was partially addressed by (London, 2016), albeit in the context of uniform stability, and we plan to extend this idea to the context of data-dependent stability.",7. Conclusions and Future Work,[0],[0]
This work was in parts funded by the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement no 637076).,Acknowledgments,[0],[0]
This work was in parts funded by the European Research Council under the European Union’s Seventh Framework Programme (FP7/2007-2013)/ERC grant agreement no 308036.,Acknowledgments,[0],[0]
"We establish a data-dependent notion of algorithmic stability for Stochastic Gradient Descent (SGD), and employ it to develop novel generalization bounds.",abstractText,[0],[0]
This is in contrast to previous distribution-free algorithmic stability results for SGD which depend on the worst-case constants.,abstractText,[0],[0]
"By virtue of the data-dependent argument, our bounds provide new insights into learning with SGD on convex and non-convex problems.",abstractText,[0],[0]
"In the convex case, we show that the bound on the generalization error depends on the risk at the initialization point.",abstractText,[0],[0]
"In the non-convex case, we prove that the expected curvature of the objective function around the initialization point has crucial influence on the generalization error.",abstractText,[0],[0]
"In both cases, our results suggest a simple data-driven strategy to stabilize SGD by pre-screening its initialization.",abstractText,[0],[0]
"As a corollary, our results allow us to show optimistic generalization bounds that exhibit fast convergence rates for SGD subject to a vanishing empirical risk and low noise of stochastic gradient.",abstractText,[0],[0]
Data-Dependent Stability of Stochastic Gradient Descent,title,[0],[0]
"Many sequential decision problems, including diabetes treatment (Bastani, 2014), digital marketing (Theocharous et al., 2015), and robot control (Lillicrap et al., 2015), are modeled as Markov decision processes (MDPs) and solved using reinforcement learning (RL) algorithms.",1. Introduction,[0],[0]
One important problem when applying RL to real problems is policy evaluation.,1. Introduction,[0],[0]
The goal in policy evaluation is to estimate the expected return (sum of rewards) produced by a policy.,1. Introduction,[0],[0]
"We refer to this policy as the evaluation policy, πe.",1. Introduction,[0],[0]
The standard policy evaluation approach is to repeatedly deploy πe and average the resulting returns.,1. Introduction,[0],[0]
"While this naı̈ve Monte Carlo estimator is unbiased, it may have high variance.
",1. Introduction,[0],[0]
"1The University of Texas at Austin, Austin, Texas, USA 2The University of Massachusetts, Amherst, Massachusetts, USA 3Carnegie Mellon University, Pittsburgh, Pennsylvania, USA.",1. Introduction,[0],[0]
"Correspondence to: Josiah P. Hanna <jphanna@cs.utexas.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
Methods that evaluate πe while selecting actions according to πe are termed on-policy.,1. Introduction,[0],[0]
"Previous work has addressed variance reduction for on-policy returns (Zinkevich et al., 2006; White & Bowling, 2009; Veness et al., 2011).",1. Introduction,[0],[0]
"An alternative approach is to estimate the performance of πe while following a different, behavior policy, πb. Methods that evaluate πe with data generated from πb are termed offpolicy.",1. Introduction,[0],[0]
Importance sampling (IS) is one standard approach for using off-policy data in RL.,1. Introduction,[0],[0]
"IS reweights returns observed while executing πb such that they are unbiased estimates of the performance of πe.
",1. Introduction,[0],[0]
"Presently, IS is usually used when off-policy data is already available or when executing πe is impractical.",1. Introduction,[0],[0]
"If πb is not chosen carefully, IS often has high variance (Thomas et al., 2015).",1. Introduction,[0],[0]
"For this reason, an implicit assumption in the RL community has generally been that on-policy evaluation is more accurate when it is feasible.",1. Introduction,[0],[0]
"However, IS can also be used for variance reduction when done with an appropriately selected distribution of returns (Hammersley & Handscomb, 1964).",1. Introduction,[0],[0]
"While IS-based variance reduction has been explored in RL, this prior work has required knowledge of the environment’s transition probabilities and remains onpolicy (Desai & Glynn, 2001; Frank et al., 2008; Ciosek & Whiteson, 2017).",1. Introduction,[0],[0]
"In contrast to this earlier work, we show how careful selection of the behavior policy can lead to lower variance policy evaluation than using the evaluation policy and do not require knowledge of the environment’s transition probabilities.
",1. Introduction,[0],[0]
"In this paper, we formalize the selection of πb as the behavior policy search problem.",1. Introduction,[0],[0]
We introduce a method for this problem that adapts the policy parameters of πb with gradient descent on the variance of importance-sampling.,1. Introduction,[0],[0]
Empirically we demonstrate behavior policy search with our method lowers the mean squared error of estimates compared to on-policy estimates.,1. Introduction,[0],[0]
"To the best of our knowledge, this work is the first to propose adapting the behavior policy to obtain better policy evaluation in RL.",1. Introduction,[0],[0]
Furthermore we present the first method to address this problem.,1. Introduction,[0],[0]
"This section details the policy evaluation problem setting, the Monte Carlo and Advantage Sum on-policy methods, and importance-sampling for off-policy evaluation.",2. Preliminaries,[0],[0]
"We use notational standard MDPNv1 (Thomas, 2015), and for simplicity, we assume that S,A, and R are finite.1 Let H := (S0, A0, R0, S1, . . .",2.1. Background,[0],[0]
", SL, AL, RL) be a trajectory and g(H) := ∑L t=0 γ
tRt be the discounted return of trajectory H .",2.1. Background,[0],[0]
Let ρ(π),2.1. Background,[0],[0]
:= E[g(H)|H ∼ π] be the expected discounted return when the stochastic policy π is used from S0 sampled from the initial state distribution.,2.1. Background,[0],[0]
"In this work, we consider parameterized policies, πθ, where the distribution over actions is determined by the vector θ.",2.1. Background,[0],[0]
"We assume that the transitions and reward function are unknown and that L is finite.
",2.1. Background,[0],[0]
"We are given an evaluation policy, πe, for which we would like to estimate ρ(πe).",2.1. Background,[0],[0]
We assume there exists a policy parameter vector θe such that πe = πθe and that this vector is known.,2.1. Background,[0],[0]
"We consider an incremental setting where, at iteration i, we sample a single trajectory Hi with a policy πθi and add {Hi,θi} to a set D. We use Di to denote the set at iteration i. Methods that always (i.e., ∀i) choose θi = θe are on-policy; otherwise, the method is off-policy.",2.1. Background,[0],[0]
"A policy evaluation method, PE, uses all trajectories in Di to estimate ρ(πe).",2.1. Background,[0],[0]
Our goal is to design a policy evaluation algorithm that produces estimates of ρ(πe) that have low mean squared error (MSE).,2.1. Background,[0],[0]
"Formally, the goal of policy evaluation with PE is to minimize (PE(Di)− ρ(πe))2.",2.1. Background,[0],[0]
"While other measures of policy evaluation accuracy could be considered, we follow earlier work in using MSE (e.g., (Thomas & Brunskill, 2016; Precup et al., 2000)).
",2.1. Background,[0],[0]
We focus on unbiased estimators of ρ(πe).,2.1. Background,[0],[0]
"While biased estimators (e.g., bootstrapping methods (Sutton & Barto, 1998), approximate models (Kearns & Singh, 2002), etc.) can sometimes produce lower MSE estimates they are problematic for high risk applications requiring confidence intervals.",2.1. Background,[0],[0]
"For unbiased estimators, minimizing variance is equivalent to minimizing MSE.",2.1. Background,[0],[0]
Perhaps the most commonly used policy evaluation method is the on-policy Monte-Carlo (MC) estimator.,2.2. Monte-Carlo Estimates,[0],[0]
"The estimate of ρ(πe) at iteration i is the average return:
MC(Di) :",2.2. Monte-Carlo Estimates,[0],[0]
"= 1
i+ 1 i∑ j=0 L∑ t=0 γtRt = 1 i+ 1 i∑ j=0 g(Hj).
",2.2. Monte-Carlo Estimates,[0],[0]
This estimator is unbiased and strongly consistent given mild assumptions.2,2.2. Monte-Carlo Estimates,[0],[0]
"However, this method can have high variance.
1The methods, and theoretical results discussed in this paper are applicable to both finite and infinite S,A and R as well as partially-observable Markov decision processes.
",2.2. Monte-Carlo Estimates,[0],[0]
2Being a strongly consistent estimator of ρ(πe) means that,2.2. Monte-Carlo Estimates,[0],[0]
"Like the Monte-Carlo estimator, the advantage sum (ASE) estimator selects θi = θe for all i. However, it introduces a control variate to reduce the variance without introducing bias.",2.3. Advantage Sum Estimates,[0],[0]
This control variate requires an approximate model of the MDP to be provided.,2.3. Advantage Sum Estimates,[0],[0]
"Let the reward function of this model be given as r̂(s, a).",2.3. Advantage Sum Estimates,[0],[0]
"Let q̂πe(st, at) =",2.3. Advantage Sum Estimates,[0],[0]
"E[ ∑L t′=t γ
t′ r̂(st′ , at′)] and v̂πe(st) =",2.3. Advantage Sum Estimates,[0],[0]
"E[q̂πe(st, at)|at ∼ πe], i.e., the action-value function and state-value function of πe in this approximate model.",2.3. Advantage Sum Estimates,[0],[0]
"Then, the advantage sum estimator is given by:
AS(Di) := 1
i+ 1 i∑ j=0 L∑ t=0 γt(Rt− q̂πe(St, At)+ v̂πe(St)).
",2.3. Advantage Sum Estimates,[0],[0]
"Intuitively, ASE is replacing part of the randomness of the Monte Carlo return with the known expected return under the approximate model.",2.3. Advantage Sum Estimates,[0],[0]
"Provided qπe(St, At)− v̂πe(St) is sufficiently correlated with Rt, the variance of ASE is less than that of MC.
Notice that, like the MC estimator, the ASE estimator is on-policy, in that the behavior policy is always the policy that we wish to evaluate.",2.3. Advantage Sum Estimates,[0],[0]
Intuitively it may seems like this choice should be optimal.,2.3. Advantage Sum Estimates,[0],[0]
"However, we will show that it is not—selecting behavior policies that are different from the evaluation policy can result in estimates of ρ(πe) that have lower variance.",2.3. Advantage Sum Estimates,[0],[0]
"Importance Sampling is a method for reweighting returns from a behavior policy, θ, such that they are unbiased returns from the evaluation policy.",2.4. Importance Sampling,[0],[0]
"In RL, the re-weighted IS return of a trajectory, H , sampled from πθ is:
IS(H,θ) := g(H) L∏ t=0 πe(St|At) πθ(St|At) .
",2.4. Importance Sampling,[0],[0]
The IS off-policy estimator is then a Monte Carlo estimate of E,2.4. Importance Sampling,[0],[0]
"[IS(H,θ)|H ∼ πθ]:
IS(Di) := 1
i+ 1 i∑ j=0 IS(Hj ,θj).
",2.4. Importance Sampling,[0],[0]
"In RL, importance sampling allows off-policy data to be used as if it were on-policy.",2.4. Importance Sampling,[0],[0]
"In this case the variance of the IS estimate is often much worse than the variance of on-policy MC estimates because the behavior policy is not
Pr ( lim i→∞ MC(Di) = ρ(πe) )",2.4. Importance Sampling,[0],[0]
= 1.,2.4. Importance Sampling,[0],[0]
"If ρ(πe) exists, MC is strongly consistent by the Khintchine Strong law of large numbers (Sen & Singer, 1993).
chosen to minimize variance, but is a policy that is dictated by circumstance.",2.4. Importance Sampling,[0],[0]
"Importance sampling was originally intended as a variance reduction technique for Monte Carlo evaluation (Hammersley & Handscomb, 1964).",3. Behavior Policy Search,[0],[0]
When an evaluation policy rarely samples trajectories with high magnitude returns a Monte Carlo evaluation will have high variance.,3. Behavior Policy Search,[0],[0]
If a behavior policy can increase the probability of observing such trajectories then the off-policy IS estimate will have lower variance than an on-policy Monte Carlo estimate.,3. Behavior Policy Search,[0],[0]
In this section we first describe the theoretical potential for variance reduction with an appropriately selected behavior policy.,3. Behavior Policy Search,[0],[0]
In general this policy will be unknown.,3. Behavior Policy Search,[0],[0]
"Thus, we propose a policy evaluation subproblem — the behavior policy search problem — solutions to which will adapt the behavior policy to provide lower mean squared error policy performance estimates.",3. Behavior Policy Search,[0],[0]
"To the best of our knowledge, we are the first to propose behavior policy adaptation for policy evaluation.",3. Behavior Policy Search,[0],[0]
An appropriately selected behavior policy can lower variance to zero.,3.1. The Optimal Behavior Policy,[0],[0]
"While this fact is generally known for importance-sampling, we show here that this policy exists for any MDP and evaluation policy under two restrictive assumptions: all returns are positive and the domain is deterministic.",3.1. The Optimal Behavior Policy,[0],[0]
"In the following section we describe how an initial policy can be adapted towards the optimal behavior policy even when these conditions fail to hold.
",3.1. The Optimal Behavior Policy,[0],[0]
Let wπ(H) := ∏L t=0 π(At|St).,3.1. The Optimal Behavior Policy,[0],[0]
"Consider a behavior policy π?b such that for any trajectory, H:
ρ(πe) = IS(H,π ?",3.1. The Optimal Behavior Policy,[0],[0]
"b ) = g(H)
wπe(H) wπ?b (H) .
",3.1. The Optimal Behavior Policy,[0],[0]
"Rearranging the terms of this expressions yields:
wπ?b (H) = g(H) wπe(H)
ρ(πe) .
",3.1. The Optimal Behavior Policy,[0],[0]
"Thus, if we can select π?b such that the probability of observing any H ∼ π?b is g(H) ρ(πe)
times the likelihood of observing H ∼ πe then the IS estimate has zero MSE with only a single sampled trajectory.",3.1. The Optimal Behavior Policy,[0],[0]
"Regardless of g(H), the importance-sampled return will equal ρ(πe).
",3.1. The Optimal Behavior Policy,[0],[0]
"Furthermore, the policy π?b exists within the space of all feasible stochastic policies.",3.1. The Optimal Behavior Policy,[0],[0]
"Consider that a stochastic policy can be viewed as a mixture policy over time-dependent (i.e., action selection depends on the current time-step) deterministic policies.",3.1. The Optimal Behavior Policy,[0],[0]
"For example, in an MDP with one state, two actions and a horizon of L there are 2L possible time-dependent deterministic policies, each of which
defines a unique sequence of actions.",3.1. The Optimal Behavior Policy,[0],[0]
We can express any evaluation policy as a mixture of these deterministic policies.,3.1. The Optimal Behavior Policy,[0],[0]
"The optimal behavior policy π?b can be expressed similarly and thus the optimal behavior policy exists.
",3.1. The Optimal Behavior Policy,[0],[0]
"Unfortunately, the optimal behavior policy depends on the unknown value ρ(πe) as well as the unknown reward function R (via g(H)).",3.1. The Optimal Behavior Policy,[0],[0]
"Thus, while there exists an optimal behavior policy for IS – which is not πe – in practice we cannot analytically determine π?b .",3.1. The Optimal Behavior Policy,[0],[0]
"Additionally, π ?",3.1. The Optimal Behavior Policy,[0],[0]
b may not be representable by any θ in our policy class.,3.1. The Optimal Behavior Policy,[0],[0]
"Since the optimal behavior policy cannot be analytically determined, we instead propose the behavior policy search (BPS) problem for finding πb that lowers the MSE of estimates of ρ(πe).",3.2. The Behavior Policy Search Problem,[0],[0]
"A BPS problem is defined by the inputs:
1.",3.2. The Behavior Policy Search Problem,[0],[0]
An evaluation policy πe with policy parameters θe. 2.,3.2. The Behavior Policy Search Problem,[0],[0]
"An off-policy policy evaluation algorithm,
OPE(H,θ), that takes a trajectory, H ∼ πθ, or, alternatively, a set of trajectories, and returns an estimate of ρ(πe).
",3.2. The Behavior Policy Search Problem,[0],[0]
"A BPS solution is a policy, πθb such that off-policy estimates with OPE have lower MSE than on-policy estimates.",3.2. The Behavior Policy Search Problem,[0],[0]
"Methods for this problem are BPS algorithms.
",3.2. The Behavior Policy Search Problem,[0],[0]
Recall we have formalized policy evaluation within an incremental setting where one trajectory for policy evaluation is generated each iteration.,3.2. The Behavior Policy Search Problem,[0],[0]
"At the ith iteration, a BPS algorithm selects a behavior policy that will be used to generate a trajectory, Hi.",3.2. The Behavior Policy Search Problem,[0],[0]
"The policy evaluation algorithm, OPE, then estimates ρ(πe) using trajectories in Di.",3.2. The Behavior Policy Search Problem,[0],[0]
"Naturally, the selection of the behavior policy depends on how OPE estimates ρ(πe).
",3.2. The Behavior Policy Search Problem,[0],[0]
"In a BPS problem, the ith iteration proceeds as follows.",3.2. The Behavior Policy Search Problem,[0],[0]
"First, given all of the past behavior policies, {θi}i−1i=0, and the resulting trajectories, {Hi}i−1i=0, the BPS algorithm must select θi.",3.2. The Behavior Policy Search Problem,[0],[0]
The policy πθi is then run for one episode to create the trajectory Hi.,3.2. The Behavior Policy Search Problem,[0],[0]
"Then the BPS algorithm uses OPE to estimate ρ(πe) given the available data, Di := {(θi, Hi)}ii=0.",3.2. The Behavior Policy Search Problem,[0],[0]
"In this paper, we consider the one-step problem of selecting θi and estimating ρ(πe) at iteration i in a way that minimizes MSE.",3.2. The Behavior Policy Search Problem,[0],[0]
"That is, we do not consider how our selection of θi will impact our future ability to select an appropriate θj for j > i and thus to produce more accurate estimates in the future.
",3.2. The Behavior Policy Search Problem,[0],[0]
"One natural question is: if we are given a limit on the number of trajectories that can be sampled, is it better to “spend” some of our limited trajectories on BPS instead of using on-policy estimates?",3.2. The Behavior Policy Search Problem,[0],[0]
"Since each OPE(Hi,θi) is an unbiased estimator of ρ(πe), we can use all sampled trajectories to compute OPE(Di).",3.2. The Behavior Policy Search Problem,[0],[0]
"Provided for all itera-
tions, Var[OPE(H,θi)] ≤ V ar[MC] then, in expectation, a BPS algorithm will always achieve lower MSE than MC, showing that it is, in fact, worthwhile to do so.",3.2. The Behavior Policy Search Problem,[0],[0]
This claim is supported by our empirical study.,3.2. The Behavior Policy Search Problem,[0],[0]
We now introduce our primary contributions: an analytic expression for the gradient of the mean squared error of the IS estimator and a stochastic gradient descent algorithm that adapts θ to minimize the MSE between the IS estimate and ρ(πe).,4. Behavior Policy Gradient Theorem,[0],[0]
Our algorithm — Behavior Policy Gradient (BPG) — begins with on-policy estimates and adapts the behavior policy with gradient descent on the MSE with respect to θ.,4. Behavior Policy Gradient Theorem,[0],[0]
"The gradient of the MSE with respect to the policy parameters is given by the following theorem: Theorem 1.
∂
∂θ MSE[IS(H,θ)]",4. Behavior Policy Gradient Theorem,[0],[0]
"= E
[ − IS(H,θ)2
L∑ t=0 ∂ ∂θ log πθ(At|St)
]
where the expectation is taken over H ∼ πθ.
",4. Behavior Policy Gradient Theorem,[0],[0]
Proof.,4. Behavior Policy Gradient Theorem,[0],[0]
"Proofs for all theoretical results are included in Appendix A of an extended version available at http: //arxiv.org/abs/1706.03469.
",4. Behavior Policy Gradient Theorem,[0],[0]
BPG uses stochastic gradient descent in place of exact gradient descent: replacing the intractable expectation in Theorem 1 with an unbiased estimate of the true gradient.,4. Behavior Policy Gradient Theorem,[0],[0]
"In our experiments, we sample a batch, Bi, of k trajectories with πθi to lower the variance of the gradient estimate at iteration i.",4. Behavior Policy Gradient Theorem,[0],[0]
"In the BPS setting, sampling a batch of trajectories is equivalent to holding θ fixed for k iterations and then updating θ with the k most recent trajectories used to compute the gradient estimate.
",4. Behavior Policy Gradient Theorem,[0],[0]
Full details of BPG are given in Algorithm 1.,4. Behavior Policy Gradient Theorem,[0],[0]
"At iteration i, BPG samples a batch, Bi, of k trajectories and adds {(θi, Hi)ki=0} to a data set D (Lines 4-5).",4. Behavior Policy Gradient Theorem,[0],[0]
Then BPG updates θ with an empirical estimate of Theorem 1 (Line 6).,4. Behavior Policy Gradient Theorem,[0],[0]
"After n iterations, the BPG estimate of ρ(πe) is IS(Dn) as defined in Section 2.4.
",4. Behavior Policy Gradient Theorem,[0],[0]
"Given that the step-size, αi, is consistent with standard gradient descent convergence conditions, BPG will converge to a behavior policy that locally minimizes the variance (Bertsekas & Tsitsiklis, 2000).",4. Behavior Policy Gradient Theorem,[0],[0]
"At best, BPG converges to the globally optimal behavior policy within the parameterization of πe.",4. Behavior Policy Gradient Theorem,[0],[0]
Since the parameterization of πe determines the class of representable distributions it is possible that the theoretically optimal behavior policy is unrepresentable under this parameterization.,4. Behavior Policy Gradient Theorem,[0],[0]
"Nevertheless, a suboptimal behavior policy still yields better estimates of ρ(πe), provided it decreases variance compared to on-policy returns.
",4. Behavior Policy Gradient Theorem,[0],[0]
"Algorithm 1 Behavior Policy Gradient Input: Evaluation policy parameters, θe, batch size k, a step-size for each iteration, αi, and number of iterations n. Output: Final behavior policy parameters θn and the IS estimate of ρ(πe) using all sampled trajectories.",4. Behavior Policy Gradient Theorem,[0],[0]
1: θ0 ← θe 2: D0 = {} 3: for all i ∈ 0...n,4. Behavior Policy Gradient Theorem,[0],[0]
"do 4: Bi = Sample k trajectories H ∼ πθi 5: Di+1 = Di ∪ Bi
6: θi+1 = θi + αik ∑ H∈B",4. Behavior Policy Gradient Theorem,[0],[0]
"IS(H,θ)2 L∑ t=0 ∂ ∂θ log πθi(At|St) 7: end for 8: Return θn, IS(Dn)",4. Behavior Policy Gradient Theorem,[0],[0]
"In cases where an approximate model is available, we can further lower variance adapting the behavior policy of the doubly robust estimator (Jiang & Li, 2016; Thomas & Brunskill, 2016).",4.1. Control Variate Extension,[0],[0]
"Based on a similar intuition as the Advantage Sum estimator (Section 2.3), the Doubly Robust (DR) estimator uses the value functions of an approximate model as a control variate to lower the variance of importancesampling.3 We show here that we can adapt the behavior policy to lower the mean squared error of DR estimates.",4.1. Control Variate Extension,[0],[0]
"We denote this new method DR-BPG for Doubly Robust Behavior Policy Gradient.
",4.1. Control Variate Extension,[0],[0]
"Let wπ,t(H) = ∏t i=0 π(At|St) and recall that v̂πe and q̂πe are the state and action value functions of πe in the approximate model.",4.1. Control Variate Extension,[0],[0]
"The DR estimator is:
DR(H,θ) := v̂(S0)+ L∑ t=0 wπe,t wπθ ,t (Rt−q̂πe(St, At)+v̂πe(St+1)).
",4.1. Control Variate Extension,[0],[0]
"We can reduce the mean squared error of DR with gradient descent using unbiased estimates of the following corollary to Theorem 1: Corollary 1.
∂
∂θ MSE",4.1. Control Variate Extension,[0],[0]
"[DR(H,θ)]",4.1. Control Variate Extension,[0],[0]
"= E[(DR(H,θ)2 L∑ t=0 ∂ ∂θ log πθ(At|St)
",4.1. Control Variate Extension,[0],[0]
"− 2DR(H,θ)( L∑ t=0 γtδt wπe,t wθ,t t∑ i=0 ∂ ∂θ log πθ(Ai|Si))",4.1. Control Variate Extension,[0],[0]
"]
where δt = Rt− q̂(St, At) + v̂(St+1) and the expectation is taken over H ∼ πθ.
",4.1. Control Variate Extension,[0],[0]
"The first term of ∂∂θMSE is analogous to the gradient of the importance-sampling estimate with IS(H,θ) replaced
3DR lowers the variance of per-decision importance-sampling which importance samples the per time-step reward.
by DR(H,θ).",4.1. Control Variate Extension,[0],[0]
"The second term accounts for the covariance of the DR terms.
",4.1. Control Variate Extension,[0],[0]
"AS and DR both assume access to a model, however, they make no assumption about where the model comes from except that it must be independent of the trajectories used to compute the final estimate.",4.1. Control Variate Extension,[0],[0]
"In practice, AS and DR perform best when all trajectories are used to estimate the model and then used to estimate ρ(πe) (Thomas & Brunskill, 2016).",4.1. Control Variate Extension,[0],[0]
"However, for DR-BPG, changes to the model change the surface of the MSE objective we seek to minimize and thus DR-BPG will only converge once the model stops changing.",4.1. Control Variate Extension,[0],[0]
"In our experiments, we consider both a changing and a fixed model.",4.1. Control Variate Extension,[0],[0]
"BPG is closely related to existing work in policy gradient RL (c.f., (Sutton et al., 2000))",4.2. Connection to REINFORCE,[0],[0]
and we draw connections between one such method and BPG to illustrate how BPG changes the distribution of trajectories.,4.2. Connection to REINFORCE,[0],[0]
"REINFORCE (Williams, 1992) attempts to maximize ρ(πθ) through gradient ascent on ρ(πθ) using the following unbiased gradient of ρ(πθ):
∂
∂θ ρ(πθ) =",4.2. Connection to REINFORCE,[0],[0]
"E
[ g(H)
L∑ t=0 ∂ ∂θ log πθ(At|St) ∣∣∣∣∣H ∼ πθ ] .
",4.2. Connection to REINFORCE,[0],[0]
"Intuitively, REINFORCE increases the probability of all actions taken during H as a function of g(H).",4.2. Connection to REINFORCE,[0],[0]
This update increases the probability of actions that lead to high return trajectories.,4.2. Connection to REINFORCE,[0],[0]
BPG can be interpreted as REINFORCE where the return of a trajectory is the square of its importance-sampled return.,4.2. Connection to REINFORCE,[0],[0]
"Thus BPG increases the probability of all actions taken along H as a function of IS(H,θ)2.",4.2. Connection to REINFORCE,[0],[0]
"The magnitude of IS(H,θ)2 depends on two qualities of H:
1.",4.2. Connection to REINFORCE,[0],[0]
"g(H)2 is large (i.e., a high magnitude event).",4.2. Connection to REINFORCE,[0],[0]
2.,4.2. Connection to REINFORCE,[0],[0]
"H is rare relative to its probability under the evalua-
tion policy (i.e., ∏L t=0 πe(At|St) πθ(At|St) is large).
",4.2. Connection to REINFORCE,[0],[0]
These two qualities demonstrate a balance in how BPG changes trajectory probabilities.,4.2. Connection to REINFORCE,[0],[0]
"Increasing the probability of a trajectory under πθ will decrease IS(H,θ)2 and so BPG increases the probability of a trajectory when g(H)2 is large enough to offset the decrease in IS(H,θ)2 caused by decreasing the importance weight.",4.2. Connection to REINFORCE,[0],[0]
This section presents an empirical study of variance reduction through behavior policy search.,5. Empirical Study,[0],[0]
"We design our experiments to answer the following questions:
• Can behavior policy search with BPG reduce policy evaluation MSE compared to on-policy estimates in
both tabular and continuous domains?",5. Empirical Study,[0],[0]
"• Does adapting the behavior policy of the Doubly Ro-
bust estimator with DR-BPG lower the MSE of the Advantage Sum estimator?",5. Empirical Study,[0],[0]
•,5. Empirical Study,[0],[0]
Does the rarety of actions that cause high magnitude rewards affect the performance gap between BPG and Monte Carlo estimates?,5. Empirical Study,[0],[0]
We address our first experimental question by evaluating BPG in three domains.,5.1. Experimental Set-up,[0],[0]
"We briefly describe each domain here; full details are available in appendix C.
The first domain is a 4x4 Gridworld.",5.1. Experimental Set-up,[0],[0]
"We obtain two evaluation policies by applying REINFORCE to this task, starting from a policy that selects actions uniformly at random.",5.1. Experimental Set-up,[0],[0]
"We then select one evaluation policy, π1, from the early stages of learning – an improved policy but still far from converged – and one after learning has converged, π2.",5.1. Experimental Set-up,[0],[0]
"We run all experiments once with πe := π1 and a second time with πe := π2.
",5.1. Experimental Set-up,[0],[0]
"Our second and third tasks are the continuous control Cartpole Swing Up and Acrobot tasks implemented within RLLAB (Duan et al., 2016).",5.1. Experimental Set-up,[0],[0]
The evaluation policy in each domain is a neural network that maps the state to the mean of a Gaussian distribution.,5.1. Experimental Set-up,[0],[0]
"Policies are partially optimized with trust-region policy optimization (Schulman et al., 2015) applied to a randomly initialized policy.",5.1. Experimental Set-up,[0],[0]
"Gridworld Experiments Figure 1 compares BPG to Monte Carlo for both Gridworld policies, π1 and π2.",5.2. Main Results,[0],[0]
Our main point of comparison is the mean squared error (MSE) of both estimates at iteration i over 100 trials.,5.2. Main Results,[0],[0]
"For π1, BPG significantly reduces the MSE of on-policy estimates (Figure 1a).",5.2. Main Results,[0],[0]
"For π2, BPG also reduces MSE, however, it is only a marginal improvement.
",5.2. Main Results,[0],[0]
At the end of each trial we used the final behavior policy to collect 100 more trajectories and estimate ρ(πe).,5.2. Main Results,[0],[0]
"In comparison to a Monte Carlo estimate with 100 trajectories from π1, MSE is 85.48 % lower with this improved behavior policy.",5.2. Main Results,[0],[0]
"For π2, the MSE is 31.02 % lower.",5.2. Main Results,[0],[0]
"This result demonstrates that BPG can find behavior policies that substantially lower MSE.
",5.2. Main Results,[0],[0]
"To understand the disparity in performance between these two instances of policy evaluation, we plot the distribution of g(H) under πe (Figures 1c and 1d).",5.2. Main Results,[0],[0]
These plots show the variance of π1 to be much higher; it sometimes samples returns with twice the magnitude of any sampled by π2.,5.2. Main Results,[0],[0]
"To quantify this difference, we also measure the variance of IS(H,θi) as E [ IS(H)2
∣∣H ∼ πθi]−E",5.2. Main Results,[0],[0]
"[IS(H)|H ∼ πθi ]2 where the expectations are estimated with 10,000 trajecto-
ries.",5.2. Main Results,[0],[0]
This evaluation is repeated 5 times per iteration and the reported variance is the mean over these evaluations.,5.2. Main Results,[0],[0]
The decrease in variance for each policy is shown in Figure 1e.,5.2. Main Results,[0],[0]
"The high initial variance means there is much more room for BPG to improve the behavior policy when θe is the partially optimized policy.
",5.2. Main Results,[0],[0]
We also test the sensitivity of BPG to the learning rate parameter.,5.2. Main Results,[0],[0]
"A critical issue in the use of BPG is selecting the
step size parameter α.",5.2. Main Results,[0],[0]
If α is set too high we risk making too large of an update to θ — potentially stepping to a worse behavior policy.,5.2. Main Results,[0],[0]
If we are too conservative then it will take many iterations for a noticeable improvement over Monte Carlo estimation.,5.2. Main Results,[0],[0]
Figure 1f shows variance reduction for a number of different α values in the GridWorld domain.,5.2. Main Results,[0],[0]
We found BPG in this domain was robust to a variety of step size values.,5.2. Main Results,[0],[0]
"We do not claim this result is representative for all problem domains; stepsize selection in the behavior policy search problem is an important area for future work.
",5.2. Main Results,[0],[0]
Continuous Control Figure 2 shows reduction of MSE on the Cartpole Swing-up and Acrobot domains.,5.2. Main Results,[0],[0]
Again we see that BPG reduces MSE faster than Monte Carlo evaluation.,5.2. Main Results,[0],[0]
"In contrast to the discrete Gridworld experiment, this experiment demonstrates the applicability of BPG to the continuous control setting.",5.2. Main Results,[0],[0]
"While BPG significantly outperforms Monte Carlo evaluation in Cart-pole Swingup, the gap is much smaller in Acrobot.",5.2. Main Results,[0],[0]
This result also demonstrates BPG (and behavior policy search) when the policy must generalize across different states.,5.2. Main Results,[0],[0]
"In this section, we evaluate the combination of modelbased control variates with behavior policy search.",5.3. Control Variate Extensions,[0],[0]
"Specifically, we compare the AS estimator with Doubly Robust BPG (DR-BPG).",5.3. Control Variate Extensions,[0],[0]
In these experiments we use a 10x10 stochastic gridworld.,5.3. Control Variate Extensions,[0],[0]
"The added stochasticity increases the difficulty of building an accurate model from trajectories.
",5.3. Control Variate Extensions,[0],[0]
Since these methods require a model we construct this model in one of two ways.,5.3. Control Variate Extensions,[0],[0]
The first method uses all trajectories in D to build the model and then uses the same set to estimate ρ(πe) with ASE or DR.,5.3. Control Variate Extensions,[0],[0]
The second method uses trajectories from the first 10 iterations to build the model and then fixes the model for the remaining iterations.,5.3. Control Variate Extensions,[0],[0]
"For DR-BPG, behavior policy search starts at iteration 10 un-
der this second condition.",5.3. Control Variate Extensions,[0],[0]
We call the first method “update” and the second method “fixed.”,5.3. Control Variate Extensions,[0],[0]
The update method invalidates the theoretical guarantees of these methods but learns a more accurate model.,5.3. Control Variate Extensions,[0],[0]
"In both instances, we build maximum likelihood tabular models.
",5.3. Control Variate Extensions,[0],[0]
Figure 3 demonstrates that combining BPG with a modelbased control variate (DR-BPG) can lead to further reduction of MSE compared to the control variate alone (ASE).,5.3. Control Variate Extensions,[0],[0]
"Specifically, with the fixed model, DR-BPG outperformed all other methods.",5.3. Control Variate Extensions,[0],[0]
DR-BPG using the update method for building the model performed competitively with ASE although not statistically significantly better.,5.3. Control Variate Extensions,[0],[0]
We also evaluate the final learned behavior policy of the fixed model variant of DR-BPG.,5.3. Control Variate Extensions,[0],[0]
"For a batch size of 100 trajectories, the DR estimator with this behavior policy improves upon the ASE estimator with the same model by 56.9 %.
",5.3. Control Variate Extensions,[0],[0]
"For DR-BPG, estimating the model with all data still allowed steady progress towards lower variance.",5.3. Control Variate Extensions,[0],[0]
This result is interesting since a changing model changes the surface of our variance objective and thus gradient descent on the variance has no theoretical guarantees of convergence.,5.3. Control Variate Extensions,[0],[0]
"Empirically, we observe that setting the learning rate for DRBPG was more challenging for either model type.",5.3. Control Variate Extensions,[0],[0]
"Thus while we have shown BPG can be combined with control variates, more work is needed to produce a robust method.",5.3. Control Variate Extensions,[0],[0]
Our final experiment aims to understand how the gap between on- and off-policy variance is affected by the probability of rare events.,5.4. Rareness of Event,[0],[0]
The intuition for why behavior policy search can lower the variance of on-policy estimates is that a well selected behavior policy can cause rare and high magnitude events to occur.,5.4. Rareness of Event,[0],[0]
"We test this intuition by varying the probability of a rare, high magnitude event and observing how this change affects the performance gap between on- and off-policy evaluation.",5.4. Rareness of Event,[0],[0]
"For this experiment, we use a variant of the deterministic Gridworld where taking the UP action in the initial state (the upper left corner) causes a transition to the terminal state with a reward of +50.",5.4. Rareness of Event,[0],[0]
We use π1 from our earlier Gridworld experiments but we vary the probability of choosing UP when in the initial state.,5.4. Rareness of Event,[0],[0]
So with probability p the agent will receive a large reward and end the trajectory.,5.4. Rareness of Event,[0],[0]
We use a constant learning rate of 10−5 for all values of p and run BPG for 500 iterations.,5.4. Rareness of Event,[0],[0]
We plot the relative decrease of the variance as a function of p over 100 trials for each value of p.,5.4. Rareness of Event,[0],[0]
We use relative variance to normalize across problem instances.,5.4. Rareness of Event,[0],[0]
"Note that under this measure, even when p is close to 1, the relative variance is not equal to zero because as p approaches 1 the initial variance also goes to zero.
",5.4. Rareness of Event,[0],[0]
"This experiment illustrates that as the initial variance increases, the amount of improvement BPG can achieve increases.",5.4. Rareness of Event,[0],[0]
"As p becomes closer to 1, the initial variance becomes closer to zero and BPG barely improves over the variance of Monte Carlo (in terms of absolute variance there is no improvement).",5.4. Rareness of Event,[0],[0]
"When the πe rarely takes the high rewarding UP action (p close to 0), BPG improves policy evaluation by increasing the probability of this action.",5.4. Rareness of Event,[0],[0]
This experiment supports our intuition for why off-policy evaluation can outperform on-policy evaluation.,5.4. Rareness of Event,[0],[0]
Behavior policy search and BPG are closely related to existing work on adaptive importance-sampling.,6. Related Work,[0],[0]
"While adaptive importance-sampling has been studied in the estimation literature, we focus here on adaptive importancesampling for MDPs and Markov Reward Processes (i.e., an MDP with a fixed policy).",6. Related Work,[0],[0]
"Existing work on adaptive IS in RL has considered changing the transition probabilities to lower the variance of policy evaluation (Desai & Glynn, 2001; Frank et al., 2008) or lower the variance of policy gradient estimates (Ciosek & Whiteson, 2017).",6. Related Work,[0],[0]
"Since the transition probabilities are typically unknown in RL, adapting the behavior policy is a more general approach to adaptive IS.",6. Related Work,[0],[0]
"Ciosek and Whiteson also adapt the distribution of trajectories with gradient descent on the variance (Ciosek & Whiteson, 2017) with respect to parameters of the transition probabilities.",6. Related Work,[0],[0]
"The main focus of this work is increasing
the probability of simulated rare events so that policy improvement can learn an appropriate response.",6. Related Work,[0],[0]
"In contrast, we address the problem of policy evaluation and differentiate with respect to the (known) policy parameters.
",6. Related Work,[0],[0]
The cross-entropy method (CEM) is a general method for adaptive importance-sampling.,6. Related Work,[0],[0]
CEM attempts to minimize the Kullback-Leibler divergence between the current sampling distribution and the optimal sampling distribution.,6. Related Work,[0],[0]
"As discussed in Section 3.1, this optimal behavior policy only exists under a set of restrictive conditions.",6. Related Work,[0],[0]
"In contrast we adapt the behavior policy by minimizing variance.
",6. Related Work,[0],[0]
Other methods exist for lowering the variance of on-policy estimates.,6. Related Work,[0],[0]
"In addition to the control variate technique used by the Advantage Sum estimator (Zinkevich et al., 2006; White & Bowling, 2009), Veness et al. consider using common random numbers and antithetic variates to reduce the variance of roll-outs in Monte Carlo Tree Search (MCTS) (2011).",6. Related Work,[0],[0]
These techniques require a model of the environment (as is typical for MCTS) and do not appear to be applicable to the general RL policy evaluation problem.,6. Related Work,[0],[0]
"BPG could potentially be applied to find a lower variance rollout policy for MCTS.
",6. Related Work,[0],[0]
In this work we have focused on unbiased policy evaluation.,6. Related Work,[0],[0]
"When the goal is to minimize MSE it is often permissible to use biased methods such as temporal difference learning (van Seijen & Sutton, 2014), model-based policy evaluation (Kearns & Singh, 2002; Strehl et al., 2009), or variants of weighted importance sampling (Precup et al., 2000).",6. Related Work,[0],[0]
It may be possible to use similar ideas to BPG to reduce bias and variance although this appears to be difficult since the bias contribution to the mean squared error is squared and thus any gradient involving bias requires knowledge of the estimator’s bias.,6. Related Work,[0],[0]
We leave behavior policy search with biased off-policy methods to future work.,6. Related Work,[0],[0]
Our experiments demonstrate that behavior policy search with BPG can lower the variance of policy evaluation.,7. Discussion and Future Work,[0],[0]
One open question is characterizing the settings where adapting the behavior policy substantially improves over on-policy estimates.,7. Discussion and Future Work,[0],[0]
"Towards answering this question, our Gridworld experiment showed that when πe has little variance, BPG can only offer marginal improvement.",7. Discussion and Future Work,[0],[0]
BPG increases the probability of observing rare events with a high magnitude.,7. Discussion and Future Work,[0],[0]
If the evaluation policy never sees such events then there is little benefit to using BPG.,7. Discussion and Future Work,[0],[0]
"However, in expectation and with an appropriately selected step-size, BPG will never lower the data-efficiency of policy evaluation.
",7. Discussion and Future Work,[0],[0]
It is also necessary that the evaluation policy contributes to the variance of the returns.,7. Discussion and Future Work,[0],[0]
"If all variance is due to the environment then it seems unlikely that BPG will offer much
improvement.",7. Discussion and Future Work,[0],[0]
"For example, Ciosek and Whiteson (2017) consider a variant of the Mountain Car task where the dynamics can trigger a rare event — independent of the action — in which rewards are multiplied by 1000.",7. Discussion and Future Work,[0],[0]
"No behavior policy adaptation can lower the variance due to this event.
",7. Discussion and Future Work,[0],[0]
One limitation of gradient-based BPS methods is the necessity of good step-size selection.,7. Discussion and Future Work,[0],[0]
"In theory, BPG can never lead to worse policy evaluation compared to on-policy estimates.",7. Discussion and Future Work,[0],[0]
"In practice, a poorly selected step-size may cause a step to a worse behavior policy at step iwhich may increase the variance of the gradient estimate at step i + 1.",7. Discussion and Future Work,[0],[0]
"Future work could consider methods for adaptive step-sizes, second order methods, or natural behavior policy gradients.
",7. Discussion and Future Work,[0],[0]
One interesting direction for future work is incorporating behavior policy search into policy improvement.,7. Discussion and Future Work,[0],[0]
A similar idea was explored by Ciosek and Whiteson who explored off-environment learning to improve the performance of policy gradient methods (2017).,7. Discussion and Future Work,[0],[0]
The method presented in that work is limited to simulated environments with differential dynamics.,7. Discussion and Future Work,[0],[0]
Adapting the behavior policy is a potentially much more general approach.,7. Discussion and Future Work,[0],[0]
We have introduced the behavior policy search problem in order to improve estimation of ρ(πe) for an evaluation policy πe.,8. Conclusion,[0],[0]
We present a solution — Behavior Policy Gradient — for this problem which adapts the behavior policy with stochastic gradient descent on the variance of the importance-sampling estimator.,8. Conclusion,[0],[0]
Experiments demonstrate BPG lowers the mean squared error of estimates of ρ(πe) compared to on-policy estimates.,8. Conclusion,[0],[0]
We also demonstrate BPG can further decrease the MSE of estimates in conjunction with a model-based control variate method.,8. Conclusion,[0],[0]
We thank Daniel Brown and the anonymous reviewers for useful comments on the work and its presentation.,9. Acknowledgements,[0],[0]
"This work has taken place in the Personal Autonomous Robotics Lab (PeARL) and Learning Agents Research Group (LARG) at the Artificial Intelligence Laboratory, The University of Texas at Austin.",9. Acknowledgements,[0],[0]
"PeARL research is supported in part by NSF (IIS-1638107, IIS-1617639).",9. Acknowledgements,[0],[0]
"LARG research is supported in part by NSF (CNS-1330072, CNS-1305287, IIS-1637736, IIS-1651089), ONR (21C184-01), AFOSR (FA9550-14-1-0087), Raytheon, Toyota, AT&T, and Lockheed Martin.",9. Acknowledgements,[0],[0]
Josiah Hanna is supported by an NSF Graduate Research Fellowship.,9. Acknowledgements,[0],[0]
"Peter Stone serves on the Board of Directors of Cogitai, Inc.",9. Acknowledgements,[0],[0]
The terms of this arrangement have been reviewed and approved by the University of Texas at Austin in accordance with its policy on objectivity in research.,9. Acknowledgements,[0],[0]
We consider the task of evaluating a policy for a Markov decision process (MDP).,abstractText,[0],[0]
The standard unbiased technique for evaluating a policy is to deploy the policy and observe its performance.,abstractText,[0],[0]
"We show that the data collected from deploying a different policy, commonly called the behavior policy, can be used to produce unbiased estimates with lower mean squared error than this standard technique.",abstractText,[0],[0]
We derive an analytic expression for the optimal behavior policy—the behavior policy that minimizes the mean squared error of the resulting estimates.,abstractText,[0],[0]
"Because this expression depends on terms that are unknown in practice, we propose a novel policy evaluation sub-problem, behavior policy search: searching for a behavior policy that reduces mean squared error.",abstractText,[0],[0]
We present a behavior policy search algorithm and empirically demonstrate its effectiveness in lowering the mean squared error of policy performance estimates.,abstractText,[0],[0]
Data-Efficient Policy Evaluation Through Behavior Policy Search,title,[0],[0]
Convolutional Neural Network (CNN) has become one of the most successful computational models in machine learning and artificial intelligence.,1. Introduction,[0],[0]
"Remarkable progress has been achieved in the design of successful CNN network structures, such as the VGG-Net (Simonyan & Zisserman, 2014), ResNet (He et al., 2016), and DenseNet (Huang et al., 2016).",1. Introduction,[0],[0]
Less attention has been paid to the design of filter structures in CNNs.,1. Introduction,[0],[0]
"Filters, namely the weights in the convolutional layers, are one of the most important ingredients of a CNN model, as filters contain the actual model parameters learned from enormous amounts of data.",1. Introduction,[0],[0]
"Filters in CNNs are typically randomly initialized, and then updated using variants and extensions of gradient descent (“back-propagation”).
",1. Introduction,[0],[0]
"1Duke University, Durham, North Carolina, USA.",1. Introduction,[0],[0]
"Work partially supported by NSF, DoD, NIH and AFOSR.",1. Introduction,[0],[0]
"Correspondence to: Xiuyuan Cheng <xiuyuan.cheng@duke.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"As a result, trained CNN filters have no specific structures, which often leads to significant redundancy in the learned model (Denton et al., 2014; Han et al., 2015; Iandola et al., 2016).",1. Introduction,[0],[0]
"Filters with improved properties will have a direct impact on the accuracy and efficiency of CNN, and the theoretical analysis of filters is also of central importance to the mathematical understanding of deep networks.
",1. Introduction,[0],[0]
"This paper suggests to decompose convolutional filters in CNN into a truncated expansion with pre-fixed bases in the spatial domain, namely the Decomposed Convolutional Filters network (DCFNet), where the expansion coefficients remain learned from data.",1. Introduction,[0],[0]
"By representing the filters in terms of functional bases, which can come from prior data or task knowledge, rather than as pixel values, the number of trainable parameters is reduced to the expansion coefficients; and furthermore, regularity conditions can be imposed on the filters via the truncated expansion.",1. Introduction,[0],[0]
"For image classification tasks, we empirically observe that DCFNet is able to maintain the accuracy with a significant reduction in the number of parameters.",1. Introduction,[0],[0]
"Such observation holds even when random bases are used.
",1. Introduction,[0],[0]
"In particular, we adopt in DCFNet the leading FourierBessel (FB) bases (Abramowitz & Stegun, 1964), which correspond to the low-frequency components in the input.",1. Introduction,[0],[0]
We experimentally observe the superior performance of DCFNet with FB bases (DCF-FB) in both image classification and denoising tasks.,1. Introduction,[0],[0]
"DCF-FB network reduces the response to the high-frequency components in the input, which are least stable under image variations such as deformation and often do not affect recognition after being
suppressed.",1. Introduction,[0],[0]
"Such an intuition is further supported by a mathematical analysis of the CNN representation, where we firstly develop a general result for the CNN representation stability when the input image undergoes a deformation, under proper boundedness conditions of the convolutional filters (Propositions 3.1, 3.3, 3.4).",1. Introduction,[0],[0]
"After imposing the DCF structure, we show that as long as the trainable expansion coefficients at each layer of a DCF-FB network satisfy a boundedness condition, the L-th-layer output is stable with respect to input deformation and the difference is bounded by the magnitude of the distortion (Theorems 3.7, 3.8).
",1. Introduction,[0],[0]
"Apart from FB bases, the DCFNet structure studied in this paper is compatible with general choices of bases, such as standard Fourier bases, wavelet bases, random bases and PCA bases.",1. Introduction,[0],[0]
We numerically test several options in Section 4.,1. Introduction,[0],[0]
"The stability analysis for DCF-FB networks can be extended to other bases choices as well, based upon the general theory developed for CNN representation and using similar techniques.
",1. Introduction,[0],[0]
"Our work is related to recent results on the topics of the usage of bases in deep networks, the model reduction of CNN, as well as the stability analysis of the deep representation.",1. Introduction,[0],[0]
We review these connections in Section 1.1.,1. Introduction,[0],[0]
"Finally, though the current paper focuses on supervised networks for classification and recognition applications in image data, the introduced DCF layers are a generic concept and can potentially be used in reconstruction and generative models as well.",1. Introduction,[0],[0]
We discuss possible extensions in the last section.,1. Introduction,[0],[0]
Deep network with bases and representation stability.,1.1. Related works,[0],[0]
"The usage of bases in deep networks has been previously studied, including wavelet bases, PCA bases, learned dictionary atoms, etc.",1.1. Related works,[0],[0]
"Wavelets are a powerful tool in signal processing (Mallat, 2008) and have been shown to be the optimal basis for data representation under generic settings (Donoho & Johnstone, 1994).",1.1. Related works,[0],[0]
"As a pioneering mathematical model of CNN, the scattering transform (Mallat, 2012; Bruna & Mallat, 2013; Sifre & Mallat, 2013) used pre-fixed weights in the network which are wavelet filters, and showed that the representation produced by a scattering network is stable with respect to certain variations in the input.",1.1. Related works,[0],[0]
"The extension of the scattering transform has been studied in (Wiatowski & Bölcskei, 2015; 2017) which includes a larger class of bases used in the network.",1.1. Related works,[0],[0]
"Apart from wavelet, deep network with PCA bases has been studied in (Chan et al., 2015).",1.1. Related works,[0],[0]
"Making a connection to dictionary learning (Aharon et al., 2006), (Papyan et al., 2016) studied deep networks in form of a cascade of convolutional sparse coding layers with theoretical analysis.",1.1. Related works,[0],[0]
"Deep networks with random weights have been studied in (Giryes et al., 2016), with proved representation stability.",1.1. Related works,[0],[0]
"The DCFNet studied in this
paper incorporates structured pre-fixed bases combined by adapted expansion coefficients learned from data in a supervised way, and demonstrates comparable and even improved classification accuracy on image datasets.",1.1. Related works,[0],[0]
"While the combination of fixed bases and learned coefficients has been studied in classical signal processing (Freeman et al., 1991; Mahalanobis et al., 1987), dictionary learning (Rubinstein et al., 2010) and computer vision (Henriques et al., 2013; Bertinetto et al., 2016), they were not designed with deep architectures in mind.",1.1. Related works,[0],[0]
"Meanwhile, the representation stability of DCFNet is inherited thanks to the filter regularity imposed by the truncated bases decomposition.
",1.1. Related works,[0],[0]
Network redundancy.,1.1. Related works,[0],[0]
"Various approaches have been studied to suppress redundancy in the weights of trained CNNs, including model compression and sparse connections.",1.1. Related works,[0],[0]
"In model compression, network pruning has been studied in (Han et al., 2015) and combined with quantization and Huffman encoding in (Han et al., 2016).",1.1. Related works,[0],[0]
"(Chen et al., 2015) used hash functions to reduce model size without sacrificing generalization performance.",1.1. Related works,[0],[0]
"Low-rank compression of filters in CNN has been studied in (Denton et al., 2014; Ioannou et al., 2015).",1.1. Related works,[0],[0]
"(Iandola et al., 2016; Lin et al., 2014) explored model compression with specific CNN architectures, e.g., replacing regular filters with 1× 1 filters.",1.1. Related works,[0],[0]
"Sparse connections in CNNs have been recently studied in (Ioannou et al., 2016; Anwar et al., 2017; Changpinyo et al., 2017).",1.1. Related works,[0],[0]
"On the theoretical side, (Bölcskei et al., 2017) showed that a sparsely-connected network can achieve certain asymptotic statistical optimality.",1.1. Related works,[0],[0]
The proposed DCFNet relates model redundancy compression to the regularity conditions imposed on the filters.,1.1. Related works,[0],[0]
"In DCF-FB network, redundancy reduction is achieved by suppressing network response to the high-frequency components in the inputs.",1.1. Related works,[0],[0]
"The output at the l-th layer of a convolutional neural network (CNN) can be written as {x(l)(u, λ)}u∈R2,λ∈[Ml], where Ml is the number of channels in that layer and",2.1. Notations of CNN,[0],[0]
"[M ] = {1, · · · ,M} for any integer M .",2.1. Notations of CNN,[0],[0]
"A CNN with L layers can be written as a mapping from {x(0)(u, λ)}u∈R2,λ∈[M0] to {x(L)(u, λ)}u∈R2,λ∈[ML], recursively defined via x(l)(u, λ) = σ(x(l)1
2 (u, λ) + b(l)(λ)), σ being the nonlinear mapping, e.g., ReLU, and
x (l) 1 2 (u, λ) = Ml−1∑ λ′=1 ∫ W (l) λ′,λ(v",2.1. Notations of CNN,[0],[0]
"′)x(l−1)(u+ v′, λ′)dv′. (1)
",2.1. Notations of CNN,[0],[0]
"The filtersW (l)λ′,λ(u) and the biases b (l) are the parameters of the CNN.",2.1. Notations of CNN,[0],[0]
"In practice, both x(l)(u, λ) and W (l)λ′,λ(u) are discretized on a Cartesian grid, and the continuous convolution
in (1) is approximated by its discrete analogue.",2.1. Notations of CNN,[0],[0]
Throughout the paper we use the continuous spatial variable u for simplicity.,2.1. Notations of CNN,[0],[0]
"Very importantly, the filters W (l)λ′,λ(u) are locally supported, e.g., on 3× 3 or 5× 5 image patches.",2.1. Notations of CNN,[0],[0]
"CNNs typically represent and store filters as vectors of the size of the local patches, which is equivalent to expanding the filters under the delta bases.",2.2. Decomposition of convolutional filters,[0],[0]
Delta bases are not optimal for representing smooth functions.,2.2. Decomposition of convolutional filters,[0],[0]
"For example, regular functions have fast decaying coefficients under Fourier bases, and natural images have sparse representation under wavelet bases.",2.2. Decomposition of convolutional filters,[0],[0]
"DCF layers represent the convolutional filters as a truncated expansion under basis functions which are non-adapted through the training process, while adaption comes via the combination of such bases.",2.2. Decomposition of convolutional filters,[0],[0]
"Specifically, suppose that the convolutional filters Wλ′,λ(u) at certain layer, after a proper rescaling of the spatial variable (detailed in Section 3), are supported on the unit disk D in R2.",2.2. Decomposition of convolutional filters,[0],[0]
"Given a bases {ψk}k of the space L2(D), the filters can be represented as
Wλ′,λ(u) = K∑ k=1 (aλ′,λ)kψk(u), (2)
where K is the truncation.",2.2. Decomposition of convolutional filters,[0],[0]
"The decomposition (2) is illustrated in Figure 1, and conceptually, it can be viewed as a two-step scheme of a convolutional layer:
1.",2.2. Decomposition of convolutional filters,[0],[0]
"(Ψ-step) the input is convolved with each of the basis ψk, k = 1, · · · ,K, which are pre-fixed.",2.2. Decomposition of convolutional filters,[0],[0]
"The convolution for each input channel is independent from other channels, adding computational efficiency.
",2.2. Decomposition of convolutional filters,[0],[0]
2.,2.2. Decomposition of convolutional filters,[0],[0]
"(a-step) the intermediate output is linearly transformed by an effectively fully-connected weight matrix (aλ′,λ)k mapping from index (λ′, k) to λ, which is adapted to data.
",2.2. Decomposition of convolutional filters,[0],[0]
"In (2), ψk can be any bases, and we numerically test on different choices in Section 4, including data-adapted bases and random bases.",2.2. Decomposition of convolutional filters,[0],[0]
"All experiments consistently show that the convolutional layers can be drastically decomposed and compressed with almost no reduction on the classification accuracy, and sometimes even using random bases gives strong performance.",2.2. Decomposition of convolutional filters,[0],[0]
"In particular, motivated by classical results of harmonic analysis, we use FB bases in DCFNet, with which the regularity of the filters Wλ′,λ can be imposed though constraining the magnitude the coefficients {(aλ′,λ)k}k (Proposition 3.6).",2.2. Decomposition of convolutional filters,[0],[0]
"As an example, Gabor filters approximated using the leading FB bases are plotted in the right of Figure 2.",2.2. Decomposition of convolutional filters,[0],[0]
"In experiments, DCFNet with FB bases shows superior performance in image classification and denoising tasks compared to original CNN and other bases being tested (Section 4).",2.2. Decomposition of convolutional filters,[0],[0]
"Theoretically, Section 3 analyzes the representation stability of DCFNet with respect to input variations, which provides a theoretical explanation of the advantage of FB bases.",2.2. Decomposition of convolutional filters,[0],[0]
Suppose that the original convolutional layer is of size L× L×M ′,2.3. Parameter and computation reduction,[0],[0]
"×M , as shown in Figure 1, where typically L = 3, 5 and usually less than 11, M ′ and M grow from 3 (number of input channels) to a few hundreds in the deep layers in CNN.",2.3. Parameter and computation reduction,[0],[0]
"After switching to the DCFNet as in (2), there are M ′×M×K tunable parameters (aλ′,λ)k.",2.3. Parameter and computation reduction,[0],[0]
"Thus the number of parameters in that layer is a factor KL2 smaller, which can be significant if K is allowed to be small, particularly when M ′",2.3. Parameter and computation reduction,[0],[0]
"and M are large.
",2.3. Parameter and computation reduction,[0],[0]
The theoretical computational complexity can be calculated directly.,2.3. Parameter and computation reduction,[0],[0]
"Suppose that the input and output activation is W × W in spatial size, the original convolutional layer needs M ′W 2 ·M(1 + 2L2) flops (the number of convolution operations is M ′M , each take 2L2W 2 flops, and the summation over channels take an extra W 2M ′M ).",2.3. Parameter and computation reduction,[0],[0]
"In contract, a DCF layer takesM ′W 2 ·2K(L2 +M) flops, (M ′K
many convolutions in the Ψ step, and 2KM ′MW 2 flops in the a step).",2.3. Parameter and computation reduction,[0],[0]
"Thus when M L2, the leading computation cost is KL2 of that of a regular CNN layer.
",2.3. Parameter and computation reduction,[0],[0]
"The reduction rate of KL2 in both model complexity and theoretical computational flops is confirmed on actual networks used in experiments, c.f. Table 3.",2.3. Parameter and computation reduction,[0],[0]
The analysis in this section is firstly done for regular CNN and then the conditions on filters are reduced to generic conditions on learnt coefficients in a DCF Net.,3. Analysis of Representation Stability,[0],[0]
"In the latter, the proof is for the Fourier-Bessel (FB) bases, and can be extended to other bases using similar techniques.",3. Analysis of Representation Stability,[0],[0]
"We consider the spatial deformation operator denoted by Dτ , where τ : R2 → R2 and is C2, ρ(u) = u− τ(u), and
Dτx(u, λ) = x(ρ(u), λ), ∀u, λ.
",3.1. Stable representation by CNN,[0],[0]
"We assume that the distortion is controlled, and specifically,
(A0) |∇τ |∞ = supu ‖∇τ(u)‖ < 15 , ‖ · ‖ being the operator norm.
",3.1. Stable representation by CNN,[0],[0]
The choice of the constant 15 is purely technical.,3.1. Stable representation by CNN,[0],[0]
"Thus ρ−1 exists, at least locally.",3.1. Stable representation by CNN,[0],[0]
"Our goal is to control ‖x(L)[Dτx(0)]− x(L)[x(0)]‖, namely when the input undergoes a deformation the output at L-the layer is not severely changed.",3.1. Stable representation by CNN,[0],[0]
We achieve this in two steps: (1) We show that ‖Dτx(L)[x(0)],3.1. Stable representation by CNN,[0],[0]
"− x(L)[Dτx(0)]‖ is bounded by the magnitude of deformation up to a constant proportional to the norm of the signal, c.f. Proposition 3.3.",3.1. Stable representation by CNN,[0],[0]
"(2) We show that x(L) is stable under Dτ when L is large,",3.1. Stable representation by CNN,[0],[0]
c.f. Proposition 3.4.,3.1. Stable representation by CNN,[0],[0]
"To proceed, define the L2 norm of x(u, λ) to be
‖x‖2 = 1 M ∑ λ∈[M ] 1 |Ω| ∫ R2 |x(u, λ)|2du, (3)
where |Ω|2 = (2 · 2J)2 is the area of the image-support domain,",3.1. Stable representation by CNN,[0],[0]
c.f. Figure 2.,3.1. Stable representation by CNN,[0],[0]
"We assume that
(A1) σ",3.1. Stable representation by CNN,[0],[0]
": R→ R is non-expansive,
which holds for ReLU.",3.1. Stable representation by CNN,[0],[0]
"We also define the constants
Bl := max{sup λ Ml−1∑ λ′=1 ‖W (l)λ′,λ‖1, sup λ′",3.1. Stable representation by CNN,[0],[0]
"Ml−1 Ml Ml∑ λ=1 ‖W (l)λ′,λ‖1},
Cl := max{sup λ Ml−1∑ λ′=1 ‖|v||∇W",3.1. Stable representation by CNN,[0],[0]
"(l)λ′,λ(v)|‖1,
sup λ′",3.1. Stable representation by CNN,[0],[0]
"Ml−1 Ml Ml∑ λ=1 ‖|v||∇W (l)λ′,λ(v)|‖1}, (4)
where ‖|v||∇W",3.1. Stable representation by CNN,[0],[0]
(v)|‖1 denotes ∫ R2 |v||∇W,3.1. Stable representation by CNN,[0],[0]
"(v)|dv.
Firstly, the following proposition shows that the layer-wise mapping is non-expansive whenever Bl ≤ 1, the proof of which is left to Supplementary Material (S.M.).
",3.1. Stable representation by CNN,[0],[0]
Proposition 3.1.,3.1. Stable representation by CNN,[0],[0]
"In a CNN, under (A1), if Bl ≤ 1 for all l,
(a) The mapping of the l-th convolutional layer (including σ), denoted as x(l)[x(l−1)], is non-expansive, i.e., ‖x(l)[x1]",3.1. Stable representation by CNN,[0],[0]
− x(l)[x2]‖ ≤ ‖x1,3.1. Stable representation by CNN,[0],[0]
"− x2‖ for arbitrary x1 and x2.
(b) ‖x(l)c ‖ ≤ ‖x(l−1)c ‖ for all l, where x(l)c (u, λ) = x(l)(u, λ)−x(l)0 (λ) is the centered version of x(l), x (l) 0 being the output at the l-th layer from a zero input at the bottom layer.",3.1. Stable representation by CNN,[0],[0]
"As a result, ‖x(l)c ‖ ≤ ‖x(0)c ‖ = ‖x(0)‖.
To switch the operator Dτ with the L-layer mapping x(L)[x(0)], the idea is to control the residual of the switching at each layer, which is the following lemma proved in S.M..
Lemma 3.2.",3.1. Stable representation by CNN,[0],[0]
"In a CNN, under (A0) (A1), Bl, Cl as in (4),
‖Dτx(l)[x(l−1)]− x(l)[Dτx(l−1)]‖ ≤ 4(Bl + Cl) · |∇τ |∞‖x(l−1)c ‖,
where x(l)c is as in Proposition 3.1.
",3.1. Stable representation by CNN,[0],[0]
"We thus impose the assumption on the filters to be
(A2) For all l, Bl and Cl as in (4) are less than 1.
",3.1. Stable representation by CNN,[0],[0]
"The assumption (A2) corresponds to a proper scaling of the convolutional filters so that the mapping in each convolutional layer is non-expansive (Proposition 3.1), and in practice, this can be qualitatively maintained by the standard normalization layers in CNN.
",3.1. Stable representation by CNN,[0],[0]
"Now we can bound the residual of a L-layer switching to be additive as L increases:
Proposition 3.3.",3.1. Stable representation by CNN,[0],[0]
"In a CNN, under (A0), (A1), (A2),
‖Dτx(L)[x(0)]− x(L)[Dτx(0)]‖ ≤",3.1. Stable representation by CNN,[0],[0]
8L|∇τ,3.1. Stable representation by CNN,[0],[0]
"|∞‖x(0)‖. (5)
",3.1. Stable representation by CNN,[0],[0]
"Proof is left to S.M. We remark that it is possible to derive a more technical bound in terms of the constants Bl, Cl without assuming (A2), using the same technique.",3.1. Stable representation by CNN,[0],[0]
"We present the simplified result here.
",3.1. Stable representation by CNN,[0],[0]
"In the later analysis of DCF Net, (A2) will be implied by a single condition on the bases expansion coefficients, c.f. (A2’).
",3.1. Stable representation by CNN,[0],[0]
To be able to control ‖Dτx(L),3.1. Stable representation by CNN,[0],[0]
"− x(L)‖, we have the following proposition, proved in S.M.
Proposition 3.4.",3.1. Stable representation by CNN,[0],[0]
"In a CNN, under (A1),
‖Dτx(l) − x(l)‖ ≤ 2|τ |∞Dl‖x(l−1)c",3.1. Stable representation by CNN,[0],[0]
"‖,
where x(l)c is as in Proposition 3.1, and Dl := max{supλ ∑Ml−1 λ′=1 ‖∇W (l) λ′,λ‖1, supλ′ Ml−1",3.1. Stable representation by CNN,[0],[0]
"Ml ∑Ml λ=1 ‖∇W (l) λ′,λ‖1}.
",3.1. Stable representation by CNN,[0],[0]
"One may notice that |τ |∞ is not proportional to |∇τ |∞ when the deformation happens on a large domain, e.g., a rotation.",3.1. Stable representation by CNN,[0],[0]
"It turns out that the multi-scale architecture of CNN induces a decrease of the quantity Dl proportional to the inverse of the domain diameter, which compensate the increase of |τ |∞ as scale grows, as long as the rescaled filters are properly bounded in integral.",3.1. Stable representation by CNN,[0],[0]
"Thus a unified deformation theory can be derived for DCFNets, see next section.",3.1. Stable representation by CNN,[0],[0]
"Due to the downsampling (“pooling”) in CNN, the support of the l-th layer filters W (l)λ′,λ enlarges as l increases.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
Suppose that the input is supported on Ω which is a (2 · 2J),3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"× (2 · 2J) domain, and the CNN has L layers.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"In accordance with the 2× 2 pooling, we assume that W (l)λ′,λ is supported on D(jl), vanishing on the boundary, where D(j) is a disk of radius 2j , j0 ≤ · · · ≤ jL ≤",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"J , and D(j0) is of size of patches at the smallest scale.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"Let {ψk}k be a set of bases supported on the unit disk D(0), and we introduce the rescaled bases
ψj,k(u)",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
= 2,3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"−2jψk(2 −ju), u ∈ D(j),
where the normalization 2−2j is introduced so that ‖ψj,k‖1 = ‖ψk‖1, where ‖f‖1 := ∫ R2 |f(u)|du.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
The multiscale filters and bases are illustrated in the left of Figure 2.,3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"By (2), we have that
W (l) λ′,λ(u)",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"= ∑ k (a (l) λ′,λ)kψjl,k(u), u ∈ D(jl).",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"(6)
While DCFNet is compatible with general choices of bases, we focus on the FB bases in this section as an example.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"FB bases ψk are indexed by k = (m, q) where m and q are the angular and radial frequencies respectively.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"They are supported on the unit disk D = D(0), and in polar coordinates,
ψm,q(r, θ) = cm,qJm(Rm,qr)e imθ, r ∈",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"[0, 1], θ ∈",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"[0, 2π],
where Jm is the Bessel function of the first kind, m are integers, q = 1, 2, · · · , Rm,q is the q-th root of Jm, and cm,q is the normalizing constant s.t. 〈ψm,q, ψm′,q′〉 =∫ D ψm,q(u)ψ ∗",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"m′,q′(u)du = πδm,m′δq,q′ .",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"Furthermore, FB bases are eigenfunctions of the Dirichlet Laplacian on D, i.e., −4ψk = µkψk, where µm,q = R2m,q.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
The eigenvalue µk grows as k increases (Weyl’s law).,3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"Thus FB bases can be ordered by k so that µk increases, of which the leading few are shown in Table 1 and illustrated in Fig. 2.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"In principle, the frequency q and m should be truncated according to the Nyquist sampling rate.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"This truncation turned out to be
k 1 2,3 4,5 6 7,8 9,10 11,12 13,14 m 0 1 2 0 3 1 4 2 q 1 1 1 2 1 2 1 2 µk 5.78 14.68 26.37 30.47 40.71 49.22 57.58 70.85
Table 1.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"The angular frequency m, radial frequency q and Dirichlet eigenvalue µk of the first 14 Fourier-Bessel bases.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"Two k corresponds to one pair of (m, q) when m 6= 0",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"due to that both real and complex parts of the bases are used as real-valued bases.
",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"not often used in our setting, due to the significant bases truncation in DCFNet.
",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"The key technical quantities in the stability analysis of CNN are ‖W (l)λ′,λ‖1 and ‖|v||∇W",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"(l) λ′,λ(v)|‖1, and with FB bases, these integrals are bounded by a µk-weighted L2-norm of a (l) λ′,λ defined as ‖a‖FB = ( ∑ k µka 2 k)
1/2 for all l. The following lemma and proposition are proved in S.M. Lemma 3.5.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"Suppose that {ψk} are FB bases, the function F (u) = ∑ k akψk(u) is smooth on the unit disk.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"Then 1√ π ‖∇F‖2 = ‖a‖FB , where µk are the eigenvalues of ψk as eigenfunctions of the negative Dirichlet laplacian on the unit disk.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"As a result, ‖∇F‖1 ≤ π‖a‖FB .",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
Proposition 3.6.,3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"Using FB bases, ‖|v||∇W (l)λ′,λ(v)|‖1 and ‖W (l)λ′,λ‖1 are bounded by π‖a (l) λ′,λ‖FB for all λ′, λ and l.
Notice that the boundedness of ‖a‖FB implies a decay of |ak| at least as fast as µ−1/2k .",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"This justifies the truncation of the FB expansion to the leading few bases, which correspond to the low-frequency modes.
",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"Proposition 3.6 implies that Bl and Cl are all bounded by Al defined as
Al := πmax{sup λ Ml−1∑ λ′=1 ‖a(l)λ′,λ‖FB ,
sup λ′",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"Ml−1 Ml Ml∑ λ=1 ‖a(l)λ′,λ‖FB}.
",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"Then we introduce
(A2’)",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"For all l, Al ≤ 1,
and the result of Proposition 3.3 extends to DCFNet: Theorem 3.7.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"In a DCFNet with FB bases, under (A0),(A1), (A2’), then
‖Dτx(L)[x(0)]− x(L)[Dτx(0)]‖ ≤ 8L|∇τ |∞‖x(0)‖.
Combined with Proposition 3.4, we have the following deformation stability bound, proved in S.M.: Theorem 3.8.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"In a DCFNet with FB bases, under (A0),(A1), (A2’),
‖x(L)[x(0)]− x(L)[Dτx(0)]‖ ≤ (8L|∇τ |∞ + 2 · 2−jL |τ |∞)‖x(0)‖. (7)",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"In this section, we experimentally demonstrate that convolutional filters in CNN can be decomposed as a truncated expansion with pre-fixed bases, where the expansion coefficients remain learned from data.",4. Experiments,[0],[0]
"Though the number of trainable parameters are significantly reduced, the accuracy in tasks such as image classification and face verification is still maintained.",4. Experiments,[0],[0]
"Such empirical observations hold for data-independent Fourier-Bessel (FB) and random bases, and data-dependent PCA bases.",4. Experiments,[0],[0]
"We perform an experimental evaluation on DCFNets using the following public datasets:
MNIST.",4.1. Datasets,[0],[0]
"28 × 28 grayscale images of digits from 0 to 9, with 60,000 training and 10,000 testing samples.
SVHN.",4.1. Datasets,[0],[0]
"The Street View House Numbers (SVHN) dataset (Netzer et al., 2011) contains 32× 32 colored images of digits 0 to 9, with 73,257 training and 26,032 testing samples.",4.1. Datasets,[0],[0]
"The additional training images were not used.
CIFAR10.",4.1. Datasets,[0],[0]
"The dataset (Krizhevsky, 2009) contains 32×32 colored images from 10 object classes, with 50,000 training
and 10,000 testing samples.
",4.1. Datasets,[0],[0]
VGG-Face.,4.1. Datasets,[0],[0]
"A large-scale face dataset, which contains about 2.6M face images from over 2.6K people (Parkhi et al., 2015).",4.1. Datasets,[0],[0]
1,4.1. Datasets,[0],[0]
"In our object classification experiments, we evaluate the DCFNet with three types of predefined bases: FourierBessel bases (DCF-FB), random bases which are generated by Gaussian vectors (DCF-RB), and PCA bases which are principal components of the convolutional filters in a pre-trained corresponding CNN model (DCF-PCA).
",4.2. Object classification,[0],[0]
"Three CNN network architectures are used for classification, Conv-2 and Conv-3 shown in Table 2, and VGG-16 (Simonyan & Zisserman, 2014).",4.2. Object classification,[0],[0]
"To generate the corresponding DCFNet structure from CNN, each CNN conv layer is expended over a set of pre-defined bases, and the obtained trainable expansion coefficients are implemented as a 1× 1 conv layer.",4.2. Object classification,[0],[0]
"For example, a 5× 5×M ′ ×M conv layer is expended over K 5 × 5 bases for trainable coefficients in a 1 × 1 ×M ′K ×M convolutional layer.",4.2. Object classification,[0],[0]
"K denotes the number of basis used, and we evaluate multiple K for different levels of parameter reduction.",4.2. Object classification,[0],[0]
"In order to be compatible with existing deep learning frameworks, pre-fixed bases are currently implemented as regular convolutional layers with zero learning rate.",4.2. Object classification,[0],[0]
"The additional memory cost incurred in such convenient implementation can be eliminated with a more careful implementation, as bases are pre-fixed and the addition across channels can be computed on the fly.
",4.2. Object classification,[0],[0]
The classification accuracy using DCFNets on various datasets are shown in Table 3.,4.2. Object classification,[0],[0]
"We observe that, by using only 3 Fourier-Bessel (FB) bases, we already obtain comparable accuracy as the original full CNN models on all
1The software is publicly available at https://github.",4.2. Object classification,[0],[0]
"com/xycheng/DCFNet.
",4.2. Object classification,[0],[0]
"datasets, while using 12% parameters for 5×5 filters.",4.2. Object classification,[0],[0]
"When more FB bases are used, DCFNets outperform corresponding CNN models, still with significantly less parameters.",4.2. Object classification,[0],[0]
"As FB bases correspond to the low-frequency components in the inputs, DCF-FB network responds less to the highfrequency nuance details, which are often irrelevant for classification tasks.",4.2. Object classification,[0],[0]
The superiority of DCF-FB network is further shown with less training data.,4.2. Object classification,[0],[0]
"For SVHN with 500 training samples, the testing accuracy (on a 50,000 testing set) of regular CNN and DCF-FB are 63.88% and 66.79% respectively.",4.2. Object classification,[0],[0]
"With 1000 training samples, the test accuracy are 73.53% v.s. 75.45%.",4.2. Object classification,[0],[0]
"Surprisingly, we observe that DCF with random bases also report acceptable performance.
",4.2. Object classification,[0],[0]
Both the FB and random bases are data independent.,4.2. Object classification,[0],[0]
"For comparison purposes, we also evaluate DCFNets with data dependent PCA bases, which are principal components of corresponding convolutional filters in pre-trained CNN models.",4.2. Object classification,[0],[0]
"When the CNN model is pre-trained with all training data, PCA bases (pca-f) shows comparable performance as FB bases.",4.2. Object classification,[0],[0]
"However, the quality of the PCA bases (pca-s) degenerates, when only a randomly selected subset of the training set is used for the pre-training.",4.2. Object classification,[0],[0]
"To gain intuitions behind the superior classification performance of DCFNet, we conduct a set of “toy” image denoising experiments on the SVHN image dataset.",4.3. Image denoising,[0],[0]
"We take the first three 5 × 5 convolution blocks from the Conv-3 CNN network in Table 2, which is used in our SVHN object
classification experiments.",4.3. Image denoising,[0],[0]
"We remove all pooling layers, and append at the end an FC-256 followed with a Euclidean loss layer.",4.3. Image denoising,[0],[0]
"We then decompose each 5 × 5 conv layer in this CNN network over 3 random bases and 3 FB bases respectively, to produce DCF-RB and DCF-FB networks.
",4.3. Image denoising,[0],[0]
We use SVHN training images with their gray-scale version as labels to train all three networks to simply reconstruct an input image (in gray-scale).,4.3. Image denoising,[0],[0]
Figure 4 shows how three trained networks behave while reconstructing examples from the SVHN testing images.,4.3. Image denoising,[0],[0]
"Without noise added to input images, Figure 4a, all three networks report decent reconstruction, while DCF-RB shows inferior to both CNN and DCF-FB.",4.3. Image denoising,[0],[0]
"PSNR values indicate CNN often produces more precise reconstructions; however, those missing high-frequency components in DCF-FB reconstructions are mostly nuance details.",4.3. Image denoising,[0],[0]
"With noise added as in figures 4b and 4c, DCF-FB produces significantly superior reconstruction over both CNN and DCF-RB, with about one tenth of the parameter number of CNN.
",4.3. Image denoising,[0],[0]
"The above empirical observations clearly indicate that Fourier-Bessel bases, which correspond to the lowfrequency components in the inputs, enable DCF to ignore the high-frequency nuance details, which are often less stable under input variations, and mostly irrelevant for tasks such as classification.",4.3. Image denoising,[0],[0]
"Such empirical observation provides good intuitions behind the superior classification performance of DCF, and is also consistent with the theoretical analysis on representation stability in Section 3.",4.3. Image denoising,[0],[0]
"We present a further evaluation of DCFNet on face verification tasks using “very deep” network architectures, which comprise a long sequence of convolutional layers.",4.4. Face verification,[0],[0]
"In order to train such complex networks, we adopt a very large scale VGG-face (Parkhi et al., 2015) dataset, which contains about 2.6M face images from over 2.6K people.
",4.4. Face verification,[0],[0]
"As shown in Table 4, we adopt the VGG-Very-Deep-16 CNN architecture as detailed in (Parkhi et al., 2015) by modifying layer 32 and 35 to change output features from 4,096 dimension to 512.",4.4. Face verification,[0],[0]
"Such CNN network comprises 16 weight layers, and all except the last Fully-Connected (FC) layer utilize 3× 3 or 5× 5 filters.
",4.4. Face verification,[0],[0]
The input to both CNN and DCFNet are face images of size 224 × 224 (with the average face image subtracted).,4.4. Face verification,[0],[0]
"As shown in Table 5, with FB bases, even only using 13 parameters at weight layers (K = 3 for 3 × 3, K = 8 for 5× 5), the DCFNet shows similar verification accuracy as the CNN structure on the challenging LFW benchmark.",4.4. Face verification,[0],[0]
"Note that our CNN model outperforms the VGG-face model in (Parkhi et al., 2015), and such improvement is mostly due to the smaller output dimension we adopted, as both models share similar architecture and are trained on the same face dataset.",4.4. Face verification,[0],[0]
The paper studies CNNs where the convolutional filters are represented as a truncated expansion under pre-fixed bases and the expansion coefficients are learned from labeled data.,5. Conclusion and Discussion,[0],[0]
"Experimentally, we observe that on various object recognition datasets the classification accuracy are maintained with a significant reduction of the number of parameters, and the performance of Fourier-Bessel (FB) bases is constantly superior.",5. Conclusion and Discussion,[0],[0]
The truncated FB expansion in DCFNet can be viewed as a regularization of the filters.,5. Conclusion and Discussion,[0],[0]
"In other words, DCF-FB is less susceptible to the high-frequency components in the input, which are least stable under expected input variations and often do not affect recognition when suppressed.",5. Conclusion and Discussion,[0],[0]
"This interpretation is supported by image denoising experiments, where DCF-FB performs preferably over the original CNN and other basis options on noisy inputs.",5. Conclusion and Discussion,[0],[0]
"The stability of DCFNet representation is also proved theoretically, showing that the perturbation of the deep features with respect to input variations can be bounded under generic conditions on the decomposed filters.
",5. Conclusion and Discussion,[0],[0]
"To extend the work, firstly, DCF layers can be incorporated in networks for unsupervised learning, for which the denoising experiment serves as a first step.",5. Conclusion and Discussion,[0],[0]
The stability analysis can be extended by testing the resilience to adversarial noise.,5. Conclusion and Discussion,[0],[0]
"Finally, more structures may be imposed across the channels, concurrently with the structures of the filters in space.",5. Conclusion and Discussion,[0],[0]
Filters in a Convolutional Neural Network (CNN) contain model parameters learned from enormous amounts of data.,abstractText,[0],[0]
"In this paper, we suggest to decompose convolutional filters in CNN as a truncated expansion with pre-fixed bases, namely the Decomposed Convolutional Filters network (DCFNet), where the expansion coefficients remain learned from data.",abstractText,[0],[0]
"Such a structure not only reduces the number of trainable parameters and computation, but also imposes filter regularity by bases truncation.",abstractText,[0],[0]
"Through extensive experiments, we consistently observe that DCFNet maintains accuracy for image classification tasks with a significant reduction of model parameters, particularly with Fourier-Bessel (FB) bases, and even with random bases.",abstractText,[0],[0]
"Theoretically, we analyze the representation stability of DCFNet with respect to input variations, and prove representation stability under generic assumptions on the expansion coefficients.",abstractText,[0],[0]
The analysis is consistent with the empirical observations.,abstractText,[0],[0]
DCFNet: Deep Neural Network with Decomposed Convolutional Filters,title,[0],[0]
"ar X
iv :1
70 6.
05 12
5v 1
[ cs
.A I]
1 6
Ju n
20 17
cooperative settings, where agents with different goals attempt to agree on common decisions. Negotiations require complex communication and reasoning skills, but success is easy to measure, making this an interesting task for AI. We gather a large dataset of human-human negotiations on a multi-issue bargaining task, where agents who cannot observe each other’s reward functions must reach an agreement (or a deal) via natural language dialogue. For the first time, we show it is possible to train end-to-end models for negotiation, which must learn both linguistic and reasoning skills with no annotated dialogue states. We also introduce dialogue rollouts, in which the model plans ahead by simulating possible complete continuations of the conversation, and find that this technique dramatically improves performance. Our code and dataset are publicly available.1",text,[0],[0]
"Intelligent agents often need to cooperate with others who have different goals, and typically use natural language to agree on decisions.",1 Introduction,[0],[0]
"Negotiation is simultaneously a linguistic and a reasoning problem, in which an intent must be formulated and then verbally realised.",1 Introduction,[0],[0]
"Such dialogues contain both cooperative and adversarial elements, and require agents to understand, plan, and generate utterances to achieve their goals (Traum et al., 2008; Asher et al., 2012).
",1 Introduction,[0],[0]
"We collect the first large dataset of natural language negotiations between two people, and show
1 https://github.com/facebookresearch/end-to-end-negotiator
that end-to-end neural models can be trained to negotiate by maximizing the likelihood of human actions.",1 Introduction,[0],[0]
"This approach is scalable and domainindependent, but does not model the strategic skills required for negotiating well.",1 Introduction,[0],[0]
"We further show that models can be improved by training and decoding to maximize reward instead of likelihood—by training with self-play reinforcement learning, and using rollouts to estimate the expected reward of utterances during decoding.
",1 Introduction,[0],[0]
"To study semi-cooperative dialogue, we gather a dataset of 5808 dialogues between humans on a negotiation task.",1 Introduction,[0],[0]
"Users were shown a set of items with a value for each, and asked to agree how to divide the items with another user who has a different, unseen, value function (Figure 1).
",1 Introduction,[0],[0]
We first train recurrent neural networks to imitate human actions.,1 Introduction,[0],[0]
"We find that models trained to maximise the likelihood of human utterances can generate fluent language, but make comparatively poor negotiators, which are overly willing to compromise.",1 Introduction,[0],[0]
"We therefore explore two methods for improving the model’s strategic reasoning skills— both of which attempt to optimise for the agent’s goals, rather than simply imitating humans:
Firstly, instead of training to optimise likelihood, we show that our agents can be considerably improved using self play, in which pre-trained models practice negotiating with each other in order to optimise performance.",1 Introduction,[0],[0]
"To avoid the models diverging from human language, we interleave reinforcement learning updates with supervised updates.",1 Introduction,[0],[0]
"For the first time, we show that end-toend dialogue agents trained using reinforcement learning outperform their supervised counterparts in negotiations with humans.
",1 Introduction,[0],[0]
"Secondly, we introduce a new form of planning for dialogue called dialogue rollouts, in which an agent simulates complete dialogues during decoding to estimate the eward of utterances.",1 Introduction,[0],[0]
"We show
that decoding to maximise the reward function (rather than likelihood) significantly improves performance against both humans and machines.
",1 Introduction,[0],[0]
"Analysing the performance of our agents, we find evidence of sophisticated negotiation strategies.",1 Introduction,[0],[0]
"For example, we find instances of the model feigning interest in a valueless issue, so that it can later ‘compromise’ by conceding it.",1 Introduction,[0],[0]
"Deceit is a complex skill that requires hypothesising the other agent’s beliefs, and is learnt relatively late in child development (Talwar and Lee, 2002).",1 Introduction,[0],[0]
"Our agents have learnt to deceive without any explicit human design, simply by trying to achieve their goals.
",1 Introduction,[0],[0]
The rest of the paper proceeds as follows: §2 describes the collection of a large dataset of humanhuman negotiation dialogues.,1 Introduction,[0],[0]
"§3 describes a baseline supervised model, which we then show can be improved by goal-based training (§4) and decoding (§5).",1 Introduction,[0],[0]
"§6 measures the performance of our models and humans on this task, and §7 gives a detailed analysis and suggests future directions.",1 Introduction,[0],[0]
"To enable end-to-end training of negotiation agents, we first develop a novel negotiation task and curate a dataset of human-human dialogues for this task.",2.1 Overview,[0],[0]
This task and dataset follow our proposed general framework for studying semicooperative dialogue.,2.1 Overview,[0],[0]
"Initially, each agent is shown an input specifying a space of possible actions and a reward function which will score the outcome of the negotiation.",2.1 Overview,[0],[0]
"Agents then sequentially take turns of either sending natural language messages, or selecting that a final decision has been reached.",2.1 Overview,[0],[0]
"When one agent selects that an
agreement has been made, both agents independently output what they think the agreed decision was.",2.1 Overview,[0],[0]
"If conflicting decisions are made, both agents are given zero reward.",2.1 Overview,[0],[0]
"Our task is an instance of multi issue bargaining (Fershtman, 1990), and is based on DeVault et al. (2015).",2.2 Task,[0],[0]
"Two agents are both shown the same collection of items, and instructed to divide them so that each item assigned to one agent.
",2.2 Task,[0],[0]
"Each agent is given a different randomly generated value function, which gives a non-negative value for each item.",2.2 Task,[0],[0]
The value functions are constrained so that: (1) the total value for a user of all items is 10; (2) each item has non-zero value to at least one user; and (3) some items have nonzero value to both users.,2.2 Task,[0],[0]
"These constraints enforce that it is not possible for both agents to receive a maximum score, and that no item is worthless to both agents, so the negotiation will be competitive.",2.2 Task,[0],[0]
"After 10 turns, we allow agents the option to complete the negotiation with no agreement, which is worth 0 points to both users.",2.2 Task,[0],[0]
"We use 3 item types (books, hats, balls), and between 5 and 7 total items in the pool.",2.2 Task,[0],[0]
Figure 1 shows our interface.,2.2 Task,[0],[0]
We collected a set of human-human dialogues using Amazon Mechanical Turk.,2.3 Data Collection,[0],[0]
"Workers were paid $0.15 per dialogue, with a $0.05 bonus for maximal scores.",2.3 Data Collection,[0],[0]
We only used workers based in the United States with a 95% approval rating and at least 5000 previous HITs.,2.3 Data Collection,[0],[0]
"Our data collection interface was adapted from that of Das et al. (2016).
",2.3 Data Collection,[0],[0]
"We collected a total of 5808 dialogues, based on 2236 unique scenarios (where a scenario is the
available items and values for the two users).",2.3 Data Collection,[0],[0]
We held out a test set of 252 scenarios (526 dialogues).,2.3 Data Collection,[0],[0]
Holding out test scenarios means that models must generalise to new situations.,2.3 Data Collection,[0],[0]
"We propose a simple but effective baseline model for the conversational agent, in which a sequenceto-sequence model is trained to produce the complete dialogue, conditioned on an agent’s input.",3 Likelihood Model,[0],[0]
"Each dialogue is converted into two training examples, showing the complete conversation from the perspective of each agent.",3.1 Data Representation,[0],[0]
"The examples differ on their input goals, output choice, and whether utterances were read or written.
",3.1 Data Representation,[0],[0]
"Training examples contain an input goal g, specifying the available items and their values, a dialogue x, and an output decision o specifying which items each agent will receive.",3.1 Data Representation,[0],[0]
"Specifically, we represent g as a list of six integers corresponding to the count and value of each of the three item types.",3.1 Data Representation,[0],[0]
Dialogue x is a list of tokens x0..,3.1 Data Representation,[0],[0]
"T containing the turns of each agent interleaved with symbols marking whether a turn was written by the agent or their partner, terminating in a special token indicating one agent has marked that an agree-
ment has been made.",3.1 Data Representation,[0],[0]
Output o is six integers describing how many of each of the three item types are assigned to each agent.,3.1 Data Representation,[0],[0]
See Figure 2.,3.1 Data Representation,[0],[0]
"We train a sequence-to-sequence network to generate an agent’s perspective of the dialogue conditioned on the agent’s input goals (Figure 3a).
",3.2 Supervised Learning,[0],[0]
"The model uses 4 recurrent neural networks, implemented as GRUs (Cho et al., 2014): GRUw, GRUg, GRU−→o , and GRU←−o .
",3.2 Supervised Learning,[0],[0]
The agent’s input goals g are encoded using GRUg.,3.2 Supervised Learning,[0],[0]
We refer to the final hidden state as h g .,3.2 Supervised Learning,[0],[0]
"The model then predicts each token xt from left to right, conditioned on the previous tokens and hg .",3.2 Supervised Learning,[0],[0]
"At each time step t, GRUw takes as input the previous hidden state ht−1, previous token xt−1 (embedded with a matrix E), and input encoding hg .",3.2 Supervised Learning,[0],[0]
"Conditioning on the input at each time step helps the model learn dependencies between language and goals.
",3.2 Supervised Learning,[0],[0]
"ht = GRUw(ht−1, [Ext−1, h g]) (1)
The token at each time step is predicted with a softmax, which uses weight tying with the embedding matrix E (Mao et al., 2015):
pθ(xt|x0..t−1, g) ∝ exp(E",3.2 Supervised Learning,[0],[0]
"Tht) (2)
Note that the model predicts both agent’s words, enabling its use as a forward model in Section 5.
",3.2 Supervised Learning,[0],[0]
"At the end of the dialogue, the agent outputs a set of tokens o representing the decision.",3.2 Supervised Learning,[0],[0]
"We generate each output conditionally independently, using a separate classifier for each.",3.2 Supervised Learning,[0],[0]
"The classifiers share bidirectional GRUo and attention mechanism (Bahdanau et al., 2014) over the dialogue, and additionally conditions on the input goals.
",3.2 Supervised Learning,[0],[0]
"h −→o t = GRU−→o (h −→o t−1,",3.2 Supervised Learning,[0],[0]
"[Ext, ht]) (3) h ←−o",3.2 Supervised Learning,[0],[0]
"t = GRU←−o (h ←−o t+1,",3.2 Supervised Learning,[0],[0]
"[Ext, ht]) (4)
hot =",3.2 Supervised Learning,[0],[0]
"[h ←−o t , h −→o t ] (5) hat = W",3.2 Supervised Learning,[0],[0]
[tanh(W ′hot )],3.2 Supervised Learning,[0],[0]
"(6) αt = exp(w · hat ) ∑
t′ exp(w · h a t′)
(7)
hs",3.2 Supervised Learning,[0],[0]
"= tanh(W s[hg, ∑
t
αtht]) (8)
The output tokens are predicted using softmax:
pθ(oi|x0..t, g) ∝",3.2 Supervised Learning,[0],[0]
"exp(W oihs) (9)
",3.2 Supervised Learning,[0],[0]
The model is trained to minimize the negative log likelihood of the token sequence,3.2 Supervised Learning,[0],[0]
"x0..T conditioned on the input goals g, and of the outputs o conditioned on x and g. The two terms are weighted with a hyperparameter α.
L(θ) =− ∑
x,g
∑
t
log pθ(xt|x0..t−1, g)
︸ ︷︷ ︸
Token prediction loss
− α ∑
x,g,o
∑
j
log pθ(oj |x0..T , g)
︸ ︷︷ ︸
Output choice prediction loss
(10)
Unlike the Neural Conversational Model (Vinyals and Le, 2015), our approach shares all parameters for reading and generating tokens.",3.2 Supervised Learning,[0],[0]
"During decoding, the model must generate an output token xt conditioned on dialogue history x0..t−1 and input goals g, by sampling from pθ:
xt ∼ pθ(xt|x0..t−1, g) (11)
",3.3 Decoding,[0],[0]
"If the model generates a special end-of-turn token, it then encodes a series of tokens output by the other agent, until its next turn (Figure 3b).
",3.3 Decoding,[0],[0]
The dialogue ends when either agent outputs a special end-of-dialogue token.,3.3 Decoding,[0],[0]
The model then outputs a set of choices o.,3.3 Decoding,[0],[0]
"We choose each item independently, but enforce consistency by checking the solution is in a feasible set O:
o∗ = argmax o∈O
∏
i
pθ(oi|x0..T , g) (12)
",3.3 Decoding,[0],[0]
"In our task, a solution is feasible if each item is assigned to exactly one agent.",3.3 Decoding,[0],[0]
The space of solutions is small enough to be tractably enumerated.,3.3 Decoding,[0],[0]
"Supervised learning aims to imitate the actions of human users, but does not explicitly attempt to maximise an agent’s goals.",4 Goal-based Training,[0],[0]
"Instead, we explore pre-training with supervised learning, and then fine-tuning against the evaluation metric using reinforcement learning.",4 Goal-based Training,[0],[0]
"Similar two-stage learning strategies have been used previously (e.g. Li et al. (2016); Das et al. (2017)).
",4 Goal-based Training,[0],[0]
"During reinforcement learning, an agent A attempts to improve its parameters from conversations with another agent B. While the other agent B could be a human, in our experiments we used our fixed supervised model that was trained to imitate humans.",4 Goal-based Training,[0],[0]
The second model is fixed as we found that updating the parameters of both agents led to divergence from human language.,4 Goal-based Training,[0],[0]
"In effect,
agent A learns to improve by simulating conversations with the help of a surrogate forward model.
",4 Goal-based Training,[0],[0]
Agent A reads its goals g and then generates tokens x0..n by sampling from pθ.,4 Goal-based Training,[0],[0]
"When x generates an end-of-turn marker, it then reads in tokens xn+1..m generated by agent B. These turns alternate until one agent emits a token ending the dialogue.",4 Goal-based Training,[0],[0]
Both agents then output a decision o and collect a reward from the environment (which will be 0 if they output different decisions).,4 Goal-based Training,[0],[0]
"We denote the subset of tokens generated by A as XA (e.g. tokens with incoming arrows in Figure 3b).
",4 Goal-based Training,[0],[0]
"After a complete dialogue has been generated, we update agent A’s parameters based on the outcome of the negotiation.",4 Goal-based Training,[0],[0]
"Let rA be the score agent A achieved in the completed dialogue, T be the length of the dialogue, γ be a discount factor that rewards actions at the end of the dialogue more strongly, and µ be a running average of completed dialogue rewards so far2.",4 Goal-based Training,[0],[0]
We define the future reward R for an action xt ∈,4 Goal-based Training,[0],[0]
X,4 Goal-based Training,[0],[0]
"A as follows:
R(xt) = ∑
xt∈XA
γT−t(rA(o)− µ) (13)
",4 Goal-based Training,[0],[0]
"We then optimise the expected reward of each
action xt ∈",4 Goal-based Training,[0],[0]
X,4 Goal-based Training,[0],[0]
"A:
LRLθ = Ext∼pθ(xt|x0..t−1,g)[R(xt)]",4 Goal-based Training,[0],[0]
"(14)
The gradient of LRLθ is calculated as in REINFORCE (Williams, 1992):
∇θL RL θ =
∑
xt∈XA
Ext[R(xt)∇θ log(pθ(xt|x0..t−1, g))",4 Goal-based Training,[0],[0]
"]
(15)
2As all rewards are non-negative, we instead re-scale them by subtracting the mean reward found during self play.",4 Goal-based Training,[0],[0]
"Shifting in this way can reduce the variance of our estimator.
",4 Goal-based Training,[0],[0]
"Algorithm 1 Dialogue Rollouts algorithm.
1: procedure ROLLOUT(x0..i, g) 2: u∗ ← ∅ 3: for c ∈ {1..C} do ⊲",4 Goal-based Training,[0],[0]
C candidate moves 4: j ← i 5: do ⊲,4 Goal-based Training,[0],[0]
"Rollout to end of turn 6: j ← j + 1 7: xj ∼ pθ(xj |x0..j−1, g) 8: while xk /∈ {read:, choose:} 9: u← xi+1..xj ⊲",4 Goal-based Training,[0],[0]
u is candidate move 10: for s ∈ {1..S} do ⊲,4 Goal-based Training,[0],[0]
S samples per move 11: k ←,4 Goal-based Training,[0],[0]
j ⊲,4 Goal-based Training,[0],[0]
Start rollout from end of u 12: while xk 6= choose: do ⊲,4 Goal-based Training,[0],[0]
"Rollout to end of dialogue 13: k ← k + 1 14: xk ∼ pθ(xk|x0..k−1, g)
⊲",4 Goal-based Training,[0],[0]
"Calculate rollout output and reward 15: o← argmaxo′∈O p(o
′|x0..k, g) 16: R(u)← R(u)",4 Goal-based Training,[0],[0]
"+ r(o)p(o′|x0..k, g) 17: if R(u) > R(u∗)",4 Goal-based Training,[0],[0]
then 18: u∗ ← u 19: return u∗ ⊲,4 Goal-based Training,[0],[0]
Return best move,4 Goal-based Training,[0],[0]
Likelihood-based decoding (§3.3) may not be optimal.,5 Goal-based Decoding,[0],[0]
"For instance, an agent may be choosing between accepting an offer, or making a counter offer.",5 Goal-based Decoding,[0],[0]
"The former will often have a higher likelihood under our model, as there are fewer ways to agree than to make another offer, but the latter may lead to a better outcome.",5 Goal-based Decoding,[0],[0]
Goal-based decoding also allows more complex dialogue strategies.,5 Goal-based Decoding,[0],[0]
"For example, a deceptive utterance is likely to have a low model score (as users were generally honest in the supervised data), but may achieve high reward.
",5 Goal-based Decoding,[0],[0]
We instead explore decoding by maximising expected reward.,5 Goal-based Decoding,[0],[0]
"We achieve this by using pθ as a
forward model for the complete dialogue, and then deterministically computing the reward.",5 Goal-based Decoding,[0],[0]
"Rewards for an utterance are averaged over samples to calculate expected future reward (Figure 4).
",5 Goal-based Decoding,[0],[0]
"We use a two stage process: First, we generate c candidate utterances U = u0..c, representing possible complete turns that the agent could make, which are generated by sampling from pθ until the end-of-turn token is reached.",5 Goal-based Decoding,[0],[0]
Let x0..n−1 be current dialogue history.,5 Goal-based Decoding,[0],[0]
"We then calculate the expected reward R(u) of candidate utterance u = xn,n+k by repeatedly sampling xn+k+1,T from pθ, then choosing the best output o using Equation 12, and finally deterministically computing the reward r(o).",5 Goal-based Decoding,[0],[0]
"The reward is scaled by the probability of the output given the dialogue, because if the agents select different outputs then they both receive 0 reward.
",5 Goal-based Decoding,[0],[0]
R(xn..n+k),5 Goal-based Decoding,[0],[0]
"= Ex(n+k+1..T ;o)∼pθ [r(o)pθ(o|x0..T )]
(16)
",5 Goal-based Decoding,[0],[0]
"We then return the utterance maximizing R.
u∗ = argmax u∈U R(u) (17)
We use 5 rollouts for each of 10 candidate turns.",5 Goal-based Decoding,[0],[0]
We implement our models using PyTorch.,6.1 Training Details,[0],[0]
All hyper-parameters were chosen on a development dataset.,6.1 Training Details,[0],[0]
"The input tokens are embedded into a 64-dimensional space, while the dialogue tokens are embedded with 256-dimensional embeddings (with no pre-training).",6.1 Training Details,[0],[0]
The input GRUg has a hidden layer of size 64 and the dialogue GRUw is of size 128.,6.1 Training Details,[0],[0]
"The output GRU−→o and GRU←−o both have a hidden state of size 256, the size of hs is 256 as well.",6.1 Training Details,[0],[0]
"During supervised training, we optimise using stochastic gradient descent with a minibatch size of 16, an initial learning rate of 1.0, Nesterov momentum with µ=0.1 (Nesterov, 1983), and clipping gradients whose L2 norm exceeds 0.5.",6.1 Training Details,[0],[0]
We train the model for 30 epochs and pick the snapshot of the model with the best validation perplexity.,6.1 Training Details,[0],[0]
We then annealed the learning rate by a factor of 5 each epoch.,6.1 Training Details,[0],[0]
We weight the terms in the loss function (Equation 10) using α=0.5.,6.1 Training Details,[0],[0]
We do not train against output decisions where humans selected different agreements.,6.1 Training Details,[0],[0]
"Tokens occurring fewer than 20 times are replaced with an ‘unknown’ token.
",6.1 Training Details,[0],[0]
"During reinforcement learning, we use a learning rate of 0.1, clip gradients above 1.0, and use a discount factor of γ=0.95.",6.1 Training Details,[0],[0]
"After every 4 reinforcement learning updates, we make a supervised update with mini-batch size 16 and learning rate 0.5, and we clip gradients at 1.0.",6.1 Training Details,[0],[0]
"We used 4086 simulated conversations.
",6.1 Training Details,[0],[0]
"When sampling words from pθ, we reduce the variance by doubling the values of logits (i.e. using temperature of 0.5).",6.1 Training Details,[0],[0]
"We compare the performance of the following: LIKELIHOOD uses supervised training and decoding (§3), RL is fine-tuned with goal-based selfplay (§4), ROLLOUTS uses supervised training combined with goal-based decoding using rollouts (§5), and RL+ROLLOUTS uses rollouts with a base model trained with reinforcement learning.",6.2 Comparison Systems,[0],[0]
"For development, we use measured the perplexity of user generated utterances, conditioned on the input and previous dialogue.
",6.3 Intrinsic Evaluation,[0],[0]
"Results are shown in Table 3, and show that the simple LIKELIHOOD model produces the most human-like responses, and the alternative training and decoding strategies cause a divergence from human language.",6.3 Intrinsic Evaluation,[0],[0]
"Note however, that this divergence may not necessarily correspond to lower quality language—it may also indicate different strategic decisions about what to say.",6.3 Intrinsic Evaluation,[0],[0]
Results in §6.4 show all models could converse with humans.,6.3 Intrinsic Evaluation,[0],[0]
"We measure end-to-end performance in dialogues both with the likelihood-based agent and with humans on Mechanical Turk, on held out scenarios.
Humans were told that they were interacting with other humans, as they had been during the collection of our dataset (and few appeared to realize they were in conversation with machines).
",6.4 End-to-End Evaluation,[0],[0]
"We measure the following statistics:
",6.4 End-to-End Evaluation,[0],[0]
Score:,6.4 End-to-End Evaluation,[0],[0]
"The average score for each agent (which could be a human or model), out of 10.",6.4 End-to-End Evaluation,[0],[0]
Agreement:,6.4 End-to-End Evaluation,[0],[0]
The percentage of dialogues where both agents agreed on the same decision.,6.4 End-to-End Evaluation,[0],[0]
Pareto Optimality: The percentage of Pareto optimal solutions for agreed deals (a solution is Pareto optimal if neither agent’s score can be improved without lowering the other’s score).,6.4 End-to-End Evaluation,[0],[0]
"Lower scores indicate inefficient negotiations.
",6.4 End-to-End Evaluation,[0],[0]
Results are shown in Table 1.,6.4 End-to-End Evaluation,[0],[0]
"Firstly, we see that the RL and ROLLOUTS models achieve significantly better results when negotiating with the LIKELIHOOD model, particularly the RL+ROLLOUTS model.",6.4 End-to-End Evaluation,[0],[0]
"The percentage of Pareto optimal solutions also increases, showing a better exploration of the solution space.",6.4 End-to-End Evaluation,[0],[0]
"Compared to human-human negotiations (Table 2), the best models achieve a higher agreement rate, better scores, and similar Pareto efficiency.",6.4 End-to-End Evaluation,[0],[0]
"This result confirms that attempting to maximise reward can outperform simply imitating humans.
",6.4 End-to-End Evaluation,[0],[0]
"Similar trends hold in dialogues with humans, with goal-based reasoning outperforming imitation learning.",6.4 End-to-End Evaluation,[0],[0]
"The ROLLOUTS model achieves comparable scores to its human partners, and the RL+ROLLOUTS model actually achieves higher scores.",6.4 End-to-End Evaluation,[0],[0]
"However, we also find significantly more cases of the goal-based models failing to agree a deal with humans—largely a consequence of their more aggressive negotiation tactics (see §7).",6.4 End-to-End Evaluation,[0],[0]
Table 1 shows large gains from goal-based methods.,7 Analysis,[0],[0]
"In this section, we explore the strengths and weaknesses of our models.
",7 Analysis,[0],[0]
Goal-based models negotiate harder.,7 Analysis,[0],[0]
"The RL+ROLLOUTS model has much longer dialogues with humans than LIKELIHOOD (7.2 turns vs. 5.3 on average), indicating that the model is accepting deals less quickly, and negotiating harder.
",7 Analysis,[0],[0]
"A negative consequence of this more aggressive negotiation strategy is that humans were more likely to walk away with no deal, which is reflected in the lower agreement rates.",7 Analysis,[0],[0]
"Even though failing to agree was worth 0 points, people often preferred this course over capitulating to an uncompromising opponent—a factor not well captured by the simulated partner in reinforcement learning training or rollouts (as reflected by the larger gains from goal-based models in dialogues with the LIKELIHOOD model).",7 Analysis,[0],[0]
"In particular, the goal-based models are prone to simply rephrasing the same demand each turn, which is a more effective strategy against the LIKELIHOOD model than humans.",7 Analysis,[0],[0]
"Future work should address this issue.
",7 Analysis,[0],[0]
"Figure 5 shows an example of our goal-based model stubbornly negotiating until it achieves a good outcome.
",7 Analysis,[0],[0]
Models learn to be deceptive.,7 Analysis,[0],[0]
Deception can be an effective negotiation tactic.,7 Analysis,[0],[0]
"We found numerous cases of our models initially feigning interest in a valueless item, only to later ‘compromise’ by conceding it.",7 Analysis,[0],[0]
"Figure 7 shows an example.
",7 Analysis,[0],[0]
Models produce meaningful novel sentences.,7 Analysis,[0],[0]
"One interesting question is whether our models are capable of generating novel sentences in the new circumstances they find themselves in, or if they simply repeat messages from the training data verbatim.",7 Analysis,[0],[0]
We find that 76% of messages produced by the LIKELIHOOD model in self-play were found in the training data.,7 Analysis,[0],[0]
"We manually examined the novel
utterances produced by our model, and found that the overwhelming majority were fluent English sentences in isolation—showing that the model has learnt a good language model for the domain (in addition to results that show it uses language effectively to achieve its goals).",7 Analysis,[0],[0]
"These results suggest that although neural models are prone to the safer option of repeating sentences from training data, they are capable of generalising when necessary.",7 Analysis,[0],[0]
"Future work should choose domains that force a higher degree of diversity in utterances.
",7 Analysis,[0],[0]
Maintaining multi-sentence coherence is challenging.,7 Analysis,[0],[0]
"One common linguistic error we see RL+ROLLOUTS make is to start a message by indicating agreement (e.g. I agree or Deal), but then going on to propose a counter offer—a behaviour that human partners found frustrating.",7 Analysis,[0],[0]
"One explanation is that the model has learnt that in the supervised data, messages beginning with I agree are often at the end of the dialogue, and partners rarely reply with further negotiation—so the models using rollouts and reinforcement learning believe this tactic will help their offer to be accepted.",7 Analysis,[0],[0]
"Most work on goal orientated dialogue systems has assumed that state representations are anno-
tated in the training data (Williams and Young, 2007; Henderson et al., 2014; Wen et al., 2016).",8 Related Work,[0],[0]
"The use of state annotations allows a cleaner separation of the reasoning and natural language aspects of dialogues, but our end-to-end approach makes data collection cheaper and allows tasks where it is unclear how to annotate state.",8 Related Work,[0],[0]
Bordes and Weston (2016) explore end-toend goal orientated dialogue with a supervised model—we show improvements over supervised learning with goal-based training and decoding.,8 Related Work,[0],[0]
"Recently, He et al. (2017) use task-specific rules to combine the task input and dialogue history into a more structured state representation than ours.
",8 Related Work,[0],[0]
Reinforcement learning (RL) has been applied in many dialogue settings.,8 Related Work,[0],[0]
"RL has been widely used to improve dialogue managers, which manage transitions between dialogue states (Singh et al., 2002; Pietquin et al., 2011; Rieser and Lemon, 2011; Gašic et al., 2013; Fatemi et al., 2016).",8 Related Work,[0],[0]
"In contrast, our end-toend approach has no explicit dialogue manager.",8 Related Work,[0],[0]
"Li et al. (2016) improve metrics such as diversity for non-goal-orientated dialogue using RL, which would make an interesting extension to our work.",8 Related Work,[0],[0]
Das et al. (2017) use reinforcement learning to improve cooperative bot-bot dialogues.,8 Related Work,[0],[0]
"RL has also been used to allow agents to invent new languages (Das et al., 2017; Mordatch and Abbeel, 2017).",8 Related Work,[0],[0]
"To our knowledge, our model is the first to use RL to improve the performance of an end-toend goal orientated dialogue system in dialogues with humans.
",8 Related Work,[0],[0]
"Work on learning end-to-end dialogues has concentrated on ‘chat’ settings, without explicit goals (Ritter et al., 2011; Vinyals and Le, 2015; Li et al., 2015).",8 Related Work,[0],[0]
"These dialogues contain a much greater diversity of vocabulary than our domain, but do not
have the challenging adversarial elements.",8 Related Work,[0],[0]
"Such models are notoriously hard to evaluate (Liu et al., 2016), because the huge diversity of reasonable responses, whereas our task has a clear objective.",8 Related Work,[0],[0]
"Our end-to-end approach would also be much more straightforward to integrate into a generalpurpose dialogue agent than one that relied on annotated dialogue states (Dodge et al., 2016).
",8 Related Work,[0],[0]
"There is a substantial literature on multi-agent bargaining in game-theory, e.g. Nash Jr (1950).",8 Related Work,[0],[0]
"There has also been computational work on modelling negotiations (Baarslag et al., 2013)—our work differs in that agents communicate in unrestricted natural language, rather than pre-specified symbolic actions, and our focus on improving performance relative to humans rather than other automated systems.",8 Related Work,[0],[0]
"Our task is based on that of DeVault et al. (2015), who study natural language negotiations for pedagogical purposes—their version includes speech rather than textual dialogue, and embodied agents, which would make interesting extensions to our work.",8 Related Work,[0],[0]
"The only automated natural language negotiations systems we are aware of have first mapped language to domainspecific logical forms, and then focused on choosing the next dialogue act (Rosenfeld et al., 2014; Cuayáhuitl et al., 2015; Keizer et al., 2017).",8 Related Work,[0],[0]
"Our end-to-end approach is the first to to learn comprehension, reasoning and generation skills in a domain-independent data driven way.
",8 Related Work,[0],[0]
"Our use of a combination of supervised and reinforcement learning for training, and stochastic rollouts for decoding, builds on strategies used in game playing agents such as AlphaGo (Silver et al., 2016).",8 Related Work,[0],[0]
Our work is a step towards real-world applications for these techniques.,8 Related Work,[0],[0]
"Our use of rollouts could be extended by choosing the other agent’s responses based on sampling, using Monte Carlo Tree Search (MCTS) (Kocsis and Szepesvári, 2006).",8 Related Work,[0],[0]
"However, our setting has a higher branching factor than in domains where MCTS has been successfully applied, such as Go (Silver et al., 2016)—future work should explore scaling tree search to dialogue modelling.",8 Related Work,[0],[0]
"We have introduced end-to-end learning of natural language negotiations as a task for AI, arguing that it challenges both linguistic and reasoning skills while having robust evaluation metrics.",9 Conclusion,[0],[0]
"We gathered a large dataset of human-human ne-
gotiations, which contain a variety of interesting tactics.",9 Conclusion,[0],[0]
"We have shown that it is possible to train dialogue agents end-to-end, but that their ability can be much improved by training and decoding to maximise their goals, rather than likelihood.",9 Conclusion,[0],[0]
"There remains much potential for future work, particularly in exploring other reasoning strategies, and in improving the diversity of utterances without diverging from human language.",9 Conclusion,[0],[0]
"We will also explore other negotiation tasks, to investigate whether models can learn to share negotiation strategies across domains.",9 Conclusion,[0],[0]
"We would like to thank Luke Zettlemoyer and the anonymous EMNLP reviewers for their insightful comments, and the Mechanical Turk workers who helped us collect data.",Acknowledgments,[0],[0]
"Much of human dialogue occurs in semicooperative settings, where agents with different goals attempt to agree on common decisions.",abstractText,[0],[0]
"Negotiations require complex communication and reasoning skills, but success is easy to measure, making this an interesting task for AI.",abstractText,[0],[0]
"We gather a large dataset of human-human negotiations on a multi-issue bargaining task, where agents who cannot observe each other’s reward functions must reach an agreement (or a deal) via natural language dialogue.",abstractText,[0],[0]
"For the first time, we show it is possible to train end-to-end models for negotiation, which must learn both linguistic and reasoning skills with no annotated dialogue states.",abstractText,[0],[0]
"We also introduce dialogue rollouts, in which the model plans ahead by simulating possible complete continuations of the conversation, and find that this technique dramatically improves performance.",abstractText,[0],[0]
Our code and dataset are publicly available.,abstractText,[0],[0]
"In recent years, we have reached unprecedented data volumes that are high dimensional and sit over (clouds of) networked machines.",1. Introduction,[0],[0]
"As a result, decentralized collection of these data sets along with accompanying distributed op-
1Laboratory for Information and Decision Systems, Massachusetts Institute of Technology 2Department of Electrical and Systems Engineering, University of Pennsylvania 3Department of Electrical Engineering and Computer Science, Yale University.",1. Introduction,[0],[0]
"Correspondence to: Aryan Mokhtari <aryanm@mit.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"timization methods are not only desirable but very often necessary (Boyd et al., 2011).
",1. Introduction,[0],[0]
"The focus of this paper is on decentralized optimization, the goal of which is to maximize/minimize a global objective function –distributed over a network of computing units– through local computation and communications among nodes.",1. Introduction,[0],[0]
"A canonical example in machine learning is fitting models using M-estimators where given a set of data points the parameters of the model are estimated through an empirical risk minimization (Vapnik, 1998).",1. Introduction,[0],[0]
"Here, the global objective function is defined as an average of local loss functions associated with each data point.",1. Introduction,[0],[0]
"Such local loss functions can be convex (e.g., logistic regression, SVM, etc) or non-convex (e.g., non-linear square loss, robust regression, mixture of Gaussians, deep neural nets, etc) (Mei et al., 2016).",1. Introduction,[0],[0]
"Due to the sheer volume of data points, these optimization tasks cannot be fulfilled on a single computing cluster node.",1. Introduction,[0],[0]
"Instead, we need to opt for decentralized solutions that can efficiently exploit dispersed (and often distant) computational resources linked through a tightly connected network.",1. Introduction,[0],[0]
"Furthermore, local computations should be light so that they can be done on single machines.",1. Introduction,[0],[0]
"In particular, when the data is high dimensional, extra care should be given to any optimization procedure that relies on projections over the feasibility domain.
",1. Introduction,[0],[0]
"In addition to large scale machine learning applications, decentralized optimization is a method of choice in many other domains such as Internet of Things (IoT) (Abu-Elkheir et al., 2013), remote sensing (Ma et al., 2015), multi-robot systems (Tanner & Kumar, 2005), and sensor networks (Rabbat & Nowak, 2004).",1. Introduction,[0],[0]
"In such scenarios, individual entities can communicate over a network and interact with the environment by exchanging the data generated through sensing.",1. Introduction,[0],[0]
At the same time they can react to events and trigger actions to control the physical world.,1. Introduction,[0],[0]
"These applications highlight another important aspect of decentralized optimization where private data is collected by different sensing units (Yang et al., 2017).",1. Introduction,[0],[0]
"Here again, we aim to optimize a global objective function while avoiding to share the private data among computing units.",1. Introduction,[0],[0]
"Thus, by design, one cannot solve such private optimization problems in a centralized manner and should rely on decentralized solutions where local private
computation is done where the data is collected.
",1. Introduction,[0],[0]
"Continuous submodular functions, a broad subclass of nonconvex functions with diminishing returns property, have recently received considerable attention (Bach, 2015; Bian et al., 2017).",1. Introduction,[0],[0]
"Due to their interesting structures that allow strong approximation guarantees (Mokhtari et al., 2018a; Bian et al., 2017), they have found various applications, including the design of online experiments (Chen et al., 2018), budget and resource allocations (Eghbali & Fazel, 2016; Staib & Jegelka, 2017), and learning assignments (Golovin et al., 2014).",1. Introduction,[0],[0]
"However, all the existing work suffer from centralized computing.",1. Introduction,[0],[0]
"Given that many information gathering, data summarization, and non-parametric learning problems are inherently related to large-scale submodular maximization, the demand for a fully decentralized solution is immediate.",1. Introduction,[0],[0]
"In this paper, we develop the first decentralized framework for both continuous and discrete submodular functions.",1. Introduction,[0],[0]
"Our contributions are as follows:
• Continuous submodular maximization: For any global objective function that is monotone and continuous DR-submodular and subject to any downclosed and bounded convex body, we develop Decentralized Continuous Greedy, a decentralized and projection-free algorithm that achieves the tight (1 1/e ✏) approximation guarantee in O(1/✏2) rounds of local communication.
",1. Introduction,[0],[0]
"• Discrete submodular maximization: For any global objective function that is monotone and submodular and subject to any matroid constraint, we develop a discrete variant of Decentralized Continuous Greedy that achieves the tight (1 1/e ✏) approximation ratio in O(1/✏3) rounds of communication.
",1. Introduction,[0],[0]
All proofs are provided in the supplementary material.,1. Introduction,[0],[0]
"Decentralized optimization is a challenging problem as nodes only have access to separate components of the global objective function, while they aim to collectively reach the global optimum point.",2. Related Work,[0],[0]
"Indeed, one naive approach to tackle this problem is to broadcast local objective functions to all the nodes in the network and then solve the problem locally.",2. Related Work,[0],[0]
"However, this scheme requires high communication overhead and disregards the privacy associated with the data of each node.",2. Related Work,[0],[0]
"An alternative approach is the master-slave setting (Bekkerman et al., 2011; Shamir et al., 2014; Zhang & Lin, 2015) where at each iteration, nodes use their local data to compute the information needed by the master node.",2. Related Work,[0],[0]
"Once the master node receives all the local information, it updates its decision and broadcasts the decision to all the nodes.",2. Related Work,[0],[0]
"Although this scheme protects the privacy of nodes it is not robust to machine failures and is prone to high overall
communication time.",2. Related Work,[0],[0]
"In decentralized methods, these issues are overcame by removing the master node and considering each node as an independent unit that is allowed to exchange information with its neighbors.
",2. Related Work,[0],[0]
"Convex decentralized consensus optimization is a relatively mature area with a myriad of primal and dual algorithms (Bertsekas & Tsitsiklis, 1989).",2. Related Work,[0],[0]
"Among primal methods, decentralized (sub)gradient descent is perhaps the most well known algorithm which is a mix of local gradient descent and successive averaging (Nedic & Ozdaglar, 2009; Yuan et al., 2016).",2. Related Work,[0],[0]
It also can be interpreted as a penalty method that encourages agreement among neighboring nodes.,2. Related Work,[0],[0]
"This latter interpretation has been exploited to solve the penalized objective function using accelerated gradient descent (Jakovetić et al., 2014; Qu & Li, 2017), Newton’s method (Mokhtari et al., 2017; Bajovic et al., 2017), or quasi-Newton algorithms (Eisen et al., 2017).",2. Related Work,[0],[0]
The methods that operate in the dual domain consider a constraint that enforces equality between nodes’ variables and solve the problem by ascending on the dual function to find optimal Lagrange multipliers.,2. Related Work,[0],[0]
"A short list of dual methods are the alternating directions method of multipliers (ADMM) (Boyd et al., 2011), dual ascent algorithm (Rabbat et al., 2005), and augmented Lagrangian methods (Jakovetic et al., 2015; Chatzipanagiotis & Zavlanos, 2015).",2. Related Work,[0],[0]
"Recently, there have been many attempts to extend the tools in decentralized consensus optimization to the case that the objective function is non-convex (Di Lorenzo & Scutari, 2016; Sun et al., 2016; Hajinezhad et al., 2016; Tatarenko & Touri, 2017).",2. Related Work,[0],[0]
"However, such works are mainly concerned with reaching a stationary point and naturally cannot provide any optimality guarantee.
",2. Related Work,[0],[0]
"In this paper, our focus is to provide the first decentralized algorithms for both discrete and continuous submodular functions.",2. Related Work,[0],[0]
"It is known that the centralized greedy approach of (Nemhauser et al., 1978), and its many variants (Feige et al., 2011; Buchbinder et al., 2015; 2014; Feldman et al., 2017; Mirzasoleiman et al., 2016), reach tight approximation guarantees in various scenarios.",2. Related Work,[0],[0]
"As such methods are sequential in nature, they do not scale to massive datasets.",2. Related Work,[0],[0]
"To partially resolve this issue, MapReduce style methods, with a master-slave architecture, have been proposed (Mirzasoleiman et al., 2013; Kumar et al., 2015; da Ponte Barbosa et al., 2015; Mirrokni & Zadimoghaddam, 2015; Qu et al., 2015).
",2. Related Work,[0],[0]
"One can extend the notion of diminishing returns to continuous domains (Wolsey, 1982; Bach, 2015).",2. Related Work,[0],[0]
"Even though continuous submodular functions are not generally convex (nor concave) Hassani et al. (2017) showed that in the monotone setting and subject to a general bounded convex body constraint, stochastic gradient methods can achieve a 1/2 approximation guarantee.",2. Related Work,[0],[0]
"The approximation guarantee can be tightened to (1 1/e) by using Frank-Wolfe (Bian et al.,
2017) or stochastic Frank-Wolfe (Mokhtari et al., 2018a).",2. Related Work,[0],[0]
"In this section, we review the notation that we use throughout the paper.",3. Notation and Background,[0],[0]
"We then give the precise definition of submodularity in discrete and continuous domains.
Notation.",3. Notation and Background,[0],[0]
Lowercase boldface v denotes a vector and uppercase boldface W a matrix.,3. Notation and Background,[0],[0]
"The i-th element of v is written as vi and the element on the i-th row and j-th column of W is denoted by wi,j .",3. Notation and Background,[0],[0]
We use kvk to denote the Euclidean norm of vector v and kWk to denote the spectral norm of matrix W.,3. Notation and Background,[0],[0]
The null space of matrix W is denoted by null(W).,3. Notation and Background,[0],[0]
"The inner product of vectors x,y is indicated by hx,yi, and the transpose of a vector v or matrix W are denoted by v† and W†, respectively.",3. Notation and Background,[0],[0]
The vector 1n 2,3. Notation and Background,[0],[0]
"Rn is the vector of all ones with n components, and the vector 0p 2",3. Notation and Background,[0],[0]
Rp is the vector of all zeros with p components.,3. Notation and Background,[0],[0]
Submodulary.,3. Notation and Background,[0],[0]
A set function f : 2V !,3. Notation and Background,[0],[0]
"R+, defined on the ground set V , is called submodular if for all A,B ✓ V , we have f(A)+f(B) f(A\B)+f(A[B).",3. Notation and Background,[0],[0]
We often need to maximize submodular functions subject to a down-closed set family I .,3. Notation and Background,[0],[0]
"In particular, we say I ⇢ 2V is a matroid if 1) for any A ⇢ B ⇢ V , if B 2 I, then A 2 I and 2) for any A,B 2 I if |A| < |B|, then there is an element e 2 B such that A [ {e} 2 I.",3. Notation and Background,[0],[0]
"The notion of submodularity goes beyond the discrete domain (Wolsey, 1982; Vondrák, 2007; Bach, 2015).",3. Notation and Background,[0],[0]
Consider a continuous function F : X ! R+ where the set X ✓,3. Notation and Background,[0],[0]
Rp is of the form X =Qpi=1 Xi and each Xi is a compact subset of R+.,3. Notation and Background,[0],[0]
"We call the continuous function F submodular if for all x,y 2 X",3. Notation and Background,[0],[0]
"we have
F (x) + F (y) F",3. Notation and Background,[0],[0]
(x _ y) + F,3. Notation and Background,[0],[0]
"(x ^ y), (1) where x_y := max(x,y) (component-wise) and x^y",3. Notation and Background,[0],[0]
":= min(x,y) (component-wise).",3. Notation and Background,[0],[0]
"In this paper, our focus is on differentiable continuous submodular functions with two additional properties: monotonicity and diminishing returns.",3. Notation and Background,[0],[0]
"Formally, a submodular function F is monotone if
x  y =) F (x)  ",3. Notation and Background,[0],[0]
"F (y), (2) for all x,y 2 X .",3. Notation and Background,[0],[0]
"Note that x  y in (2) means that xi  yi for all i = 1, . . .",3. Notation and Background,[0],[0]
",",3. Notation and Background,[0],[0]
p.,3. Notation and Background,[0],[0]
"Furthermore, a differentiable submodular function F is called DR-submodular (i.e., shows diminishing returns) if the gradients are antitone, namely, for all x,y 2 X we have
x  y =) rF (x) rF (y).",3. Notation and Background,[0],[0]
"(3) When the function F is twice differentiable, submodularity implies that all cross-second-derivatives are non-positive (Bach, 2015), and DR-submodularity implies that all secondderivatives are non-positive (Bian et al., 2017)",3. Notation and Background,[0],[0]
"In this work,
we consider the maximization of continuous submodular functions subject to down-closed convex bodies C ⇢ Rp+ defined as follows.",3. Notation and Background,[0],[0]
"For any two vectors x,y 2 Rp+, where x  y, down-closedness means that if y 2 C, then so is x 2 C. Note that for a down-closed set we have 0p 2 C.",3. Notation and Background,[0],[0]
"In this section, we state the problem of decentralized submodular maximization in continuous and discrete settings.
",4. Decentralized Submodular Maximization,[0],[0]
Continuous Case.,4. Decentralized Submodular Maximization,[0],[0]
We consider a set of n computing machines/sensors that communicate over a graph to maximize a global objective function.,4. Decentralized Submodular Maximization,[0],[0]
"Each machine can be viewed as a node i 2 N , {1, · · · , n}.",4. Decentralized Submodular Maximization,[0],[0]
"We further assume that the possible communication links among nodes are given by a bidirectional connected communication graph G = (N , E) where each node can only communicate with its neighbors in G. We formally use Ni to denote node i’s neighbors.",4. Decentralized Submodular Maximization,[0],[0]
"In our setting, we assume that each node i 2 N has access to a local function Fi : X !",4. Decentralized Submodular Maximization,[0],[0]
R+.,4. Decentralized Submodular Maximization,[0],[0]
The nodes cooperate in order to maximize the aggregate monotone and continuous DR-submodular function F : X !,4. Decentralized Submodular Maximization,[0],[0]
"R+ subject to a down-closed convex body C ⇢ X ⇢ Rp+, i.e.,
max x2C F (x) = max x2C
1
n
nX
i=1
Fi(x).",4. Decentralized Submodular Maximization,[0],[0]
"(4)
The goal is to design a message passing algorithm to solve (4) such that: (i) at each iteration t, the nodes send their messages (and share their information) to their neighbors in G, and (ii) as t grows, all the nodes reach to a point x 2 C that provides a (near-) optimal solution for (4).
",4. Decentralized Submodular Maximization,[0],[0]
Discrete Case.,4. Decentralized Submodular Maximization,[0],[0]
Let us now consider the discrete counterpart of problem (4).,4. Decentralized Submodular Maximization,[0],[0]
"In this setting, each node i 2 N has access to a local set function fi : 2V !",4. Decentralized Submodular Maximization,[0],[0]
R+.,4. Decentralized Submodular Maximization,[0],[0]
The nodes cooperate in maximizing the aggregate monotone submodular function f : 2V !,4. Decentralized Submodular Maximization,[0],[0]
R+ subject to a matroid constraint,4. Decentralized Submodular Maximization,[0],[0]
"I, i.e.
max S2I f(S) = max S2I
1
n
nX
i=1
fi(S).",4. Decentralized Submodular Maximization,[0],[0]
"(5)
Note that even in the centralized case, and under reasonable complexity-theoretic assumptions, the best approximation guarantee we can achieve for Problems (4) and (5) is (1 1/e) (Feige, 1998).",4. Decentralized Submodular Maximization,[0],[0]
"In the following, we show that it is possible to achieve the same approximation guarantee in a decentralized setting.",4. Decentralized Submodular Maximization,[0],[0]
"In this section, we introduce the Decentralized Continuous Greedy (DCG) algorithm for solving Problem (4).",5. Decentralized Continuous Greedy Method,[0],[0]
"Recall that in a decentralized setting, the nodes
have to cooperate (i.e., send messages to their neighbors) in order to solve the global optimization problem.",5. Decentralized Continuous Greedy Method,[0],[0]
We will explain how such messages are designed and communicated in DCG.,5. Decentralized Continuous Greedy Method,[0],[0]
"Each node i in the network keeps track of two local variables xi,di 2 Rp which are iteratively updated at each round t using the information gathered from the neighboring nodes.",5. Decentralized Continuous Greedy Method,[0],[0]
The vector xti is the local decision variable of node i at step t whose value we expect to eventually converge to the (1 1/e) fraction of the optimal solution of Problem (4).,5. Decentralized Continuous Greedy Method,[0],[0]
"The vector dti is the estimate of the gradient of the global objective function that node i keeps at step t.
To properly incorporate the received information from their neighbors, nodes should assign nonnegative weights to their neighbors.",5. Decentralized Continuous Greedy Method,[0],[0]
Define wij 0,5. Decentralized Continuous Greedy Method,[0],[0]
to be the weight that node i assigns to node j. These weights indicate the effect of (variable or gradient) information nodes received from their neighbors in order to update their local (variable or gradient) information.,5. Decentralized Continuous Greedy Method,[0],[0]
"Indeed, the weights wij must fulfill some requirements (later described in Assumption 1), but they are design parameters of DCG and can be properly chosen by the nodes prior to the implementation of the algorithm.
",5. Decentralized Continuous Greedy Method,[0],[0]
The first step at each round t of DCG is updating the local gradient approximation vectors dti using local and neighboring gradient information.,5. Decentralized Continuous Greedy Method,[0],[0]
"In particular, node i computes its vector dti according to the update rule
d t i = (1 ↵)
X
j2Ni[{i}
wijd t 1 j + ↵rFi(xti), (6)
where ↵ 2 [0, 1] is an averaging coefficient.",5. Decentralized Continuous Greedy Method,[0],[0]
"Note that the sum
P j2Ni[{i} wijd t 1 j in (6) is a weighted average of
node i’s vector dt 1i and its neighbors d t 1 j , evaluated at step t 1.",5. Decentralized Continuous Greedy Method,[0],[0]
"Hence, node i computes the vector dti by evaluating a weighted average of its current local gradient rFi(xti) and the local and neighboring gradient information at step t 1, i.e.,Pj2Ni[{i} wijdt 1j .",5. Decentralized Continuous Greedy Method,[0],[0]
"Since the vector dti is evaluated by aggregating gradient information from neighboring nodes, it is reasonable to expect that dti becomes a proper approximation for the global objective function gradient (1/n)",5. Decentralized Continuous Greedy Method,[0],[0]
Pn k=1 rfk(x) as time progresses.,5. Decentralized Continuous Greedy Method,[0],[0]
"Note that to implement the update in (6) nodes should exchange their local vectors dti with their neighbors.
",5. Decentralized Continuous Greedy Method,[0],[0]
"Using the gradient approximation vector dti, each node i evaluates its local ascent direction vti by solving
v t i = argmax v2C hdti,vi. (7)
",5. Decentralized Continuous Greedy Method,[0],[0]
The update in (7) is also known as conditional gradient update.,5. Decentralized Continuous Greedy Method,[0],[0]
"Ideally, in a conditional gradient method, we should choose the feasible direction v 2 C that maximizes the inner product by the full gradient vector 1n",5. Decentralized Continuous Greedy Method,[0],[0]
Pn k=1 rFk(xti).,5. Decentralized Continuous Greedy Method,[0],[0]
"However, since in the decentralized setting the exact gradient 1n Pn k=1 rFk(xti) is not available at the i-th node,",5. Decentralized Continuous Greedy Method,[0],[0]
"we
Algorithm 1 DCG at node i Require:",5. Decentralized Continuous Greedy Method,[0],[0]
Stepsize ↵ and weights wij for j 2 Ni,5. Decentralized Continuous Greedy Method,[0],[0]
"[ {i}
1: Initialize local vectors as x0i = d0i = 0p 2:",5. Decentralized Continuous Greedy Method,[0],[0]
"Initialize neighbor’s vectors as x0j = d0j = 0p if j2Ni 3: for t = 1, 2, . . .",5. Decentralized Continuous Greedy Method,[0],[0]
", T do 4: Compute dti = (1 ↵) X
j2Ni[{i}
wijd t 1 j + ↵rFi(xti);
5: Exchange dti with neighboring nodes j 2 Ni 6: Evaluate vti = argmaxv2C hdti,vi; 7: Update the variable xt+1i =",5. Decentralized Continuous Greedy Method,[0],[0]
"X
j2Ni[{i}
wijx t j + 1
T v
t i ;
8: Exchange xt+1i with neighboring nodes j 2 Ni 9: end for
replace it by its current approximation dti",5. Decentralized Continuous Greedy Method,[0],[0]
"and hence we obtain the update rule (7).
",5. Decentralized Continuous Greedy Method,[0],[0]
"After computing the local ascent directions vti , the nodes update their local variables xti by averaging their local and neighboring iterates and ascend in the direction vti with stepsize 1/T where T is the total number of iterations, i.e.,
x t+1 i =
X
j2Ni[{i}
wijx t j + 1
T v
t i .",5. Decentralized Continuous Greedy Method,[0],[0]
"(8)
The update rule (8) ensures that the neighboring iterates are not far from each other via the averaging termP
j2Ni[{i} wijx t j , while the iterates approach the optimal maximizer of the global objective function by ascending in the conditional gradient direction vti .",5. Decentralized Continuous Greedy Method,[0],[0]
The update in (8) requires a round of local communication among neighbors to exchange their local variables xti.,5. Decentralized Continuous Greedy Method,[0],[0]
"The steps of the DCG method are summarized in Algorithm 1.
",5. Decentralized Continuous Greedy Method,[0],[0]
"Indeed, the weights wij that nodes assign to each other cannot be arbitrary.",5. Decentralized Continuous Greedy Method,[0],[0]
"In the following, we formalize the conditions that they should satisfy (Yuan et al., 2016).
",5. Decentralized Continuous Greedy Method,[0],[0]
"Assumption 1 The weights that nodes assign to each other are nonegative, i.e., wij 0 for all i, j 2 N , and if node j is not a neighbor of node",5. Decentralized Continuous Greedy Method,[0],[0]
"i then the corresponding weight is zero, i.e., wij = 0",5. Decentralized Continuous Greedy Method,[0],[0]
if j /2 Ni.,5. Decentralized Continuous Greedy Method,[0],[0]
"Further, the weight matrix W 2 Rn⇥n with entries wij satisfies W † = W, W1n = 1n, null(I W) = span(1n).",5. Decentralized Continuous Greedy Method,[0],[0]
"(9)
The first condition in (9) ensures that the weights are symmetric, i.e., wij = wji.",5. Decentralized Continuous Greedy Method,[0],[0]
"The second condition guarantees the weights that each node assigns to itself and its neighbors sum up to 1, i.e., Pn j=1 wij = 1 for all i. Note that the condition",5. Decentralized Continuous Greedy Method,[0],[0]
W1n = 1n implies that I W is rank deficient.,5. Decentralized Continuous Greedy Method,[0],[0]
"Hence, the last condition in (9) ensures that the rank of I W is exactly n 1.",5. Decentralized Continuous Greedy Method,[0],[0]
"Indeed, it is possible to optimally
design the weight matrix W to accelerate the averaging process as discussed in (Boyd et al., 2004), but this is not the focus of this paper.",5. Decentralized Continuous Greedy Method,[0],[0]
"We emphasize that W is not a problem parameter, and we design it prior to running DCG.
",5. Decentralized Continuous Greedy Method,[0],[0]
"Notice that the stepsize 1/T and the conditions in Assumption 1 on the weights wij are needed to ensure that the local variables xti are in the feasible set C, as stated in the following proposition.
",5. Decentralized Continuous Greedy Method,[0],[0]
Proposition 1 Consider the DCG method outlined in Algorithm 1.,5. Decentralized Continuous Greedy Method,[0],[0]
"If Assumption 1 holds and nodes start from x
0",5. Decentralized Continuous Greedy Method,[0],[0]
"i = 0p 2 C, then the local iterates xti are always in the
feasible set C, i.e., xti 2 C for all i 2 N and t = 1, . . .",5. Decentralized Continuous Greedy Method,[0],[0]
", T .
Let us now explain how DCG relates to and innovates beyond the exisiting work in submodular maximization as well as decentralized convex optimization.",5. Decentralized Continuous Greedy Method,[0],[0]
"Note that in order to solve Problem (4) in a centralized fashion (i.e., when every node has access to all the local functions) we can use the continuous greedy algorithm (Vondrák, 2008), a variant of the conditional gradient method.",5. Decentralized Continuous Greedy Method,[0],[0]
"However, in decentralized settings, nodes have only access to their local gradients, and therefore, continuous greedy is not implementable.",5. Decentralized Continuous Greedy Method,[0],[0]
"Similar to the decentralized convex optimization, we can address this issue via local information aggregation.",5. Decentralized Continuous Greedy Method,[0],[0]
"Our proposed DCG method incorporates the idea of choosing the ascent direction according to a conditional gradient update as is done in the continuous greedy algorithm (i.e., the update rule (7)), while it aggregates the global objective function information through local communications with neighboring nodes (i.e., the update rule (8)).",5. Decentralized Continuous Greedy Method,[0],[0]
"Unlike traditional consensus optimization methods that require exchanging nodes’ local variables only (Nedic & Ozdaglar, 2009; Nedic et al., 2010), DCG also requires exchanging local gradient vectors to achieve a (1 1/e) fraction of the optimal solution at each node (i.e., the update rule (6)).",5. Decentralized Continuous Greedy Method,[0],[0]
"This major difference is due to the fact that in conditional gradient methods, unlike proximal gradient algorithms, the local gradients can not be used instead of the global gradient.",5. Decentralized Continuous Greedy Method,[0],[0]
"In other words, in the update rule (7), we can not use the local gradients rFi(xti) in lieu of dti.",5. Decentralized Continuous Greedy Method,[0],[0]
"Indeed, there are settings for which such a replacement provides arbitrarily bad solutions.",5. Decentralized Continuous Greedy Method,[0],[0]
We formally characterize the convergence of DCG in Theorem 1.,5. Decentralized Continuous Greedy Method,[0],[0]
"In this section we show how DCG can be used for maximizing a decentralized submodular set function f , namely Problem (5), through its continuous relaxation.",5.1. Extension to the Discrete Setting,[0],[0]
"Formally, in lieu of solving Problem (5), we can form the following decentralized continuous optimization problem
max x2C
1
n
nX
i=1
Fi(x), (10)
",5.1. Extension to the Discrete Setting,[0],[0]
"Algorithm 2 Discrete DCG at node i Require: ↵, 2 [0, 1] and weights wij for",5.1. Extension to the Discrete Setting,[0],[0]
j 2 Ni,5.1. Extension to the Discrete Setting,[0],[0]
"[ {i}
1: Initialize local vectors as x0i = d0i = g0i = 0 2: Initialize neighbor’s vectors as x0j = d0j = 0",5.1. Extension to the Discrete Setting,[0],[0]
"if j 2 Ni 3: for t = 1, 2, . . .",5.1. Extension to the Discrete Setting,[0],[0]
", T do 4: Compute gti = (1 )gt 1i + r ˜Fi(xti); 5: Compute dti = (1 ↵) X
j2Ni[{i}
wijd t 1 j + ↵g t i ;
6: Exchange dti with neighboring nodes j 2 Ni 7: Evaluate vti = argmaxv2C hdti,vi; 8: Update the variable xt+1i =",5.1. Extension to the Discrete Setting,[0],[0]
"X
j2Ni[{i}
wijx t j + 1
T v
t i ;
9: Exchange xt+1i with neighboring nodes j 2 Ni; 10: end for 11:",5.1. Extension to the Discrete Setting,[0],[0]
"Apply proper rounding to obtain a solution for (5);
where Fi is the multilinear extension of fi defined as
Fi(x) = X
S⇢V fi(S)
Y i2S xi Y j /2S (1 xj), (11)
and the down-closed convex set C = conv{1I :",5.1. Extension to the Discrete Setting,[0],[0]
I 2 I} is the matroid polytope.,5.1. Extension to the Discrete Setting,[0],[0]
"Note that the discrete and continuous optimization formulations lead to the same optimal value (Calinescu et al., 2011).
",5.1. Extension to the Discrete Setting,[0],[0]
"Based on the expression in (11), computing the full gradient rFi at each node i will require an exponential computation in terms of |V |, since the number of summands in (11) is 2
|V |.",5.1. Extension to the Discrete Setting,[0],[0]
"As a result, in the discrete setting, we will slightly modify the DCG algorithm and work with unbiased estimates of the gradient that can be computed in time O(|V |) (see Appendix 9.7 for one such estimator).",5.1. Extension to the Discrete Setting,[0],[0]
"More precisely, in the discrete setting, each node i 2 N updates three local variables xti,dti,gti 2 R|V |.",5.1. Extension to the Discrete Setting,[0],[0]
"The variables xti,dti play the same role as in DCG and are updated using the messages received from the neighboring nodes.",5.1. Extension to the Discrete Setting,[0],[0]
The variable gti at node i is defined to approximate the local gradient rFi(xti).,5.1. Extension to the Discrete Setting,[0],[0]
"Consider the vector r ˜Fi(xti) as an unbiased estimator of the local gradient rFi(xti) at time t, and define the vector g
t i as the outcome of the recursion
g t i = (1 )gt 1i + r ˜Fi(xti), (12)
where 2 [0, 1] is the averaging parameter.",5.1. Extension to the Discrete Setting,[0],[0]
We initialize all vectors as g0i = 0 2 R|V |.,5.1. Extension to the Discrete Setting,[0],[0]
"It was shown recently (Mokhtari et al., 2018a;b) that the averaging technique in (12) reduces the noise of the gradient approximations.",5.1. Extension to the Discrete Setting,[0],[0]
"Therefore, the sequence of gti approaches the true local gradient rFi(xti) as time progresses.
",5.1. Extension to the Discrete Setting,[0],[0]
"The steps of the Decentralized Continuous Greedy for the discrete setting is summarized in Algo-
rithm 2.",5.1. Extension to the Discrete Setting,[0],[0]
Note that the major difference between the Discrete DCG method (Algorithm 2) and the continuous DCG method (Algorithm 1) is in Step 5 in which the exact local gradient rFi(xti) is replaced by the stochastic approximation gti which only requires access to the computationally cheap unbiased gradient estimator r ˜Fi(xti).,5.1. Extension to the Discrete Setting,[0],[0]
The communication complexity of both the discrete and continuous versions of DCG are the same at each round.,5.1. Extension to the Discrete Setting,[0],[0]
"However, since we are using unbiased estimations of the local gradients rFi(xi), the Discrete DCG takes more rounds to converge to a near-optimal solution compared to continuous DCG.",5.1. Extension to the Discrete Setting,[0],[0]
We characterize the convergence of Discrete DCG in Theorem 2.,5.1. Extension to the Discrete Setting,[0],[0]
"Further, the implementation of Discrete DCG requires rounding the continuous solution to obtain a discrete solution for the original problem without any loss in terms of the objective function value.",5.1. Extension to the Discrete Setting,[0],[0]
"The provably lossless rounding schemes include the pipage rounding (Calinescu et al., 2011) and contention resolution (Chekuri et al., 2014).",5.1. Extension to the Discrete Setting,[0],[0]
"In this section, we study the convergence properties of DCG in both continuous and discrete settings.",6. Convergence Analysis,[0],[0]
"In this regard, we assume that the following conditions hold.
",6. Convergence Analysis,[0],[0]
"Assumption 2 Euclidean distance of the elements in the set C are uniformly bounded, i.e., for all x,y 2 C we have
kx yk  D. (13) Assumption 3",6. Convergence Analysis,[0],[0]
The local objective functions Fi(x) are monotone and DR-submodular.,6. Convergence Analysis,[0],[0]
"Further, their gradients are L-Lipschitz continuous over the set X , i.e., for all x,y 2 X
krFi(x) rFi(y)k  ",6. Convergence Analysis,[0],[0]
Lkx yk.,6. Convergence Analysis,[0],[0]
(14) Assumption 4,6. Convergence Analysis,[0],[0]
"The norm of gradients krFi(x)k are bounded over the convex set C, i.e., for all x 2 C, i 2 N ,
krFi(x)k  G. (15)
",6. Convergence Analysis,[0],[0]
The condition in Assumption 2 guarantees that the diameter of the convex set C is bounded.,6. Convergence Analysis,[0],[0]
Assumption 3 is needed to ensure that the local objective functions Fi are smooth.,6. Convergence Analysis,[0],[0]
"Finally, the condition in Assumption 4 enforces the gradients norm to be bounded over the convex set C. All these assumptions are customary and necessary in the analysis of decentralized algorithms.",6. Convergence Analysis,[0],[0]
"For more details, please check Section VII-B in Jakovetić et al. (2014).
",6. Convergence Analysis,[0],[0]
We proceed to derive a constant factor approximation for DCG.,6. Convergence Analysis,[0],[0]
Our main result is stated in Theorem 1.,6. Convergence Analysis,[0],[0]
"However, to better illustrate the main result, we first need to provide several definitions and technical lemmas.",6. Convergence Analysis,[0],[0]
"Let us begin by defining the average variables ¯xt as
¯ x t = 1
n
nX
i=1
x t i. (16)
",6. Convergence Analysis,[0],[0]
"In the following lemma, we establish an upper bound on the variation in the sequence of average variables {¯xt}.",6. Convergence Analysis,[0],[0]
Lemma 1 Consider the proposed DCG algorithm defined in Algorithm 1.,6. Convergence Analysis,[0],[0]
"Further, recall the definition of ¯xt in (16).",6. Convergence Analysis,[0],[0]
"If Assumptions 1 and 2 hold, then the difference between two consecutive average vectors is upper bounded by
k¯xt+1 ¯xtk  D T .",6. Convergence Analysis,[0],[0]
"(17)
Recall that at every node i, the messages are mixed using the coefficients wij , i.e., the i-th row of the matrix W. It is thus not hard to see that the spectral properties of W (e.g. the spectral gap) play an important role in the the speed of achieving consensus in decentralized methods.
",6. Convergence Analysis,[0],[0]
Definition 1 Consider the eigenvalues of W which can be sorted in a nonincreasing order as 1 = 1(W) 2(W) · · · n(W),6. Convergence Analysis,[0],[0]
> 1.,6. Convergence Analysis,[0],[0]
"Define as the second largest magnitude of the eigenvalues of W, i.e.,
:= max{| 2(W)|, | n(W)|}.",6. Convergence Analysis,[0],[0]
"(18)
",6. Convergence Analysis,[0],[0]
"As we will see, a mixing matrix W with smaller has a larger spectral gap 1 which yields faster convergence (Boyd et al., 2004; Duchi et al., 2012).",6. Convergence Analysis,[0],[0]
"In the following lemma, we derive an upper bound on the sum of the distances between the local iterates xti and their average ¯xt, where the bound is a function of the graph spectral gap 1 , size of the network n, and the total number of iterations T .
",6. Convergence Analysis,[0],[0]
Lemma 2 Consider the proposed DCG algorithm defined in Algorithm 1.,6. Convergence Analysis,[0],[0]
"Further, recall the definition of ¯xt in (16).",6. Convergence Analysis,[0],[0]
"If Assumptions 1 and 2 hold, then for all t  T we have
nX
i=1
x t i ¯xt
2 !1/2  p nD
T (1 ) .",6. Convergence Analysis,[0],[0]
"(19)
Let us now define ¯dt as the average of local gradient approximations dti at step t, i.e., ¯dt = 1 n",6. Convergence Analysis,[0],[0]
Pn i=1 d t i.,6. Convergence Analysis,[0],[0]
"We will show in the following that the vectors dti also become uniformly close to ¯dt.
",6. Convergence Analysis,[0],[0]
Lemma 3 Consider the proposed DCG algorithm defined in Algorithm 1.,6. Convergence Analysis,[0],[0]
"If Assumptions 1 and 3 hold, then
nX
i=1
kdti ¯dtk2 !1/2  ↵ p nG
1 (1 ↵)",6. Convergence Analysis,[0],[0]
.,6. Convergence Analysis,[0],[0]
"(20)
Lemma 3 guarantees that the individual local gradient approximation vectors dti are close to the average vector ¯dt if the parameter ↵ is small.",6. Convergence Analysis,[0],[0]
"To show that the gradient vectors dti, generated by DCG, approximate the gradient of the
global objective function, we further need to show that the average vector ¯dt approaches the global objective function gradient rF .",6. Convergence Analysis,[0],[0]
We prove this claim in the following lemma.,6. Convergence Analysis,[0],[0]
Lemma 4 Consider the proposed DCG algorithm defined in Algorithm 1.,6. Convergence Analysis,[0],[0]
"If Assumptions 1-4 hold, then
¯ d t 1 n
nX
i=1
rFi(¯xt)
 (1 ↵)tG+ ✓ (1 ↵)LD
↵T +
LD T (1 ) ◆ .",6. Convergence Analysis,[0],[0]
"(21)
",6. Convergence Analysis,[0],[0]
"By combining Lemmas 3 and 4 and setting ↵ = 1/ p T we can conclude that the local gradient approximation vector d
t",6. Convergence Analysis,[0],[0]
"i of each node i is within O(1/
p T )",6. Convergence Analysis,[0],[0]
"distance of the global
objective gradient rF (¯xt) evaluated at ¯xt.",6. Convergence Analysis,[0],[0]
"We use this observation in the following theorem to show that the sequence of iterates generated by DCG achieves the tight (1 1/e) approximation ratio of the optimum global solution.
",6. Convergence Analysis,[0],[0]
Theorem 1 Consider the proposed DCG method outlined in Algorithm 1.,6. Convergence Analysis,[0],[0]
"Further, consider x⇤ as the global maximizer of Problem (4).",6. Convergence Analysis,[0],[0]
"If Assumptions 1-4 hold and we set ↵ = 1/ p T , for all nodes j 2 N , the local variable xTj obtained after T iterations satisfies
F (xTj ) (1 e 1)F",6. Convergence Analysis,[0],[0]
"(x⇤) LD2 +GD T 1/2 GD
T 1/2(1 )",6. Convergence Analysis,[0],[0]
"LD 2
2T GD + LD
2
T (1 ) .",6. Convergence Analysis,[0],[0]
"(22)
Theorem 1 shows that the sequence of the local variables x
t j , generated by DCG, is able to achieve the optimal approximation ratio (1 1/e), while the error term vanishes at a sublinear rate of O(1/T 1/2), i.e.,
F (xTj ) (1 1/e)F (x⇤) O",6. Convergence Analysis,[0],[0]
"✓
1 (1 )T 1/2 ◆ , (23)
which implies that the iterate of each node reaches an objective value larger than (1 1/e ✏)OPT after O(1/✏2) rounds of communication.",6. Convergence Analysis,[0],[0]
It is worth mentioning that the result in Theorem 1 is consistent with classical results in decentralized optimization that the error term vanishes faster for the graphs with larger spectral gap 1 .,6. Convergence Analysis,[0],[0]
We proceed to study the convergence properties of Discrete DCG in Algorithm 2.,6. Convergence Analysis,[0],[0]
"To do so, we first assume that the variance of the stochastic gradients r ˜Fi(x) used in Discrete DCG is bounded.",6. Convergence Analysis,[0],[0]
"We justify this assumption in Remark 1.
",6. Convergence Analysis,[0],[0]
Assumption 5,6. Convergence Analysis,[0],[0]
The variance of the unbiased estimators r ˜F,6. Convergence Analysis,[0],[0]
"(x) is bounded above by 2 over the convex set C, i.e., for any i 2 N and any vector x 2 C we can write
E h kr ˜Fi(x) rFi(x)k2 i  2, (24)
where the expectation is with respect to the randomness of the unbiased estimator.
",6. Convergence Analysis,[0],[0]
"In the following theorem, we show that Discrete DCG achieves a (1 1/e) approximation ration for Problem (5).
",6. Convergence Analysis,[0],[0]
Theorem 2 Consider our proposed Discrete DCG algorithm outlined in Algorithm 2.,6. Convergence Analysis,[0],[0]
Recall the definition of the multilinear extension function Fi in (11).,6. Convergence Analysis,[0],[0]
"If Assumptions 1-5 hold and we set ↵ = T 1/2 and = T 2/3, then for all nodes j 2 N the local variables xTj obtained after running Discrete DCG for T iterations satisfy
E ⇥",6. Convergence Analysis,[0],[0]
F (xTj ) ⇤ (1 e 1)F,6. Convergence Analysis,[0],[0]
(x⇤) O,6. Convergence Analysis,[0],[0]
"✓
1 (1 )T 1/3 ◆ , (25)
where x⇤ is the global maximizer of Problem (10).
",6. Convergence Analysis,[0],[0]
Theorem 2 states that the sequence of iterates generated by Discrete DCG achieves the tight (1 1/e ✏) approximation guarantee for Problem (10) after O(1/✏3) iterations.,6. Convergence Analysis,[0],[0]
Remark 1,6. Convergence Analysis,[0],[0]
For any submodular set function h : 2V !,6. Convergence Analysis,[0],[0]
"R with associated multilinear extension H , it can be shown that its Lipschitz constant L and the gradient norm G are both bounded above by mf
p|V |, where mf is the maximum marginal value of f , i.e., mf = maxi2V f({i}) (see, Hassani et al. (2017)).",6. Convergence Analysis,[0],[0]
"Similarly, it can be shown that for the unbiased estimator in Appendix 9.7 we have  mf p|V |.",6. Convergence Analysis,[0],[0]
We will consider a discrete setting for our experiments and use Algorithm 2 to find a decentralized solution.,7. Numerical Experiments,[0],[0]
"The main objective is to demonstrate how consensus is reached and how the global objective increases depending on the topology of the network and the parameters of the algorithm.
",7. Numerical Experiments,[0],[0]
"For our experiments, we have used the MovieLens data set.",7. Numerical Experiments,[0],[0]
It consists of 1 million ratings (from 1 to 5) by M = 6000 users for p = 4000 movies.,7. Numerical Experiments,[0],[0]
We consider a network of n = 100 nodes.,7. Numerical Experiments,[0],[0]
"The data has been distributed equally between the nodes of the network, i.e., the set of users has been partitioned into 100 equally-sized sets and each node in the network has access to only one chunk (partition) of the data.",7. Numerical Experiments,[0],[0]
The global task is to find a set of k movies that are most satisfactory to all the users (the precise formulation will appear shortly).,7. Numerical Experiments,[0],[0]
"However, as each of the nodes in the network has access to the data of a small portion of the users, the nodes have to cooperate to fulfill the global task.
",7. Numerical Experiments,[0],[0]
We consider a well motivated objective function for the experiments.,7. Numerical Experiments,[0],[0]
"Let r`,j denote the rating of user ` for movie j (if such a rating does not exist in the data we assign r`,j to 0).",7. Numerical Experiments,[0],[0]
We associate to each user ` a “facility location” objective function g`(S),7. Numerical Experiments,[0],[0]
"= maxj2S r`,j , where S is any subset of
the movies (i.e. the ground set V is the set of the movies).",7. Numerical Experiments,[0],[0]
Such a function shows how much user ` will be “satisfied” by a subset S of the movies.,7. Numerical Experiments,[0],[0]
Recall that each node i in the network has access to the data of a (small) subset of users which we denote by Ui.,7. Numerical Experiments,[0],[0]
The objective function associated with node i is given by fi(S) = P `2Ui g`(S).,7. Numerical Experiments,[0],[0]
"With such a choice of the local functions, our global task is hence to solve problem (5) when the matroid I is the k-uniform matroid (a.k.a.",7. Numerical Experiments,[0],[0]
"the k-cardinality constraint).
",7. Numerical Experiments,[0],[0]
"We consider three different choices for the underlying communication graph between the 100 nodes: A line graph (which looks like a simple path from node 1 to node 100), an Erdos-Renyi random graph (with average degree 5), and a complete graph.",7. Numerical Experiments,[0],[0]
The matrix W is chosen as follows (based on each of the three graphs).,7. Numerical Experiments,[0],[0]
"If (i, j) is and edge of the graph, we let wi,j = 1/(1+max(di, dj)).",7. Numerical Experiments,[0],[0]
"If (i, j) is not an edge and i, j are distinct integers, we have wi,j = 0.",7. Numerical Experiments,[0],[0]
"Finally we let wi,i = 1 P j2N wi,j .",7. Numerical Experiments,[0],[0]
"It is not hard to show that the above choice for W satisfies Assumption 1.
",7. Numerical Experiments,[0],[0]
Figure 1 shows how consensus is reached w.r.t each of the three underlying networks.,7. Numerical Experiments,[0],[0]
"To measure consensus, we plot the (logarithm of) distance-to-average value 1n",7. Numerical Experiments,[0],[0]
Pn i=1,7. Numerical Experiments,[0],[0]
"||xTi
¯",7. Numerical Experiments,[0],[0]
x T ||,7. Numerical Experiments,[0],[0]
as a function of the total number of iterations T averaged over many trials (see (16) for the definition of ¯xT ).,7. Numerical Experiments,[0],[0]
It is easy to see that the distance to average is small if and only if all the local decisions xTi are close to the average decision ¯xT .,7. Numerical Experiments,[0],[0]
"As expected, it takes much less time to reach consensus when the underlying graph is fully connected (i.e. complete graph).",7. Numerical Experiments,[0],[0]
"For the line graph, the convergence is very slow as this graph has the least degree of connectivity.
",7. Numerical Experiments,[0],[0]
Figure 2 depicts the obtained objective value of Discrete DCG (Algorithm 2) for the three networks considered above.,7. Numerical Experiments,[0],[0]
"More precisely, we plot the value 1n",7. Numerical Experiments,[0],[0]
Pn i=1,7. Numerical Experiments,[0],[0]
"f(x T i ) obtained
at the end of Algorithm 2 as a function of the cardinality constraint k.",7. Numerical Experiments,[0],[0]
We also compare these values with the value obtained by the centralized greedy algorithm (i.e. the centralized solution).,7. Numerical Experiments,[0],[0]
A few comments are in order.,7. Numerical Experiments,[0],[0]
The performance of Algorithm 2 is close to the centralized solution when the underlying graph is the Erdos-Renyi (with average degree 5) graph or the complete graphs.,7. Numerical Experiments,[0],[0]
This is because for both such graphs consensus is achieved from the early stages of the algorithm.,7. Numerical Experiments,[0],[0]
"By increasing T , we see that the performance becomes closer to the centralized solution.",7. Numerical Experiments,[0],[0]
"However, when the underlying graph is the line graph, then consensus will not be achieved unless the number of iterations is significantly increased.",7. Numerical Experiments,[0],[0]
"Consequently, for small number of iterations (e.g. T  1000) the performance of the algorithm will not be close to the centralized solution.",7. Numerical Experiments,[0],[0]
"In this paper, we proposed the first fully decentralized optimization method for maximizing discrete and continuous submodular functions.",8. Conclusion,[0],[0]
"We developed Decentralized Continuous Greedy (DCG) that achieves a (1 1/e ✏) approximation guarantee with O(1/✏2) and (1/✏3) local rounds of communication in the continuous and discrete settings, respectively.",8. Conclusion,[0],[0]
"This work was done while A. Mokhtari was visiting the Simons Institute for the Theory of Computing, and his work was partially supported by the DIMACS/Simons Collaboration on Bridging Continuous and Discrete Optimization through NSF grant #CCF-1740425.",Acknowledgements,[0],[0]
The work of A. Karbasi was supported by DARPA Young Faculty Award (D16AP00046) and AFOSR YIP (FA9550-18-1-0160).,Acknowledgements,[0],[0]
"In this paper, we showcase the interplay between discrete and continuous optimization in network-structured settings.",abstractText,[0],[0]
We propose the first fully decentralized optimization method for a wide class of non-convex objective functions that possess a diminishing returns property.,abstractText,[0],[0]
"More specifically, given an arbitrary connected network and a global continuous submodular function, formed by a sum of local functions, we develop Decentralized Continuous Greedy (DCG), a message passing algorithm that converges to the tight (1 1/e) approximation factor of the optimum global solution using only local computation and communication.",abstractText,[0],[0]
We also provide strong convergence bounds as a function of network size and spectral characteristics of the underlying topology.,abstractText,[0],[0]
"Interestingly, DCG readily provides a simple recipe for decentralized discrete submodular maximization through the means of continuous relaxations.",abstractText,[0],[0]
"Formally, we demonstrate that by lifting the local discrete functions to continuous domains and using DCG as an interface we can develop a consensus algorithm that also achieves the tight (1 1/e) approximation guarantee of the global discrete solution once a proper rounding scheme is applied.",abstractText,[0],[0]
Decentralized Submodular Maximization: Bridging Discrete and Continuous Settings,title,[0],[0]
"Some decisions are easier to make than others—for example, large, unoccluded objects are easier to recognize.",1. Introduction,[0],[0]
"Additionally, different difficult decisions may require different expertise—an avid birder may know very little about identifying cars.",1. Introduction,[0],[0]
"We hypothesize that complex decision-making tasks like visual classification can be meaningfully divided into specialized subtasks, and that a system designed to perform a complex task should first attempt to identify the subtask being presented to it, then use that information to select the most suitable algorithm for its solution.
",1. Introduction,[0],[0]
"This approach—dynamically routing signals through an inference system, based on their content—has already been incorporated into machine vision pipelines via methods such as boosting (Viola et al., 2005), coarse-to-fine cascades (Zhou et al., 2013), and random decision forests (Ho, 1995).",1. Introduction,[0],[0]
"Dynamic routing is also performed in the primate visual system: spatial information is processed somewhat separately from object identity information (Goodale &
1California Institute of Technology, Pasadena, California, USA.",1. Introduction,[0],[0]
"Correspondence to: Mason McGill <mmcgill@caltech.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"Milner, 1992), and faces and other behaviorally-relevant stimuli ellicit responses in anatomically distinct, specialized regions (Moeller et al., 2008; Kornblith et al., 2013).",1. Introduction,[0],[0]
"However, state-of-the-art artificial neural networks (ANNs) for visual inference are routed statically (Simonyan & Zisserman, 2014; He et al., 2016; Dosovitskiy et al., 2015; Newell et al., 2016); every input triggers an identical sequence of operations.
",1. Introduction,[0],[0]
"With this in mind, we propose a mechanism for introducing cascaded evaluation to arbitrary feedforward ANNs, focusing on the task of object recognition as a proof of concept.",1. Introduction,[0],[0]
"Instead of classifying images only at the final layer, every layer in the network may attempt to classify images in lowambiguity regions of its input space, passing ambiguous images forward to subsequent layers for further consideration (see Fig. 1 for an illustration).",1. Introduction,[0],[0]
"We propose three approaches to training these networks, test them on small image datasets synthesized from MNIST (LeCun et al., 1998) and CIFAR-10 (Krizhevsky & Hinton, 2009), and quantify the accuracy/efficiency trade-off that occurs when the network parameters are tuned to yield more aggressive early classification policies.",1. Introduction,[0],[0]
"Additionally, we propose and evaluate methods for appropriating regularization and optimization techniques developed for statically-routed networks.",1. Introduction,[0],[0]
"Since the late 1980s, researchers have combined artificial neural networks with decision trees in various
Deciding How to Decide: Dynamic Routing in Artificial Neural Networks
ways (Utgoff, 1989)",2. Related Work,[0],[0]
"(Sirat & Nadal, 1990).",2. Related Work,[0],[0]
"More recently, Kontschieder et al. (2015) performed joint optimization of ANN and decision tree parameters, and Bulo & Kontschieder (2014) used randomized multi-layer networks to compute decision tree split functions.
",2. Related Work,[0],[0]
"To our knowledge, the family of inference systems we discuss was first described by Denoyer & Gallinari (2014).",2. Related Work,[0],[0]
"Additionally, Bengio et al. (2015) explored dynamically skipping layers in neural networks, and Ioannou et al. (2016) explored dynamic routing in networks with equallength paths.",2. Related Work,[0],[0]
"Some recently-developed visual detection systems perform cascaded evaluation of convolutional neural network layers (Li et al., 2015; Cai et al., 2015; Girshick, 2015; Ren et al., 2015); though highly specialized for the task of visual detection, these modifications can radically improve efficiency.
",2. Related Work,[0],[0]
"While these approaches lend evidence that dynamic routing can be effective, they either ignore the cost of computation, or do not represent it explicitly, and instead use opaque heuristics to trade accuracy for efficiency.",2. Related Work,[0],[0]
"We build on this foundation by deriving training procedures from arbitrary application-provided costs of error and computation, comparing one actor-style and two critic-style strategies, and considering regularization and optimization in the context of dynamically-routed networks.",2. Related Work,[0],[0]
"In a statically-routed, feedforward artificial neural network, every layer transforms a single input feature vector into a single output feature vector.",3. Setup,[0],[0]
"The output feature vector is then used as the input to the following layer (which we’ll refer to as the current layer’s sink), if it exists, or as the ouptut of the network as a whole, if it does not.
",3. Setup,[0],[0]
We consider networks in which layers may have more than one sink.,3. Setup,[0],[0]
"In such a network, for every n-way junction j a signal reaches, the network must make a decision, dj ∈ {0..n}, such that the signal will propagate through the ith sink if and only if dj = i (this is illustrated in Fig. 2).",3. Setup,[0],[0]
"We compute dj as the argmax of the score vector sj , a learned function of the last feature vector computed before reaching j. We’ll refer to this rule for generating d from s as the inference routing policy.",3. Setup,[0],[0]
Convolutional network layers compute collections of local descriptions of the input signal.,3.1. Multipath Architectures for Convolutional Networks,[0],[0]
"It is unreasonable to expect that this kind of feature vector can explicitly encode the global information relevant to deciding how to route the entire signal (e.g., in the case of object recognition, whether the image was taken indoors, whether the image contains
dj = 0 dj = 1 Source Sink 1 Sink 0",3.1. Multipath Architectures for Convolutional Networks,[0],[0]
"Source Sink 1 Sink 0
Figure 2.",3.1. Multipath Architectures for Convolutional Networks,[0],[0]
"A 2-way junction, j. dj is an integer function of the source features.",3.1. Multipath Architectures for Convolutional Networks,[0],[0]
"When dj = 0, the signal is propagated through the top sink, and the bottom sink is inactive.",3.1. Multipath Architectures for Convolutional Networks,[0],[0]
"When dj = 1, the signal is propagated through the bottom sink, and the top sink is inactive.
an animal, or the prevalence of occlusion in the scene).
",3.1. Multipath Architectures for Convolutional Networks,[0],[0]
"To address this, instead of computing a 2-dimensional array of local features at each layer, we compute a pyramid of features (resembling the pyramids described by Ke et al. (2016)), with local descriptors at the bottom and global descriptors at the top.",3.1. Multipath Architectures for Convolutional Networks,[0],[0]
"At every junction j, the score vector sj is computed by a small routing network operating on the last-computed global descriptor.",3.1. Multipath Architectures for Convolutional Networks,[0],[0]
Our multipath architecture is illustrated in Fig. 3.,3.1. Multipath Architectures for Convolutional Networks,[0],[0]
"For a given input, network ν, and set of routing decisions d, we define the cost of performing inference:
",3.2. Balancing Accuracy and Efficiency,[0],[0]
"cinf(ν, d) = cerr(ν, d) + ccpt(ν, d), (1)
where cerr(ν, d) is the cost of the inference errors made by the network, and ccpt(ν, d) is the cost of computation.",3.2. Balancing Accuracy and Efficiency,[0],[0]
"In our experiments, unless stated otherwise, cerr is the crossentropy loss and
ccpt(ν, d) = kcptnops(ν, d), (2)
where nops(ν, d) is the number of multiply-accumulate operations performed and kcpt is a scalar hyperparameter.",3.2. Balancing Accuracy and Efficiency,[0],[0]
"This definition assumes a time- or energy-constrained system—every operation consumes roughly the same amount of time and energy, so every operation is equally expensive.",3.2. Balancing Accuracy and Efficiency,[0],[0]
ccpt may be defined differently under other constraints (e.g. memory bandwidth).,3.2. Balancing Accuracy and Efficiency,[0],[0]
"We propose three approaches to training dynamicallyrouted networks, along with complementary approaches to regularization and optimization, and a method for adapting to changes in the cost of computation.
4×4 8×8
16×16 32×32
16 chan.",4. Training,[0],[0]
16 chan.,4. Training,[0],[0]
32 chan. 32 chan.,4. Training,[0],[0]
64 chan.,4. Training,[0],[0]
64 chan.,4. Training,[0],[0]
128 chan.,4. Training,[0],[0]
"128 chan.
“Horse”
Convolution, Batch Normalization, RectificationLinear Transformation, Batch Normalization, RectificationLinear Transformation, SoftmaxLinear Transformation, Argmax“Stop” Signal“Go” Signal
RoutingSubnetworks
Figure 3.",4. Training,[0],[0]
Our multiscale convolutional architecture.,4. Training,[0],[0]
"Once a column is evaluated, the network decides whether to classify the image or evaluate subsequent columns.",4. Training,[0],[0]
"Deeper columns operate at coarser scales, but compute higher-dimensional representations at each location.",4. Training,[0],[0]
"All convolutions use 3×3 kernels, downsampling is achieved via 2×2 max pooling, and all routing layers have 16 channels.",4. Training,[0],[0]
"Since d is discrete, cinf(ν, d) cannot be minimized via gradient-based methods.",4.1. Training Strategy I: Actor Learning,[0],[0]
"However, if d is replaced by a stochastic approximation, d̂, during training, we can engineer the gradient of E[cinf(ν, d̂)] to be nonzero.",4.1. Training Strategy I: Actor Learning,[0],[0]
"We can then learn the routing parameters and classification parameters simultaneously by minimizing the loss
Lac = E[cinf(ν, d̂)].",4.1. Training Strategy I: Actor Learning,[0],[0]
"(3)
In our experiments, the training routing policy samples d̂ such that
Pr(d̂j = i) = softmax(sj/τ)i, (4)
where τ is the network “temperature”: a scalar hyperparameter that decays over the course of training, converging the training routing policy towards the inference routing policy.",4.1. Training Strategy I: Actor Learning,[0],[0]
"Alternatively, we can attempt to learn to predict the expected utility of making every routing decision.",4.2. Training Strategy II: Pragmatic Critic Learning,[0],[0]
"In this case, we minimize the loss
Lcr = E cinf(ν, d̂)",4.2. Training Strategy II: Pragmatic Critic Learning,[0],[0]
"+∑ j∈J cjure  , (5)
where J is the set of junctions encountered when making the routing decisions d̂, and cure is the utility regression error cost, defined:
cjure = kure‖sj − uj‖2, (6)
where
uij = −cinf(νij , d), (7)
kure is a scalar hyperparameter, and νij is the subnetwork consisting of the ith child of νj , and all of its descendants.",4.2. Training Strategy II: Pragmatic Critic Learning,[0],[0]
"Since we want to learn the policy indirectly (via cost prediction), d̂ is treated as constant with respect to optimization.",4.2. Training Strategy II: Pragmatic Critic Learning,[0],[0]
"To improve the stability of the loss and potentially accelerate training, we can adjust the routing utility function u such that, for every junction j, uj is independent of the routing parameters downstream of j. Instead of predicting the cost of making routing decisions given the current downstream routing policy, we can predict the cost of making routing decisions given the optimal downstream routing policy.",4.3. Training Strategy III: Optimistic Critic Learning,[0],[0]
"In this optimistic variant of the critic method,
uij = −mind′(cinf(νij , d′)).",4.3. Training Strategy III: Optimistic Critic Learning,[0],[0]
(8),4.3. Training Strategy III: Optimistic Critic Learning,[0],[0]
"Many regularization techniques involve adding a modelcomplexity term, cmod, to the loss function to influence learning, effectively imposing soft constraints upon the network parameters (Hoerl & Kennard, 1970; Rudin et al., 1992; Tibshirani, 1996).",4.4. Regularization,[0],[0]
"However, if such a term affects layers in a way that is independent of the amount of signal routed through them, it will either underconstrain frequently-used layers or overconstrain infrequently-used layers.",4.4. Regularization,[0],[0]
"To support both frequently- and infrequently-used layers, we regularize subnetworks as they are activated by d̂, instead of regularizing the entire network directly.
",4.4. Regularization,[0],[0]
"For example, to apply L2 regularization to critic networks, we define cmod:
cmod = E [ kL2
∑ w∈W w2
] , (9)
where W is the set of weights associated with the layers activated by d̂, and kL2 is a scalar hyperparameter.
",4.4. Regularization,[0],[0]
"For actor networks, we apply an extra term to control the magnitude of s, and therefore the extent to which the net explores subpotimal paths:
cmod = E kL2 ∑ w∈W w2 + kdec ∑ j∈J ‖sj‖2  , (10) where kdec is a scalar hyperparameter indicating the relative cost of decisiveness.
",4.4. Regularization,[0],[0]
cmod is added to the loss function in all of our experiments.,4.4. Regularization,[0],[0]
"Within cmod, unless stated otherwise, d̂ is treated as constant with respect to optimization.",4.4. Regularization,[0],[0]
"Throughput Variations
Both training techniques attempt to minimize the expected cost of performing inference with the network, over the training routing policy.",4.5. Adjusting Learning Rates to Compensate for,[0],[0]
"With this setup, if we use a constant learning rate for every layer in the network, then layers through which the policy routes examples more frequently will receive larger parameter updates, since they contribute more to the expected cost.
",4.5. Adjusting Learning Rates to Compensate for,[0],[0]
"To allow every layer to learn as quickly as possible, we scale the learning rate of each layer ` dynamically, by a factor α`, such that the elementwise variance of the loss gradient with respect to `’s parameters is independent of the amount of probability density routed through it.
",4.5. Adjusting Learning Rates to Compensate for,[0],[0]
"To derive α`, we consider an alternative routing policy, d∗` , that routes all signals though `, then routes through subse-
quent layers based on d̂. With this policy, at every training interation, mini-batch stochastic gradient descent shifts the parameters associated with layer ` by a vector δ∗` , defined:
δ∗` = −λ ∑ i gi`, (11)
where λ is the global learning rate and gi` is the gradient of the loss with respect to the parameters in `, for training example i, under d∗` .",4.5. Adjusting Learning Rates to Compensate for,[0],[0]
"Analogously, the scaled parameter adjustment under d̂ can be written
δ` = −α`λ ∑ i pi`g i `, (12)
where pi` is the probability with which d̂ routes example i through `.
",4.5. Adjusting Learning Rates to Compensate for,[0],[0]
"We want to select α` such that
Var(δ`) = Var(δ ∗ ` ).",4.5. Adjusting Learning Rates to Compensate for,[0],[0]
"(13)
Substituting the definitions of δ` and δ∗` ,
Var ( α` ∑ i pi`g i ` ) =",4.5. Adjusting Learning Rates to Compensate for,[0],[0]
Var (∑ i gi` ) .,4.5. Adjusting Learning Rates to Compensate for,[0],[0]
"(14)
Since every gi` is sampled independently, we can rewrite this equation:
nexv`α 2 `‖p`‖2 = nexv`, (15)
where nex is the number of training examples in the minibatch and v` is the elementwise variance of gi`, for any i (since every example is sampled via the same mechanism).",4.5. Adjusting Learning Rates to Compensate for,[0],[0]
"We can now show that
α` = ‖p`‖−1.",4.5. Adjusting Learning Rates to Compensate for,[0],[0]
"(16)
",4.5. Adjusting Learning Rates to Compensate for,[0],[0]
"So, for every layer `, we can scale the learning rate by ‖p`‖−1, and the variance of the weight updates will be similar thoughout the network.",4.5. Adjusting Learning Rates to Compensate for,[0],[0]
"We use this technique, unless otherwise specified, in all of our experiments.",4.5. Adjusting Learning Rates to Compensate for,[0],[0]
We may want a single network to perform well in situations with various degrees of computational resource scarcity (e.g. computation may be more expensive when a device battery is low).,4.6. Responding to Changes in the Cost of Computation,[0],[0]
"To make the network’s routing behavior responsive to a dynamic ccpt, we can concatenate ccpt’s known
parameters—in our case, {kcpt}—to the input of every routing subnetwork, to allow them to modulate the routing policy.",4.6. Responding to Changes in the Cost of Computation,[0],[0]
"To match the scale of the image features and facilitate optimization, we express kcpt in units of cost per tenmillion operations.",4.6. Responding to Changes in the Cost of Computation,[0],[0]
"In all of our experiments, we use a mini-batch size, nex, of 128, and run 80,000 training iterations.",4.7. Hyperparameters,[0],[0]
We perform stochastic gradient descent with initial learning rate 0.1/nex and momentum 0.9.,4.7. Hyperparameters,[0],[0]
"The learning rate decays continuously with a half-life of 10,000 iterations.
",4.7. Hyperparameters,[0],[0]
"The weights of the final layers of routing networks are zero-initialized, and we initialize all other weights using the Xavier initialization method (Glorot & Bengio, 2010).",4.7. Hyperparameters,[0],[0]
All biases are zero-initialized.,4.7. Hyperparameters,[0],[0]
"We perform batch normalization (Ioffe & Szegedy, 2015) before every rectification operation, with an of 1×10−6, and an exponential moving average decay constant of 0.9.
",4.7. Hyperparameters,[0],[0]
"τ is initialized to 1.0 for actor networks and 0.1 for critic networks, and decays with a half-life of 10,000 iterations.",4.7. Hyperparameters,[0],[0]
"kdec = 0.01, kure = 0.001, and kL2 = 1 × 10−4.",4.7. Hyperparameters,[0],[0]
"We selected these values (for τ , kdec, kure, and kL2) by exploring the hyperparameter space logarithmically, by powers of 10, training and evaluating on the hybrid MNIST/CIFAR-10 dataset (described in section 5.1).",4.7. Hyperparameters,[0],[0]
"At a coarse level, these values are locally optimal—multiplying or dividing any of them by 10 will not improve performance.",4.7. Hyperparameters,[0],[0]
"We augment our data using an approach that is popular for use with CIFAR-10 (Lin et al., 2013) (Srivastava et al., 2015) (Clevert et al., 2015).",4.8. Data Augmentation,[0],[0]
"We augment each image by applying vertical and horizontal shifts sampled uniformly from the range [-4px,4px], and, if the image is from CIFAR-10, flipping it horizontally with probability 0.5.",4.8. Data Augmentation,[0],[0]
We fill blank pixels introduced by shifts with the mean color of the image (after gamma-decoding).,4.8. Data Augmentation,[0],[0]
"We compare approaches to dynamic routing by training 153 networks to classify small images, varying the policy-learning strategy, regularization strategy, optimization strategy, architecture, cost of computation, and details of the task.",5. Experiments,[0],[0]
"The results of these experiments are reported in
Fig. 5–10.",5. Experiments,[0],[0]
Our code is available via GitLab.,5. Experiments,[0],[0]
"To compare routing strategies in the context of a simple dataset with a high degree of difficulty variation, we train
networks to classify images from a small-image dataset synthesized from MNIST (LeCun et al., 1998) and CIFAR10 (Krizhevsky & Hinton, 2009) (see Fig. 4).",5.1. Comparing Policy-Learning Strategies,[0],[0]
"Our dataset includes the classes “0”, “1”, “2”, “3”, and “4” from MNIST and “airplane”, “automobile”, “deer”, “horse”, and “frog” from CIFAR-10 (see Fig. 4).",5.1. Comparing Policy-Learning Strategies,[0],[0]
"The images from MNIST are resized to match the scale of images from CIFAR-10 (32×32), via linear interpolation, and are colormodulated to make them more difficult to trivially distinguish from CIFAR-10 images (MNIST is a grayscale dataset).
",5.1. Comparing Policy-Learning Strategies,[0],[0]
"For a given computational budget, dynamically-routed networks achieve higher accuracy rates than architecturematched statically-routed baselines (networks composed of the first n columns of the architecture illustrated in Fig. 3, for n ∈ {1..8}).",5.1. Comparing Policy-Learning Strategies,[0],[0]
"Additionally, dynamically-routed networks tend to avoid routing data along deep paths at the beginning of training (see Fig. 8).",5.1. Comparing Policy-Learning Strategies,[0],[0]
"This is possibly because the error surfaces of deeper networks are more complicated, or because deeper paths are less stable—changing the parameters in any component layer to better classify images routed along other, overlapping paths may decrease performance.",5.1. Comparing Policy-Learning Strategies,[0],[0]
"Whatever the mechanism, this tendency to initially find simpler solutions seems to prevent some of the overfitting that occurs with 7- and 8-layer statically-routed networks.
",5.1. Comparing Policy-Learning Strategies,[0],[0]
"Compared to other dynamically-routed networks, optimistic critic networks perform poorly, possibly because optimal routers are a poor approximation for our small, lowcapacity router networks.",5.1. Comparing Policy-Learning Strategies,[0],[0]
"Actor networks perform better than critic networks, possibly because critic networks are forced to learn a potentially-intractable auxilliary task (i.e. it’s easier to decide who to call to fix your printer than it is to predict exactly how quickly and effectively everyone you know would fix it).",5.1. Comparing Policy-Learning Strategies,[0],[0]
"Actor networks also consistently achieve higher peak accuracy rates than comparable statically-routed networks, across experiments.
4 8
40k
80k
E p
o ch
In d
ex
kcpt = 0
4 8
kcpt = 1×10−9
4 8
kcpt = 2×10−9
4 8
kcpt = 4×10−9
0.0
0.2
0.4
0.6
0.8
1.0
Layer Index
Figure 8.",5.1. Comparing Policy-Learning Strategies,[0],[0]
Dataflow over the course of training.,5.1. Comparing Policy-Learning Strategies,[0],[0]
"The heatmaps illustrate the fraction of validation images classified at every terminal node in the bottom four networks in Fig. 6, over the course of training.
",5.1. Comparing Policy-Learning Strategies,[0],[0]
"Although actor networks may be more performant, critic networks are more flexible.",5.1. Comparing Policy-Learning Strategies,[0],[0]
"Since critic networks don’t require E[cinf(ν, d̂)] to be a differentiable function of d̂, they can be trained by sampling d̂, saving memory, and they support a wider selection of training routing policies (e.g. -greedy) and cinf definitions.",5.1. Comparing Policy-Learning Strategies,[0],[0]
"In addition to training the standard critic networks, we train networks using a variant of the pragmatic critic training policy, in which we replace
the cross-entropy error in the cure term with the classification error.",5.1. Comparing Policy-Learning Strategies,[0],[0]
"Although these networks do not perform as well as the original pragmatic critic networks, they still outperform comparable statically-routed networks.",5.1. Comparing Policy-Learning Strategies,[0],[0]
"Based on our experiments with the hybrid dataset, regularizing d̂, as described in section 4.4, discourages networks from routing data along deep paths, reducing peak accuracy.",5.2. Comparing Regularization Strategies,[0],[0]
"Additionally, some mechanism for encouraging exploration (in our case, a nonzero kdec) appears to be necessary to train effective actor networks.",5.2. Comparing Regularization Strategies,[0],[0]
"Throughput-adjusting the learning rates (TALR), as described in section 4.5, improves the hybrid dataset performance of both actor and critic networks in computationalresource-abundant, high-accuracy contexts.",5.3. Comparing Optimization Strategies,[0],[0]
"For a given computational budget, architectures with both 2- and 3-way junctions have a higher capacity than subtrees with only 2-way junctions.",5.4. Comparing Architectures,[0],[0]
"On the hybrid dataset, under tight computational constraints, we find that trees with higher degrees of branching achieve higher accuracy rates.",5.4. Comparing Architectures,[0],[0]
"Unconstrained, however, they are prone to overfitting.
",5.4. Comparing Architectures,[0],[0]
"In dynamically-routed networks, early classification layers tend to have high accuracy rates, pushing difficult decisions downstream.",5.4. Comparing Architectures,[0],[0]
"Even without energy contraints, terminal layers specialize in detecting instances of certain classes of images.",5.4. Comparing Architectures,[0],[0]
These classes are usually related (they either all come from MNIST or all come from CIFAR-10.),5.4. Comparing Architectures,[0],[0]
"In networks with both 2- and 3-way junctions, branches specialize to an even greater extent.",5.4. Comparing Architectures,[0],[0]
(See Fig. 6 and 7.),5.4. Comparing Architectures,[0],[0]
"We train a single actor network to classify images from the hybrid datset under various levels of computational constraints, using the approach described in section 4.6, sampling kcpt randomly from the set mentioned in Fig. 5 for each training example.",5.5. Comparing Specialized and Adaptive Networks,[0],[0]
"This network performs comparably to a collection of 8 actor nets trained with various static values of kcpt, over a significant, central region of the accuracy/efficiency curve, with an 8-fold reduction in memory consumption and training time.",5.5. Comparing Specialized and Adaptive Networks,[0],[0]
"To probe the effect of the inference task’s difficulty distribution on the performance of dynamically-routed net-
works, we train networks to classify images from CIFAR10, adjusting the classification task to vary the frequency of difficult decisions (see Fig. 9).",5.6. Exploring the Effects of the Decision Difficulty Distribution,[0],[0]
"We call these variants CIFAR-2—labelling images as “horse” or “other”— and CIFAR-5—labelling images as “cat”, “dog”, “deer”, “horse”, or “other”.",5.6. Exploring the Effects of the Decision Difficulty Distribution,[0],[0]
"In this experiment, we compare actor networks (the best-performing networks from the first set of experiments) to architecture-matched statically-routed networks.
",5.6. Exploring the Effects of the Decision Difficulty Distribution,[0],[0]
"We find that dynamic routing is more beneficial when the task involves many low-difficulty decisions, allowing the network to route more data along shorter paths.",5.6. Exploring the Effects of the Decision Difficulty Distribution,[0],[0]
"While dynamic routing offers only a slight advantage on CIFAR-10, dynamically-routed networks achieve a higher peak accuracy rate on CIFAR-2 than statically-routed networks, at a third of the computational cost.",5.6. Exploring the Effects of the Decision Difficulty Distribution,[0],[0]
"To test whether dynamic routing is advantageous in highercapacity settings, we train actor networks and architecturematched statically-routed networks to classify images from CIFAR-10, varying the width of the networks (see Fig. 10).",5.7. Exploring the Effects of Model Capacity,[0],[0]
"Increasing the model capacity either increases or does not affect the relative advantage of dynamically-routed networks, suggesting that our approach is applicable to more complicated tasks.",5.7. Exploring the Effects of Model Capacity,[0],[0]
"Our experiments suggest that dynamically-routed networks trained under mild computational constraints can operate 2–3 times more efficiently than comparable staticallyrouted networks, without sacrificing performance.",6. Discussion,[0],[0]
"Additionally, despite their higher capacity, dynamically-routed networks may be less prone to overfitting.
",6. Discussion,[0],[0]
"When designing a multipath architecture, we suggest supporting early decision-making wherever possible, since cheap, simple routing networks seem to work well.",6. Discussion,[0],[0]
"In convolutional architectures, pyramidal layers appear to be reasonable sites for branching.
",6. Discussion,[0],[0]
The actor strategy described in section 4.1 is generally an effective way to learn a routing policy.,6. Discussion,[0],[0]
"However, the pragmatic critic strategy described in section 4.2 may be better suited for very large networks (trained via decision sampling to conserve memory) or networks designed for applications with nonsmooth cost-of-inference functions—e.g. one in which kcpt has units errors/operation.",6. Discussion,[0],[0]
"Adjusting learning rates to compensate for throughput variations, as described in section 4.5, may improve the performance of deep networks.",6. Discussion,[0],[0]
"If the cost of computation is dynamic, a single network, trained with the procedure described in section 5.5, may still be sufficient.
",6. Discussion,[0],[0]
"While we test our approach on tasks with some degree of difficulty variation, it is possible that dynamic routing is even more advantageous when performing more complex tasks.",6. Discussion,[0],[0]
"For example, video annotation may require specialized modules to recognize locations, objects, faces, human actions, and other scene components or attributes, but having every module constantly operating may be extremely inefficient.",6. Discussion,[0],[0]
"A dynamic routing policy could fuse these modules, allowing them to share common components, and activate specialized components as necessary.
",6. Discussion,[0],[0]
Another interesting topic for future research is growing and shrinking dynamically-routed networks during training.,6. Discussion,[0],[0]
"With such a network, it is not necessary to specify an architecture.",6. Discussion,[0],[0]
"The network will instead take shape over the course of training, as computational contraints, memory contraints, and the data dictate.",6. Discussion,[0],[0]
This work was funded by a generous grant from Google Inc.,Acknowledgements,[0],[0]
"We would also like to thank Krzysztof Chalupka, Cristina Segalin, and Oisin Mac Aodha for their thoughtful comments.",Acknowledgements,[0],[0]
We propose and systematically evaluate three strategies for training dynamically-routed artificial neural networks: graphs of learned transformations through which different input signals may take different paths.,abstractText,[0],[0]
"Though some approaches have advantages over others, the resulting networks are often qualitatively similar.",abstractText,[0],[0]
"We find that, in dynamically-routed networks trained to classify images, layers and branches become specialized to process distinct categories of images.",abstractText,[0],[0]
"Additionally, given a fixed computational budget, dynamically-routed networks tend to perform better than comparable staticallyrouted networks.",abstractText,[0],[0]
Deciding How to Decide:  Dynamic Routing in Artificial Neural Networks,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 869–874 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
869",text,[0],[0]
Breaking substitution ciphers recovers the plaintext from a ciphertext that uses a 1:1 or homophonic cipher key.,1 Introduction,[1.0],['Breaking substitution ciphers recovers the plaintext from a ciphertext that uses a 1:1 or homophonic cipher key.']
"Previous work using pretrained language models (LMs) for decipherment use n-gram LMs (Ravi and Knight, 2011; Nuhn et al., 2013).",1 Introduction,[1.0],"['Previous work using pretrained language models (LMs) for decipherment use n-gram LMs (Ravi and Knight, 2011; Nuhn et al., 2013).']"
"Some methods use the ExpectationMaximization (EM) algorithm (Knight et al., 2006) while most state-of-the-art approaches for decipherment of 1:1 and homophonic substitution ciphers use beam search and rely on the clever use of n-gram LMs (Nuhn et al., 2014; Hauer et al., 2014).",1 Introduction,[0],[0]
"Neural LMs globally score the entire candidate plaintext sequence (Mikolov et al., 2010).",1 Introduction,[0],[0]
"However, using a neural LM for decipherment is not trivial because scoring the entire candidate partially deciphered plaintext is computationally challenging.",1 Introduction,[0],[0]
We solve both of these problems in this paper and provide an improved beam search based decipherment algorithm for homophonic ciphers that exploits pre-trained neural LMs for the first time.,1 Introduction,[0],[0]
We use the notation from Nuhn et al. (2013).,2 Decipherment Model,[0],[0]
Ciphertext fN1 = f1..fi..fN and plaintext e N 1 = e1..ei..eN consist of vocabularies,2 Decipherment Model,[0],[0]
fi ∈ Vf and ei ∈ Ve respectively.,2 Decipherment Model,[0],[0]
The beginning tokens in the ciphertext (f0) and plaintext (e0) are set to “$” denoting the beginning of a sentence.,2 Decipherment Model,[1.0],['The beginning tokens in the ciphertext (f0) and plaintext (e0) are set to “$” denoting the beginning of a sentence.']
The substitutions are represented by a function φ :,2 Decipherment Model,[0],[0]
Vf → Ve such that 1:1 substitutions are bijective while homophonic substitutions are general.,2 Decipherment Model,[0],[0]
"A cipher function φwhich does not have every φ(f) fixed is called a partial cipher function (Corlett and Penn, 2010).",2 Decipherment Model,[0],[0]
The number of fs that are fixed in φ is given by its cardinality. φ′,2 Decipherment Model,[0],[0]
"is called an extension of φ, if f is fixed in φ′ such that δ(φ′(f), φ(f))",2 Decipherment Model,[0],[0]
yields true ∀f ∈,2 Decipherment Model,[0],[0]
Vf which are already fixed in φ where δ is Kronecker delta.,2 Decipherment Model,[0],[0]
"Decipherment is then the task of finding the φ for which the probability of the deciphered text is maximized.
φ̂ = argmax φ p(φ(f1)...φ(fN ))",2 Decipherment Model,[0],[0]
"(1)
where p(.) is the language model (LM).",2 Decipherment Model,[0],[0]
"Finding this argmax is solved using a beam search algorithm (Nuhn et al., 2013) which incrementally finds the most likely substitutions using the language model scores as the ranking.",2 Decipherment Model,[1.0],"['Finding this argmax is solved using a beam search algorithm (Nuhn et al., 2013) which incrementally finds the most likely substitutions using the language model scores as the ranking.']"
The advantage of a neural LM is that it can be used to score the entire candidate plaintext for a hypothesized partial decipherment.,2.1 Neural Language Model,[0],[0]
"In this work, we use a state of the art byte (character) level neural LM using a multiplicative LSTM (Radford et al., 2017).
Consider a sequence S = w1, w2, w3, ..., wN .",2.1 Neural Language Model,[0],[0]
"The LM score of S is SCORE(S):
P (S) = P (w1, w2, w3, ..., wN )
P (S) = N∏ i=1",2.1 Neural Language Model,[0],[0]
"P (wi | w1, w2, ..., wi−1))
SCORE(S) =",2.1 Neural Language Model,[0],[0]
− N∑ i=1,2.1 Neural Language Model,[0],[0]
log(P,2.1 Neural Language Model,[0],[0]
"(wi | w<i))
(2)",2.1 Neural Language Model,[0],[0]
"Algorithm 1 is the beam search algorithm (Nuhn et al., 2013, 2014) for solving substitution ciphers.",2.2 Beam Search,[0],[0]
It monitors all partial hypotheses in lists Hs and Ht based on their quality.,2.2 Beam Search,[0],[0]
"As the search progresses, the partial hypotheses are extended, scored with SCORE and appended to Ht.",2.2 Beam Search,[0],[0]
EXT LIMITS determines which extensions should be allowed and EXT ORDER picks the next cipher symbol for extension.,2.2 Beam Search,[1.0],['EXT LIMITS determines which extensions should be allowed and EXT ORDER picks the next cipher symbol for extension.']
The search continues after pruning:,2.2 Beam Search,[0],[0]
Hs ← HISTOGRAM_PRUNE(Ht).,2.2 Beam Search,[0],[0]
"We augment this algorithm by updating the SCORE function with a neural LM.
Algorithm 1 Beam Search for Decipherment 1: function (BEAM SEARCH (EXT ORDER, EXT LIM-
ITS)) 2: initialize sets Hs, Ht 3: CARDINALITY = 0 4: Hs.ADD((∅,0)) 5: while CARDINALITY < |Vf",2.2 Beam Search,[0],[0]
| do 6: f = EXT ORDER[CARDINALITY] 7: for all φ ∈,2.2 Beam Search,[0],[0]
Hs do 8: for all e ∈,2.2 Beam Search,[0],[0]
"Ve do 9: φ’ := φ ∪ {(e, f)}
10: if EXT LIMITS(φ’)",2.2 Beam Search,[0],[0]
"then 11: Ht.ADD(φ’,SCORE(φ’)) 12: HISTOGRAM PRUNE(Ht) 13: CARDINALITY = CARDINALITY",2.2 Beam Search,[0],[0]
"+ 1 14: Hs = Ht 15: Ht.CLEAR() 16: return WINNER(Hs)
3 Score Estimation (SCORE)
",2.2 Beam Search,[0],[0]
Score estimation evaluates the quality of the partial hypotheses φ.,2.2 Beam Search,[0],[0]
"Using the example from Nuhn et al. (2014), consider the vocabularies Ve = {a, b, c, d} and Vf = {A,B,C,D}, extension order (B,A,C,D), and ciphertext $ ABDDCABCDADCABDC $.",2.2 Beam Search,[1.0],"['Using the example from Nuhn et al. (2014), consider the vocabularies Ve = {a, b, c, d} and Vf = {A,B,C,D}, extension order (B,A,C,D), and ciphertext $ ABDDCABCDADCABDC $.']"
"Let φ = {(a,A), (b, B))} be the partial hypothesis.",2.2 Beam Search,[1.0],"['Let φ = {(a,A), (b, B))} be the partial hypothesis.']"
Then SCORE(φ) scores this hypothesized partial decipherment (only A and B are converted to plaintext) using a pre-trained language model in the hypothesized plaintext language.,2.2 Beam Search,[1.0],['Then SCORE(φ) scores this hypothesized partial decipherment (only A and B are converted to plaintext) using a pre-trained language model in the hypothesized plaintext language.']
The initial rest cost estimator introduced by Nuhn et al. nuhnbeam computes the score of hypotheses only based on partially deciphered text that builds a shard of n adjacent solved symbols.,3.1 Baseline,[1.0],['The initial rest cost estimator introduced by Nuhn et al. nuhnbeam computes the score of hypotheses only based on partially deciphered text that builds a shard of n adjacent solved symbols.']
"As a heuristic, n-grams which still consist of unsolved cipher-symbols are assigned a trivial estimate of probability 1.",3.1 Baseline,[0],[0]
"An improved version of rest cost es-
timation (Nuhn et al., 2014) consults lower order n-grams to score each position.",3.1 Baseline,[0],[0]
"The baseline scoring method greatly relies on local context, i.e. the estimation is strictly based on partial character sequences.",3.2 Global Rest Cost Estimation,[0],[0]
"Since this depends solely on the n-gram LM, the true conditional probability under Markov assumption is not modeled and, therefore, context dependency beyond the window of (n− 1) is ignored.",3.2 Global Rest Cost Estimation,[0],[0]
"Thus, attempting to utilize a higher amount of context can lower the probability of some tokens resulting in poor scores.
",3.2 Global Rest Cost Estimation,[0],[0]
We address this issue with a new improved version of the rest cost estimator by supplementing the partial decipherment φ(fN1 ) with predicted plaintext text symbols using our neural language model (NLM).,3.2 Global Rest Cost Estimation,[1.0],['We address this issue with a new improved version of the rest cost estimator by supplementing the partial decipherment φ(fN1 ) with predicted plaintext text symbols using our neural language model (NLM).']
"Applying φ = {(a,A), (b, B))} to the ciphertext above, we get the following partial hypothesis: φ(fN1 ) = $a1b2...a6b7..a10..a13b14..$ We introduce a scoring function that is able to score the entire plaintext including the missing plaintext symbols.",3.2 Global Rest Cost Estimation,[1.0],"['Applying φ = {(a,A), (b, B))} to the ciphertext above, we get the following partial hypothesis: φ(fN1 ) = $a1b2...a6b7..a10..a13b14..$ We introduce a scoring function that is able to score the entire plaintext including the missing plaintext symbols.']"
"First, we sample1 the plaintext symbols from the NLM at all locations depending on the deciphered tokens from the partial hypothesis φ such that these tokens maintain their respective positions in the sequence, and at the same time are sampled from the neural LM to fit (probabilistically) in this context.",3.2 Global Rest Cost Estimation,[1.0],"['First, we sample1 the plaintext symbols from the NLM at all locations depending on the deciphered tokens from the partial hypothesis φ such that these tokens maintain their respective positions in the sequence, and at the same time are sampled from the neural LM to fit (probabilistically) in this context.']"
"Next, we determine the probability of the entire sequence including the scores of sampled plaintext as our rest cost estimate.
",3.2 Global Rest Cost Estimation,[0],[0]
"NLM
In our running example, this would yield a score estimation of the partial decipherment, φ(fN1 ) :
φ(fN1 ) = $ a1b2d3c4c5a6b7c8d9a10d11d12a13b14d15c16 $
Thus, the neural LM is used to predict the score of the full sequence.",3.2 Global Rest Cost Estimation,[0],[0]
"This method of global scoring evaluates each candidate partial decipherment by scoring the entire message, augmented by the sam-
1The char-level sampling is done incrementally from left to right to generate a sequence that contains the deciphered tokens from φ at the exact locations they occur in the above φ(fN1 ).",3.2 Global Rest Cost Estimation,[0],[0]
"If the LM prediction contradicts the hypothesized decipherment we stop sampling and start from the next character.
pled plaintext symbols from the NLM.",3.2 Global Rest Cost Estimation,[0],[0]
"Since more terms participate in the rest cost estimation with global context, we use the plaintext LM to provide us with a better rest cost in the beam search.",3.2 Global Rest Cost Estimation,[1.0],"['Since more terms participate in the rest cost estimation with global context, we use the plaintext LM to provide us with a better rest cost in the beam search.']"
"Alignment by frequency similarity (Yarowsky and Wicentowski, 2000) assumes that two forms belong to the same lemma when their relative frequency fits the expected distribution.",3.3 Frequency Matching Heuristic,[1.0],"['Alignment by frequency similarity (Yarowsky and Wicentowski, 2000) assumes that two forms belong to the same lemma when their relative frequency fits the expected distribution.']"
"We use this heuristic to augment the score estimation (SCORE):
FMH(φ′) = ∣∣∣∣log(ν(f)ν(e) )",3.3 Frequency Matching Heuristic,[0],[0]
∣∣∣∣ f ∈,3.3 Frequency Matching Heuristic,[0],[0]
"Vf , e ∈",3.3 Frequency Matching Heuristic,[0],[0]
"Ve (3) ν(f) is the percentage relative frequency of the ciphertext symbol f , while ν(e) is the percentage relative frequency of the plaintext token e in the plaintext language model.",3.3 Frequency Matching Heuristic,[0],[0]
"The closer this value to 0, the more likely it is that f is mapped to e.
Thus given a φ with the SCORE(φ), the extension φ′",3.3 Frequency Matching Heuristic,[0.9926357862022158],"['The closer this value to 0, the more likely it is that f is mapped to e. Thus given a φ with the SCORE(φ), the extension φ′ (Algo.']"
(Algo. 1) is scored as: SCORE(φ′) = SCORE(φ) + NEW(φ′)− FMH(φ′) (4) where NEW is the score for symbols that have been newly fixed in φ′ while extending φ to φ′.,3.3 Frequency Matching Heuristic,[0],[0]
Our experimental evaluations show that the global rest cost estimator and the frequency matching heuristic contribute positively towards the accuracy of different ciphertexts.,3.3 Frequency Matching Heuristic,[1.0],['Our experimental evaluations show that the global rest cost estimator and the frequency matching heuristic contribute positively towards the accuracy of different ciphertexts.']
"We carry out 2 sets of experiments: one on letter based 1:1, and another on homophonic substitution ciphers.",4 Experimental Evaluation,[1.0],"['We carry out 2 sets of experiments: one on letter based 1:1, and another on homophonic substitution ciphers.']"
"We report Symbol Error Rate (SER) which is the fraction of characters in the deciphered text that are incorrect.
",4 Experimental Evaluation,[0.9999999920877457],['We report Symbol Error Rate (SER) which is the fraction of characters in the deciphered text that are incorrect.']
"The character NLM uses a single layer multiplicative LSTM (mLSTM) (Radford et al., 2017) with 4096 units.",4 Experimental Evaluation,[0],[0]
The model was trained for a single epoch on mini-batches of 128 subsequences of length 256 for a total of 1 million weight updates.,4 Experimental Evaluation,[0],[0]
"States were initialized to zero at the beginning of each data shard and persisted across updates to simulate full-backprop and allow for the forward propagation of information outside of a given sub-
sequence.",4 Experimental Evaluation,[0],[0]
In all the experiments we use a character NLM trained on English Gigaword corpus augmented with a short corpus of plaintext letters of about 2000 words authored by the Zodiac killer2.,4 Experimental Evaluation,[0],[0]
"In this experiment we use a synthetic 1:1 letter substitution cipher dataset following Ravi and Knight (2008), Nuhn et al. (2013) and Hauer et al. (2014).",4.1 1:1 Substitution Ciphers,[1.0],"['In this experiment we use a synthetic 1:1 letter substitution cipher dataset following Ravi and Knight (2008), Nuhn et al. (2013) and Hauer et al. (2014).']"
"The text is from English Wikipedia articles about history3, preprocessed by stripping the text of all images, tables, then lower-casing all characters, and removing all non-alphabetic and non-space characters.",4.1 1:1 Substitution Ciphers,[1.0],"['The text is from English Wikipedia articles about history3, preprocessed by stripping the text of all images, tables, then lower-casing all characters, and removing all non-alphabetic and non-space characters.']"
"We create 50 cryptograms for each length 16, 32, 64, 128 and 256 using a random Caesar-cipher 1:1 substitution.
",4.1 1:1 Substitution Ciphers,[0],[0]
"2https://en.wikisource.org/wiki/Zodiac Killer letters 3http://en.wikipedia.org/wiki/History
Fig 1 plots the results of our method for cipher lengths of 16, 32, 64, 128 and 256 alongside Beam 6-gram (the best performing model) model (Nuhn et al., 2013)",4.1 1:1 Substitution Ciphers,[0.9989632991926579],"['Fig 1 plots the results of our method for cipher lengths of 16, 32, 64, 128 and 256 alongside Beam 6-gram (the best performing model) model (Nuhn et al., 2013)']"
"Zodiac-408, a homophonic cipher, is commonly used to evaluate decipherment algorithms.
",4.2 An Easy Cipher: Zodiac-408,[1.0000000346742794],"['Zodiac-408, a homophonic cipher, is commonly used to evaluate decipherment algorithms.']"
"Our neural LM model with global rest cost estimation and frequency matching heuristic with a beam size of 1M has SER of 1.2% compared to the beam search algorithm (Nuhn et al., 2013) with beam size of 10M with a 6-gram LM which gives an SER of 2%.",4.2 An Easy Cipher: Zodiac-408,[1.0],"['Our neural LM model with global rest cost estimation and frequency matching heuristic with a beam size of 1M has SER of 1.2% compared to the beam search algorithm (Nuhn et al., 2013) with beam size of 10M with a 6-gram LM which gives an SER of 2%.']"
"The improved beam search (Nuhn et al., 2014) with an 8-gram LM, however, gets 52 out of 54 mappings correct on the Zodiac-408 cipher.",4.2 An Easy Cipher: Zodiac-408,[1.0],"['The improved beam search (Nuhn et al., 2014) with an 8-gram LM, however, gets 52 out of 54 mappings correct on the Zodiac-408 cipher.']"
Part 2 of the Beale Cipher is a more challenging homophonic cipher because of a much larger search space of solutions.,4.3 A Hard Cipher: Beale Pt 2,[1.0],['Part 2 of the Beale Cipher is a more challenging homophonic cipher because of a much larger search space of solutions.']
"Nunh et al. (2014) were the first to automatically decipher this Beale Cipher.
",4.3 A Hard Cipher: Beale Pt 2,[1.0000000018597084],['Nunh et al. (2014) were the first to automatically decipher this Beale Cipher.']
"With an error of 5% with beam size of 1M vs 5.4% with 8-gram LM and a pruning size of 10M, our system outperforms the state of the art (Nuhn et al., 2014) on this task.
!",4.3 A Hard Cipher: Beale Pt 2,[0],[0]
1,4.3 A Hard Cipher: Beale Pt 2,[0],[0]
"Automatic decipherment for substitution ciphers started with dictionary attacks (Hart, 1994; Jakobsen, 1995; Olson, 2007).",5 Related Work,[0],[0]
Ravi and Knight (2008) frame the decipherment problem as an integer linear programming (ILP) problem.,5 Related Work,[0],[0]
Knight et al. (2006) use an HMM-based EM algorithm for solving a variety of decipherment problems.,5 Related Work,[0],[0]
"Ravi and Knight (2011) extend the HMM-based EM approach with a Bayesian approach, and report the
first automatic decipherment of the Zodiac-408 cipher.
",5 Related Work,[0],[0]
Berg-Kirkpatrick and Klein (2013) show that a large number of random restarts can help the EM approach.,5 Related Work,[0],[0]
Corlett and Penn (2010) presented an efficient A* search algorithm to solve letter substitution ciphers.,5 Related Work,[0],[0]
Nuhn et al. (2013) produce better results in faster time compared to ILP and EM-based decipherment methods by employing a higher order language model and an iterative beam search algorithm.,5 Related Work,[0],[0]
Nuhn et al. (2014) present various improvements to the beam search algorithm in Nuhn et al. (2013) including improved rest cost estimation and an optimized strategy for ordering decipherment of the cipher symbols.,5 Related Work,[0],[0]
Hauer et al. (2014) propose a novel approach for solving mono-alphabetic substitution ciphers which combines character-level and word-level language model.,5 Related Work,[0],[0]
"They formulate decipherment as a tree search problem, and use Monte Carlo Tree Search (MCTS) as an alternative to beam search.",5 Related Work,[0],[0]
"Their approach is the best for short ciphers.
",5 Related Work,[0],[0]
Greydanus (2017) frames the decryption process as a sequence-to-sequence translation task and uses a deep LSTM-based model to learn the decryption algorithms for three polyalphabetic ciphers including the Enigma cipher.,5 Related Work,[0],[0]
"However, this approach needs supervision compared to our approach which uses a pre-trained neural LM.",5 Related Work,[0],[0]
Gomez et al. (2018) (CipherGAN) use a generative adversarial network to learn the mapping between the learned letter embedding distributions in the ciphertext and plaintext.,5 Related Work,[0],[0]
They apply this approach to shift ciphers (including Vigenère ciphers).,5 Related Work,[0],[0]
Their approach cannot be extended to homophonic ciphers and full message neural LMs as in our work.,5 Related Work,[0],[0]
"This paper presents, to our knowledge, the first application of large pre-trained neural LMs to the decipherment problem.",6 Conclusion,[0],[0]
We modify the beam search algorithm for decipherment from Nuhn et al. (2013; 2014) and extend it to use global scoring of the plaintext message using neural LMs.,6 Conclusion,[0],[0]
To enable full plaintext scoring we use the neural LM to sample plaintext characters which reduces the beam size required.,6 Conclusion,[0],[0]
For challenging ciphers such as Beale Pt 2 we obtain lower error rates with smaller beam sizes when compared to the state of the art in decipherment for such ciphers.,6 Conclusion,[0],[0]
We would like to thank the anonymous reviewers for their helpful remarks.,Acknowledgments,[0],[0]
The research was also partially supported by the Natural Sciences and Engineering Research Council of Canada grants NSERC RGPIN-2018-06437 and RGPAS-2018522574 and a Department of National Defence (DND) and NSERC grant DGDND-2018-00025 to the third author.,Acknowledgments,[0],[0]
Decipherment of homophonic substitution ciphers using language models (LMs) is a wellstudied task in NLP.,abstractText,[0],[0]
Previous work in this topic scores short local spans of possible plaintext decipherments using n-gram LMs.,abstractText,[0],[0]
The most widely used technique is the use of beam search with n-gram LMs proposed by Nuhn et al. (2013).,abstractText,[0],[0]
We propose a beam search algorithm that scores the entire candidate plaintext at each step of the decipherment using a neural LM.,abstractText,[0],[0]
We augment beam search with a novel rest cost estimation that exploits the prediction power of a neural LM.,abstractText,[0],[0]
We compare against the state of the art n-gram based methods on many different decipherment tasks.,abstractText,[0],[0]
On challenging ciphers such as the Beale cipher we provide significantly better error rates with much smaller beam sizes.,abstractText,[0],[0]
Decipherment of Substitution Ciphers with Neural Language Models,title,[0],[0]
