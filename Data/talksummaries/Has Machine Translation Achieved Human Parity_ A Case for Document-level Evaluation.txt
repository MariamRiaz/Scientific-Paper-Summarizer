1	21	Some recent results suggest that neural machine translation “approaches the accuracy achieved by average bilingual human translators [on some test sets]” (Wu et al., 2016), or even that its “translation quality is at human parity when compared to professional human translators” (Hassan et al., 2018).
2	79	Claims of human parity in machine translation are certainly extraordinary, and require extraordinary evidence.1 Laudably, Hassan et al. (2018) have released their data publicly to allow external validation of their claims.
3	49	Their claims are further strengthened by the fact that they follow best practices in human machine translation evaluation, using evaluation protocols and tools that are also used at the yearly Conference on Machine Translation (WMT) (Bojar et al., 2017), and take great care in guarding against some confounds such as test set selection and rater inconsistency.
5	45	We perform an independent evaluation of the professional translation and best machine translation system that were found to be of equal quality by Hassan et al. (2018).
6	23	Our main interest lies in the evaluation protocol, and we empirically investigate if the lack of document-level context could explain the inability of human raters to find a quality difference between human and machine translations.
9	33	As such, our evaluation is not a direct replication of that by Hassan et al. (2018), and a failure to reproduce their findings does not imply an error on either our or their part.
12	11	Machine translation is typically evaluated by comparing system outputs to source texts, reference translations, other system outputs, or a combination thereof (for examples, see Bojar et al., 2016a).
13	18	The scientific community concentrates on two aspects: adequacy, typically assessed by bilinguals; and target language fluency, typically assessed by monolinguals.
38	11	Granularity of Measurement We elicit judgements by means of pairwise ranking.
39	13	Raters choose the better (with ties allowed) of two translations for each item: one produced by a professional translator (HUMAN), the other by machine translation (MT).
44	40	Experimental Unit To test the effect of context on perceived translation quality, raters evaluate entire documents as well as single sentences in random order (i. e., context is a within-subjects factor).
45	73	They are shown both translations (HUMAN and MT) for each unit; the source text is only shown in the adequacy condition.
51	11	To this end, we randomly sampled 55 documents and 2×120 sentences from the WMT 2017 test set.
61	32	While the number of ties is similar in sentenceand document-level evaluation, preference for MT drops from 50 to 37 % in the latter (Figure 1a).
62	51	In the fluency condition, raters prefer HUMAN on both the sentence (x= 106, n=172, p<.01) and document level (x=99, n=143, p< .001).
66	10	Our results emphasise the need for suprasentential context in human evaluation of machine translation.
71	19	We hypothesise that document-level evaluation unveils errors such as mistranslation of an ambiguous word, or errors related to textual cohesion and coherence, which remain hard or impossible to spot in a sentence-level evaluation.
75	77	In MT, we find three different translations in the same article: “Twitter Move Car”, “WeChat mobile”, and “WeChat Move”.
77	24	To our surprise, fluency raters show a stronger preference for HUMAN than adequacy raters (Figure 1).
78	11	The main strength of neural machine translation in comparison to previous statistical approaches was found to be increased fluency, while adequacy improvements were less clear (Bojar et al., 2016b; Castilho et al., 2017b), and we expected a similar pattern in our evaluation.
83	10	In response to recent claims of parity between human and machine translation, we have empirically tested the impact of sentence and document level context on human assessment of machine translation.
85	16	We believe that our findings have several implications for machine translation research.
86	11	Most importantly, if we accept our interpretation that human translation is indeed of higher quality in the dataset we tested, this points to a failure of current best practices in machine translation evaluation.
87	13	As machine translation quality improves, translations will become harder to discriminate in terms of quality, and it may be time to shift towards document-level evaluation, which gives raters more context to understand the original text and its translation, and also exposes translation errors related to discourse phenomena which remain invisible in a sentence-level evaluation.
88	58	Our evaluation protocol was designed with the aim of providing maximal validity, which is why we chose to use professional translators and pairwise ranking.
89	34	For future work, it would be of high practical relevance to test whether we can also elicit accurate quality judgements on the document-level via crowdsourcing and direct assessment, or via alternative evaluation protocols.
90	116	The data released by Hassan et al. (2018) could serve as a test bed to this end.
91	58	One reason why document-level evaluation widens the quality gap between machine translation and human translation is that the machine translation system we tested still operates on the sentence level, ignoring wider context.
92	39	It will be interesting to explore to what extent existing and future techniques for document-level machine translation can narrow this gap.
93	24	We ex- pect that this will require further efforts in creating document-level training data, designing appropriate models, and supporting research with discourse-aware automatic metrics.
