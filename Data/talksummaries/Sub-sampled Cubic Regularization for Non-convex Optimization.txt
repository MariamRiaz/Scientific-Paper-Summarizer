0	28	In this paper we address the problem of minimizing an objective function of the form x∗ = arg min x∈Rd [ f(x) := 1 n n∑ i=1 fi(x) ] , (1) where f(x) ∈ C2(Rd,R) is a not necessarily convex, (regularized) loss function over n datapoints.
1	65	Stochastic Gradient Descent (SGD) is a popular method to optimize this type of objective especially in the context of large-scale learning when n is very large.
2	88	Its convergence properties are well understood for convex functions, which arise in many machine learning applications (Nesterov, 2004).
4	17	Yet, non-convex functions are extremely hard to optimize due to the presence of saddle points and local minima which are not global optima (Dauphin et al., 2014; Choromanska et al., 2015).
6	17	Instead of aiming for a global minimizer, we will thus seek for a local optimum of the objective.
13	27	One of the keys for efficiency of these methods is to pick a model that is comparably easy to optimize, such as a quadratic function (Conn et al., 2000).
14	144	Following the trust region paradigm, cubic regularization methods (Nesterov & Polyak, 2006; Cartis et al., 2011a) suggest finding the step sk that minimizes a cubic model of the form mk(sk) := f(xk) + s ᵀ k∇f(xk) + 1 2 sᵀkHksk + σk 3 ‖sk‖3 , (2) where Hk := ∇2f(xk) and σk > 0 1.
15	77	(Nesterov & Polyak, 2006) were able to show that, if the step is computed by globally minimizing the cubic model and if the Hessian Hk is globally Lipschitz continuous, Cubic regularization methods possess the best known worst case complexity to solve Eq.
17	22	2 in an exact manner impedes the performance of this method for machine learning applications as it requires access to the full Hessian matrix.
18	13	More recently, (Cartis et al., 2011a) presented a method (hereafter referred to as ARC) which relaxed this requirement by assuming that one can construct an approximate Hessian Bk that is sufficiently close to Hk in the following way: ‖(Bk −Hk)sk‖ ≤ C ‖sk‖2 , ∀k ≥ 0, C > 0 (3) Furthermore, they showed that it is sufficient to find an approximate minimizer by applying a Lanczos method to build up evolving Krylov spaces, which can be constructed in a Hessian-free manner, i.e. by accessing the Hessians only indirectly via matrix-vector products.
19	46	However there are still two obstacles for the application of ARC in the field of machine learning: (1) The cost of the Lanczos process increases linearly in n and is thus not suitable for large datasets and (2) there is no theoretical guarantee that quasiNewton approaches satisfy Eq.
26	9	• Finally, we provide experimental results demonstrating significant speed-ups compared to standard first and second-order optimization methods for various convex and non-convex objectives.
69	10	We then construct a local cubic model that is (approximately) minimized in each iteration: mk(sk) := f(xk) + s ᵀ kgk + 1 2 sᵀkBksk + σk 3 ‖sk‖3 (4) where gk := 1|Sg| ∑ i∈Sg ∇fi(xk) and Bk := 1|SB | ∑ i∈SB ∇ 2fi(xk).
71	20	Our Sub-sampled Cubic Regularization approach (SCR) is presented in Algorithm 1.
72	46	At iteration step k, we subsample two sets of datapoints from which we compute a stochastic estimate of the gradient and the Hessian.
73	43	We then solve the problem in Eq.
80	25	(9) In order to find a solution we can express s∗k := sk(λ ∗ k) = −(Bk+λ∗kI)−1gk, apply this in the second equation of (9) and obtain a univariate, nonlinear equation in λk∥∥−(Bk + λ∗kI)−1gk∥∥− λ∗kσk = 0.
81	10	(10) Algorithm 1 Sub-sampled Cubic Regularization (SCR) 1: Input: Starting point x0 ∈ Rd (e.g x0 = 0) γ > 1, 1 > η2 > η1 > 0, and σ0 > 0 2: for k = 0, 1, .
82	31	, until convergence do 3: Sample gradient gk and Hessian Hk according to Eq.
85	18	8: end for Furthermore, we need λ∗k ≥ max{−λ1(Bk), 0}, where λ1(Bk) is the leftmost eigenvalue of Bk, in order to guarantee the semi-positive definiteness of (Bk + λ∗kI).
88	13	The problem can be solved by Newton’s method, which involves factorizing Bk +λkI for various λk and is thus prohibitively expensive for large problem dimensions d. See Section 6.2 in (Cartis et al., 2011a) for more details.
90	17	(Cartis et al., 2011a) showed that it is possible to retain the remarkable properties of the cubic regularization algorithm with an inexact model minimizer.
95	31	This comes in handy, as minimizing mk in the Krylov subspace only involves factorizing a tri-diagonal matrix, which can be done at the cost of O(d).
103	26	Second, we show that one can theoretically satisfy these assumptions with high probability by sub-sampling first- and second-order information.
104	166	Third, we give a condensed convergence analysis of SCR which is widely based on (Cartis et al., 2011a), but adapted for the case of stochastic gradients.
106	32	The functions fi ∈ C2(Rd), gi and Hi are Lipschitz continuous for all i, with Lipschitz constants κf , κg and κH respectively.
107	80	By use of the triangle inequality, it follows that these assumptions hold for all g and H, independent of the sample size.
112	11	While quasiNewton approximations satisfy the latter, there is no theoretical guarantee that they also satisfy the former (Cartis et al., 2011a).
116	23	We use this inequality to upper bound the `2-norm distance ‖∇f − g‖, as well as the spectral-norm distance ‖B −H‖ by quantities involving the sample size |S|.
120	62	Let the sub-sampled gradient gk be defined as in Eq.
