25	4	Stein Variational Gradient Descent (SVGD) Let p(x) be a positive differentiable density function on Rd.
27	1	SVGD achieves this by iteratively updating the particles with deterministic transforms of form x` ← x` + φ(x`), ∀` = 1, .
31	1	(1) This result suggests that the decreasing rate of KL divergence when applying transform x′ = x + φ(x) is dominated by Ex∼q[P>x φ(x)] when the step size is small.
32	1	In a special case when q = p, Stein operator draws connection to Stein’s identity, which shows Ex∼q[P>x φ(x)] = 0 when p = q.
60	2	In order to extend the above example, we observe that running SVGD on each marginal distribution pi(xi) independently can be viewed as a special SVGD applied on the joint distribution p(x) = ∏ i pi(xi), but updating each co- ordinate xi using its own kernel ki(x, x′) that only depends on the i-th coordinate, that is, ki(x, x′) := ki(xi, x′i).
67	2	We justify these two choices theoretically in Section 3.2 and Section 3.3, respectively.
79	4	, kd] satisfies D(q || p)2 = d∑ i=1 Ex,x′∼q[PxiPx′iki(x, x ′)].
80	2	Further, Assume both p(x) and q(x) are positive and differentiable densities.
81	5	Denote by Qx the Stein operator of distribution q.
83	3	If all the kernels ki(x, x′) are strictly integrally positive definite in the sense of (6), then D(q || p) = 0 implies q = p.
90	6	Assuming that p(x) is a graphical model in which the Markov neighborhood of node i is Ni and Ci = Ni ∪ {i}, and that ki(x, x′) = ki(xCi , x′Ci), ∀i ∈ [d], then (10) reduces to D(q || p)2 = d∑ i=1 Ex,x′∼q[δi(xCi)ki(xCi , x′Ci)δi(x ′ Ci)], where δi(xCi) = ∇xi log p(xi|xNi) − ∇xi log q(xi|xNi).
97	3	Therefore, graphical SVGD can be viewed as an interesting hybrid of deterministic approximation (via the use of local kernels) and particle approximation (by approximating q with the empirical distributions of the particles).
98	5	We compare our method with a number of baselines, including the vanilla SVGD, particle message passing (PMP), and Langevin dynamics.
100	2	We evaluate the results on three sets of experiments, including a Gaussian MRFs toy example, a sensor localization example, and a crowdsourcing application with realworld datasets.
103	1	Specifically, for graphical SVGD, the kernel we use is ki(x, x′) := exp(−||xCi − x′Ci || 2 2/hi) with bandwidth hi = med 2 i where medi is the median of pairwise distances between {x`Ci} n `=1 for each node xi.
106	12	The model parameters (A, b) are generated first with bi ∼ N (0, 1), and Aij ∼ uniform([−0.1, 0.1]), followed with A← (A+A>)/2 and Aii ← 0.1 + ∑ j 6=i |Aij |.
108	1	We compare our graphical SVGD (denoted by SVGD (graphical)) with a number of baselines, including the typical SVGD (denoted by SVGD (vanilla)), exact Monte Carlo sampling, and Langevin dynamics.
112	1	ii) Figure 2(b) shows the MSE for estimating the second order moment E[x2i ] of each node i, again averaged across the dimensions.
123	2	In Figure 3, we also tested SVGD (combine) whose kernel is ki(x, x′) = αki(xDi , x ′ Di) + (1− α)k(x, x ′), which combines the local kernel ki(xDi , x ′ Di) with a global RBF kernel k(x, x′) (we take α = 0.5).
124	1	Compared with SVGD (graphical+random), SVGD (combine) has the theoretical advantage that ki(x, x′) is strictly integrally positive definite and hence exactly matches the joint distribution p asymptotically.
132	1	Particle message passing (PMP) algorithms have been widely used for approximate inference of continuous graphical models, especially for sensor network location (Ihler et al., 2005; Ihler & McAllester, 2009).
133	1	We compare with two recent versions of PMP methods, including T-PMP (Besse et al., 2014) and D-PMP (Pacheco et al., 2014).
134	2	We use the settings suggested in Pacheco & Sudderth (2015), utilizing a combination of proposals with 75% neighbor-based proposals and 25% Gaussian random walk proposals in the augmentation step.
137	1	Figure 4 reports the contours of 50 particles returned by different approaches on a small sensor network of size d = 9,m = 3, which includes three anchor points put on the corners.
138	1	We observe that SVGD and Monte Carlo-type methods tend to capture multiple modes when the location information is ambiguous, while D-PMP and T-PMP obtain more concentrated posteriors.
147	2	It is interesting to see that Langevin dynamics performs significantly worse because it finds difficulty in converging well in this case (even if we searched the best step size extensively).
162	9	For evaluation, we generate a large set of samples for ground truth by running NUTS for a large number steps with the true task labels as initialization.
163	25	As shown in figure 6, our graphical SVGD again outperforms both the typical SVGD and Langevin dynamics.
164	153	In this paper, we propose a particle-based distributed inference algorithm for approximate inference on continuous graphical models based on Stein variational gradient descent (SVGD).
165	164	Our approach leverages the inherent graphical structures to improve the performance in high dimensions, and also incorporates the key advantages of gradient optimization compared to traditional PMP methods.
