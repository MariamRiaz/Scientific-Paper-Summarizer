{"sections": [{"heading": "1. Introduction", "text": "Kernel methods constitute a powerful paradigm for devising non-parametric modeling techniques for a wide range of problems in machine learning. One of the most elementary is Kernel Ridge Regression (KRR). Given training data (x\n1 , y 1 ), . . . , (x n , y n ) 2 X \u21e5Y , where X \u2713 Rd is an input domain and Y \u2713 R is an output domain, a positive definite kernel function k : X \u21e5 X ! R, and a regularization parameter > 0, the response for a given input x is estimated as:\n\u00aff(x) \u2318 nX\nj=1\nk(x j ,x)\u21b5 j\nwhere \u21b5 = (\u21b5 1 \u00b7 \u00b7 \u00b7\u21b5 n ) T is the solution of the equation\n(K+ I n )\u21b5 = y. (1) *Equal contribution 1School of Mathematical Sciences, Tel Aviv University, Israel 2School of Computer and Communication Sciences, EPFL, Switzerland 3Computer Science and Artificial Intelligence Laboratory, MIT, USA. Correspondence to: Haim Avron <haimav@post.tau.ac.il>, Michael Kapralov <michael.kapralov@epfl.ch>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nIn the above, K 2 Rn\u21e5n is the kernel matrix or Gram matrix defined by K\nij \u2318 k(x i ,x j ) and y \u2318 [y 1 \u00b7 \u00b7 \u00b7 y n ] T is the vector of responses. The KRR estimator can be derived by minimizing a regularized square loss objective function over a hypothesis space defined by the reproducing kernel Hilbert space associated with k(\u00b7, \u00b7); however, the details are not important for this paper.\nWhile simple, KRR is a powerful technique that is well understood statistically and capable of achieving impressive empirical results. Nevertheless, the method has a key weakness: computing the KRR estimator can be prohibitively expensive for large datasets. Solving (1) generally requires \u21e5(n3) time and \u21e5(n2) memory. Thus, the design of scalable methods for KRR (and other kernel based methods) has been the focus of intensive research in recent years (Zhang et al., 2015; Alaoui & Mahoney, 2015; Musco & Musco, 2016; Avron et al., 2016).\nOne of the most popular approaches to scaling up kernel based methods is random Fourier features sampling, originally proposed by Rahimi & Recht (2007). For shiftinvariant kernels (e.g. the Gaussian kernel), Rahimi & Recht (2007) presented a distribution D on functions from X to Cs (s is a parameter) such that for every x, z 2 Rd\nk(x, z) = E '\u21e0D ['(x)\u21e4'(z)] .\nThe idea is to sample ' from D and use \u02dck(x, z) \u2318 '(x)\u21e4'(y) as a surrogate kernel. The resulting approximate KRR estimator can be computed in O(ns2) time and O(ns) memory (see \u00a72.2 for details), giving substantial computational savings if s \u2327 n. This approach naturally raises the question: how large should s be to ensure a high quality estimator? Or, using the exact KRR estimator as a natural baseline: how large should s be for the random Fourier features estimator to be almost as good as the exact KRR estimator? Answering this question can help us determine when random Fourier features can be useful, whether the method needs to be improved, and how to go about improving it.\nThe original random Fourier features analysis (Rahimi & Recht, 2007) bounds the point-wise distance between\nk(\u00b7, \u00b7) and \u02dck(\u00b7, \u00b7) (for other approaches for analyzing random Fourier features, see \u00a72.3). However, the bounds do not naturally lead to an answer to the aforementioned question. In contrast, spectral approximation bounds on the entire kernel matrix, i.e. of the form\n(1 )(K+ I n ) \u02dcK+ I n (1+ )(K+ I n\n) , (2)\nnaturally have statistical and algorithmic implications. Indeed, in \u00a73 we show that when (2) holds we can bound the excess risk introduced by the random Fourier features estimator when compared to the KRR estimator. We also show that \u02dcK+ I\nn can be used as an effective preconditioner for the solution of (1). This motivates the study of how large s should be as a function of for (2) to hold.\nIn this paper we rigorously analyze the relation between the number of random Fourier features and the spectral approximation bound (2). Our main results are the following:\nWe give an upper bound on the number of random features needed to achieve (2) (Theorem 7). This bound, in conjunction with the results in \u00a73, positively shows that random Fourier features can give guarantees for KRR under reasonable assumptions. We give a lower bound showing that our upper bound is tight for the Gaussian kernel (Theorem 8). We show that the upper bound can be improved dramatically by modifying the sampling distribution used in classical random Fourier features (\u00a74). Our sampling distribution is based on an appropriately defined leverage function of the kernel, closely related to so-called leverage scores frequently encountered in the analysis of sampling based methods for linear regression. Unfortunately, it is unclear how to efficiently sample using the leverage function. To address the lack of an efficient way to sample using the leverage function, we propose a novel, easy-tosample distribution for the Gaussian kernel which approximates the true leverage function distribution and allows random Fourier features to achieve a significantly improved upper bound (Theorem 10). The bound has an exponential dependence on the data dimension, so it is only applicable to low dimensional datasets. Nevertheless, it demonstrate that classic random Fourier features can be improved for spectral approximation and motivates further study. As an application, our improved understanding of the leverage function yields a novel asymptotic bound on the statistical dimension of Gaussian kernel matrices over bounded datasets, which may be of independent interest (Corollary 15)."}, {"heading": "2. Preliminaries", "text": ""}, {"heading": "2.1. Setup and Notation", "text": "The complex conjugate of x 2 C is denoted by x\u21e4. For a vector x or a matrix A, x\u21e4 or A\u21e4 denotes the Hermitian transpose. The l \u21e5 l identity matrix is denoted I\nl . We use the convention that vectors are column-vectors.\nA Hermitian matrix A is positive semidefinite (PSD) if x \u21e4 Ax 0 for every vector x. It is positive definite (PD) if\nx \u21e4 Ax > 0 for every vector x 6= 0. For any two Hermitian matrices A and B of the same size, A B means that B A is PSD. We use L\n2 (d\u21e2) = L 2 (Rd, d\u21e2) to denote the space of complex-valued square-integrable functions with respect to some measure \u21e2(\u00b7). L\n2 (d\u21e2) is a Hilbert space equipped with the inner product\nhf, gi L\n2\n(d\u21e2)\n=\nZ\nRd f(\u2318)g(\u2318)\u21e4d\u21e2(\u2318)\n=\nZ\nRd f(\u2318)g(\u2318)\u21e4p \u21e2 (\u2318)d\u2318 .\nIn the above, p \u21e2 (\u00b7) is the density associated with \u21e2(\u00b7). We denote the training set by (x\n1 , y 1 ), . . . , (x n , y n ) 2 X \u21e5 Y \u2713 Rd \u21e5 R. Note that n denotes the number of training examples, and d their dimension. We denote the kernel, which is a function from X \u21e5 X to R, by k. We denote the kernel matrix by K, with K\nij \u2318 k(x i ,x j\n). The associated reproducing kernel Hilbert space (RKHS) is denoted by H\nk , and the associated inner product by (\u00b7, \u00b7)Hk . Some results are stated for the Gaussian kernel k(x, z) = exp( kx zk2\n2 /2 2) for some bandwidth parameter .\nWe use = n to denote the ridge regularization parameter. While for brevity we omit the n subscript, the choice of regularization parameter generally depends on n. Typically,\nn = !(1) and n = o(n). See Caponnetto & De Vito (2007) and Bach (2013) for discussion on the asymptotic behavior of\nn , noting that in our notation, is scaled by an n factor as compared to those works. As the ratio between n and will be an important quantity in our bounds, we denote it as n\n\u2318 n/ . The statistical dimension or effective degrees of freedom is denoted by s (K) \u2318 Tr (K+ I n ) 1 K ."}, {"heading": "2.2. Random Fourier Features", "text": ""}, {"heading": "2.2.1. CLASSICAL RANDOM FOURIER FEATURES", "text": "Random Fourier features (Rahimi & Recht, 2007) is an approach to scaling up kernel methods for shift-invariant kernels. A shift-invariant kernel is a kernel of the form k(x, z) = k(x z) where k(\u00b7) is a positive definite func-\ntion (we abuse notation by using k to denote both the kernel and the defining positive definite function).\nThe underlying observation behind random Fourier features is a simple consequence of Bochner\u2019s Theorem: for every shift-invariant kernel for which k(0) = 1 there is a probability measure \u00b5\nk (\u00b7) and a corresponding probability density function p\nk\n(\u00b7), both on Rd, such that\nk(x, z) =\nZ\nRd e 2\u21e1i\u2318\nT (x z)d\u00b5\nk\n(\u2318)\n=\nZ\nRd e 2\u21e1i\u2318\nT (x z)p\nk\n(\u2318)d\u2318 . (3)\nIn other words, the inverse Fourier transform of the kernel k(\u00b7) is a probability density function, p\nk (\u00b7). For simplicity we typically drop the k subscript, writing \u00b5(\u00b7) = \u00b5\nk (\u00b7) and p(\u00b7) = p\nk (\u00b7), with the associated kernel function clear from context.\nIf \u2318 1 , . . . ,\u2318 s are drawn according to p(\u00b7), and we define '(x) \u2318 1p\ns\n\u21e3 e 2\u21e1i\u2318 T 1 x, \u00b7 \u00b7 \u00b7 , e 2\u21e1i\u2318Ts x \u2318\u21e4\n, then it is not hard to see that\nk(x, z) = E ' ['(x)\u21e4'(z)] .\nThe idea of the Random Fourier features method is then to define\n\u02dck(x, z) \u2318 '(x)\u21e4'(z) = 1 s\nsX\nl=1\ne 2\u21e1i\u2318 T l (x z) (4)\nas a substitute kernel.\nNow suppose that Z 2 Cn\u21e5s is the matrix whose jth row is '(x\nj\n) \u21e4, and let \u02dcK = ZZ\u21e4. \u02dcK is the kernel matrix corresponding to \u02dck(\u00b7, \u00b7). The resulting random Fourier features KRR estimator is \u02dcf(x) \u2318Pn\nj=1\n\u02dck(x j ,x)\u21b5\u0303 j where \u21b5\u0303 is the solution of ( \u02dcK+ I\nn )\u21b5\u0303 = y. Typically, s < n and we can represent \u02dcf(\u00b7) more efficiently as:\n\u02dcf(x) = '(x)\u21e4w\nwhere w = (Z \u21e4 Z+ I\ns\n) 1 Z \u21e4 y\nWe can compute w in O(ns2) time, making random Fourier features computationally attractive if s < n."}, {"heading": "2.2.2. MODIFIED RANDOM FOURIER FEATURES", "text": "While it seems to be a natural choice, there is no fundamental reason that we must sample the frequencies \u2318\n1 , . . . ,\u2318 s\nusing the Fourier transform density function p(\u00b7). In fact, our results show that it is advantageous to use a different sampling distribution based on the kernel leverage function (defined later).\nLet q(\u00b7) be any probability density function whose support includes that of p(\u00b7). If we sample \u2318\n1 , . . . ,\u2318 s using q(\u00b7), and define\n'(x) \u2318 1p s\ns p(\u2318\n1\n)\nq(\u2318 1 )\ne 2\u21e1i\u2318 T 1 x, \u00b7 \u00b7 \u00b7 , s p(\u2318 s )\nq(\u2318 s )\ne 2\u21e1i\u2318 T s x\n!\u21e4\nwe still have k(x, z) = E ' ['(x)\u21e4'(z)]. We refer to this method as modified random Fourier features and remark that it can be viewed as a form of importance sampling."}, {"heading": "2.2.3. ADDITIONAL NOTATIONS AND IDENTITIES", "text": "Now that we have defined (modified) random Fourier features, we can introduce some additional notation and identities that shall prove useful in the rest of the paper.\nThe (j, l) entry of Z is given by\nZ\njl\n= 1p s e 2\u21e1ix\nT j\u2318l p p(\u2318\nl )/q(\u2318 l ). (5)\nLet z : Rd ! Cn be defined by\nz(\u2318) j\n= e 2\u21e1ix T j\u2318 .\nNote that column l of Z from the previous section is exactly z(\u2318\nl\n) p p(\u2318\nl )/[s \u00b7 q(\u2318 l )]. So we have:\nZZ \u21e4 = 1\ns\nsX\nl=1\np(\u2318 l ) q(\u2318 l ) z(\u2318 l )z(\u2318 l ) \u21e4.\nFinally, by (3) we have E [ZZ\u21e4] = K since\nK =\nZ\nRd z(\u2318)z(\u2318)\u21e4d\u00b5(\u2318) =\nZ\nRd z(\u2318)z(\u2318)\u21e4p(\u2318)d\u2318 ."}, {"heading": "2.3. Related Work", "text": "Rahimi & Recht (2007)\u2019s original analysis of random Fourier features bounded the point-wise distance between k(\u00b7, \u00b7) and \u02dck(\u00b7, \u00b7) . In follow-up work, they give learning rate bounds for a broad class of estimators using random Fourier features. However, their results do not apply to classic KRR (Rahimi & Recht, 2008). Their main bound becomes relevant only when the number of sampled features is on order of the training set size.\nRudi et al. (2016) prove generalization properties for KRR with random features, under somewhat difficult to verify technical assumptions, some of which can be seen as constraining the leverage function distribution that we study. They leave open improving their bounds via a more refined sampling approach. Bach (2017) analyzes random Fourier features from a function approximation point of view. He defines a similar leverage function distribution to the one that we consider, but leaves open establishing\nbounds on and effectively sampling from this distribution, both of which we address in this work. Finally, Tropp (2015) analyzes the distance between the kernel matrix and its approximation in terms of the spectral norm, kK \u02dcKk\n2 , which can be a significantly weaker error metric than (2).\nOutside of work on random Fourier features, risk inflation bounds for approximate KRR and leverage score sampling have been used to analyze and improve the Nystro\u0308m method for kernel approximation (Bach, 2013; Alaoui & Mahoney, 2015; Rudi et al., 2015; Musco & Musco, 2016). We apply a number of techniques from this line of work.\nSpectral approximation bounds, such as (2), are quite popular in the sketching literature; see Woodruff (2014). Most closely related to our work is analysis of spectral approximation bounds without regularization (i.e. = 0) for the polynomial kernel (Avron et al., 2014). Improved bounds with regularization (still for the polynomial kernel) were recently proved by Avron et al. (2016)."}, {"heading": "3. Spectral Bounds and Statistical Guarantees", "text": "Given a feature transformation, like random Fourier features, how do we analyze it and relate its use to nonapproximate methods? A common approach, taken for example in the original paper on random Fourier features (Rahimi & Recht, 2007), is to bound the difference between the true kernel k(\u00b7, \u00b7) and the approximate kernel \u02dck(\u00b7, \u00b7). However, it is unclear how such bounds translate to downstream guarantees on statistical learning methods, such as KRR. In this paper we advocate and focus on spectral approximation bounds on the regularized kernel matrix, specifically, bounds of the form\n(1 )(K+ I n ) ZZ\u21e4+ I n (1+ )(K+ I n ) (6)\nfor some < 1. Definition 1. We say that a matrix A is a -spectral approximation of another matrix B, if (1 )B A (1 + )B. Remark 1. When = 0, bounds of the form of (6) can be viewed as a low-distortion subspace embedding bounds. Indeed, when = 0 it follows from (6) that Sp (k(x\n1 , \u00b7), . . . , k(x n , \u00b7)) \u2713 H k\ncan be embedded with -distortion in Sp ('(x\n1 ), . . . ,'(x n )) \u2713 Rs. The main mathematical question we seek to address in this paper is: when using random Fourier features, how large should s be in order to guarantee that ZZ\u21e4 + I\nn is a - spectral approximation of K+ I\nn ? To motivate this question, in the following two subsections we show that such bounds can be used to derive risk inflation bounds for approximate kernel ridge regression. We also show that such bounds can be used to analyze the use of ZZ\u21e4 + I\nn as a preconditioner for K+ I\nn\n.\nWhile this paper focuses on KRR for conciseness, we remark that in the sketching literature, spectral approximation bounds also form the basis for analyzing sketching based methods for tasks like low-rank approximation, kmeans and more. In the kernel setting, such bounds where analyzed, without regularization, for the polynomial kernel (Avron et al., 2014). Cohen et al. (2017) recently showed that (6) along with a trace condition on ZZ\u21e4 (which holds for all sampling approaches we consider) yields a so called \u201cprojection-cost preservation\u201d condition for the kernel approximation. With chosen appropriately, this condition ensures that ZZ\u21e4 can be used in place of K for approximately solving kernel k-means clustering and for certain versions of kernel PCA and kernel CCA. See Musco & Musco (2016) for details, where this analysis is carried out for the Nystro\u0308m method."}, {"heading": "3.1. Risk Bounds", "text": "One way to analyze estimators is via risk bounds; several recent papers on approximate KRR employ such an analysis (Bach, 2013; Alaoui & Mahoney, 2015; Musco & Musco, 2016). In particular, these papers consider the fixed design setting and seek to bound the expected in-sample predication error of the KRR estimator \u00aff , viewing it as an empirical estimate of the statistical risk. More specifically, the underlying assumption is that y\ni\nsatisfies\ny i = f?(x i ) + \u232b i\n(7)\nfor some f? : X ! R. The {\u232b i }\u2019s are i.i.d noise terms, distributed as normal variables with variance 2\n\u232b . The empirical risk of an estimator f , which can be viewed as a measure of the quality of the estimator, is\nR(f) \u2318 E{\u232bi} 2\n4 1 n\nnX\nj=1\n(f(x i ) f?(x i )) 2\n3\n5\n(note that f itself might be a function of {\u232b i }). Let f 2 Rn be the vector whose jth entry is f?(x\nj ). It is quite straightforward to show that for the KRR estimator \u00aff we have (Bach, 2013; Alaoui & Mahoney, 2015):\nR( \u00aff) = n 1 2fT(K+ I n ) 2 f\n+ n 1 2 \u232b Tr K 2 (K+ I n ) 2 .\nSince 2fT(K + I n ) 2 f  fT(K + I n ) 1 f and\nTr K\n2\n(K+ I n ) 2  Tr K(K+ I n ) 1 = s\n(K), we define\nbR K (f) \u2318 n 1 fT(K+ I n ) 1 f + n 1 2\n\u232b\ns (K)\nand note that R( \u00aff)  bR K\n(f). The first term in the above expressions for R( \u00aff) and bR\nK (f) is frequently referred to as the bias term, while the second is the variance term.\nLemma 2. Suppose that (7) holds, and let f 2 Rn be the vector whose jth entry is f?(x\nj ). Let \u00aff be the KRR estimator, and let \u02dcf be KRR estimator obtained using some other kernel \u02dck(\u00b7, \u00b7) whose kernel matrix is \u02dcK. Suppose that \u02dc K + I n is a -spectral approximation to K + I n\nfor some < 1, and that kKk\n2 1. The following bound holds:\nR( \u02dcf)  (1 ) 1 bR K (f) +\n(1 + )\n\u00b7 rank( \u02dc K) n \u00b7 2 \u232b\n(8)\nThe proof appears in the supplementary material (Appendix B).\nIn short, Lemma 2 bounds the risk of the approximate KRR estimator as a function of both the risk upper bound bR K\n(f) (8) and an additive term which is small if the rank of rank( \u02dc\nK) and/or is small. In particular, it is instructive to compare the additive term ( /(1+ ))n 1 2\n\u232b \u00b7rank( \u02dcK) to the variance term n 1 2\n\u232b\n\u00b7 s\n(K). Since approximation \u02dc\nK is only useful computationally if rank( \u02dcK) \u2327 n we should expect the additive term in (8) to also approach 0 an generally be small when n is large.\nRemark 2. An approximation \u02dcK is only useful computationally if rank( \u02dcK) \u2327 n so \u02dcK gives a significantly compressed approximation to the original kernel matrix. Ideally we should have rank( \u02dcK)/n ! 0 as n ! 1 and so the additive term in (8) will also approach 0 and generally be small when n is large."}, {"heading": "3.2. Random Features Preconditioning", "text": "Suppose we choose to solve (K + I n )\u21b5 = y using an iterative method (e.g. CG). In this case, we can apply ZZ\u21e4 + I\nn as a preconditioner. Using standard analysis of Krylov-subspace iterative methods it is immediate that if ZZ\u21e4 + I\nn is a -spectral approximation of K + I\nn then the number of iterations until convergence is O( p (1 + )/(1 ))). Thus, if ZZ\u21e4 + I\nn is, say, a 1/2-spectral approximation of K+ I\nn , then the number of iterations is bounded by a constant. The preconditioner can be efficiently applied (after preprocessing) via the Woodbury formula, giving cost per iteration (if s  n) of O(n2). The overall cost of computing the KRR estimator is therefore O(ns2+n2). Thus, as long as s = o(n) this approach gives an advantage over direct methods which cost O(n3). For small s it also beats non-preconditioned iterative methods cost O(n2 p (K)). We reach again the question that was poised earlier: how big should s be so that ZZ\u21e4 + I n is a 1/2-spectral approximation of K+ I n ?\nSee Cutajar et al. (2016) and Avron et al. (2016) for more details and discussion on random features preconditioning."}, {"heading": "4. Ridge Leverage Function Sampling and Random Fourier Features", "text": "In this section we present upper bounds on the number of random Fourier features needed to guarantee that ZZ \u21e4 + I\nn is a -spectral approximation to K+ I n . Our bounds are applicable to any shift-invariant kernel, and a wide range of feature sampling distributions (and, in particular, for classical random Fourier features).\nOur analysis is based on relating the sampling density to an appropriately defined ridge leverage function. This function is a continuous generalization of the popular leverage scores (Mahoney & Drineas, 2009) and ridge leverage scores (Alaoui & Mahoney, 2015; Cohen et al., 2017) used in the analysis of linear methods. Bach (2017) defined the leverage function of the integral operator given by the kernel function and the data distribution. For our purposes, a more appropriate definition is with respect to a fixed input dataset:\nDefinition 3. For given x 1 , . . . ,x n and shift-invariant kernel k(\u00b7, \u00b7), define the ridge leverage function as\n\u2327 (\u2318) \u2318 p(\u2318)z(\u2318)\u21e4(K+ I) 1z(\u2318) .\nIn the above, K is the kernel matrix and p(\u00b7) is the distribution associated with k(\u00b7, \u00b7). Proposition 4.\np(\u2318)n/(n+ )  \u2327 (\u2318)  p(\u2318)n/ Z\nRd \u2327 (\u2318)d\u2318 = s (K)\nThe (simple) proof of the proposition is given in the supplementary material (Appendix C).\nRecall that we denote the ratio n/ , which appears frequently in our analysis, by n\n= n/ . As discussed, theoretical bounds generally set = !(1) (as a function of n) so n\n= o(n). However we remark that in practice, it may frequently be the case that is very small and n\nn. Corollary 5. For any K, s (K)  n .\nFor any shift-invariant kernel with k(x,x) = 1 and k(x, z) ! 0 as kx zk\n2 ! 1 (e.g., the Gaussian kernel) if we allow points to be arbitrarily spread out, the kernel matrix converges to the identity matrix, and s (I\nn\n) =\nn/(1+ ) = \u2326(n ) if = \u2326(1) so the above bound is tight. However, this requires datasets of increasingly large diameter (as n grows). In contrast, the usual assumption in statistical learning is that the data is sampled from a bounded domain X . In \u00a77.2 we show via a leverage function upper bound that for the important Gaussian kernel, for bounded datasets we have s (K) = o(n ).\nIn the matrix sketching literature it is well known that spectral approximation bounds similar to (6) can be constructed by sampling columns relative to upper bounds on the leverage scores. In the following, we generalize this for the case of sampling Fourier features from a continuous domain. Lemma 6. Let \u2327\u0303 : Rd ! R be a measurable function such that \u2327\u0303(\u2318) \u2327\n(\u2318) for all \u2318 2 Rd, and furthermore assume that\ns \u2327\u0303\n\u2318 Z\nRd \u2327\u0303(\u2318)d\u2318\nis finite. Denote p \u2327\u0303 (\u2318) = \u2327\u0303(\u2318)/s \u2327\u0303 . Let  1/2 and \u21e2 2 (0, 1). Assume that kKk\n2 . Suppose we take s 8\n3\n2s \u2327\u0303 ln(16s (K)/\u21e2) samples \u2318 1 , . . . ,\u2318 s from the distribution associated with the density p\n\u2327\u0303 (\u00b7) and the construct the matrix Z according to (5) with q = p\n\u2327\u0303 . Then ZZ \u21e4 + I\nn is -spectral approximation of K + I n with probability of at least 1 \u21e2. The proof is based on matrix concentration inequalities, and appears in the supplementary material (Appendix D).\nLemma 6 shows that if we could sample using the ridge leverage function, then O(s (K) log(s\n(K))) samples suffice for spectral approximation of K (for a fixed and failure probability). While there is no straightforward way to perform this sampling, we can consider how well the classic random Fourier features sampling distribution approximates the leverage function, obtaining a bound on its performance (the proof is in Appendix D as well): Theorem 7. Let  1/2 and 2 (0, 1). Assume that kKk\n2\n. If we use s 8 3\n2n ln(16s (K)/\u21e2) random Fourier features (i.e., sampled according to p(\u00b7)), then ZZ \u21e4 + I\nn is -spectral approximation of K + I n with probability of at least 1 \u21e2. Theorem 7 establishes that if = !(log(n)) and is fixed, o(n) random Fourier features suffice for spectral approximation, and so the method can provably speed up KRR. Nevertheless, the bound depends on n instead of s\n(K), as is possible with true leverage function sampling (see Lemma 6). This gap arises from our use of the simple, often loose, ridge leverage function upper bound given by Proposition 4.\nUnfortunately, as the next section shows, the bound in Theorem 7 cannot be improved since the classic random Fourier features sampling distribution can be far enough from the ridge leverage distribution that \u2326(n\n) features may be needed even when s (K) = o(n )."}, {"heading": "5. Lower Bound", "text": "Our lower bound shows that the upper bound of Theorem 7 on the number of samples required by classic random Fourier features to obtain a spectral approximation to K+\nI n is essentially best possible. The full proof is given in the supplementary material (Appendix I). Theorem 8. Consider the Gaussian kernel with = (2\u21e1) 1 (so p(\u2318) = 1p\n2\u21e1\ne \u2318 2 /2). Suppose n 17 is an odd integer, satisfies 10\nn <  n 2\n, and R satisfies 3000 log 1.5 (n\n)  R  n 500 p log(n ) . Then, there exists a\ndataset of n points {x j }n j=1 \u2713 [ R,R] such that if s random Fourier features (i.e., sampled according to p(\u00b7)) are used for some s  n\n400 , then with probability at least 1/2, there exists a vector \u21b5 2 Rn such that\n\u21b5T(K+ I n )\u21b5 < 2\n3\n\u21b5T(ZZ\u21e4 + I n )\u21b5. (9)\nFurthermore, for the said dataset we have s (K) = O(R \u00b7 poly (log n )).\nThus, the number of samples s required for ZZ\u21e4 + I n\nto be a 1/2-spectral approximation to K+ I\nn for a bounded dataset of points must either depend exponentially on the radius of the point set, or at least linearly on n\n, and there is an asymptotic gap between what is achieved with classical random Fourier features and what is achieved by modified random Fourier features using leverage function sampling.\nWe note that the above lower bound is proven for a onedimensional point set, which makes it only stronger: even at low dimensions, and for the common Gaussian kernel, there is a large gap between the performance of classic random Fourier features and leverage function sampling.\nThe bound applies for datasets bounded on the range [ R,R] for R = \u2326 log1.5 n . As we will see in \u00a77, the key idea behind the proof is to show that for such a dataset, the ridge leverage function is large on a range of low frequencies. In contrast, the classic random Fourier features distribution is very small at the edges of this frequency range, and so significantly undersamples some frequencies and does not achieve spectral approximation.\nWe remark that it would be preferable if Theorem 8 applied to bounded datasets (i.e. with R fixed), as the usual assumption in statistical learning theory is that data is sampled from a bounded domain. However, our current techniques are unable to address this scenario. Nevertheless, our analysis allows R to grow very slowly with n and we conjecture that the upper bound is tight even for bounded domains."}, {"heading": "6. Improved Sampling (Gaussian Kernel)", "text": "Contrasting with the lower bound of Theorem 8, we now give a modified Fourier feature sampling distribution that does perform well for the Gaussian kernel on bounded input sets. Furthermore, unlike the true ridge leverage function, this distribution is simple and efficient to sample from.\nTo reduce clutter, we state the result for a fixed bandwidth = (2\u21e1) 1. This is without loss of generality since we can rescale the points and adjust the bounding interval.\nOur modified distribution essentially corrects the classic distribution by \u201ccapping\u201d the probability of sampling low frequencies near the origin. This allows it to allocate more samples to higher frequencies, which are undersampled by classical random Fourier features. For simplicity, we focus on the one-dimensional setting. Our results extend to higher dimensions, albeit with an exponential in the dimension loss. Definition 9 (Improved Fourier Feature Distribution for the Gaussian Kernel). Define the function\n\u2327\u0304 R\n(\u2318) \u2318 \u21e2 25max(R, 3000 log1.5 n ) |\u2318|  10plog(n )\np(\u2318)n\no.w.\nLet s \u2327\u0304R = R R \u2327\u0304R(\u2318)d\u2318 and define the probability density function p\u0304 R (\u2318) = \u2327\u0304 R (\u2318)/s \u2327\u0304R .\nNote that p\u0304 R (\u2318) is just the uniform distribution for low frequencies with |\u2318|  10plog(n\n), and the classic Fourier features distribution, appropriately scaled, outside this range. As we show in \u00a77, \u2327\u0304\nR (\u2318) upper bounds the true ridge leverage function \u2327\n(\u2318) for all \u2318. Hence, simply applying Lemma 6: Theorem 10. For any integer n and parameter 0 <  n\n2 , consider the one dimensional Gaussian kernel with = (2\u21e1) 1 (so p(\u2318) = 1p\n2\u21e1\ne \u2318 2 /2) and any dataset of n points {x\nj }n j=1 \u2713 [ R,R] with any radius R > 0. If we sample s 8\n3\n2s \u2327\u0304R ln(16s\u2327\u0304R/\u21e2) random Fourier features ac-\ncording to p\u0304 R (\u00b7) and construct Z according to (5), then with probability at least 1 \u21e2, ZZ\u21e4 + I\nn is -spectral approximation of K+ I\nn for any  1/2 and \u21e2 2 (0, 1). Furthermore, s \u2327\u0304R = O(R p log(n ) + log 2 n ) and p\u0304 R\n(\u00b7) can be sampled from in O(1) time.\nTheorem 10 represents a possibly exponential improvement over the bound obtainable by classic random Fourier features. For R log1.5(n\n) our modified distribution requires O(R p log(n\n)) samples, as compared to the lower bound of n\n400\ngiven by Theorem 8."}, {"heading": "7. Bounding the Ridge Leverage Function", "text": "We conclude by discussing our approach to bounding the ridge leverage function of the Gaussian kernel, which leads to Theorems 8 and 10. The key idea is to reformulate the leverage function as the solution of two dual optimization problems. By exhibiting suitable test functions for these optimization problems, we are able to give both upper and lower bounds on the ridge leverage function, and correspondingly on the sampling performance of classic and modified Fourier feature sampling."}, {"heading": "7.1. Primal-Dual Characterization", "text": "In this section we prove two alternative characterizations of the ridge leverage function: one as a minimization, and the other as a maximization. These characterization are useful for bounding the leverage function, as we exhibit in the next subsection for the Gaussian kernel.\nDefine the operator : L 2 (d\u00b5) ! Cn by\ny \u2318 Z\nRd z(\u21e0)y(\u21e0)d\u00b5(\u21e0). (10)\nThe following two lemmas constitute the main result of this subsection. The proofs can be found in the supplementary material (Appendix E). Lemma 11. The ridge leverage function can alternatively be defined as follows:\n\u2327 (\u2318) = min y2L\n2\n(d\u00b5)\n1k y p p(\u2318)z(\u2318)k2\n2 + kyk2 L\n2\n(d\u00b5)\n(11) Lemma 12. The ridge leverage function can alternatively be defined as follows:\n\u2327 (\u2318) = max \u21b52Cn p(\u2318) \u00b7 |\u21b5\u21e4z(\u2318)|2 k \u21e4\u21b5k2\nL\n2\n(d\u00b5)\n+ k\u21b5k2 2\n(12)\nSimilar results are well known for the finite dimensional case. Here we extend these results to an infinite dimensional case. Lemma 11 allows us to upper bound the leverage function at any point \u2318 2 Rd by exhibiting a carefully constructed function y(\u00b7) and upper bounding the ratio in (11), while Lemma 12 allows us to lower bound it in a similar fashion."}, {"heading": "7.2. Leverage Function: the Gaussian Case", "text": "In this section we prove nearly matching bounds on the leverage score function for the one-dimensional Gaussian kernel on bounded datasets. For simplicity of presentation we focus on the one-dimensional setting. Our results extend to higher dimensions, albeit with an exponential in the dimension loss in the gap between upper and lower bounds.\nOur bounds are parameterized by the width of the point set, which we denote by R. To reduce clutter, we present all results for fixed = (2\u21e1) 1. This is without loss of generality since we can rescale the points. All the proofs appear in the supplementary material (Appendices F\u2013H). Theorem 13. Consider the one dimensional Gaussian kernel with = (2\u21e1) 1. For any integer n and parameter 0 <  n\n2\n, and any radius R > 0, if x 1 , ..., x n 2 [ R,R], for every |\u2318|  10plog n :\n\u2327 (\u2318)  25max(R, 3000 log1.5 n ) .\nTheorem 14. Consider the one dimensional Gaussian kernel with = (2\u21e1) 1. For any integer n 17, any parameter 10\nn   n 16 , and every radius 1000 log1.5 n  R  n\n500 p log(n ) , there exist x 1 , ..., x n 2 [ R,R] such that for every \u2318 2 [ 100plog n ,+100 p log n ] we have:\n\u2327 (\u2318) R 150\n\u2713 p(\u2318)\np(\u2318) + 2Rn 1\n\u25c6 .\nThe last two theorems lead to a tight bound on the statistical dimension matrices corresponding to bounded points sets:\nCorollary 15. Consider the Gaussian kernel with = (2\u21e1) 1. For any integer n and parameter 0 <  n\n2 , and any R > 0, if x\n1 , ..., x n\n2 [ R,R] then we have:\ns (K)  500 \u00b7max(R, 3000 log1.5 n ) p log n + 1\n= O(R p log n + log 2 n )\nFurthermore, if 1000 log1.5 n  R  n 500 p log(n ) there exists a set of points x 1 , . . . , x n \u2713 [ R,R] such that:\ns (K) = \u2326 \u21e3 R p log(n /R) \u2318 .\nThe bounds above match up to constant factors if 1000 log 1.5 n  R  n0.99 . For any 1000 log1.5 n\n R  n\n500 p log(n )\nthey match up to a p log n factor."}, {"heading": "7.3. Theorems 13 and 14: Proof Outline", "text": "Lemma 11 allows us to bound \u2327 (\u2318) simply by exhibiting any y(\u00b7) which makes the cost function small. One simple attempt might be y(s)\n\u2318 (\u21e0) = (\u2318 \u21e0) where (\u00b7) is the Dirac delta function. This choice zeros out the first term. However the delta function is not square integrable, y(s) \u2318 62 L 2\n(d\u00b5), so the lemma cannot be used. Another trivial attempt is y(0)(\u21e0) = 0, which zeros out the second term and recovers the trivial bound \u2327 (\u2318)  p(\u2318)n\n. Nevertheless, a smarter test functions y(\u00b7) can yield improved bounds, yielding results on the leverage score function that are parameterized by the diameter of the point set.\nAt a high level, our approach is to replace the spike function at \u2318 with a \u2018soft spike\u2019 whose Fourier transform still looks approximately like a cosine wave on [ R,R], yet is still square integrable. The smaller R is, the more spread out this function will be able to be, and hence the smaller its `\n2 norm, and the better the leverage score bound. A natural candidate for a \u2018soft spike\u2019 is a Gaussian of appropriate variance, but this choice does not suffice to obtain tight bounds, due to two difficulties. First, for the upper bound a simple Gaussian does not result in a function that is close enough to a pure frequency in time domain (first\nterm of the objective function in Lemma 11) unless we settle for an upper bound of O(R \u00b7 poly (n\n)) as opposed to the tight O(R) on the leverage score density function. Second, the lower bound on the leverage score function resulting from using a Gaussian pulse would only be of the form \u2326(R/ p log n\n), leading to a weak lower bound on the statistical dimension, namely \u2326(R) as opposed to \u2326(R \u00b7 plog n\n), thereby missing entirely the effect of the regularization parameter on the statistical dimension!\nThe remedy to the issues above turns out to be the convolution of a (modulated) Gaussian with a rectangular pulse in time domain (product of a shifted Gaussian with the sinc function in frequency domain). Specifically, our bounds are based on variants of a flattened Gaussian spike function\ny \u2318,b,v\n(\u21e0) \u2318 e (\u21e0 \u2318)2b2/4 \u00b7 v \u00b7 sinc (v(\u21e0 \u2318)) . (13)\nfor some b > 0, v > 0 and \u2318 2 R. It turns out that with a proper setting of parameters (where one should think of b as large, i.e. the spike y is rather narrow) the function y\n\u2318,b,v\nsatisfies\n( y \u2318,b,v )(x) \u21e1 p(\u2318) \u00b7 exp(2\u21e1i\u2318x) R x+ v2 x v\n2\n1p 2\u21e1b e t 2 /2b 2 dt.\nAn illustration of this function in y is given in Fig. 1, (left) and the function y in Fig. 1, (right). Note that if the parameter v is chosen to be large, then for x not too large we have R x+ v 2\nx v 2 1p 2\u21e1b e t 2 /2b 2 dt \u21e1 R +1 1 1p 2\u21e1b e t 2 /2b 2\ndt, i.e. the second multiplier is essentially constant, i.e. flat as a function of x (hence the term \u2018flattened Gaussian spike\u2019). This means that y\n\u2318,b,v is essentially the kernel density evaluated at \u2318 times a pure harmonic term exp(2\u21e1i\u2318x), which is exactly what one needs to minimize the first term on the rhs of (11) in Lemma 11, up to a factor of p p(\u2318) \u2013 see Appendix F. One can also see that setting v to be not too large results in a good function to use in the maximization problem in (12) in Lemma 12 \u2013 see Appendix G. Obtaining tight bounds and in particular achieving the right dependence on p log n\nrequires several modifications to the function y above, but the intuition we just described works!"}, {"heading": "Acknowledgements", "text": "The authors thank Arturs Backurs helpful discussions at early stages of this project. Haim Avron acknowledges the support from the XDATA program of the Defense Advanced Research Projects Agency (DARPA), administered through Air Force Research Laboratory contract FA875012-C-0323 and an IBM Faculty Award. Cameron Musco acknowledges the support by NSF Graduate Research Fellowship, AFOSR grant FA9550-13-1-0042 and the NSF Center for Science of Information."}], "year": 2017, "references": [{"title": "Fast randomized kernel ridge regression with statistical guarantees", "authors": ["Alaoui", "Ahmed El", "Mahoney", "Michael W"], "venue": "In Neural Information Processing Systems (NIPS),", "year": 2015}, {"title": "Subspace embeddings for the polynomial kernel", "authors": ["Avron", "Haim", "Nguyen", "Huy", "Woodruff", "David"], "venue": "In Neural Information Processing Systems (NIPS),", "year": 2014}, {"title": "Faster kernel ridge regression using sketching and preconditioning", "authors": ["Avron", "Haim", "Clarkson", "Kenneth L", "Woodruff", "David P"], "venue": "CoRR, abs/1611.03220,", "year": 2016}, {"title": "On the equivalence between kernel quadrature rules and random feature expansions", "authors": ["Bach", "Francis"], "venue": "Journal of Machine Learning Research,", "year": 2017}, {"title": "Sharp analysis of low-rank kernel matrix approximations", "authors": ["Bach", "Francis R"], "venue": "In Conference on Learning Theory (COLT),", "year": 2013}, {"title": "Optimal rates for the regularized least-squares algorithm", "authors": ["A. Caponnetto", "E. De Vito"], "venue": "Foundations of Computational Mathematics,", "year": 2007}, {"title": "An introduction to probability theory and its applications. Volume 1. Wiley series in probability and mathematical statistics", "authors": ["Feller", "William"], "venue": "http://opac.inria.fr/", "year": 1968}, {"title": "CUR matrix decompositions for improved data analysis", "authors": ["Mahoney", "Michael W", "Drineas", "Petros"], "venue": "Proceedings of the National Academy of Sciences,", "year": 2009}, {"title": "Recursive sampling for the Nystr\u00f6m method", "authors": ["Musco", "Cameron", "Christopher"], "venue": "CoRR, abs/1605.07583,", "year": 2016}, {"title": "An operator pseudo-inversion lemma", "authors": ["Ogawa", "Hidemitsu"], "venue": "SIAM Journal on Applied Mathematics,", "year": 1988}, {"title": "Random features for large-scale kernel machines", "authors": ["A. Rahimi", "B. Recht"], "venue": "In Neural Information Processing Systems (NIPS),", "year": 2007}, {"title": "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning", "authors": ["Rahimi", "Ali", "Recht", "Benjamin"], "venue": "In Neural Information Processing Systems (NIPS),", "year": 2008}, {"title": "Less is more: Nystr\u00f6m computational regularization", "authors": ["Rudi", "Alessandro", "Camoriano", "Raffaello", "Rosasco", "Lorenzo"], "venue": "In Neural Information Processing Systems (NIPS),", "year": 2015}, {"title": "Generalization properties of learning with random features", "authors": ["Rudi", "Alessandro", "Camoriano", "Raffaello", "Rosasco", "Lorenzo"], "venue": "ArXiv e-prints,", "year": 2016}, {"title": "An introduction to matrix concentration inequalities", "authors": ["Tropp", "Joel A"], "venue": "Foundations and Trends in Machine Learning,", "year": 2015}, {"title": "Sketching as a tool for numerical linear algebra", "authors": ["Woodruff", "David P"], "venue": "Found. Trends Theor. Comput. Sci.,", "year": 2014}], "id": "SP:55313fcb875acc9b9e0f8667cb37a2da8fe3b374", "authors": [{"name": "Haim Avron", "affiliations": []}, {"name": "Michael Kapralov", "affiliations": []}, {"name": "Cameron Musco", "affiliations": []}, {"name": "Christopher Musco", "affiliations": []}, {"name": "Ameya Velingker", "affiliations": []}, {"name": "Amir Zandieh", "affiliations": []}], "abstractText": "Random Fourier features is one of the most popular techniques for scaling up kernel methods, such as kernel ridge regression. However, despite impressive empirical results, the statistical properties of random Fourier features are still not well understood. In this paper we take steps toward filling this gap. Specifically, we approach random Fourier features from a spectral matrix approximation point of view, give tight bounds on the number of Fourier features required to achieve a spectral approximation, and show how spectral matrix approximation bounds imply statistical guarantees for kernel ridge regression.", "title": "Random Fourier Features for Kernel Ridge Regression: Approximation Bounds and Statistical Guarantees"}