5	22	Leveraging this source of data seems an obvious solution.
8	22	Significant time and effort is consumed at this stage by language specialists to build grammars that offer a good coverage needed for a first working system.
9	14	Once this first system reaches a certain performance threshold, it can be shared with beta users.
16	10	Machine Translation can be a useful tool for the quick expansion to new languages by automatically translating customer data from existing resources to new languages.
17	13	This could decrease significantly the time needed to develop an NLU system that replies well to customer queries and is robust to new features.
33	6	Next, we detail the experimental setup in section 4, including details on the used NLU and MT systems as well as the monolingual and bilingual corpora used.
37	17	The results of such works depend on the availability of an MT system (general-purpose or in-domain), on the quality of the acquired translations and on the precision of NLU label-word alignment when passing from one language to another.
38	25	Garcı́a et al. (2012) combine multiple online general-purpose translation systems to achieve transferability between French and Spanish for a dialog system.
41	8	Servan et al. (2010) translate the conceptual segments (i.e. NLU labeled) separately to maintain the chunking between source and target language but at the cost of degrading the translation quality.
46	6	A straight-forward method is using human translated data as the true reference and correct MT errors using this ground truth.
52	24	Other methods include measuring the probability of a translated utterance by applying a target language LM, i.e. measuring if a translated utterance is typical, or computing the likelihood that an alignment between the source and the translated utterance is correct, as Klinger and Cimiano (2015) explore for the sentiment analysis task.
55	8	In order to select utterances among possibly er- roneous translation results, the authors use backtranslation results to check whether the translation result maintains the semantic meaning of the original sentence.
73	7	In addition, we extend the approach by 1) determining if the recognised slots are retained in addition to the intent, and 2) making use of the NLU model’s confidence, i.e. we remove utterances retaining the intent, if the confidence of the NLU model is very low (< 0.1 on a scale from 0− 1).
77	8	The score we used is the weighted overall translation score as given by Moses MT toolkit and combining the scores of the translation model, the language model, the reordering score and some word penalty.
81	6	In this work, we evaluated different thresholds like mean of the translation scores, mean+stdev (standard deviation), mean+(0.5*stdev), and mean+(0.25*stdev).
88	18	In particular, we replaced slot values in the translated data using target language catalog entries corresponding to the slot.
91	10	For example, the number of orders can be used to weight albums and population size can be used to weight cities.
92	34	Machine translation systems might incorrectly translate slot values which should not be translated.
93	8	For example, in an utterance ”play we are the champions by queen”, the song title ”we are the champions” and the band name ”queen” should not be translated.
94	12	While we can apply slot resampling to ingest existing slot values into such utterances, we also explore a different approach.
98	15	In the following, we first briefly describe the MT and NLU systems and subsequently the datasets.
101	8	The system was fine-tuned using a small manually created parallel corpus for QA, comprising 4,000 segments, and 424,921 indomain target language segments were used for the target language model.
102	67	Training data were preprocessed, in particular they were converted into spoken form before building the MT system to better match spoken user utterances of an NLU system.
103	15	We used an MT system for a similar task rather than an MT system adapted for our data, because we would expect that suitable in-domain data for adaptation might not yet be available for early bootstrapping, i.e. when target language data have not yet been collected.
105	16	We translated 10M of training data utterances from a US English NLU system.
108	14	NLU labels were kept and aligned during the MT decoding to project them from the English source utterances to the corresponding German translations.
110	13	For testing, we created a dataset collected from German Beta users; German test data were manually transcribed and annotated with intents and slots/named entities.
111	9	The resulting test dataset comprised 119,772 utterances.
117	15	However, one of the domains was not covered by grammars, because it supports very diverse features and requests, which are difficult to capture by a grammar.
120	32	For this, we trained NLU models on MT data, on the in-house data collection, on grammar-generated data as well as on MT data together with each baseline dataset.
