{"sections": [{"text": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2256\u20132262, Austin, Texas, November 1-5, 2016. c\u00a92016 Association for Computational Linguistics"}, {"heading": "1 Introduction", "text": "Coreference resolution systems typically operate by making sequences of local decisions (e.g., adding a coreference link between two mentions). However, most measures of coreference resolution performance do not decompose over local decisions, which means the utility of a particular decision is not known until all other decisions have been made.\nDue to this difficulty, coreference systems are usually trained with loss functions that heuristically define the goodness of a particular coreference decision. These losses contain hyperparameters that are carefully selected to ensure the model performs well according to coreference evaluation metrics. This complicates training, especially across different languages and datasets where systems may work best with different settings of the hyperparameters.\nTo address this, we explore using two variants of reinforcement learning to directly optimize a coreference system for coreference evaluation metrics. In\nparticular, we modify the max-margin coreference objective proposed by Wiseman et al. (2015) by incorporating the reward associated with each coreference decision into the loss\u2019s slack rescaling. We also test the REINFORCE policy gradient algorithm (Williams, 1992).\nOur model is a neural mention-ranking model. Mention-ranking models score pairs of mentions for their likelihood of coreference rather than comparing partial coreference clusters. Hence they operate in a simple setting where coreference decisions are made independently. Although they are less expressive than entity-centric approaches to coreference (e.g., Haghighi and Klein, 2010), mention-ranking models are fast, scalable, and simple to train, causing them to be the dominant approach to coreference in recent years (Durrett and Klein, 2013; Wiseman et al., 2015). Having independent actions is particularly useful when applying reinforcement learning because it means a particular action\u2019s effect on the final reward can be computed efficiently.\nWe evaluate the models on the English and Chinese portions of the CoNLL 2012 Shared Task. The REINFORCE algorithm is competitive with a heuristic loss function while the reward-rescaled objective significantly outperforms both1. We attribute this to reward rescaling being well suited for a ranking task due to its max-margin loss as well as benefiting from directly optimizing for coreference metrics. Error analysis shows that using the reward-rescaling loss results in a similar number of mistakes as the heuristic loss, but the mistakes tend to be less severe.\n1Code and trained models are available at https://github.com/clarkkev/deep-coref.\n2256"}, {"heading": "2 Neural Mention-Ranking Model", "text": "We use the neural mention-ranking model described in Clark and Manning (2016), which we briefly go over in this section. Given a mention m and candidate antecedent c, the mention-ranking model produces a score for the pair s(c,m) indicating their compatibility for coreference with a feedforward neural network. The candidate antecedent may be any mention that occurs before m in the document or NA, indicating that m has no antecedent.\nInput Layer. For each mention, the model extracts various words (e.g., the mention\u2019s head word) and groups of words (e.g., all words in the mention\u2019s sentence) that are fed into the neural network. Each word is represented by a vector wi \u2208 Rdw . Each group of words is represented by the average of the vectors of each word in the group. In addition to the embeddings, a small number of additional features are used, including distance, string matching, and speaker identification features. See Clark and Manning (2016) for the full set of features and an ablation study.\nThese features are concatenated to produce an Idimensional vector h0, the input to the neural network. If c = NA, features defined over pairs of mentions are not included. For this case, we train a separate network with an identical architecture to the pair network except for the input layer to produce anaphoricity scores.\nHidden Layers. The input gets passed through three hidden layers of rectified linear (ReLU) units (Nair and Hinton, 2010). Each unit in a hidden layer is fully connected to the previous layer:\nhi(c,m) = max(0,Wihi\u22121(c,m) + bi)\nwhere W1 is aM1\u00d7I weight matrix, W2 is aM2\u00d7 M1 matrix, and W3 is a M3 \u00d7M2 matrix. Scoring Layer. The final layer is a fully connected layer of size 1:\ns(c,m) = W4h3(c,m) + b4\nwhere W4 is a 1 \u00d7M3 weight matrix. At test time, the mention-ranking model links each mention with its highest scoring candidate antecedent."}, {"heading": "3 Learning Algorithms", "text": "Mention-ranking models are typically trained with heuristic loss functions that are tuned via hyperparameters. These hyperparameters are usually given as costs for different error types, which are used to bias the coreference system towards making more or fewer coreference links. In this section we first describe a heuristic loss function incorporating this idea from Wiseman et al. (2015). We then propose new training procedures based on reinforcement learning that instead directly optimize for coreference evaluation metrics."}, {"heading": "3.1 Heuristic Max-Margin Objective", "text": "The heuristic loss from Wiseman et al. is governed by the following error types, which were first proposed by Durrett et al. (2013).\nSuppose the training set consists of N mentions m1,m2, ...,mN . Let C(mi) denote the set of candidate antecedents of a mention mi (i.e., mentions preceding mi and NA) and T (mi) denote the set of true antecedents of mi (i.e., mentions preceding mi that are coreferent with it or {NA} if mi has no antecedent). Then we define the following costs for linking mi to a candidate antecedent c \u2208 C(mi):\n\u2206h(c,mi) =    \u03b1FN if c = NA \u2227 T (mi) 6= {NA} \u03b1FA if c 6= NA \u2227 T (mi) = {NA} \u03b1WL if c 6= NA \u2227 a /\u2208 T (mi) 0 if a \u2208 T (mi)\nfor \u201cfalse new,\u201d \u201cfalse anaphor,\u201d \u201cwrong link\u201d, and correct coreference decisions.\nThe heuristic loss is a slack-rescaled max-margin objective parameterized by these error costs. Let t\u0302i be the highest scoring true antecedent of mi:\nt\u0302i = argmax c\u2208C(mi)\u2227\u2206h(c,mi)=0 s(c,mi)\nThen the heuristic loss is given as\nL(\u03b8) = N\u2211 i=1 max c\u2208C(mi) \u2206h(c,mi)(1 + s(c,mi)\u2212 s(t\u0302i,mi))\nFinding Effective Error Penalties. We fix \u03b1WL = 1.0 and search for \u03b1FA and \u03b1FN out of {0.1, 0.2, ..., 1.5}with a variant of grid search. Each new trial uses the unexplored set of hyperparame-\nters that has the closest Manhattan distance to the best setting found so far on the dev set. The search is halted when all immediate neighbors (within 0.1 distance) of the best setting have been explored. We found (\u03b1FN, \u03b1FA, \u03b1WL) = (0.8, 0.4, 1.0) to be best for English and (\u03b1FN, \u03b1FA, \u03b1WL) = (0.8, 0.5, 1.0) to be best for Chinese on the CoNLL 2012 data."}, {"heading": "3.2 Reinforcement Learning", "text": "Finding the best hyperparameter settings for the heuristic loss requires training many variants of the model, and at best results in an objective that is correlated with coreference evaluation metrics. To address this, we pose mention ranking in the reinforcement learning framework (Sutton and Barto, 1998) and propose methods that directly optimize the model for coreference metrics.\nWe can view the mention-ranking model as an agent taking a series of actions a1:T = a1, a2, ..., aT , where T is the number of mentions in the current document. Each action ai links the ith mention in the document mi to a candidate antecedent. Formally, we denote the set of actions available for the ith mention as Ai = {(c,mi) : c \u2208 C(mi)}, where an action (c,m) adds a coreference link between mentions m and c. The mentionranking model assigns each action the score s(c,m) and takes the highest-scoring action at each step.\nOnce the agent has executed a sequence of actions, it observes a reward R(a1:T ), which can be any function. We use the B3 coreference metric for this reward (Bagga and Baldwin, 1998). Although our system evaluation also includes the MUC (Vilain et al., 1995) and CEAF\u03c64 (Luo, 2005) metrics, we do not incorporate them into the loss because MUC has the flaw of treating all errors equally and CEAF\u03c64 is slow to compute.\nReward Rescaling. Crucially, the actions taken by a mention-ranking model are independent. This means it is possible to change any action ai to a different one a\u2032i \u2208 Ai and see what reward the model would have gotten by taking that action instead: R(a1, ..., ai\u22121, a\u2032i, ai+1, ..., aT ). We use this idea to improve the slack-rescaling parameter \u2206 in the maxmargin loss L(\u03b8). Instead of setting its value based on the error type, we compute exactly how much\neach action hurts the final reward:\n\u2206r(c,mi) = \u2212R(a1, ..., (c,mi), ..., aT ) + max a\u2032i\u2208Ai R(a1, ..., a \u2032 i, ..., aT )\nwhere a1:T is the highest scoring sequence of actions according to the model\u2019s current parameters. Otherwise the model is trained in the same way as with the heuristic loss.\nThe REINFORCE Algorithm. We also explore using the REINFORCE policy gradient algorithm (Williams, 1992). We can define a probability distribution over actions using the mention-ranking model\u2019s scoring function as follows:\np\u03b8(a) \u221d es(c,m)\nfor any action a = (c,m). The REINFORCE algorithm seeks to maximize the expected reward\nJ(\u03b8) = E[a1:T\u223cp\u03b8]R(a1:T )\nIt does this through gradient ascent. Computing the full gradient is prohibitive because of the expectation over all possible action sequences, which is exponential in the length of the sequence. Instead, it gets an unbiased estimate of the gradient by sampling a sequence of actions a1:T according to p\u03b8 and computing the gradient only over the sample.\nWe take advantage of the independence of actions by using the following gradient estimate, which has lower variance than the standard REINFORCE gradient estimate.\n\u2207\u03b8 J(\u03b8) \u2248 T\u2211 i=1 \u2211 a\u2032i\u2208Ai [\u2207\u03b8 p\u03b8(a\u2032i)](R(a1, ..., a\u2032i, ..., aT )\u2212 bi)\nwhere bi is a baseline used to reduce the variance, which we set to Ea\u2032i\u2208Ai\u223cp\u03b8 R(a1, ..., a \u2032 i, ..., aT )."}, {"heading": "4 Experiments and Results", "text": "We run experiments on the English and Chinese portions of the CoNLL 2012 Shared Task data (Pradhan et al., 2012) and evaluate with the MUC, B3, and CEAF\u03c64 metrics. Our experiments were run using predicted mentions from Stanford\u2019s rule-based coreference system (Raghunathan et al., 2010).\nWe follow the training methodology from Clark and Manning (2016): hidden layers of sizes M1 = 1000, M2 = M3 = 500, the RMSprop optimizer\n(Hinton and Tieleman, 2012), dropout (Hinton et al., 2012) with a rate of 0.5, and pretraining with the all pairs classification and top pairs classification tasks. However, we improve on the previous system by using using better mention detection, more effective hyperparameters, and more epochs of training."}, {"heading": "4.1 Results", "text": "We compare the heuristic loss, REINFORCE, and reward rescaling approaches on both datasets. We find that REINFORCE does slightly better than the heuristic loss, but reward rescaling performs significantly better than both across both languages.\nWe attribute the modest improvement of REINFORCE to it being poorly suited for a ranking task. During training it optimizes the model\u2019s performance in expectation, but at test-time it takes the most probable sequence of actions. This mismatch occurs even at the level of an individual decision: the model only links the current mention to a single antecedent, but is trained to assign high probability to all correct antecedents. We believe the benefit of REINFORCE being guided by coreference evaluation metrics is offset by this disadvantage, which does not occur in the max-margin approaches. The reward-rescaled max-margin loss combines the best of both worlds, resulting in superior performance."}, {"heading": "4.2 The Benefits of Reinforcement Learning", "text": "In this section we examine the reward-based cost function \u2206r and perform error analysis to determine\nhow reward rescaling improves the mention-ranking model\u2019s accuracy.\nComparison with Heuristic Costs. We compare the reward-based cost function \u2206r with the error types used in the heuristic loss. For English, the average value of \u2206r is 0.79 for FN errors and 0.38 for FA errors when the costs are scaled so the average value of a WL error is 1.0. These are very close to the hyperparameter values (\u03b1FN, \u03b1FA, \u03b1WL) = (0.8, 0.4, 1.0) found by grid search. However, there is a high variance in costs for each error type, suggesting that using a fixed penalty for each type as in the heuristic loss is insufficient (see Figure 1).\nAvoiding Costly Mistakes. Embedding the costs of actions into the loss function causes the rewardrescaling model to prioritize getting the more important coreference decisions (i.e., the ones with the biggest impact on the final score) correct. As a\nresult, it makes fewer costly mistakes at test time. Costly mistakes often involve large clusters of mentions: incorrectly combining two coreference clusters of size ten is much worse than incorrectly combining two clusters of size one. However, the cost of an action also depends on other factors like the number of errors already present in the clusters and the utilities of the other available actions.\nTable 2 shows the breakdown of errors made by the heuristic and reward-rescaling models on the test set. The reward-rescaling model makes slightly more errors, meaning its improvement in performance must come from its errors being less severe.\nExample Improvements. Table 3 shows two classes of mentions where the reward-rescaling loss particularly improves over the heuristic loss.\nProper nouns have a higher average cost for \u201cfalse new\u201d errors (0.90) than other mentions types (0.77). This is perhaps because proper nouns are important for connecting clusters of mentions far apart in a document, so incorrectly linking a proper noun to NA could result in a large decrease in recall. Because it more heavily weights these high-cost errors during training, the reward-rescaling model makes fewer \u201cfalse new\u201d errors for proper nouns than the heuristic loss. Although there is an increase in other kinds of errors as a result, most of these are low-cost \u201cfalse anaphoric\u201d errors.\nThe pronouns in the \u201ctelephone conversation\u201d genre often group into extremely large coreference clusters, which means a \u201cwrong link\u201d error can have a very large negative effect on the score. This is reflected in its high average cost of 1.21. After prior-\nitizing these examples during training, the rewardrescaling model creates significantly fewer wrong links than the heuristic loss, which is trained using a fixed cost of 1.0 for all wrong links."}, {"heading": "5 Related Work", "text": "Mention-ranking models have been widely used for coreference resolution (Denis and Baldridge, 2007; Rahman and Ng, 2009; Durrett and Klein, 2013). These models are typically trained with heuristic loss functions that assign costs to different error types, as in the heuristic loss we describe in Section 3.1 (Fernandes et al., 2012; Durrett et al., 2013; Bjo\u0308rkelund and Kuhn, 2014; Wiseman et al., 2015; Martschat and Strube, 2015; Wiseman et al., 2016).\nTo the best of our knowledge reinforcement learning has not been applied to coreference resolution before. However, imitation learning algorithms such as SEARN (Daume\u0301 III et al., 2009) have been used to train coreference resolvers (Daume\u0301 III, 2006; Ma et al., 2014; Clark and Manning, 2015). These algorithms also directly optimize for coreference evaluation metrics, but they require an expert policy to learn from instead of relying on rewards alone."}, {"heading": "6 Conclusion", "text": "We propose using reinforcement learning to directly optimize mention-ranking models for coreference evaluation metrics, obviating the need for hyperparameters that must be carefully selected for each particular language, dataset, and evaluation metric. Our reward-rescaling approach also increases the model\u2019s accuracy, resulting in significant gains over the current state-of-the-art."}, {"heading": "Acknowledgments", "text": "We thank Kelvin Guu, William Hamilton, Will Monroe, and the anonymous reviewers for their thoughtful comments and suggestions. This work was supported by NSF Award IIS-1514268."}], "year": 2016, "references": [{"title": "Algorithms for scoring coreference chains", "authors": ["Amit Bagga", "Breck Baldwin."], "venue": "The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference, pages 563\u2013566.", "year": 1998}, {"title": "Learning structured perceptrons for coreference resolution with latent antecedents and non-local features", "authors": ["Anders Bj\u00f6rkelund", "Jonas Kuhn."], "venue": "Association of Computational Linguistics (ACL).", "year": 2014}, {"title": "Entitycentric coreference resolution with model stacking", "authors": ["Kevin Clark", "Christopher D. Manning."], "venue": "Association for Computational Linguistics (ACL).", "year": 2015}, {"title": "Improving coreference resolution with entity-level distributed representations", "authors": ["Kevin Clark", "Christopher D. Manning."], "venue": "Association for Computational Linguistics (ACL).", "year": 2016}, {"title": "Search-based structured prediction", "authors": ["Hal Daum\u00e9 III", "John Langford", "Daniel Marcu."], "venue": "Machine Learning, 75(3):297\u2013325.", "year": 2009}, {"title": "Practical structured learning techniques for natural language processing", "authors": ["Hal Daum\u00e9 III."], "venue": "Ph.D. thesis, University of Southern California, Los Angeles, CA.", "year": 2006}, {"title": "A ranking approach to pronoun resolution", "authors": ["Pascal Denis", "Jason Baldridge."], "venue": "International Joint Conferences on Artificial Intelligence (IJCAI), pages 1588\u20131593.", "year": 2007}, {"title": "Easy victories and uphill battles in coreference resolution", "authors": ["Greg Durrett", "Dan Klein."], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1971\u20131982.", "year": 2013}, {"title": "Decentralized entity-level modeling for coreference resolution", "authors": ["Greg Durrett", "David Leo Wright Hall", "Dan Klein."], "venue": "Association for Computational Linguistics (ACL), pages 114\u2013124.", "year": 2013}, {"title": "Latent structure perceptron with feature induction for unrestricted coreference resolution", "authors": ["Eraldo Rezende Fernandes", "C\u0131\u0301cero Nogueira Dos Santos", "Ruy Luiz Milidi\u00fa"], "venue": "In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing", "year": 2012}, {"title": "Coreference resolution in a modular, entity-centered model", "authors": ["Aria Haghighi", "Dan Klein."], "venue": "Human Language Technology and North American Association for Computational Linguistics (HLT-NAACL), pages 385\u2013393.", "year": 2010}, {"title": "On coreference resolution performance metrics", "authors": ["Xiaoqiang Luo."], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 25\u201332.", "year": 2005}, {"title": "Prune-and-score: Learning for greedy coreference resolution", "authors": ["Chao Ma", "Janardhan Rao Doppa", "J Walker Orr", "Prashanth Mannem", "Xiaoli Fern", "Tom Dietterich", "Prasad Tadepalli."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "year": 2014}, {"title": "Latent structures for coreference resolution", "authors": ["Sebastian Martschat", "Michael Strube."], "venue": "Transactions of the Association for Computational Linguistics (TACL), 3:405\u2013418.", "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "authors": ["Vinod Nair", "Geoffrey E. Hinton."], "venue": "International Conference on Machine Learning (ICML), pages 807\u2013814.", "year": 2010}, {"title": "Conll-2012 shared task: Modeling multilingual unrestricted coreference in ontonotes", "authors": ["Sameer Pradhan", "Alessandro Moschitti", "Nianwen Xue", "Olga Uryupina", "Yuchen Zhang."], "venue": "Proceedings of the Joint Conference on Empirical Methods in Natural Language", "year": 2012}, {"title": "A multipass sieve for coreference resolution", "authors": ["Karthik Raghunathan", "Heeyoung Lee", "Sudarshan Rangarajan", "Nathanael Chambers", "Mihai Surdeanu", "Dan Jurafsky", "Christopher Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP),", "year": 2010}, {"title": "Supervised models for coreference resolution", "authors": ["Altaf Rahman", "Vincent Ng."], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 968\u2013 977.", "year": 2009}, {"title": "Reinforcement learning: An introduction", "authors": ["Richard S Sutton", "Andrew G Barto."], "venue": "MIT Press.", "year": 1998}, {"title": "A modeltheoretic coreference scoring scheme", "authors": ["Marc Vilain", "John Burger", "John Aberdeen", "Dennis Connolly", "Lynette Hirschman."], "venue": "Proceedings of the 6th conference on Message understanding, pages 45\u201352.", "year": 1995}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "authors": ["Ronald J Williams."], "venue": "Machine learning, 8(3-4):229\u2013256.", "year": 1992}, {"title": "Learning anaphoricity and antecedent ranking features for coreference resolution", "authors": ["Sam Wiseman", "Alexander M Rush", "Stuart M Shieber", "Jason Weston."], "venue": "Association of Computational Linguistics (ACL), pages 92\u2013100.", "year": 2015}, {"title": "Learning global features", "authors": ["Sam Wiseman", "Alexander M Rush", "Stuart M Shieber", "Jason Weston"], "year": 2016}], "id": "SP:baa0d76d4c6460e1fe0d42d1dae9d92e92a5a421", "authors": [{"name": "Kevin Clark", "affiliations": []}, {"name": "Christopher D. Manning", "affiliations": []}], "abstractText": "Coreference resolution systems are typically trained with heuristic loss functions that require careful tuning. In this paper we instead apply reinforcement learning to directly optimize a neural mention-ranking model for coreference evaluation metrics. We experiment with two approaches: the REINFORCE policy gradient algorithm and a rewardrescaled max-margin objective. We find the latter to be more effective, resulting in a significant improvement over the current stateof-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task.", "title": "Deep Reinforcement Learning for Mention-Ranking Coreference Models"}