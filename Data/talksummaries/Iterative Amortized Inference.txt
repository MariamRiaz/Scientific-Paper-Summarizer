8	15	To combat this problem, our work offers a departure from previous approaches by re-examining inference from an optimization perspective.
15	54	Latent variable models are generative probabilistic models that use local (per data example) latent variables, z, to model observations, x, using global (across data examples) parameters, θ.
19	31	Variational inference reformulates this intractable integration as an optimization problem by introducing an approximate posterior1, q(z|x), typically chosen from some tractable family of distributions, and minimizing the KLdivergence from the posterior, DKL(q(z|x)||p(z|x)).
21	16	Instead, KL-divergence can be decomposed into DKL(q(z|x)||p(z|x)) = log pθ(x)− L, (1) where L is the evidence lower bound (ELBO), which is defined as: L ≡ Ez∼q(z|x) [log pθ(x, z)− log q(z|x)] (2) = Ez∼q(z|x) [log pθ(x|z)]−DKL(q(z|x)||pθ(z)).
28	7	The optimization procedures for variational inference and learning are respectively the expectation and maximization steps of the variational EM algorithm (Dempster et al., 1977; Neal & Hinton, 1998), which alternate until convergence.
31	14	L. With a factorized Gaussian density over continuous latent variables, i.e. λ(i) = {µ(i)q ,σ2(i)q } and q(z(i)|x(i)) = N (z(i);µ(i)q ,diagσ2(i)q ), conventional optimization techniques repeatedly estimate the stochastic gradients ∇λL to optimize L w.r.t.
32	12	λ(i), e.g.: λ(i) ← λ(i) + α∇λL(x(i),λ(i); θ), (4) where α is the step size.
35	8	Rather, inference models are often used to map observations to approximate posterior estimates.
36	19	Optimization of each data example’s approximate posterior parameters, λ(i), is replaced with the optimization of a shared, i.e. amortized (Gershman & Goodman, 2014), set of parameters, φ, contained within an inference model, f , of the form: λ(i) ← f(x(i);φ).
37	10	(5) While inference models have a long history, e.g. (Dayan et al., 1995), the most notable recent example is the variational auto-encoder (VAE) (Kingma & Welling, 2014; Rezende et al., 2014), which employs the reparameterization trick to propagate stochastic gradients from the generative model to the inference model, both of which are parameterized by neural networks.
47	8	Of course, generative models can adapt to accommodate sub-optimal approximate posteriors.
49	41	We demonstrate this concept in Figure 1 by visualizing the optimization surface of L defined by a 2-D latent Gaussian model and a particular binarized MNIST (LeCun et al., 1998) data example.
52	15	The inference model is unable to achieve the optimum, but manages to output a reasonable estimate in one pass.
53	21	Gradient ascent requires many iterations and is sensitive to step-size, but through the iterative estimation procedure, ultimately arrives at a better final estimate.
56	28	While offering significant benefits in computational efficiency, standard inference models can suffer from sizable amortization gaps (Krishnan et al., 2018).
57	13	Parameterizing inference models as direct, static mappings from x to q(z|x) may be overly restrictive, widening this gap.
58	9	To improve upon this direct encoding paradigm, we pose the following question: can we retain the computational efficiency of inference models while incorporating more powerful iterative estimation capabilities?
59	43	Our proposed solution is a new class of inference models, capable of learning how to update approximate posterior estimates by encoding gradients or errors.
60	23	Due to the iterative nature of these models, we refer to them as iterative inference models.
61	76	Through an analysis with latent Gaussian models, we show that iterative inference models generalize standard inference models (Section 4.3) and offer theoretical justification for top-down inference in hierarchical models (Section 4.1).
62	52	Our approach relates to learning to learn (Andrychowicz et al., 2016), where an optimizer model learns to optimize the parameters of an optimizee model.
63	26	The optimizer receives the optimizee’s parameter gradients and outputs updates to these parameters to improve the optimizee’s loss.
64	22	The optimizer itself can be learned due to the differentiable computation graph.
65	70	Such models can adaptively adjust step sizes, potentially outperforming conventional optimizers.
66	24	For inference optimization, previous works have combined standard inference models with gradient updates (Hjelm et al., 2016; Krishnan et al., 2018; Kim et al., 2018), however, these works do not learn to iteratively optimize.
67	20	(Putzky & Welling, 2017) use recurrent inference models for MAP estimation of denoised images in linear models.
69	14	Our work extends techniques for learning to optimize along several novel directions, discussed in Section 4.
70	15	We denote an iterative inference model as f with parameters φ.
73	67	4, as well as the residual, nonlinear update used in (Andrychowicz et al., 2016).
74	22	Figure 2 displays a computation graph of the inference procedure, and Algorithm 1 in Appendix B describes the procedure in detail.
