0	14	Event Detection (ED) is an important subtask of event extraction.
1	10	It extracts event triggers from individual sentences and further identifies the type of the corresponding events.
2	17	For instance, according to the ACE-2005 annotation guideline, in the sentence “Jane and John are married”, an ED system should be able to identify the word “married” as a trigger of the event “Marry”.
7	11	However, document-level information is also important for ED, because the sentences in the same document, although they may contain different types of events, are often correlated with respect to the theme of the document.
11	14	However, if we can capture the contextual information of this sentence, it is more confident for us to label “leave” as the trigger of an “End-Position” event.
12	13	Upon such observation, there have been some feature-based studies (Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012) that construct rules to capture document-level information for improving sentence-level ED.
14	14	First, the features used therein often need to be manually designed and may involve error propagation due to natural language processing; Second, they discover inter-event information at document level by constructing inference rules, which is time-consuming and is hard to make the rule set as complete as possible.
15	11	Besides, a representation-based study has been presented in (Duan et al., 2017), which employs the PV-DM model to train document embeddings and further uses it in a RNN-based event classifier.
16	10	However, as being limited by the unsupervised training process, the document-level representation cannot specifically capture event-related information.
17	14	In this paper, we propose a novel Document Embedding Enhanced Bi-RNN model, called DEEB-RNN, for ED at sentence level.
18	79	This model first learns ED oriented embeddings of documents through a hierarchical and supervised attention based bidirectional RNN, which pays word-level attention to event triggers and sentence-level attention to those sentences containing events.
19	23	It then uses the learned document embeddings to facilitate another bidirectional RNN model to identify event triggers and their types in individual sentences.
32	13	Given a document with L sentences, DEEB-RNN learns its embedding for detecting events in all sentences.
33	11	Word-level embeddings Given a sentence si (i = 1, 2, ..., L) consisting of words {wit|t = 1, 2, ..., T}.
35	19	(1) We then feed hit to a perceptron with no bias to get uit = tanh(Wwhit) as a hidden representation of hit and also obtain an attention weight αit = u T itcw, which should be normalized through a softmax function.
38	14	(2) To pay more attention to trigger words than other words, we construct the gold word-level attention signalsα∗i for the sentence si, as illustrated in Figure 2a.
39	19	We can then take the square error as the general loss of the attention at word level to supervise the learning process: Ew(α ∗,α) = L∑ i=1 T∑ t=1 (α∗it − αit)2.
40	15	(3) Sentence-level embeddings Given the sentence embeddings {si|i = 1, 2, ..., L}, we first get the hidden state qi via a Bi-GRU: qi = [ −−−→ GRUs(si), ←−−− GRUs(si)].
44	23	(5) We also think that the sentences containing event should obtain more attention than other ones.
45	28	Therefore, similar to the case at word level, we construct the gold sentence-level attention signals β∗ for the document d, as illustrated in Figure 2b, and further take the square error as the general loss of the attention at sentence level to supervise the learning process: Es(β ∗,β) = L∑ i=1 (β∗i − βi)2.
47	15	Specifically, given a sentence sj (j = 1, 2, ..., L) in document d, for each of its word wjt (t = 1, 2, ..., T ), we concatenate its word embeddingwjt and entity type embedding ejt with the corresponding document embedding d as the input rjt of the Bi-GRU and thus obtain the hidden state fjt: fjt = [ −−−→ GRUe(rjt), ←−−− GRUe(rjt)].
49	14	The loss function, J(y,o), can thus be defined in terms of the cross-entropy error of the real event type yjt and the predicted probability o(k)jt as follows: J(y,o) = − L∑ j=1 T∑ t=1 K∑ k=1 I(yjt = k)log o (k) jt , (8) where I(·) is the indicator function.
50	41	In the DEEB-RNN model, the above two modules are jointly trained.
53	29	In the experiments, the validation set has 30 documents from different genres, the test set has 40 documents and the training set contains the remaining 529 documents.
56	13	We set the dimension of the hidden layers corresponding to GRUw, GRUs, and GRUe to 300, 200, and 300, respectively, the output size of Ww and Ws to 600 and 400, respectively, the dimension of entity type embeddings to 50, the batch size to 25, the dropout rate to 0.5.
60	17	In order to validate the proposed DEEB-RNN model through experimental comparison, we choose the following typical models as the baselines.
85	25	We can see that different versions of DEEB-RNN consistently out- perform the existing state-of-the-art methods in terms of both recall and F1-measure, while their precision is comparable to that of others.
87	33	In this study, we proposed a hierarchical and supervised attention based and document embedding enhanced Bi-RNN method, called DEEB-RNN, for event detection.
88	29	We explored different strategies to construct gold word- and sentence-level attentions to focus on event information.
89	11	Experiments on the ACE-2005 dataset demonstrate that DEEB-RNN achieves better performance as compared to the state-of-the-art methods in terms of both recall and F1-measure.
