1	71	Typically, this involves two steps: First, Frame Identification (FrameId), capturing the context around a predicate (frame evoking element) and assigning a frame, basically a word sense label for a prototypical situation, to it.
5	58	The main challenge and source of prediction errors of FrameId systems are ambiguous predicates, which can evoke several frames, e.g., the verb sit evokes the frame Change posture in a context like ‘a person is sitting back on a bench’, while it evokes Being located when ‘a company is sitting in a city’.
6	20	Understanding the predicate context, and thereby the context of the situation (here, ‘Who / what is sitting where?’), is crucial to identifying the correct frame for ambiguous cases.
7	26	State-of-the-art FrameId systems model the situational context using pretrained distributed word embeddings (see Hermann et al., 2014).
8	21	Hence, it is assumed that the context of the situation is explicitly expressed in words.
10	13	Such implicit common sense knowledge is obvious enough to be rarely expressed in sentences, but is more likely to be present in images.
16	17	We analyze whether multimodal representations grounded in images can encode common sense knowledge to improve FrameId.
68	23	Same as SimpleFrameId, our system is based on pretrained embeddings to build the input representation out of the predicate context and the predicate itself.
69	14	However, different to SimpleFrameId, our representation of the predicate context is multimodal: beyond textual embeddings we also use IMAGINED and visual embeddings.
76	13	Optionally, filtering based on the lexicon can be performed on the predicted probabilities for each frame label.
79	27	These are: first, the most-frequent-sense baseline using the data majority (Data Baseline) to determine the most frequent frame for a predicate; second, the baseline introduced by Hartmann et al. (2017) using a lexicon (Lexicon Baseline) to consider the data counts of the Data Baseline only for those frames available for a predicate.
80	15	We propose to combine them into a Data-Lexicon Baseline, which uses the lexicon for unambiguous predicates and for ambiguous ones it uses the data majority.
88	25	): we apply the pretrained VGG-m128 Convolutional Neural Network model (Chatfield et al., 2014) to images for synsets from ImageNet (Deng et al., 2009), we extract the 128- dimensional activation of the last layer (before the softmax) and then we L2-normalize it.
99	30	We use the IMAGINED method (Collell et al., 2017) for learning a mapping function: it maps from the word embedding space to the visual embedding space given those words that occur in both pretrained embedding spaces (7220 for English and 7739 for German).
110	23	The mapping can be used to facilitate the frame identification for a predicate in a sentence, e.g., a sentence in the fulltext corpus.
160	14	Our new strong Data-Lexicon Baseline reaches a considerable accuracy of 86.32 %, which is hard to beat by trained models.
170	15	In conclusion, even our unimodal approach outperforms prior state of the art in terms of accuracy.
175	60	In conclusion, the multimodal approach has a slight overall advan- tage and, interestingly, has a considerable advantage over the unimodal one when confronted with a more difficult setting of not using the lexicon.
177	13	The difference in F1-macro between the majority baselines and our system is smaller than for the English FrameNet.
178	25	This indicates that the majorities learned from data are more powerful in the German case with SALSA than in the English case, when comparing against our system.
180	18	We report results achieved without the lexicon to evaluate independently of its quality (Hartmann et al., 2017).
183	13	For German data, the increase of F1-macro with lexicon versus without is small (one point).
184	26	Many indicators point to our approach not just learning the data majority: our trained models have better F1-macro and especially much higher ambiguous F1-macro scores with lexicon.
193	32	Interestingly, replacing visual synset embeddings with linguistic synset embeddings (AutoExtend by Rothe and Schütze (2015), see Sec.
194	35	4) in further investigations also showed that visual embeddings yield better performance.
195	81	This points out the potential for incorporating even more image evidence to extend our approach.
199	21	In SALSA, a smaller portion of sentences has at least one synset embedding, see Table 2.
200	36	For further investigations, we reduced the dataset to only sentences actually containing a synset embedding.
201	26	Then, minor improvements of the multimodal approach were visible for SALSA.
202	57	This points out that a dataset containing more words linking to implicit knowledge in images (visual synset embeddings) can profit more from visual and IMAGINED embeddings.
