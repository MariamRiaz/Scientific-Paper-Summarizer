11	17	To the best of our knowledge, the only existing DAG transducer for NLG is the one proposed by Quernheim and Knight (2012).
12	52	Quernheim and Knight introduced a DAG-to-tree transducer that can be applied to AMR-to-text generation.
13	20	This transducer is designed to handle hierarchical structures with limited reentrencies, and it is unsuitable for meaning graphs transformed from type-logical semantics.
16	35	The design for string and tree transducers (Comon et al., 1997) focuses on not only the logic of the computation for a new data structure, but also the corresponding control flow.
23	20	This idea can be extended to other types of linguistic structures, e.g. syntactic trees or semantic representations of another language.
24	65	We conduct experiments on richly detailed semantic annotations licensed by English Resource Grammar (ERG; Flickinger, 2000).
25	23	We introduce a principled method to derive transduction rules from DeepBank (Flickinger et al., 2012).
43	33	• Q is a finite set of states.
47	20	A transition t getsm states on the incoming edges of a node and puts n states on the outgoing edges.
50	24	The weight of a run ρ (denoted as δ′(ρ)) is the product of all weights of local transitions: δ′(ρ) = ⊗ v∈V δ ( ρ(in(v)) ℓ(v)−−→ ρ(out(v)) ) Here, for a function f , we use f({a1, · · · , an}) to represent {f(a1), · · · , f(an)}.
51	40	If K is a boolean semiring, the automata fall backs to an unweighted 2 proper q and named are node labels, while BV is the edge label.
60	20	First, it lacks the ability to reverse the direction of edges during transduction because it is difficult to keep acyclicy anymore if edge reversing is allowed.
71	20	Instead, our transducer obtains target structures based on side effects of DAG recognition.
76	19	The syntax in the BNF format of our declarative programming language, denoted as Lc, for string calculation is: ⟨program⟩ ::= ⟨statement⟩∗ ⟨statement⟩ ::= ⟨variable⟩ = ⟨expr⟩ ⟨expr⟩ ::= ⟨variable⟩ | ⟨string⟩ | ⟨expr⟩ + ⟨expr⟩ Here a string is a sequence of characters selected from an alphabet (denoted as Σout) and can be empty (denoted as ϵ).
79	32	For every statement, the left hand side is a variable and the right hand side is a sequence of string literals and variables that are combined through ‘+’.
99	21	Variable vl(j,d) represents the jth (1 ≤ j ≤ n) variable related to state q.
106	18	The red dashed edges in Figure 3 make up an intermediate graph T (ρ), which is a subgraph of D if edge direction is not taken into account.
141	17	To unify a general framework for DAG transduction-based NLG, we propose a two-step strategy to achive meaning-to-text transformation.
171	17	The induced rules are directly obtained by following three steps: Finding intermediate tree T EDS graphs are highly regular semantic graphs.
238	20	The fourth column shows the fraction of graphs in the test data set that can reach output sentences.
240	52	As we can conclude from Table 2, using only induced rules achieves the highest accuracy but the coverage is not satisfactory.
245	17	The lemma sequences generated by our transducer are really close to the golden one.
246	115	This means that our model actually works and most reordering patterns are handled well by induced rules.
249	29	The baseline converts a DAG into a concept sequence by a pre-order DFS traversal on the intermediate tree of this DAG.
250	18	Then we use a sequenceto-sequence model to transform this concept sequence to the lemma sequence for comparison.
251	106	This is a kind of implementation of Konstas et al.’s model but evaluated on the EDS data.
252	48	We can see that on this task, our transducer is much better than a pure sequence-to-sequence model on DeepBank data.
253	17	Table 3 shows the efficiency of the beam search decoder with a beam size of 128.
254	31	The platform for this experiment is x86 64 GNU/Linux with two Intel Xeon E5-2620 CPUs.
257	31	Since the data for experiments is newswire data, i.e. WSJ sentences from PTB (Marcus et al., 1993), the input graphs are quite large on average.
