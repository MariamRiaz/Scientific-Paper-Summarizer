{"sections": [{"heading": "1. Introduction", "text": "Let the input space H be a separable Hilbert space with inner product denoted by \u3008\u00b7, \u00b7\u3009H , and the output space R. Let \u03c1 be an unknown probability measure on H \u00d7R. In this paper, we study the following expected risk minimization,\ninf \u03c9\u2208H E\u0303(\u03c9), E\u0303(\u03c9) = \u222b H\u00d7R (\u3008\u03c9, x\u3009H \u2212 y)2d\u03c1(x, y), (1)\nwhere the measure \u03c1 is known only through a sample z = {zi = (xi, yi)}ni=1 of size n \u2208 N, independently and identically distributed (i.i.d.) according to \u03c1. The above regression setting covers nonparametric regression over a reproducing kernel Hilbert space (Cucker & Zhou, 2007; Steinwart & Christmann, 2008), and it is close\n1 Laboratory for Information and Inference Systems, E\u0301cole Polytechnique Fe\u0301de\u0301rale de Lausanne, Lausanne, Switzerland. Correspondence to: Junhong Lin <junhong.lin@epfl.ch>, Volkan Cevher <volkan.cevher@epfl.ch>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nto functional regression (Ramsay, 2006) and linear inverse problems (Engl et al., 1996). A basic algorithm for the problem is ridge regression, and its generalization, spectralregularized algorithm. Such algorithms can be viewed as solving an empirical, linear equation with the empirical covariance operator replaced by a regularized one, see (Caponnetto & Yao, 2006; Bauer et al., 2007; Gerfo et al., 2008; Lin et al., 2018) and references therein. Here, the regularization is used to control the complexity of the solution to against over-fitting and to achieve best generalization ability. The function/estimator generated by classic regularized algorithm is in the subspace span{x} of H , where x = {x1, \u00b7 \u00b7 \u00b7 , xn}. More often, the search of an estimator for some specific algorithms is restricted to a different (and possibly smaller) subspace S, which leads to regularized algorithms with projection. Such approaches have computational advantages in nonparametric regression with kernel methods (Williams & Seeger, 2000; Smola & Scho\u0308lkopf, 2000). Typically, with a subsample/sketch dimension m < n, S = span{x\u0303j : 1 \u2264 j \u2264 m} where x\u0303j is chosen randomly from the input set x, or S = span{ \u2211m j=1Gijxj : 1 \u2264 i \u2264 m} where G = [Gij ]1\u2264i\u2264m,1\u2264j\u2264n is a general randomized matrix whose rows are drawn according to a distribution. The resulted algorithms are called Nystro\u0308m regularized algorithm and sketched-regularized algorithm, respectively. Our starting points of this paper are recent papers (Bach, 2013; Alaoui & Mahoney, 2015; Yang et al., 2015; Rudi et al., 2015; Myleiko et al., 2017) where convergence results on Nystro\u0308m/sketched regularized algorithms for learning with kernel methods are given. Particularly, within the fixed design setting, i.e., the input set x are deterministic while the output set y = {y1, \u00b7 \u00b7 \u00b7 , yn} treated randomly, convergence results have been derived, in (Bach, 2013; Alaoui & Mahoney, 2015) for Nystro\u0308m ridge regression and in (Yang et al., 2015) for sketched ridge regression. Within the random design setting (which is more meaningful (Hsu et al., 2014) in statistical learning theory) and involving a regularity/smoothness condition on the target function (Smale & Zhou, 2007), optimal statistical results on generalization error bounds (excess risks) have been obtained in (Rudi et al., 2015) for Nystro\u0308m ridge regression. The latter results were further generalized in (Myleiko et al., 2017) to a general\nNystro\u0308m regularized algorithm. Although results have been developed for sketched ridge regression in the fixed design setting, it is still unclear if one can get statistical results for a general sketched-regularized algorithms in the random design setting. Besides, all the derived results, either for sketched or Nystro\u0308m regularized algorithms, are only for the attainable case, i.e., the case that the expected risk minimization (1) has at least one solution in H . Moreover, they saturate (Bauer et al., 2007) at a critical value, meaning that they can not lead to better convergence rates even with a smoother target function. Motivated by these, in this paper, we study statistical results of projected-regularized algorithms for least-squares regression over a separable Hilbert space within the random design setting. We first extend the analysis in (Lin et al., 2018) for classicregularized algorithms to projected-regularized algorithms, and prove statistical results with respect to a broader class of norms. We then show that optimal rates can be retained for sketched-regularized algorithms, provided that the sketch dimension is proportional to the effective dimension (Zhang, 2005) up to a logarithmic factor. As a byproduct, we obtain similar results for Nystro\u0308m regularized algorithms. Interestingly, our results are the first ones with optimal, distribution-dependent rates that do not have any saturation effect for sketched/Nystro\u0308m regularized algorithms, considering both the attainable and non-attainable cases. In our proof, we naturally integrate proof techniques from (Smale & Zhou, 2007; Caponnetto & De Vito, 2007; Rudi et al., 2015; Myleiko et al., 2017; Lin et al., 2018). Our novelties lie in a new estimates on the projection error for sketched-regularized algorithms, a novel analysis to conquer the saturation effect, and a refined analysis for Nystro\u0308m regularized algorithms, see Section 4 for details. The rest of the paper is organized as follows. Section 2 introduces some auxiliary notations and projected-regularized algorithms. Section 3 present assumptions and our main results, followed with simple discussions. Finally, Section 4 gives the proofs of our main results."}, {"heading": "2. Learning with Projected-regularized Algorithms", "text": "In this section, we introduce some notations as well as auxiliary operators, and present projected-regularized algorithms."}, {"heading": "2.1. Notations and Auxiliary Operators", "text": "Let Z = H \u00d7R, \u03c1X(\u00b7) the induced marginal measure on H of \u03c1, and \u03c1(\u00b7|x) the conditional probability measure on R with respect to x \u2208 H and \u03c1. For simplicity, we assume that the support of \u03c1X is compact and that there exists a constant \u03ba \u2208 [1,\u221e[, such that\n\u3008x, x\u2032\u3009H \u2264 \u03ba2, \u2200x, x\u2032 \u2208 H, \u03c1X -almost every. (2)\nDefine the hypothesis space H\u03c1 = {f : H \u2192 R|\u2203\u03c9 \u2208 H with f(x) = \u3008\u03c9, x\u3009H , \u03c1X -almost surely}. Denote L2\u03c1X the Hilbert space of square integral functions from H to R with respect to \u03c1X , with its norm given by \u2016f\u2016\u03c1 =(\u222b H |f(x)|2d\u03c1X ) 1 2 . For a given bounded operator L : L2\u03c1X \u2192 H, \u2016L\u2016 denotes the operator norm of L, i.e., \u2016L\u2016 = supf\u2208L2\u03c1X ,\u2016f\u2016\u03c1=1\n\u2016Lf\u2016H . Let r \u2208 N+, the set {1, \u00b7 \u00b7 \u00b7 , r} is denoted by [r]. For any real number a, a+ = max(a, 0), a\u2212 = min(0, a). Let S\u03c1 : H \u2192 L2\u03c1X be the linear map \u03c9 \u2192 \u3008\u03c9, \u00b7\u3009H , which is bounded by \u03ba under Assumption (2). Furthermore, we consider the adjoint operator S\u2217\u03c1 : L2\u03c1X \u2192 H , the covariance operator T : H \u2192 H given by T = S\u2217\u03c1S\u03c1, and the integeral operator L : L2\u03c1X \u2192 L 2 \u03c1X given by S\u03c1S \u2217 \u03c1 . It\ncan be easily proved that S\u2217\u03c1g = \u222b H xg(x)d\u03c1X(x), Lf =\u222b\nH f(x)\u3008x, \u00b7\u3009Hd\u03c1X(x) and T = \u222b H \u3008\u00b7, x\u3009Hxd\u03c1X(x). Under Assumption (2), the operators T and L can be proved to be positive trace class operators (and hence compact):\n\u2016L\u2016 = \u2016T \u2016 \u2264 tr(T ) = \u222b H tr(x\u2297 x)d\u03c1X(x)\n= \u222b H \u2016x\u20162Hd\u03c1X(x) \u2264 \u03ba2. (3)\nFor any \u03c9 \u2208 H , it is easy to prove the following isometry property (Bauer et al., 2007),\n\u2016S\u03c1\u03c9\u2016\u03c1 = \u2016 \u221a T \u03c9\u2016H . (4)\nMoreover, according to the singular value decomposition of a compact operator, one can prove that\n\u2016L\u2212 12S\u03c1\u03c9\u2016\u03c1 \u2264 \u2016\u03c9\u2016H . (5)\nWe define the sampling operator Sx : H \u2192 Rn by (Sx\u03c9)i = \u3008\u03c9, xi\u3009H , i \u2208 [n], where the norm \u2016 \u00b7 \u2016Rn in Rn is the Euclidean norm times 1/ \u221a n. Its adjoint operator S\u2217x : Rn \u2192 H, defined by \u3008S\u2217xy, \u03c9\u3009H = \u3008y,Sx\u03c9\u3009Rn for y \u2208 Rn is thus given by S\u2217xy = 1n \u2211n i=1 yixi. Moreover, we can define the empirical covariance operator Tx : H \u2192 H such that Tx = S\u2217xSx. Obviously, Tx = 1n \u2211n i=1\u3008\u00b7, xi\u3009Hxi. By Assumption (2), similar to (3), we have\n\u2016Tx\u2016 \u2264 tr(Tx) \u2264 \u03ba2. (6)\nIt is easy to see that Problem (1) is equivalent to\ninf f\u2208H\u03c1 E(f), E(f) = \u222b H\u00d7R (f(x)\u2212 y)2d\u03c1(x, y), (7)\nThe function that minimizes the expected risk over all measurable functions is the regression function (Cucker & Zhou, 2007; Steinwart & Christmann, 2008), defined as,\nf\u03c1(x) = \u222b R yd\u03c1(y|x), x \u2208 H, \u03c1X -almost every. (8)\nA simple calculation shows that the following well-known fact holds (Cucker & Zhou, 2007; Steinwart & Christmann, 2008), for all f \u2208 L2\u03c1X , E(f)\u2212 E(f\u03c1) = \u2016f \u2212 f\u03c1\u2016 2 \u03c1. Then it is easy to see that (7) is equivalent to inff\u2208H\u03c1 \u2016f \u2212 f\u03c1\u20162\u03c1. Under Assumption (2), H\u03c1 is a subspace of L2\u03c1X . Using the projection theorem, one can prove that a solution fH for the problem (7) is the projection of the regression function f\u03c1 onto the closure of H\u03c1 in L2\u03c1X , and moreover, for all f \u2208 H\u03c1 (Lin & Rosasco, 2017),\nS\u2217\u03c1f\u03c1 = S\u2217\u03c1fH , (9)\nand E(f)\u2212 E(fH) = \u2016f \u2212 fH\u20162\u03c1. (10)\nNote that fH does not necessarily be in H\u03c1, as indicated by a simple example constructed in the appendix. Throughput this paper, S is a closed, finite-dimensional subspace of H , and P is the projection operator onto S."}, {"heading": "2.2. Projected-regularized Algorithms", "text": "In this subsection, we demonstrate and introduce projectedregularized algorithms. The expected risk E\u0303(\u03c9) in (1) can not be computed exactly. It can be only approximated through the empirical risk E\u0303z(\u03c9), E\u0303z(\u03c9) = 1n \u2211n i=1(\u3008\u03c9, xi\u3009H \u2212 yi)2. A first idea to deal with the problem is to replace the objective function in (1) with the empirical risk. Moreover, we restrict the solution to the subspace S. This leads to the projected empirical risk minimization, inf\u03c9\u2208S E\u0303z(\u03c9). Using P 2 = P, a simple calculation shows that a solution for the above is given by \u03c9\u0302 = P\u03b1\u0302, with \u03b1\u0302 satisfying PTxP\u03b1\u0302 = PS\u2217xy. Motivated by the classic (iterated) ridge regression, we replace PTxP with a regularized one, and thus leads to the following projected (iterated) ridge regression.\nAlgorithm 1. The projected (iterated) ridge regression algorithm of order \u03c4 over the samples z and the subspace S is given by fz\u03bb = S\u03c1\u03c9z\u03bb, where 1\n\u03c9z\u03bb = PG\u03bb(PTxP )PS\u2217xy, G\u03bb(u) = \u03c4\u2211 i=1 \u03bbi\u22121(\u03bb+ u)\u2212i.\n(11)\nRemark 1. 1) In this paper, we focus on projected ridge regression, but all the derived results hold for a general projected-regularized algorithm, in which G\u03bb is a general filter function. Given \u039b \u2282 R+, a class of functions {G\u03bb : [0, \u03ba2] \u2192 [0,\u221e[, \u03bb \u2208 \u039b} are called filter functions with qualification \u03c4 (\u03c4 \u2265 1) if there exist some positive constants\n1Let L be a self-adjoint, compact operator over a separable Hilbert space H . G\u03bb(L) is an operator on L defined by spectral calculus: suppose that {(\u03c3i, \u03c8i)}i is a set of normalized eigenpairs of L with the eigenfunctions {\u03c8i}i forming an orthonormal basis of H , then G\u03bb(L) = \u2211 i G\u03bb(\u03c3i)\u03c8i \u2297 \u03c8i.\nE,F <\u221e such that\nsup \u03bb\u2208\u039b sup u\u2208]0,\u03ba2]\n|G\u03bb(u)(u+ \u03bb)| \u2264 E. (12)\nand\nsup \u03b1\u2208[0,\u03c4 ] sup \u03bb\u2208\u039b sup u\u2208]0,\u03ba2]\n|1\u2212 G\u03bb(u)u|(u+ \u03bb)\u03b1\u03bb\u2212\u03b1 \u2264 F. (13)\n2) A simple calculation shows that\nG\u03bb(u) = 1\u2212 q\u03c4\nu =\n\u2211\u03c4\u22121 i=0 q i\nu+ \u03bb , q =\n\u03bb\n\u03bb+ u . (14)\nThus, G\u03bb(u) is a filter function with qualification \u03c4 , E = \u03c4 and F = 1. When \u03c4 = 1, it is a filter function for classic ridge regression and the algorithm is projected ridge regression. 3) Another typical filter function studied in the literature is G\u03bb(u) = u\u221211{u\u2265\u03bb}, which corresponds to principal component (spectral cut-off) regularization. Here, 1{\u00b7} denotes the indication function. In this case, E = 2, F = 2\u03c4 and \u03c4 could be any positive number.\nIn the above, \u03bb is a regularization parameter which needs to be well chosen in order to achieve best performance. Throughout this paper, we assume that 1/n \u2264 \u03bb \u2264 1. The performance of an estimator fz\u03bb can be measured in terms of excess risk (generalization error), E(fz\u03bb) \u2212 infH\u03c1 E = E\u0303(\u03c9z\u03bb)\u2212infH E\u0303 ,which is exactly \u2016fz\u03bb\u2212fH\u20162\u03c1 according to (10). Assuming that fH \u2208 H\u03c1, i.e., fH = S\u03c1\u03c9\u2217 for some \u03c9\u2217 \u2208 H (in this case, the solution with minimalHnorm for fH = S\u03c1\u03c9 is denoted by \u03c9H ), it can be measured in terms of H-norm, \u2016\u03c9z\u03bb \u2212 \u03c9H\u2016H , which is closely related to \u2016L\u2212 12S\u03c1(\u03c9z\u03bb\u2212\u03c9H)\u2016H = \u2016L\u2212 1 2 (fz\u03bb \u2212 fH)\u2016\u03c1, according to (5). In what follows, we will measure the performance of an estimator fz\u03bb in terms of a broader class of norms, \u2016L\u2212a(fz\u03bb \u2212 fH)\u2016\u03c1, where a \u2208 [0, 12 ] is such that L\n\u2212afH is well defined. But one should keep in mind that all the derived results also hold if we replace \u2016L\u2212a(fz\u03bb \u2212 fH)\u2016\u03c1 with \u2016T 12\u2212a(\u03c9z\u03bb \u2212 \u03c9H)\u2016H in the attainable case, i.e., fH \u2208 H\u03c1. We will report these results in a longer version of this paper. Convergence with respect to different norms has its strong backgrounds in convex optimization, inverse problems, and statistical learning theory. Particularly, convergence with respect to target function values and H-norm has been studied in convex optimization. Interestingly, convergence in H-norm can imply convergence in target function values (although the derived rate is not optimal), while the opposite is not true."}, {"heading": "3. Convergence Results", "text": "In this section, we first introduce some basic assumptions and then present convergence results for projectedregularized algorithms. Finally, we give results for sketched/Nystro\u0308m regularized algorithms."}, {"heading": "3.1. Assumptions", "text": "In this subsection, we introduce three standard assumptions made in statistical learning theory (Steinwart & Christmann, 2008; Cucker & Zhou, 2007; Lin et al., 2018). The first assumption relates to a moment condition on the output value y.\nAssumption 1. There exist positive constants Q and M such that for all l \u2265 2 with l \u2208 N,\u222b\nR |y|ld\u03c1(y|x) \u2264 1 2 l!M l\u22122Q2, (15)\n\u03c1X -almost surely.\nTypically, the above assumption is satisfied if y is bounded almost surely, or if y = \u3008\u03c9\u2217, x\u3009H + , where is a Gaussian random variable with zero mean and it is independent from x. Condition (15) implies that the regression function is bounded almost surely, using the Cauchy-Schwarz inequality. The next assumption relates to the regularity/smoothness of the target function fH .\nAssumption 2. fH satisfies\u222b H (fH(x)\u2212 f\u03c1(x))2x\u2297 xd\u03c1X(x) B2T , (16)\nand the following Ho\u0308lder source condition\nfH = L\u03b6g0, with \u2016g0\u2016\u03c1 \u2264 R. (17)\nHere, B,R, \u03b6 are non-negative numbers.\nCondition (16) is trivially satisfied if fH \u2212 f\u03c1 is bounded almost surely. Moreover, when making a consistency assumption, i.e., infH\u03c1 E = E(f\u03c1), as that in (Smale & Zhou, 2007; Caponnetto, 2006; Caponnetto & De Vito, 2007; Steinwart et al., 2009), for kernel-based non-parametric regression, it is satisfied with B = 0. Condition (17) characterizes the regularity of the target function fH (Smale & Zhou, 2007). A bigger \u03b6 corresponds to a higher regularity and a stronger assumption, and it can lead to a faster convergence rate. Particularly, when \u03b6 \u2265 1/2, fH \u2208 H\u03c1 (Steinwart & Christmann, 2008). This means that the expected risk minimization (1) has at least one solution inH , which is referred to as the attainable case. Finally, the last assumption relates to the capacity of the space H (H\u03c1).\nAssumption 3. For some \u03b3 \u2208 [0, 1] and c\u03b3 > 0, T satisfies\ntr(T (T + \u03bbI)\u22121) \u2264 c\u03b3\u03bb\u2212\u03b3 , for all \u03bb > 0. (18)\nThe left hand-side of (18) is called degrees of freedom (Zhang, 2005), or effective dimension (Caponnetto & De Vito, 2007). Assumption 3 is always true for \u03b3 = 1\nand c\u03b3 = \u03ba2, since T is a trace class operator. This is referred to as the capacity independent setting. Assumption 3 with \u03b3 \u2208 [0, 1] allows to derive better rates. It is satisfied, e.g., if the eigenvalues of T satisfy a polynomial decaying condition \u03c3i \u223c i\u22121/\u03b3 , or with \u03b3 = 0 if T is finite rank."}, {"heading": "3.2. Results for Projected-regularized Algorithms", "text": "We are now ready to state our first result as follows. Throughout this paper, C denotes a positive constant that depends only on \u03ba2, c\u03b3 , \u03b3, \u03b6 B,M,Q,R, \u03c4 and \u2016T \u2016, and it could be different at its each appearance. Moreover, we write a1 . a2 to mean a1 \u2264 Ca2. Theorem 1. Under Assumptions 1, 2 and 3, let \u03bb = n\u03b8\u22121 for some \u03b8 \u2208 [0, 1], \u03c4 \u2265 \u03b6, and a \u2208 [0, 12 \u2227 \u03b6]. Then the following holds with probability at least 1\u2212 \u03b4 (0 < \u03b4 < 1). 1) If \u03b6 \u2208 [0, 1],\n\u2016L\u2212a(fz\u03bb \u2212 fH)\u2016\u03c1 . \u03bb\u2212a log 2 3 \u03b4 t1\u2212a\u03b8,n \u00d7(\n\u03bb\u03b6 + 1\u221a n\u03bb\u03b3\n+ \u03bb\u03b6\u22121 ( \u22065 + \u2206 1\u2212a 5 \u03bb\na ))\n. (19)"}, {"heading": "2) If \u03b6 \u2265 1 and \u03bb \u2265 n\u22121/2,", "text": "\u2016L\u2212a(fz\u03bb \u2212 fH)\u2016\u03c1 . \u03bb\u2212a log 2 3\n\u03b4 \u00d7(\n\u03bb\u03b6 + 1\u221a n\u03bb\u03b3 + (\u22065 + \u03bb\u2206 (\u03b6\u22121)\u22271 5 + \u2206 1\u2212a 5 \u03bb a)\n) . (20)\nHere, \u22065 is the projection error \u2016(I \u2212 P )T 1 2 \u20162 and\nt\u03b8,n = [1 \u2228 (\u03b8\u22121 \u2227 log n\u03b3)]. (21)\nThe above result provides high-probability error bounds with respect to variants of norms for projected-regularized algorithms. The upper bound consists of three terms. The first term depends on the regularity parameter \u03b6, and it arises from estimating bias. The second term depends on the sample size, and it arises from estimating variance. The third term depends on the projection error. Note that there is a trade-off between the bias and variance terms. Ignoring the projection error, solving this trade-off leads to the best choice on \u03bb and the following results.\nCorollary 2. Under the assumptions and notations of Theorem 1, let \u03bb = n\u2212 1 1\u2228(2\u03b6+\u03b3) . Then the following holds with probability at least 1\u2212 \u03b4. 1) If 2\u03b6 + \u03b3 \u2264 1,\n\u2016L\u2212a(fz\u03bb \u2212 fH)\u2016\u03c1\n. n\u2212(\u03b6\u2212a) ( 1 + (\u03b3 log n)1\u2212a)(1 + \u03bb\u22121\u22065 ) log2 3\n\u03b4 .\n(22)\n2) If \u03b6 \u2208 [0, 1] and 2\u03b6 + \u03b3 > 1,\n\u2016L\u2212a(fz\u03bb \u2212 fH)\u2016\u03c1 . n \u2212 \u03b6\u2212a2\u03b6+\u03b3 ( 1 + \u03bb\u22121\u22065 ) log2 3\n\u03b4 .\n(23)\n3) If \u03b6 \u2265 1,\n\u2016L\u2212a(fz\u03bb \u2212 fH)\u2016\u03c1 . \u03bb\u2212a log 2 3\n\u03b4 \u00d7(\nn\u2212 \u03b6\n2\u03b6+\u03b3 + \u22065\n( 1 + ( \u03bb \u22065 ) \u2206 (\u03b6\u22121)\u22271 5 + ( \u03bb \u22065 )a)) .\n(24)\nComparing the derived upper bound for projectedregularized algorithms with that for classic regularized algorithms in (Lin et al., 2018), we see that the former has an extra term, which is caused by projection. The above result asserts that projected-regularized algorithms perform similarly as classic regularized algorithms if the projection operator is well chosen such that the projection error is small enough. In the special case that P = I , we get the follow result.\nCorollary 3. Under the assumptions and notations of Theorem 1, let \u03bb = n\u2212 1 1\u2228(2\u03b6+\u03b3) and P = I . Then with probability at least 1\u2212 \u03b4,\n\u2016L\u2212a(fz\u03bb \u2212 fH)\u2016\u03c1\n. log2 3\n\u03b4\n{ n\u2212(\u03b6\u2212a) ( 1 + (\u03b3 log n)1\u2212a ) , if 2\u03b6 + \u03b3 \u2264 1,\nn\u2212 \u03b6\u2212a 2\u03b6+\u03b3 , if 2\u03b6 + \u03b3 > 1.\n(25)\nThe above result recovers the result derived in (Lin et al., 2018). The convergence rates are optimal as they match the mini-max rates with \u03b6 \u2265 1/2 derived in (Caponnetto & De Vito, 2007; Blanchard & Mucke, 2016)."}, {"heading": "3.3. Results for Sketched-regularized Algorithms", "text": "In this subsection, we state results for sketched-regularized algorithms. In sketched-regularized algorithms, the range of the projection operator P is the subspace range{S\u2217xG\u2217}, where G \u2208 Cm\u00d7n is a sketch matrix whose rows are i.i.d drawn according to a distribution F . In this paper, we assume the distribution F satisfies the following two properties.\n\u2022 Isotropy property: We say that F obeys the isotropy property if\nE[aa\u2217] = I, a \u223c F. (26)\n\u2022 Bounded property: We assume that the random vector a \u223c F is bounded almost surely: for some \u00b5 > 0,\n\u2016a\u20162 \u2264 \u221a n\u00b5. (27)\nExamples for the above sketch mechanics include subsampled orthogonal systems (OS), subsampled tight or continuous frames, and random convolutions, etc, see (Candes & Plan, 2011) for further details. In this paper, we focus on OS sketches, which are based on randomly sampling the rows of a fixed orthonormal matrix K \u2208 Rn\u00d7n. Examples of such matrices include the discrete Fourier transform (DFT) matrix, and the Hadamard matrix. Using OS sketches has an advantage in computation, as that for suitably chosen orthonormal matrices such as the DFT and Hadamard matrices, a matrix-vector product can be executed in O(n logm) time, in contrast to O(nm) time required for the same operation with generic dense sketches. Conditions (27) implies that \u00b5 \u2265 1, due to the isotropy property. Without loss of generality, we assume that \u00b5 = 1 throughout. The following corollary shows that sketched-regularized algorithms have optimal rates provided the sketch dimension m is not too small.\nCorollary 4. Under the assumptions of Theorem 1, let S = range{S\u2217xG\u2217}, where G \u2208 Cm\u00d7n is a randomized matrix whose rows are i.i.d drawn from the distribution F . Let \u03bb = n\u2212 1 1\u2228(2\u03b6+\u03b3) and\nm &  n\u03b3 log2 3n \u03b3 \u03b4 log 3 \u03b4 if 2\u03b6 + \u03b3 \u2264 1, n \u03b3(\u03b6\u2212a) (1\u2212a)(2\u03b6+\u03b3) log 3n \u03b3 \u03b4 log 2 3 \u03b4 if \u03b6 \u2265 1,\nn \u03b3 2\u03b6+\u03b3 log 3n \u03b3\n\u03b4 log 2 3 \u03b4 otherwise.\n(28)\nThen with confidence at least 1\u2212 \u03b4, for \u03b6 \u2264 1, or \u03b6 > 1 and a \u2264 \u03b3/(2\u03b6 + \u03b3 \u2212 2), the following holds\n\u2016L\u2212a(fz\u03bb \u2212 fH)\u2016\u03c1\n. log3 3\n\u03b4\n{ n\u2212(\u03b6\u2212a) ( 1 + (\u03b3 log n)2(1\u2212a) ) , if 2\u03b6 + \u03b3 \u2264 1,\nn\u2212 \u03b6\u2212a 2\u03b6+\u03b3 , if 2\u03b6 + \u03b3 > 1.\n(29)\nThe above results assert that sketched-regularized algorithms converge optimally, provided the sketch dimension is not too small, or in another words the error caused by projection is negligible when the sketch dimension is large enough. Note that the minimal sketch dimension from the above is proportional to the effective dimension \u03bb\u2212\u03b3 up to a logarithmic factor for the case \u03b6 \u2264 1. Remark 2. 1) The bounded assumption (27) may be replaced with a high-probability bounded assumption as that in (Candes & Plan, 2011), which is satisfied for Gaussian sketches. 2) Considering only the case \u03b6 = 1/2 and a = 0, (Yang et al., 2015) provides optimal error bounds for sketched ridge regression within the fixed design setting. 3) Letting \u03b6 = 1/2, the minimal sketch dimension from the above is smaller than O(n \u03b3 \u03b3+1 log4 n) from (Yang et al., 2015) using OS sketches."}, {"heading": "3.4. Results for Nystro\u0308m Regularized Algorithms", "text": "As a byproduct of the paper, using Corollary 2, we derive the following results for Nystro\u0308m regularized algorithms. Corollary 5. Under the assumptions of Theorem 1, let S = span{x1, \u00b7 \u00b7 \u00b7 , xm}, 2\u03b6 + \u03b3 > 1, and \u03bb = n\u2212 1 2\u03b6+\u03b3 . Then with probability at least 1\u2212 \u03b4,\n\u2016L\u2212a(fz\u03bb \u2212 fH)\u2016\u03c1 . n \u2212 \u03b6\u2212a2\u03b6+\u03b3 log3 3\n\u03b4 ,\nprovided that\nm & (1 + log n\u03b3)\n{ n \u03b6\u2212a (1\u2212a)(2\u03b6+\u03b3) if \u03b6 \u2265 1,\nn 1 2\u03b6+\u03b3 if \u03b6 \u2264 1.\nRemark 3. 1) In the above, we only consider the plain Nystro\u0308m subsampling. Using the ALS Nystro\u0308m subsampling (Drineas et al., 2012; Gittens & Mahoney, 2013; Alaoui & Mahoney, 2015) and the proof technique developed in this paper and (Rudi et al., 2015), we can further improve the projection dimension condition to (28) (possibly with an extra log n). We will report this result in a longer version of this paper. 2) Considering only the case 1/2 \u2264 \u03b6 \u2264 1 and a = 0, (Rudi et al., 2015) provides optimal generalization error bounds for Nystro\u0308m ridge regression. This result was further extended in (Myleiko et al., 2017) to a general Nystro\u0308m regularized algorithm with a general source assumption indexed with an operator monotone function (but only in the attainable cases). Note that as in classic ridge regression, Nystro\u0308m ridge regression saturates over \u03b6 \u2265 1, i.e., it does not have a better rate even for a bigger \u03b6 \u2265 1. 3) For the case \u03b6 \u2265 1 and a = 0, (Myleiko et al., 2017) provides certain generalization error bounds for plain Nystro\u0308m regularized algorithms, but the rates are capacity-independent, and the minimal projection dimension O(n 2\u03b6\u22121 2\u03b6+1 ) is larger than ours (considering the case \u03b3 = 1 for the sake of fairness).\nAll the results stated in this section will be proved in the next section."}, {"heading": "4. Proof", "text": "In this section, we prove the results stated in Section 3. We first give some deterministic estimates and an analytics result. We then give some probabilistic estimates. Applying the probabilistic estimates into the analytics result, we prove the results for projected-regularized algorithms. We finally estimate the projection errors and present the proof for sketched-regularized algorithms."}, {"heading": "4.1. Deterministic Estimates", "text": "In this subsection, we introduce some deterministic estimates. For notational simplicity, throughout this paper, we\ndenote T\u03bb = T + \u03bb, Tx\u03bb = Tx + \u03bb.\nWe define a deterministic vector \u03c9\u03bb as follows,\n\u03c9\u03bb = G\u03bb(T )S\u2217\u03c1fH . (30)\nThe vector \u03c9\u03bb is often called population function. We introduce the following lemma. The proof is essentially the same as that for Lemma 26 from (Lin & Cevher, 2018). We thus omit it.\nLemma 6. Under Assumption 2, the following holds. 1) For any \u03b6 \u2212 \u03c4 \u2264 a \u2264 \u03b6,\n\u2016L\u2212a(S\u03c1\u03c9\u03bb \u2212 fH)\u2016\u03c1 \u2264 R\u03bb\u03b6\u2212a. (31)\n2)\n\u2016T a\u22121/2\u03c9\u03bb\u2016H \u2264 \u03c4R\u00b7 { \u03bb\u03b6+a\u22121, if \u2212 \u03b6 \u2264 a \u2264 1\u2212 \u03b6, \u03ba2(\u03b6+a\u22121), if a \u2265 1\u2212 \u03b6.\n(32)\nThe above lemma provides some basic properties for the population function. It will be useful for the proof of our main results. The left hand-side of (31) is often called true bias. Using the above lemma and some basic operator inequalities, we can prove the following analytic, deterministic result.\nProposition 7. Under Assumption 2, let\n1 \u2228 \u2016T 1 2 \u03bb T \u2212 12 x\u03bb \u2016 2 \u2228 \u2016T \u2212 1 2 \u03bb T 1 2 x\u03bb\u2016 2 \u2264 \u22061,\n\u2016T \u22121/2\u03bb [(Tx\u03c9\u03bb \u2212 S \u2217 xy)\u2212 (T \u03c9\u03bb \u2212 S\u2217\u03c1fH)]\u2016H \u2264 \u22062,\n\u2016T \u2212 Tx\u2016 \u2264 \u22063,\n\u2016T \u2212 1 2\n\u03bb (T \u2212 Tx)\u2016 \u2264 \u22064,\n\u2016(I \u2212 P )T 12 \u20162 = \u22065.\nThen, for any 0 \u2264 a \u2264 \u03b6 \u2227 12 , the following holds. 1) If \u03b6 \u2208 [0, 1],\n\u2016L\u2212a(S\u03c1\u03c9z\u03bb \u2212 fH)\u2016\u03c1 \u2264 \u03c4\u03bb\u2212a\u22061\u2212a1 \u00d7 ( \u22062 + 2(\u03c4 + 1)R\u03bb \u03b6 + \u03c4R\u03bb\u03b6\u22121(\u22065 + \u2206 1\u2212a 5 \u03bb a) ) .\n(33)\n2) If \u03b6 \u2265 1,\n\u2016L\u2212a(S\u03c1\u03c9z\u03bb \u2212 fH)\u2016\u03c1 \u2264 \u03c4\u03bb\u2212a\u22061\u2212a1 \u00d7 ( \u22062 + 3R\u03bb \u03b6 + \u03ba2(\u03b6\u22121)R ( \u03ba\u03c4\u22064 + \u03c4\u22065\n+\u03bb(\u22063 + \u22065) (\u03b6\u22121)\u22271 + \u03bb 1 2 \u2206 (\u03b6\u2212 12 )\u22271 3 + \u2206 1\u2212a 5 \u03bb a )) .\n(34)\nThe above proposition is key to our proof. The proof of the above proposition for the case \u03b6 \u2264 1 borrows ideas from (Smale & Zhou, 2007; Caponnetto & De Vito, 2007; Rudi et al., 2015; Myleiko et al., 2017; Lin et al., 2018), whereas the key step is an error decomposition from (Lin & Cevher, 2018). Our novelty lies in the proof for the case \u03b6 \u2265 1, see the appendix for further details."}, {"heading": "4.2. Proof for Projected-regularized Algorithms", "text": "To derive total error bounds from Proposition 7, it is necessary to develop probabilistic estimates for the random quantities \u22061, \u22062, \u22063 and \u22064. We thus introduce the following four lemmas. Lemma 8. (Lin et al., 2018) Under Assumption 3, let \u03b4 \u2208 (0, 1), \u03bb = n\u2212\u03b8 for some \u03b8 \u2265 0, and\nan,\u03b4,\u03b3(\u03b8) = 8\u03ba2 ( log 4\u03ba2(c\u03b3 + 1)\n\u03b4\u2016T \u2016 + \u03b8\u03b3min\n( 1\ne(1\u2212 \u03b8)+ , log n\n)) .\n(35)\nWe have with probability at least 1\u2212 \u03b4,\n\u2016(T + \u03bb)1/2(Tx + \u03bb)\u22121/2\u20162 \u2264 3an,\u03b4,\u03b3(\u03b8)(1 \u2228 n\u03b8\u22121),\nand\n\u2016(T + \u03bb)\u22121/2(Tx + \u03bb)1/2\u20162 \u2264 4\n3 an,\u03b4,\u03b3(\u03b8)(1 \u2228 n\u03b8\u22121).\nLemma 9. Let 0 < \u03b4 < 1/2. It holds with probability at least 1\u2212 \u03b4 :\n\u2016T \u2212 Tx\u2016HS \u2264 6\u03ba2\u221a n log 2 \u03b4 .\nHere, \u2016 \u00b7 \u2016HS denotes the Hilbert-Schmidt norm. Lemma 10. Under Assumption 3, let 0 < \u03b4 < 1/2. It holds with probability at least 1\u2212 \u03b4 :\n\u2016T \u2212 1 2 \u03bb (T \u2212 Tx)\u2016HS \u2264 2\u03ba ( 2\u03ba\nn \u221a \u03bb + \u221a c\u03b3 n\u03bb\u03b3 ) log 2 \u03b4 .\nThe proof of the above lemmas can be done simply applying concentration inequalities for sums of Hilbert-space-valued random variables. We refer to (Lin & Rosasco, 2017) for the proofs. Lemma 11. (Lin et al., 2018) Under Assumptions 1, 2 and 3, let \u03c9\u03bb be given by (30). For all \u03b4 \u2208]0, 1/2[, the following holds with probability at least 1\u2212 \u03b4 :\n\u2016T \u22121/2\u03bb [(Tx\u03c9\u03bb \u2212 S \u2217 xy)\u2212 (T \u03c9\u03bb \u2212 S\u2217\u03c1f\u03c1)]\u2016H\n\u2264\n( C1\nn\u03bb 1 2\u2228(1\u2212\u03b6)\n+\n\u221a C2\u03bb2\u03b6\nn\u03bb + C3 n\u03bb\u03b3\n) log 2\n\u03b4 .\n(36)\nHere, C1 = 4(M + R\u03ba(2\u03b6\u22121)+), C2 = 96R2\u03ba2 and C3 = 32(3B2 + 4Q2)c\u03b3 .\nWith the above probabilistic estimates and the analytics result, Proposition 7, we are now ready prove results for projected-regularized algorithms.\nProof of Theorem 1. We use Proposition 7 to prove the result. We thus need to estimate \u22061, \u22062, \u22063 and \u22064. Following from Lemmas 8, 9, 10 and 11, with n\u22121 \u2264 \u03bb \u2264 1, we know that with probability at least 1\u2212 \u03b4,\n\u22061 . t\u03b8,n log 3\n\u03b4 , (37)\n\u22062 .\n( 1\nn\u03bb 1 2\u2228(1\u2212\u03b6) + \u03bb\u03b6 + 1\u221a n\u03bb\u03b3\n) log 3\n\u03b4 ,\n\u22063 . 1\u221a n log 3 \u03b4 , (38)\n\u22064 . 1\u221a n\u03bb\u03b3 log 3 \u03b4 .\nThe results thus follow by introducing the above estimates into (33) or (34), combining with a direct calculation and 1/n \u2264 \u03bb \u2264 1."}, {"heading": "4.3. Proof for Sketched-regularized Algorithms", "text": "In order to use Corollary 2 for sketched-regularized algorithms, we need to estimate the projection error. The basic idea is to approximate the projection error in terms of its \u2018empirical\u2019 version, \u2016(I \u2212 P )T 1 2\nx \u20162. The estimate for \u2016(I \u2212P )T 1 2\nx \u20162 is quite lengthy and it is divided into several steps. We begin with the following concentration inequalities.\nLemma 12. Let 0 < \u03b4 < 1 and \u03bb > 0. For any given x \u2286 Hn, there exists a subset Ux of Rm\u00d7n with measure at least 1\u2212 \u03b4, such that for all G \u2208 Ux,\u2225\u2225\u2225(Tx + \u03bb)\u22121/2(Tx \u2212m\u22121S\u2217xG\u2217GSx)(Tx + \u03bb)\u22121/2\u2225\u2225\u2225\n\u2264 4Nx(\u03bb)\u03b2 3m +\n\u221a 2Nx(\u03bb)\u03b2\nm ,\nwhere Nx(\u03bb) = tr((Tx + \u03bb)\u22121Tx),\n\u03b2 = log 4Nx(\u03bb)(1 + \u03bb/\u2016Tx\u2016)\n\u03b4 .\nThe above lemma can be proved using the concentration inequalities from (Tropp, 2012; Minsker, 2011). With the above lemma and Lemma 8, we can estimate \u2016(I\u2212P )T 1 2\nx \u20162 as follows.\nLemma 13. Let 0 < \u03b4 < 1 and \u03b8 \u2208 [0, 1]. Given a fix x \u2208 Hn, assume that for \u03bb = n\u2212\u03b8,\ntr((Tx + \u03bb)\u22121Tx) \u2264 b\u03b3\u03bb\u2212\u03b3 (39)\nholds for some b\u03b3 > 0, \u03b3 \u2208 [0, 1]. Then there exists a subset Ux of Rm\u00d7n with measure at least 1\u2212 \u03b4, such that for all G \u2208 Ux,\n\u2016(I \u2212 P )T 1 2 x \u20162 \u2264 3\nn\u03b8 ,\nprovided that\nm \u2265 8b\u03b3n\u03b8\u03b3 log 8b\u03b3n \u03b8\u03b3\n\u03b4 . (40)\nUnder the condition (39), Lemma 13 provides an upper bound for \u2016(I \u2212 P )T 1 2\nx \u2016, which will be used to control the projection error using the following lemma.\nLemma 14. Let P be a projection operator in a Hilbert space H , and A, B be two semidefinite positive operators on H. For any 0 \u2264 s, t \u2264 12 , we have\n\u2016As(I \u2212 P )At\u2016 \u2264 \u2016A\u2212B\u2016s+t + \u2016B 12 (I \u2212 P )B 12 \u2016s+t.\nThe left-hand side of (39) is called empirical effective dimension. It can be estimated as follows.\nLemma 15. Under Assumption 3, let \u03bb = n\u2212\u03b8 for some \u03b8 \u2208 [0, 1] and 0 < \u03b4 < 1. With confidence 1\u2212 \u03b4,\ntr((Tx + \u03bb)\u22121Tx)\n\u2264 3(4\u03ba2 + 2\u03ba\u221ac\u03b3 + c\u03b3) log 4\n\u03b4 an,\u03b4/2,\u03b3(\u03b8)\u03bb\n\u2212\u03b3 , (41)\nwhere an,\u03b4/2,\u03b3(\u03b8) is given as in Lemma 8.\nThe above lemma improves Proposition 1 of (Rudi et al., 2015). It does not require the extra assumption that the sample size is large enough, and our proof is simpler. Now we are ready to estimate the projection error and give the proof for sketched-regularized algorithms.\nProof of Corollary 4. Let \u03bb\u2032 = n\u2212\u03b8 \u2032 , with\n\u03b8\u2032 =  1, if 2\u03b6 + \u03b3 \u2264 1, \u03b6\u2212a (1\u2212a)(2\u03b6+\u03b3) , if \u03b6 \u2265 1,\n1 2\u03b6+\u03b3 , otherwise\nFollowing from Corollary 2, Lemmas 8, 9 and 15, we know that there exists a subset V of Zn with measure at least 1\u2212 4\u03b4, such that for all z \u2208 V , (22) (or (23), or (24)), (37), (38), and (41) (with \u03b8 and \u03bb replaced by \u03b8\u2032 and \u03bb\u2032 in (41), respectively) hold. For any z \u2208 V , using Lemma 13 with\nb\u03b3 =3(4\u03ba 2 + 2\u03ba \u221a c\u03b3 + c\u03b3) log 4\n\u03b4 an,\u03b4/2,\u03b3(\u03b8\n\u2032)\n.(1 \u2228 [(1\u2212 \u03b8\u2032)\u22121 \u2227 log n\u03b3 ] + log 3 \u03b4 ) log 3 \u03b4 ,\nwe know that there exists a subsetUz of Rm\u00d7n with measure at least 1\u2212 \u03b4, such that for all G \u2208 Uz,\n\u2016(I \u2212 P )T 1 2 x \u20162 . 1\nn\u03b8\u2032 , (42)\nprovided m & n\u03b8 \u2032\u03b3b\u03b3 log\n3b\u03b3n \u03b8\u2032\u03b3\n\u03b4 , which is guaranteed by\nCondition (28). Using \u2016(I \u2212P )T 1 2 x \u20162 = \u2016T 1 2 x (I \u2212P )T 1 2\nx \u2016, and Lemma 14,\n\u2016T 12 (I \u2212 P )T 12 \u2016 \u2264 \u2016Tx \u2212 T \u2016+ \u2016(I \u2212 P )T 1 2 x \u20162.\nIntroducing with (38), and (42), and noting that \u2016T 12 (I \u2212 P )T 12 \u2016 = \u2016(I \u2212 P )T 12 \u20162, we get\n\u2016(I \u2212 P )T 12 \u20162 . log 3\u03b4\u221a n + 1 n\u03b8\u2032 .\nIntroduce the above into (24), one can prove the desired results for the case \u03b6 \u2265 1. Now consider the case \u03b6 \u2264 1. Note that\n\u2016(I \u2212 P )T 12 \u20162 \u2264 \u2016(I \u2212 P )T 1 2 x\u03bb\u2016 2\u2016T \u2212 1 2 x\u03bb T 1 2 \u03bb \u2016 2.\nIntroducing with (37) and using a similar argument as that for (50),\n\u2016(I \u2212 P )T 12 \u20162 . (\u2016(I \u2212 P )T 1 2 x \u20162 + \u03bb)t\u03b8,n log 3\n\u03b4 .\nApplying (42), \u2016(I \u2212 P )T 12 \u20162 . ( 1 n\u03b8\u2032 + \u03bb)t\u03b8,n log 3 \u03b4 . Introducing the above into (22), or (23), one can prove the desired results for the case \u03b6 \u2264 1.\nThe proof of Corollary 5 will be given in the appendix due to space limitation."}, {"heading": "5. Conclusion", "text": "In this paper, we prove optimal statistical results with respect to variants of norms for sketched/Nystro\u0308m regularized algorithms. Our contributions are mainly on theoretical aspects. First, our results for sketched-regularized algorithms generalize previous results (Yang et al., 2015) from the fixed design setting to the random design setting. Moreover, our results involve the regularity/smoothness of the target function and thus can have a faster convergence rate. Second, our results cover the non-attainable cases, which have not been studied before for both Nystro\u0308m and sketched regularized algorithms. Third, our results provide the first optimal, capacity-dependent rates even when \u03b6 \u2265 1. This may suggest that sketched/Nystro\u0308m regularized algorithms have certain advantages in comparison with distributed learning algorithms (Zhang et al., 2015), as the latter suffer a saturation effect over \u03b6 = 1. A future direction is to extend our analysis to learning with random features, see (Sriperumbudur & Sterge, 2017; Lin & Rosasco, 2018) and references therein."}, {"heading": "Acknowledgements", "text": "This work was sponsored by the Department of the Navy, Office of Naval Research (ONR) under a grant number N62909-17-1-2111. It has also received funding from Hasler Foundation Program: Cyber Human Systems (project number 16066), and from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation program (grant agreement n 725594-timedata)."}], "year": 2018, "references": [{"title": "Fast randomized kernel ridge regression with statistical guarantees", "authors": ["Alaoui", "Ahmed", "Mahoney", "Michael W"], "year": 2015}, {"title": "Sharp analysis of low-rank kernel matrix approximations", "authors": ["Bach", "Francis"], "venue": "In Conference on Learning Theory, pp", "year": 2013}, {"title": "On the equivalence between kernel quadrature rules and random feature expansions", "authors": ["Bach", "Francis"], "venue": "Arxiv,", "year": 2015}, {"title": "On regularization algorithms in learning theory", "authors": ["Bauer", "Frank", "Pereverzev", "Sergei", "Rosasco", "Lorenzo"], "venue": "Journal of Complexity,", "year": 2007}, {"title": "Optimal rates for regularization of statistical inverse learning problems", "authors": ["Blanchard", "Gilles", "Mucke", "Nicole"], "venue": "arXiv preprint arXiv:1604.04054,", "year": 2016}, {"title": "A probabilistic and RIPless theory of compressed sensing", "authors": ["Candes", "Emmanuel J", "Plan", "Yaniv"], "venue": "IEEE Transactions on Information Theory,", "year": 2011}, {"title": "Optimal learning rates for regularization operators in learning theory", "authors": ["Caponnetto", "Andrea"], "venue": "Technical report,", "year": 2006}, {"title": "Optimal rates for the regularized least-squares algorithm", "authors": ["Caponnetto", "Andrea", "De Vito", "Ernesto"], "venue": "Foundations of Computational Mathematics,", "year": 2007}, {"title": "Adaptation for regularization operators in learning theory", "authors": ["Caponnetto", "Andrea", "Yao", "Yuan"], "year": 2006}, {"title": "Learning theory: an approximation theory viewpoint, volume 24", "authors": ["Cucker", "Felipe", "Zhou", "Ding Xuan"], "year": 2007}, {"title": "Kernel ridge vs. principal component regression: Minimax bounds and the qualification of regularization operators. 2016", "authors": ["Dicker", "Lee H", "Foster", "Dean P", "Hsu", "Daniel"], "year": 2016}, {"title": "Kernel ridge vs. principal component regression: Minimax bounds and the qualification of regularization operators", "authors": ["Dicker", "Lee H", "Foster", "Dean P", "Hsu", "Daniel"], "venue": "Electronic Journal of Statistics,", "year": 2017}, {"title": "Fast approximation of matrix coherence and statistical leverage", "authors": ["Drineas", "Petros", "Magdon-Ismail", "Malik", "Mahoney", "Michael W", "Woodruff", "David P"], "venue": "Journal of Machine Learning Research,", "year": 2012}, {"title": "Regularization of inverse problems, volume 375", "authors": ["Engl", "Heinz Werner", "Hanke", "Martin", "Neubauer", "Andreas"], "venue": "Springer Science & Business Media,", "year": 1996}, {"title": "Norm inequalities equivalent to Heinz inequality", "authors": ["Fujii", "Junichi", "Masatoshi", "Furuta", "Takayuki", "Nakamoto", "Ritsuo"], "venue": "Proceedings of the American Mathematical Society,", "year": 1993}, {"title": "Spectral algorithms for supervised learning", "authors": ["Gerfo", "L Lo", "Rosasco", "Lorenzo", "Odone", "Francesca", "De Vito", "Ernesto", "Verri", "Alessandro"], "venue": "Neural Computation,", "year": 2008}, {"title": "Revisiting the nystrom method for improved large-scale machine learning", "authors": ["Gittens", "Alex", "Mahoney", "Michael W"], "venue": "arXiv preprint arXiv:1303.1849,", "year": 2013}, {"title": "An operator inequality", "authors": ["Hansen", "Frank"], "venue": "Mathematische Annalen,", "year": 1980}, {"title": "Random design analysis of ridge regression", "authors": ["Hsu", "Daniel", "Kakade", "Sham M", "Zhang", "Tong"], "venue": "Foundations of Computational Mathematics,", "year": 2014}, {"title": "Optimal convergence for distributed learning with stochastic gradient methods and spectral-regularization algorithms", "authors": ["Lin", "Junhong", "Cevher", "Volkan"], "venue": "arXiv preprint arXiv:1801.07226,", "year": 2018}, {"title": "Optimal rates for multipass stochastic gradient methods", "authors": ["Lin", "Junhong", "Rosasco", "Lorenzo"], "venue": "Journal of Machine Learning Research,", "year": 2017}, {"title": "Generalization properties of doubly stochastic learning algorithms", "authors": ["Lin", "Junhong", "Rosasco", "Lorenzo"], "venue": "Journal of Complexity,", "year": 2018}, {"title": "Optimal rates for spectral-regularized algorithms with least-squares regression over hilbert spaces", "authors": ["Lin", "Junhong", "Rudi", "Alessandro", "Rosasco", "Lorenzo", "Cevher", "Volkan"], "venue": "arXiv preprint arXiv:1801.06720,", "year": 2018}, {"title": "On some extensions of Bernstein\u2019s inequality for self-adjoint operators", "authors": ["Minsker", "Stanislav"], "venue": "arXiv preprint arXiv:1112.5448,", "year": 2011}, {"title": "Regularized Nystr\u00f6m subsampling in regression and ranking problems under general smoothness assumptions", "authors": ["GL Myleiko", "S Pereverzyev Jr.", "Solodky", "SG"], "year": 2017}, {"title": "Remarks on inequalities for large deviation probabilities", "authors": ["IF Pinelis", "Sakhanenko", "AI"], "venue": "Theory of Probability & Its Applications,", "year": 1986}, {"title": "Functional data analysis", "authors": ["Ramsay", "James O"], "venue": "Wiley Online Library,", "year": 2006}, {"title": "Less is more: Nystr\u00f6m computational regularization", "authors": ["Rudi", "Alessandro", "Camoriano", "Raffaello", "Rosasco", "Lorenzo"], "venue": "Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Learning theory estimates via integral operators and their approximations", "authors": ["Smale", "Steve", "Zhou", "Ding-Xuan"], "venue": "Constructive approximation,", "year": 2007}, {"title": "Sparse greedy matrix approximation for machine learning", "authors": ["Smola", "Alex J", "Sch\u00f6lkopf", "Bernhard"], "year": 2000}, {"title": "Approximate kernel pca using random features: Computational vs. statistical trade-off", "authors": ["Sriperumbudur", "Bharath", "Sterge", "Nicholas"], "venue": "arXiv preprint arXiv:1706.06296,", "year": 2017}, {"title": "Support Vector Machines", "authors": ["Steinwart", "Ingo", "Christmann", "Andreas"], "venue": "Springer Science & Business Media,", "year": 2008}, {"title": "Optimal rates for regularized least squares regression", "authors": ["Steinwart", "Ingo", "Hush", "Don R", "Scovel", "Clint"], "venue": "In Conference On Learning Theory,", "year": 2009}, {"title": "User-friendly tools for random matrices: An introduction", "authors": ["Tropp", "Joel A"], "venue": "Technical report, DTIC Document,", "year": 2012}, {"title": "Using the Nystr\u00f6m method to speed up kernel machines", "authors": ["Williams", "Christopher KI", "Seeger", "Matthias"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2000}, {"title": "Randomized sketches for kernels: Fast and optimal nonparametric regression", "authors": ["Yang", "Yun", "Pilanci", "Mert", "Wainwright", "Martin J"], "venue": "arXiv preprint arXiv:1501.06195,", "year": 2015}, {"title": "Learning bounds for kernel regression using effective data dimensionality", "authors": ["Zhang", "Tong"], "venue": "Neural Computation,", "year": 2005}, {"title": "Divide and conquer kernel ridge regression: a distributed algorithm with minimax optimal rates", "authors": ["Zhang", "Yuchen", "Duchi", "John C", "Wainwright", "Martin J"], "venue": "Journal of Machine Learning Research,", "year": 2015}], "id": "SP:481a75a4c612730540e5c941ebdf041c728f9326", "authors": [{"name": "Junhong Lin", "affiliations": []}, {"name": "Volkan Cevher", "affiliations": []}], "abstractText": "We investigate regularized algorithms combining with projection for least-squares regression problem over a Hilbert space, covering nonparametric regression over a reproducing kernel Hilbert space. We prove convergence results with respect to variants of norms, under a capacity assumption on the hypothesis space and a regularity condition on the target function. As a result, we obtain optimal rates for regularized algorithms with randomized sketches, provided that the sketch dimension is proportional to the effective dimension up to a logarithmic factor. As a byproduct, we obtain similar results for Nystr\u00f6m regularized algorithms. Our results provide optimal, distributiondependent rates that do not have any saturation effect for sketched/Nystr\u00f6m regularized algorithms, considering both the attainable and non-attainable cases.", "title": "Optimal Rates of Sketched-regularized Algorithms for Least-Squares Regression over Hilbert Spaces"}