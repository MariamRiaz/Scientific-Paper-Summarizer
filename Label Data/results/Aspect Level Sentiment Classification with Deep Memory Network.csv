0,1,label2,summary_sentences
"Dependency parsing is a longstanding natural language processing task, with its outputs crucial to various downstream tasks including relation extraction (Schmitz et al., 2012; Angeli et al., 2015), language modeling (Gubbins and Vlachos, 2013), and natural logic inference (Bowman et al., 2016).
",1 Introduction,[0],[0]
"Attractive for their linear time complexity and amenability to conventional classification methods, transition-based dependency parsers have sparked much research interest recently.",1 Introduction,[0],[0]
"A transition-based parser makes sequential predictions of transitions between states under the restrictions of a transition system (Nivre, 2003).",1 Introduction,[0],[0]
"Transition-based parsers have been shown to excel at parsing shorter-range dependency structures, as well as languages where non-projective parses are less pervasive (McDonald and Nivre, 2007).
",1 Introduction,[0],[0]
"However, the transition systems employed in state-of-the-art dependency parsers usually define very local transitions.",1 Introduction,[0],[0]
"At each step, only one or two words are affected, with very local attachments made.",1 Introduction,[0],[0]
"As a result, distant attachments require long and not immediately obvious transition sequences (e.g., ate→chopsticks in Figure 1, which requires two transitions).",1 Introduction,[0],[0]
"This is further aggravated by the usually local lexical information leveraged to make transition predictions (Chen and Manning, 2014; Andor et al., 2016).
",1 Introduction,[0],[0]
"In this paper, we introduce a novel transition system, arc-swift, which defines non-local transitions that directly induce attachments of distance up to n (n = the number of tokens in the sentence).",1 Introduction,[0],[0]
"Such an approach is connected to graph-based dependency parsing, in that it leverages pairwise scores between tokens in making parsing decisions (McDonald et al., 2005).
",1 Introduction,[0],[0]
We make two main contributions in this paper.,1 Introduction,[0],[0]
"Firstly, we introduce a novel transition system for dependency parsing, which alleviates the difficulty of distant attachments in previous systems by allowing direct attachments anywhere in the stack.",1 Introduction,[0],[0]
"Secondly, we compare parsers by the number of mistakes they make in common linguistic con-
ar X
iv :1
70 5.
04 43
4v 1
[ cs
.C",1 Introduction,[0],[0]
"L
] 1
2 M
ay 2
01 7
structions.",1 Introduction,[0],[0]
We show that arc-swift parsers reduce errors in attaching prepositional phrases and conjunctions compared to parsers using existing transition systems.,1 Introduction,[0],[0]
Transition-based dependency parsing is performed by predicting transitions between states (see Figure 1 for an example).,2 Transition-based Dependency Parsing,[0],[0]
"Parser states are usually written as (σ|i, j|β,A), where σ|i denotes the stack with token i on the top, j|β denotes the buffer with token j at its leftmost, and A the set of dependency arcs.",2 Transition-based Dependency Parsing,[0],[0]
"Given a state, the goal of a dependency parser is to predict a transition to a new state that would lead to the correct parse.",2 Transition-based Dependency Parsing,[0],[0]
"A transition system defines a set of transitions that are sound and complete for parsers, that is, every transition sequence would derive a well-formed parse tree, and every possible parse tree can also be derived from some transition sequence.1
Arc-standard (Nivre, 2004) is one of the first transition systems proposed for dependency parsing.",2 Transition-based Dependency Parsing,[0],[0]
"It defines three transitions: shift, left arc (LArc), and right arc (RArc) (see Figure 2 for definitions, same for the following transition systems), where all arc-inducing transitions operate on the stack.",2 Transition-based Dependency Parsing,[0],[0]
"This system builds the parse bottom-up, i.e., a constituent is only attached to its head after it has received all of its dependents.",2 Transition-based Dependency Parsing,[0],[0]
"A potential drawback is that during parsing, it is difficult to predict if a constituent has consumed all of its right dependents.",2 Transition-based Dependency Parsing,[0],[0]
"Arc-eager (Nivre, 2003) remedies this drawback by defining arc-inducing transitions that operate between the stack and the buffer.",2 Transition-based Dependency Parsing,[0],[0]
"As a result, a constituent no longer needs to be complete
1We only focus on projective parses for the scope of this paper.
",2 Transition-based Dependency Parsing,[0],[0]
"before it can be attached to its head to the left, as a right arc doesn’t prevent the attached dependent from taking further dependents of its own.2 Kuhlmann et al. (2011) propose a hybrid system derived from a tabular parsing scheme, which they have shown both arc-standard and arc-eager can be derived from.",2 Transition-based Dependency Parsing,[0],[0]
"Arc-hybrid combines LArc from arc-eager and RArc from arc-standard to build dependencies bottom-up.
",2 Transition-based Dependency Parsing,[0],[0]
"3 Non-local Transitions with arc-swift
The traditional transition systems discussed in Section 2 only allow very local transitions affecting one or two words, which makes long-distance dependencies difficult to predict.",2 Transition-based Dependency Parsing,[0],[0]
"To illustrate the limitation of local transitions, consider parsing the following sentences:
I ate fish with ketchup.",2 Transition-based Dependency Parsing,[0],[0]
"I ate fish with chopsticks.
",2 Transition-based Dependency Parsing,[0],[0]
"The two sentences have almost identical structures, with the notable difference that the prepositional phrase is complementing the direct object in the first case, and the main verb in the second.
",2 Transition-based Dependency Parsing,[0],[0]
"For arc-standard and arc-hybrid, the parser would have to decide between Shift and RArc when the parser state is as shown in Figure 3a, where ? stands for either “ketchup” or “chopsticks”.3 Similarly, an arc-eager parser would deal with the state shown in Figure 3b.",2 Transition-based Dependency Parsing,[0],[0]
"Making the correct transition requires information about context words “ate” and “fish”, as well as “?”.
",2 Transition-based Dependency Parsing,[0],[0]
2A side-effect of arc-eager is that there is sometimes spurious ambiguity between Shift and Reduce transitions.,2 Transition-based Dependency Parsing,[0],[0]
"For the example in Figure 1, the first Reduce can be inserted before the third Shift without changing the correctness of the resulting parse, i.e., both are feasible at that time.
",2 Transition-based Dependency Parsing,[0],[0]
"3For this example, we assume that the sentence is being parsed into Universal Dependencies.
",2 Transition-based Dependency Parsing,[0],[0]
"Parsers employing traditional transition systems would usually incorporate more features about the context in the transition decision, or employ beam search during parsing (Chen and Manning, 2014; Andor et al., 2016).
",2 Transition-based Dependency Parsing,[0],[0]
"In contrast, inspired by graph-based parsers, we propose arc-swift, which defines non-local transitions as shown in Figure 2.",2 Transition-based Dependency Parsing,[0],[0]
"This allows direct comparison of different attachment points, and provides a direct solution to parsing the two example sentences.",2 Transition-based Dependency Parsing,[0],[0]
"When the arc-swift parser encounters a state identical to Figure 3b, it could directly compare transitions RArc[1] and RArc[2] instead of evaluating between local transitions.",2 Transition-based Dependency Parsing,[0],[0]
"This results in a direct attachment much like that in a graph-based parser, informed by lexical information about affinity of the pairs of words.
",2 Transition-based Dependency Parsing,[0],[0]
Arc-swift also bears much resemblance to arceager.,2 Transition-based Dependency Parsing,[0],[0]
"In fact, an LArc[k] transition can be viewed as k− 1 Reduce operations followed by one LArc in arc-eager, and similarly for RArc[k].",2 Transition-based Dependency Parsing,[0],[0]
"Reduce is no longer needed in arc-swift as it becomes part of LArc[k] and RArc[k], removing the ambiguity in derived transitions in arc-eager.",2 Transition-based Dependency Parsing,[0],[0]
"arc-swift is also equivalent to arc-eager in terms of soundness and completeness.4 A caveat is that the worst-case time complexity of arc-swift is O(n2) instead of O(n), which existing transition-based parsers enjoy.",2 Transition-based Dependency Parsing,[0],[0]
"However, in practice the runtime is nearly
4This is easy to show because in arc-eager, all Reduce transitions can be viewed as preparing for a later LArc or RArc transition.",2 Transition-based Dependency Parsing,[0],[0]
"We also note that similar to arc-eager transitions, arc-swift transitions must also satisfy certain pre-conditions.",2 Transition-based Dependency Parsing,[0],[0]
"Specifically, an RArc[k] transition requires that the top k − 1 elements in the stack are already attached; LArc[k] additionally requires that the k-th element is unattached, resulting in no more than one feasible LArc candidate for any parser state.
",2 Transition-based Dependency Parsing,[0],[0]
"linear, thanks to the usually small number of reducible tokens in the stack.",2 Transition-based Dependency Parsing,[0],[0]
"We use the Wall Street Journal portion of Penn Treebank with standard parsing splits (PTBSD), along with Universal Dependencies v1.3 (Nivre et al., 2016) (EN-UD).",4.1 Data and Model,[0],[0]
"PTB-SD is converted to Stanford Dependencies (De Marneffe and Manning, 2008) with CoreNLP 3.3.0 (Manning et al., 2014) following previous work.",4.1 Data and Model,[0],[0]
"We report labelled and unlabelled attachment scores (LAS/UAS), removing punctuation from all evaluations.
",4.1 Data and Model,[0],[0]
"Our model is very similar to that of (Kiperwasser and Goldberg, 2016), where features are extracted from tokens with bidirectional LSTMs, and concatenated for classification.",4.1 Data and Model,[0],[0]
"For the three traditional transition systems, features of the top 3 tokens on the stack and the leftmost token in the buffer are concatenated as classifier input.",4.1 Data and Model,[0],[0]
"For arc-swift, features of the head and dependent tokens for each arc-inducing transition are concatenated to compute scores for classification, and features of the leftmost buffer token is used for Shift.",4.1 Data and Model,[0],[0]
For other details we defer to Appendix A.,4.1 Data and Model,[0],[0]
The full specification of the model can also be found in our released code online at https://github.,4.1 Data and Model,[0],[0]
com/qipeng/arc-swift.,4.1 Data and Model,[0],[0]
"We use static oracles for all transition systems, and for arc-eager we implement oracles that always Shift/Reduce when ambiguity is present (arceager-S/R).",4.2 Results,[0],[0]
"We evaluate our parsers with greedy parsing (i.e., beam size 1).",4.2 Results,[0],[0]
"The results are shown in Table 1.5 Note that K&G 2016 is trained with a dynamic oracle (Goldberg and Nivre, 2012), Andor 2016 with a CRF-like loss, and both Andor 2016 and Weiss 2015 employed beam search (with sizes 32 and 8, respectively).
",4.2 Results,[0],[0]
"For each pair of the systems we implemented, we studied the statistical significance of their difference by performing a paired test with 10,000 bootstrap samples on PTB-SD.",4.2 Results,[0],[0]
"The resulting pvalues are analyzed with a 10-group BonferroniHolm test, with results shown in Table 2.",4.2 Results,[0],[0]
"We note
5In the interest of space, we abbreviate all transition systems (TS) as follows in tables: asw for arc-swift, asd for arcstandard, aeS/R for arc-eager-S/R, and ah for arc-hybrid.
",4.2 Results,[0],[0]
"that with almost the same implementation, arcswift parsers significantly outperform those using traditional transition systems.",4.2 Results,[0],[0]
We also analyzed the performance of parsers on attachments of different distances.,4.2 Results,[0],[0]
"As shown in Figure 4, arc-swift is equally accurate as existing systems for short dependencies, but is more robust for longer ones.
",4.2 Results,[0],[0]
"While arc-swift introduces direct long-distance transitions, it also shortens the overall sequence necessary to induce the same parse.",4.2 Results,[0],[0]
"A parser could potentially benefit from both factors: direct attachments could make an easier classification task, and shorter sequences limit the effect of error propagation.",4.2 Results,[0],[0]
"However, since the two effects are correlated in a transition system, precise attribution of the gain is out of the scope of this paper.
",4.2 Results,[0],[0]
Computational efficiency.,4.2 Results,[0],[0]
"We study the computational efficiency of the arc-swift parser by
6https://github.com/tensorflow/models/ blob/master/syntaxnet/g3doc/universal.md
comparing it to an arc-eager parser.",4.2 Results,[0],[0]
"On the PTBSD development set, the average transition sequence length per sentence of arc-swift is 77.5% of that of arc-eager.",4.2 Results,[0],[0]
"At each step of parsing, arc-swift needs to evaluate only about 1.24 times the number of transition candidates as arc-eager, which results in very similar runtime.",4.2 Results,[0],[0]
"In contrast, beam search with beam size 2 for arc-eager requires evaluating 4 times the number of transition candidates compared to greedy parsing, which results in a UAS 0.14% worse and LAS 0.22% worse for arc-eager compared to greedily decoded arcswift.",4.2 Results,[0],[0]
"We automatically extracted all labelled attachment errors by error type (incorrect attachment or relation), and categorized a few top parser errors by hand into linguistic constructions.",4.3 Linguistic Analysis,[0],[0]
"Results on PTB-SD are shown in Table 3.7 We note that the arc-swift parser improves accuracy on prepositional phrase (PP) and conjunction attachments, while it remains comparable to other parsers on other common errors.",4.3 Linguistic Analysis,[0],[0]
Analysis on EN-UD shows a similar trend.,4.3 Linguistic Analysis,[0],[0]
"As shown in the table, there are still many parser errors unaccounted for in our analysis.",4.3 Linguistic Analysis,[0],[0]
"We leave this to future work.
",4.3 Linguistic Analysis,[0],[0]
"7We notice that for some examples the parsers predicted a ccomp (complement clause) attachment to verbs “says” and “said”, where the CoreNLP output simply labelled the relation as dep (unspecified).",4.3 Linguistic Analysis,[0],[0]
For other examples the relation between the prepositions in “out of” is labelled as prep (preposition) instead of pcomp (prepositional complement).,4.3 Linguistic Analysis,[0],[0]
"We suspect this is due to the converter’s inability to handle certain corner cases, but further study is warranted.",4.3 Linguistic Analysis,[0],[0]
Previous work has also explored augmenting transition systems to facilitate longer-range attachments.,5 Related Work,[0],[0]
"Attardi (2006) extended the arcstandard system for non-projective parsing, with arc-inducing transitions that are very similar to those in arc-swift.",5 Related Work,[0],[0]
A notable difference is that their transitions retain tokens between the head and dependent.,5 Related Work,[0],[0]
"Fernández-González and GómezRodrı́guez (2012) augmented the arc-eager system with transitions that operate on the buffer, which shorten the transition sequence by reducing the number of Shift transitions needed.",5 Related Work,[0],[0]
"However, limited by the sparse feature-based classifiers used, both of these parsers just mentioned only allow direct attachments of distance up to 3 and 2, respectively.",5 Related Work,[0],[0]
"More recently, Sartorio et al. (2013) extended arc-standard with transitions that directly attach to left and right “spines” of the top two nodes in the stack.",5 Related Work,[0],[0]
"While this work shares very similar motivations as arc-swift, it requires additional data structures to keep track of the left and right spines of nodes.",5 Related Work,[0],[0]
"This transition system also introduces spurious ambiguity where multiple transition sequences could lead to the same correct parse, which necessitates easy-first training to achieve a more noticeable improvement over arcstandard.",5 Related Work,[0],[0]
"In contrast, arc-swift can be easily implemented given the parser state alone, and does not give rise to spurious ambiguity.
",5 Related Work,[0],[0]
"For a comprehensive study of transition systems for dependency parsing, we refer the reader to (Bohnet et al., 2016), which proposed a generalized framework that could derive all of the traditional transition systems we described by configuring the size of the active token set and the maximum arc length, among other control parameters.",5 Related Work,[0],[0]
"However, this framework does not cover
arc-swift in its original form, as the authors limit each of their transitions to reduce at most one token from the active token set (the buffer).",5 Related Work,[0],[0]
"On the other hand, the framework presented in (GómezRodrı́guez and Nivre, 2013) does not explicitly make this constraint, and therefore generalizes to arc-swift.",5 Related Work,[0],[0]
"However, we note that arc-swift still falls out of the scope of existing discussions in that work, by introducing multiple Reduces in a single transition.",5 Related Work,[0],[0]
"In this paper, we introduced arc-swift, a novel transition system for dependency parsing.",6 Conclusion,[0],[0]
We also performed linguistic analyses on parser outputs and showed arc-swift parsers reduce errors in conjunction and adverbial attachments compared to parsers using traditional transition systems.,6 Conclusion,[0],[0]
"We thank Timothy Dozat, Arun Chaganty, Danqi Chen, and the anonymous reviewers for helpful discussions.",Acknowledgments,[0],[0]
Stanford University gratefully acknowledges the support of the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract,Acknowledgments,[0],[0]
No. FA8750-13-2-0040.,Acknowledgments,[0],[0]
"Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the DARPA, AFRL, or the US government.",Acknowledgments,[0],[0]
"Our model setup is similar to that of (Kiperwasser and Goldberg, 2016)",A Model and Training Details,[0],[0]
(See Figure 5).,A Model and Training Details,[0],[0]
"We employ two blocks of bidirectional long short-term memory (BiLSTM) networks (Hochreiter and Schmidhuber, 1997) that share very similar structures, one for part-of-speech (POS) tagging, the other for parsing.",A Model and Training Details,[0],[0]
"Both BiLSTMs have 400 hidden units in each direction, and the output of both are concatenated and fed into a dense layer of rectified linear units (ReLU) before 32-dimensional representations are derived as classification features.",A Model and Training Details,[0],[0]
"As the input to the tagger BiLSTM, we represent words with 100-dimensional word embeddings, initialized with GloVe vectors (Pennington et al., 2014).8",A Model and Training Details,[0],[0]
"The output distribution of the tagger classifier is used to compute a weighted sum of 32- dimensional POS embeddings, which is then concatenated with the output of the tagger BiLSTM",A Model and Training Details,[0],[0]
(800-dimensional per token) as the input to the parser BiLSTM.,A Model and Training Details,[0],[0]
"For the parser BiLSTM, we use two separate sets of dense layers to derive a “head” and a “dependent” representation for each token.",A Model and Training Details,[0],[0]
"These representations are later merged according to the parser state to make transition predictions.
",A Model and Training Details,[0],[0]
"For traditional transition systems, we follow (Kiperwasser and Goldberg, 2016) by featurizing the top 3 tokens on the stack and the leftmost token in the buffer.",A Model and Training Details,[0],[0]
"To derive features for each token, we take its head representation vhead and dependent representation vdep, and perform the following biaffine combination
vfeat,i =",A Model and Training Details,[0],[0]
"[f(vhead, vdep)]i = ReLU ( v>headWivdep +",A Model and Training Details,[0],[0]
b,A Model and Training Details,[0],[0]
>,A Model and Training Details,[0],[0]
"i vhead
+ c",A Model and Training Details,[0],[0]
">i vdep + di ) (1)
where Wi ∈ R32×32, bi, ci ∈ R32, and di is a scalar for i = 1, . . .",A Model and Training Details,[0],[0]
", 32.",A Model and Training Details,[0],[0]
"The resulting 32- dimensional features are concatenated as the input
8We also kept the vectors of the top 400k words trained on Wikipedia and English Gigaword for a broader coverage of unseen words.
to a fixed-dimensional softmax classifier for transition decisions.
",A Model and Training Details,[0],[0]
"For arc-swift, we featurize for each arcinducing transition with the same composition function in Equation (1) with vhead of the head token and vdep of the dependent token of the arc to be induced.",A Model and Training Details,[0],[0]
"For Shift, we simply combine vhead and vdep of the leftmost token in the buffer with the biaffine combination, and obtain its score by computing the inner-product of the feature and a vector.",A Model and Training Details,[0],[0]
"At each step, the scores of all feasible transitions are normalized to a probability distribution by a softmax function.
",A Model and Training Details,[0],[0]
"In all of our experiments, the parsers are trained to maximize the log likelihood of the desired transition sequence, along with the tagger being trained to maximize the log likelihood of the correct POS tag for each token.
",A Model and Training Details,[0],[0]
"To train the parsers, we use the ADAM optimizer (Kingma and Ba, 2014), with β2 = 0.9, an initial learning rate of 0.001, and minibatches of size 32 sentences.",A Model and Training Details,[0],[0]
Parsers are trained for 10 passes through the dataset on PTB-SD.,A Model and Training Details,[0],[0]
We also find that annealing the learning rate by a factor of 0.5 for every pass after the 5th helped improve performance.,A Model and Training Details,[0],[0]
"For EN-UD, we train for 30 passes, and anneal the learning rate for every 3 passes after the 15th due to the smaller size of the dataset.",A Model and Training Details,[0],[0]
"For all of the biaffine combination layers and dense layers, we dropout their units with a small probability of 5%.",A Model and Training Details,[0],[0]
"Also during training time, we randomly replace 10% of the input words by an artificial 〈UNK〉 token, which is then used to replace
all unseen words in the development and test sets.",A Model and Training Details,[0],[0]
"Finally, we repeat each experiment with 3 independent random initializations, and use the average result for reporting and statistical significance tests.
",A Model and Training Details,[0],[0]
The code for the full specification of our models and aforementioned training details are available at https://github.com/qipeng/ arc-swift.,A Model and Training Details,[0],[0]
Transition-based dependency parsers often need sequences of local shift and reduce operations to produce certain attachments.,abstractText,[0],[0]
Correct individual decisions hence require global information about the sentence context and mistakes cause error propagation.,abstractText,[0],[0]
"This paper proposes a novel transition system, arc-swift, that enables direct attachments between tokens farther apart with a single transition.",abstractText,[0],[0]
This allows the parser to leverage lexical information more directly in transition decisions.,abstractText,[0],[0]
"Hence, arc-swift can achieve significantly better performance with a very small beam size.",abstractText,[0],[0]
Our parsers reduce error by 3.7–7.6% relative to those using existing transition systems on the Penn Treebank dependency parsing task and English Universal Dependencies.,abstractText,[0],[0]
Arc-swift: A Novel Transition System for Dependency Parsing,title,[0],[0]
"1 Are BLEU and Meaning Representation in Opposition?
One of possible ways of obtaining continuous-space sentence representations is by training neural machine translation (NMT) systems. The recent attention mechanism however removes the single point in the neural network from which the source sentence representation can be extracted. We propose several variations of the attentive NMT architecture bringing this meeting point back. Empirical evaluation suggests that the better the translation quality, the worse the learned sentence representations serve in a wide range of classification and similarity tasks.",text,[0],[0]
Deep learning has brought the possibility of automatically learning continuous representations of sentences.,1 Introduction,[0],[0]
"On the one hand, such representations can be geared towards particular tasks such as classifying the sentence in various aspects (e.g. sentiment, register, question type) or relating the sentence to other sentences (e.g. semantic similarity, paraphrasing, entailment).",1 Introduction,[0],[0]
"On the other hand, we can aim at “universal” sentence representations, that is representations performing reasonably well in a range of such tasks.
",1 Introduction,[0],[0]
"Regardless the evaluation criterion, the representations can be learned either in an unsupervised way (from simple, unannotated texts) or supervised, relying on manually constructed training sets of sentences equipped with annotations of the appropriate type.",1 Introduction,[0],[0]
"A different approach is to obtain sentence representations from training neural machine translation models (Hill et al., 2016).
",1 Introduction,[0],[0]
"Since Hill et al. (2016), NMT has seen substantial advances in translation quality and it is thus
natural to ask how these improvements affect the learned representations.
",1 Introduction,[0],[0]
"One of the key technological changes was the introduction of “attention” (Bahdanau et al., 2014), making it even the very central component in the network (Vaswani et al., 2017).",1 Introduction,[0],[0]
Attention allows the NMT system to dynamically choose which parts of the source are most important when deciding on the current output token.,1 Introduction,[0],[0]
"As a consequence, there is no longer a static vector representation of the sentence available in the system.
",1 Introduction,[0],[0]
"In this paper, we remove this limitation by proposing a novel encoder-decoder architecture with a structured fixed-size representation of the input that still allows the decoder to explicitly focus on different parts of the input.",1 Introduction,[0],[0]
"In other words, our NMT system has both the capacity to attend to various parts of the input and to produce static representations of input sentences.
",1 Introduction,[0],[0]
"We train this architecture on English-to-German and English-to-Czech translation and evaluate the learned representations of English on a wide range of tasks in order to assess its performance in learning “universal” meaning representations.
",1 Introduction,[0],[0]
"In Section 2, we briefly review recent efforts in obtaining sentence representations.",1 Introduction,[0],[0]
"In Section 3, we introduce a number of variants of our novel architecture.",1 Introduction,[0],[0]
Section 4 describes some standard and our own methods for evaluating sentence representations.,1 Introduction,[0],[0]
Section 5 then provides experimental results: translation and representation quality.,1 Introduction,[0],[0]
The relation between the two is discussed in Section 6.,1 Introduction,[0],[0]
The properties of continuous sentence representations have always been of interest to researchers working on neural machine translation.,2 Related Work,[0],[0]
"In the first works on RNN sequence-to-sequence models, Cho et al. (2014) and Sutskever et al. (2014)
ar X
iv :1
80 5.
06 53
6v 1
[ cs
.C",2 Related Work,[0],[0]
"L
] 1
6 M
ay 2
01 8
2
provided visualizations of the phrase and sentence embedding spaces and observed that they reflect semantic and syntactic structure to some extent.",2 Related Work,[0],[0]
"Hill et al. (2016) perform a systematic evaluation of sentence representation in different models, including NMT, by applying them to various sentence classification tasks and by relating semantic similarity to closeness in the representation space.",2 Related Work,[0],[0]
"Shi et al. (2016) investigate the syntactic properties of representations learned by NMT systems by predicting sentence- and word-level syntactic labels (e.g. tense, part of speech) and by generating syntax trees from these representations.",2 Related Work,[0],[0]
Schwenk and Douze (2017) aim to learn language-independent sentence representations using NMT systems with multiple source and target languages.,2 Related Work,[0],[0]
They do not consider the attention mechanism and evaluate primarily by similarity scores of the learned representations for similar sentences (within or across languages).,2 Related Work,[0],[0]
"Our proposed model architectures differ in (a) which encoder states are considered in subsequent processing, (b) how they are combined, and (c) how they are used in the decoder.",3 Model Architectures,[0],[0]
Table 1 summarizes all the examined configurations of RNN-based models.,3 Model Architectures,[0],[0]
"The first three (ATTN, FINAL, FINAL-CTX) correspond roughly to the standard sequence-to-sequence models, Bahdanau et al. (2014), Sutskever et al. (2014) and Cho et al. (2014), respectively.",3 Model Architectures,[0],[0]
"The last column (ATTN-ATTN) is our main proposed architecture: compound attention, described here in Section 3.1.",3 Model Architectures,[0],[0]
"In addition to RNN-based models, we experiment with the Transformer model, see Section 3.3.",3 Model Architectures,[0],[0]
Our compound attention model incorporates attention in both the encoder and the decoder.,3.1 Compound Attention,[0],[0]
Its architecture is shown in Fig. 1.,3.1 Compound Attention,[0],[0]
Encoder with inner attention.,3.1 Compound Attention,[0],[0]
"First, we process the input sequence x1, x2, . . .",3.1 Compound Attention,[0],[0]
", xT using a bidirectional recurrent network with gated recurrent units (GRU, Cho et al., 2014):",3.1 Compound Attention,[0],[0]
"−→ ht = −−→ GRU(xt, −−→ ht−1), ←− ht = ←−− GRU(xt, ←−− ht+1), ht =",3.1 Compound Attention,[0],[0]
"[ −→ ht , ←− ht ].
",3.1 Compound Attention,[0],[0]
3,3.1 Compound Attention,[0],[0]
"We denote by u the combined number of units in the two RNNs, i.e. the dimensionality of ht.",3.1 Compound Attention,[0],[0]
"Next, our goal is to combine the states (h1, h2, . . .",3.1 Compound Attention,[0],[0]
", hT )",3.1 Compound Attention,[0],[0]
= H of the encoder into a vector of fixed dimensionality that represents the entire sentence.,3.1 Compound Attention,[0],[0]
Traditional seq2seq models concatenate the final states of both encoder RNNs ( −→ hT and ←− h1) to obtain the sentence representation (denoted as FINAL in Table 1).,3.1 Compound Attention,[0],[0]
"Another option is to combine all encoder states using the average or maximum over time (Collobert and Weston, 2008; Schwenk and Douze, 2017) (AVGPOOL and MAXPOOL in Table 1 and following).",3.1 Compound Attention,[0],[0]
"We adopt an alternative approach, which is to use inner attention1 (Liu et al., 2016; Li et al., 2016) to compute several weighted averages of the encoder states (Lin et al., 2017).",3.1 Compound Attention,[0],[0]
"The main motivation for incorporating these multiple “views” of the state sequence is that it removes the need for the RNN cell to accumulate the representation of the whole sentence as it processes the input, and therefore it should have more capacity for modeling local dependencies.",3.1 Compound Attention,[0],[0]
"Specifically, we fix a number r, the number of attention heads, and compute an r×T matrixA of attention weights αjt, representing the importance of position t in the input for the jth attention head.",3.1 Compound Attention,[0],[0]
"We then use this matrix to compute r weighted sums of the encoder states, which become the rows of a new matrix M : M = AH.",3.1 Compound Attention,[0],[0]
(1) A vector representation of the source sentence (the “sentence embedding”) can be obtained by flattening the matrix M .,3.1 Compound Attention,[0],[0]
"In our experiments, we project the encoder states h1, h2, . . .",3.1 Compound Attention,[0],[0]
", hT down to a given dimensionality before applying Eq.",3.1 Compound Attention,[0],[0]
"(1), so that we can control the size of the representation.",3.1 Compound Attention,[0],[0]
"Following Lin et al. (2017), we compute the attention matrix by feeding the encoder states to a two-layer feed-forward network: A = softmax(U tanh(WH>)), (2) where W and U are weight matrices of dimensions d× u and r × d, respectively (d is the number of hidden units); the softmax function is applied along the second dimension, i.e. across the encoder states.",3.1 Compound Attention,[0],[0]
1Some papers call the same or similar approach selfattention or single-time attention.,3.1 Compound Attention,[0],[0]
Attentive decoder.,3.1 Compound Attention,[0],[0]
"In vanilla seq2seq models with a fixed-size sentence representation, the decoder is usually conditioned on this representation via the initial RNN state.",3.1 Compound Attention,[0],[0]
We propose to instead leverage the structured sentence embedding by applying attention to its components.,3.1 Compound Attention,[0],[0]
"This is no different from the classical attention mechanism used in NMT (Bahdanau et al., 2014), except that it acts on this fixed-size representation instead of the sequence of encoder states.",3.1 Compound Attention,[0],[0]
"In the ith decoding step, the attention mechanism computes a distribution {βij}rj=1 over the r components of the structured representation.",3.1 Compound Attention,[0],[0]
"This is then used to weight these components to obtain the context vector ci, which in turn is used to update the decoder state.",3.1 Compound Attention,[0],[0]
"Again, we can write this in matrix form as C = BM, (3) where B = (βij) T ′,r i=1,j=1 is the attention matrix and C = (ci, c2, . . .",3.1 Compound Attention,[0],[0]
", cT ′) are the context vectors.",3.1 Compound Attention,[0],[0]
Note that by combining Eqs.,3.1 Compound Attention,[0],[0]
"(1) and (3), we get C = (BA)H. (4) Hence, the composition of the encoder and decoder attentions (the “compound attention”) defines an implicit alignment between the source and the target sequence.",3.1 Compound Attention,[0],[0]
"From this viewpoint, our model can be regarded as a restriction of the conventional attention model.",3.1 Compound Attention,[0],[0]
"The decoder uses a conditional GRU cell (cGRUatt; Sennrich et al., 2017), which consists of two consecutively applied GRU blocks.",3.1 Compound Attention,[0],[0]
"The first block processes the previous target token yi−1, while the second block receives the context vector ci and predicts the next target token yi.",3.1 Compound Attention,[0],[0]
"Compared to the FINAL model, the compound attention architecture described in the previous section undoubtedly benefits from the fact that the decoder is presented with information from the encoder (i.e. the context vectors ci) in every decoding step.",3.2 Constant Context,[0],[0]
"To investigate this effect, we include baseline models where we replace all context vectors ci with the entire sentence embedding (indicated by the suffix “-CTX” in Table 1).",3.2 Constant Context,[0],[0]
"Specifically, we provide either the flattened matrixM (for models with inner attention; ATTN or ATTN-CTX), the final state of the encoder (FINAL-CTX), or the
4
result of mean- or max-pooling (*POOL-CTX) as a constant input to the decoder cell.",3.2 Constant Context,[0],[0]
"The Transformer (Vaswani et al., 2017) is a recently proposed model based entirely on feedforward layers and attention.",3.3 Transformer with Inner Attention,[0],[0]
"It consists of an encoder and a decoder, each with 6 layers, consisting of multi-head attention on the previous layer and a position-wise feed-forward network.",3.3 Transformer with Inner Attention,[0],[0]
"In order to introduce a fixed-size sentence representation into the model, we modify it by adding inner attention after the last encoder layer.",3.3 Transformer with Inner Attention,[0],[0]
The attention in the decoder then operates on the components of this representation (i.e. the rows of the matrix M ).,3.3 Transformer with Inner Attention,[0],[0]
This variation on the Transformer model corresponds to the ATTN-ATTN column in Table 1 and is therefore denoted TRF-ATTN-ATTN.,3.3 Transformer with Inner Attention,[0],[0]
"Continuous sentence representations can be evaluated in many ways, see e.g. Kiros et al. (2015), Conneau et al. (2017) or the RepEval workshops.2 We evaluate our learned representations with classification and similarity tasks from SentEval (Section 4.1) and by examining clusters of sentence paraphrase representations (Section 4.2).",4 Representation Evaluation,[0],[0]
"We perform evaluation on 10 classification and 7 similarity tasks using the SentEval3 (Conneau et al., 2017) evaluation tool.",4.1 SentEval,[0],[0]
This is a superset of the tasks from Kiros et al. (2015).,4.1 SentEval,[0],[0]
"2https://repeval2017.github.io/ 3https://github.com/facebookresearch/ SentEval/
Table 2 describes the classification tasks (number of classes, data size, task type and an example) and Table 3 lists the similarity tasks.",4.1 SentEval,[0],[0]
The similarity (relatedness) datasets contain pairs of sentences labeled with a real-valued similarity score.,4.1 SentEval,[0],[0]
"A given sentence representation model is evaluated either by learning to directly predict this score given the respective sentence embeddings (“regression”), or by computing the cosine similarity of the embeddings (“similarity”) without the need of any training.",4.1 SentEval,[0],[0]
"In both cases, Pearson and Spearman correlation of the predictions with the gold ratings is reported.",4.1 SentEval,[0],[0]
See Dolan et al. (2004) for details on MRPC and Hill et al. (2016) for the remaining tasks.,4.1 SentEval,[0],[0]
We also evaluate the representation of paraphrases.,4.2 Paraphrases,[0],[0]
We use two paraphrase sources for this purpose: COCO and HyTER Networks.,4.2 Paraphrases,[0],[0]
"COCO (Common Objects in Context; Lin et al., 2014) is an object recognition and image captioning dataset, containing 5 captions for each image.",4.2 Paraphrases,[0],[0]
We extracted the captions from its validation set to form a set of 5 × 5k = 25k sentences grouped by the source image.,4.2 Paraphrases,[0],[0]
The average sentence length is 11 tokens and the vocabulary size is 9k types.,4.2 Paraphrases,[0],[0]
"HyTER Networks (Dreyer and Marcu, 2014)
5 are large finite-state networks representing a subset of all possible English translations of 102 Arabic and 102 Chinese sentences.",4.2 Paraphrases,[0],[0]
"The networks were built by manually based on reference sentences in Arabic, Chinese and English.",4.2 Paraphrases,[0],[0]
Each network contains up to hundreds of thousands of possible translations of the given source sentence.,4.2 Paraphrases,[0],[0]
"We randomly generated 500 translations for each source sentence, obtaining a corpus of 102k sentences grouped into 204 clusters, each containing 500 paraphrases.",4.2 Paraphrases,[0],[0]
The average length of the 102k English sentences is 28 tokens and the vocabulary size is 11k token types.,4.2 Paraphrases,[0],[0]
"For every model, we encode each dataset to obtain a set of sentence embeddings with cluster labels.",4.2 Paraphrases,[0],[0]
We then compute the following metrics: Cluster classification accuracy (denoted “Cl”).,4.2 Paraphrases,[0],[0]
"We remove 1 point (COCO) or half of the points (HyTER) from each cluster, and fit an LDA classifier on the rest.",4.2 Paraphrases,[0],[0]
We then compute the accuracy of the classifier on the removed points.,4.2 Paraphrases,[0],[0]
Nearest-neighbor paraphrase retrieval accuracy (NN).,4.2 Paraphrases,[0],[0]
"For each point, we find its nearest neighbor according to cosine or L2 distance, and count how often the neighbor lies in the same cluster as the original point.",4.2 Paraphrases,[0],[0]
Inverse Davies-Bouldin index (iDB).,4.2 Paraphrases,[0],[0]
"The Davies-Bouldin index (Davies and Bouldin, 1979) measures cluster separation.",4.2 Paraphrases,[0],[0]
"For every pair of clusters, we compute the ratio Rij of their combined scatter (average L2 distance to the centroid) Si + Sj and the L2 distance of their centroids dij , then average the maximum values for all clusters: Rij = Si + Sj dij (5) DB = 1 N N∑ i=1",4.2 Paraphrases,[0],[0]
"max j 6=i Rij (6) The lower the DB index, the better the separation.",4.2 Paraphrases,[0],[0]
"To match with the rest of our metrics, we take its inverse: iDB = 1DB .",4.2 Paraphrases,[0],[0]
"We trained English-to-German and English-toCzech NMT models using Neural Monkey4 (Helcl and Libovický, 2017a).",5 Experimental Results,[0],[0]
"In the following, we distinguish these models using the code of the target language, i.e. de or cs. 4https://github.com/ufal/neuralmonkey",5 Experimental Results,[0],[0]
"The de models were trained on the Multi30K multilingual image caption dataset (Elliott et al., 2016), extended by Helcl and Libovický (2017b), who acquired additional parallel data using backtranslation (Sennrich et al., 2016) and perplexitybased selection (Yasuda et al., 2008).",5 Experimental Results,[0],[0]
"This extended dataset contains 410k sentence pairs, with the average sentence length of 12 ± 4 tokens in English.",5 Experimental Results,[0],[0]
We train each model for 20 epochs with the batch size of 32.,5 Experimental Results,[0],[0]
We truecased the training data as well as all data we evaluate on.,5 Experimental Results,[0],[0]
"For German, we employed Neural Monkey’s reversible pre-processing scheme, which expands contractions and performs morphological segmentation of determiners.",5 Experimental Results,[0],[0]
We used a vocabulary of at most 30k tokens for each language (no subword units).,5 Experimental Results,[0],[0]
"The cs models were trained on CzEng 1.7 (Bojar et al.,",5 Experimental Results,[0],[0]
"2016).5 We used byte-pair encoding (BPE) with a vocabulary of 30k sub-word units, shared for both languages.",5 Experimental Results,[0],[0]
"For English, the average sentence length is 15±19 BPE tokens and the original vocabulary size is 1.9M. We performed 1 training epoch with the batch size of 128 on the entire training section (57M sentence pairs).",5 Experimental Results,[0],[0]
"The datasets for both de and cs models come with their respective development and test sets of sentence pairs, which we use for the evaluation of translation quality.",5 Experimental Results,[0],[0]
(We use 1k randomly selected sentence pairs from CzEng 1.7 dtest as a development set.,5 Experimental Results,[0],[0]
"For evaluation, we use the entire etest.)",5 Experimental Results,[0],[0]
"We also evaluate the InferSent model6 (Conneau et al., 2017) as pre-trained on the natural language inference (NLI) task.",5 Experimental Results,[0],[0]
InferSent has been shown to achieve state-of-the-art results on the SentEval tasks.,5 Experimental Results,[0],[0]
"We also include a bag-ofwords baseline (GloVe-BOW) obtained by averaging GloVe7 word vectors (Pennington et al., 2014).",5 Experimental Results,[0],[0]
"We estimate translation quality of the various models using single-reference case-sensitive BLEU (Papineni et al., 2002) as implemented in Neural Monkey (the reference implementation is mteval-v13a.pl from NIST or Moses).",5.1 Translation Quality,[0],[0]
Tables 4 and 5 provide the results on the two datasets.,5.1 Translation Quality,[0],[0]
The cs dataset is much larger and the training takes much longer.,5.1 Translation Quality,[0],[0]
"We were thus able 5http://ufal.mff.cuni.cz/czeng/czeng17 6https://github.com/facebookresearch/ InferSent 7https://nlp.stanford.edu/projects/ glove/
6
to experiment with only a subset of the possible model configurations.",5.1 Translation Quality,[0],[0]
The columns “Size” and “Heads” specify the total size of sentence representation and the number of heads of encoder inner attention.,5.1 Translation Quality,[0],[0]
"In both cases, the best performing is the ATTN Bahdanau et al. model, followed by Transformer (de only) and our ATTN-ATTN (compound attention).",5.1 Translation Quality,[0],[0]
"The non-attentive FINAL Cho et al. is the worst, except cs-MAXPOOL.",5.1 Translation Quality,[0],[0]
"For 5 selected cs models, we also performed the WMT-style 5-way manual ranking on 200 sentence pairs.",5.1 Translation Quality,[0],[0]
The annotations are interpreted as simulated pairwise comparisons.,5.1 Translation Quality,[0],[0]
"For each model, the final score is the number of times the model was judged better than the other model in the pair.",5.1 Translation Quality,[0],[0]
Tied pairs are excluded.,5.1 Translation Quality,[0],[0]
"The results, shown in Table 5, confirm the automatic evaluation results.",5.1 Translation Quality,[0],[0]
"We also checked the relation between BLEU
and the number of heads and representation size.",5.1 Translation Quality,[0],[0]
"While there are many exceptions, the general tendency is that the larger the representation or the more heads, the higher the BLEU score.",5.1 Translation Quality,[0],[0]
The Pearson correlation between BLEU and the number of heads is 0.87 for cs and 0.31 for de.,5.1 Translation Quality,[0],[0]
"Due to the large number of SentEval tasks, we present the results abridged in two different ways: by reporting averages (Table 6) and by showing only the best models in comparison with other methods (Table 7).",5.2 SentEval,[0],[0]
The full results can be found in the supplementary material.,5.2 SentEval,[0],[0]
"Table 6 provides averages of the classification and similarity results, along with the results of selected tasks (SNLI, SICK-E).",5.2 SentEval,[0],[0]
"As the baseline for classifications tasks, we assign the most frequent class to all test examples.8",5.2 SentEval,[0],[0]
"The de models are generally worse, most likely due to the higher OOV rate and overall simplicity of the training sentences.",5.2 SentEval,[0],[0]
"On cs, we see a clear pattern that more heads hurt the performance.",5.2 SentEval,[0],[0]
The de set has more variations to consider but the results are less conclusive.,5.2 SentEval,[0],[0]
"For the similarity results, it is worth noting that cs-ATTN-ATTN performs very well with 1 attention head but fails miserably with more heads.",5.2 SentEval,[0],[0]
"Otherwise, the relation to the number of heads is less clear.",5.2 SentEval,[0],[0]
Table 7 compares our strongest models with other approaches on all tasks.,5.2 SentEval,[0],[0]
"Besides InferSent and GloVe-BOW, we include SkipThought as evaluated by Conneau et al. (2017), and the NMTbased embeddings by Hill et al. (2016) trained on the English-French WMT15 dataset (this is the best result reported by Hill et al. for NMT).",5.2 SentEval,[0],[0]
We see that the supervised InferSent clearly outperforms all other models in all tasks except for MRPC and TREC.,5.2 SentEval,[0],[0]
"Results by Hill et al. are always lower than our best setups, except MRPC.",5.2 SentEval,[0],[0]
"On classification tasks, our models are outperformed even by GloVe-BOW, except for the NLI tasks (SICK-E and SNLI) where cs-FINAL-CTX is better.",5.2 SentEval,[0],[0]
Table 6 also provides our measurements based on sentence paraphrases.,5.3 Paraphrase Scores,[0],[0]
"For paraphrase retrieval 8For MR, CR, SUBJ, and MPQA, where there is no distinct test set, the class is established on the whole collection.",5.3 Paraphrase Scores,[0],[0]
"For other tasks, the class is learned from the training set.
7
8
B L
E U
M R C R S U
B J
M P
Q A
S S
T 2
S S
T 5
T R
E C
M R
P C
S IC
K -E
S N
L I",5.3 Paraphrase Scores,[0],[0]
A v g,5.3 Paraphrase Scores,[0],[0]
A cc S IC K -R S T S B S T S 1 2 S T S 1 3 S T S 1 4 S T S 1 5 S T S 1 6 A v g S,5.3 Paraphrase Scores,[0],[0]
im H y -C l H y -N N H y -i D B C O -C,5.3 Paraphrase Scores,[0],[0]
l C O -N N C,5.3 Paraphrase Scores,[0],[0]
"O -i D B
BLEU MR CR
SUBJ MPQA
SST2 SST5 TREC MRPC
SICK-E SNLI AvgAcc SICK-R
STSB STS12 STS13",5.3 Paraphrase Scores,[0],[0]
"STS14 STS15 STS16 AvgSim Hy-Cl Hy-NN Hy-iDB CO-Cl CO-NN CO-iDB
−1.00 −0.75 −0.50 −0.25 0.00 0.25 0.50 0.75 1.00
Figure 2: Pearson correlations.",5.3 Paraphrase Scores,[0],[0]
"Upper triangle: de models, lower triangle: cs models.",5.3 Paraphrase Scores,[0],[0]
Positive values shown in shades of green.,5.3 Paraphrase Scores,[0],[0]
"For similarity tasks, only the Pearson (not Spearman) coefficient is represented.
",5.3 Paraphrase Scores,[0],[0]
"(NN), we found cosine distance to work better than L2 distance.",5.3 Paraphrase Scores,[0],[0]
"We therefore do not list or further consider L2-based results (except in the supplementary material).
",5.3 Paraphrase Scores,[0],[0]
"This evaluation seems less stable and discerning than the previous two, but we can again confirm the victory of InferSent followed by our nonattentive cs models.",5.3 Paraphrase Scores,[0],[0]
cs and de models are no longer clearly separated.,5.3 Paraphrase Scores,[0],[0]
"To assess the relation between the various measures of sentence representations and translation quality as estimated by BLEU, we plot a heatmap of Pearson correlations in Fig. 2.",6 Discussion,[0],[0]
"As one example, Fig. 3 details the cs models’ BLEU scores and AvgAcc (average of SentEval accuracies).
",6 Discussion,[0],[0]
"A good sign is that on the cs dataset, most metrics of representation are positively correlated (the pairwise Pearson correlation is 0.78± 0.32 on average), the outlier being TREC (−0.16±0.16 correlation with the other metrics on average)
",6 Discussion,[0],[0]
"On the other hand, most representation metrics correlate with BLEU negatively (−0.57±0.31) on cs.",6 Discussion,[0],[0]
"The pattern is less pronounced but still clear also on the de dataset.
",6 Discussion,[0],[0]
"A detailed understanding of what the learned
representations contain is difficult.",6 Discussion,[0],[0]
"We can only speculate that if the NMT model has some capability for following the source sentence superficially, it will use it and spend its capacity on closely matching the target sentences rather than on deriving some representation of meaning which would reflect e.g. semantic similarity.",6 Discussion,[0],[0]
We assume that this can be a direct consequence of NMT being trained for cross entropy: putting the exact word forms in exact positions as the target sentence requires.,6 Discussion,[0],[0]
"Performing well in single-reference BLEU is not an indication that the system understands the meaning but rather that it can maximize the chance of producing the n-grams required by the reference.
",6 Discussion,[0],[0]
"The negative correlation between the number of attention heads and the representation metrics from Fig. 3 (−0.81±0.12 for cs and−0.18±0.19 for de, on average) can be partly explained by the following observation.",6 Discussion,[0],[0]
We plotted the induced alignments (e.g. Fig. 4) and noticed that the heads tend to “divide” the sentence into segments.,6 Discussion,[0],[0]
"While one would hope that the segments correspond to some meaningful units of the sentence (e.g. subject, predicate, object), we failed to find any such interpretation for ATTN-ATTN and for cs models in general.",6 Discussion,[0],[0]
"Instead, the heads divide the source sentence more or less equidistantly, as documented by Fig. 5.",6 Discussion,[0],[0]
"Such a multi-headed sentence representation is then less fit for representing e.g. paraphrases where the subject and object swap their position due to passivization, because their representations are then accessed by different heads, and thus end up in different parts of the sentence embedding vector.
9
For de-ATTN-CTX models, we observed a much flatter distribution of attention weights for each head and, unlike in the other models, we were often able to identify a head focusing on the main verb.",6 Discussion,[0],[0]
"This difference between ATTN-ATTN and some ATTN-CTX models could be explained by the fact that in the former, the decoder is oblivious to the ordering of the heads (because of decoder attention), and hence it may not be useful for a given head to look for a specific syntactic or semantic role.",6 Discussion,[0],[0]
"We presented a novel variation of attentive NMT models (Bahdanau et al., 2014; Vaswani et al., 2017) that again provides a single meeting point with a continuous representation of the source sen-
tence.",7 Conclusion,[0],[0]
We evaluated these representations with a number of measures reflecting how well the meaning of the source sentence is captured.,7 Conclusion,[0],[0]
"While our proposed “compound attention” leads to translation quality not much worse than the fully attentive model, it generally does not perform well in the meaning representation.",7 Conclusion,[0],[0]
"Quite on the contrary, the better the BLEU score, the worse the meaning representation.",7 Conclusion,[0],[0]
"We believe that this observation is important for representation learning where bilingual MT now seems less likely to provide useful data, but perhaps more so for MT itself, where the struggle towards a high single-reference BLEU score (or even worse, cross entropy) leads to systems that refuse to consider the meaning of the sentence.",7 Conclusion,[0],[0]
"This work has been supported by the grants 18-24210S of the Czech Science Foundation, SVV 260 453 and “Progress” Q18+Q48 of Charles University, and using language resources distributed by the LINDAT/CLARIN project of the Ministry of Education, Youth and Sports of the Czech Republic (projects LM2015071 and OP VVV VI CZ.02.1.01/0.0/0.0/16 013/0001781).
",Acknowledgement,[0],[0]
10,Acknowledgement,[0],[0]
One of possible ways of obtaining continuous-space sentence representations is by training neural machine translation (NMT) systems.,abstractText,[0],[0]
The recent attention mechanism however removes the single point in the neural network from which the source sentence representation can be extracted.,abstractText,[0],[0]
We propose several variations of the attentive NMT architecture bringing this meeting point back.,abstractText,[0],[0]
"Empirical evaluation suggests that the better the translation quality, the worse the learned sentence representations serve in a wide range of classification and similarity tasks.",abstractText,[0],[0]
Are BLEU and Meaning Representation in Opposition?,title,[0],[0]
"Argument mining consists of the automatic identification of argumentative structures in documents, a valuable task with applications in policy making, summarization, and education, among others.",1 Introduction,[0],[0]
The argument mining task includes the tightly-knit subproblems of classifying propositions into elementary unit types and detecting argumentative relations between the elementary units.,1 Introduction,[0],[0]
"The desired output is a document argumentation graph structure, such as the one in Figure 1, where propositions are denoted by letter subscripts, and the associated argumentation graph shows their types and support relations between them.
",1 Introduction,[0],[0]
"Most annotation and prediction efforts in argument mining have focused on tree or forest structures (Peldszus and Stede, 2015; Stab and Gurevych, 2016), constraining argument structures to form one or more trees.",1 Introduction,[0],[0]
"This makes the problem computationally easier by enabling the use of maximum spanning tree–style parsing ap-
proaches.",1 Introduction,[0],[0]
"However, argumentation in the wild can be less well-formed.",1 Introduction,[0],[0]
"The argument put forth in Figure 1, for instance, consists of two components: a simple tree structure and a more complex graph structure (c jointly supports b and d).",1 Introduction,[0],[0]
"In this work, we design a flexible and highly expressive structured prediction model for argument mining, jointly learning to classify elementary units (henceforth propositions) and to identify the argumentative relations between them (henceforth links).",1 Introduction,[0],[0]
"By formulating argument mining as inference in a factor graph (Kschischang et al., 2001), our model (described in Section 4) can account for correlations between the two tasks, can consider second order link structures (e.g., in Figure 1, c → b → a), and can impose arbitrary constraints (e.g., transitivity).
",1 Introduction,[0],[0]
"To parametrize our models, we evaluate two alternative directions: linear structured SVMs
1We describe proposition types (FACT, etc.)",1 Introduction,[0],[0]
"in Section 3.
ar X
iv :1
70 4.
06 86
9v 1
[ cs
.C",1 Introduction,[0],[0]
"L
] 2
3 A
pr 2
01 7
(Tsochantaridis et al., 2005), and recurrent neural networks with structured loss, extending (Kiperwasser and Goldberg, 2016).",1 Introduction,[0],[0]
"Interestingly, RNNs perform poorly when trained with classification losses, but become competitive with the featureengineered structured SVMs when trained within our proposed structured learning model.
",1 Introduction,[0],[0]
We evaluate our approach on two argument mining datasets.,1 Introduction,[0],[0]
"Firstly, on our new Cornell eRulemaking Corpus – CDCP,2 consisting of argument annotations on comments from an eRulemaking discussion forum, where links don’t always form trees (Figure 1 shows an abridged example comment, and Section 3 describes the dataset in more detail).",1 Introduction,[0],[0]
"Secondly, on the UKP argumentative essays v2 (henceforth UKP), where argument graphs are annotated strictly as multiple trees (Stab and Gurevych, 2016).",1 Introduction,[0],[0]
"In both cases, the results presented in Section 5 confirm that our models outperform unstructured baselines.",1 Introduction,[0],[0]
"On UKP, we improve link prediction over the best reported result in (Stab and Gurevych, 2016), which is based on integer linear programming postprocessing.",1 Introduction,[0],[0]
"For insight into the strengths and weaknesses of the proposed models, as well as into the differences between SVM and RNN parameterizations, we perform an error analysis in Section 5.1.",1 Introduction,[0],[0]
"To support argument mining research, we also release our Python implementation, Marseille.3",1 Introduction,[0],[0]
Our factor graph formulation draws from ideas previously used independently in parsing and argument mining.,2 Related work,[0],[0]
"In particular, maximum spanning tree (MST) methods for arc-factored dependency parsing have been successfully used by McDonald et al. (2005) and applied to argument mining with mixed results by Peldszus and Stede (2015).",2 Related work,[0],[0]
"As they are not designed for the task, MST parsers cannot directly handle proposition classification or model the correlation between proposition and link prediction—a limitation our model addresses.",2 Related work,[0],[0]
"Using RNN features in an MST parser with a structured loss was proposed by Kiperwasser and Goldberg (2016); their model can be seen as a particular case of our factor graph approach, limited to link prediction with a tree structure constraint.",2 Related work,[0],[0]
"Our models support multi-task learning for proposition classification, parameter-
2Dataset available at http://joonsuk.org.",2 Related work,[0],[0]
"3Available at https://github.com/vene/marseille.
",2 Related work,[0],[0]
"izing adjacent links with higher-order structures (e.g., c → b → a) and enforcing arbitrary constraints on the link structure, not limited to trees.",2 Related work,[0],[0]
"Such higher-order structures and logic constraints have been successfully used for dependency and semantic parsing by Martins et al. (2013) and Martins and Almeida (2014); to our knowledge we are the first to apply them to argument mining, as well as the first to parametrize them with neural networks.",2 Related work,[0],[0]
"Stab and Gurevych (2016) used an integer linear program to combine the output of independent proposition and link classifiers using a hand-crafted scoring formula, an approach similar to our baseline.",2 Related work,[0],[0]
"Our factor graph method can combine the two tasks in a more principled way, as it fully learns the correlation between the two tasks without relying on hand-crafted scoring, and therefore can readily be applied to other argumentation datasets.",2 Related work,[0],[0]
"Furthermore, our model can enforce the tree structure constraint, required on the UKP dataset, using MST cycle constraints used by Stab and Gurevych (2016), thanks to the AD3 inference algorithm (Martins et al., 2015).
",2 Related work,[0],[0]
"Sequence tagging has been applied to the related structured tasks of proposition identification and classification (Stab and Gurevych, 2016; Habernal and Gurevych, 2016; Park et al., 2015b); integrating such models is an important next step.",2 Related work,[0],[0]
"Meanwhile, a new direction in argument mining explores pointer networks (Potash et al., 2016); a promising method, currently lacking support for tree structures and domain-specific constraints.",2 Related work,[0],[0]
"We release a new argument mining dataset consisting of user comments about rule proposals regarding Consumer Debt Collection Practices (CDCP) by the Consumer Financial Protection Bureau collected from an eRulemaking website, http:// regulationroom.org.
Argumentation structures found in web discussion forums, such as the eRulemaking one we use, can be more free-form than the ones encountered in controlled, elicited writing such as (Peldszus and Stede, 2015).",3 Data,[0],[0]
"For this reason, we adopt the model proposed by Park et al. (2015a), which does not constrain links to form tree structures, but unrestricted directed graphs.",3 Data,[0],[0]
"Indeed, over 20% of the comments in our dataset exhibit local structures that would not be allowable in a tree.",3 Data,[0],[0]
"Possible link types are reason and evidence, and propo-
sition types are split into five fine-grained categories: POLICY and VALUE contain subjective judgements/interpretations, where only the former specifies a specific course of action to be taken.",3 Data,[0],[0]
"On the other hand, TESTIMONY and FACT do not contain subjective expressions, the former being about personal experience, or “anecdotal.”",3 Data,[0],[0]
"Lastly, REFERENCE covers URLs and citations, which are used to point to objective evidence in an online setting.
",3 Data,[0],[0]
"In comparison, the UKP dataset (Stab and Gurevych, 2016) only makes the syntactic distinction between CLAIM, MAJOR CLAIM, and PREMISE types, but it also includes attack links.",3 Data,[0],[0]
"The permissible link structure is stricter in UKP, with links constrained in annotation to form one or more disjoint directed trees within each paragraph.",3 Data,[0],[0]
"Also, since web arguments are not necessarily fully developed, our dataset has many argumentative propositions that are not in any argumentation relations.",3 Data,[0],[0]
"In fact, it isn’t unusual for comments to have no argumentative links at all: 28% of CDCP comments have no links, unlike UKP, where all essays have complete argument structures.",3 Data,[0],[0]
"Such comments with no links make the problem harder, emphasizing the importance of capturing the lack of argumentative support, not only its presence.",3 Data,[0],[0]
"Each user comment was annotated by two annotators, who independently annotated the boundaries and types of propositions, as well as the links among them.4 To produce the final corpus, a third annotator manually resolved the conflicts,5 and two automatic preprocessing steps were applied: we take the link transitive closure, and we remove a small number of nested propositions.6 The resulting dataset contains 731 comments, consisting of about 3800 sentences (≈4700 propositions) and 88k words.",3.1 Annotation results,[0],[0]
"Out of the 43k possible pairs of propositions, links are present between only 1300 (roughly 3%).",3.1 Annotation results,[0],[0]
"In comparison, UKP has fewer documents (402), but they are longer, with a total of 7100 sentences (6100 propositions) and 147k
4The annotators used the GATE annotation tool (Cunningham et al., 2011).
",3.1 Annotation results,[0],[0]
"5Inter-annotator agreement is measured with Krippendorf’s α (Krippendorff, 1980) with respect to elementary unit type (α=64.8%) and links (α=44.1%).",3.1 Annotation results,[0],[0]
"A separate paper describing the dataset is under preparation.
",3.1 Annotation results,[0],[0]
"6When two propositions overlap, we keep the one that results in losing the fewest links.",3.1 Annotation results,[0],[0]
"For generality, we release the dataset without this preprocessing, and include code to reproduce it; we believe that handling nested argumentative units is an important direction for further research.
words.",3.1 Annotation results,[0],[0]
"Since UKP links only occur within the same paragraph and propositions not connected to the argument are removed in a preprocessing step, link prediction is less imbalanced in UKP, with 3800 pairs of propositions being linked out of a total of 22k (17%).",3.1 Annotation results,[0],[0]
"We reserve a test set of 150 documents (973 propositions, 272 links) from CDCP, and use the provided 80-document test split from UKP (1266 propositions, 809 links).",3.1 Annotation results,[0],[0]
for argument mining,4 Structured learning,[0],[0]
"Binary and multi-class classification have been applied with some success to proposition and link prediction separately, but we seek a way to jointly learn the argument mining problem at the document level, to better model contextual dependencies and constraints.",4.1 Preliminaries,[0],[0]
"We therefore turn to structured learning, a framework that provides the desired level of expressivity.
",4.1 Preliminaries,[0],[0]
"In general, learning from a dataset of documents xi ∈ X and their associated labels yi ∈ Y involves seeking model parameters w that can “pick out” the best label under a scoring function f :
ŷ",4.1 Preliminaries,[0],[0]
":= argmaxy∈Y f(x, y;w).",4.1 Preliminaries,[0],[0]
"(1)
Unlike classification or regression, where X is usually a feature space Rd and Y ⊆ R (e.g., we predict an integer class index or a probability), in structured learning, more complex inputs and outputs are allowed.",4.1 Preliminaries,[0],[0]
"This makes the argmax in Equation 1 impossible to evaluate by enumeration, so it is desirable to find models that decompose over smaller units and dependencies between them; for instance, as factor graphs.",4.1 Preliminaries,[0],[0]
"In this section, we give a factor graph description of our proposed structured model for argument mining.",4.1 Preliminaries,[0],[0]
An input document is a string of words with proposition offsets delimited.,4.2 Model description,[0],[0]
"We denote the propositions in a document by {a, b, c, ...} and the possible directed link between a and b as a → b.",4.2 Model description,[0],[0]
The argument structure we seek to predict consists of the type of each proposition ya ∈ P and a binary label for each link ya→b ∈,4.2 Model description,[0],[0]
"R = {on, off}.7
7For simplicity and comparability, we follow Stab and Gurevych (2016) in using binary link labels even if links could be of different types.",4.2 Model description,[0],[0]
"This can be addressed in our model by incorporating “labeled link” factors.
",4.2 Model description,[0],[0]
The possible proposition types P differ for the two datasets; such differences are documented in Table 1.,4.2 Model description,[0],[0]
"As we describe the variables and factors constituting a document’s factor graph, we shall refer to Figure 2 for illustration.
",4.2 Model description,[0],[0]
Unary potentials.,4.2 Model description,[0],[0]
Each proposition a and each link a → b has a corresponding random variable in the factor graph (the circles in Figure 2).,4.2 Model description,[0],[0]
"To encode the model’s belief in each possible value for these variables, we parametrize the unary factors (gray boxes in Figure 2) with unary potentials: φ(a) ∈ R|P| is a score of ya for each possible proposition type.",4.2 Model description,[0],[0]
"Similarly, link unary potentials φ(a → b) ∈ R|R| are scores for ya→b being on/off.",4.2 Model description,[0],[0]
"Without any other factors, this would amount to independent classifiers for each task.
",4.2 Model description,[0],[0]
Compatibility factors.,4.2 Model description,[0],[0]
"For every possible link a → b, the variables (a, b, a → b) are bound by a dense factor scoring their joint assignment (the black boxes in Figure 2).",4.2 Model description,[0],[0]
"Such a factor could automatically learn to encourage links from compatible types (e.g., from TESTIMONY to POLICY) or discourage links between less compatible ones (e.g., from FACT to TESTIMONY).",4.2 Model description,[0],[0]
"In the simplest form, this factor would be parametrized as a tensor T ∈ R|P|×|P|×|R|, with tijk retaining the score of a source proposition of type i to be (k = on) or not to be (k = off) in a link with a proposition of type j. For more flexibility, we parametrize this factor with compatibility features depending
only on simple structure: tijk becomes a vector, and the score of configuration (i, j, k) is given by v>abtijk where vab consists of three binary features:
• bias: a constant value of 1, allowing T to learn a base score for a label configuration (i, j, k), as in the simple form above,
• adjacency: when there are no other propositions between the source and the target,
• order: when the source precedes the target.
",4.2 Model description,[0],[0]
Second order factors.,4.2 Model description,[0],[0]
Local argumentation graph structures such as a → b → c might be modeled better together rather than through separate link factors for a → b and b → c.,4.2 Model description,[0],[0]
"As in higher-order structured models for semantic and dependency parsing (Martins et al., 2013; Martins and Almeida, 2014), we implement three types of second order factors: grandparent (a → b → c), sibling (a ← b → c), and co-parent (a → b ← c).",4.2 Model description,[0],[0]
"Not all of these types of factors make sense on all datasets: as sibling structures cannot exist in directed trees, we don’t use sibling factors on UKP.",4.2 Model description,[0],[0]
"On CDCP, by transitivity, every grandparent structure implies a corresponding sibling, so it is sufficient to parametrize siblings.",4.2 Model description,[0],[0]
"This difference between datasets is emphasized in Figure 2, where one example of each type of factor is pictured on the right side of the graphs (orange boxes with curved edges): on CDCP we illustrate a coparent factor (top right) and a sibling factor (bot-
tom right), while on UKP we show a co-parent factor (top right) and a grandparent factor (bottom right).",4.2 Model description,[0],[0]
"We call these factors second order because they involve two link variables, scoring the joint assignment of both links being on.
",4.2 Model description,[0],[0]
Valid link structure.,4.2 Model description,[0],[0]
The global structure of argument links can be further constrained using domain knowledge.,4.2 Model description,[0],[0]
We implement this using constraint factors; these have no parameters and are denoted by empty boxes in Figure 2.,4.2 Model description,[0],[0]
"In general, well-formed arguments should be cycle-free.",4.2 Model description,[0],[0]
"In the UKP dataset, links form a directed forest and can never cross paragraphs.",4.2 Model description,[0],[0]
"This particular constraint can be expressed as a series of tree factors,8 one for each paragraph (the factor connected to all link variables in Figure 2).",4.2 Model description,[0],[0]
"In CDCP, links do not form a tree, but we use logic constraints to enforce transitivity (top left factor in Figure 2) and to prevent symmetry (bottom left); the logic formulas implemented by these factors are described in Table 1.",4.2 Model description,[0],[0]
"Together, the two constraints have the desirable side effect of preventing cycles.
",4.2 Model description,[0],[0]
Strict constraints.,4.2 Model description,[0],[0]
"We may include further domain-specific constraints into the model, to express certain disallowed configurations.",4.2 Model description,[0],[0]
"For instance, proposition types that appear in CDCP data can be ordered by the level of objectivity (Park et al., 2015a), as shown in Table 1.",4.2 Model description,[0],[0]
"In a wellformed argument, we would want to see links from more objective to equally or less objective propositions: it’s fine to provide FACT as reason for VALUE, but not the other way around.",4.2 Model description,[0],[0]
"While the training data sometimes violates this constraint, enforcing it might provide a useful inductive bias.
Inference.",4.2 Model description,[0],[0]
"The argmax in Equation 1 is a MAP over a factor graph with cycles and many overlapping factors, including logic factors.",4.2 Model description,[0],[0]
"While exact inference methods are generally unavailable, our setting is perfectly suited for the Alternating Directions Dual Decomposition (AD3) algorithm: approximate inference on expressive factor graphs with overlapping factors, logic constraints, and generic factors (e.g., directed tree factors) defined through maximization oracles (Martins et al., 2015).",4.2 Model description,[0],[0]
"When AD3 returns an integral solution, it is globally optimal, but when solutions are frac-
8A tree factor regards each bound variable as an edge in a graph and assigns −∞ scores to configurations that are not valid trees.",4.2 Model description,[0],[0]
"For inference, we can use maximum spanning arborescence algorithms such as Chu-Liu/Edmonds.
tional, several options are available.",4.2 Model description,[0],[0]
"At test time, for analysis, we retrieve exact solutions using the branch-and-bound method.",4.2 Model description,[0],[0]
"At training time, however, fractional solutions can be used as-is; this makes better use of each iteration and actually increases the ratio of integral solutions in future iterations, as well as at test time, as proven by Meshi et al. (2016).",4.2 Model description,[0],[0]
"We also find that after around 15 training iterations with fractional solutions, over 99% of inference calls are integral.
Learning.",4.2 Model description,[0],[0]
"We train the models by minimizing the structured hinge loss (Taskar et al., 2004):∑ (x,",4.2 Model description,[0],[0]
"y)∈D max y′∈Y (f(x, y′;w)",4.2 Model description,[0],[0]
+,4.2 Model description,[0],[0]
"ρ(y, y′))− f(x, y;w) (2) where ρ is a configurable misclassification cost.",4.2 Model description,[0],[0]
"The max in Equation 2 is not the same as the one used for prediction, in Equation 1.",4.2 Model description,[0],[0]
"However, when the cost function ρ decomposes over the variables, cost-augmented inference amounts to regular inference after augmenting the potentials accordingly.",4.2 Model description,[0],[0]
"We use a weighted Hamming cost:
ρ(y, ŷ)",4.2 Model description,[0],[0]
":= ∑ v ρ(yv)I[yv = ŷv]
where v is summed over all variables in a document {a} ∪ {a → b}, and ρ(yv) is a misclassification cost.",4.2 Model description,[0],[0]
"We assign uniform costs ρ to 1 for all mistakes except false-negative links, where we use higher cost proportional to the class imbalance in the training split, effectively giving more weight to positive links during training.",4.2 Model description,[0],[0]
"One option for parameterizing the potentials of the unary and higher-order factors is with linear models, using proposition, link, and higher-order features.",4.3 Argument structure SVM,[0],[0]
"This gives birth to a linear structured SVM (Tsochantaridis et al., 2005), which, when using l2 regularization, can be trained efficiently in the dual using the online block-coordinate FrankWolfe algorithm of Lacoste-Julien et al. (2013), as implemented in the pystruct library (Müller and Behnke, 2014).",4.3 Argument structure SVM,[0],[0]
"This algorithm is more convenient than subgradient methods, as it does not require tuning a learning rate parameter.
",4.3 Argument structure SVM,[0],[0]
Features.,4.3 Argument structure SVM,[0],[0]
"For unary proposition and link features, we faithfully follow Stab and Gurevych (2016, Tables 9 and 10): proposition features are
lexical (unigrams and dependency tuples), structural (token statistics and proposition location), indicators (from hand-crafted lexicons), contextual, syntactic (subclauses, depth, tense, modal, and POS), probability, discourse (Lin et al., 2014), and average GloVe embeddings (Pennington et al., 2014).",4.3 Argument structure SVM,[0],[0]
"Link features are lexical (unigrams), syntactic (POS and productions), structural (token statistics, proposition statistics and location features), hand-crafted indicators, discourse triples, PMI, and shared noun counts.
",4.3 Argument structure SVM,[0],[0]
"Our proposed higher-order factors for grandparent, co-parent, and sibling structures require features extracted from a proposition triplet a, b, c. In dependency and semantic parsing, higher-order factors capture relationships between words, so sparse indicator features can be efficiently used.",4.3 Argument structure SVM,[0],[0]
"In our case, since propositions consist of many words, BOW features may be too noisy and too dense; so for simplicity we again take a cue from the link-specific features used by Stab and Gurevych (2016).",4.3 Argument structure SVM,[0],[0]
"Our higher-order factor features are: same sentence indicators (for all 3 and for each pair), proposition order (one for each of the 6 possible orderings), Jaccard similarity (between all 3 and between each pair), presence of any shared nouns (between all 3 and between each pair), and shared noun ratios: nouns shared by all 3 divided by total nouns in each proposition and each pair, and shared nouns between each pair with respect to each proposition.",4.3 Argument structure SVM,[0],[0]
"Up to vocabulary size difference, our total feature dimensionality is approximately 7000 for propositions and 2100 for links.",4.3 Argument structure SVM,[0],[0]
"The number of second order features is 35.
Hyperparameters.",4.3 Argument structure SVM,[0],[0]
"We pick the SVM regularization parameter C ∈ {0.001, 0.003, 0.01, 0.03, 0.1, 0.3} by k-fold cross validation at document level, optimizing for the average between link and proposition F1 scores.",4.3 Argument structure SVM,[0],[0]
Neural network methods have proven effective for natural language problems even with minimalto-no feature engineering.,4.4 Argument structure RNN,[0],[0]
"Inspired by the use of LSTMs (Hochreiter and Schmidhuber, 1997) for MST dependency parsing by Kiperwasser and Goldberg (2016), we parametrize the potentials in our factor graph with an LSTM-based neural network,9 replacing MST inference with the more general AD3 algorithm, and using relaxed solutions for training when inference is inexact.
",4.4 Argument structure RNN,[0],[0]
"We extract embeddings of all words with a corpus frequency > 1, initialized with GloVe word vectors.",4.4 Argument structure RNN,[0],[0]
"We use a deep bidirectional LSTM to encode contextual information, representing a proposition a as the average of the LSTM outputs of its words, henceforth denoted ↔ a.
Proposition potentials.",4.4 Argument structure RNN,[0],[0]
"We apply a multi-layer perceptron (MLP) with rectified linear activations to each proposition, with all layer dimensions equal except the final output layer, which has size |P| and is not passed through any nonlinearities.
",4.4 Argument structure RNN,[0],[0]
Link potentials.,4.4 Argument structure RNN,[0],[0]
"To score a dependency a → b, Kiperwasser and Goldberg (2016) pass the concatenation",4.4 Argument structure RNN,[0],[0]
[ ↔ a; ↔ b ] through an MLP.,4.4 Argument structure RNN,[0],[0]
"After trying this, we found slightly better performance by first passing each proposition through a slot-specific
dense layer ( a := σsrc( ↔ a), b := σtrg( ↔ b) )
followed by a bilinear transformation:
φon(a→ b) := a > Wb+w>srca+w > trgb+ w",4.4 Argument structure RNN,[0],[0]
"(on) 0 .
",4.4 Argument structure RNN,[0],[0]
"Since the bilinear expression returns a scalar, but the link potentials must have a value for both the on and off states, we set the full potential to φ(a → b) := [φon(a → b), w(off)0 ] where w (off) 0 is a learned scalar bias.",4.4 Argument structure RNN,[0],[0]
"We initialize W to the diagonal identity matrix.
",4.4 Argument structure RNN,[0],[0]
"9We use the dynet library (Neubig et al., 2017).
",4.4 Argument structure RNN,[0],[0]
Second order potentials.,4.4 Argument structure RNN,[0],[0]
"Grandparent potentials φ(a → b → c) score two adjacent directed edges, in other words three propositions.",4.4 Argument structure RNN,[0],[0]
We again first pass each proposition representation through a slot-specific dense layer.,4.4 Argument structure RNN,[0],[0]
"We implement a multilinear scorer analogously to the link potentials:
φ(a→ b→ c) := ∑ i,j,k aibjckwijk
where W = (w)ijk is a third-order cube tensor.",4.4 Argument structure RNN,[0],[0]
"To reduce the large numbers of parameters, we implicitly represent W as a rank r tensor: wijk = ∑r s=1",4.4 Argument structure RNN,[0],[0]
u (1) is u (2),4.4 Argument structure RNN,[0],[0]
js u,4.4 Argument structure RNN,[0],[0]
(3) ks .,4.4 Argument structure RNN,[0],[0]
"Notably, this model captures only third-order interactions between the representation of the three propositions.",4.4 Argument structure RNN,[0],[0]
"To capture first-order “bias” terms, we could include slotspecific linear terms, e.g., w>a a; but to further capture quadratic backoff effects (for instance, if two propositions carry a strong signal of being siblings regardless of their parent), we would require quadratically many parameters.",4.4 Argument structure RNN,[0],[0]
"Instead of explicit lower-order terms, we propose augmenting a, b, and c with a constant feature of 1, which has approximately the same effect, while benefiting from the parameter sharing in the low-rank factorization; an effect described by Blondel et al. (2016).",4.4 Argument structure RNN,[0],[0]
"Siblings and co-parents factors are similarly parametrized with their own tensors.
",4.4 Argument structure RNN,[0],[0]
Hyperparameters.,4.4 Argument structure RNN,[0],[0]
"We perform grid search using k-fold document-level cross-validation, tuning the dropout probability in the dense MLP layers over {0.05, 0.1, 0.15, 0.2, 0.25} and the optimal number of passes over the training data over {10, 25, 50, 75, 100}.",4.4 Argument structure RNN,[0],[0]
"We use 2 layers for the LSTM and the proposition classifier, 128 hidden units in all layers, and a multilinear decomposition with rank r = 16, after preliminary CV runs.",4.4 Argument structure RNN,[0],[0]
We compare our proposed models to equivalent independent unary classifiers.,4.5 Baseline models,[0],[0]
"The unary-only version of a structured SVM is an l2-regularized linear SVM.10 For the RNN, we compute unary potentials in the same way as in the structured model, but apply independent hinge losses at each variable, instead of the global structured hinge loss.",4.5 Baseline models,[0],[0]
"Since the RNN weights are shared, this is a form of multi-task learning.",4.5 Baseline models,[0],[0]
"The baseline predictions can
10We train our SVM using SAGA (Defazio et al., 2014) in lightning (Blondel and Pedregosa, 2016).
be interpreted as unary potentials, therefore we can simply round their output to the highest scoring labels, or we can, alternatively, perform testtime inference, imposing the desired structure.",4.5 Baseline models,[0],[0]
We evaluate our proposed models on both datasets.,5 Results,[0],[0]
"For model selection and development we used kfold cross-validation at document level: on CDCP we set k = 3 to avoid small validation folds, while on UKP we follow Stab and Gurevych (2016) setting k = 5.",5 Results,[0],[0]
We compare our proposed structured learning systems (the linear structured SVM and the structured RNN) to the corresponding baseline versions.,5 Results,[0],[0]
"We organize our experiments in three incremental variants of our factor graph: basic, full, and strict, each with the following components:11
component basic full strict (baseline)
unaries X X X X compat.",5 Results,[0],[0]
factors X X X compat.,5 Results,[0],[0]
"features X X higher-order X X link structure X X X strict constraints X X
Following Stab and Gurevych (2016), we compute F1 scores at proposition and link level, and also report their average as a summary of overall performance.12 The results of a single prediction run on the test set are displayed in Table 2.",5 Results,[0],[0]
"The overall trend is that training using a structured objective is better than the baseline models, even when structured inference is applied on the baseline predictions.",5 Results,[0],[0]
"On UKP, for link prediction, the linear baseline can reach good performance when using inference, similar to the approach of Stab and Gurevych (2016), but the improvement in proposition prediction leads to higher overall F1 for the structured models.",5 Results,[0],[0]
"Meanwhile, on the more difficult CDCP setting, performing inference on the baseline output is not competitive.",5 Results,[0],[0]
"While feature engineering still outperforms our RNN model, we find that RNNs shine on proposition classification, especially on UKP, and that structured training can make them competitive, reducing their observed lag on link prediction (Katiyar and Cardie, 2016), possibly through mitigating class imbalance.
11Components are described in Section 4.",5 Results,[0],[0]
"The baselines with inference support only unaries and factors with no parameters, as indicated in the last column.
",5 Results,[0],[0]
"12For link F1 scores, however, we find it more intuitive to only consider retrieval of positive links rather than macroaveraged two-class scores.",5 Results,[0],[0]
Contribution of compatibility features.,5.1 Discussion and analysis,[0],[0]
The compatibility factor in our model can be visualized as conditional odds ratios given the source and target proposition types.,5.1 Discussion and analysis,[0],[0]
"Since there are only four possible configurations of the compatibility features, we can plot all cases in Figure 3, alongside the basic model.",5.1 Discussion and analysis,[0],[0]
"Not using compatibility features, the basic model can only learn whether certain configurations are more likely than others (e.g. a REFERENCE supporting another REFERENCE is unlikely, while a REFERENCE supporting a FACT is more likely; essentially a soft version of our domain-specific strict constraints.",5.1 Discussion and analysis,[0],[0]
"The full model with compatibility features is finer grained, capturing, for example, that links from REFERENCE to FACT are more likely when the reference comes after, or that links from VALUE to POLICY are extremely likely only when the two are adjacent.
",5.1 Discussion and analysis,[0],[0]
Proposition errors.,5.1 Discussion and analysis,[0],[0]
The confusion matrices in Figure 4 reveal that the most common confusion is misclassifying FACT as VALUE.,5.1 Discussion and analysis,[0],[0]
The strongest difference between the various models tested is that the RNN-based models make this error less often.,5.1 Discussion and analysis,[0],[0]
"For instance, in the proposition:
And the single most frequently used excuse of any debtor is “I didn’t receive the letter/invoice/statement”
the pronouns in the nested quote may be mistaken for subjectivity, leading to the structured SVMs
predictions of VALUE or TESTIMONY, while the basic structured RNN correctly classifies it as FACT.
",5.1 Discussion and analysis,[0],[0]
Link errors.,5.1 Discussion and analysis,[0],[0]
"While structured inference certainly helps baselines by preventing invalid structures such as cycles, it still depends on local decisions, losing to fully structured training in cases where joint proposition and link decisions are needed.",5.1 Discussion and analysis,[0],[0]
"For instance, in the following conclusion of an UKP essay, the annotators found no links:
In short, [ the individual should finance his or her education ]a because [ it is a personal choice.",5.1 Discussion and analysis,[0],[0]
"]b Otherwise, [ it would cause too much cost from taxpayers and the government.",5.1 Discussion and analysis,[0],[0]
"]c
Indeed, no reasons are provided, but baseline are misled by the connectives: the SVM baseline outputs that b and c are PREMISEs supporting the CLAIM a.",5.1 Discussion and analysis,[0],[0]
"The full structured SVM combines the two tasks and correctly recognizes the link structure.
",5.1 Discussion and analysis,[0],[0]
"Linear SVMs are still a very good baseline, but they tend to overgenerate links due to class imbalance, even if we use class weights during training.",5.1 Discussion and analysis,[0],[0]
"Surprisingly, RNNs are at the opposite end, being extremely conservative, and getting the highest precision among the models.",5.1 Discussion and analysis,[0],[0]
"On CDCP, where the number of true links is 272, the linear baseline with strict inference predicts 796 links with a precision of only 16%, while the strict structured RNN only predicts 52 links, with 33% precision; the example in Figure 5 illustrates this.",5.1 Discussion and analysis,[0],[0]
"In terms of higher-order structures, we find that using higherorder factors increases precision, at a cost in recall.
",5.1 Discussion and analysis,[0],[0]
"This is most beneficial for the 856 co-parent structures in the UKP test set: the full structured SVM has 53% F1, while the basic structured SVM and the basic baseline get 47% and 45% respectively.",5.1 Discussion and analysis,[0],[0]
"On CDCP, while higher-order factors help, performance on siblings and co-parents is below 10% F1 score.",5.1 Discussion and analysis,[0],[0]
This is likely due to link sparsity and suggests plenty of room for further development.,5.1 Discussion and analysis,[0],[0]
"We introduce an argumentation parsing model based on AD3 relaxed inference in expressive factor graphs, experimenting with both linear struc-
tured SVMs and structured RNNs, parametrized with higher-order factors and link structure constraints.",6 Conclusions and future work,[0],[0]
We demonstrate our model on a new argumentation mining dataset with more permissive argument structure annotation.,6 Conclusions and future work,[0],[0]
"Our model also achieves state-of-the-art link prediction performance on the UKP essays dataset.
",6 Conclusions and future work,[0],[0]
Future work.,6 Conclusions and future work,[0],[0]
"Stab and Gurevych (2016) found polynomial kernels useful for modeling feature interactions, but kernel structured SVMs scale poorly, we intend to investigate alternate ways to capture feature interactions.",6 Conclusions and future work,[0],[0]
"While we focus on monological argumentation, our model could be extended to dialogs, for which argumentation theory thoroughly motivates non-tree structures (Afantenos and Asher, 2014).",6 Conclusions and future work,[0],[0]
We are grateful to André,Acknowledgements,[0],[0]
"Martins, Andreas Müller, Arzoo Katyiar, Chenhao Tan, Felix Wu, Jack Hessel, Justine Zhang, Mathieu Blondel, Tianze Shi, Tobias Schnabel, and the rest of the Cornell NLP seminar for extremely helpful discussions.",Acknowledgements,[0],[0]
We thank the anonymous reviewers for their thorough and well-argued feedback.,Acknowledgements,[0],[0]
"We propose a novel factor graph model for argument mining, designed for settings in which the argumentative relations in a document do not necessarily form a tree structure.",abstractText,[0],[0]
(This is the case in over 20% of the web comments dataset we release.),abstractText,[0],[0]
Our model jointly learns elementary unit type classification and argumentative relation prediction.,abstractText,[0],[0]
"Moreover, our model supports SVM and RNN parametrizations, can enforce structure constraints (e.g., transitivity), and can express dependencies between adjacent relations and propositions.",abstractText,[0],[0]
Our approaches outperform unstructured baselines in both web comments and argumentative essay datasets.,abstractText,[0],[0]
Argument Mining with Structured SVMs and RNNs,title,[0],[0]
"Proceedings of the SIGDIAL 2015 Conference, pages 217–226, Prague, Czech Republic, 2-4 September 2015. c©2015 Association for Computational Linguistics",text,[0],[0]
Online forums are now one of the primary venues for public dialogue on current social and political issues.,1 Introduction,[0],[0]
"The related corpora are often huge, covering any topic imaginable, thus providing novel opportunities to address a number of open questions about the structure of dialogue.",1 Introduction,[0],[0]
Our aim is to use these dialogue corpora to automatically discover the semantic aspects of arguments that conversants are making across multiple dialogues on a topic.,1 Introduction,[0],[0]
"We build a new dataset of 109,074 posts on the topics gay marriage, gun control, death penalty and evolution.",1 Introduction,[0],[0]
"We frame our problem as consisting of two separate tasks:
• Argument Extraction: How can we extract argument segments in dialogue that clearly express a particular argument facet?
",1 Introduction,[0],[0]
• Argument,1 Introduction,[0],[0]
"Facet Similarity: How can we recognize that two argument segments are semantically similar, i.e. about the same facet of the argument?
",1 Introduction,[0],[0]
Consider for example the sample posts and responses in Fig. 1.,1 Introduction,[0],[0]
"Argument segments that are good targets for argument extraction are indicated, in their dialogic context, in bold.",1 Introduction,[0],[0]
"Given extracted segments, the argument facet similarity module should recognize that R3 and R4 paraphrase the same argument facet, namely that there is a strong relationship between the availability of guns and the murder rate.",1 Introduction,[0],[0]
"This paper addresses only the argument extraction task, as an important first step towards producing argument summaries that reflect the range and type of arguments being made,
217
on a topic, over time, by citizens in public forums.",1 Introduction,[0],[0]
"Our approach to the argument extraction task is driven by a novel hypothesis, the IMPLICIT MARKUP hypothesis.",1 Introduction,[0],[0]
"We posit that the arguments that are good candidates for extraction will be marked by cues (implicit markups) provided by the dialog conversants themselves, i.e. their choices about the surface realization of their arguments.",1 Introduction,[0],[0]
"We examine a number of theoretically motivated cues for extraction, that we expect to be domain-independent.",1 Introduction,[0],[0]
"We describe how we use these cues to sample from the corpus in a way that lets us test the impact of the hypothesized cues.
",1 Introduction,[0],[0]
Both the argument extraction and facet similarity tasks have strong similarities to other work in natural language processing.,1 Introduction,[0],[0]
Argument extraction resembles the sentence extraction phase of multi-document summarization.,1 Introduction,[0],[0]
"Facet similarity resembles semantic textual similarity and paraphrase recognition (Misra et al., 2015; Boltuzic and Šnajder, 2014; Conrad et al., 2012; Han et al., 2013; Agirre et al., 2012).",1 Introduction,[0],[0]
"Work on multidocument summarization also uses a similar module to merge redundant content from extracted candidate sentences (Barzilay, 2003; Gurevych and Strube, 2004; Misra et al., 2015).
",1 Introduction,[0],[0]
"Sec. 2 describes our corpus of arguments, and describes the hypothesized markers of highquality argument segments.",1 Introduction,[0],[0]
"We sample from the corpus using these markers, and then annotate the extracted argument segments for ARGUMENT QUALITY.",1 Introduction,[0],[0]
Sec. 3.2 describes experiments to test whether: (1) we can predict argument quality; (2) our hypothesized cues are good indicators of argument quality; and (3) an argument quality predictor trained on one topic or a set of topics can be used on unseen topics.,1 Introduction,[0],[0]
The results in Sec. 4 show that we can predict argument quality with RRSE values as low as .73 for some topics.,1 Introduction,[0],[0]
"Cross-domain training combined with domainadaptation yields RRSE values for several topics as low as .72, when trained on topic independent features, however some topics are much more difficult.",1 Introduction,[0],[0]
We provide a comparison of our work to previous research and sum up in Sec. 5.,1 Introduction,[0],[0]
"We created a large corpus consisting of 109,074 posts on the topics gay marriage (GM, 22425 posts), gun control (GC, 38102 posts), death penalty (DP, 5283 posts) and evolution (EV, 43624), by combining the Internet Argument Corpus (IAC) (Walker et al., 2012), with dialogues from http://www.createdebate.com/.
Our aim is to develop a method that can extract high quality arguments from a large corpus of argumentative dialogues, in a topic and domain-
independent way.",2 Corpus and Method,[0],[0]
It is important to note that arbitrarily selected utterances are unlikely to be high quality arguments.,2 Corpus and Method,[0],[0]
"Consider for example all the utterances in Fig. 1: many utterances are either not interpretable out of context, or fail to clearly frame an argument facet.",2 Corpus and Method,[0],[0]
Our IMPLICIT MARKUP hypothesis posits that arguments that are good candidates for extraction will be marked by cues from the surface realization of the arguments.,2 Corpus and Method,[0],[0]
We first describe different types of cues that we use to sample from the corpus in a way that lets us test their impact.,2 Corpus and Method,[0],[0]
"We then describe the MT HIT, and how we use our initial HIT results to refine our sampling process.",2 Corpus and Method,[0],[0]
"Table 2 presents the results of our sampling and annotation processes, which we will now explain in more detail.",2 Corpus and Method,[0],[0]
"The IMPLICIT MARKUP hypothesis is composed of several different sub-hypotheses as to how speakers in dialogue may mark argumentative structure.
",2.1 Implicit Markup Hypothesis,[0],[0]
"The Discourse Relation hypothesis suggests that the Arg1 and Arg2 of explicit SPECIFICATION, CONTRAST, CONCESSION and CONTINGENCY markers are more likely to contain good argumentative segments (Prasad et al., 2008).",2.1 Implicit Markup Hypothesis,[0],[0]
"In the case of explicit connectives, Arg2 is the argument to which the connective is syntactically bound, and Arg1 is the other argument.",2.1 Implicit Markup Hypothesis,[0],[0]
"For example, a CONTINGENCY relation is frequently marked by the lexical anchor If, as in R1 in Fig. 1.",2.1 Implicit Markup Hypothesis,[0],[0]
"A CONTRAST relation may mark a challenge to an opponent’s claim, what Ghosh et al. call callout-target argument pairs (Ghosh et al., 2014b; Maynard, 1985).",2.1 Implicit Markup Hypothesis,[0],[0]
"The CONTRAST relation is frequently marked by But, as in R3 and R4 in Fig. 1.",2.1 Implicit Markup Hypothesis,[0],[0]
"A SPECIFICATION relation may indicate a focused detailed argument, as marked by First in R2 in Fig. 1 (Li and Nenkova, 2015).",2.1 Implicit Markup Hypothesis,[0],[0]
"We decided to extract only the Arg2, where the discourse argument is syntactically bound to the connective, since Arg1’s are more difficult to locate, especially in dialogue.",2.1 Implicit Markup Hypothesis,[0],[0]
"We began by extracting the Arg2’s for the connectives most strongly associated with these discourse relations over the whole corpus, and then once we saw what the most frequent connectives were in our corpus, we refined this selection to include only but, if, so, and first.",2.1 Implicit Markup Hypothesis,[0],[0]
"We sampled a roughly even distribution of sentences from each category as well as sentences without any discourse connectives, i.e. None.",2.1 Implicit Markup Hypothesis,[0],[0]
See Table.,2.1 Implicit Markup Hypothesis,[0],[0]
"2.
",2.1 Implicit Markup Hypothesis,[0],[0]
"The Syntactic Properties hypothesis posits that syntactic properties of a clause may indicate good argument segments, such as being the main clause (Marcu, 1999), or the sentential complement of mental state or speech-act verbs, e.g. the SBAR
in you agree that SBAR as in P2 in Fig. 1.",2.1 Implicit Markup Hypothesis,[0],[0]
"Because these markers are not as frequent in our corpus, we do not test this with sampling: rather we test it as a feature as described in Sec. 3.2.
",2.1 Implicit Markup Hypothesis,[0],[0]
"The Dialogue Structure hypothesis suggests that position in the post or the relation to a verbatim quote could influence argument quality, e.g. being turn-initial in a response as exemplified by P2, R3 and R4 in Fig. 1.",2.1 Implicit Markup Hypothesis,[0],[0]
We indicate sampling by position in post with Starts:,2.1 Implicit Markup Hypothesis,[0],[0]
Yes/No in Table.,2.1 Implicit Markup Hypothesis,[0],[0]
2.,2.1 Implicit Markup Hypothesis,[0],[0]
Our corpora are drawn from websites that offer a “quoting affordance” in addition to a direct reply.,2.1 Implicit Markup Hypothesis,[0],[0]
"An example of a post from the IAC corpus utilizing this mechanism is shown in Table 1, where the quoted text is highlighted in blue and the response is directly below it.
",2.1 Implicit Markup Hypothesis,[0],[0]
"The Semantic Density hypothesis suggests that measures of rich content or SPECIFICITY will indicate good candidates for argument extraction (Louis and Nenkova, 2011).",2.1 Implicit Markup Hypothesis,[0],[0]
We initially posited that short sentences and sentences without any topic-specific words are less likely to be good.,2.1 Implicit Markup Hypothesis,[0],[0]
"For the topics gun control and gay marriage, we filtered sentences less than 4 words long, which removed about 8-9% of the sentences.",2.1 Implicit Markup Hypothesis,[0],[0]
"After collecting the argument quality annotations for these two topics and examining the distribution of scores (see Sec. 2.2 below), we developed an additional measure of semantic density that weights words in each candidate by its pointwise mutual information (PMI), and applied it to the evolution and death penalty.",2.1 Implicit Markup Hypothesis,[0],[0]
"Using the 26 topic annotations in the IAC, we calculate the PMI between every word in the corpus appearing more than 5 times and each topic.",2.1 Implicit Markup Hypothesis,[0],[0]
We only keep those sentences that have at least one word whose PMI is above our threshold of 0.1.,2.1 Implicit Markup Hypothesis,[0],[0]
"We determined this threshold by examining the values in gun control and gay marriage, such that at least 2/3 of the filtered sentences were in the bottom third of the argument quality score.",2.1 Implicit Markup Hypothesis,[0],[0]
"The PMI filter eliminates 39% of the sentences from death penalty (40% combined with the length filter) and 85% of the sentences from
evolution (87% combined with the length filter).",2.1 Implicit Markup Hypothesis,[0],[0]
Table 2 summarizes the results of our sampling procedure.,2.1 Implicit Markup Hypothesis,[0],[0]
"Overall our experiments are based on 5,374 sampled sentences, with roughly equal numbers over each topic, and equal numbers representing each of our hypotheses and their interactions.",2.1 Implicit Markup Hypothesis,[0],[0]
Table 8 in the Appendix provides example argument segments resulting from the sampling and annotation process.,"2.2 Data Sampling, Annotation and Analysis",[0],[0]
"Sometimes arguments are completely self contained, e.g. S1 to S8 in Table 8.","2.2 Data Sampling, Annotation and Analysis",[0],[0]
"In other cases, e.g. S9 to S16 we can guess what the argument is based on using world knowledge of the domain, but it is not explicitly stated or requires several steps of inference.","2.2 Data Sampling, Annotation and Analysis",[0],[0]
"For example, we might be able to infer the argument in S14 in Table 8, and the context in which it arose, even though it is not explicitly stated.","2.2 Data Sampling, Annotation and Analysis",[0],[0]
"Finally, there are cases where the user is not making an argument or the argument cannot be reconstructed without significantly more context, e.g. S21 in Table 8.
","2.2 Data Sampling, Annotation and Analysis",[0],[0]
We collect annotations for ARGUMENT QUALITY for all the sentences summarized in Table 2 on Amazon’s Mechanical Turk (AMT) platform.,"2.2 Data Sampling, Annotation and Analysis",[0],[0]
Figure 3 in the Appendix illustrates the basic layout of the HIT.,"2.2 Data Sampling, Annotation and Analysis",[0],[0]
Each HIT consisted of 20 sentences on one topic which is indicated on the page.,"2.2 Data Sampling, Annotation and Analysis",[0],[0]
"The annotator first checked a box if the sentence expressed an argument, and then rated the argument quality using a continuous slider ranging from hard (0.0) to easy to interpret (1.0).
","2.2 Data Sampling, Annotation and Analysis",[0],[0]
We collected 7 annotations per sentence.,"2.2 Data Sampling, Annotation and Analysis",[0],[0]
"All Turkers were required to pass our qualifier, have a HIT approval rating above 95%, and be located in the United States, Canada, Australia, or Great Britain.","2.2 Data Sampling, Annotation and Analysis",[0],[0]
"The results of the sampling and annotation on the final annotated corpus are in Table 2.
","2.2 Data Sampling, Annotation and Analysis",[0],[0]
"We measured the inter-annotator agreement (IAA) of the binary annotations using Krippendorff’s α (Krippendorff, 2013) and the continuous values using the intraclass correlation coefficient (ICC) for each topic.","2.2 Data Sampling, Annotation and Analysis",[0],[0]
We found that annotators could not distinguish between phrases that did not express an argument and hard sentences.,"2.2 Data Sampling, Annotation and Analysis",[0],[0]
See examples and definitions in Fig. 3.,"2.2 Data Sampling, Annotation and Analysis",[0],[0]
"We therefore mapped unchecked sentences (i.e., non arguments) to zero argument quality.","2.2 Data Sampling, Annotation and Analysis",[0],[0]
"We then calculated the average pairwise ICC value for each rater between all Turkers with overlapping annotations, and removed the judgements of any Turker that did not have a positive ICC value.","2.2 Data Sampling, Annotation and Analysis",[0],[0]
The ICC for each topic is shown in Table 2.,"2.2 Data Sampling, Annotation and Analysis",[0],[0]
"The mean rating across the remaining annotators for each sentence was used as the gold standard for argument quality, with means in the Argument Quality (AQ) column of Table 2.","2.2 Data Sampling, Annotation and Analysis",[0],[0]
"The effect of the sampling on
argument quality can be seen in Table 2.","2.2 Data Sampling, Annotation and Analysis",[0],[0]
"The differences between gun control and gay marriage, and the other two topics is due to effective use of the semantic density filter, which shifted the distribution of the annotated data towards higher quality arguments as we intended.","2.2 Data Sampling, Annotation and Analysis",[0],[0]
We can now briefly validate some of the IMPLICIT MARKUP hypothesis using an ANOVA testing the effect of a connective and its position in post on argument quality.,3.1 Implicit Markup Hypothesis Validation,[0],[0]
"Across all sentences in all topics, the presence of a connective is significant (p = 0.00).",3.1 Implicit Markup Hypothesis Validation,[0],[0]
"Three connectives, if, but, and so, show significant differences in AQ from no-connective phrases (p = 0.00, 0.02, 0.00, respectively).",3.1 Implicit Markup Hypothesis Validation,[0],[0]
First does not show a significant effect.,3.1 Implicit Markup Hypothesis Validation,[0],[0]
"The mean AQ scores for sentences marked by if, but, and so differ from that of a no-connective sentence by 0.11, 0.04, and 0.04, respectively.",3.1 Implicit Markup Hypothesis Validation,[0],[0]
"These numbers support our hypothesis that there are certain discourse connectives or cue words which can help to signal the existence of arguments, and they seem to suggest that the CONTINGENCY category may be most useful, but more research using more cue words is necessary to validate this suggestion.
",3.1 Implicit Markup Hypothesis Validation,[0],[0]
"In addition to the presence of a connective, the dialogue structural position of being an initial sentence in a response post did not predict argument quality as we expected.",3.1 Implicit Markup Hypothesis Validation,[0],[0]
"Response-initial sentences provide significantly lower quality arguments (p = 0.00), with response-initial sentences having an average AQ score 0.03 lower (0.40 vs. 0.43).",3.1 Implicit Markup Hypothesis Validation,[0],[0]
"We use 3 regression algorithms from the Java Statistical Analysis Toolkit1: Linear Least Squared Error (LLS), Ordinary Kriging (OK) and Support Vector Machines using a radial basis function kernel (SVM).",3.2 Argument Quality Regression,[0],[0]
A random 75% of the sentences of each domain were put into training/development and 25% into the held out test.,3.2 Argument Quality Regression,[0],[0]
Training involved a grid search over the hyper-parameters of each model2 and a subset (23-29 and the complete set) of the top N features whose values correlate best with the argument quality dependent variable (using Pearson’s).,3.2 Argument Quality Regression,[0],[0]
"The combined set of parameters and features that achieved the best mean squared error over a 5-fold cross validation on the training data was used to train the complete model.
",3.2 Argument Quality Regression,[0],[0]
"We also compare hand-curated feature sets that are motivated by our hypotheses to this simple
1https://github.com/EdwardRaff/JSAT 2We used the default parameters for LLS and OK and only
searched hyper-parameters for the SVM model.
feature selection method, and the performance of in-domain, cross-domain, and domain-adaptation training using “the frustratingly easy” approach (Daumé III, 2007).
",3.2 Argument Quality Regression,[0],[0]
We use our training and development data to develop a set of feature templates.,3.2 Argument Quality Regression,[0],[0]
"The features are real-valued and normalized between 0 and 1, based on the min and max values in the training data for each domain.",3.2 Argument Quality Regression,[0],[0]
If not stated otherwise the presence of a feature was represented by 1.0 and its absence by 0.0.,3.2 Argument Quality Regression,[0],[0]
We describe all the handcurated feature sets below.,3.2 Argument Quality Regression,[0],[0]
Semantic Density Features: Deictic Pronouns (DEI): The presence of anaphoric references are likely to inhibit the interpretation of an utterance.,3.2 Argument Quality Regression,[0],[0]
"These features count the deictic pronouns in the sentence, such as this, that and it.
",3.2 Argument Quality Regression,[0],[0]
"Sentence Length (SLEN): Short sentences, particularly those under 5 words, are usually hard to interpret without context and complex linguistic processing, such as resolving long distance discourse anaphora.",3.2 Argument Quality Regression,[0],[0]
"We thus include a single aggregate feature whose value is the number of words.
",3.2 Argument Quality Regression,[0],[0]
Word Length (WLEN): Sentences that clearly articulate an argument should generally contain words with a high information content.,3.2 Argument Quality Regression,[0],[0]
"Several studies show that word length is a surprisingly good indicator that outperforms more complex measures, such as rarity (Piantadosi et al., 2011).",3.2 Argument Quality Regression,[0],[0]
"Thus we include features based on word length, including the min, max, mean and median.",3.2 Argument Quality Regression,[0],[0]
"We also create a feature whose value is the count of words of lengths 1 to 20 (or longer).
",3.2 Argument Quality Regression,[0],[0]
"Speciteller (SPTL): We add a single aggregate feature from the result of Speciteller, a tool that assesses the specificity of a sentence in the range of 0 (least specific) to 1 (most specific) (Li and Nenkova, 2015; Louis and Nenkova, 2011).",3.2 Argument Quality Regression,[0],[0]
"High specificity should correlate with argument quality.
",3.2 Argument Quality Regression,[0],[0]
Kullback-Leibler Divergence (KLDiv): We expect that sentences on one topic domain will have different content than sentences outside the domain.,3.2 Argument Quality Regression,[0],[0]
"We built two trigram language models using the Berkeley LM toolkit (Pauls and Klein, 2011).",3.2 Argument Quality Regression,[0],[0]
"One (P) built from all the sentences in the IAC within the domain, excluding all sentences from the annotated dataset, and one (Q) built from all sentences in IAC outside the domain.",3.2 Argument Quality Regression,[0],[0]
"The KL Divergence is then computed using the discrete n-gram probabilities in the sentence from each model as in equation (1).
",3.2 Argument Quality Regression,[0],[0]
DKL(P ||Q),3.2 Argument Quality Regression,[0],[0]
"= ∑
i
P (i) ln P (i) Q(i)
(1)
",3.2 Argument Quality Regression,[0],[0]
"Lexical N-Grams (LNG): N-Grams are a standard feature that are often a difficult baseline to
beat.",3.2 Argument Quality Regression,[0],[0]
However they are not domain independent.,3.2 Argument Quality Regression,[0],[0]
We created a feature for every unigram and bigram in the sentence.,3.2 Argument Quality Regression,[0],[0]
The feature value was the inverse document frequency of that n-gram over all posts in the entire combined IAC plus CreateDebate corpus.,3.2 Argument Quality Regression,[0],[0]
Any n-gram seen less than 5 times was not included.,3.2 Argument Quality Regression,[0],[0]
"In addition to the specific lexical features a set of aggregate features were also generated that only considered summary statistics of the lexical feature values, for example the min, max and mean IDF values in the sentence.",3.2 Argument Quality Regression,[0],[0]
"Discourse and Dialogue Features: We expect our features related to the discourse and dialogue hypotheses to be domain independent.
",3.2 Argument Quality Regression,[0],[0]
Discourse (DIS): We developed features based on discourse connectives found in the Penn Discourse Treebank as well as a set of additional connectives in our corpus that are related to dialogic discourse and not represented in the PDTB.,3.2 Argument Quality Regression,[0],[0]
We first determine if a discourse connective is present in the sentence.,3.2 Argument Quality Regression,[0],[0]
"If not, we create a NO CONNECTIVE feature with a value of 1.",3.2 Argument Quality Regression,[0],[0]
"Otherwise, we identify all connectives that are present.",3.2 Argument Quality Regression,[0],[0]
"For each of them, we derive a set of specific lexical features and a set of generic aggregate features.
",3.2 Argument Quality Regression,[0],[0]
The specific features make use of the lexical (String) and PDTB categories (Category) of the found connectives.,3.2 Argument Quality Regression,[0],[0]
We start by identifying the connective and whether it started the sentence or not (Location).,3.2 Argument Quality Regression,[0],[0]
"We then identify the connective’s most likely PDTB category based on the frequencies stated in the PDTB manual and all of its parent categories, for example but → CONTRAST → COMPARISON.",3.2 Argument Quality Regression,[0],[0]
The aggregate features only consider how many discourse connectives and if any of them started the sentence.,3.2 Argument Quality Regression,[0],[0]
"The templates are:
Specific:{Location}:{String} Specific:{Location}:{Category} Aggregate:{Location}:{Count}
",3.2 Argument Quality Regression,[0],[0]
"For example, the first sentence in Table 8 would generate the following features:
Specific:Starts:but Specific:Starts:Contrast
Specific:Starts:COMPARISON Aggregate:Starts:1 Aggregate:Any:1
Because our hypothesis about dialogue structure was disconfirmed by the results described in section 3.1, we did not develop a feature to independently test position in post.",3.2 Argument Quality Regression,[0],[0]
Rather the Discourse features only encode whether the discourse cue starts the post or not.,3.2 Argument Quality Regression,[0],[0]
"Syntactic Property Features: We also expect syntactic property features to generalize across domains.
",3.2 Argument Quality Regression,[0],[0]
Part-Of-Speech N-Grams (PNG): Lexical features require large amounts of training data and are likely to be topic-dependent.,3.2 Argument Quality Regression,[0],[0]
Part-of-speech tags are less sparse and and less likely to be topicspecific.,3.2 Argument Quality Regression,[0],[0]
"We created a feature for every unigram, bigram and trigram POS tag sequence in the sentence.",3.2 Argument Quality Regression,[0],[0]
"Each feature’s value was the relative frequency of the n-gram in the sentence.
",3.2 Argument Quality Regression,[0],[0]
Syntactic (SYN):,3.2 Argument Quality Regression,[0],[0]
"Certain syntactic structures may be used more frequently for expressing argumentative content, such as complex sentences with verbs that take clausal complements.",3.2 Argument Quality Regression,[0],[0]
"In CreateDebate, we found a number of phrases of the form",3.2 Argument Quality Regression,[0],[0]
"I <VERB> that <X>, such as I agree that, you said that, except that",3.2 Argument Quality Regression,[0],[0]
and I disagree because.,3.2 Argument Quality Regression,[0],[0]
"Thus we included two types of syntactic features: one for every internal node, excluding POS tags, of the parse tree (NODE) and another for each context free production rule (RULE) in the parse tree.",3.2 Argument Quality Regression,[0],[0]
"The feature value is the relative frequency of the node or rule within the sentence.
",3.2 Argument Quality Regression,[0],[0]
Meta Features: The 3 meta feature sets are: (1) all features except lexical n-grams (!LNG); (2) all features that use specific lexical or categorical information (SPFC); and (3) aggregate statistics (AGG) obtained from our feature extraction process.,3.2 Argument Quality Regression,[0],[0]
"The AGG set included features, such as sentence and word length, and summary statistics about the IDF values of lexical n-grams, but did not actually reference any lexical properties in the
feature name.",3.2 Argument Quality Regression,[0],[0]
We expect both !,3.2 Argument Quality Regression,[0],[0]
LNG and AGG to generalize across domains.,3.2 Argument Quality Regression,[0],[0]
"Sec. 4.1 presents the results of feature selection, which finds a large number of general features.",4 Results,[0],[0]
The results for argument quality prediction are in Secs.,4 Results,[0],[0]
4.2 and 4.3.,4 Results,[0],[0]
"Our standard training procedure (SEL) incorporates all the feature templates described in Sec. 3.2, which generates a total of 23,345 features.",4.1 Feature Selection,[0],[0]
It then performs a grid search over the model hyper-parameters and a subset of all the features using the simple feature selection technique described in section 3.2.,4.1 Feature Selection,[0],[0]
Table 3 shows the 10 features most correlated with the annotated quality value in the training data for the topics gun control and gay marriage.,4.1 Feature Selection,[0],[0]
"A few domain specific lexical items appear, but in general the top features tend to be non-lexical and relatively domain independent, such as part-of-speech tags and sentence specificity, as measured by Speciteller (Li and Nenkova, 2015; Louis and Nenkova, 2011).
",4.1 Feature Selection,[0],[0]
"Sentence length has the highest correlation with the target value in both topics, as does the node:root feature, inversely correlated with length.",4.1 Feature Selection,[0],[0]
"Therefore, in order to shift the quality distribution of the sample that we put out on MTurk for the death penalty or evolution topics, we applied a filter that removed all sentences shorter than 4 words.",4.1 Feature Selection,[0],[0]
"For these topics, domain specific features such as lexical n-grams are better predictors of argument quality.",4.1 Feature Selection,[0],[0]
"As discussed above, the PMI filter that was applied only to these two topics during sampling removed some shorter low quality sentences, which probably altered the predictive value of this feature in these domains.",4.1 Feature Selection,[0],[0]
"We first tested the performance of 3 regression algorithms using the training and testing data within each topic using 3 standard evaluation measures: R2, Root Mean Squared Error (RMSE) and Root
Relative Squared Error (RRSE).",4.2 In-Domain Training,[0],[0]
R2 estimates the amount of variability in the data that is explained by the model.,4.2 In-Domain Training,[0],[0]
Higher values indicate a better fit to the data.,4.2 In-Domain Training,[0],[0]
"The RMSE measures the average squared difference between predicted values and true values, which penalizes wrong answers more as the difference increases.",4.2 In-Domain Training,[0],[0]
"The RRSE is similar to RMSE, but is normalized by the squared error of a simple predictor that always guesses the mean target value in the test set.",4.2 In-Domain Training,[0],[0]
"Anything below a 1.0 indicates an improvement over the baseline.
",4.2 In-Domain Training,[0],[0]
"Table 4 shows that SVMs and OK perform the best, with better than baseline results for all topics.",4.2 In-Domain Training,[0],[0]
Performance for gun control and gay marriage are significantly better.,4.2 In-Domain Training,[0],[0]
See Fig. 2.,4.2 In-Domain Training,[0],[0]
"Since SVM was nearly always the best model, we only report SVM results in what follows.
",4.2 In-Domain Training,[0],[0]
We also test the impact of our theoretically motivated features and domain specific features.,4.2 In-Domain Training,[0],[0]
The top half of Table 5 shows the RRSE for each feature set with darker cells indicating better performance.,4.2 In-Domain Training,[0],[0]
The feature acronyms are described in Sec 3.2.,4.2 In-Domain Training,[0],[0]
"When training and testing on the same domain, using lexical features leads to the best performance for all topics (SEL, LEX, LNG and SPFC).",4.2 In-Domain Training,[0],[0]
"However, we can obtain good performance on all of the topics without using any lexical information at all (!LNG, WLEN, PNG, and AGG), sometimes close to our best results.",4.2 In-Domain Training,[0],[0]
"Despite the high correlation to the target value, sentence specificity as a single feature does not outperform any other feature sets.",4.2 In-Domain Training,[0],[0]
"In general, we do better for gun control and gay marriage than for death penalty and evolution.",4.2 In-Domain Training,[0],[0]
"Since the length and domain specific words are important features in the trained models, it seems likely that the filtering process made it harder to learn a good function.
",4.2 In-Domain Training,[0],[0]
"The bottom half of Table 5 shows the results using training data from all other topics, when testing on one topic.",4.2 In-Domain Training,[0],[0]
"The best results for GC are significantly better for several feature sets (SEL,
LEX, LNG), In general the performance remains similar to the in-domain training, with some minor improvements over the best performing models.",4.2 In-Domain Training,[0],[0]
"These results suggest that having more data outweighs any negative consequences of domain specific properties.
",4.2 In-Domain Training,[0],[0]
"●
●
●
● ●
● ● ● ●
●
0.8
0.9
1.0
250 500 750 1000 1250
Number of Training Instances
R oo
t R el
at iv
e S
qu ar
ed E
rr or
Domain ● Gun Control
Gay Marriage Evolution Death Penalty
Figure 2:",4.2 In-Domain Training,[0],[0]
"Learning curves for each of the 4 topics with 95% confidence intervals.
",4.2 In-Domain Training,[0],[0]
We also examine the effect of training set size on performance given the best performing feature sets.,4.2 In-Domain Training,[0],[0]
See Fig. 2.,4.2 In-Domain Training,[0],[0]
"We randomly divided our entire dataset into an 80/20 training/testing split and trained incrementally larger models from the 80% using the default training procedure, which were then applied to the 20% testing data.",4.2 In-Domain Training,[0],[0]
"The plotted points are the mean value of repeating this process 10 times, with the shaded region showing the 95% confidence interval.",4.2 In-Domain Training,[0],[0]
"Although most gains are achieved within 500-750 training examples, all models are still trending downward, suggesting that more training data would be useful.
",4.2 In-Domain Training,[0],[0]
"Finally, our results are actually even better than they appear.",4.2 In-Domain Training,[0],[0]
"Our primary application requires extracting arguments at the high end of the scale (e.g., those above 0.8 or 0.9), but the bulk of our data is closer to the middle of the scale, so our regressors are conservative in assigning high or low
values.",4.2 In-Domain Training,[0],[0]
To demonstrate this point we split the predicted values for each topic into 5 quantiles.,4.2 In-Domain Training,[0],[0]
The RMSE for each of the quantiles and domains in Table 6 demonstrates that the lowest RMSE is obtained in the top quantile.,4.2 In-Domain Training,[0],[0]
To investigate whether learned models generalize across domains we also evaluate the performance of training with data from one domain and testing on another.,4.3 Cross-Domain and Domain Adaptation,[0],[0]
The columns labeled CD in Table 7 summarize these results.,4.3 Cross-Domain and Domain Adaptation,[0],[0]
"Although cross domain training does not perform as well as in-domain training, we are able to achieve much better than baseline results between gun control and gay marriage for many of the feature sets and some other minor transferability for the other domains.",4.3 Cross-Domain and Domain Adaptation,[0],[0]
"Although lexical features (e.g., lexical n-grams) perform best in-domain, the best performing features across domains are all non-lexical, i.e. !",4.3 Cross-Domain and Domain Adaptation,[0],[0]
"LNG, PNG and AGG.
",4.3 Cross-Domain and Domain Adaptation,[0],[0]
"We then applied Daume’s “frustratingly easy domain adaptation” technique (DA), by transforming the original features into a new augmented feature space where, each feature, is transformed into a general feature and a domain specific feature, source or target, depending on the input domain (Daumé III, 2007).",4.3 Cross-Domain and Domain Adaptation,[0],[0]
"The training data from both the source and target domains are used to train
the model, unlike the cross-domain experiments where only the source data is used.",4.3 Cross-Domain and Domain Adaptation,[0],[0]
"These results are given in the columns labeled DA in Table 7, which are on par with the best in-domain training results, with minor performance degradation on some gay marriage and gun control pairs, and slight improvements on the difficult death penalty and evolution topics.",4.3 Cross-Domain and Domain Adaptation,[0],[0]
"This paper addresses the Argument Extraction task in a framework whose long-term aim is to first extract arguments from online dialogues, and then use them to produce a summary of the different facets of an issue.",5 Discussion and Conclusions,[0],[0]
We have shown that we can find sentences that express clear arguments with RRSE values of .72 for gay marriage and gun control (Table 6) and .93 for death penalty and evolution (Table 8 cross domain with adaptation).,5 Discussion and Conclusions,[0],[0]
"These results show that sometimes the best quality predictors can be trained in a domain-independent way.
",5 Discussion and Conclusions,[0],[0]
"The two step method that we propose is different than much of the other work on argument mining, either for more formal texts or for social media, primarily because the bulk of previous work takes a supervised approach on a labelled topicspecific dataset (Conrad et al., 2012; Boltuzic and Šnajder, 2014; Ghosh et al., 2014b).",5 Discussion and Conclusions,[0],[0]
Conrad & Wiebe developed a data set for supervised training of an argument mining system on weblogs and news about universal healthcare.,5 Discussion and Conclusions,[0],[0]
They separate the task into two components: one component identifies ARGUING SEGMENTS and the second component labels the segments with the relevant ARGUMENT TAGS.,5 Discussion and Conclusions,[0],[0]
Our argument extraction phase has the same goals as their first component.,5 Discussion and Conclusions,[0],[0]
"Boltuzic & Snajder also apply a supervised learning approach, producing arguments labelled with a concept similar to what we call FACETS.",5 Discussion and Conclusions,[0],[0]
"However they perform what we call argument extraction by hand, eliminating comments from com-
ment streams that they call “spam” (Boltuzic and Šnajder, 2014).",5 Discussion and Conclusions,[0],[0]
"Ghosh et al. also take a supervised approach, developing techniques for argument mining on online forums about technical topics and applying a theory of argument structure that is based on identifying TARGETS and CALLOUTS, where the callout attacks a target proposition in another speaker’s utterance (Ghosh et al., 2014b).",5 Discussion and Conclusions,[0],[0]
"However, their work does not attempt to discover high quality callouts and targets that can be understood out of context like we do.",5 Discussion and Conclusions,[0],[0]
"More recent work also attempts to do some aspects of argument mining in an unsupervised way (Boltuzic and Šnajder, 2015; Sobhani et al., 2015).",5 Discussion and Conclusions,[0],[0]
"However (Boltuzic and Šnajder, 2015) focus on the argument facet similarity task, using as input a corpus where the arguments have already been extracted.",5 Discussion and Conclusions,[0],[0]
"(Sobhani et al., 2015) present an architecture where arguments are first topic-labelled in a semi-supervised way, and then used for stance classification, however this approach treats the whole comment as the extracted argument, rather than attempting to pull out specific focused argument segments as we do here.
",5 Discussion and Conclusions,[0],[0]
A potential criticism of our approach is that we have no way to measure the recall of our argument extraction system.,5 Discussion and Conclusions,[0],[0]
However we do not think that this is a serious issue.,5 Discussion and Conclusions,[0],[0]
"Because we are only interested in determining the similarity between phrases that are high quality arguments and thus potential contributors to summaries of a specific facet for a specific topic, we believe that precision is more important than recall at this point in time.",5 Discussion and Conclusions,[0],[0]
"Also, given the redundancy of the arguments presented over thousands of posts on an issue it seems unlikely we would miss an important facet.",5 Discussion and Conclusions,[0],[0]
"Finally, a measure of recall applied to the facets of a topic may be irreconcilable with our notion that an argument does not have a limited, enumerable number of facets, and our belief that each facet is subject to judgements of granularity.",5 Discussion and Conclusions,[0],[0]
Fig. 3 shows how the Mechanical Turk hit was defined and the examples that were used in the qualification task.,6 Appendix,[0],[0]
"Table 8 illustrates the argument quality scale annotations collected from Mechanical Turk.
",6 Appendix,[0],[0]
We invite other researchers to improve upon our results.,6 Appendix,[0],[0]
Our corpus and the relevant annotated data is available at http://nldslab.soe.ucsc.edu/ arg-extraction/sigdial2015/.,6 Appendix,[0],[0]
This research is supported by National Science Foundation Grant CISE-IIS-RI #1302668.,7 Acknowledgements,[0],[0]
Online forums are now one of the primary venues for public dialogue on current social and political issues.,abstractText,[0],[0]
"The related corpora are often huge, covering any topic imaginable.",abstractText,[0],[0]
Our aim is to use these dialogue corpora to automatically discover the semantic aspects of arguments that conversants are making across multiple dialogues on a topic.,abstractText,[0],[0]
We frame this goal as consisting of two tasks: argument extraction and argument facet similarity.,abstractText,[0],[0]
"We focus here on the argument extraction task, and show that we can train regressors to predict the quality of extracted arguments with RRSE values as low as .73 for some topics.",abstractText,[0],[0]
"A secondary goal is to develop regressors that are topic independent: we report results of cross-domain training and domain-adaptation with RRSE values for several topics as low as .72, when trained on topic independent features.",abstractText,[0],[0]
Argument Mining: Extracting Arguments from Online Dialogue,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1558–1572 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics
In this work we introduce an unsupervised methodology for extracting surface motifs that recur in questions, and for grouping them according to their latent rhetorical role. By applying this framework to the setting of question sessions in the UK parliament, we show that the resulting typology encodes key aspects of the political discourse—such as the bifurcation in questioning behavior between government and opposition parties—and reveals new insights into the effects of a legislator’s tenure and political career ambitions.",text,[0],[0]
"“We’d now like to open the floor to shorter speeches disguised as questions...”
– Steve Macone, New Yorker cartoon caption
Why do we ask questions?",1 Introduction,[0],[0]
"Perhaps we are seeking factual information that others hold, or maybe we are requesting a favor.",1 Introduction,[0],[0]
"Alternatively we could be simply making a rhetorical point, perhaps at the start of an academic paper.
",1 Introduction,[0],[0]
"Questions play a prominent role in social interactions (Goffman, 1976), performing a multitude of rhetorical functions that go beyond mere factual
information gathering (Kearsley, 1976).",1 Introduction,[0],[0]
"While the informational component of questions has been well-studied in the context of question-answering applications, there is relatively little computational work addressing the rhetorical and social role of these basic dialogic units.
",1 Introduction,[0],[0]
One domain where questions have a particularly salient rhetorical role is politics.,1 Introduction,[0],[0]
"The ability to question the actions and intentions of governments is a crucial part of democracy (Pitkin, 1967), particularly in parliamentary systems.",1 Introduction,[0],[0]
"Consequently, scholars have studied parliamentary questions in detail, in terms of their origins (Chester and Bowring, 1962), their institutionalization (Eggers and Spirling, 2014) and their importance for oversight (Proksch and Slapin, 2011).",1 Introduction,[0],[0]
"In particular, the United Kingdom’s House of Commons, renowned for theatrical questions periods, has been studied in some depth.",1 Introduction,[0],[0]
"However, those accounts are largely qualitative in nature (Bull and Wells, 2012; Bates et al., 2014).",1 Introduction,[0],[0]
The present work: methodology.,1 Introduction,[0],[0]
"In order to approach these problems computationally, we introduce an unsupervised framework to structure the space of questions according to their rhetorical role.",1 Introduction,[0],[0]
"First, we identify common ways in which questions are phrased.",1 Introduction,[0],[0]
"To this end, we automatically extract these recurring surface forms, or motifs, based on the lexico-syntactic structure of the questions posed (Section 4).",1 Introduction,[0],[0]
"To capture rhetorical aspects we then group these motifs according to their role, relying on the intuition that this role is encoded in the type of answer a question receives.",1 Introduction,[0],[0]
To operationalize this intuition we construct a latent question-answer space in which question motifs triggering similar answers are mapped to the same region (Section 5).,1 Introduction,[0],[0]
The present work: application.,1 Introduction,[0],[0]
"We apply this general framework to the political discourse that occurs during parliamentary question sessions in
1558
the British House of Commons, a new dataset which we make publicly available (Section 3).",1 Introduction,[0],[0]
"Our framework extracts intuitive question types ranging from narrow factual queries to pointed criticisms disguised as questions (Section 5, Table 1).",1 Introduction,[0],[0]
We validate our framework by aligning these types with prior understandings of parliamentary proceedings from the political science literature (Section 6).,1 Introduction,[0],[0]
"In particular, previous work (Bates et al., 2014) has categorized questions asked in Parliament according to the intentions of the asker (e.g., to help the answerer, or to adversarially put them on the spot); we find a clear, predictive mapping between these expert-coded categories and the induced typology.",1 Introduction,[0],[0]
"We further show that the types of questions specific legislators tend to ask vary with whether they are part of the governing or opposition party, consistent with wellestablished accounts of partisan differences (Cowley, 2002; Spirling and McLean, 2007; Eggers and Spirling, 2014).",1 Introduction,[0],[0]
"Concretely, government legislators exhibit a preference for overtly friendly questions, while the opposition slants towards more aggressive question types.
",1 Introduction,[0],[0]
We then apply our methodology to provide new insights into how a legislator’s questioning behavior varies with their career trajectory.,1 Introduction,[0],[0]
"The pressures faced by legislators at various stages in their career are cross-cutting, and multiple possible hypotheses emerge.",1 Introduction,[0],[0]
"Younger, more enthusiastic legislators may be motivated to ask harderhitting questions, but risk being passed over for future promotion if they are too combative (Cowley, 2002).",1 Introduction,[0],[0]
"Older legislators, whose opportunities for promotion are largely behind them and hence have “less to lose”, may act more aggressively (Benedetto and Hix, 2007); or simply seek a quiet path to retirement.",1 Introduction,[0],[0]
"Viewing each group’s behavior through the questions they ask brings evidence for the latter hypothesis that more tenured legislators are more aggressive, even when questioning their own leaders.",1 Introduction,[0],[0]
"In this way, their presence in the House of Commons, and their refusal to simply ‘keep their heads down’, facilitates a core component of democracy.",1 Introduction,[0],[0]
Question-answering.,2 Related Work,[0],[0]
"Computationally, questions have received considerable attention in the context of question-answering (QA) systems—for a survey see Gupta and Gupta (2012)—with an em-
phasis on understanding their information need (Harabagiu, 2008).",2 Related Work,[0],[0]
"Techniques have been developed to categorize questions based on the nature of these information needs in the context of the TREC QA challenge (Harabagiu et al., 2000), and to identify questions asking for similar information (Shtok et al., 2012; Zhang et al., 2017; Jeon et al., 2005); questions have also been classified by topic (Cao et al., 2010) and quality (Treude et al., 2011; Ravi et al., 2014).",2 Related Work,[0],[0]
"In contrast, our work is not concerned with the information need central to QA applications, and instead focuses on the rhetorical aspect of questions.
",2 Related Work,[0],[0]
Question types.,2 Related Work,[0],[0]
"To facilitate retrieval of frequently asked questions, Lytinen and Tomuro (2002) manually developed a typology of surface question forms (e.g., ‘what’- and ‘why’-questions) starting from Lehnerts’ conceptual question categories (Lehnert, 1978).",2 Related Work,[0],[0]
"Question types were also hand annotated for dialog-act labeling, distinguishing between yes-no, wh-, open-ended and rhetorical questions (Dhillon et al., 2004).",2 Related Work,[0],[0]
"To complement this line of work, this paper introduces a completely unsupervised methodology to automatically build a domain-tailored question typology, bypassing the need for human annotation.
",2 Related Work,[0],[0]
Pragmatic dimensions.,2 Related Work,[0],[0]
"One important pragmatic dimension of questions that has been previously studied computationally is their level of politeness (Danescu-Niculescu-Mizil et al., 2013; Aubakirova and Bansal, 2016); in the specific context of making requests, politeness was shown to correlate with the social status of the asker.",2 Related Work,[0],[0]
Sachdeva and Kumaraguru (2017) studied another rhetorical aspect by examining linguistic attributes distinguishing serviceable requests addressed to police on social media from general conversation.,2 Related Work,[0],[0]
"Previous research has also been directed at identifying rhetorical questions (Bhattasali et al., 2015) and understanding the motivations of their “askers” (Ranganath et al., 2016).",2 Related Work,[0],[0]
"Using the relationship between questions and answers, our work examines the rhetorical and social aspect of questions without predefining a pragmatic dimension and without relying on labeled data.",2 Related Work,[0],[0]
"We also complement these efforts in analyzing a broader range of situations in which questions may be posed without an information-seeking intent.
",2 Related Work,[0],[0]
Political discourse.,2 Related Work,[0],[0]
"Finally, our work contributes to a rapidly growing area of NLP applications to political domains (Monroe et al., 2008; Card et al.,
2016; Gonzalez-Bailon et al., 2010; Niculae et al., 2015; Grimmer and Stewart, 2013; Grimmer et al., 2012; Iyyer et al., 2014b, inter alia).",2 Related Work,[0],[0]
"Particularly relevant are applications to discourse in congressional and parliamentary settings (Thomas et al., 2006; Boydstun et al., 2014; Rheault et al., 2016).",2 Related Work,[0],[0]
"The bulk of our analysis focuses on the questions asked, and responses given during parliamentary question periods in the British House of Commons.",3 Data: Parliamentary Question Periods,[0],[0]
"Below, we provide a brief overview of key features of this political system in general, as well as a description of the question period setting.",3 Data: Parliamentary Question Periods,[0],[0]
Parliamentary systems.,3 Data: Parliamentary Question Periods,[0],[0]
"Legislators in the House of Commons (Members of Parliament, henceforth MPs or members) belong to two main voting and debating affiliations: a government party which controls the executive and holds a majority of the seats in the chamber, and a set of opposition parties.1",3 Data: Parliamentary Question Periods,[0],[0]
"The executive is headed by the Prime Minister (PM) and run by a cabinet of ministers, highranking government MPs responsible for various departments such as finance and education.",3 Data: Parliamentary Question Periods,[0],[0]
Question periods.,3 Data: Parliamentary Question Periods,[0],[0]
"The House of Commons holds weekly, moderated question periods, in which MPs of all affiliations take turns to ask questions to (and theoretically receive answers from) government ministers for each department regarding their specific domains.",3 Data: Parliamentary Question Periods,[0],[0]
Such events are a primary way in which legislators hold senior policy-makers responsible for their decisions.,3 Data: Parliamentary Question Periods,[0],[0]
"In practice, beyond narrow requests for information about specific policy points, MPs use their questions to critique or praise the government, or to self-promote; indeed, certain sessions, such as Questions to the Prime Minister, have gained renown for their partisan clashes, often fueled by the (mis)handling of a current crisis.",3 Data: Parliamentary Question Periods,[0],[0]
"The following question, asked to the Prime Minister by an opposition MP about contamination of the meat supply in 2013, encapsulates this odd mix of purposes:
“The Prime Minister is rightly shocked by the revelations that many food products contain 100% horse.",3 Data: Parliamentary Question Periods,[0],[0]
"Does he share my concern that, if tested, many of his answers may contain 100% bull?”2
1We use affiliation to refer broadly to the government and opposition roles, independent of the identity of the current government and opposition parties.",3 Data: Parliamentary Question Periods,[0],[0]
"In subsequent analysis we only consider the largest, “official” opposition party as the opposition.
",3 Data: Parliamentary Question Periods,[0],[0]
"2MPs almost always address each other in 3rd person.
",3 Data: Parliamentary Question Periods,[0],[0]
"The moderated, relatively rigid format of questions periods, along with the multifaceted array of underlying incentives and interpersonal relationships, yields a structurally controlled setting with a rich variety of social interactions, taking place in the realm of important policy discussions.",3 Data: Parliamentary Question Periods,[0],[0]
"This complexity makes question periods a particularly fruitful and consequential setting in which to study questions as social signals, and expand our understanding of their role beyond factual queries.",3 Data: Parliamentary Question Periods,[0],[0]
Dataset description.,3 Data: Parliamentary Question Periods,[0],[0]
"Our dataset covers question periods from May 1979 to December 2016, encompassing six different Prime Ministers.",3 Data: Parliamentary Question Periods,[0],[0]
"For each question period, we extract all questionanswer pairs, along with the identity of the asker and answerer.",3 Data: Parliamentary Question Periods,[0],[0]
"Because our focus here is on how questions are posed in a social setting, and not on the subsequent dialogue, we ignore questions which were tabled prior to the session, as well as any followup back-and-forth dialogue between the asker and answerer.
",3 Data: Parliamentary Question Periods,[0],[0]
"We augment this collection with metadata about each asker and answerer, including their political party, the time when they first took office, and whether they were serving as a minister at a given point in time.",3 Data: Parliamentary Question Periods,[0],[0]
"Such information is used to validate our methodology and interpret our results in light of the social context in which the questions were asked, described further in Sections 6 and 7.
",3 Data: Parliamentary Question Periods,[0],[0]
"In total there are 216,894 question-answer pairs in our data, occurring over 4,776 days and 6 prime-ministerships.",3 Data: Parliamentary Question Periods,[0],[0]
"The questions cover 1,975 different askers, 1,066 different answerers, and a variety of government departments with responsibilities ranging from defense to transport.",3 Data: Parliamentary Question Periods,[0],[0]
"We make this dataset publicly available, along with the code implementing our methodology, as part of the Cornell Conversational Analysis Toolkit.3",3 Data: Parliamentary Question Periods,[0],[0]
"The first component of our framework identifies lexico-syntactic phrasing patterns recurring in a collection of questions, which we call motifs.",4 Question Motifs,[0],[0]
"Intuitively, motifs constitute wordings commonly used to pose questions.",4 Question Motifs,[0],[0]
"To find motifs in a given collection, we first extract relevant fragments from each question.",4 Question Motifs,[0],[0]
"We then group sets of frequently co-occurring fragments into motifs.
",4 Question Motifs,[0],[0]
"3 https://github.com/CornellNLP/
Cornell-Conversational-Analysis-Toolkit
Question fragments.",4 Question Motifs,[0],[0]
Our goal is to find motifs which reflect functional characteristics of questions.,4 Question Motifs,[0],[0]
"Hence, we start by extracting the key fragments within a question which encapsulate its functional nature.",4 Question Motifs,[0],[0]
"Following the intuition that the bulk of this functional information is contained in the root of a question’s dependency parse along with its outgoing arcs (Iyyer et al., 2014a), we take the fragments of a question to be the root of its parse tree, along with each (root, child) pair.",4 Question Motifs,[0],[0]
"To capture cases when the operational word in the question is not connected to its root (such as “What...”), we also consider the initial unigram and bigram of a question as fragments.",4 Question Motifs,[0],[0]
"The following question has 5 fragments: what, what is, going→*, is←going and going→do.
",4 Question Motifs,[0],[0]
"(1) What is the minister going to do about ... ?
",4 Question Motifs,[0],[0]
"Because our goal is to capture topic-agnostic patterns, we ignore all fragments which contain a noun phrase (NP) or pronoun.",4 Question Motifs,[0],[0]
"NP subtrees are identified based on their outgoing dependencies to the root;4 in the event that an NP starts with a WHdeterminer (WDT), we consider (root, WDT) to be a fragment and drop the remainder of the NP.5
Finally, we note that some questions consist of multiple sub-questions (“What does the Minister think [...], and why [...]?”).",4 Question Motifs,[0],[0]
"For such questions, we recursively extract fragments from each child subtree in the same manner, starting from their roots.",4 Question Motifs,[0],[0]
From fragments to motifs.,4 Question Motifs,[0],[0]
We define motifs as sets of question fragments that frequently co-occur (in at least n questions).,4 Question Motifs,[0],[0]
"We find motifs by applying the apriori algorithm (Agrawal and Srikant, 1994) to find these common itemsets.",4 Question Motifs,[0],[0]
"This results in a collection of motifs M which correspond to different question phrasings.6 Examples of motifs are shown in Table 1.
",4 Question Motifs,[0],[0]
Motifs can identify phrasings to varying degrees of specificity.,4 Question Motifs,[0],[0]
"For example, the singleton motif {what is} corresponds to all questions starting with that bigram, while {what is, going→do} nar-
4We take as NPs subtrees connected to the root with the following: nsubj, nsubjpass, dobj, iobj, pobj, attr.
",4 Question Motifs,[0],[0]
"5In the particular case of the Parliament dataset, removing NPs also removes conventional, partisan address terms (e.g. “my hon. Friend”).
",4 Question Motifs,[0],[0]
"6In some cases, a pair of motifs almost always co-occurs in the same questions, making them redundant.",4 Question Motifs,[0],[0]
"We treat two motifs m1 and m2 as equivalent if, for some probability p, Pr(m1|m2) > p and Pr(m2|m1)",4 Question Motifs,[0],[0]
"> p; we keep the smaller of the two as the representative motif, or pick one of them arbitrarily if they are of equal sizes.
rows these down to questions also containing the fragment going→do.",4 Question Motifs,[0],[0]
"To model the specificity relation between motifs, we structure M as a directed acyclic graph where an edge points from a motif m1 to another motif m2 if the latter has exactly one more fragment in addition to those in m1, corresponding to a narrower set of phrasings.",4 Question Motifs,[0],[0]
Motif-representation of a question.,4 Question Motifs,[0],[0]
"Finally, a question q contains a motif if it includes all of the fragments comprising that motif.",4 Question Motifs,[0],[0]
"We can hence capture the phrasing of a given question q using the subset of motifs it contains, structured as the subgraphMq ⊂ M induced by this subset.",4 Question Motifs,[0],[0]
"This directed subgraph represents the question at multiple levels of specificity simultaneously; in particular, the set of sinks (i.e., nodes with outdegree 0; henceforth sink motifs) ofMq is the most finegrained way to specify the phrasing of q. For example {what is, is←going, going→do} is the only sink motif of the question in example (1); its entire subgraph is shown in Figure 3 in the appendix.",4 Question Motifs,[0],[0]
"The second component of our framework structures the space of questions according to their functional roles, thus going beyond the lexicosyntactic representation captured via motifs.",5 Latent Question Types,[0],[0]
The main intuition is that the nature of the answer that a question receives provides a good indication of its intention.,5 Latent Question Types,[0],[0]
"Therefore, if two questions are phrased differently but answered in similar ways, the parallels exhibited by their answers should reflect commonalities in the askers’ intentions.
",5 Latent Question Types,[0],[0]
"To operationalize this intuition, we first construct a latent space based on answers, and then map question motifs (Section 4) to the same space.",5 Latent Question Types,[0],[0]
"Using the resultant latent representations, we can then cluster questions with similar rhetorical functions, even if their surface forms are different.",5 Latent Question Types,[0],[0]
Constructing a space of answers.,5 Latent Question Types,[0],[0]
"In line with our focus on functional characterizations, we extract the fragments from each sentence of an answer, defined in the same way as question fragments.",5 Latent Question Types,[0],[0]
"We then construct a term-document matrix, where terms correspond to answer fragments, and documents correspond to individual answers in the corpus.",5 Latent Question Types,[0],[0]
"We filter out infrequent fragments occurring less than nA times, reweight the rows of this matrix with tf-idf reweighting, and scale to unit norm, producing a fragment-answer matrix A. We perform singular value decomposi-
tion on A and obtain a low-rank representation A ≈ Â = UASV TA , for some rank d, where rows of UA correspond to answer fragments and rows of VA correspond to answers.7
Latent projection of question motifs.",5 Latent Question Types,[0],[0]
We can draw a natural correspondence between a question motif m and answer term t if m occurs in a question whose answer contains t.,5 Latent Question Types,[0],[0]
"This enables us to compute representations of question motifs in the same space as Â. Concretely, we construct a motif-question matrix Q = (qij) where qij = 1 if motif i occurred in question j; we scale rows of Q to unit norm.",5 Latent Question Types,[0],[0]
"To represent Q in the latent answer space, we solve for Q̂ in Q = Q̂SV TA as Q̂ = QVAS−1, again scaling rows to unit norm.",5 Latent Question Types,[0],[0]
"Row i of Q̂ then gives a d-dimensional representation of motif i, denoted q̂i.
",5 Latent Question Types,[0],[0]
Grouping similar questions.,5 Latent Question Types,[0],[0]
"Finally, we identify question types—broad groups of similar motifs.",5 Latent Question Types,[0],[0]
"Intuitively, if two motifs mi and mj have vectors qi and qj which are close together, they elicit answers that are close in the latent space, so are functionally similar in this sense.",5 Latent Question Types,[0],[0]
"We use the KMeans algorithm (Pedregosa et al., 2011) to cluster motif vectors into k clusters; these clusters then constitute the desired set of question types.
",5 Latent Question Types,[0],[0]
"To determine the type of a particular question q∗, we transform it to a binary vector (q∗i ) where q∗i = 1 if motif i is a sink motif of q
∗; using only sink motifs at this stage allows us to characterize a question according to the most specific representation of its phrasing, thus avoiding spurious associations resulting from more general motifs.",5 Latent Question Types,[0],[0]
"We scale q∗, project it to the latent space as before, and assign the resultant projection q̂∗ to a cluster t, hence determining its type.
",5 Latent Question Types,[0],[0]
"Since question motifs and answer fragments have both been mapped to the same latent space (as rows of Q̂ and UA respectively), we can also assign each answer fragment to a question type.",5 Latent Question Types,[0],[0]
"This further facilitates interpretability through characterizing the answers commonly triggered by a particular type of question.
",5 Latent Question Types,[0],[0]
"7We experimented with grouping answer fragments into motifs as well, but found that most of the motifs produced were one fragment large.",5 Latent Question Types,[0],[0]
"While future work could focus more on understanding consistent phrasings of answers, we note that at least in our chosen corpus, answers are longer and encompass a much greater variation of possible phrasings.",5 Latent Question Types,[0],[0]
"We now apply our general framework to the particular setting of parliamentary question periods, structuring the space of questions posed within these sessions according to their rhetorical function.",6 Validation,[0],[0]
"To validate the induced typology, we quantitatively show that it recovers asker intentions in an expert-coded dataset, and qualitatively aligns with prior findings in the political science literature on parliamentary dynamics.",6 Validation,[0],[0]
Question types in Parliament.,6 Validation,[0],[0]
"We apply our motif extraction and question type induction pipeline to the questions in the parliamentary dataset.8 Over 90% of the questions in the dataset contain at least one of the resulting 2,817 motifs; in subsequent analyses we discard questions without a matching motif.",6 Validation,[0],[0]
"We apply our pipeline to the questions in the parliamentary dataset, and induce a typology of k = 8 question types to capture the rich array of questions represented in this space while preserving interpretability.
",6 Validation,[0],[0]
"Table 1 displays extracted types, along with example questions, answers, and motifs.9",6 Validation,[0],[0]
"The second author, a political scientist with domain expertise in the UK parliamentary setting, manually investigated each type and provided interpretable labels.",6 Validation,[0],[0]
"For example, in questions of type 4, the asker is aware that his main premise is supported by the minister, and thus will be met with a positive statement backing the thrust of the question; we call this the agreement cluster.",6 Validation,[0],[0]
"Types 6 and 7 are much more combative: in type 6 questions the asker explicitly attempts to force the minister to concede/accept a point that would undermine some government stance, while type 7 contains condemnatory questions that prompt the minister to justify a policy that is self-evidently bad in the eyes of the asker.",6 Validation,[0],[0]
"In contrast, type 2 constitutes tamer narrow queries that require the minister to simply report on non-partisan matters of policy.",6 Validation,[0],[0]
(Extended interpretations in the appendix.),6 Validation,[0],[0]
Quantitative validation.,6 Validation,[0],[0]
"We compare our output to a dataset of 1,256 questions asked to various Prime Ministers labeled by Bates et al. (2014)
8We consider questions to be sentences ending in question marks.",6 Validation,[0],[0]
"If an utterance consists of multiple questions, we extract fragments sets from each question separately, and take the motifs of the utterance to be the union of motifs of each component question.",6 Validation,[0],[0]
"We set n = 100, p = 0.9, nA = 100 and d = 25.",6 Validation,[0],[0]
"The choice of parameters was done via manual inspection of the dataset.
",6 Validation,[0],[0]
"9Each type contains a few hundred question motifs and answer fragments.
(also included in our data distribution).",6 Validation,[0],[0]
"Each question in this data is hand-coded by a domain expert with one of three labels indicating the rhetorical intention of the asker: compared to standard questions—denoting straightforward factual queries, helpful questions serve as prompts for the PM to talk favorably about their government, while unanswerable questions are effectively vehicles for delivering criticisms that the PM cannot respond to.",6 Validation,[0],[0]
Questions which are unanswered by the PM are also labeled.,6 Validation,[0],[0]
"If our framework captures meaningful rhetorical dimensions, we expect a given label to be over-represented in some of our induced types, and under-represented in others.
",6 Validation,[0],[0]
"Even though our clustering of questions is generated in an unsupervised fashion without any guidance from the coded rhetorical roles, we see that several of the types we discover closely align with these annotations.",6 Validation,[0],[0]
"In particular, helpful questions are highly associated with the agreement type (constituting 28% of questions of that type compared to 14% over the entire dataset; binomial test p < 0.01), reinforcing our interpretation
that this type captures MPs cheerleading their own government.",6 Validation,[0],[0]
"Conversely, unanswerable questions are frequently of the concede/accept type (20% in-type vs. 11% overall), while condemnatory questions are often unanswered (43% vs. 24% overall), suggesting that questions of these types have an increased tendency to be posed as aggressive criticisms packaged as questions.
",6 Validation,[0],[0]
"We also validate our framework in a prediction setting using these labels, in three binary classification tasks: distinguishing helpful vs. standard, unanswerable vs. standard, and unanswered vs. answered questions.",6 Validation,[0],[0]
"(In each task, we balance the two classes.)",6 Validation,[0],[0]
"To control for asker affiliation effects, we consider only questions asked by government MPs for the helpful task, and opposition questions in the unanswerable and unanswered tasks; we train on questions to Conservative PMs and evaluate on Labour PMs.10 For each setting, we train logistic regression classifiers; as
10These choices are motivated by the number of questions from each affiliation and party in the dataset (see appendix for further details on this dataset).
",6 Validation,[0],[0]
"features we compare the latent representation of each question to a unigram BOW baseline.11
In the unanswerable and unanswered tasks, we find that the BOW features do not perform significantly better than a random (50%) baseline.",6 Validation,[0],[0]
"However, the latent question features produced by our framework bring additional predictive signal and outperform the baseline when combined with BOW (binomial p < 0.05), achieving accuracies of 66% and 62% respectively (compared with 55% and 50% for BOW alone).",6 Validation,[0],[0]
"This suggests that our representation captures useful rhetorical information that, given our train-test split, generalizes across parties.",6 Validation,[0],[0]
"None of the models significantly outperform the random baseline on the helpful task, perhaps owing to the small data size.",6 Validation,[0],[0]
Qualitative validation: question partisanship.,6 Validation,[0],[0]
We additionally provide a qualitative validation of our framework by comparing the questionasking activity of government and oppositionaffiliated MPs—as viewed through the extracted question types—to well-established characterizations of these affiliations in the political science literature.,6 Validation,[0],[0]
"In particular, prior work has examined the bifurcation in behavior between government and opposition members, in their differing focus on various issues (Louwerse, 2012), and in settings such as roll call votes (Cowley, 2002;",6 Validation,[0],[0]
"Spirling and McLean, 2007; Eggers and Spirling, 2014).",6 Validation,[0],[0]
"Since government MPs are elected on the same party ticket and manifesto, they primarily act to sup-
11We used tf-idf reweighting and excluded unigrams occurring less than 5 times.
",6 Validation,[0],[0]
"port the government’s various policies and bolster the status of their cabinet, seldom airing disagreements publicly.",6 Validation,[0],[0]
"In contrast, opposition members tend to offer trenchant partisan criticism of government policies, seeking to destabilize the government’s relationship with its MPs and create negative press in the country at large.",6 Validation,[0],[0]
"In characterizing the question-asking activity of government and opposition MPs, this friendly vs. adversarial behavior should also be reflected in a rhetorical typology of questions.12
Concretely, to quantify the relationship between a particular question type t and asker affiliation P , we compute the log-odds ratio of type t questions asked by MPs in P , compared to MPs not in P .13
Figure 1A shows the resultant log-odds ratios of each question type for government and opposition members.",6 Validation,[0],[0]
"Notably, we see that agreement-type questions are significantly more likely to originate from government than from opposition MPs, while the opposite holds for concede/accept and condemnatory questions (binomial p < 10−4 for each, comparing within-type to overall proportions of questions from an affiliation).",6 Validation,[0],[0]
"No such
12While we induce the typology over our entire dataset, we perform all subsequent analyses on a filtered subset of 50,152 questions.",6 Validation,[0],[0]
"In particular, we omit utterances with multiple questions—i.e. multiple question marks—to ensure that we don’t confound effects arising from different co-occurring question types.",6 Validation,[0],[0]
Our filtering decisions are also determined by the availability of information about the asker and answerers’ roles in Parliament.,6 Validation,[0],[0]
"Further information about these filtering choices can be found in the appendix.
",6 Validation,[0],[0]
"13The log-odds values are not symmetric between government and opposition, because they includes questions asked by MPs not in the official opposition.
",6 Validation,[0],[0]
"slant is exhibited in the narrow factual type, further reinforcing the role of such questions as informational queries about relatively non-partisan issues.",6 Validation,[0],[0]
"These results strongly cohere with the “textbook” accounts of parliamentary activity in the literature, as well as our interpretation of these types as bolstering or antagonistic.
",6 Validation,[0],[0]
"Moreover, we find that the same MP shifts in her propensity for different question types as her affiliation changes.",6 Validation,[0],[0]
"When a new political party is elected into office, MPs who were previously in the opposition now belong to the government party, and vice versa.",6 Validation,[0],[0]
"Such a switch occurs within our data between the Major and Blair governments (Conservative to Labour, 1997), and between the Brown and Cameron governments (Labour to Conservative, 2010).",6 Validation,[0],[0]
"For both switches, we consider all MPs who asked at least 5 questions both before and after the switch, resulting in 88 members who became government MPs and 102 who became opposition MPs.",6 Validation,[0],[0]
"For an MP M we compute PM,t, their propensity for a question type t, as the proportion of questions they ask which are from t. Comparing PM,t before and after a switch, we replicate the key differences observed above—for instance, we find that former opposition MPs who become government MPs decrease in their propensity for condemnatory questions, while newly opposition MPs move in the other direction (Wilcoxon p < 0.001, Figure 1B).",6 Validation,[0],[0]
"This suggests that the general trends we observed before are driven by the shift in affiliation, and hence parliamentary role, of individual MPs.",6 Validation,[0],[0]
"We now apply our framework to gain further insights into the nature of political discourse in Parliament, focusing on how questioning behavior varies with a member’s tenure in the institution.",7 Career Trajectory Effects,[0],[0]
"As stated in the introduction, two alternative hypotheses arise: younger MPs may be more vigorously critical out of enthusiasm, but are potentially tempered by their stake in future promotion prospects compared to older members (Cowley, 2002, 2012).",7 Career Trajectory Effects,[0],[0]
"Alternatively, older MPs who have less at stake in terms of prospects of further promotion may ask more antagonistic questions.",7 Career Trajectory Effects,[0],[0]
"Throughout, young and old refer to tenure—i.e., how many years someone has served as an MP— rather than biological age.
",7 Career Trajectory Effects,[0],[0]
"In order to understand the extent to which young or old members contribute a specific type of question, for each question type t we compute the median tenure of askers of each question in t, and compare the median tenures of different question types, for each affiliation (Figure 2A).14 We see that among both affiliations, more aggressive questions tend to originate more from older members, reflected in significantly higher median tenures (for types 6 in both affiliations, and 7 in government MPs; Mann Whitney U test p < 0.001 comparing within-type median tenure with outside-type median tenure); whereas standard issue update questions tend to come from younger
14Median tenures for opposition members are generally higher; winning an election tends to result in more newlyelected and therefore younger MPs (Webb and Farrell, 1999).
",7 Career Trajectory Effects,[0],[0]
"members (p < 0.001, both affiliations).",7 Career Trajectory Effects,[0],[0]
"Notably, the disproportionate aggressiveness of older members manifests even among government MPs who direct these questions towards their own government.",7 Career Trajectory Effects,[0],[0]
"This supports the “less to lose” intuition, offering a rhetorical parallel to previous findings about the increased tendency to vote contrary to party lines from MPs with little chance of ministerial promotion (Benedetto and Hix, 2007).
",7 Career Trajectory Effects,[0],[0]
"Interestingly, we find that these differential preferences across member tenure also manifest at a finer granularity than simply less to more aggressive.",7 Career Trajectory Effects,[0],[0]
"For instance, younger opposition members tend to contribute more condemnatory questions compared to older members (Mann Whitney U test p < 0.01), who disproportionately favor concede/accept questions.",7 Career Trajectory Effects,[0],[0]
"While further work is needed to fully explain these differences, we speculate that they are potentially reflective of strategic attempts by younger MPs to signal traits that could facilitate future promotion, such as partisan loyalty (Kam, 2009).
",7 Career Trajectory Effects,[0],[0]
"To discount the possibility of these effects being solely driven by a few very prolific young or old MPs, we also consider a setting where type propensities are macroaveraged over MPs.",7 Career Trajectory Effects,[0],[0]
"For each affiliation we compare the cohort of younger MPs who are newly voted in at the 1997 and 2010 elections, with older MPs who have been in office prior to the election.15 We compute the type propensities of these two cohorts over the questions they asked during the subsequent parliamentary sitting, and replicate the tenure effects observed previously (Figure 2B).",7 Career Trajectory Effects,[0],[0]
"This suggests that these parliamentary career effects reflect behavioral changes at the level of individual MPs, whose incentives evolve over their tenure.",7 Career Trajectory Effects,[0],[0]
In this work we introduced an unsupervised framework for structuring the space of questions according to their rhetorical role.,8 Conclusion and Future Work,[0],[0]
"We instantiated and validated our approach in the domain of parliamentary question periods, and revealed new interactions between questioning behavior and career trajectories.
",8 Conclusion and Future Work,[0],[0]
We note that our methodology is not tied to a particular domain.,8 Conclusion and Future Work,[0],[0]
"It would be interesting to explore its potential in a variety of less structured do-
15This totals 272 new and 184 old government MPs, and 84 new and 179 old opposition MPs.
mains where questions likewise play a crucial role.",8 Conclusion and Future Work,[0],[0]
"For example, examining how interviewers in highprofile media settings (e.g., Frost on Nixon) can use their questions to elicit substantive responses from influential people would aid us in the broader normative goal of holding elites to account, by gaining a better understanding of what and how to ask, and what (not) to accept as an answer.
",8 Conclusion and Future Work,[0],[0]
"From a technical standpoint, future work could also augment the representation of questions and answers presently used in our framework, beyond our heuristic of using root arcs without noun phrases.",8 Conclusion and Future Work,[0],[0]
"Richer linguistic representations, as well as more judicious ways of weighting different fragments and motifs, could enable us to capture a wider range of possible surface and rhetorical forms, especially in settings where phrasings are potentially less structured by institutional conventions.",8 Conclusion and Future Work,[0],[0]
"Additionally, as with most unsupervised methods, our approach is limited by the need to hand-select parameters such as the number of clusters, and manually interpret the typology’s output.",8 Conclusion and Future Work,[0],[0]
"Having annotations of these corpora could better motivate the methodology and enable further evaluation and interpretation; we hope to encourage such annotation efforts by releasing the dataset.
",8 Conclusion and Future Work,[0],[0]
"Inevitably, drawing causal lessons from observational data is difficult.",8 Conclusion and Future Work,[0],[0]
"Moving forward, experimental tests of insights gathered through such explorations would enable us to establish causal effects of question-asking rhetoric, perhaps offering prescriptive insights into questioning strategies for objectives such as information-seeking (Dillman, 1978), request-making (Althoff et al., 2014; Mitra and Gilbert, 2014) and persuasion (Tan et al., 2016; Zhang et al., 2016; Wang et al., 2017).
",8 Conclusion and Future Work,[0],[0]
Acknowledgements.,8 Conclusion and Future Work,[0],[0]
"The first author thanks John Bercow, the Speaker of the House, for suggesting she “calm [her]self
down by taking up yoga” during the hectic deadline push
(https://youtu.be/AiAWdLAIj3c).",8 Conclusion and Future Work,[0],[0]
"The authors
thank the anonymous reviewers and Liye Fu for their com-
ments and for their helpful questions.",8 Conclusion and Future Work,[0],[0]
"We are grateful to the
organizers of the conference on New Directions in Text as
Data for fostering the inter-disciplinary collaboration that led
to this work, to Amber Boydstun and Philip Resnik for their
insights on questions in the political domain, and to Stephen
Bates, Peter Kerr and Christopher Byrne for sharing the la-
beled PMQ dataset.",8 Conclusion and Future Work,[0],[0]
"This research has been supported in part
by a Discovery and Innovation Research Seed Award from
the Office of the Vice Provost for Research at Cornell.",8 Conclusion and Future Work,[0],[0]
"Questions play a prominent role in social interactions, performing rhetorical functions that go beyond that of simple informational exchange.",abstractText,[0],[0]
"The surface form of a question can signal the intention and background of the person asking it, as well as the nature of their relation with the interlocutor.",abstractText,[0],[0]
"While the informational nature of questions has been extensively examined in the context of question-answering applications, their rhetorical aspects have been largely understudied.",abstractText,[0],[0]
"In this work we introduce an unsupervised methodology for extracting surface motifs that recur in questions, and for grouping them according to their latent rhetorical role.",abstractText,[0],[0]
"By applying this framework to the setting of question sessions in the UK parliament, we show that the resulting typology encodes key aspects of the political discourse—such as the bifurcation in questioning behavior between government and opposition parties—and reveals new insights into the effects of a legislator’s tenure and political career ambitions.",abstractText,[0],[0]
Asking too much? The rhetorical role of questions in political discourse,title,[0],[0]
"Aspect level sentiment classification is a fundamental task in the field of sentiment analysis (Pang and Lee, 2008; Liu, 2012; Pontiki et al., 2014).",1 Introduction,[1.0],"['Aspect level sentiment classification is a fundamental task in the field of sentiment analysis (Pang and Lee, 2008; Liu, 2012; Pontiki et al., 2014).']"
"Given a sentence and an aspect occurring in the sentence, this task aims at inferring the sentiment polarity (e.g. positive, negative, neutral) of the aspect.",1 Introduction,[1.0],"['Given a sentence and an aspect occurring in the sentence, this task aims at inferring the sentiment polarity (e.g. positive, negative, neutral) of the aspect.']"
"For example, in sentence “great food but the service was dreadful!”, the sentiment polarity of aspect “food” is positive while the polarity of aspect
∗ Corresponding author.
",1 Introduction,[0],[0]
“service” is negative.,1 Introduction,[0],[0]
Researchers typically use machine learning algorithms and build sentiment classifier in a supervised manner.,1 Introduction,[0],[0]
"Representative approaches in literature include feature based Support Vector Machine (Kiritchenko et al., 2014; Wagner et al., 2014) and neural network models (Dong et al., 2014; Lakkaraju et al., 2014; Vo and Zhang, 2015; Nguyen and Shirai, 2015; Tang et al., 2015a).",1 Introduction,[1.0],"['Representative approaches in literature include feature based Support Vector Machine (Kiritchenko et al., 2014; Wagner et al., 2014) and neural network models (Dong et al., 2014; Lakkaraju et al., 2014; Vo and Zhang, 2015; Nguyen and Shirai, 2015; Tang et al., 2015a).']"
"Neural models are of growing interest for their capacity to learn text representation from data without careful engineering of features, and to capture semantic relations between aspect and context words in a more scalable way than feature based SVM.
",1 Introduction,[1.000000076046164],"['Neural models are of growing interest for their capacity to learn text representation from data without careful engineering of features, and to capture semantic relations between aspect and context words in a more scalable way than feature based SVM.']"
"Despite these advantages, conventional neural models like long short-term memory (LSTM) (Tang et al., 2015a) capture context information in an implicit way, and are incapable of explicitly exhibiting important context clues of an aspect.",1 Introduction,[0],[0]
We believe that only some subset of context words are needed to infer the sentiment towards an aspect.,1 Introduction,[0],[0]
"For example, in sentence “great food but the service was dreadful!”, “dreadful” is an important clue for the aspect “service” but “great” is not needed.",1 Introduction,[0],[0]
"Standard LSTM works in a sequential way and manipulates each context word with the same operation, so that it cannot explicitly reveal the importance of each context word.",1 Introduction,[0],[0]
A desirable solution should be capable of explicitly capturing the importance of context words and using that information to build up features for the sentence after given an aspect word.,1 Introduction,[0],[0]
"Furthermore, a human asked to do this task will selectively focus on parts of the contexts, and acquire information where it is needed to build up an internal representation towards an aspect in his/her mind.",1 Introduction,[0],[0]
"ar X iv :1 60 5.
",1 Introduction,[0],[0]
"08 90
0v 2
[ cs
.C",1 Introduction,[0],[0]
"L
] 2
4 Se
p 20
In pursuit of this goal, we develop deep memory network for aspect level sentiment classification, which is inspired by the recent success of computational models with attention mechanism and explicit memory (Graves et al., 2014; Bahdanau et al., 2015; Sukhbaatar et al., 2015).",1 Introduction,[0],[0]
"Our approach is data-driven, computationally efficient and does not rely on syntactic parser or sentiment lexicon.",1 Introduction,[0],[0]
The approach consists of multiple computational layers with shared parameters.,1 Introduction,[0],[0]
"Each layer is a content- and location- based attention model, which first learns the importance/weight of each context word and then utilizes this information to calculate continuous text representation.",1 Introduction,[1.0],"['Each layer is a content- and location- based attention model, which first learns the importance/weight of each context word and then utilizes this information to calculate continuous text representation.']"
The text representation in the last layer is regarded as the feature for sentiment classification.,1 Introduction,[1.0],['The text representation in the last layer is regarded as the feature for sentiment classification.']
"As every component is differentiable, the entire model could be efficiently trained end-toend with gradient descent, where the loss function is the cross-entropy error of sentiment classification.
",1 Introduction,[0],[0]
"We apply the proposed approach to laptop and restaurant datasets from SemEval 2014 (Pontiki et al., 2014).",1 Introduction,[0],[0]
"Experimental results show that our approach performs comparable to a top system using feature-based SVM (Kiritchenko et al., 2014).",1 Introduction,[0],[0]
"On both datasets, our approach outperforms both LSTM and attention-based LSTM models (Tang et al., 2015a) in terms of classification accuracy and running speed.",1 Introduction,[0],[0]
"Lastly, we show that using multiple computational layers over external memory could achieve improved performance.",1 Introduction,[0],[0]
"Our approach is inspired by the recent success of memory network in question answering (Weston et al., 2014; Sukhbaatar et al., 2015).",2 Background: Memory Network,[0],[0]
"We describe the background on memory network in this part.
",2 Background: Memory Network,[0],[0]
Memory network is a general machine learning framework introduced by Weston et al. (2014).,2 Background: Memory Network,[0],[0]
"Its central idea is inference with a long-term memory component, which could be read, written to, and jointly learned with the goal of using it for prediction.",2 Background: Memory Network,[0],[0]
"Formally, a memory network consists of a memory m and four components I , G, O and R, where m is an array of objects such as an array of vectors.",2 Background: Memory Network,[0],[0]
"Among these four components, I converts input to internal feature representation, G updates old memories with new input, O generates an out-
put representation given a new input and the current memory state, R outputs a response based on the output representation.
",2 Background: Memory Network,[0],[0]
Let us take question answering as an example to explain the work flow of memory network.,2 Background: Memory Network,[0],[0]
"Given a list of sentences and a question, the task aims to find evidences from these sentences and generate an answer, e.g. a word.",2 Background: Memory Network,[1.0],"['Given a list of sentences and a question, the task aims to find evidences from these sentences and generate an answer, e.g. a word.']"
"During inference, I component reads one sentence si at a time and encodes it into a vector representation.",2 Background: Memory Network,[0],[0]
Then G component updates a piece of memory mi based on current sentence representation.,2 Background: Memory Network,[0],[0]
"After all sentences are processed, we get a memory matrix m which stores the semantics of these sentences, each row representing a sentence.",2 Background: Memory Network,[0],[0]
"Given a question q, memory network encodes it into vector representation eq, and then O component uses eq to select question related evidences from memory m and generates an output vector o. Finally, R component takes o as the input and outputs the final response.",2 Background: Memory Network,[0],[0]
It is worth noting that O component could consist of one or more computational layers (hops).,2 Background: Memory Network,[0],[0]
The intuition of utilizing multiple hops is that more abstractive evidences could be found based on previously extracted evidences.,2 Background: Memory Network,[0],[0]
"Sukhbaatar et al. (2015) demonstrate that multiple hops could uncover more abstractive evidences than single hop, and could yield improved results on question answering and language modeling.",2 Background: Memory Network,[0],[0]
"In this section, we describe the deep memory network approach for aspect level sentiment classification.",3 Deep Memory Network for Aspect Level Sentiment Classification,[0],[0]
We first give the task definition.,3 Deep Memory Network for Aspect Level Sentiment Classification,[0],[0]
"Afterwards, we describe an overview of the approach before presenting the content- and location- based attention models in each computational layer.",3 Deep Memory Network for Aspect Level Sentiment Classification,[0],[0]
"Lastly, we describe the use of this approach for aspect level sentiment classification.",3 Deep Memory Network for Aspect Level Sentiment Classification,[0],[0]
"Given a sentence s = {w1, w2, ..., wi, ...wn} consisting of n words and an aspect word wi 1 occur-
1In practice, an aspect might be a multi word expression such as “battery life”.",3.1 Task Definition and Notation,[0],[0]
"For simplicity we still consider aspect as a single word in this definition.
ring in sentence s, aspect level sentiment classification aims at determining the sentiment polarity of sentence s towards the aspect wi.",3.1 Task Definition and Notation,[0],[0]
"For example, the sentiment polarity of sentence “great food but the service was dreadful!”",3.1 Task Definition and Notation,[0],[0]
"towards aspect “food” is positive, while the polarity towards aspect “service” is negative.",3.1 Task Definition and Notation,[0],[0]
"When dealing with a text corpus, we map each word into a low dimensional, continuous and real-valued vector, also known as word embedding (Mikolov et al., 2013; Pennington et al., 2014).",3.1 Task Definition and Notation,[0],[0]
"All the word vectors are stacked in a word embedding matrix L ∈ Rd×|V |, where d is the dimension of word vector and |V",3.1 Task Definition and Notation,[0],[0]
| is vocabulary size.,3.1 Task Definition and Notation,[0],[0]
"The word embedding of wi is notated as ei ∈ Rd×1, which is a column in the embedding matrix L.",3.1 Task Definition and Notation,[0],[0]
"We present an overview of the deep memory network for aspect level sentiment classification.
",3.2 An Overview of the Approach,[0],[0]
"Given a sentence s = {w1, w2, ..., wi, ...wn} and the aspect word wi, we map each word into its embedding vector.",3.2 An Overview of the Approach,[0],[0]
"These word vectors are separated into two parts, aspect representation and context representation.",3.2 An Overview of the Approach,[0],[0]
"If aspect is a single word like “food” or “service”, aspect representation is the embedding of aspect word.",3.2 An Overview of the Approach,[0],[0]
"For the case where aspect is multi word expression like “battery life”, aspect representation is an average of its constituting word vectors (Sun et al., 2015).",3.2 An Overview of the Approach,[0],[0]
"To simplify the interpretation, we consider aspect as a single word wi.",3.2 An Overview of the Approach,[0],[0]
"Context word vectors {e1, e2 ...",3.2 An Overview of the Approach,[0],[0]
"ei−1, ei+1 ...",3.2 An Overview of the Approach,[0],[0]
"en} are stacked and regarded as the external memory m ∈ Rd×(n−1), where n is the sentence length.
",3.2 An Overview of the Approach,[0],[0]
"An illustration of our approach is given in Figure 1, which is inspired by the use of memory network in question answering (Sukhbaatar et al., 2015).",3.2 An Overview of the Approach,[0],[0]
"Our approach consists of multiple computational layers (hops), each of which contains an attention layer and a linear layer.",3.2 An Overview of the Approach,[0],[0]
"In the first computational layer (hop 1), we regard aspect vector as the input to adaptively select important evidences from memory m through attention layer.",3.2 An Overview of the Approach,[1.0],"['In the first computational layer (hop 1), we regard aspect vector as the input to adaptively select important evidences from memory m through attention layer.']"
The output of attention layer and the linear transformation of aspect vector2 are summed and the result is considered as the input of next layer (hop 2).,3.2 An Overview of the Approach,[0],[0]
"In a similar way, we stack multiple hops and
2In preliminary experiments, we tried directly using aspect vector without a linear transformation, and found that adding a linear layer works slightly better.
run these steps multiple times, so that more abstractive evidences could be selected from the external memory m. The output vector in last hop is considered as the representation of sentence with regard to the aspect, and is further used as the feature for aspect level sentiment classification.
",3.2 An Overview of the Approach,[0],[0]
It is helpful to note that the parameters of attention and linear layers are shared in different hops.,3.2 An Overview of the Approach,[0],[0]
"Therefore, the model with one layer and the model with nine layers have the same number of parameters.",3.2 An Overview of the Approach,[0],[0]
We describe our attention model in this part.,3.3 Content Attention,[0],[0]
"The basic idea of attention mechanism is that it assigns a weight/importance to each lower position when computing an upper level representation (Bahdanau et al., 2015).",3.3 Content Attention,[0],[0]
"In this work, we use attention model to compute the representation of a sentence with regard to an aspect.",3.3 Content Attention,[0],[0]
The intuition is that context words do not contribute equally to the semantic meaning of a sentence.,3.3 Content Attention,[0],[0]
"Furthermore, the importance of a word should be different if we focus on different aspect.",3.3 Content Attention,[0],[0]
Let us again take the example of “great food but the service was dreadful!”.,3.3 Content Attention,[0],[0]
The context word “great” is more important than “dreadful” for aspect “food”.,3.3 Content Attention,[0],[0]
"On the contrary, “dreadful” is more important than “great” for aspect “service”.
",3.3 Content Attention,[0],[0]
"Taking an external memory m ∈ Rd×k and an aspect vector vaspect ∈ Rd×1 as input, the attention model outputs a continuous vector vec ∈ Rd×1.",3.3 Content Attention,[0],[0]
"The
output vector is computed as a weighted sum of each piece of memory in m, namely
vec = k∑
i=1
αimi (1)
where k is the memory size, αi ∈",3.3 Content Attention,[0],[0]
"[0, 1] is the weight of mi and ∑ i αi = 1.",3.3 Content Attention,[0],[0]
We implement a neural network based attention model.,3.3 Content Attention,[0],[0]
"For each piece of memory mi, we use a feed forward neural network to compute its semantic relatedness with the aspect.",3.3 Content Attention,[0],[0]
"The scoring function is calculated as follows, where Watt ∈ R1×2d and batt ∈ R1×1.
gi = tanh(Watt[mi; vaspect] + batt) (2)
After obtaining {g1, g2, ...",3.3 Content Attention,[0],[0]
"gk}, we feed them to a softmax function to calculate the final importance scores {α1, α2, ... αk}.
",3.3 Content Attention,[0],[0]
αi =,3.3 Content Attention,[0],[0]
"exp(gi)∑k j=1 exp(gj)
(3)
",3.3 Content Attention,[0],[0]
We believe that such an attention model has two advantages.,3.3 Content Attention,[0],[0]
One advantage is that this model could adaptively assign an importance score to each piece of memory mi according to its semantic relatedness with the aspect.,3.3 Content Attention,[0],[0]
"Another advantage is that this attention model is differentiable, so that it could be easily trained together with other components in an end-to-end fashion.",3.3 Content Attention,[0],[0]
We have described our neural attention framework and a content-based model in previous subsection.,3.4 Location Attention,[0],[0]
"However, the model mentioned above ignores the location information between context word and aspect.",3.4 Location Attention,[0],[0]
Such location information is helpful for an attention model because intuitively a context word closer to the aspect should be more important than a farther one.,3.4 Location Attention,[1.0],['Such location information is helpful for an attention model because intuitively a context word closer to the aspect should be more important than a farther one.']
"In this work, we define the location of a context word as its absolute distance with the aspect in the original sentence sequence3.",3.4 Location Attention,[1.0],"['In this work, we define the location of a context word as its absolute distance with the aspect in the original sentence sequence3.']"
"On this basis, we study four strategies to encode the location information in the attention model.",3.4 Location Attention,[0],[0]
"The details are described below.
3The location of a context word could also be measured by its distance to the aspect along a syntactic path.",3.4 Location Attention,[0],[0]
"We leave this as a future work as we prefer to developing a purely data-driven approach without using external parsing results.
",3.4 Location Attention,[0],[0]
• Model 1.,3.4 Location Attention,[0],[0]
"Following Sukhbaatar et al. (2015), we calculate the memory vector mi with
mi = ei vi (4)
where means element-wise multiplication and vi ∈ Rd×1 is a location vector for word wi.",3.4 Location Attention,[0],[0]
"Every element in vi is calculated as follows,
vki = (1− li/n)− (k/d)(1− 2× li/n) (5)
where n is sentence length, k is the hop number and li is the location of wi. •",3.4 Location Attention,[0],[0]
Model 2.,3.4 Location Attention,[0],[0]
"This is a simplified version of Model 1, using the same location vector vi for wi in different hops.",3.4 Location Attention,[0],[0]
"Location vector vi is calculated as follows.
",3.4 Location Attention,[0],[0]
"vi = 1− li/n (6)
• Model 3.",3.4 Location Attention,[0],[0]
"We regard location vector vi as a parameter and compute a piece of memory with vector addition, namely
mi = ei + vi (7)
All the position vectors are stacked in a position embedding matrix, which is jointly learned with gradient descent.",3.4 Location Attention,[0],[0]
•,3.4 Location Attention,[0],[0]
Model 4.,3.4 Location Attention,[0],[0]
Location vectors are also regarded as parameters.,3.4 Location Attention,[0],[0]
"Different from Model 3, location representations are regarded as neural gates to control how many percent of word semantics is written into the memory.",3.4 Location Attention,[0],[0]
"We feed location vector vi to a sigmoid function σ, and calculatemi with element-wise multiplication:
mi = ei σ(vi) (8)",3.4 Location Attention,[1.0000000460079677],"['We feed location vector vi to a sigmoid function σ, and calculatemi with element-wise multiplication: mi = ei σ(vi) (8)']"
"It is widely accepted that computational models that are composed of multiple processing layers have the ability to learn representations of data with multiple levels of abstraction (LeCun et al., 2015).",3.5 The Need for Multiple Hops,[0],[0]
"In this work, the attention layer in one layer is essentially a weighted average compositional function, which is not powerful enough to handle the sophisticated computationality like negation, intensification and contrary in language.",3.5 The Need for Multiple Hops,[0],[0]
Multiple computational layers allow the deep memory network to learn representations of text with multiple levels of abstraction.,3.5 The Need for Multiple Hops,[0],[0]
"Each layer/hop retrieves important context words,
and transforms the representation at previous level into a representation at a higher, slightly more abstract level.",3.5 The Need for Multiple Hops,[0],[0]
"With the composition of enough such transformations, very complex functions of sentence representation towards an aspect can be learned.",3.5 The Need for Multiple Hops,[0],[0]
"We regard the output vector in last hop as the feature, and feed it to a softmax layer for aspect level sentiment classification.",3.6 Aspect Level Sentiment Classification,[0],[0]
"The model is trained in a supervised manner by minimizing the cross entropy error of sentiment classification, whose loss function is given below, where T means all training instances, C is the collection of sentiment categories, (s, a) means a sentence-aspect pair.
",3.6 Aspect Level Sentiment Classification,[0],[0]
"loss = − ∑
(s,a)∈T ∑ c∈C P gc (s, a) · log(Pc(s, a))",3.6 Aspect Level Sentiment Classification,[0],[0]
"(9)
Pc(s, a) is the probability of predicting (s, a) as category c produced by our system.",3.6 Aspect Level Sentiment Classification,[0],[0]
"P gc (s, a) is 1 or 0, indicating whether the correct answer is c. We use back propagation to calculate the gradients of all the parameters, and update them with stochastic gradient descent.",3.6 Aspect Level Sentiment Classification,[0],[0]
"We clamp the word embeddings with 300-dimensional Glove vectors (Pennington et al., 2014), which is trained from web data and the vocabulary size is 1.9M4.",3.6 Aspect Level Sentiment Classification,[0],[0]
"We randomize other parameters with uniform distribution U(−0.01, 0.01), and set the learning rate as 0.01.",3.6 Aspect Level Sentiment Classification,[1.0],"['We randomize other parameters with uniform distribution U(−0.01, 0.01), and set the learning rate as 0.01.']"
We describe experimental settings and report empirical results in this section.,4 Experiment,[0],[0]
"We conduct experiments on two datasets from SemEval 2014 (Pontiki et al., 2014), one from laptop domain and another from restaurant domain.",4.1 Experimental Setting,[0],[0]
Statistics of the datasets are given in Table 1.,4.1 Experimental Setting,[0],[0]
"It is worth noting that the original dataset contains the fourth category - conflict, which means that a sentence expresses both positive and negative opinion towards an aspect.",4.1 Experimental Setting,[0],[0]
"We remove conflict category as the number of instances is very tiny, incorporating which
4Available at: http://nlp.stanford.edu/projects/glove/.
will make the dataset extremely unbalanced.",4.1 Experimental Setting,[0],[0]
Evaluation metric is classification accuracy.,4.1 Experimental Setting,[0],[0]
"We compare with the following baseline methods on both datasets.
",4.2 Comparison to Other Methods,[0],[0]
"(1) Majority is a basic baseline method, which assigns the majority sentiment label in training set to each instance in the test set.
",4.2 Comparison to Other Methods,[0],[0]
(2) Feature-based SVM performs state-of-the-art on aspect level sentiment classification.,4.2 Comparison to Other Methods,[0],[0]
"We compare with a top system using ngram features, parse features and lexicon features (Kiritchenko et al., 2014).
",4.2 Comparison to Other Methods,[0],[0]
"(3) We compare with three LSTM models (Tang et al., 2015a)).",4.2 Comparison to Other Methods,[0],[0]
"In LSTM, a LSTM based recurrent model is applied from the start to the end of a sentence, and the last hidden vector is used as the sentence representation.",4.2 Comparison to Other Methods,[0],[0]
"TDLSTM extends LSTM by taking into account of the aspect, and uses two LSTM networks, a forward one and a backward one, towards the aspect.",4.2 Comparison to Other Methods,[0],[0]
"TDLSTM+ATT extends TDLSTM by incorporating an attention mechanism (Bahdanau et al., 2015) over the hidden vectors.",4.2 Comparison to Other Methods,[0],[0]
"We use the same Glove word vectors for fair comparison.
",4.2 Comparison to Other Methods,[0],[0]
"(4) We also implement ContextAVG, a simplistic version of our approach.",4.2 Comparison to Other Methods,[0],[0]
Context word vectors are averaged and the result is added to the aspect vector.,4.2 Comparison to Other Methods,[0],[0]
"The output is fed to a softmax function.
",4.2 Comparison to Other Methods,[0],[0]
Experimental results are given in Table 2.,4.2 Comparison to Other Methods,[0],[0]
"Our approach using only content attention is abbreviated to MemNet (k), where k is the number of hops.",4.2 Comparison to Other Methods,[0],[0]
"We can find that feature-based SVM is an extremely strong performer and substantially outperforms other baseline methods, which demonstrates the importance of a powerful feature representation for aspect level sentiment classification.",4.2 Comparison to Other Methods,[1.0],"['We can find that feature-based SVM is an extremely strong performer and substantially outperforms other baseline methods, which demonstrates the importance of a powerful feature representation for aspect level sentiment classification.']"
"Among three recurrent models, TDLSTM performs better than LSTM, which indicates that taking into account of the aspect information is helpful.",4.2 Comparison to Other Methods,[1.0],"['Among three recurrent models, TDLSTM performs better than LSTM, which indicates that taking into account of the aspect information is helpful.']"
"This is reason-
able as the sentiment polarity of a sentence towards different aspects (e.g. “food” and “service”) might be different.",4.2 Comparison to Other Methods,[0],[0]
It is somewhat disappointing that incorporating attention model over TDLSTM does not bring any improvement.,4.2 Comparison to Other Methods,[0],[0]
We consider that each hidden vector of TDLSTM encodes the semantics of word sequence until the current position.,4.2 Comparison to Other Methods,[0],[0]
"Therefore, the model of TDLSTM+ATT actually selects such mixed semantics of word sequence, which is weird and not an intuitive way to selectively focus on parts of contexts.",4.2 Comparison to Other Methods,[0],[0]
"Different from TDLSTM+ATT, the proposed memory network approach removes the recurrent calculator over word sequence and directly apply attention mechanism on context word representations.
",4.2 Comparison to Other Methods,[0],[0]
"We can also find that the performance of ContextAVG is very poor, which means that assigning the same weight/importance to all the context words is not an effective way.",4.2 Comparison to Other Methods,[0],[0]
"Among all our models from single hop to nine hops, we can observe that using more computational layers could generally lead to better performance, especially when the number of hops is less than six.",4.2 Comparison to Other Methods,[1.0],"['Among all our models from single hop to nine hops, we can observe that using more computational layers could generally lead to better performance, especially when the number of hops is less than six.']"
"The best performances are achieved when the model contains seven and nine hops, respectively.",4.2 Comparison to Other Methods,[0],[0]
"On both datasets, the proposed approach could obtain comparable accuracy compared to the state-of-art feature-based SVM system.",4.2 Comparison to Other Methods,[0],[0]
We study the runtime of recurrent neural models and the proposed deep memory network approach with different hops.,4.3 Runtime Analysis,[0],[0]
"We implement all these approaches based on the same neural network infrastructure, use the same 300-dimensional Glove word vectors, and run them on the same CPU server.
",4.3 Runtime Analysis,[0],[0]
The training time of each iteration on the restaurant dataset is given in Table 3.,4.3 Runtime Analysis,[1.0],['The training time of each iteration on the restaurant dataset is given in Table 3.']
"We can find that LSTM based recurrent models are indeed computationally expensive, which is caused by the complex operations in each LSTM unit along the word sequence.",4.3 Runtime Analysis,[0],[0]
"Instead, the memory network approach is simpler and evidently faster because it does not need recurrent calculators of sequence length.",4.3 Runtime Analysis,[0],[0]
Our approach with nine hops is almost 15 times faster than the basic LSTM model.,4.3 Runtime Analysis,[0],[0]
"As described in Section 3.4, we explore four strategies to integrate location information into the attention model.",4.4 Effects of Location Attention,[0],[0]
We incorporate each of them separately into the basic content-based attention model.,4.4 Effects of Location Attention,[0],[0]
It is helpful to restate that the difference between four location-based attention models lies in the usage of location vectors for context words.,4.4 Effects of Location Attention,[0],[0]
"In Model 1 and Model 2, the values of location vectors are fixed and calculated in a heuristic way.",4.4 Effects of Location Attention,[0],[0]
"In Model 3 and Model 4, location vectors are also regarded as the parameters and jointly learned along with other parameters in the deep memory network.
",4.4 Effects of Location Attention,[0],[0]
Figure 2 shows the classification accuracy of each attention model on the restaurant dataset.,4.4 Effects of Location Attention,[0],[0]
We can find that using multiple computational layers could consistently improve the classification accuracy in all these models.,4.4 Effects of Location Attention,[0],[0]
All these models perform comparably when the number of hops is larger than five.,4.4 Effects of Location Attention,[0],[0]
"Among these four location-based models, we prefer Model 2 as it is intuitive and has less computation cost without loss of accuracy.",4.4 Effects of Location Attention,[0],[0]
"We also find
that Model 4 is very sensitive to the choice of neural gate.",4.4 Effects of Location Attention,[1.0000000282252441],['We also find that Model 4 is very sensitive to the choice of neural gate.']
Its classification accuracy decreases by almost 5 percentage when the sigmoid operation over location vector is removed.,4.4 Effects of Location Attention,[0],[0]
We visualize the attention weight of each context word to get a better understanding of the deep memory network approach.,4.5 Visualize Attention Models,[1.0],['We visualize the attention weight of each context word to get a better understanding of the deep memory network approach.']
"The results of context-based model and location-based model (Model 2) are given in Table 4 and Table 5, respectively.
From Table 4(a), we can find that in the first hop the context words “great”, “but” and “dreadful” contribute equally to the aspect “service”.",4.5 Visualize Attention Models,[0],[0]
"While after the second hop, the weight of “dreadful” increases and finally the model correctly predict the polarity towards “service” as negative.",4.5 Visualize Attention Models,[1.0],"['While after the second hop, the weight of “dreadful” increases and finally the model correctly predict the polarity towards “service” as negative.']"
This case shows the effects of multiple hops.,4.5 Visualize Attention Models,[0],[0]
"However, in Table 4(b), the content-based model also gives a larger weight to “dreadful” when the target we focus on is “food”.",4.5 Visualize Attention Models,[0],[0]
"As a result, the model incorrectly predicts the polarity towards “food” as negative.",4.5 Visualize Attention Models,[1.0],"['As a result, the model incorrectly predicts the polarity towards “food” as negative.']"
This phenomenon might be caused by the neglect of location information.,4.5 Visualize Attention Models,[0],[0]
"From Table 5(b), we can find that the weight
of “great” is increased when the location of context word is considered.",4.5 Visualize Attention Models,[0],[0]
"Accordingly, Model 2 predicts the correct sentiment label towards “food”.",4.5 Visualize Attention Models,[0],[0]
We believe that location-enhanced model captures both content and location information.,4.5 Visualize Attention Models,[1.0],['We believe that location-enhanced model captures both content and location information.']
"For instance, in Table 5(a) the closest context words of the aspect “service” are “the” and “was”, while “dreadful” has the largest weight.",4.5 Visualize Attention Models,[0],[0]
"We carry out an error analysis of our location enhanced model (Model 2) on the restaurant dataset, and find that most of the errors could be summarized as follows.",4.6 Error Analysis,[1.0],"['We carry out an error analysis of our location enhanced model (Model 2) on the restaurant dataset, and find that most of the errors could be summarized as follows.']"
The first factor is noncompositional sentiment expression.,4.6 Error Analysis,[0],[0]
This model regards single context word as the basic computational unit and cannot handle this situation.,4.6 Error Analysis,[0],[0]
"An example is “dessert was also to die for!”, where the aspect is underlined.",4.6 Error Analysis,[0],[0]
"The sentiment expression is “die for”, whose meaning could not be composed from its constituents “die” and “for”.",4.6 Error Analysis,[0],[0]
"The second factor is complex aspect expression consisting of many words, such as “ask for the round corner table next to the large window.”",4.6 Error Analysis,[0],[0]
"This model represents an aspect expression by averaging its constituting word vectors, which could not well handle this situation.",4.6 Error Analysis,[0],[0]
"The third factor is sentimental relation between context words such as negation, comparison and condition.",4.6 Error Analysis,[0],[0]
"An example is “but dinner here is never disappointing, even if the prices are a bit over the top”.",4.6 Error Analysis,[0],[0]
We believe that this is caused by the weakness of weighted average compositional function in each hop.,4.6 Error Analysis,[0],[0]
There are also cases when comparative opinions are expressed such as “i ’ve had better japanese food at a mall food court”.,4.6 Error Analysis,[0],[0]
This work is connected to three research areas in natural language processing.,5 Related Work,[0],[0]
We briefly describe related studies in each area.,5 Related Work,[0],[0]
"Aspect level sentiment classification is a finegrained classification task in sentiment analysis, which aims at identifying the sentiment polarity of a sentence expressed towards an aspect (Pontiki et al., 2014).",5.1 Aspect Level Sentiment Classification,[0],[0]
"Most existing works use machine learning algorithms, and build sentiment classifier from
sentences with manually annotated polarity labels.",5.1 Aspect Level Sentiment Classification,[0],[0]
One of the most successful approaches in literature is feature based SVM.,5.1 Aspect Level Sentiment Classification,[0],[0]
"Experts could design effective feature templates and make use of external resources like parser and sentiment lexicons (Kiritchenko et al., 2014; Wagner et al., 2014).",5.1 Aspect Level Sentiment Classification,[1.0],"['Experts could design effective feature templates and make use of external resources like parser and sentiment lexicons (Kiritchenko et al., 2014; Wagner et al., 2014).']"
"In recent years, neural network approaches (Dong et al., 2014; Lakkaraju et al., 2014; Nguyen and Shirai, 2015; Tang et al., 2015a) are of growing attention for their capacity to learn powerful text representation from data.",5.1 Aspect Level Sentiment Classification,[0],[0]
"However, these neural models (e.g. LSTM) are computationally expensive, and could not explicitly reveal the importance of context evidences with regard to an aspect.",5.1 Aspect Level Sentiment Classification,[0],[0]
"Instead, we develop simple and fast approach that explicitly encodes the context importance towards a given aspect.",5.1 Aspect Level Sentiment Classification,[0],[0]
"It is worth noting that the task we focus on differs from finegrained opinion extraction, which assigns each word a tag (e.g. B,I,O) to indicate whether it is an aspect/sentiment word (Choi and Cardie, 2010; Irsoy and Cardie, 2014; Liu et al., 2015).",5.1 Aspect Level Sentiment Classification,[0],[0]
The aspect word in this work is given as a part of the input.,5.1 Aspect Level Sentiment Classification,[0],[0]
"In NLP community, compositionality means that the meaning of a composed expression (e.g. a phrase/sentence/document) comes from the meanings of its constituents (Frege, 1892).",5.2 Compositionality in Vector Space,[0],[0]
Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector.,5.2 Compositionality in Vector Space,[0],[0]
Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases.,5.2 Compositionality in Vector Space,[0],[0]
"To compute sentence representation, researchers develop denoising autoencoder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Schütze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015).",5.2 Compositionality in Vector Space,[0],[0]
"Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014; Bhatia et al., 2015; Li et al., 2015a; Tang et al., 2015b; Yang et al., 2016).",5.2 Compositionality in Vector Space,[0],[0]
"Recently, there is a resurgence in computational models with attention mechanism and explicit mem-
ory to learn representations of texts (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Bahdanau et al., 2015).",5.3 Attention and Memory Networks,[0],[0]
"In this line of research, memory is encoded as a continuous representation and operations on memory (e.g. reading and writing) are typically implemented with neural networks.",5.3 Attention and Memory Networks,[0],[0]
"Attention mechanism could be viewed as a compositional function, where lower level representations are regarded as the memory, and the function is to choose “where to look” by assigning a weight/importance to each lower position when computing an upper level representation.",5.3 Attention and Memory Networks,[0],[0]
"Such attention based approaches have achieved promising performances on a variety of NLP tasks (Luong et al., 2015; Kumar et al., 2015; Rush et al., 2015).",5.3 Attention and Memory Networks,[0],[0]
We develop deep memory networks that capture importances of context words for aspect level sentiment classification.,6 Conclusion,[0],[0]
"Compared with recurrent neural models like LSTM, this approach is simpler and faster.",6 Conclusion,[0],[0]
"Empirical results on two datasets verify that the proposed approach performs comparable to state-of-the-art feature based SVM system, and substantively better than LSTM architectures.",6 Conclusion,[0],[0]
We implement different attention strategies and show that leveraging both content and location information could learn better context weight and text representation.,6 Conclusion,[0],[0]
We also demonstrate that using multiple computational layers in memory network could obtain improved performance.,6 Conclusion,[0],[0]
Our potential future plans are incorporating sentence structure like parsing results into the deep memory network.,6 Conclusion,[0],[0]
We would especially want to thank Xiaodan Zhu for running their system on our setup.,Acknowledgments,[0],[0]
We greatly thank Yaming Sun for tremendously helpful discussions.,Acknowledgments,[0],[0]
We also thank the anonymous reviewers for their valuable comments.,Acknowledgments,[0],[0]
"This work was supported by the National High Technology Development 863 Program of China (No. 2015AA015407), National Natural Science Foundation of China (No. 61632011 and No.61273321).",Acknowledgments,[0],[0]
We introduce a deep memory network for aspect level sentiment classification.,abstractText,[0],[0]
"Unlike feature-based SVM and sequential neural models such as LSTM, this approach explicitly captures the importance of each context word when inferring the sentiment polarity of an aspect.",abstractText,[0],[0]
"Such importance degree and text representation are calculated with multiple computational layers, each of which is a neural attention model over an external memory.",abstractText,[0],[0]
"Experiments on laptop and restaurant datasets demonstrate that our approach performs comparable to state-of-art feature based SVM system, and substantially better than LSTM and attention-based LSTM architectures.",abstractText,[0],[0]
On both datasets we show that multiple computational layers could improve the performance.,abstractText,[0],[0]
"Moreover, our approach is also fast.",abstractText,[0],[0]
The deep memory network with 9 layers is 15 times faster than LSTM with a CPU implementation.,abstractText,[0],[0]
Aspect Level Sentiment Classification with Deep Memory Network,title,[0],[0]
