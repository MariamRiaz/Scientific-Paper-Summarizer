{"sections": [{"heading": "1. Introduction", "text": "Neural networks have achieved state-of-the-art accuracy on many machine learning tasks. AlexNet (Krizhevsky et al., 2012) had a deep impact a few years ago in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) and triggered intensive research efforts on deep neural networks. Recently, ResNet (He et al., 2016) has outperformed humans in recognition tasks.\nThese networks have very high computational complexity. For instance, AlexNet has 60 million parameters and 650,000 neurons (Krizhevsky et al., 2012). Its convolutional layers alone require 666 million multiply-accumulates (MACs) per 227 \u00d7 227 image (13k MACs/pixel) and 2.3 million weights (Chen et al., 2016). Deepface\u2019s network involves more than 120 million parameters (Taigman et al., 2014). ResNet is a 152-layer\nThe authors are with the University of Illinois at UrbanaChampaign, 1308 W Main St., Urabna, IL 61801 USA. Correspondence to: Charbel Sakr <sakr2@illinois.edu>, Yongjune Kim <yongjune@illinois.edu>, Naresh Shanbhag <shanbhag@illinois.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ndeep residual network. This high complexity of deep neural networks prevents its deployment on energy and resource-constrained platforms such as mobile devices and autonomous platforms."}, {"heading": "1.1. Related Work", "text": "One of the most effective approaches for reducing resource utilization is to implement fixed-point neural networks. As mentioned in (Lin et al., 2016a), there are two approaches for designing fixed-point neural networks: (1) directly train a fixed-point neural network, and (2) quantize a pre-trained floating-point neural network to obtain a fixed-point network.\nAs an example of fixed-point training, Gupta et al. (2015) showed that 16-bit fixed-point representation incurs little accuracy degradation by using stochastic rounding. A more aggressive approach is to design binary networks such as Kim & Smaragdis (2016) which used bitwise operations to replace the arithmetic operations and Rastegari et al. (2016) which explored optimal binarization schemes. BinaryConnect (Courbariaux et al., 2015) trained networks using binary weights while BinaryNet (Hubara et al., 2016b) trained networks with binary weights and activations.\nAlthough these fixed-point training approaches make it possible to design fixed-point neural networks achieving excellent accuracy, training based on fixed-point arithmetic is generally harder than floating-point training since the optimization is done in a discrete space.\nHence, in this paper, we focus on the second approach that quantizes a pre-trained floating-pointing network to a fixed-point network. This approach leverages the extensive work in training state-of-the-art floating-point neural networks such as dropout (Srivastava et al., 2014), maxout (Goodfellow et al., 2013), network-in-network (Lin et al., 2013), and residual learning (He et al., 2016) to name a few. In this approach, proper precision needs to be determined after training to reduce complexity while minimizing the accuracy loss. In (Hwang & Sung, 2014), exhaustive search is performed to determine a suitable precision allocation. Recently, Lin et al. (2016a) offered an analytical solution for non-uniform bit precision based on the signalto-quantization-noise ratio (SQNR). However, the use of non-uniform quantization step sizes at each layer is diffi-\ncult to implement as it requires multiple variable precision arithmetic units.\nIn addition to fixed-point implementation, many approaches have been proposed to lower the complexity of deep neural networks in terms of the number of arithmetic operations. Han et al. (2015) employs a three-step training method to identify important connections, prune the unimportant ones, and retrain on the pruned network. Zhang et al. (2015) replaces the original convolutional layers by smaller sequential layers to reduce computations. These approaches are complementary to our technique of quantizing a pre-trained floating-point neural network into a fixedpoint one.\nIn this paper, we obtain analytical bounds on the accuracy of fixed-point networks that are obtained by quantizing a conventionally trained floating-point network. Furthermore, by defining meaningful measures of a fixed-point network\u2019s hardware complexity viz. computational and representational costs, we develop a principled approach to precision assignment using these bounds in order to minimize these complexity measures."}, {"heading": "1.2. Contributions", "text": "Our contributions are both theoretical and practical. We summarize our main contributions:\n\u2022 We derive theoretical bounds on the misclassification rate in presence of limited precision and thus determine analytically how accuracy and precision tradeoff with each other. \u2022 Employing the theoretical bounds and the backpropagation algorithm, we show that proper precision assignments can be readily determined while maintaining accuracy close to floating-point networks. \u2022 We analytically determine which of weights or activations need more precision, and we show that typically the precision requirements of weights are greater than those of activations for fully-connected networks and are similar to each other for networks with shared weights such as convolutional neural networks. \u2022 We introduce computational and representational costs as meaningful metrics to evaluate the complexity of neural networks under fixed precision assignment. \u2022 We validate our findings on the MNIST and CIFAR10 datasets demonstrating the ease with which fixedpoint networks with complexity smaller than stateof-the-art binary networks can be derived from pretrained floating-point networks with minimal loss in accuracy.\nIt is worth mentioning that our proposed method is general and can be applied to every class of neural networks such as multilayer perceptrons and convolutional neural networks."}, {"heading": "2. Fixed-Point Neural Networks", "text": ""}, {"heading": "2.1. Accuracy of Fixed-Point and Floating-Point Networks", "text": "For a given floating-point neural network and its fixedpoint counterpart we define: 1) the floating-point error probability pe,fl = Pr{Y\u0302fl 6= Y } where Y\u0302fl is the output of the floating-point network and Y is the true label; 2) the fixed-point error probability pe,fx = Pr{Y\u0302fx 6= Y } where Y\u0302fx is the output of the fixed-point network; 3) the mismatch probability between fixed-point and floating-point pm = Pr{Y\u0302fx 6= Y\u0302fl}. Observe that:\npe,fx \u2264 pe,fl + pm (1)\nThe right-hand-side represents the worst case of having no overlap between misclassified samples and samples whose predicted labels are in error due to quantization. We provide a formal proof of (1) in the supplementary section. Note that pe,fx is a quantity of interest as it characterizes the accuracy of the fixed-point system. We employ pm as a proxy to pe,fx because it brings in the effects of quantization into the picture as opposed to pe,fl which solely depends on the algorithm. This observation was made in (Sakr et al., 2017) and allowed for an analytical characterization of linear classifiers as a function of precision."}, {"heading": "2.2. Fixed-Point Quantization", "text": "The study of fixed-point systems and algorithms is well established in the context of signal processing and communication systems (Shanbhag, 2016). A popular example is the least mean square (LMS) algorithm for which bounds on precision requirements for input, weights, and updates have been derived (Goel & Shanbhag, 1998). In such analyses, it is standard practice (Caraiscos & Liu, 1984) to assume all signed quantities lie in [\u22121, 1] and all unsigned quantities lie in [0, 2]. Of course, activations and weights can be designed to satisfy this assumption during training. A Bbit fixed-point number afx would be related to its floatingpoint value a as follows:\nafx = a + qa (2)\nwhere qa is the quantization noise which is modeled as an independent uniform random variable distributed over[ \u2212\u22062 , \u2206 2 ] , where \u2206 = 2\u2212(B\u22121) is the quantization step (Caraiscos & Liu, 1984)."}, {"heading": "2.3. Complexity in Fixed-Point", "text": "We argue that the complexity of a fixed-point system has two aspects: computational and representational costs. In what follows, we consider activations and weights to be quantized to BA and BW bits, respectively.\nThe computational cost is a measure of the computational resources utilized for generating a single decision, and is defined as the number of 1 bit full adders (FAs). A full adder is a canonical building block of arithmetic units. We assume arithmetic operations are executed using the commonly used ripple carry adder (Knauer, 1989) and BaughWooley multiplier (Baugh & Wooley, 1973) architectures designed using FAs. Consequently, the number of FAs used to compute a D-dimensional dot product of activations and weights is (Lin et al., 2016b):\nDBABW + (D \u2212 1)(BA + BW + dlog2(D)e \u2212 1) (3)\nHence, an important aspect of the computational cost of a dot product is that it is an increasing function of the product of activation precision (BA), weight precision (BW ), and dimension (D).\nWe define the representational cost as the total number of bits needed to represent all network parameters, i.e., both activations and weights. This cost is a measure of the storage complexity and communications costs associated with data movement. The total representational cost of a fixedpoint network is:\n|A|BA + |W|BW (4)\nbits, where A and W are the index sets of all activations and weights in the network, respectively. Observe that the representational cost is linear in activation and weight precisions as compared to the computational cost.\nEquations (3) - (4) illustrate that, though computational and representational costs are not independent, they are different. Together, they describe the implementation costs associated with a network. We shall employ both when evaluating the complexity of fixed-point networks."}, {"heading": "2.4. Setup", "text": "Here, we establish notation. Let us consider neural networks deployed on a M -class classification task. For a given input, the network would typically have M numerical outputs {zi}Mi=1 and the decision would be y\u0302 = arg max\ni=1,...,M zi. Each numerical output is a function of\nweights and activations in the network:\nzi = f ({ah}h\u2208A, {wh}h\u2208W) (5)\nfor i = 1, . . . ,M , where ah denotes the activation indexed by h and wh denotes the weight indexed by h. When activations and weights are quantized to BA and BW bits, respectively, the output zi is corrupted by quantization noise qzi so that:\nzi + qzi = f ({ah + qah}h\u2208A, {wh + qwh}h\u2208W) (6)\nwhere qah and qwh are the quantization noise terms of the activation ah and weight wh, respectively. Here, {qah}h\u2208A are independent uniformly distributed random variables on [ \u2212\u2206A2 , \u2206A 2 ] and {qwh}h\u2208W are independent uniformly\ndistributed random variables on [ \u2212\u2206W2 , \u2206W 2 ] , with \u2206A = 2\u2212(BA\u22121) and \u2206W = 2\u2212(BW\u22121).\nIn quantization noise analysis, it is standard to ignore crossproducts of quantization noise terms as their contribution is negligible. Therefore, using Taylor\u2019s theorem, we express the total quantization noise at the output of the fixed-point network as:\nqzi = \u2211 h\u2208A qah \u2202zi \u2202ah + \u2211 h\u2208W qwh \u2202zi \u2202wh . (7)\nNote that the derivatives in (7) are obtained as part of the back-propagation algorithm. Thus, using our results, it is possible to estimate the precision requirements of deep neural networks during training itself. As will be shown later, this requires one additional back-propagation iteration to be executed after the weights have converged."}, {"heading": "3. Bounds on Mismatch Probability", "text": ""}, {"heading": "3.1. Second Order Bound", "text": "We present our first result. It is an analytical upper bound on the mismatch probability pm between a fixed-point neural network and its floating-point counterpart.\nTheorem 1. Given BA and BW , the mismatch probability pm between a fixed-point network and its floating-point counterpart is upper bounded as follows:\npm \u2264 \u22062A 24 E  M\u2211 i=1\ni 6=Y\u0302fl\n\u2211 h\u2208A \u2223\u2223\u2223\u2223\u2202(Zi\u2212ZY\u0302fl )\u2202Ah \u2223\u2223\u2223\u22232\n|Zi \u2212 ZY\u0302fl | 2\n\n+ \u22062W 24 E  M\u2211 i=1\ni 6=Y\u0302fl\n\u2211 h\u2208W \u2223\u2223\u2223\u2223\u2202(Zi\u2212ZY\u0302fl )\u2202wh \u2223\u2223\u2223\u22232\n|Zi \u2212 ZY\u0302fl | 2  (8) where expectations are taken over a random input and {Ah}h\u2208A, {Zi}Mi=1, and Y\u0302fl are thus random variables.\nProof. The detailed proof can be found in the supplementary section. Here, we provide the main idea and the intuition behind the proof. The heart of the proof lies in evaluating Pr ( zi + qzi > zj + qzj ) for any pair of outputs zi and zj where zj > zi. Equivalently, we need to evaluate\nPr ( qzi \u2212 qzj > zj \u2212 zi ) . But from (7), we have:\nqzi \u2212 qzj = \u2211 h\u2208A qah \u2202(zi \u2212 zj) \u2202ah + \u2211 h\u2208W qwh \u2202(zi \u2212 zj) \u2202wh .\n(9)\nIn (9), we have a linear combination of quantization noise terms, qzi \u2212 qzj is a zero mean random variable having a symmetric distribution. This means that Pr ( qzi \u2212 qzj > zj \u2212 zi ) = 1 2 Pr ( |qzi \u2212 qzj | > |zj \u2212 zi| ) , which allows us to use Chebyshev\u2019s inequality. Indeed, from (9), the variance of qzi \u2212 qzj is given by:\n\u22062A 12 \u2211 h\u2208A \u2223\u2223\u2223\u2223\u2202(zi \u2212 zj)\u2202ah \u2223\u2223\u2223\u22232 + \u22062W12 \u2211 h\u2208W \u2223\u2223\u2223\u2223\u2202(zi \u2212 zj)\u2202wh \u2223\u2223\u2223\u22232 ,\nso that Pr ( zi + qzi > zj + qzj ) \u2264 \u22062A \u2211 h\u2208A \u2223\u2223\u2223\u2202(zi\u2212zj)\u2202ah \u2223\u2223\u22232 + \u22062W \u2211h\u2208W \u2223\u2223\u2223\u2202(zi\u2212zj)\u2202wh \u2223\u2223\u22232 24 |zi \u2212 zj |2 .\n(10)\nAs explained in the supplementary section, it is possible to obtain to (8) from (10) using standard probabilistic arguments.\nBefore proceeding, we point out that the two expectations in (8) are taken over a random input but the weights {wh}h\u2208W are frozen after training and are hence deterministic.\nSeveral observations are to be made. First notice that the mismatch probability pm increases with \u22062A and \u2206 2 W . This is to be expected as smaller precision leads to more mismatch. Theorem 1 says a little bit more: the mismatch probability decreases exponentially with precision, because \u2206A = 2 \u2212(BA\u22121) and \u2206W = 2\u2212(BW\u22121).\nNote that the quantities in the expectations in (8) can be obtained as part of a standard back-propagation procedure. Indeed, once the weights are frozen, it is enough to perform one forward pass on an estimation set (which should have statistically significant cardinality), record the numerical outputs, perform one backward pass and probe all relevant derivatives. Thus, (8) can be readily computed.\nAnother practical aspect of Theorem 1 is that this operation needs to be done only once as these quantities do not depend on precision. Once they are determined, for any given precision assignment, we simply evaluate (8) and combine it with (1) to obtain an estimate (upper bound) on the accuracy of the fixed-point instance. This way the precision necessary to achieve a specific mismatch probability is obtained from a trained floating-point network. This\nclearly highlights the gains in practicality of our analytical approach over a trial-and-error based search.\nFinally, (8) reveals a very interesting aspect of the trade-off between activation precision BA and weight precision BW . We rewrite (8) as:\npm \u2264 \u22062AEA + \u22062WEW (11)\nwhere\nEA = E  M\u2211 i=1\ni 6=Y\u0302fl\n\u2211 h\u2208A \u2223\u2223\u2223\u2223\u2202(Zi\u2212ZY\u0302fl )\u2202Ah \u2223\u2223\u2223\u22232\n24|Zi \u2212 ZY\u0302fl | 2  and\nEW = E  M\u2211 i=1\ni6=Y\u0302fl\n\u2211 h\u2208W \u2223\u2223\u2223\u2223\u2202(Zi\u2212ZY\u0302fl )\u2202wh \u2223\u2223\u2223\u22232\n24|Zi \u2212 ZY\u0302fl | 2  . The first term in (11) characterizes the impact of quantizing activations on the overall accuracy while the second characterizes that of weight quantization. It might be the case that one of the two terms dominates the sum depending on the values of EA and EW . This means that either the activations or the weights are assigned more precision than necessary. An intuitive first step to efficiently get a smaller upper bound is to make the two terms of comparable order. That can be made by setting \u22062AEA = \u2206 2 WEW which is equivalent to\nBA \u2212BW = round ( log2 \u221a EA EW ) (12)\nwhere round() denotes the rounding operation. This is an effective way of taking care of one of the two degrees of freedom introduced by (8).\nA natural question to ask would be which of EA and EW is typically larger. That is to say, to whom, activations or weights, should one assign more precision. In deep neural networks, there are more weights than activations, a trend particularly observed in deep networks with most layers fully connected. This trend, though not as pronounced, is also observed in networks with shared weights, such as convolutional neural networks. However, there exist a few counterexamples such as the networks in (Hubara et al., 2016b) and (Hubara et al., 2016a). It is thus reasonable to expect EW \u2265 EA, and consequently the precision requirements of weights will, in general, be more than those of activations.\nOne way to interpret (11) is to consider minimizing the upper bound in (8) subject to BA+BW = c for some constant\nc. Indeed, it can be shown that (12) would be a necessary condition of the corresponding solution. This is an application of the arithmetic-geometric mean inequality. Effectively, (11) is of particular interest when considering computational cost which increases as a function of the product of both precisions (see Section 2.3)."}, {"heading": "3.2. Tighter Bound", "text": "We present a tighter upper bound on pm based on the Chernoff bound.\nTheorem 2. Given BA and BW , the mismatch probability pm between a fixed-point network and its floating-point counterpart is upper bounded as follows:\npm \u2264 E  M\u2211 i=1\ni6=Y\u0302fl\ne\u2212S (i,Y\u0302fl) P (i,Y\u0302fl) 1 P (i,Y\u0302fl) 2  (13) where, for i 6= j,\nS(i,j) = 3(Zi \u2212 Zj)2\u2211\nh\u2208A\n( D\n(i,j) Ah\n)2 + \u2211\nh\u2208W\n( D\n(i,j) wh )2 , D\n(i,j) Ah = \u2206A 2 \u2202(Zi \u2212 Zj) \u2202Ah , D(i,j)wh = \u2206W 2 \u2202(Zi \u2212 Zj) \u2202wh ,\nP (i,j) 1 = \u220f h\u2208A\nsinh ( T (i,j)D\n(i,j) Ah ) T (i,j)D\n(i,j) Ah\n,\nP (i,j) 2 = \u220f h\u2208W\nsinh ( T (i,j)D\n(i,j) wh ) T (i,j)D\n(i,j) wh\n,\nand\nT (i,j) = S(i,j)\nZj \u2212 Zi .\nProof. Again, we leave the technical details for the supplementary section. Here we also provide the main idea and intuition. As in Theorem 1, we shall focus on evaluating Pr ( zi + qzi > zj + qzj ) = Pr ( qzi \u2212 qzj > zj \u2212 zi ) for any pair of outputs zi and zj where zj > zi. The key difference here is that we will use the Chernoff bound in order to account for the complete quantization noise statistics. Indeed, letting v = zj \u2212 zi, we have:\nPr ( qzi \u2212 qzj > v ) \u2264 e\u2212tvE [ et(qzi\u2212qzj ) ] for any t > 0. We show that:\nE [ et(qzi\u2212qzj ) ] = \u220f h\u2208A sinh (tda,h) tda,h \u220f h\u2208W sinh (tdw,h) tdw,h\nwhere da,h = \u2206A2 \u2202(zi\u2212zj) \u2202ah and dw,h = \u2206W2 \u2202(zi\u2212zj) \u2202wh\n. This yields:\nPr ( qzi \u2212 qzj > v ) \u2264 e\u2212tv\n\u220f h\u2208A sinh (tda,h) tda,h \u220f h\u2208W sinh (tdw,h) tdw,h . (14)\nWe show that the right-hand-side is minimized over positive values of t when:\nt = 3v\u2211\nh\u2208A (da,h) 2 + \u2211 h\u2208W (dw,h) 2 .\nSubstituting this value of t into (14) and using standard probabilistic arguments, we obtain (13).\nThe first observation to be made is that Theorem 2 indicates that, on average, pm is upper bounded by an exponentially decaying function of the quantity S(i,Y\u0302fl) for all i 6= Y\u0302fl up to a correction factor P (i,Y\u0302fl)1 P (i,Y\u0302fl) 2 . This correction factor is a product of terms typically centered around 1 (each term is of the form sinh(x)x \u2248 1 for small x). On the other hand, S(i,Y\u0302fl), by definition, is the ratio of the excess confidence the floating-point network has in the label Y\u0302fl over the total quantization noise variance reflected at the output, i.e., S(i,Y\u0302fl) is the SQNR. Hence, Theorem 2 states that the tolerance of a neural network to quantization is, on average, exponentially decaying with the SQNR at its output. In terms of precision, Theorem 2 states that pm is bounded by a double exponentially decaying function of precision (that is an exponential function of an exponential function). Note how this bound is tighter than that of Theorem 1.\nThis double exponential relationship between accuracy and precision is not too surprising when one considers the problem of binary hypothesis testing under additive Gaussian noise (Blahut, 2010) scenario. In this scenario, it is wellknown that the probability of error is an exponentially decaying function of the signal-to-noise ratio (SNR) in the high-SNR regime. Theorem 2 points out a similar relationship between accuracy and precision but it does so using rudimentary probability principles without relying on highSNR approximations.\nWhile Theorem 2 is much tighter than Theorem 1 theoretically, it is not as convenient to use. In order to use Theorem 2, one has to perform a forward-backward pass and select relevant quantities and apply (13) for each choice of BA and BW . However, a lot of information, e.g. the derivatives, can be reused at each run, and so the runs may be lumped into one forward-backward pass. In a sense, the complexity of computing the bound in Theorem 2 lies between the evaluation of (11) and the complicated conventional trial-and-error based search.\nWe now illustrate the applications of these bounds."}, {"heading": "4. Simulation Results", "text": "We conduct numerical simulations to illustrate both the validity and usefulness of the analysis developed in the previous section. We show how it is possible to reduce precision in an aggressive yet principled manner. We present results on two popular datasets: MNIST and CIFAR-10. The metrics we address are threefold:\n\u2022 Accuracy measured in terms of test error. \u2022 Computational cost measured in #FAs (see Section\n2.3, (3) was used to compute #FAs per MAC). \u2022 Representational cost measured in bits (see Section\n2.3, (4) was used).\nWe compare our results to similar works conducting similar experiments: 1) the work on fixed-point training with stochastic quantization (SQ) (Gupta et al., 2015) and 2) BinaryNet (BN) (Hubara et al., 2016b)."}, {"heading": "4.1. DNN on MNIST", "text": "First, we conduct simulations on the MNIST dataset for handwritten character recognition (LeCun et al., 1998). The dataset consists of 60K training samples and 10K test samples. Each sample consists of an image and a label. Images are of size 28 \u00d7 28 pixels representing a handwritten digit between 0 and 9. Labels take the value of the corresponding digit.\nIn this first experiment, we chose an architecture of 784 \u2212 512 \u2212 512 \u2212 512 \u2212 10, i.e., 3 hidden layers, each of 512 units. We first trained the network in floating-point using the back-propagation algorithm. We used a batch size of 200 and a learning rate of 0.1 with a decay rate of 0.978 per epoch. We restore the learning rate every 100 epochs, the decay rate makes the learning rate vary between 0.1 and 0.01. We train the first 300 epochs using 15% dropout, the second 300 epochs using 20% dropout, and the third\n300 epochs using 25% dropout (900 epochs overall). It appears from the original dropout work (Srivastava et al., 2014) that the typical 50% dropout fraction works best for very wide multi-layer perceptrons (MLPs) (4096 to 8912 hidden units). For this reason, we chose to experiment with smaller dropout fractions.\nThe only pre-processing done is to scale the inputs between \u22121 and 1. We used ReLU activations with the subtle addition of a right rectifier for values larger than 2 (as discussed in Section 2). The resulting activation is also called a hard sigmoid. We also clipped the weights to lie in [\u22121, 1] at each iteration. The resulting test error we obtained in floating-point is 1.36%.\nFigure 1 illustrates the validity of our analysis. Indeed, both bounds (based on Theorems 1 & 2) successfully upper bound the test error obtained through fixed-point simulations. Figure 1 (b) demonstrates the utility of (12). Indeed, setting BW = BA allows us to reduce the precision to about 6 or 7 bits before the accuracy start degrading. In addition, under these conditions we found EA = 41 and EW = 3803 so that log2 \u221a EW EA \u2248 3.2. Thus, setting BW = BA + 3 as dictated by (12) allows for more aggressive precision reduction. Activation precision BA can now be reduced to about 3 or 4 bits before the accuracy degrades. To compute the bounds, we used an estimation set of 1000 random samples from the dataset.\nWe compare our results with SQ which used a 784\u22121000\u2212 1000\u221210 architecture on 16-bit fixed-point activations and weights. A stochastic rounding scheme was used to compensate for quantization. We also compare our results with BN with a 784\u2212 2048\u2212 2048\u2212 2048\u2212 10 architecture on binary quantities. A stochastic rounding scheme was also used during training.\nTable 1 shows some comparisons with related works in terms of accuracy, computational cost, and representational\ncost. For comparison, we selected four notable design options from Figures 1 (a,b):\nA. Smallest (BA, BW ) such that BW = BA and pm \u2264 1% as bounded by Theorem 1. In this case (BA, BW ) = (8, 8). B. Smallest (BA, BW ) such that BW = BA and pm \u2264 1% as bounded by Theorem 2. In this case (BA, BW ) = (6, 6). C. Smallest (BA, BW ) such that BW = BA + 3 as dictated by (12) and pm \u2264 1% as bounded by Theorem 1. In this case (BA, BW ) = (6, 9). D. Smallest (BA, BW ) such that BW = BA + 3 as dictated by (12) and pm \u2264 1% as bounded by Theorem 2. In this case (BA, BW ) = (4, 7).\nAs can be seen in Table 1, the accuracy is similar across all design options including the results reported by SQ and BN. Interestingly, for all four design options, our network has a smaller computational cost than BN. In addition, SQ\u2019s computational cost is about 4.6\u00d7 that of BN (533M/117M). The greatest reduction in computational cost is obtained for a precision assignment of (4, 7) corresponding to a 2.6\u00d7 and 11.9\u00d7 reduction compared to BN (117M/44.7M) and SQ (533M/44.7M), respectively. The corresponding test error rate is of 1.43%. Similar trends are observed for representational costs. Again, our four designs have smaller representational cost than even BN. BN itself has 2.8\u00d7 smaller representational cost than SQ (28M/10M). Note that a precision assignment of (6, 6) yields 1.8\u00d7 and 5.0\u00d7 smaller representational costs than BN (10M/5.63M) and SQ (28M/5.63M), respectively. The corresponding test error rate is 1.54%.\nThe fact that we are able to achieve lesser computational and representational costs than BN while maintaining similar accuracy highlights two important points. First, the width of a network severely impacts its complexity. We made our network four times as narrow as BN\u2019s and still managed to use eight times as many bits per parameter without exceeding BN\u2019s complexity. Second, our results illustrate the strength of numbering systems, specifically, the\nstrength of fixed-point representations. Our results indicate that a correct and meaningful multi-bit representation of parameters is better in both complexity and accuracy than a 1-bit unstructured allocation."}, {"heading": "4.2. CNN on CIFAR 10", "text": "We conduct a similar experiment on the CIFAR10 dataset (Krizhevsky & Hinton, 2009). The dataset consists of 60K color images each representing airplanes, automobiles, birds, cats, deers, dogs, frogs, horses, ships, and trucks. 50K of these images constitute the training set, and the 10K remaining are for testing. SQ\u2019s architecture on this dataset is a simple one: three convolutional layers, interleaved by max pooling layers. The output of the final pooling layer is fed to a 10-way softmax output layer. The reported accuracy using 16-bit fixed-point arithmetic is a 25.4% test error. BN\u2019s architecture is a much wider and deeper architecture based on VGG (Simonyan & Zisserman, 2014). The reported accuracy of the binary network is an impressive 10.15% which is of benchmarking quality even for full precision networks.\nWe adopt a similar architecture as SQ, but leverage re-\ncent advances in convolutional neural networks (CNNs) research. It has been shown that adding networks within convolutional layers (in the \u2018Network in Network\u2019 sense) as described in (Lin et al., 2013) significantly enhances accuracy, while not incurring much complexity overhead. Hence, we replace SQ\u2019s architecture by a deep one which we describe as 64C5\u2212 64C1\u2212 64C1\u2212MP2\u2212 64C5\u2212 64C1\u221264C1\u2212MP2\u221264C5\u221264FC\u221264FC\u221264FC\u2212 10, where C5 denotes 5 \u00d7 5 kernels, C1 denotes 1 \u00d7 1 kernels (they emulate the networks in networks), MP2 denotes 2 \u00d7 2 max pooling, and FC denotes fully connected components. As is customary for this dataset, we apply zero-phase component analysis (ZCA) whitening to the data before training. Because this dataset is a challenging one, we first fine-tune the hyperparameters (learning rate, weight decay rate, and momentum), then train for 300 epochs. The best accuracy we reach in floating point using this 12-layer deep network is 17.02%.\nFigure 2 shows the results of our fixed-point simulation and analysis. Note that, while both bounds from Theorems 1 and 2 still successfully upper bound the test error, these are not as tight as in our MNIST experiment. Furthermore, in this case, (12) dictates keeping BW = BA as EA = 21033\nand EW = 31641 so that log2 \u221a EW EA \u2248 0.29. The fact that EW \u2265 EA is expected as there are typically more weights than activations in a neural network. However, note that in this case the contrast between EW and EA is not as sharp as in our MNIST experiment. This is mainly due to the higher weight to activation ratio in fully connected DNNs than in CNNs.\nWe again select two design options:\nA. Smallest (BA, BW ) such that BW = BA and pm \u2264 1% as bounded by Theorem 1. In this case (BA, BW ) = (12, 12). B. Smallest (BA, BW ) such that BW = BA and pm \u2264 1% as bounded by Theorem 2. In this case (BA, BW ) = (10, 10).\nTable 2 indicates that BN is the most accurate with 10.15% test error. Interestingly, it has lesser computational cost but more representational cost than SQ. This is due to the dependence of the computational cost on the product of BA\nand BW . The least complex network is ours when setting (BA, BW ) = (10, 10) and its test error is 17.23% which is already a large improvement on SQ in spite of having smaller computational and representational costs. This network is also less complex than that of BN.\nThe main take away here is that CNNs are quite different from fully connected DNNs when it comes to precision requirements. Furthermore, from Table 2 we observe that BN achieves the least test error. It seems that this better accuracy is due to its greater representational power rather than its computational power (BN\u2019s representational cost is much higher than the others as opposed to its computational cost)."}, {"heading": "5. Conclusion", "text": "In this paper we analyzed the quantization tolerance of neural networks. We used our analysis to efficiently reduce weight and activation precisions while maintaining similar fidelity as the floating-point initiation. Specifically, we obtained bounds on the mismatch probability between a fixedpoint network and its floating-point counterpart in terms of precision. We showed that a neural network\u2019s accuracy degradation due to quantization decreases double exponentially as a function of precision. Our analysis provides a straightforward method to obtain an upper bound on the network\u2019s error probability as a function of precision. We used these results on real datasets to minimize the computational and representational costs of a fixed-point network while maintaining accuracy.\nOur work addresses the general problem of resource constrained machine learning. One take away is that it is imperative to understand the trade-offs between accuracy and complexity. In our work, we used precision as a parameter to analytically characterize this trade-off. Nevertheless, additional aspects of complexity in neural networks such as their structure and their sparsity can also be accounted for. In fact, more work can be done in that regard. Our work may be viewed as a first step in developing a unified and principled framework to understand complexity vs. accuracy trade-offs in deep neural networks and other machine learning algorithms."}, {"heading": "Acknowledgment", "text": "This work was supported in part by Systems on Nanoscale Information fabriCs (SONIC), one of the six SRC STARnet Centers, sponsored by MARCO and DARPA."}], "year": 2017, "references": [{"title": "A two\u2019s complement parallel array multiplication algorithm", "authors": ["Baugh", "Charles R", "Wooley", "Bruce A"], "venue": "IEEE Transactions on Computers,", "year": 1973}, {"title": "Fast algorithms for signal processing", "authors": ["Blahut", "Richard E"], "year": 2010}, {"title": "A roundoff error analysis of the lms adaptive algorithm", "authors": ["Caraiscos", "Christos", "Liu", "Bede"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing,", "year": 1984}, {"title": "Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks", "authors": ["Y Chen"], "venue": "In 2016 IEEE International Solid-State Circuits Conference (ISSCC),", "year": 2016}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "authors": ["Courbariaux", "Matthieu"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Finite-precision analysis of the pipelined strength-reduced adaptive filter", "authors": ["M. Goel", "N. Shanbhag"], "venue": "Signal Processing, IEEE Transactions on,", "year": 1998}, {"title": "Deep Learning with Limited Numerical Precision", "authors": ["S Gupta"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "authors": ["Han", "Song"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "year": 2015}, {"title": "Deep residual learning for image recognition", "authors": ["He", "Kaiming"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "year": 2016}, {"title": "Quantized neural networks: Training neural networks with low precision weights and activations", "authors": ["Hubara", "Itay", "Courbariaux", "Matthieu", "Soudry", "Daniel", "ElYaniv", "Ran", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1609.07061,", "year": 2016}, {"title": "Binarized neural networks", "authors": ["Hubara", "Itay"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}, {"title": "Fixed-point feedforward deep neural network design using weights+", "authors": ["Hwang", "Kyuyeon", "Sung", "Wonyong"], "venue": "In Signal Processing Systems (SiPS),", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "authors": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "authors": ["Krizhevsky", "Alex"], "venue": "In Advances in neural information processing systems,", "year": 2012}, {"title": "Fixed point quantization of deep convolutional networks", "authors": ["Lin", "Darryl"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "year": 2016}, {"title": "Network in network", "authors": ["Lin", "Min"], "venue": "arXiv preprint arXiv:1312.4400,", "year": 2013}, {"title": "Variation-tolerant architectures for convolutional neural networks in the near threshold voltage regime", "authors": ["Lin", "Yingyan"], "venue": "In Signal Processing Systems (SiPS),", "year": 2016}, {"title": "Xnor-net: Imagenet classification using binary convolutional neural networks", "authors": ["Rastegari", "Mohammad"], "venue": "In European Conference on Computer Vision,", "year": 2016}, {"title": "Minimum precision requirements for the SVM-SGD learning algorithm", "authors": ["Sakr", "Charbel"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "year": 2017}, {"title": "Energy-efficient machine learning in silicon: A communications-inspired approach", "authors": ["Shanbhag", "Naresh R"], "venue": "arXiv preprint arXiv:1611.03109,", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "authors": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1409.1556,", "year": 2014}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "authors": ["Srivastava", "Nitish"], "venue": "Journal of Machine Learning Research,", "year": 2014}, {"title": "Deepface: Closing the gap to humanlevel performance in face verification", "authors": ["Taigman", "Yaniv"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "year": 2014}, {"title": "Accelerating very deep convolutional networks for classification and detection", "authors": ["Zhang", "Xiangyu"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "year": 2015}], "id": "SP:79cdf55d19025e39300cf99c3c5796b923a552e2", "authors": [{"name": "Charbel Sakr", "affiliations": []}, {"name": "Yongjune Kim", "affiliations": []}, {"name": "Naresh Shanbhag", "affiliations": []}], "abstractText": "The acclaimed successes of neural networks often overshadow their tremendous complexity. We focus on numerical precision a key parameter defining the complexity of neural networks. First, we present theoretical bounds on the accuracy in presence of limited precision. Interestingly, these bounds can be computed via the back-propagation algorithm. Hence, by combining our theoretical analysis and the backpropagation algorithm, we are able to readily determine the minimum precision needed to preserve accuracy without having to resort to timeconsuming fixed-point simulations. We provide numerical evidence showing how our approach allows us to maintain high accuracy but with lower complexity than state-of-the-art binary networks.", "title": "Analytical Guarantees on Numerical Precision of Deep Neural Networks"}