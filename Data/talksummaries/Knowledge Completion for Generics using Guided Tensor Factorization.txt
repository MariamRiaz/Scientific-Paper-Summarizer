0	78	We consider the problem of completing a partial knowledge base (KB) containing facts about generics or common nouns, represented as a third-order tensor of (source, relation, target) triples, such as (butterfly, pollinate, flower) and (thermometer, measure, temperature).
1	17	Such facts capture common knowledge that humans have about the world.
2	20	They are arguably essential for intelligent agents with human-like conversational abilities as well as for specific applications such as question answering.
3	98	We demonstrate that state-of-the-art KB completion methods perform poorly when faced with generics, while our strategies for incorporating external knowledge as well as obtaining additional annotations for rare entities provide the first successful solution to this challenging new task.
4	108	Since generics represent classes of similar individuals, the truth value yi of a generics triple xi = (s, r, t) depends on the quantification semantics one associates with s and t. Indeed, the semantics of generics statements can be ambiguous, even selfcontradictory, due to cultural norms.
5	16	As Leslie (2008) points out, ‘ducks lay eggs’ is generally considered true while ‘ducks are female’, which is true for a broader set of ducks than the former statement, is generally considered false.
6	43	To avoid deep philosophical issues, we fix a particular mathematical semantics that is especially relevant for noisy facts derived automatically from text: associate s with a categorical quantification from {all, some, none} and associate t (implicitly) with some.
7	44	For instance, “all butterflies pollinate (some) flower” and “some animals live in (some) forest”.
9	23	As a notational shortcut, we treat the quantification of s as the categorical label yi for the triple xi.
14	16	Given a noisy KB of such labeled triples, the task is to infer more triples.
21	16	On the other hand, if we take two animals that share a ‘parent’ in some taxonomy (e.g., reindeer and deer), then the likelihood of knowledge transfer increases.
22	38	We propose to make use of additional rich background knowledge complementing the information present in the KB itself, such as a taxonomic hierarchy of entities (available from sources such as WordNet (Miller, 1995)) and the corresponding entity types and relation schema.
23	32	Our key insight is that, if used appropriately, taxonomic and schema information can be surprisingly effective in making tensor factorization methods vastly more effective for generics for deriving high precision facts.
27	46	We propose three ways of using this information and empirically demonstrate the effectiveness of each on two variants of a KB of elementary level science facts (Dalvi et al., 2017).1 First, we observe that simply imposing schema consistency (Section 3.1) on derived facts can significantly boost state-of-the-art methods such as Holographic Embeddings (HolE) (Nickel et al., 2016b) from nearly no new facts at 80% precision to over 10,000 new facts, starting with a generics KB of a similar size.
30	134	We show that expanding the starting tensor this way before applying tensor factorization is complementary and results in a statistically significantly higher precision (86.4% as opposed to 82%) over new facts at the same yield.
32	55	Dalvi et al. (2017) have observed that, when using information extraction methods, it is much harder to derive reliable facts about generics than about named entities.
33	51	This makes generics KBs vastly incomplete, with no or very little information about certain entities such as caribou or oriole.
34	19	Our active learning approach addresses the following question: Given a new entity3 ẽ and a budget B, what is a good set Q of B queries about ẽ to annotate (via humans) such that expanding the original tensor with Q helps a KB completion method infer many more high precision facts about ẽ?
35	20	We propose to define a correlation based measure of the uncertainty of each unannotated triple (i.e., a potential query) involving ẽ, based on how frequently the corresponding triple is true for ẽ’s siblings in the taxonomic hierarchy (Section 4.1).
36	45	We then develop a submodular objective function, and a corresponding greedy (1 − 1/e)-approximation, to search for a small subset of triples to annotate that optimally balances diversity with coverage (Section 4.2).
37	47	We demonstrate that annotating this balanced subset makes tensor factorization derive substantially more new and interesting facts compared to several active learning baselines.
38	48	For example, with a budget to annotate 100 queries about a new entity oriole, random queries lead to no new true facts at all (via annotation followed by tensor factorization), imposing schema consistency results in 83 new facts, and our proposed method ends up with 483 new facts.
39	15	This demonstrates that well-designed intelligent queries can be substantially more effective in gathering facts about the new entity.
40	24	In summary, this work tackles, for the first time, the challenging task of knowledge completion for generics, by imposing consistency with external knowledge.
118	80	This allows one to make meaningful existential statements such as: if a property holds for all or most members of even one class ci, then it holds for some (reasonable number of) members of its parent class p. We use the following rules:6 ((p, rj , tj), all)⇒ ∀i ((ci, rj , tj), all) ∀i ((ci, rj , tj), all)⇒ ((p, ej , tj), all) ∃i ((ci, rj , tj), all)⇒ ((p, ej , tj), some) ∃i ((ci, rj , tj), some)⇒ ((p, ej , tj), some) We apply these rules to address sparsity of generics tensors, making tensor factorization more robust.
122	21	The goal is to use tensor factorization to generate high quality facts about such entities.
123	143	For instance, consider the task of inferring facts about oriole, where all we know is that it is a bird.
124	16	We assume a restricted budget on the number of facts we can query (for human annotation) about oriole, using which we would like to predict many more high-quality facts about it.
126	33	We view this as an active learning problem and propose a two-step algorithm.
130	16	For notational simplicity and without loss of generality, throughout this section, we consider the case where ẽ appears as the source entity in the triple; the ideas apply equally when ẽ appears as the target entity in the triple.
