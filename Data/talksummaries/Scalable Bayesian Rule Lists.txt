27	1	It was created in about 9 seconds on a laptop and achieves perfect out-of-sample accuracy.
36	1	The default rule is at the bottom, which makes predictions for observations that are not satisfied by any of the conditions.
42	1	The support of antecedent aj is denoted supp(aj), which is the number of observations that obey condition aj .
43	1	A condition is a conjunction of expressions “feature∈values,” e.g., age∈[40,50] and color=white.
51	1	E.g, if a is ‘x1=green’ and ‘x2<50’, this has cardinality 2.
57	1	Here, the number of rules m is Poisson, truncated at the total number of pre-selected antecedents: p(m|A, λ) = (λ m/m!
65	1	This is the full model, and the posterior p(d|x,y,A, α, λ, η) is what we optimize to obtain the best rule lists.
70	1	However, to optimize performance we use a more efficient rule list representation that is amenable to fast computation.
77	1	For a one million sample data set (or more precisely up to 1,048,576 observations) each rule carries with it a 128 KB vector (since a byte consists of 8 bits), which fits comfortably in most L2 caches.
109	1	The sequence of b’s that we define next is a lower bound for the possible sequence of |Qc|’s.
115	1	In our notation, rule list d is defined by the antecedents and the probabilities on the right side of the rules, d = (m, {al, θl}ml=1).
121	1	We next provide a bound that eliminates certain regions of the rule space from consideration.
123	1	If the best possible rule list starting with a1, .., ap cannot beat the posterior of the best rule list we have found so far, then we know any rule list starting with a1, .., ap is suboptimal.
125	1	This is a type of branch and bound strategy, in that we have now eliminated (bounded) the entire set of lists starting with a1, .., ap.
126	1	We formalize this intuition below.
132	1	Here, Nj,0 is the number of points captured by rule j with label 0, and Nj,1 is the number of points captured by rule j with label 1, Nj,0 = |{i : Captr(i) = j and yi = 0}|, Nj,1 = |{i : Captr(i) = j and yi = 1}|.
140	1	The proofs are lengthy and contained in the longer version of this work (Yang et al., 2017).
143	1	To represent uninterpretable methods, we chose logistic regression, SVM RBF, random forests (RF), and boosted decision trees (ADA).
144	1	To represent the class of “interpretable” algorithms, we chose CART, C4.5, RIPPER (Cohen, 1995), CBA (Liu et al., 1998), and CMAR (Li et al., 2001).
145	1	Other works (see Letham et al., 2015; Wang & Rudin, 2015a) have accuracy/interpretability comparisons to Bayesian Rule Lists and Falling Rule Lists, so our main effort here will be to study the scalability component.
147	1	We benchmark using publicly available datasets (see Bache & Lichman, 2013) that have interpretable features: the Tic Tac Toe dataset, where the goal is to determine whether the “X” player wins; the Adult dataset, where we aim to predict whether an individual makes over $50K peryear; the Mushroom dataset, where the goal is to predict whether a mushroom is edible; the Nursery dataset, where the goal is to predict whether a child’s application to nursery school will be in either the “very recommended” or “special priority” categories; and the Telco customer churn dataset (see WatsonAnalytics, 2015), where the goal is to predict whether a customer will leave the service provider.
151	1	Then we fixed λ at the length of the returned rule list for that dataset.
158	1	This should not be a difficult learning problem since there are solutions with perfect accuracy on the training set that generalize to the test set.
160	1	We tried many different parameter settings for CART (in blue), and many different parameter settings for C4.5 (in gray), none of which were able to achieve points on the efficient frontier defined by SBRL.
162	1	Adult: For the Adult dataset, results are in Figure 3, Figure 4 and Table 2.
169	1	Mushroom: Table 1 contains an SBRL rule list for Mushroom; other results are in Yang et al. (2017).
178	3	The runtime comparison with CART is shown in Table 5.
179	15	For problem (A), the run times are similar; for (B); SBRL is slower (2.5 hours), which is not prohibitive for important problems.
180	117	One can see why CART does not perform as well in high dimensions, as it often spends less time on harder problems than on easier ones; details are in (Yang et al., Table 3.
181	117	Run Time on Telco dataset RUN TIME LR SVM RF ADA CART C4.5 RIPPER CBA CMAR SBRL MEAN 0.267 7.468 3.703 7.839 0.168 0.250 37.14 8.028 1.679 5.239 MEDIAN 0.272 7.550 3.695 8.726 0.168 0.252 37.63 8.050 1.705 5.271 STD 0.009 0.207 0.183 0.111 0.008 0.017 3.202 0.400 0.161 0.149 2017).
