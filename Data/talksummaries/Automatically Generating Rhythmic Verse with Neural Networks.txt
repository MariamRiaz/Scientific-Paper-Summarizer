10	31	Poetry generation is an interesting application, since performing this task automatically requires the creation of models that not only focus on what is being written (content), but also on how it is being written (form).
12	18	The first involves training a model to learn an implicit representation of content and form through the use of a phonological encoding.
13	11	The second involves training a generative language model to represent content, which is then constrained by a discriminative pronunciation model, representing form.
14	55	This second model is of particular interest because poetry with arbitrary rhyme, rhythm, repetition and themes can be generated by tuning the pronunciation model.
31	24	1) Phonetic encoding results in information loss: words that have the same pronunciation (homophones) cannot be perfectly reconstructed from the corresponding phonemes.
33	12	2) The variety of poetry and poetic devices one can use— e.g., rhyme, rhythm, repetition—means that poems sampled from a model trained on all poetry would be unlikely to maintain internal consistency of meter and rhyme.
39	14	In addition, virtually all letters can, in some contexts, map to zero phones, which is known as ‘wild’ or epsilon.
42	16	Irregularities in the English language result in difficulty determining general letter-to-sound rules that can manage words with unusual pronunciations such as “colonel” and “receipt” 2.
44	15	This makes decipherment, when converting back into an orthographic representation, much easier.
50	18	We apply backpropagationthrough-time (Werbos, 1990) for 150 timesteps, which roughly equates to four lines of poetry in sonnet form.
54	9	That is, we compute the most probable hypothesis word W given a phoneme sequence ρ: argmaxi P (Wi | ρ ) (1) We can consider the phonetic encoding of plaintext to be a homophonic cipher; that is, a cipher in which each symbol can correspond to one or more possible decodings.
74	11	Instead of attempting to represent both form and content in a single model, we construct a pipeline containing a generative language model representing content, and a discriminative model representing form.
82	17	The increase in corpus size facilitates a corresponding increase in the number of permissible model parameters.
83	26	This allows us to train a 3-layer LSTM model with 2048- dimensional hidden layers, with embeddings in 128 dimensions.
85	11	We attenuate the learning rate over time, and by 20 epochs the model converges.
94	16	This allows us to derive a syllablestress distribution.
96	9	We represent each line of poetry as a cascade of Weighted Finite State Transducers (WFST).
117	13	If a new word is rejected by the classifier, the state of the network is rolled back to the last formulaically acceptable state of the line, removing the rejected word from memory.
129	16	First, we compile a list of similar words to a key theme word by retrieving its semantic neighbours from a distributional semantic model (Mikolov et al., 2013).
135	12	That is, to sample a line with many instances of alliteration (multiple words with the same initial sound) we record the historical frequencies of characters sampled at the beginning of each previous word.
143	53	Second, we perform an extrinsic evaluation where we evaluate the generated output using human annotators, and compare it to human-generated poetry.
144	75	To evaluate the ability of both models to generate formulaic poetry that adheres to rhythmic rules, we compared sets of fifty sampled lines from each model.
146	40	The second set was sampled from the characterlevel model, constrained to Iambic form.
147	26	For com- parison, and to act as a baseline, we also sampled from the unconstrained character model.
149	31	We then compared these observations to loose Iambic Pentameter (containing all four variants), to determine how many syllabic misclassifications existed on each line.
150	146	This was done by speaking each line aloud, and noting where the speaker put stresses.
151	17	As Table 1 shows, the constrained character level model generated the most formulaic poetry.
152	23	Results from this model show that 70% of lines had zero mistakes, with frequency obeying an inverse power-law relationship with the number of errors.
153	9	We can see that the phonetic model performed similarly, but produced more subtle mistakes than the constrained character model: many of the errors were single mistakes in an otherwise correct line of poetry.
158	15	The relatively low per-word accuracy achieved on the Wikipedia corpus is likely due to the high frequency of out-ofvocabulary words.
