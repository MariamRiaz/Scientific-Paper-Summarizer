{"sections": [{"heading": "1. Introduction", "text": "The Poisson process is an important model for point data in which samples of the process are locally finite subsets of some domain such as time or space. The process is parametrised by an intensity function, the integral of which gives the expected number of points in the domain of integration \u2014 for a gentle introduction we recommend (Baddeley, 2007). In the typical case of unknown intensity function we may place a non-parametric prior over it via e.g. the Gaussian Process (GP) and perform Bayesian inference.\nInference under such models is challenging due to both the GP prior and the non factorial nature of the Poisson process likelihood (1), which includes an integral of the intensity function. One may resort to discretising the domain (Rathbun & Cressie, 1994; M\u00f8ller et al., 1998; Rue et al., 2009) or performing Monte Carlo approximations (Adams et al., 2009; Diggle et al., 2013). Fast Laplace approximates were studied in (Cunningham et al., 2008; Illian et al., 2012; Flaxman et al., 2015) and variational methods were applied\n1Data61, CSIRO, Australia 2The Australian National University 3University of Technology Sydney. Correspondence to: Christian <christian.walder@anu.edu.au>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nin (Lloyd et al., 2015; Kom Samo & Roberts, 2015).\nTo satisfy non-negativity of the intensity function one transforms the GP prior. The log-Gaussian Cox Process, with GP distributed log intensity, has been the subject of much study; see e.g. (Rathbun & Cressie, 1994; M\u00f8ller et al., 1998; Illian et al., 2012; Diggle et al., 2013), Alternative formulations for introducing a GP prior exist, e.g. (Adams et al., 2009). More recent research has highlighted the analytical and computational advantages (Lloyd et al., 2015; 2016; Flaxman et al., 2017; M\u00f8ller et al., 1998) of the permanental process, which has GP distributed square root intensity (Shirai & Takahashi, 2003; McCullagh & M\u00f8ller, 2006) \u2014 we discuss the relationship between these methods and the present work in more detail in subsection 2.2.\nIn section 2 we introduce the Poisson and permanental processes, and place our work in the context of existing literature. Section 3 reviews Flaxman et al. (2017), slightly recasting it as regularised maximum likelihood for the permanental process. Our Bayesian scheme is then derived in section 4. In section 5 we discuss the choice of covariance function for the GP prior, before presenting some numerical experiments in section 6 and concluding in section 7."}, {"heading": "2. The Model", "text": ""}, {"heading": "2.1. The Poisson Process", "text": "We view the inhomogeneous Poisson process on \u2126 as a distribution over locally finite subsets of \u2126. The number N(X ) of elements in some X \u2286 \u2126 is assumed to be distributed as Poisson(\u039b(X , \u00b5)), where \u039b(S, \u00b5) :=\u222b x\u2208S \u03bb(x)d\u00b5(x) gives the mean of the Poisson distribution. It turns out that this implies the likelihood function\np ({xi}mi=1 |\u03bb,\u2126) = m\u220f i=1 \u03bb(xi) exp (\u2212\u039b(\u2126)) . (1)"}, {"heading": "2.2. Latent Gaussian Process Intensities", "text": "To model unknown \u03bb(x), we employ a non-parametric prior over functions, namely the Gaussian process (GP). To ensure that \u03bb is non-negative valued we include a deterministic \u201clink\u201d function g : R \u2192 R+ so that we have the prior over \u03bb defined by \u03bb = g \u25e6 f and f \u223c GP(k), where k is the covariance function for f . The most com-\nmon choice for g is the exponential function exp(\u00b7), leading to the log-Gaussian Cox process (LGCP) (M\u00f8ller et al., 1998). Recently Adams et al. (2009) employed the transformation g(z) = \u03bb\u2217(1 + exp(\u2212z))\u22121 , which permits efficient sampling via thinning (Lewis & Shedler, 1979) due to the bound 0 \u2264 \u03bb(x) \u2264 \u03bb\u2217."}, {"heading": "2.2.1. PERMANENTAL PROCESSES: SQUARED LINK FUNCTION", "text": "In this paper we focus on the choice g(z) = 12z 2, known as the permanental process (Shirai & Takahashi, 2003; McCullagh & M\u00f8ller, 2006). Two recent papers have demonstrated the analytical and computational advantages of this link function.\n1. Flaxman et al. (2017) derived a non-probabilistic regularisation based algorithm which we review in section 3, and which exploited properties of reproducing kernel Hilbert spaces. The present work generalises their result, providing probabilistic predictions and Bayesian model selection. Our derivation is by necessity entirely different to Flaxman et al. (2017), as their representer theorem (Scho\u0308lkopf et al., 2001) argument is insufficient for our probabilistic setting (see e.g. subsubsection 4.1.6).\n2. (Lloyd et al., 2015) derived a variational approximation to a Bayesian model with the squared link function, based on an inducing variable scheme similar to (Titsias, 2009), and exploiting the tractability of certain required integrals. The present work has the advantage of 1) not requiring the inducing point approximation, 2) being free of non-closed form expressions such as their G\u0303 and 3) being simpler to implement and orders of magnitude faster in practice while, as we demonstrate, exhibiting comparable predictive accuracy."}, {"heading": "3. Regularised Maximum Likelihood", "text": "Flaxman et al. (2017) combined (1) with the regularisation term \u2016f\u20162H(k), leading to the regularised maximum likelihood estimator for f , namely f\u0302 :=\nargmax f m\u2211 i=1 log 1 2 f2(xi)\u2212 1 2 ( \u2016f\u20162L2(\u2126,\u00b5) + \u2016f\u2016 2 H(k) ) \ufe38 \ufe37\ufe37 \ufe38\n:=\u2016f\u20162H(k,\u2126,\u00b5)\n,\n(2)\nwhere we have implicitly defined the new RKHS H(k,\u2126, \u00b5) := H(k\u0303). Now, provided we can compute the associated new reproducing kernel k\u0303, then we may appeal to the representer theorem (Kimeldorf & Wahba, 1971) in order to compute the f\u0302 , which takes the form\n\u2211m i=1 \u03b1ik\u0303(xi, \u00b7) for some \u03b1i. The function k\u0303 may be expressed in terms of the Mercer expansion (Mercer, 1909)\nk(x,y) = N\u2211 i=1 \u03bbi\u03c6i(x)\u03c6i(y), (3)\nwhere \u03c6i are orthonormal in L2(\u2126, \u00b5). To satisfy for arbitrary f = \u2211 i wi\u03c6i the reproducing property (Aronszajn, 1950)\u2329 k(x, \u00b7),\n\u2211 i wi\u03c6(\u00b7)i \u232a H(k) := f(x) = \u2211 i wi, \u03c6i(x) (4)\nwe let \u03c6i be orthogonal in H(k), obtaining \u3008\u03c6i, \u03c6j\u3009 = \u03b4ij\u03bb \u22121 i . Hence, \u2016 \u2211 i wi\u03c6i\u2016 2 H(k) = \u2211 i w 2 i /\u03bbi, and from\n(2) we have \u2016 \u2211 i wi\u03c6i\u2016 2 H(k,\u2126,\u00b5) = \u2211 i w 2 i (1 + \u03bb \u22121 i ), so\nk\u0303(x,y) = N\u2211 i=1\n1\n1 + \u03bb\u22121i \u03c6i(x)\u03c6i(y). (5)\nFor approximate Bayesian inference however, we cannot simply appeal to the representer theorem."}, {"heading": "4. Approximate Bayesian Inference", "text": "In subsection A.3 of the supplementary material, we review the standard Laplace approximation to the GP with non-Gaussian likelihood. This a useful set-up for what follows, but is not directly generalisable to our case due to the integral in (1). Instead, in subsection 4.1 we now take a different approach based on the Mercer expansion."}, {"heading": "4.1. Laplace Approximation", "text": "It is tempting to na\u0131\u0308vely substitute k\u0303 into subsection A.3 of the supplementary material, and to neglect the integral part of the likelihood. Indeed, this gives the correct approximate predictive distribution. The marginal likelihood does not work in this way however (due to the log determinant in (18)). We now perform a more direct analysis."}, {"heading": "4.1.1. MERCER EXPANSION SETUP", "text": "Mercer\u2019s theorem allows us to write (3), where for nondegenerate kernels, N = \u221e. Assume a linear model in \u03a6(x) = (\u03c6i(x))i so that1\nf(x) = w>\u03a6(x), (6)\nand let w \u223c N (0,\u039b) where \u039b = (\u03bbi)ii is a diagonal covariance matrix. This is equivalent to f \u223c GP(k) because\ncov(f(x), f(z)) = \u03a6(x)>\u039b \u03a6(z) = k(x, z).\n1We use a sloppy notation where (x)i is the i-th element of x while (xi)i is a vector with i-th element xi, etc.\nRecall that the Poisson process on \u2126 with intensity \u03bb(x) = 1 2f 2(x) has likelihood for X := {xi}mi=1\nlog p(X|w,\u2126, k) = m\u2211 i=1 log 1\n2 f2(xi)\ufe38 \ufe37\ufe37 \ufe38\n:=log h(X|w)\n\u22121 2 \u222b x\u2208\u2126\nf2(x)d\u00b5(x)\ufe38 \ufe37\ufe37 \ufe38 w>w\nThe joint in w, X is\nlog p(w, X|\u2126, k)\n= log h(X|w)\u22121 2 w>(I+\u039b\u22121)w\u22121 2 log |\u039b|\u2212N 2 log 2\u03c0."}, {"heading": "4.1.2. LAPLACE APPROXIMATION", "text": "We make a Laplace approximation to the posterior, which is the normal distribution\nlog p(w|X,\u2126, k) \u2248 logN (w|w\u0302, Q) (7)\n= \u22121 2 (w \u2212 w\u0302)>Q\u22121(w \u2212 w\u0302)\u2212 1 2 log |Q| \u2212 N 2 log 2\u03c0\n:= log q(w|X,\u2126, k),\nwhere w\u0302 is chosen as the mode of the true posterior, and Q is the inverse Hessian of the true posterior, evaluated at w\u0302."}, {"heading": "4.1.3. PREDICTIVE MEAN", "text": "The mode w\u0302 is\nw\u0302 = argmax w\nlog p(w|X,\u2126, k)\n= argmax w log h(X|w)\u2212 1 2 w>(I + \u039b\u22121)w. (8)\nCrucially, w\u0302 must satisfy the stationarity condition\nw\u0302 = (I + \u039b\u22121)\u22121 \u2207w log h(X|w)|w=w\u0302 , (9)\nwhere\n\u2207w log h(X|w)|w=w\u0302 = 2 m\u2211 i=1 \u03a6(xi) \u03a6(xi)>w\u0302 .\nThe approximate predictive mean is therefore\nf\u0302(x\u2217) := E [f(x\u2217)|X,\u2126, k] = \u03a6(x\u2217)>w\u0302\n= m\u2211 i=1\n2\n\u03a6(xi)>w\u0302 \u00b7 \u03a6(xi)>(I + \u039b\u22121)\u22121\u03a6(x\u2217)\n:= m\u2211 i=1 \u03b1ik\u0303(xi,x \u2217). (10)\nThis reveals the same k\u0303 as (5). From (10) we have\n\u03b1\u0302i = 2/f\u0302(xi). (11)\nPutting (9), (10) and (11) into (8), we obtain\n\u03b1\u0302 = argmin \u03b1 m\u2211 i=1 log\u03b12i + 1 2 \u03b1>K\u0303\u03b1,\nwhere K\u0303 = (k\u0303(xi),xj)ij . This is equivalent to Flaxman et al. (2017), though slightly simplified by (11). Interestingly, unlike Flaxman et al. (2017) (or the analogous section 3), we did not appeal to the representer theorem."}, {"heading": "4.1.4. PREDICTIVE VARIANCE", "text": "We now compute theQ in (7). The Hessian term giving the inverse covariance becomes\nQ\u22121 = \u2212 \u2202 2\n\u2202w\u2202w> log p(w, X,\u2126, k) \u2223\u2223\u2223\u2223 w=w\u0302\n= I + \u039b\u22121 +W\nW = \u2212 \u2202 2\n\u2202w\u2202w> log h(X|w) \u2223\u2223\u2223\u2223 w=w\u0302\n= 2 m\u2211 i=1 \u03a6(xi)\u03a6(xi) > (\u03a6(xi)>w\u0302)2 := V DV >,\nwhere V:i = \u03b1i \u00d7 \u03a6(xi) and D = 12I \u2208 R m\u00d7m. The approximate predictive variance can now be rewritten as an m-dimensional matrix expression using the identity (Z + V DV >) = Z\u22121\u2212Z\u22121V (V >Z\u22121V +D\u22121)V >Z\u22121 with and Z = I + \u039b\u22121 along with a little algebra, to derive2\n\u03c32(x\u2217) := Var [f(x\u2217)|X,\u2126, k] = \u03a6(x\u2217)>Q \u03a6(x\u2217)\n= k\u0303(x\u2217,x\u2217)\u2212 ( k\u0303(x\u2217, X) \u03b1 ) S\u22121 ( \u03b1> k\u0303(X,x\u2217) ) ,\nwhere is the Hadamard product, or element-wise multiplication, and S := ( k\u0303(X,X) (\u03b1\u03b1>) + 2I ) ."}, {"heading": "4.1.5. PREDICTIVE DISTRIBUTION", "text": "Given the approximate predictive distribution f(x\u2217)|X,\u2126, k \u223c N (f\u0302(x\u2217), \u03c32(x\u2217)) := N (\u00b5, \u03c32) and the relation \u03bb(\u00b7) = 12f\n2(\u00b7) it is straightforward to derive the corresponding3 \u03bb(x\u2217)|X,\u2126, k \u223c Gamma(\u03b1, \u03b2) where the shape \u03b1 = (\u00b5 2+\u03c32)2\n2\u03c32(2\u00b52+\u03c32) and the scale \u03b2 = 2\u00b52\u03c32+\u03c34 \u00b52+\u03c32 ."}, {"heading": "4.1.6. MARGINAL LIKELIHOOD", "text": "Letting q(w\u0302, X|\u2126, k) be the Taylor expansion of log p(w, X|\u2126, k) about the mode w = w\u0302 and evaluating at w\u0302 gives, as linear and quadratic terms vanish,\nlog q(w\u0302, X|\u2126, k) = log p(w\u0302, X|\u2126, k)\n= log h(X|w\u0302)\u22121 2 w\u0302>(I+\u039b\u22121)w\u0302\u22121 2 log |\u039b|\u2212N 2 log 2\u03c0.\n2Where e.g. k\u0303(X,x\u2217) is an m\u00d7 1 matrix of evaluations of k\u0303. 3Gamma(x|\u03b1, \u03b2) has p.d.f. 1\n\u0393(k)\u03b2k x\u03b1\u22121 exp(\u2212x/\u03b2).\nSimilarly to (19) we get approximate marginal likelihood\nlog p(X|\u2126, k) \u2248 log q(w\u0302, X|\u2126, k)\u2212 log q(w\u0302|X,\u2126, k)\n= log h(X|w\u0302)\ufe38 \ufe37\ufe37 \ufe38 \u2212 \u2211m i=1 log 2\u03b1 2 i \u22121 2 ( w\u0302>(I + \u039b\u22121)w\u0302\ufe38 \ufe37\ufe37 \ufe38 \u03b1>k\u0303(X,X)\u03b1 \u2212 log |\u039b|+log |Q| ) (12)\nWe now use the determinant identity |Z + V DV >| = |Z||D||V >Z\u22121V + D\u22121| with the same Z, V and D as subsubsection 4.1.4 to derive\n\u2212 log |\u039b|+ log |Q| = \u2212 log |\u039b| \u2212 log |Z + V DV >| = \u2212 log \u2223\u2223\u039b(I + \u039b\u22121)\u2223\u2223\u2212 log \u2223\u2223D\u22121 + V >Z\u22121V \u2223\u2223+ c =\nN\u2211 i=1 log 1\n1 + \u03bbi\ufe38 \ufe37\ufe37 \ufe38 :=V(k,\u2126,\u00b5)\n\u2212 log \u2223\u2223\u2223k\u0303(X,X) (\u03b1\u03b1>) + 2I\u2223\u2223\u2223+ c,\n(13)\nwhere c = m log(2). V(k,\u2126, \u00b5) is the crucial ingredient, not accounted for by na\u0131\u0308vely putting k\u0303 into subsection A.3."}, {"heading": "5. Covariance Functions", "text": "To apply our inference scheme we need to compute:\n1. The function k\u0303 from equation (10), studied recently by Flaxman et al. (2017) and earlier by Sollich & Williams (2005) as the equivalent kernel.\n2. The associated term V(k,\u2126, \u00b5) from equation (13), required for the marginal likelihood.\nThis is often challenging for compact domains such as the unit hyper-cube. Such domains are crucial however, if we are to avoid the well-known edge-effects which arise from neglecting the fact that our data are sampled from, say, a two dimensional rectangle. In subsection 5.1 we provide a simple constructive approach to the case \u2126 = [0, 1]d. The following subsection 5.2 presents the general approximation scheme due to Flaxman et al. (2017), for the case when we have k but not its Mercer expansion."}, {"heading": "5.1. Thin-Plate Semi-norms on the Hyper-Cube", "text": "Consider the input domain \u2126 = [0, \u03c0]d with Lebesgue measure \u00b5. A classical function regularisation term is the so called m-th order thin-plate spline semi-norm,\n\u3008f, g\u3009T P(m) := \u2211 |\u03b1|=m m!\u220f j \u03b1j ! \u222b x\u2208\u2126 \u2202mf \u2202x\u03b1 \u2202mg \u2202x\u03b1 d\u00b5(x)\n= \u3008f,\u2206mg\u3009L2(\u2126) + B. (14)\nHere \u03b1 is a multi-index running over all indices of total order |\u03b1| := \u2211 j \u03b1j = m, and the boundary conditions B come from formal integration (see e.g. Wahba (1990, section 2.4). We neglect B (for reasons explained shortly) and include the zero-th derivative to define\n\u3008f, g\u3009H(k) := \u3008f, (a\u2206m + b)g\u3009L2(\u2126).\nWe may select the free parameters a > 0, b > 0 and m \u2208 Z+ using the maximum marginal likelihood criterion. In general, it is challenging to obtain the expressions we require in closed form for arbitrary d, \u2126 and m. The analytical limit in the literature appears to be the case m = 2 with dimension d = 1 along with so-called Neumann boundary conditions (which impose a vanishing gradient on the boundary (Sommerfeld & Straus, 1949)). That k\u0303 has been derived in closed form as the reproducing kernel of an associated Sobolev space by Thomas-Agnan (1996).\nWe now present a simple but powerful scheme which sidesteps these challenges via a well chosen series expansion. Consider the basis function\n\u03c6\u03b2(x) := (2/\u03c0) d/2 d\u220f j=1 \u221a 1/2 [\u03b2j=0] cos(\u03b2jxj),\nwhere \u03b2 is a multi-index with non-negative (integral) values, and [\u00b7] denotes the indicator function (which is one if the condition is satisfied and zero otherwise). The \u03c6\u03b2 form a convenient basis for our purposes. They are orthonormal:\n\u3008\u03c6\u03b2 , \u03c6\u03b3\u3009L2(\u2126) = [\u03b2 = \u03b3],\nand also eigenfunctions of our regularisation operator with\n(a\u2206m + b)\u03c6\u03b2 = ( a ( d\u2211 j=1 \u03b22j )m + b ) \u03c6\u03b2 . (15)\nNow if we restrict the function space to H(k) := { f = \u2211 \u03b2\u22650 c\u03b2\u03c6\u03b2 : \u2016f\u20162H(k) = \u2211 \u03b2\u22650 c2\u03b2/\u03bb\u03b2 <\u221e } ,\nthen it is easily verified that the boundary conditions B in (14) vanish. This is a common approach to solving partial differential equations with Neumann boundary conditions (see e.g. Sommerfeld & Straus (1949)). By restricting in this way, we merely impose zero partial derivatives at the boundary, while otherwise enjoying the usual Fourier series approximation properties. Hence we can combine the reproducing property (4) with (14) and (15) to derive\nk(x,y) = \u2211 \u03b2\u22650 \u03bb\u03b2\u03c6\u03b2(x)\u03c6\u03b2(y), (16)\nwhere \u03bb\u03b2 := 1/(a (\u2211d j=1 \u03b2 2 j )m + b).\nThe above covariance function is not required for our inference algorithm. Rather, the point is that since the basis is also orthonormal, we may substitute \u03bb\u03b2 and \u03c6\u03b2 into (10) and (13) to obtain k\u0303 and V(k), as required.\nSeries truncation. We have discovered closed form expressions for k\u0303 only for m \u2264 2 and d = 1. In practice we may truncate the series at any order and still obtain a valid model due to the equivalence with the linear model (6). Hence, a large approximation error (in terms of k\u0303) due to truncation may be irrelevant from a machine learning perspective, merely implying a different GP prior over functions. Indeed, the maximum marginal likelihood criterion based on subsubsection 4.1.6 may guide the selection of an appropriate truncation order, although some care needs to be taken in this case."}, {"heading": "5.2. Arbitrary Covariances and Domains", "text": "Flaxman et al. (2017) suggested the following approximation for k\u0303, for the case when k is known but the associated Mercer expansion is not. The approximation is remarkably general and elegant, and may even be applied to nonvectorial data by employing, say, a kernel function defined on strings (Lodhi et al., 2002). The idea is to note that the \u03c6i, \u03bbi pairs are eigenfunctions of the integral operator (see Rasmussen & Williams (2006) section 4.3)\nTk : H(k)\u2192 H(k) f 7\u2192 Tkf := \u222b x\u2208\u2126 k(x, \u00b7)f(x)p(x) dx,\nwhere p is related to \u00b5 of the previous subsection by \u00b5(x) = p(x) dx. The Nystro\u0308m approximation (Nystro\u0308m, 1928) to Tk draws m samples X from p and defines T\n(X) k g := 1 m \u2211 x\u2208X k(x, \u00b7)g(x). Then the eigenfunctions and eigenvectors of Tk may be approximated via the eigenvectors e(mat)i and eigenvalues \u03bb (mat) i of 2 k(X,X), as\n\u03c6 (X) i :=\n\u221a m/\u03bb\n(mat) i k(\u00b7, X)e (mat) i\n\u03bb (X) i := \u03bb (mat) i /m.\nThese approximations may be used for k\u0303, as in (Flaxman et al., 2017), as well as our V(k,\u2126, \u00b5)."}, {"heading": "6. Experiments", "text": ""}, {"heading": "6.1. Setup", "text": "Evaluation We use two metrics. The `2 Error is the squared difference to the ground truth w.r.t. the Lebesgue measure: \u222b x\u2208\u2126(\u03bb(x) \u2212 \u03bbtrue(x))\n2 dx. The test log likelihood is the logarithm of (1) at an independent test sample (one sample being a set of points, i.e. a sample from the process), which we summarise by averaging over a finite number of test sets (for real data where the ground truth intensity is unknown) and otherwise (if we have the ground truth) by the analytical expression EX\u223cPP(\u03bb) [ log pX\u223cPP(\u03bb\u0302)(X)\n] =\u222b\nx\u2208\u2126\n( \u03bb(x) log \u03bb\u0302(x)\u2212 \u03bb\u0302(x) ) dx,\nwhere PP (\u03bb) is the process with intensity \u03bb (see the supplementary subsection A.1). This evaluation metric is novel in this context, yet more accurate and computationally cheaper than the sampling of e.g. (Adams et al., 2009).\nDecision Theory The above metrics are functions of a single estimated intensity. In all cases we use the predictive mean intensity for evaluation. We demonstrate in subsection A.2 of the supplementary material that this is optimal\nfor the expected test log likelihood evaluation (the `2 error cases is similar as is trivial to show).\nAlgorithms We compare our new Laplace Bayesian Point Process (LBPP) with two covariances: the cosine kernel of subsection 5.1 with fixed m = 2 and hyperparameters a and b (LBPP-Cos), and the Gaussian kernel k(x, z) = \u03b32 exp(|x\u2212 y|2 /(2\u03b22)) with the method of subsection 5.2 (LBPP-G). We compared with the Variational Bayesian Point Process (VBPP) (Lloyd et al., 2015) using the same Gaussian kernel. LBPP-G and VBPP use a regular grid for X (of subsection 5.2) and the inducing points, respectively. To compare timing we vary the number of basis functions, i.e. the number of grid points for LBPP-G and VBPP, and cosine terms for LBPP-Cos. We include the baseline kernel smoothing with edge correction (KS+EC) method (Diggle, 1985; Lloyd et al., 2015). All\ninference is performed with maximum marginal likelihood, except for KS+EC where we maximise the leave one out metric described in (Lloyd et al., 2015)."}, {"heading": "6.2. 1D Toy Examples", "text": "We drew five toy intensities, \u03bb0, \u03bb2, . . . , \u03bb4 as 12f 2 where f was sampled from the GP of Gaussian covariance (defined above) with \u03b3 = 5 and \u03b2 = 0.5. Figure 1 depicts \u03bb0 \u2014 see the caption for a description. The remaining test functions are shown in figure 6 of the supplementary material."}, {"heading": "6.2.1. MODEL SELECTION", "text": "As the marginal likelihood log p(X|\u2126, k) is a key advantage of our method over the non-probabilistic approach of Flaxman et al. (2017), we investigated its efficacy for model selection. Figure 3 plots log p(X|\u2126, k) against our two error metrics, both rescaled to [0, 1] for effective visualisation, based on a single training sample per test function. We observe a strong relationship, with larger values of log p(X|\u2126, k) generally corresponding to lower error. This demonstrates the practical utility of both the marginal likelihood itself, and our Laplace approximation to it."}, {"heading": "6.2.2. EVALUATION", "text": "We sampled 100 training sets from each of our five toy functions. Figure 4 shows our evaluation metrics along with the fitting time as a function of the number of basis functions. For visualisation all metrics (including fit time) are scaled to [0, 1] by dividing by the maximum for the given test function, over data replicates and algorithms. LBBP-G and and VBPP achieve the best performance, but our LBPP-G is two orders of magnitude faster. Our KS+EC implementation follows the methodology of Lloyd et al. (2015): we fit the kernel density bandwidth using average leave one out log likelihood. This involves a quadratic number of log p.d.f. of the truncated normal calculations,\nand log-sum-exp calculations, both of which involve large time constants, but are asymptotically superior to the other methods we considered. LBBP-Cos is slightly inferior in terms of expected test log likelihood, which is expected due to the toy functions having been sampled according to the same Gaussian kernel of LBPP-G and VBPP (as well as the density estimator of KS+EC)."}, {"heading": "6.3. Real Data", "text": "We compared the methods on three real world datasets,\n\u2022 coal: 190 points in one temporal dimension, indicating the time of fatal coal mining accidents in the United Kingdom, from 1851 to 1962 (Collins, 2013);\n\u2022 redwood: 195 California redwood tree locations from a square sampling region (Ripley, 1977);\n\u2022 cav: 138 caveolae locations from a square sampling region of muscle fiber (Davison & Hinkley, 2013)."}, {"heading": "6.4. Computational Speed", "text": "Similarly to subsubsection 6.2.2 we evaluate the fitting speed and statistical performance vs. number of basis functions \u2014 see figure 5. We omit the `2 error as the ground truth is unknown. Instead we generate 100 test problems by each time randomly assigning each original datum to either the training or the testing set with equal probability. Again we observe similar predictive performance of LBPP and VBPP, but with much faster fit times for our LBPP. Interestingly LBPP-Cos slightly outperform LBPP-G."}, {"heading": "6.5. 2D California Redwood Dataset", "text": "We conclude by further investigating the redwood dataset. Once again we employed the ML-II procedure to determine a and b, fixing m = 2, for the covariance function of subsection 5.1, using the lowest 32 cosine frequencies in each\ndimension for a total of N = 322 basis functions in the expansion (16). For ease of visualisation we also fixed a = b.\nFigure 2 plots the results, including a decomposition of the log marginal likelihood log p(X|\u2126, k) and a visualisation of the predictive mean. The mean function strongly resembles the result presented by Adams et al. (2009), where computationally expensive MCMC was employed.\nThe decomposition of the marginal likelihood on the left of figure 2 provides insight into the role of the individual terms in (12) and (13) which make up log p(X|\u2126, k). In particular, the term V(k,\u2126, \u00b5) from (13) acts as a regulariser, guarding against over-fitting, and balancing against the data term h of (12)."}, {"heading": "7. Conclusion", "text": "We have discussed the permanental process, which places a Gaussian Process prior over the square root of the inten-\nsity function of the Poisson process, and derived the equations required for empirical Bayes under a Laplace posterior approximation. Our analysis provides 1) an alternative derivation and probabilistic generalization of (Flaxman et al., 2017), and 2) an efficient and easier to implement alternative which does not rely on inducing inputs (but rather reproducing kernel Hilbert space theory), to the related Bayesian approach of Lloyd et al. (2015). This further demonstrates, in a new way, the mathematical convenience and practical utility of the permanental process formulation (in comparison with say log Gaussian Cox processes)."}, {"heading": "Acknowledgements", "text": "Thanks to Young Lee, Kar Wai Lim and Cheng Soon Ong for useful discussions. Adrian is supported by the Australian Research Council (ARC) via a Discovery Early Career Researcher Award (DE-120102873)."}], "year": 2017, "references": [{"title": "Tractable nonparametric Bayesian inference in Poisson processes with gaussian process intensities", "authors": ["Adams", "Ryan P", "Murray", "Iain", "MacKay", "David J. C"], "year": 2009}, {"title": "Spatial point processes and their applications", "authors": ["Baddeley", "Adrian"], "venue": "Lecture Notes in Mathematics: Stochastic Geometry,", "year": 2007}, {"title": "The Inside-Outside Algorithm", "authors": ["Collins", "Michael"], "venue": "Columbia University Lecture Notes,", "year": 2013}, {"title": "Financial modelling with jump processes", "authors": ["Cont", "Rama", "Tankov", "Peter"], "venue": "Financial mathematics series. Chapman and Hall/CRC,", "year": 2004}, {"title": "Fast Gaussian Process Methods for Point Process Intensity Estimation", "authors": ["J.P. Cunningham", "K.V. Shenoy", "M. Sahani"], "venue": "In Proceedings of the 25th International Conference on Machine Learning (ICML),", "year": 2008}, {"title": "Bootstrap Methods and Their Application", "authors": ["A.C. Davison", "D.V. Hinkley"], "year": 2013}, {"title": "A kernel method for smoothing point process data", "authors": ["Diggle", "Peter"], "venue": "Applied Statistics,", "year": 1985}, {"title": "Spatial and spatio-temporal log-Gaussian Cox processes: Extending the geostatistical paradigm", "authors": ["P.J. Diggle", "P. Moraga", "B. Rowlingson", "B.M. Taylor"], "venue": "Statistical Science,", "year": 2013}, {"title": "Fast Kronecker Inference in Gaussian Processes with non-Gaussian Likelihoods", "authors": ["S. Flaxman", "A.G. Wilson", "D.B. Neill", "H. Nickisch", "A.J. Smola"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML),", "year": 2015}, {"title": "A toolbox for fitting complex spatial point process models using integrated nested Laplace approximation (INLA)", "authors": ["J.B. Illian", "S.H. S\u00f8rbye", "H. Rue"], "venue": "The Annals of Applied Statistics,", "year": 2012}, {"title": "Some results on Tchebycheffian spline functions", "authors": ["G. Kimeldorf", "G. Wahba"], "venue": "Journal of Mathematical Analysis and Appl.,", "year": 1971}, {"title": "Scalable nonparametric bayesian inference on point processes with gaussian processes", "authors": ["Kom Samo", "Y.-L", "S. Roberts"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML),", "year": 2015}, {"title": "Simulation of nonhomogeneous Poisson processes by thinning", "authors": ["P.A.W. Lewis", "G.S. Shedler"], "venue": "Naval Res. Logistics Quart,", "year": 1979}, {"title": "Latent point process allocation", "authors": ["Lloyd", "Chris M", "Gunter", "Tom", "Osborne", "Michael A", "Roberts", "Stephen J", "Nickson"], "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,", "year": 2016}, {"title": "Variational inference for gaussian process modulated poisson processes", "authors": ["C.M. Lloyd", "T. Gunter", "M.A. Osborne", "S.J. Roberts"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML),", "year": 2015}, {"title": "Text Classification using String Kernels", "authors": ["Lodhi", "Huma", "Saunders", "Craig", "Shawe-Taylor", "John", "Cristianini", "Nello", "Watkins", "Chris"], "venue": "Journal of Machine Learning Research,", "year": 2002}, {"title": "The permanental process", "authors": ["McCullagh", "Peter", "M\u00f8ller", "Jesper"], "venue": "Advances in Applied Probability,", "year": 2006}, {"title": "Functions of positive and negative type, and their connection with the theory of integral equations", "authors": ["J. Mercer"], "venue": "Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences,", "year": 1909}, {"title": "Log gaussian cox processes", "authors": ["J M\u00f8ller", "A Syversveen", "R. Waagepetersen"], "venue": "Scandanavian Journal of Statistics,", "year": 1998}, {"title": "\u00dcber die praktische aufl\u00f6sung von linearen integralgleichungen mit anwendungen auf randwertaufgaben der potentialtheorie", "authors": ["Nystr\u00f6m", "Evert Johannes"], "venue": "Commentationes physico-mathematicae,", "year": 1928}, {"title": "Gaussian Processes for Machine Learning. Adaptive Computation and Machine Learning", "authors": ["C.E. Rasmussen", "Williams", "C.K.I"], "year": 2006}, {"title": "Asymptotic properties of estimators for the parameters of spatial inhomogeneous poisson point processes", "authors": ["Rathbun", "Stephen L", "Cressie", "Noel"], "venue": "Advances in Applied Probability,", "year": 1994}, {"title": "Modeling Spatial Patterns", "authors": ["B.D. Ripley"], "venue": "Journal of the Royal Statistical Society Series B Statistical Methodolology,", "year": 1977}, {"title": "Approximate bayesian inference for latent gaussian models by using integrated nested laplace approximations", "authors": ["Rue", "H\u00e5vard", "Martino", "Sara", "Chopin", "Nicolas"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "year": 2009}, {"title": "A generalized representer theorem", "authors": ["Sch\u00f6lkopf", "Bernhard", "Herbrich", "Ralf", "Smola", "Alex J"], "venue": "In Proc. of the 14th Annual Conf. on Computational Learning Theory,", "year": 2001}, {"title": "Random point fields associated with certain fredholm determinants ii: Fermion shifts and their ergodic and gibbs properties", "authors": ["Shirai", "Tomoyuki", "Takahashi", "Yoichiro"], "venue": "Ann. Probab., 31(3):1533\u20131564,", "year": 2003}, {"title": "Understanding Gaussian Process Regression Using the Equivalent Kernel, pp. 211\u2013228", "authors": ["Sollich", "Peter", "Williams", "Christopher K. I"], "year": 2005}, {"title": "Partial differential equations in physics. Pure and applied mathematics", "authors": ["Sommerfeld", "Arnold", "Straus", "Ernst Gabor"], "year": 1949}, {"title": "Computing a family of reproducing kernels for statistical applications", "authors": ["Thomas-Agnan", "Christine"], "venue": "Numerical Algorithms,", "year": 1996}, {"title": "Variational learning of inducing variables in sparse gaussian processes", "authors": ["Titsias", "Michalis K"], "venue": "Artificial Intelligence and Statistics", "year": 2009}, {"title": "Spline Models for Observational Data", "authors": ["G. Wahba"], "venue": "Series in Applied Math.,", "year": 1990}], "id": "SP:5ec110420e5e2afcd2714334305a0707a98485d4", "authors": [{"name": "Christian J. Walder", "affiliations": []}, {"name": "Adrian N. Bishop", "affiliations": []}], "abstractText": "The Cox process is a stochastic process which generalises the Poisson process by letting the underlying intensity function itself be a stochastic process. In this paper we present a fast Bayesian inference scheme for the permanental process, a Cox process under which the square root of the intensity is a Gaussian process. In particular we exploit connections with reproducing kernel Hilbert spaces, to derive efficient approximate Bayesian inference algorithms based on the Laplace approximation to the predictive distribution and marginal likelihood. We obtain a simple algorithm which we apply to toy and real-world problems, obtaining orders of magnitude speed improvements over previous work.", "title": "Fast Bayesian Intensity Estimation for the Permanental Process"}