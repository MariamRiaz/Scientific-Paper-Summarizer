0	42	Consider automated systems that are used for determining credit ratings, setting insurance policy rates, or helping in hiring decisions about individuals.
1	74	We would like such decisions to not take into account factors such as the gender or the race of the individual, or any other factor which we deem to be irrelevant to the decision.
2	9	We refer to such irrelevant factors as protected attributes.
3	12	The naive solution of not including protected attributes in the features to a Machine Learning system is insufficient: other features may be highly correlated with—and thus predictive of—the protected attributes (Pedreshi et al., 2008).
6	13	In this paper we are interested in languagebased features.
7	49	It is well established that textual information can be predictive of age, race, gender, and many other social factors of the author (Koppel et al., 2002; Burger et al., 2011; Nguyen et al., 2013; Weren et al., 2014; Verhoeven and Daelemans, 2014; Rangel et al., 2016; Verhoeven et al., 2016; Blodgett et al., 2016), or even the audience of the text (Voigt et al., 2018).
18	12	Following (Ganin and Lempitsky, 2015; Xie et al., 2017), we structure f as an encoder h(x) that maps x into a representation vector hx, and a classifier c(h(x)) that is used for predicting y based on hx.
34	38	The classifier c and the adversarial adv are both multi-layer perceptrons with one hidden layer, sharing the same hidden layer size and activation function (tanh).2
36	10	While our motivating example used prediction tasks for credit rating, insurance rates or hiring decisions, to the best of our knowledge there are no publicly available datasets for these sensitive tasks that meet our criteria.
37	31	We thus opted to use much less sensitive main-tasks, for which we can obtain the needed data.
42	23	Main Tasks: Sentiment and Mention-detection Both tasks can be derived automatically from twitter data.
62	20	We say that the protected attribute has leaked if an attacker manages to predict the protected attribute with better than 50% accuracy, which is always the probability of that attribute (P (Z) = 0.5).
66	11	This provides an upper bound on the protected attribute leakage for the main tasks results.
87	20	Here, we demonstrate leakage using a popular encoder trained for emotion detection: the DeepMoji encoder (Felbo et al., 2017) trained to predict the most suitable emoji usage for a sentence (one of 64 in total), based on 1.2 billion tweets.
90	18	We encode the sentences of the different protected attributes using the DeepMoji encoder and train three different attackers to predict race, gender and age.
95	21	We repeat the experiments in Table 2 with an adversarial component (Ganin and Lempitsky, 2015) as described in Section 2, in order to actively remove the protected attribute information from the encoded representation during training.
97	48	Figure 2 shows the main task and adversary prediction accuracies on the development set as training progresses, for the Sentiment/Race pair.
98	28	After an initial peak in task prediction accuracy, the adversary prediction drops and starts to fluctuate around chance level (50%), as desired, along with a drop in main task accuracy as well.
101	10	However, training the attacker network on the resulting encoder vectors reveals a different story.
130	24	To summarize, while all methods are effective to some extent, it appears that (a) no method and parameter setting performs equally well across the different setups; and (b) no method succeeds in completely preventing the leakage of the protected attributes.
134	44	However, using an ensemble of 5 adversaries does manage to reduce the leakage, but it is still far from a satisfying result.
138	14	The encoder consists of two components: (1) Embedding Matrix and (2) an RNN.
139	12	Therefore, the leakage can be caused due to one of them (or due to their combination).
151	13	Leakage via Embeddings Even though we found out the RNN is much more responsible to the leakage then the Embedding, those still contribute to the leakage and are easier to inspect.
157	18	We discard words appearing in both groups, and associate each word with its training set frequency.
167	35	Even though when trained directly to predict these attributes without the adversarial setup, the training accuracies are much higher, a substantial amount of signal is still left, even in the training data.
185	102	We show that demographic information leaks into intermediate representations of neural networks trained on text data.
186	28	Systems that train on text data and do not want to condition on demographic information must take active steps against accidental conditioning.
187	61	Our experiments suggest that: (1) Adversarial training is effective for mitigating protected attribute leakage, but, when dealing with text data, may fail to remove it completely.
188	39	(2) When using the adversarial training method, the adversary score during training cannot be trusted, and must be verified with an externallytrained attacker, preferably on unseen data.
