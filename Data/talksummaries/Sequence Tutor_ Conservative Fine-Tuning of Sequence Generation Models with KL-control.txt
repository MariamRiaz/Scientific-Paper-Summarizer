0	98	The approach of training sequence generation models using likelihood maximization suffers from known failure modes, and it is notoriously difficult to ensure multi-step generated sequences have coherent global structure.
1	83	For example, long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997) networks trained to predict the next character in sequences of text may produce text that has correct spelling, punctuation, and even a semblance of grammar, but the generated text shifts so rapidly from topic to topic, that it is almost completely nonsensical (see (Graves, 2013) for an example).
18	36	The first, music generation, is a difficult problem in which the aesthetic beauty of generated sequences cannot be fully captured in a known reward function, but in which models trained purely on data cannot produce well-structured sequences.
69	19	The separation between the trained sequence model and the tuning method is important, as it prevents RL training from overwriting the original policy.
71	57	An LSTM trained on data supplies the initial weights for three networks in the model: a recurrent Q-network and target Q-network, and a Reward RNN.
74	17	The state of the environment consists of all of the tokens generated so far, i.e. st = {a1, a2, ...at−1}.
77	31	The simplest and most naı̈ve way to incorporate information about the prior policy is to directly augment the taskspecific rewards with the output of the Reward RNN.
87	25	KL control (Todorov, 2007; Kappen et al., 2012; Rawlik et al., 2012) is a branch of stochastic optimal control (SOC) (Stengel, 1986), which studies an RL, or control, problem in which the agent tries maximizing its task reward while minimizing deviation from a prior policy.
88	57	For our purposes, we treat a trained MLE sequence model as the prior policy, and thus the objective is to train a new policy, or sequence model, to maximize some rewards while keeping close to the original MLE model.
92	36	(6) We express q(τ) in terms of a parametrized recurrent policy πθ(at|st), i.e. q(τ) = ∏T t=1 πθ(at|st) where st = {a1, a2, ..., at−1}, indicates that the system is nonMarkovian.
100	31	The value function V π(st) can be recursively expressed in terms of Ψπ , V π(st) = Eπ[Ψπ(st, at)] + H[π(.|st)] (9) = Eπ[Ψπ(st, at)− log π(at|st)] (10) Fixing Ψ(st, at) = Ψπ(st, at) and constraining π to be a probability distribution, the optimal greedy policy update π∗ can be derived, along with the corresponding optimal value function, π∗(at|st) ∝ eΨ(st,at) (11) V ∗(st) = log ∑ at eΨ(st,at) (12) Given Eq.
101	61	8 and 12, the following Bellman optimality equation for generalized Ψ function is derived.
107	21	Unlike Ψ learning, which directly builds knowledge about the prior policy into the Ψ function, theG-function does not give the policy directly but instead needs to be dynamically mixed with the prior policy probabilities.
115	30	Model evaluation is performed every 100,000 training epochs, by generating 100 sequences and assessing the average rT and log p(a|s).
117	72	Music compositions adhere to relatively well-defined structural rules, making music an interesting sequence generation challenge.
118	51	For example, music theory tells that groups of notes belong to keys, chords follow progressions, and songs have consistent structures made up of musical phrases.
123	23	The rules comprising the music-specific reward function rT (a, s) encourage melodies to: stay in key, start with the tonic note, resolve melodic leaps, have a unique maximum and minimum note, prefer harmonious intervals, play motifs and repeat them, have a low autocorrelation at a lag of 1, 2, and 3 beats, and avoid excessively repeating notes.
138	22	The results above demonstrate that the application of RL is able to correct almost all of the targeted “bad behaviors” of the MLE RNN, while improving performance on the desired metrics.
147	17	Since each model is initialized with the weights of the trained MLE RNN, we see that as the models quickly learn to adhere to the music theory constraints, log p(a|s) falls from its initial point.
148	26	For the RL only model, log p(a|s) reaches an average of -3.65, which is equivalent to an average p(a|s) of approximately 0.026, or essentially a random policy over the 38 actions with respect to the distribution defined by the Reward RNN.
149	34	Figure 2a shows that each of our models (Q, Ψ, and G) attain higher log p(a|s) values than this baseline, indicating they have maintained information about the data distribution, even over 3,000,000 training steps.
153	117	The question remains whether the RL-tutored models actually produce more pleasing melodies.
172	28	As a follow-on experiment, we tested the effectiveness of Sequence Tutor for generating a higher yield of synthetically accessible drug-like molecules.
175	36	Using this character encoding, it is straightforward to train an MLE RNN to generate sequences of SMILES characters; we trained such a model using the same settings as described above for the melody MLE RNN.
176	19	However, only about a third of the molecules generated using this simple approach are actually valid SMILES encodings.
177	18	Further, this approach does not directly optimize for metrics of molecule or drug quality.
178	21	These metrics include: a) the water-octanol partition coefficient (logP), which is important in assessing the druglikeness of a molecule; b) synthetic accessibility (SA) (Ertl & Schuffenhauer, 2009), a score from 1-10 that is lower if the molecule is easier to synthesize; and c) Quantitative Estimation of Drug-likeness (QED) (Bickerton et al., 2012), a more subjective measure of drug-likeness based on abstract ideas of medicinal aesthetics.
192	23	We have derived a novel sequence learning framework which uses RL to correct properties of sequences generated by an RNN, while maintaining information learned from MLE training on data, and ensuring the diversity of generated samples.
196	26	We believe the Sequence Tutor approach of using RL to refine RNN models could be promising for a number of applications, including the reduction of bias in deep learning models.
197	41	While manually writing a domain-specific reward function may seem unappealing, that approach is limited by the quality of the data that can be collected, and besides, even state-of-the-art sequence models often fail to learn all the aspects of high-level structure (van den Oord et al., 2016; Graves, 2013).
