1	27	The goal of K-Means clustering is to find a set of k cluster centers for a dataset such that the sum of squared distances of each point to its closest cluster center is minimized.
2	55	While it is known that k-means clustering is an NP hard optimization problem even for k = 2 (Dasgupta, 2008), in practice a local search heuristic due to Lloyd (Lloyd, 1982) is widely used for solving K-means clustering problem.
3	18	Lloyd’s iterative algorithm begins with k arbitrary “cluster centers”, and in each iteration, each point is assigned to the nearest cluster center, and each cluster center is recomputed as the center of mass of all points assigned to it.
6	24	K-means clustering is typically performed on a data matrix A ∈ Rn×d, consisting of n data points each having d attributes/features and per iteration computational cost of Lloyd’s algorithm is O(nkd).
7	22	In recent years there has been a series of work towards reducing this computational cost and speeding up k-means clustering computation.
8	16	Most of these works can broadly be classified into three categories.
14	11	Additionally, random feature selection method reduces data dimensionality from d to Ω(k log k/ 2) resulting in (1 + ) approximation of the optimal k-means objective function (Boutsidis et al., 2015; Cohen et al., 2015).
19	47	In fact, the dot product between a cluster center µ and all n data points can be computed by a simple matrix-vector multiplication: Aµ.
20	48	If the data matrix A is significantly sparse (i.e., number of non-zero entries is reasonable small) the above matrix vector multiplication can be performed reasonably fast.
21	15	A key question that is not addressed in the literature is, To what extent the data matrix A can be made sparse without significantly affecting optimal k-means clustering objective?
23	17	Note that such a sparsification scheme can be extremely useful in practice.
25	28	However, it may seem at first that for many real world high dimensional datasets that are very sparse to begin with, such as text datasets represented in “bag of word” format, such sparsification scheme may not be useful.
26	11	But, note that instead of directly working with high dimensional data, typically a random projection step is often applied first to reduce data dimensionality since it is known that optimal k-means clustering solution of this randomly projected dataset results in approximately optimal k-means clustering objective of the original high dimensional dataset (Boutsidis et al., 2010; 2015; Cohen et al., 2015; Liu et al., 2017).
27	14	Unfortunately, such a random projection step results in a dense projected data matrix.
28	23	Interestingly, our sparsification method can now be applied on this projected dense data matrix to reap further computational benefit in addition to the computational benefit already achieved by random projection step (see Figure 1).
30	35	To the best of our knowledge, this is the first result that quantifies how random matrix sparsification affects k-means clustering.
50	30	, Ck} such that points that are close to each other belong to the same cluster and points that are far from each other belong to to different clusters.
54	57	The k-means objective function given in equation 1 can now be represented in the matrix notation as, ‖A−XCX>CA‖2F = n∑ j=1 ‖aj −µC(aj)‖ 2 2 (2) By construction, the columns of XC have disjoint support and are orthonormal vectors and XCX>C is an orthogonal projection matrix of rank k. Let S be the set of all possible rank k cluster projection matrices of the form XCX>C .
56	38	Any cluster indicator matrix Xγ is called an γapproximation for the k-means clustering problem (γ ≥ 1) for data matrix A if it satisfies, ‖A−XγX>γA‖2F ≤ γ min XCX>C ∈S ‖A−XCX>CA‖2F = γ‖A−XCoptX>CoptA‖ 2 F
60	30	A fundamental result of random matrix theory is that, as long as N is a random matrix whose entries are zero mean, independent random variables with bounded variance, no low dimensional subspace accommodates N well, i.e., ‖Nm‖2 and ‖Nm‖F are small for small m. In fact, optimal rank m approximation to Ã approximates A nearly as well as Am as long as ‖Am‖ ‖Nm‖ (for both Frobenious and spectral norm) and the quantity ‖Nm‖ bounds the influence that N may exert on the optimal rank m approximation to Ã.
62	17	In random matrix sparsificaion using uniform sampling scheme, p fraction of entries of A are set to zero to obtain a sparse Ã.
69	16	Ã(i, j) = { A(i, j)/pij with probability pij 0 otherwise (4) Such non-uniform sampling scheme yields greater sparsification when entry magnitudes vary, without increasing error bound of Theorem 1.
86	11	It turns out that for appropriate choice of m = m(k, 1), the best rankm approximation of A, namely Am, constructed by the m largest SVD structure form a rank k projection-cost preserving sketch for A with error 1. is a rank k projection-cost preserving sketch of A ∈ Rn×d, with error 0 ≤ ≤ 1 if, for all rank k orthogonal projection matrices P ∈ Rn×n it holds that (1− )‖A−PA‖2F ≤ ‖B − PB‖2F + c ≤ (1 + )‖A − PA‖2F for some fixed nonnegative constant c that may depend on A and B but is independent of P. Similarly, Ãm is a rank k projection-cost preserving sketch for Ã.
87	22	We show that optimal k-means solution of Ã is close to optimal k-mean solution of A in two steps.
89	26	In particular, we show that an optimal k-means solution of Ã is also (1 +O( 1)) optimal for Ãm.
95	27	For any set S of rank k cluster projection matrices, let P̃∗ = argminP∈S ‖Ã − PÃ‖2F and P̃∗m = argminP∈S ‖Ãm−PÃm‖2F .
99	19	Now we show that optimal cluster indicator matrix obtained by solving k-means clustering problem on Ãm results in close to optimal k-means objective of A.
100	22	Our goal is to show that ‖A−P̃∗mA‖2F is close to ‖A−P∗A‖2F .
106	12	Then Ã contains O ( n 22 + d(log n)4 ) non-zero entries in expectation and with probability at least (1− 1/n19 log3 n), ‖A− Ãm‖F ≤ ‖A−Am‖F + 3 √ 2m 1/4‖A‖F We tailor the above result to show that under mild conditions Ãm approximates Am reasonably well.
107	29	Fix any k that satisfies ∑k/ 3 i=1 σ 2 i (A) ≤ 12 ∑ρ i=1 σ 2 i (A), and let m = dk/ 3e.
