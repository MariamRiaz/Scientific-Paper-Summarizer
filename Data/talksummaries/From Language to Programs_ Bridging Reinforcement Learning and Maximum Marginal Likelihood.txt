0	37	We are interested in learning a semantic parser that maps natural language utterances into executable programs (e.g., logical forms).
2	10	We would like to learn from indirect supervision, where each training example is only labeled with the correct output (e.g. a target world state), but not the program that produced that out- z* z'0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 p(z') = 10-4 p(z*) = 10-6 red yellow hasHat blue hasShirt leftOf move move1hasShirt put (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Liang et al., 2017).
3	11	The process of constructing a program can be formulated as a sequential decision-making process, where feedback is only received at the end of the sequence when the completed program is executed.
15	27	We apply RANDOMER to train a new neural semantic parser, which outputs programs in a stackbased programming language.
21	50	, uM ), the task is to generate a program that manipulates the world state according to the utterances.
22	63	Each 1 https://nlp.stanford.edu/projects/scone utterance um describes a single action that transforms the world state wm−1 into a new world state wm.
33	15	Such programs encourage the model to learn an incorrect mapping from language to program operations: e.g., the spurious program in Figure 1 would cause the model to learn that “man in the yellow hat” maps to hasShirt(red).
34	57	In this dataset, utterances often reference objects in different ways (e.g. a person can be referenced by shirt color, hat color, or position).
39	48	We formulate program generation as a sequence prediction problem.
45	23	from left to right using a neural encoder-decoder model with attention (Bahdanau et al., 2015).
50	10	If an action token (e.g., move) is generated, the model increments the utterance pointer m. The process terminates when all M utterances are processed.
51	8	The final probability of generating a particular program z = (z1, .
53	16	The final utterance embedding is the concatenation em = [hF|um|;h B 1 ].
60	9	We also consider a new approach which leverages our ability to incrementally execute programs using a stack.
67	7	, zT ), and then receives a reward at the end of the episode: R(z) = 1 if z executes to y and 0 otherwise (dependence on x and y has been omitted from the notation).
68	19	We focus on policy gradient methods, in which a stochastic policy function is trained to maximize the expected reward.
73	51	This implies the marginal likelihood: pθ(y | x) = ∑ z p(y | z) pθ(z | x).
74	31	(3) Note that since the execution of z is deterministic, pθ(y | z) = 1 if z executes to y and 0 otherwise.
78	44	Hence the only difference between the two formulations is that in RL we optimize the sum of expected rewards (2), whereas in MML we optimize the product (5).3
83	257	Taking a step in the direction of∇ log pθ(z | x) upweights the probability of z, so we can heuristically think of the gradient as attempting to upweight each reward-earning program z by a gradient weight q(z).
84	15	In Subsection 5.2, we argue why qMML is better at guarding against spurious programs, and propose an even better alternative.
87	7	In the policy gradient literature, Monte Carlo integration (MC) is the typical approximation strategy.
100	34	We will refer to this as beam search MML (BS-MML).
101	149	In both policy gradient and MML, we think of the procedure used to produce the set of programs S as an exploration strategy which searches for programs that produce reward.
102	122	One advantage of numerical integration is that it allows us to decouple the exploration strategy from the gradient weights assigned to each program.
103	126	In this section, we illustrate why spurious programs are problematic for the most commonly used methods in RL (REINFORCE) and MML (beam search MML).
104	36	We describe two key problems and propose a solution to each, based on insights gained from our comparison of RL and MML in Section 4.
111	18	From this, we see that exploration is sensitive to initial conditions: the rich get richer, and the poor get poorer.
112	12	Since there are often thousands of spurious programs and only a few correct programs, spurious programs are usually found first.
119	91	Solution: randomized beam search One solution to biased exploration is to simply rely less on the untrustworthy current policy.
