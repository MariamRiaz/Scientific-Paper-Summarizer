2	12	Interpretability used to be for its own sake; now, due to safety-critical applications such as self-driving cars and tumor diagnosis, it is no longer satisfying to have a black box that is unaccountable for its decisions.
3	39	The demand for explainable artificial intelligence (XAI) (Gunning, 2017) – human interpretable explanations of model decisions – has driven the development of visualization techniques, including image synthesis via activation maximization (Simonyan et al., 2013; Johnson et al., 2016; Nguyen et al., 2016) and backpropagation-based visualizations (Simonyan et al., 2013; Zeiler & Fergus, 2014; Springenberg et al., 2014; Shrikumar et al., 2017; Kindermans et al., 2017).
4	40	The basic idea of backpropagation-based visualizations is to highlight class-relevant pixels by propagating the network output back to the input image space.
7	10	Despite its simplicity, the results of saliency map are normally very noisy which makes the interpretation difficult.
13	20	Despite their good visual quality, the question of how they are actually related to the decision-making has remained largely unexplored.
14	16	Do the pretty visualizations actually tell us reliably about what the network is doing internally?
18	18	The most commonly used explanation for these visualizations is to approximate the neural networks with a linear function (Simonyan et al., 2013; Kindermans et al., 2017), where the derivative of output with respect to input image is just the weight vector of the model.
22	10	The linear model explanation thus cannot answer questions regarding why GBP and DeconvNet outperform saliency map in terms of visual quality whereas they are less class-sensitive than saliency map, as both of them reduce to saliency map in a linear model.
32	19	Also, denote by R (l) i the top gradient before activation, i.e., gradient of the output score with respect to o (l) i and denote by T (l) i the (modified) gradient after activation, i.e., gradient of the output score with respect to y (l) i .
35	17	More importantly, it should also reveal how the neural networks make decisions.
37	33	Without loss of generality, the visualizations are obtained by choosing one of the class logits (i.e. the unnormalized class probability output right before the softmax function) as the output score to be taken derivative with respect to the input image.
38	24	For the visual quality, saliency map is very noisy while DeconvNet and GBP produce human-interpretable visualizations with a subtle difference: DeconvNet unexpectedly produces some kind of texture-like pattern, and GBP is cleaner with some background information filtered out.
39	16	For the class-sensitivity, saliency map changes greatly for different class logits while DeconvNet and GBP are almost invariant to which class logit we choose.
41	22	In the next section, we will explain these empirical behaviors and discuss the reason why GBP and DeconvNet differ greatly from saliency map.
42	11	We first analyze the backpropagation-based methods in a three-layer CNN with random Gaussian weights, which is then extended to more complicated models such as CNNs with max-pooling and deep CNNs.
52	20	The following lemma provides the formula for backpropagation-based visualizations in a random three-layer CNN.
59	12	The above theorem shows that after introducing the backward ReLU, the input image can be approximately recovered by GBP in a random three-layer CNN, regardless of the class label.
62	14	(5) builds on an assumption that the number of filters N is sufficiently large, a key question is: How many filters are needed to guarantee an accurate recovery?
64	20	As an upper bound, it reveals that the number of convolutional filters needed heavily depends on the filter size p. As the filter size intrinsically determined by the local connections in CNNs is usually small, we could use a mild number of convolutional filters to recover the input image.
66	10	This strongly suggests that GBP visualizations are human-interpretable in most of the CNNs, and thus the local connections property is another key factor underlying crisp visualizations.
110	11	(8) for GBP and DeconvNet (with max-pooling) in the trained CNN can be approximated as sk(x) = 1 Zk ∂õ(fc1) ∂x · h(V̂ (fc1) ·,k ) = 1 Zk M ∑ m=1 ∂õ (fc1) m ∂x · h(V̂ (fc1) m,k ) (a) ≈ Fconv(x) (9) where (a) follows from setting the normalization coefficient to be Zk = 1 ∑ M m=1 h(V̂ (fc1) m,k ) .
115	19	To verify our theoretical analysis, we conduct a series of experiments on a three-layer CNN, a three-layer fully- connected network (FCN) and a VGG-16 net.
126	45	To further highlight the impact of local connections in the visual quality of GBP, we vary the number of filters N in the CNN and the number of hidden neurons Nh in the FCN, respectively, while keep other parameters fixed.
129	37	We can see that as the number of filters N increases (resp.
132	12	Therefore, it confirms that the local connections in the CNN really contribute to the good visual quality of GBP.
133	36	To show the impact of the max-pooling in backpropagationbased visualizations, we then add a max-pooling layer in the above random three-layer CNN while keeping other parameters fixed, and the results are given in Figure 6 (top row).
140	9	To quantitatively describe how backpropagation-based visualizations change with respect to different class logits, we also provide the average l2 distance statistics as shown in Figure 7.
143	15	As we can see, the average l2 distance of saliency map is much larger than that of both GBP and DeconvNet in either a random VGG or a trained VGG, which clearly demonstrates that saliency map is class-sensitive but GBP and DeconvNet are not.
146	42	Adversarial attack provides another way of directly testing whether visualizations are class-sensitive or doing image recovery.
147	50	The class-sensitive visualizations should change drastically as both the predicted class label and ReLU states of intermediate layers have changed, while the visualizations doing image recovery should change little as only a tiny adversarial perturbation is added into the input image.
