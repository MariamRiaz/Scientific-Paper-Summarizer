{"sections": [{"heading": "1. Introduction", "text": "An accurate identification of the system dynamics is the first and very crucial step to many modern control methods. Although reinforcement learning also allows model-free search for optimal policies, it is known to be less efficient and difficult to analyze. Therefore, classical control engineers employ system identification techniques to obtain parametric model descriptions of dynamical systems from observation data, e.g., in the linear case ARX and ARMAX models. The identification focuses on model selection, i.e., finding the model structure and the corresponding set of parameters. But often this set of model candidates is difficult to find, especially for complex, possibly non-deterministic, systems (Ljung, 1998). Therefore, the need for data-driven models has emerged recently as control engineering is increasingly applied in areas without analytic description of the dynamical system. We consider the following two application scenarios: First, assume a set of trajectories for a\n1Chair of Information-oriented Control, Technical University of Munich, Munich, Germany. Correspondence to: Jonas Umlauft <jonas.umlauft@tum.de>.\nProceedings of the 34th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nrobotic task is given through human demonstrations, e.g., object grasping. The goal is to represent the motion with a dynamical system. To ensure the reproduction terminates at the desired final point (object to grasp), we introduce the stability constraint. Second, consider a dynamical system which is known to be stable, e.g., a pendulum which rests in hanging position. The goal is to identify the dynamics precisely without further physical insights.\nBayesian non-parametric methods, more particularly Gaussian Processes (GPs) where successfully employed by Kocijan et al. (2005) and Wang et al. (2005) for system identification. Other approaches focus on learning switching linear systems (Fox et al., 2009) or employ an EM algorithm (Ghahramani & Roweis, 1999) for nonlinear systems. However, these approaches neglect the prior assumption that the dynamical system is stable, which becomes crucial when the learned model is used as a generative process such as in movement generation for robotics (Ijspeert et al., 2002). If stability is not considered during learning, the identified model suffers from spurious attractors which are not part of the true dynamics or instability.\nOnly little work has merged the extensive knowledge on stability theory from control engineering with the powerful data-driven approaches for system identification: For example Boots et al. (2008) and Chiuso & Pillonetto (2010) take stability constraints for learning dynamical systems into account but are limited to linear systems. The work by Khansari-Zadeh & Billard (2011) ensures stability of the system by constraining the optimization of a Gaussian Mixture Model (GMM) to stability conditions derived from Lyapunov methods. The work by Paraschos et al. (2013) relies on a phase variable to ensure stability, which makes the approach time-dependent and therefore less robust. Control Lyapunov functions are used by KhansariZadeh & Billard (2014) to ensure global stability for the learned system. These approaches partially employ probabilistic models (GP, GMM), but limit the analysis to the deterministic part by only considering the mean regression. By discarding the true underlying probability distribution, information regarding reliability of the model provided by the data is lost. This leads to overconfident conclusions regarding performance or safety on the real system.\nTherefore, this work proposes a framework for learning\nprobabilistic nonlinear dynamical systems from observation, which takes the prior assumption of stability into account. The required stochastic stability conditions of the discrete-time Markov processes are derived from Lyapunov theory. We provide simulation results to validate the proposed approach and compare it to previously mentioned methods for identifying dynamical systems."}, {"heading": "2. Problem Formulation", "text": "We consider an autonomous, dynamical, discrete-time system with continuous-valued state xk \u2208 X = Rd. The state evolves according to an unknown stochastic process1\nxk+1 = f\u0302(xk, \u03c9\u0302k), (1)\nwith initial value x0 \u2208 X and \u03c9\u0302k is a random variable from the probability space (\u2126,F ,P) with sample space \u2126, the corresponding \u03c3-algebra F and the probability measure P . Since xk \u2208 X is fixed at each step, (1) describes a state dependent distribution overxk+1. A realization of \u03c9\u0302k \u2208 \u2126, is drawn at every time step, yielding a realization of the next step. As the distribution for xk+1 only depends on the state at time step k, f\u0302 is a Markov process, denoted by {xk}. We assume that consecutive measurements of the state are available, thus N data pairs are given in the trainings set D = {x\u0304n, x\u0304n+1}Nn=1. Based on these measurements, we model the unknown dynamics f\u0302 including the distribution \u03c9\u0302k using the prior knowledge, that the stochastic process (1) converges to the origin xk = 0. The model consists of the mapping f\u03c8 and a encoding of the random variable \u03c9 defined by a finite parameter vector \u03c8 \u2208 \u03a8. As the model f\u03c8 must best possibly explain the data D, the problem is formulated as constrained likelihood maximization\n\u03c8\u2217 = arg max \u03c8 N\u2211 n=1 logP ( x\u0304n+1|x\u0304n,f\u03c8 ) , (2a) s.t. {xk} converges to the origin for k \u2192\u221e. (2b)\nAs different stochastic stability concepts exist, the convergence in (2b) is defined as convergence with probability one (w.p.1) (Kushner, 1971):\nDefinition 1 (Convergence w.p.1). {xk} converges to the origin w.p.1 if, for each > 0, \u2016xk\u2016 \u2265 only finitely often.\n1Notation: Bold symbols denote vectors or multivariate functions, capital letters matrices and Ip the p \u00d7 p identity matrix. A 0 denotes positive definiteness of the matrix A, E [\u00b7] the expected value, V [\u00b7] the variance of a random variable and C [\u00b7, \u00b7] the covariance between two random variables, where C [a] = C [a,a]. X\u0304 denotes a realization of the random variable X . Imitating Matlab indexing, A(:,i) denotes the i-th column, A(j,:) the j-th row and A(1:2,i) the first and second element in the i-the column of A. The i-th entry of the vector xk is denoted xk,i.\nThis also implies the following type of convergence, which might be more intuitive to the reader. Definition 2 (Convergence in probability). The chain {xk} converges to the origin in probability if P(\u2016xk\u2016 \u2265 )\u2192 0, for each > 0.\nWe do not consider any control input here, thus the identification takes place for the closed-loop system for a existing controller or an uncontrolled system."}, {"heading": "3. Stability Conditions for the Model", "text": ""}, {"heading": "3.1. The Model", "text": "Consider the state-dependent coefficient form of f\u03c8\nxk+1 = A(xk)xk, (3)\nwhere, for a fixedxk,A is a random variable from the probability space (\u2126A,FA,PA) with the sample space \u2126A \u2286 Rd\u00d7d. The probability density function of A is specified by the vector \u03b8 \u2208 \u0398, which is state dependent through \u03b8\u03c8 : X \u2192 \u0398. This mapping is itself parametrized by a vector \u03c8. At each step, a realization of A, denoted by A\u0304, is drawn and multiplied by the state xk to proceed by one step. This is visualized in Figure 1 along with the two-layer model structure: The first layer maps current state xk \u2208 X onto the parameter \u03b8 \u2208 \u0398, denoted by \u03b8\u03c8 : X \u2192 \u0398. The mapping is parametrized by \u03c8. The second layer is the probability distribution onAwhich assigns to each element in the sample space \u2126A a probability based on \u03b8.\nTo illustrate this multilayer design, we give a brief example in the scalar case d = 1: Assume A(xk) follows, for a given xk, a Gaussian distribution A \u223c N (\u00b5,\u03c3). Therefore, the parameter vector is \u03b8N = [\u00b5 \u03c3]\u1d40 with \u00b5 \u2208 R,\u03c3 \u2208 R+, thus \u0398N \u2282 R\u00d7 R+. The dependency of these parameters on the current state xk is expressed in \u03b8\n\u03c8 N , e.g.,[\n\u00b5(xk) \u03c3(xk)\n] = \u03b8\u03c8N (xk) = [ wxk zx2k ] , (4)\nwhere linear dependency of the mean on the state and a quadratic relation between variance and the state is assumed. The parameters defining \u03b8\u03c8N here are \u03c8 = [w z]\n\u1d40. Generally, the first layer \u03b8\u03c8 : X \u2192 \u0398 can be any state of the art parametric regression method which is parametrized by \u03c8. For layer two, any probability distribution with a fixed set of parameters is applicable for A.\nLeaving the stochastic aspect aside, model (3) is the statedependent coefficient (SDC) form which is reached by factorizing a nonlinear system into a linear-like structure. It was shown, that for a any continuous differentiable function f with f(0) = 0, their exists a matrix-valued function A(x) such that f(x) = A(x)x, see Cimen (2008). Thus, the SDC form is not limiting the expressive power of our model. It also reflects the setup of many real-world system, e.g., consider an actuator whose output is generally noisy and the magnitude of the noise is dependent on the temperature. By modeling the temperature as a state, the model (3) allows to capture this varying precision of the actuator.\nThe structure of the model (3) combines two important criteria. First, it provides more flexibility than a linear system with random parameters, so it encodes also nonlinear dynamics.Second, it is simple enough to allow a quadratic Lyapunov function analysis and therefore the derivation of analytic constraints for convergence as needed for the optimization in (2)."}, {"heading": "3.2. Stability Analysis", "text": "For approaching the problem as formulated in Section 2, an analytic condition for the constraint in the optimization problem (2b), given that f\u03c8 is of the form (3), is needed. The literature on stability criteria for dynamical systems is very rich and for nonlinear systems Lyapunov type methods are often used. They are based on the following idea: If there is a function representing the \u201denergy\u201d in the system (called Lyapunov function) which constantly decreases over time, the state will converge to a \u201dzero energy\u201d state, the origin. More precise, the Lyapunov function must be positive definite and it must be strictly decreasing over time, except in the origin. Using the stochastic discretetime version of Lyapunov methods and the Borel-Cantelli Lemma leads to the following conditions for exponential stability (which implies convergence w.p.1 as defined in Definition 1)\nTheorem 1 (Exponential Stability, (Kushner, 1971)). Given a positive definite function V (xk) \u2265 0 for which\nE [V (xk+1)|xk]\u2212V (xk) \u2264 \u2212\u03b1V (xk), \u2200xk \u2208 X \\ 0, (5)\nfor some \u03b1 > 0 then\nE [V (xk+m)|xk] \u2264 (1\u2212 \u03b1)mV (xk) and (6) V (xk+m)\u2192 0 for m\u2192\u221e (w.p.1). (7)\nFor the class of systems in (3) a quadratic function V (xk) is a proper Lyapunov function to derive sufficient stability constraints for arbitrary distributions on A as shown in the following proposition: Proposition 1 (Stability of the model (3)). Consider a stochastic process generated from (3) where in each step a realization of A is drawn from sample space \u2126A \u2282 Rd\u00d7d. The process is globally exponentially stable at xk = 0 if there exists a P 0 such that E [A\u1d40(xk)]P E [A(xk)] +Q\u2212 (1\u2212 \u03b1)P 0, \u2200xk \u2208 X ,\n(8)\nfor some \u03b1 > 0, where Q is defined as Q(i,j)(xk) = \u2211 l P(l,:) C [ A(:,i)(xk),A(l,j)(xk) ] , (9)\nfor any x0 \u2208 X .\nProof. Considering a quadratic Lyapunov function V (xk) = x \u1d40 kPxk with P 0, the inequality from Theorem 1 in (5) is given as\nE [ x\u1d40k+1Pxk+1|xk+1 ] \u2212x\u1d40kPxk \u2264 \u2212\u03b1x \u1d40 kPxk,\nwhich yields for the stochastic2 process xk+1 = A(xk)xk\nx\u1d40k E [A \u1d40]P E [A]xk + Tr (P C [Axk])\u2212 (10) \u2212 (1\u2212 \u03b1)x\u1d40kPxk \u2264 0, \u2200xk \u2208 X . Now, an expression for the trace is derived as follows\nTr (P C [Axk]) = Tr ( P C [\u2211 i A(:,i)xk,i ])\n= Tr P\u2211 i,j xk,ixk,j C [ A(:,i),A(:,j) ] = \u2211 i,j,l P(l,:) C [ A(:,i),A(l,j) ] xk,ixk,j , = x \u1d40 kQxk\nwhere definition of Q in (9) was substituted. Using this simplification, (10) is rewritten as\nx\u1d40k\n( \u1d40 E [A]P E [A] +Q\u2212 (1\u2212 \u03b1)P ) xk \u2264 0,\nwhich must hold for \u2200xk \u2208 X . To ensure this, the matrix E [A]\u1d40 P E [A] +Q\u2212 (1\u2212\u03b1)P must be negative semidefinite, which concludes the proof.\n2The xk dependency of the random process A as been dropped for notational convenience.\nThe interpretation of Proposition 1 is analogue to the linear deterministic case xk+1 = Axk which is stable if there exists a matrix P for which A\u1d40PA \u2212 P \u227a 0: In the nonlinear case in (3) the negative definiteness must be fulfilled for A(xk), \u2200xk \u2208 X . The probabilistic nature of the system (3) in addition requires \u201da buffer\u201d, which here is Q. The deterministic case is reconstructed if A has zero variance. The scalar case, considered in the following remark, also allows an intuitive insight to the Proposition 1: There is a trade-off between the magnitude of the expected value and the variance of A as follows:\nRemark 1. In the scalar case3, i.e. d = 1 in (3), with Q = P V [A(xk)] condition (8) simplifies for any P > 0 to\nE [A(xk)]2 + V [A(xk)] \u2264 1\u2212 \u03b1, \u2200xk \u2208 X . (11)"}, {"heading": "4. Stable Learning with Various Distributions", "text": "Our learning framework consists of three major steps:\n1. Chose any probability distribution for the random variable A in (3) which is given by a fixed set of parameters \u03b8 \u2208 \u0398 and whose first two moments are available. It is assumed that subset \u0398\u2217 \u2286 \u0398 for which (8) is fulfilled is non-empty, thus \u0398\u2217 6= \u2205.\n2. Chose any parametric regression method to represent the mapping \u03b8\u03c8 : X \u2192 \u0398. The parameters of this mapping are denoted by \u03c8 \u2208 \u03a8. The set of all \u03c8 for which all xk \u2208 X map to \u0398\u2217 is denoted by \u03a8\u2217.\n3. The likelihood maximization under constraints\n\u03c8\u2217 = arg max \u03c8\u2208\u03a8\u2217 N\u2211 n=1 logP ( xn+1|xn,\u03b8\u03c8 ) , (12)\nis solved, where\u03c8 \u2208 \u03a8\u2217 is equivalent to constraint (8) with P 0 and \u03b1 > 0.\nThe optimization (12) is a general constrained nonlinear program in a rather high dimensional space (depending on number of parameters of the regression method in step 2). However, independent of the optimality, the model f\u03c8\u2217 of the form (3) is exponentially stable, thus any sample path of the system converges. For computational simplicity, we focus on two types of distribution which naturally fulfill the constraints as explained in the next sections."}, {"heading": "4.1. Stability with Beta Distribution", "text": "For certain choices of distributions, constraint (8) is fulfilled for all possible parameter \u03b8, thus \u0398\u2217 = \u0398, which\n3Even though A,Q,P are scalars here, we keep them capitalized for notational consistency.\nmakes the optimization unconstrained. One example of such a distribution is the Beta distribution as given in the following corollary. Corollary 1. The scalar system xk+1 = A(xk)xk where A(xk) = \u03ba(A\u0303(xk)\u2212 \u03b7) with Beta distributed A\u0303(xk) \u223c B(a(xk), b(xk)) and \u03ba = 2, \u03b7 = 0.5 with state dependent parameters [a(xk) b(xk)]\u1d40 = \u03b8 \u03c8 B (xk), with any \u03b8\u03c8B : X \u2192 \u0398B = R2+ is exponentially stable.\nProof. Applying the affine transformation to mean and variance leads to4\nE [A(xk)] = \u03ba ( E [ A\u0303(xk) ] \u2212\u03b7 ) = \u03ba ( a a+ b \u2212\u03b7 ) ,\nV [A(xk)] = \u03ba2 V [ A\u0303(xk) ] = \u03ba2\nab\n(a+ b)2(a+ b+ 1) .\nCondition (11) is rewritten to\n(\u03ba(E [A(xk)]\u2212\u03b7))2 + \u03ba2 V [A(xk)] \u2264 1\u2212 \u03b1, \u2200xk \u2208 X , where the best possible choice for \u03b7 minimizes (E [A(xk)]\u2212\u03b7)2, because it leaves the largest possible range for \u03ba. As E [A] is in the interval ]0, 1[ the minimization is achieved with the choice \u03b7 = 12 . Then, condition (11), divided by \u03ba2 on both sides, evaluates to\na2\n(a+ b)2 \u2212 a a+ b + 1 4 +\nab\n(a+ b)2(a+ b+ 1) =\n\u2212 ab (a+ b)(a+ b+ 1)\ufe38 \ufe37\ufe37 \ufe38\n0... 14\n+ 1 4 \u2264 1\u2212 \u03b1 \u03ba2 .\nAs \u03b1 > 0 can be chosen arbitrarily small this condition holds for every |\u03ba| \u2264 2. Hence, according to Theorem 1 the system xk+1 = A(xk)xk is exponentially stable.\nTo ensure maximal flexibility of the model, \u03ba = 2 is set for further considerations. This leads to the conclusion that \u0398\u2217B = \u0398B = R2+. Therefore, in the optimization, no constraints on \u03c8 must be considered, thus \u03a8 = \u03a8\u2217."}, {"heading": "4.2. Stability with Dirichlet Distribution", "text": "ConstructingA from a Dirichlet distribution also allows for unconstrained optimization as it also leads to stable behavior as shown in the following corollary. Corollary 2. The d-dimensional system xk+1 = A(xk)xk where each row of A(xk) consists of the first d elements of a d+ 1 dimensional Dirichlet distributed vector, thus,\nA(i,:) = a (i) (1:d), with a (i) \u223c D ( \u03b8 \u03c8i D (xk) ) , \u2200i = 1 . . . d,\nwith any \u03b8\u03c8iD : X \u2192 \u0398D,\u2200i, is asymptotically stable w.p.1. 4The state dependency of a, b is dropped for notational convenience.\nProof. By construction the sample space ofA, \u2126A contains only elements for which\nA(i,j) > 0, A(i,j) < 1, \u2200i, j = 1 . . . d and d\u2211\nj=1\nA(i,j) < 1, \u2200i = 1 . . . d. (13)\nConsider now a realization of A(xk) denoted by A\u0304 and since the following statements hold for any realization in the sample space, we omit writing \u2200A\u0304 \u2208 \u2126A. It follows\nd\u2211 j=1 A\u0304(i,j) < 1 \u2200i \u21d2 max i=1:d d\u2211 j=1 A\u0304(i,j) < 1\n\u21d2 \u2016A\u0304\u2016\u221e = max i=1:d d\u2211 j=1 |A\u0304(i,j)| < 1,\nwhere the last inequality holds because all elements of A\u0304 are strictly positive and \u2016A\u0304\u2016\u221e denotes the Maximum Absolute Row Sum Norm. Consider now M consecutive realizations A\u0304(i) with i = 1, . . . ,M . For the maximum norm of state in the M -th step holds\n\u2016xk+M\u2016\u221e = \u2225\u2225\u2225\u2225\u2225 M\u220f m=1 A\u0304(m)xk \u2225\u2225\u2225\u2225\u2225 \u221e \u2264 M\u220f m=1 \u2225\u2225\u2225A\u0304(m)\u2225\u2225\u2225 \u221e \u2016xk\u2016\u221e\n\u2264 (\nmax m \u2225\u2225\u2225A\u0304(m)\u2225\u2225\u2225 \u221e )M \u2016xk\u2016\u221e M\u2192\u221e\u2212\u2212\u2212\u2212\u2192 0,\nwhere the submultiplicativity property of induced matrices (Horn & Johnson, 2013) is used. As convergence towards the origin holds for each element in the sample space, the system is stable with probability one. Therefore, the parameter space is unrestricted \u0398\u2217D = \u0398D = R d+1 + .\nRemark 2. Note that this approach only allows to represent the special class of positive systems. Nevertheless, positive systems play an important role in control engineering for modeling the evolution of strictly positive quantities as shown in (Farina & Rinaldi, 2011).\nRemark 3. An affine transformation, as shown for Beta distribution is not possible here because absolute values are taken in the row sum. Therefore, from\n\u2211d j=1 |A\u0304(i,j)| < 1 one cannot conclude\u2211d\nj=1 |\u03ba(A\u0304(i,j) \u2212 0.5)| < 1 for any \u03ba > 1."}, {"heading": "5. Simulations", "text": ""}, {"heading": "5.1. Setup", "text": "We validate our approach, labeled LeSSS (for Learning Stable Stochastic Systems), using synthetic and human motion data and the simulation of a chemical reactor. For the Beta distribution, Gaussian Mixture Regression (GMR)\nis used for the mapping from the state to the parameters \u03b8B : X \u2192 \u0398B. Thus, the parameter vector \u03c8, is the concatenation of the prior \u03c0l, the means\u00b5l and the covariances \u03a3l for l = 1, . . . L . The code (based on Calinon (2009)) includes k-means clustering initialization and a transformation of \u03a3l and \u03c0l to make it an unconstrained optimization. To evaluate the likelihood function for each training point {x\u0304n+1, x\u0304n}, the Beta distribution parameters are computed [an bn]\n\u1d40 = \u03b8\u03c8B (x\u0304n) using GMR. Then, the log likelihood of A\u0304n = x\u0304n+1/x\u0304n given the parameters [an bn] is evaluated using the density function of the Beta distribution. As all possible parameter \u03b8B = [a b]\u1d40 \u2208 R2+ lead to stability, finding \u03c8\u2217 is an unconstrained optimization problem.\nFor the Dirichlet distribution, the mapping from the state to the parameters \u03b8\u03c8D : X \u2192 \u0398D uses a nearest neighbor approach for computational simplicity. The 2d = 4 closest data points are considered for fitting the training parameters of the Dirichlet distribution locally. Then a training point is placed at the center of these four points. At reproduction, the closest such training point and its Dirichlet parameters are taken for regression. This does not necessarily maximize the likelihood, but shows accurate results for reproduction. We compare the following models from literature regarding reproduction precision and convergence properties:\n\u2022 The approach introduced by Boots et al. (2008) learns stable linear dynamical system (stable LDS) from data. It constraints the search of the deterministic dynamic matrix A to ensure the stability\nof xk+1 = Axk.\n\u2022 Gaussian Process Dynamical Models (GPDM) (Wang et al., 2005) represent dynamical system in the general form xk+1 = f(xk), with Gaussian Process f \u223c GP(0, k(xk,x\u2032k)). We employ a zero prior mean function and a squared exponential kernel. The hyperparameters of the kernel are optimized using the likelihood as described by Rasmussen & Williams (2006). In reproduction, this method can either be used in deterministic setting by only taking the posterior mean prediction \u00b5GP(xk) thus xk+1 = \u00b5GP(xk) or the stochastic setting xk+1 \u223c N (\u00b5GP(xk), \u03a3GP(xk)), where \u03a3GP(xk) is the posterior variance. GPDMs are bounded (Beckers & Hirche, 2016a;b) but not stable.\n\u2022 The Stable Estimator of Dynamical Systems (SEDS) as introduced by Khansari-Zadeh & Billard (2011) constraints the likelihood optimization of GMR parameters to a class of mean stable dynamical timecontinuous systems. The GMR maps from current state x to the time derivative x\u0307. It focuses on deterministic systems by only considering stability criteria for the mean prediction of the GMR, \u00b5GMM(x), while ignoring the stochastic nature of GMMs, (its variance prediction \u03a3GMM). We also run this method in a stochastic setting, where x\u0307 \u223c N (\u00b5GMM(x), \u03a3GMM(x)). For our simulations, five mixtures are employed.\nBefore starting the comparison to existing approaches, LeSSS is demonstrated on a synthetic dataset."}, {"heading": "5.2. Simulation 1: Synthetic Data", "text": "For the first simulation, the task is to identify the stable nonlinear stochastic system given by\nxk+1 = A(xk)xk, (14) where A(xk) \u223c B ( (xk \u2212 5)2, (xk + 5)2 ) .\nThe learning algorithm is given 100 training points {x\u0304n, x\u0304n+1}100n=1 equally spaced in the state space interval [\u22128, 8] which are drawn from the state dependent Beta distribution (14). Here L = 3 was chosen for the number of mixtures in the GMR for the mapping \u03b8\u03c8B : X \u2192 \u0398B. Figure 2 compares the mean and variance of the original system (14) to the one inferred by our model. It clearly shows that the model offers sufficient flexibility to reconstruct the original system. Note: It is also possible to verify the parameter functions a(xk), b(xk) as given in (14), but we directly look at the mean and variance functions as there exists a unique mapping and it is more intuitive for interpretation. It must be omitted, that the data was generated from the same model which the algorithm is learning. This\nexplains the good fitting, but is of course not often the case in practical application. Therefore, we continue with a real world dataset in the following."}, {"heading": "5.3. Simulation 2: Human Motion Data", "text": "For the next simulation, we use the data set for lettershaped motions provided by Khansari-Zadeh & Billard (2011). The 225 trainings points of 3 trajectories of the two dimensional Z-shaped motion are projected on the y-axis. The GMR for \u03b8\u03c8B : X \u2192 \u0398B is trained with two mixtures. Figure 3 shows the training data along with the fit of the mean and variance functions. The mean function shows a smoothed estimate of the training data. The model identifies properly that the training data has higher variability (around xk = 0) and captures this in its variance function.\nFigure 4 compares the reproduction of the models stable LDS, GPDM, SEDS and LeSSS if taking the deterministic (mean) output of each model (all starting from the same\ninitial point). The stable LDS approach leads to a converging trajectory, but fails to capture the complexity of the dynamic (as the true dynamic is nonlinear). The GPDM converges to a spurious attractor at x \u2248 \u22129.3 which is undesired but not surprising. SEDS and LeSSS both lead to asymptotic stable reproductions of the movement. Since the data does not contain the full state (due to the projection on the y-axis), it is not possible to reproduce the movement precisely with a dynamical system model.\nFigure 5 compares the reproduction of the three stochastic dynamical models GPDM, SEDS and LeSSS based on three sample paths drawn from each model. The GPDM again converges to the spurious attractor. SEDS clearly shows that convergence of the mean is not sufficient for converging trajectories of a stochastic system, as the drawn sample paths are strongly oscillating around the origin without tendency to converge. In the stochastic case only LeSSS generates converging trajectories.\nFigure 6 shows an example for the human motion imitation in the 2D case on a different training data set. It shows the deterministic trajectory and 5 sample path realizations, where all of them show high reproduction precision and convergence to the orign."}, {"heading": "5.4. Simulation 3: Chemical Reactor Simulation", "text": "For the last validation, we utilize simulated data from a simplified chemical reactor (Einarsson, 1998). The closedloop reactor is modeled by a piecewise affine system with two states: the fluid level x1 and the temperature x2. Both states are physically positive quantities, therefore the approach in Section 4.2 is suitable. The switching between different dynamic matrices is state dependent and occurs at x1 = 3 and x2 = 50, which corresponds to a discrete change of the control inputs. The training data consists of 8 trajectories of 15 steps each, which are pairwise initialized at the 4 different regions of the dynamics and perturbed\nwith white noise with \u03c3 = 0.01 for both states.\nFigure 7 shows the training data along with the reproduction using stable LDS, GPDM, SEDS and LeSSS in the deterministic setting. The initial points in the test case were set close to the one in the training data. The stable LDS is not capable to capture the varying behavior in the different regions of the piecewise affine system and therefore fails in accuracy of the reproduction. GPDM leads again to convergence outside the origin, which is undesirable. SEDS and LeSSS are both converging as it is enforced by design. Figure 8 shows that similar to the 1D case GPDM and SEDS fail to converge in the stochastic case while LeSSS is stable in all sample paths. Table 1 compares the methods with regard to the reproduction precision quantitatively. It shows that LeSSS outperforms other methods in this measure while providing the necessary guarantees regarding convergence."}, {"heading": "5.5. Discussion", "text": "The simulations show that LeSSS is powerful enough to represent various nonlinear dynamics, while capturing the probabilistic nature of the process. The incorporation of the prior knowledge on goal convergence ensures that the learned model is stable in probability.\nThe computational complexity for learning the parameters of the model using interior-point methods, is mainly determined by the employed mapping in the first layer \u03b8\u03c8 . The\ncomputation times on a i5 CPU 2.30GHz, 2 Cores and 8GB RAM are given for Simulation 2 and 3 in Table 2. Since the GPDM, SEDS and LeSSS all solve non-convex optimization problems, their commutation times are in the same order of magnitude. The linear model has advantage here. Regarding the scalability with more training points, the parameter fitting performs similarly to other approaches requiring likelihood computation since this is the major factor. However, the scalability strongly depends on the employed distribution and the mapping in the first layer \u03b8\u03c8 .\nThis work only deals with system with a single equilibrium point, but could be extended to system with more complex attractor dynamics. However, further knowledge is required because - in addition to the position of all equilibrium points - their regions of attraction must be known."}, {"heading": "6. Conclusion", "text": "This work proposes a framework for learning nonlinear stable stochastic dynamical systems from data. We introduce a flexible model, which builds on the state-dependent coefficient form and derive exponential stability conditions based on stochastic Lyapunov methods. The criteria is applicable to various probability distributions, while we focus to investigate the application to Beta and Dirichlet distributions. Simulation results verify sufficient flexibility of the model and the correct identification of the system\u2019s uncertainty. In comparison to existing approaches it showed advantages in reproduction precision and convergence properties on human motion data and simulated data from a real system."}, {"heading": "Acknowledgment", "text": "The research leading to these results has received funding from the European Research Council under the European Union Seventh Framework Program (FP7/2007-2013) ERC Starting Grant \u201dControl based on Human Models (conhumo)\u201d agreement number 337654. We also would like to thank the reviewers for very constructive feedback."}], "year": 2017, "references": [{"title": "Equilibrium distributions and stability analysis of Gaussian process state space models", "authors": ["Beckers", "Thomas", "Hirche", "Sandra"], "venue": "In Conference on Decision and Control (CDC),", "year": 2016}, {"title": "Stability of Gaussian process state space models", "authors": ["Beckers", "Thomas", "Hirche", "Sandra"], "venue": "In European Control Conference,", "year": 2016}, {"title": "A constraint generation approach to learning stable linear dynamical systems", "authors": ["Boots", "Byron", "Gordon", "Geoffrey J", "Siddiqi", "Sajid M"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "year": 2008}, {"title": "Robot Programming by Demonstration: A Probabilistic Approach", "authors": ["Calinon", "Sylvain"], "venue": "EPFL/CRC Press,", "year": 2009}, {"title": "Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors", "authors": ["Chiuso", "Alessandro", "Pillonetto", "Gianluigi"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "year": 2010}, {"title": "State-dependent Riccati equation (SDRE) control: A survey", "authors": ["Cimen", "Tayfun"], "venue": "IFAC Proceedings Volumes,", "year": 2008}, {"title": "On Verification of Switched Systems using Abstractions", "authors": ["Einarsson", "Valur"], "venue": "PhD thesis,", "year": 1998}, {"title": "Positive linear systems: Theory and applications", "authors": ["Farina", "Lorenzo", "Rinaldi", "Sergio"], "venue": "John Wiley&Sons,", "year": 2011}, {"title": "Nonparametric Bayesian learning of switching linear dynamical systems", "authors": ["Fox", "Emily", "Sudderth", "Erik B", "Jordan", "Michael I", "Willsky", "Alan S"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "year": 2009}, {"title": "Learning nonlinear dynamical systems using an EM algorithm", "authors": ["Ghahramani", "Zoubin", "Roweis", "Sam T"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "year": 1999}, {"title": "Matrix analysis", "authors": ["Horn", "Roger A", "Johnson", "Charles R"], "year": 2013}, {"title": "Movement imitation with nonlinear dynamical systems in humanoid robots", "authors": ["Ijspeert", "Auke Jan", "Nakanishi", "Jun", "Schaal", "Stefan"], "venue": "In International Conference on Robotics and Automation (ICRA). IEEE,", "year": 2002}, {"title": "Learning stable nonlinear dynamical systems with Gaussian mixture models", "authors": ["Khansari-Zadeh", "Seyed Mohammad", "Billard", "Aude"], "venue": "IEEE Transactions on Robotics,", "year": 2011}, {"title": "Dynamic systems identification with Gaussian processes", "authors": ["J. Kocijan", "A. Girard", "B. Banko", "R. Murray-Smith"], "venue": "Mathematical and Computer Modelling of Dynamical Systems,", "year": 2005}, {"title": "Introduction to stochastic control", "authors": ["Kushner", "Harold Joseph"], "venue": "Holt, Rinehart and Winston New York,", "year": 1971}, {"title": "System Identification", "authors": ["Ljung", "Lennart"], "venue": "Prentice Hall PTR, NJ,", "year": 1998}, {"title": "Probabilistic movement primitives", "authors": ["Paraschos", "Alexandros", "Daniel", "Christian", "Peters", "Jan", "Neumann", "Gerhard"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "year": 2013}, {"title": "Gaussian Processes for Machine Learning", "authors": ["Rasmussen", "Carl Edward", "Williams", "Christopher KI"], "year": 2006}, {"title": "Gaussian process dynamical models", "authors": ["Wang", "Jack M", "Fleet", "David J", "Hertzmann", "Aaron"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "year": 2005}], "id": "SP:8d3619e5ade1c0d4bca8ea97b27c45370fa6dddc", "authors": [{"name": "Jonas Umlauft", "affiliations": []}, {"name": "Sandra Hirche", "affiliations": []}], "abstractText": "A data-driven identification of dynamical systems requiring only minimal prior knowledge is promising whenever no analytically derived model structure is available, e.g., from first principles in physics. However, meta-knowledge on the system\u2019s behavior is often given and should be exploited: Stability as fundamental property is essential when the model is used for controller design or movement generation. Therefore, this paper proposes a framework for learning stable stochastic systems from data. We focus on identifying a state-dependent coefficient form of the nonlinear stochastic model which is globally asymptotically stable according to probabilistic Lyapunov methods. We compare our approach to other state of the art methods on real-world datasets in terms of flexibility and stability.", "title": "Learning Stable Stochastic Nonlinear Dynamical Systems"}