{"sections": [{"heading": "1 Introduction", "text": "How can the elements from two sets be paired one-to-one to have the largest sum of pairwise utilities? This maximum weighted perfect bipartite matching problem is a classical combinatorial optimization problem in computer science. It can be formulated and efficiently solved in polynomial time as a linear program or using more specialized Hungarian algorithm techniques (Kuhn, 1955). This has made it an\n*Equal contribution 1Department of Computer Science, University of Illinois at Chicago. Correspondence to: Rizal Fathony <rfatho2@uic.edu>, Sima Behpour <sbehpo2@uic.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nattractive formalism for posing a wide range of problems, including recognizing correspondences in similar images (Belongie et al., 2002; Liu et al., 2008; Zhu et al., 2008; Rui et al., 2007), finding word alignments in text (Chan & Ng, 2008), and providing ranked lists of items for information retrieval tasks (Amini et al., 2008).\nMachine learning methods seek to estimate the pairwise utilities of bipartite graphs so that the maximum weighted complete matching is most compatible with the (distribution of) ground truth matchings of training data. When these utilities are learned abstractly, they can be employed to make predictive matchings for test samples. Unfortunately, important measures of incompatibility (e.g., the Hamming loss) are often non-continuous with many local optima in the predictors\u2019 parameter spaces, making direct minimization intractable. Given this difficulty, two natural desiderata for any predictor are:\n\u2022 Efficiency: learning from training data and making predictions must be computed efficiently in (lowdegree) polynomial time; and \u2022 Consistency: the predictor\u2019s training objectives must also minimize the underlying Hamming loss, at least under ideal learning conditions (given the true distribution and fully expressive model parameters).\nExisting methods for learning bipartite matchings fail in one or the other of these desiderata; exponentiated potential fields models (Lafferty et al., 2001; Petterson et al., 2009) are intractable for large sets of items, while maximum margin methods based on the hinge loss surrogate (Taskar et al., 2005a; Tsochantaridis et al., 2005) lack Fisher consistency (Tewari & Bartlett, 2007; Liu, 2007). We discuss these limitations formally in Section 2.\nGiven the deficiencies of the existing methods, we contribute the first approach for learning bipartite matchings that is both computationally efficient and Fisher consistent. Our approach is based on an adversarial formulation for learning (Tops\u00f8e, 1979; Gru\u0308nwald & Dawid, 2004; Asif et al., 2015) that poses prediction-making as a dataconstrained zero-sum game between a player seeking to minimize the expected loss and an adversarial data approximator seeking to maximize the expected loss. We present two approaches for solving the corresponding zero-sum game arising from our formulation: (1) using the double\noracle method of constraint generation to find a sparselysupported equilibrium for the zero-sum game; and (2) decomposing the game\u2019s solution into marginal probabilities and optimizes these marginal probabilities directly to obtain an equilibrium saddle point for the game. We then establish the computational efficiency and consistency of this approach and demonstrate its benefits experimentally."}, {"heading": "2 Previous Inefficiency and Inconsistency", "text": ""}, {"heading": "2.1 Bipartite Matching Task", "text": "Given two sets of elements A and B of equal size (|A| = |B|), a maximum weighted bipartite matching \u03c0 is the one-toone mapping (e.g., Figure 1) from each element in A to each element in B that maximizes the sum of potentials: max\u03c0\u2208\u03a0 \u03c8(\u03c0) = max\u03c0\u2208\u03a0 \u2211 i \u03c8i(\u03c0i). Here \u03c0i \u2208\n[n] := {1, 2, . . . , n} is the entry in B that is matched with the i-th entry of A. The set of possible solutions \u03a0 is simply all permutation of [n]. Many machine learning tasks pose prediction as the solution to this problem, including: word alignment for natural language processing tasks (Taskar et al., 2005b; Pado\u0301 & Lapata, 2006; MacCartney et al., 2008); learning correspondences between images in computer vision applications (Belongie et al., 2002; Dellaert et al., 2003); protein structure analysis in computational biology (Taylor, 2002; Wang et al., 2004); and learning to rank a set of items for information retrieval tasks (Dwork et al., 2001; Le & Smola, 2007). Thus, learning appropriate weights \u03c8i(\u00b7) for bipartite graph matchings is a key problem for many application areas."}, {"heading": "2.2 Performance Evaluation and Fisher Consistency", "text": "Given a predicted permutation, \u03c0\u2032, and the \u201cground truth\u201d permutation, \u03c0, the Hamming loss counts the number of mistaken pairings: lossHam(\u03c0, \u03c0\u2032) = \u2211n i=1 1(\u03c0 \u2032 i 6= \u03c0i), where 1(\u00b7) = 1 if \u00b7 is true and 0 otherwise. When the \u201cground truth\u201d is a distribution over permutations, P (\u03c0), rather than a single permutation, the (set of) Bayes optimal prediction(s) is: argmin\u03c0\u2032 \u2211 \u03c0 P (\u03c0) lossHam(\u03c0, \u03c0\n\u2032). For a predictor to be Fisher consistent, it must provide a Bayes optimal prediction for any possible distribution P (\u03c0) when trained from that exact distribution using the predictor\u2019s most general possible parameterization (e.g., all measurable functions \u03c8 for potential-based models)."}, {"heading": "2.3 Exponential Family Random Field Approach", "text": "A probabilistic approach to learning bipartite graphs uses an exponential family distribution over permutations,\nP\u03c8(\u03c0) = e \u2211n i=1 \u03c8i(\u03c0i)/Z\u03c8 , trained by maximizing training data likelihood. This provides certain statistical consistency guarantees for its marginal probability estimates (Petterson et al., 2009). Specifically, if the potentials \u03c8 are chosen from the space of all measurable functions to maximize the likelihood of the true distribution of permutations P (\u03c0), then P\u03c8(\u03c0) will match the marginal probabilities of the true distribution: \u2200i, j, P\u03c8(\u03c0i = j) = P (\u03c0i = j). This implies Fisher consistency because the MAP estimate under this distribution, which can be obtained as a maximum weighted bipartite matching, is Bayes optimal.\nThe key challenge with this approach is its computational complexity. The normalization term, Z\u03c8 , is the permanent of a matrix defined in terms of exponentiated potential terms: Z\u03c8 = \u2211 \u03c0 \u220fn i=1 e\n\u03c8i(\u03c0i) = perm(M) where Mi,j = e\n\u03c8i(j). For sets of small size (e.g., n = 5), enumerating the permutations is tractable and learning using the exponential random field model incurs a run-time cost that is acceptable in practice (Petterson et al., 2009). However, the matrix permanent computation is a #P-hard problem to compute exactly (Valiant, 1979). Monte Carlo sampling approaches are used instead of permutation enumeration to maximize the data likelihood (Petterson et al., 2009; Volkovs & Zemel, 2012). Though exact samples can be generated efficiently in polynomial time (Huber & Law, 2008), the number of samples needed for reliable likelihood or gradient estimates makes this approach infeasible for applications with even modestly-sized sets of n = 20 elements (Petterson et al., 2009)."}, {"heading": "2.4 Maximum Margin Approach", "text": "Maximum margin methods for structured prediction seek potentials \u03c8 that minimize the training sample hinge loss:\nmin \u03c8 E\u03c0\u223cP\u0303 [ max \u03c0\u2032 {loss(\u03c0, \u03c0\u2032) + \u03c8(\u03c0\u2032)} \u2212 \u03c8(\u03c0) ] , (1)\nwhere P\u0303 is the empirical distribution. Finding the optimal \u03c8 is a convex optimization problem (Boyd & Vandenberghe, 2004) that can generally be tractably solved using constraint generation methods as long as the maximizing assignments can be found efficiently. In the case of permutation learning, finding the permutation \u03c0\u2032 with highest hinge loss reduces to a maximum weighted bipartite matching problem and can therefore be solved efficiently.\nThough computationally efficient, maximum margin approaches for learning to make perfect bipartite matches lack Fisher consistency, which requires the prediction \u03c0\u2217 = argmax\u03c0 \u03c8(\u03c0) resulting from Equation (1) to minimize the expected risk, E\u03c0\u223cP\u0303 [loss(\u03c0, \u03c0\u2032)], for all distributions P\u0303 . We consider a distribution over permutations that is an extension of a counterexample for multiclass classification consistency analysis with no majority label (Liu,\n2007): P (\u03c0 = [1 2 3]) = 0.4;P (\u03c0 = [2 3 1]) = 0.3; and P (\u03c0 = [3 1 2]) = 0.3. The potential function \u03c8i(j) = 1 if i = j and 0 otherwise, provides a Bayes optimal permutation prediction for this distribution and an expected hinge loss of 3.6 = 0.4(3 \u2212 3) + 0.3(3 + 3) + 0.3(3 + 3). However, the expected hinge loss is optimally minimized with a value of 3 when \u03c8i(j) = 0,\u2200i, j, which is indifferent between all permutations and is not Bayes optimal. Thus, Fisher consistency is not guaranteed."}, {"heading": "3 Approach", "text": "To overcome the computational inefficiency of exponential random field methods and the Fisher inconsistency of maximum margin methods, we formulate the task of learning for bipartite matching problems as an adversarial structured prediction task. We present two approaches for efficiently solving the resulting game over permutations."}, {"heading": "3.1 Permutation Mixture Formulation", "text": "The training data for bipartite matching consists of triplets (A,B, \u03c0) where A and B are two sets of nodes with equal size and \u03c0 is the assignment. To simplify the notation, we denote x as the bipartite graph containing the nodes A and B. We also denote \u03c6(x, \u03c0) as a vector that enumerates the joint feature representations based on the bipartite graph x and the matching assignment \u03c0. This joint feature is defined additively over each node assignment, i.e., \u03c6(x, \u03c0) = \u2211n i=1 \u03c6i(x, \u03c0i).\nOur approach seeks a predictor that robustly minimizes the Hamming loss against the worst-case permutation mixture probability that is consistent with the statistics of the training data. In this setting, a predictor makes a probabilistic prediction over the set of all possible assignments (denoted as P\u0302 ). Instead of evaluating the predictor with the empirical distribution, the predictor is pitted against an adversary that also makes a probabilistic prediction (denoted as P\u030c ). The predictor\u2019s objective is to minimize the expected loss function calculated from the predictor\u2019s and adversary\u2019s probabilistic predictions, while the adversary seeks to maximize the loss. The adversary (and only the adversary) is constrained to select a probabilistic prediction that matches the statistical summaries of the empirical training distribution (denoted as P\u0303 ) via moment matching constraints on joint features \u03c6(x, \u03c0). Formally, we write our formulation as:\nmin P\u0302 (\u03c0\u0302|x) max P\u030c (\u03c0\u030c|x) Ex\u223cP\u0303 ;\u03c0\u0302|x\u223cP\u0302 ;\u03c0\u030c|x\u223cP\u030c [loss(\u03c0\u0302, \u03c0\u030c)] s.t. (2)\nEx\u223cP\u0303 ;\u03c0\u030c|x\u223cP\u030c [ n\u2211 i=1 \u03c6i(x, \u03c0\u030ci) ] = E(x,\u03c0)\u223cP\u0303 [ n\u2211 i=1 \u03c6i(x, \u03c0i) ] .\nThis follows a recent line of work for adversarial classification under additive (Asif et al., 2015) and non-additive\n(Wang et al., 2015) loss functions that has been employed for chain-structured prediction (Li et al., 2016), object detection (Behpour et al., 2017), and robust cut learning (Behpour et al., 2018). Using the method of Lagrangian multipliers and strong duality for convex-concave saddle point problems (Von Neumann & Morgenstern, 1945; Sion, 1958), The optimization in Eq. (2) can be equivalently solved in the dual formulation:\nmin \u03b8 Ex,\u03c0\u223cP\u0303 min P\u0302 (\u03c0\u0302|x) max P\u030c (\u03c0\u030c|x) E\u03c0\u0302|x\u223cP\u0302 \u03c0\u030c|x\u223cP\u030c\n[ loss(\u03c0\u0302, \u03c0\u030c)+ (3)\n\u03b8 \u00b7 n\u2211 i=1 (\u03c6i(x, \u03c0\u030ci)\u2212 \u03c6i(x, \u03c0i)) ] ,\nwhere \u03b8 is the Lagrange dual variable for the moment matching constraints. We refer the reader to Appendix A in the supplementary materials for a more detailed explanation of this construction (i.e., the transformation from Eq. (2) to Eq. (3)). In this paper, we use Hamming distance, loss(\u03c0\u0302, \u03c0\u030c) = \u2211n i=1 1(\u03c0\u0302i 6= \u03c0\u030ci), as the loss function.\nTable 1 shows the payoff matrix for the game of size n = 3 with 3! actions (permutations) for the predictor player \u03c0\u0302 and for the adversarial approximation player \u03c0\u030c. Here, we define the difference between the Lagrangian potential of the adversary\u2019s action and the ground truth permutation as \u03b4\u03c0\u030c = \u03c8(\u03c0\u030c)\u2212 \u03c8(\u03c0) = \u03b8 \u00b7 \u2211n i=1 (\u03c6i(x, \u03c0\u030ci)\u2212 \u03c6i(x, \u03c0i)) .\nUnfortunately, the number of permutations, \u03c0, grows factorially (O(n!)) with the number of elements being matched (n). This makes explicit construction of the Lagrangian minimax game intractable for modestly-sized problems."}, {"heading": "3.2 Optimization by Constraint Generation", "text": "Our first approach for taming the factorial computational complexity of explicitly constructing games for matching tasks is a constraint-generation approach known as the double oracle method (McMahan et al., 2003). It obtains the equilibrium solution to the adversarial prediction game without explicitly constructing the entire game matrix (Table 1). Based on the key observation that the equilibrium of the zero-sum game is typically supported by a relatively small number of permutations, it seeks to efficiently uncover this sparse set of permutations for each player.\nAlgorithm 1 Double Oracle Algorithm for Adversarial Bipartite Matching Equilibria. Input: Lagrangian potentials \u03a8(\u00b7); Initial label \u03c0initial Output: The (sparse) Nash equilibrium (S\u030c, S\u0302, P\u0302 , P\u030c )\n1: S\u030c \u2190 S\u0302 \u2190 {\u03c0initial} 2: repeat 3: (P\u0302 , P\u030c , V\u030c )\u2190 solveGame(\u03a8(S\u030c), lossHam(S\u0302, S\u030c)) 4: (\u03c0\u030cnew,Vmax)\u2190argmax\u03c0\u030cE\u03c0\u0302\u223cP\u0302 [lossHam(\u03c0\u0302, \u03c0\u030c)+\u03a8(\u03c0\u030c)] 5: if (V\u030c 6= Vmax) then S\u030c \u2190 S\u030c \u222a \u03c0\u030cnew 6: (P\u0302 , P\u030c , V\u0302 )\u2190 solveGame(\u03a8(S\u030c), lossHam(S\u0302, S\u030c)) 7: (\u03c0\u0302new, Vmin)\u2190 argmin\u03c0\u0302 E\u03c0\u030c\u223cP\u030c [lossHam(\u03c0\u0302, \u03c0\u030c)] 8: if (V\u0302 6= Vmin) then S\u0302 \u2190 S\u0302 \u222a \u03c0\u0302new 9: until V\u030c = Vmax = V\u0302 = Vmin\n10: return (S\u030c, S\u0302, P\u0302 , P\u030c )\nAlgorithm 1 produces this set of \u201cactive\u201d permutations for each player, S\u0302 and S\u030c (subsets of rows and columns in Table 1), and the associated Nash equilibrium (P\u0302 , P\u030c ). Starting from an initial permutation, \u03c0initial (Line 1), it repeatedly obtains the Nash equilibrium solution (P\u0302 , P\u030c ) with value V\u0302 or V\u030c for the zero-sum game defined only by permutations in S\u0302 and S\u030c (Lines 3 and 6). This is efficiently accomplished using a linear program (Von Neumann & Morgenstern, 1945). The algorithm then obtains the other player\u2019s best response to either P\u0302 or P\u030c (Lines 4 and 7) with values Vmax and Vmin using the Kuhn-Munkres (Hungarian) algorithm in O(n3) time for sets of size n. These best responses, \u03c0\u030cnew and \u03c0\u0302new, are added to the set of active permutations (i.e., new rows or columns in the game matrix) if they have better values than the previous equilibrium values (Lines 5 and 8). This is repeated until no game value improvement exists for either player (Line 9), at which point a Nash equilibrium for the full game has been obtained.\nWe solve the convex optimization of Lagrange parameters \u03b8 in Eq. (3) using the results of Algorithm 1. We employ AdaGrad (Duchi et al., 2011) with the gradient calculated as the difference between expected features under the adversary\u2019s distribution and the empirical training data: Ex\u223cP\u0303 ;\u03c0\u030c|x\u223cP\u030c [ \u2211n i=1 \u03c6i(x, \u03c0\u030ci)]\u2212 Ex,\u03c0\u223cP\u0303 [ \u2211n i=1 \u03c6i(x, \u03c0i)].\nIn contrast with SSVM, which compute the hinge loss for each training instance using only a single run of the Hungarian algorithm, our double oracle method must solve this problem repeatedly to find the equilibrium. Though in practice the total number of active permutations is much smaller than the n! possibilities, no formal polynomial bound is known\u2014and, consequentially, the run time of the approach as a whole cannot be characterized as polynomial."}, {"heading": "3.3 Marginal Distribution Formulation", "text": "Our second approach, which significantly improves the efficiency of solving the adversarial bipartite matching game,\nleverages the key insight that all quantities of interest for evaluating the loss and satisfying the constraints depend only on marginal probabilities of the permutation\u2019s value assignments. Based on this, we employ a marginal distribution decomposition of the game.\nWe begin this reformulation by first defining a matrix representation of permutation \u03c0 as Y(\u03c0) \u2208 Rn\u00d7n (or simply Y) where the value of its cell Yi,j is 1 when \u03c0i = j and 0 otherwise. To be a valid complete bipartite matching or permutation, each column and row of Y can only have one entry of 1. For each feature function \u03c6(k)i (x, \u03c0i), we also denote its matrix representation as Xk whose (i, j)-th cell represents the k-th entry of \u03c6i(x, j). For a given distribution of permutations, P (\u03c0), we denote the marginal probabilities of matching i with j as pi,j , P (\u03c0i = j). We let P = \u2211 \u03c0 P (\u03c0)Y(\u03c0) be the predictor\u2019s marginal probability matrix where its (i, j) cell represents P\u0302 (\u03c0\u0302i = j), and similarly let Q be the adversary\u2019s marginal probability matrix (based on P\u030c ), as shown in Table 2.\nThe Birkhoff\u2013von Neumann theorem (Birkhoff, 1946; Von Neumann, 1953) states that the convex hull of the set of n\u00d7 n permutation matrices forms a convex polytope in Rn2 (known as the Birkhoff polytope Bn) in which points are doubly stochastic matrices, i.e., the n\u00d7nmatrices with non-negative elements where each row and column must sum to one. This implies that both marginal probability matrices P and Q are doubly stochastic matrices. In contrast to the space of distributions over permutation of n objects, which grows factorially (O(n!) with n! \u2212 1 free parameters), the size of this marginal matrices grows only quadratically (O(n2) with n2 \u2212 2n free parameters). This provides a significant benefit in terms of the optimization.\nStarting with the minimax over P\u0302 (\u03c0\u0302) and P\u030c (\u03c0\u030c) in the permutation mixture formulation, and using the matrix notation above, we rewrite Eq. (3) as a minimax over marginal probability matrices P and Q with additional constraints that both P and Q are doubly-stochastic matrices, i.e., P \u2265 0 (elementwise), Q \u2265 0, P1 = P>1 = Q1 = Q>1 = 1 where 1 = (1, . . . , 1)>). That is:\nmin \u03b8 EX,Y\u223cP\u0303 minP\u22650 maxQ\u22650 [n\u2212\u3008P,Q\u3009+\u3008Q\u2212Y, \u2211 k \u03b8kXk\u3009]\ns.t. : P1 = P>1 = Q1 = Q>1 = 1, (4)\nwhere \u3008\u00b7, \u00b7\u3009 denotes the Frobenius inner product between two matrices, i.e., \u3008A,B\u3009 = \u2211 i,j Ai,jBi,j ."}, {"heading": "3.3.1 OPTIMIZATION", "text": "We reduce the computational costs of the optimization in Eq. (4) by focusing on optimizing the adversary\u2019s marginal probability Q. By strong duality, we then push the maximization over Q in the formulation above to the outermost level of Eq. (4). Note that the objective above is a non-smooth function (i.e., piece-wise linear). For the purpose of smoothing the objective, we add a small amount of strongly convex prox-functions to both P and Q. We also add a regularization penalty to the parameter \u03b8 to improve the generalizability of our model. We unfold Eq. (4) by replacing the empirical expectation with an average over all training examples, resulting in the following optimization:\nmax Q\u22650 min \u03b8\n1\nm m\u2211 i=1 min Pi\u22650 [ \u3008Qi \u2212Yi, \u2211 k \u03b8kXi,k\u3009 \u2212 \u3008Pi,Qi\u3009\n+ \u00b52 \u2016Pi\u2016 2 F \u2212 \u00b5 2 \u2016Qi\u2016 2 F ] + \u03bb2 \u2016\u03b8\u2016 2 2\ns.t. : Pi1 = P>i 1 = Qi1 = Q > i 1 = 1, \u2200i, (5)\nwhere m is the number of bipartite matching problems in the training set, \u03bb is the regularization penalty parameter, \u00b5 is the smoothing penalty parameter, and \u2016A\u2016F denotes the Frobenius norm of matrixA. The subscript i in Pi,Qi,Xi, and Yi refers to the i-th example in the training set.\nIn the formulation above, given a fixed Q, the inner minimization over \u03b8 and P can then be solved separately. The optimal \u03b8 in the inner minimization admits a closed-form solution, in which the k-th element of \u03b8\u2217 is:\n\u03b8\u2217k = \u2212 1\n\u03bbm m\u2211 i=1 \u3008Qi \u2212Yi,Xi,k\u3009 . (6)\nThe inner minimization over P can be solved independently for each training example. Given the adversary\u2019s marginal probability matrix Qi for the i-th example, the optimal Pi can be formulated as:\nP\u2217i = argmin {Pi\u22650|Pi1=P>i 1=1} \u00b5 2 \u2016Pi\u2016 2 F \u2212 \u3008Pi,Qi\u3009 (7)\n= argmin {Pi\u22650|Pi1=P>i 1=1}\n\u2016Pi \u2212 1\u00b5Qi\u2016 2 F . (8)\nWe can interpret this minimization as projecting the matrix 1 \u00b5Qi to the set of doubly-stochastic matrices. We will discuss our projection technique in the upcoming subsection.\nFor solving the outer optimization over Q with the doublystochastic constraints, we employ a projected QuasiNewton algorithm (Schmidt et al., 2009). Each iteration of the algorithm optimizes the quadratic approximation of the objective function (using limited-memory Quasi-Newton) over the the convex set. In each update step, a projection to the set of doubly-stochastic matrices is needed, akin to the inner minimization of P in Eq. (8).\nThe optimization above provides the adversary\u2019s optimal marginal probability Q\u2217. To achieve our learning goal, we recover \u03b8\u2217 using Eq. (6) computed over the optimal Q\u2217. We use the \u03b8\u2217 that our model learns from this optimization to construct a weighted bipartite graph for making predictions for test examples."}, {"heading": "3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION", "text": "The projection from an arbitrary matrix R to the set of doubly-stochastic matrices can be formulated as:\nmin P\u22650 \u2016P\u2212R\u20162F , s.t. : P1 = P>1 = 1. (9)\nWe employ the alternating direction method of multipliers (ADMM) technique (Douglas & Rachford, 1956; Glowinski & Marroco, 1975; Boyd et al., 2011) to solve the optimization problem above. We divide the doubly-stochastic matrix constraint into two sets of constraints C1 : P1 = 1 and P \u2265 0, andC2 : P>1 = 1 and P \u2265 0. Using this construction, we convert the optimization above into ADMM form as follows:\nmin P,S\n1 2\u2016P\u2212R\u2016 2 F + 1 2\u2016S\u2212R\u2016 2 F + IC1(P) + IC2(S)\ns.t. : P\u2212 S = 0. (10)\nThe augmented Lagrangian for this optimization is:\nL\u03c1(P,S,W) = 12\u2016P\u2212R\u2016 2 F + 1 2\u2016S\u2212R\u2016 2 F + IC1(P)\n+ IC2(S) + \u03c1 2\u2016P\u2212 S + W\u2016 2 F , (11)\nwhere \u03c1 is the ADMM penalty parameter and W is the scaled dual variable. From the augmented Lagrangian, we compute the update for P as:\nPt+1 = argmin P L\u03c1(P,St,Wt) (12)\n= argmin {P\u22650|P1=1}\n1 2\u2016P\u2212R\u2016 2 F + \u03c1 2\u2016P\u2212 S t + Wt\u20162F\n= argmin {P\u22650|P1=1}\n\u2016P\u2212 11+\u03c1 ( R + \u03c1 ( St \u2212Wt )) \u20162F .\nThe minimization above can be interpreted as a projection to the set {P \u2265 0|P1 = 1} which can be realized by projecting to the probability simplex independently for each row of the matrix 11+\u03c1 (R + \u03c1 (S\nt \u2212Wt)). Similarly, the ADMM update for S can also be formulated as a columnwise probability simplex projection. The technique for projecting a point to the probability simplex has been studied previously, e.g., by Duchi et al. (2008). Therefore, our ADMM algorithm consists of the following updates:\nPt+1 = ProjC1 ( 1 1+\u03c1 ( R + \u03c1 ( St \u2212Wt ))) (13)\nSt+1 = ProjC2 ( 1 1+\u03c1 ( R + \u03c1 ( Pt+1 + Wt ))) (14)\nWt+1 = Wt + Pt+1 \u2212 St+1. (15)\nWe run this series of updates until the stopping conditions are met. Our stopping conditions are based on the primal and dual residual optimality as described in Boyd et al. (2011). In our overall algorithm, this ADMM projection algorithm is used both in the projected Quasi-Newton algorithm for optimizing Q (Eq. (5)) and in the inner optimization for minimizing Pi (Eq. (8))."}, {"heading": "3.3.3 CONVERGENCE PROPERTY", "text": "The convergence rate of ADMM is O(log 1 ) thanks to the strong convexity of the objective (Deng & Yin, 2016). Each step inside ADMM is simply a projection to a simplex, hence costing O\u0303(n) computations (Duchi et al., 2008).\nIn terms of optimization on Q, since no explicit rates of convergence are available for the projected Quasi-Newton algorithm (Schmidt et al., 2009) that finely characterize the dependency on the condition numbers, we simply illustrate the \u221a L/\u00b5 log 1 rate using Nesterov\u2019s accelerated gradient algorithm (Nesterov, 2003), where L is the Lipschitz continuous constant of the gradient. In our case, L = 1m2\u03bb \u2211 k \u2211m i=1 \u2016Xi,k\u2016 2 F + 1/\u00b5.\nComparison with Structured SVM (SSVM) Conventional SSVMs for learning bipartite matchings have only O(1/ ) rates due to the lack of smoothness (Joachims et al., 2009; Teo et al., 2010). If smoothing is added, then similar linear convergence rates can be achieved with similar condition numbers. However, it is noteworthy that at each iteration we need to apply ADMM to solve a projection problem to the doubly stochastic matrix set (Eq. (9)), while SSVMs (without smoothing) solves a matching problem with the Hungarian algorithm, incurring O(n3) time."}, {"heading": "3.4 Consistency Analysis", "text": "Despite its apparent differences from standard empirical risk minimization (ERM), adversarial loss minimization (Eq. (3)) can be equivalently recast as an ERM:\nmin \u03b8 E x\u223cP \u03c0|x\u223cP\u0303\n[ ALpermf\u03b8 (x, \u03c0) ] where ALpermf\u03b8 (x, \u03c0) ,\nmin P\u0302 (\u03c0\u0302|x) max P\u030c (\u03c0\u030c|x) E\u03c0\u0302|x\u223cP\u0302 \u03c0\u030c|x\u223cP\u030c\n[ loss(\u03c0\u0302, \u03c0\u030c) + f\u03b8(x, \u03c0\u030c)\u2212 f\u03b8(x, \u03c0) ]\nand f\u03b8(x, \u03c0) = \u03b8 \u00b7 \u2211n i=1 \u03c6(x, \u03c0i) is the Lagrangian potential function. Here we consider f\u03b8 as the linear discriminant function for a proposed permutation \u03c0, using parameter value \u03b8. ALpermf\u03b8 (x, \u03c0) is then the surrogate loss for input x and permutation \u03c0.\nAs described in Section 2.2, Fisher consistency is an important property for a surrogate loss L. It requires that under the true distribution P (x, \u03c0), the hypothesis that minimizes L is Bayes optimal (Tewari & Bartlett, 2007; Liu,\n2007). For the cases of multiclass classification and ordinal regression, Fisher consistency for adversarial surrogate loss has been established by Fathony et al. (2016; 2017). In our setting, the Fisher consistency ofALpermf can be written as:\nf\u2217 \u2208 F\u2217 , argmin f\nE\u03c0|x\u223cP [ ALpermf (x, \u03c0) ] (16)\n\u21d2 argmax \u03c0 f\u2217(x, \u03c0) \u2286 \u03a0 , argmin \u03c0 E\u03c0\u0304|x\u223cP [loss(\u03c0, \u03c0\u0304)].\nNote that in Eq. (16) we allow f to be optimized over the set of all measurable functions on the input space (x, \u03c0). In our formulation, we have restricted f to be additively decomposable over individual elements of permutation, f(x, \u03c0) = \u2211 i gi(x, \u03c0i). In the sequel, we will show that the condition in Eq. (16) also holds for this restricted set provided that g is allowed to be optimized over the set of all measurable functions on the space of individual input (x, \u03c0i). We start by establishing Fisher consistency for the case of singleton loss minimizing sets \u03a0 in Theorem 1 and then for more general cases in Theorem 2.\nTheorem 1. Suppose loss(\u03c0, \u03c0\u0304) = loss(\u03c0\u0304, \u03c0) (symmetry) and loss(\u03c0, \u03c0) < loss(\u03c0\u0304, \u03c0) for all \u03c0\u0304 6= \u03c0. Then the adversarial permutation loss ALpermf is Fisher consistent if f is over all measurable functions and \u03a0 is a singleton.\nTheorem 2. Suppose loss(\u03c0, \u03c0\u0304) = loss(\u03c0\u0304, \u03c0) (symmetry) and loss(\u03c0, \u03c0) < loss(\u03c0\u0304, \u03c0) for all \u03c0\u0304 6= \u03c0. Furthermore if f is over all measurable functions, then:\n(a) there exists f\u2217 \u2208 F\u2217 such that argmax\u03c0 f\u2217(x, \u03c0) \u2286 \u03a0 (i.e., satisfies the Fisher consistency requirement). In fact, all elements in \u03a0 can be recovered by some f\u2217 \u2208 F\u2217 .\n(b) if argmin\u03c0 \u2211 \u03c0\u2032\u2208\u03a0 \u03b1\u03c0\u2032 loss(\u03c0\n\u2032, \u03c0) \u2286 \u03a0 for all \u03b1(\u00b7) \u2265 0; \u2211 \u03c0\u2032\u2208\u03a0 \u03b1\u03c0\u2032 = 1, then argmax\u03c0 f \u2217(x, \u03c0) \u2286 \u03a0 for all f\u2217 \u2208 F\u2217. In this case, all f\u2217 \u2208 F\u2217 satisfy the Fisher consistency requirement.\nThese assumptions of loss functions in the theorems above are quite mild, requiring only that wrong predictions suffer higher loss than correct ones. We refer the reader to Appendix B for the detailed proofs of theorems. The key to the proofs is the observation that for the optimal potential function f\u2217, f\u2217(x, \u03c0) + loss(\u03c0, \u03c0 ) is invariant to \u03c0 when \u03a0 = {\u03c0 }. We refer to this as the loss reflective property. Note that this generalizes the observation for the case of ordinal regression loss (Fathony et al., 2017) into matching loss functions, subject to the mild pre-conditions assumed by the theorem.\nTheorem 3. Suppose the loss is Hamming loss, and the potential function f(x, \u03c0) decomposes additively by\u2211 i gi(x, \u03c0i). Then, the adversarial permutation loss ALpermf is Fisher consistent provided that gi is allowed to\nbe optimized over the set of all measurable functions on the space of individual inputs (x, \u03c0i).\nProof. Simply choose gi such that for each sample x in the population, gi(x, \u03c0i) = \u2212(\u03c0i 6= \u03c0 i ). This renders the loss reflective property under the Hamming loss."}, {"heading": "4 Experimental Evaluation", "text": "To evaluate our approach, we apply our adversarial bipartite matching model to video tracking tasks using public benchmark datasets (Leal-Taixe\u0301 et al., 2015). In this problem, we are given a set of images (video frames) and a list of objects in each image. We are also given the correspondence matching between objects in frame t and objects in frame t + 1. Figure 2 shows an example of the problem setup. It is important to note that the number of objects are not the same in every frames. Some of the objects may enter, leave, or remain in the consecutive frames. To handle the this issue, we setup our experiment as follows. Let kt be the number of objects in frame t and k\u2217 be the maximum number of objects a frame can have, i.e., k\u2217 = maxt\u2208T kt. Starting from k\u2217 nodes to represent the objects, we add k\u2217 more nodes as \u201cinvisible\u201d nodes to allow new objects to enter and existing objects to leave. As a result, the total number of nodes in each frame doubles to n = 2k\u2217."}, {"heading": "4.1 Feature Representation", "text": "We define the features for pairs of bounding boxes (i.e., \u03c6i(x, j) for pairing bounding box i with bounding box j) in two consecutive video frames so that we can compute the associative feature vectors, \u03c6(x, \u03c0) = \u2211n i=1 \u03c6i(x, \u03c0i), for each possible matching \u03c0. To define the feature vector \u03c6i(\u00b7, \u00b7), we follow the feature representation reported by Kim et al. (2012) using six different types of features:\n\u2022 Intersection over union (IoU) overlap ratio between bounding boxes, area(BBti \u2229 BBt+1j )/ area(BBti \u222a BBt+1j ), where BB t i denotes the bounding\nbox of object i at time frame t; \u2022 Euclidean distance between object centers; \u2022 21 color histogram distance features (RGB) from\nthe Bhattacharyaa distance, 14 ln ( 1 4 (\u03c32p \u03c32q + \u03c32q \u03c32p + 2 )) +\nWe explain this feature representation in more detail in Appendix C."}, {"heading": "4.2 Experimental Setup", "text": "We compare our approach with the Structured SVM (SSVM) model (Taskar et al., 2005a; Tsochantaridis et al., 2005) implemented based on Kim et al. (2012) using SVM-Struct (Joachims, 2008; Vedaldi, 2011). We implement our marginal version of adversarial bipartite matching using minConf (Schmidt, 2008) for performing projected Quasi-Newton optimization.\nWe consider two different groups of datasets in our experiment: TUD datasets and ETH datasets. Each dataset contains different numbers of elements (i.e., the number of pedestrian bounding box in the frame plus the number of extra nodes to indicate entering or leaving) and different numbers of examples (i.e., pairs of two consecutive frames that we want to match). Table 3 contains the detailed information about the datasets.\nTo avoid having test examples that are too similar with the training set, we train the models on one dataset and test the model on another dataset that has similar characteristics. In particular, we perform evaluations for every pair of datasets in TUD and ETH collections. This results in eight pairs of training/test datasets, as shown in Table 4.\nTo tune the regularization parameter (\u03bb in adversarial matching, and C in SSVM), we perform 5-fold cross validation based on the training dataset only. The resulting best regularization parameter is used to train the model over all training examples to obtain parameters \u03b8, which we then use to predict the matching for the testing data. For SSVM and the marginal version of adversarial matching, the pre-\ndiction is done by finding the bipartite matching that maximizes the potential value, i.e., argmaxY \u3008Y, \u2211 k \u03b8kXk\u3009 which can be solved using the Hungarian algorithm. The double oracle version of adversarial matching makes predictions by finding the most likely permutation from the predictor\u2019s strategy in the equilibrium."}, {"heading": "4.3 Results", "text": "We report the average accuracy, which in this case is defined as (1 \u2212 the average Hamming loss) over all examples in the testing dataset. Table 4 shows the mean and the standard deviation of our metric across different dataset pairs. We report the results for both the double-oracle (DO) and marginal (MARG) versions of the adversarial model. Our experiment indicates that both methods result in very similar values of \u03b8. The slight advantage of the double-oracle version is caused by the difference in prediction techniques between the double-oracle (argmax over predictor\u2019s equilibrium strategy) and marginal version (argmax over potentials). We also observe that the double-oracle approach requires only a small number of augmenting permutations to converge as shown in the last column (the average number of permutations) of Table 4. This indicates the sparseness of the set of permutations that support the equilibrium.\nTo compare with SSVM, we highlight (using bold font) the cases in which our result is better with statistical significance (under paired t-test with \u03b1 < 0.05) in Table 4. Compared with SSVM, our proposed adversarial matching outperforms SSVM in all pairs of datasets\u2014with statistical\nsignificance on all six pairs of the ETH datasets and slightly better than SSVM on the TUD datasets. This suggests that our adversarial bipartite matching model benefits from its Fisher consistency property.\nIn terms of the running time, Table 5 shows that the marginal version of adversarial method is relatively fast. It only takes a few seconds to train until convergence in the case of 50 examples, with the number of elements varied up to 34. The running time grows roughly quadratically in the number of elements, which is natural since the size of the marginal probability matrices P and Q also grow quadratically in the number of elements. This shows that our approach is much more efficient than the CRF approach, which has a running time that is impractical even for small problems with 20 elements. The training time of SSVM is faster than the adversarial methods due to two different factors: (1) the inner optimization of SSVM can be solved using a single execution of the Hungarian algorithm compared with the inner optimization of adversarial method which requires ADMM optimization for projection to doubly stochastic matrix set; (2) different tools for implementation, i.e., C++ for SSVM and MATLAB for our method, which benefits the running time of SSVM. In addition, though the game size is relatively small, as indicated by the final column in Table 4, the double oracle version of adversarial method takes much longer to train compared to the marginal version."}, {"heading": "5 Conclusions and Future Work", "text": "In this paper, we have presented an adversarial approach for learning bipartite matchings that is not only computationally efficient to employ but also provides Fisher consistency guarantees. We showed that these theoretical advantages translate into better empirical performance for our model compared with previous approaches. Our future work will explore matching problems with different loss functions and other graphical structures."}, {"heading": "Acknowledgements", "text": "We thank our anonymous reviewers for their useful feedback and suggestions. This research was supported in part by NSF Grants RI-#1526379 and CAREER-#1652530."}], "year": 2018, "references": [{"title": "Face description with local binary patterns: Application to face recognition", "authors": ["T. Ahonen", "A. Hadid", "M. Pietikainen"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "year": 2006}, {"title": "A boosting algorithm for learning bipartite ranking functions with partially labeled data", "authors": ["M.R. Amini", "T.V. Truong", "C. Goutte"], "venue": "In Proceedings of the International ACM SIGIR Conference,", "year": 2008}, {"title": "Adversarial cost-sensitive classification", "authors": ["K. Asif", "W. Xing", "S. Behpour", "B.D. Ziebart"], "venue": "In Proceedings of the Conference on Uncertainty in Artificial Intelligence,", "year": 2015}, {"title": "The computation of optical flow", "authors": ["S.S. Beauchemin", "J.L. Barron"], "venue": "ACM Computing Surveys (CSUR),", "year": 1995}, {"title": "Adversarially optimizing intersection over union for object localization", "authors": ["S. Behpour", "K.M. Kitani", "B.D. Ziebart"], "venue": "tasks. CoRR,", "year": 2017}, {"title": "ARC: Adversarial robust cuts for semi-supervised and multi-label classification", "authors": ["S. Behpour", "W. Xing", "B.D. Ziebart"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,", "year": 2018}, {"title": "Shape matching and object recognition using shape contexts", "authors": ["S. Belongie", "J. Malik", "J. Puzicha"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "year": 2002}, {"title": "Three observations on linear algebra", "authors": ["G. Birkhoff"], "venue": "Univ. Nac. Tacuman, Rev. Ser. A,", "year": 1946}, {"title": "Convex Optimization", "authors": ["S. Boyd", "L. Vandenberghe"], "year": 2004}, {"title": "Maxsim: A maximum similarity metric for machine translation evaluation", "authors": ["Y.S. Chan", "H.T. Ng"], "venue": "Proceedings of ACL-08: HLT, pp", "year": 2008}, {"title": "EM, MCMC, and chain flipping for structure from motion with unknown correspondence", "authors": ["F. Dellaert", "S.M. Seitz", "C.E. Thorpe", "S. Thrun"], "venue": "Machine Learning,", "year": 2003}, {"title": "On the global and linear convergence of the generalized alternating direction method of multipliers", "authors": ["W. Deng", "W. Yin"], "venue": "Journal of Scientific Computing,", "year": 2016}, {"title": "On the numerical solution of heat conduction problems in two and three space variables", "authors": ["J. Douglas", "H.H. Rachford"], "venue": "Transactions of the American Mathematical Society,", "year": 1956}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "authors": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "year": 2011}, {"title": "Rank aggregation methods for the web", "authors": ["C. Dwork", "R. Kumar", "M. Naor", "D. Sivakumar"], "venue": "In Proceedings of the International Conference on World Wide Web,", "year": 2001}, {"title": "Adversarial multiclass classification: A risk minimization perspective", "authors": ["R. Fathony", "A. Liu", "K. Asif", "B. Ziebart"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}, {"title": "Adversarial surrogate losses for ordinal regression", "authors": ["R. Fathony", "M.A. Bashiri", "B. Ziebart"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"title": "Game theory, maximum entropy, minimum discrepancy, and robust Bayesian decision theory", "authors": ["P.D. Gr\u00fcnwald", "A.P. Dawid"], "venue": "Annals of Statistics,", "year": 2004}, {"title": "Fast approximation of the permanent for very dense problems", "authors": ["M. Huber", "J. Law"], "venue": "In Proceedings of the nineteenth annual ACM-SIAM symposium on Discrete algorithms,", "year": 2008}, {"title": "Cutting-plane training of structural SVMs", "authors": ["T. Joachims", "T. Finley", "Yu", "C.-N"], "venue": "Machine Learning,", "year": 2009}, {"title": "Online multi-target tracking by large margin structured learning", "authors": ["S. Kim", "S. Kwak", "J. Feyereisl", "B. Han"], "venue": "In Asian Conference on Computer Vision,", "year": 2012}, {"title": "The hungarian method for the assignment problem", "authors": ["H.W. Kuhn"], "venue": "Naval Research Logistics,", "year": 1955}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "authors": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "In Proc. of the International Conference on Machine Learning,", "year": 2001}, {"title": "Direct optimization of ranking measures", "authors": ["Q. Le", "A. Smola"], "venue": "arXiv preprint arXiv:0704.3359,", "year": 2007}, {"title": "Towards a benchmark for multitarget tracking", "authors": ["K. Motchallenge"], "venue": "arXiv preprint arXiv:1504.01942,", "year": 2015}, {"title": "Adversarial sequence tagging", "authors": ["J. Li", "K. Asif", "H. Wang", "B.D. Ziebart", "T.Y. Berger-Wolf"], "venue": "In International Joint Conference on Artificial Intelligence,", "year": 2016}, {"title": "Web video topic discovery and tracking via bipartite graph reinforcement model", "authors": ["L. Liu", "L. Sun", "Y. Rui", "Y. Shi", "S. Yang"], "venue": "In Proceedings of the 17th International Conference on World Wide Web,", "year": 2008}, {"title": "Fisher consistency of multicategory support vector machines", "authors": ["Y. Liu"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "year": 2007}, {"title": "A phrasebased alignment model for natural language inference", "authors": ["B. MacCartney", "M. Galley", "C.D. Manning"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "year": 2008}, {"title": "Planning in the presence of cost functions controlled by an adversary", "authors": ["H.B. McMahan", "G.J. Gordon", "A. Blum"], "venue": "In Proceedings of the International Conference on Machine Learning,", "year": 2003}, {"title": "Introductory Lectures on Convex Optimization: A Basic Course", "authors": ["Y. Nesterov"], "year": 2003}, {"title": "Optimal constituent alignment with edge covers for semantic projection", "authors": ["S. Pad\u00f3", "M. Lapata"], "venue": "In Proceedings of the International Conference on Computational Linguistics,", "year": 2006}, {"title": "Exponential family graph matching and ranking", "authors": ["J. Petterson", "J. Yu", "J.J. McAuley", "T.S. Caetano"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2009}, {"title": "Bipartite graph reinforcement model for web image annotation", "authors": ["X. Rui", "M. Li", "Z. Li", "Ma", "W.-Y", "N. Yu"], "venue": "In Proceedings of the 15th ACM International Conference on Multimedia,", "year": 2007}, {"title": "minConf: projection methods for optimization with simple constraints in Matlab", "authors": ["M. Schmidt"], "venue": "http://www.cs.ubc.ca/ \u0303schmidtm/ Software/minConf.html,", "year": 2008}, {"title": "Optimizing costly functions with simple constraints: A limited-memory projected quasi-Newton algorithm", "authors": ["M. Schmidt", "E. Berg", "M. Friedlander", "K. Murphy"], "venue": "In Artificial Intelligence and Statistics,", "year": 2009}, {"title": "On general minimax theorems", "authors": ["M. Sion"], "venue": "Pacific Journal of mathematics,", "year": 1958}, {"title": "Learning structured prediction models: A large margin approach", "authors": ["B. Taskar", "V. Chatalbashev", "D. Koller", "C. Guestrin"], "venue": "In Proceedings of the International Conference on Machine Learning,", "year": 2005}, {"title": "Protein structure comparison using bipartite graph matching and its application to protein structure classification", "authors": ["W.R. Taylor"], "venue": "Molecular & Cellular Proteomics,", "year": 2002}, {"title": "Bundle methods for regularized risk minimization", "authors": ["C.H. Teo", "S.V.N. Vishwanthan", "A.J. Smola", "Q.V. Le"], "venue": "Journal of Machine Learning Research,", "year": 2010}, {"title": "On the consistency of multiclass classification methods", "authors": ["A. Tewari", "P. Bartlett"], "venue": "Journal of Machine Learning Research,", "year": 2007}, {"title": "Information-theoretical optimization techniques", "authors": ["F. Tops\u00f8e"], "venue": "Kybernetika, 15(1):8\u201327,", "year": 1979}, {"title": "Large margin methods for structured and interdependent output variables", "authors": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "Journal of Machine Learning Research,", "year": 2005}, {"title": "The complexity of computing the permanent", "authors": ["L.G. Valiant"], "venue": "Theoretical Computer Science,", "year": 1979}, {"title": "Efficient sampling for bipartite matching problems", "authors": ["M. Volkovs", "R.S. Zemel"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2012}, {"title": "A certain zero-sum two-person game equivalent to the optimal assignment problem", "authors": ["J. Von Neumann"], "venue": "Contributions to the Theory of Games,", "year": 1953}, {"title": "Theory of games and economic behavior", "authors": ["J. Von Neumann", "O. Morgenstern"], "venue": "Bull. Amer. Math. Soc,", "year": 1945}, {"title": "Adversarial prediction games for multivariate losses", "authors": ["H. Wang", "W. Xing", "K. Asif", "B. Ziebart"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Robust image retrieval based on color histogram of local feature regions", "authors": ["Wang", "X.-Y", "Wu", "J.-F", "Yang", "H.-Y"], "venue": "Multimedia Tools and Applications,", "year": 2010}, {"title": "A bipartite graph matching framework for finding correspondences between structural elements in two proteins", "authors": ["Y. Wang", "F. Makedon", "J. Ford", "H. Huang"], "venue": "In International Conference of the IEEE Engineering in Medicine and Biology Society,", "year": 2004}, {"title": "Near-duplicate keyframe retrieval by nonrigid image matching", "authors": ["J. Zhu", "S.C. Hoi", "M.R. Lyu", "S. Yan"], "venue": "In Proceedings of the 16th ACM international conference on Multimedia,", "year": 2008}], "id": "SP:0d628d0d299880abf2736da8ad51866e2ac3f016", "authors": [{"name": "Rizal Fathony", "affiliations": []}, {"name": "Sima Behpour", "affiliations": []}, {"name": "Xinhua Zhang", "affiliations": []}, {"name": "Brian D. Ziebart", "affiliations": []}], "abstractText": "Many important structured prediction problems, including learning to rank items, correspondence-based natural language processing, and multi-object tracking, can be formulated as weighted bipartite matching optimizations. Existing structured prediction approaches have significant drawbacks when applied under the constraints of perfect bipartite matchings. Exponential family probabilistic models, such as the conditional random field (CRF), provide statistical consistency guarantees, but suffer computationally from the need to compute the normalization term of its distribution over matchings, which is a #P-hard matrix permanent computation. In contrast, the structured support vector machine (SSVM) provides computational efficiency, but lacks Fisher consistency, meaning that there are distributions of data for which it cannot learn the optimal matching even under ideal learning conditions (i.e., given the true distribution and selecting from all measurable potential functions). We propose adversarial bipartite matching to avoid both of these limitations. We develop this approach algorithmically, establish its computational efficiency and Fisher consistency properties, and apply it to matching problems that demonstrate its empirical benefits.", "title": "Efficient and Consistent Adversarial Bipartite Matching"}