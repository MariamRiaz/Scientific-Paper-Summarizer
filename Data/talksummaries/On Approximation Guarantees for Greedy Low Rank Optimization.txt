0	31	Low rank matrix optimization stands as a major tool in modern dimensionality reduction and unsupervised learning.
1	18	The singular value decomposition can be used when the optimization objective is rotationally invariant to the parameters.
3	33	More concretely, in the low rank matrix optimization problem, we wish to solve argmax ⇥ `(⇥) s.t.
15	105	In this paper, we interpret low rank matrix estimation as a set optimization problem over an infinite set of atoms.
16	34	Specifically, we wish to optimize argmax {X1,...Xk}2A ` kX i=1 ↵iXi !
17	48	, where the set of atoms A is the set of all rank one matrices with unit operator norm.
19	123	This formulation allows us to connect the problem of low rank matrix estimation to that of submodular set function optimization, which we discuss in the sequel.
52	45	We represent sets using sans script fonts e.g. A,B.
55	55	The transpose of a vector or a matrix is represented by > e.g. X>.
56	39	For singleton sets, we write f(j) := f({j}).
57	24	Size of a set S is denoted by |S|.
60	62	Consider the classic greedy algorithm that picks up the next element myopically i.e. given the solution set built so far, the algorithm picks the next element as the one which maximizes the gain obtained by adding the said element into the solution set.
62	18	Submodular set functions are well studied and have many desirable properties that allow for efficient minimization and maximization with approximation guarantees.
63	100	Our low rank estimation problem also falls under the purview of another class of functions called monotone functions.
64	33	A function is called monotone if and only if f(A)  f(B) for all A ✓ B.
66	27	Without further assumptions or knowledge of the function, no other polynomial time algorithm can provide a better approximation guarantee unless P=NP (Feige, 1998).
86	14	While our results hold for general vector sets U ,V assuming an oracle access to subroutines GreedySel and OMPSel (to be detailed later), for the rest of the paper we focus on the case of norm 1 balls U := {x 2 Rn s.t.
96	19	Our greedy algorithm, illustrated in Algorithm 1, builds the support set incrementally – adding rank 1 matrices one at a time such that at iteration i for 1  i  k the size of the chosen support set (and hence rank of the current iterate) is i.
103	41	An alternative is to choose the next atom by using the linear maximization oracle used by Frank-Wolfe (Jaggi, 2013) or Matching Pursuit algorithms (Gribonval & Vandergheynst, 2006; Locatello et al., 2017).
111	23	However, as we shall see, our analysis provides stronger bounds than their Theorem 2.
129	14	The proof technique for the first inequality of Theorem 3 relies on lower bounding the progress made in each iteration of Algorithm 1.
130	14	Intuitively, it exploits weak submodularity to make sure that each iteration makes enough progress, and then applies an induction argument for r iterations.
168	13	While understanding approximation guarantees are useful, providing parameter recovery bounds can further help us understand the practical utility of greedy algorithms.
194	15	A pair of nodes within the same cluster have an edge between them with probability p, while a pair of nodes belonging to different clusters have an edge between them with probability q.
205	22	We use our greedy algorithm to cluster the graph by optimizing a logistic PCA objective function, which is a special case of the exponential family PCA (Collins et al., 2001).
218	21	Spectral clustering algorithms typically select k by computing an SVD and rerunning k-means for different values of k. In addition to being more robust, our greedy algorithm does not need to be rerun for different values of k – it produces solutions incrementally.
242	44	Note that since we keep only the most common words, several queries from the datasets are invalid because we do not have embeddings for words appearing in them.
244	15	Table 1 shows the empirical evaluation.
246	41	We run each of these for k = {5, 10, 15, 20} and report the best results.
249	16	Through that connection we have provided improved exponential rates of convergence for the algorithm.
