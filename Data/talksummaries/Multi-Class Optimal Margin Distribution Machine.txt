7	15	A brief summary of this line of early research can be found in (Zhou, 2014).
14	19	We further provide a generalization error bound based on Rademacher complexity, and further present the analysis of the relationship between generalization error and margin distribution for multi-class classification.
29	23	,wk ∈ H, where each weight vector wl, l ∈ Y defines a scoring function x 7→ w⊤l ϕ(x) and the label of instance x is the one resulting in the largest score, i.e., h(x) = argmaxl∈Y w ⊤ l ϕ(x).
30	12	This decision function naturally leads to the following definition of the margin for a labeled instance (x, y): γh(x, y) = w ⊤ y ϕ(x)−max l ̸=y w⊤l ϕ(x).
31	39	Thus h misclassifies (x, y) if and only if it produces a negative margin for this instance.
34	13	(Gao & Zhou, 2013) proved that, to characterize the margin distribution, it is important to consider not only the margin mean but also the margin variance.
35	33	Inspired by this idea, (Zhang & Zhou, 2014; 2016) proposed the optimal margin distribution machine for binary classification, which characterizes the margin distribution according to the first- and second-order statistics, that is, maximizing the margin mean and minimizing the margin variance simultaneously.
36	22	Specifically, let γ̄ denote the margin mean, and the optimal margin distribution machine can be formulated as: min w,γ̄,ξi,ϵi Ω(w)− ηγ̄ + λ m m∑ i=1 (ξ2i + ϵ 2 i ) s.t.
39	16	By scaling w which doesn’t affect the final classification results, the margin mean can be fixed as 1, then the de- viation of the margin of (xi, yi) to the margin mean is |γh(xi, yi) − 1|, and the optimal margin distribution machine can be reformulated as min w,ξi,ϵi Ω(w) + λ m m∑ i=1 ξ2i + µϵ 2 i (1− θ)2 s.t.
41	21	where µ ∈ (0, 1] is a parameter to trade off two different kinds of deviation (larger or less than margin mean).
42	15	θ ∈ [0, 1) is a parameter of the zero loss band, which can control the number of support vectors, i.e., the sparsity of the solution, and (1− θ)2 in the denominator is to scale the second term to be a surrogate loss for 0-1 loss.
46	15	Due to themax operator in the second constraint, mcODM is a non-differentiable non-convex programming, which is quite difficult to solve directly.
48	21	Specifically, at each iteration, we recast the first constraint as k − 1 linear inequality constraints: w⊤yiϕ(xi)−w ⊤ l ϕ(xi) ≥ 1− θ − ξi, l ̸= yi, and replace the second constraint with w⊤yiϕ(xi)−Mi ≤ 1 + θ + ϵi, where Mi = maxl ̸=yi w̄ ⊤ l ϕ(xi) and w̄l is the solution to the previous iteration.
57	13	Note that all the constraints can be partitioned into m disjoint sets, and the i-th set only involves α1i , .
59	11	Specifically, we sequentially select a group of k + 1 variables α1i , .
60	19	, α k i , βi associated with instance xi to minimize, while keeping other variables as constants, and repeat this procedure until convergence.
110	23	Theorem 1 shows that we can get a tighter generalization bound for smaller rΛ and smaller θ.
111	17	Since γ ≤ 2rΛ, so the former can be viewed as an upper bound of the margin.
113	15	This verifies that better margin distribution (i.e., larger margin mean and smaller margin variance) can yield better generalization performance, which is also consistent with the work of (Gao & Zhou, 2013).
128	14	Specially, ovaSVM consists of learning k scoring functions hl : X 7→ {−1,+1}, l ∈ Y , each seeking to discriminate one class l ∈ Y from all the others, as can be seen it need train k SVM models.
145	12	In addition, as can be seen, in comparing with other four methods which don’t consider margin distribution, the win/tie/loss counts show that mcODM is always better or comparable, almost never worse than it.
152	41	Figure 2 plots the frequency histogram of margin distribution produced by mcSVM, ovaSVM and mcODM on data set segment as the number of classes increases from two to seven.
153	17	As can be seen, when the number of classes is less than four, all methods can achieve good margin distribution, whereas with the increase of the number of classes, the other two methods begin to produce negative margins.
154	16	At the same time, the distribution of our method becomes “sharper”, which prevents the instance with small margin, so our method can still perform relatively well as the number of classes increases, which is also consistent with the observation from Figure 1.
159	13	It can be seen that for small data sets, the efficiency of all the methods are similar, however, for data sets with more than ten classes, e.g., sector and rcv1, mcSVM and mcODM, which learn all the scroing functions at once, are much faster than ovaSVM and ovoSVM, owing to the inefficiency of binarydecomposition as discussed in Section 6.1.
160	28	Note that LIBLINEAR are very fast implementations of binary SVM and mcSVM, and this shows that our method is also computationally efficient.
161	22	Recent studies disclose that for binary class classification, maximizing the minimum margin does not necessarily lead to better generalization performances, and instead, it is crucial to optimize the margin distribution.
162	24	However, it remains open to the influence of margin distribution for multi-class classification.
163	14	We try to answer this question in this paper.
164	13	After maximizing the margin mean and minimizing the margin variance simultaneously, the resultant optimization is a difficult non-differentiable non-convex programming.
