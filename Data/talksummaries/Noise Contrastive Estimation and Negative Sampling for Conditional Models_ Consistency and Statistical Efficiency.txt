2	45	In log-linear models, including both the original work on maximum-entropy models (Berger et al., 1996), and later work on conditional random fields (Lafferty et al., 2001), ⇤Part of this work done at Google.
3	95	the scoring function s(x, y; ✓) = ✓ · f(x, y) where f(x, y) 2 Rd is a feature vector, and ✓ 2 Rd are the parameters of the model.
6	20	In many NLP applications the set Y is large.
10	37	Prominent examples are the binary objective used in word2vec ((Mikolov et al., 2013), see also (Levy and Goldberg, 2014)), and the Noise Contrastive Estimation methods of (Mnih and Teh, 2012; Jozefowicz et al., 2016) for estimation of language models.
11	97	In spite of the centrality of negative sampling methods, they are arguably not well understood from a theoretical standpoint.
12	29	There are clear connections to noise contrastive estimation (NCE) (Gutmann and Hyvärinen, 2012), a negative sampling method for parameter estimation in joint models of the form p(y) = exp (s(y; ✓)) Z(✓) ; Z(✓) = X y2Y exp (s(y; ✓)) (2) However there has not been a rigorous theoretical analysis of NCE in the estimation of conditional models of the form in Eq.
13	24	1, and we will argue there are subtle but important questions when generalizing NCE to the conditional case.
14	89	In particular, the joint model in Eq 2 has a single partition function Z(✓) which is estimated as a param- eter of the model (Gutmann and Hyvärinen, 2012) whereas the conditional model in Eq 1 has a separate partition function Z(x; ✓) for each value of x.
15	109	We show the following (throughout we define K 1 to be the number of negative examples sampled per training example): • For any K 1, a binary classification variant of NCE, as used by (Mnih and Teh, 2012; Mikolov et al., 2013), gives consistent parameter estimates under the assumption that Z(x; ✓) is constant with respect to x (i.e., Z(x; ✓) = H(✓) for some function H).
23	15	• There is some unknown joint distribution p X,Y (x, y) where x 2 X and y 2 Y .
41	20	Figure 1 shows two NCE-based parameter estimation algorithms, based respectively on binary objective and ranking objective.
42	59	The input to either algorithm is a set of training examples {x(i), y(i)}n i=1, a parameter K specifying the number of negative examples per training example, and a distribution p N (·) from which negative examples are sampled.
44	41	Binary objective essentially corresponds to a problem where the scoring function s(x, y; ✓) is used to construct a binary classifier that discriminates between positive and negative examples.
45	17	Ranking objective corresponds to a problem where the scoring function s(x, y; ✓) is used to rank the true label y(i) above negative examples y(i,1) .
108	22	The following theorem shows that any parameter vector ¯✓ 2 ⇥⇤ R if and only if it gives the correct conditional distribution p Y |X(y|x).
110	13	For any ✓ 2 ⇥, if there exists a function c(x) such that s(x, y; ✓) s(x, y; ✓⇤) ⌘ c(x) for all (x, y) 2 X ⇥ Y , then ✓ = ✓⇤ and thus c(x) = 0 for all x. Theorem 4.1 Under Assumption 2.1, ¯✓ 2 ⇥⇤ R if and only if, for all (x, y) 2 X ⇥ Y , p Y |X(y|x) = exp(s(x, y; ¯✓))/Z(x, ¯✓).
119	27	Theorem 4.2 (Consistency) Under Assump- tions 2.1, 4.2, 4.3, the estimates based on the ranking objective are strongly consistent in the sense that for any fixed K 1, P n lim n!1 min ✓ ⇤2⇥⇤ R kb✓n R ✓⇤k = 0 o = P n lim n!1 d ⇣ bpn Y |X , pY |X ⌘ = 0 o = 1 Further, if Assumption 4.1 holds, P n lim n!1 b✓n R = ✓⇤ o = 1.
122	15	First consider the following function, L1 B (✓, ) = X x,y n p X,Y (x, y) log (g(x, y; ✓, )) +Kp X (x)p N (y) log (1 g(x, y; ✓, )) o One can find that L1 B (✓, ) = E [Ln B (✓, )] .
131	22	based on the binary objective.
132	88	Theorem 4.4 (Consistency) Under Assump- tion 2.2, 4.2, 4.5, the estimates defined by the binary objective are strongly consistent in the sense that for any K 1, P n lim n!1 min (✓⇤, ⇤)2⌦⇤ B k(b✓n B , b n B ) (✓⇤, ⇤)k = 0 o = P n lim n!1 d ⇣ bpn Y |X , pY |X ⌘ = 0 o = 1 If further Assumption 4.4 holds, P n lim n!1 ( b✓n B , b n B ) = (✓⇤, ⇤) o = 1.
133	37	In this section, we give a simple example to demonstrate that the binary classification approach fails to be consistent when assumption 2.1 holds but assumption 2.2 fails (i.e. the partition function depends on the input).
134	100	Consider X 2 X = {x1, x2} with marginal distribution p X (x1) = pX(x2) = 1/2, and Y 2 Y = {y1, y2} generated by the conditional model specified in assumption 2.1 with the score function parametrized by ✓ = (✓1, ✓2) and s(x1, y1; ✓) = log ✓1, s(x1, y2; ✓) = s(x2, y1; ✓) = s(x2, y2; ✓) = log ✓2.
135	13	Assume the true parameter is ✓⇤ = (✓⇤1, ✓⇤2) = (1, 3).
137	114	Suppose we choose the negative sampling distribution p N (y1) = pN (y2) = 1/2.
138	56	For any K 1, by the Law of Large Numbers, as n goes to infinity, Ln B (✓, ) will converge to L1 B (✓, ).
139	41	Substitute in the parameters above.
140	82	One can show that L1 B (✓, ) = 1 8 log 2✓1 2✓1 +K exp( ) + K 4 log K exp( ) 2✓1 +K exp( ) + 7 8 log 2✓2 2✓2 +K exp( ) + 4 log K exp( ) 2✓2 +K exp( ) .
141	18	Setting the derivatives w.r.t.
144	25	Then the estimated distribution ep Y |X will satisfy ep Y |X(y1|x1) ep Y |X(y2|x1) = e✓1 e✓2 = 1/4 7/12 = 3 7 , which contradicts the fact that p Y |X(y1|x1) p Y |X(y2|x1) = p X,Y (x1, y1) p X,Y (x1, y2) = 1 3 .
148	41	Classical likelihood theory tells us, under mild conditions, the maximum likelihood estimator (MLE) has nice properties like asymptotic normality and Fisher efficiency.
