5	17	It is necessary to evaluate new features immediately and output intermediate result.
6	10	The feature evaluation process in a stream is called online feature selection (Perkins & Theiler, 2003; Zhou et al., 2005; Wu et al., 2010).
8	22	Suppose that there are n samples but initially we do not observe all of the features.
9	12	We call the sequence a1,a2, · · · ∈ Rn is a feature stream, with each ai ∈ Rn being the ith feature, or the ith covariate of n samples.
10	40	Note that in our setting, the feature ai is revealed at time stamp i.
14	32	For example, Perkins & Theiler (2003) added a new feature which contributes to a predictor learning and optimization analysis into the model.
17	11	Yet successful, most of the results in this line of research are empirical in nature.
21	10	In this paper, we consider the high-dimensional regime that the number of features is much larger than the sample size, and the features are revealed in an online manner.
22	12	We propose an unsupervised algorithm termed Online leverage scores for Feature Selection (OFS).
25	17	Furthermore, we apply k-means clustering on the set of selected features, and show that the clustering performance does not degrade a lot.
26	10	Computationally, our algorithm enjoys low time complexity and little memory usage, which makes it a perfect fit for big data analytics.
53	5	We use bold lower-case letters, e.g. v ∈ Rd to denote a column vector.
60	9	The ith column and jth row of the matrix X are denoted by xi and (xj)>, respectively.
66	15	In this section, we propose an online algorithm for feature selection, where the goal is to approximate the original data with much fewer attributes in some sense.
67	15	To the end, we make use of the leverage score that, from a high level, reflects the importance of each feature.
71	8	In the online setting, however, we are not able to access all the data to compute the leverage score.
72	11	The key idea of our algorithm is that when a new feature arrives, we approximate its leverage score based on the obtained features, which can further be used to guide the selection process.
74	18	A natural way for the sake is to compute the approximate leverage score of ai as follows: li = a > i (Ãi−1Ã > i−1) †ai.
76	10	1: for i = 1, · · · do 2: Reveal the ith feature ai.
88	5	Then, the sampling probability is computed as pi = min ( 8 −2 log n · l̃i, 1 ) .
92	21	Let Ã be the output when it terminates.
129	8	Formally, k-means clustering seeks to partition the data matrix A ∈ Rn×d into k clusters {C1, · · · , Ck} to minimize the distance between data points and its closest center {µ1, · · · ,µk} (Awasthi et al., 2010): min µ1,...,µk k∑ i=1 ∑ j∈Ci ∥∥aj − µi∥∥22 , (3.5) where µi be the center of data points in Ci.
141	7	Using the notation Y = I −XX>, we can rewrite the objective function of k-means based on the data matrices A and Ã as∥∥∥A−XX>A∥∥∥2 F = ‖Y A‖2F = Tr ( Y AA>Y ) ,∥∥∥Ã−XX>Ã∥∥∥2 F = ∥∥∥Y Ã∥∥∥2 F = Tr ( Y ÃÃ > Y ) .
151	6	We perform the experiments on 6 realistic data sets, including USPS1, AR2, COIL203, CIFAR-104, MNIST5 and ORL6.
152	7	The summary of them is shown in Table 2.
153	9	We compare our algorithm with state-of-the-art feature selection approaches, including supervised model, for instance, alpha-investing (Alpha) (Zhou et al., 2005), as well as unsupervised model, e.g., λ-ridge leverage score (LevS) (Alaoui & Mahoney, 2015) and Laplacian score (LapS) (He et al., 2005).
158	10	We report the clustering accuracy against the number of selected features in Figure 1.
160	18	For example, our algorithm outperforms all the attarchive/facedatabase.html baseline methods on COIL20, CIFAR-10 and ORL when the number of selected features varies from 10 to 500.
166	8	Our algorithm, in contrast, only requires a few seconds.
167	15	The reason is that in each iteration, we operate with a skinny matrix Ã instead of the whole data matrixA.
