0	32	Natural Language Generation (NLG) is the task of automatically converting non-linguistic data into coherent natural language text (Reiter and Dale, 2000; Gatt and Krahmer, 2018).
3	16	First, the referential form needs to be decided, asking whether a reference at a given point in the text should assume the form of, for example, a proper name (“Frida Kahlo”), a pronoun (“she”) or description (“the Mexican painter”).
10	29	Some of these approaches have recently focused on inputs which references to entities are delexicalized to general tags (e.g., ENTITY-1, ENTITY-2) in order to decrease data sparsity.
37	13	The source side of the corpus are sets of Resource Description Framework (RDF) triples.
47	19	Once all entities in the text were mapped to different roles, the first two authors of this study manually replaced the referring expressions in the original target texts by their respective tags.
50	26	While this dataset (which we make available) has various uses, we used it to extract a collection of referring expressions to Wikipedia entities in order to evaluate how well our REG model can produce references to entities throughout a (small) text.
52	195	For instance, by processing the text in Figure 1 and its delexicalized template in Figure 2, we would extract referring expressions like “108 St Georges Terrace” and “It” to 〈 AGENT-1, 108 St Georges Terrace 〉, “Perth” to 〈 BRIDGE-1, Perth 〉, “Australia” to 〈 PATIENT1, Australia 〉 and so on.
53	294	Once all texts were processed and the referring expressions extracted, we filtered only the ones referring to Wikipedia entities, removing references to constants like dates and numbers, for which no references are generated by the model.
54	75	In total, the final version of our dataset contains 78,901 referring expressions to 1,501 Wikipedia entities, in which 71.4% (56,321) are proper names, 5.6% (4,467) pronouns, 22.6% (17,795) descriptions and 0.4% (318) demonstrative referring expressions.
55	15	We split this collection in training, developing and test sets, totaling 63,061, 7,097 and 8,743 referring expressions in each one of them.
56	32	Each instance of the final dataset consists of a truecased tokenized referring expression, the target entity (distinguished by its Wikipedia ID), and the discourse context preceding and following the relevant reference (we refer to these as the pre- and pos-context).
57	45	Pre- and pos-contexts are the lowercased, tokenized and delexicalized pieces of text before and after the target reference.
58	19	References to other discourse entities in the pre- and pos-contexts are represented by their Wikipedia ID, whereas constants (numbers, dates) are represented by a one-word ID removing quotes and replacing white spaces with underscores (e.g., 120 million (Australian dollars) for “120 million (Australian dollars)” in Figure 2).
61	30	In this context, it is important to observe that the conversion of the general tags to the Wikipedia IDs can be done in constant time during the generation process, since their mapping, like the first representation in Figure 2, is the first step of the process.
62	23	In the next section, we show in detail how NeuralREG models the problem of generating a referring expression to a discourse entity.
72	35	All decoders at each timestep i of the generation process take as input features their previous state si−1, the target entity-embedding Vwiki, the embedding of the previous word of the referring expression Vyi−1 and finally the summary vector of the pre- and poscontexts ci.
74	48	Seq2Seq models the context vector ci at each timestep i concatenating the pre- and pos-context annotation vectors averaged over time: ĥ(pre) = 1 N N∑ i h (pre) i (1) ĥ(pos) = 1 N N∑ i h (pos) i (2) ci = [ĥ (pre), ĥ(pos)] (3) CAtt is an LSTM decoder augmented with an attention mechanism (Bahdanau et al., 2015) over the pre- and pos-context encodings, which is used to compute ci at each timestep.
77	21	Equations 4 and 5 summarize the process with k ranging over the two encoders (k ∈ [pre, pos]), being the projection matrices W (k)a and U (k) a and attention vectors v (k) a trained parameters.
83	26	si = Φdec(si−1, [ci, Vyi−1 , Vwiki]) (10) p(yi|y<i, X(pre),x(wiki), X(pos)) = softmax(Wcsi + b) (11) In Equation 10, s0 and c0 are zero-initialized vectors.
86	14	OnlyNames is motivated by the similarity among the Wikipedia ID of an element and a proper name reference to it.
88	33	Ferreira works by first choosing whether a reference should be a proper name, pronoun, description or demonstrative.
93	15	Finally, if a referring expression is not found in the training set for a given entity, the same method as OnlyNames is used.
95	46	Text and sentence information statuses mark whether a reference is a initial or a subsequent mention to an entity in the text and the sentence, respectively.
97	29	Data We evaluated our models on the training, development and test referring expression sets described in Section 3.3.
98	36	Metrics We compared the referring expressions produced by the evaluated models with the goldstandards ones using accuracy and String Edit Distance (Levenshtein, 1966).
100	16	Finally, we lexicalized the original templates with the referring expressions produced by the models and compared them with the original texts in the corpus using accuracy and BLEU score (Papineni et al., 2002) as a measure of fluency.
124	18	For each of the selected instances, we took into account its source triple set and its 6 target texts: one original (randomly chosen) and its versions with the referring expressions generated by each of the 5 models introduced in this study (two baselines, three neural models).
138	33	In comparison with the neural models, NeuralREG+CAtt significantly outperformed the baselines in terms of fluency, whereas the other comparisons between baselines and neural models were not statistically significant.
140	72	Finally, the original texts were rated significantly higher than both baselines in terms of the three metrics, also than NeuralREG+Seq2Seq and NeuralREG+HierAtt in terms of fluency, and than NeuralREG+Seq2Seq in terms of clarity.
146	23	Data The collection of referring expressions used in our experiments was extracted from a novel, delexicalized and publicly available version of the WebNLG corpus (Gardent et al., 2017a,b), where the discourse entities were replaced with general tags for decreasing the data sparsity.
