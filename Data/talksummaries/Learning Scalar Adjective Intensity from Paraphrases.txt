2	41	Hot and scalding are scalar adjectives that describe temperature, but they are not interchangeable because they vary in intensity.
9	21	As with pattern-based approaches for other tasks (such as hypernym discovery (Hearst, 1992)), they are precise but have relatively sparse coverage of comparable adjectives, even when using webscale corpora (de Melo and Bansal, 2013; Ruppenhofer et al., 2014).
12	57	We propose paraphrases as a new source of evidence for the relative intensity of scalar adjectives.
13	59	A paraphrase is a pair of words or phrases with approximately similar meaning, such as really great↔ phenomenal.
14	21	Adjectival paraphrases can be exploited to uncover intensity relationships.
15	16	A paraphrase pair of the above form, where one phrase is composed of an intensifying adverb and an adjective (really great) and the other is a single-word adjective (phenomenal), provides evidence that great < phenomenal.
37	48	A paraphrase of the form RB JJu ↔ JJv – where one phrase is comprised of an adjective modified by an intensifying adverb (RB JJu), and the other is a single-word adjective (JJv) – is evidence that the first adjective is less intense than the second (JJu < JJv).
46	27	For this purpose, we generate a set R of likely intensifying adverbs within PPDB using a bootstrapping approach (Figure 2).
52	26	Finally, the adjective pairs extracted in this second iteration are used to identify additional intensifying adverbs R3, which are added to the final set R = R1 ∪R3 (Round 3).
60	30	There can also be contradictory or cyclic edges in JJGRAPH, as in the example depicted in the JJGRAPH subgraph in Figure 3, where the adverb really connects tasty to lovely and vice versa.
66	27	Its binary features correspond to adverb edges from ju to jv and from jv to ju in JJGRAPH.
70	19	For each adjective pair (ju, jv) with one or more direct edges from ju to jv, a positive training instance for pair (ju, jv) and a negative training instance for pair (jv, ju) are added to the training set.
72	24	The model parameters output by the training process are in a feature weights vector w ∈ R2m (with no bias term) which can be used to generate a paraphrase-based score for each adjective pair: scorepp(ju, jv) = 1 1 + exp−wxuv − 0.5 (1) where xuv is the binary feature vector for adjective pair (ju, jv).
81	31	For example, given pair (good, great), Wu might incorporate evidence from patterns “good, but not great” and “not only good but great”, while Sv might incorporate evidence from the pattern “not great, just good”.
88	23	The sign of the weight encodes sentiment polarity (positive or negative), and the value encodes intensity (e.g. atrocious, with a weight of -5, is more intense than unlikable, with a weight of -3).
89	22	SO-CAL is used to derive a pairwise intensity prediction for adjectives (ju,jv) as follows: scoresocal(ju, jv) = |L(jv)| − |L(ju)|, iff sign(ju) = sign(jv) (4) where L(jv) gives the lexicon weight for jv.
90	39	Note that scoresocal is computed only for adjectives having the same polarity direction in the lexicon; otherwise the score is undefined.
91	27	This is because adjectives belonging to different half scales, such as freezing and steaming, are frequently incomparable in terms of intensity (de Marneffe et al., 2010).
92	45	While the pattern-based and lexicon-based pairwise intensity scores are known to be precise but low-coverage (de Melo and Bansal, 2013; Ruppenhofer et al., 2015), we expect that the paraphrase-based score will produce higher coverage at lower accuracy.
94	44	When combining two metrics x and y to generate a score for a pair (ju, jv), we simply use the first metric x if it can be reliably calculated for the pair, and back off to metric y otherwise.
96	22	When combining three metrics x, y, and z, the combined score is given by: scorex+y+z(ju, jv) = αx · gx(scorex(ju, jv)) + (1− αx) · scorey+z(ju, jv) (6) The criteria for having αx = 1 varies depending on the metric type.
104	20	Global ranking models infer that warm< scalding based on evidence from the other adjective pairs in the scale.
117	32	Each dataset contains ordered sets of scalar adjectives belonging to the same scale.
133	30	For each pair of adjectives along the same scale, we compare the predicted ordering of the pair after global ranking (<, >, or =) to the gold-standard ordering of the pair, and report overall accuracy of the pairwise predictions.
151	27	Further, we find that boosting coverage with a combined metric that incorporates paraphrase evidence produces the highest post-ranking pairwise accuracy scores overall for all three datasets, and the highest average τb and ρ on the Crowd and Wilkinson datasets.
162	20	In some cases the implied answer depends on the relative intensity of adjective modifiers in the question and answer.
169	78	In each exchange, the implied answer (annotated by crowd workers to be yes or no5) depends on the relative intensity relationship between modifiers in the question and answer texts.
175	100	If the pair of adjectives is not scorable, then the predicted answer is no, as the pair could be antonyms or completely unrelated.
185	17	As in the global ranking experiments, the paraphrase-based evidence is complementary to the lexicon-based evidence, and thus the combined scoresocal+pp and scoresocal+pat+pp produce significantly better accuracy than any score in isolation (McNemar’s test, p < .01), and also out-perform the original expected ranking method of de Marneffe et al. (2010) (although they do not beat the best-reported score on this dataset, F-score=0.706 (Kim and de Marneffe, 2013)).
188	23	Thus paraphrases can be successfully used as a complementary source of information for reasoning about adjective intensity.
