{"sections": [{"heading": "1. Introduction", "text": "Recent work on deep latent variable models, such as variational autoencoders (Kingma & Welling, 2014) and generative adversarial networks (Goodfellow et al., 2014), has shown significant progress in learning smooth representations of complex, high-dimensional continuous data such as images. These latent variable representations facilitate the ability to apply smooth transformations in latent space in order to produce complex modifications of generated outputs, while still remaining on the data manifold.\nUnfortunately, learning similar latent variable models of\n*Equal contribution 1Department of Computer Science, New York University 2Facebook AI Research 3School of Engineering and Applied Sciences, Harvard University. Correspondence to: Jake Zhao <jakezhao@cs.nyu.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\ndiscrete structures, such as text sequences or discretized images, remains a challenging problem. Initial work on VAEs for text has shown that optimization is difficult, as the generative model can easily degenerate into a unconditional language model (Bowman et al., 2016). Recent work on generative adversarial networks (GANs) for text has mostly focused on dealing with the non-differentiable objective either through policy gradient methods (Che et al., 2017; Hjelm et al., 2018; Yu et al., 2017) or with the GumbelSoftmax distribution (Kusner & Hernandez-Lobato, 2016). However, neither approach can yet produce robust representations directly.\nIn this work, we extend the adversarial autoencoder (AAE) (Makhzani et al., 2015) to discrete sequences/structures. Similar to the AAE, our model learns an encoder from an input space to an adversarially regularized continuous latent space. However unlike the AAE which utilizes a fixed prior, we instead learn a parameterized prior as a GAN. Like sequence VAEs, the model does not require using policy gradients or continuous relaxations. Like GANs, the model provides flexibility in learning a prior through a parameterized generator.\nThis adversarially regularized autoencoder (ARAE) can further be formalized under the recently-introduced Wasserstein autoencoder (WAE) framework (Tolstikhin et al., 2018), which also generalizes the adversarial autoencoder. This framework connects regularized autoencoders to an optimal transport objective for an implicit generative model. We extend this class of latent variable models to the case of discrete output, specifically showing that the autoencoder cross-entropy loss upper-bounds the total variational distance between the model/data distributions. Under this setup, commonly-used discrete decoders such as RNNs, can be incorporated into the model. Finally to handle non-trivial sequence examples, we consider several different (fixed and learned) prior distributions. These include a standard Gaussian prior used in image models and in the AAE/WAE models, a learned parametric generator acting as a GAN in latent variable space, and a transfer-based parametric generator that is trained to ignore targeted attributes of the input. The last prior can be directly used for unaligned transfer tasks such as sentiment or style transfer.\nExperiments apply ARAE to discretized images and text\nar X\niv :1\n70 6.\n04 22\n3v 3\n[ cs\n.L G\n] 2\n9 Ju\nn 20\n18\nsequences. The latent variable model is able to generate varied samples that can be quantitatively shown to cover the input spaces and to generate consistent image and sentence manipulations by moving around in the latent space via interpolation and offset vector arithmetic. When the ARAE model is trained with task-specific adversarial regularization, the model improves upon strong results on sentiment transfer reported in Shen et al. (2017) and produces compelling outputs on a topic transfer task using only a single shared space. Code is available at https://github.com/jakezhaojb/ARAE."}, {"heading": "2. Background and Notation", "text": "Discrete Autoencoder Define X = Vn to be a set of discrete sequences where V is a vocabulary of symbols. Our discrete autoencoder will consist of two parameterized functions: a deterministic encoder function enc\u03c6 : X 7\u2192 Z with parameters \u03c6 that maps from input space to code space, and a conditional decoder p\u03c8(x | z) over structures X with parameters \u03c8. The parameters are trained based on the cross-entropy reconstruction loss:\nLrec(\u03c6, \u03c8) = \u2212 log p\u03c8(x | enc\u03c6(x))\nThe choice of the encoder and decoder parameterization is problem-specific, for example we use RNNs for sequences. We use the notation, x\u0302 = arg maxx p\u03c8(x | enc\u03c6(x)) for the decoder mode, and call the model distribution P\u03c8 .\nGenerative Adversarial Networks GANs are a class of parameterized implicit generative models (Goodfellow et al., 2014). The method approximates drawing samples from a true distribution z \u223c P\u2217 by instead employing a noise sample s and a parameterized generator function z\u0303 = g\u03b8(s) to produce z\u0303 \u223c Pz. Initial work on GANs implicitly minimized the Jensen-Shannon divergence between the distributions. Recent work on Wasserstein GAN (WGAN) (Arjovsky et al., 2017), replaces this with the Earth-Mover (Wasserstein-1) distance.\nGAN training utilizes two separate models: a generator g\u03b8(s) maps a latent vector from some easy-to-sample noise distribution to a sample from a more complex distribution, and a critic/discriminator fw(z) aims to distinguish real data and generated samples from g\u03b8. Informally, the generator is trained to fool the critic, and the critic to tell real from generated. WGAN training uses the following min-max optimization over generator \u03b8 and critic w,\nmin \u03b8 max w\u2208W\nEz\u223cP\u2217 [fw(z)]\u2212 Ez\u0303\u223cPz [fw(z\u0303)],\nwhere fw : Z 7\u2192 R denotes the critic function, z\u0303 is obtained from the generator, z\u0303 = g\u03b8(s), and P\u2217 and Pz are real and generated distributions. If the critic parameters w are restricted to an 1-Lipschitz function setW , this term correspond to minimizing Wasserstein-1 distance W (P\u2217,Pz).\nWe use a naive approximation to enforce this property by weight-clipping, i.e. w = [\u2212 , ]d (Arjovsky et al., 2017).1"}, {"heading": "3. Adversarially Regularized Autoencoder", "text": "ARAE combines a discrete autoencoder with a GANregularized latent representation. The full model is shown in Figure 1, which produces a learned distribution over the discrete space P\u03c8. Intuitively, this method aims to provide smoother hidden encoding for discrete sequences with a flexible prior. In the next section we show how this simple network can be formally interpreted as a latent variable model under the Wasserstein autoencoder framework.\nThe model consists of a discrete autoencoder regularized with a prior distribution,\nmin \u03c6,\u03c8\nLrec(\u03c6, \u03c8) + \u03bb(1)W (PQ,Pz)\nHere W is the Wasserstein distance between PQ, the distribution from a discrete encoder model (i.e. enc\u03c6(x) where x \u223c P?), and Pz, a prior distribution. As above, theW function is computed with an embedded critic function which is optimized adversarially to the generator and encoder.2\nThe model is trained with coordinate descent across: (1) the encoder and decoder to minimize reconstruction, (2) the critic function to approximate the W term, (3) the encoder adversarially to the critic to minimize W :\n1)min \u03c6,\u03c8\nLrec(\u03c6, \u03c8) = Ex\u223cP? [\u2212 log p\u03c8(x | enc\u03c6(x))]\n2) max w\u2208W\nLcri(w) = Ex\u223cP? [fw(enc\u03c6(x))]\u2212 Ez\u0303\u223cPz [fw(z\u0303)]\n3)min \u03c6\nLenc(\u03c6) = Ex\u223cP? [fw(enc\u03c6(x))]\u2212 Ez\u0303\u223cPz [fw(z\u0303)]\nThe full training algorithm is shown in Algorithm 1. Empirically we found that the choice of the prior distribution Pz strongly impacted the performance of the model. The simplest choice is to use a fixed distribution such as a Gaussian N (0, I), which yields a discrete version of the adversarial autoencoder (AAE). However in practice this choice is seemingly too constrained and suffers from modecollapse.3\nInstead we exploit the adversarial setup and use learned prior parameterized through a generator model. This is analogous to the use of learned priors in VAEs (Chen et al., 2017; Tomczak & Welling, 2018). Specifically we introduce a generator model, g\u03b8(s) over noise s \u223c N (0, I) to act as an\n1While we did not experiment with enforcing the Lipschitz constraint via gradient penalty (Gulrajani et al., 2017) or spectral normalization (Miyato et al., 2018), other researchers have found slight improvements by training ARAE with the gradient-penalty version of WGAN (private correspondence).\n2Other GANs could be used for this optimization. Experimentally we found that WGANs to be more stable than other models.\n3We note that recent work has successfully utilized AAE for text by instead employing a spherical prior (C\u00edfka et al., 2018).\nAlgorithm 1 ARAE Training for each training iteration do\n(1) Train the encoder/decoder for reconstruction (\u03c6, \u03c8) Sample {x(i)}mi=1 \u223c P? and compute z(i) = enc\u03c6(x(i)) Backprop loss, Lrec = \u2212 1m \u2211m i=1 log p\u03c8(x (i) | z(i))\n(2) Train the critic (w) Sample {x(i)}mi=1 \u223c P? and {s(i)}mi=1 \u223c N (0, I) Compute z(i) = enc\u03c6(x(i)) and z\u0303(i) = g\u03b8(z(i)) Backprop loss \u2212 1\nm \u2211m i=1 fw(z (i)) + 1 m \u2211m i=1 fw(z\u0303 (i))\nClip critic w to [\u2212 , ]d.\n(3) Train the encoder/generator adversarially (\u03c6, \u03b8) Sample {x(i)}mi=1 \u223c P? and {s(i)}mi=1 \u223c N (0, I) Compute z(i) = enc\u03c6(x(i)) and z\u0303(i) = g\u03b8(s(i)). Backprop loss 1\nm \u2211m i=1 fw(z (i))\u2212 1 m \u2211m i=1 fw(z\u0303\n(i)) end for\nimplicit prior distribution Pz.4 We optimize its parameters \u03b8 as part of training in Step 3.\nAlgorithm 2 ARAE Transfer Extension Each loop additionally:\n(2b) Train attribute classifier (u) Sample {x(i)}mi=1 \u223c P?, lookup y(i), and compute z(i) = enc\u03c6(x(i)) Backprop loss \u2212 1m \u2211m i=1 log pu(y\n(i)|z(i)) (3b) Train the encoder adversarially (\u03c6) Sample {x(i)}mi=1 \u223c P?, lookup y(i), and compute z(i) = enc\u03c6(x(i)) Backprop loss \u2212 1m \u2211m i=1 log pu(1\u2212 y(i) | z(i))\nExtension: Unaligned Transfer Regularization of the latent space makes it more adaptable for direct continuous optimization that would be difficult over discrete sequences. For example, consider the problem of unaligned transfer,\n4The downside of this approach is that the latent variable z is now much less constrained. However we find experimentally that using a a simple MLP for g\u03b8 significantly regularizes the encoder RNN.\nwhere we want to change an attribute of a discrete input without aligned examples, e.g. to change the topic or sentiment of a sentence. Define this attribute as y and redefine the decoder to be conditional p\u03c8(x | z, y). To adapt ARAE to this setup, we modify the objective to learn to remove attribute distinctions from the prior (i.e. we want the prior to encode all the relevant information except about y). Following similar techniques from other domains, notably in images (Lample et al., 2017) and video modeling (Denton & Birodkar, 2017), we introduce a latent space attribute classifier:\nmin \u03c6,\u03c8,\u03b8\nLrec(\u03c6, \u03c8) + \u03bb(1)W (PQ,Pz)\u2212 \u03bb(2)Lclass(\u03c6, u)\nwhere Lclass(\u03c6, u) is the loss of a classifier pu(y | z) from latent variable to labels (in our experiments we always set \u03bb(2) = 1). This requires two more update steps: (2b) training the classifier, and (3b) adversarially training the encoder to this classifier. This algorithm is shown in Algorithm 2."}, {"heading": "4. Theoretical Properties", "text": "Standard GANs implicitly minimize a divergence measure (e.g. f -divergence or Wasserstein distance) between the true/model distributions. In our case however, we implicitly minimize the divergence between learned code distributions, and it is not clear if this training objective is matching the distributions in the original discrete space. Tolstikhin et al. (2018) recently showed that this style of training is minimizing the Wasserstein distance between the data distribution P? and the model distribution P\u03c8 with latent variables (with density p\u03c8(x) = \u222b z p\u03c8(x | z) p(z) dz).\nIn this section we apply the above result to the discrete case and show that the ARAE loss minimizes an upper bound on the total variation distance between P? and P\u03c8 . Definition 1 (Kantorovich\u2019s formulation of optimal transport). Let P?,P\u03c8 be distributions over X , and further let c(x,y) : X \u00d7X \u2192 R+ be a cost function. Then the optimal transport (OT) problem is given by\nWc(P?,P\u03c8) = inf \u0393\u2208P(x\u223cP?,y\u223cP\u03c8) Ex,y\u223c\u0393[c(x,y)]\nwhere P(x \u223c P?,y \u223c P\u03c8) is the set of all joint distributions of (x,y) with marginals P? and P\u03c8 .\nIn particular, if c(x,y) = \u2016x \u2212 y\u2016pp then Wc(P?,P\u03c8) 1 p is the Wasserstein-p distance between P? and P\u03c8. Now suppose we utilize a latent variable model to fit the data, i.e. z \u223c Pz,x \u223c P\u03c8(x | z). Then Tolstikhin et al. (2018) prove the following theorem:\nTheorem 1. Let G\u03c8 : Z \u2192 X be a deterministic function (parameterized by \u03c8) from the latent space Z to data space X that induces a dirac distribution P\u03c8(x | z) on X , i.e. p\u03c8(x | z) = 1{x = G\u03c8(z)}. Let Q(z | x) be\nany conditional distribution on Z with density pQ(z | x). Define its marginal to be PQ, which has density pQ(x) =\u222b x pQ(z | x) p?(x)dx. Then,\nWc(P?,P\u03c8) = inf Q(z | x):PQ=Pz EP?EQ(z | x)[c(x, G\u03c8(z))]\nTheorem 1 essentially says that learning an autoencoder can be interpreted as learning a generative model with latent variables, as long as we ensure that the marginalized encoded space is the same as the prior. This provides theoretical justification for adversarial autoencoders (Makhzani et al., 2015), and Tolstikhin et al. (2018) used the above to train deep generative models of images by minimizing the Wasserstein-2 distance (i.e. squared loss between real/generated images). We now apply Theorem 1 to discrete autoencoders trained with cross-entropy loss.\nCorollary 1 (Discrete case). Suppose x \u2208 X where X is the set of all one-hot vectors of length n, and let f\u03c8 : Z \u2192 \u2206n\u22121 be a deterministic function that goes from the latent space Z to the n \u2212 1 dimensional simplex \u2206n\u22121. Further let G\u03c8 : Z \u2192 X be a deterministic function such that G\u03c8(z) = arg maxw\u2208X w\n>f\u03c8(z), and as above let P\u03c8(x | z) be the dirac distribution derived from G\u03c8 such that p\u03c8(x | z) = 1{x = G\u03c8(z)}. Then the following is an upper bound on \u2016P\u03c8 \u2212 P?\u2016TV, the total variation distance between P? and P\u03c8:\ninf Q(z | x):PQ=Pz\nEP?EQ(z | x) [ \u2212 2\nlog 2 logx>f\u03c8(z) ] The proof is in Appendix A. For natural language we have n = |V|m and therefore X is the set of sentences of length m, where m is the maximum sentence length (shorter sentences are padded if necessary). Then the total variational (TV) distance is given by\n\u2016P\u03c8 \u2212 P?\u2016TV = 1\n2 \u2211 x\u2208Vm |p\u03c8(x)\u2212 p?(x)|\nThis is an interesting alternative to the usual maximum likelihood approach which instead minimizes KL(P?,P\u03c8).5 It is also clear that \u2212 logx>f\u03c8(z) = \u2212 log p\u03c8(x | z), the standard autoencoder cross-entropy loss at the sentence level with f\u03c8 as the decoder. As the above objective is hard to minimize directly, we follow Tolstikhin et al. (2018) and consider an easier objective by (i) restricting Q(z | x) to a family of distributions induced by a deterministic encoder parameterized by \u03c6, and (ii) using a Langrangian relaxation of the constraint PQ = Pz. In particular, letting Q(z | x) = 1{z = enc\u03c6(x)} be the dirac distribution induced by a deterministic encoder (with associated marginal P\u03c6), the objective is given by\n5The relationship between KL-divergence and total variation distance is also given by Pinsker\u2019s inquality, which states that 2\u2016P\u03c8 \u2212 P?\u20162TV \u2264 KL(P?,P\u03c8).\nmin \u03c6,\u03c8\nEP? [\u2212 log p\u03c8(x | enc\u03c6(z))] + \u03bbW (P\u03c6,Pz)\nNote that our minimizing the Wasserstein distance in the latent space W (P\u03c6,Pz) is independent from the Wassertein distance minimization in the output space in WAEs. Finally, instead of using a fixed prior (which led to mode-collapse in our experiments) we parameterize Pz implicitly by transforming a simple random variable with a generator (i.e. s \u223c N (0, I), z = g\u03b8(s)). This recovers the ARAE objective from the previous section.\nWe conclude this section by noting that while the theoretical formalization of the AAE as a latent variable model was an important step, in practice there are many approximations made to the actual optimal transport objective. Meaningfully quantifying (and reducing) such approximation gaps remains an avenue for future work."}, {"heading": "5. Methods and Architectures", "text": "We experiment with ARAE on three setups: (1) a small model using discretized images trained on the binarized version of MNIST, (2) a model for text sequences trained on the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), and (3) a model trained for text transfer trained on the Yelp/Yahoo datasets for unaligned sentiment/topic transfer. For experiments using a learned prior, the generator architecture uses a low dimensional s with a Gaussian prior s \u223c N (0, I), and maps it to z using an MLP g\u03b8. The critic fw is also parameterized as an MLP.\nThe image model encodes/decodes binarized images. Here X = {0, 1}n where n is the image size. The encoder used is an MLP mapping from {0, 1}n 7\u2192 Rm, enc\u03c6(x) = MLP(x;\u03c6) = z. The decoder predicts each pixel in x with as a parameterized logistic regression, p\u03c8(x | z) =\u220fn j=1 \u03c3(h)\nxj (1\u2212 \u03c3(h))1\u2212xj where h = MLP(z;\u03c8). The text model uses a recurrent neural network (RNN) for both the encoder and decoder. Here X = Vn where n is the sentence length and V is the vocabulary of the underlying language. We define enc\u03c6(x) = z to be the last hidden state of an encoder RNN. For decoding we feed z as an additional input to the decoder RNN at each time step, and calculate the distribution over V at each time step via softmax, p\u03c8(x | z) = \u220fn j=1 softmax(Whj + b)xj where W and b are parameters (part of \u03c8) and hj is the decoder RNN hidden state. To be consistent with Corollary 1 we need to find the highest-scoring sequence x\u0302 under this distribution during decoding, which is intractable in general. Instead we approximate this with greedy search. The text transfer model uses the same architecture as the text model but extends it with a classifier pu(y | z) which is modeled using an MLP and trained to minimize cross-entropy.\nWe further compare our approach with a standard autoencoder (AE) and the cross-aligned autoencoder (Shen et al.,\n2017) for transfer. In both our ARAE and standard AE experiments, the encoder output is normalized to lie on the unit sphere, and the generator output is bounded to lie in (\u22121, 1)n by the tanh function at output layer. Note, learning deep latent variable models for text sequences has been a significantly more challenging empirical problem than for images. Standard models such as VAEs suffer from optimization issues that have been widely documented. We performed experiments with recurrent VAE, introduced by (Bowman et al., 2016), as well as the adversarial autoencoder (AAE) (Makhzani et al., 2015), both with Gaussian priors. We found that neither model was able to learn meaningful latent representations\u2014the VAE simply ignored the latent code and the AAE experienced mode-collapse and repeatedly generated the same samples.6 Appendix F includes detailed descriptions of the hyperparameters, model architecture, and training regimes."}, {"heading": "6. Experiments", "text": ""}, {"heading": "6.1. Distributional Coverage", "text": "Section 4 argues that P\u03c8 is trained to approximate the true data distribution over discrete sequences P?. While it is difficult to test for this property directly (as is the case with most GAN models), we can take samples from model to test the fidelity and coverage of the data space. Figure 2 shows a set of samples from discretized MNIST and Appendix C shows a set of generations from the text ARAE.\nA common quantitative measure of sample quality for generative models is to evaluate a strong surrogate model trained on its generated samples. While there are pitfalls of this style of evaluation methods (Theis et al., 2016), it has pro-\n6However there have been some recent successes training such models, as noted in the related works section\nvided a starting point for image generation models. Here we use a similar method for text generation, which we call reverse perplexity. We generate 100k samples from each of the models, train an RNN language model on generated samples and evaluate perplexity on held-out data.7 While similar metrics for images (e.g. Parzen windows) have been shown to be problematic, we argue that this is less of an issue for text as RNN language models achieve state-of-theart perplexities on text datasets. We also calculate the usual \u201cforward\u201d perplexity by training an RNN language model on real data and testing on generated data. This measures the fluency of the generated samples, but cannot detect modecollapse, a common issue in training GANs (Arjovsky & Bottou, 2017; Hu et al., 2018).\nTable 1 shows these metrics for (i) ARAE, (ii) an autoencoder (AE),8 (iii) an RNN language model (LM), and (iv) the real training set. We further find that with a fixed prior, the reverse perplexity of an AAE-style text model (Makhzani et al., 2015) was quite high (980) due to modecollapse. All models are of the same size to allow for fair comparison. Training directly on real data (understandably) outperforms training on generated data by a large margin. Surprisingly however, training on ARAE samples outperforms training on LM/AE samples in terms of reverse perplexity."}, {"heading": "6.2. Unaligned Text Style Transfer", "text": "Next we evaluate the model in the context of a learned adversarial prior, as described in Section 3. We experiment with two unaligned text transfer tasks: (i) transfer of sentiment on the Yelp corpus, and (ii) topic on the Yahoo corpus (Zhang\n7We also found this metric to be helpful for early-stopping. 8To \u201csample\u201d from an AE we fit a multivariate Gaussian to the code space after training and generate code vectors from this Gaussian to decode back into sentence space.\net al., 2015). For sentiment we follow the setup of Shen et al. (2017) and split the Yelp corpus into two sets of unaligned positive and negative reviews. We train ARAE with two separate decoder RNNs, one for positive, p(x | z, y = 1), and one for negative sentiment p(x | z, y = 0), and incorporate adversarial training of the encoder to remove sentiment information from the prior. Transfer corresponds to encoding sentences of one class and decoding, greedily, with the opposite decoder. Experiments compare against the crossaligned AE of Shen et al. (2017) and also an AE trained without the adversarial regularization. For ARAE, we experimented with different \u03bb(1) weighting on the adversarial loss (see section 4) with \u03bb(1)a = 1, \u03bb (1) b = 10. Both use \u03bb\n(2) = 1. Empirically the adversarial regularization enhances transfer and perplexity, but tends to make the transferred text less similar to the original, compared to the AE. Randomly selected example sentences are shown in Table 2 and additional outputs are available in Appendix G.\nTable 3 (top) shows quantitative evaluation. We use four automatic metrics: (i) Transfer: how successful the model is at altering sentiment based on an automatic classifier (we use the fastText library (Joulin et al., 2017)); (ii) BLEU: the consistency between the transferred text and the original; (iii) Forward PPL: the fluency of the generated text; (iv) Reverse PPL: measuring the extent to which the generations are representative of the underlying data distribution. Both perplexity numbers are obtained by training an RNN language model. Table 3 (bottom) shows human evaluations on the cross-aligned AE and our best ARAE model. We randomly select 1000 sentences (500/500 positive/negative), obtain the corresponding transfers from both models, and ask crowdworkers to evaluate the sentiment (Positive/Neutral/Negative) and naturalness (1-5, 5 being most natural) of the transferred sentences. We create a separate task in which we show the original and the transferred sentences, and ask them to evaluate the similarity based on sentence structure (1-5, 5 being most similar). We explicitly requested that the reader disregard sentiment in similarity\nassessment.\nThe same method can be applied to other style transfer tasks, for instance the more challenging Yahoo QA data (Zhang et al., 2015). For Yahoo we chose 3 relatively distinct topic classes for transfer: SCIENCE & MATH, ENTERTAINMENT & MUSIC, and POLITICS & GOVERNMENT. As the dataset contains both questions and answers, we separated our experiments into titles (questions) and replies (answers). Randomly-selected generations are shown in Table 4. See Appendix G for additional generation examples."}, {"heading": "6.3. Semi-Supervised Training", "text": "Latent variable models can also provide an easy method for semi-supervised training. We use a natural language inference task to compare semi-supervised ARAE with other training methods. Results are shown in Table 5. The full SNLI training set contains 543k sentence pairs, and we use supervised sets of 120k (Medium), 59k (Small), and 28k (Tiny) and use the rest of the training set for unlabeled training. As a baseline we use an AE trained on the additional\ndata, similar to the setting explored in Dai & Le (2015). For ARAE we use the subset of unsupervised data of length < 15 (i.e. ARAE is trained on less data than AE for unsupervised training). The results are shown in Table 5. Training on unlabeled data with an AE objective improves upon a model just trained on labeled data. Training with adversarial regularization provides further gains."}, {"heading": "7. Discussion", "text": "Impact of Regularization on Discrete Encoding We further examine the impact of adversarial regularization on the encoded representation produced by the model as it is trained. Figure 3 (left), shows a sanity check that the `2 norm of encoder output z and prior samples z\u0303 converge quickly in ARAE training. The middle plot compares the trace of the covariance matrix between these terms as training progresses. It shows that variance of the encoder and the prior match after several epochs.\nSmoothness and Reconstruction We can also assess the \u201csmoothness\u201d of the encoder model learned ARAE (Rifai et al., 2011). We start with a simple proxy that a smooth encoder model should map similar sentences to similar z values. For 250 sentences, we calculate the average cosine similarity of 100 randomly-selected sentences within an edit-distance of at most 5 to the original. The graph in Figure 3 (right) shows that the cosine similarity of nearby sentences is quite high for ARAE compared to a standard AE and increases in early rounds of training. To further test this property, we feed noised discrete input to the encoder and (i) calculate the score given to the original input, and\n(ii) compare the resulting reconstructions. Figure 4 (right) shows results for text where k words are first permuted in each sentence. We observe that ARAE is able to map a noised sentence to a natural sentence (though not necessarily the denoised sentence). Figure 4 (left) shows empirical results for these experiments. We obtain the reconstruction error (negative log likelihood) of the original non-noised sentence under the decoder, utilizing the noised code. We find that when k = 0 (i.e. no swaps), the regular AE better reconstructs the exact input. However, as the number of swaps pushes the input further away, ARAE is more likely to produce the original sentence. (Note that unlike denoising autoencoders which require a domain-specific noising function (Hill et al., 2016; Vincent et al., 2008), the ARAE is not explicitly trained to denoise an input.)\nManipulation through the Prior An interesting property of latent variable models such as VAEs and GANs is the ability to manipulate output samples through the prior. In particular, for ARAE, the Gaussian form of the noise sample s induces the ability to smoothly interpolate between outputs by exploiting the structure. While language models may provide a better estimate of the underlying probability space, constructing this style of interpolation would require combinatorial search, which makes this a useful feature of latent variable text models. In Appendix D we show interpolations from for the text model, while Figure 2 (bottom) shows the interpolations for discretized MNIST ARAE.\nA related property of GANs is the ability to move in the latent space via offset vectors.9 To experiment with this property we generate sentences from the ARAE and compute vector transforms in this space to attempt to change main verbs, subjects and modifier (details in Appendix E). Some examples of successful transformations are shown in Figure 5 (bottom). Quantitative evaluation of the success of the vector transformations is given in Figure 5 (top).\n9Similar to the case with word vectors (Mikolov et al., 2013), Radford et al. (2016) observe that when the mean latent vector for \u201cmen with glasses\u201d is subtracted from the mean latent vector for \u201cmen without glasses\u201d and applied to an image of a \u201cwoman without glasses\u201d, the resulting image is that of a \u201cwoman with glasses\u201d."}, {"heading": "8. Related Work", "text": "While ideally autoencoders would learn latent spaces which compactly capture useful features that explain the observed data, in practice they often learn a degenerate identity mapping where the latent code space is free of any structure, necessitating the need for some regularization on the latent space. A popular approach is to regularize through an explicit prior on the code space and use a variational approximation to the posterior, leading to a family of models called variational autoencoders (VAE) (Kingma & Welling, 2014; Rezende et al., 2014). Unfortunately VAEs for discrete text sequences can be challenging to train\u2014for example, if the training procedure is not carefully tuned with techniques like word dropout and KL annealing (Bowman et al., 2016), the decoder simply becomes a language model and ignores the latent code. However there have been some recent successes through employing convolutional decoders (Yang et al., 2017; Semeniuta et al., 2017), training the latent representation as a topic model (Dieng et al., 2017; Wang et al., 2018), using the von Mises\u2013Fisher distribution (Guu et al., 2017), and combining VAE with iterative inference (Kim et al., 2018). There has also been some work on making the prior more flexible through explicit parameterization (Chen et al., 2017; Tomczak & Welling, 2018). A notable technique is adversarial autoencoders (AAE) (Makhzani et al., 2015) which attempt to imbue the model with a more flexible prior implicitly through adversarial training. Recent work on Wasserstein autoencoders (Tolstikhin et al., 2018) provides a theoretical foundation for the AAE and shows that AAE minimizes the Wasserstein distance between the data/model distributions.\nThe success of GANs on images have led many researchers to consider applying GANs to discrete data such as text. Policy gradient methods are a natural way to deal with the resulting non-differentiable generator objective when training directly in discrete space (Glynn, 1987; Williams, 1992). When trained on text data however, such methods often require pre-training/co-training with a maximum likelihood (i.e. language modeling) objective (Che et al., 2017; Yu et al., 2017; Li et al., 2017). Another direction of work has been through reparameterizing the categorical distribution with the Gumbel-Softmax trick (Jang et al., 2017; Maddison et al., 2017)\u2014while initial experiments were encouraging on a synthetic task (Kusner & Hernandez-Lobato, 2016), scaling them to work on natural language is a challenging open problem. There have also been recent related approaches that work directly with the soft outputs from a generator (Gulrajani et al., 2017; Rajeswar et al., 2017; Shen et al., 2017; Press et al., 2017). For example, Shen et al. (2017) exploits adversarial loss for unaligned style transfer between text by having the discriminator act on the RNN hidden states and using the soft outputs at each step as input to an RNN generator. Our approach instead works entirely in fixed-dimensional continuous space and does not require utilizing RNN hidden states directly. It is therefore also different from methods that discriminate in the joint latent/data space, such as ALI (Vincent Dumoulin, 2017) and BiGAN (Donahue et al., 2017). Finally, our work adds to the recent line of work on unaligned style transfer for text (Hu et al., 2017; Mueller et al., 2017; Li et al., 2018; Prabhumoye et al., 2018; Yang et al., 2018)."}, {"heading": "9. Conclusion", "text": "We present adversarially regularized autoencoders (ARAE) as a simple approach for training a discrete structure autoencoder jointly with a code-space generative adversarial network. Utilizing the Wasserstein autoencoder framework (Tolstikhin et al., 2018), we also interpret ARAE as learning a latent variable model that minimizes an upper bound on the total variation distance between the data/model distributions. We find that the model learns an improved autoencoder and exhibits a smooth latent space, as demonstrated by semisupervised experiments, improvements on text style transfer, and manipulations in the latent space.\nWe note that (as has been frequently observed when training GANs) the proposed model seemed to be quite sensitive to hyperparameters, and that we only tested our model on simple structures such as binarized digits and short sentences. C\u00edfka et al. (2018) recently evaluated a suite of sentence generation models and found that models are quite sensitive to their training setup, and that different models do well on different metrics. Training deep latent variable models that can robustly model complex discrete structures (e.g. documents) remains an important open issue in the field."}, {"heading": "Acknowledgements", "text": "We thank Sam Wiseman, Kyunghyun Cho, Sam Bowman, Joan Bruna, Yacine Jernite, Mart\u00edn Arjovsky, Mikael Henaff, and Michael Mathieu for fruitful discussions. We are particularly grateful to Tianxiao Shen for providing the results for style transfer. We also thank the NVIDIA Corporation for the donation of a Titan X Pascal GPU that was used for this research. Yoon Kim was supported by a gift from Amazon AWS Machine Learning Research."}, {"heading": "A. Proof of Corollary 1", "text": "Corollary (Discrete case). Suppose x \u2208 X where X is the set of all one-hot vectors of length n, and let f\u03c8 : Z \u2192 \u2206n\u22121 be a deterministic function that goes from the latent space Z to the n \u2212 1 dimensional simplex \u2206n\u22121. Further let G\u03c8 : Z \u2192 X be a deterministic function such that G\u03c8(z) = arg maxw\u2208X w\n>f\u03c8(z), and as above let P\u03c8(x | z) be the dirac distribution derived from G\u03c8 such that p\u03c8(x | z) = 1{x = G\u03c8(z)}. Then the following is an upper bound on \u2016P\u03c8 \u2212 P?\u2016TV, the total variation distance between P? and P\u03c8 .\ninf Q(z | x):PQ=Pz\nEP?EQ(z | x) [ \u2212 2\nlog 2 logx>f\u03c8(z) ] Proof. Let our cost function be c(x,y) = 1{x 6= y}. We first note that for all x, z\nlog 21{x 6= arg max w\u2208X w>f\u03c8(z)} < \u2212 logx>f\u03c8(z)\nThis holds since if 1{x 6= arg maxw\u2208X w>f\u03c8(z)} = 1, we have x>f\u03c8(z) < 0.5, and \u2212 logx>f\u03c8(z) > \u2212 log 0.5 = log 2. If on the other hand x = arg maxw\u2208X w\n>f\u03c8(z), then the LHS is 0 and RHS is always postive since f\u03c8(z) \u2208 \u2206n\u22121. Then,\ninf Q:PQ=Pz\nEP?EQ(z | x)[\u2212 2\nlog 2 logx>f\u03c8(z)]\n> inf Q:PQ=Pz EP?EQ(z | x)[21{x 6= arg max w\u2208X w>f\u03c8(z)}]\n=2 inf Q:PQ=Pz\nEP?EQ(z | x)[1{x 6= G\u03c8(z)}]\n=2 inf Q:PQ=Pz EP?EQ(z | x)[c(x, G\u03c8(z))]\n= 2Wc(P?,P\u03c8) = \u2016P? \u2212 P\u03c8\u2016TV\nThe fifth line follows from Theorem 1, and the last equality uses the well-known correspondence between total variation distance and optimal transport with the indicator cost function (Gozlan & L\u00e9onard, 2010)."}, {"heading": "B. Optimality Property", "text": "One can interpret the ARAE framework as a dual pathway network mapping two distinct distributions into a similar one; enc\u03c6 and g\u03b8 both output code vectors that are kept similar in terms of Wasserstein distance as measured by the critic. We provide the following proposition showing that under our parameterization of the encoder and the generator, as the Wasserstein distance converges, the encoder distribution (PQ) converges to the generator distribution (Pz), and further, their moments converge.\nThis is ideal since under our setting the generated distribution is simpler than the encoded distribution, because the\ninput to the generator is from a simple distribution (e.g. spherical Gaussian) and the generator possesses less capacity than the encoder. However, it is not so simple that it is overly restrictive (e.g. as in VAEs). Empirically we observe that the first and second moments do indeed converge as training progresses (Section 7).\nProposition 1. Let P be a distribution on a compact set \u03c7, and (Pn)n\u2208N be a sequence of distributions on \u03c7. Further suppose that W (Pn,P)\u2192 0. Then the following statements hold:\n(i) Pn P (i.e. convergence in distribution).\n(ii) All moments converge, i.e. for all k > 1, k \u2208 N,\nEX\u223cPn [ d\u220f i=1 Xpii ] \u2192 EX\u223cP [ d\u220f i=1 Xpii ] for all p1, . . . , pd such that \u2211d i=1 pi = k\nProof. (i) has been proved in (Villani, 2008) Theorem 6.9.\nFor (ii), using The Portmanteau Theorem, (i) is equivalent to the following statement:\nEX\u223cPn [f(X)]\u2192 EX\u223cP[f(X)] for all bounded and continuous function f : Rd \u2192 R, where d is the dimension of the random variable.\nThe k-th moment of a distribution is given by\nE [ d\u220f i=1 Xpii ] such that d\u2211 i=1 pi = k\nOur encoded code is bounded as we normalize the encoder output to lie on the unit sphere, and our generated code is also bounded to lie in (\u22121, 1)n by the tanh function. Hence f(X) = \u220fd i=1X qi i is a bounded continuous function for all qi \u2265 0. Therefore,\nEX\u223cPn [ d\u220f i=1 Xpii ] \u2192 EX\u223cP [ d\u220f i=1 Xpii ] where \u2211d i=1 pi = k"}, {"heading": "C. Sample Generations", "text": "In Figure 6 we show some generated samples from the ARAE, AE, and a LM."}, {"heading": "D. Sentence Interpolations", "text": "In Figure 7 we show generations from interpolated latent vectors. Specifically, we sample two points z0 and z1 from\nARAE Samples A woman preparing three fish . A woman is seeing a man in the river . There passes a woman near birds in the air . Some ten people is sitting through their office . The man got stolen with young dinner bag . Monks are running in court . The Two boys in glasses are all girl . The man is small sitting in two men that tell a children . The two children are eating the balloon animal . A woman is trying on a microscope . The dogs are sleeping in bed .\nAE Samples Two Three woman in a cart tearing over of a tree . A man is hugging and art . The fancy skier is starting under the drag cup in . A dog are <unk> a A man is not standing . The Boys in their swimming . A surfer and a couple waiting for a show . A couple is a kids at a barbecue . The motorcycles is in the ocean loading I \u2019s bike is on empty The actor was walking in a a small dog area . no dog is young their mother\nLM Samples a man walking outside on a dirt road , sitting on the dock . A large group of people is taking a photo for Christmas and at night . Someone is avoiding a soccer game . The man and woman are dressed for a movie . Person in an empty stadium pointing at a mountain . Two children and a little boy are <unk> a man in a blue shirt . A boy rides a bicycle . A girl is running another in the forest . the man is an indian women .\nFigure 6: Text samples generated from ARAE, a simple AE, and from a baseline LM trained on the same data. To generate from an AE we fit a multivariate Gaussian to the learned code space and generate code vectors from this Gaussian.\nA man is on the corner in a sport area . A man is on corner in a road all . A lady is on outside a racetrack . A lady is outside on a racetrack . A lot of people is outdoors in an urban setting . A lot of people is outdoors in an urban setting . A lot of people is outdoors in an urban setting .\nA man is on a ship path with the woman . A man is on a ship path with the woman . A man is passing on a bridge with the girl . A man is passing on a bridge with the girl . A man is passing on a bridge with the girl . A man is passing on a bridge with the dogs . A man is passing on a bridge with the dogs .\nA man in a cave is used an escalator .\nA man in a cave is used an escalator A man in a cave is used chairs . A man in a number is used many equipment A man in a number is posing so on a big rock . People are posing in a rural area . People are posing in a rural area.\nFigure 7: Sample interpolations from the ARAE. Constructed by linearly interpolating in the latent space and decoding to the output space. Word changes are highlighted in black.\np(z) and construct intermediary points z\u03bb = \u03bbz1 + (1 \u2212 \u03bb)z0. For each we generate the argmax output x\u0303\u03bb.\nE. Vector Arithmetic We generate 1 million sentences from the ARAE and parse the sentences to obtain the main verb, subject, and modifier. Then for a given sentence, to change the main verb we subtract the mean latent vector (t) for all other sentences with the same main verb (in the first example in Figure 5 this would correspond to all sentences that had \u201csleeping\u201d as the main verb) and add the mean latent vector for all sentences that have the desired transformation (with the running example this would be all sentences whose main verb was \u201cwalking\u201d). We do the same to transform the subject and the modifier. We decode back into sentence space with the transformed latent vector via sampling from p\u03c8(g(z + t)). Some examples of successful transformations are shown in Figure 5 (right). Quantitative evaluation of the success of the vector transformations is given in Figure 5 (left). For each original vector z we sample 100 sentences from p\u03c8(g(z + t)) over the transformed new latent vector and\nconsider it a match if any of the sentences demonstrate the desired transformation. Match % is proportion of original vectors that yield a match post transformation. As we ideally want the generated samples to only differ in the specified transformation, we also calculate the average word precision against the original sentence (Prec) for any match."}, {"heading": "F. Experimental Details", "text": "MNIST experiments\n\u2022 The encoder is a three-layer MLP, 784-800-400-100.\n\u2022 Additive Gaussian noise is injected into c then gradually decayed to 0.\n\u2022 The decoder is a four-layer MLP, 100-400-800-1000-784\n\u2022 The autoencoder is optimized by Adam, with learning rate 5e-04.\n\u2022 An MLP generator 32-64-100-150-100. \u2022 An MLP critic 100-100-60-20-1with weight clip-\nping = 0.05. The critic is trained 10 iterations in every loop.\n\u2022 GAN is optimized by Adam, with learning rate 5e-04 on the generator and 5e-05 on the critic.\n\u2022 Weighing factor \u03bb(1) = 0.2.\nText experiments\n\u2022 The encoder is an one-layer LSTM with 300 hidden units.\n\u2022 Additive Gaussian noise is injected into c then gradually decayed to 0.\n\u2022 The decoder is an one-layer LSTM with 300 hidden units.\n\u2022 The LSTM state vector is augmented by the hidden code c at every decoding time step, before forwarding into the output softmax layer.\n\u2022 The word embedding is of size 300. \u2022 The autoencoder is optimized by SGD with learning\nrate 1. A grad clipping on the autoencoder, with max grad_norm set to 1.\n\u2022 An MLP generator 100-300-300. \u2022 An MLP critic 300-300-1 with weight clipping =\n0.01. The critic is trained 5 iterations in every loop.\n\u2022 GAN is optimized by Adam, with learning rate 5e-05 on the generator, and 1e-05 on the critic.\nSemi-supervised experiments\nThe following changes are made based on the SNLI experiments:\n\u2022 Larger network to GAN components: an MLP generator 100-150-300-500 and an MLP critic 500-500-150-80-20-1 with weight clipping factor = 0.02.\nYelp/Yahoo transfer\n\u2022 An MLP style adversarial classifier 300-200-100, trained by SGD learning rate 0.1.\n\u2022 Weighing factor from both adversarial forces \u03bb(1)a = 1, \u03bb\n(1) b = 10."}, {"heading": "G. Style Transfer Samples", "text": "In the following pages we show randomly sampled style transfers from the Yelp/Yahoo corpus.\nYahoo Topic Transfer on Questions\nYahoo Topic Transfer on Answers"}], "year": 2018, "references": [{"title": "Towards Principled Methods for Training Generative Adversarial Networks", "authors": ["M. Arjovsky", "L. Bottou"], "venue": "In Proceedings of ICML,", "year": 2017}, {"title": "A large annotated corpus for learning natural language inference", "authors": ["S.R. Bowman", "G. Angeli", "C. Potts", "C.D. Manning"], "venue": "In Proceedings of EMNLP,", "year": 2015}, {"title": "Generating Sentences from a Continuous Space. 2016", "authors": ["S.R. Bowman", "L. Vilnis", "O. Vinyals", "A.M. Dai", "R. Jozefowicz", "S. Bengio"], "year": 2016}, {"title": "Variational Lossy Autoencoder", "authors": ["X. Chen", "D.P. Kingma", "T. Salimans", "Y. Duan", "P. Dhariwal", "J. Schulman", "I. Sutskever", "P. Abbeel"], "venue": "In Proceedings of ICLR,", "year": 2017}, {"title": "Eval all, trust a few, do wrong to none: Comparing sentence generation models", "authors": ["O. C\u00edfka", "A. Severyn", "E. Alfonseca", "K. Filippova"], "year": 2018}, {"title": "Semi-supervised sequence learning", "authors": ["A.M. Dai", "Q.V. Le"], "venue": "In Proceedings of NIPS,", "year": 2015}, {"title": "Unsupervised learning of disentangled representations from video", "authors": ["E. Denton", "V. Birodkar"], "venue": "In Proceedings of NIPS,", "year": 2017}, {"title": "TopicRNN: A Recurrent Neural Network With Long-Range Semantic Dependency", "authors": ["A.B. Dieng", "C. Wang", "J. Gao", "J. Paisley"], "venue": "In Proceedings of ICLR,", "year": 2017}, {"title": "Adversarial Feature Learning", "authors": ["J. Donahue", "P. Krahenb\u00fchl", "T. Darrell"], "venue": "In Proceedings of ICLR,", "year": 2017}, {"title": "Likelihood Ratio Gradient Estimation: An Overview", "authors": ["P. Glynn"], "venue": "In Proceedings of Winter Simulation Conference,", "year": 1987}, {"title": "Generative adversarial nets", "authors": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. WardeFarley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "In Proceedings of NIPS,", "year": 2014}, {"title": "Improved Training of Wasserstein GANs", "authors": ["I. Gulrajani", "F. Ahmed", "M. Arjovsky", "A.C. Vincent Dumoulin"], "venue": "In Proceedings of NIPS,", "year": 2017}, {"title": "Generating Sentences by Editing Prototypes", "authors": ["K. Guu", "T.B. Hashimoto", "Y. Oren", "P. Liang"], "year": 2017}, {"title": "Learning distributed representations of sentences from unlabelled data", "authors": ["F. Hill", "K. Cho", "A. Korhonen"], "venue": "In Proceedings of NAACL,", "year": 2016}, {"title": "Boundary-Seeking Generative Adversarial Networks", "authors": ["R.D. Hjelm", "A.P. Jacob", "T. Che", "K. Cho", "Y. Bengio"], "venue": "In Proceedings of ICLR,", "year": 2018}, {"title": "Controllable Text Generation", "authors": ["Z. Hu", "Z. Yang", "X. Liang", "R. Salakhutdinov", "E.P. Xing"], "venue": "In Proceedings of ICML,", "year": 2017}, {"title": "Categorical Reparameterization with Gumbel-Softmax", "authors": ["E. Jang", "S. Gu", "B. Poole"], "venue": "In Proceedings of ICLR,", "year": 2017}, {"title": "Bag of Tricks for Efficient Text Classification", "authors": ["A. Joulin", "E. Grave", "P. Bojanowski", "T. Mikolov"], "venue": "In Proceedings of ACL,", "year": 2017}, {"title": "Semi-Amortized Variational Autoencoders", "authors": ["Y. Kim", "S. Wiseman", "A.C. Miller", "D. Sontag", "A.M. Rush"], "venue": "In Proceedings of ICML,", "year": 2018}, {"title": "Auto-Encoding Variational Bayes", "authors": ["D.P. Kingma", "M. Welling"], "venue": "In Proceedings of ICLR,", "year": 2014}, {"title": "GANs for Sequences of Discrete Elements with the Gumbel-Softmax Distribution", "authors": ["M. Kusner", "J.M. Hernandez-Lobato"], "year": 2016}, {"title": "Fader networks: Manipulating images by sliding attributes", "authors": ["G. Lample", "N. Zeghidour", "N. Usuniera", "A. Bordes", "L. Denoyer", "M. Ranzato"], "venue": "In Proceedings of NIPS,", "year": 2017}, {"title": "Adversarial Learning for Neural Dialogue Generation", "authors": ["J. Li", "W. Monroe", "T. Shi", "S. Jean", "A. Ritter", "D. Jurafsky"], "venue": "In Proceedings of EMNLP,", "year": 2017}, {"title": "Delete, Retrieve, Generate: A Simple Approach to Sentiment and Style Transfer", "authors": ["J. Li", "R. Jia", "H. He", "P. Liang"], "venue": "In Proceedings of NAACL,", "year": 2018}, {"title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables", "authors": ["C.J. Maddison", "A. Mnih", "Y.W. Teh"], "venue": "In Proceedings of ICLR,", "year": 2017}, {"title": "Linguistic Regularities in Continuous Space Word Representations", "authors": ["T. Mikolov", "S.W. tau Yih", "G. Zweig"], "venue": "In Proceedings of NAACL,", "year": 2013}, {"title": "Spectral Normalization For Generative Adversarial Networks", "authors": ["T. Miyato", "T. Kataoka", "M. Koyama", "Y. Yoshida"], "venue": "In Proceedings of ICLR,", "year": 2018}, {"title": "Sequence to Better Sequence: Continuous Revision of Combinatorial Structures", "authors": ["J. Mueller", "D. Gifford", "T. Jaakkola"], "venue": "In Proceedings of ICML,", "year": 2017}, {"title": "Style Transfer Through Back-Translation", "authors": ["S. Prabhumoye", "Y. Tsvetkov", "R. Salakhutdinov", "A.W. Black"], "venue": "In Proceedings of ACL,", "year": 2018}, {"title": "Language Generation with Recurrent Generative Adversarial Networks without Pre-training", "authors": ["O. Press", "A. Bar", "B. Bogin", "J. Berant", "L. Wolf"], "year": 2017}, {"title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks", "authors": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "In Proceedings of ICLR,", "year": 2016}, {"title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models", "authors": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "In Proceedings of ICML,", "year": 2014}, {"title": "Contractive Auto-Encoders: Explicit Invariance During Feature Extraction", "authors": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": "In Proceedings of ICML,", "year": 2011}, {"title": "A Hybrid Convolutional Variational Autoencoder for Text Generation", "authors": ["S. Semeniuta", "A. Severyn", "E. Barth"], "venue": "In Proceedings of EMNLP,", "year": 2017}, {"title": "Style Transfer from Non-Parallel Text by Cross-Alignment", "authors": ["T. Shen", "T. Lei", "R. Barzilay", "T. Jaakkola"], "venue": "In Proceedings of NIPS,", "year": 2017}, {"title": "A note on the evaluation of generative models", "authors": ["L. Theis", "A. van den Oord", "M. Bethge"], "venue": "In Proceedings of ICLR,", "year": 2016}, {"title": "VAE with a VampPrior", "authors": ["J.M. Tomczak", "M. Welling"], "venue": "In Proceedings of AISTATS,", "year": 2018}, {"title": "Optimal transport: old and new, volume 338", "authors": ["C. Villani"], "venue": "Springer Science & Business Media,", "year": 2008}, {"title": "Extracting and Composing Robust Features with Denoising Autoencoders", "authors": ["P. Vincent", "H. Larochelle", "Y. Bengio", "Manzagol", "P.-A"], "venue": "In Proceedings of ICML,", "year": 2008}, {"title": "Adversarially Learned Inference", "authors": ["Vincent Dumoulin", "Ishmael Belghazi", "B.P.O.M.A.L.M.A.A. C"], "venue": "In Proceedings of ICLR,", "year": 2017}, {"title": "Simple Statistical Gradient-following Algorithms for Connectionist Reinforcement Learning", "authors": ["R.J. Williams"], "venue": "Machine Learning,", "year": 1992}, {"title": "Improved Variational Autoencoders for Text Modeling using Dilated Convolutions", "authors": ["Z. Yang", "Z. Hu", "R. Salakhutdinov", "T. Berg-Kirkpatrick"], "venue": "In Proceedings of ICML,", "year": 2017}, {"title": "Unsupervised Text Style Transfer using Language Models as Discriminators", "authors": ["Z. Yang", "Z. Hu", "C. Dyer", "E.P. Xing", "T. Berg-Kirkpatrick"], "year": 2018}, {"title": "SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient", "authors": ["L. Yu", "W. Zhang", "J. Wang", "Y. Yu"], "venue": "In Proceedings of AAAI,", "year": 2017}, {"title": "Character-level Convolutional Networks for Text Classification", "authors": ["X. Zhang", "J. Zhao", "Y. LeCun"], "venue": "In Proceedings of NIPS,", "year": 2015}], "id": "SP:65e2f693234a3598f5a4a1cf85b549d2be03b87a", "authors": [{"name": "Jake (Junbo) Zhao", "affiliations": []}, {"name": "Yoon Kim", "affiliations": []}, {"name": "Kelly Zhang", "affiliations": []}, {"name": "Alexander M. Rush", "affiliations": []}, {"name": "Yann LeCun", "affiliations": []}], "abstractText": "Deep latent variable models, trained using variational autoencoders or generative adversarial networks, are now a key technique for representation learning of continuous structures. However, applying similar methods to discrete structures, such as text sequences or discretized images, has proven to be more challenging. In this work, we propose a flexible method for training deep latent variable models of discrete structures. Our approach is based on the recently-proposed Wasserstein autoencoder (WAE) which formalizes the adversarial autoencoder (AAE) as an optimal transport problem. We first extend this framework to model discrete sequences, and then further explore different learned priors targeting a controllable representation. This adversarially regularized autoencoder (ARAE) allows us to generate natural textual outputs as well as perform manipulations in the latent space to induce change in the output space. Finally we show that the latent representation can be trained to perform unaligned textual style transfer, giving improvements both in automatic/human evaluation compared to existing methods.", "title": "Adversarially Regularized Autoencoders"}