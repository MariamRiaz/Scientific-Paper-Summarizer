{"sections": [{"heading": "1. Introduction", "text": "Mathematical optimization is an importance pillar of machine learning. We consider the following optimization problem\nmin x\u2208Rd\nF (x) , 1\nn n\u2211 i=1 fi(x), (1)\nwhere the fi(x) are smooth functions. Many machine learning models can be expressed as (1) where each fi is the loss with respect to (w.r.t.) the i-th training sample. There are many examples such as logistic regressions, smoothed support vector machines, neural networks, and graphical models.\nMany optimization algorithms to solve the problem in (1) are based on the following iteration:\nx(t+1) = x(t) \u2212 \u03b7tQtg(x(t)), t = 0, 1, 2, . . . , 1Shanghai Jiao Tong University, Shanghai, China 2Peking University & Beijing Institute of Big Data Research, Beijing, China. Correspondence to: Zhihua Zhang <zhzhang@gmail.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nwhere \u03b7t > 0 is the step length. If Qt is the identity matrix and g(x(t)) = \u2207F (x(t)), the resulting procedure is called Gradient Descent (GD) which achieves sublinear convergence for a general smooth convex objective function and linear convergence for a smooth-strongly convex objective function. When n is large, the full gradient method is inefficient due to its iteration cost scaling linearly in n. Consequently, stochastic gradient descent (SGD) has been a typical alternative (Robbins & Monro, 1951; Li et al., 2014; Cotter et al., 2011). In order to achieve cheaper cost in each iteration, such a method constructs an approximate gradient on a small mini-batch of data. However, the convergence rate can be significantly slower than that of the full gradient methods (Nemirovski et al., 2009). Thus, a great deal of efforts have been made to devise modification to achieve the convergence rate of the full gradient while keeping low iteration cost (Johnson & Zhang, 2013; Roux et al., 2012; Schmidt et al., 2013; Zhang et al., 2013).\nIfQt is a d\u00d7d positive definite matrix containing the curvature information, this formulation leads us to second-order methods. It is well known that second order methods enjoy superior convergence rate in both theory and practice in contrast to first-order methods which only make use of the gradient information. The standard Newton method, where Qt = [\u22072F (x(t))]\u22121, g(x(t)) = \u2207F (x(t)) and \u03b7t = 1, achieves a quadratic convergence rate for smoothstrongly convex objective functions. However, the Newton method takes O(nd2 + d3) cost per iteration, so it becomes extremely expensive when n or d is very large. As a result, one tries to construct an approximation of the Hessian in which way the update is computationally feasible, and while keeping sufficient second order information. One class of such methods are quasi-Newton methods, which are generalizations of the secant methods to find the root of the first derivative for multidimensional problems. The celebrated Broyden-Fletcher-Goldfarb-Shanno (BFGS) and its limited memory version (L-BFGS) are the most popular and widely used (Nocedal & Wright, 2006). They take O(nd+ d2) cost per iteration. Recently, when n d, so-called subsampled Newton methods have been proposed, which define an approximate Hessian matrix with a small subset of samples. The most naive approach is to sample a subset of functions fi randomly (Roosta-Khorasani & Mahoney, 2016; Byrd et al.,\n2011; Xu et al., 2016) to construct a subsampled Hessian. Erdogdu & Montanari (2015) proposed a regularized subsampled Newton method called NewSamp. When the Hessian can be written as \u22072F (x) = [B(x)]TB(x) where B(x) is an available n \u00d7 d matrix, Pilanci & Wainwright (2015) used sketching techniques to approximate the Hessian and proposed a sketch Newton method. Similarly, Xu et al. (2016) proposed to sample rows of B(x) with non-uniform probability distribution. Agarwal et al. (2016) brought up an algorithm called LiSSA to approximate the inversion of Hessian directly.\nAlthough the convergence performance of stochastic second order methods has been analyzed, the convergence properties are still not well understood. There are several important gaps lying between the convergence theory and real application.\nThe first gap is the necessity of Lipschitz continuity of Hessian. In previous work, to achieve a linear-quadratic convergence rate, stochastic second order methods all assume that \u22072F (x) is Lipschitz continuous. However, in real applications without this assumption, they might also converge to the optimal point. For example, Erdogdu & Montanari (2015) used NewSamp to successfully train smoothed-SVM in which the Hessian is not Lipschitz continuous.\nThe second gap is about the sketched size of sketch Newton methods. To obtain a linear convergence, the sketched size is O(d\u03ba2) in (Pilanci & Wainwright, 2015) and then is improved to O(d\u03ba) in (Xu et al., 2016) using Gaussian sketching matrices, where \u03ba is the condition number of the Hessian matrix in question. However, the sketch Newton empirically performs well even when the Hessian matrix is ill-conditioned. Sketched size being several tens of times or even several times of d can achieve a linear convergence rate in unconstrained optimization. But the theoretical result of Pilanci & Wainwright (2015); Xu et al. (2016) implies that sketched size may be beyond n in ill-condition cases.\nThe third gap is about the sample size in regularized subsampled Newton methods. In both (Erdogdu & Montanari, 2015) and (Roosta-Khorasani & Mahoney, 2016), their theoretical analysis shows that the sample size of regularized subsampled Newton methods should be set as the same as the conventional subsampled Newton method. In practice, however, adding a large regularizer can obviously reduce the sample size while keeping convergence. Thus, this contradicts the extant theoretical analysis (Erdogdu & Montanari, 2015; Roosta-Khorasani & Mahoney, 2016).\nIn this paper, we aim to fill these gaps between the current theory and empirical performance. More specifically, we first cast these second order methods into an algorith-\nmic framework that we call approximate Newton. Then we propose a general result for analysis of local convergence properties of second order methods. Based on this framework, we give detailed theoretical analysis which matches the empirical performance very well. We summarize our contribution as follows:\n\u2022 We propose a unifying framework (Theorem 3) to analyze local convergence properties of second order methods including stochastic and deterministic versions. The convergence performance of second order methods can be analyzed easily and systematically in this framework.\n\u2022 We prove that the Lipschitz continuity condition of Hessian is not necessary for achieving linear and superlinear convergence in variants of subsampled Newton. But it is needed to obtain quadratic convergence. This explains the phenomenon that NewSamp (Erdogdu & Montanari, 2015) can be used to train smoothed SVM in which the Lipschitz continuity condition of Hessian is not satisfied. It also reveals the reason why previous stochastic second order methods, such as subsampled Newton, sketch Newton, LiSSA, etc., all achieve a linear-quadratic convergence rate.\n\u2022 We prove that the sketched size is independent of the condition number of the Hessian matrix which explains that sketched Newton performs well even when the Hessian matrix is ill-conditioned.\n\u2022 We provide a theoretical guarantee that adding a regularizer is an effective way to reduce the sample size in subsampled Newton methods while keeping converging. Our theoretical analysis also shows that adding a regularizer will lead to poor convergence behavior as the sample size decreases."}, {"heading": "1.1. Organization", "text": "The remainder of the paper is organized as follows. In Section 2 we present notation and preliminaries. In Section 3 we present a unifying framework for local convergence analysis of second order methods. In Section 4 we analyze the local convergence properties of sketch Newton methods and prove that sketched size is independent of condition number of the Hessian. In Section 5 we give the local convergence behaviors of several variants of subsampled Newton method. Especially, we reveal the relationship among the sample size, regularizer and convergence rate. In Section 6, we derive the local convergence properties of inexact Newton methods from our framework. In Section 7, we validate our theoretical results experimentally. Finally, we conclude our work in Section 8. All the proofs are presented in the supplementary metarials."}, {"heading": "2. Notation and Preliminaries", "text": "In this section, we introduce the notation and preliminaries that will be used in this paper."}, {"heading": "2.1. Notation", "text": "Given a matrix A = [aij ] \u2208 Rm\u00d7n of rank ` and a positive integer k \u2264 `, its condensed SVD is given as A = U\u03a3V T = Uk\u03a3kV T k +U\\k\u03a3\\kV T \\k, whereUk andU\\k contain the left singular vectors of A, Vk and V\\k contain the right singular vectors of A, and \u03a3 = diag(\u03c31, . . . , \u03c3`) with \u03c31 \u2265 \u03c32 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3` > 0 are the nonzero singular values of A. We use \u03c3max(A) to denote the largest singular value and \u03c3min(A) to denote the smallest non-zero singular value. Thus, the condition number of A is defined by \u03ba(A) , \u03c3max(A)\u03c3min(A) . If A is positive semidefinite, then U = V and the square root of A can be defined as A1/2 = U\u03a31/2UT . It also holds that \u03bbi(A) = \u03c3i(A), where \u03bbi(A) is the i-th largest eigenvalue of A, \u03bbmax(A) = \u03c3max(A), and \u03bbmin(A) = \u03c3min(A).\nAdditionally, \u2016A\u2016 , \u03c31 is the spectral norm. Given a positive definite matrix M , \u2016x\u2016M , \u2016M1/2x\u2016 is called the M -norm of x. Give square matricesA andB with the same size, we denote A B if B \u2212A is positive semidefinite."}, {"heading": "2.2. Randomized sketching matrices", "text": "We first give an -subspace embedding property which will be used to sketch Hessian matrices. Then we list two most popular types of randomized sketching matrices.\nDefinition 1 S \u2208 Rs\u00d7m is said to be an -subspace embedding matrix w.r.t. a fixed matrix A \u2208 Rm\u00d7d where d < m, if \u2016SAx\u20162 = (1\u00b1 )\u2016Ax\u20162 (i.e., (1\u2212 )\u2016Ax\u20162 \u2264 \u2016SAx\u20162 \u2264 (1 + )\u2016Ax\u20162) for all x \u2208 Rd.\nFrom the definition of the -subspace embedding matrix, we can derive the following property directly.\nLemma 2 S \u2208 Rs\u00d7m is an -subspace embedding matrix w.r.t. the matrix A \u2208 Rm\u00d7d if and only if\n(1\u2212 )ATA ATSTSA (1 + )ATA.\nLeverage score sketching matrix. A leverage score sketching matrix S = D\u2126 \u2208 Rs\u00d7m w.r.t. A \u2208 Rm\u00d7d is defined by sampling probabilities pi, a sampling matrix \u2126 \u2208 Rm\u00d7s and a diagonal rescaling matrix D \u2208 Rs\u00d7s. Specifically, we construct S as follows. For every j = 1, . . . , s, independently and with replacement, pick an index i from the set {1, 2 . . . ,m} with probability pi, and set \u2126ji = 1 and \u2126jk = 0 for k 6= i as well asDjj = 1/\u221apis. The sampling probabilities pi are the leverage scores of A defined as follows. Let V \u2208 Rm\u00d7d be the column orthonormal\nbasis of A, and let vi,\u2217 denote the i-th row of V . Then `i , \u2016vi,\u2217\u20162/d for i = 1, . . . ,m are the leverage scores of A. To achieve an -subspace embedding property w.r.t. A, s = O(d log d/ 2) is sufficient.\nSparse embedding matrix. A sparse embedding matrix S \u2208 Rs\u00d7m is such a matrix in each column of which there is only one nonzero entry uniformly sampled from {1,\u22121} (Clarkson & Woodruff, 2013). Hence, it is very efficient to compute SA, especially when A is sparse. To achieve an -subspace embedding property w.r.t. A \u2208 Rm\u00d7d, s = O(d2/ 2) is sufficient (Meng & Mahoney, 2013; Woodruff, 2014).\nOther sketching matrices such as Gaussian Random Projection and Subsampled Randomized Hadamard Transformation as well as their properties can be found in the survey (Woodruff, 2014)."}, {"heading": "2.3. Assumptions and Notions", "text": "In this paper, we focus on the problem described in Eqn. (1). Moreover, we will make the following two assumptions.\nAssumption 1 The objective function F is \u00b5-strongly convex, that is,\nF (y) \u2265 F (x)+[\u2207F (x)]T (y\u2212x)+\u00b5 2 \u2016y\u2212x\u20162, for \u00b5 > 0.\nAssumption 2 \u2207F (x) is L-Lipschitz continuous, that is, \u2016\u2207F (x)\u2212\u2207F (y)\u2016 \u2264 L\u2016y \u2212 x\u2016, for L > 0.\nAssumptions 1 and 2 imply that for any x \u2208 Rd, we have \u00b5I \u22072F (x) LI,\nwhere I is the identity matrix of appropriate size. With a little confusion, we define \u03ba , L\u00b5 . In fact, \u03ba is an upper bound of condition number of the Hessian matrix \u22072F (x) for any x.\nBesides, if \u22072F (x) is Lipschitz continuous, then we have \u2016\u22072F (x)\u2212\u22072F (y)\u2016 \u2264 L\u0302\u2016x\u2212 y\u2016,\nwhere L\u0302 > 0 is the Lipschitz constant of\u22072F (x). Throughout this paper, we use notions of linear convergence rate, superlinear convergence rate and quadratic convergence rate. In our paper, the convergence rates we will use are defined w.r.t. \u2016 \u00b7 \u2016M\u2217 , where M\u2217 = [\u22072F (x\u2217)]\u22121 and x\u2217 is the optimal solution to Problem (1). A sequence of vectors {x(t)} is said to converge linearly to a limit point x\u2217, if for some 0 < \u03c1 < 1,\nlim sup t\u2192\u221e \u2016\u2207F (x(t+1))\u2016M\u2217 \u2016\u2207F (x(t))\u2016M\u2217 = \u03c1.\nSimilarly, superlinear convergence and quadratic convergence are respectively defined as\nlim sup t\u2192\u221e \u2016\u2207F (x(t+1))\u2016M\u2217 \u2016\u2207F (x(t))\u2016M\u2217 = 0, lim sup t\u2192\u221e \u2016\u2207F (x(t+1))\u2016M\u2217 \u2016\u2207F (x(t))\u20162M\u2217 = \u03c1.\nWe call it the linear-quadratic convergence rate if the following condition holds:\n\u2016\u2207F (x(t+1))\u2016M\u2217 \u2264 \u03c11\u2016\u2207F (x(t))\u2016M\u2217+\u03c12\u2016\u2207F (x(t))\u20162M\u2217 ,\nwhere 0 < \u03c11 < 1."}, {"heading": "3. Approximate Newton Methods and Local Convergence Analysis", "text": "The existing variants of stochastic second order methods share some important attributes. First, these methods such as NewSamp (Erdogdu & Montanari, 2015), LiSSA (Agarwal et al., 2016), subsampled Newton with conjugate gradient (Byrd et al., 2011), and subsampled Newton with nonuniformly sampling (Xu et al., 2016), all have the same convergence properties; that is, they have a linear-quadratic convergence rate.\nSecond, they also enjoy the same algorithm procedure summarized as follows. In each iteration, they first construct an approximate Hessian matrix H(t) such that\n(1\u2212 0)H(t) \u22072F (x(t)) (1 + 0)H(t), (2)\nwhere 0 \u2264 0 < 1. Then they solve the following optimization problem\nmin p\n1 2 pTH(t)p\u2212 pT\u2207F (x(t)) (3)\napproximately or exactly to obtain the direction vector p(t). Finally, their update equation is given as x(t+1) = x(t)\u2212 p(t). With this procedure, we regard these stochastic second order methods as approximate Newton methods.\nIn the following theorem, we propose a unifying framework which describes the convergence properties of the second order optimization procedure depicted above.\nTheorem 3 Let Assumptions 1 and 2 hold. Suppose that \u22072F (x) exists and is continuous in a neighborhood of a minimizer x\u2217. H(t) is a positive definite matrix that satisfies Eqn. (2) with 0 \u2264 0 < 1. Let p(t) be an approximate solution of Problem (3) such that\n\u2016\u2207F (x(t))\u2212H(t)p(t)\u2016 \u2264 1 \u03ba \u2016\u2207F (x(t))\u2016, (4)\nwhere 0 < 1 < 1. Define the iteration x(t+1) = x(t)\u2212p(t).\n(a) There exists a sufficient small value \u03b3, 0 < \u03bd(t) < 1, and 0 < \u03b7(t) < 1 such that when \u2016x(t) \u2212 x\u2217\u2016 \u2264 \u03b3, we have that\n\u2016\u2207F (x(t+1))\u2016M\u2217 \u2264 ( 0 +\n1 1\u2212 0\n+ 2\u03b7(t)\n1\u2212 0\n) 1 + \u03bd(t)\n1\u2212 \u03bd(t)\u2016\u2207F (x (t))\u2016M\u2217 .\n(5)\nBesides, \u03bd(t) and \u03b7(t) will go to 0 as x(t) goes to x\u2217.\n(b) Furthermore, if \u22072F (x) is Lipschitz continuous with parameter L\u0302, and x(t) satisfies\n\u2016x(t) \u2212 x\u2217\u2016 \u2264 \u00b5 L\u0302\u03ba \u03bd(t), (6)\nwhere 0 < \u03bd(t) < 1, then it holds that\n\u2016\u2207F (x(t+1))\u2016M\u2217 \u2264 ( 0 +\n1 1\u2212 0\n) 1 + \u03bd(t)\n1\u2212 \u03bd(t)\u2016\u2207F (x (t))\u2016M\u2217\n+ 2 (1\u2212 0)2 L\u0302\u03ba \u00b5 \u221a \u00b5\n(1 + \u03bd(t))2\n1\u2212 \u03bd(t) \u2016\u2207F (x (t))\u20162M\u2217 . (7)\nFrom Theorem 3, we can find some important insights. First, Theorem 3 provides sufficient conditions to get different convergence rates including super-liner and quadratic convergence rates. If ( 0 +\n1 1\u2212 0\n) is a constant,\nthen sequence {x(t)} converges linearly because \u03bd(t) and \u03b7(t) will go to 0 as t goes to infinity. If we set 0 = 0(t) and 1 = 1(t) such that 0(t) and 1(t) decrease to 0 as t increases, then sequence {x(t)} will converge superlinearly. Similarly, if 0(t) = O(\u2016\u2207F (x(t))\u2016M\u2217), 1(t) = O(\u2016\u2207F (x(t))\u2016M\u2217), and \u22072F (x) is Lipschitz continuous, then sequence {x(t)} will converge quadratically. Second, Theorem 3 makes it clear that the Lipschitz continuity of \u22072F (x) is not necessary for linear convergence and superlinear convergence of stochastic second order methods including Subsampled Newton method, Sketch Newton, NewSamp, etc. This reveals the reason why NewSamp can be used to train the smoothed SVM where the Lipschitz continuity of the Hessian matrix is not satisfied. The Lipschitz continuity condition is only needed to get a quadratic convergence or linear-quadratic convergence. This explains the phenomena that LiSSA(Agarwal et al., 2016), NewSamp (Erdogdu & Montanari, 2015), subsampled Newton with non-uniformly sampling (Xu et al., 2016), Sketched Newton (Pilanci & Wainwright, 2015) have linear-quadratic convergence rate because they all assume that the Hessian is Lipschitz continuous. In fact, it is well known that the Lipschitz continuity condition of \u22072F (x) is not necessary to achieve a linear or superlinear convergence rate for inexact Newton methods.\nAlgorithm 1 Sketch Newton. 1: Input: x(0), 0 < \u03b4 < 1, 0 < 0 < 1; 2: for t = 0, 1, . . . until termination do 3: Construct an 0-subspace embedding matrix S\nfor B(x(t)) and where \u22072F (x) is of the form \u22072F (x) = (B(x(t)))TB(x(t)), and calculate H(t) = [B(x(t))]TSTSB(x(t));\n4: Calculate p(t) \u2248 argminp 12p TH(t)p\u2212 pT\u2207F (x(t)); 5: Update x(t+1) = x(t) \u2212 p(t); 6: end for\nThird, the unifying framework of Theorem 3 contains not only stochastic second order methods, but also the deterministic versions. For example, letting H(t) = \u22072F (x(t)) and using conjugate gradient to get p(t), we obtain the famous \u201cNewton-CG\u201d method. In fact, different choice of H(t) and different way to calculate p(t) lead us to different second order methods. In the following sections, we will use this framework to analyze the local convergence performance of these second order methods in detail."}, {"heading": "4. Sketch Newton Method", "text": "In this section, we use Theorem 3 to analyze the local convergence properties of Sketch Newton (Algorithm 1). We mainly focus on the case that the Hessian matrix is of the form\n\u22072F (x) = B(x)TB(x) (8)\nwhere B(x) is an explicitly available n \u00d7 d matrix. Our result can be easily extended to the case that\n\u22072F (x) = B(x)TB(x) +Q(x),\nwhere Q(x) is a positive semi-definite matrix related to the Hessian of regularizer.\nTheorem 4 Let F (x) satisfy the conditions described in Theorem 3. Assume the Hessian matrix is given as Eqn. (8). Let 0 < \u03b4 < 1, 0 < 0 < 1/2 and 0 \u2264 1 < 1 be given. S \u2208 R`\u00d7n is an 0-subspace embedding matrix w.r.t. B(x) with probability at least 1 \u2212 \u03b4, and direction vector p(t) satisfies Eqn. (4). Then Algorithm 1 has the following convergence properties:\n(a) There exists a sufficient small value \u03b3, 0 < \u03bd(t) < 1, and 0 < \u03b7(t) < 1 such that when \u2016x(t) \u2212 x\u2217\u2016 \u2264 \u03b3, then each iteration satisfies Eqn. (5) with probability at least 1\u2212 \u03b4.\n(b) If \u22072F (x(t)) is also Lipschitz continuous and {x(t)} satisfies Eqn. (6), then each iteration satisfies Eqn. (7) with probability at least 1\u2212 \u03b4.\nTable 1. Comparison with previous work\nReference Sketched Size Pilanci & Wainwright (2015) O\n(\nd\u03ba2 log d\n20 )\nXu et al. (2016) O ( d\u03ba log d 20\n) Our result(Theorem 4) O ( d log d 20\n) Theorem 4 directly provides a bound of the sketched size. Using the leverage score sketching matrix as an example, the sketched size ` = O(d log d/ 20) is sufficient. We compare our theoretical bound of the sketched size with the ones of Pilanci & Wainwright (2015) and Xu et al. (2016) in Table 1. As we can see, our sketched size is much smaller than the other two, especially when the Hessian matrix is ill-conditioned.\nTheorem 4 shows that the sketched size ` is independent on the condition number of the Hessian matrix \u22072F (x) just as shown in Table 1. This explains the phenomena that when the Hessian matrix is ill-conditioned, Sketch Newton performs well even when the sketched size is only several times of d. For a large condition number, the theoretical bounds of both Xu et al. (2016) and Pilanci & Wainwright (2015) may be beyond the number of samples n. Note that the theoretical results of (Xu et al., 2016) and (Pilanci & Wainwright, 2015) still hold in the constrained optimization problem. However, our result proves the effectiveness of the sketch Newton method for the unconstrained optimization problem in the ill-conditioned case."}, {"heading": "5. The Subsampled Newton method and Variants", "text": "In this section, we apply Theorem 3 to analyze Subsampled Newton and regularized subsampled Newton method.\nFirst, we make the assumption that each fi(x) and F (x) have the following properties:\nmax 1\u2264i\u2264n\n\u2016\u22072fi(x)\u2016 \u2264 K <\u221e, (9)\n\u03bbmin(\u22072F (x)) \u2265 \u03c3 > 0. (10)\nAccordingly, if \u22072F (x) is ill-conditioned, then the value K \u03c3 is large."}, {"heading": "5.1. The Subsampled Newton method", "text": "The Subsampled Newton method is depicted in Algorithm 2, and we now give its local convergence properties in the following theorem.\nTheorem 5 Let F (x) satisfy the properties described in Theorem 3. Assume Eqn. (9) and Eqn. (10) hold and let 0 < \u03b4 < 1, 0 < 0 < 1/2 and 0 \u2264 1 < 1 be given. |S| andH(t) are set as in Algorithm 2, and the direction vector p(t) satisfies Eqn. (4). Then Algorithm 2 has the following\nAlgorithm 2 Subsampled Newton. 1: Input: x(0), 0 < \u03b4 < 1, 0 < 0 < 1; 2: Set the sample size |S| \u2265 16K 2 log(2d/\u03b4)\n\u03c32 20 .\n3: for t = 0, 1, . . . until termination do 4: Select a sample set S, of size |S| and construct H(t) =\n1 |S| \u2211 j\u2208S \u2207 2fj(x (t));\n5: Calculate p(t) \u2248 argminp 12p TH(t)p\u2212 pT\u2207F (x(t)); 6: Update x(t+1) = x(t) \u2212 p(t); 7: end for\nconvergence properties:\n(a) There exists a sufficient small value \u03b3, 0 < \u03bd(t) < 1, and 0 < \u03b7(t) < 1 such that when \u2016x(t) \u2212 x\u2217\u2016 \u2264 \u03b3, then each iteration satisfies Eqn. (5) with probability at least 1\u2212 \u03b4.\n(b) If \u22072F (x(t)) is also Lipschitz continuous and {x(t)} satisfies Eqn. (6), then each iteration satisfies Eqn. (7) with probability at least 1\u2212 \u03b4.\nAs we can see, Algorithm 2 almost has the same convergence properties as Algorithm 1 except several minor differences. The main difference is the construction manner of H(t) which should satisfy Eqn. (2). Algorithm 2 relies on the assumption that each \u2016\u22072fi(x)\u2016 is upper bounded (i.e., Eqn. (9) holds), while Algorithm 1 is built on the setting of the Hessian matrix as in Eqn. (8)."}, {"heading": "5.2. Regularized Subsampled Newton", "text": "In ill-conditioned cases (i.e., K\u03c3 is large), the subsampled Newton method in Algorithm 2 should take a lot of samples because the sample size |S| depends on K\u03c3 quadratically. To overcome this problem, one resorts to a regularized subsampled Newton method. The key idea is to add \u03b1I to the original subsampled Hessian just as described in Algorithm 3. Erdogdu & Montanari (2015) proposed NewSamp which is another regularized subsampled Newton method depicted in Algorithm 4. In the following analysis, we prove that adding a regularizer is an effective way to reduce the sample size while keeping converging in theory.\nWe first give the theoretical analysis of local convergence properties of Algorithm 3.\nTheorem 6 Let F (x) satisfy the properties described in Theorem 3. Assume Eqns. (9) and (10) hold, and let 0 < \u03b4 < 1, 0 \u2264 1 < 1 and 0 < \u03b1 be given. Assume \u03b2 is a constant such that 0 < \u03b2 < \u03b1 + \u03c32 , the subsampled size |S| satisfies |S| \u2265 16K 2 log(2d/\u03b4) \u03b22 , and H (t) is constructed\nas in Algorithm 3. Define\n0 = max\n( \u03b2 \u2212 \u03b1\n\u03c3 + \u03b1\u2212 \u03b2 , \u03b1+ \u03b2 \u03c3 + \u03b1+ \u03b2\n) , (11)\nwhich implies that 0 < 0 < 1. Besides, the direction vector p(t) satisfies Eqn. (4). Then Algorithm 3 has the following convergence properties:\n(a) There exists a sufficient small value \u03b3, 0 < \u03bd(t) < 1, and 0 < \u03b7(t) < 1 such that when \u2016x(t) \u2212 x\u2217\u2016 \u2264 \u03b3, each iteration satisfies Eqn. (5) with probability at least 1\u2212 \u03b4.\n(b) If \u22072F (x(t)) is also Lipschitz continuous and {x(t)} satisfies Eqn. (6), then each iteration satisfies Eqn. (7) with probability at least 1\u2212 \u03b4.\nIn Theorem 6 the parameter 0 mainly decides convergence properties of Algorithm 3. It is determined by two terms just as shown in Eqn. (11). These two terms depict the relationship among the sample size, regularizer \u03b1I , and convergence rate.\nThe first term describes the relationship between the regularizer \u03b1I and sample size. Without loss of generality, we set \u03b2 = \u03b1 which satisfies 0 < \u03b2 < \u03b1 + \u03c3/2. Then the sample size |S| = 16K\n2 log(2d/\u03b4) \u03b12 decreases as \u03b1 increases.\nHence Theorem 6 gives a theoretical guarantee that adding the regularizer \u03b1I is an effective approach for reducing the sample size when K/\u03c3 is large. Conversely, if we want to sample a small part of fi\u2019s, then we should choose a large \u03b1. Otherwise, \u03b2 will go to \u03b1 + \u03c3/2 which means 0 = 1, i.e., the sequence {x(t)} does not converge. Though a large \u03b1 can reduce the sample size, it is at the expense of slower convergence rate just as the second term shows. As we can see, \u03b1+\u03b2\u03c3+\u03b1+\u03b2 goes to 1 as \u03b1 increases. Besides, 1 also has to decrease. Otherwise, 0 + 11\u2212 0 may be beyond 1 which means that Algorithm 3 will not converge.\nIn fact, slower convergence rate via adding a regularizer is because the sample size becomes small, which implies less curvature information is obtained. However, a small sample size implies low computational cost in each iteration. Therefore, a proper regularizer which balances the cost of each iteration and convergence rate is the key in the regularized subsampled Newton algorithm.\nNext, we give the theoretical analysis of local convergence properties of NewSamp (Algorithm 4).\nTheorem 7 Let F (x) satisfy the properties described in Theorem 3. Assume Eqn. (9) and Eqn. (10) hold and let 0 < \u03b4 < 1 and target rank r be given. Let \u03b2 be a constant such that 0 < \u03b2 < \u03bb (t) r+1\n2 , where \u03bb (t) r+1 is the (r+ 1)-th\nAlgorithm 3 Regularized Subsample Newton. 1: Input: x(0), 0 < \u03b4 < 1, regularizer parameter \u03b1, sample size |S| ;\n2: for t = 0, 1, . . . until termination do 3: Select a sample set S, of size |S| and construct H(t) =\n1 |S| \u2211 j\u2208S \u2207 2fj(x (t)) + \u03b1I;\n4: Calculate p(t) \u2248 argminp 12p TH(t)p\u2212 pT\u2207F (x(t)) 5: Update x(t+1) = x(t) \u2212 p(t); 6: end for\nAlgorithm 4 NewSamp. 1: Input: x(0), 0 < \u03b4 < 1, r, sample size |S|; 2: for t = 0, 1, . . . until termination do 3: Select a sample set S, of size |S| and get H(t)|S| =\n1 |S| \u2211 j\u2208S \u2207 2fj(x (t));\n4: Compute rank r + 1 truncated SVD deompostion of H(t)|S| to get Ur+1 and \u039b\u0302r+1. Construct H(t) = H (t)\n|S| +\nU\\r(\u03bb\u0302 (t) r+1I \u2212 \u039b\u0302\\r)UT\\r\n5: Calculate p(t) \u2248 argminp 12p TH(t)p\u2212 pT\u2207F (x(t)) 6: Update x(t+1) = x(t) \u2212 p(t); 7: end for\neigenvalue of \u22072F (x(t)). Set the subsampled size |S| such that |S| \u2265 16K\n2 log(2d/\u03b4) \u03b22 , and define\n0 = max\n( \u03b2\n\u03bb (t) r+1 \u2212 \u03b2\n, 2\u03b2 + \u03bb\n(t) r+1\n\u03c3 + 2\u03b2 + \u03bb (t) r+1\n) , (12)\nwhich implies 0 < 0 < 1. Assume the direction vector p(t) satisfies Eqn. (4). Then Algorithm 4 has the following convergence properties:\n(a) There exists a sufficient small value \u03b3, 0 < \u03bd(t) < 1, and 0 < \u03b7(t) < 1 such that when \u2016x(t) \u2212 x\u2217\u2016 \u2264 \u03b3, each iteration satisfies Eqn. (5) with probability at least 1\u2212 \u03b4.\n(b) If \u22072F (x(t)) is also Lipschitz continuous and {x(t)} satisfies Eqn. (6), then each iteration satisfies Eqn. (7) with probability at least 1\u2212 \u03b4.\nSimilar to Theorem 6, parameter 0 in NewSamp is also determined by two terms. The first term reveals the the relationship between the target rank r and sample size. Without loss of generality, we can set \u03b2 = \u03bb(t)r+1/4. Then the sample size is linear in 1/[\u03bb(t)r+1]\n2. Hence, a small r means that a small sample size is sufficient. Conversely, if we want to sample a small portion of fi\u2019s, then we should choose a small r. Otherwise, \u03b2 will go to \u03bb(t)r+1/2 which means 0 = 1, i.e., the sequence {x(t)} does not converge. The second term shows that a small sample size will lead to a poor convergence rate. If we set r = 0 and \u03b2 = \u03bb1/2, then\n0 will be 1 \u2212 11+2\u03bb1/\u03c3 . Consequently, the convergence rate of NewSamp is almost the same as gradient descent. Similar to Algorithm 3, a small r means a precise solution to Problem (3) and the initial point x(0) being close to the optimal point x\u2217.\nIt is worth pointing out that Theorem 7 explains the empirical results that NewSamp is applicable in training SVM in which the Lipschitz continuity condition of \u22072F (x) is not satisfied (Erdogdu & Montanari, 2015).\nWe now conduct comparison between Theorem 6 and Theorem 7. We mainly focus on the parameter 0 in these two theorems which mainly determines convergence properties of Algorithm 3 and Algorithm 4. Specifically, if we\nset \u03b1 = \u03b2 + \u03bb(t)r+1 in Eqn. (11), then 0 = 2\u03b2+\u03bb\n(t) r+1\n\u03c3+2\u03b2+\u03bb (t) r+1\nwhich equals to the second term on the right-hand side in Eqn. (12). Hence, we can regard NewSamp as a special case of Algorithm 3. However, NewSamp provides an approach for automatical choice of \u03b1.\nRecall that NewSamp includes another parameter: the target rank r. Thus, NewSamp and Algorithm 3 have the same number of free parameters. If r is not properly chosen, NewSamp will still have poor performance. Therefore, Algorithm 3 is theoretically preferred because NewSamp needs extra cost to perform SVDs."}, {"heading": "6. Inexact Newton Methods", "text": "Let H(t) = \u22072F (x(t)), that is, 0 = 0. Then Theorem 3 depicts the convergence properties of inexact Newton methods.\nTheorem 8 Let F (x) satisfy the properties described in Theorem 3, and p(t) be a direction vector such that\n\u2016\u2207F (x(t))\u2212\u22072F (x(t))p(t)\u2016 \u2264 1 \u03ba \u2016\u2207F (x(t))\u2016,\nwhere 0 < 1 < 1. Consider the iteration x(t+1) = x(t) \u2212 p(t).\n(a) There exists a sufficient small value \u03b3, 0 < \u03bd(t) < 1, and 0 < \u03b7(t) < 1 such that when \u2016x(t) \u2212 x\u2217\u2016 \u2264 \u03b3, then it holds that\n\u2016\u2207F (x(t+1))\u2016M\u2217 \u2264 ( 1+2\u03b7(t)) 1 + \u03bd(t)\n1\u2212 \u03bd(t)\u2016\u2207F (x (t))\u2016M\u2217 .\n(b) If \u22072F (x) is also Lipschitz continuous with parameter L\u0302, and {x(t)} satisfies Eqn. (6), then it holds that\n\u2016\u2207F (x(t+1))\u2016M\u2217 \u2264 1 1 + \u03bd(t)\n1\u2212 \u03bd(t)\u2016\u2207F (x (t))\u2016M\u2217+\n2L\u0302\u03ba \u00b5 \u221a \u00b5\n(1 + \u03bd(t))2\n1\u2212 \u03bd(t) \u2016\u2207F (x (t))\u20162M\u2217 ."}, {"heading": "7. Empirical Study", "text": "In this section, we validate our theoretical results about sketched size of the sketch Newton, and sample size of regularized Newton, experimentally. Experiments for validating unnecessity of the Lipschitz continuity condition of \u22072F (x) are given in the supplementary materials."}, {"heading": "7.1. Sketched Size of Sketch Newton", "text": "Now we validate that our theoretical result that sketched size is independent of the condition number of the Hessian in Sketch Newton. To control the condition number of the Hessian conveniently, we conduct the experiment on least squares regression which is defined as\nmin x\n1 2 \u2016Ax\u2212 b\u20162. (13)\nIn each iteration, the Hessian matrix is ATA. In our experiment, A is a 10000\u00d754 matrix. We set the singular values \u03c3i of A as: \u03c3i = 1.2\u2212i. Then the condition number of A is \u03ba(A) = 1.254 = 1.8741 \u00d7 104. We use different sketch matrices in Sketch Newton (Algorithm 1) and set different values of the sketched size `. We report our empirical results in Figure 1.\nFrom Figure 1, we can see that Sketch Newton performs well when the sketch size ` is several times of d for all different sketching matrices. Moreover, the corresponding algorithms converge linearly. This matches our theory that the sketched size is independent of the condition number of the Hessian matrix to achieve a linear convergence rate. In contrast, the theoretical result of (Xu et al., 2016) shows that the sketched size is ` = d \u2217 \u03ba(A) = 1.02\u00d7 106 bigger than n = 104."}, {"heading": "7.2. Sample Size of Regularized Subsampled Newton", "text": "We also choose least squares regression defined in Eqn. (13) in our experiment to validate the theory that adding a regularizer is an effective approach to reducing the sample size while keeping convergence in Subsampled Newton. Let A \u2208 Rn\u00d7d where n = 8000 and d = 5000. Hence Sketch Newton can not be used in this case because n and d are close to each other. In our experiment, we set different sample sizes |S|. For each |S|we choose different\nregularizer terms \u03b1 and different target ranks r. We report our results in Figure 2.\nAs we can see, if the sample size |S| is small, then we should choose a large \u03b1; otherwise, the algorithm will diverge. However, if the regularizer term \u03b1 is too large, then the algorithm will converge slowly. Increasing the sample size and choosing a proper regularizer will improve convergence properties obviously. When |S| = 600, it only needs about 1200 iterations to obtain a precise solution while it needs about 8000 iterations when |S| = 100. Similarly, if the sample size |S| is small, then we should choose a small target rank. Otherwise NewSamp may diverge. Also, if the target rank is not chosen properly, NewSamp will have poor convergence properties. Furthermore, from Figure 2, we can see that the two algorithms have similar convergence properties. This validates the theoretical result that NewSamp provides a method to choose \u03b1 automatically. Our empirical analysis matches the theoretical analysis in Subsection 5.2 very well."}, {"heading": "8. Conclusion", "text": "In this paper, we have proposed a framework to analyze the local convergence properties of second order methods including stochastic and deterministic versions. This framework reveals some important convergence properties of the subsampled Newton method and sketch Newton method, which are unknown before. The most important thing is that our analysis lays the theoretical foundation of several important stochastic second order methods.\nWe believe that this framework might also provide some useful insights for developing new subsampled Newtontype algorithms. We would like to address this issue in future."}, {"heading": "Acknowledgements", "text": "Ye has been supported by the National Natural Science Foundation of China (Grant No. 11426026, 61632017, 61173011) and a Project 985 grant of Shanghai Jiao Tong University. Luo and Zhang have been supported by he National Natural Science Foundation of China (No. 61572017), Natural Science Foundation of Shanghai City (No. 15ZR1424200), and Microsoft Research Asia Collaborative Research Award."}], "year": 2017, "references": [{"title": "Second order stochastic optimization in linear time", "authors": ["References Agarwal", "Naman", "Bullins", "Brian", "Hazan", "Elad"], "venue": "arXiv preprint arXiv:1602.03943,", "year": 2016}, {"title": "On the use of stochastic hessian information in optimization methods for machine learning", "authors": ["Byrd", "Richard H", "Chin", "Gillian M", "Neveitt", "Will", "Nocedal", "Jorge"], "venue": "SIAM Journal on Optimization,", "year": 2011}, {"title": "Low rank approximation and regression in input sparsity time", "authors": ["Clarkson", "Kenneth L", "Woodruff", "David P"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "year": 2013}, {"title": "Better mini-batch algorithms via accelerated gradient methods", "authors": ["Cotter", "Andrew", "Shamir", "Ohad", "Srebro", "Nati", "Sridharan", "Karthik"], "venue": "In Advances in neural information processing systems,", "year": 2011}, {"title": "Convergence rates of sub-sampled newton methods", "authors": ["Erdogdu", "Murat A", "Montanari", "Andrea"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "authors": ["Johnson", "Rie", "Zhang", "Tong"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2013}, {"title": "Efficient mini-batch training for stochastic optimization", "authors": ["Li", "Mu", "Zhang", "Tong", "Chen", "Yuqiang", "Smola", "Alexander J"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "year": 2014}, {"title": "Low-distortion subspace embeddings in input-sparsity time and applications to robust linear regression", "authors": ["Meng", "Xiangrui", "Mahoney", "Michael W"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "year": 2013}, {"title": "Robust stochastic approximation approach to stochastic programming", "authors": ["Nemirovski", "Arkadi", "Juditsky", "Anatoli", "Lan", "Guanghui", "Shapiro", "Alexander"], "venue": "SIAM Journal on Optimization,", "year": 2009}, {"title": "Newton sketch: A linear-time optimization algorithm with linear-quadratic convergence", "authors": ["Pilanci", "Mert", "Wainwright", "Martin J"], "venue": "arXiv preprint arXiv:1505.02250,", "year": 2015}, {"title": "A stochastic approximation method", "authors": ["Robbins", "Herbert", "Monro", "Sutton"], "venue": "The annals of mathematical statistics,", "year": 1951}, {"title": "Subsampled newton methods ii: Local convergence rates", "authors": ["Roosta-Khorasani", "Farbod", "Mahoney", "Michael W"], "venue": "arXiv preprint arXiv:1601.04738,", "year": 2016}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "authors": ["Roux", "Nicolas L", "Schmidt", "Mark", "Bach", "Francis R"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2012}, {"title": "Minimizing finite sums with the stochastic average gradient", "authors": ["Schmidt", "Mark", "Roux", "Nicolas Le", "Bach", "Francis"], "venue": "arXiv preprint arXiv:1309.2388,", "year": 2013}, {"title": "Sub-sampled newton methods with non-uniform sampling", "authors": ["Xu", "Peng", "Yang", "Jiyan", "Roosta-Khorasani", "Farbod", "R\u00e9", "Christopher", "Mahoney", "Michael W"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}], "id": "SP:27a2c20c2d4acd12c1189ec8ac41df06f59af8de", "authors": [{"name": "Haishan Ye", "affiliations": []}, {"name": "Luo Luo", "affiliations": []}, {"name": "Zhihua Zhang", "affiliations": []}], "abstractText": "Many machine learning models are reformulated as optimization problems. Thus, it is important to solve a large-scale optimization problem in big data applications. Recently, stochastic second order methods have emerged to attract much attention for optimization due to their efficiency at each iteration, rectified a weakness in the ordinary Newton method of suffering a high cost in each iteration while commanding a high convergence rate. However, the convergence properties of these methods are still not well understood. There are also several important gaps between the current convergence theory and the performance in real applications. In this paper, we aim to fill these gaps. We propose a unifying framework to analyze local convergence properties of second order methods. Based on this framework, our theoretical analysis matches the performance in real applications.", "title": "Approximate Newton Methods and Their Local Convergence"}