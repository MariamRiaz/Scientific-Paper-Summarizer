57	1	Using both synthetic and real-world traces, we show that AdaUCB significantly outperforms other bandit algorithms, such as UCB and TS (Thompson Sampling), under large load fluctuations.
62	1	Let u∗ = maxk uk be the maximum expected reward and k∗ = arg maxuk be the best arm.
90	1	To achieve good performance, the truncation thresholds should be appropriately chosen and can be learned online in practice, as discussed in Sec.
149	1	As we will see in the simulations, AdaUCB with α > 1/2 works well under general random load.
150	1	We first propose a loose but useful bound for the number of pulls for the optimal arm.
152	1	In the opportunistic bandit with random binaryvalued load and random rewards, for a constant η ∈ (0, ρ), there exists a constant T2, such that under AdaUCB, for all t ≥ T2 P { C (0) 1 (t) < (ρ− η)t 2 } ≤ e−2η 2t + [2(K − 1)]2α−1 2α− 2 [ (ρ− η)t ]−2α+2 .
157	1	Then, we can bound the probability of the event C (0) 1 (t) < (ρ−η)t 2 by bounding the deviation of UCBs.
161	1	When the load is high, i.e., Lt = 1− 1, the index becomes ûk(t) = ūk(t)+ √ α 1 log t (1− 0)Ck(t−1) .
162	1	In this case, with high probability, the index of the optimal arm is lower bounded by u1 − ( 1− √ 1 (1− 0) )√ α log t C1(t−1) according to Lemma 2.
163	1	With similar adjustment on the UCB index for the suboptimal arm, we can bound the probability of pulling the suboptimal arm under high load.
164	1	The conclusion of the lemma then follows by combining the above two cases.
168	1	To be more specific, we focus on the slot t′ when the optimal arm is pulled for the last time before t under load Lt = 0.
171	1	On the other hand, we can show that for the suboptimal arm, ûk(t ′) > u1 + δ = uk + (∆k + δ) with high probability when Ck(t′ − 1) < α log t4(∆k+δ)2 .
172	1	Thus, the probability of pulling the optimal arm at t′ is bounded by a small value, implying the conclusion of the lemma.
176	1	In fact, the existence of this δ is guaranteed under the assumptions α > 16 and √ 1 1− 0 < 1 8 .
179	1	The conclusion of this theorem then follows according to Lemma 3.
193	1	Because of the dummy low-priority traffic injected into the network, we can learn the true value of Xat,t under the peak load.
212	1	When the load is continuous, we need to choose appropriate l(−) and l(+) for AdaUCB.
222	1	(8) Next, we illustrate the advantages of AdaUCB under continuous load by studying the regret bound for AdaUCB with special thresholds l(+) = l(−).
223	1	In the opportunistic bandit with random continuous load and random rewards, under AdaUCB with P{Lt ≤ l(−)} = ρ > 0 and l(+) = l(−) , we have RAdaUCB(T ) ≤ 4α log TE[Lt|Lt ≤ l(−)] ∑ k>1 1 ∆k +O(1), (9) where E[Lt|Lt ≤ l(−)] is the expectation of Lt conditioned on Lt ≤ l(−).
229	1	Furthermore, the number of pulls under load level Lt ≤ l(−) is bounded according to Lemma 5.
234	1	Thus, AdaUCB achieves much smaller regret when T is large and ρ is relatively small.
240	1	In addition, appropriately chosen thresholds also handle the case when the load has little or no fluctuation, i.e., Lt ≈ c. For example, if we set l(−) = c and l(+) = 2c, AdaUCB degenerates to UCB(α).
243	1	Specifically, the algorithm maintains the histogram for the load levels (or its moving average version for non-stationary cases), and then select l(−) and l(+) accordingly.
245	1	We can see that, in most simulations, E-AdaUCB performs closely to AdaUCB with thresholds chosen offline.
282	3	Experimental results based on both synthetic and real data demonstrate the significant benefits of opportunistic exploration under large load fluctuations.
283	5	This work is a first attempt to study opportunistic bandits, and several open questions remain.
284	18	First, although AdaUCB achieves promising experimental performance under general settings, rigorous analysis with tighter performance bound remains challenging.
285	132	Furthermore, opportunistic TS-type algorithms are also interesting because TS-type algorithms often performs better than UCB-type algorithms in practice.
286	129	Last, we hope to investigate more general relations between the load and actual reward.
