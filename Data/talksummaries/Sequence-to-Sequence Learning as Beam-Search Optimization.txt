1	28	In addition to demonstrating impressive results for machine translation (Bahdanau et al., 2015), roughly the same model and training have also proven to be useful for sentence compression (Filippova et al., 2015), parsing (Vinyals et al., 2015), and dialogue systems (Serban et al., 2016), and they additionally underlie other text generation applications, such as image or video captioning (Venugopalan et al., 2015; Xu et al., 2015).
11	45	In this work we develop a non-probabilistic variant of the seq2seq model that can assign a score to any possible target sequence, and we propose a training procedure, inspired by the learning as search optimization (LaSO) framework of Daumé III and Marcu (2005), that defines a loss function in terms of errors made during beam search.
45	39	The decoder is tasked with generating a target sequence of words from a target vocabulary V .
51	20	Applying an RNN to any such sequence yields hidden states ht at each time-step t, as follows: ht ← RNN(mt,ht−1;θ), where θ is the set of model parameters, which are shared over time.
54	24	That is, one attempts to model the probability of the t’th target word conditioned on x and the target history by stipulating that p(wt|w1:t−1,x) = g(wt,ht−1,x), for some parameterized function g typically computed with an affine layer followed by a softmax.
63	86	Define the score of a sequence consisting of history w1:t−1 followed by a single word wt as f(wt,ht−1,x), where f is a parameterized function examining the current hidden-state of the relevant RNN at time t− 1 as well as the input representation x.
67	23	However, because finding the argmax sequence according to this model is intractable, we propose to adopt a LaSO-like (Daumé III and Marcu, 2005) scheme to train, which we will refer to as beam search optimization (BSO).
68	20	In particular, we define a loss that penalizes the gold sequence falling off the beam during training.1 The proposed training approach is a simple way to expose the model to incorrect histories and to match the training procedure to test generation.
70	33	We now formalize this notion of a search-based loss for RNN training.
71	21	Assume we have a set St of K candidate sequences of length t. We can calculate a score for each sequence in St using a scoring function f parameterized with an RNN, as above, and we define the sequence ŷ(K)1:t ∈St to be the K’th ranked sequence in St according to f .
73	25	We now define a loss function that gives loss each time the score of the gold prefix y1:t does not exceed that of ŷ(K)1:t by a margin: L(f) = T∑ t=1 ∆(ŷ (K) 1:t ) [ 1− f(yt,ht−1) + f(ŷ(K)t , ĥ (K) t−1) ] .
77	213	Finally, because we want the full gold sequence to be at the top of the beam at the end of search, when t=T we modify the loss to require the score of y1:T to exceed the score of the highest ranked incorrect prediction by a margin.
79	21	Unlike standard seq2seq training, the first-step requires running search (in our case beam search) to find margin violations.
82	60	In order to minimize this loss, we need to specify a procedure for constructing candidate sequences ŷ(k)1:t at each time step t so that we find margin violations.
84	34	If there was no margin violation at t−1, then St is constructed using a standard beam search update.
85	80	If there was a margin violation, St is constructed as the K best sequences assuming the gold history y1:t−1 through time-step t−1.
89	30	For instance, if we would like to use seq2seq models for parsing (by emitting a constituency or dependency structure encoded into a sequence in some way), we will have hard constraints on the sequences the model can output, namely, that they represent valid parses.
104	70	The top of the diagram shows a possible sequence of ŷ(k)1:t formed during search with a beam of size 3 for the target sequence y = “a red dog runs quickly today.” When the gold sequence falls off the beam at t= 4, search resumes with S5 = succ(y1:4), and so all subsequent predicted sequences have y1:4 as a prefix and are thus functions of h4.
105	31	Moreover, because our loss function only involves the scores of the gold prefix and the violating prefix, we end up with the relatively simple computation tree shown at the bottom of Figure 1.
106	69	It is evident that we can backpropagate in a single pass, accumulating gradients from sequences that diverge from the gold at the time-step that precedes their divergence.
109	24	While the method we describe applies to seq2seq RNNs in general, for all experiments we use the global attention model of Luong et al. (2015) — which consists of an LSTM (Hochreiter and Schmidhuber, 1997) encoder and an LSTM decoder with a global attention model — as both the baseline seq2seq model (i.e., as the model that computes the g in Section 3) and as the model that computes our sequence-scores f(wt,ht−1,x).
110	66	As in Luong et al. (2015), we also use “input feeding,” which involves feeding the attention distribution from the previous time-step into the decoder at the current step.
130	35	Word Ordering The task of correctly ordering the words in a shuffled sentence has recently gained some attention as a way to test the (syntactic) capabilities of text-generation systems (Zhang and Clark, 2011; Zhang and Clark, 2015; Liu et al., 2015; Schmaltz et al., 2016).
140	46	We see that on this task there is a large improvement at each beam size from switching to BSO, and a further improvement from using the constrained model.
144	19	We treat dependency parsing with arc-standard transitions as a seq2seq task by attempting to map from a source sentence to a target sequence of source sentence words interleaved with the arc-standard, reduce-actions in its parse.
146	54	All models thus see only the words in the source and, when decoding, the actions it has emitted so far; no other features are used.
161	20	We emphasize, however, that while our decoder LSTM is of the same size as that of Ranzato et al. (2016), our results are not directly comparable, because we use an LSTM encoder (rather than a convolutional encoder as they do), a slightly different attention mechanism, and input feeding (Luong et al., 2015).
164	59	In Table 4 we show our final results and those from Ranzato et al. (2016).8 While we start with an improved baseline, we see similarly large increases in accuracy as those obtained by DAD and MIXER, in particular when Kte > 1.
172	112	We have introduced a variant of seq2seq and an associated beam search training scheme, which addresses exposure bias as well as label bias, and moreover allows for both training with sequencelevel cost functions as well as with hard constraints.
173	142	Future work will examine scaling this approach to much larger datasets.
