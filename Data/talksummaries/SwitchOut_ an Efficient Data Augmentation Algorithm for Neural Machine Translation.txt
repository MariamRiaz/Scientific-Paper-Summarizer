3	17	However, it is nontrivial to find simple equivalences for NLP tasks like machine translation, because even slight modifications of sentences can result in significant changes in their semantics, or require corresponding changes in the translations in order to keep the data consistent.
5	45	Due to such difficulties, the literature in data augmentation for NMT is relatively scarce.
9	30	The second category is based on word replacements.
10	14	For instance, Fadaee et al. (2017) propose to replace words in the target sentences with rare words in the target vocabulary according to a language model, and then modify the aligned source words accordingly.
12	25	Other generic word replacement methods include word dropout (Sennrich et al., 2016a; Gal and Ghahramani, 2016), which uniformly set some word embeddings to 0 at random, and Reward Augmented Maximum Likelihood (RAML; Norouzi et al. (2016)), whose implementation essentially replaces some words in the target sentences with other words from the target vocabulary.
16	67	Second, we interpret the aforementioned solution and propose a novel method: independently replacing words in both the source sentence and the target sentence by other words uniformly sampled from the source and the target vocabularies, respectively.
24	8	A potential weakness of MLE is the mismatch between bp(X,Y ) and the true data distribution p(X,Y ).
25	20	Specifically, bp(X,Y ) is usually a bootstrap distribution defined only on the observed training pairs, while p(X,Y ) has a much larger support, i.e. the entire space of valid pairs.
28	8	Formally, let q( bX, bY ) be the augmented distribution defined on a larger support than the empirical distribution bp(X,Y ).
37	12	To formalize both assumptions, let s(bx, by;x, y) be a similarity function that measures how similar an augmented pair (bx, by) is to an observed data pair (x, y).
39	19	First, based on the smoothness assumption, if an augmented pair (bx, by) is more similar to an empirical pair (x, y), it is more likely that (bx, by) is sampled under the true data distribution p(X,Y ), and thus q( bX, bY |x, y) should assign a significant amount of probability mass to (bx, by).
40	28	Second, to quantify the diversity assumption, we propose that the entropy H[q( bX, bY |x, y)] should be large, so that the support of q( bX, bY ) is larger than the support of bp and thus is closer to the support p(X,Y ).
52	12	From the perspective of reinforcement learning, Norouzi et al. (2016) propose to train the model distribution to match a target distribution proportional to an exponentiated reward.
54	9	Appendix A.2) that RAML can be viewed as an instantiation of our generic framework, where the similarity measure is s(bx, by;x, y) = r(by; y) if bx = x and 1 otherwise.
55	13	Here, r is a task-specific reward function which measures the similarity between by and y.
56	8	Intuitively, this means that RAML only exploits the smoothness property on the target side while keeping the source side intact.
58	30	Firstly, augmentation should not be restricted to only the source side or the target side.
62	8	This allows us to factor q⇤(bx, by|x, y) into: q⇤(bx, by|x, y) = exp {rx(bx, x)/⌧x}P bx0 exp {rx(bx0, x)/⌧x} ⇥ exp {ry(by, y)/⌧y}P by0 exp {ry(by0, y)/⌧y} (4) In addition, notice that this factored formulation allows bx and by to be sampled independently.
70	17	However, this efficient sampling procedure is much easier to implement while achieving good performance.
74	25	Input : s: a sentence represented by vocab integral ids, ⌧ : the temperature, V : the vocabulary Output : bs: a sentence with words replaced 1 Function HammingDistanceSample(s, ⌧ , |V |): 2 Let Z(⌧) P|s| n=0 e n/⌧ be the partition function.
79	8	Detailed statistics and pre-processing schemes are in Appendix A.3.
81	14	Our translation model, i.e. p ✓ (y|x), is a Transformer network (Vaswani et al., 2017).
86	31	While the Transformer network without SwitchOut is already a strong baseline, we also compare SwitchOut against two other baselines that further use existing varieties of data augmentation: 1) word dropout on the source side with the dropping probability of word = 0.1; and 2) RAML on the target side, as in Section 2.4.
96	10	Notably, SwitchOut on the source demonstrates as large gains as these obtained by RAML on the target side, and SwitchOut delivers further improvements when combined with RAML.
111	30	To test this hypothesis, for each test sentence we find its most similar training sample (i.e. nearest neighbor), then bucket the instances by the distance to their nearest neighbor and measure the gain in BLEU afforded by SwitchOut for each bucket.
113	166	As we can see, SwitchOut improves increasingly more as the WER increases, indicating that SwitchOut is indeed helping on examples that are far from the sentences that the model sees during training.
114	18	This is the desirable effect of data augmentation techniques.
115	21	In this paper, we propose a method to design data augmentation algorithms by solving an optimization problem.
116	13	These solutions subsume a few existing augmentation schemes and inspire a novel augmentation method, SwitchOut.
117	17	SwitchOut delivers improvements over translation tasks at different scales.
