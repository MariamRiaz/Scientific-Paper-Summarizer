{"sections": [{"text": "\u221a n randomly selected\nitems. Along the way, we provide an optimal differentially private algorithm for singular vector computation, based on the celebrated Oja\u2019s method, that provides significant savings in terms of space and time while operating on sparse matrices. We also empirically evaluate our algorithm on a suite of datasets, and show that it consistently outperforms the state-of-the-art private algorithms."}, {"heading": "1. Introduction", "text": "Collaborative filtering (or matrix completion) is a popular approach for modeling the recommendation system problem, where the goal is to provide personalized recommendations about certain items to a user (Koren & Bell, 2015). In other words, the objective of a personalized recommendation system is to learn the entire users-items preference matrix Y \u2217 \u2208 <m\u00d7n using a small number of user-item preferences Y \u2217ij , (i, j) \u2208 [m] \u00d7 [n], where m is the number of users and n is the number of items. Naturally, in absence of any structure in Y \u2217, the problem is ill-defined as the unknown entries of Y \u2217 can be arbitrary. Hence, a popular modeling hypothesis is that the underlying preference matrix Y \u2217 is low-rank, and thus, the collaborative filtering problem reduces to that of low-rank matrix com-\n1Microsoft Research. Email: prajain@microsoft.com 2Department of Computer Science, Boston University. Email: omthkkr@bu.edu 3Computer Science Department, University of California Santa Cruz. Email: aguhatha@ucsc.edu. Correspondence to: Om Thakkar <omthkkr@bu.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\npletion (Recht, 2011; Candes & Recht, 2012). One can also enhance this formulation using side-information like userfeatures or item-features (Yu et al., 2014).\nNaturally, personalization problems require collecting and analyzing sensitive customer data like their preferences for various items, which can lead to serious privacy breaches (Korolova, 2010; Narayanan & Shmatikov, 2010; Calandrino et al., 2011). In this work, we attempt to address this problem of privacy-preserving recommendations using collaborative filtering (McSherry & Mironov, 2009; Liu et al., 2015). We answer the following question in the affirmative: Can we design a matrix completion algorithm which keeps all the ratings of a user private, i.e., guarantees userlevel privacy while still providing accurate recommendations? In particular, we provide the first differentially private (Dwork et al., 2006b) matrix completion algorithms with provable accuracy guarantees. Differential privacy (DP) is a rigorous privacy notion which formally protects the privacy of any user participating in a statistical computation by controlling her influence to the final output.\nMost of the prior works on DP matrix completion (and lowrank approximation) (Blum et al., 2005; Chan et al., 2011; Hardt & Roth, 2012; 2013; Kapralov & Talwar, 2013; Dwork et al., 2014b) have provided guarantees which are non-trivial only in the entry-level privacy setting, i.e., they preserve privacy of only a single rating of a user. Hence, they are not suitable for preserving a user\u2019s privacy in practical recommendation systems. In fact, their trivial extension to user-level privacy leads to vacuous bounds (see Table 1). Some works (McSherry & Mironov, 2009; Liu et al., 2015) do serve as an exception, and directly address the user-level privacy problem. However, they only show empirical evidences of their effectiveness; they do not provide formal error bounds.1 In contrast, we provide an efficient algorithm based on the classic Frank-Wolfe (FW) procedure (Frank & Wolfe, 1956), and show that it gives strong utility guarantees while preserving user-level privacy. Furthermore, we empirically demonstrate its effectiveness on various benchmark datasets.\nOur private FW procedure needs to compute the top right singular vector of a sparse user preference matrix, while\n1In case of (Liu et al., 2015), the DP guarantee itself might require an exponential amount of computation.\npreserving DP. For practical recommendation systems with a large number of items, this step turns out to be a significant bottleneck both in terms of space as well as time complexity. To alleviate this issue, we provide a method, based on the celebrated Oja\u2019s algorithm (Jain et al., 2016), which is nearly optimal in terms of the accuracy of the computed singular vector while still providing significant improvement in terms of space and computation. In fact, our method can be used to speed-up even the vanilla differentially private PCA computation (Dwork et al., 2013). To the best of our knowledge, this is the first algorithm for DP singular value computation with optimal utility guarantee, that also exploits the sparsity of the underlying matrix.\nNotion of privacy: To measure privacy, we select differential privacy, which is a de-facto privacy notion for largescale learning systems, and has been widely adopted by the academic community as well as big corporations like Google (Erlingsson et al., 2014), Apple (McMillan, 2016), etc. The underlying principle of standard DP is that the output of the algorithm should not change significantly due to presence or absence of any user. In the context of matrix completion, where the goal is to release the entire preference matrix while preserving privacy, this implies that the computed ratings/preferences for any particular user cannot depend strongly on her own personal preferences. Naturally, the resulting preference computation is going to be trivial and inaccurate (which also follows from the reconstruction attacks of (Dinur & Nissim, 2003) and (Hardt & Roth, 2012)).\nTo alleviate this concern, we consider a relaxed but natural DP notion (for recommendation systems) called joint differential privacy (Kearns et al., 2014). Consider an algorithm A that produces individual outputs Yi for each user i, i.e., the i-th row of preference matrix Y . Joint DP ensures that for each user i, the output ofA for all other users (denoted by Y\u2212i) does not reveal \u201cmuch\u201d about the preferences of user i. That is, the recommendations made to all the users except the i-th user do not depend significantly upon the i-th user\u2019s preferences. Although not mentioned explicitly, previous works on DP matrix completion (McSherry & Mironov, 2009; Liu et al., 2015) strive to ensure Joint DP. Formal definitions are provided in Section 2.\nGranularity of privacy: DP protects the information about a user in the context of presence or absence of her data record. Prior works on DP matrix completion (McSherry & Mironov, 2009; Liu et al., 2015), and its close analogue, low-rank approximation (Blum et al., 2005; Chan et al., 2011; Hardt & Roth, 2012; Dwork et al., 2013; Hardt & Roth, 2013), have considered different variants of the notion of a data record. Some have considered a single entry in the matrix Y \u2217 as a data record (resulting in entry-level privacy), whereas others have considered a more practical\nsetting where the complete row is a data record (resulting in user-level privacy). In this work, we present all our results in the strictly harder user-level privacy setting. To ensure a fair comparison, we present the results of prior works in the same setting."}, {"heading": "1.1. Problem definition: Matrix completion", "text": "The goal of a low-rank matrix completion problem is to estimate a low-rank (or a convex relaxation of bounded nuclear norm) matrix Y \u2217 \u2208 <m\u00d7n, having seen only a small number of entries from it. Here, m is the number of users, and n is the number of items. Let \u2126 = {(i, j) \u2286 [m]\u00d7 [n]} be the index set of the observed entries from Y \u2217, and let P\u2126 : <m\u00d7n \u2192 <m\u00d7n be a matrix operator s.t. P\u2126(Y )ij = Yij if (i, j) \u2208 \u2126, and 0 otherwise. Given, P\u2126(Y \u2217), the objective is to output a matrix Y such that the following generalization error, i.e., the error in approximating a uniformly random entry from the matrix Y \u2217, is minimized:\nF (Y ) = E(i,j)\u223cunif [m]\u00d7[n] [( Yij \u2212 Y \u2217ij )2] . (1)\nGeneralization error captures the ability of an algorithm to predict unseen samples from Y \u2217. We would want the generalization error to be o(1) in terms of the problem parameters when \u2126 = o(mn). Throughout the paper, we will assume that m > n."}, {"heading": "1.1.1. OUR CONTRIBUTIONS", "text": "In this work, we provide the first joint DP algorithm for low-rank matrix completion with formal non-trivial error bounds, which are summarized in Tables 1 and 2. At a high level, our key result can be summarized as follows:\nInformal Theorem 1.1 (Corresponds to Corollary 3.1). Assume that each entry of a hidden matrix Y \u2217 \u2208 <m\u00d7n is in [\u22121, 1], and there are \u221a n observed entries per user. Also, assume that the nuclear norm of Y \u2217 is bounded by O( \u221a mn), i.e., Y \u2217 has nearly constant rank. Then, there exist ( , \u03b4)-joint differentially private algorithms that have o(1) generalization error as long as m = \u03c9(n5/4).\nIn other words, even with \u221a n observed ratings per user, we obtain asymptotically the correct estimation of each entry of Y \u2217 on average, as long asm is large enough. The sample complexity bound dependence on m can be strengthened by making additional assumptions, such as incoherence, on Y \u2217. See the supplementary material for details.\nOur algorithm is based on two important ideas: a) using local and global computation, b) using the Frank-Wolfe method as a base optimization technique.\nLocal and global computation: The key idea that defines our algorithm, and allows us to get strong error bounds under joint DP is splitting the algorithm into two components:\nglobal and local. Recall that each row of the hidden matrix Y \u2217 belongs to an individual user. The global component of our algorithm computes statistics that are aggregate in nature (e.g., computing the correlation across columns of the revealed matrix P\u2126(Y \u2217)). On the other hand, the local component independently fine-tunes the statistics computed by the global component to generate accurate predictions for each user. Since the global component depends on the data of all users, adding noise to it (for privacy) does not significantly affect the accuracy of the predictions. (McSherry & Mironov, 2009; Liu et al., 2015) also exploit a similar idea of segregating the computation, but they do not utilize it formally to provide non-trivial error bounds.\nFrank-Wolfe based method: We use the standard nuclear norm formulation (Recht, 2011; Shalev-shwartz et al., 2011; Tewari et al., 2011; Candes & Recht, 2012) for the matrix completion problem:\nmin \u2016Y \u2016nuc\u2264k F\u0302 (Y ), (2)\nwhere F\u0302 (Y ) = 12|\u2126|\u2016P\u2126(Y \u2212 Y \u2217)\u20162F , \u2016Y \u2016nuc is the sum\nof singular values of Y , and the underlying hidden matrix Y \u2217 is assumed to have nuclear norm of at most k. Note that we denote the empirical risk of a solution Y by F\u0302 (Y ) throughout the paper. We use the popular Frank-Wolfe algorithm (Frank & Wolfe, 1956; Jaggi & Sulovsky, 2010) as our algorithmic building block. At a high-level, FW computes the solution to (2) as a convex combination of rankone matrices, each with nuclear norm at most k. These matrices are added iteratively to the solution.\nOur main contribution is to design a version of the FW method that preserves Joint DP. That is, if the standard FW algorithm decides to add matrix u \u00b7 vT during an iteration, our private FW computes a noisy version of v \u2208 <n via its global component. Then, each user computes the respective element of u \u2208 <m to obtain her update. The noisy version of v suffices for the Joint DP guarantee, and allows us to provide the strong error bound in Theorem 1.1 above.\nWe want to emphasize that the choice of FW as the underlying matrix completion algorithm is critical for our system. FW updates via rank-one matrices in each step. Hence, the error due to noise addition in each step is small (i.e., proportional to the rank), and allows for an easy decomposition into the local-global computation model. Other standard techniques like proximal gradient descent based techniques (Cai et al., 2010b; Lin et al., 2010) can involve nearly full-rank updates in an iteration, and hence might incur large error, leading to arbitrary inaccurate solutions. Note that though a prior work (Talwar et al., 2015) has proposed a DP Frank-Wolfe algorithm for high-dimensional regression, it was for a completely different problem in a different setting where the segregation of computation into global and local components was not necessary.\nPrivate singular vector of sparse matrices using Oja\u2019s method: Our private FW requires computing a noisy covariance matrix which implies \u2126(n2) space/time complexity for n items. Naturally, such an algorithm does not scale to practical recommendation systems. In fact, this drawback exists even for standard private PCA techniques (Dwork et al., 2013). Using insights from the popular Oja\u2019s method, we provide a technique (see Algorithm 2) that has a linear dependency on n as long as the number of ratings per user is small. Moreover, the performance of our private FW method isn\u2019t affected by using this technique.\nSVD-based method: In the supplementary material, we also extend our technique to a singular value decomposition (SVD) based method for matrix completion/factorization. Our utility analysis shows that there are settings where this method outperforms our FW-based method, but in general it can provide a significantly worse solution. The main goal is to study the power of the simple SVD-based method, which is still a popular method for collaborative filtering.\nEmpirical results: Finally, we show that along with providing strong analytical guarantees, our private FW also performs well empirically. In particular, we show its efficacy on benchmark collaborative filtering datasets like Jester (Goldberg et al., 2001), MovieLens (Harper & Konstan, 2015), the Netflix prize dataset (Bennett et al., 2007), and the Yahoo! Music recommender dataset (Yahoo, 2011). Our algorithm consistently outperforms (in terms of accuracy) the existing state-of-the-art DP matrix completion methods (SVD-based method by (McSherry & Mironov, 2009), and a variant of projected gradient descent (Cai et al., 2010c; Bassily et al., 2014b; Abadi et al., 2016))."}, {"heading": "1.2. Comparison to prior work", "text": "As discussed earlier, our results are the first to provide nontrivial error bounds for DP matrix completion. For comparing different results, we consider the following setting of the hidden matrix Y \u2217 \u2208 <m\u00d7n and the set of released entries \u2126: i) |\u2126| \u2248 m \u221a n, ii) each row of Y \u2217 has an `2 norm of \u221a n, and iii) each row of P\u2126(Y \u2217) has `2-norm at most n1/4, i.e., \u2248 \u221a n random entries are revealed for each row. Furthermore, we assume the spectral norm of Y \u2217 is at most O( \u221a mn), and Y \u2217 is rank-one. Note that these conditions are satisfied by a matrix Y \u2217 = u\u00b7vT where ui, vj \u2208 [\u22121, 1] \u2200i, j, and \u221a n random entries are observed per user.\nIn Table 1, we provide a comparison based on the sample complexity, i.e., the number of usersm and the number observed samples |\u2126| needed to attain a generalization error of o(1). We compare our results with the best non-private algorithm for matrix completion based on nuclear norm minimization (Shalev-shwartz et al., 2011), and the prior work on DP matrix completion (McSherry & Mironov, 2009; Liu et al., 2015). We see that for the same |\u2126|, the\nsample complexity on m increases from \u03c9(n) to \u03c9(n5/4) for our FW-based algorithm. While (McSherry & Mironov, 2009; Liu et al., 2015) work under the notion of Joint DP as well, they do not provide any formal accuracy guarantees.\nInterlude: Low-rank approximation. We also compare our results with the prior work on a related problem of DP lowrank approximation. Given a matrix Y \u2217 \u2208 <m\u00d7n, the goal is to compute a DP low-rank approximation Ypriv, s.t. Ypriv is close to Y \u2217 either in the spectral or Frobenius norm. Notice that this is similar to matrix completion if the set of revealed entries \u2126 is the complete matrix. Hence, our methods can be applied directly. To be consistent with the existing literature, we assume that Y \u2217 is rank-one matrix, and each row of Y \u2217 has `2-norm at most one . Table 2 compares the various results. While all the prior works provide trivial error bounds (in both Frobenius and spectral norm, as \u2016Y \u2217\u20162 = \u2016Y \u2217\u2016F \u2264 \u221a m), our methods provide non-trivial bounds. The key difference is that we ensure Joint DP (Definition 2.2), while existing methods ensure the stricter standard DP (Definition 2.1), with the exponential mechanism (Kapralov & Talwar, 2013) ensuring ( , 0)-standard DP.\nmatrix Ypriv isO\n(\nm2/5/n1/5\n)\nfor Private FW, whereas it isO(1) for the others. References: \u2021(Blum et al., 2005; Chan et al., 2011; Dwork et al., 2014b), S(Hardt & Roth, 2012), \u00b6(Hardt & Roth, 2013),\u00a3(Kapralov & Talwar, 2013)"}, {"heading": "2. Background: Notions of privacy", "text": "Let D = {d1, \u00b7 \u00b7 \u00b7 , dm} be a dataset of m entries. Each entry di lies in a fixed domain T , and belongs to an individual i, whom we refer to as an agent in this paper. Furthermore, di encodes potentially sensitive information about agent i. Let A be an algorithm that operates on dataset D, and produces a vector of m outputs, one for each agent i and from a set of possible outputs S. Formally, let A : T m \u2192 Sm. Let D\u2212i denote the dataset D without the entry of the i-th agent, and similarly A\u2212i(D) be the set of outputs without the output for the i-th agent. Also, let (di;D\u2212i) denote the dataset obtained by adding data entry di to the dataset D\u2212i. In the following, we define both standard differential privacy and joint differential privacy , and contrast them.\nDefinition 2.1 (Standard differential privacy (Dwork et al., 2006a;b)). An algorithm A satisfies ( , \u03b4)-differential privacy if for any agent i, any two possible values of data entry di, d \u2032 i \u2208 T for agent i, any tuple of data entries for all other agents, D\u2212i \u2208 T m\u22121, and any output S \u2208 Sm, we have Pr A [A (di;D\u2212i) \u2208 S] \u2264 e Pr A [A (d\u2032i;D\u2212i) \u2208 S] + \u03b4.\nAt a high-level, an algorithm A is ( , \u03b4)-standard DP if for any agent i and dataset D, the output A(D) and D\u2212i do not reveal \u201cmuch\u201d about her data entry di. For reasons mentioned in Section 1, our matrix completion algorithms provide privacy guarantee based on a relaxed notion of DP, called joint differential privacy , which was initially proposed in (Kearns et al., 2014). At a high-level, an algorithm A preserves ( , \u03b4)-joint DP if for any agent i and datasetD, the output of A for the other (m \u2212 1) agents (denoted by A\u2212i(D)) and D\u2212i do not reveal \u201cmuch\u201d about her data entry di. Such a relaxation is necessary for matrix completion because an accurate completion of the row of an agent can reveal a lot of information about her data entry. However, it is still a very strong privacy guarantee for an agent even if every other agent colludes against her, as long as she does not make the predictions made to her public.\nDefinition 2.2 (Joint differential privacy (Kearns et al., 2014)). An algorithm A satisfies ( , \u03b4)-joint differential privacy if for any agent i, any two possible values of data entry di, d\u2032i \u2208 T for agent i, any tuple of data entries for all other agents,D\u2212i \u2208 T m\u22121, and any output S \u2208 Sm\u22121,\nPr A [A\u2212i (di;D\u2212i) \u2208 S] \u2264 e Pr A [A\u2212i (d\u2032i;D\u2212i) \u2208 S] + \u03b4.\nIn this paper, we consider the privacy parameter to be a small constant (\u2248 0.1), and \u03b4 < 1/m. There are semantic reasons for such choice of parameters (Kasiviswanathan & Smith, 2008), but that is beyond the scope of this work."}, {"heading": "3. Private matrix completion via Frank-Wolfe", "text": "Recall that the objective is to solve the matrix completion problem (defined in Section 1.1) under Joint DP. A standard modeling assumption is that Y \u2217 is nearly low-rank, leading to the following empirical risk minimization problem (Keshavan et al., 2010; Jain et al., 2013; Jin et al., 2016):\nmin rank(Y )\u2264k\n1\n2|\u2126| \u2016P\u2126(Y \u2212 Y \u2217)\u20162F\ufe38 \ufe37\ufe37 \ufe38\nF\u0302 (Y )\n, where k min(m,n).\nAs this is a challenging non-convex optimization problem, a popular approach is to relax the rank constraint to a nuclear-norm constraint, i.e., min\n\u2016Y \u2016nuc\u2264k F\u0302 (Y ).\nTo this end, we use the FW algorithm (see the supplementary material for more details) as our building block. FW is a popular conditional gradient algorithm in which the current iterate is updated as: Y (t) \u2190 (1 \u2212 \u03b7)Y (t\u22121) + \u03b7 \u00b7 G, where \u03b7 is the step size, and G is given by: argmin \u2016G\u2016nuc\u2264k \u2329 G,\u2207Y (t\u22121) F\u0302 (Y ) \u232a . Note that the optimal solution to the above problem is G = \u2212kuv>, where (\u03bb, u, v) are the top singular components of A(t\u22121) = P\u2126(Y (t\u22121)\u2212 Y \u2217). Also, the optimal G is a rank-one matrix.\nAlgorithmic ideas: In order ensure Joint DP and still have strong error guarantees, we develop the following ideas. These ideas have been formally compiled into Algorithm 1. Notice that both the functionsAglobal andAlocal in Algorithm 1 are parts of the Private FW technique, whereAglobal consists of the global component, and each user runsAlocal at her end to carry out a local update. Throughout this discussion, we assume that max\ni\u2208[m] \u2016P\u2126(Y \u2217i )\u20162 \u2264 L.\nSplitting the update into global and local components: One can equivalently write the Frank-Wolfe update as follows:\nY (t) \u2190 (1\u2212\u03b7)Y (t\u22121)\u2212\u03b7 \u00b7 k\u03bbA (t\u22121)vv>, whereA(t\u22121),v, and \u03bb are defined as above. Note that v and \u03bb2 can also be obtained as the top right eigenvector and eigenvalue of A(t\u22121) > A(t\u22121) =\nm\u2211 i=1 Ai (t\u22121)>Ai (t\u22121), where Ai(t\u22121) =\nP\u2126(Yi (t\u22121) \u2212 Y \u2217i ) is the i-th row of A(t\u22121). We will use the global component Aglobal in Algorithm 1 to compute v and \u03bb. Using the output of Aglobal, each user (row) i \u2208 [m] can compute her local update (using Alocal) as follows:\nYi (t) = (1\u2212 \u03b7)Yi(t\u22121)\u2212\n\u03b7k\n\u03bb P\u2126(Y\n(t\u22121)\u2212 Y \u2217)ivv>. (3)\nA block schematic of this idea is presented in Figure 1.\nAlgorithm 1 Private Frank-Wolfe algorithm function Global Component Aglobal (Input- privacy parameters: ( , \u03b4) s.t. \u2264 2 log (1/\u03b4), total number of iterations: T , bound on \u2016P\u2126(Y \u2217i )\u20162: L, failure probability: \u03b2, number of users: m, number of items: n) \u03c3 \u2190 L2 \u221a 64 \u00b7 T log(1/\u03b4)/ , v\u0302\u2190 {0}n, \u03bb\u0302\u2190 0\nfor t \u2208 [T ] do W (t) \u2190 {0}n\u00d7n, \u03bb\u0302\u2032 \u2190 \u03bb\u0302+ \u221a \u03c3 log(n/\u03b2)n1/4\nfor i \u2208 [m] do W (t) \u2190W (t) +Alocal(i, v\u0302, \u03bb\u0302\u2032, T, t, L) W\u0302 (t) \u2190 W (t) + N (t), where N (t) \u2208 <n\u00d7n is a matrix with i.i.d. entries from N (0, \u03c32) (v\u0302, \u03bb\u03022)\u2190 Top eigenvector and eigenvalue of W\u0302 (t)\nend for end function function Local Update Alocal (Input- user number: i, top right singular vector: v\u0302, top singular value: \u03bb\u0302\u2032, total number of iterations: T , current iteration: t, bound on \u2016P\u2126(Y \u2217i )\u20162: L, private true matrix row: P\u2126(Y \u2217i )) Yi\n(0) \u2190 {0}n, Ai(t\u22121) \u2190 P\u2126(Yi(t\u22121) \u2212 Y \u2217i ) u\u0302i \u2190 (Ai(t\u22121) \u00b7 v\u0302)/\u03bb\u0302\u2032\nDefine \u03a0L,\u2126 (M)i,j = min {\nL \u2016P\u2126(Mi)\u20162\n, 1 } \u00b7Mi,j\nYi (t) \u2190 \u03a0L,\u2126 (( 1\u2212 1T ) Yi (t\u22121) \u2212 kT u\u0302i(v\u0302) T )\nAi (t) \u2190 P\u2126 ( Yi (t) \u2212 Y \u2217i )\nif t = T , Output Yi(T ) as prediction to user i and stop else Return Ai(t) > Ai\n(t) to Aglobal end function\nNoisy rank-one update: Observe that v and \u03bb, the statistics computed in each iteration of Aglobal, are aggregate statistics that use information from all rows of Y \u2217. This ensures that they are noise tolerant. Hence, adding sufficient noise can ensure standard DP (Definition 2.1) forAglobal. 2 Since\n2The second term in computing \u03bb\u0302\u2032 in Algorithm 1 is due to a bound on the spectral norm of the Gaussian noise matrix. We use this bound to control the error introduced in the computation of \u03bb\u0302.\nthe final objective is to satisfy Joint DP (Definition 2.2), the local component Alocal can compute the update for each user (corresponding to (3)) without adding any noise.\nControlling norm via projection: In order to control the amount of noise needed to ensure DP, any individual data entry (here, any row of Y \u2217) should have a bounded effect on the aggregate statistic computed by Aglobal. However, each intermediate computation Yi(t) in (3) can have high `2-norm even if \u2016P\u2126(Y \u2217i )\u20162 \u2264 L. We address this by applying a projection operator \u03a0L,\u2126 (defined below) to Yi (t), and compute the local update as \u03a0L,\u2126 ( Yi (t) )\nin place of (3). \u03a0L,\u2126 is defined as follows: For any matrix M , \u03a0L,\u2126 ensures that any row of the \u201czeroed out\u201d matrix P\u2126(M) does not have `2-norm higher than L. Formally, \u03a0L,\u2126 (M)i,j = min { L \u2016P\u2126(Mi)\u20162 , 1 } \u00b7Mi,j for all entries (i, j) of M . In our analysis, we show that this projection operation does not increase the error."}, {"heading": "3.1. Privacy and utility analysis", "text": "Theorem 3.1. Algorithm 1 satisfies ( , \u03b4)-joint DP.\nWe defer the proof to the supplementary material. The proof uses standard DP properties of Gaussian noise addition from (Bun & Steinke, 2016). The requirement \u2264 2 log (1/\u03b4) in the input of Algorithm 1 is due to a reduction of a Concentrated DP guarantee to a standard DP guarantee. We now show that the empirical risk of our algorithm is close to the optimal as long as the number of users m is \u201clarge\u201d. Theorem 3.2 (Excess empirical risk guarantee). Let Y \u2217 be a matrix with \u2016Y \u2217\u2016nuc \u2264 k, and max\ni\u2208[m] \u2016P\u2126(Y \u2217)i\u20162 \u2264 L.\nLet Y (T ) be a matrix, with its rows being Yi(T ) for all i \u2208 [m], computed by function Alocal in Algorithm 1 after T iterations. If \u2264 2 log ( 1 \u03b4 ) , then with probability at least 2/3 over the outcomes of Algorithm 1, the following is true: F\u0302 ( Y (T ) ) = O  k2 |\u2126|T + kT 1/4L \u221a n1/2 log1/2(1/\u03b4) logn |\u2126| \u221a\n . Furthermore, if T = O\u0303 ( k4/5 2/5\nn1/5L4/5\n) , then F\u0302 ( Y (T ) ) =\nO\u0303 ( k6/5n1/5L4/5\n|\u2126| 2/5\n) after hiding poly-logarithmic terms.\nWe defer the proof to the supplementary material. At a high-level, our proof combines the noisy eigenvector estimation error for Algorithm 1 with a noisy-gradient analysis of the FW algorithm. Also, note that the first term in the bound corresponds to the standard FW convergence error, while the second term can be attributed to the noise added for DP which directly depends on T . We also compute the optimal number of iterations required to minimize the empirical risk. Finally, the rank of Y (T ) is at most T , but its\nnuclear-norm is bounded by k. As a result, Y (T ) has low generalization error (see Section 3.1.1). Remark 1. We further illustrate our empirical risk bound by considering a simple setting: let Y \u2217 be a rank-one matrix with Y \u2217ij \u2208 [\u22121, 1] and |\u2126| = m \u221a n. Then k = O( \u221a mn),\nand L = O(n1/4), implying an error of O\u0303 (\u221a nm\u22122/5 ) hiding the privacy parameter ; in contrast, a trivial solution like Y = 0 leads to O(1) error. Naturally, the error increases with n as there is more information to be protected. However, it decreases with a larger number of users m as the presence/absence of a user has lesser effect on the solution with increasing m. We leave further investigation into the dependency of the error on m for future work. Remark 2. Our analysis does not require an upper bound on the nuclear norm of Y \u2217 (as stated in Theorem 3.2); we would instead incur an additional error of\nmin \u2016Y \u2016nuc\u2264k\n1 |\u2126| \u2016P\u2126 (Y \u2217 \u2212 Y )\u20162F . Moreover, consider a sim-\nilar scenario as in Remark 1, but |\u2126| = mn, i.e., all the entries of Y \u2217 are revealed. In such a case, L = O( \u221a n), and the problem reduces to that of standard low-rank matrix approximation of Y \u2217. Note that our result here leads to an error bound of O\u0303 ( n1/5m\u22122/5 ) , while the state-of-the-art result by (Hardt & Roth, 2013) leads to an error bound of O(1) due to being in the much stricter standard DP model."}, {"heading": "3.1.1. GENERALIZATION ERROR GUARANTEE", "text": "We now present a generalization error (defined in Equation 1) bound which shows that our approach provides accurate prediction over unknown entries. For obtaining our bound, we use Theorem 1 from (Srebro & Shraibman, 2005) (provided in the supplementary material for reference). Also, the output of Private FW (Algorithm 1) has rank at most T , where T is the number of iterations. Thus, replacing T from Theorem 3.2, we get the following: Corollary 3.1 (Generalization Error). Let \u2016Y \u2217\u2016nuc \u2264 k for a hidden matrix Y \u2217, and \u2016P\u2126(Y \u2217i )\u20162 \u2264 L for every row i of Y \u2217. If we choose the number of rounds in Algorithm 1 to be O ( k4/3\n(|\u2126|(m+n))1/3\n) , the data samples in \u2126 are drawn u.a.r.\nfrom [m]\u00d7 [n], and \u2264 2 log (\n1 \u03b4\n) , then with probability at\nleast 2/3 over the outcomes of the algorithm and choosing \u2126, the following is true for the final completed matrix Y :\nF (Y ) = O\u0303 ( k4/3Ln1/4\u221a\n|\u2126|13/6(m+ n)1/6 +\n( k \u221a m+ n\n|\u2126|\n)2/3) .\nThe O\u0303 (\u00b7) hides poly-logarithmic terms in m,n, |\u2126| and \u03b4. Remark 3. We further illustrate our bound using a setting similar to the one considered in Remark 1. Let Y \u2217 be a rank-one matrix with Y \u2217ij \u2208 [\u22121, 1] for all i, j; let |\u2126| \u2265 m \u221a n \u00b7 polylog(n), i.e., the fraction of movies rated by each user is arbitrarily small for larger n. For this setting, our generalization error is o(1) for m = \u03c9(n5/4).\nThis is slightly higher than the bound in the non-private setting by (Shalev-shwartz et al., 2011), where m = \u03c9(n) is sufficient to get generalization error o(1). Also, as the first term in the error bound pertains to DP, it decreases with a larger number of users m, and increases with n as it has to preserve privacy of a larger number of items. In contrast, the second term is the matrix completion error decreases with n. This is intuitive, as a larger number of movies enables more sharing of information between users, thus allowing a better estimation of preferences Y \u2217. However, just increasing m may not always lead to a more accurate solution (for example, consider the case of n = 1). Remark 4. The guarantee in Corollary 3.1 is for uniformly random \u2126, but using the results of (Shamir & ShalevShwartz, 2011), it is straightforward to extend our results to any i.i.d. distribution over \u2126. Moreover, we can extend our results to handle strongly convex and smooth loss functions instead of the squared loss considered in this paper."}, {"heading": "3.2. Efficient PCA via Oja\u2019s Algorithm", "text": "Algorithm 1 requires computing the top eigenvector of W\u0302 (t) = W (t) + N (t), where W (t) = \u2211 i ( Ai (t) )> Ai (t) and N (t) is a random noise matrix. However, this can be a bottleneck for computation as N (t) itself is a dense n\u00d7nmatrix, implying a space complexity of \u2126(n2 +mk), where k is the maximum number of ratings provided by a user. Similarly, standard eigenvector computation algorithms will require O(mk2 + n2) time (ignoring factors relating to rate of convergence), which can be prohibitive for practical recommendation systems with large n. We would like to stress that this issue plagues even standard DP PCA algorithms (Dwork et al., 2013), which have quadratic space-time complexity in the number of dimensions.\nWe tackle this by using a stochastic algorithm for the top eigenvector computation that significantly reduces both space and time complexity while preserving privacy. In particular, we use Oja\u2019s algorithm (Jain et al., 2016), which computes top eigenvectors of a matrix with a stochastic access to the matrix itself. That is, if we want to compute the top eigenvector of W (t), we can use the following updates:\nv\u0302\u03c4 = (I + \u03b7X\u03c4 )v\u0302\u03c4\u22121, v\u0302\u03c4 = v\u0302\u03c4/\u2016v\u0302\u03c4\u20162 (4)\nwhere E[X\u03c4 ] = W (t). For example, we can update v\u0302\u03c4 usingX\u03c4 = W (t)+N (t) \u03c4 where each entry ofN (t) \u03c4 is sampled i.i.d. from a Gaussian distribution calibrated to ensure DP. Even this algorithm in its current form does not decrease the space or time complexity as we need to generate a dense matrix N\u03c4 (t) in each iteration. However, by observing that N\u03c4\n(t)v = g\u03c4 \u223c N (0, \u03c321n) where v is independent of N\u03c4\n(t), we can now replace the generation of N\u03c4 (t) by the generation of a vector g\u03c4 , thus reducing both the space and time complexity of our algorithm. The computation of each\nAlgorithm 2 Private Oja\u2019s algorithm Input: Anm\u00d7nmatrixA s.t. each row \u2016Ai\u20162 \u2264 L, privacy parameters: ( , \u03b4) s.t. \u2264 2 log(1/\u03b4), total number of iterations: \u0393 \u03c3 \u2190 L2 \u221a 256 \u00b7 \u0393 log(2/\u03b4)/ , v\u03020 \u223c N (0, \u03c32I)\nfor \u03c4 \u2208 [\u0393] do \u03b7 = 1\n\u0393\u03c3 \u221a n , g\u03c4 \u223c N (0, \u03c321n) v\u0302\u03c4 \u2190 v\u0302\u03c4\u22121 + \u03b7 ( ATAv\u0302\u03c4\u22121 + g\u03c4 ) , v\u0302\u03c4 \u2190 v\u0302\u03c4/\u2016v\u0302\u03c4\u20162\nend for Return v\u0302\u0393, ( \u03bb\u03022\u0393 \u2190 ||A \u00b7 v\u0302\u0393||22 +N (0, \u03c32) )\nupdate is significantly cheaper as long as mk n2, which is the case for practical recommendation systems as k tends to be fairly small there (typically on the order of \u221a n).\nAlgorithm 2 provides a pseudocode of the eigenvector computation method. The computation of the approximate eigenvector v\u0302\u0393 and the eigenvalue \u03bb\u03022\u0393 in it is DP (directly follows via the proof of Theorem 3.1). The next natural question is how well can v\u0302\u0393 approximate the behavior of the top eigenvector of the non-private covariance matrix W (t)? To this end, we provide Theorem 3.3 below that analyzes Oja\u2019s algorithm, and shows that the Rayleigh quotient of the approximate eigenvector is close to the top eigenvalue of W (t). In particular, using Theorem 3.3 along with the fact that in our case, V = \u03c32n, we have \u2225\u2225A(t)\u2225\u22252 2 \u2264 \u2016A(t)v\u0302\u0393\u201622 +O (\u03c3 \u221a n log(\u03b7/\u03b2)) with high probability (w.p. \u2265 1\u2212\u03b22)), where v\u0302\u0393 is the output of Algorithm 2, \u0393 = \u2126 ( min { 1 \u03b2 , \u2016A(t)\u20162 \u03c3 \u221a n }) , and \u03b7 = 1 \u0393\u00b7\u03c3 \u221a n .\nNote that the above given bound is exactly the bound required in the proof of Theorem 3.2. Hence, computing the top eigenvector privately using Algorithm 2 does not change the utility bound of Theorem 3.2.\nTheorem 3.3 (Based on Theorem 3 (Allen-Zhu & Li, 2017)). Let X1, X2, . . . X\u0393 be sampled i.i.d. such that EXi = W = ATA. Moreover, let V = max{\u2016E(Xi \u2212W )T (Xi \u2212W )\u2016, \u2016E(Xi \u2212W )(Xi \u2212W )T \u2016}, and \u03b7 = 1\u221aV\u0393 . Then, the \u0393-th iterate of Oja\u2019s Algorithm (Update (4)) , i.e., v\u0302\u0393, satisfies (w.p. \u2265 1 \u2212 1/poly(\u0393)): v\u0302T\u0393W v\u0302\u0393 \u2265 \u2016W\u20162 \u2212O (\u221a V \u0393 + \u2016W\u20162 \u0393 ) .\nComparison with Private Power Iteration (PPI) method (Hardt & Roth, 2013): Private PCA via PPI provides utility guarantees dependent on the gap between the top and the kth eigenvalue of the input matrix A for some k > 1, whereas private Oja\u2019s utility guarantee is gap-independent."}, {"heading": "4. Experimental evaluation", "text": "We now present empirical results for Private FW (Algorithm 1) on several benchmark datasets, and compare its\nperformance to state-of-the-art methods like (McSherry & Mironov, 2009), and private as well as non-private variant of the Projected Gradient Descent (PGD) method (Cai et al., 2010c; Bassily et al., 2014a; Abadi et al., 2016). In all our experiments, we see that private FW provides accuracy very close to that of the non-private baseline, and almost always significantly outperforms both the private baselines.\nDatasets: As we want to preserve privacy of every user, and the output for each user is n-dimensional, we can expect the private recommendations to be accurate only when m n (see Theorem 3.1). Due to this constraint, we conduct experiments on the following datasets: 1) Synthetic: We generate a random rank-one matrix Y \u2217 = uvT with unit `\u221e-norm, m = 500K, and n = 400, 2) Jester: This dataset contains n = 100 jokes, and m \u2248 73K users, 3) MovieLens10M (Top 400): We pick the n = 400 most rated movies from the Movielens10M dataset, resulting in m \u2248 70K users, 4) Netflix (Top 400): We pick the n = 400 most rated movies from the Netflix prize dataset, resulting in m \u2248 474K users, and 5) Yahoo! Music (Top 400): We pick the n = 400 most rated songs from the Yahoo! music dataset, resulting in m \u2248 995K users.3 We rescale the ratings to be from 0 to 5 for Jester and Yahoo! Music.\nProcedure: For all datasets, we randomly sample 1% of the given ratings for measuring the test error. For experiments with privacy, for all datasets except Jester, we randomly select at most \u03be = 80 ratings per user to get P\u2126(Y \u2217). We vary the privacy parameter \u2208 [0.1, 5] 4, but keep \u03b4 = 10\u22126, thus ensuring that \u03b4 < 1m for all datasets. Moreover, we report results averaged over 10 independent runs.\nNote that the privacy guarantee is user-level, which effectively translates to an entry-level guarantee of entry = user \u03be , i.e., entry \u2208 [0.00125, 0.0625] as user \u2208 [0.1, 5].\nNon-private baseline: We find that non-private FW and non-private PGD converge to the same accuracy after tuning, and hence, we use this as our baseline.\nPrivate baselines: To the best of our knowledge, only (McSherry & Mironov, 2009) and (Liu et al., 2015) address the user-level DP matrix completion problem. While we present an empirical evaluation of the \u2018SVD after cleansing method\u2019 from the former, we refrain from comparing to the latter 5. We also provide a comparison with private PGD\n3For n = 900 with all the considered datasets (except Jester), we see that private PGD takes too long to complete; we present an evaluation for the other algorithms in the supplementary material.\n4The requirement in Algorithm 1 that \u2264 2 log (1/\u03b4) is satisfied by all the values of considered for the experiments.\n5The exact privacy parameters ( and \u03b4) for the Stochastic Gradient Langevin Dynamics based algorithm in (Liu et al., 2015) (correspondigly, in (Wang et al., 2015)) are unclear. They use a Markov chain based sampling method; to obtain quantifiable ( , \u03b4), the sampled distribution is required to converge (non-\n(pseudocode provided in the supplementary material).\nWe elaborate on the data normalization and the parameter choices for all algorithms in the supplementary material.\nResults: Figure 2 shows the results of our experiments6. Even though all the considered private algorithms satisfy Joint DP, our private FW method almost always incurs a significantly lower test RMSE than the two private baselines. Note that although non-private PGD provides similar empirical accuracy as non-private FW, the difference in performance for their private versions can be attributed to the noise being calibrated to a rank-one update for our private Frank-Wolfe.\nasymptotically) to a DP preserving distribution in `1 distance, for which we are not aware of any analysis.\n6In all our experiments, the implementation of private FW with Oja\u2019s method (Algorithm 2) did not suffer any perceivable loss of accuracy as compared to the variant in Algorithm 1; all the plots in Figure 2 remain identical."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Ilya Mironov, and the anonymous reviewers, for their helpful comments. This material is in part based upon work supported by NSF grants CCF-1740850 and IIS-1447700, and a grant from the Sloan foundation. The full version of this work is available at https://arxiv.org/abs/1712.09765."}], "year": 2018, "references": [{"title": "Deep learning with differential privacy", "authors": ["M. Abadi", "A. Chu", "I. Goodfellow", "H.B. McMahan", "I. Mironov", "K. Talwar", "L. Zhang"], "venue": "In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, CCS", "year": 2016}, {"title": "First efficient convergence for streaming k-pca: A global, gap-free, and near-optimal rate", "authors": ["Z. Allen-Zhu", "Y. Li"], "venue": "IEEE Annual Symposium on Foundations of Computer Science,", "year": 2017}, {"title": "Private empirical risk minimization: Efficient algorithms and tight error bounds", "authors": ["R. Bassily", "A. Smith", "A. Thakurta"], "venue": "In Foundations of Computer Science (FOCS),", "year": 2014}, {"title": "Private empirical risk minimization, revisited", "authors": ["R. Bassily", "A.D. Smith", "A. Thakurta"], "venue": "CoRR, abs/1405.7085,", "year": 2014}, {"title": "The netflix prize", "authors": ["J. Bennett", "S. Lanning", "N. Netflix"], "venue": "KDD Cup and Workshop in conjunction with KDD,", "year": 2007}, {"title": "Practical privacy: the sulq framework", "authors": ["A. Blum", "C. Dwork", "F. McSherry", "K. Nissim"], "venue": "In Proceedings of the twenty-fourth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,", "year": 2005}, {"title": "Concentrated differential privacy: Simplifications, extensions, and lower bounds", "authors": ["M. Bun", "T. Steinke"], "venue": "In TCC,", "year": 2016}, {"title": "A singular value thresholding algorithm for matrix completion", "authors": ["J. Cai", "E.J. Cand\u00e8s", "Z. Shen"], "venue": "SIAM Journal on Optimization,", "year": 2010}, {"title": "A singular value thresholding algorithm for matrix completion", "authors": ["J. Cai", "E.J. Cand\u00e8s", "Z. Shen"], "venue": "SIAM Journal on Optimization,", "year": 2010}, {"title": "A singular value thresholding algorithm for matrix completion", "authors": ["Cai", "J.-F", "E.J. Cand\u00e8s", "Z. Shen"], "venue": "SIAM Journal on Optimization,", "year": 2010}, {"title": "you might also like\u201d: Privacy risks of collaborative filtering", "authors": ["J.A. Calandrino", "A. Kilzer", "A. Narayanan", "E.W. Felten", "V. Shmatikov"], "venue": "In IEEE Symposium on Security and Privacy,", "year": 2011}, {"title": "Exact matrix completion via convex optimization", "authors": ["E. Candes", "B. Recht"], "venue": "Communications of the ACM,", "year": 2012}, {"title": "Private and continual release of statistics", "authors": ["Chan", "T.-H. H", "E. Shi", "D. Song"], "venue": "ACM Trans. Inf. Syst. Secur.,", "year": 2011}, {"title": "Coresets, sparse greedy approximation, and the frank-wolfe algorithm", "authors": ["K.L. Clarkson"], "venue": "ACM Transactions on Algorithms (TALG),", "year": 2010}, {"title": "Revealing information while preserving privacy", "authors": ["I. Dinur", "K. Nissim"], "venue": "In Proceedings of the Twenty-Second ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, June 9-12,", "year": 2003}, {"title": "Our data, ourselves: Privacy via distributed noise generation", "authors": ["C. Dwork", "K. Kenthapadi", "F. McSherry", "I. Mironov", "M. Naor"], "venue": "In EUROCRYPT,", "year": 2006}, {"title": "Calibrating noise to sensitivity in private data analysis", "authors": ["C. Dwork", "F. McSherry", "K. Nissim", "A. Smith"], "venue": "In Theory of Cryptography Conference,", "year": 2006}, {"title": "Randomized response strikes back: Private singular subspace computation with (nearly) optimal error guarantees", "authors": ["C. Dwork", "K. Talwar", "A. Thakurta", "L. Zhang"], "year": 2013}, {"title": "The algorithmic foundations of differential privacy", "authors": ["C. Dwork", "A Roth"], "venue": "Foundations and Trends in Theoretical Computer Science,", "year": 2014}, {"title": "Analyze gauss: optimal bounds for privacy-preserving principal component analysis", "authors": ["C. Dwork", "K. Talwar", "A. Thakurta", "L. Zhang"], "venue": "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,", "year": 2014}, {"title": "Rappor: Randomized aggregatable privacy-preserving ordinal response", "authors": ["\u00da. Erlingsson", "V. Pihur", "A. Korolova"], "venue": "In CCS,", "year": 2014}, {"title": "An algorithm for quadratic programming", "authors": ["M. Frank", "P. Wolfe"], "venue": "Naval research logistics quarterly,", "year": 1956}, {"title": "Eigentaste: A constant time collaborative filtering algorithm", "authors": ["K. Goldberg", "T. Roeder", "D. Gupta", "C. Perkins"], "venue": "Inf. Retr.,", "year": 2001}, {"title": "Beating randomized response on incoherent matrices", "authors": ["M. Hardt", "A. Roth"], "venue": "In STOC,", "year": 2012}, {"title": "Beyond worst-case analysis in private singular vector computation", "authors": ["M. Hardt", "A. Roth"], "venue": "In STOC,", "year": 2013}, {"title": "Fast matrix completion without the condition number", "authors": ["M. Hardt", "M. Wootters"], "venue": "In COLT,", "year": 2014}, {"title": "The movielens datasets: History and context", "authors": ["F.M. Harper", "J.A. Konstan"], "venue": "ACM Trans. Interact. Intell. Syst.,", "year": 2015}, {"title": "Revisiting frank-wolfe: Projection-free sparse convex optimization", "authors": ["M. Jaggi"], "venue": "In ICML, pp", "year": 2013}, {"title": "A simple algorithm for nuclear norm regularized problems", "authors": ["M. Jaggi", "M. Sulovsky"], "venue": "In ICML,", "year": 2010}, {"title": "Guaranteed rank minimization via singular value projection", "authors": ["P. Jain", "R. Meka", "I.S. Dhillon"], "venue": "In NIPS,", "year": 2010}, {"title": "Low-rank matrix completion using alternating minimization", "authors": ["P. Jain", "P. Netrapalli", "S. Sanghavi"], "venue": "In STOC,", "year": 2013}, {"title": "Streaming pca: Matching matrix bernstein and nearoptimal finite sample guarantees for ojas algorithm", "authors": ["P. Jain", "C. Jin", "S.M. Kakade", "P. Netrapalli", "A. Sidford"], "venue": "In Conference on Learning Theory,", "year": 2016}, {"title": "Provable efficient online matrix completion via non-convex stochastic gradient descent", "authors": ["C. Jin", "S.M. Kakade", "P. Netrapalli"], "venue": "In NIPS,", "year": 2016}, {"title": "On differentially private low rank approximation", "authors": ["M. Kapralov", "K. Talwar"], "venue": "In SODA,", "year": 2013}, {"title": "A note on differential privacy: Defining resistance to arbitrary side information", "authors": ["S.P. Kasiviswanathan", "A. Smith"], "venue": "[cs.CR],", "year": 2008}, {"title": "Mechanism design in large games: Incentives and privacy", "authors": ["M. Kearns", "M. Pai", "A. Roth", "J. Ullman"], "venue": "In ITCS,", "year": 2014}, {"title": "Matrix completion from a few entries", "authors": ["R.H. Keshavan", "A. Montanari", "S. Oh"], "venue": "IEEE Transactions on Information Theory,", "year": 2010}, {"title": "Advances in collaborative filtering", "authors": ["Y. Koren", "R.M. Bell"], "venue": "In Recommender Systems Handbook,", "year": 2015}, {"title": "Privacy violations using microtargeted ads: A case study", "authors": ["A. Korolova"], "venue": "IEEE International Conference on Data Mining Workshops", "year": 2010}, {"title": "The augmented lagrange multiplier method for exact recovery of corrupted lowrank", "authors": ["Z. Lin", "M. Chen", "Y. Ma"], "venue": "matrices. CoRR,", "year": 2010}, {"title": "Fast differentially private matrix factorization", "authors": ["Z. Liu", "Wang", "Y.-X", "A. Smola"], "venue": "In Proceedings of the 9th ACM Conference on Recommender Systems,", "year": 2015}, {"title": "Apple tries to peek at user habits without violating privacy", "authors": ["R. McMillan"], "venue": "The Wall Street Journal,", "year": 2016}, {"title": "Differentially private recommender systems: building privacy into the net", "authors": ["F. McSherry", "I. Mironov"], "venue": "In Symp. Knowledge Discovery and Datamining (KDD),", "year": 2009}, {"title": "Myths and fallacies of \u201cpersonally identifiable information", "authors": ["A. Narayanan", "V. Shmatikov"], "venue": "Commun. ACM,", "year": 2010}, {"title": "A simpler approach to matrix completion", "authors": ["B. Recht"], "venue": "Journal of Machine Learning Research,", "year": 2011}, {"title": "Largescale convex minimization with a low-rank constraint", "authors": ["S. Shalev-shwartz", "A. Gonen", "O. Shamir"], "venue": "Proceedings of the 28th International Conference on Machine Learning", "year": 2011}, {"title": "Collaborative filtering with the trace norm: Learning, bounding, and transducing", "authors": ["O. Shamir", "S. Shalev-Shwartz"], "venue": "In COLT,", "year": 2011}, {"title": "Nearly optimal private lasso", "authors": ["K. Talwar", "A. Thakurta", "L. Zhang"], "venue": "In NIPS,", "year": 2015}, {"title": "Topics in random matrix theory, volume 132", "authors": ["T. Tao"], "venue": "American Mathematical Society,", "year": 2012}, {"title": "Greedy algorithms for structurally constrained high dimensional problems", "authors": ["A. Tewari", "P.K. Ravikumar", "I.S. Dhillon"], "venue": "In NIPS,", "year": 2011}, {"title": "Privacy for free: Posterior sampling and stochastic gradient monte carlo", "authors": ["Wang", "Y.-X", "S. Fienberg", "A. Smola"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "year": 2015}, {"title": "Large-scale multi-label learning with missing labels", "authors": ["Yu", "H.-F", "P. Jain", "P. Kar", "I. Dhillon"], "venue": "In ICML,", "year": 2014}], "id": "SP:0c90acbe5114ded9fa3c5978fab5de9dc0b0e523", "authors": [{"name": "Prateek Jain", "affiliations": []}, {"name": "Om Thakkar", "affiliations": []}, {"name": "Abhradeep Thakurta", "affiliations": []}], "abstractText": "We provide the first provably joint differentially private algorithm with formal utility guarantees for the problem of user-level privacy-preserving collaborative filtering. Our algorithm is based on the Frank-Wolfe method, and it consistently estimates the underlying preference matrix as long as the number of users m is \u03c9(n), where n is the number of items, and each user provides her preference for at least \u221a n randomly selected items. Along the way, we provide an optimal differentially private algorithm for singular vector computation, based on the celebrated Oja\u2019s method, that provides significant savings in terms of space and time while operating on sparse matrices. We also empirically evaluate our algorithm on a suite of datasets, and show that it consistently outperforms the state-of-the-art private algorithms.", "title": "Differentially Private Matrix Completion Revisited"}