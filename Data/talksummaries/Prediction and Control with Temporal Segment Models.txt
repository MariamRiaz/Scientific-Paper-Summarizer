0	66	The problem of learning dynamics – where an agent learns a model of how its actions will affect its state and that of its environment – is a key open problem in robotics and reinforcement learning.
1	35	An agent equipped with a dynamics model can leverage model-predictive control or modelbased reinforcement learning (RL) to perform a wide variety of tasks, whose exact nature need not be known in advance, and without additional access to the environment.
2	13	In contrast with model-free RL, which seeks to directly learn a policy (mapping from states to actions) in order to accomplish a specific task, learning dynamics has the advantage that dynamics models can be learned without taskspecific supervision.
4	14	Additionally, learning differentiable dynamics models (such as those based on neural networks) enables the use of end-to-end backpropagationbased methods for policy and trajectory optimization that are much more efficient than model-free methods.
5	13	Typical approaches to dynamics learning build a one-step model of the dynamics, predicting the next state as a function of the current state and the current action.
6	24	However, when chained successively for many timesteps into the future, the predictions from a one-step model tend to diverge from the true dynamics, either due to the accumulation of small errors or deviation from the regime represented by the data the model was trained on.
7	29	Any learned dynamics model is only valid under the distribution of states and actions represented by its training data, and one-step models make no attempt to deal with the fact that they cannot make accurate predictions far outside this distribution.
11	35	We present a novel approach to learning dynamics based on a deep generative model over temporal segments: we wish to model the distribution over possible future state trajectories conditioned on planned future actions and a history of past states and actions.
40	13	We introduce dependency on past actions U− to support dynamics with delayed or filtered actions.
41	23	With this in mind, we propose the use of a deep conditional variational autoencoder (Kingma & Welling, 2014): our encoder will learn the distribution Q(x)(Z|X+, X−, U−, U+) over latent codes Z, and our decoder will learn to reconstruct X+ from X−, U−, U+ and a sample from Z, modeling the distribution P (x)(X+|X−, U−, U+, Z).
43	14	After training is complete, we can discard the encoder, and the decoder will allow us to predict the future state trajectory X̂+ using X−, U−, U+, as desired, sampling latent codes from an enforced prior P (Z) = N (0, I).
45	37	In the previous section we discussed a conditional variational autoencoder whose generative path serves as a stochastic dynamics model.
51	13	We then sample z ∼ N (µZ , σ2Z) in a differentiable manner using the reparametrization trick (Kingma & Welling, 2014).
64	72	, uT } from initial state x0, under dynamics model P (x).
66	33	If we attempt to solve the optimization problem as posed in (2), the solution will often attempt to apply action sequences outside the manifold where the dynamics model is valid: these actions come from a very different distribution than the action distribution of the training data.
67	17	This can be problematic: the optimization may find actions that achieve high rewards under the model (by exploiting it in a regime where it is invalid) but that do not accomplish the goal when they are executed in the real environment.
78	21	Then we can generate action sequences that are similar to the ones in our training set by sampling different latent codes z1, .
80	12	,K. The optimization problem posed in (2) can then be expressed as: max z1,...,zK E [ T∑ t=1 r(xt, ut) ] with xt ∼ P (x)(xt|x0:t−1, u1:t) ut ∼ P (u)(ut|u1:t−1, z1:K) (3) where the actions u1, .
81	103	, xT are generated by the latent action prior and dynamics model (see Figure 3 for an illustration).
104	20	(iii) How is this affected by the use of latent action priors?
157	28	For both the noisy-state and delayedaction environments, we learn a dynamics model with each method, and then use it to learn a policy for the reaching task.
159	39	Our dynamics model performs much better than the baselines, both with and without an action prior.
167	17	However, we observe that our model does learn a meaningful latent space: one that encodes uncertainty about the future.
169	14	When the dynamics are simple and deterministic (such as in the original Reacher environment), the model does express certainty by ignoring the latent code.
171	16	Interestingly, when the dynamics are deterministic but complex, the model also uses the latent codes to express uncertainty.
172	15	This can occur regarding the orientations and velocities of objects immediately following a collision, as illustrated in Figure 9.
180	24	We presented a novel approach to dynamics learning based on temporal segments, using a variational autoencoder to learn the distribution over future state trajectories conditioned on past states, past actions, and planned future actions.
185	41	However, incorporating a more sophisticated exploration strategy to gather data (in an iterative procedure, potentially using the model’s predictions to guide exploration) would allow us to tackle a more diverse set of environments, both simulated and real-world.
186	39	The action prior and segmentbased policy could be used as a starting point for hierarchical reinforcement-learning algorithms.
187	62	Leveraging existing work on few-shot learning could help finetune a dynamics model during the policy learning process.
