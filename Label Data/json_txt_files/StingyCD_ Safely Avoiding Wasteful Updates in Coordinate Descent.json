{"sections": [{"heading": "1. Introduction", "text": "Known to be simple and fast, coordinate descent is a highly popular algorithm for training machine learning models. For `1-regularized loss minimization problems, such as the Lasso (Tibshirani, 1996), CD iteratively updates just one weight variable at a time. As it turns out, these small yet inexpensive updates efficiently lead to the desired solution. Another attractive property of CD is its lack of parameters that require tuning, such as a learning rate.\nDue to its appeal, CD has been researched extensively in\n1University of Washington, Seattle, WA. Correspondence to: Tyler Johnson <tbjohns@washington.edu>, Carlos Guestrin <guestrin@cs.washington.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nrecent years. This includes theoretical (Nesterov, 2012; Shalev-Shwartz & Tewari, 2011) and more applied (Fan et al., 2008; Friedman et al., 2010) contributions. Many works also consider scaling CD using parallelism (Bradley et al., 2011; Richta\u0301rik & Taka\u0301c\u030c, 2016). For surveys of research on CD, see (Wright, 2015) or (Shi et al., 2016).\nDespite its popularity, CD has a significant drawback: in many applications, the majority of coordinate updates yield no progress toward convergence. In sparse regression, most entries of the optimal weight vector equal zero. When CD updates these weights during optimization, the weights often equal zero both before and after they are updated. This is immensely wasteful! Computing these \u201czero updates\u201d requires time yet leaves the iterate unchanged.\nIn this work, we propose StingyCD, an improved CD algorithm for sparse optimization and linear SVM problems. With minimal added overhead, StingyCD identifies many coordinate updates that are guaranteed to result in no change to the current iterate. By skipping over these zero updates, StingyCD obtains much faster convergence times.\nStingyCD is related to safe screening tests (El Ghaoui et al., 2012), which for Lasso problems, guarantee some weights equal zero at the solution. The algorithm can subsequently ignore screened weights for the remainder of optimization. Unfortunately, for screening to be effective, a good approximate solution must already be available. For this reason, screening often has little impact until convergence is near (Johnson & Guestrin, 2016).\nBy identifying zero updates rather than zero-valued weights at the solution, StingyCD drastically improves convergence times compared to safe screening. At the same time, we find that skipping only updates that are guaranteed to be zero is limiting. For this reason, we also propose StingyCD+, an algorithm that estimates a probability that each update is zero. By also skipping updates that are likely zero, StingyCD+ achieves even greater speed-ups.\nStingyCD and StingyCD+ require only simple changes to CD. Thus, we can combine these algorithms with other improvements to CD. In this work, we apply StingyCD+ to proximal Newton and working set algorithms. In both cases, incorporating StingyCD+ leads to efficiency improvements, demonstrating that \u201cstingy updates\u201d are a ver-\nAlgorithm 1 Coordinate descent for solving (P) initialize x(0) \u2190 0m; r(0) \u2190 b for t = 1, 2, . . . T do\ni\u2190 get next coordinate() \u03b4 \u2190 max { \u2212x(t\u22121)i , \u3008Ai,r(t\u22121)\u3009\u2212\u03bb \u2016Ai\u20162 } x(t) \u2190 x(t\u22121) + \u03b4ei r(t) \u2190 r(t\u22121) \u2212 \u03b4Ai\nreturn x(T )\nsatile and effective tool for scaling CD algorithms."}, {"heading": "2. StingyCD for nonnegative Lasso", "text": "We introduce StingyCD for solving the problem\nminimize x\u2208Rm\nf(x) := 12 \u2016Ax\u2212 b\u2016 2 + \u03bb \u30081,x\u3009 s.t. x \u2265 0 . (P)\n(P) is known as the \u201cnonnegative Lasso.\u201d Importantly, applications of StingyCD are not limited to (P). In \u00a74, we explain how to apply StingyCD to general Lasso and sparse logistic regression objectives as well as SVM problems.\nIn (P), A is an n\u00d7m design matrix, while b \u2208 Rn is a labels vector. Solving (P) results in a set of learned weights, which define a linear predictive model. The right term in the objective\u2014commonly written as \u03bb \u2016x\u20161 for Lasso problems without the nonnegativity constraint\u2014is a regularization term that encourages the weights to have small value. The parameter \u03bb > 0 controls the impact of the regularization term. Due to the nonnegativity constraint, a solution to (P) is sparse for sufficiently large \u03bb. That is, the majority of entries in a solution to (P) have value zero.\nAdvantages of sparsity include reduced resources needed at test time, more interpretable models, and statistical efficiency (Wainwright, 2009). In this paper, we propose an algorithm that exploits sparsity for efficient optimization."}, {"heading": "2.1. Coordinate descent", "text": "Coordinate descent (CD) is a popular algorithm for solving (P). Algorithm 1 defines a CD algorithm for this problem. During iteration t, a coordinate i \u2208 [m] is selected, usually at random or in round-robin fashion. The ith entry in x(t) is updated via x(t)i \u2190 x (t\u22121) i + \u03b4, while remaining weights do not change. The value of \u03b4 is chosen to maximally decrease the objective subject to the nonnegativity constraint. Defining the residuals vector r(t\u22121) = b \u2212 Ax(t\u22121), we can write \u03b4 as\n\u03b4 = max { \u2212x(t\u22121)i , 1\u2016Ai\u20162 (\u2329 Ai, r (t\u22121) \u232a \u2212 \u03bb )} . (1)\nIteration t requires O(NNZ (Ai)) time, where NNZ (Ai) is the number of nonzero entries in column Ai. Bottleneck operations are computing the dot product \u2329 Ai, r (t\u22121)\u232a and updating r(t). We note implementations typically compute \u2016Ai\u20162 once and then cache this value for later updates."}, {"heading": "2.2. Wasteful updates in coordinate descent", "text": "Because of the nonnegativity constraint and regularization penalty in (P), often x(t\u22121)i = 0 in Algorithm 1. In this case, if r(t\u22121) lies outside of the \u201cactive update\u201d region\nAi = {r : \u3008Ai, r\u3009 \u2212 \u03bb > 0} ,\nmeaning \u2329 Ai, r (t\u22121)\u232a\u2212\u03bb \u2264 0, then (1) implies that \u03b4 = 0. In this scenario, weight i equals zero at the beginning and end of iteration t. When solutions to (P) are sufficiently sparse, these \u201czero updates\u201d account for the majority of iterations in naive CD algorithms. Computing these updates is very wasteful! Each zero update requires O(NNZ (Ai)) time yet results in no progress toward convergence."}, {"heading": "2.3. Stingy updates", "text": "Our proposed algorithm, StingyCD, improves convergence times for CD by \u201cskipping over\u201d many zero updates. Put differently, StingyCD computes some zero updates inO(1) time rather thanO(NNZ (Ai)) time by guaranteeing \u03b4 = 0 without computing this quantity via (1).\nWe saw in \u00a72.2 that sufficient conditions for \u03b4 = 0 are (i) x(t\u22121)i = 0 and (ii) r\n(t\u22121) /\u2208 Ai. Since directly testing the second condition requires O(NNZ (Ai)) time, simply checking these conditions does not lead to a useful method for quickly guaranteeing \u03b4 = 0.\nThe insight that enables StingyCD is that we can relax the condition r(t\u22121) /\u2208 Ai to form a condition that is testable in constant time. This relaxation depends on a region S(t) for which r(t\u22121) \u2208 S(t). In particular, S(t) is a ball:\nS(t) = { r : \u2016r\u2212 rr\u20162 < q(t\u22121) } ,\nwhere q(t\u22121) = \u2225\u2225r(t\u22121) \u2212 rr\u2225\u22252 .\nAbove, rr is a \u201creference residuals\u201d vector\u2014a copy of the residuals from a previous iteration. Formally, rr = r(t\u2212k) for some k \u2265 1 (to be defined more precisely later). Note that r(t\u22121) lies on the boundary of S(t).\nAt any iteration t such that x(t\u22121)i = 0, StingyCD considers whether S(t)\u2229Ai = \u2205 before computing \u03b4. If this condition is true, it is guaranteed that \u03b4 = 0, and StingyCD continues to iteration t+1 without computing \u03b4 directly. We illustrate this concept in Figure 1. Defining gi = \u2212\u3008Ai, rr\u3009+\u03bb, we\ni = 0 and r\n(t\u22121) /\u2208 Ai, then \u03b4 = 0. In this case, computing\n\u03b4 is wasteful because the \u201cupdate\u201d makes no change to x(t\u22121). StingyCD skips over many zero updates by establishing a region S(t) for which r(t\u22121) \u2208 S(t). If S(t) \u2229 Ai = \u2205, it is guaranteed that \u03b4 = 0, and StingyCD continues to iteration t + 1 without computing \u03b4 directly. In the illustration, StingyCD successfully guarantees \u03b4 = 0, since q(t\u22121) \u2264 \u03c4i. In contrast, StingyCD would compute \u03b4 directly if q(t\u22121) > \u03c4i. We note \u221a \u03c4i is welldefined in the illustration; since rr /\u2208 Ai, we have \u03c4i \u2265 0.\ncan simplify the condition S(t) \u2229 Ai = \u2205 as follows:\nS(t) \u2229 Ai = \u2205 \u21d4 max r\u2208S(t) \u3008Ai, r\u3009 \u2212 \u03bb \u2264 0\n\u21d4 \u2212gi + \u2016Ai\u2016 \u221a q(t\u22121) \u2264 0\n\u21d4 q(t\u22121) \u2264 sign (gi) g 2 i\n\u2016Ai\u20162 := \u03c4i .\nThus, if q(t\u22121) \u2264 \u03c4i, then r(t\u22121) /\u2208 Ai (implying \u03b4 = 0 if also x(t\u22121)i = 0). We note that \u03c4i can take positive or negative value, depending on if rr \u2208 Ai. If rr /\u2208 Ai, then gi \u2265 0, which implies \u03c4i \u2265 0. However, if rr \u2208 Ai, then \u03c4i < 0, and since q(t\u22121) is nonnegative, it cannot be true that q(t\u22121) \u2264 \u03c4i\u2014StingyCD does not skip over coordinate i in this case. Thus, the magnitude of \u03c4i is not significant to StingyCD when \u03c4i < 0, though this magnitude has greater importance for StingyCD+ in \u00a73.\nImportantly, the condition q(t\u22121) \u2264 \u03c4i can be tested with minimal overhead by (i) updating rr only occasionally, (ii) precomputing \u3008Ai, rr\u3009 and \u03c4i for all i whenever rr is updated, and (iii) caching the value of q(t\u22121), which is updated appropriately after each nonzero coordinate update."}, {"heading": "2.4. StingyCD definition and guarantees", "text": "StingyCD is defined in Algorithm 2. StingyCD builds upon Algorithm 1 with three simple changes. First, during some iterations, StingyCD updates a reference residuals vector, rr \u2190 r(t\u22121). When rr is updated, StingyCD also computes a thresholds vector, \u03c4 . This requires evaluating \u3008Ai, rr\u3009 for all columns in A. While updating rr is relatively costly, more frequent updates to rr result in greater computational savings due to skipped updates.\nAlgorithm 2 StingyCD for solving (P) initialize x(0) \u2190 0m; r(0) \u2190 b; rr\u2190 r(0);\nq(0) \u2190 0; \u03c4 \u2190 compute thresholds(x(0)) for t = 1, 2, . . . T do\n# Update reference residuals on occasion: if should update reference() then\nrr\u2190 r(t\u22121) \u03c4 \u2190 compute thresholds(x(t\u22121)) q(t\u22121) \u2190 0\ni\u2190 get next coordinate()\nif q(t\u22121) \u2264 \u03c4i and x(t\u22121)i = 0 then # Skip update: x(t) \u2190 x(t\u22121); r(t) \u2190 r(t\u22121); q(t) \u2190 q(t\u22121) continue\n# Perform coordinate update: \u03b4 \u2190 max { \u2212x(t\u22121)i , \u3008Ai,r(t\u22121)\u3009\u2212\u03bb \u2016Ai\u20162 } x(t) \u2190 x(t\u22121) + \u03b4ei r(t) \u2190 r(t\u22121) \u2212 \u03b4Ai q(t) \u2190 q(t\u22121) \u2212 2\u03b4 \u2329 Ai, r (t\u22121) \u2212 rr \u232a + \u03b42 \u2016Ai\u20162\nreturn x(T )\nfunction compute thresholds(x) initialize \u03c4 \u2190 0m for i \u2208 [m] do\ngi \u2190 \u3008Ai,Ax\u2212 b\u3009+ \u03bb \u03c4i \u2190 sign (gi) g 2 i\n\u2016Ai\u20162\nreturn \u03c4\nThe second change to CD is that StingyCD tracks the quantity q(t) = \u2225\u2225r(t) \u2212 rr\u2225\u22252. After each update to rr, StingyCD sets q(t) to 0. After each nonzero residuals update, r(t) \u2190 r(t\u22121) \u2212 \u03b4Ai, StingyCD makes a corresponding update to q(t). Importantly, the quantities required for this update to q(t)\u2014\u2016Ai\u20162, \u2329 Ai, r\n(t\u22121)\u232a, \u3008Ai, rr\u3009, and \u03b4\u2014 have all been computed earlier by the algorithm. Thus, by caching these values, updating q(t) requires negligible time.\nThe final modification to CD is StingyCD\u2019s use of stingy updates. Before computing \u03b4 during each iteration t, StingyCD checks whether q(t\u22121) \u2264 \u03c4i and x(t\u22121)i = 0. If both are true, StingyCD continues to the next iteration without computing \u03b4. The threshold \u03c4i is computed after each update to rr. If rr /\u2208 Ai, the value of \u03c4i equals the squared distance between rr andAi. If rr \u2208 Ai, this quantity is the negative squared distance between rr and ACi .\nStingyCD\u2019s choice of \u03c4 ensures that each skipped update is \u201csafe.\u201d We formalize this concept with our first theorem:\nTheorem 2.1 (Safeness of StingyCD). In Algorithm 2, every skipped update would, if computed, result in \u03b4 = 0.\nThat is, if q(t\u22121) \u2264 \u03c4i and x(t\u22121)i = 0, then\nmax { \u2212x(t\u22121)i , \u2329 Ai,b\u2212Ax(t\u22121) \u232a \u2212 \u03bb\n\u2016Ai\u20162\n} = 0 .\nWe prove Theorem 2.1 in Appendix A.\nTheorem 2.1 is useful because it guarantees that although StingyCD skips many updates, CD and StingyCD have identical weight vectors for all iterations (assuming each algorithm updates coordinates in the same order). Our next theorem formalizes the notion that these skipped updates come nearly \u201cfor free.\u201d We prove this result in Appendix B.\nTheorem 2.2 (Per iteration time complexity of StingyCD). Algorithm 2 can be implemented so that iteration t requires\n\u2022 Less time than an identical iteration of Algorithm 1 if q(t\u22121) \u2264 \u03c4i and x(t\u22121)i = 0 (the update is skipped) and rr is not updated. Specifically, StingyCD requires O(1) time, while CD requires O(NNZ (Ai)) time. \u2022 The same amount of time (up to an O(1) term) as a CD iteration if the update is not skipped and rr is not updated. In particular, both algorithms require the same number of O(NNZ (Ai)) operations. \u2022 More time than a CD iteration if rr is updated. In this case, StingyCD requires O(NNZ (A)) time.\nNote StingyCD requires no more computation than CD for nearly all iterations (and often much less). However, the cost of updating rr is not negligible. To ensure updates to rr do not overly impact convergence times, we schedule reference updates so that StingyCD invests less than 20% of its time in updating rr. Specifically, StingyCD first updates rr after the second epoch and records the time that this update requires. Later on, rr is updated each time an additional 5x of this amount of time has passed."}, {"heading": "3. Skipping extra updates with StingyCD+", "text": "As we will see in \u00a76, StingyCD can significantly reduce convergence times. However, StingyCD is also limited by the requirement that only updates guaranteed to be zero are skipped. In cases where q(t\u22121) is only slightly greater than \u03c4i, intuition suggests that these updates will likely be zero too. Perhaps StingyCD should skip these updates as well.\nIn this section, we propose StingyCD+, an algorithm that also skips many updates that are not guaranteed to be zero. To do so, StingyCD+ adds two components to StingyCD: (i) a computationally inexpensive model of the probability that each update is zero, and (ii) a decision rule that applies this model to determine whether or not to skip each update."}, {"heading": "3.1. Modeling the probability of nonzero updates", "text": "During iteration t of StingyCD, suppose x(t\u22121)i = 0 but \u03c4i < q\n(t\u22121). StingyCD does not skip update t. For now, we also assume \u03c4i > \u2212q(t\u22121) (otherwise we can guarantee r(t\u22121) \u2208 Ai, which implies \u03b4 6= 0). Let U (t) be a variable that is true if \u03b4 6= 0\u2014update t is useful\u2014and false otherwise. From the algorithm\u2019s perspective, it is uncertain whether U (t) is true or false when iteration t begins. Whether or not U (t) is true depends on whether r(t\u22121) \u2208 Ai, which requires O(NNZ (Ai)) time to test. This computation will be wasteful if U (t) is false.\nStingyCD+ models the probability that U (t) is true using information available to the algorithm. Specifically, r(t\u22121) lies on the boundary of S(t), which is a ball with center rr and radius \u221a q(t\u22121). This leads to a simple assumption:\nAssumption 3.1 (Distribution of r(t\u22121)). To model the probability P (U (t)), StingyCD+ assumes r(t\u22121) is uniformly distributed on the boundary of S(t).\nBy making this assumption, P (U (t)) is tractable. In particular, we have the following equation for P (U (t)):\nTheorem 3.2 (Equation for P (U (t))). Assume x(t\u22121)i = 0 and \u03c4i \u2208 (\u2212q(t\u22121), q(t\u22121)). Then Assumption 3.1 implies\nP (U (t)) =\n{ 1 2I(1\u2212\u03c4i/q(t\u22121))( n\u22121 2 , 1 2 ) if \u03c4i \u2265 0,\n1\u2212 12I(1+\u03c4i/q(t\u22121))( n\u22121 2 , 1 2 ) otherwise,\nwhere Ix(a, b) is the regularized incomplete beta function.\nIncluded in Appendix C, Theorem 3.2\u2019s proof calculates the probability that r(t\u22121) \u2208 Ai by dividing the area of\nAi \u2229 bd(S(t)) by that of bd(S(t)) (illustrated in Figure 2). This fraction is a function of the incomplete beta function since Ai \u2229 bd(S(t)) is a hyperspherical cap (Li, 2011).\nUsing Theorem 3.2, StingyCD+ can approximately evaluate P (U (t)) efficiently using a lookup table. Specifically, for 128 values of x \u2208 (0, 1), our implementation defines an approximate lookup table for Ix(n\u221212 , 1 2 ) prior to iteration 1. Before update t, StingyCD+ computes \u03c4i/q(t\u22121) and then finds an appropriate estimate of P (U (t)) using the table. We elaborate on this procedure in Appendix D.\nSo far, P (U (t)) models the probability that \u03b4 6= 0 when \u03c4i \u2208 (\u2212q(t\u22121), q(t\u22121)) and x(t\u22121)i = 0. We can also define P (U (t)) for other x(t\u22121)i and \u03c4i. When x (t\u22121) i 6= 0, we define P (U (t)) = 1. If \u03c4i \u2265 q(t\u22121) and x(t\u22121)i = 0 (the scenario in which StingyCD skips update t), we let P (U (t)) = 0. If \u03c4i \u2264 \u2212q(t\u22121) and x(t\u22121)i = 0, we define P (U (t)) = 1 (in this final case, we can show that S(t) \u2286 Ai, which guarantees r(t\u22121) \u2208 Ai and \u03b4 6= 0)."}, {"heading": "3.2. Decision rule for skipping updates", "text": "Given P (U (t)), consider the decision of whether to skip update t. Let tlasti denote the most recent iteration during which StingyCD+ updated (did not skip) coordinate i. If this has not yet occurred, define tlasti = 0. We define the \u201cdelay\u201d D(t)i as the number of updates that StingyCD+ did not skip between iterations tlasti and t\u2212 1 inclusive.\nOur intuition for StingyCD+ is that during iteration t, if D\n(t) i is large and U (t) is true, then StingyCD+ should not skip update t. However, if D(t)i is small and U\n(t) is true, the algorithm may want to skip the update in favor of updating a coordinate with larger delay. Finally, if U (t) is false, StingyCD+ should skip the update, regardless of D(t)i .\nBased on this intuition, StingyCD+ skips update t if the \u201cexpected relevant delay,\u201d defined as E[D(t)i U (t)], is small. That is, given a threshold \u03be(t), StingyCD+ skips update t if\nP (U (t))D (t) i < \u03be (t) . (2)\nInserting (2) in place of StingyCD\u2019s condition for skipping updates is the only change from StingyCD to StingyCD+. In practice, we define \u03be(t) = NNZ ( x(t\u22121) ) . Defining \u03be(t) in this way leads to the following convergence guarantee:\nTheorem 3.3 (StingyCD+ converges to a solution of (P)). In StingyCD+, assume \u03be(t) \u2264 NNZ ( x(t\u22121) ) for all t > 0. Also, for each i \u2208 [m], assume the largest number of consecutive iterations during which get next coordinate() does not return i is bounded as t\u2192\u221e. Then\nlim t\u2192\u221e\nf(x(t)) = f(x?) .\nProven in Appendix E, Theorem 3.3 ensures StingyCD+ convergences to a solution when \u03be(t) \u2264 NNZ ( x(t\u22121) ) for all t. As long as \u03be(t) is smaller than this limit, at least one coordinate\u2014specifically a coordinate for which x (t\u22121) i 6= 0\u2014will satisfy (2) during a future iteration. By defining \u03be(t) as this limit in practice, StingyCD+ achieves fast convergence times by skipping many updates."}, {"heading": "4. Extending StingyCD to other objectives", "text": "For simplicity, we introduced StingyCD for nonnegative Lasso problems. In this section, we briefly describe how to apply StingyCD to some other objectives."}, {"heading": "4.1. General (not nonnegative) Lasso problems", "text": "It is simple to extend StingyCD to general Lasso problems:\nminimize x\u2208Rm\nfL(x) := 1 2 \u2016Ax\u2212 b\u2016 2 + \u03bb \u2016x\u20161 (PL)\n(PL) can be transformed into an instance of (P) by introducing two features (a positive and negative copy) for each column of A. That is, we can solve (PL) with design matrix A by solving (P) with design matrix [A,\u2212A]. Importantly, we perform this feature duplication implicitly in practice.\nTwo modifications to Algorithm 2 are needed to solve (PL). First, we adapt each update \u03b4 to the new objective:\n\u03b4 \u2190 argmin \u03b4 fL(x (t\u22121) + \u03b4ei) .\nSecond, we consider a positive and negative copy of Ai in the condition for skipping update t. Specifically, we define\n\u03c4+i \u2190 sign (\u03bb\u2212 \u3008Ai, rr\u3009) (\u03bb\u2212\u3008Ai,rr\u3009)2\n\u2016Ai\u20162 , and\n\u03c4\u2212i \u2190 sign (\u03bb+ \u3008Ai, rr\u3009) (\u03bb+\u3008Ai,rr\u3009)2\n\u2016Ai\u20162 .\nStingyCD skips update t if and only if x(t\u22121)i = 0 and q(t\u22121) \u2264 min{\u03c4+i , \u03c4 \u2212 i }. Modifying StingyCD+ to solve (PL) is similar. P (U (t)) becomes the sum of two probabilities corresponding to features +Ai and \u2212Ai. Specifically, P (U (t)) = P (U (t)+ ) + P (U (t) \u2212 ). We define P (U (t) + ) and P (U (t)\u2212 ) the same way as we define P (U (t)) in \u00a73.1 except we use \u03c4+i and \u03c4 \u2212 i in place of \u03c4i.\n4.2. General `1-regularized smooth loss minimization\nWe can also use StingyCD to solve problems of the form\nminimize x\u2208Rm\n\u2211n i=1 \u03c6i(\u3008ai,x\u3009) + \u03bb \u2016x\u20161 , (PL1)\nwhere each \u03c6i is smooth. To solve this problem, we redefine r(t\u22121) as a vector of derivatives:\nr(t\u22121) = [\u2212\u03c6\u20321( \u2329 a1,x (t\u22121)\u232a), . . . ,\u2212\u03c6\u2032n(\u2329an,x(t\u22121)\u232a)]T .\nWhen updating coordinate i, it remains true that \u03b4 = 0 if x (t\u22121) i = 0 and r\n(t\u22121) /\u2208 Ai\u2014the same geometry from Figure 1 applies. Unfortunately, updating q(t\u22121) no longer requires negligible computation. This is because in general, r(t) 6= r(t\u22121)\u2212\u03b4Ai. Thus, the update to q(t) in Algorithm 2 no longer applies. In other words, q(t) =\n\u2225\u2225r(t) \u2212 rr\u2225\u22252 cannot be computed from q(t\u22121) using negligible time.\nNevertheless, we can use StingyCD to efficiently solve (PL1) by incorporating StingyCD into a proximal Newton algorithm. At each outer-iteration, the loss \u2211 i \u03c6i(\u3008ai,x\u3009) is approximated by a second-order Taylor expansion. This results in a subproblem of the form (PL), which we solve using StingyCD. CD-based proximal Newton methods are known to be very fast for solving (PL1), especially in the case of sparse logistic regression (Yuan et al., 2012)."}, {"heading": "4.3. Linear support vector machines", "text": "We can also apply StingyCD to train SVMs:\nminimize x\u2208Rn\n1 2 \u2016Mx\u2016 2 \u2212 \u30081,x\u3009 s.t. x \u2208 [0, C]n .\n(PSVM)\nThis is the dual problem for training linear support vector machines. For this problem, we can apply concepts from \u00a72.3 to guarantee \u03b4 = 0 for many updates when x(t\u22121)i = 0 or x(t\u22121)i = C. To do so, the changes to StingyCD are straightforward. Due to limited space, we provide details in Appendix F."}, {"heading": "5. Related work", "text": "StingyCD is related to safe screening tests as well as alternative strategies for prioritizing coordinate updates in CD."}, {"heading": "5.1. Safe screening", "text": "Introduced in (El Ghaoui et al., 2012) for `1-regularized objectives, safe screening tests use sufficient conditions for which entries of the solution equal zero. Coordinates satisfying these conditions are discarded to simplify the problem. Follow-up works considered other problems, including sparse group Lasso (Wang & Ye, 2014), SVM training (Wang et al., 2014), and low-rank problems (Zhou & Zhao, 2015). Recent works proposed more flexible tests that avoid major issues of prior tests (Bonnefoy et al., 2014; 2015; Fercoq et al., 2015; Ndiaye et al., 2015; 2016), such as the fact that initial tests apply only prior to optimization.\nThe current state-of-the-art screening test was proposed in (Johnson & Guestrin, 2016). For the problem (P), this test relies on geometry similar to Figure 1. Specifically, the test defines a ball, SScreen, that is proven to contain the residual vector of a solution to (P). If SScreen \u2229 cl(Ai) = \u2205, it is guaranteed that the ith weight in (P)\u2019s solution has value 0.\nThe radius of SScreen is typically much larger than that of S(t) in StingyCD, however. Unlike S(t), SScreen must contain the optimal residual vector. Unless a good approximate solution is available already, SScreen is overly large, often resulting in few screened features (Johnson & Guestrin, 2016). By ensuring only that S(t) contains the current residual vector and identifying zero-valued updates rather than zero-valued entries in a solution, StingyCD improves convergence times drastically more compared to screening."}, {"heading": "5.2. Other approaches to prioritizing CD updates", "text": "Similar to StingyCD, recent work by (Fujiwara et al., 2016) also uses a reference vector concept for prioritizing updates in CD. Unlike StingyCD, this work focuses on identifying nonzero-valued coordinates, resulting in an active set algorithm. The reference vector is also a primal weight vector as opposed to a residual vector.\nSimilarly, shrinking heuristics (Fan et al., 2008; Yuan et al., 2012) and working set algorithms (Johnson & Guestrin, 2015; 2016) have been shown to be effective for prioritizing computation in CD algorithms. These algorithms solve a sequence of smaller subproblems which consider only prioritized subsets of coordinates. In these algorithms, StingyCD could be used to solve each subproblem to further prioritize computation. In \u00a76, we show that using StingyCD+ instead of CD for solving subproblems in the working set algorithm from (Johnson & Guestrin, 2015) can lead to further convergence time improvements.\nFinally, recent work has also considered adaptive sampling approaches for CD (Csiba et al., 2015). While also an interesting direction, this work does not apply to (P) due to a strong convexity requirement. Currently this approach also requires an additional pass over the data before each epoch as well as additional overhead for non-uniform sampling."}, {"heading": "6. Empirical comparisons", "text": "This section demonstrates the impact of StingyCD and StingyCD+ in practice. We first compare these algorithms to CD and safe screening for Lasso problems. Later, we show that StingyCD+ also leads to speed-ups when combined with working set and proximal Newton algorithms."}, {"heading": "6.1. Lasso problem comparisons", "text": "We implemented CD, CD with safe screening, StingyCD, and StingyCD+ to solve (PL). Coordinates are updated in round-robin fashion. We normalize columns of A and include an unregularized intercept term. We also remove features that have nonzero values in fewer than ten examples. For CD with safe screening, we apply the test from (Johnson & Guestrin, 2016), which is state-of-the-art to our knowledge. Following (Fercoq et al., 2015), screening is\nperformed after every ten epochs. Performing screening requires a full pass over the data, which is non-negligible.\nWe compare the algorithms using a financial document dataset (finance)1 and an insurance claim prediction task (allstate)2. finance contains 1.6\u00d7104 examples, 5.5\u00d7105 features, and 8.8\u00d7107 nonzero values. The result included in this section uses regularization \u03bb = 0.05\u03bbmax, where \u03bbmax is the smallest \u03bb value that results in an all-zero solution. The solution contains 1746 nonzero entries.\nThe allstate data contains 2.5 \u00d7 105 examples, 1.5 \u00d7 104 features, and 1.2 \u00d7 108 nonzero values. For this problem, we set \u03bb = 0.05\u03bbmax, resulting in 1404 selected features. We include results for additional \u03bb values in Appendix G. StingyCD seems to have slightly greater impact when \u03bb is larger, but the results generally do not change much with \u03bb.\nWe evaluate the algorithms using three metrics. The first metric is relative suboptimality, defined as\nRelative suboptimality = f(x(t))\u2212 f(x?)\nf(x?) ,\nwhere f(x(t)) is the objective value at iteration t, and x? is the problem\u2019s solution. The other metrics are support set precision and recall. Let F (t) = {i : x(t)i 6= 0}, and let F? be the analogous set for x?. We define\nPrecision = \u2223\u2223F (t) \u2229 F?\u2223\u2223\u2223\u2223F (t)\u2223\u2223 , Recall = \u2223\u2223F (t) \u2229 F?\u2223\u2223 |F?| .\n1https://www.csie.ntu.edu.tw/\u223ccjlin/libsvmtools/ datasets/regression.html#E2006-log1p\n2https://www.kaggle.com/c/allstate-claims-severity\nPrecision and recall are arguably more important than suboptimality since (PL) is typically used for feature selection.\nResults of these experiments are included in Figure 3. We see that StingyCD and StingyCD+ both greatly improve convergence times. For the reasons discussed in \u00a75, safe screening provides little improvement compared to CD in these cases\u2014even with the relative suboptimality is plotted until 10\u22129. StingyCD provides a \u201csafeness\u201d similar to safe screening yet with drastically greater impact."}, {"heading": "6.2. Combining StingyCD+ with working sets", "text": "This section demonstrates that StingyCD+ can be useful when combined with other algorithms. We consider the problem of sparse logistic regression, an instance of (PL1) in which each \u03c6i term is a logistic loss function. For each training example (ai, bi) \u2208 Rm \u00d7 [\u22121, 1], we have\n\u03c6i(\u3008ai,x\u3009) = log(1 + exp(\u2212bi \u3008ai,x\u3009)) .\nIn this section, we use StingyCD+ as a subproblem solver for a proximal Newton algorithm and a working set algorithm. Specifically, we implement StingyCD+ within the \u201cBlitz\u201d working set algorithm proposed in (Johnson & Guestrin, 2015). At each iteration of Blitz, a subproblem is formed by selecting a set of priority features. The objective is then approximately minimized by updating weights only for features in this working set. Importantly, each subproblem in Blitz is solved approximately with a proximal Newton algorithm (overviewed in \u00a74.2), and each proximal Newton subproblem is solved approximately with CD.\nFor these comparisons, we have replaced the aforemen-\ntioned CD implementation with a StingyCD+ implementation. We demonstrate the effects of this change when working sets are and are not used. In the case that working sets are omitted, we refer to the algorithm as \u201cStingyCD+ ProxNewton\u201d or \u201cCD ProxNewton,\u201d depending on whether StingyCD+ is incorporated. We note that Blitz and the proximal Newton solver have not otherwise been modified, although it is likely possible to achieve improved convergence times by accounting for the use of StingyCD+. For example, Blitz could likely be improved by including more features in each working set, since StingyCD+ provides an additional layer of update prioritization.\nThe datasets used for this comparison are an educational performance dataset (kdda)3 and a loan default prediction task (lending club)4. After removing features with fewer than ten nonzeros, kdda\u2019s design matrix contains 8.4\u00d7106 examples, 2.2 \u00d7 106 features, and 2.8 \u00d7 108 nonzero values. We solve this problem with \u03bb = 0.005\u03bbmax, which results in 692 nonzero weights at the problem\u2019s solution. The lending club data contains 1.1\u00d7 105 examples, 3.1\u00d7 104 features, and 1.0\u00d7108 nonzero values. We solve this problem with \u03bb = 0.02\u03bbmax, resulting in 878 selected features. We include plots for additional \u03bb values in Appendix H.\nResults of this experiment are shown in Figure 4. We see that replacing CD with StingyCD+ in both Blitz and ProxNewton can result in immediate efficiency improvements. We remark that the amount that StingyCD+ improved upon\n3https://www.csie.ntu.edu.tw/\u223ccjlin/libsvmtools/ datasets/binary.html#kdd2010(algebra)\n4https://www.kaggle.com/wendykan/ lending-club-loan-data\nthe working set approach depended significantly on \u03bb, at least in the case of lending club. For this dataset, when \u03bb is relatively large (and thus the solution is very sparse), we observed little or no improvement due to StingyCD+. However, for smaller values of \u03bb, StingyCD+ produced more significant gains. Moreover, StingyCD+ was the best performing algorithm in some cases (though in other cases, Blitz was much faster). This observation suggests that there likely exists a better approach to using working sets with StingyCD+\u2014an ideal algorithm would obtain excellent performance across all relevant \u03bb values."}, {"heading": "7. Discussion", "text": "We proposed StingyCD, a coordinate descent algorithm that avoids large amounts of wasteful computation in applications such as sparse regression. StingyCD borrows geometric ideas from safe screening to guarantee many updates will result in no progress toward convergence. Compared to safe screening, StingyCD achieves considerably greater convergence time speed-ups. We also introduced StingyCD+, which applies a probabilistic assumption to StingyCD in order to further prioritize coordinate updates.\nIn general, we find the idea of \u201cstingy updates\u201d to be deserving of significantly more exploration. Currently this idea is limited to CD algorithms and, for the most part, objectives with quadratic losses. However, it seems likely that similar ideas would apply in many other contexts. For example, it could be useful to use stingy updates in distributed optimization algorithms in order to significantly reduce communication requirements."}, {"heading": "Acknowledgments", "text": "We thank our anonymous reviewers for their thoughtful feedback. This work is supported in part by PECASE N00014-13-1-0023, NSF IIS-1258741, and the TerraSwarm Research Center 00008169."}], "year": 2017, "references": [{"title": "A dynamic screening principle for the lasso", "authors": ["A. Bonnefoy", "V. Emiya", "L. Ralaivola", "R. Gribonval"], "venue": "In 22nd European Signal Processing Conference,", "year": 2014}, {"title": "Dynamic screening: Accelerating first-order algorithms for the lasso and group-lasso", "authors": ["A. Bonnefoy", "V. Emiya", "L. Ralaivola", "R. Gribonval"], "venue": "IEEE Transactions on Signal Processing,", "year": 2015}, {"title": "Parallel coordinate descent for L1-regularized loss minimization", "authors": ["J.K. Bradley", "A. Kyrola", "D. Bickson", "C. Guestrin"], "venue": "In International Conference on Machine Learning,", "year": 2011}, {"title": "Stochastic dual coordinate ascent with adaptive probabilities", "authors": ["D. Csiba", "Z. Qu", "P. Richt\u00e1rik"], "venue": "In International Conference on Machine Learning,", "year": 2015}, {"title": "Safe feature elimination for the Lasso and sparse supervised learning problems", "authors": ["L. El Ghaoui", "V. Viallon", "T. Rabbani"], "venue": "Pacific Journal of Optimization,", "year": 2012}, {"title": "LIBLINEAR: A library for large linear classification", "authors": ["Fan", "R.-E", "Chang", "K.-W", "Hsieh", "C.-J", "Wang", "X.-R", "Lin"], "venue": "Journal of Machine Learning Research,", "year": 2008}, {"title": "Mind the duality gap: safer rules for the lasso", "authors": ["O. Fercoq", "A. Gramfort", "J. Salmon"], "venue": "In International Conference on Machine Learning,", "year": 2015}, {"title": "Regularization paths for generalized linear models via coordinate descent", "authors": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Journal of Statistical Software,", "year": 2010}, {"title": "Fast lasso algorithm via selective coordinate descent", "authors": ["Y. Fujiwara", "Y. Ida", "H. Shiokawa", "S. Iwamura"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,", "year": 2016}, {"title": "Blitz: a principled metaalgorithm for scaling sparse optimization", "authors": ["T.B. Johnson", "C. Guestrin"], "venue": "In International Conference on Machine Learning,", "year": 2015}, {"title": "Unified methods for exploiting piecewise linear structure in convex optimization", "authors": ["T.B. Johnson", "C. Guestrin"], "venue": "In Advances in Neural Information Processing Systems", "year": 2016}, {"title": "Concise formulas for the area and volume of a hyperspherical cap", "authors": ["S. Li"], "venue": "Asian Journal of Mathematics and Statistic,", "year": 2011}, {"title": "GAP safe screening rules for sparse multi-task and multi-class models", "authors": ["E. Ndiaye", "O. Fercoq", "A. Gramfort", "J. Salmon"], "venue": "In Advances in Neural Information Processing Systems", "year": 2015}, {"title": "Gap safe screening rules for sparse-group lasso", "authors": ["E. Ndiaye", "O. Fercoq", "A. Gramfort", "J. Salmon"], "venue": "In Advances in Neural Information Processing Systems", "year": 2016}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization problems", "authors": ["Y. Nesterov"], "venue": "SIAM Journal on Optimization,", "year": 2012}, {"title": "Parallel coordinate descent methods for big data optimization", "authors": ["P. Richt\u00e1rik", "M. Tak\u00e1\u010d"], "venue": "Mathematical Programming,", "year": 2016}, {"title": "Stochastic methods for `1-regularized loss minimization", "authors": ["S. Shalev-Shwartz", "A. Tewari"], "venue": "Journal of Machine Learning Research,", "year": 2011}, {"title": "A primer on coordinate descent algorithms", "authors": ["Shi", "H.-J. M", "S. Tu", "Y. Xu", "W. Yin"], "venue": "Technical Report", "year": 2016}, {"title": "Regression shrinkage and selection via the Lasso", "authors": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society, Series B,", "year": 1996}, {"title": "Sharp thresholds for high-dimensional and noisy sparsity recovery using `1-constrained quadratic programming (Lasso)", "authors": ["M.J. Wainwright"], "venue": "IEEE Transactions on Information Theory,", "year": 2009}, {"title": "Two-layer feature reduction for sparsegroup lasso via decomposition of convex sets", "authors": ["J. Wang", "J. Ye"], "venue": "In Advances in Neural Information Processing Systems", "year": 2014}, {"title": "Scaling SVM and least absolute deviations via exact data reduction", "authors": ["J. Wang", "P. Wonka", "J. Ye"], "venue": "In International Conference on Machine Learning,", "year": 2014}, {"title": "Coordinate descent algorithms", "authors": ["S.J. Wright"], "venue": "Mathematical Programming,", "year": 2015}, {"title": "An improved GLMNET for L1-regularized logistic regression", "authors": ["G.X. Yuan", "C.H. Ho", "C.J. Lin"], "venue": "Journal of Machine Learning Research,", "year": 2012}, {"title": "Safe subspace screening for nuclear norm regularized least squares problems", "authors": ["Q. Zhou", "Q. Zhao"], "venue": "In International Conference on Machine Learning,", "year": 2015}], "id": "SP:a21ef3d0d142de028009b822c57c38f98d3fbe33", "authors": [{"name": "Tyler B. Johnson", "affiliations": []}, {"name": "Carlos Guestrin", "affiliations": []}], "abstractText": "Coordinate descent (CD) is a scalable and simple algorithm for solving many optimization problems in machine learning. Despite this fact, CD can also be very computationally wasteful. Due to sparsity in sparse regression problems, for example, often the majority of CD updates result in no progress toward the solution. To address this inefficiency, we propose a modified CD algorithm named \u201cStingyCD.\u201d By skipping over many updates that are guaranteed to not decrease the objective value, StingyCD significantly reduces convergence times. Since StingyCD only skips updates with this guarantee, however, StingyCD does not fully exploit the problem\u2019s sparsity. For this reason, we also propose StingyCD+, an algorithm that achieves further speed-ups by skipping updates more aggressively. Since StingyCD and StingyCD+ rely on simple modifications to CD, it is also straightforward to use these algorithms with other approaches to scaling optimization. In empirical comparisons, StingyCD and StingyCD+ improve convergence times considerably for `1-regularized optimization problems.", "title": "StingyCD: Safely Avoiding Wasteful Updates in Coordinate Descent"}