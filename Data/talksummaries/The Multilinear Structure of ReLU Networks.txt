5	11	We allow the transformations defining the hidden-layers of the network to take the form of fully connected affine transformations or convolutional transformations.
7	39	We elect to use the binary hinge loss `(ŷ, y) := σ ( 1− yŷ ) (1) for binary classification, where ŷ denote the scalar output of the network and y ∈ {−1, 1} denotes the target.
8	30	Similarly, for multiclass classification we use the multiclass hinge loss, `(ŷ, r0) = ∑ r 6=r0 σ ( 1 + ŷr − ŷr0 ) (2) where ŷ = (ŷ1, .
9	34	, ŷR) ∈ RR denotes the vectorial output of the network and r0 ∈ {1, .
10	13	, R} denotes the target class.
11	65	To see the type of structure that emerges in these networks, let Ω denote the space of network parameters and let L(ω) denote the loss.
13	30	These nonlinearities encode a partition of parameter space Ω = Ω1 ∪ · · · ∪ΩM ∪N into a finite number of open cells Ωu and a closed set N of cell boundaries (c.f.
17	16	As multilinear forms are harmonic functions, an appeal to the strong maximum principle shows that non-trivial optima of the loss must happen on cell boundaries (i.e. the non-differentiable region N of the parameter space).
18	44	In other words, ReLU networks with hinge loss criteria do not have differentiable local minima, except for those trivial ones that occur in regions of parameter space where the loss surface is perfectly flat.
19	43	Figure 1b) provides a visual example of such a loss.
20	52	As a consequence the loss function has only two types of local minima.
21	77	They are • Type I (Flat): Local minima that occur in a flat (i.e. constant loss) cell or on the boundary of a flat cell.
25	8	First and foremost, Main Result 1.
26	41	L(ω) > 0 at any type II local minimum.
27	12	Importantly, if zero loss minimizers exist (which happens for most modern deep networks) then sharp local minima are always sub-optimal.
30	8	Under mild assumptions on the data we have Main Result 2.
31	8	L(ω) = 0 at any type I local minimum, while L(ω) > 0 at any type II local minimum.
32	29	Thus flat local minima are always optimal whereas sharp minima are always sub-optimal in the case where zero loss minimizers exist.
42	10	Analyzing them requires an invocation of machinery from non-smooth, non-convex analysis.
43	54	We show how to apply these techniques to study non-smooth networks in the context of binary classification.
50	10	Degeneracy in the nonlinearities (i.e. α = 0) induces sub-optimal local minima in the loss surface.
53	10	Our results for the second and third scenarios provide a mathematically precise formulation of a commonplace intuitive picture.
57	8	The loss surface of a multilinear network with the multiclass hinge loss (2) is fundamentally different than that of a binary classification problem.
60	36	Many recent works theoretically investigate the loss surface of ReLU networks.
61	53	The closest to ours is (Safran & Shamir, 2016), which uses ReLU nonlinearities to partition the parameter space into basins that, while similar in spirit, differ from our notion of cells.
62	50	Works such as (Keskar et al., 2016; Chaudhari et al., 2017) have empirically investigated the notion of “width” of a local minimizer.
64	7	Our flat and sharp local minima are reminiscent of these notions.
67	44	Additionally, (Kawaguchi, 2016) considers our first scenario with a mean squared error loss instead of the hinge loss, while (Frasconi et al., 1997) considers our second scenario with a smooth version of the hinge loss and with sigmoid nonlinearities.
70	26	We begin by describing the global structure of ReLU networks with hinge loss that arises due to their piecewise multilinear form.
71	32	Let us start by rewriting (2) as `(ŷ,y) = −1 + R∑ r=1 σ ( 1 + ŷr − 〈y, ŷ〉 ) = −1 + 〈 1 , σ ( (Id− 1⊗ y)ŷ + 1 )〉 (3) where we now view the target y ∈ {0, 1}R as a one-hot vector that encodes for the desired class.
