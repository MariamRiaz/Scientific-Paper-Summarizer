2	76	This post is underspecified and a responder (“Parker”) asks a clarifying question (a) below, but could alternatively have asked (b) or (c): (a) What version of Ubuntu do you have?
3	34	(b) What is the make of your wifi card?
4	40	(c) Are you running Ubuntu 14.10 kernel 4.4.0-59- generic on an x86 64 architecture?
5	24	Parker should not ask (b) because an answer is unlikely to be useful; they should not ask (c) because it is too specific and an answer like “No” or “I do not know” gives little help.
7	92	In this work, we design a model to rank a candidate set of clarification questions by their usefulness to the given post.
8	157	We imagine a use case (more discussion in §7) in which, while Terry is writing their post, a system suggests a shortlist of questions asking for information that it thinks people like Parker might need to provide a solution, thus enabling Terry to immediately clarify their post, potentially leading to a much quicker resolution.
10	16	In our setting, we use EVPI to calculate which questions are most likely to elicit an answer that would make the post more informative.
12	24	A novel neural-network model for address- ing the task of ranking clarification question built on the framework of expected value of perfect information (§2).
13	54	A novel dataset, derived from StackExchange2, that enables us to learn a model to ask clarifying questions by looking at the types of questions people ask (§3).
24	82	The value of this question qi is the expected utility, over all possible answers: EVPI(qi|p) = ∑ aj∈A P[aj |p, qi]U(p+ aj) (1) In Eq 1, p is the post, qi is a potential question from a set of candidate questionsQ and aj is a potential answer from a set of candidate answers A.
38	39	Using this intuition we generate question candidates for a given post by identifying posts similar to the given post and then looking at the questions asked to those posts.
43	19	Since the top-most similar candidate extracted by Lucene is always the original post itself, the original question and answer paired with the post is always one of the candidates in Q and A.
46	27	We first generate an answer representation by combining the neural representations of the post and the question using a function Fans(p̄, q̄i) (details in §2.4).
59	34	Then the second term forces the answer representation Fans(p̄i, q̄i) to be close to the answer aj corresponding to the question qj as well.
62	18	As expressed in Eq 1, this utility function measures how useful it would be if a given post p were augmented with an answer aj paired with a different question qj in the candidate set.
63	33	Although theoretically, the utility of the updated post can be calculated only using the given post (p) and the candidate answer (aj), empirically we find that our neural EVPI model performs better when the candidate question (qj) paired with the candidate answer is a part of the utility function.
85	52	We train the parameters of the three LSTMs corresponding to p, q and a, and the parameters of the two feedforward neural networks jointly to minimize the sum of the loss of our answer model (Eq 3) and our utility calculator (Eq 4) over our entire dataset:∑ i ∑ j lossans(p̄i, q̄i, āi, Qi) + lossutil(yi, p̄i, q̄j , āj) (5) Given such an estimate P[aj |p, qi] of an answer and a utility U(p + aj) of the updated post, we rank the candidate questions by their value as calculated using Eq 1.
113	36	This motivates an evaluation design that does not rely solely on the original question but also uses human judgments.
116	33	We recruit 10 such experts on Upwork11 who have prior experience in unix based operating system administration.12 We provide the annotators with a post and a randomized list of the ten question candidates obtained using Lucene (§2.1) and ask them to select a single “best” (B) question to ask, and additionally mark as “valid” (V ) other questions that they thought would be okay to ask in the context of the original post.
138	19	We train the baseline on all the positive and negative candidate triples (same as in our utility calculator (§2.3)) to minimize hinge loss on misclassification error using cross-product features between each of (p, q), (q, a) and (p, a).
142	36	Nandi et al. (2017), winners of this subtask, developed a logistic regression model using features based on string similarity, word embeddings, etc.
149	32	The major difference between the neural baselines and our EVPI model is in the loss function: the EVPI model is trained to minimize the joint loss between the answer model (defined on Fans(p, q) in Eq 3) and the utility calculator (defined on Futil(p, q, a) in Eq 4) whereas the neural baselines are trained to minimize the loss directly on F (p, q), F (p, a) or F (p, q, a).
152	19	Since the annotators had a low agreement on a single best, we evaluate against the union of the “best” annotations (B1 ∪ B2 in Table 2) and against the intersection of the “valid” annotations (V 1 ∩ V 2 in Table 2).
153	20	Among non-neural baselines, we find that the bag-of-ngrams baseline performs slightly better than random but worse than all the other models.
154	45	The Community QA baseline, on the other hand, performs better than the neural baseline (Neural (p, q)), both of which are trained without using the answers.
157	34	Both models use the same information regarding the true question and answer and are trained using the same number of model parameters.17 However, the EVPI model, unlike the neural baseline, additionally makes use of alternate question and answer candidates to compute its loss function.
165	25	We, therefore, evaluate against the “best” and the “valid” annotations on the nine other question candidates.
189	29	First, we need it to be able to generalize: for instance by constructing templates of the form “What version of are you running?” into which the system would need to fill a variable.
190	93	Second, in order to move from question ranking to question generation, one could consider sequence-to-sequence based neural network models that have recently proven to be effective for several language generation tasks (Sutskever et al., 2014; Serban et al., 2016; Yin et al., 2016).
191	150	Third is in evaluation: given that this task requires expert human annotations and also given that there are multiple possible good questions to ask, how can we automatically measure performance at this task?, a question faced in dialog and generation more broadly (Paek, 2001; Lowe et al., 2015; Liu et al., 2016).
