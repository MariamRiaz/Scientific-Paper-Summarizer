{"sections": [{"heading": "1. Introduction", "text": "We consider learning a sparse linear regressor \u03b2 minimizing the population objective:\n\u03b2 arg min \u03b2\nEX,Y D r`pY, xX,\u03b2yqs , (1)\nwhere pX, Y q P X Y Rp Y are drawn from an unknown distribution D and `p , q is a convex loss function, based on N i.i.d. samples txi, yiuNi 1 drawn from D, and when the support S : supportp\u03b2 q tj P rps | \u03b2 j 0u of \u03b2 is small, |S| \u00a4 s. In a standard single-machine setting, a common empirical approach is to minimize the `1 regularized empirical loss (see, e.g., (2) below). Here we consider a setting where data are distributed across m machines, and, for simplicity, assume1 that N nm, so that each machine j has access to n i.i.d. observations (from the same source D) txji, yjiuni 1 (equivalently, that N nm samples are randomly partitioned across machines).\nThe main contribution of the paper is a novel algorithm for estimating \u03b2 in a distributed setting. Our estimator is\n1University of Chicago, USA 2Toyota Technological Institute at Chicago, USA 3Tencent AI Lab, China. Correspondence to: Jialei Wang <jialei@uchicago.edu>, Mladen Kolar <mkolar@chicagobooth.edu>, Nathan Srebro <nati@ttic.edu>, Tong Zhang <tongzhang@tongzhang-ml.org>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\n1Results in the paper easily generalize to a setting where each machine has a different number of observations.\nable to achieve the performance of a centralized procedure that has access to all data, while keeping computation and communication costs low. Compared to the existing oneshot estimation approach (Lee et al., 2015b), our method can achieve the same statistical performance without performing the expensive debiasing step. As the number of communication rounds increases, the estimation accuracy improves until matching the performance of a centralized procedure, which happens after the logarithm of the total number of machines rounds. Furthermore, our results can be achieved under weak assumptions on the data generating procedure.\nWe assume that the communication occurs in rounds. In each round, machines exchange messages with the master machine. Between two rounds, each machine only computes based on its local information, which includes local data and previous messages (Zhang et al., 2013b; Shamir & Srebro, 2014; Arjevani & Shamir, 2015). In a nondistributed setting, efficient estimation procedures need to balance statistical efficiency with computation efficiency (runtime). In a distributed setting, the situation is more complicated and we need to balance two resources, local runtime and number of rounds of communication, with the statistical error. The local runtime refers to the amount of work each machine needs to do. The number of rounds of communication refers to how often do local machines need to exchange messages with the master machine. We compare our procedure to other algorithm using the aforementioned metrics.\nWe consider the following two baseline estimators of \u03b2 : the local estimator uses data available only on the master (first) machine and ignores data available on other machines. In particular, it computes\n\u03b2\u0302local arg min \u03b2\n1\nn\nn\u0327\ni 1\n`py1i, xx1i,\u03b2yq \u03bb||\u03b2||1 (2)\nusing locally available data. The local procedure is efficient in both communication and computation, however, the resulting estimation error is large compared to an estimator that uses all of the available data. The other idealized baseline is the centralized estimator\n\u03b2\u0302centralize arg min \u03b2\n1\nmn\nm\u0327\nj 1\nn\u0327\ni 1\n`pyji, xxji,\u03b2yq \u03bb||\u03b2||1.\nUnfortunately, due to data being huge and communication expensive, we cannot compute the centralized estimator, even though it achieves the optimal statistical error.\nIn a related setting, Lee et al. (2015b) studied a one-shot approach to learning \u03b2 , called Avg-Debias, that is based on averaging the debiased lasso estimators (Zhang & Zhang, 2013). Under strong assumptions on the data generating procedure, their approach matches the centralized error bound after one round of communication. While an encouraging result, there are limitations to this approach, that we list below.\n\u2022 The debiasing step in Avg-Debias is computationally heavy as it requires each local machine to estimate a p p matrix. For example, Javanmard (2014) (section 5.1) transforms the problem of estimating the debiasing matrix \u0398 into p generalized lasso problems. This is computationally prohibitive for high-dimensional problems (Zhang & Zhang, 2013; Javanmard & Montanari, 2014). In comparison, our procedure requires only solving one `1 penalized objective in each iteration, which has the same time complexity as computing \u03b2\u0302local in (2). See Section 2 for details.\n\u2022 Avg-Debias procedure only matches the statistical error rate of the centralized procedure when the sample size per machine satisfies n \u00c1 ms2 log p. Our approach improves this sample complexity to n \u00c1 s2 log p.\n\u2022 Avg-Debias procedure requires strong conditions on the data generating process. For example, the data matrix is required to satisfy the generalized coherence condition for debiasing to work2. As we show here, such a condition is not needed for consistent highdimensional estimation in a distributed setting. Instead, we only require standard restricted eigenvalue condition that are commonly assumed in the highdimensional estimation literature.\nOur method (EDSL) addresses the aforementioned issues 2The generalized coherence states that there exists a matrix\n\u0398, such that ||\u03a3\u0302\u0398 Ip||8 \u00c0 b\nlog p n , where \u03a3\u0302 is the empirical covariance matrix.\nof Avg-Debias. Table 1 summarizes the resources required for the approaches discussed above to solve the distributed sparse linear regression problems.\nParallel Work In parallel work (publicly announced on arXiv simultaneously with the results in this contribution), Jordan et al. (2016) present a method which is equivalent to the first iteration of our method, and thus achieves the same computational advantage over Avg-Debias as depicted in the left column of Table 1 and discussed in the first and third bullet points above. Jordan et al. extend the idea in ways different and orthogonal to this submission, by considering also low-dimensional and Bayesian inference problems. Still, for high-dimensional problems, they only consider a one-shot procedure, and so do not achieve statistical optimality in the way our method does, and do not allow using n \u00c0 ms2 log p samples per machine (see right half of Table 1). The improved one-shot approach is thus a parallel contribution, made concurrently by Jordan et al. and by us, while the multi-step approach and accompanied reduction in required number of samples (discusse in the second bullet point above) and improvement in statistical accuracy is a distinct contribution of this this submission.\nOther Related Work A large body of literature exists on distributed optimization for modern massive data sets (Dekel et al., 2012; Duchi et al., 2012; 2014; Zhang et al., 2013b; Zinkevich et al., 2010; Boyd et al., 2011; Balcan et al., 2012; Yang, 2013; Jaggi et al., 2014; Ma et al., 2015; Shamir & Srebro, 2014; Zhang & Xiao, 2015; Lee et al., 2015a; Arjevani & Shamir, 2015). A popular approach to distributed estimation is averaging estimators formed locally by different machines (Mcdonald et al., 2009; Zinkevich et al., 2010; Zhang et al., 2012; Huang & Huo, 2015). Divide-and-conquer procedures also found applications in statistical inference (Zhao et al., 2014a; Cheng & Shang, 2015; Lu et al., 2016). Shamir & Srebro (2014) and Rosenblatt & Nadler (2014) showed that averaging local estimators at the end will have bad dependence on either condition number or dimension of the problem. Yang (2013), Jaggi et al. (2014) and Smith et al. (2016) studied distributed optimization using stochastic (dual) coordinate descent, these approaches try to find a good balance between computation and communication, however, their communication com-\nplexity depends badly on the condition number. As a result, they are not better than first-order approaches, such as (proximal) accelerated gradient descent (Nesterov, 1983), in terms of communication. Shamir et al. (2014) and Zhang & Xiao (2015) proposed truly communication-efficient distributed optimization algorithms. They leveraged the local second-order information and, as a result, obtained milder dependence on the condition number compared to the firstorder approaches (Boyd et al., 2011; Shamir & Srebro, 2014; Ma et al., 2015). Lower bounds were studied in Zhang et al. (2013a), Braverman et al. (2015), and Arjevani & Shamir (2015). However, it is not clear how to extend these existing approaches to problems with non-smooth objectives, including the `1 regularized problems.\nMost of the above mentioned work is focused on estimators that are (asymptotically) linear. Averaging at the end reduces the variance of the these linear estimators, resulting in an estimator that matches the performance of a centralized procedure. Zhang et al. (2013c) studied averaging local estimators obtained by the penalized kernel ridge regression, with the `2 penalty was chosen smaller than usual to avoid the large bias problem. The situation in a high-dimensional setting is not so straightforward, since the sparsity inducing penalty introduces the bias in a nonlinear way. Zhao et al. (2014b) illustrated how averaging debiased composite quantile regression estimators can be used for efficient inference in a high-dimensional setting. Averaging debiased high-dimensional estimators was subsequently used in Lee et al. (2015b) for distributed estimation, multi-task learning (Wang et al., 2015), and statistical inference (Battey et al., 2015).\nNotation. We use rns to denote the set t1, . . . , nu. For a vector a P Rn, we let supportpaq tj : aj 0u be the support set, ||a||q , q P r1,8q, the `q-norm defined as ||a||q p \u00b0 iPrns |ai|qq1{q , and ||a||8 maxiPrns |ai|. For a matrix A P Rn1 n2 , we use the following element-wise `8 matrix norms ||A||8 maxiPrn1s,jPrn2s |aij |. Denote In as n n identity matrix. For two sequences of numbers tanu8n 1 and tbnu8n 1, we use an Opbnq to denote that an \u00a4 Cbn for some finite positive constant C, and for all n large enough. If an Opbnq and bn Opanq, we use the notation an bn. We also use an \u00c0 bn for an Opbnq and an \u00c1 bn for bn Opanq. Paper Organization. We describe our method in Section 2, and present the main results in the context of sparse linear regression in Section 3, and provide a generalized theory in Section 4. We demonstrate the effectiveness of the proposal via experiments in Section 5, and conclude the paper with discussions in Section 6. In Appendix, in Section A we illustrate some concrete examples of the general results in Section 4, and all proofs are deferred in Section B. More experimental results are presented in Section C.\nAlgorithm 1 Efficient Distributed Sparse Learning (EDSL). Input: Data txji, yjiujPrms,iPrns, loss function `p , q. Initialization: The master obtains \u03b2\u03020 by minimizing (3), and broadcast \u03b2\u03020 to every worker. for t 0, 1, . . . do\nWorkers: for j 2, 3, . . . ,m do\nif Receive \u03b2\u0302t from the master then Calculate gradient \u2207Ljp\u03b2\u0302tq and send it to the master.\nend end Master: if Receive t\u2207Ljp\u03b2\u0302tqumj 2 from all workers then\nObtain \u03b2\u0302t 1 by solving the shifted `1 regularized problem in (4). Broadcast \u03b2\u0302t 1 to every worker.\nend end"}, {"heading": "2. Methodology", "text": "In this section, we detail our procedure for estimating \u03b2 in a distributed setting. Algorithm 1 provides an outline of the steps executed by the master and worker nodes. Let\nLjp\u03b2q 1 n\nn\u0327\ni 1\n`pyji, xxji,\u03b2yq, j P rms,\nbe the empirical loss at each machine. Our method starts by solving a local `1 regularized M -estimation program. At iteration t 0, the master (first) machine obtains \u03b2\u03020 as a minimizer of the following program\nminL1p\u03b2q \u03bb0||\u03b2||1. (3)\nThe vector \u03b2\u03020 is broadcasted to all other machines, which use it to compute a gradient of the local loss at \u03b2\u03020. In particular, each worker computes\u2207Ljp\u03b2\u03020q and communicates it back to the master. This constitutes one round of communication. At the iteration t 1, the master solves the shifted `1 regularized problem\n\u03b2\u0302t 1 arg min \u03b2 L1p\u03b2q\nC 1\nm\nm\u0327\nj 1\n\u2207Ljp\u03b2\u0302tq \u2207L1p\u03b2\u0302tq,\u03b2 G\n\u03bbt 1||\u03b2||1. (4)\nA minimizer \u03b2\u0302t 1 is communicated to other machines, which use it to compute the local gradient \u2207Ljp\u03b2\u0302t 1q as before.\nFormulation (4) is inspired by the proposal in Shamir et al. (2014), where the authors studied distributed optimization\nfor smooth and strongly convex empirical objectives. Compared to Shamir et al. (2014), we do not use any averaging scheme, which would require additional rounds of communication and, moreover, we add an `1 regularization term to ensure consistent estimation in high-dimensions. Different from the distributed first-order optimization approaches, the refined objective (4) leverages both global first-order information and local higher-order information. To see this, suppose we set \u03bbt 1 0 and that Ljp\u03b2q is a quadratic objective with invertible Hessian. Then we have the following closed form solution for (4),\n\u03b2\u0302t 1 \u03b2\u0302t \u22072L1p\u03b2\u0302tq 1 m 1 \u00b8 jPrms \u2207Ljp\u03b2\u0302tq ,\nwhich is exactly a sub-sampled Newton updating rule. Unfortunately for high-dimensional problems, the Hessian is no longer invertible, and a `1 regularization is added to make the solution well behaved. The regularization parameter \u03bbt will be chosen in a way, so that it decreases with the iteration number t. As a result we will be able to show that the final estimator performs as well at the centralized solution. We discuss in details how to choose \u03bbt in the following section."}, {"heading": "3. Main Result", "text": "We illustrate our main theoretical results in the context of sparse linear regression model\nyji xxji,\u03b2 y ji, i P rns, j P rms, (5) where xji is a subgaussian p-dimensional vector of input variables and ji is i.i.d. mean zero subgaussian noise. The loss function considered is the usual the squared loss `py, y\u0302q 12 py y\u0302q2. With this notation, the centralized approach leads to the lasso estimator (Tibshirani, 1996)\n\u03b2\u0302centralize arg min \u03b2\n1\nm\nm\u0327\nj 1\nLjp\u03b2q \u03bb||\u03b2||1,\nwhere the loss at worker j is\nLjp\u03b2q 1 2n \u00b8 iPrns pyji x\u03b2,xjiyq2.\nBefore stating the main result, we provide the definition of the subgaussian norm (Vershynin, 2012).\nDefinition 1 (Subgaussian norm). The subgaussian norm ||X||\u03c82 of a subgaussian p-dimensional random vector X , is defined as\n||X||\u03c82 sup xPSp 1 sup q\u00a11 q 1{2pE|xX,xy|qq1{q,\nwhere Sp 1 is the p-dimensional unit sphere.\nWe also need an assumption on the restricted strong convexity constant (Negahban et al., 2012). Assumption 2. We assume that there exists a \u03ba \u00a1 0, such that for any \u2206 P CpS, 3q,\n1\n2n ||X1\u2206||22 \u00a5 \u03ba||\u2206||22,\nwhere\nCpS, 3q t\u2206 P Rp | ||\u2206Sc ||1 \u00a4 3||\u2206S ||1u is a restricted cone in Rp, and\nX1 rxT11; xT12; . . . ; xT1ns P Rn p\nis the data matrix on the master machine.\nWhen xji are randomly drawn from a subgaussian distribution, Assumption (2) is satisfied with high probability as long as n \u00c1 s log p (Rudelson & Zhou, 2013). We are now ready to state the estimation error bound for \u03b2\u0302t 1 obtained using Algorithm 1. Theorem 3. Assume that data are generated from a sparse linear regression model in (5) with ||xji||\u03c82 \u00a4 \u03c3X and || ji||\u03c82 \u00a4 \u03c3. Let\n\u03bbt 1 2 mn \u00b8 jPrms \u00b8 iPrns xji ji 8\n2L max j,i\n||xji||28 ||\u03b2\u0302t \u03b2 ||1\nc logp2p{\u03b4q\nn (6)\nThen for t \u00a5 0 we have, with probability at least 1 2\u03b4,\n||\u03b2\u0302t 1 \u03b2 ||1 \u00a41 a t 1 n 1 an 48s\u03c3\u03c3X \u03ba c logpp{\u03b4q mn\nat 1n s\u03c3\u03c3X \u03ba\nc logpnp{\u03b4q\nn , (7)\n||\u03b2\u0302t 1 \u03b2 ||2 \u00a41 a t 1 n 1 an 12 ? s\u03c3\u03c3X \u03ba c logpp{\u03b4q mn\natnbn s\u03c3\u03c3X \u03ba\nc logpnp{\u03b4q\nn , (8)\nwhere\nan 96s\u03c3\u03c3X \u03ba\nc logp2p{\u03b4q\nn and\nbn 24 ? s\u03c3\u03c3X \u03ba\nc logpnp{\u03b4q\nn .\nWe can simplify the bound obtained in Theorem 3 by looking at the scaling with respect to n,m, s, and p, by treating \u03ba, \u03c3 and \u03c3X as constants. Suppose n \u00c1 s2 log p and set\n\u03bbt c log p mn c log p n s c log p n t .\nThe following error bounds hold for Algorithm 1: ||\u03b2\u0302t \u03b2 ||1 \u00c0P s c log p\nmn s\nc log p\nn\nt 1 ,\n||\u03b2\u0302t \u03b2 ||2 \u00c0P c s log p mn c s log p n\ns\nc log p\nn\nt .\nWe can compare the above bounds to the performance of the local and centralized lasso (Wainwright, 2009; Meinshausen & Yu, 2009; Bickel et al., 2009). For \u03b2\u0302local, we have\n||\u03b2\u0302local \u03b2 ||1 \u00c0P s c log p\nn\nand\n||\u03b2\u0302local \u03b2 ||2 \u00c0P c s log p\nn .\nFor \u03b2\u0302centralize, we have\n||\u03b2\u0302centralize \u03b2 ||1 \u00c0P s c log p\nmn\nand\n||\u03b2\u0302centralize \u03b2 ||2 \u00c0P c s log p\nmn .\nWe see that after one round of communication, we have\n||\u03b2\u03021 \u03b2 ||1 \u00c0P s c log p\nmn s\n2 log p\nn\nand\n||\u03b2\u03021 \u03b2 ||2 \u00c0P c s log p\nmn s\n3{2 log p\nn .\nThese bounds match the results in Lee et al. (2015b) without expensive debiasing step. Furthermore, when m \u00c0\nn s2 log p , they match the performance of the centralized lasso. Finally, as long as t \u00c1 logm and n \u00c1 s2 log p, it is easy to check that s b\nlog p n\nt 1 \u00c0 s b log p mn . There-\nfore,\n||\u03b2\u0302t 1 \u03b2 ||1 \u00c0P s c log p\nmn\nand\n||\u03b2\u0302t 1 \u03b2 ||2 \u00c0P c s log p\nmn ,\nwhich matches the centralized lasso performance without additional error terms. That is, as long as n \u00c1 s2 log p, the rounds of communication to matches centralized procedure only increase logarithmically with the number of machines and independent of other parameters. Differently, for distributed learning methods studied in the literature for minimizing smooth objectives, the rounds of communication to match centralized procedure increase polynomially with m\n(see table 1 in (Zhang & Xiao, 2015)). This is because here we exploit the underlying restricted strong convexity from empirical loss functions, while prior work on distributed minimization of smooth objectives (Shamir et al., 2014; Zhang & Xiao, 2015) only consider strong convexity explicitly from regularization."}, {"heading": "4. Generalized Theory and Proof Sketch", "text": "In order to establish Theorem 3, we prove an error bound on \u03b2\u0302 \u03b2 for a general loss `p , q and \u03b2\u0302 obtained using Algorithm 1. To simplify the presentation, we assume that the domain X is bounded and that the loss function `p , q is smooth.\nAssumption 4. The loss `p , q is L-smooth with respect to the second argument:\n`1pa, bq `1pa, cq \u00a4 L|b c|, @a, b, c P R\nFurthermore, |`3pa, bq| \u00a4M for all a, b P R.\nCommonly used loss functions in statistical learning, including the squared loss for regression and logistic loss for classification, satisfy this assumption (Zhang et al., 2013b).\nNext, we state the restricted strong convexity condition for a general loss function (Negahban et al., 2012).\nAssumption 5. There exists \u03ba \u00a1 0 such that for any \u2206 P CpS, 3q\nL1p\u03b2 \u2206q L1p\u03b2 q x\u2207L1p\u03b2 q,\u2206y \u00a5 \u03ba||\u2206||22,\nwith CpS, 3q t\u2206 P Rp|||\u2206Sc ||1 \u00a4 3||\u2206S ||1u.\nThe restricted strong convexity holds with high probability for a wide range of models and designs and it is commonly assumed for showing consistent estimation in highdimensions (see, for example, van de Geer & Bu\u0308hlmann, 2009; Negahban et al., 2012; Raskutti et al., 2010; Rudelson & Zhou, 2013, for details).\nOur main theoretical result establishes a recursive estimation error bound, which relates the estimation error ||\u03b2\u0302t 1 \u03b2 || to that of the previous iteration ||\u03b2\u0302t \u03b2 ||1. Theorem 6. Suppose Assumption 4 and 5 holds. Let\n\u03bbt 1 2 1m \u00b8 jPrms \u2207Ljp\u03b2 q 8\n2L max j,i\n||xji||28 ||\u03b2 \u03b2\u0302t||1\nc logp2p{\u03b4q\nn\n2M max j,i\n||xji||38\n||\u03b2\u0302t \u03b2 ||21 .\n(9)\nThen with probability at least 1 \u03b4, we have\n||\u03b2\u0302t 1 \u03b2 ||1 \u00a448s \u03ba 1m \u00b8 jPrms \u2207Ljp\u03b2 q 8\n48sL \u03ba max j,i ||xji||28 ||\u03b2 \u03b2\u0302t||1\nc logp2p{\u03b4q\nn\n48sM \u03ba max j,i ||xji||38\n||\u03b2\u0302t \u03b2 ||21 ,\nand ||\u03b2\u0302t 1 \u03b2 ||2 \u00a412 ? s\n\u03ba\n1m \u00b8 jPrms \u2207Ljp\u03b2 q 8\n12 ? sL\n\u03ba\nmax j,i\n||xji||28 ||\u03b2 \u03b2\u0302t||1\nc logp2p{\u03b4q\nn\n4 ? sM\n\u03ba\nmax j,i\n||xji||38\n||\u03b2\u0302t \u03b2 ||21 .\nTheorem 6 upper bounds the estimation error ||\u03b2\u0302t 1 \u03b2 ||1 as a function of ||\u03b2\u0302t \u03b2 ||1. Applying Theorem 6 iteratively, we immediately obtain the following estimation error bound which depends on the quality of local `1 regularized estimation ||\u03b2\u03020 \u03b2 ||1. Corollary 7. Suppose the conditions of Theorem 6 are satisfied. Furthermore, suppose that for all t, we have\nM max j,i\n||xji||8 ||\u03b2\u0302t \u03b2 ||1 \u00a4 L\nc logp2p{\u03b4q\nn . (10)\nThen with probability at least 1 \u03b4, we have ||\u03b2\u0302t 1 \u03b2 ||1 \u00a4 at 1n ||\u03b2\u03020 \u03b2 ||1 p1 anq 1p1 at 1n q 48s\n\u03ba 1m \u00b8 jPrms \u2207Ljp\u03b2 q 8\nand\n||\u03b2\u0302t 1 \u03b2 ||2 \u00a4 atnbn ||\u03b2\u03020 \u03b2 ||1 p1 anq 1p1 at 1n q 12 ? s\n\u03ba 1m \u00b8 jPrms \u2207Ljp\u03b2 q 8 ,\nwhere\nan 96sL \u03ba max j,i\n||xji||28 c logp2p{\u03b4q n\nand\nbn 24 ? sL\n\u03ba\nmax j,i\n||xji||28 c logp2p{\u03b4q n .\nFor the quadratic loss we have that M 0 and the condition in (10) holds. For other types of losses, condition in (10) will be true for t large enough when m \u00c1 s2, leading to local exponential rate of convergence until reaching statistical optimal region."}, {"heading": "4.1. Proof Sketch of Theorem 6", "text": "We first analyze how the estimation error bound decreases after one round of communication. In particular, we bound ||\u03b2\u0302t 1 \u03b2 || with ||\u03b2\u0302t \u03b2 ||. Define\nL\u03031p\u03b2, \u03b2\u0302tq L1p\u03b2q C 1\nm \u00b8 jPrms\n\u2207Ljp\u03b2\u0302tq \u2207L1p\u03b2\u0302tq,\u03b2 G .\n(11) Then\n\u2207L\u03031p\u03b2, \u03b2\u0302tq \u2207L1p\u03b2q 1 m \u00b8 jPrms \u2207Ljp\u03b2\u0302tq \u2207L1p\u03b2\u0302tq.\nThe following lemma bounds the `8 norm of\u2207L\u03031p\u03b2, \u03b2\u0302tq. Lemma 8. With probability at least 1 \u03b4, we have \u2207L\u03031p\u03b2 , \u03b2\u0302tq\n8\n\u00a4 1m \u00b8 jPrms \u2207Ljp\u03b2 q 8\n2L max j,i\n||xji||28 ||\u03b2 \u03b2\u0302t||1\nc logp2p{\u03b4q\nn\nM max j,i\n||xji||38\n||\u03b2\u0302t \u03b2 ||21 .\nThe lemma bounds the magnitude of the gradient of the loss at optimum point \u03b2 . This will be used to guide our choice of the `1 regularization parameter \u03bbt 1 in (4). The following lemma shows that as long as \u03bbt 1 is large enough, it is guaranteed that \u03b2\u0302t 1 \u03b2 is in a restricted cone. Lemma 9. Suppose\n\u03bbt 1{2 \u00a5 \u2207L\u03031p\u03b2 , \u03b2\u0302tq\n8\n.\nThen with probability at least 1 \u03b4, we have \u03b2\u0302t 1 \u03b2 P CpS, 3q. Based on the conic condition and restricted strong convexity condition, we can obtain the recursive error bound stated in Theorem 6 following the proof strategy as in Negahban et al. (2012).\nApplications Theorem 6 can be used to establish statistical guarantees for more general sparse learning problems, for example consider the logistic regression is a popular classification model where the binary label yji P t 1, 1u is drawn according to a Bernoulli distribution:\nPpyji 1|xjiq exppyjixxji,\u03b2 yq\nexppyjixxji,\u03b2 yq 1 , (12)\nwe can establish local exponential convergence when applying Algorithm 1 to estimate \u03b2 in the high-dimensional logistic model. Section A in Appendix provide formal guarantees and more illustrative examples.\nm 5 m 10 m 20"}, {"heading": "5. Experiments", "text": "In this section we present empirical comparisons between various approaches on both simulated and real world datasets 3. We run the algorithms for both distributed regression and classification problems, and compare with the following algorithms: i) Local; ii) Centralize; iii) Distributed proximal gradient descent (Prox GD); iv) AvgDebias (Lee et al., 2015b) with hard thresholding, and v) the proposed EDSL approach."}, {"heading": "5.1. Simulations", "text": "We first examine the algorithms on simulated data. We generate txjiujPrms,iPrns from a multivariate normal distribution with mean zero and covariance matrix \u03a3. The covariance \u03a3 controls the condition number of the problem and we will varying it to see how the performance changes. We set \u03a3ij 0.5|i j| for the well-conditioned setting and \u03a3ij 0.5|i j|{5 for the ill-conditioned setting. The response variable tyjiujPrms,iPrns are drawn from (5) and (12) for regression and classification problems, respectively. For regression, the noise ji is sampled from a standard normal distribution. The true model \u03b2 is set to be s-sparse, where the first s-entries are sampled i.i.d. from a uniform distribution in r0, 1s, and the other entries are set\n3Please refer to Section C in Appendix for full experimental results and more details\nto zero.\nWe run experiments with various pn, p,m, sq settings4. The estimation error ||\u03b2\u0302t \u03b2 ||2 is shown versus rounds of communications for for Prox GD and the proposed EDSL algorithm. We also plot the estimation error of Local, AvgDebias, and Centralize as horizontal lines, since the communication cost is fixed for for these algorithms56. Figure 1 summarize the results, averaged across 10 independent trials. We have the following observations:\n\u2022 The Avg-Debias approach obtained much better estimation error compared to Local after one round of communication and sometimes performed quite close to Centralize. However, in most cases, there is still a gap compared with Centralize, especially when the problem is not well-conditioned or m is large.\n\u2022 ProxGD converges very slow when the condition number becomes bad (\u03a3ij 0.5|i j|{5 case).\n\u2022 As theory suggests, EDSL obtained a solution that is 4n: sample size per machine, p: problem dimension, m: number of machines, s: true support size. 5these algorithms have zero, one-shot and full communications, respectively. 6To give some senses about computational cost, for a problem with n 200, p 1000, at each round EDSL takes about 0.048s, while Avg-Debias takes about 40.334s.\ncompetitive with Avg-Debias after one round of communication. The estimation error decreases to match performance of Centralize within few rounds of communications; typically less than 5, even though the theory suggests EDSL will match the performance of centralize withinOplogmq rounds of communication.\nAbove experiments illustrate our theoretical results in finite samples. As suggested by theory, when sample size per machine n is relatively small, one round of communication is not sufficient to make Avg-Debias matches the performance of centralized procedure. However, EDSL could match the performance of Avg-Debias with one round of communication and further improve the estimation quality by exponentially reducing the gap between centralized procedure with Avg-Debias, until matching the centralized performance. Thus, the proposed EDSL improves the AvgDebias approach both computationally and statistically."}, {"heading": "5.2. Real-world Data Evaluation", "text": "In this section, we compare the distributed sparse learning algorithms on several real world datasets. For all data sets, we use 60% of data for training, 20% as held-out validation set for tuning the parameters, and the remaining 20% for testing. We randomly partition data 10 times and report the average performance on the test set. For regression tasks, the evaluation metric is the normalized Mean Squared Error (normalized MSE), while for classification tasks we report the miss-classification error. We randomly partition the data on m 10 machines. A subset of the results are plotted in Figure 2 where for some data sets the performance of Avg-Debias is significantly worse than others (mostly because the debiasing step fails), thus we omit these plots.\nSince there is no well-specified model on these datasets, the curves behave quite differently on different data sets. However, a large gap between the local and centralized procedure is consistent as the later uses 10 times more data. AvgDebias often fails on these real datasets and performs much worse than in the simulations, the main reason might be that\nthe assumptions, such as well-specified model or generalized coherence condition, fail, then Avg-Debias can totally fail and produce solution even much worse than the local. Nevertheless, the proposed EDSL performs quite robust on real world data sets, and can often output a solution which is highly competitive with the centralized model within a few rounds of communications. We also observed a slight \u201czig-zag\u201d behavior for EDSL approach on some data sets. For example, on the mushrooms data set, the predictive performance of EDSL is not stable. In sum, the experimental results on real world data sets verified that the proposed EDSL method is effective for distributed sparse learning problems."}, {"heading": "6. Conclusion and Discussion", "text": "We proposed a novel approach for distributed learning with sparsity, which is efficient in both computation and communication. Our theoretical analysis showed that the proposed method works under weaker conditions than AvgDebias estimator while matches its error bound with oneround communication. Furthermore, the estimation error can be improved with a logarithmic more rounds of communication until matching the centralized procedure. Experiments on both simulated and real-world data demonstrate that the proposed method significantly improves the performance over one shot averaging approaches, and matches the centralized procedure with few iterations.\nThere might be several ways to improve this work. As we see in real data experiments, the proposed approach can still perform slightly worse than the centralized approach on certain datasets. It is interesting to explore how to make EDSL provably work under even weaker assumptions. For example, EDSL requires Ops2 log pq samples per machine to match the centralized method in Oplogmq rounds of communications, however, it is not clear whether the sample size requirement can be improved, while still maintaining low-communication cost. Last but not the least, it is interesting to explore presented ideas to improve the computational cost of communication-efficient distributed multitask learning with shared support (Wang et al., 2015)."}], "year": 2017, "references": [{"title": "Communication complexity of distributed convex learning and optimization", "authors": ["Arjevani", "Yossi", "Shamir", "Ohad"], "venue": "ArXiv e-prints,", "year": 2015}, {"title": "Distributed learning, communication complexity and privacy", "authors": ["Balcan", "Maria-Florina", "Blum", "Avrim", "Fine", "Shai", "Mansour", "Yishay"], "venue": "JMLR W&CP 23: COLT 2012,", "year": 2012}, {"title": "Distributed estimation and inference with statistical guarantees", "authors": ["Battey", "Heather", "Fan", "Jianqing", "Liu", "Han", "Lu", "Junwei", "Zhu", "Ziwei"], "venue": "ArXiv e-prints,", "year": 2015}, {"title": "Simultaneous analysis of lasso and Dantzig selector", "authors": ["Bickel", "Peter J", "Ritov", "Ya\u2019acov", "Tsybakov", "Alexandre B"], "venue": "Ann. Stat.,", "year": 2009}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "authors": ["Boyd", "Stephen P", "Parikh", "Neal", "Chu", "Eric", "Peleato", "Borja", "Eckstein", "Jonathan"], "venue": "Found. Trends Mach. Learn.,", "year": 2011}, {"title": "Communication lower bounds for statistical estimation problems via a distributed data processing inequality", "authors": ["Braverman", "Mark", "Garg", "Ankit", "Ma", "Tengyu", "Nguyen", "Huy L", "Woodruff", "David P"], "venue": "ArXiv e-prints,", "year": 2015}, {"title": "Computational limits of divide-and-conquer method", "authors": ["Cheng", "Guang", "Shang", "Zuofeng"], "venue": "ArXiv e-prints,", "year": 2015}, {"title": "Optimal distributed online prediction using mini-batches", "authors": ["Dekel", "Ofer", "Gilad-Bachrach", "Ran", "Shamir", "Ohad", "Xiao", "Lin"], "venue": "J. Mach. Learn. Res.,", "year": 2012}, {"title": "Dual averaging for distributed optimization: convergence analysis and network scaling", "authors": ["Duchi", "John C", "Agarwal", "Alekh", "Wainwright", "Martin J"], "venue": "IEEE Trans. Automat. Control,", "year": 2012}, {"title": "Optimality guarantees for distributed statistical estimation", "authors": ["Duchi", "John C", "Jordan", "Michael I", "Wainwright", "Martin J", "Zhang", "Yuchen"], "venue": "ArXiv e-prints,", "year": 2014}, {"title": "Probability inequalities for sums of bounded random variables", "authors": ["Hoeffding", "Wassily"], "venue": "J. Am. Stat. Assoc., 58:13\u2013", "year": 1963}, {"title": "A distributed one-step estimator", "authors": ["Huang", "Cheng", "Huo", "Xiaoming"], "venue": "ArXiv e-prints,", "year": 2015}, {"title": "Communication-efficient distributed dual coordinate ascent", "authors": ["Jonathan", "Krishnan", "Sanjay", "Hofmann", "Thomas", "Jordan", "Michael I"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}, {"title": "Inference and estimation in highdimensional data analysis", "authors": ["Javanmard", "Adel"], "venue": "PhD dissertation, Stanford University,", "year": 2014}, {"title": "Confidence intervals and hypothesis testing for high-dimensional regression", "authors": ["Javanmard", "Adel", "Montanari", "Andrea"], "venue": "J. Mach. Learn. Res.,", "year": 2014}, {"title": "Communication-efficient distributed statistical learning", "authors": ["Jordan", "Michael I", "Lee", "Jason D", "Yang", "Yun"], "venue": "arXiv preprint arXiv:1605.07689,", "year": 2016}, {"title": "Distributed stochastic variance reduced gradient methods and a lower bound for communication complexity", "authors": ["Lee", "Jason D", "Lin", "Qihang", "Ma", "Tengyu", "Yang", "Tianbao"], "venue": "ArXiv e-prints,", "year": 2015}, {"title": "Communication-efficient sparse regression: a one-shot approach", "authors": ["Lee", "Jason D", "Sun", "Yuekai", "Liu", "Qiang", "Taylor", "Jonathan E"], "venue": "ArXiv e-prints,", "year": 2015}, {"title": "Nonparametric heterogeneity testing for massive data", "authors": ["Lu", "Junwei", "Cheng", "Guang", "Liu", "Han"], "venue": "ArXiv e-prints,", "year": 2016}, {"title": "Adding vs. averaging in distributed primal-dual optimization", "authors": ["Ma", "Chenxin", "Smith", "Virginia", "Jaggi", "Martin", "Jordan", "Michael I", "Richtrik", "Peter", "Tak"], "venue": "ArXiv e-prints,", "year": 2015}, {"title": "Generalized linear models. Monographs on Statistics and Applied Probability", "authors": ["P. McCullagh", "J.A. Nelder"], "year": 1989}, {"title": "High dimensional graphs and variable selection with the lasso", "authors": ["Meinshausen", "Nicolas", "B\u00fchlmann", "Peter"], "venue": "Ann. Stat.,", "year": 2006}, {"title": "Lasso-type recovery of sparse representations for high-dimensional data", "authors": ["Meinshausen", "Nicolas", "B. Yu"], "venue": "Ann. Stat.,", "year": 2009}, {"title": "A unified framework for highdimensional analysis of m-estimators with decomposable regularizers", "authors": ["Negahban", "Sahand N", "Ravikumar", "Pradeep", "Wainwright", "Martin J", "Yu", "Bin"], "venue": "Stat. Sci.,", "year": 2012}, {"title": "Restricted eigenvalue properties for correlated gaussian designs", "authors": ["Raskutti", "Garvesh", "Wainwright", "Martin J", "Yu", "Bin"], "venue": "The Journal of Machine Learning Research,", "year": 2010}, {"title": "High-dimensional ising model selection using `1regularized logistic regression", "authors": ["Ravikumar", "Pradeep", "Wainwright", "Martin J", "J.D. Lafferty"], "venue": "Ann. Stat.,", "year": 2010}, {"title": "On the optimality of averaging in distributed statistical learning", "authors": ["Rosenblatt", "Jonathan", "Nadler", "Boaz"], "venue": "ArXiv eprints,", "year": 2014}, {"title": "Reconstruction from anisotropic random measurements", "authors": ["Rudelson", "Mark", "Zhou", "Shuheng"], "venue": "Information Theory, IEEE Transactions on,", "year": 2013}, {"title": "Distributed stochastic optimization and learning", "authors": ["Shamir", "Ohad", "Srebro", "Nathan"], "venue": "In 52nd Annual Allerton Conference on Communication, Control, and Computing (Allerton),", "year": 2014}, {"title": "Communication efficient distributed optimization using an approximate newton-type method", "authors": ["Shamir", "Ohad", "Srebro", "Nathan", "Zhang", "Tong"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "year": 2014}, {"title": "Cocoa: A general framework for communication-efficient distributed optimization", "authors": ["Smith", "Virginia", "Forte", "Simone", "Ma", "Chenxin", "Takac", "Martin", "Jordan", "Michael I", "Jaggi"], "venue": "arXiv preprint arXiv:1611.02189,", "year": 2016}, {"title": "Regression shrinkage and selection via the lasso", "authors": ["Tibshirani", "Robert J"], "venue": "J. R. Stat. Soc. B,", "year": 1996}, {"title": "High-dimensional generalized linear models and the lasso", "authors": ["van de Geer", "Sara A"], "venue": "Ann. Stat.,", "year": 2008}, {"title": "On the conditions used to prove oracle results for the lasso", "authors": ["van de Geer", "Sara A", "B\u00fchlmann", "Peter"], "venue": "Electron. J. Stat.,", "year": 2009}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "authors": ["Vershynin", "Roman"], "year": 2012}, {"title": "Sharp thresholds for highdimensional and noisy sparsity recovery using `1constrained quadratic programming (lasso)", "authors": ["Wainwright", "Martin J"], "venue": "IEEE Trans. Inf. Theory,", "year": 2009}, {"title": "Distributed multitask learning", "authors": ["Wang", "Jialei", "Kolar", "Mladen", "Srebro", "Nathan"], "venue": "ArXiv e-prints,", "year": 2015}, {"title": "Genome-wide association analysis by lasso penalized logistic regression", "authors": ["bel", "Eric", "Lange", "Kenneth L"], "venue": "Bioinformatics, 25(6):714\u2013721,", "year": 2009}, {"title": "Model selection and estimation in the gaussian graphical model", "authors": ["M. Yuan", "Y. Lin"], "year": 2007}, {"title": "Confidence intervals for low dimensional parameters in high dimensional linear models", "authors": ["Zhang", "Cun-Hui", "Stephanie S"], "venue": "J. R. Stat. Soc. B,", "year": 2013}, {"title": "Communication-efficient distributed optimization of self-concordant empirical loss", "authors": ["Zhang", "Yuchen", "Xiao", "Lin"], "venue": "ArXiv e-prints,", "year": 2015}, {"title": "Communication-efficient algorithms for statistical optimization", "authors": ["Zhang", "Yuchen", "Wainwright", "Martin J", "Duchi", "John C"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2012}, {"title": "Information-theoretic lower bounds for distributed statistical estimation with communication constraints", "authors": ["Zhang", "Yuchen", "Duchi", "John C", "Jordan", "Michael I", "Wainwright", "Martin J"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2013}, {"title": "Communication-efficient algorithms for statistical optimization", "authors": ["Zhang", "Yuchen", "Duchi", "John C", "Wainwright", "Martin J"], "venue": "J. Mach. Learn. Res.,", "year": 2013}, {"title": "Divide and conquer kernel ridge regression: A distributed algorithm with minimax optimal rates", "authors": ["Zhang", "Yuchen", "Duchi", "John C", "Wainwright", "Martin J"], "venue": "arXiv preprint arXiv:1305.5029,", "year": 2013}, {"title": "A partially linear framework for massive heterogeneous data", "authors": ["Zhao", "Tianqi", "Cheng", "Guang", "Liu", "Han"], "venue": "ArXiv e-prints,", "year": 2014}, {"title": "A general framework for robust testing and confidence regions in high-dimensional quantile regression", "authors": ["Zhao", "Tianqi", "Kolar", "Mladen", "Liu", "Han"], "venue": "ArXiv e-prints,", "year": 2014}, {"title": "Classification of gene microarrays by penalized logistic regression", "authors": ["Zhu", "Ji", "Hastie", "Trevor J"], "venue": "Biostatistics, 5(3):427\u2013443,", "year": 2004}, {"title": "Parallelized stochastic gradient descent", "authors": ["Zinkevich", "Martin", "Weimer", "Markus", "Smola", "Alexander J", "Li", "Lihong"], "venue": "In Advances in Neural Information Processing,", "year": 2010}], "id": "SP:c42d96f414d419c64769ca607a194630a6ee7063", "authors": [{"name": "Jialei Wang", "affiliations": []}, {"name": "Mladen Kolar", "affiliations": []}, {"name": "Nathan Srebro", "affiliations": []}, {"name": "Tong Zhang", "affiliations": []}], "abstractText": "We propose a novel, efficient approach for distributed sparse learning with observations randomly partitioned across machines. In each round of the proposed method, worker machines compute the gradient of the loss on local data and the master machine solves a shifted `1 regularized loss minimization problem. After a number of communication rounds that scales only logarithmically with the number of machines, and independent of other parameters of the problem, the proposed approach provably matches the estimation error bound of centralized methods.", "title": "Efficient Distributed Learning with Sparsity"}