{"sections": [{"heading": "1. Introduction", "text": "Gaussian process (GP) (Rasmussen & Williams, 2006) is a well-known statistical learning model extensively used in various scenarios, e.g., regression, classification, optimization (Shahriari et al., 2016), visualization (Lawrence, 2005), active learning (Fu et al., 2013; Liu et al., 2017) and multi-task learning (Alvarez et al., 2012; Liu et al., 2018). Given the training setX = {xi \u2208 Rd}ni=1 and the observation set y = {y(xi) \u2208 R}ni=1, as an approximation of the underlying function \u03b7 : Rd \u2192 R, GP provides informative\n1Rolls-Royce@NTU Corporate Lab, Nanyang Technological University, Singapore 637460 2School of Computer Science and Engineering, Nanyang Technological University, Singapore 639798 3Applied Technology Group, Rolls-Royce Singapore, 6 Seletar Aerospace Rise, Singapore 797575 4Data Science and Artificial Intelligence Research Center, Nanyang Technological University, Singapore 639798. Correspondence to: Haitao Liu <htliu@ntu.edu.sg>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\npredictive distributions at test points.\nHowever, the most prominent weakness of the full GP is that it scales poorly with the training size. Given n data points, the time complexity of a standard GP paradigm scales as O(n3) in the training process due to the inversion of an n\u00d7n covariance matrix; it scales asO(n2) in the prediction process due to the matrix-vector operation. This weakness confines the full GP to training data of size O(104).\nTo cope with large-scale regression, various computationally efficient approximations have been presented. The sparse approximations reviewed in (Quin\u0303onero-Candela & Rasmussen, 2005) employ m (m n) inducing points to summarize the whole training data (Seeger et al., 2003; Snelson & Ghahramani, 2006; 2007; Titsias, 2009; Bauer et al., 2016), thus reducing the training complexity of full GP to O(nm2) and the predicting complexity to O(nm). The complexity can be further reduced through distributed inference, stochastic variational inference or Kronecker structure (Hensman et al., 2013; Gal et al., 2014; Wilson & Nickisch, 2015; Hoang et al., 2016; Peng et al., 2017). A main drawback of sparse approximations, however, is that the representational capability is limited by the number of inducing points (Moore & Russell, 2015). For example, for a quick-varying function, the sparse approximations need many inducing points to capture the local structures. That is, this kind of scheme has not reduced the scaling of the complexity (Bui & Turner, 2014).\nThe method exploited in this article belongs to the aggregation models (Hinton, 2002; Tresp, 2000; Cao & Fleet, 2014; Deisenroth & Ng, 2015; Rullie\u0300re et al., 2017), also known as consensus statistical methods (Genest & Zidek, 1986; Ranjan & Gneiting, 2010). This kind of scheme produces the final predictions by the aggregation of M submodels (GP experts) respectively trained on the subsets {Di = {Xi,yi}}Mi=1 of D = {X,y}, thus distributing the computations to \u201clocal\u201d experts. Particularly, due to the product of experts, the aggregation scheme derives a factorized marginal likelihood for efficient training; and then it combines the experts\u2019 posterior distributions according to a certain aggregation criterion. In comparison to sparse approximations, the aggregation models (i) operate directly on the full training data, (ii) require no additional\ninducing or variational parameters and (iii) distribute the computations on individual experts for straightforward parallelization (Tavassolipour et al., 2017), thus scaling them to arbitrarily large training data. In comparison to typical local GPs (Snelson & Ghahramani, 2007; Park et al., 2011), the aggregations smooth out the ugly discontinuity by the product of posterior distributions from GP experts. Note that the aggregation methods are different from the mixture-of-experts (Rasmussen & Ghahramani, 2002; Yuan & Neubauer, 2009), which suffers from intractable inference and is mainly developed for non-stationary regression.\nHowever, it has been pointed out (Rullie\u0300re et al., 2017) that there exists a particular type of training data such that typical aggregations, e.g., product-of-experts (PoE) (Hinton, 2002; Cao & Fleet, 2014) and Bayesian committee machine (BCM) (Tresp, 2000; Deisenroth & Ng, 2015), cannot offer consistent predictions, where \u201cconsistent\u201d means the aggregated predictive distribution can converge to the true underlying predictive distribution when the training size n approaches infinity.\nThe major contributions of this paper are three-fold. We first prove the inconsistency of typical aggregation models, e.g., the overconfident or conservative prediction variances illustrated in Fig. 3, using conventional disjoint or random data partition. Thereafter, we present a consistent yet efficient aggregation model for large-scale GP regression. Particularly, the proposed generalized robust Bayesian committee machine (GRBCM) selects a global subset to communicate with the remaining subsets, leading to the consistent aggregated predictive distribution derived under the Bayes rule. Finally, theoretical and empirical analyses reveal that GRBCM outperforms existing aggregations due to the consistent yet efficient predictions. We release the demo codes in https://github.com/LiuHaiTao01/GRBCM."}, {"heading": "2. Aggregation models revisited", "text": ""}, {"heading": "2.1. Factorized training", "text": "A GP usually places a probability distribution over the latent function space as f(x) \u223c GP(0, k(x,x\u2032)), which is defined by the zero mean and the covariance k(x,x\u2032). The wellknown squared exponential (SE) covariance function is\nk(x,x\u2032) = \u03c32f exp\n( \u22121\n2 d\u2211 i=1 (xi \u2212 x\u2032i)2 l2i\n) , (1)\nwhere \u03c32f is an output scale amplitude, and li is an input length-scale along the ith dimension. Given the noisy observation y(x) = f(x) + where the i.i.d. noise follows \u223c N (0, \u03c32 ) and the training dataD, we have the marginal likelihood p(y|X,\u03b8) = N (0, k(X,X) + \u03c32 I) where \u03b8 represents the hyperparameters to be inferred.\nIn order to train the GP on large-scale datasets, the aggregation models introduce a factorized training process. It first partitions the training set D into M subsets Di = {Xi,yi}, 1 \u2264 i \u2264M , and then trains GP on Di as an expertMi. In data partition, we can assign the data points randomly to the experts (random partition), or assign disjoint subsets obtained by clustering techniques to the experts (disjoint partition). Ignoring the correlation between the experts {Mi}Mi=1 leads to the factorized approximation as\np(y|X,\u03b8) \u2248 M\u220f i=1 pi(yi|Xi,\u03b8i), (2)\nwhere pi(yi|Xi,\u03b8i) \u223c N (0,Ki + \u03c32 ,iIi) with Ki = k(Xi,Xi) \u2208 Rni\u00d7ni and ni being the training size of Mi. Note that for simplicity all the M GP experts in (2) share the same hyperparameters as \u03b8i = \u03b8 (Deisenroth & Ng, 2015). The factorization (2) degenerates the full covariance matrix K = k(X,X) into a diagonal block matrix diag[K1, \u00b7 \u00b7 \u00b7 ,KM ], leading to K\u22121 \u2248 diag[K\u221211 , \u00b7 \u00b7 \u00b7 ,K \u22121 M ]. Hence, compared to the full GP, the complexity of the factorized training process is reduced to O(nm20) given ni = m0 = n/M , 1 \u2264 i \u2264M .\nConditioned on the related subset Di, the predictive distribution pi(y\u2217|Di,x\u2217) \u223c N (\u00b5i(x\u2217), \u03c32i (x\u2217)) ofMi has1\n\u00b5i(x\u2217) = k T i\u2217[Ki + \u03c3 2 I] \u22121yi, (3a) \u03c32i (x\u2217) = k(x\u2217,x\u2217)\u2212 kTi\u2217[Ki + \u03c32 I]\u22121ki\u2217 + \u03c32 , (3b)\nwhere ki\u2217 = k(Xi,x\u2217). Thereafter, the experts\u2019 predictions {\u00b5i, \u03c32i }Mi=1 are combined by the following aggregation methods to perform the final predicting."}, {"heading": "2.2. Prediction aggregation", "text": "The state-of-the-art aggregation methods include PoE (Hinton, 2002; Cao & Fleet, 2014), BCM (Tresp, 2000; Deisenroth & Ng, 2015), and nested pointwise aggregation of experts (NPAE) (Rullie\u0300re et al., 2017).\nFor the PoE and BCM family, the aggregated prediction mean and precision are generally formulated as\n\u00b5A(x\u2217) = \u03c3 2 A(x\u2217) M\u2211 i=1 \u03b2i\u03c3 \u22122 i (x\u2217)\u00b5i(x\u2217), (4a)\n\u03c3\u22122A (x\u2217) = M\u2211 i=1 \u03b2i\u03c3 \u22122 i (x\u2217) + (1\u2212 M\u2211 i=1 \u03b2i)\u03c3 \u22122 \u2217\u2217 , (4b)\nwhere the prior variance \u03c32\u2217\u2217 = k(x\u2217,x\u2217) + \u03c3 2 , which is a correction term to \u03c3\u22122A , is only available for the BCM family; and \u03b2i is the weight of the expertMi at x\u2217.\n1Instead of using pi(f\u2217|Di,x\u2217) in (Deisenroth & Ng, 2015), we here consider the aggregations in a general scenario where each expert has all its belongings at hand.\nThe predictions of the PoE family, which omit the prior precision \u03c3\u22122\u2217\u2217 in (4b), are derived from the product of M experts as\npA(y\u2217|D,x\u2217) = M\u220f i=1 p\u03b2ii (y\u2217|Di,x\u2217). (5)\nThe original PoE (Hinton, 2002) employs the constant weight \u03b2i = 1, resulting in the aggregated prediction variances that vanish with increasing M . On the contrary, the generalized PoE (GPoE) (Cao & Fleet, 2014) considers a varying \u03b2i = 0.5(log \u03c32\u2217\u2217 \u2212 log \u03c32i (x\u2217)), which represents the difference in the differential entropy between the prior p(y\u2217|x\u2217) and the posterior p(y\u2217|Di,x\u2217), to weigh the contribution ofMi at x\u2217. This varying \u03b2i brings the flexibility of increasing or reducing the importance of experts based on the predictive uncertainty. However, the varying \u03b2i may produce undesirable errors for GPoE. For instance, when x\u2217 is far away from the training data such that \u03c32i (x\u2217)\u2192 \u03c32\u2217\u2217, we have \u03b2i \u2192 0 and \u03c32GPoE \u2192\u221e.\nThe BCM family, which is opposite to the PoE family, explicitly incorporates the GP prior p(y\u2217|x\u2217) when combining predictions. For two expertsMi andMj , BCM introduces a conditional independence assumption Di \u22a5 Dj |y\u2217, leading to the aggregated predictive distribution as\npA(y\u2217|D,x\u2217) = \u220fM i=1 p \u03b2i i (y\u2217|Di,x\u2217)\np \u2211 i \u03b2i\u22121(y\u2217|x\u2217) . (6)\nThe original BCM (Tresp, 2000) employs \u03b2i = 1 but its predictions suffer from weak experts when leaving the data. Hence, inspired by GPoE, the robust BCM (RBCM) (Deisenroth & Ng, 2015) uses a varying \u03b2i to produce robust predictions by reducing the weights of weak experts. When x\u2217 is far away from the training data X , the correction term brought by the GP prior in (4b) helps the (R)BCM\u2019s prediction variance recover \u03c32\u2217\u2217. However, given M = 1, the predictions of RBCM as well as GPoE cannot recover the full GP predictions because usually \u03b21 = 0.5(log \u03c32\u2217\u2217 \u2212 log \u03c321(x\u2217)) = 0.5(log \u03c3 2 \u2217\u2217 \u2212 log \u03c32full(x\u2217)) 6= 1.\nTo achieve computation gains, the above aggregations introduce additional independence assumption for the experts\u2019 predictions, which however is often violated in practice and yields poor results. Hence, in the aggregation process, NPAE (Rullie\u0300re et al., 2017) regards the prediction mean \u00b5i(x\u2217) in (3a) as a random variable by assuming that yi has not yet been observed, thus allowing for considering the covariances between the experts\u2019 predictions. Thereafter, for the random vector [\u00b51, \u00b7 \u00b7 \u00b7 , \u00b5M , y\u2217]T, the covariances are derived as\ncov[\u00b5i, y\u2217] = k T i\u2217K \u22121 i, ki\u2217, (7a)\ncov[\u00b5i, \u00b5j ] =\n{ kTi\u2217K \u22121 i, KijK \u22121 j, kj\u2217, i 6= j,\nkTi\u2217K \u22121 i, Kij, K \u22121 j, kj\u2217, i = j,\n(7b)\nwhere Kij = k(Xi,Xj) \u2208 Rni\u00d7nj , Ki, = Ki + \u03c32 I , Kj, = Kj + \u03c3 2 I , and Kij, = Kij + \u03c3 2 I . With these covariances, a nested GP training process is performed to derive the aggregated prediction mean and variance as\n\u00b5NPAE(x\u2217) = k T A\u2217K \u22121 A \u00b5, (8a) \u03c32NPAE(x\u2217) = k(x\u2217,x\u2217)\u2212 kTA\u2217K\u22121A kA\u2217 + \u03c3 2 , (8b)\nwhere kA\u2217 \u2208 RM\u00d71 has the ith element as cov[\u00b5i, y\u2217], KA \u2208 RM\u00d7M has KijA = cov[\u00b5i, \u00b5j ], and \u00b5 = [\u00b51(x\u2217), \u00b7 \u00b7 \u00b7 , \u00b5M (x\u2217)]T. The NPAE is capable of providing consistent predictions at the cost of implementing a much more time-consuming aggregation because of the inversion ofKA at each test point."}, {"heading": "2.3. Discussions of existing aggregations", "text": "Though showcasing promising results (Deisenroth & Ng, 2015), given that n\u2192\u221e and the experts are noise-free GPs, (G)PoE and (R)BCM have been proved to be inconsistent, since there exists particular triangular array of data points that are dense in the input domain \u2126 such that the prediction variances do not go to zero (Rullie\u0300re et al., 2017).\nParticularly, we further show below the inconsistency of (G)PoE and (R)BCM using two typical data partitions (random and disjoint partition) in the scenario where the observations are blurred with noise. Note that since GPoE using a varying \u03b2i may produce undesirable errors, we adopt \u03b2i = 1/M as suggested in (Deisenroth & Ng, 2015). Now the GPoE\u2019s prediction mean is the same as that of PoE; but the prediction variance blows up as M times that of PoE.\nDefinition 1. When n \u2192 \u221e, let X \u2208 Rn\u00d7d be dense in \u2126 \u2208 [0, 1]d such that for any x \u2208 \u2126 we have limn\u2192\u221emin1\u2264i\u2264n \u2016xi \u2212 x\u2016 = 0. Besides, the underlying function to be approximated has true continuous response \u00b5\u03b7(x) and true noise variance \u03c32\u03b7 .\nFirstly, for the disjoint partition that uses clustering techniques to partition the data D into disjoint local subsets {Di}Mi=1, The proposition below reveals that when n\u2192\u221e, PoE and (R)BCM produce overconfident prediction variance that shrinks to zero; on the contrary, GPoE provides conservative prediction variance.\nProposition 1. Let {Di}Mni=1 be a disjoint partition of the training data D. Let the expertMi trained on Di be GP with zero mean and stationary covariance function k(.) > 0. We further assume that (i) limn\u2192\u221eMn = \u221e and (ii) limn\u2192\u221e n/M 2 n > 0, where the second condition implies that the subset size m0 = n/Mn and the number of experts Mn are comparable such that too weak experts are not preferred. Besides, from the second condition we have m0 \u2192n\u2192\u221e \u221e, which implies that the experts become more informative with increasing n. Then, PoE and (R)BCM\nproduce overconfident prediction variance at x\u2217 \u2208 \u2126 as\nlim n\u2192\u221e\n\u03c32A,n(x\u2217) = 0, (9)\nwhereas GPoE yields conservative prediction variance\n\u03c32\u03b7 < lim n\u2192\u221e \u03c32A,n(x\u2217) < \u03c3 2 bn(x\u2217) < \u03c3 2 \u2217\u2217, (10)\nwhere \u03c32bn(x\u2217) is offered by the farthest expertMbn (1 \u2264 bn \u2264Mn) whose prediction variance is closet to \u03c32\u2217\u2217.\nThe detailed proof is given in Appendix A. Moreover, we have the following findings. Remark 1. For the averaging \u03c3\u22122GPoE = 1 M \u2211M i=1 \u03c3 \u22122 i and\n\u00b5(G)PoE = \u2211M i=1 \u03c3\u22122i\u2211 \u03c3\u22122i\n\u00b5i using disjoint partition, more and more experts become relatively far away from x\u2217 when n\u2192\u221e, i.e., the prediction variances at x\u2217 approach \u03c32\u2217\u2217 and the prediction means approach the prior mean \u00b5\u2217\u2217. Hence, empirically, when n\u2192\u221e, the conservative \u03c32GPoE approaches \u03c32bn , and the \u00b5(G)PoE approaches \u00b5\u2217\u2217. Remark 2. The BCM\u2019s prediction variance is always larger than that of PoE since\na\u2217 = \u03c3\u22122PoE(x\u2217)\n\u03c3\u22122BCM(x\u2217) =\n\u2211M i=1 \u03c3\n\u22122 i (x\u2217)\u2211M\ni=1 \u03c3 \u22122 i (x\u2217)\u2212 (M \u2212 1)\u03c3 \u22122 \u2217\u2217\n> 1\nfor M > 1. This means \u03c32PoE deteriorates faster to zero when n\u2192\u221e. Besides, it is observed that \u00b5BCM is a\u2217 times that of PoE, which alleviates the deterioration of prediction mean when n \u2192 \u221e. However, when x\u2217 is leaving X , a\u2217 \u2192M since \u03c3\u22122i (x\u2217)\u2192 \u03c3\u22122\u2217\u2217 . That is why BCM suffers from undesirable prediction mean when leavingX .\nSecondly, for the random partition that assigns the data points randomly to the experts without replacement, The proposition below implies that when n\u2192\u221e, the prediction variances of PoE and (R)BCM will shrink to zero; the PoE\u2019s prediction mean will recover \u00b5\u03b7(x), but the (R)BCM\u2019s prediction mean cannot; interestingly, the simple GPoE can converge to the underlying true predictive distribution.\nProposition 2. Let {Di}Mni=1 be a random partition of the training data D with (i) limn\u2192\u221eMn = \u221e and (ii) limn\u2192\u221e n/M 2 n > 0. Let the experts {Mi} Mn i=1 be GPs with zero mean and stationary covariance function k(.) > 0. Then, for the aggregated predictions at x\u2217 \u2208 \u2126 we have lim n\u2192\u221e \u00b5PoE(x\u2217) = \u00b5\u03b7(x\u2217), lim n\u2192\u221e \u03c32PoE(x\u2217) = 0, lim n\u2192\u221e \u00b5GPoE(x\u2217) = \u00b5\u03b7(x\u2217), lim n\u2192\u221e \u03c32GPoE(x\u2217) = \u03c3 2 \u03b7,\nlim n\u2192\u221e \u00b5(R)BCM(x\u2217) = a\u00b5\u03b7(x\u2217), lim n\u2192\u221e\n\u03c32(R)BCM(x\u2217) = 0,\n(11)\nwhere a = \u03c3\u22122\u03b7 /(\u03c3 \u22122 \u03b7 \u2212 \u03c3\u22122\u2217\u2217 ) \u2265 1 and the equality holds when \u03c32\u03b7 = 0.\nThe detailed proof is provided in Appendix B. Propositions 1 and 2 imply that no matter what kind of data partition has been used, the prediction variances of PoE and (R)BCM will shrink to zero when n\u2192\u221e, which strictly limits their usability since no benefits can be gained from such useless uncertainty information.\nAs for data partition, intuitively, the random partition provides overlapping and coarse global information about the target function, which limits the ability to describe quickvarying characteristics. On the contrary, the disjoint partition provides separate and refined local information, which enables the model to capture the variability of target function. The superiority of disjoint partition has been empirically confirmed in (Rullie\u0300re et al., 2017). Therefore, unless otherwise indicated, we employ disjoint partition for the aggregation models throughout the article.\nAs for time complexity, the five aggregation models have the same training process, and they only differ in how to combine the experts\u2019 predictions. For (G)PoE and (R)BCM, their time complexity in prediction scales as O(nm20) + O(n\u2032nm0) where n\u2032 is the number of test points.2 For the complicated NPAE, it however needs to invert an M \u00d7M matrixKA at each test point, leading to a greatly increased time complexity in prediction as O(n\u2032n2).3\nThe inconsistency of (G)PoE and (R)BCM and the extremely time-consuming process of NPAE impose the demand of developing a consistent yet efficient aggregation model for large-scale GP regression."}, {"heading": "3. Generalized robust Bayesian committee machine", "text": ""}, {"heading": "3.1. GRBCM", "text": "Our proposed GRBCM divides M experts into two groups. The first group has a global communication expert Mc trained on the subset Dc = D1, and the second group contains the remainingM\u22121 global or local experts4 {Mi}Mi=2 trained on {Di}Mi=2, respectively. The training process of GRBCM is identical to that of typical aggregations in section 2.1. The prediction process of GRBCM, however, is different. Particularly, GRBCM assigns the global communication expert with the following properties:\n\u2022 (Random selection) The communication subset Dc is a random subset wherein the points are randomly se-\n2O(nm20) is induced by the update of M GP experts after optimizing hyperparameters.\n3The predicting complexity of NPAE can be reduced by employing various hierarchical computing structure (Rullie\u0300re et al., 2017), which however cannot provide identical predictions.\n4\u201cGlobal\u201d means the expert is trained on a random subset, whereas \u201clocal\u201d means it is trained on a disjoint subset.\nlected without replacement from D. It indicates that the points inXc spread over the entire domain, which enablesMc to capture the main features of the target function. Note that there is no limit to the partition type for the remaining M \u2212 1 subsets.\n\u2022 (Expert communication) The expertMc with predictive distribution pc(y\u2217|Dc,x\u2217) \u223c N (\u00b5c, \u03c32c ) is allowed to communicate with each of the remaining experts {Mi}Mi=2. It means we can utilize the augmented dataD+i = {Dc,Di} to improve over the base expertMc, leading to a new expertM+i with the improved predictive distribution as p+i(y\u2217|D+i,x\u2217) \u223c N (\u00b5+i, \u03c32+i) for 2 \u2264 i \u2264M .\n\u2022 (Conditional independence) Given the communication subset Dc and y\u2217, the independence assumption Di \u22a5 Dj |Dc, y\u2217 holds for 2 \u2264 i 6= j \u2264M .\nGiven the conditional independence assumption and the weights {\u03b2i}Mi=2, we approximate the exact predictive distribution p(y\u2217|D,x\u2217) using the Bayes rule as\np(y\u2217|D,x\u2217)\n\u221d p(y\u2217|x\u2217)p(Dc|y\u2217,x\u2217) M\u220f i=2 p(Di|{Dj}i\u22121j=1, y\u2217,x\u2217)\n\u2248 p(y\u2217|x\u2217)p(Dc|y\u2217,x\u2217) M\u220f i=2 p\u03b2i(Di|Dc, y\u2217,x\u2217)\n= p(y\u2217|x\u2217)\n\u220fM i=2 p\n\u03b2i(D+i|y\u2217,x\u2217) p \u2211M i=2 \u03b2i\u22121(Dc|y\u2217,x\u2217) .\n(12) Note that p(D2|Dc, y\u2217,x\u2217) is exact with no approximation in (12). Hence, we set \u03b22 = 1.\nWith (12), GRBCM\u2019s predictive distribution is\npA(y\u2217|D,x\u2217) = \u220fM i=2 p \u03b2i +i(y\u2217|D+i,x\u2217)\np \u2211M i=2 \u03b2i\u22121 c (y\u2217|Dc,x\u2217) . (13)\nwith\n\u00b5A(x\u2217) = \u03c3 2 A(x\u2217) [ M\u2211 i=2 \u03b2i\u03c3 \u22122 +i (x\u2217)\u00b5+i(x\u2217)\n\u2212 ( M\u2211 i=2 \u03b2i \u2212 1 ) \u03c3\u22122c (x\u2217)\u00b5c(x\u2217) ] , (14a)\n\u03c3\u22122A (x\u2217) = M\u2211 i=2 \u03b2i\u03c3 \u22122 +i (x\u2217)\u2212 ( M\u2211 i=2 \u03b2i \u2212 1 ) \u03c3\u22122c (x\u2217).\n(14b)\nDifferent from (R)BCM, GRBCM employs the informative \u03c3\u22122c rather than the prior \u03c3 \u22122 \u2217\u2217 to correct the prediction\nprecision in (14b), leading to consistent predictions when n \u2192 \u221e, which will be proved below. Also, the prediction mean of GRBCM in (14a) now is corrected by \u00b5c(x\u2217). Fig. 1 depicts the structure of the GRBCM aggregation model.\nIn (14a) and (14b), the parameter \u03b2i (i > 2) akin to that of RBCM is defined as the difference in the differential entropy between the base predictive distribution pc(y\u2217|Dc,x\u2217) and the enhanced predictive distribution p+i(y\u2217|D+i,x\u2217) as\n\u03b2i =\n{ 1, i = 2,\n0.5(log \u03c32c (x\u2217)\u2212 log \u03c32+i(x\u2217)), 3 \u2264 i \u2264M. (15)\nIt is found that after adding a subset Di (i \u2265 2) into the communication subset Dc, if there is little improvement of p+i(y\u2217|D+i,x\u2217) over pc(y\u2217|Dc,x\u2217), we weak the vote of M+i by assigning a small \u03b2i that approaches zero.\nAs for the size of Xc, more data points bring more informativeMc and better GRBCM predictions at the cost of higher computing complexity. In this article, we assign all the experts with the same training size as nc = ni = m0 and n+i = 2m0 for 2 \u2264 i \u2264M .\nNext, we show that the GRBCM\u2019s predictive distribution will converge to the underlying true predictive distribution when n\u2192\u221e. Proposition 3. Let {Di}Mni=1 be a partition of the training data D with (i) limn\u2192\u221eMn = \u221e and (ii) limn\u2192\u221e n/M 2 n > 0. Besides, among the M subsets, there is a global communication subset Dc, the points in which are randomly selected from D without replacement. Let the global expertMc and the enhanced experts {M+i}Mni=2 be GPs with zero mean and stationary covariance function k(.) > 0. Then, GRBCM yields consistent predictions as limn\u2192\u221e\u00b5GRBCM(x\u2217) = \u00b5\u03b7(x\u2217),lim\nn\u2192\u221e \u03c32GRBCM(x\u2217) = \u03c3 2 \u03b7.\n(16)\nThe detailed proof is provided in Appendix C. It is found in Proposition 3 that apart from the requirement that the communication subset Dc should be a random subset, the consistency of GRBCM holds for any partition of the remaining data D\\Dc. Besides, according to Propositions 2\nand 3, both GPoE and GRBCM produce consistent predictions using random partition. It is known that the GP modelM provides more confident predictions, i.e., lower uncertainty U(M) = \u222b \u03c32(x)dx, with more data points. Since GRBCM trains experts on more informative subsets {D+i}Mi=2, we have the following finding. Remark 3. When using random subsets, the GRBCM\u2019s prediction uncertainty is always lower than that of GPoE, since the discrepancy \u03b4U\u22121 = U \u22121 GRBCM \u2212 U \u22121 GPoE satisfies\n\u03b4U\u22121 = [ U\u22121(M+2)\u2212 1\nMn Mn\u2211 i=1 U\u22121(Mi)\n]\n+ \u222b Mn\u2211 i=3 \u03b2i ( \u03c3\u22122+i (x\u2217)\u2212 \u03c3 \u22122 c (x\u2217) ) dx\u2217 > 0\nfor a large enough n. It means compared to GPoE, GRBCM converges faster to the underlying function when n\u2192\u221e.\nFinally, similar to RBCM, GRBCM can be executed in multi-layer computing architectures with identical predictions (Deisenroth & Ng, 2015; Ionescu, 2015), which allow to run optimally and efficiently with the available computing infrastructure for distributed computing."}, {"heading": "3.2. Complexity", "text": "Assuming that the experts {Mi}Mi=1 have the same training size ni = m0 = n/M for 1 \u2264 i \u2264 M . Compared to (G)PoE and (R)BCM, the proposed GRBCM has a higher time complexity in prediction due to the construction of new experts {M+i}Mi=2. In prediction, it first needs to calculate the inverse of k(Xc,Xc) and M \u2212 1 augmented covariance matrices {k({Xi,Xc}, {Xi,Xc})}Mi=2, which scales as O(8nm20 \u2212 7m30), in order to obtain the predictions \u00b5c, {\u00b5+i}Mi=2 and \u03c32c , {\u03c32+i}Mi=2. Then, it combines the predictions of Mc and {M+i}Mi=2 at n\u2032 test points. Therefore, the time complexity of the GRBCM prediction process is O(\u03b1nm20) + O(\u03b2n\u2032nm0), where \u03b1 = (8M \u2212 7)/M and \u03b2 = (4M \u2212 3)/M ."}, {"heading": "4. Numerical experiments", "text": ""}, {"heading": "4.1. Toy example", "text": "We employ a 1D toy example\nf(x) = 5x2 sin(12x) + (x3 \u2212 0.5) sin(3x\u2212 0.5) + 4 cos(2x) + , (17)\nwhere \u223c N (0, 0.25), to illustrate the characteristics of existing aggregation models.\nWe generate n = 104, 5\u00d7 104, 105, 5\u00d7 105 and 106 training points, respectively, in [0, 1], and select n\u2032 = 0.1n test points randomly in [\u22120.2, 1.2]. We pre-normalize each column of X and y to zero mean and unit variance. Due to\nthe global expertMc in GRBCM, we slightly modify the disjoint partition: we first generate a random subset and then use the k-means technique to generateM\u22121 disjoint subsets. Each expert is assigned with m0 = 500 data points. We implement the aggregations by the GPML toolbox5 using the SE kernel in (1) and the conjugate gradients algorithm with the maximum number of evaluations as 500, and execute the code on a workstation with four 3.70 GHz cores and 16 GB RAM (multi-core computing in Matalb is employed). Finally, we use the Standardized Mean Square Error (SMSE) to evaluate the accuracy of prediction mean, and the Mean Standardized Log Loss (MSLL) to quantify the quality of predictive distribution (Rasmussen & Williams, 2006).\nFig. 2 depicts the comparative results of six aggregation models on the toy example. Note that NPAE using n > 5 \u00d7 104 is unavailable due to the time-consuming prediction process. Fig. 2(a) shows that these models require the same training time, but they differ in the predicting time. Due to the communication expert, the GRBCM\u2019s predicting time slightly offsets the curves of (G)PoE and (R)BCM. The NPAE however exhibits significantly larger predicting time with increasing M and n\u2032. Besides, Fig. 2(b) and (c) reveal that GRBCM and NPAE yield better predictions with increasing n, which confirm their consistency when n\u2192\u221e.6 As for NPAE, though performing slightly better than GRBCM using n = 5\u00d7 104, it requires several orders of magnitude larger predicting time, rendering it unsuitable for cases with many test points and subsets.\nFig. 3 illustrates the six aggregation models using n = 104 and n = 5\u00d7 105, respectively, in comparison to the full GP (ground truth) using n = 104.7 It is observed that in terms\n5http://www.gaussianprocess.org/gpml/ code/matlab/doc/\n6Further discussions of GRBCM is shown in Appendix D. 7The full GP is intractable using our computer for n = 5\u00d7105.\nof prediction mean, as discussed in remark 1, PoE and GPoE provide poorer results in the entire domain with increasing n. On the contrary, BCM and RBCM provide good predictions in the range [0, 1]. As discussed in remark 2, BCM however yields unreliable predictions when leaving the training data. RBCM alleviates the issue by using a varying \u03b2i. In terms of prediction variance, with increasing n, PoE and (R)BCM tend to shrink to zero (overconfident), while GPoE tends to approach \u03c32\u2217\u2217 (too conservative). Particularly, PoE always has the largest MSLL value in Fig. 2(b), since as discussed in remark 2, its prediction variance approaches zero faster."}, {"heading": "4.2. Medium-scale datasets", "text": "We use two realistic datasets, kin40k (8D, 104 training points, 3\u00d7 104 test points) (Seeger et al., 2003) and sarcos (21D, 44484 training points, 4449 test points) (Rasmussen & Williams, 2006), to assess the performance of our approach.\nThe comparison includes all the aggregations except the expensive NPAE.8 Besides, we employ the fully independent training conditional (FITC) (Snelson & Ghahramani, 2006), the GP using stochastic variational inference (SVI)9 (Hensman et al., 2013), and the subset-of-data (SOD) (Chalupka et al., 2013) for comparison. We select the inducing size m for FITC and SVI, the batch size mb for SVI, and the\n8The comparison of NPAE and GRBCM are separately provided in Appendix E.\n9https://github.com/SheffieldML/GPy\nsubset size msod for SOD, such that the computing time is similar to or a bit larger than that of GRBCM. Particularly, we choose m = 200, mb = 0.1n and msod = 2500 for kin40k, and m = 300, mb = 0.1n and msod = 3000 for sarcos. Differently, SVI employs the stochastic gradients algorithm with tsg = 1200 iterations. Finally, we adopt the disjoint partition used before to divide the kin40k dataset into 16 subsets, and the sarcos dataset into 72 subsets for the aggregations. Each experiment is repeated ten times.\nFig. 4 depicts the comparative results of different approximation models over 10 runs on the kin40k and sarcos datasets. The horizontal axis represents the sum of training and predicting time. It is first observed that GRBCM provides the best performance on the two datasets in terms of both SMSE and MSLL at the cost of requiring a bit more computing time than (G)PoE and (R)BCM. As for (R)BCM, the small SMSE values reveal that they provide better prediction mean than FITC and SOD; but the large MSLL values again confirm that they provide overconfident prediction variance. As for (G)PoE, they suffer from poor prediction mean, as indicated by the large SMSE; but GPoE performs well in terms of MSLL. Finally, the simple SOD outperforms FITC and SVI on the kin40k dataset, and performs similarly on the sarcos dataset, which are consistent with the findings in (Chalupka et al., 2013).\nNext, we explore the impact of the number M of experts on the performance of aggregations. To this end, we run them on the kin40k dataset with M respectively being 8, 16 and 64, and we run on the sarcos dataset with M respectively being 36, 72 and 288. The results in Fig. 5 turn out that all the aggregations perform worse with increasing M , since the experts become weaker; but GRBCM still yields the best performance with different M . Besides, with increasing M , the poor prediction mean and the vanishing prediction variance of PoE result in the sharp increase of MSLL values.\nFinally, we investigate the impact of data partition (disjoint or random) on the performance of aggregations. The average results in Fig. 6 turn out that the disjoint partition is more beneficial for the aggregations. The results are expectable since the disjoint subsets provide separate and refined local information, whereas the random subsets provide overlapping and coarse global information. But we observe that GPoE performs well on the sarcos dataset using random partition, which confirms the conclusions in Proposition 2. Besides, as revealed in remark 3, even using random partition, GRBCM outperforms GPoE."}, {"heading": "4.3. Large-scale datasets", "text": "This section explores the performance of aggregations and SVI on two large-scale datasets. We first assess them on the 90D song dataset, which is a subset of the million song dataset (Bertin-Mahieux et al., 2011). The song dataset is partitioned into 450000 training points and 65345 test\npoints. We then assess the models on the 11D electric dataset that is partitioned into 1.8 million training points and 249280 test points. We follow the normalization and data pre-processing in (Wilson et al., 2016) to generate the two datasets.10 For the song dataset, we use the foregoing disjoint partition to divide it into M = 720 subsets, and use m = 800, mb = 5000 and tsg = 1300 for SVI; for the electric dataset, we divide it into M = 2880 subsets, and use m = 1000, mb = 5000 and tsg = 1500 for SVI. As a result, each expert is assigned with m0 = 625 data points for the aggregations.\nTable 1 reveals that the (G)PoE\u2019s SMSE value is smaller than that of (R)BCM on the song dataset. The poor prediction mean of BCM is caused by the fact that the song dataset is highly clustered such that BCM suffers from weak experts in regions with scarce points. On the contrary, due to the almost uniform distribution of the electric data points, the (R)BCM\u2019s SMSE is much smaller than that of (G)PoE. Besides, unlike the vanishing prediction variances of PoE and (R)BCM when n \u2192 \u221e, GPoE provides conservative prediction variance, resulting in small MSLL values on the two datasets. The proposed GRBCM always outperforms the other aggregations in terms of both SMSE and MSLL on the two datasets due to the consistency. Finally, GRBCM performs similarly to SVI on the song dataset; but GRBCM outperforms SVI on the electric dataset."}, {"heading": "5. Conclusions", "text": "To scale the standard GP to large-scale regression, we present the GRBCM aggregation model, which introduces a global communication expert to yield consistent yet efficient predictions when n \u2192 \u221e. Through theoretical and empirical analyses, we demonstrated the superiority of GRBCM over existing aggregations on datasets with up to 1.8M training points.\nThe superiority of local experts is the capability of capturing local patterns. Hence, further works will consider the experts with individual hyperparameters in order to capture non-stationary and heteroscedastic features.\n10The datasets and the pre-processing scripts are available in https://people.orie.cornell.edu/andrew/."}, {"heading": "Acknowledgements", "text": "This work was conducted within the Rolls-Royce@NTU Corporate Lab with support from the National Research Foundation (NRF) Singapore under the Corp Lab@University Scheme. It is also partially supported by the Data Science and Artificial Intelligence Research Center (DSAIR) and the School of Computer Science and Engineering at Nanyang Technological University."}], "year": 2018, "references": [{"title": "Understanding probabilistic sparse Gaussian process approximations", "authors": ["Bauer", "Matthias", "van der Wilk", "Mark", "Rasmussen", "Carl Edward"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}, {"title": "The million song dataset", "authors": ["Bertin-Mahieux", "Thierry", "Ellis", "Daniel PW", "Whitman", "Brian", "Lamere", "Paul"], "venue": "In ISMIR, pp", "year": 2011}, {"title": "Tree-structured Gaussian process approximations", "authors": ["Bui", "Thang D", "Turner", "Richard E"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}, {"title": "Generalized product of experts for automatic and principled fusion of Gaussian process predictions", "authors": ["Cao", "Yanshuai", "Fleet", "David J"], "venue": "arXiv preprint arXiv:1410.7827,", "year": 2014}, {"title": "A framework for evaluating approximation methods for Gaussian process regression", "authors": ["Chalupka", "Krzysztof", "Williams", "Christopher KI", "Murray", "Iain"], "venue": "Journal of Machine Learning Research,", "year": 2013}, {"title": "Posterior consistency in nonparametric regression problems under Gaussian process priors", "authors": ["Choi", "Taeryon", "Schervish", "Mark J"], "venue": "Technical report,", "year": 2004}, {"title": "Distributed Gaussian processes", "authors": ["Deisenroth", "Marc Peter", "Ng", "Jun Wei"], "venue": "In International Conference on Machine Learning,", "year": 2015}, {"title": "A survey on instance selection for active learning", "authors": ["Fu", "Yifan", "Zhu", "Xingquan", "Li", "Bin"], "venue": "Knowledge and Information Systems,", "year": 2013}, {"title": "Combining probability distributions: A critique and an annotated bibliography", "authors": ["Genest", "Christian", "Zidek", "James V"], "venue": "Statistical Science,", "year": 1986}, {"title": "Gaussian processes for big data", "authors": ["Hensman", "James", "Fusi", "Nicol\u00f2", "Lawrence", "Neil D"], "venue": "In Proceedings of the 29th Conference on Uncertainty in Artificial Intelligence,", "year": 2013}, {"title": "Training products of experts by minimizing contrastive divergence", "authors": ["Hinton", "Geoffrey E"], "venue": "Neural Computation,", "year": 2002}, {"title": "A distributed variational inference framework for unifying parallel sparse Gaussian process regression models", "authors": ["Hoang", "Trong Nghia", "Quang Minh", "Low", "Bryan Kian Hsiang"], "venue": "In International Conference on Machine Learning,", "year": 2016}, {"title": "Revisiting large scale distributed machine learning", "authors": ["Ionescu", "Radu Cristian"], "venue": "arXiv preprint arXiv:1507.01461,", "year": 2015}, {"title": "Probabilistic non-linear principal component analysis with Gaussian process latent variable models", "authors": ["Lawrence", "Neil"], "venue": "Journal of Machine Learning Research,", "year": 2005}, {"title": "An adaptive sampling approach for Kriging metamodeling by maximizing expected prediction error", "authors": ["Liu", "Haitao", "Cai", "Jianfei", "Ong", "Yew-Soon"], "venue": "Computers & Chemical Engineering,", "year": 2017}, {"title": "Remarks on multi-output Gaussian process regression", "authors": ["Liu", "Haitao", "Cai", "Jianfei", "Ong", "Yew-Soon"], "venue": "KnowledgeBased Systems,", "year": 2018}, {"title": "Gaussian process random fields", "authors": ["Moore", "David", "Russell", "Stuart J"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Domain decomposition approach for fast Gaussian process regression of large spatial data sets", "authors": ["Park", "Chiwoo", "Huang", "Jianhua Z", "Ding", "Yu"], "venue": "Journal of Machine Learning Research,", "year": 2011}, {"title": "Asynchronous distributed variational Gaussian process for regression", "authors": ["Peng", "Hao", "Zhe", "Shandian", "Zhang", "Xiao", "Qi", "Yuan"], "venue": "In International Conference on Machine Learning,", "year": 2017}, {"title": "A unifying view of sparse approximate Gaussian process regression", "authors": ["Qui\u00f1onero-Candela", "Joaquin", "Rasmussen", "Carl Edward"], "venue": "Journal of Machine Learning Research,", "year": 1939}, {"title": "Combining probability forecasts", "authors": ["Ranjan", "Roopesh", "Gneiting", "Tilmann"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "year": 2010}, {"title": "Infinite mixtures of Gaussian process experts", "authors": ["Rasmussen", "Carl E", "Ghahramani", "Zoubin"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2002}, {"title": "Gaussian processes for machine learning", "authors": ["Rasmussen", "Carl Edward", "Williams", "Christopher K. I"], "year": 2006}, {"title": "Nested Kriging predictions for datasets with a large number of observations", "authors": ["Rulli\u00e8re", "Didier", "Durrande", "Nicolas", "Bachoc", "Fran\u00e7ois", "Chevalier", "Cl\u00e9ment"], "venue": "Statistics and Computing,", "year": 2017}, {"title": "Fast forward selection to speed up sparse Gaussian process regression", "authors": ["Seeger", "Matthias", "Williams", "Christopher", "Lawrence", "Neil"], "venue": "In Artificial Intelligence and Statistics, pp. EPFL\u2013CONF\u2013161318. PMLR,", "year": 2003}, {"title": "Taking the human out of the loop: A review of Bayesian optimization", "authors": ["Shahriari", "Bobak", "Swersky", "Kevin", "Wang", "Ziyu", "Adams", "Ryan P", "de Freitas", "Nando"], "venue": "Proceedings of the IEEE,", "year": 2016}, {"title": "Sparse Gaussian processes using pseudo-inputs", "authors": ["Snelson", "Edward", "Ghahramani", "Zoubin"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2006}, {"title": "Local and global sparse Gaussian process approximations", "authors": ["Snelson", "Edward", "Ghahramani", "Zoubin"], "venue": "In Artificial Intelligence and Statistics,", "year": 2007}, {"title": "Learning of Gaussian processes in distributed and communication limited systems", "authors": ["Tavassolipour", "Mostafa", "Motahari", "Seyed Abolfazl", "Shalmani", "Mohammad-Taghi Manzuri"], "venue": "arXiv preprint arXiv:1705.02627,", "year": 2017}, {"title": "Variational learning of inducing variables in sparse Gaussian processes", "authors": ["Titsias", "Michalis K"], "venue": "In Artificial Intelligence and Statistics,", "year": 2009}, {"title": "A Bayesian committee machine", "authors": ["Tresp", "Volker"], "venue": "Neural Computation,", "year": 2000}, {"title": "Pointwise consistency of the Kriging predictor with known mean and covariance functions", "authors": ["Vazquez", "Emmanuel", "Bect", "Julien"], "venue": "In 9th International Workshop in Model-Oriented Design and Analysis,", "year": 2010}, {"title": "Kernel interpolation for scalable structured Gaussian processes (KISS-GP)", "authors": ["Wilson", "Andrew", "Nickisch", "Hannes"], "venue": "In International Conference on Machine Learning,", "year": 2015}, {"title": "Deep kernel learning", "authors": ["Wilson", "Andrew Gordon", "Hu", "Zhiting", "Salakhutdinov", "Ruslan", "Xing", "Eric P"], "venue": "In Artificial Intelligence and Statistics,", "year": 2016}, {"title": "Variational mixture of Gaussian process experts", "authors": ["Yuan", "Chao", "Neubauer", "Claus"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2009}], "id": "SP:64241b5fe2342bc13496a2b9c9140b398ff0da0d", "authors": [{"name": "Haitao Liu", "affiliations": []}, {"name": "Jianfei Cai", "affiliations": []}, {"name": "Yi Wang", "affiliations": []}, {"name": "Yew-Soon Ong", "affiliations": []}], "abstractText": "In order to scale standard Gaussian process (GP) regression to large-scale datasets, aggregation models employ factorized training process and then combine predictions from distributed experts. The state-of-the-art aggregation models, however, either provide inconsistent predictions or require time-consuming aggregation process. We first prove the inconsistency of typical aggregations using disjoint or random data partition, and then present a consistent yet efficient aggregation model for large-scale GP. The proposed model inherits the advantages of aggregations, e.g., closed-form inference and aggregation, parallelization and distributed computing. Furthermore, theoretical and empirical analyses reveal that the new aggregation model performs better due to the consistent predictions that converge to the true underlying function when the training size approaches infinity.", "title": "Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression"}