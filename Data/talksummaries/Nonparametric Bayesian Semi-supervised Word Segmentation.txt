0	103	For any unsegmented language, especially East Asian languages such as Chinese, Japanese and Thai, word segmentation is almost an inevitable first step in natural language processing.
2	36	Texts in such media are often written in a colloquial style that contains many new words and expressions that are not present in any existing dictionaries.
4	21	For this purpose, ordinary supervised learning is clearly unsatisfactory; even hand-crafted dictionar- ies will not suffice because functional expressions more complex than simple nouns need to be recognized through their relationship with other words in text, which also might be unknown in advance.
5	52	Previous studies of this issue used character and word information in the framework of supervised learning (Kruengkrai et al., 2009; Sun et al., 2009; Sun and Xu, 2011).
30	13	If h is already empty at the unigram level, NPYLM employs a back-off distribution using character n-grams for p(w|h′): p0(w) = p(c1 · · · ck) (2) = ∏k i=1 p(ci|c1 · · · ci−1) .
37	55	First, since it optimizes the performance of the language model, its segmentation does not always conform to human standards and depends on subtle modeling decisions.
38	31	For example, NPYLM often separates inflectional suffixes in Japanese like “ Here, the parameter Λ = (λ0, λ1, · · · , λK) includes the interpolation weight λ0 and F (y,x)=(log pGEN(y,x), f1(y,x), · · · , fK(y,x)).
39	30	JESS-CM interleaves the optimization of Λ andΘ to maximize the objective function p(Yl,Xu|Xl; Λ,Θ) = p(Yl|Xl; Λ) · p(Xu; Θ) (8) where 〈Xl, Yl〉 is the labeled dataset and Xu is the unlabeled dataset.
41	27	Since CRF and HMM have the same Markov model structure, they interpolate two weights ∑K k=1 λkfk(yt, yt−1,x) and (9) λ0 log pGEN(yt|yt−1,x) (10) on the corresponding path, altenately • fixing Θ and optimizing Λ of CRF on 〈Xl, Yl〉, and • fixing Λ and optimizing Θ of HMM on Xu until convergence, and thereby iteratively maximizing the two terms in (8).
43	24	Note that the interpolation weight λ0 is automatically computed through this process.
45	18	Note that Suzuki and Isozaki (2008) implicitly assumed that the discriminative and generative models have the same structure as shown in Figure 4.
46	20	Since NPYLM is a semi-Markov model as described in Section 2, a naı̈ve approach would be to combine it with a semiMarkov CRF (Sarawagi and Cohen, 2005) as the discriminative model.
48	22	In fact, our preliminary supervised word segmentation experiments showed a F1 measure of around 95%, whereas a ⇐⇒ Using this notation, the potential that corresponds to α[t][k] is γ[t− k+1, t + 1) covering ct−k+1 · · · ct, and thus the forward recursion of the inside probability α[t][k] that incorporates the information from the CRF can be written as follows, instead of (4): α[t][k] = L∑ j=1 exp [ λ0 log p(c t t−k+1|ct−kt−k−j+1) + γ[t−k+1, t+1) ] · α[t−k][j].
51	115	On the other hand, translating the information from the semi-Markov to Markov model, i.e., translating a potential from the word-based language model into the character-wise discriminative classifier, is not trivial.
60	38	Since we do not know the endpoint of this word, we can obtain the probability p(zt = 1, zt+1=0) by marginalizing over the endpoint j (· · · means values all 0): where p(ct+j−1t |s) is obtained from (15).
66	30	Finally, we obtain the inference algorithm for NPYCRF as a variant of the MCMC-EM algorithm (Wei and Tanner, 1990) shown in Figure 9.2 In learning of a NPYLM, we add the CRF potentials as described in Section 4.1, and sample a possible segmentation from the posterior through Forward filteringBackward sampling to update the model parameters.
75	14	(pure CRF) 3: for j = 1 · · ·M do 4: for i = randperm(1 · · ·N) do 5: if j > 1 then 6: Remove customers ofX(i)u from NPYLM Θ 7: end if 8: Draw segmentations ofX(i)u from NPYCRF 9: Add customers ofX(i)u to NPYLM Θ 10: end for 11: Optimize Λ of NPYCRF on 〈Yl, Xl〉.
77	49	X(i)u denotes the i-th sentence in the unlabeled data Xu.
78	24	We can also iterate steps 4 to 10 several times untilΘ approximately converges, before updating Λ. we parallelized the NPYLM sampling over several processors and because of the possible correlation of segmentations within the samples, used the Metropolis-Hastings algorithm to correct them.
81	76	We conducted experiments on several corpora of unsegmented languages: Japanese, Chinese, and Thai.
83	74	Chinese For Chinese, we first used a standard dataset from the SIGHAN Bakeoff 2005 (Emerson, 2005) for the labeled and test data, and Chinese gigaword version 2 (LDC2009T14) for the unlabeled data.
90	38	For the unlabeled data, we used a random crawl of 600K Japanese sentences collected from Twitter in MarchApril, 2014.
96	69	We used normal priors of truncated N(1, σ2) and N(0, σ2) for λ0 and λ1 · · ·λK , respectively, and fixed the CRF regularization parameter C to 1.0, and σ to 1.0 by preliminary experiments on the same data.
98	36	In addition to those templates, we used character type bigrams, where the ‘character type’ was defined by Unicode blocks (like Hiragana or CJK Unified Ideographs for Chinese and Japanese) or Unicode character categories (Thai).
100	123	Therefore, the upper limit of L in (11) and (13) was Lt for each position t, obtained 东软集团 19 游景玉 17 任尧森 17 南昆铁路 16 东方红三号”卫星 13 刘积仁 13 ｉｎｔｅｒｎｅｔ 11 东宝 11 张肇群 10 彭云 10 玲英 10 抚州 10 亚仿 10 南丁格尔 9 中远香港集团 7 海尔-波普彗星 7 第九届全国人民代表大会 6 巨型机 6 CRF NPYLM NPYCRF Gold it is necessary to change λ0 depending on the input string in a log-linear framework.7 While this might be achieved through Density Ratio estimation framework (Sugiyama et al., 2012; Tsuboi et al., 2009), we believe it is a general problem of semisupervised learning and is beyond the scope of this paper.
101	13	This issue also affects the estimation of λ0 as a scalar: that is, we found that λ0 often fluctuates during training because Λ (which includes λ0) is estimated using only limited 〈Xl, Yl〉.
102	14	In practice, we terminated the EM algorithm in Figure 9 early after a few iterations.
104	77	In this paper, we presented a hybrid generative/discriminative model of word segmentation, leveraging a nonparametric Bayesian model for unsupervised segmentation.
105	40	By combining CRF and NPYLM within the semi-supervised framework of JESS-CM, our NPYCRF not only works as well as the state-of-the-art neural segmentation without hand tuning of hyperparameters on standard corpora, but also appropriately segments non-standard texts found in Twitter and Weibo, for example, by automatically finding “new words” thanks to a nonparametric model of infinite vocabulary.
106	62	We believe that our model lays the foundation for developing a methodology of combining nonparametric Bayesian models and discriminative classifiers, as well as providing an example of semisupervised learning on different model structures, i.e. Markov and semi-Markov models for word segmentation.
