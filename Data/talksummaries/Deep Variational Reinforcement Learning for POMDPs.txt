19	38	Generalisation is also often easier over beliefs than over trajectories since distinct histories can lead to similar or identical beliefs.
22	17	To this end, we propose DVRL, which implements this approach by providing a helpful inductive bias to the agent.
29	24	We evaluate our approach on Mountain Hike and several flickering Atari games.
35	19	A partially observable Markov decision process (POMDP) is a tuple (S,A,O, F, U,R, b0), where S is the state space, A the action space, and O the observation space.
56	22	Given a family of encoder distributions qφ(st|st−1, ot), we can also estimate the gradient of the ELBO term in the same manner as in (3), noting that: pθ(s≤T , o≤T ) = pθ(s0) T∏ t=1 pθ(st|st−1)pθ(ot|st), (4) qφ(s≤T |o≤T ) = pθ(s0) T∏ t=1 qφ(st|st−1, ot), (5) where we slightly abuse notation for qφ by ignoring the fact that we sample from the model pθ(s0) for t = 0.
59	18	One way to learn the parameters ρ of an agent’s policy πρ(at|st) is to use n-step learning with A2C (Wu et al., 2017), the synchronous simplification of asynchronous advantage actor-critic (A3C) (Mnih et al., 2016).
70	23	(12) Since this approach is model-free and does not make use of any generative model of the environment, it is unlikely to approximate belief update steps, instead relying on memory or simple heuristics.
73	55	Training everything end-to-end shapes the learned model to be useful for the RL task at hand, and not only for predicting observations.
75	29	For a fair comparison, we modify the original architecture of Zhu et al. (2017) in several ways.
80	18	While previous work often used Q-learning to train the policy (Hausknecht & Stone, 2015; Zhu et al., 2017; Foerster et al., 2016; Narasimhan et al., 2015), we use n-step A2C.
91	55	Our belief state b̂t is thus an approximation of the posterior distribution in our learned model pθ(h≤T , z≤T , o≤T |a<T ) = pθ(h0) T∏ t=1 ( pθ(zt|ht−1, at−1) pθ(ot|ht−1, zt, at−1)δψRNNθ (ht−1,zt,at−1,ot)(ht) ) , (14) with stochastic transition model pθ(zt|ht−1, at−1), decoder pθ(ot|ht−1, zt, at−1), and deterministic transition function ht = ψ RNN θ (ht−1, zt, at−1, ot) which is denoted using the Dirac delta distribution δ and for which we use an RNN.
94	95	(18) First, we resample particles based on their weight by drawing ancestor indices ukt−1.
95	29	This improves model learning (Le et al., 2018; Maddison et al., 2017) and allows us to train the model jointly with the n-step loss (see Section 3.4).
99	29	The weights wkt measure how likely each new latent state value (zkt , h k t ) is under the model and how well it explains the current observation.
100	24	t−1 and use it to sample a new stochastic latent state z k t from the encoder qφ (Eq.
111	40	Figure 2 summarises the entire update step.
114	26	(20) Compared to (9), (10) and (11), the losses now also depend on the encoder parameters φ and model parameters θ, since the policy and value function now condition on the latent states instead of st. By introducing the n-step approximation LELBOt , we can learn θ and φ to jointly optimise LELBOt and the RL loss LAt + λHLHt + λV LVt .
116	18	To make Equation (21) tractable, we approximate the expectation over p(τ) by using sampled trajectories from ne environments.
148	46	In this task, the agent has to navigate along a mountain ridge, but only receives noisy measurements of its current location.
157	32	The main difficulty in Mountain Hike is to correctly estimate the current position.
163	41	We chose flickering Atari as evaluation benchmark, since it was previously used to evaluate the performance of ADRQN (Zhu et al., 2017) and DRQN (Hausknecht & Stone, 2015).
166	22	Partial observability is introduced by flickering, i.e., by a probability of 0.5 of returning a blank screen instead of the actual observation.
175	19	Results for the deterministic environments, including returns reported for DRQN and ADRQN, can be found in Appendix A. DVRL significantly outperforms the RNN-based policy on five out of ten games and narrowly underperforms significantly on only one.
178	33	Consequently, we expect that higher particle numbers provide better information to the policy, leading to higher returns.
193	42	This is consistent with our notion that RNN is mainly performing memory based reasoning, for which longer backpropagation-through-time is required: The belief update (2) in DVRL is a one-step update from bt to bt+1, without the need to condition on past actions and observa- tions.
199	31	We also performed several ablation studies showing the necessity of using an ensemble of particles and of joint optimisation of the ELBO and RL objective.
201	39	Access to a belief distribution opens up several interesting research directions.
202	36	Investigating the role of better generalisation capabilities and the more powerful latent state representation on the policy performance of DVRL can give rise to further improvements.
203	100	DVRL is also likely to benefit from more powerful model architectures and a disentangled latent state.
204	51	Furthermore, uncertainty in the belief state and access to a learned model can be used for curiosity driven exploration in environments with sparse rewards.
