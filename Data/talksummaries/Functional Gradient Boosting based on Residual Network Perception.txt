14	11	In this paper, leveraging this observation, we propose a new gradient boosting method called ResFGB for classification tasks based on ResNet perception, that is, the feature extraction gradually grows by functional gradient methods in the space of feature extractions and the resulting predictor naturally forms a ResNet-type architecture.
16	7	As a result, more efficient optimization is expected.
17	17	In the theoretical analysis of the proposed method, we first formalize the gradient boosting perspective of ResNet math- ematically using the notion of functional gradients in the space of feature extractions.
18	22	That is, we explain that optimization in that space is achieved by stacking ResNet layers.
19	75	We next show a good consistency property of the functional gradient, which motivates us to find feature extraction with small functional gradient norms for estimating the correct label of data.
25	5	Bengio et al. (2006) introduced convex neural networks consisting of a single hidden layer, and proposed a gradient boosting-based method in which linear classifiers are incrementally added with their weights.
34	6	Let X = Rd and Y be a feature space and a finite label set of cardinal c, respectively.
36	10	We denote by νX the marginal distribution on X and by ν(·|X) the conditional distribution on Y .
40	16	The ultimate goal in classification problems is to find a predictor f ∈ Lc2(νX) such that arg maxy∈Y fy(x) correctly classifies its label.
51	11	When we focus on the problem with respect to φ, we use the notation R(φ) def= minw∈Rd×c R(φ,w).
52	16	We also denote byRn(φ,w) andRn(φ) empirical variants ofR(φ,w) and R(φ), respectively, which are defined by replacing Eν by Eνn .
54	57	The key notion used for solving the problem is the functional gradient in function spaces.
55	5	Since they are taken in some function spaces, we first introduce Fréchet differential in general Hilbert spaces.
56	92	LetH be a Hilbert space and h be a function onH.
58	7	Moreover, for simplicity, we call∇ξh(ξ) Fréchet differential or functional gradient.
66	10	Their functional gradients have the form ∇fL(f)(x) = Eν(Y |x)[∂ζ l(f(x), Y )] and∇fLn(f)(xi) = ∂ζ l(f(xi), yi).
67	6	In this paper, we derive functional gradient methods using ∇φRn(φ) rather than ∇fLn(f) like usual gradient boosting (Mason et al., 1999; Friedman, 2001), and provide convergence analyses for problems (1) and (2).
68	44	However, we cannot apply ∇φRn(φ) or ∂φRn(φ,w) directly to the expected risk minimization problem because these functional gradients are zero outside the training data.
70	23	The expected benefit of functional gradient methods using ∇φRn(φ) over usual gradient boosting is that the former can learn a deep model that is known to have high representational power.
79	27	Then, ‖∇fL(f)‖Lc1(νX) ≥ 1√ c ∑ y∈Y ‖ν(y|·)− pf (y|·)‖L1(νX), ‖∇fLn(f)‖Lc1(νn,X) ≥ 1√ c ∑ y∈Y ‖νn(y|·)− pf (y|·)‖L1(νn,X), where we denote by pf (y|x) the softmax function defined by the predictor f , i.e., exp(fy(·))/ ∑ y∈Y exp(fy(·)).
86	5	Pνn [1− pf (Y |X) ≥ 1/2] ≤ 2Eνn [1− pf (Y |X)] ≤ 2 √ c‖∇fLn(f)‖Lc1(νn,X).
87	32	Generally, we can derive a bound on the empirical margin distribution (Koltchinskii & Panchenko, 2002) by using the functional gradient norm in a similar way, and can obtain a generalization bound using it, as shown later.
88	19	Powerful optimization ability and connection to residual networks.
89	60	In the above discussion, we have seen that the classification problem can be reinterpreted as finding a predictor with small functional gradient norms, which may lead to reasonable convergence compared to minimizing the excess risk.
95	7	An iteration of the functional gradient method with a learning rate η is described as φt+1 ← φt − η∇zr ◦ φt = (id− η∇zr) ◦ φt.
96	6	We can immediately notice that this iterate makes φt one level deeper by stacking a residual network-type layer id − η∇zr (He et al., 2016), and data representation is refined through this layer.
98	11	Therefore, feature extraction φ gradually grows by using the functional gradient method to optimizeR′.
99	12	Indeed, we can show the convergence of φT to a stationary point of R′ in Ld2(νX) under smoothness and boundedness assumptions by analogy with a finite-dimensional optimization method.
102	29	We now briefly explain how powerful the functional gradient method is compared to the gradient method in a finitedimensional space, for optimizingR′.
103	9	Let us consider any parameterization of φt ∈ Ld2(νX).
