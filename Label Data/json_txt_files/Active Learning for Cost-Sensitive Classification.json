{"sections": [{"text": "different errors have different costs. Our algorithm, COAL, makes predictions by regressing to each label\u2019s cost and predicting the smallest. On a new example, it uses a set of regressors that perform well on past data to estimate possible costs for each label. It queries only the labels that could be the best, ignoring the sure losers. We prove COAL can be efficiently implemented for any regression family that admits squared loss optimization; it also enjoys strong guarantees with respect to predictive performance and labeling effort. We empirically compare COAL to passive learning and several active learning baselines, showing significant improvements in labeling effort and test cost on real-world datasets."}, {"heading": "1 Introduction", "text": "The field of active learning studies how to efficiently elicit relevant information so learning algorithms can make good decisions. Almost all active learning algorithms are designed for binary classification problems, leading to the natural question: How can active learning address more complex prediction problems? Multiclass and importance-weighted classification require only minor modifications but we know of no active learning algorithms that enjoy theoretical guarantees for more complex problems.\nOne such problem is cost-sensitive multiclass classification (CSMC). In CSMC with K classes, passive learners receive input examples x and cost vectors c \u2208 RK , where c(y) is the cost of predicting label y on x.1 A natural design for an active CSMC learner then is to adaptively query the costs of only a (possibly empty) subset of labels on each x. Since measuring label complexity is more nuanced in CSMC (e.g., is it more expensive to query three costs on a single example or one cost on three examples?), we track both the number of examples for which at least one cost is queried, along with the total number of cost queries issued. The first corresponds to a fixed human effort for inspecting the example. The second captures the additional effort for judging the cost of each prediction, which depends on the number of labels queried. (By querying a label, we mean querying the cost of predicting that label given an example.)\nIn this setup, we develop a new active learning algorithm for CSMC called Cost Overlapped Active Learning (COAL). COAL assumes access to a set of regression functions, and, when processing an example\nakshay@cs.umass.edu, alekha@microsoft.com, tkhuang@protonmail.com, hal@umiacs.umd.edu, jcl@microsoft.com 1Cost here refers to prediction cost and not labeling effort or the cost of acquiring different labels.\nar X\niv :1\n70 3.\n01 01\n4v 4\n[ cs\n.L G\n] 1\n1 O\nx, it uses the functions with good past performance to compute the range of possible costs that each label might take. Naturally, COAL only queries labels with large cost range, akin to uncertainty-based approaches in active regression [11], but furthermore, it only queries labels that could possibly have the smallest cost, avoiding the uncertain, but surely suboptimal labels. The key algorithmic innovation is an efficient way to compute the cost range realized by good regressors. This computation, and COAL as a whole, only requires that the regression functions admit efficient squared loss optimization, in contrast with prior algorithms that require 0/1 loss optimization [7, 19].\nAmong our results, we prove that when processing n (unlabeled) examples withK classes and a regression class with pseudo-dimension d (See Definition 1),\n1. The algorithm needs to solve O(Kn5) regression problems over the function class (Corollary 2). Thus COAL runs in polynomial time for convex regression sets.\n2. With no assumptions on the noise in the problem, the algorithm achieves generalization error O\u0303( \u221a Kd/n)\nand requests O\u0303(n\u03b82 \u221a Kd) costs from O\u0303(n\u03b81 \u221a Kd) examples (Theorems 3 and 6) where \u03b81, \u03b82 are the disagreement coefficients (Definition 2)2. The worst case offers minimal improvement over passive learning, akin to active learning for binary classification.\n3. With a Massart-type noise assumption (Assumption 3), the algorithm has generalization error O\u0303(Kd/n) while requesting O\u0303(Kd(\u03b82 + K\u03b81) log n) labels from O\u0303(Kd\u03b81 log n) examples (Corollary 4, Theorem 7). Thus under favorable conditions, COAL requests exponentially fewer labels than passive learning.\nWe also derive generalization and label complexity bounds under a milder Tsybakov-type noise condition (Assumption 4). Existing lower bounds from binary classification [19] suggest that our results are optimal in their dependence on n, although these lower bounds do not directly apply to our setting. We also discuss some intuitive examples highlighting the benefits of using COAL.\nCSMC provides a more expressive language for success and failure than multiclass classification, which allows learning algorithms to make the trade-offs necessary for good performance and broadens potential applications. For example, CSMC can naturally express partial failure in hierarchical classification [41]. Experimentally, we show that COAL substantially outperforms the passive learning baseline with orders of\n2O\u0303(\u00b7) suppresses logarithmic dependence on n, K, and d.\nmagnitude savings in the labeling effort on a number of hierarchical classification datasets (see Figure 1 for comparison between passive learning and COAL on Reuters text categorization).\nCSMC also forms the basis of learning to avoid cascading failures in joint prediction tasks like structured prediction and reinforcement learning [15, 37, 13]. As our second application, we consider learning to search algorithms for joint or structured prediction, which operate by a reduction to CSMC. In this reduction, evaluating the cost of a class often involves a computationally expensive \u201croll-out,\u201d so using an active learning algorithm inside such a passive joint prediction method can lead to significant computational savings. We show that using COAL within the AGGRAVATE algorithm [37, 13] reduces the number of roll-outs by a factor of 14 to 3 4 on several joint prediction tasks.\nOur code is publicly available as part of the Vowpal Wabbit machine learning library.3"}, {"heading": "2 Related Work", "text": "Active learning is a thriving research area with many theoretical and empirical studies. We recommend the survey of Settles [39] for an overview of more empirical research. We focus here on theoretical results.\nOur work falls into the framework of disagreement-based active learning, which studies general hypothesis spaces typically in an agnostic setup (see Hanneke [19] for an excellent survey). Existing results study binary classification, while our work generalizes to CSMC, assuming that we can accurately predict costs using regression functions from our class. One difference that is natural for CSMC is that our query rule checks the range of predicted costs for a label.\nThe other main difference is that we use a square loss oracle to search the version space. In contrast, prior work either explicitly enumerates the version space [5, 46] or uses a 0/1 loss classification oracle for the search [14, 7, 8, 24]. In most instantiations, the oracle solves an NP-hard problem and so does not directly lead to an efficient algorithm, although practical implementations using heuristics are still quite effective. Our approach instead uses a squared-loss regression oracle, which can be implemented efficiently via convex optimization and leads to a polynomial time algorithm.\nIn addition to disagreement-based approaches, much research has focused on plug-in rules for active learning in binary classification, where one estimates the class-conditional regression function [10, 32, 20, 9]. Apart from Hanneke and Yang [20], these works make smoothness assumptions and have a nonparametric flavor. Instead, Hanneke and Yang [20] assume a calibrated surrogate loss and abstract realizable function class, which is more similar to our setting. While the details vary, our work and these prior results employ the same algorithmic recipe of maintaining an implicit version space and querying in a suitably-defined disagreement region. Our work has two notable differences: (1) our algorithm operates in an oracle computational model, only accessing the function class through square loss minimization problems, (2) our results apply to general CSMC, which exhibit significant differences from binary classification. See Subsection 6.1 for further discussion.\nFocusing on linear representations, Balcan et al. [6], Balcan and Long [4] study active learning with distributional assumptions, while the selective sampling framework from the online learning community considers adversarial assumptions [12, 16, 33, 1]. These methods use query strategies that are specialized to linear representations and do not naturally generalize to other hypothesis classes.\nSupervised learning oracles that solve NP-hard optimization problems in the worst case have been used in other problems including contextual bandits [2, 43] and structured prediction [15]. Thus we hope that our work can inspire new algorithms for these settings as well.\n3http://hunch.net/~vw\nLastly, we mention that square loss regression has been used to estimate costs for passive CSMC [27], but, to our knowledge, using a square loss oracle for active CSMC is new.\nAdvances over Krishnamurthy et al. [26]. Active learning for CSMC was introduced recently in Krishnamurthy et al. [26] with an algorithm that also uses cost ranges to decide where to query. They compute cost ranges by using the regression oracle to perform a binary search for the maximum and minimum costs, but this computation results in a sub-optimal label complexity bound. We resolve this sub-optimality with a novel cost range computation that is inspired by the multiplicative weights technique for solving linear programs. This algorithmic improvement also requires a significantly more sophisticated statistical analysis for which we derive a novel uniform Freedman-type inequality for classes with bounded pseudo-dimension. This result may be of independent interest.\nKrishnamurthy et al. [26] also introduce an online approximation for additional scalability and use this algorithm for their experiments. Our empirical results use this same online approximation and are slightly more comprehensive. Finally, we also derive generalization and label complexity bounds for our algorithm in a setting inspired by Tsybakov\u2019s low noise condition [30, 45].\nComparison with Foster et al. [18]. In a follow-up to the present paper, Foster et al. [18] build on our work with a regression-based approach for contextual bandit learning, a problem that bears some similarities to active learning for CSMC. The results are incomparable due to the differences in setting, but it is worth discussing their techniques. As in our paper, Foster et al. [18] maintain an implicit version space and compute maximum and minimum costs for each label, which they use to make predictions. They resolve the sub-optimality in Krishnamurthy et al. [26] with epoching, which enables a simpler cost range computation than our multiplicative weights approach. However, epoching incurs an additional log(n) factor in the label complexity, and under low-noise conditions where the overall bound is O(polylog(n)), this yields a polynomially worse guarantee than ours."}, {"heading": "3 Problem Setting and Notation", "text": "We study cost-sensitive multiclass classification (CSMC) problems with K classes, where there is an instance space X , a label space Y = {1, . . . ,K}, and a distribution D supported on X \u00d7 [0, 1]K .4 If (x, c) \u223c D, we refer to c as the cost-vector, where c(y) is the cost of predicting y \u2208 Y . A classifier h : X \u2192 Y has expected cost E(x,c)\u223cD[c(h(x))] and we aim to find a classifier with minimal expected cost.\nLet G , {g : X 7\u2192 [0, 1]} denote a set of base regressors and let F , GK denote a set of vector regressors where the yth coordinate of f \u2208 F is written as f(\u00b7; y). The set of classifiers under consideration is H , {hf | f \u2208 F} where each f defines a classifier hf : X 7\u2192 Y by\nhf (x) , argmin y f(x; y). (1)\nWhen using a set of regression functions for a classification task, it is natural to assume that the expected costs under D can be predicted by some function in the set. This motivates the following realizability assumption.\nAssumption 1 (Realizability). Define the Bayes-optimal regressor f?, which has f?(x; y) , Ec[c(y)|x],\u2200x \u2208 X (with D(x) > 0), y \u2208 Y . We assume that f? \u2208 F .\n4In general, labels just serve as indices for the cost vector in CSMC, and the data distribution is over (x, c) pairs instead of (x, y) pairs as in binary and multiclass classification.\nWhile f? is always well defined, note that the cost itself may be noisy. In comparison with our assumption, the existence of a zero-cost classifier inH (which is often assumed in active learning) is stronger, while the existence of hf? inH is weaker but has not been leveraged in active learning.\nWe also require assumptions on the complexity of the class G for our statistical analysis. To this end, we assume that G is a compact convex subset of L\u221e(X ) with finite pseudo-dimension, which is a natural extension of VC-dimension for real-valued predictors.\nDefinition 1 (Pseudo-dimension). The pseudo-dimension Pdim(F) of a function class F : X \u2192 R is defined as the VC-dimension of the set of threshold functionsH+ , {(x, \u03be) 7\u2192 1{f(x) > \u03be} : f \u2208 F} \u2282 X \u00d7 R\u2192 {0, 1}.\nAssumption 2. We assume that G is a compact convex set with Pdim(G) = d <\u221e.\nAs an example, linear functions in some basis representation, e.g., g(x) = \u2211d i=1 wi\u03c6i(x), where weights wi are bounded in some norm, have pseudodimension d. In fact, our result can be stated entirely in terms of covering numbers, and we translate to pseudo-dimension using the fact that such classes have \u201cparametric\" covering numbers of the form (1/\u03b5)d. Thus, our results extend to classes with \u201cnonparametric\" growth rates as well (e.g., Holder-smooth functions), although we focus on the parametric case for simplicity. Note that this is a significant departure from Krishnamurthy et al. [26], which assumed that G was finite.\nOur assumption that G is a compact convex set introduces a computational challenging of managing this infinitely large set. To address this challenge, we follow the trend in active learning of leveraging existing algorithmic research on supervised learning [14, 8, 7] and access G exclusively through a regression oracle. Given an importance-weighted dataset D = {xi, ci, wi}ni=1 where xi \u2208 X , ci \u2208 R, wi \u2208 R+, the regression oracle computes\nORACLE(D) \u2208 argmin g\u2208G n\u2211 i=1 wi(g(xi)\u2212 ci)2. (2)\nSince we assume that G is a compact convex set it is amenable to standard convex optimization techniques, so this imposes no additional restriction. However, in the special case of linear functions, this optimization is just least squares and can be computed in closed form. Note that this is fundamentally different from prior works that use a 0/1-loss minimization oracle [14, 8, 7], which involves an NP-hard optimization in most cases of interest.\nRemark 1. Our assumption that G is convex is only for computational tractability, as it is crucial in the efficient implementation of our query strategy, but is not required for our generalization and label complexity bounds. Unfortunately recent guarantees for learning with non-convex classes [29, 36] do not immediately yield efficient active learning strategies. Note also that Krishnamurthy et al. [26] obtain an efficient algorithm without convexity, but this yields a suboptimal label complexity guarantee.\nGiven a set of examples and queried costs, we often restrict attention to regression functions that predict these costs well and assess the uncertainty in their predictions given a new example x. For a subset of regressors G \u2282 G, we measure uncertainty over possible cost values for x with\n\u03b3(x,G) , c+(x,G)\u2212 c\u2212(x,G), c+(x,G) , max g\u2208G g(x), c\u2212(x,G) , min g\u2208G g(x). (3)\nFor vector regressors F \u2282 F , we define the cost range for a label y given x as \u03b3(x, y, F ) , \u03b3(x,GF (y)) where GF (y) , {f(\u00b7; y) | f \u2208 F} are the base regressors induced by F for y. Note that since we are\nAlgorithm 1: Cost Overlapped Active Learning (COAL)\n1: Input: Regressors G, failure probability \u03b4 \u2264 1/e. 2: Set \u03c8i = 1/ \u221a i, \u03ba = 3, \u03bdn = 324(d log(n) + log(8Ke(d+ 1)n2/\u03b4)). 3: Set \u2206i = \u03bamin{ \u03bdni\u22121 , 1}. 4: for i = 1, 2, . . . , n do 5: gi,y \u2190 arg ming\u2208G R\u0302i(g; y). (See (5)). 6: Define fi \u2190 {gi,y}Ky=1. 7: (Implicitly define) Gi(y)\u2190 {g \u2208 Gi\u22121(y) | R\u0302i(g; y) \u2264 R\u0302i(gi,y; y) + \u2206i}. 8: Receive new example x. Qi(y)\u2190 0,\u2200y \u2208 Y . 9: for every y \u2208 Y do\n10: c\u0302+(y)\u2190 MAXCOST((x, y), \u03c8i/4) and c\u0302\u2212(y)\u2190 MINCOST((x, y), \u03c8i/4). 11: end for 12: Y \u2032 \u2190 {y \u2208 Y | c\u0302\u2212(y) \u2264 miny\u2032 c\u0302+(y\u2032)}. 13: if |Y \u2032| > 1 then 14: Qi(y)\u2190 1 if y \u2208 Y \u2032 and c\u0302+(y)\u2212 c\u0302\u2212(y) > \u03c8i. 15: end if 16: Query costs of each y with Qi(y) = 1. 17: end for\nassuming realizability, whenever f? \u2208 F , the quantities c+(x,GF (y)) and c\u2212(x,GF (y)) provide valid upper and lower bounds on E[c(y)|x].\nTo measure the labeling effort, we track the number of examples for which even a single cost is queried as well as the total number of queries. This bookkeeping captures settings where the editorial effort for inspecting an example is high but each cost requires minimal further effort, as well as those where each cost requires substantial effort. Formally, we define Qi(y) \u2208 {0, 1} to be the indicator that the algorithm queries label y on the ith example and measure\nL1 , n\u2211 i=1 \u2228 y Qi(y), and L2 , n\u2211 i=1 \u2211 y Qi(y). (4)"}, {"heading": "4 Cost Overlapped Active Learning", "text": "The pseudocode for our algorithm, Cost Overlapped Active Learning (COAL), is given in Algorithm 1. Given an example x, COAL queries the costs of some of the labels y for x. These costs are chosen by (1) computing a set of good regression functions based on the past data (i.e., the version space), (2) computing the range of predictions achievable by these functions for each y, and (3) querying each y that could be the best label and has substantial uncertainty. We now detail each step.\nTo compute an approximate version space we first find the regression function that minimizes the empirical risk for each label y, which at round i is:\nR\u0302i(g; y) , 1\ni\u2212 1 i\u22121\u2211 j=1 (g(xj)\u2212 cj(y))2Qj(y). (5)\nRecall that Qj(y) is the indicator that we query label y on the jth example. Computing the minimizer requires one oracle call. We implicitly construct the version space Gi(y) in Line 7 as the surviving regressors with low\nsquare loss regret to the empirical risk minimizer. The tolerance on this regret is \u2206i at round i, which scales like O\u0303(d/i), where recall that d is the pseudo-dimension of the class G.\nCOAL then computes the maximum and minimum costs predicted by the version space Gi(y) on the new example x. Since the true expected cost is f?(x; y) and, as we will see, f?(\u00b7; y) \u2208 Gi(y), these quantities serve as a confidence bound for this value. The computation is done by the MAXCOST and MINCOST subroutines which produce approximations to c+(x,Gi(y)) and c\u2212(x,Gi(y)) respectively (See (3)).\nFinally, using the predicted costs, COAL issues (possibly zero) queries. The algorithm queries any non-dominated label that has a large cost range, where a label is non-dominated if its estimated minimum cost is smaller than the smallest maximum cost (among all other labels) and the cost range is the difference between the label\u2019s estimated maximum and minimum costs.\nIntuitively, COAL queries the cost of every label which cannot be ruled out as having the smallest cost on x, but only if there is sufficient ambiguity about the actual value of the cost. The idea is that labels with little disagreement do not provide much information for further reducing the version space, since by construction all regressors would suffer similar square loss. Moreover, only the labels that could be the best need to be queried at all, since the cost-sensitive performance of a hypothesis hf depends only on the label that it predicts. Hence, labels that are dominated or have small cost range need not be queried.\nSimilar query strategies have been used in prior works on binary and multiclass classification [33, 16, 1], but specialized to linear representations. The key advantage of the linear case is that the set Gi(y) (formally, a different set with similar properties) along with the maximum and minimum costs have closed form expressions, so that the algorithms are easily implemented. However, with a general set G and a regression oracle, computing these confidence intervals is less straightforward. We use the MAXCOST and MINCOST subroutines, and discuss this aspect of our algorithm next."}, {"heading": "4.1 Efficient Computation of Cost Range", "text": "In this section, we describe the MAXCOST subroutine which uses the regression oracle to approximate the maximum cost on label y realized by Gi(y), as defined in (3). The minimum cost computation requires only minor modifications that we discuss at the end of the section.\nDescribing the algorithm requires some additional notation. Let \u2206\u0303j , \u2206j + R\u0302j(gj,y; y) be the right hand side of the constraint defining the version space at round j, where gj,y is the ERM at round j for label y, R\u0302j(\u00b7; y) is the risk functional, and \u2206j is the radius used in COAL. Note that this quantity can be efficiently computed since gj,y can be found with a single oracle call. Due to the requirement that g \u2208 Gi\u22121(y) in the definition of Gi(y), an equivalent representation is Gi(y) = \u22c2i j=1{g : R\u0302j(g; y) \u2264 \u2206\u0303j}. Our approach is based on the observation that given an example x and a label y at round i, finding a function g \u2208 Gi(y) which predicts the maximum cost for the label y on x is equivalent to solving the minimization problem:\nminimizeg\u2208G(g(x)\u2212 1)2 such that \u22001 \u2264 j \u2264 i, R\u0302j(g; y) \u2264 \u2206\u0303j . (7)\nGiven this observation, our strategy will be to find an approximate solution to the problem (7) and it is not difficult to see that this also yields an approximate value for the maximum predicted cost on x for the label y.\nIn Algorithm 2, we show how to efficiently solve this program using the regression oracle. We begin by exploiting the convexity of the set G, meaning that we can further rewrite the optimization problem (7) as\nminimizeP\u2208\u2206(G)Eg\u223cP [ (g(x)\u2212 1)2 ] such that \u22001 \u2264 j \u2264 i,Eg\u223cP [ R\u0302j(g; y) ] \u2264 \u2206\u0303j . (8)\nThe above rewriting is effectively cosmetic as G = \u2206(G) by the definition of convexity, but the upshot is that our rewriting results in both the objective and constraints being linear in the optimization variable P . Thus,\nAlgorithm 2: MAXCOST\n1: Input: (x, y), tolerance TOL, (implicitly) risk functionals {R\u0302j(\u00b7; y)}ij=1. 2: Compute gj,y = argming\u2208G R\u0302j(g; y) for each j. 3: Let \u2206j = \u03bamin{ \u03bdnj\u22121 , 1}, \u2206\u0303j = R\u0302j(gj,y; y) + \u2206j for each j. 4: Initialize parameters: c` \u2190 0, ch \u2190 1, T \u2190 log(i+1)(12/\u2206i) 2 TOL4 , \u03b7 \u2190 \u221a log(i+ 1)/T . 5: while |c` \u2212 ch| > TOL2/2 do 6: c\u2190 ch\u2212c`2 7: \u00b5(1) \u2190 1 \u2208 Ri+1. . Use MW to check feasibility of Program (9). 8: for t = 1, . . . , T do 9: Use the regression oracle to find\ngt \u2190 argmin g\u2208G \u00b5 (t) 0 (g(x)\u2212 1)2 + i\u2211 j=1 \u00b5 (t) j R\u0302j(g; y) (6)\n10: If the objective in (6) for gt is at least \u00b5 (t) 0 c+ \u2211i j=1 \u00b5 (t) j \u2206\u0303j , c` \u2190 c, go to 5. 11: Update\n\u00b5 (t+1) j \u2190 \u00b5 (t) j\n( 1\u2212 \u03b7 \u2206\u0303j \u2212 R\u0302j(gt; y)\n\u2206j + 1\n) , \u00b5\n(t+1) 0 \u2190 \u00b5 (t) 0\n( 1\u2212 \u03b7 c\u2212 (gt(x)\u2212 1) 2\n2\n) .\n12: end for 13: ch \u2190 c. 14: end while 15: Return c\u0302+(y) = 1\u2212 \u221a c`.\nwe effectively wish to solve a linear program in P , with our computational tool being a regression oracle over the set G. To do this, we create a series of feasibility problems, where we repeatedly guess the optimal objective value for the problem (8) and then check whether there is indeed a distribution P which satisfies all the constraints and gives the posited objective value. That is, we check\n?\u2203P \u2208 \u2206(G) such that Eg\u223cP (g(x)\u2212 1)2 \u2264 c and \u22001 \u2264 j \u2264 i,Eg\u223cP R\u0302j(g; y) \u2264 \u2206\u0303j . (9)\nIf we find such a solution, we increase our guess, and otherwise we reduce the guess and proceed until we localize the optimal value to a small enough interval.\nIt remains to specify how to solve the feasibility problem (9). Noting that this is a linear feasibility problem, we jointly invoke the Multiplicative Weights (MW) algorithm and the regression oracle in order to either find an approximately feasible solution or certify the problem as infeasible. MW is an iterative algorithm that maintains weights \u00b5 over the constraints. At each iteration it (1) collapses the constraints into one, by taking a linear combination weighted by \u00b5, (2) checks feasibility of the simpler problem with a single constraint, and (3) if the simpler problem is feasible, it updates the weights using the slack of the proposed solution. Details of steps (1) and (3) are described in Algorithm 2.\nFor step (2), the simpler problem that we must solve takes the form\n?\u2203P \u2208 \u2206(G) such that \u00b50Eg\u223cP (g(x)\u2212 1)2 + i\u2211\nj=1\n\u00b5jEg\u223cP R\u0302j(g; y) \u2264 \u00b50c+ i\u2211\nj=1\n\u00b5j\u2206\u0303j .\nThis program can be solved by a single call to the regression oracle, since all terms on the left-hand-side involve square losses while the right hand side is a constant. Thus we can efficiently implement the MW algorithm using the regression oracle. Finally, recalling that the above description is for a fixed value of objective c, and recalling that the maximum can be approximated by a binary search over c leads to an oraclebased algorithm for computing the maximum cost. For this procedure, we have the following computational guarantee.\nTheorem 1. Algorithm 2 returns an estimate c\u0302+(x; y) such that c+(x; y) \u2264 c\u0302+(x; y) \u2264 c+(x; y) + TOL and runs in polynomial time with O(max{1, i2/\u03bd2n} log(i) log(1/TOL)/TOL4) calls to the regression oracle.\nThe minimum cost can be estimated in exactly the same way, replacing the objective (g(x)\u2212 1)2 with (g(x) \u2212 0)2 in Program (7). In COAL, we set TOL = 1/ \u221a i at iteration i and have \u03bdn = O\u0303(d). As a consequence, we can bound the total oracle complexity after processing n examples.\nCorollary 2. After processing n examples, COAL makes O\u0303(K(d3 + n5/d2)) calls to the square loss oracle.\nThus COAL can be implemented in polynomial time for any set G that admits efficient square loss optimization. Compared to Krishnamurthy et al. [26] which required O(n2) oracle calls, the guarantee here is, at face value, worse, since the algorithm is slower. However, the algorithm enforces a much stronger constraint on the version space which leads to a much better statistical analysis, as we will discuss next. Nevertheless, these algorithms that use batch square loss optimization in an iterative or sequential fashion are too computational demanding to scale to larger problems. Our implementation alleviates this with an alternative heuristic approximation based on a sensitivity analysis of the oracle, which we detail in Section 7."}, {"heading": "5 Generalization Analysis", "text": "In this section, we derive generalization guarantees for COAL. We study three settings: one with minimal assumptions and two low-noise settings.\nOur first low-noise assumption is related to the Massart noise condition [31], which in binary classification posits that the Bayes optimal predictor is bounded away from 1/2 for all x. Our condition generalizes this to CSMC and posits that the expected cost of the best label is separated from the expected cost of all other labels.\nAssumption 3. A distribution D supported over (x, c) pairs satisfies the Massart noise condition with parameter \u03c4 > 0, if for all x (with D(x) > 0),\nf?(x; y?(x)) \u2264 min y 6=y?(x) f?(x; y)\u2212 \u03c4,\nwhere y?(x) , argminy f ?(x; y) is the true best label for x.\nThe Massart noise condition describes favorable prediction problems that lead to sharper generalization and label complexity bounds for COAL. We also study a milder noise assumption, inspired by the Tsybakov condition [30, 45], again generalized to CSMC. See also Agarwal [1].\nAssumption 4. A distribution D supported over (x, c) pairs satisfies the Tsbyakov noise condition with parameters (\u03c40, \u03b1, \u03b2) if for all 0 \u2264 \u03c4 \u2264 \u03c40,\nPx\u223cD [\nmin y 6=y?(x)\nf?(x; y)\u2212 f?(x; y?(x)) \u2264 \u03c4 ] \u2264 \u03b2\u03c4\u03b1,\nwhere y?(x) , argminy f ?(x; y).\nObserve that the Massart noise condition in Assumption 3 is a limiting case of the Tsybakov condition, with \u03c4 = \u03c40 and \u03b1 \u2192 \u221e. The Tsybakov condition states that it is polynomially unlikely for the cost of the best label to be close to the cost of the other labels. This condition has been used in previous work on cost-sensitive active learning [1] and is also related to the condition studied by Castro and Nowak [10] with the translation that \u03b1 = 1\u03ba\u22121 , where \u03ba \u2208 [0, 1] is their noise level.\nOur generalization bound is stated in terms of the noise level in the problem so that they can be readily adapted to the favorable assumptions. We define the noise level using the following quantity, given any \u03b6 > 0.\nP\u03b6 , Px\u223cD [\nmin y 6=y?(x)\nf?(x; y)\u2212 f?(x; y?(x)) \u2264 \u03b6 ] . (10)\nP\u03b6 describes the probability that the expected cost of the best label is close to the expected cost of the second best label. When P\u03b6 is small for large \u03b6 the labels are well-separated so learning is easier. For instance, under a Massart condition P\u03b6 = 0 for all \u03b6 \u2264 \u03c4 .\nWe now state our generalization guarantee.\nTheorem 3. For any \u03b4 < 1/e, for all i \u2208 [n], with probability at least 1\u2212 \u03b4, we have\nEx,c[c(hfi+1(x))\u2212 c(hf?(x))] \u2264 min \u03b6>0\n{ \u03b6P\u03b6 +\n32K\u03bdn \u03b6i\n} ,\nwhere \u03bdn, fi are defined in Algorithm 1, and hfi is defined in (1). In the worst case, we bound P\u03b6 by 1 and optimize for \u03b6 to obtain an O\u0303( \u221a Kd log(1/\u03b4)/i) bound after i samples, where recall that d is the pseudo-dimension of G. This agrees with the standard generalization bound ofO( \u221a Pdim(F) log(1/\u03b4)/i) for VC-type classes becauseF = GK hasO(Kd) statistical complexity. However, since the bound captures the difficulty of the CSMC problem as measured by P\u03b6 , we can obtain sharper results under Assumptions 3 and 4 by appropriately setting \u03b6.\nCorollary 4. Under Assumption 3, for any \u03b4 < 1/e, with probability at least 1\u2212 \u03b4, for all i \u2208 [n], we have\nEx,c[c(hfi+1(x))\u2212 c(hf?(x))] \u2264 32K\u03bdn i\u03c4 .\nCorollary 5. Under Assumption 4, for any \u03b4 < 1/e, with probability at least 1\u2212 \u03b4, for all 32K\u03bdn \u03b2\u03c4\u03b1+20 \u2264 i \u2264 n, we have\nEx,c[c(hfi+1(x))\u2212 c(hf?(x))] \u2264 2\u03b2 1 \u03b1+2\n( 32K\u03bdn\ni\n)\u03b1+1 \u03b1+2\n.\nThus, Massart and Tsybakov-type conditions lead to a faster convergence rate of O\u0303(1/n) and O\u0303(n\u2212 \u03b1+1 \u03b1+2 ). This agrees with the literature on active learning for classification [31] and can be viewed as a generalization to CSMC. Both generalization bounds match the optimal rates for binary classification under the analogous low-noise assumptions [31, 45]. We emphasize that COAL obtains these bounds as is, without changing any parameters, and hence COAL is adaptive to favorable noise conditions."}, {"heading": "6 Label Complexity Analysis", "text": "Without distributional assumptions, the label complexity of COAL can be O(n), just as in the binary classification case, since there may always be confusing labels that force querying. In line with prior work, we introduce two disagreement coefficients that characterize favorable distributional properties. We first define a set of good classifiers, the cost-sensitive regret ball:\nFcsr(r) , { f \u2208 F \u2223\u2223\u2223 E [c(hf (x))\u2212 c(hf?(x))] \u2264 r} . We also recall our earlier notation \u03b3(x, y, F ) (see (3) and the subsequent discussion) for a subset F \u2286 F which indicates the range of expected costs for (x, y) as predicted by the regressors corresponding to the classifiers in F . We now define the disagreement coefficients.\nDefinition 2 (Disagreement coefficients). Define\nDIS(r, y) , {x | \u2203f, f \u2032 \u2208 Fcsr(r), hf (x) = y 6= hf \u2032(x)} .\nThen the disagreement coefficients are defined as:\n\u03b81 , sup \u03c8,r>0\n\u03c8 r P (\u2203y | \u03b3(x, y,Fcsr(r)) > \u03c8 \u2227 x \u2208 DIS(r, y)) ,\n\u03b82 , sup \u03c8,r>0\n\u03c8\nr \u2211 y P (\u03b3(x, y,Fcsr(r)) > \u03c8 \u2227 x \u2208 DIS(r, y)) .\nIntuitively, the conditions in both coefficients correspond to the checks on the domination and cost range of a label in Lines 12 and 14 of Algorithm 1. Specifically, when x \u2208 DIS(r, y), there is confusion about whether y is the optimal label or not, and hence y is not dominated. The condition on \u03b3(x, y,Fcsr(r)) additionally captures the fact that a small cost range provides little information, even when y is non-dominated. Collectively, the coefficients capture the probability of an example x where the good classifiers disagree on x in both predicted costs and labels. Importantly, the notion of good classifiers is via the algorithm-independent set Fcsr(r), and is only a property of F and the data distribution.\nThe definitions are a natural adaptation from binary classification [19], where a similar disagreement region to DIS(r, y) is used. Our definition asks for confusion about the optimality of a specific label y, which provides more detailed information about the cost-structure than simply asking for any confusion among the good classifiers. The 1/r scaling is in agreement with previous related definitions [19], and we also scale by the cost range parameter \u03c8, so that the favorable settings for active learning can be concisely expressed as having \u03b81, \u03b82 bounded, as opposed to a complex function of \u03c8.\nThe next three results bound the labeling effort (4), in the high noise and low noise cases respectively. The low noise assumptions enable significantly sharper bounds. Before stating the bounds, we recall that L1 corresponds to the number of examples where at least one cost is queried, while L2 is the total number of costs queried across all examples.\nTheorem 6. With probability at least 1\u2212 \u03b4, the label complexity of the algorithm over n examples is at most L1 = O ( n\u03b81 \u221a K\u03bdn + log(1/\u03b4) ) , L2 = O ( n\u03b82 \u221a K\u03bdn +K log(1/\u03b4) ) .\nTheorem 7. Assume the Massart noise condition holds. With probability at least 1\u2212 \u03b4 the label complexity of the algorithm over n examples is at most\nL1 = O ( K log(n)\u03bdn\n\u03c42 \u03b81 + log(1/\u03b4)\n) , L2 = O (KL1)\nTheorem 8. Assume the Tsybakov noise condition holds. With probability at least 1\u2212 \u03b4 the label complexity of the algorithm over n examples is at most\nL1 = O ( \u03b8 \u03b1 \u03b1+1 1 (K\u03bdn) \u03b1 \u03b1+2n 2 \u03b1+2 + log(1/\u03b4) ) , L2 = O (KL1)\nIn the high-noise case, the bounds scales with n\u03b8 for the respective coefficients. In comparison, for binary classification the leading term is O\u0303 (n\u03b8error(hf?)) which involves a different disagreement coefficient and which scales with the error of the optimal classifier hf? [19, 24]. Qualitatively the bounds have similar worstcase behavior, demonstrating minimal improvement over passive learning, but by scaling with error(hf?) the binary classification bound reflects improvements on benign instances. For the special case of multiclass classification, we are able to recover the dependence on error(hf?) and the standard disagreement coefficient with a simple modification to our proof, which we discuss in detail in the next subsection.\nOn the other hand, in both low noise cases the label complexity scales sublinearly with n. With bounded disagreement coefficients, this improves over the standard passive learning analysis where all labels are queried on n examples to achieve the generalization guarantees in Theorem 3, Corollary 4, and Corollary 5 respectively. In particular, under the Massart condition, both L1 and L2 bounds scale with \u03b8 log(n) for the respective disagreement coefficients, which is an exponential improvement over the passive learning analysis. Under the milder Tsybakov condition, the bounds scale with \u03b8 \u03b1 \u03b1+1n 2 \u03b1+2 , which improves polynomially over passive learning. These label complexity bounds agree with analogous results from binary classification [10, 19, 21] in their dependence on n.\nNote that \u03b82 \u2264 K\u03b81 always and it can be much smaller, as demonstrated through an example in the next section. In such cases, only a few labels are ever queried and the L2 bound in the high noise case reflects this additional savings over passive learning. Unfortunately, in low noise conditions, we do not benefit when \u03b82 K\u03b81. This can be resolved by letting \u03c8i in the algorithm depend on the noise level \u03c4 , but we prefer to use the more robust choice \u03c8i = 1/ \u221a i which still allows COAL to partially adapt to low noise and achieve low label complexity. The main improvement over Krishnamurthy et al. [26] is demonstrated in the label complexity bounds under low noise assumptions. For example, under Massart noise, our bound has the optimal log(n)/\u03c42 rate, while the bound in Krishnamurthy et al. [26] is exponentially worse, scaling with n\u03b2/\u03c42 for \u03b2 \u2208 (0, 1). This improvement comes from explicitly enforcing monotonicity of the version space, so that once a regressor is eliminated it can never force COAL to query again. Algorithmically, computing the maximum and minimum costs with the monotonicity constraint is much more challenging and requires the new subroutine using MW."}, {"heading": "6.1 Recovering Hanneke\u2019s Disagreement Coefficient", "text": "In this subsection we show that in many cases we can obtain guarantees in terms of Hanneke\u2019s disagreement coefficient [19], which has been used extensively in active learning for binary classification. We also show that, for multiclass classification, the label complexity scales with the error of the optimal classifier h?, a refinement on Theorem 6. The guarantees require no modifications to the algorithm and enable a precise comparison with prior results. Unfortunately, they do not apply to the general CSMC setting, so they have not been incorporated into our main theorems.\nWe start with defining Hanneke\u2019s disagreement coefficient [19]. Define the disagreement ball F\u0303(r) , {f \u2208 F : P[hf (x) 6= hf?(x)] \u2264 r} and the disagreement region D\u0303IS(r) , {x | \u2203f, f \u2032 \u2208 F\u0303(r), hf (x) 6= hf \u2032(x)}. The coefficient is defined as\n\u03b80 , sup r>0\n1 r P [ x \u2208 D\u0303IS(r) ] . (11)\nThis coefficient is known to be O(1) in many cases, for example when the hypothesis class consists of linear separators and the marginal distribution is uniform over the unit sphere [19, Chapter 7]. In comparison with Definition 2, the two differences are that \u03b81, \u03b82 include the cost-range condition and involve the cost-sensitive regret ball Fcsr(r) rather than F\u0303(r). As F\u0303(r) \u2282 Fcsr(r), we expect that \u03b81 and \u03b82 are typically larger than \u03b80, so bounds in terms of \u03b80 are more desirable. We now show that such guarantees are possible in many cases.\nThe low noise case. For general CSMC, low noise conditions admit the following:\nProposition 1. Under Massart noise, with probability at least 1\u2212 \u03b4 the label complexity of the algorithm over n examples is at most L1 = O\n( log(n)\u03bdn \u03c42 \u03b80 + log(1/\u03b4) ) . Under Tsybakov noise, the label complexity is\nat most L1 = O ( \u03b80n 2 \u03b1+2 (log(n)\u03bdn) \u03b1 \u03b1+2 + log(1/\u03b4) ) . In both cases we have L2 = O(KL1).\nThat is, for any low noise CSMC problem, COAL obtains a label complexity bound in terms of Hanneke\u2019s disagreement coefficient \u03b80 directly. Note that this adaptivity requires no change to the algorithm. Proposition 1 enables a precise comparison with disagreement-based active learning for binary classification. In particular, this bound matches the guarantee for CAL [19, Theorem 5.4] with the caveat that our measure of statistical complexity is the pseudodimension of the F instead of the VC-dimension of the hypothesis class. As a consequence, under low noise assumptions, COAL has favorable label complexity in all examples where \u03b80 is small.\nThe high noise case. Outside of the low noise setting, we can introduce \u03b80 into our bounds, but only for multiclass classification, where we always have c , 1 \u2212 ey for some y \u2208 [K]. Note that f(x; y) is now interpreted as a prediction for 1 \u2212 P (y|x), so that the least cost prediction y?(x) corresponds to the most likely label. We also obtain a further refinement by introducing error(hf?) , E(x,c)[c(hf?(x))].\nProposition 2. For multiclass classification, with probability at least 1 \u2212 \u03b4, the label complexity of the algorithm over n examples is at most\nL1 = 4\u03b80n \u00b7 error(hf?) +O ( \u03b80 (\u221a Kn\u03bdn \u00b7 error(hf?) +K\u03ba\u03bdn log(n) ) + log(1/\u03b4) ) .\nThis result exploits two properties of the multiclass cost structure. First we can relate Fcsr(r) to the disagreement ball F\u0303(r), which lets us introduce Hanneke\u2019s disagreement coefficient \u03b80. Second, we can bound P\u03b6 in Theorem 3 in terms of error(hf?). Together the bound is comparable to prior results for active learning in binary classification [23, 20, 19], with a slight generalization to the multiclass setting. Unfortunately, both of these refinements do not apply for general CSMC.\nSummary. In important special cases, COAL achieves label complexity bounds directly comparable with results for active learning in binary classification, scaling with \u03b80 and error(hf?). In such cases, whenever \u03b80 is bounded \u2014 for which many examples are known \u2014 COAL has favorable label complexity. However, in general CSMC without low-noise assumptions, we are not able to obtain a bound in terms of these quantities, and we believe a bound involving \u03b80 does not hold for COAL. We leave understanding natural settings where \u03b81 and \u03b82 are small, or obtaining sharper guarantees as intriguing future directions."}, {"heading": "6.2 Three Examples", "text": "We now describe three examples to give more intuition for COAL and our label complexity bounds. Even in the low noise case, our label complexity analysis does not demonstrate all of the potential benefits of our query rule. In this section we give three examples to further demonstrate these advantages.\nOur first example shows the benefits of using the domination criterion in querying, in addition to the cost range condition. Consider a problem under Assumption 3, where the optimal cost is predicted perfectly, the second best cost is \u03c4 worse and all the other costs are substantially worse, but with variability in the predictions. Since all classifiers predict the correct label, we get \u03b81 = \u03b82 = 0, so our label complexity bound is O(1). Intuitively, since every regressor is certain of the optimal label and its cost, we actually make zero queries. On the other hand, all of the suboptimal labels have large cost ranges, so querying based solely on a cost range criteria, as would happen with an active regression algorithm [11], leads to a large label complexity.\nA related example demonstrates the improvement in our query rule over more na\u00efve approaches where we query either no label or all labels, which is the natural generalization of query rules from multiclass classification [1]. In the above example, if the best and second best labels are confused occasionally \u03b81 may be large, but we expect \u03b82 K\u03b81 since no other label can be confused with the best. Thus, the L2 bound in Theorem 6 is a factor of K smaller than with a na\u00efve query rule since COAL only queries the best and second best labels. Unfortunately, without setting \u03c8i as a function of the noise parameters, the bounds in the low noise cases do not reflect this behavior.\nThe third example shows that both \u03b80 and \u03b81 yield pessimistic bounds on the label complexity of COAL in some cases. The example is more involved, so we describe it in detail. We focus on statistical issues, using a finite regressor class F . Note that our results on generalization and label complexity hold in this setting, replacing d log(n) with log |F|, and the algorithm can be implemented by enumerating F . Throughout this example, we use O\u0303(\u00b7) to further suppress logarithmic dependence on n.\nLet X , {x1, . . . , xM}, Y , {0, 1}, and consider functions F , {f?, f1, . . . , fM}. We have f?(x) , (1/4, 1/2),\u2200x \u2208 X and fi(xi) , (1/4, 0) and fi(xj) , (1/4, 1) for i 6= j. The marginal distribution is uniform and the true expected costs are given by f? so that the problem satisfies the Massart noise condition with \u03c4 = 1/4. The key to the construction is that fis have high square loss on labels that they do not predict.\nObserve that as P[hfi(x) 6= hf?(x)] = 1/M and hfi(xi) 6= hf?(xi) for all i, the probability of disagreement is 1 until all fi are eliminated. As such, we have \u03b80 = M . Similarly, we have E[c(hfi(x))\u2212 c(hf?(x))] = 1 4M and \u03b3(x, 1,Fcsr( 1 4M )) = 1, so \u03b81 = 4M . Therefore, the bounds in Theorem 7 and Proposition 1 are both O\u0303(M log |F|) = O\u0303(|F|). On the other hand, since (fi(xj , 1)\u2212 f?(xj , 1))2 = 1/4 for all i, j \u2208 [M ], COAL eliminates every fi once it has made a total of O\u0303(log |F|) queries to label y = 1. Thus the label complexity is actually just O\u0303(log |F|), which is exponentially better than the disagreement-based analyses. Thus, COAL can perform much better than suggested by the disagreement-based analyses, and an interesting future direction is to obtain refined guarantees for cost-sensitive active learning."}, {"heading": "7 Experiments", "text": "We now turn to an empirical evaluation of COAL. For further computational efficiency, we implemented an approximate version of COAL using: 1) a relaxed version space Gi(y) \u2190 {g \u2208 G | R\u0302i(g; y) \u2264 R\u0302i(gi,y; y) + \u2206i}, which does not enforce monotonicity, and 2) online optimization, based on online linear least-squares regression. The algorithm processes the data in one pass, and the idea is to (1) replace gi,y , the ERM, with an approximation goi,y obtained by online updates, and (2) compute the minimum and maximum costs via a sensitivity analysis of the online update. We describe this algorithm in detail in Subsection 7.1.\nThen, we present our experimental results, first for simulated active learning (Subsection 7.2) and then for learning to search for joint prediction (Subsection 7.3)."}, {"heading": "7.1 Finding Cost Ranges with Online Approximation", "text": "Consider the maximum and minimum costs for a fixed example x and label y at round i, all of which may be suppressed. We ignore all the constraints on the empirical square losses for the past rounds. First, define R\u0302(g, w, c; y) , R\u0302(g; y) + w(g(x)\u2212 c)2, which is the risk functional augmented with a fake example with weight w and cost c. Also define\ng w , arg min g\u2208G R\u0302(g, w, 0; y), gw , arg min g\u2208G R\u0302(g, w, 1; y),\nand recall that gi,y is the ERM given in Algorithm 1. The functional R\u0302(g, w, c; y) has a monotonicity property that we exploit here, proved in Appendix C.\nLemma 1. For any c and for w\u2032 \u2265 w \u2265 0, define g = argming R\u0302(g, w, c) and g\u2032 = argming R\u0302(g, w\u2032, c). Then\nR\u0302(g\u2032) \u2265 R\u0302(g) and (g\u2032(x)\u2212 c)2 \u2264 (g(x)\u2212 c)2.\nAs a result, an alternative to MINCOST and MAXCOST is to find\nw , max{w | R\u0302(g w )\u2212 R\u0302(gi,y) \u2264 \u2206i}, (12)\nw , max{w | R\u0302(gw)\u2212 R\u0302(gi,y) \u2264 \u2206i}, (13)\nand return g w (x) and gw(x) as the minimum and maximum costs. We use two steps of approximation here. Using the definition of gw and gw as the minimizers of R\u0302(g, w, 1; y) and R\u0302(g, w, 0; y) respectively, we have\nR\u0302(g w )\u2212 R\u0302(gi,y) \u2264 w \u00b7 gi,y(x)2 \u2212 w \u00b7 gw(x) 2,\nR\u0302(gw)\u2212 R\u0302(gi,y) \u2264 w \u00b7 (gi,y(x)\u2212 1)2 \u2212 w \u00b7 (gw(x)\u2212 1)2.\nWe use this upper bound in place of R\u0302(gw)\u2212 R\u0302(gi,y) in (12) and (13). Second, we replace gi,y , gw, and gw with approximations obtained by online updates. More specifically, we replace gi,y with goi,y, the current regressor produced by all online linear least squares updates so far, and approximate the others by\ng w\n(x) \u2248 goi,y(x)\u2212 w \u00b7 s(x, 0, goi,y), gw(x) \u2248 goi,y(x) + w \u00b7 s(x, 1, goi,y),\nwhere s(x, y, goi,y) \u2265 0 is a sensitivity value that approximates the change in prediction on x resulting from an online update to goi,y with features x and label y. The computation of this sensitivity value is governed by the actual online update where we compute the derivative of the change in the prediction as a function of the importance weight w for a hypothetical example with cost 0 or cost 1 and the same features. This is possible for essentially all online update rules on importance weighted examples, and it corresponds to taking the limit as w \u2192 0 of the change in prediction due to an update, divided by w. Since we are using linear representations, this requires only O(s) time per example, where s is the average number of non-zero features. With these two steps, we obtain approximate minimum and maximum costs using\ngoi,y(x)\u2212 wo \u00b7 s(x, 0, goi,y), goi,y(x) + wo \u00b7 s(x, 1, goi,y),\nwhere\nwo , max{w | w ( goi,y,(x) 2 \u2212 (goi,y(x)\u2212 w \u00b7 s(x, 0, goi,y))2 ) \u2264 \u2206i}\nwo , max{w | w ( (goi,y,(x)\u2212 1)2 \u2212 (goi,y(x) + w \u00b7 s(x, 1, goi,y)\u2212 1)2 ) \u2264 \u2206i}.\nThe online update guarantees that goi,y(x) \u2208 [0, 1]. Since the minimum cost is lower bounded by 0, we have wo \u2208 ( 0, goi,y(x)\ns(x,0,goi,y)\n] . Finally, because the objective w(goi,y(x))\n2 \u2212 w(goi,y(x) \u2212 w \u00b7 s(x, 0, goi,y))2 is increasing in w within this range (which can be seen by inspecting the derivative), we can find wo with binary search. Using the same techniques, we also obtain an approximate maximum cost."}, {"heading": "7.2 Simulated Active Learning", "text": "We performed simulated active learning experiments with three datasets. ImageNet 20 and 40 are sub-trees of the ImageNet hierarchy covering the 20 and 40 most frequent classes, where each example has a single zero-cost label, and the cost for an incorrect label is the tree-distance to the correct one. The feature vectors are the top layer of the Inception neural network [44]. The third, RCV1-v2 [28], is a multilabel text-categorization dataset, which has 103 labels, organized as a tree with a similar tree-distance cost structure as the ImageNet data. Some dataset statistics are in Table 1.\nWe compare our online version of COAL to passive online learning. We use the cost-sensitive oneagainst-all (CSOAA) implementation in Vowpal Wabbit5, which performs online linear regression for each\n5http://hunch.net/~vw\nlabel separately. There are two tuning parameters in our implementation. First, instead of \u2206i, we set the radius of the version space to \u2206\u2032i = \u03ba\u03bdi\u22121 i\u22121 (i.e. the log(n) term in the definition of \u03bdn is replaced with log(i)) and instead tune the constant \u03ba. This alternate \u201cmellowness\" parameter controls how aggressive the query strategy is. The second parameter is the learning rate used by online linear regression6.\nFor all experiments, we show the results obtained by the best learning rate for each mellowness on each dataset, which is tuned as follows. We randomly permute the training data 100 times and make one pass through the training set with each parameter setting. For each dataset let perf(mel, l, q, t) denote the test performance of the algorithm using mellowness mel and learning rate l on the tth permutation of the training data under a query budget of 2(q\u22121) \u00b7 10 \u00b7K, q \u2265 1. Let query(mel, l, q, t) denote the number of queries actually made. Note that query(mel, l, q, t) < 2(q\u22121) \u00b7 10 \u00b7K if the algorithm runs out of the training data before reaching the qth query budget7. To evaluate the trade-off between test performance and number of queries, we define the following performance measure:\nAUC(mel, l, t) = 1\n2 qmax\u2211 q=1 ( perf(mel, l, q + 1, t) + perf(mel, l, q, t) ) \u00b7 log2 query(mel, l, q + 1, t) query(mel, l, q, t) , (14)\nwhere qmax is the minimum q such that 2(q\u22121) \u00b710 is larger than the size of the training data. This performance measure is the area under the curve of test performance against number of queries in log2 scale. A large value means the test performance quickly improves with the number of queries. The best learning rate for mellowness mel is then chosen as\nl?(mel) , arg max l median1\u2264t\u2264100 AUC(mel, l, t).\nThe best learning rates for different datasets and mellowness settings are in Table 2. In the top row of Figure 2, we plot, for each dataset and mellowness, the number of queries against the median test cost along with bars extending from the 15th to 85th quantile. Overall, COAL achieves a better 6We use the default online learning algorithm in Vowpal Wabbit, which is a scale-free [38] importance weight invariant [25] form of AdaGrad [17]. 7In fact, we check the test performance only in between examples, so query(mel, l, q, t) may be larger than 2(q\u22121) \u00b7 10 \u00b7K by an additive factor of K, which is negligibly small.\ntrade-off between performance and queries. With proper mellowness parameter, active learning achieves similar test cost as passive learning with a factor of 8 to 32 fewer queries. On ImageNet 40 and RCV1-v2 (reproduced in Figure 1), active learning achieves better test cost with a factor of 16 fewer queries. On RCV1-v2, COAL queries like passive up to around 256k queries, since the data is very sparse, and linear regression has the property that the cost range is maximal when an example has a new unseen feature. Once COAL sees all features a few times, it queries much more efficiently than passive. These plots correspond to the label complexity L2.\nIn the bottom row, we plot the test error as a function of the number of examples for which at least one query was requested, for each dataset and mellowness, which experimentally corresponds to the L1 label complexity. In comparison to the top row, the improvements offered by active learning are slightly less dramatic here. This suggests that our algorithm queries just a few labels for each example, but does end up issuing at least one query on most of the examples. Nevertheless, one can still achieve test cost competitive with passive learning using a factor of 2-16 less labeling effort, as measured by L1.\nWe also compare COAL with two active learning baselines. Both algorithms differ from COAL only in their query rule. ALLORNONE queries either all labels or no labels using both domination and cost-range conditions and is an adaptation of existing multiclass active learners [1]. NODOM just uses the cost-range condition, inspired by active regression [11]. The results for ImageNet 40 and RCV1-v2 are displayed in Figure 3, where we use the AUC strategy to choose the learning rate. We choose the mellowness by visual inspection for the baselines and use 0.01 for COAL8. On ImageNet 40, the ablations provide minimal improvement over passive learning, while on RCV1-v2, ALLORNONE does provide marginal improvement. However, on both datasets, COAL substantially outperforms both baselines and passive learning.\nWhile not always the best, we recommend a mellowness setting of 0.01 as it achieves reasonable performance on all three datasets. This is also confirmed by the learning-to-search experiments, which we discuss next."}, {"heading": "7.3 Learning to Search", "text": "We also experiment with COAL as the base leaner in learning-to-search [15, 13], which reduces joint prediction problems to CSMC. A joint prediction example defines a search space, where a sequence of decisions are made to generate the structured label. We focus here on sequence labeling tasks, where the input is a sentence and the output is a sequence of labels, specifically, parts of speech or named entities.\n8We use 0.01 for ALLORNONE and 10\u22123 for NODOM.\nLearning-to-search solves such problems by generating the output one label at a time, conditioning on all past decisions. Since mistakes may lead to compounding errors, it is natural to represent the decision space as a CSMC problem, where the classes are the \u201cactions\u201d available (e.g., possible labels for a word) and the costs reflect the long term loss of each choice. Intuitively, we should be able to avoid expensive computation of long term loss on decisions like \u201cis \u2018the\u2019 a DETERMINER?\u201d once we are quite sure of the answer. Similar ideas motivate adaptive sampling for structured prediction [40].\nWe specifically use AGGRAVATE [37, 13, 42], which runs a learned policy to produce a backbone sequence of labels. For each position in the input, it then considers all possible deviation actions and executes an oracle for the rest of the sequence. The loss on this complete output is used as the cost for the deviating action. Run in this way, AGGRAVATE requires len\u00d7K roll-outs when the input sentence has len words and each word can take one of K possible labels.\nSince each roll-out takes O(len) time, this can be computationally prohibitive, so we use active learning to reduce the number of roll-outs. We use COAL and a passive learning baseline inside AGGRAVATE on three joint prediction datasets (statistics are in Table 1). As above, we use several mellowness values and the same AUC criteria to select the best learning rate (see Table 2). The results are in Figure 4, and again our recommended mellowness is 0.01.\nOverall, active learning reduces the number of roll-outs required, but the improvements vary on the three datasets. On the Wikipedia data, COAL performs a factor of 4 fewer rollouts to achieve similar performance to passive learning and achieves substantially better test performance. A similar, but less dramatic, behavior arises on the NER task. On the other hand, COAL offers minimal improvement over passive learning on the POS-tagging task. This agrees with our theory and prior empirical results [23], which show that active learning may not always improve upon passive learning."}, {"heading": "8 Proofs", "text": "In this section we provide proofs for the main results, the oracle-complexity guarantee and the generalization and label complexity bounds. We start with some supporting results, including a new uniform freedman-type inequality that may be of independent interest. The proof of this inequality, and the proofs for several other supporting lemmata are deferred to the appendices."}, {"heading": "8.1 Supporting Results", "text": "A deviation bound. For both the computational and statistical analysis of COAL, we require concentration of the square loss functional R\u0302j(\u00b7; y), uniformly over the class G. To describe the result, we introduce the central random variable in the analysis:\nMj(g; y) , Qj(y) [ (g(xj)\u2212 cj(y))2 \u2212 (f?(xj ; y)\u2212 cj(y))2 ] , (15)\nwhere (xj , cj) is the jth example and cost presented to the algorithm andQj(y) \u2208 {0, 1} is the query indicator. For simplicity we often write Mj when the dependence on g and y is clear from context. Let Ej [\u00b7] and Varj [\u00b7] denote the expectation and variance conditioned on all randomness up to and including round j \u2212 1.\nTheorem 9. Let G be a function class with Pdim(G) = d, let \u03b4 \u2208 (0, 1) and define \u03bdn , 324(d log(n) + log(8Ke(d+ 1)n2/\u03b4)). Then with probability at least 1\u2212 \u03b4, the following inequalities hold simultaneously for all g \u2208 G, y \u2208 [K], and i < i\u2032 \u2208 [n].\ni\u2032\u2211 j=i Mj(g; y) \u2264 3 2 i\u2032\u2211 j=i EjMj(g; y) + \u03bdn, (16)\n1\n2 i\u2032\u2211 j=i EjMj(g; y) \u2264 i\u2032\u2211 j=i Mj(g; y) + \u03bdn. (17)\nThis result is a uniform Freedman-type inequality for the martingale difference sequence \u2211 iMi \u2212 EiMi. In general, such bounds require much stronger assumptions (e.g., sequential complexity measures [35]) on G than the finite pseudo-dimension assumption that we make. However, by exploiting the structure of our particular martingale, specifically that the dependencies arise only from the query indicator, we are able to establish this type of inequality under weaker assumptions. The result may be of independent interest, but the proof, which is based on arguments from Liang et al. [29], is quite technical and deferred to Appendix A. Note that we did not optimize the constants.\nThe Multiplicative Weights Algorithm. We also use the standard analysis of multiplicative weights for solving linear feasibility problems. We state the result here and, for completeness, provide a proof in Appendix B. See also Arora et al. [3], Plotkin et al. [34] for more details.\nConsider a linear feasibility problem with decision variable v \u2208 Rd, explicit constraints \u3008ai, v\u3009 \u2264 bi for i \u2208 [m] and some implicit constraints v \u2208 S (e.g., v is non-negative or other simple constraints). The MW algorithm either finds an approximately feasible point or certifies that the program is infeasible assuming access to an oracle that can solve a simpler feasibility problem with just one explicit constraint\u2211 i \u00b5i\u3008ai, v\u3009 \u2264 \u2211 i \u00b5ibi for any non-negative weights \u00b5 \u2208 Rm+ and the implicit constraint v \u2208 S. Specifically, given weights \u00b5, the oracle either reports that the simpler problem is infeasible, or returns any feasible point v that further satisfies \u3008ai, v\u3009 \u2212 bi \u2208 [\u2212\u03c1i, \u03c1i] for parameters \u03c1i that are known to the MW algorithm.\nThe MW algorithm proceeds iteratively, maintaining a weight vector \u00b5(t) \u2208 Rm+ over the constraints. Starting with \u00b5(1)i = 1 for all i, at each iteration, we query the oracle with the weights \u00b5\n(t) and the oracle either returns a point vt or detects infeasibility. In the latter case, we simply report infeasibility and in the former, we update the weights using the rule\n\u00b5 (t+1) i \u2190 \u00b5 (t) i \u00d7\n( 1\u2212 \u03b7 bi \u2212 \u3008ai, vt\u3009\n\u03c1i\n) .\nHere \u03b7 is a parameter of the algorithm. The intuition is that if vt satisfies the ith constraint, then we downweight the constraint, and conversely, we up-weight every constraint that is violated. Running the algorithm with appropriate choice of \u03b7 and for enough iterations is guaranteed to approximately solve the feasibility problem.\nTheorem 10 (Arora et al. [3], Plotkin et al. [34]). Consider running the MW algorithm with parameter \u03b7 = \u221a log(m)/T for T iterations on a linear feasibility problem where oracle responses satisfy \u3008ai, v\u3009\u2212 bi \u2208 [\u2212\u03c1i, \u03c1i]. If the oracle fails to find a feasible point in some iteration, then the linear program is infeasible. Otherwise the point v\u0304 , 1T \u2211T t=1 vt satisfies \u3008ai, v\u0304\u3009 \u2264 bi + 2\u03c1i \u221a log(m)/T for all i \u2208 [m].\nOther Lemmata. Our first lemma evaluates the conditional expectation and variance ofMj , defined in (15), which we will use heavily in the proofs. Proofs of the results stated here are deferred to Appendix C.\nLemma 2 (Bounding variance of regression regret). We have for all (g, y) \u2208 G \u00d7 Y , Ej [Mj ] = Ej [ Qj(y)(g(xj)\u2212 f?(xj ; y))2 ] , Var\nj [Mj ] \u2264 4Ei[Mj ].\nThe next lemma relates the cost-sensitive error to the random variables Mj . Define Fi = { f \u2208 GK | \u2200y, f(\u00b7; y) \u2208 Gi(y) } ,\nwhich is the version space of vector regressors at round i. Additionally, recall that P\u03b6 captures the noise level in the problem, defined in (10) and that \u03c8i = 1/ \u221a i is defined in the algorithm pseudocode.\nLemma 3. For all i > 0, if f? \u2208 Fi, then for all f \u2208 Fi\nEx,c[c(hf (x))\u2212 c(hf?(x))] \u2264 min \u03b6>0\n{ \u03b6P\u03b6 + 1 (\u03b6 \u2264 2\u03c8i) 2\u03c8i +\n4\u03c82i \u03b6 + 6 \u03b6 \u2211 y Ei [Mi]\n} .\nNote that the lemma requires that both f? and f belong to the version space Fi. For the label complexity analysis, we will need to understand the cost-sensitive performance of all f \u2208 Fi, which requires a different generalization bound. Since the proof is similar to that of Theorem 3, we defer the argument to appendix.\nLemma 4. Assuming the bounds in Theorem 9 hold, then for all i,Fi \u2282 Fcsr(ri) where ri , min\u03b6>0 { \u03b6P\u03b6 + 44K\u2206i \u03b6 } .\nThe final lemma relates the query rule of COAL to a hypothetical query strategy driven by Fcsr(ri), which we will subsequently bound by the disagreement coefficients. Let us fix the round i and introduce the shorthand \u03b3\u0302(xi, y) = c\u0302+(xi, y)\u2212 c\u0302\u2212(xi, y), where c\u0302+(xi, y) and c\u0302\u2212(xi, y) are the approximate maximum and minimum costs computed in Algorithm 1 on the ith example, which we now call xi. Moreover, let Yi be the set of non-dominated labels at round i of the algorithm, which in the pseudocode we call Y \u2032. Formally, Yi = {y | c\u0302\u2212(xi, y) \u2264 miny\u2032 c\u0302+(xi, y\u2032)}. Finally recall that for a set of vector regressors F \u2282 F , we use \u03b3(x, y, F ) to denote the cost range for label y on example x witnessed by the regressors in F .\nLemma 5. Suppose that the conclusion of Lemma 4 holds. Then for any example xi and any label y at round i, we have\n\u03b3\u0302(xi, y) \u2264 \u03b3(xi, y,Fcsr(ri)) + \u03c8i.\nFurther, with y?i = argminy f ?(xi; y), y\u0304i = argminy c\u0302+(xi, y), and y\u0303i = argminy 6=y?i c\u0302\u2212(xi, y),\ny 6= y?i \u2227 y \u2208 Yi \u21d2 f?(xi; y)\u2212 f?(xi; y?i ) \u2264 \u03b3(xi, y,Fcsr(ri)) + \u03b3(xi, y?i ,Fcsr(ri)) + \u03c8i/2, |Yi| > 1 \u2227 y?i \u2208 Yi \u21d2 f?(xi; y\u0303i)\u2212 f?(xi; y?i ) \u2264 \u03b3(xi, y\u0303i,Fcsr(ri)) + \u03b3(xi, y?i ,Fcsr(ri)) + \u03c8i/2."}, {"heading": "8.2 Proof of Theorem 1", "text": "The proof is based on expressing the optimization problem (7) as a linear optimization in the space of distributions over G. Then, we use binary search to re-formulate this as a series of feasibility problems and apply Theorem 10 to each of these.\nRecall that the problem of finding the maximum cost for an (x, y) pair is equivalent to solving the program (7) in terms of the optimal g. For the problem (7), we further notice that since G is a convex set, we can instead write the minimization over g as a minimization over P \u2208 \u2206(G) without changing the optimum, leading to the modified problem (8).\nThus we have a linear program in variable P , and Algorithm 2 turns this into a feasibility problem by guessing the optimal objective value and refining the guess using binary search. For each induced feasibility problem, we use MW to certify feasibility. Let c \u2208 [0, 1] be some guessed upper bound on the objective, and let us first turn to the MW component of the algorithm. The program in consideration is\n?\u2203P \u2208 \u2206(G) s.t. Eg\u223cP (g(xi)\u2212 1)2 \u2264 c and \u2200j \u2208 [i],Eg\u223cP R\u0302j(g; y) \u2264 \u2206\u0303j . (18)\nThis is a linear feasibility problem in the infinite dimensional variable P , with i + 1 constraints. Given a particular set of weights \u00b5 over the constraints, it is clear that we can use the regression oracle over g to compute\ng\u00b5 = arg min g\u2208G \u00b50(g(xi)\u2212 1)2 + \u2211 j\u2208[i] \u00b5jEg\u223cP R\u0302j(g; y). (19)\nObserve that solving this simpler program provides one-sided errors. Specifically, if the objective of (19) evaluated at g\u00b5 is larger than \u00b50c + \u2211 j\u2208[i] \u00b5j\u2206\u0303j then there cannot be a feasible solution to problem (18), since the weights \u00b5 are all non-negative. On the other hand if g\u00b5 has small objective value it does not imply that g\u00b5 is feasible for the original constraints in (18).\nAt this point, we would like to invoke the MW algorithm, and specifically Theorem 10, in order to find a feasible solution to (18) or to certify infeasibility. Invoking the theorem requires the \u03c1j parameters which specify how badly g\u00b5 might violate the jth constraint. For us, \u03c1j , \u03ba suffices since R\u0302j(g; y)\u2212 R\u0302j(gj,y; y) \u2208 [0, 1] (since gj,y is the ERM) and \u2206j \u2264 \u03ba. Since \u03ba \u2265 2 this also suffices for the cost constraint.\nIf at any iteration, MW detects infeasibility, then our guessed value c for the objective is too small since no function satisfies both (g(xi)\u2212 1)2 \u2264 c and the empirical risk constraints in (18) simultaneously. In this case, in Line 10 of Algorithm 2, our binary search procedure increases our guess for c. On the other hand, if we apply MW for T iterations and find a feasible point in every round, then, while we do not have a point that is feasible for the original constraints in (18), we will have a distribution PT such that\nEPT (g(xi)\u2212 1)2 \u2264 c+ 2\u03ba \u221a log(i+ 1)\nT and \u2200j \u2208 [i],EPT R\u0302j(g; y) \u2264 \u2206\u0303j + 2\u03ba\n\u221a log(i+ 1)\nT .\nWe will set T toward the end of the proof. If we do find an approximately feasible solution, then we reduce c and proceed with the binary search. We terminate when ch \u2212 c` \u2264 \u03c42/2 and we know that problem (18) is approximately feasible with ch and infeasible with c`. From ch we will construct a strictly feasible point, and this will lead to a bound on the true maximum c+(x, y,Gi).\nLet P\u0304 be the approximately feasible point found when running MW with the final value of ch. By Jensen\u2019s inequality and convexity of G, there exists a single regressor that is also approximately feasible, which we denote g\u0304. Observe that g? satisfies all constraints with strict inequality, since by (20) we know\nthat R\u0302j(g?; y)\u2212 R\u0302j(gj,y; y) \u2264 \u2206j/\u03ba < \u2206j . We create a strictly feasible point g\u03b6 by mixing g\u0304 with g? with proportion 1\u2212 \u03b6 and \u03b6 for\n\u03b6 = 4\u03ba\n\u2206i\n\u221a log(i+ 1)\nT ,\nwhich will be in [0, 1] when we set T . Combining inequalities, we get that for any j \u2208 [i]\n(1\u2212 \u03b6)R\u0302j(g\u0304; y) + \u03b6R\u0302j(g?; y) \u2264 R\u0302j(gj,y; y) + (1\u2212 \u03b6) ( \u2206j + 2\u03ba \u221a log(i+ 1)\nT\n) + \u03b6 ( \u2206j \u03ba )\n\u2264 R\u0302j(gj,y; y) + \u2206j \u2212\n( \u03b6\u2206j(\u03ba\u2212 1)\n\u03ba \u2212 2\u03ba\n\u221a log(i+ 1)\nT ) \u2264 R\u0302j(gj,y; y) + \u2206j ,\nand hence this mixture regressor g\u03b6 is exactly feasible. Here we use that \u03ba \u2265 2 and that \u2206i is monotonically decreasing. With the pessimistic choice g?(xi) = 0, the objective value for g\u03b6 is at most\n(g\u03b6(xi)\u2212 1)2 \u2264 (1\u2212 \u03b6)(g\u0304(xi)\u2212 1)2 + \u03b6(g?(xi)\u2212 1)2 \u2264 (1\u2212 \u03b6) ( ch + 2\u03ba \u221a log(i+ 1)\nT\n) + \u03b6\n\u2264 c` + \u03c42/2 + ( 2\u03ba+ 4\u03ba\n\u2206i\n)\u221a log(i+ 1)\nT .\nThus g\u03b6 is exactly feasible and achieves the objective value above, which provides an upper bound on the maximum cost. On the other hand c` provides a lower bound. Our setting of T = log(i+1)(8\u03ba2/\u2206i) 2\n\u03c44\nensures that that this excess term is at most \u03c42, since \u2206i \u2264 1. Note that since \u03c4 \u2264 [0, 1], this also ensures that \u03b6 \u2208 [0, 1]. With this choice of T , we know that c` \u2264 (c+(y) \u2212 1)2 \u2264 c` + \u03c42, which implies that c+(y) \u2208 [1\u2212 \u221a c` + \u03c42, 1\u2212 \u221a c`]. Since \u221a c` + \u03c42 \u2264 \u221a c` + \u03c4 , we obtain the guarantee.\nAs for the oracle complexity, since we start with c` = 0 and ch = 1 and terminate when ch \u2212 c` \u2264 \u03c42/2, we performO(log(1/\u03c42)) iterations of binary search. Each iteration requires T = O ( max{1, i2/\u03bd2n} log(i) \u03c44 ) rounds of MW, each of which requires exactly one oracle call. Hence the oracle complexity isO ( max{1, i2/\u03bd2n} log(i) log(1/\u03c4) \u03c44 ) ."}, {"heading": "8.3 Proof of the Generalization Bound", "text": "Recall the central random variable Mj(g; y), defined in (15), which is the excess square loss for function g on label y for the jth example, if we issued a query. The idea behind the proof is to first apply Theorem 9 to argue that all the random variables Mj(g; y) concentrate uniformly over the function class G. Next for a vector regressor f , we relate the cost-sensitive risk to the excess square loss via Lemma 3. Finally, using the fact that gi,y minimizes the empirical square loss at round i, this implies a cost-sensitive risk bound for the vector regressor fi = (gi,y) at round i.\nFirst, condition on the high probability event in Theorem 9, which ensures that the empirical square losses concentrate. We first prove that f? \u2208 Fi for all i \u2208 [n]. At round i, by (17), for each y and for any g we have\n0 \u2264 1 2 i\u2211 j=1 EjMj(g; y) \u2264 i\u2211 j=1 Mj(g; y) + \u03bdn.\nThe first inequality here follows from the fact that EjMj(g; y) is a quadratic form by Lemma 2. Expanding Mj(g; y), this implies that\nR\u0302i+1(f ?(\u00b7; y); y) \u2264 R\u0302i+1(g; y) + \u03bdn i .\nSince this bound applies to all g \u2208 G it proves that f? \u2208 Fi+1 for all i, using the definition of \u2206i and \u03ba. Trivially, we know that f? \u2208 F1. Together with the fact that the losses are in [0, 1] and the definition of \u2206i, the above analysis yields\nR\u0302i(f ?(\u00b7; y); y) \u2264 R\u0302i(g; y) + \u2206i \u03ba . (20)\nThis implies that f?(\u00b7; y) strictly satisfies the inequalities defining the version space, which we used in the MW proof.\nWe next prove that fi+1 \u2208 Fj for all j \u2208 [i]. Fix some label y and to simplify notation, we drop dependence on y. If gi+1 /\u2208 Gt+1 for some t \u2208 {0, . . . , i} then, first observe that we must have t large enough so that \u03bdn/t \u2264 1. In particular, since \u2206t+1 = \u03bamin{1, \u03bdn/t} and we always have R\u0302t+1(gi+1; y) \u2264 R\u0302t+1(gt+1; y) + 1 due to boundedness, we do not evict any functions until \u03bdn/t \u2264 1. For t \u2265 \u03bdn, we get\nt\u2211 j=1 Mj = t ( R\u0302t+1(gi+1)\u2212 R\u0302t+1(g?) ) = t ( R\u0302t+1(gi+1)\u2212 R\u0302t+1(gt+1) + R\u0302t+1(gt+1)\u2212 R\u0302t+1(g?)\n) \u2265 \u03ba\u03bdn \u2212 \u03bdn = (\u03ba\u2212 1)\u03bdn.\nThe inequality uses the radius of the version space and the fact that by assumption gi+1 /\u2208 Gt+1, so the excess empirical risk is at least \u2206t+1 = \u03ba\u03bdn/t since we are considering large t. We also use (20) on the second term. Moreover, we know that since gi+1 is the empirical square loss minimizer for label y after round i, we have\u2211i j=1Mj(gi+1; y) \u2264 0. These two facts together establish that\ni\u2211 j=t+1 Mj(gi+1) \u2264 (1\u2212 \u03ba)\u03bdn.\nHowever, by Theorem 9 on this intermediary sum, we know that\n0 \u2264 1 2 i\u2211 j=t+1 EjMj(gi+1) \u2264 i\u2211 j=t+1 Mj(gi+1) + \u03bdn < (2\u2212 \u03ba)\u03bdn < 0\nusing the definition of \u03ba. This is a contradiction, so we must have that gi+1 \u2208 Gj for all j \u2208 {1, . . . , i}. The same argument applies for all y and hence we can apply Lemma 3 on all rounds to obtain\ni ( Ex,c[c(hfi+1(x))\u2212 c(hf?(x))] ) \u2264 min\n\u03b6>0 i\u03b6P\u03b6 + i\u2211\nj=1\n( 1 (\u03b6 \u2264 2\u03c8j) 2\u03c8j +\n4\u03c82j \u03b6 + 6 \u03b6 \u2211 y Ej [Mj(fi+1; y)] ) .\nWe study the four terms separately. The first one is straightforward and contributes \u03b6P\u03b6 to the instantaneous cost sensitive regret. Using our definition of \u03c8j = 1/ \u221a j the second term can be bounded as\ni\u2211 j=1 1 (\u03b6 < 2\u03c8j) 2\u03c8j = d4/\u03b62e\u2211 j=1 2\u221a j \u2264 4 \u221a d4/\u03b62e \u2264 12 \u03b6 .\nThe inequality above, \u2211n i=1 1\u221a i \u2264 2 \u221a n, is well known. For the third term, using our definition of \u03c8j gives\ni\u2211 j=1 4\u03c82j \u03b6 = 4 \u03b6 i\u2211 j=1 1 j \u2264 4 \u03b6 (1 + log(i)).\nFinally, the fourth term can be bounded using (17), which reveals\ni\u2211 j=1 Ej [Mj ] \u2264 2 i\u2211 j=1 Mj + 2\u03bdn\nSince for each y, \u2211i j=1Mj(fi+1; y) \u2264 0 for the empirical square loss minimizer (which is what we are considering now), we get\n6\n\u03b6 \u2211 y i\u2211 j=1 Ej [Mj(fi+1; y)] \u2264 12 \u03b6 K\u03bdn.\nAnd hence, we obtain the generalization bound\nEx,c[c(x;hfi+1(x))\u2212 c(x;hf?(x))] \u2264 min \u03b6>0\n{ \u03b6P\u03b6 + 1\n\u03b6i (4 log(i) + 16 + 12K\u03bdn) } \u2264 min\n\u03b6>0\n{ \u03b6P\u03b6 +\n32K\u03bdn \u03b6i\n} .\nProof of Corollary 4. Under the Massart noise condition, set \u03b6 = \u03c4 so that P\u03b6 = 0 and we immediately get the result. Proof of Corollary 5. Set \u03b6 = min { \u03c40, ( 32K\u03bdn i\u03b2 ) 1 \u03b1+2 } , so that for i sufficiently large the second term is\nselected and we obtain the bound."}, {"heading": "8.4 Proof of the Label Complexity bounds", "text": "The proof for the label complexity bounds is based on first relating the version space Fi at round i to the cost-sensitive regret ball Fcsr with radius ri. In particular, the containment Fi \u2282 Fcsr(ri) in Lemma 4 implies that our query strategy is more aggressive than the query strategy induced by Fcsr(ri), except for a small error introduced when computing the maximum and minimum costs. This error is accounted for by Lemma 5. Since the probability that Fcsr will issue a query is intimately related to the disagreement coefficient, this argument leads to the label complexity bounds for our algorithm.\nProof of Theorem 6. Fix some round i with example xi, let Fi be the vector regressors used at round i and let Gi(y) be the corresponding regressors for label y. Let y\u0304i = argminy c\u0302+(xi, y), y?i = argminy f?(xi; y), and y\u0303i = argminy 6=y?i c\u0302\u2212(xi, y). Assume that Lemma 4 holds. The L2 label complexity is\u2211\ny Qi(y) = \u2211 y 1{|Yi| > 1 \u2227 y \u2208 Yi}1{\u03b3\u0302(xi, y) > \u03c8i}\n\u2264 \u2211 y 1{|Yi| > 1 \u2227 y \u2208 Yi}1{\u03b3(xi, y,Fcsr(ri)) > \u03c8i/2}.\nFor the former indicator, observe that y \u2208 Yi implies that there exists a vector regressor f \u2208 Fi \u2282 Fcsr(ri) such that hf (xi) = y. This follows since the domination condition means that there exists g \u2208 Gi(y) such that g(xi) \u2264 miny\u2032 maxg\u2032\u2208Gi(y\u2032) g\u2032(xi). Since we are using a factored representation, we can take f to use g on the yth coordinate and use the maximizers for all the other coordinates. Similarly, there exists another regressor f \u2032 \u2208 Fi such that hf \u2032(xi) 6= y. Thus this indicator can be bounded by the disagreement coefficient\u2211\ny Qi(y) \u2264 \u2211 y 1{\u2203f, f \u2032 \u2208 Fcsr(ri) | hf (xi) = y 6= hf \u2032(xi)}1{\u03b3(xi, y,Fcsr(ri)) > \u03c8i/2}\n= \u2211 y 1{x \u2208 DIS(ri, y) \u2227 \u03b3(xi, y,Fcsr(ri)) \u2265 \u03c8i/2}.\nWe will now apply Freedman\u2019s inequality on the sequence { \u2211 y Qi(y)}ni=1, which is a martingale with range K. Moreover, due to non-negativity, the conditional variance is at most K times the conditional mean, and in such cases, Freedman\u2019s inequality reveals that with probability at least 1\u2212 \u03b4\nX \u2264 EX + 2 \u221a REX log(1/\u03b4) + 2R log(1/\u03b4) \u2264 2EX + 3R log(1/\u03b4),\nwhere X is the non-negative martingale with range R and expectation EX . The last step is by the fact that 2 \u221a ab \u2264 a+ b. For us, Freedman\u2019s inequality implies that with probability at least 1\u2212 \u03b4/2\nn\u2211 i=1 \u2211 y Qi(y) \u2264 2 \u2211 i Ei \u2211 y Qi(y) + 3K log(2/\u03b4)\n\u2264 2 \u2211 i Ei \u2211 y 1{x \u2208 DIS(ri, y) \u2227 \u03b3(xi, y,Fcsr(ri)) \u2265 \u03c8i/2}+ 3K log(2/\u03b4)\n\u2264 4 \u2211 i ri \u03c8i \u03b82 + 3K log(2/\u03b4).\nThe last step here uses the definition of the disagreement coefficient \u03b82. To wrap up the proof we just need to upper bound the sequence, using our choices of \u03c8i = 1/ \u221a i, ri = 2 \u221a 44K\u2206i, and \u2206i = \u03bamin{1, \u03bdni\u22121}. With simple calculations this is easily seen to be at most\n8n\u03b82 \u221a 88K\u03ba\u03bdn + 3K log(2/\u03b4).\nSimilarly for L1 we can derive the bound L1 \u2264 \u2211 i 1 (\u2203y | \u03b3(xi, y,Fcsr(ri)) \u2265 \u03c8i/2 \u2227 x \u2208 DIS(ri, y)) ,\nand then apply Freedman\u2019s inequality to obtain that with probability at least 1\u2212 \u03b4/2\nL1 \u2264 2 n\u2211 i=1 2ri \u03c8i \u03b81 + 3 log(2/\u03b4) \u2264 8\u03b81n \u221a 88K\u03ba\u03bdn + 3 log(2/\u03b4).\nProof of Theorem 7. Using the same notations as in the bound for the high noise case we first express the L2 label complexity as \u2211\ny Qi(y) = \u2211 y 1{|Yi| > 1, y \u2208 Yi}Qi(y).\nWe need to do two things with the first part of the query indicator, so we have duplicated it here. For the second, we will use the derivation above to relate the query rule to the disagreement region. For the first, by Lemma 5, for y 6= y?i , we can derive the bound\n1{f?(xi; y)\u2212 f?(xi; y?i ) \u2264 \u03b3(xi, y,Fcsr(ri)) + \u03b3(xi, y?i ,Fcsr(ri)) + \u03c8i/2} \u2264 1{\u03c4 \u2212 \u03c8i/2 \u2264 \u03b3(xi, y,Fcsr(ri)) + \u03b3(xi, y?i ,Fcsr(ri))}.\nFor y = y?i , we get the same bound but with y\u0303i, also by Lemma 5. Focusing on just one of these terms, say where y 6= y?i and any round where \u03c4 \u2265 \u03c8i, we get\n1{\u03c4 \u2212 \u03c8i/2 \u2264 \u03b3(xi, y,Fcsr(ri)) + \u03b3(xi, y?i ,Fcsr(ri))}Qi(y) \u2264 1{\u03c4/2 \u2264 \u03b3(xi, y,Fcsr(ri)) + \u03b3(xi, y?i ,Fcsr(ri))}Qi(y) \u2264 1{\u03c4/4 \u2264 \u03b3(xi, y,Fcsr(ri))}Qi(y) + 1{\u03c4/4 \u2264 \u03b3(xi, y?i ,Fcsr(ri))}Qi(y) \u2264 Di(y) +Di(y?i ),\nwhere for shorthand we have definedDi(y) , 1{\u03c4/4 \u2264 \u03b3(xi, y,Fcsr(ri))\u2227xi \u2208 DIS(ri, y)}. The derivation for the first term is straightforward. We obtain the disagreement region for y?i since the fact that we query y (i.e. Qi(y)) implies there is f such that hf (xi) = y, so this function witnesses disagreement to y?i .\nThe term involving y?i and y\u0303i is bounded in essentially the same way, since we know that when |Yi| > 1, there exists two function f, f \u2032 \u2208 Fi such that hf (xi) = y\u0303i and hf \u2032(xi) = y?i . In total, we can bound the L2 label complexity at any round i such that \u03c4 \u2265 \u03c8i by\nL2 \u2264 Di(y\u0303i) +Di(y?i ) + \u2211 y 6=y?i (Di(y) +Di(y ? i )) \u2264 KDi(y?i ) + 2 \u2211 y Di(y)\nFor the earlier rounds, we simply upper bound the label complexity by K. Since the range of this random variable is at most 3K, using Freedman\u2019s inequality just as in the high noise case, we get that with probability at least 1\u2212 \u03b4/2\nL2 \u2264 n\u2211 i=1 K1{\u03c4 \u2264 \u03c8i}+ 2 n\u2211 i=2 Ei\n[ KDi(y ? i ) + 2 \u2211 y Di(y) ] + 9K log(2/\u03b4)\n\u2264 Kd1/\u03c42e+ 8 \u03c4 (K\u03b81 + \u03b82) n\u2211 i=2 ri + 9K log(2/\u03b4).\nThe first line here is the application of Freedman\u2019s inequality. In the second, we evaluate the expectation, which we can relate to the disagreement coefficients \u03b81, \u03b82. Moreover, we use the setting \u03c8i = 1/ \u221a i to evaluate the first term. As a technicality, we remove the index i = 1 from the second summation, since we are already accounting for queries on the first round in the first term. The last step is to evaluate the series, for which we use the definition of ri = min\u03b6>0 {\u03b6P\u03b6 + 44K\u2206i/\u03b6} and set \u03b6 = \u03c4 , the Massart noise level. This gives ri = 44K\u2206i/\u03c4 . In total, we get\nL2 \u2264 Kd1/\u03c42e+ 352K\u03ba\u03bdn\n\u03c42 (K\u03b81 + \u03b82) log(n) + 9K log(2/\u03b4).\nAs \u03b82 \u2264 K\u03b81 always, we drop \u03b82 from the above expression to obtain the stated bound. For L1 we use a very similar argument. First, by Lemmas 4 and 5\nL1 \u2264 n\u2211 i=1 1{|Yi| > 1 \u2227 \u2203y \u2208 Yi, \u03b3(xi, y,Fcsr(ri)) \u2265 \u03c8i/2}.\nAgain by Lemma 5, we know that\n|Yi| > 1 \u2227 y \u2208 Yi \u21d2 \u2203f, f \u2032 \u2208 Fcsr(ri), hf (xi) = y 6= hf \u2032(xi).\nMoreover, one of the two classifiers can be f?, and so, when \u03c4 \u2265 \u03c8i, we can deduce\n\u21d2 f?(xi, y)\u2212 f?(xi, y?i ) \u2264 \u03b3(xi, y,Fcsr(ri)) + \u03b3(xi, y?i ,Fcsr(ri)) + \u03c8i/2 \u21d2 \u03c4/4 \u2264 \u03b3(xi, y,Fcsr(ri)) \u2227 \u03c4/4 \u2264 \u03b3(xi, y?i ,Fcsr(ri)).\nCombining this argument, we bound the L1 label complexity as\nL1 \u2264 n\u2211 i=1 1{\u03c4 \u2264 \u03c8i}+ n\u2211 i=2 1{\u2203y | xi \u2208 DIS(Fcsr(ri), y) \u2227 \u03b3(xi, y,Fcsr(ri)) \u2265 \u03c4/4}. (21)\nApplying Freedman\u2019s inequality just as before gives\nL1 \u2264 d1/\u03c42e+ 2 n\u2211 i=2 4ri \u03c4 \u03b81 + 2 log(2/\u03b4) \u2264 d1/\u03c42e+ 352K\u03ba\u03bdn \u03c42 \u03b81 log(n) + 2 log(2/\u03b4),\nwith probability at least 1\u2212 \u03b4/2.\nProof of Theorem 8. For the Tsybakov case, the same argument as in the Massart case gives that with probability at least 1\u2212 \u03b4/2\nL2 \u2264 Kd1/\u03c42e+ 8\n\u03c4 (K\u03b81 + \u03b82) n\u2211 i=2 ri + 2n\u03b2\u03c4 \u03b1 + 9K log(2/\u03b4).\nThe main difference here is the term scaling with n\u03c4\u03b1 which arises since we do not have the deterministic bound f?(xi, y) \u2212 f?(xi, y?i ) \u2265 \u03c4 as we used in the Massart case, but rather this happens except with probability \u03b2\u03c4\u03b1 (provided \u03c4 \u2264 \u03c40). Now we must optimize \u03b6 in the definition of ri and then \u03c4 .\nFor \u03b6 the optimal setting is (44K\u2206i/\u03b2) 1 \u03b1+2 which gives ri \u2264 2\u03b2 1 \u03b1+2 (44K\u2206i) \u03b1+1 \u03b1+2 . Since we want to set \u03b6 \u2264 \u03c40, this requires i \u2265 1 + 44K\u03ba\u03bdn\u03b2\u03c4\u03b1+20 . For these early rounds we will simply pay K in the label complexity, but this will be dominated by other higher order terms. For the later rounds, we get\nn\u2211 i=2 ri \u2264 2\u03b2 1 \u03b1+2 (44K\u03ba\u03bdn) \u03b1+1 \u03b1+2 n\u2211 i=2 ( 1 i\u2212 1 )\u03b1+1 \u03b1+2 \u2264 2(\u03b1+ 2) (44K\u03ba\u03bdn) \u03b1+1 \u03b1+2 (\u03b2n) 1 \u03b1+2 .\nThis bound uses the integral approximation \u2211n i=2(i\u2212 1) \u2212\u03b1+1\u03b1+2 \u2264 1 + \u222b n\u22121 1 x\u2212 \u03b1+1 \u03b1+2 dx \u2264 (\u03b1+ 2)n 1 \u03b1+2 . At this point, the terms involving \u03c4 in our bound are\nKd1/\u03c42e+ 16 \u03c4 (K\u03b81 + \u03b82)(\u03b1+ 2) (44K\u03ba\u03bdn) \u03b1+1 \u03b1+2 (\u03b2n) 1 \u03b1+2 + 2n\u03b2\u03c4\u03b1.\nWe set \u03c4 = (8(\u03b1+ 2)(K\u03b81 + \u03b82)) 1 \u03b1+1 (44K\u03ba\u03bdn) 1 \u03b1+2 (\u03b2n) \u22121 \u03b1+2 by optimizing the second two terms which gives a final bound of\nL2 \u2264 O ( (K\u03b81 + \u03b82) \u03b1 \u03b1+1 (K\u03bdn) \u03b1 \u03b1+2n 2 \u03b1+2 +K log(1/\u03b4) ) .\nThis follows since the 1/\u03c42 term agrees in the n dependence and is lower order in other parameters, while the unaccounted for querying in the early rounds is independent of n. The bound of course requires that \u03c4 \u2264 \u03c40, which again requires n large enough. Note we are treating \u03b1 and \u03b2 as constants and we drop \u03b82 from the final statement.\nThe L1 bound requires only slightly different calculations. Following the derivation for the Massart case, we get\nL1 \u2264 d1/\u03c42e+ 8\u03b81 \u03c4 n\u2211 i=2 ri + 2n\u03b2\u03c4 \u03b1 + 2 log(2/\u03b4)\n\u2264 d1/\u03c42e+ 16\u03b81(\u03b1+ 2) \u03c4 (44K\u03ba\u03bdn) \u03b1+1 \u03b1+2 (\u03b2n) 1 \u03b1+2 + 2n\u03b2\u03c4\u03b1 + 2 log(2/\u03b4),\nnot counting the lower order term for the querying in the early rounds. Here we set \u03c4 = (8(\u03b1+2)\u03b81) 1 \u03b1+1 (44K\u03ba\u03bdn) 1 \u03b1+2 (\u03b2n) \u22121 \u03b1+2 to obtain\nL1 \u2264 O ( \u03b8 \u03b1 \u03b1+1 1 (K\u03bdn) \u03b1 \u03b1+2n 2 \u03b1+2 + log(1/\u03b4) ) .\nProof of Proposition 1: Massart Case. Observe that with Massart noise, we have Fcsr(r) \u2282 F\u0303(r/\u03c4), which implies that\nE1 {\u2203y | x \u2208 DIS(r, y) \u2227 \u03b3(x, y,Fcsr(r)) \u2265 \u03c4/4} \u2264 E1 { x \u2208 D\u0303IS(r/\u03c4) } \u2264 r \u03c4 sup r>0 1 r E1 { x \u2208 D\u0303IS(r) } .\nThus we may replace \u03b81 with \u03b80 in the proof of the L1 label complexity bound above.\nProof of Proposition 1: Tsybakov Case. The proof is identical to Theorem 8 except we must introduce the alternative coefficient \u03b80. To do so, define X (\u03c4) , {x \u2208 X : miny 6=y?(x) f?(x, y)\u2212 f?(x, y?(x)) \u2265 \u03c4}, and note that under the Tsybakov noise condition, we have P[x /\u2208 X (\u03c4)] \u2264 \u03b2\u03c4\u03b1 for all 0 \u2264 \u03c4 \u2264 \u03c40. For such a value of \u03c4 we have\nP[hf (x) 6= hf?(x)] \u2264 E1{x \u2208 X (\u03c4) \u2227 hf (x) 6= hf?(x)}+ P[x /\u2208 X (\u03c4)]\n\u2264 1 \u03c4 Ec(hf (x))\u2212 c(hf?(x)) + \u03b2\u03c4\u03b1.\nWe use this fact to prove that Fcsr(r) \u2282 F\u0303(2r/\u03c4) for \u03c4 sufficiently small. This can be seen from above by noting that if f \u2208 Fcsr(r) then the right hand side is r\u03c4 + \u03b2\u03c4 \u03b1, and if \u03c4 \u2264 (r/\u03b2) 1\n1+\u03b1 the containment holds. Therefore, we have\nE1 {\u2203y | x \u2208 DIS(r, y) \u2227 \u03b3(x, y,Fcsr(r)) \u2265 \u03c4/4} \u2264 E1 {\u2203y | x \u2208 DIS(r, y)} \u2264 E1 { x \u2208 D\u0303IS(2r/\u03c4) } \u2264 2r\n\u03c4 \u00b7 sup r>0\nE1 { x \u2208 D\u0303IS(r) } .\nThus, provided \u03c4 \u2264 (rn/\u03b2) 1 \u03b1+1 , we can replace \u03b81 with \u03b80 in the above argument. This gives\nL1 \u2264 d1/\u03c42e+ 4\u03b80 \u03c4 n\u2211 i=2 ri + 2n\u03b2\u03c4 \u03b1 + 2 log(2/\u03b4)\n\u2264 d1/\u03c42e+ 8\u03b80(\u03b1+ 2) \u03c4 (44K\u03ba\u03bdn) \u03b1+1 \u03b1+2 (\u03b2n) 1 \u03b1+2 + 2n\u03b2\u03c4\u03b1 + 2 log(2/\u03b4).\nAs above, we have taken ri = 2\u03b2 1 \u03b1+2 (44K\u2206i) \u03b1+1 \u03b1+2 and approximated the sum by an integral. Since \u2206n = \u03ba\u03bdn/(n\u2212 1), we can set \u03c4 = 2 \u03b1+1 \u03b1+2 (44K\u03ba\u03bdn) 1 \u03b1+2 (\u03b2n)\u2212 1 \u03b1+2 . This is a similar choice to what we used in the proof of Theorem 8 except that we are not incorporating \u03b80 into the choice of \u03c4 , and it yields a final bound of O ( \u03b80n 2 \u03b1+2 \u03bd \u03b1 \u03b1+2 n + log(1/\u03b4) ) .\nProof of Proposition 2. First we relate \u03b81 to \u03b80 in the multiclass case. For f \u2208 Fcsr(r), we have\nP[hf (x) 6= hf?(x)] \u2264 P[hf (x) 6= y] + P[hf?(x) 6= y] = Ec(hf (x))\u2212 c(hf?(x)) \u2264 2error(hf?) + r\nTherefore for any r > 0 and any x,\n1{\u2203y | x \u2208 DIS(r, y) \u2227 \u03b3(x, y,Fcsr(r)) \u2265 \u03c8/2} \u2264 1{x \u2208 D\u0303IS(ri + 2error(hf?))}.\nApplying this argument in the L1 derivation above, we obtain L1 \u2264 2 \u2211 i Ei1{x \u2208 D\u0303IS(ri + 2error(hf?))}+ 3 log(2/\u03b4)\n\u2264 2 \u2211 i (ri + 2error(hf?))\u03b80 + 3 log(2/\u03b4).\nWe now bound ri via Lemma 4. In multiclass classification, the fact that c = 1\u2212 ey for some y implies that f?(x, y) is one minus the probability that the true label is y. Thus f?(x, y) \u2208 [0, 1], \u2211 y f\n?(x, y) = K \u2212 1, and for any x we always have\nmin y 6=y?(x)\nf?(x, y)\u2212 f?(x, y?(x)) \u2265 1\u2212 2f?(x, y?(x)).\nHence, we may bound P\u03b6 , for \u03b6 \u2264 1/2, as follows\nP\u03b6 = Px\u223cD [\nmin y 6=y?(x)\nf?(x, y)\u2212 f?(x, y?(x)) \u2264 \u03b6 ] \u2264 Px\u223cD [1\u2212 2f?(x, y?(x)) \u2264 \u03b6]\n\u2264 Exf?(x, y?(x)) \u00b7 2\n1\u2212 \u03b6 \u2264 4error(hf?).\nNow apply Lemma 4 with \u03b6 = min{1/2, \u221a 44K\u2206i/error(hf?)}, and we obtain\nri \u2264 \u221a 44K\u2206ierror(hf?) + 264K\u2206i.\nUsing the definition of \u2206i the final L1 label complexity bound is L1 \u2264 4\u03b80n \u00b7 error(hf?) +O ( \u03b80 (\u221a Kn\u03ba\u03bdn \u00b7 error(hf?) +K\u03ba\u03bdn log(n) ) + log(1/\u03b4) ) ."}, {"heading": "9 Discussion", "text": "This paper presents a new active learning algorithm for cost-sensitive multiclass classification. The algorithm enjoys strong theoretical guarantees on running time, generalization error, and label complexity. The main algorithmic innovation is a new way to compute the maximum and minimum costs predicted by a regression function in the version space. We also design an online algorithm inspired by our theoretical analysis that outperforms passive baselines both in CSMC and structured prediction.\nOn a technical level, our algorithm uses a square loss oracle to search the version space and drive the query strategy. This contrasts with many recent results using argmax or 0/1-loss minimization oracles for information acquisition problems like contextual bandits [2]. As these involve NP-hard optimizations in general, an intriguing question is whether we can use a square loss oracle for other information acquisition problems. We hope to answer this question in future work."}, {"heading": "Acknowledgements", "text": "Part of this research was completed while TKH was at Microsoft Research and AK was at University of Massachusetts, Amherst. AK thanks Chicheng Zhang for insightful conversations. AK is supported in part by NSF Award IIS-1763618."}, {"heading": "A Proof of Theorem 9", "text": "In the proof, we mostly work with the empirical `1 covering number for G. At the end, we translate to pseudo-dimension using a lemma of Haussler.\nDefinition 3 (Covering numbers). Given class G \u2282 X \u2192 R, \u03b1 > 0, and sample X = (x1, . . . , xn) \u2208 Xn, the covering numberN1(\u03b1,H, X) is the minimum cardinality of a set V \u2282 Rn such that for any g \u2208 G, there exists a (v1, . . . , vn) \u2208 V with 1n \u2211n i=1 |g(xi)\u2212 vi| \u2264 \u03b1.\nLemma 6 (Covering number and Pseudo-dimension [22]). Given a hypothesis class G \u2282 X \u2192 R with Pdim(G) \u2264 d, for any X \u2208 Xn we have\nN1(\u03b1,G, X) \u2264 e(d+ 1) ( 2e\n\u03b1\n)d .\nFixing i and y, and working toward (16) we seek to bound\nP sup g\u2208G i\u2211 j=1 Mj(g; y)\u2212 3 2 Ej [Mj(g; y)] \u2265 \u03c4  . (22) The bound on the other tail is similar. In this section, we sometimes treat the query rule as a function which maps an x to a query decision. We use the notation Qj : X \u2192 {0, 1} to denote the query function used after seeing the first j \u2212 1 examples. Thus, our query indicator Qj is simply the instantiation Qj(xj). In this section, we work with an individual label y and omit the explicit dependence in all our arguments and notation. For notational convenience, we use zj = (xj , cj , Qj) and with g?(\u00b7) = f?(\u00b7; y), we define\n\u03bej = g ?(xj)\u2212 cj and `(g, x) = (g(x)\u2212 g?(x)). (23)\nNote that \u03bej is a centered random variable, independent of everything else. We now introduce some standard concepts from martingale theory for the proof of Theorem 9.\nDefinition 4 (Tangent sequence). For a dependent sequence z1, . . . , zi we use z\u20321, . . . , z\u2032i to denote a tangent sequence, where z\u2032j |z1:j\u22121 d = zj |z1:j\u22121, and, conditioned on z1, . . . , zi, the random variables z\u20321, . . . , z\u2032i are independent.\nIn fact, in our case we have z\u2032j = (x \u2032 j , c \u2032 j , Q \u2032 j) where (x \u2032 j , c \u2032 j) \u223c D and Q\u2032j : X \u2192 {0, 1} is identical to Qj . We use M \u2032j(g) to denote the empirical excess square loss on sample z \u2032 j . Note that we will continue to use our previous notation of Ej to denote conditioning on z1, . . . , zj\u22121. We next introduce one more random process, and then proceed with the proof.\nDefinition 5 (Tree process). A tree process Q is a binary tree of depth i where each node is decorated with a value from {0, 1}. For a Rademacher sequence \u2208 {\u22121, 1}i we use Qi( ) to denote the value at the node reached when applying the actions 1, . . . , i\u22121 from the root, where +1 denotes left and \u22121 denotes right.\nThe proof follows a fairly standard recipe for proving uniform convergence bounds, but has many steps that all require minor modifications from standard arguments. We compartmentalize each step in various lemmata:\n1. In Lemma 7, we introduce a ghost sample and replace the conditional expectation Ej [\u00b7] in (22) with an empirical term evaluated on an tangent sequence.\n2. In Lemma 8, we perform symmetrization and introduce Rademacher random variables and the associated tree process.\n3. In Lemma 9 we control the symmetrized process for finite G.\n4. In Lemma 10, we use the covering number to discretize G.\nWe now state and prove the intermediate results.\nLemma 7 (Ghost sample). Let Z = (z1, . . . , zi) be the sequence of (x, c,Q) triples and let Z \u2032 = (z\u20321, . . . , z\u2032i) be a tangent sequence. Then for \u03b20 \u2265 \u03b21 > 0 if \u03c4 \u2265 4(1+\u03b21) 2\n(\u03b20\u2212\u03b21) , then\nPZ sup g\u2208G i\u2211 j=1 Mj(g)\u2212 (1 + 2\u03b20)EjMj(g) \u2265 \u03c4  \u2264 2PZ,Z\u2032 sup g\u2208G i\u2211 j=1 Mj(g)\u2212M \u2032j(g)\u2212 2\u03b21Qj(x\u2032j)`2(g, x\u2032j) \u2265 \u03c4 2\n , PZ sup g\u2208G i\u2211 j=1 (1\u2212 2\u03b20)EjMj(g)\u2212Mj(g) \u2265 \u03c4\n \u2264 2PZ,Z\u2032 sup g\u2208G i\u2211 j=1 M \u2032j(g)\u2212Mj(g)\u2212 2\u03b21Qj(x\u2032j)`2(g, x\u2032j) \u2265 \u03c4 2\n . Proof. We derive the first inequality, beginning with the right hand side and working toward a lower bound. The main idea is to condition on Z and just work with the randomness in Z \u2032. To this end, let g\u0302 achieve the supremum on the left hand side, and define the events\nE =  i\u2211\nj=1\nMj(g\u0302)\u2212 (1 + 2\u03b20)EjMj(g\u0302) \u2265 \u03c4  E\u2032 =\n{ n\u2211 i=1 M \u2032j(g\u0302) + 2\u03b21Qj(x \u2032 j)` 2(g\u0302, x\u2032j)\u2212 (1 + 2\u03b20)EjM \u2032j(g\u0302) \u2264 \u03c4/2 } .\nStarting from the right hand side, by adding and subtracting (1 + 2\u03b20)EjMj(g\u0302) we get\nPZ,Z\u2032 sup g\u2208G i\u2211 j=1 Mj(g)\u2212M \u2032j(g)\u2212 2\u03b21Qj(x\u2032j)`2(g, x\u2032j) \u2265 \u03c4/2  \u2265 PZ,Z\u2032\n i\u2211 j=1 Mj(g\u0302)\u2212M \u2032j(g\u0302)\u2212 2\u03b21Qj(x\u2032j)`2(g\u0302, x\u2032j) \u2265 \u03c4/2  \u2265 PZ,Z\u2032 [ E \u22c2 E\u2032 ] = EZ [1{E} \u00d7 PZ\u2032 [E\u2032|Z]] .\nSince we have defined g\u0302 to achieve the supremum, we know that\nEZ1{E} = PZ sup g\u2208G i\u2211 j=1 Mj(g)\u2212 (1 + 2\u03b20)EjMj(g) \u2265 \u03c4  . which is precisely the left hand side of the desired inequality. Hence we need to bound the PZ\u2032 [E\u2032|Z] term. For this term, we note that\nPZ\u2032 [E\u2032|Z] = PZ\u2032 [\nn\u2211 i=1 M \u2032j(g\u0302) + 2\u03b21Qj(x \u2032 j)` 2(g\u0302, x\u2032j)\u2212 (1 + 2\u03b20)EjM \u2032j(g\u0302) \u2264 \u03c4/2 | Z ] (a) = PZ\u2032\n[ n\u2211 i=1 M \u2032j(g\u0302) + 2\u03b21EjM \u2032j(g\u0302)\u2212 (1 + 2\u03b20)EjM \u2032j(g\u0302) \u2264 \u03c4/2 | Z ]\n= PZ\u2032 [\nn\u2211 i=1 ( M \u2032j(g\u0302)\u2212 EjM \u2032j(g\u0302) ) + 2 ( \u03b21 \u2212 \u03b20)EjM \u2032j(g\u0302) ) \u2264 \u03c4/2 | Z\n] .\nHere, (a) follows since Ej [M \u2032j(g) | Z] = Qj(x\u2032j)`2(g, x\u2032j) for any g by Lemma 2. Since we are conditioning on Z, g\u0302 is also not a random function and the same equality holds when we take expectation over Z \u2032, even for g\u0302. With this, we can now invoke Chebyshev\u2019s inequality:\nPZ\u2032 [E\u2032|Z] = 1\u2212 PZ\u2032  i\u2211 j=1 M \u2032j(g\u0302) + 2\u03b21Qj(x \u2032 j)` 2(g\u0302, x\u2032j)\u2212 (1 + 2\u03b20)EjM \u2032j(g\u0302) \u2265 \u03c4/2 \u2223\u2223\u2223\u2223\u2223\u2223Z \n\u2265 1\u2212 Var\n[\u2211i j=1M \u2032 j(g\u0302) + 2\u03b21Qj(x \u2032 j)`(g\u0302, x \u2032 j) \u2223\u2223\u2223Z](\n\u03c4/2 + 2(\u03b20 \u2212 \u03b21) \u2211 i Ej [Qj(x\u2032j)`2(g\u0302, x\u2032j) \u2223\u2223Z])2 . Since we are working conditional on Z, we can leverage the independence of Z \u2032 (recall Definition 4) to bound the variance term.\nVar  i\u2211 j=1 M \u2032j(g) + 2\u03b21Qj(x \u2032 j)` 2(g\u0302, x\u2032j) \u2223\u2223\u2223\u2223\u2223\u2223Z  \u2264 i\u2211\nj=1\nEj [ M \u2032j(g\u0302) 2 + (2\u03b21) 2Qj(x \u2032 j)` 4(g\u0302, x\u2032j) + 4\u03b21M \u2032 j(g\u0302)Qj(x \u2032 j)` 2(g\u0302, x\u2032j) ]\n\u2264 4(1 + \u03b21)2 i\u2211\nj=1\nEjQj(x\u2032j)`2(g\u0302, x\u2032j).\nHere we use that Var[X] \u2264 E[X2] and then we use that `2 \u2264 ` since the loss is bounded in [0, 1] along with Lemma 2. Returning to the application of Chebyshev\u2019s inequality, if we expand the quadratic in the denominator and drop all but the cross term, we get the bound\nP\u2032Z [E\u2032|Z] \u2265 1\u2212 2(1 + \u03b21)\n2\n\u03c4(\u03b20 \u2212 \u03b21) \u2265 1/2,\nwhere the last step uses the requirement on \u03c4 . This establishes the first inequality. For the second inequality the steps are nearly identical. Let g\u0302 achieve the supremum on the left hand side and define\nE =  i\u2211\nj=1\n(1\u2212 2\u03b20)EjMj(g\u0302)\u2212Mj(g\u0302) \u2265 \u03c4  E\u2032 =  i\u2211\nj=1\n(1\u2212 2\u03b20)EjM \u2032j(g\u0302)\u2212M \u2032j(g\u0302) + 2\u03b21Qj(x\u2032j)`2(g\u0302, x\u2032j) \u2264 \u03c4/2  . Using the same argument, we can lower bound the right hand side by\nP sup g\u2208G i\u2211 j=1 M \u2032j(g)\u2212Mj(g)\u2212 2\u03b21Qj(x\u2032j)`2(g, x\u2032j) \u2265 \u03c4/2  \u2265 EZ [1{E} \u00d7 PZ\u2032 [E\u2032|Z]] Applying Chebyshev\u2019s inequality yields the same expression as for the other tail.\nLemma 8 (Symmetrization). Using the same notation as in Lemma 7, we have\nPZ,Z\u2032 sup g\u2208G i\u2211 j=1 Mj(g)\u2212M \u2032j(g)\u2212 2\u03b21Qj(x\u2032j)`2(g, x\u2032j) \u2265 \u03c4  \u2264 2E sup\nQ P sup g\u2208G i\u2211 j=1 jQj( )((1 + \u03b21)`(g, xj)2 + 2\u03bej`(g, xj))\u2212 \u03b21Qj( )`(g, xj)2 \u2265 \u03c4/2  . the same bound holds on the lower tail with (1\u2212 \u03b21) replacing (1 + \u03b21).\nProof. For this proof, we think of Qj as a binary variable that is dependent on z1, . . . , zj\u22121 and xj . Similarly Q\u2032j depends on z1, . . . , zj\u22121 and x \u2032 j . Using this notation, and decomposing the square loss, we get\nMj(g) = Qj [ (g(xj)\u2212 cj)2 \u2212 (g?(xj)\u2212 cj)2 ] = Qj` 2(g, xj) + 2Qj\u03bej`(g, xj).\nAs such, we can write\nMj(g)\u2212M \u2032j(g)\u2212 2\u03b21Q\u2032j`2(g, x\u2032j) = (1 + \u03b21)Qj`2(g, xj) + 2Qj\u03bej`(g, xj)\ufe38 \ufe37\ufe37 \ufe38 ,T1,j \u2212\u03b21Qj`2(g, xj)\ufe38 \ufe37\ufe37 \ufe38 ,T2,j\n\u2212 (1 + \u03b21)Q\u2032j`2(g, x\u2032j)\u2212 2Q\u2032j\u03be\u2032j`(g, x\u2032j)\ufe38 \ufe37\ufe37 \ufe38 ,T \u20321,j \u2212\u03b21Q\u2032j`2(g, x\u2032j)\ufe38 \ufe37\ufe37 \ufe38 ,T \u20322,j .\nHere we have introduce the short forms T1,j , T2,j and the primed version just to condense the derivations. Overall we must bound\nP sup g\u2208G i\u2211 j=1 T1,j \u2212 T2,j \u2212 T \u20321,j \u2212 T \u20322,j \u2265 \u03c4  = E1 supg\u2208G i\u2211 j=1 T1,j \u2212 T2,j \u2212 T \u20321,j \u2212 T \u20322,j \u2265 \u03c4  .\nObserve that in the final term T1,i, T \u20321,i are random variables with identical conditional distribution, since there are no further dependencies and (xi, \u03bei, Qi) are identically distributed to (x\u2032i, \u03be \u2032 i, Q \u2032 i). As such, we can symmetrize the ith term by introducing the Rademacher random variable i \u2208 {\u22121,+1} to obtain\nEZ,Z\u2032E i1 supg\u2208G i\u22121\u2211 j=1 T1,j \u2212 T \u20321,j + i(T1,i \u2212 T \u20321,i)\u2212 i\u2211 j=1 (T2,j + T \u2032 2,j) \u2265 \u03c4  \u2264 EZ,Z\u2032 sup\nQi,Q\u2032i\nE i1 supg\u2208G i\u22121\u2211 j=1 T1,j \u2212 T \u20321,j + i(T1,i \u2212 T \u20321,i)\u2212 i\u2211 j=1 (T2,j + T \u2032 2,j) \u2265 \u03c4  . Here in the second step, we have replaced the expectation over Qi, Q\u2032i with supremum, which breaks the future dependencies for the (i \u2212 1)st term. Note that while we still write EZ,Z\u2032 , we are no longer taking expectation over Qi, Q\u2032i here. The important point is that since xj , \u03bej are all i.i.d., the only dependencies in the martingale are through Qjs and by taking supremum over Qi, Q\u2032i, swapping the role of T1,i\u22121 and T \u2032 1,i\u22121 no longer has any future effects. Thus we can symmetrize the (i\u2212 1)st term. Continuing in this way, we get\nEE i\u22121 sup Qi,Q\u2032i E i1 supg\u2208G i\u22122\u2211 j=1 T1,j \u2212 T \u20321,j + i\u2211 j=i\u22121 j(T1,j \u2212 T \u20321,j)\u2212 i\u2211 j=1 (T2,j + T \u2032 2,j) \u2265 \u03c4  \u2264 E\n\u2329 sup Qj ,Q\u2032j E j \u232ai j=1 1 supg\u2208G i\u2211 j=1 j(T1,j \u2212 T \u20321,j)\u2212 i\u2211 j=1 (T2,j + T \u2032 2,j) \u2265 \u03c4  . Here in the final expression the outer expectation is just over the variables xj , x\u2032j , \u03bej , \u03be \u2032 j and the bracket notation denotes interleaved supremum and expectation. Expanding the definitions of T1,i, T2,i, we currently have\nE \u2329 sup Qj ,Q\u2032j E j \u232ai j=1 1 supg\u2208G i\u2211 j=1 j [ (1 + \u03b21)Qj` 2(g, xj) + 2Qj\u03bej`(g, xj) ] \u2212 i\u2211 j=1 \u03b21Qj` 2(g, xj)\n\u2212 i\u2211\nj=1\nj [ (1 + \u03b21)Q \u2032 j` 2(g, x\u2032j) + 2Q \u2032 j\u03be \u2032 j`(g, x \u2032 j) ] \u2212 i\u2211 j=1 \u03b21Q \u2032 j` 2(g, x\u2032j) \u2265 \u03c4  . Next we use the standard trick of splitting the supremum over g into a supremum over two functions g, g\u2032, where g\u2032 optimizes the primed terms. This provides an upper bound, but moreover if we replace \u03c4 with \u03c4/2 we can split the indicator into two and this becomes\n2E \u2329 sup Qj E j \u232ai j=1 1 supg\u2208G i\u2211 j=1 jQj [ (1 + \u03b21)` 2(g, xj) + 2\u03bej`(g, xj) ] \u2212 \u03b21Qj`2(g, xj) \u2265 \u03c4/2  = 2E sup\nQ P sup g\u2208G i\u2211 j=1 jQj( ) [ (1 + \u03b21)` 2(g, xj) + 2\u03bej`(g, xj) ] \u2212 \u03b21Qj( )`2(g, xj) \u2265 \u03c4/2  . The tree process Q arises here because the interleaved supremum and expectation is equivalent to choosing a binary tree decorated with values from {0, 1} and then navigating the tree using the Rademacher random variables . The bound for the other tail is proved in the same way, except (1+\u03b21) is replaced by (1\u2212\u03b21).\nThe next lemma is more standard, and follows from the union bound and the bound on the Rademacher moment generating function.\nLemma 9 (Finite class bound). For any x1:i, \u03be1:i,Q, and for finite G, we have\nP max g\u2208G i\u2211 j=1 jQj( ) [ (1 + \u03b21)` 2(g, xj) + 2\u03bej`(g, xj) ] \u2212 \u03b21Qj( )`2(g, xj) \u2265 \u03c4  \u2264 |G| exp( \u22122\u03b21\u03c4 (3 + \u03b21)2 ) .\nThe same bound applies for the lower tail.\nProof. Applying the union bound and the Chernoff trick, we get that for any \u03bb > 0 the LHS is bounded by\n\u2211 g exp(\u2212\u03c4\u03bb)E exp \u03bb  i\u2211 j=1 jQj( ) [ (1 + \u03b21)` 2(g, xj) + 2\u03bej`(g, xj) ] \u2212 \u03b21Qi( )`2(g, xj)\ufe38 \ufe37\ufe37 \ufe38\n,T3,j\n \n= \u2211 g exp(\u2212\u03c4\u03bb)E 1:i\u22121 exp \u03bb i\u22121\u2211 j=1 T3,j \u00d7 E i| 1:i\u22121 exp(\u03bbT3,i). Let us examine the ith term conditional on 1:i\u22121. Conditionally on 1:i\u22121, Qi( ) is no longer random, so we can apply the MGF bound for Rademacher random variables to get\nE i| 1:i\u22121 exp { \u03bb iQi( ) [ (1 + \u03b21)` 2(g, xi) + 2\u03bei`(g, xi) ] \u2212 \u03bb\u03b21Qi( )`2(g, xi) } \u2264 exp { \u03bb2\n2 Qi( )\n[ (1 + \u03b21)` 2(g, xi) + 2\u03bei`(g, xi) ]2 \u2212 \u03bb\u03b21Qi( )`2(g, xi)}\n\u2264 exp { \u03bb2Qi( ) (3 + \u03b21) 2\n2 `2(g, xi)\u2212 \u03bb\u03b21Qi( )`2(g, xi)\n} \u2264 1.\nHere the first inequality is the standard MGF bound on Rademacher random variables E exp(a ) \u2264 exp(a2/2). In the second line we expand the square and use that `, \u03be \u2208 [\u22121, 1] to upper bound all the terms. Finally, we use the choice of \u03bb = 2\u03b21(3+\u03b21)2 . Repeating this argument from i down to 1, finishes the proof of the upper tail. The same argument applies for the lower tail, but we actually get (3\u2212 \u03b21)2 in the denominator, which is of course upper bounded by (3 + \u03b21)2, since \u03b21 > 0.\nLemma 10 (Discretization). Fix x1, . . . , xi and let V \u2282 Ri be a cover of G at scale \u03b1 on points x1, . . . , xi. Then for any Q\nP sup g\u2208G i\u2211 j=1 jQj( ) [ (1 + \u03b21)` 2(g, xj) + 2\u03bej`(g, xj) ] \u2212 \u03b21Qj( )`2(g, xj) \u2265 \u03c4  \u2264 P sup v\u2208V i\u2211 j=1 jQj( ) [ (1 + \u03b21)` 2(v, xj) + 2\u03bej`(v, xj) ] \u2212 \u03b21Qj( )`2(v, xj) \u2265 \u03c4 \u2212 4i(1 + \u03b2)\u03b1\n . The same bound holds for the lower tail with 1\u2212 \u03b21. Here `(v, xj) = (vj \u2212 g?(xj)).\nProof. Observe first that if v is the covering element for g, then we are guaranteed that\n1\ni i\u2211 j=1 |`(g, xj)\u2212 `(v, xj)| = 1 i i\u2211 j=1 |g(xj)\u2212 vj | \u2264 \u03b1,\n1\ni i\u2211 j=1 |`2(g, xj)\u2212 `2(v, xj)| = 1 i i\u2211 j=1 |(g(xj)\u2212 vj)(g(xj) + vj \u2212 2g?(xj)| \u2264 2\u03b1,\nsince g, v, g? \u2208 [0, 1]. Thus, adding and subtracting the corresponding terms for v, and applying these bounds, we get a residual term of i\u03b1(2(1 + \u03b21) + 2 + 2\u03b21) = 4i\u03b1(1 + \u03b21).\nProof of Theorem 9. Finally we can derive the deviation bound. We first do the upper tail, Mj \u2212 EjMj . Set \u03b20 = 1/4, \u03b21 = 1/8 and apply Lemmas 7 and 8 to (22).\nP sup g\u2208G i\u2211 j=1 Mj(g)\u2212 3 2 EjMj(g) \u2265 \u03c4  \u2264 2P sup g\u2208G i\u2211 j=1 Mj(g)\u2212M \u2032j(g)\u2212 1 4 Qj(x \u2032 j)` 2(g, x\u2032j) \u2265 \u03c4/2\n \u2264 4E sup\nQ P sup g\u2208G i\u2211 j=1 jQj( ) ( 9 8 `2(g, xj) + 2\u03bej`(g, xj) ) \u2212 1 8 Qj( i)`2(g, xj) \u2265 \u03c4 4  . Now let V (X) be the cover for G at scale \u03b1 = \u03c432i(9/8) = \u03c4 36i , which makes \u03c4/4\u2212 4i(1 + \u03b21)\u03b1 = \u03c4 8 . Thus we get the bound\n\u2264 4EX,\u03be sup Q P  sup h\u2208H(X) i\u2211 j=1 jQj( ) ( 9 8 `2(h, xj) + 2\u03bej`(h, xj) ) \u2212 1 8 Qj( i)`2(h, xj) \u2265 \u03c4 8  \u2264 4EX |V (X)| exp ( \u22122(1/8)(\u03c4/8)\n(3 + 1/8)2\n) = 2 exp(\u22122\u03c4/625)EXN ( \u03c4 36i ,G, X ) .\nThis entire derivation requires that \u03c4 \u2265 4(9/8) 2\n(1/8)2 = 324. The lower tail bound is similar. By Lemmas 7 and 8, with \u03b20 = 1/4 and \u03b21 = 1/8,\nP [ sup g\u2208G n\u2211 i=1 1 2 EjMj(g)\u2212Mj(g) \u2265 \u03c4 ]\n\u2264 2P [ sup g\u2208G n\u2211 i=1 1 2 EjMj(g)\u2212Mj(g)\u2212 2(1/8)Qj(x\u2032j)`2(g, x\u2032j) \u2265 \u03c4 2 ]\n\u2264 4E sup Q P sup g\u2208G i\u2211 j=1 jQj( ) [ 7 8 `2(g, xj) + 2\u03bej`(g, xj) ] \u2212 1 8 i\u2211 j=1 Qi( )`2(g, xj) \u2265 \u03c4 4  \u2264 4E sup\nQ P sup g\u2208G i\u2211 j=1 jQj( ) [ 9 8 `2(g, xj) + 2\u03bej`(g, xj) ] \u2212 1 8 i\u2211 j=1 Qi( )`2(g, xj) \u2265 \u03c4 4  .\nThis is the intermediate term we had for the upper tail, so we obtain the same bound. To wrap up the proof, apply Haussler\u2019s Lemma 6, to bound the covering number\nEXN ( \u03c4\n36i ,G, X\n) \u2264 e(d+ 1) ( 72ie\n\u03c4\n)d .\nFinally take a union bound over all pairs of starting and ending indices i < i\u2032, all labels y, and both tails to get that the total failure probability is at most\n8Ke(d+ 1) exp (\u22122\u03c4/625) \u2211\ni<i\u2032\u2208[n]\n( 72e(i\u2032 \u2212 i)\n\u03c4\n)d .\nThe result now follows from standard approximations. Specifically we use the fact that we anyway require \u03c4 \u2265 324 to upper bound that 1/\u03c4d term, use (i\u2032 \u2212 i)d \u2264 nd and set the whole expression to be at most \u03b4."}, {"heading": "B Multiplicative Weights", "text": "For completeness we prove Theorem 10 here. For this section only let q(t) \u221d p(t) be the distribution used by the algorithm at round t. If the program is feasible, then there exists a point that is also feasible against every distribution q. By contraposition, if on iteration t, the oracle reports infeasibility against q(t), then the original program must be infeasible.\nNow suppose the oracle always finds vt that is feasible against q(t). This implies\n0 \u2264 T\u2211 t=1 n\u2211 i=1 q (t) i (bi \u2212 \u3008ai, vt\u3009).\nDefine `t(i) = bi\u2212\u3008ai,vt\u3009\n\u03c1i which is in [\u22121, 1] by assumption. We compare this term to the corresponding term for a single constraint i. Using the standard potential-based analysis, with \u03a6(t) = \u2211 i p (t) i , we get\n\u03a6(T+1) = m\u2211 i=1 p (T ) i (1\u2212 \u03b7`T (i)) \u2264 \u03a6 (T ) exp\n( \u2212\u03b7\nm\u2211 i=1 q (T ) i `T (i)\n) \u2264 m exp ( \u2212\u03b7\nT\u2211 t=1 m\u2211 i=1 q (t) i `t(i)\n) .\nFor any i, we also have\n\u03a6(T+1) \u2265 T\u220f t=1 (1\u2212 \u03b7`t(i)).\nThus, taking taking logarithms and re-arranging we get\n0 \u2264 T\u2211 t=1 m\u2211 i=1 q (t) i `t(i) \u2264 log(m) \u03b7 + 1 \u03b7 T\u2211 t=1 log ( 1 1\u2212 \u03b7`t(i) ) \u2264 logm \u03b7 + T\u2211 t=1 `t(i) + \u03b7T.\nHere we use standard approximations log(1/(1\u2212 x)) \u2264 x+ x2 (which holds for x \u2264 1/2) along with the fact that |`t(i)| \u2264 1. Using the definition of `t(i) we have hence proved that\nT\u2211 t=1 \u3008ai, vt\u3009 \u2264 Tbi + \u03c1i logm \u03b7 + \u03c1i\u03b7T.\nNow with our choice of \u03b7 = \u221a\nlog(m)/T we get the desired bound. If \u03b7 \u2265 1/2 then the result is trivial by the boundedness guarantee on the oracle."}, {"heading": "C Proofs of Lemmata", "text": "Proof of Lemma 1. By the definitions,\nR\u0302(g\u2032) + w\u2032(g\u2032(x)\u2212 c)2 = R\u0302(g\u2032, w\u2032, c) \u2264 R\u0302(g) + w\u2032(g(x)\u2212 c)2\n= R\u0302(g) + w(g(x)\u2212 c)2 + (w\u2032 \u2212 w)(g(x)\u2212 c)2\n\u2264 R\u0302(g\u2032) + w(g\u2032(x)\u2212 c)2 + (w\u2032 \u2212 w)(g(x)\u2212 c)2.\nRearranging shows that (w\u2032\u2212w)(g\u2032(x)\u2212c)2 \u2264 (w\u2032\u2212w)(g(x)\u2212c)2. Since w\u2032 \u2265 w, we have (g\u2032(x)\u2212c)2 \u2264 (g(x)\u2212 c)2, which is the second claim. For the first, the definition of g gives\nR\u0302(g) + w(g(x)\u2212 c)2 \u2264 R\u0302(g\u2032) + w(g\u2032(x)\u2212 c)2.\nRearranging this inequality gives, R\u0302(g\u2032) \u2212 R\u0302(g) \u2265 w((g(x) \u2212 c)2 \u2212 (g\u2032(x) \u2212 c)2) \u2265 0, which yields the result. Proof of Lemma 2. We take expectation of Mj over the cost conditioned on a fixed example xj = x and a fixed query outcome Qj(y):\nE[Mj | xj = x,Qj(y)] = Qj(y)\u00d7 Ec[g(x)2 \u2212 f?(x; y)2 \u2212 2c(y)(g(x)\u2212 f?(x; y)) | xj = x] = Qj(y) ( g(x)2 \u2212 f?(x; y)2 \u2212 2f?(x; y)(g(x)\u2212 f?(x; y)) ) = Qj(y)(g(x)\u2212 f?(x; y))2.\nThe second equality is by Assumption 1, which implies E[c(y) | xj = x] = f?(x; y). Taking expectation over xj and Qj(y), we have\nEj [Mj ] = Ej [ Qj(y)(g(xj)\u2212 f?(xj ; y))2 ] .\nFor the variance:\nVar j\n[Mj ] \u2264 Ej [M2j ] = Ej [ Qj(y)(g(xj)\u2212 f?(xj ; y))2(g(xj) + f?(xj ; y)\u2212 2c(y))2 ] \u2264 4 \u00b7 Ej [ Qj(y)(g(xj)\u2212 f?(xj ; y))2 ] = 4Ej [Mj ].\nThis concludes the proof. Proof of Lemma 3. Fix some f \u2208 Fi, and let y\u0302 = hf (x) and y? = hf?(x) for shorthand, but note that both depend on x. Define\nS\u03b6(x) = 1 (f ?(x, y\u0302) \u2264 f?(x, y?) + \u03b6) , S\u2032\u03b6(x) = 1 ( min y 6=y? f?(x, y) \u2264 f?(x, y?) + \u03b6 ) .\nObserve that for fixed \u03b6, S\u03b6(x)1 (y\u0302 6= y?) \u2264 S\u2032\u03b6(x) for all x. We can also majorize the complementary indicator to obtain the inequality\nSC\u03b6 (x) \u2264 f?(x, y\u0302)\u2212 f?(x, y?)\n\u03b6 .\nWe begin with the definition of realizability, which gives\nEx,c[c(hf (x))\u2212 c(hf?(x)] = Ex [(f?(x, y\u0302)\u2212 f?(x, y?))1 (y\u0302 6= y?)] = Ex [( S\u03b6(x) + S C \u03b6 (x) ) (f?(x, y\u0302)\u2212 f?(x, y?))1 (y\u0302 6= y?) ] \u2264 \u03b6ExS\u2032\u03b6(x) + Ei [ SC\u03b6 (x)1 (y\u0302 6= y?) (f?(x, y\u0302)\u2212 f?(x, y?)) ] .\nThe first term here is exactly the \u03b6P\u03b6 term in the bound. We now focus on the second term, which depends on our query rule. For this we must consider three cases.\nCase 1. If both y\u0302 and y? are not queried, then it must be the case that both have small cost ranges. This follows since f \u2208 Fi and hf (x) = y\u0302 so y? does not dominate y\u0302. Moreover, since the cost ranges are small on both y\u0302 and y? and since we know that f? is well separated under event SC\u03b6 (x), the relationship between \u03b6 and \u03c8i governs whether we make a mistake or not. Specifically, we get that SC\u03b6 (x)1 (y\u0302 6= y?)1 (no query) \u2264 1 (\u03b6 \u2264 2\u03c8i) at round i. In other words, if we do not query and the separation is big but we make a mistake, then it must mean that the cost range threshold \u03c8i is also big.\nUsing this argument, we can bound the second term as,\nEi [ SC\u03b6 (x)1 (y\u0302 6= y?) (1\u2212Qi(y\u0302))(1\u2212Qi(y?)) (f?(x, y\u0302)\u2212 f?(x, y?)) ] \u2264 Ei [ SC\u03b6 (x)1 (y\u0302 6= y?) (1\u2212Qi(y\u0302))(1\u2212Qi(y?))2\u03c8i\n] \u2264 Ei [1 (\u03b6 \u2264 2\u03c8i) 2\u03c8i] = 1 (\u03b6 \u2264 2\u03c8i) 2\u03c8i.\nCase 2. If both y\u0302 and y? are queried, we can relate the second term to the square loss,\nEi [ SC\u03b6 (x)Qi(y\u0302)Qi(y ?) (f?(x, y\u0302)\u2212 f?(x, y?)) ]\n\u2264 1 \u03b6 Ei [ Qi(y\u0302)Qi(y ?) (f?(x, y\u0302)\u2212 f?(x, y?))2 ]\n\u2264 1 \u03b6 Ei [ Qi(y\u0302)Qi(y ?) (f?(x, y\u0302)\u2212 f(x, y\u0302) + f(x, y?)\u2212 f?(x, y?))2 ]\n\u2264 2 \u03b6 Ei [ Qi(y\u0302)(f ?(x, y\u0302)\u2212 f(x, y\u0302))2 +Qi(y?)(f(x, y?)\u2212 f?(x, y?))2 ]\n\u2264 2 \u03b6 \u2211 y Ei [ Qi(y)(f ?(x, y)\u2212 f(x, y))2 ] = 2 \u03b6 \u2211 y Ei [Mi(f ; y)] .\nPassing from the second to third line here is justified by the fact that f?(x, y\u0302) \u2265 f?(x, y?) and f(x, y\u0302) \u2264 f(x, y?) so we added two non-negative quantities together. The last step uses Lemma 2. While not written, we also use the event 1 (y\u0302 6= y?) to save a factor of 2.\nCase 3. The last case is if one label is queried and the other is not. Both cases here are analogous, so we do the derivation for when y(x) is queried but y?(x) is not. Since in this case, y?(x) is not dominated (hf (x) is never dominated provided f \u2208 Fi), we know that the cost range for y?(x) must be small. Using this fact,\nand essentially the same argument as in case 2, we get Ei [ SC\u03b6 (x)Qi(y\u0302)(1\u2212Qi(y?)) (f?(x, y\u0302)\u2212 f?(x, y?)) ] \u2264 1 \u03b6 Ei [ Qi(y\u0302)(1\u2212Qi(y?)) (f?(x, y\u0302)\u2212 f?(x, y?))2\n] \u2264 2 \u03b6 Ei [ Qi(y\u0302) (f ?(x, y\u0302)\u2212 f(x, y\u0302))2 + (1\u2212Qi(y?)) (f(x, y?)\u2212 f?(x, y?))2 ]\n\u2264 2\u03c8 2 i\n\u03b6 +\n2 \u03b6 Ei [ Qi(y\u0302) (f ?(x, y\u0302)\u2212 f(x, y\u0302))2 ] \u2264 2\u03c8 2 i \u03b6 + 2 \u03b6 \u2211 y Ei [Mi(f ; y)] .\nWe also obtain this term for the other case where y? is queried but y\u0302 is not. To summarize, adding up the contributions from these cases (which is an over-estimate since at most one case can occur and all are non-negative), we get\nEx,c[c(hf (x))\u2212 c(hf?(x)] \u2264 \u03b6P\u03b6 + 1 (\u03b6 \u2264 2\u03c8i) 2\u03c8i + 4\u03c82i \u03b6 + 6 \u03b6 \u2211 y Ei [Mi(f ; y)] .\nThis bound holds for any \u03b6, so it holds for the minimum. Proof of Lemma 4. The proof here is an easier version of the generalization bound proof for fi. First, condition on the high probability event in Theorem 9, under which we already showed that f? \u2208 Fi for all i \u2208 [n]. Now fix some f \u2208 Fi. Since by the monotonicity property defining the sets Gi, we must have f \u2208 Fj for all 1 \u2264 j \u2264 i, we can immediately apply Lemma 3 on all rounds to bound the cost sensitive regret by\n1\ni\u2212 1 i\u22121\u2211 j=1 min \u03b6>0\n{ \u03b6P\u03b6 + 1{\u03b6 \u2264 2\u03c8j}2\u03c8j +\n4\u03c82j \u03b6 + 6 \u03b6 \u2211 y Ej [Mj(f ; y)]\n} .\nAs in the generalization bound, the first term contributes \u03b6P\u03b6 , the second is at most 12\u03b6 and the third is at most 4/\u03b6(1 + log(i\u2212 1)). The fourth term is slightly different. We still apply (17) to obtain\n1\ni\u2212 1 i\u22121\u2211 j=1 Ej [Mj(f ; y)] \u2264 2 i\u2212 1 i\u22121\u2211 j=1 Mj(f ; y) + 2\u03bdn i\u2212 1\n= 2 ( R\u0302i(f ; y)\u2212 R\u0302i(gi,y; y) + R\u0302i(gi,y; y)\u2212 R\u0302i(f?; y) ) +\n2\u03bdn i\u2212 1\n\u2264 2\u2206i + 2\u03bdn i\u2212 1 = 2(\u03ba+ 1)\u03bdn i\u2212 1 .\nWe use this bound for each label. Putting terms together, the cost sensitive regret is\nmin \u03b6>0\n{ \u03b6P\u03b6 + 12\n\u03b6 + 4(1 + log(i\u2212 1)) \u03b6 + 12K(\u03ba+ 1)\u03bdn \u03b6(i\u2212 1)\n} \u2264 min\n\u03b6>0\n{ \u03b6P\u03b6 +\n44\u03baK\u03bdn \u03b6(i\u2212 1)\n} .\nThis proves containment, since this upper bounds the cost sensitive regret of every f \u2208 Fi. Proof of Lemma 5. The first claim is straightforward, since Fi \u2282 Fcsr(ri) and since we set the tolerance parameter in the calls to MAXCOST and MINCOST to \u03c8i/4. Specifically,\n\u03b3\u0302(xi, y) \u2264 \u03b3(xi, y,Fi) + \u03c8i/2 \u2264 \u03b3(xi, y,Fcsr(ri)) + \u03c8i/2.\nFor the second claim, suppose y 6= y?i . Then\ny \u2208 Yi \u21d2 c\u0302\u2212(xi, y) \u2264 c\u0302+(xi, y\u0304i) \u21d2 c\u0302\u2212(xi, y) \u2264 c\u0302+(xi, y?i ) \u21d2 c\u2212(xi, y,Fcsr(ri)) \u2264 c+(xi, y?i ,Fcsr(ri)) + \u03c8i/2 \u21d2 f?(xi; y)\u2212 \u03b3(xi, y,Fcsr(ri)) \u2264 f?(xi; y?i ) + \u03b3(xi, y?i ,Fcsr(ri)) + \u03c8i/2.\nThis argument uses the tolerance setting \u03c8i/4, Lemma 4 to translate between the version space and the cost-sensitive regret ball, and finally the fact that f? \u2208 Fcsr(ri) since it has zero cost-sensitive regret. This latter fact lets us lower (upper) bound the minimum (maximum) cost by f? prediction minus (plus) the cost range.\nFor y?i we need to consider two cases. First assume y ? i = y\u0304i. Since by assumption |Yi| > 1, it must be the case that c\u0302\u2212(xi, y\u0303i) \u2264 c\u0302+(xi, y?i ) at which point the above derivation produces the desired implication. On the other hand, if y?i 6= y\u0304i then c\u0302+(xi, y\u0304i) \u2264 c\u0302+(xi, y?i ), but this also implies that c\u0302\u2212(xi, y\u0303i) \u2264 c\u0302+(xi, y?i ), since minimum costs are always smaller than maximum costs, and y\u0304i is included in the search defining y\u0303i."}], "year": 2021, "references": [{"title": "Selective sampling algorithms for cost-sensitive multiclass prediction", "authors": ["Alekh Agarwal"], "venue": "In International Conference on Machine Learning,", "year": 2013}, {"title": "Taming the monster: A fast and simple algorithm for contextual bandits", "authors": ["Alekh Agarwal", "Daniel Hsu", "Satyen Kale", "John Langford", "Lihong Li", "Robert E. Schapire"], "venue": "In International Conference on Machine Learning,", "year": 2014}, {"title": "The multiplicative weights update method: a metaalgorithm and applications", "authors": ["Sanjeev Arora", "Elad Hazan", "Satyen Kale"], "venue": "Theory of Computing,", "year": 2012}, {"title": "Active and passive learning of linear separators under log-concave distributions", "authors": ["Maria Florina Balcan", "Philip M. Long"], "venue": "In Conference on Learning Theory,", "year": 2013}, {"title": "Agnostic active learning", "authors": ["Maria Florina Balcan", "Alina Beygelzimer", "John Langford"], "venue": "In International Conference on Machine Learning,", "year": 2006}, {"title": "Margin based active learning", "authors": ["Maria Florina Balcan", "Andrei Broder", "Tong Zhang"], "venue": "In Conference on Learning Theory,", "year": 2007}, {"title": "Importance weighted active learning", "authors": ["Alina Beygelzimer", "Sanjoy Dasgupta", "John Langford"], "venue": "In International Conference on Machine Learning,", "year": 2009}, {"title": "Agnostic active learning without constraints", "authors": ["Alina Beygelzimer", "Daniel Hsu", "John Langford", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2010}, {"title": "Adaptivity to noise parameters in nonparametric active learning", "authors": ["Alexandra Carpentier", "Andrea Locatelli", "Samory Kpotufe"], "venue": "In Conference on Learning Theory,", "year": 2017}, {"title": "Minimax bounds for active learning", "authors": ["R.M. Castro", "R.D. Nowak"], "venue": "Transaction on Information Theory,", "year": 2008}, {"title": "Faster rates in regression via active learning", "authors": ["Rui Castro", "Rebecca Willett", "Robert D. Nowak"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2005}, {"title": "Learning noisy linear classifiers via adaptive and selective sampling", "authors": ["Giovanni Cavallanti", "Nicolo Cesa-Bianchi", "Claudio Gentile"], "venue": "Machine Learning,", "year": 2011}, {"title": "Learning to search better than your teacher", "authors": ["Kai-Wei Chang", "Akshay Krishnamurthy", "Alekh Agarwal", "Hal Daum\u00e9 III", "John Langford"], "venue": "In International Conference on Machine Learning,", "year": 2015}, {"title": "A general agnostic active learning algorithm", "authors": ["Sanjoy Dasgupta", "Daniel Hsu", "Claire Monteleoni"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2007}, {"title": "Search-based structured prediction", "authors": ["Hal Daum\u00e9 III", "John Langford", "Daniel Marcu"], "venue": "Machine Learning,", "year": 2009}, {"title": "Robust selective sampling from single and multiple teachers", "authors": ["Ofer Dekel", "Claudio Gentile", "Karthik Sridharan"], "venue": "In Conference on Learning Theory,", "year": 2010}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "authors": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "In Conference on Learning Theory,", "year": 2010}, {"title": "Practical contextual bandits with regression oracles", "authors": ["Dylan Foster", "Alekh Agarwal", "Miroslav Dudik", "Haipeng Luo", "Robert Schapire"], "venue": "In International Conference on Machine Learning,", "year": 2018}, {"title": "Theory of disagreement-based active learning", "authors": ["Steve Hanneke"], "venue": "Foundations and Trends in Machine Learning,", "year": 2014}, {"title": "Surrogate losses in passive and active learning", "authors": ["Steve Hanneke", "Liu Yang"], "year": 2012}, {"title": "Minimax analysis of active learning", "authors": ["Steve Hanneke", "Liu Yang"], "venue": "Journal of Machine Learning Research,", "year": 2015}, {"title": "Sphere packing numbers for subsets of the Boolean n-cube with bounded Vapnik- Chervonenkis dimension", "authors": ["David Haussler"], "venue": "Journal of Combinatorial Theory, Series A,", "year": 1995}, {"title": "Algorithms for Active Learning", "authors": ["Daniel Hsu"], "venue": "PhD thesis, University of California at San Diego,", "year": 2010}, {"title": "Efficient and parsimonious agnostic active learning", "authors": ["Tzu-Kuo Huang", "Alekh Agarwal", "Daniel Hsu", "John Langford", "Robert E. Schapire"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Online importance weight aware updates", "authors": ["Nikos Karampatziakis", "John Langford"], "venue": "In Uncertainty in Artificial Intelligence,", "year": 2011}, {"title": "Active learning for cost-sensitive classification", "authors": ["Akshay Krishnamurthy", "Alekh Agarwal", "Tzu-Kuo Huang", "III Hal Daum\u00e9", "John Langford"], "venue": "In International Conference on Machine Learning,", "year": 2017}, {"title": "Sensitive error correcting output codes", "authors": ["John Langford", "Alina Beygelzimer"], "venue": "In Conference on Learning Theory,", "year": 2005}, {"title": "Rcv1: A new benchmark collection for text categorization research", "authors": ["David D. Lewis", "Yiming Yang", "Tony G. Rose", "Fan Li"], "venue": "Journal of Machine Learning Research,", "year": 2004}, {"title": "Learning with square loss: Localization through offset rademacher complexity", "authors": ["Tengyuan Liang", "Alexander Rakhlin", "Karthik Sridharan"], "venue": "In Conference on Learning Theory,", "year": 2015}, {"title": "Smooth discrimination analysis", "authors": ["Enno Mammen", "Alexandre B. Tsybakov"], "venue": "The Annals of Statistics,", "year": 1999}, {"title": "Risk bounds for statistical learning", "authors": ["Pascal Massart", "\u00c9lodie N\u00e9d\u00e9lec"], "venue": "The Annals of Statistics,", "year": 2006}, {"title": "Plug-in approach to active learning", "authors": ["Stanislav Minsker"], "venue": "Journal of Machine Learning Research,", "year": 2012}, {"title": "Better algorithms for selective sampling", "authors": ["Francesco Orabona", "Nicolo Cesa-Bianchi"], "venue": "In International Conference on Machine Learning,", "year": 2011}, {"title": "Fast approximation algorithms for fractional packing and covering problems", "authors": ["Serge A. Plotkin", "David B. Shmoys", "\u00c9va Tardos"], "venue": "Mathematics of Operations Research,", "year": 1995}, {"title": "On equivalence of martingale tail bounds and deterministic regret inequalities", "authors": ["Alexander Rakhlin", "Karthik Sridharan"], "venue": "In Proceedings of the 2017 Conference on Learning", "year": 2017}, {"title": "Empirical entropy, minimax regret and minimax risk", "authors": ["Alexander Rakhlin", "Karthik Sridharan", "Alexandre B Tsybakov"], "year": 2017}, {"title": "Reinforcement and imitation learning via interactive no-regret learning", "authors": ["Stephane Ross", "J. Andrew Bagnell"], "year": 2014}, {"title": "Normalized online learning", "authors": ["Stephane Ross", "Paul Mineiro", "John Langford"], "venue": "In Uncertainty in Artificial Intelligence,", "year": 2013}, {"title": "Active learning", "authors": ["Burr Settles"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning,", "year": 2012}, {"title": "Learning where to sample in structured prediction", "authors": ["Tianlian Shi", "Jacob Steinhardt", "Percy Liang"], "venue": "In Artificial Intelligence and Statistics,", "year": 2015}, {"title": "A survey of hierarchical classification across different application domains", "authors": ["Carlos N. Silla Jr.", "Alex A. Freitas"], "venue": "Data Mining and Knowledge Discovery,", "year": 2011}, {"title": "Deeply AggreVaTeD: Differentiable imitation learning for sequential prediction", "authors": ["Wen Sun", "Arun Venkatraman", "Geoffrey J. Gordon", "Byron Boots", "J. Andrew Bagnell"], "venue": "In International Conference on Machine Learning,", "year": 2017}, {"title": "Efficient algorithms for adversarial contextual learning", "authors": ["Vasilis Syrgkanis", "Akshay Krishnamurthy", "Robert E. Schapire"], "venue": "In International Conference on Machine Learning,", "year": 2016}, {"title": "Going deeper with convolutions", "authors": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott E. Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Computer Vision and Pattern Recognition,", "year": 2015}, {"title": "Optimal aggregation of classifiers in statistical learning", "authors": ["Alexandre B. Tsybakov"], "venue": "Annals of Statistics,", "year": 2004}, {"title": "Beyond disagreement-based agnostic active learning", "authors": ["Chicheng Zhang", "Kamalika Chaudhuri"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}], "id": "SP:b2d1c9e54bd4997fc8a4c9e77b304e64da282c38", "authors": [{"name": "Akshay Krishnamurthy", "affiliations": []}, {"name": "Alekh Agarwal", "affiliations": []}, {"name": "Tzu-Kuo Huang", "affiliations": []}, {"name": "Hal Daum\u00e9 III", "affiliations": []}], "abstractText": "We design an active learning algorithm for cost-sensitive multiclass classification: problems where different errors have different costs. Our algorithm, COAL, makes predictions by regressing to each label\u2019s cost and predicting the smallest. On a new example, it uses a set of regressors that perform well on past data to estimate possible costs for each label. It queries only the labels that could be the best, ignoring the sure losers. We prove COAL can be efficiently implemented for any regression family that admits squared loss optimization; it also enjoys strong guarantees with respect to predictive performance and labeling effort. We empirically compare COAL to passive learning and several active learning baselines, showing significant improvements in labeling effort and test cost on real-world datasets.", "title": "Active Learning for Cost-Sensitive Classification"}