32	16	Finally, Section 5 concludes and lays out future research directions.
63	10	Variational inference (vi) is key to modern probabilistic modeling and Bayesian deep learning (Jordan et al., 1999; Blei et al., 2017; Zhang et al., 2017a).
64	10	In Bayesian inference, the object of interest is a posterior distribution of latent variables z given observations x. vi approximates Bayesian inference by an optimization problem which we can solve by (stochastic) gradient ascent (Jordan et al., 1999; Hoffman et al., 2013).
65	9	In more detail, vi builds a tractable approximation of the posterior p(z|x) by minimizing the KL-divergence between a variational family q(z|λ), parametrized by free parameters λ ∈ Rd, and p(z|x).
66	10	This is equivalent to maximizing the so-called evidence lower bound (elbo): L(λ) = Eq(z|λ)[log p(x, z) − log q(z|λ)].
73	15	Score Function Gradient The score function gradient (also called REINFORCE gradient) (Ranganath et al., 2014) expresses the gradient as expectation with respect to q(z|λ) and is given by ∇λL(λ) = Eq(z|λ)[∇λ log q(z|λ) ( log p(x, z) − log q(z|λ))].
75	24	This estimator applies to continuous and discrete variational distributions.
76	21	Reparameterization Gradient The second approach is based on the reparametrization trick (Kingma and Welling, 2014), where the distribution over z is expressed as a deterministic transformation of another distribution over a noise variable ε, hence z = gλ(ε) where ε ∼ p(ε).
82	25	This allows us to obtain a stochastic estimator of the gradient by an average over a finite sample {z̃1, · · · , z̃N} as ĝN(λt) = (1/N) ∑N i=1 gz̃i (λt).This way, the elbo can be optimized by stochastic optimization.
88	12	Quasi Monte Carlo Low discrepancy sequences, also called qmc sequences, are used for integrating a function ψ over the [0, 1]d hypercube.
93	8	This idea is closely linked to stratification.
99	10	rqmc sequences are unbiased and the error can be assessed by repeated simulation.
122	15	We show that for N sufficiently large, sgd with rqmc samples reaches regions closer to the true optimizer of the elbo.
124	12	RQMC for Optimizing Monte Carlo Objectives We step back from black box variational inference and consider the more general setup of optimizing Monte Carlo objectives.
127	30	5, where Γ is a reparameterization function that converts uniform samples from the hypercube into samples from the target distribution.
128	32	In this paper, u1, · · · ,uN is an rqmc sequence.
132	12	crucially depends on the variance of the estimator ĝN (Johnson and Zhang, 2013).
206	29	When using a small number of samples (N = 10), all three methods have comparable convergence speed and attain a similar optimum.
212	8	We place a normal prior over each weight, and each weight prior has an inverse Gamma hyper prior.
216	40	We approximate the posterior by a variational diagonal Gaussian.
219	34	For N = 50, all three algorithms reach approximately the same value of the elbo, but our rqmc method converges much faster.
220	21	In both settings, the variance of the rqmc gradient estimator is one to three orders of magnitude lower than the variance of the baselines.
221	12	Along with our new Monte Carlo variational inference approach qmcvi, Theorem 3 gives rise to a new stochastic optimization algorithm for Monte Carlo objectives.
222	21	Here, we investigate this algorithm empirically, using a constant learning rate and an (exponentially) increasing sample size schedule.
223	11	We show that, for strongly convex objective functions and some mild regularity assumptions, our rqmc based gradient estimator leads to a faster asymptotic convergence rate than using the ordinary mc based gradient estimator.
239	10	qmcvi can be easily integrated into automated inference packages.
240	57	All one needs to do is replace a sequence of uniform random numbers over the hypercube by an rqmc sequence, and perform the necessary reparameterizations to sample from the target distributions.
241	41	An open question remains as to which degree qmcvi can be combined with control variates, as rqmc may introduce additional unwanted correlations between the gradient and the cv.
242	40	We will leave this aspect for future studies.
243	43	We see particular potential for qmcvi in the context of reinforcement learning, which we consider to investigate.
