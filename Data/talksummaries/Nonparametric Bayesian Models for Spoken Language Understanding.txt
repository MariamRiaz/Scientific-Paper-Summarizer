0	38	Spoken language understanding (SLU) refers to the challenge of recognizing a speaker’s intent from a natural language utterance, which is typically defined as a slot filling task.
1	64	For example, in the utterance “Remind me to call John at 9am tomorrow”, the specified information {“time”: “9am tomorrow”} and {“subject”: “to call John”} should be extracted.
6	32	In this paper, we use the term arbitrary slot filling task to refer to this implicit problem statement, which inherently underlies the sequential labeling formulation.
7	34	In contrast, a different line of work has explored the case where Vsi is provided as a finite set of possible values that can be handled by a backend system (Henderson, 2015).
9	21	In this case, the slot filling task is regarded as a classification problem that explicitly considers a value-based prediction, as shown in Figure 2.
10	98	From this point of view, we can say that a distribution of slot values is actually concentrated in a small set of typical phrases, even in the arbitrary slot filling task, because users basically know what kind of function is offered by the system.
43	22	We apply the Dirichlet process (DP) to model both the distribution for an individual slot pi(si) and the joint distribution p(S).
48	18	If G is drawn from DP (α0, G0) (i.e., G ∼ DP (α0, G0)), then the following Dirichlet distributed property holds for any partition of X denoted by {A1, ..., AL}: (G(A1), ..., G(AL)) ∼ Dir(α(A1), ..., α(AL)) where α(A) = α0G0(A), which is known as the base measure of DP.
54	27	The assignment ci is an auxiliary variable to indicate which of these atoms is assigned to the ith data point xi; when xi = “fen ditton”, ci can be 1 or 3.
59	22	This explicit modeling of the length helps avoid the bias toward shorter phrases and leads to a better distribution, as reported by Zhai and Boydgraber (2013).
69	25	To obtain more accurate distribution, we formulate p(S) using another DP that recognizes a frequent combination of slot values, as p(S) ∼ DP (α1, G2) where G2 is a base distribution over VMS .
90	50	In the proposed model, we formulate the distribution of slot values as well as the distribution of non-slot parts.
116	35	For slot filling, we examine the posterior probability of content slot values S given u, which can be reformed as follows: P (S|u) ∝ ∑ F P (u|S, F )P (S)P (F ) In this equation, we can remove the summation of F because filler slot values F are uniquely identified regarding u and S in our assumption.
118	27	By using these assumptions, the posterior probability is reduced to the following formula: P (S|u) ∝ P (S)P (F ) (3) where F in this formula is fillers identified given u and S. Consequently, the proposed method attempts to find the most likely combination of the slot values and the non-slot phrases, since all words in an utterance have to belong to either of them.
124	28	Figure 5 provides an example of candidate generation by using a sequential labeling algorithm with IOB tags.
128	23	We adopt a conditional random field (CRF) as a candidate generation algorithm that generates N -best estimation as candidates.
139	16	It contains 1,442 questions spoken in Japanese.
143	24	The methods are compared in terms of slot estimation accuracy.
144	37	Let nc be the number of utterances for which the estimated slot S and the ground-truth slot Ŝ are perfectly matched, and let ne be the number of the utterances including an estimation error.
148	26	Tables 2 and 3 present the slot estimation accuracy for the DSTC corpus and the Japanese weather corpus, respectively.
157	78	In fact, the top five candidates cover almost all of the correct answers.
160	23	Nevertheless, the result shows that the drop in the performance is limited; the accuracy is still significantly better than the baseline.
161	45	This result suggests that the proposed method is less dependent on the performance of the candidate generator.
165	21	The value-based formulation allows the model to learn that the phrase “fast food” is more likely to be a food name than to be a functional filler and to reject the candidate.
166	41	The third example in Table 4 shows an error using HDP, which extracted “chine chinese takeaway” which includes a reparandum of disfluency (Georgila et al., 2010).
167	34	This error can be attributed to the fact that this kind of disfluency resembles the true slot value, which leads to a higher probability of “chine” in the food slot model compared to in the functional filler model.
173	45	In this paper, we proposed an arbitrary slot filling method that directly deals with the posterior probability of slot values by using nonparametric Bayesian models.
174	52	We presented a two-stage method that involves an N-best candidate generation step, which is typically done using a CRF.
175	15	Experimental results show that our method significantly improves recognition accuracy.
176	59	This empirical evidence suggests that the value-based formulation is a promising approach for arbitrary slot filling tasks, which is worth exploring further in future work.
