0	9	Compared to large-scale collections of conversations from social media (Felbo et al., 2017; Luo et al., 2012; Zhang et al., 2017; Tan et al., 2016) or news comments (Napoles et al., 2017), Wikipedia talk pages offer a unique perspective into goal-oriented discussions between thousands of volunteer contributors coordinating to write the largest online encyclopedia.
1	10	Talk page data already underpins research on social phenomena such as conversational behavior (DanescuNiculescu-Mizil et al., 2012, 2013), disputes (Wang and Cardie, 2014b), antisocial behavior (Wulczyn et al., 2017; Zhang et al., 2018) and collaboration (Kittur et al., 2007; Halfaker et al., 2009).
2	27	However, the scope of such studies has so far been limited by a view of the conversation that is incomplete in two crucial ways: first, it only captures a subset of all discussions; and second, it only accounts for the final form of each conversation, which frequently differs from the interlocutors experience as the conversation develops.
3	22	In this paper, we undertake the challenge of reconstructing a complete and structured history of the conversational process in Wikipedia talk pages, containing detailed information about all the interlocutors’ actions, such as adding and replying to comments, modifying or deleting them.
4	18	To this end, we devise a methodology for identifying and structuring these actions, while also addressing the challenges spurring from the inconsistent formatting and the raw scale of existing records.
5	8	This results in the largest public dataset of goal-oriented conversations, WikiConv, spanning five languages.
6	15	The largest component of this dataset is based on the English Wikipedia, and contains roughly 91 million conversations consisting of 212 million conversational actions taking place in 24 million talk pages.
9	47	Furthermore, a manual review of the English Wikipedia portion of the dataset reveals that 98% of the reply structure is recovered correctly and 98% of the interlocutor’s actions are categorized correctly.
10	8	Since the reconstruction pipeline does not rely on any language specific heuristics, we also apply it to Chinese, German, Greek and Russian Wikipedia Talk page archives, in addition to those from English Wikipadia.
12	7	To encourage further validation, refinements and updates, we have open sourced the code and published the datasets.1 Finally, we present two case studies illustrating how the corpus can bring new insights into previously observed phenomena.
13	44	We first analyze the conversational behavior of a subset of English Wikipedia contributors across the entire range of talk pages, and show that their levels of linguistic coordination vary according to where the conversation takes place.
14	8	Second, we investigate the toxicity of deleted comments, and show that community moderation of undesired behavior takes place at a much higher rate than previously estimated.
21	75	Technically, comments are added to Wikipedia talk pages the same way content is added to article pages: contributors simply edit the markdown of any part of the talk page without relying on any functionality specialized for structuring the conversations.
22	13	Figure 1 gives an example of the discussion interface and the resulting rendered conversation.
23	45	Each edit results in a revision of the whole page that is permanently stored in a public historical record.3 Because conversations on Wikipedia have no ‘official’ underlying structure, and instead are organized using indentation markup and other ad hoc visual cues, computational heuristics are necessary to interpret conversational structure.
30	9	• Restoration: a revert specifies the deleted action being undone as the Parent-id.
33	6	Our reconstruction pipeline is a Python program written for Google Cloud Dataflow (also known as Apache Beam)4 that operates on pages in parallel and on the revisions of each page sequentially in temporal order.
36	6	Given the sorted set of a page-revisions, tokenlevel diffs between sequential revisions are computed using a longest common sequence (LCS) algorithm.5 Each sequential diff is then decomposed into the set of atomic conversation actions attributed to the user who submitted the page revision.
37	44	During the sequential processing of a page’s revisions, two data structures are maintained: each comment’s current character offset, and a list of deleted comments.
43	3	Finally, reconstructed actions are processed using mwparserfromhell 6 to clean the MediaWiki formating.
47	17	We evaluate the quality of the automatic reconstruction by manually verifying a randomly drawn subset of (at least) 100 examples from each action category.
48	10	For each action we verify the accuracy of (1) the assigned action type, (2) the token-level boundary of the comment, (3) the ReplyTo relation and (4) the action’s Parent relation.
62	2	Our results show with significant difference (p < 0.001 calculated by one-way ANOVA) that contributors coordinate the least when replying on other users’ talk pages, and most on their own talk page.
72	53	Figure 3 shows the fraction of comments deleted by Wikipedians who are not the author of the comment for different lengths of time; distinguishing between comments labeled as toxic, severely toxic, and the background distribution.
73	12	The key results here are that 7www.perspectiveapi.com 8We release the scores with the dataset.
74	14	This complements results previously reported by Wulczyn et al. (2017), accounting for an additional type of community moderation that is revealed using the detailed information about the history of the conversation provided by our corpus.
76	27	We applied this pipeline to Wikipedia in multiple languages and evaluated its quality on the English and Chinese Talk page corpora, obtaining a high reconstruction accuracy for both the Chinese and English datasets (9̃8%).
77	28	This level of detail and completeness opens avenues for new research, as well as revisiting and extending existing work on online conversational and collaboration behavior.
78	48	For example, while in our use cases we have focused on contributors deleting toxic comments, one could seek to understand why and when an editor is deleting or rewording their own comments.
79	30	Beyond refining the heuristics and parsing methods used in our reconstruction pipeline, and reducing the time needed to update the corpus, a significant remaining challenge is to capture conversations that happen across page boundaries.
