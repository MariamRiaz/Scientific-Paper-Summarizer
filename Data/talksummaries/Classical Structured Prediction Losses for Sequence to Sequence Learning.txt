14	13	The general architecture of our sequence to sequence models follows the encoder-decoder approach with soft attention first introduced in (Bahdanau et al., 2014).
19	63	We denote the source sentence as x, an output sentence of our model as u, and the reference or target sentence as t. For some objectives, we choose a pseudo reference u∗ instead, such as a model output with the highest BLEU or ROUGE score among a set of candidate outputs, U , generated by our model.
38	12	We compare several objective functions for training the model architecture described in §2.
42	14	Token Negative Log Likelihood (TokNLL) Token-level likelihood (TokNLL, Equation 1) minimizes the negative log likelihood of individual reference tokens t = (t1, .
43	61	It is the most common loss function optimized in related work and serves as a baseline for our comparison.
46	9	Label smoothing addresses this by acting as a regularizer that makes the model less confident in its predictions.
61	9	In this work we use task-specific cost functions designed to maximize BLEU or ROUGE (Lin, 2004), e.g., cost(t,u) = 1−BLEU(t,u), for a given a candidate sequence u and target t. Different to SeqNLL (§3.2), this loss may increase the score of several candidates that have low cost, instead of focusing on a single sequence which may only be marginally better than any alternatives.
67	9	The size of the margin varies between samples and is given by the difference between the cost of u∗ and the cost of û.
82	32	The sequence-level objectives we consider (§3.2) are defined over the entire space of possible output sequences, which is intractable to enumerate or score with our models.
91	101	In the online setting, we regenerate the candidate set every time we encounter an input sentence x during training.
95	79	Our model may perfectly be able to discriminate between them after only a single update, hindering the ability of the loss to correct eventual search errors.4 Finally, while some past work has added the reference target to the candidate set, i.e., U ′(x) = U(x) ∪ {t}, we find this can destabilize training since the model learns to assign low probabilities nearly everywhere, ruining the candidates generated by the model, while still assigning a slightly higher score to the reference (cf.
97	32	We experiment on the IWSLT’14 German to English (Cettolo et al., 2014) task using a similar setup as Ranzato et al. (2015), which allows us to compare to other recent studies that also adopted this setup, e.g., Wiseman and Rush (2016).5 The training data consists of 160K sentence pairs and the validation set comprises 7K sentences randomly sampled and held-out from the train data.
100	21	We also experiment on the much larger WMT’14 English-French task.
107	21	We train our baseline token-level models for 200 epochs and then anneal the learning by shrinking it by a factor of 10 after each subsequent epoch until the learning rate falls below 10−4.
114	10	We length normalize all scores and probabilities in the sequence-level losses by dividing by the number of tokens in the sequence so that scores are comparable between different lengths.
121	8	Our models for this task have 12 layers in the encoder and decoder each with 256 hidden units and kernel width 3.
123	15	First, we compare all objectives based on a weighted combination with token-level label smoothing (Equation 8).
125	35	We show a like-for-like comparison to Wiseman and Rush (2016) with a similar baseline model below (§6.6).
126	12	Table 1 shows that all sequence-level losses outperform token-level losses.
129	31	For these experiments we use 5 candidate sequences per training example for faster experimental turnaround.
138	36	We believe that the regularization provided by label smoothing leads to models with less sharp distributions that are a better starting point for sequence-level training.
139	23	Next, we consider the question if refreshing the candidate subset at every training step (online) results in better accuracy compared to generating candidates before training and keeping the set static throughout training (offline).
140	14	Table 4 shows that offline generation gives lower accuracy.
169	16	Table 6 shows that our baseline (TokLS) outperforms all prior approaches in terms of ROUGE2 and ROUGE-L and it is on par to the best previous result for ROUGE-1.
172	25	§6.2) but accuracy was generally lower on the validation set: RG-1 (36.59 Risk only vs. 36.67 Weighted), RG-2 (17.34 vs. 18.05), and RG-L (33.66 vs. 33.98).
174	36	We found that combining sequence-level and tokenlevel losses is necessary to perform best, and so is training on candidates decoded with the current model.
175	29	We show that sequence-level training improves state-of-the-art baselines both for IWSLT’14 German-English translation and Gigaword abstractive sentence summarization.
176	56	Structured prediction losses are very competitive to recent work on reinforcement or beam optimization.
177	58	Classical expected risk can slightly outperform beam search optimization (Wiseman and Rush, 2016) in a likefor-like setup.
178	92	Future work may investigate better use of already generated candidates since invoking generation for each batch slows down training by a large factor, e.g., mixing with fresh and older candidates inspired by MERT (Och, 2003).
