5	68	Consider the example shown in Figure 1a, in which a speaker agent must describe a route to a target position in a hallway.
7	16	But the pragmatic speaker in this paper, which is capable of reasoning about the listener, chooses to also include additional information (the intersection with the bare concrete hall), to reduce potential ambiguity and increase the odds that the listener reaches the correct destination.
8	43	This same reasoning procedure also allows a listener agent to overcome ambiguity in instructions by reasoning counterfactually about the speaker (Figure 1b).
9	108	Given the command walk along the blue carpet and you pass two objects, a conven- 1951 tional learned instruction-following model is willing to consider all paths that pass two objects, and ultimately arrives at an unintended final position.
10	17	But a pragmatic listener that reasons about the speaker can infer that the long path would have been more easily described as go to the sofa, and thus that the shorter path is probably intended.
13	19	But as the example shows, there are real-world instruction following and generation tasks with rich action spaces that might also benefit from pragmatic modeling.
16	58	The primary contribution of this work is to show how existing models of pragmatic reasoning can be extended to support instruction following and generation for challenging, multi-step, interactive tasks.
25	17	At each time t the agent receives a percept yt, which is a feature-based representation of the current world state, and chooses an action at (e.g. move forward, or turn).
54	75	As a foundation for pragmatic inference, we assume that we have base listener and speaker models to map directions to actions and vice-versa.
58	97	Our pragmatic inference procedure requires these base models to produce candidate outputs from a given input (actions from descriptions, for the listener; descriptions from actions, for the speaker), and calculate the probability of a fixed output given an input, but is otherwise agnostic to the form of the models.
59	65	We use standard sequence-to-sequence models with attention for both the base listener and speaker (described in Section 5).
60	52	Our models use segmented action sequences, with one segment (sub-sequence of actions) aligned with each description sentence dj , for all j ∈ {1 .
98	30	Encoder We encode the sequence of vector embeddings for the actions at and world states yt using a bidirectional LSTM.
117	29	We tune the weight λ in the combined rational agents (L0 · L1 or S0 · S1) to maximize accuracy (for listener models) or BLEU (for speaker models) on each domain’s development data.
119	23	SAIL We follow standard cross-validation evaluation for the instruction following task on the SAIL dataset (Artzi and Zettlemoyer, 2013; Artzi a red guy appears on the far left then to orange’s other side et al., 2014; Mei et al., 2016).5 Table 1 shows improvements over the base listener L0 when using the rational listener L0 · L1 in the single- and multi-sentence settings.
122	32	The rational model improves on the published results of Mei et al. (2016), and while it is still below the systems of Artzi and Zettlemoyer (2013) and Artzi et al. (2014), which use additional supervision in the form of hand-annotated seed lexicons and logical domain representations, it approaches their results in the single-sentence setting.
126	61	We see gains from the rational system L0 · L1 in both the Alchemy and Scene domains.
127	60	The pragmatic inference procedure allows correcting errors or overly-literal interpretations from the base listener.
128	18	An example is shown in Figure 3.
129	40	The base listener (left) interprets then to orange’s other side incorrectly, while the rational listener discounts this interpretation (it could, for example, be better described by to the left of blue) and produces the action the descriptions were meant to describe (right).
130	134	To the extent that human annotators already account for pragmatic effects when generating instructions, examples like these suggest that our model’s explicit reasoning is able to capture interpretation behavior that the base sequence-tosequence listener model is unable to model.
131	63	As our primary evaluation for the instruction generation task, we had Mechanical Turk workers carry out directions produced by the speaker models (and by other humans) in a simulated version of each domain.
138	25	In the SAIL evaluation, we also include the directions produced by the system of Daniele et al. (2017) (DBW), and find that the rational speaker’s directions are followable to comparable accuracy.
141	36	Qualitatively, the rational inference procedure is most successful in fixing ambiguities in the base speaker model’s descriptions.
143	54	The base speaker correctly describes that the shape should be added back, but does not specify where to add it, which could lead a listener to add it in the same position it was deleted.
144	27	The human speaker also makes this mistake in their description.
145	50	This speaks to the difficulty of describing complex actions pragmatically even for humans in the Tangrams domain.
146	18	The ability of the pragmatic speaker to produce directions that are easier to follow than humans’ in this domain (Table 3) shows that the pragmatic model can generate something different (and in some cases better) than the training data.
148	84	We want to verify that a rational listener using n ensembled base listeners and n base speakers outperforms a simple ensemble of 2n base listeners (and similarly for the rational speaker).
153	26	We find that pragmatics improves upon the performance of the base models for both tasks, in most cases substantially.
