18	1	And does that justification point us towards new algorithms with better characteristics?
37	1	Under CNN-ResNet, a faster convergence for BoostResNet is observed.
38	1	One of the hallmarks of our approach is to make an explicit distinction between the classes of the multiclass learning problem and channels that are constructed by the learning procedure.
70	1	We denote by S = ((x1, y1), (x2, y2), .
74	1	Let each module map its input x̃ to ft(x̃) where t denotes the level of the modules.
91	1	More importantly, we are interested in proposing a new algorithm that avoids end-to-end back-propagation (e2eBP) through the deep network and thus is immune to the instability of SGD for non-convex optimization of deep neural networks.
99	1	Now the input, gt+1(x), of the t + 1-th residual block is the output, ft(gt(x)) + gt(x), of the t-th residual block.
102	1	We design a weak module classifier using the idea of telescoping sum as follows.
116	1	We restrict to bounded hypothesis modules, i.e., |ot(x)| ≤ 1.
118	1	Let γ̃t def = Ei∼Dt−1 [yiot(xi)] > 0 be the edge of the hypothesis module ot(x), where Dt−1 is the weight of the examples.
133	1	When γ̃t is close to 1, γ̃2t+1 only needs to be slightly better than γ̃2t as the denominator 1− γ̃2t is small.
136	1	In particular, we introduce a training procedure for deep ResNet in Algorithm 1 & 2, BoostResNet, which only requires sequential training of shallow ResNets.
143	1	This is the fundamental difference between BoostResNet and AdaNet.
144	1	AdaNet (Cortes et al., 2016) maps the feature vectors (hidden layer representations) to a classifier space and boosts the weak classifiers.
147	1	[ Training error bound ] The training error of a T -module telescoping sum boosting framework using Algorithms 1 and 2 decays exponentially with the number of modules T , Pr i∼S ( σ̃ (∑ t ht (xi) ) 6= yi ) ≤ e− 12Tγ 2 if ∀t ∈ [T ] the weak module classifier ht(x) satisfies the γ-weak learning condition defined in Definition 4.1.
156	1	Computational & Memory Efficiency BoostResNet training is memory efficient as the training process only requires parameters of two consecutive residual blocks to be in memory.
163	1	For simplicity, we consider MLP-ResNet with n multiple channels and assume that the weight vector connecting a neuron at layer t with its preceding layer neurons is l1 norm bounded by Λt,t−1.
179	1	All models are trained using the Adam variant of SGD (Kingma & Ba, 2014).
180	1	Hyperparameters are selected via random search for highest accuracy on a validation set.
187	1	Surprisingly, we find that training error degrades for e2eBP, although the ResNet’s identity loop is supposed to alleviate this problem.
188	1	Our pro- posed sequential training procedure, BoostResNet, relieves gradient instability issues, and continues to perform well as depth increases.
191	1	We fit a 50-layer, 25-residual-block CNN-ResNet using both BoostResNet and e2eBP (figure 3a).
198	1	We again fit a 50-layer, 25- residual-block CNN-ResNet using both BoostResNet and e2eBP (figure 3b).
201	1	We find that the test accuracy of the e2eBP refined BoostResNet to be slightly lower than that produced by e2eBP.
213	1	To list the hyperparameters we use in our BoostResNet training after searching over candidate hyperparamters, we optimize learning rate to be 0.004 with a 9 × 10−5 learning rate decay.
220	1	The gamma threshold is optimized to be 0.007 and the initial gamma value on CIFAR-10 is 0.93.
224	1	More importantly, the memory required by BoostResNet is trivial compared to end-to-end back-propagation.
227	9	For instance, our learning framework is amenable to take weak learning oracles using tensor decomposition techniques.
228	53	Tensor decomposition, a spectral learning framework with theoretical guarantees, is applied to learning one layer MLP in (Janzamin et al., 2015).
229	53	We plan to extend our learning framework to non-differentiable data using general weak learning oracles.
