22	23	We then incorporate these embeddings into an attention-based neural network model, called SOCIAL ATTENTION, which employs multiple basis models to focus on different regions of the social network.
30	12	Our training and development datasets lack some of the original Twitter messages, which may have been deleted since the datasets were constructed.
48	131	In this section, we describe a neural network method that leverages social network information to improve text classification.
49	66	Our approach is inspired by ensemble learning, where the system prediction is the weighted combination of the outputs of several basis models.
50	61	We encourage each basis model to focus on a local region of the social network, so that classification on socially connected individuals employs similar model combinations.
51	38	Given a set of instances {xi} and authors {ai}, the goal of personalized probabilistic classification is to estimate a conditional label distribution p(y | x, a).
52	17	For most authors, no labeled data is available, so it is impossible to estimate this distribution directly.
54	9	This idea is put into practice by modeling the conditional label distribution as a mixture over the predictions of K basis classifiers, p(y | x, a) = K∑ k=1 Pr(Za = k | a,G)× p(y | x, Za = k).
57	10	The basic intuition is that for a pair of authors ai and aj who are nearby in the social network G, the prediction rules should behave similarly if the attentional distributions are similar, p(z | ai, G) ≈ p(z | aj , G).
58	12	If we have labeled data only for ai, some of the personalization from that data will be shared by aj .
73	11	We use one convolutional layer and one max pooling layer to generate the sentence representation of x.
77	29	(4) To obtain the conditional label probability, we utilize a multiclass logistic regression model, Pr(Y = t | x, Z = k) = exp(β > t sk + βt)∑T t′=1 exp(β > t′ sk + βt′) , (5) where βt is an m dimensional weight vector, βt is the corresponding bias term, and sk is the m dimensional sentence representation produced by the k-th basis model.
79	12	Let Θ denote the parameters that need to be learned, which include {WL,WR,b, {βt, βt}Tt=1} for every basis CNN model, and the attentional weights {φk, bk}Kk=1.
80	28	We minimize the following logistic loss objective for each training instance: `(Θ) = − T∑ t=1 1[Y ∗ = t] log Pr(Y = t | x, a), (6) where Y ∗ is the ground truth class for x, and 1[·] represents an indicator function.
86	9	Ideally, dead basis models will be avoided because each basis model should focus on a unique region of the social network.
92	15	During pretraining, we train the k-th basis model by optimizing the following loss function for every instance: `k = −αa,k T∑ t=1 1[Y ∗ = t] log Pr(Y = t | x, Za = k).
93	9	(8) The pretrained basis models are then assembled to- gether and jointly trained using Equation 6.
101	12	Unfortunately, these networks are relatively sparse, with a large amount of isolated author nodes.
102	9	To improve the quality of the author embeddings, we expand the set of author nodes by adding nodes that do the most to densify the author networks: for the follower network, we add additional individuals that are followed by at least a hundred authors in the original set; for the mention and retweet networks, we add all users that have been mentioned or retweeted by at least twenty authors in the original set.
103	16	The statistics of the resulting networks are presented in Table 2.
106	29	We report the same evaluation metric as the SemEval challenge: the average F1 score of positive and negative classes.8 Competitive systems We consider five competitive Twitter sentiment classification methods.
111	22	Finally, we include SOCIAL ATTENTION, the attention-based neural network method described in § 4.
142	14	To measure the level of model-specificity for each word w, we compute the difference between the model-specific probabilities p(y | X = w,Z = k) and the average probabilities of all basis models 1 K ∑K k=1 p(y | X = w,Z = k) for positive and negative classes.
144	9	As shown in Table 5, Twitter users corresponding to basis models 1 and 4 often use some words ironically in their tweets.
145	16	Basis model 3 tends to assign positive sentiment polarity to swear words, and Twitter users related to basis model 5 seem to be less fond of fans of certain celebrities.
151	9	To select examples from this dataset, we first removed reviews that were marked by readers as “not useful.” We treated reviews with more than three stars as positive, and less than three stars as negative; reviews with exactly three stars were removed.
154	27	We utilize 145,828 trust relations between 18,999 Ciao users to train the author embeddings.
196	28	Our model achieves significant improvements over standard convolutional networks, and ablation analyses show that social network information is the critical ingredient.
197	47	In other work, language variation has been shown to pose problems for the entire NLP stack, from part-of-speech tagging to information extraction.
198	58	A key question for future research is whether we can learn a socially-infused ensemble that is useful across multiple tasks.
