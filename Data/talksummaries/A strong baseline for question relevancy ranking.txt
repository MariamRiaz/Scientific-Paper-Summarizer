0	17	Community question-answer fora are great resources, collecting answers to frequently and lessfrequently asked questions on specific topics, but these are often not moderated and contain many irrelevant answers.
1	61	Community Question Answering (CQA), cast as a question relevancy ranking problem, was the topic of two shared tasks at SemEval 2016-17.
2	68	This is a non-trivial retrieval task, typically evaluated using mean average precision (MAP).
3	22	We present a strong baseline for this task, on par with or surpassing state-of-the-art systems.
5	10	In this study, we focus on the core subtask of Question-Question similarity, defined as follows: Given a question, rank other relevant questions by their relevancy to that question.
9	11	We use a question-answer dataset as auxiliary task; but we also experiment with datasets for pairwise classification tasks such as natural language inference and fake news detection.
10	3	This simple, easy-totrain model is on par or better than state-of-theart systems for question relevancy ranking.
35	41	Natural Language Inference Natural Language Inference (NLI), consists in predicting ENTAILMENT, CONTRADICTION or NEUTRAL, given a hypothesis and a premise.
36	3	We use the MNLI dataset as opposed to the SNLI data (Bowman et al., 2015; Nangia et al., 2017), since it contains different genres.
39	6	This task has been used before in a multi-task setting as a way to utilize general information about pairwise relations (Augenstein et al., 2018).
40	12	Formally, the FNC task consists in, given a headline and the body of text which can be from the same news article or not, classify the stance of the body of text relative to what is claimed in the headline.
41	6	There are four labels: • AGREES: The body of the article is in agreement with the headline • DISAGREES: The body of the article is in dis- agreement with the headline • DISCUSSES: The body of the article does not take a position • UNRELATED: the body of the article dis- cusses a different topic We include fake news detection as a weak auxiliary signal that can lead to better generalization of our question-question ranking model.
46	4	For the SemEval-16 data, our multitask MLP architecture with a question-answer auxiliary task performed best on all metrics, except accuracy, where the multi-task MLP using all auxiliary tasks performed best.
60	11	For a more direct comparison, we also train a more expressive model than the simple MTLbased model we propose.
62	6	For this model, we input sequences of embedded words (using pre-trained word embeddings) from each query into independent BiLSTM blocks and output a vector representation for each query.
63	58	We then concatenate the vector representations with the similarity features from our MTL model and feed it into a dense layer and a classification layer.
64	2	This way we can evaluate the usefulness of the flexible, expressive LSTM network directly (as our MTL model becomes an ablation instance of the full, more complex architecture).
65	21	We use the same dropout regularization and SGD values as for the MLP.
66	8	Tuning all parameters on the development data, we do not manage to outperform our proposed model, however.
67	61	See lines MTL-LSTM-SIM in Table 1 for results.
68	39	We show that simple feature engineering, combined with an auxiliary task and a simple feedfor- ward neural architecture is appropriate for a small dataset and manages to beat the baseline and the best performing systems for the Semeval task of question relevancy ranking.
69	74	We observe that introducing pairwise classification tasks leads to significant improvements in performance and a more stable model.
70	43	Overall, our simple model introduces a new strong baseline which is particularly useful when there is a lack of labeled data.
