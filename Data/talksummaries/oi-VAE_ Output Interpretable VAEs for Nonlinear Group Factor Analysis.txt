28	25	There are many applications where this is the case.
29	35	In the analysis of neuroimaging data, studies are typically done at the level of regions of interest that aggregate over corticallylocalized signals.
37	16	In these scenarios where there is a natural notion of groupings of observations, we demonstrate the interpretability of the learned features and how these structures of interaction correspond to physically meaningful systems.
38	37	Furthermore, in such cases we show that the regularization employed by oi-VAE leads to better generalization and synthesis capabilities, especially in limited training data scenarios or when the training data might not fully capture the observed space of interest.
39	23	In addition, we found that oi-VAE produces unconditional samples that are qualitatively superior to standard VAEs due to oi-VAE’s bias towards disentangled representations in the latent space.
45	10	Despite the resemblance to autoencoding models (Ballard, 1987)— especially in the age of “disentanglement”—little work exists exploring connections between the two.
47	18	The variational autoencoder (VAE) (Kingma & Welling, 2013) is one such example that efficiently trains a generative model via amortized inference (see also Rezende et al., 2014).
48	58	Though deep generative models like the VAE have demonstrated an ability to produce convincing samples of complex data (cf., Archer et al., 2015; Johnson et al., 2017), the learned latent representations are not readily interpretable due to the entangled interactions between latent dimensions and the observations, as depicted in Fig.
49	33	We further review the VAE specification in Sec.
51	11	A common approach to encourage simple and interpretable models is through use of sparsity inducing penalties such as the lasso (Tibshirani, 1994) and group lasso (Yuan & Lin, 2006).
69	12	Let x ∈ RD denote a D-dimensional observation and z ∈ RK denote the associated latent representation of fixed dimension K. We then write the generative process of the model as: z ∼ N (0, I) (1) x ∼ N (fθ(z),D), (2) where D is a diagonal matrix containing the marginal variances of each component of x.
75	49	We model the components within each group g ∈ [G] with separate generative networks f (g)θg parameterized by θg.
77	14	Critically, the latent representation z is shared across all of the group-specific generators.
79	41	However, one of the primary goals of this framework is to capture interpretable relationships between groups through the latent representation.
81	52	Specifically, we insert a groupspecific linear transformation W(g) ∈ Rp×K between the latent representation z and the group generator f (g): x(g) ∼ N (f (g)θ (W (g)z),Dg).
82	17	(5) We refer to W(g) as the latent-to-group matrix.
83	17	For simplicity, we assume that each generator has input dimension p. When the jth column of the latent-to-group matrix for group g, W(g)·,j , is all zeros then the jth latent dimension, zj , will have no influence on group g in the generative process.
86	38	Marginalizing over γ2gj induces group sparsity over the columns of W(g); the MAP of the resulting posterior is equivalent to a group lasso penalized objective (Kyung et al., 2010).
87	39	Unlike linear factor models, the deep structure of our model allows rescaling of the parameters across layer boundaries without affecting the end behavior of the network (Neyshabur et al., 2015).
88	55	In particular, it is possible— and in fact encouraged behavior—to learn a set of W(g) matrices with very small weights and a subsequent layer with very large weights that nullify the shrinkage imposed by the sparsity-inducing prior.
89	51	In order to mitigate this we additionally place a standard normal prior with fixed scale on the parameters of each generative network, θg ∼ N (0, I).
91	14	When we treat the observations as forming a single group, the model resembles a traditional VAE since there is a single generator.
101	46	For example, in Table 1 we see that each of the dimensions of the latent space learned on motion capture recordings of human motion corresponds to a direction of variation relevant to only a subset of the joints (groups) that are used in specific submotions related to walking.
102	22	Additionally, it is observed that although the VAE and oi-VAE have similar reconstruction performance the meaningfully disentangled latent representation allows oi-VAE to produce superior unconditional random samples.
107	15	Then networks can be identified by inspecting the subset of groups influenced by a component in the latent code, zi.
108	11	Such an approach is attractive in the context of analyzing functional connectivity from MEG data where we seek modules of highly correlated regions.
109	15	Likewise, in our motion capture experiments of Sec.
114	51	Our hope is that oi-VAE serves as one initial tool to spark a new wave of interest in nonlinear factor models and their application to complicated and rich data across a variety of fields.
116	21	Sparsity in z may be desirable in certain contexts, but it does not actually provide any interpretability in the data generating process.
120	26	(1) makes the expectation in the ELBO intractable.
