{"sections": [{"heading": "1. Introduction", "text": "Finding statistically reliable high-order interaction features in predictive modeling has been important challenging task. For example, in a biomedical study, co-occurrence of multiple mutations in multiple genes may have a significant influence on a response to a drug even if occurrence of single mutation in each of these genes has no influence (Manolio & Collins, 2006; Cordell, 2009). A major challenge in prediction modeling with high-order interaction features is the exponentially expanded feature space. If one has a dataset with d original variables and takes into account interactions up to order r, the model has D := \u2211r \u03c1=1 ( d \u03c1\n) features (e.g., for d = 10, 000, r = 5, D > 1017). Unless both d and r are fairly small, D is extremely large. Feature selection and statistical inference in such an extremely high-dimensional model are challenging both computationally and statistically.\nA common approach to high-dimensional modeling is to consider a sparse model, i.e., a model only with a selected\n1Nagoya Institute of Technology, Nagoya, Japan 2University of Tokyo, Tokyo, Japan 3RIKEN, Tokyo, Japan. Correspondence to: Ichiro Takeuchi <takeuchi.ichiro@nitech.ac.jp>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nsubset of features. In the past two decades, considerable amount of studies have been done on sparse modeling and feature selection in high-dimensional models. In these studies, a variety of feature selection algorithms such as marginal screening (Fan & Lv, 2008), orthogonal matching pursuit (Pati et al., 1993), LASSO (Tibshirani, 1996), and their various extensions have been developed. On the other hand, statistical inference for sparse models (hypothesis testing or confidence interval computation of the fitted coefficients) have not been deeply studied until very recently. The main challenge in statistical inference of sparse models is that, if the data is used for selecting a subset of features, this selection event must be taken into account in the following inference stage. Otherwise, the inference results are distorted by so-called selection bias, and false positive errors cannot be controlled at desired levels. This problem is refereed to as selective inference or post selection inference (Benjamini & Yekutieli, 2005; Benjamini et al., 2009; Berk et al., 2013). After the seminal work by Lee et al. (2016), significant progress has been recently made on selective inference for sparse linear models (Fithian et al., 2014b; Lee & Taylor, 2014; Fithian et al., 2015; Tian & Taylor, 2015; Taylor & Tibshirani, 2016; Yang et al., 2016; Barber & Cande\u0300s, 2016).\nIn this paper, we study feature selection and statistical inference for sparse high-order interaction models. Unfortunately, neither existing feature selection methods nor existing selective inference methods can be applied to sparse high-order interaction models because the computational costs of these existing methods at least linearly depend on the number of features D. The main contribution in this paper is to develop computationally efficient algorithms for these two tasks when the original variables are represented in [0, 1]d. Our main idea is to exploit the underlying tree structure of high-order interaction features as depicted in\nFigure 1. In feature selection tasks, it allows us to efficiently identify interaction features that have no chance to be selected. In statistical inference tasks, it allows us to efficiently identify interaction features that do not affect the results of the selective inference.\nWe demonstrate the effectiveness of the proposed methods through numerical experiments both on synthetic and real datasets. In the latter, we apply the proposed method to HIV dataset in (Rhee et al., 2003), where the goal is to identify statistically significant high-order interactions of multiple gene mutations that are significantly associated with HIV drug responses.\nRelated works and our contributions Methods for efficiently finding high-order interaction features and properly evaluating their statistical significances have long been desired in many scientific studies.\nIn the past decade, feature selection for interaction models has been studied in the context of sparse learning (Choi et al., 2010; Hao & Zhang, 2014; Bien et al., 2013). None of these works have a special computational trick for handling exponentially large number of interaction features, which makes their empirical evaluations restricted up-to second order interactions. One commonly used heuristic in the context of interaction modeling is to introduce a prior knowledge such as strong heredity assumption where, e.g., an interaction term z1z2 would be selected only when both of z1 and z2 are selected. Such a heuristic restriction is helpful for reducing the number of interaction terms to be considered. However, in many scientific studies, researchers are primarily interested in finding interactions even when their main effects alone do not have any association with the response. The idea of considering a tree structure among interaction features has been commonly used n data mining literature (Kudo et al., 2005; Saigo et al., 2006; Nakagawa et al., 2016). However, it is difficult to properly assess the statistical significances of the selected features by these mining techniques.\nOne traditional approach to assessing the statistical significances of selected features is multiple testing correction (MTC). In the context of DNA microarray studies, many MTC procedures for high-dimensional data have been proposed (Tusher et al., 2001; Dudoit et al., 2003). An MTC approach for statistical evaluation of high-order interaction features was recently studied in (Terada et al., 2013; Llinares-Lo\u0301pez et al., 2015). A main drawback of MTC is that they are highly conservative when the number of candidate features increases. Another common approach is datasplitting (DS). (Fithian et al., 2014a). In DS approach, we split the data into two subsets, and use one for feature selection and another for statistical inference, which enables us to remove the selection bias. However, performances\nof DS approach is clearly weak both in selection and inference stages because only a part of the available data is used in each stage. In addition, it is quite annoying that different set of features would be selected if data is splitted differently. Recently, much attention has been paid to selective inference for sparse linear models. The basic idea of selective inference is to make inferences conditional on a feature selection event. Lee et al. (2016) recently proposed a practical selective inference framework for a class of feature selection algorithms.\nThe main contribution in this paper is to extend the selective inference framework into sparse high-order interaction models by introducing novel computational algorithms. To the best of our knowledge, there are no other existing works for sparse high-order interaction models in which the statistical significances of the fitted coefficients are properly evaluated in non-asymptotic sense.\nNotations We use the following notations in the remainder. For any natural number n, we define [n] := {1, . . . , n}. A vector and a matrix is denoted such as v \u2208 Rn and M \u2208 Rn\u00d7m, respectively. The index function is written as 1{z} which returns 1 if z is true, and 0 otherwise. The sign function is written as sgn(z) which returns 1 if z \u2265 0, and \u22121 otherwise. An n \u00d7 n identity matrix is denoted as In."}, {"heading": "2. Preliminaries", "text": ""}, {"heading": "2.1. Problem setup", "text": "Consider a regression problem with a response Y \u2208 R and d-dimensional original covariates z = [z1, . . . , zd]> by the following high-order interaction model up to r-th order\nY = \u2211 j1\u2208[d] \u03b1j1zj1 + \u2211\n(j1,j2)\u2208[d]\u00d7[d] j1 6=j2\n\u03b1j1,j2zj1zj2\n+ \u00b7 \u00b7 \u00b7+ \u2211\n(j1,...,jr)\u2208[d]r j1 6=...6=jr\n\u03b1j1,...,jrzj1 \u00b7 \u00b7 \u00b7 zjr + \u03b5, (1)\nwhere \u03b1s are the coefficients and \u03b5 is a random noise. We assume that each original covariate zj , j \u2208 [d] is defined in a domain [0, 1]. Here, values 1 and 0 respectively might be interpreted as the existence and the non-existence of a certain property, and values between them indicate the \u201cdegree\u201d of existence. High-order interaction features thus represent co-existence of multiple properties. For example, if we are interested in interactions among age, body mass index (BMI), and a mutation in a certain gene, we may code\nsome covariates as\nzj1 :=  1 if BMI > 30,(BMI\u2212 15)/(30\u2212 15) if BMI \u2208 [15, 30], 0 if BMI < 15,\nzj2 := 1{mutation in the gene}.\nThen, e.g., an interaction term zj1zj2 represents the coexistence of high BMI and a mutation in the gene. The high-order interaction model Eq.(1) has in total D :=\u2211 \u03c1\u2208[r] ( d \u03c1 ) features. Let us write the mapping from the original covariates z := [z1, . . . , zd]> \u2208 Rd to the highorder interaction features x := [x1, . . . , xD]> \u2208 RD as \u03c6 : [0, 1]d \u2192 [0, 1]D, z 7\u2192 x,, i.e.,\nx := \u03c6(z) = [z1, . . . , zd, z1z2, . . . , zd\u22121zd,\n. . . , z1 \u00b7\u00b7\u00b7zk, . . . , zd\u2212r+1 \u00b7\u00b7\u00b7zd]>\nThen, the high-order interaction model Eq.(1) is simply written as a D-dimensional linear model\ny = \u03b2>x = \u03b21x1 + \u00b7 \u00b7 \u00b7+ \u03b2DxD,\nwhere \u03b21, . . . , \u03b2D are D coefficients corresponding to \u03b1j1 , . . . , \u03b1j1,...,jr in Eq.(1). Since a high-order interaction feature is a product of original covariates defined in [0, 1], the range of each feature xj , j \u2208 [D] is also [0, 1].\nThe original training set is denoted as {(zi, yi) \u2208 [0, 1]d \u00d7 R}i\u2208[n], while the expanded training set is written as {(xi, yi) \u2208 [0, 1]D \u00d7R}i\u2208[n]. The latter is also denoted as (X,y) \u2208 [0, 1]n\u00d7D \u00d7Rn where each row of X is xi \u2208 Rd and each element of y is yi. Furthermore, the j-th column of X is written as x\u00b7j , j \u2208 [D]. We denote the pseudo inverse of X as X+ := (X>X)\u22121X>.\nOur goal is to identify statistically significant high-order interaction terms that have large impacts on the response Y by identifying regression coefficients \u03b1s which are significantly deviated from zero. Unfortunately, since the number of coefficients \u03b1s to be fitted would be far greater than the sample size n, traditional least-square estimation theory cannot be used for making statistical inferences on the fitted model. We thus consider first to perform feature selection and then to make statistical inference only for the selected features based on selective inference approach."}, {"heading": "2.2. Selective inference for sparse linear models", "text": "In this section, we briefly review the selective inference framework for sparse linear models developed by Lee et al. (2016). Selective inference is developed for two stage methods, where a subset of features is selected in the first stage, and inferences are made only on the selected features in the second stage. A key finding by Lee et al. (2016) is\nthat, if the first selection stage is described as a linear selection event, then exact statistical inference of the fitted coefficients conditional on the selection event can be done.\nConsider a linear regression model y = X\u03b2\u2217 + \u03b5, where \u03b2\u2217 \u2208 RD is the true coefficients and \u03b5 is distributed according to N(0, \u03c32I) with known variance \u03c32.\nFeature selection stage Suppose that, in the first feature selection stage, a subset of features S \u2286 [D] are selected. The selective inference framework in Lee et al. (2016) can be applied to feature selection algorithms whose selection process can be characterized by a set of linear inequalities in the form of Ay \u2264 b with a certain matrix A and a certain vector b that do not depend on y. This type of selection event is called a linear selection event. In the selective inference framework, inferences are made conditional on the selection event. It means that, in the case of a linear selection event, we only care about the cases where y is observed in a polytope Pol(S) := {y \u2208 Rn | Ay \u2264 b}. In Lee & Taylor (2014) and Lee et al. (2016), marginal screening, OMP and LASSO are shown to be linear selection events, indicating that the selective inference framework can be applied to statistical testing of the selected features by these algorithms.\nStatistical inference stage Consider a hypothesis testing for the j-th selected feature in S\nH0,j : \u03b2 \u2217 S,j = 0 vs. H1,j : \u03b2 \u2217 S,j 6= 0. (2)\nThe least-square estimator of the linear model only with the selected features S is written as \u03b2\u0302S = (X>S XS) \u22121X>S y.\nIf we consider the case where S is NOT selected from the data, i.e., independent of y, then, under the null hypothesis H0, the sampling distribution of each fitted coefficient is\n\u03b2\u0302S,j \u223c N(0, \u03c32S,j), where \u03c32S,j := \u03c32(X>S XS)\u22121jj . (3)\nFor two-sided test at level \u03b1, if the critical values `\u03b1/2 and u\u03b1/2 are chosen to be the lower and the upper \u03b1/2 points of the sampling distribution in Eq.(3), then the type I error at level \u03b1 is controlled as\nPr(\u03b2\u0302S,j /\u2208 [`\u03b1/2, u\u03b1/2]) \u2264 \u03b1 (4)\nOn the other hand, when S is selected from the data as we consider here, we would like to control the following selective type I error\nPr(\u03b2\u0302S,j /\u2208 [`(S,j)\u03b1/2 , u (S,j) \u03b1/2 ] | {S is selected})\n=Pr(\u03b2\u0302S,j /\u2208 [`(S,j)\u03b1/2 , u (S,j) \u03b1/2 ] | y \u2208 Pol(S)) \u2264 \u03b1 (5)\nby appropriately selecting the adjusted critical values `(S,j)\u03b1/2 and u(S,j)\u03b1/2 , where the selection event {S is selected} is\nwritten as y \u2208 Pol(S) in the case of a linear selection event. Lee et al. (2016) derived how to compute these adjusted critical values as formally stated in the following lemma.\nLemma 1. If the critical values are computed as\n` (S,j) \u03b1/2 := (F [L(S,j),U(S,j)] 0,\u03c32S,j )\u22121(\u03b1/2), (6a)\nu (S,j) \u03b1/2 := (F [L(S,j),U(S,j)] 0,\u03c32S,j )\u22121(1\u2212 \u03b1/2), (6b)\nthen the selective type I error is controlled as in Eq. (5), where F [L,U ]\u00b5,\u03c32 is the cumulative distribution function of a truncated Normal distribution TN(\u00b5, \u03c32, L, U), i.e.,\nF [L,U ] \u00b5,\u03c32 (x) = \u03a6((x\u2212 \u00b5)/\u03c3)\u2212 \u03a6((L\u2212 \u00b5)/\u03c3) \u03a6((U \u2212 \u00b5)/\u03c3)\u2212 \u03a6((L\u2212 \u00b5)/\u03c3) ,\nand the truncation points are obtained, by using the observed \u03b2\u0302S,j and y, as\nL(S, j) = \u03b2\u0302S,j + \u03b8L(X > S XS) \u22121 jj , (7a)\nwhere \u03b8L := min \u03b8\u2208R\n\u03b8 s.t. y + \u03b8(X+S ) >ej \u2208 Pol(S),\nU(S, j) = \u03b2\u0302S,j + \u03b8U (X > S XS) \u22121 jj , (7b)\nwhere \u03b8U := max \u03b8\u2208R\n\u03b8 s.t. y + \u03b8(X+S ) >ej \u2208 Pol(S).\nThe proof of Lemma 1 is is presented in Appendix A although it is easily proved by using the results in Lee et al. (2016). See Lee et al. (2016) for more general statement about the selective inference framework.\nEq.(7) indicates that the truncation points are obtained by considering the interval where the test statistic \u03b2\u0302S,j can move within the polyhedron Pol(S). Figure 2 schematically illustrates that, when we make inferences conditional on a linear selection event S, the sampling distribution is defined within the polytope Pol(S), and it follows a truncated normal distribution when y is normally distributed.\nUnfortunately, we cannot directly apply this selective inference framework to high-order interaction models because the polytope Pol(S) is characterized by extremely large number of linear inequalities, and the optimization problems in Eq.(7) are hard to solve."}, {"heading": "3. Feature selection for interaction models", "text": "In this section, we present two feature selection algorithms for high-order interaction models. Since the number of features D is extremely large, existing feature selection algorithms for linear models cannot be directly applied to interaction models. In this paper, we study marginal screening (MS) and orthogonal matching pursuit (OMP) as examples of feature selection algorithms."}, {"heading": "3.1. MS for interaction models", "text": "Consider selecting the top k interaction features from all the D interaction features that have marginal strong correlations with the response. Noting that each feature is defined in [0, 1] and the value indicates (the degree of) the existence of a certain property, we consider a score x>\u00b7jy, j \u2208 [D] for each of the D features, and select the top k features according to their absolute scores |x>\u00b7jy|. We denote the index set of the selected k features by S, and that of the unselected k\u0304 := D \u2212 k features by S\u0304 := [D] \\ S.\nSince D is extremely large, we cannot compute the score for each interaction feature. We exploit the tree structure among interaction patterns as depicted in Figure 1.\nDefinition 2. (Descendant features) For each j \u2208 [D], let Des(j) \u2286 [D] be the set of features corresponding to the descendant nodes in the tree including j itself.\nLemma 3. Consider an interaction feature x\u00b7j , j \u2208 [D], whose indices are represented in a tree structure as depicted in Figure1. Then, for any node j \u2208 [D] in the tree,\n|x\u00b7j\u0303y| \u2264 max  \u2211 i:yi>0 xijyi,\u2212 \u2211 i:yi<0 xijyi  (8) for all j\u0303 \u2208 Des(j).\nThe proof of Lemma 3 is presented in Appendix A. Lemma 3 tells that, for a descendant feature x\u00b7j\u0303 , (j, j\u0303) \u2208 S \u00d7 Des(j), an upper bound of the absolute score |x>\u00b7j\u0303y| can be computed based on its parent feature x\u00b7j .\nWe note that this simple upper bound has been used in some data mining studies such as Saigo et al. (2006); Kudo et al.\n(2004); Nakagawa et al. (2016). When we search over the tree, if the upper bound in Eq.(8) is smaller than the current k-th largest score at a certain node j, then we can quit searching over its descendant nodes j\u0303 \u2208 Des(j).\nAs pointed out in Lee & Taylor (2014), feature selection processes of marginal screening is a linear selection event, i.e., characterized by a set of linear constraints. The event that k features in S are selected, and k\u0304 features in S\u0304 are not selected is rephrased as |x>\u00b7jy| \u2265 |x>\u00b7j\u2032y|, \u2200 (j, j\u2032) \u2208 S \u00d7 S\u0304. Let sj := sgn(x>\u00b7jy), j \u2208 S. Then, the above feature selection event is rewritten with the sign constraints of the selected features by the following 2kk\u0304+k constraints\n(\u2212sjx\u00b7j \u2212 x\u00b7j\u2032)>y \u2264 0, \u2200 (j, j\u2032) \u2208 S \u00d7 S\u0304, (9a) (\u2212sjx\u00b7j + x\u00b7j\u2032)>y \u2264 0, \u2200 (j, j\u2032) \u2208 S \u00d7 S\u0304, (9b)\n\u2212sjx>\u00b7jy \u2264 0, \u2200 j \u2208 S. (9c)\nThese constraints are written as Ay \u2264 0 with a matrix A \u2208 R(2kk\u0304+k)\u00d7n. Unfortunately, finding \u03b8min and \u03b8max by naively solving the optimization problems in Eq.(7) is computationally difficult because the polyhedron Pol(S) is characterized by the extremely large number of constraints. For example, when d = 10, 000, r = 5, k = 10, the number of linear inequalities that defines the polyhedron Pol(S) is 2kk\u0304 + k > 1019."}, {"heading": "3.2. OMP for interaction models", "text": "Orthogonal matching pursuit (OMP) is a well-known iterative feature selection method (Pati et al., 1993). At each iteration, the most correlated feature with the residual of the current model which is fitted via least-squares method by using the features selected in earlier steps.\nConsider again selecting k interaction features by OMP. Let [(1), . . . , (h)] be the sequence of the indices of the selected features from step 1 to step h for h \u2208 [k], and define Sh := {(1), . . . , (h)}. Before step h + 1, we have already selected h features x\u00b7j , j \u2208 Sh. Using these h features, the current n-dimensional model output is written as\u2211 j\u2208[h] \u03b2\u0302Sh,(j)x\u00b7(j), where the coefficients \u03b2\u0302Sh,(j), j \u2208 [h] are estimated by least-squares method. Denoting by \u0393Sh the n\u00d7hmatrix whose j-th column isx\u00b7(j), the least square estimates are written as \u03b2\u0302Sh := [\u03b2\u0302Sh,(1), . . . , \u03b2\u0302Sh,(h)]\n> = (\u0393Sh)\n+y. Then, at the h + 1 step, we consider the correlation between the residual vector rh := y \u2212 \u0393Sh \u03b2\u0302Sh and a feature x\u00b7j\u2032 for j\u2032 \u2208 S\u0304h, and find the one that maximizes the absolute correlation |x>\u00b7j\u2032rh| among them. Here, since the number of remaining features |S\u0304h| = D \u2212 h is still extremely large, it is hard to compute all these D \u2212 h correlations. To overcome this difficulty, we can simply use Lemma 3 just by replacing y with the current residual rh. Specifically, for a descendant feature x\u00b7j\u0303 , j\u0303 \u2208 Des(j), an\nupper bound of |x>\u00b7j\u0303rh| is given as\n|x>\u00b7j\u0303rh| \u2264 max  \u2211 i:rh,i>0 xijrh,i,\u2212 \u2211 i:rh,i<0 xijrh,i  . At each iteration, when we search over the tree, if the upper bound is smaller than the current largest correlation, then, in the same way as the case of MS, we can quit searching over its descendant nodes j\u2032 \u2208 Des(j).\nIt is also pointed out in Lee & Taylor (2014) that a feature selection process of OMP is linear selection event. At step h, the event that the (h)-th feature is selected is formulated as |x>\u00b7(h)rh| \u2265 |x > \u00b7j\u2032rh|, for all j\u2032 \u2208 S\u0304h. Let PSh := In \u2212 \u0393Sh\u0393 + Sh\n. Then, the above selection event is rewritten as a set of linear inequalities with respect to y\n(\u2212s(h)x\u00b7(h) \u2212 x\u00b7j\u2032)>PShy \u2264 0,\u2200 j\u2032 \u2208 S\u0304h, (10a) (\u2212s(h)x\u00b7(h) + x\u00b7j\u2032)>PShy \u2264 0,\u2200 j\u2032 \u2208 S\u0304h, (10b)\n\u2212s(h)x>\u00b7(h)PShy \u2264 0, (10c)\nwhere s(h) = sgn(x>\u00b7(h)rh). By combining all the linear selection events in k steps, the entire selection event of the OMP is characterized by \u2211 h\u2208[k](2(D \u2212 h) + 1) linear inequalities in Rn. In practice, it is computationally intractable to handle these extremely large number of linear inequalities."}, {"heading": "4. Selective inference for interaction models", "text": "In this section, we present an efficient selective inference algorithm for high-order interaction models, which is our main contribution.\nThe discussion in \u00a73 suggests that it would be hard to compute critical values for selective inference in Eq.(6) because the selection event y \u2208 Pol(S) is characterized by extremely large number of inequalities. Our basic idea for addressing this computational difficulty is to note that most of the inequalities actually do not affect the results of the selective inference, and a large portion of them can be identified by exploiting the anti-monotonicity properties defined in the tree structure among high-order interaction features."}, {"heading": "4.1. Marginal screening", "text": "We consider k trees for each of the k selected features. Each tree consists of a set of nodes corresponding to each of the non-selected features j\u2032 \u2208 S\u0304. For a pair (j, j\u2032) \u2208 S\u00d7 S\u0304, the j\u2032-th node in the j-th tree corresponds to the linear inequalities Eqs.(9a) and (9b). When we search over these k trees, we introduce a novel pruning strategy by deriving a condition such that, if the j\u2032-th node in the j-th tree satisfies certain conditions, then all the (j, j\u0303\u2032)-th inequalities for all\nj\u0303\u2032 \u2208 Desj(j\u2032) are guaranteed to be irrelevant to the selective inference results because they do not affect the optimal solutions in Eq.(7), where we define Desj(j\u2032) be all the features corresponding to the descendant node of j\u2032 in the j-th tree. Lemma 4. Let \u03b7 := (X+S ) >ej . The solutions of the optimization problems in (7) are respectively written as\n\u03b8L = \u2212min{\u03b8(a)L , \u03b8 (b) L , \u03b8 (c) L }, \u03b8U = \u2212max{\u03b8(a)U , \u03b8 (b) U , \u03b8 (c) U },\nwhere\n\u03b8 (a) L := min\n(j,j\u2032)\u2208S\u00d7S\u0304, (sjx\u00b7j+x\u00b7j\u2032 ) >\u03b7>0\n(sjx\u00b7j + x\u00b7j\u2032)>y (sjx\u00b7j + x\u00b7j\u2032)>\u03b7 , (11a)\n\u03b8 (a) U := max\n(j,j\u2032)\u2208S\u00d7S\u0304, (sjx\u00b7j+x\u00b7j\u2032 ) >\u03b7<0\n(sjx\u00b7j + x\u00b7j\u2032)>y (sjx\u00b7j + x\u00b7j\u2032)>\u03b7 , (11b)\n\u03b8 (b) L := min\n(j,j\u2032)\u2208S\u00d7S\u0304, (sjx\u00b7j\u2212x\u00b7j\u2032 )>\u03b7>0\n(sjx\u00b7j \u2212 x\u00b7j\u2032)>y (sjx\u00b7j \u2212 x\u00b7j\u2032)>\u03b7 , (11c)\n\u03b8 (b) U := max\n(j,j\u2032)\u2208S\u00d7S\u0304, (sjx\u00b7j\u2212x\u00b7j\u2032 )>\u03b7<0\n(sjx\u00b7j \u2212 x\u00b7j\u2032)>y (sjx\u00b7j \u2212 x\u00b7j\u2032)>\u03b7 , (11d)\n\u03b8 (c) L := min\nj\u2208S, sjx > \u00b7j\u03b7>0\nsjx > \u00b7jy sjx>\u00b7j\u03b7 , \u03b8 (c) U := max\nj\u2208S, sjx > \u00b7j\u03b7<0\nsjx > \u00b7jy sjx>\u00b7j\u03b7 .\nThe proof of Lemma 4 is presented in Appendix A. Lemma 5. For any triplet (j, j\u2032, j\u0303\u2032) \u2208 S \u00d7 S\u0304 \u00d7Desj(j\u2032),\nL (a) E := sjx > \u00b7jy + \u2211 i:yi<0 xij\u2032yi \u2264 (sjx\u00b7j + x\u00b7j\u0303\u2032) >y, (12a)\nU (a) E := sjx > \u00b7jy + \u2211 i:yi>0 xij\u2032yi \u2265 (sjx\u00b7j + x\u00b7j\u0303\u2032) >y, (12b)\nL (a) D := sjx > \u00b7j\u03b7 + \u2211 i:\u03b7i<0 xij\u2032\u03b7i \u2264 (sjx\u00b7j + x\u00b7j\u0303\u2032) >\u03b7, (12c)\nU (a) D := sjx > \u00b7j\u03b7 + \u2211 i:\u03b7i>0 xij\u2032\u03b7i \u2265 (sjx\u00b7j + x\u00b7j\u0303\u2032) >\u03b7, (12d)\nL (b) E := sjx > \u00b7jy \u2212 \u2211 i:yi>0 xij\u2032yi \u2264 (sjx\u00b7j \u2212 x\u00b7j\u0303\u2032) >y, (12e)\nU (b) E := sjx > \u00b7jy \u2212 \u2211 i:yi<0 xij\u2032yi \u2265 (sjx\u00b7j \u2212 x\u00b7j\u0303\u2032) >y, (12f)\nL (b) D := sjx > \u00b7j\u03b7 \u2212 \u2211 i:\u03b7i>0 xij\u2032\u03b7i \u2264 (sjx\u00b7j \u2212 x\u00b7j\u0303\u2032) >\u03b7, (12g)\nU (b) D := sjx > \u00b7j\u03b7 \u2212 \u2211 i:\u03b7i<0 xij\u2032\u03b7i \u2265 (sjx\u00b7j \u2212 x\u00b7j\u0303\u2032) >\u03b7. (12h)\nThe proof of Lemma 5 is presented in Appendix A.\nTheorem 6. (i) Consider solving the optimization problem in Eq.(11a), and let \u03b8\u0302(a)L be the current optimal solution, i.e., we know that the optimal \u03b8(a)L is at least no greater than \u03b8\u0302(a)L . If\n{U (a)D < 0} \u222a {L (a) D > 0, L (a) E < 0, L (a) E /L (a) D > \u03b8\u0302 (a) L }\n\u222a {L(a)D > 0, L (a) E > 0, L (a) E /U (a) D > \u03b8\u0302 (a) L }\nis true, then the (j, j\u0303\u2032)-th constraint in Eq. (9a) for any (j, j\u2032, j\u0303\u2032) \u2208 S \u00d7 S\u0304 \u00d7Desj(j\u2032) does not affect the optimal solution in Eq.(11a).\n(ii) Next, consider solving the optimization problem in Eq.(11c), and let \u03b8\u0302(b)L be the current optimal solution. If\n{U (b)D < 0} \u222a {L (b) D > 0, L (b) E < 0, L (b) E /L (b) D > \u03b8\u0302 (b) L }\n\u222a {L(b)D > 0, L (b) E > 0, L (b) E /U (b) D > \u03b8\u0302 (b) L }\nis true, then the (j, j\u0303\u2032)-th constraint in Eq. (9b) for any (j, j\u2032, j\u0303\u2032) \u2208 S \u00d7 S\u0304 \u00d7Desj(j\u2032) does not affect the optimal solution in Eq.(11c).\n(iii) Furthermore, consider solving the optimization problem in Eq.(11b), and let \u03b8\u0302(a)U be the current optimal solution. If\n{L(a)D > 0} \u222a {U (a) D < 0, L (a) E < 0, L (a) E /U (a) D < \u03b8\u0302 (a) U }\n\u222a {U (a)D < 0, L (a) E > 0, L (a) E /L (a) D < \u03b8\u0302 (a) U }\nis true, then the (j, j\u0303\u2032)-th constraint in Eq. (9a) for any (j, j\u2032, j\u0303\u2032) \u2208 S \u00d7 S\u0304 \u00d7Desj(j\u2032) does not affect the optimal solution in Eq.(11b).\n(iv) Finally, consider solving the optimization problem in Eq.(11d), and let \u03b8\u0302(b)U be the current optimal solution. If\n{L(b)D > 0} \u222a {U (b) D > 0, L (b) E < 0, L (b) E /U (b) D < \u03b8\u0302 (b) U }\n\u222a {U (b)D > 0, L (b) E < 0, L (b) E /L (b) D < \u03b8\u0302 (b) U }\nis true, then the (j, j\u0303\u2032)-th constraint in Eq. (9b) for any (j, j\u2032, j\u0303\u2032) \u2208 S \u00d7 S\u0304 \u00d7Desj(j\u2032) does not affect the optimal solution in Eq.(11d).\nThe proof of Theorem 6 is presented in Appendix. Note that all the conditions in Theorem 6 can be checked at the j\u2032-th node in each tree. If the conditions are satisfied as the j\u2032-th node, then one can skip searching over its subtree. It allows us to perform selective inference for highorder interaction models even the number of constraints that defines the selection event is extremely large. As we demonstrate in the experiment section, these pruning conditions are quite effective in practice. For example, we can perform selective inference for an interaction models with d = 10, 000, r = 5, k = 10 in a few seconds."}, {"heading": "4.2. Orthogonal matching pursuit (OMP)", "text": "As we discuss in the previous section, the selection event at each iteration of OMP has same form as MS. Therefore, we can derive similar pruning conditions as in Theorem 6 for OMP. Due to the space limitation, we deffer the corresponding lemma and the theorem for OMP in Appendix B."}, {"heading": "5. Experiments", "text": "We demonstrate the performance of the selective inference for high-order sparse interaction models by numerical experiments on synthetic datasets and a real dataset."}, {"heading": "5.1. Experiments on synthetic datasets", "text": "First, we compared selective inference (select) with naive (naive) and data-splitting (split) on synthetic datasets. In naive, the critical values of the selected k features were naively computed without any selection bias correction mechanisms as in Eq. (4). In split, the dataset was first divided into two equally sized sets, and one of them was used for selection stage, and the other was used for inference stage. Note that the errors controlled by these methods are individual false positive rate for each of the selected features (although naive actually cannot control it), we applied Bonferroni correction within the k selected features, i.e., we reject the hypothesis in Eq. (2) with the significance level \u03b1/k where \u03b1 = 0.05, and we refer this error as family-wise false positive rates (FW-FPRs).\nThe synthetic dataset was generated as follows. In the experiments for comparing FW-FPRs, we generated the training instances (zi, yi) \u2208 [0, 1]d\u00d7R independently at random for each i \u2208 [n]. The original covariates zi were randomly generated so that it contains d(1\u2212 \u03b6) 1s on average, where \u03b6 \u2208 [0, 1] is an experimental parameter for representing the sparsity of the dataset, while the response yi was randomly generated from a Normal distribution N(0, \u03c32). In the experiments for comparing true positive rates (TPRs) the response yi was randomly generated from a Normal distribution N(\u00b5(X), \u03c32I), where, for each row of \u00b5(X) is defined as \u00b5(zi) = 2z1z2z3 in the experiments for MS, \u00b5(zi) = 0.5z1 \u2212 2z2z3 + 3z4z5z6 in the experiments for OMP. We investigated the performances by changing various experimental parameters. We set the baseline parameters as n = 100, d = 100, k = 5, r = 5, \u03b1 = 0.05, \u03c3 = 0.5, and \u03b6 = 0.6."}, {"heading": "5.1.1. FALSE POSITIVE RATES", "text": "Figure 3 shows the FW-FPRs when varying the number of transactions n \u2208 {50, 100, . . . , 250}, the number of original covariates d \u2208 {50, 100, . . . , 250}. In all cases, the FW-FPRs of naive were far greater than the desired significance level \u03b1 = 0.05, indicating that the selection bias\nis harmful. The FW-FPRs of the other two approaches select and split were successfully controlled."}, {"heading": "5.1.2. TRUE POSITIVE RATES", "text": "Figure 4 shows the TPRs of select and split (we omit naive because it cannot control FPRs). Here, TPRs are defined as the probability of finding truly correlated interaction features. In all the setups, the TPRs of select were much greater than split. Note that the performances of split would be worse than select both in the selection and the inference stages. The risk of failing to select truly correlated features in split would be higher than select because only half of the data would be used in the selection stage. Similarly, the statistical power in the inference stage in split would be smaller than select because the sample size is smaller."}, {"heading": "5.1.3. COMPUTATIONAL EFFICIENCY", "text": "Table 1 shows the computation times in seconds for the selective inference approach with and without the computational tricks described in \u00a74 for various values of the number of transactions n \u2208 {100, . . . , 10, 000}, the number of original covariates d \u2208 {100, . . . , 10, 000}, and the sparsity rates \u03b6 \u2208 {0.8, 0.9} (we terminated the search if the time exceeds 1 day). It can be observed from the table that, if we use the computational trick, the selective inferences can be conducted with reasonable computational costs except for d \u2265 5, 000 and \u03b6 = 0.8 cases with OMP. When the computational trick was not used, the cost was extremely large. Especially when the number of original covariates d is larger than 100, we could not complete the search within 1 day. From the results, we conclude that computational trick described in \u00a74 is indispensable for selective inferences for sparse high-order interaction models."}, {"heading": "5.2. Application to HIV drug resistance data", "text": "We applied the selective inference approach to HIV-1 sequence data obtained from Stanford HIV Drug Resistance\nDatabase (Rhee et al., 2003). The goal here is to find statistically significant high-order interactions of multiple mutations (up to r = 5 order interactions) that are highly associated with the drug resistances. We selected k = 30 features, and evaluated the statistical significances of these features by the selective inference framework. Table 2 shows the numbers of 1st, 2nd, 3rd and 4th order interactions that were statistically significant after Bonferroni correction, i.e., significance level is set to be \u03b1/k with \u03b1 = 0.05. (there were no statistically significant 5th order interactions).\nFigure 5 shows the degree of significances in the form of adjusted p-values after Bonferroni correction in increasing order on idv and d4t datasets by MS and OMP scenario, respectively. These results indicate that the selective inference approach could successfully identify statistically significant high-order interactions of multiple mutations."}, {"heading": "Acknowledgements", "text": "This work was partially supported by MEXT KAKENHI (17H00758, 16H06538), JST CREST (JPMJCR1302, JPMJCR1502), RIKEN Center for Advanced Intelligence Project, and JST support program for starting up innovation-hub on materials research by information integration initiative."}], "year": 2017, "references": [{"title": "A knockoff filter for high-dimensional selective inference", "authors": ["Barber", "Rina Foygel", "Cand\u00e8s", "Emmanuel J"], "venue": "arXiv preprint arXiv:1602.03574,", "year": 2016}, {"title": "False discovery rate\u2013adjusted multiple confidence intervals for selected parameters", "authors": ["Benjamini", "Yoav", "Yekutieli", "Daniel"], "venue": "Journal of the American Statistical Association,", "year": 2005}, {"title": "Selective inference in complex research", "authors": ["Benjamini", "Yoav", "Heller", "Ruth", "Yekutieli", "Daniel"], "venue": "Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences,", "year": 1906}, {"title": "A LASSO for hierarchical interactions", "authors": ["J. Bien", "J.E. Taylor", "R. Tibshirani"], "venue": "Journal of The Royal Statistical Society B,", "year": 2013}, {"title": "Variable selection with the strong heredity constraint and its oracle property", "authors": ["N.H. Choi", "W. Li", "J. Zhu"], "venue": "Journal of the American Statistical Association,", "year": 2010}, {"title": "Detecting gene\u2013gene interactions that underlie human diseases", "authors": ["Cordell", "Heather J"], "venue": "Nature Reviews Genetics,", "year": 2009}, {"title": "Multiple hypothesis testing in microarray experiments", "authors": ["Dudoit", "Sandrine", "Shaffer", "Juliet Popper", "Boldrick", "Jennifer C"], "venue": "Statistical Science,", "year": 2003}, {"title": "Sure independence screening for ultrahigh dimensional feature space", "authors": ["J. Fan", "J. Lv"], "venue": "Journal of The Royal Statistical Society B,", "year": 2008}, {"title": "Optimal inference after model selection", "authors": ["Fithian", "William", "Sun", "Dennis", "Taylor", "Jonathan"], "venue": "arXiv preprint arXiv:1410.2597,", "year": 2014}, {"title": "Optimal inference after model selection", "authors": ["Fithian", "William", "Sun", "Dennis", "Taylor", "Jonathan"], "venue": "arXiv preprint arXiv:1410.2597,", "year": 2014}, {"title": "Selective sequential model selection", "authors": ["Fithian", "William", "Taylor", "Jonathan", "Tibshirani", "Robert", "Ryan"], "venue": "arXiv preprint arXiv:1512.02565,", "year": 2015}, {"title": "Interaction screening for ultrahigh-dimensional data", "authors": ["Hao", "Ning", "Zhang", "Hao Helen"], "venue": "Journal of the American Statistical Association,", "year": 2014}, {"title": "An application of boosting to graph classification", "authors": ["T. Kudo", "E. Maeda", "Y. Matsumoto"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2005}, {"title": "An application of boosting to graph classification", "authors": ["Kudo", "Taku", "Maeda", "Eisaku", "Matsumoto", "Yuji"], "venue": "In Advances in neural information processing systems,", "year": 2004}, {"title": "Exact post model selection inference for marginal screening", "authors": ["Lee", "Jason D", "Taylor", "Jonathan E"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}, {"title": "Exact post-selection inference, with application to the lasso", "authors": ["Lee", "Jason D", "Sun", "Dennis L", "Yuekai", "Taylor", "Jonathan E"], "venue": "The Annals of Statistics,", "year": 2016}, {"title": "Genes, environment, health, and disease: facing up to complexity", "authors": ["Manolio", "Teri A", "Collins", "Francis S"], "venue": "Human heredity,", "year": 2006}, {"title": "Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition", "authors": ["Pati", "Yagyensh Chandra", "Rezaiifar", "Ramin", "Krishnaprasad", "PS"], "venue": "In Signals, Systems and Computers,", "year": 1993}, {"title": "Human immunodeficiency virus reverse transcriptase and protease sequence database", "authors": ["Rhee", "Soo-Yon", "Gonzales", "Matthew J", "Kantor", "Rami", "Betts", "Bradley J", "Ravela", "Jaideep", "Shafer", "Robert W"], "venue": "Nucleic acids research,", "year": 2003}, {"title": "Mining complex genotypic features for predicting hiv-1 drug", "authors": ["H. Saigo", "T. Uno", "K. Tsuda"], "venue": "resistance. Bioinformatics,", "year": 2006}, {"title": "Post-selection inference for l1-penalized likelihood models", "authors": ["Taylor", "Jonathan", "Tibshirani", "Robert"], "venue": "arXiv preprint arXiv:1602.07358,", "year": 2016}, {"title": "Statistical significance of combinatorial regulations", "authors": ["Terada", "Aika", "Okada-Hatakeyama", "Mariko", "Tsuda", "Koji", "Sese", "Jun"], "venue": "Proceedings of the National Academy of Sciences,", "year": 2013}, {"title": "Asymptotics of selective inference", "authors": ["Tian", "Xiaoying", "Taylor", "Jonathan"], "venue": "arXiv preprint arXiv:1501.03588,", "year": 2015}, {"title": "Regression shrinkage and selection via the lasso", "authors": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society, Series B,", "year": 1996}, {"title": "Significance analysis of microarrays applied to the ionizing radiation response", "authors": ["Tusher", "Virginia Goss", "Tibshirani", "Robert", "Chu", "Gilbert"], "venue": "Proceedings of the National Academy of Sciences,", "year": 2001}, {"title": "Selective inference for group-sparse linear models", "authors": ["Yang", "Fan", "Barber", "Rina Foygel", "Jain", "Prateek", "Lafferty", "John"], "venue": "arXiv preprint arXiv:1607.08211,", "year": 2016}], "id": "SP:d20d2f793fa738f4fd3129f6ff2cff9034cae1e8", "authors": [{"name": "Shinya Suzumura", "affiliations": []}, {"name": "Kazuya Nakagawa", "affiliations": []}, {"name": "Yuta Umezu", "affiliations": []}, {"name": "Koji Tsuda", "affiliations": []}, {"name": "Ichiro Takeuchi", "affiliations": []}], "abstractText": "Finding statistically significant high-order interactions in predictive modeling is important but challenging task because the possible number of high-order interactions is extremely large (e.g., > 10). In this paper we study feature selection and statistical inference for sparse highorder interaction models. Our main contribution is to extend recently developed selective inference framework for linear models to high-order interaction models by developing a novel algorithm for efficiently characterizing the selection event for the selective inference of high-order interactions. We demonstrate the effectiveness of the proposed algorithm by applying it to an HIV drug response prediction problem.", "title": "Selective Inference for Sparse High-Order Interaction Models"}