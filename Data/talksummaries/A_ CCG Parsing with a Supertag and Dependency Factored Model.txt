0	23	Supertagging in lexicalized grammar parsing is known as almost parsing (Bangalore and Joshi, 1999), in that each supertag is syntactically informative and most ambiguities are resolved once a correct supertag is assigned to every word.
16	22	Our model does not resort to the recursive networks while modeling tree structures via dependencies.
23	19	CCG has a nice property that since every category is highly informative about attachment decisions, assigning it to every word (supertagging) resolves most of its syntactic structure.
24	14	Lewis and Steedman (2014) utilize this characteristics of the grammar.
27	16	Their model looks for the most probable y given a sentence x of length N from the set Y (x) of possible CCG trees under the model of Eq.
28	38	Since this score is factored into each supertag, they call the model a supertag-factored model.
30	58	ci,j is the sequence of categories on such Viterbi parse, and thus b is called the Viterbi inside score, while a is the approximation (upper bound) of the Viterbi outside score.
31	19	A* parsing is a kind of CKY chart parsing augmented with an agenda, a priority queue that keeps the edges to be explored.
48	37	As we will see this keeps our joint model still locally factored and A* search tractable.
50	11	We define a CCG tree y for a sentence x = ⟨xi, .
53	18	Our model is defined as follows: P (y|x) = ∏ i∈[1,N ] Ptag(ci|x) ∏ i∈[1,N ] Pdep(hi|x).
54	19	(2) The added term Pdep is a unigram distribution of the head choice.
76	17	For Pdep, we use the biaffine transformation in Dozat and Manning (2016): gdepi = MLP dep child(ri), gdephi = MLP dep head(rhi), Pdep(hi|x) (4) ∝ exp((gdepi )TWdepg dep hi +wdepg dep hi ), where MLP is a multilayered perceptron.
79	53	Nowwe describe our conversion rules from a CCG tree to a dependency one, which we use in two pur- 2 This is inspired by the formulation of label prediction in Dozat and Manning (2016), which performs the best among other settings that remove or reverse the dependence between the head model and the supertag model.
81	13	Lewis and Steedman (2014) describe one way to extract dependencies from a CCG tree (LEWISRULE).
94	18	For example we obtain the head final dependency tree in Figure 4e from the Japanese CCG tree in Figure 4b.
96	13	Since English has the opposite, SVO word order, we define the simple “head first” rule, in which the left argument always becomes the head (Figure 4d).
103	13	Second, by fixing the direction of arcs, the prediction of heads becomes easier, meaning that the dependency predictions become more reliable.
106	68	Tri-training is one of the semi-supervised methods, in which the outputs of two parsers on unlabeled data are intersected to create (silver) new training data.
110	20	Since they make this data publicly available 5, we obtain our silver data by assigning dependency structures on top of them.6 We train two very different dependency parsers from the training data extracted from CCGbank Section 02-21.
111	30	This training data differs depending on our dependency conversion strategies (Section 4).
113	11	For HEADFIRST, we extract the head first dependencies from the CCG trees.
120	21	Following Lewis et al. (2016), we include 15 copies of CCGbank training set when using these silver data.
130	16	We follow the default train/dev/test splits of Japanese CCGbank (Uematsu et al., 2013).
136	34	One issue in Japanese experiments is evaluation.
143	16	Effect of Dependency We first see how the dependency components added in our model affect the performance.
145	19	We can see that for both LEWISRULE and HEADFIRST, adding dependency terms improves the performance.
147	49	The absolute improvements by tri-training are equally large (about 1.0 points), suggesting that our model with dependencies can also benefit from the silver data.
148	55	Excluding Normal Form Constraints One advantage of HEADFIRST is that the direction of arcs is always right, making the structures simpler and more parsable (Section 5).
150	34	Since the constituent structures of CCGbank trees basically follow the normal form (NF), we hypothesize that the model learned with HEADFIRST has an ability to force the outputs in NF automatically.
