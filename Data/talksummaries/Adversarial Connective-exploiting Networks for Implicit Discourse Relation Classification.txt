0	8	Discourse relations connect linguistic units such as clauses and sentences to form coherent semantics.
2	22	Connectives (e.g., but, so, etc) are one of the most critical linguistic cues for identifying discourse relations.
3	28	When explicit connectives are present in the text, a simple frequency-based mapping is sufficient to achieve over 85% classification accuracy (Xue et al., 2016; Li et al., 2016).
4	24	In contrast, implicit discourse relation recognition has long been seen as a challenging problem, with the best accuracy so far still lower than 50% (Chen et al., 2015).
5	100	In the implicit case, discourse relations are not lexicalized by connectives, but to be inferred from relevant sentences (i.e., arguments).
6	48	For example, the following two adjacent sentences Arg1 and Arg2 imply relation Cause (i.e., Arg2 is the cause of Arg1).
7	87	[Arg2]: You already know the answer.
8	22	[Implicit connective]: Because [Discourse relation]: Cause Various attempts have been made to directly infer underlying relations by modeling the semantics of the arguments, ranging from feature-based methods (Lin et al., 2009; Pitler et al., 2009) to the very recent end-to-end neural models (Chen et al., 2016a; Qin et al., 2016c).
10	29	In fact, even the human annotators would make use of connectives to aid relation annotation.
11	33	For instance, the popular Penn Discourse Treebank (PDTB) benchmark data (Prasad et al., 2008) was annotated by first inserting a connective expression (i.e., implicit connective, as shown in the above example) manually, and determining the abstract relation by combining both the implicit connective and contextual semantics.
12	10	1006 Therefore, the huge performance gap between explicit and implicit parsing (namely, 85% vs 50%), as well as the human annotation practice, strongly motivates to incorporate connective information to guide the reasoning process.
13	85	This paper aims to advance implicit parsing by making use of annotated implicit connectives available in training data.
17	23	Other research leveraged explicit connective examples for data augmentation (Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015; Braud and Denis, 2016).
18	5	Our work is orthogonal and complementary to this line.
20	39	We use deep neural models for relation classification, and take the intuition that, sentence arguments integrated with connectives would enable highly discriminative neural features for accurate relation inference, and an ideal implicit relation classifier, even though without access to connectives, should mimic the connective-augmented reasoning behavior by extracting similarly salient features.
21	21	We therefore setup a secondary network in addition to the implicit relation classifier, building upon connectiveaugmented inputs and serving as a feature learning model for the implicit classifier to emulate.
22	45	Methodologically, however, feature imitation in our problem is challenging due to the semantic gap induced by adding the connective cues.
23	14	It is necessary to develop an adaptive scheme to flexibly drive learning and transfer discriminability.
24	48	We devise a novel adversarial approach which enables a self-calibrated imitation mechanism.
25	24	Specifically, we build a discriminator which distinguishes between the features by the two counterpart networks.
26	14	The implicit relation network is then trained to correctly classify relations and simultaneously to fool the discriminator, resulting in an adversarial framework.
27	9	The adversarial mechanism has been an emerging method in different context, especially for image generation (Goodfellow et al., 2014) and domain adaptation (Ganin et al., 2016; Chen et al., 2016c).
28	16	Our adversarial framework is unique to address neural feature emulation between two models.
31	48	Our method is evaluated on the PDTB 2.0 benchmark in a variety of experimental settings.
33	8	We also demonstrate that our implicit recognition network successfully imitates and extracts crucial hidden representations.
36	6	Section 4 shows substantially improved experimental results over previous methods.
62	5	Our model aims at making full use of the provided implicit connectives at training time to regulate learning of implicit relation recognizer, encouraging extraction of highly discriminative semantics from raw arguments, and improving generalization at test time.
69	127	The discriminator attempts to distinguish between the features extracted by the two relation models, while the implicit relation model is trained to maximize the accuracy on implicit data, and at the same time to confuse the discriminator.
70	12	In the next we first present the overall architecture of the proposed approach (section 3.1), then develop the training procedure (section 3.2).
71	23	The components are realized as deep (convolutional) neural networks, with detailed modeling choices discussed in section 3.3.
