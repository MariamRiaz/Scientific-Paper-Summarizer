1	28	Identifying these relations has been shown to be relevant to tasks such as summarization (Louis et al., 2010a; Yoshida et al., 2014), sentiment analysis (Somasundaran et al., 2009), coherence evaluation (Lin et al., 2011), and question answering (Jansen et al., 2014).
2	29	While the Penn Discourse Treebank (PDTB) now provides a large dataset annotated for discourse relations (Prasad et al., 2008), the automatic identification of implicit relations is a difficult task, with state-of-the-art performance at roughly 40% (Lin et al., 2009).
3	53	One reason for this poor performance is that discourse relations are rooted in semantics (Forbes- Riley et al., 2006), which can be difficult to recover from surface level features.
4	58	Consider the implicit discourse relation between the following two sentences (also shown in Figure 1a): (1) Bob gave Tina the burger.
5	39	While a connector like because seems appropriate here, there is little surface information to signal this relationship, unless the model has managed to learn a bilexical relationship between burger and hungry.
7	14	329 Transactions of the Association for Computational Linguistics, vol.
8	27	Action Editor: Alexander Koller.
10	29	We address this issue by applying a discriminatively-trained model of compositional distributed semantics to discourse relation classification (Socher et al., 2013; Baroni et al., 2014a).
12	33	The discourse relation can then be predicted as a bilinear combination of these vector representations.
14	89	We show that when combined with a small number of surface features, this approach outperforms prior work on the classification of implicit discourse relations in the PDTB.
15	44	Despite these positive results, we argue that bottom-up vector-based representations of discourse arguments are insufficient to capture their relations.
16	142	To see why, consider what happens if we make a tiny change to example (1): (2) Bob gave Tina the burger.
17	58	After changing the subject of the second sentence to Bob, the connective “because” no longer seems appropriate; a contrastive connector like although is preferred.
18	106	But despite the radical difference in meaning, the bottom-up distributed representation of the second sentence will be almost unchanged: the syntactic structure remains identical, and the words he and she have very similar word representations (see Figure 2).
20	95	As Mooney (2014) puts it, “you can’t cram the meaning of a whole %&!$# sentence into a single $&!#* vector!” We address this issue by computing vector representations not only for each discourse argument, but also for each coreferent entity mention.
22	62	We compute entity-role representations using a feed-forward compositional model, which combines “upward” and “downward” passes through the syntactic structure, shown in Figure 1b.
112	15	To extract entities and their mentions from the PDTB, we ran the Berkeley coreference system (Durrett and Klein, 2013) on each document.
114	43	Line 1 in Table 2 shows the proportion of the instances with shared entities in the PDTB training and test data, as detected by the Berkeley system.
116	18	To determine whether this low rate of coreference is an intrinsic property of the data, or whether it is due to the quality of state-of-the-art coreference resolution, we also consider the gold coreference annotations in the OntoNotes corpus (Pradhan et al., 2007), a portion of which intersects with the PDTB (597 documents).
118	14	These results indicate that with perfect coreference resolution, the applicability of distributed entity semantics would reach 40% of the training set and nearly 50% of the test set.
121	15	These include four categories: word pair features, constituent parse features, dependency parse features, and contextual features.
143	16	None of these relations appear in the test or development data.
153	21	Lin et al. (2009) To our knowledge, the best published accuracy on multiclass classification of second-level implicit discourse relations is from Lin et al. (2009), who apply feature selection to obtain a set of lexical and syntactic features over the arguments.
156	38	Compositional Finally, we report results for the method described in this paper.
181	38	The following examples help highlight how entity semantics can improve the accuracy of discourse relation classification.
184	22	(4) Arg 1: [Mr. Greenberg] got out just before the 1987 crash and, to [his] regret, never went back even as the market soared.
185	50	Arg 2: This time [he]’s ready to buy in “when the panic wears off.” (5) Arg 1: Half of [them]1 are really scared and want to sell but [I]2’m trying to talk them out of it.
186	152	Arg 2: If [they]1 all were bullish, [I]2’d really be upset.
187	51	In example (3), the entity-augmented model correctly identifies the relation as RESTATEMENT, due in part to the detected coreference between The Wall Street Journal and the Journal: in both arguments, the entity experiences a drop in profits.
193	15	We follow this evaluation approach as closely as possible, using sections 2-20 of the PDTB as a training set, sections 0-1 as a development set for parameter tuning, and sections 21-22 for testing.
