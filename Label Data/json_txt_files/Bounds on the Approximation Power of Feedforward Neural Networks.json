{"sections": [{"heading": "1. Introduction", "text": "It is well-known that sufficiently large multi-layer feedforward networks can approximate any function with desired accuracy (Hornik et al., 1989). An important problem then is to determine the smallest neural network for a given task and accuracy. The standard guideline is the approximation power (variously known as expressiveness) of the network which quantifies the size of the neural network, typically in terms of depth and width, in order to approximate a class of functions within a given error. In particular, several works provided evidence that deeper networks perform better than shallow ones, given a fixed number of hidden units (Bianchini & Scarselli, 2014; Delalleau & Bengio, 2011; Liang & Srikant, 2017; Mhaskar et al., 2016; Pascanu et al., 2014; Telgarsky, 2015; 2016; Yarotsky, 2017).1\nA popular activation function is the rectified linear unit (ReLU), partly because of its low complexity when coupled with backpropagation training (Krizhevsky et al., 2012). It has, therefore, become of interest to determine the power of neural networks with ReLU\u2019s and, more generally, with piecewise linear activation functions.\n1Department of Electrical Engineering, Sharif University of Technology, Iran 2Department of Communications and Electronics, Telecom ParisTech, France. Correspondence to: Mohammad Mehrabi <mohamadmehrabi4@gmail.com>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\n1For a nice counterexample see (Lu et al., 2017).\nDetermining the capacity of a neural networks with a piecewise linear activation function typically involves two steps. First, evaluate the number of linear pieces (or break points) that the network can produce and, second, tie this number to the approximation error. The works (Montufar et al., 2014; Pascanu et al., 2014) recently showed that a linear increase in depth results in an exponential growth in the number of linear pieces as opposed to width which results only in a polynomial growth. Accordingly, the approximation capacity exhibits a similar tradeoff between depth and width. For related works with respect to classification error see (Telgarsky, 2015; 2016) and with respect to function approximation error see (Liang & Srikant, 2017; Mhaskar et al., 2016; Yarotsky, 2017).\nIn this paper we consider general feedforward neural networks with piecewise linear activation functions and establish bounds on the size of the network in terms of the approximation error, the depth d, the width, and the dimension of the input space to approximate a given function. We first establish an improved upper bound on the number of break points that such a network can produce which is a multiplicative factor dd smaller than the currently best known from (Yarotsky, 2017). This upper bound is obtained by investigating neuron state transitions as introduced in (Raghu et al., 2017). Combining this upper bound with lower bounds in terms of error and dimension, we obtain necessary conditions on the depth, width, error, and dimension for a neural network to approximate a given function. These bounds significantly improve on the corresponding state-of-the-art bounds for certain classes of functions (Theorems 1,2 and Corollaries 1,2,3).\nThe second contribution of the paper (Theorem 3) is an upper bound on the difference of two neural networks with identical weights but different activation functions. This problem is related to \u201cactivation function simulation\u201d investigated in (DasGupta & Schnitger, 1993) which leverages network topology to compensate a change in activation function.\nThe paper is organized as follows. In Section 2 we briefly introduce the setup. In Section 3 we present the main results which are then compared with the corresponding ones in the recent literature in Section 4. Finally, Section 5 contains the proofs."}, {"heading": "2. Preliminaries", "text": "Throughout the paper R denotes a compact convex set in Rn, n \u2265 1, and F\u03c3 denotes the set of feedforward neural networks with input R, output R, and activation function \u03c3 : R \u2192 R. Feedforward here refers to the fact that the neural network contains no cycles; connections are allowed between non-neighbouring layers. It is assumed that \u03c3 is a piecewise linear (not necessarily continuous) function with t \u2265 1 linear pieces. The set of all such activation functions is denoted by \u03a3t.\nA neural network f \u2208 F\u03c3 consists of a set of input units If , a set of hidden units Hf that operate according to \u03c3, non-zero weights representing connections, and a single output unit which just weight-sums its inputs. To simplify the notation we use f to represent both a neural network and the function that it represents.\nFor instance, in the neural network shown in Fig. 1, we have If = {x1, x2, x3} andHf = {uij , \u2200i, j}. Definition 1 (Depth and width). Given a neural network f \u2208 F\u03c3, the depth of a hidden unit h \u2208 Hf , denoted as df (h), is the length of the longest path from any i \u2208 If to h. The depth of f is\ndf def = max { df (h) \u2223\u2223h \u2208 Hf}. The set of hidden units with depth i is\nHif def = { h \u2208 Hf \u2223\u2223df (h) = i}. The width of the network is\n\u03c9f def = |Hf | df def = \u2211df i=1 \u03c9i df\n(1)\nwhere \u03c9i def = |Hif |.\nFor instance, in Fig. 1, the hidden unit u23 can be reached by inputs x1 and x3, by following the paths x1 \u2192 u23, x3 \u2192 u11 \u2192 u23, or x3 \u2192 u12 \u2192 u23. Therefore, df (u23) = 2. The hidden units of maximum depth are u31, u32, and u33 and hence df = 3,H3f = {u31, u32, u33} and \u03c9f = 8/3.\nThe following simple inequality is frequently used in the paper. Lemma 1. For any t \u2265 1, df \u2265 1, and |Hf | \u2265 1\n((t\u2212 1)\u03c9f + 1)df \u2264 t|Hf |.\nProof. Set \u03c9f = |Hf | df\nand observe that( (t\u2212 1) |Hf |\ndf + 1 )df is a non-decreasing function of df and that df \u2264 |Hf |.\nDefinition 2 (Affine \u03b5-approximation). Function f \u2208 F\u03c3 is an affine \u03b5-approximation of a function g : R \u2192 R if\nsup x\u2208R |f(x)\u2212 g(x)| \u2264 \u03b5.\nDefinition 3 (Break point). Given (x,y) \u2208 R2, function f : R \u2192 R admits a break point at \u03b10 \u2208 (0, 1) relative to the segment [x,y] if the first order derivative of f((1\u2212 \u03b1)x + \u03b1y) does not exist at \u03b1 = \u03b10. The total number of break points of f on the (open) segment ]x,y[ is denoted by Bx\u2192y(f). Finally, we let B\u0304x\u2192y(f) def = Bx\u2192y(f) + 1.\nSince f is piecewise linear B\u0304x\u2192y(f) simply counts the number of linear pieces that f produces as the input ranges from x to y."}, {"heading": "3. Main Results", "text": "Theorems 1,2 and Corollaries 2,3 provide bounds on the size of a neural network to approximate a given function. These bounds are expressed in terms of the approximation error and width and depth of the network, but hold irrespectively of the weights. Recall that connections are allowed between non-neighboring layers.\nAs a notational convention we use C2(R) to denote the set of functionsR \u2192 R whose second order partial derivatives are continuous over R\u030a (the interior ofR).\nTheorem 1. Let f \u2208 F\u03c3, \u03c3 \u2208 \u03a3t, be an \u03b5-approximation of a function g \u2208 C2(R) and let x,y \u2208 R. Then,\n( (t\u2212 1)\u03c9f + 1 )df \u2265 B\u0304x\u2192y(f) (2)\n\u2265 ||x\u2212 y||2 4 \u221a \u03b5 \u00b7\u03a8(g,x,y), (3)\nwhere\n\u03a8(g,x,y) def = \u221a inf\n0\u2264\u03b1\u22641\n( max { 0, \u03b3(\u03b1)\u03b4(\u03b1) }) , (4)\n\u03b3(\u03b1) def = min { |\u03b11(\u03b1)|, |\u03b12(\u03b1)| } ,\n\u03b4(\u03b1) def = sign ( \u03b11(\u03b1)\u03b12(\u03b1) ) ,\nand where \u03b11(\u03b1) and \u03b12(\u03b1) are the largest and smallest eigenvalues of the hessian matrix \u22072g ( (1 \u2212 \u03b1)x + \u03b1y ) , respectively.\nMaximizing the right-hand side of (3) over x,y and using Lemma 1 we obtain:\nCorollary 1. Under the assumptions of Theorem 1 we have\n|Hf | \u2265 logt\n( sup\n(x,y)\u2208R2\n{ ||x\u2212 y||2 4 \u221a \u03b5 \u00b7\u03a8(g,x,y) }) .\nA function g : R \u2192 R that is twice differentiable is said to be strongly convex with parameter \u00b5 if \u22072g(x) \u00b5I for all x \u2208 R\u030a. Corollary 2. Let f \u2208 F\u03c3, \u03c3 \u2208 \u03a3t, be an \u03b5-approximation of a function g \u2208 C2(R) that is strongly convex with parameter \u00b5 > 0. Then,\n|Hf | \u2265 1\n2 logt (\u00b5 \u00b7 (diam(R))2 16\u03b5 ) ,\nwhere diam(R) def= sup\n(x,y)\u2208R ||x\u2212 y||2.\nProof. By strong convexity \u03a8(g,x,y) \u2265 \u221a\u00b5. The result then follows from Theorem 1 and Lemma 1.\nAs an example, consider g(x) = x \u00b7 x over [0, 1]n. The Hessian matrix is 2In\u00d7n and from Corollary 2 we get\n|Hf | \u2265 log2 (\u221a n\n8\u03b5\n) .\nCorollary 3. Let R = [0, 1]n. Let f \u2208 F\u03c3, \u03c3 \u2208 \u03a32,2 be an \u03b5-approximation of a function g \u2208 C2(R) such that \u2207g(x) 0 for any x \u2208 R\u030a. Then,\n|Hf | \u2265 q(g)df\u03b5 \u2212 12df (5)\nwhere q(g) > 0 is a constant that only depends on g.\nProof of Corollary 3. From Theorem 1 we get(Hf df + 1 )df \u2265 c(g)\u221a \u03b5 ,\n2Recall that \u03a32 includes ReLU\u2019s.\nwhere c(g) > 0 is some strictly positive constant, since the Hessian of g is positive definite everywhere over R\u030a. Since Hf/df \u2265 1 the above inequality implies(\n2 |Hf | df\n)df \u2265 c\u221a\n\u03b5 .\nSince 12c 1 df \u2265 q where q = 12 min(c, 1), the above inequality yields the desired result.\nTheorem 2. Let R = [0, 1]n. Let f \u2208 F\u03c3, \u03c3 \u2208 \u03a3t, be an \u03b5-approximation of a function g : R \u2192 R such that |DJ(g)(x)| \u2264 \u03b4 for any x \u2208 [0, 1]n and any multi-index3 J such that |J | = 3. Then,\n( (t\u22121)\u03c9f+1 )df \u2265 \u221a\u221a\u221a\u221a( max x\u2208[0,1]n \u2223\u2223\u2206(g)(x)\u2223\u2223n\u22121 \u2212 \u03b4n 32)+ 16\u03b5 , (6) where\n\u2206(g)(x) = n\u2211 k=1 d2g dx2k , (7)\nis the Laplacian of g and where a+ = max(a, 0).\nFor instance, approximating\ng(x1, x2) = 10x 2 1 + x 2 1x 2 2 + 10x 2 2\nover [0, 1]2 requires logt ( 0.82\u221a \u03b5 ) hidden units\u2014combine Theorem 2 with Lemma 1.\nWhether it is Theorem 1 or Theorem 2 which provides a better approximation bound depends on g. For instance, for g1(x1, x2) = 20x 2 1 \u2212 2x22 + x21x22 Theorem 1 gives a trivial (zero) lower bound since the two eigenvalues of the Hessian matrix \u22072(g1) have always different signs. Theorem 2 instead gives 0.737\u221a\n\u03b5 . On the other hand, for g2(x1, x2) =\n10x21 + 10x 2 2 + x 2 1x 2 2 Theorem 1 gives 1.37\u221a \u03b5 as lower bound while Theorem 2 gives 0.82\u221a\n\u03b5 .\nThe next theorem quantifies the effect of a change of activation function on the output of the neural network. Here, the activation functions need not be piece-wise affine.\nTheorem 3. Let f1 \u2208 F\u03c31 and f2 \u2208 F\u03c32 be two neural networks with identical architectures and weights. Suppose that \u03c31 is a \u03b4-Lipschitz continuous function and suppose that the weights belong to some bounded interval [\u2212A,+A], A > 0. Then,\n||f1\u2212f2||\u221e \u2264 ||\u03c31 \u2212 \u03c32||\u221e\n\u03b4\n(( \u03b4 \u00b7A\u00b7\u03c9f+1 )df \u22121 ) . (8)\n3E.g., for J = (2, 1) we have DJ(g(x1, x2)) = \u2202 3g\n\u22022x1\u2202x2 .\nA slightly weaker version of (8) is\n||f1 \u2212 f2||\u221e \u2264 ||\u03c31 \u2212 \u03c32||\u221e\nL\n(( L2 \u00b7 \u03c9f + 1 )df \u2212 1 ) ,\nwhere L = max{A, \u03b4} denotes the Lipschitz-bound defined in (DasGupta & Schnitger, 1993).\nAs an illustration of Theorem 3 consider a feedforward neural network f1 with 100 hidden units, a maximum depth of 5, and the sigmoid as activation function. Suppose the weights belong to interval [\u22121, 1]. Replacing the sigmoid with a 32-bit quantized function results in an error of at most 0.0001\u2014which can readily be obtained from Theorem 3 with \u03b4 = 14 , A = 1, ||\u03c31 \u2212 \u03c32||\u221e = 2 \u221232."}, {"heading": "4. Comparison with Previous Works", "text": "Consider first the inequality (2). Restricting attention to neural networks with d hidden layers, at most \u03c9 units per layer, and where connections are allowed only between neighbouring layers, this inequality gives\nB\u0304x\u2192y(f) \u2264 ( (t\u2212 1)\u03c9 + 1 )d . (9)\nThis is to be compared with the previously best known bound (Lemma 3.2 in (Telgarsky, 2016))\n2(2(t\u2212 1)\u03c9)d\nwhich is larger by a multiplicative factor that is exponential in d whenever \u03c9 > 1, t \u2265 2. For n = 1, Lemma 2.1 in (Telgarsky, 2015) gives (t\u03c9)d which still differs from (9) by a multiplicative factor that is exponential is d for \u03c9 > 1, t \u2265 2.\nFor general feedforward neural networks the previously best known bound (see Lemma 4 of (Yarotsky, 2017)) was\nB\u0304x\u2192y(f) \u2264 ( t \u00b7 \u03c9 \u00b7 df )df which is a multiplicative factor df df larger than (2).\nNow consider the approximation power of neural networks in terms of number of hidden units required to approximate a given function within a given error. Theorem 11 in (Liang & Srikant, 2017) states that to approximate a function [0, 1]n \u2192 R, assumed to be differentiable and strongly convex with parameter \u00b5, with a neural network f requires\n|Hf | \u2265 1\n2 log2 ( \u00b5 16\u03b5 ) ,\nregardless of the dimension n. Corollary 2 improves this bound to\n1 2 log2 (\u00b5 \u00b7 n 16\u03b5 )\nwhich incorporates dimension as well\u2014albeit the dependency on dimension is arguably small.\nCorollary 3 provides a lower bound for ReLU types of networks in terms of the error, the depth, and a constant term which only depends on g. This bound can be compared with the bound of Theorem 6 in (Yarotsky, 2017) which is of order \u2212 12df .4 Hence, Corollary 3 provides a linear (in df ) improvement which is particularly relevant in the deep regime where df = \u2126(log(1/\u03b5)). Table 1 summarizes the above discussion.\nTo the best of our knowledge Theorem 3 is the first result to bound the effect of a change in the activation function for given network topology and weights. Noteworthy perhaps, this bound is essentially universal in the weights since it only depends on their range.\nFinally, compared to the cited papers it should perhaps be stressed that the proofs here (see next section) are relatively elementary\u2014e.g., they do not hinge on VC dimension analysis\u2014and hold true for general feedforward networks."}, {"heading": "5. Analysis", "text": "We first establish a few lemmas to prove Proposition 1 which will provide an upper bound on the number of break points. Then we establish Propositions 2 and 3 which will give lower bounds on the number of break points in terms of the approximation error. Combining these propositions will give Theorems 1 and 2. Finally, we prove Theorem 3.\nDefinition 4 (Intermediate set of units). Given f \u2208 F\u03c3 and 4Theorem 6 of (Yarotsky, 2017) provides a bound of the form q \u2212 1\n2df where q is a constant that depends on both g and df . However, a close inspection of the proof of this theorem reveals that q depends only on g.\nU \u2286 Hf we define the set of hidden units that lie on a path between the input and U as\nin(U) def= { v \u2208 Hf\\U|\u2203i \u2208 If , u \u2208 U s.t. v \u2208 (i\u2192 u) } where (i\u2192 u) denotes the set of intermediate hidden nodes on the path from i to u.\nFor instance, in Fig. 1 we have\nin({u32}) = {u11, u12, u21, u23}.\nThe following lemma follows from the above definition.\nLemma 2. Given U \u2286 Hf we have\nin(in(U) = \u2205\nand in(u) \u2286 (U \u222a in(U))\nfor any u \u2208 U . Definition 5 (State). Any \u03c3 \u2208 \u03a3t partitions the real line (its input) into t intervals I1, I2, ..., It such that on each of these intervals \u03c3 is affine. The state of a unit with activation function \u03c3 is defined to be s \u2208 {1, 2, . . . , t} if its input belongs to Is. By extension, the state of U \u2286 Hf is defined to be the vector of length |U| whose components are the state of each unit in U .\nThe following definition is inspired by the notion of pattern transition introduced in (Raghu et al., 2017):\nDefinition 6 (Transition). Let f \u2208 F\u03c3 , U \u2286 Hf and x,y \u2208 R. Let z\u03b1 = (1 \u2212 \u03b1)x + \u03b1y be a parametrization of the line segment [x,y] as \u03b1 goes from 0 to 1. We say that the state of U experiences a transition at point z\u03b1\u2217 for some \u03b1\u2217 \u2208 (0, 1] if the state vector of U changes at z\u03b1\u2217 while the state vector of in(U) does not change at z\u03b1\u2217 . The number of state transitions of U on the segment [x,y], denoted by Nx\u2192y(U), is defined to be the number of state transitions of U as the input changes from x to y on z\u03b1. If in(U) = \u2205, then Nx\u2192y(U) is defined to be the number of state transitions of U as the input changes from x to y.\nNote that if the state vectors of both U and in(U) change at\u03b1, Nx\u2192y(U) does not change at that \u03b1. For example, consider the neural network f in Fig. 1. Suppose that U = {u11, u12} and suppose that the state of u11 and u12 changes exactly once along segment z\u03b1 for some x and y, respectively at \u03b11 and \u03b12. Then Nx\u2192y({u11}) = 1 and Nx\u2192y({u12}) = 1. If \u03b11 = \u03b12, Nx\u2192y(U) = 1, otherwise Nx\u2192y(U) = 2. If U \u2032 = {u21, u22, u23}, and the state of each of u21, u22 and u23 changes exactly once at either \u03b11 or \u03b12, then Nx\u2192y(U \u2032) = 0 since the state vector of in(U \u2032) = U has also changed at both \u03b11 and \u03b12.\nLemma 3. Given f \u2208 F\u03c3 and U1,U2 \u2286 Hf such that in(U2) = \u2205 and in(U1) \u2286 U2, we have\nNx\u2192y ( U1 \u222a U2 ) \u2264 Nx\u2192y ( U1 ) +Nx\u2192y ( U2 ) .\nProof. Suppose Nx\u2192y ( U1 \u222a U2 ) increases by one at \u03b1 = \u03b1\u2217. If U2 undergoes a state transition at \u03b1\u2217 then, because in(U2) = \u2205, we have that Nx\u2192y ( U2 )\nalso increases by one at \u03b1\u2217. Instead, if no state change happens in U2 at \u03b1\u2217 then, due to the state change of U1\u222aU2 at \u03b1\u2217, the state of U1 must change as well at \u03b1\u2217. Since in(U1) \u2286 U2 and no change in the state of U2 is observed at \u03b1\u2217 we have that Nx\u2192y ( U1 ) necessarily increases by one at \u03b1\u2217.\nLemma 4. Given f \u2208 F\u03c3 and U1,U2 \u2286 Hf such that U1 \u2286 U2 and in(U2) = \u2205 we have\nNx\u2192y ( U1 ) \u2264 Nx\u2192y ( U2 ) .\nProof. Suppose Nx\u2192y ( U1 )\nincreases by one at \u03b1\u2217. Since U1 \u2286 U2 the state of U2 changes as well at \u03b1\u2217. Since in(U2) = \u2205 we deduce that Nx\u2192y ( U2 )\nincreases at \u03b1\u2217 by one, thereby concluding the proof.\nLemma 5. Given f \u2208 F\u03c3 , for any U \u2286 Hf we have\nNx\u2192y(U) \u2264 \u2211 u\u2208U Nx\u2192y(u).\nProof. Suppose that Nx\u2192y(U) increases by one at \u03b1\u2217. Let V \u2286 U be the set of units that experience a transition at \u03b1\u2217. Since we have a transition in the state of U at \u03b1\u2217 we have V 6= \u2205. Now, because the neural network is cycle-free,5 there exists some v \u2208 V such that in(v) \u2229 V = \u2205. We claim that the state of in(v) has not changed at \u03b1\u2217. To prove this note that by Lemma 2 we have in(v) \u2286 in(U)\u222aU and since in(v) \u2229 V = \u2205 we deduce that in(v) \u2286 (in(U) \u222a U\\V). On the other hand neither U\\V nor in(U) has a transition at \u03b1\u2217. This implies that in(v) has no transition at \u03b1\u2217 and therefore Nx\u2192y(v) increases by one at \u03b1\u2217. This concludes the proof since v \u2208 U .\nLemma 6. Given f \u2208 F\u03c3 , for any u \u2208 Hf we have\nNx\u2192y(u) \u2264 (t\u2212 1) ( Nx\u2192y(in(u)) + 1 ) .\nProof. To establish the lemma we show that between transitions of in(u) there are at most t\u2212 1 transitions of u.\n5Recall that throughout the paper neural networks are feedforward.\nSuppose, by way of contradiction, that at least t transitions in the state of u happen while in(u) experiences no change. Then there exists an increasing sequence of real numbers \u03b11, ..., \u03b1t+1 from interval [0, 1] and an increasing set of integers k1, k2, ..., kt+1 from S = {1, 2, ..., t}, with ki 6= ki+1, such that for particular w \u2208 Rn and b \u2208 R we have\nxi def = (1\u2212 \u03b1i)x + \u03b1iy\nw \u00b7 xi + b \u2208 Iki where Ii is defined in Definition 5. Since |S| = t there exists i < j such that ki = kj . Now since ki 6= ki+1 we deduce that j 6= i + 1 and therefore j > i + 1. But w \u00b7 xi+1 + b lies between w \u00b7 xi + b and w \u00b7 xj + b since the sequence \u03b11, \u03b12, ..., \u03b1t+1 is increasing. Since w \u00b7xj+b and w \u00b7xi+b belong to Iki , by the connectedness property of the set Ii we deduce that that w \u00b7 xi+1 + b \u2208 Ii. Therefore, we get ki = ki+1 = kj , a contradiction.\nSince a break point of f \u2208 F\u03c3 necessarily implies a change in the state of the units we get: Lemma 7. Given (x,y) \u2208 R2 and f \u2208 F\u03c3 we have\nBx\u2192y(f) \u2264 Nx\u2192y(Hf ).\nPropositions 1 and 2 establish inequalities (2) and (3) of Theorem 1. Proposition 1. Given f \u2208 F\u03c3 , \u03c3 \u2208 \u03a3t, we have\nBx\u2192y(f) \u2264 (( t\u2212 1 ) \u03c9f + 1 )df \u2212 1. (10)\nProof of Proposition 1. Fix f \u2208 F\u03c3 where \u03c3 \u2208 \u03a3t. Referring to Definition 1, consider the partition\n\u222adi=1Hif ofHf according to unit depth where d = df .\nFix u \u2208 Hi+1f , 0 \u2264 i < d. From the definitions of in(u) andHif we get\nin(u) \u2286 i\u22c3\nj=1\nHjf (11)\nin ( Hi+1f ) \u2286 i\u22c3 j=1 Hjf\nin ( i\u22c3 j=1 Hjf ) = \u2205.\nApplying Lemma 3 with U1 = Hi+1f and U2 = i\u22c3\nj=1\nHjf we\nget\nNx\u2192y( i+1\u22c3 j=1 Hjf ) \u2264 Nx\u2192y( i\u22c3 j=1 Hjf ) +Nx\u2192y(H i+1 f ).\nFrom Lemma 5\nNx\u2192y( i+1\u22c3 j=1 Hjf ) \u2264 Nx\u2192y( i\u22c3 j=1 Hjf ) + \u2211\nu\u2208Hi+1f\nNx\u2192y(u)\nand applying Lemma 6 to the previous inequality\nNx\u2192y( i+1\u22c3 j=1 Hjf ) \u2264 Nx\u2192y( i\u22c3 j=1 Hjf )\n+ \u2211\nu\u2208Hi+1f\n( t\u2212 1 )( Nx\u2192y ( in(u) ) + 1 ) .\nThen, using (11) and Lemma 4 we get\nNx\u2192y( i+1\u22c3 j=1 Hjf )\n\u2264 Nx\u2192y( i\u22c3\nj=1\nHjf ) + \u2211\nu\u2208Hi+1f\n( t\u2212 1 )( Nx\u2192y ( i\u22c3 j=1 Hjf ) + 1 )\n= ( \u03c9i+1(t\u2212 1) + 1 ) Nx\u2192y( i\u22c3 j=1 Hjf ) + \u03c9i+1(t\u2212 1).\n(12)\nFor u \u2208 H1f we have in(u) = \u2205 and according to Lemma 6 we deduce that Nx\u2192y(H1f ) \u2264 (t\u2212 1)\u03c91. With this initial condition and the recursive relation in (12) we get\nNx\u2192y( d\u22c3 j=1 Hjf )\n\u2264 d\u2211 j=1 ( \u220f 1\u2264\u03b11<\u03b12<...<\u03b1j\u2264d \u03c9\u03b11\u03c9\u03b12 \u00b7 \u00b7 \u00b7\u03c9\u03b1j ( t\u2212 1 )j) \u2264 d\u2211 j=1 ((d j )( \u03c9f (t\u2212 1) )j) = ( \u03c9f (t\u2212 1) + 1 )d \u2212 1\nwith \u03c9f as width of f . Finally, apply Lemma 7 to obtain\nBx\u2192y(f) \u2264 (( t\u2212 1 ) \u03c9f + 1 )df \u2212 1.\nProposition 2. Let R be a convex region in Rn. For any affine \u03b5-approximation f : R \u2192 R of a function g \u2208 C2(R) we have\nBx\u2192y(f) \u2265 ||x\u2212 y||2\n4 \u221a \u03b5 \u00b7\u03a8(g,x,y)\u2212 1 (13)\nwhere \u03a8(g,x,y) is defined in (4).\nProof of Proposition 2. We partition R into convex subregionsRi, such that in each subregion f(x) is an affine function. These convex subregions partition a segment [x,y] into sub-segments with end points { x0,x1, ...,xs } , where\nx0 = x,xs = y and s = Bx\u2192y(f)+1. In the sub-segment i \u2208 {0, 1, ..., s\u2212 1},\nf(x) = pi.x + qi, x \u2208 [xi,xi+1], (14)\nfor some pi and qi. Let xi(r) = (1 \u2212 r)xi + rxi+1, r \u2208 [0, 1], and define\nfi(r) = (1\u2212 r)g(xi) + rg(xi+1), hi(r) = g ( xi(r) ) ,\nli(r) = f(x(r)).\nFrom the definition of \u03b5-approximation, ||hi(r)\u2212li(r)||\u221e \u2264 \u03b5. Thus\n||fi(r)\u2212hi(r)||\u221e \u2264 ||fi(r)\u2212 li(r)||\u221e + ||li(r)\u2212 hi(r)||\u221e (a) \u2264 max { |fi(0)\u2212 li(0)|, |fi(1)\u2212 li(1)| } + \u03b5\n\u2264 2\u03b5, (15)\nwhere ||k(r)||\u221e = sup 0\u2264r\u22641 k(r) and step (a) follows because fi(r) and li(r) are both line segments and the maximum distance between them is achieved at end points.\nAs h(r) on (0, 1) is differentiable so there exists r\u2217i \u2208 (0, 1) such that h\u2032i(r \u2217 i ) = hi(1) \u2212 hi(0). Consider x\u2217i = (1 \u2212 r\u2217i )xi + r \u2217 i xi+1. From (15) we obtain\n|(1\u2212 r\u2217i ) ( g(xi)\u2212 g(xi+1) ) \u2212 g(x\u2217i ) + g(xi+1)| \u2264 2\u03b5,\n|r\u2217i ( g(xi+1)\u2212 g(xi) ) + g(xi)\u2212 g(x\u2217i )| \u2264 2\u03b5.\nThen, from the definition of r\u2217i we have\n|(r\u2217i \u2212 1)\u2207g(x\u2217i ).(xi+1 \u2212 xi)\u2212 g(x\u2217i ) + g(xi+1)| \u2264 2\u03b5 (16)\n|r\u2217i\u2207g(x\u2217i ).(xi+1 \u2212 xi)\u2212 g(x\u2217i ) + g(xi)| \u2264 2\u03b5. (17) Since g \u2208 C2(R) a Taylor expansion of g(xi) and g(xi+1) around x\u2217i gives\ng(xi) = g(x \u2217 i )\u2212 r\u2217i\u2207g ( x\u2217i ) .(xi+1 \u2212 xi)\n+ r\u2217i 2\n2 (xi+1 \u2212 xi)T\u22072g\n( xi(\u03b1i) ) (xi+1 \u2212 xi),\ng(xi+1) = g(x \u2217 i ) + (1\u2212 r\u2217i )\u2207g ( x\u2217i ) .(xi+1 \u2212 xi)\n+ (1\u2212 r\u2217i ) 2\n2 (xi+1 \u2212 xi)T\u22072g\n( xi(\u03b2i) ) (xi+1 \u2212 xi),\nwhere 0 \u2264 \u03b1i \u2264 r\u2217i \u2264 \u03b2i \u2264 1.\nSubstituting the above relations in inequalities (16) and (17) we get\n|(1\u2212 r\u2217i ) 2 (xi+1 \u2212 xi)T\u22072g ( xi(\u03b2i) ) (xi+1 \u2212 xi)| \u2264 4\u03b5,\n(18)\n|r\u2217i 2(xi+1 \u2212 xi)T\u22072g ( xi(\u03b1i) ) (xi+1 \u2212 xi)| \u2264 4\u03b5. (19)\nUse the Rayleigh quotient and the definitions of \u03b8(\u03b1), \u03b3(\u03b1) to obtain\n| (xi+1 \u2212 xi)T\u22072g\n( xi(\u03b1i) ) (xi+1 \u2212 xi)\n(xi+1 \u2212 xi)T (xi+1 \u2212 xi) |\n\u2265 inf 0\u2264\u03b1\u22641\n( max { 0, \u03b8(\u03b1)\u03b3(\u03b1) }) .\nCombining the above inequality with (18) and (19) and the fact that r\u2217i 2 + (1\u2212 r\u2217i )2 \u2265 12 we get\n||xi+1 \u2212 xi||22. inf 0\u2264\u03b1\u22641\n( max { 0, \u03b8(\u03b1)\u03b3(\u03b1) }) \u2264 16\u03b5.\nAccordingly, s\u22121\u2211 i=0 ( ||xi+1 \u2212 xi||2 4 \u221a \u03b5 . \u221a inf 0\u2264\u03b1\u22641 ( max { 0, \u03b8(\u03b1)\u03b3(\u03b1) })) \u2264 s,\nwhich gives\nBx\u2192y(f) \u2265 ||x\u2212 y||2\n4 \u221a \u03b5 \u03a8(g,x,y)\u2212 1.\nProposition 3. Let g : [0, 1]n \u2192 R be such that DJ(g)(x) \u2264 \u03b4 for any x \u2208 [0, 1]n and any multi-index J such that |J | = 3. Then, for any affine \u03b5-approximation f\nBx\u2192y(f) \u2265\n\u221a\u221a\u221a\u221a( max x\u2208[0,1]n \u2223\u2223\u2206(g)(x)\u2223\u2223 \u00b7 n\u22121 \u2212 \u03b4 \u00b7 n 32)+ 16\u03b5 \u2212 1\nfor any x,y \u2208 [0, 1]n, where \u2206 denotes the Laplace operator (7).\nProof of Proposition 3. Define\nz def = arg max x\u2208R \u03c1 ( \u22072g(x) ) where \u03c1(\u00b7) denotes the spectral radius. Let u be a normalized eigenvector corresponding to an eigenvalue \u03bb where |\u03bb| = \u03c1 ( \u22072g(z) ) , i.e.,\n\u22072g(z)u = \u03bbu, ||u|| = 1. (20)\nConsider any segment [x,y] in R in the direction of u, i.e., such that x\u2212 y = u. The convex subregions of f , defined in the proof of Proposition 2, divide this segment into sub-segments with end points {x0,x1, ...,xs} where x0 = x,xs = y and s = Bx\u2192y(f) + 1. Using the same\nanalysis as in the proof of Proposition 2, from (14)\u2013(19) we obtain (18) and (19). On the other hand, note that\n|(xi+1 \u2212 xi)T\u22072g ( xi(\u03b1i) ) (xi+1 \u2212 xi)|\n\u2265 |(xi+1 \u2212 xi)T\u22072g ( z ) (xi+1 \u2212 xi)|\n\u2212 |(xi+1 \u2212 xi)T ( \u22072g ( xi(\u03b1i) ) \u2212\u22072g ( z )) (xi+1 \u2212 xi)|\n= |\u03bb| \u00b7 ||xi+1 \u2212 xi||2 \u2212 \u2223\u2223tr{(\u22072g(xi(\u03b1i))\u2212\u22072g(z))(xi+1 \u2212 xi)(xi+1 \u2212 xi)T}\u2223\u2223\n(a) \u2265 |\u03bb| \u00b7 ||xi+1 \u2212 xi||2 \u2212 \u2223\u2223\u2223\u2223\u22072g(xi(\u03b1i))\u2212\u22072g(z)\u2223\u2223\u2223\u2223F\u2223\u2223\u2223\u2223(xi+1 \u2212 xi)(xi+1 \u2212 xi)T \u2223\u2223\u2223\u2223F = |\u03bb| \u00b7 ||xi+1 \u2212 xi||2\n\u2212 \u2223\u2223\u2223\u2223\u22072g(xi(\u03b1i))\u2212\u22072g(z)\u2223\u2223\u2223\u2223F||xi+1 \u2212 xi||2\n= ||xi+1 \u2212 xi||2 \u00b7 ( |\u03bb| \u2212 n\u03b4 \u00b7 ||z \u2212 xi(\u03b1i)|| ) \u2265 ||xi+1 \u2212 xi||2 \u00b7 ( |\u03bb| \u2212 \u03b4 \u00b7 n 32 ) ,\nwhere in step (a) we used the inequality\u2223\u2223\u2223tr(AB)\u2223\u2223\u2223 \u2264 ||A||F ||B||F , || \u00b7 ||F stands for Frobenius norm.\nCombining the above relation with (18), (19) and the fact that r\u2217i 2 + (1\u2212 r\u2217i )2 \u2265 12 we get\n16\u03b5 \u2265 ||xi+1 \u2212 xi||2 \u00b7 ( |\u03bb| \u2212 \u03b4 \u00b7 n 32 ) ,\nwhich gives\n4 \u221a \u03b5 \u00b7 ( Bx\u2192y(f) + 1 ) \u2265 ||x\u2212 y|| \u00b7 \u221a( |\u03bb| \u2212 \u03b4 \u00b7 n 32 )+ .\nFinally, rewriting the above inequality we get\nBx\u2192y(f) \u2265 1 4 \u221a \u03b5 \u00b7 \u221a( |\u03bb| \u2212 \u03b4 \u00b7 n 32 )+ \u2212 1.\nSince |\u03bb| = \u03c1 ( \u22072g(z) ) = max x\u2208[0,1]n \u03c1 ( \u22072g(x) ) and\n|\u2206(g)(x)| = |tr(\u22072g(x))| \u2264 \u03c1(\u22072g(x)) \u00b7 n,\nwe obtain the desired result.\nProofs of Theorems 1 and 2\nPropositions 1 and 2 give Theorem 1 and Propositions 1 and 3 give Theorem 2.\nProof of Theorem 3\nGiven a neural network f we use o to denote the output unit, w(u, v) to denote the weight of two connected units u and\nv, and b(u) to denote the bias of unit u. Furthermore, given u \u2208 Hf and x \u2208 R let fu1 (x) denote the output of unit u when the input to f1 is x, and similarly for f2(x). Finally, define the maximum change in hidden layer i as\n\u03b5i(x) def = max\nu\u2208Hif\n{ |fu1 (x)\u2212 fu2 (x)| } .\nFix 1 \u2264 i \u2264 df \u2212 1 and v \u2208 Hi+1f . Then,\u2223\u2223fv1 (x)\u2212 fv2 (x)\u2223\u2223 =\n\u2223\u2223\u2223\u2223\u2223\u03c31( \u2211 u\u2208\ni\u22c3 j=1 Hjf\nw(u, v) \u00b7 fu1 (x) + b(v) )\n\u2212 \u03c32 ( \u2211 u\u2208\ni\u22c3 j=1 Hjf\nw(u, v) \u00b7 fu2 (x) + b(v) )\u2223\u2223\u2223\u2223\u2223\n\u2264 \u03b5+ \u03b4 \u00b7 ( \u2211 u\u2208\ni\u22c3 j=1 Hjf\n|w(u, v)| \u00b7 \u2223\u2223fu1 (x)\u2212 fu2 (x)\u2223\u2223)\n\u2264 \u03b5+ \u03b4A \u00b7 ( i\u2211 j=1 \u2211 u\u2208Hjf \u2223\u2223fu1 (x)\u2212 fu2 (x)\u2223\u2223)\n\u2264 \u03b5+ \u03b4A \u00b7 ( i\u2211 j=1 \u03c9j\u03b5j(x) )\nwhere the first inequality holds since \u03c31 is \u03b4-Lipschitz and assuming that ||\u03c31\u2212\u03c32||\u221e \u2264 \u03b5. Hence we get the recursion between \u03b5i\u2019s\n\u03b5i+1(x) \u2264 \u03b5+ \u03b4A \u00b7 ( i\u2211 j=1 \u03c9j\u03b5j(x) )\n(21)\nfor 1 \u2264 i \u2264 df \u2212 1. Now, since \u03b51(x) \u2264 \u2223\u2223\u03c31(x)\u2212 \u03c32(x)\u2223\u2223 we get \u03b51(x) \u2264 \u03b5. From this initial condition and (21)\n\u03b5i+1(x) \u2264 \u03b5(1 + \u03b4A\u03c91)(1 + \u03b4A\u03c92) \u00b7 \u00b7 \u00b7 (1 + \u03b4A\u03c9i). (22)\nOn the other hand we have |f1(x)\u2212 f2(x)| = \u2223\u2223\u2223 \u2211 u\u2208\ndf\u22c3 j=1 Hjf\nw(u, o) \u00b7 ( fu1 (x)\u2212 fu2 (x) )\u2223\u2223\u2223 \u2264 A ( \u03b51(x)\u03c91 + \u03b52(x)\u03c92 + ...+ \u03b5d(x)\u03c9df\n) and from (22) we finally get\n|f1(x)\u2212 f2(x)|\n\u2264 \u03b5 \u03b4\n( (1 + \u03b4A\u03c91)(1 + \u03b4A\u03c92)...(1 + \u03b4A\u03c9df )\u2212 1 ) \u2264 ||\u03c31 \u2212 \u03c32||\u221e\n\u03b4\n(( \u03b4 \u00b7A \u00b7 \u03c9f + 1 )df \u2212 1 ) which gives the desired result."}], "year": 2018, "references": [{"title": "On the complexity of neural network classifiers: A comparison between shallow and deep architectures", "authors": ["Bianchini", "Monica", "Scarselli", "Franco"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "year": 2014}, {"title": "The power of approximating: A comparison of activation functions", "authors": ["DasGupta", "Bhaskar", "Schnitger", "Georg"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "year": 1993}, {"title": "Shallow vs. deep sum-product networks", "authors": ["Delalleau", "Olivier", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "year": 2011}, {"title": "Multilayer feedforward networks are universal approximators", "authors": ["Hornik", "Kurt", "Stinchcombe", "Maxwell", "White", "Halbert"], "venue": "Neural Networks,", "year": 1989}, {"title": "Imagenet classification with deep convolutional neural networks", "authors": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "year": 2012}, {"title": "Why deep neural networks for function approximation", "authors": ["Liang", "Shiyu", "R. Srikant"], "venue": "In 5th International Conference on Learning Representations (ICLR),", "year": 2017}, {"title": "The expressive power of neural networks: A view from the width", "authors": ["Lu", "Zhou", "Pu", "Hongming", "Wang", "Feicheng", "Hu", "Zhiqiang", "Liwei"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"title": "Learning functions: When is deep better than shallow", "authors": ["Mhaskar", "Hrushikesh", "Liao", "Qianli", "Poggio", "Tomaso"], "venue": "arXiv preprint,", "year": 2016}, {"title": "On the number of linear regions of deep neural networks", "authors": ["Montufar", "Guido F", "Pascanu", "Razvan", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}, {"title": "On the number of inference regions of deep feed forward networks with piece-wise linear activations", "authors": ["Pascanu", "Razvan", "Montufar", "Guido", "Bengio", "Yoshua"], "venue": "In International Conference on Learning Representations,", "year": 2014}, {"title": "On the expressive power of deep neural networks", "authors": ["Raghu", "Maithra", "Poole", "Ben", "Kleinberg", "Jon", "Ganguli", "Surya", "Sohl-Dickstein", "Jascha"], "venue": "In International Conference on Machine Learning (ICML),", "year": 2017}, {"title": "Representation benefits of deep feedforward networks", "authors": ["Telgarsky", "Matus"], "venue": "arXiv preprint,", "year": 2015}, {"title": "Benefits of depth in neural networks", "authors": ["Telgarsky", "Matus"], "venue": "Journal of Machine Learning Research (JMLR),", "year": 2016}, {"title": "Error bounds for approximations with deep relu networks", "authors": ["Yarotsky", "Dmitry"], "venue": "Neural Networks,", "year": 2017}], "id": "SP:8ecfa40f518e7a8344dd1b9d958faf41d69e6ad5", "authors": [{"name": "Mohammad Mehrabi", "affiliations": []}, {"name": "Aslan Tchamkerten", "affiliations": []}, {"name": "Mansoor I. Yousefi", "affiliations": []}], "abstractText": "The approximation power of general feedforward neural networks with piecewise linear activation functions is investigated. First, lower bounds on the size of a network are established in terms of the approximation error and network depth and width. These bounds improve upon stateof-the-art bounds for certain classes of functions, such as strongly convex functions. Second, an upper bound is established on the difference of two neural networks with identical weights but different activation functions.", "title": "Bounds on the Approximation Power of Feedforward Neural Networks"}