6	19	Inspired by the work of Jia and Liang (2017) on reading comprehension, we create a new NLI test set with examples that capture various kinds of lexical knowledge (Table 1).
14	11	The SNLI dataset (Stanford Natural Language Inference, Bowman et al., 2015) consists of 570k sentence-pairs manually labeled as entailment, contradiction, and neutral.
15	61	Premises are image captions from Young et al. (2014), while hypotheses were generated by crowd-sourced workers who were shown a premise and asked to generate entailing, contradicting, and neutral sentences.
16	5	Workers were instructed to judge the relation between sentences given that they describe the same event.
23	22	Following the release of SNLI, there has been tremendous interest in the task, and many end-to-end neural models were developed, achieving promising results.2 Methods are divided into two main approaches.
24	49	Sentence-encoding models (e.g. Bowman et al., 2015, 2016; Nie and Bansal, 2017; Shen et al., 2018) encode the premise and hypothesis individually, while attention-based models align words in the premise with similar words in the hypothesis, encoding the two sentences together (e.g. Rocktäschel et al., 2016; Chen et al., 2017).
25	53	Traditional RTE methods typically relied on resources such as WordNet (Fellbaum, 1998) to identify lexical inferences.
26	5	Conversely, neural methods rely solely on pre-trained word embeddings, yet, they achieve high accuracy on SNLI.
28	8	This raises the question whether the small performance gap is a result of the model not capturing lexical knowledge well, or the SNLI test set not requiring this knowledge in the first place.
32	4	For each premise we generate several hypotheses by replacing a single word within the premise by a different word.
44	30	To avoid introducing new information not present in the training data, we sampled premises from the SNLI training set that contain words from our lists, and generated hypotheses by replacing the selected word with its replacement.
47	3	We manually verify the correctness of the automatically constructed examples using crowdsourced workers in Amazon Mechanical Turk.
52	21	4 github.com/rmaestre/Wikipedia-Bigram-Open-Datasets 2.
53	7	Does the new sentence (hypothesis) add new information to the original sentence (premise)?
62	9	We chose 3 representative models in different approaches (sentence encoding and/or attention): RESIDUALSTACKED-ENCODER (Nie and Bansal, 2017) is a biLSTM-based single sentence-encoding model without attention.
65	5	We use the biLSTM model, which uses an inter-sentence attention mechanism to align words across sentences.
70	12	All models are based on pre-trained GloVe embeddings (Pennington et al., 2014), which are either fine-tuned during training (RESIDUAL-STACKED-ENCODER and ESIM) or stay fixed (DECOMPOSABLE ATTENTION).
73	11	We provide a simple WORDNET BASELINE, in which we classify a sentence-pair according to the WordNet relation that holds between the original word wp and the replaced word wh.
81	9	Table 3 displays the results for all the models on the original SNLI test set and the new test set.
82	11	Despite the task being considerably simpler, the drop in performance is substantial, ranging from 11 to 33 points in accuracy.
84	24	We note that adding SciTail to the training data did not similarly improve the performance; we conjecture that this stems from the differences between the datasets.
85	386	KIM substantially outperforms the other neural models, demonstrating that lexical knowledge is the only requirement for good performance on the new test set, and stressing the inability of the other models to learn it.
94	33	Indeed, the Decomposable Attention model—which does not update its embeddings during training—seems to suffer the most.
95	38	Grouping its prediction accuracy by the cosine similarity between the contradicting words reveals a clear trend that the model errs more on contradicting pairs with similar pre-trained vectors:7 Similarity 0.5-0.6 0.6-0.7 0.7-0.8 0.8-0.9 0.9-1.0 Accuracy 46.2% 42.3% 37.5% 29.7% 20.2%
96	57	Models that fine-tune the word embeddings may benefit from training examples consisting of test replacement pairs.
97	38	Namely, for a given replacement pair (wp, wh), if many training examples labeled as contradiction contain wp in the premise and wh in the hypothesis, the model may update their embeddings to optimize predicting contradiction.
98	32	Indeed, we show that the ESIM accuracy on test pairs increases with the frequency in which their replacement words appear in contradiction examples in the training data: Frequency 0 1-4 5-9 10-49 50-99 100+ Accuracy 40.2% 70.6% 91.4% 92.1% 97.5% 98.5% This demonstrates that the model is capable of learning lexical knowledge when sufficient training data is given, but relying on explicit training examples is a very inefficient way of obtaining simple lexical knowledge.
99	22	We created a new NLI test set with the goal of evaluating systems’ ability to make inferences that require simple lexical knowledge.
100	8	Although the test set is constructed to be much simpler than SNLI, and does not introduce new vocabulary, the state-of-the-art systems perform poorly on it, suggesting that they are limited in their generalization ability.
101	13	The test set can be used in the future to assess the lexical inference abilities of NLI systems and to tease apart the performance of otherwise very similarly-performing systems.
