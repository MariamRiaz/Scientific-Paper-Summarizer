0	39	Deep neural models are known to be computationally expensive to train even with fast hardware (Sutskever et al., 2014; Wu et al., 2016).
7	22	Such algorithms show that human learners can learn efficiently and effectively by increasing intervals of time between subsequent reviews of previously learned materials (Dempster, 1989; Novikoff et al., 2012).
8	25	We investigate the analogy between training neural models and findings in psychology about human memory model and develop a spaced repetition algorithm (named Repeat before Forgetting, RbF) to efficiently and effectively train neural models.
15	58	Research in psychology describes the following memory model for human learning: the probability that a human recalls a previously-seen item (e.g., the Korean translation of a given English word) depends on the difficulty of the item, delay since last review of the item, and the strength of the human memory.
17	80	(1) An accurate memory model enables estimating the time by which an item might be forgotten by a learner so that a review can be scheduled for the learner before that time.
18	16	We investigate the analogy between the above memory model and memory model of artificial neural networks.
23	37	We first define the following concepts to ease understanding the experiments (see Figure 1): • First and Last review points (fRev and lRev) of a training instance are the first and last epochs in which the instance is used to train the network respectively, • Recall point (Rec) is the epoch in which network retention is computed against some training instances; network retention is the probability that a neural network recalls (i.e. correctly classifies) a previously-seen training instance, and • Delay since last review of a training instance is the difference between the recall point and the last review point of the training instance.
24	17	Given training data and a neural network, we uniformly at random divide the data into three disjoint sets: a base set A, a review set B, and a replacement set C that respectively contain 80%, 10%, and 10% of the data.
27	43	Then, starting from the first review point, we inject the review set B and remove C, training with {A ∪ B} instances at every epoch until the last review point.
29	21	At this point, network retention is computed against set B instances, with delay defined as the number of epochs since last review point.
34	30	We evaluate the effect of delay on network retention (against set B instances) by keeping the recall point fixed while moving the sliding window in Figure 1.
36	19	The results show an inverse relationship between network retention and delay since last review in neural networks.
38	26	Figure 2(c) shows the difficulty of set B instances at the last review point against average network retention on these instances at recall point.
42	17	To understand the effect of network strength on its retention, we use the same experimental setup as before except that we keep the delay (difference between recall point and last review point) fixed while gradually increasing the recall point; this will make the networks stronger by training them for more epochs.
43	27	Then, at every recall point, we record network retention on set B instances and network accuracy on validation data.
46	36	The above experiments show that memory retention in neural networks is affected by the same factors that affect memory retention in humans: (a) neural networks forget training examples after a certain period of intervening training data (b): the period of recall is shorter for more difficult examples, and (c): recall improves as networks achieve better overall performance.
66	21	We propose considering density kernels as schedulers that favor (i.e., more confidently delay) less difficult training instances in stronger networks.
67	34	As a kernel we can use any non-increasing function of the following quantity: xi = di × ti se , (2) where di indicates the loss of network for a training instance hi ∈ H, ti indicates the number of epochs to next review of hi, and se indicates the performance of network— on validation data— at epoch e. We investigate the Gaussian, Laplace, Linear, Cosine, Quadratic, and Secant kernels as described below respectively: fgau(x, τ) = exp(−τx2), (3) flap(x, τ) = exp(−τx), (4) flin(x, τ) = { 1− τx x < 1τ 0 otherwise , (5) fcos(x, τ) = { 1 2 cos(τπx) + 1 x < 1 τ 0 otherwise , (6) fqua(x, τ) = { 1− τx2 x2 < 1τ 0 otherwise , (7) fsec(x, τ) = 2 exp(−τx2) + exp(τx2) , (8) where τ is a learning parameter.
70	15	Our Repeat before Forgetting (RbF) model is a spaced repetition algorithm that takes into account the previously validated recall indicators to train neural networks, see Algorithm 2.
71	63	RbF divides training instances into current and delayed batches based on their delay values at each iteration.
75	20	RbF makes such item-specific estimations as follows: Given the difficulty of a training instance di, the memory strength of the neural network at epoch e, se, and an RbF memory model f (see section 3.2.1), RbF scheduler estimates the maximum delay t̂i for the instance such that it can be recalled with a confidence greater than the given threshold η ∈ (0, 1) at time e+ t̂i.
101	21	For this evaluation, if a scheduler predicts a delay t for a training instance h at epoch e, we evaluate network retention with respect to h at epoch e+ t. If the network recalls (correctly classifies) the instance at epoch e+ t, the scheduler has correctly predicted network retention for h, and otherwise, it has made a wrong prediction.
106	118	Figure 6 depicts the average accuracy of schedulers in predicting networks’ retention versus the average fraction of training instances that they delayed per epoch.
125	23	We compare RbF against Leitner and curriculum learning in terms of efficiency of training and effectiveness of trained models.
137	19	In terms of efficiency (first row of Figure 8), CL starts with (small set of) easier instances and gradually increases the amount of training data by adding slightly harder instances into its training set.
138	32	On the other hand, Lit and RbF start big and gradually delay reviewing (easy) instances that the networks have learned.
160	33	This might be because these training paradigms overfit the network by paying too much training attention to very hard instances which might introduce noise to the model.
168	24	We developed a cognitively-motivated training paradigm (scheduler) that space instances over time for efficient and effective training of neural networks.
172	39	There are several avenues for future work including the extent to which our RbF model and its kernels could be combined with curriculum learning or Leitner system to either predict easiness of novel training instances to inform curriculum learning or incorporate Leitner’s queueing mechanism to the RbF model.
173	28	Other directions include extending RbF to dynamically learn the recall confidence parameter with respect to network behavior, or developing more flexible delay functions with theoretical analysis on their lower and upper bounds.
