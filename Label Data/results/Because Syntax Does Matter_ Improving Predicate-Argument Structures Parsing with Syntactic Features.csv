0,1,label2,summary_sentences
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 908–916, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
Neural networks have proven to be highly effective at many tasks in natural language.,1 Introduction,[0],[0]
"For example, neural language models and joint language/translation models improve machine translation quality significantly (Vaswani et al., 2013; Devlin et al., 2014).",1 Introduction,[0],[0]
"However, neural networks can be complicated to design and train well.",1 Introduction,[0],[0]
"Many decisions need to be made, and performance can be highly dependent on making them correctly.",1 Introduction,[0],[0]
"Yet the optimal settings are non-obvious and can be laborious to find, often requiring an extensive grid search involving numerous experiments.
",1 Introduction,[0],[0]
"In this paper, we focus on the choice of the sizes of hidden layers.",1 Introduction,[0],[0]
"We introduce a method for automatically pruning out hidden layer units, by adding a sparsity-inducing regularizer that encourages units to deactivate if not needed, so that
they can be removed from the network.",1 Introduction,[0],[0]
"Thus, after training with more units than necessary, a network is produced that has hidden layers correctly sized, saving both time and memory when actually putting the network to use.
",1 Introduction,[0],[0]
"Using a neural n-gram language model (Bengio et al., 2003), we are able to show that our novel auto-sizing method is able to learn models that are smaller than models trained without the method, while maintaining nearly the same perplexity.",1 Introduction,[0],[0]
"The method has only a single hyperparameter to adjust (as opposed to adjusting the sizes of each of the hidden layers), and we find that the same setting works consistently well across different training data sizes, vocabulary sizes, and n-gram sizes.",1 Introduction,[0],[0]
"In addition, we show that incorporating these models into a machine translation decoder still results in large BLEU point improvements.",1 Introduction,[0],[0]
The result is that fewer experiments are needed to obtain models that perform well and are correctly sized.,1 Introduction,[0],[0]
Language models are often used in natural language processing tasks involving generation of text.,2 Background,[0],[0]
"For instance, in machine translation, the language model helps to output fluent translations, and in speech recognition, the language model helps to disambiguate among possible utterances.
",2 Background,[0],[0]
"Current language models are usually n-gram models, which look at the previous (n− 1) words to predict the nth word in a sequence, based on (smoothed) counts of n-grams collected from training data.",2 Background,[0],[0]
"These models are simple but very effective in improving the performance of natural language systems.
",2 Background,[0],[0]
"However, n-gram models suffer from some limitations, such as data sparsity and memory usage.",2 Background,[0],[0]
"As an alternative, researchers have begun exploring the use of neural networks for language modeling.",2 Background,[0],[0]
"For modeling n-grams, the most common approach is the feedforward network of Bengio et
908
al. (2003), shown in Figure 1.",2 Background,[0],[0]
"Each node represents a unit or “neuron,” which has a real valued activation.",2 Background,[0],[0]
The units are organized into real-vector valued layers.,2 Background,[0],[0]
The activations at each layer are computed as follows.,2 Background,[0],[0]
(We assume n = 3; the generalization is easy.),2 Background,[0],[0]
"The two preceding words, w1, w2, are mapped into lowerdimensional word embeddings,
x1",2 Background,[0],[0]
= A:w1 x2 =,2 Background,[0],[0]
"A:w2
then passed through two hidden layers,
y = f(B1x1 +",2 Background,[0],[0]
B2x2 + b) z,2 Background,[0],[0]
"= f(Cy + c)
where f is an elementwise nonlinear activation (or transfer) function.",2 Background,[0],[0]
"Commonly used activation functions are the hyperbolic tangent, logistic function, and rectified linear units, to name a few.",2 Background,[0],[0]
"Finally, the result is mapped via a softmax to an output probability distribution,
P (wn | w1 · · ·wn−1) ∝",2 Background,[0],[0]
exp([Dz + d]wn).,2 Background,[0],[0]
"The parameters of the model are A, B1, B2, b, C, c, D, and d, which are learned by minimizing the negative log-likelihood of the the training data using stochastic gradient descent (also known as backpropagation) or variants.
",2 Background,[0],[0]
"Vaswani et al. (2013) showed that this model, with some improvements, can be used effectively during decoding in machine translation.",2 Background,[0],[0]
"In this paper, we use and extend their implementation.",2 Background,[0],[0]
Our method is focused on the challenge of choosing the number of units in the hidden layers of a feed-forward neural network.,3 Methods,[0],[0]
"The networks used for different tasks require different numbers of units, and the layers in a single network also require different numbers of units.",3 Methods,[0],[0]
"Choosing too few units can impair the performance of the network, and choosing too many units can lead to overfitting.",3 Methods,[0],[0]
"It can also slow down computations with the network, which can be a major concern for many applications such as integrating neural language models into a machine translation decoder.
",3 Methods,[0],[0]
Our method starts out with a large number of units in each layer and then jointly trains the network while pruning out individual units when possible.,3 Methods,[0],[0]
"The goal is to end up with a trained network
that also has the optimal number of units in each layer.
",3 Methods,[0],[0]
We do this by adding a regularizer to the objective function.,3 Methods,[0],[0]
"For simplicity, consider a single layer without bias, y = f(Wx).",3 Methods,[0],[0]
Let L(W) be the negative log-likelihood of the model.,3 Methods,[0],[0]
"Instead of minimizing L(W) alone, we want to minimize L(W)",3 Methods,[0],[0]
"+ λR(W), where R(W) is a convex regularizer.",3 Methods,[0],[0]
"The `1 norm, R(W) = ‖W‖1 =∑
i,j |Wij |, is a common choice for pushing parameters to zero, which can be useful for preventing overfitting and reducing model size.",3 Methods,[0],[0]
"However, we are interested not only in reducing the number of parameters but the number of units.",3 Methods,[0],[0]
"To do this, we need a different regularizer.
",3 Methods,[0],[0]
"We assume activation functions that satisfy f(0) = 0, such as the hyperbolic tangent or rectified linear unit (f(x) = max{0, x}).",3 Methods,[0],[0]
"Then, if we push the incoming weights of a unit yi to zero, that is, Wij = 0 for all j (as well as the bias, if any: bi = 0), then yi = f(0) = 0 is independent of the previous layers and contributes nothing to subsequent layers.",3 Methods,[0],[0]
So the unit can be removed without affecting the network at all.,3 Methods,[0],[0]
"Therefore, we need a regularizer that pushes all the incoming connection weights to a unit together towards zero.
",3 Methods,[0],[0]
"Here, we experiment with two, the `2,1 norm and the `∞,1 norm.1 The `2,1 norm on a ma-
1In the notation `p,q , the subscript p corresponds to the norm over each group of parameters, and q corresponds to the norm over the group norms.",3 Methods,[0],[0]
"Contrary to more common usage, in this paper, the groups are rows, not columns.
",3 Methods,[0],[0]
"trix W is
R(W) =",3 Methods,[0],[0]
∑ i ‖Wi:‖2 = ∑ i ∑ j W 2ij  12 .,3 Methods,[0],[0]
"(1) (If there are biases bi, they should be included as well.)",3 Methods,[0],[0]
"This puts equal pressure on each row, but within each row, the larger values contribute more, and therefore there is more pressure on larger values towards zero.",3 Methods,[0],[0]
"The `∞,1 norm is
R(W) =",3 Methods,[0],[0]
∑ i ‖Wi:‖∞ = ∑ i max j |Wij |.,3 Methods,[0],[0]
"(2)
Again, this puts equal pressure on each row, but within each row, only the maximum value (or values) matter, and therefore the pressure towards zero is entirely on the maximum value(s).
",3 Methods,[0],[0]
Figure 2 visualizes the sparsity-inducing behavior of the two regularizers on a single row.,3 Methods,[0],[0]
Both have a sharp tip at the origin that encourages all the parameters in a row to become exactly zero.,3 Methods,[0],[0]
"However, this also means that sparsity-inducing regularizers are not differentiable at zero, making gradient-based optimization methods trickier to apply.",4 Optimization,[0],[0]
"The methods we use are discussed in detail elsewhere (Duchi et al., 2008; Duchi and Singer, 2009); in this section, we include a short description of these methods for completeness.",4 Optimization,[0],[0]
"Most work on learning with regularizers, including this work, can be thought of as instances of the proximal gradient method (Parikh and Boyd, 2014).",4.1 Proximal gradient method,[0],[0]
"Our objective function can be split into two parts, a convex and differentiable part (L) and a
convex but non-differentiable part (λR).",4.1 Proximal gradient method,[0],[0]
"In proximal gradient descent, we alternate between improving L alone and λR alone.",4.1 Proximal gradient method,[0],[0]
Let u be the parameter values from the previous iteration.,4.1 Proximal gradient method,[0],[0]
"We compute new parameter values w using:
v← u− η∇L(u) (3)
w← arg max w",4.1 Proximal gradient method,[0],[0]
( 1 2η ‖w − v‖2 + λR(w) ),4.1 Proximal gradient method,[0],[0]
"(4)
and repeat until convergence.",4.1 Proximal gradient method,[0],[0]
The first update is just a standard gradient descent update on L; the second is known as the proximal operator for λR and in many cases has a closed-form solution.,4.1 Proximal gradient method,[0],[0]
"In the rest of this section, we provide some justification for this method, and in Sections 4.2 and 4.3 we show how to compute the proximal operator for the `2 and `∞ norms.
",4.1 Proximal gradient method,[0],[0]
We can think of the gradient descent update (3) on L as follows.,4.1 Proximal gradient method,[0],[0]
"Approximate L around u by the tangent plane,
L̄(v) = L(u) +∇L(u)(v − u) (5)
and move v to minimize L̄, but don’t move it too far from u; that is, minimize
F (v) = 1",4.1 Proximal gradient method,[0],[0]
2η ‖v,4.1 Proximal gradient method,[0],[0]
"− u‖2 + L̄(v).
",4.1 Proximal gradient method,[0],[0]
"Setting partial derivatives to zero, we get
∂F ∂v = 1 η
(v − u) +∇L(u) = 0 v = u− η∇L(u).
",4.1 Proximal gradient method,[0],[0]
"By a similar strategy, we can derive the second step (4).",4.1 Proximal gradient method,[0],[0]
"Again we want to move w to minimize the objective function, but don’t want to move it too far from u; that is, we want to minimize:
G(w) = 1 2η ‖w",4.1 Proximal gradient method,[0],[0]
"− u‖2 + L̄(w) + λR(w).
",4.1 Proximal gradient method,[0],[0]
Note that we have not approximated R by a tangent plane.,4.1 Proximal gradient method,[0],[0]
We can simplify this by substituting in (3).,4.1 Proximal gradient method,[0],[0]
"The first term becomes
1 2η ‖w",4.1 Proximal gradient method,[0],[0]
− u‖2 = 1 2η ‖w,4.1 Proximal gradient method,[0],[0]
"− v − η∇L(u)‖2
= 1 2η ‖w − v‖2 −∇L(u)(w − v)
+ η
2 ‖∇L(u)‖2
and the second term becomes
L̄(w) = L(u) +∇L(u)(w − u) = L(u) +∇L(u)(w",4.1 Proximal gradient method,[0],[0]
"− v − η∇L(u)).
",4.1 Proximal gradient method,[0],[0]
"The ∇L(u)(w − v) terms cancel out, and we can ignore terms not involving w, giving
G(w) = 1 2η ‖w",4.1 Proximal gradient method,[0],[0]
"− v‖2 + λR(w) + const.
which is minimized by the update (4).",4.1 Proximal gradient method,[0],[0]
"Thus, we have split the optimization step into two easier steps: first, do the update for L (3), then do the update for λR (4).",4.1 Proximal gradient method,[0],[0]
The latter can often be done exactly (without approximating R by a tangent plane).,4.1 Proximal gradient method,[0],[0]
"We show next how to do this for the `2 and `∞ norms.
4.2 `2 and `2,1 regularization Since the `2,1 norm on matrices (1) is separable into the `2 norm of each row, we can treat each row separately.",4.1 Proximal gradient method,[0],[0]
"Thus, for simplicity, assume that we have a single row and want to minimize
G(w) = 1 2η ‖w − v‖2 +",4.1 Proximal gradient method,[0],[0]
"λ‖w‖+ const.
",4.1 Proximal gradient method,[0],[0]
"The minimum is either at w = 0 (the tip of the cone) or where the partial derivatives are zero (Figure 3):
∂G ∂w = 1 η (w − v) + λ w‖w‖",4.1 Proximal gradient method,[0],[0]
"= 0.
",4.1 Proximal gradient method,[0],[0]
"Clearly, w and v must have the same direction and differ only in magnitude, that is, w = α v‖v‖ .",4.1 Proximal gradient method,[0],[0]
"Substituting this into the above equation, we get the solution
α = ‖v‖",4.1 Proximal gradient method,[0],[0]
− ηλ.,4.1 Proximal gradient method,[0],[0]
"Therefore the update is
w = α v ‖v‖
α = max(0, ‖v‖ − ηλ).",4.1 Proximal gradient method,[0],[0]
"As above, since the `∞,1 norm on matrices (2) is separable into the `∞ norm of each row, we can treat each row separately; thus, we want to minimize
G(w) = 1 2η ‖w − v‖2 + λmax j |xj |+ const.
","4.3 `∞ and `∞,1 regularization",[0],[0]
"Intuitively, the solution can be characterized as: Decrease all of the maximal |xj | until the total decrease reaches ηλ or all the xj are zero.","4.3 `∞ and `∞,1 regularization",[0],[0]
"See Figure 4.
","4.3 `∞ and `∞,1 regularization",[0],[0]
"If we pre-sort the |xj | in nonincreasing order, it’s easy to see how to compute this: for ρ = 1, . . .","4.3 `∞ and `∞,1 regularization",[0],[0]
", n, see if there is a value ξ ≤ xρ","4.3 `∞ and `∞,1 regularization",[0],[0]
"such that decreasing all the x1, . . .","4.3 `∞ and `∞,1 regularization",[0],[0]
", xρ to ξ amounts to a total decrease of ηλ.","4.3 `∞ and `∞,1 regularization",[0],[0]
"The largest ρ for which this is possible gives the correct solution.
","4.3 `∞ and `∞,1 regularization",[0],[0]
"But this situation seems similar to another optimization problem, projection onto the `1-ball, which Duchi et al. (2008) solve in linear time without pre-sorting.","4.3 `∞ and `∞,1 regularization",[0],[0]
"In fact, the two problems can be solved by nearly identical algorithms, because they are convex conjugates of each other (Duchi and Singer, 2009; Bach et al., 2012).","4.3 `∞ and `∞,1 regularization",[0],[0]
"Intuitively, the `1 projection of v is exactly what is cut out by the `∞ proximal operator, and vice versa (Figure 4).
","4.3 `∞ and `∞,1 regularization",[0],[0]
Duchi et al.’s algorithm modified for the present problem is shown as Algorithm 1.,"4.3 `∞ and `∞,1 regularization",[0],[0]
It partitions the xj about a pivot element (line 6) and tests whether it and the elements to its left can be decreased to a value ξ such that the total decrease is δ (line 8).,"4.3 `∞ and `∞,1 regularization",[0],[0]
"If so, it recursively searches the right side; if not, the
left side.","4.3 `∞ and `∞,1 regularization",[0],[0]
"At the conclusion of the algorithm, ρ is set to the largest value that passes the test (line 13), and finally the new xj are computed (line 16) – the only difference from Duchi et al.’s algorithm.
","4.3 `∞ and `∞,1 regularization",[0],[0]
This algorithm is asymptotically faster than that of Quattoni et al. (2009).,"4.3 `∞ and `∞,1 regularization",[0],[0]
"They reformulate `∞,1 regularization as a constrained optimization problem (in which the `∞,1 norm is bounded by µ) and provide a solution inO(n log n) time.","4.3 `∞ and `∞,1 regularization",[0],[0]
"The method shown here is simpler and faster because it can work on each row separately.
","4.3 `∞ and `∞,1 regularization",[0],[0]
"Algorithm 1 Linear-time algorithm for the proximal operator of the `∞ norm.
1: procedure UPDATE(w, δ) 2: lo, hi← 1, n 3: s← 0 4:","4.3 `∞ and `∞,1 regularization",[0],[0]
"while lo ≤ hi do 5: select md randomly from lo, . . .","4.3 `∞ and `∞,1 regularization",[0],[0]
", hi 6: ρ← PARTITION(w, lo,md, hi) 7: ξ ← 1ρ","4.3 `∞ and `∞,1 regularization",[0],[0]
"( s+ ∑ρ i=lo |xi| − δ
) 8: if ξ ≤ |xρ| then 9: s← s+∑ρi=lo |xi|
10: lo← ρ+ 1 11: else 12: hi← ρ− 1 13: ρ← hi 14: ξ ← 1ρ (s− δ) 15: for i← 1, . . .","4.3 `∞ and `∞,1 regularization",[0],[0]
", n","4.3 `∞ and `∞,1 regularization",[0],[0]
"do 16: xi ← min(max(xi,−ξ), ξ) 17: procedure PARTITION(w, lo,md, hi) 18: swap xlo and xmd 19: i← lo + 1 20: for j ← lo + 1, . . .","4.3 `∞ and `∞,1 regularization",[0],[0]
", hi do 21: if xj ≥ xlo then 22: swap xi and xj 23: i← i+ 1 24: swap xlo and xi−1 25: return i− 1","4.3 `∞ and `∞,1 regularization",[0],[0]
"We evaluate our model using the open-source NPLM toolkit released by Vaswani et al. (2013), extending it to use the additional regularizers as described in this paper.2 We use a vocabulary size of 100k and word embeddings with 50 dimensions.",5 Experiments,[0],[0]
"We use two hidden layers of rectified linear units (Nair and Hinton, 2010).
",5 Experiments,[0],[0]
"2These extensions have been contributed to the NPLM project.
",5 Experiments,[0],[0]
"We train neural language models (LMs) on two natural language corpora, Europarl v7 English and the AFP portion of English Gigaword 5.",5 Experiments,[0],[0]
"After tokenization, Europarl has 56M tokens and Gigaword AFP has 870M tokens.",5 Experiments,[0],[0]
"For both corpora, we hold out a validation set of 5,000 tokens.",5 Experiments,[0],[0]
"We train each model for 10 iterations over the training data.
",5 Experiments,[0],[0]
Our experiments break down into three parts.,5 Experiments,[0],[0]
"First, we look at the impact of our pruning method on perplexity of a held-out validation set, across a variety of settings.",5 Experiments,[0],[0]
"Second, we take a closer look at how the model evolves through the training process.",5 Experiments,[0],[0]
"Finally, we explore the downstream impact of our method on a statistical phrase-based machine translation system.",5 Experiments,[0],[0]
"We first look at the impact that the `∞,1 regularizer has on the perplexity of our validation set.",5.1 Evaluating perplexity and network size,[0],[0]
The main results are shown in Table 1.,5.1 Evaluating perplexity and network size,[0],[0]
"For λ ≤ 0.01, the regularizer seems to have little impact: no hidden units are pruned, and perplexity is also not affected.",5.1 Evaluating perplexity and network size,[0],[0]
"For λ = 1, on the other hand, most hidden units are pruned – apparently too many, since perplexity is worse.",5.1 Evaluating perplexity and network size,[0],[0]
"But for λ = 0.1, we see that we are able to prune out many hidden units: up to half of the first layer, with little impact on perplexity.",5.1 Evaluating perplexity and network size,[0],[0]
"We found this to be consistent across all our experiments, varying n-gram size, initial hidden layer size, and vocabulary size.
",5.1 Evaluating perplexity and network size,[0],[0]
Table 2 shows the same information for 5-gram models trained on the larger Gigaword AFP corpus.,5.1 Evaluating perplexity and network size,[0],[0]
"These numbers look very similar to those on Europarl: again λ = 0.1 works best, and, counter to expectation, even the final number of units is similar.
",5.1 Evaluating perplexity and network size,[0],[0]
"Table 3 shows the result of varying the vocabulary size: again λ = 0.1 works best, and, although it is not shown in the table, we also found that the final number of units did not depend strongly on the vocabulary size.
",5.1 Evaluating perplexity and network size,[0],[0]
"Table 4 shows results using the `2,1 norm (Europarl corpus, 5-grams, 100k vocabulary).",5.1 Evaluating perplexity and network size,[0],[0]
"Since this is a different regularizer, there isn’t any reason to expect that λ behaves the same way, and indeed, a smaller value of λ seems to work best.",5.1 Evaluating perplexity and network size,[0],[0]
We also studied the evolution of the network over the training process to gain some insights into how the method works.,5.2 A closer look at training,[0],[0]
"The first question we want to
answer is whether the method is simply removing units, or converging on an optimal number of units.",5.2 A closer look at training,[0],[0]
"Figure 5 suggests that it is a little of both: if we start with too many units (900 or 1000), the method converges to the same number regardless of how many extra units there were initially.",5.2 A closer look at training,[0],[0]
"But if we start with a smaller number of units, the method still prunes away about 50 units.
",5.2 A closer look at training,[0],[0]
"Next, we look at the behavior over time of different regularization strengths λ.",5.2 A closer look at training,[0],[0]
"We found that not only does λ = 1 prune out too many units, it does so at the very first iteration (Figure 6, above), perhaps prematurely.",5.2 A closer look at training,[0],[0]
"By contrast, the λ = 0.1 run prunes out units gradually.",5.2 A closer look at training,[0],[0]
"By plotting these curves together with perplexity (Figure 6, below), we can see that the λ = 0.1 run is fitting the model and pruning it at the same time, which seems preferable to fitting without any pruning (λ =
0.01) or pruning first and then fitting (λ = 1).",5.2 A closer look at training,[0],[0]
"We can also visualize the weight matrix itself over time (Figure 7), for λ = 0.1.",5.2 A closer look at training,[0],[0]
"It is striking that although this setting fits the model and prunes it at the same time, as argued above, by the first iteration it already seems to have decided roughly how many units it will eventually prune.",5.2 A closer look at training,[0],[0]
We also looked at the impact of our method on statistical machine translation systems.,5.3 Evaluating on machine translation,[0],[0]
"We used the Moses toolkit (Koehn et al., 2007) to build a phrase based machine translation system with a traditional 5-gram LM trained on the target side of our bitext.",5.3 Evaluating on machine translation,[0],[0]
We augmented this system with neural LMs trained on the Europarl data and the Gigaword AFP data.,5.3 Evaluating on machine translation,[0],[0]
"Based on the results from the perplexity experiments, we looked at models both built with a λ = 0.1 regularizer, and without regularization (λ = 0).
",5.3 Evaluating on machine translation,[0],[0]
We built our system using the newscommentary dataset v8.,5.3 Evaluating on machine translation,[0],[0]
We tuned our model using newstest13 and evaluated using newstest14.,5.3 Evaluating on machine translation,[0],[0]
"After standard cleaning and tokenization, there were 155k parallel sentences in the newscommentary dataset, and 3,000 sentences each for the tuning and test sets.
",5.3 Evaluating on machine translation,[0],[0]
"Table 5 shows that the addition of a neural LM helps substantially over the baseline, with improvements of up to 2 BLEU.",5.3 Evaluating on machine translation,[0],[0]
"Using the Europarl model, the BLEU scores obtained without and with regularization were not significantly different (p ≥ 0.05), consistent with the negligible perplexity difference between these models.",5.3 Evaluating on machine translation,[0],[0]
"On the Gigaword AFP model, regularization did decrease the BLEU score by 0.3, consistent with the small perplexity increase of the regularized model.",5.3 Evaluating on machine translation,[0],[0]
"The decrease is statistically significant, but small compared with the overall benefit of adding a neural LM.",5.3 Evaluating on machine translation,[0],[0]
Researchers have been exploring the use of neural networks for language modeling for a long time.,6 Related Work,[0],[0]
Schmidhuber and Heil (1996) proposed a character n-gram model using neural networks which they used for text compression.,6 Related Work,[0],[0]
"Xu and Rudnicky (2000) proposed a word-based probability model using a softmax output layer trained using cross-entropy, but only for bigrams.",6 Related Work,[0],[0]
Bengio et al. (2003) defined a probabilistic word n-gram model and demonstrated improvements over conventional smoothed language models.,6 Related Work,[0],[0]
Mnih and Teh (2012) sped up training of log-bilinear language models through the use of noise-contrastive estimation (NCE).,6 Related Work,[0],[0]
"Vaswani et al. (2013) also used NCE to train the architecture of Bengio et al. (2003), and were able to integrate a largevocabulary language model directly into a machine translation decoder.",6 Related Work,[0],[0]
"Baltescu et al. (2014) describe a similar model, with extensions like a hierarchical softmax (based on Brown clustering) and direct n-gram features.
",6 Related Work,[0],[0]
"Beyond feed-forward neural network language models, researchers have explored using more complicated neural network architectures.",6 Related Work,[0],[0]
"RNNLM is an open-source implementation of a language model using recurrent neural networks (RNN) where connections between units can form directed cycles (Mikolov et al., 2011).",6 Related Work,[0],[0]
Sundermeyer et al. (2015) use the long-short term memory (LSTM) neural architecture to show a perplexity improvement over the RNNLM toolkit.,6 Related Work,[0],[0]
"In future work, we plan on exploring how our method could improve these more complicated neural models as well.
",6 Related Work,[0],[0]
Automatically limiting the size of neural networks is an old idea.,6 Related Work,[0],[0]
"The “Optimal Brain Damage” (OBD) technique (LeCun et al., 1989) computes a saliency based on the second derivative of the objective function with respect to each parameter.",6 Related Work,[0],[0]
"The parameters are then sorted by saliency, and the lowest-saliency parameters are pruned.",6 Related Work,[0],[0]
"The pruning process is separate from the training process, whereas regularization performs training and pruning simultaneously.",6 Related Work,[0],[0]
"Regularization in neural networks is also an old idea; for example, Nowland and Hinton (1992) mention both `22 and `0 regularization.",6 Related Work,[0],[0]
"Our method develops on this idea by using a mixed norm to prune units, rather than parameters.
",6 Related Work,[0],[0]
"Srivastava et al. introduce a method called dropout in which units are directly deactivated at random during training (Srivastava et al., 2014), which induces sparsity in the hidden unit activations.",6 Related Work,[0],[0]
"However, at the end of training, all units are reactivated, as the goal of dropout is to reduce overfitting, not to reduce network size.",6 Related Work,[0],[0]
"Thus, dropout and our method seem to be complementary.",6 Related Work,[0],[0]
"We have presented a method for auto-sizing a neural network during training by removing units using a `∞,1 regularizer.",7 Conclusion,[0],[0]
"This regularizer drives a unit’s input weights as a group down to zero, allowing the unit to be pruned.",7 Conclusion,[0],[0]
"We can thus prune units out of our network during training with minimal impact to held-out perplexity or downstream performance of a machine translation system.
",7 Conclusion,[0],[0]
"Our results showed empirically that the choice
of a regularization coefficient of 0.1 was robust to initial configuration parameters of initial network size, vocabulary size, n-gram order, and training corpus.",7 Conclusion,[0],[0]
"Furthermore, imposing a single regularizer on the objective function can tune all of the hidden layers of a network with one setting.",7 Conclusion,[0],[0]
"This reduces the need to conduct expensive, multi-dimensional grid searches in order to determine optimal sizes.
",7 Conclusion,[0],[0]
We have demonstrated the power and efficacy of this method on a feed-forward neural network for language modeling though experiments on perplexity and machine translation.,7 Conclusion,[0],[0]
"However, this method is general enough that it should be applicable to other domains, both inside natural language processing and outside.",7 Conclusion,[0],[0]
"As neural models become more pervasive in natural language processing, the ability to auto-size networks for fast experimentation and quick exploration will become increasingly important.",7 Conclusion,[0],[0]
"We would like to thank Tomer Levinboim, Antonios Anastasopoulos, and Ashish Vaswani for their helpful discussions, as well as the reviewers for their assistance and feedback.",Acknowledgments,[0],[0]
Neural networks have been shown to improve performance across a range of natural-language tasks.,abstractText,[0],[0]
"However, designing and training them can be complicated.",abstractText,[0],[0]
"Frequently, researchers resort to repeated experimentation to pick optimal settings.",abstractText,[0],[0]
"In this paper, we address the issue of choosing the correct number of units in hidden layers.",abstractText,[0],[0]
"We introduce a method for automatically adjusting network size by pruning out hidden units through `∞,1 and `2,1 regularization.",abstractText,[0],[0]
We apply this method to language modeling and demonstrate its ability to correctly choose the number of hidden units while maintaining perplexity.,abstractText,[0],[0]
We also include these models in a machine translation decoder and show that these smaller neural models maintain the significant improvements of their unpruned versions.,abstractText,[0],[0]
Auto-Sizing Neural Networks: With Applications to n-gram Language Models,title,[0],[0]
"There has been a staggering increase in progress on generative modeling in recent years, built largely upon fundamental advances such as generative adversarial networks (Goodfellow et al., 2014), variational inference (Kingma & Welling, 2013), and autoregressive density estimation (van den Oord et al., 2016c).",1. Introduction,[0],[0]
"These have led to breakthroughs in state-ofthe-art generation of natural images (Karras et al., 2017) and audio (van den Oord et al., 2016a), and even been used for unsupervised learning of disentangled representations (Higgins et al., 2017; Chen et al., 2016).",1. Introduction,[0],[0]
"These domains often have real-valued distributions with underlying metrics; that is, there is a domain-specific notion of similarity between data points.",1. Introduction,[0],[0]
"This similarity is ignored by the predominant work-horse of generative modeling, the Kullback-Leibler (KL) divergence.",1. Introduction,[0],[0]
"Progress is now being made towards algorithms that optimize with respect to these underlying metrics (Arjovsky et al., 2017; Bousquet et al., 2017).
",1. Introduction,[0],[0]
"*Equal contribution 1DeepMind, London, UK.",1. Introduction,[0],[0]
"Correspondence to: Georg Ostrovski <ostrovski@google.com>, Will Dabney <wdabney@google.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"In this paper, we present a novel approach to generative modeling, that, while strikingly different from existing methods, is grounded in the well-understood statistical methods of quantile regression.",1. Introduction,[0],[0]
"Unlike the majority of recent work, we approach generative modeling without the use of the KL divergence, and without explicitly approximating a likelihood model.",1. Introduction,[0],[0]
"Like GANs, in this way we produce an implicitly defined model, but unlike GANs our optimization procedure is inherently stable and lacks degenerate solutions which cause loss of diversity and mode collapse.
",1. Introduction,[0],[0]
"Much of the recent research on GANs has been focused on improving stability (Radford et al., 2015; Arjovsky et al., 2017; Daskalakis et al., 2017) and sample diversity (Gulrajani et al., 2017; Salimans et al., 2016; 2018).",1. Introduction,[0],[0]
"By stark contrast, methods such as PixelCNN (van den Oord et al., 2016b) readily produce high diversity, but due to their use of KL divergence are unable to make reasonable trade-offs between likelihood and perceptual similarity (Theis et al., 2015; Bellemare et al., 2017; Bousquet et al., 2017).
",1. Introduction,[0],[0]
"Our proposed method, autoregressive implicit quantile networks (AIQN), combines the benefits of both: a loss function that respects the underlying metric of the data leading to improved perceptual quality, and a stable optimization process leading to highly diverse samples.",1. Introduction,[0],[0]
"While there has been an increasing tendency towards complex architectures (Chen et al., 2017; Salimans et al., 2017) and multiple objective loss functions to overcome these challenges, AIQN is conceptually simple and does not rely on any special architecture or optimization techniques.",1. Introduction,[0],[0]
"Empirically it proves to be robust to hyperparameter variations and easy to optimize.
",1. Introduction,[0],[0]
"Our work is motivated by the recent advances achieved by reframing GANs in terms of optimal transport, leading to the Wasserstein GAN algorithm (Arjovsky et al., 2017), as well as work towards understanding the relationship between optimal transport and both GANs and VAEs (Bousquet et al., 2017).",1. Introduction,[0],[0]
"In agreement with these results, we focus on loss functions grounded in perceptually meaningful metrics.",1. Introduction,[0],[0]
"We build upon recent work in distributional reinforcement learning (Dabney et al., 2018a), which has begun to bridge the gap between approaches in reinforcement learning and unsupervised learning.",1. Introduction,[0],[0]
"Towards a practical algorithm we base our experimental results on Gated PixelCNN (van den Oord et al., 2016b), and show that using AIQN significantly im-
proves objective performance on CIFAR-10 and ImageNet 32x32 in terms of Fréchet Inception Distance (FID) and Inception score, as well as subjective perceptual quality in image samples and inpainting.",1. Introduction,[0],[0]
"We begin by establishing some notation, before turning to a review of three of the most prevalent methods for generative modeling.",2. Background,[0],[0]
"Calligraphic letters (e.g.X ) denote sets or spaces, capital letters (e.g. X) denote random variables, and lower case letters (e.g. x) indicate values.",2. Background,[0],[0]
"A probability distribution with random variable X ∈ X is denoted pX ∈P(X ), its cumulative distribution function (c.d.f.)",2. Background,[0],[0]
"FX , and inverse c.d.f. or quantile function QX = F−1X .",2. Background,[0],[0]
When probability distributions or quantile functions are parameterized by some θ,2. Background,[0],[0]
"we will write pθ or Qθ recognizing that here we do not view θ as a random variable.
",2. Background,[0],[0]
"Perhaps the simplest way to approach generative modeling of a random variableX ∈ X is by fixing some discretization ofX into n separate values, say x1, . . .",2. Background,[0],[0]
", xn ∈ X , and parameterize the approximate distribution with pθ(xi) ∝",2. Background,[0],[0]
exp(θi).,2. Background,[0],[0]
"This type of categorical parameterization is widely used, only slightly less commonly when X does not lend itself naturally to such a partitioning.",2. Background,[0],[0]
"Typically, the parameters θ are optimized to minimize the Kullback-Leibler (KL) divergence between observed values of X and the model pθ, θ∗ = arg minθDKL(pX‖pθ).",2. Background,[0],[0]
"However, this is only tractable whenX is a small discrete set or at best low-dimensional.",2. Background,[0],[0]
A common method for extending a generative model or density estimator to multivariate distributions is to factor the density as a product of scalarvalued conditional distributions.,2. Background,[0],[0]
"Let X = (X1, . . .",2. Background,[0],[0]
", Xn), then for any permutation of the dimensions σ :",2. Background,[0],[0]
"Nn → Nn,
pX(x) =",2. Background,[0],[0]
"n∏ i=1 pXσ(i)(xσ(i)|xσ(1), . . .",2. Background,[0],[0]
", xσ(i−1)).",2. Background,[0],[0]
"(1)
When the conditional density is modeled by a simple (e.g. Gaussian) base distribution, the ordering of the dimensions can be crucial (Papamakarios et al., 2017).",2. Background,[0],[0]
"However, it is common practice to choose an arbitrary ordering and rely upon a more powerful conditional model to avoid these problems.",2. Background,[0],[0]
"This class of models includes PixelRNN and PixelCNN (van den Oord et al., 2016c;b), MAF (Papamakarios et al., 2017), MADE (Germain et al., 2015), and many others.",2. Background,[0],[0]
"Fundamentally, all these approaches use the KL divergence as their loss function.
",2. Background,[0],[0]
"Another class of methods, generally known as latent variable methods, can bypass the need for autoregressive models using a different modeling assumption.",2. Background,[0],[0]
"Specifically, consider the Variational Autoencoder (VAE) (Kingma & Welling, 2013; Rezende et al., 2014), which represents
pθ as the marginalization over a latent random variable Z ∈ Z .",2. Background,[0],[0]
"The VAE is trained to maximize an approximate lower bound of the log-likelihood of the observations:
log pθ(x) ≥ −DKL(qθ(z|x)‖p(z))",2. Background,[0],[0]
+,2. Background,[0],[0]
E,2. Background,[0],[0]
"[log pθ(x|z)] .
",2. Background,[0],[0]
"Although VAEs are straightforward to implement and optimize, and effective at capturing structure in highdimensional spaces, they often miss fine-grained detail, resulting in blurry images.
",2. Background,[0],[0]
"Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) pose the problem of learning a generative model as a two-player zero-sum game between a discriminator D, attempting to distinguish between x ∼ pX (real data) and x ∼ pθ (generated data), and a generator G, attempting to generate data indistinguishable from real data.",2. Background,[0],[0]
"The generator is an implicit latent variable model that reparameterizes samples, typically from an isotropic Gaussian distribution, into values in X .",2. Background,[0],[0]
"The original formulation of GANs,
arg min G sup D",2. Background,[0],[0]
[ E X log(D(X)),2. Background,[0],[0]
"+ E Z log(1−D(G(Z))) ] ,
can be seen as minimizing a lower-bound on the JensenShannon divergence (Goodfellow et al., 2014; Bousquet et al., 2017).",2. Background,[0],[0]
"That is, even in the case of GANs we are often minimizing functions of the KL divergence1.
",2. Background,[0],[0]
"Many recent advances have come from principled combinations of these three fundamental methods (Makhzani et al., 2015; Dumoulin et al., 2016; Rosca et al., 2017).",2. Background,[0],[0]
"A common perspective in generative modeling is that the choice of model should encode existing metric assumptions about the domain, combined with a generic likelihoodfocused loss such as the KL divergence.",2.1. Distance Metrics and Loss Functions,[0],[0]
"Under this view, the KL’s general applicability and robust optimization properties make it a natural choice, and most implementations of the methods we reviewed in the previous section attempt to, at least indirectly, minimize a version of the KL.
",2.1. Distance Metrics and Loss Functions,[0],[0]
"On the other hand, as every model inevitably makes tradeoffs when constrained by capacity or limited training, it is desirable for its optimization goal to incentivize trade-offs prioritizing approximately correct solutions, when the data space is endowed with a metric supporting a meaningful (albeit potentially subjective) notion of approximation.",2.1. Distance Metrics and Loss Functions,[0],[0]
"It has been argued (Theis et al., 2015; Bousquet et al., 2017; Arjovsky et al., 2017; Bellemare et al., 2017) that the KL may not always be appropriate from this perspective, by making sub-optimal trade-offs between likelihood and similarity.
",2.1. Distance Metrics and Loss Functions,[0],[0]
"1The Jensen-Shannon divergence is the sum of KLs between distributions P,Q and their uniform mixture M = 0.5(P +Q): JSD(P ||Q)",2.1. Distance Metrics and Loss Functions,[0],[0]
"= 0.5(DKL(P ||M) +DKL(Q||M)).
",2.1. Distance Metrics and Loss Functions,[0],[0]
"Indeed, many limitations of existing models can be traced back to the use of KL, and the resulting trade-offs in approximate solutions it implies.",2.1. Distance Metrics and Loss Functions,[0],[0]
"For instance, its use appears to play a central role in one of the primary failure modes of VAEs, that of blurry samples.",2.1. Distance Metrics and Loss Functions,[0],[0]
"Zhao et al. (2017) argue that the Gaussian posterior pθ(x|z) implies an overly simple model, which, when unable to perfectly fit the data, is forced to average (thus creating blur), and is not incentivized by the KL towards an alternative notion of approximate solution.",2.1. Distance Metrics and Loss Functions,[0],[0]
"Theis et al. (2015) emphasized that an improvement of log-likelihood does not necessarily translate to higher perceptual quality, and that the KL loss is more likely to produce atypical samples than some other training criteria.
",2.1. Distance Metrics and Loss Functions,[0],[0]
"We offer an alternative perspective: a good model should encode assumptions about the data distribution, whereas a good loss should encode the notion of similarity, that is, the underlying metric on the data space.",2.1. Distance Metrics and Loss Functions,[0],[0]
"From this point of view, the KL corresponds to an actual absence of explicit underlying metric, with complete focus on probability.
",2.1. Distance Metrics and Loss Functions,[0],[0]
"The optimal transport metrics Wc, for underlying metric c(x, x′), and in particular the p-Wasserstein distance, when c is an Lp metric, have frequently been proposed as being well-suited replacements to KL (Bousquet et al., 2017; Genevay et al., 2017).",2.1. Distance Metrics and Loss Functions,[0],[0]
"Briefly, the advantages are (1) avoidance of mode collapse (no need to choose between spreading over modes or collapsing to a single mode as in KL), and (2) the ability to trade off errors and incentivize approximations that respect the underlying metric.
",2.1. Distance Metrics and Loss Functions,[0],[0]
"Recently, Arjovsky et al. (2017) introduced the Wasserstein
GAN, reposing the two-player game as the estimation of the gradient of the 1-Wasserstein distance between the data and generator distributions.",2.1. Distance Metrics and Loss Functions,[0],[0]
"They reframe this in terms of the dual form of the 1-Wasserstein, with the critic estimating a function f which maximally separates the two distributions.",2.1. Distance Metrics and Loss Functions,[0],[0]
"While this is an exciting line of work, it still faces limitations when the critic solution is approximate, i.e. when f∗ is not found before each update.",2.1. Distance Metrics and Loss Functions,[0],[0]
"In this case, due to insufficient training of the critic (Bellemare et al., 2017) or limitations of the function approximator, the gradient direction produced can be arbitrarily bad (Bousquet et al., 2017).
",2.1. Distance Metrics and Loss Functions,[0],[0]
"Thus, we are left with the question of how to minimize a distribution loss respecting an underlying metric.",2.1. Distance Metrics and Loss Functions,[0],[0]
"Recent work in distributional reinforcement learning has proposed the use of quantile regression as a method for minimizing the 1-Wasserstein in the univariate case when approximating using a mixture of Dirac functions (Dabney et al., 2018b).",2.1. Distance Metrics and Loss Functions,[0],[0]
"In this section, we review quantile regression as a method for estimating the quantile function of a distribution at specific points, i.e. its inverse cumulative distribution function.",2.2. Quantile Regression,[0],[0]
"This leads to recent work on approximating a distribution by a neural network approximation of its quantile function, acting as a reparameterization of a random sample from the uniform distribution.
",2.2. Quantile Regression,[0],[0]
"The quantile regression loss (Koenker & Hallock, 2001) for a quantile at τ ∈",2.2. Quantile Regression,[0],[0]
"[0, 1] and error u (positive for underestimation and negative for overestimation) is given by ρτ (u) =",2.2. Quantile Regression,[0],[0]
(τ − I{u ≤,2.2. Quantile Regression,[0],[0]
0})u.,2.2. Quantile Regression,[0],[0]
It is an asymmetric loss function penalizing underestimation by weight τ and overestimation by weight 1,2.2. Quantile Regression,[0],[0]
− τ .,2.2. Quantile Regression,[0],[0]
"For a given scalar distribution Z with c.d.f. FZ and a quantile τ , the inverse c.d.f. q = F−1Z (τ) minimizes the expected quantile regression loss Ez∼Z",2.2. Quantile Regression,[0],[0]
[ρτ (z − q)].,2.2. Quantile Regression,[0],[0]
Using this loss allows one to train a neural network to approximate a scalar distribution represented by its inverse c.d.f.,2.2. Quantile Regression,[0],[0]
"For this, the network can output a fixed grid of quantiles (Dabney et al., 2018b), with the respective quantile regression losses being applied to each output independently.",2.2. Quantile Regression,[0],[0]
"A more effective approach is to provide the desired quantile τ as an additional input to the network, and train it to output the corresponding value of F−1Z (τ).",2.2. Quantile Regression,[0],[0]
"The implicit quantile network (IQN) model (Dabney et al., 2018a) reparameterizes a sample τ ∼ U([0, 1]) through a deterministic function to produce samples from the underlying data distribution.",2.2. Quantile Regression,[0],[0]
These two methods can be seen to belong to the top-right and bottom-right categories in Figure 1.,2.2. Quantile Regression,[0],[0]
"An IQN Qθ can be trained by stochastic gradient descent on the quantile regression loss, with u = z −Qθ(τ) and training samples (z, τ) drawn from z ∼ Z and τ ∼ U([0, 1]).
",2.2. Quantile Regression,[0],[0]
"One drawback to the quantile regression loss is that gradients do not scale with the magnitude of the error, but instead with the sign of the error and the quantile weight τ .",2.2. Quantile Regression,[0],[0]
This increases gradient variance and can negatively impact the final model’s sample quality.,2.2. Quantile Regression,[0],[0]
"Increasing the batch size, and thus averaging over more values of τ , would have the effect of lowering this variance.",2.2. Quantile Regression,[0],[0]
"Alternatively, we can smooth the gradients as the model converges by allowing errors, under some threshold κ, to be scaled with their magnitude, reverting to an expectile loss.",2.2. Quantile Regression,[0],[0]
"This results in the Huber quantile loss (Huber, 1964; Dabney et al., 2018b):
ρκτ (u) =
{ |τ−I{u≤0}| 2κ u
2, if |u| ≤ κ, |τ −",2.2. Quantile Regression,[0],[0]
I{u ≤ 0}|(|u|,2.2. Quantile Regression,[0],[0]
"− 12κ), otherwise.",2.2. Quantile Regression,[0],[0]
(2),2.2. Quantile Regression,[0],[0]
"Let X = (X1, . . .",3. Autoregressive Implicit Quantiles,[0],[0]
", Xn) ∈",3. Autoregressive Implicit Quantiles,[0],[0]
X1 × · · · × Xn = X be an ndimensional random variable.,3. Autoregressive Implicit Quantiles,[0],[0]
"We begin by analyzing the effect of two naive applications of IQN to modeling the distribution of X .
",3. Autoregressive Implicit Quantiles,[0],[0]
"First, suppose we use the same quantile target, τ ∈",3. Autoregressive Implicit Quantiles,[0],[0]
"[0, 1], for every output dimension.",3. Autoregressive Implicit Quantiles,[0],[0]
"The only modification to IQN would be to output n dimensions instead of 1, the loss being applied to each output dimension independently.",3. Autoregressive Implicit Quantiles,[0],[0]
This is equivalent to assuming that the dimensions of X are comonotonic.,3. Autoregressive Implicit Quantiles,[0],[0]
"Two random variables are comonotonic if and only if they can be expressed as nondecreasing (deterministic) functions of a single random variable (Dhaene et al., 2006).",3. Autoregressive Implicit Quantiles,[0],[0]
Thus a joint quantile function for a comonotonic X can be written as F−1X (τ) =,3. Autoregressive Implicit Quantiles,[0],[0]
"(F−1X1 (τ), F −1 X2
(τ), . . .",3. Autoregressive Implicit Quantiles,[0],[0]
", F−1Xn(τ)).",3. Autoregressive Implicit Quantiles,[0],[0]
"While there are many interesting uses for comonotonic random variables, we believe this assumption is too strong to be useful more broadly.
",3. Autoregressive Implicit Quantiles,[0],[0]
"Second, one could use a separate value τi ∈",3. Autoregressive Implicit Quantiles,[0],[0]
"[0, 1] for each Xi, with the IQN being unchanged from the first case.",3. Autoregressive Implicit Quantiles,[0],[0]
This corresponds to making an independence assumption on the dimensions of X .,3. Autoregressive Implicit Quantiles,[0],[0]
"Again we would expect this to be an unreasonably restrictive modeling assumption for many domains, such as the case of natural images.
",3. Autoregressive Implicit Quantiles,[0],[0]
"Now, we turn to our proposed approach of extending IQN to multivariate distributions.",3. Autoregressive Implicit Quantiles,[0],[0]
We fix an ordering of the n dimensions.,3. Autoregressive Implicit Quantiles,[0],[0]
"If the density function pX is expressed as a product of conditional likelihoods, as in Equation 1, then the joint c.d.f. can be written as
FX(x) = P(X1 ≤",3. Autoregressive Implicit Quantiles,[0],[0]
"x1, . .",3. Autoregressive Implicit Quantiles,[0],[0]
.,3. Autoregressive Implicit Quantiles,[0],[0]
", Xn ≤ xn),
",3. Autoregressive Implicit Quantiles,[0],[0]
= n∏ i=1,3. Autoregressive Implicit Quantiles,[0],[0]
"FXi|Xi−1,...,X1(xi).
",3. Autoregressive Implicit Quantiles,[0],[0]
"Furthermore, for τjoint = ∏n i=1 τi, we can write the jointquantile function of X as
F−1X (τjoint) =",3. Autoregressive Implicit Quantiles,[0],[0]
"(F −1 X1 (τ1), . . .",3. Autoregressive Implicit Quantiles,[0],[0]
", F −1 Xn|Xn−1,...(τn)).
",3. Autoregressive Implicit Quantiles,[0],[0]
"This approach has been used previously by Koenker & Xiao (2006), who introduced a quantile autoregression model for quantile regression on time-series.
",3. Autoregressive Implicit Quantiles,[0],[0]
We propose to extend IQN to an autoregressive model of the above conditional form of a joint-quantile function.,3. Autoregressive Implicit Quantiles,[0],[0]
"Denoting X1:i = X1 × · · · × Xi, let X̃ := ⋃n i=0 X1:i be the space of ‘partial’ data points.",3. Autoregressive Implicit Quantiles,[0],[0]
We can define the autoregressive IQN as a deterministic functionQθ :,3. Autoregressive Implicit Quantiles,[0],[0]
X̃ ×,3. Autoregressive Implicit Quantiles,[0],[0]
"[0, 1]n → X̃ , mapping partial samples x̃ ∈ X̃ and quantile targets τi ∈",3. Autoregressive Implicit Quantiles,[0],[0]
"[0, 1] to estimates of F−1X .",3. Autoregressive Implicit Quantiles,[0],[0]
We can then train Qθ using a quantile regression loss (Equation 2).,3. Autoregressive Implicit Quantiles,[0],[0]
"For generation, one can iterate x1:i = Qθ(x1:i−1, τi), on a sequence of growing partial samples2 x1:i−1 and independently sampled τi ∼ U([0, 1]), for i = 1, . . .",3. Autoregressive Implicit Quantiles,[0],[0]
", n, to finally obtain a sample x = x1:n.",3. Autoregressive Implicit Quantiles,[0],[0]
"As previously mentioned, for the restricted model class of a uniform mixture of Diracs, quantile regression can be shown to minimize the 1-Wasserstein metric (Dabney et al., 2018b).",3.1. Quantile Regression and the Wasserstein,[0],[0]
"We extend this analysis for the case of arbitrary approximate quantile functions, and find that quantile regression minimizes a closely related divergence which we call quantile divergence, defined, for any distributions P and Q, as
q(P,Q) := ∫ 1 0",3.1. Quantile Regression and the Wasserstein,[0],[0]
[∫ F−1Q (τ),3.1. Quantile Regression and the Wasserstein,[0],[0]
"F−1P (τ) (FP (x)− τ)dx ] dτ.
",3.1. Quantile Regression and the Wasserstein,[0],[0]
"Indeed, the expected quantile loss of any parameterized quantile function Q̄θ equals, up to a constant, the quantile divergence between P and the distribution Qθ implicitly defined by Q̄θ:
E τ∼U([0,1])",3.1. Quantile Regression and the Wasserstein,[0],[0]
[ E z∼P [ρτ (z − Q̄θ(τ))],3.1. Quantile Regression and the Wasserstein,[0],[0]
"] = q(P,Qθ) + h(P ),
where h(P ) does not depend on Qθ.",3.1. Quantile Regression and the Wasserstein,[0],[0]
"Thus quantile regression minimizes the quantile divergence q(P,Qθ) and the sample gradient ∇θρτ (z − Q̄θ(τ))",3.1. Quantile Regression and the Wasserstein,[0],[0]
"(for τ ∼ U([0, 1]) and z ∼ P ) is an unbiased estimate of ∇θq(P,Qθ).",3.1. Quantile Regression and the Wasserstein,[0],[0]
See Appendix for proofs.,3.1. Quantile Regression and the Wasserstein,[0],[0]
"Although IQN does not directly model the log-likelihood of the data distribution, observe that we can still query the implied density at a point (Jones, 1992):
∂
∂τ F−1X (τ) =
1
pX(F −1 X (τ))
.
",3.2. Quantile Density Function,[0],[0]
"Indeed, this quantity, known as the sparsity function (Tukey, 1965) or the quantile-density function (Parzen, 1979) plays
2Throughout we understand x0 = x1:0 ∈ X1:0 to denote the ‘empty tuple’, and the function Qθ to map this to a single unconditional sample x1 = x1:1 = Qθ(x0, τ1).
",3.2. Quantile Density Function,[0],[0]
"a central role in the analysis of quantile regression models (Koenker, 1994).",3.2. Quantile Density Function,[0],[0]
"A common approach involves choosing a bandwidth parameter h and estimating this quantity through finite-differences around the value of interest as (F−1X (τ+h)−F−1X (τ−h))/2h (Siddiqui, 1960).",3.2. Quantile Density Function,[0],[0]
"However, as we have the full quantile function, the quantile-density function can be computed exactly using a single step of back-propagation to compute ∂F
−1(τ) ∂τ .",3.2. Quantile Density Function,[0],[0]
"As this only allows
querying the density given the value of τ , application to general likelihoods would require finding the value of τ that produces the closest approximation to the query point x.",3.2. Quantile Density Function,[0],[0]
"Though arguably too inefficient for training, this could potentially be used to interrogate the model.",3.2. Quantile Density Function,[0],[0]
"To test our proposed method, which is architecturally compatible with many generative model approaches, we wanted to compare and contrast IQN, that is quantile regression and quantile reparameterization, with a method trained with an explicit parameterization to minimize KL divergence.",4. PixelIQN,[0],[0]
"A natural choice for this was PixelCNN, specifically we build upon the Gated PixelCNN of van den Oord et al. (2016b).
",4. PixelIQN,[0],[0]
"The Gated PixelCNN takes as input an image x ∼ X , sampled from the training distribution at training time, and potentially all zeros or partially generated at generation time, as well as a location-dependent context s.",4. PixelIQN,[0],[0]
"The model consists of a number of residual layer blocks, whose structure is chosen to allow each output pixel to be a function of all preceding input pixels (in a raster-scan order).",4. PixelIQN,[0],[0]
"At its core, each layer block computes two gated activations of the form
y = tanh(Wk,f ∗ x+ Vk,f ∗ s) σ(Wk,g ∗ x+",4. PixelIQN,[0],[0]
"Vk,g ∗ s), with k the layer index, ∗ denoting convolution, and Vk,f and Vk,g being 1 × 1 convolution kernels.",4. PixelIQN,[0],[0]
"See Figure 2 for a full schematic depiction of a Gated PixelCNN layer
block.",4. PixelIQN,[0],[0]
"After a number of such layer blocks, the PixelCNN produces a final output layer with shape (n, n, 3, 256), with a softmax across the final dimension, corresponding to the approximate conditional likelihood for the value of each pixel-channel.",4. PixelIQN,[0],[0]
"That is, the conditional likelihood is the product of these individual autoregressive models,
p(x|s) = 3n2∏ i=1",4. PixelIQN,[0],[0]
"p(xi|x1, . . .",4. PixelIQN,[0],[0]
", xi−1, si).
",4. PixelIQN,[0],[0]
"Typically the location-dependent conditioning term was used to condition on class labels, but here, we will use it to condition on the sample point3 τ ∈",4. PixelIQN,[0],[0]
"[0, 1]3n2 .",4. PixelIQN,[0],[0]
"Thus, in addition to the input image x we input, in place of s, the sample points τ = (τ1, . . .",4. PixelIQN,[0],[0]
", τ3n2) to be reparameterized, with each τi ∼ U([0, 1]).",4. PixelIQN,[0],[0]
"Finally, our network outputs only the full sample image of shape (n, n, 3), without the need for an additional softmax layer.",4. PixelIQN,[0],[0]
Note that the number of τ values generated exactly corresponds to the number of random draws from softmax distributions in the original PixelCNN.,4. PixelIQN,[0],[0]
"We are simply changing the role of the randomness, from a draw at the output to a part of the input.
",4. PixelIQN,[0],[0]
"Architecturally, our proposed model, PixelIQN, is exactly the network given by van den Oord et al. (2016b), with the one exception that we output only a single value per pixel-channel and do not require the softmax activations.
",4. PixelIQN,[0],[0]
"In PixelCNN training is done by passing the training image through the network, and training each output softmax distribution using the KL divergence between the training image and the approximate distribution,∑
i
DKL(δxi , p(·|x1, . . .",4. PixelIQN,[0],[0]
", xi−1)).
",4. PixelIQN,[0],[0]
"For PixelIQN, the input is the training image x and a sample point τ ∼ U([0, 1]3n2).",4. PixelIQN,[0],[0]
"The output values Qx(τ) ∈ R3n 2 are interpreted as the approximate quantile function at τ , Qx(τ)i = QX(τi|xi−1, . . .), trained with a single step of quantile regression towards the observed sample",4. PixelIQN,[0],[0]
"x:∑
i
",4. PixelIQN,[0],[0]
"ρκτi(xi −QX(τi|xi−1, . .",4. PixelIQN,[0],[0]
.)).,4. PixelIQN,[0],[0]
"We begin by demonstrating PixelIQN on CIFAR-10 (Krizhevsky & Hinton, 2009).",4.1. CIFAR-10,[0],[0]
"For comparison, we train both a baseline Gated PixelCNN and a PixelIQN.",4.1. CIFAR-10,[0],[0]
"Both models correspond to the 15-layer network variant in (van den Oord et al., 2016b), see Appendix for detailed hyperparameters and training procedure.",4.1. CIFAR-10,[0],[0]
"The two methods have substantially different loss functions, so we performed a
3Conditioning on labels remains possible (see Section 4.2).
hyperparameter search using a short training run, with the same number (500) of hyperparameter configurations evaluated for both models.",4.1. CIFAR-10,[0],[0]
"For all results, we report full training runs using the best found hyperparameters in each case.",4.1. CIFAR-10,[0],[0]
"The evaluation metric used for the hyperparameter search was the Fréchet Inception Distance (FID) (Heusel et al., 2017), see Appendix for details.",4.1. CIFAR-10,[0],[0]
"In addition to FID, we report Inception score (Salimans et al., 2016) for both models.
",4.1. CIFAR-10,[0],[0]
Figure 4 (left) shows Inception score and FID for both models evaluated at several points throughout training.,4.1. CIFAR-10,[0],[0]
"The fully trained PixelCNN achieves an Inception score and FID of 4.6 and 65.9 respectively, while PixelIQN substantially outperforms it with an Inception score of 5.3 and FID of 49.5.",4.1. CIFAR-10,[0],[0]
"This also compares favorably with e.g. WGAN (Arjovsky et al., 2017), which reaches an Inception score of 3.8.",4.1. CIFAR-10,[0],[0]
"For subjective evaluations, we give samples from both models in Figure 3.",4.1. CIFAR-10,[0],[0]
Samples coming from PixelIQN are much more visually coherent.,4.1. CIFAR-10,[0],[0]
"Of note, the PixelIQN model achieves
a performance level comparable to that of the fully trained PixelCNN with only about one third the number of training updates (and about one third of the wall-clock time).",4.1. CIFAR-10,[0],[0]
"Next, we turn to the small ImageNet dataset (Russakovsky et al., 2015), first used for generative modeling in the PixelRNN work (van den Oord et al., 2016c).",4.2. ImageNet 32x32,[0],[0]
"Again, we evaluate using FID and Inception score.",4.2. ImageNet 32x32,[0],[0]
"For this much harder dataset, we base our PixelCNN and PixelIQN models on the larger 20-layer variant used in (van den Oord et al., 2016b).",4.2. ImageNet 32x32,[0],[0]
"Due to substantially longer training time for this model, we did not perform additional hyperparameter tuning, and mostly used the same hyperparameter values as in the previous sections for both models; details can be found in the Appendix.
",4.2. ImageNet 32x32,[0],[0]
Figure 4 shows Inception score and FID throughout training of PixelCNN and PixelIQN.,4.2. ImageNet 32x32,[0],[0]
"Again, PixelIQN substan-
tially outperforms the baseline in terms of final performance and sample complexity.",4.2. ImageNet 32x32,[0],[0]
"For final scores and a comparison to state-of-the-art GAN models, see Table 1.",4.2. ImageNet 32x32,[0],[0]
Figure 5 shows random (non-cherry-picked) samples from both models.,4.2. ImageNet 32x32,[0],[0]
"Compared to PixelCNN, PixelIQN samples appear to have superior quality with more global consistency and less ‘high-frequency noise’.
",4.2. ImageNet 32x32,[0],[0]
"In Figure 6, we show the inpainting performance of PixelIQN, by fixing the top half of a validation set image as input and sampling repeatedly from the model to generate different completions.",4.2. ImageNet 32x32,[0],[0]
We note that the model consistently generates plausible completions with significant diversity between different completion samples for the same input image.,4.2. ImageNet 32x32,[0],[0]
"Meanwhile, WGAN-GP has been seen to produce deterministic completions (Bellemare et al., 2017).
",4.2. ImageNet 32x32,[0],[0]
"Following (van den Oord et al., 2016b), we also trained a class-conditional PixelIQN variant, providing to the model the one-hot class label corresponding to a training image (in addition to a τ sample).",4.2. ImageNet 32x32,[0],[0]
"Samples from a class-conditional model can be expected to have higher visual quality, as the class label provides log2(1000)",4.2. ImageNet 32x32,[0],[0]
"≈ 10 bits of information, see Figure 7.",4.2. ImageNet 32x32,[0],[0]
"As seen in Figure 4 and Table 1, class conditioning also further improves Inception score and FID.",4.2. ImageNet 32x32,[0],[0]
"To generate each sample for the computation of these scores, we sample one of 1000 class labels randomly, then generate an image conditioned on this label via the trained model.
",4.2. ImageNet 32x32,[0],[0]
"Finally, motivated by the very long training time for the large PixelCNN model (approximately 1 day per 100K training steps, on 16 NVIDIA Tesla P100 GPUs), we also trained smaller 15-layer versions of the models (same as the ones used on CIFAR-10) on the small ImageNet dataset.",4.2. ImageNet 32x32,[0],[0]
"For comparison, these take approximately 12 hours for 100K training steps on a single P100 GPU, or less than 3 hours on 8 P100 GPUs.",4.2. ImageNet 32x32,[0],[0]
"As expected, little PixelCNN, while suitable
for the CIFAR-10 dataset, fails to achieve competitive scores on the ImageNet dataset, achieving Inception score 5.1 and FID 66.4.",4.2. ImageNet 32x32,[0],[0]
"Astonishingly, little PixelIQN on this dataset reaches Inception score 7.3 and FID 38.5, see Figure 4 (right).",4.2. ImageNet 32x32,[0],[0]
"It thereby not only outperforms the little PixelCNN, but also the larger 20-layer version!",4.2. ImageNet 32x32,[0],[0]
"This strongly supports the hypothesis that PixelCNN, and potentially many other models, are constrained not only by their model capacity, but crucially also by the sub-optimal trade-offs made by their log-likelihood training criterion, failing to align with perceptual or evaluation metrics.",4.2. ImageNet 32x32,[0],[0]
Most existing generative models for images belong to one of two classes.,5. Discussion and Conclusions,[0],[0]
"The first are likelihood-based models, trained with an elementwise KL reconstruction loss, which,
while perceptually meaningless, provides robust optimization properties and high sample diversity.",5. Discussion and Conclusions,[0],[0]
"The second are GANs, trained based on a discriminator loss, typically better aligned with a perceptual metric and enabling the generator to produce realistic, globally consistent samples.",5. Discussion and Conclusions,[0],[0]
"Their advantages come at the cost of a harder optimization problem, high parameter sensitivity, and most importantly, a tendency to collapse modes of the data distribution.
",5. Discussion and Conclusions,[0],[0]
"AIQNs are a new, fundamentally different, technique for generative modeling.",5. Discussion and Conclusions,[0],[0]
"By using a quantile regression loss instead of KL divergence, they combine some of the best properties of the two model classes.",5. Discussion and Conclusions,[0],[0]
"By their nature, they preserve modes of the learned distribution, while producing perceptually appealing high-quality samples.",5. Discussion and Conclusions,[0],[0]
The inevitable approximation trade-offs a generative model makes when constrained by capacity or insufficient training can vary significantly depending on the loss used.,5. Discussion and Conclusions,[0],[0]
"We argue that the proposed quantile regression loss aligns more effectively with a given metric and therefore makes subjectively more advantageous trade-offs.
",5. Discussion and Conclusions,[0],[0]
Devising methods for quantile regression over multidimensional outputs is an active area of research.,5. Discussion and Conclusions,[0],[0]
"New methods are continuing to be investigated (Carlier et al., 2016; Hallin & Miroslav, 2016), and a promising direction for future work is to find ways to use these to replace autoregressive models.",5. Discussion and Conclusions,[0],[0]
"One approach to reducing the computational burden of such models is to apply AIQN to the latent dimensions
of a VAE.",5. Discussion and Conclusions,[0],[0]
"Similar in spirit to Rosca et al. (2017), this would use the VAE to reduce the dimensionality of the problem and the AIQN to sample from the true latent distribution.",5. Discussion and Conclusions,[0],[0]
"In the Appendix we give preliminary results using such an technique, on CelebA 64× 64 (Liu et al., 2015).",5. Discussion and Conclusions,[0],[0]
"We have shown that IQN, computationally cheap and technically simple, can be readily applied to existing architectures, PixelCNN and VAE (Appendix), improving robustness and sampling quality of the underlying model.",5. Discussion and Conclusions,[0],[0]
"We demonstrated that PixelIQN produces more realistic, globally coherent samples, and improves Inception score and FID.
",5. Discussion and Conclusions,[0],[0]
We further point out that many recent advances in generative models could be easily combined with our proposed method.,5. Discussion and Conclusions,[0],[0]
"Recent algorithmic improvements to GANs such as mini-batch discrimination and progressive growing (Salimans et al., 2016; Karras et al., 2017), while not strictly necessary in our work, could be applied to further improve performance.",5. Discussion and Conclusions,[0],[0]
"PixelCNN++ (Salimans et al., 2017) is an architectural improvement of PixelCNN, with several beneficial modifications supported by experimental evidence.",5. Discussion and Conclusions,[0],[0]
"Although we have built upon the original Gated PixelCNN in this work, we believe all of these modifications to be compatible with our work, except for the use of a mixture of logistics in place of PixelCNN’s softmax.",5. Discussion and Conclusions,[0],[0]
"As we have entirely replaced this model component, this change does not map onto our model.",5. Discussion and Conclusions,[0],[0]
"Of note, the motivation behind this change closely mirrors our own, in looking for a loss that respects the underlying metric between examples.",5. Discussion and Conclusions,[0],[0]
"The recent PixelSNAIL model (Chen et al., 2017) achieves stateof-the-art modeling performance by enhancing PixelCNN with ELU nonlinearities, modified block structure, and an attention mechanism.",5. Discussion and Conclusions,[0],[0]
"Again, all of these are fully compatible with our work and should improve results further.
",5. Discussion and Conclusions,[0],[0]
"Finally, the implicit quantile formulation lifts a number of architectural restrictions of previous generative models.",5. Discussion and Conclusions,[0],[0]
"Most importantly, the reparameterization as an inverse c.d.f. allows to learn distributions over continuous ranges without pre-specified boundaries or quantization.",5. Discussion and Conclusions,[0],[0]
"This enables modeling continuous-valued variables, for example for generation of sound (van den Oord et al., 2016a), opening multiple interesting avenues for further investigation.",5. Discussion and Conclusions,[0],[0]
We would like to acknowledge the important role many of our colleagues at DeepMind played for this work.,Acknowledgements,[0],[0]
"We especially thank Aäron van den Oord and Sander Dieleman for invaluable advice on the PixelCNN model; Ivo Danihelka and Danilo J. Rezende for careful reading and insightful comments on an earlier version of the paper; Igor Babuschkin, Alexandre Galashov, Dominik Grewe, Jacob Menick, and Mihaela Rosca for technical help.",Acknowledgements,[0],[0]
"We introduce autoregressive implicit quantile networks (AIQN), a fundamentally different approach to generative modeling than those commonly used, that implicitly captures the distribution using quantile regression.",abstractText,[0],[0]
"AIQN is able to achieve superior perceptual quality and improvements in evaluation metrics, without incurring a loss of sample diversity.",abstractText,[0],[0]
The method can be applied to many existing models and architectures.,abstractText,[0],[0]
"In this work we extend the PixelCNN model with AIQN and demonstrate results on CIFAR-10 and ImageNet using Inception score, FID, non-cherrypicked samples, and inpainting results.",abstractText,[0],[0]
We consistently observe that AIQN yields a highly stable algorithm that improves perceptual quality while maintaining a highly diverse distribution.,abstractText,[0],[0]
Autoregressive Quantile Networks for Generative Modeling,title,[0],[0]
"There has been a staggering increase in progress on generative modeling in recent years, built largely upon fundamental advances such as generative adversarial networks (Goodfellow et al., 2014), variational inference (Kingma & Welling, 2013), and autoregressive density estimation (van den Oord et al., 2016c).",1. Introduction,[0],[0]
"These have led to breakthroughs in state-ofthe-art generation of natural images (Karras et al., 2017) and audio (van den Oord et al., 2016a), and even been used for unsupervised learning of disentangled representations (Higgins et al., 2017; Chen et al., 2016).",1. Introduction,[0],[0]
"These domains often have real-valued distributions with underlying metrics; that is, there is a domain-specific notion of similarity between data points.",1. Introduction,[0],[0]
"This similarity is ignored by the predominant work-horse of generative modeling, the Kullback-Leibler (KL) divergence.",1. Introduction,[0],[0]
"Progress is now being made towards algorithms that optimize with respect to these underlying metrics (Arjovsky et al., 2017; Bousquet et al., 2017).
",1. Introduction,[0],[0]
"*Equal contribution 1DeepMind, London, UK.",1. Introduction,[0],[0]
"Correspondence to: Georg Ostrovski <ostrovski@google.com>, Will Dabney <wdabney@google.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"In this paper, we present a novel approach to generative modeling, that, while strikingly different from existing methods, is grounded in the well-understood statistical methods of quantile regression.",1. Introduction,[0],[0]
"Unlike the majority of recent work, we approach generative modeling without the use of the KL divergence, and without explicitly approximating a likelihood model.",1. Introduction,[0],[0]
"Like GANs, in this way we produce an implicitly defined model, but unlike GANs our optimization procedure is inherently stable and lacks degenerate solutions which cause loss of diversity and mode collapse.
",1. Introduction,[0],[0]
"Much of the recent research on GANs has been focused on improving stability (Radford et al., 2015; Arjovsky et al., 2017; Daskalakis et al., 2017) and sample diversity (Gulrajani et al., 2017; Salimans et al., 2016; 2018).",1. Introduction,[0],[0]
"By stark contrast, methods such as PixelCNN (van den Oord et al., 2016b) readily produce high diversity, but due to their use of KL divergence are unable to make reasonable trade-offs between likelihood and perceptual similarity (Theis et al., 2015; Bellemare et al., 2017; Bousquet et al., 2017).
",1. Introduction,[0],[0]
"Our proposed method, autoregressive implicit quantile networks (AIQN), combines the benefits of both: a loss function that respects the underlying metric of the data leading to improved perceptual quality, and a stable optimization process leading to highly diverse samples.",1. Introduction,[0],[0]
"While there has been an increasing tendency towards complex architectures (Chen et al., 2017; Salimans et al., 2017) and multiple objective loss functions to overcome these challenges, AIQN is conceptually simple and does not rely on any special architecture or optimization techniques.",1. Introduction,[0],[0]
"Empirically it proves to be robust to hyperparameter variations and easy to optimize.
",1. Introduction,[0],[0]
"Our work is motivated by the recent advances achieved by reframing GANs in terms of optimal transport, leading to the Wasserstein GAN algorithm (Arjovsky et al., 2017), as well as work towards understanding the relationship between optimal transport and both GANs and VAEs (Bousquet et al., 2017).",1. Introduction,[0],[0]
"In agreement with these results, we focus on loss functions grounded in perceptually meaningful metrics.",1. Introduction,[0],[0]
"We build upon recent work in distributional reinforcement learning (Dabney et al., 2018a), which has begun to bridge the gap between approaches in reinforcement learning and unsupervised learning.",1. Introduction,[0],[0]
"Towards a practical algorithm we base our experimental results on Gated PixelCNN (van den Oord et al., 2016b), and show that using AIQN significantly im-
proves objective performance on CIFAR-10 and ImageNet 32x32 in terms of Fréchet Inception Distance (FID) and Inception score, as well as subjective perceptual quality in image samples and inpainting.",1. Introduction,[0],[0]
"We begin by establishing some notation, before turning to a review of three of the most prevalent methods for generative modeling.",2. Background,[0],[0]
"Calligraphic letters (e.g.X ) denote sets or spaces, capital letters (e.g. X) denote random variables, and lower case letters (e.g. x) indicate values.",2. Background,[0],[0]
"A probability distribution with random variable X ∈ X is denoted pX ∈P(X ), its cumulative distribution function (c.d.f.)",2. Background,[0],[0]
"FX , and inverse c.d.f. or quantile function QX = F−1X .",2. Background,[0],[0]
When probability distributions or quantile functions are parameterized by some θ,2. Background,[0],[0]
"we will write pθ or Qθ recognizing that here we do not view θ as a random variable.
",2. Background,[0],[0]
"Perhaps the simplest way to approach generative modeling of a random variableX ∈ X is by fixing some discretization ofX into n separate values, say x1, . . .",2. Background,[0],[0]
", xn ∈ X , and parameterize the approximate distribution with pθ(xi) ∝",2. Background,[0],[0]
exp(θi).,2. Background,[0],[0]
"This type of categorical parameterization is widely used, only slightly less commonly when X does not lend itself naturally to such a partitioning.",2. Background,[0],[0]
"Typically, the parameters θ are optimized to minimize the Kullback-Leibler (KL) divergence between observed values of X and the model pθ, θ∗ = arg minθDKL(pX‖pθ).",2. Background,[0],[0]
"However, this is only tractable whenX is a small discrete set or at best low-dimensional.",2. Background,[0],[0]
A common method for extending a generative model or density estimator to multivariate distributions is to factor the density as a product of scalarvalued conditional distributions.,2. Background,[0],[0]
"Let X = (X1, . . .",2. Background,[0],[0]
", Xn), then for any permutation of the dimensions σ :",2. Background,[0],[0]
"Nn → Nn,
pX(x) =",2. Background,[0],[0]
"n∏ i=1 pXσ(i)(xσ(i)|xσ(1), . . .",2. Background,[0],[0]
", xσ(i−1)).",2. Background,[0],[0]
"(1)
When the conditional density is modeled by a simple (e.g. Gaussian) base distribution, the ordering of the dimensions can be crucial (Papamakarios et al., 2017).",2. Background,[0],[0]
"However, it is common practice to choose an arbitrary ordering and rely upon a more powerful conditional model to avoid these problems.",2. Background,[0],[0]
"This class of models includes PixelRNN and PixelCNN (van den Oord et al., 2016c;b), MAF (Papamakarios et al., 2017), MADE (Germain et al., 2015), and many others.",2. Background,[0],[0]
"Fundamentally, all these approaches use the KL divergence as their loss function.
",2. Background,[0],[0]
"Another class of methods, generally known as latent variable methods, can bypass the need for autoregressive models using a different modeling assumption.",2. Background,[0],[0]
"Specifically, consider the Variational Autoencoder (VAE) (Kingma & Welling, 2013; Rezende et al., 2014), which represents
pθ as the marginalization over a latent random variable Z ∈ Z .",2. Background,[0],[0]
"The VAE is trained to maximize an approximate lower bound of the log-likelihood of the observations:
log pθ(x) ≥ −DKL(qθ(z|x)‖p(z))",2. Background,[0],[0]
+,2. Background,[0],[0]
E,2. Background,[0],[0]
"[log pθ(x|z)] .
",2. Background,[0],[0]
"Although VAEs are straightforward to implement and optimize, and effective at capturing structure in highdimensional spaces, they often miss fine-grained detail, resulting in blurry images.
",2. Background,[0],[0]
"Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) pose the problem of learning a generative model as a two-player zero-sum game between a discriminator D, attempting to distinguish between x ∼ pX (real data) and x ∼ pθ (generated data), and a generator G, attempting to generate data indistinguishable from real data.",2. Background,[0],[0]
"The generator is an implicit latent variable model that reparameterizes samples, typically from an isotropic Gaussian distribution, into values in X .",2. Background,[0],[0]
"The original formulation of GANs,
arg min G sup D",2. Background,[0],[0]
[ E X log(D(X)),2. Background,[0],[0]
"+ E Z log(1−D(G(Z))) ] ,
can be seen as minimizing a lower-bound on the JensenShannon divergence (Goodfellow et al., 2014; Bousquet et al., 2017).",2. Background,[0],[0]
"That is, even in the case of GANs we are often minimizing functions of the KL divergence1.
",2. Background,[0],[0]
"Many recent advances have come from principled combinations of these three fundamental methods (Makhzani et al., 2015; Dumoulin et al., 2016; Rosca et al., 2017).",2. Background,[0],[0]
"A common perspective in generative modeling is that the choice of model should encode existing metric assumptions about the domain, combined with a generic likelihoodfocused loss such as the KL divergence.",2.1. Distance Metrics and Loss Functions,[0],[0]
"Under this view, the KL’s general applicability and robust optimization properties make it a natural choice, and most implementations of the methods we reviewed in the previous section attempt to, at least indirectly, minimize a version of the KL.
",2.1. Distance Metrics and Loss Functions,[0],[0]
"On the other hand, as every model inevitably makes tradeoffs when constrained by capacity or limited training, it is desirable for its optimization goal to incentivize trade-offs prioritizing approximately correct solutions, when the data space is endowed with a metric supporting a meaningful (albeit potentially subjective) notion of approximation.",2.1. Distance Metrics and Loss Functions,[0],[0]
"It has been argued (Theis et al., 2015; Bousquet et al., 2017; Arjovsky et al., 2017; Bellemare et al., 2017) that the KL may not always be appropriate from this perspective, by making sub-optimal trade-offs between likelihood and similarity.
",2.1. Distance Metrics and Loss Functions,[0],[0]
"1The Jensen-Shannon divergence is the sum of KLs between distributions P,Q and their uniform mixture M = 0.5(P +Q): JSD(P ||Q)",2.1. Distance Metrics and Loss Functions,[0],[0]
"= 0.5(DKL(P ||M) +DKL(Q||M)).
",2.1. Distance Metrics and Loss Functions,[0],[0]
"Indeed, many limitations of existing models can be traced back to the use of KL, and the resulting trade-offs in approximate solutions it implies.",2.1. Distance Metrics and Loss Functions,[0],[0]
"For instance, its use appears to play a central role in one of the primary failure modes of VAEs, that of blurry samples.",2.1. Distance Metrics and Loss Functions,[0],[0]
"Zhao et al. (2017) argue that the Gaussian posterior pθ(x|z) implies an overly simple model, which, when unable to perfectly fit the data, is forced to average (thus creating blur), and is not incentivized by the KL towards an alternative notion of approximate solution.",2.1. Distance Metrics and Loss Functions,[0],[0]
"Theis et al. (2015) emphasized that an improvement of log-likelihood does not necessarily translate to higher perceptual quality, and that the KL loss is more likely to produce atypical samples than some other training criteria.
",2.1. Distance Metrics and Loss Functions,[0],[0]
"We offer an alternative perspective: a good model should encode assumptions about the data distribution, whereas a good loss should encode the notion of similarity, that is, the underlying metric on the data space.",2.1. Distance Metrics and Loss Functions,[0],[0]
"From this point of view, the KL corresponds to an actual absence of explicit underlying metric, with complete focus on probability.
",2.1. Distance Metrics and Loss Functions,[0],[0]
"The optimal transport metrics Wc, for underlying metric c(x, x′), and in particular the p-Wasserstein distance, when c is an Lp metric, have frequently been proposed as being well-suited replacements to KL (Bousquet et al., 2017; Genevay et al., 2017).",2.1. Distance Metrics and Loss Functions,[0],[0]
"Briefly, the advantages are (1) avoidance of mode collapse (no need to choose between spreading over modes or collapsing to a single mode as in KL), and (2) the ability to trade off errors and incentivize approximations that respect the underlying metric.
",2.1. Distance Metrics and Loss Functions,[0],[0]
"Recently, Arjovsky et al. (2017) introduced the Wasserstein
GAN, reposing the two-player game as the estimation of the gradient of the 1-Wasserstein distance between the data and generator distributions.",2.1. Distance Metrics and Loss Functions,[0],[0]
"They reframe this in terms of the dual form of the 1-Wasserstein, with the critic estimating a function f which maximally separates the two distributions.",2.1. Distance Metrics and Loss Functions,[0],[0]
"While this is an exciting line of work, it still faces limitations when the critic solution is approximate, i.e. when f∗ is not found before each update.",2.1. Distance Metrics and Loss Functions,[0],[0]
"In this case, due to insufficient training of the critic (Bellemare et al., 2017) or limitations of the function approximator, the gradient direction produced can be arbitrarily bad (Bousquet et al., 2017).
",2.1. Distance Metrics and Loss Functions,[0],[0]
"Thus, we are left with the question of how to minimize a distribution loss respecting an underlying metric.",2.1. Distance Metrics and Loss Functions,[0],[0]
"Recent work in distributional reinforcement learning has proposed the use of quantile regression as a method for minimizing the 1-Wasserstein in the univariate case when approximating using a mixture of Dirac functions (Dabney et al., 2018b).",2.1. Distance Metrics and Loss Functions,[0],[0]
"In this section, we review quantile regression as a method for estimating the quantile function of a distribution at specific points, i.e. its inverse cumulative distribution function.",2.2. Quantile Regression,[0],[0]
"This leads to recent work on approximating a distribution by a neural network approximation of its quantile function, acting as a reparameterization of a random sample from the uniform distribution.
",2.2. Quantile Regression,[0],[0]
"The quantile regression loss (Koenker & Hallock, 2001) for a quantile at τ ∈",2.2. Quantile Regression,[0],[0]
"[0, 1] and error u (positive for underestimation and negative for overestimation) is given by ρτ (u) =",2.2. Quantile Regression,[0],[0]
(τ − I{u ≤,2.2. Quantile Regression,[0],[0]
0})u.,2.2. Quantile Regression,[0],[0]
It is an asymmetric loss function penalizing underestimation by weight τ and overestimation by weight 1,2.2. Quantile Regression,[0],[0]
− τ .,2.2. Quantile Regression,[0],[0]
"For a given scalar distribution Z with c.d.f. FZ and a quantile τ , the inverse c.d.f. q = F−1Z (τ) minimizes the expected quantile regression loss Ez∼Z",2.2. Quantile Regression,[0],[0]
[ρτ (z − q)].,2.2. Quantile Regression,[0],[0]
Using this loss allows one to train a neural network to approximate a scalar distribution represented by its inverse c.d.f.,2.2. Quantile Regression,[0],[0]
"For this, the network can output a fixed grid of quantiles (Dabney et al., 2018b), with the respective quantile regression losses being applied to each output independently.",2.2. Quantile Regression,[0],[0]
"A more effective approach is to provide the desired quantile τ as an additional input to the network, and train it to output the corresponding value of F−1Z (τ).",2.2. Quantile Regression,[0],[0]
"The implicit quantile network (IQN) model (Dabney et al., 2018a) reparameterizes a sample τ ∼ U([0, 1]) through a deterministic function to produce samples from the underlying data distribution.",2.2. Quantile Regression,[0],[0]
These two methods can be seen to belong to the top-right and bottom-right categories in Figure 1.,2.2. Quantile Regression,[0],[0]
"An IQN Qθ can be trained by stochastic gradient descent on the quantile regression loss, with u = z −Qθ(τ) and training samples (z, τ) drawn from z ∼ Z and τ ∼ U([0, 1]).
",2.2. Quantile Regression,[0],[0]
"One drawback to the quantile regression loss is that gradients do not scale with the magnitude of the error, but instead with the sign of the error and the quantile weight τ .",2.2. Quantile Regression,[0],[0]
This increases gradient variance and can negatively impact the final model’s sample quality.,2.2. Quantile Regression,[0],[0]
"Increasing the batch size, and thus averaging over more values of τ , would have the effect of lowering this variance.",2.2. Quantile Regression,[0],[0]
"Alternatively, we can smooth the gradients as the model converges by allowing errors, under some threshold κ, to be scaled with their magnitude, reverting to an expectile loss.",2.2. Quantile Regression,[0],[0]
"This results in the Huber quantile loss (Huber, 1964; Dabney et al., 2018b):
ρκτ (u) =
{ |τ−I{u≤0}| 2κ u
2, if |u| ≤ κ, |τ −",2.2. Quantile Regression,[0],[0]
I{u ≤ 0}|(|u|,2.2. Quantile Regression,[0],[0]
"− 12κ), otherwise.",2.2. Quantile Regression,[0],[0]
(2),2.2. Quantile Regression,[0],[0]
"Let X = (X1, . . .",3. Autoregressive Implicit Quantiles,[0],[0]
", Xn) ∈",3. Autoregressive Implicit Quantiles,[0],[0]
X1 × · · · × Xn = X be an ndimensional random variable.,3. Autoregressive Implicit Quantiles,[0],[0]
"We begin by analyzing the effect of two naive applications of IQN to modeling the distribution of X .
",3. Autoregressive Implicit Quantiles,[0],[0]
"First, suppose we use the same quantile target, τ ∈",3. Autoregressive Implicit Quantiles,[0],[0]
"[0, 1], for every output dimension.",3. Autoregressive Implicit Quantiles,[0],[0]
"The only modification to IQN would be to output n dimensions instead of 1, the loss being applied to each output dimension independently.",3. Autoregressive Implicit Quantiles,[0],[0]
This is equivalent to assuming that the dimensions of X are comonotonic.,3. Autoregressive Implicit Quantiles,[0],[0]
"Two random variables are comonotonic if and only if they can be expressed as nondecreasing (deterministic) functions of a single random variable (Dhaene et al., 2006).",3. Autoregressive Implicit Quantiles,[0],[0]
Thus a joint quantile function for a comonotonic X can be written as F−1X (τ) =,3. Autoregressive Implicit Quantiles,[0],[0]
"(F−1X1 (τ), F −1 X2
(τ), . . .",3. Autoregressive Implicit Quantiles,[0],[0]
", F−1Xn(τ)).",3. Autoregressive Implicit Quantiles,[0],[0]
"While there are many interesting uses for comonotonic random variables, we believe this assumption is too strong to be useful more broadly.
",3. Autoregressive Implicit Quantiles,[0],[0]
"Second, one could use a separate value τi ∈",3. Autoregressive Implicit Quantiles,[0],[0]
"[0, 1] for each Xi, with the IQN being unchanged from the first case.",3. Autoregressive Implicit Quantiles,[0],[0]
This corresponds to making an independence assumption on the dimensions of X .,3. Autoregressive Implicit Quantiles,[0],[0]
"Again we would expect this to be an unreasonably restrictive modeling assumption for many domains, such as the case of natural images.
",3. Autoregressive Implicit Quantiles,[0],[0]
"Now, we turn to our proposed approach of extending IQN to multivariate distributions.",3. Autoregressive Implicit Quantiles,[0],[0]
We fix an ordering of the n dimensions.,3. Autoregressive Implicit Quantiles,[0],[0]
"If the density function pX is expressed as a product of conditional likelihoods, as in Equation 1, then the joint c.d.f. can be written as
FX(x) = P(X1 ≤",3. Autoregressive Implicit Quantiles,[0],[0]
"x1, . .",3. Autoregressive Implicit Quantiles,[0],[0]
.,3. Autoregressive Implicit Quantiles,[0],[0]
", Xn ≤ xn),
",3. Autoregressive Implicit Quantiles,[0],[0]
= n∏ i=1,3. Autoregressive Implicit Quantiles,[0],[0]
"FXi|Xi−1,...,X1(xi).
",3. Autoregressive Implicit Quantiles,[0],[0]
"Furthermore, for τjoint = ∏n i=1 τi, we can write the jointquantile function of X as
F−1X (τjoint) =",3. Autoregressive Implicit Quantiles,[0],[0]
"(F −1 X1 (τ1), . . .",3. Autoregressive Implicit Quantiles,[0],[0]
", F −1 Xn|Xn−1,...(τn)).
",3. Autoregressive Implicit Quantiles,[0],[0]
"This approach has been used previously by Koenker & Xiao (2006), who introduced a quantile autoregression model for quantile regression on time-series.
",3. Autoregressive Implicit Quantiles,[0],[0]
We propose to extend IQN to an autoregressive model of the above conditional form of a joint-quantile function.,3. Autoregressive Implicit Quantiles,[0],[0]
"Denoting X1:i = X1 × · · · × Xi, let X̃ := ⋃n i=0 X1:i be the space of ‘partial’ data points.",3. Autoregressive Implicit Quantiles,[0],[0]
We can define the autoregressive IQN as a deterministic functionQθ :,3. Autoregressive Implicit Quantiles,[0],[0]
X̃ ×,3. Autoregressive Implicit Quantiles,[0],[0]
"[0, 1]n → X̃ , mapping partial samples x̃ ∈ X̃ and quantile targets τi ∈",3. Autoregressive Implicit Quantiles,[0],[0]
"[0, 1] to estimates of F−1X .",3. Autoregressive Implicit Quantiles,[0],[0]
We can then train Qθ using a quantile regression loss (Equation 2).,3. Autoregressive Implicit Quantiles,[0],[0]
"For generation, one can iterate x1:i = Qθ(x1:i−1, τi), on a sequence of growing partial samples2 x1:i−1 and independently sampled τi ∼ U([0, 1]), for i = 1, . . .",3. Autoregressive Implicit Quantiles,[0],[0]
", n, to finally obtain a sample x = x1:n.",3. Autoregressive Implicit Quantiles,[0],[0]
"As previously mentioned, for the restricted model class of a uniform mixture of Diracs, quantile regression can be shown to minimize the 1-Wasserstein metric (Dabney et al., 2018b).",3.1. Quantile Regression and the Wasserstein,[0],[0]
"We extend this analysis for the case of arbitrary approximate quantile functions, and find that quantile regression minimizes a closely related divergence which we call quantile divergence, defined, for any distributions P and Q, as
q(P,Q) := ∫ 1 0",3.1. Quantile Regression and the Wasserstein,[0],[0]
[∫ F−1Q (τ),3.1. Quantile Regression and the Wasserstein,[0],[0]
"F−1P (τ) (FP (x)− τ)dx ] dτ.
",3.1. Quantile Regression and the Wasserstein,[0],[0]
"Indeed, the expected quantile loss of any parameterized quantile function Q̄θ equals, up to a constant, the quantile divergence between P and the distribution Qθ implicitly defined by Q̄θ:
E τ∼U([0,1])",3.1. Quantile Regression and the Wasserstein,[0],[0]
[ E z∼P [ρτ (z − Q̄θ(τ))],3.1. Quantile Regression and the Wasserstein,[0],[0]
"] = q(P,Qθ) + h(P ),
where h(P ) does not depend on Qθ.",3.1. Quantile Regression and the Wasserstein,[0],[0]
"Thus quantile regression minimizes the quantile divergence q(P,Qθ) and the sample gradient ∇θρτ (z − Q̄θ(τ))",3.1. Quantile Regression and the Wasserstein,[0],[0]
"(for τ ∼ U([0, 1]) and z ∼ P ) is an unbiased estimate of ∇θq(P,Qθ).",3.1. Quantile Regression and the Wasserstein,[0],[0]
See Appendix for proofs.,3.1. Quantile Regression and the Wasserstein,[0],[0]
"Although IQN does not directly model the log-likelihood of the data distribution, observe that we can still query the implied density at a point (Jones, 1992):
∂
∂τ F−1X (τ) =
1
pX(F −1 X (τ))
.
",3.2. Quantile Density Function,[0],[0]
"Indeed, this quantity, known as the sparsity function (Tukey, 1965) or the quantile-density function (Parzen, 1979) plays
2Throughout we understand x0 = x1:0 ∈ X1:0 to denote the ‘empty tuple’, and the function Qθ to map this to a single unconditional sample x1 = x1:1 = Qθ(x0, τ1).
",3.2. Quantile Density Function,[0],[0]
"a central role in the analysis of quantile regression models (Koenker, 1994).",3.2. Quantile Density Function,[0],[0]
"A common approach involves choosing a bandwidth parameter h and estimating this quantity through finite-differences around the value of interest as (F−1X (τ+h)−F−1X (τ−h))/2h (Siddiqui, 1960).",3.2. Quantile Density Function,[0],[0]
"However, as we have the full quantile function, the quantile-density function can be computed exactly using a single step of back-propagation to compute ∂F
−1(τ) ∂τ .",3.2. Quantile Density Function,[0],[0]
"As this only allows
querying the density given the value of τ , application to general likelihoods would require finding the value of τ that produces the closest approximation to the query point x.",3.2. Quantile Density Function,[0],[0]
"Though arguably too inefficient for training, this could potentially be used to interrogate the model.",3.2. Quantile Density Function,[0],[0]
"To test our proposed method, which is architecturally compatible with many generative model approaches, we wanted to compare and contrast IQN, that is quantile regression and quantile reparameterization, with a method trained with an explicit parameterization to minimize KL divergence.",4. PixelIQN,[0],[0]
"A natural choice for this was PixelCNN, specifically we build upon the Gated PixelCNN of van den Oord et al. (2016b).
",4. PixelIQN,[0],[0]
"The Gated PixelCNN takes as input an image x ∼ X , sampled from the training distribution at training time, and potentially all zeros or partially generated at generation time, as well as a location-dependent context s.",4. PixelIQN,[0],[0]
"The model consists of a number of residual layer blocks, whose structure is chosen to allow each output pixel to be a function of all preceding input pixels (in a raster-scan order).",4. PixelIQN,[0],[0]
"At its core, each layer block computes two gated activations of the form
y = tanh(Wk,f ∗ x+ Vk,f ∗ s) σ(Wk,g ∗ x+",4. PixelIQN,[0],[0]
"Vk,g ∗ s), with k the layer index, ∗ denoting convolution, and Vk,f and Vk,g being 1 × 1 convolution kernels.",4. PixelIQN,[0],[0]
"See Figure 2 for a full schematic depiction of a Gated PixelCNN layer
block.",4. PixelIQN,[0],[0]
"After a number of such layer blocks, the PixelCNN produces a final output layer with shape (n, n, 3, 256), with a softmax across the final dimension, corresponding to the approximate conditional likelihood for the value of each pixel-channel.",4. PixelIQN,[0],[0]
"That is, the conditional likelihood is the product of these individual autoregressive models,
p(x|s) = 3n2∏ i=1",4. PixelIQN,[0],[0]
"p(xi|x1, . . .",4. PixelIQN,[0],[0]
", xi−1, si).
",4. PixelIQN,[0],[0]
"Typically the location-dependent conditioning term was used to condition on class labels, but here, we will use it to condition on the sample point3 τ ∈",4. PixelIQN,[0],[0]
"[0, 1]3n2 .",4. PixelIQN,[0],[0]
"Thus, in addition to the input image x we input, in place of s, the sample points τ = (τ1, . . .",4. PixelIQN,[0],[0]
", τ3n2) to be reparameterized, with each τi ∼ U([0, 1]).",4. PixelIQN,[0],[0]
"Finally, our network outputs only the full sample image of shape (n, n, 3), without the need for an additional softmax layer.",4. PixelIQN,[0],[0]
Note that the number of τ values generated exactly corresponds to the number of random draws from softmax distributions in the original PixelCNN.,4. PixelIQN,[0],[0]
"We are simply changing the role of the randomness, from a draw at the output to a part of the input.
",4. PixelIQN,[0],[0]
"Architecturally, our proposed model, PixelIQN, is exactly the network given by van den Oord et al. (2016b), with the one exception that we output only a single value per pixel-channel and do not require the softmax activations.
",4. PixelIQN,[0],[0]
"In PixelCNN training is done by passing the training image through the network, and training each output softmax distribution using the KL divergence between the training image and the approximate distribution,∑
i
DKL(δxi , p(·|x1, . . .",4. PixelIQN,[0],[0]
", xi−1)).
",4. PixelIQN,[0],[0]
"For PixelIQN, the input is the training image x and a sample point τ ∼ U([0, 1]3n2).",4. PixelIQN,[0],[0]
"The output values Qx(τ) ∈ R3n 2 are interpreted as the approximate quantile function at τ , Qx(τ)i = QX(τi|xi−1, . . .), trained with a single step of quantile regression towards the observed sample",4. PixelIQN,[0],[0]
"x:∑
i
",4. PixelIQN,[0],[0]
"ρκτi(xi −QX(τi|xi−1, . .",4. PixelIQN,[0],[0]
.)).,4. PixelIQN,[0],[0]
"We begin by demonstrating PixelIQN on CIFAR-10 (Krizhevsky & Hinton, 2009).",4.1. CIFAR-10,[0],[0]
"For comparison, we train both a baseline Gated PixelCNN and a PixelIQN.",4.1. CIFAR-10,[0],[0]
"Both models correspond to the 15-layer network variant in (van den Oord et al., 2016b), see Appendix for detailed hyperparameters and training procedure.",4.1. CIFAR-10,[0],[0]
"The two methods have substantially different loss functions, so we performed a
3Conditioning on labels remains possible (see Section 4.2).
hyperparameter search using a short training run, with the same number (500) of hyperparameter configurations evaluated for both models.",4.1. CIFAR-10,[0],[0]
"For all results, we report full training runs using the best found hyperparameters in each case.",4.1. CIFAR-10,[0],[0]
"The evaluation metric used for the hyperparameter search was the Fréchet Inception Distance (FID) (Heusel et al., 2017), see Appendix for details.",4.1. CIFAR-10,[0],[0]
"In addition to FID, we report Inception score (Salimans et al., 2016) for both models.
",4.1. CIFAR-10,[0],[0]
Figure 4 (left) shows Inception score and FID for both models evaluated at several points throughout training.,4.1. CIFAR-10,[0],[0]
"The fully trained PixelCNN achieves an Inception score and FID of 4.6 and 65.9 respectively, while PixelIQN substantially outperforms it with an Inception score of 5.3 and FID of 49.5.",4.1. CIFAR-10,[0],[0]
"This also compares favorably with e.g. WGAN (Arjovsky et al., 2017), which reaches an Inception score of 3.8.",4.1. CIFAR-10,[0],[0]
"For subjective evaluations, we give samples from both models in Figure 3.",4.1. CIFAR-10,[0],[0]
Samples coming from PixelIQN are much more visually coherent.,4.1. CIFAR-10,[0],[0]
"Of note, the PixelIQN model achieves
a performance level comparable to that of the fully trained PixelCNN with only about one third the number of training updates (and about one third of the wall-clock time).",4.1. CIFAR-10,[0],[0]
"Next, we turn to the small ImageNet dataset (Russakovsky et al., 2015), first used for generative modeling in the PixelRNN work (van den Oord et al., 2016c).",4.2. ImageNet 32x32,[0],[0]
"Again, we evaluate using FID and Inception score.",4.2. ImageNet 32x32,[0],[0]
"For this much harder dataset, we base our PixelCNN and PixelIQN models on the larger 20-layer variant used in (van den Oord et al., 2016b).",4.2. ImageNet 32x32,[0],[0]
"Due to substantially longer training time for this model, we did not perform additional hyperparameter tuning, and mostly used the same hyperparameter values as in the previous sections for both models; details can be found in the Appendix.
",4.2. ImageNet 32x32,[0],[0]
Figure 4 shows Inception score and FID throughout training of PixelCNN and PixelIQN.,4.2. ImageNet 32x32,[0],[0]
"Again, PixelIQN substan-
tially outperforms the baseline in terms of final performance and sample complexity.",4.2. ImageNet 32x32,[0],[0]
"For final scores and a comparison to state-of-the-art GAN models, see Table 1.",4.2. ImageNet 32x32,[0],[0]
Figure 5 shows random (non-cherry-picked) samples from both models.,4.2. ImageNet 32x32,[0],[0]
"Compared to PixelCNN, PixelIQN samples appear to have superior quality with more global consistency and less ‘high-frequency noise’.
",4.2. ImageNet 32x32,[0],[0]
"In Figure 6, we show the inpainting performance of PixelIQN, by fixing the top half of a validation set image as input and sampling repeatedly from the model to generate different completions.",4.2. ImageNet 32x32,[0],[0]
We note that the model consistently generates plausible completions with significant diversity between different completion samples for the same input image.,4.2. ImageNet 32x32,[0],[0]
"Meanwhile, WGAN-GP has been seen to produce deterministic completions (Bellemare et al., 2017).
",4.2. ImageNet 32x32,[0],[0]
"Following (van den Oord et al., 2016b), we also trained a class-conditional PixelIQN variant, providing to the model the one-hot class label corresponding to a training image (in addition to a τ sample).",4.2. ImageNet 32x32,[0],[0]
"Samples from a class-conditional model can be expected to have higher visual quality, as the class label provides log2(1000)",4.2. ImageNet 32x32,[0],[0]
"≈ 10 bits of information, see Figure 7.",4.2. ImageNet 32x32,[0],[0]
"As seen in Figure 4 and Table 1, class conditioning also further improves Inception score and FID.",4.2. ImageNet 32x32,[0],[0]
"To generate each sample for the computation of these scores, we sample one of 1000 class labels randomly, then generate an image conditioned on this label via the trained model.
",4.2. ImageNet 32x32,[0],[0]
"Finally, motivated by the very long training time for the large PixelCNN model (approximately 1 day per 100K training steps, on 16 NVIDIA Tesla P100 GPUs), we also trained smaller 15-layer versions of the models (same as the ones used on CIFAR-10) on the small ImageNet dataset.",4.2. ImageNet 32x32,[0],[0]
"For comparison, these take approximately 12 hours for 100K training steps on a single P100 GPU, or less than 3 hours on 8 P100 GPUs.",4.2. ImageNet 32x32,[0],[0]
"As expected, little PixelCNN, while suitable
for the CIFAR-10 dataset, fails to achieve competitive scores on the ImageNet dataset, achieving Inception score 5.1 and FID 66.4.",4.2. ImageNet 32x32,[0],[0]
"Astonishingly, little PixelIQN on this dataset reaches Inception score 7.3 and FID 38.5, see Figure 4 (right).",4.2. ImageNet 32x32,[0],[0]
"It thereby not only outperforms the little PixelCNN, but also the larger 20-layer version!",4.2. ImageNet 32x32,[0],[0]
"This strongly supports the hypothesis that PixelCNN, and potentially many other models, are constrained not only by their model capacity, but crucially also by the sub-optimal trade-offs made by their log-likelihood training criterion, failing to align with perceptual or evaluation metrics.",4.2. ImageNet 32x32,[0],[0]
Most existing generative models for images belong to one of two classes.,5. Discussion and Conclusions,[0],[0]
"The first are likelihood-based models, trained with an elementwise KL reconstruction loss, which,
while perceptually meaningless, provides robust optimization properties and high sample diversity.",5. Discussion and Conclusions,[0],[0]
"The second are GANs, trained based on a discriminator loss, typically better aligned with a perceptual metric and enabling the generator to produce realistic, globally consistent samples.",5. Discussion and Conclusions,[0],[0]
"Their advantages come at the cost of a harder optimization problem, high parameter sensitivity, and most importantly, a tendency to collapse modes of the data distribution.
",5. Discussion and Conclusions,[0],[0]
"AIQNs are a new, fundamentally different, technique for generative modeling.",5. Discussion and Conclusions,[0],[0]
"By using a quantile regression loss instead of KL divergence, they combine some of the best properties of the two model classes.",5. Discussion and Conclusions,[0],[0]
"By their nature, they preserve modes of the learned distribution, while producing perceptually appealing high-quality samples.",5. Discussion and Conclusions,[0],[0]
The inevitable approximation trade-offs a generative model makes when constrained by capacity or insufficient training can vary significantly depending on the loss used.,5. Discussion and Conclusions,[0],[0]
"We argue that the proposed quantile regression loss aligns more effectively with a given metric and therefore makes subjectively more advantageous trade-offs.
",5. Discussion and Conclusions,[0],[0]
Devising methods for quantile regression over multidimensional outputs is an active area of research.,5. Discussion and Conclusions,[0],[0]
"New methods are continuing to be investigated (Carlier et al., 2016; Hallin & Miroslav, 2016), and a promising direction for future work is to find ways to use these to replace autoregressive models.",5. Discussion and Conclusions,[0],[0]
"One approach to reducing the computational burden of such models is to apply AIQN to the latent dimensions
of a VAE.",5. Discussion and Conclusions,[0],[0]
"Similar in spirit to Rosca et al. (2017), this would use the VAE to reduce the dimensionality of the problem and the AIQN to sample from the true latent distribution.",5. Discussion and Conclusions,[0],[0]
"In the Appendix we give preliminary results using such an technique, on CelebA 64× 64 (Liu et al., 2015).",5. Discussion and Conclusions,[0],[0]
"We have shown that IQN, computationally cheap and technically simple, can be readily applied to existing architectures, PixelCNN and VAE (Appendix), improving robustness and sampling quality of the underlying model.",5. Discussion and Conclusions,[0],[0]
"We demonstrated that PixelIQN produces more realistic, globally coherent samples, and improves Inception score and FID.
",5. Discussion and Conclusions,[0],[0]
We further point out that many recent advances in generative models could be easily combined with our proposed method.,5. Discussion and Conclusions,[0],[0]
"Recent algorithmic improvements to GANs such as mini-batch discrimination and progressive growing (Salimans et al., 2016; Karras et al., 2017), while not strictly necessary in our work, could be applied to further improve performance.",5. Discussion and Conclusions,[0],[0]
"PixelCNN++ (Salimans et al., 2017) is an architectural improvement of PixelCNN, with several beneficial modifications supported by experimental evidence.",5. Discussion and Conclusions,[0],[0]
"Although we have built upon the original Gated PixelCNN in this work, we believe all of these modifications to be compatible with our work, except for the use of a mixture of logistics in place of PixelCNN’s softmax.",5. Discussion and Conclusions,[0],[0]
"As we have entirely replaced this model component, this change does not map onto our model.",5. Discussion and Conclusions,[0],[0]
"Of note, the motivation behind this change closely mirrors our own, in looking for a loss that respects the underlying metric between examples.",5. Discussion and Conclusions,[0],[0]
"The recent PixelSNAIL model (Chen et al., 2017) achieves stateof-the-art modeling performance by enhancing PixelCNN with ELU nonlinearities, modified block structure, and an attention mechanism.",5. Discussion and Conclusions,[0],[0]
"Again, all of these are fully compatible with our work and should improve results further.
",5. Discussion and Conclusions,[0],[0]
"Finally, the implicit quantile formulation lifts a number of architectural restrictions of previous generative models.",5. Discussion and Conclusions,[0],[0]
"Most importantly, the reparameterization as an inverse c.d.f. allows to learn distributions over continuous ranges without pre-specified boundaries or quantization.",5. Discussion and Conclusions,[0],[0]
"This enables modeling continuous-valued variables, for example for generation of sound (van den Oord et al., 2016a), opening multiple interesting avenues for further investigation.",5. Discussion and Conclusions,[0],[0]
We would like to acknowledge the important role many of our colleagues at DeepMind played for this work.,Acknowledgements,[0],[0]
"We especially thank Aäron van den Oord and Sander Dieleman for invaluable advice on the PixelCNN model; Ivo Danihelka and Danilo J. Rezende for careful reading and insightful comments on an earlier version of the paper; Igor Babuschkin, Alexandre Galashov, Dominik Grewe, Jacob Menick, and Mihaela Rosca for technical help.",Acknowledgements,[0],[0]
"We introduce autoregressive implicit quantile networks (AIQN), a fundamentally different approach to generative modeling than those commonly used, that implicitly captures the distribution using quantile regression.",abstractText,[0],[0]
"AIQN is able to achieve superior perceptual quality and improvements in evaluation metrics, without incurring a loss of sample diversity.",abstractText,[0],[0]
The method can be applied to many existing models and architectures.,abstractText,[0],[0]
"In this work we extend the PixelCNN model with AIQN and demonstrate results on CIFAR-10 and ImageNet using Inception score, FID, non-cherrypicked samples, and inpainting results.",abstractText,[0],[0]
We consistently observe that AIQN yields a highly stable algorithm that improves perceptual quality while maintaining a highly diverse distribution.,abstractText,[0],[0]
Autoregressive Quantile Networks for Generative Modeling,title,[0],[0]
"We study the problem of attributing the prediction of a deep network to its input features.
",1. Motivation and Summary of Results,[0],[0]
Definition 1.,1. Motivation and Summary of Results,[0],[0]
"Formally, suppose we have a function F :",1. Motivation and Summary of Results,[0],[0]
"Rn → [0, 1] that represents a deep network, and an input x = (x1, . . .",1. Motivation and Summary of Results,[0],[0]
", xn) ∈ Rn.",1. Motivation and Summary of Results,[0],[0]
"An attribution of the prediction at input x relative to a baseline input x′ is a vector AF (x, x
′) =",1. Motivation and Summary of Results,[0],[0]
"(a1, . . .",1. Motivation and Summary of Results,[0],[0]
", an) ∈ Rn where ai is the contribution of xi to the prediction F (x).
",1. Motivation and Summary of Results,[0],[0]
"For instance, in an object recognition network, an attribution method could tell us which pixels of the image were responsible for a certain label being picked (see Figure 2).",1. Motivation and Summary of Results,[0],[0]
"The attribution problem was previously studied by various papers (Baehrens et al., 2010; Simonyan et al., 2013;
",1. Motivation and Summary of Results,[0],[0]
"*Equal contribution 1Google Inc., Mountain View, USA.",1. Motivation and Summary of Results,[0],[0]
Correspondence to: Mukund Sundararajan <mukunds@google.com,1. Motivation and Summary of Results,[0],[0]
">, Ankur Taly <ataly@google.com>.
",1. Motivation and Summary of Results,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Motivation and Summary of Results,[0],[0]
"Copyright 2017 by the author(s).
",1. Motivation and Summary of Results,[0],[0]
"Shrikumar et al., 2016; Binder et al., 2016; Springenberg et al., 2014).
",1. Motivation and Summary of Results,[0],[0]
"The intention of these works is to understand the inputoutput behavior of the deep network, which gives us the ability to improve it.",1. Motivation and Summary of Results,[0],[0]
"Such understandability is critical to all computer programs, including machine learning models.",1. Motivation and Summary of Results,[0],[0]
There are also other applications of attribution.,1. Motivation and Summary of Results,[0],[0]
They could be used within a product driven by machine learning to provide a rationale for the recommendation.,1. Motivation and Summary of Results,[0],[0]
"For instance, a deep network that predicts a condition based on imaging could help inform the doctor of the part of the image that resulted in the recommendation.",1. Motivation and Summary of Results,[0],[0]
This could help the doctor understand the strengths and weaknesses of a model and compensate for it.,1. Motivation and Summary of Results,[0],[0]
We give such an example in Section 6.2.,1. Motivation and Summary of Results,[0],[0]
Attributions could also be used by developers in an exploratory sense.,1. Motivation and Summary of Results,[0],[0]
"For instance, we could use a deep network to extract insights that could be then used in a rulebased system.",1. Motivation and Summary of Results,[0],[0]
"In Section 6.3, we give such an example.
",1. Motivation and Summary of Results,[0],[0]
A significant challenge in designing an attribution technique is that they are hard to evaluate empirically.,1. Motivation and Summary of Results,[0],[0]
"As we discuss in Section 4, it is hard to tease apart errors that stem from the misbehavior of the model versus the misbehavior of the attribution method.",1. Motivation and Summary of Results,[0],[0]
"To compensate for this shortcoming, we take an axiomatic approach.",1. Motivation and Summary of Results,[0],[0]
In Section 2 we identify two axioms that every attribution method must satisfy.,1. Motivation and Summary of Results,[0],[0]
Unfortunately most previous methods do not satisfy one of these two axioms.,1. Motivation and Summary of Results,[0],[0]
"In Section 3, we use the axioms to identify a new method, called integrated gradients.
",1. Motivation and Summary of Results,[0],[0]
"Unlike previously proposed methods, integrated gradients do not need any instrumentation of the network, and can be computed easily using a few calls to the gradient operation, allowing even novice practitioners to easily apply the technique.
",1. Motivation and Summary of Results,[0],[0]
"In Section 6, we demonstrate the ease of applicability over several deep networks, including two images networks, two text processing networks, and a chemistry network.",1. Motivation and Summary of Results,[0],[0]
"These applications demonstrate the use of our technique in either improving our understanding of the network, performing debugging, performing rule extraction, or aiding an end user in understanding the network’s prediction.
",1. Motivation and Summary of Results,[0],[0]
Remark 1.,1. Motivation and Summary of Results,[0],[0]
Let us briefly examine the need for the baseline in the definition of the attribution problem.,1. Motivation and Summary of Results,[0],[0]
"A common way for humans to perform attribution relies on counter-
ar X
iv :1
70 3.
01 36
5v 2
[ cs
.L",1. Motivation and Summary of Results,[0],[0]
"G
] 1
3 Ju
n 20
17
factual intuition.",1. Motivation and Summary of Results,[0],[0]
When we assign blame to a certain cause we implicitly consider the absence of the cause as a baseline for comparing outcomes.,1. Motivation and Summary of Results,[0],[0]
"In a deep network, we model the absence using a single baseline input.",1. Motivation and Summary of Results,[0],[0]
"For most deep networks, a natural baseline exists in the input space where the prediction is neutral.",1. Motivation and Summary of Results,[0],[0]
"For instance, in object recognition networks, it is the black image.",1. Motivation and Summary of Results,[0],[0]
"The need for a baseline has also been pointed out by prior work on attribution (Shrikumar et al., 2016; Binder et al., 2016).",1. Motivation and Summary of Results,[0],[0]
We now discuss two axioms (desirable characteristics) for attribution methods.,2. Two Fundamental Axioms,[0],[0]
We find that other feature attribution methods in literature break at least one of the two axioms.,2. Two Fundamental Axioms,[0],[0]
"These methods include DeepLift (Shrikumar et al., 2016; 2017), Layer-wise relevance propagation (LRP) (Binder et al., 2016), Deconvolutional networks (Zeiler & Fergus, 2014), and Guided back-propagation (Springenberg et al., 2014).",2. Two Fundamental Axioms,[0],[0]
"As we will see in Section 3, these axioms will also guide the design of our method.
Gradients.",2. Two Fundamental Axioms,[0],[0]
"For linear models, ML practitioners regularly inspect the products of the model coefficients and the feature values in order to debug predictions.",2. Two Fundamental Axioms,[0],[0]
"Gradients (of the output with respect to the input) is a natural analog of the model coefficients for a deep network, and therefore the product of the gradient and feature values is a reasonable starting point for an attribution method (Baehrens et al., 2010; Simonyan et al., 2013); see the third column of Figure 2 for examples.",2. Two Fundamental Axioms,[0],[0]
"The problem with gradients is that they break sensitivity, a property that all attribution methods should satisfy.",2. Two Fundamental Axioms,[0],[0]
An attribution method satisfies Sensitivity(a),2.1. Axiom: Sensitivity(a),[0],[0]
if for every input and baseline that differ in one feature but have different predictions then the differing feature should be given a non-zero attribution.,2.1. Axiom: Sensitivity(a),[0],[0]
"(Later in the paper, we will have a part (b) to this definition.)
",2.1. Axiom: Sensitivity(a),[0],[0]
Gradients violate Sensitivity(a):,2.1. Axiom: Sensitivity(a),[0],[0]
"For a concrete example, consider a one variable, one ReLU network, f(x) = 1 − ReLU(1−x).",2.1. Axiom: Sensitivity(a),[0],[0]
Suppose the baseline is x = 0 and the input is x = 2.,2.1. Axiom: Sensitivity(a),[0],[0]
"The function changes from 0 to 1, but because f becomes flat at x = 1, the gradient method gives attribution of 0 to x. Intuitively, gradients break Sensitivity because the prediction function may flatten at the input and thus have zero gradient despite the function value at the input being different from that at the baseline.",2.1. Axiom: Sensitivity(a),[0],[0]
"This phenomenon has been reported in previous work (Shrikumar et al., 2016).
",2.1. Axiom: Sensitivity(a),[0],[0]
"Practically, the lack of sensitivity causes gradients to focus on irrelevant features (see the “fireboat” example in Fig-
ure 2).
",2.1. Axiom: Sensitivity(a),[0],[0]
Other back-propagation based approaches.,2.1. Axiom: Sensitivity(a),[0],[0]
A second set of approaches involve back-propagating the final prediction score through each layer of the network down to the individual features.,2.1. Axiom: Sensitivity(a),[0],[0]
"These include DeepLift, Layer-wise relevance propagation (LRP), Deconvolutional networks (DeConvNets), and Guided back-propagation.",2.1. Axiom: Sensitivity(a),[0],[0]
"These methods differ in the specific backpropagation logic for various activation functions (e.g., ReLU, MaxPool, etc.).
",2.1. Axiom: Sensitivity(a),[0],[0]
"Unfortunately, Deconvolution networks (DeConvNets), and Guided back-propagation violate Sensitivity(a).",2.1. Axiom: Sensitivity(a),[0],[0]
This is because these methods back-propogate through a ReLU node only if the ReLU is turned on at the input.,2.1. Axiom: Sensitivity(a),[0],[0]
"This makes the method similar to gradients, in that, the attribution is zero for features with zero gradient at the input despite a non-zero gradient at the baseline.",2.1. Axiom: Sensitivity(a),[0],[0]
"We defer the specific counterexamples to Appendix B.
Methods like DeepLift and LRP tackle the Sensitivity issue by employing a baseline, and in some sense try to compute “discrete gradients” instead of (instantaeneous) gradients at the input.",2.1. Axiom: Sensitivity(a),[0],[0]
(The two methods differ in the specifics of how they compute the discrete gradient).,2.1. Axiom: Sensitivity(a),[0],[0]
"But the idea is that a large, discrete step will avoid flat regions, avoiding a breakage of sensitivity.",2.1. Axiom: Sensitivity(a),[0],[0]
"Unfortunately, these methods violate a different requirement on attribution methods.",2.1. Axiom: Sensitivity(a),[0],[0]
"Two networks are functionally equivalent if their outputs are equal for all inputs, despite having very different implementations.",2.2. Axiom: Implementation Invariance,[0],[0]
"Attribution methods should satisfy Implementation Invariance, i.e., the attributions are always identical for two functionally equivalent networks.",2.2. Axiom: Implementation Invariance,[0],[0]
"To motivate this, notice that attribution can be colloquially defined as assigning the blame (or credit) for the output to the input features.",2.2. Axiom: Implementation Invariance,[0],[0]
"Such a definition does not refer to implementation details.
",2.2. Axiom: Implementation Invariance,[0],[0]
"We now discuss intuition for why DeepLift and LRP break Implementation Invariance; a concrete example is provided in Appendix B.
First, notice that gradients are invariant to implementation.",2.2. Axiom: Implementation Invariance,[0],[0]
"In fact, the chain-rule for gradients ∂f∂g = ∂f ∂h · ∂h ∂g is essentially about implementation invariance.",2.2. Axiom: Implementation Invariance,[0],[0]
"To see this, think of g and f as the input and output of a system, and h being some implementation detail of the system.",2.2. Axiom: Implementation Invariance,[0],[0]
"The gradient of output f to input g can be computed either directly by ∂f∂g , ignoring the intermediate function h (implementation detail), or by invoking the chain rule via h.",2.2. Axiom: Implementation Invariance,[0],[0]
"This is exactly how backpropagation works.
",2.2. Axiom: Implementation Invariance,[0],[0]
Methods like LRP and DeepLift replace gradients with discrete gradients and still use a modified form of backpropagation to compose discrete gradients into attributions.,2.2. Axiom: Implementation Invariance,[0],[0]
"Un-
fortunately, the chain rule does not hold for discrete gradients in general.",2.2. Axiom: Implementation Invariance,[0],[0]
Formally f(x1)−f(x0)g(x1)−g(x0) 6=,2.2. Axiom: Implementation Invariance,[0],[0]
"f(x1)−f(x0) h(x1)−h(x0) · h(x1)−h(x0) g(x1)−g(x0) , and therefore these methods fail to satisfy implementation invariance.
",2.2. Axiom: Implementation Invariance,[0],[0]
"If an attribution method fails to satisfy Implementation Invariance, the attributions are potentially sensitive to unimportant aspects of the models.",2.2. Axiom: Implementation Invariance,[0],[0]
"For instance, if the network architecture has more degrees of freedom than needed to represent a function then there may be two sets of values for the network parameters that lead to the same function.",2.2. Axiom: Implementation Invariance,[0],[0]
"The training procedure can converge at either set of values depending on the initializtion or for other reasons, but the underlying network function would remain the same.",2.2. Axiom: Implementation Invariance,[0],[0]
It is undesirable that attributions differ for such reasons.,2.2. Axiom: Implementation Invariance,[0],[0]
We are now ready to describe our technique.,3. Our Method: Integrated Gradients,[0],[0]
"Intuitively, our technique combines the Implementation Invariance of Gradients along with the Sensitivity of techniques like LRP or DeepLift.
",3. Our Method: Integrated Gradients,[0],[0]
"Formally, suppose we have a function F :",3. Our Method: Integrated Gradients,[0],[0]
"Rn → [0, 1] that represents a deep network.",3. Our Method: Integrated Gradients,[0],[0]
"Specifically, let x ∈",3. Our Method: Integrated Gradients,[0],[0]
"Rn be the input at hand, and x′ ∈ Rn be the baseline input.",3. Our Method: Integrated Gradients,[0],[0]
"For image networks, the baseline could be the black image, while for text models it could be the zero embedding vector.
",3. Our Method: Integrated Gradients,[0],[0]
"We consider the straightline path (in Rn) from the baseline x′ to the input x, and compute the gradients at all points along the path.",3. Our Method: Integrated Gradients,[0],[0]
Integrated gradients are obtained by cumulating these gradients.,3. Our Method: Integrated Gradients,[0],[0]
"Specifically, integrated gradients are defined as the path intergral of the gradients along the straightline path from the baseline x′ to the input x.
The integrated gradient along the ith dimension for an input x and baseline x′ is defined as follows.",3. Our Method: Integrated Gradients,[0],[0]
"Here, ∂F (x)∂xi is the gradient of F (x) along the ith dimension.
",3. Our Method: Integrated Gradients,[0],[0]
IntegratedGradsi(x) ::= (xi−x ′,3. Our Method: Integrated Gradients,[0],[0]
i)× ∫ 1 α=0 ∂F (x′+α×(x−x′)),3. Our Method: Integrated Gradients,[0],[0]
"∂xi dα
(1)",3. Our Method: Integrated Gradients,[0],[0]
Axiom: Completeness.,3. Our Method: Integrated Gradients,[0],[0]
"Integrated gradients satisfy an
axiom called completeness that the attributions add up to the difference between the output of F at the input x and the baseline x′.",3. Our Method: Integrated Gradients,[0],[0]
This axiom is identified as being desirable by Deeplift and LRP.,3. Our Method: Integrated Gradients,[0],[0]
"It is a sanity check that the attribution method is somewhat comprehensive in its accounting, a property that is clearly desirable if the networks score is used in a numeric sense, and not just to pick the top label, for e.g., a model estimating insurance premiums from credit features of individuals.
",3. Our Method: Integrated Gradients,[0],[0]
"This is formalized by the proposition below, which instantiates the fundamental theorem of calculus for path integrals.
",3. Our Method: Integrated Gradients,[0],[0]
Proposition 1.,3. Our Method: Integrated Gradients,[0],[0]
"If F : Rn → R is differentiable almost everywhere 1 then
Σni=1IntegratedGradsi(x)",3. Our Method: Integrated Gradients,[0],[0]
"= F (x)− F (x′)
",3. Our Method: Integrated Gradients,[0],[0]
"For most deep networks, it is possible to choose a baseline such that the prediction at the baseline is near zero (F (x′)",3. Our Method: Integrated Gradients,[0],[0]
≈ 0).,3. Our Method: Integrated Gradients,[0],[0]
"(For image models, the black image baseline indeed satisfies this property.)",3. Our Method: Integrated Gradients,[0],[0]
"In such cases, there is an intepretation of the resulting attributions that ignores the baseline and amounts to distributing the output to the individual input features.
",3. Our Method: Integrated Gradients,[0],[0]
Remark 2.,3. Our Method: Integrated Gradients,[0],[0]
Integrated gradients satisfies Sensivity(a) because Completeness implies Sensivity(a) and is thus a strengthening of the Sensitivity(a) axiom.,3. Our Method: Integrated Gradients,[0],[0]
"This is because Sensitivity(a) refers to a case where the baseline and the input differ only in one variable, for which Completeness asserts that the difference in the two output values is equal to the attribution to this variable.",3. Our Method: Integrated Gradients,[0],[0]
Attributions generated by integrated gradients satisfy Implementation Invariance since they are based only on the gradients of the function represented by the network.,3. Our Method: Integrated Gradients,[0],[0]
Prior literature has relied on empirically evaluating the attribution technique.,4. Uniqueness of Integrated Gradients,[0],[0]
"For instance, in the context of an object recognition task, (Samek et al., 2015) suggests that we select the top k pixels by attribution and randomly vary their intensities and then measure the drop in score.",4. Uniqueness of Integrated Gradients,[0],[0]
"If the attribution method is good, then the drop in score should be large.",4. Uniqueness of Integrated Gradients,[0],[0]
"However, the images resulting from pixel perturbation could be unnatural, and it could be that the scores drop simply because the network has never seen anything like it in training.",4. Uniqueness of Integrated Gradients,[0],[0]
"(This is less of a concern with linear or logistic models where the simplicity of the model ensures that ablating a feature does not cause strange interactions.)
",4. Uniqueness of Integrated Gradients,[0],[0]
"A different evaluation technique considers images with human-drawn bounding boxes around objects, and computes the percentage of pixel attribution inside the box.",4. Uniqueness of Integrated Gradients,[0],[0]
"While for most objects, one would expect the pixels located on the object to be most important for the prediction, in some cases the context in which the object occurs may also contribute to the prediction.",4. Uniqueness of Integrated Gradients,[0],[0]
"The cabbage butterfly image from Figure 2 is a good example of this where the pixels on the leaf are also surfaced by the integrated gradients.
",4. Uniqueness of Integrated Gradients,[0],[0]
"Roughly, we found that every empirical evaluation technique we could think of could not differentiate between ar-
1Formally, this means the function F is continuous everywhere and the partial derivative of F along each input dimension satisfies Lebesgue’s integrability condition, i.e., the set of discontinuous points has measure zero.",4. Uniqueness of Integrated Gradients,[0],[0]
"Deep networks built out of Sigmoids, ReLUs, and pooling operators satisfy this condition.
tifacts that stem from perturbing the data, a misbehaving model, and a misbehaving attribution method.",4. Uniqueness of Integrated Gradients,[0],[0]
This was why we turned to an axiomatic approach in designing a good attribution method (Section 2).,4. Uniqueness of Integrated Gradients,[0],[0]
"While our method satisfies Sensitivity and Implementation Invariance, it certainly isn’t the unique method to do so.
",4. Uniqueness of Integrated Gradients,[0],[0]
We now justify the selection of the integrated gradients method in two steps.,4. Uniqueness of Integrated Gradients,[0],[0]
"First, we identify a class of methods called Path methods that generalize integrated gradients.",4. Uniqueness of Integrated Gradients,[0],[0]
We discuss that path methods are the only methods to satisfy certain desirable axioms.,4. Uniqueness of Integrated Gradients,[0],[0]
"Second, we argue why integrated gradients is somehow canonical among the different path methods.",4. Uniqueness of Integrated Gradients,[0],[0]
Integrated gradients aggregate the gradients along the inputs that fall on the straightline between the baseline and the input.,4.1. Path Methods,[0],[0]
"There are many other (non-straightline) paths that monotonically interpolate between the two points, and each such path will yield a different attribution method.",4.1. Path Methods,[0],[0]
"For instance, consider the simple case when the input is two dimensional.",4.1. Path Methods,[0],[0]
"Figure 1 has examples of three paths, each of which corresponds to a different attribution method.
",4.1. Path Methods,[0],[0]
"Formally, let γ = (γ1, . . .",4.1. Path Methods,[0],[0]
", γn) :",4.1. Path Methods,[0],[0]
"[0, 1] → Rn be a smooth function specifying a path in Rn from the baseline x′ to the input x, i.e., γ(0)",4.1. Path Methods,[0],[0]
"= x′ and γ(1) = x.
Given a path function γ, path integrated gradients are obtained by integrating the gradients along the path γ(α) for α ∈",4.1. Path Methods,[0],[0]
"[0, 1].",4.1. Path Methods,[0],[0]
"Formally, path integrated gradients along the ith dimension for an input x is defined as follows.
",4.1. Path Methods,[0],[0]
PathIntegratedGradsγi (x) ::= ∫ 1 α=0 ∂F (γ(α)) ∂γi(α) ∂γi(α) ∂α,4.1. Path Methods,[0],[0]
"dα
(2) where ∂F (x)∂xi is the gradient of F along the i
th dimension at x.
Attribution methods based on path integrated gradients are
collectively known as path methods.",4.1. Path Methods,[0],[0]
Notice that integrated gradients is a path method for the straightline path specified γ(α) = x′ + α× (x− x′) for α ∈,4.1. Path Methods,[0],[0]
"[0, 1].",4.1. Path Methods,[0],[0]
Remark 3.,4.1. Path Methods,[0],[0]
All path methods satisfy Implementation Invariance.,4.1. Path Methods,[0],[0]
"This follows from the fact that they are defined using the underlying gradients, which do not depend on the implementation.",4.1. Path Methods,[0],[0]
"They also satisfy Completeness (the proof is similar to that of Proposition 1) and Sensitvity(a) which is implied by Completeness (see Remark 2).
",4.1. Path Methods,[0],[0]
"More interestingly, path methods are the only methods that satisfy certain desirable axioms.",4.1. Path Methods,[0],[0]
"(For formal definitions of the axioms and proof of Proposition 2, see Friedman (Friedman, 2004).)
",4.1. Path Methods,[0],[0]
Axiom: Sensitivity(b).,4.1. Path Methods,[0],[0]
"(called Dummy in (Friedman, 2004))",4.1. Path Methods,[0],[0]
"If the function implemented by the deep network does not depend (mathematically) on some variable, then the attribution to that variable is always zero.
",4.1. Path Methods,[0],[0]
This is a natural complement to the definition of Sensitivity(a) from Section 2.,4.1. Path Methods,[0],[0]
"This definition captures desired insensitivity of the attributions.
",4.1. Path Methods,[0],[0]
Axiom: Linearity.,4.1. Path Methods,[0],[0]
"Suppose that we linearly composed two deep networks modeled by the functions f1 and f2 to form a third network that models the function a×f1+b×f2, i.e., a linear combination of the two networks.",4.1. Path Methods,[0],[0]
Then we’d like the attributions for a× f1 + b× f2 to be the weighted sum of the attributions for f1 and f2 with weights a and b respectively.,4.1. Path Methods,[0],[0]
"Intuitively, we would like the attributions to preserve any linearity within the network.",4.1. Path Methods,[0],[0]
Proposition 2.,4.1. Path Methods,[0],[0]
"(Theorem 1 (Friedman, 2004))",4.1. Path Methods,[0],[0]
"Path methods are the only attribution methods that always satisfy Implementation Invariance, Sensitivity(b), Linearity, and Completeness.",4.1. Path Methods,[0],[0]
Remark 4.,4.1. Path Methods,[0],[0]
"We note that these path integrated gradients have been used within the cost-sharing literature in economics where the function models the cost of a project as a function of the demands of various participants, and the attributions correspond to cost-shares.",4.1. Path Methods,[0],[0]
"Integrated gradients correspond to a cost-sharing method called AumannShapley (Aumann & Shapley, 1974).",4.1. Path Methods,[0],[0]
Proposition 2 holds for our attribution problem because mathematically the cost-sharing problem corresponds to the attribution problem with the benchmark fixed at the zero vector.,4.1. Path Methods,[0],[0]
(Implementation Invariance is implicit in the cost-sharing literature as the cost functions are considered directly in their mathematical form.),4.1. Path Methods,[0],[0]
"In this section, we formalize why the straightline path chosen by integrated gradients is canonical.",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"First, observe that it is the simplest path that one can define mathematically.
",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"Second, a natural property for attribution methods is to preserve symmetry, in the following sense.
",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
Symmetry-Preserving.,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
Two input variables are symmetric w.r.t.,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
a function if swapping them does not change the function.,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"For instance, x and y are symmetric w.r.t.",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
F,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"if and only if F (x, y) = F (y, x) for all values of x and y.",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"An attribution method is symmetry preserving, if for all inputs that have identical values for symmetric variables and baselines that have identical values for symmetric variables, the symmetric variables receive identical attributions.
",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"E.g., consider the logistic model Sigmoid(x1 + x2 + . . . ).",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
x1 and x2 are symmetric variables for this model.,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"For an input where x1 = x2 = 1 (say) and baseline where x1 = x2 = 0 (say), a symmetry preserving method must offer identical attributions to x1 and x2.
",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"It seems natural to ask for symmetry-preserving attribution methods because if two variables play the exact same role in the network (i.e., they are symmetric and have the same values in the baseline and the input) then they ought to receive the same attrbiution.
",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
Theorem 1.,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"Integrated gradients is the unique path method that is symmetry-preserving.
",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"The proof is provided in Appendix A.
Remark 5.",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"If we allow averaging over the attributions from multiple paths, then are other methods that satisfy all the axioms in Theorem 1.",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"In particular, there is the method by Shapley-Shubik (Shapley & Shubik, 1971) from the cost sharing literature, and used by (Lundberg & Lee, 2016; Datta et al., 2016) to compute feature attributions (though they were not studying deep networks).",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"In this method, the attribution is the average of those from n!",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
extremal paths; here n is the number of features.,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"Here each such path considers an ordering of the input features, and sequentially changes the input feature from its value at the baseline to its value at the input.",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
This method yields attributions that are different from integrated gradients.,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"If the function of interest is min(x1, x2), the baseline is x1 = x2 = 0, and the input is x1 = 1, x2 = 3, then integrated gradients attributes the change in the function value entirely to the critical variable x1, whereas Shapley-Shubik assigns attributions of 1/2 each; it seems somewhat subjective to prefer one result over the other.
",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"We also envision other issues with applying Shapley-Shubik to deep networks: It is computationally expensive; in an object recognition network that takes an 100X100 image as input, n is 10000, and n!",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
is a gigantic number.,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"Even if one samples few paths randomly, evaluating the attributions for a single path takes n calls to the deep network.",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"In contrast, integrated gradients is able to operate with 20 to 300 calls.",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"Further, the Shapley-Shubik computation visit
inputs that are combinations of the input and the baseline.",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
It is possible that some of these combinations are very different from anything seen during training.,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
We speculate that this could lead to attribution artifacts.,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
Selecting a Benchmark.,5. Applying Integrated Gradients,[0],[0]
A key step in applying integrated gradients is to select a good baseline.,5. Applying Integrated Gradients,[0],[0]
"We recommend that developers check that the baseline has a near-zero score— as discussed in Section 3, this allows us to interpret the attributions as a function of the input.",5. Applying Integrated Gradients,[0],[0]
But there is more to a good baseline:,5. Applying Integrated Gradients,[0],[0]
"For instance, for an object recogntion network it is possible to create an adversarial example that has a zero score for a given input label (say elephant), by applying a tiny, carefully-designed perturbation to an image with a very different label (say microscope) (cf.",5. Applying Integrated Gradients,[0],[0]
"(Goodfellow et al., 2015)).",5. Applying Integrated Gradients,[0],[0]
The attributions can then include undesirable artifacts of this adversarially constructed baseline.,5. Applying Integrated Gradients,[0],[0]
"So we would additionally like the baseline to convey a complete absence of signal, so that the features that are apparent from the attributions are properties only of the input, and not of the baseline.",5. Applying Integrated Gradients,[0],[0]
"For instance, in an object recognition network, a black image signifies the absence of objects.",5. Applying Integrated Gradients,[0],[0]
The black image isn’t unique in this sense—an image consisting of noise has the same property.,5. Applying Integrated Gradients,[0],[0]
"However, using black as a baseline may result in cleaner visualizations of “edge” features.",5. Applying Integrated Gradients,[0],[0]
"For text based networks, we have found that the allzero input embedding vector is a good baseline.",5. Applying Integrated Gradients,[0],[0]
"The action of training causes unimportant words tend to have small norms, and so, in the limit, unimportance corresponds to the all-zero baseline.",5. Applying Integrated Gradients,[0],[0]
"Notice that the black image corresponds to a valid input to an object recognition network, and is also intuitively what we humans would consider absence of signal.",5. Applying Integrated Gradients,[0],[0]
"In contrast, the all-zero input vector for a text network does not correspond to a valid input; it nevertheless works for the mathematical reason described above.
",5. Applying Integrated Gradients,[0],[0]
Computing Integrated Gradients.,5. Applying Integrated Gradients,[0],[0]
The integral of integrated gradients can be efficiently approximated via a summation.,5. Applying Integrated Gradients,[0],[0]
"We simply sum the gradients at points occurring at sufficiently small intervals along the straightline path from the baseline x′ to the input x.
IntegratedGradsapproxi (x) ::=
(xi − x′i)× Σmk=1 ∂F (x′+ k",5. Applying Integrated Gradients,[0],[0]
m×(x−x ′))),5. Applying Integrated Gradients,[0],[0]
"∂xi × 1m
(3)
",5. Applying Integrated Gradients,[0],[0]
Here m is the number of steps in the Riemman approximation of the integral.,5. Applying Integrated Gradients,[0],[0]
Notice that the approximation simply involves computing the gradient in a for loop which should be straightforward and efficient in most deep learning frameworks.,5. Applying Integrated Gradients,[0],[0]
"For instance, in TensorFlow, it amounts to calling tf.gradients in a loop over the set of inputs (i.e., x′ + km × (x − x ′) for k = 1, . . .",5. Applying Integrated Gradients,[0],[0]
",m), which
could also be batched.",5. Applying Integrated Gradients,[0],[0]
"In practice, we find that somewhere between 20 and 300 steps are enough to approximate the integral (within 5%); we recommend that developers check that the attributions approximately adds up to the difference beween the score at the input and that at the baseline (cf. Proposition 1), and if not increase the step-size m.",5. Applying Integrated Gradients,[0],[0]
The integrated gradients technique is applicable to a variety of deep networks.,6. Applications,[0],[0]
"Here, we apply it to two image models, two natural language models, and a chemistry model.",6. Applications,[0],[0]
"We study feature attribution in an object recognition network built using the GoogleNet architecture (Szegedy et al., 2014) and trained over the ImageNet object recognition dataset (Russakovsky et al., 2015).",6.1. An Object Recognition Network,[0],[0]
We use the integrated gradients method to study pixel importance in predictions made by this network.,6.1. An Object Recognition Network,[0],[0]
The gradients are computed for the output of the highest-scoring class with respect to pixel of the input image.,6.1. An Object Recognition Network,[0],[0]
"The baseline input is the black image, i.e., all pixel intensities are zero.
",6.1. An Object Recognition Network,[0],[0]
Integrated gradients can be visualized by aggregating them along the color channel and scaling the pixels in the actual image by them.,6.1. An Object Recognition Network,[0],[0]
Figure 2 shows visualizations for a bunch of images2.,6.1. An Object Recognition Network,[0],[0]
"For comparison, it also presents the corresponding visualization obtained from the product of the image with the gradients at the actual image.",6.1. An Object Recognition Network,[0],[0]
Notice that integrated gradients are better at reflecting distinctive features of the input image.,6.1. An Object Recognition Network,[0],[0]
Diabetic retinopathy (DR) is a complication of the diabetes that affects the eyes.,6.2. Diabetic Retinopathy Prediction,[0],[0]
"Recently, a deep network (Gulshan et al., 2016) has been proposed to predict the severity grade for DR in retinal fundus images.",6.2. Diabetic Retinopathy Prediction,[0],[0]
"The model has good predictive accuracy on various validation datasets.
",6.2. Diabetic Retinopathy Prediction,[0],[0]
"We use integrated gradients to study feature importance for this network; like in the object recognition case, the baseline is the black image.",6.2. Diabetic Retinopathy Prediction,[0],[0]
"Feature importance explanations are important for this network as retina specialists may use it to build trust in the network’s predictions, decide the grade for borderline cases, and obtain insights for further testing and screening.
",6.2. Diabetic Retinopathy Prediction,[0],[0]
Figure 3 shows a visualization of integrated gradients for a retinal fundus image.,6.2. Diabetic Retinopathy Prediction,[0],[0]
The visualization method is a bit different from that used in Figure 2.,6.2. Diabetic Retinopathy Prediction,[0],[0]
"We aggregate integrated gradients along the color channel and overlay them on the
2More examples can be found at https://github.com/ ankurtaly/Attributions
actual image in gray scale with positive attribtutions along the green channel and negative attributions along the red channel.",6.2. Diabetic Retinopathy Prediction,[0],[0]
Notice that integrated gradients are localized to a few pixels that seem to be lesions in the retina.,6.2. Diabetic Retinopathy Prediction,[0],[0]
The interior of the lesions receive a negative attribution while the periphery receives a positive attribution indicating that the network focusses on the boundary of the lesion.,6.2. Diabetic Retinopathy Prediction,[0],[0]
Automatically answering natural language questions (over semi-structured data) is an important problem in artificial intelligence (AI).,6.3. Question Classification,[0],[0]
"A common approach is to semantically parse the question to its logical form (Liang, 2016) using a set of human-authored grammar rules.",6.3. Question Classification,[0],[0]
An alternative approach is to machine learn an end-to-end model provided there is enough training data.,6.3. Question Classification,[0],[0]
An interesting question is whether one could peek inside machine learnt models to derive new rules.,6.3. Question Classification,[0],[0]
"We explore this direction for a sub-problem of semantic parsing, called question classification, using the method of integrated gradients.
",6.3. Question Classification,[0],[0]
The goal of question classification is to identify the type of answer it is seeking.,6.3. Question Classification,[0],[0]
"For instance, is the quesiton seeking a yes/no answer, or is it seeking a date?",6.3. Question Classification,[0],[0]
"Rules for solving this problem look for trigger phrases in the question, for e.g., a “when” in the beginning indicates a date seeking question.",6.3. Question Classification,[0],[0]
"We train a model for question classification using the the text categorization architecture proposed by (Kim, 2014) over the WikiTableQuestions dataset (Pasupat & Liang, 2015).",6.3. Question Classification,[0],[0]
We use integrated gradients to attribute predictions down to the question terms in order to identify new trigger phrases for answer type.,6.3. Question Classification,[0],[0]
"The baseline input is the all zero embedding vector.
",6.3. Question Classification,[0],[0]
Figure 4 lists a few questions with constituent terms highlighted based on their attribution.,6.3. Question Classification,[0],[0]
"Notice that the attributions largely agree with commonly used rules, for e.g., “how many” indicates a numeric seeking question.",6.3. Question Classification,[0],[0]
"In addition, attributions help identify novel question classification rules, for e.g., questions containing “total number” are seeking numeric answers.",6.3. Question Classification,[0],[0]
"Attributions also point out undesirable correlations, for e.g., “charles” is used as trigger for a yes/no question.",6.3. Question Classification,[0],[0]
"We applied our technique to a complex, LSTM-based Neural Machine Translation System (Wu et al., 2016).",6.4. Neural Machine Translation,[0],[0]
We attribute the output probability of every output token (in form of wordpieces) to the input tokens.,6.4. Neural Machine Translation,[0],[0]
Such attributions “align” the output sentence with the input sentence.,6.4. Neural Machine Translation,[0],[0]
"For
baseline, we zero out the embeddings of all tokens except the start and end markers.",6.4. Neural Machine Translation,[0],[0]
Figure 5 shows an example of such an attribution-based alignments.,6.4. Neural Machine Translation,[0],[0]
We observed that the results make intuitive sense.,6.4. Neural Machine Translation,[0],[0]
"E.g. “und” is mostly attributed to “and”, and “morgen” is mostly attributed to “morning”.",6.4. Neural Machine Translation,[0],[0]
We use 100 − 1000 steps (cf. Section 5) in the integrated gradient approximation; we need this because the network is highly nonlinear.,6.4. Neural Machine Translation,[0],[0]
"We apply integrated gradients to a network performing Ligand-Based Virtual Screening which is the problem of predicting whether an input molecule is active against a certain target (e.g., protein or enzyme).",6.5. Chemistry Models,[0],[0]
"In particular, we consider a network based on the molecular graph convolution architecture proposed by (Kearnes et al., 2016).
",6.5. Chemistry Models,[0],[0]
The network requires an input molecule to be encoded by hand as a set of atom and atom-pair features describing the molecule as an undirected graph.,6.5. Chemistry Models,[0],[0]
"Atoms are featurized using a one-hot encoding specifying the atom type (e.g., C, O, S, etc.), and atom-pairs are featurized by specifying either the type of bond (e.g., single, double, triple, etc.) between the atoms, or the graph distance between them.",6.5. Chemistry Models,[0],[0]
"The baseline input is obtained zeroing out the feature vectors for atom and atom-pairs.
",6.5. Chemistry Models,[0],[0]
We visualize integrated gradients as heatmaps over the the atom and atom-pair features with the heatmap intensity depicting the strength of the contribution.,6.5. Chemistry Models,[0],[0]
Figure 6 shows the visualization for a specific molecule.,6.5. Chemistry Models,[0],[0]
"Since integrated gradients add up to the final prediction score (see Proposition 1), the magnitudes can be use for accounting the contributions of each feature.",6.5. Chemistry Models,[0],[0]
"For instance, for the molecule in the figure, atom-pairs that have a bond between them cumulatively contribute to 46% of the prediction score, while all other pairs cumulatively contribute to only −3%.
",6.5. Chemistry Models,[0],[0]
Identifying Degenerate Features.,6.5. Chemistry Models,[0],[0]
"We now discuss how attributions helped us spot an anomaly in the W1N2 architecture in (Kearnes et al., 2016).",6.5. Chemistry Models,[0],[0]
"On applying the integrated gradients method to this network, we found that several atoms in the same molecule received identical attribution despite being bonded to different atoms.",6.5. Chemistry Models,[0],[0]
"This is surprising as one would expect two atoms with different neighborhoods to be treated differently by the network.
",6.5. Chemistry Models,[0],[0]
"On investigating the problem further, in the network architecture, the atoms and atom-pair features were not fully convolved.",6.5. Chemistry Models,[0],[0]
"This caused all atoms that have the same atom type, and same number of bonds of each type to contribute identically to the network.",6.5. Chemistry Models,[0],[0]
We already covered closely related work on attribution in Section 2.,7. Other Related work,[0],[0]
We mention other related work.,7. Other Related work,[0],[0]
"Over the last few years, there has been a vast amount work on demystifying the inner workings of deep networks.",7. Other Related work,[0],[0]
"Most of this work has been on networks trained on computer vision tasks, and deals with understanding what a specific neuron computes (Erhan et al., 2009; Le, 2013) and interpreting the representations captured by neurons during a prediction (Mahendran & Vedaldi, 2015; Dosovitskiy & Brox, 2015; Yosinski et al., 2015).",7. Other Related work,[0],[0]
"In contrast, we focus on understanding the network’s behavior on a specific input in terms of the base level input features.",7. Other Related work,[0],[0]
"Our technique quantifies the importance of each feature in the prediction.
",7. Other Related work,[0],[0]
"One approach to the attribution problem proposed first by (Ribeiro et al., 2016a;b), is to locally approximate the behavior of the network in the vicinity of the input being explained with a simpler, more interpretable model.",7. Other Related work,[0],[0]
An appealing aspect of this approach is that it is completely agnostic to the implementation of the network and satisfies implemenation invariance.,7. Other Related work,[0],[0]
"However, this approach does not guarantee sensitivity.",7. Other Related work,[0],[0]
"There is no guarantee that the local region explored escapes the “flat” section of the pre-
diction function in the sense of Section 2.",7. Other Related work,[0],[0]
The other issue is that the method is expensive to implement for networks with “dense” input like image networks as one needs to explore a local region of size proportional to the number of pixels and train a model for this space.,7. Other Related work,[0],[0]
"In contrast, our technique works with a few calls to the gradient operation.
",7. Other Related work,[0],[0]
"Attention mechanisms (Bahdanau et al., 2014) have gained popularity recently.",7. Other Related work,[0],[0]
"One may think that attention could be used a proxy for attributions, but this has issues.",7. Other Related work,[0],[0]
"For instance, in a LSTM that also employs attention, there are many ways for an input token to influence an output token: the memory cell, the recurrent state, and “attention”.",7. Other Related work,[0],[0]
Focussing only an attention ignores the other modes of influence and results in an incomplete picture.,7. Other Related work,[0],[0]
The primary contribution of this paper is a method called integrated gradients that attributes the prediction of a deep network to its inputs.,8. Conclusion,[0],[0]
"It can be implemented using a few calls to the gradients operator, can be applied to a variety of deep networks, and has a strong theoretical justification.
",8. Conclusion,[0],[0]
A secondary contribution of this paper is to clarify desirable features of an attribution method using an axiomatic framework inspired by cost-sharing literature from economics.,8. Conclusion,[0],[0]
"Without the axiomatic approach it is hard to tell whether the attribution method is affected by data artifacts, network’s artifacts or artifacts of the method.",8. Conclusion,[0],[0]
"The axiomatic approach rules out artifacts of the last type.
",8. Conclusion,[0],[0]
"While our and other works have made some progress on understanding the relative importance of input features in a deep network, we have not addressed the interactions between the input features or the logic employed by the network.",8. Conclusion,[0],[0]
So there remain many unanswered questions in terms of debugging the I/O behavior of a deep network.,8. Conclusion,[0],[0]
"We would like to thank Samy Bengio, Kedar Dhamdhere, Scott Lundberg, Amir Najmi, Kevin McCurley, Patrick Riley, Christian Szegedy, Diane Tang for their feedback.",ACKNOWLEDGMENTS,[0],[0]
We would like to thank Daniel Smilkov and Federico Allocati for identifying bugs in our descriptions.,ACKNOWLEDGMENTS,[0],[0]
"We would like to thank our anonymous reviewers for identifying bugs, and their suggestions to improve presentation.",ACKNOWLEDGMENTS,[0],[0]
Proof.,A. Proof of Theorem 1,[0],[0]
Consider a non-straightline path γ :,A. Proof of Theorem 1,[0],[0]
"[0, 1] → Rn from baseline to input.",A. Proof of Theorem 1,[0],[0]
"W.l.o.g., there exists t0 ∈",A. Proof of Theorem 1,[0],[0]
"[0, 1] such that for two dimensions i, j, γi(t0) > γj(t0).",A. Proof of Theorem 1,[0],[0]
"Let (t1, t2) be the maximum real open interval containing t0 such that γi(t) > γj(t) for all t in (t1, t2), and let a = γi(t1) = γj(t1), and b = γi(t2) = γj(t2).",A. Proof of Theorem 1,[0],[0]
Define function f,A. Proof of Theorem 1,[0],[0]
: x ∈,A. Proof of Theorem 1,[0],[0]
"[0, 1]n → R as 0 if min(xi, xj) ≤ a, as (b − a)2 if max(xi, xj) ≥ b, and as (xi − a)(xj − a) otherwise.",A. Proof of Theorem 1,[0],[0]
"Next we compute the attributions of f at x = 〈1, . . .",A. Proof of Theorem 1,[0],[0]
", 1〉n with baseline x′ = 〈0, . . .",A. Proof of Theorem 1,[0],[0]
", 0〉n.",A. Proof of Theorem 1,[0],[0]
"Note that xi and xj are symmetric, and should get identical attributions.",A. Proof of Theorem 1,[0],[0]
For t /∈,A. Proof of Theorem 1,[0],[0]
"[t1, t2], the function is a constant, and the attribution of f is zero to all variables, while for t ∈ (t1, t2), the integrand of attribution of f is γj(t)",A. Proof of Theorem 1,[0],[0]
"− a to xi, and γi(t)",A. Proof of Theorem 1,[0],[0]
"− a to xj , where the latter is always strictly larger by our choice of the interval.",A. Proof of Theorem 1,[0],[0]
"Integrating, it follows that xj gets a larger attribution than xi, contradiction.",A. Proof of Theorem 1,[0],[0]
"We show that the methods DeepLift and Layer-wise relevance propagation (LRP) break the implementation invariance axiom, and the Deconvolution and Guided backpropagation methods break the sensitivity axiom.
",B. Attribution Counter-Examples,[0],[0]
"Figure 7 provides an example of two equivalent networks
f(x1, x2) and g(x1, x2) for which DeepLift and LRP yield different attributions.
",B. Attribution Counter-Examples,[0],[0]
"First, observe that the networks f and g are of the form f(x1, x2) =",B. Attribution Counter-Examples,[0],[0]
"ReLU(h(x1, x2)) and f(x1, x2) = ReLU(k(x1, x2))",B. Attribution Counter-Examples,[0],[0]
"3, where
h(x1, x2) =",B. Attribution Counter-Examples,[0],[0]
"ReLU(x1)− 1− ReLU(x2) k(x1, x2)",B. Attribution Counter-Examples,[0],[0]
"= ReLU(x1 − 1)− ReLU(x2)
",B. Attribution Counter-Examples,[0],[0]
Note that h and k are not equivalent.,B. Attribution Counter-Examples,[0],[0]
They have different values whenever x1 < 1.,B. Attribution Counter-Examples,[0],[0]
But f and g are equivalent.,B. Attribution Counter-Examples,[0],[0]
"To prove this, suppose for contradiction that f and g are different for some x1, x2.",B. Attribution Counter-Examples,[0],[0]
Then it must be the case that ReLU(x1)− 1 6= ReLU(x1 − 1).,B. Attribution Counter-Examples,[0],[0]
"This happens only when x1 < 1, which implies that f(x1, x2) = g(x1, x2) = 0.
",B. Attribution Counter-Examples,[0],[0]
Now we leverage the above example to show that Deconvolution and Guided back-propagation break sensitivity.,B. Attribution Counter-Examples,[0],[0]
"Consider the network f(x1, x2) from Figure 7.",B. Attribution Counter-Examples,[0],[0]
"For a fixed value of x1 greater than 1, the output decreases linearly as x2 increases from 0 to x1 − 1.",B. Attribution Counter-Examples,[0],[0]
"Yet, for all inputs, Deconvolutional networks and Guided back-propagation results in zero attribution for x2.",B. Attribution Counter-Examples,[0],[0]
"This happens because for all inputs the back-propagated signal received at the node ReLU(x2) is negative and is therefore not back-propagated through the ReLU operation (per the rules of deconvolution and guided back-propagation; see (Springenberg et al., 2014) for details).",B. Attribution Counter-Examples,[0],[0]
"As a result, the feature x2 receives zero
3 ReLU(x) is defined as max(x, 0).
",B. Attribution Counter-Examples,[0],[0]
attribution despite the network’s output being sensitive to it.,B. Attribution Counter-Examples,[0],[0]
"We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works.",abstractText,[0],[0]
We identify two fundamental axioms— Sensitivity and Implementation Invariance that attribution methods ought to satisfy.,abstractText,[0],[0]
"We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods.",abstractText,[0],[0]
We use the axioms to guide the design of a new attribution method called Integrated Gradients.,abstractText,[0],[0]
Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator.,abstractText,[0],[0]
"We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.",abstractText,[0],[0]
1.,abstractText,[0],[0]
Motivation and Summary of Results We study the problem of attributing the prediction of a deep network to its input features.,abstractText,[0],[0]
Definition 1.,abstractText,[0],[0]
"Formally, suppose we have a function F :",abstractText,[0],[0]
R,abstractText,[0],[0]
→,abstractText,[0],[0]
"[0, 1] that represents a deep network, and an input x =",abstractText,[0],[0]
"(x1, . . .",abstractText,[0],[0]
", xn) ∈",abstractText,[0],[0]
R.,abstractText,[0],[0]
"An attribution of the prediction at input x relative to a baseline input x′ is a vector AF (x, x ′) =",abstractText,[0],[0]
"(a1, . . .",abstractText,[0],[0]
", an) ∈ R where ai is the contribution of xi to the prediction F (x).",abstractText,[0],[0]
"For instance, in an object recognition network, an attribution method could tell us which pixels of the image were responsible for a certain label being picked (see Figure 2).",abstractText,[0],[0]
"The attribution problem was previously studied by various papers (Baehrens et al., 2010; Simonyan et al., 2013; Equal contribution Google Inc., Mountain View, USA.",abstractText,[0],[0]
Correspondence to: Mukund Sundararajan <mukunds@google.com,abstractText,[0],[0]
">, Ankur Taly <ataly@google.com>.",abstractText,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",abstractText,[0],[0]
Copyright 2017 by the author(s).,abstractText,[0],[0]
"Shrikumar et al., 2016; Binder et al., 2016; Springenberg et al., 2014).",abstractText,[0],[0]
"The intention of these works is to understand the inputoutput behavior of the deep network, which gives us the ability to improve it.",abstractText,[0],[0]
"Such understandability is critical to all computer programs, including machine learning models.",abstractText,[0],[0]
There are also other applications of attribution.,abstractText,[0],[0]
They could be used within a product driven by machine learning to provide a rationale for the recommendation.,abstractText,[0],[0]
"For instance, a deep network that predicts a condition based on imaging could help inform the doctor of the part of the image that resulted in the recommendation.",abstractText,[0],[0]
This could help the doctor understand the strengths and weaknesses of a model and compensate for it.,abstractText,[0],[0]
We give such an example in Section 6.2.,abstractText,[0],[0]
Attributions could also be used by developers in an exploratory sense.,abstractText,[0],[0]
"For instance, we could use a deep network to extract insights that could be then used in a rulebased system.",abstractText,[0],[0]
"In Section 6.3, we give such an example.",abstractText,[0],[0]
A significant challenge in designing an attribution technique is that they are hard to evaluate empirically.,abstractText,[0],[0]
"As we discuss in Section 4, it is hard to tease apart errors that stem from the misbehavior of the model versus the misbehavior of the attribution method.",abstractText,[0],[0]
"To compensate for this shortcoming, we take an axiomatic approach.",abstractText,[0],[0]
In Section 2 we identify two axioms that every attribution method must satisfy.,abstractText,[0],[0]
Unfortunately most previous methods do not satisfy one of these two axioms.,abstractText,[0],[0]
"In Section 3, we use the axioms to identify a new method, called integrated gradients.",abstractText,[0],[0]
"Unlike previously proposed methods, integrated gradients do not need any instrumentation of the network, and can be computed easily using a few calls to the gradient operation, allowing even novice practitioners to easily apply the technique.",abstractText,[0],[0]
"In Section 6, we demonstrate the ease of applicability over several deep networks, including two images networks, two text processing networks, and a chemistry network.",abstractText,[0],[0]
"These applications demonstrate the use of our technique in either improving our understanding of the network, performing debugging, performing rule extraction, or aiding an end user in understanding the network’s prediction.",abstractText,[0],[0]
Remark 1.,abstractText,[0],[0]
Let us briefly examine the need for the baseline in the definition of the attribution problem.,abstractText,[0],[0]
A common way for humans to perform attribution relies on counterar X iv :1 70 3.,abstractText,[0],[0]
01,abstractText,[0],[0]
36 5v 2,abstractText,[0],[0]
[ cs .L G ] 1 3 Ju n 20 17 Axiomatic Attribution for Deep Networks factual intuition.,abstractText,[0],[0]
When we assign blame to a certain cause we implicitly consider the absence of the cause as a baseline for comparing outcomes.,abstractText,[0],[0]
"In a deep network, we model the absence using a single baseline input.",abstractText,[0],[0]
"For most deep networks, a natural baseline exists in the input space where the prediction is neutral.",abstractText,[0],[0]
"For instance, in object recognition networks, it is the black image.",abstractText,[0],[0]
"The need for a baseline has also been pointed out by prior work on attribution (Shrikumar et al., 2016; Binder et al., 2016).",abstractText,[0],[0]
2.,abstractText,[0],[0]
Two Fundamental Axioms We now discuss two axioms (desirable characteristics) for attribution methods.,abstractText,[0],[0]
We find that other feature attribution methods in literature break at least one of the two axioms.,abstractText,[0],[0]
"These methods include DeepLift (Shrikumar et al., 2016; 2017), Layer-wise relevance propagation (LRP) (Binder et al., 2016), Deconvolutional networks (Zeiler & Fergus, 2014), and Guided back-propagation (Springenberg et al., 2014).",abstractText,[0],[0]
"As we will see in Section 3, these axioms will also guide the design of our method.",abstractText,[0],[0]
Gradients.,abstractText,[0],[0]
"For linear models, ML practitioners regularly inspect the products of the model coefficients and the feature values in order to debug predictions.",abstractText,[0],[0]
"Gradients (of the output with respect to the input) is a natural analog of the model coefficients for a deep network, and therefore the product of the gradient and feature values is a reasonable starting point for an attribution method (Baehrens et al., 2010; Simonyan et al., 2013); see the third column of Figure 2 for examples.",abstractText,[0],[0]
"The problem with gradients is that they break sensitivity, a property that all attribution methods should satisfy.",abstractText,[0],[0]
2.1.,abstractText,[0],[0]
Axiom: Sensitivity(a),abstractText,[0],[0]
An attribution method satisfies Sensitivity(a),abstractText,[0],[0]
if for every input and baseline that differ in one feature but have different predictions then the differing feature should be given a non-zero attribution.,abstractText,[0],[0]
"(Later in the paper, we will have a part (b) to this definition.)",abstractText,[0],[0]
Gradients violate Sensitivity(a):,abstractText,[0],[0]
"For a concrete example, consider a one variable, one ReLU network, f(x) = 1 − ReLU(1−x).",abstractText,[0],[0]
Suppose the baseline is x = 0 and the input is x = 2.,abstractText,[0],[0]
"The function changes from 0 to 1, but because f becomes flat at x = 1, the gradient method gives attribution of 0 to x. Intuitively, gradients break Sensitivity because the prediction function may flatten at the input and thus have zero gradient despite the function value at the input being different from that at the baseline.",abstractText,[0],[0]
"This phenomenon has been reported in previous work (Shrikumar et al., 2016).",abstractText,[0],[0]
"Practically, the lack of sensitivity causes gradients to focus on irrelevant features (see the “fireboat” example in Figure 2).",abstractText,[0],[0]
Other back-propagation based approaches.,abstractText,[0],[0]
A second set of approaches involve back-propagating the final prediction score through each layer of the network down to the individual features.,abstractText,[0],[0]
"These include DeepLift, Layer-wise relevance propagation (LRP), Deconvolutional networks (DeConvNets), and Guided back-propagation.",abstractText,[0],[0]
"These methods differ in the specific backpropagation logic for various activation functions (e.g., ReLU, MaxPool, etc.).",abstractText,[0],[0]
"Unfortunately, Deconvolution networks (DeConvNets), and Guided back-propagation violate Sensitivity(a).",abstractText,[0],[0]
This is because these methods back-propogate through a ReLU node only if the ReLU is turned on at the input.,abstractText,[0],[0]
"This makes the method similar to gradients, in that, the attribution is zero for features with zero gradient at the input despite a non-zero gradient at the baseline.",abstractText,[0],[0]
"We defer the specific counterexamples to Appendix B. Methods like DeepLift and LRP tackle the Sensitivity issue by employing a baseline, and in some sense try to compute “discrete gradients” instead of (instantaeneous) gradients at the input.",abstractText,[0],[0]
(The two methods differ in the specifics of how they compute the discrete gradient).,abstractText,[0],[0]
"But the idea is that a large, discrete step will avoid flat regions, avoiding a breakage of sensitivity.",abstractText,[0],[0]
"Unfortunately, these methods violate a different requirement on attribution methods.",abstractText,[0],[0]
2.2.,abstractText,[0],[0]
"Axiom: Implementation Invariance Two networks are functionally equivalent if their outputs are equal for all inputs, despite having very different implementations.",abstractText,[0],[0]
"Attribution methods should satisfy Implementation Invariance, i.e., the attributions are always identical for two functionally equivalent networks.",abstractText,[0],[0]
"To motivate this, notice that attribution can be colloquially defined as assigning the blame (or credit) for the output to the input features.",abstractText,[0],[0]
Such a definition does not refer to implementation details.,abstractText,[0],[0]
"We now discuss intuition for why DeepLift and LRP break Implementation Invariance; a concrete example is provided in Appendix B. First, notice that gradients are invariant to implementation.",abstractText,[0],[0]
"In fact, the chain-rule for gradients ∂f ∂g = ∂f ∂h · ∂h ∂g is essentially about implementation invariance.",abstractText,[0],[0]
"To see this, think of g and f as the input and output of a system, and h being some implementation detail of the system.",abstractText,[0],[0]
"The gradient of output f to input g can be computed either directly by ∂f ∂g , ignoring the intermediate function h (implementation detail), or by invoking the chain rule via h.",abstractText,[0],[0]
This is exactly how backpropagation works.,abstractText,[0],[0]
Methods like LRP and DeepLift replace gradients with discrete gradients and still use a modified form of backpropagation to compose discrete gradients into attributions.,abstractText,[0],[0]
"UnAxiomatic Attribution for Deep Networks fortunately, the chain rule does not hold for discrete gradients in general.",abstractText,[0],[0]
Formally f(x1)−f(x0) g(x1)−g(x0) 6=,abstractText,[0],[0]
"f(x1)−f(x0) h(x1)−h(x0) · h(x1)−h(x0) g(x1)−g(x0) , and therefore these methods fail to satisfy implementation invariance.",abstractText,[0],[0]
"If an attribution method fails to satisfy Implementation Invariance, the attributions are potentially sensitive to unimportant aspects of the models.",abstractText,[0],[0]
"For instance, if the network architecture has more degrees of freedom than needed to represent a function then there may be two sets of values for the network parameters that lead to the same function.",abstractText,[0],[0]
"The training procedure can converge at either set of values depending on the initializtion or for other reasons, but the underlying network function would remain the same.",abstractText,[0],[0]
It is undesirable that attributions differ for such reasons.,abstractText,[0],[0]
3.,abstractText,[0],[0]
Our Method: Integrated Gradients We are now ready to describe our technique.,abstractText,[0],[0]
"Intuitively, our technique combines the Implementation Invariance of Gradients along with the Sensitivity of techniques like LRP or DeepLift.",abstractText,[0],[0]
"Formally, suppose we have a function F : R",abstractText,[0],[0]
→,abstractText,[0],[0]
"[0, 1] that represents a deep network.",abstractText,[0],[0]
"Specifically, let x ∈ R be the input at hand, and x′ ∈ R be the baseline input.",abstractText,[0],[0]
"For image networks, the baseline could be the black image, while for text models it could be the zero embedding vector.",abstractText,[0],[0]
"We consider the straightline path (in R) from the baseline x′ to the input x, and compute the gradients at all points along the path.",abstractText,[0],[0]
Integrated gradients are obtained by cumulating these gradients.,abstractText,[0],[0]
"Specifically, integrated gradients are defined as the path intergral of the gradients along the straightline path from the baseline x′ to the input x.",abstractText,[0],[0]
The integrated gradient along the i dimension for an input x and baseline x′ is defined as follows.,abstractText,[0],[0]
"Here, ∂F (x) ∂xi is the gradient of F (x) along the i dimension.",abstractText,[0],[0]
IntegratedGradsi(x) ::= (xi−x ′,abstractText,[0],[0]
i)× ∫ 1 α=0 ∂F (x′+α×(x−x′)),abstractText,[0],[0]
Axiomatic Attribution for Deep Networks,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1863–1873 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1863
We experiment on two structured NLP pipelines: syntactic-then-semantic dependency parsing, and semantic parsing followed by sentiment classification. We show that training with SPIGOT leads to a larger improvement on the downstream task than a modularly-trained pipeline, the straight-through estimator, and structured attention, reaching a new state of the art on semantic dependency parsing.",text,[0],[0]
Learning methods for natural language processing are increasingly dominated by end-to-end differentiable functions that can be trained using gradient-based optimization.,1 Introduction,[0],[0]
"Yet traditional NLP often assumed modular stages of processing that formed a pipeline; e.g., text was tokenized, then tagged with parts of speech, then parsed into a
phrase-structure or dependency tree, then semantically analyzed.",1 Introduction,[0],[0]
"Pipelines, which make “hard” (i.e., discrete) decisions at each stage, appear to be incompatible with neural learning, leading many researchers to abandon earlier-stage processing.
",1 Introduction,[0],[0]
"Inspired by findings that continue to see benefit from various kinds of linguistic or domain-specific preprocessing (He et al., 2017; Oepen et al., 2017; Ji and Smith, 2017), we argue that pipelines can be treated as layers in neural architectures for NLP tasks.",1 Introduction,[0],[0]
"Several solutions are readily available: • Reinforcement learning (most notably the
REINFORCE algorithm; Williams, 1992), and structured attention (SA; Kim et al., 2017).",1 Introduction,[0],[0]
These methods replace argmax with a sampling or marginalization operation.,1 Introduction,[0],[0]
"We note two potential downsides of these approaches: (i) not all argmax-able operations have corresponding sampling or marginalization methods that are efficient, and (ii) inspection of intermediate outputs, which could benefit error analysis and system improvement, is more straightforward for hard decisions than for posteriors.",1 Introduction,[0],[0]
"• The straight-through estimator (STE; Hin-
ton, 2012) treats discrete decisions as if they were differentiable and simply passes through gradients.",1 Introduction,[0],[0]
"While fast and surprisingly effective, it ignores constraints on the argmax problem, such as the requirement that every word has exactly one syntactic parent.",1 Introduction,[0],[0]
"We will find, experimentally, that the quality of intermediate representations degrades substantially under STE.
",1 Introduction,[0],[0]
"This paper introduces a new method, the structured projection of intermediate gradients optimization technique (SPIGOT; §2), which defines a proxy for the gradient of a loss function with respect to the input to argmax.",1 Introduction,[0],[0]
"Unlike STE’s gradient proxy, SPIGOT aims to respect the constraints
in the argmax problem.",1 Introduction,[0],[0]
"SPIGOT can be applied with any intermediate layer that is expressible as a constrained maximization problem, and whose feasible set can be projected onto.",1 Introduction,[0],[0]
"We show empirically that SPIGOT works even when the maximization and the projection are done approximately.
",1 Introduction,[0],[0]
"We offer two concrete architectures that employ structured argmax as an intermediate layer: semantic parsing with syntactic parsing in the middle, and sentiment analysis with semantic parsing in the middle (§3).",1 Introduction,[0],[0]
"These architectures are trained using a joint objective, with one part using data for the intermediate task, and the other using data for the end task.",1 Introduction,[0],[0]
"The datasets are not assumed to overlap at all, but the parameters for the intermediate task are affected by both parts of the training data.
",1 Introduction,[0],[0]
"Our experiments (§4) show that our architecture improves over a state-of-the-art semantic dependency parser, and that SPIGOT offers stronger performance than a pipeline, SA, and STE.",1 Introduction,[0],[0]
"On sentiment classification, we show that semantic parsing offers improvement over a BiLSTM, more so with SPIGOT than with alternatives.",1 Introduction,[0],[0]
Our analysis considers how the behavior of the intermediate parser is affected by the end task (§5).,1 Introduction,[0],[0]
Our code is open-source and available at https:// github.com/Noahs-ARK/SPIGOT.,1 Introduction,[0],[0]
Our aim is to allow a (structured) argmax layer in a neural network to be treated almost like any other differentiable function.,2 Method,[0],[0]
"This would allow us to place, for example, a syntactic parser in the middle of a neural network, so that the forward calculation simply calls the parser and passes the parse tree to the next layer, which might derive syntactic features for the next stage of processing.
",2 Method,[0],[0]
"The challenge is in the backward computation, which is key to learning with standard gradientbased methods.",2 Method,[0],[0]
"When its output is discrete as we assume here, argmax is a piecewise constant function.",2 Method,[0],[0]
"At every point, its gradient is either zero or undefined.",2 Method,[0],[0]
"So instead of using the true gradient, we will introduce a proxy for the gradient of the loss function with respect to the inputs to argmax, allowing backpropagation to proceed through the argmax layer.",2 Method,[0],[0]
Our proxy is designed as an improvement to earlier methods (discussed below) that completely ignore constraints on the argmax operation.,2 Method,[0],[0]
"It accomplishes this through a projec-
tion of the gradients.",2 Method,[0],[0]
"We first lay out notation, and then briefly review max-decoding and its relaxation (§2.1).",2 Method,[0],[0]
"We define SPIGOT in §2.2, and show how to use it to backpropagate through NLP pipelines in §2.3.
Notation.",2 Method,[0],[0]
"Our discussion centers around two tasks: a structured intermediate task followed by an end task, where the latter considers the outputs of the former (e.g., syntactic-then-semantic parsing).",2 Method,[0],[0]
"Inputs are denoted as x, and end task outputs as y. We use z to denote intermediate structures derived from x. We will often refer to the intermediate task as “decoding”, in the structured prediction sense.",2 Method,[0],[0]
"It seeks an output ẑ = argmaxz∈Z S from the feasible set Z , maximizing a (learned, parameterized) scoring function S for the structured intermediate task.",2 Method,[0],[0]
"L denotes the loss of the end task, which may or may not also involve structured predictions.",2 Method,[0],[0]
We use ∆k−1 = {p ∈,2 Method,[0],[0]
"Rk | 1>p = 1,p ≥ 0} to denote the (k − 1)-dimensional simplex.",2 Method,[0],[0]
"We denote the domain of binary variables as B = {0, 1}, and the unit interval as U =",2 Method,[0],[0]
"[0, 1].",2 Method,[0],[0]
"By projection of a vector v onto a set A, we mean the closest point in A to v, measured by Euclidean distance: projA(v) = argminv′∈A ‖v′ − v‖2.",2 Method,[0],[0]
"Decoding problems are typically decomposed into a collection of “parts”, such as arcs in a dependency tree or graph.",2.1 Relaxed Decoding,[0],[0]
"In such a setup, each element of z, zi, corresponds to one possible part, and zi takes a boolean value to indicate whether the part is included in the output structure.",2.1 Relaxed Decoding,[0],[0]
"The scoring function S is assumed to decompose into a vector s(x) of part-local, input-specific scores:
ẑ = argmax z∈Z S(x, z) =",2.1 Relaxed Decoding,[0],[0]
"argmax z∈Z
z>s(x) (1)
In the following, we drop s’s dependence on x for clarity.
",2.1 Relaxed Decoding,[0],[0]
"In many NLP problems, the output space Z can be specified by linear constraints (Roth and Yih, 2004):
",2.1 Relaxed Decoding,[0],[0]
A [ z ψ ],2.1 Relaxed Decoding,[0],[0]
"≤ b, (2)
where ψ are auxiliary variables (also scoped by argmax), together with integer constraints (typically, each zi ∈ B).
",2.1 Relaxed Decoding,[0],[0]
"The problem in Equation 1 can be NP-complete in general, so the {0, 1} constraints are often relaxed to [0, 1] to make decoding tractable (Martins et al., 2009).",2.1 Relaxed Decoding,[0],[0]
"Then the discrete combinatorial problem over Z is transformed into the optimization of a linear objective over a convex polytope P={p ∈ Rd |Ap≤b}, which is solvable in polynomial time (Bertsimas and Tsitsiklis, 1997).",2.1 Relaxed Decoding,[0],[0]
"This is not necessary in some cases, where the argmax can be solved exactly with dynamic programming.",2.1 Relaxed Decoding,[0],[0]
"We now view structured argmax as an activation function that takes a vector of input-specific partscores s and outputs a solution ẑ. For backpropagation, to calculate gradients for parameters of s, the chain rule defines:
∇sL = J ∇ẑL, (3) where the Jacobian matrix J = ∂ẑ∂s contains the derivative of each element of ẑ with respect to each element of s. Unfortunately, argmax is a piecewise constant function, so its Jacobian is either zero (almost everywhere) or undefined (in the case of ties).
",2.2 From STE to SPIGOT,[0],[0]
"One solution, taken in structured attention, is to replace the argmax with marginal inference and a softmax function, so that ẑ encodes probability distributions over parts (Kim et al., 2017; Liu and Lapata, 2018).",2.2 From STE to SPIGOT,[0],[0]
"As discussed in §1, there are two reasons to avoid this modification.",2.2 From STE to SPIGOT,[0],[0]
"Softmax can only be used when marginal inference is feasible, by sum-product algorithms for example (Eisner, 2016; Friesen and Domingos, 2016); in general marginal inference can be #P-complete.",2.2 From STE to SPIGOT,[0],[0]
"Further, a soft intermediate layer will be less amenable to inspection by anyone wishing to understand and improve the model.
",2.2 From STE to SPIGOT,[0],[0]
"In another line of work, argmax is augmented with a strongly-convex penalty on the solutions (Martins and Astudillo, 2016; Amos and Kolter, 2017; Niculae and Blondel, 2017; Niculae et al., 2018; Mensch and Blondel, 2018).",2.2 From STE to SPIGOT,[0],[0]
"However, their approaches require solving a relaxation even when exact decoding is tractable.",2.2 From STE to SPIGOT,[0],[0]
"Also, the penalty will bias the solutions found by the decoder, which may be an undesirable conflation of computational and modeling concerns.
",2.2 From STE to SPIGOT,[0],[0]
"A simpler solution is the STE method (Hinton, 2012), which replaces the Jacobian matrix in Equation 3 by the identity matrix.",2.2 From STE to SPIGOT,[0],[0]
"This method has been demonstrated to work well when used to “backpropagate” through hard threshold functions (Bengio et al., 2013; Friesen and Domingos, 2018) and categorical random variables (Jang et al., 2016; Choi et al., 2017).
",2.2 From STE to SPIGOT,[0],[0]
"Consider for a moment what we would do if ẑ were a vector of parameters, rather than intermediate predictions.",2.2 From STE to SPIGOT,[0],[0]
"In this case, we are seeking points in Z that minimize L; denote that set of minimizers by Z∗. Given ∇ẑL and step size η, we would update ẑ to be ẑ",2.2 From STE to SPIGOT,[0],[0]
"− η∇ẑL. This update, however, might not return a value in the feasible set Z , or even (if we are using a linear relaxation) the relaxed set P .
",2.2 From STE to SPIGOT,[0],[0]
SPIGOT therefore introduces a projection step that aims to keep the “updated” ẑ in the feasible set.,2.2 From STE to SPIGOT,[0],[0]
"Of course, we do not directly update ẑ; we continue backpropagation through s and onward to the parameters.",2.2 From STE to SPIGOT,[0],[0]
"But the projection step nonetheless alters the parameter updates in the way that our proxy for “∇sL” is defined.
",2.2 From STE to SPIGOT,[0],[0]
"The procedure is defined as follows:
p̂ = ẑ− η∇ẑL, (4a)",2.2 From STE to SPIGOT,[0],[0]
"z̃ = projP(p̂), (4b) ∇sL , ẑ−",2.2 From STE to SPIGOT,[0],[0]
"z̃. (4c)
",2.2 From STE to SPIGOT,[0],[0]
"First, the method makes an “update” to ẑ as if it contained parameters (Equation 4a), letting p̂ denote the new value.",2.2 From STE to SPIGOT,[0],[0]
"Next, p̂ is projected back onto the (relaxed) feasible set (Equation 4b), yielding a feasible new value z̃.",2.2 From STE to SPIGOT,[0],[0]
"Finally, the gradients with respect to s are computed by Equation 4c.
",2.2 From STE to SPIGOT,[0],[0]
"Due to the convexity of P , the projected point z̃ will always be unique, and is guaranteed to be no farther than p̂ from any point in Z∗ (Luenberger and Ye, 2015).1 Compared to STE, SPIGOT in-
1Note that this property follows from P’s convexity, and we do not assume the convexity of L.
volves a projection and limits ∇sL to a smaller space to satisfy constraints.",2.2 From STE to SPIGOT,[0],[0]
"See Figure 1 for an illustration.
",2.2 From STE to SPIGOT,[0],[0]
"When efficient exact solutions (such as dynamic programming) are available, they can be used.",2.2 From STE to SPIGOT,[0],[0]
"Yet, we note that SPIGOT does not assume the argmax operation is solved exactly.",2.2 From STE to SPIGOT,[0],[0]
"Using SPIGOT, we now devise an algorithm to “backpropagate” through NLP pipelines.",2.3 Backpropagation through Pipelines,[0],[0]
"In these pipelines, an intermediate task’s output is fed into an end task for use as features.",2.3 Backpropagation through Pipelines,[0],[0]
"The parameters of the complete model are divided into two parts: denote the parameters of the intermediate task model byφ (used to calculate s), and those in the end task model as θ.2 As introduced earlier, the end-task loss function to be minimized is L, which depends on both φ and θ.
Algorithm 1 describes the forward and backward computations.",2.3 Backpropagation through Pipelines,[0],[0]
"It takes an end task training pair 〈x,y〉, along with the intermediate task’s feasible set Z , which is determined by x. It first runs the intermediate model and decodes to get intermediate structure ẑ, just as in a standard pipeline.",2.3 Backpropagation through Pipelines,[0],[0]
"Then forward propagation is continued into the end-task model to compute loss L, using ẑ to define input features.",2.3 Backpropagation through Pipelines,[0],[0]
"Backpropagation in the endtask model computes ∇θL and ∇ẑL, and ∇sL is then constructed using Equations 4.",2.3 Backpropagation through Pipelines,[0],[0]
"Backpropagation then continues into the intermediate model, computing∇φL.
Due to its flexibility, SPIGOT is applicable to many training scenarios.",2.3 Backpropagation through Pipelines,[0],[0]
"When there is no 〈x, z〉 training data for the intermediate task, SPIGOT can be used to induce latent structures for the end-task (Yogatama et al., 2017; Kim et al., 2017; Choi et al., 2017, inter alia).",2.3 Backpropagation through Pipelines,[0],[0]
"When intermediate-task training data is available, one can use SPIGOT to adopt joint learning by minimizing an interpolation of L (on end-task data 〈x,y〉) and an intermediate-task loss function L̃ (on intermediate task data 〈x, z〉).",2.3 Backpropagation through Pipelines,[0],[0]
This is the setting in our experiments; note that we do not assume any overlap in the training examples for the two tasks.,2.3 Backpropagation through Pipelines,[0],[0]
"In this section we discuss how to compute approximate projections for the two intermediate tasks
2Nothing prohibits tying across pre-argmax parameters and post-argmax parameters; this separation is notationally convenient but not at all necessary.
",3 Solving the Projections,[0],[0]
"Algorithm 1 Forward and backward computation with SPIGOT. 1: procedure SPIGOT(x,y,Z) 2: Construct A, b such that Z = {p ∈",3 Solving the Projections,[0],[0]
Zd | Ap ≤ b} 3: P ← {p ∈ Rd | Ap ≤ b} .,3 Solving the Projections,[0],[0]
Relaxation 4: Forwardprop and compute sφ(x) 5: ẑ← argmaxz∈Z z>sφ(x) .,3 Solving the Projections,[0],[0]
"Intermediate decoding 6: Forwardprop and compute L given x, y, and ẑ 7: Backprop and compute∇θL and∇ẑL 8: z̃← projP(ẑ− η∇ẑL) .",3 Solving the Projections,[0],[0]
"Projection 9: ∇sL← ẑ− z̃ 10: Backprop and compute∇φL 11: end procedure
considered in this work, arc-factored unlabeled dependency parsing and first-order semantic dependency parsing.
",3 Solving the Projections,[0],[0]
"In early experiments we observe that for both tasks, projecting with respect to all constraints of their original formulations using a generic quadratic program solver was prohibitively slow.",3 Solving the Projections,[0],[0]
"Therefore, we construct relaxed polytopes by considering only a subset of the constraints.3",3 Solving the Projections,[0],[0]
"The projection then decomposes into a series of singly constrained quadratic programs (QP), each of which can be efficiently solved in linear time.
",3 Solving the Projections,[0],[0]
The two approximate projections discussed here are used in backpropagation only.,3 Solving the Projections,[0],[0]
"In the forward pass, we solve the decoding problem using the models’ original decoding algorithms.
",3 Solving the Projections,[0],[0]
Arc-factored unlabeled dependency parsing.,3 Solving the Projections,[0],[0]
"For unlabeled dependency trees, we impose [0, 1] constraints and single-headedness constraints.4
Formally, given a length-n input sentence, excluding self-loops, an arc-factored parser considers d = n(n − 1) candidate arcs.",3 Solving the Projections,[0],[0]
"Let i→j denote an arc from the ith token to the jth, and σ(i→j) denote its index.",3 Solving the Projections,[0],[0]
"We construct the relaxed feasible set by:
PDEP = p ∈",3 Solving the Projections,[0],[0]
Ud ∣∣∣∣∣∣ ∑,3 Solving the Projections,[0],[0]
i 6=j pσ(i→j) =,3 Solving the Projections,[0],[0]
"1,∀j  , (5) i.e., we consider each token j individually, and force single-headedness by constraining the number of arcs incoming to j to sum to 1.",3 Solving the Projections,[0],[0]
"Algorithm 2 summarizes the procedure to project onto PDEP.
",3 Solving the Projections,[0],[0]
"3A parallel work introduces an active-set algorithm to solve the same class of quadratic programs (Niculae et al., 2018).",3 Solving the Projections,[0],[0]
"It might be an efficient approach to solve the projections in Equation 4b, which we leave to future work.
4",3 Solving the Projections,[0],[0]
"It requires O(n2) auxiliary variables and O(n3) additional constraints to ensure well-formed tree structures (Martins et al., 2013).
",3 Solving the Projections,[0],[0]
"Line 3 forms a singly constrained QP, and can be solved in O(n) time (Brucker, 1984).
",3 Solving the Projections,[0],[0]
Algorithm 2 Projection onto the relaxed polytope PDEP for dependency tree structures.,3 Solving the Projections,[0],[0]
"Let bold σ(·→j) denote the index set of arcs incoming to j. For a vector v, we use vσ(·→j) to denote vector [vk]k∈σ(·→j).
1: procedure DEPPROJ(p̂) 2: for j = 1, 2, . . .",3 Solving the Projections,[0],[0]
", n",3 Solving the Projections,[0],[0]
"do 3: z̃σ(·→j) ← proj∆n−2 ( p̂σ(·→j) ) 4: end for 5: return z̃ 6: end procedure
First-order semantic dependency parsing.",3 Solving the Projections,[0],[0]
"Semantic dependency parsing uses labeled bilexical dependencies to represent sentence-level semantics (Oepen et al., 2014, 2015, 2016).",3 Solving the Projections,[0],[0]
"Each dependency is represented by a labeled directed arc from a head token to a modifier token, where the arc label encodes broadly applicable semantic relations.",3 Solving the Projections,[0],[0]
"Figure 2 diagrams a semantic graph from the DELPH-IN MRS-derived dependencies (DM), together with a syntactic tree.
",3 Solving the Projections,[0],[0]
"We use a state-of-the-art semantic dependency parser (Peng et al., 2017) that considers three types of parts: heads, unlabeled arcs, and labeled arcs.",3 Solving the Projections,[0],[0]
Let σ(i `→ j) denote the index of the arc from i to j with semantic role `.,3 Solving the Projections,[0],[0]
"In addition to [0, 1] constraints, we constrain that the predictions for labeled arcs sum to the prediction of their associated unlabeled arc:
PSDP { p ∈ Ud ∣∣∣∣∣∑ ` p σ(i `→j) = pσ(i→j), ∀i 6= j } .
(6)
",3 Solving the Projections,[0],[0]
This ensures that exactly one label is predicted if and only if its arc is present.,3 Solving the Projections,[0],[0]
The projection onto PSDP can be solved similarly to Algorithm 2.,3 Solving the Projections,[0],[0]
We drop the determinism constraint imposed by Peng et al. (2017) in the backward computation.,3 Solving the Projections,[0],[0]
"We empirically evaluate our method with two sets of experiments: using syntactic tree structures in semantic dependency parsing, and using semantic dependency graphs in sentiment classification.",4 Experiments,[0],[0]
"In this experiment we consider an intermediate syntactic parsing task, followed by seman-
… became dismayed at
poss arg1
arg2
’sG-2 connections arrested traffickersto drug
arg2 compound
root
arg2 arg1 arg2
tic dependency parsing as the end task.",4.1 Syntactic-then-Semantic Parsing,[0],[0]
"We first briefly review the neural network architectures for the two models (§4.1.1), and then introduce the datasets (§4.1.2) and baselines (§4.1.3).",4.1 Syntactic-then-Semantic Parsing,[0],[0]
Syntactic dependency parser.,4.1.1 Architectures,[0],[0]
"For intermediate syntactic dependencies, we use the unlabeled arc-factored parser of Kiperwasser and Goldberg (2016).",4.1.1 Architectures,[0],[0]
"It uses bidirectional LSTMs (BiLSTM) to encode the input, followed by a multilayerperceptron (MLP) to score each potential dependency.",4.1.1 Architectures,[0],[0]
"One notable modification is that we replace their use of Chu-Liu/Edmonds’ algorithm (Chu and Liu, 1965; Edmonds, 1967) with the Eisner algorithm (Eisner, 1996, 2000), since our dataset is in English and mostly projective.
",4.1.1 Architectures,[0],[0]
Semantic dependency parser.,4.1.1 Architectures,[0],[0]
We use the basic model of Peng et al. (2017) (denoted as NEURBOPARSER) as the end model.,4.1.1 Architectures,[0],[0]
"It is a first-order parser, and uses local factors for heads, unlabeled arcs, and labeled arcs.",4.1.1 Architectures,[0],[0]
NEURBOPARSER does not use syntax.,4.1.1 Architectures,[0],[0]
"It first encodes an input sentence with a two-layer BiLSTM, and then computes part scores with two-layer tanh-MLPs.",4.1.1 Architectures,[0],[0]
"Inference is conducted with AD3 (Martins et al., 2015).",4.1.1 Architectures,[0],[0]
"To add syntactic features to NEURBOPARSER, we concatenate a token’s contextualized representation to that of its syntactic head, predicted by the intermediate parser.",4.1.1 Architectures,[0],[0]
"Formally, given length-n input sentence, we first run a BiLSTM.",4.1.1 Architectures,[0],[0]
We use the concatenation of the two hidden representations hj =,4.1.1 Architectures,[0],[0]
[ −→ h j ; ←−,4.1.1 Architectures,[0],[0]
h j ] at each position j as the contextualized token representations.,4.1.1 Architectures,[0],[0]
"We then concatenate
hj with the representation of its head hHEAD(j) by
h̃j =",4.1.1 Architectures,[0],[0]
[hj ;hHEAD(j)],4.1.1 Architectures,[0],[0]
= hj ;∑ i 6=j ẑσ(i→j),4.1.1 Architectures,[0],[0]
"hi  , (7)
where ẑ ∈ Bn(n−1) is a binary encoding of the tree structure predicted by by the intermediate parser.",4.1.1 Architectures,[0],[0]
We then use h̃j anywhere hj would have been used in NEURBOPARSER.,4.1.1 Architectures,[0],[0]
"In backpropagation, we compute ∇ẑL with an automatic differentiation toolkit (DyNet; Neubig et al., 2017).
",4.1.1 Architectures,[0],[0]
"We note that this approach can be generalized to convolutional neural networks over graphs (Mou et al., 2015; Duvenaud et al., 2015; Kipf and Welling, 2017, inter alia), recurrent neural networks along paths (Xu et al., 2015; Roth and Lapata, 2016, inter alia) or dependency trees (Tai et al., 2015).",4.1.1 Architectures,[0],[0]
"We choose to use concatenations to control the model’s complexity, and thus to better understand which parts of the model work.
",4.1.1 Architectures,[0],[0]
"We refer the readers to Kiperwasser and Goldberg (2016) and Peng et al. (2017) for further details of the parsing models.
",4.1.1 Architectures,[0],[0]
Training procedure.,4.1.1 Architectures,[0],[0]
"Following previous work, we minimize structured hinge loss (Tsochantaridis et al., 2004) for both models.",4.1.1 Architectures,[0],[0]
"We jointly train both models from scratch, by randomly sampling an instance from the union of their training data at each step.",4.1.1 Architectures,[0],[0]
"In order to isolate the effect of backpropagation, we do not share any parameters between the two models.5 Implementation details are summarized in the supplementary materials.",4.1.1 Architectures,[0],[0]
"• For semantic dependencies, we use the
English dataset from SemEval 2015 Task 18 (Oepen et al., 2015).",4.1.2 Datasets,[0],[0]
"Among the three formalisms provided by the shared task, we consider DELPH-IN MRS-derived dependencies (DM) and Prague Semantic Dependencies (PSD).6 It includes §00–19 of the WSJ corpus as training data, §20 and §21 for development and in-domain test data, resulting in a 33,961/1,692/1,410 train/dev./test split, and
5 Parameter sharing has proved successful in many related tasks (Collobert and Weston, 2008; Søgaard and Goldberg, 2016; Ammar et al., 2016; Swayamdipta et al., 2016, 2017, inter alia), and could be easily combined with our approach.
",4.1.2 Datasets,[0],[0]
"6We drop the third (PAS) because its structure is highly predictable from parts-of-speech, making it less interesting.
",4.1.2 Datasets,[0],[0]
"1,849 out-of-domain test instances from the Brown corpus.7 •",4.1.2 Datasets,[0],[0]
"For syntactic dependencies, we use the Stanford Dependency (de Marneffe and Manning, 2008) conversion of the the Penn Treebank WSJ portion (Marcus et al., 1993).",4.1.2 Datasets,[0],[0]
"To avoid data leak, we depart from standard split and use §20 and §21 as development and test data, and the remaining sections as training data.",4.1.2 Datasets,[0],[0]
"The number of training/dev./test instances is 40,265/2,012/1,671.",4.1.2 Datasets,[0],[0]
We compare to the following baselines: • A pipelined system (PIPELINE).,4.1.3 Baselines,[0],[0]
"The pre-
trained parser achieves 92.9 test unlabeled attachment score (UAS).8
7The organizers remove, e.g., instances with cyclic graphs, and thus only a subset of the WSJ corpus is included.",4.1.3 Baselines,[0],[0]
"See Oepen et al. (2015) for details.
",4.1.3 Baselines,[0],[0]
8 Note that this number is not comparable to the parsing literature due to the different split.,4.1.3 Baselines,[0],[0]
"As a sanity check, we found in preliminary experiments that the same parser archi-
• Structured attention networks (SA; Kim et al., 2017).",4.1.3 Baselines,[0],[0]
"We use the inside-outside algorithm (Baker, 1979) to populate z with arcs’ marginal probabilities, use log-loss as the objective in training the intermediate parser.",4.1.3 Baselines,[0],[0]
"• The straight-through estimator (STE; Hinton,
2012), introduced in §2.2.",4.1.3 Baselines,[0],[0]
Table 1 compares the semantic dependency parsing performance of SPIGOT to all five baselines.,4.1.4 Empirical Results,[0],[0]
"FREDA3 (Peng et al., 2017) is a state-of-the-art variant of NEURBOPARSER that is trained using multitask learning to jointly predict three different semantic dependency graph formalisms.",4.1.4 Empirical Results,[0],[0]
"Like the basic NEURBOPARSER model that we build from, FREDA3 does not use any syntax.",4.1.4 Empirical Results,[0],[0]
"Strong DM performance is achieved in a more recent work by using joint learning and an ensemble (Peng et al., 2018), which is beyond fair comparisons to the models discussed here.
",4.1.4 Empirical Results,[0],[0]
We found that using syntactic information improves semantic parsing performance: using pipelined syntactic head features brings 0.5– 1.4% absolute labeled F1 improvement to NEURBOPARSER.,4.1.4 Empirical Results,[0],[0]
"Such improvements are smaller compared to previous works, where dependency path and syntactic relation features are included (Almeida and Martins, 2015; Ribeyre et al., 2015; Zhang et al., 2016), indicating the potential to get better performance by using more syntactic information, which we leave to future work.
",4.1.4 Empirical Results,[0],[0]
Both STE and SPIGOT use hard syntactic features.,4.1.4 Empirical Results,[0],[0]
"By allowing backpropation into the intermediate syntactic parser, they both consistently outperform PIPELINE.",4.1.4 Empirical Results,[0],[0]
"On the other hand, when marginal syntactic tree structures are used, SA outperforms PIPELINE only on the out-of-domain PSD test set, and improvements under other cases are not observed.
",4.1.4 Empirical Results,[0],[0]
"Compared to STE, SPIGOT outperforms STE on DM by more than 0.3% absolute labeled F1, both in-domain and out-of-domain.",4.1.4 Empirical Results,[0],[0]
"For PSD, SPIGOT achieves similar performance to STE on in-domain test set, but has a 0.5% absolute labeled F1 improvement on out-of-domain data, where syntactic parsing is less accurate.
",4.1.4 Empirical Results,[0],[0]
"tecture achieves 93.5 UAS when trained and evaluated with the standard split, close to the results reported by Kiperwasser and Goldberg (2016).",4.1.4 Empirical Results,[0],[0]
Our second experiment uses semantic dependency graphs to improve sentiment classification performance.,4.2 Semantic Dependencies for Sentiment Classification,[0],[0]
"We are not aware of any efficient algorithm that solves marginal inference for semantic dependency graphs under determinism constraints, so we do not include a comparison to SA.",4.2 Semantic Dependencies for Sentiment Classification,[0],[0]
"Here we use NEURBOPARSER as the intermediate model, as described in §4.1.1, but with no syntactic enhancements.
",4.2.1 Architectures,[0],[0]
Sentiment classifier.,4.2.1 Architectures,[0],[0]
We first introduce a baseline that does not use any structural information.,4.2.1 Architectures,[0],[0]
"It learns a one-layer BiLSTM to encode the input sentence, and then feeds the sum of all hidden states into a two-layer ReLU-MLP.
",4.2.1 Architectures,[0],[0]
"To use semantic dependency features, we concatenate a word’s BiLSTM-encoded representation to the averaged representation of its heads, together with the corresponding semantic roles, similarly to that in Equation 7.9 Then the concatenation is fed into an affine transformation followed by a ReLU activation.",4.2.1 Architectures,[0],[0]
"The rest of the model is kept the same as the BiLSTM baseline.
",4.2.1 Architectures,[0],[0]
Training procedure.,4.2.1 Architectures,[0],[0]
"We use structured hinge loss to train the semantic dependency parser, and log-loss for the sentiment classifier.",4.2.1 Architectures,[0],[0]
"Due to the discrepancy in the training data size of the two tasks (33K vs. 7K), we pre-train a semantic dependency parser, and then adopt joint training together with the classifier.",4.2.1 Architectures,[0],[0]
"In the joint training stage, we randomly sample 20% of the semantic dependency training instances each epoch.",4.2.1 Architectures,[0],[0]
Implementations are detailed in the supplementary materials.,4.2.1 Architectures,[0],[0]
"For semantic dependencies, we use the DM dataset introduced in §4.1.2.
We consider a binary classification task using the Stanford Sentiment Treebank (Socher et al., 2013).",4.2.2 Datasets,[0],[0]
It consists of roughly 10K movie review sentences from Rotten Tomatoes.,4.2.2 Datasets,[0],[0]
"The full dataset includes a rating on a scale from 1 to 5 for each constituent (including the full sentences), resulting in more than 200K instances.",4.2.2 Datasets,[0],[0]
"Following previous work (Iyyer et al., 2015), we only use full-sentence
9In a well-formed semantic dependency graph, a token may have multiple heads.",4.2.2 Datasets,[0],[0]
"Therefore we use average instead of the sum in Equation 7.
instances, with neutral instances excluded (3s) and the remaining four rating levels converted to binary “positive” or “negative” labels.",4.2.2 Datasets,[0],[0]
"This results in a 6,920/872/1,821 train/dev./test split.",4.2.2 Datasets,[0],[0]
Table 2 compares our SPIGOT method to three baselines.,4.2.3 Empirical Results,[0],[0]
"Pipelined semantic dependency predictions brings 0.9% absolute improvement in classification accuracy, and SPIGOT outperforms all baselines.",4.2.3 Empirical Results,[0],[0]
In this task STE achieves slightly worse performance than a fixed pre-trained PIPELINE.,4.2.3 Empirical Results,[0],[0]
We examine here how the intermediate model is affected by the end-task training signal.,5 Analysis,[0],[0]
"Is the endtask signal able to “overrule” intermediate predictions?
",5 Analysis,[0],[0]
We use the syntactic-then-semantic parsing model (§4.1) as a case study.,5 Analysis,[0],[0]
Table 3 compares a pipelined system to one jointly trained using SPIGOT.,5 Analysis,[0],[0]
"We consider the development set instances where both syntactic and semantic annotations are available, and partition them based on whether the two systems’ syntactic predictions agree (SAME), or not (DIFF).",5 Analysis,[0],[0]
"The second group includes sentences with much lower syntactic parsing accuracy (91.3 vs. 97.4 UAS), and SPIGOT further reduces this to 89.6.",5 Analysis,[0],[0]
"Even though these changes hurt syntactic parsing accuracy, they lead to a 1.1% absolute gain in labeled F1 for semantic parsing.",5 Analysis,[0],[0]
"Furthermore, SPIGOT has an overall less detrimental effect on the intermediate parser than STE: using SPIGOT, intermediate dev. parsing UAS drops to 92.5 from the 92.9 pipelined performance, while STE reduces it to 91.8.
",5 Analysis,[0],[0]
We then take a detailed look and categorize the changes in intermediate trees by their correlations with the semantic graphs.,5 Analysis,[0],[0]
"Specifically, when a modifier m’s head is changed from h to h′ in the
tree, we consider three cases: (a) h′ is a head of m in the semantic graph; (b) h′ is a modifier of m in the semantic graph; (c) h is the modifier of m in the semantic graph.",5 Analysis,[0],[0]
The first two reflect modifications to the syntactic parse that rearrange semantically linked words to be neighbors.,5 Analysis,[0],[0]
"Under (c), the semantic parser removes a syntactic dependency that reverses the direction of a semantic dependency.",5 Analysis,[0],[0]
"These cases account for 17.6%, 10.9%, and 12.8%, respectively (41.2% combined) of the total changes.",5 Analysis,[0],[0]
"Making these changes, of course, is complicated, since they often require other modifications to maintain well-formedness of the tree.",5 Analysis,[0],[0]
Figure 2 gives an example.,5 Analysis,[0],[0]
Joint learning in NLP pipelines.,6 Related Work,[0],[0]
"To avoid cascading errors, much effort has been devoted to joint decoding in NLP pipelines (Habash and Rambow, 2005; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Lewis et al., 2015; Zhang et al., 2015, inter alia).",6 Related Work,[0],[0]
"However, joint inference can sometimes be prohibitively expensive.",6 Related Work,[0],[0]
"Recent advances in representation learning facilitate exploration in the joint learning of multiple tasks by sharing parameters (Collobert and Weston, 2008; Blitzer et al., 2006; Finkel and Manning, 2010; Zhang and Weiss, 2016; Hashimoto et al., 2017, inter alia).
",6 Related Work,[0],[0]
Differentiable optimization.,6 Related Work,[0],[0]
"Gould et al. (2016) review the generic approaches to differentiation in bi-level optimization (Bard, 2010; Kunisch and Pock, 2013).",6 Related Work,[0],[0]
Amos and Kolter (2017) extend their efforts to a class of subdifferentiable quadratic programs.,6 Related Work,[0],[0]
"However, they both require that the intermediate objective has an invertible Hessian, limiting their application
in NLP.",6 Related Work,[0],[0]
"In another line of work, the steps of a gradient-based optimization procedure are unrolled into a single computation graph (Stoyanov et al., 2011; Domke, 2012; Goodfellow et al., 2013; Brakel et al., 2013).",6 Related Work,[0],[0]
This comes at a high computational cost due to the second-order derivative computation during backpropagation.,6 Related Work,[0],[0]
"Moreover, constrained optimization problems (like many NLP problems) often require projection steps within the procedure, which can be difficult to differentiate through (Belanger and McCallum, 2016; Belanger et al., 2017).",6 Related Work,[0],[0]
"We presented SPIGOT, a novel approach to backpropagating through neural network architectures that include discrete structured decisions in intermediate layers.",7 Conclusion,[0],[0]
"SPIGOT devises a proxy for the gradients with respect to argmax’s inputs, employing a projection that aims to respect the constraints in the intermediate task.",7 Conclusion,[0],[0]
"We empirically evaluate our method with two architectures: a semantic parser with an intermediate syntactic parser, and a sentiment classifier with an intermediate semantic parser.",7 Conclusion,[0],[0]
"Experiments show that SPIGOT achieves stronger performance than baselines under both settings, and outperforms stateof-the-art systems on semantic dependency parsing.",7 Conclusion,[0],[0]
Our implementation is available at https: //github.com/Noahs-ARK/SPIGOT.,7 Conclusion,[0],[0]
"We thank the ARK, Julian Michael, Minjoon Seo, Eunsol Choi, and Maxwell Forbes for their helpful comments on an earlier version of this work, and the anonymous reviewers for their valuable feedback.",Acknowledgments,[0],[0]
This work was supported in part by NSF grant IIS-1562364.,Acknowledgments,[0],[0]
"We introduce the structured projection of intermediate gradients optimization technique (SPIGOT), a new method for backpropagating through neural networks that include hard-decision structured predictions (e.g., parsing) in intermediate layers.",abstractText,[0],[0]
"SPIGOT requires no marginal inference, unlike structured attention networks (Kim et al., 2017) and some reinforcement learning-inspired solutions (Yogatama et al., 2017).",abstractText,[0],[0]
"Like socalled straight-through estimators (Hinton, 2012), SPIGOT defines gradient-like quantities associated with intermediate nondifferentiable operations, allowing backpropagation before and after them; SPIGOT’s proxy aims to ensure that, after a parameter update, the intermediate structure will remain well-formed.",abstractText,[0],[0]
"We experiment on two structured NLP pipelines: syntactic-then-semantic dependency parsing, and semantic parsing followed by sentiment classification.",abstractText,[0],[0]
"We show that training with SPIGOT leads to a larger improvement on the downstream task than a modularly-trained pipeline, the straight-through estimator, and structured attention, reaching a new state of the art on semantic dependency parsing.",abstractText,[0],[0]
Backpropagating through Structured Argmax using a SPIGOT,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 153–161 New Orleans, Louisiana, June 1 - 6, 2018. c©2017 Association for Computational Linguistics",text,[0],[0]
"Natural language understanding (NLU) is a key component of dialog systems for commercial personal digital assistants (PDAs) such as Amazon Alexa, Google Home, Microsoft Cortana and Apple Siri.",1 Introduction,[0],[0]
"The task of the NLU component is to map input user utterances into a semantic frame consisting of domain, intent and slots (Kurata et al., 2016).",1 Introduction,[0],[0]
"The semantic frame is used by the dialog manager for state tracking and action selection.
",1 Introduction,[0],[0]
"Slot tagging can be formulated as a sequence classification task where each input word in the user utterance must be classified as belonging to one of the slot types in a predefined schema (Sarikaya et al., 2016).",1 Introduction,[0],[0]
"In a standard NLU architecture, each new domain defines a new domainspecific schema for its slots.",1 Introduction,[0],[0]
"Figure 1 shows examples of annotated queries from three different domains relevant to a typical commercial digital
assistant.",1 Introduction,[0],[0]
"Since the schemas for different domains can vary, the usual strategy is to train a separate slot tagging model for each new domain.",1 Introduction,[0],[0]
"However, the number of domains increases rapidly as the PDAs are required to support new scenarios and training a separate slot tagging model for each new domain becomes prohibitively expensive in terms of annotation costs.
",1 Introduction,[0],[0]
"Even though different domains have different slot tagging schemas, some classes of slots appear across a number of domains, as suggested by the examples in Figure 1.",1 Introduction,[0],[0]
"Both travel and flight status have date and time related slots, and all three domains have the location slot.",1 Introduction,[0],[0]
Reusing annotated data for these common slots would allow us to train models with better accuracy using less data.,1 Introduction,[0],[0]
"However, since both the input distribution and the label distribution are different across domains, we must use domain adaptation methods to train on the joint data (Daume, 2007; Kim et al.,
153
2016c; Blitzer et al., 2006).",1 Introduction,[0],[0]
"In this data-driven adaptation approach, we build a repository of annotated data containing date, time, location and other reusable slots.",1 Introduction,[0],[0]
We then combine relevant data from the reusable repository with the domain specific data during model training.,1 Introduction,[0],[0]
"Figure 2(a) shows an example of this architecture where reusable date/time data is used for training travel domain.
",1 Introduction,[0],[0]
"A drawback of the data-driven adaptation approach is that as the repository of data for reusable slots grows, the training time for new domains increases.",1 Introduction,[0],[0]
"The training data for a new domain might be in the hundreds of samples, while the training data for the reusable slots might contain hundreds of thousands of samples.",1 Introduction,[0],[0]
"This increase in training time makes iterative refinement difficult in the initial design of new domains, which is when the ability to deploy new models quickly is crucial.
",1 Introduction,[0],[0]
"An alternative strategy is to use model-driven adaptation approaches (Kim et al., 2017b) as shown in Figure 2(b).",1 Introduction,[0],[0]
"Here, instead of retraining on the data for the reusable slots, we train “expert” models for these slots, and use the output of these models directly when training new domains.",1 Introduction,[0],[0]
"Using model-driven adaptation ensures that model training time is proportional to the data size of new
target domains, as opposed to the large data size for reusable slots, allowing for faster training.
",1 Introduction,[0],[0]
"In this paper, we present a model-driven adaptation approach for slot tagging called Bag of Experts (BoE).",1 Introduction,[0],[0]
"In Section 2, we first describe how this approach can be applied to two popular machine learning methods used for slot tagging: Long Short Term Memory (LSTM) and Conditional Random Fields (CRF) models.",1 Introduction,[0],[0]
"We then describe a dataset of 10 target domains and 2 reusable domains that we’ve collected for use in a commercial digital assistant, in Section 3.",1 Introduction,[0],[0]
"Using this data, we conduct experiments comparing the BoE models with their non-expert counterparts, and show that BoE models can lead to significant F1-score improvements.",1 Introduction,[0],[0]
The experimental setup is described in Section 4.1 and the results are discussed in Section 4.3.,1 Introduction,[0],[0]
This is followed by a survey of related work in Section 5 and the conclusion in Section 6.,1 Introduction,[0],[0]
"We first describe our LSTM and CRF models for slot tagging, followed by their BoE variants: LSTM-BoE and CRF-BoE. Tensorflow (Abadi et al., 2015) was used for implementing the LSTM models, while a custom C++ implementation was
used for the CRF models.",2 Approaches,[0],[0]
"For our LSTM model, we follow a standard bidirectional LSTM architecture (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016).",2.1 LSTM,[0],[0]
Let w1...,2.1 LSTM,[0],[0]
wn denote the input word sequence.,2.1 LSTM,[0],[0]
"For every input word wi, let fCi and b C i be the outputs of the forward and backward character level LSTMs respectively, and let mi be the word embedding (initialized either randomly or with pretrained embeddings).",2.1 LSTM,[0],[0]
"The input to the word level LSTMs, gi, is the concatenation of these three vectors:
gi =",2.1 LSTM,[0],[0]
"[f C i ; b C i ;mi]
where both fCi , b C i ∈",2.1 LSTM,[0],[0]
R25 and mi has the same dimensions as the pre-trained embeddings.,2.1 LSTM,[0],[0]
"The forward and backward word level LSTMs take gi as input and produce fWi and b W i , which are then concatenated to produce hi:
hi =",2.1 LSTM,[0],[0]
"[f W i , b W i ]
",2.1 LSTM,[0],[0]
"where fWi , b W i ∈ R100, making hi ∈ R200.",2.1 LSTM,[0],[0]
hi is then input to a dense feed forward layer with a softmax activation to predict the label probabilities for each word.,2.1 LSTM,[0],[0]
"We train using stochastic gradient descent with Adam (Kingma and Ba, 2015).",2.1 LSTM,[0],[0]
"To avoid overfitting, we also use dropout on top of mi and hi layers, with a default dropout keep probability of 0.8.",2.1 LSTM,[0],[0]
"We experiment with some variations
of this default LSTM architecture, the results are described in Section 4.2.",2.1 LSTM,[0],[0]
We now describe the LSTM Bag of Experts (LSTM-BoE) architecture.,2.2 LSTM-BoE,[0],[0]
Let e1...,2.2 LSTM-BoE,[0],[0]
ek ∈ E be the set of reusable expert domains.,2.2 LSTM-BoE,[0],[0]
"For each expert ej , we train a separate LSTM with the architecture described in Section 2.1.",2.2 LSTM-BoE,[0],[0]
"Let heji be the bi-directional word LSTM output for expert ej on word wi.
",2.2 LSTM-BoE,[0],[0]
"When training on a target domain, for each word wi, we first compute the character level LSTMs fCi , b C i similarly to Section 2.1.",2.2 LSTM-BoE,[0],[0]
"We then compute a BoE representation for this word as:
hE = ∑
ei∈E h ej",2.2 LSTM-BoE,[0],[0]
"i
The input to the word level LSTM for word wi in the target domain is now a concatenation of the character level LSTM outputs (fCi , b C i ), the word embedding mi, and hE :
gi =",2.2 LSTM-BoE,[0],[0]
[f C i ; b C i ;mi;h,2.2 LSTM-BoE,[0],[0]
"E ]
gi is then input to the word level LSTM for the target domain to produce hi in the same way as Section 2.1.",2.2 LSTM-BoE,[0],[0]
"This architecture is similar to the one presented in (Kim et al., 2017b), with the exception that in their architecture, hE is concatenated with the word level LSTM output hi for the target
domain.",2.2 LSTM-BoE,[0],[0]
"In our architecture, we add hE before the word-level LSTM in order to capture long-range dependencies of label prediction for a word on expert predictions for context words.",2.2 LSTM-BoE,[0],[0]
"Conditional Random Fields (CRF) are a popular family of models that have been proven to work well in a variety of sequence tagging NLP applications (Lafferty et al., 2001).",2.3 CRF,[0],[0]
"For our experiments, we use a standard linear-chain CRF architecture with n-gram and context features.
",2.3 CRF,[0],[0]
"In particular, for each token, we use unigram, bigram and trigram features, along with previous and next unigrams, bigrams, and trigrams for context length of up to 3 words.",2.3 CRF,[0],[0]
"We also use a skip bigram feature created by concatenating the current unigram and skip-one unigram.
",2.3 CRF,[0],[0]
We train our CRF using stochastic gradient descent with L1 regularization to prevent overfitting.,2.3 CRF,[0],[0]
"The L1 coefficient was set to 0.1 and we use a learning rate of 0.1 with exponential decay for learning rate scheduling (Tsuruoka et al., 2009).",2.3 CRF,[0],[0]
"Similar to the LSTM-BoE model, we first train a CRF model cj for each of the reusable expert domains ej ∈ E. When training on a target domain, for every query word wi, a one-hot label vector",2.4 CRF-BoE,[0],[0]
l j,2.4 CRF-BoE,[0],[0]
i is emitted by each expert CRF model cj .,2.4 CRF-BoE,[0],[0]
"The length of the label vector lji is the number of labels in the expert domain, with the value corresponding to the label predicted by cj for word wi set to 1, and values for all other labels set to 0.",2.4 CRF-BoE,[0],[0]
"For each word, the label vectors for all the expert CRF models are concatenated and provided as features for the target domain CRF training, along with the n-gram features.",2.4 CRF-BoE,[0],[0]
We built a dataset of 10 target domains for experimentation.,3.1 Target Domains,[0],[0]
Table 1 shows the list of domains as well as some statistics and example utterances.,3.1 Target Domains,[0],[0]
"We treated these as new domains - that is, we do not have real interaction data with users for these domains.",3.1 Target Domains,[0],[0]
"The annotated data is therefore prepared in two steps.
",3.1 Target Domains,[0],[0]
"First, utterances are obtained using crowdsourcing, where workers are provided with prompts for different intents of a domain and asked to generate
natural language utterances corresponding to those intents.",3.1 Target Domains,[0],[0]
"Next, the generated utterances are annotated by a different set of crowd workers, using the slot schema for each domain.",3.1 Target Domains,[0],[0]
"Inter-annotator agreement as well as manual inspection are used to ensure data quality in both stages.
",3.1 Target Domains,[0],[0]
The amount of data collected varies for each domain based on its complexity and business priority.,3.1 Target Domains,[0],[0]
Dataset size statistics for the data used in our experiments are presented in section 4.1.,3.1 Target Domains,[0],[0]
"Test and dev data are sampled at 10% of the total annotated data, with stratified sampling used in order to preserve the distribution of the intents.",3.1 Target Domains,[0],[0]
We experiment with two domains containing reusable slots: timex and location.,3.2 Reusable Domains,[0],[0]
"The timex domain consists of utterances containing the slots date, time and duration.",3.2 Reusable Domains,[0],[0]
"The location domain consists of utterances containing location, location type and place name slots.",3.2 Reusable Domains,[0],[0]
"Both of these types of slots appear in more than 20 of a set of 40 domains developed for use in our commercial personal assistant, making them ideal candidates for reuse.1
1Several other candidate reusable domains exist, including: the name domain containing the slot contact name; the number domain containing the slots rating, quantity and price; and the reference domain containing the slots ordinal (whose values include “first”, “second” or “third”) and order ref (with values such as “before” or “after”).",3.2 Reusable Domains,[0],[0]
"All of these slots appear in more than 25% of the available domains.
",3.2 Reusable Domains,[0],[0]
Data for these domains was sampled from the input utterances from our commercial digital assistant.,3.2 Reusable Domains,[0],[0]
Each reusable domain contains about a million utterances.,3.2 Reusable Domains,[0],[0]
There is no overlap between utterances in the target domains used for our experiments and utterances in the reusable domains.,3.2 Reusable Domains,[0],[0]
"The data for the reusable domains is sampled from other domains available to the digital assistant, not including our target domains.
",3.2 Reusable Domains,[0],[0]
Grouping the reusable slots into domains in this way provides additional opportunities for a commercial system: the trained reusable domain models can be used in other related products which need to identify time and location related entities.,3.2 Reusable Domains,[0],[0]
Models trained on the timex and location data have F1-scores of 96% and 89% respectively on test data from their respective domains.,3.2 Reusable Domains,[0],[0]
We want to verify if BoE models can improve slot tagging performance by using the information from reusable domains.,4.1 Experimental Setup,[0],[0]
"To simulate the low data scenario for the initial model training, we create three training datasets by sampling 2000, 1000 and 500 training examples from every domain.",4.1 Experimental Setup,[0],[0]
"We use stratified sampling to maintain the input distribution of the intents across the three training datasets.
",4.1 Experimental Setup,[0],[0]
"For each training dataset, we train the four models as described in Section 2 and compute the precision, recall and F1-score on the test data.",4.1 Experimental Setup,[0],[0]
Fixed seeds are used when training all models to make the results reproducible.,4.1 Experimental Setup,[0],[0]
"Table 3 summarizes these results, with only F1-scores reported to save space.",4.1 Experimental Setup,[0],[0]
We describe these results in Section 4.3.,4.1 Experimental Setup,[0],[0]
"Using the dev data set for the 10 domains, we experimented with using different pretrained embeddings, dropout probabilities and a CRF output layer in our LSTM architecture.",4.2 LSTM architecture variants,[0],[0]
The results are summarized in Table 2.,4.2 LSTM architecture variants,[0],[0]
"For each of the 10 domains, we trained using each variant with 10 different seeds, and computed the mean F1-score for each domain.",4.2 LSTM architecture variants,[0],[0]
"For comparing two variants, we computed the mean difference in the F1-scores over the 10 domains and its p-value.
",4.2 LSTM architecture variants,[0],[0]
"We tried word level Glove embeddings of 100, 200 and 300 dimensions as well as 500- dimensional word embeddings trained over the ut-
terances from our commercial PDA logs.",4.2 LSTM architecture variants,[0],[0]
"Both 100 and 200 dimensional Glove embeddings led to statistically significant improvements, but the word embeddings trained over our logs led to the biggest improvement.",4.2 LSTM architecture variants,[0],[0]
"We also tried using a CRF output layer (Lample et al., 2016) and different values of dropout keep probability, but none of them gave statistically significant improvements over the default model.",4.2 LSTM architecture variants,[0],[0]
"Based on this, we used PDA trained 500-dimensional word embeddings for our final experiments on test data.",4.2 LSTM architecture variants,[0],[0]
Table 3(a) shows the F1-scores obtained by the different methods for the training data set of 2000 training instances for each of the 10 domains.,4.3 Results and Discussion,[0],[0]
LSTM based models in general perform better than the CRF based models.,4.3 Results and Discussion,[0],[0]
The LSTM models have a statistically significant average improvement of 3.14 absolute F1-score over the CRF models.,4.3 Results and Discussion,[0],[0]
"The better performance of LSTM over CRF can be explained by the LSTM being able to use information over longer contexts to make predictions, while the CRF model is limited to at most the previous and next 3 words.
",4.3 Results and Discussion,[0],[0]
The results in Table 3(a) also show that both the CRF-BoE and LSTM-BoE outperform the basic CRF and LSTM models.,4.3 Results and Discussion,[0],[0]
LSTM-BoE has a statistically significant mean improvement of 1.92 points over LSTM.,4.3 Results and Discussion,[0],[0]
"CRF-BoE also shows an average improvement of 2.19 points over the CRF model, but the results are not statistically significant.",4.3 Results and Discussion,[0],[0]
"Looking at results for individual domains, the highest improvement for BoE models are seen for transportation and travel.",4.3 Results and Discussion,[0],[0]
"This can be explained by these domains having a high frequency of timex and location slots, as shown in Table 4.
",4.3 Results and Discussion,[0],[0]
"The shopping model shows a regression for BoE models, and a reason could be the low frequency of expert slots (Table 4).",4.3 Results and Discussion,[0],[0]
"However, low frequency of expert slots does not always mean that BoE methods can’t help, as shown by the improvement in the purchase domain.",4.3 Results and Discussion,[0],[0]
"Finally, for sports, social network and deals domains, the LSTM-BoE improves over LSTM, while CRFBoE does not improve over CRF.",4.3 Results and Discussion,[0],[0]
"Our hypothesis is that given the query patterns for these domains, the dense vector output used by LSTM-BoE is able to transfer some information, while the categorical label output used by CRF-BoE is not.
",4.3 Results and Discussion,[0],[0]
"Table 3(b) shows the results with 500 and 1000
training data instances.",4.3 Results and Discussion,[0],[0]
Note that the improvements are even higher for the experiments with smaller training data.,4.3 Results and Discussion,[0],[0]
"In particular, LSTM-BoE shows an improvement of 4.63 in absolute F1score over LSTM when training with 500 instances.",4.3 Results and Discussion,[0],[0]
"Thus, as we reduce the amount of training data in the target domain, the performance improvement from BoE models is even higher.
",4.3 Results and Discussion,[0],[0]
"As an example, in the purchase domain, the LSTM-BoE model achieves an F1-score of 70.66% with only 500 training instances, while even with 2000 training instances the CRF model achieves an F1-score of only 66.24%.",4.3 Results and Discussion,[0],[0]
Thus the LSTM-BoE model achieves better F1-score with only one-fourth the training data.,4.3 Results and Discussion,[0],[0]
"Similarly, for flight status, travel, and transportation domains, the LSTM-BoE model gets better performance with 500 training instances, compared to a CRF model with 2000 training instances.",4.3 Results and Discussion,[0],[0]
"The LSTMBoE architecture, therefore, allows us to reuse the domain experts to produce better performing mod-
els with much lower data annotation costs.",4.3 Results and Discussion,[0],[0]
"As the target domain training data increases, the contribution due to domain experts goes down, but more experimentation is needed to establish the threshold at which it is no longer useful to add experts.",4.3 Results and Discussion,[0],[0]
"Early methods for slot-tagging used rule-based approaches (Ward and Issar, 1994).",5 Related Work,[0],[0]
"Much of the later work on supervised learning focused on CRFs, for example (Sarikaya et al., 2016), or neural networks (Deoras and Sarikaya, 2013; Yao et al., 2013; Liu et al., 2015; Celikyilmaz and HakkaniTur, 2015).",5 Related Work,[0],[0]
"Unsupervised (or weakly-supervised) methods also were used for NLU tasks, primarily leveraging search query click logs (Hakkani-Tur et al., 2011a,b, 2013) and knowledge graphs (Tur et al., 2012; Heck and Hakkani-Tur, 2012; Heck et al., 2013); hybrid methods, for example as described in (Kim et al., 2015a; Celikyilmaz et al., 2015; Chen et al., 2016), also exist.",5 Related Work,[0],[0]
"Our approach
in this paper is a purely supervised one.",5 Related Work,[0],[0]
"Transfer learning is a vast area of research, with too many publications for an exhaustive list.",5 Related Work,[0],[0]
We discuss some of the recent work most relevant to our methods.,5 Related Work,[0],[0]
"In (Kim et al., 2015b), the slot labels from across different domains are mapped into a shared space using Canonical Correlation Analysis (CCA) and automatically-induced embeddings over the label space.",5 Related Work,[0],[0]
"These label representations allow mapping of label types between different domains, which makes it possible to apply standard data-driven domain adaptation approaches (Daume, 2007).",5 Related Work,[0],[0]
"They also introduce a model-driven adaptation technique based on training a hidden unit CRF (HUCRF) on the source domain, which is then used to initialize the training for the target domain.",5 Related Work,[0],[0]
"The limitation of this approach is that only one source domain can be used, while multiple experts can be used in the proposed BoE approach.
",5 Related Work,[0],[0]
"(Kim et al., 2016a) build a single, universal slot tagging model, and constrain the decoding process to subsets of slots for various domains; this process assumes that a mapping of slot tags in the new domain to the ones in the universal slot model has already been generated.",5 Related Work,[0],[0]
"A related work by (Kim et al., 2016b) directly predicts the required schema prior to performing the constrained decoding.",5 Related Work,[0],[0]
"These approaches are attractive because only one universal model needs to be trained, but do not work in cases when a new domain contains a mixture of new and existing slots.",5 Related Work,[0],[0]
"Our approach allows transfer of partial knowledge in such cases.
",5 Related Work,[0],[0]
"(Kim et al., 2016c) uses a neural version of the approach first described in (Daume, 2007), by using existing annotated data in a variety of domains
to adapt the slot tag models of new domains where the tag space is partly shared.",5 Related Work,[0],[0]
"The drawback of such data-driven domain adaptation is the increase in training time as more experts are added.
",5 Related Work,[0],[0]
"An expert-based adaptation, similar to the techniques applied in this paper, was first described in (Kim et al., 2017b).",5 Related Work,[0],[0]
"(Jaech et al., 2016) use multitask learning, training a bidirectional LSTM with character-level embeddings, trained jointly to produce slot tags for a number of travel-related domains.",5 Related Work,[0],[0]
"Finally, (Kim et al., 2017a) frame the problem of temporal shift in data of a single domain (and the related problem of bootstrapping a new domain with imperfectly-matched synthetic data) as one of domain adaptation, applying adversarial training approaches.
",5 Related Work,[0],[0]
A number of researchers also investigated bootstrapping NLU systems using zero-shot learning.,5 Related Work,[0],[0]
"(Dauphin et al., 2014; Kumar et al., 2017) both investigated domain classification; most relevant to us is the work by (Bapna et al., 2017), who studied full semantic frame tagging using zero-shot learning, by projecting the tags into a shared embedding space, similar to work done by (Kim et al., 2015b).",5 Related Work,[0],[0]
We experimented with Bag of Experts (BoE) architectures for CRF and LSTM based slot tagging models.,6 Conclusion,[0],[0]
Our experimental results over a set of 10 domains show that BoE architectures are able to use the information from reusable expert models to perform significantly better than their nonexpert counterparts.,6 Conclusion,[0],[0]
"In particular, the LSTM-BoE model shows a statistically significant improvement of 1.92% over the LSTM model on average when training with 2000 instances.",6 Conclusion,[0],[0]
"When training with 500 instances, the improvement of LSTM-BoE model over LSTM is even higher at 4.63%.",6 Conclusion,[0],[0]
"For multiple domains, an LSTM-BoE model trained on only 500 instances is able to outperform a baseline CRF model trained over 4 times the data.",6 Conclusion,[0],[0]
"Thus, the BoE approach produces high performing models for slot tagging at much lower annotation costs.",6 Conclusion,[0],[0]
We would like to thank Ahmed El Kholy for his comments and feedback on an earlier version of this paper.,Acknowledgments,[0],[0]
"Also, thanks to Kyle Williams and Zhaleh Feizollahi for their help with code and data collection.",Acknowledgments,[0],[0]
"Slot tagging, the task of detecting entities in input user utterances, is a key component of natural language understanding systems for personal digital assistants.",abstractText,[0],[0]
"Since each new domain requires a different set of slots, the annotation costs for labeling data for training slot tagging models increases rapidly as the number of domains grow.",abstractText,[0],[0]
"To tackle this, we describe Bag of Experts (BoE) architectures for model reuse for both LSTM and CRF based models.",abstractText,[0],[0]
"Extensive experimentation over a dataset of 10 domains drawn from data relevant to our commercial personal digital assistant shows that our BoE models outperform the baseline models with a statistically significant average margin of 5.06% in absolute F1score when training with 2000 instances per domain, and achieve an even higher improvement of 12.16% when only 25% of the training data is used.",abstractText,[0],[0]
Bag of Experts Architectures for Model Reuse in Conversational Language Understanding,title,[0],[0]
The stochastic multi-armed bandit (MAB) problem is a prominent framework for capturing the explorationexploitation tradeoff in online decision making and experiment design.,1. Introduction,[0],[0]
"The MAB problem proceeds in discrete sequential rounds, where in each round, the player pulls one
1 Department of Mathematics and Statistics, Lancaster University, Lancaster, UK 2 Department of Industrial Engineering and Operations Research, Columbia University, New York, NY, USA 3 DeepMind, London, UK 4 Department of Computing Science, University of Alberta, Edmonton, AB, Canada.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Ciara Pike-Burke <ciara.pikeburke@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
of the K possible arms.",1. Introduction,[0],[0]
"In the classic stochastic MAB setting, the player immediately observes stochastic feedback from the pulled arm in the form of a ‘reward’ which can be used to improve the decisions in subsequent rounds.",1. Introduction,[0],[0]
One of the main application areas of MABs is in online advertising.,1. Introduction,[0],[0]
"Here, the arms correspond to adverts, and the feedback would correspond to conversions, that is users buying a product after seeing an advert.",1. Introduction,[0],[0]
"However, in practice, these conversions may not necessarily happen immediately after the advert is shown, and it may not always be possible to assign the credit of a sale to a particular showing of an advert.",1. Introduction,[0],[0]
"A similar challenge is encountered in many other applications, e.g., in personalized treatment planning, where the effect of a treatment on a patient’s health may be delayed, and it may be difficult to determine which out of several past treatments caused the change in the patient’s health; or, in content design applications, where the effects of multiple changes in the website design on website traffic and footfall may be delayed and difficult to distinguish.
",1. Introduction,[0],[0]
"In this paper, we propose a new bandit model to handle online problems with such ‘delayed, aggregated and anonymous’ feedback.",1. Introduction,[0],[0]
"In our model, a player interacts with an environment ofK actions (or arms) in a sequential fashion.",1. Introduction,[0],[0]
At each time step the player selects an action which leads to a reward generated at random from the underlying reward distribution.,1. Introduction,[0],[0]
"At the same time, a nonnegative random integer-valued delay is also generated i.i.d.",1. Introduction,[0],[0]
from an underlying delay distribution.,1. Introduction,[0],[0]
Denoting this delay by τ ≥ 0,1. Introduction,[0],[0]
"and the index of the current round by t, the reward generated in round t will arrive at the end of the (t + τ)th round.",1. Introduction,[0],[0]
"At the end of each round, the player observes only the sum of all the rewards that arrive in that round.",1. Introduction,[0],[0]
"Crucially, the player does not know which of the past plays have contributed to this aggregated reward.",1. Introduction,[0],[0]
"We call this problem multi-armed bandits with delayed, aggregated anonymous feedback (MABDAAF).",1. Introduction,[0],[0]
"As in the standard MAB problem, in MABDAAF, the goal is to maximize the cumulative reward from T plays of the bandit, or equivalently to minimize the regret.",1. Introduction,[0],[0]
"The regret is the total difference between the reward of the optimal action and the actions taken.
",1. Introduction,[0],[0]
"If the delays are all zero, the MABDAAF problem reduces to the standard (stochastic) MAB problem, which has been studied considerably (e.g., Thompson, 1933; Lai & Robbins, 1985; Auer et al., 2002; Bubeck & Cesa-Bianchi,
2012).",1. Introduction,[0],[0]
"Compared to the MAB problem, the job of the player in our problem appears to be significantly more difficult since the player has to deal with (i) that some feedback from the previous pulls may be missing due to the delays, and (ii) that the feedback takes the form of the sum of an unknown number of rewards of unknown origin.
",1. Introduction,[0],[0]
"An easier problem is when the observations are delayed, but they are non-aggregated and non-anonymous: that is, the player has to only deal with challenge (i) and not (ii).",1. Introduction,[0],[0]
"Here, the player receives delayed feedback in the shape of action-reward pairs that inform the player of both the individual reward and which action generated it.",1. Introduction,[0],[0]
"This problem, which we shall call the (non-anonymous) delayed feedback bandit problem, has been studied by Joulani et al. (2013), and later followed up by Mandel et al. (2015) (for bounded delays).",1. Introduction,[0],[0]
"Remarkably, they show that compared to the standard (non-delayed) stochastic MAB setting, the regret will increase only additively by a factor that scales with the expected delay.",1. Introduction,[0],[0]
"For delay distributions with a finite expected delay, E[τ ], the worst case regret scales with O( √ KT log T + KE[τ ]).",1. Introduction,[0],[0]
"Hence, the price to pay for the delay in receiving the observations is negligible.",1. Introduction,[0],[0]
"QPM-D of Joulani et al. (2013) and SBD of Mandel et al. (2015) place received rewards into queues for each arm, taking one whenever a base bandit algorithm suggests playing the arm.",1. Introduction,[0],[0]
"Throughout, we take UCB1 (Auer et al., 2002) as the base algorithm in QPM-D. Joulani et al. (2013) also present a direct modification of the UCB1 algorithm.",1. Introduction,[0],[0]
All of these algorithms achieve the stated regret.,1. Introduction,[0],[0]
"None of them require any knowledge of the delay distributions, but they all rely heavily upon the non-anonymous nature of the observations.
",1. Introduction,[0],[0]
"While these results are encouraging, the assumption that the rewards are observed individually in a non-anonymous fashion is limiting for most practical applications with delays (e.g., recall the applications discussed earlier).",1. Introduction,[0],[0]
How big is the price to be paid for receiving only aggregated anonymous feedback?,1. Introduction,[0],[0]
Our main result is to prove that essentially there is no extra price to be paid provided that the value of the expected delay (or a bound on it) is available.,1. Introduction,[0],[0]
"In particular, this means that detailed knowledge of which action led to a particular delayed reward can be replaced by the much weaker requirement that the expected delay, or a bound on it, is known.",1. Introduction,[0],[0]
"Fig. 1 summarizes the relationship between the non-delayed, the delayed and the new problem
by showing the leading terms of the regret.",1. Introduction,[0],[0]
"In all cases, the dominant term is √ KT .",1. Introduction,[0],[0]
"Hence, asymptotically, the delayed, aggregated anonymous feedback problem is no more difficult than the standard multi-armed bandit problem.",1. Introduction,[0],[0]
We now consider what sort of algorithm will be able to achieve the aforementioned results for the MABDAAF problem.,1.1. Our Techniques and Results,[0],[0]
"Since the player only observes delayed, aggregated anonymous rewards, the first problem we face is how to even estimate the mean reward of individual actions.",1.1. Our Techniques and Results,[0],[0]
"Due to the delays and anonymity, it appears that to be able to estimate the mean reward of an action, the player wants to have played it consecutively for long stretches.",1.1. Our Techniques and Results,[0],[0]
"Indeed, if the stretches are sufficiently long compared to the mean delay, the observations received during the stretch will mostly consist of rewards of the action played in that stretch.",1.1. Our Techniques and Results,[0],[0]
"This naturally leads to considering algorithms that switch actions rarely and this is indeed the basis of our approach.
",1.1. Our Techniques and Results,[0],[0]
"Several popular MAB algorithms are based on choosing the action with the largest upper confidence bound (UCB) in each round (e.g., Auer et al., 2002; Cappé et al., 2013).",1.1. Our Techniques and Results,[0],[0]
UCB-style algorithms tend to switch arms frequently and will only play the optimal arm for long stretches if a unique optimal arm exists.,1.1. Our Techniques and Results,[0],[0]
"Therefore, for MABDAAF, we will consider alternative algorithms where arm-switching is more tightly controlled.",1.1. Our Techniques and Results,[0],[0]
The design of such algorithms goes back at least to the work of Agrawal et al. (1988) where the problem of bandits with switching costs was studied.,1.1. Our Techniques and Results,[0],[0]
The general idea of these rarely switching algorithms is to gradually eliminate suboptimal arms by playing arms in phases and comparing each arm’s upper confidence bound to the lower confidence bound of a leading arm at the end of each phase.,1.1. Our Techniques and Results,[0],[0]
"Generally, this sort of rarely switching algorithm switches arms onlyO(log T ) times.",1.1. Our Techniques and Results,[0],[0]
"We base our approach on one such algorithm, the so-called Improved UCB1 algorithm of Auer & Ortner (2010).
",1.1. Our Techniques and Results,[0],[0]
Using a rarely switching algorithm alone will not be sufficient for MABDAAF.,1.1. Our Techniques and Results,[0],[0]
"The remaining problem, and where the bulk of our contribution lies, is to construct appropri-
1The adjective “Improved” indicates that the algorithm improves upon the regret bounds achieved by UCB1.",1.1. Our Techniques and Results,[0],[0]
The improvement replaces log(T )/∆j,1.1. Our Techniques and Results,[0],[0]
by log(T∆2j )/∆j,1.1. Our Techniques and Results,[0],[0]
"in the regret bound.
ate confidence bounds and adjust the length of the periods of playing each arm to account for the delayed, aggregated anonymous feedback.",1.1. Our Techniques and Results,[0],[0]
"In particular, in the confidence bounds attention must be paid to fine details: it turns out that unless the variance of the observations is dealt with, there is a blow-up by a multiplicative factor of K. We avoid this by an improved analysis involving Freedman’s inequality (Freedman, 1975).",1.1. Our Techniques and Results,[0],[0]
"Further, to handle the dependencies between the number of plays of each arm and the past rewards, we combine Doob’s optimal skipping theorem (Doob, 1953) and Azuma-Hoeffding inequalities.",1.1. Our Techniques and Results,[0],[0]
Using a rarely switching algorithm for MABDAAF means we must also consider the dependencies between the elimination of arms in one phase and the corruption of observations in the next phase (ie. past plays can influence both whether an arm is still active and the corruption of its next plays).,1.1. Our Techniques and Results,[0],[0]
"We deal with this through careful algorithmic design.
",1.1. Our Techniques and Results,[0],[0]
"Using the above, we provide an algorithm that achieves worst case regret of O( √ KT logK + KE[τ ] log T ) using only knowledge of the expected delay, E[τ ].",1.1. Our Techniques and Results,[0],[0]
We then show that this regret can be improved by using a more careful martingale argument that exploits the fact that our algorithm is designed to remove most of the dependence between the corruption of future observations and elimination of arms.,1.1. Our Techniques and Results,[0],[0]
"Particularly, if the delays are bounded with known bound 0 ≤",1.1. Our Techniques and Results,[0],[0]
"d ≤ √ T/K, we can recover worst case regret ofO( √ KT logK+KE[τ ]), matching that of Joulani et al. (2013).",1.1. Our Techniques and Results,[0],[0]
"If the delays are unbounded but have known variance V(τ), we show that the problem independent regret can be reduced to O( √ KT logK +KE[τ ] +KV(τ)).",1.1. Our Techniques and Results,[0],[0]
We have already discussed several of the most relevant works to our own.,1.2. Related Work,[0],[0]
"However, there has also been other work looking at different flavors of the bandit problem with delayed (non-anonymous) feedback.",1.2. Related Work,[0],[0]
"For example, Neu et al. (2010) and Cesa-Bianchi et al. (2016) consider nonstochastic bandits with fixed constant delays; Dudik et al. (2011) look at stochastic contextual bandits with a constant delay and Desautels et al. (2014) consider Gaussian Process bandits with a bounded stochastic delay.",1.2. Related Work,[0],[0]
The general observation that delay causes an additive regret penalty in stochastic bandits and a multiplicative one in adversarial bandits is made in Joulani et al. (2013).,1.2. Related Work,[0],[0]
The empirical performance of K-armed stochastic bandit algorithms in delayed settings was investigated in Chapelle & Li (2011).,1.2. Related Work,[0],[0]
A further related problem is the ‘batched bandit’ problem studied by Perchet et al. (2016).,1.2. Related Work,[0],[0]
Here the player must fix a set of time points at which to collect feedback on all plays leading up to that point.,1.2. Related Work,[0],[0]
"Vernade et al. (2017) consider delayed Bernoulli bandits where some observations could also be censored (e.g., no conversion is ever actually observed if the delay exceeds some threshold) but require
complete knowledge of the delay distribution.",1.2. Related Work,[0],[0]
"Crucially, here and in all the aforementioned works, the feedback is always assumed to take the form of arm-reward pairs and knowledge of the assignment of rewards to arms underpins the suggested algorithms, rendering them unsuitable for MABDAAF.",1.2. Related Work,[0],[0]
"To the best of our knowledge, ours is the first work to develop algorithms to deal with delayed, aggregated anonymous feedback in the bandit setting.",1.2. Related Work,[0],[0]
The reminder of this paper is organized as follows: In the next section (Section 2) we give the formal problem definition.,1.3. Organization,[0],[0]
We present our algorithm in Section 3.,1.3. Organization,[0],[0]
"In Section 4, we discuss the performance of our algorithm under various delay assumptions; known expectation, bounded support with known bound and expectation, and known variance and expectation.",1.3. Organization,[0],[0]
This is followed by a numerical illustration of our results in Section 5.,1.3. Organization,[0],[0]
We conclude in Section 6.,1.3. Organization,[0],[0]
There are K > 1 actions or arms in the set A. Each action j ∈,2. Problem Definition,[0],[0]
A is associated with a reward distribution ζj and a delay distribution δj .,2. Problem Definition,[0],[0]
"The reward distribution is supported in [0, 1] and the delay distribution is supported on N .=",2. Problem Definition,[0],[0]
"{0, 1, . . .",2. Problem Definition,[0],[0]
}.,2. Problem Definition,[0],[0]
"We denote by µj the mean of ζj , µ∗ = µj∗ = maxj µj and define ∆j = µ∗ − µj to be the reward gap, that is the expected loss of reward each time action j is chosen instead of an optimal action.",2. Problem Definition,[0],[0]
"Let (Rl,j , τl,j)l∈N,j∈A be an infinite array of random variables defined on the probability space (Ω,Σ, P ) which are mutually independent.",2. Problem Definition,[0],[0]
"Further, Rl,j follows the distribution ζj and τl,j follows the distribution δj .",2. Problem Definition,[0],[0]
"The meaning of these random variables is that if the player plays action j at time l, a payoff of Rl,j will be added to the aggregated feedback that the player receives at the end of the (l + τl,j)th play.",2. Problem Definition,[0],[0]
"Formally, if Jl ∈ A denotes the action chosen by the player at time l = 1, 2, . . .",2. Problem Definition,[0],[0]
", then the observation received at the end of the tth play is
Xt = t∑ l=1",2. Problem Definition,[0],[0]
K∑ j=1,2. Problem Definition,[0],[0]
"Rl,j × I{l + τl,j = t, Jl = j}.
",2. Problem Definition,[0],[0]
"For the remainder, we will consider i.i.d. delays across arms.",2. Problem Definition,[0],[0]
"We also assume discrete delay distributions, although most results hold for continuous delays by redefining the event {τl,j = t− l} as {t− l− 1 < τl,j ≤ t− l} in Xt.",2. Problem Definition,[0],[0]
"In our analysis, we will sum over stochastic index sets.",2. Problem Definition,[0],[0]
For a stochastic index set I and random variables {Zn}n∈N we denote such sums as ∑ t∈I Zt .,2. Problem Definition,[0],[0]
= ∑ t∈N I{t ∈,2. Problem Definition,[0],[0]
"I} × Zt.
",2. Problem Definition,[0],[0]
Regret definition,2. Problem Definition,[0],[0]
"In most bandit problems, the regret is the cumulative loss due to not playing an optimal action.
",2. Problem Definition,[0],[0]
"In the case of delayed feedback, there are several possible ways to define the regret.",2. Problem Definition,[0],[0]
One option is to consider only the loss of the rewards received before horizon T (as in Vernade et al. (2017)).,2. Problem Definition,[0],[0]
"However, we will not use this definition.",2. Problem Definition,[0],[0]
"Instead, as in Joulani et al. (2013), we consider the loss of all generated rewards and define the (pseudo-)regret by
RT = T∑ t=1",2. Problem Definition,[0],[0]
(µ∗ − µJt) = Tµ∗,2. Problem Definition,[0],[0]
"− T∑ t=1 µJt .
",2. Problem Definition,[0],[0]
This includes the rewards received after the horizon T and does not penalize large delays as long as an optimal action is taken.,2. Problem Definition,[0],[0]
"This definition is natural since, in practice, the player should eventually receive all outstanding reward.
",2. Problem Definition,[0],[0]
"Lai & Robbins (1985) showed that the regret of any algorithm for the standard MAB problem must satisfy,
lim inf T→∞",2. Problem Definition,[0],[0]
"E[RT ] log(T )
",2. Problem Definition,[0],[0]
"≥ ∑
j:∆j>0
∆j",2. Problem Definition,[0],[0]
"KL(ζj , ζ∗) , (1)
where KL(ζj , ζ∗) is the KL-divergence between the reward distributions of arm j and an optimal arm.",2. Problem Definition,[0],[0]
Theorem 4 of Vernade et al. (2017) shows that the lower bound in (1) also holds for delayed feedback bandits with no censoring and their alternative definition of regret.,2. Problem Definition,[0],[0]
We therefore suspect (1) should hold for MABDAAF.,2. Problem Definition,[0],[0]
"However, due to the specific problem structure, finding a lower bound for MABDAAF is non-trivial and remains an open problem.
",2. Problem Definition,[0],[0]
"Assumptions on delay distribution For our algorithm for MABDAAF, we need some assumptions on the delay distribution.",2. Problem Definition,[0],[0]
"We assume that the expected delay, E[τ ], is bounded and known.",2. Problem Definition,[0],[0]
"This quantity is used in the algorithm.
",2. Problem Definition,[0],[0]
Assumption 1,2. Problem Definition,[0],[0]
"The expected delay E[τ ] is bounded and known to the algorithm.
",2. Problem Definition,[0],[0]
"We then show that under some further mild assumptions on the delay, we can obtain better algorithms with even more efficient regret guarantees.",2. Problem Definition,[0],[0]
"We consider two settings: delay distributions with bounded support, and bounded variance.
",2. Problem Definition,[0],[0]
Assumption 2 (Bounded support),2. Problem Definition,[0],[0]
There exists some constant d > 0 known to the algorithm,2. Problem Definition,[0],[0]
"such that the support of the delay distribution is bounded by d.
Assumption 3 (Bounded variance)",2. Problem Definition,[0],[0]
"The variance, V(τ), of the delay is bounded and known to the algorithm.
",2. Problem Definition,[0],[0]
In fact the known expected value and known variance assumption can be replaced by a ‘known upper bound’ on the expected value and variance respectively.,2. Problem Definition,[0],[0]
"However, for simplicity, in the remaining, we use E[τ ] and V(τ) directly.",2. Problem Definition,[0],[0]
The next sections provide algorithms and regret analysis for different combinations of the above assumptions.,2. Problem Definition,[0],[0]
Our algorithm is a phase-based elimination algorithm based on the Improved UCB algorithm by Auer & Ortner (2010).,3. Our Algorithm,[0],[0]
The general structure is as follows.,3. Our Algorithm,[0],[0]
"In each phase, each arm is played multiple times consecutively.",3. Our Algorithm,[0],[0]
"At the end of the phase, the observations received are used to update mean estimates, and any arm with an estimated mean below the best estimated mean by a gap larger than a ‘separation gap tolerance’ is eliminated.",3. Our Algorithm,[0],[0]
"This separation tolerance is decreased exponentially over phases, so that it is very small in later phases, eliminating all but the best arm(s) with high probability.",3. Our Algorithm,[0],[0]
"An alternative formulation of the algorithm is that at the end of a phase, any arm with an upper confidence bound lower than the best lower confidence bound is eliminated.",3. Our Algorithm,[0],[0]
"These confidence bounds are computed so that with high probability they are more (less) than the true mean, but within the separation gap tolerance.",3. Our Algorithm,[0],[0]
The phase lengths are then carefully chosen to ensure that the confidence bounds hold.,3. Our Algorithm,[0],[0]
"Here we assume that the horizon T is known, but we expect that this can be relaxed as in Auer & Ortner (2010).
",3. Our Algorithm,[0],[0]
"Algorithm overview Our algorithm, ODAAF, is given in Algorithm 1.",3. Our Algorithm,[0],[0]
"It operates in phases m = 1, 2, . .",3. Our Algorithm,[0],[0]
..,3. Our Algorithm,[0],[0]
"Define Am to be the set of active arms in phase m. The algorithm takes parameter nm which defines the number of samples of each active arm required by the end of phase m.
In Step 1 of phase m of the algorithm, each active arm j is played repeatedly for nm − nm−1 steps.",3. Our Algorithm,[0],[0]
We record all timesteps where arm j was played in the first m phases (excluding bridge periods) in the set Tj(m).,3. Our Algorithm,[0],[0]
The active arms are played in any arbitrary but fixed order.,3. Our Algorithm,[0],[0]
"In Step 2, the nm observations from timesteps in Tj(m) are averaged to obtain a new estimate X̄m,j of µj .",3. Our Algorithm,[0],[0]
"Arm j is eliminated if X̄m,j is further than ∆̃m from maxj′∈Am X̄m,j′ .
",3. Our Algorithm,[0],[0]
A further nuance in the algorithm structure is the ‘bridge period’ (see Figure 2).,3. Our Algorithm,[0],[0]
The algorithm picks an active arm j ∈ Am+1 to play in this bridge period for nm − nm−1 steps.,3. Our Algorithm,[0],[0]
"The observations received during the bridge period are discarded, and not used for computing confidence intervals.",3. Our Algorithm,[0],[0]
The significance of the bridge period is that it breaks the dependence between confidence intervals calculated in phasem and the delayed payoffs seeping into phasem+1.,3. Our Algorithm,[0],[0]
Without the bridge period this dependence would impair the validity of our confidence intervals.,3. Our Algorithm,[0],[0]
"However, we suspect that, in practice, it may be possible to remove it.
",3. Our Algorithm,[0],[0]
Choice of nm A key element of our algorithm design is the careful choice of nm.,3. Our Algorithm,[0],[0]
"Since nm determines the number of times each active (possibly suboptimal) arm is played, it clearly has an impact on the regret.",3. Our Algorithm,[0],[0]
"Furthermore, nm needs to be chosen so that the confidence bounds on the estimation error hold with given probability.",3. Our Algorithm,[0],[0]
"The main chal-
Algorithm 1 Optimism for Delayed, Aggregated Anonymous Feedback (ODAAF)",3. Our Algorithm,[0],[0]
"Input: A set of arms, A; a horizon, T ; choice of nm for
each phase m = 1, 2, . .",3. Our Algorithm,[0],[0]
..,3. Our Algorithm,[0],[0]
"Initialization: Set ∆̃1 = 1/2 (tolerance), the set of active
arms A1 = A. Let Ti(1) =",3. Our Algorithm,[0],[0]
"∅, i ∈",3. Our Algorithm,[0],[0]
"A, m = 1 (phase index), t = 1 (round index) while t ≤ T do
Step 1: Play arms.",3. Our Algorithm,[0],[0]
"for j ∈ Am do
Let Tj(m) =",3. Our Algorithm,[0],[0]
"Tj(m− 1) while |Tj(m)| ≤ nm and t ≤ T do
Play arm j, receive Xt.",3. Our Algorithm,[0],[0]
Add t to Tj(m).,3. Our Algorithm,[0],[0]
Increment t by 1. end while end for Step 2: Eliminate sub-optimal arms.,3. Our Algorithm,[0],[0]
"For every arm in j ∈ Am, compute X̄m,j as the average of observations at time steps t ∈ Tj(m).",3. Our Algorithm,[0],[0]
"That is,
X̄m,j = 1 |Tj(m)| ∑
t∈Tj(m)
",3. Our Algorithm,[0],[0]
"Xt .
",3. Our Algorithm,[0],[0]
"Construct Am+1 by eliminating actions j ∈ Am with
X̄m,j +",3. Our Algorithm,[0],[0]
"∆̃m < max j′∈Am X̄m,j′ .
",3. Our Algorithm,[0],[0]
Step 3:,3. Our Algorithm,[0],[0]
"Decrease Tolerance.
",3. Our Algorithm,[0],[0]
Set ∆̃m+1 = ∆̃m2 .,3. Our Algorithm,[0],[0]
Step 4: Bridge period.,3. Our Algorithm,[0],[0]
Pick an arm j ∈ Am+1 and play it νm = nm − nm−1 times while incrementing t ≤ T .,3. Our Algorithm,[0],[0]
Discard all observations from this period.,3. Our Algorithm,[0],[0]
Do not add t to Tj(m).,3. Our Algorithm,[0],[0]
"Increment phase index m.
end while
lenge is developing these confidence bounds from delayed, aggregated anonymous feedback.",3. Our Algorithm,[0],[0]
"Handling this form of feedback involves a credit assignment problem of deciding which samples can be used for a given arm’s mean estimation, since each sample is an aggregate of rewards from multiple previously played arms.",3. Our Algorithm,[0],[0]
This credit assignment problem would be hopeless in a passive learning setting without further information on how the samples were generated.,3. Our Algorithm,[0],[0]
"Our algorithm utilizes the power of active learning to design the phases in such a way that the feedback can be effectively ‘decensored’ without losing too many samples.
",3. Our Algorithm,[0],[0]
"A naive approach to defining the confidence bounds for delays bounded by a constant d ≥ 0 would be to observe that,∣∣∣∣ ∑
t∈Tj(m)\Tj(m−1)
",3. Our Algorithm,[0],[0]
Xt,3. Our Algorithm,[0],[0]
"− ∑
t∈Tj(m)\Tj(m−1)
Rt,j ∣∣∣∣ ≤ d,
since all rewards are in [0, 1].",3. Our Algorithm,[0],[0]
"Then we could use Hoeffding’s inequality to boundRt,Jt (see Appendix F) and select
nm = C1 log(T ∆̃
2 m)
∆̃2m + C2md ∆̃m
for some constants C1, C2.",3. Our Algorithm,[0],[0]
This corresponds to worst case regret of O( √ KT logK + K log(T )d).,3. Our Algorithm,[0],[0]
"For d E[τ ] and large T , this is significantly worse than that of Joulani et al. (2013).",3. Our Algorithm,[0],[0]
"In Section 4, we show that, surprisingly, it is possible to recover the same rate of regret as Joulani et al. (2013), but this requires a significantly more nuanced argument to get tighter confidence bounds and smaller nm.",3. Our Algorithm,[0],[0]
"In the next section, we describe this improved choice of nm for every phase m ∈ N and its implications on the regret, for each of the three cases mentioned previously: (i) Known and bounded expected delay (Assumption 1), (ii) Bounded delay with known bound and expected value (Assumptions 1 and 2), (iii) Delay with known and bounded variance and expectation (Assumptions 1 and 3).",3. Our Algorithm,[0],[0]
"In this section, we specify the choice of parameters nm and provide regret guarantees for Algorithm 1 for each of the three previously mentioned cases.",4. Regret Analysis,[0],[0]
"First, we consider the setting with the weakest assumption on delay distribution: we only assume that the expected delay, E[τ ], is bounded and known.",4.1. Known and Bounded Expected Delay,[0],[0]
No assumption on the support or variance of the delay distribution is made.,4.1. Known and Bounded Expected Delay,[0],[0]
"The regret analysis for this setting will not use the bridge period, so Step 4 of the algorithm could be omitted in this case.
",4.1. Known and Bounded Expected Delay,[0],[0]
"Choice of nm Here, we use Algorithm 1 with
nm = C1 log(T ∆̃
2 m) ∆̃2m + C2mE[τ ] ∆̃m (2)
for some large enough constants C1, C2.",4.1. Known and Bounded Expected Delay,[0],[0]
"The exact value of nm is given in Equation (14) in Appendix B.
Estimation of error bounds We bound the error between X̄m,j and µj by ∆̃m/2.",4.1. Known and Bounded Expected Delay,[0],[0]
"In order to do this we first bound the corruption of the observations received during timesteps Tj(m) due to delays.
",4.1. Known and Bounded Expected Delay,[0],[0]
Fix a phase m and arm j ∈ Am.,4.1. Known and Bounded Expected Delay,[0],[0]
Then the observations Xt in the period t ∈ Tj(m),4.1. Known and Bounded Expected Delay,[0],[0]
"\ Tj(m− 1) are composed of two types of rewards: a subset of rewards from plays of arm j in this period, and delayed rewards from some of the plays before this period.",4.1. Known and Bounded Expected Delay,[0],[0]
The expected value of observations from this period would be (nm − nm−1)µj but for the rewards entering and leaving this period due to delay.,4.1. Known and Bounded Expected Delay,[0],[0]
"Since the reward is bounded by 1, a simple observation is that expected discrepancy between the sum of observations in this period and the quantity (nm − nm−1)µj is bounded by the expected delay E[τ",4.1. Known and Bounded Expected Delay,[0],[0]
"],
E  ∑ t∈Tj(m)\Tj(m−1)",4.1. Known and Bounded Expected Delay,[0],[0]
(Xt − µj)  ≤ E[τ ].,4.1. Known and Bounded Expected Delay,[0],[0]
"(3) Summing this over phases ` = 1, . .",4.1. Known and Bounded Expected Delay,[0],[0]
".m gives a bound
|E[X̄m,j ]",4.1. Known and Bounded Expected Delay,[0],[0]
− µj | ≤ mE[τ ] |Tj(m)| = mE[τ ] nm .,4.1. Known and Bounded Expected Delay,[0],[0]
"(4)
Note that given the choice of nm in (2), the above is smaller than ∆̃m/2, when large enough constants are used.",4.1. Known and Bounded Expected Delay,[0],[0]
"Using this, along with concentration inequalities and the choice of nm from (2), we can obtain the following high probability bound.",4.1. Known and Bounded Expected Delay,[0],[0]
"A detailed proof is provided in Appendix B.1.
",4.1. Known and Bounded Expected Delay,[0],[0]
Lemma 1,4.1. Known and Bounded Expected Delay,[0],[0]
"Under Assumption 1 and the choice of nm given by (2), the estimates X̄m,j constructed by Algorithm 1 satisfy the following: For every fixed arm j and phase m, with probability 1− 3
T ∆̃2m , either j /∈",4.1. Known and Bounded Expected Delay,[0],[0]
"Am, or:
X̄m,j − µj ≤",4.1. Known and Bounded Expected Delay,[0],[0]
"∆̃m/2 .
",4.1. Known and Bounded Expected Delay,[0],[0]
"Regret bounds Using Lemma 1, we derive the following regret bounds in the current setting.
",4.1. Known and Bounded Expected Delay,[0],[0]
Theorem 2,4.1. Known and Bounded Expected Delay,[0],[0]
"Under Assumption 1, the expected regret of Algorithm 1 is upper bounded as
E[RT ] ≤ K∑ j=1 j 6=j∗ O ( log(T∆2j ) ∆j + log(1/∆j)E[τ ] ) .",4.1. Known and Bounded Expected Delay,[0],[0]
"(5)
Proof: Given Lemma 1, the proof of Theorem 2 closely follows the analysis of the Improved UCB algorithm of Auer & Ortner (2010).",4.1. Known and Bounded Expected Delay,[0],[0]
"Lemma 1 and the elimination condition in Algorithm 1 ensure that, with high probability, any suboptimal arm j will be eliminated by phase mj = log(1/∆j), thus incurring regret at most nmj∆j",4.1. Known and Bounded Expected Delay,[0],[0]
"We then substitute in nmj from (2), and sum over all suboptimal arms.",4.1. Known and Bounded Expected Delay,[0],[0]
A detailed proof is in Appendix B.2.,4.1. Known and Bounded Expected Delay,[0],[0]
"As in Auer & Ortner (2010), we avoid a union bound over all arms (which would result in an extra logK) by (i) reasoning about the regret of each arm individually, and (ii) bounding the regret resulting
from erroneously eliminating the optimal arm by carefully controlling the probability it is eliminated in each phase.
",4.1. Known and Bounded Expected Delay,[0],[0]
"Considering the worst-case values of ∆j (roughly √ K/T ), we obtain the following problem independent bound.
",4.1. Known and Bounded Expected Delay,[0],[0]
"Corollary 3 For any problem instance satisfying Assumption 1, the expected regret of Algorithm 1 satisfies
E[RT ] ≤",4.1. Known and Bounded Expected Delay,[0],[0]
O( √ KT log(K) +KE[τ ] log(T )).,4.1. Known and Bounded Expected Delay,[0],[0]
"If the delay is bounded by some constant d ≥ 0 and a single arm is played repeatedly for long enough, we can restrict the number of arms corrupting the observation",4.2. Delay with Bounded Support,[0],[0]
Xt at a given time t.,4.2. Delay with Bounded Support,[0],[0]
"In fact, if each arm j is played consecutively for more than d rounds, then at any time t ∈ Tj(m), the observation Xt will be composed of the rewards from at most two arms: the current arm j, and previous arm j′. Further, from the elimination condition, with high probability, arm j′ will have been eliminated if it is clearly suboptimal.",4.2. Delay with Bounded Support,[0],[0]
We can then recursively use the confidence bounds for arms j and j′ from the previous phase to bound |µj,4.2. Delay with Bounded Support,[0],[0]
− µj′ |.,4.2. Delay with Bounded Support,[0],[0]
"Below, we formalize this intuition to obtain a tighter bound on |X̄m,j − µj",4.2. Delay with Bounded Support,[0],[0]
|,4.2. Delay with Bounded Support,[0],[0]
"for every arm j and phase m, when each active arm is played a specified number of times per phase.
",4.2. Delay with Bounded Support,[0],[0]
"Choice of nm Here, we define,
nm = C1 log(T ∆̃
2 m) ∆̃2m + C2E[τ ]",4.2. Delay with Bounded Support,[0],[0]
"∆̃m (6)
+",4.2. Delay with Bounded Support,[0],[0]
"min { md, C3 log(T ∆̃ 2 m)
∆̃2m + C4mE[τ ] ∆̃m } for some large enough constants C1, C2, C3, C4 (see Appendix C, Equation (18) for the exact values).",4.2. Delay with Bounded Support,[0],[0]
"This choice of nm means that for large d, we essentially revert back to the choice of nm from (2) for the unbounded case, and we gain nothing by using the bound on the delay.",4.2. Delay with Bounded Support,[0],[0]
"However, if d is not large, the choice of nm in (6) is smaller than (2) since the second term now scales with E[τ ] rather than mE[τ ].
",4.2. Delay with Bounded Support,[0],[0]
"Estimation of error bounds In this setting, by the elimination condition and bounded delays, the expectation of each reward entering Tj(m) will be within ∆̃m−1 of µj , with high probability.",4.2. Delay with Bounded Support,[0],[0]
"Then, using knowledge of the upper bound of the support of τ , we can obtain a tighter bound and get an error bound similar to Lemma 1 with the smaller value of nm in (6).",4.2. Delay with Bounded Support,[0],[0]
We prove the following proposition.,4.2. Delay with Bounded Support,[0],[0]
"Since ∆̃m = 2−m, this is considerably tighter than (3).
",4.2. Delay with Bounded Support,[0],[0]
Proposition 4,4.2. Delay with Bounded Support,[0],[0]
"Assume ni − ni−1 ≥ d for phases i = 1, . . .",4.2. Delay with Bounded Support,[0],[0]
",m. Define Em−1 as the event that all arms j ∈",4.2. Delay with Bounded Support,[0],[0]
"Am satisfy error bounds |X̄m−1,j − µj | ≤ ∆̃m−1/2.",4.2. Delay with Bounded Support,[0],[0]
"Then, for
every arm j ∈",4.2. Delay with Bounded Support,[0],[0]
"Am,
E  ∑ t∈Tj(m)\Tj(m−1)",4.2. Delay with Bounded Support,[0],[0]
(Xt − µj) ∣∣∣∣Em−1  ≤ ∆̃m−1E[τ ].,4.2. Delay with Bounded Support,[0],[0]
Proof: (Sketch).,4.2. Delay with Bounded Support,[0],[0]
Consider a fixed arm j ∈ Am.,4.2. Delay with Bounded Support,[0],[0]
The expected value of the sum of observations Xt for t ∈ Tj(m),4.2. Delay with Bounded Support,[0],[0]
\ Tj(m− 1) would be (nm − nm−1)µj were it not for some rewards entering and leaving this period due to the delays.,4.2. Delay with Bounded Support,[0],[0]
"Because of the i.i.d. assumption on the delay, in expectation, the number of rewards leaving the period is roughly the same as the number of rewards entering this period, i.e., E[τ ].",4.2. Delay with Bounded Support,[0],[0]
(Conditioning on Em−1 does not effect this due to the bridge period).,4.2. Delay with Bounded Support,[0],[0]
"Since nm − nm−1 ≥ d, the reward coming into the period Tj(m)\Tj(m−1) can only be from the previous arm j′. All rewards leaving the period are from arm j. Therefore the expected difference between rewards entering and leaving the period is (µj − µj′)E[τ ].",4.2. Delay with Bounded Support,[0],[0]
"Then, if µj is close to µj′ , the total reward leaving the period is compensated by total reward entering.",4.2. Delay with Bounded Support,[0],[0]
"Due to the bridge period, even when j is the first arm played in phase m, j′ ∈ Am, so it was not eliminated in phase m − 1.",4.2. Delay with Bounded Support,[0],[0]
"By the elimination condition in Algorithm 1, if the error bounds |X̄m−1,j−µj | ≤ ∆̃m−1/2 are satisfied for all arms in Am, then |µj",4.2. Delay with Bounded Support,[0],[0]
− µj′ | ≤ ∆̃m−1.,4.2. Delay with Bounded Support,[0],[0]
"This gives the result.
",4.2. Delay with Bounded Support,[0],[0]
"Repeatedly using Proposition 4 we get,
m∑ i=1",4.2. Delay with Bounded Support,[0],[0]
E  ∑ t∈Tj(i)\Tj(i−1) (Xt − µj) ∣∣∣∣Ei−1  ≤ 2E[τ ] since ∑m i=1,4.2. Delay with Bounded Support,[0],[0]
"∆̃i−1 = ∑m−1 i=0 2
−i ≤ 2.",4.2. Delay with Bounded Support,[0],[0]
"Then, observe that P(ECi ) is small.",4.2. Delay with Bounded Support,[0],[0]
This bound is an improvement of a factor of m compared to (4).,4.2. Delay with Bounded Support,[0],[0]
"For the regret analysis, we derive a high probability version of the above result.",4.2. Delay with Bounded Support,[0],[0]
"Using this, and the choice of nm ≥ Ω ( log(T ∆̃2m)
",4.2. Delay with Bounded Support,[0],[0]
"∆̃2m + E[τ ] ∆̃m
) from (6), for
large enough constants, we derive the following lemma.",4.2. Delay with Bounded Support,[0],[0]
"A detailed proof is given in Appendix C.1.
",4.2. Delay with Bounded Support,[0],[0]
Lemma 5,4.2. Delay with Bounded Support,[0],[0]
"Under Assumptions 1 of known expected delay and 2 of bounded delays, and choice of nm given in (6), the estimates X̄m,j obtained by Algorithm 1 satisfy the following: For any arm j and phase m, with probability at least 1− 12
T ∆̃2m , either j /∈",4.2. Delay with Bounded Support,[0],[0]
"Am or
X̄m,j − µj ≤ ∆̃m/2.
",4.2. Delay with Bounded Support,[0],[0]
"Regret bounds We now give regret bounds for this case.
",4.2. Delay with Bounded Support,[0],[0]
Theorem 6,4.2. Delay with Bounded Support,[0],[0]
"Under Assumption 1 and bounded delay Assumption 2, the expected regret of Algorithm 1 satisfies
E[RT ]",4.2. Delay with Bounded Support,[0],[0]
"≤ K∑
j=1;j 6=j∗ O
( log(T∆2j )
∆j + E[τ",4.2. Delay with Bounded Support,[0],[0]
"]
+ min { d, log(T∆2j )
",4.2. Delay with Bounded Support,[0],[0]
"∆j + log(
1
∆j )",4.2. Delay with Bounded Support,[0],[0]
"E[τ ]
}) .
",4.2. Delay with Bounded Support,[0],[0]
Proof: (Sketch).,4.2. Delay with Bounded Support,[0],[0]
"Given Lemma 5, the proof is similar to that of Theorem 2.",4.2. Delay with Bounded Support,[0],[0]
"The full proof is in Appendix C.2.
",4.2. Delay with Bounded Support,[0],[0]
"Then, if d ≤ √
T logK K + E[τ ], we get the following
problem independent regret bound which matches that of Joulani et al. (2013).
",4.2. Delay with Bounded Support,[0],[0]
Corollary 7 For any problem instance satisfying Assumptions 1 and 2 with d ≤,4.2. Delay with Bounded Support,[0],[0]
"√ T logK K +E[τ ], the expected regret of Algorithm 1 satisfies
E[RT ] ≤",4.2. Delay with Bounded Support,[0],[0]
O( √ KT log(K) +KE[τ ]).,4.2. Delay with Bounded Support,[0],[0]
"If the delay is unbounded but well behaved in the sense that we know (a bound on) the variance, then we can obtain similar regret bounds to the bounded delay case.",4.3. Delay with Bounded Variance,[0],[0]
"Intuitively, delays from the previous phase will only corrupt observations in the current phase if their delays exceed the length of the bridge period.",4.3. Delay with Bounded Variance,[0],[0]
"We control this by using the bound on the variance to bound the tails of the delay distributions.
",4.3. Delay with Bounded Variance,[0],[0]
"Choice of nm Let V(τ) be the known variance (or bound on the variance) of the delay, as in Assumption 3.",4.3. Delay with Bounded Variance,[0],[0]
"Then, we use Algorithm 1 with the following value of nm,
nm = C1 log(T ∆̃2m)
∆̃2m + C2 E[τ ] + V(τ) ∆̃m
(7)
for some large enough constants C1, C2.",4.3. Delay with Bounded Variance,[0],[0]
"The exact value of nm is given in Appendix D, Equation (25).
",4.3. Delay with Bounded Variance,[0],[0]
"Regret bounds We get the following instance specific and problem independent regret bound in this case.
",4.3. Delay with Bounded Variance,[0],[0]
Theorem 8,4.3. Delay with Bounded Variance,[0],[0]
"Under Assumption 1 and Assumption 3 of known (bound on) the expectation and variance of the delay, and choice of nm from (7), the expected regret of Algorithm 1 can be upper bounded by,
E[RT ]",4.3. Delay with Bounded Variance,[0],[0]
"≤ K∑
j=1:µj 6=µ∗ O
( log(T∆2j )
∆j + E[τ ] + V(τ)
) .
",4.3. Delay with Bounded Variance,[0],[0]
Proof: (Sketch).,4.3. Delay with Bounded Variance,[0],[0]
See Appendix D.2.,4.3. Delay with Bounded Variance,[0],[0]
"We use Chebychev’s inequality to get a result similar to Lemma 5 and then use a similar argument to the bounded delay case.
",4.3. Delay with Bounded Variance,[0],[0]
"Corollary 9 For any problem instance satisfying Assumptions 1 and 3, the expected regret of Algorithm 1 satisfies
E[RT ] ≤",4.3. Delay with Bounded Variance,[0],[0]
"O( √ KT log(K) +KE[τ ] +KV(τ)).
",4.3. Delay with Bounded Variance,[0],[0]
"Remark If E[τ ] ≥ 1, then the delay penalty can be reduced to O(KE[τ ] +KV(τ)/E[τ ]) (see Appendix D).
",4.3. Delay with Bounded Variance,[0],[0]
"Thus, it is sufficient to know a bound on variance to obtain regret bounds similar to those in bounded delay case.",4.3. Delay with Bounded Variance,[0],[0]
Note that this approach is not possible just using knowledge of the expected delay since we cannot guarantee that the reward entering phase i is from an arm active in phase i− 1.,4.3. Delay with Bounded Variance,[0],[0]
"We compared the performance of our algorithm (under different assumptions) to QPM-D (Joulani et al., 2013) in various experimental settings.",5. Experimental Results,[0],[0]
"In these experiments, our aim was to investigate the effect of the delay on the performance of the algorithms.",5. Experimental Results,[0],[0]
"In order to focus on this, we used a simple setup of two arms with Bernoulli rewards and µ = (0.5, 0.6).",5. Experimental Results,[0],[0]
"In every experiment, we ran each algorithm to horizon T = 250000 and used UCB1 (Auer et al., 2002) as the base algorithm in QPM-D. The regret was averaged over 200 replications.",5. Experimental Results,[0],[0]
"For ease of reading, we define ODAAF to be our algorithm using only knowledge of the expected delay, with nm defined as in (2) and run without a bridge period, and ODAAF-B and ODAAF-V to be the versions of Algorithm 1 that use a bridge period and information on the bounded support and the finite variance of the delay to define nm as in (6) and (7) respectively.
",5. Experimental Results,[0],[0]
We tested the algorithms with different delay distributions.,5. Experimental Results,[0],[0]
"In the first case, we considered bounded delay distributions whereas in the second case, the delays were unbounded.",5. Experimental Results,[0],[0]
"In Fig. 3a, we plotted the ratios of the regret of ODAAF and ODAAF-B (with knowledge of d, the delay bound) to the regret of QPM-D. We see that in all cases the ratios converge to a constant.",5. Experimental Results,[0],[0]
"This shows that the regret of our algorithm is essentially of the same order as that of QPM-D. Our algorithm predetermines the number of times to play each active arm per phase (the randomness appears in whether an arm is active), so the jumps in the regret are it changing arm.",5. Experimental Results,[0],[0]
"This occurs at the same points in all replications.
",5. Experimental Results,[0],[0]
Fig.,5. Experimental Results,[0],[0]
3b shows a similar story for unbounded delays with mean E[τ ],5. Experimental Results,[0],[0]
= 50 (where N+ denotes the the half normal distribution).,5. Experimental Results,[0],[0]
The ratios of the regret of ODAAF and ODAAF-V (with knowledge of the delay variance) to the regret of QPM-D again converge to constants.,5. Experimental Results,[0],[0]
"Note that in this case, these constants, and the location of the jumps, vary with the delay distribution and V(τ).",5. Experimental Results,[0],[0]
"When the variance of the delay is small, it can be seen that using the variance information leads to improved performance.",5. Experimental Results,[0],[0]
"However, for exponential delays where V(τ) = E[τ ]2, the large variance causes nm to be large and so the suboptimal arm is played more, increasing the regret.",5. Experimental Results,[0],[0]
"In this case ODAAF-V had only just eliminated the suboptimal arm at time T .
",5. Experimental Results,[0],[0]
It can also be illustrated experimentally that the regret of our algorithms and that of QPM-D all increase linearly in E[τ ].,5. Experimental Results,[0],[0]
This is shown in Appendix E. We also provide an experimental comparison to Vernade et al. (2017) in Appendix E.,5. Experimental Results,[0],[0]
"We have studied an extension of the multi-armed bandit problem to bandits with delayed, aggregated anonymous feedback.",6. Conclusion,[0],[0]
"Here, a sum of observations is received after some stochastic delay and we do not learn which arms contributed to each observation.",6. Conclusion,[0],[0]
"In this more difficult setting, we have proven that, surprisingly, it is possible to develop an algorithm that performs comparably to those for the simpler delayed feedback bandits problem, where the assignment of rewards to plays is known.",6. Conclusion,[0],[0]
"Particularly, using only knowledge of the expected delay, our algorithm matches the worst case regret of Joulani et al. (2013) up to a logarithmic factor.",6. Conclusion,[0],[0]
"This logarithmic factors can be removed using an improved analysis and slightly more information about the delay; if the delay is bounded, we achieve the same worst case regret as Joulani et al. (2013), and for unbounded delays with known finite variance, we have an extra additive V(τ) term.",6. Conclusion,[0],[0]
We supported these claims experimentally.,6. Conclusion,[0],[0]
"Note that while our algorithm matches the order of regret of QPM-D, the constants are worse.",6. Conclusion,[0],[0]
"Hence, it is an open problem to find algorithms with better constants.",6. Conclusion,[0],[0]
CPB would like to thank the EPSRC funded EP/L015692/1 STOR-i centre for doctoral training and Sparx.,Acknowledgments,[0],[0]
We would like to thank the reviewers for their helpful comments.,Acknowledgments,[0],[0]
"We study a variant of the stochastic K-armed bandit problem, which we call “bandits with delayed, aggregated anonymous feedback”.",abstractText,[0],[0]
"In this problem, when the player pulls an arm, a reward is generated, however it is not immediately observed.",abstractText,[0],[0]
"Instead, at the end of each round the player observes only the sum of a number of previously generated rewards which happen to arrive in the given round.",abstractText,[0],[0]
"The rewards are stochastically delayed and due to the aggregated nature of the observations, the information of which arm led to a particular reward is lost.",abstractText,[0],[0]
"The question is what is the cost of the information loss due to this delayed, aggregated anonymous feedback?",abstractText,[0],[0]
"Previous works have studied bandits with stochastic, non-anonymous delays and found that the regret increases only by an additive factor relating to the expected delay.",abstractText,[0],[0]
"In this paper, we show that this additive regret increase can be maintained in the harder delayed, aggregated anonymous feedback setting when the expected delay (or a bound on it) is known.",abstractText,[0],[0]
"We provide an algorithm that matches the worst case regret of the non-anonymous problem exactly when the delays are bounded, and up to logarithmic factors or an additive variance term for unbounded delays.",abstractText,[0],[0]
"Bandits with Delayed, Aggregated Anonymous Feedback",title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3739–3748 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3739",text,[0],[0]
Single-document summarization methods can be divided into two categories: extractive and abstractive.,1 Introduction,[0],[0]
"Extractive summarization systems form summaries by selecting and copying text snippets from the document, while abstractive methods aim to generate concise summaries with paraphrasing.",1 Introduction,[0],[0]
"This work is primarily concerned with extractive
∗Equal contribution.
summarization.",1 Introduction,[0],[0]
"Though abstractive summarization methods have made strides in recent years, extractive techniques are still very attractive as they are simpler, faster, and more reliably yield semantically and grammatically correct sentences.
",1 Introduction,[0],[0]
"Many extractive summarizers work by selecting sentences from the input document (Luhn, 1958; Mihalcea and Tarau, 2004; Wong et al., 2008; Kågebäck et al., 2014; Yin and Pei, 2015; Cao et al., 2015; Yasunaga et al., 2017).",1 Introduction,[0],[0]
"Furthermore, a growing trend is to frame this sentence selection process as a sequential binary labeling problem, where binary inclusion/exclusion labels are chosen for sentences one at a time, starting from the beginning of the document, and decisions about later sentences may be conditioned on decisions about earlier sentences.",1 Introduction,[0],[0]
"Recurrent neural networks may be trained with stochastic gradient ascent to maximize the likelihood of a set of ground-truth binary label sequences (Cheng and Lapata, 2016; Nallapati et al., 2017).",1 Introduction,[0],[0]
"However, this approach has two well-recognized disadvantages.",1 Introduction,[0],[0]
"First, it suffers from exposure bias, a form of mismatch between training and testing data distributions which can hurt performance (Ranzato et al., 2015; Bahdanau et al., 2017; Paulus et al., 2018).",1 Introduction,[0],[0]
"Second, extractive labels must be generated by a heuristic, as summarization datasets do not generally include ground-truth extractive labels; the ultimate performance of models trained on such labels is thus fundamentally limited by the quality of the heuristic.
",1 Introduction,[0],[0]
"An alternative to maximum likelihood training
is to use reinforcement learning to train the model to directly maximize a measure of summary quality, such as the ROUGE score between the generated summary and a ground-truth abstractive summary (Wu and Hu, 2018).",1 Introduction,[0],[0]
"This approach has become popular because it avoids exposure bias, and directly optimizes a measure of summary quality.",1 Introduction,[0],[0]
"However, it also has a number of downsides.",1 Introduction,[0],[0]
"For one, the search space is quite large: for a document of length T , there are 2T possible extractive summaries.",1 Introduction,[0],[0]
This makes the exploration problem faced by the reinforcement learning algorithm during training very difficult.,1 Introduction,[0],[0]
"Another issue is that due to the sequential nature of selection, the model is inherently biased in favor of selecting earlier sentences over later ones, a phenomenon which we demonstrate empirically in Section 7.",1 Introduction,[0],[0]
"The first issue can be resolved to a degree using either a cumbersome maximum likelihood-based pre-training step (using heuristically-generated labels) (Wu and Hu, 2018), or placing a hard upper limit on the number of sentences selected.",1 Introduction,[0],[0]
"The second issue is more problematic, as it is inherent to the sequential binary labeling setting.
",1 Introduction,[0],[0]
"In the current work, we introduce BANDITSUM, a novel method for training neural network-based extractive summarizers with reinforcement learning.",1 Introduction,[0],[0]
"This method does away with the sequential binary labeling setting, instead formulating extractive summarization as a contextual bandit.",1 Introduction,[0],[0]
"This move greatly reduces the size of the space that must be explored, removes the need to perform supervised pre-training, and prevents systematically privileging earlier sentences over later ones.",1 Introduction,[0],[0]
"Although the strong performance of Lead-3 indicates that good sentences often occur early in the source article, we show in Sections 6 and 7 that the contextual bandit setting greatly improves model performance when good sentences occur late without sacrificing performance when good sentences occur early.
",1 Introduction,[0],[0]
"Under this reformulation, BANDITSUM takes the document as input and outputs an affinity for each of the sentences therein.",1 Introduction,[0],[0]
"An affinity is a real number in [0, 1] which quantifies the model’s propensity for including a sentence in the summary.",1 Introduction,[0],[0]
These affinities are then used in a process of repeated sampling-without-replacement which does not privilege earlier sentences over later ones.,1 Introduction,[0],[0]
"BANDITSUM is free to process the document as a whole before yielding affinities, which permits
affinities for different sentences in the document to depend on one another in arbitrary ways.",1 Introduction,[0],[0]
"In our technical section, we show how to apply policy gradient reinforcement learning methods to this setting.
",1 Introduction,[0],[0]
"The contributions of our work are as follows:
• We propose a theoretically grounded method, based on the contextual bandit formalism, for training neural network-based extractive summarizers with reinforcement learning.",1 Introduction,[0],[0]
"Based on this training method, we propose the BANDITSUM system for extractive summarization.
",1 Introduction,[0],[0]
"• We perform experiments demonstrating that BANDITSUM obtains state-of-the-art performance on a number of datasets and requires significantly fewer update steps than competing approaches.
",1 Introduction,[0],[0]
"• We perform human evaluations showing that in the eyes of human judges, summaries created by BANDITSUM are less redundant and of higher overall quality than summaries created by competing approaches.
",1 Introduction,[0],[0]
"• We provide evidence, in the form of experiments in which models are trained on subsets of the data, that the improved performance of BANDITSUM over competitors stems in part from better handling of summary-worthy sentences that come near the end of the document (see Section 7).",1 Introduction,[0],[0]
Extractive summarization has been widely studied in the past.,2 Related Work,[0],[0]
"Recently, neural network-based methods have been gaining popularity over classical methods (Luhn, 1958; Gong and Liu, 2001; Conroy and O’leary, 2001; Mihalcea and Tarau, 2004; Wong et al., 2008), as they have demonstrated stronger performance on large corpora.",2 Related Work,[0],[0]
Central to the neural network-based models is the encoderdecoder structure.,2 Related Work,[0],[0]
"These models typically use either a convolution neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Pei, 2015; Cao et al., 2015), a recurrent neural network (Chung et al., 2014; Cheng and Lapata, 2016; Nallapati et al., 2017), or a combination of the two (Narayan et al., 2018; Wu and Hu, 2018) to create sentence and document representations, using word embeddings (Mikolov et al., 2013; Pennington et al.,
2014) to represent words at the input level.",2 Related Work,[0],[0]
"These vectors are then fed into a decoder network to generate the output summary.
",2 Related Work,[0],[0]
"The use of reinforcement learning (RL) in extractive summarization was first explored by Ryang and Abekawa (2012), who proposed to use the TD(λ) algorithm to learn a value function for sentence selection.",2 Related Work,[0],[0]
Rioux et al. (2014) improved this framework by replacing the learning agent with another TD(λ) algorithm.,2 Related Work,[0],[0]
"However, the performance of their methods was limited by the use of shallow function approximators, which required performing a fresh round of reinforcement learning for every new document to be summarized.",2 Related Work,[0],[0]
"The more recent work of Paulus et al. (2018) and Wu and Hu (2018) use reinforcement learning in a sequential labeling setting to train abstractive and extractive summarizers, respectively, while Chen and Bansal (2018) combines both approaches, applying abstractive summarization to a set of sentences extracted by a pointer network (Vinyals et al., 2015) trained via REINFORCE.",2 Related Work,[0],[0]
"However, pre-training with a maximum likelihood objective is required in all of these models.
",2 Related Work,[0],[0]
The two works most similar to ours are Yao et al. (2018) and Narayan et al. (2018).,2 Related Work,[0],[0]
"Yao et al. (2018) recently proposed an extractive summarization approach based on deep Q learning, a type of reinforcement learning.",2 Related Work,[0],[0]
"However, their approach is extremely computationally intensive (a minimum of 10 days before convergence), and was unable to achieve ROUGE scores better than the best maximum likelihood-based approach.",2 Related Work,[0],[0]
"Narayan et al. (2018) uses a cascade of filters in order to arrive at a set of candidate extractive summaries, which we can regard as an approximation of the true action space.",2 Related Work,[0],[0]
They then use an approximation of a policy gradient method to train their neural network to select summaries from this approximated action space.,2 Related Work,[0],[0]
"In contrast, BANDITSUM samples directly from the true action space, and uses exact policy gradient parameter updates.",2 Related Work,[0],[0]
Our approach formulates extractive summarization as a contextual bandit which we then train an agent to solve using policy gradient reinforcement learning.,3 Extractive Summarization as a Contextual Bandit,[0],[0]
"A bandit is a decision-making formalization in which an agent repeatedly chooses one of several actions, and receives a reward based on
this choice.",3 Extractive Summarization as a Contextual Bandit,[0],[0]
"The agent’s goal is to quickly learn which action yields the most favorable distribution over rewards, and choose that action as often as possible.",3 Extractive Summarization as a Contextual Bandit,[0],[0]
"In a contextual bandit, at each trial, a context is sampled and shown to the agent, after which the agent selects an action and receives a reward; importantly, the rewards yielded by the actions may depend on the sampled context.",3 Extractive Summarization as a Contextual Bandit,[0],[0]
The agent must quickly learn which actions are favorable in which contexts.,3 Extractive Summarization as a Contextual Bandit,[0],[0]
"Contextual bandits are a subset of Markov Decision Processes in which every episode has length one.
",3 Extractive Summarization as a Contextual Bandit,[0],[0]
Extractive summarization may be regarded as a contextual bandit as follows.,3 Extractive Summarization as a Contextual Bandit,[0],[0]
"Each document is a context, and each ordered subset of a document’s sentences is a different action.",3 Extractive Summarization as a Contextual Bandit,[0],[0]
"Formally, assume that each context is a document d consisting of sentences s = (s1, . . .",3 Extractive Summarization as a Contextual Bandit,[0],[0]
", sNd), and that each action is a length-M sequence of unique sentence indices",3 Extractive Summarization as a Contextual Bandit,[0],[0]
"i = (i1, . . .",3 Extractive Summarization as a Contextual Bandit,[0],[0]
", iM ) where it ∈ {1, . . .",3 Extractive Summarization as a Contextual Bandit,[0],[0]
", Nd}, it 6=",3 Extractive Summarization as a Contextual Bandit,[0],[0]
"it′ for t 6= t′, and M is an integer hyper-parameter.",3 Extractive Summarization as a Contextual Bandit,[0],[0]
"For each i, the extractive summary induced by i is given by (si1 , . . .",3 Extractive Summarization as a Contextual Bandit,[0],[0]
", siM ).",3 Extractive Summarization as a Contextual Bandit,[0],[0]
"An action i taken in context d is given a reward R(i, a), where a is the gold-standard abstractive summary that is paired with document d, andR is a scalar reward function quantifying the degree of match between a and the summary induced by i.
A policy for extractive summarization is a neural network pθ(·|d), parameterized by a vector θ, which, for each input document d, yields a probability distribution over index sequences.",3 Extractive Summarization as a Contextual Bandit,[0],[0]
Our goal is to find parameters θ which cause pθ(·|d) to assign high probability to index sequences that induce extractive summaries that a human reader would judge to be of high-quality.,3 Extractive Summarization as a Contextual Bandit,[0],[0]
"We achieve this by maximizing the following objective function with respect to parameters θ:
J(θ) =",3 Extractive Summarization as a Contextual Bandit,[0],[0]
"E [R(i, a)] (1)
where the expectation is taken over documents d paired with gold-standard abstractive summaries a, as well as over index sequences i generated according to pθ(·|d).",3 Extractive Summarization as a Contextual Bandit,[0],[0]
"Ideally, we would like to maximize (1) using gradient ascent.",3.1 Policy Gradient Reinforcement Learning,[0],[0]
"However, the required gradient cannot be obtained using usual techniques (e.g. simple backpropagation) because i must be discretely sampled in order to compute R(i, a).
",3.1 Policy Gradient Reinforcement Learning,[0],[0]
"Fortunately, we can use the likelihood ratio gradient estimator from reinforcement learning and stochastic optimization (Williams, 1992; Sutton et al., 2000), which tells us that the gradient of this function can be computed as:
∇θJ(θ) = E",3.1 Policy Gradient Reinforcement Learning,[0],[0]
"[∇θ log pθ(i|d)R(i, a)]",3.1 Policy Gradient Reinforcement Learning,[0],[0]
"(2)
where the expectation is taken over the same variables as (1).
",3.1 Policy Gradient Reinforcement Learning,[0],[0]
"Since we typically do not know the exact document distribution and thus cannot evaluate the expected value in (2), we instead estimate it by sampling.",3.1 Policy Gradient Reinforcement Learning,[0],[0]
"We found that we obtained the best performance when, for each update, we first sample one document/summary pair (d, a), then sample B index sequences i1, . . .",3.1 Policy Gradient Reinforcement Learning,[0],[0]
", iB from pθ(·|d), and finally take the empirical average:
∇θJ(θ)",3.1 Policy Gradient Reinforcement Learning,[0],[0]
"≈ 1
B B∑ b=1",3.1 Policy Gradient Reinforcement Learning,[0],[0]
"∇θ log pθ(ib|d)R(ib, a) (3)
",3.1 Policy Gradient Reinforcement Learning,[0],[0]
"This overall learning algorithm can be regarded as an instance of the REINFORCE policy gradient algorithm (Williams, 1992).",3.1 Policy Gradient Reinforcement Learning,[0],[0]
There are many possible choices for the structure of pθ(·|d); we opt for one that avoids privileging early sentences over later ones.,3.2 Structure of pθ(·|d),[0],[0]
"We first decompose pθ(·|d) into two parts: πθ, a deterministic function which contains all the network’s parameters, and µ, a probability distribution parameterized by the output of πθ.",3.2 Structure of pθ(·|d),[0],[0]
"Concretely:
pθ(·|d) = µ(·|πθ(d)) (4)
",3.2 Structure of pθ(·|d),[0],[0]
"Given an input document d, πθ outputs a realvalued vector of sentence affinities whose length is equal to the number of sentences in the document (i.e. πθ(d) ∈ RNd) and whose elements fall in the range [0, 1].",3.2 Structure of pθ(·|d),[0],[0]
"The t-th entry π(d)t may be roughly interpreted as the network’s propensity to include sentence st in the summary of d.
",3.2 Structure of pθ(·|d),[0],[0]
"Given sentence affinities πθ(d), µ implements a process of repeated sampling-withoutreplacement.",3.2 Structure of pθ(·|d),[0],[0]
"This proceeds by repeatedly normalizing the set of affinities corresponding to sentences that have not yet been selected, thereby obtaining a probability distribution over unselected sentences, and sampling from that distribution to obtain a new sentence to include.",3.2 Structure of pθ(·|d),[0],[0]
"This normalizeand-sample step is repeated M times, yielding M unique sentences to include in the summary.
",3.2 Structure of pθ(·|d),[0],[0]
"At each step of sampling-without-replacement, we also include a small probability of sampling uniformly from all remaining sentences.",3.2 Structure of pθ(·|d),[0],[0]
"This is used to achieve adequate exploration during training, and is similar to the -greedy technique from reinforcement learning.
",3.2 Structure of pθ(·|d),[0],[0]
"Under this sampling scheme, we have the following expression for pθ(i|d):
M∏ j=1
(
Nd − j + 1 + (1− )π(d)ij z(d)− ∑j−1",3.2 Structure of pθ(·|d),[0],[0]
"k=1 π(d)ik
) (5)
where z(d) = ∑
t π(d)t.",3.2 Structure of pθ(·|d),[0],[0]
"For index sequences that have length different from M , or that contain duplicate indices, we have pθ(i|d) = 0.",3.2 Structure of pθ(·|d),[0],[0]
"Using this expression, it is straightforward to use automatic differentiation software to compute ∇θ log pθ(i|d), which is required for the gradient estimate in (3).",3.2 Structure of pθ(·|d),[0],[0]
"Our sample-based gradient estimate can have high variance, which can slow the learning.",3.3 Baseline for Variance Reduction,[0],[0]
"One potential cause of this high variance can be seen by inspecting (3), and noting that it basically acts to change the probability of a sampled index sequence to an extent determined by the reward R(i, a).",3.3 Baseline for Variance Reduction,[0],[0]
"However, since ROUGE scores are always positive, the probability of every sampled index sequence is increased, whereas intuitively, we would prefer to decrease the probability of sequences that receive a comparatively low reward, even if it is positive.",3.3 Baseline for Variance Reduction,[0],[0]
"This can be remedied by the introduction of a so-called baseline which is subtracted from all rewards.
",3.3 Baseline for Variance Reduction,[0],[0]
"Using a baseline r, our sample-based estimate of∇θJ(θ) becomes:
1
B B∑ i=1",3.3 Baseline for Variance Reduction,[0],[0]
"∇θ log pθ(ib|d)(R(ib, a)− r) (6)
",3.3 Baseline for Variance Reduction,[0],[0]
"It can be shown that the introduction of r does not bias the gradient estimator and can significantly reduce its variance if chosen appropriately (Sutton et al., 2000).
",3.3 Baseline for Variance Reduction,[0],[0]
"There are several possibilities for the baseline, including the long-term average reward and the average reward across different samples for one document-summary pair.",3.3 Baseline for Variance Reduction,[0],[0]
"We choose an approach known as self-critical reinforcement learning, in which the test-time performance of the current model is used as the baseline (Ranzato et al., 2015;
Rennie et al., 2017; Paulus et al., 2018).",3.3 Baseline for Variance Reduction,[0],[0]
"More concretely, after sampling the document-summary pair (d, a), we greedily generate an index sequence using the current parameters θ:
igreedy =",3.3 Baseline for Variance Reduction,[0],[0]
argmax,3.3 Baseline for Variance Reduction,[0],[0]
"i
pθ(i|d) (7)
and calculate the baseline for the current update as r = R(igreedy, a).",3.3 Baseline for Variance Reduction,[0],[0]
This baseline has the intuitively satisfying property of only increasing the probability of a sampled label sequence when the summary it induces is better than what would be obtained by greedy decoding.,3.3 Baseline for Variance Reduction,[0],[0]
"A final consideration is a concrete choice for the reward function R(i, a).",3.4 Reward Function,[0],[0]
"Throughout this work we use:
R(i, a) = 1
3 (ROUGE-1f (i, a) +
ROUGE-2f (i, a) +",3.4 Reward Function,[0],[0]
"ROUGE-Lf (i, a)).",3.4 Reward Function,[0],[0]
"(8)
The above reward function optimizes the average of all the ROUGE variants (Lin, 2004) while balancing precision and recall.",3.4 Reward Function,[0],[0]
"In this section, we discuss the concrete instantiations of the neural network πθ that we use in our experiments.",4 Model,[0],[0]
"We break πθ up into two components: a document encoder fθ1, which outputs a sequence of sentence feature vectors (h1, . . .",4 Model,[0],[0]
", hNd) and a decoder gθ2 which yields sentence affinities:
h1, . . .",4 Model,[0],[0]
", hNd = fθ1(d) (9)
πθ(d) = gθ2(h1, . . .",4 Model,[0],[0]
", hNd) (10)
Encoder.",4 Model,[0],[0]
"Features for each sentence in isolation are first obtained by applying a word-level Bidirectional Recurrent Neural Network (BiRNN) to the embeddings for the words in the sentence, and averaging the hidden states over words.",4 Model,[0],[0]
A separate sentence-level BiRNN is then used to obtain a representations hi for each sentence in the context of the document.,4 Model,[0],[0]
Decoder.,4 Model,[0],[0]
"A multi-layer perceptron is used to map from the representation ht of each sentence through a final sigmoid unit to yield sentence affinities πθ(d).
",4 Model,[0],[0]
"The use of a bidirectional recurrent network in the encoder is crucial, as it allows the network to
process the document as a whole, yielding representations for each sentence that take all other sentences into account.",4 Model,[0],[0]
"This procedure is necessary to deal with some aspects of summary quality such as redundancy (avoiding the inclusion of multiple sentences with similar meaning), which requires the affinities for different sentences to depend on one another.",4 Model,[0],[0]
"For example, to avoid redundancy, if the affinity for some sentence is high, then sentences which express similar meaning should have low affinities.",4 Model,[0],[0]
"In this section, we discuss the setup of our experiments.",5 Experiments,[0],[0]
We first discuss the corpora that we used and our evaluation methodology.,5 Experiments,[0],[0]
"We then discuss the baseline methods against which we compared, and conclude with a detailed overview of the settings of the model parameters.",5 Experiments,[0],[0]
"Three datasets are used for our experiments: the CNN, the Daily Mail, and combined CNN/Daily Mail (Hermann et al., 2015; Nallapati et al., 2016).",5.1 Corpora,[0],[0]
"We use the standard split of Hermann et al. (2015) for training, validating, and testing and the same setting without anonymization on the three corpus as See et al. (2017).",5.1 Corpora,[0],[0]
"The Daily Mail corpus has 196,557 training documents, 12,147 validation documents and 10,397 test documents; while the CNN corpus has 90,266/1,220/1,093 documents, respectively.",5.1 Corpora,[0],[0]
"The models are evaluated based on ROUGE (Lin, 2004).",5.2 Evaluation,[0],[0]
We obtain our ROUGE scores using the standard pyrouge package1 for the test set evaluation and a faster python implementation of the ROUGE metric2 for training and evaluating on the validation set.,5.2 Evaluation,[0],[0]
"We report the F1 scores of ROUGE1, ROUGE-2, and ROUGE-L, which compute the uniform, bigram, and longest common subsequence overlapping with the reference summaries.",5.2 Evaluation,[0],[0]
"We compare BANDITSUM with other extractive methods including: the Lead-3 model, SummaRuNNer (Nallapati et al., 2017), Refresh
1https://pypi.python.org/pypi/pyrouge/ 0.1.3
2We use the modified version based on https:// github.com/pltrdy/rouge
(Narayan et al., 2018), RNES (Wu and Hu, 2018), DQN (Yao et al., 2018), and NN-SE (Cheng and Lapata, 2016).",5.3 Baselines,[0],[0]
The Lead-3 model simply produces the leading three sentences of the document as the summary.,5.3 Baselines,[0],[0]
"We use 100-dimensional Glove embeddings (Pennington et al., 2014) as our embedding initialization.",5.4 Model Settings,[0],[0]
"We do not limit the sentence length, nor the maximum number of sentences per document.",5.4 Model Settings,[0],[0]
"We use one-layer BiLSTM for word-level RNN, and two-layers BiLSTM for sentence-level RNN.",5.4 Model Settings,[0],[0]
The hidden state dimension is 200 for each direction on all LSTMs.,5.4 Model Settings,[0],[0]
"For the decoder, we use a feedforward network with one hidden layer of dimension 100.
",5.4 Model Settings,[0],[0]
"During training, we use Adam (Kingma and Ba, 2015) as the optimizer with the learning rate of 5e−5, beta parameters (0, 0.999), and a weight decay of 1e−6, to maximize the objective function defined in equation (1).",5.4 Model Settings,[0],[0]
We employ gradient clipping of 1 to regularize our model.,5.4 Model Settings,[0],[0]
"At each iteration, we sample B = 20 times to estimate the gradient defined in equation 3.",5.4 Model Settings,[0],[0]
"For our system, the reported performance is obtained within two epochs of training 3.
",5.4 Model Settings,[0],[0]
"At the test time, we pick sentences sorted by the predicted probabilities until the length limit is reached.",5.4 Model Settings,[0],[0]
The full-length ROUGE F1 score is used as the evaluation metric.,5.4 Model Settings,[0],[0]
"For M , the number of sentences selected per summary, we use a value of 3, based on our validation results as well as on the settings described in Nallapati et al. (2017).",5.4 Model Settings,[0],[0]
"In this section, we present quantitative results from the ROUGE evaluation and qualitative results based on human evaluation.",6 Experiment Results,[0],[0]
"In addition, we demonstrate the stability of our RL model by comparing the validation curve of BANDITSUM with SummaRuNNer (Nallapati et al., 2017) trained with a maximum likelihood objective.",6 Experiment Results,[0],[0]
"We present the results of comparing BANDITSUM to several baseline algorithms4 on the CNN/Daily
3Our code can be found at https://github.com/ yuedongP/summarization_RL
4 Due to different pre-processing methods and different numbers of selected sentences, several papers report different Lead scores (Narayan et al., 2018; See et al., 2017).",6.1 Rouge Evaluation,[0],[0]
"We use
Mail corpus in Tables 1 and 2.",6.1 Rouge Evaluation,[0],[0]
"Compared to other extractive summarization systems, BANDITSUM achieves performance that is significantly better than two RL-based approaches, Refresh (Narayan et al., 2018) and DQN (Yao et al., 2018), as well as SummaRuNNer, the state-of-the-art maximum liklihood-based extractive summarizer (Nallapati et al., 2017).",6.1 Rouge Evaluation,[0],[0]
"BANDITSUM performs a little better than RNES (Wu and Hu, 2018) in terms of ROUGE-1 and slightly worse in terms of ROUGE2.",6.1 Rouge Evaluation,[0],[0]
"However, RNES requires pre-training with the maximum likelihood objective on heuristicallygenerated extractive labels; in contrast, BANDITSUM is very light-weight and converges significantly faster.",6.1 Rouge Evaluation,[0],[0]
"We discuss the advantage of framing the extractive summarization based on the contextual bandit (BANDITSUM) over the sequential binary labeling setting (RNES) in the discussion Section 7.
",6.1 Rouge Evaluation,[0],[0]
"We also noticed that different choices for the policy gradient baseline (see Section 3.3) in BANDITSUM affect learning speed, but do not significantly affect asymptotic performance.",6.1 Rouge Evaluation,[0],[0]
"Models trained with an average reward baseline learned most quickly, while models trained with three different baselines (greedy, average reward in a
the test set provided by Narayan et al. (2018).",6.1 Rouge Evaluation,[0],[0]
"Since their Lead score is a combination of Lead-3 for CNN and Lead4 for Daily Mail, we recompute the Lead-3 scores for both CNN and Daily Mail with the preprocessing steps used in See et al. (2017).",6.1 Rouge Evaluation,[0],[0]
"Additionally, our results are not directly comparable to results based on the anonymized dataset used by Nallapati et al. (2017).
batch, average global reward) all perform roughly the same after training for one epoch.",6.1 Rouge Evaluation,[0],[0]
Models trained without a baseline were found to underperform other baseline choices by about 2 points of ROUGE score on average.,6.1 Rouge Evaluation,[0],[0]
We also conduct a qualitative evaluation to understand the effects of the improvements introduced in BANDITSUM on human judgments of the generated summaries.,6.2 Human Evaluation,[0],[0]
"To assess the effect of training with RL rather than maximum likelihood, in the first set of human evaluations we compare BANDITSUM with the state-of-the-art maximum likelihood-based model SummaRuNNer.",6.2 Human Evaluation,[0],[0]
"To evaluate the importance of using an exact, rather than approximate, policy gradient to optimize ROUGE scores, we perform another human evaluation comparing BANDITSUM and Refresh, an RL-based method that uses the an approximation of the policy gradient.
",6.2 Human Evaluation,[0],[0]
We follow a human evaluation protocol similar to the one used in Wu and Hu (2018).,6.2 Human Evaluation,[0],[0]
"Given a set of N documents, we ask K volunteers to evaluate the summaries extracted by both systems.",6.2 Human Evaluation,[0],[0]
"For each document, a reference summary, and a pair of randomly ordered extractive summaries (one generated by each of the two models) is presented to the volunteers.",6.2 Human Evaluation,[0],[0]
"They are asked to compare and rank the extracted summaries along three dimensions: overall, coverage, and non-redundancy.
",6.2 Human Evaluation,[0],[0]
"To compare with SummaRuNNer, we randomly sample 57 documents from the test set of Daily-
Mail and ask 5 volunteers to evaluate the extracted summaries.",6.2 Human Evaluation,[0],[0]
"While comparing with Refresh, we use the 20 documents (10 CNN and 10 DailyMail) provided by Narayan et al. (2018) to 4 volunteers.",6.2 Human Evaluation,[0],[0]
Tables 3 and 4 show the results of human evaluation in these two settings.,6.2 Human Evaluation,[0],[0]
BANDITSUM is shown to be better than Refresh and SummaRuNNer in terms of overall quality and nonredundancy.,6.2 Human Evaluation,[0],[0]
"These results indicate that the use of the true policy gradient, rather than the approximation used by Refresh, improves overall quality.",6.2 Human Evaluation,[0],[0]
"It is interesting to observe that, even though BANDITSUM does not have an explicit redundancy avoidance mechanism, it actually outperforms the other systems on non-redundancy.",6.2 Human Evaluation,[0],[0]
Reinforcement learning methods are known for sometimes being unstable during training.,6.3 Learning Curve,[0],[0]
"However, this seems to be less of a problem for BANDITSUM, perhaps because it is formulated as a contextual bandit rather than a sequential labeling problem.",6.3 Learning Curve,[0],[0]
"We show this by comparing the validation curves generated by BANDITSUM and the state-of-the-art maximum likelihood-based model – SummaRuNNer (Nallapati et al., 2017) (Figure 1).
",6.3 Learning Curve,[0],[0]
"From Figure 1, we observe that BANDITSUM converges significantly more quickly to good results than SummaRuNNer.",6.3 Learning Curve,[0],[0]
"Moreover, there is less variance in the performance of BANDITSUM.
",6.3 Learning Curve,[0],[0]
One possible reason is that extractive summarization does not have well-defined supervised labels.,6.3 Learning Curve,[0],[0]
There exists a mismatch between the provided labels and human-generated abstractive summaries.,6.3 Learning Curve,[0],[0]
"Hence, the gradient, computed from the maximum likelihood loss function, is not optimizing the evaluation metric of interest.",6.3 Learning Curve,[0],[0]
"Another important message is that both models are still far from the estimated upper bound5, which shows that there is still significant room for improvement.",6.3 Learning Curve,[0],[0]
"On CNN/Daily mail dataset, our model’s timeper-epoch is about 25.5 hours on a TITAN Xp.",6.4 Run Time,[0],[0]
"We trained the model for 3 epochs, which took about 76 hours in total.",6.4 Run Time,[0],[0]
"For comparison, DQN took about 10 days to train on a GTX 1080 (Yao et al., 2018).",6.4 Run Time,[0],[0]
"Refresh took about 12 hours on a single GPU to train (Narayan et al., 2018).",6.4 Run Time,[0],[0]
Note that this figure does not take into account the significant time required by Refresh for pre-computing ROUGE scores.,6.4 Run Time,[0],[0]
"We conjecture that the contextual bandit (CB) setting is a more suitable framework for modeling extractive summarization than the sequential binary labeling setting, especially in the cases when good summary sentences appear later in the document.",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"The intuition behind this is that models based on the sequential labeling setting are affected by the order of the decisions, which biases towards selecting sentences that appear earlier in the document.",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"By contrast, our CB-based RL model has more flexibility and freedom to explore the search space, as it samples the sentences without replacement based on the affinity scores.",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"Note that although we do not explicitly make the selection decisions in a sequential fashion, the sequential information about dependencies between sentences is implicitly embedded in the affinity scores, which are produced by bidirectional RNNs.
",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"We provide empirical evidence for this conjecture by comparing BANDITSUM to the sequential RL model proposed by Wu and Hu (2018) (Figure 2) on two subsets of the data: one with good
5The supervised labels for the upper bound estimation are obtained using the heuristic described in Nallapati et al. (2017).
summary sentences appearing early in the article, while the other contains articles where good summary sentences appear late.",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"Specifically, we construct two evaluation datasets by selecting the first 50 documents (Dearly, i.e., best summary occurs early) and the last 50 documents (Dlate, i.e., best summary occurs late) from a sample of 1000 documents that is ordered by the average extractive label index idx.",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"Given an article with n sentences indexed from 1, . . .",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
", n and a greedy extractive labels set with three sentences (i, j, k)6, the average index for the extractive label is computed by idx= (i+ j + k)/3n.
",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"Given these two subsets of the data, three different models (BANDITSUM, RNES and RNES3) are trained and evaluated on each of the two datasets without extractive labels.",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"Since the original sequential RL model (RNES) is unstable without supervised pre-training, we propose the RNES3 model that is limited to select no more then three sentences.",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"Starting with random initializations without supervised pre-training, we train each model ten times for 100 epochs and plot the learning curve of the average ROUGE-F1 score computed based on the trained model in Figure 2.",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"We can clearly see that BANDITSUM finds a better so-
6For each document, a length-3 extractive summary with near-optimal ROUGE score is selected following the heuristic proposed by Nallapati et al. (2017).
lution more quickly than RNES and RNES3 on both datasets.",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"Moreover, it displays a significantly speed-up in the exploration and finds the best solution when good summary sentences appeared later in the document (Dlate).",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"In this work, we presented a contextual bandit learning framework, BANDITSUM , for extractive summarization, based on neural networks and reinforcement learning algorithms.",8 Conclusion,[0],[0]
BANDITSUM does not require sentence-level extractive labels and optimizes ROUGE scores between summaries generated by the model and abstractive reference summaries.,8 Conclusion,[0],[0]
"Empirical results show that our method performs better than or comparable to state-of-the-art extractive summarization models which must be pre-trained on extractive labels, and converges using significantly fewer update steps than competing approaches.",8 Conclusion,[0],[0]
"In future work, we will explore the direction of adding an extra coherence reward (Wu and Hu, 2018) to improve the quality of extracted summaries in terms of sentence discourse relation.",8 Conclusion,[0],[0]
The research was supported in part by Natural Sciences and Engineering Research Council of Canada (NSERC).,Acknowledgements,[0],[0]
The authors would like to thank Compute Canada for providing the computational resources.,Acknowledgements,[0],[0]
"In this work, we propose a novel method for training neural networks to perform singledocument extractive summarization without heuristically-generated extractive labels.",abstractText,[0],[0]
"We call our approach BANDITSUM as it treats extractive summarization as a contextual bandit (CB) problem, where the model receives a document to summarize (the context), and chooses a sequence of sentences to include in the summary (the action).",abstractText,[0],[0]
A policy gradient reinforcement learning algorithm is used to train the model to select sequences of sentences that maximize ROUGE score.,abstractText,[0],[0]
"We perform a series of experiments demonstrating that BANDITSUM is able to achieve ROUGE scores that are better than or comparable to the state-of-the-art for extractive summarization, and converges using significantly fewer update steps than competing approaches.",abstractText,[0],[0]
"In addition, we show empirically that BANDITSUM performs significantly better than competing approaches when good summary sentences appear late in the source document.",abstractText,[0],[0]
BANDITSUM: Extractive Summarization as a Contextual Bandit,title,[0],[0]
The advancement of modern society is driven by the development of Integrated Circuits (IC).,1. Introduction,[0],[0]
"Unlike the digital circuits where the design flow is already highly automated, the automation of analog circuit design is still a challenging problem.
",1. Introduction,[0],[0]
"Traditionally, the design parameters of analog circuits like widths and lengths of transistors are manually calculated by designers with their experience and the understanding of the design specifications.",1. Introduction,[0],[0]
"However, due to the progress
1State Key Lab of ASIC and System, School of Microelectronics, Fudan University, Shanghai, China 2Department of Electrical Engineering, University of Texas at Dallas, Richardson, TX, U.S.A. Correspondence to: Fan Yang <yangfan@fudan.edu.cn>, Xuan Zeng <xzeng@fudan.edu.cn>.
Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"of IC manufacture technology forecasted by Moore’s law, the circuit devices become more and more complicated, and the parasitic effect of the circuits can no longer be ignored.",1. Introduction,[0],[0]
"On the other hand, the demands for high-performance, lowpower analog circuits are increasing.",1. Introduction,[0],[0]
It is much more difficult to meet the performance and time-to-market requirements with manual circuit design.,1. Introduction,[0],[0]
"Automated analog circuit design has thus attracted much research interest in the past decade (Rutenbar et al., 2007).
",1. Introduction,[0],[0]
The analog circuit design automation problems can be formulated as optimization problems.,1. Introduction,[0],[0]
"The aim is to find the optimal design parameters that provide the best circuit performance, which can be represented by a figure of merit (FOM) real-valued function.",1. Introduction,[0],[0]
"Prior works about analog circuit optimization include offline model-based approaches (Colleran et al., 2003; Daems et al., 2003; Wang et al., 2014) and simulation-based approaches.",1. Introduction,[0],[0]
The offline model-based methods try to build global models of the FOM via manual calculation or regression with simulated data and then optimize the cheap-to-evaluate models.,1. Introduction,[0],[0]
The problem with this approach is that the accurate models are usually hard to get.,1. Introduction,[0],[0]
"For example, in Wang et al. (2014), 100,000 randomly simulated points are used to train a sparse polynomial model for an amplifier circuit with ten design parameters.
",1. Introduction,[0],[0]
"Simulation-based methods, instead, treat the performances of the circuits as black-box functions.",1. Introduction,[0],[0]
The performances are obtained from circuit simulations.,1. Introduction,[0],[0]
Global optimization algorithms are directly applied to the black-box functions.,1. Introduction,[0],[0]
"For simulation-based circuit optimization methods, metaheuristic algorithms (Phelps et al., 2000; Liu et al., 2009) are widely used.",1. Introduction,[0],[0]
"Although these algorithms can explore the whole design space, they have relatively low convergence rate.",1. Introduction,[0],[0]
"When the circuit simulation takes a long time, both model-based and simulation-based approaches can be very time-consuming.
",1. Introduction,[0],[0]
"In recent years, the Gaussian process (GP) (Rasmussen, 2006) model has been introduced for the automated design of analog circuits to reduce the required number of circuit simulations.",1. Introduction,[0],[0]
"In Liu et al. (2014), GP is combined with differential evolution algorithm.",1. Introduction,[0],[0]
"Recently, Bayesian optimization (BO) (Shahriari et al., 2016) algorithm has also been applied for analog circuit optimization.",1. Introduction,[0],[0]
"In Lyu et al. (2017),
Bayesian optimization algorithm is firstly introduced for the single- and multi-objective optimization of general analog circuits and has shown to be much more efficient compared with other simulation-based approaches.",1. Introduction,[0],[0]
"In Wang et al. (2017), Bayesian optimization algorithm is combined with adaptive Monte-Carlo sampling to optimize the yield of analog circuits and static random-access memory (SRAM).
",1. Introduction,[0],[0]
Bayesian optimization algorithm is a well-studied algorithm and has demonstrated to be promising for the automated design of analog circuits.,1. Introduction,[0],[0]
"However, the standard Bayesian optimization algorithm is sequential.",1. Introduction,[0],[0]
It chooses only one point at each iteration by optimizing the acquisition function.,1. Introduction,[0],[0]
It is often desirable to select a batch of points at each iteration.,1. Introduction,[0],[0]
"The sequential property of Bayesian optimization limits its further applications in multi-core computer systems.
",1. Introduction,[0],[0]
Bayesian optimization algorithm has been extended to enable batch selection.,1. Introduction,[0],[0]
"Some prior works, like the qEI (Chevalier & Ginsbourger, 2013), qKG (Wu & Frazier, 2016) and parallel predictive entropy search (PPES) (Shah & Ghahramani, 2015) approaches, consider to search for the optimal batch selection for a specific acquisition function.",1. Introduction,[0],[0]
"These methods usually involve some approximations or MonteCarlo sampling, and thus scale poorly as the batch size increases.",1. Introduction,[0],[0]
"Other works, including the simulation matching (SM) (Azimi et al., 2010) method, the batch-UCB (BUCB, BLCB for minimization problems) (Desautels et al., 2014) method, the parallel UCB with pure exploration (GP-UCBPE) (Contal et al., 2013) method, and the local penalization (LP) (González et al., 2016) method, adopted the greedy strategies that select individual points until the batch is filled.
",1. Introduction,[0],[0]
All the batch Bayesian optimization algorithms mentioned above choose to use single acquisition function.,1. Introduction,[0],[0]
"And except for the SM method (Azimi et al., 2010) and LP method (González et al., 2016) which can use arbitrary acquisition function, other parallelization methods rely on a specific acquisition function.",1. Introduction,[0],[0]
"The UCB acquisition function must be used for BUCB and GP-UCB-PE, and the knowledge gradient (KG) acquisition function must be used for the qKG algorithm.",1. Introduction,[0],[0]
"As is stated in Hoffman et al. (2011), no single acquisition function can always outperform other acquisition functions.",1. Introduction,[0],[0]
"Relying on one acquisition function may result in poor performance.
",1. Introduction,[0],[0]
"In this paper, we propose to parallelize Bayesian optimization algorithm via the Multi-objective ACquisition Ensemble (MACE).",1. Introduction,[0],[0]
The proposed MACE method exploits the disagreement between different acquisition functions to enable batch selection.,1. Introduction,[0],[0]
"At each iteration, after the GP model is updated, multiple acquisition functions are selected.",1. Introduction,[0],[0]
We then perform multi-objective optimization to find the Pareto front (PF) of the acquisition functions.,1. Introduction,[0],[0]
The PF represents the best trade-off between these acquisition functions.,1. Introduction,[0],[0]
"When batch
evaluations are possible, we can sample multiple points on the PF to accelerate the optimization.
",1. Introduction,[0],[0]
"The MACE algorithm is tested using several analytical benchmark functions and two real-world analog circuits, including an operational amplifier with ten design parameters and a class-E power amplifier with twelve design parameters.",1. Introduction,[0],[0]
"The BLCB method (Desautels et al., 2014), local penalization method with expected improvement acquisition function (EI-LP) (González et al., 2016), qEI (Chevalier & Ginsbourger, 2013) and qKG (Wu & Frazier, 2016) methods are compared with MACE.",1. Introduction,[0],[0]
The proposed MACE method achieved competitive performance when compared with the state-of-the-art algorithms listed in the paper.,1. Introduction,[0],[0]
"In this section, we will present the problem formulation of analog circuit optimization, and review the background of Gaussian process regression and Bayesian optimization.",2. Background,[0],[0]
"When designing integrated circuits, the designers have to decide what circuit topology to use and then set the corrsponding design parameters.",2.1. Problem Formulation,[0],[0]
"In this work, we handle the scenarios where the topology of the analog circuit is fixed.",2.1. Problem Formulation,[0],[0]
"This is practical as there are usually a lot of classical topologies for a given design task, so unlike digital circuits, choosing appropriate topology is relatively easy.
",2.1. Problem Formulation,[0],[0]
"Once the circuit topology is fixed, the designer has to choose the appropriate design parameters according to the specifications and the circuit device model.",2.1. Problem Formulation,[0],[0]
What we want to do is automatically searching for the optimal design parameters.,2.1. Problem Formulation,[0],[0]
"This problem can then be formulated as a bound-constrained black-box optimization problem:
minimize FOM(x), (1)
where x ∈ D is the vector of design variables, FOM(x) is the objective constructed from the design specifications, the FOM(x) can be deterministric or noisy depending on the design specifications.",2.1. Problem Formulation,[0],[0]
"Given the design parameters x, the FOM value can be obtained by commercial circuit simulators like HSPICE or Spectre.",2.1. Problem Formulation,[0],[0]
"The objective function FOM(x) in (1) can be approximated by Gaussian process (GP) model (Rasmussen, 2006).",2.2. Gaussian Process Regression,[0],[0]
The GP model is the most commonly used model for Bayesian optimization.,2.2. Gaussian Process Regression,[0],[0]
The advantage of GP is that it provides a well-calibrated uncertainty of prediction.,2.2. Gaussian Process Regression,[0],[0]
"GP is characterized by a mean function m(x) and a covariance function k(x,x′).",2.2. Gaussian Process Regression,[0],[0]
"In this work, we use squared-exponential ARD kernel (Rasmussen, 2006), and a constant mean function
m(x) = µ0 for all our experiments.",2.2. Gaussian Process Regression,[0],[0]
"By default, we assume the objective function evaluations are influenced by i.i.d. noise t ∼ N(0, σ2n) and set the noise level σ2n as a hyperparameter.",2.2. Gaussian Process Regression,[0],[0]
"The introduction of the i.i.d noise also helps to improve the numerical stability.
",2.2. Gaussian Process Regression,[0],[0]
"Denote the training set as {X,y} where X = {x1, . . .",2.2. Gaussian Process Regression,[0],[0]
",xN} and y = {y1, . .",2.2. Gaussian Process Regression,[0],[0]
.,2.2. Gaussian Process Regression,[0],[0]
", yN}, given a new data point x, the prediction of f(x) is not a scalar value, but a predictive distribution
f(x) ∼ N(µ(x), σ2(x)), (2)
where µ(x) and σ2(x) can be expressed as
µ(x) = µ0 + k(x, X)[K +",2.2. Gaussian Process Regression,[0],[0]
σ 2 nI] −1(y,2.2. Gaussian Process Regression,[0],[0]
"− µ0) σ2(x) = k(x,x)− k(x, X)[K + σ2nI]−1k(X,x),
(3)",2.2. Gaussian Process Regression,[0],[0]
"where k(x, X) = (k(x,x1), . .",2.2. Gaussian Process Regression,[0],[0]
.,2.2. Gaussian Process Regression,[0],[0]
", k(x,xN ))",2.2. Gaussian Process Regression,[0],[0]
"T and k(X,x) = k(x, X)T .",2.2. Gaussian Process Regression,[0],[0]
"The µ(x) can be viewed as the prediction of the function value, while the σ2(x) is a measure of uncertainty of the prediction.",2.2. Gaussian Process Regression,[0],[0]
"Bayesian optimization (Shahriari et al., 2016) was proposed for the optimization of expensive black-box functions.",2.3. Bayesian Optimization,[0],[0]
"It consists of two essential ingredients, i.e., the probabilistic surrogate models and the acquisition functions.",2.3. Bayesian Optimization,[0],[0]
The probabilistic surrogate models provide predictions with uncertainties.,2.3. Bayesian Optimization,[0],[0]
The acquisition functions make use of the predictive distribution to explore the state space.,2.3. Bayesian Optimization,[0],[0]
"The procedure of Bayesian optimization is summarized in Algorithm 1.
",2.3. Bayesian Optimization,[0],[0]
"Algorithm 1 Bayesian Optimization Require: Number of initial sampling points Ninit, number
of iterations Niter 1: Randomly sample Ninit points in the design space 2:",2.3. Bayesian Optimization,[0],[0]
"Construct initial GP model 3: for t = 1, 2, . . .",2.3. Bayesian Optimization,[0],[0]
", Niter do 4: Construct the acquisition function 5: Find xt that optimizes the acquisition function 6: Sample yt = f(xt) 7: Update probabilistic surrogate model 8: end for 9: Return best f(x) recorded during iterations
In Bayesian optimization described in Algorithm 1, the acquisition function is used to balance the exploration and exploitation during the optimization.",2.3. Bayesian Optimization,[0],[0]
The acquisition function considers both the predictive value and the uncertainty.,2.3. Bayesian Optimization,[0],[0]
There are a lot of existing acquisition functions.,2.3. Bayesian Optimization,[0],[0]
"Examples include the lower confidence bound (LCB), the probability of improvement (PI), and the expected improvement (EI).
",2.3. Bayesian Optimization,[0],[0]
"The LCB function is defined as follows:
LCB(x) = µ(x)− κσ(x), (4)
where the µ(x) and the σ(x) are the predictive value and uncertainty of GP defined in (3), κ is a parameter that balances the exploitation and exploration.
",2.3. Bayesian Optimization,[0],[0]
"Following the suggestion of (Srinivas et al., 2010; Brochu et al., 2010), the κ in (4) is defined as:
κ = √ ντt τt",2.3. Bayesian Optimization,[0],[0]
"= 2 log(t d/2+2π2/3δ),
(5)
where t is the number of current iteration, ν and δ are two user-defined parameters.",2.3. Bayesian Optimization,[0],[0]
"We fix ν = 0.5 and δ = 0.05 in this paper for the proposed MACE algorithm and our implementation of the BLCB algorithm.
",2.3. Bayesian Optimization,[0],[0]
"The PI and EI functions are defined as
PI(x) = Φ(λ) EI(x) = σ(x)(λΦ(λ) + φ(λ))",2.3. Bayesian Optimization,[0],[0]
λ,2.3. Bayesian Optimization,[0],[0]
= τ,2.3. Bayesian Optimization,[0],[0]
− ξ,2.3. Bayesian Optimization,[0],[0]
"− µ(x)
σ(x) ,
(6)
where τ is the current best value objective value, and ξ is a small positive jitter to improvement the ability of exploration.",2.3. Bayesian Optimization,[0],[0]
The Φ(.) and φ(.) functions are the CDF and PDF functions of normal distribution.,2.3. Bayesian Optimization,[0],[0]
"In our implementation of the MACE algorithm, we fix ξ = 1e-3.
",2.3. Bayesian Optimization,[0],[0]
"There are also other acquisition functions, like the knowledge gradient (Scott et al., 2011) function, predictive entropy search (Hernández-Lobato et al., 2014), and the max-value entropy search(Wang & Jegelka, 2017).",2.3. Bayesian Optimization,[0],[0]
"A portfolio of several acquisition functions is also possible (Hoffman et al., 2011).",2.3. Bayesian Optimization,[0],[0]
We will present the proposed batch Bayesian optimization algorithm in this section.,3. Proposed Batch Bayesian Optimization Algorithm,[0],[0]
"Unlike single-objective optimization, there are multiple objectives to optimize in multi-objective optimization problems(Marler & Arora, 2004).",3.1. Multi-objective Optimization,[0],[0]
"The multi-objective optimization problem is formulated as
minimize f1(x), . . .",3.1. Multi-objective Optimization,[0],[0]
", fm(x).",3.1. Multi-objective Optimization,[0],[0]
"(7)
The multiple objectives to be optimized can be conflicting so that it is usually impossible to find a single solution that is the optimum of all objectives.",3.1. Multi-objective Optimization,[0],[0]
"The goal of multi-objective optimization algorithms is to approximate the Pareto front of
the objectives.",3.1. Multi-objective Optimization,[0],[0]
A solution x1 is said to dominate x2 if ∀i ∈ {1 . .,3.1. Multi-objective Optimization,[0],[0]
".m}, fi(x1) ≤ fi(x2) and ∃j ∈ {1 . .",3.1. Multi-objective Optimization,[0],[0]
".m}, fj(x1) < fj(x2).",3.1. Multi-objective Optimization,[0],[0]
A design is Pareto-optimal if it is not dominated by any other point in the design space and dominates at least one point.,3.1. Multi-objective Optimization,[0],[0]
"The whole set of the Pareto-optimal points in the design space is called the Pareto set, and the set of Pareto-optimal points in the objective space is called the Pareto front.",3.1. Multi-objective Optimization,[0],[0]
"It is often unlikely to get the whole Pareto front as there might be infinite points on the Paret front, multi-objective optimization algorithms try to find a set of evenly distributed solutions that approximate the true Pareto front.
",3.1. Multi-objective Optimization,[0],[0]
"There exist many mature multi-objective optimization algorithms, like the non-dominated sorting based genetic algorithm (NSGA-II) (Deb et al., 2002), and the multiobjective evolutionary algorithm based on decomposition (MOEA/D) (Zhang & Li, 2007).",3.1. Multi-objective Optimization,[0],[0]
"In this paper, the multi-objective optimization based on differential evolution (DEMO) (Robič & Filipič, 2005) is used to solve multiobjective optimization problems, but other multi-objective optimization algorithms can also be applied.",3.1. Multi-objective Optimization,[0],[0]
"Each acquisition function represents a unique selection strategy, different acquisition functions may not agree with each other about where to sample the next point.",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"For example, the value of LCB function always decreases as the σ(x) increases.",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"However, for the PI function, when σ(x) increases, the value of PI would decrease when µ(x) < τ , and increase when µ(x) > τ .",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"For the EI function, if the function is noiseless, the values of EI function at already sampled points would always be worse than the EI values at any
unsampled locations, while this property does not hold for the LCB function.
",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"Algorithm 2 Multi-objective Acquisition Ensemble Algorithm Require: Number of initial sampling points Ninit, number
of iterations Niter, batch size B. 1: Randomly sample Ninit points in the design space 2:",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"Construct initial GP model 3: for t = 1, 2, . . .",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
", Niter do 4: Construct the LCB, EI and PI functions according to (4) and (6) 5: Find the Pareto front of LCB, EI, PI functions using the DEMO algorithm 6: Randomly sample B points x1, . . .",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
",xB from the Pareto-optimal points 7: Evaluate x1, . . .",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
",xB to get y1 = f(x1), . . .",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
", yB = f(xB) 8: Update the GP model 9: end for
10: Return best f(x) recorded during iterations
With multi-objective optimization, the best trade-off between acquisition functions can be captured by the Pareto front of these acquisition functions.",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"We can then sample on the Pareto front to obtain multiple candidate points for the objective function evaluations.
",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
The proposed MACE algorithm is described in Algorithm 2.,3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"In the proposed MACE algorithm, the LCB, EI, and PI acquisition functions are selected.",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
Other acquisition functions like KG and PES can also be incorporated into the MACE framework.,3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"In each iteration, the following multi-objective
optimization problem is constructed:
minimize LCB(x), − EI(x), − PI(x).",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"(8)
Then the DEMO multi-objective optimization algorithm (Robič & Filipič, 2005) is applied to solve the multiobjective problem in (8).",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"Once the Pareto front of LCB, EI and PI is obtained, the candidate points are then randomly sampled from the Pareto front.
",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"In Figure 1, we illustrate the proposed MACE algorithm using an example of a real-world amplifier circuit.",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"The optimization objective is to maximize the phase margin (PM) of the amplifier, so the FOM is defined as FOM(x) = −PM(x).",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
The width of one of its transistor is the design variable.,3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
We sweep the width of the transistor and perform HSPICE simulations to get the FOM values.,3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
The curve of FOM values is plotted in Figure 1(a) (the blue line).,3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
Several points are randomly sampled from the FOM curve to train the GP model.,3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"The LCB, EI, PI functions and the Pareto front of the acquisition functions are plotted in Figure 1(b).",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"We can see from Figure 1(b) that the optimal locations of the three acquisition functions are different, while their best trade-off is captured by the Pareto front.",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
The Pareto set that represents the best trade-off between the three acquisition functions is the interval,3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"[43, 50.4], as plotted in Figure 1(a).",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
The candidate points for the next batch of evaluations are randomly sampled from the Pareto set.,3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
The proposed MACE algorithm1 was tested using eight benchmark functions and two real-world analog circuits.,4. Experimental Results,[0],[0]
"Four state-of-the-art parallel Bayesian optimization methods were compared, including the BLCB algorithm (Desautels et al., 2014), the local penalization method with EI acquisition function (EI-LP) (González et al., 2016), the qEI and qKG methods (Chevalier & Ginsbourger, 2013; Wu & Frazier, 2016).",4. Experimental Results,[0],[0]
"2
For the MACE, BLCB, and EI-LP method, the ARD squared-exponential kernel is used and the GP models are fitted by maximum likelihood estimations (MLE); for the qKG and qEI methods, the ARD Matern52 kernels are used, and the GP hyperparameters are integrated via MCMC sampling.",4. Experimental Results,[0],[0]
"The Matern52 kernel and MCMC integration are the default strategies of the qKG and qEI implementations and it is unclear in the documentation about how to change the GP settings.
",4. Experimental Results,[0],[0]
1Available at https://github.com/Alaya-in-Matrix/MACE 2We implemented the BLCB algorithm as the available open source implementations only allow discrete input.,4. Experimental Results,[0],[0]
"For the EI-LP method, the code is downloaded from https://github.com/SheffieldML/GPyOpt.",4. Experimental Results,[0],[0]
The code for qEI and qKG is downloaded from https://github.com/wujian16/CornellMOE.,4. Experimental Results,[0],[0]
"We tested the MACE algorithm and other parallel BO methods using eight commonly used benchmark functions, as summarized in Table 1.
",4.1. Benchmark Problems,[0],[0]
"For all functions except the two 10D functions, we set the number of initial random sampling to Ninit = 20 and the number of iterations toNiter = 45.",4.1. Benchmark Problems,[0],[0]
"Batch size is set toB = 4, the total number of function evaluations is Ninit +B × Niter.",4.1. Benchmark Problems,[0],[0]
"For the 10D Ackley and 10D Rosenbrock functions, we set Ninit = 100 and Niter = 175.",4.1. Benchmark Problems,[0],[0]
"The experiments were repeated ten times to average the random fluctuations.
",4.1. Benchmark Problems,[0],[0]
We also ran the MACE algorithm in sequential mode and compared with the EI and LCB acquisition functions.,4.1. Benchmark Problems,[0],[0]
"The sequential EI and LCB based Bayesian optimization are implemented by setting the batch size B = 1 for EI-LP and BLCB respectively.
",4.1. Benchmark Problems,[0],[0]
"The mean convergence plots of the tested algorithms on the benchmark functions are given in Figure 2, the statistics of the final regrets are listed in Table 2.",4.1. Benchmark Problems,[0],[0]
"As can be seen in Figure 2 and Table 2, when running in sequential mode, the MACE algorithm is competitive with the LCB and EI acquisition functions.",4.1. Benchmark Problems,[0],[0]
"The sequential MACE (MACE-1) algorithm gave better performances than the sequential EI (EI-1) and sequential LCB (LCB-1) algorithms in the Eggholder, Branin, Hartmann6, Ackley10, and Rosenbrock10 functions.",4.1. Benchmark Problems,[0],[0]
"Also, the parallel MACE (MACE-4) gave the best performances among all the tested algorithms for six out of the eight benchmark functions, and has shown dramatic speedup compared to the sequential MACE.",4.1. Benchmark Problems,[0],[0]
"We also performed additional experiments with varied batch sizes, the detail of those experimental results can be seen in the supplementary materials.",4.1. Benchmark Problems,[0],[0]
"We report the time spent on the ten-dimensional Rosenbrock function optimization with B = 4 as a measure of the algorithm overhead, for the ten-dimensional Rosenbrock function, it took MACE about 11 hours to finish all the Niter = 175 iterations, the BLCB algorithm took about five hours, for the EI-LP algorithm, it took only one hour to finish the optimization.",4.1. Benchmark Problems,[0],[0]
"The overheads for qEI and qKG are much larger, it took more than two days for qKG and qEI to
finish the optimization of the ten-dimensional Rosenbrock function.",4.1. Benchmark Problems,[0],[0]
"The operational amplifier (Wang et al., 2014) shown in Figure 3 is used to test Bayesian optimization algorithms.",4.2. Operational Amplifier,[0],[0]
The circuit is designed using the 180nm process.,4.2. Operational Amplifier,[0],[0]
"It has 10 design parameters, including the lengths and widths of transistors, the resistance of the resistors and the capacitance of the capacitors.",4.2. Operational Amplifier,[0],[0]
"The circuit is simulated using the commercial HSPICE circuit simulator.
",4.2. Operational Amplifier,[0],[0]
"We want to maximize the gain, unit gain frequency (UGF) and the phase margin (PM) for this amplifier.",4.2. Operational Amplifier,[0],[0]
"The Figure of Merit FOM is constructed as
FOM = −1.2× gain − 10×UGF − 1.6× PM .
",4.2. Operational Amplifier,[0],[0]
"For this circuit, we compared the MACE algorithm with the BLCB and EI-LP algorithms.",4.2. Operational Amplifier,[0],[0]
"The qKG and qEI are not compared as the computation of qEI and qKG acquisition functions become very slow for the ten-dimensional functions.
",4.2. Operational Amplifier,[0],[0]
We run the algorithms in sequential mode and batch mode.,4.2. Operational Amplifier,[0],[0]
"For the batch mode, the batch size is set to B = 4.",4.2. Operational Amplifier,[0],[0]
"The number of initial random sampling is set to Ninit = 100, and the number of iterations is set to Niter = 100.
",4.2. Operational Amplifier,[0],[0]
The mean convergence plot for the sequential and batch runs are given in Figure 4.,4.2. Operational Amplifier,[0],[0]
The mean and standard deviation of the final optimized FOM values are listed in Table 3.,4.2. Operational Amplifier,[0],[0]
"As can be seen, on average, the batch MACE algorithm had the fastest convergence rate compared with the sequential MACE algorithm and other parallel algorithms.",4.2. Operational Amplifier,[0],[0]
"It should also be noted that the final optimized FOM values given by MACE-4 have very small deviation (0.105) compared with other algorithms.
",4.2. Operational Amplifier,[0],[0]
Table 2.,4.2. Operational Amplifier,[0],[0]
"Statistics of the regrets of the benchmark functions
Eggholder Branin Alpine1 Hartmann6
MACE-1 87.65±75.83 1.05e-5±1.31e-5 2.66305±1.05844 0.0646869±0.0621189 LCB-1 153.9±112.8 6.86e-5±1.13e-4 5.66812±1.76973 0.125565±0.122684 EI-1",4.2. Operational Amplifier,[0],[0]
172.8±132.2 1.62e-2±1.63e-2 2.46061±1.56079 0.110561±0.146809,4.2. Operational Amplifier,[0],[0]
MACE-4,4.2. Operational Amplifier,[0],[0]
46.38±40.89 4.62e-6±6.64e-6 0.903805±0.835209 0.0275738±0.052254 BLCB-4 56.86±35.91 4.32e-5±6.33e-5 1.8843±0.938873 0.06447±0.0621176 EI-LP-4 44.68±56.45,4.2. Operational Amplifier,[0],[0]
"2.11e-2±1.84e-2 1.0059±0.456865 0.0540446±0.0558557 qKG-4 106.4±67.64 2.65e-1±2.70e-1 3.01513±1.13414 0.47134±0.18939 qEI-4 72.13±52.08 3.29e-4±1.14e-3 2.7074±1.05145 0.186088±0.116323
Ackley2 Rosenbrock2 Ackley10",4.2. Operational Amplifier,[0],[0]
"Rosenbrock10
MACE-1 1.71474±1.12154 0.026173±0.051189 3.1348±0.447874 499.697±300.899 LCB-1 1.624±0.926437 0.0201124±0.0205367 3.14797±0.519164 517.944±288.955 EI-1 1.0136±0.985858",4.2. Operational Amplifier,[0],[0]
13.5508±9.52734 18.8006±0.652136 1367.08±637.507 MACE-4 1.07906±0.886466 0.00095416±0.00093729 2.56439±0.535488 158.116±50.0024 BLCB-4 1.40051±1.02849 0.00191986±0.00180895 3.27543±0.735501 406.819±127.351 EI-LP-4 0.284265±0.24634 2.73645±2.05923,4.2. Operational Amplifier,[0],[0]
18.2682±0.608564 721.351±327.365 qKG-4 5.59394±1.80595 5.03976±3.72014 18.197±0.764103 705.112±412.762,4.2. Operational Amplifier,[0],[0]
qEI-4 2.87373±1.02405 10.1881±15.0432,4.2. Operational Amplifier,[0],[0]
"18.3686±0.501869 655.208±340.954
Vin
C0
C1
Vdd1=2.5V
Vg
Vdd2=1.8V
M4
M3
M2
M1
L3 C2
C3
Vout
RLC1
L1
Cc
R0
R1
L2
Figure 5.",4.2. Operational Amplifier,[0],[0]
Schematic of the power amplifier,4.2. Operational Amplifier,[0],[0]
The class-E power amplifier shown in Figure 5 is used to test Bayesian optimization algorithms.,4.3. Class-E Power Amplifier,[0],[0]
"The circuit is designed using the 180nm process with 12 design parameters, the circuit is simulated by the commercial HSPICE circuit simulator to get its performances.
",4.3. Class-E Power Amplifier,[0],[0]
"For this power amplifier, we aim to maximize the power added efficiency (PAE) and the output power (Pout), the Figure of Merit FOM is constructed as
FOM = −3× PAE − Pout .
",4.3. Class-E Power Amplifier,[0],[0]
"The MACE, BLCB, and EI-LP algorithms were tested in both sequential and batch modes.",4.3. Class-E Power Amplifier,[0],[0]
The number of initial sampling is Ninit = 100.,4.3. Class-E Power Amplifier,[0],[0]
"The number of iterations is
Niter = 100.",4.3. Class-E Power Amplifier,[0],[0]
The batch size is set to B = 4.,4.3. Class-E Power Amplifier,[0],[0]
"The total number of HSPICE simulations is 500 for each batch run and 200 for each sequential run.
",4.3. Class-E Power Amplifier,[0],[0]
The optimization results of the class-E power amplifier are given in Figure 6 and Table 4.,4.3. Class-E Power Amplifier,[0],[0]
We can see that the MACE outperformed the BLCB and EI-LP in both sequential and batch mode.,4.3. Class-E Power Amplifier,[0],[0]
"For the batch runs, the MACE converges fastest among the three algorithms, while the sequential MACE (MACE-1) has comparable performance as the batch EI-LP (EI-LP-4) method.",4.3. Class-E Power Amplifier,[0],[0]
"In this paper, a batch Bayesian optimization algorithm is proposed for the automation of analog circuit design.",5. Conclusion,[0],[0]
The parallelization is achieved via the multi-objective ensemble of acquisition functions.,5. Conclusion,[0],[0]
"In each iteration, the candidate points are sampled from the Pareto front of multiple acquisition functions.",5. Conclusion,[0],[0]
"We compared the proposed MACE algorithm using analytical benchmark functions and real-world circuits, it is shown that the MACE algorithm is competitive compared with the state-of-the-art methods listed in the paper.",5. Conclusion,[0],[0]
"This research was supported partly by the National Major Science and Technology Special Project of China (2017ZX01028101-003), partly by National Key Research and Development Program of China 2016YFB0201304, partly by National Natural Science Foundation of China (NSFC) research projects 61774045, 61574044, 61474026, 61574046, 61674042, and 61628402 and partly by the Recruitment Program of Global Experts (the Thousand Talents Plan).",Acknowledgements,[0],[0]
Bayesian optimization methods are promising for the optimization of black-box functions that are expensive to evaluate.,abstractText,[0],[0]
"In this paper, a novel batch Bayesian optimization approach is proposed.",abstractText,[0],[0]
The parallelization is realized via a multi-objective ensemble of multiple acquisition functions.,abstractText,[0],[0]
"In each iteration, the multi-objective optimization of the multiple acquisition functions is performed to search for the Pareto front of the acquisition functions.",abstractText,[0],[0]
The batch of inputs are then selected from the Pareto front.,abstractText,[0],[0]
The Pareto front represents the best trade-off between the multiple acquisition functions.,abstractText,[0],[0]
Such a policy for batch Bayesian optimization can significantly improve the efficiency of optimization.,abstractText,[0],[0]
The proposed method is compared with several state-of-the-art batch Bayesian optimization algorithms using analytical benchmark functions and real-world analog integrated circuits.,abstractText,[0],[0]
The experimental results show that the proposed method is competitive compared with the state-of-the-art algorithms.,abstractText,[0],[0]
Batch Bayesian Optimization via Multi-objective Acquisition Ensemble for Automated Analog Circuit Design,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1853–1862 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1853",text,[0],[0]
"Representing words using dense and real-valued vectors, aka word embeddings, has become the cornerstone for many natural language processing (NLP) tasks, such as document classification (Sebastiani, 2002), parsing (Huang et al., 2012), discourse relation recognition (Lei et al., 2017) and named entity recognition (Turian et al., 2010).",1 Introduction,[0],[0]
"Word embeddings can be learned by optimizing that words occurring in similar contexts have similar embeddings, i.e. the well-known distributional hypothesis (Harris, 1954).",1 Introduction,[0],[0]
"A representative method is skip-gram (SG) (Mikolov et al., 2013a,b), which realizes the hypothesis using a
∗The first two authors contributed equally to this paper and share the first-authorship.
",1 Introduction,[0],[0]
shallow neural network model.,1 Introduction,[0],[0]
"The other family of methods is count-based, such as GloVe (Pennington et al., 2014) and LexVec (Salle et al., 2016a,b), which exploit low-rank models such as matrix factorization (MF) to learn embeddings by reconstructing the word co-occurrence statistics.
",1 Introduction,[0],[0]
"By far, most state-of-the-art embedding methods rely on SGD and negative sampling for optimization.",1 Introduction,[0],[0]
"However, the performance of SGD is highly sensitive to the sampling distribution and the number of negative samples (Chen et al., 2018; Yuan et al., 2016), as shown in Figure 1.",1 Introduction,[0],[0]
"Essentially, sampling is biased, making it difficult to converge to the same loss with all examples, regardless of how many update steps have been taken.",1 Introduction,[0],[0]
"Moreover, SGD exhibits dramatic fluctuation and suffers from overshooting on local minimums (Ruder, 2016).",1 Introduction,[0],[0]
"These drawbacks of SGD can be attributed to its one-sample learning scheme, which updates parameters based on one training sample in each step.
",1 Introduction,[0],[0]
"To address the above-mentioned limitations of SGD, a natural solution is to perform exact (full) batch learning.",1 Introduction,[0],[0]
"In contrast to SGD, batch learning does not involve any sampling procedure and computes the gradient over all training samples.",1 Introduction,[0],[0]
"As such, it can easily converge to a better optimum in a more stable way.",1 Introduction,[0],[0]
"Nevertheless, a well-known
difficulty in applying full batch learning lies in the expensive computational cost for large-scale data.",1 Introduction,[0],[0]
"Taking the word embedding learning as an example, if the vocabulary size is |V |, then evaluating the loss function and computing the full gradient takes O(|V |2k) time, where k is the embedding size.",1 Introduction,[0],[0]
"This high complexity is unaffordable in practice, since |V |2 can easily reach billion level or even higher.
",1 Introduction,[0],[0]
"In this paper, we introduce AllVec, an exact and efficient word embedding method based on full batch learning.",1 Introduction,[0],[0]
"To address the efficiency challenge in learning from all training samples, we devise a regression-based loss function for word embedding, which allows fast optimization with memorization strategies.",1 Introduction,[0],[0]
"Specifically, the acceleration is achieved by reformulating the expensive loss over all negative samples using a partition and a decouple operation.",1 Introduction,[0],[0]
"By decoupling and caching the bottleneck terms, we succeed to use all samples for each parameter update in a manageable time complexity which is mainly determined by the positive samples.",1 Introduction,[0],[0]
"The main contributions of this work are summarized as follows:
• We present a fine-grained weighted least square loss for learning word embeddings.",1 Introduction,[0],[0]
"Unlike GloVe, it explicitly accounts for all negative samples and reweights them with a frequency-aware strategy.
",1 Introduction,[0],[0]
• We propose an efficient and exact optimization algorithm based on full batch gradient optimization.,1 Introduction,[0],[0]
"It has a comparable time complexity with SGD, but being more effective and stable due to the consideration of all samples in each parameter update.
",1 Introduction,[0],[0]
"• We perform extensive experiments on several benchmark datasets and tasks to demonstrate the effectiveness, efficiency, and convergence property of our AllVec method.",1 Introduction,[0],[0]
"Mikolov et al. (2013a,b) proposed the skip-gram model to learn word embeddings.",2.1 Skip-gram with Negative Sampling,[0],[0]
"SG formulates the problem as a predictive task, aiming at predicting the proper context c for a target word w within a local window.",2.1 Skip-gram with Negative Sampling,[0],[0]
"To speed up the training process, it applies the negative sampling (Mikolov et al., 2013b) to approximate the full softmax.",2.1 Skip-gram with Negative Sampling,[0],[0]
"That is,
each positive (w, c) pair is trained with n randomly sampled negative pairs (w,wi).",2.1 Skip-gram with Negative Sampling,[0],[0]
"The sampled loss function of SG is defined as
LSGwc =log σ(UwŨ T c )+ n∑ i=1",2.1 Skip-gram with Negative Sampling,[0],[0]
"Ewi∼Pn(w) log σ(−UwŨ T wi)
where Uw and Ũc denote the k-dimensional embedding vectors for word w and context c. Pn(w) is the distribution from which negative context wi is sampled.
",2.1 Skip-gram with Negative Sampling,[0],[0]
"Plenty of research has been done based on SG, such as the use of prior knowledge from another source (Kumar and Araki, 2016; Liu et al., 2015a; Bollegala et al., 2016), incorporating word type information (Cao and Lu, 2017; Niu et al., 2017), character level n-gram models (Bojanowski et al., 2016; Joulin et al., 2016) and jointly learning with topic models like LDA (Shi et al., 2017; Liu et al., 2015b).",2.1 Skip-gram with Negative Sampling,[0],[0]
Mikolov et al. (2013b) showed that the unigram distribution raised to the 3/4th power as Pn(w) significantly outperformed both the unigram and the uniform distribution.,2.2 Importance of the Sampling Distribution,[0],[0]
This suggests that the sampling distribution (of negative words) has a great impact on the embedding quality.,2.2 Importance of the Sampling Distribution,[0],[0]
"Furthermore, Chen et al. (2018) and Guo et al. (2018) recently found that replacing the original sampler with adaptive samplers could result in better performance.",2.2 Importance of the Sampling Distribution,[0],[0]
The adaptive samplers are used to find more informative negative examples during the training process.,2.2 Importance of the Sampling Distribution,[0],[0]
"Compared with the original word-frequency based sampler, adaptive samplers adapt to both the target word and the current state of the model.",2.2 Importance of the Sampling Distribution,[0],[0]
They also showed that the finegrained samplers not only speeded up the convergence but also significantly improved the embedding quality.,2.2 Importance of the Sampling Distribution,[0],[0]
"Similar observations were also found in other fields like collaborative filtering (Yuan et al., 2016).",2.2 Importance of the Sampling Distribution,[0],[0]
"While being effective, it is proven that negative sampling is a biased approximation and does not converges to the same loss as the full softmax — regardless of how many update steps have been taken (Bengio and Senécal, 2008; Blanc and Rendle, 2017).",2.2 Importance of the Sampling Distribution,[0],[0]
"Another line of research is the count-based embedding, such as GloVe (Pennington et al., 2014).",2.3 Count-based Embedding Methods,[0],[0]
"GloVe performs a biased MF on the word-context co-occurrence statistics, which is a common ap-
proach in the field of collaborative filtering (Koren, 2008).",2.3 Count-based Embedding Methods,[0],[0]
"However, GloVe only formulates the loss on positive entries of the co-occurrence matrix, meaning that negative signals about wordcontext co-occurrence are discarded.",2.3 Count-based Embedding Methods,[0],[0]
"A remedy solution is LexVec (Salle et al., 2016a,b) which integrates negative sampling into MF.",2.3 Count-based Embedding Methods,[0],[0]
"Some other methods (Li et al., 2015; Stratos et al., 2015; Ailem et al., 2017) also use MF to approximate the word-context co-occurrence statistics.",2.3 Count-based Embedding Methods,[0],[0]
"Although predictive models and count-based models seem different at first glance, Levy and Goldberg (2014) proved that SG with negative sampling is implicitly factorizing a shifted pointwise mutual information (PMI) matrix, which means that the two families of embedding models resemble each other to a certain degree.
",2.3 Count-based Embedding Methods,[0],[0]
Our proposed method departs from all above methods by using the full batch gradient optimizer to learn from all (positive and negative) samples.,2.3 Count-based Embedding Methods,[0],[0]
We propose a fast learning algorithm to show that such batch learning is not “heavy” even with tens of billions of training examples.,2.3 Count-based Embedding Methods,[0],[0]
"In this work, we adopt the regression loss that is commonly used in count-based models (Pennington et al., 2014; Stratos et al., 2015; Ailem et al., 2017) to perform matrix factorization on word cooccurrence statistics.",3 AllVec Loss,[0],[0]
"As highlighted, to retain the modeling fidelity, AllVec eschews using any sampling but optimizes the loss on all positive and negative word-context pairs.
",3 AllVec Loss,[0],[0]
"Given a word w and a symmetric window of win contexts, the set of positive contexts can be obtained by sliding through the corpus.",3 AllVec Loss,[0],[0]
"Let c denote a specific context, Mwc be the number of cooccurred (w, c) pairs in the corpus within the window.",3 AllVec Loss,[0],[0]
"Mwc=0 means that the pair (w, c) has never been observed, i.e. the negative signal.",3 AllVec Loss,[0],[0]
"rwc is the association coefficient between w and c, which is calculated from Mwc.",3 AllVec Loss,[0],[0]
"Specifically, we use r+wc to denote the ground truth value for positive (w, c) pairs and a constant value r−(e.g., 0 or -1) for negative ones since there is no interaction between w and c in negative pairs.",3 AllVec Loss,[0],[0]
"Finally, with all positive and negative pairs considered, a regular loss function can be given as Eq.(1), where V is the vocabulary and S is the set of positive pairs.",3 AllVec Loss,[0],[0]
"α+wc and α−wc represent the weight for positive and negative
(w, c) pairs, respectively.",3 AllVec Loss,[0],[0]
"L = ∑
(w,c)∈S α+wc(r + wc − UwŨTc )2︸ ︷︷ ︸
LP + ∑ (w,c)∈(V×V )",3 AllVec Loss,[0],[0]
"\S
α−wc(r − − UwŨTc )2︸ ︷︷ ︸
LN
(1)
When it comes to r+wc, there are several choices.",3 AllVec Loss,[0],[0]
"For example, GloVe applies the log of Mwc with bias terms for w and c. However, research from Levy and Goldberg (2014) showed that the SG model with negative sampling implicitly factorizes a shifted PMI matrix.",3 AllVec Loss,[0],[0]
"The PMI value for a (w, c) pair can be defined as
PMIwc = log P (w, c)
P (w)P (c) = log MwcM∗∗ Mw∗M∗c",3 AllVec Loss,[0],[0]
"(2)
where ‘*’ denotes the summation of all corresponding indexes (e.g., Mw∗= ∑ c∈V Mwc).",3 AllVec Loss,[0],[0]
"Inspired by this connection, we set r+wc as the positive point-wise mutual information (PPMI) which has been commonly used in the NLP literature (Stratos et al., 2015; Levy and Goldberg, 2014).",3 AllVec Loss,[0],[0]
"Sepcifically, PPMI is the positive version of PMI by setting the negative values to zero.",3 AllVec Loss,[0],[0]
"Finally, r+wc is defined as
r+wc = PPMIwc = max(PMIwc, 0) (3)",3 AllVec Loss,[0],[0]
"Regarding α+wc, we follow the design in GloVe, where it is defined as
α+wc =
{ (Mwc/xmax) ρ",3.1 Weighting Strategies,[0],[0]
"Mwc < xmax
1 Mwc ≥ xmax (4)
",3.1 Weighting Strategies,[0],[0]
"As for the weight for negative instances α−wc, considering that there is no interaction between w and negative c, we set α−wc as α − c (or α − w), which means that the weight is determined by the word itself rather than the word-context interaction.",3.1 Weighting Strategies,[0],[0]
Note that either α−wc = α − c or α − wc = α,3.1 Weighting Strategies,[0],[0]
− w does not influence the complexity of AllVec learning algorithm described in the next section.,3.1 Weighting Strategies,[0],[0]
"The design of α−c is inspired by the frequency-based oversampling scheme in skip-gram and missing data reweighting in recommendation (He et al., 2016).",3.1 Weighting Strategies,[0],[0]
The intuition is that a word with high frequency is more likely to be a true negative context word if there is no observed word-context interactions.,3.1 Weighting Strategies,[0],[0]
"Hence, to effectively differentiate the positive and negative examples, we assign a higher weight for the negative examples that have a higher word fre-
quency, and a smaller weight for infrequent words.",3.1 Weighting Strategies,[0],[0]
"Formally, α−wc is defined as
α−wc = α − c = α0 M δ∗c∑",3.1 Weighting Strategies,[0],[0]
"c∈V M δ ∗c
(5)
where α0 can be seen as a global weight to control the overall importance of negative samples.",3.1 Weighting Strategies,[0],[0]
α0 = 0 means that no negative information is utilized in the training.,3.1 Weighting Strategies,[0],[0]
The exponent δ is used for smoothing the weights.,3.1 Weighting Strategies,[0],[0]
"Specially, δ = 0 means a uniform weight for all negative examples and δ = 1 means that no smoothing is applied.",3.1 Weighting Strategies,[0],[0]
"Once specifying the loss function, the main challenge is how to perform an efficient optimization for Eq.(1).",4 Fast Batch Gradient Optimization,[0],[0]
"In the following, we develop a fast batch gradient optimization algorithm that is based on a partition reformulation for the loss and a decouple operation for the inner product.",4 Fast Batch Gradient Optimization,[0],[0]
"As can be seen, the major computational cost in Eq.(1) lies in the term LN , because the size of (V×V ) \S is very huge, which typically contains over billions of negative examples.",4.1 Loss Partition,[0],[0]
"To this end, we show our first key design that separates the loss of negative samples into the difference between the loss on all samples and that on positive samples1.",4.1 Loss Partition,[0],[0]
"The loss partition serves as the prerequisite for the efficient computation of full batch gradients.
",4.1 Loss Partition,[0],[0]
"LN= ∑ w∈V ∑ c∈V α−c (r −−UwŨTc )2− ∑ (w,c)∈S α−c (r −− UwŨTc )2 (6)
By replacing LN in Eq.(1) with Eq.(6), we can obtain a new loss function with a more clear structure.",4.1 Loss Partition,[0],[0]
We further simplify the loss function by merging the terms on positive examples.,4.1 Loss Partition,[0],[0]
"Finally, we achieve a reformulated loss
L = ∑ w∈V ∑ c∈V α−c (r −−UwŨTc ) 2
︸ ︷︷ ︸ LA
+ ∑
(w,c)∈S
(α+wc − α−c )(∆− UwŨTc ) 2
︸ ︷︷ ︸ L
P ′
+C (7)
where ∆ =",4.1 Loss Partition,[0],[0]
(α+wcr + wc − α−c r−)/(α+wc − α−c ).,4.1 Loss Partition,[0],[0]
It can be seen that the new loss function consists of two components: the loss LA on the whole V ×V training examples and LP ′ on positive examples.,4.1 Loss Partition,[0],[0]
"The major computation now lies in LA which has
1The idea here is similar to that used in (He et al., 2016; Li et al., 2016) for a different problem.
",4.1 Loss Partition,[0],[0]
a time complexity of O(k|V |2).,4.1 Loss Partition,[0],[0]
"In the following, we show how to reduce the huge volume of computation by a simple mathematical decouple.",4.1 Loss Partition,[0],[0]
"To clearly show the decouple operation, we rewrite LA as L̃A by omitting the constant term α−c (r
−)2.",4.2 Decouple,[0],[0]
"Note that uwd and ũcd denote the d-th element in Uw and Ũc, respectively.
",4.2 Decouple,[0],[0]
"L̃A = ∑ w∈V ∑ c∈V α−c k∑ d=0 uwdũcd k∑ d′=0 uwd′ ũcd′
− 2r− ∑ w∈V ∑ c∈V α−c k∑",4.2 Decouple,[0],[0]
"d=0 uwdũcd
(8)
Now we show our second key design that is based on a decouple manipulation for the inner product operation.",4.2 Decouple,[0],[0]
"Interestingly, we observe that the summation operator and elements in Uw and Ũc can be rearranged by the commutative property (Dai et al., 2007), as shown below.
",4.2 Decouple,[0],[0]
L̃A = k∑ d=0 k∑ d′=0 ∑ w∈V uwduwd′,4.2 Decouple,[0],[0]
"∑ c∈V α−c ũcdũcd′
− 2r− k∑
d=0 ∑ w∈V uwd ∑ c∈V α−c ũcd
(9)
",4.2 Decouple,[0],[0]
"An important feature in Eq.(9) is that the original inner product terms are disappeared, while in the new equation ∑ c∈V α",4.2 Decouple,[0],[0]
− c ũcdũcd′ and ∑ c∈V α,4.2 Decouple,[0],[0]
− c ũcd are “constant” values relative to uwduwd′ and uwd respectively.,4.2 Decouple,[0],[0]
This means that they can be pre-calculated before training in each iteration.,4.2 Decouple,[0],[0]
"Specifically, we define pwdd′ , p c",4.2 Decouple,[0],[0]
"dd′ , q w d and q c d as the pre-calculated terms
pwdd′",4.2 Decouple,[0],[0]
=,4.2 Decouple,[0],[0]
∑,4.2 Decouple,[0],[0]
"w∈V uwduwd′ q w d = ∑ w∈V uwd
pcdd′ = ∑ c∈V α−c ũcdũcd′ q c",4.2 Decouple,[0],[0]
d = ∑ c∈V α−c ũcd (10),4.2 Decouple,[0],[0]
Then the computation of L̃A can be simplified to∑k d=0 ∑k d′=0 p w dd′p,4.2 Decouple,[0],[0]
c,4.2 Decouple,[0],[0]
dd′,4.2 Decouple,[0],[0]
"− 2r−qwd qcd.
",4.2 Decouple,[0],[0]
"It can be seen that the time complexity to compute all pwdd′ is O(|V |k2), and similarly, O(|V |k2) for pcdd′ andO(|V |k) for qwd and qcd.",4.2 Decouple,[0],[0]
"With all terms pre-calculated before each iteration, the time complexity of computing L̃A is justO(k2).",4.2 Decouple,[0],[0]
"As a result, the total time complexity of computing LA is decreased toO(2|V |k2+2|V |k+k2)",4.2 Decouple,[0],[0]
"≈ O(2|V |k2), which is much smaller than the originalO(k|V |2).",4.2 Decouple,[0],[0]
"Moreover, it’s worth noting that our efficient computation for L̃A is strictly equal to its original value, which means AllVec does not introduce any approximation in evaluating the loss function.
",4.2 Decouple,[0],[0]
"Finally, we can derive the batch gradients for
uwd and ũcd as ∂L
∂uwd = k∑ d′=0 uwd′p c dd′",4.2 Decouple,[0],[0]
"− ∑ c∈I+w Λ · ũcd − r−qcd
∂L
∂ũcd = k∑ d′=0 ũcd′p w dd′α",4.2 Decouple,[0],[0]
"− c− ∑ w∈I+c Λ · uwd − r−α−c qwd (11) where I+w denotes the set of positive contexts for w, I+c denotes the set of positive words for c and Λ = (α+wc−α−c )(∆−UwŨTc ).",4.2 Decouple,[0],[0]
"Algorithm 1 shows the training procedure of AllVec.
",4.2 Decouple,[0],[0]
"Algorithm 1 AllVec learning Input: corpus Γ, win, α0, δ, iter, learning rate η Output: embedding matrices U and Ũ
1: Build vocabulary V from Γ 2:",4.2 Decouple,[0],[0]
"Obtain all positive (w, c) and Mwc from Γ 3: Compute all r+wc, α + wc and α − c 4: Initialize U and Ũ 5: for i = 1, ..., iter do 6: for d ∈ {0, .., k} do 7: Compute and store qcd .O(|V |k) 8: for d′ ∈ {0, .., k} do 9: Compute and store pcdd′ .O(|V",4.2 Decouple,[0],[0]
"|k 2) 10: end for 11: end for 12: for w ∈ V do 13: Compute Λ .O(|S|k) 14: for d ∈ {0, .., k} do 15: Update uwd .O(|S|k + |V |k2) 16: end for 17: end for 18: Repeat 6-17 for ũcd .O(2|S|k+2|V |k2) 19: end for",4.2 Decouple,[0],[0]
"In the following, we show that AllVec can achieve the same time complexity with negative sampling based SGD methods.
",4.3 Time Complexity Analysis,[0],[0]
"Given the sample size n, the total time complexity for SG is O((n + 1)|S|k), where n + 1 denotes n negative samples and 1 positive example.",4.3 Time Complexity Analysis,[0],[0]
"Regarding the complexity of AllVec, we can see that the overall complexity of Algorithm 1 is O(4|S|k + 4|V |k2).
",4.3 Time Complexity Analysis,[0],[0]
"For the ease of discussion, we denote c as the average number of positive contexts for a word in the training corpus, i.e. |S| = c|V | (c ≥ 1000 in most cases).",4.3 Time Complexity Analysis,[0],[0]
"We then obtain the ratio
4|S|k + 4|V |k2
(n+ 1)|S|k =
4
n+ 1 (1 +
k c ) (12)
where k is typically set from 100 to 300 (Mikolov et al., 2013a; Pennington et al., 2014), resulting in k ≤ c.",4.3 Time Complexity Analysis,[0],[0]
"Hence, we can give the lower and upper bound for the ratio:
4
n+1",4.3 Time Complexity Analysis,[0],[0]
"<
",4.3 Time Complexity Analysis,[0],[0]
"4|S|k+4|V |k2
(n+1)|S|k = 4 n+1 (1+ k c )≤ 8 n+1",4.3 Time Complexity Analysis,[0],[0]
"(13)
",4.3 Time Complexity Analysis,[0],[0]
The above analysis suggests that the complexity of AllVec is same as that of SGD with negative sample size between 3 and 7.,4.3 Time Complexity Analysis,[0],[0]
"In fact, considering that c is much larger than k in most datasets, the major cost of AllVec comes from the part 4|S|k (see Section 5.4 for details), which is linear with respect to the number of positive samples.",4.3 Time Complexity Analysis,[0],[0]
"We conduct experiments on three popular evaluation tasks, namely word analogy (Mikolov et al., 2013a), word similarity (Faruqui and Dyer, 2014) and QVEC (Tsvetkov et al., 2015).
",5 Experiments,[0],[0]
Word analogy task.,5 Experiments,[0],[0]
"The task aims to answer questions like, “a is to b as c is to ?”.",5 Experiments,[0],[0]
"We adopt the Google testbed2 which contains 19, 544 such questions in two categories: semantic and syntactic.",5 Experiments,[0],[0]
"The semantic questions are usually analogies about people or locations, like “king is to man as queen is to ?”, while the syntactic questions focus on forms or tenses, e.g., “swimming is to swim as running to ?”.
",5 Experiments,[0],[0]
Word similarity tasks.,5 Experiments,[0],[0]
"We perform evaluation on six datasets, including MEN (Bruni et al., 2012), MC (Miller and Charles, 1991), RW (Luong et al., 2013), RG (Rubenstein and Goodenough, 1965), WS-353 Similarity (WSim) and Relatedness (WRel) (Finkelstein et al., 2001).",5 Experiments,[0],[0]
"We compute the spearman rank correlation between the similarity scores calculated based on the trained embeddings and human labeled scores.
QVEC.",5 Experiments,[0],[0]
QVEC is an intrinsic evaluation metric of word embeddings based on the alignment to features extracted from manually crafted lexical resources.,5 Experiments,[0],[0]
"QVEC has shown strong correlation with the performance of embeddings in several semantic tasks (Tsvetkov et al., 2015).
",5 Experiments,[0],[0]
"We compare AllVec with the following word embedding methods.
",5 Experiments,[0],[0]
"• SG: This is the original skip-gram model with SGD and negative sampling (Mikolov et al., 2013a,b).",5 Experiments,[0],[0]
"• SGA: This is the skip-gram model with an
adaptive sampler (Chen et al., 2018).",5 Experiments,[0],[0]
"2https://code.google.com/archive/p/word2vec/
For all baselines, we use the original implementation released by the authors.",5 Experiments,[0],[0]
"We evaluate the performance of AllVec on four real-world corpora, namely Text83, NewsIR4, Wiki-sub and Wiki-all.",5.1 Datasets and Experimental Setup,[0],[0]
Wiki-sub is a subset of 2017 Wikipedia dump5.,5.1 Datasets and Experimental Setup,[0],[0]
"All corpora have been pre-processed by a standard pipeline (i.e. removing non-textual elements, lowercasing and tokenization).",5.1 Datasets and Experimental Setup,[0],[0]
"Table 1 summarizes the statistics of these corpora.
",5.1 Datasets and Experimental Setup,[0],[0]
"To obtain Mwc for positive (w, c) pairs, we follow GloVe where word pairs that are xwords apart contribute 1/x to Mwc.",5.1 Datasets and Experimental Setup,[0],[0]
The window size is set as win = 8.,5.1 Datasets and Experimental Setup,[0],[0]
"Regarding α+wc, we set xmax = 100 and ρ = 0.75.",5.1 Datasets and Experimental Setup,[0],[0]
"For a fair comparison, the embedding size k is set as 200 for all models and corpora.",5.1 Datasets and Experimental Setup,[0],[0]
"AllVec can be easily trained by AdaGrad (Zeiler, 2012) like GloVe or Newton-like (Bayer et al., 2017; Bradley et al., 2011) second order methods.",5.1 Datasets and Experimental Setup,[0],[0]
"For models based on negative sampling (i.e. SG, SGA and LexVec), the sample size is set as n = 25 for Text8, n = 10 for NewsIR and n = 5 for Wiki-sub and Wiki-all.",5.1 Datasets and Experimental Setup,[0],[0]
The setting is also suggested by Mikolov et al. (2013b).,5.1 Datasets and Experimental Setup,[0],[0]
Other detailed hyper-parameters are reported in Table 2.,5.1 Datasets and Experimental Setup,[0],[0]
We present results on the word analogy task in Table 2.,5.2 Accuracy Comparison,[0],[0]
"As shown, AllVec achieves the highest total accuracy (Tot.) in all corpora, particu-
3http://mattmahoney.net/dc/text8.zip 4http://research.signalmedia.co/newsir16/signal-
dataset.html 5https://dumps.wikimedia.org/enwiki/
larly in smaller corpora (Text8 and NewsIR).",5.2 Accuracy Comparison,[0],[0]
"The reason is that in smaller corpora the number of positive (w, c) pairs is very limited, thus making use of negative examples will bring more benefits.",5.2 Accuracy Comparison,[0],[0]
"Similar reason also explains the poor accuracy of GloVe in Text8, because GloVe does not consider negative samples.",5.2 Accuracy Comparison,[0],[0]
"Even in the very large corpus (Wiki-all), ignoring negative samples still results in sub-optimal performance.
",5.2 Accuracy Comparison,[0],[0]
"Our results also show that SGA achieves better performance than SG, which demonstrates the importance of a good sampling strategy.",5.2 Accuracy Comparison,[0],[0]
"However, regardless what sampler (except the full softmax sampling) is utilized and how many updates are taken, sampling is still a biased approach.",5.2 Accuracy Comparison,[0],[0]
"AllVec achieves the best performance because it is trained on the whole batch data for each parameter update rather than a fraction of sampled data.
",5.2 Accuracy Comparison,[0],[0]
Another interesting observation is AllVec performs better in semantic tasks in general.,5.2 Accuracy Comparison,[0],[0]
"The reason is that our model utilizes global co-occurrence statistics, which capture more semantic signals than syntactic signals.",5.2 Accuracy Comparison,[0],[0]
"While both AllVec and GloVe use global contexts, AllVec performs much better than GloVe in syntactic tasks.",5.2 Accuracy Comparison,[0],[0]
"We argue that the main reason is because AllVec can distill useful signals from negative examples, while GloVe simply ignores all negative information.",5.2 Accuracy Comparison,[0],[0]
"By contrast, local-window based methods, such as SG and SGA, are more effective to capture local sentence features, resulting in good performance on syntactic analogies.",5.2 Accuracy Comparison,[0],[0]
"However, Rekabsaz et al. (2017) argues that these local-window based methods may suffer from the topic shifting issue.
",5.2 Accuracy Comparison,[0],[0]
Table 3 and Table 4 provide results in the word similarity and QVEC tasks.,5.2 Accuracy Comparison,[0],[0]
"We can see that AllVec achieves the best performance in most tasks, which admits the advantage of batch learning with all samples.",5.2 Accuracy Comparison,[0],[0]
"Interestingly, although GloVe performs well in semantic analogy tasks, it shows extremely worse results in word similarity and QVEC.",5.2 Accuracy Comparison,[0],[0]
The reason shall be the same as that it performs poorly in syntactic tasks.,5.2 Accuracy Comparison,[0],[0]
"In this subsection, we investigate the impact of the proposed weighting scheme for negative (context) words.",5.3 Impact of α−c,[0],[0]
We show the performance change of word analogy tasks on NewsIR in Figure 2 by tuning α0 and δ.,5.3 Impact of α−c,[0],[0]
"Results in other corpora show similar trends thus are omitted due to space limitation.
",5.3 Impact of α−c,[0],[0]
"Table 2: Results (“Tot.” denotes total accuracy) on the word analogy task.
",5.3 Impact of α−c,[0],[0]
"Corpus Text8 NewsIR
para.",5.3 Impact of α−c,[0],[0]
Sem.,5.3 Impact of α−c,[0],[0]
Syn.,5.3 Impact of α−c,[0],[0]
Tot.,5.3 Impact of α−c,[0],[0]
para.,5.3 Impact of α−c,[0],[0]
Sem.,5.3 Impact of α−c,[0],[0]
Syn.,5.3 Impact of α−c,[0],[0]
"Tot.
SG 1e-4 8 25 47.51 32.26 38.60 1e-5 10 10 70.81 47.48 58.10 SGA 6e-3 - - 48.10 33.78 39.74 6e-3 - - 71.74 48.71 59.20 GloVe 10 15 1 45.11 26.89 34.47 50 8 1 78.79 41.58 58.52 LexVec 1e-4 25 - 51.87 31.78 40.14 1e-5 10 - 76.11 39.09 55.95 AllVec 350 0.75 - 56.66 32.42 42.50 100 0.8 - 78.47 48.33 61.57
Wiki-sub Wiki-all
SG 1e-5 10 5 72.05 55.88 63.24 1e-5 10 5 73.91 61.91 67.37 SGA 6e-3 - - 73.93 56.10 63.81 6e-3 - - 75.11 61.94 67.92 GloVe 100 8 1 77.22 53.16 64.13 100 8 1 77.38 58.94 67.33 LexVec 1e-5 5 - 75.95 52.78 63.33 1e-5 5 - 76.31 56.83 65.48 AllVec 100 0.75 - 76.66 54.72 64.75 50 0.75 - 77.64 60.96 68.52
The parameter columns (para.)",5.3 Impact of α−c,[0],[0]
for each model are given from left to right as follows.,5.3 Impact of α−c,[0],[0]
"SG: subsampling of frequent words, window size and the number of negative samples; SGA: λ (Chen et al., 2018) that controls the distribution of the rank, the other parameters are the same with SG; GloVe:",5.3 Impact of α−c,[0],[0]
"xmax, window size and symmetric window; LexVec: subsampling of frequent words and the number of negative samples; AllVec: the negative weight α0 and δ.",5.3 Impact of α−c,[0],[0]
"Boldface denotes the highest total accuracy.
",5.3 Impact of α−c,[0],[0]
Figure 2(a) shows the impact of the overall weight α0 by setting δ as 0.75 (inspired by the setting of skip-gram).,5.3 Impact of α−c,[0],[0]
"Clearly, we observe that all results (including semantic, syntactic and total accuracy) have been greatly improved when α0 increases from 0 to a larger value.",5.3 Impact of α−c,[0],[0]
"As mentioned before, α0 = 0 means that no negative information is considered.",5.3 Impact of α−c,[0],[0]
This observation verifies that negative samples are very important for learning good embeddings.,5.3 Impact of α−c,[0],[0]
It also helps to explain why GloVe performs poorly on syntactic tasks.,5.3 Impact of α−c,[0],[0]
"In addition, we find that in all corpora the optimal results are usually obtained when α0 falls in the range of 50 to 400.",5.3 Impact of α−c,[0],[0]
"For example, in the NewIR corpus as shown, AllVec achieves the best performance when α0 = 100.",5.3 Impact of α−c,[0],[0]
Figure 2(b) shows the impact of δ with α0 = 100.,5.3 Impact of α−c,[0],[0]
"As mentioned before, δ = 0 denotes a uniform value for all negative words and δ = 1 denotes that no smoothing is applied to word frequency.",5.3 Impact of α−c,[0],[0]
We can see that the total accuracy is only around 55% when δ = 0.,5.3 Impact of α−c,[0],[0]
"By increasing its value, the performance is gradually improved, achieving the highest score when δ is around 0.8.",5.3 Impact of α−c,[0],[0]
Further increase of δ will degrade the total accuracy.,5.3 Impact of α−c,[0],[0]
This analysis demonstrates the effectiveness of the proposed negative weighting scheme.,5.3 Impact of α−c,[0],[0]
Figure 3(a) compares the convergence between AllVec and GloVe on NewsIR.,5.4 Convergence Rate and Runtime,[0],[0]
"Clearly, AllVec ex-
hibits a more stable convergence due to its full batch learning.",5.4 Convergence Rate and Runtime,[0],[0]
"In contrast, GloVe has a more dramatic fluctuation because of the one-sample learning scheme.",5.4 Convergence Rate and Runtime,[0],[0]
Figure 3(b) shows the relationship between the embedding size k and runtime on NewsIR.,5.4 Convergence Rate and Runtime,[0],[0]
"Although the analysis in Section 4.3 demonstrates that the time complexity of AllVec is O(4|S|k + 4|V |k2), the actual runtime shows a near linear relationship with k.",5.4 Convergence Rate and Runtime,[0],[0]
"This is because 4|V |k2/4|S|k = k/c, where c generally ranges from 1000 ∼ 6000 and k is set from 200 to 300 in practice.",5.4 Convergence Rate and Runtime,[0],[0]
"The above ratio explains the fact that 4|S|k dominates the complexity, which is linear
with k and |S|.",5.4 Convergence Rate and Runtime,[0],[0]
We also compare the overall runtime of AllVec and SG on NewsIR and show the results in Table 5.,5.4 Convergence Rate and Runtime,[0],[0]
"As can be seen, the runtime of AllVec falls in the range of SG-3 and SG-7 in a single iteration, which confirms the theoretical analysis in Section 4.3.",5.4 Convergence Rate and Runtime,[0],[0]
"In contrast with SG, AllVec needs more iterations to converge.",5.4 Convergence Rate and Runtime,[0],[0]
"The reason is that each parameter in SG is updated many times during each iteration, although only one training example is used in each update.",5.4 Convergence Rate and Runtime,[0],[0]
"Despite this, the total run time of AllVec is still in a feasible range.",5.4 Convergence Rate and Runtime,[0],[0]
"Assuming the convergence is measured by the number of parameter updates, our AllVec yields a much faster convergence rate than the one-sample SG method.
",5.4 Convergence Rate and Runtime,[0],[0]
"In practice, the runtime of our model in each iteration can be further reduced by increasing the number of parallel workers.",5.4 Convergence Rate and Runtime,[0],[0]
"Although baseline methods like SG and GloVe can also be parallelized, the stochastic gradient steps in these methods unnecessarily influence each other as there is no exact way to separate these updates for different workers.",5.4 Convergence Rate and Runtime,[0],[0]
"In other words, the parallelization of SGD is not well suited to a large number of work-
ers.",5.4 Convergence Rate and Runtime,[0],[0]
"In contrast, the parameter updates in AllVec are completely independent of each other, therefore AllVec does not have the update collision issue.",5.4 Convergence Rate and Runtime,[0],[0]
"This means we can achieve the embarrassing parallelization by simply separating the updates by words; that is, letting different workers update the model parameters for disjoint sets of words.",5.4 Convergence Rate and Runtime,[0],[0]
"As such, AllVec can provide a near linear scaling without any approximation since there is no potential conflicts between updates.",5.4 Convergence Rate and Runtime,[0],[0]
"In this paper, we presented AllVec, an efficient batch learning based word embedding model that is capable to leverage all positive and negative training examples without any sampling and approximation.",6 Conclusion,[0],[0]
"In contrast with models based on SGD and negative sampling, AllVec shows more stable convergence and better embedding quality by the all-sample optimization.",6 Conclusion,[0],[0]
"Besides, both theoretical analysis and experiments demonstrate that AllVec achieves the same time complexity with the classic SGD models.",6 Conclusion,[0],[0]
"In future, we will extend
our proposed all-sample learning scheme to deep learning methods, which are more expressive than the shallow embedding model.",6 Conclusion,[0],[0]
"Moreover, we will integrate prior knowledge, such as the words that are synonyms and antonyms, into the word embedding process.",6 Conclusion,[0],[0]
"Lastly, we are interested in exploring the recent adversarial learning techniques to enhance the robustness of word embeddings.
",6 Conclusion,[0],[0]
Acknowledgements.,6 Conclusion,[0],[0]
"This research is supported by the National Research Foundation, Prime Minister’s Office, Singapore under its IRC@SG Funding Initiative.",6 Conclusion,[0],[0]
Joemon M.Jose and Xiangnan,6 Conclusion,[0],[0]
He are corresponding authors.,6 Conclusion,[0],[0]
Stochastic Gradient Descent (SGD) with negative sampling is the most prevalent approach to learn word representations.,abstractText,[0],[0]
"However, it is known that sampling methods are biased especially when the sampling distribution deviates from the true data distribution.",abstractText,[0],[0]
"Besides, SGD suffers from dramatic fluctuation due to the onesample learning scheme.",abstractText,[0],[0]
"In this work, we propose AllVec that uses batch gradient learning to generate word representations from all training samples.",abstractText,[0],[0]
"Remarkably, the time complexity of AllVec remains at the same level as SGD, being determined by the number of positive samples rather than all samples.",abstractText,[0],[0]
We evaluate AllVec on several benchmark tasks.,abstractText,[0],[0]
"Experiments show that AllVec outperforms samplingbased SGD methods with comparable efficiency, especially for small training corpora.",abstractText,[0],[0]
Batch IS NOT Heavy: Learning Word Representations From All Samples,title,[0],[0]
Optimization is one of the fundamental pillars of modern machine learning.,1. Introduction,[0],[0]
"Considering that most modern machine learning methods involve the solution of some optimization problem, it is not surprising that many recent breakthroughs in this area have been on the back of more effective techniques for optimization.",1. Introduction,[0],[0]
"A case in point is deep learning, whose rise has been mirrored by the development of numerous techniques like batch normalization.
",1. Introduction,[0],[0]
"While modern algorithms have been shown to be very
*Equal contribution 1Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Massachusetts, USA 2DeepMind, London, UK.",1. Introduction,[0],[0]
"Correspondence to: Zi Wang <ziw@csail.mit.edu>, Chengtao Li <ctli@mit.edu>, Stefanie Jegelka <stefje@csail.mit.edu>, Pushmeet Kohli <pushmeet@google.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
effective for convex optimization problems defined over continuous domains, the same cannot be stated for nonconvex optimization, which has generally been dominated by stochastic techniques.",1. Introduction,[0],[0]
"During the last decade, Bayesian optimization has emerged as a popular approach for optimizing black-box functions.",1. Introduction,[0],[0]
"However, its applicability is limited to low-dimensional problems because of computational and statistical challenges that arise from optimization in high-dimensional settings.
",1. Introduction,[0],[0]
"In the past, these two problems have been addressed by assuming a simpler underlying structure of the black-box function.",1. Introduction,[0],[0]
"For instance, Djolonga et al. (2013) assume that the function being optimized has a low-dimensional effective subspace, and learn this subspace via low-rank matrix recovery.",1. Introduction,[0],[0]
"Similarly, Kandasamy et al. (2015) assume additive structure of the function where different constituent functions operate on disjoint low-dimensional subspaces.",1. Introduction,[0],[0]
The subspace decomposition can be partially optimized by searching possible decompositions and choosing the one with the highest GP marginal likelihood (treating the decomposition as a hyper-parameter of the GP).,1. Introduction,[0],[0]
"Fully optimizing the decomposition is, however, intractable.",1. Introduction,[0],[0]
"Li et al. (2016) extended (Kandasamy et al., 2015) to functions with a projected-additive structure, and approximate the projective matrix via projection pursuit with the assumption that the projected subspaces have the same and known dimensions.",1. Introduction,[0],[0]
The aforementioned approaches share the computational challenge of learning the groups of decomposed subspaces without assuming the dimensions of the subspaces are known.,1. Introduction,[0],[0]
"Both (Kandasamy et al., 2015) and subsequently (Li et al., 2016) adapt the decomposition by maximizing the GP marginal likelihood every certain number of iterations.",1. Introduction,[0],[0]
"However, such maximization is computationally intractable due to the combinatorial nature of the partitions of the feature space, which forces prior work to adopt randomized search heuristics.
",1. Introduction,[0],[0]
"In this paper, we develop a new formulation of Bayesian optimization specialized for high dimensions.",1. Introduction,[0],[0]
"One of the key contributions of this work is a new formulation that interprets prior work on high-dimensional Bayesian optimization (HDBO) through the lens of structured kernels, and places a prior on the kernel structure.",1. Introduction,[0],[0]
"Thereby, our
formulation enables simultaneous learning of the decomposition of the function domain.
",1. Introduction,[0],[0]
Prior work on latent decomposition of the feature space considers the setting where exploration/evaluation is performed once at a time.,1. Introduction,[0],[0]
"This approach makes Bayesian optimization time-consuming for problems where a large number of function evaluations need to be made, which is the case for high dimensional problems.",1. Introduction,[0],[0]
"To overcome this restriction, we extend our approach to a batched version that allows multiple function evaluations to be performed in parallel (Desautels et al., 2014; González et al., 2016; Kathuria et al., 2016).",1. Introduction,[0],[0]
"Our second contribution is an approach to select the batch of evaluations for structured kernel learning-based HDBO.
",1. Introduction,[0],[0]
Other Related Work.,1. Introduction,[0],[0]
"In the past half century, a series of different acquisition functions was developed for sequential BO in relatively low dimensions (Kushner, 1964; Moc̆kus, 1974; Srinivas et al., 2012; Hennig & Schuler, 2012; Hernández-Lobato et al., 2014; Kawaguchi et al., 2015; Wang et al., 2016a; Kawaguchi et al., 2016; Wang & Jegelka, 2017).",1. Introduction,[0],[0]
"More recent developments address high dimensional BO by making assumptions on the latent structure of the function to be optimized, such as lowdimensional structure (Wang et al., 2016b; Djolonga et al., 2013) or additive structure of the function (Li et al., 2016; Kandasamy et al., 2015).",1. Introduction,[0],[0]
"Duvenaud et al. (2013) explicitly search over kernel structures.
",1. Introduction,[0],[0]
"While the aforementioned methods are sequential in nature, the growth of computing power has motivated settings where at once a batch of points is selected for observation (Contal et al., 2013; Desautels et al., 2014; González et al., 2016; Snoek et al., 2012; Wang et al., 2017).",1. Introduction,[0],[0]
"For example, the UCB-PE algorithm (Contal et al., 2013) exploits that the posterior variance of a Gaussian Process is independent of the function mean.",1. Introduction,[0],[0]
"It greedily selects points with the highest posterior variance, and is able to update the variances without observations in between selections.",1. Introduction,[0],[0]
"Similarly, B-UCB (Desautels et al., 2014) greedily chooses points with the highest UCB score computed via the out-dated function mean but up-to-date function variances.",1. Introduction,[0],[0]
"However, these methods may be too greedy in their selection, resulting in points that lie far from an optimum.",1. Introduction,[0],[0]
"More recently, Kathuria et al. (2016) tries to resolve this issue by sampling the batch via a diversity-promoting distribution for better randomized exploration, while Wang et al. (2017) quantifies the goodness of the batch with a submodular surrogate function that trades off quality and diversity.",1. Introduction,[0],[0]
Let f : X → R be an unknown function and we aim to optimize it over a compact set X ⊆ RD.,2. Background,[0],[0]
"Within as few
function evaluations as possible, we want to find
f(x∗) = max x∈X f(x).
",2. Background,[0],[0]
"Following (Kandasamy et al., 2015), we assume a latent decomposition of the feature dimensions [D] = {1, . . .",2. Background,[0],[0]
", D} into disjoint subspaces, namely, ⋃M m=1Am = [D] and Ai ∩ Aj = ∅ for all i 6= j, i, j ∈",2. Background,[0],[0]
[D].,2. Background,[0],[0]
"Further, f can be decomposed into the following additive form:
f(x) = ∑
m∈[M ]
fm(x Am).
",2. Background,[0],[0]
"To make the problem tractable, we assume that each fm is drawn independently from GP(0, k(m)) for all m ∈",2. Background,[0],[0]
[M ].,2. Background,[0],[0]
"The resulting f will also be a sample from a GP: f ∼ GP(µ, k), where the priors are µ(x) = ∑ m∈[M ] µm(x",2. Background,[0],[0]
"Am)
and k(x, x′) = ∑ m∈[M ] k (m)(xAm , x′ Am).",2. Background,[0],[0]
"Let Dn = {(xt, yt)}nt=1 be the data we observed from f , where yt ∼ N (f(xt), σ).",2. Background,[0],[0]
"The log data likelihood for Dn is
log p(Dn|{k(m), Am}m∈[M ]) (2.1)
",2. Background,[0],[0]
= −1 2 (yT(Kn + σ,2. Background,[0],[0]
2I)−1y + log |Kn + σ2I|+ n,2. Background,[0],[0]
"log 2π)
",2. Background,[0],[0]
where Kn =,2. Background,[0],[0]
"[∑M m=1 k (m)(xAmi , x Am j ) ]",2. Background,[0],[0]
"i≤n,j≤n is the gram matrix associated with Dn, and y =",2. Background,[0],[0]
[yt]t≤n are the concatenated observed function values.,2. Background,[0],[0]
"Conditioned on the observations Dn, we can infer the posterior mean and covariance function of the function component f (m) to be
µ(m)n (x Am) =",2. Background,[0],[0]
"k(m)n (x Am)T(Kn + σ 2I)−1y,
k(m)n (x Am , x′ Am) = k(m)(xAm , x′ Am)
",2. Background,[0],[0]
− k(m)n (xAm)T(Kn + σ2I)−1k(m)n,2. Background,[0],[0]
"(x′ Am),
where k(m)n (xAm) =",2. Background,[0],[0]
"[k(m)(xAmt , x Am)]t≤n.
",2. Background,[0],[0]
"We use regret to evaluate the BO algorithms, both in the sequential and the batch selection case.",2. Background,[0],[0]
"For the sequential selection, let r̃t = maxx∈X f(x) − f(xt) denote the immediate regret at iteration t. We are interested in both the averaged cumulative regret RT = 1T ∑ t r̃t and the simple regret rT = mint≤T r̃t for a total number of T iterations.",2. Background,[0],[0]
"For batch evaluations, r̃t = maxx∈X ,b∈[B] f(x) − f(xt,b) denotes the immediate regret obtained by the batch at iteration t. The averaged cumulative regret of the batch setting is RT = 1T ∑ t r̃t, and the simple regret rT = mint≤T r̃t.",2. Background,[0],[0]
"We use the averaged cumulative regret in the bandit setting, where each evaluation of the function incurs a cost.",2. Background,[0],[0]
"If we simply want to optimize the function, we use the simple regret to capture the minimum gap between the best point found and the global optimum of the black-box function f .",2. Background,[0],[0]
Note that the averaged cumulative regret upper bounds the simple regret.,2. Background,[0],[0]
We take a Bayesian view on the task of learning the latent structure of the GP kernel.,3. Learning Additive Kernel Structure,[0],[0]
The decomposition of the input space X will be learned simultaneously with optimization as more and more data is observed.,3. Learning Additive Kernel Structure,[0],[0]
Our generative model draws mixing proportions θ ∼ DIR(α).,3. Learning Additive Kernel Structure,[0],[0]
Each dimension j is assigned to one out of M groups via the decomposition assignment variable zj ∼ MULTI(θ).,3. Learning Additive Kernel Structure,[0],[0]
"The objective function is then f(x) = ∑M m=1 fm(x
Am), where Am = {j : zj = m} is the set of support dimensions for function fm, and each fm is drawn from a Gaussian Process.",3. Learning Additive Kernel Structure,[0],[0]
"Finally, given an input x, we observe y ∼ N (f(x), σ).",3. Learning Additive Kernel Structure,[0],[0]
"Figure 1 illustrates the corresponding graphical model.
",3. Learning Additive Kernel Structure,[0],[0]
"Given the observed data Dn = {(xt, yt)}nt=1, we obtain a posterior distribution over possible decompositions z (and mixing proportions θ) that we will include later in the BO process:
p(z, θ | Dn;α) ∝",3. Learning Additive Kernel Structure,[0],[0]
p(Dn |,3. Learning Additive Kernel Structure,[0],[0]
z)p(z,3. Learning Additive Kernel Structure,[0],[0]
"| θ)p(θ;α).
",3. Learning Additive Kernel Structure,[0],[0]
"Marginalizing over θ yields the posterior distribution of the decomposition assignment
p(z | Dn;α) ∝",3. Learning Additive Kernel Structure,[0],[0]
p(Dn,3. Learning Additive Kernel Structure,[0],[0]
| z) ∫,3. Learning Additive Kernel Structure,[0],[0]
p(z | θ)p(θ;α),3. Learning Additive Kernel Structure,[0],[0]
"dθ
∝ p(Dn",3. Learning Additive Kernel Structure,[0],[0]
| z),3. Learning Additive Kernel Structure,[0],[0]
"Γ( ∑ m αm)
Γ(D + ∑ m αm)",3. Learning Additive Kernel Structure,[0],[0]
∏,3. Learning Additive Kernel Structure,[0],[0]
"m Γ(|Am|+ αm) Γ(αm)
where p(Dn|z) is the data likelihood (2.1) for the additive GP given a fixed structure defined by",3. Learning Additive Kernel Structure,[0],[0]
"z. We learn the posterior distribution for z via Gibbs sampling, choose the decomposition among the samples that achieves the highest data likelihood, and then proceed with BO.",3. Learning Additive Kernel Structure,[0],[0]
"The Gibbs sampler repeatedly draws coordinate assignments zj according to
p(zj = m | z¬j ,Dn; α) ∝ p(Dn",3. Learning Additive Kernel Structure,[0],[0]
| z)p(zj | z¬j) ∝,3. Learning Additive Kernel Structure,[0],[0]
"p(Dn | z)(|Am|+ αm) ∝ eφm ,
where
φm = − 1
2 yT(K(zj=m)n + σ",3. Learning Additive Kernel Structure,[0],[0]
"2I)−1y
− 1 2 log |K(zj=m)n + σ2I|+ log(|Am|+ αm)
and K(zj=m)n is the gram matrix associated with the observations Dn by setting zj = m. We can use the Gumbel trick to efficiently sample from this categorical distribution.",3. Learning Additive Kernel Structure,[0],[0]
"Namely, we sample a vector of i.i.d standard Gumbel variables ωi of length M , and then choose the sampled decomposition assignment zj = arg maxi≤M φi + ωi.
",3. Learning Additive Kernel Structure,[0],[0]
"With a Dirichlet process, we could make the model nonparametric and the number M of possible groups in the decomposition infinite.",3. Learning Additive Kernel Structure,[0],[0]
"Given that we have a fixed number of input dimension D, we set M = D in practice.",3. Learning Additive Kernel Structure,[0],[0]
"In real-world applications where function evaluations translate into time-intensive experiments, the typical sequential exploration strategy – observe one function value, update the model, then select the next observation – is undesirable.",4. Diverse Batch Sampling,[0],[0]
"Batched Bayesian Optimization (BBO) (Azimi et al., 2010; Contal et al., 2013; Kathuria et al., 2016) instead selects a batch of B observations to be made in parallel, then the model is updated with all simultaneously.
",4. Diverse Batch Sampling,[0],[0]
"Extending this scenario to high dimensions, two questions arise: (1) the acquisition function is expensive to optimize and (2), by itself, does not sufficiently account for exploration.",4. Diverse Batch Sampling,[0],[0]
The additive kernel structure improves efficiency for (1).,4. Diverse Batch Sampling,[0],[0]
"For batch selection (2), we need an efficient strategy that enourages observations that are both informative and non-redundant.",4. Diverse Batch Sampling,[0],[0]
"Recent work (Contal et al., 2013; Kathuria et al., 2016) selects a point that maximizes the acquisition function, and adds additional batch points via a diversity criterion.",4. Diverse Batch Sampling,[0],[0]
"In high dimensions, this diverse selection becomes expensive.",4. Diverse Batch Sampling,[0],[0]
"For example, if each dimension has a finite number of possible values1, the cost of sampling batch points via a Determinantal Point Process (DPP), as proposed in (Kathuria et al., 2016), grows exponentially with the number of dimensions.",4. Diverse Batch Sampling,[0],[0]
"The same obstacle arises with the approach by Contal et al. (2013), where points are selected greedily.",4. Diverse Batch Sampling,[0],[0]
"Thus, naı̈ve adoptions of these approaches in our setting would result in intractable algorithms.",4. Diverse Batch Sampling,[0],[0]
"Instead, we propose a general approach that explicitly takes advantage of the structured kernel to enable relevant, non-redundant high-dimensional batch selection.
",4. Diverse Batch Sampling,[0],[0]
We describe our approach for a single decomposition sampled from the posterior; it extends to a distribution of decompositions by sampling a set of decompositions from the posterior and then sampling points for each decomposition individually.,4. Diverse Batch Sampling,[0],[0]
"Given a decomposition z, we define a separate Determinantal Point Process (DPP) on each group of Am dimensions.",4. Diverse Batch Sampling,[0],[0]
"A set S of points in the subspace R|Am| is sampled with probability proportional to det(K(m)n (S)),
1While we use this discrete categorical domain to illustrate the batch setting, our proposed method is general and is applicable to continuous box-constrained domains.
where K(m)n is the posterior covariance matrix of the mth group given n observations, and K(S) is the submatrix of K with rows and columns indexed by S. Assuming the group sizes are upper-bounded by some constant, sampling from each such DPP individually implies an exponential speedup compared to using the full kernel.
",4. Diverse Batch Sampling,[0],[0]
Sampling vs. Greedy Maximization The determinant det(K (m) n,4. Diverse Batch Sampling,[0],[0]
"(S)) measures diversity, and hence the DPP assigns higher probability to diverse subsets S. An alternative to sampling is to directly maximize the determinant.",4. Diverse Batch Sampling,[0],[0]
"While this is NP-hard, a greedy strategy gives an approximate solution, and is used in (Kathuria et al., 2016), and in (Contal et al., 2013) as Pure Exploration (PE).",4. Diverse Batch Sampling,[0],[0]
We too test this strategy in the experiments.,4. Diverse Batch Sampling,[0],[0]
"In the beginning, if the GP is not approximating the function well, then greedy may perform no better than a stochastic combination of coordinates, as we observe in Fig. 6.
",4. Diverse Batch Sampling,[0],[0]
Sample Combination Now we have chosen a diverse subset Xm = {x(m)i }i∈[B−1],4. Diverse Batch Sampling,[0],[0]
⊂,4. Diverse Batch Sampling,[0],[0]
R|Am| of size (B − 1) for each group Am.,4. Diverse Batch Sampling,[0],[0]
We need to combine these subspace points to obtain B − 1 final batch query points in RD.,4. Diverse Batch Sampling,[0],[0]
"A simple way to combine samples from each group is to do it randomly without replacement, i.e., we sample one x (m) i from each Xm uniformly randomly without replacement, and combine the parts, one for each m ∈",4. Diverse Batch Sampling,[0],[0]
"[M ], to get one sample in RD.",4. Diverse Batch Sampling,[0],[0]
We repeat this procedure until we have (B − 1) points.,4. Diverse Batch Sampling,[0],[0]
"This retains diversity across the batch of samples, since the samples are diverse within each group of features.
",4. Diverse Batch Sampling,[0],[0]
"Besides this random combination, we can also combine samples greedily.",4. Diverse Batch Sampling,[0],[0]
We define a quality function ψ(m)t for each group m ∈,4. Diverse Batch Sampling,[0],[0]
"[M ] at time t, and combine samples to maximize this quality function.",4. Diverse Batch Sampling,[0],[0]
"Concretely, for the first point, we combine the maximizers x(m)∗ = arg maxx(m)∈Xm ψ",4. Diverse Batch Sampling,[0],[0]
"(m) t (x
(m)) from each group.",4. Diverse Batch Sampling,[0],[0]
"We remove those used parts, Xm ← Xm\{x(m)∗ }, and repeat the procedure until we have (B − 1) samples.",4. Diverse Batch Sampling,[0],[0]
"In each iteration, the sample achieving the highest quality score gets selected, while diversity is retained.
",4. Diverse Batch Sampling,[0],[0]
"Both selection strategies can be combined with a wide range of existing quality and acquisition functions.
",4. Diverse Batch Sampling,[0],[0]
"Add-UCB-DPP-BBO We illustrate the above framework with GP-UCB (Srinivas et al., 2012) as both the acquisition and quality functions.",4. Diverse Batch Sampling,[0],[0]
The Upper Confidence Bound (f (m)t ) + and Lower Confidence Bound (f (m)t ),4. Diverse Batch Sampling,[0],[0]
"− with parameter βt for group m at time t are
(f (m) t )",4. Diverse Batch Sampling,[0],[0]
+(x) = µ (m) t−1(x) +,4. Diverse Batch Sampling,[0],[0]
β 1/2 t σ,4. Diverse Batch Sampling,[0],[0]
"(m) t (x); (4.1)
(f (m) t ) −(x) = µ (m) t−1(x)− β 1/2 t σ",4. Diverse Batch Sampling,[0],[0]
"(m) t (x),
and combine the expected value µ(m)t−1(x) of f (m) t with the uncertainty β1/2t σ (m) t (x).",4. Diverse Batch Sampling,[0],[0]
"We set both the acquisition function and quality function ψ(m)t to be (f (m) t )
+ for group m at time t.
To ensure that we select points with high acquisition function values, we follow (Contal et al., 2013; Kathuria et al., 2016) and define a relevance region R(m)t for each group m as
R(m)t = {x ∈",4. Diverse Batch Sampling,[0],[0]
"Xm |
µ (m) t−1(x) + 2 √ β (m) t+1σ",4. Diverse Batch Sampling,[0],[0]
(m) t−1(x) ≥ (y (m) t ),4. Diverse Batch Sampling,[0],[0]
"• } ,
where (y(m)t ) • = maxx(m)∈Xm(f (m) t ) −(x(m)).",4. Diverse Batch Sampling,[0],[0]
We then use R(m)t as the ground set to sample with PE/DPP.,4. Diverse Batch Sampling,[0],[0]
The full algorithm is shown in the appendix.,4. Diverse Batch Sampling,[0],[0]
"We empirically evaluate our approach in two parts: First, we verify the effectiveness of using our Gibbs sampling algorithm to learn the additive structure of the unknown function, and then we test our batch BO for high dimensional problems with the Gibbs sampler.",5. Empirical Results,[0],[0]
Our code is available at https://github.com/zi-w/ Structural-Kernel-Learning-for-HDBBO.,5. Empirical Results,[0],[0]
We first probe the effectiveness of using the Gibbs sampling method described in Section 3 to learn the decomposition of the input space.,5.1. Effectiveness of Decomposition Learning,[0],[0]
"More details of the experiments including sensitivity analysis for α can be found in the appendix.
",5.1. Effectiveness of Decomposition Learning,[0],[0]
"Recovering Decompositions First, we sample test functions from a known additive Gaussian Process prior with zero-mean and isotropic Gaussian kernel with bandwidth = 0.1 and scale = 5 for each function component.",5.1. Effectiveness of Decomposition Learning,[0],[0]
"For D = 2, 5, 10, 20, 50, 100 input dimensions, we randomly sample decomposition settings that have at least two groups in the decomposition and at most 3 dimensions in each group.
",5.1. Effectiveness of Decomposition Learning,[0],[0]
"We set the burn-in period to be 50 iterations, and the total number of iterations for Gibbs sampling to be 100.",5.1. Effectiveness of Decomposition Learning,[0],[0]
"In Tables 1 and 2, we show two quantities that are closely related
to the learned empirical posterior of the decompositions with different numbers of randomly sampled observed data points (N ).",5.1. Effectiveness of Decomposition Learning,[0],[0]
"Table 1 shows the probability of two dimensions being correctly grouped together by Gibbs sampling in each iteration of Gibbs sampling after the burn-in period, namely, ( ∑ i<j≤D 1zgi≡z g j∧zi≡zj )/",5.1. Effectiveness of Decomposition Learning,[0],[0]
( ∑ i<j≤D 1zi≡zj ).,5.1. Effectiveness of Decomposition Learning,[0],[0]
"Table 2 reports the probability of two dimensions being correctly separated in each iteration of Gibbs sampling after the burn-in period, namely, ( ∑ i<j≤D 1zgi 6=z g j∧zi 6=zj )/",5.1. Effectiveness of Decomposition Learning,[0],[0]
( ∑ i<j≤D 1zi 6=zj ).,5.1. Effectiveness of Decomposition Learning,[0],[0]
"The results show that the more data we observe, the more accurate the learned decompositions are.",5.1. Effectiveness of Decomposition Learning,[0],[0]
They also suggest that the Gibbs sampling procedure can converge to the ground truth decomposition with enough data for relatively small numbers of dimensions.,5.1. Effectiveness of Decomposition Learning,[0],[0]
"The higher the dimension, the more data we need to recover the true decomposition.
",5.1. Effectiveness of Decomposition Learning,[0],[0]
"Effectiveness of Learning Decompositions for Bayesian Optimization To verify the effectiveness of the learned decomposition for Bayesian optimization, we tested on 2, 10, 20 and 50 dimensional functions sampled from a zero-mean Add-GP with randomly sampled decomposi-
tion settings (at least two groups, at most 3 dimensions in each group) and isotropic Gaussian kernel with bandwidth = 0.1 and scale = 5.",5.1. Effectiveness of Decomposition Learning,[0],[0]
Each experiment was repeated 50 times.,5.1. Effectiveness of Decomposition Learning,[0],[0]
An example of a 2-dimensional function component is shown in the appendix.,5.1. Effectiveness of Decomposition Learning,[0],[0]
"For Add-GPUCB, we used β(m)t = |Am| log 2t for lower dimensions (D = 2, 5, 10), and β(m)t = |Am| log 2t/5 for higher dimensions (D = 20, 30, 50).",5.1. Effectiveness of Decomposition Learning,[0],[0]
"We show parts of the results on averaged cumulative regret and simple regret in Fig. 2, and the rest in the appendix.",5.1. Effectiveness of Decomposition Learning,[0],[0]
"We compare Add-GP-UCB with known additive structure (Known), no partitions (NP), fully partitioned with one dimension for each group (FP)
and the following methods of learning the decomposition: Gibbs sampling (Gibbs), randomly sampling the same number of decompositions sampled by Gibbs and select the one with the highest data likelihood (PL-1), randomly sampling 5 decompositions and selecting the one with the highest data likelihood (PL-2).",5.1. Effectiveness of Decomposition Learning,[0],[0]
"For the latter two learning methods are referred to as “partial learning” in (Kandasamy et al., 2015).",5.1. Effectiveness of Decomposition Learning,[0],[0]
The learning of the decomposition is done every 50 iterations.,5.1. Effectiveness of Decomposition Learning,[0],[0]
"Fig. 3 shows the improvement of learning decompositions with Gibbs over optimizing without partitions (NP).
",5.1. Effectiveness of Decomposition Learning,[0],[0]
"Overall, the results show that Gibbs outperforms both of the partial learning methods, and for higher dimensions, Gibbs is sometimes even better than Known.",5.1. Effectiveness of Decomposition Learning,[0],[0]
"Interestingly, similar results can be found in Fig. 3 (c) of (Kandasamy et al., 2015), where different decompositions than the ground truth may give better simple regret.",5.1. Effectiveness of Decomposition Learning,[0],[0]
"We conjecture that this is because Gibbs is able to explore more than Known, for two reasons:
1.",5.1. Effectiveness of Decomposition Learning,[0],[0]
"Empirically, Gibbs changes the decompositions across iterations, especially in the beginning.",5.1. Effectiveness of Decomposition Learning,[0],[0]
"With fluctuating partitions, even exploitation leads to moving around, because the supposedly “good” points are influenced by the partition.",5.1. Effectiveness of Decomposition Learning,[0],[0]
"The result is an implicit “exploration” effect that is absent with a fixed partition.
",5.1. Effectiveness of Decomposition Learning,[0],[0]
2.,5.1. Effectiveness of Decomposition Learning,[0],[0]
Gibbs sometimes merges “true” parts into larger parts.,5.1. Effectiveness of Decomposition Learning,[0],[0]
"The parameter βt in UCB depends on the size of the part, |Am|(log 2t)/5 (as in (Kandasamy et al., 2015)).",5.1. Effectiveness of Decomposition Learning,[0],[0]
"Larger parts hence lead to larger βt and hence more exploration.
",5.1. Effectiveness of Decomposition Learning,[0],[0]
"Of course, more exploration is not always better, but Gibbs was able to find a good balance between exploration and exploitation, which leads to better performance.",5.1. Effectiveness of Decomposition Learning,[0],[0]
Our preliminary experiments indicate that one solution to ensure that the ground truth decomposition produces the best result is to tune βt.,5.1. Effectiveness of Decomposition Learning,[0],[0]
"Hyperparameter selection (such as choosing βt) for BO is, however, very challenging and an active topic of research (e.g. (Wang et al., 2016a)).
",5.1. Effectiveness of Decomposition Learning,[0],[0]
"Next, we test the decomposition learning algorithm on a real-world function, which returns the distance between a designated goal location and two objects being pushed by two robot hands, whose trajectory is determined by 14 parameters specifying the location, rotation, velocity, moving direction etc.",5.1. Effectiveness of Decomposition Learning,[0],[0]
"This function is implemented with a physics engine, the Box2D simulator (Catto, 2011).",5.1. Effectiveness of Decomposition Learning,[0],[0]
We use add-GP-UCB with different ways of setting the additive structure to tune the parameters for the robot hand so as to push the object closer to the goal.,5.1. Effectiveness of Decomposition Learning,[0],[0]
The regrets are shown in Fig. 4.,5.1. Effectiveness of Decomposition Learning,[0],[0]
"We observe that the performance of learning the decomposition with Gibbs dominates all existing
alternatives including partial learning.",5.1. Effectiveness of Decomposition Learning,[0],[0]
"Since the function we tested here is composed of the distance to two objects, there could be some underlying additive structure for this function in certain regions of the input space, e.g. when the two robots hands are relatively distant from each other so that one of the hands only impacts one of the objects.",5.1. Effectiveness of Decomposition Learning,[0],[0]
"Hence, it is possible for Gibbs to learn a good underlying additive structure and perform effective BO with the structures it learned.",5.1. Effectiveness of Decomposition Learning,[0],[0]
"Next, we probe the effectiveness of batch BO in high dimensions.",5.2. Diverse Batch Sampling,[0],[0]
"In particular, we compare variants of the AddUCB-DPP-BBO approach outlined in Section 4, and a baseline:
• Rand:",5.2. Diverse Batch Sampling,[0],[0]
"All batch points are chosen uniformly at random from X .
• Batch-UCB-*: *∈ {PE,DPP}.",5.2. Diverse Batch Sampling,[0],[0]
All acquisition functions are UCB (Eq. 4.1).,5.2. Diverse Batch Sampling,[0],[0]
Exploration is done via PE or DPP with posterior covariance kernels for each group.,5.2. Diverse Batch Sampling,[0],[0]
"Combination is via sampling without replacement.
",5.2. Diverse Batch Sampling,[0],[0]
"• *-Fnc: *∈ {Batch-UCB-PE,Batch-UCB-DPP}.",5.2. Diverse Batch Sampling,[0],[0]
"All quality functions are also UCB’s, and combination is done by maximizing the quality functions.
",5.2. Diverse Batch Sampling,[0],[0]
"A direct application of existing batch selection methods is very inefficient in the high-dimensional settings where they differ more, algorithmically, from our approach that ex-
ploits decompositions.",5.2. Diverse Batch Sampling,[0],[0]
"Hence, we only compare to uniform sampling as a baseline.
",5.2. Diverse Batch Sampling,[0],[0]
"Effectiveness We tested on 2, 10, 20 and 50-dimensional functions sampled the same way as in Section 5.1; we assume the ground-truth decomposition of the feature space is known.",5.2. Diverse Batch Sampling,[0],[0]
"Since Rand performs the worst, we show relative averaged cumulative regret and simple regret of all methods compared to Rand in Fig. 5.",5.2. Diverse Batch Sampling,[0],[0]
Results for absolute values of regrets are shown in the appendix.,5.2. Diverse Batch Sampling,[0],[0]
Each experiment was repeated for 20 times.,5.2. Diverse Batch Sampling,[0],[0]
"For all experiments, we set βmt = |Am| log 2t and B = 10.",5.2. Diverse Batch Sampling,[0],[0]
"All diverse batch sampling methods perform comparably well and far better than Rand, although there exist slight differences.",5.2. Diverse Batch Sampling,[0],[0]
"While in lower dimensions (D ∈ {2, 10}), Batch-UCB-PE-Fnc performs among the best, in higher dimensions (D ∈ {20, 50}), Batch-UCB-DPP-Fnc performs better than (or comparable to) all other variants.",5.2. Diverse Batch Sampling,[0],[0]
"We will see a larger performance gap in later real-world experiments, showing that biasing the combination towards higher quality functions while retaining diversity across the batch of samples provides a better exploration-exploitation trade-off.
",5.2. Diverse Batch Sampling,[0],[0]
"For a real-data experiment, we tested the diverse batch sampling algorithms for BBO on the Walker function which returns the walking speed of a three-link planar bipedal walker implemented in Matlab (Westervelt et al., 2007).",5.2. Diverse Batch Sampling,[0],[0]
"We tune 25 parameters that may influence the walking speed, including 3 sets of 8 parameters for the ODE solver and 1 parameter specifying the initial velocity of the stance leg.",5.2. Diverse Batch Sampling,[0],[0]
"We discretize each dimension into 40 points, resulting in a function domain of |X | = 4025.",5.2. Diverse Batch Sampling,[0],[0]
This size is very inefficient for existing batch sampling techniques.,5.2. Diverse Batch Sampling,[0],[0]
We learn the additive structure via Gibbs sampling and sample batches of size B = 5.,5.2. Diverse Batch Sampling,[0],[0]
"To further improve efficiency, we limit the maximum size of each group to 2.",5.2. Diverse Batch Sampling,[0],[0]
The regrets for all methods are shown in Fig. 6.,5.2. Diverse Batch Sampling,[0],[0]
"Again, all diverse batch sampling methods outperform Rand by a large gap.",5.2. Diverse Batch Sampling,[0],[0]
"Moreover, Batch-UCB-DPP-Fnc is a bit better than other variants, suggesting that a selection by quality functions is useful.
",5.2. Diverse Batch Sampling,[0],[0]
"Batch Sizes Finally, we show how the batch size B affects the performance of the proposed methods.",5.2. Diverse Batch Sampling,[0],[0]
"We test the algorithms on the 14-dimensional Robot dataset with B ∈ {5, 10}.",5.2. Diverse Batch Sampling,[0],[0]
The regrets are shown in Fig. 4.,5.2. Diverse Batch Sampling,[0],[0]
"With larger batches, the differences between the batch selection approaches become more pronounced.",5.2. Diverse Batch Sampling,[0],[0]
"In both settings, Batch-UCB-DPP-Fnc performs a bit better than other variants, in particular with larger batch sizes.",5.2. Diverse Batch Sampling,[0],[0]
"In this paper, we propose two novel solutions for high dimensional BO: inferring latent structure, and combining it with batch Bayesian Optimization.",6. Conclusion,[0],[0]
"The experimental results demonstrate that the proposed techniques are effective at optimizing high-dimensional black-box functions.
",6. Conclusion,[0],[0]
"Moreover, their gain over existing methods increases as the dimensionality of the input grows.",6. Conclusion,[0],[0]
We believe that these results have the potential to enable the increased use of Bayesian optimization for challenging black-box optimization problems in machine learning that typically involve a large number of parameters.,6. Conclusion,[0],[0]
"We gratefully acknowledge support from NSF CAREER award 1553284, NSF grants 1420927 and 1523767, from ONR grant N00014-14-1-0486, and from ARO grant W911NF1410433.",Acknowledgements,[0],[0]
We thank MIT Supercloud and the Lincoln Laboratory Supercomputing Center for providing computational resources.,Acknowledgements,[0],[0]
"Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of our sponsors.",Acknowledgements,[0],[0]
Optimization of high-dimensional black-box functions is an extremely challenging problem.,abstractText,[0],[0]
"While Bayesian optimization has emerged as a popular approach for optimizing black-box functions, its applicability has been limited to low-dimensional problems due to its computational and statistical challenges arising from high-dimensional settings.",abstractText,[0],[0]
"In this paper, we propose to tackle these challenges by (1) assuming a latent additive structure in the function and inferring it properly for more efficient and effective BO, and (2) performing multiple evaluations in parallel to reduce the number of iterations required by the method.",abstractText,[0],[0]
Our novel approach learns the latent structure with Gibbs sampling and constructs batched queries using determinantal point processes.,abstractText,[0],[0]
Experimental validations on both synthetic and real-world functions demonstrate that the proposed method outperforms the existing state-of-the-art approaches.,abstractText,[0],[0]
Batched High-dimensional Bayesian Optimization via Structural Kernel Learning,title,[0],[0]
"Boolean matrix factorisation aims to decompose a binary data matrix into an approximate Boolean product of two low rank, binary matrices: one containing meaningful patterns, the other quantifying how the observations can be expressed as a combination of these patterns. We introduce the OrMachine, a probabilistic generative model for Boolean matrix factorisation and derive a Metropolised Gibbs sampler that facilitates efficient parallel posterior inference. On real world and simulated data, our method outperforms all currently existing approaches for Boolean matrix factorisation and completion. This is the first method to provide full posterior inference for Boolean Matrix factorisation which is relevant in applications, e.g. for controlling false positive rates in collaborative filtering and, crucially, improves the interpretability of the inferred patterns. The proposed algorithm scales to large datasets as we demonstrate by analysing single cell gene expression data in 1.3 million mouse brain cells across 11 thousand genes on commodity hardware.",text,[0],[0]
"Boolean matrix factorisation (BooMF) can infer interpretable decompositions of a binary data matrix X ∈ {0, 1}N×D into a pair of low-rank, binary matrices Z ∈ {0, 1}N×L and U ∈ {0, 1}D×L.",1. Introduction,[0],[0]
"The data generating process is based on the Boolean product, a special case of matrix product between binary matrices where all values
1Department of Statistics, University of Oxford, UK 2Nuffield Department of Medicine, University of Oxford, UK 3Department of Informatics, Athens University of Economics and Business, Greece 4Centre for Computational Biology, Institute of Cancer and Genomic Sciences, University of Birmingham, UK.",1. Introduction,[0],[0]
"Correspondence to: Tammo Rukat <tammo.rukat@stats.ox.ac.uk>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
n=0 n=1 n=2,1. Introduction,[0],[0]
"n=3 n=4 n=5 n=6 n=7 n=8 n=9
N=10 observation of size D=17x10
' l=0",1. Introduction,[0],[0]
l=1,1. Introduction,[0],[0]
"l=2
l=3 l=4",1. Introduction,[0],[0]
"l=5
Codes u
⊗
0 1 2 3 4 5 6 7 8 9 n
5
4
3
2
1
0
l
0.0
0.2
0.4
0.6
0.8
1.0 Latent representations z
Figure 1.",1. Introduction,[0],[0]
The observed images are 10 digits from 0 to 9 as they are traditionally represented in calculators.,1. Introduction,[0],[0]
"The data is factorised into matrices of rank 6, which is not sufficient for full error-free reconstruction.",1. Introduction,[0],[0]
"Every digit, except 7, can be constructed by Boolean combination of the inferred codes.",1. Introduction,[0],[0]
The OrMachine infers a posterior mean probability of 50% for using code l = 5 in constructing a 7.,1. Introduction,[0],[0]
Note that there exist other equally valid solutions to this problem with 6 latent dimensions.,1. Introduction,[0],[0]
The pixels represent posterior means.,1. Introduction,[0],[0]
"Codes and observations are arranged to 10×17 images for interpretation.
larger than zero are set to one, i.e.
xnd",1. Introduction,[0],[0]
= L∨ l=1 znl ∧ uld .,1. Introduction,[0],[0]
"(1)
Here, ∨ and∧ encode the Boolean disjunction and conjunction, respectively.",1. Introduction,[0],[0]
BooMF provides a framework for learning from binary data where the inferred codes U provide a basis and the indicator variables Z encode the presence or absence of these codes.,1. Introduction,[0],[0]
This representation is illustrated in the calculator digits example in Fig. 1.,1. Introduction,[0],[0]
"We can think of BooMF as binary factor analysis or as clustering with joint assignments, where each observation is assigned to a subset of L cluster centroids or codes.",1. Introduction,[0],[0]
The L-dimensional indicators provide a compact representation of which codes are allocated to each observation.,1. Introduction,[0],[0]
As stated in Eq.,1. Introduction,[0],[0]
"(1), a feature xnd takes a value of one if it equals one in any of the assigned codes.
",1. Introduction,[0],[0]
"BooMF has many real-world applications ranging from topic modelling (Blei, 2012) to collaborating filtering (Su
& Khoshgoftaar, 2009) and computer vision (LázaroGredilla et al., 2016).",1. Introduction,[0],[0]
"In this paper, we introduce the OrMachine, a Bayesian approach to BooMF, and fit the model using a fast and scalable Metropolised Gibbs sampling algorithm.",1. Introduction,[0],[0]
"On simulated and real-world data, our method is shown to significantly outperform the current state-of-theart message passing approaches for learning BooMF models.",1. Introduction,[0],[0]
"Moreover, we consider a challenging application in the analysis of high-throughput single cell genomics data.",1. Introduction,[0],[0]
BooMF is used to identify latent gene signatures (codes) that correspond to key cellular pathways or biological processes from large gene expression datasets consisting of 1.3 million cells across 11 thousand genes.,1. Introduction,[0],[0]
"Genes are expressed if one or more relevant biological processes are active, a property which is naturally modelled by the Boolean OR operation.",1. Introduction,[0],[0]
We also introduce a multi-layered extensions of Bayesian BooMF that can capture hierarchical dependencies in the latent representations.,1. Introduction,[0],[0]
There has been a sustained interest in BooMF and related methods of which we will give a brief review.,2. Related Work,[0],[0]
"The Discrete Basis Problem (Miettinen et al., 2006) provides a greedy heuristic algorithm to solve BooMF without recourse to an underlying probabilistic model.",2. Related Work,[0],[0]
"It is based on association rule mining (Agrawal et al., 1994) and has more recently been extended to automatically select the optimal dimensionality of the latent space based on the minimum description length principle (Miettinen & Vreeken, 2014).",2. Related Work,[0],[0]
"In contrast, multi assignment clustering for Boolean data (Streich et al., 2009) leverages on a probabilistic model for BooMF, adding a further global noise source to the generative process.",2. Related Work,[0],[0]
Point estimates are inferred by deterministic annealing.,2. Related Work,[0],[0]
"Similarly, Wood et al. (2012) develop a probabilistic model to infer hidden causes.",2. Related Work,[0],[0]
"In contrast to the Boolean OR, the likelihood of an observation increases with the number of active hidden codes.",2. Related Work,[0],[0]
They use an Indian Buffet process prior over the latent space and a Gibbs sampler to infer the distribution over the unbounded number of hidden causes.,2. Related Work,[0],[0]
"A more expressive model for matrix factorisation with binary latent variables is introcued by (Meeds et al., 2007), who combine binary interactions with continous weights.",2. Related Work,[0],[0]
A similar approach to ours is the work by Ravanbakhsh et al. (2016).,2. Related Work,[0],[0]
The authors tackle BooMF using a probabilistic graphical model and derive a message passing algorithm to perform MAP inference.,2. Related Work,[0],[0]
Their method is shown to have state-of-the-art performance for BooMF and completion.,2. Related Work,[0],[0]
It therefore serves us as baseline benchmark in these tasks.,2. Related Work,[0],[0]
The message passing approach has recently been employed by Lázaro-Gredilla et al. (2016) in a hierarchical network combined with pooling layers to infer the building blocks of binary images.,2. Related Work,[0],[0]
The OrMachine is a probabilistic generative model for Boolean matrix factorisation.,3.1. Model Formulation,[0],[0]
"A matrix of N binary observations xn ∈ {0, 1}D is generated from a discrete mixture of L binary codes ul ∈ {0, 1}D. Binary latent variables znl denote whether or not code l is used in generating a particular observation xn.",3.1. Model Formulation,[0],[0]
"The probability for a data point xnd to be one is greater than 1/2 if the corresponding codes and latent variables in at least one latent dimension both equal one; conversely, if there exists no dimension where codes and latent variables both equal one, the probability for the data point to be one is less than 1/2.",3.1. Model Formulation,[0],[0]
"The exact magnitude of this probability is inferred from the data and, for later notational convenience, is parametrised as the logistic sigmoid of a global dispersion parameter σ(λ) =",3.1. Model Formulation,[0],[0]
"(1+e−λ)−1, with λ ∈ R+.",3.1. Model Formulation,[0],[0]
"Next, we give a full description of the likelihood and prior distributions used in the OrM.
The likelihood function is factorised across the N observations and D features with each factor given by
p(xnd|u, z, λ) = { σ(λ); if x=min(1,uTd zn) 1−σ(λ); if x 6=min(1,uTd zn) (2)
= σ",3.1. Model Formulation,[0],[0]
"[ λx̃nd ( 1− 2
∏ l (1− znluld)
)] .
(3)
",3.1. Model Formulation,[0],[0]
"Tilde denotes the {0, 1} → {−1, 1} mapping so that for any binary variable x ∈ {0, 1}, x̃ = 2x − 1.",3.1. Model Formulation,[0],[0]
The expression inside the parentheses of Eq.,3.1. Model Formulation,[0],[0]
"(3) encodes the OR operation and evaluates to 1 if znl = uld = 1 for at least one l, and to −1 otherwise.",3.1. Model Formulation,[0],[0]
"The dispersion parameter controls the noise in the generative process, i.e. as λ→∞, all probabilities tend to 0 or 1 and the model describes a deterministic Boolean matrix product.",3.1. Model Formulation,[0],[0]
Note that the likelihood can be computed efficiently from Eq. (3) as we describe in detail in the next section.,3.1. Model Formulation,[0],[0]
We further assume independent Bernoulli priors for all variables uld and znl.,3.1. Model Formulation,[0],[0]
Such priors allow us to promote denseness or sparsity in codes and latent variables.,3.1. Model Formulation,[0],[0]
Notice that the designation of U as codes and Z as latent variables is not necessary since these matrices appear in a symmetric manner.,3.1. Model Formulation,[0],[0]
"If we transpose the matrix of observations X , then codes and latent variables merely swap roles.
",3.1. Model Formulation,[0],[0]
"Finally, we do not place a prior on the dispersion parameter λ, but maximise it using an EM-type algorithm described below.",3.1. Model Formulation,[0],[0]
"The full joint distribution of all data and random variables is given by p(X,U ,Z|λ) = p(X|U ,Z, λ)p(U)p(Z).
",3.2. Fast Posterior Inference,[0],[0]
"The full conditional for znl (and analogous for uld) is
p(znl| ·)",3.2. Fast Posterior Inference,[0],[0]
"=
σ [ λz̃nl ∑ d x̃nduld ∏ l′ 6=l (1−znl′ul′d)+logit(p(znl)) ] .",3.2. Fast Posterior Inference,[0],[0]
"(4)
We give a detailed derivation in the supplement.",3.2. Fast Posterior Inference,[0],[0]
"Notice that the independent Bernoulli prior enters the expression as additive term inside the sigmoid function that vanishes for the uninformative Bernoulli prior p(z) = 1/2.
",3.2. Fast Posterior Inference,[0],[0]
The form of Eq.,3.2. Fast Posterior Inference,[0],[0]
(4) allows for computationally efficient evaluation of the conditionals.,3.2. Fast Posterior Inference,[0],[0]
"The underlying principle is that once certain conditions are met, the result of the full conditional is known without considering the remainder of a variable’s Markov blanket.",3.2. Fast Posterior Inference,[0],[0]
"For instance, when computing updates for znl, terms in the sum over d necessarily evaluate to zero if one of the following conditions is met: (i) uld = 0 or (ii) znl′ul′d",3.2. Fast Posterior Inference,[0],[0]
= 1 for some l′ 6=,3.2. Fast Posterior Inference,[0],[0]
"l. This leads to Algorithm 1 for fast evaluation of the conditionals.
",3.2. Fast Posterior Inference,[0],[0]
"Algorithm 1 Computation of the full conditional of znl accumulator = 0 for d in 1, . . .",3.2. Fast Posterior Inference,[0],[0]
", D do
if uld = 0 then continue (next iteration over d) end if for l′ in 1, . . .",3.2. Fast Posterior Inference,[0],[0]
", L do
if l′",3.2. Fast Posterior Inference,[0],[0]
6=,3.2. Fast Posterior Inference,[0],[0]
"l and znl′ = 1 and ul′d = 1 then continue (next iteration over d)
end if end for accumulator = accumulator +",3.2. Fast Posterior Inference,[0],[0]
"x̃nd
end for p(znl| ·)",3.2. Fast Posterior Inference,[0],[0]
= σ,3.2. Fast Posterior Inference,[0],[0]
"(λ · z̃nl · accumulator)
To infer the posterior distribution over all variables uld and znl we could iteratively sample from the above conditionals using standard Gibbs sampling.",3.2. Fast Posterior Inference,[0],[0]
In practice we use a modification of this procedure which is referred to as Metropolised Gibbs sampler and was proposed by Liu (1996).,3.2. Fast Posterior Inference,[0],[0]
"We always propose to flip the current state, leading to a Hastings acceptance probability of p(z| ·)/(1−p(z|",3.2. Fast Posterior Inference,[0],[0]
·)),3.2. Fast Posterior Inference,[0],[0]
.,3.2. Fast Posterior Inference,[0],[0]
"This is guaranteed to yield lower variance Monte Carlo estimates (Peskun, 1973).
",3.2. Fast Posterior Inference,[0],[0]
"After every sweep through all variables, the dispersion parameter λ is updated to maximise the likelihood akin to the M-step of a Monte Carlo EM algorithm.",3.2. Fast Posterior Inference,[0],[0]
"Specifically, given the current values of the codes U and latent variables Z we can compute how many observations xnd are correctly predicted by the model, as
P = ∑ n,d",3.2. Fast Posterior Inference,[0],[0]
I,3.2. Fast Posterior Inference,[0],[0]
[ xnd = 1− ∏,3.2. Fast Posterior Inference,[0],[0]
l (1− znluld) ] .,3.2. Fast Posterior Inference,[0],[0]
"This allows us to
Algorithm 2 Sampling from the OrMachine for i in 1, . . .",3.2. Fast Posterior Inference,[0],[0]
",max-iters do
for n in 1, . . .",3.2. Fast Posterior Inference,[0],[0]
",N (in parallel) do for l in 1, . . .",3.2. Fast Posterior Inference,[0],[0]
",L do
Compute p(znl| ·) following Algorithm 1 Flip znl with probability [p(znl| ·)−1−1]−1
end for end for for d in 1, . . .",3.2. Fast Posterior Inference,[0],[0]
", d (in parallel) do
for l in 1, . . .",3.2. Fast Posterior Inference,[0],[0]
",L do Compute p(uld| ·) following Algorithm 1 Flip uld with probability [p(uld| ·)−1−1]−1
end for end for Set λ to its MLE according to Eq.",3.2. Fast Posterior Inference,[0],[0]
"(5).
end for
rewrite the likelihood as σ(λ)Pσ(−λ)ND−P which can be subsequently maximised with respect to λ to yield the update
σ(λ̂) = P
ND .",3.2. Fast Posterior Inference,[0],[0]
"(5)
The alternation between sampling (U ,Z) and updating the dispersion parameter is carried out until convergence; see Algorithm 2 for all steps of this procedure.",3.2. Fast Posterior Inference,[0],[0]
"We can handle unobserved data, by marginalising the likelihood over the missing observations.",3.3. Dealing with Missing Data,[0],[0]
"More precisely, if X = (Xobs,Xmis) is the decomposition of the full matrix into the observed part Xobs and the missing part Xmis, after marginalisation, the initial likelihood p(X|U ,Z, λ) simplifies to p(Xobs|U ,Z, λ).",3.3. Dealing with Missing Data,[0],[0]
"Then, a naı̈ve implementation could be based on indexing the observed components inside matrix X and modifying the inference procedure so that the posterior conditionals of znl and uld involve only sums over observed elements.",3.3. Dealing with Missing Data,[0],[0]
"A simpler, equivalent implementation, which we follow in our experiments, is to represent the data as x̃nd ∈ {−1, 0, 1} where missing observations are encoded as zeros, each contributing the constant factor σ(0) = 1/2 to the full likelihood, so that p(X|U ,Z, λ) = C p(Xobs|U ,Z, λ), where C is a constant.",3.3. Dealing with Missing Data,[0],[0]
"Thus, the missing values do not contribute to the posterior over U and Z which is also clear from the form of the full conditionals in Eq.",3.3. Dealing with Missing Data,[0],[0]
(4) that depend on a sum weighted by xnds.,3.3. Dealing with Missing Data,[0],[0]
"For the update of the dispersion parameter in Eq. (5)), we need to subtract the number of all missing observations in the denominator.",3.3. Dealing with Missing Data,[0],[0]
The dispersion now indicates the fraction of correct prediction in the observed data.,3.3. Dealing with Missing Data,[0],[0]
"Following this inference procedure, we can impute missing data based on a Monte Carlo estimate of
the predictive distribution of some unobserved xnd as
1
S S∑ s=1",3.3. Dealing with Missing Data,[0],[0]
"p(xnd|U (s),Z(s), λ̂) , (6)
where each (U (s),Z(s)) is a posterior sample.",3.3. Dealing with Missing Data,[0],[0]
"A much faster approximation of the predictive distribution is obtained by p(xnd|Û , Ẑ, λ̂), where we simply plug the posterior mean estimates for (U ,Z) into the predictive distribution.",3.3. Dealing with Missing Data,[0],[0]
"For the simulated data in Section 4.2, we find both methods to perform equally well and therefore follow the second, faster approach for all remaining experiments.",3.3. Dealing with Missing Data,[0],[0]
BooMF learns patterns of correlation in the data.,3.4. Multi-Layer OrMachine,[0],[0]
"In analogy to multi-layer neural networks, we can build a hierarchy of correlations by applying another layer of factorisation to the factor matrix Z. This is reminiscent of the idea of deep exponential families, as introduced by Ranganath et al. (2015).",3.4. Multi-Layer OrMachine,[0],[0]
"The ability to learn features at different levels of abstraction is commonly cited as an explanation for the success that deep neural networks have across many domains of application (Lin & Tegmark, 2016; Bengio et al., 2013).",3.4. Multi-Layer OrMachine,[0],[0]
"In the present setting, with stochasticity at every step of the generative process and posterior inference, we are able to infer meaningful and interpretable hierarchies of abstraction.
",3.4. Multi-Layer OrMachine,[0],[0]
"To give an example, we determine the optimal multi-layer architecture for representing the calculator digit toy dataset as introduced in Fig. 1.",3.4. Multi-Layer OrMachine,[0],[0]
We observe 50 digits and consider 70% of the data points randomly as unobserved.,3.4. Multi-Layer OrMachine,[0],[0]
"We then
train multi-layer OrMachines with various depths and layer widths, iterating through the individual layers during 200 iterations of burn-in.",3.4. Multi-Layer OrMachine,[0],[0]
We then draw 200 samples from each consecutive layer with the remaining layers held fixed to their MAP estimate.,3.4. Multi-Layer OrMachine,[0],[0]
"In order to enforce distributed representations, we choose independent Bernoulli sparsity priors for the codes: p(uld) =",3.4. Multi-Layer OrMachine,[0],[0]
"[0.01, 0.05, 0.2] for each layer, respectively.",3.4. Multi-Layer OrMachine,[0],[0]
"Superior performance in reconstructing the unobserved data is achieved by a 3-hidden layer architecture with hidden layers of size L1 = 7, L2 = 4, L3 = 2.",3.4. Multi-Layer OrMachine,[0],[0]
This 3-layer model reduces the reconstruction error from 1.4% to 0.4% compared to the single-layer model with width L = 7.,3.4. Multi-Layer OrMachine,[0],[0]
Maximum likelihood estimates of the dispersion for the three layers are λ̂ =,3.4. Multi-Layer OrMachine,[0],[0]
"[1.0, 0.93, 0.8].",3.4. Multi-Layer OrMachine,[0],[0]
"The first layer infers the seven bars that compose all digits, the following layer infers dominant groupings of these bars, and so on.",3.4. Multi-Layer OrMachine,[0],[0]
"In Fig. 2, we plot the probabilities that each prototype induces in the observation layer.",3.4. Multi-Layer OrMachine,[0],[0]
"They are given by the means of the posterior predictive as described in the the previous section, conditioned on the one-hot activations of zn with znl ∈ {0, 1} and ∑ l znl = 1.",3.4. Multi-Layer OrMachine,[0],[0]
"Alongside, we depict the average posterior mean of the corresponding representations for each digit in the training data.",3.4. Multi-Layer OrMachine,[0],[0]
This example illustrates that the multi-layer OrMachine infers interpretable higherorder correlations and is able to exploit them to achieve significant improvements in missing data imputation.,3.4. Multi-Layer OrMachine,[0],[0]
The algorithm is implemented in Python with the core sampling routines in compiled Cython.,3.5. Practical Implementation and Speed,[0],[0]
Code is avaialble on GitHub1.,3.5. Practical Implementation and Speed,[0],[0]
"Binary data is represented as {−1, 1} with missing data encoded as 0.",3.5. Practical Implementation and Speed,[0],[0]
This economical representation of data and variables as integer types simplifies computations considerably.,3.5. Practical Implementation and Speed,[0],[0]
"Algorithm 1 is implemented in parallel across the observations [n] = {1, . . .",3.5. Practical Implementation and Speed,[0],[0]
", N} and conversely updates for uld are implemented in parallel across all features [d] = {1, . . .",3.5. Practical Implementation and Speed,[0],[0]
", D}.",3.5. Practical Implementation and Speed,[0],[0]
The computation time scales linearly in each dimension.,3.5. Practical Implementation and Speed,[0],[0]
A single sweep through high-resolution calculator digits toy dataset with ND = 1.7 × 106 data points and L = 7 latent dimensions takes approximately 1 second on a desktop computer.,3.5. Practical Implementation and Speed,[0],[0]
A single sweep through the approximately 1.4× 1010 data points presented in the biological example in Section 5.2 with L = 2 latent dimensions takes approximately 5 minutes executed on 24 computing cores.,3.5. Practical Implementation and Speed,[0],[0]
For all examples presented here 100 iterations suffice for the algorithm to converge to a (local) posterior mode.,3.5. Practical Implementation and Speed,[0],[0]
"In this section, we probe the performance of the OrMachine (OrM) at random matrix factorisation and completion
1https://github.com/TammoR/OrMachine/
tasks.",4. Experiments on Simulated Data,[0],[0]
"Message passing (MP) has been shown to compare favourably with other state-of-the-art methods for BooMF that we introduced in Section 2 (Ravanbakhsh et al., 2016).",4. Experiments on Simulated Data,[0],[0]
It therefore is the focus of our comparison.,4. Experiments on Simulated Data,[0],[0]
"The following settings for MP and the OrM are used throughout our experiments, unless mentioned otherwise.",4. Experiments on Simulated Data,[0],[0]
"For MP, we use the Python implementation provided by the authors.",4. Experiments on Simulated Data,[0],[0]
"We also proceed with their choice of hyper-parameters, as experimentation with different learning rates and maximum number of iterations did not lead to any improvements.",4. Experiments on Simulated Data,[0],[0]
"For both methods, we set the priors p(u) and p(z) to the factor matrices’ expected value based on the density of the product matrix in an Empirical-Bayes fashion.",4. Experiments on Simulated Data,[0],[0]
"The only exception is MP in the matrix completion task, where uniform priors, as used by Ravanbakhsh et al. (2016), lead to slightly better performance.",4. Experiments on Simulated Data,[0],[0]
"For the OrM, we initialise the parameters uniformly at random and draw 100 iterations after 100 samples of burn-in.",4. Experiments on Simulated Data,[0],[0]
Note that around 10–100 sampling steps are usually sufficient for convergence.,4. Experiments on Simulated Data,[0],[0]
"We generate a quadratic matrix X ∈ {0, 1}N×N of rank L by taking the Boolean product of two random N × L factor matrices.",4.1. Random Matrix Factorisation,[0],[0]
The Boolean product X of two rank L binary matrices that are sampled i.i.d. from a Bernoulli distribution with parameter p has an expected value of E(X),4.1. Random Matrix Factorisation,[0],[0]
"= 1− (1− p2)L. Since we generally prefer X to be neither sparse nor dense, we fix its expected density to 1/2, unless stated otherwise.",4.1. Random Matrix Factorisation,[0],[0]
This ensures that a simple bias toward zeroes or ones in either method is not met with reward.,4.1. Random Matrix Factorisation,[0],[0]
Bits in the data are flipped at random with probabilities ranging from 5% to 50%.,4.1. Random Matrix Factorisation,[0],[0]
Factor matrices of the correct underlying dimension are inferred and the data is reconstructed from the inferred factorisation.,4.1. Random Matrix Factorisation,[0],[0]
"An example of the task is shown in Fig. 3.
",4.1. Random Matrix Factorisation,[0],[0]
"Results for the reconstruction error, defined as the fraction of misclassified data points, are depicted in Fig. 4.",4.1. Random Matrix Factorisation,[0],[0]
All experiments were repeated 10 times with error bars denoting standard deviations.,4.1. Random Matrix Factorisation,[0],[0]
"The OrM outperforms MP under all conditions, except when both methods infer equally errorfree reconstructions.",4.1. Random Matrix Factorisation,[0],[0]
"Fig. 4 (top) reproduces the experi-
mental settings of Fig. 2 in Ravanbakhsh et al. (2016).",4.1. Random Matrix Factorisation,[0],[0]
We find that the OrMachine enables virtually perfect reconstruction of a 1000 × 1000 matrix of rank L = 5 for up to 35% bit flip probability.,4.1. Random Matrix Factorisation,[0],[0]
"Notably, MP performs worse for smaller noise levels.",4.1. Random Matrix Factorisation,[0],[0]
It was hypothesised by Ravanbakhsh et al. (2016) that symmetry breaking at higher noise levels helps message passage to converge to a better solution.,4.1. Random Matrix Factorisation,[0],[0]
Fig. 4 (middle) demonstrates the consistently improved performance of the OrMachine for a more challenging example of 100 × 100 matrices of rank 7.,4.1. Random Matrix Factorisation,[0],[0]
"The reconstruction performance of both methods is similar for lower noise levels, while the OrMachine consistently out-
performs MP for larger noise levels.",4.1. Random Matrix Factorisation,[0],[0]
"For biased data with E[xnd] = 0.7 in Fig. 4 (bottom), we observe a similar pattern with a larger performance gap for higher noise levels.",4.1. Random Matrix Factorisation,[0],[0]
"Even for a bit flip-probability of 50% the OrMachine retains a reconstruction error of approximately 30%, which is achieved by levering the bias in the data.
",4.1. Random Matrix Factorisation,[0],[0]
"Fig. 4 (middle) also shows the reconstruction error on the observed data, indicating that MP overfits the data more than the OrM for larger noise levels.",4.1. Random Matrix Factorisation,[0],[0]
This may contribute to the improved performance of the OrMachine.,4.1. Random Matrix Factorisation,[0],[0]
"We further investigate the problem of matrix completion or collaborative filtering, where bits of the data matrix are unobserved and reconstructed from the inferred factor matrices.",4.2. Random Matrix Completion,[0],[0]
"Following the procedure outlined in Section 4.1, we generate random matrices of rank 5 and size 250 × 250.",4.2. Random Matrix Completion,[0],[0]
"We only observe a random subset of the data, ranging from 0.5% and 3.5%.",4.2. Random Matrix Completion,[0],[0]
The missing data is reconstructed from the inferred factor matrices.,4.2. Random Matrix Completion,[0],[0]
"As shown in Fig. 5, the OrMachine outperforms message passing throughout.",4.2. Random Matrix Completion,[0],[0]
"The plot indicates means and standard deviations from 10 repetitions of each experiment.
",4.2. Random Matrix Completion,[0],[0]
"Notably, the OrMachine does not only provide a MAP estimate, but also an estimate of the posterior probability for each unobserved data point xnd.",4.2. Random Matrix Completion,[0],[0]
Fig. 5 (bottom) shows an estimate of the density of the posterior means for the correctly and incorrectly completed data points.,4.2. Random Matrix Completion,[0],[0]
"The distribution of incorrect predictions peaks around a probability of 1/2, indicating that the OrMachine’s uncertainty about
its reconstruction provides further useful information about the missing data.",4.2. Random Matrix Completion,[0],[0]
"For instance, this information can be used to control for false positives or false negatives, simply by setting a threshold for the posterior mean.",4.2. Random Matrix Completion,[0],[0]
We investigate the OrMachine’s performance for collaborative filtering on a real-world dataset.,5.1. MovieLens Matrix Completion,[0],[0]
"The MovieLens-1M dataset2 contains 106 integer film ratings from 1 to 5 from 6000 users for 4000 films, i.e. 1/24 of the possible ratings are available.",5.1. MovieLens Matrix Completion,[0],[0]
"Similarly, the MovieLens 100k dataset contains 943 users and 1682 films.",5.1. MovieLens Matrix Completion,[0],[0]
"Following Ravanbakhsh et al. (2016), we binarise the data taking the global mean as threshold.",5.1. MovieLens Matrix Completion,[0],[0]
"We observe only a fraction of the available data, varying from 1% to 95%, and reconstruct the remaining available data following the procedure in Section 4.2 with L = 2 latent dimensions.",5.1. MovieLens Matrix Completion,[0],[0]
Reconstruction accuracies are given as fractions of correctly reconstructed unobserved ratings in Table 1.,5.1. MovieLens Matrix Completion,[0],[0]
The given values are means from 10 randomly initialised runs of each algorithm.,5.1. MovieLens Matrix Completion,[0],[0]
The corresponding standard deviations are always smaller than 0.2%.,5.1. MovieLens Matrix Completion,[0],[0]
"The OrMachine is more accurate than message passing in all cases, except for the 1M dataset with 95% available ratings.",5.1. MovieLens Matrix Completion,[0],[0]
The OrMachine’s advantage is particularly significant if only little data is observed.,5.1. MovieLens Matrix Completion,[0],[0]
"Increasing the latent dimension L to values of 3 or 4 yields no consistent improvement, while a further increase is met with diminishing
2The MovieLens dataset is available online: https:// grouplens.org/datasets/movielens/.
returns.",5.1. MovieLens Matrix Completion,[0],[0]
We achieve the best within-sample performance for a two-layer OrMachine with different architectures performing best for different amounts of observed data.,5.1. MovieLens Matrix Completion,[0],[0]
An OrMachine with two hidden layers of sizes 4 and 2 respectively yields the best average performance.,5.1. MovieLens Matrix Completion,[0],[0]
"As indicated in Table 1, it provides better results throughout but exceeds the performance of the shallow OrMachine rarely by more than 1%.",5.1. MovieLens Matrix Completion,[0],[0]
"This indicates that there is not much higher order structure in the data, which is unsurprising given the sparsity of the observations and the low dimensionality of the first hidden layer.
",5.1. MovieLens Matrix Completion,[0],[0]
We illustrate a further advantage of full posterior inference for collaborative filtering.,5.1. MovieLens Matrix Completion,[0],[0]
We can choose a threshold for how likely we want a certain prediction to take a certain value and trade off false with true positives.,5.1. MovieLens Matrix Completion,[0],[0]
"A corresponding ROC curve for the MovieLens 100k dataset, where 10% of the available data was observed, is shown in Fig. 6.",5.1. MovieLens Matrix Completion,[0],[0]
"Single-cell RNA expression analysis is a revolutionary experimental technique that facilitates the measurement of gene expression on the level of a single cell (Blainey & Quake, 2014).",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"In recent years this has led to the discovery of new cell types and to a better understanding of tissue heterogeneity (Trapnell, 2015).",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"The latter is particularly relevant in cancer research where it helps to understand the cellular composition of a tumour and its relationship to disease progression and treatment (Patel et al., 2014).",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
Here we apply the OrMachine to binarised gene expression profiles of about 1.3 million cells for about 28 thousand genes per cell.,5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"Cell specimens were obtained from cortex, hip-
pocampus and subventricular zone of E18 (embryonic day 18) mice; the data is publicly available3.",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
Only 7% of the data points are non-zero.,5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"We set all non-zero expression levels to one, retaining the essential information of whether or not a particular gene is expressed.",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
We remove genes that are expressed in fewer than 1% of cells with roughly 11 thousand genes remaining.,5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
This leaves us with approximately 1.4×1010 data points.,5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"We apply the OrMachine for latent dimensions L = 2, . . .",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
", 10.",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"The algorithm converges to a posterior mode after 10–20 iteration, taking roughly an hour on a 4-core desktop computer and 10–30 minutes on a cluster with 24 cores.",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"We draw 125 samples and discard the first 25 as burn-in.
",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"Factorisations with different latent dimensionality form hierarchies of representations, where features that appear together in codes for lower dimensions are progressively split apart when moving to a higher dimensional latent space.",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
We illustrate this approach to analysing the inferred factorisations on calculator digits in Fig. 7.,5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
Each row corresponds to an independently trained OrMachine with the dimensionality L increasing from 3 to 7.,5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
We observe denser patterns dividing up consecutively until only the seven constituent bars remain.,5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"This is a form of hierarchical clustering that, in contrast to traditional methods, does not impose any hierarchical structure on the model.",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"We perform the same analysis on the single cell gene expression data with the results for both, gene patterns and specimen patterns shown in Fig. 8.",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
This Figure should be interpreted in analogy to Fig. 7.,5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"Furthermore, we run a gene set enrichment analysis for the genes that are unique to each inferred code, looking for associated biological states.",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"This
3https://support.10xgenomics.com
is done using the Enrichr analysis tool (Chen et al., 2013) and a mouse gene atlas (Su et al., 2004).",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"Biological states are denoted above each code, together with the logarithm to base 10 of their adjusted p-value.",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"Increasing the latent dimensionality leads to a more distributed representation with subtler, biologically plausible patterns.",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
The columns in Fig. 8 are ordered to emphasise the hierarchical structure.,5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"E.g., in the first column for L = 5 and second column for L = 6, a gene set with significant overlap to two biological processes (olfactory bulb and hippocampus) splits into two gene sets each corresponding to one of the two processes.",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"In the assignment of cells to these sets (Fig. 8B), this is associated with an increase in posterior uncertainty as to which cell expresses this property.",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
The significance levels of the associated biological processes drop from p-values on the order of 10−3 to p-values on the order of 1.,5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"In addition, typical genes for each of the biological states are annotated (Lopez-Bendito et al., 2007; Zheng et al., 2008; Demyanenko et al., 2010; Upadhya et al., 2011; Raman et al., 2013).",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
This examples illustrates the OrMachine’s ability to scale posterior inference to massive datasets.,5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"It enables the
discovery of readily interpretable patterns, representations and hierarchies, all of which are biologically plausible.",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"We have developed the OrMachine, a probabilistic model for Boolean matrix factorisation.",6. Conclusion,[0],[0]
The extremely efficient Metropolised Gibbs sampler outperforms state-of-the-art methods in matrix factorisation and completion.,6. Conclusion,[0],[0]
"It is the first method that infers posterior distributions for Boolean matrix factorisation, a property which is highly relevant in practical applications where full uncertainty quantification matters.",6. Conclusion,[0],[0]
"Despite full posterior inference, the proposed method scales to very large datasets.",6. Conclusion,[0],[0]
We have shown that tens of billions of data points can be handled on commodity hardware.,6. Conclusion,[0],[0]
The OrMachine can readily accommodate missing data and prior knowledge.,6. Conclusion,[0],[0]
"Layers of OrMachines can be stacked, akin to deep belief networks, inferring representations at different levels of abstraction.",6. Conclusion,[0],[0]
This leads to improved reconstruction performance on simulated and real world data.,6. Conclusion,[0],[0]
"Boolean matrix factorisation aims to decompose a binary data matrix into an approximate Boolean product of two low rank, binary matrices: one containing meaningful patterns, the other quantifying how the observations can be expressed as a combination of these patterns.",abstractText,[0],[0]
"We introduce the OrMachine, a probabilistic generative model for Boolean matrix factorisation and derive a Metropolised Gibbs sampler that facilitates efficient parallel posterior inference.",abstractText,[0],[0]
"On real world and simulated data, our method outperforms all currently existing approaches for Boolean matrix factorisation and completion.",abstractText,[0],[0]
"This is the first method to provide full posterior inference for Boolean Matrix factorisation which is relevant in applications, e.g. for controlling false positive rates in collaborative filtering and, crucially, improves the interpretability of the inferred patterns.",abstractText,[0],[0]
The proposed algorithm scales to large datasets as we demonstrate by analysing single cell gene expression data in 1.3 million mouse brain cells across 11 thousand genes on commodity hardware.,abstractText,[0],[0]
Bayesian Boolean Matrix Factorisation,title,[0],[0]
"In statistical applications, random graphs serve as Bayesian models for network data, that is, data consisting of objects and the observed linkages between them.",1. Introduction,[0],[0]
"Here we will focus on models for random simple graphs (that is, graphs with edges that take binary values), which are appropriate for applications where we observe either the presence or
1Pohang University of Science and Technology, Pohang, South Korea 2University of Cambridge, Cambridge, UK 3Uber AI Labs, San Francisco, CA, USA 4Hong Kong University of Science and Technology, Hong Kong.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Juho Lee <stonecold@postech.ac.kr>, Seungjin Choi <seungjin@postech.ac.kr>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
absence of links between objects in the network.",1. Introduction,[0],[0]
"For example, in social networks, nodes may represent individuals and a link (i.e., a nonzero value of an edge) could represent friendship.",1. Introduction,[0],[0]
"In a protein-protein interaction network, nodes may represent proteins and links could represent an observed physical or chemical interaction between proteins.",1. Introduction,[0],[0]
"Many domains involving network data (including social and protein-protein interaction networks) have been shown to exhibit power law, i.e., heavy-tailed, degree distributions (Barabási & Albert, 1999).",1. Introduction,[0],[0]
"Models for random graphs with power law degree distributions, also called scale-free random graphs, have therefore become one of the most actively studied areas of graph theory and network science (Bollobás et al., 2001; Albert & Barabási, 2002; Dorogovtsev & Mendes, 2002).",1. Introduction,[0],[0]
"In this paper we present a model for simple, scale-free random graphs, which we apply as a probabilistic model for several network datasets.
",1. Introduction,[0],[0]
"The model we present in this paper is a special case of the generalized random graph defined by Britton et al. (2006), and studied further by van der Hofstad (2016, Ch. 6), which outlines a framework for defining scale-free random graphs, but does not provide practical constructions, much less algorithms for performing statistical inference on the model components given data.",1. Introduction,[0],[0]
"Here we provide one such practical construction, along with a variational inference routine (Jordan et al., 1999) for efficient posterior inference.",1. Introduction,[0],[0]
"What’s more, our construction readily generalizes to include the structure of latent factors/clusters, as captured by the popular stochastic blockmodels (Nowicki & Snijders, 2001; Airoldi et al., 2009), while maintaining power law behavior in the graph.
",1. Introduction,[0],[0]
"Applying Bayesian inference algorithms on network datasets is a challenge because likelihood computations, in general, scale with the number of edges in the graph, which is O(n2) in a network with n nodes.",1. Introduction,[0],[0]
"To help overcome these difficulties, we follow Hoffman et al. (2013) and develop a stochastic variational inference algorithm in which we approximate many likelihood computations on only subsets of the data, called minibatches.",1. Introduction,[0],[0]
"In the case of a network dataset, the minibatches are comprised of subsets of edges in the graph.
",1. Introduction,[0],[0]
"We apply this inference procedure to several network
datasets that are commonly observed to possess power law structure.",1. Introduction,[0],[0]
Our experiments show that accurately capturing this power law structure improves performance on tasks predicting missing edges in the networks.,1. Introduction,[0],[0]
"We represent a simple graph with n nodes by an adjacency matrix X := (Xi,j)i,j≤n, where Xi,j = 1 if there is a link between nodes i and j and Xi,j = 0 otherwise.",2. Bayesian models for simple graphs,[0],[0]
"Here we will only consider undirected graphs, in which case X represents a symmetric matrix.",2. Bayesian models for simple graphs,[0],[0]
"Furthermore, we do not allow self links, so the diagonal entries in X are meaningless.",2. Bayesian models for simple graphs,[0],[0]
"Most probabilistic models for simple graphs take the entries in X to be conditionally independent Bernoulli random variables; in particular, for every i, j ≤ n, let pi,j be the (random) probability of a link between nodes i and j, and let Xi,j | pi,j ∼ Bernoulli(pi,j).",2. Bayesian models for simple graphs,[0],[0]
"For every simple graph x := (xi,j)i,j≤n, we may then write the likelihood for the parameters p := (pi,j)i,j≥1 given X as
P (X = x | p) =",2. Bayesian models for simple graphs,[0],[0]
"∏
i<j≤n
p xi,j i,j (1− pi,j) 1−xi,j , (1)
where in our case it should be clear that the product is only over i, j ≤ n",2. Bayesian models for simple graphs,[0],[0]
such that i < j and i 6=,2. Bayesian models for simple graphs,[0],[0]
"j. Random simple graphs date back to the Erdös–Rényi model, which may be reviewed, along with the more general theory of random graphs, in the text by Bollobás (1998).",2. Bayesian models for simple graphs,[0],[0]
A random graph is called scale-free when the fraction of nodes in the network having k connections to other nodes behaves like k−τ for large values of k and some exponent τ > 1.,2. Bayesian models for simple graphs,[0],[0]
"More precisely, let Dn,i := ∑ j 6=iXi,j denote the (random) degree of node i, for every i ≤ n.",2. Bayesian models for simple graphs,[0],[0]
"Then X is (asymptotically) scale-free when, for every node i ≤ n,
P{Dn,i = k} ∼ ck−τ , as n→∞, (2)
for some constant c > 0, a power law exponent τ",2. Bayesian models for simple graphs,[0],[0]
"> 1, and k sufficiently large.",2. Bayesian models for simple graphs,[0],[0]
"Here the notation A ∼ B denotes that the ratio A/B → 1 in the specified limit.
",2. Bayesian models for simple graphs,[0],[0]
"In order to model scale-free random graphs, Britton et al. (2006) suggested reparameterizing the model in Eq.",2. Bayesian models for simple graphs,[0],[0]
"(1) by a sequence of odds ratios ri,j := pi,j/(1 − pi,j), for every i < j ≤ n, which factorize as ri,j = UiUj , for some U := (U1, . . .",2. Bayesian models for simple graphs,[0],[0]
", Un).",2. Bayesian models for simple graphs,[0],[0]
"The node-specific factors Ui are then modeled as Ui := Wi/ √ L for some sequence of nonnegative random variables W := (W1, . . .",2. Bayesian models for simple graphs,[0],[0]
",Wn) and where L := ∑n i=1Wi.",2. Bayesian models for simple graphs,[0],[0]
"In a series of results, (Britton et al., 2006, Thms.",2. Bayesian models for simple graphs,[0],[0]
"3.1 & 3.2) and (van der Hofstad, 2016, Cor. 6.11 & Thm.",2. Bayesian models for simple graphs,[0],[0]
"6.13) assert conditions on the random variablesW so that the limiting distribution of the degrees Dn,i is a mixed Poisson distribution.",2. Bayesian models for simple graphs,[0],[0]
"We will further detail these previous results in Section 4.
",2. Bayesian models for simple graphs,[0],[0]
"The distribution of Wi is interpreted here as a prior distribution for the degree Dn,i of node i, and if its distribution has heavy tails, then so will the distribution of Dn,i. Conversely, if the distribution of Wi does not have heavy tails, then neither will the distribution of the degrees Dn,i.",2. Bayesian models for simple graphs,[0],[0]
"We explore this alternative in Section 7.
",2. Bayesian models for simple graphs,[0],[0]
"Previous authors did not suggest any particular choices for the distribution of Wi, and so we elect to model them with BFRY random variables (Bertoin et al., 2006; Devroye & James, 2014), which have a heavy-tailed distribution and have recently played a role in the construction of several power law models in Bayesian statistics.",2. Bayesian models for simple graphs,[0],[0]
"Other heavy tailed distributions, such as those exhibited by log normal random variables, may also be used to model the Wi, and these options may be explored.",2. Bayesian models for simple graphs,[0],[0]
"One benefit of the BFRY distribution is that the thickness of its tails, and thus the power law behavior of the resulting graph, may be straightforwardly controlled by the discount parameter α.",2. Bayesian models for simple graphs,[0],[0]
"Consider the model from the previous section, parameterized by the odds ratios r := (ri,j : i < j ≤ n).",3. A generalized random graph,[0],[0]
"Define
G(r) := ∏
i<j≤n
(1 + ri,j) = ∏
i<j≤n
(1 + UiUj), (3)
and note that the conditional likelihood in Eq.",3. A generalized random graph,[0],[0]
"(1) may be rewritten in terms of the degrees Dn,i as
P (X = x | r) =",3. A generalized random graph,[0],[0]
"G(r)−1 ∏
i<j≤n
(UiUj) xi,j (4)
= G(r)−1 ∏ i≤n U Dn,i i .",3. A generalized random graph,[0],[0]
"(5)
The random simple graphX is called a generalized random graph, and we will henceforth write X | r ∼ GRG(n, r).
",3. A generalized random graph,[0],[0]
"Let α ∈ (0, 1), which we call the discount parameter, and let C1, C2, . . .",3. A generalized random graph,[0],[0]
"be a sequence of positive values satisfying
lim n→∞ Cn =∞ and lim n→∞ Cαn/n = 0.",3. A generalized random graph,[0],[0]
"(6)
Let the weights W1, . . .",3. A generalized random graph,[0],[0]
",Wn be i.i.d.",3. A generalized random graph,[0],[0]
"with density
fn(w) ∝ w−α−1(1− e−w)1{0≤w≤Cn}.",3. A generalized random graph,[0],[0]
"(7)
(These are truncated BFRY random variables and will be discussed, along with a method for simulation, in Section 3.1.)",3. A generalized random graph,[0],[0]
Then the corresponding generalized random graph has an (asymptotic) power law degree distribution with power law exponent τ,3. A generalized random graph,[0],[0]
= 1,3. A generalized random graph,[0],[0]
+ α.,3. A generalized random graph,[0],[0]
"We summarize this construction in the following theorem:
Theorem 3.1.",3. A generalized random graph,[0],[0]
"For every n, let W1, . . .",3. A generalized random graph,[0],[0]
",Wn be i.i.d.",3. A generalized random graph,[0],[0]
"with density fn and let (Dn,i)i≤n be the degrees of the generalized random graph X | r ∼ GRG(n, r), where
r := (ri,j)i<j≤n is the sequence of odds ratios
ri,j = WiWj/L, i < j ≤ n, (8)
and L := ∑ iWi.",3. A generalized random graph,[0],[0]
"Then the following hold:
1.",3. A generalized random graph,[0],[0]
"For y 1, P{Dn,i = y} ∼ cy−1−α, for every node i and for some constant c, as n→∞. 2.",3. A generalized random graph,[0],[0]
"For any m, the collection Dn,1, . . .",3. A generalized random graph,[0],[0]
", Dn,m are asymptotically independent, as n→∞.
This construction is closely related to the model described by van der Hofstad (2016, Thm. 6.13), and the proof of Theorem 3.1, which is provided in the supplementary material, follows analogously to the results by Britton et al. (2006, Thms.",3. A generalized random graph,[0],[0]
3.1 & 3.2),3. A generalized random graph,[0],[0]
.,3. A generalized random graph,[0],[0]
Note that the power law exponent τ,3. A generalized random graph,[0],[0]
= 1 + α of the graph (as described by Eq.,3. A generalized random graph,[0],[0]
"(2)) is determined by the parameter α ∈ (0, 1), and takes values in (1, 2).",3. A generalized random graph,[0],[0]
"While power law exponents in (2, 3) has often been suggested in the past, it has more recently been shown that exponents within the (1, 2) range of our model is more appropriate in many domains (van der Hofstad, 2016, Ch. 1); (Crane & Dempsey, 2015).",3. A generalized random graph,[0],[0]
"A random variable W with density function fn given by Eq. (7) is a ratio of gamma and beta random variables, upper truncated at Cn.",3.1. Truncated BFRY random variables,[0],[0]
"In particular let
g ∼ gamma(1− α, 1) and b ∼ beta(α, 1), (9)
be independent, then the ratio Z := g/b has density p(z) ∝ z−α−1(1",3.1. Truncated BFRY random variables,[0],[0]
"− e−z) on (0,∞)",3.1. Truncated BFRY random variables,[0],[0]
"(by construction), which is known as the Bertoin-Fujita-Roynette-Yor (BFRY) distribution (Bertoin et al., 2006; Devroye & James, 2014) and has been used in the construction of power law models in some recent applications in machine learning (James et al., 2015; Lee et al., 2016).",3.1. Truncated BFRY random variables,[0],[0]
"The random variable W is then
obtained by upper truncating the random variable Z at Cn.",3.1. Truncated BFRY random variables,[0],[0]
By our requirements on the sequence Cn (c.f. Eq.,3.1. Truncated BFRY random variables,[0],[0]
"(6)), the density function fn of W approaches the density function of the BFRY random variable Z as n→∞, that is,
lim n→∞
fn(w) = α
Γ(1− α) w−α−1(1− e−w), (10)
which is heavy-tailed with infinite moments.",3.1. Truncated BFRY random variables,[0],[0]
It is straightforward to simulate these truncated BFRY random variables by repeatedly simulating g and b as in Eq.,3.1. Truncated BFRY random variables,[0],[0]
"(9), accepting W := g/b as a sample when W < Cn.
",3.1. Truncated BFRY random variables,[0],[0]
"The truncation of W at Cn produces a random variable with finite mean (for n < ∞), which is essential when constructing the generalized random graph and motivates the construction by van der Hofstad (2016, Thm. 6.13) alluded to earlier; see Section 4.",3.1. Truncated BFRY random variables,[0],[0]
"For simplicity, one could take Cn = n, but the flexibility to set this parameter allows us to control other properties of the model.",3.1. Truncated BFRY random variables,[0],[0]
"For example, in the next section we show how to vary this truncation level to control the sparsity of the graph.",3.1. Truncated BFRY random variables,[0],[0]
"The discount parameter α ∈ (0, 1) controls the power law behavior of the graph, where decreasing α results in heavier tails in the degree distribution of the nodes in the graph.",3.2. Controlling power law and sparsity in the graph,[0],[0]
We can visualize this behavior by simulating graphs at different values of α.,3.2. Controlling power law and sparsity in the graph,[0],[0]
"In Section 3, we set Cn = n and show the number of nodes of varying degrees in two simulated graphs, one with α = 0.2 and one with α = 0.8.
",3.2. Controlling power law and sparsity in the graph,[0],[0]
"The degree distribution of the nodes in a graph of course affects the sparsity of the graph; to characterize this relationship, we can upper bound the expected number of links in the graph as follows:
Theorem 3.2.",3.2. Controlling power law and sparsity in the graph,[0],[0]
Let En be the number of positive edges in the graph.,3.2. Controlling power law and sparsity in the graph,[0],[0]
"Then E[En] = O(nC1−αn ).
",3.2. Controlling power law and sparsity in the graph,[0],[0]
The derivation of this result is provided in the supplementary material.,3.2. Controlling power law and sparsity in the graph,[0],[0]
"While varying α can thus control the sparsity of the graph in addition to the power law behavior, we often want to decouple these behaviors, in which case we could parameterize the truncation level as Cn = nβ , for some sparsity parameter β > 0.",3.2. Controlling power law and sparsity in the graph,[0],[0]
"Note the restriction α < min{1, 1/β} must be enforced in order to ensure that the conditions in Eq.",3.2. Controlling power law and sparsity in the graph,[0],[0]
(6) are satisfied.,3.2. Controlling power law and sparsity in the graph,[0],[0]
"In this case, the bound in Theorem 3.2 becomes E[En] = O(n1+β(1−α)).",3.2. Controlling power law and sparsity in the graph,[0],[0]
"The interpretation here is that increasing the upper bound Cn increases the likelihood that any particular node will link to others, but does not affect the (asymptotic) power law characterized by Theorem 3.1.",3.2. Controlling power law and sparsity in the graph,[0],[0]
"In Section 3.2, we display the average number of positive edges in graphs that were simulated with fixed α = 0.3 and varying values of the sparsity parameter β.",3.2. Controlling power law and sparsity in the graph,[0],[0]
"We note that in simulations, we encountered numerical issues in β > 1.4 regimes.",3.2. Controlling power law and sparsity in the graph,[0],[0]
"Referring to the construction for generalized random graphs in Section 2, Britton et al. (2006, Thm. 3.1) shows that when the weights Wi have finite first and second moments, then the limiting distribution of the degree Dn,i is a mixed Poisson distribution.",4. Related work,[0],[0]
"Most such distributions are light-tailed, however, in which case the degrees will not exhibit power law behavior.",4. Related work,[0],[0]
"Britton et al. (2006, Thm. 3.2) therefore provides an alternative construction in which Wi may have infinite moments (so that it may exhibit a heavy tail), which results in a graph with a power law exponent of τ = 2.",4. Related work,[0],[0]
"Finally, van der Hofstad (2016, Thm. 6.13) suggests yet another construction where the Wi are upper truncated to be of order o(n), where n is the number of nodes in the graph.",4. Related work,[0],[0]
"The resulting random variables therefore have finite moments, yet exhibit a heavy tail, and the resulting random graph has a heavy tailed degree distribution with an arbitrary power law exponent.",4. Related work,[0],[0]
"None of these results suggest a particular choice for the distribution of Wi, however, and so we have elected to use BFRY random variables (which are heavy tailed) that are upper truncated (so that they have finite moments).",4. Related work,[0],[0]
We note that the requirements on our truncation level (c.f. Eq.,4. Related work,[0],[0]
"(6)) is less strict than the o(n) criterion of the van der Hofstad (2016, Thm.",4. Related work,[0],[0]
"6.13) construction.
",4. Related work,[0],[0]
"The reader may consult the surveys by Bollobás & Riordan (2003); Albert & Barabási (2002); Dorogovtsev & Mendes (2002) for a background on scale-free random graphs, which is too large to review here.",4. Related work,[0],[0]
"While these models are numerous, the following recent pieces of work in the Bayesian statistics and machine learning communities may be of interest to the reader: Caron & Fox (2014); Veitch & Roy (2015); Crane & Dempsey (2016); Cai & Broderick (2015).",4. Related work,[0],[0]
"This collection of work discusses power law degree distributions, albeit in some cases in multi-graphs
(i.e., graphs with nonnegative integer-valued edges) and in some cases the power law behavior is not characterized, only numerically observed in simulations.",4. Related work,[0],[0]
"Many of these models can be seen to invoke their power law properties from the Pitman–Yor process (Pitman & Yor, 1997) (or related stochastic processes), where the extent of this behavior is controlled by the discount parameter α ∈ (0, 1) of the Pitman–Yor model, which, like the BFRY distribution, is related to a stable subordinator of index α.",4. Related work,[0],[0]
Latent factor models for relational data assume that a set of latent clusters underlie the network.,5. Incorporating latent factors,[0],[0]
"For example, in a social network, the latent factors could be the unobserved hobbies or interests of individuals, which determine the observed friendships in the network.",5. Incorporating latent factors,[0],[0]
"Bayesian models for latent factors in relational data are widespread, with some of the most popular based on stochastic blockmodels, where models for unsupervised learning, or clustering, are used to infer the latent factors (Nowicki & Snijders, 2001; Kemp et al., 2006; Airoldi et al., 2009; Miller et al., 2009).",5. Incorporating latent factors,[0],[0]
"In this section, we present extensions of the generalized random graph that incorporate latent factors by scaling the odds ratios, while maintaining their power law degree distribution.
",5. Incorporating latent factors,[0],[0]
"We will first provide a general result showing how to incorporate random scaling variables into the model, followed by specific examples that model these scaling variables with latent clusters.",5. Incorporating latent factors,[0],[0]
"Let the odds ratios in the generalized random graph be given by ri,j = Ai,jUiUj for some Ai,j ≥ 0.",5. Incorporating latent factors,[0],[0]
"Note that pi,j → 1 as Ai,j → ∞ and pi,j → 0 as Ai,j → 0, and so the edge-specific weight Ai,j simply scales the link probability.",5. Incorporating latent factors,[0],[0]
"The random graph X | r ∼ GRG(n, r) then has the likelihood
P (X = x | r) =",5. Incorporating latent factors,[0],[0]
"G(r)−1 ∏
i<j≤n
A xi,j i,j ∏ i≤n U Dn,i i , (11)
where the normalization term G(r) in Eq.",5. Incorporating latent factors,[0],[0]
"(3) is now
G(r) := ∏
i<j≤n
(1 +Ai,jUiUj) (12)
= ∑ x ∏",5. Incorporating latent factors,[0],[0]
"i<j≤n A xi,j i,j ∏ i≤n U Dn,i i , (13)
where the final equality follows simply because∑ x P (X = x | r) = 1.",5. Incorporating latent factors,[0],[0]
"So constructed, the odds ratios r will influence the link probabilities in the generalized random graph, but will not affect the power law behavior of the degree distributions (under some assumptions on the random variables Ai,j).",5. Incorporating latent factors,[0],[0]
"We summarize this construction in the following theorem, the proof for which is provided in the supplementary material:
Theorem 5.1.",5. Incorporating latent factors,[0],[0]
Let (Wi)i≤n be i.i.d. random variables with density function fn(w) (in Eq. (7)).,5. Incorporating latent factors,[0],[0]
"Let (Ai,j)i<j≤n be a collection of uniformly bounded random variables, where, for every i ≤ n, the collection (Ai,j)j>i is exchangeable.",5. Incorporating latent factors,[0],[0]
"Let (Dn,i)i≤n be the degrees of the random graph X | r ∼ GRG(n, r), where r := (ri,j)i<j≤n is the sequence of odds ratios
ri,j = Ai,jWiWj/L, i < j ≤ n, (14) and where L := ∑ iWi.",5. Incorporating latent factors,[0],[0]
"Then the degrees (Dn,i)i≤n satisfy statements (1) and (2) in Theorem 3.1.
",5. Incorporating latent factors,[0],[0]
"For example, we may construct stochastic blockmodels, such as those introduced by Nowicki & Snijders (2001), as follows: For every i ≤ n, let Zi be a random variable taking values in {1, . . .",5. Incorporating latent factors,[0],[0]
",K}, indicating which one (and only one) of K different factors to associate with node i.",5. Incorporating latent factors,[0],[0]
"We want the latent cluster assignments for two nodes i and j to influence their link probability, which we could capture with a set of parameters θk,`, for k, ` = 1, . . .",5. Incorporating latent factors,[0],[0]
",K. Then the parameter θZi,Zj could represent, or influence, the probability of a link between nodes i and j. Taking a Bayesian approach, the indicator variables Zi may be modeled with a Dirichlet-categorical conjugate distribution and their values may be inferred via probabilistic inference.",5. Incorporating latent factors,[0],[0]
"An example of such a model could be summarized as follows: Let
Zi ∼ categorical(π), i ≤ n, (15) π ∼ Dirichlet(c/K), where c > 0, (16)
θ`,k ∼ gamma(aθ, bθ), `, k ≤ K, (17) Ai,j = θZi,Zj , i < j ≤ n, (18)
and construct the random graph X as in Theorem 5.1.",5. Incorporating latent factors,[0],[0]
"Kemp et al. (2006) developed a nonparametric extension of a similar model that in a sense takes the limit K → ∞, allowing an appropriate number of clusters to be automatically inferred from the data.",5. Incorporating latent factors,[0],[0]
"In this case, the marginal law of the indicator variables Z1, . . .",5. Incorporating latent factors,[0],[0]
", Zn is given by a Chinese restaurant process (with concentration parameter c).
",5. Incorporating latent factors,[0],[0]
"Several generalizations of the stochastic blockmodel allow the clusters underlying the network to overlap, leading to mixed membership stochastic blockmodels (Airoldi et al., 2009) or the related latent feature relational models (Miller et al., 2009).",5. Incorporating latent factors,[0],[0]
"To capture this structure, we may generalize the indicators Zi to now represent a binary K-vector with entry Zi,k = 1 indicating node i is associated with cluster k, now called a feature, and Zi,k = 0 otherwise.",5. Incorporating latent factors,[0],[0]
"One example of such a model could be summarized as follows:
Zi,k ∼ Bernoulli(pk), i ≤ n, k ≤ K, (19) pk ∼ beta(c, cγ/K), k ≤ K, and c, γ > 0, (20) θ`,k ∼ gamma(aθ, bθ), `, k = 1, 2, . . .",5. Incorporating latent factors,[0],[0]
", (21)
Ai,j = ∑ k,` θk,`Zi,kZj,`, i < j ≤ n, (22)
and construct the random graph X as in Theorem 5.1.",5. Incorporating latent factors,[0],[0]
"Miller et al. (2009) derived a nonparametric extension of this model that in a sense takes the limit K →∞, in which case the marginal law of the vectors Z1, . . .",5. Incorporating latent factors,[0],[0]
", Zn is that of an Indian buffet process (with mass parameter γ and concentration parameter c)",5. Incorporating latent factors,[0],[0]
"(Ghahramani et al., 2007).",5. Incorporating latent factors,[0],[0]
"We derive a variational Bayesian inference algorithm (Jordan et al., 1999) that approximates the (optimal state of the) posterior distribution of the model components, given a network dataset.",6. Variational inference,[0],[0]
"We approximate the required gradients in this procedure with stochastic gradient ascent (Bottou, 2010; Hoffman et al., 2013), computed on minibatches (i.e., subsets) of edges in the graph.",6. Variational inference,[0],[0]
"In variational inference, we approximate the posterior distribution on the latent variables W := (W1, . . .",6.1. The variational lower bound,[0],[0]
",Wn) with a variational distribution q(W ; θ), the parameters θ of which are fit to maximize the following lower bound on the marginal likelihood
log p(X) ≥ Eq(W ;θ) [ log
p(X |W ;α)p(W ;α) q(W ; θ)
] , (23)
where p(X | W ) is the likelihood function computed as in Eq.",6.1. The variational lower bound,[0],[0]
"(5), and p(W ;α) is the prior on W represented by the density function in Eq.",6.1. The variational lower bound,[0],[0]
(7).,6.1. The variational lower bound,[0],[0]
"The (non random) discount parameter α is inferred by corresponding gradient ascent updates maximizing the likelihood of the model, which is described in Section 6.4.",6.1. The variational lower bound,[0],[0]
We specify a mean field variational distribution q(W ; θ) =∏n i=1 q(Wi; θi).,6.1. The variational lower bound,[0],[0]
"We considered several approximations for the marginals q(Wi; θi) including truncated BFRY and truncated gamma distributions, however, in our experiments we found that the following rectified gamma distribution performed well:
Wi =q min{W ′i , Cn}, (24) W ′i ∼ gamma(θi,shp, θi,rte), (25)
independently for every i ≤ n, where θi,shp and θi,rte denote the shape and rate parameters of the gamma distribution, respectively, and the notation =q emphasizes that this formula holds under the variational distribution q.",6.1. The variational lower bound,[0],[0]
We maximize the lower bound on the right hand side of Eq.,6.2. Stochastic gradient ascent,[0],[0]
"(23) by stochastic gradient ascent, where on the t-th step of the algorithm, we make the following updates to the
parameters in parallel
θ (t+1) i ← θ (t)",6.2. Stochastic gradient ascent,[0],[0]
"i + ρt∇θiEq(W ;θ(t))[L(X,W ; θ (t))",6.2. Stochastic gradient ascent,[0],[0]
"], (26)
",6.2. Stochastic gradient ascent,[0],[0]
"for i ≤ n and some sequence (ρt)t≥1 of positive numbers satisfying the Robbins–Monro criterion (Robbins & Monro, 1951) ∑ t ρt =∞ and ∑ t ρ",6.2. Stochastic gradient ascent,[0],[0]
2,6.2. Stochastic gradient ascent,[0],[0]
"t <∞, and where
L(X,W ; θ) := log p(X,W ;α)− log q(W ; θ) (27)
= ∑
(i,j)∈E log p(Xi,j |W ) + n∑ i=1",6.2. Stochastic gradient ascent,[0],[0]
"log p(Wi;α)
",6.2. Stochastic gradient ascent,[0],[0]
− n∑ i=1,6.2. Stochastic gradient ascent,[0],[0]
"log q(Wi; θi), (28)
where E denotes the observed edges (both links and nonlinks) in the dataset.",6.2. Stochastic gradient ascent,[0],[0]
"We cannot evaluate the expectation (with respect to the rectified gamma distributions q(W ; θ)) analytically, and so we elect to use a particular Monte Carlo approximation of this gradient detailed by Knowles (2015), which was developed for gamma variational distributions and easily applies to the rectified gamma case.
",6.2. Stochastic gradient ascent,[0],[0]
"Briefly, for every i ≤ n, create the collection of S Monte Carlo samples from the variational distribution as follows: Independently for s ≤ S, let z (s) i ∼ Uniform(0, 1), and set W (s) i = ψ(z (s) i ; θi), where ψ(z; θ) :",6.2. Stochastic gradient ascent,[0],[0]
"= min{F−1θ (z), Cn} and F −1 θ",6.2. Stochastic gradient ascent,[0],[0]
(x) is the inverse of the cumulative distribution function for a gamma random variable.,6.2. Stochastic gradient ascent,[0],[0]
"For convenience, we recall that
Fa,b(x) = ∫ x 0 ba Γ(a) ta−1e−btdt. (29)
",6.2. Stochastic gradient ascent,[0],[0]
"For every k ≤ n, the gradient with respect to the parameters θk is then approximated by
∇θkEq(W ;θ)[L(X,W ; θ)]
≈ 1 S ∑ s ∇WkL(X,W (s); θ)∇θkψ(z (s) k ; θk), (30)
where W (s) := (W (s)1 , . . .",6.2. Stochastic gradient ascent,[0],[0]
",W (s) n ).",6.2. Stochastic gradient ascent,[0],[0]
"This estimator is unbiased and has low enough variance that often a single sample suffices for the approximation (Salimans & Knowles, 2013; Kingma & Welling, 2014).",6.2. Stochastic gradient ascent,[0],[0]
The gradient of ψ is nonzero only when {F−1θk (z (s) k ),6.2. Stochastic gradient ascent,[0],[0]
"< Cn}, in which case we may immediately obtain the partial derivative with respect to the rate parameter; in particular, we have
∇θk,rteψ(z (s) k ; θk) =
{ z (s) k
θk,rte , if F−1θk (z (s) k )",6.2. Stochastic gradient ascent,[0],[0]
"< Cn,
0, otherwise.",6.2. Stochastic gradient ascent,[0],[0]
"(31)
",6.2. Stochastic gradient ascent,[0],[0]
"The partial derivative with respect to the shape parameter ∇θk,shpψ(z (s) k ; θk) does not have a closed form solution and must be approximated.",6.2. Stochastic gradient ascent,[0],[0]
"Different approximation routines are suggested by Knowles (2015) for different regimes of the shape parameter θk,shp, and we found these approximations to be accurate and efficient in our experiments.",6.2. Stochastic gradient ascent,[0],[0]
"Computing the n required gradients in Eq. (26) may be done in parallel, and this computation, whether performed analytically or with automatic differentiation methods, scales with the number of edges in the graph.",6.3. Minibatches of edges in the graph,[0],[0]
"This can be prohibitive for many network datasets, and we therefore introduce a further approximation where this gradient is evaluated on subsets (a.k.a. minibatches) of the dataset, a technique from stochastic gradient ascent (Bottou, 2010) adopted in the context of variational Bayesian inference by Hoffman et al. (2013).",6.3. Minibatches of edges in the graph,[0],[0]
"In the case of a network dataset, we may select minibatches that are subsets of the observed edges in the graph.",6.3. Minibatches of edges in the graph,[0],[0]
"In particular, write the gradient of Eq.",6.3. Minibatches of edges in the graph,[0],[0]
(28) with respect to the variable Wk (which is required by Eq. (30)),6.3. Minibatches of edges in the graph,[0],[0]
"as
∇WkL(W (s); θ) = ∑
(i,j)∈E
g(i,j)(X,W (s); k), (32)
where g(i,j)(X,W ; k) := ∇Wk",6.3. Minibatches of edges in the graph,[0],[0]
"[log p(Xi,j | W ) + |E|−1 log p(W ;α)− |E|−1 log q(W ; θ)] is the gradient that ignores all but one edge of the graph.",6.3. Minibatches of edges in the graph,[0],[0]
"We may therefore compute the unbiased estimate of this gradient
∇WkL(W",6.3. Minibatches of edges in the graph,[0],[0]
"(s); θ) ≈ |E| |B| ∑ (i,j)∈B g(i,j)(X,W (s); k), (33)
on a minibatch B ⊆ E of the observed edges.",6.3. Minibatches of edges in the graph,[0],[0]
"Without good prior knowledge of how to set the discount parameter α and the sparsity parameter β controlling the power law and sparsity behaviors of the graph, respectively, we infer their values from the data.",6.4. Inference on the parameters α and β,[0],[0]
"First consider the discount parameter, which we infer with gradient ascent.",6.4. Inference on the parameters α and β,[0],[0]
"After every update to the latent variables W , we fix them to their mean under the distribution q, i.e., Ŵ := (Ŵ1, . . .",6.4. Inference on the parameters α and β,[0],[0]
", Ŵn) where Ŵi = Eq(Wi;θi)[Wi], and take a step in the direction of the gradient
∇α log p(Ŵ ;α) = n∑ i=1",6.4. Inference on the parameters α and β,[0],[0]
"∇α log p(Ŵi;α) (34)
= n∑ i=1",6.4. Inference on the parameters α and β,[0],[0]
"[ −∇αZα,β Zα,β − log(Ŵi) ] , (35)
which is straightforward to derive from the density function in Eq. (7), and where the normalization term
Zα,β := ∫ Cn 0 w−α−1(1− e−w) dw (36)
is a function of α and β, if we let Cn = nβ as suggested in Section 3.2.",6.4. Inference on the parameters α and β,[0],[0]
"We do not have a closed form solution for
α = 0.3 BFRY -57323.19 ± 91.62 -57675.72",6.4. Inference on the parameters α and β,[0],[0]
"± 31.71 Gamma -71341.90 ± 116.82 -71841.66 ± 47.38
α = 0.5 BFRY -21077.62 ± 79.64 -21289.75 ±",6.4. Inference on the parameters α and β,[0],[0]
34.23 Gamma -24430.38 ± 73.06 -24701.06 ±,6.4. Inference on the parameters α and β,[0],[0]
"11.31
α = 0.7 BFRY -7894.67 ± 41.84 -8027.42 ± 51.08 Gamma -8511.48 ± 22.45 -8601.50 ± 15.42
this term when Cn < ∞, and, unfortunately, inference on model parameters where the likelihood is difficult to evaluate is a challenging problem; for example, see the approaches taken by Murray et al. (2006) on such problems, which those authors call doubly intractable distributions.",6.4. Inference on the parameters α and β,[0],[0]
"Accurate inference for α is important in our model, because it controls the power law behavior of the graph.",6.4. Inference on the parameters α and β,[0],[0]
"In our experiments, we approximate the gradient in Eq.",6.4. Inference on the parameters α and β,[0],[0]
"(35) for (fixed β) by approximating Zα,β (via Eq. (36)) and ∇αZα,β = ∫ Cn 0 −w−α−1(1 − e−w) logw dw, with line integrals.",6.4. Inference on the parameters α and β,[0],[0]
"In the Section 7, we demonstrate that this approximation works well in various regimes of α, with slight overestimation for moderate values.
",6.4. Inference on the parameters α and β,[0],[0]
"Similar approaches to infer β may be derived with finite difference approximations; we did not find these approaches successful in our experiments, however, and so we instead select β by cross validation.",6.4. Inference on the parameters α and β,[0],[0]
"We first demonstrate how the inference procedure in Section 6.4 can correctly differentiate between various regimes
of α.",7. Experiments,[0],[0]
"We ran an experiment where for each value α ∈ {0.1, 0.3, 0.5, 0.7}, we simulated 10 datasets from the model with n = 1, 000 nodes, while fixing β = 1.0.",7. Experiments,[0],[0]
"For each simulated dataset, we ran an instance of the inference routine with α randomly initialized.",7. Experiments,[0],[0]
"In Fig. 3, we show the trace plots of alpha during each instance of the inference routine.",7. Experiments,[0],[0]
"For comparison, the true values of α are also shown as horizontal dashed lines.",7. Experiments,[0],[0]
"We can see that the inference routine can correctly distinguish between these different regimes of α, with slight overestimation in the moderate α regime.",7. Experiments,[0],[0]
"Interestingly, despite random initializations of α ∈ (0, 1), the algorithm always immediately inflates α to around 0.9, and then slowly decreases this value during inference, regardless of what value of α generated the data.
",7. Experiments,[0],[0]
We next demonstrate that accurately capturing power law structures in datasets will improve predictive performance.,7. Experiments,[0],[0]
"While fixing β = 1.0, we simulate three network datasets with 5,000 nodes from our model with discount parameters α = 0.3, 0.5, and 0.7, respectively, which therefore exhibit increasingly lighter-tailed degree distributions.",7. Experiments,[0],[0]
"The generated graphs have 117,300, 32,925, and 9,460 links, respectively.",7. Experiments,[0],[0]
"To establish a baseline model that does not exhibit power law degree distributions but is otherwise comparable to our model, we implement the generalized random graph where the node-specific weights are constructed from the gamma random variables Wi ∼ gamma(θ, 1), for some positive parameter θ, i.i.d. for every node i ≤",7. Experiments,[0],[0]
n. Note that the parameter θ controls the sparsity of the generated graph; larger values of θ imply denser graphs.,7. Experiments,[0],[0]
"It follows analogously to Theorem 5.1 that
P{Dn,i = k} ∼ kθ−1
2k+θ , (37)
for k 1, as n → ∞.",7. Experiments,[0],[0]
"This model therefore does not exhibit power law behavior, as desired.",7. Experiments,[0],[0]
"We refer to this model as “Gamma” and the power law graph model as “BFRY”.
",7. Experiments,[0],[0]
"We ran an experiment holding out 20% of the edges in the
simulated graphs as test sets, training the two models on the remaining 80% of the edges.",7. Experiments,[0],[0]
"We used a mini-batch size of 5,000 edges (note that the training dataset corresponds to almost 10 million observed edges).",7. Experiments,[0],[0]
"We ran each inference procedure for 20,000 steps of stochastic gradient ascent updates, using Adam (Kingma & Ba, 2015) to adjust the learning rates at each step.",7. Experiments,[0],[0]
"We repeated each experiment 5 times, each time holding out a different test set and using a different random initialization.",7. Experiments,[0],[0]
"Again, for this experiment we fixed β = 1.",7. Experiments,[0],[0]
"In Table 1 we report a mean loglikelihood metric for the test datasets, where the metric for each run is obtained by averaging the test log-likelihoods across the states for the last 4,000 steps of the inference procedure; the displayed intervals are at ±1 standard deviation about the metric, from across the 5 repeats.",7. Experiments,[0],[0]
"We also report a max log-likelihood metric, which simply records the maximum test log-likelihood across the last 4,000 steps of the inference procedure, instead of the average.",7. Experiments,[0],[0]
"The best performing method is highlighted in bold (which in each case was the BFRY model).
",7. Experiments,[0],[0]
"In each case, we see that the BFRY model achieves higher test log-likelihood metrics than the Gamma model, as expected, implying that accurately capturing a power law degree distribution improves predictive performance (when power law behavior is truly present in the network).",7. Experiments,[0],[0]
"In Table 3, we report the inferred values for α, which were reasonably accurate, though we see slight overestimation for some regimes, as seen in the demonstration earlier.",7. Experiments,[0],[0]
"For the baseline Gamma model, we optimized the hyperparameter θ using gradient ascent maximizing the evidence lower bound of the model",7. Experiments,[0],[0]
"(c.f. Eq. (23)), and the inferred values are also reported in Table 3.
",7. Experiments,[0],[0]
"Next, we ran similar experiments on the following network datasets, each of which are expected to exhibit power law degree distributions:
• ‘USTop500Airports’: 500 nodes, 2,980 links • ‘openflights’: 7,976 nodes, 15,243 links • ‘polblogs’: 1,490 nodes, 9,517 links • ‘Facebook107’: 1,034 nodes, 26,749 links
Where appropriate, we saved only the upper triangular parts of the adjacency matrices.",7. Experiments,[0],[0]
"The ‘USTop500Airports’ dataset contains the (undirected, unweighted) flight connections between the 500 busiest US airports.",7. Experiments,[0],[0]
"The similar,
though much larger, ‘openflights’ dataset contains the flight connections between non-US airports.",7. Experiments,[0],[0]
"Scale-free networks have been proposed for such traffic networks, detailed for these datasets by Colizza et al. (2007).",7. Experiments,[0],[0]
"The ‘polblogs’ dataset contains the links between political blogs (judged by hyperlinks between the front webpages of the blogs) in the period leading up to the 2004 US presidential election, which is observed to exhibit power law degree distributions by Adamic & Glance (2005).",7. Experiments,[0],[0]
"The ‘Facebook107’ dataset contains “friendships” between users of a Facebook app, collected by Leskovec & McAuley (2012); social networks are widely studied for their power law degree distributions.
",7. Experiments,[0],[0]
"For both the Gamma and BFRY models, we ran our variational inference procedure for 20,000 steps on each dataset.",7. Experiments,[0],[0]
"As before, we repeated the experiment 5 times for each network, each time holding out a different 20% of the edges in the network as a testing set.",7. Experiments,[0],[0]
"We selected the value of β from among the grid {0.6, 0.9, 1.0, 1.2, 1.4} with 5-fold cross validation on the training set.",7. Experiments,[0],[0]
"We set the minibatch size to be equal to the number of nodes in the graph; for example, we used minibatches of 1,490 edges for the polblog dataset.",7. Experiments,[0],[0]
"The evaluation metrics on the test datasets are summarized in Table 2, and the inferred hyperparameter values are reported in Table 3.",7. Experiments,[0],[0]
"We see that the BFRY model once again outperforms the Gamma baseline model, according to the test log-likelihood metrics.
",7. Experiments,[0],[0]
Probabilistic inference on α by the BFRY model provides some of the most interesting analyses here.,7. Experiments,[0],[0]
"With α ≈ 0.00 (underflowing our machine’s precision), the Facebook107 social network has the degree distribution with the heaviest tails, followed by the USTop500Airports traffic network with α ≈ 0.23, the polblog citation network with α ≈ 0.64, and the openflights network has the lightest tailed degree distribution with α ≈ 0.67.",7. Experiments,[0],[0]
"Future work could focus on implementing the latent factor modeling generalizations presented in Section 5, which are natural assumptions in many domains where networks are expected to exhibit power law degree distributions.",8. Future work,[0],[0]
"Alternative approaches to inference on the sparsity parameter β should also be explored, since controlling the sparsity in the graph was important for good predictive performance.",8. Future work,[0],[0]
The authors thank Remco van der Hofstad for helpful advice and anonymous reviewers for helpful feedback.,Acknowledgements,[0],[0]
"J. Lee and S. Choi were partly supported by an Institute for Information & Communications Technology Promotion (IITP) grant, funded by the Korean government (MSIP) (No.20140-00147, Basic Software Research in Human-level Lifelong Machine Learning (Machine Learning Center)) and Naver, Inc.",Acknowledgements,[0],[0]
"C. Heaukulani undertook this work in part while a visiting researcher at the Hong Kong University of Science and Technology, who along with L. F. James was funded by grant rgc-hkust 601712 of the Hong Kong Special Administrative Region.",Acknowledgements,[0],[0]
"We present a model for random simple graphs with power law (i.e., heavy-tailed) degree distributions.",abstractText,[0],[0]
"To attain this behavior, the edge probabilities in the graph are constructed from Bertoin–Fujita–Roynette–Yor (BFRY) random variables, which have been recently utilized in Bayesian statistics for the construction of power law models in several applications.",abstractText,[0],[0]
"Our construction readily extends to capture the structure of latent factors, similarly to stochastic blockmodels, while maintaining its power law degree distribution.",abstractText,[0],[0]
"The BFRY random variables are well approximated by gamma random variables in a variational Bayesian inference routine, which we apply to several network datasets for which power law degree distributions are a natural assumption.",abstractText,[0],[0]
"By learning the parameters of the BFRY distribution via probabilistic inference, we are able to automatically select the appropriate power law behavior from the data.",abstractText,[0],[0]
"In order to further scale our inference procedure, we adopt stochastic gradient ascent routines where the gradients are computed on minibatches (i.e., subsets) of the edges in the graph.",abstractText,[0],[0]
Bayesian inference on random simple graphs with power law degree distributions,title,[0],[0]
A classical estimation problem in many scientific inquiries is the well-studied change point detection problem where one tries to estimate when some properties of a sequence of random variables changes.,1. Introduction,[0],[0]
"This local property is of prime importance in many learning tasks such as signal segmentation (Abou-Elailah et al., 2016; Kim et al., 2009), change point detection in comparative genomics for early cancer diagnosis (Lai et al., 2005), and modeling and forecasting of changes in financial data (Lavielle & TeyssiÃĺre, 2006; Spokoiny, 2009).
",1. Introduction,[0],[0]
"For other applications, one needs more than this local answer and is interested in a more general overview of the time series where for instance earlier data samples behave like new ones creating a clustering effect.",1. Introduction,[0],[0]
"Examples of this are found in: electricity market data, where prices
1KTH Royal Institute of Technology, Stockholm, Sweden.",1. Introduction,[0],[0]
"Correspondence to: Othmane Mazhar <othmane@kth.se>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
might have different behavior corresponding to different price regimes that might reappear depending on some triggering events; signal partitioning with some parts of the signal sharing similar properties; and speech segmentation with different alternating sources.",1. Introduction,[0],[0]
"Generally speaking, it is of interest in these situations to determine not only the changes but also the clusters for a more precise description of the inhomogeneous time series.
",1. Introduction,[0],[0]
Parametric models for solving the change point detection problem have been proposed in Cleynen & Lebarbier (2014) and Rigaill et al. (2012).,1. Introduction,[0],[0]
"However, in dealing with the change point and clustering problem we would naturally require that our solution does not assume any knowledge of the number of changes nor the actual number of clusters, as these numbers would evolve over time, so we expect new changes in the process to happen and new clusters to form as N , the number of samples, grows.",1. Introduction,[0],[0]
"Thus, any practical procedure should be able to estimate these numbers and also have adaptive guarantees with respect to how fast these numbers grow.",1. Introduction,[0],[0]
"Similar setups for change point detection have been the subject of study by Harchaoui & Cappé (2007), Arlot et al. (2016) and Garreau & Arlot (2017) who use characteristic kernels for detecting changes in the distribution, while from a computational standpoint a more effective implementation has been proposed by Celisse et al. (2017).",1. Introduction,[0],[0]
"In this study, we will restrict ourselves to an iid (independent and identically distributed) Gaussian sequence model of the data with known variance, noting that the same study can be done using kernels and that the algorithm we develop can be effectively implemented using the same procedure as in (Celisse et al., 2017), as explained later in the paper.
",1. Introduction,[0],[0]
"Two other related lines of research, but which we do not explore here, are on-line algorithms for segmentation and L1-regularized segmentation.",1. Introduction,[0],[0]
"We refer the reader to (Tartakovsky et al., 2014) for an extensive review of on-line algorithms.",1. Introduction,[0],[0]
Data segmentation using the L1-penalty was introduced by Rudin et al. (1992).,1. Introduction,[0],[0]
"The one-dimensional case, corresponding to the Fussed LASSO, has been studied in (Tibshirani et al., 2005) and (Rennie & Dobson, 1969) and an efficient algorithm has been proposed by Arnold & Tibshirani (2016).",1. Introduction,[0],[0]
"More recent results can be found in (Dalalyan et al., 2017) for the one-dimensional case and (Hütter & Rigollet, 2016) for two-dimensional case.
",1. Introduction,[0],[0]
"Main contribution: The generalized setting of change point detection while clustering the segments for sequences
of data points does not seem to have been previously studied.",1. Introduction,[0],[0]
"In this work, we propose a two-pass dynamic programming algorithm for selecting an adequate model from a collection of candidate models.",1. Introduction,[0],[0]
We motivate the choice of the algorithm computationally by showing that it runs in O(N2D,1. Introduction,[0],[0]
"+ D4) time (where D is an upper bound on the number of change points), statistically by showing that it can be seen as an approximation of a computationally hard MAP optimization problem for which we can derive an oracle inequality that guarantees low sample complexity, consistency and adaptivity, and practically by testing the model on simulation data.
",1. Introduction,[0],[0]
Structure of the paper: In Section 2 we formulate the problem as one of nonparametric model selection from a family of models over all partitions of the data set.,1. Introduction,[0],[0]
"After some preliminaries and notations are given in Section 3, we propose in Section 4 a two-pass dynamic programming algorithm as a computationally effective relaxation of the optimization criterion and analyze its computational cost.",1. Introduction,[0],[0]
"We then put the model selection problem in a Bayesian framework in Section 5, and use a Laplace-type approximation to derive as optimization criterion the maximum a-posteriori probability.",1. Introduction,[0],[0]
"In Section 6 we derive an oracle inequality for the criterion that our algorithm is approximating, and study its properties.",1. Introduction,[0],[0]
Experimental results showing that the clusters and segments can be effectively estimated are presented in Section 7 using simulation data.,1. Introduction,[0],[0]
"Let Y be a measurable space and Y1, Y2, . . .",2. Problem formulation,[0],[0]
", YN ∈ Y denote random variables with distributions PYi .",2. Problem formulation,[0],[0]
Our goal is on one hand to detect changes in the sequence of distribution measures (PYi) N i=1,2. Problem formulation,[0],[0]
and on the other hand to cluster the data points coming from the same process.,2. Problem formulation,[0],[0]
"Hence we put random variables between two consecutive changes in the same segment, and we think of random variables of the same segment or different segments as belonging to the same cluster if they are the realization of the same process.
",2. Problem formulation,[0],[0]
"One important case both in theory and in practice is the uniform constant design model were the Yis depend on deterministic variables uniformly spaced on a grid Xi = i for i ∈ J1, NK := {1, . . .",2. Problem formulation,[0],[0]
", N} through a regression function f∗ with an additive iid random noise ( i)Ni=1.",2. Problem formulation,[0],[0]
"Taking the distribution of the i’s as N (0, σ2) with known variance, we end up with the following Gaussian sequence model:
Yi = f ∗",2. Problem formulation,[0],[0]
"i + i, for i ∈ J1, NK.",2. Problem formulation,[0],[0]
"(1)
Here we are placed in a regression setting of the form Y = f∗+ , where Y =",2. Problem formulation,[0],[0]
"[Y1 · · · YN ]T , f∗ =",2. Problem formulation,[0],[0]
"[f∗1 · · · f∗N ]T and = [ 1 · · · N ]T ∼ N (0, σ2IN ), and we are interested in estimating f∗ as a piecewise constant function that takes limited number of values.
",2. Problem formulation,[0],[0]
"We emphasize that it is unlikely that the data correspond exactly to a piecewise constant function plus independent
random Gaussian noise and that we are in this low dimensional hidden structure exactly, yet there might exist a good sparse linear approximation.",2. Problem formulation,[0],[0]
"Hence our search is not for an exact model, rather we are trying to select the best model in a collection of candidates, as we explain in the next section.",2. Problem formulation,[0],[0]
"We would like to perform dimensionality reduction by exploiting the hidden structure on the data sequence Y1, Y2, . . .",3. Preliminaries and notation,[0],[0]
", YN .",3. Preliminaries and notation,[0],[0]
To do this we split it into different segments while also putting the segments sharing the same mean into the same cluster.,3. Preliminaries and notation,[0],[0]
Hence if we knew the clusters our problem reduces to fitting a constant to a set of observations over each cluster.,3. Preliminaries and notation,[0],[0]
"Observe that if f∗ is constant over parts of J1, NK, then it determines a clustering of Y1, Y2, . . .",3. Preliminaries and notation,[0],[0]
", YN over the values where it is constant.",3. Preliminaries and notation,[0],[0]
"Hence, we can think about the problem as, first determining the clustering of the Y1, Y2, . . .",3. Preliminaries and notation,[0],[0]
", YN which would result in a partition π of J1, NK, and then choosing the best value of f̂ over each part as our estimate.",3. Preliminaries and notation,[0],[0]
"So f∗ belong to the subspace Fπ: subspace of functions that are constant over the parts of the partition π.
To formalize this, let M be an index set over the collection of partitions ΠN of J1, NK; given m ∈ M, denote by Fm the subspace of functions that are constant over the parts of πm.",3. Preliminaries and notation,[0],[0]
"Our goal is two-fold: find m̂ as the index estimate of Fm̂, the subspace where the estimate of f∗ lives, and from Fm̂ compute f̂m̂ as our estimate.",3. Preliminaries and notation,[0],[0]
We represent a partition π as an unordered collection of its subsets π = {,3. Preliminaries and notation,[0],[0]
"[1], [2], . . .",3. Preliminaries and notation,[0],[0]
", [|π|]} with [k] being the kth-equivalent class, -part or -cluster, and |π| the cardinality of the partition.",3. Preliminaries and notation,[0],[0]
Every part [k] can be seen as the union of segments [k] = {,3. Preliminaries and notation,[0],[0]
"[k1], [k2], . . .",3. Preliminaries and notation,[0],[0]
",",3. Preliminaries and notation,[0],[0]
[k|[k]|]} where (ki) |[k]| i=1 is the collection of maximal intervals in [k] that we call segments of the kth-cluster.,3. Preliminaries and notation,[0],[0]
The last element in each segment [ki] is called a change point.,3. Preliminaries and notation,[0],[0]
We define d′m := |πm|−1 = dim(Fm)−1 as the clustering dimension.,3. Preliminaries and notation,[0],[0]
Even though this choice might create some confusion it will be consistent the notations used in the proofs of sections 5 and 6.,3. Preliminaries and notation,[0],[0]
Also we define d′′m = |πm|0 := |∪ d′m+1 k=1,3. Preliminaries and notation,[0],[0]
"[k]| as the change point dimension.
",3. Preliminaries and notation,[0],[0]
"To link partitions to subspaces let el := (0, . . .",3. Preliminaries and notation,[0],[0]
", 1, . . .",3. Preliminaries and notation,[0],[0]
", 0) be the lth-component of the standard orthonormal basis of RN , and define for a subset A of J1, NK the vector 1A :=∑ l∈A el.",3. Preliminaries and notation,[0],[0]
"For [k], the k
th cluster of πm, with a slight abuse of notation we define 1[k] := ∑|[k]| i=1 eki , and observe that Fm = span{1[k] : k ∈ πm}, which is consistent with the definition of the clustering dimension d′m := |πm| − 1 = dim(Fm)− 1.
We define 〈f∗〉 := span{f∗}, S1 ⊕ S2 as the direct sum of the two vector space S1 and S2, and S1 S2 as their direct difference.",3. Preliminaries and notation,[0],[0]
"PS denotes the (orthogonal) projection operator onto the subspace S. We also define the partitions inclusion as m1 ⊂ m2 if Fm1 ⊂ Fm2 , or equivalently if
πm2 is finer than πm1 .",3. Preliminaries and notation,[0],[0]
Example 1.,3. Preliminaries and notation,[0],[0]
"Consider the signal f∗ of Figure 1, whose partition is
π = {[1]; [2];",3. Preliminaries and notation,[0],[0]
"[3]; [4]; [5]},
where
[1] = J615, 678K ∪ J821, 926K ∪ J1019, 1211K ∪ J1753, 2000K [2] = J1, 100K ∪ J679, 820K ∪ J1212, 1280K [3] = J101, 214K ∪ J505, 614K ∪ J926, 1018K ∪ J1281, 1600K [4] = J215, 504K [5] = J1601, 1752K.
Hence, d′π = 4 and d ′′",3. Preliminaries and notation,[0],[0]
"π = 12 for this signal.
",3. Preliminaries and notation,[0],[0]
"We also denote by CNk the binomial coefficient that gives the number of ways, disregarding order, that k objects can be chosen from among N objects.",3. Preliminaries and notation,[0],[0]
"This is given by
CNk := N !
k!(N − k)! .",3. Preliminaries and notation,[0],[0]
"(2)
The Stirling numbers of the second kind, S(N, k), correspond to the number of ways to partition a set of N objects into k non-empty subsets, or, similarly, to the number of different equivalence relations with precisely k equivalence classes that can be defined on an set of N elements.
",3. Preliminaries and notation,[0],[0]
"We are precisely interested in the case where the element set is J1, NK and the distance between every two elements in each equivalence class is at least 2; we denote the number of such equivalent classes by S2(N, k).",3. Preliminaries and notation,[0],[0]
"S(N, k) and S2(N, k) satisfy the following recurrence relations:
S(N, k) = S(N − 1, k − 1) + kS(N",3. Preliminaries and notation,[0],[0]
"− 1, k), N > k, S2(N, k) = S(N",3. Preliminaries and notation,[0],[0]
"− 1, k − 1), N, k > 2. (3)
",3. Preliminaries and notation,[0],[0]
"For the proofs of these results, we refer the reader to (Graham et al., 1988) and (Mohr & Porter, 2009).",3. Preliminaries and notation,[0],[0]
"To solve the change point and clustering problem, a natural approach is to consider the minimization of a criterion of the form,
Crit(m) = ‖y",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
− f̂m‖22 + σ2K pen(m).,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"(4)
Uniqueness, continuity and stability properties of similar criterion have been studied in (O. et al.), we restrict to a penalty term pen(d′m, d ′′ m) := pen(m) depending only on d′ and d′′ and a multiplicative tuning parameter K. Indeed, as we shall see the penalty can be chosen such that the minimizer f̂m̂ of (4) behaves like an approximation to a maximum a-posteriori estimator (MAP), and also, the average expected risk 1NE[‖f̂m̂−",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"f
∗‖22]→ 0",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"for a large class of signals f∗, namely, those corresponding to models with d′ 6 d′′ = o(N/ lnN), i.e., f∗ is a consistent estimator for those signals.",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"The specific form of pen(m) will be derived in the next section, based on an oracle inequality that will guarantee consistency and adaptivity of our estimator.
",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"Although the estimator f̂m̂ enjoys good statistical properties, from a computational stand it would involve the exploration ofM.",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"The setM is identified with the collection of all the partitions of J1, NK, whose number asymptotically behaves like O(NeN/ lnN), rendering the minimization of the criterion (4) computationally challenging.",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"A way to bypass this issue for the change point only detection problem is via dynamic programming (Harchaoui & Cappé, 2007); this approach works in this simplified setup since there is a natural ordering for exploring the subproblems, which does not hold here.",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"To overcome this, we will relax the criterion in such a way to create a subproblem ordering and thus derive a computationally feasible approximation.",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"The proposed new method is outlined in Algorithm 1.
",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"Let ȳ[k] := ( ∑ i∈[k] Yi)/|[k]|, the average of the elements of Y in the [k]-th part.",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"Notice that, given πm := {[1]; [2]; . . .",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"; [d′m − 1]}, we have
PFm Y = d′m−1∑ k=1 〈Y,1[k]〉 ‖1[k]‖2 1[k] = d′m−1∑ k=1 ȳ[k]1[k].
",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"The minimization of criterion (4) can then be equivalently written as
min m∈M Crit(m)
=",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"min m∈M
{‖y",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"− PFm Y ‖22 + σ2K pen(d′m, d′′m)}
= min 06d′
6d′′6D  min|m|=d′ |m|0=d′′ ‖y",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"− PFm Y ‖22 + σ2K pen(d′, d′′)  ,
Algorithm 1 Two-Pass Dynamic Programming Algorithm input data points (yi)Ni=1, maximum number of changes D and
penalty strength K. 1:
ȳ[k,l] :=
∑l i=k Yi k",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"− l + 1
R[k,l] := l∑",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"i=k (yi − ȳ[k,l])2, 1 6 k 6 l 6 N.
2: for d = 1 to D do 3: use the dynamic programming recurrence in (9) and a
backtracking step to compute
Cd(N) := min |m̄|=d
‖Y − PFm̄ Y ‖ 2, (5)
m̃d ∈ arg min |m̄|=d ‖Y − PFm̄ Y ‖ 2.
4: end for 5: for d = 1 to D do 6: m̃d =: {0 6 i1 < i2 < · · ·",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"< id < N}
(αk) d k=0 := (ik+1 − ik)d0, (i0 = 0, id+1 = N).
7: sort (ȳ[1,i1], ȳ[i1+1,i2], . . .",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
", ȳ[id+1,N ]).",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
8: ( ȳ(k) ),4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"d k=0 := ordered sequence of (ȳ[ik+1,ik+1])",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"d k=0
(α(k))",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
d k=0 := corresponding permuted (αk) d k=0,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"according to permutation φd.
9: ȳ(k,l) := ∑l i=k α(i)ȳ(i)∑l−1 i=k α(i) and R̄[k,l] := ∑l i=k α(i)(ȳ(i) −
ȳ(k,l)) 2, 1 6 k 6 l 6 d.
10: for δ = 1 to d do 11: use the dynamic programming recurrence in (10) and a
backtracking step to compute
G(d,δ) := min m∈Mȳm̃,δ
‖PFm̄ Y − PFm PFm̄ Y ‖ 2, (6)
˜̃m(d,δ) ∈ arg min m∈Mȳm̃δ ‖PFm̄",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"Y − PFm PFm̄ Y ‖ 2.
12: end for 13: end for 14: B(d,δ) := Cd+G(d,δ) +σ2K pen((d, δ)), 1 6 δ 6 d 6 D. 15: (d̂, δ̂) := arg min
16δ6d6D B(d,δ).
16: reconstruct m(d̂,δ̂) from m̃d̂ and ˜̃m(d̂,δ̂).",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"output value of criterion Crit(m(d̂,δ̂))",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"= B(d̂,δ̂) and selected
model for change points and clusters m(d̂,δ̂).
",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
where D is a reasonable upper bound on the number of change points.,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"As we shall see later, from a statistical point of view there is no need to explore all possible values of d′ and d′′, since the statistical guarantees only hold in a regime where d′ 6 d′′ = o(N/ lnN).
",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"We define πm̄ to be the partition having as elements all the segments of πm and instead of computing the minimum exactly we will take a greedy step by defining
m̃ := arg min |m̄|=d′′
‖Y − PFm̄ Y ‖2
and defining Mm̃,d′ := {m ∈ M : m ⊂ m̃, |m| = d′}, which can be identified with the collection of all partitions of J1, d′′K into d′ sets.",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"We restrict further this collection to partitions π satisfying what we call the clustering property, which states that if I1, I2 and I are segments in some (possibly different) parts of π, then
{I1, I2 ∈",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
[k] ȳI1 6,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
ȳI 6 ȳI2 ⇒,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
I ∈,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"[k]. (7)
",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"This sub-collection will be denoted asMȳm̃,d′ .",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"Simply put, this property says that the partitions considered are those that respect the ordering of (ȳ[ik+1,ik+1])",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"d′′
k=0, since if two segments I1, I2 belong to [k], and the segment I satisfies ȳI1 6",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"ȳI 6 ȳI2 , then it should also be in cluster [k].
",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"This leads to the following upper bound, whose detailed derivation is given in appendix B:
min m∈M Crit(m) 6 min 06d′′6D { min |m|=d′′ ‖Y − PFm",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
Y,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"‖2
+ min 06d′6d′′
m∈Mȳm̃,d′′
‖PFm̃ Y",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
− PFm PFm̃,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"Y ‖2
+ σ2K pen(d′, d′′) } .
",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"Therefore, we can define the following relaxation for the minimization of the criterion in (4):
Critr(d ′′) := min |m|=d′′ ‖Y − PFm",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
Y,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"‖2
+ min 06d′6d′′
m∈Mȳm̃,d′′
{ ‖PFm̃ Y",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
− PFm PFm̃,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"Y ‖2
+ σ2K pen(d′m, d ′′ m)
} .",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"(8)
and our algorithm computes min 06d′′6D
Critr(d ′′) and returns
m(d̂,δ̂).",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"From this last definition we observe that
min m∈M Crit(m) 6 Crit(m(d̂,δ̂))",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"= min06d′′6D Critr(d
′′).
",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"Thus, obtainingm(d̂,δ̂) ensures making progress toward the minimization of Crit(m).",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"The Two-Pass Dynamic Programming Algorithm 1 is aimed at doing this by computing the value of the minimum in (8) and returning a solution m̂ = m(d̂,δ̂) in the following way:
Details of Main Steps in Algorithm 1
• Step 3: It computes Cd(n) defined in (9) for all d and n to obtain Cd(N) for all d ∈ J1, NK.",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"It does so by using a dynamic programming algorithm that computes recursively for all 2 6 d 6 D and d 6 n 6 N the following recurrence, similar to the one in Hawkins (1976):
C1(n)",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
":= R[1,n] (9)
Cd(n) := min i∈Jd,nK
{Cd−1(i− 1) +R[i,n]}, d > 2.
•",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"Step 7: For all values of d, it sorts the obtained segments according to their levels to yield ( ȳ(k) )",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"d 0 , and it keeps track
of the segments’ sizes as (αk)dk=0 = (ik+1 − ik)d0 .
",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
• Step 11: It runs a modified dynamic programming recurrence on ( ȳ(k) ),4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"d 0
that uses weights according to the sizes (α(k)) d 0 .",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"It does so using the following recurrence for all 1 6 δ 6 t 6 d:
G(t,1)",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
":= R̄[1,t], (10)
G(t,δ) := min i∈Jδ,tK
{G(i−1,δ−1) + R̄[i,t]}, δ > 2.
• Step 15: It computes the minimum in (8) and finds for which model it is attained by solving the minimization problem:
(d̂, δ̂) := arg min 16δ6d6D B(d,δ).
",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"• Step 16: It finally reconstructs m(d̂,δ̂) from m̃d̂ and ˜̃m(d̂,δ̂) using the permutation φ(d̂).
",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"This algorithm can be thought of as an efficient way to compute the relaxation in (8), based on solving the change point detection problem in (5) using the dynamic programming recurrence of (9), followed by a solving a clustering problem in (6) using the dynamic programming recurrence of (10).
",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
The next theorem shows that Algorithm 1 correctly solves the minimization problem in (8) and explicits its time and space complexity.,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
Theorem 4.1.,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"Let (yi)Ni=1 ⊂ R, D ∈ N and K > 0.",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"Then,
• for all 1 6 d 6 D,
m̃d ∈ arg min |m̄|=d ‖Y − PFm̄ Y ‖2,
• for all 1 6 δ 6 d 6 D,
˜̃m(d,δ) ∈ arg",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
min,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
m∈Mȳm̃δ ‖PFm̄,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
Y − PFm PFm̄,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"Y ‖2.
",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"Furthermore, Algorithm 1 correctly solves the minimization problem in (8), with time and space complexity O(N3 +D4) and O(N2 +D3), respectively.
",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
Proof.,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"See Appendix B.
The time and space complexity can be improved to O(N2D",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
+,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"D4) and O(DN +D3), respectively.",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
We refer the reader to the discussion after the proof in Appendix B for the derivation of this result.,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
In this way we obtain a computationally feasible algorithm that finds the minimum in (8) and returns an approximation to the criterion in (4).,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"In the next section, we will motivate the use of Algorithm 1 from a statistical point of view by showing that the minimization of criterion (4) can be viewed as an approximate maximum a-posteriori estimator.",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"In this part, we provide a derivation of the optimization criterion in (4).",5. Model selection criterion for change point detection and clustering,[0],[0]
"We start by proposing a Bayesian model selection scheme, which is later inverted to arrive at an integral form of the maximum a-posteriori probability (MAP) estimator.",5. Model selection criterion for change point detection and clustering,[0],[0]
"Then we use a Laplace approximation to derive turn the MAP into an optimization problem of the desired form.
",5. Model selection criterion for change point detection and clustering,[0],[0]
Here we show that the proposed selection criterion in (4) follows naturally from a Bayesian reasoning.,5. Model selection criterion for change point detection and clustering,[0],[0]
"For this, we model the data as being the outcome of the following sampling model.",5. Model selection criterion for change point detection and clustering,[0],[0]
The observation Y is generated from a multivariate Gaussian of mean F and variance σ2IN as described by (1).,5. Model selection criterion for change point detection and clustering,[0],[0]
"For the random variable F , given that it belongs to a subspace Fm, we choose an absolutely continuous measure Ld′m with respect to λd′m , the Lebesgue measure on Rd′m+1, such that dLd′m = lf/mdλd ′",5. Model selection criterion for change point detection and clustering,[0],[0]
"m+1 =
d′m+1∏ k=1 (lfk/mdλ) with lf1/m = · · · = lfd′m+1/m. Later we will see that the choice lf/m will not matter in comparison to the order of approximation, nevertheless we would like it to be a bounded continuous prior satisfying some additional conditions given in Lemma 2, even though we might be chosen as an improper prior.",5. Model selection criterion for change point detection and clustering,[0],[0]
"On the family of models M we impose a categorical distribution measure PM as prior, with a weight pm for model m. Thus, we obtain the following sampling model for the data1:
Y/F ∼ N (F, σ2IN ) F/m ∼ Ld ′ m (11)
m ∼ PM = Categorical((pm)m∈M).
",5. Model selection criterion for change point detection and clustering,[0],[0]
"Since Y , F and m are now random variables, it makes sense to compute µm/Y , the posterior distribution of m
1Here and in the sequel, the dependence of pm and PM on the number of samples N is omitted, for simplicity of notation.
",5. Model selection criterion for change point detection and clustering,[0],[0]
"given Y , and maximize it, to arrive at a MAP estimate of m given bellow.
pm/",5. Model selection criterion for change point detection and clustering,[0],[0]
"Y =
pm ∫ f∈Fm φN",5. Model selection criterion for change point detection and clustering,[0],[0]
"( Y − f σ ) lf/m(f)df
∑ m′∈M pm′ ∫ f ′∈Fm φN",5. Model selection criterion for change point detection and clustering,[0],[0]
"( Y − f ′ σ ) lf/m′(f ′)df ′ .
(12)
",5. Model selection criterion for change point detection and clustering,[0],[0]
"For the complete derivation of the formula in 12 we refer you to appendix B.
Starting from the a-posteriori distribution (12) we can derive an approximation for the MAP as follows:
pm/Y ∝ pm ∫",5. Model selection criterion for change point detection and clustering,[0],[0]
f∈Fm φN,5. Model selection criterion for change point detection and clustering,[0],[0]
"( Y − f σ ) lf/m(f)df
= pm",5. Model selection criterion for change point detection and clustering,[0],[0]
"d′m+1∏ k=1
1
(2πσ2) |[k]| 2
(13)
· ∫ R exp ( − ‖y[k]",5. Model selection criterion for change point detection and clustering,[0],[0]
− fk1[k]‖22 2σ2 ),5. Model selection criterion for change point detection and clustering,[0],[0]
"lfk/m(fk)dfk.
",5. Model selection criterion for change point detection and clustering,[0],[0]
In the last step of (13) we define y[k] as the vector obtained from the entries of y corresponding to cluster [k].,5. Model selection criterion for change point detection and clustering,[0],[0]
To obtain an approximation of the MAP estimate as a solution of a criterion of the form (4) we need the result of lemma 2 stated and proved in Appendix C using a Laplace approximation type of argument.,5. Model selection criterion for change point detection and clustering,[0],[0]
"We then obtain the following upper bound for the MAP for all K > 1 :
CritMAP(m) 6 ‖y",5. Model selection criterion for change point detection and clustering,[0],[0]
"− PFm y‖22
2σ2
+K ( ln 1
pm +
1 2 (d′m + 1) ln N
d′m
) +O(d′m).",5. Model selection criterion for change point detection and clustering,[0],[0]
"(14)
The complete derivation of (14) can be found in Appendix C.",5. Model selection criterion for change point detection and clustering,[0],[0]
"Now we define our approximate MAP criterion as:
Crit(m) = ‖y",5. Model selection criterion for change point detection and clustering,[0],[0]
"− PFm y‖22 + σ2K pen(m),
pen(m) =",5. Model selection criterion for change point detection and clustering,[0],[0]
"( 2 ln 1
pm + (d′m + 1)",5. Model selection criterion for change point detection and clustering,[0],[0]
"ln
N
d′m
) .",5. Model selection criterion for change point detection and clustering,[0],[0]
"(15)
In the next section, we finish the specification of the penalty term by providing the probabilities pm over the space of models.",5. Model selection criterion for change point detection and clustering,[0],[0]
"To do so we will exhibit an oracle inequality satisfied by the estimator that minimizes (4), and choose a probability mass function (pm) that gives a reasonable upper bound on the expected quadratic risk defined below.",5. Model selection criterion for change point detection and clustering,[0],[0]
The standard way of assessing the performance of a statistical algorithm is by comparing its performance to a reasonable oracle.,6. Oracle inequality and upper bound for the risk,[0],[0]
"For this we use as a measure of performance
of an estimator f̂ the expected quadratic risk:
Rn(f̂) =",6. Oracle inequality and upper bound for the risk,[0],[0]
E[‖f̂,6. Oracle inequality and upper bound for the risk,[0],[0]
"− f∗‖22].
In the case of the change point detection and clustering problem, the comparison should be non-asymptotic, reflecting our lack of knowledge about both the clustering dimension and the change point dimension.",6. Oracle inequality and upper bound for the risk,[0],[0]
"For this we state below a non-asymptotic oracle inequality for Crit(m) using an oracle with remainder of the form:
inf m∈M {Rn(PFm y) + om(1)}.
",6. Oracle inequality and upper bound for the risk,[0],[0]
"This type of oracle has access to f∗ and chooses the m that minimizes the risk criterion up to a remainder term.
",6. Oracle inequality and upper bound for the risk,[0],[0]
To derive this we finish the specification of Crit(m) by providing an appropriate prior pm.,6. Oracle inequality and upper bound for the risk,[0],[0]
The intuition behind our choice is the following.,6. Oracle inequality and upper bound for the risk,[0],[0]
Defining r̂m = ‖y,6. Oracle inequality and upper bound for the risk,[0],[0]
− f̂m‖22 and pen(m) = 2σ2,6. Oracle inequality and upper bound for the risk,[0],[0]
ln 1pm + σ 2(d′m + 1),6. Oracle inequality and upper bound for the risk,[0],[0]
"ln N d′m
we see that the criterion (15) is of the form:
Crit(m)",6. Oracle inequality and upper bound for the risk,[0],[0]
"= r̂m + pen(m).
",6. Oracle inequality and upper bound for the risk,[0],[0]
The number of models in the family M having the same values of d′m and d ′′ m grows exponentially with those dimensions.,6. Oracle inequality and upper bound for the risk,[0],[0]
"Thus for fix d′m and d ′′ m we might find a model with low r̂m just because of randomness since some of them will deviate largely from their means, which would correspond to an over-fitting case, this was the problem of case with the traditional AIC type of estimators.",6. Oracle inequality and upper bound for the risk,[0],[0]
"Therefore, we need to penalize models of high dimensions more by taking into account the number of models with same dimensions.",6. Oracle inequality and upper bound for the risk,[0],[0]
On the other hand we want this penalty to be as small as possible this way we give more importance to the fitting term r̂m.,6. Oracle inequality and upper bound for the risk,[0],[0]
"In particular we would prefer the term 2σ2 ln 1pm to stay close to σ
2(d′+1)",6. Oracle inequality and upper bound for the risk,[0],[0]
ln Nd′ at least for values of d′m close to d ′′ m.,6. Oracle inequality and upper bound for the risk,[0],[0]
"Our choice for pm, useful inequalities and a complete discussion of the role of pm as a prior and tuning parameter for the risk can be found in Appendix D. From Lemmas 3 and 4, the following oracle inequality can be derived for f̂m̂:
Theorem 6.1 (Oracle inequality for f̂m̂).",6. Oracle inequality and upper bound for the risk,[0],[0]
"With M restricted to models such that ed′m 6 N and for the choice of K = 3a, pm as in 3, pen(m) as in 15 and m̂ ∈ M corresponding to
m̂ ∈ arg min",6. Oracle inequality and upper bound for the risk,[0],[0]
m∈M ‖y,6. Oracle inequality and upper bound for the risk,[0],[0]
"− f̂m‖22 + σ2K pen(m), (16)
We obtain for all a > 1,
Ef∗",6. Oracle inequality and upper bound for the risk,[0],[0]
"[‖PFm̂ Y − f∗‖2] 6
arg min m∈M
{ a
a− 1 Ef∗",6. Oracle inequality and upper bound for the risk,[0],[0]
"[‖PFm Y − f∗‖2]
+ a2σ2
a− 1
( 7 + 3(d′m + 1) ln N
d′m + 6 ln
1
pm
)} .",6. Oracle inequality and upper bound for the risk,[0],[0]
"(17)
Proof.",6. Oracle inequality and upper bound for the risk,[0],[0]
"See Appendix D.
By investigating the oracle inequality, one notices that for an optimal choice of a one has to make a trade-off between the performance of the oracle part and the bias part of the inequality.",6. Oracle inequality and upper bound for the risk,[0],[0]
In general this trade-off is not possible to optimize since the value of the oracle part is not available to us and depends on the variance of the noise.,6. Oracle inequality and upper bound for the risk,[0],[0]
"In practice, one can use the SLOPE heuristic introduced in Lebarbier (2002) and described in Baudry et al. (2012) and in (Arlot & Massart, 2009).",6. Oracle inequality and upper bound for the risk,[0],[0]
"In our case, the value of the tuning parameter can be chosen independently of the variance of the noise and we can use the value of a for which we know that our estimator f̂m̂ will perform well.
",6. Oracle inequality and upper bound for the risk,[0],[0]
Corollary 6.1.,6. Oracle inequality and upper bound for the risk,[0],[0]
"For the set of models described in 6.1 with f∗ ∈ Fm∗ the following properties hold:
• Adaptation and Risk Upper bound: The following adaptive upper bound in terms of d′m∗ and d ′′ m∗ holds
for a = 2:
Ef∗",6. Oracle inequality and upper bound for the risk,[0],[0]
"[‖PFm̂ Y − f ∗‖2] 6 4σ2 ( 7 + 3(d′m∗ + 1) ln N
d′m∗
+ 6 ( d′m∗ ln[d ′′ m∗e 13 6 ] + d′′m∗ ln[d ′ m∗e 2] + d′′m∗ ln N
d′′m∗
)) .
",6. Oracle inequality and upper bound for the risk,[0],[0]
• Consistency:,6. Oracle inequality and upper bound for the risk,[0],[0]
"If d′′m∗ = o(N/ lnN), then limN→∞N −1Ef∗",6. Oracle inequality and upper bound for the risk,[0],[0]
"[‖f̂m̂ − f∗‖2] = 0.
",6. Oracle inequality and upper bound for the risk,[0],[0]
Proof.,6. Oracle inequality and upper bound for the risk,[0],[0]
"See Appendix D.
We notice that the consistency condition d′′m∗ = o(N/ lnN) is within the restriction on the models in theorem 6.1, hence there is no loss of generality of having only models with ed′m 6 N in M since for other models we cannot guarantee convergent mean square risk anyway.",6. Oracle inequality and upper bound for the risk,[0],[0]
"In the special case d′m∗ = d ′′ m∗ , i.e when the change point and clustering problem reduces to a change point only problem, Kernel methods have comparable accuracy (Celisse et al., 2017).",6. Oracle inequality and upper bound for the risk,[0],[0]
"The interesting case is when the numbers are different, we gain a logarithmic factor in accuracy with almost the same computational cost.",6. Oracle inequality and upper bound for the risk,[0],[0]
"In the next section, we validate these theoretical guarantees by a series of tests on simulated data to get a sense of how tight the oracle inequality is, which signals are difficult to estimate and how the algorithm behaves in practice.",6. Oracle inequality and upper bound for the risk,[0],[0]
Consider first an experiment based data generated randomly according to the setup of (1) with the same change points of Example 1.,7. Experimental results,[0],[0]
"This is considered to be an easy case since d′m∗ = 4 < d ′′ m∗ = 12 N = 2000, which is within the range of signals for which the consistency result of Corollary 6.1 holds.
",7. Experimental results,[0],[0]
"The experiments in Figure 2 show that the algorithm is quite robust to the level of noise as measured by the signalto-noise ratio S/N = magnitude of smallest jump in f ∗
σ2 .",7. Experimental results,[0],[0]
"We observe that the difference between the ground truth f∗ and f̂m̂ is quite small even for small S/N levels such as S/N = 0.5 and the change point locations do not vary appreciably; in fact, for this experiment, S/N = 0.3 seems to be the limiting case for which the algorithm performs well, and for lower values the risk upper-bound in Corollary 6.1 becomes loose when σ increases.",7. Experimental results,[0],[0]
"Also, we note that an S/N of 0.5 is quite low for this kind of problems.",7. Experimental results,[0],[0]
"In particular, algorithms relying on the L1-penalty such as Fussed LASSO do not achieve this kind of performance on the simpler task of change point only detection, while on the other hand, they are more computational efficient (Xin et al., 2014).
",7. Experimental results,[0],[0]
"Figure 3 illustrates a difficult case, where we reduced the number of observation by segment by scaling down the signal f∗ to a support of size N = 500.",7. Experimental results,[0],[0]
"Now we are outside of the useful regime of Corollary 6.1 and we notice that the second segment J15, 53K is wider than what it should since the first change point at 25 was detected at 14; also the segment J206, 237K belongs to cluster [4] while it is actually in cluster",7. Experimental results,[0],[0]
[3] in the original signal f∗.,7. Experimental results,[0],[0]
"Nevertheless we can observe an interesting property for segment J324, 346K, namely, that the end point 346 does not correspond to any real change point, yet this segment belongs to the optimal solution of the 1st dynamic programming pass.",7. Experimental results,[0],[0]
"On the other hand the 2nd dynamic programming pass puts it in the same cluster [3] as J347, 399K, turning them into one single segment of cluster [3].",7. Experimental results,[0],[0]
"This behavior actually is the norm for the algorithm, where false changes are often detected in difficult signals in the 1st
dynamic programming pass but are removed after the 2nd pass.",7. Experimental results,[0],[0]
"These kinds of false discoveries are actually one of the weaknesses of many change point only detection algorithms like Fussed LASSO, and they have been studied in (Levy-leduc & Harchaoui, 2008), (Rinaldo, 2009) and (Rojas & Wahlberg, 2014).",7. Experimental results,[0],[0]
"In the last experiment,
we run Algorithm 1 300 times with the parameter values d′m∗ = 4 < d ′′ m∗ = 12 N = 2000 and signal-to-noise ratio S/N = 1; Figure 4 summarizes the results.",7. Experimental results,[0],[0]
"In the top histogram we notice that the algorithm successfully detects the change points most of the time; in fact, the achieved accuracy was number of change points correctly detectednumber of change points detected ≈ 0.8528.",7. Experimental results,[0],[0]
"The
middle histogram shows the placement of estimated clusters and the true values of the clusters; we observe that the true values lie in a small neighborhood of the estimated values for every cluster.",7. Experimental results,[0],[0]
"In the bottom histogram we observe that the theoretical upper bound on the average mean square error –in this case 12.1575– found in Corollary 6.1 is very conservative and most of the 300 estimates –given by ‖f̂m̂−f
∗‖2 N – are significantly smaller.",7. Experimental results,[0],[0]
"In this work, we considered a novel problem related to change point detection where we have to address the simultaneous task of segmenting and clustering the observed signal.",8. Conclusions,[0],[0]
Our approach has been to view this problem as a non-parametric model selection problem on the set of all possible partitions.,8. Conclusions,[0],[0]
"We derived for this the computationally tractable Algorithm 1, that computes a relaxation of the penalized minimization of criterion (4), and we justified it from a statistical standpoint by showing that this minimization can be viewed as an approximate MAP.",8. Conclusions,[0],[0]
This approximate MAP estimate enjoys the properties of being adaptive and consistent in the sense of Corollary 6.1.,8. Conclusions,[0],[0]
"We finally justified the use of Algorithm 1 by simulation data that shows some useful properties of the resulting estimate and validates the theoretical guarantees.
",8. Conclusions,[0],[0]
"One extension of this work concerns developing a more complete analysis of Algorithm 1, to obtain consistency results on the number and locations of the change points and clusters.",8. Conclusions,[0],[0]
"Another possible extension relates to the use of Algorithm 1 in the non-scalar case; this was already explored for change point only detection in (Arlot et al., 2016) through the use of characteristic kernels (Sriperumbudur et al., 2011).",8. Conclusions,[0],[0]
"We believe that the same approach can be adopted here except that we cannot perform the sorting step; this can be overcome using a Kernel clustering algorithm (Filipponea et al., 2008) or a spectral version of it (Schölkopf et al., 1998) for the second stage.",8. Conclusions,[0],[0]
"Finally, the remark after Figure 3 hints to the possibility of using a combined algorithm starting with the sparse solution of Fussed LASSO and running the 2nd dynamic programming pass of our algorithm as a way to boost the performance of Fussed LASSO to get rid of false discoveries.",8. Conclusions,[0],[0]
"This would be still computationally attractive according to the comment after Theorem 4.1, since the solution of Fussed LASSO has a small number of changes.",8. Conclusions,[0],[0]
We address a generalization of change point detection with the purpose of detecting the change locations and the levels of clusters of a piecewise constant signal.,abstractText,[0],[0]
Our approach is to model it as a nonparametric penalized least square model selection on a family of models indexed over the collection of partitions of the design points and propose a computationally efficient algorithm to approximately solve it.,abstractText,[0],[0]
"Statistically, minimizing such a penalized criterion yields an approximation to the maximum a-posteriori probability (MAP) estimator.",abstractText,[0],[0]
The criterion is then analyzed and an oracle inequality is derived using a Gaussian concentration inequality.,abstractText,[0],[0]
"The oracle inequality is used to derive on one hand conditions for consistency and on the other hand an adaptive upper bound on the expected square risk of the estimator, which statistically motivates our approximation.",abstractText,[0],[0]
"Finally, we apply our algorithm to simulated data to experimentally validate the statistical guarantees and illustrate its behavior.",abstractText,[0],[0]
Bayesian Model Selection for Change Point Detection and Clustering,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1029–1039 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1095",text,[0],[0]
Dictionaries and gazetteers are useful in many natural language processing tasks.,1 Introduction,[0],[0]
These lexical resources may be derived from freely available sources (such as Wikidata and Wiktionary) or constructed for a particular domain.,1 Introduction,[0],[0]
"Lexical resources are typically used to complement existing annotations for a given task (Ando and Zhang, 2005; Collobert et al., 2011).",1 Introduction,[0],[0]
"In this paper, we focus instead on low-resource settings where task annotations are unavailable or scarce.",1 Introduction,[0],[0]
"Specifically, we use lexical resources to guide part-of-speech induction (§4) and to bootstrap named-entity recognizers in low-resource languages (§5).
",1 Introduction,[0],[0]
"Given their success, it is perhaps surprising that incorporating gazetteers or dictionaries into dis-
criminative models (e.g. conditional random fields) may sometimes hurt performance.",1 Introduction,[0],[0]
"This phenomena is called weight under-training, in which lexical features—which detect whether a name is listed in the dictionary or gazetteer—are given excessive weight at the expense of other useful features such as spelling features that would generalize to unlisted names (Smith et al., 2005; Sutton et al., 2006; Smith and Osborne, 2006).",1 Introduction,[0],[0]
"Furthermore, discriminative training with lexical features requires sufficient annotated training data, which poses challenges for the unsupervised and low-resource settings we consider here.
",1 Introduction,[0],[0]
Our observation is that Bayesian modeling provides a principled solution.,1 Introduction,[0],[0]
The lexicon is itself a dataset that was generated by some process.,1 Introduction,[0],[0]
"Practically, this means that lexicon entries (words or phrases) may be treated as additional observations.",1 Introduction,[0],[0]
"As a result, these entries provide information about how names are spelled.",1 Introduction,[0],[0]
"The presence of the lexicon therefore now improves training of the spelling features, rather than competing with the spelling features to help explain the labeled corpus.
",1 Introduction,[0],[0]
A downside is that generative models are typically less feature-rich than their globally normalized discriminative counterparts (e.g. conditional random fields).,1 Introduction,[0],[0]
In designing our approach—the hierarchical sequence memoizer (HSM)—we aim to be reasonably expressive while retaining practically useful inference algorithms.,1 Introduction,[0],[0]
We propose a Bayesian nonparametric model to serve as a generative distribution responsible for both lexicon and corpus data.,1 Introduction,[0],[0]
"The proposed model memoizes previously used lexical entries (words or phrases) but backs off to a character-level distribution when generating novel types (Teh, 2006; Mochihashi et al., 2009).",1 Introduction,[0],[0]
We propose an efficient inference algorithm for the proposed model using particle Gibbs sampling (§3).,1 Introduction,[0],[0]
"Our code is available at https://github.com/noa/bayesner.
1029",1 Introduction,[0],[0]
Our goal is to fit a model that can automatically annotate text.,2 Model,[0],[0]
We observe a supervised or unsupervised training corpus.,2 Model,[0],[0]
"For each label y in the annotation scheme, we also observe a lexicon of strings of type y.",2 Model,[0],[0]
"For example, in our tagging task (§4), a dictionary provides us with a list of words for each part-of-speech tag y.",2 Model,[0],[0]
(These lists need not be disjoint.),2 Model,[0],[0]
"For named-entity recognition (NER, §5), we use a list of words or phrases for each named-entity type y (PER, LOC, ORG, etc.).1",2 Model,[0],[0]
"We may treat the lexicon for type y, of size my, as having been produced by a set of my IID draws from an unknown distribution Py over the words or named entities of type y.",2.1 Modeling the lexicon,[0],[0]
It therefore provides some evidence about Py.,2.1 Modeling the lexicon,[0],[0]
We will later assume that Py is also used when generating mentions of these words or entities in text.,2.1 Modeling the lexicon,[0],[0]
"Thanks to this sharing of Py, if x = Washington is listed in the gazetteer of locations (y = LOC), we can draw the same conclusions as if we had seen a LOC-labeled instance of Washington in a supervised corpus.
",2.1 Modeling the lexicon,[0],[0]
"Generalizing this a bit, we may suppose that one observation of string x in the lexicon is equivalent to c labeled tokens of x in a corpus, where the constant c > 0 is known as a pseudocount.",2.1 Modeling the lexicon,[0],[0]
"In other words, observing a lexicon of my distinct types {x1, . .",2.1 Modeling the lexicon,[0],[0]
.,2.1 Modeling the lexicon,[0],[0]
", xmy} is equivalent to observing a labeled pseudocorpus of cmy tokens.",2.1 Modeling the lexicon,[0],[0]
"Notice that given such an observation, the prior probability of any candidate distribution Py is reweighted by the likelihood (cmy)!(c!)my · (Py(x1)Py(x2) · · ·Py(xmy))c.",2.1 Modeling the lexicon,[0],[0]
"Therefore, this choice of Py can have relatively high posterior probability only to the extent that it assigns high probability to all of the lexicon types.",2.1 Modeling the lexicon,[0],[0]
"We employ the above model because it has reasonable qualitative behavior and because computationally, it allows us to condition on observed lexicons as easily as we condition on observed corpora.",2.2 Discussion,[0],[0]
"However, we caution that as a generative model of the lexicon, it is deficient, in the sense that it
1Dictionaries and knowledge bases provide more information than we use in this paper.",2.2 Discussion,[0],[0]
"For instance, Wikidata also provides a wealth of attributes and other metadata for each entity s. In principle, this additional information could also be helpful in estimating Py(s); we leave this intriguing possibility for future work.
",2.2 Discussion,[0],[0]
allocates probability mass to events that cannot actually correspond to any lexicon.,2.2 Discussion,[0],[0]
"After all, drawing cmy IID tokens from Py is highly unlikely to result in exactly c tokens of each of my different types, and yet a run of our system will always assume that precisely this happened to produce each observed lexicon!",2.2 Discussion,[0],[0]
"To avoid the deficiency, one could assume that the lexicon was generated by rejection sampling: that is, the gazetteer author repeatedly drew samples of size cmy from Py until one was obtained that had this property, and then returned the set of distinct types in that sample as the lexicon for y.",2.2 Discussion,[0],[0]
But this is hardly a realistic description of how gazetteers are actually constructed.,2.2 Discussion,[0],[0]
"Rather, one imagines that the gazetteer author simply harvested a lexicon of frequent types from Py or from a corpus of tokens generated from Py.",2.2 Discussion,[0],[0]
"For example, a much better generative story is that the lexicon was constructed as the first my distinct types to appear ≥ c times in an unbounded sequence of IID draws from Py.",2.2 Discussion,[0],[0]
"When c = 1, this is equivalent to modeling the lexicon as my draws without replacement from Py.2 Unfortunately, draws without replacement are no longer IID or exchangeable: order matters.",2.2 Discussion,[0],[0]
"It would therefore become difficult to condition inference and learning on an observed lexicon, because we would need to explicitly sum or sample over the possibilities for the latent sequence of tokens (or stick segments).",2.2 Discussion,[0],[0]
"We therefore adopt the simpler deficient model.
",2.2 Discussion,[0],[0]
"A version of our lexicon model (with c = 1) was previously used by Dreyer and Eisner (2011, Appendix C), who observed a list of verb paradigm types rather than word or entity-name types.",2.2 Discussion,[0],[0]
"We assume a priori that Py was drawn from a Pitman-Yor process (PYP) (Pitman and Yor, 1997).",2.3 Prior distribution over Py,[0],[0]
Both the lexicon and the ordinary corpus are observations that provide information about Py.,2.3 Prior distribution over Py,[0],[0]
"The PYP is defined by three parameters: a concentration parameter α, a discount parameter d, and a base distribution Hy.",2.3 Prior distribution over Py,[0],[0]
"In our case, Hy is a distribution over X = Σ∗, the set of possible strings over a finite character alphabet",2.3 Prior distribution over Py,[0],[0]
"Σ.
For example, HLOC is used to choose new place names, so it describes what place names tend to
2If we assume that Py was drawn from a Pitman-Yor process prior (as in §2.3) using the stick-breaking method (Pitman, 1996), it is also equivalent to modeling the lexicon as the set of labels of the first my stick segments (which tend to have high probability).
look like in the language.",2.3 Prior distribution over Py,[0],[0]
"The draw PLOC ∼ PYP(d, α,HLOC) is an “adapted” version of HLOC.",2.3 Prior distribution over Py,[0],[0]
It is PLOC that determines how often each name is mentioned in text (and whether it is mentioned in the lexicon).,2.3 Prior distribution over Py,[0],[0]
"Some names such as Washington that are merely plausible under HLOC are far more frequent under PLOC, presumably because they were chosen as the names of actual, significant places.",2.3 Prior distribution over Py,[0],[0]
"These place names were randomly drawn from HLOC as part of the procedure for drawing Py.
",2.3 Prior distribution over Py,[0],[0]
"The expected value of Py is H (i.e., H is the mean of the PYP distribution), but if α and d are small, then a typical draw of Py will be rather different from H , with much of the probability mass falling on a subset of the strings.
",2.3 Prior distribution over Py,[0],[0]
"At training or test time, when deciding whether to label a corpus token of x = Washington as a place or person, we will be interested in the relative values of PLOC(x) and PPER(x).",2.3 Prior distribution over Py,[0],[0]
"In practice, we do not have to represent the unknown infinite object Py, but can integrate over its possible values.",2.3 Prior distribution over Py,[0],[0]
"When Py ∼ PYP(d, α,Hy), then a sequence of draws X1, X2, . . .",2.3 Prior distribution over Py,[0],[0]
"∼ Py is distributed according to a Chinese restaurant process, via
Py(Xi+1 = x",2.3 Prior distribution over Py,[0],[0]
"| X1, . . .",2.3 Prior distribution over Py,[0],[0]
", Xi) (1)
",2.3 Prior distribution over Py,[0],[0]
"= customers(x)− d · tables(x)
α+",2.3 Prior distribution over Py,[0],[0]
"i
+ α+ d ·∑x′ tables(x′)
α+ i Hy(x)
where customers(x) ≤ i is the number of times that x appeared among X1, . . .",2.3 Prior distribution over Py,[0],[0]
", Xi, and tables(x) ≤ customers(x) is the number of those times that x was drawn from Hy (where each Py(Xi | · · · ) defined by (1) is interpreted as a mixture distribution that sometimes uses Hy).",2.3 Prior distribution over Py,[0],[0]
"By fitting Hy on corpus and lexicon data, we learn what place names or noun strings tend to look like in the language.",2.4 Form of the base distribution Hy,[0],[0]
"By simultaneously fitting Py, we learn which ones are commonly mentioned.",2.4 Form of the base distribution Hy,[0],[0]
"Recall that under our model, tokens are drawn from Py but the underlying types are drawn fromHy, e.g.,Hy is responsible for (at least) the first token of each type.
",2.4 Form of the base distribution Hy,[0],[0]
"A simple choice for Hy is a Markov process that emits characters in Σ ∪ {$}, where $ is a distinguished stop symbol that indicates the end of the string.",2.4 Form of the base distribution Hy,[0],[0]
"Thus, the probability of producing $ controls the typical string length under Hy.
We use a more sophisticated model of strings—a sequence memoizer (SM), which is a (hierarchical) Bayesian treatment of variable-order Markov modeling (Wood et al., 2009).",2.4 Form of the base distribution Hy,[0],[0]
"The SM allows dependence on an unbounded history, and the probability of a given sequence (string) can be found efficiently much as in equation (1).
",2.4 Form of the base distribution Hy,[0],[0]
"Given a string x = a1 · · · aJ ∈ Σ∗, the SM assigns a probability to it via
Hy(a1:J) =",2.4 Form of the base distribution Hy,[0],[0]
"( J∏
j=1
Hy(aj |",2.4 Form of the base distribution Hy,[0],[0]
a1:,2.4 Form of the base distribution Hy,[0],[0]
j−1) ),2.4 Form of the base distribution Hy,[0],[0]
"Hy($ | a1:J)
=",2.4 Form of the base distribution Hy,[0],[0]
"( J∏
j=1
Hy,a1:j−1(aj) )",2.4 Form of the base distribution Hy,[0],[0]
"Hy,a1:J ($) (2)
where Hy,u(a) denotes the conditional probability of character a given the left context u ∈ Σ∗. Each Hy,u is a distribution over Σ, defined recursively as
Hy, ∼ PYP(d , α ,UΣ) (3) Hy,u ∼ PYP(d|u|, α|u|, Hy,σ(u))
where is the empty sequence, UΣ is the uniform distribution over Σ ∪ {$}, and σ(u) drops the first symbol from u.",2.4 Form of the base distribution Hy,[0],[0]
"The discount and concentration parameters (d|u|, α|u|) are associated with the lengths of the contexts |u|, and should generally be larger for longer (more specific) contexts, implying stronger backoff from those contexts.3
Our inference procedure is largely indifferent to the form of Hy, so the SM is not the only option.",2.4 Form of the base distribution Hy,[0],[0]
"It would be possible to inject more assumptions into Hy, for instance via structured priors for morphology or a grammar of name structure.",2.4 Form of the base distribution Hy,[0],[0]
"Another possibility is to use a parametric model such as a neural language model (e.g., Jozefowicz et al. (2016)), although this would require an inner-loop of gradient optimization.",2.4 Form of the base distribution Hy,[0],[0]
We now turn to modeling the corpus.,2.5 Modeling the sequence of tags y,[0],[0]
We assume that each sentence is generated via a sequence of latent labels y = y1:T ∈ Y∗.4,2.5 Modeling the sequence of tags y,[0],[0]
"The observations
3We fix these hyperparameters using the values suggested in (Wood et al., 2009; Gasthaus and Teh, 2010), which we find to be quite robust in practice.",2.5 Modeling the sequence of tags y,[0],[0]
"One could also resample their values (Blunsom and Cohn, 2010); we experimented with this but did not observe any consistent advantage to doing so in our setting.
",2.5 Modeling the sequence of tags y,[0],[0]
"4The label sequence is terminated by a distinguished endof-sequence label, again written as $.
",2.5 Modeling the sequence of tags y,[0],[0]
x1:T are then generated conditioned on the label sequence via the corresponding Py distribution (defined in §2.3).,2.5 Modeling the sequence of tags y,[0],[0]
"All observations with the same label y are drawn from the same Py, and thus this subsequence of observations is distributed according to the Chinese restaurant process (1).
",2.5 Modeling the sequence of tags y,[0],[0]
We model y using another sequence memoizer model.,2.5 Modeling the sequence of tags y,[0],[0]
"This is similar to other hierarchical Bayesian models of latent sequences (Goldwater and Griffiths, 2007; Blunsom and Cohn, 2010), but again, it does not limit the Markov order (the number of preceding labels that are conditioned on).",2.5 Modeling the sequence of tags y,[0],[0]
"Thus, the probability of a sequence of latent types is computed in the same way as the base distribution in §2.4, that is,
p(y1:T ) := ( T∏
t=1
Gy1:t−1(yt) ) Gy1",2.5 Modeling the sequence of tags y,[0],[0]
":T ($) (4)
where Gv(y) denotes the conditional probability of latent label y ∈ Y given the left context v ∈ Y∗.",2.5 Modeling the sequence of tags y,[0],[0]
"Each Gv is a distribution over Y , defined recursively as
G ∼ PYP(d , α ,UY) (5) Gv ∼ PYP(d|v|, α|v|, Gσ(v))
",2.5 Modeling the sequence of tags y,[0],[0]
The probability of transitioning to label yt depends on the assignments of all previous labels y1 . . .,2.5 Modeling the sequence of tags y,[0],[0]
"yt−1.
",2.5 Modeling the sequence of tags y,[0],[0]
"For part-of-speech induction, each label yt is the part-of-speech associated with the corresponding word xt.",2.5 Modeling the sequence of tags y,[0],[0]
"For named-entity recognition, we say that each word token is labeled with a named entity type (LOC, PER, . . . ),5 or with itself if it is not a named entity but rather a “context word.”",2.5 Modeling the sequence of tags y,[0],[0]
"For example, the word token xt",2.5 Modeling the sequence of tags y,[0],[0]
"= Washington could have been emitted from the label yt = LOC, or from yt = PER, or from yt = Washington itself (in which case p(xt | yt) = 1).",2.5 Modeling the sequence of tags y,[0],[0]
"This uses a much larger set of labels Y than in the traditional setup where all context words are emitted from the same latent label type O. Of course, most labels are impossible at most positions (e.g., yt cannot be Washington unless xt = Washington).",2.5 Modeling the sequence of tags y,[0],[0]
This scheme makes our generative model sensitive to specific contexts (which is accomplished in discriminative NER systems by contextual features).,2.5 Modeling the sequence of tags y,[0],[0]
"For example, the SM for y can learn that spoke to PER yesterday is a common 4-gram
5In §3.2, we will generalize this labeling scheme to allow multi-word named entities such as New York.
in the label sequence y, and thus we are more likely to label Washington as a person if x = . .",2.5 Modeling the sequence of tags y,[0],[0]
.spoke,2.5 Modeling the sequence of tags y,[0],[0]
to Washington,2.5 Modeling the sequence of tags y,[0],[0]
yesterday .,2.5 Modeling the sequence of tags y,[0],[0]
.,2.5 Modeling the sequence of tags y,[0],[0]
"..
",2.5 Modeling the sequence of tags y,[0],[0]
"We need one change to make this work, since now Y must include not only the standard NER labels Y ′ = {PER, LOC, ORG, GPE} but also words like Washington.",2.5 Modeling the sequence of tags y,[0],[0]
"Indeed, now Y = Y ′ ∪ Σ∗.",2.5 Modeling the sequence of tags y,[0],[0]
"But no uniform distribution exists over the infinite set Σ∗, so how should we replace the base distribution UY over labels in equation (5)?",2.5 Modeling the sequence of tags y,[0],[0]
"Answer: To draw from the new base distribution, sample y ∼ UY ′ ∪{CONTEXT}.",2.5 Modeling the sequence of tags y,[0],[0]
"If y = CONTEXT, however, then “expand” it by resampling y ∼ HCONTEXT.",2.5 Modeling the sequence of tags y,[0],[0]
"Here HCONTEXT is the base distribution over spellings of context words, and is learned just like the other Hy distributions in §2.4.",2.5 Modeling the sequence of tags y,[0],[0]
"Taking Y to be a random variable, we are interested in the posterior distribution p(Y = y | x) over label sequences y given the emitted word sequence",3.1 Sequential sampler,[0],[0]
x.,3.1 Sequential sampler,[0],[0]
"Our model does not admit an efficient dynamic programming algorithm, owing to the dependencies introduced among the Yt when we marginalize over the unknown G and P distributions that govern transitions and emissions, respectively.",3.1 Sequential sampler,[0],[0]
"In contrast to tagging with a hidden Markov model tagging, the distribution of each label Yt depends on all previous labels y1:t−1, for two reasons: ¬ The transition distribution p(Yt = y | y1:t−1) has unbounded dependence because of the PYP prior (4).  ",3.1 Sequential sampler,[0],[0]
"The emission distribution p(xt | Yt = y) depends on the emissions observed from any earlier tokens of y, because of the Chinese restaurant process (1).",3.1 Sequential sampler,[0],[0]
"When  is the only complication, block Metropolis-Hastings samplers have proven effective (Johnson et al., 2007).",3.1 Sequential sampler,[0],[0]
"However, this approach uses dynamic programming to sample from a proposal distribution efficiently, which ¬ precludes in our case.",3.1 Sequential sampler,[0],[0]
"Instead, we use sequential Monte Carlo (SMC)—sometimes called particle filtering—as a proposal distribution.",3.1 Sequential sampler,[0],[0]
"Particle filtering is typically used in online settings, including word segmentation (Borschinger and Johnson, 2011), to make decisions before all of x has been observed.",3.1 Sequential sampler,[0],[0]
"However, we are interested in the inference (or smoothing) problem that conditions on all of x (Dubbin and Blunsom, 2012; Tripuraneni et al., 2015).
",3.1 Sequential sampler,[0],[0]
"SMC employs a proposal distribution q(y | x)
whose definition decomposes as follows:
q(y1",3.1 Sequential sampler,[0],[0]
| x1),3.1 Sequential sampler,[0],[0]
"T∏
t=2
q(yt | y1:t−1,x1:t) (6)
for T = |x|.",3.1 Sequential sampler,[0],[0]
"To sample a sequence of latent labels, first sample an initial label y1 from q1, then proceed incrementally by sampling yt from qt(· | y1:t−1,x1:t) for t = 2, . . .",3.1 Sequential sampler,[0],[0]
", T .",3.1 Sequential sampler,[0],[0]
"The final sampled sequence y is called a particle, and is given an unnormalized importance weight of w̃ = w̃T · p($",3.1 Sequential sampler,[0],[0]
"| y1:T ) where w̃T was built up via
w̃t := w̃t−1 · p(y1:t,x1:t)
p(y1:t−1,x1:t−1) q(yt",3.1 Sequential sampler,[0],[0]
"| y1:t−1,x1:t) (7)
",3.1 Sequential sampler,[0],[0]
The SMC procedure consists of generating a system of M weighted particles whose unnormalized importance weights w̃(m) : 1 ≤ m ≤ M are normalized into w(m),3.1 Sequential sampler,[0],[0]
:= w̃(m)/ ∑M m=1,3.1 Sequential sampler,[0],[0]
"w̃
(m).",3.1 Sequential sampler,[0],[0]
"As M → ∞, SMC provides a consistent estimate of the marginal likelihood p(x) as 1M ∑M m=1 w̃
(m), and samples from the weighted particle system are distributed as samples from the desired posterior p(y | x) (Doucet and Johansen, 2009).",3.1 Sequential sampler,[0],[0]
Particle Gibbs.,3.1 Sequential sampler,[0],[0]
"We employ SMC as a kernel in an MCMC sampler (Andrieu et al., 2010).",3.1 Sequential sampler,[0],[0]
"In particular, we use a block Gibbs sampler in which we iteratively resample the hidden labeling y of a sentence x conditioned on the current labelings for all other sentences in the corpus.",3.1 Sequential sampler,[0],[0]
"In this context, the algorithm is called conditional SMC since one particle is always fixed to the previous sampler state for the sentence being resampled, which ensures that the MCMC procedure is ergodic.",3.1 Sequential sampler,[0],[0]
"At a high level, this procedure is analogous to other Gibbs samplers (e.g. for topic models), except that the conditional SMC (CSMC) kernel uses auxiliary variables (particles) in order to generate the new block variable assignments.",3.1 Sequential sampler,[0],[0]
The procedure is outlined in Algorithm 1.,3.1 Sequential sampler,[0],[0]
"Given a previous latent state assignment y′1:T and observations x1:T , the CSMC kernel produces a new latent state assignment via M auxiliary particles where one particle is fixed to the previous assignment.",3.1 Sequential sampler,[0],[0]
"For ergodicity, M ≥ 2, where larger values of M may improve mixing rate at the expense of increased computation per step.
",3.1 Sequential sampler,[0],[0]
Proposal distribution.,3.1 Sequential sampler,[0],[0]
The choice of proposal distribution q is crucial to the performance of SMC methods.,3.1 Sequential sampler,[0],[0]
"In the case of continuous latent variables,
it is common to propose yt from the transition probability p(Yt | y1:t−1) because this distribution usually has a simple form that permits efficient sampling.",3.1 Sequential sampler,[0],[0]
"However, it is possible to do better in the case of discrete latent variables.",3.1 Sequential sampler,[0],[0]
"The optimal proposal distribution is the one which minimizes the variance of the importance weights, and is given by
q(yt | y1:t−1,x1:t)",3.1 Sequential sampler,[0],[0]
":= p(yt | y1:t−1,x1:t) (8)
= p(yt | y1:t−1)p(xt | yt)
p(xt | y1:t−1)
where
p(xt | y1:t−1)= ∑
yt∈Y p(yt | y1:t−1)p(xt | yt) (9)
Substituting this expression in equation (7) and simplifying yields the incremental weight update:
w̃t := w̃t−1 · p(xt | y1:t−1) (10)
Resampling.",3.1 Sequential sampler,[0],[0]
"In filtering applications, it is common to use resampling operations to prevent weight degeneracy.",3.1 Sequential sampler,[0],[0]
We do not find resampling necessary here for three reasons.,3.1 Sequential sampler,[0],[0]
"First, note that we resample hidden label sequences that are only as long as the number of words in a given sentence.",3.1 Sequential sampler,[0],[0]
"Second, we use a proposal which minimizes the variance of the weights.",3.1 Sequential sampler,[0],[0]
"Finally, we use SMC as a kernel embedded in an MCMC sampler; asymptotically, this procedure yields samples from the desired posterior regardless of degeneracy (which only affects the mixing rate).",3.1 Sequential sampler,[0],[0]
"Practically speaking, one can diagnose the need for resampling via the effective sample size (ESS) of the particle system:
ESS := 1
∑M m=1(w̃",3.1 Sequential sampler,[0],[0]
"(m))2 =
( ∑M
m=1w (m))2
∑M m=1(w",3.1 Sequential sampler,[0],[0]
"(m))2
In our experiments, we find that ESS remains high (a significant fraction of M ) even for long sentences, suggesting that resampling is not necessary to enable mixing of the the Gibbs sampler.
",3.1 Sequential sampler,[0],[0]
Decoding.,3.1 Sequential sampler,[0],[0]
"In order to obtain a single latent variable assignment for evaluation purposes, we simply take the state of the Markov chain after a fixed number of iterations of particle Gibbs.",3.1 Sequential sampler,[0],[0]
"In principle, one could collect many samples during particle Gibbs and use them to perform minimum Bayes risk decoding under a given loss function.",3.1 Sequential sampler,[0],[0]
"However, this approach is somewhat slower and did not appear to improve performance in preliminary experiments
Algorithm 1 Conditional SMC 1: procedure CSMC(x1:T , y′1:T , M ) 2:",3.1 Sequential sampler,[0],[0]
Draw y(m)1 (eqn.,3.1 Sequential sampler,[0],[0]
8) for m ∈,3.1 Sequential sampler,[0],[0]
"[1,M − 1] 3: Set y(M)1 = y ′ 1
4: Set w̃(m)1 (eqn. 10) for m ∈",3.1 Sequential sampler,[0],[0]
"[1,M ] 5: for t",3.1 Sequential sampler,[0],[0]
= 2 to T do 6: Draw y(m)t (eqn.,3.1 Sequential sampler,[0],[0]
8) for m ∈,3.1 Sequential sampler,[0],[0]
"[1,M −1] 7: Set yMt = y ′ t 8: Set w̃(m)t (eqn. 10) for m ∈",3.1 Sequential sampler,[0],[0]
"[1,M ] 9: Set w̃(m) = w̃(m)T",3.1 Sequential sampler,[0],[0]
p($|y1:T ) for m ∈,3.1 Sequential sampler,[0],[0]
"[1,M ]
10: Draw index k where p(k = m) ∝",3.1 Sequential sampler,[0],[0]
w̃(m) 11: return y(k)1:T,3.1 Sequential sampler,[0],[0]
We now present an sampler for settings such as NER where each latent label emits a segment consisting of 1 or more words.,3.2 Segmental sampler,[0],[0]
"We make use of the same transition distribution p(yt | y1:t−1), which determines the probability of a label in a given context, and an emission distribution p(xt | yt) (namely Pyt); these are assumed to be drawn from hierarchical Pitman-Yor processes described in §2.5 and §2.1, respectively.",3.2 Segmental sampler,[0],[0]
"To allow the xt to be a multi-word string, we simply augment the character set with a distinguished space symbol ∈ Σ that separates words within a string.",3.2 Segmental sampler,[0],[0]
"For instance, New York would be generated as the 9-symbol sequence New York$.
Although the model emits New York all at once, we still formulate our inference procedure as a particle filter that proposes one tag for each word.",3.2 Segmental sampler,[0],[0]
"Thus, for a given segment label type y, we allow two tag types for its words:
• I-y corresponds to a non-final word in a segment of type y (in effect, a word with a following attached).",3.2 Segmental sampler,[0],[0]
•,3.2 Segmental sampler,[0],[0]
"E-y corresponds to the final word in a segment
of type y.
For instance, x1:2 = New York would be annotated as a location segment by defining y1:2 = I-LOC E-LOC.",3.2 Segmental sampler,[0],[0]
"This says that y1:2 has jointly emitted x1:2, an event with prior probability PLOC(New York).",3.2 Segmental sampler,[0],[0]
Each word that is not part of a named entity is considered to be a singleword segment.,3.2 Segmental sampler,[0],[0]
"For example, if the next word were x3 = hosted then it should be tagged with y3 = hosted as in §2.5, in which case x3 was emitted with probability 1.
",3.2 Segmental sampler,[0],[0]
"To adapt the sampler described in §3.1 for the segmental case, we need only to define the transition and emission probabilities used in equation (8) and its denominator (9).
",3.2 Segmental sampler,[0],[0]
"For the transition probabilities, we want to model the sequence of segment labels.",3.2 Segmental sampler,[0],[0]
"If yt−1 is an I- tag, we take p(yt | y1:t−1) = 1 , since then yt merely continues an existing segment.",3.2 Segmental sampler,[0],[0]
"Otherwise yt starts a new segment, and we take p(yt | y1:t−1) = 1 to be defined by the PYP’s probability Gy1:t−1(yt) as usual, but where we interpret the subscript y1:t−1 to refer to the possibly shorter sequence of segment labels implied by those t− 1 tags.
",3.2 Segmental sampler,[0],[0]
"For the emission probabilities, if yt has the form I-y or E-y, then its associated emission probability no longer has the form p(xt | yt), since the choice of xt also depends on any words emitted earlier in the segment.",3.2 Segmental sampler,[0],[0]
Let s ≤ t be the starting position of the segment that contains t.,3.2 Segmental sampler,[0],[0]
"If yt = E-y, then the emission probability is proportional to Py(xs xs+1 . . .",3.2 Segmental sampler,[0],[0]
xt).,3.2 Segmental sampler,[0],[0]
If yt = I-y then the emission probability is proportional to the prefix probability ∑ x Py(x) where x ranges over all strings in Σ∗,3.2 Segmental sampler,[0],[0]
that have xs xs+1 . . .,3.2 Segmental sampler,[0],[0]
xt,3.2 Segmental sampler,[0],[0]
as a proper prefix.,3.2 Segmental sampler,[0],[0]
"Prefix probabilities in Hy are easy to compute because Hy has the form of a language model, and prefix probabilities in Py are therefore also easy to compute (using a prefix tree for efficiency).
",3.2 Segmental sampler,[0],[0]
This concludes the description of the segmental sampler.,3.2 Segmental sampler,[0],[0]
Note that the particle Gibbs procedure is unchanged.,3.2 Segmental sampler,[0],[0]
"Automatically inducing parts-of-speech from raw text is a challenging problem (Goldwater et al., 2005).",4 Inducing parts-of-speech with type-level supervision,[0],[0]
"Our focus here is on the easier problem of type-supervised part-of-speech induction, in which (partial) dictionaries are used to guide inference (Garrette and Baldridge, 2012; Li et al., 2012).",4 Inducing parts-of-speech with type-level supervision,[0],[0]
"Conditioned on the unlabeled corpus and dictionary, we use the MCMC procedure described in §3.1 to impute the latent parts-of-speech.
",4 Inducing parts-of-speech with type-level supervision,[0],[0]
"Since dictionaries are freely available for hundreds of languages,6 we see this as a mild additional requirement in practice over the purely unsupervised setting.
",4 Inducing parts-of-speech with type-level supervision,[0],[0]
"In prior work, dictionaries have been used as constraints on possible parts-of-speech: words appearing in the dictionary take one of their known parts-
6https://www.wiktionary.org/
of-speech.",4 Inducing parts-of-speech with type-level supervision,[0],[0]
"In our setting, however, the dictionaries are not constraints but evidence.",4 Inducing parts-of-speech with type-level supervision,[0],[0]
"If monthly is listed in (only) the adjective lexicon, this tells us that PADJ sometimes generates monthly and therefore that HADJ may also tend to generate other words that end with -ly.",4 Inducing parts-of-speech with type-level supervision,[0],[0]
"However, for us, PADV(monthly) > 0",4 Inducing parts-of-speech with type-level supervision,[0],[0]
"as well, allowing us to still correctly treat monthly as a possible adverb if we later encounter it in a training or test corpus.",4 Inducing parts-of-speech with type-level supervision,[0],[0]
"We follow the experimental procedure described in Li et al. (2012), and use their released code and data to compare to their best model: a second-order maximum entropy Markov model parametrized with log-linear features (SHMM-ME).",4.1 Experiments,[0],[0]
"This model uses hand-crafted features designed to distinguish between different parts-of-speech, and it has special handling for rare words.",4.1 Experiments,[0],[0]
"This approach is surprisingly effective and outperforms alternate approaches such as cross-lingual transfer (Das and Petrov, 2011).",4.1 Experiments,[0],[0]
"However, it also has limitations, since words that do not appear in the dictionary will be unconstrained, and spurious or incorrect lexical entries may lead to propagation of errors.
",4.1 Experiments,[0],[0]
"The lexicons are taken from the Wiktionary project; their size and coverage are documented by (Li et al., 2012).",4.1 Experiments,[0],[0]
We evaluate our model on multi-lingual data released as part of the CoNLL 2007 and CoNLL-X shared tasks.,4.1 Experiments,[0],[0]
"In particular, we use the same set of languages as Li et al. (2012).7 For our method, we impute the parts-of-speech by running particle Gibbs for 100 epochs, where one epoch consists of resampling the states for a each sentence in the corpus.",4.1 Experiments,[0],[0]
"The final sampler state is then taken as a 1-best tagging of the unlabeled data.
",4.1 Experiments,[0],[0]
Results.,4.1 Experiments,[0],[0]
The results are reported in Table 1.,4.1 Experiments,[0],[0]
"We find that our hierarchical sequence memoizer (HSM) matches or exceeds the performance of the baseline (SHMM-ME) for nearly all the tested languages, particularly for morphologically rich languages such as German where the spelling distributions Hy may capture regularities.",4.1 Experiments,[0],[0]
"It is interesting to note that our model performs worse relative to the baseline for English; one possible explanation is that the baseline uses hand-engineered features whereas ours does not, and these features may have been tuned using English data for validation.
",4.1 Experiments,[0],[0]
7With the exception of Dutch.,4.1 Experiments,[0],[0]
"Unlike the other CoNLL languages, Dutch includes phrases, and the procedure by which these were split into tokens was not fully documented.
",4.1 Experiments,[0],[0]
Our generative model is supposed to exploit lexicons well.,4.1 Experiments,[0],[0]
"To see what is lost from using a generative model, we also compared with Li et al. (2012) on standard supervised tagging without any lexicons.",4.1 Experiments,[0],[0]
"Even here our generative model is very competive, losing only on English and Swedish.",4.1 Experiments,[0],[0]
Name lists and dictionaries are useful for NER particularly when in-domain annotations are scarce.,5 Boostrapping NER with type-level supervision,[0],[0]
"However, with little annotated data, discriminative training may be unable to reliably estimate lexical feature weights and may overfit.",5 Boostrapping NER with type-level supervision,[0],[0]
"In this section, we are interested in evaluating our proposed Bayesian model in the context of low-resource NER.",5 Boostrapping NER with type-level supervision,[0],[0]
"Most languages do not have corpora annotated for parts-of-speech, named-entities, syntactic parses, or other linguistic annotations.",5.1 Data,[0],[0]
"Therefore, rapidly deploying natural language technologies in a new language may be challenging.",5.1 Data,[0],[0]
"In the context of facilitating relief responses in emergencies such as natural disasters, the DARPA LORELEI (Low Resource Languages for Emergent Incidents) program has sponsored the development and release of representative “language packs” for Turkish and Uzbek with more languages planned (Strassel and Tracey, 2016).",5.1 Data,[0],[0]
"We use the named-entity annotations as part of these language packs which include persons, locations, organizations, and geo-political entities, in order to explore bootstrapping named-entity recognition from small amounts of data.",5.1 Data,[0],[0]
"We consider two types of data: ¬ in-context annotations, where sentences are fully annotated for named-entities, and  lexical resources.
",5.1 Data,[0],[0]
The LORELEI language packs lack adequate indomain lexical resources for our purposes.,5.1 Data,[0],[0]
"Therefore, we simulate in-domain lexical resources by holding out portions of the annotated development data and deriving dictionaries and name lists from them.",5.1 Data,[0],[0]
"For each label y ∈ {PER, LOC, ORG, GPE, CONTEXT}, our lexicon for y lists all distinct y-labeled strings that appear in the held-out data.",5.1 Data,[0],[0]
This setup ensures that the labels associated with lexicon entries correspond to the annotation guidelines used in the data we use for evaluation.,5.1 Data,[0],[0]
"It avoids possible problems that might arise when leveraging noisy out-of-domain knowledge bases, which we may explore in future.",5.1 Data,[0],[0]
In this section we report supervised NER experiments on two low-resource languages: Turkish and Uzbek.,5.2 Evaluation,[0],[0]
We vary both the amount of supervision as well as the size of the lexical resources.,5.2 Evaluation,[0],[0]
A challenge when evaluating the performance of a model with small amounts of training data is that there may be high-variance in the results.,5.2 Evaluation,[0],[0]
"In order to have more confidence in our results, we perform bootstrap resampling experiments in which the training set, evaluation set, and lexical resources are randomized across several replications of the same experiment (for each of the data conditions).",5.2 Evaluation,[0],[0]
"We use 10 replications for each of the data conditions reported in Figures 1–2, and report both the mean performance and 95% confidence intervals.
Baseline.",5.2 Evaluation,[0],[0]
"We use the Stanford NER system with a standard set of language-independent features (Finkel et al., 2005).8.",5.2 Evaluation,[0],[0]
This model is a conditional random field (CRF) with feature templates which include character n-grams as well as word shape features.,5.2 Evaluation,[0],[0]
"Crucially, we also incorporate lexical features.",5.2 Evaluation,[0],[0]
"The CRF parameters are regularized using an L1 penalty and optimized via Orthant-wise limited-memory quasi-Newton optimization (Andrew and Gao, 2007).",5.2 Evaluation,[0],[0]
"For both our proposed method and the discriminative baseline, we use a fixed set of hyperparameters (i.e. we do not use a separate validation set for tuning each data condition).",5.2 Evaluation,[0],[0]
"In order to make a fair comparison to the CRF, we use our sampler for forward inference only, without resampling on the test data.
Results.",5.2 Evaluation,[0],[0]
We show learning curves as a function of supervised training corpus size.,5.2 Evaluation,[0],[0]
Figure 1 shows that our generative model strongly beats the baseline in this low-data regime.,5.2 Evaluation,[0],[0]
"In particular, when there is little annotated training data, our proposed generative model can compensate by exploiting the lexicon, while the discriminative baseline scores terribly.",5.2 Evaluation,[0],[0]
"The performance gap decreases with larger
8We also experimented with neural models, but found that the CRF outperformed them in low-data conditions.
supervised corpora, which is consistent with prior results comparing generative and discriminative training (Ng and Jordan, 2002).
",5.2 Evaluation,[0],[0]
"In Figure 2, we show the effect of the lexicon’s size: as expected, larger lexicons are better.",5.2 Evaluation,[0],[0]
"The generative approach significantly outperforms the discriminative baseline at any lexicon size, although its advantage drops for smaller lexicons or larger training corpora.
",5.2 Evaluation,[0],[0]
"In Figure 1 we found that increasing the pseudocount c consistently decreases performance, so we used c = 1 in our other experiments.9",5.2 Evaluation,[0],[0]
This paper has described a generative model for low-resource sequence labeling and segmentation tasks using lexical resources.,6 Conclusion,[0],[0]
Experiments in semisupervised and low-resource settings have demonstrated its applicability to part-of-speech induction and low-resource named-entity recognition.,6 Conclusion,[0],[0]
There are many potential avenues for future work.,6 Conclusion,[0],[0]
Our model may be useful in the context of active learning where efficient re-estimation and performance in low-data conditions are important.,6 Conclusion,[0],[0]
"It would also be interesting to explore more expressive parameterizations, such recurrent neural networks for Hy.",6 Conclusion,[0],[0]
"In the space of neural methods, differentiable memory (Santoro et al., 2016) may be more flexible than the PYP prior, while retaining the ability of the model to cache strings observed in the gazetteer.",6 Conclusion,[0],[0]
"This work was supported by the JHU Human Language Technology Center of Excellence, DARPA LORELEI, and NSF grant IIS-1423276.",Acknowledgments,[0],[0]
"Thanks to Jay Feldman for early discussions.
",Acknowledgments,[0],[0]
9Why?,Acknowledgments,[0],[0]
"Even a pseudocount of c = 1 is enough to ensure that Py(s) Hy(s), since the prior probability Hy(s) is rather small for most strings in the lexicon.",Acknowledgments,[0],[0]
"Indeed, perhaps c < 1 would have increased performance, particularly if the lexicon reflects out-of-domain data.",Acknowledgments,[0],[0]
"This could be arranged, in effect, by using a hierarchical Bayesian model in which the lexicon and corpus emissions are not drawn from the identical distribution Py but only from similar (coupled) distributions.",Acknowledgments,[0],[0]
Lexical resources such as dictionaries and gazetteers are often used as auxiliary data for tasks such as part-of-speech induction and named-entity recognition.,abstractText,[0],[0]
"However, discriminative training with lexical features requires annotated data to reliably estimate the lexical feature weights and may result in overfitting the lexical features at the expense of features which generalize better.",abstractText,[0],[0]
"In this paper, we investigate a more robust approach: we stipulate that the lexicon is the result of an assumed generative process.",abstractText,[0],[0]
"Practically, this means that we may treat the lexical resources as observations under the proposed generative model.",abstractText,[0],[0]
The lexical resources provide training data for the generative model without requiring separate data to estimate lexical feature weights.,abstractText,[0],[0]
We evaluate the proposed approach in two settings: part-of-speech induction and lowresource named-entity recognition.,abstractText,[0],[0]
Bayesian Modeling of Lexical Resources for Low-Resource Settings,title,[0],[0]
"Flexible and computationally efficient models for streaming data are required in many machine learning applications, and in this paper we propose a new class of models for these situations.",1. Introduction,[0],[0]
"Specifically, we are interested in models suitable for domains that exhibit changes in the underlying generative process (Gama et al., 2014).",1. Introduction,[0],[0]
"We envision a situation, where one receives batches of data at discrete points in time.",1. Introduction,[0],[0]
"As each new batch arrives, we want to glean information from the new data, while also retaining relevant information from the historical observations.
",1. Introduction,[0],[0]
"Our modelling is inspired by previous works on Bayesian recursive estimation (Özkan et al., 2013; Kárnỳ, 2014), power priors (Ibrahim & Chen, 2000) and exponential for-
1Department of Mathematics, Unversity of Almerı́a, Almerı́a, Spain 2Department of Computer and Information Science, Norwegian University of Science and Technology, Trondheim, Norway 3Department of Computer Science, Aalborg University, Aalborg, Denmark 4Hugin",1. Introduction,[0],[0]
"Expert A/S, Aalborg, Denmark.",1. Introduction,[0],[0]
Correspondence to: Andrés,1. Introduction,[0],[0]
Masegosa,1. Introduction,[0],[0]
"<andresmasegosa@ual.es>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
getting approaches (Honkela & Valpola, 2003).",1. Introduction,[0],[0]
"However, all of these methods were developed for slowly changing processes, where the rate of change anticipated by the model is controlled by a quantity that must be set manually.",1. Introduction,[0],[0]
"Our solution, on the other hand, can accommodate both gradual and abrupt concept drift by continuously assessing the similarity between new and historic data using a fully Bayesian paradigm.
",1. Introduction,[0],[0]
"Building Bayesian models for data streams raises computational problems, as data may arrive with high velocity and is unbounded in size.",1. Introduction,[0],[0]
We therefore develop an approximate variational inference technique based on a novel lower-bound of the data likelihood function.,1. Introduction,[0],[0]
"The appropriateness of the approach is investigated through experiments using both synthetic and real-life data, giving encouraging results.",1. Introduction,[0],[0]
The proposed methods are released as part of an open-source toolbox for scalable probabilistic machine learning (http://www.amidsttoolbox.com),1. Introduction,[0],[0]
"(Masegosa et al., 2017; 2016b; Cabañas et al., 2016).",1. Introduction,[0],[0]
In this paper we focus on conjugate exponential Bayesian network models for performing Bayesian learning on streaming data.,2. Preliminaries,[0],[0]
"To simplify the presentation, we shall initially focus on the model structure shown in Figure 1 (a).",2. Preliminaries,[0],[0]
This model includes the observed data x = xi=1,2. Preliminaries,[0],[0]
":N , global hidden variables (or parameters) β = β1:M , a set of local hidden variables z = z1:N , and a vector of fixed (hyper) parameters denoted by α.",2. Preliminaries,[0],[0]
"Notice how the dynamics of the process is not included in the model of Figure 1 (a); the model will be set in the context of data streams in Section 4, where we extend it to incorporate explicit dynamics over the (global) parameters to capture concept drift.
",2. Preliminaries,[0],[0]
"With the conditional distributions in the model belonging to the exponential family, we have that all distributions are of the following form
ln p(Y |pa(Y ))",2. Preliminaries,[0],[0]
= lnhY + ηY,2. Preliminaries,[0],[0]
(pa(Y )),2. Preliminaries,[0],[0]
"T tY (Y )− aY (ηY (pa(Y ))),
where pa(Y ) denotes the parents of Y in the directed acyclic graph of the induced Bayesian network model.",2. Preliminaries,[0],[0]
"The scalar functions hY and aY (·) are the base measure and
the log-normalizer, respectively; the vector functions ηY (·) and tY (·) are the natural parameters and the sufficient statistics vectors, respectively.",2. Preliminaries,[0],[0]
"The subscript Y means that the associated functional forms may be different for the different factors of the model, but we may remove the subscript when clear from the context.",2. Preliminaries,[0],[0]
"By also requiring that the distributions are conjugate, we have that the posterior distribution for each variable in the model has the same functional form as its prior distribution.",2. Preliminaries,[0],[0]
"Consequently, learning (i.e. conditioning the model on observations) only changes the values of the parameters of the model, and not the functional form of the distributions.
",2. Preliminaries,[0],[0]
"Variational inference is a deterministic technique for finding tractable posterior distributions, denoted by q, which approximates the Bayesian posterior, p(β, z|x), that is often intractable to compute.",2. Preliminaries,[0],[0]
"More specifically, by letting Q be a set of possible approximations of this posterior, variational inference solves the following optimization problem for any model in the conjugate exponential family:
min q(β,z)∈Q
KL(q(β, z)|p(β, z|x)), (1)
where KL denotes the Kullback-Leibler divergence between two probability distributions.
",2. Preliminaries,[0],[0]
In the mean field variational approach the approximation familyQ is assumed to fully factorize.,2. Preliminaries,[0],[0]
"Extending the notation of Hoffman et al. (2013), we have that
q(β, z|λ,φ) = M∏ k=1 q(βk|λk) N∏ i=1",2. Preliminaries,[0],[0]
"J∏ j=1 q(zi,j |φi,j),
where J is the number of local hidden variables, which is assumed fixed for all i = 1, . . .",2. Preliminaries,[0],[0]
", N .",2. Preliminaries,[0],[0]
"The parameterizations of the variational distributions are made explicit, in that λ parameterize the variational distribution of β, while φ has the same role for the variational distribution of z.
To solve the minimization problem in Equation (1), the
variational approach exploits the transformation
lnP (x) = L(λ,φ|x,αu) + KL(q(β, z|λ,φ)|p(β, z|x)), (2) where L(·|·) is a lower bound of lnP (x) since KL is nonnegative.",2. Preliminaries,[0],[0]
x,2. Preliminaries,[0],[0]
"and αu are introduced in L’s notation to make explicit the function’s dependency on x, the data sample, and αu, the natural parameters of the prior over β.",2. Preliminaries,[0],[0]
"As lnP (x) is constant, minimizing the KL term is equivalent to maximizing the lower bound.",2. Preliminaries,[0],[0]
"Variational methods maximize this lower bound by applying a coordinate ascent that iteratively updates the individual variational distributions while holding the others fixed (Winn & Bishop, 2005).",2. Preliminaries,[0],[0]
"The key advantage of having a conjugate exponential model is that the gradients of theL function can be always computed in closed form (Winn & Bishop, 2005).",2. Preliminaries,[0],[0]
"Bayesian inference on streaming data has been widely studied (Ahmed et al., 2011; Doucet et al., 2000; Yao et al., 2009).",3. Related Work,[0],[0]
"In the context of variational inference, there are two main approaches.",3. Related Work,[0],[0]
Ghahramani & Attias (2000); Broderick et al. (2013) propose recursive Bayesian updating of the variational approximation.,3. Related Work,[0],[0]
"The streaming variational Bayes (SVB) algorithm (Broderick et al., 2013) is the most known approach of this category.",3. Related Work,[0],[0]
"Alternatively, one could cast the inference problem as a stochastic optimization problem.",3. Related Work,[0],[0]
"Stochastic variational inference (SVI) (Hoffman et al., 2013) and the closely related population variational Bayes (PVB) (McInerney et al., 2015) are prominent examples from this group.",3. Related Work,[0],[0]
"SVI assumes the existence of a fixed data set observed in a sequential manner, and in particular that this data set has a known finite size.",3. Related Work,[0],[0]
This is unrealistic when modeling data streams.,3. Related Work,[0],[0]
"PVB addresses this problem by using the frequentist notion of a population distribution, F, which is assumed to generate the data stream by repeatedly sampling M data points at the time.",3. Related Work,[0],[0]
"M parameterizes the size of the population, and helps control the variance of the population posterior.",3. Related Work,[0],[0]
"Unfortunately, M must be specified by the user.",3. Related Work,[0],[0]
"No clear rule exists regarding how to set it, and McInerney et al. (2015) show that its optimal value may differ from one data stream to another.
",3. Related Work,[0],[0]
"The problem of Bayesian modeling of non-stationary data streams (i.e., with concept drift (Gama et al., 2014)) is not addressed by SVB, as it assumes data exchangeability.",3. Related Work,[0],[0]
"An online variational inference method, which exponentially forgets the variational parameters associated with old data, was proposed by Honkela & Valpola (2003).",3. Related Work,[0],[0]
"The so-called power prior approach (Ibrahim & Chen, 2000) is also based on an exponential forgetting mechanisms, and has nice theoretical properties (Ibrahim et al., 2003).",3. Related Work,[0],[0]
"Nevertheless, both approaches rely on a hyper-parameter determining forgetting, which has to be set manually.",3. Related Work,[0],[0]
"PVB can
also adapt to concept drift, because the variance of the variational posterior never decreases below a given threshold indirectly controlled by M , but again, the hyper-parameter has to be set manually.
",3. Related Work,[0],[0]
A time series based modeling approach for concept drift using implicit transition models was pursued by Özkan et al. (2013); Kárnỳ (2014).,3. Related Work,[0],[0]
"Unfortunately, the implicit transition model depends on a hyper-parameter determining the forgetting-factor, which has to be manually set.",3. Related Work,[0],[0]
"In this paper we build on this approach, adapt it to variational settings, and place a hierarchical prior on its forgetting parameter.",3. Related Work,[0],[0]
This greatly improves the flexibility and accuracy of the resulting model when making inferences over drifting data streams.,3. Related Work,[0],[0]
In this section we extend the model in Figure 1 (a) to also account for the dynamics of the data stream being modeled.,4. Hierarchical Power Priors,[0],[0]
"We shall here assume that only the parameters β in Figure 1 (a) are time-varying, which we will indicate with the subscript t, i.e., βt.",4. Hierarchical Power Priors,[0],[0]
First we briefly describe the approach on which the proposed model is based.,4. Hierarchical Power Priors,[0],[0]
"Afterwards, we introduce the hierarchical power prior and detail a variational inference procedure for this model class.",4. Hierarchical Power Priors,[0],[0]
"In order to extend the model in Figure 1 (a) to data streams, we may introduce a transition model p(βt|βt−1) to explicitly model the evolution of the parameters over time, enabling the estimation of the predictive density at time t:
p(βt|x1:t−1) = ∫",4.1. Power Priors as Implicit Transition Models,[0],[0]
"p(βt|βt−1)p(βt−1|x1:t−1)dβt−1.
(3) However, this approach introduces two problems.",4.1. Power Priors as Implicit Transition Models,[0],[0]
"First of all, in non-stationary domains we may not have a single transition model or the transition model may be unknown.",4.1. Power Priors as Implicit Transition Models,[0],[0]
"Secondly, if we seek to position the model within the conjugate exponential family in order to be able to compute the gradients of L in closed-form, we need to ensure that the distribution family for βt is its own conjugate distribution, thereby severely limiting model expressivity (we can, e.g., not assign a Dirichlet distribution to βt).
",4.1. Power Priors as Implicit Transition Models,[0],[0]
"Rather than explicitly modeling the evolution of the βt parameters as in Equation (3), we instead follow the approach of Kárnỳ (2014) and Özkan et al. (2013) who define the time evolution model implicitly by constraining the maximum KL divergence over consecutive parameter distributions.",4.1. Power Priors as Implicit Transition Models,[0],[0]
"Specifically, by defining
pδ(βt|x1:t−1) = ∫",4.1. Power Priors as Implicit Transition Models,[0],[0]
"δ(βt − βt−1)p(βt−1|x1:t−1)dβt−1
(4)
one can restrict the space of possible distributions p(βt|x1:t−1), supported by an unknown transition model, by the constraint
KL(p(βt|x1:t−1), pδ(βt|x1:t−1)) ≤ κ.",4.1. Power Priors as Implicit Transition Models,[0],[0]
"(5)
Kárnỳ (2014) and Özkan et al. (2013) seek to approximate p(βt|x1:t−1) by the distribution p̂(βt|x1:t−1) having maximum entropy under the constraint in (5); for continuous distributions the maximum entropy can be formulated relative to an uninformative prior density pu(βt), which corresponds to the Kullbach-Leibler divergence between the two distributions.",4.1. Power Priors as Implicit Transition Models,[0],[0]
"This approach ensures that we will not underestimate the uncertainty in the parameter distribution and the particular solution being sought takes the form
p̂(βt|x1:t−1, ρt) ∝",4.1. Power Priors as Implicit Transition Models,[0],[0]
"pδ(βt|x1:t−1)ρtpu(βt)(1−ρt), (6)
",4.1. Power Priors as Implicit Transition Models,[0],[0]
"where 0 ≤ ρt ≤ 1 is indirectly defined by (5) which in turn depends on the user defined parameter κ.
",4.1. Power Priors as Implicit Transition Models,[0],[0]
"In our streaming data setting we follow assumed density filtering (Lauritzen, 1992) and the SVB approach (Broderick et al., 2013) and employ the approximation p(βt−1|x1:t−1)",4.1. Power Priors as Implicit Transition Models,[0],[0]
"≈ q(βt−1|λt−1), where q(βt−1|λt−1) is the variational distribution calculated in the previous time step.",4.1. Power Priors as Implicit Transition Models,[0],[0]
"Using this approximation in (3) and (4), we can express pδ in terms of λt−1 in which case (6) becomes
p̂(βt|λt−1, ρt) ∝ pδ(βt|λt−1)ρtpu(βt)(1−ρt), (7)
which we use as the prior density for time step t. Now, if pu(βt) belong to the same family as q(βt−1|λt−1), then p̂(βt|λt−1, ρt) will stay within the same family and have natural parameters ρtλt−1+(1−ρt)αu, where αu are the natural parameters of pu(βt).",4.1. Power Priors as Implicit Transition Models,[0],[0]
"Thus, under this approach, the transitioned posterior remains within the same exponential family, so we can enjoy the full flexibility of the conjugate exponential family (i.e. computing gradients of the L function in closed form), an option that would not be available if one were to explicitly specify a transition model as in Equation (3).
",4.1. Power Priors as Implicit Transition Models,[0],[0]
"So, at each time step, we simply have to solve the following variational problem, where only the prior changes with respect to the original SVB approach,
arg max λt,φt L(λt,φt|xt, ρtλt−1 + (1− ρt)αu).
",4.1. Power Priors as Implicit Transition Models,[0],[0]
"As stated in the following lemma, this approach coincides with the so-called power priors approach (Ibrahim & Chen, 2000), a term that we will also adopt in the following.
",4.1. Power Priors as Implicit Transition Models,[0],[0]
Lemma 1.,4.1. Power Priors as Implicit Transition Models,[0],[0]
"The Bayesian updating scheme described by Figure 1 (b) and Equation 6, but with ρt fixed to a constant value, is equivalent to the recursive application of
the Bayesian updating scheme of power priors (Ibrahim & Chen, 2000).",4.1. Power Priors as Implicit Transition Models,[0],[0]
"This scheme is expressed as follows:
p(β|x1,x0, ρ) ∝ p(x1|β)p(x0|β)ρp(β),
where x0 and x1 is the observation at time 0 (historical observation) and time 1 (current observation), respectively.
",4.1. Power Priors as Implicit Transition Models,[0],[0]
Proof sketch.,4.1. Power Priors as Implicit Transition Models,[0],[0]
"Translate the recursive Bayesian updating approach of power priors into an equivalent two time slice model, where β0 is given a prior distribution p and p(β1|β0) is a Dirac delta function.",4.1. Power Priors as Implicit Transition Models,[0],[0]
"The distribution p(β1|x0,x1, ρ) in this model is equivalent to p(β|x1,x0, ρ), which, in turn, is equivalent (up to proportionality) to p(x1|β1)p̂(β1|x0, ρt).",4.1. Power Priors as Implicit Transition Models,[0],[0]
"Note that the last p̂ term can alternatively be expressed as p̂(β1|x0, ρt) ∝ pδ(β1|x0)ρp(β1)1−ρ ∝ pδ(x0|β1)ρp(β1).
",4.1. Power Priors as Implicit Transition Models,[0],[0]
"The perspective provided by Lemma 1 introduces a well known result of power priors, which is also applicable in the current context (see the discussion after Theorem 1 in (Ibrahim et al., 2003)): “the power prior is an optimal prior to use and in fact minimizes the convex combination of KL divergences between two extremes: one in which no historical data is used and the other in which the historical data and current data are given equal weight.”",4.1. Power Priors as Implicit Transition Models,[0],[0]
"As noted in (Olesen et al., 1992; Özkan et al., 2013), this schema works as a moving window with exponential forgetting of past data, where the effective number of samples or, more technically, the so-called equivalent sample size of the posterior (Heckerman et al., 1995), converges to,
lim t→∞ ESSt = |xt| 1− ρ
(8)
if the size of the data batches is constant1.
",4.1. Power Priors as Implicit Transition Models,[0],[0]
For the experimental results reported in Section 5 we shall refer to the method outlined above as SVB with power priors (SVB-PP).,4.1. Power Priors as Implicit Transition Models,[0],[0]
"In the approach taken by Özkan et al. (2013) (and, by extension, SVB-PP), the forgetting factor ρt is user-defined.",4.2. The Hierarchical Power Prior Model,[0],[0]
"In this paper, we instead pursue a (hierarchical) Bayesian approach and introduce a prior distribution over ρt allowing the distribution over ρt (and thereby the forgetting mechanism) to adapt to the data stream.
",4.2. The Hierarchical Power Prior Model,[0],[0]
"As we shall see below, in order to support a variational updating scheme we need to restrict the prior distribution over ρt, effectively limiting the choice of prior distribution to
1For instance, the ESS of a Beta distribution is equal to the sum of the components of λt and, in turn, equal to the number of data samples seen so far plus the prior’s pseudo-samples.
",4.2. The Hierarchical Power Prior Model,[0],[0]
"either an exponential distribution or a normal distribution with fixed variance, both of which should be truncated to the interval",4.2. The Hierarchical Power Prior Model,[0],[0]
"[0, 1].",4.2. The Hierarchical Power Prior Model,[0],[0]
"Unless explicitly stated otherwise, we shall for now assume a truncated exponential distribution with natural parameter γ as prior distribution over ρt:
p(ρt|γ) = γ exp(−γρt) 1− exp(−γ) .",4.2. The Hierarchical Power Prior Model,[0],[0]
"(9)
The resulting model can be illustrated as in Figure 1 (b).",4.2. The Hierarchical Power Prior Model,[0],[0]
We shall refer to models of this type as hierarchical power prior (HPP) models.,4.2. The Hierarchical Power Prior Model,[0],[0]
"For updating the model distributions we pursue a variational approach, where we seek to maximize the evidence lower bound L in Equation (2) for time step t. However, since the model in Figure 1 (b) does not define a conjugate exponential distribution due to the introduction of p(ρt) we cannot maximize L directly.",4.3. Variational Updating,[0],[0]
"Instead we will derive a (double) lower bound L̂ (L̂ ≤ L) and use this lower bound as a proxy for the updating rules for the variational posteriors.
",4.3. Variational Updating,[0],[0]
"First of all, by instantiating the lower bound LHPP (λt,φt, ωt|xt,λt−1) in Equation (2) for the HPP model we obtain
LHPP (λt,φt, ωt|xt,λt−1) = Eq[ln p(xt,Zt|βt)]",4.3. Variational Updating,[0],[0]
"+ Eq[ln p̂(βt|λt−1, ρt)]",4.3. Variational Updating,[0],[0]
+ Eq[p(ρt|γ)]− Eq[ln q(Zt|φt)],4.3. Variational Updating,[0],[0]
"− Eq[q(βt|λt)]− Eq[q(ρt|ωt)], (10)
where ωt is the variational parameter for the variational distribution for ρt; as we shall see later, ωt is a scalar and is therefore not shown in boldface.",4.3. Variational Updating,[0],[0]
"For ease of presentation we shall sometimes drop from LHPP (λt,φt, ωt|xt,λt−1) the subscript as well as the explicit specification of the parameters when these is otherwise clear from the context.
",4.3. Variational Updating,[0],[0]
We now define L̂HPP,4.3. Variational Updating,[0],[0]
"(λt,φt, ωt|xt,λt−1)",4.3. Variational Updating,[0],[0]
"as
L̂HPP (λt,φt, ωt|xt,λt−1) = Eq[ln p(xt,Zt|βt)]",4.3. Variational Updating,[0],[0]
+ Eq[ρt]Eq[ln pδ(βt|λt−1)],4.3. Variational Updating,[0],[0]
+ (1− Eq[ρt])Eq[ln pu(βt)],4.3. Variational Updating,[0],[0]
+ Eq[p(ρt|γ)]− Eq[ln q(Zt|φt)],4.3. Variational Updating,[0],[0]
"− Eq[q(βt|λt)]− Eq[q(ρt|ωt)], (11)
which provide a lower bound for L. Theorem 1. L̂HPP gives a lower bound for LHPP :
L̂HPP",4.3. Variational Updating,[0],[0]
"(λt,φt, ωt|xt,λt−1) ≤ LHPP (λt,φt, ωt|xt,λt−1).
",4.3. Variational Updating,[0],[0]
Proof sketch.,4.3. Variational Updating,[0],[0]
The inequality derives by using Equation (12) and observing that ag(ρtλt−1,4.3. Variational Updating,[0],[0]
+ (1 − ρt)αu) ≤ ρtag(λt−1),4.3. Variational Updating,[0],[0]
"+ (1− ρt)ag(αu) because the log-normalizer
ag is always a convex function (Wainwright et al., 2008)",4.3. Variational Updating,[0],[0]
.,4.3. Variational Updating,[0],[0]
"Full details are given in the supplementary material.
",4.3. Variational Updating,[0],[0]
"Rather than seeking to maximize L we will instead maximize L̂. The gap between the two bounds is determined only by the log-normalizer of p̂(βt|λt−1, ρt):
L̂ − L = Eq[ρtag(λt−1)",4.3. Variational Updating,[0],[0]
+ (1− ρt)ag(αu) + ag(ρtλt−1,4.3. Variational Updating,[0],[0]
"+ (1− ρt)αu)]
(12)
",4.3. Variational Updating,[0],[0]
"Thus, maximizing L̂ wrt.",4.3. Variational Updating,[0],[0]
"the variational parameters λt and φ also maxmizes L. By the same observation, we also have that the (natural) gradients are consistent relative to the two bounds:
Corollary 1.
",4.3. Variational Updating,[0],[0]
"∇̂λtL = ∇̂λtL̂ ∇̂φtL = ∇̂φtL̂ .
",4.3. Variational Updating,[0],[0]
Proof.,4.3. Variational Updating,[0],[0]
"Follows immediately from Equation (12) because the difference does not depend of λt and φt.
",4.3. Variational Updating,[0],[0]
"Thus, updating the variational parametersλt andφt in HPP models can be done as for regular conjugate exponential models of the form in Figure 1.
",4.3. Variational Updating,[0],[0]
"In order to update ωt we rely on L̂, which we can maximize using the natural gradient wrt.",4.3. Variational Updating,[0],[0]
"ωt (Sato, 2001) and which can be calculated in closed form for a restricted distribution family for ρt.
",4.3. Variational Updating,[0],[0]
Lemma 2.,4.3. Variational Updating,[0],[0]
"Assuming that the sufficient statistics function for ρt is the identity function, t(ρt) = ρt, then we have
∇̂ωtL̂ =KL(q(βt|λt), pu(βt))",4.3. Variational Updating,[0],[0]
"− KL(q(βt|λt), pδ(βt|λt−1))",4.3. Variational Updating,[0],[0]
"+ γ − ωt (13)
",4.3. Variational Updating,[0],[0]
Proof sketch.,4.3. Variational Updating,[0],[0]
Based on a straightforward algebraic derivation of the gradient using standard properties of the exponential family.,4.3. Variational Updating,[0],[0]
"Full details are given in the supplementary material.
",4.3. Variational Updating,[0],[0]
"Note that the truncated exponential distribution (see Equation (9)) satisfies the restriction expressed in Lemma 2, and also note that the variational posterior q(ρt|ωt) will be a truncated exponential density too.
",4.3. Variational Updating,[0],[0]
"On the other hand, observe that the form of the natural gradient of ωt have an intuitive semantic interpretation, which also extends to the coordinate ascent variational message passing framework (Winn & Bishop, 2005) as shown by Masegosa et al. (2016a).",4.3. Variational Updating,[0],[0]
"Specifically, using the constant γ as a threshold, we see that if the uninformed prior pu(βt) provides a better fit to the variational posterior at time t than the variational parameters λt from the previous time step (KL(q(βt|λt), pu(βt))",4.3. Variational Updating,[0],[0]
"+
γ < KL(q(βt|λt), pδ(βt|λt−1))), then we will get a negative value for ωt when performing coordinate ascent using Equation (13).",4.3. Variational Updating,[0],[0]
This in turn implies that Eq[ρ] < 0.5 because Eq[ρ] = 1/(1 − e−ωt),4.3. Variational Updating,[0],[0]
"− 1/ωt (plotted in Figure 2), which means that we have a higher degree of forgetting for past data.",4.3. Variational Updating,[0],[0]
If ωt > 0 then Eq[ρ] > 0.5 and less past data is forgotten.,4.3. Variational Updating,[0],[0]
Figure 2 graphically illustrates this trade-off.,4.3. Variational Updating,[0],[0]
"The HPP model can immediately be extended to include multiple power priors ρ(i)t , one for each global parameter βi.",4.4. The Multiple Hierarchical Power Prior Model,[0],[0]
In this model the ρ (i) t ’s are pair-wise independent.,4.4. The Multiple Hierarchical Power Prior Model,[0],[0]
"The latter ensures that optimizing the L̂ can be performed as above, since the variational distribution for each ρ(i)t can be updated independently of the other variational distributions over ρ(j)t , for j 6= i.",4.4. The Multiple Hierarchical Power Prior Model,[0],[0]
"This extended model allows local model substructures to have different forgetting mechanisms, thereby extending the expressivity of the model.",4.4. The Multiple Hierarchical Power Prior Model,[0],[0]
We shall refer to this extended model as a multiple hierarchical power prior (MHPP) model.,4.4. The Multiple Hierarchical Power Prior Model,[0],[0]
"In this section we will evaluate the following methods:
• Streaming variational Bayes (SVB).",5.1. Experimental Set-up,[0],[0]
"• Four versions of Population Variational Bayes
(PVB)2: Population-size M equal to the average size of each data-batch, or M equal to a fixed value (M = 1000 in Section 5.2 and M = 10 000 in Section 5.3).",5.1. Experimental Set-up,[0],[0]
Learning-rate ν = 0.1 or ν = 0.01.,5.1. Experimental Set-up,[0],[0]
• Two versions of SVB-PP: ρ = 0.9 or ρ = 0.99.,5.1. Experimental Set-up,[0],[0]
"• Two versions of SVB-HPP: A single shared ρ (de-
noted SVB-HPP) or separate ρ(i) parameters (SVBMHPP).
",5.1. Experimental Set-up,[0],[0]
"The underlying variational engine is the VMP algorithm (Winn & Bishop, 2005) for all models; VMP was termi-
2We do not compare with SVI, because SVI is a special case of PVB when M is equal to the total size of the stream.
nated after 100 iterations or if the relative increase in the lower bound fell below 0.01%.",5.1. Experimental Set-up,[0],[0]
"All priors were uninformative, using either flat Gaussians, flat Gamma priors or uniform Dirichlet priors.",5.1. Experimental Set-up,[0],[0]
We set γ = 0.1 for the HPP priors.,5.1. Experimental Set-up,[0],[0]
Variational parameters were randomly initialized using the same seed for all methods.,5.1. Experimental Set-up,[0],[0]
"First, we illustrate the behavior of the different approaches in a controlled experimental setting: We produced an artificial data stream by generating 100 samples (i.e., |xt| = 100) from a Binomial distribution at each time step.",5.2. Evaluation using an Artificial Data Set,[0],[0]
"We artificially introduce concept drift by changing the parameter p of the Binomial distribution: p = 0.2 for the first 30 time steps, then p = 0.5 for the following 30 time steps, and finally p = 0.8 for the last 40 time steps.",5.2. Evaluation using an Artificial Data Set,[0],[0]
"The data stream was modelled using a Beta-Binomial model.
",5.2. Evaluation using an Artificial Data Set,[0],[0]
Parameter Estimation: Figure 3 shows the evolution of Eq[βt] for the different methods.,5.2. Evaluation using an Artificial Data Set,[0],[0]
"We recognize that SVB simply generates a running average of the data, as it is not able to adapt to the concept drift.",5.2. Evaluation using an Artificial Data Set,[0],[0]
"The results from PVB depend heavily on the learning rate ν, where the higher learning rate, which results in the more aggressive forgetting, works better in this example.",5.2. Evaluation using an Artificial Data Set,[0],[0]
"Recall, though, that ν needs to be hand-tuned to achieve an optimal performance.",5.2. Evaluation using an Artificial Data Set,[0],[0]
"As expected, the choice of M does not have an impact, because the present model has no local hidden variables (cf. Section 3).",5.2. Evaluation using an Artificial Data Set,[0],[0]
"SVB-PP produces results almost identical to PVB when ρ matches the learning rate of PVB (i.e., ρ = 1 − ν).",5.2. Evaluation using an Artificial Data Set,[0],[0]
"Finally, SVB-HPP provides the best results, almost mirroring the true model.
",5.2. Evaluation using an Artificial Data Set,[0],[0]
"Equivalent Sample Size (ESS): Figure 4 (left) gives the evolution of the equivalent sample size, ESSt, for the different methods 3.",5.2. Evaluation using an Artificial Data Set,[0],[0]
The ESS of PVB is always given by the constant M .,5.2. Evaluation using an Artificial Data Set,[0],[0]
"For SVB, the ESS monotonically increases as more data is seen, while SVB-PP exhibits convergence to the limiting value computed in Equation (8).",5.2. Evaluation using an Artificial Data Set,[0],[0]
A different behaviour is observed for SVB-HPP:,5.2. Evaluation using an Artificial Data Set,[0],[0]
"It is automatically ad-
3For this model, ESS is simply computed by summing up the components of the λt defining the Beta posterior.
justed.",5.2. Evaluation using an Artificial Data Set,[0],[0]
Notice that the values for this model is to be read off the alternative y-axis.,5.2. Evaluation using an Artificial Data Set,[0],[0]
"We can detect the the concept drift, by identifying where the ESS rapidly declines.
",5.2. Evaluation using an Artificial Data Set,[0],[0]
Evolution of Expected Forgetting factor: In Figure 4 (right) the series denoted “E[ρ]−100” shows the evolution of Eq[ρt] for the artificial data set.,5.2. Evaluation using an Artificial Data Set,[0],[0]
Notice how the model clearly identifies abrupt concept drift at time steps t = 30 and t = 60.,5.2. Evaluation using an Artificial Data Set,[0],[0]
The series denoted “E[ρ],5.2. Evaluation using an Artificial Data Set,[0],[0]
− 1000” illustrates the evolution of the parameter when we increase the batch size to 1000 samples.,5.2. Evaluation using an Artificial Data Set,[0],[0]
We recognize a more confident assessment about the absence of concept drift as more data is made available.,5.2. Evaluation using an Artificial Data Set,[0],[0]
"For this evaluation we consider three real data sets from different domains:
Electricity Market (Harries, 1999):",5.3.1. DATA AND MODELS,[0],[0]
The data set describes the electricity market of two Australian states.,5.3.1. DATA AND MODELS,[0],[0]
"It contains 45312 instances of 6 attributes, including a class label comparing the change of the electricity price related to a moving average of the last 24 hours.",5.3.1. DATA AND MODELS,[0],[0]
"Each instance in the data set represents 30 minutes of trading; during our analysis we created batches such that xt contains all information associated with month t.
The data is analyzed using a Bayesian linear regression model.",5.3.1. DATA AND MODELS,[0],[0]
The binary class label is assumed to follow a Gaussian distribution in order to fit within the conjugate model class.,5.3.1. DATA AND MODELS,[0],[0]
"Similarly, the marginal densities of the predictive attributes are also assumed to be Gaussian.",5.3.1. DATA AND MODELS,[0],[0]
"The regression coefficients are given Gaussian prior distributions, and the variance is given a Gamma prior.",5.3.1. DATA AND MODELS,[0],[0]
"Note that the overall distribution does not fall inside the conditional conjugate exponential family (Hoffman et al., 2013), hence PVB cannot be applied here, because lower-bound’s gradient cannot be computed in closed-form.
",5.3.1. DATA AND MODELS,[0],[0]
"GPS (Zheng et al., 2008; 2009; 2010):",5.3.1. DATA AND MODELS,[0],[0]
"This data set contains 17 621 GPS trajectories (time-stamped x and y coordinates), totalling more than 4.5 million observations.",5.3.1. DATA AND MODELS,[0],[0]
To reduce the data-size we kept only one out of every ten measurements.,5.3.1. DATA AND MODELS,[0],[0]
"We grouped the data so that xt contains all data collected during hour t of the day, giving a total of 24 batches of this stream.
",5.3.1. DATA AND MODELS,[0],[0]
"Here we employ a model with one independent Gaussian mixture model per day of the week, each mixture with 5 components.",5.3.1. DATA AND MODELS,[0],[0]
"This enables us to track changes in the users’ profiles across hours of the day, and also to monitor how the changes are affected by the day of the week.
",5.3.1. DATA AND MODELS,[0],[0]
"Finance (reference withheld): The data contains monthly aggregated information about the financial profile of
around 50 000 customers over 62 (non-consecutive) months.",5.3.1. DATA AND MODELS,[0],[0]
"Three attributes were extracted per customer, in addition to a class-label telling whether or not the customer will default within the next 24 months.
",5.3.1. DATA AND MODELS,[0],[0]
"We fit a naı̈ve Bayes model to this data set, where the distribution at the leaf-nodes is 5-component mixture of Gaussians distribution.",5.3.1. DATA AND MODELS,[0],[0]
"The distribution over the mixture node is shared by all the attributes, but not between the two classes of customers.
",5.3.1. DATA AND MODELS,[0],[0]
"A detailed description of all the models, including their structure and their variational families, is given at the supplementary material.",5.3.1. DATA AND MODELS,[0],[0]
"To evaluate the different methods discussed, we look at the test marginal log-likelihood (TMLL).",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"Specifically, each data batch is randomly split in a train data set, xt, and a test data set, x̃t, containing two thirds and one third of the data batch, respectively.",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"Then, TMLLt is computed as TMLLt = 1|x̃t| ∫ p(x̃t, zt|βt)p(βt|xt)dztdβt.",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
Figure 5 (left) shows for each method the difference between its TMLLt and that obtained by SVB (which is considered the baseline method).,5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"To improve readability, we only plot the results of the best performing method inside each group of methods.",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
The right-hand side of Figure 5 shows the development of Eq[ρt] over time for SVB-HPP and SVB-MHPP.,5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"For SVB-HPP we only have one ρt-parameter, and its value is given by the solid line.",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
SVB-MHPP utilizes one ρ(i) for each variational parameter.4,5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"In this case, we plot Eq[ρ(i)t ] at each point in time to indicate the variability between the different estimates throughout the series.",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"Finally, we compute each method’s aggregated test marginal log-likelihood measure ∑T t=1 TMLLt, and report these values in Table 1.
",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"For the electricity data set, we can see that the two proposed methods (SVB-HPP and SVB-MHPP) perform best.",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"All models are comparable during the first nine months, which is a period where our models detect no or very limited con-
4The numbers of variational parameters are 14, 78 and 33 for the Electricity, GPS and Financial model, respectively.
",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
cept drift (cf. top right plot or Figure 5).,5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"However, after this period, both SVB-HPP and SVB-MHPP detects substantial drift, and is able to adapt better than the other methods, which appear unable to adjust to the complex concept drift structure in the latter part of the data.",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"SVB-HPP and SVB-MHPP continue to behave at a similar level, mainly because when drift happens it typically includes a high proportion of the parameters of the model.
",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"For the GPS data set, we can observe how the SVB-MHPP is superior to the rest of the methods, particularly towards the end of the series.",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"When looking at Figure 5 (middle right panel), we can see that a significative proportion of the model parameters are drifting (i.e., Eq[ρ(i)t ]",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
≤ 0.05),5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"at all times, while another proportion of the parameters show a quite stable behavior (ρ-values above 0.9)",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
.,5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"This complex pattern is not captured well by SVB-HPP, which ends up assuming no concept drift after the initial time-step.
",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
The financial data set shows a different behavior.,5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"During the first months, SVB-MHPP slightly outperforms the rest of the approaches, but after month 30, SVB-PP with ρ = 0.9 is superior, with SVB-MHPP second.",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"Looking at the E[ρ(i)t ]-values of SVB-MHPP, we observe that there is significant concept drift in some of the parameters over the first few months.",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"However, only a few parameters exhibit noteworthy drift after the first third of the sequence.",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"Apparently, the simple SVB-PP approach has the upper hand when the drift is constant and fairly limited, at least when the optimal forgetting factor ρ has been identified.
",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"We conclude this section by highlighting that the performance of SVB-PP and PVB depend heavily on the hyperparameters of the model, cf. Table 1.",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"As an example, consider SVB-PP for the financial data set.",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"While it was the best overall with ρ = 0.9, it is inferior to SVB-MHPP if ρ = 0.99.",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"Similarly, PVB’s performance is sensitive both to ν (see in particular the results for the GPS data) and M (financial data).",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"These hyper-parameters are hard to fix, as their optimal values depend on data characteristics (see Broderick et al. (2013); McInerney et al. (2015) for similar conclusions).",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
We therefore believe that the fully Bayesian formulation is an important strong point of our approach.,5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"We have introduced a new class of Bayesian models for streaming data, able to capture changes in the underlying generative process.",6. Conclusions and Future Work,[0],[0]
"Unlike existing solutions to this problem, aimed at modeling slowly changing processes, our proposal is able to handle both abrupt and gradual concept drift following a Bayesian approach.",6. Conclusions and Future Work,[0],[0]
The new model accounts for the dynamics of the data stream by assuming that only the global parameters evolve over time.,6. Conclusions and Future Work,[0],[0]
"We intro-
duce the so-called hierarchical power priors, where a prior on the learning rate is given allowing it to adapt to the data stream.",6. Conclusions and Future Work,[0],[0]
"We have addressed the complexity of the underlying inference tasks by developing an approximate variational inference scheme that optimizes a novel lower bound of the likelihood function.
",6. Conclusions and Future Work,[0],[0]
As future work we aim to provide a sound approach to semantically characterize concept drift by inspecting the E[ρ(i)t ] values provided by SVB-MHPP.,6. Conclusions and Future Work,[0],[0]
This work was partly carried out as part of the AMIDST project.,Acknowledgements,[0],[0]
"AMIDST has received funding from the European Union’s Seventh Framework Programme for research, technological development and demonstration under grant agreement no 619209.",Acknowledgements,[0],[0]
"Furthermore, this research has been partly funded by the Spanish Ministry of Economy and Competitiveness, through projects TIN2015-74368JIN, TIN2013-46638-C3-1-P, TIN2016-77902-C3-3-P and by ERDF funds.",Acknowledgements,[0],[0]
Making inferences from data streams is a pervasive problem in many modern data analysis applications.,abstractText,[0],[0]
"But it requires to address the problem of continuous model updating, and adapt to changes or drifts in the underlying data generating distribution.",abstractText,[0],[0]
"In this paper, we approach these problems from a Bayesian perspective covering general conjugate exponential models.",abstractText,[0],[0]
Our proposal makes use of non-conjugate hierarchical priors to explicitly model temporal changes of the model parameters.,abstractText,[0],[0]
We also derive a novel variational inference scheme which overcomes the use of non-conjugate priors while maintaining the computational efficiency of variational methods over conjugate models.,abstractText,[0],[0]
The approach is validated on three real data sets over three latent variable models.,abstractText,[0],[0]
Bayesian Models of Data Streams with Hierarchical Power Priors,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2100–2105, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
NLP researchers and practitioners spend a considerable amount of time comparing machine-learned models of text that differ in relatively uninteresting ways.,1 Introduction,[0],[0]
"For example, in categorizing texts, should the “bag of words” include bigrams, and is tf-idf weighting a good idea?",1 Introduction,[0],[0]
"In learning word embeddings, distributional similarity approaches have been shown to perform competitively with neural network models when the hyperparameters (e.g., context window, subsampling rate, smoothing constant) are carefully tuned (Levy et al., 2015).",1 Introduction,[0],[0]
"These choices matter experimentally, often leading to big differences in performance, with little consistency across tasks and datasets in which combination of choices works best.",1 Introduction,[0],[0]
"Unfortunately, these differences tell us little about language or the problems that machine learners are supposed to solve.
",1 Introduction,[0],[0]
"We propose that these decisions can be automated in a similar way to hyperparameter selection (e.g., choosing the strength of a ridge or lasso regularizer).",1 Introduction,[0],[0]
"Given a particular text dataset and classification task, we show a technique for optimizing over the space of representational choices, along
with other “nuisances” that interact with these decisions, like hyperparameter selection.",1 Introduction,[0],[0]
"For example, using higher-order n-grams means more features and a need for stronger regularization and more training iterations.",1 Introduction,[0],[0]
"Generally, these decisions about instance representation are made by humans, heuristically; our work seeks to automate them, not unlike Daelemans et al. (2003), who proposed to use genetic algorithms to optimize representational choices.
",1 Introduction,[0],[0]
"Our technique instantiates sequential modelbased optimization (SMBO; Hutter et al., 2011).",1 Introduction,[0],[0]
"SMBO and other Bayesian optimization approaches have been shown to work well for hyperparameter tuning (Bergstra et al., 2011; Hoffman et al., 2011; Snoek et al., 2012).",1 Introduction,[0],[0]
"Though popular in computer vision (Bergstra et al., 2013), these techniques have received little attention in NLP.
",1 Introduction,[0],[0]
We apply it to logistic regression on a range of topic and sentiment classification tasks.,1 Introduction,[0],[0]
"Consistently, our method finds representational choices that perform better than linear baselines previously reported in the literature, and that, in some cases, are competitive with more sophisticated non-linear models trained using neural networks.",1 Introduction,[0],[0]
"Let the training data consist of a collection of pairs dtrain = 〈〈d.i1, d.o1〉, . . .",2 Problem Formulation and Notation,[0],[0]
", 〈d.in, d.on〉〉, where each input d.i ∈ I is a text document and each output",2 Problem Formulation and Notation,[0],[0]
d.o ∈,2 Problem Formulation and Notation,[0],[0]
"O, the output space.",2 Problem Formulation and Notation,[0],[0]
"The overall training goal is to maximize a performance function f (e.g., classification accuracy, log-likelihood, F1 score, etc.) of a machine-learned model, on a held-out dataset, ddev ∈ (I× O)n′ .
",2 Problem Formulation and Notation,[0],[0]
"Classification proceeds in three steps: first, x : I → RN maps each input to a vector representation.",2 Problem Formulation and Notation,[0],[0]
"Second, a predictive model (typically, its parameters) is learned from the inputs (now transformed into vectors) and outputs:",2 Problem Formulation and Notation,[0],[0]
L : (RN × O)n,2 Problem Formulation and Notation,[0],[0]
→ (RN → O).,2 Problem Formulation and Notation,[0],[0]
"Finally, the resulting classifier c :",2 Problem Formulation and Notation,[0],[0]
I → O is fixed as L(dtrain) ◦,2 Problem Formulation and Notation,[0],[0]
"x (i.e., the composition of the representation function with
2100
the learned mapping).",2 Problem Formulation and Notation,[0],[0]
Here we consider linear classifiers of the form c(d.i) = arg maxo∈O,2 Problem Formulation and Notation,[0],[0]
"w>o x(d.i), where the parameters wo ∈ RN , for each output o, are learned using logistic regression on the training data.",2 Problem Formulation and Notation,[0],[0]
We let w denote the concatenation of all wo.,2 Problem Formulation and Notation,[0],[0]
Hence the parameters can be understood as a function of the training data and the representation function x.,2 Problem Formulation and Notation,[0],[0]
"The performance function f , in turn, is a function of the held-out data ddev and x—also w and dtrain , through x.",2 Problem Formulation and Notation,[0],[0]
"For simplicity, we will write “f(x)” when the rest are clear from context.
",2 Problem Formulation and Notation,[0],[0]
"Typically, x is fixed by the model designer, perhaps after some experimentation, and learning focuses on selecting the parameters w. For logistic regression and many other linear models, this training step reduces to convex optimization in N |O| dimensions—a solvable problem that is costly for large datasets and/or large output spaces.",2 Problem Formulation and Notation,[0],[0]
"In seeking to maximize f with respect to x, we do not wish to carry out training any more times than necessary.
",2 Problem Formulation and Notation,[0],[0]
Choosing x can be understood as a problem of selecting hyperparameter values.,2 Problem Formulation and Notation,[0],[0]
"We therefore turn to Bayesian optimization, a family of techniques that can be used to select hyperparameter values intelligently when solving for parameters (w) is costly.",2 Problem Formulation and Notation,[0],[0]
"Our approach is based on sequential model-based optimization (SMBO; Hutter et al., 2011).",3 Bayesian Optimization,[0],[0]
It iteratively chooses representation functions x.,3 Bayesian Optimization,[0],[0]
"On each round, it makes this choice through a probabilistic model of f , then evaluates f—we call this a “trial.”",3 Bayesian Optimization,[0],[0]
"As in any iterative search algorithm, the goal is to balance exploration of options for x with exploitation of previously-explored options, so that a good choice is found in a small number of trials.
",3 Bayesian Optimization,[0],[0]
"More concretely, in the tth trial, xt is selected using an acquisition function A and a “surrogate” probabilistic model pt.",3 Bayesian Optimization,[0],[0]
"Second, f is evaluated given xt—an expensive operation which involves training to learn parameters w and assessing performance on the held-out data.",3 Bayesian Optimization,[0],[0]
"Third, the surrogate model is updated.",3 Bayesian Optimization,[0],[0]
"See Algorithm 1; details on A and pt follow.
",3 Bayesian Optimization,[0],[0]
Acquisition Function.,3 Bayesian Optimization,[0],[0]
"A good acquisition function returns high values for x when either the value f(x) is predicted to be high, or the uncertainty about f(x)’s value is high; balancing between these is the classic tradeoff between exploitation
Algorithm 1",3 Bayesian Optimization,[0],[0]
"SMBO algorithm Input: number of trials T , target function f p1 = initial surrogate model Initialize y∗
for t = 1 to T do xt ← arg maxx A(x; pt, y∗) yt",3 Bayesian Optimization,[0],[0]
← evaluate f(xt),3 Bayesian Optimization,[0],[0]
"Update y∗
Estimate pt given x1:t and y1:",3 Bayesian Optimization,[0],[0]
"t end for
and exploration.",3 Bayesian Optimization,[0],[0]
"We use a criterion called Expected Improvement (EI; Jones, 2001),1 which is the expectation (under the current surrogate model pt) that f(x) = y will exceed f(x∗) = y∗:
A(x; pt, y∗) = ∫ ∞ −∞ max(y",3 Bayesian Optimization,[0],[0]
"− y∗, 0)pt(y",3 Bayesian Optimization,[0],[0]
"| x)dy
where x∗ is chosen depending on the surrogate model, discussed below.",3 Bayesian Optimization,[0],[0]
"(For now, think of it as a strongly-performing “benchmark” discovered in earlier iterations.)",3 Bayesian Optimization,[0],[0]
"Other options for the acquisition function include maximum probability of improvement (Jones, 2001), minimum conditional entropy (Villemonteix et al., 2009), Gaussian process upper confidence bound (Srinivas et al., 2010), or a combination of them (Hoffman et al., 2011).
",3 Bayesian Optimization,[0],[0]
Surrogate Model.,3 Bayesian Optimization,[0],[0]
"As a surrogate model, we use a tree-structured Parzen estimator (TPE; Bergstra et al., 2011).2",3 Bayesian Optimization,[0],[0]
This is a nonparametric approach to density estimation.,3 Bayesian Optimization,[0],[0]
"We seek to estimate pt(y | x) where y = f(x), the performance function that is expensive to compute exactly.",3 Bayesian Optimization,[0],[0]
"The TPE approach
seeks pt(y | x) ∝",3 Bayesian Optimization,[0],[0]
"pt(y) · { p<t (x), if y<y ∗
p≥t (x), if y≥y∗ , where
p<t and p ≥ t are densities estimated using observations from previous trials that are less than and greater than y∗, respectively.",3 Bayesian Optimization,[0],[0]
"In TPE, y∗ is defined as some quantile of the observed y from previous trials; we use 15-quantiles.
",3 Bayesian Optimization,[0],[0]
"As shown by Bergstra et al. (2011), the Expected Improvement in TPE can be written as:
1EI is the most widely used acquisition function that has been shown to work well on a range of tasks.
",3 Bayesian Optimization,[0],[0]
"2Another common approach to the surrogate is the Gaussian process (Rasmussen and Williams, 2006; Hoffman et al., 2011; Snoek et al., 2012).",3 Bayesian Optimization,[0],[0]
"Like Bergstra et al. (2011), our preliminary experiments found the TPE to perform favorably.",3 Bayesian Optimization,[0],[0]
"Further TPE’s tree-structured configuration space is advantageous, because it allows nested definitions of hyperparameters, which we exploit in our experiments (e.g., only allows bigrams to be chosen if unigrams are also chosen).
",3 Bayesian Optimization,[0],[0]
"A(x; pt, y∗) ∝",3 Bayesian Optimization,[0],[0]
"( γ + p < t (x)
p≥t (x) (1− γ)
)−1 , where
γ = pt(y < y∗), fixed at 0.15 by definition of y∗ (above).",3 Bayesian Optimization,[0],[0]
"Here, we prefer x with high probability under p≥t (x) and low probability under p < t (x).",3 Bayesian Optimization,[0],[0]
"To maximize this quantity, we draw many candidates according to p≥t (x) and evaluate them according to p<t (x)/p ≥ t (x).",3 Bayesian Optimization,[0],[0]
Note that p(y) does not need to be given an explicit form.,3 Bayesian Optimization,[0],[0]
"To compute p<t (x) and p≥t (x), we associate each hyperparameter with a node in the graphical model and multiply individual probabilities at every node—see Bergstra et al. (2011) for details.",3 Bayesian Optimization,[0],[0]
We fix L to logistic regression.,4 Experiments,[0],[0]
"We optimize text representation based on the types of n-grams used, the type of weighting scheme, and the removal of stopwords; we also optimize the regularizer and training convergence criterion, which interact with the representation.",4 Experiments,[0],[0]
"See Table 1 for a complete list.
",4 Experiments,[0],[0]
"Note that even with this limited number of options, the number of possible combinations is huge,3 so exhaustive search is computationally expensive.",4 Experiments,[0],[0]
"In all our experiments for all datasets, we limit ourselves to 30 trials per dataset.",4 Experiments,[0],[0]
"The only preprocessing we applied was downcasing.
",4 Experiments,[0],[0]
We always use a development set to evaluate f(x) during learning and report the final result on an unseen test set.,4 Experiments,[0],[0]
"We summarize the hyperparameters selected by our method, and the accuracies achieved (on test data) in Table 5.",4 Experiments,[0],[0]
We discuss comparisons to baselines for each dataset in turn.,4 Experiments,[0],[0]
"For each of our datasets, we select supervised, nonensemble classification methods from previous literature as baselines.",4 Experiments,[0],[0]
"In each case, we emphasize comparisons with the best-published linear method
3It is actually infinite since the reg. strength and conv.",4 Experiments,[0],[0]
"tolerance are continuous values, but we could discretize them.
",4 Experiments,[0],[0]
(often an SVM with a linear kernel with representation selected by experts) and the best-published method overall.,4 Experiments,[0],[0]
"In the following, “SVM” always means “linear SVM.”",4 Experiments,[0],[0]
"All methods were trained and evaluated on the same training/testing splits as baselines; in cases where standard development sets were not available, we used a random 20% of the training data as a development set.
",4 Experiments,[0],[0]
"Stanford sentiment treebank (Socher et al., 2013)—Table 2.",4 Experiments,[0],[0]
A sentence-level sentiment analysis dataset of rottentomatoes.com movie reviews: http://nlp.stanford.edu/sentiment.,4 Experiments,[0],[0]
We use the binary classification task where the goal is to predict whether a review is positive or negative (no neutral).,4 Experiments,[0],[0]
"Our logistic regression model outperforms the baseline SVM reported by Socher et al. (2013), who used only unigrams but did not specify the weighting scheme for their SVM baseline.",4 Experiments,[0],[0]
"While our result is still below the state-of-the-art based on the the recursive neural tensor networks (Socher et al., 2013) and the paragraph vector (Le and Mikolov, 2014), we show that logistic regression is comparable with recursive and matrix-vector neural networks (Socher et al., 2011; Socher et al., 2012).
",4 Experiments,[0],[0]
"Amazon electronics (McAuley and Leskovec, 2013)—Table 3.",4 Experiments,[0],[0]
A binary sentiment analysis dataset of Amazon electronics product reviews: http://riejohnson.com/cnn data.html.,4 Experiments,[0],[0]
"The bestperforming methods on this dataset are based on convolutional neural networks (Johnson and Zhang, 2015).4 Our method is on par with the secondbest of these, outperforming all of the reported feed-forward neural networks and SVM variants Johnson and Zhang used as baselines.",4 Experiments,[0],[0]
"They varied
4These are convolutional neural networks with a rectifier activation function, trained under `2 regularization with stochastic gradient descent.",4 Experiments,[0],[0]
"The authors also consider an extension based on parallel CNN that we do not include here.
",4 Experiments,[0],[0]
"the representations, and used log term frequency and normalization to unit vectors as the weighting scheme, after finding that this outperformed term frequency.",4 Experiments,[0],[0]
"Our method achieved the best performance with binary weighting, which they did not consider.
",4 Experiments,[0],[0]
"IMDB movie reviews (Maas et al., 2011)— Table 3.",4 Experiments,[0],[0]
A binary sentiment analysis dataset of highly polar IMDB movie reviews: http://ai.stanford.edu/~amaas/data/sentiment.,4 Experiments,[0],[0]
"The results parallel those for Amazon electronics; our method comes close to convolutional neural networks (Johnson and Zhang, 2015), which are state-of-the-art.5 It outperforms SVMs and feed-forward neural networks, the restricted Boltzmann machine approach presented by Dahl et al. (2012), and compressive feature learning (Paskov et al., 2013).6
Congressional vote (Thomas et al., 2006)—Table 4.",4 Experiments,[0],[0]
A dataset of transcripts from the U.S. Congressional debates: http://www.cs.cornell.edu/~ainur/sle-data.html.,4 Experiments,[0],[0]
"Similar to previous work (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010), we consider the task to predict the vote (“yea” or “nay”) for the speaker of each speech segment (speaker-based speech-segment classification).",4 Experiments,[0],[0]
"Our method outperforms the best results of Yessenalina et al. (2010), which use a multi-level structured
5As noted, semi-supervised and ensemble methods are excluded for a fair comparison.
",4 Experiments,[0],[0]
"6This approach is based on minimum description length, using unlabeled data to select a set of higher-order n-grams to use as features.
model based on a latent-variable SVM.",4 Experiments,[0],[0]
We show comparisons to two weaker baselines as well.,4 Experiments,[0],[0]
20 Newsgroups is a benchmark topic classification dataset: http://qwone.com/~jason/20Newsgroups.,"20 Newsgroups (Lang, 1995) all topics—Table 6.",[0],[0]
There are 20 topics in this dataset.,"20 Newsgroups (Lang, 1995) all topics—Table 6.",[0],[0]
"Our method outperforms state-of-the-art methods including the distributed structured output model (Srikumar and Manning, 2014).7 The strong logistic regression baseline from Paskov et al. (2013) uses all 5-grams, heuristic normalization, and elastic net regularization; our method found that unigrams and bigrams, with binary weighting and `2 penalty, achieved far better results.
20 Newsgroups: talk.religion.misc vs. alt.atheism and comp.graphics vs. comp.windows.x.","20 Newsgroups (Lang, 1995) all topics—Table 6.",[0],[0]
We derived three additional topic classification tasks from the 20N dataset.,"20 Newsgroups (Lang, 1995) all topics—Table 6.",[0],[0]
The first and second tasks are talk.religion.misc vs. alt.atheism (test size = 686) and comp.graphics vs. comp.windows.x (test size = 942).,"20 Newsgroups (Lang, 1995) all topics—Table 6.",[0],[0]
"Wang and Manning (2012) report a bigram naı̈ve Bayes model achieving 85.1% and 91.2% on these tasks, respectively (best single model results).8 Our
7This method was designed for structured prediction, but Srikumar and Manning (2014) also applied it to classification.","20 Newsgroups (Lang, 1995) all topics—Table 6.",[0],[0]
It attempts to learn a distributed representation for features and for labels.,"20 Newsgroups (Lang, 1995) all topics—Table 6.",[0],[0]
"The authors used unigrams and did not discuss the weighting scheme.
","20 Newsgroups (Lang, 1995) all topics—Table 6.",[0],[0]
"8They also report a naı̈ve Bayes/SVM ensemble achieving 87.9% and 91.2%.
method achieves 86.3% and 92.1% using slightly different representations (see Table 5).","20 Newsgroups (Lang, 1995) all topics—Table 6.",[0],[0]
"The last task is to classify related science documents into four science topics (sci.crypt, sci.electronics, sci.space, sci.med; test size = 1, 899).","20 Newsgroups (Lang, 1995) all topics—Table 6.",[0],[0]
We were not able to find previous results that are comparable to ours on this task; we include our result (95.82%) to enable further comparisons in the future.,"20 Newsgroups (Lang, 1995) all topics—Table 6.",[0],[0]
Optimized representations.,5 Discussion,[0],[0]
"For each task, the chosen representation is different.",5 Discussion,[0],[0]
"Out of all possible choices in our experiments (Table 1), each of them is used by at least one of the datsets (Table 5).",5 Discussion,[0],[0]
"For example, on the Congress vote dataset, we only need to use bigrams, whereas on the Amazon electronics dataset we need to use {1, 2, 3}-grams.",5 Discussion,[0],[0]
"The binary weighting scheme works well for most of the datasets, except the sentence-level sentiment analysis task, where the tf-idf weighting scheme was selected.",5 Discussion,[0],[0]
`2 regularization was best in all cases but one.,5 Discussion,[0],[0]
"We do not believe that an NLP expert would be likely to make these particular choices, except through the same kind of trial-and-error process our method automates efficiently.
",5 Discussion,[0],[0]
Number of trials.,5 Discussion,[0],[0]
We ran 30 trials for each dataset in our experiments.,5 Discussion,[0],[0]
Figure 1 shows each trial accuracy and the best accuracy on development data as we increase the number of trials for two datasets.,5 Discussion,[0],[0]
"We can see that 30 trials are generally enough for the model to obtain good results, although the search space is large.
",5 Discussion,[0],[0]
Transfer learning and multitask setting.,5 Discussion,[0],[0]
We treat each dataset independently and create a separate model for each of them.,5 Discussion,[0],[0]
"It is also possible to learn from previous datasets (i.e., transfer learning) or to learn from all datasets simultaneously (i.e., multitask learning) to improve performance.",5 Discussion,[0],[0]
"This has the potential to reduce the number of trials
required even further.",5 Discussion,[0],[0]
"See Bardenet et al. (2013), Swersky et al. (2013), and Yogatama and Mann (2014) for more about how to perform Bayesian optimization in these settings.
",5 Discussion,[0],[0]
Beyond supervised learning.,5 Discussion,[0],[0]
Our framework could also be extended to unsupervised and semisupervised models.,5 Discussion,[0],[0]
"For example, in document clustering (e.g., k-means), we also need to construct representations for documents.",5 Discussion,[0],[0]
Log-likelihood might serve as a performance function.,5 Discussion,[0],[0]
A range of random initializations might be considered.,5 Discussion,[0],[0]
Investigation of this approach for nonconvex problems is an exciting area for future work.,5 Discussion,[0],[0]
We used Bayesian optimization to optimize choices about text representations for various categorization problems.,6 Conclusion,[0],[0]
Our technique identifies settings for a standard linear model (logistic regression) that are competitive with far more sophisticated methods on topic classification and sentiment analysis.,6 Conclusion,[0],[0]
We thank several reviewers for their helpful feedback.,Acknowledgments,[0],[0]
This work was supported by the Defense Advanced Research Projects Agency through grant FA87501420244 and computing resources provided by Amazon.,Acknowledgments,[0],[0]
This research was completed while NAS was at CMU.,Acknowledgments,[0],[0]
"When applying machine learning to problems in NLP, there are many choices to make about how to represent input texts.",abstractText,[0],[0]
"They can have a big effect on performance, but they are often uninteresting to researchers or practitioners who simply need a module that performs well.",abstractText,[0],[0]
"We apply sequential model-based optimization over this space of choices and show that it makes standard linear models competitive with more sophisticated, expensive state-ofthe-art methods based on latent variables or neural networks on various topic classification and sentiment analysis problems.",abstractText,[0],[0]
Our approach is a first step towards black-box NLP systems that work with raw text and do not require manual tuning.,abstractText,[0],[0]
Bayesian Optimization of Text Representations,title,[0],[0]
"In recent years, Bayesian optimization has gained a growing attention from machine learning experts in, both, academia and industry (Shahriari et al., 2016).",1. Introduction,[0],[0]
"It takes the widespread application of machine learning to the next level of sophistication as it enables to automatically fine-tune hyperparameters (Snoek et al., 2012), whether they are parametrizing data pre-processors, models or the learning algorithms.",1. Introduction,[0],[0]
"Finetuning is essential to obtain state-of-the-art performance
1Amazon, Berlin, Germany.",1. Introduction,[0],[0]
"2Amazon, Cambridge, United Kingdom.",1. Introduction,[0],[0]
"Correspondence to: Rodolphe Jenatton <jenatton@amazon.de>, Cedric Archambeau <cedrica@amazon.de>, Javier Gonzalez <gojav@amazon.co.uk>, Matthias Seeger <matthias@amazon.de>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
with complex machine learning models, such as deep neural networks.",1. Introduction,[0],[0]
"Historically, this vital step has been done, either manually, or via regular or random grid search, which can consume vast amounts of human expert time and are wasteful of computing resources.",1. Introduction,[0],[0]
"Hence, one of the main benefits of Bayesian optimization is that it removes this burden from the shoulders of the practitioners, who can then focus their attention on more rewarding value-adding tasks.
",1. Introduction,[0],[0]
"To set the stage, our goal is to solve a global optimization problem:
min x∈X f(x),
where X is the optimization domain and f is a black-box function, typically continuous and multimodal.",1. Introduction,[0],[0]
We further assume that querying f is costly.,1. Introduction,[0],[0]
"For example, f may be the outcome of a physical experiment or require a large amount of computation.",1. Introduction,[0],[0]
"The latter arises when f corresponds to a model selection score for a machine learning model trained on a possibly large dataset.
",1. Introduction,[0],[0]
"The protocol for sequential Bayesian optimization proceeds as follows (Mockus et al., 1978; Shahriari et al., 2016).",1. Introduction,[0],[0]
"Given n noisy evaluations yi ≈ f(xi), i ∈ {1, . . .",1. Introduction,[0],[0]
",",1. Introduction,[0],[0]
"n}, a surrogate probabilistic model of f is maintained.",1. Introduction,[0],[0]
Our goal is to find a global optimum of f by querying it as few times as possible.,1. Introduction,[0],[0]
The location xn+1 is chosen by maximizing an acquisition function which performs an explorationexploitation trade-off.,1. Introduction,[0],[0]
"A common choice for the surrogate model is a Gaussian process (GP) (Rasmussen & Williams, 2006).",1. Introduction,[0],[0]
"For a GP surrogate model, common acquisition functions can be tractably computed and optimized via gradientbased optimization algorithms.",1. Introduction,[0],[0]
"While existing Bayesian optimization approaches mitigate the high evaluation cost of f , they suffer from the curse of dimensionality when facing a high-dimensional space X .
",1. Introduction,[0],[0]
"In this paper, we introduce a novel methodology able to exploit a given tree-shaped dependency structure on X by transferring information between overlapping paths.",1. Introduction,[0],[0]
"By constructing a surrogate model tailored to the structure, we can reduce the number of evaluations of commonly used acquisition functions.",1. Introduction,[0],[0]
"The same structure also allows us to take acquisition decisions more efficiently, thus speeding up the search of candidates.
",1. Introduction,[0],[0]
Tree-based dependencies occur often in practice.,1. Introduction,[0],[0]
"For exam-
ple, faced with a classification problem, we may want to simultaneously search over many different machine learning models, each coming with their own hyperparameters.",1. Introduction,[0],[0]
"Some configurations may also share parameters (e.g., logistic regression with `2 and `1 penalty may share the learning rate).",1. Introduction,[0],[0]
"These choices could be encoded in a decision tree, where inner nodes select between different models and hyperparameters populate leaf nodes.",1. Introduction,[0],[0]
"Another example arises when having to decide on a deep neural network architecture: the size of a layer, choice of activation function, or dropout fraction may depend on the number of layers (Bengio, 2009).",1. Introduction,[0],[0]
"A baseline approach to Bayesian optimization in this setting is to ignore the structure of X and, as a result, choose a GP with covariance kernel K(x,x′) defined over the joint input space.",1.1. Baselines and Related Work,[0],[0]
"When comparing a pair of points, all coordinates are taken into account.",1.1. Baselines and Related Work,[0],[0]
"While easy to run in existing Bayesian optimization toolboxes, this approach can be highly inefficient.",1.1. Baselines and Related Work,[0],[0]
"Not only do we encounter a cost of O(n3) after n acquisitions due to the global nature of the GP, but we also suffer from the curse of dimensionality when searching over X .",1.1. Baselines and Related Work,[0],[0]
"Several authors attempted to design covariance functions that are aware of the structure: Duvenaud et al. (2011) consider kernels with an additive structure, while Swersky et al. (2014a); Hutter & Osborne (2013) introduce the Arc-kernel.",1.1. Baselines and Related Work,[0],[0]
"However, the cost remains O(n3).
",1.1. Baselines and Related Work,[0],[0]
"Another idea is to consider an independent GP for every valid subset of hyperparameters, as proposed by Bergstra et al. (2011).",1.1. Baselines and Related Work,[0],[0]
This approach corresponds to having an independent GP per leaf in the dependency tree.,1.1. Baselines and Related Work,[0],[0]
"It scales as O( ∑ p n 3 p), where np is the number of evaluations at leaf
node p and ∑ p np = n.",1.1. Baselines and Related Work,[0],[0]
"However, it lacks a mechanism for information sharing across the leaves.",1.1. Baselines and Related Work,[0],[0]
"As we will show, information sharing can be beneficial in order to cut down on the number of evaluations.",1.1. Baselines and Related Work,[0],[0]
"Moreover, the independent approach requires a sizable number of evaluations at each leaf, which can be problematic when there are many leafs.
",1.1. Baselines and Related Work,[0],[0]
"Tree-structured dependencies can also be dealt with by assigning default values to coordinates of x which do not fall into the leaf node under consideration, using a Random Forest model to make this choice (Hutter et al., 2011).",1.1. Baselines and Related Work,[0],[0]
"This strategy is implemented in the SMAC library.
",1.1. Baselines and Related Work,[0],[0]
"Finally, Zhang et al. (2016) proposed a dedicated approach to tune data analytic pipelines, via a two-layer Bayesian optimization framework.",1.1. Baselines and Related Work,[0],[0]
"Their method first uses a parametric model to select some promising algorithms, whose hyperparameters are then refined by a nonparametric model.",1.1. Baselines and Related Work,[0],[0]
"First, we introduce a novel Bayesian optimization methodology able to leverage conditional dependencies between hyperparameters.",1.2. Contributions,[0],[0]
"To this end, we build a tree-structured surrogate model, with separate GPs at the leaf nodes, and random linear (or constant) functions at the inner nodes.",1.2. Contributions,[0],[0]
"This allows us to transfer information between leafs that share nodes on their respective paths, which enables us in turn to efficiently search the space X .",1.2. Contributions,[0],[0]
"Yet, we also retain the beneficial scaling of the independent approach (Bergstra et al., 2011).",1.2. Contributions,[0],[0]
"To our knowledge, no prior published work satisfied these two aspects.",1.2. Contributions,[0],[0]
"The Arc-kernel allows for information sharing, but comes with O(n3) computations.",1.2. Contributions,[0],[0]
"Hutter et al. (2011) rely on Random Forests to represent correlations, but no particular sharing mechanism exists.
",1.2. Contributions,[0],[0]
"Second, we introduce a novel acquisition function which is also able to exploit the tree structure and relies on the expected improvement (Mockus et al., 1978).",1.2. Contributions,[0],[0]
The acquisition operates in two steps.,1.2. Contributions,[0],[0]
"We first select the most promising leaf node to score, effectively restricting our attention to a portion of X .",1.2. Contributions,[0],[0]
We then optimize over all possible anchor points in this portion of space.,1.2. Contributions,[0],[0]
This can result in a drastic reduction in the number of surrogate functions to optimize over.,1.2. Contributions,[0],[0]
"In comparison, the independent baseline requires to score every anchor point of every leaf in the tree at each iteration.
",1.2. Contributions,[0],[0]
The paper is organized as follows.,1.2. Contributions,[0],[0]
"In Section 2, we detail our surrogate GP model and inference computations.",1.2. Contributions,[0],[0]
"In Section 3, we show how the model structure gives rise to efficient acquisition optimization.",1.2. Contributions,[0],[0]
"For a range of experiments on simulated and real data, we report in Section 4 favorable comparisons with existing alternatives.",1.2. Contributions,[0],[0]
We conclude with possible extensions in Section 5.,1.2. Contributions,[0],[0]
"We assume that the hyperparameters exhibit conditional dependencies, which can be modeled with decision tree T .",2. Tree-structured semi-parametric Gaussian process regression model,[0],[0]
"The set of inner nodes V is indexed by v ∈ {1, . . .",2. Tree-structured semi-parametric Gaussian process regression model,[0],[0]
", V }; each v has a decision variable and a weight variable cv.",2. Tree-structured semi-parametric Gaussian process regression model,[0],[0]
"The set of leaf nodes P is indexed by p ∈ {1, . . .",2. Tree-structured semi-parametric Gaussian process regression model,[0],[0]
", P}.",2. Tree-structured semi-parametric Gaussian process regression model,[0],[0]
"Equivalently, p indexes (unique) paths from the root to a leaf.
",2. Tree-structured semi-parametric Gaussian process regression model,[0],[0]
"Further, let Dn = {(xi, yi)}ni=1 be the set of observations.",2. Tree-structured semi-parametric Gaussian process regression model,[0],[0]
We introduce a set of auxiliary variables {pi | pi ∈ P}ni=1 that indicate the leaf to which observation i is associated and let np= |{i | pi=p}|.,2. Tree-structured semi-parametric Gaussian process regression model,[0],[0]
Note that xi∈Xpi since the input domain may vary from one leaf to another.,2. Tree-structured semi-parametric Gaussian process regression model,[0],[0]
"We consider a surrogate model that associates with leaf p a latent function gp with GP prior, whose mean function and covariance kernel are bp and Kp(x,x′).",2.1. Model with Random Inner Node Parameters,[0],[0]
We impose a zero-mean Gaussian prior over the weight vector c =,2.1. Model with Random Inner Node Parameters,[0],[0]
"[c1, . .",2.1. Model with Random Inner Node Parameters,[0],[0]
.,2.1. Model with Random Inner Node Parameters,[0],[0]
", cV ] T .
",2.1. Model with Random Inner Node Parameters,[0],[0]
"The resulting generative model is given by
c ∼ N (0,Σc), gp(·) ∼ GP ( bp,Kp ) ,
yi|pi, {gp(xi)}Pp=1, c ∼ N (gpi(xi) + z>pic, σ 2).",2.1. Model with Random Inner Node Parameters,[0],[0]
"(1)
where Σc, {bp}Pp=1 and σ2 are the prior covariance, the scalar offsets and the noise variance.",2.1. Model with Random Inner Node Parameters,[0],[0]
"Vector zp∈ {0, 1}V is a binary mask that activates the weights of the inner node decision variables on the path to leaf node p.",2.1. Model with Random Inner Node Parameters,[0],[0]
"In other words, (zp)v = 1 iff v lies on the path from the root to p.",2.1. Model with Random Inner Node Parameters,[0],[0]
"The prior covariance Σc will be diagonal in our experiments.
",2.1. Model with Random Inner Node Parameters,[0],[0]
"When c=0, model (1) boils down to assuming P independent GPs.",2.1. Model with Random Inner Node Parameters,[0],[0]
"While inference only scales as O( ∑ p n 3 p) in this case, information is not transferred between overlapping paths.",2.1. Model with Random Inner Node Parameters,[0],[0]
"Introducing the weight vector c allows us to couple inference for such paths, while keeping the favorable scaling and better exploring the optimization space (see Section 4).
",2.1. Model with Random Inner Node Parameters,[0],[0]
"Next, we show how to perform efficient inference in this model and give an interpretation of the induced kernel when computing the marginal likelihood.",2.1. Model with Random Inner Node Parameters,[0],[0]
Posterior inference over the surrogate models {gp(·)} and the random weights c is needed to compute the acquisition functions (see Section 3).,2.1. Model with Random Inner Node Parameters,[0],[0]
"Before starting, we need some notation.",2.2. Posterior Inference,[0],[0]
Let y ∈,2.2. Posterior Inference,[0],[0]
Rn be the vector of all observations and g ∈,2.2. Posterior Inference,[0],[0]
Rn the vector of latent function values at {xi}ni=1.,2.2. Posterior Inference,[0],[0]
"Further, let Ip = {i | pi = p}, noting that np = |Ip|.",2.2. Posterior Inference,[0],[0]
"We partition the data accordingly, so that yp =",2.2. Posterior Inference,[0],[0]
"[yi]i∈Ip , and similarly gp = [gp(xi)]i∈Ip .",2.2. Posterior Inference,[0],[0]
"Also, we define the matrix Zp = zp1>np ∈ R
V×np , where 1np =",2.2. Posterior Inference,[0],[0]
"[1] ∈ Rnp , and the vector bp = bp1np ∈ Rnp .
",2.2. Posterior Inference,[0],[0]
"The joint distribution P (y,g, c) of our model is given by P (c) ∏ pN (gp; bp,Kp)N (yp; gp + Z > p c, σ 2Inp), (2)
where Kp = [Kp(xi,xj)]i,j∈Ip are kernel matrices, with the prior P (c) = N (c; 0,Σc).",2.2. Posterior Inference,[0],[0]
"Our goal is to obtain the posterior process P (gp(·)|c,yp) and the posterior distribution P (c|y).
",2.2. Posterior Inference,[0],[0]
"We can directly read off the posterior over the latent functions and parameters after rewriting the joint distribution into the following form (see Section 2 of the Appendix for details):
P (y)P (c|y)",2.2. Posterior Inference,[0],[0]
"∏ pP (gp|c,yp).
",2.2. Posterior Inference,[0],[0]
"First, we obtain the posterior GP over the latent functions:
gp(·)|c,yp ∼ GP ( mp(·), Sp(·, ·) ) ,
where mp(x) =",2.2. Posterior Inference,[0],[0]
kp(x)>M−1p (yp − Z>p c,2.2. Posterior Inference,[0],[0]
"− bp) + bp, Sp(x,x
′)",2.2. Posterior Inference,[0],[0]
"= Kp(x,x′)",2.2. Posterior Inference,[0],[0]
"− kp(x)>M−1p kp(x′) and Mp = Kp + σ 2Inp .
",2.2. Posterior Inference,[0],[0]
"Next, we obtain the posterior for the weights c:
c|y",2.2. Posterior Inference,[0],[0]
"∼ N (Λ−1c fc,Λ −1 c ),
where fc = ∑ p ZpM −1",2.2. Posterior Inference,[0],[0]
p (yp − bp) and Λc = Σ −1,2.2. Posterior Inference,[0],[0]
"c +∑
p ZpM −1",2.2. Posterior Inference,[0],[0]
"p Z > p .
",2.2. Posterior Inference,[0],[0]
"In the sequel, we compute expressions such as M−1p and log |Mp| by using the Cholesky decomposition Mp = LpL > p .",2.2. Posterior Inference,[0],[0]
"Similarly, the expressions depending on Λc are computed using its Cholesky decomposition.",2.2. Posterior Inference,[0],[0]
"As shown in Section 2 of the Appendix, we can derive the expression for the log-marginal likelihood logP (y) in closed form:
logP (y) = ∑ p logN (yp; Z>p c + bp,Mp)
+ logN (c;",2.3. Marginal Likelihood and Its Interpretation,[0],[0]
"0,Σc)− logP (c|y), (3)
",2.3. Marginal Likelihood and Its Interpretation,[0],[0]
"The p-dependent terms require computing the Cholesky decompositions of all Mp ∈ Rnp×np , whereas the final term needs the Cholesky decomposition of Λc ∈ RV×V .",2.3. Marginal Likelihood and Its Interpretation,[0],[0]
"Therefore, logP (y) can be computed in O(V 3 + ∑ p n 3 p).",2.3. Marginal Likelihood and Its Interpretation,[0],[0]
"Note that this computation is required for optimizing the hyperparameters of the GPs.
",2.3. Marginal Likelihood and Its Interpretation,[0],[0]
We can also obtain an interesting interpretation for the induced kernel of the marginal likelihood by computing it in a different way.,2.3. Marginal Likelihood and Its Interpretation,[0],[0]
Let Z =,2.3. Marginal Likelihood and Its Interpretation,[0],[0]
"[Zp] ∈ RV×n, b =",2.3. Marginal Likelihood and Its Interpretation,[0],[0]
[bp] ∈,2.3. Marginal Likelihood and Its Interpretation,[0],[0]
Rn and Kblock ∈ Rn×n the block-diagonal matrix with blocks Kp.,2.3. Marginal Likelihood and Its Interpretation,[0],[0]
"With these notations, it can be shown that P (yp|c) = N (yp|Z>p c + bp,Mp).",2.3. Marginal Likelihood and Its Interpretation,[0],[0]
"Integrating out c leads to
P (y) = N",2.3. Marginal Likelihood and Its Interpretation,[0],[0]
"( b,Z>ΣcZ + K block + σ2I ) .",2.3. Marginal Likelihood and Its Interpretation,[0],[0]
"(4)
If we further assume that Σc = σ2cIV , then
Z>ΣcZ = [ σ2cZ > p Zp′ ]",2.3. Marginal Likelihood and Its Interpretation,[0],[0]
"p,p′ = [ σ2c (z > p zp′)1np1 > np′",2.3. Marginal Likelihood and Its Interpretation,[0],[0]
],2.3. Marginal Likelihood and Its Interpretation,[0],[0]
"p,p′ .
",2.3. Marginal Likelihood and Its Interpretation,[0],[0]
"Hence, the diagonal blocks are proportional to z>p zp, which is the length of path p, and the off-diagonal blocks are proportional to z>p zp′ , which is the path overlap length between p and p′. The resulting kernel is thus the intersection kernel (see Shawe-Taylor & Cristianini (2004), Section 9.5).",2.3. Marginal Likelihood and Its Interpretation,[0],[0]
We have so far associated a random scalar cv with each inner node.,2.4. Model with Random Linear Inner Node Functions,[0],[0]
"More generally, we can use linear functions c>v rv, where cv is a weight vector and rv∈Rdv is a feature vector.",2.4. Model with Random Linear Inner Node Functions,[0],[0]
"The special case above is obtained with dv=1 and rv=[1].
",2.4. Model with Random Linear Inner Node Functions,[0],[0]
We collect the weight vectors in c =,2.4. Model with Random Linear Inner Node Functions,[0],[0]
"[cv] ∈ Rd, where d = ∑ v dv.",2.4. Model with Random Linear Inner Node Functions,[0],[0]
Let Vp ⊆ V be the set of inner nodes on the path from the root to leaf p.,2.4. Model with Random Linear Inner Node Functions,[0],[0]
"Concatenating the rv’s, we define the induced feature vector zp =",2.4. Model with Random Linear Inner Node Functions,[0],[0]
"[rv]v∈Vp such that
c>zp = ∑ v∈Vp c > v rv.
",2.4. Model with Random Linear Inner Node Functions,[0],[0]
"Hence, the dataset we collect during the optimization is now the extended set Dn = {(xi, yi, pi, zpi =[rv,i]v∈Vpi )} n i=1.",2.4. Model with Random Linear Inner Node Functions,[0],[0]
"It is easy to see that all our results above transfer to this more general case, if only we redefine
Zp =",2.4. Model with Random Linear Inner Node Functions,[0],[0]
[zpi ]i∈Ip =,2.4. Model with Random Linear Inner Node Functions,[0],[0]
"[rv,i]i∈Ip,v∈Vpi ∈ R d×np .
",2.4. Model with Random Linear Inner Node Functions,[0],[0]
"Except for an increased dimensionality d > V of the weight vector c, the extension with random linear inner node functions is not more difficult to implement or run.
",2.4. Model with Random Linear Inner Node Functions,[0],[0]
"In Section 4, we will use rv to encode both numerical (i.e., dv = 1) and categorical parameters (via one-hot representations, so that dv equals the number of categories).",2.4. Model with Random Linear Inner Node Functions,[0],[0]
"In our deep learning use case, parameters such as the learning rate, the number of units and the type of activation functions are encoded via the rv’s (see Figure 3, bottom).",2.4. Model with Random Linear Inner Node Functions,[0],[0]
We will refer to the parameter associated with rv as a shared parameter since it is shared across all the leaves whose paths contain v.,2.4. Model with Random Linear Inner Node Functions,[0],[0]
"Bayesian optimization generally proceeds by discretizing the search space X into a set of anchor points, for example by using quasi-random sequences (Sobol, 1967).",3. Acquisition Functions,[0],[0]
"We then maximize an acquisition function starting from the most promising anchor point(s), typically with a numerical solver like L-BFGS (Nocedal & Wright, 2006).",3. Acquisition Functions,[0],[0]
Acquisition functions are defined in terms of expectations over the surrogate model posterior.,3. Acquisition Functions,[0],[0]
"Frequently used choices include Thompson sampling (Thompson, 1933), probability of improvement (PI) (Kushner, 1964), expected improvement (EI) (Mockus et al., 1978), or GP-UCB (Srinivas et al., 2010).",3. Acquisition Functions,[0],[0]
We will focus on EI in the sequel as it has been shown to perform better than PI.,3. Acquisition Functions,[0],[0]
"Our initial experiments also showed that Thompson sampling was not performing well.
",3. Acquisition Functions,[0],[0]
The naive approach of globally optimizing EI over anchor points does not scale well with a high-dimensional X .,3. Acquisition Functions,[0],[0]
"In the previous section, we specified a tree-structured model for the (random) surrogate function, with which the evaluation of an acquisition function at some x ∈ X is sped up.",3. Acquisition Functions,[0],[0]
"In this section, we show how the model structure can also be exploited in order to speed up the optimization itself.",3. Acquisition Functions,[0],[0]
The acquisition function α(x|Dn) plays a critical role in Bayesian Optimization as it selects anchor points by performing an exploration-exploitation trade-off.,3.1. Acquisition Strategies,[0],[0]
The key question that concerns us is whether we can leverage the explicit structure in high-dimensional structured space in order to make the search more efficient.,3.1. Acquisition Strategies,[0],[0]
"The naive approach ignores structure in the search space, using a surrogate model based on a global kernel, like the one proposed by Swersky et al. (2014a).",3.1. Acquisition Strategies,[0],[0]
"While the design of a kernel that incorporate structure is non-trivial, it is not explicitly used to guide the search and the cost of evaluations still scales as O(n3).
",3.1. Acquisition Strategies,[0],[0]
"As noted above, we can speed up evaluations to O( ∑ p n 3 p) by adopting an independent model, which corresponds to our tree model with c = 0, so that the surrogate models {gp(·)}Pp=1 can be learnt and queried independently from each other.",3.1. Acquisition Strategies,[0],[0]
"With this approach, the search decouples across the leaf nodes and can be parallelized accordingly.",3.1. Acquisition Strategies,[0],[0]
"However, if acquisitions are done sequentially, then all leafs have to be searched in order to find the overall best candidate.",3.1. Acquisition Strategies,[0],[0]
"The independent model also fails to represent dependencies between the leaf nodes, so that a larger total number of evaluations may be required to reach a good solution.
",3.1. Acquisition Strategies,[0],[0]
"Given our tree-structured surrogate model, we can improve on both the naive and the independent approach.",3.1. Acquisition Strategies,[0],[0]
"The acquisition function becomes α(x, p|Dn), p being the leaf node where x is evaluated.",3.1. Acquisition Strategies,[0],[0]
"For our model, α(x, p|Dn) can be evaluated in O(V 3 + ∑ p n 3 p), which is often much cheaper than O(n3) required in the naive approach, and is comparable to O( ∑ p n 3 p) for independent.",3.1. Acquisition Strategies,[0],[0]
"We could maximize α(x, p|Dn) separately at each leaf p, and then pick the best candidate across leaf nodes:
(x?, p?) ∈ arg max p∈P,x∈Xp α(x, p|Dn).
",3.1. Acquisition Strategies,[0],[0]
"In practice, the set of leaf nodes P can become large, in which case the requirement to search in every leaf node can be costly.",3.1. Acquisition Strategies,[0],[0]
We propose to further exploit the tree structure of our surrogate model in order to speed up the optimization.,3.1. Acquisition Strategies,[0],[0]
"Namely, our model implies a path acquisition function α(p|Dn).",3.1. Acquisition Strategies,[0],[0]
"Based on this, we select p? and x?",3.1. Acquisition Strategies,[0],[0]
"in two steps:
p? = arg max p∈P α(p|Dn), x? ∈ arg max x∈Xp? α(x, p?|Dn).
",3.1. Acquisition Strategies,[0],[0]
This strategy can greatly speed up the optimization.,3.1. Acquisition Strategies,[0],[0]
"There are obvious intermediates, such as searching in a subset of top-ranked leafs p, which we defer for future work.",3.1. Acquisition Strategies,[0],[0]
"Given our surrogate model, the EI acquisition function is: α(x, p|Dn) = E {",3.2. Two-step Expected Improvement,[0],[0]
[ymin − gp(x)−,3.2. Two-step Expected Improvement,[0],[0]
"z>p c]+ } , (5)
Table 1.",3.2. Two-step Expected Improvement,[0],[0]
Comparison of different surrogate models and acquisition strategies (see text for details).,3.2. Two-step Expected Improvement,[0],[0]
"Here,M(X ) is the complexity of optimizing a surrogate function over the space X .",3.2. Two-step Expected Improvement,[0],[0]
"p? is the path selected by tree, and Xp? is the corresponding leaf domain.
",3.2. Two-step Expected Improvement,[0],[0]
sharing?,3.2. Two-step Expected Improvement,[0],[0]
"complexity independent × O
(∑
p n 3 p · M(Xp)
)
",3.2. Two-step Expected Improvement,[0],[0]
"naive X O ( ( ∑
p np) 3 · M
( P× ∏ pXp ))",3.2. Two-step Expected Improvement,[0],[0]
tree X O ( V 3 + n3p? ·,3.2. Two-step Expected Improvement,[0],[0]
"M(Xp?)
)",3.2. Two-step Expected Improvement,[0],[0]
"where [u]+ = max{u, 0}, and ymin is the best evaluation so far (across all leafs).",3.2. Two-step Expected Improvement,[0],[0]
The expectation is computed with respect to the posterior of gp(x) + z,3.2. Two-step Expected Improvement,[0],[0]
">p c, which is a GP with mean and covariance functions respectively given by
m̃p(x) = kp(x) >",3.2. Two-step Expected Improvement,[0],[0]
M−1p (yp − bp) + tp(x)>Λ −1,3.2. Two-step Expected Improvement,[0],[0]
"c fc + bp,
S̃p(x,x ′)",3.2. Two-step Expected Improvement,[0],[0]
=,3.2. Two-step Expected Improvement,[0],[0]
"Sp(x,x ′) + tp(x) >Λ−1c tp(x ′),
where tp(x) = zp",3.2. Two-step Expected Improvement,[0],[0]
− ZpM−1p kp(x).,3.2. Two-step Expected Improvement,[0],[0]
"We can analytically compute (5), leading to
α(x, p|Dn) = σ̃p(x) (ξΦ(ξ) +N (ξ; 0, 1)) ,
where σ̃p(x) = {S̃p(x,x)}1/2, ξ = m̃p(x)−yminσ̃p(x) and Φ(ξ) is the CDF of a standard Gaussian.
",3.2. Two-step Expected Improvement,[0],[0]
"As noted above, we could optimize α(x, p|Dn) at all leaves and pick the overall winner.",3.2. Two-step Expected Improvement,[0],[0]
"Instead, we propose a two-step approach, based on a path EI acquisition function:
α(p|Dn) = E {",3.2. Two-step Expected Improvement,[0],[0]
"[ymin − bp − z>p c]+ } , (6)
where the expectation is taken with respect to z",3.2. Two-step Expected Improvement,[0],[0]
>p c + bp ∼ N,3.2. Two-step Expected Improvement,[0],[0]
(z>p Λ −1,3.2. Two-step Expected Improvement,[0],[0]
"c fc + bp, z > p Λ −1 c zp).",3.2. Two-step Expected Improvement,[0],[0]
"We first select the path p? = arg maxp α(p|Dn), then find x? by maximizing α(x, p?|Dn) at leaf p?",3.2. Two-step Expected Improvement,[0],[0]
only.,3.2. Two-step Expected Improvement,[0],[0]
Our tree acquisition strategy is related to the naive and independent ones in Table 1.,3.2. Two-step Expected Improvement,[0],[0]
"Interestingly, tree can be faster than independent overall.",3.2. Two-step Expected Improvement,[0],[0]
"Finally, (6) is easily extended to the case where we have random linear functions at the inner nodes by considering the augmented induced variable zp =",3.2. Two-step Expected Improvement,[0],[0]
[rv]v∈Vp (see Section 2.4 for details).,3.2. Two-step Expected Improvement,[0],[0]
"In particular, the resulting optimization of (6) is carried out jointly over p and zp =",3.2. Two-step Expected Improvement,[0],[0]
[rv]v∈Vp .,3.2. Two-step Expected Improvement,[0],[0]
"In this section, we conduct two sets of experiments.",4. Experiments,[0],[0]
"First, we focus on optimizing synthetic functions designed to have tree-structured conditional relationships.",4. Experiments,[0],[0]
"We then consider the tuning of a multi-layer perceptron for binary classification, which we evaluate over a large number of datasets.
",4. Experiments,[0],[0]
"Throughout the experiments, we use the following acronyms to refer to the different competing methods: tree is our proposed approach, independent is a baseline that consider an independent GP for every leaf, arc corresponds
to (Swersky et al., 2014a), smac refers to (Hutter et al., 2011) and gp-baseline is a standard GP-based Bayesian optimization solver taken from (GPyOpt, 2016).",4. Experiments,[0],[0]
"For tree, independent and gp-baseline, we use 5/2 Matérn kernels.",4. Experiments,[0],[0]
"marginal is another baseline obtained by replacing the kernel of gp-baseline by that stemming from the marginal (4), where c is viewed as a nuisance variable and integrated out.",4. Experiments,[0],[0]
"Finally, random is standard random search (Bergstra & Bengio, 2012).
",4. Experiments,[0],[0]
"Unless otherwise specified, all the results displayed in this section correspond to the means and twice the standard errors computed over 25 random replications.",4. Experiments,[0],[0]
"Also, in order to minimize the initialization bias, all methods (except smac1) start from the same set of random candidates; there is one random candidate drawn per conditional path.",4. Experiments,[0],[0]
Our implementation is in Python and we ran the experiments on a fleet of Amazon AWS c4.8xlarge machines.,4. Experiments,[0],[0]
"The functions we consider are defined over binary trees: Each inner node, including the root, corresponds to a binary variable.",4.1. Synthetic Tree-structured Functions,[0],[0]
A path in this tree thus represents successive binary decisions.,4.1. Synthetic Tree-structured Functions,[0],[0]
The leaves contain univariate quadratic functions that are shifted by different constant terms.,4.1. Synthetic Tree-structured Functions,[0],[0]
We give an example of such a function in Figure 1.,4.1. Synthetic Tree-structured Functions,[0],[0]
"In the sequel, we study the two functions from Figure 1 (referred to as small balanced), in addition to a higher-dimensional version of those, with a depth of 4 and 8 leaves whose constant shifts are {a× 0.1}8a=1 (referred to as large balanced).",4.1. Synthetic Tree-structured Functions,[0],[0]
"In the supplementary material, we provide further results based
1We use https://github.com/sfalkner/pySMAC.",4.1. Synthetic Tree-structured Functions,[0],[0]
"To the best of our knowledge, we cannot specify the starting point.
on unbalanced binary trees of increasing sizes, for which similar conclusions hold.",4.1. Synthetic Tree-structured Functions,[0],[0]
"All the non-shared continuous variables xj’s are defined in [−1, 1], while the shared ones are in [0, 1].",4.1. Synthetic Tree-structured Functions,[0],[0]
"The best function value will thus always be 0.1.
",4.1. Synthetic Tree-structured Functions,[0],[0]
"Those functions encode conditional relationships since given a path p and its leaf `p, all the binary variables outside of the path p and all the continuous variables defined in the leaves `′ 6=",4.1. Synthetic Tree-structured Functions,[0],[0]
`p are irrelevant.,4.1. Synthetic Tree-structured Functions,[0],[0]
We report in Figure 2 the optimization results for the different competing methods.,4.1. Synthetic Tree-structured Functions,[0],[0]
"We make the following observations:
Approaches blind to structure perform poorly: The results show that, both gp-baseline and random, which cannot use the conditional structure, do not fare well.",4.1. Synthetic Tree-structured Functions,[0],[0]
"As expected, the performance gap widens as the trees get deeper.
",4.1. Synthetic Tree-structured Functions,[0],[0]
"Independent vs. tree vs. arc: independent, tree and arc represent 3 ways of increasingly incorporating conditional structure.",4.1. Synthetic Tree-structured Functions,[0],[0]
"Indeed, independent takes into account the tree structure but does not allow for any sharing of information across different paths, arc defines a joint kernel over the union of all the leaves, while tree makes intermediate modeling assumptions.",4.1. Synthetic Tree-structured Functions,[0],[0]
"We can observe that, thanks to its joint nature, arc tends to perform well initially, but it is quickly overtaken by tree and later also by independent that lags behind because of the absence of sharing, but catches up once sufficient observations were collected.",4.1. Synthetic Tree-structured Functions,[0],[0]
"Also, as the dimension of the optimization space gets larger, the performance of independent worsens, while that of tree is barely affected (we do observe the same scalability with respect to the dimension on unbalanced binary trees, as reported in the supplementary material).",4.1. Synthetic Tree-structured Functions,[0],[0]
"At this juncture, we would also like to emphasize that while independent catches up with tree in some cases, it is more wasteful of resources as it requires to score every leaf at each iteration unlike tree (see also Table 1).
",4.1. Synthetic Tree-structured Functions,[0],[0]
"Importance of exploiting the latent variables c: It is interesting to observe that marginal, which considers c to be a nuisance variable and integrates it out, performs significantly worse than tree.",4.1. Synthetic Tree-structured Functions,[0],[0]
"Note that marginal cannot de facto be applied in presence of shared variables, which explains why it does not appear in the right panels of Figure 1.
",4.1. Synthetic Tree-structured Functions,[0],[0]
"Approach not based on GPs: smac is known to be stateof-the-art for optimization tasks in presence of conditional relationships (Eggensperger et al., 2013).",4.1. Synthetic Tree-structured Functions,[0],[0]
"In particular, it is known to work better than GP-based approaches, especially when the dimension gets large.",4.1. Synthetic Tree-structured Functions,[0],[0]
"We observe in our experiments (e.g., for large balanced, 7 categorical and 10 continuous parameters, 2 of which being shared) that smac does not reach good solutions on these synthetic tasks.",4.1. Synthetic Tree-structured Functions,[0],[0]
We now focus our attention on the tuning of a multilayer perceptron (MLP) for binary classification.,4.2. Multi-layer Perceptron Tuning,[0],[0]
The setting we consider is reminiscent of that proposed by Swersky et al. (2014a).,4.2. Multi-layer Perceptron Tuning,[0],[0]
"We optimize for the number of hidden layers in {0, 1, 2, 3, 4}, the number of units per layer in {1, 2, . . .",4.2. Multi-layer Perceptron Tuning,[0],[0]
", 30} (provided the corresponding layer is activated), the choice of the activation function in {identity, logistic, tanh, relu}, which we constrain to be identical across all layers, the amount of `2 regularization in [10−6, 10−1], the learning rate in [10−5, 10−1] of the underlying Adam solver (Kingma & Ba, 2014), the tolerance in [10−5, 10−2] of the solver (based on relative decrease), and the type of data pre-processing, which can be unit `2-norm observation-wise normalization, `∞-norm feature-wise normalization, mean/standarddeviation feature-wise whitening or no normalization at all.
",4.2. Multi-layer Perceptron Tuning,[0],[0]
"The optimization task can be specified in various ways, resulting in different topologies for the trees of conditional relationships.",4.2. Multi-layer Perceptron Tuning,[0],[0]
We consider the two instantiations of conditional relationships illustrated in Figure 3.,4.2. Multi-layer Perceptron Tuning,[0],[0]
"The first one has all the variables duplicated (top tree), which is similar to how independent proceeds.",4.2. Multi-layer Perceptron Tuning,[0],[0]
The second one consists in having most of the variables shared (bottom tree).,4.2. Multi-layer Perceptron Tuning,[0],[0]
"Note that in the two settings, we have one regularization parameter λk per number k of hidden layer(s) of the network.",4.2. Multi-layer Perceptron Tuning,[0],[0]
"We do
so to account for the fact that the λk’s regularize matrices of different dimensions.",4.2. Multi-layer Perceptron Tuning,[0],[0]
"In between those two extreme settings, we could consider intermediate modeling assumptions (e.g., a learning rate ηlinear for the case with no hidden layers and a shared learning rate ηnon-linear otherwise).
",4.2. Multi-layer Perceptron Tuning,[0],[0]
"To provide a robust evaluation of the different competing methods, we consider a subset of the datasets from the Libsvm repository (Chang & Lin, 2011).",4.2. Multi-layer Perceptron Tuning,[0],[0]
"More specifically, we consider all the datasets whose number of features is smaller than 106, which results in 45 data sets.",4.2. Multi-layer Perceptron Tuning,[0],[0]
"In absence of pre-defined default train-test split, we took a random 80%−20% split.",4.2. Multi-layer Perceptron Tuning,[0],[0]
"To limit the overall computational burden, we cap the training and test set sizes to a maximum of respectively 103 and 104 instances (randomly selected when the subsampling applies).",4.2. Multi-layer Perceptron Tuning,[0],[0]
"Note that this subsampling step is not related to a computational limitation of our approach, but is a practical consideration only modifying the properties of the black-box function we optimize.",4.2. Multi-layer Perceptron Tuning,[0],[0]
"We use the MLP implementation of scikit-learn (Pedregosa et al., 2011)
and we add a CPU-time constraint of 5 minutes to each evaluation, beyond which the worst classification error 1.0 is returned.",4.2. Multi-layer Perceptron Tuning,[0],[0]
"Under this constraint, the total computational time of the experiment was roughly 100 CPU days.
",4.2. Multi-layer Perceptron Tuning,[0],[0]
We run all the methods for 85 iterations and initialize them with one random choice for each of the 5 conditional paths.,4.2. Multi-layer Perceptron Tuning,[0],[0]
We aggregate the average classification errors per dataset by displaying the average rank of each method as a function of the number of iterations.,4.2. Multi-layer Perceptron Tuning,[0],[0]
"We say that the rank of a method is equal to i if it performs the ith best (see, e.g., Bardenet et al. (2013); Feurer et al. (2015)).",4.2. Multi-layer Perceptron Tuning,[0],[0]
"We can draw the following conclusions:
Effect of z>p c without shared variables: The top panel in Figure 4 compares independent with tree-based method when it is defined on the independent topology shown in Figure 3(top).",4.2. Multi-layer Perceptron Tuning,[0],[0]
"Since there are no shared variables in the inner nodes, the sharing mechanism of tree only
happens via the term z>p c",4.2. Multi-layer Perceptron Tuning,[0],[0]
which contributes to the mean.,4.2. Multi-layer Perceptron Tuning,[0],[0]
"As expected, sharing results in tree makes faster progress towards the optimum.",4.2. Multi-layer Perceptron Tuning,[0],[0]
"However, when more observations are collected, independent outperforms tree because it better explores all the leafs (though, at a higher computational cost; see Table 1).",4.2. Multi-layer Perceptron Tuning,[0],[0]
"We next show how we can additionally benefit from sharing parameters at inner nodes.
",4.2. Multi-layer Perceptron Tuning,[0],[0]
Shared topology: The lower panel in Figure 4 compares all the methods using the shared topology shown in Figure 3(bottom).,4.2. Multi-layer Perceptron Tuning,[0],[0]
"We found that arc, gp-baseline, random and smac all benefitted from running with the shared topology.",4.2. Multi-layer Perceptron Tuning,[0],[0]
"The results show that tree not only greatly improves upon all other GP-based approaches, but also converges faster than smac that finally reaches the same level of performance after about 75 iterations.",4.2. Multi-layer Perceptron Tuning,[0],[0]
"We can observe that a standard GP-based technique that is blind to the conditional structure, like gp-baseline, performs poorly.",4.2. Multi-layer Perceptron Tuning,[0],[0]
"Of independent interest is the comparison of arc with smac, which was not reported by Swersky et al. (2014a).",4.2. Multi-layer Perceptron Tuning,[0],[0]
"Finally, it is worth emphasizing that tree obtains good results while only modeling shared variables at the inner nodes in a linear fashion.",4.2. Multi-layer Perceptron Tuning,[0],[0]
"This conclusion is in agreement with the recent observations from (Zhang et al., 2016) where linear models lead to good results in the context of the optimization of data analytic pipelines.",4.2. Multi-layer Perceptron Tuning,[0],[0]
"Next, we discuss an extension to model the shared variables non-linearly.",4.2. Multi-layer Perceptron Tuning,[0],[0]
The approach we have introduced in Section 2.4 can easily be extended to account for non-linearities through the use of basis expansions.,4.3. Nonlinear Extensions,[0],[0]
"More specifically, we focus on the use of random Fourier features (Rahimi et al., 2007) that proved successful for large-scale kernel methods (Lu et al., 2014).",4.3. Nonlinear Extensions,[0],[0]
"Combining basis expansion with linear models for Bayesian optimization is by no means new (see (Shahriari et al., 2016) and references therein).",4.3. Nonlinear Extensions,[0],[0]
"We also follow this methodology since it naturally fits our proposed semi-parametric model.
",4.3. Nonlinear Extensions,[0],[0]
"In the supplementary material, we report results on synthetic tree-structured functions where the objectives at the leaves depend now quadratically on the shared variables and on the MLP tuning task.",4.3. Nonlinear Extensions,[0],[0]
"In a nutshell, on the synthetic functions with linearly-dependent shared variables, tree-nonlinear converges slower than the linear version tree, which might be due to the fact that c is of higher dimensionality.",4.3. Nonlinear Extensions,[0],[0]
"Moreover, in presence of quadratically-dependent shared variables, we observe that tree fails to model adequately the non-linearities, while tree-nonlinear, as expected, can.",4.3. Nonlinear Extensions,[0],[0]
"As for the MLP task, we notice that the non-linear extension of tree tends to perform worse than its linear counterpart.",4.3. Nonlinear Extensions,[0],[0]
"The black-box functions typically encountered in machine learning rely on incremental learning procedures, such as the application of (stochastic) gradient descent over several epochs.",5. Concluding Remarks,[0],[0]
"A recent line of work has been focusing on leveraging this property to speed up Bayesian optimization (Swersky et al., 2013; 2014b; Domhan et al., 2014; Li et al., 2016; Klein et al., 2016).",5. Concluding Remarks,[0],[0]
"In particular, Li et al. (2016) and Klein et al. (2016) have reported state-of-the-art results with methods based respectively on bandits and GPs, exploiting a dynamic subsampling of the training sets.
",5. Concluding Remarks,[0],[0]
The goal of our work is orthogonal to this idea and consists instead in efficiently encoding conditional relationships with GPs.,5. Concluding Remarks,[0],[0]
"We next outline ways of combining our work with the aforementioned subsampling idea:
Combination with Klein et al. (2016): The proposal of Klein et al. (2016) uses some contextual variable (also referred to as environmental variable) to encode the subsampling rate of the training set.",5. Concluding Remarks,[0],[0]
Let us denote it by β ∈,5. Concluding Remarks,[0],[0]
"[0, 1].",5. Concluding Remarks,[0],[0]
"Klein et al. (2016) define the following joint kernel:
K((x, β), (x′, β′))",5. Concluding Remarks,[0],[0]
"= K0(x,x′) · Kcontext(β, β′).
",5. Concluding Remarks,[0],[0]
"The optimization is then driven by a cost-normalized acquisition function maxβ′,x α(x, β = 1|Dn)/cost(x, β′) where both x and β are sought to perform well on the final task of interest where no subsampling is applied (i.e., β = 1).
",5. Concluding Remarks,[0],[0]
"Looking at our case, we could easily replace our kernel Kp by K̃p((x, β), (x′, β′)) , Kp(x,x′) · Kcontext(β, β′).",5. Concluding Remarks,[0],[0]
"To apply the two-step procedure, we could normalize (6) by a cost following a separate model (1) where the contextual variable β would be a shared variables at the root.",5. Concluding Remarks,[0],[0]
"Formally, we could consider a joint path/subsampling selection criterion:
(p?, β?)",5. Concluding Remarks,[0],[0]
"∈ arg max p∈P,β′∈[0,1]
E {",5. Concluding Remarks,[0],[0]
"[ymin − bp − zp(β = 1)>c]+ }
E { zp(β′)>ccost } , where zp(β) refers to the feature representation of the path p with context variable β.
Combination with Li et al. (2016): The approach of Li et al. (2016) is based on successive halving procedures where a pool containing initially many models is progressively refined and trimmed.",5. Concluding Remarks,[0],[0]
"The output of their theoreticallyjustifed algorithm, named hyperband, can be seen as triplets Hn , {(xi, yi, βi)}ni=1",5. Concluding Remarks,[0],[0]
"representing all the tested configurations xi along with their corresponding evaluations yi and subsampling rates βi.
",5. Concluding Remarks,[0],[0]
A natural approach to leverage hyperband is therefore to useHn to warm-start our context-aware extension with kernel K̃p while fixing β = 1.,5. Concluding Remarks,[0],[0]
"In other words, our approach would be used to refine the smart and computationallyefficient initialization provided by hyperband.",5. Concluding Remarks,[0],[0]
Bayesian optimization has been successfully used to optimize complex black-box functions whose evaluations are expensive.,abstractText,[0],[0]
"In many applications, like in deep learning and predictive analytics, the optimization domain is itself complex and structured.",abstractText,[0],[0]
"In this work, we focus on use cases where this domain exhibits a known dependency structure.",abstractText,[0],[0]
The benefit of leveraging this structure is twofold: we explore the search space more efficiently and posterior inference scales more favorably with the number of observations than Gaussian Process-based approaches published in the literature.,abstractText,[0],[0]
We introduce a novel surrogate model for Bayesian optimization which combines independent Gaussian Processes with a linear model that encodes a tree-based dependency structure and can transfer information between overlapping decision sequences.,abstractText,[0],[0]
We also design a specialized two-step acquisition function that explores the search space more effectively.,abstractText,[0],[0]
Our experiments on synthetic tree-structured objectives and on the tuning of feedforward neural networks show that our method compares favorably with competing approaches.,abstractText,[0],[0]
Bayesian Optimization with Tree-structured Dependencies,title,[0],[0]
"Probabilistic numerics (Hennig et al., 2015) proposes approaching problems of numerical analysis from the point of view of statistics.",1. Introduction,[0],[0]
"In particular, Bayesian probabilistic numerical methods approach this problem from a Bayesian point of view, and can provide posterior distributions on the solutions of numerical problems (e.g. in the case of this paper, the solution of some integral).",1. Introduction,[0],[0]
These posterior distributions represent our epistemic uncertainty about these quantities of interest.,1. Introduction,[0],[0]
"In the case of quadrature rules, the uncertainty is due to the fact that we only have a finite number of
*Equal contribution 1Department of Mathematics, Imperial College London 2Department of Statistics, University of Warwick 3The Alan Turing Institute for Data Science and AI.",1. Introduction,[0],[0]
"Correspondence to: François-Xavier Briol <f-x.briol@warwick.ac.uk>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
function evaluations and therefore are uncertaint about the value of the integral.",1. Introduction,[0],[0]
"The notion of Bayesian probabilistic numerical method was independently introduced by several authors (Larkin, 1972; Kadane & Wasilkowski, 1985; Diaconis, 1988; O’Hagan, 1992), but only recently formalised by (Cockayne et al., 2017).
",1. Introduction,[0],[0]
"Apart from the uncertainty quantification property described above, these methods have several other advantages over “classical” (i.e. non-Bayesian) numerical methods (although some of the classical and Bayesian methods coincide (Diaconis, 1988)).",1. Introduction,[0],[0]
"First of all, they allow the user to formulate all of its prior knowledge in the form of a prior, making all of the assumptions of the numerical scheme explicit.",1. Introduction,[0],[0]
"Second of all, they can allow for coherent propagation of numerical uncertainties through chains of computation; see (Cockayne et al., 2017; Oates et al., 2017a).
",1. Introduction,[0],[0]
"However, one property which has not been studied so far is the possibility of jointly inferring several quantities of interest.",1. Introduction,[0],[0]
"In this paper, we study the problem of numerically integrating a sequence of functions f1, . . .",1. Introduction,[0],[0]
", fD (which are correlated to one another) with respect to some probability measure Π, and hence propose to build a model for joint inference of ∫ f1dΠ, . . .",1. Introduction,[0],[0]
", ∫ fDdΠ. Such a joint model allows for better finite-sample performance, and can also lead to more refined posterior distributions on each of the individual integrals.
",1. Introduction,[0],[0]
"To tackle this problem, we extend the well-known Bayesian quadrature (O’Hagan, 1991) algorithm and study the performance of the proposed methodology from a theoretical and experimental point of view.",1. Introduction,[0],[0]
"In particular, we provide asymptotic convergence results for the marginal posterior variance on each of the integrals, both in the case of a well specified and misspecified prior.",1. Introduction,[0],[0]
"We also demonstrate the performance of our algorithm on some toy problems from the engineering literature on multi-fidelity models, and on a challenging problem from the field of computer graphics.",1. Introduction,[0],[0]
"Bayesian Quadrature Let (X ,B,Π) be a probability space and consider some function f :",2. Methodology,[0],[0]
"X → R where X ⊆ Rp, p ∈",2. Methodology,[0],[0]
N+.,2. Methodology,[0],[0]
"The classical problem of numerical
integration is concerned with approximating the integral:
Π[f ]",2. Methodology,[0],[0]
:,2. Methodology,[0],[0]
"= ∫ X f(x)Π(dx),
where we assume ∫ X f
2(x)Π(dx)",2. Methodology,[0],[0]
<,2. Methodology,[0],[0]
"∞. Under fairly general conditions on f , one can show that an optimal algorithm (in terms of worst-case integration error in some function space) takes the form of a quadrature (or cubature) rule Π̂[f ] = ∑N i=1 wif(xi) for some weights {wi}Ni=1 ∈ R and samples {xi}Ni=1 ∈ X (see (Bakhvalov, 1971)).",2. Methodology,[0],[0]
"These are also sometimes denoted in vectorised form as Π[f ] = w>f(X) where w = (w1, . . .",2. Methodology,[0],[0]
", wN )>, X =",2. Methodology,[0],[0]
"(x1, . . .",2. Methodology,[0],[0]
",xN )
> and f(X) = (f(x1), . . .",2. Methodology,[0],[0]
", f(xN ))",2. Methodology,[0],[0]
>.,2. Methodology,[0],[0]
The notation Π̂[f ] is motivated by the fact that we can see this object as an exact integral with respect to a discrete measure Π̂ = ∑N i=1,2. Methodology,[0],[0]
"wiδxi , where δxi denotes the Dirac delta measure taking value 1 at xi and 0 otherwise.",2. Methodology,[0],[0]
"Many popular numerical integration methods take this form, including Newton–Cotes rules, Gaussian quadrature, Monte Carlo methods and sparse grids.
",2. Methodology,[0],[0]
"Let (Ω,F ,P) be another probability space.",2. Methodology,[0],[0]
"Bayesian quadrature (BQ), introduced by (O’Hagan, 1991), proposes to approach the problem of numerical integration by first formulating a prior stochastic model g : X × Ω → R for the integrand f (where ∀ω ∈ Ω, g(·, ω) represents a realisation of g).",2. Methodology,[0],[0]
"This prior model is then conditioned on the vector of observations f(X) to obtain a posterior model for f , which is then pushed forward through the integral operator to give a posterior on Π[f ].
",2. Methodology,[0],[0]
"A popular choice of prior is a Gaussian Process (GP) GP(m, k) with m : X → R denoting the mean function (i.e. m(x) =",2. Methodology,[0],[0]
"Eω[g(x, ω)]), and c :",2. Methodology,[0],[0]
"X × X → R denoting the covariance function/kernel (i.e. c(x,x′) = Eω[(g(x, ω)−m(x))(g(x′, ω)−m(x′))]).",2. Methodology,[0],[0]
Let us assume that m = 0,2. Methodology,[0],[0]
(this can be done without loss of generality since the domain can be re-parametrized to be centred at 0).,2. Methodology,[0],[0]
"After conditioning on X , we have a new Gaussian process gN with mean and covariance:
mN (x) = c(x,X)c(X,X) −1f(X),
cN (x,x ′)",2. Methodology,[0],[0]
"= c(x,x′)− c(x,X)c(X,X)−1c(X,x′),
for all x,x′ ∈ X .",2. Methodology,[0],[0]
"Here, c(X,X) is the Gram matrix with entries (c(X,X))ij = c(xi,xj) and c(x,X) = (c(x,x1), . . .",2. Methodology,[0],[0]
", c(x,xN ))",2. Methodology,[0],[0]
"whilst c(X,x) = c(x,X)> .",2. Methodology,[0],[0]
"The push-forward of this posterior through the integral operator is a Gaussian distribution with mean and variance:
E",2. Methodology,[0],[0]
"[Π[gN ]] = Π[c(·,X)]c(X,X)−1f(X), V [Π[gN",2. Methodology,[0],[0]
]] = ΠΠ̄,2. Methodology,[0],[0]
"[c]−Π[c(·,X)]c(X,X)−1Π̄[c(X, ·)],
where Π[c(·,X)]",2. Methodology,[0],[0]
"= (Π[c(·,x1)], . . .",2. Methodology,[0],[0]
",Π[c(·,xN )]).",2. Methodology,[0],[0]
"These expression can be obtained in closed-form if the kernel mean Π[c(·,x)]",2. Methodology,[0],[0]
"= ∫ X c(x ′,x)Π(dx′) (also called
the representer of integration) and initial error ΠΠ̄[c] =∫ X×X c(x,x
′)Π(dx)Π(dx′) can be obtained in closed form (here Π̄ indicates that the integral is taken with respect to the second argument).
",2. Methodology,[0],[0]
"The choice of covariance function c can be used to encode prior beliefs about the function f , such as smoothness or periodicity, and is very important to obtain good performance in practice.",2. Methodology,[0],[0]
"A popular example is the family of Matérn kernels
cα(x,x ′) = λ2
21−α
Γ(α)
(√ 2α ‖x− x′‖22
σ2 )α ×",2. Methodology,[0],[0]
"Jα (√ 2α ‖x− x′‖22
σ2
) , (1)
for σ, λ > 0 where Jα is the Bessel function of the second kind and α > 0 gives the smoothness of the kernel.",2. Methodology,[0],[0]
"On X = Rp, this will give an RKHS normequivalent to the Sobolev space Wα2 (Rd)1.",2. Methodology,[0],[0]
"Examples of infinitely smooth kernels include the squared-exponential kernel c(x,x′) = exp(−‖x− x′‖22/σ2) where σ > 0, the multi-quadric kernel c(x,x′) =",2. Methodology,[0],[0]
"(−1)dβe(σ2+‖x−x′‖22)β for β, σ > 0, β 6∈ N and the inverse multi-quadric kernel c(x,x′) = (σ2 + ‖x− x′‖22)−β for β, σ > 0.
",2. Methodology,[0],[0]
"In practice, numerical inversion can be challenging since the Gram matrix tends to be nearly singular, and so one may wish to regularise the matrix using a Tikhonov penalty.",2. Methodology,[0],[0]
"The inverses above can also potentially render the computation of the BQ estimator computationally expensive (up to O(N3) cost in the most general settings), although this can be alleviated in specific cases (Karvonen & Särkkä, 2017b).",2. Methodology,[0],[0]
"Even if this is not the case, the additional cost can be worthwhile regardless since the method has been shown to attain fast convergence rates (Briol et al., 2015a;b; Kanagawa et al., 2016; 2017; Bach, 2017) when the target integrand and the kernel used are smooth.
",2. Methodology,[0],[0]
"Recent research directions in BQ include efficient sampling algorithms (for the point set X) to improve the performance of the method (Rasmussen & Ghahramani, 2002; Huszar & Duvenaud, 2012; Gunter et al., 2014; Briol et al., 2015a; Karvonen & Särkkä, 2017a; Briol et al., 2017), asymptotic convergence results (Briol et al., 2015a;b; Kanagawa et al., 2016; Bach, 2017) and equivalence of BQ with known quadrature rules for certain choices of point sets and kernels (Sarkka et al., 2016; Karvonen & Särkkä, 2017a).",2. Methodology,[0],[0]
"Furthermore, there has also been a wide range of new applications, including to other numerical methods in optimization, linear algebra and functional approximation (Kersting & Hennig, 2016; Fitzsimons et al., 2017), inference in complex computer models
1We say that two norms ‖ · ‖1 and ‖ · ‖2 on a vector space are norm-equivalent if and only if ∃C1, C2 > 0",2. Methodology,[0],[0]
"such that C1‖ · ‖2 ≤ ‖ · ‖1 ≤ C2‖ · ‖2.
(Oates et al., 2017c), and problems in econometrics (Oettershagen, 2017) and computer graphics (Brouillat et al., 2009; Marques et al., 2013; Briol et al., 2015b).
",2. Methodology,[0],[0]
"Although other stochastic processes could of course be used (Cockayne et al., 2017), GPs are popular due to their conjugacy properties, and the terminology Bayesian quadrature usually refers to this case.",2. Methodology,[0],[0]
"Note that other names for BQ with GP priors include Gaussian-process quadrature (Sarkka et al., 2016) or kernel quadrature (Bach, 2017; Briol et al., 2017; Kanagawa et al., 2017).",2. Methodology,[0],[0]
"In fact, a well-known alternative view of the posterior mean provided by BQ is that of an optimally-weighted quadrature rule in a reproducing kernel Hilbert spaces (RKHS) in the classical worst-case setting (Ritter, 2000).",2. Methodology,[0],[0]
"Let Hk be an RKHS with inner product and norm denoted 〈·, ·〉k and ‖·‖k respectively; i.e. a Hilbert space with an associated symmetric and positive definite reproducing kernel k :",2. Methodology,[0],[0]
"X × X → R such that f(x) = 〈f, k(·,x)〉k (see (Berlinet & Thomas-Agnan, 2004) for a detailed study).",2. Methodology,[0],[0]
"Suppose that our integrand f ∈ Hk and that ∫ X k(x,x)Π(dx) <",2. Methodology,[0],[0]
"∞. In that case, using the Cauchy–Schwarz inequality, the integration error can be decomposed as:∣∣∣Π[f ]",2. Methodology,[0],[0]
− Π̂[f ]∣∣∣ ≤ ‖f‖k ∥∥∥Π,2. Methodology,[0],[0]
"[k(·,x)]− Π̂",2. Methodology,[0],[0]
"[k(·,x)]∥∥∥
k .
",2. Methodology,[0],[0]
"The corresponding worst-case error over the unit ball of the spaceHk is given by:
e ( Hk, Π̂,X )",2. Methodology,[0],[0]
= sup ‖f‖k≤1 ∣∣∣Π[f,2. Methodology,[0],[0]
]− Π̂[f ]∣∣∣ = ∥∥∥Π,2. Methodology,[0],[0]
"[k(·,x)]− Π̂",2. Methodology,[0],[0]
"[k(·,x)]∥∥∥
k = ( w>k(X,X)w − 2Π[k(·,X)]>w + ΠΠ̄[k] )",2. Methodology,[0],[0]
"1 2 .
",2. Methodology,[0],[0]
"This final expression can be minimised in closed form over w ∈ RN to show that the optimal quadrature rule has weights w = Π[k(·,X)]k(X,X)−1.",2. Methodology,[0],[0]
"This corresponds exactly to the weights for the BQ posterior mean if we take our prior on f to be a GP(0, k), whilst the worst-case error can be shown to correspond to the posterior variance squared.",2. Methodology,[0],[0]
"The BQ estimator with prior GP(0, c) is therefore optimal in the classical worst-case sense for the RKHSHc.
",2. Methodology,[0],[0]
Multi-output Bayesian Quadrature We now extend the set-up of our problem.,2. Methodology,[0],[0]
"Suppose we have a sequence of probability spaces (Xd,Bd,Πd) and functions fd :",2. Methodology,[0],[0]
"Xd → R for which we are interested in numerically computing integrals of the form Πd[fd] for d = 1, . . .",2. Methodology,[0],[0]
", D.",2. Methodology,[0],[0]
"In many applications where we are faced with this type of problem, we also have prior knowledge about correlations between the individual fd.",2. Methodology,[0],[0]
"However, this information is often ignored and the integrals are approximated individually.",2. Methodology,[0],[0]
"This is not a principled approach from a Bayesian point of
view since it means we are not conditioning on all available information.",2. Methodology,[0],[0]
"In this section, we extend the BQ algorithm to solve this problem by building a joint model of f1, . . .",2. Methodology,[0],[0]
", fD in order to obtain a joint posterior on the integrals Π1[f1], . . .",2. Methodology,[0],[0]
",ΠD[fD].
",2. Methodology,[0],[0]
"For notational convenience, we will restrict ourselves to the case where all of the input domains are identical and denoted X , all of the probability measures are identical and denoted Π, and the input sets X = {Xd}Dd=1 consist of N points",2. Methodology,[0],[0]
"Xd = (xd,1, . . .",2. Methodology,[0],[0]
",xd,N ) per output function fd (note the setup can be made more general if necessary).",2. Methodology,[0],[0]
We re-frame the integration problem as that of integrating some vector-valued function f : X,2. Methodology,[0],[0]
"→ RD such that f(x) = (f1(x), . . .",2. Methodology,[0],[0]
", fD(x))
",2. Methodology,[0],[0]
">; i.e. we want to estimate Π[f ] = (Π[f1], . . .",2. Methodology,[0],[0]
",Π[fD])
>.",2. Methodology,[0],[0]
"In this multiple-integral setting, we can have generalised quadrature rules of the form:
Π̂[fd] = D∑ d′=1 N∑ i=1",2. Methodology,[0],[0]
"(Wi)dd′fd′(xd′,i)
where Wi ∈ RD×D are weight matrices and (Wi)dd′ gives the influence of the value of fd′ at xd′,i on the estimate of Π[fd].",2. Methodology,[0],[0]
"The quadrature rule for f can be re-written in compact form as Π̂[f ] = W>f(X) for some weight matrix W ∈ RND×D (a concatenation of {Wi}Ni=1) and function-evaluations vector f(X) = (f1(x1,1), . . .",2. Methodology,[0],[0]
", f1(x1,N ), . . .",2. Methodology,[0],[0]
", fD(xD,1), . . .",2. Methodology,[0],[0]
", fD(xD,N ))",2. Methodology,[0],[0]
">.
",2. Methodology,[0],[0]
"These generalised quadrature rules encompass popular Monte Carlo methods such as control variates or functionals (Glasserman, 2004; Oates et al., 2017b), multilevel Monte Carlo (Giles, 2015) and multi-fidelity Monte Carlo (Peherstorfer et al., 2016b).",2. Methodology,[0],[0]
"However, it is important to point out that these methods can only deal with very specific relations between integrands, usually requiring ( ∫ X (fd(x)−fd′(x)) 2Π(dx))",2. Methodology,[0],[0]
"1 2 to be small for all pairs of integrands fd, fd′ .",2. Methodology,[0],[0]
"Our method will be able to make use of much more complex relations.
",2. Methodology,[0],[0]
"We propose to approach this problem using an extended version of BQ, where we impose a prior g : X × Ω→ RD which is a GP(0,C) on the extended space (this is often called a multi-output GP or co-kriging model (Alvarez et al., 2012))",2. Methodology,[0],[0]
"where now C is matrix-valued and (C(x,x′))dd′ = Eω∼P[gd(x, ω)gd′(x′, ω)",2. Methodology,[0],[0]
].,2. Methodology,[0],[0]
"In this case, after conditioning on X , we have a GP gN with vectorvalued mean mN :",2. Methodology,[0],[0]
"X → RD and matrix-valued covariance CN : X × X → RD×D:
mN (x) = C(x,X)C(X,X) −1f(X),
CN (x,x ′) = C(x,x′)−C(x,X)C(X,X)−1C(X,x′),
for C(x,X) = (C(x,x1), . . .",2. Methodology,[0],[0]
", C(x,xN ))",2. Methodology,[0],[0]
"∈ RD×ND and Gram matrix C(X,X) ∈",2. Methodology,[0],[0]
"RND×ND is:
C(X,X) =  (C(X1,X1))1,1 . . .",2. Methodology,[0],[0]
"(C(X1,XD))1,D (C(X2,X1))2,1 ... (C(X2,XD))2,D
... ... ...",2. Methodology,[0],[0]
"(C(XD,X1))D,1 . . .",2. Methodology,[0],[0]
"(C(XD,XD))D,D
 ,
where C(Xd,Xd′)d,d′ is an N × N matrix.",2. Methodology,[0],[0]
"The posterior on the value of the integral vector Π[f ] can also be obtained whenever the kernel mean Π[C(·,x)] and initial error ΠΠ̄",2. Methodology,[0],[0]
"[C] are available in closed form, which is potentially a restrictive condition.",2. Methodology,[0],[0]
"The authors of (Briol et al., 2015b) give a table of closed-form expressions of these quantities for popular kernels in the uni-output case, and we envision the same type of table being necessary for future extensions of multi-output BQ.",2. Methodology,[0],[0]
"Alternatively, (Oates et al., 2017b; 2016) proposed a kernel which is tailored to the target probability measure Π and which could also be extended to the multi-output case.
",2. Methodology,[0],[0]
Proposition 1.,2. Methodology,[0],[0]
"Consider multi-output Bayesian Quadrature with a GP(0,C) prior on f = (f1, . . .",2. Methodology,[0],[0]
", fD)>.",2. Methodology,[0],[0]
"The posterior distribution on Π[f ] is a D-dimensional Gaussian distribution with mean and covariance matrix:
E",2. Methodology,[0],[0]
"[Π[gN ]] = Π[C(·,X)]C(X,X)−1f(X), V [Π[gN ]] = ΠΠ̄",2. Methodology,[0],[0]
"[C]−Π[C(·,X)]C(X,X)−1Π̄[C(X, ·)].
",2. Methodology,[0],[0]
"All proofs can be found in Appendix B. In this case, we clearly end up with a generalised quadrature rule with weight matrix: W BQ = (Π [C(·,X)]C(X,X)−1)",2. Methodology,[0],[0]
>,2. Methodology,[0],[0]
∈,2. Methodology,[0],[0]
"RND×D. In general, the computational cost for computing the posterior mean and variance is now of orderO(N3D3).",2. Methodology,[0],[0]
"However, several choices of kernels can reduce this cost significantly, and it is also possible to obtain sparse GP approximations; see e.g. (Álvarez & Lawrence, 2011).
",2. Methodology,[0],[0]
The choice of kernel C is of course once again of great importance since it encodes prior knowledge about each of the integrand and their correlation structure and should be made based on the application considered.,2. Methodology,[0],[0]
"We also remark that matrix valued kernels C can be described in term of some scalar-valued kernel r on the extended space X×{1, . . .",2. Methodology,[0],[0]
", D} as (C(x,x′))dd′ =",2. Methodology,[0],[0]
"r((x, d), (x′, d′)).",2. Methodology,[0],[0]
"We now present two choices of covariance functions which are popular in the literature and will be used in this paper:
• The separable kernel is of the form
C(x,x′) = Bc(x,x′),
whereB ∈ RD×D is symmetric and positive definite, and c :",2. Methodology,[0],[0]
X × X → R is a scalar-valued reproducing kernel.,2. Methodology,[0],[0]
"This treats the kernel as the product of two scalar-valued reproducing kernels, one defined on X
and the other on {1, . . .",2. Methodology,[0],[0]
", D}.",2. Methodology,[0],[0]
A particular case of interest is the linear model of coregionalization (LMC) where the matrix is of the form (B)dd′ = ∑R i=1,2. Methodology,[0],[0]
a i da,2. Methodology,[0],[0]
i d′ for some aid ∈ R.,2. Methodology,[0],[0]
"This type of kernel can lead to a lower computational cost of orderO(N3 +D3) when evaluating all fd on the same input set and using tensor product formulations (see Appendix C).
",2. Methodology,[0],[0]
•,2. Methodology,[0],[0]
"The process convolution kernel (Ver Hoef & Barry, 1998; Higdon, 2002; Alvarez et al., 2012) models the individual functions f1, . . .",2. Methodology,[0],[0]
", fD",2. Methodology,[0],[0]
as blurred transformations of R ∈ N+ underlying functions.,2. Methodology,[0],[0]
"It is given by:
(C(x,x′))d,d′ = cd,d′(x,x ′) + cwd(x,x ′)δd,d′ ,
where δdd′ = 1 if d = d′ and 0 else.",2. Methodology,[0],[0]
"Here there are two parts of the kernel, first cd,d′ : X × X → R defined as:
cd,d′(x,x ′)",2. Methodology,[0],[0]
= R∑ i=1,2. Methodology,[0],[0]
∫,2. Methodology,[0],[0]
"X Gid(x− z)×∫
X Gid′(x ′",2. Methodology,[0],[0]
"− z′)ci(z, z′)dz′dz,
and cwd : X ×X → R representing covariance inherent to the dth function and Gid : X → R is a blurring kernel2 which is a continuous function either having compact support or being square integrable.",2. Methodology,[0],[0]
"Notice that taking Gid(x− z) = aidδ(x− z) (where δ(·) represents a Dirac function) gives back the LMC case.
",2. Methodology,[0],[0]
"Note that it is also common to combine kernels, by summing them (i.e. C(x,x′) = ∑Q q=1Cq(x,x
′))",2. Methodology,[0],[0]
in order to obtain more flexible models.,2. Methodology,[0],[0]
"The kernel means and initial error, as well as other details for implementation are provided in Appendix C.",2. Methodology,[0],[0]
"In this section, we begin by exploring properties of multioutput BQ with GP(0,C) prior as an optimally-weighted quadrature algorithm in vector-valued RKHSHC .
",3. Theoretical results,[0],[0]
"Let HK be a vector-valued RKHS with norm and inner product denoted ‖ · ‖K and 〈·, ·〉K respectively.",3. Theoretical results,[0],[0]
"These spaces were extensively studied in (Pedrick, 1957; Micchelli & Pontil, 2005; Carmeli et al., 2006; 2010; De Vito et al., 2013), and generalise the notion of RKHS to vectorvalued functions.",3. Theoretical results,[0],[0]
"In the multi-output case, there is also a one-to-one correspondance between the RKHS HK and the kernel K. Theorem 3.1 in (Micchelli & Pontil, 2005) shows that the minimizer of the variational problem:
min h∈HK
{ ‖h‖2K : h : X → RD,h(xi) = f(xi) ∀xi ∈X } 2Note that the term “blurring kernel” does not mean the func-
tion is a reproducing kernel.
takes the form of the multi-output posterior GP mean mN obtained after conditioning a GP(0,K) on some data set X .",3. Theoretical results,[0],[0]
"We can therefore extend a well-known result from the uni-output case to show that Π̂BQ[fd] is an optimally weighted quadrature rule for all fd in terms of their worstcase integration error, denoted:
e(HC , Π̂,X, d)",3. Theoretical results,[0],[0]
:= sup ‖f‖C≤1 ∣∣∣Π[fd]− Π̂[fd]∣∣∣ .,3. Theoretical results,[0],[0]
(2) Proposition 2 (Optimally weighted quadrature rule in HC).,3. Theoretical results,[0],[0]
"For a fixed point set X , denote by Π̂[f ] = W>f(X) any quadrature rule for the vector-valued function f = (f1, . . .",3. Theoretical results,[0],[0]
", fD) and by Π̂BQ[f ]",3. Theoretical results,[0],[0]
"= W>BQf(X) the BQ rule with GP(0,C) prior.",3. Theoretical results,[0],[0]
"Then, ∀d = 1, . . .",3. Theoretical results,[0],[0]
", D:
WBQ = arg min W∈RND×D
e(HC , Π̂,X, d).
",3. Theoretical results,[0],[0]
"In specific cases, it is also possible to characterise the rate of convergence of the worst-case error for each element fd.",3. Theoretical results,[0],[0]
"This is for example the case with the separable kernel introduced in Sec. 2, as will be demonstrated in the Theorem 1 below.",3. Theoretical results,[0],[0]
"First, we introduce some technical definitions which will be required for the statement of the theorem.
",3. Theoretical results,[0],[0]
We say that a domain X ⊂,3. Theoretical results,[0],[0]
"Rp satisfies an interior cone condition if there exists an angle θ ∈ (0, π2 ) and a radius r > 0",3. Theoretical results,[0],[0]
"such that ∀x ∈ X , a unit vector ξ(x) exists such that the cone {x + λy : y ∈ Rp, ‖y‖2 = 1,y>ξ(x) ≥ cos θ, λ ∈",3. Theoretical results,[0],[0]
"[0, r]} is a subset of X .
",3. Theoretical results,[0],[0]
"For a point setX , we call hX,X :=",3. Theoretical results,[0],[0]
supx∈X infxj∈X,3. Theoretical results,[0],[0]
"‖x− xj‖2 the fill distance, qX := 12 minj 6=k ‖xj−xk‖2",3. Theoretical results,[0],[0]
"the separation radius and ρX,X := hX,X /qX the mesh ratio.",3. Theoretical results,[0],[0]
"We will assume we evaluate all integrands on the same point setX which satisfies either of these assumptions:
(A1) X consists of independently and identically distributed (IID) samples from some probability measure Π′ which admits a density π′ > 0",3. Theoretical results,[0],[0]
"on X .
(A2) X is a quasi-uniform grid on X ⊂",3. Theoretical results,[0],[0]
"Rp (i.e. satisfies hX,X ≤ C1N− 1 p for some C1 > 0) and satisfies
hX,X ≤ C2qX,X for some C2 > 0.
",3. Theoretical results,[0],[0]
"Examples of point sets satisfying (A2) include uniform grid points in some hypercube.
",3. Theoretical results,[0],[0]
Theorem 1 (Convergence rate for BQ with separable kernel).,3. Theoretical results,[0],[0]
Suppose we want to approximate Π[f ] for some f : X,3. Theoretical results,[0],[0]
"→ RD and Π̂BQ[f ] is the multi-output BQ rule with the kernel C(x,x′) = Bc(x,x′) for some positive definite B ∈ RD×D and scalar-valued",3. Theoretical results,[0],[0]
kernel c :,3. Theoretical results,[0],[0]
"X × X → R. Then, ∀d = 1, . . .",3. Theoretical results,[0],[0]
", D, we have:
e(HC , Π̂BQ,X, d) =",3. Theoretical results,[0],[0]
"O ( e(Hc, Π̂BQ,X) ) .
",3. Theoretical results,[0],[0]
"In particular, assume that X ⊂",3. Theoretical results,[0],[0]
Rp satisfies an interior cone condition with Lipschitz boundary3 and X satisfies assumption (A1) or (A2).,3. Theoretical results,[0],[0]
"Then, the following rates hold:
• IfHc is norm-equivalent to an RKHS with Matérn kernel of smoothness α > p2 , we have ∀d = 1, . . .",3. Theoretical results,[0],[0]
", D:
e(HC , Π̂BQ,X, d) =",3. Theoretical results,[0],[0]
"O ( N− α p+ ) ,
for > 0 arbitrarily small.
",3. Theoretical results,[0],[0]
"• If Hc is norm-equivalent to the RKHS with squaredexponential, multiquadric or inverse multiquadric kernel, we have ∀d = 1, . . .",3. Theoretical results,[0],[0]
", D:
e(HC , Π̂BQ,X, d) =",3. Theoretical results,[0],[0]
"O ( exp ( −C1N 1 p− )) ,
for someC1 > 0 and for some > 0 arbitrarily small.
",3. Theoretical results,[0],[0]
Proposition 3 (Convergence rate for sum of kernels).,3. Theoretical results,[0],[0]
"Suppose that C(x,x′) = ∑Q q=1Cq(x,x ′).",3. Theoretical results,[0],[0]
"Then:
e(HC , Π̂BQ,X, d) = arg max q∈{1,...,Q}
O",3. Theoretical results,[0],[0]
"( e(HCq , Π̂BQ,X, d) ) .
",3. Theoretical results,[0],[0]
"We clarify that the notation with is common in the numerical integration literature, and is used to hide powers of log n terms since these do not have a significant influence on the asymptotic convergence rate.
",3. Theoretical results,[0],[0]
"It is interesting to note that the rate of convergence for multi-output BQ is the same as that of uni-output BQ (Briol et al., 2015b).",3. Theoretical results,[0],[0]
"This can be explained intuitively by the fact that, when adding a new integrand, we can only gain by a constant factor since we always evaluate the functions at the same input points.",3. Theoretical results,[0],[0]
"In fact the proof of Thm. 1 provides an expression for this improvement factor (in terms of WCE) for any integrand fd, and this depends explicitly on its correlation with the other functions: | ∑D i,j=1(B
−1)ijBidBjd|.",3. Theoretical results,[0],[0]
"From a practitioner’s viewpoint, this can clearly be used to balance the value of using several integrands with the additional computational cost incurred by using multi-output BQ.
",3. Theoretical results,[0],[0]
We now give a result in the misspecified setting when the function f is assumed to be smoother than it is.,3. Theoretical results,[0],[0]
"In this case, it is still possible to recover the optimal convergence rate:
Theorem 2 (Misspecified Convergence Result for Separable Kernel).",3. Theoretical results,[0],[0]
Let cα be a kernel norm-equivalent to a Matérn kernel of smoothness α on some domain X with Lipschitz boundary and satisfying an interior cone condition.,3. Theoretical results,[0],[0]
"Consider the BQ rule Π̂BQ[f ] corresponding to a separable kernel Cα(x, x′) = Bcα(x, x′) with X satisfying
3Formally defined in Appendix A for completeness.
",3. Theoretical results,[0],[0]
Figure 1.,3. Theoretical results,[0],[0]
"Multi-fidelity modelling: Plot of the Step function (top), Forrester function (bottom) for the low fidelity (left) and high fidelity (right).",3. Theoretical results,[0],[0]
"Each plot gives the true function (blue) and their unit-output (dashed, red), LMC-based multi-output (dashed, yellow) and PC-based multi-output (dotted purple) approximations.
",3. Theoretical results,[0],[0]
"(A2), and suppose that f ∈ HCβ where p 2 ≤ β ≤ α.",3. Theoretical results,[0],[0]
"Then, ∀d = 1, . . .",3. Theoretical results,[0],[0]
", D:∣∣∣Π[fd]−",3. Theoretical results,[0],[0]
"Π̂BQ[fd]∣∣∣ = O (N− βp+ ) , for some > 0.
",3. Theoretical results,[0],[0]
This last theorem demonstrate that the method is rate adaptive as long as we choose a kernel which is too smooth.,3. Theoretical results,[0],[0]
"However, it also demonstrates a drawback of the separable kernels: if one of the integrands is rough but all other are smooth, then the worst-case error could potentially converge slowly for all of them.
",3. Theoretical results,[0],[0]
"Finally, we note that studying the method in other information complexity settings than the worst-case would also be interesting.",3. Theoretical results,[0],[0]
"For example, it is trivial to show that the method above satisfies the definition of Bayesian probabilistic numerical method of (Cockayne et al., 2017) (Def. 2.5).",3. Theoretical results,[0],[0]
"Furthermore, optimality conditions for this method could also be obtained in a game-theoretic setting (in terms of a two-player mixed strategies game) by extending the theory on gamblets by (Owhadi & Scovel, 2017).",3. Theoretical results,[0],[0]
"Multi-fidelity modelling Consider some function f high : X → R representing some complex engineering model of interest, which we would like to use for some task such as statistical inference or optimization.",4. Applications,[0],[0]
"These models usually require the simulation of underlying physical systems, which can make each evaluation prohibitively expensive and will therefore limit N to the order of tens or hundreds.",4. Applications,[0],[0]
"To tackle this issue, multi-fidelity modelling proposes to build cheap, but less accurate, alternatives f low1 , . . .",4. Applications,[0],[0]
", f low",4. Applications,[0],[0]
"D−1 :
Model BQ LMC-BQ PC-BQ
Step (l) 0.02 (0.22) 0.02 (0.21) 0.02 (0.52)",4. Applications,[0],[0]
Step (h) 0.41 (0.03) 0.09 (0.09) 0.04 (0.15) For.,4. Applications,[0],[0]
(l) 0.08 (4.91) 0.08 (4.95) 0.07 (33.95) For.,4. Applications,[0],[0]
"(h) 3.96 (3.98) 2.86 (27.01) 1.06 (63.80)
",4. Applications,[0],[0]
"X → R to f high, and use the cheaper models in order to accelerate computation for the task of interest.",4. Applications,[0],[0]
"This can be done using surrogate models (e.g. support vector machines, GPs or neural networks), projection-based models (Krylov subspace or reduced basis methods) or models where the underlying physics is simplified; see (Peherstorfer et al., 2016a) for an overview.
",4. Applications,[0],[0]
"In this section, we consider the problem of numerical integration in such a multi-fidelity setup.",4. Applications,[0],[0]
"Note that two related methods for Monte Carlo estimation are the multi-fidelity Monte Carlo estimator (Peherstorfer et al., 2016a) and the multilevel Monte Carlo of (Giles, 2015), both of which are based on control variate identities.
",4. Applications,[0],[0]
"We approach this problem with multi-output BQ on the vector-valued function f = (f high, f low1 , . . .",4. Applications,[0],[0]
", f low D−1)
>.",4. Applications,[0],[0]
"Note that multi-output Gaussian processes were already proposed for multi-fidelity modelling in (Perdikaris et al., 2016; Parussini et al., 2017), and we extend their methodologies to the task of numerical integration.",4. Applications,[0],[0]
"We consider two toy problems from this literature (Raissi & Karniadakis, 2016) to highlight some of the advantages and disadvantages of our methodology
1.",4. Applications,[0],[0]
A step function on X =,4. Applications,[0],[0]
"[0, 2]:
f low1 (x) = { 0, x ≤ 1 1, x > 1 f high(x) = { −1, x ≤ 1 2, x > 1
2.",4. Applications,[0],[0]
The Forrester function with Jump on X =,4. Applications,[0],[0]
"[0, 1]:
f low1 (x) =
{ (3x−1)2 sin(12x−4)
4 + 10(x− 1), x ≤ 1 2
3 + (3x−1) 2 sin(12x−4)
4 + 10(x− 1), x > 1 2
f high(x) =
{ 2f low(x)− 20(x− 1), x ≤ 12
4 + 2f low(x)− 20(x− 1), x > 12
The functions and conditioned GPs are given in Fig. 1, whilst the uni-output and multi-output BQ estimates for integration of these functions against a uniform measure Π
are given in the table in Fig. 2.",4. Applications,[0],[0]
"In both cases, 20 equidistant points are used, with point number 4, 10, 11, 14 and 17 used to evaluate the high fidelity model and the others used for the low fidelity model.",4. Applications,[0],[0]
The choice of kernel hyperparameters is made by maximising the marginal likelihood (often called empirical Bayes).,4. Applications,[0],[0]
"Further details, and an additional test function can be found in Appendix D.2.
",4. Applications,[0],[0]
Note that both of these problems are challenging for several reasons.,4. Applications,[0],[0]
"Firstly, due to their discontinuity, the integrands are not in the RKHS HC corresponding to the kernel C used in multi-output BQ.",4. Applications,[0],[0]
"In particular, the problems are misspecified in the sense that the true function is not in the support of the prior.",4. Applications,[0],[0]
"It is therefore difficult to interpret the posterior distribution on Π[f ], and we end up with credible intervals which are too wide.",4. Applications,[0],[0]
This is for example illustrated in the values of the posterior variance for the high-fidelity Forrester function.,4. Applications,[0],[0]
"Secondly, in each case, the high and low-fidelity models are defined on different scales and so require tuning of several kernel hyper-parameters.",4. Applications,[0],[0]
"This can of course make it challenging for multi-output BQ since the number of function evaluations N is small and empirical Bayes will tend to be inefficient in those cases.
",4. Applications,[0],[0]
"However, despite these two issues, it is interesting to note that both of the multi-output BQ methods manage to significantly outperform uni-output BQ in terms of point estimate, as the sharing of information allows the multi-output models to better represent the main trends in the functions.",4. Applications,[0],[0]
"Furthermore, the multi-output BQ does not suffer from the issues of overconfident posterior credible intervals present in uni-output BQ; contrast for example the posterior variances for the high-fidelity step function.
",4. Applications,[0],[0]
"Global illumination In this section, we apply multioutput BQ to a challenging numerical integration problem from the field of computer graphics, known as global illumination.",4. Applications,[0],[0]
"BQ was previously applied to this problem in several papers (Brouillat et al., 2009; Marques et al., 2013; Briol et al., 2015b), but we propose to extend these results using multi-output BQ.
",4. Applications,[0],[0]
Global illumination is a problem which occurs when trying to obtain realistic representation of light interactions for the design of virtual environments (e.g. a video game).,4. Applications,[0],[0]
"One model of the amount of light coming from an object towards the camera (representing the current viewpoint on this environment) is given by the following equation:
L0(ω0) = Le(ω0) + ∫ S2 Li(ωi)ρ(ωi, ω0)[ωi · n]+dΠ(ωi).
",4. Applications,[0],[0]
"where [x]+ = max(0, x).",4. Applications,[0],[0]
"The function L0 : S2 → R evaluated at ω0 is called the outgoing radiance in direction ω0 (the angle of the outgoing light from the object normal n), Le(ω0) : S2 → R is the amount of light emitted by
the object, and Li : S2 → R evaluated at ωi is the amount of light reflected by the object (which originated from an angle ωi from the object’s normal n).",4. Applications,[0],[0]
"Here, S2 = {x = (x1, x2, x3) ∈ R3 : ‖x‖2 = 1} and ρ(ωi, ω0) :",4. Applications,[0],[0]
"S2 × S2 → R is called the bidirectional reflectance distribution and represents the proportion of light being reflected.
",4. Applications,[0],[0]
"We follow (Briol et al., 2015b) and consider the problem as Π[hω0 ]",4. Applications,[0],[0]
"= ∫ S2 h
ω0(ωi)Π(dωi) where Π is the uniform measure on S2, and hω0(ωi) = Li(ωi)ρ(ωi, ω0)[ωi · ω0]+ is a function which can be evaluated by making a call to an environment map (which we consider to be a black box).",4. Applications,[0],[0]
"One scenario which is common in these type of problems is to look at an object from different angles ω0, with the camera moving.",4. Applications,[0],[0]
"In this case, it is reasonable to assume that the different integrands hω0 will be very similar when the difference in the angle ω0 is small, and it is therefore natural to consider jointly estimating their integrals.",4. Applications,[0],[0]
"In the experiments we consider five integrands fi = hω i 0 for i = 1, . . .",4. Applications,[0],[0]
", 5 where ω10 , . .",4. Applications,[0],[0]
.,4. Applications,[0],[0]
", ω 5 0 are on a great circle of the sphere at intervals determined by an angle of 0.005π.
",4. Applications,[0],[0]
We therefore consider two-output and five-output BQ with independent and identically distributed (Monte Carlo) samples X from the uniform measure,4. Applications,[0],[0]
"Π. We propose to use a separable kernel with scalar-valued RKHS Hc being a Sobolev space of smoothness 32 over S
2 and has kernel c(x,x′) = 83",4. Applications,[0],[0]
− ‖x,4. Applications,[0],[0]
"− x
′‖22.",4. Applications,[0],[0]
"For the matrix B representing the covariance between outputs, we propose to make this covariance proportional to the difference in angle at which the camera looks at the object.",4. Applications,[0],[0]
In particular we choose (B)ij = exp(ω i 0 · ω,4. Applications,[0],[0]
j 0,4. Applications,[0],[0]
"− 1) for simplicity, but this could be generalised to include a lengthscale and amplitude hyperparameter to be learnt together with the hyperparameters of the scalar-valued kernel c.
The GP means for the one-output and five-output cases are given in Fig. 3, and we can clearly notice a significant improvement in approximation accuracy with the larger number of outputs.",4. Applications,[0],[0]
Results for integration error are given in Fig. 4.,4. Applications,[0],[0]
"As noticed, the integration error (for a fixed number of evaluationsN of each integrand) is significantly reduced by increasing the number of outputs D. The individual posterior variances for this problem (see Appendix D.3 Fig. 10) are also smaller, reflecting the fact that our uncertainty is reduced due to use of observations from other integrands.
",4. Applications,[0],[0]
"In fact, a small extension of Thm. 1 (combined with the rate for the scalar-valued kernel in (Briol et al., 2015b)) allows us to obtain an asymptotic convergence rate for the posterior variance on each integral Π[fd]: Corollary 1.",4. Applications,[0],[0]
LetX be the sphere S2 andX be IID uniform points onX .,4. Applications,[0],[0]
AssumeC is a separable kernel with c defined above.,4. Applications,[0],[0]
"Then e(HC , Π̂BQ,X, d) = OP ( N− 3 4 ) .
",4. Applications,[0],[0]
"The same rate with improved rate constant was observed
in (Briol et al., 2015b) when using QMC point sets, and similar gains could be obtained in this multi-output case.
",4. Applications,[0],[0]
We note that there a significant potential further gains for the use of multi-output BQ in this setting.,4. Applications,[0],[0]
"Similar integration problems need to be computed for three colors in every pixel of an image, and for every image in a video.",4. Applications,[0],[0]
This is challenging computationally and limits the use of Monte Carlo methods to a few dozen points.,4. Applications,[0],[0]
Designing specific matrix-valued kernels could provide enormous gains since we end up with thousands of correlated integrands.,4. Applications,[0],[0]
"Furthermore, the weights only depend on the choice of kernel and not on function values, so that all of the weights could be pre-computed off-line to be later used in real-time.",4. Applications,[0],[0]
We have proposed an extension of Bayesian Quadrature to the case where we are interested in numerically computing the integral of several functions which are related.,5. Conclusion,[0],[0]
In particular we have proposed a new algorithm based on jointly modelling the integrands with a Gaussian prior.,5. Conclusion,[0],[0]
"Then, we provided a theoretical study of the rate of convergence for the case where the kernel is separable and illustrated the potential of our methodology on applications in multi-fidelity
modelling and computer graphics.",5. Conclusion,[0],[0]
"Our main contribution however, has been to highlight the natural extension of Bayesian probabilistic numerical methods to the joint estimation of the solution of several numerical problems (in this case, numerical integration problems).
",5. Conclusion,[0],[0]
There are several possible extensions of multi-output BQ which we reserve for future work.,5. Conclusion,[0],[0]
One important question remaining is that of the choice of sampling distribution.,5. Conclusion,[0],[0]
"In the uni-output case, it is well known that obtaining an optimal sampling distribution with respect to the Vn[Π[f ]] is intractable in most cases.",5. Conclusion,[0],[0]
"(Briol et al., 2017) proposed an algorithm to approach such a distribution, and (Kanagawa et al., 2017) provided conditions on the point sets to guarantee fast convergence.",5. Conclusion,[0],[0]
"In the multi-output case, the problem is even more complex due to the interaction between the different integration problems.",5. Conclusion,[0],[0]
"However, the literature on the design of experiments for co-kriging/multi-output GPs may be of interest, and the use of more advanced sampling distributions will certainly provide significant gains.",5. Conclusion,[0],[0]
"The authors are grateful to Alessandro Barp, Aretha Teckentrup, Chris Oates and Motonobu Kanagawa for helpful discussions.",Acknowledgements,[0],[0]
"FXB was supported by the EPSRC grants [EP/L016710/1, EP/R018413/1, EP/N510129/1].",Acknowledgements,[0],[0]
"MG was supported by the EPSRC grants [EP/J016934/3, EP/K034154/1, EP/P020720/1, EP/R018413/1, EP/N510129/1], an EPSRC Established Career Fellowship, the EU grant [EU/259348] and the
Lloyds Register Foundation Programme on Data-Centric Engineering.",Acknowledgements,[0],[0]
The authors would like to thank the Isaac Newton Institute for Mathematical Sciences for support and hospitality during the programme on “Uncertainty Quantification for Complex Systems: Theory and Methodologies”.,Acknowledgements,[0],[0]
This work was supported by EPSRC grant no [EP/K032208/1].,Acknowledgements,[0],[0]
"Finally, this material was also based upon work partially supported by the National Science Foundation under Grant DMS-1127914 to the Statistical and Applied Mathematical Sciences Institute.",Acknowledgements,[0],[0]
Bayesian probabilistic numerical methods are a set of tools providing posterior distributions on the output of numerical methods.,abstractText,[0],[0]
The use of these methods is usually motivated by the fact that they can represent our uncertainty due to incomplete/finite information about the continuous mathematical problem being approximated.,abstractText,[0],[0]
"In this paper, we demonstrate that this paradigm can provide additional advantages, such as the possibility of transferring information between several numerical methods.",abstractText,[0],[0]
"This allows users to represent uncertainty in a more faithful manner and, as a by-product, provide increased numerical efficiency.",abstractText,[0],[0]
We propose the first such numerical method by extending the well-known Bayesian quadrature algorithm to the case where we are interested in computing the integral of several related functions.,abstractText,[0],[0]
"We then prove convergence rates for the method in the well-specified and misspecified cases, and demonstrate its efficiency in the context of multi-fidelity models for complex engineering systems and a problem of global illumination in computer graphics.",abstractText,[0],[0]
Bayesian Quadrature for Multiple Related Integrals,title,[0],[0]
Deep learning has dramatically advanced the state of the art in a number of domains.,1. Introduction,[0],[0]
"Despite their unprecedented discriminative power, deep networks are prone to make mistakes.",1. Introduction,[0],[0]
"Nevertheless, they can already be found in settings where errors carry serious repercussions such as autonomous vehicles (Chen et al., 2016) and high frequency trading.",1. Introduction,[0],[0]
"We can soon expect automated systems to screen for various types of cancer (Esteva et al., 2017; Shen, 2017) and diagnose biopsies (Djuric et al., 2017).",1. Introduction,[0],[0]
"As autonomous systems based on deep learning are increasingly deployed in settings with the potential to cause physical or economic harm, we need to develop a better understanding of when we can be confident in the estimates produced by deep networks, and when we should be less certain.
",1. Introduction,[0],[0]
Standard deep learning techniques used for supervised learning lack methods to account for uncertainty in the model.,1. Introduction,[0],[0]
"This can be problematic when the network encounters conditions it was not exposed to during training,
* Co-first authorship 1School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden 2Current address: Electronic Arts, SEED, Stockholm, Sweden.",1. Introduction,[0],[0]
This work was carried out at Budbee AB.,1. Introduction,[0],[0]
3Science for Life Laboratory.,1. Introduction,[0],[0]
"Correspondence to: Kevin Smith <ksmith@kth.se>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"or if the network is confronted with adversarial examples (Goodfellow et al., 2014).",1. Introduction,[0],[0]
"When exposed to data outside the distribution it was trained on, the network is forced to extrapolate, which can lead to unpredictable behavior.
",1. Introduction,[0],[0]
"If the network can provide information about its uncertainty in addition to its point estimate, disaster may be avoided.",1. Introduction,[0],[0]
"In this work, we focus on estimating such predictive uncertainties in deep networks (Figure 1).
",1. Introduction,[0],[0]
"The Bayesian approach provides a theoretical framework for modeling uncertainty (Ghahramani, 2015), which has prompted several attempts to extend neural networks (NN) into a Bayesian setting.",1. Introduction,[0],[0]
"Most notably, Bayesian neural networks (BNNs) have been studied since the 1990’s (Neal, 2012), but do not scale well and struggle to compete with modern deep learning architectures.",1. Introduction,[0],[0]
"Recently, (Gal & Ghahramani, 2015) developed a practical solution to obtain uncertainty estimates by casting dropout training in conventional deep networks as a Bayesian approximation of a Gaussian Process (its correspondence to a general approximate Bayesian model was shown in (Gal, 2016)).",1. Introduction,[0],[0]
"They showed that any network trained with dropout is an approximate Bayesian model, and uncertainty estimates can be obtained by computing the variance on multiple predictions with different dropout masks.
",1. Introduction,[0],[0]
"The inference in this technique, called Monte Carlo Dropout (MCDO), has an attractive quality: it can be applied to any pre-trained networks with dropout layers.",1. Introduction,[0],[0]
Uncertainty estimates come (nearly) for free.,1. Introduction,[0],[0]
"However, not all architectures use dropout, and most modern networks have adopted other regularization techniques.",1. Introduction,[0],[0]
"Batch normalization (BN), in particular, has become widespread thanks to its ability to stabilize learning with improved generalization (Ioffe & Szegedy, 2015).
",1. Introduction,[0],[0]
An interesting aspect of BN is that the mini-batch statistics used for training each iteration depend on randomly selected batch members.,1. Introduction,[0],[0]
"We exploit this stochasticity and show that training using batch normalization, like dropout, can be cast as an approximate Bayesian inference.",1. Introduction,[0],[0]
We demonstrate how this finding allows us to make meaningful estimates of the model uncertainty in a technique we call Monte Carlo Batch Normalization (MCBN) (Figure 1).,1. Introduction,[0],[0]
"The method we propose can be applied to any network using standard batch normalization.
",1. Introduction,[0],[0]
"We validate our approach by empirical experiments on a variety of datasets and tasks, including regression and image classification.",1. Introduction,[0],[0]
"We measure uncertainty quality relative to a baseline of fixed uncertainty, and show that MCBN outperforms the baseline on nearly all datasets with strong statistical significance.",1. Introduction,[0],[0]
We also show that the uncertainty quality of MCBN is on par with other recent approximate Bayesian networks.,1. Introduction,[0],[0]
"Bayesian models provide a natural framework for modeling uncertainty, and several approaches have been developed to adapt NNs to Bayesian reasoning.",2. Related Work,[0],[0]
A common approach is to place a prior distribution (often a Gaussian) over each parameter.,2. Related Work,[0],[0]
"The resulting model corresponds to a Gaussian process for infinite parameters (Neal, 1995), and a Bayesian NN (MacKay, 1992) for a finite number of parameters.",2. Related Work,[0],[0]
"Inference in BNNs is difficult however (Gal, 2016), so focus has thus shifted to techniques that approximate the posterior, approximate BNNs.",2. Related Work,[0],[0]
"Methods based on variational inference (VI) typically rely on a fully factorized approximate distribution (Kingma & Welling, 2014; Hinton & Van Camp, 1993), but often do not scale.",2. Related Work,[0],[0]
"To alleviate these difficulties, (Graves, 2011) proposed a model using sampling methods to estimate a factorized posterior.",2. Related Work,[0],[0]
"Probabilistic backpropagation (PBP), estimates a factorized posterior via expectation propagation (HernándezLobato & Adams, 2015).
",2. Related Work,[0],[0]
"Using several strategies to address scaling issues, Deep
Gaussian Processes show superior performance in terms of RMSE and uncertainty quality compared to state-of-the-art approximate BNNs (Bui et al., 2016)1.",2. Related Work,[0],[0]
"Another recent approach to Bayesian learning, Bayesian hypernetworks, use a NN to learn a distribution of parameters over another network (Krueger et al., 2017).",2. Related Work,[0],[0]
"Multiplicative Normalizing Flows for variational Bayesian networks (MNF) (Louizos & Welling, 2017) is a recent model that formulates a posterior dependent on auxiliary variables.",2. Related Work,[0],[0]
"MNF achieves a highly flexible posterior by the application of normalizing flows to the auxiliary variables.
",2. Related Work,[0],[0]
"Although these recent techniques address some of the difficulties with approximate BNNs, they all require modifications to the architecture or the way networks are trained, as well as specialized knowledge from practitioners.",2. Related Work,[0],[0]
"Recently, (Gal & Ghahramani, 2015) showed that a network trained with dropout implicitly performs the VI objective.",2. Related Work,[0],[0]
Therefore any network trained with dropout can be treated as an approximate Bayesian model by making multiple predictions through the network while sampling different dropout masks for each prediction.,2. Related Work,[0],[0]
The mean and variance of the predictions are used in the estimation of the mean and variance of the predictive distribution 2.,2. Related Work,[0],[0]
"In the following, we introduce Bayesian models and a variational approximation using Kullback-Leibler (KL) divergence following (Gal, 2016).",3. Method,[0],[0]
We continue by showing that a batch normalized deep network can be seen as an approximate Bayesian model.,3. Method,[0],[0]
"Employing theoretical insights and empirical analysis, we study the induced prior on the parameters when using batch normalization.",3. Method,[0],[0]
"Finally, we describe the procedure for estimating the uncertainty of a batch normalized network’s output.3",3. Method,[0],[0]
"We assume a finite training set D = {(xi,yi)}i=1:N where each (xi,yi) is a sample-label pair.",3.1. Bayesian Modeling,[0],[0]
"Using D, we are interested in learning an inference function fω(x,y) with parameters ω.",3.1. Bayesian Modeling,[0],[0]
"In deterministic models, the estimated label ŷ is obtained as follows:
ŷ = arg max y fω(x,y)
",3.1. Bayesian Modeling,[0],[0]
"In probabilistic models we let fω(x,y) = p(y|x,ω).",3.1. Bayesian Modeling,[0],[0]
"In Bayesian modeling, in contrast to finding a point estimate
1By uncertainty quality, we refer to predictive probability distributions as measured by PLL and CRPS.
2This technique is referred to as “MC Dropout” in the original work, though we refer to it here as MCDO.
",3.1. Bayesian Modeling,[0],[0]
"3While the method applies to FC or Conv layers, the induced prior from weight decay (Section 3.3) is studied for FC layers.
of the model parameters, the idea is to estimate an (approximate) posterior distribution of the model parameters p(ω|D) to be used for probabilistic prediction:
p(y|x,D) = ∫ fω(x,y)p(ω|D)dω
The predicted label, ŷ, can then be accordingly obtained by sampling p(y|x,D) or taking its maxima.
",3.1. Bayesian Modeling,[0],[0]
"Variational Approximation In approximate Bayesian modeling, a common approach is to learn a parameterized approximating distribution qθ(ω) that minimizes KL(qθ(ω)||p(ω|D)); the Kullback-Leibler divergence of the true posterior w.r.t.",3.1. Bayesian Modeling,[0],[0]
its approximation.,3.1. Bayesian Modeling,[0],[0]
"Minimizing this KL divergence is equivalent to the following minimization while being free of the data term p(D) 4:
LVA(θ) :",3.1. Bayesian Modeling,[0],[0]
"=− N∑ i=1 ∫ qθ(ω) ln fω(xi,yi)dω
+ KL(qθ(ω)||p(ω))
",3.1. Bayesian Modeling,[0],[0]
"During optimization, we want to take the derivative of the expected likelihood w.r.t.",3.1. Bayesian Modeling,[0],[0]
the learnable parameters θ.,3.1. Bayesian Modeling,[0],[0]
"We use the same MC estimate as in (Gal, 2016) (explained in Appendix Section 1.1), such that one realized ω̂i is taken for each sample i 5.",3.1. Bayesian Modeling,[0],[0]
"Optimizing over mini-batches of size M , the approximated objective becomes:
L̂VA(θ) :",3.1. Bayesian Modeling,[0],[0]
"= − N
M M∑ i=1",3.1. Bayesian Modeling,[0],[0]
"ln fω̂i(xi,yi) + KL(qθ(ω)||p(ω))",3.1. Bayesian Modeling,[0],[0]
"(1)
The first term is the data likelihood and the second term is the divergence of the prior w.r.t.",3.1. Bayesian Modeling,[0],[0]
the approximated posterior.,3.1. Bayesian Modeling,[0],[0]
"We now describe the optimization procedure of a deep network with batch normalization and draw the resemblance to the approximate Bayesian modeling in Eq (1).
",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"The inference function of a feed-forward deep network with L layers can be described as:
fω(x) = W La(WL−1...a(W2a(W1x))
4Achieved by constructing the Evidence Lower Bound, called ELBO, and assuming i.i.d. observation noise; details can be found in Appendix Section 1.1.
",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"5While a MC integration using a single sample is a weak approximation, in an iterative optimization for θ several samples will be taken over time.
",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"where a(.) is an element-wise nonlinearity function and Wl is the weight vector at layer l. Furthermore, we denote the input to layer l as xl with x1 = x",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
and we then set hl = Wlxl.,3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
Parenthesized super-index for matrices (e.g. W(j)) and vectors (e.g. x(j)) indicates jth row and element respectively.,3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"Super-index u refers to a specific unit at layer l, (e.g. Wu = Wl,(j), hu = hl,(j)).",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"6
Batch Normalization Each layer of a deep network is constructed by several linear units whose parameters are the rows of the weight matrix W. Batch normalization is a unit-wise operation proposed in (Ioffe & Szegedy, 2015) to standardize the distribution of each unit’s input.",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"For FC layers, it converts a unit’s input hu in the following way:
ĥu = hu",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"− E[hu]√
Var[hu]
where the expectations are computed over the training set during evaluation, and mini-batch during training (in deep networks, the weight matrices are often optimized using back-propagated errors calculated on mini-batches of data)7.",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"Therefore, during training, the estimated mean and variance on the mini-batch B is used, which we denote by µB and σB respectively.",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"This makes the inference at training time for a sample x a stochastic process, varying based on other samples in the mini-batch.
",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"Loss Function and Optimization Training deep networks with mini-batch optimization involves a (regularized) risk minimization with the following form:
LRR(ω) :",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"= 1
M M∑ i=1",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"l(ŷi,yi) + Ω(ω)
where the first term is the empirical loss on the training data and the second term is a regularization penalty acting as a prior on model parameters ω.",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
If the loss l is cross-entropy for classification or sum-of-squares for regression problems (assuming i.i.d.,3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"Gaussian noise on labels), the first term is equivalent to minimizing the negative log-likelihood:
LRR(ω) := − 1
Mτ M∑ i=1",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"ln fω(xi,yi) + Ω(ω)
6For a (softmax) classification network, fω(x) is a vector with fω(x,y) = fω(x)
(y), for regression networks with i.i.d.",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"Gaussian noise we have fω(x,y) = N (fω(x), τ−1I).
",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"7It also learns an affine transformation for each unit with parameters γ and β, omitted for brevity: x̂(j)affine = γ (j)x̂(j) + β(j).
with τ = 1 for classification.",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"In a network with batch normalization, the model parameters include {W1:L,γ1:L,β1:L,µ1:LB ,σ1:LB }.",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"If we decouple the learnable parameters θ = {W1:L,γ1:L,β1:L} from the stochastic parameters ω = {µ1:LB ,σ1:LB }, we get the following objective at each step of the mini-batch optimization:
LRR(θ) := − 1
Mτ M∑ i=1",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"ln f{θ,ω̂i}(xi,yi) + Ω(θ) (2)
where ω̂i is the means and variances for sample i’s minibatch at a certain training step.",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
Note that while ω̂i formally needs to be i.i.d.,3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"for each training example, a batch normalized network samples the stochastic parameters once per training step (mini-batch).",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"For a large number of epochs, however, the distribution of sampled batch members for a given training example converges to the i.i.d. case.
",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"In a batch normalized network, qθ(ω) corresponds to the joint distribution of the weights, induced by the randomness of the normalization parameters µ1:LB ,σ 1:L B , as implied by the repeated sampling from D during training.",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"This is an approximation of the true posterior, where we have restricted the posterior to lie within the domain of our parametric network and source of randomness.",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"With that, we can estimate the uncertainty of predictions from a trained batch normalized network using the inherent stochasticity of BN (Section 3.4).",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
Equivalence between the VA and BN training procedures requires ∂∂θ of Eq.,3.3. Prior p(ω),[0],[0]
(1) and Eq. (2) to be equivalent up to a scaling factor.,3.3. Prior p(ω),[0],[0]
This is the case if ∂∂θKL(qθ(ω)||p(ω)),3.3. Prior p(ω),[0],[0]
"= Nτ ∂∂θΩ(θ).
",3.3. Prior p(ω),[0],[0]
"To reconcile this condition, one option is to let the prior p(ω) imply the regularization term Ω(θ).",3.3. Prior p(ω),[0],[0]
Eq. (1) reveals that the contribution of KL(qθ(ω)||p(ω)) to the optimization objective is inversely scaled with N .,3.3. Prior p(ω),[0],[0]
"For BN, this corresponds to a model with a small Ω(θ) when N is large.",3.3. Prior p(ω),[0],[0]
"In the limit as N →∞, the optimization objectives of Eq.",3.3. Prior p(ω),[0],[0]
"(1) and Eq. (2) become identical with no regularization.8
Another option is to let some Ω(θ) imply p(ω).",3.3. Prior p(ω),[0],[0]
"In Appendix Section 1.4 we explore this with L2-regularization, also called weight decay (Ω(θ) = λ ∑ l=1:L ||W l||2).",3.3. Prior p(ω),[0],[0]
"We find that unlike in MCDO (Gal, 2016), some simplifying
8To prove the existence and find an expression of KL(qθ(ω)||p(ω)), in Appendix Section 1.3 we find that BN approximately induces Gaussian distributions over BN units’ means and standard deviations, centered around the population values given by D. We assume a factorized distribution and Gaussian priors, and find the corresponding KL(qθ(ω)||p(ω)) components in Appendix Section 1.4 Eq.",3.3. Prior p(ω),[0],[0]
(7).,3.3. Prior p(ω),[0],[0]
"These could be used to construct a custom Ω(θ) for any Gaussian choice of p(ω).
assumptions are necessary to reconcile the VA and BN objectives with weight decay: no scale and shift applied to BN layers, uncorrelated units in each layer, BN applied on all layers, and large N and M .",3.3. Prior p(ω),[0],[0]
"Given these conditions:
p(µuB) = N (µµ,p, σµ,p) p(σuB) = N (µσ,p, σσ,p)
where µµ,p = 0, σµ,p →∞, µσ,p = 0 and σσ,p → 12Nτλl .
",3.3. Prior p(ω),[0],[0]
This corresponds to a wide and narrow distribution on BN units’ means and std.,3.3. Prior p(ω),[0],[0]
"devs respectively, where N accounts for the narrowness of the prior.",3.3. Prior p(ω),[0],[0]
"Due to its popularity in deep learning, our experiments in Section 4 are performed with weight decay.",3.3. Prior p(ω),[0],[0]
"In the absence of the true posterior, we rely on the approximate posterior to express an approximate predictive distribution:
p∗(y|x,D) := ∫ fω(x,y)qθ(ω)dω
Following (Gal, 2016)",3.4. Predictive Uncertainty in Batch Normalized Deep Nets,[0],[0]
"we estimate the first (for regression and classification) and second (for regression) moments of the predictive distribution empirically (see Appendix Section 1.5 for details):
Ep∗",3.4. Predictive Uncertainty in Batch Normalized Deep Nets,[0],[0]
"[y] ≈ 1
T T∑ i=1",3.4. Predictive Uncertainty in Batch Normalized Deep Nets,[0],[0]
"fω̂i(x)
Covp∗",3.4. Predictive Uncertainty in Batch Normalized Deep Nets,[0],[0]
"[y] ≈ τ−1I + 1
T T∑ i=1",3.4. Predictive Uncertainty in Batch Normalized Deep Nets,[0],[0]
"fω̂i(x) ᵀfω̂i(x)
",3.4. Predictive Uncertainty in Batch Normalized Deep Nets,[0],[0]
− Ep∗,3.4. Predictive Uncertainty in Batch Normalized Deep Nets,[0],[0]
"[y]ᵀEp∗ [y]
where each ω̂i corresponds to sampling the net’s stochastic parameters ω = {µ1:LB ,σ1:LB } the same way as during training.",3.4. Predictive Uncertainty in Batch Normalized Deep Nets,[0],[0]
"Sampling ω̂i therefore involves sampling a batch B from the training set and updating the parameters in the BN units, just as if we were taking a training step with B. From a VA perspective, training the network amounted to minimizing KL(qθ(ω)||p(ω|D))",3.4. Predictive Uncertainty in Batch Normalized Deep Nets,[0],[0]
wrt θ.,3.4. Predictive Uncertainty in Batch Normalized Deep Nets,[0],[0]
"Sampling ω̂i from the training set, and keeping the size of B consistent with the mini-batch size used during training, ensures that qθ(ω) during inference remains identical to the approximate posterior optimized during training.
",3.4. Predictive Uncertainty in Batch Normalized Deep Nets,[0],[0]
"The network is trained just as a regular BN network, but instead of replacing ω = {µ1:LB ,σ1:LB } with population values from D for inference, we update these parameters stochastically, once for each forward pass.9 Pseudocode for estimating predictive mean and variance is given in Algorithm 1.
9As an alternative to using the training set D to sample ω̂i,
",3.4. Predictive Uncertainty in Batch Normalized Deep Nets,[0],[0]
"Algorithm 1 MCBN Algorithm Input: sample x, number of inferences T , batchsize b Output: mean prediction ŷ, predictive uncertainty σ2
1: y = {} 2: loop for T iterations 3: B ∼ D // mini batch 4: ω̂ = {µB ,σB} // mini batch mean and variance 5: y = y ∪ fω̂(x) 6: end loop 7: ŷ = E[y] 8: σ2 = Cov[y] + τ−1I // for regression",3.4. Predictive Uncertainty in Batch Normalized Deep Nets,[0],[0]
We assess the uncertainty quality of MCBN quantitatively and qualitatively.,4. Experiments and Results,[0],[0]
"Our quantitative analysis relies on CIFAR10 for image classification and eight standard regression datasets, listed in Appendix Table 1.",4. Experiments and Results,[0],[0]
"Publicly available from the UCI Machine Learning Repository (University of California, 2017) and Delve (Ghahramani, 1996), these datasets have been used to benchmark comparative models in recent related literature (see (Hernández-Lobato & Adams, 2015), (Gal & Ghahramani, 2015), (Bui et al., 2016) and (Li & Gal, 2017)).",4. Experiments and Results,[0],[0]
"We report results using standard metrics, and also propose useful upper and lower bounds to normalize these metrics for an easier interpretation in Section 4.2.
",4. Experiments and Results,[0],[0]
"Our qualitative results include the toy dataset in Figure 1 in the style of (Karpathy, 2015), a new visualization of uncertainty quality that plots test errors sorted by predicted variance (Figure 2 and Appendix), and image segmentation results (Figure 2 and Appendix).",4. Experiments and Results,[0],[0]
"We evaluate uncertainty quality based on two standard metrics, described below: Predictive Log Likelihood (PLL) and Continuous Ranked Probability Score (CRPS).",4.1. Metrics,[0],[0]
"To improve the interpretability of the metrics, we propose to normalize them by upper and lower bounds.
",4.1. Metrics,[0],[0]
Predictive Log Likelihood (PLL),4.1. Metrics,[0],[0]
"Predictive Log Likelihood is widely accepted as the main uncertainty quality metric for regression (Hernández-Lobato & Adams, 2015; Gal & Ghahramani, 2015; Bui et al., 2016; Li & Gal, 2017).",4.1. Metrics,[0],[0]
A key property of PLL is that it makes no assumptions about the form of the distribution.,4.1. Metrics,[0],[0]
"The measure is defined for a probabilistic model fω(x) and a single observation
we could sample from the implied qθ(ω) as modeled in the Appendix.",4.1. Metrics,[0],[0]
This would alleviate having to store D for use during prediction.,4.1. Metrics,[0],[0]
"In our experiments we used D to sample ω̂i however, and leave the evaluation of the modeled qθ(ω) for future research.
",4.1. Metrics,[0],[0]
"(yi,xi) as:
PLL(fω(x), (yi,xi))",4.1. Metrics,[0],[0]
"= log p(yi|fω(xi))
where p(yi|fω(xi))",4.1. Metrics,[0],[0]
"is the model’s predicted PDF evaluated at yi, given the input xi.",4.1. Metrics,[0],[0]
A more detailed description is given in the Appendix Section 1.5.,4.1. Metrics,[0],[0]
The metric is unbounded and maximized by a perfect prediction (mode at yi) with no variance.,4.1. Metrics,[0],[0]
"As the predictive mode moves away from yi, increasing the variance tends to increase PLL (by maximizing probability mass at yi).",4.1. Metrics,[0],[0]
"While PLL is an elegant measure, it has been criticized for allowing outliers to have an overly negative effect on the score (Selten, 1998).
",4.1. Metrics,[0],[0]
Continuous Ranked Probability Score (CRPS) Continuous Ranked Probability Score is a measure that takes the full predicted PDF into account with less sensitivity to outliers.,4.1. Metrics,[0],[0]
A prediction with low variance that is slightly offset from the true observation will receive a higher score form CRPS than PLL.,4.1. Metrics,[0],[0]
"In order for CRPS to be analytically tractable, we need to assume a Gaussian unimodal predictive distribution.",4.1. Metrics,[0],[0]
"CRPS is defined as
CRPS(fω(xi), (yi, xi))",4.1. Metrics,[0],[0]
= ∫ ∞ −∞,4.1. Metrics,[0],[0]
( F (y)− 1(y ≥ yi) ),4.1. Metrics,[0],[0]
"2 dy
where F (y) is the predictive CDF, and 1(y ≥ yi) = 1 if y ≥ yi and 0 otherwise (for univariate distributions) (Gneiting & Raftery, 2007).",4.1. Metrics,[0],[0]
CRPS is interpreted as the sum of the squared area between the CDF and 0 where y < yi and between the CDF and 1 where y ≥ yi.,4.1. Metrics,[0],[0]
A perfect prediction with no variance yields a CRPS of 0; for all other cases the value is larger.,4.1. Metrics,[0],[0]
CRPS has no upper bound.,4.1. Metrics,[0],[0]
It is difficult to interpret the quality of uncertainty from raw PLL and CRPS values.,4.2. Benchmark models and normalized metrics,[0],[0]
We propose to normalize the metrics between useful lower and upper bounds.,4.2. Benchmark models and normalized metrics,[0],[0]
The normalized measures estimate the performance of an uncertainty model between the trivial solution (constant uncertainty) and optimal uncertainty for each prediction.,4.2. Benchmark models and normalized metrics,[0],[0]
"For the lower bound, we define a baseline that predicts constant variance regardless of input.",4.2. Benchmark models and normalized metrics,[0],[0]
The variance is set to a fixed value that optimizes CRPS on validation data.,4.2. Benchmark models and normalized metrics,[0],[0]
We call this model Constant Uncertainty BN (CUBN).,4.2. Benchmark models and normalized metrics,[0],[0]
"It reflects our best guess of constant variance on test data – thus, any improvement in uncertainty quality over CUBN indicates a sensible estimate of uncertainty.",4.2. Benchmark models and normalized metrics,[0],[0]
"We similarly define a baseline for dropout, Constant Uncertainty Dropout (CUDO).",4.2. Benchmark models and normalized metrics,[0],[0]
"The modeling of variance (uncertainty) by MCBN and CUBN are visualized in Figure 1.
",4.2. Benchmark models and normalized metrics,[0],[0]
An upper bound on uncertainty performance can also be defined for a probabilistic model f with respect to CRPS or PLL.,4.2. Benchmark models and normalized metrics,[0],[0]
"For each observation (yi, xi), a value
for the predictive variance Ti can be chosen that maximizes PLL or minimizes CRPS10.",4.2. Benchmark models and normalized metrics,[0],[0]
"Using CUBN as a lower bound and the optimized CRPS score as the upper bound, uncertainty estimates can be normalized between these bounds (1 indicating optimal performance, and 0 indicating same performance as fixed uncertainty).",4.2. Benchmark models and normalized metrics,[0],[0]
"We call this normalized measure CRPS =
CRPS(f,(yi,xi))−CRPS(fCU ,(yi,xi)) minT CRPS(f,(yi,xi))−CRPS(fCU ,(yi,xi)) × 100, and the PLL analogue PLL = PLL(f,(yi,xi))−PLL(fCU ,(yi,xi))maxT PLL(f,(yi,xi))−PLL(fCU ,(yi,xi))×100.",4.2. Benchmark models and normalized metrics,[0],[0]
"Our evaluation compares MCBN to MCDO (Gal & Ghahramani, 2015) and MNF (Louizos & Welling, 2017) using the datasets and metrics described above.",4.3. Test setup,[0],[0]
"Our setup is similar to (Hernández-Lobato & Adams, 2015), which was also followed by (Gal & Ghahramani, 2015).",4.3. Test setup,[0],[0]
"However, our comparison implements a different hyperparameter selection, allows for a larger range of dropout rates, and uses larger networks with two hidden layers.
",4.3. Test setup,[0],[0]
"For the regression task, all models share a similar architecture: two hidden layers with 50 units each, and ReLU activations, with the exception of Protein Tertiary Structure dataset (100 units per hidden layer).",4.3. Test setup,[0],[0]
Inputs and outputs were normalized during training.,4.3. Test setup,[0],[0]
Results were averaged over five random splits of 20% test and 80% training and cross-validation (CV) data.,4.3. Test setup,[0],[0]
"For each split, 5-fold CV by grid search with a RMSE minimization objective was used to find training hyperparameters and optimal n.o. epochs, out of a maximum of 2000.",4.3. Test setup,[0],[0]
"For BN-based models, the hyperparameter grid consisted of a weight decay factor ranging from 0.1 to 1−15 by a log 10 scale, and a batch size range from 32 to 1024 by a log 2 scale.",4.3. Test setup,[0],[0]
"For DO-based models, the hyperparameter grid consisted of the same weight decay range, and dropout probabilities in {0.2, 0.1, 0.05, 0.01, 0.005, 0.001}.",4.3. Test setup,[0],[0]
DO-based models used a batch size of 32 in all evaluations.,4.3. Test setup,[0],[0]
"For MNF11, the n.o. epochs was optimized, the batch size was set to 100, and early stopping test performed each epoch (compared to every 20th for MCBN, MCDO).
",4.3. Test setup,[0],[0]
"For MCBN and MCDO, the model with optimal training hyperparameters was used to optimize τ numerically.",4.3. Test setup,[0],[0]
"This optimization was made in terms of average CV CRPS for MCBN, CUBN, MCDO, and CUDO respectively.
",4.3. Test setup,[0],[0]
Estimates for the predictive distribution were obtained by taking T = 500 stochastic forward passes through the network.,4.3. Test setup,[0],[0]
"For each split, test set evaluation was done 5 times with different seeds.",4.3. Test setup,[0],[0]
"Implementation was done in TensorFlow with the Adam optimizer and a learning rate of 0.001.
",4.3. Test setup,[0],[0]
"10Ti can be found analytically for PLL, but must be found numerically for CRPS.
",4.3. Test setup,[0],[0]
"11Where we used an adapted version of the authors’ code.
",4.3. Test setup,[0],[0]
"For the image classification test we use CIFAR10 (Krizhevsky & Hinton, 2009) which includes 10 object classes with 5,000 and 1,000 images in the training and test sets, respectively.",4.3. Test setup,[0],[0]
Images are 32x32 RGB format.,4.3. Test setup,[0],[0]
"We trained a ResNet32 architecture with a batch size of 32, learning rate of 0.1, weight decay of 0.0002, leaky ReLU slope of 0.1, and 5 residual units.",4.3. Test setup,[0],[0]
"SGD with momentum was used as the optimizer.
",4.3. Test setup,[0],[0]
Code for reproducing our experiments is available at https://github.com/icml-mcbn/mcbn.,4.3. Test setup,[0],[0]
The regression experiment comparing uncertainty quality is summarized in Table 1.,4.4. Test results,[0],[0]
"We report CRPS and PLL, expressed as a percentage, which reflects how close the model is to the upper bound, and check to see if the model significantly exceeds the lower bound using a one sample t-test (significance level is indicated by *’s).",4.4. Test results,[0],[0]
"Further details are provided in Appendix Section 1.7.
",4.4. Test results,[0],[0]
"In Figure 2 (left), we present a novel visualization of uncertainty quality for regression problems.",4.4. Test results,[0],[0]
Data are sorted by estimated uncertainty in the x-axis.,4.4. Test results,[0],[0]
"Grey dots show the errors in model predictions, and the shaded areas show the model uncertainty.",4.4. Test results,[0],[0]
A running mean of the errors appears as a gray line.,4.4. Test results,[0],[0]
"If uncertainty estimation is working well, a correlation should exist between the mean error (gray line) and uncertainty (shaded area).",4.4. Test results,[0],[0]
"This indicates that the uncertainty estimation recognizes samples with larger (or smaller) potential for predictive errors.
",4.4. Test results,[0],[0]
We applied MCBN on the image classification task of CIFAR10.,4.4. Test results,[0],[0]
The baseline in this case is the softmax distribution using the moving average for BN units.,4.4. Test results,[0],[0]
Log likelihood (PLL) is the metric used to compare with the baseline.,4.4. Test results,[0],[0]
"The baseline achieves a PLL of -0.32 on the test set, while MCBN obtains a PLL of -0.28.",4.4. Test results,[0],[0]
Table 2 shows the performance of MCBN when using different number of stochastic forward passes (the MCBN batchsize is fixed to the training batch size at 32).,4.4. Test results,[0],[0]
"PLL improves as the number of the stochastic passes increases, until it is significantly better than the softmax baseline.
",4.4. Test results,[0],[0]
"To demonstrate how model uncertainty can be obtained from an existing network with minimal effort, we applied MCBN to an image segmentation task using Bayesian SegNet with the main CamVid and PASCAL-VOC models in (Kendall et al., 2015).",4.4. Test results,[0],[0]
We simply ran multiple forward passes with different mini-batches randomly taken from the train set.,4.4. Test results,[0],[0]
The models obtained from the online model zoo have BN blocks after each layer.,4.4. Test results,[0],[0]
We recalculate mean and variance for the first 2 blocks only and use the training statistics for the rest of the blocks.,4.4. Test results,[0],[0]
"Mini-batches of size 10 and 36 were used for CamVid and VOC respectively
due to memory limits.",4.4. Test results,[0],[0]
"The results in Figure 2 (right) were obtained from 20 stochastic forward passes, showing high uncertainty near object boundaries.",4.4. Test results,[0],[0]
"The VOC results are more appealing because of larger mini-batches.
",4.4. Test results,[0],[0]
We provide additional experimental results in the Appendix.,4.4. Test results,[0],[0]
Appendix Tables 2 and 3 show the mean CRPS and PLL values from the regression experiment.,4.4. Test results,[0],[0]
Table 4 provides the raw CRPS and PLL scores.,4.4. Test results,[0],[0]
In Table 5 we provide RMSE results of the MCBN and MCDO networks in comparison with non-stochastic BN and DO networks.,4.4. Test results,[0],[0]
These results indicate that the procedure of multiple forward passes in MCBN and MCDO show slight improvements in the predictive accuracy compared to their nonBayesian counterparts.,4.4. Test results,[0],[0]
"In Tables 6 and 7, we investigate the effect of varying batch size while keeping other hyperparameters fixed.",4.4. Test results,[0],[0]
"We see that performance deteriorates with small batch sizes (≤16), a known issue of BN (Ioffe, 2017).",4.4. Test results,[0],[0]
"Similarly, results varying the number of stochastic forward passes T is reported in Tables 8 and 9.",4.4. Test results,[0],[0]
"While performance benefits from large T , in some cases T = 50 (i.e. 1/10 of T in the main evaluation) performs well.",4.4. Test results,[0],[0]
Uncertainty-error plots for all the datasets are provided in the Appendix.,4.4. Test results,[0],[0]
"The results presented in Tables 1-2 and Appendix Tables 2-9 indicate that MCBN generates meaningful uncertainty
estimates that correlate with actual errors in the model’s prediction.",5. Discussion,[0],[0]
"In Table 1, we show statistically significant improvements over CUBN in the majority of the datasets, both in terms of CRPS and PLL.",5. Discussion,[0],[0]
The visualizations in Figure 2 and in the Appendix Figures 2-3 show correlations between the estimated model uncertainty and errors of the network’s predictions.,5. Discussion,[0],[0]
"We perform the same experiments using MCDO and MNF, and find that MCBN generally performs on par with both methods.",5. Discussion,[0],[0]
"Looking closer, MCBN outperforms MCDO and MNF in more cases than not, measured by CRPS.",5. Discussion,[0],[0]
"However, care must be used.",5. Discussion,[0],[0]
"The learned parameters are different, leading to different predictive means and confounding direct comparison.
",5. Discussion,[0],[0]
The results on the Yacht Hydrodynamics dataset seem contradictory.,5. Discussion,[0],[0]
"The CRPS score for MCBN are extremely negative, while the PLL score is extremely positive.",5. Discussion,[0],[0]
The opposite trend is observed for MCDO.,5. Discussion,[0],[0]
"To add to the puzzle, the visualization in Figure 2 depicts an extremely promising uncertainty estimation that models the predictive errors with high fidelity.",5. Discussion,[0],[0]
"We hypothesize that this strange behavior is due to the small size of the data set, which only contains 60 test samples, or due to the Gaussian assumption of CRPS.",5. Discussion,[0],[0]
"There is also a large variability in the model’s accuracy on this dataset, which further confounds the measurements for such limited data.
",5. Discussion,[0],[0]
"One might criticize the overall quality of uncertainty estimates observed in all the models we tested, due to the magnitude of CRPS and PLL in Table 1.",5. Discussion,[0],[0]
The scores rarely exceed 10% improvement over the lower bound.,5. Discussion,[0],[0]
"However, we caution that these measures should be taken in context.",5. Discussion,[0],[0]
"The upper bound is very difficult to achieve in practice – it is optimized for each test sample individually – and the lower bound is a quite reasonable estimate.
",5. Discussion,[0],[0]
"The study of MCBN sensitivity to batch size revealed that a certain batch size is required for the best performance, dependent on the data.",5. Discussion,[0],[0]
"When doing inference on a GPU, large
batch sizes may cause memory issues for cases where the input is large and the network has a large number of parameters, as is common for state-of-the-art image classification networks.",5. Discussion,[0],[0]
"However, there are various workarounds to this problem.",5. Discussion,[0],[0]
"One can store BN statistics, instead of batches, to reduce memory issues.",5. Discussion,[0],[0]
"Furthermore, we can use the Gaussian estimate of the BN statistics as discussed previously, which makes memory and computation extremely efficient.",5. Discussion,[0],[0]
"In this work, we have shown that training a deep network using batch normalization is equivalent to approximate inference in Bayesian models.",6. Conclusion,[0],[0]
"We show evidence that the uncertainty estimates from MCBN correlate with actual errors in the model’s prediction, and are useful for practical
tasks such as regression, image classification, and image segmentation.",6. Conclusion,[0],[0]
"Our experiments show that MCBN yields a significant improvement over the optimized constant uncertainty baseline, on par with MCDO and MNF.",6. Conclusion,[0],[0]
"Our evaluation also suggests new normalized metrics based on useful upper and lower bounds, and a new visualization which provides an intuitive explanation of uncertainty quality.
",6. Conclusion,[0],[0]
"Finally, it should be noted that over the past few years, batch normalization has become an integral part of most – if not all – cutting edge deep networks.",6. Conclusion,[0],[0]
We have shown that it is possible to obtain meaningful uncertainty estimates from existing models without modifying the network or the training procedure.,6. Conclusion,[0],[0]
"With a few lines of code, robust uncertainty estimates can be obtained by computing the variance of multiple stochastic forward passes through an existing network.",6. Conclusion,[0],[0]
We show that training a deep network using batch normalization is equivalent to approximate inference in Bayesian models.,abstractText,[0],[0]
"We further demonstrate that this finding allows us to make meaningful estimates of the model uncertainty using conventional architectures, without modifications to the network or the training procedure.",abstractText,[0],[0]
Our approach is thoroughly validated by measuring the quality of uncertainty in a series of empirical experiments on different tasks.,abstractText,[0],[0]
"It outperforms baselines with strong statistical significance, and displays competitive performance with recent Bayesian approaches.",abstractText,[0],[0]
Bayesian Uncertainty Estimation for Batch Normalized Deep Networks,title,[0],[0]
"Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 64–74, Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics",text,[0],[0]
"For the majority of the state-of-the-art parsers that routinely reach ninety percent performance plateau in capturing tree structures, the question of what next crucially arises.",1 Introduction,[0],[0]
"Indeed, it has long been thought that the bottleneck preventing the advent of accurate syntax-to-semantic interfaces lies in the quality of the preceding phase of analysis: the better the parse, the better the output.",1 Introduction,[0],[0]
"The truth is that most of the structures used to train current parsing models are degraded versions of a more informative data set: the Wall Street journal section of the Penn treebank (PTB, (Marcus et al., 1993)) which is often stripped of its richer set of annotations (i.e. traces and functional labels are removed), while, for reasons of efficiency and availability, projective dependency trees are often given preference over richer graph structures (Nivre and Nilsson, 2005; Sagae
and Tsujii, 2008).",1 Introduction,[0],[0]
"This led to the emergence of surface syntax-based parsers (Charniak, 2000; Nivre, 2003; Petrov et al., 2006) whose output cannot by themselves be used to extract full-fledged predicateargument structures.",1 Introduction,[0],[0]
"For example, control verb constructions, it-cleft structures, argument sharing in ellipsis coordination, etc. are among the phenomena requiring a graph to be properly accounted for.",1 Introduction,[0],[0]
The dichotomy between what can usually be parsed with high accuracy and what lies in the deeper syntactic description has initiated a line of research devoted to closing the gap between surface syntax and richer structures.,1 Introduction,[0],[0]
"For most of the previous decade, the term deep syntax was used for rich parsing models built upon enriched versions of a constituency treebank, either with added HPSG or LFG annotation or CCG (almost) full rewrites (Miyao and Tsujii, 2005; Cahill et al., 2004; Hockenmaier, 2003).",1 Introduction,[0],[0]
"Its use now spreads by misnomer to models that provide more abstract structures, capable of generalizing classical functional labels to more semantic (in a logical view) arguments, potentially capable of neutralizing diathesis distinctions and of providing accurate predicate-argument structures.",1 Introduction,[0],[0]
"Although the building of syntax-to-semantic interface seems inextricably linked to an efficient parsing stage, inspirational works on semantic role labelling (Toutanova et al., 2005) and more recently on broad coverage semantic parsing (Du et al., 2014) that provide stateof-the-art results without relying on surface syntax, lead us to question the usefulness of syntactic parses for predicate-argument structure parsing.
",1 Introduction,[0],[0]
"In this study, we investigate the impact of syntactic features on a transition-based graph parser by testing on two treebanks.",1 Introduction,[1.0],"['In this study, we investigate the impact of syntactic features on a transition-based graph parser by testing on two treebanks.']"
"We take advantage of the recent release for the SemEval 2014 shared task on semantic dependency parsing, by Oepen et
64
al. (2014) of two semantic-based treebanks, derived from two HPSG resources, the DeepBank (DM, (Flickinger et al., 2012)) and the Enju’s predicate argument structure (PAS, (Miyao and Tsujii, 2005)), to investigate the impact of syntactic features on a transition-based graph parser.",1 Introduction,[0],[0]
Our results show that surface syntactic features significantly improve the parsing of predicate-argument structures.,1 Introduction,[0],[0]
"More specifically, we show that adding syntactic context improves the recognition of long distance dependencies and elliptical constructions.",1 Introduction,[0],[0]
"We finally discuss the usefulness of our approach, when applied on a second-order model based on dual decomposition (Martins and Almeida, 2014), showing that our use of syntactic features enhances this model accuracy and provides state-of-the-art performance.",1 Introduction,[0],[0]
"DeepBank Corpus Semantic dependency graphs in the DM Corpus are the result of a two-step simplification of the underspecified logical-form meaning representations, based on Minimal Recursion Semantic (MRS, (Copestake et al., 1995; Copestake et al., 2005)), derived from the manually annotated DeepBank treebank (Flickinger et al., 2012).",2 Deep Syntax and Underspecified Semantic Corpora,[1.0],"['DeepBank Corpus Semantic dependency graphs in the DM Corpus are the result of a two-step simplification of the underspecified logical-form meaning representations, based on Minimal Recursion Semantic (MRS, (Copestake et al., 1995; Copestake et al., 2005)), derived from the manually annotated DeepBank treebank (Flickinger et al., 2012).']"
"First, Oepen and Lønning (2006) define a conversion from original MRS formulae to variable-free Elementary Dependency Structures (EDS), which (a) maps each predication in the MRS logical-form meaning representation to a node in a dependency graph and (b) transforms argument relations represented by shared logical variables into directed dependency links between graph nodes.",2 Deep Syntax and Underspecified Semantic Corpora,[0],[0]
"Then, in a second conversion step, the EDS graphs are further reduced into strict bi-lexical form, i.e. a set of directed, binary dependency relations holding exclusively between lexical units (Ivanova et al., 2012).",2 Deep Syntax and Underspecified Semantic Corpora,[0],[0]
"Even though both conversion steps are, by design, lossy, DM semantic dependency graphs present a true subset of the information encoded in the full, original MRS data set.
",2 Deep Syntax and Underspecified Semantic Corpora,[0],[0]
"Predicate-Argument Structure Corpus Enju Predicate-Argument Structures (PAS Corpus) are derived from the automatic HPSG-style annotation of the Penn Treebank (Miyao and Tsujii, 2004) that was primarily used for the development of the Enju parsing system (Miyao and Tsujii, 2005).",2 Deep Syntax and Underspecified Semantic Corpora,[1.0],"['Predicate-Argument Structure Corpus Enju Predicate-Argument Structures (PAS Corpus) are derived from the automatic HPSG-style annotation of the Penn Treebank (Miyao and Tsujii, 2004) that was primarily used for the development of the Enju parsing system (Miyao and Tsujii, 2005).']"
"The
PAS data set is an extraction of predicate-argument structures from the Enju HPSG treebank and contains word-to-word semantic dependencies.",2 Deep Syntax and Underspecified Semantic Corpora,[0],[0]
"Each dependency type is made of two elements: a coarse part-of-speech of the head predicate dependent (e.g. verb and adjective), and the argument (e.g. ARG1 and ARG2).
",2 Deep Syntax and Underspecified Semantic Corpora,[0.999999964752272],"['Each dependency type is made of two elements: a coarse part-of-speech of the head predicate dependent (e.g. verb and adjective), and the argument (e.g. ARG1 and ARG2).']"
"Although both are derived from HSPG resources (a hand-crafted grammar for DM, a treebank-based one for PAS), they differ in their core linguistic choices (functional heads vs lexical heads, coordination scheme, etc.) leading to different views of the predicate argument structure for the same sentence (Ivanova et al., 2012).",2 Deep Syntax and Underspecified Semantic Corpora,[0],[0]
"Thus, even though both corpora may appear to contain a similar number of dependency labels, as shown in Table 1, their annotation schemes depict a deeply divergent linguistic reality exposed by two very different distributions.",2 Deep Syntax and Underspecified Semantic Corpora,[0],[0]
"In DM, 9 labels account for almost 95% of all dependencies whereas a label set twice as large covers the same percentage for PAS, as shown in Table 2.",2 Deep Syntax and Underspecified Semantic Corpora,[0],[0]
"Furthermore, semantically empty elements are widespread in the DeepBank (around 21.5%), compared to a low rate of 4.3% in PAS.",2 Deep Syntax and Underspecified Semantic Corpora,[0],[0]
"In other words, the latter is somewhat more dense and consequently more syntactic.",2 Deep Syntax and Underspecified Semantic Corpora,[0],[0]
"This is due to the fact that PAS integrates markers for infinitives, auxiliaries, and most punctuation marks into its graphs, whereas DM considers them as semantically void.",2 Deep Syntax and Underspecified Semantic Corpora,[0],[0]
DM corpus is clearly heading toward more semantic analysis while the PAS corpus aims at providing a more abstract deep syntax analysis than regular surface syntax trees.,2 Deep Syntax and Underspecified Semantic Corpora,[1.0],['DM corpus is clearly heading toward more semantic analysis while the PAS corpus aims at providing a more abstract deep syntax analysis than regular surface syntax trees.']
Both treebanks are used in their bilexical dependency formats.,2 Deep Syntax and Underspecified Semantic Corpora,[0],[0]
"Shift-reduce transition-based parsers essentially rely on configurations formed of a stack and a buffer, with stack transitions used to move from a configuration to the next one, until reaching a final configuration.",3 Transition-based Graphs Parsing,[1.0],"['Shift-reduce transition-based parsers essentially rely on configurations formed of a stack and a buffer, with stack transitions used to move from a configuration to the next one, until reaching a final configuration.']"
"Following Kübler et al. (2009), we define a configuration by c = (σ, β,A) where σ denotes a stack of words wi, β a buffer of words, and A a set of dependency arcs of the form (wi, r, wj), with wi the head, wj the dependent, and r a label in some set R. As shown in Figure 1, besides the usual shift and reduce transitions (lR & rR) of the arc-standard strategy, we introduced the new left and right attach (lA & rA) transitions for adding new dependencies (while keeping the dependent on the stack) and a pop0 transition to remove a word from the stack after attachment of its dependents.",3 Transition-based Graphs Parsing,[1.0],"['Following Kübler et al. (2009), we define a configuration by c = (σ, β,A) where σ denotes a stack of words wi, β a buffer of words, and A a set of dependency arcs of the form (wi, r, wj), with wi the head, wj the dependent, and r a label in some set R. As shown in Figure 1, besides the usual shift and reduce transitions (lR & rR) of the arc-standard strategy, we introduced the new left and right attach (lA & rA) transitions for adding new dependencies (while keeping the dependent on the stack) and a pop0 transition to remove a word from the stack after attachment of its dependents.']"
"All the transitions that add an edge must also satisfy the condition that the newly created edge does not introduce a cycle or
multiple edges between the same pair of nodes.",3 Transition-based Graphs Parsing,[0],[0]
"It is to be noted that the pop0 action may also be used to remove words with no heads.
",3 Transition-based Graphs Parsing,[0.9999999530951418],['It is to be noted that the pop0 action may also be used to remove words with no heads.']
"We base our work on the the DAG parser of Sagae and Tsujii (2008) (henceforth S&T) which we extended with the set of actions displayed above (Figure 1) to cope with partially connected planar graphs, and we gave it the ability to take advantage of an extended set of features.",3 Transition-based Graphs Parsing,[1.0],"['We base our work on the the DAG parser of Sagae and Tsujii (2008) (henceforth S&T) which we extended with the set of actions displayed above (Figure 1) to cope with partially connected planar graphs, and we gave it the ability to take advantage of an extended set of features.']"
"Finally, for efficiency reasons (memory consumption and speed), we replaced the original Maxent model with an averaged structured perceptron (Freund and Schapire, 1999; Collins, 2002).",3 Transition-based Graphs Parsing,[0],[0]
We define Wordβi (resp.,4.1 Baseline Features,[0],[0]
Lemmaβi and POSβi) as the word (resp.,4.1 Baseline Features,[0],[0]
lemma and part-of-speech) at position i in the queue.,4.1 Baseline Features,[0],[0]
"The same goes for σi, which is the position i in the stack.",4.1 Baseline Features,[0],[0]
"Let di,j be the distance between Wordσi and Wordσj .",4.1 Baseline Features,[0],[0]
"We also define d′i,j , the distance between Wordβi and Wordσj .",4.1 Baseline Features,[0],[0]
"In addition, we define leftPOSσi (resp. leftLabelσi)",4.1 Baseline Features,[0],[0]
the part-of-speech (resp.,4.1 Baseline Features,[0],[0]
"the label if any) of the word immediately to the left of σi, and the same goes for rightPOSσi (resp.",4.1 Baseline Features,[0],[0]
rightLabelσi).,4.1 Baseline Features,[0],[0]
"Finally, a is the previous action predicted by the parser.",4.1 Baseline Features,[0],[0]
Table 3 lists our baseline features.,4.1 Baseline Features,[0],[0]
"Xσi, σj , σk means that we use Xσi, Xσj , Xσk as unigram features as well as bigram and trigram features.",4.1 Baseline Features,[0],[0]
"We combined the previous features with different types of syntactic features (constituents and dependencies), our intuition being that syntax and semantic are interdependent, and that syntactic features should therefore help predicate-argument parsing.",4.2 Syntactic Features,[1.0],"['We combined the previous features with different types of syntactic features (constituents and dependencies), our intuition being that syntax and semantic are interdependent, and that syntactic features should therefore help predicate-argument parsing.']"
"In fact, we considered that the low density of syntactic information (compared to regular dependency treebanks) would be counterbalanced by
adding more context.",4.2 Syntactic Features,[0],[0]
"We considered the following pieces of information in particular.
",4.2 Syntactic Features,[0],[0]
Constituent Tree Fragments These consist of fragments of syntactic trees predicted by the Petrov et al. (2006) parser in a 10-way jackknife setting.,4.2 Syntactic Features,[1.0],['Constituent Tree Fragments These consist of fragments of syntactic trees predicted by the Petrov et al. (2006) parser in a 10-way jackknife setting.']
"They can be used as enhanced POS or as features.
",4.2 Syntactic Features,[0],[0]
Spinal Elementary Trees,4.2 Syntactic Features,[0],[0]
"A full set of parses was reconstructed from the tree fragments using a slightly tweaked version of the CONLL 2009 shared task processing tools (Hajič et al., 2009).",4.2 Syntactic Features,[0],[0]
"We then extracted a spine grammar (Seddah, 2010) using the head percolation table of the Bikel (2002) parser, slightly modified to avoid certain determiners being marked as heads in certain configurations.",4.2 Syntactic Features,[0],[0]
"The resulting spines were assigned in a deterministic way (red part in Figure 2).
",4.2 Syntactic Features,[0],[0]
"Predicted MATE Dependency Labels These consist of the dependency labels predicted by the MATE parser (Bohnet, 2010), trained on a Stanford surface dependency version of the Penn Treebank.",4.2 Syntactic Features,[0],[0]
We combined the labels with a distance δ =,4.2 Syntactic Features,[0],[0]
t − h where t is the token position and h the head position (brown labels and δ in Figure 2).,4.2 Syntactic Features,[0],[0]
"In addition, we expanded these features with the part-of-speech of the head of a given token (HPOS).",4.2 Syntactic Features,[1.0],"['In addition, we expanded these features with the part-of-speech of the head of a given token (HPOS).']"
"The idea is to evaluate the informativeness of more abstract syntactic features since a <LABEL,HPOS> pair can be seen as generalizing many constituent subtrees.
",4.2 Syntactic Features,[0],[0]
Constituent Head Paths.,4.2 Syntactic Features,[0],[0]
"Inspired by Björkelund et al. (2013), we used MATE dependencies to extract the shortest path between a token and its lexical head and included the path length w (in terms of traversed nodes) as a feature (blue part in Figure 2).",4.2 Syntactic Features,[0],[0]
The global idea is to use the phrase-based features to provide different kinds of syntactic context and the dependency-based features to provide generalisations over the functional label governing a token.,4.2 Syntactic Features,[0],[0]
"The spines are seen as deterministic supertags, bringing a vertical context.
",4.2 Syntactic Features,[0],[0]
"We report, in Table 4, the counts for each syntactic feature on each set.",4.2 Syntactic Features,[0],[0]
Experimental Setup Both DM and PAS treebanks consist of texts from the PTB and which were either automatically derived from the original annotations or annotated with a hand-crafted grammar (see above).,5 Experiments,[0],[0]
"We use them in their bi-lexical dependency format, aligned at the token level as provided by Oepen et al. (2014)1.",5 Experiments,[0],[0]
"The following split is used: sections 00-19 for training, 20 for the dev. set and 21 for test2.",5 Experiments,[0],[0]
"All predicted parses are evaluated against the gold standard with labeled precision, recall and f-measure metrics.
",5 Experiments,[0],[0]
"Results Our experiments are based on the evaluation of the combinations of the 4 main types of syntactic features described in section 4: tree fragments (BKY), predicted mate dependencies (BN) and their extension with POS heads (BN(HPOS)), spinal elementary trees (SPINES) and head paths (PATHS).
",5 Experiments,[0],[0]
The results are shown in Tables 5 and 6.,5 Experiments,[0],[0]
All improvements from the baseline are significant with a p-value p < 0.05.,5 Experiments,[1.0],['All improvements from the baseline are significant with a p-value p < 0.05.']
"There was no significant difference of the same p value between our two best mod-
1This alignment entailed the removal of all unparsed sentences.
",5 Experiments,[0],[0]
"2We used the same unusual split as in (Oepen et al., 2014) to be able to conduct meaningful comparisons with others.
",5 Experiments,[0],[0]
els for each of the treebanks.,5 Experiments,[0],[0]
"3
As expected from the rapid overview of our datasets exposed earlier in section 2, the use of each single feature alone increases the performance over the baseline by 0.5 points for the BN feature in DM to 1.44 for PATHS, and by 1.10 for the SPINES to 1.85 for the PATHS features in PAS.",5 Experiments,[0],[0]
"Looking at the conjunction of two classes in the DM table, it seems that dependency-based features benefit from the extra context brought by constituents features, reaching an increase of 2.21 points for BKY+BN(HPOS).",5 Experiments,[1.0],"['Looking at the conjunction of two classes in the DM table, it seems that dependency-based features benefit from the extra context brought by constituents features, reaching an increase of 2.21 points for BKY+BN(HPOS).']"
"Interestingly, the maximum gain is brought by the addition of topologically different phrase-based features such as SPINES (+2.80, inherently vertical) or BKY (+2.76, often wider) to the previous best.",5 Experiments,[1.0],"['Interestingly, the maximum gain is brought by the addition of topologically different phrase-based features such as SPINES (+2.80, inherently vertical) or BKY (+2.76, often wider) to the previous best.']"
"Regarding PAS, similar trends can be observed, although the gains are more distributed.",5 Experiments,[0],[0]
"As opposed to DM where the conjunction of more features led to inferior results, here using a four-features class provides the second best improvement (ALL(HPOS) = BKY+BN(HPOS)+SPINES+PATHS), +2.82) while removing the SPINES slightly increases the score (+2.92).",5 Experiments,[0],[0]
"In fact, adding too many features to the model slightly degrades our scores, at least with regard to DM which has a larger label set than PAS.
Results show that syntactic information improves our parser performances.",5 Experiments,[0],[0]
"As each feature represents one unique piece of information, they benefit from being combined in order to provide more structural information.",5 Experiments,[0],[0]
"Following Mcdonald and Nivre (2007), we conducted an error analysis based on the two best models and the baseline for each corpus.",6 Results Analysis,[0],[0]
"As shown in section 5, syntactic features greatly improve semantic parsing.",6 Results Analysis,[0],[0]
"However, it is interesting to explore more precisely what kind of syntactic information boosts or penalizes our predictions.",6 Results Analysis,[0],[0]
"We consider, among other factors, the impact in terms of distance between the head and the dependent (edge length) and the labels.",6 Results Analysis,[1.0],"['We consider, among other factors, the impact in terms of distance between the head and the dependent (edge length) and the labels.']"
"We also explore several linguistic phenomena well known to be difficult to recover.
",6 Results Analysis,[0],[0]
"3We tested the statistical significance between our best models and the baseline with the paired bootstrap test (BergKirkpatrick et al., 2012).",6 Results Analysis,[0],[0]
"In Figures 3(a) and 4(a), we detail the scores for the five most frequent labels.
",6.1 Breakdown by Labels,[0],[0]
"As observed in the charts, the scores are higher for the most frequent labels on both corpora, especially when dealing with verbal arguments.",6.1 Breakdown by Labels,[0],[0]
"There are also two interesting cases for DM: the predictions of _and_c and ARG3 edges show an improvement by at least 5 points (Figures 3(b) & 4(b)), showing that the recovery of coordination structures and the disambiguation of less frequent or more distant arguments is achieved by adding non-local features.",6.1 Breakdown by Labels,[0],[0]
Longer sentences are notoriously difficult to parse for most parsing models.,6.2 Length Factor,[0],[0]
"Figures 3(c) and 4(c) show the F1-measure of our models with respect to sentence length (in bins of size 10: 1-10, 11-20, etc.)",6.2 Length Factor,[0],[0]
"for the DM and PAS corpora.
",6.2 Length Factor,[0],[0]
It is worth noting that we greatly improve the scores for longer sentences.,6.2 Length Factor,[0],[0]
"The use of paths and of the output of a graph-based parser (Bohnet, 2010) favors the capture of complex dependencies and enhances the learning of these constructions for our local transition-based parser.",6.2 Length Factor,[0],[0]
"However, we also observe that the features are not able to completely stop the loss of F1-score for longer sentences.",6.2 Length Factor,[0],[0]
"The slopes of the curves in the different charts show the same trend: the longer the sentence, the lower the score.",6.2 Length Factor,[0],[0]
"We now center our analysis on long-distance dependencies (LDDs), by focusing our attention on edges length, i.e. the distance between two words linked by an edge.",6.3 Linguistic Factors,[0],[0]
"We will then concentrate on subject ellipsis, in a treatment of LDDs more similar to the linguistic definition of Cahill et al. (2004).
",6.3 Linguistic Factors,[0],[0]
Long-distance Dependencies (LDDs),6.3 Linguistic Factors,[0],[0]
"For many systems, LDDs are difficult to recover because they are generally under-represented in the training corpus and the constructions involved in LDDs often require deep linguistic knowledge to be recovered.",6.3 Linguistic Factors,[0],[0]
"In
Figure 7, we report the distribution of long-distance dependencies by bins of size 5 up to 40.",6.3 Linguistic Factors,[0],[0]
They only account for 15% of all the dependencies in both corpora.,6.3 Linguistic Factors,[0],[0]
The longest dependencies consist of the first and second arguments of the verb as well as coordination links.,6.3 Linguistic Factors,[0],[0]
"In the case of elided coordination structures, we have long-distance dependencies when two coordinated verbs share the same first or second argument, which explains the distribution of lengths.
BINS 5-10 11-15 16-20 21-25 26-40
DM 2907 734 329 141 92 PAS 3705 1007 408 175 127
Table 7: Number of LDDs edges (dev. set).
",6.3 Linguistic Factors,[0],[0]
"As outlined in Figures 3(d) and 4(d), we can see that without structural information such as spines, surfacic dependencies or paths, the longest dependencies have low F1-scores.",6.3 Linguistic Factors,[0],[0]
"When using these features, our models tend to perform better, with a gain of up to 25 points for high-dependency lengths (bins between 16-20 and 21-25).
",6.3 Linguistic Factors,[0.9999999992285292],"['When using these features, our models tend to perform better, with a gain of up to 25 points for high-dependency lengths (bins between 16-20 and 21-25).']"
"In Table 8, we show the global improvement when considering edge lengths between 5 and 40.",6.3 Linguistic Factors,[0],[0]
"For both corpora, the improvement is the same (around 9 points), showing that structural information is the key to better predictions.",6.3 Linguistic Factors,[0],[0]
"Looking into this improvement more closely, we found that PATHS combined with BN tend to be crucial, whereas SPINES
may sometimes penalize the models.",6.3 Linguistic Factors,[0],[0]
"Even though, BN+SPINES+PATHS is the best model for DM, a spine is only a partial projection which lacks attachment information.",6.3 Linguistic Factors,[0],[0]
"Spines alone only therefore provide a local context and are unable to cope well with LDDs.
",6.3 Linguistic Factors,[0],[0]
Coordination Structures We now focus on structures with subject ellipsis.,6.3 Linguistic Factors,[0],[0]
"We extracted them by using a simple graph pattern, i.e. two verbs with a shared ARG1 and a coordination dependency.
",6.3 Linguistic Factors,[0],[0]
Our best models’ scores are displayed in Tables 9.,6.3 Linguistic Factors,[0],[0]
"Once again, our models improve the F1 score, but not in the same proportion.",6.3 Linguistic Factors,[0],[0]
DM considers the conjunction as a semantically empty word and attaches an edge _and_c between the two verbs to mark the coordination.,6.3 Linguistic Factors,[0],[0]
"Consequently this edge is more difficult to predict, because it is less informative, our baseline model relying on tokens, lemmas and POS.
",6.3 Linguistic Factors,[0],[0]
We note that the difference in the number of evaluated dependencies in both corpora comes from an annotation scheme divergence between PAS and DM regarding subject ellipsis.,6.3 Linguistic Factors,[0],[0]
"DM opts for coordinate structures with a chain of dependencies rooted at the first conjunct, the coordinating conjunctions being therefore semantically empty.",6.3 Linguistic Factors,[0],[0]
"In PAS, the final coordinating conjunction and each coordinating conjunction is a two-place predicate, taking left and right conjuncts as its arguments.
",6.3 Linguistic Factors,[0],[0]
"The gain of 6.30 points for DM (Table 9(a), resp.",6.3 Linguistic Factors,[0],[0]
"+3 for PAS) indicates that, when an annotation scheme is designed to have many semantically empty words, using syntactic information tends to enhance the parser accuracy.",6.3 Linguistic Factors,[0],[0]
"This gives a clear insight into what type of information is required to
parse semantic graphs: the greater the distance between the head and the dependent, the larger the context needed to disambiguate the attachments.",6.3 Linguistic Factors,[0],[0]
"PAS DM
Overlap +2.87 +2.67",6.4 Ruling out the Structural Factor Bias,[0],[0]
"Rest +2.70 +2.74 It may argued that the improvement we noticed could stem from a potentially strong overlap between surface trees and predicate-argument structures, both in terms of edges and labels.",6.4 Ruling out the Structural Factor Bias,[0.9980597076421902],"['PAS DM Overlap +2.87 +2.67 Rest +2.70 +2.74 It may argued that the improvement we noticed could stem from a potentially strong overlap between surface trees and predicate-argument structures, both in terms of edges and labels.']"
"In fact, the conversion from surfacic parses into predicate-argument structures requires a large amount of edges relabeling (for instance, when nsubj is relabeled to ARG1).",6.4 Ruling out the Structural Factor Bias,[0],[0]
We tested this hypothesis by computing the number of common edges between MATE predictions and DM and PAS.,6.4 Ruling out the Structural Factor Bias,[0],[0]
The overlap corresponds to about 22% of all edges in PAS and 27% in DM.,6.4 Ruling out the Structural Factor Bias,[0],[0]
"Although important, it does not represent the majority of dependencies in our corpora, because most of edges are not present in surface predictions.",6.4 Ruling out the Structural Factor Bias,[0],[0]
We evaluated the improvement of the overlap as well as for the rest.,6.4 Ruling out the Structural Factor Bias,[1.0],['We evaluated the improvement of the overlap as well as for the rest.']
Results show that our best models perform roughly the same on both sets.,6.4 Ruling out the Structural Factor Bias,[0],[0]
"Interestingly, as opposed to PAS’s model, DM’s model performs better on the non-overlap part.",6.4 Ruling out the Structural Factor Bias,[0],[0]
"This suggests that the use of PTB-based features is somehow not optimal when applied on a none PTB-based treebank, such as DM which comes from a handcrafted grammar.",6.4 Ruling out the Structural Factor Bias,[0],[0]
"Our point was to prove that providing more syntactic context, in the form of phrased-based tree fragments and surface dependencies, helps transition-
based parsers to predict predicate-argument structures, especially for LDDs.",7 Discussion,[0],[0]
"Yet, compared to stateof-the-art systems, our results built on the S&T parser score lower than the top performers (Table 10).",7 Discussion,[0],[0]
"However, we are currently extending a more advanced lattice-aware transition-based parser (DSR) with beams (Villemonte De La Clergerie, 2013) that takes advantage of cutting-edge techniques (dynamic programming, averaged perceptron with early updates, etc. following (Goldberg et al., 2013; Huang et al., 2012)) 4, which proves effective by reaching the state-of-the-art on PAS, outperforming Thomson et al. (2014) and second to the model of Martins and Almeida (2014).",7 Discussion,[0],[0]
"5
The point here is that using the same syntactic features as our base system exhibits the same improvement over a now much stronger baseline.",7 Discussion,[0],[0]
"We can conjecture that the ambiguities added by the relative scarcity of the deep annotations is efficiently handled by a more complete exploration of the search space, made possible by beam optimization.
",7 Discussion,[0],[0]
"We can also wonder whether the lower improvement brought to DM parsing by the PTB-based syntactic features does not come from the fact that the DM corpus and the PTB have divergent annotation
4It uses a different set of transitions, notably pop actions instead of left and right reduce, and a swap that allow limited amount of non-planarity.",7 Discussion,[0],[0]
"Such a set raises issues with beams (several paths leading to a same item, final items reached with paths of various lengths, . . . ), overcome by adding a ’noop’ action only applied on final items to balance path lengths.
",7 Discussion,[0],[0]
"5Leaving aside the multiple (19) ensemble models of Du et al. (2014), because of the impracticability of the approach.
schemes.",7 Discussion,[0],[0]
"In that aspect, PTB syntactic features may add some noise to the learning process, because they give more weight to conflicting decisions that led to correct structures in one but not in the other scheme.
",7 Discussion,[0],[0]
"By using features which, to a certain extent, (i) extend the domain of locality available at a given node and (ii) generalize some structural and functional contexts otherwise unavailable, we tried to overcome the main issue of transition-based parsers: they remain local in the sense that they lack a global view of the whole sentence.
",7 Discussion,[0],[0]
"Impact Beyond Transition-based Parser Of course, it can be argued that improving over a somewhat weak baseline is of limited interest.",7 Discussion,[0],[0]
Our point was to investigate how the direct parsing of relatively sparse graph structures would benefit from the inclusion of more context via the use of topologically different syntactic pieces of information.,7 Discussion,[0],[0]
"However in that work, we mostly focused on transition based-parsing, which raises the question of the impact of our feature-set on a much more powerful and state-of-the-art model such as the TURBOSEMANTICPARSER developed by Martins and Almeida (2014).
",7 Discussion,[1.000000067903668],"['However in that work, we mostly focused on transition based-parsing, which raises the question of the impact of our feature-set on a much more powerful and state-of-the-art model such as the TURBOSEMANTICPARSER developed by Martins and Almeida (2014).']"
"To this end, we extended the T.PARSER so that it could cope with our syntactic features and studied the interaction of our best feature set with second order features (i.e. grand-parents and co-parents).",7 Discussion,[0],[0]
Results in Table 11 show that the gain brought by adding syntactic features (+2.14 on DM over the baseline) is higher than the sole use of second order ones (+1.09).,7 Discussion,[0],[0]
"Furthermore, the gain brought by
the second-order features is reduced by half when used jointly with our feature set (+1.09 vs +0.57 with them).",7 Discussion,[0],[0]
"However, although we could assess that the need of second order models is thus alleviated, the conjunction of both types of features still improves the parser performance by an overall gain of 1.62 points on DM (1.18 on PAS), suggesting that both feature sets contribute to different types of “structures”.",7 Discussion,[0],[0]
"In short, the use of syntactic features is also relevant with a strong baseline, as they provide a global view to graph-based models, establishing a new state-of-the-art on these corpora.
",7 Discussion,[1.000000026365837],"['In short, the use of syntactic features is also relevant with a strong baseline, as they provide a global view to graph-based models, establishing a new state-of-the-art on these corpora.']"
"Baseline = arc-factored + siblings
Related Work A growing interest for semantic parsing has emerged over the past few years, with the availability of resources such as PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004) built on top of the Penn Treebank.",7 Discussion,[0],[0]
"The shallow semantic annotations they provide were among the targets of successful shared tasks on semantic role labeling (Surdeanu et al., 2008; Carreras and Màrquez, 2005).",7 Discussion,[0],[0]
"Actually, the conjoint use of such annotations with surface syntax dependencies bears some resemblance with predicate-argument structure parsing like we presented here.",7 Discussion,[1.0],"['Actually, the conjoint use of such annotations with surface syntax dependencies bears some resemblance with predicate-argument structure parsing like we presented here.']"
"However, they diverge in that Propbank/Nombank annotations
do not form connected graphs by themselves, as they only cover argument identification and nominal predicates.",7 Discussion,[0],[0]
"The range of phenomena they describe is also limited, compared to a full predicate-argument analysis as provided by DM and PAS (Oepen et al., 2014).",7 Discussion,[0],[0]
"More importantly, as pointed out by Yi et al. (2007), being verb-specific, Propbank’s roles do not generalize well beyond the ARG0 argument (i.e. the subject/agent role) leading to inconsistencies.",7 Discussion,[0],[0]
"However, the advent of such semantic-based resources have ignited a fruitful line of research, of which the use of heterogeneous sources of information to boost parsing performance has been investigated over the past decade (Chen and Rambow, 2003; Tsuruoka et al., 2004) with a strong regain of interest raised by the work of Moschitti et al. (2008), Henderson et al. (2008), Sagae (2009).",7 Discussion,[0],[0]
We described the use and combination of several kinds of syntactic features to improve predicateargument parsing.,8 Conclusion,[1.0],['We described the use and combination of several kinds of syntactic features to improve predicateargument parsing.']
"To do so, we tested our approach of injecting surface-syntax features by thoroughly evaluating their impact on one transitionbased graph parser, then validating on two more efficient parsers, over two deep syntax and semantic treebanks.",8 Conclusion,[1.0],"['To do so, we tested our approach of injecting surface-syntax features by thoroughly evaluating their impact on one transitionbased graph parser, then validating on two more efficient parsers, over two deep syntax and semantic treebanks.']"
"Results of the syntax-enhanced semantic parsers exhibit a constant improvement, regardless of the annotation scheme and the parser used.",8 Conclusion,[1.0],"['Results of the syntax-enhanced semantic parsers exhibit a constant improvement, regardless of the annotation scheme and the parser used.']"
The question is now to establish whether will this be verified in other semantic data sets?,8 Conclusion,[1.0],['The question is now to establish whether will this be verified in other semantic data sets?']
"From the parsing of deep syntax treebanks a la Meaning Text Theory (Ballesteros et al., 2014), to Framenet semantic parsing (Das et al., 2014) or data-driven approaches closer to ours (Flanigan et al., 2014), it is difficult to know which models will predominate from this bubbling field and what kind of semantic data sets will benefit the most from syntax.",8 Conclusion,[1.0],"['From the parsing of deep syntax treebanks a la Meaning Text Theory (Ballesteros et al., 2014), to Framenet semantic parsing (Das et al., 2014) or data-driven approaches closer to ours (Flanigan et al., 2014), it is difficult to know which models will predominate from this bubbling field and what kind of semantic data sets will benefit the most from syntax.']"
We would like to thank Kenji Sagae and André F. T. Martins for making their parsers available and for kindly answering our questions.,Acknowledgements,[0],[0]
We also thank our anonymous reviewers for their comments.,Acknowledgements,[0],[0]
"This work was partly funded by the Program ""Investissements d’avenir"" managed by Agence Nationale de la Recherche ANR-10-LABX-0083 (Labex EFL).",Acknowledgements,[0],[0]
Parsing full-fledged predicate-argument structures in a deep syntax framework requires graphs to be predicted.,abstractText,[0],[0]
"Using the DeepBank (Flickinger et al., 2012) and the PredicateArgument Structure treebank (Miyao and Tsujii, 2005) as a test field, we show how transition-based parsers, extended to handle connected graphs, benefit from the use of topologically different syntactic features such as dependencies, tree fragments, spines or syntactic paths, bringing a much needed context to the parsing models, improving notably over long distance dependencies and elided coordinate structures.",abstractText,[0],[0]
"By confirming this positive impact on an accurate 2nd-order graphbased parser (Martins and Almeida, 2014), we establish a new state-of-the-art on these data sets.",abstractText,[0],[0]
Because Syntax Does Matter: Improving Predicate-Argument Structures Parsing with Syntactic Features,title,[0],[0]
