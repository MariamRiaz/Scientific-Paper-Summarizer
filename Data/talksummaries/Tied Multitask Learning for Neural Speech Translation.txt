0	42	Recent efforts in endangered language documentation focus on collecting spoken language resources, accompanied by spoken translations in a high resource language to make the resource interpretable (Bird et al., 2014a).
1	68	For example, the BULB project (Adda et al., 2016) used the LIGAikuma mobile app (Bird et al., 2014b; Blachon et al., 2016) to collect parallel speech corpora between three Bantu languages and French.
2	79	Since it’s common for speakers of endangered languages to speak one or more additional languages, collection of such a resource is a realistic goal.
3	82	Speech can be interpreted either by transcription in the original language or translation to another language.
4	61	Since the size of the data is extremely small, multitask models that jointly train a model for both tasks can take advantage of both signals.
6	164	Higher-level intermediate representations, such as transcriptions, should in principle carry information useful for an end task like speech translation.
7	28	A typical multitask setup (Weiss et al., 2017) shares information at the level of encoded frames, but intuitively, a human translating speech must work from a higher level of representation, at least at the level of phonemes if not syntax or semantics.
8	50	Thus, we present a novel architecture for tied multitask learning with sequence-to-sequence models, in which the decoder of the second task receives information not only from the encoder, but also from the decoder of the first task.
9	18	In addition, transitivity and invertibility are two properties that should hold when mapping between levels of representation or across languages.
10	6	We demonstrate how these two notions can be implemented through regularization of the attention matrices, and how they lead to further improved performance.
12	39	Our highresource experiments are performed on English, French, and German.
14	70	In the speech transcription and translation tasks, our proposed model leads to improved performance against all baselines as well as previous multitask architectures.
17	10	Finally, on the word discovery task, we improve upon previous work by about 3% F-score on both tokens and types.
18	8	Our models are based on a sequence-to-sequence model with attention (Bahdanau et al., 2015).
19	15	In general, this type of model is composed of three parts: a recurrent encoder, the attention, and a recurrent decoder (see Figure 1a).1 The encoder transforms an input sequence of words or feature frames x1, .
22	11	Finally, the decoder computes a sequence of output states from which a probability distribution over output words can be computed.
24	16	In a standard encoder-decoder multitask model (Figure 1b) (Dong et al., 2015; Weiss et al., 2017), we jointly model two output sequences using a shared encoder, but separate attentions and decoders: c1m = ∑ n α1mnhn s1m = dec 1(s1m−1, c 1 m, y 1 m−1) P(y1m) = softmax(s 1 m) and c2m = ∑ n α2mnhn s2m = dec 2(s2m−1, c 2 m, y 2 m−1) P(y2m) = softmax(s 2 m).
25	56	We can also arrange the decoders in a cascade (Figure 1c), in which the second decoder attends only to the output states of the first decoder: c2m = ∑ m′ α12mm′s 1 m′ s2m = dec 2(s2m−1, c 2 m, y 2 m−1) P(y2m) = softmax(s 2 m).
26	48	Tu et al. (2017) use exactly this architecture to train on bitext by setting the second output sequence to be equal to the input sequence (y2i = xi).
27	63	In our proposed triangle model (Figure 1d), the first decoder is as above, but the second decoder has two attentions, one for the input states of the encoder and one for the output states of the first decoder: c2m = [∑ m′ α 12 mm′s 1 m′ ∑ n α 2 mnhn ] s2m = dec 2(s2m−1, c 2 m, y 2 m−1) P(y2m) = softmax(s 2 m).
31	19	Let θ be the parameters of our model, which we train on sentence triples (X,Y1,Y2).
34	18	We then train the model to maximize L(θ) = ∑ score(Y1,Y2 | X; θ), where the summation is over all sentence triples in the training data.
36	75	Transitivity attention regularizer To a first approximation, the translation relation should be transitive (Wang et al., 2006; Levinboim and Chiang, 2015): If source word xi aligns to target word y1j and y 1 j aligns to target word y 2 k , then xi should also probably align to y2k .
37	17	To encourage the model to preserve this relationship, we add the following transitivity regularizer to the loss function of the triangle models with a small weight λtrans = 0.2: Ltrans = score(Y1,Y2) − λtrans ∥∥∥A12A1 − A2 ∥∥∥2 2.
38	55	Invertibility attention regularizer The translation relation also ought to be roughly invertible (Levinboim et al., 2015): if, in the reconstruction version of the cascade model, source word xi aligns to target word y1j , then it stands to reason that y j is likely to align to xi.
41	6	The first decoder produces, through standard beam search, a set of triples each consisting of a candidate transcription Ŷ1, a score P(Ŷ1), and a hidden state sequence Ŝ.
