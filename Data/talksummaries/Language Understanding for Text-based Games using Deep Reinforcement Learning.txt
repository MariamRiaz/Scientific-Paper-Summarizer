0	64	In this paper, we address the task of learning control policies for text-based strategy games.
1	39	These games, predecessors to modern graphical ones, still enjoy a large following worldwide.2 They often involve complex worlds with rich interactions and elaborate textual descriptions of the underlying states (see Figure 1).
2	21	Players read descriptions of the current world state and respond with natural language commands to take actions.
5	30	The simplest method is to use a bag-of-words representation derived from the text description.
9	24	In contrast, our goal is to learn useful representations in conjunction with control policies.
10	28	We adopt a reinforcement learning framework and formulate game sequences as Markov Decision Processes.
49	23	The agent takes an action a in state s by consulting a state-action value function Q(s, a), which is a measure of the action’s expected long-term reward.
58	23	The DQN approximates the Q-value function with a deep neural network to predict Q(s, a) for all possible actions a simultaneously given the current state s. The non-linear function layers of the DQN also enable it to learn better value functions than linear approximators.
59	36	In this section, we describe our model (DQN) and describe its use in learning good Q-value approximations for games with stochastic textual descriptions.
61	41	The first module is a representation generator that converts the textual description of the current state into a vector.
64	37	We learn the parameters of both the representation generator and the action scorer jointly, using the in-game reward feedback.
65	34	Representation Generator (φR) The representation generator reads raw text displayed to the agent and converts it to a vector representation vs. A bag-of-words (BOW) representation is not sufficient to capture higher-order structures of sentences and paragraphs.
66	34	The need for a better semantic representation of the text is evident from the average performance of this representation in playing MUD-games (as we show in Section 6).
72	37	To get the final state representation vs, we add a mean pooling layer which computes the elementwise mean over the output vectors xk.3 (2)vs = 1 n n∑ k=1 xk Action Scorer (φA) The action scorer module produces scores for the set of possible actions given the current state representation.
89	82	Algorithm 1 Training Procedure for DQN with prioritized sampling 1: Initialize experience memory D 2: Initialize parameters of representation generator (φR) and action scorer (φA) randomly 3: for episode = 1,M do 4: Initialize game and get start state description s1 5: for t = 1, T do 6: Convert st (text) to representation vst using φR 7: if random() < then 8: Select a random action at 9: else 10: Compute Q(st, a) for all actions using φA(vst) 11: Select at = argmax Q(st, a) 12: Execute action at and observe reward rt and new state st+1 13: Set priority pt = 1 if rt > 0, else pt = 0 14: Store transition (st, at, rt, st+1, pt) in D 15: Sample random mini batch of transitions (sj , aj , rj , sj+1, pj) from D, with fraction ρ having pj = 1 16: Set yj = { rj if sj+1 is terminal rj + γ maxa′ Q(sj+1, a′; θ) if sj+1 is non-terminal 17: Perform gradient descent step on the loss L(θ) = (yj −Q(sj , aj ; θ))2 Mini-batch Sampling In practice, online updates to the parameters θ are performed over a mini batch of state transitions, instead of a single transition.
101	69	We observe that the Fantasy world is moderately sized with a vocabulary of 1340 words and up to 100 different descriptions for a room.
102	21	These descriptions were created manually by the game developers.
113	21	Every room is reachable from every other room.
120	23	An example of a quest given to the player in text is Not you are sleepy now but you are hungry now.
121	25	To complete this quest and obtain a reward, the player has to navigate through the house to reach the kitchen and eat the apple (i.e type in the command eat apple).
125	22	This game also has stochastic transitions in addition to varying state descriptions provided to the player.
130	57	Evaluation We use two metrics for measuring an agent’s performance: (1) the cumulative reward obtained per episode averaged over the episodes and (2) the fraction of quests completed by the agent.
138	40	The first is a Random agent that chooses both actions and objects uniformly at random from all available choices.8 The other two are BOW-DQN and BI-DQN, which use a bagof-words and a bag-of-bigrams representation of the text, respectively, as input to the DQN action scorer.
182	21	We can see that semantically similar words appear close together to form coherent subspaces.
186	38	Table 2 shows some examples of descriptions from Fantasy world and their nearest neighbors using cosine similarity between their corresponding vector representations produced by LSTM-DQN.
187	30	The model is able to correlate descriptions of the same (or similar) underlying states and project them onto nearby points in the representation subspace.
191	28	We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback.
192	79	This framework enables us to map text descriptions into vector representations that capture the semantics of the game states.
193	248	Our experiments demonstrate the importance of learning good representations of text in order to play these games well.
194	36	Future directions include tackling high-level planning and strategy learning to improve the performance of intelligent agents.
