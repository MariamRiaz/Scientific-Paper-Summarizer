2	29	However, less is known about the extent to which such models correlate with and reflect human conceptual representation.
3	78	Much research in the cognitive neuroscience community has been concerned with uncovering how the brain represents conceptual knowledge, by leveraging brain activation data associated with the meanings of concepts obtained during functional magnetic resonance imaging (fMRI) experiments.
6	28	Other researchers followed in their steps, evaluating traditional count-based distributional models (Devereux et al., 2010; Murphy et al., 2012), topic model-based semantic features (Pereira et al., 2013), psycholinguistic and behavioural features (Palatucci et al., 2009; Chang et al., 2010; Fernandino et al., 2015) and visual representations (Anderson et al., 2013, 2017).
10	23	We take inspiration from the works of Mitchell et al. (2008) and Murphy et al. (2012); however, we conduct a more extensive study of the ability of different types of semantic models to predict the patterns of brain activity associated with conceptual representation.
11	42	We evaluate and compare several kinds of semantic models, using different modalities and data sources: (1) traditional countbased distributional models (with word windowbased and dependency-based contexts) learnt from text; (2) log-linear skip-gram models (with word window-based and dependency-based contexts); (3) behavioural models based on the free association task; (4) word representations learnt from 1081 visual data; and (5) multi-modal word representations combining linguistic and visual information.
39	24	The entire set of 60 stimulus words was presented six times to every participant, in a different order for each presentation.
40	47	The fMRI images were acquired on a Siemens Allegra 3.0T scanner.
53	36	The features of this semantic space are 25 sensory-motor verbs.
56	41	DISTRIB We obtain count-based distributional semantic models, using the top 10K most frequent lemmatised words in the corpus (excluding stopwords) as contexts.
68	23	EMBED-DEPS In addition to the embeddings trained with linear bag-of-words contexts, we also obtain 300-dimensional dependency-based word embeddings using the Levy and Goldberg (2014) implementation of the generalised skip-gram with arbitrary contexts model.
69	52	Using both incoming and outgoing dependency relations output by the C&C parser, we create word-context pairs using all words and contexts occurring more than 400 times in the corpus.
73	28	Recent studies have shown the superiority of se- mantic models built using data collected from multiple-response free association tasks — where subjects are asked to list multiple associative cues for every target word rather than a single association — over the models built from single-response ones (De Deyne et al., 2013).
75	24	We make use of the word association dataset collected as part of the Small World of Words4 project, where more than 100K fluent English speakers were asked to list three associations for each target word.
78	31	ASSOC We construct a count-based semantic model of word associations (henceforth ASSOC) similarly to a count-based distributional model: the responses are treated as semantic features, and counts are replaced by the sum of primary, secondary and tertiary association frequencies between the target word and the responses.
89	23	First, we compare these semantic models in their predictive power, by looking at how well they can synthesise, i.e. predict, brain activation patterns for unseen concepts (Section 5.1).
104	30	The analysis in this case does not involve synthesising brain activation vectors for new concepts, but predicting the correct label (stimulus) associated with a given fMRI pattern.
106	37	The first step is to obtain the semantic model similarity matrix — by computing the semantic model similarity codes for each of the 60 concepts in the Mitchell et al. (2008) dataset (as described above) — and the brain activity similarity matrix — by computing brain activity similarity codes.
108	35	Next, ~s′i, ~s ′ j , ~a′i and ~a ′ j are obtained by removing the i-th and j-th elements in ~si, ~sj ,~ai and~aj respectively, because entries in the similarity vectors corresponding to the test words would reveal the correct answer in the matching task.
112	29	All experiments detailed in this section were performed separately for every participant and evaluated using leave-two-out cross validation.
117	52	A matching score was computed by analysing the cosine similarity between the predicted and the observed brain activation vectors.
118	51	If the sum of similarities for the correct pairing was higher than the one for the incorrect pairing the matching accuracy was set to 1 for this cross-validation fold, and otherwise it was set to 0.
131	38	These grounded semantic models perform as well as models that encode mental representations through associations (ASSOC).
137	59	Predicted brain activation vectors were then synthesised for the two test words by weighting a superposition of brain activity vectors using their semantic model similarity codes.
154	27	Following Mitchell et al. (2008), we also compare the models in their ability to make accurate predictions when the two test words are exemplars of the same semantic category11.
155	36	This formulation of the task is more difficult, since items in the same semantic class (e.g. dog and cat) are more similar than items from different semantic classes (e.g. eye and desk).
159	134	Visually-grounded models still perform the best in all three experiments (mean performance across participants for multimodal models in all three tasks is in the [0.61- 0.63] range).
161	51	Firstly, we demonstrated that visual information is a stronger predictor of brain activity than linguistic information for concrete nouns.
164	23	Secondly, our results suggest that sparse textbased models, whether dependency-based or built using linear bag-of-words context, predict neural activity more accurately than dense models.
169	89	Previous studies that use fMRI data always report variation across participants (Devereux et al., 2010; Anderson et al., 2017) and most often attribute it to head motion.
170	48	However, understanding how individual variations in participants can impact modeling de- cisions would be of great value to the computational semantics community.
