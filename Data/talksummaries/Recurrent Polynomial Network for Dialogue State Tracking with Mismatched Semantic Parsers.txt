30	12	Complex RPN structures are also introduced in this section.
31	15	Section 4 discusses the influence of SLU on tracking and the SLU mismatch condition.
42	24	There are two ways of bridging rule-based approaches and statistical approaches.
43	149	One starts from rule-based models and uses data-driven approaches to find a good rule, while the other one is a statistical model taking advantage of prior knowledge and constraints.
44	70	Constrained Markov Bayesian Polynomial (CMBP) (Sun et al., 2014a; Yu et al., 2015) takes the first way of bridging rule-based models and statistical models.
45	80	Several probability features extracted from SLU results shown below are used in CMBP for each slot (Sun et al., 2014a; Yu et al., 2015): • P+t (v): sum of scores of SLU hypotheses informing or affirming value v at turn t • P−t (v): sum of scores of SLU hypotheses denying or negating value v at turn t • P̃+t (v) = ∑ v′ /∈{v,None} P + t (v ′) • P̃−t (v) = ∑ v′ /∈{v,None} P − t (v ′) • bt(v): belief of “the value being v at turn t” • brt : probability of the value being None (the value not mentioned) at turn t. Because slots and values are assumed independent in CMBP.
47	27	With these probability features , a CMBP model is defined by bt =P ( P+t , P − t , P̃ + t , P̃ − t , b r t−1, bt−1 ) s.t.
53	9	Then the tracking accuracy of each parameters can be evaluated and the best one is picked out.
59	26	RPN contains two types of nodes, input node or computational node.
61	31	The values of computational nodes at time t are evaluated using the nodes’ values at time t and the nodes’ values at time t − 1 as inputs just like Recurrent Neural Networks (RNNs).
64	21	Similarly, let Îx denote the set of nodes which are connected to node x by type-2 edges.
65	27	Generally, three types of computational node are used in RPN, which are sum node, product node and activation node.
66	53	• Sum node: For sum node x at time t, its value u (t) x is the weighted sum of its inputs: u(t)x = ∑ y∈Ix wx,yu (t−1) y + ∑ y∈Îx ŵx,yu (t) y (4) where wx,y, ŵx,y ∈ R are the weights of edges.
68	16	Note that there may be multiple edges connecting from node y to node x.
76	10	A basic 3-layer RPN shown in figure 1 is introduced here to help understand the correlation between 3-order CMBP and RPN.
77	40	For simplicity, (l, i) is used to denote the index of the i-th node in the l-th layer.
87	11	From the explanation of basic structure in this section, it can be easily observed that a CMBP can be used to initialize RPN and thus RPN can achieve at least the same results with CMBP.
88	25	So prior knowledge and constraints are used to find a suboptimum point in RPN parameter space and RPN as a statistical approach, can further optimize its parameters.
92	11	A new sum node x = (3, 1) in the third layer is introduced to capture some property across turns just like belief bt.
94	31	Further, to map the output belief to [0, 1], activation nodes with softclip(·) as their activation function are introduced.
97	51	Similarly, the complete structure can also be initialized using CMBP by setting the weights of edges that do not appear in the basic structure to 0.
126	13	Even with an effective way to get SLU robust to ASR errors, it is hard to implement these SLUs for a new domain due to insufficient labelled data.
128	21	Following the work of Zhu et al. (2014), the following steps are used to handle the two challenges stated above: • Data generation: with sufficient data in restaurants domain in DSTC-2, data on tourists domain using ontology of DSTC-3 can be generated.
129	9	Utterance patterns of data in the original domain are used to generate data for the new domain of DSTC-3.
137	36	Although a semantic parser with state-of-theart techniques can achieve good performance in some degree, parsing without any error is impossible because it is typical that a semantic parser gets high performance in speech patterns existing in the training dataset, while it fails to predict the correct semantics for some utterances unseen in training dataset.
138	35	So it is common for SLU performance to differ significantly between training and test conditions in real world end-to-end systems.
140	29	When these confidence scores become unreliable, the performance of tracker is sure to degrade.
142	30	Hence, most of the state-of-the-art results from DSTC-2 and DSTC3 used refined SLU (either explicitly rebuild a SLU component or take the ASR hypotheses into the trackers (Williams, 2014; Sun et al., 2014b; Henderson et al., 2014d; Henderson et al., 2014c; Kadlec et al., 2014; Sun et al., 2014a)).
143	30	Kadlec et al.(2014) gets a tracking accuracy improvement of 7.6% when they use SLU refined by themselves instead of organiser-provided live SLU.
147	96	In this section, the performance of three structures shown in this paper is compared and RPN with the simple structure is evaluated on DSTC-3 and compared with the best submitted trackers.
