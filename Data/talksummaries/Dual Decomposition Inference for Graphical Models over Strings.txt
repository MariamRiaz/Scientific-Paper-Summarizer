4	56	Inference in general graphical models is NPhard even when the variables’ values are finite discrete values such as categories, tags or domains.
5	89	In this paper, we address the more challenging setting where the variables in the graphical models range over strings.
6	33	Thus, the domain of the variables is an infinite space of discrete structures.
8	215	They could be used to model diverse relationships among strings that represent spellings or pronunciations; morphemes, words, phrases (such as named entities and URLs), or utterances; standard or variant forms; clean or noisy forms; contemporary or historical forms; underlying or surface forms; source or target language forms.
9	30	Such relationships arise in domains such as morphology, phonology, historical linguistics, translation between related languages, and social media text analysis.
10	80	In this paper, we assume a given graphical model, whose factors evaluate the relationships among observed and unobserved strings.1 We present a dual decomposition algorithm for MAP inference, which returns a certifiably optimal solution when it converges.
11	128	We demonstrate our method on a graphical model for phonology proposed by Cotterell et al. (2015).
16	24	To perform inference on a graphical model (directed or undirected), one first converts the model to a factor graph representation (Kschischang et al., 2001).
24	30	This is a generic constraint satisfaction problem with soft constraints.
27	22	Tree-shaped graphical models naturally model the evolutionary tree of word forms (Bouchard-Côté et al., 2007; Bouchard-Côté et al., 2008; Hall and Klein, 2010; Hall and Klein, 2011).
49	15	In section 4, we will present a dual decomposition (DD) method that decomposes the original complex problem into many small subproblems that are free of cycles and high degree nodes.
50	33	BP can solve each subproblem without approximation.4 The subproblems “communicate” through Lagrange multipliers that guide them towards agreement on a single global solution.
75	33	Notice that the 8 edges in the first layer of Figure 1 form a cycle; such cycles make BP inexact.
95	15	We now attempt to enforce the equality constraint indirectly, by adding Lagrange multipliers that encourage agreement among the subproblems.
109	34	We follow Paul and Eisner (2012) and use a substring count feature for each w ∈ Σ∗.
110	30	In other words, γ(x) is an infinitely long vector, which maps each w to the number of times that w appears in x as a substring.8 Computing λki · γ(xki ) in (3) remains possible because in practice, λki will have only finitely many nonzeros.
111	14	This is so because our feature vector γ(x) has only finitely many nonzeros for any string x, and the subgradient algorithm in section 4.3 below always updates λki by adding multiples of such γ(x) vectors.
114	16	Only these features may have nonzero Lagrange multipliers.
116	21	Given the Lagrange multipliers, subproblem k of (3) is simply MAP inference on the factor graph consisting of the variables X k and factors Fk as well as an extra unary factor Gki at each Xi ∈ X k: Gki (x k) def= λki · γ(xki ) (4) These unary factors penalize strings according to the Lagrange multipliers.
127	17	For example, we will add the abc feature only when the xki already agree on their counts of its substrings ab and bc.9 Algorithm 1 summarizes the whole method.
131	27	(They applied this to solve instances of the NP-hard Steiner 9In principle, we should check that they also (still) agree on a, b, and c, but we skip this check.
157	39	We compare DD to belief propagation, using the graphical model for generative phonology discussed in section 3.
158	21	Inference in this model aims to reconstruct underlying morphemes.
159	19	Since our focus is inference, we will evaluate these reconstructions directly (whereas Cotterell et al. (2015) evaluated their ability to predict novel surface forms using the reconstructions).
161	143	How- ever, they are actually derived from datasets constructed by Cotterell et al. (2015), which are available with full descriptions at http://hubal.cs.
165	21	Each dataset contains 1000 surface words, formed from 341 to 381 underlying morphemes.
167	70	SP Perform approximate marginal inference by sum-product loopy BP with pruning (Cotterell et al., 2015).
168	33	MP Perform approximate MAP inference by max-product loopy BP with pruning.
169	52	DD and SP improve this baseline in different ways.
173	51	We also compare these predicted URs to one another, to see how well the methods agree.
