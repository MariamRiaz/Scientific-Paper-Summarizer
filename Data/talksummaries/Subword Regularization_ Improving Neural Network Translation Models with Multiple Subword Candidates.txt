2	24	A common approach for dealing with the open vocabulary issue is to break up rare words into subword units (Schuster and Nakajima, 2012; Chitnis and DeNero, 2015; Sennrich et al., 2016; Wu et al., 2016).
4	63	BPE segmentation gives a good balance between the vocabulary size and the decoding efficiency, and also sidesteps the need for a special treatment of unknown words.
10	86	These variants can be viewed as a spurious ambiguity, which might not always be resolved in decoding process.
11	29	At training time of NMT, multiple segmentation candidates will make the model robust to noise and segmentation errors, as they can indirectly help the model to learn the compositionality of words, e.g., “books” can be decomposed into “book” + “s”.
16	28	• We also propose a new subword segmentation algorithm based on a language model, which provides multiple segmentations with probabilities.
20	29	Given a source sentence X and a target sentence Y , let x = (x1, .
21	19	, yN ) be the corresponding subword sequences segmented with an underlying subword segmenter, e.g., BPE.
25	21	NMT is trained using the standard maximum likelihood estimation, i.e., maximizing the loglikelihood L(θ) of a given parallel corpus D = {⟨X(s), Y (s)⟩}|D|s=1 = {⟨x(s),y(s)⟩} |D| s=1, θMLE = argmax θ L(θ) where, L(θ) = |D|∑ s=1 logP (y(s)|x(s); θ).
26	26	(2) We here assume that the source and target sentences X and Y can be segmented into multiple subword sequences with the segmentation probabilities P (x|X) and P (y|Y ) respectively.
35	24	A straightforward approach for decoding is to translate from the best segmentation x∗ that maximizes the probability P (x|X), i.e., x∗ = argmaxxP (x|X).
42	18	Byte-Pair-Encoding (BPE) (Sennrich et al., 2016; Schuster and Nakajima, 2012) is a subword segmentation algorithm widely used in many NMT systems1.
43	46	BPE first splits the whole sentence into individual characters.
51	17	One downside is, however, that BPE is based on a greedy and deterministic symbol replacement, which can not provide multiple segmentations with probabilities.
55	32	, xM ) is formulated as the product of the subword occurrence probabilities p(xi)3: P (x) = M∏ i=1 p(xi), (6) ∀i xi ∈ V, ∑ x∈V p(x) = 1, where V is a pre-determined vocabulary.
56	44	The most probable segmentation x∗ for the input sentenceX is then given by x∗ = argmax x∈S(X) P (x), (7) where S(X) is a set of segmentation candidates built from the input sentence X .
59	31	L = |D|∑ s=1 log(P (X(s))) = |D|∑ s=1 log ( ∑ x∈S(X(s)) P (x) ) In the real setting, however, the vocabulary set V is also unknown.
82	40	In order to exactly sample from all possible segmentations, we use the Forward-Filtering and Backward-Sampling algorithm (FFBS) (Scott, 2002), a variant of the dynamic programming originally introduced by Bayesian hidden Markov model training.
123	52	We conducted experiments using multiple corpora with different sizes and languages.
130	18	We generally followed the settings and training procedure described in (Wu et al., 2016), however, we changed the settings according to the corpus size.
136	23	The data was preprocessed with Moses tokenizer before training subword models.
148	16	First, as can be seen in the table, BPE and unigram language model without subword regularization (l = 1) show almost comparable BLEU scores.
153	48	As for the sampling algorithm, (l = ∞ α = 0.2/0.5) slightly outperforms (l = 64, α = 0.1) on IWSLT corpus, but they show almost comparable results on larger data set.
169	44	First, we can find that the peaks of BLEU scores against smoothing parameter α are different de- pending on the sampling size l. This is expected, because l = ∞ has larger search space than l = 64, and needs to set α larger to sample sequences close to the Viterbi sequence x∗.
170	25	Another interesting observation is that α = 0.0 leads to performance drops especially on l = ∞.
176	126	Although we can see in general that the optimal hyperparameters are roughly predicted with the held-out estimation, it is still an open question how to choose the optimal size l in subword sampling.
177	51	Table 6 summarizes the BLEU scores with subword regularization either on source or target sentence to figure out which components (encoder or decoder) are more affected.
178	27	As expected, we can see that the BLEU scores with single side regularization are worse than full regularization.
181	36	In this paper, we presented a simple regularization method, subword regularization13, for NMT, with no change to the network architecture.
182	127	The central idea is to virtually augment training data with on-the-fly subword sampling, which helps to improve the accuracy as well as robustness of NMTmodels.
187	22	Additionally, we would like to explore the application of subword regularization for machine learning, including Denoising Auto Encoder (Vincent et al., 2008) and Adversarial Training (Goodfellow et al., 2015).
