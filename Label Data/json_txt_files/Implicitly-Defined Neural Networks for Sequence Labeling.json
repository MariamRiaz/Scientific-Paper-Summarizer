{"sections": [{"text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 172\u2013177 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2027"}, {"heading": "1 Introduction", "text": "Feedforward neural networks were designed to approximate and interpolate functions. Recurrent Neural Networks (RNNs) were developed to predict sequences. RNNs can be \u2018unwrapped\u2019 and thought of as very deep feedforward networks, with weights shared between each layer. Computation proceeds one step at a time, like the trajectory of an ordinary differential equation when solving an initial value problem. The path of an initial value problem depends only on the current state and the current value of the forcing function. In a RNN, the analogy is the current hidden state and the current input sequence. However, in certain applications in natural language processing, especially those with long-distance dependencies or where grammar matters, sequence predic-\n\u2217This work is sponsored by the Air Force Research Laboratory under Air Force contract FA-8721-05-C-0002. Opinions, interpretations, conclusions and recommendations are those of the authors and are not necessarily endorsed by the United States Government.\ntion may be better thought of as a boundary value problem. Changing the value of the forcing function (analogously, of an input sequence element) at any point in the sequence will affect the values everywhere else. The bidirectional recurrent network (Schuster and Paliwal, 1997) attempts to addresses this problem by creating a network with two recurrent hidden states \u2013 one that progresses in the forward direction and one that progresses in the reverse. This allows information to flow in both directions, but each state can only consider information from one direction. In practice many algorithms require more than two passes through the data to determine an answer. We provide a novel mechanism that is able to process information in both directions, with the motivation being a program which iterates over itself until convergence."}, {"heading": "1.1 Related Work", "text": "Bidirectional, long-distance dependencies in sequences have been an issue as long as there have been NLP tasks, and there are many approaches to dealing with them.\nHidden Markov models (HMMs) (Rabiner, 1989) have been used extensively for sequencebased tasks, but they rely on the Markov assumption \u2013 that a hidden variable changes its state based only on its current state and observables. In finding maximum likelihood state sequences, the Forward-Backward algorithm can take into account the entire set of observables, but the underlying model is still local.\nIn recent years, popularity of the Long ShortTerm Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and variants such as the Gated Recurrent Unit (GRU) (Cho et al., 2014) has soared, as they enable RNNs to process long sequences without the problem of vanishing or exploding gradients (Pascanu et al., 2013). However, these models\n172\nonly allow for information/gradient information to flow in the forward direction.\nThe Bidirectional LSTM (b-LSTM) (Graves and Schmidhuber, 2005), a natural extension of (Schuster and Paliwal, 1997), incorporates past and future hidden states via two separate recurrent networks, allowing information/gradients to flow in both directions of a sequence. This is a very loose coupling, however.\nIn contrast to these methods, our work goes a step further, fully coupling the entire sequences of hidden states of an RNN. Our work is similar to (Finkel et al., 2005), which augments a CRF with long-distance constraints. However, our work differs in that we extend an RNN and uses NetwonKrylov (Knoll and Keyes, 2004) instead of Gibbs Sampling."}, {"heading": "2 The Implicit Neural Network (INN)", "text": ""}, {"heading": "2.1 Traditional Recurrent Neural Networks", "text": "A typical recurrent neural network has a (possibly transformed) input sequence [\u03be1, \u03be2, . . . , \u03ben] and initial state hs and iteratively produces future states:\nh1 = f(\u03be1, hs) h2 = f(\u03be2, h1) . . . hn = f(\u03ben, hn\u22121)\nThe LSTM, GRU, and related variants follow this formula, with different choices for the state transition function. Computation proceeds linearly, with each next state depending only on inputs and previously computed hidden states."}, {"heading": "2.2 Proposed Architecture", "text": "In this work, we relax this assumption by allowing ht = f(\u03bet, ht\u22121, ht+1)1. This leads to an implicit set of equations for the entire sequence of hidden states, which can be thought of as a single tensor\n1A wider stencil can also be used, e.g. f(ht\u22122, ht\u22121, . . .).\nH: H = [h1, h2, . . . , hn]\nThis yields a system of nonlinear equations. This setup has the potential to arrive at nonlocal, whole sequence-dependent results. We also hope such a system is more \u2018stable\u2019, in the sense that the predicted sequence may drift less from the true meaning, since errors will not compound with each time step in the same way.\nThere are many potential ways to architect a neural network \u2013 in fact, this flexibility is one of deep learning\u2019s best features \u2013 but we restrict our discussion to the structure depicted in Figure 2. In this setup, we have the following variables:\ndata X labels Y parameters \u03b8\nand functions:\ninput layer transformation \u03be = g(\u03b8,X) implicit hidden layer def. H = F (\u03b8, \u03be,H) loss function L = `(\u03b8,H, Y )\nOur implicit definition function, F , is made up of local state transitions and forms a system of nonlinear equations that require solving, denoting n as the length of the input sequence and hs, he as boundary states:\nh1 = f(hs, h2, \u03be1) . . . hi = f(hi\u22121, hi+1, \u03bei) . . . hn = f(hn\u22121, he, \u03ben)"}, {"heading": "2.3 Computing the forward pass", "text": "To evaluate the network, we must solve the equationH = F (H). We computed this via an approximate Newton solve, where we successively refine an approximation Hn of H:\nHn+1 = Hn \u2212 (I \u2212\u2207HF )\u22121(Hn \u2212 F (Hn))\nLet k be the dimension of a single hidden state. (I \u2212\u2207HF ) is a sparse matrix, since\u2207HF is zero except for k pairs of n \u00d7 n block matrices, corresponding to the influence of the left and right neighbors of each state.\nBecause of this sparsity, we can apply Krylov subspace methods (Knoll and Keyes, 2004), specifically the BiCG-STAB method (Van der Vorst, 1992), since the system is non-symmetric. This has the added advantage of only relying on matrix-vector multiplies of the gradient of F ."}, {"heading": "2.4 Gradients", "text": "In order to train the model, we perform stochastic gradient descent. We take the gradient of the loss function:\n\u2207\u03b8L = \u2207\u03b8`+\u2207H`\u2207\u03b8H\nThe gradient of the hidden units with respect to the parameters can found via the implicit definition:\n\u2207\u03b8H = \u2207\u03b8F +\u2207HF\u2207\u03b8H +\u2207\u03beF\u2207\u03b8\u03be = (I \u2212\u2207HF )\u22121 (\u2207\u03b8F +\u2207\u03beF\u2207\u03b8\u03be)\nwhere the factorization follows from the noting that\n(I \u2212\u2207HF )\u2207\u03b8H = \u2207\u03b8F +\u2207\u03beF\u2207\u03b8\u03be.\nThe entire gradient is thus:\n\u2207\u03b8L =\u2207H`(I \u2212\u2207HF )\u22121 (\u2207\u03b8F +\u2207\u03beF\u2207\u03b8\u03be) +\u2207\u03b8`\n(1) Once again, the inverse of I \u2212\u2207HF appears, and we can compute it via Krylov subspace methods. It is worth mentioning the technique of computing parameter updates by implicit differentiation and conjugate gradients have been applied before, in the context of energy minimization models in image labeling and denoising (Domke, 2012)."}, {"heading": "2.5 Transition Functions", "text": "Recall the original GRU equations (Cho et al., 2014), with slight notational modifications:\nfinal h ht = (1\u2212 zt)h\u0302t + zth\u0303t candidate h h\u0303t = tanh(Wxt + U(rth\u0302t) + b\u0303) update weight zt = \u03c3(Wzxt + Uzh\u0302t + bz) reset gate rt = \u03c3(Wrxt + Urh\u0302t + br)\nWe make the following substitution for h\u0302t (which was set to ht\u22121 in the original GRU definition):\nstate comb. h\u0302t = sht\u22121 + (1\u2212 s)ht+1 switch s = spsp+sn prev. switch sp = \u03c3(Wpxt + Upht\u22121 + bp) next switch sn = \u03c3(Wnxt + Unht+1 + bn) (2) This modification makes the architecture both implicit and bidirectional, since h\u0302t is a linear combination of previous and future hidden states. The switch variable s is determined by a competition between two sigmoidal units sp and sn, representing the contributions of the previous and next hidden states, respectively."}, {"heading": "2.6 Implementation Details", "text": "We implemented the implicit GRU structure using Theano (Bergstra et al., 2011). The product \u2207HFv for various v, required for the BiCG-STAB method, was computed via the Rop operator. In computing \u2207\u03b8L (Equation 1), we noted it is more efficient to compute \u2207H`(I \u2212\u2207HF )\u22121 first, and thus used the Lop operator.\nAll experiments used a batch size of 20. To batch solve the linear equations, we simply solved a single, very large block diagonal system of equations: each sequence in the batch was a single block matrix, and we input the encompassing matrix into our Theano BiCG solver. (In practice the block diagonal system is represented as a 3-tensor, but it is equivalent.) In this setup, each step does receive separate update directions, but one global step length. hS and he were fixed at zero, but could be trained as parameters.\nIn solving multiple simultaneous systems of equations, we noted some elements converged significantly faster than others. For this reason, we found it helpful to run Newton\u2019s method from two separate initializations for each element in our batch, one selected randomly and the other set to a\n\u201cone-step\u201d approximation: Hidden states of a traditional GRU were computed in both forward (hfi ) and reverse (hbi ) directions, and hi was initialized to f(hfi\u22121, h b i+1, \u03bei). If either of the two candidates converged, we took its value and stopped computing the other. We also limited both the number Newton iterations and BiCG-STAB iterations per Newton iteration to 40."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Biased random walks", "text": "We developed an artificial task with bidirectional sequence-level dependencies to explore the performance of our model. Our task was to find the point at which a random walk, in the spirit of the Wiener Process (Durrett, 2010), changes from a zero to nonzero mean. We trained a network to predict when the walk is no longer unbiased. We generated algorithmic data for this problem, the specifics of which are as follows: First, we chose an integer interval lengthN uniformly in the range 1 to 40. Then, we chose a (continuous) time t\u2032 \u2208 [0, N), and a direction v \u2208 Rd. We produced the input sequence xi \u2208 Rd, setting x0 = 0 and iteratively computing xi+1 = xi +N (0, 1). After time t, a bias term of b \u00b7 v was added at each time step (b\u00b7v\u00b7(t\u2032\u2212t)) for the first time step greater than t\u2032. b is a global scalar parameter. The network was fed in these elements, and asked to predict y = 0 for times t \u2264 t\u2032 and y = 1 for times t > t\u2032.\nFor each architecture, \u03be was simply the unmodified input vectors, zero-padded to the embedding dimension size. The output was a simple binary logistic regression. We produced 50,000 random training examples, 2500 random validation examples, and 5000 random test examples. The implicit algorithm used a hidden dimension of 200, and the b-LSTM had an embedding dimension ranging from 100 to 1000. b-LSTM dimension of 300 was the point where the total number of parameters were roughly equal.\nThe results are shown in Table 1. The b-LSTM scores reported are the maximum over sweeps from 100 to 1500 hidden dimension size. The INN outperforms the best b-LSTM in the more challenging cases where the bias size b is small."}, {"heading": "3.2 Part-of-speech tagging", "text": "We next applied our model to a real-world problem. Part-of-speech tagging fits naturally in the sequence labeling framework, and has the advantage\nof a standard dataset that we can use to compare our network with other techniques. To train a partof-speech tagger, we simply let L be a softmax layer transforming each hidden unit output into a part of speech tag. Our input encoding \u03be, is a concatenation of three sets of features, adapted from (Huang et al., 2015): first, word vectors for 39,000 case-insensitive vocabulary words; second, six additional \u2018word vector\u2019 components indicating the presence of the top-2000 most common prefixes and suffixes of words, for affix lengths 2 to 4; and finally, eight other binary features to indicate the presence of numbers, symbols, punctuation, and more rich case data.\nWe trained the Part of Speech (POS) tagger on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993), blocks 0-18, validated on 19-21, and tested on 22-24, per convention. Training was done using stochastic gradient descent, with an initial learning rate of 0.5. The learning rate was halved if validation perplexity increased. Word vectors were of dimension 320, prefix and suffix vectors were of dimension 20. Hidden unit size was equal to feature input size, so in this case, 448.\nAs shown in Table 2, the INN outperformed baseline GRU, bidirectional GRU, LSTM, and bLSTM networks, all with 628-dimensional hidden layers (1256 for the bidirectional architectures), The INN also outperforms the Stanford Part-ofSpeech tagger (Toutanova et al., 2003) (model wsj-0-18-bidirectional-distsim.tagger from 10-31-2016). Note that performance gains past approximately 97% are difficult due to errors/inconsistencies in the dataset, ambiguity, and complex linguistic constructions including dependencies across sentence boundaries (Manning, 2011)."}, {"heading": "4 Time Complexity", "text": "The implicit experiments in this paper took approximately 3-5 days to run on a single Tesla K40, while the explicit experiments took approximately 1-3 hours. Running time of the solver is approximately nn \u00d7 nb \u00d7 tb where nn is the number of Newton iterations, nb is the number of BiCGSTAB iterations, and tb is the time for a single BiCG-STAB iteration. tb is proportional to the number of non-zero entries in the matrix (Van der Vorst, 1992), in our case n(2k2 + 1). Newton\u2019s method has second order convergence (Isaacson and Keller, 1994), and while the specific bound depends on the norm of (I \u2212\u2207HF )\u22121 and the norm of its derivatives, convergence is wellbehaved. For nb, however, we are not aware of a bound. For symmetric matrices, the Conjugate Gradient method is known to take O( \u221a \u03ba) iterations (Shewchuk et al., 1994), where \u03ba is the condition number of the matrix. However, our matrix is nonsymmetric, and we expect \u03ba to vary from problem to problem. Because of this, we empirically estimated the correlation between sequence length and total time to compute a batch of 20 hidden layer states.\nFor the random walk experiment with b = 0.5, we found the the average run time for a given sequence length to be approximately 0.17n1.8, with r2 = 0.994. Note that the exponent would have been larger had we not truncated the number of BiCG-STAB iterations to 40, as the inner iteration frequently hit this limit for larger n. However, the average number of Newton iterations did not go above 10, indicating that exiting early from the BiCG-STAB loop did not prevent the Newton solver from converging. Run times for the other random walk experiments were very similar, indicating run time does not depend on b; However, for the POS task runtime was 0.29n1.3, with\nr2 = 0.910."}, {"heading": "5 Conclusion and Future Work", "text": "We have introduced a novel, implicitly defined neural network architecture based on the GRU and shown that it outperforms a b-LSTM on an artificial random walk task and slightly outperforms both the Stanford Parser and a baseline bidirectional network on the Penn Treebank Part-ofSpeech tagging task.\nIn future work, we intend to consider implicit variations of other architectures, such as the LSTM, as well as additional, more challenging, and/or data-rich applications. We also plan to explore ways to speed up the computation of (I\u2212\u2207HF )\u22121. Potential speedups include approximating the hidden state values by reducing the number of Newton and/or BiCG-STAB iterations, using cached previous solutions as initial values, and modifying the gradient update strategy to keep the batch full at every Newton iteration."}, {"heading": "6 Acknowledgements", "text": "This work would not be possible without the support and funding of the Air Force Research Laboratory. We also acknowledge Nick Malyska, Elizabeth Salesky, and Jonathan Taylor at MIT Lincoln Lab for interesting technical discussions related to this work.\nCleared for Public Release on 29 Jul 2016. Originator reference number: RH-16-115722. Case Number: 88ABW2016-3809."}], "year": 2017, "references": [{"title": "Theano: Deep learning on gpus with python", "authors": ["James Bergstra", "Fr\u00e9d\u00e9ric Bastien", "Olivier Breuleux", "Pascal Lamblin", "Razvan Pascanu", "Olivier Delalleau", "Guillaume Desjardins", "David Warde-Farley", "Ian Goodfellow", "Arnaud Bergeron"], "year": 2011}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches", "authors": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Syntax, Semantics and Structure in Statistical Translation page 103.", "year": 2014}, {"title": "Generic methods for optimization-based modeling", "authors": ["Justin Domke."], "venue": "AISTATS. volume 22, pages 318\u2013326.", "year": 2012}, {"title": "Probability : theory and examples", "authors": ["Richard Durrett."], "venue": "Cambridge University Press, Cambridge New York.", "year": 2010}, {"title": "Incorporating non-local information into information extraction systems by gibbs sampling", "authors": ["Jenny Rose Finkel", "Trond Grenager", "Christopher Manning."], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics.", "year": 2005}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "authors": ["Alex Graves", "J\u00fcrgen Schmidhuber."], "venue": "Neural Networks 18(5):602\u2013610.", "year": 2005}, {"title": "Long short-term memory", "authors": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "year": 1997}, {"title": "Bidirectional lstm-crf models for sequence tagging", "authors": ["Zhiheng Huang", "Wei Xu", "Kai Yu."], "venue": "arXiv preprint arXiv:1508.01991 .", "year": 2015}, {"title": "Analysis of numerical methods", "authors": ["Eugene Isaacson", "Herbert Bishop Keller."], "venue": "Courier Corporation, New York.", "year": 1994}, {"title": "Jacobian-free newton\u2013krylov methods: a survey of approaches and applications", "authors": ["Dana A Knoll", "David E Keyes."], "venue": "Journal of Computational Physics 193(2):357\u2013397.", "year": 2004}, {"title": "Part-of-speech tagging from 97% to 100%: is it time for some linguistics? In International Conference on Intelligent Text Processing and Computational Linguistics", "authors": ["Christopher D Manning."], "venue": "Springer, pages 171\u2013189.", "year": 2011}, {"title": "Building a large annotated corpus of english: The penn treebank", "authors": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Computational linguistics 19(2):313\u2013330.", "year": 1993}, {"title": "On the difficulty of training recurrent neural networks", "authors": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."], "venue": "ICML (3) 28:1310\u20131318.", "year": 2013}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "authors": ["Lawrence R Rabiner."], "venue": "Proceedings of the IEEE 77(2):257\u2013 286.", "year": 1989}, {"title": "Bidirectional recurrent neural networks", "authors": ["Mike Schuster", "Kuldip K Paliwal."], "venue": "Signal Processing, IEEE Transactions on 45(11):2673\u20132681.", "year": 1997}, {"title": "An introduction to the conjugate gradient method without the agonizing pain", "authors": ["Jonathan Richard Shewchuk"], "year": 1994}, {"title": "Feature-rich part-ofspeech tagging with a cyclic dependency network", "authors": ["Kristina Toutanova", "Dan Klein", "Christopher D Manning", "Yoram Singer."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computa-", "year": 2003}, {"title": "Bi-cgstab: A fast and smoothly converging variant of bi-cg for the solution of nonsymmetric linear systems", "authors": ["Henk A Van der Vorst."], "venue": "SIAM Journal on scientific and Statistical Computing 13(2):631\u2013644.", "year": 1992}], "id": "SP:808ee54a122b6ca357e8022ea43a808a8e1da8fa", "authors": [{"name": "Michaeel Kazi", "affiliations": []}, {"name": "Brian Thompson", "affiliations": []}], "abstractText": "In this work, we propose a novel, implicitly-defined neural network architecture and describe a method to compute its components. The proposed architecture forgoes the causality assumption used to formulate recurrent neural networks and instead couples the hidden states of the network, allowing improvement on problems with complex, long-distance dependencies. Initial experiments demonstrate the new architecture outperforms both the Stanford Parser and baseline bidirectional networks on the Penn Treebank Part-ofSpeech tagging task and a baseline bidirectional network on an additional artificial random biased walk task.", "title": "Implicitly-Defined Neural Networks for Sequence Labeling"}