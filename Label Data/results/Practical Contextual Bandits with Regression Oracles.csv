0,1,label2,summary_sentences
"Deep neural networks achieve near-human accuracy on many perception tasks (He et al., 2016; Amodei et al., 2015).",1. Introduction,[0],[0]
"However, they lack robustness to small alterations of the inputs at test time (Szegedy et al., 2014).",1. Introduction,[0],[0]
"Indeed when presented with a corrupted image that is barely distinguishable from a legitimate one by a human, they can predict incorrect labels, with high-confidence.",1. Introduction,[0],[0]
"An adversary can design such so-called adversarial examples, by adding a small perturbation to a legitimate input to maximize the likelihood of an incorrect class under constraints on the magnitude of the perturbation (Szegedy et al., 2014; Goodfellow et al., 2015; Moosavi-Dezfooli et al., 2015; Pa-
1Facebook AI Research.",1. Introduction,[0],[0]
"Correspondence to: Moustapha Cisse <moustaphacisse@fb.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"pernot et al., 2016a).",1. Introduction,[0],[0]
"In practice, for a significant portion of inputs, a single step in the direction of the gradient sign is sufficient to generate an adversarial example (Goodfellow et al., 2015) that is even transferable from one network to another one trained for the same problem but with a different architecture (Liu et al., 2016; Kurakin et al., 2016).
",1. Introduction,[0.9531970251096048],"['Empirically, in the contextual semibandit setting, Krishnamurthy et al. (2016) found that the realizability-based LinUCB approach outperforms all agnostic baselines using a linear policy class.']"
The existence of transferable adversarial examples has two undesirable corollaries.,1. Introduction,[0],[0]
"First, it creates a security threat for production systems by enabling black-box attacks (Papernot et al., 2016a).",1. Introduction,[0],[0]
"Second, it underlines the lack of robustness of neural networks and questions their ability to generalize in settings where the train and test distributions can be (slightly) different as is the case for the distributions of legitimate and adversarial examples.
",1. Introduction,[0.9531917736428306],"['While bounding the disagreement coefficients a priori often requires strong assumptions on the model class and the distribution, the size of disagreement set can be easily checked empirically under the product class assumption, and we include this diagnostic in our experimental results.']"
"Whereas the earliest works on adversarial examples already suggested that their existence was related to the magnitude of the hidden activations gradient with respect to their inputs (Szegedy et al., 2014), they also empirically assessed that standard regularization schemes such as weight decay or training with random noise do not solve the problem (Goodfellow et al., 2015; Fawzi et al., 2016).",1. Introduction,[0],[0]
The current mainstream approach to improving the robustness of deep networks is adversarial training.,1. Introduction,[0],[0]
"It consists in generating adversarial examples on-line using the current network’s parameters (Goodfellow et al., 2015; Miyato et al., 2015; Moosavi-Dezfooli et al., 2015; Szegedy et al., 2014; Kurakin et al., 2016) and adding them to the training data.",1. Introduction,[0],[0]
"This data augmentation method can be interpreted as a robust optimization procedure (Shaham et al., 2015).
",1. Introduction,[0],[0]
"In this paper, we introduce Parseval networks, a layerwise regularization method for reducing the network’s sensitivity to small perturbations by carefully controlling its global Lipschitz constant.",1. Introduction,[0],[0]
"Since the network is a composition of functions represented by its layers, we achieve increased robustness by maintaining a small Lipschitz constant (e.g., 1) at every hidden layer; be it fully-connected, convolutional or residual.",1. Introduction,[0],[0]
"In particular, a critical quantity governing the local Lipschitz constant in both fully connected and convolutional layers is the spectral norm of the weight matrix.",1. Introduction,[0],[0]
"Our main idea is to control this norm by parameterizing the network with parseval tight frames (Kovačević & Chebira, 2008), a generalization of orthogonal matrices.
",1. Introduction,[0],[0]
"The idea that regularizing the spectral norm of each weight
matrix could help in the context of robustness appeared as early as (Szegedy et al., 2014), but no experiment nor algorithm was proposed, and no clear conclusion was drawn on how to deal with convolutional layers.",1. Introduction,[0],[0]
"Previous work, such as double backpropagation (Drucker & Le Cun, 1992) has also explored jacobian normalization as a way to improve generalization.",1. Introduction,[0],[0]
Our contribution is twofold.,1. Introduction,[0],[0]
"First, we provide a deeper analysis which applies to fully connected networks, convolutional networks, as well as Residual networks (He et al., 2016).",1. Introduction,[0],[0]
"Second, we propose a computationally efficient algorithm and validate its effectiveness on standard benchmark datasets.",1. Introduction,[0],[0]
"We report results on MNIST, CIFAR-10, CIFAR-100 and Street View House Numbers (SVHN), in which fully connected and wide residual networks were trained (Zagoruyko & Komodakis, 2016) with Parseval regularization.",1. Introduction,[0],[0]
"The accuracy of Parseval networks on legitimate test examples matches the state-of-the-art, while the results show notable improvements on adversarial examples.",1. Introduction,[0],[0]
"Besides, Parseval networks train significantly faster than their vanilla counterpart.
",1. Introduction,[0],[0]
"In the remainder of the paper, we first discuss the previous work on adversarial examples.",1. Introduction,[0],[0]
"Next, we give formal definitions of the adversarial examples and provide an analysis of the robustness of deep neural networks.",1. Introduction,[0],[0]
"Then, we introduce Parseval networks and its efficient training algorithm.",1. Introduction,[0],[0]
Section 5 presents experimental results validating the model and providing several insights.,1. Introduction,[0],[0]
"Early papers on adversarial examples attributed the vulnerability of deep networks to high local variations (Szegedy et al., 2014; Goodfellow et al., 2015).",2. Related work,[0],[0]
"Some authors argued that this sensitivity of deep networks to small changes in their inputs is because neural networks only learn the discriminative information sufficient to obtain good accuracy rather than capturing the true concepts defining the classes (Fawzi et al., 2015; Nguyen et al., 2015).
",2. Related work,[0],[0]
"Strategies to improve the robustness of deep networks include defensive distillation (Papernot et al., 2016b), as well as various regularization procedures such as contractive networks (Gu & Rigazio, 2015).",2. Related work,[0],[0]
"However, the bulk of recent proposals relies on data augmentation (Goodfellow et al., 2015; Miyato et al., 2015; Moosavi-Dezfooli et al., 2015; Shaham et al., 2015; Szegedy et al., 2014; Kurakin et al., 2016).",2. Related work,[0.9536926346396095],"['This assumption is used by essentially all regression-based contextual bandit algorithms (Chu et al., 2011; Filippi et al., 2010; Russo & Van Roy, 2013; Li et al., 2017).']"
It uses adversarial examples generated online during training.,2. Related work,[0],[0]
"As we shall see in the experimental section, regularization can be complemented with data augmentation; in particular, Parseval networks with data augmentation appear more robust than either data augmentation or Parseval networks considered in isolation.",2. Related work,[0],[0]
"We consider a multiclass prediction setting, where we have Y classes in Y = {1, ..., Y }.",3. Robustness in Neural Networks,[0],[0]
"A multiclass classifier is a function ĝ : (x ∈ RD,W ∈ W) 7→ argmaxȳ∈Y",3. Robustness in Neural Networks,[0],[0]
"gȳ(x,W ), where W are the parameters to be learnt, and gȳ(x,W ) is the score given to the (input, class) pair (x, ȳ) by a function g :",3. Robustness in Neural Networks,[0],[0]
RD × W → RY .,3. Robustness in Neural Networks,[0],[0]
"We take g to be a neural network, represented by a computation graph G = (N , E), which is a directed acyclic graph with a single root node, and each node n ∈ N takes values in Rd (n) out and is a function of its children in the graph, with learnable parameters W (n):
n : x 7→ φ(n) ( W (n), ( n′(x) )",3. Robustness in Neural Networks,[0],[0]
"n′:(n,n′)∈E ) .",3. Robustness in Neural Networks,[0],[0]
"(1)
The function g we want to learn is the root of G. The training data ((xi, yi))mi=1 ∈",3. Robustness in Neural Networks,[0],[0]
"(X × Y)m is an i.i.d. sample of D, and we assume X ⊂",3. Robustness in Neural Networks,[0],[0]
RD is compact.,3. Robustness in Neural Networks,[0],[0]
"A function ` : RY × Y → R measures the loss of g on an example (x, y); in a single-label classification setting for instance, a common choice for ` is the log-loss:
` ( g(x,W ), y ) = −gy(x,W ) + log (∑ ȳ∈Y egȳ(x,W ) ) .",3. Robustness in Neural Networks,[0],[0]
"(2)
The arguments that we develop below depend only on the Lipschitz constant of the loss, with respect to the norm of interest.",3. Robustness in Neural Networks,[0],[0]
"Formally, we assume that given a p-norm of interest ‖.‖p, there is a constant λp such that
∀z, z′ ∈ RY ,∀ȳ ∈ Y, |`(z, ȳ)−`(z′, ȳ)| ≤",3. Robustness in Neural Networks,[0],[0]
"λp‖z−z′‖p .
",3. Robustness in Neural Networks,[0],[0]
"For the log-loss of (2), we have λ2 ≤ √
2 and λ∞ ≤ 2.",3. Robustness in Neural Networks,[0],[0]
"In the next subsection, we define adversarial examples and the generalization performance of the classifier.",3. Robustness in Neural Networks,[0],[0]
"Then, we make the relationship between robustness to adversarial examples and the lipschitz constant of the networks.",3. Robustness in Neural Networks,[0],[0]
"Given an input (train or test) example (x, y), an adversarial example is a perturbation of the input pattern x̃ = x",3.1. Adversarial examples,[0],[0]
"+ δx where δx is small enough so that x̃ is nearly undistinguishable from x (at least from the point of view of a human annotator), but has the network predict an incorrect label.",3.1. Adversarial examples,[0],[0]
"Given the network parameters and structure g(.,W ) and a p-norm, the adversarial example is formally defined as
x̃ = argmax x̃:‖x̃−x‖p≤
` ( g(x̃,W ), y ) , (3)
where represents the strength of the adversary.",3.1. Adversarial examples,[0],[0]
"Since the optimization problem above is non-convex, Shaham et al. (2015) propose to take the first order taylor expansion of x 7→ `(g(x,W ), y) to compute δx by solving
x̃ = argmax x̃:‖x̃−x‖p≤
( ∇x`(g(x,W ), y) )",3.1. Adversarial examples,[0],[0]
T (x̃− x) .,3.1. Adversarial examples,[0],[0]
"(4)
If p = ∞, then x̃ = x + sign(∇x`(g(x,W ), y)).",3.1. Adversarial examples,[0],[0]
This is the fast gradient sign method.,3.1. Adversarial examples,[0],[0]
"For the case p = 2, we obtain x̃ = x + ∇x`(g(x,W ), y).",3.1. Adversarial examples,[0],[0]
"A more involved method is the iterative fast gradient sign method, in which several gradient steps of (4) are performed with a smaller stepsize to obtain a local minimum of (3).",3.1. Adversarial examples,[0],[0]
"In the context of adversarial examples, there are two different generalization errors of interest:
L(W )",3.2. Generalization with adversarial examples,[0],[0]
"= E (x,y)∼D
[ `(g(x,W ), y) ] ,
Ladv(W,p, ) =",3.2. Generalization with adversarial examples,[0],[0]
"E (x,y)∼D
[ max
x̃:‖x̃−x‖p≤ `(g(x̃,W ), y)
] .
",3.2. Generalization with adversarial examples,[0],[0]
"By definition, L(W )",3.2. Generalization with adversarial examples,[0],[0]
"≤ Ladv(W,p, ) for every p and >0.",3.2. Generalization with adversarial examples,[0],[0]
"Reciprocally, denoting by λp and Λp the Lipschitz constant (with respect to ‖.‖p) of ` and g respectively, we have:
Ladv(W,p, ) ≤ L(W )",3.2. Generalization with adversarial examples,[0],[0]
+,3.2. Generalization with adversarial examples,[0],[0]
"E
(x,y)∼D
[ max
x̃:‖x̃−x‖p≤ |`(g(x̃,W ), y)− `(g(x,W ), y)| ] ≤ L(W ) +",3.2. Generalization with adversarial examples,[0],[0]
"λpΛp .
",3.2. Generalization with adversarial examples,[0],[0]
This suggests that the sensitivity to adversarial examples can be controlled by the Lipschitz constant of the network.,3.2. Generalization with adversarial examples,[0],[0]
"In the robustness framework of (Xu & Mannor, 2012), the Lipschitz constant also controls the difference between the average loss on the training set and the generalization performance.",3.2. Generalization with adversarial examples,[0],[0]
"More precisely, let us denote by Cp(X , γ) the covering number of X using γ-balls for ‖.‖p.",3.2. Generalization with adversarial examples,[0],[0]
"Using M = supx,W,y `(g(x,W ), y), Theorem 3 of (Xu & Mannor, 2012) implies that for every δ ∈ (0, 1), with probability 1− δ over the i.i.d. sample ((xi, yi)mi=1, we have:
L(W )",3.2. Generalization with adversarial examples,[0],[0]
≤ 1 m m∑ i=1,3.2. Generalization with adversarial examples,[0],[0]
"`(g(xi,W ), yi)
+ λpΛpγ",3.2. Generalization with adversarial examples,[0],[0]
"+M
√ 2Y Cp(X , γ2 )",3.2. Generalization with adversarial examples,[0],[0]
"ln(2)− 2 ln(δ)
m .
",3.2. Generalization with adversarial examples,[0],[0]
"Since covering numbers of a p-norm ball in RD increases exponentially with RD, the bound above suggests that it is critical to control the Lipschitz constant of g, for both good generalization and robustness to adversarial examples.",3.2. Generalization with adversarial examples,[0],[0]
"From the network structure we consider (1), for every node n ∈",3.3. Lipschitz constant of neural networks,[0],[0]
"N , we have (see below for the definition of Λ(n,n ′) p ):
",3.3. Lipschitz constant of neural networks,[0],[0]
"‖n(x)− n(x̃)‖p ≤ ∑
n′:(n,n′)∈E
Λ(n,n ′) p ‖n′(x)− n′(x̃)‖p ,
for any Λ(n,n ′)
p that is greater than the worst case variation of n with respect to a change in its input n′(x).",3.3. Lipschitz constant of neural networks,[0],[0]
"In particular we can take for Λ(n,n ′)",3.3. Lipschitz constant of neural networks,[0],[0]
"p any value greater than the
supremum over x0 ∈ X of the Lipschitz constant for ‖.‖p of the function (1n′′ = n′ is 1 if n′′ = n′ and 0 otherwise):
x 7→ φ(n) ( W (n), ( n′′(x0+1n ′′ = n′(x−x0)) )",3.3. Lipschitz constant of neural networks,[0],[0]
"n′′:(n,n′′)∈E ) .
",3.3. Lipschitz constant of neural networks,[0],[0]
"The Lipschitz constant of n, denoted by Λ(n)p satisfies:
Λ(n)p ≤ ∑
n′:(n,n′)∈E
Λ(n,n ′) p Λ (n′) p (5)
",3.3. Lipschitz constant of neural networks,[0],[0]
"Thus, the Lipschitz constant of the network g can grow exponentially with its depth.",3.3. Lipschitz constant of neural networks,[0],[0]
"We now give the Lipschitz constants of standard layers as a function of their parameters:
Linear layers: For layer n(x) = W (n)n′(x) where n′ is the unique child of n in the graph, the Lipschitz constant for ‖.‖p is, by definition, the matrix norm of W (n) induced by ‖.‖p, which is usually denoted ‖W (n)‖p and defined by
‖W (n)‖p = sup z:‖z‖p=1 ‖W (n)z‖p .
",3.3. Lipschitz constant of neural networks,[0],[0]
"Then Λ(n)2 = ‖W (n)‖2Λ (n′) 2 , where ‖W (n)‖2, called the spectral norm of W (n), is the maximum singular value of W (n).",3.3. Lipschitz constant of neural networks,[0],[0]
"We also have Λ(n)∞ = ‖W (n)‖∞Λ(n ′) ∞ , where
‖W (n)‖∞ = maxi ∑ j |W (n)",3.3. Lipschitz constant of neural networks,[0],[0]
ij,3.3. Lipschitz constant of neural networks,[0],[0]
| is the maximum 1-norm of the rows.,3.3. Lipschitz constant of neural networks,[0],[0]
"W (n).
",3.3. Lipschitz constant of neural networks,[0],[0]
"Convolutional layers: To simplify notation, let us consider convolutions on 1D inputs without striding, and we take the width of the convolution to be 2k + 1 for k ∈ N. To write convolutional layers in the same way as linear layers, we first define an unfolding operator U , which prepares the input z, denoted by U(z).",3.3. Lipschitz constant of neural networks,[0],[0]
"If the input has length T with din inputs channels, the unfolding operator maps z",3.3. Lipschitz constant of neural networks,[0],[0]
"For a convolution of the unfolding of z considered as a T × (2k + 1)din matrix, its j-th column is:
Uj(z) =",3.3. Lipschitz constant of neural networks,[0],[0]
"[zj−k; ...; zj+k] ,
where “;” is the concatenation along the vertical axis (each zi is seen as a column din-dimensional vector), and zi",3.3. Lipschitz constant of neural networks,[0],[0]
= 0,3.3. Lipschitz constant of neural networks,[0],[0]
if i is out of bounds (0-padding).,3.3. Lipschitz constant of neural networks,[0],[0]
"A convolutional layer with dout output channels is then defined as
n(x) =",3.3. Lipschitz constant of neural networks,[0],[0]
"W (n) ∗ n′(x) = W (n)U(n′(x)) ,
where W (n) is a dout × (2k + 1)din matrix.",3.3. Lipschitz constant of neural networks,[0],[0]
"We thus have Λ
(n) 2 ≤ ‖W‖2‖U(n′(x))‖2.",3.3. Lipschitz constant of neural networks,[0],[0]
"Since U is a linear operator that essentially repeats its input (2k + 1) times, we have ‖U(n′(x))",3.3. Lipschitz constant of neural networks,[0],[0]
− U(n′(x̃))‖22 ≤ (2k + 1)‖n′(x),3.3. Lipschitz constant of neural networks,[0],[0]
"− n′(x̃)‖22, so that Λ(n)2 ≤ √ 2k + 1‖W‖2Λ(n ′) 2 .",3.3. Lipschitz constant of neural networks,[0],[0]
"Also, ‖U(n′(x))",3.3. Lipschitz constant of neural networks,[0],[0]
"− U(n′(x̃))‖∞ = ‖n′(x) − n′(x̃)‖∞, and so for a convolutional layer, Λ(n)∞ ≤ ‖W (n)‖∞Λ(n ′) ∞ .
",3.3. Lipschitz constant of neural networks,[0],[0]
"Aggregation layers/transfer functions: Layers that perform the sum of their inputs, as in Residual Netowrks (He et al., 2016), fall in the case where the values Λ(n,n ′) p in (5) come into play.",3.3. Lipschitz constant of neural networks,[0],[0]
"For a node n that sums its inputs, we have Λ (n,n′) p = 1, and thus Λ (n) p ≤ ∑ n′:(n,n′)∈E Λ (n′) p .",3.3. Lipschitz constant of neural networks,[0],[0]
"If n is a tranfer function layer (e.g., an element-wise application of ReLU)",3.3. Lipschitz constant of neural networks,[0],[0]
"we can check that Λ(n)p ≤ Λ(n ′) p , where n′ is the input node, as soon as the Lipschitz constant of the transfer function (as a function R→ R) is ≤ 1.",3.3. Lipschitz constant of neural networks,[0],[0]
"Parseval regularization, which we introduce in this section, is a regularization scheme to make deep neural networks robust, by constraining the Lipschitz constant (5) of each hidden layer to be smaller than one, assuming the Lipschitz constant of children nodes is smaller than one.",4. Parseval networks,[0],[0]
"That way, we avoid the exponential growth of the Lipschitz constant, and a usual regularization scheme (i.e., weight decay) at the last layer then controls the overall Lipschitz constant of the network.",4. Parseval networks,[0],[0]
"To enforce these constraints in practice, Parseval networks use two ideas: maintaining orthonormal rows in linear/convolutional layers, and performing convex combinations in aggregation layers.",4. Parseval networks,[0],[0]
"Below, we first explain the rationale of these constraints and then describe our approach to efficiently enforce the constraints during training.",4. Parseval networks,[0],[0]
"Orthonormality of weight matrices: For linear layers, we need to maintain the spectral norm of the weight matrix at 1.",4.1. Parseval Regularization,[0],[0]
Computing the largest singular value of weight matrices is not practical in an SGD setting unless the rows of the matrix are kept orthogonal.,4.1. Parseval Regularization,[0],[0]
"For a weight matrix W ∈ Rdout×din with dout ≤ din, Parseval regularization maintains WTW",4.1. Parseval Regularization,[0],[0]
"≈ Idout×dout , where I refers to the identity matrix.",4.1. Parseval Regularization,[0],[0]
"W is then approximately a Parseval tight frame (Kovačević & Chebira, 2008), hence the name of Parseval networks.",4.1. Parseval Regularization,[0],[0]
"For convolutional layers, the matrix W ∈ Rdout×(2k+1)din is constrained to be a Parseval tight frame (with the notations of the previous section), and the output is rescaled by a factor (2k + 1)−1/2.",4.1. Parseval Regularization,[0],[0]
"This maintains all singular values of W to (2k+ 1)−1/2, so that Λ
(n) 2 ≤ Λ (n′) 2 where n
′ is the input node.",4.1. Parseval Regularization,[0],[0]
"More generally, keeping the rows of weight matrices orthogonal makes it possible to control both the spectral norm and the ‖.‖∞ of a weight matrix through the norm of its individual rows.",4.1. Parseval Regularization,[0],[0]
Robustness for ‖.‖∞ is achieved by rescaling the rows so that their 1-norm is smaller than 1.,4.1. Parseval Regularization,[0],[0]
"For now, we only experimented with constraints on the 2-norm of the rows, so we aim for robustness in the sense of ‖.‖2.
",4.1. Parseval Regularization,[0],[0]
Remark 1 (Orthogonality is required).,4.1. Parseval Regularization,[0],[0]
"Without orthogonality, constraints on the 2-norm of the rows of weight ma-
trices are not sufficient to control the spectral norm.",4.1. Parseval Regularization,[0],[0]
"Parseval networks are thus fundamentally different from weight normalization (Salimans & Kingma, 2016).
",4.1. Parseval Regularization,[0],[0]
Aggregation Layers:,4.1. Parseval Regularization,[0],[0]
"In parseval networks, aggregation layers do not make the sum of their inputs, but rather take a convex combination of them:
n(x) = ∑
n′:(n,n′)∈E
α(n,n ′)n′(x)
with ∑ n′:(n,n′)∈E α (n,n′) = 1 and α(n,n ′) ≥ 0.",4.1. Parseval Regularization,[0],[0]
"The parameters α(n,n ′) are learnt, but using (5), these constraint guarantee that Λ(n)p ≤ 1 as soon as the children satisfy the inequality for the same p-norm.",4.1. Parseval Regularization,[0],[0]
Orthonormality constraints: The first significant difference between Parseval networks and its vanilla counterpart is the orthogonality constraint on the weight matrices.,4.2. Parseval Training,[0],[0]
"This requirement calls for an optimization algorithm on the manifold of orthogonal matrices, namely the Stiefel manifold.",4.2. Parseval Training,[0],[0]
"Optimization on matrix manifolds is a well-studied topic (see (Absil et al., 2009) for a comprehensive survey).",4.2. Parseval Training,[0],[0]
The simplest first-order geometry approaches consist in optimizing the unconstrained function of interest by moving in the direction of steepest descent (given by the gradient of the function) while at the same time staying on the manifold.,4.2. Parseval Training,[0],[0]
"To guarantee that we remain in the manifold after every parameter update, we need to define a retraction operator.",4.2. Parseval Training,[0],[0]
"There exist several pullback operators for embedded submanifolds such as the Stiefel manifold based for example on Cayley transforms (Absil et al., 2009).",4.2. Parseval Training,[0],[0]
"However, when learning the parameters of neural networks, these methods are computationally prohibitive.",4.2. Parseval Training,[0],[0]
"To overcome this difficulty, we use an approximate operator derived from the following layer-wise regularizer of weight matrices to ensure their parseval tightness (Kovačević & Chebira, 2008):
Rβ(Wk) =",4.2. Parseval Training,[0],[0]
"β
2 ‖W>k Wk − I‖22.
Optimizing Rβ(Wk) to convergence after every gradient descent step (w.r.t the main objective) guarantees us to stay on the desired manifold but this is an expensive procedure.",4.2. Parseval Training,[0],[0]
"Moreover, it may result in parameters that are far from the ones obtained after the main gradient update.",4.2. Parseval Training,[0],[0]
"We use two approximations to make the algorithm more efficient: First, we only do one step of descent on the function Rα(Wk).",4.2. Parseval Training,[0],[0]
The gradient of this regularization term is∇WkRβ(Wk) =,4.2. Parseval Training,[0],[0]
β(WkW > k − I)Wk.,4.2. Parseval Training,[0],[0]
"Consequently, after every main update we perform the following secondary update:
Wk ← (1 + β)Wk",4.2. Parseval Training,[0],[0]
"− βWkW>k Wk.
Algorithm 1 Parseval Training Θ = {Wk,αk}Kk=1, e← 0",4.2. Parseval Training,[0],[0]
"while e ≤ E do
Sample a minibatch {(xi, yi)}Bi=1.",4.2. Parseval Training,[0],[0]
"for k ∈ {1, . . .",4.2. Parseval Training,[0],[0]
",K} do
Compute the gradient:",4.2. Parseval Training,[0],[0]
"GWk ← ∇Wk`(Θ, {(xi, yi)}), Gαk ← ∇αk`(Θ, {(xi, yi)}).",4.2. Parseval Training,[0],[0]
Update the parameters: Wk ←Wk − ·GWk αk ← αk − ·Gαk .,4.2. Parseval Training,[0],[0]
"if hidden layer then
Sample a subset S of rows of Wk.",4.2. Parseval Training,[0],[0]
Projection: WS ← (1 + β)WS,4.2. Parseval Training,[0],[0]
− βWSW>S WS .,4.2. Parseval Training,[0],[0]
αk ← argminγ∈∆K−1‖αK,4.2. Parseval Training,[0],[0]
"− γ‖22
e← e+ 1.
Optionally, instead of updating the whole matrix, one can randomly select a subset S of rows and perform the update from Eq.",4.2. Parseval Training,[0],[0]
(4.2) on the submatrix composed of rows indexed by S. This sampling based approach reduces the overall complexity to O(|S|2d).,4.2. Parseval Training,[0],[0]
"Provided the rows are carefully sampled, the procedure is an accurate Monte Carlo approximation of the regularizer loss function (Drineas et al., 2006).",4.2. Parseval Training,[0],[0]
"The optimal sampling probabilities, also called statistical leverages are approximately equal if we start from an orthogonal matrix and (approximately) stay on the manifold throughout the optimization since they are proportional to the eigenvalues of W (Mahoney et al., 2011).",4.2. Parseval Training,[0],[0]
"Therefore, we can sample a subset of columns uniformly at random when applying this projection step.
",4.2. Parseval Training,[0],[0]
"While the full update does not result in an increased overhead for convolutional layers, the picture can be very different for large fully connected layers making the sampling approach computationally more appealing for such layers.",4.2. Parseval Training,[0.950236032437061],"['Agnostic approaches, on the other hand, typically assume an oracle for cost-sensitive classification, which is computationally intractable in the worst case, but often practically feasible for many natural policy classes.']"
We show in the experiments that the weight matrices resulting from this procedure are (quasi)-orthogonal.,4.2. Parseval Training,[0],[0]
"Also, note that quasi-orthogonalization procedures similar to the one described here have been successfully used previously in the context of learning overcomplete representations with independent component analysis (Hyvärinen & Oja, 2000).
",4.2. Parseval Training,[0],[0]
"Convexity constraints in aggregation layers: In Parseval networks, aggregation layers output a convex combination of their inputs instead of e.g., their sum as in Residual networks (He et al., 2016).",4.2. Parseval Training,[0],[0]
"For an aggregation node n of the network, let us denote by α = (α(n,n
′))n′:(n,n′)∈E the K-size vector of coefficients used for the convex combination output by the layer.",4.2. Parseval Training,[0],[0]
"To ensure that the Lipschitz constant at the node n is such that Λ(n)p ≤ 1, the constraints of 4.1 call for a euclidean projection of α onto the positive simplex after a gradient update:
α∗ = argmin γ∈∆K−1 ‖α− γ‖22 ,
where ∆K−1 = {γ ∈ RK |1>γ = 1,γ ≥ 0}.",4.2. Parseval Training,[0],[0]
"This is a well studied problem (Michelot, 1986; Pardalos & Kovoor, 1990; Duchi et al., 2008; Condat, 2016).",4.2. Parseval Training,[0],[0]
"Its solution is of the form: α∗i = max(0, αi − τ(α)), with τ : RK → R the unique function satisfying ∑ i(xi − τ(α))",4.2. Parseval Training,[0],[0]
= 1 for every x ∈ RK .,4.2. Parseval Training,[0],[0]
"Therefore, the solution essentially boils down to a soft thresholding operation.",4.2. Parseval Training,[0],[0]
If we denote α1 ≥ α2 ≥ . . .,4.2. Parseval Training,[0],[0]
"αK the sorted coefficients and k(α) = max{k ∈ (1, . . .",4.2. Parseval Training,[0],[0]
",K)|1+kαk > ∑ j≤k αj}, the optimal thresholding is given by (Duchi et al., 2008):
τ(α) =",4.2. Parseval Training,[0],[0]
"( ∑ j≤k(α) αj)− 1 k(α)
",4.2. Parseval Training,[0],[0]
"Consequently, the complexity of the projection is O(K log(K))",4.2. Parseval Training,[0],[0]
since it is only dominated by the sorting of the coefficients and is typically cheap because aggregation nodes will only have few children in practice (e.g. 2).,4.2. Parseval Training,[0],[0]
"If the number of children is large, there exist efficient linear time algorithms for finding the optimal thresholding τ(α) (Michelot, 1986; Pardalos & Kovoor, 1990; Condat, 2016).",4.2. Parseval Training,[0],[0]
"In this work, we use the method detailed above (Duchi et al., 2008) to perform the projection of the coefficient α after every gradient update step.",4.2. Parseval Training,[0],[0]
"We evaluate the effectiveness of Parseval networks on well-established image classification benchmark datasets namely MNIST, CIFAR-10, CIFAR-100 (Krizhevsky, 2009) and Street View House Numbers (SVHN) (Netzer et al.).",5. Experimental evaluation,[0],[0]
We train both fully connected networks and wide residual networks.,5. Experimental evaluation,[0],[0]
"The details of the datasets, the models, and the training routines are summarized below.",5. Experimental evaluation,[0],[0]
CIFAR.,5.1. Datasets,[0],[0]
Each of the CIFAR datasets is composed of 60K natural scene color images of size 32 × 32 split between 50K training images and 10K test images.,5.1. Datasets,[0],[0]
CIFAR-10 and CIFAR-100 have respectively 10 and 100 classes.,5.1. Datasets,[0],[0]
"For these two datasets, we adopt the following standard preprocessing and data augmentation scheme (Lin et al., 2013;",5.1. Datasets,[0],[0]
"He et al., 2016; Huang et al., 2016a; Zagoruyko & Komodakis, 2016): Each training image is first zero-padded with 4 pixels on each side.",5.1. Datasets,[0],[0]
The resulting image is randomly cropped to produce a new 32 × 32 image which is subsequently horizontally flipped with probability 0.5.,5.1. Datasets,[0],[0]
We also normalize every image with the mean and standard deviation of its channels.,5.1. Datasets,[0],[0]
"Following the same practice as (Huang et al., 2016a), we initially use 5K images from the training as a validation set.",5.1. Datasets,[0],[0]
"Next, we train de novo the best model on the full set of 50K images and report the results on the test set.",5.1. Datasets,[0],[0]
SVHN The Street View House Number dataset is a set of 32× 32 color digit images officially split into 73257 training images and 26032 test images.,5.1. Datasets,[0],[0]
"Following common practice (Zagoruyko & Komodakis, 2016; He et al., 2016; Huang et al., 2016a;b), we randomly sample 10000 images from the available extra set of about 600K images as a validation set and combine the rest of the pictures with the official training set.",5.1. Datasets,[0],[0]
We divide the pixel values by 255 as a preprocessing step and report the test set performance of the best performing model on the validation set.,5.1. Datasets,[0],[0]
ConvNet Models.,5.2. Models and Implementation details,[0],[0]
"For the CIFAR and SVHN datasets, we trained wide residual networks (Zagoruyko & Komodakis, 2016) as they perform on par with standard resnets (He et al., 2016) while being faster to train thanks to a reduced depth.",5.2. Models and Implementation details,[0],[0]
We used wide resnets of depth 28 and width 10 for both CIFAR-10 and CIFAR-100.,5.2. Models and Implementation details,[0],[0]
For SVHN we used wide resnet of depth 16 and width 4.,5.2. Models and Implementation details,[0],[0]
"For each architecture, we compare Parseval networks with the vanilla model trained with standard regularization both in the adversarial and the non-adversarial training settings.
",5.2. Models and Implementation details,[0],[0]
ConvNet Training.,5.2. Models and Implementation details,[0],[0]
We train the networks with stochastic gradient descent using a momentum of 0.9.,5.2. Models and Implementation details,[0],[0]
"On CIFAR datasets, the initial learning rate is set to 0.1 and scaled by a factor of 0.2 after epochs 60, 120 and 160, for a total number of 200 epochs.",5.2. Models and Implementation details,[0],[0]
We used mini-batches of size 128.,5.2. Models and Implementation details,[0],[0]
"For SVHN, we trained the models with mini-batches of size 128 for 160 epochs starting with a learning rate of 0.01 and decreasing it by a factor of 0.1 at epochs 80 and 120.",5.2. Models and Implementation details,[0],[0]
"For all the vanilla models, we applied by default weight decay regularization (with parameter λ = 0.0005) together with batch normalization and dropout since this combination resulted in better accuracy and increased robustness in preliminary experiments.",5.2. Models and Implementation details,[0],[0]
"The dropout rate use
is 0.3 for CIFAR and 0.4 for SVHN.",5.2. Models and Implementation details,[0],[0]
"For Parseval regularized models, we choose the value of the retraction parameter to be β = 0.0003 for CIFAR datasets and β = 0.0001 for SVHN based on the performance on the validation set.",5.2. Models and Implementation details,[0],[0]
"In all cases, We also adversarially trained each of the models on CIFAR-10 and CIFAR-100 following the guidelines in (Goodfellow et al., 2015; Shaham et al., 2015; Kurakin et al., 2016).",5.2. Models and Implementation details,[0],[0]
"In particular, we replace 50% of the examples of every minibatch by their adversarially perturbed version generated using the one-step method to avoid label leaking (Kurakin et al., 2016).",5.2. Models and Implementation details,[0],[0]
"For each mini-batch, the magnitude of the adversarial perturbation is obtained by sampling from a truncated Gaussian centered at 0 with standard deviation 2.
Fully Connected Model.",5.2. Models and Implementation details,[0],[0]
We also train feedforward networks composed of 4 fully connected hidden layers of size 2048 and a classification layer.,5.2. Models and Implementation details,[0],[0]
The input to these networks are images unrolled into a C × 1024 dimensional vector where C is the number of channels.,5.2. Models and Implementation details,[0],[0]
We used these models on MNIST and CIFAR-10 mainly to demonstrate that the proposed approach is also useful on non-convolutional networks.,5.2. Models and Implementation details,[0],[0]
We compare a Parseval networks to vanilla models with and without weight decay regularization.,5.2. Models and Implementation details,[0],[0]
"For adversarially trained models, we follow the guidelines previously described for the convolutional networks.
",5.2. Models and Implementation details,[0],[0]
Fully Connected Training.,5.2. Models and Implementation details,[0],[0]
We train the models with SGD and divide the learning rate by two every 10 epochs.,5.2. Models and Implementation details,[0],[0]
We use mini-batches of size 100 and train the model for 50 epochs.,5.2. Models and Implementation details,[0],[0]
We chose the hyperparameters on the validation set and retrain the model on the union of the training and validation sets.,5.2. Models and Implementation details,[0],[0]
"The hyperparameters are β, the size of the row subset S, the learning rate and its decrease rate.",5.2. Models and Implementation details,[0],[0]
Using a subset S of 30% of all the rows of each of weight matrix for the retraction step worked well in practice.,5.2. Models and Implementation details,[0],[0]
We first validate that Parseval training (Algorithm 1) indeed yields (near)-orthonormal weight matrices.,5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
"To do so, we analyze the spectrum of the weight matrices of the different models by plotting the histograms of their singular values, and compare these histograms for Parseval networks to networks trained using standard SGD with and without weight decay (SGD-wd and SGD).
",5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
The histograms representing the distribution of singular values at layers 1 and 4 for the fully connected network (using S = 30%) trained on the dataset CIFAR-10 are shown in Fig. 2 (the figures for convolutional networks are similar).,5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
The singular values obtained with our method are tightly concentrated around 1.,5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
"This experiment confirms that the weight matrices produced by the proposed opti-
mization procedure are (almost) orthonormal.",5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
"The distribution of the singular values of the weight matrices obtained with SGD has a lot more variance, with nearly as many small values as large ones.",5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
"Adding weight decay to standard SGD leads to a sparse spectrum for the weight matrices, especially in the higher layers of the network suggesting a low-rank structure.",5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
"This observation has motivated recent work on compressing deep neural networks (Denton et al., 2014).",5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
"We evaluate the robustness of the models to adversarial noise by generating adversarial examples from the test set, for various magnitudes of the noise vector.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Following common practice (Kurakin et al., 2016), we use the fast gradient sign method to generate the adversarial examples (using ‖.‖∞, see Section 3.1).",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Since these adversarial examples transfer from one network to the other, the fast gradient sign method allows to benchmark the network for reasonable settings where the opponent does not know the network.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
We report the accuracy of each model as a function of the magnitude of the noise.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"To make the results easier to interpret, we compute the corresponding Signal to Noise Ratio (SNR).",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"For an input x and perturbation δx, the SNR is defined as SNR(x, δx) = 20 log10 ‖x‖2 ‖δx‖2 .",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"We show some adversarial examples in Fig. 1.
Fully Connected Nets.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
Figure 3 depicts a comparison of Parseval and vanilla networks with and without adversarial training at various noise levels.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"On both MNIST and CIFAR-10, Parseval networks consistently outperforms weight decay regularization.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"In addition, it is as robust as
adversarial training (SGD-wd-da) on CIFAR-10.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Combining Parseval Networks and adversarial training results in the most robust method on MNIST.
ResNets.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Table 1 summarizes the results of our experiments with wide residual Parseval and vanilla networks on CIFAR-10, CIFAR-100 and SVHN.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"In the table, we denote Parseval(OC) the Parseval network with orthogonality constraint and without using a convex combination in aggregation layers.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
Parseval indicates the configuration where both of the orthogonality and convexity constraints are used.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
We first observe that Parseval networks outperform vanilla ones on all datasets on the clean examples and match the state of the art performances on CIFAR-10 (96.28%) and SVHN (98.44%).,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"On CIFAR-100, when we use Parseval wide Resnet of depth 40 instead of 28, we achieve an accuracy of 81.76%.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"In comparison, the best performance achieved by a vanilla wide resnet (Zagoruyko & Komodakis, 2016) and a pre-activation resnet (He et al., 2016) are respectively 81.12% and 77.29%.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Therefore, our proposal is a useful regularizer for legitimate examples.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Also note that in most cases, Parseval networks combining both the orthogonality constraint and the convexity constraint is superior to use the orthogonality constraint solely.
",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
The results presented in the table validate our most important claim: Parseval networks significantly improve the robustness of vanilla models to adversarial examples.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"When no adversarial training is used, the gap in accuracy be-
tween the two methods is significant (particularly in the high noise scenario).",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"For an SNR value of 40, the best Parseval network achieves 55.41% accuracy while the best vanilla model is at 44.62%.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"When the models are adversarially trained, Parseval networks remain superior to vanilla models in most cases.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Interestingly, adversarial training only slightly improves the robustness of Parseval networks in low noise setting (e.g. SNR values of 45-50) and sometimes even deteriorates it (e.g. on CIFAR-10).",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"In contrast, combining adversarial training and Parseval networks is an effective approach in the high noise setting.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"This result suggests that thanks to the particular form of regularizer (controlling the Lipschitz constant of the network), Parseval networks achieves robustness to adversarial examples located in the immediate vicinity of each data point.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Therefore, adversarial training only helps for adversarial examples found further away from the legitimate patterns.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"This observation holds consistently across all our datasets.
",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Better use of capacity Given the distribution of singular values observed in Figure 2, we want to analyze the intrinsic dimensionality of the representation learned by the different networks at every layer.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"To that end, we use the local covariance dimension (Dasgupta & Freund, 2008) which can be measured from the covariance matrix of the data.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"For each layer k of the fully connected network, we compute the activation’s empirical covariance matrix 1 n",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"∑n i=1 φk(x)φk(x)
",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
>,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
and obtain its sorted eigenvalues σ1 ≥ · · · ≥ σd.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"For each method and each layer, we select the smallest integer p such that ∑p i=1",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
σi ≥ 0.99 ∑d i=1,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
σi.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
This gives us the number of dimensions that we need to explain 99% of the covariance.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"We can also compute the same quantity for the examples of each class, by only considering in the empirical estimation of the covariance of the examples xi such that yi = c. Table 2 report these numbers for all examples and the per-class average on CIFAR-10.
Table 2 shows that the local covariance dimension of all the data is consistently higher for Parseval networks than all the other approaches at any layer of the network.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"SGDwd-da contracts all the data in very low dimensional spaces at the upper levels of the network by using only 0.4% of the total dimension (layer 3 and 4) while Parseval networks use about 81% and 56% at of the whole dimension respectively
in the same layers.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"This is intriguing given that SGD-wd-da also increases the robustness of the network, apparently not in the same way as Parseval networks.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"For the average local covariance dimension of the classes, SGD-wd-da contracts each class into the same dimensionality as it contracts all the data at the upper layers of the network.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"For Parseval, the data of each class is contracted in about 30% and 19% of the overall dimension.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"These results suggest that Parseval contracts the data of each class in a lower dimensional manifold (compared to the intrinsic dimensionality of the whole data) hence making classification easier.
faster convergence Parseval networks converge significantly faster than vanilla networks trained with batch normalization and dropout as depicted by figure 4.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Thanks to the orthogonalization step following each gradient update, the weight matrices are well conditioned at each step during the optimization.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
We hypothesize this is the main explanation of this phenomenon.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"For convolutional networks (resnets), the faster convergence is not obtained at the expense of larger wall-time since the cost of the projection step is negligible compared to the total cost of the forward pass on modern GPU architecture thanks to the small size of the filters.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"We introduced Parseval networks, a new approach for learning neural networks that are intrinsically robust to adversarial noise.",6. Conclusion,[0],[0]
We proposed an algorithm that allows us to optimize the model efficiently.,6. Conclusion,[0],[0]
Empirical results on three classification datasets with fully connected and wide residual networks illustrate the performance of our approach.,6. Conclusion,[0],[0]
"As a byproduct of the regularization we propose, the model trains faster and makes a better use of its capacity.",6. Conclusion,[0],[0]
Further investigation of this phenomenon is left to future work.,6. Conclusion,[0],[0]
"The authors would like to thank M.A. Ranzato, Y. Tian, A. Bordes and F. Perronnin for their valuable feedback on this work.",Acknowledgements,[0],[0]
"We introduce Parseval networks, a form of deep neural networks in which the Lipschitz constant of linear, convolutional and aggregation layers is constrained to be smaller than 1.",abstractText,[0],[0]
Parseval networks are empirically and theoretically motivated by an analysis of the robustness of the predictions made by deep neural networks when their input is subject to an adversarial perturbation.,abstractText,[0],[0]
"The most important feature of Parseval networks is to maintain weight matrices of linear and convolutional layers to be (approximately) Parseval tight frames, which are extensions of orthogonal matrices to non-square matrices.",abstractText,[0],[0]
We describe how these constraints can be maintained efficiently during SGD.,abstractText,[0],[0]
"We show that Parseval networks match the state-of-the-art in terms of accuracy on CIFAR-10/100 and Street View House Numbers (SVHN), while being more robust than their vanilla counterpart against adversarial examples.",abstractText,[0],[0]
"Incidentally, Parseval networks also tend to train faster and make a better usage of the full capacity of the networks.",abstractText,[0],[0]
Parseval Networks: Improving Robustness to Adversarial Examples,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2331–2336, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics
We recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve a new state of the art for constituency Penn Treebank parsing — 93.8 F1 on section 23, using 2-21 as training, 24 as development, plus tri-training. When trees are converted to Stanford dependencies, UAS and LAS are 95.9% and 94.1%.",text,[0],[0]
"Recent work on deep learning syntactic parsing models has achieved notably good results, e.g., Dyer et al. (2016) with 92.4 F1 on Penn Treebank constituency parsing and Vinyals et al. (2015) with 92.8 F1.",1 Introduction,[0],[0]
"In this paper we borrow from the approaches of both of these works and present a neural-net parse reranker that achieves very good results, 93.8 F1, with a comparatively simple architecture.
",1 Introduction,[0],[0]
In the remainder of this section we outline the major difference between this and previous work — viewing parsing as a language modeling problem.,1 Introduction,[0],[0]
Section 2 looks more closely at three of the most relevant previous papers.,1 Introduction,[0],[0]
"We then describe our exact model (Section 3), followed by the experimental setup and results (Sections 4 and 5).",1 Introduction,[0],[0]
"Formally, a language model (LM) is a probability distribution over strings of a language:
P (x) = P (x1, · · · , xn)
=
n∏
t=1
P (xt|x1, · · · , xt−1), (1)
where x is a sentence and t indicates a word position.",1.1 Language Modeling,[0],[0]
"The efforts in language modeling go into computing P (xt|x1, · · · , xt−1), which as described next is useful for parsing as well.",1.1 Language Modeling,[0],[0]
"A generative parsing model parses a sentence (x) into its phrasal structure (y) according to
argmax y′∈Y(x)
P (x,y′),
where Y(x) lists all possible structures of x.",1.2 Parsing as Language Modeling,[0],[0]
"If we think of a tree (x,y) as a sequence (z) (Vinyals et
2331
al., 2015) as illustrated in Figure 1, we can define a probability distribution over (x,y) as follows:
P (x,y) = P (z) = P (z1, · · · , zm)
= m∏
t=1
P (zt|z1, · · · , zt−1), (2)
which is equivalent to Equation (1).",1.2 Parsing as Language Modeling,[0],[0]
"We have reduced parsing to language modeling and can use language modeling techniques of estimating P (zt|z1, · · · , zt−1) for parsing.",1.2 Parsing as Language Modeling,[0],[0]
We look here at three neural net (NN) models closest to our research along various dimensions.,2 Previous Work,[0],[0]
"The first (Zaremba et al., 2014) gives the basic language modeling architecture that we have adopted, while the other two (Vinyals et al., 2015; Dyer et al., 2016) are parsing models that have the current best results in NN parsing.",2 Previous Work,[0],[0]
"The LSTM-LM of Zaremba et al. (2014) turns (x1, · · · , xt−1) into ht, a hidden state of an LSTM (Hochreiter and Schmidhuber, 1997; Gers et al., 2003; Graves, 2013), and uses ht to guess xt:
P (xt|x1, · · · , xt−1) = P (xt|ht) = softmax(Wht)[xt],
where W is a parameter matrix and [i] indexes ith element of a vector.",2.1 LSTM-LM,[0],[0]
"The simplicity of the model makes it easily extendable and scalable, which has inspired a character-based LSTM-LM that works well for many languages (Kim et al., 2016) and an ensemble of large LSTM-LMs for English with astonishing perplexity of 23.7 (Jozefowicz et al., 2016).",2.1 LSTM-LM,[0],[0]
"In this paper, we build a parsing model based on the LSTM-LM of Zaremba et al. (2014).",2.1 LSTM-LM,[0],[0]
"Vinyals et al. (2015) observe that a phrasal structure (y) can be expressed as a sequence and build a machine translation parser (MTP), a sequence-tosequence model, which translates x into y using a
conditional probability:
P (y|x)",2.2 MTP,[0],[0]
"= P (y1, · · · , yl|x)
= l∏
t=1
P (yt|x, y1, · · · , yt−1),
where the conditioning event (x, y1, · · · , yt−1) is modeled by an LSTM encoder and an LSTM decoder.",2.2 MTP,[0],[0]
"The encoder maps x into he, a set of vectors that represents x, and the decoder obtains a summary vector (h′t) which is concatenation of the decoder’s hidden state (hdt ) and weighted sum of word representations ( ∑n i=1 αih e i ) with an alignment vector (α).",2.2 MTP,[0],[0]
Finally the decoder predicts yt given h′t.,2.2 MTP,[0],[0]
"Inspired by MTP, our model processes sequential trees.",2.2 MTP,[0],[0]
"Recurrent Neural Network Grammars (RNNG), a generative parsing model, defines a joint distribution over a tree in terms of actions the model takes to generate the tree (Dyer et al., 2016):
P (x,y) = P (a) =
m∏
t=1
P (at|a1, · · · , at−1), (3)
where a is a sequence of actions whose output precisely matches the sequence of symbols in z, which implies Equation (3) is the same as Equation (2).",2.3 RNNG,[0],[0]
"RNNG and our model differ in how they compute the conditioning event (z1, · · · , zt−1): RNNG combines hidden states of three LSTMs that keep track of actions the model has taken, an incomplete tree the model has generated and words the model has generated whereas our model uses one LSTM’s hidden state as shown in the next section.",2.3 RNNG,[0],[0]
"Our model, the model of Zaremba et al. (2014) applied to sequential trees and we call LSTM-LM from now on, is a joint distribution over trees:
P (x,y)",3 Model,[0],[0]
"= P (z) = m∏
t=1
P (zt|z1, · · · , zt−1)
=
m∏
t=1
P (zt|ht)
= m∏
t=1
softmax(Wht)[zt],
where ht is a hidden state of an LSTM.",3 Model,[0],[0]
"Due to lack of an algorithm that searches through an exponentially large phrase-structure space, we use an n-best parser to reduce Y(x) to Y ′(x), whose size is polynomial, and use LSTM-LM to find y that satisfies
argmax y′∈Y ′(x)
P (x,y′).",3 Model,[0],[0]
(4),3 Model,[0],[0]
"The model has three LSTM layers with 1,500 units and gets trained with truncated backpropagation through time with mini-batch size 20 and step size 50.",3.1 Hyper-parameters,[0],[0]
"We initialize starting states with previous minibatch’s last hidden states (Sutskever, 2013).",3.1 Hyper-parameters,[0],[0]
"The forget gate bias is initialized to be one (Jozefowicz et al., 2015) and the rest of model parameters are sampled from U(−0.05, 0.05).",3.1 Hyper-parameters,[0],[0]
"Dropout is applied to non-recurrent connections (Pham et al., 2014) and gradients are clipped when their norm is bigger than 20 (Pascanu et al., 2013).",3.1 Hyper-parameters,[0],[0]
"The learning rate is 0.25 · 0.85max( −15, 0) where is an epoch number.",3.1 Hyper-parameters,[0],[0]
"For simplicity, we use vanilla softmax over an entire vocabulary as opposed to hierarchical softmax (Morin and Bengio, 2005) or noise contrastive estimation (Gutmann and Hyvärinen, 2012).",3.1 Hyper-parameters,[0],[0]
"We describe datasets we use for evaluation, detail training and development processes.1",4 Experiments,[0],[0]
"We use the Wall Street Journal (WSJ) of the Penn Treebank (Marcus et al., 1993) for training (2-21), development (24) and testing (23) and millions of auto-parsed “silver” trees (McClosky et al., 2006; Huang et al., 2010; Vinyals et al., 2015) for tritraining.",4.1 Data,[0],[0]
"To obtain silver trees, we parse the entire section of the New York Times (NYT) of the fifth Gigaword (Parker et al., 2011) with a product of eight Berkeley parsers (Petrov, 2010)2 and ZPar (Zhu et al., 2013) and select 24 million trees on which both parsers agree (Li et al., 2014).",4.1 Data,[0],[0]
"We do not resample trees to match the sentence length distribution of the NYT to that of the WSJ (Vinyals et
1The code and trained models used for experiments are available at github.com/cdg720/emnlp2016.
",4.1 Data,[0],[0]
2We use the reimplementation by Huang et al. (2010).,4.1 Data,[0],[0]
"al., 2015) because in preliminary experiments Charniak parser (Charniak, 2000) performed better when trained on all of 24 million trees than when trained on resampled two million trees.
",500 97.0 91.8 40.0,[0],[0]
"Given x, we produce Y ′(x), 50-best trees, with Charniak parser and find y with LSTM-LM as Dyer et al. (2016) do with their discriminative and generative models.3",500 97.0 91.8 40.0,[0],[0]
"We unk words that appear fewer than 10 times in the WSJ training (6,922 types) and drop activations with probability 0.7.",4.2.1 Supervision,[0],[0]
"At the beginning of each epoch, we shuffle the order of trees in the training data.",4.2.1 Supervision,[0],[0]
Both perplexity and F1 of LSTM-LM (G) improve and then plateau (Figure 2).,4.2.1 Supervision,[0],[0]
"Perplexity, the
3Dyer et al. (2016)’s discriminative model performs comparably to Charniak (89.8 vs. 89.7).
model’s training objective, nicely correlates with F1, what we care about.",4.2.1 Supervision,[0],[0]
Training takes 12 hours (37 epochs) on a Titan X. We also evaluate our model with varying n-best trees including optimal 51-best trees that contain gold trees (51o).,4.2.1 Supervision,[0],[0]
"As shown in Table 1, the LSTM-LM (G) is robust given sufficiently large n, i.e. 50, but does not exhibit its full capacity because of search errors in Charniak parser.",4.2.1 Supervision,[0],[0]
We address this problem in Section 5.3.,4.2.1 Supervision,[0],[0]
"We unk words that appear at most once in the training (21,755 types).",4.2.2 Semi-supervision,[0],[0]
"We drop activations with probability 0.45, smaller than 0.7, thanks to many silver trees, which help regularization.",4.2.2 Semi-supervision,[0],[0]
"We train LSTM-LM (GS) on the WSJ and a different set of 400,000 NYT trees for each epoch except for the last one during which we use the WSJ only.",4.2.2 Semi-supervision,[0],[0]
Training takes 26 epochs and 68 hours on a Titan X. LSTMLM (GS) achieves 92.5 F1 on the development.,4.2.2 Semi-supervision,[0],[0]
"As shown in Table 2, with 92.6 F1 LSTM-LM (G) outperforms an ensemble of five MTPs (Vinyals et al., 2015) and RNNG (Dyer et al., 2016), both of which are trained on the WSJ only.",5.1 Supervision,[0],[0]
"We compare LSTM-LM (GS) to two very strong semi-supervised NN parsers: an ensemble of five MTPs trained on 11 million trees of the highconfidence corpus4 (HC) (Vinyals et al., 2015); and an ensemble of six one-to-many sequence models
4The HC consists of 90,000 gold trees, from the WSJ, English Web Treebank and Question Treebank, and 11 million silver trees, whose sentence length distribution matches that of the WSJ, parsed and agreed on by Berkeley parser and ZPar.
trained on the HC and 4.5 millions of EnglishGerman translation sentence pairs (Luong et al., 2016).",5.2 Semi-supervision,[0],[0]
We also compare LSTM-LM (GS) to best performing non-NN parsers in the literature.,5.2 Semi-supervision,[0],[0]
Parsers’ parsing performance along with their training data is reported in Table 3.,5.2 Semi-supervision,[0],[0]
LSTM-LM (GS) outperforms all the other parsers with 93.1 F1.,5.2 Semi-supervision,[0],[0]
"Due to search errors – good trees are missing in 50-best trees – in Charniak (G), our supervised and semi-supervised models do not exhibit their full potentials when Charniak (G) provides Y ′(x).",5.3 Improved Semi-supervision,[0],[0]
"To mitigate the search problem, we tri-train Charniak (GS) on all of 24 million NYT trees in addition to the WSJ, to yield Y ′(x).",5.3 Improved Semi-supervision,[0],[0]
"As shown in Table 3, both LSTM-LM (G) and LSTM-LM (GS) are affected by the quality of Y ′(x).",5.3 Improved Semi-supervision,[0],[0]
"A single LSTM-LM (GS) together with Charniak (GS) reaches 93.6 and an ensemble of eight LSTM-LMs (GS) with Charniak (GS) achieves a new state of the art, 93.8 F1.",5.3 Improved Semi-supervision,[0],[0]
"When trees are converted to Stanford dependencies,5 UAS and LAS are 95.9% and 94.1%,6 more than 1% higher than those of the state of the art dependency parser (Andor et al., 2016).",5.3 Improved Semi-supervision,[0],[0]
"Why an indirect method (converting trees to dependencies) is more accurate than a direct one (dependency parsing) remains unanswered (Kong and Smith, 2014).",5.3 Improved Semi-supervision,[0],[0]
The generative parsing model we presented in this paper is very powerful.,6 Conclusion,[0],[0]
"In fact, we see that a generative parsing model, LSTM-LM, is more effective than discriminative parsing models (Dyer et al., 2016).",6 Conclusion,[0],[0]
"We suspect building large models with character embeddings would lead to further improvement as in language modeling (Kim et al., 2016; Jozefowicz et al., 2016).",6 Conclusion,[0],[0]
We also wish to develop a complete parsing model using the LSTMLM framework.,6 Conclusion,[0],[0]
"We thank the NVIDIA corporation for its donation of a Titan X GPU, Tstaff of Computer Science
5Version 3.3.0.",Acknowledgments,[0],[0]
6We use the CoNLL evaluator available through the CoNLL website: ilk.uvt.nl/conll/software/eval.pl.,Acknowledgments,[0],[0]
"Following the convention, we ignore punctuation.
at Brown University for setting up GPU machines and David McClosky for helping us train Charniak parser on millions trees.",Acknowledgments,[0],[0]
"We recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve a new state of the art for constituency Penn Treebank parsing — 93.8 F1 on section 23, using 2-21 as training, 24 as development, plus tri-training.",abstractText,[0],[0]
"When trees are converted to Stanford dependencies, UAS and LAS are 95.9% and 94.1%.",abstractText,[0],[0]
Parsing as Language Modeling,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 69–81 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"While parsing has become a relatively mature technology for written text, parser performance on conversational speech lags behind.",1 Introduction,[0],[0]
"Speech poses challenges for parsing: transcripts may contain errors and lack punctuation; even perfect transcripts can be difficult to handle because of disfluencies (restarts, repetitions, and self-corrections), filled pauses (“um”, “uh”), interjections (“like”), parentheticals (“you know”, “I mean”), and sentence fragments.",1 Introduction,[0],[0]
"Some of these phenomena can be handled in standard grammars, but disfluencies typically require extensions of the model.",1 Introduction,[0],[0]
"Different approaches have been explored in both constituency parsing (Charniak and Johnson, 2001; Johnson and Charniak, 2004) and dependency parsing (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014).
∗Equal Contribution.
",1 Introduction,[0],[0]
"Despite these challenges, speech carries helpful extra information – beyond the words – associated with the prosodic structure of an utterance and encoded via variation in timing and intonation.",1 Introduction,[0],[0]
"Speakers pause in locations that are correlated with syntactic structure (Grosjean et al., 1979), and listeners use prosodic structure in resolving syntactic ambiguities (Price et al., 1991).",1 Introduction,[0],[0]
"Prosodic cues also signal disfluencies by marking the interruption point (Shriberg, 1994).",1 Introduction,[0],[0]
"However, most speech parsing systems in practice take little advantage of these cues.",1 Introduction,[0],[0]
"Our study focuses on this last challenge, aiming to incorporate prosodic cues in a neural parser, handling disfluencies as constituents via a neural attention mechanism.
",1 Introduction,[0],[0]
"A challenge of incorporating prosody in parsing is that multiple acoustic cues interact to signal prosodic structure, including pauses, lengthening, fundamental frequency modulation, and spectral shape.",1 Introduction,[0],[0]
"These cues also vary with the phonetic segment, emphasis, emotion and speaker, so feature extraction typically involves multiple time windows and normalization techniques.",1 Introduction,[0],[0]
"The most successful constituent parsers have mapped these features to prosodic boundary posteriors by using labeled training data (Kahn et al., 2005; Hale et al., 2006; Dreyer and Shafran, 2007).",1 Introduction,[0],[0]
The approach proposed here takes advantage of advances in neural networks to automatically learn a good feature representation without the need to explicitly represent prosodic constituents.,1 Introduction,[0.9574094050058131],"['As is often done in agnostic approaches, we assume the availability of an oracle which reduces to a standard learning setting and knows how to efficiently leverage the structure of the model class.']"
"To narrow the scope of this work and facilitate error analysis, our experiments use known transcripts and sentence segmentation.
",1 Introduction,[0],[0]
Our work offers the following contributions.,1 Introduction,[0],[0]
We introduce a framework for directly integrating acoustic-prosodic features with text in a neural encoder-decoder parser that does not require handannotated prosodic structure.,1 Introduction,[0],[0]
"We demonstrate improvements in constituent parsing of conversational
69
speech over a high-quality text-only parser and provide analyses showing where prosodic features help and that assessment of their utility is affected by human transcription errors.",1 Introduction,[0],[0]
Our model maps a sequence of word-level input features to a linearized parse output sequence.,2 Task and Model Description,[0],[0]
"The word-level input feature vector consists of the concatenation of (learnable) word embeddings ei and several types of acoustic-prosodic features, described in Section 2.3.",2 Task and Model Description,[0],[0]
"We assume the availability of a training treebank of conversational speech (in our case, SwitchboardNXT (Calhoun et al., 2010)) and corresponding constituent parses.",2.1 Task Setup,[0],[0]
The transcriptions are preprocessed by removing punctuation and lower-casing all text to better mimic the speech recognition setting.,2.1 Task Setup,[0],[0]
"Following Vinyals et al. (2015), the parse trees are linearized, and pre-terminals are normalized as “XX” (see Appendix A.1).",2.1 Task Setup,[0],[0]
Our attention-based encoder-decoder model is similar to the one used by Vinyals et al. (2015).,2.2 Encoder-Decoder Parser,[0],[0]
"The encoder is a deep long short-term memory recurrent neural network (LSTM-RNN) (Hochreiter and Schmidhuber, 1997) that reads in a word-level inputs,1 represented as a sequence of vectors x = (x1, · · · ,xTs), and outputs high-level features h = (h1, · · · ,hTs) where hi = LSTM(xi,hi−1).2
",2.2 Encoder-Decoder Parser,[0],[0]
"The parse decoder is also a deep LSTM-RNN that predicts the linearized parse sequence y = (y1, · · · , yTo) as follows:
P (y|x) = To∏
t=1
P (yt|h,y<t)
",2.2 Encoder-Decoder Parser,[0],[0]
"In attention-based models, the posterior distribution of the output yt at time step t is given by:
P (yt|h,y<t) =",2.2 Encoder-Decoder Parser,[0],[0]
softmax(W s[ct;dt],2.2 Encoder-Decoder Parser,[0],[0]
"+ bs),
where vector bs and matrix W s are learnable parameters; ct is referred to as a context vector that summarizes the encoder’s output h; and dt is the
1As in Vinyals et al. (2015)",2.2 Encoder-Decoder Parser,[0],[0]
"the input sequence is processed in reverse order, as shown in Figure 1.
",2.2 Encoder-Decoder Parser,[0],[0]
2For brevity we omit the LSTM equations.,2.2 Encoder-Decoder Parser,[0],[0]
"The details can be found, e.g., in Zaremba et al. (2014).
",2.2 Encoder-Decoder Parser,[0],[0]
"decoder hidden state at time step t, which captures the previous output sequence context y<t.
uit = v >",2.2 Encoder-Decoder Parser,[0],[0]
tanh(W 1hi,2.2 Encoder-Decoder Parser,[0],[0]
"+W 2dt + ba)
",2.2 Encoder-Decoder Parser,[0],[0]
"αt = softmax(ut) ct = Ts∑
i=1
αtihi
where vectors v, ba and matrices W 1, W 2 are learnable parameters; ut and αt are the attention score and attention weight vector, respectively, for decoder time step t.
The above attention mechanism is only contentbased, i.e., it is only dependent on hi, dt.",2.2 Encoder-Decoder Parser,[0],[0]
"It is not location-aware, i.e., it does not consider the “location” of the previous attention vector.",2.2 Encoder-Decoder Parser,[0],[0]
"For parsing conversational text, location awareness is beneficial since disfluent structures can have duplicate words/phrases that may “confuse” the attention mechanism.
",2.2 Encoder-Decoder Parser,[0],[0]
"In order to make the model location-aware, the attention mechanism takes into account the previous attention weight vector αt−1.",2.2 Encoder-Decoder Parser,[0],[0]
"In particular, we use the attention mechanism proposed by Chorowski et al. (2015), in which αt−1 is represented via a feature vector f t = F ∗αt−1, where F ∈ Rk×r represents k learnable convolution filters of width r. The filters are used for performing 1-D convolution over αt−1 to extract k features f ti for each time step i of the input sequence.",2.2 Encoder-Decoder Parser,[0],[0]
"The extracted features are then incorporated in the alignment score calculation as:
uit = v >",2.2 Encoder-Decoder Parser,[0],[0]
tanh(W 1hi,2.2 Encoder-Decoder Parser,[0],[0]
+W 2dt,2.2 Encoder-Decoder Parser,[0],[0]
"+W ff ti + ba)
where W f is another learnable parameter matrix.",2.2 Encoder-Decoder Parser,[0],[0]
"Finally, the decoder state dt is computed as dt = LSTM([ỹt−1; ct−1],dt−1), where ỹt−1 is the embedding vector corresponding to the previous output symbol yt−1.",2.2 Encoder-Decoder Parser,[0],[0]
"As we will see in Sec. 4.1, the location-aware attention mechanism is especially useful for handling disfluencies.",2.2 Encoder-Decoder Parser,[0],[0]
"In previous work using encoder-decoder models for parsing (Vinyals et al., 2015; Luong et al., 2016), vector xi is simply the word embedding ei of the word at position i of the input sentence.",2.3 Acoustic-Prosodic Features,[0],[0]
"For parsing conversational speech, we can incorporate acousticprosodic features.",2.3 Acoustic-Prosodic Features,[0],[0]
"Here we explore four types of features widely used in computational models of prosody: pauses, duration lengthening, fundamental frequency, and energy.",2.3 Acoustic-Prosodic Features,[0],[0]
"Since prosodic cues are
at sub- and multi-word time scales, they are integrated with the encoder-decoder using different mechanisms.
",2.3 Acoustic-Prosodic Features,[0],[0]
All features are extracted from transcriptions that are time-aligned at the word level.3,2.3 Acoustic-Prosodic Features,[0],[0]
We use time alignments associated with the corpus to be consistent with other studies.,2.3 Acoustic-Prosodic Features,[0],[0]
"In a small number of cases, the time alignment for a particular word boundary is missing.",2.3 Acoustic-Prosodic Features,[0],[0]
Some cases are due to tokenization.,2.3 Acoustic-Prosodic Features,[0],[0]
"For example, contractions, such as don’t in the original transcript, are treated as separated words for the parser (do and n’t), and the internal word boundary time is missing.",2.3 Acoustic-Prosodic Features,[0],[0]
"In such cases, these internal times are estimated.",2.3 Acoustic-Prosodic Features,[0],[0]
"In other cases, there are transcription mismatches that lead to missing time alignments, where we cannot estimate times.",2.3 Acoustic-Prosodic Features,[0],[0]
"For the roughly 1% of sentences where time alignments are missing, we simply back off to the text-based parser.
Pause.",2.3 Acoustic-Prosodic Features,[0],[0]
"The pause feature vector pi for word i is the concatenation of pre-word pause feature ppre,i and post-word pause feature ppost,i, where each subvector is a learned embedding for 6 pause categories: no pause, missing, 0 < p ≤ 0.05 s, 0.05 s < p ≤ 0.2 s, 0.2 < p ≤ 1 s, and p > 1 s (including turn boundaries).",2.3 Acoustic-Prosodic Features,[0],[0]
The bins are chosen based on the observed distribution (see Appendix A.1).,2.3 Acoustic-Prosodic Features,[0],[0]
"We did not use (real-valued) pause duration directly, for two main reasons: (1) to handle missing time alignments; and (2) duration of pause does
3The assumption of known word alignments is standard for prosodic feature extraction in many spoken language processing studies.",2.3 Acoustic-Prosodic Features,[0],[0]
"Time alignments can be obtained as a by-product of recognition or from forced alignment.
not matter beyond a threshold (e.g. p > 1 s).
",2.3 Acoustic-Prosodic Features,[0],[0]
Word duration.,2.3 Acoustic-Prosodic Features,[0],[0]
"Both word duration and wordfinal duration lengthening are strong cues to prosodic phrase boundaries (Wightman et al., 1992; Pate and Goldwater, 2013).",2.3 Acoustic-Prosodic Features,[0],[0]
"The word duration feature δi is computed as the actual word duration divided by the mean duration of the word, clipped to a maximum value of 5.",2.3 Acoustic-Prosodic Features,[0],[0]
The sample mean is used for frequent words (count ≥ 15).,2.3 Acoustic-Prosodic Features,[0],[0]
For infrequent words we estimate the mean as the sum over the sample means for the phonemes in the word’s dictionary pronunciation.,2.3 Acoustic-Prosodic Features,[0],[0]
"We refer to the manually defined prosodic feature pair of pi and δi as φi.
Fundament frequency (f0) and Energy (E) contours (f0/E).",2.3 Acoustic-Prosodic Features,[0],[0]
We use a CNN to automatically learn the mapping from the time series of f0/E features to a word-level vector.,2.3 Acoustic-Prosodic Features,[0],[0]
"The contour features are extracted from 25-ms frames with 10-ms hops using Kaldi (Povey et al., 2011).",2.3 Acoustic-Prosodic Features,[0],[0]
"Three f0 features are used: warped Normalized Cross Correlation Function (NCCF), log-pitch with Probability of Voicing (",2.3 Acoustic-Prosodic Features,[0],[0]
"POV)-weighted mean subtraction over a 1.5-second window, and the estimated derivative (delta) of the raw log pitch.",2.3 Acoustic-Prosodic Features,[0],[0]
"Three energy features are extracted from the Kaldi 40-mel-frequency filter bank features: Etotal, the log of total energy normalized by dividing by the speaker side’s max total energy; Elow, the log of total energy in the lower 20 mel-frequency bands, normalized by total energy, and Ehigh, the log of total energy in the higher 20 mel-frequency bands, normalized by total energy.",2.3 Acoustic-Prosodic Features,[0],[0]
"Multi-band energy features are used as a
simple mechanism to capture articulatory strengthening at prosodic constituent onsets (Fourgeron and Keating, 1997).
",2.3 Acoustic-Prosodic Features,[0],[0]
Figure 1 summarizes the feature learning approach.,2.3 Acoustic-Prosodic Features,[0],[0]
The f0 and E features are processed at the word level: each sequence of frames corresponding to a time-aligned word (and potentially its surrounding context) is convolved with N filters of m sizes (a total of mN filters).,2.3 Acoustic-Prosodic Features,[0],[0]
The motivation for the multiple filter sizes is to enable the computation of features that capture information on different time scales.,2.3 Acoustic-Prosodic Features,[0],[0]
"For each filter, we perform a 1-D convolution over the 6-dimensional f0/E features with a stride of 1.",2.3 Acoustic-Prosodic Features,[0],[0]
"Each filter output is max-pooled, resulting in mN -dimensional speech features si.",2.3 Acoustic-Prosodic Features,[0],[0]
"Our overall acoustic-prosodic feature vector is the concatenation of pi, δi, and si in various combinations.",2.3 Acoustic-Prosodic Features,[0],[0]
"Our core corpus is Switchboard-NXT (Calhoun et al., 2010), a subset of the Switchboard corpus (Godfrey and Holliman, 1993): 2,400 telephone conversations between strangers; 642 of these were hand-annotated with syntactic parses and further augmented with richer layers of annotation facilitated by the NITE XML toolkit (Calhoun et al., 2010).",3.1 Dataset,[0],[0]
"Our sentence segmentations and syntactic trees are based on the annotations from the Treebank set, with a few manual corrections from the NXT release.",3.1 Dataset,[0],[0]
"This core dataset consists of 100K sentences, totaling 830K tokens forming a vocabulary of 13.5K words.",3.1 Dataset,[0],[0]
"We use the time alignments available from NXT, which is based on a corrected word transcript that occasionally differs from the Treebank, leading to some missing time alignments.",3.1 Dataset,[0.951517867060496],"['The challenge here is to maintain such version spaces and compute upper and lower confidence bounds efficiently, and we show that this can be done using a binary search together with a small number of regression oracle calls.']"
"We follow the sentence boundaries defined by the parsed data available,4 and the data split (90% train; 5% dev; 5% test) defined by related work done on Switchboard (Charniak and Johnson, 2001; Kahn et al., 2005; Honnibal and Johnson, 2014).",3.1 Dataset,[0],[0]
"The standard evaluation metric for constituent parsing is the parseval metric which uses bracketing precision, recall, and F1, as in the canonical implementation of EVALB.5",3.2 Evaluation Metrics and Baselines,[0],[0]
"For written text, punc-
4Note that these sentence units can be inconsistent with other layers of Switchboard annotations, such as slash units.
",3.2 Evaluation Metrics and Baselines,[0],[0]
"5http://nlp.cs.nyu.edu/evalb/
tuation is sometimes represented as part of the sequence and impacts the final score, but for speech the punctuation is not explicitly available so it does not contribute to the score.",3.2 Evaluation Metrics and Baselines,[0],[0]
Another challenge of transcribed speech is the presence of disfluencies.,3.2 Evaluation Metrics and Baselines,[0],[0]
"Speech repairs are indicated under “EDITED” nodes in Switchboard parse trees, which include structure under these nodes that is not of interest for simple text clean-up.",3.2 Evaluation Metrics and Baselines,[0],[0]
"Therefore, some studies report flattened-edit parseval F1 scores (“flatF1”), which is parseval computed on trees where the structure under edit nodes has been eliminated so that all leaves are immediate children.",3.2 Evaluation Metrics and Baselines,[0],[0]
"We report both scores for the baseline text-only model showing that the differences are small, then use the standard parseval F1 score for most results.6
Disfluencies are particularly problematic for statistical parsers, as explained by Charniak and Johnson (2001), and some systems incorporate a separate disfluency detection stage.",3.2 Evaluation Metrics and Baselines,[0.9537475483436009],"['Computationally tractable realizability-based algorithms are only known for specific model families, such as when the conditional reward distributions come from a generalized linear model.']"
"For this reason, and because it is useful for understanding system performance, most studies also report disfluency detection performance, which is measured in terms of the F1 score for detecting whether a word is in an edit region.",3.2 Evaluation Metrics and Baselines,[0],[0]
"Our approach does not involve a separate disfluency detection stage, but identifies disfluencies implicitly via the parse structure.",3.2 Evaluation Metrics and Baselines,[0],[0]
"Consequently, the disfluency detection results are not competitive with work that directly optimize for disfluency detection.",3.2 Evaluation Metrics and Baselines,[0],[0]
"We report disfluency detection scores primarily as a diagnostic.
",3.2 Evaluation Metrics and Baselines,[0],[0]
"Most previous work on integrating prosody and parsing has used the Switchboard corpus, but it is still difficult to compare results because of differences in constraints, objectives and the use of constituent vs. dependency structure, as discussed further in Section 6.",3.2 Evaluation Metrics and Baselines,[0.9561036856849726],"['The elimination and optimistic variants of RegCB have comparable performance, with elimination performing slightly better in aggregate.']"
The most relevant prior studies (on constituent parsing) that we compare to are a bit old.,3.2 Evaluation Metrics and Baselines,[0],[0]
The text-only result from our neural parser represents a stronger baseline and is important for decoupling the impact of prosody vs. the parsing framework.,3.2 Evaluation Metrics and Baselines,[0],[0]
Both the encoder and decoder are 3-layer deep LSTM-RNNs with 256 hidden units in each layer.,3.3 Model Training and Inference,[0],[0]
"For the location-aware attention, the convolution operation uses 5 filters of width 40 each.",3.3 Model Training and Inference,[0],[0]
"We use 512-dimensional embedding vectors to repre-
6A variant of the “flat-F1” score is used in (Charniak and Johnson, 2001; Kahn et al., 2005), which uses a relaxed edited node precision and recall but also ignores filled pauses.
sent words and linearized parsing symbols, such as “(S”.7
A number of configurations are explored for the acoustic-prosodic features, tuning based on dev set parsing performance.",3.3 Model Training and Inference,[0],[0]
"Pause embeddings are tuned over {4, 16, 32} dimensions.",3.3 Model Training and Inference,[0],[0]
"For the CNN, we try different configurations of filter widths w ∈ {",3.3 Model Training and Inference,[0],[0]
"[10, 25, 50], [5, 10, 25, 50]} and number of filters N ∈ {16, 32, 64, 128} for each filter width.8 These filter size combinations are chosen to capture f0 and energy phenomena on various levels: w = 5, 10 for sub-word, w = 25 for word, and w = 50 for word and extended context.",3.3 Model Training and Inference,[0],[0]
Our best model uses 32-dimensional pause embeddings and N = 32 filters of widthsw =,3.3 Model Training and Inference,[0],[0]
"[5, 10, 25, 50], which corresponds to m = 4 and 128 filters.
",3.3 Model Training and Inference,[0],[0]
"For optimization we use Adam (Kingma and Ba, 2014) with a minibatch size of 64.",3.3 Model Training and Inference,[0],[0]
"The initial learning rate is 0.001 which is decayed by a factor of 0.9 whenever training loss, calculated after every 500 updates, degrades relative to the worst of its previous 3 values.",3.3 Model Training and Inference,[0],[0]
All models are trained for up to 50 epochs with early stopping.,3.3 Model Training and Inference,[0],[0]
"For regularization, dropout with 0.3 probability is applied on the output of all LSTM layers (Pham et al., 2014).
",3.3 Model Training and Inference,[0],[0]
"For inference, we use a greedy decoder to generate the linearized parse.",3.3 Model Training and Inference,[0],[0]
The output token with maximum posterior probability is chosen at every time step and fed as input in the next time step.,3.3 Model Training and Inference,[0],[0]
The decoder stops upon producing the end-of-sentence symbol.,3.3 Model Training and Inference,[0],[0]
"We use TensorFlow (Abadi et al., 2015) to implement all models.9",3.3 Model Training and Inference,[0],[0]
"7The number of layers, dimension of hidden units, dimension of embedding, and convolutional attention filter parameters of the text-only parser were explored in earlier experiments on the development set and then fixed as described.
8Note that a filter of width 10 has size 6 × 10, since the features are of dimension 6.
",4.1 Text-only Results,[0],[0]
"9Our code resources can be found in Appendix A.1.
",4.1 Text-only Results,[0],[0]
"We first show our results on the model using only text (i.e. xi = ei) to establish a strong baseline, on top of which we can add acousticprosodic features.",4.1 Text-only Results,[0],[0]
We experiment with the contentonly attention model used by Vinyals et al. (2015) and the content+location attention of Chorowski et al. (2015).,4.1 Text-only Results,[0],[0]
"For comparison with previous nonneural models, we use a high-quality latent-variable parser, the Berkeley parser (Petrov et al., 2006), retrained on our Switchboard data.",4.1 Text-only Results,[0],[0]
Table 1 compares the three text-only models.,4.1 Text-only Results,[0],[0]
"In terms of F1, the content+location attention beats the Berkeley parser by about 2.5% and content-only attention by about 4.5%.",4.1 Text-only Results,[0],[0]
"Flat-F1 scores for both encoder-decoder models is lower than their corresponding F1 scores, suggesting that the encoder-decoder models do well on predicting the internal structure of EDIT nodes while the reverse is true for the Berkeley parser.
",4.1 Text-only Results,[0],[0]
"To explain the gains of content+location attention over content-only attention, we compare their scores on fluent (without EDIT nodes) and disfluent sentences, shown in Table 1.",4.1 Text-only Results,[0],[0]
It is clear that most of the gains for content+location attention are from disfluent sentences.,4.1 Text-only Results,[0],[0]
"A possible explanation is the presence of duplicate words or phrases in disfluent sentences, which can be problematic for a contentonly attention model.",4.1 Text-only Results,[0],[0]
"Since our best model is the content+location attention model, we will henceforth refer to it as the “CL-attn” text-only model.",4.1 Text-only Results,[0],[0]
"All models using acoustic-prosodic features are extensions of this model, which provides a strong text-only baseline.",4.1 Text-only Results,[0],[0]
"We extend our CL-attn model with the three kinds of acoustic-prosodic features: pause (p), word duration (δ), and CNN mappings of fundamental frequency (f0) and energy (E) features (f0/E-CNN).
",4.2 Adding Acoustic-Prosodic Features,[0],[0]
The results of several model configurations on our dev set are presented in Table 2.,4.2 Adding Acoustic-Prosodic Features,[0],[0]
"First, we note that adding any combination of acoustic-prosodic features (individually or in sets) improves performance over the text-only baseline.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"However, certain combinations of acoustic-prosodic features are not always better than their subsets.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
The text + p + δ + f0/E-CNN model that uses all three types of features has the best performance with a gain of 0.7% over the already-strong text-only baseline.,4.2 Adding Acoustic-Prosodic Features,[0],[0]
"We will henceforth refer to the text + p + δ + f0/E-CNN model as our “best model”.
",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"As a robustness check, we report results of averaging 10 runs on the CL-attn text-only and the best model in Table 3.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"We performed a bootstrap test (Efron and Tibshirani, 1993) that simulates 105 random test draws on the models giving median performance on the dev set.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
These median models gave a statistically significant difference between the text-only and best model (p-value < 0.02).,4.2 Adding Acoustic-Prosodic Features,[0],[0]
"Additionally, a simple t-test over the two sets of 10 results also shows statistical significance p-value < 0.03.
",4.2 Adding Acoustic-Prosodic Features,[0],[0]
Table 4 presents the results on the test set.,4.2 Adding Acoustic-Prosodic Features,[0],[0]
"Again, adding the acoustic-prosodic features improves over the text-only baseline.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"The gains are statistically significant for the best model with p-value < 0.02, again using a bootstrap test with simulated 105 random test draws on the two models.
",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"Table 5 includes results from prior studies that compare systems using text alone with ones that incorporate prosody, given hand transcripts and sentence segmentation.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"It is difficult to compare systems directly, because of the many differences in the experimental set-up.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"For example, the original Charniak and Johnson (2001) result (reporting F=85.9 for parsing and F=78.2 for disfluencies) leverages punctuation in the text stream, which is not realistic for speech transcripts and not used in most other work.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"Our work benefits from more text training material than others, but others benefit from gold part-of-speech tags.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
Kahn et al. (2005) use a modified sentence segmentation.,4.2 Adding Acoustic-Prosodic Features,[0],[0]
There are probably minor differences in handling of word fragments and scoring edit regions.,4.2 Adding Acoustic-Prosodic Features,[0],[0]
"Thus, this table primarily shows that our framework leads to more benefits from sentence-internal prosodic cues than others have obtained.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
Effect of sentence length.,5 Analysis,[0],[0]
"Figure 2 shows performance differences between our best model and the text-only model for varying sentence lengths.
",5 Analysis,[0],[0]
"Both models do worse on longer sentences, as expected since the corresponding parse trees tend to be more complex.",5 Analysis,[0],[0]
The performance difference between our best model and the text-only model increases with sentence length.,5 Analysis,[0],[0]
"This is likely because longer sentences more often have multiple prosodic phrases and disfluencies.
",5 Analysis,[0],[0]
Effect of disfluencies.,5 Analysis,[0],[0]
"Table 6 presents parse scores on the subsets of fluent and disfluent sentences, showing that the performance gain is in the disfluent set (65% of the dev set sentences).",5 Analysis,[0],[0]
"Because sentence boundaries are given, and so many fluent sentences in spontaneous speech are short, there is less potential for benefit from prosody in the fluent set.
",5 Analysis,[0],[0]
Types of errors.,5 Analysis,[0],[0]
"We use the Berkeley Parser Analyzer (Kummerfeld et al., 2012) to compare the types of errors made by the different parsers.10 Table 7 presents the relative error reductions over the text-only baseline achieved by the text + p model and our best model for disfluent sentences.",5 Analysis,[0],[0]
The two models differ in the types of error reductions they provide.,5 Analysis,[0],[0]
"Including pause information gives largest improvements on PP attachment and Modifier at-
10This analysis omits the 1% of the sentences that did not have timing information.
tachment errors.",5 Analysis,[0],[0]
"Adding the remaining acousticprosodic features helps to correct more types of attachment errors, especially VP and NP attachment.",5 Analysis,[0],[0]
Figure 3 demonstrates one case where the pause feature helps in correcting a PP attachment error made by a text-only parser.,5 Analysis,[0],[0]
"Other interesting examples (see Appendix A.2) suggest that the learned f0/E features help reduce NP attachment errors where the audio reveals a prominent word at the constituent boundary, even though there is no pause at that word.
",5 Analysis,[0],[0]
Effect of transcription errors.,5 Analysis,[0],[0]
The results and analyses so far have assumed that we have reliable transcripts.,5 Analysis,[0],[0]
"In fact, the original transcripts contained errors, and the Treebank annotators used these without reference to audio files.",5 Analysis,[0],[0]
"Mississippi State University (MS-State) ran a clean-up project
that produced more accurate word transcripts and time alignments (Deshmukh et al., 1998).",5 Analysis,[0],[0]
"The NXT corpus provides reconciliation between Treebank and MS-State transcripts in terms of annotating missed/extra/substituted words, but parses were not re-annotated.",5 Analysis,[0],[0]
The transcript errors mean that the acoustic signal is inconsistent with the “gold” parse tree.,5 Analysis,[0],[0]
"Below are some examples of “fluent” sentences (according to the Treebank transcripts) with transcription errors, for which prosodic features “hurt” parsing.",5 Analysis,[0],[0]
Words that transcribers missed are in brackets and those inserted are underlined.,5 Analysis,[0],[0]
S1: and because <,5 Analysis,[0],[0]
uh> like if your spouse died <all of a sudden you be> all alone it ’d be nice to go someplace with people similar to you to have friends S2: uh uh <i have had>,5 Analysis,[0],[0]
"my wife ’s picked up a couple of things saying uh boy if we could refinish that ’d be a beautiful piece of furniture
Multi-syllable errors are especially problematic, leading to serious inconsistencies between the text and the acoustic signal.",5 Analysis,[0],[0]
"Further, the missed words lead to an incorrect attachment in the “gold” parse in S1 and a missing restart edit in S2.",5 Analysis,[0],[0]
"Indeed, for sentences with consecutive transcript errors, which we expect to impact the prosodic features, there is a statistically significant (p-value < 0.05) negative effect on parsing with prosody.",5 Analysis,[0],[0]
"Not included in this analysis are sentence boundary errors, which also change the “gold” parse.",5 Analysis,[0],[0]
"Thus, prosody may be more useful than results here indicate.",5 Analysis,[0],[0]
"Related work on parsing conversational speech has mainly addressed four problems: speech recognition errors, unknown sentence segmentation, disfluencies, and integrating prosodic cues.",6 Related Work,[0],[0]
"Our work addresses the last two problems, which involve studies based on hand-transcribed text and known sentence boundaries, as in much speech parsing work.",6 Related Work,[0],[0]
The related studies are thus the focus of this discussion.,6 Related Work,[0],[0]
"We describe studies using the Switchboard corpus, since it has dominated work in this area, being the largest source of treebanked English spontaneous speech.
",6 Related Work,[0],[0]
"One major challenge of parsing conversational speech is the presence of disfluencies, which are grammatical and prosodic interruptions.",6 Related Work,[0],[0]
Disfluencies include repetitions (‘I am +,6 Related Work,[0],[0]
"I am’), repairs (‘I am +",6 Related Work,[0],[0]
"we are’), and restarts (‘What I",6 Related Work,[0],[0]
+,6 Related Work,[0],[0]
"Today is the...’), where the ‘+’ corresponds to an interruption point.",6 Related Work,[0],[0]
"Repairs often involve parallel grammatical
constructions, but they can be more complex, involving hedging, clarifications, etc.",6 Related Work,[0],[0]
"Charniak and Johnson (Charniak and Johnson, 2001; Johnson and Charniak, 2004) demonstrated that disfluencies are different in character than other constituents and that parsing performance improves from combining a PCFG parser with a separate module for disfluency detection via parse rescoring.",6 Related Work,[0],[0]
Our approach does not use a separate disfluency detection module; we hypothesized that the location-sensitive attention model helps handle these differences based on analysis of the text-only results (Table 1).,6 Related Work,[0],[0]
"However, more explicit modeling of disfluency pattern match characteristics in a dependency parser (Honnibal and Johnson, 2014) leads to better disfluency detection performance (F = 84.1 vs. 76.7 for our text only model).",6 Related Work,[0],[0]
"Pattern match features also benefit a neural model for disfluency detection alone (F = 87.0) (Zayats et al., 2016), and similar gains are observed by formulating disfluency detection in a transition-based framework (F = 87.5) (Wang et al., 2017).",6 Related Work,[0],[0]
"Experiments with oracle disfluencies as features improve the CL-attn text-only parsing performance from 87.85 to 89.38 on the test set, showing that more accurate disfluency modeling is a potential area of improvement.
",6 Related Work,[0],[0]
"It is well known that prosodic features play a role in human resolution of syntactic ambiguities, with more than two decades of studies seeking to incorporate prosodic features in parsing.",6 Related Work,[0],[0]
"A series of studies looked at constituent parsing informed by the presence (or likelihood) of prosodic breaks at word boundaries (Kahn et al., 2004, 2005; Hale et al., 2006; Dreyer and Shafran, 2007).",6 Related Work,[0],[0]
"Our approach improves over performance of these systems using raw acoustic features, without the need for handlabeling prosodic breaks.",6 Related Work,[0],[0]
"The gain is in part due to the improved text-based parser, but the incremental benefit of prosody here is similar to that in these prior studies.",6 Related Work,[0],[0]
"(In prior work using acoustic feature directly (Gregory et al., 2004), prosody actually degraded performance.)",6 Related Work,[0],[0]
"Our analyses of the impact of prosody also extends prior work.
",6 Related Work,[0],[0]
"Prosody is also known to provide useful cues to sentence boundaries (Liu et al., 2006), and automatic sentence segmentation performance has been shown to have a significant impact on parsing performance (Kahn and Ostendorf, 2012).",6 Related Work,[0],[0]
"In our study, sentence boundaries are given so as to focus on the role of prosody in resolving sentenceinternal parse ambiguity, for which prior work had
obtained smaller gains.",6 Related Work,[0],[0]
"Studies have also shown that parsing lattices or confusion networks can improve ASR performance (Kahn and Ostendorf, 2012; Yoshikawa et al., 2016).",6 Related Work,[0],[0]
"Our analysis of performance degradation for the system with prosody when the gold transcript and associated parse are in error suggests that prosody may have benefits for parsers operating on alternative ASR hypotheses.
",6 Related Work,[0],[0]
The results we compare to in Section 4 are relatively old.,6 Related Work,[0],[0]
"More recent parsing results on spontaneous speech involve dependency parsers using only text (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016), with the exception of a study on unsupervised dependency parsing (Pate and Goldwater, 2013).",6 Related Work,[0],[0]
"With the recent success of transition-based neural approaches in dependency parsing, researchers have adapted transition-based ideas to constituent parsing (Zhu et al., 2013; Watanabe and Sumita, 2015; Dyer et al., 2016).",6 Related Work,[0],[0]
"These approaches have not yet been used with speech, to our knowledge, but we expect it to be straightforward to extend our prosody integration framework to these systems, both for dependency and constituency parsing.",6 Related Work,[0],[0]
We have presented a framework for directly integrating acoustic-prosodic features with text in a neural encoder-decoder parser that does not require hand-annotated prosodic structure.,7 Conclusion,[0],[0]
"On conversational sentences, we obtained strong results when including word-level acoustic-prosodic features over using only transcriptions.",7 Conclusion,[0],[0]
"The acousticprosodic features provide the largest gains when sentences are disfluent or long, and analysis of error types shows that these features are especially helpful in repairing attachment errors.",7 Conclusion,[0],[0]
"In cases where prosodic features hurt performance, we observe a statistically significant negative effect caused by imperfect human transcriptions that make the “ground truth” parse tree and the acoustic signal inconsistent, which suggests that there is more to be gained from prosody than observed in prior studies.",7 Conclusion,[0],[0]
"We thus plan to investigate aligning the Treebank and MS-State versions of Switchboard for future work.
",7 Conclusion,[0],[0]
"Here, we assumed known sentence boundaries and hand transcripts, leaving open the question of whether increased benefits from prosody can be gained by incorporating sentence segmentation in parsing and/or in parsing ASR lattices.",7 Conclusion,[0],[0]
"Most prior work using prosody in parsing has been on con-
stituent parsing, since prosodic cues tend to align with constituent boundaries.",7 Conclusion,[0],[0]
"However, it remains an open question as to whether dependency, constituency or other parsing frameworks are better suited to leveraging prosody.",7 Conclusion,[0],[0]
"Our study builds on a parser that uses reverse order text processing, since it provides a stronger text-only baseline.",7 Conclusion,[0],[0]
"However, the prosody modeling component relies only on a 1 second lookahead of the current word (for pause binning), so it could be easily incorporated in an incremental parser.",7 Conclusion,[0],[0]
We thank the anonymous reviewers for their helpful feedback.,Acknowledgement,[0],[0]
"We also thank Pranava Swaroop Madhyastha, Hao Tang, Jon Cai, Hao Cheng, and Navdeep Jaitly for their help with initial discussions and code setup.",Acknowledgement,[0],[0]
"This research was partially funded by a Google Faculty Research Award to Mohit Bansal, Karen Livescu, and Kevin Gimpel; and NSF grant no. IIS-1617176.",Acknowledgement,[0],[0]
The opinions expressed in this work are those of the authors and do not necessarily reflect the views of the funding agency.,Acknowledgement,[0],[0]
"A.1 Miscellany
Our main model code is available at https://github.com/shtoshni92/ speech_parsing.",A Appendix,[0],[0]
Most of the data preprocessing code is available at https://github. com/trangham283/seq2seq_parser/ tree/master/src/data_preps.,A Appendix,[0],[0]
"Part of our data preprocessing pipeline also uses https: //github.com/syllog1sm/swbd_tools.
",A Appendix,[0],[0]
Table 8 shows statistics of our Switchboard dataset.,A Appendix,[0],[0]
"As defined, for example, in (Charniak and Johnson, 2001; Honnibal and Johnson, 2014), the splits are: conversations sw2000 to sw3000 for training, sw4500 to sw4936 for validation (dev), and sw4000 to sw4153 for evaluation (test).",A Appendix,[0],[0]
"In addition, previous work has reserved sw4154 to sw4500 for “future use” (dev2), but we added this set to our training set.",A Appendix,[0],[0]
"That is, all of our models are trained on Switchboard conversations sw2000 to sw3000 as well as sw4154 to sw4500.
",A Appendix,[0],[0]
Figure 4 illustrates the data preprocessing step.,A Appendix,[0],[0]
"On the decoder end, we also use a post-processing step that merges the original sentence with the decoder output to obtain the standard constituent tree representation.",A Appendix,[0],[0]
"During inference, in rare cases (and virtually none as our models converge), the decoder does not generate a valid parse sequence, due to the mismatch in brackets and/or the mismatch in the number of pre-terminals and terminals, i.e., num(XX) 6= num(tokens).",A Appendix,[0],[0]
"In such cases, we simply add/remove brackets from either end of the parse, or add/remove pre-terminal symbols XX in the middle of the parse to match the number of input tokens.
",A Appendix,[0],[0]
Figure 5 shows the distribution of pause durations in our training data.,A Appendix,[0],[0]
"Our pause buckets of
0 < p ≤ 0.05 s, 0.05 s < p ≤ 0.2 s, 0.2 < p ≤ 1 s, and p > 1 s described in the main paper were based on this distribution of pause lengths.
",A Appendix,[0],[0]
"Table 9 shows the comprehensive error counts in all error categories defined in the Berkeley Parse Analyzer (Kummerfeld et al., 2012) in both the fluent and disfluent subsets.
",A Appendix,[0],[0]
"A.2 Tree Examples In figures 6, 7, and 8, we follow node correction notations as in (Kummerfeld et al., 2012).",A Appendix,[0],[0]
"In particular, missing nodes are marked in blue on the gold tree, extra nodes are marked red in the predicted tree, and yellow nodes denote crossing.",A Appendix,[0],[0]
"In conversational speech, the acoustic signal provides cues that help listeners disambiguate difficult parses.",abstractText,[0],[0]
"For automatically parsing spoken utterances, we introduce a model that integrates transcribed text and acoustic-prosodic features using a convolutional neural network over energy and pitch trajectories coupled with an attention-based recurrent neural network that accepts text and prosodic features.",abstractText,[0],[0]
"We find that different types of acoustic-prosodic features are individually helpful, and together give statistically significant improvements in parse and disfluency detection F1 scores over a strong text-only baseline.",abstractText,[0],[0]
"For this study with known sentence boundaries, error analyses show that the main benefit of acousticprosodic features is in sentences with disfluencies, attachment decisions are most improved, and transcription errors obscure gains from prosody.",abstractText,[0],[0]
Parsing Speech: A Neural Approach to Integrating Lexical and Acoustic-Prosodic Information,title,[0],[0]
"General treebank analyses are graph structured, but parsers are typically restricted to tree structures for efficiency and modeling reasons. We propose a new representation and algorithm for a class of graph structures that is flexible enough to cover almost all treebank structures, while still admitting efficient learning and inference. In particular, we consider directed, acyclic, one-endpoint-crossing graph structures, which cover most long-distance dislocation, shared argumentation, and similar tree-violating linguistic phenomena. We describe how to convert phrase structure parses, including traces, to our new representation in a reversible manner. Our dynamic program uniquely decomposes structures, is sound and complete, and covers 97.3% of the Penn English Treebank. We also implement a proofof-concept parser that recovers a range of null elements and trace types.",text,[0.9524352043922145],"['The aggregate performance with the GB5 oracle across all datasets can be briefly summarized as follows: RegCB always beats ✏-Greedy and ILTCB, but sometimes loses out to Bootstrap-TS, and Bootstrap-TS itself sometimes underperforms relative to the other baselines, especially on the UCI datasets.']"
"Many syntactic representations use graphs and/or discontinuous structures, such as traces in Government and Binding theory and f-structure in Lexical Functional Grammar (Chomsky 1981; Kaplan and Bresnan 1982).",1 Introduction,[0],[0]
"Sentences in the Penn Treebank (PTB, Marcus et al. 1993) have a core projective tree structure and trace edges that represent control structures, wh-movement and more.",1 Introduction,[0],[0]
"However, most parsers and the standard evaluation metric ignore these edges and all null elements.",1 Introduction,[0.9511369254912709],"['Finally, with these confidence intervals, we either optimistically pick the action with the highest upper bound, similar to UCB and LinUCB, or randomize among all actions that are potentially the best.']"
"By leaving out parts of the structure, they fail to provide key relations to downstream tasks such as question answering.",1 Introduction,[0],[0]
"While there has been work on capturing
some parts of this extra structure, it has generally either been through post-processing on trees (Johnson 2002; Jijkoun 2003; Campbell 2004; Levy and Manning 2004; Gabbard et al. 2006) or has only captured a limited set of phenomena via grammar augmentation (Collins 1997; Dienes and Dubey 2003; Schmid 2006; Cai et al. 2011).
",1 Introduction,[0],[0]
We propose a new general-purpose parsing algorithm that can efficiently search over a wide range of syntactic phenomena.,1 Introduction,[0],[0]
"Our algorithm extends a non-projective tree parsing algorithm (Pitler et al. 2013; Pitler 2014) to graph structures, with improvements to avoid derivational ambiguity while maintaining an O(n4) runtime.",1 Introduction,[0.9525668248190661],"['The third baseline is a bootstrapping-based exploration strategy of Dimakopoulou et al. (2017) (Bootstrap-TS), which uses bootstrapping to estimate confidence intervals and then performs Thompson sampling to select an action based on the intervals.']"
"Our algorithm also includes an optional extension to ensure parses contain a directed projective tree of non-trace edges.
",1 Introduction,[0],[0]
Our algorithm cannot apply directly to constituency parses–it requires lexicalized structures similar to dependency parses.,1 Introduction,[0],[0]
We extend and improve previous work on lexicalized constituent representations (Shen et al. 2007; Carreras et al. 2008; Hayashi and Nagata 2016) to handle traces.,1 Introduction,[0],[0]
"In this form, traces can create problematic structures such as directed cycles, but we show how careful choice of head rules can minimize such issues.
We implement a proof-of-concept parser, scoring 88.1 on trees in section 23 and 70.6 on traces.",1 Introduction,[0],[0]
"Together, our representation and algorithm cover 97.3% of sentences, far above the coverage of projective tree parsers (43.9%).",1 Introduction,[0],[0]
"This work builds on two areas: non-projective tree parsing, and parsing with null elements.
",2 Background,[0],[0]
"Non-projectivity is important in syntax for rep-
441
Transactions of the Association for Computational Linguistics, vol. 5, pp.",2 Background,[0],[0]
"441–454, 2017.",2 Background,[0],[0]
Action Editor: Marco Kuhlmann.,2 Background,[0],[0]
"Submission batch: 4/2017; Published 11/2017.
",2 Background,[0],[0]
c©2017 Association for Computational Linguistics.,2 Background,[0],[0]
"Distributed under a CC-BY 4.0 license.
",2 Background,[0],[0]
"resenting many structures, but inference over the space of all non-projective graphs is intractable.",2 Background,[0],[0]
"Fortunately, in practice almost all parses are covered by well-defined subsets of this space.",2 Background,[0],[0]
"For dependency parsing, recent work has defined algorithms for inference within various subspaces (GómezRodrı́guez and Nivre 2010; Pitler et al. 2013).",2 Background,[0],[0]
We build upon these algorithms and adapt them to constituency parsing.,2 Background,[0],[0]
"For constituency parsing, a range of formalisms have been developed that are mildlycontext sensitive, such as CCG (Steedman 2000), LFG (Kaplan and Bresnan 1982), and LTAG (Joshi and Schabes 1997).
",2 Background,[0],[0]
"Concurrently with this work, Cao et al. (2017) also proposed a graph version of Pitler et al. (2013)’s One-Endpoint Crossing (1-EC) algorithm.",2 Background,[0],[0]
"However, Cao’s algorithm does not consider the direction of edges1 and so it could produce cycles, or graphs with multiple root nodes.",2 Background,[0],[0]
"Their algorithm also has spurious ambiguity, with multiple derivations of the same parse structure permitted.",2 Background,[0],[0]
"One advantage of their algorithm is that by introducing a new item type it can handle some cases of the Locked-Chain we define below (specifically, when N is even), though in practise they also restrict their algorithm to ignore such cases.",2 Background,[0],[0]
"They also show that the class of graphs they generate corresponds to the 1-EC pagenumber-2 space, a property that applies to this work as well2.
",2 Background,[0],[0]
Parsing with Null Elements in the PTB has taken two general approaches.,2 Background,[0],[0]
"The first broadly effective system was Johnson (2002), which post-processed the output of a parser, inserting extra elements.",2 Background,[0],[0]
"This was effective for some types of structure, such as null complementizers, but had difficulty with long distance dependencies.",2 Background,[0],[0]
The other common approach has been to thread a trace through the tree structure on the non-terminal symbols.,2 Background,[0],[0]
"Collins (1997)’s third model used this approach to recover wh-traces, while Cai et al. (2011) used it to recover null pronouns, and others have used it for a range of movement types (Dienes and Dubey 2003; Schmid 2006).",2 Background,[0],[0]
"These approaches have the disadvantage that each
1 To produce directed edges, their parser treats the direction as part of the edge label.
",2 Background,[0],[0]
2,2 Background,[0],[0]
This is a topological space with two half-planes sharing a boundary.,2 Background,[0],[0]
"All edges are drawn on one of the two half-planes and each half-plane contains no crossings.
",2 Background,[0],[0]
additional trace dramatically expands the grammar.,2 Background,[0],[0]
Our representation is similar to LTAG-Spinal (Shen et al. 2007) but has the advantage that it can be converted back into the PTB representation.,2 Background,[0],[0]
Hayashi and Nagata (2016) also incorporated null elements into a spinal structure but did not include a representation of co-indexation.,2 Background,[0],[0]
"In related work, dependency parsers have been used to assist in constituency parsing, with varying degrees of representation design, but only for trees (Hall, Nivre, and Nilsson 2007; Hall and Nivre 2008; FernándezGonzález and Martins 2015; Kong et al. 2015).
",2 Background,[0],[0]
"Kato and Matsubara (2016) described a new approach, modifying a transition-based parser to recover null elements and traces, with strong results, but using heuristics to determine trace referents.",2 Background,[0],[0]
"Our algorithm is a dynamic program, similar at a high level to CKY (Kasami 1966;",3 Algorithm,[0],[0]
Younger 1967; Cocke 1969).,3 Algorithm,[0],[0]
The states of our dynamic program (items) represent partial parses.,3 Algorithm,[0],[0]
"Usually in CKY, items are defined as covering the n words in a sentence, starting and ending at the spaces between words.",3 Algorithm,[0],[0]
"We follow Eisner (1996), defining items as covering the n−1 spaces in a sentence, starting and ending on words, as shown in Figure 1.",3 Algorithm,[0],[0]
"This means that we process each word’s left and right dependents separately, then combine the two halves.
",3 Algorithm,[0],[0]
"We use three types of items: (1) a single edge, linking two words, (2) a continuous span, going from one word to another, representing all edges linking pairs of words within the span, (3) a span (as defined in 2) plus an additional word outside the span, enabling the inclusion of edges between that word and words in the span.
",3 Algorithm,[0],[0]
"Within the CKY framework, the key to defining our algorithm is a set of rules that specify which items are allowed to combine.",3 Algorithm,[0],[0]
"From a bottom-up perspective, a parse is built in a series of steps, which come in three types: (1) adding an edge to an item, (2) combining two items that have non-overlapping adjacent spans to produce a new item with a larger span, (3) combining three items, similarly to (2).
",3 Algorithm,[0],[0]
"Example: To build intuition for the algorithm, we will describe the derivation in Figure 1.",3 Algorithm,[0],[0]
"Note, item sub-types (I, X, and N) are defined below, and in-
cluded here for completeness.",3 Algorithm,[0],[0]
"(1) We initialize with spans of width one, going between adjacent words, e.g. between ROOT and We. ∅",3 Algorithm,[0],[0]
"7→ I0,1 (2) Edges can be introduced in exactly two ways, either by linking the two ends of a span, e.g. like– running, or by linking one end of a span with a word outside the span, e.g. like–. (which in this case forms a new item that has a span and an external word).
",3 Algorithm,[0],[0]
"I2,3 ∧ like–running 7→ I2,3 I3,4 ∧ like–. 7→ X3,4,2
(3) We add a second edge to one of the items.",3 Algorithm,[0],[0]
"I1,2 ∧ running–We 7→ X1,2,3 (4) Now that all the edges to We have been added, the two items either side of it are combined to form an item that covers it.",3 Algorithm,[0],[0]
"I0,1 ∧ X1,2,3 7→",3 Algorithm,[0],[0]
"N0,2,3 (5) We add an edge, creating a crossing because We is an argument of a word to the right of like.",3 Algorithm,[0],[0]
"N0,2,3 ∧ ROOT–like 7→ N0,2,3 (7) We use a ternary rule to combine three adjacent items.",3 Algorithm,[0],[0]
"In the process we create another crossing.
",3 Algorithm,[0],[0]
"N0,2,3 ∧ I2,3 ∧ X3,4,2 7→ I0,6",3 Algorithm,[0],[0]
"Notation Vertices are p, q, etc.",3.1 Algorithm definition,[0],[0]
"Continuous ranges are [pq], [pq), (pq], or (pq), where the brackets indicate inclusion, [ ], or exclusion, ( ), of each endpoint.",3.1 Algorithm definition,[0],[0]
A span [pq] and vertex o that are part of the same item are [pq.o].,3.1 Algorithm definition,[0],[0]
"Two vertices and an arrow indicate an edge, ~pq.",3.1 Algorithm definition,[0],[0]
"Two vertices without an arrow are an edge in either direction, pq. Ranges and/or vertices connected by a dash define a set of edges, e.g. the
set of edges between o and (pq) is o–(pq) (in some places we will also use this to refer to an edge from the set, rather than the whole set).",3.1 Algorithm definition,[0],[0]
"If there is a path from p to q, q is reachable from p.
Item Types As shown in Figure 1, our items start and end on words, fully covering the spaces in between.",3.1 Algorithm definition,[0],[0]
"Earlier we described three item types: an edge, a span, and a span plus an external vertex.",3.1 Algorithm definition,[0],[0]
"Here we define spans more precisely as I , and divide the span plus an external point case into five types differing in the type of edge crossing they contain: p qI , Interval A span for which there are no edges sr :",3.1 Algorithm definition,[0],[0]
r ∈ (pq) and s /∈,3.1 Algorithm definition,[0],[0]
"[pq].
o X , Exterval An interval and either op or oq, where",3.1 Algorithm definition,[0],[0]
o /∈,3.1 Algorithm definition,[0],[0]
[pq].,3.1 Algorithm definition,[0],[0]
"B, Both A span and vertex [pq.o], for which there are no edges sr : r ∈ (pq) and s /∈",3.1 Algorithm definition,[0],[0]
"[pq] ∪ o. Edges o–[pq] may be crossed by pq, p–(pq) or q–(pq), and at least one crossing of the second and third types occurs.",3.1 Algorithm definition,[0],[0]
Edges o–(pq) may not be crossed by (pq)–(pq) edges.,3.1 Algorithm definition,[0],[0]
"L, Left Same as B, but o–(pq) edges may only cross p–(pq] edges.",3.1 Algorithm definition,[0],[0]
"R, Right Symmetric with L. N , Neither An interval and a vertex [pq.o], with at least one o–(pq) edge, which can be crossed by pq, but no other [pq]–[pq] edges.
",3.1 Algorithm definition,[0],[0]
Items are further specified as described in Alg.,3.1 Algorithm definition,[0],[0]
1.,3.1 Algorithm definition,[0],[0]
"Most importantly, for each pair of o, p, and q in an item, the rules specify whether one is a parent of the other, and if they are directly linked by an edge.
",3.1 Algorithm definition,[0],[0]
"For an item H with span [ij], define covered(H) as (ij), and define visible(H) as {i, j}.",3.1 Algorithm definition,[0],[0]
"When an external vertex x is present, it is in visible(H).",3.1 Algorithm definition,[0],[0]
"Call the union of multiple such sets covered(F,G,H), and visible(F,G,H).
",3.1 Algorithm definition,[0],[0]
Deduction Rules To make the deduction rules manageable,3.1 Algorithm definition,[0],[0]
", we use templates to define some constraints explicitly, and then use code to generate the rules.",3.1 Algorithm definition,[0],[0]
"During rule generation, we automatically apply additional constraints to prevent rules that would leave a word in the middle of a span without a parent or that would form a cycle (proven possible below).",3.1 Algorithm definition,[0],[0]
Algorithm 1 presents the explicit constraints.,3.1 Algorithm definition,[0],[0]
"Once expanded, these give rules that specify all properties for each item (general type, external vertex position
Algorithm 1 Dynamic program for Lock-Free, One-Endpoint Crossing, Directed, Acyclic graph parsing.",3.1 Algorithm definition,[0],[0]
Adding Edges: Consider a span [lr] and vertex x /∈,3.1 Algorithm definition,[0],[0]
[lr].,3.1 Algorithm definition,[0],[0]
"Edges between l and r can be added to items I , N , L, R, and B (making L̂ and N̂ in those cases).",3.1 Algorithm definition,[0],[0]
"Edges between l and x can be added to items I (forming an X), R, and N .",3.1 Algorithm definition,[0],[0]
"Edges between r and x can be added to items I (forming an X), L, and N .",3.1 Algorithm definition,[0],[0]
"The l–r edge cannot be added after another edge, and N items cannot get both l–x and r–x edges.",3.1 Algorithm definition,[0],[0]
Combining Items: In the rules below the following notation is used: For this explanation items are T,3.1 Algorithm definition,[0],[0]
[lr crl clr] and T,3.1 Algorithm definition,[0],[0]
[lrx crl cxl clr cxr clx crx].,3.1 Algorithm definition,[0],[0]
T is the type of item.,3.1 Algorithm definition,[0],[0]
Multiple letters indicate any of those types are allowed.,3.1 Algorithm definition,[0],[0]
"For the next three types of notation, if an item does not have a mark, either option is valid.",3.1 Algorithm definition,[0],[0]
˙ T and T : indicate the number of edges between the external vertex and the span: one or more than one respectively.,3.1 Algorithm definition,[0],[0]
·T and T · indicate the position of the external vertex relative to the item’s span (left or right respectively).,3.1 Algorithm definition,[0],[0]
T̂ indicates for N and L that ∀p ∈,3.1 Algorithm definition,[0],[0]
(ij)∃rs : i≤r<p<s≤j.,3.1 Algorithm definition,[0],[0]
"In (11) and (12) it is optional, but true for output iff true for input.",3.1 Algorithm definition,[0],[0]
"l, r, and x: the position of the left end of the span, the right end, and the external vertex, respectively.",3.1 Algorithm definition,[0],[0]
"crl, cxl, etc: connectivity of each pair of visible vertices, from the first subscript to the second.",3.1 Algorithm definition,[0],[0]
"Using crl as an example, these can be .",3.1 Algorithm definition,[0],[0]
"(unconstrained), d ( ~rl must exist), p (l is reachable from r, but ~rl does not exist), n",3.1 Algorithm definition,[0],[0]
"(l is not reachable from r), d (= p ∨ n), n",3.1 Algorithm definition,[0],[0]
(= d ∨ p).,3.1 Algorithm definition,[0],[0]
Note:,3.1 Algorithm definition,[0],[0]
"In the generated rules every value is d, p, or n, leading to multiple rules per template below.
I[ij nd]← max   (Init) j = i+1 (1) I[i i+1 nn] I[i+1 j nn] maxk∈(i,j)   (2) I[ik nd] I[kj",3.1 Algorithm definition,[0],[0]
..],3.1 Algorithm definition,[0],[0]
(3) BLRN · [ikj nndddd] I[kj ..],3.1 Algorithm definition,[0],[0]
"maxl∈(k,j){ (4) RN ·",3.1 Algorithm definition,[0],[0]
[ikl nndddd] I[kl ..] ·LNX[ljk .d..d.] (5) BLRN · [ikl nndddd] I[kl ..] I[lj ..],3.1 Algorithm definition,[0],[0]
"maxl∈(i,k){ (6) I[il n.] ·LN [lki .d.dnn]",3.1 Algorithm definition,[0],[0]
"·N : [kjl ddd.d.]
(7) RNX· [ilk nn.ddd] I[lk ..]",3.1 Algorithm definition,[0],[0]
"·LN :: [kjl .d..d.]
B· [ijx nndddd]← maxk∈(i,j)  
(8) L̂N̂ ·",3.1 Algorithm definition,[0],[0]
[ikx nn.ddd] R·,3.1 Algorithm definition,[0],[0]
[kjx ...d.d] (9) L̂N̂ ·,3.1 Algorithm definition,[0],[0]
[ikx nn.ddd] N ·,3.1 Algorithm definition,[0],[0]
[kjx d.dd.d] (10) L̂N̂ ·,3.1 Algorithm definition,[0],[0]
[ikx nn.ddd] N ·,3.1 Algorithm definition,[0],[0]
"[kjx d.dd.d]
˙ L̂[ijx dddddd]←",3.1 Algorithm definition,[0],[0]
"maxk∈(i,j){
(11) X[ikx .d.dnn] · L̂N̂",3.1 Algorithm definition,[0],[0]
[kji .d.ddd] (12) X[ikx .d.ddd,3.1 Algorithm definition,[0],[0]
],3.1 Algorithm definition,[0],[0]
· L̂N̂,3.1 Algorithm definition,[0],[0]
"[kji .d.ddd]
L :",3.1 Algorithm definition,[0],[0]
"[ijx dddddd]← maxk∈(i,j)   (13) LN",3.1 Algorithm definition,[0],[0]
[ikx .d.ddd],3.1 Algorithm definition,[0],[0]
·N,3.1 Algorithm definition,[0],[0]
[kji dddddd] (14) LN,3.1 Algorithm definition,[0],[0]
[ikx .d.ddd],3.1 Algorithm definition,[0],[0]
·N,3.1 Algorithm definition,[0],[0]
[kji dddddd] (15) L[ikx .d.ddd] I[kj ..],3.1 Algorithm definition,[0],[0]
(16) L[ikx .d.ddd] I[kj ..],3.1 Algorithm definition,[0],[0]
(17) N,3.1 Algorithm definition,[0],[0]
[ikx dddddd] I[kj ..],3.1 Algorithm definition,[0],[0]
(18) N,3.1 Algorithm definition,[0],[0]
[ikx dddddd] I[kj ..],3.1 Algorithm definition,[0],[0]
(19) N,3.1 Algorithm definition,[0],[0]
[ikx dddddd] I[kj ..,3.1 Algorithm definition,[0],[0]
"]
(20) N",3.1 Algorithm definition,[0],[0]
"[ikx dddddd] I[kj ..]
N :",3.1 Algorithm definition,[0],[0]
"[ijx dddddd]← maxk∈(i,j)   (21) ·N [ikx dddddd] I[kj ..]",3.1 Algorithm definition,[0],[0]
(22) ·N [ikx dddddd] I[kj ..] (23) I[ik ..] N ·,3.1 Algorithm definition,[0],[0]
[kjx dddddd] (24) I[ik ..] N ·,3.1 Algorithm definition,[0],[0]
"[kjx dddddd]
˙ N",3.1 Algorithm definition,[0],[0]
"[ijx dddddd]← maxk∈(i,j)   (25) ·X[ikx .d.ddd] I[kj ..] (26) ·X[ikx .d.ddd] I[kj ..] (27) I[ik ..]",3.1 Algorithm definition,[0],[0]
X· [kjx .d.ddd] (28) I[ik ..],3.1 Algorithm definition,[0],[0]
"X· [kjx .d.ddd]
I[ij pn], ·B[ijx ddnndd], R : [ijx dddddd], and ˙ R[ijx dddddd] are symmetric with cases above.
",3.1 Algorithm definition,[0],[0]
"relative to the item spans, connectivity of every pair of vertices in each item, etc).
",3.1 Algorithm definition,[0],[0]
The final item for n vertices is an interval where the left end has a parent.,3.1 Algorithm definition,[0],[0]
For parsing we assume there is a special root word at the end of the sentence.,3.1 Algorithm definition,[0],[0]
Definition 1.,3.2 Properties,[0],[0]
"A graph is One-Endpoint Crossing if, when drawn with vertices along the edge of a halfplane and edges drawn in the open half-plane above, for any edge e, all edges that cross e share a vertex.",3.2 Properties,[0],[0]
"Let that vertex be Pt(e).
",3.2 Properties,[0],[0]
"Aside from applying to graphs, this is the same as
Pitler et al. (2013)’s 1-EC tree definition.
",3.2 Properties,[0],[0]
Definition 2.,3.2 Properties,[0],[0]
"A Locked-Chain (shown in Fig. 2) is formed by a set of consecutive vertices in order from 0 to N , where N > 3, with edges {(0, N−1), (1, N)} ∪ {(i, i+2)∀i ∈",3.2 Properties,[0],[0]
"[0, N−2]}.",3.2 Properties,[0],[0]
Definition 3.,3.2 Properties,[0],[0]
"A graph is Lock-Free if it does not contain edges that form a Locked-Chain.
",3.2 Properties,[0],[0]
"Note that in practice, most parse structures satisfy 1-EC, and the Locked-Chain structure does not occur in the PTB when using our head rules.
",3.2 Properties,[0],[0]
Theorem 1.,3.2 Properties,[0],[0]
"For the space of Lock-Free OneEndpoint Crossing graphs, the algorithm is sound, complete and gives unique decompositions.
",3.2 Properties,[0],[0]
Our proof is very similar in style and structure to Pitler et al. (2013).,3.2 Properties,[0],[0]
"The general approach is to consider the set of structures an item could represent, and divide them into cases based on properties of the internal structure.",3.2 Properties,[0],[0]
"We then show how each case can be decomposed into items, taking care to ensure all the properties that defined the case are satisfied.",3.2 Properties,[0],[0]
Uniqueness follows from having no ambiguity in how a given structure could be decomposed.,3.2 Properties,[0],[0]
"Completeness and soundness follow from the fact that our rules apply equally well in either direction, and so our top-down decomposition implies a bottom-up formation.",3.2 Properties,[0],[0]
"To give intuition for the proof, we show the derivation of one rule below.",3.2 Properties,[0],[0]
The complete proof can be found in Kummerfeld (2016).,3.2 Properties,[0],[0]
"We do not include it here due to lack of space.
",3.2 Properties,[0],[0]
"We do provide the complete set of rule templates in Algorithm 1, and in the proof of Lemma 2 we show that the case in which an item cannot be decomposed occurs if and only if the graph contains a Locked-Chain.",3.2 Properties,[0],[0]
"To empirically check our rule generation code, we checked that our parser uniquely decomposes all 1-EC parses in the PTB and is unable to decompose the rest.
",3.2 Properties,[0],[0]
"Note that by using subsets of our rules, we can restrict the space of structures we generate, giving parsing algorithms for projective DAGs, projective trees (Eisner 1996), or 1-EC trees (Pitler et al. 2013).",3.2 Properties,[0],[0]
"Versions of these spaces with undirected edges could also be easily handled with the same approach.
",3.2 Properties,[0],[0]
p qs t Derivation of rule (4) in Algorithm 1:,3.2 Properties,[0],[0]
"This rule applies to intervals with the substructure shown, and with no parent in this item for p.",3.2 Properties,[0],[0]
They have at least one p–(pq) edge (otherwise rule 1 applies).,3.2 Properties,[0],[0]
"The longest p–(pq) edge, ps, is crossed (otherwise rule 2 applies).",3.2 Properties,[0],[0]
Let C be the set of (ps)–(sq) edges (note: these cross ps).,3.2 Properties,[0],[0]
"Either all of the edges in C have a common endpoint t ∈ (sq), or if |C| = 1 let t be the endpoint in (sq) (otherwise rule 6 or 7 applies).",3.2 Properties,[0],[0]
Let D be the set of s–(tq) edges.,3.2 Properties,[0],[0]
|D| > 0,3.2 Properties,[0],[0]
"(otherwise rule 3 or 5 applies).
",3.2 Properties,[0],[0]
We will break this into three items.,3.2 Properties,[0],[0]
"First, (st)–(tq] edges would violate the 1-EC property and (st)–[ps) edges do not exist by construction.",3.2 Properties,[0],[0]
"Therefore, the middle item is an Interval [st], the left item is [ps.t], and the right item is [tq.s] (since |C| > 0 and |D| > 0).",3.2 Properties,[0],[0]
"The left item can be either
an N or R, but not an L or B because that would violate the 1-EC property for the C edges.",3.2 Properties,[0],[0]
"The right item can be an X , L, or N , but not an R or B because that would violate the 1-EC property for the D edges.",3.2 Properties,[0],[0]
"We will require edge ps to be present in the first item, and not allow pt.",3.2 Properties,[0],[0]
"To avoid a spurious ambiguity, we also prevent the first or third items from having st (which could otherwise occur in any of the three items).",3.2 Properties,[0],[0]
"Now we have broken down the original item into valid sub-items, and we have ensured that those sub-items contain all of the structure used to define the case in a unique way.
",3.2 Properties,[0],[0]
"Now we will further characterize the nature of the Lock-Free restriction to the space of graphs.
",3.2 Properties,[0],[0]
Lemma 1.,3.2 Properties,[0],[0]
No edge in a Locked-Chain in a 1-EC graph is crossed by edges that are not part of it.,3.2 Properties,[0],[0]
Proof.,3.2 Properties,[0],[0]
"First, note that: Pt((0, N−1))",3.2 Properties,[0],[0]
"= N , Pt((1, N))",3.2 Properties,[0],[0]
"= 0, and {Pt((i, i+2))",3.2 Properties,[0],[0]
= i+1,3.2 Properties,[0],[0]
∀i ∈,3.2 Properties,[0],[0]
"[0, N−2]} Call the set {(i, i+2)∀i ∈",3.2 Properties,[0],[0]
"[0, N−2]}, the chain.
Consider an edge e that crosses an edge f in a Locked-Chain.",3.2 Properties,[0],[0]
"Let ein be the end of e that is between the two ends of f , and eout be the other end.",3.2 Properties,[0],[0]
"One of e’s endpoints is at Pt(f), and Pt(e) is an endpoint of f .",3.2 Properties,[0],[0]
"There are three cases:
(i) f",3.2 Properties,[0],[0]
"= (1, N).",3.2 Properties,[0],[0]
"Here, eout = Pt(f) = 0, and ein ∈ (1, N).",3.2 Properties,[0],[0]
"For all vertices v ∈ (1, N) there is an edge g in the chain such that v is between the endpoints of g. Therefore, e will cross such an edge g. To satisfy the 1-EC property, g must share an endpoint with f , which means g is either (1, 3) or (N−2, N).",3.2 Properties,[0],[0]
"In the first case, the 1-EC property forces e = (0, 2), and in the second e = (0, N−1), both of which are part of the Locked-Chain.
(ii) f = (0, N−1), symmetrical with (i).",3.2 Properties,[0],[0]
"(iii) f = (i, i+2), for some i ∈",3.2 Properties,[0],[0]
"[0, N−2].",3.2 Properties,[0],[0]
"Here, ein = Pt(f) = i+1.",3.2 Properties,[0],[0]
"We can assume e does not cross (0, N−1) or (1, N), as those cases are covered by (i).",3.2 Properties,[0],[0]
"As in (i), e must cross another edge in the chain, and that edge must share an endpoint with f .
",3.2 Properties,[0],[0]
"This forces e to be either (i−1, i+1) or (i+1, i+3) (excluding one or both if they cross (0, N−1) or (1, N)), which are both in the Locked-Chain.
",3.2 Properties,[0],[0]
Our rules define a unique way to decompose almost any item into a set of other items.,3.2 Properties,[0],[0]
"The exception is B, which in some cases can not be divided into two items (i.e. has no valid binary division).
",3.2 Properties,[0],[0]
Lemma 2.,3.2 Properties,[0],[0]
A B[ij.x] has no valid binary division if and only if the graph has a Locked-Chain.,3.2 Properties,[0],[0]
Proof.,3.2 Properties,[0],[0]
Consider the k and l that give the longest ik and lj edges in a B with no valid binary division (at least one edge of each type must exist by definition).,3.2 Properties,[0],[0]
"No vertex in (ik) or (jl) is a valid split point, as they would all require one of the items to have two external vertices.
",3.2 Properties,[0],[0]
"Now, consider p ∈",3.2 Properties,[0],[0]
[kj].,3.2 Properties,[0],[0]
"If there is no edge l1r1, where i ≤ l1 < p < r1 ≤ j, then p would be a valid split point.",3.2 Properties,[0],[0]
"Therefore, such an edge must exist.",3.2 Properties,[0],[0]
"Consider l1, either l1 ∈ (ik) or there is an edge l2c, where i ≤ l2",3.2 Properties,[0],[0]
< l1 < c ≤ j (by the same logic as for l1r1).,3.2 Properties,[0],[0]
"Similarly, either r1 ∈ (jl) or there is an edge cr2 (it must be c to satisfy 1-EC).",3.2 Properties,[0],[0]
"We can also apply this logic to edges l2c and cr2, giving edges l3l1 and r1r3.",3.2 Properties,[0],[0]
This pattern will terminate when it reaches lu ∈ (ik) and rv ∈ (jl) with edges lulu−2 and rv−2rv.,3.2 Properties,[0],[0]
"Note that k = lu−1 and l = rv−1, to satisfy 1-EC.
",3.2 Properties,[0],[0]
"Since it is a B, there must be at least two x–(ij) edges.",3.2 Properties,[0],[0]
"To satisfy 1-EC, these end at lu−1 and rv−1.
",3.2 Properties,[0],[0]
"Let x be to the right (the left is symmetrical), and call i = 0, j = N−1, and x = N .",3.2 Properties,[0],[0]
"Comparing with the Locked-Chain definition, we have all the edges except one: 0 to N−1.",3.2 Properties,[0],[0]
"However, that edge must be present in the overall graph, as all B items start with an ij edge (see rules 3 and 5 in Algorithm 1).",3.2 Properties,[0],[0]
"Therefore, if there is no valid split point for a B, the overall graph must contain a Locked-Chain.
",3.2 Properties,[0],[0]
"Now, for a graph that contains a Locked-Chain, consider the items that contain the Locked-Chain.",3.2 Properties,[0],[0]
"Grouping them by their span [ij], there are five valid options:",3.2 Properties,[0],[0]
"[0, N−1], [1, N ], [0, N ], (i ≤ 0",3.2 Properties,[0],[0]
"∧ j > N ), and (i < 0",3.2 Properties,[0],[0]
∧ j ≥ N ).,3.2 Properties,[0],[0]
"Items of the last three types would be divided by our rules into smaller items, one of which contains the whole Locked-Chain.",3.2 Properties,[0],[0]
"The first two are Bs of the type discussed above.
",3.2 Properties,[0],[0]
"Now we will prove that our code to generate rules from the templates can guarantee a DAG is formed.
",3.2 Properties,[0],[0]
Lemma 3.,3.2 Properties,[0],[0]
"For any item H , ∀v ∈ covered(H) ∃u ∈ visible(H) : v is reachable from u. Proof.",3.2 Properties,[0],[0]
"This is true for initial items because covered(H) = ∅. To apply induction, consider adding edges and combing items.",3.2 Properties,[0],[0]
The lemma clearly remains true when adding an edge.,3.2 Properties,[0],[0]
"Consider combining items E, F , G to form H[ij.x], and assume the lemma is true for E, F , and G (the binary case is similar).",3.2 Properties,[0],[0]
"Since all vertices are reachable from visible(E,F,G), we only need to ensure that ∀v ∈",3.2 Properties,[0],[0]
"visible(E,F,G) ∃u ∈",3.2 Properties,[0],[0]
visible(H) : v is reachable from u.,3.2 Properties,[0],[0]
"The connectivity between all pairs {(u, v) | u ∈ visible(H), v ∈",3.2 Properties,[0],[0]
"visible(E,F,G)} can be inferred from the item definitions, and so this requirement can be enforced in rule generation.
",3.2 Properties,[0],[0]
Lemma 4.,3.2 Properties,[0],[0]
The final item is a directed acyclic graph.,3.2 Properties,[0],[0]
Proof.,3.2 Properties,[0],[0]
"First, consider acyclicity.",3.2 Properties,[0],[0]
Initial items do not contain any edges and so cannot contain a cycle.,3.2 Properties,[0],[0]
"For induction, there are two cases:
(i) Adding an Edge ~pq to an item H: Assume that H does not contain any cycles.",3.2 Properties,[0],[0]
"~pq will create a cycle if and only if p is reachable from q. By construction, p and q ∈ visible(H), and so the item definition contains whether p is reachable from q.
(ii)",3.2 Properties,[0],[0]
"Combining Items: Assume that in isolation, none of the items being combined contain cycles.",3.2 Properties,[0],[0]
"Therefore, a cycle in the combined item must be composed of paths in multiple items.",3.2 Properties,[0],[0]
A path in one item can only continue in another item by passing through a visible vertex.,3.2 Properties,[0],[0]
"Therefore, a cycle would have to be formed by a set of paths between visible vertices.",3.2 Properties,[0],[0]
"But the connectivity of every pair of visible vertices is specified in the item definitions.
",3.2 Properties,[0],[0]
"In both cases, rules that create a cycle can be excluded during rule generation.
",3.2 Properties,[0],[0]
"By induction, the items constructed by our algorithm do not contain cycles.",3.2 Properties,[0],[0]
"Together with Lemma 3 and the final item definition, this means the final structure is an acyclic graph with all vertices reachable from vertex n.
Next, we will show two properties that give intuition for the algorithm.",3.2 Properties,[0],[0]
"Specifically, we will prove which rules add edges that are crossed in the final derivation.
",3.2 Properties,[0],[0]
Lemma 5.,3.2 Properties,[0],[0]
An edge ij added to I[ij] is not crossed.,3.2 Properties,[0],[0]
Proof.,3.2 Properties,[0],[0]
"First, we will show three properties of any pair of items in a derivation (using [ij.x] and [kl.y]).
",3.2 Properties,[0],[0]
(1) It is impossible for either i <,3.2 Properties,[0],[0]
k,3.2 Properties,[0],[0]
< j < l or k,3.2 Properties,[0],[0]
< i,3.2 Properties,[0],[0]
<,3.2 Properties,[0],[0]
"l < j, i.e., items cannot have partially overlapping spans.",3.2 Properties,[0],[0]
"As a base case, the final item is an interval spanning all vertices, and so no other item can partially overlap with it.",3.2 Properties,[0],[0]
"Now assume it is true for an item H and consider the rules in reverse, breaking H up.",3.2 Properties,[0],[0]
"By construction, each rule divides H into items with spans that are adjacent, overlapping only at their visible vertices.",3.2 Properties,[0],[0]
"Since the new items are nested within H , they do not overlap with any items H did not overlap with.",3.2 Properties,[0],[0]
"By induction, no pair of items have partially overlapping spans.
",3.2 Properties,[0],[0]
(2) For items with nested spans (i ≤ k,3.2 Properties,[0],[0]
"< l ≤ j), y ∈",3.2 Properties,[0],[0]
[ij]∪{x}.,3.2 Properties,[0],[0]
"Following the argument for the previous case, the [ij.x] item must be decomposed into a set of items that includes [kl.y].",3.2 Properties,[0],[0]
"Now, consider how those items are combined.",3.2 Properties,[0],[0]
"The rules that start with an item with an external vertex produce an item that either has the same external vertex, or with the external vertex inside the span of the new item.",3.2 Properties,[0],[0]
"Therefore, y must either be equal to x or inside [ij].
(3) For items without nested spans, x /∈",3.2 Properties,[0],[0]
(kl).,3.2 Properties,[0],[0]
Assume x ∈,3.2 Properties,[0],[0]
(kl) for two items without nested spans.,3.2 Properties,[0],[0]
"None of the rules combine such a pair of items, or allow one to be extended so that the other is nested within it.",3.2 Properties,[0],[0]
"However, all items are eventually combined to complete the derivation.",3.2 Properties,[0],[0]
"By contradiction, x /∈ (kl).
",3.2 Properties,[0],[0]
"Together, these mean that given an interval H with span [ij], and another item G, either ∀v ∈ visible(G), v ∈",3.2 Properties,[0],[0]
"[ij] or ∀v ∈ visible(G),",3.2 Properties,[0],[0]
v /∈,3.2 Properties,[0],[0]
(ij).,3.2 Properties,[0],[0]
"Since edges are only created between visible vertices, no edge can cross edge ij.
Lemma 6.",3.2 Properties,[0],[0]
"All edges aside from those considered in Lemma 5 are crossed.
",3.2 Properties,[0],[0]
Proof.,3.2 Properties,[0],[0]
"First, consider an edge ij added to an item [ij.x] of type B, L, R, or N. This edge is crossed by all x–(ij) edges, and in these items |x–(ij)| ≥ 1 by definition.",3.2 Properties,[0],[0]
"Note, by the same argument as Lemma 5, the edge is not crossed later in the derivation.
",3.2 Properties,[0],[0]
"Second, consider adding e ∈ {xi, xj}, to H , an item with [ij] or [ij.x], forming an item G[ij.x].",3.2 Properties,[0],[0]
"Note, e does not cross any edges in H .",3.2 Properties,[0],[0]
Let E(F,3.2 Properties,[0],[0]
[kl.y]) be the set of y–[kl] edges in some item F .,3.2 Properties,[0],[0]
Note that e ∈ E(G).,3.2 Properties,[0],[0]
"We will show how this set of edges is affected by the rules and what that implies for e. Consider each input item A[kl.y] in each
rule, with output item C. Every item A falls into one of four categories: (1) ∀f ∈ E(A), f is crossed by an edge in another of the rule’s input items, (2) E(A) ⊆ E(C), (3) A∧ kl 7→ C",3.2 Properties,[0],[0]
"and there are no ky or ly edges in A, (4)",3.2 Properties,[0],[0]
"A contains edge kl and there are no ky or ly edges in A.
Cases 2-4 are straightforward to identify.",3.2 Properties,[0],[0]
"For an example of the first case, consider the rightmost item in rule 4.",3.2 Properties,[0],[0]
"The relevant edges are k–(lj] (by construction, kl is not present).",3.2 Properties,[0],[0]
"Since the leftmost item is either an R or N, |l–(ik)| ≥ 1.",3.2 Properties,[0],[0]
Since i < k,3.2 Properties,[0],[0]
"< l < j, all k–(lj] edges will cross all l–[ik) edges.",3.2 Properties,[0],[0]
"Therefore applying this rule will cross all k–(lj] edges in the rightmost item.
",3.2 Properties,[0],[0]
"Initially, e is not crossed and e ∈ E(G).",3.2 Properties,[0],[0]
"For each rule application, edges in E(A) are either crossed (1 and 3), remain in the set E(C)",3.2 Properties,[0],[0]
"(2), or must already be crossed (4).",3.2 Properties,[0],[0]
Since the final item is an interval and E(Interval) =,3.2 Properties,[0],[0]
"∅, there must be a subsequent rule that is not in case 2.",3.2 Properties,[0],[0]
Therefore e will be crossed.,3.2 Properties,[0],[0]
"Our algorithm is based on Pitler et al. (2013), which had the crucial idea of One-Endpoint crossing and a complete decomposition of the tree case.",3.3 Comparison with Pitler et al. (2013),[0],[0]
"Our changes and extensions provide several benefits:
Extension to graphs: By extending to support multiple parents while preventing cycles, we substantially expand the space of generatable structures.
",3.3 Comparison with Pitler et al. (2013),[0],[0]
Uniqueness:,3.3 Comparison with Pitler et al. (2013),[0],[0]
By avoiding derivational ambiguity we reduce the search space and enable efficient summing as well as maxing.,3.3 Comparison with Pitler et al. (2013),[0],[0]
Most of the cases in which ambiguity arises in Pitler et al. (2013)’s algorithm are due to symmetry that is not explicitly broken.,3.3 Comparison with Pitler et al. (2013),[0],[0]
"For example, the rule we worked through in the previous section defined t ∈ (sq)",3.3 Comparison with Pitler et al. (2013),[0],[0]
when |C| = 1.,3.3 Comparison with Pitler et al. (2013),[0],[0]
"Picking t ∈ (ps) would also lead to a valid set of rules, but allowing either creates a spurious ambiguity.",3.3 Comparison with Pitler et al. (2013),[0],[0]
"This ambiguity is resolved by tracking whether there is only one edge to the external vertex or more than one, and requiring more than one in rules 6 and 7.",3.3 Comparison with Pitler et al. (2013),[0],[0]
"Other changes include ensuring equivalent structures cannot be represented by multiple item types and enforcing a unique split point in B items.
",3.3 Comparison with Pitler et al. (2013),[0],[0]
"More concise algorithm definition: By separating edge creation from item merging, and defining our rules via a combination of templates and code, we are able to define our algorithm more concisely.",3.3 Comparison with Pitler et al. (2013),[0],[0]
Edge labels can be added by calculating either the sum or max over edge types when adding each edge.,3.4.1 Edge Labels and Word Labels,[0],[0]
"Word labels (e.g., POS Tags) must be added to the state, specifying a label for each visible word (p, q and o).",3.4.1 Edge Labels and Word Labels,[0],[0]
This state expansion is necessary to ensure agreement when combining items.,3.4.1 Edge Labels and Word Labels,[0],[0]
"Our algorithm constrains the space of graph structures, but we also want to ensure that our parse contains a projective tree of non-trace edges.
",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"To ensure every word gets one and only one structural parent, we add booleans to the state, indicating whether p, q and o have structural parents.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"When adding edges, a structural edge cannot be added if a word already has a structural parent.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"When combining items, no word can receive more than one structural parent, and words that will end up in the middle of the span must have exactly one.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"Together, these constraints ensure we have a tree.
",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"To ensure the tree is projective, we need to prevent structural edges from crossing.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"Crossing edges are introduced in two ways, and in both we can avoid structural edges crossing by tracking whether there are structural o–[pq] edges.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"Such edges are present if a rule adds a structural op or oq edge, or if a rule combines an item with structural o–[pq] edges and o will still be external in the item formed by the rule.
",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"For adding edges, every time we add a pq edge in the N , L, R and B items we create a crossing with all o–(pq) edges.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"We do not create a crossing with oq or op, but our ordering of edge creation means these are not present when we add a pq edge, so tracking structural o–[pq] edges gives us the information we need to prevent two structural edges crossing.
",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"For combining items, in Lemma 6 we showed that during combinations, o–[pq] edges in each pair of items will cross.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"As a result, knowing whether any o–[pq] edge is structural is sufficient to determine whether two structural edges will cross.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"Consider a sentence with n tokens, and let E and S be the number of edge types and word labels in our grammar respectively.
",3.5 Complexity,[0],[0]
"Parses without word or edge labels: Rules have up to four positions, leading to complexity of O(n4).",3.5 Complexity,[0],[0]
"Note, there is also an important constant–once our templates are expanded, there are 49,292 rules.
",3.5 Complexity,[0],[0]
"With edge labels: When using a first-order model, edge labels only impact the rules for edge creation, leading to a complexity of O(n4 + En2).
",3.5 Complexity,[0],[0]
"With word labels: Since we need to track word labels in the state, we need to adjust every n by a factor of S, leading to O(S4n4 + ES2n2).",3.5 Complexity,[0],[0]
Our algorithm relies on the assumption that we can process the dependents to the left and right of a word independently and then combine the two halves.,4 Parse Representation,[0],[0]
"This means we need lexicalized structures, which the PTB does not provide.",4 Parse Representation,[0],[0]
We define a new representation in which each non-terminal symbol is associated with a specific word (the head).,4 Parse Representation,[0],[0]
"Unlike dependency parsing, we retain all the information required to reconstruct the constituency parse.
",4 Parse Representation,[0],[0]
"Our approach is related to Carreras et al. (2008) and Hayashi and Nagata (2016), with three key differences: (1) we encode non-terminals explicitly, rather than implicitly through adjunction operations, which can cause ambiguity, (2) we add representations of null elements and co-indexation, (3) we modify head rules to avoid problematic structures.
",4 Parse Representation,[0],[0]
Figure 3 shows a comparison of the PTB representation and ours.,4 Parse Representation,[0],[0]
"We add lexicalization, assigning each non-terminal to a word.",4 Parse Representation,[0],[0]
"The only other changes are visual notation, with non-terminals moved to be directly above the words to more clearly show the distinction between spines and edges.
",4 Parse Representation,[0],[0]
Spines:,4 Parse Representation,[0],[0]
"Each word is assigned a spine, shown im-
mediately above the word.",4 Parse Representation,[0],[0]
"A spine is the ordered set of non-terminals that the word is the head of, e.g. SVP for like.",4 Parse Representation,[0],[0]
"If a symbol occurs more than once in a spine, we use indices to distinguish instances.
",4 Parse Representation,[0],[0]
"Edges: An edge is a link between two words, with a label indicating the symbols it links in the child and parent spines.",4 Parse Representation,[0],[0]
"In our figures, edge labels are indicated by where edges start and end.
",4 Parse Representation,[0],[0]
"Null Elements: We include each null element in the spine of its parent, unlike Hayashi and Nagata (2016), who effectively treated null elements as words, assigning them independent spines.",4 Parse Representation,[0],[0]
"We also considered encoding null elements entirely on edges but found this led to poorer performance.
",4 Parse Representation,[0],[0]
Co-indexation:,4 Parse Representation,[0],[0]
"The treebank represents movement with index pairs on null elements and nonterminals, e.g. *1 and NP1 in Figure 3.",4 Parse Representation,[0],[0]
"We represent co-indexation with edges, one per reference, going from the null element to the non-terminal.",4 Parse Representation,[0],[0]
There are three special cases of co-indexation: (1) It is possible for trace edges to have the same start and end points as a non-trace edge.,4 Parse Representation,[0],[0]
We restrict this case to allow at most one trace edge.,4 Parse Representation,[0],[0]
This decreases edge coverage in the training set by 0.006%.,4 Parse Representation,[0],[0]
"(2) In some cases the reference non-terminal only spans a null element, e.g. the WHNP in Figure 4a.",4 Parse Representation,[0],[0]
For these we use a reversed edge to avoid creating a cycle.,4 Parse Representation,[0],[0]
"Figure 4a shows a situation where the trace edge links two positions in the same spine, which we assign with the spine during parsing.",4 Parse Representation,[0],[0]
(3) For parallel constructions the treebank coindexes arguments that fulfill the same roles (Fig. 4b).,4 Parse Representation,[0],[0]
These are distinct from the previous cases because neither index is on a null element.,4 Parse Representation,[0],[0]
"We considered two options: add edges from the repetition
to the referent (middle), or add edges from the repetition to the parent of the first occurrence (bottom).",4 Parse Representation,[0],[0]
"Option two produces fewer non-1-EC structures and explicitly represents all predicates, but only implicitly captures the original structure.",4 Parse Representation,[0],[0]
Prior work on parsing with spines has used radjunction to add additional non-terminals to spines.,4.1 Avoiding Adjunction Ambiguity,[0],[0]
"This introduces ambiguity, because edges modifying the same spine from different sides may not have a unique order of application.",4.1 Avoiding Adjunction Ambiguity,[0],[0]
We resolve this issue by using more articulated spines with the complete set of non-terminals.,4.1 Avoiding Adjunction Ambiguity,[0],[0]
"We found that 0.045% of spine instances in the development set are not observed in training, though in 70% of those cases an equivalent spine sans null elements is observed in training.",4.1 Avoiding Adjunction Ambiguity,[0],[0]
"To construct the spines, we lexicalize with head rules that consider the type of each non-terminal and its children.",4.2 Head Rules,[0],[0]
Different heads often represent more syntactic or semantic aspects of the phrase.,4.2 Head Rules,[0],[0]
"For trees, all head rules generate valid structures.",4.2 Head Rules,[0],[0]
"For graphs, head rules influence the creation of two problematic structures:
Cycles: These arise when the head chosen for a phrase is also an argument of another word in the phrase.",4.2 Head Rules,[0],[0]
Figure 5a shows a cycle between which and proposed.,4.2 Head Rules,[0],[0]
"We resolve this by changing the head of an SBAR to be an S rather than a Wh-noun phrase.
",4.2 Head Rules,[0],[0]
"One-Endpoint Crossing Violations: Figure 5b shows an example, with the trace from CEO to Page crossing two edges with no endpoints in common.",4.2 Head Rules,[0],[0]
We resolve this case by changing the head for VPs to be a child VP rather than an auxiliary.,4.2 Head Rules,[0],[0]
Algorithm Coverage: In Table 1 we show the impact of design decisions for our representation.,5 Results,[0],[0]
The percentages indicate how many sentences in the training set are completely recoverable by our algorithm.,5 Results,[0],[0]
"Each row shows the outcome of an addition to the previous row, starting from no traces at all, going to our representation with the head rules of Carreras et al. (2008), then changing the head rules, reversing null-null edges, and changing the target of edges in parallel constructions.",5 Results,[0],[0]
"The largest gain comes from changing the head rules, which is unsurprising since Carreras et al. (2008)’s rules were designed for trees (any set of rules form valid structures for trees).
",5 Results,[0],[0]
"Problematic Structures: Of the sentences we do not cover, 54% contain a cycle, 45% contain a 1- EC violation, and 1% contain both.",5 Results,[0],[0]
"To understand these problematic sentences, we manually inspected a random sample of twenty parses that contained a cycle and twenty parses with a 1-EC violation (these forty are 6% of all problematic parses, enough to identify the key remaining challenges).
",5 Results,[0],[0]
"For the cycles, eleven cases related to sentences containing variations of NP said interposed between two parts of a single quote.",5 Results,[0],[0]
A cycle was present because the top node of the parse was co-indexed with a null argument of said while said was an argument of the head word of the quote.,5 Results,[0],[0]
"The remaining cases were all instances of pseudo-attachment, which the treebank uses to show that non-adjacent constituents are related (Bies et al. 1995).",5 Results,[0],[0]
"These cases were split between use of Expletive (5) and Interpret Constituent Here (4) traces.
",5 Results,[0],[0]
It was more difficult to determine trends for cases where the parse structure has a 1-EC violation.,5 Results,[0],[0]
"The same three cases, Expletive, Interpret Constituent Here, and NP said accounted for half of the issues.",5 Results,[0],[0]
We implemented a parser with a first-order model using our algorithm and representation.,5.1 Implementation,[0],[0]
"Code for the parser, for conversion to and from our representation, and for our metrics is available3.",5.1 Implementation,[0],[0]
"Our parser uses a linear discriminative model, with features based on McDonald et al. (2005).",5.1 Implementation,[0],[0]
"We train
3 https://github.com/jkkummerfeld/ 1ec-graph-parser
with an online primal subgradient approach (Ratliff et al. 2007) as described by Kummerfeld, BergKirkpatrick, et al. (2015), with parallel lock-free sparse updates.
",5.1 Implementation,[0],[0]
"Loss Function: We use a weighted Hamming distance for loss-augmented decoding, as it can be efficiently decomposed within our dynamic program.",5.1 Implementation,[0],[0]
Calculating the loss for incorrect spines and extra edges is easy.,5.1 Implementation,[0],[0]
"For missing edges, we add when a deduction rule joins two spans that cover an end of the edge, since if it does not exist in one of those items it is not going to be created in future.",5.1 Implementation,[0],[0]
"To avoid double counting we subtract when combining two halves that contain the two ends of a gold edge4.
",5.1 Implementation,[0],[0]
Inside–Outside Calculations:,5.1 Implementation,[0],[0]
"Assigning scores to edges is simple, as they are introduced in a single item in the derivation.",5.1 Implementation,[0],[0]
"Spines must be introduced in multiple items (left, right, and external positions) and must be assigned a score in every case to avoid ties in beams.",5.1 Implementation,[0],[0]
"We add the score every time the spine is introduced and then subtract when two items with a spine in common are combined.
",5.1 Implementation,[0],[0]
Algorithm rule pruning: Many 1-EC structures are not seen in our data.,5.1 Implementation,[0],[0]
"We keep only the rules used in gold training parses, reducing the set of 49,292 from the general algorithm to 627 (including rules for both adding arcs and combining items).",5.1 Implementation,[0],[0]
"Almost every template in Algorithm 1 generates some unnecessary rules, and no items of type B are needed.
",5.1 Implementation,[0],[0]
"4 One alternative is to count half of it on each end, removing the need for subtraction later.",5.1 Implementation,[0],[0]
"Another is to add it during the combination step.
",5.1 Implementation,[0],[0]
"The remaining rules still have high coverage of the development set, missing only 15 rules, each applied once (out of 78,692 rule applications).",5.1 Implementation,[0],[0]
"By pruning in this way, we are considering the intersection of 1-EC graphs and the true space of structures used in language.
",5.1 Implementation,[0],[0]
"Chart Pruning: To improve speed we use beams and cube pruning (Chiang 2007), discarding items based on their Viterbi inside score.",5.1 Implementation,[0],[0]
We divide each beam into sub-beams based on aspects of the state.,5.1 Implementation,[0],[0]
"This ensures diversity and enables consideration of only compatible items during binary and ternary compositions.
",5.1 Implementation,[0],[0]
"Coarse to Fine Pruning: Rather than parsing immediately with the full model we use several passes with progressively richer structure (Goodman 1997): (1) Projective parsing without traces or spines, and simultaneously a trace classifier, (2) Non-projective parsing without spines, and simultaneously a spine classifier, (3) Full structure parsing.",5.1 Implementation,[0],[0]
"Each pass prunes using parse max-marginals and classifier scores, tuned on the development set.",5.1 Implementation,[0],[0]
The third pass also prunes spines that are not consistent with any unpruned edge from the second pass.,5.1 Implementation,[0],[0]
"For the spine classifier we use a bidirectional LSTM tagger, implemented in DyNet (Neubig et al. 2017).
",5.1 Implementation,[0],[0]
Speed: Parsing took an average of 8.6 seconds per sentence for graph parsing and 0.5 seconds when the parser is restricted to trees5.,5.1 Implementation,[0],[0]
"Our algorithm is also amenable to methods such as semi-supervised and adaptive supertagging, which can improve the speed of a parser after training (Kummerfeld, Roesner, et al. 2010; Lewis and Steedman 2014).
",5.1 Implementation,[0],[0]
Tree Accuracy:,5.1 Implementation,[0],[0]
"On the standard tree-metric, we score 88.1.",5.1 Implementation,[0],[0]
"Using the same non-gold POS tags as input, Carreras et al. (2008) score 90.9, probably due to their second-order features and head rules tuned for performance6.",5.1 Implementation,[0],[0]
"Shifting to use their head rules, we score 88.9.",5.1 Implementation,[0],[0]
"Second-order features could be added to our model through the use of forest reranking, an improvement that would be orthogonal to this paper’s contributions.
",5.1 Implementation,[0],[0]
We can also evaluate on spines and edges.,5.1 Implementation,[0],[0]
"Since their system produces regular PTB trees, we con-
5 Using a single core of an Amazon EC2 m4.2xlarge instance (2.4 GHz Xeon CPU and 32 Gb of RAM).
",5.1 Implementation,[0],[0]
"6 Previous work has shown that the choice of head can significantly impact accuracy (Schwartz et al. 2012).
vert its output to our representation and compare its results with our system using their head rules.",5.1 Implementation,[0],[0]
"We see slightly lower accuracy for our system on both spines (94.0 vs. 94.3) and edges (90.4 vs. 91.1).
",5.1 Implementation,[0],[0]
Trace Accuracy: Table 2 shows results using Johnson (2002)’s trace metric.,5.1 Implementation,[0],[0]
"Our parser is competitive with previous work that has highly-engineered models: Johnson’s system has complex non-local features on tree fragments, and similarly Kato and Matsubara (K&M 2016) consider complete items in the stack of their transition-based parser.",5.1 Implementation,[0],[0]
On co-indexation our results fall between Johnson and K&M.,5.1 Implementation,[0],[0]
"Converting to our representation, our parser has higher precision than K&M on trace edges (84.1 vs. 78.1) but lower recall (59.5 vs. 71.3).",5.1 Implementation,[0],[0]
"One modeling challenge we observed is class imbalance: of the many places a trace could be added, only a small number are correct, and so our model tends to be conservative (as shown by the P/R tradeoff).",5.1 Implementation,[0],[0]
We propose a representation and algorithm that cover 97.3% of graph structures in the PTB.,6 Conclusion,[0],[0]
"Our algorithm is O(n4), uniquely decomposes parses, and enforces the property that parses are composed of a core tree with additional traces and null elements.",6 Conclusion,[0],[0]
A proof of concept parser shows that our algorithm can be used to parse and recover traces.,6 Conclusion,[0],[0]
"We thank Greg Durrett for advice on parser implementation and debugging, and the action editor and anonymous reviewers for their helpful feedback.",Acknowledgments,[0],[0]
"This research was partially supported by a General Sir John Monash Fellowship and the Office of Naval
Research under MURI Grant",Acknowledgments,[0],[0]
No. N000140911081.,Acknowledgments,[0],[0]
"General treebank analyses are graph structured, but parsers are typically restricted to tree structures for efficiency and modeling reasons.",abstractText,[0],[0]
"We propose a new representation and algorithm for a class of graph structures that is flexible enough to cover almost all treebank structures, while still admitting efficient learning and inference.",abstractText,[0],[0]
"In particular, we consider directed, acyclic, one-endpoint-crossing graph structures, which cover most long-distance dislocation, shared argumentation, and similar tree-violating linguistic phenomena.",abstractText,[0],[0]
"We describe how to convert phrase structure parses, including traces, to our new representation in a reversible manner.",abstractText,[0],[0]
"Our dynamic program uniquely decomposes structures, is sound and complete, and covers 97.3% of the Penn English Treebank.",abstractText,[0],[0]
We also implement a proofof-concept parser that recovers a range of null elements and trace types.,abstractText,[0],[0]
Parsing with Traces: An O(n) Algorithm and a Structural Representation,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2411–2420 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"During the last decade, social media have become extremely popular, on which billions of usergenerated contents are posted every day.",1 Introduction,[0],[0]
Many users have been writing about their thoughts and lives on the go.,1 Introduction,[0],[0]
"The massive unstructured data from social media provides valuable information for a variety of applications such as stock prediction (Bollen et al., 2011), public health analysis (Wilson and Brownstein, 2009; Paul and Dredze, 2011), real-time event detection (Sakaki et al., 2010), and so on.",1 Introduction,[0],[0]
"The quality of these applications is highly impacted by the performance of natural language processing tasks.
∗Corresponding author.
",1 Introduction,[0],[0]
Part-of-speech (POS) tagging is one of the most important natural language processing tasks.,1 Introduction,[0],[0]
"It has also been widely used in the social media analysis systems (Ritter et al., 2012; Lamb et al., 2013; Kiritchenko et al., 2014).",1 Introduction,[0],[0]
Most stateof-the-art POS tagging approaches are based on supervised methods.,1 Introduction,[0],[0]
"Hence, they usually require a large amount of annotated data to train models.",1 Introduction,[0],[0]
Many datasets have been constructed for POS tagging task.,1 Introduction,[0],[0]
"Because newswire articles are carefully edited, benchmarks usually use them for annotation (Marcus et al., 1993).",1 Introduction,[0],[0]
"However, usergenerated contents on social media are usually informal and contain many nonstandard lexical items.",1 Introduction,[0],[0]
"Moreover, the difference in domains between training data and evaluation data may heavily impact the performance of approaches based on supervised methods (Caruana and NiculescuMizil, 2006).",1 Introduction,[0],[0]
"Hence, most POS tagging methods cannot achieve the same performance as reported on newswire domain when applied on Twitter (Owoputi et al., 2013).
",1 Introduction,[0],[0]
"To perform the Twitter POS tagging task, some approaches have been proposed to perform the task.",1 Introduction,[0],[0]
"Gimpel et al. (2011) manually annotated 1,827 tweets and carefully studied various fea-
2411
tures.",1 Introduction,[0],[0]
"Ritter et al. (2011) also constructed a labeled dataset, which contained 787 tweets, to empirically evaluate the performance of supervised methods on Twitter.",1 Introduction,[0],[0]
Owoputi et al. (2013) incorporated word clusters into the feature sets and further improved the performance.,1 Introduction,[0],[0]
"From these works, we can observe that the size of the training data was much smaller than the newswire domain’s.
",1 Introduction,[0],[0]
"Besides the challenge of lack of training data, the frequent use of out-of-vocabulary words also makes this problem difficult to address.",1 Introduction,[0],[0]
"Social media users often use informal ways of expressing their ideas and often spell words phonetically (e.g., “2mor” for “tomorrow”).",1 Introduction,[0],[0]
"In addition, they also make extensive use of emoticons and abbreviations (e.g., “:-)” for smiling emotion and “LOL” for laughing out loud).",1 Introduction,[0],[0]
"Moreover, new symbols, abbreviations, and words are constantly being created.",1 Introduction,[0],[0]
"Figure 1 shows an example of tagged Tweet.
",1 Introduction,[0],[0]
"To tackle the challenges posed by the lack of training data and the out-of-vocabulary words, in this paper, we propose a novel recurrent neural network, which we call Target Preserved Adversarial Neural Network (TPANN) to perform the task.",1 Introduction,[0],[0]
"It can make use of a large quantity of annotated data from other resourcerich domains, unlabeled in-domain data, and a small amount of labeled in-domain data.",1 Introduction,[0],[0]
All of these datasets can be easily obtained.,1 Introduction,[0],[0]
"To make use of unlabeled data, motivated by the work of Goodfellow et al. (2014) and Chen et al. (2016), the proposed method extends the bi-directional long short-term memory recurrent neural network (bi-LSTM) with an adversarial predictor.",1 Introduction,[0],[0]
"To overcome the defect that adversarial networks can merely learn the common features, we propose to use an autoencoder only acting on target dataset to preserve its own specific features.",1 Introduction,[0],[0]
"For tackling the out-of-vocabulary problem, the proposed method also incorporates a character level convolutional neutral network to leverage subword information.
",1 Introduction,[0],[0]
"The contributions of this work are as follows:
• We propose to incorporate large scale unlabeled in-domain data, out-of-domain labeled data, and in-domain labeled data for Twitter part-of-speech tagging task.
",1 Introduction,[0],[0]
"• We introduce a novel recurrent neural network, which can learn domain-invariant rep-
resentations through in-domain and out-ofdomain data and construct a cross domain POS tagger through the learned representations.",1 Introduction,[0],[0]
"The proposed method also tries to preserve the specific features of target domain.
",1 Introduction,[0],[0]
• Experimental results demonstrate that the proposed method can lead to better performance in most of cases on three different datasets.,1 Introduction,[0],[0]
"In this work, we propose a novel recurrent neural network, Target Preserved Adversarial Neural Network (TPANN), to learn common features between resource-rich domain and target domain, simultaneously to preserve target domain-specific features.",2 Approach,[0],[0]
It extends the bi-directional LSTM with adversarial network and autoencoder.,2 Approach,[0],[0]
The architecture of TPANN is illustrated in Figure 2.,2 Approach,[0],[0]
"The model consists of four components: Feature Extractor, POS Tagging Classifier, Domain Discriminator and Target Domain Autoencoder.",2 Approach,[0],[0]
"In the following sections, we will detail each part of the proposed architecture and training methods.",2 Approach,[0],[0]
"The feature extractor F adopts CNN to extract character embedding features, which can tackle the out-of-vocabulary word problem effectively.",2.1 Feature Extractor,[0],[0]
"To incorporate word embedding features, we concatenate word embedding to character embedding as the input of bi-LSTM on the next layer.",2.1 Feature Extractor,[0],[0]
"Utilizing a bi-LSTM to model sentences, F can extract sequential relations and context information.
",2.1 Feature Extractor,[0],[0]
"We denote the input sentence as x and the i-th word as xi. xi ∈ S(x) and xi ∈ T (x) represent input samples are from source domain and target domain, respectively.",2.1 Feature Extractor,[0],[0]
We denote the parameters of F as θf .,2.1 Feature Extractor,[0],[0]
"Let V be the vocabulary of words, and C be the vocabulary of characters.",2.1 Feature Extractor,[0],[0]
d is the dimensionality of character embedding then Q ∈ Rd×|C| is the representation matrix of vocabulary.,2.1 Feature Extractor,[0],[0]
We assume that word xi ∈ V is made up of a sequence of characters Ci =,2.1 Feature Extractor,[0],[0]
"[c1, c2, . . .",2.1 Feature Extractor,[0],[0]
", cl], where l is the max length of word and every word will be padded to this length.",2.1 Feature Extractor,[0],[0]
"Then Ci ∈ Rd×l would be the inputs of CNN.
",2.1 Feature Extractor,[0],[0]
"We apply a narrow convolution between Ci and filter H ∈ Rd×k, where k is the width of the filter.
",2.1 Feature Extractor,[0],[0]
After that we add a bias and apply nonlinearity to obtain a feature map mi ∈ Rl−k+1.,2.1 Feature Extractor,[0],[0]
"Specifically, the j-th element of mi is given by:
ik[j] = tanh(〈Ci[∗, j : j + k",2.1 Feature Extractor,[0],[0]
"− 1],H〉+ b), (1)
where Ck[∗, j : j + k",2.1 Feature Extractor,[0],[0]
− 1] is the j-to-(j + k,2.1 Feature Extractor,[0],[0]
"− 1)-th column of Ci and 〈A,B〉 = Tr(ABT ) is the Frobenius inner product.",2.1 Feature Extractor,[0],[0]
"We then apply a max-over-time pooling operation (Collobert et al., 2011) over the feature map.",2.1 Feature Extractor,[0],[0]
CNN uses multiple filters with varying widths to obtain the feature vector ~ci for word xi.,2.1 Feature Extractor,[0],[0]
"Then, the character-level feature vector ~ci is concatenated to the word embedding ~wi to form the input of bi-LSTM on the next layer.",2.1 Feature Extractor,[0],[0]
The word embedding ~w is pretrained on 30 million tweets.,2.1 Feature Extractor,[0],[0]
"Then, the hidden states h of bi-LSTM turn into the features that will be transfered to P , Q andR, i.e. F(x) = h.",2.1 Feature Extractor,[0],[0]
POS tagging classifier P and domain discriminator Q take F(x) as input.,2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
They are standard feed-forward networks with a softmax layer for classification.,2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"P predicts POS tagging label to get classification capacity, and Q discriminates domain label to make F(x) domain-invariant.
",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
The POS tagging classifier P maps the feature vector F(xi) to its label.,2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
We denote the parameters of this mapping as θy.,2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"The POS tagging
classifier is trained on Ns samples from the source domain with the cross entropy loss:
",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
Ltask =,2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
− Ns∑ i=1,2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"yi ∗ log ŷi, (2)
where yi is the one-hot vector of POS tagging label corresponding to xi ∈ S(x), ŷi is the output of top softmax layer:",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
ŷi = P(F(xi)).,2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"During the training time, The parameters θf and θy are optimized to minimize the classification loss Ltask.",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"This ensures that P(F(xi)) can make accurate prediction on the source domain.
",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"Conversely, domain discriminator maps the same hidden states h to the domain labels with parameters θd.",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"The domain discriminator aims to discriminate the domain label with following loss function: Ltype = − Ns+Nt∑
i=1",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
{di log d̂i+(1−di),2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"log(1− d̂i)}, (3)
where di is the ground truth domain label for sample",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"i, d̂i is the output of top layer: d̂i = Q(F(xi)).",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
Nt meansNt samples from the target domain.,2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"The domain discriminator is trained towards a saddle point of the loss function through minimizing the loss over θd while maximizing the loss over θf (Ganin et al., 2016).",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"Optimizing θf ensures that the domain discriminator can’t discriminate
the domain, i.e., the feature extractor finds the common features between the two domains.",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"Through training adversarial networks, we can obtain domain-invariant features hcommon, but it will weaken some domain-specific features which are useful for POS tagging classification.",2.3 Target Domain Autoencoder,[0],[0]
"Merely obtaining domain invariant features would therefore limit the classification ability.
",2.3 Target Domain Autoencoder,[0],[0]
"Our model tries to tackle this defect by introducing domain-specific autoencoder R, which attempts to reconstruct target domain data.",2.3 Target Domain Autoencoder,[0],[0]
"Inspired by (Sutskever et al., 2014) but different from (Dai and Le, 2015), we treat the feature extractor F as encoder.",2.3 Target Domain Autoencoder,[0],[0]
"In addition, we combine the last hidden states of the forward LSTM and backward LSTM in F as the initial state h0(dec) of the decoder LSTM.",2.3 Target Domain Autoencoder,[0],[0]
"Hence, we don’t need to reverse the order of words of the input sentences and the model avoids the difficulty of ”establish communication” between the input and the output (Sutskever et al., 2014).
",2.3 Target Domain Autoencoder,[0],[0]
"Similar to (Zhang et al., 2016), we use h0(dec) and embedding vector of the previous word as the inputs of the decoder, but in a computationally more efficient manner by computing previous word representation.",2.3 Target Domain Autoencoder,[0],[0]
"We assume that (x̂1, · · · , x̂T ) is the output sequence.",2.3 Target Domain Autoencoder,[0],[0]
"zt is the t-th word representation: zt = MLP (ht), and MLP is the multiple perceptron function.",2.3 Target Domain Autoencoder,[0],[0]
Hidden state ht = LSTM([h0(dec) :,2.3 Target Domain Autoencoder,[0],[0]
"zt−1], ht−1), where [· : ·] is the concatenation operation.",2.3 Target Domain Autoencoder,[0],[0]
"We estimate the conditional probability p(x̂1, · · · , x̂T |h0(dec))",2.3 Target Domain Autoencoder,[0],[0]
"as follows:
p(x̂1, · · · , x̂T |h0(dec))",2.3 Target Domain Autoencoder,[0],[0]
"= T∏
t=1
p(x̂t|h0(dec), z1, · · · , zt−1), (4)
where each p(x̂t|h0(dec), z1, · · · , zt−1) distribution is computed with softmax over all the words in the vacabulary.
",2.3 Target Domain Autoencoder,[0],[0]
"Our aim is to minimize the following loss function with respect to parameters θr:
Ltarget = − Nt∑ i=1",2.3 Target Domain Autoencoder,[0],[0]
"xi ∗ log x̂i, (5)
where xi is the one-hot vector of i-th word.",2.3 Target Domain Autoencoder,[0],[0]
"This makes h0(dec) learn an undercomplete and most salient sentence representation of target domain
data.",2.3 Target Domain Autoencoder,[0],[0]
"When the adversarial networks try to optimize the hidden representation to common representation hcommon, The target domain autoencoder counteracts a tendency of the adversarial network to erase target domain features by optimizing the common representation to be informative on the target-domain data.",2.3 Target Domain Autoencoder,[0],[0]
"Our model can be trained end-to-end with standard back-propagation, which we will detail in this section.
",2.4 Training,[0],[0]
"Our ultimate training goal is to minimize the total loss function with parameters {θf , θy, θr, θd} as follows:
Ltotal = αLtask + βLtarget + γLtype, (6) where α, β, γ are the weights to balance the effects of P ,R and Q.
For obtaining domain-invariant representation hcommon, inspired by (Ganin and Lempitsky, 2015), we introduce a special gradient reversal layer (GRL), which does nothing during forward propagation, but negates the gradients if it receives backward propagation, i.e. g(F(x))",2.4 Training,[0],[0]
= F(x) but ∇g(F(x)),2.4 Training,[0],[0]
= −λ∇F(x).,2.4 Training,[0],[0]
"We insert the GRL between F and Q, which can run standard Stochastic Gradient Descent with respect to θf and θd.",2.4 Training,[0],[0]
The parameter −λ drives the parameters θf not to amplify the dissimilarity of features when minimize Ltpye.,2.4 Training,[0],[0]
"So by introducing a GRL, F can drive its parameters θf to extract hidden representations that help the POS tagging classification and hamper the domain discrimination.
",2.4 Training,[0],[0]
"In order to preserve target domain-specific features, we only optimize the autoencoder on target domain data for reconstruction tasks.
",2.4 Training,[0],[0]
"Through above procedures, the model can learn the common features between domains, simultaneously preserve target domain-specific features.",2.4 Training,[0],[0]
"Finally, we can update the parameters as follows:
θf = θf − µ(α∂L i task
∂θf + β",2.4 Training,[0],[0]
"∂Litarget ∂θf − γ · λ∂L i type ∂θf )
θy = θy − µ · α∂L i task
∂θy
θr = θr − µ · β ∂Litarget ∂θr θd = θd − µ · γ ∂Litype ∂θd ,
(7)
where µ is the learning rate.",2.4 Training,[0],[0]
"Because the size of the WSJ is more than 100 times that of the labeled Twitter dataset, if we directly train the model with the combined dataset, the final results are much worse than those using two training steps.",2.4 Training,[0],[0]
"So, we adopt adversarial training on WSJ and unlabeled Twitter dataset at the first step, then use a small number of in-domain labeled data to fine-tune the parameters with a low learning rate.",2.4 Training,[0],[0]
"In this section, we will detail the datasets used for experiments and experimental setup.",3 Experiments,[0],[0]
"The methods proposed in this work incorporate out-of-domain labeled data from resource-rich domains, large scale unlabeled in-domain data, and a small number of labeled in-domain data.",3.1 Datasets,[0],[0]
The datasets used in this work are as follows: Labeled out-of-domain data.,3.1 Datasets,[0],[0]
"We use a standard benchmark dataset for adversarial POS tagging, namely the Wall Street Journal (WSJ) data from the Penn TreeBank v3 (Marcus et al., 1993), sections 0-24 for the out-of-domain data.",3.1 Datasets,[0],[0]
Labeled in-domain data.,3.1 Datasets,[0],[0]
"For training and evaluating POS tagging approaches, we compare the proposed method with other approaches on three benchmarks: RIT-Twitter (Ritter et al., 2011), NPSCHAT (Forsyth, 2007), and ARKTwitter (Gimpel et al., 2011).",3.1 Datasets,[0],[0]
Unlabeled in-domain data.,3.1 Datasets,[0],[0]
"For training the adversarial network, we need to use a dataset that has large scale unlabeled tweets.",3.1 Datasets,[0],[0]
"Hence, in this work, we construct large scale unlabeled data (UNL), from Twitter using its API.
",3.1 Datasets,[0],[0]
The detailed data statistics of the datasets used in this work are listed in Table 1.,3.1 Datasets,[0],[0]
"We select both state-of-the-art and classic methods for comparison, as follows:
• Stanford POS",3.2 Experimental Setup,[0],[0]
Tagger:,3.2 Experimental Setup,[0],[0]
"Stanford POS Tagger is a widely used tool for newswire domains (Toutanova et al., 2003).",3.2 Experimental Setup,[0],[0]
"In this work, we train it using two different sets, the WSJ (sections 0-18) and a WSJ, IRC, and Twitter mixed corpus.",3.2 Experimental Setup,[0],[0]
"We use StanfordWSJ and Stanford-MIX to represent them, respectively.
",3.2 Experimental Setup,[0],[0]
"• T-POS: T-Pos (Ritter et al., 2011) adopts the Conditional Random Fields and clustering algorithm to perform the task.",3.2 Experimental Setup,[0],[0]
"It was trained from a mixture of hand-annotated tweets and existing POS-labeled data.
",3.2 Experimental Setup,[0],[0]
• GATE,3.2 Experimental Setup,[0],[0]
"Tagger: GATE tagger (Derczynski et al., 2013) is based on vote-constrained bootstrapping with unlabeled data.",3.2 Experimental Setup,[0],[0]
"It combines cases where available taggers use different tagsets.
",3.2 Experimental Setup,[0],[0]
"• ARK Tagger: ARK tagger (Owoputi et al., 2013) is a system that reports the best accuracy on the RIT dataset.",3.2 Experimental Setup,[0],[0]
"It uses unsupervised word clustering and a variety of lexical features.
",3.2 Experimental Setup,[0],[0]
• bi-LSTM:,3.2 Experimental Setup,[0],[0]
"Bidirectional Long Short-Term Memory (LSTM) networks have been widely used in a variety of sequence labeling tasks (Graves and Schmidhuber, 2005).",3.2 Experimental Setup,[0],[0]
"In this work, we evaluate it at character level, word level, and combining them together.",3.2 Experimental Setup,[0],[0]
bi-LSTM (word level) uses one layer of bi-LSTM to extract word-level features and adopts a random initialization method to transform words to vectors.,3.2 Experimental Setup,[0],[0]
"bi-LSTM (character level) represents a method that combines bi-LSTM and CNN-based character embedding, a similar approach with character-aware neural network described in (Kim et al., 2015) to handle the out-ofvocabulary words.",3.2 Experimental Setup,[0],[0]
"bi-LSTM (word level pretrain) architecture is the same as that of bi-LSTM(word level) but adopts word2vec tool (Mikolov et al., 2013) to vectorize.",3.2 Experimental Setup,[0],[0]
"bi-LSTM (combine) concatenates word to character features.
",3.2 Experimental Setup,[0],[0]
The hyper-parameters used for our model are as follows.,3.2 Experimental Setup,[0],[0]
AdaGrad optimizer trained with crossentropy loss is used with 0.1 as the default learning rate.,3.2 Experimental Setup,[0],[0]
The dimensionality of word embedding is set to 200.,3.2 Experimental Setup,[0],[0]
The dimensionality for random initialized character embedding is set to 25.,3.2 Experimental Setup,[0],[0]
We adopt a bi-LSTM for encoding with each layer consisting of 250 hidden neurons.,3.2 Experimental Setup,[0],[0]
We set three layers of standard LSTM for decoding.,3.2 Experimental Setup,[0],[0]
Each LSTM layer consists of 500 hidden neurons.,3.2 Experimental Setup,[0],[0]
Adam optimizer trained with cross-entropy loss is used to fine-tune with 0.0001 as the default learning rate.,3.2 Experimental Setup,[0],[0]
Finetuning is run for 100 epochs using early stop.,3.2 Experimental Setup,[0],[0]
"In this section, we will report experimental results and a detailed analysis of the results for the three different datasets.",4 Results and Discussion,[0],[0]
"The RIT-Twitter is split into training, development and evaluation sets (RIT-Train, RIT-Dev, RITTest).",4.1 Evaluation on RIT-Twitter,[0],[0]
"The splitting method is shown in (Derczynski et al., 2013), and the dataset statistics are listed in Table 1.",4.1 Evaluation on RIT-Twitter,[0],[0]
Table 2 shows the results of our method and other approaches on the RIT-Twitter dataset.,4.1 Evaluation on RIT-Twitter,[0],[0]
"RIT-Twitter uses the PTB tagset with several Twitter-specific tags: retweets, @usernames, hashtags, and urls.",4.1 Evaluation on RIT-Twitter,[0],[0]
"Since words in these
categories can be tagged almost perfectly using simple regular expressions, similar to (Owoputi et al., 2013), we use regular expressions to tags these words appropriately for all systems.
",4.1 Evaluation on RIT-Twitter,[0],[0]
"From the results of the Stanford-WSJ, we can observe that the newswire domain is different from Twitter.",4.1 Evaluation on RIT-Twitter,[0],[0]
"Although the token-level accuracy of the Stanford POS Tagger is higher than 97.0% on the PTB dataset, its performance on Twitter drops sharply to 73.37%.",4.1 Evaluation on RIT-Twitter,[0],[0]
"By incorporating some indomain labeled data for training, the accuracy of Stanford POS Tagger can reach up to 83.14%.",4.1 Evaluation on RIT-Twitter,[0],[0]
"Taking a variety of linguistic features and many other resources into consideration, the T-POS, GATE tagger, and ARK tagger can achieve better performance.
",4.1 Evaluation on RIT-Twitter,[0],[0]
"The second part of Table 2 shows the results of the bi-LSTM based methods, which are trained on the RIT-Train dataset.",4.1 Evaluation on RIT-Twitter,[0],[0]
"According to the results of word level, we can see that word2vec can provide valuable information.",4.1 Evaluation on RIT-Twitter,[0],[0]
"The pre-trained word vectors in bi-LSTM(word level pretrain) give almost 10% higher accuracy than bi-LSTM(word level).
",4.1 Evaluation on RIT-Twitter,[0],[0]
"Comparing the character-level bi-LSTM with word-level bi-LSTM with random initialization, we can observe that the character-level method can achieve better performance than the word-level method.",4.1 Evaluation on RIT-Twitter,[0],[0]
"bi-LSTM(combine) combines word with character features, as described in Section 2.1,
which achieves the best results at 89.48% in the bi-LSTM based baseline systems and shows that the morphological features and pre-trained word vectors are both useful for POS tagging.
",4.1 Evaluation on RIT-Twitter,[0],[0]
"The third part of Table 2 shows the results of our methods incorporating out-of-domain labeled data, in-domain unlabeled data, and in-domain labeled data.",4.1 Evaluation on RIT-Twitter,[0],[0]
"Putting everything together, our model can achieve 90.92% on this dataset.",4.1 Evaluation on RIT-Twitter,[0],[0]
"Compared with the architecture without an adversarial model, our method is almost 1% better.",4.1 Evaluation on RIT-Twitter,[0],[0]
It demonstrates that adversarial networks can significantly help with tasks of this nature.,4.1 Evaluation on RIT-Twitter,[0],[0]
"Through introducing the autoencoder in target domain, we can preserve domain-specific features for better performance.",4.1 Evaluation on RIT-Twitter,[0],[0]
"Compared with the ARK tagger, which achieves the previous best result on this dataset, our model is also 0.52% better, the error reduction rate is more than 5.5%.
",4.1 Evaluation on RIT-Twitter,[0],[0]
"To better understand why adversarial networks can help transfer domains from newswire to Twitter, in this work we also followed the method Ganin and Lempitsky (2015) used to visualize the outputs of LSTM with tSNE (Van Der Maaten, 2013).",4.1 Evaluation on RIT-Twitter,[0],[0]
Figure 3 shows the visualization results.,4.1 Evaluation on RIT-Twitter,[0],[0]
"From the figure, we can see that the adversary in our method makes the two distributions of features much more similar, which means that the outputs of bi-LSTM are domain-invariant.",4.1 Evaluation on RIT-Twitter,[0],[0]
"Hence, the PTB training data can provide much more help than directly combining PTB and RIT-Train together.",4.1 Evaluation on RIT-Twitter,[0],[0]
"IRC, which contains Internet relay room messages from 2006, is a medium of online conversational text.",4.2 Evaluation on NPSChat,[0],[0]
Its content is very similar to tweets.,4.2 Evaluation on NPSChat,[0],[0]
"We evaluate the proposed method on the NPSChat corpus (Forsyth, 2007), a PTB-part-of-speech annotated dataset of IRC.
",4.2 Evaluation on NPSChat,[0],[0]
"We compared our method with a tagger in the same setup as experiments with (Forsyth, 2007).",4.2 Evaluation on NPSChat,[0],[0]
The training part contains 90% of the data.,4.2 Evaluation on NPSChat,[0],[0]
The testing part contains the other 10%.,4.2 Evaluation on NPSChat,[0],[0]
Table 3 shows the results of the ARK Tagger and our method.,4.2 Evaluation on NPSChat,[0],[0]
"We used PTB, unlabeled Twitter, and the training part of NPSChat to train our model.",4.2 Evaluation on NPSChat,[0],[0]
"From the results, we can see that our model achieved 94.1% accuracy.",4.2 Evaluation on NPSChat,[0],[0]
"This is significantly better than the result Forsyth (2007) reported, which was 90.8%.",4.2 Evaluation on NPSChat,[0],[0]
"They trained their tagger with a mix of several POS-annotated corpora (12K from Twitter, 40K from IRC, and 50K from PTB).",4.2 Evaluation on NPSChat,[0],[0]
"Our method also outperforms state-of-the-art results 93.4%±0.3%, which was achieved by the ARK Tagger with various external corpus and features, e.g., Brown clustering, PTB, Freebase lists of celebrities, and video games.",4.2 Evaluation on NPSChat,[0],[0]
"ARK-Twitter data contains an entire dataset consisting of a number of tweets sampled from one particular day (October 27, 2010) described in (Gimpel et al., 2011).",4.3 Evaluation on ARK-Twitter,[0],[0]
This part is used for training.,4.3 Evaluation on ARK-Twitter,[0],[0]
"They also created another dataset, which consists of 547 tweets, for evaluation (DAILY547).",4.3 Evaluation on ARK-Twitter,[0],[0]
"This dataset consists of one random English tweet from every day between January 1, 2011 and June 30, 2012.",4.3 Evaluation on ARK-Twitter,[0],[0]
"The distribution of training data may be slightly different from the testing data, for example a substantial fraction of the messages in the training data are about a basketball game.",4.3 Evaluation on ARK-Twitter,[0],[0]
"Since ARK-Twitter uses a different tagset with PTB, we manually construct a table to link tags for the two datasets.
",4.3 Evaluation on ARK-Twitter,[0],[0]
Table 4 shows the results of different methods on this dataset.,4.3 Evaluation on ARK-Twitter,[0],[0]
"From the results, we can see that our method can achieve a better result than (Gimpel et al., 2011).",4.3 Evaluation on ARK-Twitter,[0],[0]
"However, the performance of our method is worse than the ARK Tagger.",4.3 Evaluation on ARK-Twitter,[0],[0]
"Through analyzing the errors, we find that 16.7% errors occurr between nouns and proper nouns.",4.3 Evaluation on ARK-Twitter,[0],[0]
"Since our method do not include any ontology or knowledge, proper nouns can not be easily detected.",4.3 Evaluation on ARK-Twitter,[0],[0]
"However, the ATK Tagge add a tokenlevel name list feature.",4.3 Evaluation on ARK-Twitter,[0],[0]
"The name list is useful for proper nouns recognition, which fires on names from many sources, such as Freebase lists of celebrities, the Moby Words list of US Locations, proper names from Mark Kantrowitz’s name corpus and so on.",4.3 Evaluation on ARK-Twitter,[0],[0]
"So, our model is also competitive when lacking of manual feature knowledge.",4.3 Evaluation on ARK-Twitter,[0],[0]
Part-of-Speech tagging is an important preprocessing step and can provide valuable information for various natural language processing tasks.,5 Related Work,[0],[0]
"In recent years, deep learning algorithms have been successfully used for POS tagging.",5 Related Work,[0],[0]
A number of approaches have been proposed and have achieved some progress.,5 Related Work,[0],[0]
"Santos and Guimaraes (2015) proposed
using a character-based convolutional neural network to perform the POS tagging problem.",5 Related Work,[0],[0]
"Bi-LSTMs with word, character or unicode byte embedding were also introduced to achieve the POS tagging and named entity recognition tasks (Plank et al., 2016; Chiu and Nichols, 2015; Ma and Hovy, 2016).",5 Related Work,[0],[0]
"In this work, we study the problem from a domain adaption perspective.",5 Related Work,[0],[0]
"Inspired by these works, we also propose to use character-level methods to handle out-ofvocabulary words and bi-LSTMs to model the sequence relations.
",5 Related Work,[0],[0]
"Adversarial networks were successfully used for image generation (Goodfellow et al., 2014; Dosovitskiy et al., 2015; Denton et al., 2015), domain adaption (Tzeng et al., 2014; Ganin et al., 2016), and semi-supervised learning (Denton et al., 2016).",5 Related Work,[0],[0]
"The key idea of adversarial networks for domain adaption is to construct invariant features by optimizing the feature extractor as an adversary against the domain classifier (Zhang et al., 2017).
",5 Related Work,[0],[0]
Sequence autoencoder reads the input sequence into a vector and then tries to reconstruct it.,5 Related Work,[0],[0]
Dai and Le (2015) used the model on a number of different tasks and verified its validity.,5 Related Work,[0],[0]
"Li et al. (2015) introduced the model to hierarchically build an embedding for a paragraph, showing that the model was able to encode texts to preserve syntactic, semantic, and discourse coherence.
",5 Related Work,[0],[0]
"In this work, we incorporate adversarial networks with autoencoder to obtain domaininvariant features and keep domain-specific features.",5 Related Work,[0],[0]
Our model is more suitable for target domain tasks.,5 Related Work,[0],[0]
"In this work, we propose a novel adversarial neural network to address the POS tagging problem.",6 Conclusion,[0],[0]
"Besides learning common representations between source domain and target domain, it can simultaneously preserve specific features of target domain.",6 Conclusion,[0],[0]
"The proposed method leverages newswire resources and large scale in-domain unlabeled data to help POS tagging classification on Twitter, which has a few of labeled data.",6 Conclusion,[0],[0]
We evaluate the proposed method and several state-ofthe-art methods on three different corpora.,6 Conclusion,[0],[0]
"In most of the cases, the proposed method can achieve better performance than previous methods.",6 Conclusion,[0],[0]
"Experimental results demonstrate that the proposed
method can make full use of these resources, which can be easily obtained.",6 Conclusion,[0],[0]
The authors wish to thank the anonymous reviewers for their helpful comments.,Acknowledgments,[0],[0]
"This work was partially funded by National Natural Science Foundation of China (No. 61532011, 61473092, and 61472088) and STCSM (No.16JC1420401).",Acknowledgments,[0],[0]
"In this work, we study the problem of partof-speech tagging for Tweets.",abstractText,[0],[0]
"In contrast to newswire articles, Tweets are usually informal and contain numerous out-ofvocabulary words.",abstractText,[0],[0]
"Moreover, there is a lack of large scale labeled datasets for this domain.",abstractText,[0],[0]
"To tackle these challenges, we propose a novel neural network to make use of out-of-domain labeled data, unlabeled in-domain data, and labeled indomain data.",abstractText,[0],[0]
"Inspired by adversarial neural networks, the proposed method tries to learn common features through adversarial discriminator.",abstractText,[0],[0]
"In addition, we hypothesize that domain-specific features of target domain should be preserved in some degree.",abstractText,[0],[0]
"Hence, the proposed method adopts a sequence-to-sequence autoencoder to perform this task.",abstractText,[0],[0]
Experimental results on three different datasets show that our method achieves better performance than state-of-the-art methods.,abstractText,[0],[0]
Part-of-Speech Tagging for Twitter with Adversarial Neural Networks,title,[0],[0]
"This paper is about weighted correlation clustering (Bansal et al., 2004), a combinatorial optimization problem whose feasible solutions are all clusterings of a graph, and whose objective function is a sum of weights w0, w1 : E → R+0 defined on the edgesE of the graph.",1. Introduction,[0],[0]
"The weightw0e is added to the sum if the nodes {u, v} = e ∈",1. Introduction,[0],[0]
"E are in the same cluster, and the weight w1e is added to the sum if these nodes are in distinct clusters.",1. Introduction,[0],[0]
"The problem consists in finding a clustering of minimum weight.
",1. Introduction,[0],[0]
"Weighted correlation clustering has found applications in the fields of network analysis (Cesa-Bianchi et al., 2012) and, more recently, computer vision (Kappes et al., 2011; Keuper et al., 2015; Insafutdinov et al., 2016; Beier et al., 2017; Tang et al., 2017), partly due to its key property that the number of clusters is not fixed, constrained or penalized in the problem statement but is instead defined by the (any)
1Max Planck Institute for Informatics, Saarbrücken, Germany 2Saarland University, Saarbrücken, Germany 3Bosch Center for AI, Renningen, Germany 4University of Tübingen, Germany.",1. Introduction,[0],[0]
"Correspondence to: Jan-Hendrik Lange <jlange@mpi-inf.mpg.de>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
solution.",1. Introduction,[0],[0]
"Weighted correlation clustering in general graphs is hard to solve exactly and hard to approximate (Demaine et al., 2006).",1. Introduction,[0],[0]
Remarkable progress has been made toward algorithms that find feasible solutions by approximations or heuristics (cf. Section 2).,1. Introduction,[0],[0]
"Yet, the computation of lower bounds remains challenging for large instances (Swoboda & Andres, 2017).
",1. Introduction,[0],[0]
"We make the following contributions: Firstly, in order to reduce instances in size, we establish partial optimality conditions on the graph and weights that can be checked combinatorially in polynomial time and determine the values of some variables in an optimal solution.",1. Introduction,[0],[0]
"By applying these conditions recursively, we reduce an instance in size without restricting the quality of solutions.",1. Introduction,[0],[0]
"For series-parallel graphs, our algorithm solves weighted correlation clustering exactly and in linear time, as we show.",1. Introduction,[0],[0]
"For general graphs, we demonstrate its effectiveness empirically.
",1. Introduction,[0],[0]
"Secondly, in order to compute lower bounds to the optimal objective value efficiently, we define an algorithm that outputs a heuristic solution to a packing problem that is the dual of a reformulation of weighted correlation clustering.",1. Introduction,[0],[0]
"Empirically, this algorithm is shown to exhibit a run-time/tightness trade-off that is different from both the cutting plane algorithm of Kappes et al. (2015) and the message passing algorithm of Swoboda & Andres (2017), both of which solve a canonical linear program relaxation of weighted correlation clustering.
",1. Introduction,[0],[0]
"Thirdly, toward the goal of obtaining primal feasible solutions, we define a transformation of the weights w.r.t.",1. Introduction,[0],[0]
our heuristic solution to the dual problem.,1. Introduction,[0],[0]
This transformation is again a heuristic and is motivated by complementary slackness.,1. Introduction,[0],[0]
"Empirically, local search algorithms are shown to find feasible solutions of lower original weight when applied to instances with transformed weights.
",1. Introduction,[0],[0]
"In the supplementary material, we provide additional results that were omitted from the main paper for the sake of space.",1. Introduction,[0],[0]
Implementations of our algorithms are provided on GitHub.,1. Introduction,[0],[0]
Weighted correlation clustering has a long history in the field of combinatorial optimization.,2. Related Work,[0],[0]
"Grötschel & Wakabayashi (1989) state an equivalent problem for complete graphs and
devise a branch-and-cut algorithm for solving this problem exactly.",2. Related Work,[0],[0]
"The polyhedral geometry of its feasible set is studied by Grötschel & Wakabayashi (1990); Deza et al. (1990; 1992), in the case of general graphs by Chopra & Rao (1993); Chopra (1994) and, for a more general problem, by Horňáková et al. (2017).",2. Related Work,[0],[0]
"For uniform absolute edge costs, Bansal et al. (2004) coined the name correlation clustering, established NP-hardness and the first approximation results.",2. Related Work,[0],[0]
The connection between correlation clustering in general weighted graphs and weighted multicuts was made by Demaine et al. (2006) who thus established APX-hardness and obtained anO(log|V |) approximation algorithm for the problem.,2. Related Work,[0],[0]
"Further hardness results and improved approximation algorithms for particular classes of graphs are due to Charikar et al. (2005); Chawla et al. (2006; 2015); Ailon et al. (2012); Klein et al. (2015).
",2. Related Work,[0],[0]
Approximation algorithms are typically based on rounding the solution of a linear or semi-definite program relaxation.,2. Related Work,[0],[0]
"Due to its importance, tailored algorithms for solving the linear program relaxation more efficiently than standard methods have been proposed by Yarkony et al. (2012; 2015); Swoboda & Andres (2017).",2. Related Work,[0],[0]
"Complementary to these lower bounds, a variety of fast primal heuristics have been developed to tackle large instances (Beier et al., 2014; Pan et al., 2015; Levinkov et al., 2017).",2. Related Work,[0],[0]
"Although it has been observed that, in practice, heuristic solutions are often good, it remains difficult for large instances to determine non-trivial bounds on their optimality gap.
",2. Related Work,[0],[0]
"Partial optimality concepts have been developed and exploited successfully for node labeling problems that arise from pseudo-Boolean optimization and from maximum a-posteriori inference in Markov Random Fields, cf.",2. Related Work,[0],[0]
"(Shekhovtsov, 2014; Swoboda et al., 2016).",2. Related Work,[0],[0]
"Transferring this knowledge to weighted correlation clustering is nontrivial, due to the different nature of the problem.",2. Related Work,[0],[0]
Two partial optimality conditions for weighted correlation clustering are established by Alush & Goldberger (2012) and are here generalized.,2. Related Work,[0],[0]
"Weighted correlation clustering is a combinatorial optimization problem whose feasible solutions are all clusterings of a graph.
",3.1. Weighted Correlation Clustering,[0],[0]
"Let G = (V,E) be a simple graph.",3.1. Weighted Correlation Clustering,[0],[0]
"We call a partition Π of V a clustering if every S ∈ Π induces a connected subgraph (cluster) of G. For any clustering Π of G, we denote by E0Π the set of those edges whose nodes are in the same cluster, and by E1Π the (complementary) set of those edges whose
nodes are in distinct clusters:
E0Π = {uv ∈ E",3.1. Weighted Correlation Clustering,[0],[0]
"| ∃S ∈ Π : u ∈ S and v ∈ S}, (1) E1Π = E",3.1. Weighted Correlation Clustering,[0],[0]
\,3.1. Weighted Correlation Clustering,[0],[0]
"E0Π. (2)
",3.1. Weighted Correlation Clustering,[0],[0]
"The set of edges E1Π is known as the multicut of G that corresponds to the clustering Π.
Definition 1.",3.1. Weighted Correlation Clustering,[0],[0]
"For any graph G = (V,E) and any w0, w1 : E → R+0 , the instance of weighted correlation clustering w.r.t.",3.1. Weighted Correlation Clustering,[0],[0]
"G, w0 and w1 is the optimization problem
min Π ∑ e∈E0Π w0e + ∑ e∈E1Π w1e .",3.1. Weighted Correlation Clustering,[0],[0]
(3),3.1. Weighted Correlation Clustering,[0],[0]
Weighted correlation clustering is commonly stated in the form of a binary program whose feasible solutions are the incidence vectors of the multicuts of the graph.,3.2. Minimum Cost Multicut,[0],[0]
The incidence vector xΠ ∈,3.2. Minimum Cost Multicut,[0],[0]
"{0, 1}E corresponding to the multicut induced by Π is defined as
xΠe = { 1 if e ∈",3.2. Minimum Cost Multicut,[0],[0]
"E1Π 0 else.
",3.2. Minimum Cost Multicut,[0],[0]
"(4)
Definition 2.",3.2. Minimum Cost Multicut,[0],[0]
"For any graph G = (V,E) and any c : E → R, the instance of the minimum cost multicut problem w.r.t.",3.2. Minimum Cost Multicut,[0],[0]
"G and c is the binary program
min Π ∑ e∈E ce x Π e .",3.2. Minimum Cost Multicut,[0],[0]
"(5)
The minimizers of an instance of weighted correlation clustering (Def. 1) coincide with the minimizers of the instance of minimum cost multicut (Def. 2) with c = w1−w0, since
min Π ∑ e∈E0Π w0e + ∑ e∈E1Π w1e (6)
",3.2. Minimum Cost Multicut,[0],[0]
= min Π ∑ e∈E ( w0e (1− xΠe ) +,3.2. Minimum Cost Multicut,[0],[0]
"w1e xΠe ) (7)
= ∑ e∈E
w0e︸ ︷︷ ︸ const.",3.2. Minimum Cost Multicut,[0],[0]
"+ min Π
∑ e∈E (w1e − w0e)︸ ︷︷ ︸ ce xΠe .",3.2. Minimum Cost Multicut,[0],[0]
(8),3.2. Minimum Cost Multicut,[0],[0]
"By taking the convex hull of multicut incidence vectors
MC(G) := conv{xΠ | Π clustering of G}, (9)
the minimum cost multicut problem (Def. 2) can be written as the integer linear programming problem
min x∈MC(G) ∑ e∈E ce xe.",3.3. Linear Program Relaxation,[0],[0]
"(PMC)
The set MC(G) is called multicut polytope of G (Chopra & Rao, 1993).",3.3. Linear Program Relaxation,[0],[0]
"As the minimum cost multicut problem is NP-hard, a full description of the multicut polytope in terms of its facets is impractical.",3.3. Linear Program Relaxation,[0],[0]
"For practical purposes a linear programming (LP) relaxation of PMC is derived as follows.
",3.3. Linear Program Relaxation,[0],[0]
"Denote by C(G) the set of all simple cycles of G. For any cycle C ∈ C(G), we write EC for the edge set of C. It is straight-forward to check the fact that any multicut incidence vector xΠ satisfies the system of linear inequalities
∀C ∈ C(G) ∀f",3.3. Linear Program Relaxation,[0],[0]
"∈ EC : xf ≤ ∑
e∈EC\{f}
xe , (10)
the so-called cycle inequalities (Chopra & Rao, 1993).",3.3. Linear Program Relaxation,[0],[0]
"Therefore, the standard linear programming relaxation is given by the program
min x∈CYC(G) ∑ e∈E ce xe (PCYC)
",3.3. Linear Program Relaxation,[0],[0]
"whose feasible set
CYC(G) :",3.3. Linear Program Relaxation,[0],[0]
= { x ∈,3.3. Linear Program Relaxation,[0],[0]
"[0, 1]E ∣∣x satisfies (10)} (11) is also known as the cycle relaxation of MC(G).",3.3. Linear Program Relaxation,[0],[0]
"The problem PCYC is practical, because the cycle inequalities in (10) can be separated in polynomial time.",3.3. Linear Program Relaxation,[0],[0]
"The lower bounds thus obtained can serve to solve (small) instances of the minimum cost multicut problem by branch-and-cut because the cycle relaxation has no integer vertices except the incidence vectors of multicuts, according to Lemma 1.
",3.3. Linear Program Relaxation,[0],[0]
Lemma 1 (Chopra & Rao (1993)).,3.3. Linear Program Relaxation,[0],[0]
"For any graph G = (V,E), it holds that MC(G) = CYC(G) ∩ ZE .
",3.3. Linear Program Relaxation,[0],[0]
A reference algorithm that we use for the experiments in Section 7 further exploits the fact that a cycle inequality in (10) defines a facet of MC(G) iff the associated cycle is chordless.,3.3. Linear Program Relaxation,[0],[0]
"For the presentation of this paper, we employ an alternative (integer) linear programming formulation in terms of covering cycles, which was similarly considered, e.g., by Demaine et al. (2006) for the combinatorial problem and by Charikar et al. (2005) in connection with the LP relaxation for complete graphs.",3.4. Cycle Covering Formulation,[0],[0]
"We rewrite the feasible set of the general LP relaxation relative to the cost vector c. Therefore, let G and c be fixed.
",3.4. Cycle Covering Formulation,[0],[0]
We call an edge e ∈ E repulsive if ce < 0,3.4. Cycle Covering Formulation,[0],[0]
and we call it attractive if ce > 0.,3.4. Cycle Covering Formulation,[0],[0]
"Note that we may w.l.o.g. remove all edges e ∈ E with ce = 0, since the choice of xe is irrelevant to the objective.",3.4. Cycle Covering Formulation,[0],[0]
"We write E = E+ ∪ E− where E+, E− collect all attractive and repulsive edges, respectively.
",3.4. Cycle Covering Formulation,[0],[0]
We call a cycle of G conflicted w.r.t.,3.4. Cycle Covering Formulation,[0],[0]
"(G, c) if it contains precisely one repulsive edge.",3.4. Cycle Covering Formulation,[0],[0]
"We denote by C−(G, c) ⊆ C(G) the set of all such cycles.
",3.4. Cycle Covering Formulation,[0],[0]
We consider the relaxation of CYC(G) that is constrained only by conflicted cycles.,3.4. Cycle Covering Formulation,[0],[0]
"More specifically, we consider the system
∀C ∈ C−(G, c), f ∈ EC ∩",3.4. Cycle Covering Formulation,[0],[0]
E− :,3.4. Cycle Covering Formulation,[0],[0]
"xf ≤ ∑
e∈EC\{f}
xe
(12)
of only those linear inequalities of (10) for which the edge on the left-hand side is repulsive and all other edges are attractive.",3.4. Cycle Covering Formulation,[0],[0]
"Defining
CYC−(G, c) := { x ∈",3.4. Cycle Covering Formulation,[0],[0]
"[0, 1]E ∣∣ x satisfies (12)} (13) and replacing CYC(G) by CYC−(G, c) in PCYC has no effect on the solutions, due to the following lemma, a weaker form of which was also given by Yarkony et al. (2015).",3.4. Cycle Covering Formulation,[0],[0]
Lemma 2.,3.4. Cycle Covering Formulation,[0],[0]
"For any c : E → R it holds that
min x∈CYC(G)",3.4. Cycle Covering Formulation,[0],[0]
"c>x = min x∈CYC−(G,c) c",3.4. Cycle Covering Formulation,[0],[0]
">x (14)
and
min x∈MC(G)",3.4. Cycle Covering Formulation,[0],[0]
"c>x = min x∈CYC−(G,c)∩ZE c>x. (15)
",3.4. Cycle Covering Formulation,[0],[0]
Proof.,3.4. Cycle Covering Formulation,[0],[0]
Let x∗ be an optimal solution to the right-hand side of (14).,3.4. Cycle Covering Formulation,[0],[0]
We show that x∗ satisfies all cycle inequalities (10) by contradiction.,3.4. Cycle Covering Formulation,[0],[0]
"To this end, suppose there exists a cycle C ∈ C(G) and f ∈ EC such that
x∗f > ∑
e∈EC\{f}
x∗e.
",3.4. Cycle Covering Formulation,[0],[0]
"If any edge g ∈ EC \ {f} is repulsive, then increasing x∗g would lower the objective.",3.4. Cycle Covering Formulation,[0],[0]
"Since x
∗ is optimal, there must be a conflicted cycle C ′",3.4. Cycle Covering Formulation,[0],[0]
"with g ∈ EC′ such that x∗g = ∑ e∈EC′\{g}
x∗e .",3.4. Cycle Covering Formulation,[0],[0]
Note that this means f /∈ EC′ .,3.4. Cycle Covering Formulation,[0],[0]
We write C4C ′ for the cycle obtained from the symmetric difference of EC and EC′ .,3.4. Cycle Covering Formulation,[0],[0]
"Apparently, the cycle C4C ′ has one repulsive edge less and f ∈ EC4C′ .",3.4. Cycle Covering Formulation,[0],[0]
"Therefore, by repeating the argument, we may w.l.o.g. assume that all edges in EC \ {f} are attractive.",3.4. Cycle Covering Formulation,[0],[0]
"Now assume that f is attractive as well, then decreasing x∗f would lower the objective.",3.4. Cycle Covering Formulation,[0],[0]
"Therefore, since x
∗ is optimal, there is a conflicted cycle C ′ with f ∈ EC′ and g ∈ EC′ ∩ E− such that
x∗g = x ∗",3.4. Cycle Covering Formulation,[0],[0]
"f + ∑ e∈EC′\{f,g} x∗e
> ∑
e∈EC\{f}
x∗e + ∑
e∈EC′\{f,g}
x∗e
≥ ∑
e∈EC4C′\{g}
x∗e.
",3.4. Cycle Covering Formulation,[0],[0]
Note that C4C ′ is a conflicted cycle.,3.4. Cycle Covering Formulation,[0],[0]
"Thus, we conclude that x∗ violates an inequality of (12) and hence cannot be feasible.",3.4. Cycle Covering Formulation,[0],[0]
"This concludes the proof of (14), the argument for (15) is analogous.
",3.4. Cycle Covering Formulation,[0],[0]
"With the help of Lemma 2, we formulate PMC as a set covering problem: Definition 3.",3.4. Cycle Covering Formulation,[0],[0]
"For any graph G = (V,E) and any c ∈ RE , we call
min x̂∈SC(G,c) ∑ e∈E |ce| x̂e (PSC)
with SC(G, c) the convex hull of all x̂ ∈ ZE that satisfy the system
∀C ∈ C−(G, c) : ∑ e∈EC",3.4. Cycle Covering Formulation,[0],[0]
"x̂e ≥ 1 (16)
∀e ∈ E : x̂e ≥ 0 (17)
the set covering problem w.r.t.",3.4. Cycle Covering Formulation,[0],[0]
"conflicted cycles, and we call SC(G, c) the set covering polyhedron w.r.t.",3.4. Cycle Covering Formulation,[0],[0]
conflicted cycles.,3.4. Cycle Covering Formulation,[0],[0]
Lemma 3.,3.4. Cycle Covering Formulation,[0],[0]
"For any graph G = (V,E) and any c ∈ RE , we have
min x∈CYC−(G,c)∩ZE ∑ e∈E ce xe
= Ltriv + min x̂∈SC(G,c) ∑ e∈E |ce| x̂e (18)
with
Ltriv = ∑ e∈E− ce (19)
the sum of negative edge costs (a trivial lower bound to the optimal value of PMC).
",3.4. Cycle Covering Formulation,[0],[0]
Proof.,3.4. Cycle Covering Formulation,[0],[0]
We define x̂ via x̂e,3.4. Cycle Covering Formulation,[0],[0]
:= xe for any attractive edge e ∈ E+,3.4. Cycle Covering Formulation,[0],[0]
and x̂e,3.4. Cycle Covering Formulation,[0],[0]
:= 1− xe for any repulsive edge e ∈,3.4. Cycle Covering Formulation,[0],[0]
"E−. Since any conflicted cycle C ∈ C−(G, c) has precisely one repulsive edge, all conflicted cycle inequalities (12) become covering inequalities.",3.4. Cycle Covering Formulation,[0],[0]
"In this section, we study partial optimality for PMC.",4. Partial Optimality,[0],[0]
"More precisely, we establish conditions on an edge e ∈ E which guarantee that xe assumes one particular value, either 0 or 1, in at least one optimal solution (weak persistency).",4. Partial Optimality,[0],[0]
"Fixations to 0 are of particular interest as they can be implemented as edge contractions (with subsequent merging of parallel edges), which effectively reduce the size of a given instance of the problem.",4. Partial Optimality,[0],[0]
"As a corollary, we obtain an algorithm that solves weighted correlation clustering problems on seriesparallel graphs in linear time.",4. Partial Optimality,[0],[0]
A direct consequence from Lemma 3 is that we may disregard all edges that are not contained in any conflicted cycle.,4.1. Basic Conditions,[0],[0]
There are (at least) two ways this can happen: 1.,4.1. Basic Conditions,[0],[0]
"An edge e ∈ E is not contained in any cycle at all, that is, e is a bridge.",4.1. Basic Conditions,[0],[0]
2.,4.1. Basic Conditions,[0],[0]
"The endpoints of a repulsive edge e = {u, v} ∈",4.1. Basic Conditions,[0],[0]
"E− belong to different components of G+ = (V,E+).",4.1. Basic Conditions,[0],[0]
"In both cases, for any optimal solution x∗ of PMC, it holds that x∗e = 0",4.1. Basic Conditions,[0],[0]
"if e is attractive, and x ∗ e",4.1. Basic Conditions,[0],[0]
= 1 if e is repulsive.,4.1. Basic Conditions,[0],[0]
"Thus, we can restrict the instance of the problem to the maximal components ofG that are connected in G+ and biconnected in G. This was also observed by Alush & Goldberger (2012).
",4.1. Basic Conditions,[0],[0]
"Below, we establish more general partial optimality conditions.",4.1. Basic Conditions,[0],[0]
"To this end, we need the following notation.",4.1. Basic Conditions,[0],[0]
"A cut of G is a bipartition B = (S1, S2) of the nodes V , i.e. V = S1 ∪̇S2.",4.1. Basic Conditions,[0],[0]
The edge set of the cut B is denoted by EB = {uv ∈ E,4.1. Basic Conditions,[0],[0]
"| u ∈ S1, v ∈ S2}.",4.1. Basic Conditions,[0],[0]
Definition 4.,4.2. Dominant Edges,[0],[0]
"Let G = (V,E) be any graph and let c ∈ RE .",4.2. Dominant Edges,[0],[0]
An edge f ∈ E is called dominant attractive iff cf > 0,4.2. Dominant Edges,[0],[0]
"and there exists a cut B with f ∈ EB such that
cf ≥ ∑
e∈EB\{f}
|ce| .",4.2. Dominant Edges,[0],[0]
"(20)
An edge f ∈",4.2. Dominant Edges,[0],[0]
E− is called dominant repulsive iff cf < 0,4.2. Dominant Edges,[0],[0]
"and there exists a cut B with f ∈ EB such that
|cf | ≥ ∑
e∈EB∩E+ ce.",4.2. Dominant Edges,[0],[0]
"(21)
",4.2. Dominant Edges,[0],[0]
"An edge is called dominant iff it is dominant attractive or dominant repulsive.
",4.2. Dominant Edges,[0],[0]
Lemma 4.,4.2. Dominant Edges,[0],[0]
"Let G = (V,E) be any graph and let c ∈ RE .
",4.2. Dominant Edges,[0],[0]
(i),4.2. Dominant Edges,[0],[0]
"If f ∈ E is dominant attractive, then x∗f = 0 in at least one optimal solution x∗ of PMC.
(ii) If f ∈ E is dominant repulsive, then x∗f = 1 in at least one optimal solution x∗ of PMC.
",4.2. Dominant Edges,[0],[0]
Proof.,4.2. Dominant Edges,[0],[0]
(i),4.2. Dominant Edges,[0],[0]
We use the set covering formulation of PMC.,4.2. Dominant Edges,[0],[0]
Suppose f ∈ E+ is dominant and x̂∗f = 1 in an optimal solution x̂∗ of PSC.,4.2. Dominant Edges,[0],[0]
"Every conflicted cycle that contains f also contains some edge e ∈ EB , since B is a cut.",4.2. Dominant Edges,[0],[0]
"Therefore, the vector x̂ ∈ {0, 1}E defined by
x̂e =  0",4.2. Dominant Edges,[0],[0]
"if e = f 1 if e ∈ EB , e 6= f x̂∗e else
is a feasible solution to PSC.",4.2. Dominant Edges,[0],[0]
"It has the same objective value as x̂∗, since f is dominant and x̂∗ is optimal.
",4.2. Dominant Edges,[0],[0]
(ii) Suppose f ∈,4.2. Dominant Edges,[0],[0]
E− is dominant and x̂∗f,4.2. Dominant Edges,[0],[0]
= 1 in an optimal solution x̂∗ of PSC,4.2. Dominant Edges,[0],[0]
.,4.2. Dominant Edges,[0],[0]
Every conflicted cycle that contains f also contains some edge e ∈ EB ∩,4.2. Dominant Edges,[0],[0]
"E+, since B is a cut and every conflicted cycle contains only one repulsive edge.",4.2. Dominant Edges,[0],[0]
"Then the vector x̂ ∈ {0, 1}E defined by x̂f = 0, x̂e = 1 for all e ∈ EB ∩ E+ and x̂e = x̂∗e elsewhere is a feasible solution to PSC.",4.2. Dominant Edges,[0],[0]
"It has the same objective value as x̂∗, since f is dominant and x̂∗ is optimal.
",4.2. Dominant Edges,[0],[0]
"Lemma 4 generalizes the basic conditions discussed in Section 4.1, since each edge f ∈ E that is not contained in any conflicted cycle is also dominant.",4.2. Dominant Edges,[0],[0]
"Dominance of edges can be decided in polynomial time, by computing minimum st-cuts in G for a suitable choice of capacities.",4.2. Dominant Edges,[0],[0]
"In practice, the required computational effort may be mitigated by constructing a cut tree of G (Gomory & Hu, 1961).",4.2. Dominant Edges,[0],[0]
"The practically most relevant cuts can even be checked in linear time, which we discuss in the following section.",4.2. Dominant Edges,[0],[0]
"In practice, it is expected that dominant edges are more likely to be found in cuts that are relatively sparse.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"We discuss two special cases of sparse cuts that are of particular interest, due to the following reasons.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"First, they can be checked in linear time, which gives rise to a fast preprocessing algorithm.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"Second, we show that our techniques solve PMC to optimality if G is series-parallel.
",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
Two-edge cuts.,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"Suppose B is a two-edge cut of G, i.e. EB = {e, f} for two edges e, f ∈",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
E.,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"Apparently, according to (20) and (21), at least one of them must be dominant.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"Further, it is guaranteed that we can simplify the instance by edge deletions or contractions.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"To see this, distinguish the following cases.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"If both e and f are repulsive, then both of them are dominant and we can delete them, as they are not contained in any conflicted cycle.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"If f is dominant attractive, we can contract f .",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"Finally, if f is dominant repulsive and e is attractive, then we can switch the signs of their coefficients and redefine xf := 1 − xf as well as xe := 1 − xe.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"Since |EB | = 2, this operation does not change the set of conflicted cycles of G and thus is valid (while only adding a constant to the objective).",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"Afterwards, the edge f is dominant attractive and we can contract f .",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"The two-edge cuts of G can be found in linear time, by computing the 3-edge-connected components of G, cf.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"(Mehlhorn et al., 2017).
",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
Single-node cuts.,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"For any v ∈ V , let Bv = ({v}, V \ {v}) denote the cut that is induced by v. Whether EBv contains a dominant edge is easily decided by considering all edges incident to v. Moreover, if deg v = 2, then Bv is also a two-edge cut and we can apply the operation described in
Algorithm 1 Single-Node Cut Preprocessing input G = (V,E), c :",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"E → R
1: Initialize objective value offset ∆ = 0.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
2: Initialize a queue Q = V .,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
3: while Q 6= ∅,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
do 4: Extract a vertex v ∈ Q. 5: if deg v = 1 then 6: Get neighbor u ∈ V .,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"7: if cuv ≥ 0 then 8: Set xuv = 0 and contract uv ∈ E. 9: else
10: Set xuv = 1, ∆ = ∆ + cuv and delete uv ∈ E. 11: end if 12: else if deg v = 2 then 13: Get neighbors u,w ∈ V with |cuv| ≥ |cwv|.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
14: if uv ∈ E+ then 15: Set xuv = 0 and contract uv ∈ E. 16: else if uv ∈,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
E− and wv ∈,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
E− then 17: Adjust offset ∆ = ∆ + cuv + cwv .,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"18: Set xuv = xwv = 1 and delete uv,wv ∈",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
E. 19: else if uv ∈,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
E− and wv ∈ E+ then 20:,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
Adjust offset ∆ = ∆ + cuv + cwv .,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"21: Redefine xuv = 1− xuv , xwv = 1− xwv and cuv = −cuv , cwv = −cwv .",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
22: Set xuv = 0 and contract uv ∈ E. 23: end if 24: else if ∃f ∈ Bv dominant attractive then 25: Set xf = 0 and contract f ∈,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
E. 26: end if 27:,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
Add to Q all vertices u /∈,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
Q whose neighborhood was changed.,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"28: end while 29: return (G, c), x,∆
the last paragraph.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
Updating the graph and applying these techniques recursively as specified in Algorithm 1 takes linear time.,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"This has the following theoretical consequence.
",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
Corollary 1.,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"If G has treewidth at most 2, then Algorithm 1 can be implemented to solve PMC exactly in O(|V |) time.
",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
Proof.,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
Place the vertices of G into buckets of ascending degree and always pick a vertex of minimal degree.,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
Every graph of treewidth 2 has a vertex v with deg v ≤ 2.,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"Since Algorithm 1 only contracts or deletes edges, fixing the variables according to Lemma 4, the updated graph still has treewidth at most 2.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"The number of nodes decreases by 1 in every iteration, hence the algorithm terminates in time O(|E|) = O(|V |) and outputs an optimal solution.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"In this section, we define an algorithm for computing lower bounds for PMC.",5. Dual Lower Bounds,[0],[0]
"This algorithm exploits the structure of
Algorithm 2 Iterative Cycle Packing (ICP) input G = (V,E), c :",5. Dual Lower Bounds,[0],[0]
"E → R
1: Initialize we = |ce| for all e ∈ E and y = 0, L = Ltriv. 2: for ` = 3 . . .",5. Dual Lower Bounds,[0],[0]
"|E| do 3: while ∃C ∈ C−(G, c) : |EC | ≤ ` do 4: Pick C ∈ C−(G, c) such that |EC | ≤",5. Dual Lower Bounds,[0],[0]
"`. 5: Compute yC = mine∈EC we.
6: Redefine we = { we − yC",5. Dual Lower Bounds,[0],[0]
if e ∈ EC we else.,5. Dual Lower Bounds,[0],[0]
7: Increase lower bound L = L+ yC .,5. Dual Lower Bounds,[0],[0]
"8: Remove all edges e ∈ E with we = 0 from G. 9: end while
10: if C−(G, c) = ∅ then 11: return y, L 12: end if 13: end for
the reformulation PSC.",5. Dual Lower Bounds,[0],[0]
"It computes a heuristic solution to the dual of its LP relaxation.
",5. Dual Lower Bounds,[0],[0]
"The LP relaxation (up to the constant Ltriv) of problem PSC is given by
min ∑ e∈E |ce|x̂e (22)
subject to ∑ e∈EC x̂e ≥ 1 ∀C ∈ C−(G, c) (23)
",5. Dual Lower Bounds,[0],[0]
"x̂e ≥ 0 ∀e ∈ E .
",5. Dual Lower Bounds,[0],[0]
"The corresponding dual program reads
max ∑
C∈C−(G,c)
yC (24)
subject to ∑
C: e∈EC
yC ≤ |ce| ∀e ∈ E (25)
yC ≥ 0 ∀C ∈ C−(G, c) .
",5. Dual Lower Bounds,[0],[0]
"A heuristic solution of (24), and thus a lower bound for (22), is found by Algorithm 2 that we call Iterative Cycle Packing (ICP).",5. Dual Lower Bounds,[0],[0]
It works as follows:,5. Dual Lower Bounds,[0],[0]
"Firstly, it chooses a conflicted cycle C and increases yC as much as possible.",5. Dual Lower Bounds,[0],[0]
"Secondly, it decreases the weights we (initially |ce|) of all edges e ∈ EC by yC and removes all edges of zero weight.",5. Dual Lower Bounds,[0],[0]
"These steps are repeated until there are no conflicted cycles left.
",5. Dual Lower Bounds,[0],[0]
Implementation details.,5. Dual Lower Bounds,[0],[0]
"The absolute running time of ICP as well as the quality of the output lower bounds depends on the choice of cycles C. We pursue the following strategy that we found to perform well empirically in both aspects: In each iteration of the main loop, we choose a repulsive edge e = uv ∈",5. Dual Lower Bounds,[0],[0]
"E− such that u and v are in the same connected component of G+ = (V,E+).",5. Dual Lower Bounds,[0],[0]
"Then, we find a conflicted cycle containing e by searching for a shortest path
(in terms of hop distance) from u to v in G+.",5. Dual Lower Bounds,[0],[0]
"We apply this search for conflicted cycles in rounds of increasing cycle length, using breadth-first search with an early termination criterion based on the hop distance.",5. Dual Lower Bounds,[0],[0]
We also maintain and periodically update a component labeling of G+ in order to to reduce the number of redundant shortest path searches.,5. Dual Lower Bounds,[0],[0]
"In this section, we exploit the dual solution in primal algorithms.",6. Re-weighting for Primal Algorithms,[0],[0]
"The motivation is due to complementary slackness, which is made explicit in the following lemma.
",6. Re-weighting for Primal Algorithms,[0],[0]
Lemma 5.,6. Re-weighting for Primal Algorithms,[0],[0]
"Assume the primal LP (22) is tight, i.e., its optimal solution x̂∗ also solves PSC, and the solution output by ICP solves the dual (24) optimally.",6. Re-weighting for Primal Algorithms,[0],[0]
"Then, for every e ∈ E with positive residual weight we > 0, it holds that x̂∗e = 0.
",6. Re-weighting for Primal Algorithms,[0],[0]
Proof.,6. Re-weighting for Primal Algorithms,[0],[0]
"If we > 0, the constraint (25) at e ∈ E is inactive at the optimal dual solution.",6. Re-weighting for Primal Algorithms,[0],[0]
"Thus, x̂∗e = 0 in the optimal primal solution, by complementary slackness.
",6. Re-weighting for Primal Algorithms,[0],[0]
"Of course, the assumption of Lemma 5 is too strong for practical purposes.",6. Re-weighting for Primal Algorithms,[0],[0]
"However, the intuition is that if the LP relaxation is fairly tight and the obtained dual solution is close to optimal, it can still provide useful information about the primal problem.",6. Re-weighting for Primal Algorithms,[0],[0]
"More specifically, the weights we output by ICP can be interpreted as an indication of how likely the primal variable x̂e is zero in an optimal solution.",6. Re-weighting for Primal Algorithms,[0],[0]
"In order to make use of this information, we propose to shift the weights of the primal problem to a convex combination λ|ce|+ (1− λ)we of the original and residual weights, for a suitable choice of λ ∈ (0, 1).",6. Re-weighting for Primal Algorithms,[0],[0]
Experiments in Section 7 show that this shift can guide primal heuristics toward better feasible solutions to the original problem.,6. Re-weighting for Primal Algorithms,[0],[0]
"In this section, we study partial optimality, dual lower bounds and re-weightings empirically, for all instances of
the weighted correlation clustering problem from Kappes et al. (2015) and Leskovec et al. (2010).
Instances.",7. Experiments,[0],[0]
"From Kappes et al. (2015), we consider all three collections of instances: Image Segmentation contains instances w.r.t.",7. Experiments,[0],[0]
planar superpixel adjacency graphs of photographs.,7. Experiments,[0],[0]
Knott-3D contains instances w.r.t.,7. Experiments,[0],[0]
non-planar supervoxel adjacency graphs of volume images taken by a serial sectioning electron microscope.,7. Experiments,[0],[0]
Modularity Clustering contains instances w.r.t.,7. Experiments,[0],[0]
complete graphs.,7. Experiments,[0],[0]
"In all three collections, the edge costs ce are fractional and non-uniform.",7. Experiments,[0],[0]
"For all these instances, except one in the collection Modularity Clustering, optimal solutions are accessible and are computed here as a reference.",7. Experiments,[0],[0]
"From Leskovec et al. (2010), we consider directed graphs of the social networks Epinions and Slashdot, each with more than half a million edges labeled either +1 or −1.",7. Experiments,[0],[0]
"Instances of the minimum cost multicut problem are defined here by removing the orientation of edges, by deleting all self-loops, and by replacing parallel edges by a single edge with the sum of their costs1.",7. Experiments,[0],[0]
"In order to study the partial optimality conditions of Section 4 empirically, we process the above instances as follows: First, we remove all edges of cost 0, all bridges, as well as all repulsive edges whose nodes belong to distinct connected components of G+.",7.1. Partial Optimality,[0],[0]
"Second, we check for every v ∈ V whether the cut Bv = ({v}, V \ {v}) induces dominant edges.",7.1. Partial Optimality,[0],[0]
"If we find dominant attractive edges or vertices of degree ≤ 2, we perform contractions and deletions according to Alg. 1.",7.1. Partial Optimality,[0],[0]
"Both steps are repeated until no further edges can be removed or contracted.
",7.1. Partial Optimality,[0],[0]
"After the main reduction step, which takes linear time and is thus very fast, we further check all remaining edges uv ∈ E for dominance in any (general) uv-cut.",7.1. Partial Optimality,[0],[0]
"To this end, we construct a cut tree of G with the help of Gusfield’s algorithm (Gusfield, 1990), which takes |V | − 1 max-flow computations.",7.1. Partial Optimality,[0],[0]
"Despite the increased computational effort, we only found a small number of additional dominant attractive edges and thus could only perform few further contractions.",7.1. Partial Optimality,[0],[0]
"However, we found a significant number of additional dom-
1This results in 2703 edges of cost 0 for Epinions, and 1949 such edges for Slashdot.
inant repulsive edges.
",7.1. Partial Optimality,[0],[0]
The effect of our method in the total number of nodes and edges is shown in Table 1.,7.1. Partial Optimality,[0],[0]
We also report the number of remaining edges that are not dominant repulsive.,7.1. Partial Optimality,[0],[0]
It can be seen from this table that the numbers are effectively reduced.,7.1. Partial Optimality,[0],[0]
"This is explained, firstly, by the sparsity of the graphs and, secondly, by the non-uniformity of the costs.",7.1. Partial Optimality,[0],[0]
"From the comparison to the number of remaining non-persistent variables when only the criteria of Alush & Goldberger (2012) are applied, it can be seen that our more general criteria reveal considerably more persistency.
",7.1. Partial Optimality,[0],[0]
It may be expected that optimization methods benefit in terms of runtime from the reduced size of the instances.,7.1. Partial Optimality,[0],[0]
"On the instances of Kappes et al. (2015), we found the effect to be insignificant due to their small original size.",7.1. Partial Optimality,[0],[0]
"On Epinions and Slashdot, however, the runtime of the local search algorithm GAEC+KLj (cf. Section 7.3) decreased by more than 70%.",7.1. Partial Optimality,[0],[0]
"For completeness, we provide the numbers in the supplements.",7.1. Partial Optimality,[0],[0]
"In order to put into perspective the dual lower bounds output by Iterative Cycle Packing (ICP) as described in Section 5, we compare this algorithm, firstly, to the cutting plane algorithm for PCYC of Kappes et al. (2015), with Gurobi for solving the LPs (denoted here by LP) and, secondly, to the message passing algorithm of Swoboda & Andres (2017), applied to PCYC, with code and parameter settings kindly provided by the authors (denoted here by MPC).
",7.2. Dual Lower Bounds,[0],[0]
Results are shown in Figure 1 and Table 2.,7.2. Dual Lower Bounds,[0],[0]
"It can be seen from the figure and the table that, for the large and hard instances Epinions and Slashdot, ICP converges at under 102 seconds, outputting lower bounds that are matched and exceeded by MPC at around 103 seconds.",7.2. Dual Lower Bounds,[0],[0]
"It can be seen from Table 2 that the situation is similar for the smaller instances: The lower bounds output by ICP are a bit worse than those output by LP or MPC (here compared to the best optimal solution known) but are obtained faster (by as much as three orders of magnitude for Knott-3D-450).
",7.2. Dual Lower Bounds,[0],[0]
"It is known from Kappes et al. (2015) that their instances can be solved faster than their LP relaxations by means of branch-and-cut, separating only integer infeasible points
·105
Epinions
·105
Slashdot
by cycle inequalities using BFS (instead of Dijkstra’s algorithm), and resorting to the strong (undisclosed) cuts of Gurobi for cutting off fractional solutions.",7.2. Dual Lower Bounds,[0],[0]
We restrict our comparison here to algorithms that seek to solve the LP relaxation PCYC.,7.2. Dual Lower Bounds,[0],[0]
This is justified by the fact that size ultimately renders integer linear programming intractable.,7.2. Dual Lower Bounds,[0],[0]
We conclude that ICP is capable of computing non-trivial lower bounds fast.,7.2. Dual Lower Bounds,[0],[0]
"In order to study the re-weighting described in Section 6, we measure its effect on heuristic algorithms for finding feasible solutions.",7.3. Re-weighting,[0],[0]
"To this end, we employ the implementations of Levinkov et al. (2017) of Greedy Additive Edge Contraction (GAEC), an algorithm that starts from singleton clusters and greedily contracts attractive edges with maximum nonnegative cost, and of KLj, the well-known Kernighan-Lin heuristic for graph partitioning that recursively improves an initial clustering by splitting, merging or exchanging nodes between neighboring clusters.
",7.3. Re-weighting,[0],[0]
"A comparison between the feasible solutions found by applying the heuristics GAEC and GAEC+KLj to original instances, on the one hand, and to instances re-weighted by ICP with λ = 12 , on the other hand, can be found in Table
3.",7.3. Re-weighting,[0],[0]
"Note that we only re-weight the input to GAEC and let KLj run with original weights, starting from the solution returned by GAEC, as we found this approach to be advantageous.",7.3. Re-weighting,[0],[0]
It can be seen from Table 3 that our re-weighting consistently improves the gap.,7.3. Re-weighting,[0],[0]
"On average, it is slightly less effective than the reparameterization with the more accurate dual solutions obtained from MPC, as proposed by Swoboda & Andres (2017).",7.3. Re-weighting,[0],[0]
A more detailed comparison is provided in the supplements.,7.3. Re-weighting,[0],[0]
"We have established partial optimality conditions, a heuristic lower bound and a heuristic re-weighting for instances of the weighted correlation clustering problem.",8. Conclusion,[0],[0]
We have shown advantages of each of these constructions empirically.,8. Conclusion,[0],[0]
Checking a subset of our partial optimality conditions recursively gives a fast combinatorial algorithm that efficiently reduces the size of problem instances.,8. Conclusion,[0],[0]
"Conceptually, it solves the problem for series-parallel graphs to optimality, in linear time.",8. Conclusion,[0],[0]
Our dual heuristic algorithm provides nontrivial lower bounds and valuable dual information fast.,8. Conclusion,[0],[0]
"For future work, it is relevant to examine if more sophisticated dual solvers such as MPC benefit from a “warm-start” that transforms and exploits the heuristic dual solution.",8. Conclusion,[0],[0]
Weighted correlation clustering is hard to solve and hard to approximate for general graphs.,abstractText,[0],[0]
Its applications in network analysis and computer vision call for efficient algorithms.,abstractText,[0],[0]
"To this end, we make three contributions: We establish partial optimality conditions that can be checked efficiently, and doing so recursively solves the problem for series-parallel graphs to optimality, in linear time.",abstractText,[0],[0]
"We exploit the packing dual of the problem to compute a heuristic, but non-trivial lower bound faster than that of a canonical linear program relaxation.",abstractText,[0],[0]
We introduce a re-weighting with the dual solution by which efficient local search algorithms converge to better feasible solutions.,abstractText,[0],[0]
The effectiveness of our methods is demonstrated empirically on a number of benchmark instances.,abstractText,[0],[0]
Partial Optimality and Fast Lower Bounds for Weighted Correlation Clustering,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3719–3728 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3719",text,[0],[0]
Many interpretation methods for neural networks explain the model’s prediction as a counterfactual: how does the prediction change when the input is modified?,1 Introduction,[0],[0]
"Adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015) highlight the instability of neural network predictions by showing how small perturbations to the input dramatically change the output.
",1 Introduction,[0],[0]
"A common, non-adversarial form of model interpretation is feature attribution: features that are crucial for predictions are highlighted in a heatmap.",1 Introduction,[0],[0]
One can measure a feature’s importance by input perturbation.,1 Introduction,[0],[0]
"Given an input for text classification, a word’s importance can be measured by the difference in model confidence before and after that word is removed from the input—the word is important if confidence decreases significantly.",1 Introduction,[0],[0]
"This is the leave-one-out method (Li et al., 2016b).",1 Introduction,[0],[0]
"Gradients can also measure feature importance; for example, a feature is influential to the prediction if its gradient is a large positive value.",1 Introduction,[0],[0]
"Both perturbation and gradient-based methods can generate heatmaps, implying that the model’s prediction is highly influenced by the highlighted, important words.
",1 Introduction,[0],[0]
"Instead, we study how the model’s prediction is influenced by the unimportant words.",1 Introduction,[0],[0]
"We use input reduction, a process that iteratively removes the unimportant words from the input while maintaining the model’s prediction.",1 Introduction,[0],[0]
"Intuitively, the words remaining after input reduction should be important for prediction.",1 Introduction,[0],[0]
"Moreover, the words
should match the leave-one-out method’s selections, which closely align with human perception (Li et al., 2016b; Murdoch et al., 2018).",1 Introduction,[0],[0]
"However, rather than providing explanations of the original prediction, our reduced examples more closely resemble adversarial examples.",1 Introduction,[0],[0]
"In Figure 1, the reduced input is meaningless to a human but retains the same model prediction with high confidence.",1 Introduction,[0],[0]
"Gradient-based input reduction exposes pathological model behaviors that contradict what one expects based on existing interpretation methods.
",1 Introduction,[0],[0]
"In Section 2, we construct more of these counterintuitive examples by augmenting input reduction with beam search and experiment with three tasks: SQUAD (Rajpurkar et al., 2016) for reading comprehension, SNLI (Bowman et al., 2015) for textual entailment, and VQA (Antol et al., 2015) for visual question answering.",1 Introduction,[0],[0]
Input reduction with beam search consistently reduces the input sentence to very short lengths—often only one or two words—without lowering model confidence on its original prediction.,1 Introduction,[0],[0]
"The reduced examples appear nonsensical to humans, which we verify with crowdsourced experiments.",1 Introduction,[0],[0]
"In Section 3, we draw connections to adversarial examples and confidence calibration; we explain why the observed pathologies are a consequence of the overconfidence of neural models.",1 Introduction,[0],[0]
This elucidates limitations of interpretation methods that rely on model confidence.,1 Introduction,[0],[0]
"In Section 4, we encourage high model uncertainty on reduced examples with entropy regularization.",1 Introduction,[0],[0]
"The pathological model behavior under input reduction is mitigated, leading to more reasonable reduced examples.",1 Introduction,[0],[0]
"To explain model predictions using a set of important words, we must first define importance.",2 Input Reduction,[0],[0]
"After defining input perturbation and gradient-based approximation, we describe input reduction with these importance metrics.",2 Input Reduction,[0],[0]
Input reduction drastically shortens inputs without causing the model to change its prediction or significantly decrease its confidence.,2 Input Reduction,[0],[0]
Crowdsourced experiments confirm that reduced examples appear nonsensical to humans: input reduction uncovers pathological model behaviors.,2 Input Reduction,[0],[0]
"Ribeiro et al. (2016) and Li et al. (2016b) define importance by seeing how confidence changes when a feature is removed; a natural approximation is to use the gradient (Baehrens et al., 2010; Simonyan et al., 2014).",2.1 Importance from Input Gradient,[0],[0]
We formally define these importance metrics in natural language contexts and introduce the efficient gradient-based approximation.,2.1 Importance from Input Gradient,[0],[0]
"For each word in an input sentence, we measure its importance by the change in the confidence of the original prediction when we remove that word from the sentence.",2.1 Importance from Input Gradient,[0],[0]
"We switch the sign so that when the confidence decreases, the importance value is positive.
",2.1 Importance from Input Gradient,[0],[0]
"Formally, let x = 〈x1, x2, . . .",2.1 Importance from Input Gradient,[0],[0]
"xn〉 denote the input sentence, f(y |x)",2.1 Importance from Input Gradient,[0],[0]
"the predicted probability of label y, and y = argmaxy′ f(y
′ |x)",2.1 Importance from Input Gradient,[0],[0]
the original predicted label.,2.1 Importance from Input Gradient,[0],[0]
"The importance is then
g(xi",2.1 Importance from Input Gradient,[0],[0]
| x) = f(y |x)− f(y |x−i).,2.1 Importance from Input Gradient,[0],[0]
"(1)
To calculate the importance of each word in a sentence with n words, we need n forward passes of the model, each time with one of the words left out.",2.1 Importance from Input Gradient,[0],[0]
"This is highly inefficient, especially for longer sentences.",2.1 Importance from Input Gradient,[0],[0]
"Instead, we approximate the importance value with the input gradient.",2.1 Importance from Input Gradient,[0],[0]
"For each word in the sentence, we calculate the dot product of its word embedding and the gradient of the output with respect to the embedding.",2.1 Importance from Input Gradient,[0],[0]
The importance of n words can thus be computed with a single forward-backward pass.,2.1 Importance from Input Gradient,[0],[0]
"This gradient approximation has been used for various interpretation methods for natural language classification models (Li et al., 2016a; Arras et al., 2016); see Ebrahimi et al. (2017) for further details on the derivation.",2.1 Importance from Input Gradient,[0],[0]
We use this approximation in all our experiments as it selects the same words for removal as an exhaustive search (no approximation).,2.1 Importance from Input Gradient,[0],[0]
Instead of looking at the words with high importance values—what interpretation methods commonly do—we take a complementary approach and study how the model behaves when the supposedly unimportant words are removed.,2.2 Removing Unimportant Words,[0],[0]
"Intuitively, the important words should remain after the unimportant ones are removed.
",2.2 Removing Unimportant Words,[0],[0]
Our input reduction process iteratively removes the unimportant words.,2.2 Removing Unimportant Words,[0],[0]
"At each step, we remove the word with the lowest importance value until the model changes its prediction.",2.2 Removing Unimportant Words,[0],[0]
"We experi-
ment with three popular datasets: SQUAD (Rajpurkar et al., 2016) for reading comprehension, SNLI (Bowman et al., 2015) for textual entailment, and VQA (Antol et al., 2015) for visual question answering.",2.2 Removing Unimportant Words,[0],[0]
"We describe each of these tasks and the model we use below, providing full details in the Supplement.
",2.2 Removing Unimportant Words,[0],[0]
"In SQUAD, each example is a context paragraph and a question.",2.2 Removing Unimportant Words,[0],[0]
The task is to predict a span in the paragraph as the answer.,2.2 Removing Unimportant Words,[0],[0]
We reduce only the question while keeping the context paragraph unchanged.,2.2 Removing Unimportant Words,[0],[0]
"The model we use is the DRQA Document Reader (Chen et al., 2017).
",2.2 Removing Unimportant Words,[0],[0]
"In SNLI, each example consists of two sentences: a premise and a hypothesis.",2.2 Removing Unimportant Words,[0],[0]
"The task is to predict one of three relationships: entailment, neutral, or contradiction.",2.2 Removing Unimportant Words,[0],[0]
We reduce only the hypothesis while keeping the premise unchanged.,2.2 Removing Unimportant Words,[0],[0]
"The model we use is Bilateral Multi-Perspective Matching (BIMPM) (Wang et al., 2017).
",2.2 Removing Unimportant Words,[0],[0]
"In VQA, each example consists of an image and a natural language question.",2.2 Removing Unimportant Words,[0],[0]
We reduce only the question while keeping the image unchanged.,2.2 Removing Unimportant Words,[0],[0]
"The model we use is Show, Ask, Attend, and Answer (Kazemi and Elqursh, 2017).
",2.2 Removing Unimportant Words,[0],[0]
"During the iterative reduction process, we ensure that the prediction does not change (exact same span for SQUAD); consequently, the model accuracy on the reduced examples is identical to the original.",2.2 Removing Unimportant Words,[0],[0]
The predicted label is used for input reduction and the ground-truth is never revealed.,2.2 Removing Unimportant Words,[0],[0]
"We use the validation set for all three tasks.
",2.2 Removing Unimportant Words,[0],[0]
Most reduced inputs are nonsensical to humans (Figure 2) as they lack information for any reasonable human prediction.,2.2 Removing Unimportant Words,[0],[0]
"However, models make confident predictions, at times even more confident than the original.
",2.2 Removing Unimportant Words,[0],[0]
"To find the shortest possible reduced inputs (potentially the most meaningless), we relax the requirement of removing only the least important word and augment input reduction with beam search.",2.2 Removing Unimportant Words,[0],[0]
"We limit the removal to the k least important words, where k is the beam size, and decrease the beam size as the remaining input is shortened.1",2.2 Removing Unimportant Words,[0],[0]
We empirically select beam size five as it produces comparable results to larger beam sizes with reasonable computation cost.,2.2 Removing Unimportant Words,[0],[0]
"The requirement of maintaining model prediction is unchanged.
",2.2 Removing Unimportant Words,[0],[0]
"1We set beam size to max(1,min(k, L − 3))",2.2 Removing Unimportant Words,[0],[0]
"where k is maximum beam size and L is the current length of the input sentence.
",2.2 Removing Unimportant Words,[0],[0]
"With beam search, input reduction finds extremely short reduced examples with little to no decrease in the model’s confidence on its original predictions.",2.2 Removing Unimportant Words,[0],[0]
Figure 3 compares the length of input sentences before and after the reduction.,2.2 Removing Unimportant Words,[0],[0]
"For all three tasks, we can often reduce the sentence to only one word.",2.2 Removing Unimportant Words,[0],[0]
Figure 4 compares the model’s confidence on original and reduced inputs.,2.2 Removing Unimportant Words,[0],[0]
"On SQUAD and SNLI the confidence decreases slightly, and on VQA the confidence even increases.",2.2 Removing Unimportant Words,[0],[0]
"On the reduced examples, the models retain their original predictions despite short input lengths.",2.3 Humans Confused by Reduced Inputs,[0],[0]
"The following experiments examine whether these predictions are justified or pathological, based on how humans react to the reduced inputs.
",2.3 Humans Confused by Reduced Inputs,[0],[0]
"For each task, we sample 200 examples that are correctly classified by the model and generate their reduced examples.",2.3 Humans Confused by Reduced Inputs,[0],[0]
"In the first setting, we compare the human accuracy on original and reduced examples.",2.3 Humans Confused by Reduced Inputs,[0],[0]
"We recruit two groups of crowd workers and task them with textual entailment, reading comprehension, or visual question answering.",2.3 Humans Confused by Reduced Inputs,[0],[0]
We show one group the original inputs and the other the reduced.,2.3 Humans Confused by Reduced Inputs,[0],[0]
"Humans are no longer able to give
the correct answer, showing a significant accuracy loss on all three tasks (compare Original and Reduced in Table 1).
",2.3 Humans Confused by Reduced Inputs,[0],[0]
The second setting examines how random the reduced examples appear to humans.,2.3 Humans Confused by Reduced Inputs,[0],[0]
"For each of the original examples, we generate a version where words are randomly removed until the length matches the one generated by input reduction.",2.3 Humans Confused by Reduced Inputs,[0],[0]
We present the original example along with the two reduced examples and ask crowd workers their preference between the two reduced ones.,2.3 Humans Confused by Reduced Inputs,[0],[0]
"The workers’ choice is almost fifty-fifty (the vs. Random in Table 1): the reduced examples appear almost random to humans.
",2.3 Humans Confused by Reduced Inputs,[0],[0]
These results leave us with two puzzles: why are the models highly confident on the nonsensical reduced examples?,2.3 Humans Confused by Reduced Inputs,[0],[0]
"And why, when the leave-oneout method selects important words that appear reasonable to humans, the input reduction process selects ones that are nonsensical?",2.3 Humans Confused by Reduced Inputs,[0],[0]
"Having established the incongruity of our definition of importance vis-à-vis human judgements, we now investigate possible explanations for these results.",3 Making Sense of Reduced Inputs,[0],[0]
We explain why model confidence can empower methods such as leave-one-out to generate reasonable interpretations but also lead to pathologies under input reduction.,3 Making Sense of Reduced Inputs,[0],[0]
We attribute these results to two issues of neural models.,3 Making Sense of Reduced Inputs,[0],[0]
"Neural models are overconfident in their predictions (Guo et al., 2017).",3.1 Model Overconfidence,[0],[0]
One explanation for overconfidence is overfitting: the model overfits the negative log-likelihood loss during training by learning to output low-entropy distributions over classes.,3.1 Model Overconfidence,[0],[0]
Neural models are also overconfident on examples outside the training data distribution.,3.1 Model Overconfidence,[0],[0]
"As Goodfellow et al. (2015) observe for image classification, samples from pure noise can sometimes trigger highly confident predictions.",3.1 Model Overconfidence,[0],[0]
"These socalled rubbish examples are degenerate inputs that
a human would trivially classify as not belonging to any class but for which the model predicts with high confidence.",3.1 Model Overconfidence,[0],[0]
Goodfellow et al. (2015) argue that the rubbish examples exist for the same reason that adversarial examples do: the surprising linear nature of neural models.,3.1 Model Overconfidence,[0],[0]
"In short, the confidence of a neural model is not a robust estimate of its prediction uncertainty.
",3.1 Model Overconfidence,[0],[0]
"Our reduced inputs satisfy the definition of rubbish examples: humans have a hard time making predictions based on the reduced inputs (Table 1), but models make predictions with high confidence (Figure 4).",3.1 Model Overconfidence,[0],[0]
"Starting from a valid example, input reduction transforms it into a rubbish example.
",3.1 Model Overconfidence,[0],[0]
"The nonsensical, almost random results are best explained by looking at a complete reduction path (Figure 5).",3.1 Model Overconfidence,[0],[0]
"In this example, the transition from valid to rubbish happens immediately after the first step: following the removal of “Broncos”, humans can no longer determine which team the question is asking about, but model confidence remains high.",3.1 Model Overconfidence,[0],[0]
"Not being able to lower its confidence on rubbish examples—as it is not trained to do so— the model neglects “Broncos” and eventually the process generates nonsensical results.
",3.1 Model Overconfidence,[0],[0]
"In this example, the leave-one-out method will not highlight “Broncos”.",3.1 Model Overconfidence,[0],[0]
"However, this is not a failure of the interpretation method but of the model itself.",3.1 Model Overconfidence,[0],[0]
"The model assigns a low importance to “Broncos” in the first step, causing it to be removed—leave-one-out would be able to expose this particular issue by not highlighting “Broncos”.",3.1 Model Overconfidence,[0],[0]
"However, in cases where a similar issue only appear after a few unimportant words are removed, the leave-one-out method would fail to expose the unreasonable model behavior.
",3.1 Model Overconfidence,[0],[0]
Input reduction can expose deeper issues of model overconfidence and stress test a model’s uncertainty estimation and interpretability.,3.1 Model Overconfidence,[0],[0]
"So far, we have seen that the output of a neural model is sensitive to small changes in its input.",3.2 Second-order Sensitivity,[0],[0]
"We call this first-order sensitivity, because interpretation based on input gradient is a first-order Taylor expansion of the model near the input (Simonyan et al., 2014).",3.2 Second-order Sensitivity,[0],[0]
"However, the interpretation also shifts drastically with small input changes (Figure 6).",3.2 Second-order Sensitivity,[0],[0]
"We call this second-order sensitivity.
",3.2 Second-order Sensitivity,[0],[0]
"The shifting heatmap suggests a mismatch between the model’s first- and second-order sensi-
tivities.",3.2 Second-order Sensitivity,[0],[0]
"The heatmap shifts when, with respect to the removed word, the model has low first-order sensitivity but high second-order sensitivity.
",3.2 Second-order Sensitivity,[0],[0]
Similar issues complicate comparable interpretation methods for image classification models.,3.2 Second-order Sensitivity,[0],[0]
"For example, Ghorbani et al. (2017) modify image inputs so the highlighted features in the interpretation change while maintaining the same prediction.",3.2 Second-order Sensitivity,[0],[0]
"To achieve this, they iteratively modify the input to maximize changes in the distribution of feature importance.",3.2 Second-order Sensitivity,[0],[0]
"In contrast, the shifting heatmap we observe occurs by only removing the least impactful features without a targeted optimization.",3.2 Second-order Sensitivity,[0],[0]
They also speculate that the steepest gradient direction for the first- and secondorder sensitivity values are generally orthogonal.,3.2 Second-order Sensitivity,[0],[0]
"Loosely speaking, the shifting heatmap suggests that the direction of the smallest gradient value can sometimes align with very steep changes in second-order sensitivity.
",3.2 Second-order Sensitivity,[0],[0]
"When explaining individual model predictions, the heatmap suggests that the prediction is made based on a weighted combination of words, as in a linear model, which is not true unless the model is indeed taking a weighted sum such as in a DAN (Iyyer et al., 2015).",3.2 Second-order Sensitivity,[0.9556589630410603],"['tractable alternative to Thompson sampling as it works in the regression-oracle model we consider here, but it does not have a theoretical analysis.5 Note that the LinUCB algorithm (Chu et al., 2011; AbbasiYadkori et al., 2011), which is a natural baseline as well, coincides with our Algorithm 2 (with a linear oracle), so we only plot the performance of RegCB with a linear oracle.']"
"When the model composes representations by a non-linear combination of words, a linear interpretation oblivious to second-order sensitivity can be misleading.",3.2 Second-order Sensitivity,[0],[0]
The previous section explains the observed pathologies from the perspective of overconfidence: models are too certain on rubbish examples when they should not make any prediction.,4 Mitigating Model Pathologies,[0],[0]
Human experiments in Section 2.3 confirm that the reduced examples fit the definition of rubbish examples.,4 Mitigating Model Pathologies,[0],[0]
"Hence, a natural way to mitigate the pathologies is to maximize model uncertainty on the reduced examples.",4 Mitigating Model Pathologies,[0],[0]
"To maximize model uncertainty on reduced examples, we use the entropy of the output distribution as an objective.",4.1 Regularization on Reduced Inputs,[0],[0]
"Given a model f trained on a dataset (X ,Y), we generate reduced examples using input reduction for all training examples X .",4.1 Regularization on Reduced Inputs,[0],[0]
"Beam search often yields multiple reduced versions with the same minimum length for each input x, and we collect all of these versions together to form X̃ as the “negative” example set.
",4.1 Regularization on Reduced Inputs,[0],[0]
Let H (·) denote the entropy and f(y |x) denote the probability of the model predicting y given x.,4.1 Regularization on Reduced Inputs,[0],[0]
"We fine-tune the existing model to simultaneously maximize the log-likelihood on regular examples and the entropy on reduced examples:∑ (x,y)∈(X ,Y) log(f(y |x))",4.1 Regularization on Reduced Inputs,[0],[0]
+ λ,4.1 Regularization on Reduced Inputs,[0],[0]
"∑ x̃∈X̃ H (f(y | x̃)) , (2) where hyperparameter λ controls the trade-off between the two terms.",4.1 Regularization on Reduced Inputs,[0],[0]
"Similar entropy regularization is used by Pereyra et al. (2017), but not in
combination with input reduction; their entropy term is calculated on regular examples rather than reduced examples.",4.1 Regularization on Reduced Inputs,[0],[0]
"On regular examples, entropy regularization does no harm to model accuracy, with a slight increase for SQUAD (Accuracy in Table 2).
",4.2 Regularization Mitigates Pathologies,[0],[0]
"After entropy regularization, input reduction produces more reasonable reduced inputs (Figure 7).",4.2 Regularization Mitigates Pathologies,[0],[0]
"In the SQUAD example from Figure 1, the reduced question changed from “did” to “spend Astor money on ?”",4.2 Regularization Mitigates Pathologies,[0],[0]
after fine-tuning.,4.2 Regularization Mitigates Pathologies,[0],[0]
The average length of reduced examples also increases across all tasks (Reduced length in Table 2).,4.2 Regularization Mitigates Pathologies,[0],[0]
"To verify that model overconfidence is indeed mitigated— that the reduced examples are less “rubbish” compared to before fine-tuning—we repeat the human experiments from Section 2.3.
",4.2 Regularization Mitigates Pathologies,[0],[0]
Human accuracy increases across all three tasks (Table 3).,4.2 Regularization Mitigates Pathologies,[0],[0]
"We also repeat the vs. Random experiment: we re-generate the random examples to match the lengths of the new reduced examples from input reduction, and find humans now prefer the reduced examples to random ones.",4.2 Regularization Mitigates Pathologies,[0],[0]
"The increase in both human performance and preference suggests that the reduced examples are more reasonable; model pathologies have been mitigated.
",4.2 Regularization Mitigates Pathologies,[0],[0]
"While these results are promising, it is not clear whether our input reduction method is necessary to achieve them.",4.2 Regularization Mitigates Pathologies,[0],[0]
"To provide a baseline, we finetune models using inputs randomly reduced to the same lengths as the ones generated by input reduction.",4.2 Regularization Mitigates Pathologies,[0],[0]
This baseline improves neither the model accuracy on regular examples nor interpretability under input reduction (judged by lengths of reduced examples).,4.2 Regularization Mitigates Pathologies,[0],[0]
Input reduction is effective in generating negative examples to counter model overconfidence.,4.2 Regularization Mitigates Pathologies,[0],[0]
"Rubbish examples have been studied in the image domain (Goodfellow et al., 2015; Nguyen et al., 2015), but to our knowledge not for NLP.",5 Discussion,[0],[0]
Our input reduction process gradually transforms a valid input into a rubbish example.,5 Discussion,[0],[0]
"We can often determine which word’s removal causes the transition to occur—for example, removing “Broncos” in Figure 5.",5 Discussion,[0],[0]
"These rubbish examples are particularly interesting, as they are also adversarial: the difference from a valid example is small, unlike image rubbish examples generated from pure noise which are far outside the training data distribution.
",5 Discussion,[0],[0]
"The robustness of NLP models has been studied extensively (Papernot et al., 2016; Jia and Liang, 2017; Iyyer et al., 2018; Ribeiro et al., 2018), and most studies define adversarial examples similar to the image domain: small perturbations to the input lead to large changes in the output.",5 Discussion,[0],[0]
"HotFlip (Ebrahimi et al., 2017) uses a gradient-based approach, similar to image adversarial examples, to flip the model prediction by perturbing a few characters or words.",5 Discussion,[0],[0]
"Our work and Belinkov and Bisk (2018) both identify cases where noisy
user inputs become adversarial by accident: common misspellings break neural machine translation models; we show that incomplete user input can lead to unreasonably high model confidence.
",5 Discussion,[0],[0]
Other failures of interpretation methods have been explored in the image domain.,5 Discussion,[0],[0]
"The sensitivity issue of gradient-based interpretation methods, similar to our shifting heatmaps, are observed by Ghorbani et al. (2017) and Kindermans et al. (2017).",5 Discussion,[0],[0]
They show that various forms of input perturbation—from adversarial changes to simple constant shifts in the image input—cause significant changes in the interpretation.,5 Discussion,[0],[0]
"Ghorbani et al. (2017) make a similar observation about secondorder sensitivity, that “the fragility of interpretation is orthogonal to fragility of the prediction”.
",5 Discussion,[0],[0]
Previous work studies biases in the annotation process that lead to datasets easier than desired or expected which eventually induce pathological models.,5 Discussion,[0],[0]
We attribute our observed pathologies primarily to the lack of accurate uncertainty estimates in neural models trained with maximum likelihood.,5 Discussion,[0],[0]
"SNLI hypotheses contain artifacts that allow training a model without the premises (Gururangan et al., 2018); we apply input reduction at test time to the hypothesis.",5 Discussion,[0],[0]
"Similarly, VQA images are surprisingly unimportant for training a model; we reduce the question.",5 Discussion,[0],[0]
"The recent SQUAD 2.0 (Rajpurkar et al., 2018) augments the original reading comprehension task with an uncertainty modeling requirement, the goal being to make the task more realistic and challenging.
",5 Discussion,[0],[0]
Section 3.1 explains the pathologies from the overconfidence perspective.,5 Discussion,[0],[0]
"One explanation for overconfidence is overfitting: Guo et al. (2017) show that, late in maximum likelihood training,
the model learns to minimize loss by outputting low-entropy distributions without improving validation accuracy.",5 Discussion,[0],[0]
"To examine if overfitting can explain the input reduction results, we run input reduction using DRQA model checkpoints from every training epoch.",5 Discussion,[0],[0]
"Input reduction still achieves similar results on earlier checkpoints, suggesting that better convergence in maximum likelihood training cannot fix the issues by itself—we need new training objectives with uncertainty estimation in mind.",5 Discussion,[0],[0]
We use the reduced examples generated by input reduction to regularize the model and improve its interpretability.,5.1 Methods for Mitigating Pathologies,[0],[0]
"This resembles adversarial training (Goodfellow et al., 2015), where adversarial examples are added to the training set to improve model robustness.",5.1 Methods for Mitigating Pathologies,[0],[0]
"The objectives are different: entropy regularization encourages high uncertainty on rubbish examples, while adversarial training makes the model less sensitive to adversarial perturbations.
",5.1 Methods for Mitigating Pathologies,[0],[0]
Pereyra et al. (2017) apply entropy regularization on regular examples from the start of training to improve model generalization.,5.1 Methods for Mitigating Pathologies,[0],[0]
"A similar method is label smoothing (Szegedy et al., 2016).",5.1 Methods for Mitigating Pathologies,[0],[0]
"In comparison, we fine-tune a model with entropy regularization on the reduced examples for better uncertainty estimates and interpretations.
",5.1 Methods for Mitigating Pathologies,[0],[0]
"To mitigate overconfidence, Guo et al. (2017) propose post-hoc fine-tuning a model’s confidence with Platt scaling.",5.1 Methods for Mitigating Pathologies,[0],[0]
This method adjusts the softmax function’s temperature parameter using a small held-out dataset to align confidence with accuracy.,5.1 Methods for Mitigating Pathologies,[0],[0]
"However, because the output is calibrated using the entire confidence distribution, not individual values, this does not reduce overconfidence on specific inputs, such as the reduced examples.",5.1 Methods for Mitigating Pathologies,[0],[0]
"To highlight the erratic model predictions on short examples and provide a more intuitive demonstration, we present paired-input tasks.",5.2 Generalizability of Findings,[0],[0]
"On these tasks, the short lengths of reduced questions and hypotheses obviously contradict the necessary number of words for a human prediction (further supported by our human studies).",5.2 Generalizability of Findings,[0],[0]
"We also apply input reduction to single-input tasks including sentiment analysis (Maas et al., 2011) and Quizbowl (BoydGraber et al., 2012), achieving similar results.
",5.2 Generalizability of Findings,[0],[0]
"Interestingly, the reduced examples transfer to other architectures.",5.2 Generalizability of Findings,[0],[0]
"In particular, when we feed fifty reduced SNLI inputs from each class—generated with the BIMPM model (Wang et al., 2017)—through the Decomposable Attention Model (Parikh et al., 2016),2 the same prediction is triggered 81.3% of the time.",5.2 Generalizability of Findings,[0],[0]
"We introduce input reduction, a process that iteratively removes unimportant words from an input while maintaining a model’s prediction.",6 Conclusion,[0],[0]
"Combined with gradient-based importance estimates often used for interpretations, we expose pathological behaviors of neural models.",6 Conclusion,[0],[0]
"Without lowering model confidence on its original prediction, an input sentence can be reduced to the point where it appears nonsensical, often consisting of one or two words.",6 Conclusion,[0],[0]
"Human accuracy degrades when shown the reduced examples instead of the original, in contrast to neural models which maintain their original predictions.
",6 Conclusion,[0],[0]
We explain these pathologies with known issues of neural models: overconfidence and sensitivity to small input changes.,6 Conclusion,[0],[0]
The nonsensical reduced examples are caused by inaccurate uncertainty estimates—the model is not able to lower its confidence on inputs that do not belong to any label.,6 Conclusion,[0],[0]
"The second-order sensitivity is another issue why gradient-based interpretation methods may fail to align with human perception: a small change in the input can cause, at the same time, a minor change in the prediction but a large change in the interpretation.",6 Conclusion,[0],[0]
Input reduction perturbs the input multiple times and can expose deeper issues of model overconfidence and oversensitivity that other methods cannot.,6 Conclusion,[0],[0]
"Therefore, it can be used to stress test the interpretability of a model.
",6 Conclusion,[0.9533524582357475],"['Our analysis crucially requires that any plausibly optimal action a be chosen with a reasonable probability, something which the optimistic algorithm fails to ensure.']"
"Finally, we fine-tune the models by maximizing entropy on reduced examples to mitigate the deficiencies.",6 Conclusion,[0],[0]
"This improves interpretability without sacrificing model accuracy on regular examples.
",6 Conclusion,[0],[0]
"To properly interpret neural models, it is important to understand their fundamental characteristics: the nature of their decision surfaces, robustness against adversaries, and limitations of their training objectives.",6 Conclusion,[0],[0]
We explain fundamental difficulties of interpretation due to pathologies in neural models trained with maximum likelihood.,6 Conclusion,[0],[0]
"Our
2http://demo.allennlp.org/ textual-entailment
work suggests several future directions to improve interpretability: more thorough evaluation of interpretation methods, better uncertainty and confidence estimates, and interpretation beyond bagof-word heatmap.",6 Conclusion,[0],[0]
Feng was supported under subcontract to Raytheon BBN Technologies by DARPA award HR0011-15-C-0113.,Acknowledgments,[0],[0]
JBG is supported by NSF Grant IIS1652666.,Acknowledgments,[0],[0]
"Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsor.",Acknowledgments,[0],[0]
"The authors would like to thank Hal Daumé III, Alexander M. Rush, Nicolas Papernot, members of the CLIP lab at the University of Maryland, and the anonymous reviewers for their feedback.",Acknowledgments,[0],[0]
"One way to interpret neural model predictions is to highlight the most important input features—for example, a heatmap visualization over the words in an input sentence.",abstractText,[0],[0]
"In existing interpretation methods for NLP, a word’s importance is determined by either input perturbation—measuring the decrease in model confidence when that word is removed—or by the gradient with respect to that word.",abstractText,[0],[0]
"To understand the limitations of these methods, we use input reduction, which iteratively removes the least important word from the input.",abstractText,[0],[0]
This exposes pathological behaviors of neural models: the remaining words appear nonsensical to humans and are not the ones determined as important by interpretation methods.,abstractText,[0],[0]
"As we confirm with human experiments, the reduced examples lack information to support the prediction of any label, but models still make the same predictions with high confidence.",abstractText,[0],[0]
"To explain these counterintuitive results, we draw connections to adversarial examples and confidence calibration: pathological behaviors reveal difficulties in interpreting neural models trained with maximum likelihood.",abstractText,[0],[0]
"To mitigate their deficiencies, we fine-tune the models by encouraging high entropy outputs on reduced examples.",abstractText,[0],[0]
Fine-tuned models become more interpretable under input reduction without accuracy loss on regular examples.,abstractText,[0],[0]
Pathologies of Neural Models Make Interpretations Difficult,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1805–1816, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Recent progress in NLP has given rise to the field of personality profiling - automated classification of personality traits based on written, verbal and multimodal behavior of an individual.",1 Introduction,[0],[0]
"This research builds upon findings from classical personality psychology and has applications in a wide range of areas from medicine (suicide prevention) across security (forensics, paedophile detection, cyberbullying) to marketing and sales (recommendation systems, target group profiles).",1 Introduction,[0],[0]
"The gold standard labels for an objective evaluation of personality are mostly obtained by means of personality tests of the Five Factor Model (FFM) (McCrae and Costa, 1987; Goldberg, 1990), which is wellknown and widely accepted in psychology and other research fields.",1 Introduction,[0],[0]
"The FFM defines personality
along five bipolar scales: Extraversion (sociable vs. reserved), Emotional stability (secure vs. neurotic), Agreeableness (friendly vs. unsympathic), Conscientiousness (organized vs. careless) and Openness to experience (insightful vs. unimaginative).",1 Introduction,[0],[0]
"Psychologists have shown that these five personality traits are stable across individual lifespan, demographical and cultural differences (John and Srivastava, 1999) and affect many life aspects.",1 Introduction,[0],[0]
"(Terracciano et al., 2008; Rentfrow et al., 2011).
",1 Introduction,[0],[0]
"It has been shown that the personality traits of readers impact their literature preferences (Tirre and Dixit, 1995; Mar et al., 2009).",1 Introduction,[0],[0]
"Psychology researchers also found that perceived similarity is predictive of interpersonal attraction (Montoya et al., 2008; Byrne, 1961; Chartrand and Bargh, 1999).",1 Introduction,[0],[0]
"More explicitly, recent research (Kaufman and Libby, 2012) shows that readers of a narrative develop more favorable attitudes and less stereotype application towards a character, if his difference (e.g. racial) is revealed only later in the story.",1 Introduction,[0],[0]
We therefore hypothesize that readers might have a preference for reading novels depicting fictional characters that are similar to themselves.,1 Introduction,[0],[0]
Finding a direct link between reader’s and protagonist’s personality traits would advance the development of content-based recommendation systems.,1 Introduction,[0],[0]
"As a first step to explore this hypothesis further, it needs to be determined if we are able to construct a personality profile of a fictional character in a similar way as it is done for humans, and which aspects of personality profiling can be exploited to automatize such procedure.
",1 Introduction,[0],[0]
"In this paper, we open this research topic by presenting a novel collaboratively built dataset of fictional character personality in Section 3, which we make available on our website.1 Framing the personality prediction as a text classification task, we incorporate features of both lexical-
1https://www.ukp.tu-darmstadt.de/data/ personality-profiling/
1805
resource-based and vector space semantics, including WordNet and VerbNet sense-level information and vectorial word representations.",1 Introduction,[0],[0]
"We evaluate three machine learning models based on the speech (Section 4), actions (Section 5) and predicatives (Section 6) of the protagonists, and show that especially on the direct speech and action data the lexical-semantic features significantly outperform the baselines.",1 Introduction,[0],[0]
Qualitative analysis reveals that the most predictive features correspond to reported findings in psychology and NLP.,1 Introduction,[0],[0]
"Research in the the area of content-based recommendation systems have shown that incorporating semantic information is valuable for the user and leads to measurable improvements (Passant, 2010; Di Noia et al., 2012; Heitmann and Hayes, 2010).",2 Related work,[0],[0]
De Clercq et al. (2014) incorporated semantic frames from FrameNet into the recommendation system for books.,2 Related work,[0],[0]
They represent the plot of each book with a sequence of ca.,2 Related work,[0],[0]
200 semantic frames,2 Related work,[0],[0]
and has shown that the frame information (such as Killing - Revenge - Death) outperforms the bag-of-words approach.,2 Related work,[0],[0]
Recent NLP experiments begin to reveal the importance of entitycentric models in a variety of tasks.,2 Related work,[0],[0]
"Chambers (2013) show improvement in event schema induction by learning entity-centric rules (e.g., a victim is likely to be a person).",2 Related work,[0],[0]
"Bamman et al. (2014) and Smith et al. (2013) present latent variable models for unsupervised learning of latent character types in movie plot summaries and in English novels, taking authorial style into account.",2 Related work,[0],[0]
"However, even the state-of-the-art NLP work rather describes personas of fictional characters by their role in the story - e.g., action hero, valley girl, best friend, villain etc. - or by their relations to other characters, such as mother or daughter (Elson et al., 2010; Kokkinakis and Malm, 2011), rather than by their inner preferences and motivations.",2 Related work,[0],[0]
It is important to note here that determining a personality of a character is a very different task from determining its role in the story.,2 Related work,[0],[0]
"Psychological understanding of personality, in contrast to role attribution requires a certain detached objectivity - even outright villains may have traits considered desirable in real life.",2 Related work,[0],[0]
"For example, the devil has in many tales a very high aspiration level, appearing highly conscientious and agreeable.",2 Related work,[0],[0]
"We hypothesize that these deeper personality aspects are
those which drive reader’s affiliation to the character, thus deserve to be examined closer.
",2 Related work,[0],[0]
"Also literary scholars formulate ad hoc personality descriptions for their experiments, for example to test hypotheses from evolutionary psychology (Johnson et al., 2011) or examine fictional portrayals of physicists (Dotson, 2009).",2 Related work,[0],[0]
"These descriptions are usually adjusted to the experiment focus (e.g. emotions, relationships, ambitions).",2 Related work,[0],[0]
"As McCrae et al. () point out, a standard set of personality traits, that encompass the full range of characteristics found in all characters in literature (p.77), is needed for a better comparison.
",2 Related work,[0],[0]
Hence we base our present study primarily on the previous NLP research on personality prediction of human individuals.,2 Related work,[0],[0]
"Correlations between lexical and stylistic aspects of text and the five FFM personality traits of the author have been found in numerous experiments, with extraversion receiving the most attention (Pennebaker and King, 1999; Dewaele and Furnham, 1999; Gill and Oberlander, 2002; Mehl et al., 2006; Aran and Gatica-Perez, 2013; Lepri et al., 2010).",2 Related work,[0],[0]
"The LIWC lexicon (Pennebaker et al., 2001) established its position as a powerful mean of such analysis.
",2 Related work,[0],[0]
"The first machine learning experiments in this area were conducted by Argamon et al. (2005), Oberlander and Nowson (2006) and Mairesse et al. (2007).",2 Related work,[0],[0]
"Researchers predicted the five personality traits of the authors of stream-ofconscientiousness essays, blog posts and recorded conversation snippets.",2 Related work,[0],[0]
"Given balanced data sets, Mairesse et al. (2007) report binary classification accuracy of 50-56% on extraversion in text and 47-57% in speech, using word ngrams, LIWC, MRC psycholinguistic database (Coltheart, 1981) and prosodic features.",2 Related work,[0],[0]
Additional improvement is reported when the extraversion was labeled by external judges rather than by self-testing.,2 Related work,[0],[0]
"Extended studies on larger datasets achieve accuracies around 55% (Nowson, 2007; Estival et al., 2007).",2 Related work,[0],[0]
"More recent work in this area focuses on the personality prediction in social networks (Kosinski et al., 2013; Kosinski et al., 2014) and multimodal personality prediction (Biel and Gatica-Perez, 2013; Aran and Gatica-Perez, 2013).",2 Related work,[0],[0]
"These trends emphasized the correlation of network features and audiovisual features with extraversion, giving rise to the Workshop on Computational Personality Recognition (for an overview see (Celli et al., 2013; Celli et al., 2014).",2 Related work,[0],[0]
"Traditionally, the gold standard for this supervised classification task is obtained by the means of personality questionnaires, used for the Five-Factor Model, taken by each of the individuals assessed.",3 Data set construction,[0],[0]
This poses a challenge for fictional characters.,3 Data set construction,[0],[0]
"However, strong correlations have been found between the self-reported and perceived personality traits (Mehl et al., 2006).",3 Data set construction,[0],[0]
Our gold standard benefits from the fact that readers enjoy discussing the personality of their favourite book character online.,3 Data set construction,[0],[0]
"A popular layman instrument for personality classification is the Myers-Brigggs Type Indicator (Myers et al., 1985), shortly MBTI, which sorts personal preferences into four opposite pairs, or dichotomies, such as Thinking vs. Feeling or Judging vs. Perceiving.",3 Data set construction,[0],[0]
"While the MBTI validity has been questioned by the research community (Pittenger, 2005), the Extraversion scale is showing rather strong validity and correlation to similar trait in the Five-Factor Model (McCrae and Costa, 1989; MacDonald et al., 1994).",3 Data set construction,[0],[0]
"Our study hence focuses on the Extraversion scale.
",3 Data set construction,[0],[0]
"Our data was collected from the collaboratively constructed Personality Databank2 where the readers can vote if a book character is, among other aspects, introverted or extraverted.",3 Data set construction,[0],[0]
"While the readers used codes based on the MBTI typology, they did not apply the MBTI assessment strategies.",3 Data set construction,[0],[0]
There was no explicit annotation guideline and the interpretation was left to readers’ intuition and knowledge.3,3 Data set construction,[0],[0]
This approach of gold standard collection has several obvious drawbacks.,3 Data set construction,[0],[0]
"First, the question is posed as dichotomic, while in reality the extraversion is a normally distributed trait in human population (Goldberg, 1990).",3 Data set construction,[0],[0]
"Second, users can view the vote of previous participants, which may influence their decision.",3 Data set construction,[0],[0]
"While we address both of these issues in our ongoing data collection project based on the Five-Factor Model, we consider them acceptable for this study due to the exploratory character of our pilot research.
",3 Data set construction,[0],[0]
"We have collected extraversion ratings for 298 book characters, of which 129 (43%) are rather extraverted and 166 (56%) rather introverted.",3 Data set construction,[0],[0]
"Rated
2http://www.mbti-databank.com/ 3MBTI defines extraversion as “getting energy from active involvement in events, having a lot of different activities, enjoying being around people.”",3 Data set construction,[0],[0]
"In the NEO Five-Factor Inventory (Costa and McCrae, 1992), underlying facets of extraversion are warmth, gregariousness, assertiveness, activity, excitement seeking and positive emotion.
",3 Data set construction,[0],[0]
"characters come from a wide range of novels that the online users are familiar with, often covering classical literature which is part of the high school syllabus, as well as the most popular modern fiction, such as the Harry Potter series, Twilight, Star Wars or A Game of Thrones.",3 Data set construction,[0],[0]
A sample of the most rated introverts and extraverts is given in table 1.,3 Data set construction,[0],[0]
The rating distribution in our data is strongly Ushaped.,3 Data set construction,[0],[0]
"The percentage agreement of voters in our data is 84.9%, calculated as:
P = 1 N N∑ i=1",3 Data set construction,[0],[0]
k∑ j=1 nij(nij,3 Data set construction,[0],[0]
"− 1) n(n− 1)
where k = 2 (introvert, extravert), N is the number of book characters and n the number of votes per character.",3 Data set construction,[0],[0]
Voters on the website were anonymous and cannot be uniquely identified for additional corrections.,3 Data set construction,[0],[0]
"There is no correlation between the extraversion and the gender of the character.
",3 Data set construction,[0],[0]
Our set of English e-books covered 220 of the characters from our gold standard.,3 Data set construction,[0],[0]
"We have built three systems to assess the following:
1.",3 Data set construction,[0],[0]
Direct speech: Does the style and content of character’s utterances predict his extraversion in a similar way as it was shown for living individuals?,3 Data set construction,[0],[0]
"2. Actions: Is the behavior, of which a character is an agent, predictive for extraversion?",3 Data set construction,[0],[0]
3.,3 Data set construction,[0],[0]
"Predicatives and adverbs: Are the explicit (John was an exhibitionist) or implicit (John shouted abruptly) descriptions of the character in the book predictive for extraversion?
",3 Data set construction,[0],[0]
In the next three sections we present the experimental settings and results for each of the systems.,3 Data set construction,[0],[0]
"The system for the direct speech resembles the most to the previous systems developed for author personality profiling, e.g. on stream of consciousness essays (Mairesse et al., 2007) or social media posts (Celli et al., 2013) and therefore provides the best opportunity for comparison between human individuals and fictional characters.",4 Direct speech of fictional characters,[0],[0]
"On top of the comparison to previous research, we exploit the sense links between WordNet and VerbNet to extract additional features - an approach which is novel for this type of task.",4 Direct speech of fictional characters,[0],[0]
"We process the book text using freely available components of the DKPro framework (Gurevych et al., 2007).",4.1 Extraction and assignment of speech,[0],[0]
The most challenging task in building the direct speech data set is assigning to the direct speech utterance the correct speaker.,4.1 Extraction and assignment of speech,[0],[0]
"We benefit from the epub format of the e-books which defines a paragraph structure in such a way, that only the indirect speech chunk immediately surrounding the direct speech can be considered:
<p> John turned to Harry.",4.1 Extraction and assignment of speech,[0],[0]
"""Let’s go,"" he said.</p>
Given the large amount of text available in the books we focus on precision and discard all utterances with no explicit speaker (i.e., 30-70% of the utterances, dependent on the book), as the performance of current systems on such utterance types is still fairly low (O’Keefe et al., 2012; He et al., 2013; Iosif and Mishra, 2014).",4.1 Extraction and assignment of speech,[0],[0]
"Similarly, conventional coreference resolution systems did not perform well on this type of data and were therefore not used in the final setup.",4.1 Extraction and assignment of speech,[0],[0]
"We adapt the Stanford Named Entity Recognizer(Finkel et al., 2005) to consider titles (Mr., Mrs., Sir...) as a part of the name and to treat the first person I as a named entity.",4.1 Extraction and assignment of speech,[0],[0]
"However, identifying only the named entity PERSON in this way is not sufficient.",4.1 Extraction and assignment of speech,[0],[0]
"On our evaluation sample consisting of A Game of Thrones and Pride and Prejudice books (the former annotated by us, the latter by He et al. (2013)), 20% of utterances with explicit named speaker were not recognized.",4.1 Extraction and assignment of speech,[0],[0]
"Of those correctly identified as a Person in the adjacent indirect speech, 17% were not the speakers.",4.1 Extraction and assignment of speech,[0],[0]
"Therefore we implemented a
custom heuristics (Algorithm 1), which additionally benefits from the WordNet semantic classes of verbs, enriching the speaker detection by grabbing the nouns .",4.1 Extraction and assignment of speech,[0],[0]
"With this method we retrieve 89% of known speakers, of which 92% is assigned correctly.",4.1 Extraction and assignment of speech,[0],[0]
"Retrieved names are grouped based on string overlap (e.g. Ser Jaime and Jaime Lannister), excluding the match on last name, and corrected for non-obvious groupings (such as Margaret and Peggy).",4.1 Extraction and assignment of speech,[0],[0]
"Algorithm 1 Assign speaker 1: nsubj← subjects in adjacent indirect speech 2: if count(nsubj(i) = PERSON) = 1 then speaker ←
nsubj 3: else if count(nsubj(i) = PERSON) ≥ 1 then
speaker ← the nearest one to directSpeech 4: else if directSpeech preceded by
VERB.COMMUNICATION then speaker ← the preceding noun(s) 5: else if directSpeech followed by VERB.COMMUNICATION then speaker ← the following noun(s) 6: else if directSpeech followed by gap & VERB.COMMUNICATION then speaker ← the noun(s) in gap 7: else if directSpeech preceded by gap & VERB.COMMUNICATION then speaker ← the noun(s) in gap return speaker
Our experimental data consists of usable direct speech sets of 175 characters - 80 extraverts (E) and 95 introverts (I) - containing 289 274 words in 21 857 utterances (on average 111 utterances for E and 136 for I, as I are often central in books).4",4.1 Extraction and assignment of speech,[0],[0]
All speech utterances of one book character are represented as one instance in our system.,4.2 Classification approach for direct speech,[0],[0]
"We use the leave-one-out classification setup due to the relatively small dataset size, using the support vector machines (SVM-SMO) classifier, which performs well on comparable tasks (Celli et al., 2013).",4.2 Classification approach for direct speech,[0],[0]
"The classification is performed through the DKPro TC Framework (Daxenberger et al., 2014).
",4.2 Classification approach for direct speech,[0],[0]
"Lexical features As a bottom-up approach we use the 1000 most frequent word uni-, bi- and trigrams, 1000 dependency word pairs, 1000 character trigrams and 500 most frequent verbs, adverbs, adjectives and interjections as binary features.
",4.2 Classification approach for direct speech,[0],[0]
"Semantic features Since the top-down approach, i.e. not focusing on individual words, has
4The data set size is comparable to ongoing personality profiling challenges - see http://pan.webis.de
been found more suitable for the personality profiling task on smaller data sets (Celli et al., 2013), we aim on capturing additional phenomena on a higher level of abstraction.",4.2 Classification approach for direct speech,[0],[0]
The main part of our features is extracted on sense level.,4.2 Classification approach for direct speech,[0],[0]
"We use the most frequent sense of WordNet (Miller, 1995) to annotate all verbs in the direct speech (a simple but well performing approach for books).",4.2 Classification approach for direct speech,[0],[0]
"We then label the disambiguated verbs with their semantic field given in WordNet (WordNet defines 14 semantic classes of verbs which group verbs by their semantic field) and we measure frequency and occurence of each of these classes (e.g. cognition, communication, motion, perception)5.",4.2 Classification approach for direct speech,[0],[0]
"Additionally, we use the lexical-semantic resource UBY (Gurevych et al., 2012) to access the WordNet and VerbNet information, and to exploit the VerbNet sense-level links which connects WordNet senses with the corresponding 273 main VerbNet classes (Kipper-Schuler, 2005).",4.2 Classification approach for direct speech,[0],[0]
"These are more fine-grained (e.g. pay, conspire, neglect, discover) than the WordNet semantic fields.",4.2 Classification approach for direct speech,[0],[0]
"WordNet covered 90% and VerbNet 86% of all the verb occurences.
",4.2 Classification approach for direct speech,[0],[0]
"On word level, we extract 81 additional features using the Linguistic Inquiry and Word Count (LIWC) tools (Pennebaker et al., 2001), which consists of lexicons related to psychological processes (cognitive, perceptual, social, biological, affective) and personal concerns (achievement, religion, death...) and other categories such as fillers, disfluencies or swear words6.",4.2 Classification approach for direct speech,[0],[0]
"Additionally, since emotion detection has been found predictive in previous personality work (Mohammad and Kiritchenko, 2013), we measure overall positive and negative sentiment expressed per character, using SentiWordNet (Esuli and Sebastiani, 2006) and NRC Emotion Lexicon (Mohammad and Turney, 2010) for the word lookup, inverting sentiment scores for negated dependency sub-tree given by the Stanford Parser.
Stylistic features Features of this group capture the syntactic and stylistic properties of the utterances of a character, disregarding the content.",4.2 Classification approach for direct speech,[0],[0]
"Starting from the surfacial properties, we measure the sentence, utterance and word length, including the proportion of words shorter than 4 or longer than 6 letters, frequency of each punctuation mark,
5https://wordnet.princeton.edu/man/ lexnames.5WN.html
6For complete overview refer to www.liwc.net
and endings of each adjective as per Corney et al. (2002).",4.2 Classification approach for direct speech,[0],[0]
"On the syntax level we measure the frequency of each part of speech as well as the 500 most frequent part-of-speech bi-, tri- and quadrigrams, and the frequency of each dependency obtained from the Stanford Parser.",4.2 Classification approach for direct speech,[0],[0]
"We additionally capture the frequency of superlatives, comparatives and modal verbs, the proportion of verbs in present, past and future tense, and the formality of the language as per the part-of-speech-based formality coefficient (Heylighen and Dewaele, 2002), and measure the average depth of the parse trees.
",4.2 Classification approach for direct speech,[0],[0]
"Word embeddings as features Since vector space semantics has been beneficial for predicting author’s personality in previous work (Neuman and Cohen, 2014), we use a pre-trained word vector model created by the GloVe algorithm (Pennington et al., 2014) on English Wikipedia.",4.2 Classification approach for direct speech,[0],[0]
GloVe employs a global log-bilinear regression model that combines the advantages of the global matrix factorization and local context window methods.,4.2 Classification approach for direct speech,[0],[0]
"We assign the resulting 300-dimensional vectors to the words in character’s direct speech, excluding stopwords, and calculate an average vector for each character.",4.2 Classification approach for direct speech,[0],[0]
"We calculate for each test character the cosine similarity to the mean vector of extravert, resp.",4.2 Classification approach for direct speech,[0],[0]
"introvert, in the training data, and to each character in the training set individually using the DL4J NLP package7.",4.2 Classification approach for direct speech,[0],[0]
We consider both the final scalar outcome and the difference of each of the individual vector dimensions as features.,4.2 Classification approach for direct speech,[0],[0]
"Table 2 shows the precision, recall, F1-score and accuracy for extraversion and introversion as a weighted average of the two class values.
",4.3 Classification results on direct speech,[0],[0]
"7http://deeplearning4j.org/
Similarly to previous research (Mairesse et al., 2007; Celli et al., 2013), the bottom-up word based approach is outperformed by top-down semantic approaches which employ a more abstract feature representation.",4.3 Classification results on direct speech,[0],[0]
"As in previous work, LIWC features exhibit good performance.",4.3 Classification results on direct speech,[0],[0]
"However, the highest performance is achieved employing the VerbNet verb classes with WordNet wordsense disambiguation.",4.3 Classification results on direct speech,[0],[0]
Also stylistic features contribute substantially to the classification despite the mixture of genres in our book corpus - especially frequencies of modal verbs and part-ofspeech ratios were particularly informative.,4.3 Classification results on direct speech,[0],[0]
"The most predictive features from each group are listed in Table 3 together with their correlation merit (Hall, 1999), and compared with previous work in Table 4.
",4.3 Classification results on direct speech,[0],[0]
"In accordance with the experiments of Pennebaker and King (1999), we observe more frequent exclusions (e.g. without, but), hedging and negation expressed by introverts, and inclusion (e.g. with, and) by extraverts.",4.3 Classification results on direct speech,[0],[0]
"Extraverts talk more in first person plural, use more back-channels and interjections, and talk more about aspects related to their body.",4.3 Classification results on direct speech,[0],[0]
"Introverts show more rationalization through insight words and more factual speech using less pronouns.
",4.3 Classification results on direct speech,[0],[0]
"Additionally, the semantic features in Table 3 confirm the broad psychological characteristics of both types in general, i.e., for introverts the rationalization, uncertainty and preference for individual or rather static activities, and for extraverts their spontaneity, talkativeness and preference for motion.",4.3 Classification results on direct speech,[0],[0]
"Furthermore, we observe certain directness in extraverts’ speech - note the predictive words fat and dirty and frequent descriptions of body functions.
",4.3 Classification results on direct speech,[0],[0]
Discussion Exploiting the links between lexicalsemantic resources (performing WordNet wordsense disambiguation and using VerbNet verb classes linked to the disambiguated senses) was particularly beneficial for this task.,4.3 Classification results on direct speech,[0],[0]
"WordNet semantic fields for verbs alone are too coarsegrained to capture the nuances in direct speech, and experiments with fine-grained VerbNet classes without WSD resulted in noisy labels.",4.3 Classification results on direct speech,[0],[0]
"We did not confirm the previously reported findings on emotional polarity - we observe that the genre of the books (e.g. love romance vs horror story) have blurred the subtle differences between individual characters, unfortunately the dataset size did not allow for genre distinctions.",4.3 Classification results on direct speech,[0],[0]
"Furthermore, a perceived extravert in our case can be a pure villain (Draco Malfoy, Joffrey Baratheon...) as well as a friendly companion (Gimli, Ron Weasley...), while the evil extravert types are possibly rarer in the experiments on human writing, or are more likely to fit under the MBTI definition of extraversion than FFM facets.",4.3 Classification results on direct speech,[0],[0]
"Another potential cause, based on the error analysis, is the different target of the same sentiment for extraverts and introverts.",4.3 Classification results on direct speech,[0],[0]
"For example, the ngram ”I fear” is highly predictive for an introvert in our data while extraverts would rather use formulations to imply that others should fear.",4.3 Classification results on direct speech,[0],[0]
"Similarly to Nowson et al. (2005), we did not find any difference in the formality measure of Heylighen and Dewaele (2002).",4.3 Classification results on direct speech,[0],[0]
"Neither we did in the complexity of sentences as per the parse tree depth
and sentence length.",4.3 Classification results on direct speech,[0],[0]
It is probable that these aspects were also impacted by our broad variety of author style (F. Dostoyevsky vs J. K. Rowling).,4.3 Classification results on direct speech,[0],[0]
"Our basic vector-based features carried no useful information in our case, in contrast to the personality research of Neuman and Cohen (2014).",4.3 Classification results on direct speech,[0],[0]
We observed that the factual content of the stories contributed to the character similarity measure more than the subtle personality differences.,4.3 Classification results on direct speech,[0],[0]
"While psycholinguists and consequenlty NLP researchers analyzed the relation between speech, resp.",5 Actions of fictional characters,[0],[0]
"writing, and personality of an individual, psychologists often evaluate extraversion through behavioral personality questionnaries (Costa and McCrae, 1992; Goldberg et al., 2006).",5 Actions of fictional characters,[0],[0]
We hypothesize that similar behavior shall be predictive for extraversion of fictional characters as perceived by the readers.,5 Actions of fictional characters,[0],[0]
"For our purpose we define actions as the subject, verb and context of a sentence, where the subject is a named entity Person and the context is either a direct object in relation dobj to the verb or a first child of the adjacent verb phrase in a parse tree.",5.1 Action extraction,[0],[0]
"After grouping the actions per character, the subject name is removed.",5.1 Action extraction,[0],[0]
"For example, a sample of actions of the character Eddard Stark of Game of Thrones would be: X paused a moment, X studied his face, X changed his mind, X unrolled the paper, X said etc., visualized in Figure 1.",5.1 Action extraction,[0],[0]
"We obtained 22 030 actions for 205 characters (102 E, 116 I), with on average 100 actions for E and 101 for I. Note that also actions for those characters who do not talk enough in the books (often first-person perspectives) could be used.",5.1 Action extraction,[0],[0]
In the system based on actions we use only a subset of the features described in 4.2.,5.2 Action classification setup,[0],[0]
From the lexical features we focus on the 500 most frequent verbs and dependency word pairs.,5.2 Action classification setup,[0],[0]
"Semantic features are used the same way as in 4.2, profiting from LIWC, WordNet, Verbnet and the sentiment lexicons.",5.2 Action classification setup,[0],[0]
Word embedding vectors for book characters are in this case computed by taking only the verbs into account rather than all content words.,5.2 Action classification setup,[0],[0]
"From the stylistic features we use the part-ofspeech bigrams and trigrams, verb modality and verb tense.",5.2 Action classification setup,[0],[0]
"Table 5 shows the performance of the classification models based on the protagonists’ actions, using different feature groups.",5.3 Classification results on actions,[0],[0]
"The overall performance is higher than for the direct speech model.
",5.3 Classification results on actions,[0],[0]
"Due to the lack of previous NLP experiments on this task, we compare our features to the actions measured in the International Personality Item Pool (Goldberg et al., 2006), frequently used personality assesment questionnaire (Table 6).
",5.3 Classification results on actions,[0],[0]
The most predictive features of this model capture the activity and excitement seeking facets of extraversion.,5.3 Classification results on actions,[0],[0]
"Stylistic features reflect the complexity difference of the verb phrases (John jumped vs. John thought about it), extraverts being characterized by plain verbs.",5.3 Classification results on actions,[0],[0]
Semantic features exhibit higher precision than stylistic ones.,5.3 Classification results on actions,[0],[0]
"Sense-linked semantic classes of VerbNet demonstrate the preference of extraverts for being active and expressing themselves - they jump, fight, shout, run in and run out, eat and drink, see and hear and get easily bored.",5.3 Classification results on actions,[0],[0]
"Extraverts in books also
often bring or hold something.",5.3 Classification results on actions,[0],[0]
"Introverts, on the other hand, seem to favor slow movements - while they are thinking, reflecting, creating, looking for explanations and find out solutions, they tend to lie down, sit or walk, eventually even sleep or snooze.",5.3 Classification results on actions,[0],[0]
"The uncertainty typical for introverts is also notable in their actions, as they often hope or wish for something they might like to do.",5.3 Classification results on actions,[0],[0]
"Additionally, semantic classes Social and Family, reported as correlated to extraversion by Pennebaker and King (1999) and not confirmed in our first model, became predictive in protaonists’ actions.",5.3 Classification results on actions,[0],[0]
"Also in this task, the VerbNet classes brought significant improvement in performance.",5.4 Discussion,[0],[0]
"The classification model based on actions outperforms not only the direct speech model, but also the state-of-the-art systems predicting authors’ extraversion from the stream-of-consciousness essays (Mairesse et al., 2007; Celli et al., 2013; Neuman and Cohen, 2014).",5.4 Discussion,[0],[0]
"While surely not directly comparable, this result hints to the fact that the personality is easier to detect from behavior than from person’s verbal expression.",5.4 Discussion,[0],[0]
"This would correspond to the findings of Mairesse et al. (2007), Biel and Gatica-Perez (2013) and Aran and Gatica-Perez (2013) on multimodal data sets.",5.4 Discussion,[0],[0]
Our third extraversion prediction system is subordinate to how fictional characters are described and to the manners in which they behave.,6 Predicatives of fictional characters,[0],[0]
We are not aware of a previous NLP work predicting extraversion using descriptive adjectives of the persons in question.,6 Predicatives of fictional characters,[0],[0]
We thus juxtapose the most predictive features of our system to the adjectival extraversion markers developed by Goldberg (1992).,6 Predicatives of fictional characters,[0],[0]
In this setup we extract predicatives of the named entities PERSON in the books - relations amod (angry John) and cop (John was smart).,6.1 Extraction of descriptive properties,[0],[0]
"As these explicit statements are very sparse in modern novels, we additionally include adverbial modifiers (advmod) related to person’s actions (John said angrily).",6.1 Extraction of descriptive properties,[0],[0]
"We extract data for 205 characters, with on average 43 words per character.",6.1 Extraction of descriptive properties,[0],[0]
"This system uses similar set of lexical, semantic and vectorial features similarly as in 5.2, this time with the focus on adjectives, nouns and adverbs instead of verbs.",6.2 Classification setup,[0],[0]
"Stylistic and VerbNet features are hence not included, word vectors are as in 4.2.",6.2 Classification setup,[0],[0]
Table 7 reports on the performance of individual feature groups.,6.3 Classification results on descriptions,[0],[0]
"With only few words per character semantic lexicons are less powerful than ngrams.
",6.3 Classification results on descriptions,[0],[0]
Table 8 displays the most predictive features in our system contrasted to the adjectival markers.,6.3 Classification results on descriptions,[0],[0]
All our systems had issues with characters rated by less than five readers and with protagonists with low agreement.,6.4 Discussion on errors,[0],[0]
"Other challenges arise from authorial style, age of the novel and speech individuality of characters (e.g. Yoda).",6.4 Discussion on errors,[0],[0]
"Varied length of information for different characters poses issues in measuring normally distributed features (e.g. ratio of jumping verbs), being in shorter texts less reliable.",6.4 Discussion on errors,[0],[0]
"Ongoing and future work on this task addresses the limitations of these initial experiments, especially the data set size and the gold standard quality.",6.4 Discussion on errors,[0],[0]
Extending the data will also enable us to examine different book genres as variables for the personality distribution and feature impact.,6.4 Discussion on errors,[0],[0]
"It will be worth examining the relations between characters, since we observed certain patterns in our data, such as the main introvert character supported by his best friend extravert.",6.4 Discussion on errors,[0],[0]
"Additionally, we want to verify if the system in Section 6 is overly optimistic due to the data size.",6.4 Discussion on errors,[0],[0]
"Automated personality profiling of fictional characters, based on rigorous models from personality psychology, has a potential to impact numerous domains.",7 Conclusion and future work,[0],[0]
We framed it as a text classification problem and presented a novel collaboratively built dataset of fictional personality.,7 Conclusion and future work,[0],[0]
"We incor-
porate features of both lexical resource-based and vectorial semantics, including WordNet and VerbNet sense-level information and vectorial word representations.",7 Conclusion and future work,[0],[0]
"In models based on the speech and actions of the protagonists, we demonstrated that the sense-linked lexical-semantic features significantly outperform the baselines.",7 Conclusion and future work,[0],[0]
The most predictive features correspond to the reported findings in personality psychology and NLP experiments on human personality.,7 Conclusion and future work,[0],[0]
"Our systems based on actions and appearance of characters demonstrate higher performance than systems based on direct speech, which is in accordance with recent research on personality in social networks (Kosinski et al., 2014; Biel and Gatica-Perez, 2013), revealing the importance of the metadata.",7 Conclusion and future work,[0],[0]
"We have shown that exploiting the links between lexical resources to leverage more accurate semantic information can be beneficial for this type of tasks, oriented to actions performed by the entity.",7 Conclusion and future work,[0],[0]
"However, the human annotator agreement in our task stays high above the performance achieved.",7 Conclusion and future work,[0],[0]
"Considering that most of the sucessful novels were produced as movies, we cannot exclude that our annotators based their decision on the multimodal representation of the protagonists.",7 Conclusion and future work,[0],[0]
"In the future we aim on collecting a more detail and rigorous gold standard through gamification and expanding our work on all five personality traits from the FiveFactor Model and their facets, and ultimately extend our system to a semi-supervised model dealing with notably larger amount of data.",7 Conclusion and future work,[0],[0]
"We also plan to examine closer the differences between perceived human and fictional personality, and the relationship between the personality of the reader and the characters.",7 Conclusion and future work,[0],[0]
This work has been supported by the Volkswagen Foundation as part of the Lichtenberg Professorship Program under grant No. I/82806 and by the German Research Foundation under grant No. GU 798/14-1.,Acknowledgments,[0],[0]
Additional support was provided by the German Federal Ministry of Education and Research (BMBF) as a part of the Software Campus program under the promotional reference 01-S12054 and by the German Institute for Educational Research (DIPF).,Acknowledgments,[0],[0]
"We also warmly thank Holtzbrinck Digital GmbH for providing a substantial part of the e-book resources, and the EMNLP reviewers for their helpful comments.",Acknowledgments,[0],[0]
This study focuses on personality prediction of protagonists in novels based on the Five-Factor Model of personality.,abstractText,[0],[0]
We present and publish a novel collaboratively built dataset of fictional character personality and design our task as a text classification problem.,abstractText,[0],[0]
"We incorporate a range of semantic features, including WordNet and VerbNet sense-level information and word vector representations.",abstractText,[0],[0]
"We evaluate three machine learning models based on the speech, actions and predicatives of the main characters, and show that especially the lexical-semantic features significantly outperform the baselines.",abstractText,[0],[0]
The most predictive features correspond to reported findings in personality psychology.,abstractText,[0],[0]
Personality Profiling of Fictional Characters using Sense-Level Links between Lexical Resources,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 700–705 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
700",text,[0],[0]
Query auto-completion (QAC) is a feature used by search engines that provides a list of suggested queries for the user as they are typing.,1 Introduction,[0],[0]
"For instance, if the user types the prefix “mete” then the system might suggest “meters” or “meteorite” as completions.",1 Introduction,[0],[0]
"This feature can save the user time and reduce cognitive load (Cai et al., 2016).
",1 Introduction,[0],[0]
"Most approaches to QAC are extensions of the Most Popular Completion (MPC) algorithm (BarYossef and Kraus, 2011).",1 Introduction,[0],[0]
MPC suggests completions based on the most popular queries in the training data that match the specified prefix.,1 Introduction,[0],[0]
"One way to improve MPC is to consider additional signals such as temporal information (Shokouhi and Radinsky, 2012; Whiting and Jose, 2014) or information gleaned from a users’ past queries (Shokouhi, 2013).",1 Introduction,[0],[0]
"This paper deals with the latter of those two signals, i.e. personalization.",1 Introduction,[0],[0]
"Personalization relies on the fact that query likelihoods are drastically different among different people depending on their needs and interests.
",1 Introduction,[0],[0]
"Recently, Park and Chiba (2017) suggested a significantly different approach to QAC.",1 Introduction,[0],[0]
"In their
work, completions are generated from a character LSTM language model instead of by ranking completions retrieved from a database, as in the MPC algorithm.",1 Introduction,[0],[0]
"This approach is able to complete queries whose prefixes were not seen during training and has significant memory savings over having to store a large query database.
",1 Introduction,[0],[0]
"Building on this work, we consider the task of personalized QAC, advancing current methods by combining the obvious advantages of personalization with the effectiveness of a language model in handling rare and previously unseen prefixes.",1 Introduction,[0],[0]
The model must learn how to extract information from a user’s past queries and use it to adapt the generative model for that person’s future queries.,1 Introduction,[0],[0]
"To do this, we leverage recent advances in contextadaptive neural language modeling.",1 Introduction,[0],[0]
"In particular, we make use of the recently introduced FactorCell model that uses an embedding vector to additively transform the weights of the language model’s recurrent layer with a low-rank matrix (Jaech and Ostendorf, 2017).",1 Introduction,[0],[0]
"By allowing a greater fraction of the weights to change during personalization, the FactorCell model has advantages over the traditional approach to adaptation of concatenating a context vector to the input of the LSTM (Mikolov and Zweig, 2012).
",1 Introduction,[0],[0]
"Table 1 provides an anecdotal example from
the trained FactorCell model to demonstrate the intended behavior.",1 Introduction,[0],[0]
The table shows the top five completions for the prefix “ba” in a cold start scenario and again after the user has completed five sports related queries.,1 Introduction,[0],[0]
"In the warm start scenario, the “baby names” and “babiesrus” completions no longer appear in the top five and have been replaced with “basketball” and “baseball”.
",1 Introduction,[0],[0]
The novel aspects of this work are the application of an adaptive language model to the task of QAC personalization and the demonstration of how RNN language models can be adapted to contexts (users) not seen during training.,1 Introduction,[0],[0]
An additional contribution is showing that a richer adaptation framework gives added gains with added data.,1 Introduction,[0],[0]
"Adaptation depends on learning an embedding for each user, which we discuss in Section 2.1, and then using that embedding to adjust the weights of the recurrent layer, discussed in Section 2.2.",2 Model,[0],[0]
"During training, we learn an embedding for each of the users.",2.1 Learning User Embeddings,[0],[0]
We think of these embeddings as holding latent demographic factors for each user.,2.1 Learning User Embeddings,[0],[0]
"Users who have less than 15 queries in the training data (around half the users but less than 13% of the queries) are grouped together as a single entity, user1, leaving k users.",2.1 Learning User Embeddings,[0],[0]
"The user embeddings matrix Uk×m, wherem is the user embedding size, is learned via back-propagation as part of the end-toend model.",2.1 Learning User Embeddings,[0],[0]
"The embedding for an individual user is the ith row of U and is denoted by ui.
",2.1 Learning User Embeddings,[0],[0]
It is important to be able to apply the model to users that are not seen during training.,2.1 Learning User Embeddings,[0],[0]
This is done by online updating of the user embeddings during evaluation.,2.1 Learning User Embeddings,[0],[0]
"When a new person, userk+1 is seen, a new row is added to U and initialized to u1.",2.1 Learning User Embeddings,[0],[0]
Each person’s user embedding is updated via back-propagation every time they select a query.,2.1 Learning User Embeddings,[0],[0]
"When doing online updating of the user embeddings, the rest of the model parameters (everything except U) are frozen.",2.1 Learning User Embeddings,[0],[0]
We consider three model architectures which differ only in the method for adapting the recurrent layer.,2.2 Recurrent Layer Adaptation,[0],[0]
"First is the unadapted LM, analogous to the model from Park and Chiba (2017), which does no personalization.",2.2 Recurrent Layer Adaptation,[0],[0]
"The second architecture was
introduced by Mikolov and Zweig (2012) and has been used multiple times for LM personalization (Wen et al., 2013; Huang et al., 2014; Li et al., 2016).",2.2 Recurrent Layer Adaptation,[0],[0]
It works by concatenating a user embedding to the character embedding at every step of the input to the recurrent layer.,2.2 Recurrent Layer Adaptation,[0],[0]
Jaech and Ostendorf (2017) refer to this model as the ConcatCell and show that it is equivalent to adding a term Vu to adjust the bias of the recurrent layer.,2.2 Recurrent Layer Adaptation,[0],[0]
"The hidden state of a ConcatCell with embedding size e and hidden state size h is given in Equation 1 where σ is the activation function, wt is the character embedding, ht−1 is the previous hidden state, and W ∈ Re+h×h and b ∈",2.2 Recurrent Layer Adaptation,[0],[0]
"Rh are the recurrent layer weight matrix and bias vector.
",2.2 Recurrent Layer Adaptation,[0],[0]
"ht = σ([wt, ht−1]W + b+Vu) (1)
",2.2 Recurrent Layer Adaptation,[0],[0]
Adapting just the bias vector is a significant limitation.,2.2 Recurrent Layer Adaptation,[0],[0]
"The FactorCell model, (Jaech and Ostendorf, 2017), remedies this by letting the user embedding transform the weights of the recurrent layer via the use of a low-rank adaptation matrix.",2.2 Recurrent Layer Adaptation,[0],[0]
"The FactorCell uses a weight matrix W′ = W +A that has been additively transformed by a personalized low-rank matrix A. Because the FactorCell weight matrix W′ is different for each user (See Equation 2), it allows for a much stronger adaptation than what is possible using the more standard ConcatCell model.1
ht = σ([wt, ht−1]W ′ + b) (2)
The low-rank adaptation matrix A is generated by taking the product between a user’s m dimensional embedding and left and right bases tensors, ZL ∈ Rm×e+h×r and ZR ∈ Rr×h×m",2.2 Recurrent Layer Adaptation,[0],[0]
"as so,
A = (ui ×1 ZL)(ZR ×3 ui) (3)
where ×i denotes the mode-i tensor product.",2.2 Recurrent Layer Adaptation,[0],[0]
The above product selects a user specific adaptation matrix by taking a weighted combination of the m rank r matrices held between ZL and ZR.,2.2 Recurrent Layer Adaptation,[0],[0]
"The rank, r, is a hyperparameter which controls the degree of personalization.",2.2 Recurrent Layer Adaptation,[0],[0]
"Our experiments make use of the AOL Query data collected over three months in 2006 (Pass et al., 2006).",3 Data,[0],[0]
"The first six of the ten files were used for
1In the case of an LSTM, W′ is extended to incorporate all of the gates.
training.",3 Data,[0],[0]
"This contains approximately 12 million queries from 173,000 users for an average of 70 queries per user (median 15).",3 Data,[0],[0]
"A set of 240,000 queries from those same users (2% of the data) was reserved for tuning and validation.",3 Data,[0],[0]
"From the remaining files, one million queries from 30,000 users are used to test the models on a disjoint set of users.",3 Data,[0],[0]
The vocabulary consists of 79 characters including special start and stop tokens.,4.1 Implementation Details,[0],[0]
Models were trained for six epochs.,4.1 Implementation Details,[0],[0]
"The Adam optimizer is used during training with a learning rate of 10−3 (Kingma and Ba, 2014).",4.1 Implementation Details,[0],[0]
"When updating the user embeddings during evaluation, we found that it is easier to use an optimizer without momentum.",4.1 Implementation Details,[0],[0]
"We use Adadelta (Zeiler, 2012) and tune the online learning rate to give the best perplexity on a held-out set of 12,000 queries, having previously verified that perplexity is a good indicator of performance on the QAC task.2
",4.1 Implementation Details,[0],[0]
"The language model is a single-layer characterlevel LSTM with coupled input and forget gates and layer normalization (Melis et al., 2018; Ba et al., 2016).",4.1 Implementation Details,[0],[0]
We do experiments on two model configurations: small and large.,4.1 Implementation Details,[0],[0]
The small models use an LSTM hidden state size of 300 and 20 dimensional user embeddings.,4.1 Implementation Details,[0],[0]
The large models use a hidden state size of 600 and 40 dimensional user embeddings.,4.1 Implementation Details,[0],[0]
Both sizes use 24 dimensional character embeddings.,4.1 Implementation Details,[0],[0]
"For the small sized models, we experimented with different values of the FactorCell rank hyperparameter between 30 and 50 dimensions finding that bigger rank is better.",4.1 Implementation Details,[0],[0]
The large sized models used a fixed value of 60 for the rank hyperparemeter.,4.1 Implementation Details,[0],[0]
"During training only and due to limited computational resources, queries are truncated to a length of 40 characters.
",4.1 Implementation Details,[0],[0]
Prefixes are selected uniformly at random with the constraint that they contain at least two characters in the prefix and that there is at least one character in the completion.,4.1 Implementation Details,[0],[0]
"To generate completions using beam search, we use a beam width of 100 and a branching factor of 4.",4.1 Implementation Details,[0],[0]
"Results are reported using mean reciprocal rank (MRR), the standard method of evaluating QAC systems.",4.1 Implementation Details,[0],[0]
"It is the mean of the reciprocal rank of the true completion in the
2Code at http://github.com/ajaech/query completion
top ten proposed completions.",4.1 Implementation Details,[0],[0]
"The reciprocal rank is zero if the true completion is not in the top ten.
",4.1 Implementation Details,[0],[0]
Neural models are compared against an MPC baseline.,4.1 Implementation Details,[0],[0]
"Following Park and Chiba (2017), we remove queries seen less than three times from the MPC training data.",4.1 Implementation Details,[0],[0]
Table 2 compares the performance of the different models against the MPC baseline on a test set of one million queries from a user population that is disjoint with the training set.,4.2 Results,[0],[0]
Results are presented separately for prefixes that are seen or unseen in the training data.,4.2 Results,[0],[0]
"Consistent with prior work, the neural models do better than the MPC baseline.",4.2 Results,[0],[0]
The personalized models are both better than the unadapted one.,4.2 Results,[0],[0]
"The FactorCell model is the best overall in both the big and small sized experiments, but the gain is mainly for the seen prefixes.
",4.2 Results,[0],[0]
Figure 1 shows the relative improvement in MRR over an unpersonalized model versus the number of queries seen per user.,4.2 Results,[0],[0]
"Both the Factor-
Cell and the ConcatCell show continued improvement as more queries from each user are seen, and the FactorCell outperforms the ConcatCell by an increasing margin over time.",4.2 Results,[0],[0]
"In the long run, we expect that the system will have seen many queries from most users.",4.2 Results,[0],[0]
"Therefore, the right side of Figure 1, where the relative gain of FactorCell is up to 2% better than that of the ConcatCell, is more indicative of the potential of these models for active users.",4.2 Results,[0],[0]
"Since the data was collected over a limited time frame and half of all users have fifteen or fewer queries, the results in Table 2 do not reflect the full benefit of personalization.
",4.2 Results,[0],[0]
Figure 2 shows the MRR for different prefix and query lengths.,4.2 Results,[0],[0]
We find that longer prefixes help the model make longer completions and (more obviously) shorter completions have higher MRR.,4.2 Results,[0],[0]
"Comparing the personalized model against the unpersonalized baseline, we see that the biggest gains are for short queries and prefixes of length one or two.
",4.2 Results,[0],[0]
We found that one reason why the FactorCell outperforms the ConcatCell is that it is able to pick up sooner on the repetitive search behaviors that some users have.,4.2 Results,[0],[0]
This commonly happens for navigational queries where someone searches for the name of their favorite website once or more per day.,4.2 Results,[0],[0]
At the extreme tail there are users who search for nothing but free online poker.,4.2 Results,[0],[0]
"Both models do well on these highly predictable users but the FactorCell is generally a bit quicker to adapt.
",4.2 Results,[0],[0]
We conducted case studies to better understand what information is represented in the user embeddings and what makes the FactorCell different from the ConcatCell.,4.2 Results,[0],[0]
From a cold start user embedding we ran two queries and allowed the model to update the user embedding.,4.2 Results,[0],[0]
"Then, we ranked
the most frequent 1,500 queries based on the ratio of their likelihood from before and after updating the user embeddings.
",4.2 Results,[0],[0]
"Tables 3 and 4 show the queries with the highest relative likelihood of the adapted vs. unadapted models after two related search queries: “high school softball” and “math homework help” for Table 3, and “Prada handbags” and “Versace eyewear” for Table 4.",4.2 Results,[0],[0]
"In both cases, the FactorCell model examples are more semantically coherent than the ConcatCell examples.",4.2 Results,[0],[0]
"In the first case, the FactorCell model identifies queries that a high school student might make, including entertainment sources and a celebrity entertainer popular with that demographic.",4.2 Results,[0],[0]
"In the second case, the FactorCell model chooses retailers that carry woman’s apparel and those that sell home goods.",4.2 Results,[0],[0]
"While these companies’ brands are not as luxurious as Prada or Versace, most of the top luxury brand names do not appear in the top 1,500 queries and our model may not be capable of being that specific.",4.2 Results,[0],[0]
There is no obvious semantic connection between the highest likelihood ratio phrases for the ConcatCell; it seems to be focusing more on orthography than semantics (e.g. “home” in the first example)..,4.2 Results,[0],[0]
Not shown are the queries which experienced the greatest decrease in likelihood.,4.2 Results,[0],[0]
"For the “high school” case, these included searches for travel agencies and airline tickets— websites not targeted towards the high school age demographic.",4.2 Results,[0],[0]
"While the standard implementation of MPC can not handle unseen prefixes, there are variants which do have that ability.",5 Related Work,[0],[0]
"Park and Chiba (2017) find that the neural LM outperforms MPC even when MPC has been augmented with the approach from Mitra and Craswell (2015) for handling rare
prefixes.",5 Related Work,[0],[0]
"There has also been work on personalizing MPC (Shokouhi, 2013; Cai et al., 2014).",5 Related Work,[0],[0]
We did not compare against these specific models because our goal was to show how personalization can improve the already-proven generative neural model approach.,5 Related Work,[0],[0]
"RNN’s have also previously been used for the related task of next query suggestion (Sordoni et al., 2015).
",5 Related Work,[0],[0]
Our results are not directly comparable to Park and Chiba (2017) or Mitra and Craswell (2015) due to differences in the partitioning of the data and the method for selecting random prefixes.,5 Related Work,[0],[0]
Prior work partitions the data by time instead of by user.,5 Related Work,[0],[0]
"Splitting by users is necessary in order to properly test personalization over longer time ranges.
",5 Related Work,[0],[0]
Wang et al. (2018) show how spelling correction can be integrated into an RNN language model query auto-completion system and how the completions can be generated in real time using a GPU.,5 Related Work,[0],[0]
"Our method of updating the model during evaluation resembles work on dynamic evaluation for language modeling (Krause et al., 2017), but differs in that only the user embeddings (latent demographic factors) are updated.",5 Related Work,[0],[0]
Our experiments show that the LSTM model can be improved using personalization.,6 Conclusion and Future Work,[0],[0]
The method of adapting the recurrent layer clearly matters and we obtained an advantage by using the FactorCell model.,6 Conclusion and Future Work,[0],[0]
The reason the FactorCell does better is in part attributable to having two to three times as many parameters in the recurrent layer as either the ConcatCell or the unadapted models.,6 Conclusion and Future Work,[0],[0]
"By design, the adapted weight matrix W′ only needs to be computed at most once per query and is reused many thousands of times during beam search.",6 Conclusion and Future Work,[0],[0]
"As a result, for a given latency budget, the FactorCell
model outperforms the Mikolov and Zweig (2012) model for LSTM adaptation.
",6 Conclusion and Future Work,[0],[0]
"The cost for updating the user embeddings is similar to the cost of the forward pass and depends on the size of the user embedding, hidden state size, FactorCell rank, and query length.",6 Conclusion and Future Work,[0],[0]
"In most cases there will be time between queries for updates, but updates can be less frequent to reduce computational costs.
",6 Conclusion and Future Work,[0],[0]
We also showed that language model personalization can be effective even on users who are not seen during training.,6 Conclusion and Future Work,[0],[0]
The benefits of personalization are immediate and increase over time as the system continues to leverage the incoming data to build better user representations.,6 Conclusion and Future Work,[0],[0]
The approach can easily be extended to include time as an additional conditioning factor.,6 Conclusion and Future Work,[0],[0]
We leave the question of whether the results can be improved by combining the language model with MPC for future work.,6 Conclusion and Future Work,[0],[0]
Query auto-completion is a search engine feature whereby the system suggests completed queries as the user types.,abstractText,[0],[0]
"Recently, the use of a recurrent neural network language model was suggested as a method of generating query completions.",abstractText,[0],[0]
We show how an adaptable language model can be used to generate personalized completions and how the model can use online updating to make predictions for users not seen during training.,abstractText,[0],[0]
The personalized predictions are significantly better than a baseline that uses no user information.,abstractText,[0],[0]
Personalized Language Model for Query Auto-Completion,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2019–2025, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Technologies are increasingly personalized, accommodating their behavior for each user.",1 Introduction,[0],[0]
Such personalization is done through user modeling where the goal is to “get to know” the user.,1 Introduction,[0],[0]
"To that end, personalization is based on users’ attributes, such as demographics (gender, age etc.), personalities, and preferences.",1 Introduction,[0],[0]
"For example, in Information Retrieval, results are customized according to the user’s information and search history (Speretta and Gauch, 2005), performance of Automatic Speech Recognition substantially improves when adapted to a specific speaker (Neumeyer et al., 1995), and Targeted Advertising makes use of the user’s location and prior purchases (Kölmel and Alexakis, 2002).
",1 Introduction,[0],[0]
Personalization in machine translation has a somewhat different nature.,1 Introduction,[0],[0]
"Providers of MT tools and services offer means to “customize” or “personalize” the translation engine for each client, mostly through domain adaptation techniques, and a great deal of effort is made to make the human-involved translation process more efficient (see Section 2.2).",1 Introduction,[0],[0]
"Most of the focus, though, goes to customization for companies or professional translators.",1 Introduction,[0],[0]
"We argue that Personalized Machine Translation (PMT below) should and can take the next step and directly address individual end-users.
∗This work was done while the first author was at Xerox Research Centre Europe.
",1 Introduction,[0],[0]
The difficulty to objectively determine whether one (automatic) translation is better than another has been repeatedly revealed in the MT literature.,1 Introduction,[0],[0]
"Our conjecture is that one reason is individual preferences, to which we refer as Translational Preferences (TP).",1 Introduction,[0],[0]
"TP come into play both when the alternative translations are all correct, and when each of them is wrong in a different way.",1 Introduction,[0],[0]
"In the former case, a preference may be a stylistic choice, and in the latter, a matter of comprehension or a selection of the least intolerable error in one’s opinion.",1 Introduction,[0],[0]
"For instance, one user may prefer shorter sentences than others; she may favor a more formal style, while another would rather have it casual.",1 Introduction,[0],[0]
A user could be fine with some reordering errors but be more picky concerning punctuations.,1 Introduction,[0],[0]
"One user will not be bothered if some words are left untranslated (perhaps because the source language belongs to the same language family as the target language that he speaks), while another will find it utterly displeasing.",1 Introduction,[0],[0]
"Such differences may be the result of the type of translation system being employed (e.g. syntax- vs. phrased-based), the specific training data or many other factors.",1 Introduction,[0],[0]
"On the user’s side, a preference may be attributed, for example, to her mother tongue, her age or her personality.
",1 Introduction,[0],[0]
"Two aspects of end-user PMT may be considered: (i) Personalized translation of texts written by a specific user, and (ii) PMT to provide better translations for a specific reader.",1 Introduction,[0],[0]
"In this work we address the second task, aiming to identify translations each user is more likely to prefer.1 Specifically, we consider a setting where at least two MT systems are available, and the goal is to predict which of the translation systems the user would choose, assuming we have no knowledge about her preference between them.",1 Introduction,[0],[0]
"Benchmarking the systems in advance with respect to a reference set, or estimating the quality of the translations (Specia et al., 2009) are viable alternatives for translation selection; these, however, are not personalized to the target user.",1 Introduction,[0],[0]
"Instead, we employ a user-user Collaborative Filtering approach, common in Recommender Systems, which we map to the TP prediction task.
",1 Introduction,[0],[0]
"We assess this approach using a collection of user rankings of MT systems from a shared translation task
1In (Mirkin et al., 2015)",1 Introduction,[0],[0]
"we investigate the first task, assessing whether the author’s demographic and personality traits are preserved over machine translation.
2019
(see Section 3).",1 Introduction,[0],[0]
"Our results show that the personalized method modestly, but consistently, outperforms several other approaches that rank the systems in general, disregarding the specific user.",1 Introduction,[0],[0]
We consider this as an indication that user feedback can be employed towards a more personalized approach to machine translation.,1 Introduction,[0],[0]
"Collaborative filtering (CF) is a common approach employed by recommender systems for suggesting to users items, such as books or movies.",2.1 Collaborative filtering,[0],[0]
"A recommender system may simply suggest to all users the most popular items; often, however, the recommendations are personalized for each individual user to fit her taste or preferences.",2.1 Collaborative filtering,[0],[0]
User-user CF relies on community preferences.,2.1 Collaborative filtering,[0],[0]
"The idea is to recommend to the user items that are liked by users similar to her, as manifested, for example, by high rating.",2.1 Collaborative filtering,[0],[0]
Similar users are those that agree with the current user on previously-rated items.,2.1 Collaborative filtering,[0],[0]
"In k-nearest-neighbors CF, a user is typically represented by a vector of her preferences, where each entry of the vector is, e.g., a rating of a movie.",2.1 Collaborative filtering,[0],[0]
k similar users are then identified by measuring the similarity between the users’ vectors.,2.1 Collaborative filtering,[0],[0]
"Cosine similarity is a popular function for that purpose, and we also use it in our work (Resnick et al., 1994; Sarwar et al., 2001; Ricci et al., 2011).",2.1 Collaborative filtering,[0],[0]
"An alternative to cosine, Pearson’s correlation coefficient (Pearson, 1895), allows addressing different rating patterns across users.",2.1 Collaborative filtering,[0],[0]
"In comparison to cosine, here vector entries are normalized with respect to the user’s average rating.",2.1 Collaborative filtering,[0],[0]
"In our case, such normalization is not very meaningful since the entries of the users vectors represent comparisons rather than absolute ratings, as will be made clear in Section 4.",2.1 Collaborative filtering,[0],[0]
"Nevertheless, we have experimented with Pearson correlation as well, and found no advantage in using it instead of cosine.",2.1 Collaborative filtering,[0],[0]
"Various means of customization and personalization are available, in both academic and commercial MT.","2.2 Customization, personalization and adaptation in MT",[0],[0]
"Many of them target the company, rather than the individual user, and much of the effort is invested in designing tools for professional translators, aiming to improve their productivity, through intelligent Computer Aided Translation (CAT).
","2.2 Customization, personalization and adaptation in MT",[0],[0]
"Domain adaptation methods are commonly used to adapt to the topic, the genre and even the style of the translated material.","2.2 Customization, personalization and adaptation in MT",[0],[0]
"Using the company’s own corpora is one of the simplest techniques to do so, but many more approaches have been proposed, including dataselection (Axelrod et al., 2011; Gascó","2.2 Customization, personalization and adaptation in MT",[0],[0]
"et al., 2012; Mirkin and Besacier, 2014), mixture models (Foster and Kuhn, 2007) and table fill-up (Bisazza et al., 2011).","2.2 Customization, personalization and adaptation in MT",[0],[0]
"Clients can utilize their own glossaries (Federico et al., 2014), corpora (parallel or monolingual) and translation memories (TM), either shared or private ones
(Caskey and Maskey, 2014; Federico et al., 2014).","2.2 Customization, personalization and adaptation in MT",[0],[0]
"Through Adaptive and Interactive MT (Nepveu et al., 2004), the system learns from the translator’s edits, in order to avoid repeating errors that have already been corrected.","2.2 Customization, personalization and adaptation in MT",[0],[0]
"Post-editions can continuously be added to the translator’s TM or be used as additional training material, for tighter adaptation to the domain of interest, through batch or incremental training.","2.2 Customization, personalization and adaptation in MT",[0],[0]
Many tasks that require annotation by humans are affected by the annotator and not only by the item being judged.,2.3 User preferences in MT,[0],[0]
"Metrics for inter-rater reliability or interannotator agreement, such as Cohen’s Kappa (Cohen, 1960), help measuring the extent to which annotators disagree.",2.3 User preferences in MT,[0],[0]
"Disagreement may be due to untrained or inattentive annotators, a result of a task that is not well defined, or when there is no obvious “truth”.",2.3 User preferences in MT,[0],[0]
Such is the case with the evaluation of translation quality – it is not always straightforward to tell whether one translation is better than another.,2.3 User preferences in MT,[0],[0]
A single sentence can be translated in multiple correct ways.,2.3 User preferences in MT,[0],[0]
The decision becomes even harder when the translations are automatically produced and are imperfect: Is one error worse than another?,2.3 User preferences in MT,[0],[0]
The answer is in the eye of the beholder.,2.3 User preferences in MT,[0],[0]
"MT papers regularly report rather low Kappa levels, even when measured on simpler tasks, such as short segments (Macháček and Bojar, 2015).
",2.3 User preferences in MT,[0],[0]
Turchi et al. (2013) refer to the issue of “subjectivity” of human annotators.,2.3 User preferences in MT,[0],[0]
"They address the task of binary classification of “good” vs. “bad” translations, and show that relying on human annotation for training a binary quality estimator is less effective than using automatically-generated labels.",2.3 User preferences in MT,[0],[0]
This subjectivity is exactly what we are after.,2.3 User preferences in MT,[0],[0]
"We treat it as a preference, trying to identify the systems or specific translations which the user subjectively prefers.
Kichhoff et al. (2012) analyze user preferences with respect to MT errors.",2.3 User preferences in MT,[0],[0]
"They show that some types, e.g. word order errors, are the most dis-preferred by users, and that this is a more important factor than the number of errors.",2.3 User preferences in MT,[0],[0]
"While very relevant for our research, their analysis is aggregated over all users participating in the study, and is not focusing on individuals’ preferences.",2.3 User preferences in MT,[0],[0]
"In this work we used the data provided for the MT Shared Task in the 2013 Workshop on Statistical Machine Translation (WMT) (Bojar et al., 2013).2",3 Data,[0],[0]
"This data was of a particularly large scale, with crowdsourced human judges, either volunteer researchers or paid Amazon Turkers.",3 Data,[0],[0]
"For each source sentence, a judge was presented with the source sentence itself, a reference translation, and the outputs of five machine translation systems.",3 Data,[0],[0]
"The five systems were randomly selected from the pool of participating systems,
2http://www.statmt.org/wmt13/ translation-task.html
and were anonymized and randomly-ordered when presented to the judge.",3 Data,[0],[0]
"The judge had to rank the translations, with ties allowed (i.e. two system can receive the same ranking).",3 Data,[0],[0]
"Hence, each annotation point provided with 10 pairwise rankings between systems.",3 Data,[0],[0]
"Translations of 10 language pairs were assessed, with 11 to 19 systems for each pair.",3 Data,[0],[0]
"In total, over 900K non-tied pairwise rankings were collected.",3 Data,[0],[0]
"The Turkers’ annotation included a control task for quality assurance, rejecting Turkers failing more than 50% of the control points.",3 Data,[0],[0]
The inter-annotator score showed on average a fair to moderate level of agreement.,3 Data,[0],[0]
"Our method, denoted CTP (Collaborative Translational Preferences), is based on a k-nearest-neighbors approach for user-user CF.",4 Translational preferences with collaborative filtering,[0],[0]
"That is, we predict the translational preferences of a user based on those of similar users.",4 Translational preferences with collaborative filtering,[0],[0]
"In our setting, a user preference is the choice between two translation systems – which system’s translations does the user prefer.",4 Translational preferences with collaborative filtering,[0],[0]
"Given two systems (or models of the same system) we wish to predict which one the user would prefer, without assuming the user has ever expressed her preference between these two specific systems.",4 Translational preferences with collaborative filtering,[0],[0]
"It is important to emphasize that the method presented here considers the users’ overall preferences of systems, and does not regard the specific sentence that is being translated.",4 Translational preferences with collaborative filtering,[0],[0]
In future work we intend to make use of this information as well.,4 Translational preferences with collaborative filtering,[0],[0]
"As mentioned in Section 3, each annotation consists of a ranking of five systems.",4.1 Representation,[0],[0]
"From that, we extract pairwise rankings for every pair of systems that were ranked for a given language pair.",4.1 Representation,[0],[0]
"For each user u ∈ U (where U are all users who annotated the language pair), we create a user-preference vector, pu, that contains an entry for each pair of translation systems.",4.1 Representation,[0],[0]
"Denoting the set of systems with S, we have |S|·(|S|−1)2 system pairs.",4.1 Representation,[0],[0]
"E.g., for Czech-English, with 11 participating systems, the user vector size is 55.",4.1 Representation,[0],[0]
"Each entry (i, j) of the vector is assigned the following value:
pu (i,j) =
w",4.1 Representation,[0],[0]
"(i,j) u − l(i,j)u w",4.1 Representation,[0],[0]
"(i,j) u +",4.1 Representation,[0],[0]
"l (i,j) u
(1)
where w(i,j)u and l (i,j) u are the number of wins and loses of system si vs. system sj as judged by user u.3
With this representation, a user vector contains values between −1 (if si always lost to sj) and 1 (if si always won).",4.1 Representation,[0],[0]
"If the user always ranked the two systems identically, the value is 0, and if she has never evaluated the pair, the entry is regarded as a missing value (NA).",4.1 Representation,[0],[0]
"Altogether, we have a matrix of users by system pairs, as depicted in Figure 1.
",4.1 Representation,[0],[0]
3We have also considered including ties in the denominator of the equation; discarding them was found superior.,4.1 Representation,[0],[0]
"Given a user preference to predict for a pair of systems (si, sj), we compute the similarity between pu and each one of pu′ for all other u′ ∈ U .",4.2 Finding similar users,[0],[0]
In our experiments we used cosine as the similarity measure.,4.2 Finding similar users,[0],[0]
The k most-similar-users (MSU ) are then selected.,4.2 Finding similar users,[0],[0]
"To be included in MSU (u), we require that u and u′ have judged at least 2 common system pairs.",4.2 Finding similar users,[0],[0]
"Given the similarity scores, to predict the user’s preference for the target system pair, we compute a weighted average of the predictions of the users in MSU (u).
",4.3 Preference prediction,[0],[0]
We include in the average only users with similarity scores above a certain positive threshold (0.05).,4.3 Preference prediction,[0],[0]
We then require that a minimum number of users meet the above criteria of common annotations and minimum similarity (we used 5).,4.3 Preference prediction,[0],[0]
"If not enough such similar users are found, we turn to a fallback, where we use the non-weighted average preference across all users (AVPF presented in Section 5).4 The prediction is then the sign of the weighted average.",4.3 Preference prediction,[0],[0]
"A positive value means si is the preferred system; a negative one means it is sj , and a zero is a draw.",4.3 Preference prediction,[0],[0]
"In our evaluation we compare this prediction to the sign of the actual preference of the user, pu(i,j).",4.3 Preference prediction,[0],[0]
"Formally, CTP computes the following prediction function f for a given user u and a system pair (si, sj):
fCTP(u)(i,j) = sign( ∑ u′ pu′",4.3 Preference prediction,[0],[0]
"(i,j) · sim(u, u′)∑
u′ sim(u, u′) )",4.3 Preference prediction,[0],[0]
"(2)
where u′ ∈ MSU (u) are the most similar users (the nearest neighbors) of u; pu′ (i,j) are the preferences of user u′ for (si, sj) and sim(u, u′) is the similarity score between the two users.5",4.3 Preference prediction,[0],[0]
"In our experiments we try to predict which one of two translation systems would be preferred by a given user.
4The fallback was used 0.1% of the times.",5.1 Evaluation methodology,[0],[0]
5The denominator is not required as long as we predict only the sign since all used similarity scores are positive.,5.1 Evaluation methodology,[0],[0]
"We keep it in order to obtain a normalized score that can be used for other decisions, e.g. ranking multiple systems.
",5.1 Evaluation methodology,[0],[0]
"We evaluate our method, as well as several other prediction functions, when compared with the user’s pairwise system preference according to the annotation – pu
(i,j), shown in Equation 1.",5.1 Evaluation methodology,[0],[0]
"For each user this is an aggregated figure over all her pairwise rankings for the pair, determining the preferred system as the one chosen by the user (i.e. ranked higher) more times.
",5.1 Evaluation methodology,[0],[0]
We conduct a leave-one-out experiment.,5.1 Evaluation methodology,[0],[0]
"For each language pair, we iterate over all non-NA entries in the user-preferences matrix, remove the entry and try to predict it.",5.1 Evaluation methodology,[0],[0]
"User similarity scores are re-computed for each evaluation point, to ensure they do not consider the target pair.",5.1 Evaluation methodology,[0],[0]
"The “gold” preference is positive when the user prefers si, negative when she prefers sj and 0 when she has no preference between them.",5.1 Evaluation methodology,[0],[0]
"Hence, each of the assessed methods is measured by the accuracy of predicting the sign of the preference.",5.1 Evaluation methodology,[0],[0]
"We compare CTP to the following prediction methods:
Always i (ALI)",5.2 Non-personalized methods,[0],[0]
This is a naı̈ve baseline showing the score when always predicting that system i wins.,5.2 Non-personalized methods,[0],[0]
"Note that the baseline is not simply 50% due to ties.
",5.2 Non-personalized methods,[0],[0]
Average rank (RANK),5.2 Non-personalized methods,[0],[0]
"Here, two systems are compared by the average of their rankings across all annotations (r ∈ {1, 2, 3, 4, 5}):
fRANK(u)(i,j) = sign(rj − ri) (3) rj and ri are the average ranks of sj and si respectively.",5.2 Non-personalized methods,[0],[0]
"Since a smaller value of r corresponds to a higher rank, we subtract the rank of si from sj and not the other way around.",5.2 Non-personalized methods,[0],[0]
"This way, if for instance, si is ranked on averaged higher than sj , the prediction would be positive, as desired.
",5.2 Non-personalized methods,[0],[0]
Expected (EXPT),5.2 Non-personalized methods,[0],[0]
"This metric, proposed by Koehn (2012) and used by Bojar et al. (2013) in order to rank the participating systems in the WMT benchmark, compares the expected wins of the two systems.",5.2 Non-personalized methods,[0],[0]
"Its intuition is explained as follows: “If the system is compared against a randomly picked opposing system, on a randomly picked sentence, by a randomly picked judge, what is the probability that its translation is ranked higher?”",5.2 Non-personalized methods,[0],[0]
"The expected wins of si, e(si), is the probability of si to win when compared to another system, estimated as the total number of wins of si relative to the total number of comparisons involving it, excluding ties, and normalized by the total number of systems excluding si, |{sk}|:
e(i)",5.2 Non-personalized methods,[0],[0]
"= 1 |{sk}| ∑ k 6=i
w(i,k)
w(i,k) + l(i,k) (4)
where w(i,k) and l(i,k) are summed over all users.",5.2 Non-personalized methods,[0],[0]
"The preference prediction is therefore:
fEXPT(u)(i,j) = sign(e(i)− e(j)) (5)
RANK and EXPT predict preferences based on a system’s performance in general, when compared to all other systems.",5.2 Non-personalized methods,[0],[0]
"We propose an additional prediction function for comparison which uses only the information concerning the system pair under consideration.
",5.2 Non-personalized methods,[0],[0]
Average user preference (AVPF),5.2 Non-personalized methods,[0],[0]
This method takes into account only the specific system pair and averages the user preferences for the pair.,5.2 Non-personalized methods,[0],[0]
"Formally:
fAVPF(u)(i,j) = sign( ∑ u′ p (i,j) u′
|{u′}| ) (6)
where u′ 6= u, and {u′} are all users except u.",5.2 Non-personalized methods,[0],[0]
"This method can be viewed as a non-personalized version of CTP, with two differences:
(1) It considers all users, and not only similar ones.",5.2 Non-personalized methods,[0],[0]
"(2) It does not weight the preferences of the other
users by their similarity to the target user.",5.2 Non-personalized methods,[0],[0]
Table 1 shows the results of an experiment comparing the performance of the various methods in terms of prediction accuracy.,5.3 Results,[0],[0]
"Figure 2 shows the micro-average scores, when giving each of the 97,412 test points an equal weight in the average.",5.3 Results,[0],[0]
"CTP outperforms all others for 9 out of 10 language pairs, and in the overall microaveraged results.",5.3 Results,[0],[0]
"The difference between CTP and each of the other metrics was found statistically significance with p < 5 · 10−6 at worse, as measured with a paired Wilcoxon signed rank test (Wilcoxon, 1945) on the predictions of the two methods.",5.3 Results,[0],[0]
"The significance test captures in this case the fact that the methods disagreed in many more cases than is visible by the score difference.
",5.3 Results,[0],[0]
"Our method was found superior to all others also when computing macro-average, taking the average of the scores of each language pair, as well as when the ties are included in the computation of pu.
",5.3 Results,[0],[0]
The parameters with which the above results were obtained are found within the method’s description in Section 4.,5.3 Results,[0],[0]
"Yet, in our experiments, CTP turned out to be rather insensitive to their values.",5.3 Results,[0],[0]
In this experiment we used a global set of parameters and did not tune them for each language pair separately.,5.3 Results,[0],[0]
It is reasonable to assume that such tuning would improve results.,5.3 Results,[0],[0]
"For instance, choosing k, the number of users to include in the average, depends on the total number of users.",5.3 Results,[0],[0]
"E.g., for en-es, where there are only 57 users in total, reducing k’s value from 50 to 25, improves results of CTP from 62.6% to 63.2%, higher than all other methods (whose scores are not affected).
",5.3 Results,[0],[0]
"Specifically in comparison to AVPF, weighting by the similarity scores was found to be a more significant factor than selecting a small subset of the users.",5.3 Results,[0],[0]
"This may not come as a surprise, since less similar users that are added to MSU (u) have a smaller impact on the final decision since their weight in the average is smaller.
",5.3 Results,[0],[0]
"One weakness of CTP, as well as of other methods, is that it poorly predict ties.",5.3 Results,[0],[0]
"In the above experiment, approximately 13.5% of the preferences were 0, none of them was correctly identified.",5.3 Results,[0],[0]
Our analysis showed that numerical accuracy is not the main cause; setting any prediction that is smaller than some values of |ε| to 0 was not found helpful.,5.3 Results,[0],[0]
"Arguably, ties need not be predicted, since if the user has no preference between two systems, any choice is just as good.",5.3 Results,[0],[0]
"Still, we believe that better ties prediction could lead to general improvement of our method and we wish to address it in future work.",5.3 Results,[0],[0]
We addressed the task of predicting user preference with respect to MT output via a collaborative filtering approach whose prediction is based on preferences of similar users.,6 Discussion,[0],[0]
This method predicts TP better than a set of non-personalized methods.,6 Discussion,[0],[0]
"The gain is modest in absolute numbers, but the results are highly statistically significant and stable over parameter values.
",6 Discussion,[0],[0]
We consider this work as a step towards more personalized MT.,6 Discussion,[0],[0]
This line of research can be extended in multiple ways.,6 Discussion,[0],[0]
"First and foremost, as mentioned, we did not consider the actual content of the sentences, but rather identified a general preference for one system over another.",6 Discussion,[0],[0]
"It is plausible, however, that one system is better – from the user’s perspective – at translating one type of text, while another is preferred for other texts.",6 Discussion,[0],[0]
"Taking the actual texts into account seems therefore es-
sential.",6 Discussion,[0],[0]
Content-based methods for recommender systems may be useful for this purpose.,6 Discussion,[0],[0]
"Another factor that may be affecting preferences is translation quality: when compared translations are all poor, preferences play a less significant role.",6 Discussion,[0],[0]
"Hence, it may be informative to assess TP prediction separately across different levels of translation quality.
",6 Discussion,[0],[0]
Large parallel corpora are typically required for training reasonable statistical translation models.,6 Discussion,[0],[0]
"Yet, parallel corpora, and even more so in-domain ones, are hard to gather.",6 Discussion,[0],[0]
"It is virtually impossible to find a user-specific parallel corpus, and methods for monolingual domain adaptation are easier to envisage if one wishes to address author-aware PMT (the first PMT task mentioned in Section 1).",6 Discussion,[0],[0]
"Collecting user feedback is another challenge, especially since most endusers do not speak the source language.",6 Discussion,[0],[0]
"For that and other reasons, it currently seems more feasible to collect preference information from professional translators, explicitly or implicitly.",6 Discussion,[0],[0]
"Yet, in this research we aim at end-users rather than translators whose preferences are often driven by the ease of correction more than anything else.",6 Discussion,[0],[0]
"We believe that one way to tackle this issue is to exploit other kinds of feedback, from which we can infer user preferences and similarity.",6 Discussion,[0],[0]
Online MT providers are recently collecting end-user feedback for their proposed translations which may be useful for TP prediction.,6 Discussion,[0],[0]
"For instance, in early 2015 Facebook introduced a feature letting users rate (Bing) translations, and Google Translate asks for suggested improvements.",6 Discussion,[0],[0]
We are hopeful that such data becomes publicly available.,6 Discussion,[0],[0]
"Nevertheless, it remains unlikely to obtain feedback from each and every user.",6 Discussion,[0],[0]
"A potential direction for both corpora and feedback collection is personalizing models and identifying preferences for groups of users based on socio-demographic traits, such as gender, age or mother tongue, or based on (e.g. Big 5) personality traits.",6 Discussion,[0],[0]
These can even be inferred by automatically analyzing user texts.,6 Discussion,[0],[0]
We wish to thank Hervé Déjean and the EMNLP reviewers for their valuable feedback on this work.,Acknowledgments,[0],[0]
"Machine Translation (MT) has advanced in recent years to produce better translations for clients’ specific domains, and sophisticated tools allow professional translators to obtain translations according to their prior edits.",abstractText,[0],[0]
We suggest that MT should be further personalized to the end-user level – the receiver or the author of the text – as done in other applications.,abstractText,[0],[0]
"As a step in that direction, we propose a method based on a recommender systems approach where the user’s preferred translation is predicted based on preferences of similar users.",abstractText,[0],[0]
"In our experiments, this method outperforms a set of non-personalized methods, suggesting that user preference information can be employed to provide better-suited translations for each user.",abstractText,[0],[0]
Personalized Machine Translation: Predicting Translational Preferences,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 208–215 New Orleans, Louisiana, June 1 - 6, 2018. c©2017 Association for Computational Linguistics",text,[0],[0]
"Predicting the next characters or words following a prefix has had multiple uses from helping handicapped people (Swiffin et al., 1987) to, more recently, helping search engine users (Cai et al., 2016).",1 Introduction,[0],[0]
"In practice, most search engines today use query auto completion (QAC) systems, consisting of suggesting queries as users type in the search box (Fiorini et al., 2017).",1 Introduction,[0],[0]
"The task suffers from high dimensionality, because the number of possible solutions increases as the length of the target query increases.",1 Introduction,[0],[0]
"Historically, the query prediction task has been addressed by relying on query logs, particularly the popularity of past queries (BarYossef and Kraus, 2011; Lu et al., 2009).",1 Introduction,[0],[0]
"The idea is to rely on the wisdom of the crowd, as popular
queries matching a typed prefix are more likely to be the user’s intent.
",1 Introduction,[0],[0]
"This traditional approach is usually referred to as MostPopularCompletion (MPC)(Bar-Yossef and Kraus, 2011).",1 Introduction,[0],[0]
"However, the performance of MPC is skewed: it is very high for popular queries and very low for rare queries.",1 Introduction,[0],[0]
"At the extreme, MPC simply cannot predict a query it has never seen.",1 Introduction,[0],[0]
"This becomes a bigger problem in academic search (Lankinen et al., 2016), where systems are typically less used, with a wider range of possible queries.",1 Introduction,[0],[0]
"Recent advances in deep learning, particularly in semantic modeling (Mitra and Craswell, 2015) and neural language modeling (Park and Chiba, 2017) showed promising results for predicting rare queries.",1 Introduction,[0],[0]
"In this work, we propose to improve the state-of-the-art approaches in neural QAC by integrating personalization and time sensitivity information as well as addressing current MPC limitations by diversifying the suggestions, thus approaching a production-ready architecture.",1 Introduction,[0],[0]
"While QAC has been well studied, the field has recently started to shift towards deep learningbased models, which can be categorized into two main classes: semantic models (using Convolutional Neural Nets, or CNNs) (Mitra and Craswell, 2015) and language models (using Recurrent Neural Nets, or RNNs) (Park and Chiba, 2017).",2.1 Neural query auto completion,[0],[0]
"Both approaches are frequently used in natural language processing in general (Kim et al., 2016) and tend to capture different features.",2.1 Neural query auto completion,[0],[0]
"In this work, we focus on RNNs as they provide a flexible solution to generate text, even when it is not previously seen in the training data.
",2.1 Neural query auto completion,[0],[0]
"Yet, recent work in this field (Park and Chiba, 2017) suffers from some limitations.",2.1 Neural query auto completion,[0],[0]
"Most importantly, the probability estimates for full queries
208
are directly correlated to the length of the suggestions, consequently favoring shorter queries in some cases and hampering some predictions (Park and Chiba, 2017).",2.1 Neural query auto completion,[0],[0]
"By appending these results to MPC’s and re-ranking the list with LambdaMART (Burges, 2010) in another step as suggested in previous work (Mitra and Craswell, 2015), they achieve state-of-the-art performance in neural query auto completion at the cost of a higher complexity and more computation time.",2.1 Neural query auto completion,[0],[0]
"Still, these preliminary approaches have yet to integrate standards in QAC, e.g. query personalization (Koutrika and Ioannidis, 2005; Margaris et al., 2018) and time sensitivity (Cai et al., 2014).",2.2 Context information,[0],[0]
This integration has to differ from traditional approaches by taking full advantage of neural language modeling.,2.2 Context information,[0],[0]
"For example, neural language models could be refined to capture interests of some users as well as their actual language or query formulation.",2.2 Context information,[0],[0]
"The same can apply to timesensitivity, where the probability of queries might change over time (e.g. for queries such as “tv guide”, or “weather”).",2.2 Context information,[0],[0]
"Furthermore, the feasibility of these approaches in real-world settings has not been demonstrated, even more so on specialized domains.
",2.2 Context information,[0],[0]
"By addressing these issues, we make the following contributions in this work compared to the previous approaches:
• We propose a more straightforward architecture with improved scalability;
• Our method integrates user information when available as well as time-sensitivity;
• We propose to use a balanced beam search for ensuring diversity;
• We test on a second dataset and compare the generalizability of different methods in a specialized domain;
•",2.2 Context information,[0],[0]
"Our method achieves stronger performance than the state of the art on both datasets.
",2.2 Context information,[0],[0]
"Finally, our source code is made available in a public repository1.",2.2 Context information,[0],[0]
"This allows complete reproducibility of our results and future comparisons.
1https://github.com/ncbi-nlp/NQAC",2.2 Context information,[0],[0]
"The justification of using a neural language model for the task of predicting queries is that it has been proven to perform well to generate text that has never been seen in the training data (Sutskever et al., 2011).",3.1 Personalized neural Language Model,[0],[0]
"Particularly, character-level models work with a finer granularity.",3.1 Personalized neural Language Model,[0],[0]
"That is, if a given prefix has not been seen in the training data (e.g. a novel or incomplete word), the model can use the information shared across similar prefixes to make a prediction nonetheless.
",3.1 Personalized neural Language Model,[0],[0]
Recurrent Neural Network,3.1 Personalized neural Language Model,[0],[0]
The difficulty of predicting queries given a prefix is that the number of candidates explodes as the query becomes longer.,3.1 Personalized neural Language Model,[0],[0]
"RNNs allow to represent each character (or word) of a sequence as a cell state, therefore reducing the dimensionality of the task.",3.1 Personalized neural Language Model,[0],[0]
"However, they also introduce the vanishing gradient problem during backpropagation, preventing them from learning long-term dependencies.",3.1 Personalized neural Language Model,[0],[0]
"Both gated recurrent units (GRU) (Cho et al., 2014) and long-short term memory cells (LSTMs) solve this limitation — albeit with a different approach — and are increasingly used.",3.1 Personalized neural Language Model,[0],[0]
"In preliminary experiments, we tried various forms of RNNs: vanilla RNNs, GRUs and LSTMs.",3.1 Personalized neural Language Model,[0],[0]
"GRUs performed similarly to LSTM with a smaller computational complexity due to fewer parameters to learn as was previously observed (Jozefowicz et al., 2015).
",3.1 Personalized neural Language Model,[0],[0]
Word embedded character-level Neural Language Model,3.1 Personalized neural Language Model,[0],[0]
"The main novelty in (Park and Chiba, 2017) is to combine a character-level neural language model with a word-embedded space character.",3.1 Personalized neural Language Model,[0],[0]
"The incentive is that character-level neural language models benefit from a finer granularity for predictions but they lack the semantic understanding words-level models provide, and vice versa.",3.1 Personalized neural Language Model,[0],[0]
"Therefore, they encode text sequences using one-hot encoding of characters, character embedding and pre-trained word embedding (using word2vec (Mikolov et al., 2013)) of the previous word when a space character is encountered.",3.1 Personalized neural Language Model,[0],[0]
"Our preliminary results showed that the character embedding does not bring much to the learning, so we traded it with the context feature vectors below to save some computation time while enriching the model with additional, diverse information.
",3.1 Personalized neural Language Model,[0],[0]
"User representation We make the assumption that the way a user types a query is a function of their actual language/vocabulary, but also a function of their interests.",3.1 Personalized neural Language Model,[0],[0]
"Therefore, a language model could capture these user characteristics to better predict the query, if we feed the learner with the information.",3.1 Personalized neural Language Model,[0],[0]
"Each query qi is a set of words such that qi = {w1, ..., wn}.",3.1 Personalized neural Language Model,[0],[0]
"U is a column matrix and a user u ∈ U is characterized by the union of words in their k past queries, i.e. Qu = ∪ki=1qi.",3.1 Personalized neural Language Model,[0],[0]
"The objective is to reduce, for each user, the vocabulary used in their queries to a vector of a dimensionality d of choice, or Qu → Rd.",3.1 Personalized neural Language Model,[0],[0]
"We chose d = 30, in order to stay in the same computation order of previous work using character embedding (Park and Chiba, 2017).",3.1 Personalized neural Language Model,[0],[0]
"To this end, we adapted the approach PV-DBOW detailed in (Le and Mikolov, 2014).",3.1 Personalized neural Language Model,[0],[0]
"That is, at each training iteration, a random word wi is sampled from Qu.",3.1 Personalized neural Language Model,[0],[0]
"The model is trained by maximizing the probability of predicting the user u given the word wi, i.e.:
1 |U | ∑
u∈U
∑
wi∈Qu log P (u|wi).",3.1 Personalized neural Language Model,[0],[0]
"(1)
The resulting vectors are stored for each user ID and are used as input for the neural net (NN) (see Architecture section).
",3.1 Personalized neural Language Model,[0],[0]
"Time representation As an example, in the background data (see Section 4.1), the query “tv guide” appears 1,682 times and it is vastly represented in evening and nights.",3.1 Personalized neural Language Model,[0],[0]
"For this reason, we propose to integrate time features in the language model.",3.1 Personalized neural Language Model,[0],[0]
"While there has been more elaborated approaches to model it in the past (Shokouhi and Radinsky, 2012), we instead propose a straightforward encoding and leave the rest of the work to the neural net.",3.1 Personalized neural Language Model,[0],[0]
"For each query, we look at the time it was issued, consisting of hour x , minute y and second z, and we derive the following features:
sin
( 2π(3600x+ 60y + z)
86400
) ,
cos
( 2π(3600x+ 60y + z)
86400
) .
",3.1 Personalized neural Language Model,[0],[0]
"(2)
This encoding has the benefit of belonging to [−1, 1], which is a range comparable to the rest of the features.",3.1 Personalized neural Language Model,[0],[0]
"It is also capable to model cyclic data, which is important particularly around boundaries (e.g. considering a query at 11:55PM
and another at 00:05AM).",3.1 Personalized neural Language Model,[0],[0]
"We proceed the same way to encode weekdays and we end up with four time features.
",3.1 Personalized neural Language Model,[0],[0]
Overall architecture An overview of the architecture is proposed in Figure 1.,3.1 Personalized neural Language Model,[0],[0]
"The input of our neural language model is a concatenation of the vectors defined above, for each character and for each query in the training set.",3.1 Personalized neural Language Model,[0],[0]
"We use zeropadding after the “\n” character to keep the sequence length consistent, and the NN learns to recognize it.",3.1 Personalized neural Language Model,[0],[0]
"We feed this input vector into 2 layers of 1024 GRUs2, each followed by a dropout layer (with a dropout rate of 50%) to prevent overfitting.",3.1 Personalized neural Language Model,[0],[0]
Each GRU cell is activated with ReLu(x) =,3.1 Personalized neural Language Model,[0],[0]
x+ and gradients are clipped to a norm of 0.5 to avoid gradient exploding problems.,3.1 Personalized neural Language Model,[0],[0]
"The output of the second dropout layer is fed to a temporal softmax layer, which allows to make predictions at each state.",3.1 Personalized neural Language Model,[0],[0]
"The softmax function returns the probability P (ci|c1, ..., ci−1) of the character ci given the previous characters of the sequence, which is then used to calculate the loss function by comparing it to the next character in the target query.",3.1 Personalized neural Language Model,[0],[0]
"Instead of using the objective denoted in (Park and Chiba, 2017), we minimize the loss L defined as the average cross entropy of this probability with the reference probability P̂ (ci) across all queries, that is
L =
− 1|Q| ∑
q∈Q
|q|−1∑
i=1
P̂ (ci+1)× log P (ci+1|c1, ..., ci).
",3.1 Personalized neural Language Model,[0],[0]
"(3)
Q is the set of queries in the training dataset, |Q| is the total number of queries in the set and |q| is the number of characters in the query q. Convergence stabilizes around 5-10 epochs for the AOL dataset (depending on the model) and 15-20 epochs for the biomedical specialized dataset (see Section 4.1).",3.1 Personalized neural Language Model,[0],[0]
"The straightforward approach for decoding the most likely output sequence — in this case, a suffix given a prefix — is to use a greedy approach.",3.2 Balanced diverse beam search,[0],[0]
"That is, we feed the prefix into the trained NN and pick the most likely output at every step, until the sequence is complete.",3.2 Balanced diverse beam search,[0],[0]
"This approach has a high
2It was reported that using more cells may not help the prediction while hurting computation (Park and Chiba, 2017).
chance to output a locally optimal sequence and a common alternative is to use a beam search instead.",3.2 Balanced diverse beam search,[0],[0]
"We propose to improve the beam search by adding a greedy heuristic within it, in order to account for the diversity in the results.",3.2 Balanced diverse beam search,[0],[0]
"A similar suggestion has been made in (Vijayakumar et al., 2016), and our proposition differs by rebalancing the probabilities after diversity was introduced.",3.2 Balanced diverse beam search,[0],[0]
"In (Vijayakumar et al., 2016), at every step the most likely prediction is not weighted while all others are, by greedily comparing them.",3.2 Balanced diverse beam search,[0],[0]
This approach effectively always prefers the most likely character over all other alternatives at each step.,3.2 Balanced diverse beam search,[0],[0]
"The first result will thus be the same as the local optimum using a greedy approach, which becomes problematic for QAC where order is critical.",3.2 Balanced diverse beam search,[0],[0]
"By rebalancing the probability of the most likely suggestion with the average diversity weight given to other suggestions, we make sure probabilities stay uniform yet suggestions are diverse.",3.2 Balanced diverse beam search,[0],[0]
We use a normalized Levenshtein distance to assess the diversity.,3.2 Balanced diverse beam search,[0],[0]
"The AOL query logs (Pass et al., 2006) are commonly used to evaluate the quality of QAC systems.",4.1 Dataset,[0],[0]
"We rely on a background dataset for the
NN; training and validation datasets for lambdaMART integrations; and a test dataset for evaluations.",4.1 Dataset,[0],[0]
"Some adaptations are done to the AOL background dataset as in (Park and Chiba, 2017), such as removing the queries appearing less than 3 times or longer that 100 characters.",4.1 Dataset,[0],[0]
"For each query in the training, validation and test datasets, we use all possible prefixes starting after the first word as in (Shokouhi, 2013).",4.1 Dataset,[0],[0]
"We use the sets from (Park and Chiba, 2017) available online, enriched with user and time information provided in the original AOL dataset.",4.1 Dataset,[0],[0]
"In addition, we evaluate the systems on a second real-world dataset from a production search engine in the biomedical domain, PubMed (Fiorini et al., 2017; Lu, 2011; Mohan et al., 2018), that was created in the same manner.",4.1 Dataset,[0],[0]
"The biomedical dataset consists of 8,490,317 queries.",4.1 Dataset,[0],[0]
"The sizes of training, validation and test sets are comparable to those used for the AOL dataset.",4.1 Dataset,[0],[0]
Systems are evaluated using the traditional Mean Reciprocal Rank (MRR) metric.,4.2 Evaluation,[0],[0]
This metric assesses the quality of suggestions by identifying the rank of the real query in the suggestions given one of its prefixes.,4.2 Evaluation,[0],[0]
"We also tested PMRR as introduced in (Park and Chiba, 2017) and observed the same trends in results as MRR, so we do not show them due to space limitation.",4.2 Evaluation,[0],[0]
"Given the set of prefixes
P in the test dataset, MRR is defined as follows:
MRR = 1 |Q| ∑
r∈P
1
rp , (4)
where rp represent the rank of the match.",4.2 Evaluation,[0],[0]
Paired t-tests measure the significance of score variations among systems and are reported in the Results section.,4.2 Evaluation,[0],[0]
We also evaluate prediction time as this is an important parameter for building production systems.,4.2 Evaluation,[0],[0]
"The prediction time is averaged over 10 runs on the test set, on the same hardware for all models.",4.2 Evaluation,[0],[0]
We do not evaluate throughput but rather compare the time required by all approaches to process one prefix.,4.2 Evaluation,[0],[0]
"We implemented the method in (Park and Chiba, 2017) and used their best-performing model as a baseline.",4.3 Systems and setups,[0],[0]
"We also compare our results to the standard MPC (Bar-Yossef and Kraus, 2011).",4.3 Systems and setups,[0],[0]
"For our method, we evaluate several incremental versions, starting with NQAC which follows the architecture detailed above but with the word embeddings and the one-hot encoding of characters only.",4.3 Systems and setups,[0],[0]
We add the subscript U when the language model is enriched with user vectors and T when it integrates time features.,4.3 Systems and setups,[0],[0]
We append +D to indicate the use of the diverse beam search to predict queries instead of a standard beam search.,4.3 Systems and setups,[0],[0]
"Finally, we also study the impact of adding MPC and LambdaMART (+MPC, +λMART).",4.3 Systems and setups,[0],[0]
A summary of the results is presented in Table 1.,5 Results,[0],[0]
"Interestingly, our simple NQAC model performs similarly to the state-of-the-art on this dataset, called Neural Query Language Model (NQLM), on all queries.",5 Results,[0],[0]
It is significantly less good for seen queries (-5.6%) and significantly better for unseen queries (+4.2%).,5 Results,[0],[0]
"Although GRUs have less expressive power than LSTMs, their smaller number of parameters to train allowed them to better converge than all LSTM models we tested, including that of (Park and Chiba, 2017).",5 Results,[0],[0]
NQAC also benefits from a significantly better scalability (28% faster than NQLM) and thus seems more appropriate for production systems.,5 Results,[0],[0]
"When we enrich the language model with user information, it becomes better for seen queries (+1.9%) while being about as fast.",5 Results,[0],[0]
"Adding time sensitivity does not yield significant improvements on this
dataset overall, but improves significantly the performance for seen queries (+1.7%).",5 Results,[0],[0]
Relying on the diverse beam search significantly hurts the processing time (39% longer) while not providing significantly better performance.,5 Results,[0],[0]
Our integration of MPC differs from previous studies.,5 Results,[0],[0]
"We noticed that for Web search, MPC performs extremely well and is computationally cheap (0.24 seconds).",5 Results,[0],[0]
"On the other hand, all neural QAC systems are better for unseen queries but struggle to stay under a second of processing time.",5 Results,[0],[0]
"Since identifying if a query has been seen or not is done in constant time, we route the query either to MPC or to NQACUT and we note the overall performance as NQACUT+MPC.",5 Results,[0],[0]
This method provides a significant improvement over NQLM (+6.7%) overall while being faster on average.,5 Results,[0],[0]
"Finally, appending NQACUT ’s results to MPC’s and reranking the list with LambdaMART provides the best results on this dataset, but at the expense of greater computational cost (+60%).
",5 Results,[0],[0]
"While NQACUT+MPC appears clearly as the best compromise between performance and quality for the AOL dataset, the landscape changes drastically on the biomedical dataset and the quality drops significantly for all systems.",5 Results,[0],[0]
"This shows the potential difficulties associated with real-world systems, which particularly occur in specialized domains.",5 Results,[0],[0]
"In this case, the drop in performance is mostly due to the fact that biomedical queries are longer and it becomes more difficult for models to predict the entire query accurately only with the first keywords.",5 Results,[0],[0]
"While the generated queries make sense and are relevant candidates, the chance for generative models to predict the exact target query diminishes as the target query is longer because of combinatorial explosion.",5 Results,[0],[0]
"This is even more true when the target queries are diverse as in specialized domains (Islamaj Dogan et al., 2009; Névéol et al., 2011).",5 Results,[0],[0]
"For example, for the prefix “breast cancer”, there are 1169 diverse suffixes in a single day of logs used for training.",5 Results,[0],[0]
"These include “local recurrence”, “nodular prognosis”, “hormone receptor”, “circulating cells”, “family history”, “chromosome 4p16” or “herceptin review”, to cite only a few.",5 Results,[0],[0]
"Hence, while the model predicts plausible queries, it is a lot more difficult to predict the one the user intended.",5 Results,[0],[0]
"The target query length also has an impact on prediction time, as roughly twice the time is needed for Web searches.",5 Results,[0],[0]
"MPC is the exception, however, it per-
forms poorly even on seen queries (0.165).",5 Results,[0],[0]
This observation suggests that more elaborate models are specifically needed for specialized domains.,5 Results,[0],[0]
"On this dataset, NQAC does not perform as well as NQLM",5 Results,[0],[0]
and it seems this time that the higher number of parameters in NQLM is more appropriate for the task.,5 Results,[0],[0]
"Still, user information helps significantly for seen queries (+23%), probably because some users frequently check the same queries to keep up-to-date.",5 Results,[0],[0]
Time sensitivity seems to help significantly unseen queries (+21%) while significantly hurting the quality for seen queries (-47%).,5 Results,[0],[0]
Diversity is significantly helpful on this dataset (+19%) and provides a balance in performance for both seen and unseen queries.,5 Results,[0],[0]
"NQACUT+MPC yields the best overall MRR score for this dataset, and LambdaMART is unable to learn how to rerank the suggestions, thus decreasing the score.",5 Results,[0],[0]
"From these results, we draw several conclusions.",5 Results,[0],[0]
"First, MPC performs very well on seen queries for Web searches and it should be used on them.",5 Results,[0],[0]
"For unseen queries, the NQACUT model we propose achieves a sub-second state-of-the-art performance.",5 Results,[0],[0]
"Second, it is clear that the field of application will affect many of the decisions when designing a QAC system.",5 Results,[0],[0]
"On a specialized domain, the task is more challenging: fast approaches like MPC perform too poorly while more elaborate approaches do not meet production requirements.",5 Results,[0],[0]
"NQACU performs best on seen queries, NQACUT on unseen queries.",5 Results,[0],[0]
"Finally, NQACUT+D provides an equilibrium between the two at a greater computational cost.",5 Results,[0],[0]
Its overall MRR is similar to that of NQACUT+MPC but it is less redundant (see Table 2).,5 Results,[0],[0]
"Particularly, the system seems not to be limited anymore by the higher probability associ-
ated with shorter suggestions (e.g. “www google”, a form of “www google com”), thus bringing more diversity.",5 Results,[0],[0]
This aspect can be more useful for specialized domains where the range of possible queries is broader.,5 Results,[0],[0]
"Finally, we found that a lot more data was needed for the biomedical domain than for general Web search.",5 Results,[0],[0]
"After about a million queries, NQAC suggests meaningful and plausible queries for both datasets.",5 Results,[0],[0]
"However, for the biomedical dataset, the loss needs more epochs to stabilize than for the AOL dataset, mainly due to the combinatorial explosion mentioned above.",5 Results,[0],[0]
"To the best of our knowledge, we proposed the first neural language model that integrates user information and time sensitivity for query auto completion with a focus on scalability for real-world systems.",6 Conclusions and future work,[0],[0]
Personalization is provided through pretrained user vectors based on their past queries.,6 Conclusions and future work,[0],[0]
"By incorporating this information and by adapting the architecture, we were able to achieve stateof-the-art performance in neural query auto completion without relying on re-ranking, making this approach significantly more scalable in practice.
",6 Conclusions and future work,[0],[0]
"We studied multiple variants, their benefits and drawbacks for various use cases.",6 Conclusions and future work,[0],[0]
"We also demonstrate the utility of this method for specialized domains such as biomedicine, where the query diversity and vocabulary are broader and MPC fails to provide the same performance as in Web search.",6 Conclusions and future work,[0],[0]
We also found that user information and diversity improve the performance significantly more than for Web search engines.,6 Conclusions and future work,[0],[0]
"To allow readers to easily reproduce, evaluate and improve our models, we provide all the code on a public repository.",6 Conclusions and future work,[0],[0]
"The handling of time-sensitivity may benefit from a more elaborate integration, for example sessionbased rather than absolute time.",6 Conclusions and future work,[0],[0]
"Also, we evaluated our approaches on a general search setup for both datasets, while searches in the biomedical domain commonly contain fields (i.e. authors, title, abstract, etc.) which adds to the difficulty.",6 Conclusions and future work,[0],[0]
"The choice of a diversity metric is also important and could be faster or more efficient (e.g., using word embeddings to diversify the semantics of the suggestions).",6 Conclusions and future work,[0],[0]
These limitations warrant further work and we leave them as perspectives.,6 Conclusions and future work,[0],[0]
"This research was supported by the Intramural Research Program of the NIH, National Library of Medicine.",Acknowledgement,[0],[0]
"Query auto completion (QAC) systems are a standard part of search engines in industry, helping users formulate their query.",abstractText,[0],[0]
"Such systems update their suggestions after the user types each character, predicting the user’s intent using various signals — one of the most common being popularity.",abstractText,[0],[0]
"Recently, deep learning approaches have been proposed for the QAC task, to specifically address the main limitation of previous popularity-based methods: the inability to predict unseen queries.",abstractText,[0],[0]
"In this work we improve previous methods based on neural language modeling, with the goal of building an end-to-end system.",abstractText,[0],[0]
We particularly focus on using real-world data by integrating user information for personalized suggestions when possible.,abstractText,[0],[0]
We also make use of time information and study how to increase diversity in the suggestions while studying the impact on scalability.,abstractText,[0],[0]
"Our empirical results demonstrate a marked improvement on two separate datasets over previous best methods in both accuracy and scalability, making a step towards neural query auto-completion in production search engines.",abstractText,[0],[0]
Personalized neural language models for real-world query auto completion,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 706–711 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
706",text,[0],[0]
"Contextual, or ‘data-to-text’ natural language generation is one of the core tasks in natural language processing and has a considerable impact on various fields (Gatt and Krahmer, 2017).",1 Introduction,[0],[0]
"Within the field of recommender systems, a promising application is to estimate (or generate) personalized reviews that a user would write about a product, i.e., to discover their nuanced opinions about each of its individual aspects.",1 Introduction,[0],[0]
"A successful model could work (for instance) as (a) a highly-nuanced recommender system that tells users their likely reaction to a product in the form of text fragments; (b) a writing tool that helps users ‘brainstorm’ the review-writing process; or (c) a querying system that facilitates personalized natural lan-
guage queries (i.e., to find items about which a user would be most likely to write a particular phrase).",1 Introduction,[0],[0]
"Some recent works have explored the review generation task and shown success in generating cohesive reviews (Dong et al., 2017; Ni et al., 2017; Zang and Wan, 2017).",1 Introduction,[0],[0]
"Most of these works treat the user and item identity as input; we seek a system with more nuance and more precision by allowing users to ‘guide’ the model via short phrases, or auxiliary data such as item specifications.",1 Introduction,[0],[0]
"For example, a review writing assistant might allow users to write short phrases and expand these key points into a plausible review.
",1 Introduction,[0],[0]
"Review text has been widely studied in traditional tasks such as aspect extraction (Mukherjee and Liu, 2012; He et al., 2017), extraction of sentiment lexicons (Zhang et al., 2014), and aspectaware sentiment analysis (Wang et al., 2016; McAuley et al., 2012).",1 Introduction,[0],[0]
These works are related to review generation since they can provide prior knowledge to supervise the generative process.,1 Introduction,[0],[0]
"We are interested in exploring how such knowledge (e.g. extracted aspects) can be used in the review generation task.
",1 Introduction,[0],[0]
"In this paper, we focus on designing a review generation model that is able to leverage both user and item information as well as auxiliary, textual input and aspect-aware knowledge.",1 Introduction,[0],[0]
"Specifically, we study the task of expanding short phrases into complete, coherent reviews that accurately reflect the opinions and knowledge learned from those phrases.
",1 Introduction,[0],[0]
"These short phrases could include snippets provided by the user, or manifest aspects about the items themselves (e.g. brand words, technical specifications, etc.).",1 Introduction,[0],[0]
"We propose an encoderdecoder framework that takes into consideration three encoders (a sequence encoder, an attribute encoder, and an aspect encoder), and one decoder.",1 Introduction,[0],[0]
"The sequence encoder uses a gated recurrent unit
(GRU) network to encode text information; the attribute encoder learns a latent representation of user and item identity; finally, the aspect encoder finds an aspect-aware representation of users and items, which reflects user-aspect preferences and item-aspect relationships.",1 Introduction,[0],[0]
The aspect-aware representation is helpful to discover what each user is likely to discuss about each item.,1 Introduction,[0],[0]
"Finally, the output of these encoders is passed to the sequence decoder with an attention fusion layer.",1 Introduction,[0],[0]
The decoder attends on the encoded information and biases the model to generate words that are consistent with the input phrases and words belonging to the most relevant aspects.,1 Introduction,[0],[0]
"Review generation belongs to a large body of work on data-to-text natural language generation (Gatt and Krahmer, 2017), which has applications including summarization (See et al., 2017), image captioning (Vinyals et al., 2015), and dialogue response generation (Xing et al., 2017; Li et al., 2016; Ghosh et al., 2017), among others.",2 Related Work,[0],[0]
"Among these, review generation is characterized by the need to generate long sequences and estimate high-order interactions between users and items.
",2 Related Work,[0],[0]
Several approaches have been recently proposed to tackle these problems.,2 Related Work,[0],[0]
Dong et al. (2017) proposed an attribute-to-sequence (Attr2Seq) method to encode user and item identities as well as rating information with a multi-layer perceptron and a decoder then generates reviews conditioned on this information.,2 Related Work,[0],[0]
"They also used an attention mechanism to strengthen the alignment between
output and input attributes.",2 Related Work,[0],[0]
Ni et al. (2017) trained a collaborative-filtering generative concatenative network to jointly learn the tasks of review generation and item recommendation.,2 Related Work,[0],[0]
"Zang and Wan (2017) proposed a hierarchical structure to generate long reviews; they assume each sentence is associated with an aspect score, and learn the attention between aspect scores and sentences during training.",2 Related Work,[0],[0]
"Our approach differs from these mainly in our goal of incorporating auxiliary textual information (short phrases, product specifications, etc.) into the generative process, which facilitates the generation of higher-fidelity reviews.
",2 Related Work,[0],[0]
"Another line of work related to review generation is aspect extraction and opinion mining (Park et al., 2015; Qiu et al., 2017; He et al., 2017; Chen et al., 2014).",2 Related Work,[0],[0]
"In this paper, we argue that the extra aspect (opinion) information extracted using these previous works can effectively improve the quality of generated reviews.",2 Related Work,[0],[0]
We propose a simple but effective way to combine aspect information into the generative model.,2 Related Work,[0],[0]
We describe the review generation task as follows.,3 Approach,[0],[0]
"Given a user u, item i, several short phrases {d1, d2, ..., dM}, and a group of extracted aspects {A1, A2, ..., Ak}, our goal is to generate a review (w1, w2, ..., wT) that maximizes the probability P (w1:T|u, i, d1:M).",3 Approach,[0],[0]
"To solve this task, we propose a method called ExpansionNet which contains two parts: 1) three encoders to leverage the input phrases and aspect information; and 2) a decoder with an attention fusion layer to generate sequences and align the generation with the input
sources.",3 Approach,[0],[0]
The model structure is shown in Figure 1.,3 Approach,[0],[0]
"Our sequence encoder is a two-layer bi-directional GRU, as is commonly used in sequence-tosequence (Seq2Seq) models (Cho et al., 2014).","3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
"Input phrases first pass a word embedding layer, then go through the GRU one-by-one and finally yield a sequence of hidden states {e1, e2..., eL}.","3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
"In the case of multiple phrases, these share the same sequence encoder and have different lengths L. To simplify notation, we only consider one input phrase in this section.
","3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
The attribute encoder and aspect encoder both consist of two embedding layers and a projection layer.,"3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
"For the attribute encoder, we define two general embedding layers Eu ∈ R|U|×m and Ei ∈ R|I|×m to obtain the attribute latent factors γu and γi; for the aspect encoder, we use two aspect-aware embedding layers E ′","3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
u ∈ R|U|×k,"3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
and E ′,"3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
i ∈ R|I|×k to obtain aspect-aware latent factors βu,"3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
and βi.,"3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
"Here |U|, |I|, m and k are the number of users, number of items, the dimension of attributes, and the number of aspects, respectively.","3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
"After the embedding layers, the attribute and aspect-aware latent factors are concatenated and fed into a projection layer with tanh activation.","3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
"The outputs are calculated as:
γu = Eu(u), γi = Ei(i) (1) βu = E ′ u(u), βi = E ′ i(i) (2)
u = tanh(Wu[γu; γi] + bu) (3)
v = tanh(Wv[βu;βi] + bv) (4)
where Wu ∈","3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
"Rn×2m,","3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
"bu ∈ Rn, Wv ∈ Rn×2k, bv ∈ Rn are learnable parameters and n is the dimensionality of the hidden units in the decoder.","3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
The decoder is a two-layer GRU that predicts the target words given the start token.,3.2 Decoder with attention fusion layer,[0],[0]
The hidden state of the decoder is initialized using the sum of the three encoders’ outputs.,3.2 Decoder with attention fusion layer,[0],[0]
The hidden state at time-step t is updated via the GRU unit based on the previous hidden state and the input word.,3.2 Decoder with attention fusion layer,[0],[0]
"Specifically:
h0 = eL + u+ v (5)
ht = GRU(wt,ht−1), (6)
where h0 ∈ Rn is the decoder’s initial hidden state and ht ∈",3.2 Decoder with attention fusion layer,[0],[0]
"Rn is the hidden state at time-step t.
To fully exploit the encoder-side information, we apply an attention fusion layer to summarize the output of each encoder and jointly determine the final word distribution.",3.2 Decoder with attention fusion layer,[0],[0]
"For the sequence encoder, the attention vector is defined as in many other applications (Bahdanau et al., 2014; Luong et al., 2015):
a1t = L∑ j=1 α1tjej (7)
α1tj = exp(tanh(v 1 α >",3.2 Decoder with attention fusion layer,[0],[0]
"(W 1α[ej ;ht] + b 1 α)))/Z,
(8)
where a1t ∈",3.2 Decoder with attention fusion layer,[0],[0]
"Rn is the attention vector on the sequence encoder at time-step t, α1tj is the attention score over the encoder hidden state ej and decoder hidden state ht, and Z is a normalization term.
",3.2 Decoder with attention fusion layer,[0],[0]
"For the attribute encoder, the attention vector is calculated as:
a2t = ∑ j∈u,i α2tjγj (9)
α2tj = exp(tanh(v 2 α >",3.2 Decoder with attention fusion layer,[0],[0]
"(W 2α[γj ;ht] + b 2 α)))/Z,
(10)
where a2t ∈",3.2 Decoder with attention fusion layer,[0],[0]
"Rn is the attention vector on the attribute encoder, and α2tj is the attention score between the attribute latent factor γj and decoder hidden state ht.
",3.2 Decoder with attention fusion layer,[0],[0]
"Inspired by the copy mechanism (Gu et al., 2016; See et al., 2017), we design an attention vector that estimates the probability that each aspect will be discussed in the next time-step:
sui =Ws[βu;βi] + bs (11) a3t = tanh(W 3 α[sui; et;ht] + b 3 α), (12)
where sui ∈ Rk is the aspect importance considering the interaction between u and i, et is the decoder input after embedding layer at time-step t, and a3t ∈",3.2 Decoder with attention fusion layer,[0],[0]
"Rk is a probability vector to bias each aspect at time-step t. Finally, the first two attention vectors are concatenated with the decoder hidden state at time-step t and projected to obtain the output word distribution Pv.",3.2 Decoder with attention fusion layer,[0],[0]
The attention scores from the aspect encoder are then directly added to the aspect words in the final word distribution.,3.2 Decoder with attention fusion layer,[0],[0]
"The output probability for word w at time-step t is given by:
Pv(wt) =",3.2 Decoder with attention fusion layer,[0],[0]
tanh(W,3.2 Decoder with attention fusion layer,[0],[0]
[ht;a 1 t ;a 2 t ] + b),3.2 Decoder with attention fusion layer,[0],[0]
"(13)
P (wt) = Pv(wt) + a 3 t [k] · 1wt∈Ak , (14)
where wt is the target word at time-step t, a3t",3.2 Decoder with attention fusion layer,[0],[0]
"[k] is the probability that aspect k will be discussed at time-step t, Ak represents all words belonging to aspect k and 1wt∈Ak is a binary variable indicating whether wt belongs to aspect k.
During inference, we use greedy decoding by choosing the word with maximum probability, denoted as yt = argmaxwtsoftmax(P (wt)).",3.2 Decoder with attention fusion layer,[0],[0]
Decoding finishes when an end token is encountered.,3.2 Decoder with attention fusion layer,[0],[0]
"We consider a real world dataset from Amazon Electronics (McAuley et al., 2015) to evaluate our model.",4 Experiments,[0],[0]
"We convert all text into lowercase, add start and end tokens to each review, and perform tokenization using NLTK.1",4 Experiments,[0],[0]
"We discard reviews with length greater than 100 tokens and consider a vocabulary of 30,000 tokens.",4 Experiments,[0],[0]
"After preprocessing, the dataset contains 182,850 users, 59,043 items, and 992,172 reviews (sparsity 99.993%), which is much sparser than the datasets used in previous works (Dong et al., 2017; Ni et al., 2017).",4 Experiments,[0],[0]
"On average, each review contains 49.32 tokens as well as a short-text summary of 4.52 tokens.",4 Experiments,[0],[0]
"In our experiments, the basic ExpansionNet uses these summaries as input phrases.",4 Experiments,[0],[0]
"We split the dataset into training (80%), validation (10%) and test sets (10%).",4 Experiments,[0],[0]
All results are reported on the test set.,4 Experiments,[0],[0]
"We use the method2 in (He et al., 2017) to extract 15 aspects and consider the top 100 words from each aspect.",4.1 Aspect Extraction,[0],[0]
Table 2 shows 10 inferred aspects and representative words (inferred aspects are manually labeled).,4.1 Aspect Extraction,[0],[0]
"ExpansionNet calculates an attention score based on the user and item aspect-aware representation, then determines how much these representative words are biased in the output word distribution.
",4.1 Aspect Extraction,[0],[0]
1 https://www.nltk.org/ 2 https://github.com/ruidan/ Unsupervised-Aspect-Extraction,4.1 Aspect Extraction,[0],[0]
We use PyTorch3 to implement our model.4 Parameter settings are shown in Table 1.,4.2 Experiment Details,[0],[0]
"For the attribute encoder and aspect encoder, we set the dimensionality to 64 and 15 respectively.",4.2 Experiment Details,[0],[0]
"For both the sequence encoder and decoder, we use a 2- layer GRU with hidden size 512.",4.2 Experiment Details,[0],[0]
We also add dropout layers before and after the GRUs.,4.2 Experiment Details,[0],[0]
The dropout rate is set to 0.1.,4.2 Experiment Details,[0],[0]
"During training, the input sequences of the same source (e.g. review, summary) inside each batch are padded to the same length.",4.2 Experiment Details,[0],[0]
"We evaluate the model on six automatic metrics (Table 3): Perplexity, BLEU-1/BLEU-4, ROUGEL and Distinct-1/2 (percentage of distinct unigrams and bi-grams) (Li et al., 2016).",4.3 Performance Evaluation,[0],[0]
"We compare
3 http://pytorch.org/docs/master/index.html 4 https://github.com/nijianmo/textExpansion
against three baselines: Rand (randomly choose a review from the training set), GRU-LM (the GRU decoder works alone as a language model) and a state-of-the-art model Attr2Seq that only considers user and item attribute (Dong et al., 2017).",4.3 Performance Evaluation,[0],[0]
"ExpansionNet (with summary, item title, attribute and aspect as input) achieves significant improvements over Attr2Seq on all metrics.",4.3 Performance Evaluation,[0],[0]
"As we add more input information, the model continues to obtain better results, except for the ROUGE-L metric.",4.3 Performance Evaluation,[0],[0]
"This proves that our model can effectively learn from short input phrases and aspect information and improve the correctness and diversity of generated results.
",4.3 Performance Evaluation,[0],[0]
Figure 2 presents a sample generation result.,4.3 Performance Evaluation,[0],[0]
"ExpansionNet captures fine-grained item information (e.g. that the item is a tablet), which Attr2Seq fails to recognize.",4.3 Performance Evaluation,[0],[0]
"Moreover, given a phrase like “easy to use” in the summary, ExpansionNet generates reviews containing the same text.",4.3 Performance Evaluation,[0],[0]
This demonstrates the possibility of using our model in an assistive review generation scenario.,4.3 Performance Evaluation,[0],[0]
"Finally, given extra aspect information, the model successfully estimates that the screen would be an important aspect (i.e., for the current user and item); it generates phrases such as “screen is very respon-
sive” about the aspect “screen” which is also covered in the real (ground-truth) review (“display is beautiful”).
",4.3 Performance Evaluation,[0],[0]
We are also interested in seeing how the aspectaware representation can find related aspects and bias the generation to discuss more about those aspects.,4.3 Performance Evaluation,[0],[0]
We analyze the average number of aspects in real and generated reviews and show on average how many aspects in real reviews are covered in generated reviews.,4.3 Performance Evaluation,[0],[0]
We consider a review as covering an aspect if any of the aspect’s representative words exists in the review.,4.3 Performance Evaluation,[0],[0]
"As shown in Table 4, Attr2Seq tends to cover more aspects in generation, many of which are not discussed in real reviews.",4.3 Performance Evaluation,[0],[0]
"On the other hand, ExpansionNet better captures the distribution of aspects that are discussed in real reviews.",4.3 Performance Evaluation,[0],[0]
"In this paper, we focus on the problem of building assistive systems that can help users to write reviews.",abstractText,[0],[0]
"We cast this problem using an encoder-decoder framework that generates personalized reviews by expanding short phrases (e.g. review summaries, product titles) provided as input to the system.",abstractText,[0],[0]
We incorporate aspect-level information via an aspect encoder that learns ‘aspect-aware’ user and item representations.,abstractText,[0],[0]
An attention fusion layer is applied to control generation by attending on the outputs of multiple encoders.,abstractText,[0],[0]
Experimental results show that our model is capable of generating coherent and diverse reviews that expand the contents of input phrases.,abstractText,[0],[0]
"In addition, the learned aspectaware representations discover those aspects that users are more inclined to discuss and bias the generated text toward their personalized aspect preferences.",abstractText,[0],[0]
Personalized Review Generation by Expanding Phrases and Attending on Aspect-Aware Representations,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2204–2213 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
2204",text,[0],[0]
"Despite much recent success in natural language processing and dialogue research, communication between a human and a machine is still in its infancy.",1 Introduction,[0],[0]
It is only recently that neural models have had sufficient capacity and access to sufficiently large datasets that they appear to generate meaningful responses in a chit-chat setting.,1 Introduction,[0],[0]
"Still, conversing with such generic chit-chat models for even a short amount of time quickly exposes their weaknesses (Serban et al., 2016; Vinyals and Le, 2015).
",1 Introduction,[0],[0]
"Common issues with chit-chat models include: (i) the lack of a consistent personality (Li et al., 2016a) as they are typically trained over many dialogs each with different speakers, (ii) the lack of an explicit long-term memory as they are typically trained to produce an utterance given only the recent dialogue history (Vinyals and Le, 2015);
1Work done while at Facebook AI Research.
and (iii) a tendency to produce non-specific answers like “I don’t know” (Li et al., 2015).",1 Introduction,[0],[0]
Those three problems combine to produce an unsatisfying overall experience for a human to engage with.,1 Introduction,[0],[0]
"We believe some of those problems are due to there being no good publicly available dataset for general chit-chat.
",1 Introduction,[0],[0]
"Because of the low quality of current conversational models, and because of the difficulty in evaluating these models, chit-chat is often ignored as an end-application.",1 Introduction,[0],[0]
"Instead, the research community has focused on task-oriented communication, such as airline or restaurant booking (Bordes and Weston, 2016), or else single-turn information seeking, i.e. question answering (Rajpurkar et al., 2016).",1 Introduction,[0],[0]
"Despite the success of the latter, simpler, domain, it is well-known that a large quantity of human dialogue centers on socialization, personal interests and chit-chat (Dunbar et al., 1997).",1 Introduction,[0],[0]
"For example, less than 5% of posts on Twitter are questions, whereas around 80% are about personal emotional state, thoughts or activities, authored by so called “Meformers” (Naaman et al., 2010).
",1 Introduction,[0],[0]
"In this work we make a step towards more engaging chit-chat dialogue agents by endowing them with a configurable, but persistent persona, encoded by multiple sentences of textual description, termed a profile.",1 Introduction,[0],[0]
"This profile can be stored in a memory-augmented neural network and then used to produce more personal, specific, consistent and engaging responses than a persona-free model, thus alleviating some of the common issues in chit-chat models.",1 Introduction,[0],[0]
"Using the same mechanism, any existing information about the persona of the dialogue partner can also be used in the same way.",1 Introduction,[0],[0]
"Our models are thus trained to both ask and answer questions about personal topics, and the resulting dialogue can be used to build a model of the persona of the speaking partner.
",1 Introduction,[0],[0]
"To support the training of such models, we
present the PERSONA-CHAT dataset, a new dialogue dataset consisting of 164,356 utterances between crowdworkers who were randomly paired and each asked to act the part of a given provided persona (randomly assigned, and created by another set of crowdworkers).",1 Introduction,[0],[0]
The paired workers were asked to chat naturally and to get to know each other during the conversation.,1 Introduction,[0],[0]
"This produces interesting and engaging conversations that our agents can try to learn to mimic.
",1 Introduction,[0],[0]
"Studying the next utterance prediction task during dialogue, we compare a range of models: both generative and ranking models, including Seq2Seq models and Memory Networks (Sukhbaatar et al., 2015) as well as other standard retrieval baselines.",1 Introduction,[0],[0]
We show experimentally that in either the generative or ranking case conditioning the agent with persona information gives improved prediction of the next dialogue utterance.,1 Introduction,[0],[0]
"The PERSONA-CHAT dataset is designed to facilitate research into alleviating some of the issues that traditional chitchat models face, and with the aim of making such models more consistent and engaging, by endowing them with a persona.",1 Introduction,[0],[0]
"By comparing against chit-chat models built using the OpenSubtitles and Twitter datasets, human evaluations show that our dataset provides more engaging models, that are simultaneously capable of being fluent and consistent via conditioning on a persistent, recognizable profile.",1 Introduction,[0],[0]
"Traditional dialogue systems consist of building blocks, such as dialogue state tracking components and response generators, and have typically been applied to tasks with labeled internal dialogue state and precisely defined user intent (i.e., goal-oriented dialogue), see e.g. (Young, 2000).",2 Related Work,[0],[0]
"The most successful goal-oriented dialogue systems model conversation as partially observable Markov decision processes (POMDPs) (Young et al., 2013).",2 Related Work,[0],[0]
All those methods typically do not consider the chit-chat setting and are more concerned with achieving functional goals (e.g. booking an airline flight) than displaying a personality.,2 Related Work,[0],[0]
"In particular, many of the tasks and datasets available are constrained to narrow domains (Serban et al., 2015).
",2 Related Work,[0],[0]
"Non-goal driven dialogue systems go back to Weizenbaum’s famous program ELIZA (Weizenbaum, 1966), and hand-coded systems have con-
tinued to be used in applications to this day.",2 Related Work,[0],[0]
"For example, modern solutions that build an openended dialogue system to the Alexa challenge combine hand-coded and machine-learned elements (Serban et al., 2017a).",2 Related Work,[0],[0]
"Amongst the simplest of statistical systems that can be used in this domain, that are based on data rather than handcoding, are information retrieval models (Sordoni et al., 2015), which retrieve and rank responses based on their matching score with the recent dialogue history.",2 Related Work,[0],[0]
"We use IR systems as a baseline in this work.
",2 Related Work,[0],[0]
End-to-end neural approaches are a class of models which have seen growing recent interest.,2 Related Work,[0],[0]
"A popular class of methods are generative recurrent systems like seq2seq applied to dialogue (Sutskever et al., 2014; Vinyals and Le, 2015; Sordoni et al., 2015; Li et al., 2016b; Serban et al., 2017b).",2 Related Work,[0],[0]
"Rooted in language modeling, they are able to produce syntactically coherent novel responses, but their memory-free approach means they lack long-term coherence and a persistent personality, as discussed before.",2 Related Work,[0],[0]
"A promising direction, that is still in its infancy, to fix this issue is to use a memory-augmented network instead (Sukhbaatar et al., 2015; Dodge et al., 2015) by providing or learning appropriate memories.
",2 Related Work,[0],[0]
Serban et al. (2015) list available corpora for training dialogue systems.,2 Related Work,[0],[0]
"Perhaps the most relevant to learning chit-chat models are ones based on movie scripts such as OpenSubtitles and Cornell Movie-Dialogue Corpus, and dialogue from web platforms such as Reddit and Twitter, all of which have been used for training neural approaches (Vinyals and Le, 2015; Dodge et al., 2015; Li et al., 2016b; Serban et al., 2017b).",2 Related Work,[0],[0]
Naively training on these datasets leads to models with the lack of a consistent personality as they will learn a model averaged over many different speakers.,2 Related Work,[0],[0]
"Moreover, the data does little to encourage the model to engage in understanding and maintaining knowledge of the dialogue partner’s personality and topic interests.
",2 Related Work,[0],[0]
"According to Serban et al. (2015)’s survey, personalization of dialogue systems is “an important task, which so far has not received much attention”.",2 Related Work,[0],[0]
"In the case of goal-oriented dialogue some work has focused on the agent being aware of the human’s profile and adjusting the dialogue accordingly, but without a personality to the agent itself (Lucas et al., 2009; Joshi et al., 2017).",2 Related Work,[0],[0]
"For
the chit-chat setting, the most relevant work is (Li et al., 2016a).",2 Related Work,[0],[0]
"For each user in the Twitter corpus, personas were captured via distributed embeddings (one per speaker) to encapsulate individual characteristics such as background information and speaking style, and they then showed using those vectors improved the output of their seq2seq model for the same speaker.",2 Related Work,[0],[0]
"Their work does not focus on attempting to engage the other speaker by getting to know them, as we do here.",2 Related Work,[0],[0]
"For that reason, our focus is on explicit profile information, not hard-to-interpret latent variables.
",2 Related Work,[0],[0]
3,2 Related Work,[0],[0]
"The PERSONA-CHAT Dataset
The aim of this work is to facilitate more engaging and more personal chit-chat dialogue.",2 Related Work,[0],[0]
"The PERSONA-CHAT dataset is a crowd-sourced dataset, collected via Amazon Mechanical Turk, where each of the pair of speakers condition their dialogue on a given profile, which is provided.
",2 Related Work,[0],[0]
"The data collection consists of three stages: (i) Personas: we crowdsource a set of 1155 possible personas, each consisting of at least 5 profile sentences, setting aside 100 never seen before personas for validation, and 100 for test.
",2 Related Work,[0],[0]
"(ii) Revised personas: to avoid modeling that takes advantage of trivial word overlap, we crowdsource additional rewritten sets of the same 1155 personas, with related sentences that are rephrases, generalizations or specializations, rendering the task much more challenging.
",2 Related Work,[0],[0]
"(iii) Persona chat: we pair two Turkers and assign them each a random (original) persona from the pool, and ask them to chat.",2 Related Work,[0],[0]
"This resulted in a dataset of 164,356 utterances over 10,981 dialogs, 15,705 utterances (968 dialogs) of which are set aside for validation, and 15,119 utterances (1000 dialogs) for test.
",2 Related Work,[0],[0]
"The final dataset and its corresponding data collection source code, as well as models trained on the data, are all available open source in ParlAI2.
",2 Related Work,[0],[0]
"In the following, we describe each data collection stage and the resulting tasks in more detail.",2 Related Work,[0],[0]
"We asked the crowdsourced workers to create a character (persona) description using 5 sentences, providing them only a single example:
2https://github.com/facebookresearch/ ParlAI/tree/master/projects/personachat
“I am a vegetarian.",3.1 Personas,[0],[0]
I like swimming.,3.1 Personas,[0],[0]
My father used to work for Ford.,3.1 Personas,[0],[0]
My favorite band is Maroon5.,3.1 Personas,[0],[0]
"I got a new job last month, which is about advertising design.”
",3.1 Personas,[0],[0]
"Our aim was to create profiles that are natural and descriptive, and contain typical topics of human interest that the speaker can bring up in conversation.",3.1 Personas,[0],[0]
"Because the personas are not the real profiles of the Turkers, the dataset does not contain personal information (and they are told specifically not to use any).",3.1 Personas,[0],[0]
"We asked the workers to make each sentence short, with a maximum of 15 words per sentence.",3.1 Personas,[0],[0]
"This is advantageous both for humans and machines: if they are too long, crowdsourced workers are likely to lose interest, and for machines the task could become more difficult.
",3.1 Personas,[0],[0]
Some examples of the personas collected are given in Table 1 (left).,3.1 Personas,[0],[0]
"A difficulty when constructing dialogue datasets, or text datasets in general, is that in order to encourage research progress, the task must be carefully constructed so that is neither too easy nor too difficult for the current technology (Voorhees et al., 1999).",3.2 Revised Personas,[0],[0]
"One issue with conditioning on textual personas is that there is a danger that humans will, even if asked not to, unwittingly repeat profile information either verbatim or with significant word overlap.",3.2 Revised Personas,[0],[0]
"This may make any subsequent machine learning tasks less challenging, and the solutions will not generalize to more difficult tasks.",3.2 Revised Personas,[0],[0]
"This has been a problem in some recent datasets: for example, the dataset curation technique used for the well-known SQuAD dataset suffers from this word overlap problem to a certain extent (Chen et al., 2017).
",3.2 Revised Personas,[0],[0]
"To alleviate this problem, we presented the original personas we collected to a new set of crowdworkers and asked them to rewrite the sentences so that a new sentence is about “a related characteristic that the same person may have”, hence the revisions could be rephrases, generalizations or specializations.",3.2 Revised Personas,[0],[0]
"For example “I like basketball” can be revised as “I am a big fan of Michael Jordan” not because they mean the same thing but because the same persona could contain both.
",3.2 Revised Personas,[0],[0]
"In the revision task, workers are instructed not to trivially rephrase the sentence by copying the original words.",3.2 Revised Personas,[0],[0]
"However, during the entry stage if a non-stop word is copied we issue a warning,
and ask them to rephrase, guaranteeing that the instructions are followed.",3.2 Revised Personas,[0],[0]
"For example, “My father worked for Ford.”",3.2 Revised Personas,[0],[0]
"can be revised to “My dad worked in the car industry”, but not “My dad was employed by Ford.”",3.2 Revised Personas,[0],[0]
"due to word overlap.
",3.2 Revised Personas,[0],[0]
Some examples of the revised personas collected are given in Table 1 (right).,3.2 Revised Personas,[0],[0]
"After collecting personas, we then collected the dialogues themselves, conditioned on the personas.",3.3 Persona Chat,[0],[0]
"For each dialogue, we paired two random crowdworkers, and gave them the instruction that they will chit-chat with another worker, while playing the part of a given character.",3.3 Persona Chat,[0],[0]
"We then provide them with a randomly chosen persona from our pool, different to their partners.",3.3 Persona Chat,[0],[0]
"The instructions are on
purpose quite terse and simply ask them to “chat with the other person naturally and try to get to know each other”.",3.3 Persona Chat,[0],[0]
"In an early study we noticed the crowdworkers tending to talk about themselves (their own persona) too much, so we also added the instructions “both ask questions and answer questions of your chat partner” which seemed to help.",3.3 Persona Chat,[0],[0]
We also gave a bonus for high quality dialogs.,3.3 Persona Chat,[0],[0]
"The dialog is turn-based, with a maximum of 15 words per message.",3.3 Persona Chat,[0],[0]
"We again gave instructions to not trivially copy the character descriptions into the messages, but also wrote explicit code sending them an error if they tried to do so, using simple string matching.",3.3 Persona Chat,[0],[0]
We define a minimum dialogue length which is randomly between 6 and 8 turns each for each dialogue.,3.3 Persona Chat,[0],[0]
An example dialogue from the dataset is given in Table 2.,3.3 Persona Chat,[0],[0]
"We focus on the standard dialogue task of predicting the next utterance given the dialogue history, but consider this task both with and without the profile information being given to the learning agent.",3.4 Evaluation,[0],[0]
"Our goal is to enable interesting directions for future research, where chatbots can for instance have personalities, or imputed personas could be used to make dialogue more engaging to the user.
",3.4 Evaluation,[0],[0]
"We consider this in four possible scenarios: conditioning on no persona, your own persona, their persona, or both.",3.4 Evaluation,[0],[0]
"These scenarios can be tried using either the original personas, or the revised ones.",3.4 Evaluation,[0],[0]
"We then evaluate the task using three metrics: (i) the log likelihood of the correct sequence, measured via perplexity, (ii) F1 score, and (iii) next utterance classification loss, following Lowe et al. (2015).",3.4 Evaluation,[0],[0]
"The latter consists of choosing N random distractor responses from other dialogues (in our setting, N=19) and the model selecting the best response among them, resulting in a score of one if the model chooses the correct response, and zero otherwise (called hits@1 in the experiments).",3.4 Evaluation,[0],[0]
We consider two classes of model for next utterance prediction: ranking models and generative models.,4 Models,[0],[0]
Ranking models produce a next utterance by considering any utterance in the training set as a possible candidate reply.,4 Models,[0],[0]
"Generative models generate novel sentences by conditioning on the dialogue history (and possibly, the persona), and then generating the response word-by-word.",4 Models,[0],[0]
"Note one can still evaluate the latter as ranking models by computing the probability of generating a given candidate, and ranking candidates by those scores.",4 Models,[0.9518238531527008],"['Description of the datasets, benchmark algorithms, and oracle configurations, as well as further experimental results are included in Appendix B. Datasets We begin with 10 datasets with full reward information and simulate bandit feedback by withholding the rewards for actions not selected by the algorithm.']"
"We first consider two baseline models, an IR baseline (Sordoni et al., 2015) and a supervised embedding model, Starspace (Wu et al., 2017)3.",4.1 Baseline ranking models,[0],[0]
"While there are many IR variants, we adopt the simplest one: find the most similar message in the (training) dataset and output the response from that exchange.",4.1 Baseline ranking models,[0],[0]
Similarity is measured by the tfidf weighted cosine similarity between the bags of words.,4.1 Baseline ranking models,[0],[0]
"Starspace is a recent model that also performs information retrieval but by learning the
3github.com/facebookresearch/StarSpace
similarity between the dialog and the next utterance by optimizing the embeddings directly for that task using the margin ranking loss and k-negative sampling.",4.1 Baseline ranking models,[0],[0]
"The similarity function sim(q, c′) is the cosine similarity of the sum of word embeddings of the query q and candidate c′.",4.1 Baseline ranking models,[0],[0]
"Denoting the dictionary of D word embeddings as W which is a D× d matrix, where Wi indexes the ith word (row), yielding its d-dimensional embedding, it embeds the sequences q and c′.
In both methods, IR and StarSpace, to incorporate the profile we simply concatenate it to the query vector bag of words.",4.1 Baseline ranking models,[0],[0]
"Both the previous models use the profile information by combining it with the dialogue history, which means those models cannot differentiate between the two when deciding on the next utterance.",4.2 Ranking Profile Memory Network,[0],[0]
"In this model we instead use a memory network with the dialogue history as input, which then performs attention over the profile to find relevant lines from the profile to combine with the input, and then finally predicts the next utterance.",4.2 Ranking Profile Memory Network,[0],[0]
"We use the same representation and loss as in the Starspace model, so without the profile, the two models are identical.",4.2 Ranking Profile Memory Network,[0],[0]
"When the profile is available attention is performed by computing the similarity of the input q with the profile sentences pi, computing the softmax, and taking the weighted sum:
q+ = q+ ∑ sipi, si = Softmax(sim(q, pi))",4.2 Ranking Profile Memory Network,[0],[0]
where Softmax(zi) = ezi/ ∑ j e zj .,4.2 Ranking Profile Memory Network,[0],[0]
"One can then rank the candidates c′ using sim(q+, c′).",4.2 Ranking Profile Memory Network,[0],[0]
"One can also perform multiple “hops” of attention over the profile rather than one, as shown here, although that did not bring significant gains in our parameter sweeps.",4.2 Ranking Profile Memory Network,[0],[0]
"The key-value (KV) memory network (Miller et al., 2016) was proposed as an improvement to the memory network by performing attention over keys and outputting the values (instead of the same keys as in the original), which can outperform memory networks dependent on the task and definition of the key-value pairs.",4.3 Key-Value Profile Memory Network,[0],[0]
"Here, we apply this model to dialogue, and consider the keys as dialog histories (from the training set), and the values as the next dialogue utterances, i.e., the replies from the speaking partner.",4.3 Key-Value Profile Memory Network,[0],[0]
"This allows the model
to have a memory of past dialogues that it can directly use to help influence its prediction for the current conversation.",4.3 Key-Value Profile Memory Network,[0],[0]
"The model we choose is identical to the profile memory network just described in the first hop over profiles, while in the second hop, q+ is used to attend over the keys and output a weighted sum of values as before, producing q++.",4.3 Key-Value Profile Memory Network,[0],[0]
"This is then used to rank the candidates c′ using sim(q++, c′) as before.",4.3 Key-Value Profile Memory Network,[0],[0]
As the set of (key-value) pairs is large this would make training very slow.,4.3 Key-Value Profile Memory Network,[0],[0]
In our experiments we simply trained the profile memory network and used the same weights from that model and applied this architecture at test time instead.,4.3 Key-Value Profile Memory Network,[0],[0]
"Training the model directly would presumably give better results, however this heuristic already proved beneficial compared to the original network.",4.3 Key-Value Profile Memory Network,[0],[0]
The input sequence x is encoded by applying het = LSTMenc(xt | het−1).,4.4 Seq2Seq,[0],[0]
"We use GloVe (Pennington et al., 2014) for our word embeddings.",4.4 Seq2Seq,[0],[0]
"The final hidden state, het , is fed into the decoder LSTMdec as the initial state hd0.",4.4 Seq2Seq,[0],[0]
"For each time step t, the decoder then produces the probability of a word j occurring in that place via the softmax, i.e.,
p(yt,j = 1 | yt−1, . . .",4.4 Seq2Seq,[0],[0]
", y1) = exp(wjh d t )",4.4 Seq2Seq,[0],[0]
"∑K
j′=1 exp(wj′h d t ) .
",4.4 Seq2Seq,[0],[0]
The model is trained via negative log likelihood.,4.4 Seq2Seq,[0],[0]
"The basic model can be extended to include persona information, in which case we simply prepend it to the input sequence x, i.e., x = ∀p ∈ P ||",4.4 Seq2Seq,[0],[0]
"x, where || denotes concatenation.",4.4 Seq2Seq,[0],[0]
"For the OpenSubtitles and Twitter datasets trained in Section 5.2 we found training a language model (LM), essentially just the decoder part of this model, worked better and we report that instead.",4.4 Seq2Seq,[0],[0]
"Finally, we introduce a generative model that encodes each of the profile entries as individual memory representations in a memory network.",4.5 Generative Profile Memory Network,[0],[0]
"As before, the dialogue history is encoded via LSTMenc, the final state of which is used as the initial hidden state of the decoder.",4.5 Generative Profile Memory Network,[0],[0]
"Each entry pi = 〈pi,1, . . .",4.5 Generative Profile Memory Network,[0],[0]
", pi,n〉 ∈ P is then encoded via f(pi)",4.5 Generative Profile Memory Network,[0],[0]
"=∑|pi|
j αipi,j .",4.5 Generative Profile Memory Network,[0],[0]
"That is, we weight words by their inverse term frequency: αi = 1/(1 + log(1 + tf)) where tf is computed from the GloVe index via
Zipf’s law4.",4.5 Generative Profile Memory Network,[0],[0]
Let F be the set of encoded memories.,4.5 Generative Profile Memory Network,[0],[0]
"The decoder now attends over the encoded profile entries, i.e., we compute the mask at, context ct and next input x̂t as:
at = softmax(FWah d t ),
ct = a ᵀ tF ; x̂t = tanh(Wc[ct−1, xt]).
",4.5 Generative Profile Memory Network,[0],[0]
"If the model has no profile information, and hence no memory, it becomes equivalent to the Seq2Seq model.",4.5 Generative Profile Memory Network,[0],[0]
"We first report results using automated evaluation metrics, and subsequently perform an extrinsic evaluation where crowdsourced workers perform a human evaluation of our models.",5 Experiments,[0],[0]
The main results are reported in Table 3.,5.1 Automated metrics,[0],[0]
"Overall, the results show the following key points:
Persona Conditioning Most models improve significantly when conditioning prediction on their own persona at least for the original (non-revised) versions, which is an easier task than the revised ones which have no word overlap.",5.1 Automated metrics,[0],[0]
"For example, the Profile Memory generation model has improved perplexity and hits@1 compared to Seq2Seq, and all the ranking algorithms (IR baseline, Starspace and Profile Memory Networks) obtain improved hits@1.
",5.1 Automated metrics,[0],[0]
Ranking vs. Generative.,5.1 Automated metrics,[0],[0]
Ranking models are far better than generative models at ranking.,5.1 Automated metrics,[0],[0]
"This is perhaps obvious as that is the metric they are optimizing, but still the performance difference is quite stark.",5.1 Automated metrics,[0],[0]
"It may be that the word-based probability which generative models use works well, but is not calibrated well enough to give a sentencebased probability which ranking requires.",5.1 Automated metrics,[0],[0]
"Human evaluation is also used to compare these methods, which we perform in Sec. 5.2.
",5.1 Automated metrics,[0],[0]
Ranking Models.,5.1 Automated metrics,[0],[0]
"For the ranking models, the IR baseline is outperformed by Starspace due to its learnt similarity metric, which in turn is outperformed by Profile Memory networks due to the attention mechanism over the profiles (as all other parts of the models are the same).",5.1 Automated metrics,[0],[0]
"Finally KV Profile Memory networks outperform Profile Memory Networks in the no persona case due to the ability to consider neighboring dialogue history and next
4tf = 1e6 ∗ 1/(idx1.07)
utterance pairs in the training set that are similar to the current dialogue, however when using persona information the performance is similar.
",5.1 Automated metrics,[0],[0]
Revised Personas.,5.1 Automated metrics,[0],[0]
Revised personas are much harder to use.,5.1 Automated metrics,[0],[0]
We do however still see some gain for the Profile Memory networks compared to none (0.354 vs. 0.318 hits@1).,5.1 Automated metrics,[0],[0]
"We also tried two variants of training: with the original personas in the training set or the revised ones, a comparison of which is shown in Table 6 of the Appendix.",5.1 Automated metrics,[0],[0]
"Training on revised personas helps, both for test examples that are in original form or revised form, likely due to the model be forced to learn more than simple word overlap, forcing the model to generalize more (i.e., learn semantic similarity of differing phrases).
",5.1 Automated metrics,[0],[0]
Their Persona.,5.1 Automated metrics,[0],[0]
"We can also condition a model on the other speaker’s persona, or both personas at once, the results of which are in Tables 5 and 6 in the Appendix.",5.1 Automated metrics,[0],[0]
Using “Their persona” has less impact on this dataset.,5.1 Automated metrics,[0],[0]
We believe this is because most speakers tend to focus on themselves when it comes to their interests.,5.1 Automated metrics,[0],[0]
It would be interesting how often this is the case in other datasets.,5.1 Automated metrics,[0],[0]
Certainly this is skewed by the particular instructions one could give to the crowdworkers.,5.1 Automated metrics,[0],[0]
"For example if we gave the instructions “try not to talk about yourself, but about the other’s interests’ likely these metrics would change.",5.1 Automated metrics,[0],[0]
"As automated metrics are notoriously poor for evaluating dialogue (Liu et al., 2016)",5.2 Human Evaluation,[0],[0]
we also perform human evaluation using crowdsourced workers.,5.2 Human Evaluation,[0],[0]
The procedure is as follows.,5.2 Human Evaluation,[0],[0]
We perform almost exactly the same setup as in the dataset collection process itself as in Section 3.3.,5.2 Human Evaluation,[0],[0]
"In that setup, we paired two Turkers and assigned them each a random (original) persona from the collected pool, and asked them to chat.",5.2 Human Evaluation,[0],[0]
"Here, from the Turker’s point of view everything looks the same except instead of being paired with a Turker they are paired with one of our models instead (they do not know this).",5.2 Human Evaluation,[0],[0]
"In this setting, for both the Turker and the model, the personas come from the test set pool.
",5.2 Human Evaluation,[0],[0]
"After the dialogue, we then ask the Turker some additional questions in order to evaluate the quality of the model.",5.2 Human Evaluation,[0],[0]
"We ask them to evaluate fluency, engagingness and consistency (scored between 1- 5).",5.2 Human Evaluation,[0],[0]
"Finally, we measure the ability to detect the other speaker’s profile by displaying two possible profiles, and ask which is more likely to be the profile of the person the Turker just spoke to.",5.2 Human Evaluation,[0],[0]
"More details of these measures are given in the Appendix.
",5.2 Human Evaluation,[0],[0]
"The results are reported in Table 4 for the best performing generative and ranking models, in both the No Persona and Self Persona categories, 100 dialogues each.",5.2 Human Evaluation,[0],[0]
We also evaluate the scores of human performance by replacing the chatbot with a human (another Turker).,5.2 Human Evaluation,[0],[0]
This effectively gives us upper bound scores which we can aim for with our models.,5.2 Human Evaluation,[0],[0]
"Finally, and importantly, we compare our models trained on PERSONA-CHAT with chit-chat models trained with the Twitter and OpenSubtitles datasets (2009 and 2018 versions) instead, following Vinyals and Le (2015).",5.2 Human Evaluation,[0],[0]
"Example chats from a few of the models are shown in the Appendix in Tables 7, 8, 9, 10, 11 and 12.
",5.2 Human Evaluation,[0],[0]
"Firstly, we see a difference in fluency, engagingness and consistency between all PERSONACHAT models and the models trained on OpenSubtitles and Twitter.",5.2 Human Evaluation,[0],[0]
"PERSONA-CHAT is a resource that is particularly strong at providing training data for the beginning of conversations, when the two speakers do not know each other, focusing on asking and answering questions, in contrast to other resources.",5.2 Human Evaluation,[0],[0]
"We also see suggestions of more subtle differences between the models, although these differences are obscured by the high variance of
the human raters’ evaluations.",5.2 Human Evaluation,[0],[0]
"For example, in both the generative and ranking model cases, models endowed with a persona can be detected by the human conversation partner, as evidenced by the persona detection accuracies, whilst maintaining fluency and consistency compared to their nonpersona driven counterparts.
",5.2 Human Evaluation,[0],[0]
"Finding the balance between fluency, engagement, consistency, and a persistent persona remains a strong challenge for future research.",5.2 Human Evaluation,[0],[0]
"Two tasks could naturally be considered using PERSONACHAT: (1) next utterance prediction during dialogue, and (2) profile prediction given dialogue history.",5.3 Profile Prediction,[0],[0]
"The main study of this work has been Task 1, where we have shown the use of profile information.",5.3 Profile Prediction,[0],[0]
"Task 2, however, can be used to extract such information.",5.3 Profile Prediction,[0],[0]
"While a full study is beyond the scope of this paper, we conducted some preliminary experiments, the details of which are in Appendix D. They show (i) human speaker’s profiles can be predicted from their dialogue with high accuracy (94.3%, similar to human performance in Table 4) or even from the model’s dialogue (23% with KV Profile Memory) showing the model is paying attention to the human’s interests.",5.3 Profile Prediction,[0],[0]
"Further, the accuracies clearly improve with further dialogue, as shown in Table 14.",5.3 Profile Prediction,[0],[0]
Combining Task 1 and Task 2 into a full system is an exciting area of future research.,5.3 Profile Prediction,[0],[0]
"In this work we have introduced the PERSONACHAT dataset, which consists of crowd-sourced dialogues where each participant plays the part of an assigned persona; and each (crowd-sourced) persona has a word-distinct paraphrase.",6 Conclusion & Discussion,[0],[0]
"We test various baseline models on this dataset, and show that models that have access to their own personas in addition to the state of the dialogue are scored as more consistent by annotators, although not more engaging.",6 Conclusion & Discussion,[0],[0]
"On the other hand, we show that models trained on PERSONA-CHAT (with or without personas) are more engaging than models trained on dialogue from other resources (movies, Twitter).
",6 Conclusion & Discussion,[0],[0]
We believe PERSONA-CHAT will be a useful resource for training components of future dialogue systems.,6 Conclusion & Discussion,[0],[0]
"Because we have paired human generated profiles and conversations, the data aids the construction of agents that have consistent per-
sonalities and viewpoints.",6 Conclusion & Discussion,[0],[0]
"Furthermore, predicting the profiles from a conversation moves chitchat tasks in the direction of goal-directed dialogue, which has metrics for success.",6 Conclusion & Discussion,[0],[0]
"Because we collect paraphrases of the profiles, they cannot be trivially matched; indeed, we believe the original and rephrased profiles are interesting as a semantic similarity dataset in their own right.",6 Conclusion & Discussion,[0],[0]
"We hope that the data will aid training agents that can ask questions about users’ profiles, remember the answers, and use them naturally in conversation.",6 Conclusion & Discussion,[0],[0]
"Chit-chat models are known to have several problems: they lack specificity, do not display a consistent personality and are often not very captivating.",abstractText,[0],[0]
In this work we present the task of making chit-chat more engaging by conditioning on profile information.,abstractText,[0],[0]
"We collect data and train models to (i) condition on their given profile information; and (ii) information about the person they are talking to, resulting in improved dialogues, as measured by next utterance prediction.",abstractText,[0],[0]
"Since (ii) is initially unknown, our model is trained to engage its partner with personal topics, and we show the resulting dialogue can be used to predict profile information about the interlocutors.",abstractText,[0],[0]
"Personalizing Dialogue Agents: I have a dog, do you have pets too?",title,[0],[0]
Machine Translation (MT) is a flagship of the recent successes and advances in the field of natural language processing.,1 Introduction,[0],[0]
"Its practical applications and use as a testbed for sequence transduction algorithms have spurred renewed interest in this topic.
",1 Introduction,[0],[0]
"While recent advances have reported near human-level performance on several language
†Sorbonne Universités, UPMC Univ Paris 06, CNRS, UMR 7606, LIP6, F-75005, Paris, France.
1https://github.com/facebookresearch/ UnsupervisedMT
pairs using neural approaches (Wu et al., 2016; Hassan et al., 2018), other studies have highlighted several open challenges (Koehn and Knowles, 2017; Isabelle et al., 2017; Sennrich, 2017).",1 Introduction,[0],[0]
A major challenge is the reliance of current learning algorithms on large parallel corpora.,1 Introduction,[0],[0]
"Unfortunately, the vast majority of language pairs have very little, if any, parallel data: learning algorithms need to better leverage monolingual data in order to make MT more widely applicable.
",1 Introduction,[0],[0]
"While a large body of literature has studied the use of monolingual data to boost translation performance when limited supervision is available, two recent approaches have explored the fully unsupervised setting (Lample et al., 2018; Artetxe et al., 2018), relying only on monolingual corpora in each language, as in the pioneering work by Ravi and Knight (2011).",1 Introduction,[0],[0]
"While there are subtle technical differences between these two recent works, we identify several common principles underlying their success.
",1 Introduction,[0],[0]
"First, they carefully initialize the MT system with an inferred bilingual dictionary.",1 Introduction,[0],[0]
"Second, they leverage strong language models, via training the sequence-to-sequence system (Sutskever et al., 2014; Bahdanau et al., 2015) as a denoising autoencoder (Vincent et al., 2008).",1 Introduction,[0],[0]
"Third, they turn the unsupervised problem into a supervised one by automatic generation of sentence pairs via back-translation (Sennrich et al., 2015a), i.e., the source-to-target model is applied to source sentences to generate inputs for training the targetto-source model, and vice versa.",1 Introduction,[0],[0]
"Finally, they constrain the latent representations produced by the encoder to be shared across the two languages.",1 Introduction,[0],[0]
"Empirically, these methods achieve remarkable results considering the fully unsupervised setting; for instance, about 15 BLEU points on the WMT’14 English-French benchmark.
",1 Introduction,[0],[0]
"The first contribution of this paper is a model
ar X
iv :1
80 4.
07 75
5v 2
[ cs
.C",1 Introduction,[0],[0]
"L
] 1
3 A
ug 2
01 8
that combines these two previous neural approaches, simplifying the architecture and loss function while still following the above mentioned principles.",1 Introduction,[0],[0]
The resulting model outperforms previous approaches and is both easier to train and tune.,1 Introduction,[0],[0]
"Then, we apply the same ideas and methodology to a traditional phrase-based statistical machine translation (PBSMT) system (Koehn et al., 2003).",1 Introduction,[0],[0]
"PBSMT models are well-known to outperform neural models when labeled data is scarce because they merely count occurrences, whereas neural models typically fit hundred of millions of parameters to learn distributed representations, which may generalize better when data is abundant but is prone to overfit when data is scarce.",1 Introduction,[0],[0]
"Our PBSMT model is simple, easy to interpret, fast to train and often achieves similar or better results than its NMT counterpart.",1 Introduction,[0],[0]
"We report gains of up to +10 BLEU points on widely used benchmarks when using our NMT model, and up to +12 points with our PBSMT model.",1 Introduction,[0],[0]
"Furthermore, we apply these methods to distant and low-resource languages, like EnglishRussian, English-Romanian and English-Urdu, and report competitive performance against both semi-supervised and supervised baselines.",1 Introduction,[0],[0]
"Learning to translate with only monolingual data is an ill-posed task, since there are potentially many ways to associate target with source sentences.",2 Principles of Unsupervised MT,[0],[0]
"Nevertheless, there has been exciting progress towards this goal in recent years, as discussed in the related work of Section 5.",2 Principles of Unsupervised MT,[0],[0]
"In this sec-
tion, we abstract away from the specific assumptions made by each prior work and instead focus on identifying the common principles underlying unsupervised MT.
",2 Principles of Unsupervised MT,[0],[0]
"We claim that unsupervised MT can be accomplished by leveraging the three components illustrated in Figure 1: (i) suitable initialization of the translation models, (ii) language modeling and (iii) iterative back-translation.",2 Principles of Unsupervised MT,[0],[0]
"In the following, we describe each of these components and later discuss how they can be better instantiated in both a neural model and phrase-based model.
",2 Principles of Unsupervised MT,[0],[0]
Initialization:,2 Principles of Unsupervised MT,[0],[0]
"Given the ill-posed nature of the task, model initialization expresses a natural prior over the space of solutions we expect to reach, jump-starting the process by leveraging approximate translations of words, short phrases or even sub-word units (Sennrich et al., 2015b).",2 Principles of Unsupervised MT,[0],[0]
"For instance, Klementiev et al. (2012) used a provided bilingual dictionary, while Lample et al. (2018) and Artetxe et al. (2018) used dictionaries inferred in an unsupervised way (Conneau et al., 2018; Artetxe et al., 2017).",2 Principles of Unsupervised MT,[0],[0]
"The motivating intuition is that while such initial “word-by-word” translation may be poor if languages or corpora are not closely related, it still preserves some of the original semantics.
",2 Principles of Unsupervised MT,[0],[0]
Language Modeling:,2 Principles of Unsupervised MT,[0],[0]
"Given large amounts of monolingual data, we can train language models on both source and target languages.",2 Principles of Unsupervised MT,[0],[0]
"These models express a data-driven prior about how sentences should read in each language, and they improve the quality of the translation models by per-
Algorithm 1:",2 Principles of Unsupervised MT,[0],[0]
"Unsupervised MT 1 Language models: Learn language models Ps and Pt
over source and target languages; 2 Initial translation models: Leveraging Ps and Pt,
learn two initial translation models, one in each direction: P (0)s→t and P (0) t→s;
3 for k=1 to N do 4 Back-translation: Generate source and target
sentences using the current translation models, P (k−1) t→s and P (k−1) s→t , factoring in language
models, Ps and Pt; 5 Train new translation models P (k)s→t and P (k) t→s
using the generated sentences and leveraging Ps and Pt;
6 end
forming local substitutions and word reorderings.
",2 Principles of Unsupervised MT,[0],[0]
Iterative Back-translation:,2 Principles of Unsupervised MT,[0],[0]
"The third principle is back-translation (Sennrich et al., 2015a), which is perhaps the most effective way to leverage monolingual data in a semi-supervised setting.",2 Principles of Unsupervised MT,[0],[0]
Its application in the unsupervised setting is to couple the source-to-target translation system with a backward model translating from the target to source language.,2 Principles of Unsupervised MT,[0],[0]
The goal of this model is to generate a source sentence for each target sentence in the monolingual corpus.,2 Principles of Unsupervised MT,[0],[0]
"This turns the daunting unsupervised problem into a supervised learning task, albeit with noisy source sentences.",2 Principles of Unsupervised MT,[0],[0]
"As the original model gets better at translating, we use the current model to improve the back-translation model, resulting in a coupled system trained with an iterative algorithm (He et al., 2016).",2 Principles of Unsupervised MT,[0],[0]
"Equipped with the three principles detailed in Section 2, we now discuss how to effectively combine them in the context of a NMT model (Section 3.1) and PBSMT model (Section 3.2).
",3 Unsupervised MT systems,[0],[0]
"In the reminder of the paper, we denote the space of source and target sentences by S and T , respectively, and the language models trained on source and target monolingual datasets by Ps and Pt, respectively.",3 Unsupervised MT systems,[0],[0]
"Finally, we denote by Ps→t and Pt→s the translation models from source to target and vice versa.",3 Unsupervised MT systems,[0],[0]
An overview of our approach is given in Algorithm 1.,3 Unsupervised MT systems,[0],[0]
"We now introduce a new unsupervised NMT method, which is derived from earlier work by Artetxe et al. (2018) and Lample et al. (2018).",3.1 Unsupervised NMT,[0],[0]
"We first discuss how the previously mentioned
three key principles are instantiated in our work, and then introduce an additional property of the system, the sharing of internal representations across languages, which is specific and critical to NMT.",3.1 Unsupervised NMT,[0],[0]
"From now on, we assume that a NMT model consists of an encoder and a decoder.",3.1 Unsupervised NMT,[0],[0]
"Section 4 gives the specific details of this architecture.
",3.1 Unsupervised NMT,[0],[0]
"Initialization: While prior work relied on bilingual dictionaries, here we propose a more effective and simpler approach which is particularly suitable for related languages.2 First, instead of considering words, we consider byte-pair encodings (BPE) (Sennrich et al., 2015b), which have two major advantages: they reduce the vocabulary size and they eliminate the presence of unknown words in the output translation.",3.1 Unsupervised NMT,[0.9556538293459939],"['We study the design of practically useful, theoretically wellfounded, general-purpose algorithms for the contextual bandits (CBs) problem.']"
"Second, instead of learning an explicit mapping between BPEs in the source and target languages, we define BPE tokens by jointly processing both monolingual corpora.",3.1 Unsupervised NMT,[0],[0]
"If languages are related, they will naturally share a good fraction of BPE tokens, which eliminates the need to infer a bilingual dictionary.",3.1 Unsupervised NMT,[0],[0]
"In practice, we i) join the monolingual corpora, ii) apply BPE tokenization on the resulting corpus, and iii) learn token embeddings (Mikolov et al., 2013) on the same corpus, which are then used to initialize the lookup tables in the encoder and decoder.
Language Modeling: In NMT, language modeling is accomplished via denoising autoencoding, by minimizing:
Llm = Ex∼S",3.1 Unsupervised NMT,[0],[0]
[− logPs→s(x|C(x))],3.1 Unsupervised NMT,[0],[0]
+ Ey∼T [− logPt→t(y|C(y))],3.1 Unsupervised NMT,[0],[0]
"(1)
where C is a noise model with some words dropped and swapped as in Lample et al. (2018).",3.1 Unsupervised NMT,[0],[0]
"Ps→s andPt→t are the composition of encoder and decoder both operating on the source and target sides, respectively.
",3.1 Unsupervised NMT,[0],[0]
Back-translation: Let us denote by u∗(y) the sentence in the source language inferred from y ∈ T such that u∗(y) = argmaxPt→s(u|y).,3.1 Unsupervised NMT,[0],[0]
"Similarly, let us denote by v∗(x) the sentence in the target language inferred from x ∈ S such that v∗(x) = argmaxPs→t(v|x).",3.1 Unsupervised NMT,[0],[0]
"The pairs (u∗(y), y) and (x, v∗(x))) constitute automatically-generated parallel sentences which, following the back-translation principle, can be
2For unrelated languages, we need to infer a dictionary to properly initialize the embeddings (Conneau et al., 2018).
",3.1 Unsupervised NMT,[0],[0]
"used to train the two MT models by minimizing the following loss:
Lback = Ey∼T [− logPs→t(y|u∗(y))",3.1 Unsupervised NMT,[0],[0]
],3.1 Unsupervised NMT,[0],[0]
+ Ex∼S,3.1 Unsupervised NMT,[0],[0]
[− logPt→s(x|v∗(x))].,3.1 Unsupervised NMT,[0],[0]
"(2)
Note that when minimizing this objective function we do not back-prop through the reverse model which generated the data, both for the sake of simplicity and because we did not observe improvements when doing so.",3.1 Unsupervised NMT,[0],[0]
"The objective function minimized at every iteration of stochastic gradient descent, is simply the sum of Llm in Eq. 1 and Lback in Eq. 2.",3.1 Unsupervised NMT,[0],[0]
"To prevent the model from cheating by using different subspaces for the language modeling and translation tasks, we add an additional constraint which we discuss next.
",3.1 Unsupervised NMT,[0],[0]
"Sharing Latent Representations: A shared encoder representation acts like an interlingua, which is translated in the decoder target language regardless of the input source language.",3.1 Unsupervised NMT,[0],[0]
"This ensures that the benefits of language modeling, implemented via the denoising autoencoder objective, nicely transfer to translation from noisy sources and eventually help the NMT model to translate more fluently.",3.1 Unsupervised NMT,[0],[0]
"In order to share the encoder representations, we share all encoder parameters (including the embedding matrices since we perform joint tokenization) across the two languages to ensure that the latent representation of the source sentence is robust to the source language.",3.1 Unsupervised NMT,[0],[0]
"Similarly, we share the decoder parameters across the two languages.",3.1 Unsupervised NMT,[0],[0]
"While sharing the encoder is critical to get the model to work, sharing the decoder simply induces useful regularization.",3.1 Unsupervised NMT,[0],[0]
"Unlike prior work (Johnson et al., 2016), the first token of the decoder specifies the language the module is operating with, while the encoder does not have any language identifier.",3.1 Unsupervised NMT,[0],[0]
"In this section, we discuss how to perform unsupervised machine translation using a PhraseBased Statistical Machine Translation (PBSMT) system (Koehn et al., 2003) as the underlying backbone model.",3.2 Unsupervised PBSMT,[0],[0]
"Note that PBSMT models are known to perform well on low-resource language pairs, and are therefore a potentially good alternative to neural models in the unsupervised setting.
",3.2 Unsupervised PBSMT,[0],[0]
"When translating from x to y, a PBSMT system scores y according to: argmaxy P (y|x) = argmaxy P (x|y)P (y), where P (x|y) is derived
from so called “phrase tables”, and P (y) is the score assigned by a language model.
",3.2 Unsupervised PBSMT,[0],[0]
"Given a dataset of bitexts, PBSMT first infers an alignment between source and target phrases.",3.2 Unsupervised PBSMT,[0],[0]
"It then populates phrase tables, whose entries store the probability that a certain n-gram in the source/target language is mapped to another ngram in the target/source language.
",3.2 Unsupervised PBSMT,[0],[0]
"In the unsupervised setting, we can easily train a language model on monolingual data, but it is less clear how to populate the phrase tables, which are a necessary component for good translation.",3.2 Unsupervised PBSMT,[0],[0]
"Fortunately, similar to the neural case, the principles of Section 2 are effective to solve this problem.
",3.2 Unsupervised PBSMT,[0],[0]
Initialization: We populate the initial phrase tables (from source to target and from target to source) using an inferred bilingual dictionary built from monolingual corpora using the method proposed by Conneau et al. (2018).,3.2 Unsupervised PBSMT,[0],[0]
"In the following, we will refer to phrases as single words, but the very same arguments trivially apply to longer ngrams.",3.2 Unsupervised PBSMT,[0],[0]
"Phrase tables are populated with the scores of the translation of a source word to:
p(tj |si) = e 1 T cos(e(tj),We(si))∑
k e 1 T cos(e(tk),We(si))
, (3)
where tj is the j-th word in the target vocabulary and si is the i-th word in the source vocabulary, T is a hyper-parameter used to tune the peakiness of the distribution3, W is the rotation matrix mapping the source embeddings into the target embeddings (Conneau et al., 2018), and e(x) is the embedding of x.
Language Modeling: Both in the source and target domains we learn smoothed n-gram language models using KenLM (Heafield, 2011), although neural models could also be considered.",3.2 Unsupervised PBSMT,[0],[0]
"These remain fixed throughout training iterations.
",3.2 Unsupervised PBSMT,[0],[0]
"Iterative Back-Translation: To jump-start the iterative process, we use the unsupervised phrase tables and the language model on the target side to construct a seed PBSMT.",3.2 Unsupervised PBSMT,[0],[0]
We then use this model to translate the source monolingual corpus into the target language (back-translation step).,3.2 Unsupervised PBSMT,[0],[0]
"Once the data has been generated, we train a PBSMT in supervised mode to map the generated data back to the original source sentences.",3.2 Unsupervised PBSMT,[0],[0]
"Next, we perform
3We set T = 30 in all our experiments, following the setting of Smith et al. (2017).
",3.2 Unsupervised PBSMT,[0],[0]
both generation and training process but in the reverse direction.,3.2 Unsupervised PBSMT,[0],[0]
"We repeat these steps as many times as desired (see Algorithm 2 in Section A).
",3.2 Unsupervised PBSMT,[0],[0]
"Intuitively, many entries in the phrase tables are not correct because the input to the PBSMT at any given point during training is noisy.",3.2 Unsupervised PBSMT,[0],[0]
"Despite that, the language model may be able to fix some of these mistakes at generation time.",3.2 Unsupervised PBSMT,[0],[0]
"As long as that happens, the translation improves, and with that also the phrase tables at the next round.",3.2 Unsupervised PBSMT,[0],[0]
"There will be more entries that correspond to correct phrases, which makes the PBSMT model stronger because it has bigger tables and it enables phrase swaps over longer spans.",3.2 Unsupervised PBSMT,[0],[0]
We first describe the datasets and experimental protocol we used.,4 Experiments,[0],[0]
"Then, we compare the two proposed unsupervised approaches to earlier attempts, to semi-supervised methods and to the very same models but trained with varying amounts of labeled data.",4 Experiments,[0],[0]
We conclude with an ablation study to understand the relative importance of the three principles introduced in Section 2.,4 Experiments,[0],[0]
"We consider five language pairs: English-French, English-German, English-Romanian, EnglishRussian and English-Urdu.",4.1 Datasets and Methodology,[0],[0]
"The first two pairs are used to compare to recent work on unsupervised MT (Artetxe et al., 2018; Lample et al., 2018).",4.1 Datasets and Methodology,[0],[0]
"The last three pairs are instead used to test our PBSMT unsupervised method on truly low-resource pairs (Gu et al., 2018) or unrelated languages that do not even share the same alphabet.
",4.1 Datasets and Methodology,[0],[0]
"For English, French, German and Russian, we use all available sentences from the WMT monolingual News Crawl datasets from years 2007 through 2017.",4.1 Datasets and Methodology,[0],[0]
"For Romanian, the News Crawl dataset is only composed of 2.2 million sentences, so we augment it with the monolingual data from WMT’16, resulting in 2.9 million sentences.",4.1 Datasets and Methodology,[0],[0]
"In Urdu, we use the dataset of Jawaid et al. (2014), composed of about 5.5 million monolingual sentences.",4.1 Datasets and Methodology,[0],[0]
"We report results on newstest 2014 for en− fr, and newstest 2016 for en− de, en− ro and en− ru.",4.1 Datasets and Methodology,[0],[0]
"For Urdu, we use the LDC2010T21 and LDC2010T23 corpora each with about 1800 sentences as validation and test sets, respectively.
",4.1 Datasets and Methodology,[0],[0]
"We use Moses scripts (Koehn et al., 2007) for tokenization.",4.1 Datasets and Methodology,[0],[0]
"NMT is trained with 60,000 BPE
codes.",4.1 Datasets and Methodology,[0],[0]
"PBSMT is trained with true-casing, and by removing diacritics from Romanian on the source side to deal with their inconsistent use across the monolingual dataset (Sennrich et al., 2016).",4.1 Datasets and Methodology,[0],[0]
Both the NMT and PBSMT approaches require either cross-lingual BPE embeddings (to initialize the shared lookup tables) or n-gram embeddings (to initialize the phrase table).,4.2 Initialization,[0],[0]
"We generate embeddings using fastText (Bojanowski et al., 2017) with an embedding dimension of 512, a context window of size 5 and 10 negative samples.",4.2 Initialization,[0],[0]
"For NMT, fastText is applied on the concatenation of source and target corpora, which results in crosslingual BPE embeddings.
",4.2 Initialization,[0],[0]
"For PBSMT, we generate n-gram embeddings on the source and target corpora independently, and align them using the MUSE library (Conneau et al., 2018).",4.2 Initialization,[0],[0]
"Since learning unique embeddings of every possible phrase would be intractable, we consider the most frequent 300,000 source phrases, and align each of them to its 200 nearest neighbors in the target space, resulting in a phrase table of 60 million phrase pairs which we score using the formula in Eq. 3.
",4.2 Initialization,[0],[0]
"In practice, we observe a small but significant difference of about 1 BLEU point using a phrase table of bigrams compared to a phrase table of unigrams, but did not observe any improvement using longer phrases.",4.2 Initialization,[0],[0]
"Table 1 shows an extract of a French-English unsupervised phrase table, where we can see that unigrams are correctly aligned to bigrams, and vice versa.",4.2 Initialization,[0],[0]
The next subsections provide details about the architecture and training procedure of our models.,4.3 Training,[0],[0]
"In this study, we use NMT models built upon LSTM (Hochreiter and Schmidhuber, 1997) and Transformer (Vaswani et al., 2017) cells.",4.3.1 NMT,[0],[0]
For the LSTM model we use the same architecture as in Lample et al. (2018).,4.3.1 NMT,[0],[0]
"For the Transformer, we use 4 layers both in the encoder and in the decoder.",4.3.1 NMT,[0],[0]
"Following Press and Wolf (2016), we share all lookup tables between the encoder and the decoder, and between the source and the target languages.",4.3.1 NMT,[0],[0]
The dimensionality of the embeddings and of the hidden layers is set to 512.,4.3.1 NMT,[0],[0]
"We used the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 10−4, β1 = 0.5, and a batch size of 32.",4.3.1 NMT,[0],[0]
"At decoding time, we generate greedily.",4.3.1 NMT,[0],[0]
The PBSMT uses Moses’ default smoothed ngram language model with phrase reordering disabled during the very first generation.,4.3.2 PBSMT,[0],[0]
PBSMT is trained in a iterative manner using Algorithm 2.,4.3.2 PBSMT,[0],[0]
"At each iteration, we translate 5 million sentences randomly sampled from the monolingual dataset in the source language.",4.3.2 PBSMT,[0],[0]
"Except for initialization, we use phrase tables with phrases up to length 4.",4.3.2 PBSMT,[0],[0]
"Moses’ implementation of PBSMT has 15 hyperparameters, such as relative weighting of each scoring function, word penalty, etc.",4.4 Model selection,[0],[0]
"In this work, we consider two methods to set these hyperparameters.",4.4 Model selection,[0],[0]
"We either set them to their default values in the toolbox, or we set them using a small validation set of parallel sentences.",4.4 Model selection,[0],[0]
"It turns out
that with only 100 labeled sentences in the validation set, PBSMT would overfit to the validation set.",4.4 Model selection,[0],[0]
"For instance, on en → fr, PBSMT tuned on 100 parallel sentences obtains a BLEU score of 26.42 on newstest 2014, compared to 27.09 with default hyper-parameters, and 28.02 when tuned on the 3000 parallel sentences of newstest 2013.",4.4 Model selection,[0],[0]
"Therefore, unless otherwise specified, all PBSMT models considered in the paper use default hyperparameter values, and do not use any parallel resource whatsoever.
",4.4 Model selection,[0],[0]
"For the NMT, we also consider two model selection procedures: an unsupervised criterion based on the BLEU score of a “round-trip” translation (source → target → source and target → source → target) as in Lample et al. (2018), and crossvalidation using a small validation set with 100 parallel sentences.",4.4 Model selection,[0],[0]
"In our experiments, we found the unsupervised criterion to be highly correlated with the test metric when using the Transformer model, but not always for the LSTM.",4.4 Model selection,[0],[0]
"Therefore, unless otherwise specified, we select the best LSTM models using a small validation set of 100 parallel sentences, and the best Transformer models with the unsupervised criterion.",4.4 Model selection,[0],[0]
The results reported in Table 2 show that our unsupervised NMT and PBSMT systems largely outperform previous unsupervised baselines.,4.5 Results,[0],[0]
We report large gains on all language pairs and directions.,4.5 Results,[0],[0]
"For instance, on the en → fr task, our unsupervised PBSMT obtains a BLEU score of 28.1, outperforming the previous best result by more than 11 BLEU points.",4.5 Results,[0],[0]
"Even on a more complex task like en → de, both PBSMT and NMT surpass the baseline score by more than 10 BLEU
points.",4.5 Results,[0],[0]
"Even before iterative back-translation, the PBSMT model significantly outperforms previous approaches, and can be trained in a few minutes.
",4.5 Results,[0],[0]
Table 3 illustrates the quality of the PBSMT model during the iterative training process.,4.5 Results,[0],[0]
"For instance, the fr → en model obtains a BLEU score of 17.5 at iteration 0 – i.e. after the unsupervised phrase table construction – while it achieves a score of 27.2 at iteration 4.",4.5 Results,[0],[0]
This highlights the importance of multiple back-translation iterations.,4.5 Results,[0],[0]
The last rows of Table 3 also show that we get additional gains by further tuning the NMT model on the data generated by PBSMT (PBSMT + NMT).,4.5 Results,[0],[0]
We simply add the data generated by the unsupervised PBSMT system to the back-translated data produced by the NMT model.,4.5 Results,[0],[0]
"By combining PBSMT and NMT, we achieve BLEU scores of 20.2 and 25.2 on the challenging en → de and de → en translation tasks.",4.5 Results,[0],[0]
"While we also tried bootstraping the PBSMT model with back-translated data generated by a NMT model (NMT + PBSMT), this did not improve over PBSMT alone.
",4.5 Results,[0],[0]
"Next, we compare to fully supervised models.",4.5 Results,[0],[0]
Figure 2 shows the performance of the same architectures trained in a fully supervised way using parallel training sets of varying size.,4.5 Results,[0],[0]
"The unsupervised PBSMT model achieves the same performance as its supervised counterpart trained on more than 100,000 parallel sentences.
",4.5 Results,[0],[0]
This is confirmed on low-resource languages.,4.5 Results,[0],[0]
"In particular, on ro → en, our unsupervised PBSMT model obtains a BLEU score of 23.9, outperforming Gu et al. (2018)’s method by 1 point, despite its use of 6,000 parallel sentences, a seed dictionary, and a multi-NMT system combining par-
allel resources from 5 different languages.",4.5 Results,[0],[0]
"On Russian, our unsupervised PBSMT model obtains a BLEU score of 16.6 on ru→ en, showing that this approach works reasonably well on distant languages.",4.5 Results,[0],[0]
"Finally we train on ur → en, which is both low resource and distant.",4.5 Results,[0],[0]
"In a supervised mode, PBSMT using the noisy and outof-domain 800,000 parallel sentences from Tiedemann (2012) achieves a BLEU score of 9.8.",4.5 Results,[0],[0]
"Instead, our unsupervised PBSMT system achieves 12.3 BLEU using only a validation set of 1800 sentences to tune Moses hyper-parameters.",4.5 Results,[0],[0]
"In Figure 3 we report results from an ablation study, to better understand the importance of the three principles when training PBSMT.",4.6 Ablation Study,[0],[0]
"This study shows that more iterations only partially compensate for lower quality phrase table initialization (Left), language models trained over less data (Middle) or less monolingual data (Right).",4.6 Ablation Study,[0],[0]
"Moreover, the influence of the quality of the language model becomes more prominent as we iterate.",4.6 Ablation Study,[0],[0]
"These findings suggests that better initialization methods and more powerful language models may further improve our results.
",4.6 Ablation Study,[0],[0]
We perform a similar ablation study for the NMT system (see Appendix).,4.6 Ablation Study,[0],[0]
"We find that backtranslation and auto-encoding are critical components, without which the system fails to learn.",4.6 Ablation Study,[0],[0]
"We also find that initialization of embeddings is very important, and we gain 7 BLEU points compared to prior work (Artetxe et al., 2018; Lample et al., 2018) by learning BPE embeddings over the concatenated monolingual corpora.",4.6 Ablation Study,[0],[0]
A large body of literature has studied using monolingual data to boost translation performance when limited supervision is available.,5 Related Work,[0],[0]
"This limited supervision is typically provided as a small set of parallel sentences (Sennrich et al., 2015a; Gulcehre et al., 2015; He et al., 2016; Gu et al., 2018; Wang et al., 2018); large sets of parallel sentences in related languages (Firat et al., 2016; Johnson et al., 2016; Chen et al., 2017; Zheng et al., 2017); cross-lingual dictionaries (Klementiev et al., 2012; Irvine and Callison-Burch, 2014, 2016); or comparable corpora (Munteanu et al., 2004; Irvine and Callison-Burch, 2013).
",5 Related Work,[0],[0]
"Learning to translate without any form of supervision has also attracted interest, but is challenging.",5 Related Work,[0],[0]
"In their seminal work, Ravi and Knight (2011) leverage linguistic prior knowledge to reframe the unsupervised MT task as deciphering and demonstrate the feasibility on short sentences with limited vocabulary.",5 Related Work,[0],[0]
"Earlier work by Carbonell et al. (2006) also aimed at unsupervised MT, but leveraged a bilingual dictionary to seed the translation.",5 Related Work,[0],[0]
"Both works rely on a language model on the target side to correct for translation fluency.
",5 Related Work,[0],[0]
"Subsequent work (Klementiev et al., 2012; Irvine and Callison-Burch, 2014, 2016) relied on bilingual dictionaries, small parallel corpora of several thousand sentences, and linguistically motivated features to prune the search space.",5 Related Work,[0],[0]
Irvine and Callison-Burch (2014) use monolingual data to expand phrase tables learned in a supervised setting.,5 Related Work,[0],[0]
"In our work we also expand phrase tables, but we initialize them with an inferred bilingual n-gram dictionary, following work from the connectionist community aimed at improving PBSMT with neural models (Schwenk, 2012; Kalchbrenner and Blunsom, 2013; Cho et al., 2014).
",5 Related Work,[0],[0]
"In recent years back-translation has become a
popular method of augmenting training sets with monolingual data on the target side (Sennrich et al., 2015a), and has been integrated in the “dual learning” framework of He et al. (2016) and subsequent extensions (Wang et al., 2018).",5 Related Work,[0],[0]
"Our approach is similar to the dual learning framework, except that in their model gradients are backpropagated through the reverse model and they pretrain using a relatively large amount of labeled data, whereas our approach is fully unsupervised.
",5 Related Work,[0],[0]
"Finally, our work can be seen as an extension of recent studies (Lample et al., 2018; Artetxe et al., 2018; Yang et al., 2018) on fully unsupervised MT with two major contributions.",5 Related Work,[0],[0]
"First, we propose a much simpler and more effective initialization method for related languages.",5 Related Work,[0],[0]
"Second, we abstract away three principles of unsupervised MT and apply them to a PBSMT, which even outperforms the original NMT.",5 Related Work,[0],[0]
"Moreover, our results show that the combination of PBSMT and NMT achieves even better performance.",5 Related Work,[0],[0]
"In this work, we identify three principles underlying recent successes in fully unsupervised MT and show how to apply these principles to PBSMT and NMT systems.",6 Conclusions and Future Work,[0],[0]
"We find that PBSMT systems often outperform NMT systems in the fully unsupervised setting, and that by combining these systems we can greatly outperform previous approaches from the literature.",6 Conclusions and Future Work,[0],[0]
"We apply our approach to several popular benchmark language pairs, obtaining state of the art results, and to several low-resource and under-explored language pairs.
",6 Conclusions and Future Work,[0],[0]
"It’s an open question whether there are more effective instantiations of these principles or other principles altogether, and under what conditions our iterative process is guaranteed to converge.",6 Conclusions and Future Work,[0],[0]
Future work may also extend to the semisupervised setting.,6 Conclusions and Future Work,[0],[0]
"In this Appendix, we report the detailed algorithm for unsupervised PBSMT, a detailed ablation study using NMT and conclude with some example translations.
",A Supplemental Material,[0],[0]
"Algorithm 2: Unsupervised PBSMT 1 Learn bilingual dictionary using Conneau
et al. (2018); 2 Populate phrase tables using Eq. 3 and learn a
language model to build P (0)s→t;
3 Use P (0)s→t to translate the source monolingual
dataset, yielding D(0)t ; 4 for i=1 to N do 5 Train model P (i)t→s using D (i−1) t ; 6 Use P (i)t→s to translate the target monolingual dataset, yielding D(i)s ; 7 Train model P (i)s→t using D (i) s ; 8 Use P (i)s→t to translate the source monolingual dataset, yielding D(i)t ; 9 end
A.1 NMT Ablation study In Table 4 we report results from an ablation study we performed for NMT using the Transformer architecture.",A Supplemental Material,[0],[0]
"All results are for the en→ fr task.
",A Supplemental Material,[0],[0]
"First, we analyze the effect of different initialization methods for the embedding matrices.",A Supplemental Material,[0],[0]
"If we switch from BPE tokens to words, BLEU drops by 4 points.",A Supplemental Material,[0],[0]
"If we used BPE but train embeddings in each language independently and then map them via MUSE (Conneau et al., 2018), BLEU drops by 3 points.",A Supplemental Material,[0],[0]
"Finally, compared to the word aligned procedure used by Lample et al. (2018), based on words and MUSE, the gain is about 7 points.",A Supplemental Material,[0],[0]
"To stress the importance of initialization, we also report performance using random initialization of BPE embeddings.",A Supplemental Material,[0],[0]
"In this case, convergence is much slower and to a much lower accuracy, achieving a BLEU score of 10.5.
",A Supplemental Material,[0],[0]
"The table also demonstrates the critical importance of the auto-encoding and back-translation terms in the loss, and the robustness of our approach to choice of architectures.
",A Supplemental Material,[0],[0]
"A.2 Qualitative study Table 5 shows example translations from the French-English newstest 2014 dataset at different
iterations of the learning algorithm for both NMT and PBSMT models.",A Supplemental Material,[0],[0]
"Prior to the first iteration of back-translation, using only the unsupervised phrase table, the PBSMT translations are similar to word-by-word translations and do not respect the syntax of the target language, yet still contain most of the semantics of the original sentences.",A Supplemental Material,[0],[0]
"As we increase the number of epochs in NMT and as we iterate for PBSMT, we observe a continuous improvement in the quality of the unsupervised translations.",A Supplemental Material,[0],[0]
"Interestingly, in the second example, both the PBSMT and NMT models fail to adapt to the polysemy of the French word “langue”, which can be translated as “tongue” or “language” in English.",A Supplemental Material,[0],[0]
"These translations were both present in the unsupervised phrase table, but the conditional probability of “language” to be the correct translation of “langue” was very high compared to the one of “tongue”: P (language|langue) = 0.92, while P (tongue|langue) = 0.0005.",A Supplemental Material,[0],[0]
"As a comparison, the phrase table of a Moses model trained in a supervised way contains P (language|langue) = 0.633, P (tongue|langue) = 0.0076, giving a higher probability for “langue” to be properly translated.",A Supplemental Material,[0],[0]
"This underlines the importance of the initial unsupervised phrase alignment procedure, as it was shown in Figure 1.
",A Supplemental Material,[0],[0]
"Finally, in Table 6 we report a random subset of test sentences translated from Russian to English, showing that the model mostly retains the semantics, while making some mistakes in the grammar as well as in the choice of words and entities.",A Supplemental Material,[0],[0]
"In Table 7, we show examples of translations from German to English with PBSMT, NMT, and PBSMT+NMT to show how the combination of these models performs better than them individually.
",A Supplemental Material,[0],[0]
"German→ English
Source Flüchtlinge brauchen Unterkünfte : Studie warnt vor Wohnungslücke PBSMT Refugees need accommodation : Study warns Wohnungslücke NMT Refugees need forestry study to warn housing gap PBSMT+NMT",A Supplemental Material,[0],[0]
"Refugees need accommodation : Study warns of housing gap Reference Refugees need accommodation : study warns of housing gap
Source Konflikte : Mehrheit unterstützt die Anti-IS-Ausbildungsmission PBSMT Conflict : Majority supports Anti-IS-Ausbildungsmission NMT Tensions support majority anti-IS-recruiting mission PBSMT+NMT",A Supplemental Material,[0],[0]
"Tensions : Majority supports the anti-IS-recruitment mission Reference Conflicts : the majority support anti ISIS training mission
Source Roboterautos : Regierung will Vorreiterrolle für Deutschland PBSMT Roboterautos : Government will step forward for Germany NMT Robotic cars will pre-reiterate government for Germany PBSMT+NMT",A Supplemental Material,[0],[0]
"Robotic cars : government wants pre-orders for Germany Reference Robot cars : Government wants Germany to take a leading role
Source Pfund steigt durch beschleunigtes Lohnwachstum im Vereinigten Königreich PBSMT Pound rises as UK beschleunigtes Lohnwachstum .",A Supplemental Material,[0],[0]
NMT £ rises through rapid wage growth in the U.S. PBSMT+NMT,A Supplemental Material,[0],[0]
"Pound rises by accelerating wage growth in the United Kingdom Reference Pound rises as UK wage growth accelerates
Source 46 Prozent sagten , dass sie die Tür offen lassen , um zu anderen Kandidaten zu wechseln .",A Supplemental Material,[0],[0]
PBSMT 52 per cent said that they left the door open to them to switch to other candidates .,A Supplemental Material,[0],[0]
NMT 46 percent said that they would let the door open to switch to other candidates .,A Supplemental Material,[0],[0]
PBSMT+NMT,A Supplemental Material,[0],[0]
46 percent said that they left the door open to switch to other candidates .,A Supplemental Material,[0],[0]
"Reference 46 percent said they are leaving the door open to switching candidates .
",A Supplemental Material,[0],[0]
"Source Selbst wenn die Republikaner sich um einen anderen Kandidaten sammelten , schlägt Trump noch fast jeden .",A Supplemental Material,[0],[0]
"PBSMT Even if the Republicans a way to other candidates collected , beats Trump , yet almost everyone .",A Supplemental Material,[0],[0]
NMT,A Supplemental Material,[0],[0]
"Even if Republicans are to donate to a different candidate , Trump takes to almost every place . PBSMT+NMT",A Supplemental Material,[0],[0]
"Even if Republicans gather to nominate another candidate , Trump still beats nearly everyone .",A Supplemental Material,[0],[0]
"Reference Even if Republicans rallied around another candidate , Trump still beats almost everyone .
",A Supplemental Material,[0],[0]
"Source Ich glaube sicher , dass es nicht genügend",A Supplemental Material,[0],[0]
"Beweise gibt , um ein Todesurteil zu rechtfertigen .",A Supplemental Material,[0],[0]
PBSMT I think for sure that there was not enough evidence to justify a death sentence .,A Supplemental Material,[0],[0]
NMT I believe it ’s not sure there ’s enough evidence to justify a executions .,A Supplemental Material,[0],[0]
PBSMT+NMT,A Supplemental Material,[0],[0]
I sure believe there is not enough evidence to justify a death sentence .,A Supplemental Material,[0],[0]
"Reference I certainly believe there was not enough evidence to justify a death sentence .
",A Supplemental Material,[0],[0]
"Source Auch wenn der Laden gut besucht ist , ist es nicht schwer , einen Parkplatz zu finden .",A Supplemental Material,[0],[0]
"PBSMT Even if the store visited it is , it is not hard to find a parking lot .",A Supplemental Material,[0],[0]
"NMT To be sure , the shop is well visited , but it ’s not a troubled driveway .",A Supplemental Material,[0],[0]
PBSMT+NMT,A Supplemental Material,[0],[0]
"Even if the shop is well visited , it is not hard to find a parking lot .",A Supplemental Material,[0],[0]
"Reference While the store can get busy , parking is usually not hard to find .
",A Supplemental Material,[0],[0]
"Source Die Suite , in dem der kanadische Sänger wohnt , kostet am Tag genau so viel , wie ihre Mama Ewa",A Supplemental Material,[0],[0]
im halben Jahr verdient .,A Supplemental Material,[0],[0]
"PBSMT The suite in which the Canadian singer grew up , costs on the day so much as their mum Vera in half year earned .",A Supplemental Material,[0],[0]
"NMT The Suite , in which the Canadian singer lived , costs day care exactly as much as her mom Ewa earned during the decade .",A Supplemental Material,[0],[0]
PBSMT+NMT,A Supplemental Material,[0],[0]
"The suite , in which the Canadian singer lived , costs a day precisely as much as her mom Ewa earned in half .",A Supplemental Material,[0],[0]
"Reference The suite where the Canadian singer is staying costs as much for one night as her mother Ewa earns in six months .
",A Supplemental Material,[0],[0]
"Source Der durchschnittliche BMI unter denen , die sich dieser Operation unterzogen , sank von 31 auf",A Supplemental Material,[0],[0]
"24,5 bis",A Supplemental Material,[0],[0]
Ende des fünften Jahres in dieser Studie .,A Supplemental Material,[0],[0]
PBSMT The average BMI among those who undergoing this operation decreased by 22 to 325 by the end of fifth year in this study .,A Supplemental Material,[0],[0]
NMT,A Supplemental Material,[0],[0]
"The average BMI among those who undergo surgery , sank from 31 to 24,500 by the end of the fifth year in that study .",A Supplemental Material,[0],[0]
PBSMT+NMT,A Supplemental Material,[0],[0]
The average BMI among those who undergo this surgery fell from 31 to 24.5 by the end of the fifth year in this study .,A Supplemental Material,[0],[0]
Reference,A Supplemental Material,[0],[0]
"The average BMI among those who had surgery fell from 31 to 24.5 by the end of their fifth year in the study .
",A Supplemental Material,[0],[0]
"Source Die 300 Plakate sind von Künstlern , die ihre Arbeit im Museum für grausame Designs in Banksys Dismaland ausgestellt haben .",A Supplemental Material,[0],[0]
PBSMT The 250 posters are by artists and their work in the museum for gruesome designs in Dismaland Banksys have displayed .,A Supplemental Material,[0],[0]
NMT,A Supplemental Material,[0],[0]
The 300 posters are posters of artists who have their work Museum for real-life Designs in Banksys and Dismalausgestellt .,A Supplemental Material,[0],[0]
PBSMT+NMT,A Supplemental Material,[0],[0]
The 300 posters are from artists who have displayed their work at the Museum of cruel design in Banksys Dismaland .,A Supplemental Material,[0],[0]
Reference,A Supplemental Material,[0],[0]
"The 300 posters are by artists who exhibited work at the Museum of Cruel Designs in Banksy ’s Dismaland .
",A Supplemental Material,[0],[0]
Source Bis zum Ende des Tages gab es einen weiteren Tod :,A Supplemental Material,[0],[0]
"Lamm nahm sich das Leben , als die Polizei ihn einkesselte .",A Supplemental Material,[0],[0]
PBSMT At the end of the day it ’s a further death :,A Supplemental Material,[0],[0]
Lamb took out life as police him einkesselte .,A Supplemental Material,[0],[0]
NMT,A Supplemental Material,[0],[0]
By the end of the day there was another death : Lamm emerged this year as police were trying to looseless him .,A Supplemental Material,[0],[0]
PBSMT+NMT,A Supplemental Material,[0],[0]
"By the end of the day , there ’s another death : Lamm took out life as police arrived to him .",A Supplemental Material,[0],[0]
"Reference By the end of the day , there would be one more death : Lamb took his own life as police closed in on him .
",A Supplemental Material,[0],[0]
"Source Chaos folgte an der Grenze , als Hunderte von Migranten sich in einem Niemandsland ansammelten und serbische Beamte mit Empörung reagierten .",A Supplemental Material,[0],[0]
"PBSMT Chaos followed at the border , as hundreds of migrants in a frontier ansammelten and Serb officers reacted with outrage .",A Supplemental Material,[0],[0]
NMT Chaos followed an avalanche as hundreds of thousands of immigrants fled to a mandsdom in answerage and Serbian officers responded with outrage . PBSMT+NMT,A Supplemental Material,[0],[0]
Chaos followed at the border as hundreds of immigrants gathered in a bush town and Serb officers reacted with outrage .,A Supplemental Material,[0],[0]
"Reference Chaos ensued at the border , as hundreds of migrants piled up in a no man ’s land , and Serbian officials reacted with outrage .
",A Supplemental Material,[0],[0]
"Source ” Zu unserer Reise gehörten viele dunkle Bahn- und Busfahrten , ebenso Hunger , Durst , Kälte und Angst ” , schrieb sie .",A Supplemental Material,[0],[0]
"PBSMT ” To our trip included many of the dark rail and bus rides , such as hunger , thirst , cold and fear , ” she wrote .",A Supplemental Material,[0],[0]
NMT ”,A Supplemental Material,[0],[0]
"During our trip , many included dark bus and bus trips , especially hunger , Durst , and cold fear , ” she wrote . PBSMT+NMT ”",A Supplemental Material,[0],[0]
"Our trip included many dark rail and bus journeys , as well as hunger , thirst , cold and fear , ” she wrote .",A Supplemental Material,[0],[0]
"Reference ” Our journey involved many dark train and bus rides , as well as hunger , thirst , cold and fear , ” she wrote .
",A Supplemental Material,[0],[0]
Table 7: Unsupervised translations: German-English.,A Supplemental Material,[0],[0]
"Examples of translations on the German-English pair of newstest 2016 using the PBSMT, NMT, and PBSMT+NMT.",A Supplemental Material,[0],[0]
"Machine translation systems achieve near human-level performance on some languages, yet their effectiveness strongly relies on the availability of large amounts of parallel sentences, which hinders their applicability to the majority of language pairs.",abstractText,[0],[0]
This work investigates how to learn to translate when having access to only large monolingual corpora in each language.,abstractText,[0],[0]
"We propose two model variants, a neural and a phrase-based model.",abstractText,[0],[0]
"Both versions leverage a careful initialization of the parameters, the denoising effect of language models and automatic generation of parallel data by iterative back-translation.",abstractText,[0],[0]
"These models are significantly better than methods from the literature, while being simpler and having fewer hyper-parameters.",abstractText,[0],[0]
"On the widely used WMT’14 English-French and WMT’16 German-English benchmarks, our models respectively obtain 28.1 and 25.2 BLEU points without using a single parallel sentence, outperforming the state of the art by more than 11 BLEU points.",abstractText,[0],[0]
"On low-resource languages like English-Urdu and English-Romanian, our methods achieve even better results than semisupervised and supervised approaches leveraging the paucity of available bitexts.",abstractText,[0],[0]
Our code for NMT and PBSMT is publicly available.1,abstractText,[0],[0]
Phrase-Based & Neural Unsupervised Machine Translation,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 559–564 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
559",text,[0],[0]
Extractive question answering (QA) is the task of selecting an answer phrase (span) to a question given an evidence document.,1 Introduction,[0],[0]
"Due to the easiness of evaluation (compared to generative QA) and the fine-grainess of the answer (compared to sentence-level QA), it has become one of the most popular QA tasks, driven by massive new datasets such as SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017).",1 Introduction,[0],[0]
"Current QA models heavily rely on explicitly learning the interaction between the evidence document and the question using neural attention mechanisms (Wang and Jiang, 2017; Xiong et al., 2017; Seo et al., 2017; Lee et al., 2016, inter alia), in which the model is fully aware of the question before or as it reads the document.",1 Introduction,[0],[0]
"As a result, despite significant advances, they have not led to the standalone representation of document discourse which is never-
∗Most work done during internship with Google AI.
",1 Introduction,[0],[0]
theless a key goal of research in reading comprehension.,1 Introduction,[0],[0]
"Furthermore, QA models that condition the document representation on a question have the practical scalability downside that the entire model should be re-applied on the same document for every question.
",1 Introduction,[0],[0]
"In this paper, we formalize a modular variant of the QA task, Phrase Indexed Question Answering (PIQA), that enforces complete independence between document encoder and question encoder (Figure 1).",1 Introduction,[0],[0]
"In PIQA, all documents are processed independently of any question to generate phrase index vectors (blue nodes in the figure) for each answer candidate (left boxes in the figure).",1 Introduction,[0],[0]
"Similarly, the questions are independently mapped to query vectors (red nodes in figure).",1 Introduction,[0],[0]
"Then, at inference time, the answer is obtained by retrieving the nearest indexed phrase vector to the query vector.",1 Introduction,[0],[0]
"Hence the algorithms aimed at tackling PIQA have the inherent benefit of modularity and scalability compared to current QA systems.
",1 Introduction,[0],[0]
"The task setup is analogous to how documents or sentences are retrieved in modern search engines via similarity search algorithms (Shrivastava and Li, 2015).",1 Introduction,[0],[0]
"Nevertheless, there is a key distinction that search engines index each document by its content, while PIQA requires one to index each phrase in documents by its context.
",1 Introduction,[0],[0]
We formally define the PIQA problem and provide baseline models for the new task.,1 Introduction,[0],[0]
"Our experiments show that the constraint introduced
by PIQA leads to meaningful standalone document representations and practical scalability advantage, demonstrating the significance of the new task.",1 Introduction,[0],[0]
"Moreover, there is still a large gap between the baselines and the unconstrained state of the art, showing that the task is yet far from being solved.",1 Introduction,[0],[0]
We have set up a leaderboard1for PIQA challenge and invite the research community to participate.,1 Introduction,[0],[0]
We currently support SQuAD and plan to expand to other datasets as well.,1 Introduction,[0],[0]
Reading comprehension.,2 Related Work,[0],[0]
"Massive reading comprehension question answering datasets (Hermann et al., 2015; Hill et al., 2016; Dhingra et al., 2017; Dunn et al., 2017) have driven a large number of successful neural approaches (Kadlec et al., 2016; Hu et al., 2017, inter alia).",2 Related Work,[0],[0]
"Choi et al. (2017); Chen et al. (2017); Clark and Gardner (2017); Min et al. (2018) tackled large-scale QA by using a fast, coarse model (e.g. TF-IDF) to retrieve few documents or sentences and then using a slower, accurate model to obtain the answer.",2 Related Work,[0],[0]
Salant and Berant (2018) proposed to minimize (but not prohibit) the influence of question when modeling the document.,2 Related Work,[0],[0]
"Similarly to ours, Lee et al. (2016) proposed to explicitly learn the representation for each answer candidate (phrase) in the document, but it was conditioned (dependent) on the question.",2 Related Work,[0],[0]
Sentence retrieval.,2 Related Work,[0],[0]
"A closely related task to ours is that of retrieving a sentence/paragraph in a corpus that answers the question (Tay et al., 2017).",2 Related Work,[0],[0]
A comprehensive survey for neural approaches in information retrieval literature is discussed in Mitra and Craswell (2017).,2 Related Work,[0],[0]
"We note that our problem is focused on phrasal answer extraction, which presents a unique challenge over sentence retrieval—the need for context-based representation as opposed to the content-based representation in the sentence-retrieval literature.",2 Related Work,[0],[0]
Language representation.,2 Related Work,[0],[0]
"Recently there has been a growing interest in developing natural language representations that can be transferred across tasks (Vendrov et al., 2016; Wieting et al., 2016; Conneau et al., 2017, inter alia).",2 Related Work,[0],[0]
"In particular, SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2017) encourage architectures that first encode the hypothesis and the premise independently before a comparator neu-
1nlp.cs.washington.edu/piqa
ral network is applied.",2 Related Work,[0],[0]
Our proposed problem shares similar traits but has a stronger constraint that only inner product comparison is allowed and one needs to model phrases instead of complete sentences.,2 Related Work,[0],[0]
Extractive question answering is the task of obtaining the answer â to a questionQ = {q1 . . .,3 Phrase-Indexed Question Answering,[0],[0]
qn} given an evidence document D = {d1 . . .,3 Phrase-Indexed Question Answering,[0],[0]
"dm}, where the answer â = (s, e) indicates the start and end of a span in the document.",3 Phrase-Indexed Question Answering,[0],[0]
The task is often formulated as learning the probabilistic distribution of the answer given the question and the document.,3 Phrase-Indexed Question Answering,[0],[0]
"In existing literature (Section 2), the distribution is mainly featurized by Pr(a|Q,D) ∝ exp(Fθ(Q,D, a)) where Fθ could be any realvalued scoring function parameterized by θ.",3 Phrase-Indexed Question Answering,[0],[0]
"Once θ is learned, the prediction â is obtained by
â = argmax a Fθ(Q,D, a).",3 Phrase-Indexed Question Answering,[0],[0]
"(1)
So far, most competitive designs of Fθ(Q,D, a) make use of attention connections between the words in Q and D. As a result, these models cannot yield a query independent representation of the document D. It is subsequently not possible to independently assess the document understanding capability of the model.",3 Phrase-Indexed Question Answering,[0],[0]
"Furthermore, Fθ(Q,D, a) needs to be re-computed for the entire document for every new question.",3 Phrase-Indexed Question Answering,[0],[0]
"We believe that this inefficiency precludes all current models as the candidates for end-to-end QA systems.
",3 Phrase-Indexed Question Answering,[0],[0]
We propose a new task—,3 Phrase-Indexed Question Answering,[0],[0]
Phrase-Indexed Question Answering (PIQA)—that addresses these issues.,3 Phrase-Indexed Question Answering,[0],[0]
"We enforce the decomposability of Fθ into two exclusive functions Gθ(Q), Hθ(D, a) ∈ Rk.",3 Phrase-Indexed Question Answering,[0],[0]
"The answer distribution is then modeled by Pr(a|Q,D) ∝ exp(Gθ(Q) •",3 Phrase-Indexed Question Answering,[0],[0]
"Hθ(D, a)), where • is the inner product.",3 Phrase-Indexed Question Answering,[0],[0]
"The prediction is obtained by
â = argmax a
Gθ(Q) •Hθ(D, a).",3 Phrase-Indexed Question Answering,[0],[0]
"(2)
",3 Phrase-Indexed Question Answering,[0],[0]
"In this setting, the document encoder Hθ learns models the document independently of the question.",3 Phrase-Indexed Question Answering,[0],[0]
"Successful question answering models that follow the structure of PIQA will have two important advantages over current QA models: full document comprehension and scalablity.
",3 Phrase-Indexed Question Answering,[0],[0]
Full document comprehension.,3 Phrase-Indexed Question Answering,[0],[0]
"Language understanding ability is widely associated with learning a good standalone representation of text (or its
components such as phrases) independent of the end task (Bowman et al., 2015).",3 Phrase-Indexed Question Answering,[0],[0]
"Under PIQA constraints, the document encoder Hθ learns the representation of the answer candidate phrases a in the document D independent of the question.",3 Phrase-Indexed Question Answering,[0],[0]
"In order to correctly answer questions, these phrase representations (index vectors) need to correctly encode their meaning with respect to their context.",3 Phrase-Indexed Question Answering,[0],[0]
"Therefore, PIQA constraint enforces evaluating research in document comprehension and phrase representation learning.
",3 Phrase-Indexed Question Answering,[0],[0]
Scalability.,3 Phrase-Indexed Question Answering,[0],[0]
"Models that adhere to the PIQA constraint only need to be run once for each document, regardless of the number of questions asked.",3 Phrase-Indexed Question Answering,[0],[0]
"To answer a question, the model then just needs to encode the question and compare it to each of the answer candidates via the inner product in Equation 2.",3 Phrase-Indexed Question Answering,[0],[0]
"Implemented naively, computing a single inner product for each answer candidate is more efficient than building a new document encoding; after the documents are pre-encoded, Equation 2 is O(k) time per word where k is the vector size (most neural models require O(k2) per word for matrix multiplications).
",3 Phrase-Indexed Question Answering,[0],[0]
"More importantly, PIQA also permits an approximate solution in sublinear time using asymmetric locality-sensitive hashing (aLSH) (Shrivastava and Li, 2014, 2015), through which Equation 2 can be approximated for N answer candidates with O(kNρ logN) time, where ρ < 1 is a function of the approximation factor and the properties of the hash functions.",3 Phrase-Indexed Question Answering,[0],[0]
"We argue that this type of approach will be essential for the development of real world QA systems, where the number of potential answers N is extremely large.",3 Phrase-Indexed Question Answering,[0],[0]
"We introduce several baselines for PIQA that are motivated by related literature.
",4 Baseline Models,[0],[0]
"For all (neural) baselines, we represent the words in D and Q with one of three embedding mechanisms: CharCNN (Kim, 2014) +",4 Baseline Models,[0],[0]
"GloVe (Pennington et al., 2014), and ELMo (Peters et al., 2018).",4 Baseline Models,[0],[0]
"We follow the majority of the related literature and apply bidirectional LSTMs (Hochreiter and Schmidhuber, 1997) to these embeddings to build the context-aware representations of the document D = {d1 . .",4 Baseline Models,[0],[0]
.dm},4 Baseline Models,[0],[0]
and question Q = {q1 . .,4 Baseline Models,[0],[0]
".qn}, where the forward & backward LSTM outputs are concatenated to get a single word representation, i.e. di,qi ∈ R2k
where k is the hidden state size of LSTMs.",4 Baseline Models,[0],[0]
PIQA disallows cross-attention between document and question.,4 Baseline Models,[0],[0]
"However, we can still benefit from self-attention, which has become crucial for machine translation (Vaswani et al., 2017) and QA (Huang et al., 2018; Yu et al., 2018).",4 Baseline Models,[0],[0]
"In all of our baselines, each variable-length question is collapsed into a fixed length vector via the sum qSA",4 Baseline Models,[0],[0]
= ∑ i uiqi where u = {u1 . . .,4 Baseline Models,[0],[0]
un} is a vector containing a single weight for each word in the question.,4 Baseline Models,[0],[0]
"Similarly, we experiment with document side self attention to represent each document word dj as a weighted sum of itself and all neighboring words dSAj = ∑ i h j idj .",4 Baseline Models,[0],[0]
"The weight vectors u and hj are calculated as
u = softmaxi(w >qi)
hj = softmaxi(Rθ(D, j)",4 Baseline Models,[0],[0]
>,4 Baseline Models,[0],[0]
"Kθ(D, i))
where Rθ, and Kθ are trainable neural networks with the same ouptut size, and w ∈",4 Baseline Models,[0],[0]
R2k is a trainable weight vector.,4 Baseline Models,[0],[0]
We use independent BiLSTMs with hidden state size k (i.e. the output size is 2k) to model both Rθ and Kθ.,4 Baseline Models,[0],[0]
"That is, Rθ(D, j) is the j-th output of BiLSTM on top of D, and we similarly define Kθ with unshared parameters.
",4 Baseline Models,[0],[0]
"For all (neural) baselines, the question is represented using the concatenation of two copies of qSA, one that should have high inner product with the vector for the answer’s start span and another that should have high inner product with the vector for the answer’s end.",4 Baseline Models,[0],[0]
"Thus, Equation 2’s Gθ(Q) =",4 Baseline Models,[0],[0]
"[q SA s ,q SA e ] where the subscripts s (start) and e (end) imply that different sets of parameters were used.",4 Baseline Models,[0],[0]
"Now we define several baselines.
",4 Baseline Models,[0],[0]
LSTM baseline.,4 Baseline Models,[0],[0]
"An answer candidate a = (s, e) is represented using the LSTM outputs at its endpoints: from Equation 2, Hθ(D, (s, e))",4 Baseline Models,[0],[0]
=,4 Baseline Models,[0],[0]
"[ds,de] ∈ R4k and Gθ(Q) =",4 Baseline Models,[0],[0]
"[qSAs ,qSAe ] ∈ R4k.
LSTM+SA baseline.",4 Baseline Models,[0],[0]
"The LSTM outputs are augmented with the endpoint representations that come out of the document’s self-attention (SA): Hθ(D, (s, e))",4 Baseline Models,[0],[0]
=,4 Baseline Models,[0],[0]
"[ds,d SA s ,de,d SA e ] ∈ R8k and Gθ(Q) =",4 Baseline Models,[0],[0]
"[q SA s1 ,q SA s2 ,q SA e1 ,q SA e2 ] ∈ R8k.
TF-IDF.",4 Baseline Models,[0],[0]
"We lastly include a purely TF-IDFbased model, where each answer candidate phrase is associated with a bag of neighbor words within a distance of 7.",4 Baseline Models,[0],[0]
Then the BOW vector is normalized via TF-IDF and indexed.,4 Baseline Models,[0],[0]
"When the query comes in, its TF-IDF vector is queried on the indexed phrases to yield the answer.
",4 Baseline Models,[0],[0]
"For training the (neural) models, we minimize the negative log probability of getting the correct answer: the loss function for each example (D,Q, a∗) is L(θ) =",4 Baseline Models,[0],[0]
"− log Pr(a∗|D,Q) where a∗ is the correct answer.",4 Baseline Models,[0],[0]
We impose the independence restrictions from PIQA on the Stanford Question Answering Dataset2.,5 Experiments,[0],[0]
We only consider answer spans with length ≤ 7.,5 Experiments,[0],[0]
"We use the hidden state size (k) of 128, which results in a 512D (4k) and 1024D (8k) vector for each phrase in LSTM and LSTM+SA, respectively.",5 Experiments,[0],[0]
"The default embedding model is CharCNN concatenated with 200D GloVe, with an option to append ELMo vectors following the same setup for SQuAD experiments discussed in Peters et al. (2018).",5 Experiments,[0],[0]
"We use a batch size of 64 and train for 20 epochs with the default Adam optimizer (Kingma and Ba, 2015), and take the best model on the validation set during training.
Results.",5 Experiments,[0],[0]
Table 1 shows the results for the PIQA baselines (top) and the unconstrained state of the art (bottom).,5 Experiments,[0],[0]
"First, the TF-IDF model performs poorly, which signifies the limitations of traditional document retrieval models for the task.",5 Experiments,[0],[0]
"Second, we note that the addition of self-attention makes a significant impact on results, improving F1 by 2.6%.",5 Experiments,[0],[0]
"Next, we see that adding ELMo gives 3.7% and 2.9% improvement on F1 for LSTM and LSTM+SA models, respectively.",5 Experiments,[0],[0]
"Lastly, the best PIQA baseline model is 11.7% higher than the first (unconstrained) baseline model (Rajpurkar et al., 2016) and 26.6% lower than the state of the art (Yu et al., 2018).",5 Experiments,[0],[0]
"This gives us a reasonable starting point of the new task and a significant gap
2PIQA paradigm can be also extended to other extractive QA datasets.
to close for future work.
",5 Experiments,[0],[0]
Phrase representations.,5 Experiments,[0],[0]
"Since PIQA models encode all answer candidates into the same space, we expect similar answer candidates to have high inner products with one another.",5 Experiments,[0],[0]
"Table 2 shows pairs of answer candidates that come from different documents in SQuAD, but that have similar encodings (high inner product).",5 Experiments,[0],[0]
We observe that phrase representations learned through the PIQA task capture different interesting characteristics of the phrases.,5 Experiments,[0],[0]
"In all three rows, we can see that the phrase pairs seem to fit into natural categories: national, or multi-national organizational constructs; mechanical engines; and mechanical properties, respectively.",5 Experiments,[0],[0]
This suggests that the model has learned interesting typing information above the word level.,5 Experiments,[0],[0]
The second and third rows also indicate that the model has learned a rich representation of context.,5 Experiments,[0],[0]
"This is particularly obvious in the third row where the two phrases are lexically dissimilar, but preceded by the similar contexts ‘primarily accomplished through’ and ‘directly derived from’.",5 Experiments,[0],[0]
"We believe that this analysis, while not complete, points toward exciting future lines of work in learning highly contextualized phrase representations through question answering.
Scalability.",5 Experiments,[0],[0]
"PIQA can also gain massive execution time speedups once the documents are preencoded: in our simple benchmark on a consumergrade CPU and NumPy (for LSTM+SA model, 1024D vectors), one can easily perform exact search over 1 million document words per second.",5 Experiments,[0],[0]
"BiDAF (Seo et al., 2017), an open-sourced and relatively light QA model reaching 77.5% F1 (66.5% EM), can process less than 1k document words per second with an equivalent computing power (after pre-encoding the document as much as possible), which is more than 1,000x slower.3
3The difference will be even higher with a dedicated similarity search package such as Faiss (Johnson et al., 2017) or approximate search (Section 3).
",5 Experiments,[0],[0]
It is also important to consider the memory cost for storing a vector representation of each of the answer candidates.,5 Experiments,[0],[0]
We train an independent single-layer perceptron classifier that predicts whether the phrase encoding is likely to be a good one.,5 Experiments,[0],[0]
"By varying a threshold on the score assigned by this classifier, we can filter answer candidates prior to storage.",5 Experiments,[0],[0]
Figure 2 illustrates the trade-off between accuracy and memory (measured in mean number of vectors per document word) resulting from this filtering procedure for the LSTM+SA model.,5 Experiments,[0],[0]
We observe that 1.3 vectors (candidates) per word on average reaches > 98% of the model’s F1 accuracy.,5 Experiments,[0],[0]
"This is equivalent to 5.2 KB per word with 1024D (4 KB) float vectors, or around 15 TB for the entire English Wikipedia (3 billion words).",5 Experiments,[0],[0]
Future work will also involve creating a better classifier (i.e. improving the trade-off curve in Figure 2) for determining which phrase vectors to store.,5 Experiments,[0],[0]
"We introduced Phrase-Indexed Question Answering (PIQA), a new variant of the extractive question answering task that requires documents and question encoded completely independently and that they only interact each other via inner product.",6 Conclusion and Future Work,[0],[0]
We argued that building a question-agnostic document encoder for question answering should be an important consideration for those in the QA community with the research goal of learning a model that reads and comprehends documents.,6 Conclusion and Future Work,[0],[0]
"Furthermore, the imposed constraint of the task implies a sublinear scalability benefit.",6 Conclusion and Future Work,[0],[0]
"Given that SQuAD models have recently outperformed hu-
mans, PIQA formulation motivates a new challenge for which we hope that the community’s effort gradually closes the gap between our constrained baselines and the unconstrained models.",6 Conclusion and Future Work,[0],[0]
"This research was supported by ONR (N0001418-1-2826), NSF (IIS 1616112), Allen Distinguished Investigator Award, and gifts from Google, Allen Institute for AI, Amazon, and Bloomberg.",Acknowledgments,[0],[0]
We thank the anonymous reviewers for their helpful comments.,Acknowledgments,[0],[0]
We formalize a new modular variant of current question answering tasks by enforcing complete independence of the document encoder from the question encoder.,abstractText,[0],[0]
This formulation addresses a key challenge in machine comprehension by requiring a standalone representation of the document discourse.,abstractText,[0],[0]
It additionally leads to a significant scalability advantage since the encoding of the answer candidate phrases in the document can be pre-computed and indexed offline for efficient retrieval.,abstractText,[0],[0]
"We experiment with baseline models for the new task, which achieve a reasonable accuracy but significantly underperform unconstrained QA models.",abstractText,[0],[0]
"We invite the QA research community to engage in Phrase-Indexed Question Answering (PIQA, pika) for closing the gap.",abstractText,[0],[0]
The leaderboard is at: nlp.cs.washington.,abstractText,[0],[0]
Phrase-Indexed Question Answering: A New Challenge for Scalable Document Comprehension,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3729–3738 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3729",text,[0],[0]
"Following the success of word embeddings (Bengio et al., 2003; Mikolov et al., 2013), one of NLP’s next challenges has become the hunt for universal sentence encoders.",1 Introduction,[0],[0]
"The goal is to learn a general-purpose sentence encoding model on a large corpus, which can be readily transferred to other tasks.",1 Introduction,[0],[0]
"The learned sentence representations are able to generalize to unseen combination of words, which makes them highly desirable for
downstream NLP tasks, especially for those with relatively small datasets.
",1 Introduction,[0],[0]
"Previous models for sentence encoding typically rely on Recurrent Neural Networks (RNNs) (Hochreiter and Schmidhuber, 1997; Chung et al., 2014) or Convolutional Neural Networks (CNNs) (Kalchbrenner et al., 2014; dos Santos and Gatti, 2014; Kim, 2014; Mou et al., 2016) to produce context-aware representation.",1 Introduction,[0],[0]
"RNNs encode a sentence by reading words in sequential order, they are capable of learning long-term dependencies but are hard to parallelize and not time-efficient.",1 Introduction,[0],[0]
"CNNs focus on local or positioninvariant dependencies but do not perform well on many tasks (Shen et al., 2017).
",1 Introduction,[0],[0]
"Fully attention-based neural networks have attracted wide interest recently, because they can model both dependencies while being more parallelizable and requiring significantly less time to train.",1 Introduction,[0],[0]
"Vaswani et al. (2017) proposed the multihead attention to project a sentence to multiple semantic subspaces, then apply self-attention in each subspace and concatenate the attention results.",1 Introduction,[0],[0]
"Shen et al. (2017) proposed the directional self-attention, they apply forward and backward masks to the alignment score matrix to encode temporal order information, and computed attention at feature level to select the features that can best describe the word’s meaning in given context.",1 Introduction,[0],[0]
"Effective as their models are, the memory required to store the alignment scores of all the token pairs grows quadratically with the sentence length.",1 Introduction,[0],[0]
"Furthermore, the syntactic property that is intrinsic to natural language is not considered at all.
",1 Introduction,[0],[0]
"Language is inherently tree structured, and the meaning of a sentence comes largely from composing the meanings of subtrees (Chomsky, 1957).",1 Introduction,[0],[0]
"Previous syntactic tree-based sentence encoders (Socher et al., 2013; Tai et al., 2015) mainly rely on recursive networks.",1 Introduction,[0],[0]
"Although the composition-
ality can be explicitly modeled, their models need expensive recursion computation and are hard to be trained by batched gradient descent methods.
",1 Introduction,[0],[0]
"In this paper, we propose the Phrase-level SelfAttention Networks (PSAN), for RNN/CNN-free sentence encoding, it inherits all the advantages of fully attention-based models while requires much less memory consumption.",1 Introduction,[0],[0]
"In addition, syntactic information can be incorporated into the model more easily.",1 Introduction,[0],[0]
"In our model, every sentence is split into multiple phrases based on parse tree, selfattention is performed at the phrase level instead of the sentence level, thus the memory consumption reduces rapidly as the number of phrases increases.",1 Introduction,[0],[0]
"Furthermore, a gated memory component is employed to refine word representations hierarchically by incorporating longer-term context dependencies.",1 Introduction,[0],[0]
"As a result, syntactic information can be integrated into the model without expensive recursion computation.",1 Introduction,[0],[0]
"At last, multi-dimensional attention is applied on the refined word representations to obtain the final sentence representation.
",1 Introduction,[0],[0]
"Following Conneau et al. (2017), we trained our sentence encoder on the SNLI (Bowman et al., 2015) dataset, and evaluate the quality of the obtained universal sentence representations on a wide range of transfer tasks.",1 Introduction,[0],[0]
"The SNLI dataset is extremely suitable for training sentence encoders because it is the largest high-quality humanannotated dataset that involves reasoning about the semantic relationships within sentences.
",1 Introduction,[0],[0]
"The main contributions of our work can be summarized as follows:
• We propose the Phrase-level Self-Attention mechanism (PSA) for contextualization.",1 Introduction,[0],[0]
"The memory consumption can be reduced because self-attention is performed at the phrase level instead of the sentence level.
",1 Introduction,[0],[0]
"• A gated memory updating mechanism is proposed to refine each word representation hierarchically by incorporating different levels of contextual information along the parse tree.
",1 Introduction,[0],[0]
• Our proposed PSAN model outperforms the state-of-the-art supervised sentence encoders on a wide range of transfer tasks with significantly less memory consumption.,1 Introduction,[0],[0]
"In this section, we introduce the Phrase-level SelfAttention Networks (PSAN) for sentence encod-
ing.",2 Proposed Model,[0],[0]
A phrase is a group of words that carry a specific idiomatic meaning and function as a constituent in the syntax of a sentence.,2 Proposed Model,[0],[0]
Words in a phrase are syntactically and semantically related to each other.,2 Proposed Model,[0],[0]
"Therefore, it can be advantageous to learn a context-aware representation inside a phrase while filtering out information from outside the phrase using self-attention mechanism.",2 Proposed Model,[0],[0]
"In an attempt to better utilize the tree structure which is intrinsic to language, we propose the gated memory updating mechanism to combine different levels of context information.",2 Proposed Model,[0],[0]
"At last, an attention mechanism is utilized to summarize all the token representations into a fixed-length sentence vector.",2 Proposed Model,[0],[0]
The phrase structure organizes words into nested constituents which can be successively divided into their parts as we move down the constituencybased parse trees.,2.1 Phrase Division,[0],[0]
One phrase division shows only one aspect of context dependency.,2.1 Phrase Division,[0],[0]
"In order to capture different levels of context dependencies, we can split a sentence at different granularities.",2.1 Phrase Division,[0],[0]
"The number of levels T is a hyper-parameter to be tuned.
",2.1 Phrase Division,[0],[0]
"We can break down the nodes at T different layers in the parse tree to capture T levels of context dependencies1, as illustrated in Figure 1.",2.1 Phrase Division,[0],[0]
This is the core component of our model.,2.2 Phrase-level Self-Attention,[0],[0]
It aims to learn a context-aware representation for each token inside a phrase.,2.2 Phrase-level Self-Attention,[0],[0]
"In order to filter out information that is semantically or syntactically distant, self-attention is performed at the phrase level instead of the sentence level.
",2.2 Phrase-level Self-Attention,[0],[0]
"Similar to directional self-attention network (DiSAN) (Shen et al., 2017), Phrase-level SelfAttention uses multi-dimensional attention to compute the alignment score for each dimension of token embedding.",2.2 Phrase-level Self-Attention,[0],[0]
"Therefore, it can select the features that can best describe a word’s specific meaning in any given context.
",2.2 Phrase-level Self-Attention,[0],[0]
"Given a phrase P ∈ Rl×d represented as a sequence of word embeddings [p1, . . .",2.2 Phrase-level Self-Attention,[0],[0]
",pl], where l is the length of the phrase and d is the dimension of word embedding representation, we first compute the alignment score for each token pair in the
1To avoid the situation that the produced phrases are too small, a phrase will not be further divided if its length is smaller than 4.
phrase: aij = σ",2.2 Phrase-level Self-Attention,[0],[0]
"( W a1pi +W a2pj + b a ) +Mij
Mij = { 0, i 6= j −∞, i = j
(1)
where σ (·) is an activation function, W a1,W a2 ∈ Rd×d and ba ∈ Rd are parameters to be learned, and M is a diagonal-diabled mask (Hu et al., 2017) that aims to prevent a word from being aligned with itself.
",2.2 Phrase-level Self-Attention,[0],[0]
"The output of the attention mechanism is a weighted sum of embeddings from all tokens for each token in the phrase:
p̃i = l∑ j=1",2.2 Phrase-level Self-Attention,[0],[0]
"[ exp (aij)∑l k=1 exp (aik) pj ] (2)
where means point-wise product.",2.2 Phrase-level Self-Attention,[0],[0]
"Note that the alignment score for each token pair is a vector rather than a scalar in the multi-dimensional attention.
",2.2 Phrase-level Self-Attention,[0],[0]
The final output of Phrase-level Self-Attention is obtained by comparing each input representation with its attention-weighted counterpart.,2.2 Phrase-level Self-Attention,[0],[0]
We use a comparison function based on absolute difference and element-wise multiplication which was similar to Wang and Jiang (2016).,2.2 Phrase-level Self-Attention,[0],[0]
"This comparison function has the advantage of measuring the semantic similarity or relatedness of two sequences.
",2.2 Phrase-level Self-Attention,[0],[0]
ci = σ,2.2 Phrase-level Self-Attention,[0],[0]
"(W c [|pi − p̃i| ;pi p̃i] + bc) (3)
where",2.2 Phrase-level Self-Attention,[0],[0]
W c ∈ Rd×2d,2.2 Phrase-level Self-Attention,[0],[0]
and ba ∈ Rd are parameters to be learned.,2.2 Phrase-level Self-Attention,[0],[0]
"ci is the representation for the i-th word in the phrase that captures local dependencies within the phrase.
",2.2 Phrase-level Self-Attention,[0],[0]
"At last, we put together the Phrase-level SelfAttention results for non-overlapping phrases from the same phrase division of a sentence.",2.2 Phrase-level Self-Attention,[0],[0]
For the t-th phrase division we can get C(t) =,2.2 Phrase-level Self-Attention,[0],[0]
"[c1, . .",2.2 Phrase-level Self-Attention,[0],[0]
.,2.2 Phrase-level Self-Attention,[0],[0]
", cls ], the phrase-level self-attention results for the sentence from the t-th layer split, where ls is the sentence length.",2.2 Phrase-level Self-Attention,[0],[0]
Above describes the Phrase-level Self-Attention (PSA) for one split of the parse tree.,2.3 Gated Memory Updating,[0],[0]
The parse tree can be split at different granularities.,2.3 Gated Memory Updating,[0],[0]
We propose a novel gated memory updating mechanism to refine each word representation hierarchically with longer-term dependencies captured in a larger granularity.,2.3 Gated Memory Updating,[0],[0]
"Inspired by the idea of adaptive gate in highway networks (Srivastava et al., 2015), our memory mechanism add a gate to original memory networks (Weston et al., 2014; Sukhbaatar et al., 2015).",2.3 Gated Memory Updating,[0],[0]
"This gate has the ability to determine the importance of the new input and the original memory in the memory updating.
C(t) = PSA ( M (t−1) ) G(t) = sigmoid ( W g",2.3 Gated Memory Updating,[0],[0]
[ M (t−1);C(t) ],2.3 Gated Memory Updating,[0],[0]
"+ bg
) M (t) = G(t) σ",2.3 Gated Memory Updating,[0],[0]
( Wm [ M (t−1);C(t) ],2.3 Gated Memory Updating,[0],[0]
"+ bm
) (4)
whereW g,Wm ∈ Rd×2d and bg,",2.3 Gated Memory Updating,[0],[0]
bm ∈ Rd are parameters to be learned.,2.3 Gated Memory Updating,[0],[0]
"Note that in order to share representation power and to reduce the number of parameters, the parameters of gated memory updating are shared among different layers.",2.3 Gated Memory Updating,[0],[0]
"In this layer, self-attention mechanism is employed to summarize the refined representation of a sentence into a fixed-length vector.",2.4 Sentence Summarization,[0],[0]
The selfattention mechanism can explore the dependencies among tokens within the whole sentence.,2.4 Sentence Summarization,[0],[0]
"As a result, global dependencies can also be incorporated in the model.
",2.4 Sentence Summarization,[0],[0]
ei =W e2σ ( W e1m (T ),2.4 Sentence Summarization,[0],[0]
i + b e1 ) +,2.4 Sentence Summarization,[0],[0]
"be2
v = l∑ i=1",2.4 Sentence Summarization,[0],[0]
[ exp (ei)∑l j=1 exp (ej) m(T ),2.4 Sentence Summarization,[0],[0]
"i ] (5) where W g,Wm ∈ Rd×d and bg, bm ∈ Rd are parameters to be learned.",2.4 Sentence Summarization,[0],[0]
"After this step, the refined context-aware sentence representation is compressed into a fixed-length vector.",2.4 Sentence Summarization,[0],[0]
"In this section, we conduct a plethora of experiments to study the effectiveness of the PSAN model.",3 Experiments,[0],[0]
"Following Conneau et al. (2017), we train our sentence encoder using the SNLI dataset, and evaluate it across a variety of NLP tasks including sentence classification, natural language inference and sentence textual similarity.",3 Experiments,[0],[0]
"300-dimensional GloVe (Pennington et al., 2014) word embeddings (Common Crawl, uncased) are used to represent words.",3.1 Model Configuration,[0],[0]
"Following Parikh et al. (2016), out-of-vocabulary words are hashed to one of 128 random embeddings initialized by uniform distribution between (-0.05, 0.05).",3.1 Model Configuration,[0],[0]
All the word embeddings remain fixed during training.,3.1 Model Configuration,[0],[0]
Hidden dimension d is set to 300.,3.1 Model Configuration,[0],[0]
"All other parameters are initialized with Glorot normal initialization (Glorot and Bengio, 2010).",3.1 Model Configuration,[0],[0]
"Activation function σ (·) is ELU (Clevert et al., 2015) if not specified.",3.1 Model Configuration,[0],[0]
Minibatch size is set to 16.,3.1 Model Configuration,[0],[0]
The number of levels T is fixed to 3 in all of our experiments.,3.1 Model Configuration,[0],[0]
"The syntactic parse trees of SNLI are provided within the corpus. parse trees for all test corpus are produced by the Stanford PCFG Parser 3.5.2 (Klein and Manning, 2003), the same parser that produced parse trees for the SNLI dataset.
",3.1 Model Configuration,[0],[0]
"To train the model, Adadelta optimizer (Zeiler, 2012) with a learning rate of 0.75 is used on the SNLI dataset.",3.1 Model Configuration,[0],[0]
"The dropout (Srivastava et al., 2014) rate and L2 regularization weight decay factor γ are set to 0.5 and 5e-5.",3.1 Model Configuration,[0],[0]
"To test the model, the SentEval toolkit (Conneau and Kiela, 2018) is used as the evaluation pipeline for fairer comparison.",3.1 Model Configuration,[0],[0]
"Natural language inference (NLI) is a fundamental task in the field of natural language processing that involves reasoning about the semantic relationship between two sentences, which makes it a suitable task to train sentence encoding models.
",3.2 Training Setting,[0],[0]
"We conduct experiments on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015).",3.2 Training Setting,[0],[0]
"The dataset has 570k human-annotated sentence pairs, each labeled with one of the following pre-defined relationships: Entailment (the premise entails the hypothesis), Contradiction (they contradict each other) and Neutral (they are irrelevant).",3.2 Training Setting,[0],[0]
"Following previous work (Bowman et al., 2015; Mou et al., 2016), we remove the instances which annotators can not reach consensus on.",3.2 Training Setting,[0],[0]
"In this way we get 549367/9842/9824 sentence pairs for train/validation/test set.
",3.2 Training Setting,[0],[0]
"Following the siamese architecture (Bromley et al., 1993), we apply PSAN to both the premise and the hypothesis with their parameters tied.",3.2 Training Setting,[0],[0]
vp and vh are fixed-length vector representations for the premise and the hypothesis respectively.,3.2 Training Setting,[0],[0]
"The final sentence-pair representation is formed by concatenating the original vectors with the absolute difference and element-wise multiplication between them:
vinp =",3.2 Training Setting,[0],[0]
[ vp;vh; ∣∣∣vp − vh∣∣∣ ;vp vh] (6),3.2 Training Setting,[0],[0]
"At last, we feed the sentence-pair representation vinp into a two layer feed-forward network and use a softmax layer to make the classification.",3.2 Training Setting,[0],[0]
This is the de facto scheme for sentence encoders trained on SNLI.,3.2 Training Setting,[0],[0]
"(Mou et al., 2016; Liu et al., 2016; Shen et al., 2017)",3.2 Training Setting,[0],[0]
"To show the modeling capacity and robustness of our proposed model, we evaluate our model across a wide range of tasks that can be solved purely based on the encoded semantics.",3.3 Evaluation Setting,[0],[0]
"The set of tasks
was selected based on what appears to be the community consensus regarding the appropriate evaluations for universal sentence representations.",3.3 Evaluation Setting,[0],[0]
"To facilitate comparison, we use the same sentence evaluation tool as Conneau et al. (2017) to automate evaluation on all the tasks mentioned in this paper.
",3.3 Evaluation Setting,[0],[0]
"The transfer tasks used in evaluation can be concluded in the following classes: sentence classification (MR, CR, MPQA, SUBJ, SST2, SST5, TREC), natural language inference (SICKE, SICK-R), semantic relatedness (STS14) and paraphrase detection (MRPC).",3.3 Evaluation Setting,[0],[0]
Table 1 presents some statistics about the datasets 2.,3.3 Evaluation Setting,[0],[0]
"We compare our model with the following supervised sentence encoders:
• BiLSTM-Max (Conneau et al., 2017) is a simple but effective baseline that performs max-pooling over a bi-directional LSTM.
",3.4 Baselines,[0],[0]
"• AdaSent (Zhao et al., 2015) forms a hierarchy of representations from words to phrases and then to sentences through recursive gated local composition of adjacent segments.
",3.4 Baselines,[0],[0]
"• TBCNN (Mou et al., 2015) is a tree-based CNN model where convolution is applied over the parse tree.
",3.4 Baselines,[0],[0]
"2For further information on the datasets, please refer to Conneau et al. (2017).
",3.4 Baselines,[0],[0]
"• DiSAN (Shen et al., 2017) is composed of a directional self-attention block with temporal order encoded, and a multi-dimensional attention that compresses the sequence into a vector representation.",3.4 Baselines,[0],[0]
Experiment results of our model and four baselines are shown in Table 2.,4.1 Overall Performance,[0],[0]
Micro and macro accuracies are two composite indicators for evaluating transfer performance of tasks whose metric is classification accuracy.,4.1 Overall Performance,[0],[0]
Macro accuracy is the proportion of true results in the population of instances from all tasks.,4.1 Overall Performance,[0],[0]
"Micro accuracy is the arithmetic mean of dev accuracies for each task.
",4.1 Overall Performance,[0],[0]
"PSAN achieves the state-of-the-art performance
with considerably fewer parameters, outperforming a RNN-based model, a CNN-based model, a fully attention-based model and a model that utilize syntactic information.",4.1 Overall Performance,[0],[0]
"Especially when compared with previous best model BiLSTM-Max, PSAN can outperform their model with only 5% of their parameter numbers, demonstrating the effectiveness of our model at extracting semantically important information from a sentence.
",4.1 Overall Performance,[0],[0]
"In Table 3, we compare our model with baseline sentence encoders in each transfer task.",4.1 Overall Performance,[0],[0]
PSAN can consistently outperform the baselines in almost every task considered.,4.1 Overall Performance,[0],[0]
"On the SICK dataset, which can be seen as an out-domain version of SNLI, our model can outperform the baselines by a large margin, demonstrating the semantic relationship learned on the SNLI can be well transfered to other domains.",4.1 Overall Performance,[0],[0]
"On the STS14 dataset, where sentence vectors can be more directly measured by the cosine distance, our model can also achieve the stateof-the-art performance, indicating that our learned sentence representations are of high quality.",4.1 Overall Performance,[0],[0]
"For thorough comparison, we implement seven extra baselines to analyze the improvements con-
tributed by each part of our PSAN model:
• PSA on the first/second/third layer only only uses the Phrase-level Self-Attention at the first/second/third layer of phrase division.
",4.2 Ablation Study,[0],[0]
"• w/o PSA applies self-attention at the sentence level and uses the gated memory updating mechanism to refine each token representation hierarchically.
",4.2 Ablation Study,[0],[0]
"• w/o syntactic division divides each sentence equally into small blocks, and applies PSA within each block.",4.2 Ablation Study,[0],[0]
"The number of blocks equals the number of phrases in that layer.
",4.2 Ablation Study,[0],[0]
"• w/o gated memory updating concatenates the outputs of Phrase-level Self-Attention from three layers of phrase division and feeds the result to a feed-forward layer.
",4.2 Ablation Study,[0],[0]
"• w/o both applies self-attention at the sentence level, and uses sentence summarization to summarize the attention results into a fixed length vector.
",4.2 Ablation Study,[0],[0]
The results are listed in Table 4.,4.2 Ablation Study,[0],[0]
"We can see that (2) performs best among (1), (2) and (3), demonstrating that the second layer split is more expressive, because the number of words per phrase in the second layer is the most suitable.",4.2 Ablation Study,[0],[0]
"It is neither too small to capture context dependencies, nor too large to filter out irrelevant noise.",4.2 Ablation Study,[0],[0]
"(8) outperforms (1), (2) and (3), showing that combining phraselevel information from different granularities can further improve performance.
",4.2 Ablation Study,[0],[0]
We also experiment on models where the alignment matrix is calculated at the sentence level or at the syntactic-irrelevant block level.,4.2 Ablation Study,[0],[0]
"(5) performs quite well, showing that hierarchical refinement on smaller units can bring about reasonable
performance gain.",4.2 Ablation Study,[0],[0]
"(8) outperforms (4) and (5), demonstrating syntactic information helps in sentence representation.
",4.2 Ablation Study,[0],[0]
"When comparing (6) with (8), we can tell that gated memory updating is a better method when used to refine token representation along the parse tree.",4.2 Ablation Study,[0],[0]
"We assume that memory updating resembles the tree structure of language in that larger phrase is composed in the knowledge of how smaller phrases are composed inside it.
",4.2 Ablation Study,[0],[0]
"Comparing (7) with (1), (2) and (3), we can find that performing self-attention at the phrase level is generally better than at the sentence level, indicating that reducing attention context into phrase level can effectively filter out words that are syntactically and semantically distant, thus focusing on the interaction with important words.",4.2 Ablation Study,[0],[0]
"Comparing (7) with (4), we can draw the conclusion that memory updating is effective even when the inputs to each layer are the same.",4.2 Ablation Study,[0],[0]
"Long-term dependencies are typically hard to capture for sequential models like RNNs (Bengio et al., 1994; Hochreiter and Schmidhuber, 1997).",4.3 Analysis of Sentence Length,[0],[0]
We conduct experiments to see how performance changes as the sentence length increases.,4.3 Analysis of Sentence Length,[0],[0]
"In Figure 2, we show the relationship between classification accuracy and the average length of sentence pair on the SNLI dataset.",4.3 Analysis of Sentence Length,[0],[0]
Sentence-level SelfAttention (w/o PSA model described in subsection 4.2) is used as a baseline for our model.,4.3 Analysis of Sentence Length,[0],[0]
"PSAN
outperforms Sentence-level Self-Attention model consistently for longer sentences of length 14 to 20.",4.3 Analysis of Sentence Length,[0],[0]
This demonstrates that incorporating syntactic information by performing self-attention at the phrase level and refining each word’s representation hierarchically can help to capture long-term dependencies across words in a sentence.,4.3 Analysis of Sentence Length,[0],[0]
We conduct experiments to analyze the memory consumption reduction resulted from Phrase-level Self-Attention.,4.4 Analysis of Memory Consumption,[0],[0]
"To this end, we re-implement two fully attention-based models (Vaswani et al., 2017; Shen et al., 2017) on the TREC dataset.",4.4 Analysis of Memory Consumption,[0],[0]
"To make fair comparison, the dimensions of sentence vectors are set to 300, the same number as our model.",4.4 Analysis of Memory Consumption,[0],[0]
Table 5 lists the results.,4.4 Analysis of Memory Consumption,[0],[0]
"Our PSAN model can outperform the other two fully attention-based models, while being more memory efficient.",4.4 Analysis of Memory Consumption,[0],[0]
reducing more than 20% of memory consumption.,4.4 Analysis of Memory Consumption,[0],[0]
"In order to analyze the attention changing process and the importance of each word in the sentence vector, we visualize the attention scores in the alignment matrix of each layer in Phraselevel Self-Attention and sentence summarization layer.",4.5 Visualization and Case Study,[0],[0]
"To facilitate the visualization of the multidimension attention vector, we use the l2 norm of the attention vector for representation.
",4.5 Visualization and Case Study,[0],[0]
"In Figure 3, we can see that, the difference in attention weights between semantically important and unimportant words gets larger as the context becomes larger.",4.5 Visualization and Case Study,[0],[0]
This implies that token representation can be gradually refined by the gated memory updating mechanism.,4.5 Visualization and Case Study,[0],[0]
"Furthermore, the alignment matrix of a phrase can be refined even if the phrase division does not change between layers.",4.5 Visualization and Case Study,[0],[0]
"For instance, the word “girl” gets larger attention weight in the second layer division than in the first layer.",4.5 Visualization and Case Study,[0],[0]
"This demonstrates that the memory
updating mechanism can gradually pick out important words for sentence representation.",4.5 Visualization and Case Study,[0],[0]
"Finally, nouns and verbs dominate the attention weights, while stop words like “a” and “its”, contribute little to the final sentence representation, this indicates that PSAN can effectively pick out semantically important words that are most representative for the meaning of the whole sentence.",4.5 Visualization and Case Study,[0],[0]
"Recently, self-attention mechanism has been successfully applied to the field of sentence encoding, it utilizes the attention mechanism to relate elements at different positions from a single sentence.",5 Related Work,[0],[0]
"Due to its direct access to each token representation, both long-term and local dependencies can be modeled flexibly.",5 Related Work,[0],[0]
Liu et al. (2016) leveraged the average-pooled word representation to attend words appear in the sentence itself.,5 Related Work,[0],[0]
"Cheng et al. (2016) proposed the LSTMN model for machine reading, an attention vector is produced for each of its hidden states during the recurrent iteration, thus empowering the recurrent network with stronger memorization capability and the ability to discover relations among tokens.",5 Related Work,[0],[0]
Lin et al. (2017) obtained a fixed-size sentence embedding matrix by introducing self-attention.,5 Related Work,[0],[0]
"Different from the feature-level attention used in our model, their attention mechanism extracted different aspects of the sentence into multiple vector representations, and utilized a penalization term to encourage the diversity of different attention results.
",5 Related Work,[0],[0]
Syntactic information can be useful for understanding a natural language sentence.,5 Related Work,[0],[0]
"Many previous researches utilized syntactic information to build sentence encoder from composing the mean-
ings of subtrees.",5 Related Work,[0],[0]
"Tree-LSTM (Tai et al., 2015; Zhu et al., 2015) composed its hidden state from an input vector and the hidden states of arbitrarily many child units.",5 Related Work,[0],[0]
"In Tree-based CNN (Mou et al., 2015, 2016), a set of subtree feature detectors slide over the parse tree of a sentence, and a max-pooling layer is utilized to aggregate information along different parts of the tree.
",5 Related Work,[0],[0]
"Apart from the models that use parse information, there have been several researches that aimed to learn the hierarchical latent structure of text by recursively composing words into sentence representation.",5 Related Work,[0],[0]
"Among them, neural tree indexer (Munkhdalai and Yu, 2017b) utilized LSTM or attentive node composition function to construct full n-ary tree for input text.",5 Related Work,[0],[0]
"Gumbel TreeLSTM (Choi et al., 2018) used Straight-Through Gumbel-Softmax estimator to decide the parent node among candidates dynamically.",5 Related Work,[0],[0]
A major drawback of these models is that the recursion computation can be expensive and hard to be processed in batches.,5 Related Work,[0],[0]
"We propose the Phrase-level Self-Attention Networks (PSAN), a fully attention-based model that can utilize syntactic information for universal sentence encoding.",6 Conclusion,[0],[0]
"By applying self-attention at the phrase level, we can filter out distant and unrelated words and focus on modeling interaction between semantically and syntactically important words, a gated memory updating mechanism is utilized to incorporate different levels of contextual information along the parse tree.",6 Conclusion,[0],[0]
Empirical results on a wide range of transfer tasks demonstrate the effectiveness of our model.,6 Conclusion,[0],[0]
Our work is supported by National Natural Science Foundation of China under Grant No.61433015 and the National Key Research and Development Program of China under Grant No.2017YFB1002101.,Acknowledgments,[0],[0]
The corresponding authors of this paper are Houfeng Wang.,Acknowledgments,[0],[0]
Universal sentence encoding is a hot topic in recent NLP research.,abstractText,[0],[0]
"Attention mechanism has been an integral part in many sentence encoding models, allowing the models to capture context dependencies regardless of the distance between elements in the sequence.",abstractText,[0],[0]
Fully attention-based models have recently attracted enormous interest due to their highly parallelizable computation and significantly less training time.,abstractText,[0],[0]
"However, the memory consumption of their models grows quadratically with sentence length, and the syntactic information is neglected.",abstractText,[0],[0]
"To this end, we propose Phrase-level Self-Attention Networks (PSAN) that perform self-attention across words inside a phrase to capture context dependencies at the phrase level, and use the gated memory updating mechanism to refine each word’s representation hierarchically with longer-term context dependencies captured in a larger phrase.",abstractText,[0],[0]
"As a result, the memory consumption can be reduced because the self-attention is performed at the phrase level instead of the sentence level.",abstractText,[0],[0]
"At the same time, syntactic information can be easily integrated in the model.",abstractText,[0],[0]
"Experiment results show that PSAN can achieve the state-ofthe-art transfer performance across a plethora of NLP tasks including sentence classification, natural language inference and sentence textual similarity.",abstractText,[0],[0]
Phrase-level Self-Attention Networks for Universal Sentence Encoding,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 114–120 New Orleans, Louisiana, June 1 - 6, 2018. c©2017 Association for Computational Linguistics",text,[0],[0]
"Neural machine translation (NMT) (Bahdanau et al., 2014; Sutskever et al., 2014) has recently achieved remarkable performance improving fluency and adequacy over phrase-based machine translation and is being deployed in commercial settings (Koehn and Knowles, 2017).",1 Introduction,[0],[0]
"However, this comes at a cost of slow decoding speeds compared to phrase-based and syntax-based SMT (see section 3).
",1 Introduction,[0],[0]
NMT models are generally trained using 32-bit floating point values.,1 Introduction,[0],[0]
"At training time, multiple sentences can be processed in parallel leveraging graphical processing units (GPUs) to good advantage since the data is processed in batches.",1 Introduction,[0],[0]
"This is also true for decoding for non-interactive applications such as bulk document translation.
",1 Introduction,[0],[0]
Why is fast execution on CPUs important?,1 Introduction,[0],[0]
"First, CPUs are cheaper than GPUs.",1 Introduction,[0],[0]
Fast CPU computation will reduce commercial deployment costs.,1 Introduction,[0],[0]
"Second, for low-latency applications such as speech-to-speech translation (Neubig et al.,
∗A piece of eight was a Spanish dollar that was divided into 8 reales, also known as Real de a Ocho.
2017a), it is important to translate individual sentences quickly enough so that users can have an application experience that responds seamlessly.",1 Introduction,[0],[0]
"Translating individual sentences with NMT requires many memory bandwidth intensive matrixvector or matrix-narrow matrix multiplications (Abdelfattah et al., 2016).",1 Introduction,[0],[0]
"In addition, the batch size is 1 and GPUs do not have a speed advantage over CPUs due to the lack of adequate parallel work (as evidenced by increasingly difficult batching scenarios in dynamic frameworks (Neubig et al., 2017b)).
",1 Introduction,[0],[0]
Others have successfully used low precision approximations to neural net models.,1 Introduction,[0],[0]
Vanhoucke et al. (2011) explored 8-bit quantization for feedforward neural nets for speech recognition.,1 Introduction,[0],[0]
Devlin (2017) explored 16-bit quantization for machine translation.,1 Introduction,[0],[0]
In this paper we show the effectiveness of 8-bit decoding for models that have been trained using 32-bit floating point values.,1 Introduction,[0],[0]
"Results show that 8-bit decoding does not hurt the fluency or adequacy of the output, while producing results up to 4-6x times faster.",1 Introduction,[0],[0]
"In addition, implementation is straightforward and we can use the models as is without altering training.
",1 Introduction,[0],[0]
"The paper is organized as follows: Section 2 reviews the attentional model of translation to be sped up, Section 3 presents our 8-bit quantization in our implementation, Section 4 presents automatic measurements of speed and translation quality plus human evaluations, Section 5 discusses the results and some illustrative examples, Section 6 describes prior work, and Section 7 concludes the paper.",1 Introduction,[0],[0]
"Our translation system implements the attentional model of translation (Bahdanau et al., 2014) consisting of an encoder-decoder network with an at-
114
tention mechanism.",2 The Attentional Model of Translation,[0],[0]
"The encoder uses a bidirectional GRU recurrent neural network (Cho et al., 2014) to encode a source sentence x = (x1, ..., xl), where xi is the embedding vector for the ith word and l is the sentence length.",2 The Attentional Model of Translation,[0],[0]
"The encoded form is a sequence of hidden states h = (h1, ..., hl) where each hi is computed as follows
hi =",2 The Attentional Model of Translation,[0],[0]
[←− hi−→ hi ] =,2 The Attentional Model of Translation,[0],[0]
"[←− f (xi, ←− h i+1)−→ f (xi, −→ h i−1) ] , (1)
where −→ h0 = ←− h0 = 0.",2 The Attentional Model of Translation,[0],[0]
Here ←− f and −→ f are GRU cells.,2 The Attentional Model of Translation,[0],[0]
"Given h, the decoder predicts the target translation y by computing the output token sequence (y1, ...ym), where m is the length of the sequence.",2 The Attentional Model of Translation,[0],[0]
"At each time t, the probability of each token yt from a target vocabulary is
p(yt|h, yt−1..y1)",2 The Attentional Model of Translation,[0],[0]
"= g(st, yt−1, Ht), (2)
where g is a two layer feed-forward network over the embedding of the previous target word (yt−1), the decoder hidden state (st), and the weighted sum of encoder states h",2 The Attentional Model of Translation,[0],[0]
"(Ht), followed by a softmax to predict the probability distribution over the output vocabulary.
",2 The Attentional Model of Translation,[0],[0]
"We compute st with a two layer GRU as
s′t = r(st−1, y ∗ t−1) (3)
and st = q(s ′ t, Ht), (4) where s′t is an intermediate state and s0 = ←− h0.",2 The Attentional Model of Translation,[0],[0]
The two GRU units r and q together with the attention constitute the conditional GRU layer of Sennrich et al. (2017).,2 The Attentional Model of Translation,[0],[0]
"Ht is computed as
Ht =
[∑l i=1(αt,i · ←− h i)∑l
i=1(αt,i · −→ h i)
] , (5)
",2 The Attentional Model of Translation,[0],[0]
"where αt,i are the elements of αt which is the output vector of the attention model.",2 The Attentional Model of Translation,[0],[0]
"This is computed with a two layer feed-forward network
α′t = v(tanh(w(hi) + u(s ′ t−1))), (6)
where w and u are weight matrices, and v is another matrix resulting in one real value per encoder",2 The Attentional Model of Translation,[0],[0]
state hi.,2 The Attentional Model of Translation,[0],[0]
"αt is then the softmax over α′t.
",2 The Attentional Model of Translation,[0],[0]
"We train our model using a program written using the Theano framework (Bastien et al.,
2012).",2 The Attentional Model of Translation,[0],[0]
"Generally models are trained with batch sizes ranging from 64 to 128 and unbiased Adam stochastic optimizer (Kingma and Ba, 2014).",2 The Attentional Model of Translation,[0],[0]
We use an embedding size of 620 and hidden layer sizes of 1000.,2 The Attentional Model of Translation,[0],[0]
We select model parameters according to the best BLEU score on a held-out development set over 10 epochs.,2 The Attentional Model of Translation,[0],[0]
Our translation engine is a C++ implementation.,3 8-bit Translation,[0],[0]
"The engine is implemented using the Eigen matrix library, which provides efficient matrix operations.",3 8-bit Translation,[0],[0]
Each CPU core translates a single sentence at a time.,3 8-bit Translation,[0],[0]
"The same engine supports both batch and interactive applications, the latter making single-sentence translation latency important.",3 8-bit Translation,[0],[0]
"We report speed numbers as both words per second (WPS) and words per core second (WPCS), which is WPS divided by the number of cores running.",3 8-bit Translation,[0],[0]
"This gives us a measure of overall scaling across many cores and memory buses as well as the single-sentence speed.
",3 8-bit Translation,[0],[0]
"Phrase-based SMT systems, such as (Tillmann, 2006), for English-German run at 170 words per core second (3400 words per second) on a 20 core Xeon 2690v2 system.",3 8-bit Translation,[0],[0]
"Similarly, syntax-based SMT systems, such as (Zhao and Al-onaizan, 2008), for the same language pair run at 21.5 words per core second (430 words per second).
",3 8-bit Translation,[0],[0]
"In contrast, our NMT system (described in Section 2) with 32-bit decoding runs at 6.5 words per core second (131 words per second).",3 8-bit Translation,[0],[0]
"Our goal is to increase decoding speed for the NMT system to what can be achieved with phrase-based systems while maintaining the levels of fluency and adequacy that NMT offers.
",3 8-bit Translation,[0],[0]
Benchmarks of our NMT decoder unsurprisingly show matrix multiplication as the number one source of compute cycles.,3 8-bit Translation,[0],[0]
In Table 1 we see that more than 85% of computation is spent in Eigen’s matrix and vector multiply routines (Eigen matrix vector product and Eigen matrix multiply).,3 8-bit Translation,[0],[0]
"It dwarfs the costs of the transcendental function computations as well as the bias additions.
",3 8-bit Translation,[0],[0]
"Given this distribution of computing time, it makes sense to try to accelerate the matrix operations as much as possible.",3 8-bit Translation,[0],[0]
One approach to increasing speed is to quantize matrix operations.,3 8-bit Translation,[0],[0]
"Replacing 32-bit floating point math operations with 8-bit integer approximations in neural nets has been shown to give speedups and similar ac-
curacy (Vanhoucke et al., 2011).",3 8-bit Translation,[0],[0]
"We chose to apply similar optimization to our translation system, both to reduce memory traffic as well as increase parallelism in the CPU.
",3 8-bit Translation,[0],[0]
Our 8-bit matrix multiply routine uses a naive implementation with no blocking or copy.,3 8-bit Translation,[0],[0]
"The code is implemented using Intel SSE4 vector instructions and computes 4 rows at a time, similar to (Devlin, 2017).",3 8-bit Translation,[0],[0]
Simplicity led to implementing 8-bit matrix multiplication with the results being placed into a 32-bit floating point result.,3 8-bit Translation,[0],[0]
This has the advantage of not needing to know the scale of the result.,3 8-bit Translation,[0],[0]
"In addition, the output is a vector or narrow matrix, so little extra memory bandwidth is consumed.
",3 8-bit Translation,[0],[0]
"Multilayer matrix multiply algorithms result in significantly faster performance than naive algorithms (Goto and Geijn, 2008).",3 8-bit Translation,[0],[0]
"This is due to the fact that there are O(N3) math operations on O(N2) elements when multiplying NxN matrices, therefore it is worth significant effort to minimize memory operations while maximizing math operations.",3 8-bit Translation,[0],[0]
"However, when multiplying an NxN matrix by an NxP matrix where P is very small (<10), memory operations dominate and performance does not benefit from the complex algorithm.",3 8-bit Translation,[0],[0]
"When decoding single sentences, we typically set our beam size to a value less than 8 following standard practice in this kind of systems (Koehn and Knowles, 2017).",3 8-bit Translation,[0],[0]
"We actually find that at such small values of P, the naive algorithm is a bit faster.
",3 8-bit Translation,[0],[0]
Table 2 shows the profile after converting the matrix routines to 8-bit integer computation.,3 8-bit Translation,[0],[0]
There is only one entry for matrix-matrix and matrix-vector multiplies since they are handled by the same routine.,3 8-bit Translation,[0],[0]
"After conversion, tanh and sigmoid still consume less than 7% of CPU time.",3 8-bit Translation,[0],[0]
"We decided not to convert these operations to integer in light of that fact.
",3 8-bit Translation,[0],[0]
"It is possible to replace all the operations with 8-bit approximations (Wu et al., 2016), but this makes implementation more complex, as the scale of the result of a matrix multiplication must be known to correctly output 8-bit numbers without dangerous loss of precision.
",3 8-bit Translation,[0],[0]
"Assuming we have 2 matrices of size 1000x1000 with a range of values [−10, 10], the individual dot products in the result could be as large as 108.",3 8-bit Translation,[0],[0]
"In practice with neural nets, the scale of the result is similar to that of the input matrices.",3 8-bit Translation,[0],[0]
"So if we scale the result to [−127, 127] assuming the worst case, the loss of precision will give us a matrix full of zeros.",3 8-bit Translation,[0],[0]
"The choices are to either scale the result of the matrix multiplication with a reasonable value, or to store the result as floating point.",3 8-bit Translation,[0],[0]
"We opted for the latter.
8-bit computation achieves 32.3 words per core second (646 words per second), compared to the 6.5 words per core second (131 words per second) of the 32-bit system (both systems load parameters from the same model).",3 8-bit Translation,[0],[0]
This is even faster than the syntax-based system that runs at 21.5 words per core second (430 words per second).,3 8-bit Translation,[0],[0]
"Table 3 summarizes running speeds for the phrase-based SMT system, syntax-based system and NMT with 32-bit decoding and 8-bit decoding.",3 8-bit Translation,[0],[0]
"To demonstrate the effectiveness of approximating the floating point math with 8-bit integer computation, we show automatic evaluation results
on several models, as well as independent human evaluations.",4 Measurements,[0],[0]
"We report results on Dutch-English, English-Dutch, Russian-English, German-English and English-German models.",4 Measurements,[0],[0]
Table 4 shows training data sizes and vocabulary sizes.,4 Measurements,[0],[0]
All models have 620 dimension embeddings and 1000 dimension hidden states.,4 Measurements,[0],[0]
Here we report automatic results comparing decoding results on 32-bit and 8-bit implementations.,4.1 Automatic results,[0],[0]
"As others have found (Wu et al., 2016), 8-bit implementations impact quality very little.
",4.1 Automatic results,[0],[0]
"In Table 6, we compared automatic scores and speeds for Dutch-English, English-Dutch, Russian-English, German-English and EnglishGerman models on news data.",4.1 Automatic results,[0],[0]
"The EnglishGerman model was run with both a single model (1x) and an ensemble of two models (2x) (Freitag et al., 2017).",4.1 Automatic results,[0],[0]
"Table 5 gives the number of sentences and average sentence length for the test sets used.
",4.1 Automatic results,[0],[0]
Speed is reported in words per core second (WPCS).,4.1 Automatic results,[0],[0]
This gives us a better sense of the speed of individual engines when deployed on multicore systems with all cores performing translations.,4.1 Automatic results,[0],[0]
Total throughput is simply the product of WPCS and the number of cores in the machine.,4.1 Automatic results,[0],[0]
The reported speed is the median of 9 runs to ensure consistent numbers.,4.1 Automatic results,[0],[0]
"The results show that we see a 4-6x speedup over 32-bit floating point de-
coding.",4.1 Automatic results,[0],[0]
German-English shows the largest deficit for the 8-bit mode versus the 32-bit mode.,4.1 Automatic results,[0],[0]
The German-English test set only includes 168 sentences so this may be a spurious difference.,4.1 Automatic results,[0],[0]
These automatic results suggest that 8-bit quantization can be done without perceptible degradation.,4.2 Human evaluation,[0],[0]
"To confirm this, we carried out a human evaluation experiment.
",4.2 Human evaluation,[0],[0]
"In Table 7, we show the results of performing human evaluations on some of the same language pairs in the previous section.",4.2 Human evaluation,[0],[0]
An independent native speaker of the language being translated to/from different than English (who is also proficient in English) scored 100 randomly selected sentences.,4.2 Human evaluation,[0],[0]
The sentences were shuffled during the evaluation to avoid evaluator bias towards different runs.,4.2 Human evaluation,[0],[0]
"We employ a scale from 0 to 5, with 0 being unintelligible and 5 being perfect translation.
",4.2 Human evaluation,[0],[0]
"The Table shows that the automatic scores shown in the previous section are also sustained
by humans.",4.2 Human evaluation,[0],[0]
8-bit decoding is as good as 32-bit decoding according to the human evaluators.,4.2 Human evaluation,[0],[0]
Having a faster NMT engine with no loss of accuracy is commercially useful.,5 Discussion,[0],[0]
"In our deployment scenarios, it is the difference between an interactive user experience that is sluggish and one that is not.",5 Discussion,[0],[0]
"Even in batch mode operation, the same throughput can be delivered with 1/4 the hardware.
",5 Discussion,[0],[0]
"In addition, this speedup makes it practical to deploy small ensembles of models.",5 Discussion,[0],[0]
"As shown above in the En-De model in Table 6, an ensemble can deliver higher accuracy at the cost of a 2x slowdown.",5 Discussion,[0],[0]
"This work makes it possible to translate with higher quality while still being at least twice as fast as the previous baseline.
",5 Discussion,[0],[0]
"As the numbers reported in Section 4 demonstrate, 8-bit and 32-bit decoding have similar average quality.",5 Discussion,[0],[0]
"As expected, the outputs produced by the two decoders are not identical.",5 Discussion,[0],[0]
"In fact, on a run of 166 sentences of De-En translation, only 51 were identical between the two.",5 Discussion,[0],[0]
"In addition, our human evaluation results and the automatic scoring suggest that there is no specific degradation by the 8-bit decoder compared to the 32-bit decoder.",5 Discussion,[0],[0]
"In order to emphasize these claims, Table 8 shows several examples of output from the two systems for a German-English system.",5 Discussion,[0],[0]
"Table 9 shows 2 more examples from a Dutch-English system.
",5 Discussion,[0],[0]
"In general, there are minor differences without any loss in adequacy or fluency due to 8-bit decoding.",5 Discussion,[0],[0]
"Sentence 2 in Table 8 shows a spelling error (“predictated”) in the 32-bit output due to re-
assembly of incorrect subword units.1",5 Discussion,[0],[0]
"Reducing the resources required for decoding neural nets in general and neural machine translation in particular has been the focus of some attention in recent years.
",6 Related Work,[0],[0]
Vanhoucke et al. (2011) explored accelerating convolutional neural nets with 8-bit integer decoding for speech recognition.,6 Related Work,[0],[0]
They demonstrated that low precision computation could be used with no significant loss of accuracy.,6 Related Work,[0],[0]
"Han et al. (2015) investigated highly compressing image classification neural networks using network pruning, quantization, and Huffman coding so as to fit completely into on-chip cache, seeing significant improvements in speed and energy efficiency while keeping accuracy losses small.
",6 Related Work,[0],[0]
"Focusing on machine translation, Devlin (2017) implemented 16-bit fixed-point integer math to speed up matrix multiplication operations, seeing a 2.59x improvement.",6 Related Work,[0],[0]
They show competitive BLEU scores on WMT English-French NewsTest2014 while offering significant speedup.,6 Related Work,[0],[0]
"Similarly, (Wu et al., 2016) applies 8-bit end-toend quantization in translation models.",6 Related Work,[0],[0]
They also show that automatic metrics do not suffer as a result.,6 Related Work,[0],[0]
"In this work, quantization requires modification to model training to limit the size of matrix outputs.",6 Related Work,[0],[0]
"In this paper, we show that 8-bit decoding for neural machine translation runs up to 4-6x times faster than a similar optimized floating point implementation.",7 Conclusions and Future Work,[0],[0]
We show that the quality of this approximation is similar to that of the 32-bit version.,7 Conclusions and Future Work,[0],[0]
"We also show that it is unnecessary to modify the training procedure to produce models compatible with 8- bit decoding.
",7 Conclusions and Future Work,[0],[0]
"To conclude, this paper shows that 8-bit decoding is as good as 32-bit decoding both in automatic measures and from a human perception perspective, while it improves latency substantially.
",7 Conclusions and Future Work,[0],[0]
In the future we plan to implement a multilayered matrix multiplication that falls back to the naive algorithm for matrix-panel multiplications.,7 Conclusions and Future Work,[0],[0]
This will provide speed for batch decoding for applications that can take advantage of it.,7 Conclusions and Future Work,[0],[0]
"We also
1In order to limit the vocabulary, we use BPE subword units (Sennrich et al., 2016) in all models.
plan to explore training with low precision for faster experiment turnaround time.
",7 Conclusions and Future Work,[0],[0]
Our results offer hints of improved accuracy rather than just parity.,7 Conclusions and Future Work,[0],[0]
Other work has used training as part of the compression process.,7 Conclusions and Future Work,[0],[0]
We would like to see if training quantized models changes the results for better or worse.,7 Conclusions and Future Work,[0],[0]
Neural machine translation has achieved levels of fluency and adequacy that would have been surprising a short time ago.,abstractText,[0],[0]
"Output quality is extremely relevant for industry purposes, however it is equally important to produce results in the shortest time possible, mainly for latency-sensitive applications and to control cloud hosting costs.",abstractText,[0],[0]
In this paper we show the effectiveness of translating with 8-bit quantization for models that have been trained using 32-bit floating point values.,abstractText,[0],[0]
Results show that 8-bit translation makes a non-negligible impact in terms of speed with no degradation in accuracy and adequacy.,abstractText,[0],[0]
Pieces of Eight: 8-bit Neural Machine Translation,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3233–3242 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3233",text,[0],[0]
The 20 Question Game (Q20 Game) is a classic game that requires deductive reasoning and creativity.,1 Introduction,[0],[0]
"At the beginning of the game, the answerer thinks of a target object and keeps it concealed.",1 Introduction,[0],[0]
"Then the questioner tries to figure out the target object by asking questions about it, and the answerer answers each question with a simple “Yes”, “No” or “Unknown”, honestly.",1 Introduction,[0],[0]
The questioner wins the game if the target object is found within 20 questions.,1 Introduction,[0],[0]
"In a Q20 game system, the
∗The work was done when the first author was an intern in Microsoft XiaoIce team.
",1 Introduction,[0],[0]
"user is considered as the answerer while the system itself acts as the questioner which requires a good question selection strategy to win the game.
",1 Introduction,[0],[0]
"As a game with the hype read your mind, Q20 has been played since the 19th century, and was brought to screen in the 1950s by the TV show Twenty Questions.",1 Introduction,[0],[0]
"Burgener’s program (Burgener, 2006) further popularized Q20 as an electronic game in 1988, and modern virtual assistants like Microsoft XiaoIce and Amazon Alexa also incorporate this game into their system to demonstrate their intelligence.
",1 Introduction,[0],[0]
"However, it is not easy to design the algorithm to construct a Q20 game system.",1 Introduction,[0],[0]
"Although the decision tree based method seems like a natural fit to the Q20 game, it typically require a well defined Knowledge Base (KB) that contains enough information about each object, which is usually not available in practice.",1 Introduction,[0],[0]
"Burgener (2006) instead uses a object-question relevance table as the pivot for question and object selection, which does not depend on an existing KB.",1 Introduction,[0],[0]
Wu et al. (2018) further improve the relevance table with a lot of engineering tricks.,1 Introduction,[0],[0]
"Since these table-based methods greedily select questions and the model parameters are only updated by rules, their models are very sensitive to noisy answers from users, which is common in the real-world Q20 games.",1 Introduction,[0],[0]
"Zhao and Maxine (2016) utilizes a value-based Reinforcement Learning (RL) model to improve the generalization ability but still relies on the existing KB.
",1 Introduction,[0],[0]
"In this paper, we formulate the process of question selction in the game as a Markov Decision Process (MDP), and further propose a novel policy-based RL framework to learn the optimal policy of question selection in the Q20 game.",1 Introduction,[0],[0]
"Our questioner agent maintains a probability distribution over all objects to model the confidence of the target object, and updates the confidence based on answers from the user.",1 Introduction,[0],[0]
At each time-step.,1 Introduction,[0],[0]
"the agent uses a policy network πθ(a|s) to take in
the confidence vector and output a question distribution for selecting the next question.",1 Introduction,[0],[0]
"To solve the problem that there is no immediate reward for each selected question, we also propose to employ a RewardNet to estimate the appropriate immediate reward at each time-step, which is further used to calculate the long-term return to train our RL model.",1 Introduction,[0],[0]
"Our RL framework makes the agent robust to noisy answers since the model parameters are fully learnable and the question distribution from πθ(a|s) provides us with a principled way to sample questions, which enables the agent to jump out of the local optimum caused by incorrect answers and also introduces more randomness during training to improve the model generalization ability.",1 Introduction,[0],[0]
"Furthermore, the ability to sample questions, compared to greedy selection, also improves the diversity of the questions asked by our agent, which is crucial for user experience.
",1 Introduction,[0],[0]
Our contributions can be summarized as follows: (1) We propose a novel RL framework to learn the optimal policy of question selection in the Q20 game without any dependencies on the existing KBs of target objects.,1 Introduction,[0],[0]
Our trained agent is robust to noisy answers and has a good diversity in its selected questions.,1 Introduction,[0],[0]
"(2) To make the reward more meaningful, we also propose a novel neural network on reward function approximation to deliver the appropriate immediate rewards at each time-step.",1 Introduction,[0],[0]
(3) Extensive experiments show that our RL method clearly outperforms a highly engineered baseline in the real-world Q20 games where noisy answers are common.,1 Introduction,[0],[0]
"Besides, our RL method is also competitive to that baseline on a noise-free simulation environment.",1 Introduction,[0],[0]
"In this section, we first describe our RL framework for playing the Q20 game, which is shown in the
Fig. 1.",2 Method,[0],[0]
The user in our system is the answerer who thinks of a target object otgt in the object set O at the beginning of the game.,2 Method,[0],[0]
Our policy-based agent acts as the questioner that can ask 20 questions to figure out what exactly otgt is.,2 Method,[0],[0]
"Specifically, an internal state vector s is maintained by our agent, which describes the confidence about otgt.",2 Method,[0],[0]
"At each time-step t, the agent picks up the promising action (select a question) according to the policy πθ(a|st), and transits from the state st to the next state st+1 after receiving the answer (“Yes”/“No”/“Unknown”) from the user.",2 Method,[0],[0]
"The historical trajectories 〈st, at, rt+1, st+1〉 are stored in a replay memory which enables the agent to be trained on previously observed data by sampling from it.",2 Method,[0],[0]
"Note that only when a guess is made about otgt at the end of game can the agent receive a reward signal, which makes it unable to distinguish the importance of each selected question.",2 Method,[0],[0]
"Therefore, we design a RewardNet to learn the more informative reward at each time-step and thus lead the agent to achieve the better performance.
",2 Method,[0],[0]
"In the rest of this section, we first describe how to formulate the Q20 game into a RL framework, and then introduce the RewardNet.",2 Method,[0],[0]
"Finally, we will demonstrate our training procedure in detail.",2 Method,[0],[0]
"In the Q20 game, the goal of our agent is to figure out the object otgt that the user thinks of at the beginning of game by asking 20 questions.",2.1 Modeling of the Q20 Game,[0],[0]
We formulate the process of question selection as a finite Markov Decision Process (MDP) which can be solved with RL.,2.1 Modeling of the Q20 Game,[0],[0]
"A tuple 〈S,A, P ,R, γ〉 is defined to represent the MDP, where S is the continuous state space, A = {a1, a2, · · · , am} is the set of all available actions, P(St+1 = s′|St = s,At = a) is the transition probability matrix,R(s, a) is the reward function and γ ∈",2.1 Modeling of the Q20 Game,[0],[0]
"[0, 1] is the discount factor used to calculate the long-time return.",2.1 Modeling of the Q20 Game,[0],[0]
"In the RL framework, at each time-step t, the agent takes an action at under the state st according to the policy πθ(a|st).",2.1 Modeling of the Q20 Game,[0],[0]
"After interacting with the environment, the agent receives a reward scalar rt+1 and transits to the next state st+1, then another time-step begins.",2.1 Modeling of the Q20 Game,[0],[0]
"All these trajectories 〈st, at, rt+1, st+1〉 in a game constitute an episode which is an instance of the finite MDP.",2.1 Modeling of the Q20 Game,[0],[0]
"The long-time return Gt of the time-step t is calculated as follows:
Gt = T∑ k=0 γkrt+k+1 (1)
",2.1 Modeling of the Q20 Game,[0],[0]
"In the following parts, we describe each component of RL corresponding to the Q20 game.
Environment.",2.1 Modeling of the Q20 Game,[0],[0]
The major component of our environment is the user in the Q20 game who decides the target object otgt and answers questions from the agent.,2.1 Modeling of the Q20 Game,[0],[0]
"Besides, the environment also needs to deliver the reward based on the outcome of the game and store historical data into the replay memory (see Fig. 1).
Action.",2.1 Modeling of the Q20 Game,[0],[0]
"Since the agent interacts with the user by asking questions, the action at ∈ A taken by our agent refers to selecting the question qat at timestep t, andA is the set of the indices to all available questions in the Q20 game.
State.",2.1 Modeling of the Q20 Game,[0],[0]
"In our method, we use the state st to keep track of the current confidence of target object otgt.",2.1 Modeling of the Q20 Game,[0],[0]
Specifically st ∈ R|O| and ∑n i=1,2.1 Modeling of the Q20 Game,[0],[0]
"st,i = 1, where O = {o1, o2, · · · , on} represents the set of all the objects that can be chosen by the user.",2.1 Modeling of the Q20 Game,[0],[0]
"Therefore, the state st is a probability distribution over all the objects and st,i is the confidence that the object oi is the target object otgt at time-step t.
The initial state s0 can either be a uniform distribution or initialized by the prior knowledge.",2.1 Modeling of the Q20 Game,[0],[0]
We observe that users typically prefer to choose popular objects which are more concerned by the public.,2.1 Modeling of the Q20 Game,[0],[0]
"For example, the founder of Tesla Inc. and the designer of SpaceX, “Elon Musk”, is more likely to be chosen compared to a CEO of a new startup.",2.1 Modeling of the Q20 Game,[0],[0]
"Motivated by this, we could use the yearly retrieval frequency C(oi) of object oi on a commercial search engine to calculate the initial state s0, where s0,i = C(oi) /",2.1 Modeling of the Q20 Game,[0],[0]
"∑n j=1C(oj).
",2.1 Modeling of the Q20 Game,[0],[0]
Transition Dynamics.,2.1 Modeling of the Q20 Game,[0],[0]
"In our method, the transition dynamics is deterministic.",2.1 Modeling of the Q20 Game,[0],[0]
"Given the object set O and the question set A, we collect the normalized probabilities of the answer over “Yes”, “No” and “Unknown” for each object-question pair.",2.1 Modeling of the Q20 Game,[0],[0]
"And the rule of state transition is define as:
st+1 = st α (2)
where α depends on the answer xt to the question qat which is selected by the agent at the step t:
α =  ",2.1 Modeling of the Q20 Game,[0],[0]
"[R(1, at), . . .",2.1 Modeling of the Q20 Game,[0],[0]
", R(|O|, at)], xt = Y es
[W (1, at), . . .",2.1 Modeling of the Q20 Game,[0],[0]
",W (|O|, at)], xt =",2.1 Modeling of the Q20 Game,[0],[0]
"No [U(1, at), . . .",2.1 Modeling of the Q20 Game,[0],[0]
", U(|O|, at)], xt = Unk
(3) where O is the object set and for each objectquestion pair (oi, qj), R(i, j) and W (i, j) are cal-
culated as follows:
R(i, j) = Cyes(i, j) + δ
Cyes(i, j) +",2.1 Modeling of the Q20 Game,[0],[0]
"Cno(i, j) +",2.1 Modeling of the Q20 Game,[0],[0]
"Cunk(i, j) + λ
W (i, j) = Cno(i, j) + δ
Cyes(i, j) + Cno(i, j) +",2.1 Modeling of the Q20 Game,[0],[0]
"Cunk(i, j) + λ
(4)
R(i, j) and W (i, j) are probabilities of answering “Yes” and “No” to question qj with respect to the object oi respectively.",2.1 Modeling of the Q20 Game,[0],[0]
"Cyes(i, j), Cno(i, j) and Cunk(i, j) are frequencies of answering “Yes”, “No” and “Unknown” to question qj with respect to the object oi.",2.1 Modeling of the Q20 Game,[0],[0]
δ,2.1 Modeling of the Q20 Game,[0],[0]
and λ are smoothing parameters.,2.1 Modeling of the Q20 Game,[0],[0]
"Then the probability of answering “Unknown” to question qj with respect to the object oi is:
U(i, j) = 1−R(i, j)−W (i, j) (5)
",2.1 Modeling of the Q20 Game,[0],[0]
"In this way, the confidence st,i that the object oi is the target object otgt is updated following the user’s answer xt to the selected question qat at the time-step t.
Policy Network.",2.1 Modeling of the Q20 Game,[0],[0]
We directly parameterize the policy πθ(a|st) with a neural network which maps the state st to a probability distribution over all available actions: πθ(a|st) = P[a|st; θ].,2.1 Modeling of the Q20 Game,[0],[0]
The parameters θ are updated to maximize the expected return which is received from the environment.,2.1 Modeling of the Q20 Game,[0],[0]
"Instead of learning a greedy policy in value-based methods like DQN, the policy network is able to learn a stochastic policy which can increase the diversity of questions asked by our agent and potentially make the agent more robust to noisy answers in the real-world Q20 game.",2.1 Modeling of the Q20 Game,[0],[0]
The policy πθ(a|s) is modeled by a Multi-Layer Perceptron (MLP) and the output layer is normalized by using a masked softmax function to avoid selecting the question that has been asked before.,2.1 Modeling of the Q20 Game,[0],[0]
Because asking the same question twice does not provide extra information about otgt in a game.,2.1 Modeling of the Q20 Game,[0],[0]
"For most reinforcement learning applications, it is always a critical part to design reward functions, especially when the agent needs to precisely take actions in a complex task.",2.2 Problem of Direct Reward,[0],[0]
"A good reward function can improve the learning efficiency and help the agent achieve better performances.
",2.2 Problem of Direct Reward,[0],[0]
"In the Q20 game, however, the immediate reward rt of selecting question qat is unknown at the time-step t (t < T ) because each selected question is just answered with a simple “Yes”, “No” or
“Unknown” and there is no extra information provided by user.",2.2 Problem of Direct Reward,[0],[0]
Only when the game ends (t = T ) can the agent receive a reward signal of win or loss.,2.2 Problem of Direct Reward,[0],[0]
So we intuitively consider the direct reward: rT = 30 and −30 for the win and loss respectively while rt = 0 for all t < T .,2.2 Problem of Direct Reward,[0],[0]
"Unfortunately, the direct reward is not discriminative because the agent receives the same immediate reward rt = 0 (t < T ) for selecting both good and bad questions.",2.2 Problem of Direct Reward,[0],[0]
"For example, if the otgt is “Donald Trump”, then selecting question (a) “Is your role the American president?” should receive more immediate reward rt than selecting question (b) “Has your role been married?”.",2.2 Problem of Direct Reward,[0],[0]
"The reason is that as for the otgt, question (a) is more relevant and can narrow down the searching space to a greater extent.
",2.2 Problem of Direct Reward,[0],[0]
"Therefore, it is necessary to design a better reward function to estimate a non-zero immediate reward rt, and make the long-time return Gt =∑T
k=0",2.2 Problem of Direct Reward,[0],[0]
γ krt+k+1 more informative.,2.2 Problem of Direct Reward,[0],[0]
"To solve the problem of the direct reward, we propose a reward function which employs a neural network to estimate a non-zero immediate reward rt at each time-step.",2.3 Reward Function Approximation by Neural Network,[0],[0]
"So that Gt can be more informative, which thus leads to a better trained questioner agent.
",2.3 Reward Function Approximation by Neural Network,[0],[0]
"The reward function takes the state-action pair (st, at) as input and outputs the corresponding immediate reward rt+1.",2.3 Reward Function Approximation by Neural Network,[0],[0]
"In our method, we use a MLP with sigmoid output to learn the appropriate immediate reward during training, and this network is referred as RewardNet.",2.3 Reward Function Approximation by Neural Network,[0],[0]
"In each episode, the long-term return Gt is used as a surrogate indicator of rt+1 to train our RewardNet with the following loss function:
L1(σ) =",2.3 Reward Function Approximation by Neural Network,[0],[0]
"(R(st, at;σ)− sigmoid(Gt))2 (6)
where σ is the network parameters.",2.3 Reward Function Approximation by Neural Network,[0],[0]
Here we apply the sigmoid function on Gt so as to prevent Gt from growing too large.,2.3 Reward Function Approximation by Neural Network,[0],[0]
"Besides, we also use the replay memory to store both old and recent experiences, and then train the network by sampling mini-batches from it.",2.3 Reward Function Approximation by Neural Network,[0],[0]
"The training process based on the experience replay technique can decorrelate the sample data and thus make the training of the RewardNet more efficient.
",2.3 Reward Function Approximation by Neural Network,[0],[0]
"Furthermore, since the target object otgt can be obtained at the end of each episode, we can
use the extra information provided by otgt to estimate a better immediate reward rt.",2.3 Reward Function Approximation by Neural Network,[0],[0]
"To capture the relevance between the selected questions and otgt in an episode, we further propose a objectaware RewardNet which takes the 〈st, at, otgt〉 tuple as input and produces corresponding rt+1 as output.",2.3 Reward Function Approximation by Neural Network,[0],[0]
The detailed training algorithm is shown in Algo.,2.3 Reward Function Approximation by Neural Network,[0],[0]
"1.
",2.3 Reward Function Approximation by Neural Network,[0],[0]
Algorithm 1: Training Object-Aware RewardNet 1 Initialize replay memory D1 to capacity N1 2 Initialize RewardNet with random weights σ 3 for episode i← 1 to Z do 4,2.3 Reward Function Approximation by Neural Network,[0],[0]
"User chooses object oi from O 5 Initialize temporary set S1 and S2 6 Play with policy πθ(at|st), and store (st, at) in S1, where t ∈",2.3 Reward Function Approximation by Neural Network,[0],[0]
"[0, T ] 7 rT ← 30 or −30 for a win or loss 8 for (st, at) in S1 do 9 Get rt+1 from RewardNet
10 Store (st, at, rt+1) tuple in S2 11 for (st, at, rt+1) in S2 do 12 Gt ← ∑T k=0",2.3 Reward Function Approximation by Neural Network,[0],[0]
"γ
krt+k+1 13 r′t+1 ← sigmoid(Gt) 14 Store (st, at, oi, r′t+1) in D1 15 if len(D1) > K1 then 16 Sample mini-batch from D1 17 Update σ with loss L1(σ) in Eq. 6",2.3 Reward Function Approximation by Neural Network,[0],[0]
"We train the policy network using REINFORCE (Williams, 1992) algorithm and the corresponding loss function is defined as follows:
L2(θ) = −Eπθ",2.4 Training the Policy-Based Agent,[0],[0]
[log πθ(at|st)(Gt − bt)],2.4 Training the Policy-Based Agent,[0],[0]
"(7)
where the baseline bt is a estimated value of the expected future reward at the state st, which is produced by a value network Vη(st).",2.4 Training the Policy-Based Agent,[0],[0]
"Similarly, the value network Vη(st) is modeled as a MLP which takes the state st as input and outputs a real value as the expected return.",2.4 Training the Policy-Based Agent,[0],[0]
"By introducing the baseline bt for the policy gradient, we can reduce the variance of gradients and thus make the training process of policy network more stable.",2.4 Training the Policy-Based Agent,[0],[0]
"The network parameters η are updated by minimizing the loss function below:
L3(η) = (Vη(st)−Gt)2 (8)
Note that, in our method, both the RewardNet and the value network Vη(st) approximate the reward during training.",2.4 Training the Policy-Based Agent,[0],[0]
But the difference lies in that the RewardNet is designed to estimate a appropriate non-zero reward rt and further derive the more informative return Gt while Vη(st) aims to learn a baseline bt to reduce the variance of policy gradients.,2.4 Training the Policy-Based Agent,[0],[0]
We combine both of two networks to improve the gradients for our policy network and thus lead to a better agent.,2.4 Training the Policy-Based Agent,[0],[0]
The training procedure is described in Algo.,2.4 Training the Policy-Based Agent,[0],[0]
"2.
",2.4 Training the Policy-Based Agent,[0],[0]
Algorithm 2: Training the Agent 1 Initialize replay memory D2 to capacity N2 2 Initialize policy net π with random weights θ 3 Initialize value net V with random weights η 4,2.4 Training the Policy-Based Agent,[0],[0]
"Initialize RewardNet with random weights σ 5 for episode i← 1 to Z do 6 Rollout, collect rewards, and save the history in S2 (4-10 in Algo.",2.4 Training the Policy-Based Agent,[0],[0]
"1) 7 for (st, at, rt+1) in S2 do 8 Gt ← ∑T k=0",2.4 Training the Policy-Based Agent,[0],[0]
"γ
krt+k+1 9 Update RewardNet (13-17 in
Algo.",2.4 Training the Policy-Based Agent,[0],[0]
"1) 10 Store (st, at, Gt) in D2 11 if len(D2) > K2 then 12 Sample mini-batch from D2 13 Update η with loss L3 in Eq. 8 14 Update θ with loss L2 in Eq. 7",2.4 Training the Policy-Based Agent,[0],[0]
We use a user simulator to train our questioner agent and test the agent with the simulated answerer and real users.,3 Experimental Setup,[0],[0]
"Specifically, our experiments answer three questions: (1) Is our method more robust in real-world Q20 games, compared to the methods based on relevance table?",3 Experimental Setup,[0],[0]
(Section. 4.2),3 Experimental Setup,[0],[0]
And how does it perform in the simulation environment?,3 Experimental Setup,[0],[0]
(Section. 4.1) (2) Does our RewardNet help in the training process?,3 Experimental Setup,[0],[0]
"(Section. 4.3) (3) How the winning rate grows with the number of questions, and whether it is possible to stop earlier?",3 Experimental Setup,[0],[0]
(Section. 4.4),3 Experimental Setup,[0],[0]
Training the RL agent is challenging because the agent needs to continuously interact with the environment.,3.1 User Simulator,[0],[0]
"To speed up the training process of the proposed RL model, we construct a user simulator
which has enough prior knowledge to choose objects and answer questions selected by the agent.
",3.1 User Simulator,[0],[0]
"We collect 1,000 famous people and 500 questions for them.",3.1 User Simulator,[0],[0]
"Besides, for every person-question pair in our dataset, a prior frequency distribution over “Yes”, “No” and “Unknown” is also collected from thousands of real users.",3.1 User Simulator,[0],[0]
"For example, as for “Donald Trump”, question (a) “Is your role the American president?” is answered with “Yes” for 9,500 times, “No” for 50 times and “Unknown” for 450 times.",3.1 User Simulator,[0],[0]
"We use Eq.4 and 5 to construct three matrices R,W,U ∈ R|O|∗|A| (|O| = 1000, |A| = 500) which are used for state transition in the Section. 2.1.",3.1 User Simulator,[0],[0]
"Then given the object oi and question qj , the user simulator answers “Yes”, “No” and “Unknown” whenR(i, j),W (i, j), andU(i, j) has the max value among them respectively.
",3.1 User Simulator,[0],[0]
"Constructed by the prior knowledge, the simulator can give noise-free answer in most cases.",3.1 User Simulator,[0],[0]
"Because the prior frequency distribution for each person-question pair is collected from thousands of users with the assumption that most of them do not lie when answering questions in the Q20 game.
",3.1 User Simulator,[0],[0]
"In an episode, the simulator randomly samples a person following the object distribution s0, which is generated from the object popularity (see the state part of Section. 2.1), as the target object.",3.1 User Simulator,[0],[0]
Then the agent gives a guess when the number of selected questions reaches 20.,3.1 User Simulator,[0],[0]
"After that, the simulator check the agent’s answer and return a reward signal of win or loss.",3.1 User Simulator,[0],[0]
There is only one chance for the agent to guess in an episode.,3.1 User Simulator,[0],[0]
The win and loss reward are 30 and -30 respectively.,3.1 User Simulator,[0],[0]
"While the architectures of the policy network, RewardNet and value network can vary in different scenarios, in this paper, we simply use the MLP with one hidden layer of size 1,000 for all of them, but with different parameters.",3.2 Implementation Details,[0],[0]
"These networks take in the state vector directly, which is a probability distribution over all objects.",3.2 Implementation Details,[0],[0]
The RewardNet further takes in the one-hot vector of action at.,3.2 Implementation Details,[0],[0]
"Based on the input of RewardNet, the objectaware RewardNet takes one more target object otgt as the feature which is also a one-hot vector.
",3.2 Implementation Details,[0],[0]
"We use the ADAM optimizer (Kingma and Ba, 2014) with the learning rate 1e-3 for policy network and 1e-2 for both RewardNet and value network.",3.2 Implementation Details,[0],[0]
The discounted factor γ for calculating the long-term return is 0.99.,3.2 Implementation Details,[0],[0]
"The model was trained up
to 2,000,000 steps (2,00,000 games) and the policy network was evaluated every 5,000 steps.",3.2 Implementation Details,[0],[0]
"Each evaluation records the agent’s performance with a greedy policy for 2,000 independent episodes.",3.2 Implementation Details,[0],[0]
"The 2,000 target objects for these 2,000 episodes are randomly selected following the distribution s0, which is generated from the object popularity and kept the same for all the training settings.",3.2 Implementation Details,[0],[0]
"We compare our RL method with the entropybased model proposed by Wu et al. (2018), which utilizes the real-world answers to each objectquestion pair to calculate an object-question relevance matrix with the entropy-based method.",3.3 Competitor,[0],[0]
The relevance matrix is then used for question ranking and object ranking via carefully designed formulas and engineering tricks.,3.3 Competitor,[0],[0]
"Since this method is shown to be effective in their production environment, we consider it to be a strong baseline to our proposed RL model.",3.3 Competitor,[0],[0]
"We first evaluate our agent and the entropy-based baseline (referred to as EntropyModel, see Section. 3.3) by using the simulated user (Section. 3.1).",4.1 Simulated Evaluation,[0],[0]
"To investigate which initialization strategy of the state s0 is better (see the state part of Section. 2.1), we further evaluate two variants of our model: the agent with uniform distribution s0 (RL uniform) and the agent with the distribution s0 initialized by the prior knowledge on the object popularity (RL popularity).
",4.1 Simulated Evaluation,[0],[0]
"Fig. 2 shows the curves on the win rate of these methods evaluated on 2,000 independent episodes
with respect to the number of training steps.",4.1 Simulated Evaluation,[0],[0]
"Note that, the EntropyModel only needs to update its statistics during training and has already accumulated a significant number of data since it has been run for over a year in their production environment.",4.1 Simulated Evaluation,[0],[0]
"Therefore, only a small fraction of its statistics can be changed, which leads to a small rise at the beginning of training, and its win rate remains at around 95% afterwards.
",4.1 Simulated Evaluation,[0],[0]
"On the other hand, both our RL models continuously improve the win rate with the growing number of interactions with the user simulator, and they achieve 50% win rate after around 20,000 steps.",4.1 Simulated Evaluation,[0],[0]
"As we can see, although the s0 initialized with the prior knowledge of object popularity keeps consistent with the object selection strategy of the simulator, the agent with uniform distribution s0 (RL uniform) still performs clearly better than the agent with s0 based on the prior knowledge (RL popularity).",4.1 Simulated Evaluation,[0],[0]
The reason is that the former can explore the Q20 game environment more fully.,4.1 Simulated Evaluation,[0],[0]
The prior knowledge based s0 helps the agent narrow down the candidate space more quickly when the target object is a popular object.,4.1 Simulated Evaluation,[0],[0]
"However, it also becomes misleading when the target object is not popular and makes the agent even harder to correct the confidence of the target object.",4.1 Simulated Evaluation,[0],[0]
"On the contrary, the uniform distribution s0 makes the agent keep track of the target object only based on the user’s answers.",4.1 Simulated Evaluation,[0],[0]
"And the superior performance of the RL uniform indicates that our question selection policy is highly effective, which means it is not necessary to use the RL popularity to increase the win rate of hot objects in the game.
",4.1 Simulated Evaluation,[0],[0]
"As shown in Fig. 2, RL uniform achieves win rate 94% which is very close to EntropyModel.",4.1 Simulated Evaluation,[0],[0]
"Compared to our RL method, EntropyModel needs more user data to calculate their entropybased relevance matrix and involves many engineering tricks.",4.1 Simulated Evaluation,[0],[0]
The fact that RL uniform is competitive to EntropyModel in the noise-free simulation environment indicates that our RL method is very cost-effective: it makes use of user data more efficiently and is easier to implement.,4.1 Simulated Evaluation,[0],[0]
"To further investigate the performance of our RL method in the real-world Q20 game where noisy answers are common, we also conduct an human evaluation experiment.",4.2 Human Evaluation,[0],[0]
"Specifically, we let real
users to play the game with EntropyModel and RL uniform for 1,000 times respectively.",4.2 Human Evaluation,[0],[0]
"In the real-world Q20 game, users sometimes make mistakes when they answer the questions during the game.",4.2 Human Evaluation,[0],[0]
"For example, as for the target object “Donald Trump”, question (a) “Is your role the American president?” is sometimes answered with “No” or “Unknown” by real users.",4.2 Human Evaluation,[0],[0]
"On the contrary, the simulator hardly makes such mistakes since we have provided it with enough prior knowledge.",4.2 Human Evaluation,[0],[0]
As shown in Table.,4.2 Human Evaluation,[0],[0]
"1, RL uniform outperforms EntropyModel by about 4.5% on win rate in the real-world Q20 games.",4.2 Human Evaluation,[0],[0]
It shows that our RL method is more robust to noisy answers than EntropyModel.,4.2 Human Evaluation,[0],[0]
"Specifically, the robustness of our RL method to the noise is shown in the following two aspects.",4.2 Human Evaluation,[0],[0]
"First, compared to the rulebased statistics update in EntropyModel, our RL model can be trained by modern neural network optimizers in a principled way, which results in the better generalization ability of our model.",4.2 Human Evaluation,[0],[0]
"Secondly, different from the EntropyModel selecting the top-ranked question at each time-step, RL uniform samples a question following its question probability distribution πθ(a|s), which enables our agent to jump out of the local optimum caused by incorrect answers from users.",4.2 Human Evaluation,[0],[0]
"And since more randomness is introduced by sampling from the question probability distribution during training, it also improves the tolerance of our model towards the unexpected question sequences.
",4.2 Human Evaluation,[0],[0]
"Besides, we also find some interesting cases during human evaluation.",4.2 Human Evaluation,[0],[0]
"Sometimes, the RL agent selects a few strange questions which seems to be not that much relevant to the chosen object, but it can still find the correct answer at the end of game.",4.2 Human Evaluation,[0],[0]
"This situation is caused by the fact that our method samples questions based on the output of policy net, rather than greedy selection during training.",4.2 Human Evaluation,[0],[0]
We find that this phenomenon increases the user experience since it makes the agent more unpredictable to the users.,4.2 Human Evaluation,[0],[0]
"To investigate the effectiveness of our RewardNet (Section. 2.3), we further evaluate three variants of our model in the simulation environment: the model trained with with direct reward, RewardNet, and object-aware RewardNet, which are referred to as DirectReward, RewardNet, and ObjectRewardNet respectively.",4.3 The Effectiveness of RewardNet,[0],[0]
"They are all trained with the uniform distribution s0.
",4.3 The Effectiveness of RewardNet,[0],[0]
"As shown in Fig. 3, DirectReward converges in the early steps and has a relatively poor performance with the win rate 89%.",4.3 The Effectiveness of RewardNet,[0],[0]
Both RewardNet and ObjectRewardNet achieve the better performance with a win rate of 94% after convergence.,4.3 The Effectiveness of RewardNet,[0],[0]
"This clear improvement shows that the more informative long-term return, calculated with the immediate reward delivered by our RewardNet method, significantly helps the training of the agent.
",4.3 The Effectiveness of RewardNet,[0],[0]
"Furthermore, as shown in Fig. 3, we can also see that ObjectRewardNet learns faster than RewardNet in the early steps.",4.3 The Effectiveness of RewardNet,[0],[0]
"This indicates that ObjectRewardNet can estimate the immediate reward more quickly with the extra information provided by the target object, which leads to the faster convergence of the agent.",4.3 The Effectiveness of RewardNet,[0],[0]
"In this section, we investigate how the win rate grows with the number of asked questions and whether a early-stop strategy can be adopted in the game.",4.4 Win Rate Regarding Question Numbers,[0],[0]
"We use the user simulator to play the game with the RL uniform agent and two settings are taken into account: the simulator samples the target object following the uniform object distribution (UnifSimulator), and samples following
the prior object distribution based on the object popularity (PopSimulator).",4.4 Win Rate Regarding Question Numbers,[0],[0]
"We perform 1,000 simulations for each number of questions, and the win rate curve is shown in Fig. 4.
",4.4 Win Rate Regarding Question Numbers,[0],[0]
As we can see that UnifSimulator achieves the win rate of 80% with only 14 questions in both settings.,4.4 Win Rate Regarding Question Numbers,[0],[0]
And the flat curves in the region after 18 questions indicate that the game can be early stopped with the almost same win rate at step 18.,4.4 Win Rate Regarding Question Numbers,[0],[0]
"Since a lower win rate is acceptable sometimes, other early-stop strategies can also be derived for the better user experience with the trade-off between the win rate and game steps.
",4.4 Win Rate Regarding Question Numbers,[0],[0]
"Besides, the fact that RL uniform performs similarly under both settings actually shows that our RL method is robust to different objects.",4.4 Win Rate Regarding Question Numbers,[0],[0]
It also performs well on infrequent objects where we may have the limited user data for constructing a well-tuned state transition dynamics.,4.4 Win Rate Regarding Question Numbers,[0],[0]
"When our agent is playing the game with real users, we select two cases from records.",4.5 Case Study,[0],[0]
"In the first case, the person that the user chooses is Cristiano Ronaldo, the famous football player.",4.5 Case Study,[0],[0]
"As we can see in Tab. 2, our agent can still figure out the target person while No.17 and No.19 questions are answered wrong by the user, which indicates our agent is robust to noisy answers.",4.5 Case Study,[0],[0]
"In the second case, the chosen person is Napoleon Bonaparte who was the French Emperor.",4.5 Case Study,[0],[0]
"Although there are some other candidates satisfied the constraints, the target person can be figured out because of the people popularity, which is shown in Tab. 3.",4.5 Case Study,[0],[0]
Q20.,5 Related Work,[0],[0]
"The Q20 game is popularized as an electronic game by the program of Robin Burgener in 1988 (Burgener, 2006), which uses a objectquestion relevance table to rank questions and target objects.",5 Related Work,[0],[0]
"Wu et al. (Wu et al., 2018) improves the relevance table with entropy-based metrics, and uses complicated engineering tricks to make it perform quite well in their production environment.",5 Related Work,[0],[0]
"These table-based methods use rules to update parameters, which makes them easily affected by noisy answers.",5 Related Work,[0],[0]
"Besides, Zhao and Maxine (2016) also explores Q20 in their dialogue state tracking research.",5 Related Work,[0],[0]
"However, they only use a small toy Q20 setting where the designed questions are about 6 person attributes in the Knowledge Base (KB).",5 Related Work,[0],[0]
"Since their method relies on the KB for narrowing down the scope of target object, it is not applicable to real-world Q20 games where a welldefined object KB is often unavailable.",5 Related Work,[0],[0]
"Compared to previous approaches, our RL method is robust to the answer noise and does not rely on the KB.
",5 Related Work,[0],[0]
Deep Reinforcement Learning.,5 Related Work,[0],[0]
"DRL has witnessed great success in playing complex games like Atari games (Mnih et al., 2015) , Go (Silver et al., 2016), and etc.",5 Related Work,[0],[0]
"In the natural language processing (NLP), DRL is also used to play text-based games (Narasimhan et al., 2015), and used to handle fundamental NLP tasks like machine translation (He et al., 2016) and machine comprehension (Hu et al., 2017) as well.",5 Related Work,[0],[0]
Our Q20 game lies in the intersection of the field of game and NLP.,5 Related Work,[0],[0]
"In this work, we propose a policy-based RL model that acts as the questioner in the Q20 game, and it exhibits the superior performance in our human evaluation.
",5 Related Work,[0],[0]
Natural Language Games.,5 Related Work,[0],[0]
"In the literature, there are some works focusing on solving and generating English riddles (De Palma and Weiner, 1992; Binsted, 1996) and Chinese character riddles (Tan et al., 2016).",5 Related Work,[0],[0]
"Compared to riddles, the Q20 game is a sequential decision process which requires careful modeling of this property.",5 Related Work,[0],[0]
"In this paper, we propose a policy-based RL method to solve the question selection problem in the Q20 Game.",6 Conclusions,[0],[0]
"Instead of using the direct reward, we further propose an object-aware RewardNet to estimate the appropriate non-zero reward and
thus make the long-time return more informative.",6 Conclusions,[0],[0]
"Compared to previous approaches, our RL method is more robust to the answer noise which is common in the real-world Q20 game.",6 Conclusions,[0],[0]
"Besides, our RL agent can also ask various questions and does not require the existing KB and complicated engineering tricks.",6 Conclusions,[0],[0]
"The experiments on a noisy-free simulation environment show that our RL method is competitive to an entropy-based engineering system, and clearly outperforms it on the human evaluation where noisy answers are common.
",6 Conclusions,[0],[0]
"As for the future work, we plan to explore methods to use machine reading to automatically construct the state transition dynamics from corpora like Wikipedia.",6 Conclusions,[0],[0]
"In this way, we can further build an end-to-end framework for the large-scale Q20 games in the real world.",6 Conclusions,[0],[0]
We gratefully thank the anonymous reviewers for their insightful comments and suggestions on the earlier version of this paper.,Acknowledgement,[0],[0]
The first author also thanks the Microsoft for providing resources for the research.,Acknowledgement,[0],[0]
The 20 Questions (Q20) game is a well known game which encourages deductive reasoning and creativity.,abstractText,[0],[0]
"In the game, the answerer first thinks of an object such as a famous person or a kind of animal.",abstractText,[0],[0]
Then the questioner tries to guess the object by asking 20 questions.,abstractText,[0],[0]
"In a Q20 game system, the user is considered as the answerer while the system itself acts as the questioner which requires a good strategy of question selection to figure out the correct object and win the game.",abstractText,[0],[0]
"However, the optimal policy of question selection is hard to be derived due to the complexity and volatility of the game environment.",abstractText,[0],[0]
"In this paper, we propose a novel policy-based Reinforcement Learning (RL) method, which enables the questioner agent to learn the optimal policy of question selection through continuous interactions with users.",abstractText,[0],[0]
"To facilitate training, we also propose to use a reward network to estimate the more informative reward.",abstractText,[0],[0]
"Compared to previous methods, our RL method is robust to noisy answers and does not rely on the Knowledge Base of objects.",abstractText,[0],[0]
Experimental results show that our RL method clearly outperforms an entropy-based engineering system and has competitive performance in a noisyfree simulation environment.,abstractText,[0],[0]
Playing 20 Question Game with Policy-Based Reinforcement Learning,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 92–102 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Every public speech involving a large audience can be seen as a game of coordination (Asch, 1951): at each moment, each individual member of the audience must decide in a split second whether to applaud at what has just been said.",1 Introduction,[0],[0]
"Applause is a potentially risky action: if an individual spontaneously claps but no one joins in, they suffer some negative social cost; the game is to judge from their own private information and content of the speech whether the rest of the audience will applaud at the same time they do.
",1 Introduction,[0],[0]
"Because of this cost, audiences respond to several interacting factors in a speaker’s behavior: a.) the content of the message; b.) their delivery (so that changes in pitch, duration and gaze signal salient moments for which applause may be licensed); and c.) the verbal design of the message—those rhetorical strategies that speakers use to signal that applause is welcome (Atkinson, 1984; Heritage and Greatbatch, 1986).
",1 Introduction,[0],[0]
"In this work, we attempt to model all three of these dimensions in developing a computational model for applause.",1 Introduction,[0],[0]
"While past work has focused on these elements in isolation (Guerini et al., 2015; Liu et al., 2017) or for related problems such as laughter detection (Purandare and Litman, 2006; Chen and Lee, 2017; Bertero and Fung, 2016), we find that developing a holistic model encompassing all three aspects yields the most robust predictor of applause.
",1 Introduction,[0],[0]
"We focus on political speeches, and in particular those at campaign rallies, which lend themselves well to analysis of rhetorical strategies for several reasons.",1 Introduction,[0],[0]
"First, the speakers at these events prioritize maintaining the crowd’s attention (Strangert, 2005).",1 Introduction,[0],[0]
"Motivated to drum up excitement and fervor among their supporters that they hope will carry beyond the event and into the voting booth, speakers pull out their strongest rhetorical tactics.",1 Introduction,[0],[0]
"Second, campaign speeches usually consist of a series of self-contained messages that can be fully expressed within a few utterances (Heritage and Greatbatch, 1986), yielding a well-defined observation of a complete rhetorical strategy.",1 Introduction,[0],[0]
"Lastly, these speeches are delivered by a single speaker to a partisan crowd, and clapping, cheering, and other responses are invited and expected.
",1 Introduction,[0],[0]
"We focus in particular in this work on operationalizating the verbal design of the speech; in so doing, one contribution we make is operationalizing the concepts of tension and release.",1 Introduction,[0],[0]
"Writers and performers often communicate with their audience on a fundamental level by building up tension, and then, at the proper time, delivering a satisfying release.",1 Introduction,[0],[0]
"These simple but pervasive concepts structure our experience of different modes of communication used throughout everyday life, including music (Madsen and Fredrickson, 1993), literature (Rabkin, 1973) and film (Carroll, 1996).
",1 Introduction,[0],[0]
"Tension in music can be built up by harmonic
92
movement away from a tonal center; release then comes with a return to that established tonic (Hindemith, 1937).",1 Introduction,[0],[0]
"One form of tension in literature is realized as suspense (Barthes and Duisit, 1975; Vorderer et al., 1996; Algee-Hewitt, 2016), in which a reader’s knowledge of events is uncertain (either because those events take place in the narrative future or are withheld from narration), and released when that knowledge is revealed.",1 Introduction,[0],[0]
"In film, sudden changes in camera perspective create graphic tension, which is then released as the shot returns to a stable position (Bordwell, 2013).",1 Introduction,[0],[0]
"Often, it is the confluence of multiple sources of tension that mark the climax of a narrative (Hume, 2017).",1 Introduction,[0],[0]
"We draw on each of these strands of work in operationalizing tension and release as a rhetorical strategy.
",1 Introduction,[0],[0]
"In this work, we make the following contributions:
• We collect a new dataset of text and audio from 310 speeches from campaign events leading up to the 2016 U.S presidential election with associated tags for over 19,000 instances of audience applause.
",1 Introduction,[0],[0]
"• We introduce new textual and acoustic features inspired by tension and release, combine and compare them with features used in previous work, and deploy those features in a logistic regression model and in an LSTM to predict when applause is likely to occur.",1 Introduction,[0],[0]
"Code, data, and trained models are openly available to the public at https://github.com/ jrgillick/Applause/.",1 Introduction,[0],[0]
"Heritage and Greatbatch (1986) conduct an extensive analysis of nearly 500 speeches from British political party conferences, manually associating each of over 2000 instances of applause with coded message types (e.g. External Attacks or Statements of Approval), rhetorical devices (e.g. Contrast/Antithesis or Headline-Punchline), and performance factors (e.g. speech stress or body language).",2.1 Rhetoric and Response,[0],[0]
"They find most of these factors to be positively correlated with applause; one especially striking result is over two thirds of observed instances of applause can be explained through a set of seven rhetorical devices (including contrast,
pursuit, position taking, and “the 3-part list”).",2.1 Rhetoric and Response,[0],[0]
"Though each device is different, a common feature of most of these techniques is that they are not always carried out within a single sentence or utterance; they often depend on the relationship between a series of utterances or phrases.",2.1 Rhetoric and Response,[0],[0]
We argue in this work that some of these relationships can be characterized and subsequently operationalized within models as tension and release.,2.1 Rhetoric and Response,[0],[0]
Recent work from Guerini et al. (2015) and Liu et al. (2017) approaches the task of applause prediction by looking at textual features of the individual sentences that immediately precede audience applause.,2.2 Predicting Applause,[0],[0]
"Both follow the methodology proposed by Danescu-Niculescu-Mizil et al. (2012) in constructing a data set for binary classification, which is composed of sentences that generated applause, each paired with a single nearby sentence from the same document that did not lead to applause.
",2.2 Predicting Applause,[0],[0]
"Guerini et al. (2015) examine a set of features designed to capture aspects of euphony, or “the inherent pleasantness of the sounds of words” that might make an utterance memorable or persuasive—such as rhyme, alliteration, homogeneity, and plosives.",2.2 Predicting Applause,[0],[0]
"On the CORPS dataset (Guerini et al., 2013), which consists of the text of several thousand political speeches dating from 1917 to 2011, they define persuasive sentences as those that preceded annotations of either applause or laughter.
",2.2 Predicting Applause,[0],[0]
"Liu et al. (2017), working with a corpus of TED talks, use logistic regression to predict applause from sentences using a combination of features: euphony (again from Guerini et al. (2015)), linguistic style markers derived from membership in LIWC categories, markers of emotional expression derived from membership in the NRC Emotion Lexicon, mentions of names, rhetorical questions (string matching for “?”), expressions of gratitude (matching a handcrafted list of word stems including “thank∗” and “grateful∗”), and expressions seeking applause (matching the pattern “applau∗”).",2.2 Predicting Applause,[0],[0]
Liu et al. (2017) also report that adding the same features for earlier sentences beyond the final sentence that preceded the applause caused the prediction accuracy to go down.,2.2 Predicting Applause,[0],[0]
"Chen and Lee (2017) and Bertero and Fung (2016) run similar binary classification experiments but pre-
dict laughter as opposed to applause.",2.2 Predicting Applause,[0],[0]
Bertero and Fung (2016) analyze punchlines from the TV sitcom “The Big Bang Theory” and report 70% accuracy using an LSTM.,2.2 Predicting Applause,[0],[0]
"They touch briefly on the notion of tension and release in humor, as punchlines typically depend on a previous line as a setup in order to be funny.",2.2 Predicting Applause,[0],[0]
"In this work, we focus on a new data set of campaign speeches from the 2016 U.S. presidential race, which we obtain from the public domain broadcasts of C-SPAN.",3.1 Corpus Acquisition,[0],[0]
"We downloaded about 500 speeches from presidential candidates, vice presidential candidates, or former presidents, collecting audio files and transcripts that were tagged in the categories “Campaign 2016” and “Speech” and which took place between 12/01/2015 and 12/01/2016.",3.1 Corpus Acquisition,[0],[0]
"We then excluded events that took place outside of a traditional campaign speech setting (e.g. town hall events) or events that contained multiple speakers without a speaker identification tied to the transcript, which yielded a final set of 310 speeches from 16 speakers.",3.1 Corpus Acquisition,[0],[0]
"Because different types of events have different social norms around when and whether applause is appropriate (Atkinson, 1984; Heritage and Greatbatch, 1986), we control for these factors to some degree by restricting our dataset to events in similar settings and within a single year.",3.1 Corpus Acquisition,[0],[0]
"As a point of comparison, the C-SPAN dataset contains 62 instances of applause per speech on average, whereas the CORPS data (Guerini et al., 2013) contains 13.",3.1 Corpus Acquisition,[0],[0]
"Since our C-SPAN data originates in video, we have access to the audio information of a speech event, which we employ both for feature extraction and for automatically identifying when applause occurs.",3.2 Applause Detection in Audio,[0],[0]
"Following Clement and McLaughlin (2016), we train an acoustic model using a set of poetry readings from the PennSound archive to distinguish applause from speech.",3.2 Applause Detection in Audio,[0],[0]
We used logistic regression on the standard set of MFCC features and found similar results on the PennSound data to the reported classification accuracy of 99.4%.,3.2 Applause Detection in Audio,[0],[0]
"In a manual inspection of 100 applause segments from 5 different speeches in the C-SPAN corpus, our applause detector achieved 92% preci-
sion, 90% recall, and 91% F1 score.",3.2 Applause Detection in Audio,[0],[0]
"Due to variation in the nature of applause in a crowd (sometimes we observe examples of isolated clapping and cheering, mixed laughter and applause, or applause interrupting the speaker), some ambiguity is inherent among the labels.
",3.2 Applause Detection in Audio,[0],[0]
"We also measure the applause by first running the speeches through the audio source separation algorithm from Chandna et al. (2017), which was trained to separate voice from music, and then measuring the RMSE loudness of the separated non-vocal track.",3.2 Applause Detection in Audio,[0],[0]
"We found that the separation worked well, qualitatively matching with the results from the applause detection classifier.",3.2 Applause Detection in Audio,[0],[0]
"To match the identified segments of applause in the audio files with the relevant text from the transcriptions, we ran forced alignment using the Kaldi Toolkit (Povey et al., 2011).",3.3 Forced Alignment,[0],[0]
"Since the CSPAN transcripts are sourced from uncorrected closed captioning, the text contains a number of misspellings and paraphrases, which we handled by discarding the 12% of words for which forced alignment failed.",3.3 Forced Alignment,[0],[0]
"Though these transcriptions are not as accurate as what we would find in professionally transcribed datasets, previous work has shown that it is possible to achieve good accuracy in downstream tasks even with high error rates in transcription (Peskin et al., 1993; Novotney and Callison-Burch, 2010).",3.3 Forced Alignment,[0],[0]
"Moreover, the caliber of transcripts derived from closed captioning is representative of the data that would be available in real time for practical use at future speech events.
",3.3 Forced Alignment,[0],[0]
"To estimate the accuracy of the closed captions, we manually transcribed selections from 5 speeches in the C-SPAN data totaling about 25 minutes and 2250 words, finding 30.9% WER relative to the reference transcriptions in our sample.",3.3 Forced Alignment,[0],[0]
"Many of the errors are due to omitted words and phrases in the closed captions, which may occur as a result of transcribers’ inability to keep up with the pace of fast speeches; in this sample, the closed caption texts contained 17% fewer words than our gold standard transcriptions.
",3.3 Forced Alignment,[0],[0]
"After finding the alignments, we segmented out a list of utterances by defining a minimum period of silence between words.",3.3 Forced Alignment,[0],[0]
"Since many of the transcripts do not have punctuation, we find that dividing the text into utterances yielded qualitatively more coherent units than sentence boundary detec-
tion.",3.3 Forced Alignment,[0],[0]
"Dividing into utterances is also conducive to building a dataset for binary classification, since every pause by the speaker yields an opportunity for applause.",3.3 Forced Alignment,[0],[0]
"We chose a pause length of 0.7 seconds, but in future work we might be able to improve our models by adapting this threshold to the rate of speech in order to maintain consistent phrase sizes across different speakers.",3.3 Forced Alignment,[0],[0]
"Given this set of utterances, we paired each utterance with a “positive” or “negative” label, determined by whether applause occurred within 1.5 seconds of the end of the utterance.",3.3 Forced Alignment,[0],[0]
"All of these preprocessing choices were made during the corpus preparation phase, prior to any experimental evaluation.
",3.3 Forced Alignment,[0],[0]
"Table 1 provides summary statistics for the number of speakers, speeches, utterances, and acts of applause in our data.",3.3 Forced Alignment,[0],[0]
"In our models, we draw features from previous work on applause or humor prediction and then supplement them with a new set of features inspired by the ideas of tension and release and by the rhetorical strategies of Heritage and Greatbatch (1986).",4 Models,[0],[0]
LIWC.,4.1 Features adapted from existing work,[0],[0]
"Features for membership in 73 LIWC categories proved to be the most effective for applause prediction in TED talks (Liu et al., 2017).
",4.1 Features adapted from existing work,[0],[0]
Euphony.,4.1 Features adapted from existing work,[0],[0]
We adopt the 4 features for “euphony” defined by Guerini et al. (2015):,4.1 Features adapted from existing work,[0],[0]
"Rhyme, Alliteration, Homogeneity, and Plosives.
",4.1 Features adapted from existing work,[0],[0]
Lexical.,4.1 Features adapted from existing work,[0],[0]
Guerini et al. (2015) find n-grams to be highly predictive of both applause and laughter.,4.1 Features adapted from existing work,[0],[0]
"We operationalize these features with bigrams, including in our model all bigrams that appear at least 5 times in the corpus.
Embeddings.",4.1 Features adapted from existing work,[0],[0]
Bertero and Fung (2016) use sentence embeddings learned from a CNN encoder as input to an LSTM.,4.1 Features adapted from existing work,[0],[0]
"We adopt this feature for use in our neural models, encoding phrases using the Skip-Thought model of Kiros et al. (2015).
Acoustic.",4.1 Features adapted from existing work,[0],[0]
Purandare and Litman (2006) use a set of features intended to capture elements of prosody in a model for humor prediction in television dialogue.,4.1 Features adapted from existing work,[0],[0]
"These features include the mean, max, min, range, and standard deviation values in an utterance’s pitch (F0) and energy (RMS), along with features for internal silence and for tempo.",4.1 Features adapted from existing work,[0],[0]
"We compute the F0 statistics with Reaper (Talkin, 2015) and the energy statistics with Librosa (McFee et al., 2015).",4.1 Features adapted from existing work,[0],[0]
Repeated Words.,4.2.1 Repetition,[0],[0]
Rhetorical strategies such as “The 3-part List” and “Contrast” rely on repetition to drive home important points.,4.2.1 Repetition,[0],[0]
"We capture this phenomenon by computing the proportion of words in each utterance that also appear in the immediately preceding phrase.
",4.2.1 Repetition,[0],[0]
Longest Common Subsequence.,4.2.1 Repetition,[0],[0]
"Repeating an entire phrase, especially one with a politically charged topic, serves to build tension through the notion of “theme and variation” as is often realized
in music (Cope, 2005); an example of this phenomenon in our data can be found in the following passage:
We will not allow the party of Lincoln and Reagan to fall into the hands of a con artist.",4.2.1 Repetition,[0],[0]
We will not allow the next president of the United States to be a socialist like Bernie Sanders.,4.2.1 Repetition,[0],[0]
"And we will not allow the next president of the United States to be someone under FBI investigation like Hillary Clinton.
",4.2.1 Repetition,[0],[0]
"[Marco Rubio, Mar. 1, 2016]
We calculate this theme and variation by measuring the longest common subsequence between adjacent phrases.",4.2.1 Repetition,[0],[0]
"Delta features (local approximations to derivatives) are commonly used in speech recognition and audio classification systems (Povey et al., 2011).",4.2.2 Deltas,[0],[0]
"In a discourse, either highly similar or drastically different neighboring pairs of utterances may indicate dramatic moments.",4.2.2 Deltas,[0],[0]
"We operationalize these features by explicitly adding a delta measurement for every feature in our model, which captures the difference between every feature at time t and the same feature at time t − 1.",4.2.2 Deltas,[0],[0]
"For K-dimensional vector embeddings, we calculate deltas as their cosine distance.",4.2.2 Deltas,[0],[0]
"Rhetorical Structure Theory (RST) provides a foundation for describing the ways in which functional components of a text combine to form a coherent whole (Thompson and Mann, 1987).",4.2.3 RST,[0],[0]
At the core of RST is a categorization system consisting of relations between elementary discourse units (EDUs).,4.2.3 RST,[0],[0]
"Relations between units are typically hierarchical (a nucleus and a satellite), but can also be defined between equally significant units (two nuclei).
",4.2.3 RST,[0],[0]
"A typical RST tree can be seen below, where the sentence “He won’t win, but I’ll vote for him anyway”, he said is decomposed into three elementary discourse units (EDUs); those discourse units form the leaves of a tree with intermediate structure between subphrases and labeled edges along each branch.
",4.2.3 RST,[0],[0]
"ATTRIBUTION
CONTRAST
“He won’t win,
but I’ll vote for him anyway”
he said.
",4.2.3 RST,[0],[0]
"Some of the rhetorical strategies defined by Heritage and Greatbatch (1986), such as “Contrast,” map directly to RST relations, while others do not have a clear one-to-one mapping but are qualitatively similar in their descriptions.",4.2.3 RST,[0],[0]
"While RST has been used with success for classification problems in the past (Ji and Smith, 2017; Bhatia et al., 2015), it has not yet been employed in existing models for applause prediction.",4.2.3 RST,[0],[0]
"In our work, we parse the rhetorical structure of the extracted sequence of phrases using the RST parser of Ji and Eisenstein (2014).",4.2.3 RST,[0],[0]
"From the structure of this RST tree, we extract two classes of features.
RST label.",4.2.3 RST,[0],[0]
"First, we operationalize the rhetorical category for an individual elementary discourse unit.",4.2.3 RST,[0],[0]
While the span of text within a single EDU is implicated in several rhetorical relations throughout the tree (as He won’t win bears a CONTRAST relationship with,4.2.3 RST,[0],[0]
but I’ll vote for him anyway and is part of the ATTRIBUTION relationship with he,4.2.3 RST,[0],[0]
"said), each EDU bears exactly one leaf relationship with the rest of the tree—here, He won’t win is a nucleus of a CONTRAST relationship, but I’ll vote for him anyway is also a nucleus of a CONTRAST relationship, and he said is the satellite of an ATTRIBUTION relationship.
",4.2.3 RST,[0],[0]
"We featurize a sentence as the set of all such typed relationships that EDUs within it hold; each typed relationship is the conjunction of the label (e.g., CONTRAST, ATTRIBUTION) and directionality (Nucleus, Satellite).
",4.2.3 RST,[0],[0]
Rhetorical phrase closures.,4.2.3 RST,[0],[0]
"In order to further operationalize the notion of predictability of applause, we measure the number of rhetorical phrases that a given discourse segment brings to closure.",4.2.3 RST,[0],[0]
"We can illustrate this with figure 1, which presents a sample RST tree with only the spans annotated (i.e., without RST labels or nucleus/satellite directed edges).",4.2.3 RST,[0],[0]
"This tree spans 10 elementary discourse units; each non-terminal node is annotated with the span of the subtree
rooted at that node (so the root spans all ten EDUs, while its left child spans only the first five).",4.2.3 RST,[0],[0]
"The final discourse unit (EDU 10) is the final EDU in three rhetorical phrases (those spanning EDUs 9-10, 6-10 and the entire discourse 1-10).",4.2.3 RST,[0],[0]
"We might hypothesize that the greater number of discourse phrases that a given discourse unit closes, the stronger the signal it provides that applause is licensed (and hence the greater likelihood to be followed by applause empirically).",4.2.3 RST,[0],[0]
"For a sentence with multiple discourse units, we featurize this value as the maximum number of rhetorical phrases closed by any unit it contains.",4.2.3 RST,[0],[0]
"We present two experiments to uncover the degree to which we are able to predict applause from different operationalizations of a politician’s campaign speech: one in which have access to a politician’s previous speeches, and can learn their specific nuances and stock phrases used to solicit applause; and another in which we seek to uncover the broader rhetorical strategies common to multiple speakers.
",5 Experiments,[0],[0]
"We refer to the following sets of features when we summarize results:
• Guerini.",5 Experiments,[0],[0]
"Euphony features from Guerini et al. (2015).
",5 Experiments,[0],[0]
• Liu.,5 Experiments,[0],[0]
"LIWC features and additional matchers for handcrafted regular expressions from Liu et al. (2017)
• Audio.",5 Experiments,[0],[0]
All acoustic features described in §4.1 above.,5 Experiments,[0],[0]
•,5 Experiments,[0],[0]
Combined.,5 Experiments,[0],[0]
"Combination of features from
Guerini, Liu, and Audio.
• Tension.",5 Experiments,[0],[0]
"Combination of RST (§4.2.3), repetition (§4.2.1), and delta features (§4.2.2).",5 Experiments,[0],[0]
• N-gram.,5 Experiments,[0],[0]
Bigram features.,5 Experiments,[0],[0]
• Skip-Thought.,5 Experiments,[0],[0]
"4800 dimensional Skip-
Thought embeddings.",5 Experiments,[0],[0]
"Access to a politician’s previous speeches provides a great deal of evidence for understanding their rhetorical strategies for soliciting applause; speakers often give variations of the same speech at different campaign events, and rely on a fixed set of stock phrases (e.g., “Yes, We Can,” “Make America Great Again”) and general strategies to solicit reactions (Lu, 1999; Miller, 1939; Petrow and Sullivan, 2007).",5.1 Intra-speaker validation,[0],[0]
"To model this, we attempt to predict a speaker’s likelihood of applause using only information from their own speeches.
",5.1 Intra-speaker validation,[0],[0]
"We use logistic regression with `2 regularization for this experiment, with hyperparameters chosen through cross-validation on the training data.",5.1 Intra-speaker validation,[0],[0]
"We run 10-fold cross validation for each speaker, and leave-one-out cross validation for those speakers with fewer than 10 speeches (we exclude Rick Santorum from this experiment because we have only one speech from him), with whole speeches divided across folds so that no utterances from the same speech ever appear in both training and test sets.",5.1 Intra-speaker validation,[0],[0]
Reported results aggregate the predictions across all speakers to calculate the final accuracies.,5.1 Intra-speaker validation,[0],[0]
"We choose utterances (or sequences of utterances) that directly precede applause as positive examples, pairing each one with a negative example randomly chosen from the same speech.",5.1 Intra-speaker validation,[0],[0]
"Since we use different amounts of data for each speaker, we are not able to compare accuracies across all speakers, but we can see that some speakers are significantly easier to model: for example, our best model reaches 0.719 accuracy on Bernie Sanders but only 0.660 on Donald Trump.
",5.1 Intra-speaker validation,[0],[0]
"Table 2 summarizes the results, comparing across different combinations of features as well as across a scope of a single phrase or multiple phrases.",5.1 Intra-speaker validation,[0],[0]
All feature combinations are scoped over a single utterance unless otherwise noted.,5.1 Intra-speaker validation,[0],[0]
"At the same time, many of the strategies identified by Heritage and Greatbatch (1986) are gener-
alized rhetorical devices used to solicit applause; we should expect then that a model trained on a fixed set of speakers should be able to generalize to speakers not in the training data.",5.2 Inter-speaker validation,[0],[0]
"To test this more realistic scenario, we performed Kfold cross-validation on all of the speakers in our dataset, holding out one speaker in turn for each fold (so that the same speaker did not appear in the training and test partitions).
",5.2 Inter-speaker validation,[0],[0]
"In this experiment, we use both logistic regression and neural models (sharing training data between speakers has the added benefit of allowing us enough data to reasonably train a neural model).",5.2 Inter-speaker validation,[0],[0]
All logistic regression models were trained in the same way is in the intra-speaker case.,5.2 Inter-speaker validation,[0],[0]
Our feedforward and LSTM models use a hidden state size of 100 for models including phrase embeddings (4800 dimensions) and a hidden state of size 25 for models without phrase embeddings.,5.2 Inter-speaker validation,[0],[0]
"All LSTM models use a standard formulation of attention (Bahdanau et al., 2014), and all neural models are trained with dropout (Srivastava et al., 2014) and the ADAM optimizer (Kingma and Ba, 2014).",5.2 Inter-speaker validation,[0],[0]
"We implemented the models using Keras (Chollet et al., 2015) and Tensorflow (Abadi et al., 2016).
",5.2 Inter-speaker validation,[0],[0]
"Table 3 summarizes these results, and table 4 shows the coefficients for the most significant features.",5.2 Inter-speaker validation,[0],[0]
"Each of the feature classes we operationalize offers some ability to recognize what Heritage and Greatbatch (1986) term the “projectability” of applause—the ability of an audience to see an applaudable moment on the horizon.
Audio.",6 Analysis,[0],[0]
"Perhaps not surprising in retrospect is the ability of acoustic features (only summary statistics of the pitch and energy) to solicit applause:
higher pitch and energy, and a broader pitch range are all predictive of applause; while past work has focused on textual indicators of applause, these results suggest that how a message is delivered is equally important.
",6 Analysis,[0],[0]
Lexical.,6 Analysis,[0],[0]
"The use of explicit n-grams improves performance significantly in the intra-speaker setting, where they are able to capture stock phrases employed by the same speaker at different events.",6 Analysis,[0],[0]
"N-grams are also predictive across different speakers, though the performance gains are not as high in the inter-speaker setting.
",6 Analysis,[0],[0]
"The strongest bigrams predictive of applause include moral declaratives like should not (e.g., “and billionaires should not be able to buy elections”",6 Analysis,[0],[0]
"[Bernie Sanders]), right to (“you have a right to be angry”",6 Analysis,[0],[0]
"[Marco Rubio]), and should be (“They should be ashamed of that kind of behavior”",6 Analysis,[0],[0]
"[Hillary Clinton]); call-outs to the audience such as this room (“Love the people in this room”
[Donald Trump]) and listening to (“our campaign is listening to our Latino brothers and sisters”",6 Analysis,[0],[0]
"[Bernie Sanders]); and politically charged topics such as political revolution, equal pay, immigration reform, planned parenthood, campaign contributors and police officers.
LIWC.",6 Analysis,[0],[0]
"Among broader lexical category features, we see the LIWC FOCUSFUTURE category strongly indicative of applause; this category includes auxilaries like will, going, gonna (including conjunctions I’ll) and future-oriented verbs like anticipate; also important are categories of BODY (including heart, hands, brain) and REWARD (including succeed, optimism, great).
",6 Analysis,[0],[0]
Rhetorical.,6 Analysis,[0],[0]
"While RST features were not as predictive for applause as other (likely correlated) features, we still see a strong alignment between the RST features most associated with applause and those rhetorical devices outlined by Heritage and Greatbatch (1986): in particular, a clear relationship between applause and the RST category of ANTITHESIS (a contrastive relation between two discourse units with a clear nucleus and satellite, rather than two equal nuclei) and PURPOSE (a relation between a discourse unit that must take place in order for another to be realized).",6 Analysis,[0],[0]
"As expected, phrases that close more discourse units tend to be more predictive of applause.
Contextual.",6 Analysis,[0],[0]
"Though lexical features from the final utterance significantly outweigh the effects of previous context in the intra-speaker setting, in the inter-speaker case we leveraged gains from longterm context in the LSTM to reach a similar level of performance attained from the lexical features,
but without access to lexical cues provided by the n-grams at all.",6 Analysis,[0],[0]
"This result suggests that the improved performance in the intra-speaker setting may be largely due to the presence of specific words and catch-phrases; the other stylistic features are more easily generalized to new speakers.
7 “Please clap”
As a further measure of out-of-sample validity, we can analyze the predictions we make for the single example where a speaker wears his communicative intent on his sleeve.",6 Analysis,[0],[0]
"On February 2, 2016, presidential candidate Jeb Bush spoke to a crowd in New Hampshire a week before their state primary.",6 Analysis,[0],[0]
"His speech ended with the following:
So here’s my pledge to you.",6 Analysis,[0],[0]
"[I] will be a commander-in-chief who will have the back of the military, I won’t trash talk, I won’t be a divider-in-chief or an agitator-in-chief, I won’t be out there blowharding talking a big game without backing it up; I think the next President needs to be a lot quieter but send a signal that we’re prepared to act in the national security interests of this country to get back in the business of creating a more peaceful world . . . . . . . . .",6 Analysis,[0],[0]
"Please clap.
",6 Analysis,[0],[0]
"[Jeb Bush, Feb 2, 2016]1
Bush’s admonition to the audience (“please clap”) earned criticism in news coverage at the time (Benen, 2016), but also presents us with a rare insight into a speaker’s true rhetorical intention; in this case, Bush was soliciting applause and was vocal about not being able to do so.
",6 Analysis,[0],[0]
Does our model recover this true intention?,6 Analysis,[0],[0]
"Indeed it does; while the opening So here’s my pledge to you is predicted to not solicit applause (with applause probability of 24.8%), the segment that ends with peaceful world is strongly predicted to have been followed by applause",6 Analysis,[0],[0]
(with an applause probability of 94.5%).,6 Analysis,[0],[0]
"The strongest features are again lexical (this country, commander in chief ), a LIWC focus on the future (elicited by will), and an RST PURPOSE relation (evoked by to get back in the business of creating a more peaceful world).
",6 Analysis,[0],[0]
1Video of this speech can be found at: https://www.,6 Analysis,[0],[0]
youtube.com/watch?v=DdCYMvaUcrA,6 Analysis,[0],[0]
"We present in this work a new dataset for the analysis of political rhetoric derived from the public campaign speeches of politicians during the 2016 United States presidential election, along with empirical results assessing the performance of different operationalizations of rhetoric derived from the theoretical work of Heritage and Greatbatch (1986) and others in order to measure and predict the occurrence of applause.",8 Conclusion,[0],[0]
"We introduce several new features designed to capture elements of tension and release in public performance, including rhetorical contrast, closure, repetition and movement across speech segments; while each of these features in isolation is able to predict applause to varying degree and comport with our prior understanding of their utility, we find that lexicalized features are among the strongest source of information in determining applause; while audiences react to many dimensions of a speaker’s style, the words they use—as slogan, stock phrases, and indicators of more complex rhetorical functions like moral valuations and imperatives—matter most.
",8 Conclusion,[0],[0]
"As detailed in previous work (Liu et al., 2017; Haider et al., 2017; Clement and McLaughlin, 2016), understanding and identifying climactic moments in speeches can be useful for a variety of reasons, including learning to give better talks, automatically summarizing videos and transcripts, and analyzing social dynamics within crowds.",8 Conclusion,[0],[0]
"One additional interesting application of this work is to bring to the surface occasions where a speaker uses typical applause-seeking devices but does not receive applause (the “Please Clap” moments); we leave to future work identifying the reverse, when speakers receive applause without invoking common techniques (for example, to identify instances of claques paid to clap).",8 Conclusion,[0],[0]
Many thanks to the anonymous reviewers for their helpful feedback.,9 Acknowledgments,[0],[0]
The research reported in this article was supported by a UC Berkeley Fellowship for Graduate Study to J.G. and by resources provided by NVIDIA.,9 Acknowledgments,[0],[0]
This work examines the rhetorical techniques that speakers employ during political campaigns.,abstractText,[0],[0]
We introduce a new corpus of speeches from campaign events in the months leading up to the 2016 U.S. presidential election and develop new models for predicting moments of audience applause.,abstractText,[0],[0]
"In contrast to existing datasets, we tackle the challenge of working with transcripts that derive from uncorrected closed captioning, using associated audio recordings to automatically extract and align labels for instances of audience applause.",abstractText,[0],[0]
"In prediction experiments, we find that lexical features carry the most information, but that a variety of features are predictive, including prosody, long-term contextual dependencies, and theoretically motivated features designed to capture rhetorical techniques.",abstractText,[0],[0]
Please Clap: Modeling Applause in Campaign Speeches,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1763–1775 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1763",text,[0],[0]
Computing the co-occurrence strength between two linguistic expressions is a fundamental task in natural language processing (NLP).,1 Introduction,[0],[0]
"For example, in collocation extraction (Manning and Schütze, 1999), word bigrams are collected from corpora and then strongly co-occurring bigrams (e.g., “New York”) are found.",1 Introduction,[0],[0]
"In dialogue response selection (Lowe et al., 2015), pairs comprising a context and its response sentence are collected from dialogue corpora and the goal is to rank the candidate responses for each given context sentence.",1 Introduction,[0],[0]
"In either case, a set of linguistic expression pairs D = {(xi, yi)}ni=1 is first collected and then the co-occurrence strength of a (new) pair (x, y) is computed.
",1 Introduction,[0],[0]
"Pointwise mutual information (PMI) (Church and Hanks, 1989) is frequently used to model the co-occurrence strength of linguistic expression pairs.",1 Introduction,[0],[0]
There are two typical types of PMI estimation (computation) method.,1 Introduction,[0],[0]
"One is a countingbased estimator using maximum likelihood estimation, sometimes with smoothing techniques, for example,
P̂MIMLE(x, y;D)= log n · c(x, y)∑ y′c(x, y ′) ∑ x′c(x ′, y) ,
(1)
where c(x, y) denotes the frequency of the pair (x, y) in given dataD. This is easy to compute and is commonly used to measure co-occurrence between words, such as in collocation extraction1; however, when data D is sparse, i.e., when x or y is a phrase or sentence, this approach is unrealistic.",1 Introduction,[0],[0]
The second method uses recurrent neural networks (RNNs).,1 Introduction,[0],[0]
"Li et al. (2016) proposed to em1 In collocation extraction, simple counting c(x, y) ∝",1 Introduction,[0],[0]
"P̂(x, y), rather than PMI, ranks undesirable function-word pairs (e.g., “of the”) higher (Manning and Schütze, 1999).
",1 Introduction,[0],[0]
ploy PMI to suppress dull responses for utterance generation in dialogue systems2.,1 Introduction,[0],[0]
"They estimated P(y) and P(y|x) using RNN language models and estimated PMI as follows:
P̂MIRNN(x, y;D) = log P̂RNN(y|x) P̂RNN(y) .",1 Introduction,[0],[0]
"(2)
This way of estimating PMI is applicable to sparse language expressions; however, learning RNN language models is computationally costly.
",1 Introduction,[0],[0]
"To eliminate this trade-off between robustness to data sparsity and learning time, in this study we propose a new kernel-based co-occurrence measure, which we call the pointwise Hilbert–Schmidt independence criterion (PHSIC) (see Table 1).",1 Introduction,[0],[0]
"Our contributions are as follows: • We formalize PHSIC, which is derived from
HSIC (Gretton et al., 2005), a kernel-based dependence measure, in the same way that PMI is derived from mutual information (Section 3).",1 Introduction,[0],[0]
• We give an intuitive explanation why PHSIC is robust to data sparsity.,1 Introduction,[0],[0]
"PHSIC is a “smoothed variant of PMI”, which allows various similarity metrics to be plugged in as kernels (Section 4).",1 Introduction,[0],[0]
"• We propose fast estimators of PHSIC, which are reduced to a simple and fast matrix calculation regardless of whether we use linear or nonlinear kernels (Section 5).",1 Introduction,[0],[0]
"• We empirically confirmed the effectiveness of PHSIC, i.e., its robustness to data sparsity and learning time, in two different types of experiment, a dialogue response selection task and a data selection task for machine translation (Section 6).",1 Introduction,[0],[0]
"Let X and Y denote random variables on X and Y , respectively.",2 Problem Setting,[0],[0]
"In this paper, we deal with the tasks of taking a set of linguistic expression pairs
D = {(xi, yi)}ni=1 ∼i.i.d.",2 Problem Setting,[0],[0]
"PXY , (3)
which is regarded as a set of i.i.d.",2 Problem Setting,[0],[0]
"samples drawn from a joint distribution PXY , and then measuring the “co-occurrence strength” for each given pair (x, y) ∈ X × Y .",2 Problem Setting,[0],[0]
"Such tasks include collocation extraction and dialogue response selection (Section 1).
",2 Problem Setting,[0],[0]
2,2 Problem Setting,[0],[0]
"In dialogue response selection or generation, a simple conditional probability P̂(y|x), rather than PMI, ranks dull responses (e.g., “I don’t know.”) higher (Li et al., 2016).",2 Problem Setting,[0],[0]
"In this section, we give the formal definition of PHSIC, a new kernel-based co-occurrence measure.",3 Pointwise HSIC,[0],[0]
We show a summary of this section in Table 2.,3 Pointwise HSIC,[0],[0]
"Intuitively, PHSIC is a “kernelized variant of PMI.”",3 Pointwise HSIC,[0],[0]
"As a preliminary step, we introduce the simple concept of dependence (see Dependence Measure in Table 2).",3.1 Dependence Measure,[0],[0]
Recall that random variables X and Y are independent if and only if the joint probability density PXY and the product of the marginals PXPY are equivalent.,3.1 Dependence Measure,[0],[0]
"Therefore, we can measure the dependence between random variables X and Y via the difference between PXY and PXPY .
",3.1 Dependence Measure,[0],[0]
"Both the mutual information and the Hilbert– Schmidt independence criterion, to be described below, are such dependence measures.",3.1 Dependence Measure,[0],[0]
"We briefly review the well-known mutual information and PMI (see MI & PMI in Table 2).
",3.2 MI and PMI,[0],[0]
"The mutual information (MI)3 between two random variables X and Y is defined by
MI(X,Y ) := KL[PXY ‖PXPY ] (4)
(Cover and Thomas, 2006), where KL[·‖·] denotes the Kullback–Leibler (KL) divergence.",3.2 MI and PMI,[0],[0]
"Thus, MI(X,Y ) is the degree of dependence between X and Y measured by the KL divergence between PXY and PXPY .
",3.2 MI and PMI,[0],[0]
"Here, by definition of the KL divergence, MI can be represented in the form of the expectation over PXY , i.e., the summation over all possible pairs (x, y) ∈ X×Y:
MI(X,Y ) =",3.2 MI and PMI,[0],[0]
"E (x,y)
[ log PXY (x, y)
PX(x)PY",3.2 MI and PMI,[0],[0]
"(y)
] .",3.2 MI and PMI,[0],[0]
"(5)
The shaded part in Equation (5) is actually the pointwise mutual information (PMI) (Church and Hanks, 1989):
PMI(x, y;X,Y )",3.2 MI and PMI,[0],[0]
":= log PXY (x, y)
",3.2 MI and PMI,[0],[0]
PX(x)PY,3.2 MI and PMI,[0],[0]
(y) .,3.2 MI and PMI,[0],[0]
"(6)
Therefore, PMI(x, y) can be thought of as the contribution of (x, y) to MI(X,Y ).
3 Conventionally, mutual information is denoted by I(X;Y ); in this paper, however, for notational consistency, mutual information is denoted by MI(X,Y ).",3.2 MI and PMI,[0],[0]
"As seen in the previous section, PMI can be derived from MI.",3.3 HSIC and PHSIC,[0],[0]
"Here, we consider replacing MI with the Hilbert–Schmidt independence criterion (HSIC).",3.3 HSIC and PHSIC,[0],[0]
"Then, in analogy with the relationship between PMI and MI, we derive PHSIC from HSIC (see HSIC & PHSIC in Table 2).
",3.3 HSIC and PHSIC,[0],[0]
Let k :,3.3 HSIC and PHSIC,[0],[0]
"X × X → R and ` : Y × Y → R denote positive definite kernels on X and Y , respectively (intuitively, they are similarity functions between linguistic expressions).",3.3 HSIC and PHSIC,[0],[0]
"The Hilbert– Schmidt independence criterion (HSIC) (Gretton et al., 2005), a kernel-based dependence measure, is defined by
HSIC(X,Y; k, `) :=MMD2k,`[PXY ,PXPY ], (7)
where MMD[·, ·] denotes the maximum mean discrepancy (MMD) (Gretton et al., 2012), which measures the difference between random variables on a kernel-induced feature space.",3.3 HSIC and PHSIC,[0],[0]
"Thus, HSIC(X,Y ; k, `) is the degree of dependence between X and Y measured by the MMD between PXY and PXPY , while MI is measured by the KL divergence (Equation (4)).
",3.3 HSIC and PHSIC,[0],[0]
"Analogous to MI in Equation (5), HSIC can be represented in the form of the expectation on PXY by a simple deformation:
HSIC(X,Y ; k, `)
= E (x,y)
",3.3 HSIC and PHSIC,[0],[0]
"[ (φ(x)−mX)>CXY (ψ(y)−mY ) ] (8)
= E (x,y)
[ E (x′,y′)",3.3 HSIC and PHSIC,[0],[0]
"[k̃(x, x′)˜̀(y, y′)]",3.3 HSIC and PHSIC,[0],[0]
"], (9)
where
φ(x) := k(x, ·), ψ(y) := `(y, ·), (10)
mX := Ex[φ(x)], mY := Ey[ψ(y)], (11)
",3.3 HSIC and PHSIC,[0],[0]
"CXY := E (x,y)
",3.3 HSIC and PHSIC,[0],[0]
"[ (φ(x)−mX)(ψ(y)−mY )> ] , (12)
k̃(x, x′)",3.3 HSIC and PHSIC,[0],[0]
":= k(x, x′)−Ex′",3.3 HSIC and PHSIC,[0],[0]
"[k(x, x′)]",3.3 HSIC and PHSIC,[0],[0]
"−Ex[k(x, x′)]",3.3 HSIC and PHSIC,[0],[0]
"+Ex,x′ [k(x, x′)].",3.3 HSIC and PHSIC,[0],[0]
"(13)
At first glance, these equations are somewhat complicated; however, the estimators of PHSIC we actually use are reduced to a simple matrix calculation in Section 5.",3.3 HSIC and PHSIC,[0],[0]
"Unlike MI in Equation (5), HSIC has two representations: Equation (8) is the representation in feature space and Equation (9) is the representation in data space.
",3.3 HSIC and PHSIC,[0],[0]
"Similar to the relationship between MI and PMI (Section 3.2), we define the pointwise Hilbert– Schmidt independence criterion (PHSIC) by the shaded parts in Equations (8) and (9):
PHSIC(x, y;X,Y, k, `)
:= (φ(x)−mX)>CXY (ψ(y)−mY ) (14)
",3.3 HSIC and PHSIC,[0],[0]
"= E (x′,y′)",3.3 HSIC and PHSIC,[0],[0]
"[k̃(x, x′)˜̀(y, y′)] .",3.3 HSIC and PHSIC,[0],[0]
"(15) Namely, PHSIC(x, y) is defined as the contribution of (x, y) to HSIC(X,Y ).
",3.3 HSIC and PHSIC,[0],[0]
"In summary, we define PHSIC such that “MI:PMI = HSIC:PHSIC” holds (see Table 2).",3.3 HSIC and PHSIC,[0],[0]
"This section gives an intuitive explanation for the first feature of PHSIC, i.e., the robustness to data sparsity, using Table 3.",4 PHSIC as Smoothed PMI,[0],[0]
"In short, we show that PHSIC is a “smoothed variant of PMI.”
",4 PHSIC as Smoothed PMI,[0],[0]
"First, the maximum likelihood estimator of PMI
in Equation (1) can be rewritten as P̂MI(x, y;D)= log n ·",4 PHSIC as Smoothed PMI,[0],[0]
"∑
iI[x=xi ∧ y=yi]∑ iI[x=xi] ∑ iI[y=yi] , (16)
where I[condition] = 1 if the condition is true and I[condition] = 0 otherwise.",4 PHSIC as Smoothed PMI,[0],[0]
"According to Equation (16), P̂MI(x, y) is the amount computed by repeating the following operation (see the first row in Table 3):
collate the given (x, y) and the observed (xi, yi) in D in order, and add the scores if (x, y) and (xi, yi) match exactly or deduct the scores if either the x side or the y side (but nor both) matches.
",4 PHSIC as Smoothed PMI,[0],[0]
"Moreover, an estimator of PHSIC in data space (Equation (15)) is
P̂HSIC(x, y;D, k, `)=",4 PHSIC as Smoothed PMI,[0],[0]
"1n ∑ i ̂̃ k(x, xi)̂˜̀(y, yi) ,
(17)
where ̂̃k(·, ·) and ̂̀̃(·, ·) are similarity functions centered on the data4.",4 PHSIC as Smoothed PMI,[0],[0]
"According to Equation (17), P̂HSIC(x, y) is the amount computed by repeating the following operation (see the second row in Table 3):
collate the given (x, y) and the observed (xi, yi) in D in order, and add the scores if the similarities on the x and y sides are both
higher (both ̂̃k(x, xi) > 0 and ̂̀̃(y, yi) > 0",4 PHSIC as Smoothed PMI,[0],[0]
"hold)5 or deduct the scores if the similarities on either the x or y sides are similar but those on the other side are not similar.
",4 PHSIC as Smoothed PMI,[0],[0]
"4 To be exact, ̂̃k(x, x′)",4 PHSIC as Smoothed PMI,[0],[0]
":= k(x, x′)",4 PHSIC as Smoothed PMI,[0],[0]
"− 1 n ∑n j=1 k(x, xj)",4 PHSIC as Smoothed PMI,[0],[0]
− 1 n ∑n i=1,4 PHSIC as Smoothed PMI,[0],[0]
"k(xi, x ′) + 1 n2 ∑n i=1",4 PHSIC as Smoothed PMI,[0],[0]
"∑n j=1 k(xi, xj), which is an estimator of the centered kernel k̃(x, x′) in Equation (13).",4 PHSIC as Smoothed PMI,[0],[0]
5,4 PHSIC as Smoothed PMI,[0],[0]
"In addition, the scores are added if the similarity on the x
side and that on the y side are both lower, that is, if ̂̃k(x, xi) < 0 and ̂̀̃(y, yi) < 0 hold.
",4 PHSIC as Smoothed PMI,[0],[0]
"As described above, when comparing the estimators of PMI and PHSIC from the viewpoint of “methods of matching the given (x, y) and the observed (xi, yi),” it is understood that PMI matches them in an exact manner, while PHSIC smooths the matching using kernels (similarity functions).
",4 PHSIC as Smoothed PMI,[0],[0]
"With this mechanism, even for completely unknown pairs, it is possible to estimate the cooccurrence strength by referring to observed pairs through the kernels.",4 PHSIC as Smoothed PMI,[0],[0]
"Therefore, PHSIC is expected to be robust to data sparsity and can be applied to phrases and sentences.
",4 PHSIC as Smoothed PMI,[0],[0]
"Available Kernels for PHSIC In NLP, a variety of similarity functions (i.e., positive definite kernels) are available.",4 PHSIC as Smoothed PMI,[0],[0]
"We can freely utilize such resources, such as cosine similarity between sentence embeddings.",4 PHSIC as Smoothed PMI,[0],[0]
"For a more detailed discussion, see Appendix A.",4 PHSIC as Smoothed PMI,[0],[0]
"Recall that we have two types of empirical estimator of PMI, the maximum likelihood estimator (Equation (1)) and the RNN-based estimator (Equation (2)).",5 Empirical Estimators of PHSIC,[0],[0]
"In this section, we describe how to rapidly estimate PHSIC from data.",5 Empirical Estimators of PHSIC,[0],[0]
"When using the linear kernel or cosine similarity (e.g., cosine similarity between sentence embeddings), PHSIC can be efficiently estimated in feature space (Section 5.1).",5 Empirical Estimators of PHSIC,[0],[0]
"When using a nonlinear kernel such as the Gaussian kernel, PHSIC can also be estimated efficiently in data space via a simple matrix decomposition (Section 5.2).",5 Empirical Estimators of PHSIC,[0],[0]
"When using the linear kernel or cosine similarity, the estimator of PHSIC in feature space (14) is as follows:
P̂HSICfeature(x, y;D, k, `)
=",5.1 Estimation Using Linear Kernel or Cosine,[0],[0]
"(φ(x)−φ(x))>ĈXY (ψ(y)−ψ(y)) , (18)
where
φ(x) = { x (k(x, x′) = x>x′)",5.1 Estimation Using Linear Kernel or Cosine,[0],[0]
"x/‖x‖ (k(x, x′) = cos(x, x′)) , (19)
φ(x) := 1
n n∑ i=1 φ(xi), ψ(y)",5.1 Estimation Using Linear Kernel or Cosine,[0],[0]
":= 1 n n∑ i=1 ψ(yi), (20)
",5.1 Estimation Using Linear Kernel or Cosine,[0],[0]
"ĈXY := 1
n n∑ i=1 φ(xi)ψ(yi)",5.1 Estimation Using Linear Kernel or Cosine,[0],[0]
>,5.1 Estimation Using Linear Kernel or Cosine,[0],[0]
− φ(x)ψ(y)>.,5.1 Estimation Using Linear Kernel or Cosine,[0],[0]
"(21)
Generally in kernel methods, a feature map φ(·) induced by a kernel k(·, ·) is unknown or highdimensional and it is difficult to compute estimated values in feature space6.",5.1 Estimation Using Linear Kernel or Cosine,[0],[0]
"However, when we use the linear kernel or cosine similarity, feature maps can be explicitly determined (Equation (19)).
",5.1 Estimation Using Linear Kernel or Cosine,[0],[0]
"Computational Cost When learning Equation (18) with feature maps φ : X → Rd and ψ : Y → Rd, computing the vectors φ(x), ψ(y) ∈ Rd and the matrix ĈXY ∈ Rd×d takes O(nd2) time and O(nd) space (linear in the size of the input, n).",5.1 Estimation Using Linear Kernel or Cosine,[0],[0]
"When estimating PHSIC(x, y), computing φ(x), ψ(y) ∈ Rd and Equation (18) takes O(d2) time (constant; does not depend on the size of the input, n).",5.1 Estimation Using Linear Kernel or Cosine,[0],[0]
"When using a nonlinear kernel such as the Gaussian kernel, it is necessary to estimate PHSIC in data space.",5.2 Estimation Using Nonlinear Kernels,[0],[0]
"Using a simple matrix decomposition, this can be achieved with the same computational cost as the estimation in feature space.",5.2 Estimation Using Nonlinear Kernels,[0],[0]
See Appendix B for a detailed derivation.,5.2 Estimation Using Nonlinear Kernels,[0],[0]
"In this section, we provide empirical evidence for the greater effectiveness of PHSIC than PMI, i.e., a very short learning time and robustness to data sparsity.",6 Experiments,[0],[0]
"Among the many potential applications of PHSIC, we choose two fundamental scenarios, (re-)ranking/classification and data selection.",6 Experiments,[0],[0]
"• In the ranking/classification scenario (measuring
the co-occurrence strength of new data pairs with reference to observed pairs), PHSIC is applied
6 One of the characteristics of kernel methods is that an intractable estimation in feature space is replaced with an efficient estimation in data space.
as a criterion for the dialogue response selection task (Section 6.2).",6 Experiments,[0],[0]
•,6 Experiments,[0],[0]
"In the data selection/filtering scenario (ordering the entire set of observed data pairs according to the co-occurrence strength), PHSIC is also applied as a criterion for data selection in the context of machine translation (Section 6.3).",6 Experiments,[0],[0]
"To take advantage of recent developments in representation learning, we used several pre-trained models for encoding sentences into vectors and several kernels between these vectors for PHSIC.
Encoders",6.1 PHSIC Settings,[0],[0]
"As sentence encorders, we used two pre-trained models without fine-tuning.",6.1 PHSIC Settings,[0],[0]
"First, the sum of the word vectors effectively represents a sentence (Mikolov et al., 2013a):
x= ∑ w∈xvec(w), y= ∑ w∈yvec(w).",6.1 PHSIC Settings,[0],[0]
"(22)
For vec(·), we used the pre-trained fastText model7, which is a high-accuracy and popular word embedding model (Bojanowski et al., 2017); models in 157 languages are publicly distributed (Grave et al., 2018).",6.1 PHSIC Settings,[0],[0]
"Second, we also used a DNN-based sentence encoder, called the universal sentence encoder (Cer et al., 2018), which utilizes the deep averaging network (DAN) (Iyyer et al., 2015).",6.1 PHSIC Settings,[0],[0]
"The pre-trained model for English sentences we used is publicly available8.
Kernels",6.1 PHSIC Settings,[0],[0]
"As kernels between these vectors, we used cosine similarity (cos)
k(x,x′) = cos(x,x′) (23)
and the Gaussian kernel (also known as the radial basis function kernel; RBF kernel)
k(x,x′) = exp ( −‖x− x
′‖22 2σ2
) , (24)
and similarly for `(y,y′).",6.1 PHSIC Settings,[0],[0]
"The experiments are ran with hyperparameter σ = 1.0 for the RBF kernel, and d = 100 for incomplete Cholesky decomposition (for more detail, see Section B).",6.1 PHSIC Settings,[0],[0]
"In the first experiment, we applied PHSIC as a ranking criterion of the task of dialogue response 7 https://fasttext.cc/docs/en/english-vectors.",6.2 Ranking: Dialogue Response Selection,[0],[0]
"html, https://fasttext.cc/docs/en/crawl-vectors.",6.2 Ranking: Dialogue Response Selection,[0],[0]
"html 8 https://www.tensorflow.org/hub/modules/google/ universal-sentence-encoder/1
selection (Lowe et al., 2015); in the task, pairs comprising a context (previous utterance sequence) and its response are collected from dialogue corpora and the goal is to rank the candidate responses for each given context sentence.
",6.2 Ranking: Dialogue Response Selection,[0],[0]
"The task entails sentence sequences (very sparse linguistic expressions); moreover, Li et al. (2016) pointed out that (RNN-based) PMI has a positive impact on suppressing dull responses (e.g., “I don’t know.”) in dialogue systems.",6.2 Ranking: Dialogue Response Selection,[0],[0]
"Therefore, PHSIC, another co-occurrence measure, is also expected to be effective for this.",6.2 Ranking: Dialogue Response Selection,[0],[0]
"With this setting, where the validity of PMI is confirmed, we investigate whether PHSIC can replace RNN-based PMI in terms of both learning time and robustness to data sparsity.
",6.2 Ranking: Dialogue Response Selection,[0],[0]
"Experimental Settings
Dataset For the training data, we gathered approximately 5× 105 reply chains from Twitter, following Sordoni et al. (2015)9.",6.2 Ranking: Dialogue Response Selection,[0],[0]
"In addition, we randomly selected {103, 104, 105} reply chains from that dataset.",6.2 Ranking: Dialogue Response Selection,[0],[0]
"Using these small subsets, we confirmed the effect of the difference in the size of the training set (data sparseness) on the learning time and predictive performance.
",6.2 Ranking: Dialogue Response Selection,[0],[0]
"For validation and test data, we used a small (approximately 2000 pairs each) but highly reliable dataset created by Sordoni et al. (2015)10, which consists only of conversations given high scores by human annotators.",6.2 Ranking: Dialogue Response Selection,[0],[0]
"Therefore, this set was not expected to include dull responses.
",6.2 Ranking: Dialogue Response Selection,[0],[0]
"For each dataset, we converted each contextmessage-response triple into a context-response pair by concatenating the context and message following Li et al. (2016).",6.2 Ranking: Dialogue Response Selection,[0],[0]
"In addition, to convert the test set (positive examples) to ten-choice multiplechoice questions, we shuffled the combinations of context and response to generate pseudo-negative examples.
",6.2 Ranking: Dialogue Response Selection,[0],[0]
Evaluation Metrics,6.2 Ranking: Dialogue Response Selection,[0],[0]
"We adopted the following evaluation metrics for the task: (i) ROC-AUC (the area under the receiver operating characteristic curve), (ii) MRR (the mean reciprocal rank), and (iii) Recall@{1,2}.
9",6.2 Ranking: Dialogue Response Selection,[0],[0]
"We collected tweets after 2017 for our training set to avoid duplication with the test set, which contains tweets from the year 2012.",6.2 Ranking: Dialogue Response Selection,[0],[0]
10 https://www.microsoft.com/en-us/download/,6.2 Ranking: Dialogue Response Selection,[0],[0]
"details.aspx?id=52375
Experimental Procedure",6.2 Ranking: Dialogue Response Selection,[0],[0]
"We used the following procedure: (i) train the model with a set of context-response pairs D = {(xi, yi)}ni=1; (ii) for each context sentence x in the test data, rank the candidate responses {yj}10j=1 by the model; and (iii) report three evaluation metrics.
",6.2 Ranking: Dialogue Response Selection,[0],[0]
"Baseline Measures As baseline measures, both (1) an RNN language model P̂RNN(y) (Mikolov et al., 2010) and (2) a conditional RNN language model P̂RNN(y|x) (Sutskever et al., 2014) were trained, and (3) PMI based on these language models, RNN-PMI, was also used for experiments (see Equation (2)).",6.2 Ranking: Dialogue Response Selection,[0],[0]
"We trained these models with all combinations of the following settings: (a) the number of dimensions of the hidden layers being 300 or 1200 and (b) the initialization of the embedding layer being random (uniform on [−0.1, 0.1]) or fastText.",6.2 Ranking: Dialogue Response Selection,[0],[0]
"For more detailed settings, see Appendix C.
Experimental Results Learning Time Table 4 shows the experimental results of the learning time11.",6.2 Ranking: Dialogue Response Selection,[0],[0]
"Regardless of the size of the training set n, the learning time for
11 The computing environment was as follows: (i) CPU: Xeon E5-1650-v3 (3.5 GHz, 6 Cores); (ii) GPU: GTX 1080 (8 GB).
",6.2 Ranking: Dialogue Response Selection,[0],[0]
PHSIC is much shorter than that of the RNN-based method.,6.2 Ranking: Dialogue Response Selection,[0],[0]
"For example, even when the size of the training set n is 5× 105, PHSIC is approximately 1400–4000 times faster than RNN-based PMI.",6.2 Ranking: Dialogue Response Selection,[0],[0]
"This is because the estimators of PHSIC are reduced to a deterministic and efficient matrix calculation (Section 5), whereas neural network-based models involve the sequential optimization of parameters via gradient descent methods.
",6.2 Ranking: Dialogue Response Selection,[0],[0]
Robustness to Data Sparsity Table 5 shows the experimental results of the predictive performance.,6.2 Ranking: Dialogue Response Selection,[0],[0]
"When the size of the training data is small (n=103, 104), that is, when the data is extremely sparse, the predictive performance of PHSIC hardly deteriorates while that of PMI rapidly decays as the number of data decreases.",6.2 Ranking: Dialogue Response Selection,[0],[0]
This indicates that PHSIC is more robust to data sparsity than RNN-based PMI owing to the effect of kernels.,6.2 Ranking: Dialogue Response Selection,[0],[0]
"Moreover, PHSIC with the simple cosine kernel outperforms the RNN-based model regardless of the number of data, while the learning time of PHSIC is thousands of times shorter than those of the baseline methods (Section 6.2).
",6.2 Ranking: Dialogue Response Selection,[0],[0]
Additionally we report Spearman’s rank correlation coefficient between models to verify whether PHSIC shows similar behavior to PMI.,6.2 Ranking: Dialogue Response Selection,[0],[0]
See Appendix D for more detail.,6.2 Ranking: Dialogue Response Selection,[0],[0]
The aim of our second experiment was to demonstrate that PHSIC is also beneficial as a criterion of data selection.,6.3 Data Selection for Machine Translation,[0],[0]
"To achieve this, we attempted to apply PHSIC to a parallel corpus filtering task that has been intensively discussed in recent (neural) machine translation (MT, NMT) studies.",6.3 Data Selection for Machine Translation,[0],[0]
"This task was first adopted as a shared task in the third conference on machine translation (WMT 2018)12.
",6.3 Data Selection for Machine Translation,[0],[0]
"Several existing parallel corpora, especially those automatically gathered from large-scale text data, such as the Web, contain unacceptable amounts of noisy (low-quality) sentence pairs that greatly affect the translation quality.",6.3 Data Selection for Machine Translation,[0],[0]
"Therefore, the development of an effective method for parallel corpus filtering would potentially have a large influence on the MT community; discarding such noisy pairs may improve the translation quality and shorten the training time.
",6.3 Data Selection for Machine Translation,[0],[0]
"We expect PHSIC to give low scores to exceptional sentence pairs (misalignments or missing 12 http://www.statmt.org/wmt18/ parallel-corpus-filtering.html
translations) during the selection process because PHSIC assigns low scores to pairs that are highly inconsistent with other pairs (see Section 4).",6.3 Data Selection for Machine Translation,[0],[0]
"Note that applying RNN-based PMI to a parallel corpus selection task is unprofitable since obtaining RNNbased PMI also has an identical computational cost for training a sequence-to-sequence model for MT, and thus, we cannot expect a reduction of the total training time.
",6.3 Data Selection for Machine Translation,[0],[0]
"Experimental Settings
Dataset",6.3 Data Selection for Machine Translation,[0],[0]
"We used the ASPEC-JE corpus13, which is an official dataset used for the MT-evaluation shared task held in the fourth workshop on Asian translation (WAT 2017)14 (Nakazawa et al., 2017).",6.3 Data Selection for Machine Translation,[0],[0]
ASPEC-JE consists of approximately three million (3M) Japanese–English parallel sentences from scientific paper abstracts.,6.3 Data Selection for Machine Translation,[0],[0]
"As discussed by Kocmi et al. (2017), ASPEC-JE contains many low-quality parallel sentences that have the potential to significantly degrade the MT quality.",6.3 Data Selection for Machine Translation,[0],[0]
"In fact, they empirically revealed that using only the reliable part of the training parallel corpus significantly improved the translation quality.",6.3 Data Selection for Machine Translation,[0],[0]
"Therefore, ASPEC-JE is a suitable dataset for evaluating the data selection ability.
",6.3 Data Selection for Machine Translation,[0],[0]
"Model For our data selection evaluation, we selected the Transformer architecture (Vaswani et al., 2017) as our baseline NMT model, which is widelyused in the NMT community and known as one of the current state-of-the-art architectures.",6.3 Data Selection for Machine Translation,[0],[0]
"We utilized fairseq15, a publicly available tool for neural sequence-to-sequence models, for building our models.
",6.3 Data Selection for Machine Translation,[0],[0]
"Experimental Procedure We used the following procedure for this evaluation: (1) rank all parallel sentences in a given parallel corpus according to each criterion, (2) extract the top K ranked parallel sentences, (3) train the NMT model using the extracted parallel sentences, and (4) evaluate the translation quality of the test data using a typical MT automatic evaluation measure, i.e., BLEU (Papineni et al., 2002)16.",6.3 Data Selection for Machine Translation,[0],[0]
In our experiments we evaluated PHSIC with K = 0.5M and 1M.,6.3 Data Selection for Machine Translation,[0],[0]
"16 We used multi-bleu.perl in the Moses tool (https:// github.com/moses-smt/mosesdecoder).
",15 https://github.com/pytorch/fairseq,[0],[0]
"Baseline Measure As a baseline measure, we utilize a publicly available script17 of fast align (Dyer et al., 2013), which is one of the state-of-theart word aligner.",15 https://github.com/pytorch/fairseq,[0],[0]
"We firstly used the fast align for the training set D = {(xi, yi)}i to obtain the word alignment between each sentence pair (xi, yi), i.e., a set of aligned word pairs with its probabilities.",15 https://github.com/pytorch/fairseq,[0],[0]
"We then computed the co-occurrence score of (xi, yi) with sentence-length normalization, i.e., the average log probability of aligned word pairs.
",15 https://github.com/pytorch/fairseq,[0],[0]
Experimental Results Table 6 shows the results of our data selection evaluation.,15 https://github.com/pytorch/fairseq,[0],[0]
It is common knowledge in NMT that more data gives better performance in general.,15 https://github.com/pytorch/fairseq,[0],[0]
"However, we observed that PHSIC successfully extracted beneficial parallel sentences from the noisy parallel 17 https://github.com/clab/fast align
corpus; the result using 1M data extracted from the 3M corpus by PHSIC was almost the same as that using 3M data (the decrease in the BLEU score was only 0.07), whereas that by random extraction reduced the BLEU score by 1.20.
",15 https://github.com/pytorch/fairseq,[0],[0]
This was actually a surprising result because PHSIC utilizes only monolingual similarity measures (kernels) without any other language resources.,15 https://github.com/pytorch/fairseq,[0],[0]
This indicates that PHSIC can be applied to a language pair poor in parallel resources.,15 https://github.com/pytorch/fairseq,[0],[0]
"In addition, the surface form and grammatical characteristics between English and Japanese are extremely different18; therefore, we expect that PHSIC will work well regardless of the similarity of the language pair.",15 https://github.com/pytorch/fairseq,[0],[0]
"Dependence Measures Measuring independence or dependence (correlation) between two random variables, i.e., estimating dependence from a set of paired data, is a fundamental task in statistics and a very wide area of data science.",7 Related Work,[0],[0]
"To measure the complex nonlinear dependence that real data has, we have several choices.
",7 Related Work,[0],[0]
"First, information-theoretic MI (Cover and Thomas, 2006) and its variants (Suzuki et al., 2009; Reshef et al., 2011) are the most commonly used dependence measures.",7 Related Work,[0],[0]
"However, to the best of our knowledge, there is no practical method of computing MIs for large-multi class high-dimensional 18",7 Related Work,[0],[0]
"For example, word order; English is an SVO (subject-verbobject) language and Japanese is an SOV (subject-object-verb) language.
",7 Related Work,[0],[0]
"(having a complex generative model) discrete data, such as sparse linguistic data.
",7 Related Work,[0],[0]
"Second, several kernel-based dependence measures have been proposed for measuring nonlinear dependence (Akaho, 2001; Bach and Jordan, 2002; Gretton et al., 2005).",7 Related Work,[0],[0]
"The reason why kernelbased dependence measures work well for real data is that they do not explicitly estimate densities, which is difficult for high-dimensional data.",7 Related Work,[0],[0]
"Among them, HSIC (Gretton et al., 2005) is popular because it has a simple estimation method, which is used for various tasks such as feature selection (Song et al., 2012), dimensionality reduction (Fukumizu et al., 2009), and",7 Related Work,[0],[0]
"unsupervised object matching (Quadrianto et al., 2009; Jagarlamudi et al., 2010).",7 Related Work,[0],[0]
"We follow this line.
",7 Related Work,[0],[0]
"Co-occurrence Measures First, In NLP, PMI (Church and Hanks, 1989) and its variants (Bouma, 2009) are the de facto co-occurrence measures between dense linguistic expressions, such as words (Bouma, 2009) and simple narrative-event expressions (Chambers and Jurafsky, 2008).",7 Related Work,[0],[0]
"In recent years, positive PMI (PPMI) has played an important role as a component of word vectors (Levy and Goldberg, 2014).
",7 Related Work,[0],[0]
"Second, there are several studies in which the pairwise ranking problem has been solved by using deep neural networks (DNNs) in NLP.",7 Related Work,[0],[0]
Li et al. (2016) proposed a PMI estimation using RNN language models; this was used as a baseline model in our experiments (see Section 6.2).,7 Related Work,[0],[0]
"Several studies have used DNN-based binary classifiers modeling P(C = positive | (x, y)) to solve the given ranking problem directly (Hu et al., 2014; Yin et al., 2016; Mueller and Thyagarajan, 2016) (these networks are sometimes called Siamese neural networks).",7 Related Work,[0],[0]
Our study focuses on comparing co-occurrence measures.,7 Related Work,[0],[0]
"It is unknown whether Siamese NNs capture the co-occurrence strength; therefore we did not deal with Siamese NNs in this paper.
",7 Related Work,[0],[0]
"Finally, to the best of our knowledge, Yokoi et al. (2017)’s paper is the first study that suggested converting HSIC to a pointwise measure.",7 Related Work,[0],[0]
"The present study was inspired by their suggestion; here, we have (i) provided a formal definition (population) of PHSIC; (ii) analyzed the relationship between PHSIC and PMI; (iii) proposed linear-time estimation methods; and (iv) experimentally verified the computation speed and robustness to data sparsity of PHSIC for practical applications.",7 Related Work,[0],[0]
"The NLP community has commonly employed PMI to estimate the co-occurrence strength between linguistic expressions; however, existing PMI estimators have a high computational cost when applied to sparse linguistic expressions (Section 1).",8 Conclusion,[0],[0]
"We proposed a new kernel-based co-occurrence measure, the pointwise Hilbert– Schmidt independent criterion (PHSIC).",8 Conclusion,[0],[0]
"As well as defining PMI as the contribution to mutual information, PHSIC is defined as the contribution to HSIC; PHSIC is intuitively a “kernelized variant of PMI” (Section 3).",8 Conclusion,[0],[0]
PHSIC can be applied to sparse linguistic expressions owing to the mechanism of smoothing by kernels.,8 Conclusion,[0],[0]
"Comparing the estimators of PMI and PHSIC, PHSIC can be interpreted as a smoothed variant of PMI, which allows various similarity metrics to be plugged in as kernels (Section 4).",8 Conclusion,[0],[0]
"In addition, PHSIC can be estimated in linear time owing to the efficient matrix calculation, regardless of whether we use linear or nonlinear kernels (Section 5).",8 Conclusion,[0],[0]
We conducted a ranking task for dialogue systems and a data selection task for machine translation (Section 6).,8 Conclusion,[0],[0]
"The experimental results show that (i) the learning of PHSIC was completed thousands of times faster than that of the RNN-based PMI while outperforming it in ranking accuracy (Section 6.2); and (ii) even when using a nonlinear kernel, PHSIC can be applied to a large dataset.",8 Conclusion,[0],[0]
"Moreover, PHSIC reduces the amount of training data to one third without sacrificing the output translation quality (Section 6.3).
",8 Conclusion,[0],[0]
"Future Work Using the PHSIC estimator in feature space (Equation (18)), we can generate the most appropriate ψ(y) for a given φ(x) (uniquely, up to scale).",8 Conclusion,[0],[0]
"That is, if a DNN-based sentence decoder is used, y (a sentence) can be restored from ψ(y) (a feature vector) so that generative models of strong co-occurring sentences can be realized.",8 Conclusion,[0],[0]
We are grateful to anonymous reviewers for their helpful comments.,Acknowledgments,[0],[0]
"We also thank Weihua Hu for useful discussions, Kenshi Yamaguchi for collecting data, and Paul Reisert for proofreading.",Acknowledgments,[0],[0]
"This work was supported in part by JSPS KAKENHI Grant Number JP15H01702 and JST CREST Grant Number JPMJCR1513, Japan.",Acknowledgments,[0],[0]
"Similarity between Sentence Vectors A variety of vector representations of phrases and sentences based on the distributional hypothesis have recently been proposed, including sentence encoders (Kiros et al., 2015; Dai and Le, 2015; Iyyer et al., 2015; Hill et al., 2016; Cer et al., 2018) and the sum of word embeddings; it is known as additive compositionality (Mitchell and Lapata, 2010; Mikolov et al., 2013a; Wieting et al., 2015) that we can express the meaning of phrases and sentences well with the sum of word vectors (e.g., word2vec (Mikolov et al., 2013b), GloVe (Pennington et al., 2014), and fastText (Bojanowski et al., 2017)).",A Available Kernels for PHSIC,[0],[0]
"Note that various pre-trained models of sentence encoders and word embeddings have also been made available.
",A Available Kernels for PHSIC,[0],[0]
"The cosine of these vectors, which is a positive definite kernel, can be used as a convenient and highly accurate similarity function between phrases or sentences.",A Available Kernels for PHSIC,[0],[0]
"Other major kernels can also be used, such as the RBF kernel, the Laplacian kernel, and polynomial kernels.
",A Available Kernels for PHSIC,[0],[0]
"Structured Kernels Various structured kernels for NLP, such as tree kernels, which capture fine structure of sentences such as syntax, were devised in the support vector machine era (Collins and Duffy, 2002; Bunescu and Mooney, 2006; Moschitti, 2006).
",A Available Kernels for PHSIC,[0],[0]
"Combinations We can freely combine the previously mentioned kernels because the sum and the product of positive definite kernels are also positive definite kernels (Shawe-Taylor and Cristianini, 2004, Proposition 3.22).",A Available Kernels for PHSIC,[0],[0]
"Although estimators of HSIC and PHSIC depend on kernels k, ` and data D, hereinafter, we use the following notation for the sake of simplicity:
ĤSIC(X,Y ) := ĤSIC(X,Y ;D, k, `), (25)
P̂HSIC(x, y) := P̂HSIC(x, y;D, k, `).",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"(26)
Naı̈ve Estimation Fist, an estimator of PHSIC in the data space (15) is
P̂HSICkernel(x, y)=(k",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
− k)>,B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"( 1nH)(`− `), (27)
where k := (k(x, x1), . . .",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
", k(x, xn))",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"> ∈ Rn, so as `; and vector k := 1nK1 denotes empirical mean of {ki}ni=1, so as `.",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"This estimation has a
large computational cost.",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"When learning, computing the vectors k, ` takes O(n2) time and O(n) space.",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"When estimating PHSIC, computing k, ` and multiplying the matrix 1nH takes O(n) time.
",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
Fast Estimation via Incomplete Cholesky Decomposition Equation (27) has a large computational cost because it is necessary to construct the Gram matrices K and L ∈ Rn×n.,B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"In kernel methods, several methods have been proposed for approximating Gram matrices at low cost without constructing them explicitly, such as incomplete Cholesky decomposition (Fine and Scheinberg, 2001).
",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"By incomplete Cholesky decomposition, from data points {x1, . . .",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
", xn} ⊆ X and a positive definite kernel",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
k,B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
:,B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"X × X → R, a matrix A = (a1, . . .",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
",an)
> ∈ Rn×d (d n) can be obtained with O(nd2) time complexity.",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"This makes it possible to approximate the Gram matrix K by vectors ai ∈ Rd without configuring the entire of K:
a>",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"i aj ≈ k(xi, xj) (",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
28) AA>,B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"≈ K. (29)
",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"Also, for HSIC, an efficient approximation method utilizing incomplete Cholesky decomposition has been proposed (Gretton et al., 2005, Lemma 2):
ĤSICICD(X,Y ) = 1 n2 ‖(HA)>B‖2F, (30)
where A = (a1, . . .",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
",an)> ∈ Rn×d is a matrix satisfying AA>",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"≈ K computed via incomplete Cholesky decomposition, so as B (BB> ≈ L).",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"Equation (30) can be represented in the form of the expectation on data points:
ĤSICICD(X,Y )= 1
n n∑ i=1",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
[ (ai−a)>ĈICD(bi−b) ],B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"(31)
ĈICD :",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
= 1 n,B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"(HA)>B ∈ Rd×d, (32)
where vector a := 1nA >",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"1 ∈ Rd denotes empirical mean of {ai}ni=1, so as b := 1nB >1.
",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"Recall that PHSIC(x, y) is the contribution of (x, y) to HSIC(X,Y ) (see Section 3.3); PHSIC then can be efficiently estimated by the shaded part of Equation (31):
P̂HSICICD(x, y)= (a−a)>ĈICD(b−b) .",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"(33)
Here, the vector a ∈ Rd corresponding to the new x can be calculated by “performing from halfway”
on the incomplete Cholesky decomposition algorithm.",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"Let x(1), . . .",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
", x(d) denote the dominant xis adopted during decomposition algorithm.",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"The jth element of a can be computed as follows:
a[j]= [ k(x, x(j))− j−1∑ m=1 a[m]Ajm ] /Ajj , (34)
so as b ∈ Rd corresponding to the new y.",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
The estimation via incomplete Cholesky decomposition (33) is extremely efficient compared to the naive estimation (27); Equation (33)’s computational complexity is equivalent to the estimation in the feature space (18).,B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
Detailed settings for learning RNNs used in this research are as follows.,C Detailed Settings for Learning RNNs,[0],[0]
"• Hidden layers: single layer LSTMs (Hochreiter
and Schmidhuber, 1997) •",C Detailed Settings for Learning RNNs,[0],[0]
"Vocabulary: words with a frequency: 10 or more
(n = 5× 105), 2 or more (otherwise) •",C Detailed Settings for Learning RNNs,[0],[0]
"Dropout rate: 0.1 (300-dim), 0.3 (1200-dim) •",C Detailed Settings for Learning RNNs,[0],[0]
"Batch size: 64 • Max epoch number: 5 (n = 5× 105), 30 (other-
wise) • Deep learning framework: Chainer (Tokui et al.,
2015)",C Detailed Settings for Learning RNNs,[0],[0]
Table D shows Spearman’s rank correlation coefficient (Spearman’s ρ) between the co-occurrence scores on the test set computed by the models in the dialogue response selection task (Section 6.2).,D Correlation Between Models in Dialogue Response Selection Task,[0],[0]
"This shows that the behavior of RNN-based PMI and
PHSIC are considerably different.",D Correlation Between Models in Dialogue Response Selection Task,[0],[0]
"Furthermore, interestingly, the behavior of PHSICs using different kernels is also different.",D Correlation Between Models in Dialogue Response Selection Task,[0],[0]
Possible reasons for these observations are as follows: (1) the difference in the dependence measures (MI or HSIC) on which each model is based; (2) the validity or numerical stability of estimating PMI with RNN language models; and (3) differences in the behavior of PHSIC originating from differences in the plugged in kernels.,D Correlation Between Models in Dialogue Response Selection Task,[0],[0]
A more detailed analysis of the compatibility between tasks and measures (or kernels) is attractive future work.,D Correlation Between Models in Dialogue Response Selection Task,[0],[0]
"In this paper, we propose a new kernel-based co-occurrence measure that can be applied to sparse linguistic expressions (e.g., sentences) with a very short learning time, as an alternative to pointwise mutual information (PMI).",abstractText,[0],[0]
"As well as deriving PMI from mutual information, we derive this new measure from the Hilbert–Schmidt independence criterion (HSIC); thus, we call the new measure the pointwise HSIC (PHSIC).",abstractText,[0],[0]
"PHSIC can be interpreted as a smoothed variant of PMI that allows various similarity metrics (e.g., sentence embeddings) to be plugged in as kernels.",abstractText,[0],[0]
"Moreover, PHSIC can be estimated by simple and fast (linear in the size of the data) matrix calculations regardless of whether we use linear or nonlinear kernels.",abstractText,[0],[0]
"Empirically, in a dialogue response selection task, PHSIC is learned thousands of times faster than an RNNbased PMI while outperforming PMI in accuracy.",abstractText,[0],[0]
"In addition, we also demonstrate that PHSIC is beneficial as a criterion of a data selection task for machine translation owing to its ability to give high (low) scores to a consistent (inconsistent) pair with other pairs.",abstractText,[0],[0]
Pointwise HSIC: A Linear-Time Kernelized Co-occurrence Norm for Sparse Linguistic Expressions,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 469–476 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
469",text,[0],[0]
"Many recent state-of-the-art models for constituency parsing are transition based, decomposing production of each parse tree into a sequence of action decisions (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017; Stern et al., 2017), building on a long line of work in transition-based parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Henderson, 2004; Zhang and Clark, 2011; Chen and Manning, 2014; Andor et al., 2016; Kiperwasser and Goldberg, 2016).
",1 Introduction,[0],[0]
"However, models of this type, which decompose structure prediction into sequential decisions, can be prone to two issues (Ranzato et al., 2016; Wiseman and Rush, 2016).",1 Introduction,[0],[0]
"The first is exposure bias: if, at training time, the model only observes
states resulting from correct past decisions, it will not be prepared to recover from its own mistakes during prediction.",1 Introduction,[0],[0]
"Second is the loss mismatch between the action-level loss used at training and any structure-level evaluation metric, for example F1.
",1 Introduction,[0],[0]
"A large family of techniques address the exposure bias problem by allowing the model to make mistakes and explore incorrect states during training, supervising actions at the resulting states using an expert policy (Daumé III et al., 2009; Ross et al., 2011; Choi and Palmer, 2011; Chang et al., 2015); these expert policies are typically referred to as dynamic oracles in parsing (Goldberg and Nivre, 2012; Ballesteros et al., 2016).",1 Introduction,[0],[0]
"While dynamic oracles have produced substantial improvements in constituency parsing performance (Coavoux and Crabbé, 2016; Cross and Huang, 2016; Stern et al., 2017; González and Gómez-Rodrı́guez, 2018), they must be custom designed for each transition system.
",1 Introduction,[0],[0]
"To address the loss mismatch problem, another line of work has directly optimized for structurelevel cost functions (Goodman, 1996; Och, 2003).",1 Introduction,[0],[0]
"Recent methods applied to models that produce output sequentially commonly use policy gradient (Auli and Gao, 2014; Ranzato et al., 2016; Shen et al., 2016) or beam search (Xu et al., 2016; Wiseman and Rush, 2016; Edunov et al., 2017) at training time to minimize a structured cost.",1 Introduction,[0],[0]
"These methods also reduce exposure bias through exploration but do not require an expert policy for supervision.
",1 Introduction,[0],[0]
"In this work, we apply a simple policy gradient method to train four different state-of-theart transition-based constituency parsers to maximize expected F1.",1 Introduction,[0],[0]
"We compare against training with a dynamic oracle (both to supervise exploration and provide loss-augmentation) where one is available, including a novel dynamic oracle that we define for the top-down transition system of
Dyer et al. (2016).",1 Introduction,[0],[0]
"We find that while policy gradient usually outperforms standard likelihood training, it typically underperforms the dynamic oracle-based methods – which provide direct, model-aware supervision about which actions are best to take from arbitrary parser states.",1 Introduction,[0],[0]
"However, a substantial fraction of each dynamic oracle’s performance gain is often recovered using the model-agnostic policy gradient method.",1 Introduction,[0],[0]
"In the process, we obtain new state-of-the-art results for single-model discriminative transition-based parsers trained on the English PTB (92.6 F1), French Treebank (83.5 F1), and Penn Chinese Treebank Version 5.1 (87.0 F1).",1 Introduction,[0],[0]
"The transition-based parsers we use all decompose production of a parse tree y for a sentence x into a sequence of actions (a1, . . .",2 Models,[0],[0]
"aT ) and resulting states (s1, . . .",2 Models,[0],[0]
sT+1).,2 Models,[0],[0]
"Actions at are predicted sequentially, conditioned on a representation of the parser’s current state st and parameters θ:
p(y|x; θ) = T∏ t=1 p(at",2 Models,[0],[0]
"| st; θ) (1)
We investigate four parsers with varying transition systems and methods of encoding the current state and sentence: (1) the discriminative Recurrent Neural Network Grammars (RNNG) parser of Dyer et al. (2016), (2) the In-Order parser of Liu and Zhang (2017), (3) the Span-Based parser of Cross and Huang (2016), and (4) the Top-Down parser of Stern et al. (2017).1 We refer to the original papers for descriptions of the transition systems and model parameterizations.",2 Models,[0],[0]
"Likelihood training without exploration maximizes Eq. 1 for trees in the training corpus, but may be prone to exposure bias and loss mismatch (Section 1).",3 Training Procedures,[0],[0]
"Dynamic oracle methods are known to improve on this training procedure for a variety of parsers (Coavoux and Crabbé, 2016; Cross and Huang, 2016; Stern et al., 2017; González and Gómez-Rodrı́guez, 2018), supervising exploration
1Stern et al. (2017) trained their model using a nonprobabilistic, max-margin objective.",3 Training Procedures,[0],[0]
"For comparison to the other models and to allow training with policy gradient, we create a locally-normalized probabilistic variant of their model by applying a softmax function to the predicted scores for each action.
during training by providing the parser with the best action to take at each explored state.",3 Training Procedures,[0],[0]
We describe how policy gradient can be applied as an oracle-free alternative.,3 Training Procedures,[0],[0]
"We then compare to several variants of dynamic oracle training which focus on addressing exposure bias, loss mismatch, or both.",3 Training Procedures,[0],[0]
"Given an arbitrary cost function ∆ comparing structured outputs (e.g. negative labeled F1, for trees), we use the risk objective:
R(θ) =",3.1 Policy Gradient,[0],[0]
N∑ i=1,3.1 Policy Gradient,[0],[0]
"∑ y p(y | x(i); θ)∆(y,y(i))
which measures the model’s expected cost over possible outputs y for each of the training examples (x(1),y(1)), . . .",3.1 Policy Gradient,[0],[0]
", (x(N),y(N)).
",3.1 Policy Gradient,[0],[0]
"Minimizing a risk objective has a long history in structured prediction (Povey and Woodland, 2002; Smith and Eisner, 2006; Li and Eisner, 2009; Gimpel and Smith, 2010) but often relies on the cost function decomposing according to the output structure.",3.1 Policy Gradient,[0],[0]
"However, we can avoid any restrictions on the cost using reinforcement learning-style approaches (Xu et al., 2016; Shen et al., 2016; Edunov et al., 2017) where cost is ascribed to the entire output structure – albeit at the expense of introducing a potentially difficult credit assignment problem.
",3.1 Policy Gradient,[0],[0]
"The policy gradient method we apply is a simple variant of REINFORCE (Williams, 1992).",3.1 Policy Gradient,[0],[0]
"We perform mini-batch gradient descent on the gradient of the risk objective:
∇R(θ) =",3.1 Policy Gradient,[0],[0]
N∑ i=1,3.1 Policy Gradient,[0],[0]
"∑ y p(y|x(i))∆(y,y(i))∇ log p(y|x(i); θ)
",3.1 Policy Gradient,[0],[0]
≈ N∑ i=1,3.1 Policy Gradient,[0],[0]
"∑ y∈Y(x(i)) ∆(y,y(i))∇ log p(y|x(i); θ)
where Y(x(i)) is a set of k candidate trees obtained by sampling from the model’s distribution for sentence x(i).",3.1 Policy Gradient,[0],[0]
"We use negative labeled F1 for ∆.
To reduce the variance of the gradient estimates, we standardize ∆ using its running mean and standard deviation across all candidates used so far throughout training.",3.1 Policy Gradient,[0],[0]
"Following Shen et al. (2016), we also found better performance when including the gold tree y(i) in the set of k candidates Y(x(i)), and do so for all experiments reported here.2
2Including the gold tree in the set of candidates does bias",3.1 Policy Gradient,[0],[0]
"For a given parser state st, a dynamic oracle defines an action a∗(st) which should be taken to incrementally produce the best tree still reachable from that state.3
Dynamic oracles provide strong supervision for training with exploration, but require custom design for a given transition system.",3.2 Dynamic Oracle Supervision,[0],[0]
"Cross and Huang (2016) and Stern et al. (2017) defined optimal (with respect to F1) dynamic oracles for their respective transition systems, and below we define a novel dynamic oracle for the top-down system of RNNG.
",3.2 Dynamic Oracle Supervision,[0],[0]
"In RNNG, tree production occurs in a stackbased, top-down traversal which produces a leftto-right linearized representation of the tree using three actions: OPEN a labeled constituent (which fixes the constituent’s span to begin at the next word in the sentence which has not been shifted), SHIFT the next word in the sentence to add it to the current constituent, or CLOSE the current constituent (which fixes its span to end after the last word that has been shifted).",3.2 Dynamic Oracle Supervision,[0],[0]
"The parser stores opened constituents on the stack, and must therefore close them in the reverse of the order that they were opened.
",3.2 Dynamic Oracle Supervision,[0],[0]
"At a given parser state, our oracle does the following:
1.",3.2 Dynamic Oracle Supervision,[0],[0]
"If there are any open constituents on the stack which can be closed (i.e. have had a word shifted since being opened), check the topmost of these (the one that has been opened most recently).",3.2 Dynamic Oracle Supervision,[0],[0]
"If closing it would produce a constituent from the the gold tree that has not yet been produced (which is determined by the constituent’s label, span beginning position, and the number of words currently shifted), or if the constituent could not be closed at a later position in the sentence to produce a constituent in the gold tree, return CLOSE.
",3.2 Dynamic Oracle Supervision,[0],[0]
"the estimate of the risk objective’s gradient; however since in the parsing tasks we consider, the gold tree has constant and minimal cost, augmenting with the gold is equivalent to jointly optimizing the standard likelihood and risk objectives, using an adaptive scaling factor for each objective that is dependent on the cost for the trees that have been sampled from the model.",3.2 Dynamic Oracle Supervision,[0],[0]
"We found that including the gold candidate in this manner outperformed initial experiments that first trained a model using likelihood training and then fine-tuned using unbiased policy gradient.
",3.2 Dynamic Oracle Supervision,[0],[0]
"3More generally, an oracle can return a set of such actions that could be taken from the current state, but the oracles we use select a single canonical action.
2.",3.2 Dynamic Oracle Supervision,[0],[0]
"Otherwise, if there are constituents in the gold tree which have not yet been opened in the parser state, with span beginning at the next unshifted word, OPEN the outermost of these.
",3.2 Dynamic Oracle Supervision,[0],[0]
3.,3.2 Dynamic Oracle Supervision,[0],[0]
"Otherwise, SHIFT the next word.
",3.2 Dynamic Oracle Supervision,[0],[0]
"While we do not claim that this dynamic oracle is optimal with respect to F1, we find that it still helps substantially in supervising exploration (Section 5).
",3.2 Dynamic Oracle Supervision,[0],[0]
"Likelihood Training with Exploration Past work has differed on how to use dynamic oracles to guide exploration during oracle training (Ballesteros et al., 2016; Cross and Huang, 2016; Stern et al., 2017).",3.2 Dynamic Oracle Supervision,[0],[0]
"We use the same sample-based method of generating candidate sets Y as for policy gradient, which allows us to control the dynamic oracle and policy gradient methods to perform an equal amount of exploration.",3.2 Dynamic Oracle Supervision,[0],[0]
"Likelihood training with exploration then maximizes the sum of the log probabilities for the oracle actions for all states composing the candidate trees:
LE(θ) =",3.2 Dynamic Oracle Supervision,[0],[0]
N∑ i=1,3.2 Dynamic Oracle Supervision,[0],[0]
"∑ y∈Y(x(i)) ∑ s∈y log p(a∗(s) | s)
where a∗(s) is the dynamic oracle’s action for state s.
Softmax Margin Softmax margin loss (Gimpel and Smith, 2010; Auli and Lopez, 2011) addresses loss mismatch by incorporating task cost into the training loss.",3.2 Dynamic Oracle Supervision,[0],[0]
"Since trees are decomposed into a sequence of local action predictions, we cannot use a global cost, such as F1, directly.",3.2 Dynamic Oracle Supervision,[0],[0]
"As a proxy, we rely on the dynamic oracles’ action-level supervision.
",3.2 Dynamic Oracle Supervision,[0],[0]
"In all models we consider, action probabilities (Eq. 1) are parameterized by a softmax function
pML(a",3.2 Dynamic Oracle Supervision,[0],[0]
| st; θ) ∝,3.2 Dynamic Oracle Supervision,[0],[0]
"exp(z(a, st, θ))
for some state–action scoring function z.",3.2 Dynamic Oracle Supervision,[0],[0]
"The softmax-margin objective replaces this by
pSMM (a | st; θ) ∝",3.2 Dynamic Oracle Supervision,[0],[0]
"exp(z(a, st, θ) + ∆(a, a∗t ))",3.2 Dynamic Oracle Supervision,[0],[0]
"(2) We use ∆(a, a∗t )",3.2 Dynamic Oracle Supervision,[0],[0]
= 0,3.2 Dynamic Oracle Supervision,[0],[0]
if a = a ∗ t and 1 otherwise.,3.2 Dynamic Oracle Supervision,[0],[0]
"This can be viewed as a “soft” version of the maxmargin objective used by Stern et al. (2017) for training without exploration, but retains a locallynormalized model that we can use for samplingbased exploration.
",3.2 Dynamic Oracle Supervision,[0],[0]
"Softmax Margin with Exploration Finally, we train using a combination of softmax margin loss augmentation and exploration.",3.2 Dynamic Oracle Supervision,[0],[0]
"We perform the same sample-based candidate generation as for policy gradient and likelihood training with exploration, but use Eq. 2 to compute the training loss for candidate states.",3.2 Dynamic Oracle Supervision,[0],[0]
"For those parsers that have a dynamic oracle, this provides a means of training that more directly provides both exploration and cost-aware losses.",3.2 Dynamic Oracle Supervision,[0],[0]
We compare the constituency parsers listed in Section 2 using the above training methods.,4 Experiments,[0],[0]
"Our experiments use the English PTB (Marcus et al., 1993), French Treebank (Abeillé et al., 2003), and Penn Chinese Treebank (CTB) Version 5.1 (Xue et al., 2005).
",4 Experiments,[0],[0]
"Training To compare the training procedures as closely as possible, we train all models for a given parser in a given language from the same randomly-initialized parameter values.
",4 Experiments,[0],[0]
"We train two different versions of the RNNG model: one model using size 128 for the LSTMs and hidden states (following the original work), and a larger model with size 256.",4 Experiments,[0],[0]
"We perform evaluation using greedy search in the Span-Based and Top-Down parsers, and beam search with beam size 10 for the RNNG and In-Order parsers.",4 Experiments,[0],[0]
"We found that beam search improved performance for these two parsers by around 0.1-0.3 F1 on the development sets, and use it at inference time in every setting for these two parsers.
",4 Experiments,[0],[0]
"In our experiments, policy gradient typically requires more epochs of training to reach performance comparable to either of the dynamic oraclebased exploration methods.",4 Experiments,[0],[0]
"Figure 1 gives a typical learning curve, for the Top-Down parser on English.",4 Experiments,[0],[0]
"We found that policy gradient is also more sensitive to the number of candidates sampled per
sentence than either of the other exploration methods, with best performance on the development set usually obtained with k = 10 for k ∈ {2, 5, 10} (where k also counts the sentence’s gold tree, included in the candidate set).",4 Experiments,[0],[0]
"See Appendix A in the supplemental material for the values of k used.
",4 Experiments,[0],[0]
"Tags, Embeddings, and Morphology We largely follow previous work for each parser in our use of predicted part-of-speech tags, pretrained word embeddings, and morphological features.
",4 Experiments,[0],[0]
All parsers use predicted part-of-speech tags as part of their sentence representations.,4 Experiments,[0],[0]
"For English and Chinese, we follow the setup of Cross and Huang (2016): training the Stanford tagger (Toutanova et al., 2003) on the training set of each parsing corpus to predict development and test set tags, and using 10-way jackknifing to predict tags for the training set.
",4 Experiments,[0],[0]
"For French, we use the predicted tags and morphological features provided with the SPMRL dataset (Seddah et al., 2014).",4 Experiments,[0],[0]
We modified the publicly released code for all parsers to use predicted morphological features for French.,4 Experiments,[0],[0]
"We follow the approach outlined by Cross and Huang (2016) and Stern et al. (2017) for representing morphological features as learned embeddings, and use the same dimensions for these embeddings as in their papers.",4 Experiments,[0],[0]
"For RNNG and In-Order, we similarly use 10-dimensional learned embeddings for each morphological feature, feeding them as LSTM inputs for each word alongside the word and part-of-speech tag embeddings.
",4 Experiments,[0],[0]
"For RNNG and the In-Order parser, we use the same word embeddings as the original papers for English and Chinese, and train 100-dimensional word embeddings for French using the structured skip-gram method of Ling et al. (2015) on French Wikipedia.",4 Experiments,[0],[0]
Table 1 compares parser F1 by training procedure for each language.,5 Results and Discussion,[0],[0]
"Policy gradient improves upon likelihood training in 14 out of 15 cases, with improvements of up to 1.5 F1.",5 Results and Discussion,[0],[0]
"One of the three dynamic oracle-based training methods – either likelihood with exploration, softmax margin (SMM), or softmax margin with exploration – obtains better performance than policy gradient in 10 out of 12 cases.",5 Results and Discussion,[0],[0]
"This is perhaps unsurprising given the strong supervision provided by the dynamic oracles and the credit assignment problem faced by
policy gradient.",5 Results and Discussion,[0],[0]
"However, a substantial fraction of this performance gain is recaptured by policy gradient in most cases.
",5 Results and Discussion,[0],[0]
"While likelihood training with exploration using a dynamic oracle more directly addresses exploration bias, and softmax margin training more directly addresses loss mismatch, these two phenomena are still entangled, and the best dynamic oracle-based method to use varies.",5 Results and Discussion,[0],[0]
The effectiveness of the oracle method is also likely to be influenced by the nature of the dynamic oracle available for the parser.,5 Results and Discussion,[0],[0]
"For example, the oracle for RNNG lacks F1 optimality guarantees, and softmax margin without exploration often underperforms likelihood for this parser.",5 Results and Discussion,[0],[0]
"However, exploration improves softmax margin training across all parsers and conditions.
",5 Results and Discussion,[0],[0]
"Although results from likelihood training are mostly comparable between RNNG-128 and the larger model RNNG-256 across languages, policy gradient and likelihood training with exploration both typically yield larger improvements in the larger models, obtaining 92.6 F1 for English and 86.0 for Chinese (using likelihood training with exploration), although results are slightly higher for the policy gradient and dynamic oracle-based methods for the smaller model on French (including 83.5 with softmax margin with exploration).",5 Results and Discussion,[0],[0]
"Finally, we observe that policy gradient also provides large improvements for the In-Order parser, where a dynamic oracle has not been defined.
",5 Results and Discussion,[0],[0]
"We note that although some of these results (92.6 for English, 83.5 for French, 87.0 for Chinese) are state-of-the-art for single model, discriminative transition-based parsers, other work on constituency parsing achieves better performance through other methods.",5 Results and Discussion,[0],[0]
"Techniques that combine multiple models or add semi-supervised data (Vinyals et al., 2015; Dyer et al., 2016; Choe and Charniak, 2016; Kuncoro et al., 2017; Liu and Zhang, 2017; Fried et al., 2017) are orthogonal to, and could be combined with, the singlemodel, fixed training data methods we explore.",5 Results and Discussion,[0],[0]
"Other recent work (Gaddy et al., 2018; Kitaev and Klein, 2018) obtains comparable or stronger performance with global chart decoders, where training uses loss augmentation provided by an oracle.",5 Results and Discussion,[0],[0]
"By performing model-optimal global inference, these parsers likely avoid the exposure bias problem of the sequential transition-based parsers we investigate, at the cost of requiring a chart decoding procedure for inference.
",5 Results and Discussion,[0],[0]
"Overall, we find that although optimizing for F1 in a model-agnostic fashion with policy gradient typically underperforms the model-aware expert supervision given by the dynamic oracle training methods, it provides a simple method for consistently improving upon static oracle likelihood training, at the expense of increased training costs.",5 Results and Discussion,[0],[0]
DF is supported by a Huawei / Berkeley AI fellowship.,Acknowledgments,[0],[0]
"This research used the Savio computational cluster provided by the Berkeley Research Computing program at the University of California, Berkeley.",Acknowledgments,[0],[0]
"Dynamic oracles provide strong supervision for training constituency parsers with exploration, but must be custom defined for a given parser’s transition system.",abstractText,[0],[0]
We explore using a policy gradient method as a parser-agnostic alternative.,abstractText,[0],[0]
"In addition to directly optimizing for a tree-level metric such as F1, policy gradient has the potential to reduce exposure bias by allowing exploration during training; moreover, it does not require a dynamic oracle for supervision.",abstractText,[0],[0]
"On four constituency parsers in three languages, the method substantially outperforms static oracle likelihood training in almost all settings.",abstractText,[0],[0]
"For parsers where a dynamic oracle is available (including a novel oracle which we define for the transition system of Dyer et al. (2016)), policy gradient typically recaptures a substantial fraction of the performance gain afforded by the dynamic oracle.",abstractText,[0],[0]
Policy Gradient as a Proxy for Dynamic Oracles in Constituency Parsing,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2442–2452 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
2442",text,[0],[0]
"Semantic parsing from denotations (SpFD) is the problem of mapping text to executable formal representations (or program) in a situated environment and executing them to generate denotations (or answer), in the absence of access to correct representations.",1 Introduction,[0],[0]
"Several problems have been handled within this framework, including question answering (Berant et al., 2013; Iyyer et al., 2017) and instructions for robots (Artzi and Zettlemoyer, 2013; Misra et al., 2015).
",1 Introduction,[0],[0]
Consider the example in Figure 1.,1 Introduction,[0],[0]
"Given the question and a table environment, a semantic parser maps the question to an executable program, in this case a SQL query, and then executes the query on the environment to generate the answer England.",1 Introduction,[0],[0]
"In the SpFD setting, the training data does not contain the correct programs.",1 Introduction,[0],[0]
"Thus, the existing learning approaches for SpFD perform two steps for every training example, a search step that explores the space of programs
Question: what nation scored the most points
Index Name Nation Points Games Pts/game 1 Karen Andrew England 44 5 8.8 2 Daniella Waterman England 40 5 8 3 Christelle Le Duff France 33 5 6.6 4 Charlotte Barras England 30 5 6 5",1 Introduction,[0],[0]
"Naomi Thomas Wales 25 5 5
Select Nation Where Points is Maximum
Program:
",1 Introduction,[0],[0]
"Answer:
Environment:
England
Figure 1: An example of semantic parsing from denotations.",1 Introduction,[0],[0]
"Given the table environment, map the question to an executable program that evaluates to the answer.
and finds suitable candidates, and an update step that uses these programs to update the model.",1 Introduction,[0],[0]
"Figure 2 shows the two step training procedure for the above example.
",1 Introduction,[0],[0]
"In this paper, we address two key challenges in model training for SpFD by proposing a novel learning framework, improving both the search and update steps.",1 Introduction,[0],[0]
"The first challenge, the existence of spurious programs, lies in the search step.",1 Introduction,[0],[0]
"More specifically, while the success of the search step relies on its ability to find programs that are semantically correct, we can only verify if the program can generate correct answers, given that no gold programs are presented.",1 Introduction,[0],[0]
"The search step is complicated by spurious programs, which happen to evaluate to the correct answer but do not represent accurately the meaning of the natural language question.",1 Introduction,[0],[0]
"For example, for the environment in Figure 1, the program Select Nation",1 Introduction,[0],[0]
Where Name = Karen Andrew is spurious.,1 Introduction,[0],[0]
"Selecting spurious programs as positive examples can greatly affect the performance of semantic parsers as these programs generally do not gen-
eralize to unseen questions and environments.",1 Introduction,[0],[0]
"The second challenge, choosing a learning algorithm, lies in the update step.",1 Introduction,[0],[0]
"Because of the unique indirect supervision setting of SpFD, the quality of the learned semantic parser is dictated by the choice of how to update the model parameters, often determined empirically.",1 Introduction,[0],[0]
"As a result, several families of learning methods, including maximum marginal likelihood, reinforcement learning and margin based methods have been used.",1 Introduction,[0],[0]
"How to effectively explore different model choices could be crucial in practice.
",1 Introduction,[0],[0]
Our contributions in this work are twofold.,1 Introduction,[0],[0]
"To address the first challenge, we propose a policy shaping (Griffith et al., 2013) method that incorporates simple, lightweight domain knowledge, such as a small set of lexical pairs of tokens in the question and program, in the form of a critique policy (§ 3).",1 Introduction,[0],[0]
"This helps bias the search towards the correct program, an important step to improve supervision signals, which benefits learning regardless of the choice of algorithm.",1 Introduction,[0],[0]
"To address the second challenge, we prove that the parameter update step in several algorithms are similar and can be viewed as special cases of a generalized update equation (§ 4).",1 Introduction,[0],[0]
The equation contains two variable terms that govern the update behavior.,1 Introduction,[0],[0]
Changing these two terms effectively defines an infinite class of learning algorithms where different values lead to significantly different results.,1 Introduction,[0],[0]
"We study this effect and propose a novel learning framework that improves over existing methods.
",1 Introduction,[0],[0]
"We evaluate our methods using the sequential question answering (SQA) dataset (Iyyer et al., 2017), and show that our proposed improvements to the search and update steps consistently enhance existing approaches.",1 Introduction,[0],[0]
The proposed algorithm achieves new state-of-the-art and outperforms existing parsers by 5.0%.,1 Introduction,[0],[0]
"We give a formal problem definition of the semantic parsing task, followed by the general learning framework for solving it.",2 Background,[0],[0]
The problem discussed in this paper can be formally defined as follows.,2.1 The Semantic Parsing Task,[0],[0]
"Let X be the set of all possible questions, Y programs (e.g., SQL-like queries), T tables (i.e., the structured data in this work) and Z answers.",2.1 The Semantic Parsing Task,[0],[0]
We further assume access to an executor : Y ⇥ T !,2.1 The Semantic Parsing Task,[0],[0]
"Z , that given a program y 2 Y and a table t 2 T , generates an answer (y, t) 2 Z .",2.1 The Semantic Parsing Task,[0],[0]
We assume that the executor and all tables are deterministic and the executor can be called as many times as possible.,2.1 The Semantic Parsing Task,[0],[0]
"To facilitate discussion in the following sections, we define an environment function et : Y !",2.1 The Semantic Parsing Task,[0],[0]
"Z , by applying the executor to the program as et(y) = (y, t).
",2.1 The Semantic Parsing Task,[0],[0]
"Given a question x and an environment et, our aim is to generate a program y⇤ 2 Y and then execute it to produce the answer et(y⇤).",2.1 The Semantic Parsing Task,[0],[0]
"Assume that for any y 2 Y , the score of y being a correct program for x is score✓(y, x, t), parameterized by ✓.",2.1 The Semantic Parsing Task,[0],[0]
"The inference task is thus:
y⇤ = arg max y2Y score✓(y, x, t) (1)
",2.1 The Semantic Parsing Task,[0],[0]
"As the size of Y is exponential to the length of the program, a generic search procedure is typically employed for Eq.",2.1 The Semantic Parsing Task,[0],[0]
"(1), as efficient dynamic algorithms typically do not exist.",2.1 The Semantic Parsing Task,[0],[0]
"These search procedures generally maintain a beam of program states sorted according to some scoring function, where each program state represents an incomplete program.",2.1 The Semantic Parsing Task,[0],[0]
The search then generates a new program state from an existing state by performing an action.,2.1 The Semantic Parsing Task,[0],[0]
"Each action adds a set of tokens (e.g., Nation) and keyword (e.g., Select) to a
program state.",2.1 The Semantic Parsing Task,[0],[0]
"For example, in order to generate the program in Figure 1, the DynSP parser (Iyyer et al., 2017) will take the first action as adding the SQL expression Select Nation.",2.1 The Semantic Parsing Task,[0],[0]
Notice that score✓ can be used in either probabilistic or nonprobabilistic models.,2.1 The Semantic Parsing Task,[0],[0]
"For probabilistic models, we assume that it is a Boltzmann policy, meaning that p✓(y | x, t) / exp{score✓(y, x, t)}.",2.1 The Semantic Parsing Task,[0],[0]
"Learning a semantic parser is equivalent to learning the parameters ✓ in the scoring function, which is a structured learning problem, due to the large, structured output space Y .",2.2 Learning,[0],[0]
Structured learning algorithms generally consist of two major components: search and update.,2.2 Learning,[0],[0]
"When the gold programs are available during training, the search procedure finds a set of high-scoring incorrect programs.",2.2 Learning,[0],[0]
These programs are used by the update step to derive loss for updating parameters.,2.2 Learning,[0],[0]
"For example, these programs are used for approximating the partition-function in maximum-likelihood objective (Liang et al., 2011) and finding set of programs causing margin violation in margin based methods (Daumé III and Marcu, 2005).",2.2 Learning,[0],[0]
"Depending on the exact algorithm being used, these two components are not necessarily separated into isolated steps.",2.2 Learning,[0],[0]
"For instance, parameters can be updated in the middle of search (e.g., Huang et al., 2012).
",2.2 Learning,[0],[0]
"For learning semantic parsers from denotations, where we assume only answers are available in a training set {(xi, ti, zi)}Ni=1 of N examples, the basic construction of the learning algorithms remains the same.",2.2 Learning,[0],[0]
"However, the problems that search needs to handle in SpFD is more challenging.",2.2 Learning,[0],[0]
"In addition to finding a set of high-scoring incorrect programs, the search procedure also needs to guess the correct program(s) evaluating to the gold answer zi.",2.2 Learning,[0],[0]
"This problem is further complicated by the presence of spurious programs, which generate the correct answer but are semantically incompatible with the question.",2.2 Learning,[0],[0]
"For example, although all programs in Figure 2 evaluate to the same answer, only one of them is correct.",2.2 Learning,[0],[0]
The issue of the spurious programs also affects the design of model update.,2.2 Learning,[0],[0]
"For instance, maximum marginal likelihood methods treat all the programs that evaluate to the gold answer equally, while maximum margin reward networks use model score to break tie and pick one of the
programs as the correct reference.",2.2 Learning,[0],[0]
"Given a training example (x, t, z), the aim of the search step is to find a set K(x, t, z) of programs consisting of correct programs that evaluate to z and high-scoring incorrect programs.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
The search step should avoid picking up spurious programs for learning since such programs typically do not generalize.,3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"For example, in Figure 2, the spurious program Select Nation Where Index is Min will evaluate to an incorrect answer if the indices of the first two rows are swapped1.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"This problem is challenging since among the programs that evaluate to the correct answer, most of them are spurious.
",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"The search step can be viewed as following an exploration policy b✓(y|x, t, z) to explore the set of programs Y .",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"This exploration is often performed by beam search and at each step, we either sample from b✓ or take the top scoring programs.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"The set K(x, t, z) is then used by the update step for parameter update.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"Most search strategies use an exploration policy which is based on the score function, for example b✓(y|x, t, z) / exp{score✓(y, t)}.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"However, this approach can suffer from a divergence phenomenon whereby the score of spurious programs picked up by the search in the first epoch increases, making it more likely for the search to pick them up in the future.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"Such divergence issues are common with latent-variable learning and often require careful initialization to overcome (Rose, 1998).",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"Unfortunately such initialization schemes are not applicable for deep neural networks which form the model of most successful semantic parsers today (Jia and Liang, 2016; Misra and Artzi, 2016; Iyyer et al., 2017).",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"Prior work, such as ✏-greedy exploration (Guu et al., 2017), has reduced the severity of this problem by introducing random noise in the search procedure to avoid saturating the search on high-scoring spurious programs.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"However, random noise need not bias the search towards the correct program(s).",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"In this paper, we introduce a simple policy-shaping method to guide the search.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"This approach allows incorporating prior knowledge in the exploration policy and can bias the search away from spurious programs.
",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"1This transformation preserves the answer of the question.
",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"Algorithm 1 Learning a semantic parser from denotation using generalized updates.
",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"Input: Training set {(xi, ti, zi}Ni=1 (see Section 2), learning rate µ and stopping epoch T (̃see Section 4).",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"Definitions: score✓(y, x, t) is a semantic parsing model parameterized by ✓.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
ps(y,3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"| x, t) is the policy used for exploration and search(✓, x, t, z, ps) generates candidate programs for updating parameters (see Section 3).",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
is the generalized update (see Section 4).,3 Addressing Spurious Programs: Policy Shaping,[0],[0]
Output: Model parameters ✓.,3 Addressing Spurious Programs: Policy Shaping,[0],[0]
1: » Iterate over the training data.,3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"2: for t = 1 to T , i = 1 to N do 3: » Find candidate programs using the shaped policy.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"4: K = search(✓, xi, ti, zi, ps) 5: » Compute generalized gradient updates 6: ✓ = ✓ + µ (K) 7: return ✓
Policy Shaping Policy shaping is a method to introduce prior knowledge into a policy (Griffith et al., 2013).",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"Formally, let the current behavior policy be b✓(y|x, t, z) and a predefined critique policy, the prior knowledge, be pc(y|x, t).",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"Policy shaping defines a new shaped behavior policy pb(y|x, t) given by:
pb(y|x, t) = b✓(y|x, t, z)pc(y|x, t)P
y02Y",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"b✓(y 0|x, t, z)pc(y0|x, t)
.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"(2)
Using the shaped policy for exploration biases the search towards the critique policy’s preference.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"We next describe a simple critique policy that we use in this paper.
",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
Lexical Policy Shaping We qualitatively observed that correct programs often contains tokens which are also present in the question.,3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"For example, the correct program in Figure 2 contains the token Points, which is also present in the question.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"We therefore, define a simple surface form similarity feature match(x, y) that computes the ratio of number of non-keyword tokens in the program y that are also present in the question",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"x.
However, surface-form similarity is often not enough.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"For example, both the first and fourth program in Figure 2 contain the token Points but only the fourth program is correct.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"Therefore, we also use a simple co-occurrence feature that triggers on frequently co-occurring pairs of tokens in the program and instruction.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"For example, the token most is highly likely to co-occur with a correct program containing the keyword Max.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
This happens for the example in Figure 2.,3 Addressing Spurious Programs: Policy Shaping,[0],[0]
Similarly the token not may co-occur with the keyword NotEqual.,3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"We assume access to a lexicon ⇤ = {(wj , !j)}kj=1",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"containing
k lexical pairs of tokens and keywords.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"Each lexical pair (w, !) maps the token w in a text to a keyword !",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
in a program.,3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"For a given program y and question x, we define a co-occurrence score as co_occur(y, x) = P (w,!)2⇤ {w 2 x ^ !",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
2 y}}.,3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"We define critique score critique(y, x) as the sum of the match and co_occur scores.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"The critique policy is given by:
pc(y|x, t) / exp",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"(⌘ ⇤ critique(y, x)) , (3)
where ⌘ is a single scalar hyper-parameter denoting the confidence in the critique policy.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"Given the set of programs generated by the search step, one can use many objectives to update the parameters.",4 Addressing Update Strategy Selection: Generalized Update Equation,[0],[0]
"For example, previous work have utilized maximum marginal likelihood (Krishnamurthy et al., 2017; Guu et al., 2017), reinforcement learning (Zhong et al., 2017; Guu et al., 2017) and margin based methods (Iyyer et al., 2017).",4 Addressing Update Strategy Selection: Generalized Update Equation,[0],[0]
"It could be difficult to choose the suitable algorithm from these options.
",4 Addressing Update Strategy Selection: Generalized Update Equation,[0],[0]
"In this section, we propose a principle and general update equation such that previous update algorithms can be considered as special cases to this equation.",4 Addressing Update Strategy Selection: Generalized Update Equation,[0],[0]
Having a general update is important for the following reasons.,4 Addressing Update Strategy Selection: Generalized Update Equation,[0],[0]
"First, it allows us to understand existing algorithms better by examining their basic properties.",4 Addressing Update Strategy Selection: Generalized Update Equation,[0],[0]
"Second, the generalized update equation also makes it easy to implement and experiment with various different algorithms.",4 Addressing Update Strategy Selection: Generalized Update Equation,[0],[0]
"Moreover, it provides a framework that enables the development of new variations or extensions of existing learning methods.
",4 Addressing Update Strategy Selection: Generalized Update Equation,[0],[0]
"In the following, we describe how the commonly used algorithms are in fact very similar – their update rules can all be viewed as special cases of the proposed generalized update equation.",4 Addressing Update Strategy Selection: Generalized Update Equation,[0],[0]
Algorithm 1 shows the meta-learning framework.,4 Addressing Update Strategy Selection: Generalized Update Equation,[0],[0]
"For every training example, we first find a set of candidates using an exploration policy (line 4).",4 Addressing Update Strategy Selection: Generalized Update Equation,[0],[0]
We use the program candidates to update the parameters (line 6).,4 Addressing Update Strategy Selection: Generalized Update Equation,[0],[0]
"We briefly describe three algorithms: maximum marginalized likelihood, policy gradient and maximum margin reward.
",4.1 Commonly Used Learning Algorithms,[0],[0]
"Maximum Marginalized Likelihood The maximum marginalized likelihood method maximizes the log-likelihood of the training data by marginalizing over the set of programs.
",4.1 Commonly Used Learning Algorithms,[0],[0]
"JMML = log p(zi|xi, ti) = log X
y2Y p(zi|y, ti)p(y|xi, ti) (4)
",4.1 Commonly Used Learning Algorithms,[0],[0]
"Because an answer is deterministically computed given a program and a table, we define p(z | y, t) as 1 or 0 depending upon whether the y evaluates to z given t, or not.",4.1 Commonly Used Learning Algorithms,[0],[0]
"Let Gen(z, t) ✓ Y be the set of compatible programs that evaluate to z given the table t.",4.1 Commonly Used Learning Algorithms,[0],[0]
"The objective can then be expressed as:
JMML = log X
y2Gen(zi,ti)
p(y|xi, ti) (5)
",4.1 Commonly Used Learning Algorithms,[0],[0]
"In practice, the summation over Gen(.) is approximated by only using the compatible programs in the set K generated by the search step.
",4.1 Commonly Used Learning Algorithms,[0],[0]
Policy Gradient Methods Most reinforcement learning approaches for semantic parsing assume access to a reward function R : Y ⇥X ⇥Z !,4.1 Commonly Used Learning Algorithms,[0],[0]
"R, giving a scalar reward R(y, z) for a given program y and the correct answer z.2",4.1 Commonly Used Learning Algorithms,[0],[0]
"We can further assume without loss of generality that the reward is always in [0, 1].",4.1 Commonly Used Learning Algorithms,[0],[0]
"Reinforcement learning approaches maximize the expected reward JRL:
JRL = X
y2Y p(y|xi, ti)R(y, zi) (6)
JRL is hard to approximate using numerical integration since the reward for all programs may not be known a priori.",4.1 Commonly Used Learning Algorithms,[0],[0]
Policy gradient methods solve this by approximating the derivative using a sample from the policy.,4.1 Commonly Used Learning Algorithms,[0],[0]
"When the search space is large, the policy may fail to sample a correct program, which can greatly slow down the learning.",4.1 Commonly Used Learning Algorithms,[0],[0]
"Therefore, off-policy methods are sometimes introduced to bias the sampling towards high-reward yielding programs.",4.1 Commonly Used Learning Algorithms,[0],[0]
"In those methods, an additional exploration policy u(y|xi, ti, zi) is used to improve sampling.",4.1 Commonly Used Learning Algorithms,[0],[0]
"Importance weights are used to make the gradient unbiased (see Appendix for derivation).
",4.1 Commonly Used Learning Algorithms,[0],[0]
2This is essentially a contextual bandit setting.,4.1 Commonly Used Learning Algorithms,[0],[0]
Guu et al. (2017) also used this setting.,4.1 Commonly Used Learning Algorithms,[0],[0]
A general reinforcement learning setting requires taking a sequence of actions and receiving a reward for each action.,4.1 Commonly Used Learning Algorithms,[0],[0]
"For example, a program can be viewed as a sequence of parsing actions, where each action can get a reward.",4.1 Commonly Used Learning Algorithms,[0],[0]
"We do not consider the general setting here.
",4.1 Commonly Used Learning Algorithms,[0],[0]
"Maximum Margin Reward For every training example (xi, ti, zi), the maximum margin reward method finds the highest scoring program yi that evaluates to zi, as the reference program, from the set K of programs generated by the search.",4.1 Commonly Used Learning Algorithms,[0],[0]
With a margin function : Y⇥Y⇥Z !,4.1 Commonly Used Learning Algorithms,[0],[0]
"R and reference program y, the set of programs V that violate the margin constraint can thus be defined as:
V = {y0 | y0 2 Y and score✓(y, x, t)  score✓(y0, x, t)",4.1 Commonly Used Learning Algorithms,[0],[0]
+,4.1 Commonly Used Learning Algorithms,[0],[0]
"(y, y0, z)}, (7)
where (y, y0, z) = R(y, z)",4.1 Commonly Used Learning Algorithms,[0],[0]
"R(y0, z).",4.1 Commonly Used Learning Algorithms,[0],[0]
"Similarly, the program that most violates the constraint can be written as:
ȳ = arg max y02Y
{score✓(y0, x, t) +",4.1 Commonly Used Learning Algorithms,[0],[0]
"(y, y0, z)
score✓(y, x, t)}",4.1 Commonly Used Learning Algorithms,[0],[0]
"(8)
The most-violation margin objective (negative margin loss) is thus defined as:
JMMR = max{0, score✓(ȳ, xi, ti) score✓(yi, xi, ti) +",4.1 Commonly Used Learning Algorithms,[0],[0]
"(yi, ȳ, zi)}
Unlike the previous two learning algorithms, margin methods only update the score of the reference program and the program that violates the margin.",4.1 Commonly Used Learning Algorithms,[0],[0]
"Although the algorithms described in §4.1 seem very different on the surface, the gradients of their loss functions can in fact be described in the same generalized form, given in Eq.",4.2 Generalized Update Equation,[0],[0]
(9)3.,4.2 Generalized Update Equation,[0],[0]
"In addition to the gradient of the model scoring function, this equation has two variable terms, w(·), q(·).",4.2 Generalized Update Equation,[0],[0]
"We call the first term w(y, x, t, z) intensity, which is a positive scalar value and the second term q(y|x, t) the competing distribution, which is a probability distribution over programs.",4.2 Generalized Update Equation,[0],[0]
"Varying them makes the equation equivalent to the update rule of the algorithms we discussed, as shown in Table 1.",4.2 Generalized Update Equation,[0],[0]
"We also consider meritocratic update policy which uses a hyperparameter to sharpen or smooth the intensity of maximum marginal likelihood (Guu et al., 2017).
",4.2 Generalized Update Equation,[0],[0]
"Intuitively, w(y, x, t, z) defines the positive part of the update equation, which defines how aggressively the update favors program y. Likewise, q(y|x, t) defines the negative part of the learning
3See Appendix for the detailed derivation.
",4.2 Generalized Update Equation,[0],[0]
"Generalized Update Equation:
(K) = X
y2K w(y, x, t, z)
0 @r✓score✓(y, x, t) X
y02Y",4.2 Generalized Update Equation,[0],[0]
"q(y0|x, t)r✓score✓(y0, x, t)
1
A (9)
algorithm, namely how aggressively the update penalizes the members of the program set.
",4.2 Generalized Update Equation,[0],[0]
"The generalized update equation provides a tool for better understanding individual algorithm, and helps shed some light on when a particular method may perform better.
",4.2 Generalized Update Equation,[0],[0]
"Intensity versus Search Quality In SpFD, the effectiveness of the algorithms for SpFD is closely related to the quality of the search results given that the gold program is not available.",4.2 Generalized Update Equation,[0],[0]
"Intuitively, if the search quality is good, the update algorithm could be aggressive on updating the model parameters.",4.2 Generalized Update Equation,[0],[0]
"When the search quality is poor, the algorithm should be conservative.
",4.2 Generalized Update Equation,[0],[0]
The intensity w(·) is closely related to the aggressiveness of the algorithm.,4.2 Generalized Update Equation,[0],[0]
"For example, the maximum marginal likelihood is less aggressive given that it produces a non-zero intensity over all programs in the program set K that evaluate to the correct answer.",4.2 Generalized Update Equation,[0],[0]
"The intensity for a particular correct program y is proportional to its probability p(y|x, t).",4.2 Generalized Update Equation,[0],[0]
"Further, meritocratic update becomes more aggressive as becomes larger.
",4.2 Generalized Update Equation,[0],[0]
"In contrast, REINFORCE and maximum margin reward both have a non-zero intensity only on a single program in K.",4.2 Generalized Update Equation,[0],[0]
"This value is 1.0 for maximum margin reward, while for reinforcement learning, this value is the reward.",4.2 Generalized Update Equation,[0],[0]
"Maximum margin reward therefore updates most aggressively in favor of its selection while maximum marginal
likelihood tends to hedge its bet.",4.2 Generalized Update Equation,[0],[0]
"Therefore, the maximum margin methods should benefit the most when the search quality improves.
",4.2 Generalized Update Equation,[0],[0]
Stability The general equation also allows us to investigate the stability of a model update algorithm.,4.2 Generalized Update Equation,[0],[0]
"In general, the variance of update direction can be high, hence less stable, if the model update algorithm has peaky competing distribution, or it puts all of its intensity on a single program.",4.2 Generalized Update Equation,[0],[0]
"For example, REINFORCE only samples one program and puts non-zero intensity only on that program, so it could be unstable depending on the sampling results.
",4.2 Generalized Update Equation,[0],[0]
The competing distribution affects the stability of the algorithm.,4.2 Generalized Update Equation,[0],[0]
"For example, maximum margin reward penalizes only the most violating program and is benign to other incorrect programs.",4.2 Generalized Update Equation,[0],[0]
"Therefore, the MMR algorithm could be unstable during training.
",4.2 Generalized Update Equation,[0],[0]
New Model Update Algorithm,4.2 Generalized Update Equation,[0],[0]
The general equation provides a framework that enables the development of new variations or extensions of existing learning methods.,4.2 Generalized Update Equation,[0],[0]
"For example, in order to improve the stability of the MMR algorithm, we propose a simple variant of maximum margin reward, which penalizes all violating programs instead of only the most violating one.",4.2 Generalized Update Equation,[0],[0]
"We call this approach maximum margin average violation reward (MAVER), which is included in Table 1 as well.",4.2 Generalized Update Equation,[0],[0]
"Given that MAVER effectively considers
more negative examples during each update, we expect that it is more stable compared to the MMR algorithm.",4.2 Generalized Update Equation,[0],[0]
We describe the setup in §5.1 and results in §5.2.,5 Experiments,[0],[0]
"Dataset We use the sequential question answering (SQA) dataset (Iyyer et al., 2017) for our experiments.",5.1 Setup,[0],[0]
"SQA contains 6,066 sequences and each sequence contains up to 3 questions, with 17,553 questions in total.",5.1 Setup,[0],[0]
The data is partitioned into training (83%) and test (17%) splits.,5.1 Setup,[0],[0]
We use 4/5 of the original train split as our training set and the remaining 1/5 as the dev set.,5.1 Setup,[0],[0]
We evaluate using exact match on answer.,5.1 Setup,[0],[0]
"Previous state-of-theart result on the SQA dataset is 44.7% accuracy, using maximum margin reward learning.
",5.1 Setup,[0],[0]
"Semantic Parser Our semantic parser is based on DynSP (Iyyer et al., 2017), which contains a set of SQL actions, such as adding a clause (e.g., Select Column) or adding an operator (e.g., Max).",5.1 Setup,[0],[0]
"Each action has an associated neural network module that generates the score for the action based on the instruction, the table and the list of past actions.",5.1 Setup,[0],[0]
"The score of the entire program is given by the sum of scores of all actions.
",5.1 Setup,[0],[0]
We modified DynSP to improve its representational capacity.,5.1 Setup,[0],[0]
We refer to the new parser as DynSP++.,5.1 Setup,[0],[0]
"Most notably, we included new features and introduced two additional parser actions.",5.1 Setup,[0],[0]
See Appendix 8.2 for more details.,5.1 Setup,[0],[0]
"While these improvements help us achieve state-of-the-art results, the majority of the gain comes from the learning contributions described in this paper.
",5.1 Setup,[0],[0]
"Hyperparameters For each experiment, we train the model for 30 epochs.",5.1 Setup,[0],[0]
We find the optimal stopping epoch by evaluating the model on the dev set.,5.1 Setup,[0],[0]
We then train on train+dev set till the stopping epoch and evaluate the model on the held-out test set.,5.1 Setup,[0],[0]
Model parameters are trained using stochastic gradient descent with learning rate of 0.1.,5.1 Setup,[0],[0]
We set the hyperparameter ⌘ for policy shaping to 5.,5.1 Setup,[0],[0]
All hyperparameters were tuned on the dev set.,5.1 Setup,[0],[0]
We use 40 lexical pairs for defining the co-occur score.,5.1 Setup,[0],[0]
"We used common English superlatives (e.g., highest, most) and comparators (e.g., more, larger) and did not fit the lexical pairs based on the dataset.
",5.1 Setup,[0],[0]
"Given the model parameter ✓, we use a base exploration policy defined in (Iyyer et al., 2017).",5.1 Setup,[0],[0]
"This exploration policy is given by b✓(y | x, t, z) / exp( · R(y, z) + score",5.1 Setup,[0],[0]
"✓(y, ✓, z)).",5.1 Setup,[0],[0]
"R(y, z) is the reward function of the incomplete program y, given the answer z.",5.1 Setup,[0],[0]
"We use a reward function R(y, z) given by the Jaccard similarity of the gold answer z and the answer generated by the program y.",5.1 Setup,[0],[0]
"The value of is set to infinity, which essentially is equivalent to sorting the programs based on the reward and using the current model score for tie breaking.",5.1 Setup,[0],[0]
"Further, we prune all syntactically invalid programs.",5.1 Setup,[0],[0]
"For more details, we refer the reader to (Iyyer et al., 2017).",5.1 Setup,[0],[0]
Table 2 contains the dev and test results when using our algorithm on the SQA dataset.,5.2 Results,[0],[0]
We observe that margin based methods perform better than maximum likelihood methods and policy gradient in our experiment.,5.2 Results,[0],[0]
Policy shaping in general improves the performance across different algorithms.,5.2 Results,[0],[0]
"Our best test results outperform previous SOTA by 5.0%.
",5.2 Results,[0],[0]
"Policy Gradient vs Off-Policy Gradient REINFORCE, a simple policy gradient method, achieved extremely poor performance.",5.2 Results,[0],[0]
This likely due to the problem of exploration and having to sample from a large space of programs.,5.2 Results,[0],[0]
This is further corroborated from observing the much superior performance of off-policy policy gradient methods.,5.2 Results,[0],[0]
"Thus, the sampling policy is an important factor to consider for policy gradient methods.
",5.2 Results,[0],[0]
The Effect of Policy Shaping We observe that the improvement due to policy shaping is 6.0% on the SQA dataset for MAVER and only 1.3% for maximum marginal likelihood.,5.2 Results,[0],[0]
"We also observe that as increases, the improvement due to policy shaping for meritocratic update increases.",5.2 Results,[0],[0]
"This supports our hypothesis that aggressive updates of margin based methods is beneficial when the search method is more accurate as compared to maximum marginal likelihood which hedges its bet between all programs that evaluate to the right answer.
",5.2 Results,[0],[0]
"Stability of MMR In Section 4, the general update equation helps us point out that MMR could be unstable due to the peaky competing distribution.",5.2 Results,[0],[0]
MAVER was proposed to increase the stability of the algorithm.,5.2 Results,[0],[0]
"To measure stability, we cal-
culate the mean absolute difference of the development set accuracy between successive epochs during training, as it indicates how much an algorithm’s performance fluctuates during training.",5.2 Results,[0],[0]
"With this metric, we found mean difference for MAVER is 0.57% where the mean difference for MMR is 0.9%.",5.2 Results,[0],[0]
"This indicates that MAVER is in fact more stable than MMR.
",5.2 Results,[0],[0]
Other variations We also analyze other possible novel learning algorithms that are made possible due to generalized update equations.,5.2 Results,[0],[0]
Table 3 reports development results using these algorithms.,5.2 Results,[0],[0]
"By mixing different intensity scalars and competing distribution from different algorithms, we can create new variations of the model update algorithm.",5.2 Results,[0],[0]
"In Table 3, we show that by mixing the MMR’s intensity and MML’s competing distribution, we can create an algorithm that outperform MMR on the development set.
",5.2 Results,[0],[0]
"Policy Shaping helps against Spurious Programs In order to better understand if policy shaping helps bias the search away from spurious programs, we analyze 100 training examples.",5.2 Results,[0],[0]
We look at the highest scoring program in the beam at the end of training using MAVER.,5.2 Results,[0],[0]
"Without policy shaping, we found that 53 programs were spurious while using policy shaping this number came down to 23.",5.2 Results,[0],[0]
"We list few examples of spurious program errors corrected by policy shaping in Table 4.
Policy Shaping vs Model Shaping Critique policy contains useful information that can bias the search away from spurious programs.",5.2 Results,[0],[0]
"Therefore, one can also consider making the critique policy as part of the model.",5.2 Results,[0],[0]
We call this model shaping.,5.2 Results,[0],[0]
We define our model to be the shaped policy and train and test using the new model.,5.2 Results,[0],[0]
"Using MAVER updates, we found that the dev accuracy dropped to 37.1%.",5.2 Results,[0],[0]
We conjecture that the strong prior in the critique policy can hinder generalization in model shaping.,5.2 Results,[0],[0]
Semantic Parsing from Denotation Mapping natural language text to formal meaning representation was first studied by Montague (1970).,6 Related Work,[0],[0]
"Early work on learning semantic parsers rely on labeled formal representations as the supervision signals (Zettlemoyer and Collins, 2005, 2007; Zelle and Mooney, 1993).",6 Related Work,[0],[0]
"However, because getting access to gold formal representation generally requires expensive annotations by an expert, distant supervision approaches, where semantic parsers are learned from denotation only, have become the main learning paradigm (e.g., Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Berant et al., 2013; Iyyer et al., 2017; Krishnamurthy et al., 2017).",6 Related Work,[0],[0]
"Guu et al. (2017) studied the problem of spurious programs and considered adding noise to diversify the search procedure and introduced meritocratic updates.
",6 Related Work,[0],[0]
"Reinforcement Learning Algorithms Reinforcement learning algorithms have been applied to various NLP problems including dialogue (Li et al., 2016), text-based games (Narasimhan et al., 2015), information extraction (Narasimhan et al., 2016), coreference resolution (Clark and Man-
ning, 2016), semantic parsing (Guu et al., 2017) and instruction following (Misra et al., 2017).",6 Related Work,[0],[0]
Guu et al. (2017) show that policy gradient methods underperform maximum marginal likelihood approaches.,6 Related Work,[0],[0]
Our result on the SQA dataset supports their observation.,6 Related Work,[0],[0]
"However, we show that using off-policy sampling, policy gradient methods can provide superior performance to maximum marginal likelihood methods.
",6 Related Work,[0],[0]
Margin-based Learning Margin-based methods have been considered in the context of SVM learning.,6 Related Work,[0],[0]
"In the NLP literature, margin based learning has been applied to parsing (Taskar et al., 2004; McDonald et al., 2005), text classification (Taskar et al., 2003), machine translation (Watanabe et al., 2007) and semantic parsing (Iyyer et al., 2017).",6 Related Work,[0],[0]
Kummerfeld et al. (2015) found that max-margin based methods generally outperform likelihood maximization on a range of tasks.,6 Related Work,[0],[0]
Previous work have studied connections between margin based method and likelihood maximization for supervised learning setting.,6 Related Work,[0],[0]
We show them as special cases of our unified update equation for distant supervision learning.,6 Related Work,[0],[0]
"Similar to this work, Lee et al. (2016) also found that in the context of supervised learning, margin-based algorithms which update all violated examples perform better than the one that only updates the most violated example.
",6 Related Work,[0],[0]
"Latent Variable Modeling Learning semantic parsers from denotation can be viewed as a latent variable modeling problem, where the program is the latent variable.",6 Related Work,[0],[0]
"Probabilistic latent variable models have been studied using EM-algorithm and its variant (Dempster et al., 1977).",6 Related Work,[0],[0]
"The graphical model literature has studied latent variable learning on margin-based methods (Yu and Joachims, 2009) and probabilistic models (Quattoni et al., 2007).",6 Related Work,[0],[0]
"Samdani et al. (2012) studied various vari-
ants of EM algorithm and showed that all of them are special cases of a unified framework.",6 Related Work,[0],[0]
Our generalized update framework is similar in spirit.,6 Related Work,[0],[0]
"In this paper, we propose a general update equation from semantic parsing from denotation and propose a policy shaping method for addressing the spurious program challenge.",7 Conclusion,[0],[0]
"For the future, we plan to apply the proposed learning framework to more semantic parsing tasks and consider new methods for policy shaping.",7 Conclusion,[0],[0]
"We thank Ryan Benmalek, Alane Suhr, Yoav Artzi, Claire Cardie, Chris Quirk, Michel Galley and members of the Cornell NLP group for their valuable comments.",8 Acknowledgements,[0],[0]
We are also grateful to Allen Institute for Artificial Intelligence for the computing resource support.,8 Acknowledgements,[0],[0]
This work was initially started when the first author interned at Microsoft Research.,8 Acknowledgements,[0],[0]
"Semantic parsing from denotations faces two key challenges in model training: (1) given only the denotations (e.g., answers), search for good candidate semantic parses, and (2) choose the best model update algorithm.",abstractText,[0],[0]
We propose effective and general solutions to each of them.,abstractText,[0],[0]
"Using policy shaping, we bias the search procedure towards semantic parses that are more compatible to the text, which provide better supervision signals for training.",abstractText,[0],[0]
"In addition, we propose an update equation that generalizes three different families of learning algorithms, which enables fast model exploration.",abstractText,[0],[0]
"When experimented on a recently proposed sequential question answering dataset, our framework leads to a new state-of-theart model that outperforms previous work by 5.0% absolute on exact match accuracy.",abstractText,[0],[0]
Policy Shaping and Generalized Update Equations for Semantic Parsing from Denotations,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 784–792 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1073",text,[0],[0]
"The basic idea of distributional semantics, i.e. determining the meaning of a word based on its co-occurrence with other words, is derived from the empiricists – Harris (1954) and Firth (1957).",1.1 Distributional semantics,[0],[0]
"John R. Firth drew attention to the contextdependent nature of meaning especially with his
1The dataset is obtainable at: http://zil.ipipan.waw.pl/Scwad/CDSCorpus
famous maxim “You shall know a word by the company it keeps” (Firth, 1957, p. 11).
",1.1 Distributional semantics,[0],[0]
"Nowadays, distributional semantics models are estimated with various methods, e.g. word embedding techniques (Bengio et al., 2003, 2006; Mikolov et al., 2013).",1.1 Distributional semantics,[0],[0]
"To ascertain the purport of a word, e.g. bath, you can use the context of other words that surround it.",1.1 Distributional semantics,[0],[0]
"If we assume that the meaning of this word expressed by its lexical context is associated with a distributional vector, the distance between distributional vectors of two semantically similar words, e.g bath and shower, should be smaller than between vectors representing semantically distinct words, e.g. bath and tree.",1.1 Distributional semantics,[0],[0]
"Based on empirical observations that distributional vectors encode certain aspects of word meaning, it is expected that similar aspects of the meaning of phrases and sentences can also be represented with vectors obtained via composition of distributional word vectors.",1.2 Compositional distributional semantics,[0],[0]
The idea of semantic composition is not new.,1.2 Compositional distributional semantics,[0],[0]
It is well known as the principle of compositionality:2,1.2 Compositional distributional semantics,[0],[0]
“The meaning of a compound expression is a function of the meaning of its parts and of the way they are syntactically combined.”,1.2 Compositional distributional semantics,[0],[0]
"(Janssen, 2012, p. 19).
",1.2 Compositional distributional semantics,[0],[0]
"Modelling the meaning of textual units larger than words using compositional and distributional information is the main subject of compositional distributional semantics (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012, to name a few studies).",1.2 Compositional distributional semantics,[0],[0]
"The fundamental principles of compositional distributional semantics, henceforth referred to as CDS, are mainly propagated with papers written on the topic.",1.2 Compositional distributional semantics,[0],[0]
"Apart from the papers, it was the SemEval-2014 Shared Task 1
2As the principle of compositionality is attributed to Gottlob Frege, it is often called Frege’s principle.
784
(Marelli et al., 2014) that essentially contributed to the expansion of CDS and increased an interest in this domain.",1.2 Compositional distributional semantics,[0],[0]
The goal of the task was to evaluate CDS models of English in terms of semantic relatedness and entailment on proper sentences from the SICK corpus.,1.2 Compositional distributional semantics,[0],[0]
"The SICK corpus (Bentivogli et al., 2014) consists of 10K pairs of English sentences containing multiple lexical, syntactic, and semantic phenomena.",1.3 The SICK corpus,[0],[0]
"It builds on two external data sources – the 8K ImageFlickr dataset (Rashtchian et al., 2010) and SemEval-2012 Semantic Textual Similarity dataset (Agirre et al., 2012).",1.3 The SICK corpus,[0],[0]
"Each sentence pair is human-annotated for relatedness in meaning and entailment.
",1.3 The SICK corpus,[0],[0]
The relatedness score corresponds to the degree of semantic relatedness between two sentences and is calculated as the average of ten human ratings collected for this sentence pair on the 5-point Likert scale.,1.3 The SICK corpus,[0],[0]
"This score indicates the extent to which the meanings of two sentences are related.
",1.3 The SICK corpus,[0],[0]
"The entailment relation between two sentences, in turn, is labelled with entailment, contradiction, or neutral.",1.3 The SICK corpus,[0],[0]
"According to the SICK guidelines, the label assigned by the majority of human annotators is selected as the valid entailment label.",1.3 The SICK corpus,[0],[0]
"Studying approaches to various natural language processing (henceforth NLP) problems, we have observed that the availability of language resources (e.g. training or testing data) stimulates the development of NLP tools and the estimation of NLP models.",1.4 Motivation and organisation of the paper,[0],[0]
English is undoubtedly the most prominent in this regard and English resources are the most numerous.,1.4 Motivation and organisation of the paper,[0],[0]
"Therefore, NLP methods are mostly designed for English and tested on English data, even if there is no guarantee that they are universal.",1.4 Motivation and organisation of the paper,[0],[0]
"In order to verify whether an NLP algorithm is adequate, it is not enough to evaluate it solely for English.",1.4 Motivation and organisation of the paper,[0],[0]
It is also valuable to have high-quality resources for languages typologically different to English.,1.4 Motivation and organisation of the paper,[0],[0]
"Hence, we aim at building datasets for the evaluation of CDS models in languages other than English, which are often underresourced.",1.4 Motivation and organisation of the paper,[0],[0]
"We strongly believe that the availability of test data will encourage development of CDS models in these languages and allow to better test the universality of CDS methods.
",1.4 Motivation and organisation of the paper,[0],[0]
"We start with a high-quality dataset for Polish, which is a completely different language than English in at least two dimensions.",1.4 Motivation and organisation of the paper,[0],[0]
"First, it is a rather under-resourced language in contrast to the resource-rich English.",1.4 Motivation and organisation of the paper,[0],[0]
"Second, it is a fusional language with a relatively free word order in contrast to the isolated English with a relatively fixed word order.",1.4 Motivation and organisation of the paper,[0],[0]
"If some heuristics is tested on e.g. Polish, the evaluation results can be approximately generalised to other Slavic languages.",1.4 Motivation and organisation of the paper,[0],[0]
"We hope the Slavic NLP community will be interested in designing and evaluating methods of semantic modelling for Slavic languages.
",1.4 Motivation and organisation of the paper,[0],[0]
The procedure of building an evaluation dataset for validating compositional distributional semantics models of Polish generally builds on steps designed to assemble the SICK corpus (described in Section 1.3) because we aim at building an evaluation dataset which is comparable to the SICK corpus.,1.4 Motivation and organisation of the paper,[0],[0]
"However, the implementation of particular building steps significantly differs from the original SICK design assumptions, which is caused by both lack of necessary extraneous resources for Polish (see Section 2.1) and the need for Polish-specific transformation rules (see Section 2.2).",1.4 Motivation and organisation of the paper,[0],[0]
"Furthermore, the rules of arranging sentences into pairs (see Section 2.3) are defined anew taking into account the characteristic of data and bidirectional entailment annotations, since an entailment relation between two sentences must not be symmetric.",1.4 Motivation and organisation of the paper,[0],[0]
"Even if our assumptions of annotating sentence pairs coincide with the SICK principles to a certain extent (see Section 3.1), the annotation process differs from the SICK procedure, in particular by introducing an element of human verification of correctness of automatically transformed sentences (see Section 3.2) and some additional post-corrections (see Section 3.3).",1.4 Motivation and organisation of the paper,[0],[0]
"Finally, a summary of the dataset is provided in Section 4.1 and the dataset evaluation is given in Section 4.2.",1.4 Motivation and organisation of the paper,[0],[0]
"The first step of building the SICK corpus consisted in the random selection of English sentence pairs from existing datasets (Rashtchian et al., 2010; Agirre et al., 2012).",2.1 Selection and description of images,[0],[0]
"Since we are not aware of accessibility of analogous resources for Polish, we have to select images first and then describe the selected images.
",2.1 Selection and description of images,[0],[0]
"Images are selected from the 8K ImageFlickr
dataset (Rashtchian et al., 2010).",2.1 Selection and description of images,[0],[0]
At first we wanted to take only these images the descriptions of which were selected for the SICK corpus.,2.1 Selection and description of images,[0],[0]
"However, a cursory check shows that these images are quite homogeneous, with a predominant number of dogs depictions.",2.1 Selection and description of images,[0],[0]
"Therefore, we independently extract 1K images and split them into 46 thematic groups (e.g. children, musical instruments, motorbikes, football, dogs).",2.1 Selection and description of images,[0],[0]
The numbers of images within individual thematic groups vary from 6 images in the volleyball and telephoning groups to 94 images in the various people group.,2.1 Selection and description of images,[0],[0]
"The second largest groups are children and dogs with 50 images each.
",2.1 Selection and description of images,[0],[0]
The chosen images are given to two authors who independently of each other formulate their descriptions based on a short instruction.,2.1 Selection and description of images,[0],[0]
The authors are instructed to write one single sentence (with a sentence predicate) describing the action in a displayed image.,2.1 Selection and description of images,[0],[0]
They should not describe an imaginable context or an interpretation of what may lie behind the scene in the picture.,2.1 Selection and description of images,[0],[0]
"If some details in the picture are not obvious, they should not be described either.",2.1 Selection and description of images,[0],[0]
"Furthermore, the authors should avoid multiword expressions, such as idioms, metaphors, and named entities, because those are not compositional linguistic phenomena.",2.1 Selection and description of images,[0],[0]
"Finally, descriptions should contain Polish diacritics and proper punctuation.",2.1 Selection and description of images,[0],[0]
"The second step of building the SICK corpus consisted in pre-processing extracted sentences, i.e. normalisation and expansion (Bentivogli et al., 2014, p. 3–4).",2.2 Transformation of descriptions,[0],[0]
"Since the authors of Polish descriptions are asked to follow the guidelines (presented in Section 2.1), the normalisation step is not essential for our data.",2.2 Transformation of descriptions,[0],[0]
"The expansion step, in turn, is implemented and the sentences provided by the authors are lexically and syntactically transformed in order to obtain derivative sentences with similar, contrastive, or neutral meanings.",2.2 Transformation of descriptions,[0],[0]
"The following transformations are implemented:
1.",2.2 Transformation of descriptions,[0],[0]
"dropping conjunction concerns sentences with coordinated predicates sharing a subject, e.g. Rowerzysta odpoczywa i obserwuje morze.",2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
‘A cyclist is resting and watching the sea.’).,2.2 Transformation of descriptions,[0],[0]
"The finite form of one of the coordinated predicates is transformed into:
• an active adjectival participle, e.g. Odpoczywający rowerzysta obserwuje
morze.",2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
‘A resting cyclist is watching the sea.’) or Obserwujący morze rowerzysta odpoczywa.,2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
"‘A cyclist, who is watching the sea, is resting.’), • a contemporary adverbial participle,
e.g. Rowerzysta, odpoczywając, obserwuje morze.",2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
"‘A cyclist is watching the sea, while resting.’)",2.2 Transformation of descriptions,[0],[0]
"or Rowerzysta odpoczywa, obserwując morze.",2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
"‘A cyclist is resting, while watching the sea.’).
",2.2 Transformation of descriptions,[0],[0]
"2. removing conjunct in adjuncts, i.e. the deletion of one of coordinated elements of an adjunct, e.g. Mały, ale zwinny kot miauczy.",2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
‘A small but agile cat miaows.’) can be changed into either Mały kot miauczy.,2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
‘A small cat miaows.’) or Zwinny kot miauczy.,2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
"‘An agile cat miaows.’).
",2.2 Transformation of descriptions,[0],[0]
3.,2.2 Transformation of descriptions,[0],[0]
"passivisation, e.g. Człowiek ujeżdża byka.",2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
‘A man is breaking a bull in.’) can be transformed into Byk jest ujeżdżany przez człowieka.,2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
"‘A bull is being broken in by a man.’).
",2.2 Transformation of descriptions,[0],[0]
"4. removing adjuncts, e.g. Dwa białe króliki siedzą na trawie.",2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
‘Two small rabbits are sitting on the grass.’) can be changed into Króliki,2.2 Transformation of descriptions,[0],[0]
siedzą.,2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
"‘The rabbits are sitting.’).
",2.2 Transformation of descriptions,[0],[0]
"5. swapping relative clause for participles, i.e. a relative clause swaps with a participle (and vice versa), e.g. Kobieta przytula psa, którego trzyma na smyczy.",2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
‘A woman hugs a dog which she keeps on a leash.’).,2.2 Transformation of descriptions,[0],[0]
"The relative clause is interchanged for a participle construction, e.g. Kobieta przytula trzymanego na smyczy psa.",2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
"‘A woman hugs a dog kept on a leash.’).
6.",2.2 Transformation of descriptions,[0],[0]
"negation, e.g. Mężczyźni w turbanach na głowach siedzą na słoniach.",2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
‘Men in turbans on their heads are sitting on elephants.’) can be transformed into Nikt nie siedzi na słoniach.,2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
"‘Nobody is sitting on elephants.’), Żadni mężczyźni w turbanach na głowach nie siedzą na słoniach.",2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
"‘No men in turbans on their heads are sitting on elephants.’), and Mężczyźni w turbanach na głowach nie siedzą na słoniach.",2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
"‘Men in turbans on their heads are not sitting on elephants.’).
",2.2 Transformation of descriptions,[0],[0]
7.,2.2 Transformation of descriptions,[0],[0]
constrained mixing of dependents from various sentences,2.2 Transformation of descriptions,[0],[0]
", e.g. Dwoje dzieci siedzi na wielbłądach w pobliżu wysokich gór.",2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
‘Two children are sitting on camels near high mountains.’) can be changed into Dwoje dzieci siedzi przy zastawionym stole w pobliżu wysokich gór.,2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
"‘Two children are sitting at the table laid with food near high mountains.’).
",2.2 Transformation of descriptions,[0],[0]
"The first five transformations are designed to produce sentences with a similar meaning, the sixth transformation outputs sentences with a contradictory meaning, and the seventh transformation should generate sentences with a neutral (or unrelated) meaning.",2.2 Transformation of descriptions,[0],[0]
"All transformations are performed on the dependency structures of input sentences (Wróblewska, 2014).
",2.2 Transformation of descriptions,[0],[0]
Some of the transformations are very productive (e.g. mixing dependents).,2.2 Transformation of descriptions,[0],[0]
"Other, in turn, are sparsely represented in the output (e.g. dropping conjunction).",2.2 Transformation of descriptions,[0],[0]
The number of transformed sentences randomly selected to build the dataset is in the second column of Table 1.,2.2 Transformation of descriptions,[0],[0]
The final step of building the SICK corpus consisted in arranging normalised and expanded sentences into pairs.,2.3 Data ensemble,[0],[0]
"Since our data diverges from SICK data, the process of arranging Polish sentences into pairs also differs from pairing in the SICK corpus.",2.3 Data ensemble,[0],[0]
The general idea behind the pair-ensembling procedure was to introduce sentence pairs with different levels of relatedness into the dataset.,2.3 Data ensemble,[0],[0]
"Apart from pairs connecting two sentences originally written by humans (as described in Section 2.1), there are also pairs in which an original sentence is connected with
a transformed sentence.",2.3 Data ensemble,[0],[0]
"For each of the 1K images, the following 10 pairs are constructed (for A being the set of all sentences originally written by the first author, B being the set of all sentences originally written by the second author, a ∈ A and b ∈ B being the original descriptions of the picture):
1.",2.3 Data ensemble,[0],[0]
"(a,b)
2.",2.3 Data ensemble,[0],[0]
"(a,a1), where a1 ∈ t(a), and t(a) is the set of all transformations of the sentence a
3.",2.3 Data ensemble,[0],[0]
"(b,b1), where b1 ∈ t(b)
4.",2.3 Data ensemble,[0],[0]
"(a,b2), where b2 ∈ t(b)
5.",2.3 Data ensemble,[0],[0]
"(b,a2), where a2 ∈ t(a)
6.",2.3 Data ensemble,[0],[0]
"(a,a3), where a3 ∈ t(a′),a′ ∈",2.3 Data ensemble,[0],[0]
"A, T (a′) = T (a),a′ 6= a, for T (a) being the thematic group3 of a
7.",2.3 Data ensemble,[0],[0]
"(b,b3), where b3 ∈ t(b′),b′ ∈ B, T (b′)",2.3 Data ensemble,[0],[0]
"= T (b),b′ 6= b
8.",2.3 Data ensemble,[0],[0]
"(a,a4), where a4 ∈ A, T (a4) 6=",2.3 Data ensemble,[0],[0]
"T (a)4
9.",2.3 Data ensemble,[0],[0]
"(b,b4), where b4 ∈ B, T (b4) 6=",2.3 Data ensemble,[0],[0]
"T (b)
10.",2.3 Data ensemble,[0],[0]
"(a,a5), where a5 ∈",2.3 Data ensemble,[0],[0]
"t(a),a5 6=",2.3 Data ensemble,[0],[0]
"a1 for 50% images, (b,b5) (analogously) for other 50%.5
For each sentence pair (a,b) created according to this procedure, its reverse (b,a) is also included in our corpus.",2.3 Data ensemble,[0],[0]
"As a result, the working set consists of 20K sentence pairs.",2.3 Data ensemble,[0],[0]
The degree of semantic relatedness between two sentences is calculated as the average of all human ratings on the Likert scale with the range from 0 to 5.,3.1 Annotation assumptions,[0],[0]
"Since we do not want to excessively influence
3The thematic group of a sentence a corresponds to the thematic group of an image being the source of a (as described in Section 2.1).
4The pairs (a,a4) of the same authors’ descriptions of two images from different thematic groups are expected to be unrelated.",3.1 Annotation assumptions,[0],[0]
"The same applies to (b,b4).
",3.1 Annotation assumptions,[0],[0]
5A repetition of point 2 with a restriction that a different pair is created (pairs of very related sentences are expected).,3.1 Annotation assumptions,[0],[0]
"We alternate between authors A and B to obtain equal author proportions in the final ensemble of pairs.
",3.1 Annotation assumptions,[0],[0]
"the annotations, the guidelines given to annotators are mainly example-based:6
• 5 (very related): Kot siedzi na płocie.",3.1 Annotation assumptions,[0],[0]
(Eng.,3.1 Annotation assumptions,[0],[0]
‘A cat is sitting on the fence.’),3.1 Annotation assumptions,[0],[0]
vs. Na płocie jest duży kot.,3.1 Annotation assumptions,[0],[0]
(Eng.,3.1 Annotation assumptions,[0],[0]
"‘There is a large cat on the fence.’),
• 1–4 (more or less related): Kot siedzi na płocie.",3.1 Annotation assumptions,[0],[0]
(Eng.,3.1 Annotation assumptions,[0],[0]
‘A cat is sitting on the fence.’),3.1 Annotation assumptions,[0],[0]
vs. Kot nie siedzi na płocie.,3.1 Annotation assumptions,[0],[0]
(Eng.,3.1 Annotation assumptions,[0],[0]
‘A cat is not sitting on the fence.’); Kot siedzi na płocie.,3.1 Annotation assumptions,[0],[0]
(Eng.,3.1 Annotation assumptions,[0],[0]
‘A cat is sitting on the fence.’),3.1 Annotation assumptions,[0],[0]
vs. Właściciel dał kotu chrupki.,3.1 Annotation assumptions,[0],[0]
(Eng.,3.1 Annotation assumptions,[0],[0]
‘The owner gave kibble to his cat.’),3.1 Annotation assumptions,[0],[0]
; Kot siedzi na płocie.,3.1 Annotation assumptions,[0],[0]
(Eng.,3.1 Annotation assumptions,[0],[0]
‘A cat is sitting on the fence.’),3.1 Annotation assumptions,[0],[0]
vs. Kot miauczy pod płotem.,3.1 Annotation assumptions,[0],[0]
(Eng.,3.1 Annotation assumptions,[0],[0]
"‘A cat miaows by the fence.’).
",3.1 Annotation assumptions,[0],[0]
• 0 (unrelated): Kot siedzi na płocie.,3.1 Annotation assumptions,[0],[0]
(Eng.,3.1 Annotation assumptions,[0],[0]
‘A cat is sitting on the fence.’),3.1 Annotation assumptions,[0],[0]
vs. Zaczął padać deszcz.,3.1 Annotation assumptions,[0],[0]
(Eng.,3.1 Annotation assumptions,[0],[0]
"‘It started to rain.’).
",3.1 Annotation assumptions,[0],[0]
"Apart from these examples, there is a note in the annotation guidelines indicating that the degree of semantic relatedness is not equivalent to the degree of semantic similarity.",3.1 Annotation assumptions,[0],[0]
"Semantic similarity is only a special case of semantic relatedness, semantic relatedness is thus a more general term than the other one.
",3.1 Annotation assumptions,[0],[0]
"Polish entailment labels correspond directly to the SICK labels (i.e. entailment, contradiction, neutral).",3.1 Annotation assumptions,[0],[0]
The entailment label assigned by the majority of human judges is selected as the gold label.,3.1 Annotation assumptions,[0],[0]
"The entailment labels are defined as follows:
• a wynika z b (b entails a) – if a situation or an event described by sentence b occurs, it is recognised that a situation or an event described by a occurs as well, i.e. a and b refer to the same event or the same situation,
• a jest zaprzeczeniem b (a is the negation of b) – if a situation or an event described by b occurs, it is recognised that a situation or an event described by a may not occur at the same time,
6We realise that the boundary between semantic perception of a sentence by various speakers is fuzzy (it depends on speakers’ education, origin, age, etc.).",3.1 Annotation assumptions,[0],[0]
"It was thus our wellthought-out decision to draw only general annotation frames and to enable annotators to rely on their feel for language.
",3.1 Annotation assumptions,[0],[0]
• a jest neutralne wobec b,3.1 Annotation assumptions,[0],[0]
(a is neutral to b) – the truth of a situation described by a cannot be determined on the basis of b.,3.1 Annotation assumptions,[0],[0]
"Similar to the SICK corpus, each Polish sentence pair is human-annotated for semantic relatedness and entailment by 3 human judges experienced in Polish linguistics.7 Since for each annotated pair (a,b), its reverse (b,a) is also subject to annotation, the entailment relation is in practice determined ‘in both directions’ for 10K sentence pairs.",3.2 Annotation procedure,[0],[0]
"For the task of relatedness annotation, the order of sentences within pairs seems to be irrelevant, we can thus assume to obtain 6 relatedness scores for 10K unique pairs.
",3.2 Annotation procedure,[0],[0]
"Since the transformation process is fully automatic and to a certain extent based on imperfect dependency parsing, we cannot ignore errors in the transformed sentences.",3.2 Annotation procedure,[0],[0]
"In order to avoid annotating erroneous sentences, the annotation process is divided into two stages:
1.",3.2 Annotation procedure,[0],[0]
"a sentence pair is sent to a judge with the leader role, who is expected to edit and to correct the transformed sentence from this pair before annotation, if necessary,
2.",3.2 Annotation procedure,[0],[0]
"the verified and possibly enhanced sentence pair is sent to the other two judges, who can only annotate it.
",3.2 Annotation procedure,[0],[0]
The leader judges should correct incomprehensible and ungrammatical sentences with a minimal number of necessary changes.,3.2 Annotation procedure,[0],[0]
Unusual sentences which could be accepted by Polish speakers should not be modified.,3.2 Annotation procedure,[0],[0]
"Moreover, the modified sentence may not be identical with the other sentence in the pair.",3.2 Annotation procedure,[0],[0]
"The classification and statistics of distinct corrections made by the leader judges are provided in Table 2.
",3.2 Annotation procedure,[0],[0]
A strict classification of error types is quite hard to provide because some sentences contain more than one error.,3.2 Annotation procedure,[0],[0]
We thus order the error types from the most serious errors (i.e. ‘sense’ errors) to the redundant corrections (i.e. ‘other’ type).,3.2 Annotation procedure,[0],[0]
"If a sentence contains several errors, it is qualified for the higher order error type.
",3.2 Annotation procedure,[0],[0]
"In the case of sentences with ‘sense’ errors, the need for correction is uncontroversial and
7Our annotators have relatively strong linguistic background.",3.2 Annotation procedure,[0],[0]
"Five of them have PhD in linguistics, five are PhD students, one is a graduate, and one is an undergraduate.
arises from an internal logical contradiction.8",3.2 Annotation procedure,[0],[0]
"The sentences with ‘semantic’ changes are syntactically correct, but deemed unacceptable by the leader annotators from the semantic or pragmatic point of view.9 The ‘grammatical’ errors mostly concern missing agreement.10 The majority of ‘word order’ corrections are unnecessary, but we found some examples which can be classified as actual word or phrase order errors.11 The correction of punctuation consists in adding or deleting a comma.12 The sentences in the ‘other’ group, in turn, could as well have been left unchanged because they are proper Polish sentences, but were apparently considered odd by the leader annotators.
",3.2 Annotation procedure,[0],[0]
8An example of ‘sense’ error: the sentence Chłopak w zielonej bluzie,3.2 Annotation procedure,[0],[0]
i czapce zjeżdża na rolkach na leżąco.,3.2 Annotation procedure,[0],[0]
(Eng.,3.2 Annotation procedure,[0],[0]
‘A boy in a green sweatshirt and a cap roller-skates downhill in a lying position.’) is corrected into Chłopak w zielonej bluzie,3.2 Annotation procedure,[0],[0]
i czapce zjeżdża na rolkach.,3.2 Annotation procedure,[0],[0]
(Eng.,3.2 Annotation procedure,[0],[0]
"‘A boy in a green sweatshirt and a cap roller-skates downhill.’).
",3.2 Annotation procedure,[0],[0]
9An example of ‘semantic’ correction: the sentence Dziewczyna trzyma w pysku patyk.,3.2 Annotation procedure,[0],[0]
(Eng.,3.2 Annotation procedure,[0],[0]
‘A girl holds a stick in her muzzle.’) is corrected into Dziewczyna trzyma w ustach patyk.,3.2 Annotation procedure,[0],[0]
(Eng.,3.2 Annotation procedure,[0],[0]
"‘A girl holds a stick in her mouth.’).
",3.2 Annotation procedure,[0],[0]
10An example of ‘grammatical’ error: the sentence Grupasg.nom uśmiechających się ludzi tańcząpl.,3.2 Annotation procedure,[0],[0]
(Eng.,3.2 Annotation procedure,[0],[0]
*‘A group of smiling people are dancing.’) is corrected into Grupasg.nom uśmiechających się ludzi tańczysg .,3.2 Annotation procedure,[0],[0]
(Eng.,3.2 Annotation procedure,[0],[0]
"‘A group of smiling people is dancing.’).
",3.2 Annotation procedure,[0],[0]
"11An example of word order error: the sentence Samochód, który jest uszkodzony, koloru białego stoi na lawecie dużego auta.",3.2 Annotation procedure,[0],[0]
(lit.,3.2 Annotation procedure,[0],[0]
"‘A car that is damaged, of the white color stands on the trailer of a large car.’, Eng.",3.2 Annotation procedure,[0],[0]
‘A white car that is damaged is standing on the trailer of a large car.’),3.2 Annotation procedure,[0],[0]
"is corrected into Samochód koloru białego, który jest uszkodzony, stoi na lawecie dużego auta.
12An example of punctuation correction: the wrong comma in the sentence Nad brzegiem wody, stoją dwaj mężczyźni z wędkami.",3.2 Annotation procedure,[0],[0]
(lit.,3.2 Annotation procedure,[0],[0]
"‘On the water’s edge, two men are standing with rods.’; Eng.",3.2 Annotation procedure,[0],[0]
"‘Two men with rods are standing on the water’s edge.’) should be deleted, i.e. Nad brzegiem wody stoją dwaj mężczyźni z wędkami.",3.2 Annotation procedure,[0],[0]
During the annotation process it came out that sentences accepted by some human annotators are unacceptable for other annotators.,3.3 Impromptu post-corrections,[0],[0]
We thus decided to garner annotators’ comments and suggestions for improving sentences.,3.3 Impromptu post-corrections,[0],[0]
"After validation of these suggestions by an experienced linguist, it turns out that most of these proposals concern punctuation errors (e.g. missing comma) and typos in 312 distinct sentences.",3.3 Impromptu post-corrections,[0],[0]
These errors are fixed directly in the corpus because they should not impact the annotations of sentence pairs.,3.3 Impromptu post-corrections,[0],[0]
The other suggestions concern more significant changes in 29 distinct sentences (mostly minor grammatical or semantic problems overlooked by the leader annotators).,3.3 Impromptu post-corrections,[0],[0]
The annotations of pairs with modified sentences are resent to the annotators so that they can verify and update them.,3.3 Impromptu post-corrections,[0],[0]
Tables 3 and 4 summarise the annotations of the resulting 10K sentence pairs corpus.,4.1 Corpus statistics,[0],[0]
"Table 3 aggregates the occurrences of 6 possible relatedness scores, calculated as the mean of all 6 individual annotations, rounded to an integer.
",4.1 Corpus statistics,[0],[0]
Table 4 shows the number of the particular entailment labels in the corpus.,4.1 Corpus statistics,[0],[0]
"Since each sentence pair is annotated for entailment in both directions, the final entailment label is actually a pair of two labels:
• entailment+neutral points to ‘one-way’ entailment,
• contradiction+neutral points to ‘one-way’ contradiction,
• entailment+entailment, contradiction+contradiction, and neutral+neutral point to equivalence.
",4.1 Corpus statistics,[0],[0]
"While the actual corpus labels are ordered in the sense that there is a difference between e.g. entailment+neutral and neutral+entailment (the entailment occurs in different directions), we treat all labels as unordered for the purpose of this summary (e.g. entailment+neutral covers neutral+entailment as well, representing the same type of relation between two sentences).",4.1 Corpus statistics,[0],[0]
"The standard measure of inter-annotator agreement in various natural language labelling tasks is Cohen’s kappa (Cohen, 1960).",4.2 Inter-annotator agreement,[0],[0]
"However, this coefficient is designed to measure agreement between two annotators only.",4.2 Inter-annotator agreement,[0],[0]
"Since there are three annotators of each pair of ordered sentences, we decided to apply Fleiss’ kappa13 (Fleiss, 1971) designed for measuring agreement between multiple raters who give categorical ratings to a fixed number of items.",4.2 Inter-annotator agreement,[0],[0]
"An additional advantage of this measure is that different items can be rated by different human judges, which doesn’t impact measurement.",4.2 Inter-annotator agreement,[0],[0]
"The normalised Fleiss’ measure of inter-annotator agreement is:
κ = P̄",4.2 Inter-annotator agreement,[0],[0]
"− P̄e 1− P̄e
where the quantity P̄ − P̄e measures the degree of agreement actually attained in excess of chance, while “[t]he quantity 1 − P̄e measures the degree of agreement attainable over and above what would be predicted by chance” (Fleiss, 1971, p. 379).
",4.2 Inter-annotator agreement,[0],[0]
We recognise Fleiss’ kappa as particularly useful for measuring inter-annotator agreement with respect to entailment labelling in our evaluation dataset.,4.2 Inter-annotator agreement,[0],[0]
"First, there are more than two raters.",4.2 Inter-annotator agreement,[0],[0]
"Second, entailment labels are categorically.",4.2 Inter-annotator agreement,[0],[0]
"Measured
13As Fleiss’ kappa is actually the generalisation of Scott’s π (Scott, 1955), it is sometimes referred to as Fleiss’ multi-π, cf.",4.2 Inter-annotator agreement,[0],[0]
"Artstein and Poesio (2008).
with Fleiss’ kappa, there is an inter-annotator agreement of κ = 0.734 for entailment labels in Polish evaluation dataset, which is quite satisfactory as for a semantic labelling task.
",4.2 Inter-annotator agreement,[0],[0]
"Relative to semantic relatedness, the distinction in meaning of two sentences made by human judges is often very subtle.",4.2 Inter-annotator agreement,[0],[0]
This is also reflected in the inter-annotator agreement scores measured with Fleiss’ kappa.,4.2 Inter-annotator agreement,[0],[0]
Inter-annotator agreement measured for six semantic relatedness groups corresponding to points on the Likert scale is quite low: κ = 0.337.,4.2 Inter-annotator agreement,[0],[0]
"If we measure interannotator agreement for three classes corresponding to the three relatedness groups from the annotation guidelines (see Section 3.1), i.e. <0>, <1, 2, 3, 4>, and <5>, the Fleiss’ score is significantly higher: κ = 0.543.",4.2 Inter-annotator agreement,[0],[0]
"Hence, we conclude that Fleiss’ kappa is not a reliable measure of inter-annotator agreement in relation to relatedness scores.",4.2 Inter-annotator agreement,[0],[0]
"Therefore, we decided to use Krippendorff’s α instead.
",4.2 Inter-annotator agreement,[0],[0]
"Krippendorff’s α (Krippendorff, 1980, 2013) is a coefficient appropriate for measuring the interannotator agreement of a dataset which is annotated with multiple judges and characterised by different magnitudes of disagreement and missing values.",4.2 Inter-annotator agreement,[0],[0]
"Krippendorff proposes distance metrics suitable for various scales: binary, nominal, interval, ordinal, and ratio.",4.2 Inter-annotator agreement,[0],[0]
"In ordinal measurement14 the attributes can be rank-ordered, but distances between them do not have any meaning.",4.2 Inter-annotator agreement,[0],[0]
"Measured with Krippendorff’s ordinal α, there is an inter-annotator agreement of α = 0.780 for relatedness scores in the Polish evaluation dataset, which is quite satisfactory as well.",4.2 Inter-annotator agreement,[0],[0]
"Hence, we conclude that our dataset is a reliable resource for the purpose of evaluating compositional distributional semantics model of Polish.",4.2 Inter-annotator agreement,[0],[0]
The goal of this paper is to present the procedure of building a Polish evaluation dataset for the validation of compositional distributional semantics models.,5 Conclusions,[0],[0]
"As we aim at building an evalua-
14Nominal measurement is useless for measuring agreement between relatedness scores (α = 0.340 is the identical value as Fleiss’ kappa, since all disagreements are considered equal).",5 Conclusions,[0],[0]
"We also test interval measurement, in which the distance between the attributes does have meaning and an average of an interval variable is computed.",5 Conclusions,[0],[0]
"The interval score measured for relatedness annotations is quite high α = 0.785, but we doubt whether the distance between relatedness scores is meaningful in this case.
tion dataset which is comparable to the SICK corpus, the general assumptions of our procedure correspond to the design principles of the SICK corpus.",5 Conclusions,[0],[0]
"However, the procedure of building the SICK corpus cannot be adapted without modifications.",5 Conclusions,[0],[0]
"First, the Polish seed-sentences have to be written based on the images which are selected from 8K ImageFlickr dataset and split into thematic groups, since usable datasets are not publicly available.",5 Conclusions,[0],[0]
"Second, since the process of transforming sentences seems to be language-specific, the linguistic transformation rules appropriate for Polish have to be defined from scratch.",5 Conclusions,[0],[0]
"Third, the process of arranging Polish sentences into pairs is defined anew taking into account the data characteristic and bidirectional entailment annotations.",5 Conclusions,[0],[0]
The discrepancies relative to the SICK procedure also concern the annotation process itself.,5 Conclusions,[0],[0]
"Since an entailment relation between two sentences must not be symmetric, each sentence pair is annotated for entailment in both directions.",5 Conclusions,[0],[0]
"Furthermore, we introduce an element of human verification of correctness of automatically transformed sentences and some additional post-corrections.
",5 Conclusions,[0],[0]
The presented procedure of building a dataset was tested on Polish.,5 Conclusions,[0],[0]
"However, it is very likely that the annotation framework will work for other Slavic languages (e.g. Czech with an excellent dependency parser).
",5 Conclusions,[0],[0]
"The presented procedure results in building the Polish test corpus of relatively high quality, confirmed by the inter-annotator agreement coefficients of κ = 0.734 (measured with Fleiss’ kappa) for entailment labels and of α = 0.780 (measured with Krippendorff’s ordinal alpha) for relatedness scores.",5 Conclusions,[0],[0]
"We would like to thank the reliable and tenacious annotators of our dataset: Alicja DziedzicRawska, Bożena Itoya, Magdalena Król, Anna Latusek, Justyna Małek, Małgorzata Michalik, Agnieszka Norwa, Małgorzata Szajbel-Keck, Alicja Walichnowska, Konrad Zieliński, and some other.",Acknowledgments,[0],[0]
The research presented in this paper was supported by SONATA 8 grant no 2014/15/D/HS2/03486 from the National Science Centre Poland.,Acknowledgments,[0],[0]
The paper presents a procedure of building an evaluation dataset1.,abstractText,[0],[0]
for the validation of compositional distributional semantics models estimated for languages other than English.,abstractText,[0],[0]
"The procedure generally builds on steps designed to assemble the SICK corpus, which contains pairs of English sentences annotated for semantic relatedness and entailment, because we aim at building a comparable dataset.",abstractText,[0],[0]
"However, the implementation of particular building steps significantly differs from the original SICK design assumptions, which is caused by both lack of necessary extraneous resources for an investigated language and the need for language-specific transformation rules.",abstractText,[0],[0]
"The designed procedure is verified on Polish, a fusional language with a relatively free word order, and contributes to building a Polish evaluation dataset.",abstractText,[0],[0]
The resource consists of 10K sentence pairs which are human-annotated for semantic relatedness and entailment.,abstractText,[0],[0]
The dataset may be used for the evaluation of compositional distributional semantics models of Polish.,abstractText,[0],[0]
Polish evaluation dataset for compositional distributional semantics models,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 667–672 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
667",text,[0],[0]
"The standard approach to multilingual NLP is to design a single architecture, but tune and train a separate model for each language.",1 Introduction,[0],[0]
"While this method allows for customizing the model to the particulars of each language and the available data, it also presents a problem when little data is available: extensive language-specific annotation is required.",1 Introduction,[0],[0]
"The reality is that most languages have very little annotated data for most NLP tasks.
",1 Introduction,[0],[0]
"Ammar et al. (2016a) found that using training data from multiple languages annotated with Universal Dependencies (Nivre et al., 2016), and represented using multilingual word vectors, outperformed monolingual training.",1 Introduction,[0],[0]
"Inspired by this, we apply the idea of training one model on multiple languages—which we call polyglot training— to PropBank-style semantic role labeling (SRL).",1 Introduction,[0],[0]
"We train several parsers for each language in the CoNLL 2009 dataset (Hajič et al., 2009): a tra-
I think Peter even made some deals with the gorillas .",1 Introduction,[0],[0]
O,1 Introduction,[0],[0]
O A0 AM-ADV O,1 Introduction,[0],[0]
"O A1 AM-ADV O O
Pero el suizo difícilmente atacará a Rominger en la montaña .",1 Introduction,[0],[0]
O,1 Introduction,[0],[0]
"O arg0-agt argM-adv O O arg1-pat argM-loc O O
Četrans oslovil sedm velkých evropských výrobců nákladních automobilů.",1 Introduction,[0],[0]
O O RSTR RSTR RSTR,1 Introduction,[0],[0]
"O O PAT
Figure 1:",1 Introduction,[0],[0]
"Example predicate-argument structures from English, Spanish, and Czech.",1 Introduction,[0],[0]
"Note that the argument labels are different in each language.
ditional monolingual version, and variants which additionally incorporate supervision from English portion of the dataset.",1 Introduction,[0],[0]
"To our knowledge, this is the first multilingual SRL approach to combine supervision from several languages.
",1 Introduction,[0],[0]
"The CoNLL 2009 dataset includes seven different languages, allowing study of trends across the same.",1 Introduction,[0],[0]
"Unlike the Universal Dependencies dataset, however, the semantic label spaces are entirely language-specific, making our task more challenging.",1 Introduction,[0],[0]
"Nonetheless, the success of polyglot training in this setting demonstrates that sharing of statistical strength across languages does not depend on explicit alignment in annotation conventions, and can be done simply through parameter sharing.",1 Introduction,[0],[0]
"We show that polyglot training can result in better labeling accuracy than a monolingual parser, especially for low-resource languages.",1 Introduction,[0],[0]
We find that even a simple combination of data is as effective as more complex kinds of polyglot training.,1 Introduction,[0],[0]
We include a breakdown into label categories of the differences between the monolingual and polyglot models.,1 Introduction,[0],[0]
Our findings indicate that polyglot training consistently improves label accuracy for common labels.,1 Introduction,[0],[0]
"We evaluate our system on the semantic role labeling portion of the CoNLL-2009 shared task (Hajič et al., 2009), on all seven languages, namely Catalan, Chinese, Czech, English, German, Japanese and Spanish.",2 Data,[0],[0]
"For each language, certain tokens in each sentence in the dataset are marked as predicates.",2 Data,[0],[0]
"Each predicate takes as arguments other words in the same sentence, their relationship marked by labeled dependency arcs.",2 Data,[0],[0]
"Sentences may contain no predicates.
",2 Data,[0],[0]
"Despite the consistency of this format, there are significant differences between the training sets across languages.1 English uses PropBank role labels (Palmer et al., 2005).",2 Data,[0],[0]
"Catalan, Chinese, English, German, and Spanish include (but are not limited to) labels such as “arg0-agt” (for “agent”) or “A0” that may correspond to some degree to each other and to the English roles.",2 Data,[0],[0]
"Catalan and Spanish share most labels (being drawn from the same source corpus, AnCora; Taulé et al., 2008), and English and German share some labels.",2 Data,[0],[0]
"Czech and Japanese each have their own distinct sets of argument labels, most of which do not have clear correspondences to English or to each other.
",2 Data,[0],[0]
"We also note that, due to semi-automatic projection of annotations to construct the German dataset, more than half of German sentences do not include labeled predicate and arguments.",2 Data,[0],[0]
"Thus while German has almost as many sentences as Czech, it has by far the fewest training examples (predicate-argument structures); see Table 1.
",2 Data,[0],[0]
"1This is expected, as the datasets were annotated independently under diverse formalisms and only later converted into CoNLL format (Hajič et al., 2009).",2 Data,[0],[0]
"Given a sentence with a marked predicate, the CoNLL 2009 shared task requires disambiguation of the sense of the predicate, and labeling all its dependent arguments.",3 Model,[0],[0]
"The shared task assumed predicates have already been identified, hence we do not handle the predicate identification task.
",3 Model,[0],[0]
Our basic model adapts the span-based dependency SRL model of He et al. (2017).,3 Model,[0],[0]
This adaptation treats the dependent arguments as argument spans of length 1.,3 Model,[0],[0]
"Additionally, BIO consistency constraints are removed from the original model— each token is tagged simply with the argument label or an empty tag.",3 Model,[0],[0]
"A similar approach has also been proposed by Marcheggiani et al. (2017).
",3 Model,[0],[0]
The input to the model consists of a sequence of pretrained embeddings for the surface forms of the sentence tokens.,3 Model,[0],[0]
Each token embedding is also concatenated with a vector indicating whether the word is a predicate or not.,3 Model,[0],[0]
"Since the part-ofspeech tags in the CoNLL 2009 dataset are based on a different tagset for each language, we do not use these.",3 Model,[0],[0]
Each training instance consists of the annotations for a single predicate.,3 Model,[0],[0]
"These representations are then passed through a deep, multilayer bidirectional LSTM (Graves, 2013; Hochreiter and Schmidhuber, 1997) with highway connections (Srivastava et al., 2015).
",3 Model,[0],[0]
"We use the hidden representations produced by the deep biLSTM for both argument labeling and predicate sense disambiguation in a multitask setup; this is a modification to the models of He et al. (2017), who did not handle predicate senses, and of Marcheggiani et al. (2017), who used a separate model.",3 Model,[0],[0]
"These two predictions are made independently, with separate softmaxes over different last-layer parameters; we then combine the losses for each task when training.",3 Model,[0],[0]
"For predicate sense disambiguation, since the predicate has been identified, we choose from a small set of valid predicate senses as the tag for that token.",3 Model,[0],[0]
This set of possible senses is selected based on the training data: we map from lemmatized tokens to predicates and from predicates to the set of all senses of that predicate.,3 Model,[0],[0]
"Most predicates are only observed to have one or two corresponding senses, making the set of available senses at test time quite small (less than five senses/predicate on average across all languages).",3 Model,[0],[0]
"If a particular lemma was not observed in training, we heuristically predict it as the first sense of that predicate.",3 Model,[0],[0]
"For Czech and
Japanese, the predicate sense annotation is simply the lemmatized token of the predicate, giving a one-to-one predicate-“sense” mapping.
",3 Model,[0],[0]
"For argument labeling, every token in the sentence is assigned one of the argument labels, or NULL if the model predicts it is not an argument to the indicated predicate.",3 Model,[0],[0]
We use pretrained word embeddings as input to the model.,3.1 Monolingual Baseline,[0],[0]
"For each of the shared task languages, we produced GloVe vectors (Pennington et al., 2014) from the news, web, and Wikipedia text of the Leipzig Corpora Collection (Goldhahn et al., 2012).2 We trained 300-dimensional vectors, then reduced them to 100 dimensions with principal component analysis for efficiency.",3.1 Monolingual Baseline,[0],[0]
"In the first polyglot variant, we consider multilingual sharing between each language and English by using pretrained multilingual embeddings.",3.2 Simple Polyglot Sharing,[0],[0]
This polyglot model is trained on the union of annotations in the two languages.,3.2 Simple Polyglot Sharing,[0],[0]
"We use stratified sampling to give the two datasets equal effective weight in training, and we ensure that every training instance is seen at least once per epoch.
",3.2 Simple Polyglot Sharing,[0],[0]
Pretrained multilingual embeddings.,3.2 Simple Polyglot Sharing,[0],[0]
"The basis of our polyglot training is the use of pretrained multilingual word vectors, which allow representing entirely distinct vocabularies (such as the tokens of different languages) in a shared representation space, allowing crosslingual learning (Klementiev et al., 2012).",3.2 Simple Polyglot Sharing,[0],[0]
"We produced multilingual embeddings from the monolingual embeddings using the method of Ammar et al. (2016b): for each non-English language, a small crosslingual dictionary and canonical correlation analysis was used to find a transformation of the non-English vectors into the English vector space (Faruqui and Dyer, 2014).
",3.2 Simple Polyglot Sharing,[0],[0]
"Unlike multilingual word representations, argument label sets are disjoint between language pairs, and correspondences are not clearly defined.",3.2 Simple Polyglot Sharing,[0],[0]
"Hence, we use separate label representations for each language’s labels.",3.2 Simple Polyglot Sharing,[0],[0]
"Similarly, while (for example) ENG:look and SPA:mira may be semantically connected, the senses look.01 and
2For English we used the vectors provided on the GloVe website nlp.stanford.edu/projects/glove/.
mira.01 may not correspond.",3.2 Simple Polyglot Sharing,[0],[0]
"Hence, predicate sense representations are also language-specific.",3.2 Simple Polyglot Sharing,[0],[0]
"In the second variant, we concatenate a language ID vector to each multilingual word embedding and predicate indicator feature in the input representation.",3.3 Language Identification,[0],[0]
This vector is randomly initialized and updated in training.,3.3 Language Identification,[0],[0]
"These additional parameters provide a small degree of language-specificity in the model, while still sharing most parameters.",3.3 Language Identification,[0],[0]
This third variant takes inspiration from the “frustratingly easy” architecture of Daume III (2007) for domain adaptation.,3.4 Language-Specific LSTMs,[0],[0]
"In addition to processing every example with a shared biLSTM as in previous models, we add language-specific biLSTMs that are trained only on the examples belonging to one language.",3.4 Language-Specific LSTMs,[0],[0]
"Each of these languagespecific biLSTMs is two layers deep, and is combined with the shared biSLTM in the input to the third layer.",3.4 Language-Specific LSTMs,[0],[0]
This adds a greater degree of languagespecific processing while still sharing representations across languages.,3.4 Language-Specific LSTMs,[0],[0]
It also uses the language identification vector and multilingual word vectors in the input.,3.4 Language-Specific LSTMs,[0],[0]
We present our results in Table 2.,4 Experiments,[0],[0]
"We observe that simple polyglot training improves over monolingual training, with the exception of Czech, where we observe no change in performance.",4 Experiments,[0],[0]
"The languages with the fewest training examples (German, Japanese, Catalan) show the most improvement, while large-dataset languages such as Czech or Chinese see little or no improvement (Figure 2).
",4 Experiments,[0],[0]
"The language ID model performs inconsistently; it is better than the simple polyglot model in some cases, including Czech, but not in all.",4 Experiments,[0],[0]
"The language-specific LSTMs model performs best on a few languages, such as Catalan and Chinese, but worst on others.",4 Experiments,[0],[0]
"While these results may reflect differences between languages in the optimal amount of crosslingual sharing, we focus on the simple polyglot results in our analysis, which sufficiently demonstrate that polyglot training can improve performance over monolingual training.
",4 Experiments,[0],[0]
"We also report performance of state-of-the-art systems in each of these languages, all of which make explicit use of syntactic features, Marcheg-
giani et al. (2017) excepted.",4 Experiments,[0],[0]
"While this results in better performance on many languages, our model has the advantage of not relying on a syntactic parser, and is hence more applicable to languages with lower resources.",4 Experiments,[0],[0]
"However, the results suggest that syntactic information is critical for strong performance on German, which has the fewest predicates and thus the least semantic annotation for a semantics-only model to learn from.",4 Experiments,[0],[0]
"Nevertheless, our baseline is on par with the best published scores for Chinese, and it shows strong performance on most languages.
",4 Experiments,[0],[0]
Label-wise results.,4 Experiments,[0],[0]
"Table 3 gives the F1 scores for individual label categories in the Catalan and Spanish datasets, as an illustration of the larger trend.",4 Experiments,[0],[0]
"In both languages, we find a small but consistent improvement in the most common label categories (e.g., arg1 and argM ).",4 Experiments,[0],[0]
"Less common label categories are sensitive to small changes in performance; they have the largest changes in F1 in absolute value, but without a consistent direction.",4 Experiments,[0],[0]
"This could be attributed to the addition of English data, which improves learning of representations that are useful for the most common labels, but is essentially a random perturbation for the rarer ones.",4 Experiments,[0],[0]
"This pattern is seen across languages, and consistently results in overall gains from polyglot training.
",4 Experiments,[0],[0]
"One exception is in Czech, where polyglot training reduces accuracy on several common argument labels, e.g., PAT and LOC.",4 Experiments,[0],[0]
"While the effect sizes are small (consistent with other languages), the overall F1 score on Czech decreases slightly in the polyglot condition.",4 Experiments,[0],[0]
"It may be that the Czech dataset is too large to make use of the comparatively small amount of English data, or that differences in the annotation schemes prevent
effective crosslingual transfer.",4 Experiments,[0],[0]
Future work on language pairs that do not include English could provide further insights.,4 Experiments,[0],[0]
"Catalan and Spanish, for example, are closely related and use the same argument label set (both being drawn from the AnCora corpus) which would allow for sharing output representations as well as input tokens and parameters.
",4 Experiments,[0],[0]
Polyglot English results.,4 Experiments,[0],[0]
"For each language pair, we also evaluated the simple polyglot model on the English test set from the CoNLL 2009 shared task (Table 4).",4 Experiments,[0],[0]
"English SRL consistently benefits from polyglot training, with an increase of 0.25–0.7 absolute F1 points, depending on the language.",4 Experiments,[0],[0]
"Surprisingly, Czech provides the smallest improvement, despite the large amount of data added; the absence of crosslingual transfer in both directions for the English-Czech case, breaking the pattern seen in other languages, could therefore be due to differences in annotation rather than questions of dataset size.
",4 Experiments,[0],[0]
Labeled vs. unlabeled F1.,4 Experiments,[0],[0]
Table 5 provides unlabeled F1 scores for each language pair.,4 Experiments,[0],[0]
"As can be seen here, the unlabeled F1 improvements are generally positive but small, indicating that polyglot training can help both in structure prediction and labeling of arguments.",4 Experiments,[0],[0]
"The pattern of seeing the largest improvements on the languages with the smallest datasets generally holds here: the largest F1 gains are in German and Catalan, followed by Japanese, with minimal or no improvement elsewhere.",4 Experiments,[0],[0]
Recent improvements in multilingual SRL can be attributed to neural architectures.,5 Related Work,[0],[0]
"Swayamdipta et al. (2016) present a transition-based stack LSTM model that predicts syntax and semantics jointly, as a remedy to the reliance on pipelined models.",5 Related Work,[0],[0]
Guo et al. (2016) and Roth and Lapata (2016) use deep biLSTM architectures which use syntactic information to guide the composition.,5 Related Work,[0],[0]
"Marcheggiani et al. (2017) use a simple LSTM model over word tokens to tag semantic dependencies, like our model.",5 Related Work,[0],[0]
"Their model predicts a token’s label based on the combination of the token vector and the predicate vector, and saw benefits from using POS tags, both improvements that could be added to our model.",5 Related Work,[0],[0]
"Marcheggiani and Titov (2017) apply the recently-developed graph
convolutional networks to SRL, obtaining state of the art results on English and Chinese.",5 Related Work,[0],[0]
"All of these approaches are orthogonal to ours, and might benefit from polyglot training.
",5 Related Work,[0],[0]
Other polyglot models have been proposed for semantics.,5 Related Work,[0],[0]
Richardson et al. (2018) train on multiple (natural language)-(programming language) pairs to improve a model that translates API text into code signature representations.,5 Related Work,[0],[0]
"Duong et al. (2017) treat English and German semantic parsing as a multi-task learning problem and saw improvement over monolingual baselines, especially for small datasets.",5 Related Work,[0],[0]
"Most relevant to our work is Johannsen et al. (2015), which trains a polyglot
model for frame-semantic parsing.",5 Related Work,[0],[0]
"In addition to sharing features with multilingual word vectors, they use them to find word translations of target language words for additional lexical features.",5 Related Work,[0],[0]
"In this work, we have explored a straightforward method for polyglot training in SRL: use multilingual word vectors and combine training data across languages.",6 Conclusion,[0],[0]
"This allows sharing without crosslingual alignments, shared annotation, or parallel data.",6 Conclusion,[0],[0]
"We demonstrate that a polyglot model can outperform a monolingual one for semantic analysis, particularly for languages with less data.",6 Conclusion,[0],[0]
"We thank Luke Zettlemoyer, Luheng He, and the anonymous reviewers for helpful comments and feedback.",Acknowledgments,[0],[0]
This research was supported in part by the Defense Advanced Research Projects Agency (DARPA) Information Innovation Office (I2O) under the Low Resource Languages for Emergent Incidents (LORELEI) program issued by DARPA/I2O under contract HR001115C0113 to BBN.,Acknowledgments,[0],[0]
Views expressed are those of the authors alone.,Acknowledgments,[0],[0]
"Previous approaches to multilingual semantic dependency parsing treat languages independently, without exploiting the similarities between semantic structures across languages.",abstractText,[0],[0]
"We experiment with a new approach where we combine resources from a pair of languages in the CoNLL 2009 shared task (Hajič et al., 2009) to build a polyglot semantic role labeler.",abstractText,[0],[0]
"Notwithstanding the absence of parallel data, and the dissimilarity in annotations between languages, our approach results in an improvement in SRL performance on multiple languages over a monolingual baseline.",abstractText,[0],[0]
Analysis of the polyglot model shows it to be advantageous in lower-resource settings.,abstractText,[0],[0]
Polyglot Semantic Role Labeling,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2183–2192, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics
Language resources that systematically organize paraphrases for binary relations are of great value for various NLP tasks and have recently been advanced in projects like PATTY, WiseNet and DEFIE. This paper presents a new method for building such a resource and the resource itself, called POLY. Starting with a very large collection of multilingual sentences parsed into triples of phrases, our method clusters relational phrases using probabilistic measures. We judiciously leverage fine-grained semantic typing of relational arguments for identifying synonymous phrases. The evaluation of POLY shows significant improvements in precision and recall over the prior works on PATTY and DEFIE. An extrinsic use case demonstrates the benefits of POLY for question answering.",text,[0],[0]
Motivation.,1 Introduction,[0],[0]
Information extraction from text typically yields relational triples: a binary relation along with its two arguments.,1 Introduction,[0],[0]
"Often the relation is expressed by a verb phrase, and the two arguments are named entities.",1 Introduction,[0],[0]
We refer to the surface form of the relation in a triple as a relational phrase.,1 Introduction,[0],[0]
"Repositories of relational phrases are an asset for a variety of tasks, including information extraction, textual entailment, and question answering.
",1 Introduction,[0],[0]
This paper presents a new method for systematically organizing a large set of such phrases.,1 Introduction,[0],[0]
"We aim to construct equivalence classes of synonymous phrases, analogously to how WordNet organizes
unary predicates as noun-centric synsets (aka. semantic types).",1 Introduction,[0],[0]
"For example, the following relational phrases should be in the same equivalence class: sings in, is vocalist in, voice in denoting a relation between a musician and a song.
",1 Introduction,[0],[0]
State of the Art and its Limitations.,1 Introduction,[0],[0]
"Starting with the seminal work on DIRT (Lin and Pantel, 2001), there have been various attempts on building comprehensive resources for relational phrases.",1 Introduction,[0],[0]
"Recent works include PATTY (Nakashole et al., 2012), WiseNet (Moro and Navigli, 2012) and DEFIE (Bovi et al., 2015).",1 Introduction,[0],[0]
Out of these DEFIE is the cleanest resource.,1 Introduction,[0],[0]
"However, the equivalence classes tend to be small, prioritizing precision over recall.",1 Introduction,[0],[0]
"On the other hand, PPDB (Ganitkevitch et al., 2013) offers the largest repository of paraphrases.",1 Introduction,[0],[0]
"However, the paraphrases are not relation-centric and they are not semantically typed.",1 Introduction,[0],[0]
"So it misses out on the opportunity of using types to distinguish identical phrases with different semantics, for example, performance in with argument types musician and song versus performance in with types athlete and competition.
",1 Introduction,[0],[0]
Our Approach.,1 Introduction,[0],[0]
"We start with a large collection of relational triples, obtained by shallow information extraction.",1 Introduction,[0],[0]
"Specifically, we use the collection of Faruqui and Kumar (2015), obtained by combining the OLLIE tool with Google Translate and projecting multilingual sentences back to English.",1 Introduction,[0],[0]
"Note that the task addressed in that work is relational triple extraction, which is orthogonal to our problem of organizing the relational phrases in these triples into synonymy sets.
",1 Introduction,[0],[0]
"We canonicalize the subject and object arguments
2183
of triples by applying named entity disambiguation and word sense disambiguation wherever possible.",1 Introduction,[0],[0]
"Using a knowledge base of entity types, we can then infer prevalent type signatures for relational phrases.",1 Introduction,[0],[0]
"Finally, based on a suite of judiciously devised probabilistic distance measures, we cluster phrases in a type-compatible way using a graph-cut technique.",1 Introduction,[0],[0]
The resulting repository contains ca.,1 Introduction,[0],[0]
"1 Million relational phrases, organized into ca.",1 Introduction,[0],[0]
"160,000 clusters.
",1 Introduction,[0],[0]
Contribution.,1 Introduction,[0],[0]
"Our salient contributions are: i) a novel method for constructing a large repository of relational phrases, based on judicious clustering and type filtering; ii) a new linguistic resource, coined POLY, of relational phrases with semantic typing, organized in equivalence classes; iii) an intrinsic evaluation of POLY, demonstrating its high quality in comparison to PATTY and DEFIE; iv) an extrinsic evaluation of POLY, demonstrating its benefits for question answering.",1 Introduction,[0],[0]
The POLY resource is publicly available 1.,1 Introduction,[0],[0]
Our approach consists of two stages: relational phrase typing and relational phrase clustering.,2 Method Overview,[0],[0]
"In Section 3, we explain how we infer semantic types of the arguments of a relational phrase.",2 Method Overview,[0],[0]
"In Section 4, we present the model for computing synonyms of relational phrases (i.e., paraphrases) and organizing them into clusters.
",2 Method Overview,[0],[0]
A major asset for our approach is a large corpus of multilingual sentences from the work of Faruqui and Kumar (2015).,2 Method Overview,[0],[0]
That dataset contains sentences from Wikipedia articles in many languages.,2 Method Overview,[0],[0]
"Each sentence has been processed by an Open Information Extraction method (Banko et al., 2007), specifically the OLLIE tool (Mausam et al., 2012), which produces a triple of surface phrases that correspond to a relational phrase candidate and its two arguments (subject and object).",2 Method Overview,[0],[0]
"Each non-English sentence has been translated into English using Google Translate, thus leveraging the rich statistics that Google has obtained from all kinds of parallel multilingual texts.",2 Method Overview,[0],[0]
"Altogether, the data from Faruqui and Kumar (2015) provides 135 million triples in 61 languages and in English (from the translations of the corresponding sentences).",2 Method Overview,[0],[0]
"This is the noisy input to our
1www.mpi-inf.mpg.de/yago-naga/poly/
method.",2 Method Overview,[0],[0]
"Figure 1 shows two Spanish sentences, the extracted triples of Spanish phrases, the sentences’ translations to English, and the extracted triples of English phrases.
",2 Method Overview,[0],[0]
"The figure shows that identical phrases in the foreign language - “fue filmado por” - may be translated into different English phrases: “was shot by” vs. “was filmed by”, depending on the context in the respective sentences.",2 Method Overview,[0],[0]
This is the main insight that our method builds on.,2 Method Overview,[0],[0]
The two resulting English phrases have a certain likelihood of being paraphrases of the same relation.,2 Method Overview,[0],[0]
"However, this is an uncertain hypotheses only, given the ambiguity of language, the noise induced by machine translation and the potential errors of the triple extraction.",2 Method Overview,[0],[0]
"Therefore, our method needs to de-noise these input phrases and quantify to what extent the the relational phrases are indeed synonymous.",2 Method Overview,[0],[0]
We discuss this in Sections 3 and 4.,2 Method Overview,[0],[0]
This section explains how we assign semantic types to relational phrases.,3 Relation Typing,[0],[0]
"For example, the relational phrase wrote could be typed as <author> wrote <paper>, as one candidate.",3 Relation Typing,[0],[0]
The typing helps us to disambiguate the meaning of the relational phrase and later find correct synonyms.,3 Relation Typing,[0],[0]
The relational phrase shot could have synonyms directed or killed with a gun.,3 Relation Typing,[0],[0]
"However, they represent different senses of the phrase shot.",3 Relation Typing,[0],[0]
"With semantic typing, we can separate these two meanings and determine that <person> shot <person> is a synonym of <person> killed with a gun <person>, whereas <director> shot<movie> is a synonym of<director> directed <movie>.
",3 Relation Typing,[0],[0]
"Relation typing has the following steps: argument extraction, argument disambiguation, argument typing and type filtering.",3 Relation Typing,[0],[0]
The output is a set of candidate types for the left and right arguments of each English relational phrase.,3 Relation Typing,[0],[0]
"For the typing of a relational phrase, we have to determine words in the left and right arguments that give cues for semantic types.",3.1 Argument Extraction,[0],[0]
"To this end, we identify named entities, whose types can be looked up in a knowledge base, and the head words of common
noun phrases.",3.1 Argument Extraction,[0],[0]
"As output, we produce a ranked list of entity mentions and common nouns.
",3.1 Argument Extraction,[0],[0]
"To create this ranking, we perform POS tagging and noun phrase chunking using Stanford CoreNLP",3.1 Argument Extraction,[0],[0]
"(Manning et al., 2014) and Apache OpenNLP 2.",3.1 Argument Extraction,[0],[0]
"For head noun extraction, we use the YAGO Javatools3 and a set of manually crafted regular expressions.",3.1 Argument Extraction,[0],[0]
"Since the input sentences result from machine translation, we could not use dependency parsing, because sentences are often ungrammatical.
",3.1 Argument Extraction,[0],[0]
"Finally, we extract all noun phrases which contain the same head noun.",3.1 Argument Extraction,[0],[0]
"These noun phrases are then sorted according to their lengths.
",3.1 Argument Extraction,[0],[0]
"For example, for input phrase contemporary British director who also created “Inception”, our method would yield contemporary British director, British director, director in decreasing order.",3.1 Argument Extraction,[0],[0]
The second step is responsible for the disambiguation of the noun phrase and named entity candidates.,3.2 Argument Disambiguation,[0],[0]
"We use the YAGO3 knowledge base (Mahdisoltani et al., 2015) for named entities, and WordNet (Fellbaum, 1998) for noun phrases.",3.2 Argument Disambiguation,[0],[0]
"We proceed in the ranking order of the phrases from the first step.
",3.2 Argument Disambiguation,[0],[0]
"Candidate senses are looked up in YAGO3 and WordNet, respectively, and each candidate is scored.",3.2 Argument Disambiguation,[0],[0]
"The scores are based on:
• Frequency count prior: This is the number of Wikipedia incoming links for named entities in YAGO3, or the frequency count of noun phrase senses in WordNet.
",3.2 Argument Disambiguation,[0],[0]
"• Wikipedia prior: We increase scores of YAGO3 entities whose URL strings (i.e., Wikipedia titles) occur in the Wikipedia page from which the triple was extracted.
",3.2 Argument Disambiguation,[0],[0]
"2opennlp.apache.org/ 3mpi-inf.mpg.de/yago-naga/javatools/
•",3.2 Argument Disambiguation,[0],[0]
Translation prior: We boost the scores of senses whose translations occur in the original input sentence.,3.2 Argument Disambiguation,[0],[0]
"For example, the word stage is disambiguated as opera stage rather than phase, because the original German sentence contains the word Bühne",3.2 Argument Disambiguation,[0],[0]
(German word for a concert stage) and not Phase.,3.2 Argument Disambiguation,[0],[0]
"The translations of word senses are obtained from Universal WordNet (de Melo and Weikum, 2009).
",3.2 Argument Disambiguation,[0],[0]
We prefer WordNet noun phrases over YAGO3 named entities since noun phrases have lower type ambiguity (fewer possible types).,3.2 Argument Disambiguation,[0],[0]
"The final score of a sense s is:
score(s) = αfreq(s)+βwiki(s)+γtrans(s) (1)
where freq(s) is the frequency count of s, and wiki(s) and trans(s) equal maximal frequency count if the Wikipedia prior and Translation prior conditions hold (and otherwise set to 0).",3.2 Argument Disambiguation,[0],[0]
"α, β, γ are tunable hyper-parameters (set using withheld data).
",3.2 Argument Disambiguation,[0],[0]
"Finally, from the list of candidates, we generate a disambiguated argument: either a WordNet synset or a YAGO3 entity identifier.",3.2 Argument Disambiguation,[0],[0]
"In the third step of relation typing, we assign candidate types to the disambiguated arguments.",3.3 Argument Typing,[0],[0]
"To this end, we query YAGO3 for semantic types (incl. transitive hypernyms) for a given YAGO3 or WordNet identifier.
",3.3 Argument Typing,[0],[0]
The type system used in POLY consists of a subset of the WordNet noun hierarchy.,3.3 Argument Typing,[0],[0]
"We restrict ourselves to 734 types, chosen semi-automatically as follows.",3.3 Argument Typing,[0],[0]
We selected the 1000 most frequent WordNet types in YAGO3 (incl. transitive hypernyms).,3.3 Argument Typing,[0],[0]
"Redundant and non-informative types were filtered out by the following technique: all types were organized into a directed acyclic graph (DAG), and
we removed a type when the frequency count of some of its children was higher than 80% of the parent’s count.",3.3 Argument Typing,[0],[0]
"For example, we removed type trainer since more than 80% of trainers in YAGO3 are also coaches.",3.3 Argument Typing,[0],[0]
"In addition, we manually removed a few non-informative types (e.g. expressive style).
",3.3 Argument Typing,[0],[0]
"As output, we obtain lists of semantic types for the two arguments of each relational phrase.",3.3 Argument Typing,[0],[0]
"In the last step, we filter types one more time.",3.4 Type Filtering,[0],[0]
"This time we filter candidate types separately for each distinct relational phrase, in order to choose the most suitable specific type signature for each phrase.",3.4 Type Filtering,[0],[0]
"This choice is made by type tree pruning.
",3.4 Type Filtering,[0],[0]
"For each relational phrase, we aggregate all types of the left arguments and all types of the right arguments, summing up their their frequency counts.",3.4 Type Filtering,[0],[0]
"This information is organized into a DAG, based on type hypernymy.",3.4 Type Filtering,[0],[0]
"Then we prune types as follows (similarly to Section 3.3): i) remove a parent type when the relative frequency count of one of the children types is larger than 80% of the parent’s count; ii) remove a child type when its relative frequency count is smaller than 20% of the parent’s count.
",3.4 Type Filtering,[0],[0]
For each of the two arguments of the relational phrase we allow only those types which are left after the pruning.,3.4 Type Filtering,[0],[0]
"The final output is a set of relational phrases where each has a set of likely type signatures (i.e., pairs of types for the relation’s arguments).",3.4 Type Filtering,[0],[0]
The second stage of POLY addresses the relation clustering.,4 Relation Clustering,[0],[0]
"The algorithm takes semantically typed relational phrases as input, quantifies the semantic similarity between relational phrases, and organizes them into clusters of synonyms.",4 Relation Clustering,[0],[0]
The key insight that our approach hinges on is that synonymous phrases have similar translations in a different language.,4 Relation Clustering,[0],[0]
"In our setting, two English phrases are semantically similar if they were translated from the same relational phrases in a foreign language and their argument types agree (see Figure 1 for an example).",4 Relation Clustering,[0],[0]
Similarities between English phrases are cast into edge weights of a graph with phrases as nodes.,4 Relation Clustering,[0],[0]
This graph is then partitioned to obtain clusters.,4 Relation Clustering,[0],[0]
The phrase similarities in POLY are based on probabilistic measures.,4.1 Probabilistic Similarity Measures,[0],[0]
"We use the notation:
• F : a set of relational phrases from a foreign language F
• E: a set of translations of relational phrases from language F to English
• c(f, e): no. of times of translating relational phrase f ∈ F into relational phrase e ∈ E
• c(f), c(e): frequency counts for relational phrase f ∈ F and its translation e ∈ E
• p(e|f) = c(f,e)c(f) : (estimator for the) probability of translating f ∈ F into e ∈ E
• p(f |e) = c(f,e)c(e) : (estimator for the) probability of e ∈ E being a translation of f ∈ F
We define:
p(e1|e2) = ∑
f
p(e1|f) ∗ p(f |e2) (2)
as the probability of generating relational phrase e1 ∈ E from phrase e2 ∈",4.1 Probabilistic Similarity Measures,[0],[0]
"E. Finally we define:
support(e1, e2) =",4.1 Probabilistic Similarity Measures,[0],[0]
"∑
f∈F c(f, e1) ∗",4.1 Probabilistic Similarity Measures,[0],[0]
"c(f, e2) (3)
confidence(e1, e2)",4.1 Probabilistic Similarity Measures,[0],[0]
"= 2
1 p(e1|e2) + 1 p(e2|e1)
(4)
",4.1 Probabilistic Similarity Measures,[0],[0]
Confidence is the final similarity measure used in POLY.,4.1 Probabilistic Similarity Measures,[0],[0]
We use the harmonic mean in Equation 4 to dampen similarity scores that have big differences in their probabilities in Equation 2.,4.1 Probabilistic Similarity Measures,[0],[0]
"Typically, pairs e1, e2 with such wide gaps in their probabilities come from subsumptions, not synonymous phrases.",4.1 Probabilistic Similarity Measures,[0],[0]
"Finally, we compute the support and confidence for every pair of English relational phrases which have a common source phrase of translation.",4.1 Probabilistic Similarity Measures,[0],[0]
"We prune phrase pairs with low support (below a threshold), and rank the remaining pairs by confidence.",4.1 Probabilistic Similarity Measures,[0],[0]
"To compute clusters of relational phrases, we use modularity-based graph partitioning.",4.2 Graph Clustering,[0],[0]
"Specifically, we use the partitioning algorithm of Blondel et al. (2008).",4.2 Graph Clustering,[0],[0]
"The resulting clusters (i.e., subgraphs) are
then ranked by their weighted graph density multiplied by the graph size (Equation 5).",4.2 Graph Clustering,[0],[0]
"The example of a cluster is shown in Table 1.
∑ (ei,ej)∈E sim(ei, ej)
|V",4.2 Graph Clustering,[0],[0]
| ∗ |V,4.2 Graph Clustering,[0],[0]
− 1| ∗ |V,4.2 Graph Clustering,[0],[0]
| (5),4.2 Graph Clustering,[0],[0]
"For the experimental evaluation, we primarily chose triples from the German language (and their English translations).",5 Evaluation,[0],[0]
"With about 23 million triples, German is the language with the largest number of extractions in the dataset, and there are about 2.5 million distinct relational phrases from the German-toEnglish translation.",5 Evaluation,[0],[0]
"The POLY method is implemented using Apache Spark, so it scales out to handle such large inputs.
",5 Evaluation,[0],[0]
"After applying the relation typing algorithm, we obtain around 10 million typed relational phrases.",5 Evaluation,[0],[0]
"If we ignored the semantic types, we would have about 950,000 distinct phrases.",5 Evaluation,[0],[0]
"On this input data, POLY detected 1,401,599 pairs of synonyms.",5 Evaluation,[0],[0]
"The synonyms were organized into 158,725 clusters.
",5 Evaluation,[0],[0]
"In the following, we present both an intrinsic evaluation and an extrinsic use case.",5 Evaluation,[0],[0]
"For the intrinsic evaluation, we asked human annotators to judge whether two typed relational phrases are synonymous or not.",5 Evaluation,[0],[0]
We also studied source languages other than German.,5 Evaluation,[0],[0]
"In addition, we compared POLY against PATTY (Nakashole et al., 2012) and DEFIE (Bovi et al., 2015) on the relation paraphrasing task.",5 Evaluation,[0],[0]
"For the extrinsic evaluation, we considered a simple question answering system and studied to what extent similarities between typed relational phrases can contribute to answering more questions.",5 Evaluation,[0],[0]
"To assess the precision of the discovered synonymy among relational phrases (i.e., clusters of para-
phrases), we sampled POLY’s output.",5.1 Precision of Synonyms,[0],[0]
We assessed the 250 pairs of synonyms with the highest similarity scores.,5.1 Precision of Synonyms,[0],[0]
"We also assessed a sample of 250 pairs of synonyms, randomly drawn from POLY’s output.
",5.1 Precision of Synonyms,[0],[0]
These pairs of synonyms were shown to several human annotators to check their correctness.,5.1 Precision of Synonyms,[0],[0]
"Relational phrases were presented by showing the semantic types, the textual representation of the relational phrase and sample sentences where the phrase was found.",5.1 Precision of Synonyms,[0],[0]
The annotators were asked whether two relational phrases have the same meaning or not.,5.1 Precision of Synonyms,[0],[0]
"They could also abstain.
",5.1 Precision of Synonyms,[0],[0]
"The results of this evaluation are shown in Table 2 with (lower bounds and upper bounds of) the 0.95-confidence Wilson score intervals (Brown et al., 2001).",5.1 Precision of Synonyms,[0],[0]
"This evaluation task had good interannotator agreement, with Fleiss’ Kappa around 0.6.",5.1 Precision of Synonyms,[0],[0]
"Table 3 shows anecdotal examples of synonymous pairs of relational phrases.
",5.1 Precision of Synonyms,[0],[0]
These results show that POLY’s quality is comparable with state-of-the-art baselines resources.,5.1 Precision of Synonyms,[0],[0]
"WiseNet (Moro and Navigli, 2012) is reported to have precision of 0.85 for 30,000 clusters.",5.1 Precision of Synonyms,[0],[0]
This is also the only prior work where the precision of synonymy of semantically typed relational phrases was evaluated.,5.1 Precision of Synonyms,[0],[0]
The other systems did not report that measure.,5.1 Precision of Synonyms,[0],[0]
"However, they performed the evaluation of subsumption, entailment or hypernymy relationships which are related to synonymy.",5.1 Precision of Synonyms,[0],[0]
Subsumptions in PATTY have precision of 0.83 for top 100 and 0.75 for a random sample.,5.1 Precision of Synonyms,[0],[0]
Hypernyms in RELLY are reported to have precision of 0.87 for top 100 and 0.78 for a random sample.,5.1 Precision of Synonyms,[0],[0]
"DEFIE performed separate evaluations for hypernyms generated directly from WordNet (precision 0.87) and hypernyms obtained through a substring generalization algorithm (precision 0.9).
",5.1 Precision of Synonyms,[0],[0]
Typical errors in the paraphrase discovery of POLY come from incorrect translations or extraction errors.,5.1 Precision of Synonyms,[0],[0]
"For example, heard and belongs to were clustered together because they were translated from the
same semantically ambiguous German word gehört.",5.1 Precision of Synonyms,[0],[0]
An example for extraction errors is that took and participated in were clustered together because took was incorrectly extracted from a sentence with the phrase took part in.,5.1 Precision of Synonyms,[0],[0]
"Other errors are caused by swapped order of arguments in a triple (i.e., mistakes in detecting passive form) and incorrect argument disambiguation.",5.1 Precision of Synonyms,[0],[0]
"To compare POLY with the closest competitors PATTY and DEFIE, we designed an experiment along the lines of the evaluation of Information Retrieval systems (e.g. TREC benchmarks).",5.2 Comparison to Competitors,[0],[0]
"First, we randomly chose 100 semantically typed relational phrases with at least three words (to focus on the more interesting multi-word case, rather than single verbs).",5.2 Comparison to Competitors,[0],[0]
These relational phrases had to occur in all three resources.,5.2 Comparison to Competitors,[0],[0]
"For every relational phrase we retrieved synonyms from all of the systems, forming a pool of candidates.",5.2 Comparison to Competitors,[0],[0]
"Next, to remove minor syntactic variations of the same phrase, the relational phrases were lemmatized.",5.2 Comparison to Competitors,[0],[0]
"In addition, we removed all leading prepositions, modal verbs, and adverbs.
",5.2 Comparison to Competitors,[0],[0]
We manually evaluated the correctness of the remaining paraphrase candidates for each of the 100 phrases.,5.2 Comparison to Competitors,[0],[0]
Precision was computed as the ratio of the correct synonyms by one system to the number of all synonyms provided by that system.,5.2 Comparison to Competitors,[0],[0]
"Recall was computed as the ratio of the number of correct synonyms by one system to the number of all correct synonyms in the candidate pool from all three systems.
",5.2 Comparison to Competitors,[0],[0]
The results are presented in Table 4.,5.2 Comparison to Competitors,[0],[0]
All results are macro-averaged over the 100 sampled phrases.,5.2 Comparison to Competitors,[0],[0]
We performed a paired t-test for precision and recall of POLY against each of the systems and obtained p-values below 0.05.,5.2 Comparison to Competitors,[0],[0]
"POLY and DEFIE of-
fer much higher diversity of synonyms than PATTY.",5.2 Comparison to Competitors,[0],[0]
"However, DEFIE’s synonyms often do not fit the semantic type signature of the given relational phrase and are thus incorrect.",5.2 Comparison to Competitors,[0],[0]
"For example, was assumed by was found to be a synonym of <group> was acquired by <group>.",5.2 Comparison to Competitors,[0],[0]
"PATTY, on the other hand, has higher recall due to its variety of prepositions attached to relational phrases; however, these also include spurious phrases, leading to lower precision.",5.2 Comparison to Competitors,[0],[0]
"For example, succeeded in was found to be a synonym of <person> was succeeded by <leader>.",5.2 Comparison to Competitors,[0],[0]
"Overall, POLY achieves much higher precision and recall than both of these baselines.",5.2 Comparison to Competitors,[0],[0]
"To evaluate the influence of different components, we performed an ablation study.",5.3 Ablation Study,[0],[0]
"We consider versions of POLY where Wikipedia prior and Translation prior (Section 3.2) are disregarded (− disambiguation), where the type system (Section 3.3) was limited to the 100 most frequent YAGO types (Type system 100) or to the 5 top-level types from the YAGO hierarchy (Type system 5), or where the type filtering parameter (Section 3.4) was set to 70% or 90% (Type filtering 0.7/0.9).",5.3 Ablation Study,[0],[0]
"The evaluation was done on random samples of 250 pairs of synonyms.
",5.3 Ablation Study,[0],[0]
Table 5 shows the results with the 0.95-confidence Wilson score intervals.,5.3 Ablation Study,[0],[0]
"Without our argument disambiguation techniques, the precision drops heavily.",5.3 Ablation Study,[0],[0]
"When weakening the type system, our tech-
niques for argument typing and type filtering are penalized, resulting in lower precision.",5.3 Ablation Study,[0],[0]
So we see that all components of the POLY architecture are essential for achieving high-quality output.,5.3 Ablation Study,[0],[0]
Lowering the type-filtering threshold yields results with comparable precision.,5.3 Ablation Study,[0],[0]
"However, increasing the threshold results in a worse noise filtering procedure.",5.3 Ablation Study,[0],[0]
"In addition to paraphrases derived from German, we evaluated the relational phrase synonymy derived from a few other languages with lower numbers of extractions.",5.4 Evaluation with Other Languages,[0],[0]
"We chose French, Hindi, and Russian (cf.",5.4 Evaluation with Other Languages,[0],[0]
"(Faruqui and Kumar, 2015)).",5.4 Evaluation with Other Languages,[0],[0]
"The results are presented in Table 6, again with the 0.95-confidence Wilson score intervals.
",5.4 Evaluation with Other Languages,[0],[0]
Synonyms derived from French have similar quality as those from German.,5.4 Evaluation with Other Languages,[0],[0]
This is plausible as one would assume that French and German have similar quality in translation to English.,5.4 Evaluation with Other Languages,[0],[0]
Synonyms derived from Russian and Hindi have lower precision due to the lower translation quality.,5.4 Evaluation with Other Languages,[0],[0]
"The precision for Hindi is lower, as the Hindi input corpus has much fewer sentences than for the other languages.",5.4 Evaluation with Other Languages,[0],[0]
"As an extrinsic use case for the POLY resource, we constructed a simple Question Answering (QA) system over knowledge graphs such as Freebase, and determined the number of questions for which the
system can find a correct answer.",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
We followed the approach presented by Fader et al. (2014).,5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"The system consists of question parsing, query rewriting and database look-up stages.",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"We disregard the stage of ranking answer candidates, and merely test whether the system could return the right answer (i.e., would return with the perfect ranking).
",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"In the question parsing stage, we use 10 highprecision parsing operators by Fader et al. (2014), which map questions (e.g., Who invented papyrus?)",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"to knowledge graph queries (e.g., (?x, invented, papyrus)).",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"Additionally, we map question words to semantic types.",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"For example, the word who is mapped to person, where to location, when to abstract entity and the rest of the question words are mapped to type entity.
",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
We harness synonyms and hyponyms of relational phrases to paraphrase the predicate of the query.,5.5 Extrinsic Evaluation: Question Answering,[0],[0]
The paraphrases must be compatible with the semantic type of the question word.,5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"In the end, we use the original query, as well as found paraphrases, to query a database of subject, predicate, object triples.",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"As the knowledge graph for this experiment we used the union of collections: a triples database from OpenIE (Fader et al., 2011), Freebase (Bollacker et al., 2008), Probase (Wu et al., 2012) and NELL (Carlson et al., 2010).",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"In total, this knowledge graph contained more than 900 Million triples.
",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"We compared six systems for paraphrasing semantically typed relational phrases:
• Basic: no paraphrasing at all, merely using the originally generated query.
• DEFIE: using the taxonomy of relational phrases by Bovi et al. (2015).
",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"• PATTY: using the taxonomy of relational phrases by Nakashole et al. (2012).
",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"• RELLY: using the subset of the PATTY taxonomy with additional entailment relationships between phrases (Grycner et al., 2015).
• POLY DE: using synonyms of relational phrases derived from the German language.
",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"• POLY ALL: using synonyms of relational phrases derived from the 61 languages.
",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"Since DEFIE’s relational phrases are represented by BabelNet (Navigli and Ponzetto, 2012) word sense identifiers, we generated all possible lemmas for
each identifier.",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"We ran the paraphrase-enhanced QA system for three benchmark sets of questions:
• TREC: the set of questions used for the evaluation of information retrieval QA systems (Voorhees and Tice, 2000)
",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"• WikiAnswers: a random subset of questions from WikiAnswers (Fader et al., 2013).
• WebQuestions: the set of questions about Freebase entities (Berant et al., 2013).
",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"From these question sets, we kept only those questions which can be parsed by one of the 10 question parsing templates and have a correct answer in the gold-standard ground truth.",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"In total, we executed 451 questions for TREC, 516 for WikiAnswers and 1979 for WebQuestions.
",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"For every question, each paraphrasing system generates a set of answers.",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
We measured for how many questions we could obtain at least one correct answer.,5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"Table 7 shows the results.
",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
The best results were obtained by POLY ALL.,5.5 Extrinsic Evaluation: Question Answering,[0],[0]
We performed a paired t-test for the results of POLY DE and POLY ALL against all other systems.,5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"The differences between POLY ALL and the other systems are statistically significant with pvalue below 0.05.
",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"Additionally, we evaluated paraphrasing systems which consist of combination of all of the described datasets and all of the described datasets without POLY.",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
The difference between these two versions suggest that POLY contains many paraphrases which are available in none of the competing resources.,5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"Knowledge bases (KBs) contribute to many NLP tasks, including Word Sense Disambiguation (Moro et al., 2014), Named Entity Disambiguation (Hoffart et al., 2011), Question Answering (Fader et al., 2014), and Textual Entailment (Sha et al., 2015).",6 Related Work,[0],[0]
"Widely used KBs are DBpedia (Lehmann et al., 2015), Freebase (Bollacker et al., 2008), YAGO (Mahdisoltani et al., 2015), Wikidata (Vrandecic and Krötzsch, 2014) and the Google Knowledge Vault (Dong et al., 2014).",6 Related Work,[0],[0]
"KBs have rich information about named entities, but are pretty sparse on relations.",6 Related Work,[0],[0]
"In the latter regard, manually created resources such as WordNet (Fellbaum, 1998), VerbNet (Kipper et al., 2008) or FrameNet (Baker et al., 1998) are much richer, but still face the limitation of labor-intensive input and human curation.
",6 Related Work,[0],[0]
The paradigm of Open Information Extraction (OIE) was developed to overcome the weak coverage of relations in automatically constructed KBs.,6 Related Work,[0],[0]
OIE methods process natural language texts to produce triples of surface forms for the arguments and relational phrase of binary relations.,6 Related Work,[0],[0]
"The first large-scale approach along these lines, TextRunner (Banko et al., 2007), was later improved by ReVerb (Fader et al., 2011) and OLLIE (Mausam et al., 2012).",6 Related Work,[0],[0]
"The focus of these methods has been on verbal phrases as relations, and there is little effort to determine lexical synonymy among them.
",6 Related Work,[0],[0]
"The first notable effort to build up a resource for relational paraphrases is DIRT (Lin and Pantel, 2001), based on Harris’ Distributional Hypothesis to cluster syntactic patterns.",6 Related Work,[0],[0]
"RESOLVER (Yates and Etzioni, 2009) introduced a probabilistic relational model for predicting synonymy.",6 Related Work,[0],[0]
Yao et al. (2012) incorporated latent topic models to resolve the ambiguity of relational phrases.,6 Related Work,[0],[0]
"Other probabilistic approaches employed matrix factorization for finding entailments between relations (Riedel et al., 2013; Petroni et al., 2015) or used probabilistic graphical models to find clusters of relations (Grycner et al., 2014).",6 Related Work,[0],[0]
"All of these approaches rely on the cooccurrence of the arguments of the relation.
",6 Related Work,[0],[0]
"Recent endeavors to construct large repositories of relational paraphrases are PATTY, WiseNet and DEFIE.",6 Related Work,[0],[0]
"PATTY (Nakashole et al., 2012) devised a sequence mining algorithm to extract relational
phrases with semantic type signatures, and organized them into synonymy sets and hypernymy hierarchies.",6 Related Work,[0],[0]
"WiseNet (Moro and Navigli, 2012) tapped Wikipedia categories for a similar way of organizing relational paraphrases.",6 Related Work,[0],[0]
"DEFIE (Bovi et al., 2015) went even further and used word sense disambiguation, anchored in WordNet, to group phrases with the same meanings.
",6 Related Work,[0],[0]
Translation models have previously been used for paraphrase detection.,6 Related Work,[0],[0]
Barzilay and McKeown (2001) utilized multiple English translations of the same source text for paraphrase extraction.,6 Related Work,[0],[0]
Bannard and Callison-Burch (2005) used the bilingual pivoting method on parallel corpora for the same task.,6 Related Work,[0],[0]
"Similar methods were performed at a much bigger scale by the Paraphrase Database (PPDB) project (Pavlick et al., 2015).",6 Related Work,[0],[0]
"Unlike POLY, the focus of these projects was not on paraphrases of binary relations.",6 Related Work,[0],[0]
"Moreover, POLY considers the semantic type signatures of relations, which is missing in PPDB.
",6 Related Work,[0],[0]
Research on OIE for languages other than English has received little attention.,6 Related Work,[0],[0]
Kim et al. (2011) uses Korean-English parallel corpora for cross-lingual projection.,6 Related Work,[0],[0]
Gamallo et al. (2012) developed an OIE system for Spanish and Portuguese using rules over shallow dependency parsing.,6 Related Work,[0],[0]
The recent work of Faruqui and Kumar (2015) extracted relational phrases from Wikipedia in 61 languages using crosslingual projection.,6 Related Work,[0],[0]
"Lewis and Steedman (2013) clustered semantically equivalent English and French phrases, based on the arguments of relations.",6 Related Work,[0],[0]
"We presented POLY, a method for clustering semantically typed English relational phrases using a multilingual corpus, resulting in a repository of semantically typed paraphrases with high coverage and precision.",7 Conclusions,[0],[0]
"Future work includes jointly processing all 61 languages in the corpus, rather than considering them pairwise, to build a resource for all languages.",7 Conclusions,[0],[0]
The POLY resource is publicly available at www.mpi-inf.mpg.de/yago-naga/poly/.,7 Conclusions,[0],[0]
"Language resources that systematically organize paraphrases for binary relations are of great value for various NLP tasks and have recently been advanced in projects like PATTY, WiseNet and DEFIE.",abstractText,[0],[0]
"This paper presents a new method for building such a resource and the resource itself, called POLY.",abstractText,[0],[0]
"Starting with a very large collection of multilingual sentences parsed into triples of phrases, our method clusters relational phrases using probabilistic measures.",abstractText,[0],[0]
We judiciously leverage fine-grained semantic typing of relational arguments for identifying synonymous phrases.,abstractText,[0],[0]
The evaluation of POLY shows significant improvements in precision and recall over the prior works on PATTY and DEFIE.,abstractText,[0],[0]
An extrinsic use case demonstrates the benefits of POLY for question answering.,abstractText,[0],[0]
POLY: Mining Relational Paraphrases from Multilingual Sentences,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 35–45 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
A basic but highly important challenge in natural language understanding is being able to populate a knowledge base with relational facts contained in a piece of text.,1 Introduction,[0],[0]
"For the text shown in Figure 1, the system should extract triples, or equivalently, knowledge graph edges, such as hPenner, per:spouse, Lisa Dillmani.",1 Introduction,[0],[0]
"Combining such extractions, a system can produce a knowledge graph of relational facts between persons, organizations, and locations in the text.",1 Introduction,[0],[0]
"This task involves entity recognition, mention coreference and/or entity linking, and relation extraction; we focus on the
most challenging “slot filling” task of filling in the relations between entities in the text.
",1 Introduction,[0],[0]
Organized relational knowledge in the form of “knowledge graphs” has become an important knowledge resource.,1 Introduction,[0],[0]
"These graphs are now extensively used by search engine companies, both to provide information to end-users and internally to the system, as a way to understand relationships.",1 Introduction,[0],[0]
"However, up until now, automatic knowledge extraction has proven sufficiently difficult that most of the facts in these knowledge graphs have been built up by hand.",1 Introduction,[0],[0]
"It is therefore a key challenge to show that NLP technology can effectively contribute to this important problem.
",1 Introduction,[0],[0]
"Existing work on relation extraction (e.g., Zelenko et al., 2003; Mintz et al., 2009; Adel et al., 2016) has been unable to achieve sufficient recall or precision for the results to be usable versus hand-constructed knowledge bases.",1 Introduction,[0],[0]
"Supervised training data has been scarce and, while techniques like distant supervision appear to be a promising way to extend knowledge bases at low cost, in practice the training data has often been too noisy for reliable training of relation extraction systems (Angeli et al., 2015).",1 Introduction,[0],[0]
"As a result most systems fail to make correct extractions even in apparently straightforward cases like Figure 1,
35
where the best system at the NIST TAC Knowledge Base Population (TAC KBP) 2015 evaluation failed to recognize the relation between Penner and Dillman.1 Consequently most automatic systems continue to make heavy use of hand-written rules or patterns because it has been hard for machine learning systems to achieve adequate precision or to generalize as well across text types.",1 Introduction,[0],[0]
"We believe machine learning approaches have suffered from two key problems: (1) the models used have been insufficiently tailored to relation extraction, and (2) there has been insufficient annotated data available to satisfy the training of data-hungry models, such as deep learning models.
",1 Introduction,[0],[0]
This work addresses both of these problems.,1 Introduction,[0],[0]
"We propose a new, effective neural network sequence model for relation classification.",1 Introduction,[0],[0]
Its architecture is better customized for the slot filling task: the word representations are augmented by extra distributed representations of word position relative to the subject and object of the putative relation.,1 Introduction,[0],[0]
This means that the neural attention model can effectively exploit the combination of semantic similarity-based attention and positionbased attention.,1 Introduction,[0],[0]
"Secondly, we markedly improve the availability of supervised training data by using Mechanical Turk crowd annotation to produce a large supervised training dataset (Table 1), suitable for the common relations between people, organizations and locations which are used in the TAC KBP evaluations.",1 Introduction,[0],[0]
"We name this dataset the TAC Relation Extraction Dataset (TACRED), and will make it available through the Linguistic Data Consortium (LDC) in order to respect copyrights on the underlying text.
",1 Introduction,[0],[0]
Combining these two gives a system with markedly better slot filling performance.,1 Introduction,[0],[0]
"This is
1Note: former spouses count as spouses in the ontology.
shown not only for a relation classification task on the crowd-annotated data but also for the incorporation of the resulting classifiers into a complete cold start knowledge base population system.",1 Introduction,[0],[0]
"On TACRED, our system achieves a relation classification F1 score that is 7.9% higher than that of a strong feature-based classifier, and 3.5% higher than that of the best previous neural architecture that we re-implemented.",1 Introduction,[0],[0]
"When this model is used in concert with a pattern-based system on the TAC KBP 2015 Cold Start Slot Filling evaluation data, the system achieves an F1 score of 26.7%, which exceeds the previous state-of-the-art by 4.5% absolute.",1 Introduction,[0],[0]
While this performance certainly does not solve the knowledge base population problem – achieving sufficient recall remains a formidable challenge – this is nevertheless notable progress.,1 Introduction,[0],[0]
"Existing work on neural relation extraction (e.g., Zeng et al., 2014; Nguyen and Grishman, 2015; Zhou et al., 2016) has focused on convolutional neural networks (CNNs), recurrent neural networks (RNNs), or their combination.",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"While these models generally work well on the datasets they are tested on, as we will show, they often fail to generalize to the longer sentences that are common in real-world text (such as in TAC KBP).
",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"We believe that existing model architectures suffer from two problems: (1) Although modern sequence models such as Long Short-Term Memory (LSTM) networks have gating mechanisms to control the relative influence of each individual word to the final sentence representation (Hochreiter and Schmidhuber, 1997), these controls are not explicitly conditioned on the entire sentence being classified; (2) Most existing work either
does not explicitly model the positions of entities (i.e., subject and object) in the sequence, or models the positions only within a local region.
",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"Here, we propose a new neural sequence model with a position-aware attention mechanism over an LSTM network to tackle these challenges.",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"This model can (1) evaluate the relative contribution of each word after seeing the entire sequence, and (2) base this evaluation not only on the semantic information of the sequence, but also on the global positions of the entities within the sequence.
",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
We formalize the relation extraction task as follows: Let X =,2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"[x1, ..., xn] denote a sentence, where xi is the i-th token.",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"A subject entity s and an object entity o are identified in the sentence, corresponding to two non-overlapping consecutive spans:",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
Xs =,2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"[xs1 , xs1+1, . . .",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
", xs2 ] and Xo =",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"[xo1 , xo1+1, . . .",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
", xo2 ].",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"Given the sentence X and the positions of s and o, the goal is to predict a relation r 2 R (R is the set of relations) that holds between s and o or no relation otherwise.
",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"Inspired by the position encoding vectors used in Collobert et al. (2011) and Zeng et al. (2014), we define a position sequence relative to the subject entity [ps1, ..., p s n], where
psi = 8>",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
<,2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
>,2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
": i s1, i < s1 0, s1  i  s2 i s2, i > s2
(1)
",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"Here s1, s2 are the starting and ending indices of the subject entity respectively, and psi 2 Z can be viewed as the relative distance of token xi to the subject entity.",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"Similarly, we obtain a position sequence [po1, ..., p o n] relative to the object entities.
",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
Let x,2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
=,2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"[x1, ...,xn] be word embeddings of the sentence, obtained using an embedding matrix E. Similarly, we obtain position embedding vectors ps =",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"[ps1, ...,p s n] and p o =",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"[po1, ...,p",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
o n,2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
] using a shared position embedding matrix P respectively.,2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"Next, as shown in Figure 2, we obtain hidden state representations of the sentence by feeding x into an LSTM:
{h1, ...,hn} = LSTM({x1, ...,xn}) (2)
We define a summary vector q =",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"hn (i.e., the output state of the LSTM).",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
This summary vector encodes information about the entire sentence.,2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"Then for each hidden state hi, we calculate an attention weight ai as:
ui = v> tanh(Whhi + Wqq+
Wspsi +",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"Wop o i ) (3)
ai = exp(ui)Pn
j=1 exp(uj) (4)
",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"Here Wh,Wq 2 Rda⇥d, Ws,Wo 2 Rda⇥dp and v 2 Rda are learnable parameters of the network, where d is the dimension of hidden states, dp is the dimension of position embeddings, and da is the size of attention layer.",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"Additional parameters of the network include embedding matrices E 2 R|V|⇥d and P 2 R(2L 1)⇥dp , where V is the vocabulary and L is the maximum sentence length.
",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
We regard attention weight ai as the relative contribution of the specific word to the sentence representation.,2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"The final sentence representation z is computed as:
z = Xn
i=1",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"aihi (5)
z is later fed into a fully-connected layer followed by a softmax layer for relation classification.
",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"Note that our model significantly differs from the attention mechanism in Bahdanau et al. (2015) and Zhou et al. (2016) in our use of the summary vector and position embeddings, and the way our attention weights are computed.",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"An intuitive way to understand the model is to view the attention calculation as a selection process, where the goal is to select relevant contexts over irrelevant ones.
",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"Here the summary vector (q) helps the model to base this selection on the semantic information of the entire sentence (rather than on each word only), while the position vectors (psi and p o i ) provides important spatial information between each word and the entities.",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
Previous research has shown that slot filling systems can greatly benefit from supervised data.,3 The TAC Relation Extraction Dataset,[0],[0]
"For example, Angeli et al. (2014b) showed that even a small amount of supervised data can boost the end-to-end F1 score by 3.9% on the TAC KBP tasks.",3 The TAC Relation Extraction Dataset,[0],[0]
"However, existing relation extraction datasets such as the SemEval-2010 Task 8 dataset (Hendrickx et al., 2009) and the Automatic Content Extraction (ACE) (Strassel et al., 2008) dataset are less useful for this purpose.",3 The TAC Relation Extraction Dataset,[0],[0]
"This is mainly because: (1) these datasets are relatively small for effectively training high-capacity models (see Table 2), and (2) they capture very different types of relations.",3 The TAC Relation Extraction Dataset,[0],[0]
"For example, the SemEval dataset focuses on semantic relations (e.g., CauseEffect, Component-Whole) between two nominals.
",3 The TAC Relation Extraction Dataset,[0],[0]
"One can further argue that it is easy to obtain a large amount of training data using distant supervision (Mintz et al., 2009).",3 The TAC Relation Extraction Dataset,[0],[0]
"In practice, however, due to the large amount of noise in the induced data, training relation extractors that perform well becomes very difficult.",3 The TAC Relation Extraction Dataset,[0],[0]
"For example, Riedel et al. (2010) show that up to 31% of the distantly supervised labels are wrong when creating training data from aligning Freebase to newswire text.
",3 The TAC Relation Extraction Dataset,[0],[0]
"To tackle these challenges, we collect a large supervised dataset TACRED, targeted towards the TAC KBP relations.
",3 The TAC Relation Extraction Dataset,[0],[0]
Data collection.,3 The TAC Relation Extraction Dataset,[0],[0]
We create TACRED based on query entities and annotated system responses in the yearly TAC KBP evaluations.,3 The TAC Relation Extraction Dataset,[0],[0]
"In each year of the TAC KBP evaluation (2009–2015), 100 entities (people or organizations) are given as queries,
for which participating systems should find associated relations and object entities.",3 The TAC Relation Extraction Dataset,[0],[0]
We make use of Mechanical Turk to annotate each sentence in the source corpus that contains one of these query entities.,3 The TAC Relation Extraction Dataset,[0],[0]
"For each sentence, we ask crowd workers to annotate both the subject and object entity spans and the relation types.
",3 The TAC Relation Extraction Dataset,[0],[0]
Dataset stratification.,3 The TAC Relation Extraction Dataset,[0],[0]
"In total we collect 119,474 examples.",3 The TAC Relation Extraction Dataset,[0],[0]
"We stratify TACRED across years in which the TAC KBP challenge was run, and use examples corresponding to query entities from 2009 to 2012 as training split, 2013 as development split, and 2014 as test split.",3 The TAC Relation Extraction Dataset,[0],[0]
"We reserve the TAC KBP 2015 evaluation data for running slot filling evaluations, as presented in Section 4.",3 The TAC Relation Extraction Dataset,[0],[0]
"Detailed statistics are given in Table 3.
Discussion.",3 The TAC Relation Extraction Dataset,[0],[0]
Table 1 presents sampled examples from TACRED.,3 The TAC Relation Extraction Dataset,[0],[0]
"Compared to existing datasets, TACRED has four advantages.",3 The TAC Relation Extraction Dataset,[0],[0]
"First, it contains an order of magnitude more relation instances (Table 2), enabling the training of expressive models.",3 The TAC Relation Extraction Dataset,[0],[0]
"Second, we reuse the entity and relation types of the TAC KBP tasks.",3 The TAC Relation Extraction Dataset,[0],[0]
We believe these relation types are of more interest to downstream applications.,3 The TAC Relation Extraction Dataset,[0],[0]
"Third, we fully annotate all negative instances that appear in our data collection process, to ensure that models trained on TACRED are not biased towards predicting false positives on realworld text.",3 The TAC Relation Extraction Dataset,[0],[0]
"Lastly, the average sentence length in TACRED is 36.2, compared to 19.1 in the SemEval dataset, reflecting the complexity of contexts in which relations occur in real-world text.
",3 The TAC Relation Extraction Dataset,[0],[0]
"Due to space constraints, we describe the data collection and validation process, system interfaces, and more statistics and examples of TACRED in the supplementary material.",3 The TAC Relation Extraction Dataset,[0],[0]
We will make TACRED publicly available through the LDC.,3 The TAC Relation Extraction Dataset,[0],[0]
"In this section we evaluate the effectiveness of our proposed model and TACRED on improving slot
filling systems.",4 Experiments,[0],[0]
"Specifically, we run two sets of experiments: (1) we evaluate model performance on the relation extraction task using TACRED, and (2) we evaluate model performance on the TAC KBP 2015 cold start slot filling task, by training the models on TACRED.",4 Experiments,[0],[0]
"We compare our model against the following baseline models for relation extraction and slot filling:
TAC KBP 2015 winning system.",4.1 Baseline Models,[0],[0]
"To judge our proposed model against a strong baseline, we compare against Stanford’s top performing system on the TAC KBP 2015 cold start slot filling task (Angeli et al., 2015).",4.1 Baseline Models,[0],[0]
At the core of this system are two relation extractors: a pattern-based extractor and a logistic regression (LR) classifier.,4.1 Baseline Models,[0],[0]
"The pattern-based system uses a total of 4,528 surface patterns and 169 dependency patterns.",4.1 Baseline Models,[0],[0]
The logistic regression model was trained on approximately 2 million bootstrapped examples (using a small annotated dataset and high-precision pattern system output) that are carefully tuned for TAC KBP slot filling evaluation.,4.1 Baseline Models,[0],[0]
"It uses a comprehensive feature set similar to the MIML-RE system for relation extraction (Surdeanu et al., 2012), including lemmatized n-grams, sequence NER tags and POS tags, positions of entities, and various features over dependency paths, etc.
",4.1 Baseline Models,[0],[0]
Convolutional neural networks.,4.1 Baseline Models,[0],[0]
We follow the 1-dimensional CNN architecture by Nguyen and Grishman (2015) for relation extraction.,4.1 Baseline Models,[0],[0]
"This model learns a representation of the input sentence, by first running a series of convolutional operations on the sentence with various filters, and then feeding the output into a max-pooling layer to reduce the dimension.",4.1 Baseline Models,[0],[0]
The resulting representation is then fed into a fully-connected layer followed by a softmax layer for relation classification.,4.1 Baseline Models,[0],[0]
"As an extension, positional embeddings are also introduced into this model to better capture the relative position of each word to the subject and object entities and were shown to achieve improved results.",4.1 Baseline Models,[0],[0]
"We use “CNN-PE” to represent the CNN model with positional embeddings.
",4.1 Baseline Models,[0],[0]
Dependency-based recurrent neural networks.,4.1 Baseline Models,[0],[0]
"In dependency-based neural models, shortest dependency paths between entities are often used as input to the neural networks.",4.1 Baseline Models,[0],[0]
"The intuition is to eliminate tokens that are potentially less relevant
to the classification of the relation.",4.1 Baseline Models,[0],[0]
"For the example in Figure 1, the shortest dependency path between the two entities is:
[Penner] survived!",4.1 Baseline Models,[0],[0]
brother !,4.1 Baseline Models,[0],[0]
wife!,4.1 Baseline Models,[0],[0]
"[Lisa Dillman]
We follow the SDP-LSTM model proposed by Xu et al. (2015b).",4.1 Baseline Models,[0],[0]
"In this model, each shortest dependency path is divided into two separate sub-paths from the subject entity and the object entity to the lowest common ancestor node.",4.1 Baseline Models,[0],[0]
"Each sub-path is fed into an LSTM network, and the resulting hidden units at each word position are passed into a max-over-time pooling layer to form the output of this sub-path.",4.1 Baseline Models,[0],[0]
"Outputs from the two sub-paths are then concatenated to form the final representation.
",4.1 Baseline Models,[0],[0]
"In addition to the above models, we also compare our proposed model against an LSTM sequence model without attention mechanism.",4.1 Baseline Models,[0],[0]
We map words that occur less than 2 times in the training set to a special <UNK> token.,4.2 Implementation Details,[0],[0]
"We use the pre-trained GloVe vectors (Pennington et al., 2014) to initialize word embeddings.",4.2 Implementation Details,[0],[0]
"For all the LSTM layers, we find that 2-layer stacked LSTMs generally work better than one-layer LSTMs.",4.2 Implementation Details,[0],[0]
"We minimize cross-entropy loss over all 42 relations using AdaGrad (Duchi et al., 2011).",4.2 Implementation Details,[0],[0]
We apply Dropout with p = 0.5 to CNNs and LSTMs.,4.2 Implementation Details,[0],[0]
During training we also find a word dropout strategy to be very effective: we randomly set a token to be <UNK> with a probability p.,4.2 Implementation Details,[0],[0]
"We set p to be 0.06 for the SDP-LSTM model and 0.04 for all other models.
",4.2 Implementation Details,[0],[0]
Entity masking.,4.2 Implementation Details,[0],[0]
We replace each subject entity in the original sentence with a special <NER>SUBJ token where <NER> is the corresponding NER signature of the subject as provided in TACRED.,4.2 Implementation Details,[0],[0]
We do the same processing for object entities.,4.2 Implementation Details,[0],[0]
"This processing step helps (1) provide a model with entity type information, and (2) prevent a model from overfitting its predictions to specific entities.
",4.2 Implementation Details,[0],[0]
Multi-channel augmentation.,4.2 Implementation Details,[0],[0]
"Instead of using only word vectors as input to the network, we augment the input with part-of-speech (POS) and named entity recognition (NER) embeddings.",4.2 Implementation Details,[0],[0]
We run Stanford CoreNLP,4.2 Implementation Details,[0],[0]
"(Manning et al., 2014) to obtain the POS and NER annotations.
",4.2 Implementation Details,[0],[0]
We describe our model hyperparameters and training in detail in the supplementary material.,4.2 Implementation Details,[0],[0]
We first evaluate all models on TACRED.,4.3 Evaluation on TACRED,[0],[0]
We train each model for 5 separate runs with independent random initializations.,4.3 Evaluation on TACRED,[0],[0]
For each run we perform early stopping using the dev set.,4.3 Evaluation on TACRED,[0],[0]
"We then select the run (among 5) that achieves the median F1 score on the dev set, and report its test set performance.
",4.3 Evaluation on TACRED,[0],[0]
Table 4 summarizes our results.,4.3 Evaluation on TACRED,[0],[0]
"We observe that all neural models achieve higher F1 scores than the logistic regression and patterns systems, which demonstrates the effectiveness of neural models for relation extraction.",4.3 Evaluation on TACRED,[0],[0]
"Although positional embeddings help increase the F1 by around 2% over the plain CNN model, a simple (2-layer) LSTM model performs surprisingly better than CNN and dependency-based models.",4.3 Evaluation on TACRED,[0],[0]
"Lastly, our proposed position-aware mechanism is very effective and achieves an F1 score of 65.4%, with an absolute increase of 3.9% over the best baseline neural model (LSTM) and 7.9% over the baseline logistic regression system.",4.3 Evaluation on TACRED,[0],[0]
"We also run an ensemble of our position-aware attention model which takes majority votes from 5 runs with random initializations and it further pushes the F1 score up by 1.6%.
",4.3 Evaluation on TACRED,[0],[0]
We find that different neural architectures show a different balance between precision and recall.,4.3 Evaluation on TACRED,[0],[0]
CNN-based models tend to have higher precision; RNN-based models have better recall.,4.3 Evaluation on TACRED,[0],[0]
This can be explained by noting that the filters in CNNs are essentially a form of “fuzzy n-gram patterns”.,4.3 Evaluation on TACRED,[0],[0]
"Second, we evaluate the slot filling performance of all models using the TAC KBP 2015 cold start slot filling task (Ellis et al., 2015).",4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
"In this task, about 50k newswire and Web forum documents are selected as the evaluation corpus.",4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
A slot filling system is asked to answer a series of queries with two-hop slots (Figure 3):,4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
"The first slot asks about fillers of a relation with the query entity as the subject (Mike Penner), and we term this a hop-0 slot; the second slot asks about fillers with the system’s hop-0 output as the subject, and we term this a hop-1 slot.",4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
"System predictions are then evaluated against gold annotations, and micro-averaged precision, recall and F1 scores are calculated at the hop-0 and hop-1 levels.",4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
"Lastly hop-all scores are calculated by combining hop-0 and hop-1 scores.2
Evaluating relation extraction systems on slot filling is particularly challenging in that: (1) Endto-end cold start slot filling scores conflate the performance of all modules in the system (i.e., entity recognizer, entity linker and relation extractor).",4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
(2) Errors in hop-0 predictions can easily propagate to hop-1 predictions.,4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
"To fairly evaluate each relation extraction model on this task, we use Stanford’s 2015 slot filling system as our basic pipeline.3 It is a very strong baseline specifically tuned for TAC KBP evaluation and ranked top in the 2015 evaluation.",4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
"We then plug in the corresponding relation extractor trained on TACRED, keeping all other modules unchanged.
",4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
Table 5 presents our results.,4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
"We find that: (1) by only training our logistic regression model on TACRED (in contrast to on the 2 million bootstrapped examples used in the 2015 Stanford system) and combining it with patterns, we obtain a higher hop-0 F1 score than the 2015 Stanford sys-
2In the TAC KBP cold start slot filling evaluation, a hop-1 slot is transferred to a pseudo-slot which is treated equally as a hop-0 slot.",4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
"Hop-all precision, recall and F1 are then calculated by combining these pseudo-slot predictions and hop-0 predictions.
",4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
3This system uses the fine-grained NER system in Stanford CoreNLP,4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
"(Manning et al., 2014) for entity detection and the Illinois Wikifier (Ratinov et al., 2011) for entity linking.
",4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
"tem, and a similar hop-all F1; (2) our proposed position-aware attention model substantially outperforms the 2015 Stanford system on all hop-0, hop-1 and hop-all F1 scores.",4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
"Combining it with the patterns, we achieve a hop-all F1 of 26.7%, an absolute improvement of 4.5% over the previous state-of-the-art result.",4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
Model ablation.,4.5 Analysis,[0],[0]
Table 6 presents the results of an ablation test of our position-aware attention model on the development set of TACRED.,4.5 Analysis,[0],[0]
"The entire attention mechanism contributes about 1.5% F1, where the position-aware term in Eq.",4.5 Analysis,[0],[0]
"(3) alone contributes about 1% F1 score.
",4.5 Analysis,[0],[0]
Impact of negative examples.,4.5 Analysis,[0],[0]
"Figure 4 shows how the slot filling evaluation scores change as we change the amount of negative (i.e., no relation) training data provided to our proposed model.",4.5 Analysis,[0],[0]
"We find that: (1) At hop-0 level, precision increases as we provide more negative examples, while recall stays almost unchanged.",4.5 Analysis,[0],[0]
F1 score keeps increasing.,4.5 Analysis,[0],[0]
"(2) At hop-all level, F1 score increases by
about 10% as we change the amount of negative examples from 20% to 100%.
",4.5 Analysis,[0],[0]
Performance by sentence length.,4.5 Analysis,[0],[0]
Figure 5 shows performance on varying sentence lengths.,4.5 Analysis,[0],[0]
We find that: (1) Performance of all models degrades substantially as the sentences get longer.,4.5 Analysis,[0],[0]
"(2) Compared to the baseline Logistic Regression model, all neural models handle long sentences better.",4.5 Analysis,[0],[0]
"(3) Compared to CNN-PE model, RNNbased models are more robust on long sentences, and notably SDP-LSTM model is least sensitive to sentence length.",4.5 Analysis,[0],[0]
"(4) Our proposed model achieves equal or better results on sentences of all lengths, except for sentences with more than 60 tokens where SDP-LSTM model achieves the best result.
",4.5 Analysis,[0],[0]
Improvement by slot types.,4.5 Analysis,[0],[0]
We calculate the F1 score for each slot type and compare the improvement from using our proposed model across slot types.,4.5 Analysis,[0],[0]
"When compared with the CNN-PE model, our position-aware attention model achieves improved F1 scores on 30 out of the 41 slot types, with the top 5 slot types being org:members, per:country of death, org:shareholders, per:children and per:religion.",4.5 Analysis,[0],[0]
"When compared with SDP-LSTM model, our model achieves improved F1 scores on 26 out of the 41 slot types, with the top 5 slot types being org:political/religious affiliation, per:country of death, org:alternate names, per:religion and per:alternate names.",4.5 Analysis,[0],[0]
"We observe that slot types with relatively sparse training examples tend to be improved by using the position-aware attention model.
",4.5 Analysis,[0],[0]
Attention visualization.,4.5 Analysis,[0],[0]
"Lastly, Figure 6 shows the visualization of attention weights assigned by our model on sampled sentences from the development set.",4.5 Analysis,[0],[0]
"We find that the model learns to pay more attention to words that are informative for the relation (e.g., “graduated from”, “niece” and “chairman”), though it still makes mistakes (e.g., “refused to name the three”).",4.5 Analysis,[0],[0]
"We also observe that the model tends to put a lot of weight onto object entities, as the object NER signatures are very informative to the classification of relations.",4.5 Analysis,[0],[0]
Relation extraction.,5 Related Work,[0],[0]
"There are broadly three main lines of work on relation extraction: first, fully-supervised approaches (Zelenko et al., 2003; Bunescu and Mooney, 2005), where a statisti-
cal classifier is trained on an annotated dataset; second, distant supervision (Mintz et al., 2009; Surdeanu et al., 2012), where a training set is formed by projecting the relations in an existing knowledge base onto textual instances that contain the entities that the relation connects; and third, Open IE (Fader et al., 2011; Mausam et al., 2012), which views its goal as producing subject-relationobject triples and expressing the relation in text.
",5 Related Work,[0],[0]
Slot filling and knowledge base population.,5 Related Work,[0],[0]
"The most widely-known effort to evaluate slot filling and KBP systems is the yearly TAC KBP slot filling tasks, starting from 2009 (McNamee and Dang, 2009).",5 Related Work,[0],[0]
"Participants in slot filling tasks usually make use of hybrid systems that combine patterns, Open IE, distant supervision and supervised systems for relation extraction (Kisiel et al., 2015; Finin et al., 2015; Zhang et al., 2016).
",5 Related Work,[0],[0]
Datasets for relation extraction.,5 Related Work,[0],[0]
"Popular general-domain datasets include the ACE dataset (Strassel et al., 2008) and the SemEval-2010 task 8 dataset (Hendrickx et al., 2009).",5 Related Work,[0],[0]
"In addition, the BioNLP Shared Tasks (Kim et al., 2009) are yearly efforts on creating datasets and evaluations for biomedical information extraction systems.
",5 Related Work,[0],[0]
Deep learning models for relation extraction.,5 Related Work,[0],[0]
"Many deep learning models have been proposed for relation extraction, with a focus on end-to-end training using CNNs (Zeng et al., 2014; Nguyen and Grishman, 2015) and RNNs (Zhang et al., 2015).",5 Related Work,[0],[0]
"Other popular approaches include using CNN or RNN over dependency paths between entities (Xu et al., 2015a,b), augmenting RNNs with different components (Xu et al., 2016; Zhou et al., 2016), and combining RNNs and CNNs (Vu et al., 2016; Wang et al., 2016).",5 Related Work,[0],[0]
Adel et al. (2016) compares the performance of CNN models against traditional approaches on slot filling using a portion of the TAC KBP evaluation data.,5 Related Work,[0],[0]
"We introduce a state-of-the-art position-aware neural sequence model for relation extraction, as well as TACRED, a large-scale, crowd-sourced dataset that is orders of magnitude larger than previous relation extraction datasets.",6 Conclusion,[0],[0]
Our proposed model outperforms a strong feature-based classifier and all baseline neural models.,6 Conclusion,[0],[0]
"In combination with the new dataset, it improves the state-of-the-
art hop-all F1 on the TAC KBP 2015 slot filling task by 4.5% absolute.",6 Conclusion,[0],[0]
We thank the anonymous reviewers for their helpful suggestions.,Acknowledgments,[0],[0]
We gratefully acknowledge the support of the Allen Institute for Artificial Intelligence and the support of the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract,Acknowledgments,[0],[0]
No. FA8750-13-2-0040.,Acknowledgments,[0],[0]
"Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the DARPA, AFRL, or the US government.",Acknowledgments,[0],[0]
Organized relational knowledge in the form of “knowledge graphs” is important for many applications.,abstractText,[0],[0]
"However, the ability to populate knowledge bases with facts automatically extracted from documents has improved frustratingly slowly.",abstractText,[0],[0]
This paper simultaneously addresses two issues that have held back prior work.,abstractText,[0],[0]
"We first propose an effective new model, which combines an LSTM sequence model with a form of entity position-aware attention that is better suited to relation extraction.",abstractText,[0],[0]
"Then we build TACRED, a large (119,474 examples) supervised relation extraction dataset, obtained via crowdsourcing and targeted towards TAC KBP relations.",abstractText,[0],[0]
The combination of better supervised data and a more appropriate high-capacity model enables much better relation extraction performance.,abstractText,[0],[0]
"When the model trained on this new dataset replaces the previous relation extraction component of the best TAC KBP 2015 slot filling system, its F1 score increases markedly from 22.2% to 26.7%.",abstractText,[0],[0]
Position-aware Attention and Supervised Data Improve Slot Filling,title,[0],[0]
There are many cases in Bayesian modeling where a certain choice of prior distribution allows for computationally simple or tractable inference.,1. Introduction,[0],[0]
"For example,
• Conjugate priors yield posteriors with a known parametric form and therefore allow for non-iterative, exact inference (Diaconis et al., 1979).
",1. Introduction,[0],[0]
"• Certain priors yield models with tractable conditional or marginal distributions, which allows efficient approximate inference algorithms to be applied (e.g. Gibbs sampling (Smith & Roberts, 1993), sampling
1Carnegie Mellon University, Machine Learning Department, Pittsburgh, USA 2CMU School of Computer Science.",1. Introduction,[0],[0]
"Correspondence to: Willie Neiswanger <willie@cs.cmu.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
in collapsed models (Teh et al., 2006), or mean-field variational methods (Wang & Blei, 2013)).
",1. Introduction,[0],[0]
"• Simple parametric priors allow for computationally cheap density queries, maximization, and sampling, which can reduce costs in iterative inference algorithms (e.g. Metropolis-Hastings (Metropolis et al., 1953), gradient-based MCMC (Neal, 2011), or sequential Monte Carlo (Doucet et al., 2000)).
",1. Introduction,[0],[0]
"For these reasons, one might hope to infer a result under a convenient-but-unrealistic prior, and afterwards, attempt to correct the result.",1. Introduction,[0],[0]
"More generally, given an inference result (under a convenient prior or otherwise), one might wish to incorporate updated prior information, or see a result under different prior assumptions, without having to re-run a costly inference algorithm.
",1. Introduction,[0],[0]
"This leads to the main question of this paper: for a given model, is it possible to use any convenient false prior to infer a false posterior, and afterwards, given any target prior of interest, efficiently and accurately infer the associated target posterior?
",1. Introduction,[0],[0]
One potential strategy involves sampling from the false posterior and reweighting these samples via importance sampling (IS).,1. Introduction,[0],[0]
"However, depending on the chosen target prior—both its parametric form and similarity to the false prior—the resulting inference can be inaccurate due to high or infinite variance IS estimates (demonstrated in Sec. 2.1).
",1. Introduction,[0],[0]
We instead aim to devise a method that yields accurate inferences for arbitrary target priors.,1. Introduction,[0],[0]
"Furthermore, like IS, we want to make use of the pre-inferred false posterior, without simply running standard inference algorithms on the target posterior.",1. Introduction,[0],[0]
"Note that most standard inference algorithms are iterative and data-dependent: parameter updates at each iteration involve data, and the computational cost or quality of each update depends on the amount of data used.",1. Introduction,[0],[0]
"Hence, running inference algorithms directly on the target posterior can be costly (especially given a large amount of data or many target priors of interest) and defeats the purpose of using a convenient false prior.
",1. Introduction,[0],[0]
"In this paper, we propose prior swapping, an iterative, dataindependent method for generating accurate posterior samples under arbitrary target priors.",1. Introduction,[0],[0]
"Prior swapping uses the pre-inferred false posterior to perform efficient updates that
do not depend on the data, and thus proceeds very quickly.",1. Introduction,[0],[0]
"We therefore advocate breaking difficult inference problems into two easier steps: first, do inference using the most computationally convenient prior for a given model, and then, for all future priors of interest, use prior swapping.
",1. Introduction,[0],[0]
"In the following sections, we demonstrate the pitfalls of using IS, describe the proposed prior swapping methods for different types of false posterior inference results (e.g. exact or approximate density functions, or samples) and give theoretical guarantees for these methods.",1. Introduction,[0],[0]
"Finally, we show empirical results on heavy-tailed and sparsity priors in Bayesian generalized linear models, and relational priors over components in mixture and topic models.",1. Introduction,[0],[0]
"Suppose we have a dataset of n vectors xn = {x1, . . .",2. Methodology,[0],[0]
", xn},",2. Methodology,[0],[0]
"xi ∈ Rp, and we have chosen a family of models with the likelihood function L(θ|xn) = p(xn|θ), parameterized by θ ∈ Rd.",2. Methodology,[0],[0]
"Suppose we have a prior distribution over the space of model parameters θ, with probability density function (PDF) π(θ).",2. Methodology,[0],[0]
"The likelihood and prior define a joint model with PDF p(θ, xn) = π(θ)L(θ|xn).",2. Methodology,[0],[0]
"In Bayesian inference, we are interested in computing the posterior (conditional) distribution of this joint model, with PDF
p(θ|xn) = π(θ)L(θ|x n)∫
π(θ)L(θ|xn) dθ .",2. Methodology,[0],[0]
"(1)
Suppose we’ve chosen a different prior distribution πf (θ), which we refer to as a false prior (while we refer to π(θ) as the target prior).",2. Methodology,[0],[0]
"We can now define a new posterior
pf (θ|xn) =",2. Methodology,[0],[0]
"πf (θ)L(θ|xn)∫ πf (θ)L(θ)|xn) dθ
(2)
which we refer to as a false posterior.
",2. Methodology,[0],[0]
"We are interested in the following task: given a false posterior inference result (i.e. samples from pf (θ|xn), or some exact or approximate PDF), choose an arbitrary target prior π(θ) and efficiently sample from the associated target posterior p(θ|xn)—or, more generally, compute an expectation µh = Ep [h(θ)] for some test function h(θ) with respect to the target posterior.",2. Methodology,[0],[0]
"We begin by describing an initial strategy, and existing work in a related task known as prior sensitivity analysis.
",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
Suppose we have T false posterior samples {θ̃t}Tt=1 ∼ pf (θ|xn).,2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"In importance sampling (IS), samples from an importance distribution are used to estimate the expectation of a test function with respect to a target distribution.",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"A straightforward idea is to use the false posterior as an
importance distribution, and compute the IS estimate
µ̂ISh = T∑ t=1 w(θ̃t)h(θ̃t) (3)
where the weight function w(θ) ∝ p(θ|x n) pf (θ|xn) ∝ π(θ) πf (θ)
, and the T weights are normalized to sum to one.
",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
IS-based methods have been developed for the task of prior sensitivity analysis (PSA).,2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"In PSA, the goal is to determine how the posterior varies over a sequence of priors (e.g. over a parameterized family of priors π(θ; γi), i = 0, 1, . . .).",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"Existing work has proposed inferring a single posterior under prior π(θ; γ0), and then using IS methods to infer further posteriors in the sequence (Besag et al., 1995; Hastings, 1970; Bornn et al., 2010).
",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"This strategy is effective when subsequent priors are similar enough, but breaks down when two priors are sufficiently dissimilar, or are from ill-matched parametric families, which we illustrate in an example below.
",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"Note that, in general for IS, as T → ∞, µ̂ISh → µh almost surely.",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"However, IS estimates can still fail in practice if µ̂ISh has high or infinite variance.",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"If so, the variance of the weights w(θ̃t) will be large (a problem often referred to as weight degeneracy), which can lead to inaccurate estimates.",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"In our case, the variance of µ̂ISh is only finite if
Epf [ h(θ)2 π(θ)2
πf (θ)2
] ∝",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"Ep [ h(θ)2 π(θ)
πf (θ)
]",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"<∞. (4)
",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"For a broad class of h, this is satisfied",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"if there existsM ∈ R such that π(θ)πf (θ) < M, ∀θ (Geweke, 1989)",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
.,2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"Given some preinferred pf (θ|xn) with false prior πf (θ), the accuracy of IS thus depends on the target prior of interest.",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"For example, if π(θ) has heavier tails than πf (θ), the variance of µ̂ISh will be infinite for many h. Intuitively, we expect the variance to be higher for π that are more dissimilar to πf .
",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
We show a concrete example of this in Fig. 1.,2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"Consider a normal model for data xn ∼ N (θ, 1), with a standard normal false prior πf (θ) = N (θ|0, 1).",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"This yields a closedform false posterior (due to the conjugate πf ), which is also normal.",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"Suppose we’d like to estimate the posterior expectation under a Laplace target prior, with mean 10 and variance 1, for test function h(θ) = θ (i.e. an estimate of the target posterior mean).",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"We draw T false posterior samples {θ̃t}Tt=1 ∼ pf (θ|xn), compute weights w(θ̃t) and IS estimate µ̂ISh , and compare it with the true expectation µh.
",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
We see in Fig. 1 that |µh,2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"− µ̂ISh | slows significantly as T increases, and maintains a high error even as T is made very large.",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
We can analyze this issue theoretically.,2.1. Importance Sampling and Prior Sensitivity,[0],[0]
Suppose we want |µh,2.1. Importance Sampling and Prior Sensitivity,[0],[0]
− µ̂ISh | < δ.,2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"Since we know pf (θ|xn) is normal, we can compute a lower bound on the number of false posterior samples T that would be needed for
the expected estimate to be within δ of µh.",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"Namely, if pf (θ|xn) = N (θ|m, s2), in order for |µh",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
− Epf,2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"[µ̂ISh ]| < δ, we’d need
T ≥ exp { 1
2s2 (|µh −m|",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"− δ)2
} .
",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"In the example in Fig. 1, we have m = 1, s2 = 0.25, and µh = 7.9892.",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"Hence, for |µh",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
− Epf,2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"[µ̂ISh ]| < 1, we’d need T > 1031 samples (see appendix for full details of this analysis).",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"Note that this bound actually has nothing to do with the parametric form of π(θ)—it is based solely on the normal false posterior, and its distance to the target posterior mean µh.",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"However, even if this distance was small, the importance estimate would still have infinite variance due to the Laplace target prior.",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"Further, note that the situation can significantly worsen in higher dimensions, or if the false posterior has a lower variance.",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"We’d like a method that will work well even when false and target priors πf (θ) and π(θ) are significantly different, or are from different parametric families, with performance that does not worsen (in accuracy nor computational complexity) as the priors are made more dissimilar.
",2.2. Prior Swapping,[0],[0]
"Redoing inference for each new target posterior can be very costly, especially when the data size n is large, because the per-iteration cost of most standard inference algorithms scales with n, and many iterations may be needed for accurate inference.",2.2. Prior Swapping,[0],[0]
This includes both MCMC and sequential monte carlo (SMC) algorithms (i.e. repeated-ISmethods that infer a sequence of distributions).,2.2. Prior Swapping,[0],[0]
"In SMC, the per-iteration cost still scales with n, and the variance estimates can still be infinite if subsequent distributions are ill-matched.
",2.2. Prior Swapping,[0],[0]
"Instead, we aim to leverage the inferred false posterior to more-efficiently compute any future target posterior.",2.2. Prior Swapping,[0],[0]
We begin by defining a prior swap density ps(θ).,2.2. Prior Swapping,[0],[0]
"Suppose for now that a false posterior inference algorithm has returned a density function p̃f (θ) (we will give more details on p̃f
later; assume for now that it is either equal to pf (θ|xn) or approximates it).",2.2. Prior Swapping,[0],[0]
"We then define the prior swap density as
ps(θ) ∝",2.2. Prior Swapping,[0],[0]
"p̃f (θ)π(θ)
πf (θ) .",2.2. Prior Swapping,[0],[0]
"(5)
Note that if p̃f (θ) = pf (θ|xn), then ps(θ) = p(θ|xn).",2.2. Prior Swapping,[0],[0]
"However, depending on how we represent p̃f (θ), ps(θ) can have a much simpler analytic representation than p(θ|xn), which is typically defined via a likelihood function (i.e. a function of the data) and causes inference algorithms to have costs that scale with the data size n. Specifically, we will only use low-complexity p̃f (θ) that can be evaluated in constant time with respect to the data size",2.2. Prior Swapping,[0],[0]
"n.
Our general strategy is to use ps(θ) as a surrogate for p(θ|xn) in standard MCMC or optimization procedures, to yield data-independent algorithms with constant cost per iteration.",2.2. Prior Swapping,[0],[0]
"Intuitively, the likelihood information is captured by the false posterior—we make use of this instead of the likelihood function, which is costly to evaluate.
More concretely, at each iteration in standard inference algorithms, we must evaluate a data-dependent function associated with the posterior density.",2.2. Prior Swapping,[0],[0]
"For example, we evaluate a function proportional to p(θ|xn) in Metropolis-Hastings (MH) (Metropolis et al., 1953), and ∇θ log p(θ|xn) in gradient-based MCMC methods (such as Langevin dynamics (LD) (Rossky et al., 1978) and Hamiltonian Monte Carlo (HMC) (Neal, 2011)) and in optimization procedures that yield a MAP point estimate.",2.2. Prior Swapping,[0],[0]
"In prior swapping, we instead evaluate ps(θ) in MH, or ∇θ log ps(θ) in LD, HMC, or gradient optimization to a MAP estimate (see appendix for algorithm pseudocode).",2.2. Prior Swapping,[0],[0]
"Here, each iteration only requires evaluating a few simple analytic expressions, and thus has O(1) complexity with respect to data size.
",2.2. Prior Swapping,[0],[0]
"We demonstrate prior swapping on our previous example (using a normal false prior and Laplace target prior) in Fig. 2, where we have a closed-form (normal PDF) p̃f (θ).",2.2. Prior Swapping,[0],[0]
"To do prior swapping, we run a Metropolis-Hastings algorithm on the target density ps(θ).",2.2. Prior Swapping,[0],[0]
"Note that drawing each
sample in this Markov chain does not involve the data xn, and can be done in constant time with respect to n (which we can see by viewing the wall time for different T ).",2.2. Prior Swapping,[0],[0]
"In Fig. 2, we draw T samples {θt}Tt=1 ∼ ps(θ), compute a sample estimate µ̂PSh = 1 T ∑T t=1 θt, and compare it with the true value µh.",2.2. Prior Swapping,[0],[0]
We see that µ̂PSh converges to µh after a relatively small number of samples T.,2.2. Prior Swapping,[0],[0]
The previous method is only applicable if our false posterior inference result is a PDF p̃f (θ) (such as in closed-form inference or variational approximations).,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Here, we develop prior swapping methods for the setting where we only have access to samples {θ̃t} Tf t=1 ∼ pf (θ|xn).",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"We propose the following procedure:
1.",2.3. Prior Swapping with False Posterior Samples,[0],[0]
Use {θ̃t} Tf t=1 to form an estimate p̃f (θ) ≈ pf (θ|xn).,2.3. Prior Swapping with False Posterior Samples,[0],[0]
2.,2.3. Prior Swapping with False Posterior Samples,[0],[0]
Sample from ps(θ) ∝,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"π(θ)p̃f (θ)πf (θ) with prior swapping, as before.
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Note that, in general, ps(θ) only approximates p(θ|xn).",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"As a final step, after sampling from ps(θ), we can:
3.",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Apply a correction to samples from ps(θ).
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"We will describe two methods for applying a correction to ps samples—one involving importance sampling, and one involving semiparametric density estimation.",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Additionally, we will discuss forms for p̃f (θ), guarantees about these forms, and how to optimize the choice of p̃f (θ).",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"In particular, we will argue why (in constrast to the initial IS strategy) these methods do not fail when p(θ|xn) and pf (θ|xn) are very dissimilar or have ill-matching parametric forms.
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
Prior swap importance sampling.,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Our first proposal for applying a correction to prior swap samples involves IS: after estimating some p̃f (θ), and sampling {θt}Tt=1 ∼ ps(θ), we can treat {θt}Tt=1 as importance samples, and compute the IS estimate
µ̂PSish = T∑ t=1 w(θt)h(θt) (6)
where the weight function is now
w(θ) ∝ p(θ|x n) ps(θ) ∝",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"pf (θ|x n) p̃f (θ) (7)
and the weights are normalized so that ∑T t=1 w(θt) = 1.
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
The key difference between this and the previous IS strategy is the weight function.,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Recall that, previously, an accurate estimate depended on the similarity between π(θ) and πf (θ); both the distance to and parametric form of π(θ) could produce high or infinite variance estimates.",2.3. Prior Swapping with False Posterior Samples,[0],[0]
This was an issue because we wanted the procedure to work well for any π(θ).,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Now, however, the performance depends on the similarity between p̃f (θ) and pf (θ|xn)—and by using the false posterior samples, we can estimate a p̃f (θ) that well approximates pf (θ|xn).",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Additionally, we can prove that certain choices of p̃f (θ) guarantee a finite variance IS estimate.",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Note that the variance of µ̂PSish is only finite if
Epf [ h(θ)2 pf (θ|xn)2
p̃f (θ)2
] ∝",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Ep [ h(θ)2
pf (θ|xn) p̃f (θ)
]",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"<∞.
To bound this, it is sufficient to show that there exists M ∈ R such that pf (θ|x
n) p̃f (θ)
< M for all θ (assuming a test function h(θ) with finite variance) (Geweke, 1989).",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"To satisfy this condition, we will propose a certain parametric family p̃αf (θ).",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Note that, to maintain a prior swapping procedure with O(1) cost, we want a p̃αf (θ) that can be evaluated in constant time.",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"In general, a p̃αf (θ) with fewer terms will yield a faster procedure.",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"With these in mind, we propose the following family of densities.
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
Definition.,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"For a parameter α = (α1, . . .",2.3. Prior Swapping with False Posterior Samples,[0],[0]
", αk), αj ∈",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Rp, k > 0, let density p̃αf (θ) satisfy
p̃αf (θ) ∝",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"πf (θ) k∏ j=1 p(αj |θ)n/k (8)
where p(αj |θ) denotes the model conditional PDF.
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"The number of terms in p̃αf (θ) (and cost to evaluate) is determined by the parameter k. Note that this family is
inspired by the true form of the false posterior pf (θ|xn).",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"However, p̃αf (θ) has constant-time evaluation, and we can estimate its parameter α using samples {θ̃t} Tf t=1 ∼ pf (θ|xn).",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Furthermore, we have the following guarantees.",2.3. Prior Swapping with False Posterior Samples,[0],[0]
Theorem 2.1.,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"For any α = (α1, . . .",2.3. Prior Swapping with False Posterior Samples,[0],[0]
", αk) ⊂",2.3. Prior Swapping with False Posterior Samples,[0],[0]
Rp and k > 0,2.3. Prior Swapping with False Posterior Samples,[0],[0]
let p̃αf (θ) be defined as in Eq.,2.3. Prior Swapping with False Posterior Samples,[0],[0]
(8).,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Then, there existsM > 0",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"such that pf (θ|x n)
p̃αf (θ) < M , for all θ ∈ Rd.
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
Corollary 2.1.1.,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"For {θt}Tt=1 ∼ pαs (θ) ∝ p̃αf (θ)π(θ) πf (θ) , w(θt) = pf (θt|xn) p̃αf (θt) (∑T r=1",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"pf (θr|xn) p̃αf (θr) )−1 , and test function that satisfies Varp [h(θ)]",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"< ∞, the variance of IS estimate µ̂PSish = ∑T t=1 h(θt)w(θt) is finite.
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Proofs for these theorems are given in the appendix.
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
Note that we do not know the normalization constant for p̃αf (θ).,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"This is not an issue for its use in prior swapping, since we only need access to a function proportional to pαs (θ) ∝",2.3. Prior Swapping with False Posterior Samples,[0],[0]
p̃αf (θ)π(θ)πf (θ)−1 in most MCMC algorithms.,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"However, we still need to estimate α, which is an issue because the unknown normalization constant is a function of α.",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Fortunately, we can use the method of score matching (Hyvärinen, 2005) to estimate α given a density such as p̃αf (θ) with unknown normalization constant.
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Once we have found an optimal parameter α∗, we draw samples from pα ∗ s (θ) ∝ p̃α ∗ f (θ)π(θ)πf (θ) −1, compute weights for these samples (Eq. (7)), and compute the IS estimate µ̂PSish .",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"We give pseudocode for the full prior swap importance sampling procedure in Alg. 1.
Algorithm 1:",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Prior Swap Importance Sampling
Input: False posterior samples {θ̃t} Tf t=1 ∼ pf (θ|xn).
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
Output: IS estimate µ̂PSish .,2.3. Prior Swapping with False Posterior Samples,[0],[0]
1 Score matching: estimate α∗ using {θ̃t} Tf t=1.,2.3. Prior Swapping with False Posterior Samples,[0],[0]
2,2.3. Prior Swapping with False Posterior Samples,[0],[0]
Prior swapping: sample {θt}Tt=1 ∼ pα ∗ s (θ) ∝,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"p̃α ∗ f (θ)π(θ) πf (θ) .
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"3 Importance sampling: compute µ̂PSish = ∑T t=1 h(θt)w(θt).
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
Semiparametric prior swapping.,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"In the previous method, we chose a parametric form for p̃αf (θ); in general, even the optimal α will yield an inexact approximation to pf (θ|xn).",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Here, we aim to incorporate methods that return an increasingly exact estimate p̃f (θ) when given more false posterior samples {θ̃t} Tf t=1.
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
One idea is to use a nonparametric kernel density estimate p̃npf (θ) and plug this into p np s (θ) ∝,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"p̃ np f (θ)π(θ)πf (θ)
−1.",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"However, nonparametric density estimates can yield inaccurate density tails and fare badly in high dimensions.",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"To help mitigate these problems, we turn to a semiparametric estimate, which begins with a parametric estimate, and
adjusts it as samples are generated.",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"In particular, we use a density estimate that can be viewed as the product of a parametric density estimate and a nonparametric correction function (Hjort & Glad, 1995).",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"This density estimate is consistent as the number of samples Tf → ∞. Instead of (or in addition to) correcting prior swap samples with importance sampling, we can correct them by updating the nonparametric correction function as we continue to generate false posterior samples.
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Given Tf samples {θ̃t} Tf t=1 ∼ pf (θ|xn), we write the semiparametric false posterior estimate as
p̃spf (θ) = 1
Tf Tf∑ t=1
[ 1
bd K
( ‖θ − θ̃t‖
b
) p̃αf (θ)
p̃αf (θ̃t)
] , (9)
where K denotes a probability density kernel, with bandwidth b, where b→ 0 as Tf →∞ (see (Wasserman, 2006) for details on probability density kernels and bandwidth selection).",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"The semiparametric prior swap density is then
psps (θ) ∝",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"p̃spf (θ)π(θ)
πf (θ) =
1
Tf Tf∑ t=1
K ( ‖θ−θ̃t‖
b )",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"p̃αf (θ)π(θ)
p̃αf (θ̃t)πf (θ)b d
∝",2.3. Prior Swapping with False Posterior Samples,[0],[0]
[pαs (θ)]  1 Tf Tf∑ t=1,2.3. Prior Swapping with False Posterior Samples,[0],[0]
K,2.3. Prior Swapping with False Posterior Samples,[0],[0]
(,2.3. Prior Swapping with False Posterior Samples,[0],[0]
‖θ−θ̃t‖ b ) p̃αf (θ̃t)  .,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"(10) Hence, the prior swap density psps (θ) is proportional to the product of two densities: the parametric prior swap density pαs (θ), and a correction density.",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"To estimate expectations with respect to psps (θ), we can follow Alg.",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"1 as before, but replace the weight function in the final IS estimate with
w(θ) ∝",2.3. Prior Swapping with False Posterior Samples,[0],[0]
p sp s (θ) pαs (θ) ∝,2.3. Prior Swapping with False Posterior Samples,[0],[0]
1,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Tf Tf∑ t=1
K ( ‖θ−θ̃t‖
b ) p̃αf (θ̃t) .",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"(11)
One advantage of this strategy is that computing the weights doesn’t require the data—it thus has constant cost with respect to data size n (though its cost does increase with the number of false posterior samples Tf ).",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Additionally, as in importance sampling, we can prove that this procedure yields an exact estimate of E[h(θ)], asymptotically, as Tf → ∞ (and we can provide an explicit bound on the rate at which psps (θ) converges to p(θ|xn)).",2.3. Prior Swapping with False Posterior Samples,[0],[0]
We do this by showing that psps (θ) is consistent for p(θ|xn).,2.3. Prior Swapping with False Posterior Samples,[0],[0]
Theorem 2.2.,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Given false posterior samples {θ̃t} Tf t=1 ∼ pf (θ|xn) and b T−1/(4+d)f , the estimator psps is consistent for p(θ|xn), i.e. its mean-squared error satisfies
sup p(θ|xn)
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"E [∫
(psps (θ)− p(θ|xn))",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"2 dθ
] <
c
T 4/(4+d)",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"f
for some c > 0 and 0 < b ≤ 1.
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
The proof for this theorem is given in the appendix.,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"We show empirical results on Bayesian generalized linear models (including linear and logistic regression) with sparsity and heavy tailed priors, and on latent factor models (including mixture models and topic models) with relational priors over factors (e.g. diversity-encouraging, agglomerate-encouraging, etc.).",3. Empirical Results,[0],[0]
"We aim to demonstrate empirically that prior swapping efficiently yields correct samples and, in some cases, allows us to apply certain inference algorithms to more-complex models than was previously possible.",3. Empirical Results,[0],[0]
"In the following experiments, we will refer to the following procedures:
• Target posterior inference: some standard inference algorithm (e.g. MCMC) run on p(θ|xn).
",3. Empirical Results,[0],[0]
"• False posterior inference: some standard inference algorithm run on pf (θ|xn).
",3. Empirical Results,[0],[0]
"• False posterior IS: IS using samples from pf (θ|xn).
",3. Empirical Results,[0],[0]
"• Prior swap exact: prior swapping with closed-form p̃f (θ) = pf (θ|xn).
",3. Empirical Results,[0],[0]
• Prior swap parametric: prior swapping with parametric p̃αf (θ) given by Eq.,3. Empirical Results,[0],[0]
"(8).
",3. Empirical Results,[0],[0]
"• Prior swap IS: correcting samples from p̃αf (θ) with IS.
",3. Empirical Results,[0],[0]
"• Prior swap semiparametric: correcting samples from p̃αf (θ) with the semiparametric estimate IS procedure.
",3. Empirical Results,[0],[0]
"To assess performance, we choose a test function h(θ), and compute the Euclidean distance between µh = Ep[h(θ)] and some estimate µ̂h returned by a procedure.",3. Empirical Results,[0],[0]
We denote this performance metric by posterior error = ‖µh − µ̂h‖2.,3. Empirical Results,[0],[0]
"Since µh is typically not available analytically, we run a single chain of MCMC on the target posterior for one million steps, and use these samples as ground truth to compute µh.",3. Empirical Results,[0],[0]
"For timing plots, to assess error of a method at a given time point, we collect samples drawn before this time point, remove the first quarter as burn in, and add the time it takes to compute any of the corrections.",3. Empirical Results,[0],[0]
Sparsity-encouraging regularizers have gained a high level of popularity over the past decade due to their ability to produce models with greater interpretability and parsimony.,3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"For example, the L1 norm has been used to induce sparsity with great effect (Tibshirani, 1996), and has been shown to be equivalent to a mean-zero independent Laplace prior (Tibshirani, 1996; Seeger, 2008).",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"In a Bayesian setting, inference given a sparsity prior can be difficult, and often requires a computationally intensive method (such as MH or
HMC) or posterior approximations (e.g. expectation propagation (Minka, 2001)) that make factorization or parametric assumptions (Seeger, 2008; Gerwinn et al., 2010).",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"We propose a cheap yet accurate solution: first get an inference result with a more-tractable prior (such as a normal prior), and then use prior swapping to quickly convert the result to the posterior given a sparsity prior.
",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"Our first set of experiments are on Bayesian linear regression models, which we can write as yi =",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"Xiθ + , ∼ N (0, σ2), θ ∼ π, i = 1,...,n.",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"For π, we compute results on Laplace, Student’s t, and VerySparse (with PDF VerySparse(σ) = ∏d i=1 1 2σ exp{−|θi|
0.4/σ} (Seeger, 2008)) priors.",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"Here, a normal πf is conjugate and allows for exact false posterior inference.",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"Our second set of experiments are on Bayesian logistic regression models, which we write as yi ∼ Bern(pi), pi = logistic(Xiθ), θ ∼ π, i = 1,...,n.",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"which we will pair with both heavy tailed priors and a hierarchical target prior π = N (0, α−1I), α ∼ Gamma(γ, 1).",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"For these experiments, we also use a normal πf .",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"However, this false prior is no longer conjugate, and so we use MCMC to sample from pf (θ|xn).
",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"For linear regression, we use the YearPredictionMSD data set*, (n = 515345, d = 90), in which regression is used to predict the year associated with a a song, and for logistic regression we use the MiniBooNE particle identification data set†, (n = 130065, d = 50), in which binary classification is used to distinguish particles.
",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"In Fig. 3, we compare prior swapping and IS methods, in order to show that the prior swapping procedures yield accurate posterior estimates, and to compare their speeds of convergence.",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
We plot posterior error vs. wall time for each method’s estimate of the posterior mean Ep[h(θ)],3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"= Ep[θ] for two sparsity target priors (Laplace and VerySparse), for both linear and logistic regression.",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"In linear regression (only), since the normal conjugate πf allows us to compute a closed form pf (θ|xn), we can run the prior swap exact method, where p̃f (θ) = pf (θ|xn).",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"However, we can also sample from pf (θ|xn) to compute p̃α ∗
f (θ), and therefore compare methods such as prior swap parametric and the two correction methods.",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"In logistic regression, we do not have a closed form pf (θ|xn); here, we only compare the methods that make use of samples from pf (θ|xn).",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"In Fig. 3, we see that the prior swapping methods (particularly prior swap IS) quickly converge to nearly zero posterior error.",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"Additionally, in linear regression, we see that prior swap parametric, using p̃f (θ) = p̃α ∗
f (θ), yields similar posterior error as prior swap exact, which uses p̃f (θ) = p(θ|xn).
",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"*https://archive.ics.uci.edu/ml/datasets/ YearPredictionMSD
†https://archive.ics.uci.edu/ml/datasets/ MiniBooNE+particle+identification
In Fig. 4, we show how prior swapping can be used for fast inference in Bayesian linear models with sparsity or heavy-tailed priors.",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"We plot the time needed to first compute the false posterior (via exact inference) and then run prior swapping (via the MH procedure) on some target posterior, and compare this with the MH algorithm run directly on the target posterior.",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
In (a) and (b) we show convergence plots and see that prior swapping performs faster inference (by a few orders of magnitude) than direct MH.,3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"In plot (b) we reduce the variance of the target prior; while this hurts the accuracy of false posterior IS, prior swapping still quickly converges to zero error.",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"In (c) we show 1-d density marginals as we increase the prior sparsity, and in (d) we show prior swapping results for various sparsity priors.
",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"In the appendix, we also include results on logistic regression with the hierarchical target prior, as well as results for synthetic data where we are able to compare timing and posterior error as we tune n and d.",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"Many latent variable models in machine learning—such as mixture models, topic models, probabilistic matrix factorization, and others—involve a set of latent factors (e.g. components or topics).",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"Often, we’d like to use priors that encourage interesting behaviors among the factors.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"For example, we might want dissimilar factors through a diversity-promoting prior (Kwok & Adams, 2012; Xie et al., 2016) or for the factors to show some sort of sparsity pattern (Mayrink et al., 2013; Knowles & Ghahramani, 2011).",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"Inference in such models is often computationally expensive or designed on a case-by-case basis (Xie et al., 2016; Knowles & Ghahramani, 2011).
",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"However, when conjugate priors are placed over the factor parameters, collapsed Gibbs sampling can be applied.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"In this method, the factor parameters are integrated out, leaving only a subset of variables; on these, the conditional
(a) (b)
",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"Collapsed Gibbs Prior Swapping
W al
",3.2. Priors over Factors in Latent Variable Models,[0],[0]
l t,3.2. Priors over Factors in Latent Variable Models,[0],[0]
"im
e (s
ec o
n d
s)
0
0.5
1
1.5
2
2.5
3x 10 4
1.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
southern 2.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
northern 3.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
region 4.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
western 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
eastern 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
"south
Topic 6
Topic Model: False Posterior via Collapsed Gibbs
C lu
st er : G eo",3.2. Priors over Factors in Latent Variable Models,[0],[0]
gr,3.2. Priors over Factors in Latent Variable Models,[0],[0]
ap h y C,3.2. Priors over Factors in Latent Variable Models,[0],[0]
"lu st er : F am il y
1.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
west 2.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
south 3.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
coast 4.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
north 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
east 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
western Topic 171 1.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
north 2.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
asia 3.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
south 4.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
western 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
southern 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
eastern,3.2. Priors over Factors in Latent Variable Models,[0],[0]
"Topic 285
1.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
father 2.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
family 3.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
brother 4. born 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
son 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
children,3.2. Priors over Factors in Latent Variable Models,[0],[0]
Topic 11 1.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
children 2.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
daughter 3. born 4.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
son 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
family 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
father Topic 243 1. born 2. died 3.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
father 4.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
years 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
family 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
lived Topic 280 1. born 2.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
parents 3.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
studied 4. moved 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
age 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
year,3.2. Priors over Factors in Latent Variable Models,[0],[0]
"Topic 306
1.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
north 2.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
west 3.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
east 4.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
south 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
eastern 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
"western
Topic 353
Topic Model: Target Posterior via Prior Swapping
1.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
brother 2.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
sister 3.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
younger 4.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
older 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
youngest 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
sisters,3.2. Priors over Factors in Latent Variable Models,[0],[0]
Topic 11 1.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
husband 2.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
marriage 3.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
wife 4.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
death 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
marry 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
children,3.2. Priors over Factors in Latent Variable Models,[0],[0]
Topic 243 1.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
school 2.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
college 3.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
graduated 4.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
studies 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
university 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
fellow,3.2. Priors over Factors in Latent Variable Models,[0],[0]
Topic 306Topic 280 1.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
important 2.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
stayed 3.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
wrote 4. travelled 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
started 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
"died
1.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
territory 2.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
region 3.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
regions 4.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
provinces 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
capital 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
"territories
Topic 6
T er
ri to
ri es 1.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"bay
2.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
south 3.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
coast 4.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
area 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
land 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
"sea
Topic 171
C oa
st
1.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
america 2.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
europe 3.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
asia 4.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
world 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
countries 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
"africa
Topic 285
C ou
n tr
ie s 1.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"side
2.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
east 3.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
bordered 4.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
west 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
middle 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
"border
Topic 353
B or
d er
Si b
li n
gs
M ar
ri ag
e
B io
gr ap
h y
Sc h
oo",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"l
Pr io
r sw
ap p
in g
fo r
di ve
rs e
to p
ic s.
= D
iv er
se (0
.5 )
",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"(c)
Relational target priors (over factors )
−3 −2 −1 0 1 2 3 4
−3
−2
−1
0
1
2
3
4
−3 −2 −1 0 1 2 3 4
−3
−2
−1
0
1
2
3
4
−3 −2 −1 0 1 2 3 4
−3
−2
−1
0
1
2
3
4
−3 −2 −1 0 1 2 3 4
−3
−2
−1
0
1
2
3
4
−3 −2 −1 0 1 2 3 4
−3
−2
−1
0
1
2
3
4
−3 −2 −1 0 1 2 3 4
−3
−2
−1
0
1
2
3
4
−3 −2 −1 0 1 2 3 4
−3
−2
−1
0
1
2
3
4
−3 −2 −1 0 1 2 3 4
−3
−2
−1
0
1
2
3
4
−3 −2 −1 0 1 2 3 4
−3
−2
−1
0
1
2
3
4 3. Chain(0.1)
7. SparseChain(0.1) 8.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"Origin+Diverse(0.1)6. SparseAgglom(0.1)
",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"False-Posterior 1. Origin(0.1) 2. Agglom(0.1) 4. Diverse(0.1)
5.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"SparseOrigin(0.1)Wall time (seconds)
",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"Mixture Model: False Posterior via Collapsed Gibbs, Target Posterior via Prior Swapping
Figure 5.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
Latent factor models: (a) Prior swapping results for relational target priors (defined in (b)) over components in a mixture model.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
(c) Prior swapping with a diversity-promoting target prior on an LDA topic model (Simple English Wikipedia corpus) to separate redundant topic clusters; the top 6 words per topic are shown.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
"In (a, c)",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"we show wall times for the initial inference and prior swapping.
distributions can be computed analytically, which allows for Gibbs sampling over these variables.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"Afterwards, samples of the collapsed factor parameters can be computed.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"Hence, we propose the following strategy: first, assign a prior for the factor parameters that allows for collapsed Gibbs sampling; afterwards, reconstruct the factor samples and apply prior swapping for more complex relational priors over the factors.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"We can thus perform convenient inference in the collapsed model, yet apply more-sophisticated priors to variables in the uncollapsed model.
",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"We first show results on a Gaussian mixture model (GMM), written xi ∼ N (µzi ,Σzi), zi ∼ Dir(α), {µm}Mm=1 ∼ π, i = 1,...,n.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
Using a normal πf over {µm}Mm=1 allows for collapsed Gibbs sampling.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
"We also show results on a topic model (latent Dirichlet allocation (LDA) (Blei et al., 2003)) for text data (for the form of this model, see (Blei et al., 2003; Wang & Blei, 2011)).",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"Here, using a Dirichlet πf over topics allows for collapsed Gibbs sampling.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"For mixture models, we generate synthetic data from the above model (n=10,000, d=2, M=9), and for topic models, we use the Simple English Wikipedia‡ corpus (n=27,443 documents, vocab=10,192 words), and set M=400 topics.
‡https://simple.wikipedia.org/
In Fig. 5, we show results for mixture and topic models.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"In (a) we show inferred posteriors over GMM components for a number of relational target priors, which we define in (b).",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"In (c), we apply the diversity-promoting target prior to LDA, to separate redundant topics.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"Here, we show two topic clusters (“geography” and “family”) in pf (θ|xn), which are separated into distinct, yet thematically-similar, topics after prior swapping.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
In (a) and (c) we also show wall times of the inference methods.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
"Given some false posterior inference result, and an arbitrary target prior, we have studied methods to accurately compute the associated target posterior (or expectations with respect to it), and to do this efficiently by leveraging the pre-inferred result.",4. Conclusion,[0],[0]
We have argued and shown empirically that this strategy is effective even when the false and target posteriors are quite dissimilar.,4. Conclusion,[0],[0]
"We believe that this strategy shows promise to allow a wider range of (and possibly less-costly) inference alorithms to be applied to certain models, and to allow updated or new prior information to be more-easily incorporated into models without re-incurring the full costs of standard inference algorithms.",4. Conclusion,[0],[0]
"While Bayesian methods are praised for their ability to incorporate useful prior knowledge, in practice, convenient priors that allow for computationally cheap or tractable inference are commonly used.",abstractText,[0],[0]
"In this paper, we investigate the following question: for a given model, is it possible to compute an inference result with any convenient false prior, and afterwards, given any target prior of interest, quickly transform this result into the target posterior?",abstractText,[0],[0]
A potential solution is to use importance sampling (IS).,abstractText,[0],[0]
"However, we demonstrate that IS will fail for many choices of the target prior, depending on its parametric form and similarity to the false prior.",abstractText,[0],[0]
"Instead, we propose prior swapping, a method that leverages the pre-inferred false posterior to efficiently generate accurate posterior samples under arbitrary target priors.",abstractText,[0],[0]
"Prior swapping lets us apply less-costly inference algorithms to certain models, and incorporate new or updated prior information “post-inference”.",abstractText,[0],[0]
"We give theoretical guarantees about our method, and demonstrate it empirically on a number of models and priors.",abstractText,[0],[0]
Post-Inference Prior Swapping,title,[0],[0]
"We study the design of practically useful, theoretically wellfounded, general-purpose algorithms for the contextual bandits (CBs) problem.",1. Introduction,[0],[0]
"In this setting, the learner repeatedly receives context, then selects an action, resulting in a received reward.",1. Introduction,[0],[0]
"The aim is to learn a policy, a mapping from contexts to actions, to maximize the long-term cumulative reward.",1. Introduction,[0],[0]
"For instance, a news portal must repeatedly choose articles to present to each user to maximize clicks.",1. Introduction,[1.0],"['For instance, a news portal must repeatedly choose articles to present to each user to maximize clicks.']"
"Here, the context is information about the user, the actions are the articles, and the reward might be indicator of a click.",1. Introduction,[0],[0]
"We refer the reader to an ICML 2017 tutorial (http://hunch.net/
˜
rwil/) for further examples.
CB algorithms can be put into two groups.",1. Introduction,[0],[0]
"Some methods (Langford & Zhang, 2008; Agarwal et al., 2014) are
1Cornell University.",1. Introduction,[0],[0]
Work performed while the author was an intern at Microsoft Research.,1. Introduction,[0],[0]
2Microsoft Research 3University of Southern California.,1. Introduction,[0],[0]
"Correspondence to: Dylan J. Foster <djf244@cornell.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
agnostic in the sense that they are provably effective for any given policy class and data distribution.,1. Introduction,[0],[0]
"In contrast, realizability-based approaches such as LinUCB and variants (Chu et al., 2011; Li et al., 2017; Filippi et al., 2010) or Thompson sampling (Thompson, 1933) assume the data is generated from a particular parametrized family of models.",1. Introduction,[0],[0]
"Computationally tractable realizability-based algorithms are only known for specific model families, such as when the conditional reward distributions come from a generalized linear model.
",1. Introduction,[0],[0]
The two groups of approaches seem to have different advantages and disadvantages.,1. Introduction,[1.0],['The two groups of approaches seem to have different advantages and disadvantages.']
"Empirically, in the contextual semibandit setting, Krishnamurthy et al. (2016) found that the realizability-based LinUCB approach outperforms all agnostic baselines using a linear policy class.",1. Introduction,[0],[0]
"However, the agnostic approaches were able to overcome this shortcoming by using a more powerful policy class.",1. Introduction,[0],[0]
"Computationally, previous realizability-based approaches have been limited by their reliance on either closed-form confidence bounds (as in LinUCB variants), or the ability to efficiently sample from and frequently update the posterior (as in Thompson sampling).",1. Introduction,[0],[0]
"Agnostic approaches, on the other hand, typically assume an oracle for cost-sensitive classification, which is computationally intractable in the worst case, but often practically feasible for many natural policy classes.
",1. Introduction,[0],[0]
"In this paper, we aim to develop techniques that combine the best of both of these approaches.",1. Introduction,[0],[0]
"To this end, in Section 3, we propose computationally efficient and practical realizability-based algorithms for arbitrary model classes.",1. Introduction,[0],[0]
"As is often done in agnostic approaches, we assume the availability of an oracle which reduces to a standard learning setting and knows how to efficiently leverage the structure of the model class.",1. Introduction,[0],[0]
"Specifically, we require access to a leastsquares regression oracle over the model class that we use for predicting rewards given contexts.",1. Introduction,[0],[0]
"Since regression can often be solved efficiently, the availability of such an oracle is a far more reasonable assumption than the cost-sensitive classification oracle usually assumed, which typically must solve NP-hard problems.",1. Introduction,[1.0],"['Since regression can often be solved efficiently, the availability of such an oracle is a far more reasonable assumption than the cost-sensitive classification oracle usually assumed, which typically must solve NP-hard problems.']"
"In fact, for this reason, even the classification oracles are typically approximated by regression oracles in practice (see, e.g., Beygelzimer & Langford, 2009).",1. Introduction,[0],[0]
"Our main algorithmic components here are motivated by and adapted from a recent work of Krishnamurthy et al. (2017) on cost-sensitive active learning.
",1. Introduction,[0],[0]
"In Section 4, we prove that our algorithms are effective in achieving low regret under certain distributional assumptions.",1. Introduction,[0],[0]
"Specifically, we show that our methods enjoy low regret so long as certain quantities like the disagreement coefficient (Hanneke, 2014; Krishnamurthy et al., 2017) are bounded, or when some other distributional coefficients inspired by Bastani & Bayati (2015) are well-behaved.",1. Introduction,[0],[0]
"As a special consequence, we obtain nearly dimension-free results for sparse linear bandits in high dimensions.
",1. Introduction,[0],[0]
"Finally, in Section 5, we conduct a very extensive empirical evaluation of our algorithms on a number of datasets and against both realizability-based and agnostic baselines.",1. Introduction,[0],[0]
"In this test of practical effectiveness, we find that our approach gives comparable or superior results in nearly all cases, and we also validate the distributional assumptions required for low-regret guarantees on these datasets.",1. Introduction,[0],[0]
We consider the following contextual bandit protocol.,2. Preliminaries,[0],[0]
"Contexts are drawn from an arbitrary space, x ∈ X , actions are from a finite set, a ∈",2. Preliminaries,[0],[0]
"A ∶= {1, . . .",2. Preliminaries,[0],[0]
",K}, for some fixed K, and reward vectors are from a bounded set, r ∈",2. Preliminaries,[0],[0]
"[0,1]K , with component r(a) denoting the reward for action a ∈ A.",2. Preliminaries,[0],[0]
We consider an i.i.d.,2. Preliminaries,[0],[0]
"setting where there is a fixed and unknown distribution D over the context-reward pairs (x, r), with DX denoting its marginal over X .",2. Preliminaries,[0],[0]
"At each round t = 1,2, . . .",2. Preliminaries,[0],[0]
", T , nature samples (xt, rt) according to D and reveals xt to the learner.",2. Preliminaries,[0],[0]
The learner chooses an action at ∈ A and observes the reward rt(at).,2. Preliminaries,[0],[0]
The learner aims to maximize its reward and compete with strategies that model the expected reward E[r(a)  ,2. Preliminaries,[0],[0]
"x, a] via functions f ∶ X ×A → [0,1].",2. Preliminaries,[0],[0]
"We consider mappings f from a given class F , such as linear predictors or regression trees.",2. Preliminaries,[1.0],"['We consider mappings f from a given class F , such as linear predictors or regression trees.']"
"The main assumption this paper follows is that the class F is rich enough to contain a predictor that perfectly predicts the expected reward of any action under any context, that is: Assumption 1 (Realizability).",2. Preliminaries,[1.0],"['The main assumption this paper follows is that the class F is rich enough to contain a predictor that perfectly predicts the expected reward of any action under any context, that is: Assumption 1 (Realizability).']"
There is a predictor f ∈ F such that E[r(a)  ,2. Preliminaries,[0],[0]
"x, a] = f(x, a) ∀x ∈ X , a ∈ A.",2. Preliminaries,[0],[0]
"This assumption is used by essentially all regression-based contextual bandit algorithms (Chu et al., 2011; Filippi et al., 2010; Russo & Van Roy, 2013; Li et al., 2017).",2. Preliminaries,[0],[0]
"Given a predictor f ∈ F , the associated optimal strategy ⇡f ∶ X → A, called a policy, picks the action with the highest predicted reward, i.e., ⇡f(x) ∶= argmaxa∈A f(x, a) (ties broken arbitrarily).",2. Preliminaries,[0],[0]
"Using ⇡ ∶= ⇡f to denote an optimal policy, the learner aims to minimize its regret
RegT = ∑Tt=1",2. Preliminaries,[0],[0]
"rt(⇡(xt)) −∑Tt=1 rt(at), which compares the accumulated rewards between the optimal policy and the learner’s strategy.",2. Preliminaries,[0],[0]
"The classic Exp4
algorithm (Auer et al., 2002b) achieves an optimal regret bound of order O(TK ln F ) (for any finite F), but the computational complexity is unfortunately linear in F .",2. Preliminaries,[0],[0]
"Regression Oracle To overcome the computational obstacle, our algorithms reduce the contextual bandit problem to weighted least-squares regression.",2. Preliminaries,[1.0],"['Regression Oracle To overcome the computational obstacle, our algorithms reduce the contextual bandit problem to weighted least-squares regression.']"
"Abstracting the computational complexity, we assume access to a weighted least-squares regression oracle over the predictor class F , which takes any set H of weighted examples (w,x, a, y) ∈ R+ ×X ×A ×",2. Preliminaries,[0],[0]
"[0,1] as input, and outputs the predictor with the smallest weighted squared loss: ORACLE(H) = argminf∈F ∑(w,x,a,y)∈H w(f(x, a)− y)2.",2. Preliminaries,[0],[0]
"As mentioned, such regression tasks are very common in machine learning practice and the availability of such oracle is thus a very mild assumption.",2. Preliminaries,[0],[0]
The high-level idea of our algorithms is the following.,3. Algorithms,[0],[0]
"As data is collected, we maintain a subset of F , referred to as the version space, that only contains f ∈ F with small squared loss on observed data.",3. Algorithms,[1.0],"['As data is collected, we maintain a subset of F , referred to as the version space, that only contains f ∈ F with small squared loss on observed data.']"
"For a new example, we construct a confidence interval for the expected reward of each action based on this version space.",3. Algorithms,[1.0],"['For a new example, we construct a confidence interval for the expected reward of each action based on this version space.']"
"Finally, with these confidence intervals, we either optimistically pick the action with the highest upper bound, similar to UCB and LinUCB, or randomize among all actions that are potentially the best.
",3. Algorithms,[0],[0]
"The challenge here is to maintain such version spaces and compute upper and lower confidence bounds efficiently, and we show that this can be done using a binary search together with a small number of regression oracle calls.
",3. Algorithms,[0],[0]
"More formally, we define the upper and lower bounds on the expected reward with respect to a subset F ′ ⊆ F as HIGHF ′(x, a) =max
f∈F ′ f(x, a), LOWF ′(x, a) = minf∈F ′ f(x, a).",3. Algorithms,[0],[0]
"Our algorithms will induce the confidence bounds by instantiating these quantities using the version space as F ′. To reduce computation, our algorithms update on a doubling epoch schedule.",3. Algorithms,[0],[0]
There are M = O(logT ) epochs and each epoch m begins at time ⌧m = 2m−1.,3. Algorithms,[0],[0]
"At epoch m our algorithms (implicitly) construct a version space Fm ⊆ F , and then select an action based on the reward ranges defined by HIGHFm(x, a) and LOWFm(x, a) for each time t that falls into epoch m. Specifically, we consider two algorithm variants: the first one uniformly at random picks from actions that are plausible to be the best (see lines 6-7 in Algorithm 1); the second one simply behaves optimistically and picks the action with the highest upper bound (see line 9 in Algorithm 2).",3. Algorithms,[0],[0]
"For technical reasons, the optimistic variant also performs pure exploration in the first few epochs to warm-start the algorithm.
",3. Algorithms,[0],[0]
Algorithm 1 REGCB.ELIMINATION 1:,3. Algorithms,[0],[0]
"Input: square-loss tolerance m 2: for epoch m = 1, . . .",3. Algorithms,[0],[0]
",M do 3:",3. Algorithms,[0],[0]
"Fm ←
 ∏a∈A Ĝm( m, a) (OPTION I)F̂m( m) (OPTION II)
4: for time t = ⌧m, . . .",3. Algorithms,[0],[0]
", ⌧m+1 − 1 do 5: Receive xt and define At as: 6: {a ∶ HIGHFm(xt, a) ≥maxa′∈A LOWFm(xt, a′)}.",3. Algorithms,[0],[0]
7: Sample at ∼ Unif (At) and receive rt(at).,3. Algorithms,[0],[0]
"8: end for 9: end for
To construct these version spaces, we further introduce the following least-squares notation for any m ≥ 2:
• R̂m(f)",3. Algorithms,[0],[0]
"= 1⌧m−1 ∑s<⌧mf(xs, as) − rs(as)2, • F̂m( ) = f ∈ F  R̂m(f) −minf∈F R̂m(f) ≤ ,
and also let F̂ 1 ( ) = F for any .",3. Algorithms,[0],[0]
"With this notation Fm is simply set to F̂( m) for some m, and HIGHFm and LOWFm recover the confidence bounds in UCB (Auer et al., 2002a) and LinUCB (Chu et al., 2011) for appropriate m.
Product Classes",3. Algorithms,[0],[0]
"Sometimes it is desirable to have a product predictor class, that is, F = GA, where G ∶ X →",3. Algorithms,[0],[0]
"[0,1] is a “base class” and each f ∈ F , described by a K-tuple(ga)a∈A where ga ∈ G, predicts according to f(x, a) = ga(x).",3. Algorithms,[0],[0]
"Similar to the general case, we define: • R̂m(g, a) = 1⌧m−1 ∑s<⌧m(g(xs)",3. Algorithms,[0],[0]
"− rs(as))21{as = a}, • Ĝm( , a) = g ∈ G  R̂m(g, a) −ming∈G R̂m(g, a) ≤ , and let Ĝ
1 ( , a) = G for any .",3. Algorithms,[0],[0]
In this case we constructFm as∏a∈A,3. Algorithms,[0],[0]
"Ĝm( m, a) for some tolerance parameter m. Our two procedures are described in Algorithms 1 and 2.",3. Algorithms,[0],[0]
Algorithms 1 and 2 hinge on the computation of the bounds HIGHFm and LOWFm .,3.1. Efficient Reward-Range Computation,[0],[0]
"This can be carried out efficiently via a small number of calls to the regression oracle.
",3.1. Efficient Reward-Range Computation,[0],[0]
"Specifically, to calculate the confidence bounds for a given pair (x, a), we augment the data set Hm with a single example (x, a, r) with a weight w.",3.1. Efficient Reward-Range Computation,[0],[0]
For the upper bound HIGHFm we use r = 2; for the lower bound r = −1,3.1. Efficient Reward-Range Computation,[0],[0]
(these values are chosen as they lie outside the reward range).,3.1. Efficient Reward-Range Computation,[0],[0]
"By changing the weight w, we trade-off the loss on this single example against that on the history Hm.",3.1. Efficient Reward-Range Computation,[1.0],"['By changing the weight w, we trade-off the loss on this single example against that on the history Hm.']"
"The binary search over w identifies—up to a given precision—the weight w at which
Algorithm 2 REGCB.OPTIMISTIC 1:",3.1. Efficient Reward-Range Computation,[0],[0]
"Input: square-loss tolerance m
number of warm-start epochs M 0
2: for time t = 1, . . .",3.1. Efficient Reward-Range Computation,[0],[0]
", ⌧M0 − 1 do 3: Receive xt, play at ∼ Unif (A), and receive rt(at).",3.1. Efficient Reward-Range Computation,[0],[0]
"4: end for 5: for epoch m =M
0 , . . .",3.1. Efficient Reward-Range Computation,[0],[0]
",M do 6: Fm ← F̂m( m).",3.1. Efficient Reward-Range Computation,[0],[0]
"7: for time t = ⌧m, . . . ,",3.1. Efficient Reward-Range Computation,[0],[0]
"⌧m+1 − 1 do 8: Receive xt. 9: Select at = argmaxa∈A HIGHFm(xt, a).
10: Receive rt(at).",3.1. Efficient Reward-Range Computation,[0],[0]
"11: end for 12: end for
the empirical regret on Hm is exactly the desired tolerance , with the corresponding prediction on x, a yielding HIGHF̂m( )(x, a) or LOWF̂m( )(x, a) (see Algorithm 3).",3.1. Efficient Reward-Range Computation,[0],[0]
In Appendix A.1 we prove that this strategy works as intended and in O(log(1↵)) iterations computes the confidence bounds up to a precision of ↵.,3.1. Efficient Reward-Range Computation,[0],[0]
Theorem 1.,3.1. Efficient Reward-Range Computation,[0],[0]
"Let Hm = {(xs, as, rs(as))}⌧m−1s=1 .",3.1. Efficient Reward-Range Computation,[1.0],"['Let Hm = {(xs, as, rs(as))}⌧m−1s=1 .']"
"If the function class F is convex and closed under pointwise convergence, then the calls
zHIGH ← BINSEARCH(HIGH, (x, a),Hm, ,↵) zLOW",3.1. Efficient Reward-Range Computation,[0],[0]
"← BINSEARCH(LOW, (x, a),Hm, ,↵)
terminate after O(log(1↵))",3.1. Efficient Reward-Range Computation,[0.9574978993206367],"['If the function class F is convex and closed under pointwise convergence, then the calls zHIGH ← BINSEARCH(HIGH, (x, a),Hm, ,↵) zLOW ← BINSEARCH(LOW, (x, a),Hm, ,↵) terminate after O(log(1↵)) oracle invocations and HIGHF̂m( )(x, a) − zHIGH ≤ ↵, LOWF̂m( )(x, a) − zLOW ≤ ↵.']"
"oracle invocations and HIGHF̂m( )(x, a) − zHIGH ≤ ↵, LOWF̂m( )(x, a) − zLOW ≤ ↵.
",3.1. Efficient Reward-Range Computation,[0],[0]
"Compared to the procedure from Krishnamurthy et al. (2017), Algorithm 3 is much simpler and achieves an exponential improvement in terms of oracle calls, namely O(log(1↵))",3.1. Efficient Reward-Range Computation,[0],[0]
"as opposed to O(1↵), when F is convex.",3.1. Efficient Reward-Range Computation,[0],[0]
"Compared to oracles for cost-sensitive classification, convexity is not a strong assumption for regression oracles.",3.1. Efficient Reward-Range Computation,[0],[0]
"When F is not convex, reward bounds can be computed in O(1↵) oracle calls (see Krishnamurthy et al. 2017).",3.1. Efficient Reward-Range Computation,[0],[0]
In this section we provide regret guarantees for RegCB (Algorithm 1 and Algorithm 2).,4. Regret Guarantees,[0],[0]
"Note that RegCB is not minimax optimal: while it can obtain OKT logF  regret or even logarithmic regret under certain distributional assumptions, which we describe shortly, for some instances it can make as many as F  mistakes, which is suboptimal: Proposition 1.",4. Regret Guarantees,[0],[0]
"For every ✏ ∈ (0,1] and N ∈ N there exists a class of reward predictors satisfying Assumption 1 with
Algorithm 3 BINSEARCH 1: Input: bound type ∈ {LOW,HIGH}, target pair (x, a)
history H , radius > 0, precision ↵ > 0 2:",4. Regret Guarantees,[0],[0]
"Based on bound type: r←2 if HIGH and r←−1 if LOW 3: Let R(f) ∶= ∑(x′,a′,r′)∈H(f(x′, a′)",4. Regret Guarantees,[0],[0]
"− r′)2 4: Let R̃(f,w) ∶= R(f) + w
2 (f(x, a)",4. Regret Guarantees,[0],[0]
"− r)2 5: wL ← 0, wH",4. Regret Guarantees,[0],[0]
"← ↵
//",4. Regret Guarantees,[0],[0]
"Invoke oracle twice 6: fL ← argminf∈F R̃(f,wL), zL ← fL(x, a) 7: fH ← argminf∈F R̃(f,wH), zH",4. Regret Guarantees,[0],[0]
"← fH(x, a) 8:",4. Regret Guarantees,[0],[0]
"Rmin ← R(fL) 9: ← ↵ (r − zL)3
10: while zH",4. Regret Guarantees,[0],[0]
− zL >,4. Regret Guarantees,[0],[0]
↵ and wH,4. Regret Guarantees,[0],[0]
"−wL > do 11: w ← (wH +wL)2
//",4. Regret Guarantees,[0],[0]
Invoke oracle.,4. Regret Guarantees,[0],[0]
"12: f ← argmin ˜f∈F R̃( ˜f,w), z ← f(x, a) 13: if R(f) ≥",4. Regret Guarantees,[0],[0]
"Rmin + then 14: wH ← w, zH",4. Regret Guarantees,[0],[0]
"← z 15: else 16: wL ← w, zL",4. Regret Guarantees,[0],[0]
"← z 17: end if 18: end while 19: return zH.
F  = N + 1 and a distribution for which both Algorithms 1 and 2 have regret lower bounded by (1−✏) ⋅minN, ⌦̃(T ).",4. Regret Guarantees,[0],[0]
Proposition 1 is proved in Appendix A.2.,4. Regret Guarantees,[0],[0]
The proof builds on a well-known albeit rather pathological instance.,4. Regret Guarantees,[0],[0]
"In contrast, our strong empirical results in the following section show that such instances are not encountered in practice.",4. Regret Guarantees,[0],[0]
"In order to understand the typical behavior of such algorithms, prior works have considered structural assumptions such as finite eluder dimension (Russo & Van Roy, 2013) or disagreement coefficients (Hanneke, 2014; Krishnamurthy et al., 2017).",4. Regret Guarantees,[0],[0]
"In the next two subsections, we use similar ideas to analyze the regret incurred by our algorithm.",4. Regret Guarantees,[0],[0]
"We assume that HIGHFm and LOWFm are computed exactly, but extension to the approximate case is straightforward.",4. Regret Guarantees,[0],[0]
"Disagreement coefficients come from the active learning literature (Hanneke, 2014), and roughly assume that given a set of functions which fit the historical data well, the probability that these functions make differing predictions on a new example is small.",4.1. Disagreement-based Analysis,[0],[0]
"This rules out the bad case of Proposition 1, where a near-optimal predictor significantly disagrees from the others on each context.",4.1. Disagreement-based Analysis,[0],[0]
"Our development in this subsection largely follows Krishnamurthy et al. (2017), with appropriate modifications to translate from active learning to contextual bandits.",4.1. Disagreement-based Analysis,[0],[0]
"We begin with a formal definition of the disagreement coefficient.
",4.1. Disagreement-based Analysis,[0],[0]
Definition 1 (Disagreement Coefficient).,4.1. Disagreement-based Analysis,[0],[0]
"The disagreement coefficient for F (with respect to DX ) is defined as ✓ 0
∶= sup >0,"">0 "" PrDX x ∈",4.1. Disagreement-based Analysis,[0],[0]
"Dis(F("")) and
∃a ∈ AF("")(x) ∶WF("")(x, a) > .",4.1. Disagreement-based Analysis,[0],[0]
"Here F("") is the set of all predictors f whose greedy policies have regret at most "", Dis(F(""))",4.1. Disagreement-based Analysis,[0],[0]
"is the set of x’s where the greedy policies of at least two functions in F("") choose different actions, AF(x) = f∈Fargmaxa∈A",4.1. Disagreement-based Analysis,[0],[0]
"f(x, a), and WF(x, a) is the difference between the upper and lower bounds HIGHF(x, a) − LOWF(x, a).",4.1. Disagreement-based Analysis,[0],[0]
Formal definitions of these quantities can be found in Appendix A.3.,4.1. Disagreement-based Analysis,[0],[0]
"Informally, the disagreement coefficient is small if on most contexts either all f ∈ F("") choose the same action according to their greedy policies or all actions chosen by those policies have a low range of predicted rewards.
",4.1. Disagreement-based Analysis,[0],[0]
The following theorem provides regret bounds in terms of the disagreement coefficient.,4.1. Disagreement-based Analysis,[0],[0]
"In all theorems we use Õ to suppress polynomial terms in logT , logK, and log(1 ), where is the failure probability.",4.1. Disagreement-based Analysis,[0],[0]
"Moreover, all results can be improved to be logarithmic (in T ) under the standard Massart noise condition (see the appendix for the details).",4.1. Disagreement-based Analysis,[0],[0]
Theorem 2.,4.1. Disagreement-based Analysis,[0],[0]
"With m = (M−m+1)C ⌧m−1 and C = 16 log 2GKT 2 , Algorithm 1 with Option I incurs RegT = Õ T 34 (log G) 14√✓
0 K with probability at least 1 − .",4.1. Disagreement-based Analysis,[0],[0]
"See Theorem 5 in Appendix A.3 for the full version of this theorem, which applies to infinite classes and additionally obtains faster rates under the Massart noise condition.
",4.1. Disagreement-based Analysis,[0],[0]
"Discussion Theorem 2 critically uses the product class structure, specifically the fact that the set At computed by the algorithm coincides with the disagreement set AFm(xt) for t ∈ {⌧m, . . .",4.1. Disagreement-based Analysis,[0],[0]
", ⌧m+1 − 1}.",4.1. Disagreement-based Analysis,[0],[0]
"This is true for product classes, but not necessarily for general (non-product) predictor classes.",4.1. Disagreement-based Analysis,[0],[0]
"Computing the disagreement set efficiently for non-product classes is a challenge for future work.
",4.1. Disagreement-based Analysis,[0],[0]
"While bounding the disagreement coefficients a priori often requires strong assumptions on the model class and the distribution, the size of disagreement set can be easily checked empirically under the product class assumption, and we include this diagnostic in our experimental results.
",4.1. Disagreement-based Analysis,[0],[0]
"Finally, while the disagreement coefficient enables the analysis of Algorithm 1, it is not obvious how to use it to analyze Algorithm 2.",4.1. Disagreement-based Analysis,[0],[0]
"Our analysis crucially requires that any plausibly optimal action a be chosen with a reasonable probability, something which the optimistic algorithm fails to ensure.",4.1. Disagreement-based Analysis,[0],[0]
"The disagreement-based analysis of Theorem 2 is not entirely satisfying, because even for linear predictors (e.g.,
as in LinUCB, Chu et al. 2011), fairly strong assumptions on DX (e.g., log-concavity) are required to bound the disagreement coefficient ✓
0",4.2. Moment-based Analysis,[0],[0]
"(Hanneke, 2014).",4.2. Moment-based Analysis,[0],[0]
"To generalize the analysis to richer than linear classes without distributional assumptions on the contexts, prior work has used the notion of eluder dimension (Russo & Van Roy, 2013).",4.2. Moment-based Analysis,[0],[0]
"It remains challenging, however, to show examples with a small eluder dimension beyond linearly parameterized functions.",4.2. Moment-based Analysis,[0],[0]
"In addition, taking the worst-case over all histories, as in eluder dimension, is overly pessimistic for i.i.d. contextual bandits.
",4.2. Moment-based Analysis,[0],[0]
"To address the shortcomings of both the disagreement-based analysis as well as eluder dimension for i.i.d. settings, we define two new distributional properties which we will use to analyze the regret of both of our algorithms.",4.2. Moment-based Analysis,[0],[0]
Definition 2 (Surprise bound).,4.2. Moment-based Analysis,[0],[0]
"The surprise bound L
1 > 0 is the smallest constant such that for all f ∈ F , x ∈ X , and a ∈ A, the gap (f(x, a) − f(x, a))2 is at most
L 1 ⋅Ex′∼DX",4.2. Moment-based Analysis,[0],[0]
"Ea′∼Unif(A)f(x′, a′)",4.2. Moment-based Analysis,[0],[0]
"− f(x′, a′)2 .",4.2. Moment-based Analysis,[0],[0]
"The surprise bound is small if functions with a small expected squared error relative to f (under a uniform choice of actions) do not encounter a much larger squared error on any single context-action pair.
",4.2. Moment-based Analysis,[0],[0]
"The second quantity, which we call the implicit exploration coefficient (or IEC) relates the expected regression error under actions chosen by the optimal policy to the worst-case error on any other context-action pair.",4.2. Moment-based Analysis,[0],[0]
"For ∈ (0,1] define U (a) = {x  f(x, a) ≥ f(x, a′) + ∀a′ ≠ a}.",4.2. Moment-based Analysis,[0],[0]
Definition 3,4.2. Moment-based Analysis,[0],[0]
(Implicit exploration coefficient—IEC).,4.2. Moment-based Analysis,[0],[0]
"For any ∈ (0,1], the implicit exploration coefficient L
2, > 0 is the smallest constant such that for all f ∈ F , x ∈ X , and a ∈ A, the gap (f(x, a) − f(x, a))2 is at most L 2, Ex′∼DX",4.2. Moment-based Analysis,[0],[0]
"Ea′∼Unif(A)1x′ ∈ U (a′) (1) ⋅ f(x′, a′)",4.2. Moment-based Analysis,[0],[0]
"− f(x′, a′)2.",4.2. Moment-based Analysis,[0],[0]
"We now make two remarks about these definitions and their impact on the performance of Algorithms 1 and 2.
",4.2. Moment-based Analysis,[0],[0]
"• By definition, L 2, is non-decreasing in .",4.2. Moment-based Analysis,[0],[0]
"For Al-
gorithm 1 we can simply use = 0, for which L 2,0 is defined by replacing the right-hand side of (1) with L2,0K Ex∼DX",4.2. Moment-based Analysis,[0],[0]
"[(f(x,⇡(x))",4.2. Moment-based Analysis,[0],[0]
"− f(x,⇡(x)))2].",4.2. Moment-based Analysis,[0],[0]
"The analysis of Algorithm 2 requires > 0, and this must be used to tune the algorithm’s warm-start period.
",4.2. Moment-based Analysis,[0],[0]
"• We always have L 1 ≤ L 2,0, but L1 may be much
smaller.",4.2. Moment-based Analysis,[0],[0]
"L 1 appears in the regret bound of Algorithm 2, but not Algorithm 1.
",4.2. Moment-based Analysis,[0],[0]
"We now state the regret bound for Algorithm 1 with a general class F , and employ the shorthand C ′",4.2. Moment-based Analysis,[0],[0]
"= 16 log 2F T 2 .
",4.2. Moment-based Analysis,[0],[0]
Theorem 3.,4.2. Moment-based Analysis,[0],[0]
"With m = (M−m+1)C′ ⌧m−1 , Algorithm 1 with Option II incurs RegT = Õ",4.2. Moment-based Analysis,[0],[0]
"TL2,0 log F  with probability at least 1 − .",4.2. Moment-based Analysis,[0],[0]
We now move on to describe the performance guarantee for Algorithm 2.,4.2. Moment-based Analysis,[0],[0]
"Because this optimistic strategy does not explore as readily as the elimination-based strategy of Algorithm 1, the analysis requires both that (i) the IEC L
2, be invoked for some > 0 and (ii) that the algorithm use a warm-start period whose size grows as 1 2.",4.2. Moment-based Analysis,[0],[0]
Theorem 4.,4.2. Moment-based Analysis,[0],[0]
"With m = (M−m+1)C′ ⌧m−1 and M0 = 2 + log
2
 1 + (2M+3)L1C′ 2  for any ∈ (0,1), Algorithm 2
incurs RegT = Õ L1 logF  2 +TL2, log F  with probability at least 1 − .",4.2. Moment-based Analysis,[0],[0]
"As Algorithm 2 requires a warm start, the regret bounds of Theorem 4 are always worse than those of Theorem 3.",4.2. Moment-based Analysis,[0],[0]
"Appendix A.4 contains full versions of these theorems, Theorem 6 and Theorem 7, which obtain faster rates under the Massart noise condition and apply to infinite classes.
",4.2. Moment-based Analysis,[0],[0]
"Linear classes For concreteness, let us discuss the regret of both algorithms in a linear setting with a fixed feature map ∶ X ×A → Rd and F = {(x, a) w",4.2. Moment-based Analysis,[0.9594629569554358],"['Linear classes For concreteness, let us discuss the regret of both algorithms in a linear setting with a fixed feature map ∶ X ×A → Rd and F = {(x, a) w (x, a) w ∈W} for some W ⊆ Rd (e.g., as in LinUCB).']"
"(x, a) w ∈W} for some W ⊆ Rd (e.g., as in LinUCB).",4.2. Moment-based Analysis,[0],[0]
"In the basic ` 2 -bounded case, L 1 and L 2, can be bounded in terms of the minimum eigenvalues of Ex[ (x, a) (x, a)] and Ex1{x ∈ U (a)} (x, a) (x, a), respectively.",4.2. Moment-based Analysis,[0],[0]
When predictors are s-sparse we can instead obtain bounds in terms of (A) ∶= minw≠0∶ w0≤2swAw,4.2. Moment-based Analysis,[0],[0]
"ww, the minimum restricted eigenvalue for 2s-sparse predictors (Raskutti et al., 2010).",4.2. Moment-based Analysis,[0],[0]
"For Algorithm 1 this yields a near dimensionindependent bound on RegT of Õ sKT log d  Ex (x,⇡(x))",4.2. Moment-based Analysis,[0],[0]
"(x,⇡(x))",4.2. Moment-based Analysis,[0],[0]
.1,4.2. Moment-based Analysis,[0],[0]
"This improves upon the moment matrix conditions of Bastani & Bayati (2015), although our algorithm requires nonconvex optimization oracles.2 Note that without the scaling with K as in our result, a √ d dependence is unavoidable (Abbasi-Yadkori et al., 2012).",4.2. Moment-based Analysis,[0],[0]
"The result highlights the strengths of our analysis in the best case compared with eluder dimension, which does not adapt to sparsity structures.",4.2. Moment-based Analysis,[0],[0]
"On the other hand, for the standard LinUCB setting, our result is inferior by at least a factor of K.
Discussion Our analysis is influenced by the results of Bastani & Bayati (2015) for the (high-dimensional) linear setting, but extends to general classes F , and when applied to Algorithm 1 with linear classes, the assumed bound on
1See Proposition 3, Lemma 9, and Theorem 3 in the appendix.",4.2. Moment-based Analysis,[0],[0]
"2Also, since the class F is non-convex, this requires the slower
binary search algorithm of Krishnamurthy et al. (2017).
",4.2. Moment-based Analysis,[0],[0]
"L 2, is weaker than their “diversity condition”.",4.2. Moment-based Analysis,[0],[0]
"Similar assumptions have been used to analyze purely greedy linear contextual bandits (Bastani et al., 2017; Kannan et al., 2018); our assumptions are strictly weaker.",4.2. Moment-based Analysis,[0],[0]
We compared our new algorithms with existing oracle-based alternatives.,5. Experiments,[0],[0]
"In addition to showing that RegCB3 has strong empirical performance, our experiments provide a more extensive empirical study of oracle-based contextual bandit algorithms than any past works (e.g., Agarwal et al., 2014, Krishnamurthy et al., 2016).",5. Experiments,[0],[0]
"Description of the datasets, benchmark algorithms, and oracle configurations, as well as further experimental results are included in Appendix B.
Datasets We begin with 10 datasets with full reward information and simulate bandit feedback by withholding the rewards for actions not selected by the algorithm.",5. Experiments,[0],[0]
"We use two large-scale learning-to-rank datasets, Microsoft MSLRWEB30k (mslr) (Qin & Liu, 2010) and Yahoo!",5. Experiments,[0],[0]
"Learning to Rank Challenge V2.0 (yahoo) (Chapelle & Chang, 2011), which were previously used to evaluate contextual semibandits (Krishnamurthy et al., 2016).",5. Experiments,[0],[0]
"We also use eight classification datasets from the UCI repository (Lichman, 2013), summarized in Table 1 of Appendix B.1.
",5. Experiments,[0],[0]
"The ranking datasets have natural rewards (relevances), but the rewards for the classification datasets always have multiclass structure (1 for the correct action and 0 for all others).",5. Experiments,[0],[0]
"To ensure that we evaluate the full generality of the CB setting, we create eight “noisy” UCI datasets by sampling new rewards for the datasets according to a noisy reward matrix model described in Appendix B. This yields additional 8 datasets for a total of 18.",5. Experiments,[0],[0]
"On each dataset we consider several replicates obtained by randomly permuting examples and, on noisy UCI, also randomly generating rewards.",5. Experiments,[0],[0]
"All the methods are evaluated on the same set of replicates.
",5. Experiments,[0],[0]
"Algorithms We evaluate Algorithms 1 and 2 against three baselines, all based on various optimization-oracle assumptions.",5. Experiments,[0],[0]
"First two are agnostic baselines, ✏-Greedy (Langford & Zhang, 2008) and the minimax-optimal ILOVETOCONBANDITS (ILTCB) strategy of Agarwal et al. (2014).4
✏-Greedy and ILTCB both assume cost-sensitive classification oracles and come with theoretical guarantees.",5. Experiments,[0],[0]
"The third baseline is a bootstrapping-based exploration strategy of Dimakopoulou et al. (2017) (Bootstrap-TS), which uses bootstrapping to estimate confidence intervals and then performs Thompson sampling to select an action based on the intervals.",5. Experiments,[0],[0]
"This algorithm represents a computationally
3RegCB refers collectively to both Algorithms 1 and 2.",5. Experiments,[0],[0]
"4We use an implementation available at https://github.
com/akshaykr/oracle_cb, which was also used by Krishnamurthy et al. (2016).
tractable alternative to Thompson sampling as it works in the regression-oracle model we consider here, but it does not have a theoretical analysis.5
Note that the LinUCB algorithm (Chu et al., 2011; AbbasiYadkori et al., 2011), which is a natural baseline as well, coincides with our Algorithm 2 (with a linear oracle), so we only plot the performance of RegCB with a linear oracle.
",5. Experiments,[0],[0]
"All of the algorithms update on an epoch schedule with epoch lengths of 2i2, which is a theoretically rigorous choice for each algorithm.",5. Experiments,[0],[0]
"Oracles We consider two baseline predictor classes F : ` 2
- regularized linear functions (Linear) and gradient-boosted depth-5 regression trees (GB5) (Friedman, 2001).",5. Experiments,[0],[0]
"For the regularized linear class, Algorithm 2 is equivalent to LinUCB on an epoch schedule.6 See Appendix B.3 for details.
",5. Experiments,[0],[0]
"When running both RegCB variants with the GB5 oracle, we use a simple heuristic to substantially speed up the computation.",5. Experiments,[0],[0]
"At the beginning of each epoch m, we find the best regression-tree ensemble on the dataset so far (i.e., with respect to R̂m).",5. Experiments,[0],[0]
"Throughout the epoch, we keep the structure of the ensemble fixed and in each call to ORACLE(H) we only re-optimize the predictions in leaves.",5. Experiments,[0],[0]
"This can be solved in closed form, similar to LinUCB, so the full binary search procedure (Algorithm 3) does not need to be run.
",5. Experiments,[0],[0]
Parameter Tuning We evaluate each algorithm for eight exponentially spaced parameter values across five replicates.,5. Experiments,[0],[0]
"For ✏-Greedy we tune the constant ✏, and for ILTCB we tune a certain smoothing parameter (see Appendix B).",5. Experiments,[0],[0]
For Algorithms 1 and 2 we set m = for all m and tune .,5. Experiments,[0],[0]
For Algorithm 2 we use a warm start of 0.,5. Experiments,[0],[0]
"We tune a confidence parameter similar to for Bootstrap-TS.
",5. Experiments,[0],[0]
"Evaluation Each dataset is split into “training data”, for which algorithm receives one example at a time and must predict online, and a holdout validation set.",5. Experiments,[0],[0]
Validation is performed by simulating the algorithm’s predictions on examples from the holdout set without allowing the algorithm to incorporate these examples.,5. Experiments,[0],[0]
"We also plot the validation reward of a “supervised” baseline obtained by training the oracle (either Linear or GB5) on the entire training set at once (including rewards for all actions).
",5. Experiments,[0],[0]
For Algorithms 1 and 2 we show average reward at various numbers of training examples for the best fixed parameter value in each dataset.,5. Experiments,[0],[0]
"For the baselines, we take the pointwise maximum of the average reward across all parameter values for each number of examples.",5. Experiments,[0],[0]
"Thus,
5It is not known how to implement the standard formulation of Thompson sampling for contextual bandits (e.g., Russo & Van Roy 2013) with optimization oracles.
",5. Experiments,[0],[0]
"6More precisely, it is equivalent to the well-known OFUL variant of LinUCB (Abbasi-Yadkori et al., 2011).
",5. Experiments,[0],[0]
"the curves for our methods correspond to an actual run of the algorithm, while the baselines are an upper envelope aggregating multiple parameter values.
",5. Experiments,[0],[0]
"Results: Performance Figure 1 shows average reward of each algorithm on a holdout validation set for three representative datasets, letter from UCI, letter-noise (the variant with simulated rewards), and yahoo.
",5. Experiments,[0],[0]
"RegCB (both Algorithms 1 and 2) outperforms all baselines on the unmodified UCI datasets (e.g., letter in Figure 1).",5. Experiments,[0],[0]
"On the noisy variants (e.g., letter+N in Figure 1), the performance of the ILTCB and Bootstrap-TS benchmarks improves significantly, with Bootstrap-TS slightly edging out the rest of the algorithms.",5. Experiments,[0],[0]
"On the yahoo ranking dataset (Figure 1, right), the ordering of the algorithms in performance is similar to noisy UCI datasets.
",5. Experiments,[0],[0]
"Validation performance plots for all datasets are in Appendix B. Overall, RegCB methods and Bootstrap-TS generally dominate the field.",5. Experiments,[0],[0]
"While Bootstrap-TS can outperform RegCB methods when using GB5 models, the gap is typically quite small.",5. Experiments,[0],[0]
"For linear models, RegCB methods generally outperform Bootstrap-TS, hinting that the approximate binary search might be hurting RegCB with GB5 models.",5. Experiments,[0],[0]
"We also observe that when RegCB methods outperform Bootstrap-TS, the gap is often quite large.",5. Experiments,[0],[0]
"We will see further evidence of this behavior in the next set of results.
",5. Experiments,[0],[0]
"Results: Aggregate Performance To rigorously draw conclusions about overall performance, Figure 2 aggregates performance across all datasets.",5. Experiments,[0],[0]
"We compute “normalized relative loss” for each algorithm by rescaling the validation reward (computed as in Figure 1) so that, at each round, the best performing algorithm has loss 0 and the worst has loss 1.",5. Experiments,[0],[0]
"In each plot of Figure 2 we consider normalized relative losses at a specific cutoff time (1000 examples in the left plot, and all examples in the center and right), and for each method we plot the number of datasets where it achieves loss below a threshold, as a function of the threshold.",5. Experiments,[0],[0]
"Thus, curves towards top left corner correspond to methods that achieve lower relative loss on more datasets.",5. Experiments,[0],[0]
"The intercept at loss 0 is the number of datasets where an algorithm is the best, and the intercept at 0.99 is the number of datasets where the it is not the worst (so the distance from top is the number of datasets where it is the worst).",5. Experiments,[0],[0]
"Solid lines are runs with GB5 and dashed lines are with the Linear oracle.
",5. Experiments,[0],[0]
"The aggregate performance with the GB5 oracle across all datasets can be briefly summarized as follows: RegCB always beats ✏-Greedy and ILTCB, but sometimes loses out to Bootstrap-TS, and Bootstrap-TS itself sometimes underperforms relative to the other baselines, especially on the UCI datasets.",5. Experiments,[0],[0]
"Even when RegCB is not the best, it is almost always within 20% of the best.",5. Experiments,[1.0],"['Even when RegCB is not the best, it is almost always within 20% of the best.']"
"The elimination and optimistic variants of RegCB have comparable performance,
with elimination performing slightly better in aggregate.
",5. Experiments,[0],[0]
"The RegCB algorithms with the GB5 oracle also dominate the ✏-Greedy, ILTCB, and Bootstrap-TS baselines when they are equipped with Linear oracles (the dashed lines in Figure 2).",5. Experiments,[1.0],"['The RegCB algorithms with the GB5 oracle also dominate the ✏-Greedy, ILTCB, and Bootstrap-TS baselines when they are equipped with Linear oracles (the dashed lines in Figure 2).']"
"When the RegCB algorithms use the Linear oracle they also dominate the baselines with the Linear oracle across all datasets, including Bootstrap-TS.7",5. Experiments,[0],[0]
"This suggests that the gap between RegCB and Bootstrap-TS for GB5 may be due to the approximation of fixing the ensemble structure in each epoch, as noted earlier.
",5. Experiments,[0],[0]
Results: Confidence Width,5. Experiments,[0],[0]
The analysis of RegCB relies on assumptions on D (disagreement coefficient or moment parameters) that are not easy to verify.,5. Experiments,[0.9853080790734373],['Results: Confidence Width The analysis of RegCB relies on assumptions on D (disagreement coefficient or moment parameters) that are not easy to verify.']
"The main role of these parameters is to control the rate at which confidence width WFm(xt, a) = HIGHFm(xt, a) − LOWFm(xt, a) used in RegCB shrinks, since small widths imply that the algorithm makes good decisions and thus has low regret.
",5. Experiments,[0],[0]
"To investigate whether the width indeed shrinks empirically, we compute WFm(xt, a) on each dataset for Algorithm 2 and Bootstrap-TS, where a is the “optimistic” action with highest upper confidence bound under each algorithm.",5. Experiments,[0],[0]
"Finally for both Algorithm 2 and Bootstrap-TS we compute the size of the “disagreement set” At, defined in Algorithm 1, which measures how many actions the algorithm thinks are plausibly best.8
Figure 3 shows width and disagreement for a representative sample of datasets under the GB5 oracle; the remaining datasets are in Appendix B. The figure suggests that our distributional assumptions are reasonable for real-world datasets.",5. Experiments,[0],[0]
"In particular, for our algorithm, the width decays roughly as T −13 for letter and T −12 for letter+N and yahoo.",5. Experiments,[0],[0]
"Interestingly, the best hyper-parameter setting for Bootstrap-TS on letter yields low but essentially constant (i.e., not shrinking) width, and obtains a poor validation reward in Figure 1 (left).",5. Experiments,[0],[0]
"This suggests that while the Bootstrap-TS confidence intervals are small, they may not be faithful in the sense of containing f(x, a).",5. Experiments,[0],[0]
This work serves as a starting point for what we hope will be a fruitful line of research on oracle-efficient contextual bandit algorithms in realizability-based settings.,6. Conclusion and Discussion,[0],[0]
"We have shown that the RegCB family of algorithms have strong empirical performance and enjoy nice theoretical properties.
7The aggregate plots for RegCB with the Linear oracle can be found in Appendix B along with additional aggregate plots.
8This set is well-defined for both RegCB-Opt and Bootstrap-TS even through neither algorithm instantiates it explicitly.",6. Conclusion and Discussion,[0],[0]
"For the yahoo and mslr datasets this At is technically a lower bound on the true disagreement set size AFm(xt) because our classesF do not have product structure on these datasets—see Section 4.1.
",6. Conclusion and Discussion,[0],[0]
"These results suggest several compelling future directions.
",6. Conclusion and Discussion,[0],[0]
"First, is there a regression oracle–based algorithm that achieves the optimal Õ(KT log F ) regret?",6. Conclusion and Discussion,[0],[0]
"For example, is it possible to oraclize regressor elimination of Agarwal et al. (2012)?
Second, given the competitive empirical performance of
Bootstrap-TS, are there reasonable assumptions as in Section 4 under which it can be analyzed?",6. Conclusion and Discussion,[0],[0]
"There is recent work in this direction for linear models (Lu & Van Roy, 2017).
",6. Conclusion and Discussion,[0],[0]
"Finally, randomizing uniformly or putting all the mass on the optimistic choice are two extreme cases of choosing amongst the plausibly optimal actions.",6. Conclusion and Discussion,[0],[0]
Are there better randomization schemes that lead to stronger regret guarantees?,6. Conclusion and Discussion,[0],[0]
We thank Akshay Krishnamurthy and Alberto Bietti for helpful discussions.,Acknowledgements,[0],[0]
A major challenge in contextual bandits is to design general-purpose algorithms that are both practically useful and theoretically well-founded.,abstractText,[0],[0]
We present a new technique that has the empirical and computational advantages of realizabilitybased approaches combined with the flexibility of agnostic methods.,abstractText,[0],[0]
"Our algorithms leverage the availability of a regression oracle for the valuefunction class, a more realistic and reasonable oracle than the classification oracles over policies typically assumed by agnostic methods.",abstractText,[0],[0]
Our approach generalizes both UCB and LinUCB to far more expressive possible model classes and achieves low regret under certain distributional assumptions.,abstractText,[0],[0]
"In an extensive empirical evaluation, we find that our approach typically matches or outperforms both realizability-based and agnostic baselines.",abstractText,[0],[0]
Practical Contextual Bandits with Regression Oracles,title,[0],[0]
