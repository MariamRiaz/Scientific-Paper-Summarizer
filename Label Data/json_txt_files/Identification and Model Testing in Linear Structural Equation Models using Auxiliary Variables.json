{"sections": [{"heading": "1. Introduction", "text": "The problem of estimating causal effects is one of the fundamental problems in the data-driven sciences. In order to estimate a causal effect, the desired effect must be identified or uniquely expressible in terms of the probability distribution over the available data. Causal effects are identified by design in randomized control trials, but in many applications, such experiments are not possible. When only observational data is available, determining whether a causal\n1IBM Research, San Jose, California, USA 2Purdue University, West Lafeyette, Indiana, USA. Correspondence to: Bryant Chen <bryant.chen@ibm.com>, Daniel Kumor <dkumor@purdue.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\neffect is identified requires modeling the underlying causal structure, which is generally done using structural equation models (SEMs) (also called structural causal models) (Pearl, 2009; Bareinboim and Pearl, 2016).\nA structural equation model consists of a set of equations that describe the underlying data-generating process for a set of variables. While SEMs, in their most general, nonparametric form do not require any assumptions about the form of these functions, in many fields, including machine learning, psychology, and the social sciences, linear SEMs are used. A linear SEM consists of a set of equations of the form, X = \u039bX + U , where X = [x1, ..., xn]t is a vector containing the model variables, \u039b is a matrix containing the coefficients of the model, and \u039bij represents the direct effect of xi on xj , and U = [u1, ..., un]t is a vector of normally distributed error terms, which represents omitted or latent variables.1 The matrix \u039b contains zeroes on the diagonal, and \u039bij = 0 whenever xi is not a cause of xj . The covariance matrix of X will be denoted by \u03a3 and the covariance matrix over the error terms, U, by \u2126. In this paper, we will restrict our attention to semi-Markovian models (Pearl, 2009), models where the rows of \u039b can be arranged so that it is lower triangular, and the corresponding graph is acyclic.\nWhen modeling using SEMs, researchers typically specify the model by setting certain entries of \u039b and \u2126 to zero (i.e. exclusion and independence restrictions), while leaving the rest of the entries as free parameters to be estimated from data2. Restricting a particular entry \u039bij to zero reflects the assumption that Yi has no direct effect on Yj . Similarly, restricting \u2126ij to zero reflects the assumption that there are no unobserved common causes of both Yi and Yj . Once the\n1Instrumental and auxiliary variables can also be used when normality is not assumed, but to simplify the proofs in the paper, we will, as is commonly done by empirical researchers, assume normality.\n2There are a number of algorithms for discovering the model structure from data(Spirtes et al., 2000; Shimizu et al., 2006; Pearl, 2009; Zhang and Hyva\u0308rinen, 2009; Mooij et al., 2016). However, it is only in very rare instances that these methods are able to uniquely determine the model structure. As a result, model specification generally utilizes knowledge about the domain under study.\nparameters are estimated, causal effects (as well as counterfactual quantities) can be computed from the structural coefficients directly (Pearl, 2009; Chen and Pearl, 2014). However, in order to be estimable from data, a parameter must first be identified. In some cases, the modeling assumptions are not strong enough, and there are multiple, often infinite, values for the parameter that are consistent with the observed data. As a result, two fundamental problems in SEMs are to identify and estimate the model parameters and to test the underlying assumptions that enable identification.\nThe problem of identification has been studied extensively by econometricians and social scientists (Fisher, 1966; Bowden and Turkington, 1984; Bekker et al., 1994; Rigdon, 1995) and more recently by the AI and statistics communities using graphical methods (Spirtes et al., 1998; Tian, 2007; 2009; Brito and Pearl, 2002a;c; 2006; Bareinboim and Pearl, 2016). To our knowledge, the most general, efficient algorithm for model identification is the g-HT algorithm given by Chen (2016) combined with ancestor decomposition (Drton and Weihs, 2016). This method generalizes the half-trek algorithm of Foygel et al. (2012) and utilizes ancestor decomposition, which expands on an idea by Tian (2005) where the model is decomposed into simpler sub-models. Graphical methods have also been applied to the problem of testing the causal assumptions embedded in an SEM. For example, d-separation (Pearl, 2009) and overidentification (Pearl, 2004; Chen et al., 2014) provide the means to discover testable implications of the model, which can be used to test it against data.\nDespite decades of attention and work from diverse fields, the identification problem3 has still not been efficiently solved4. There are identifiable parameters and models that none of the above methods are able to identify. Similarly, there are testable implications of SEMs that the above methods are unable to detect. One promising avenue to aid in both tasks are auxiliary variables (Chen et al., 2016). Each of the aforementioned methods for identification and model testing only utilizes restrictions on the entries of \u039b and \u2126 to zero. Auxiliary variables can be used to incorporate knowledge of non-zero coefficient values into existing methods for identification and model testing. These coefficient values could be obtained, for example, from a previously conducted randomized experiment, from substantive understanding of the domain, or even from another identification technique. The intuition behind auxiliary vari-\n3To be precise, we are referring to the problem of identification almost everywhere (Brito and Pearl, 2002b), also called generic identification (Foygel et al., 2012).\n4An exhaustive procedure can be obtained using Gro\u0308bner bases methods (Foygel et al., 2012). However, these methods are computationally intractable for anything but the smallest of graphs.\nables is simple: if the coefficient from variable w to z, \u03b2, is known, then we would like to remove the direct effect of w on z by subtracting it from z. This removal eliminates confounding paths through w and is performed by creating a variable z\u2217 = z \u2212 \u03b2w, which is used as a proxy for z. In many cases, z\u2217 allows the identification of parameters or testable implications using existing methods when z could not.\nChen et al. (2016) demonstrated how auxiliary variables could be utilized in simple instrumental sets (instrumental sets that do not utilize conditioning to block spurious paths) (Brito and Pearl, 2002a; van der Zander et al., 2015) and proved that any model identifiable using the g-HT algorithm is also identifiable using auxiliary simple instrumental sets.\nSince auxiliary variables allow knowledge of non-zero coefficient values to be incorporated into existing methods for identification, they are also directly applicable to the problem of z-identification (Bareinboim and Pearl, 2012), in which partial experimental data is available. Additionally, the cancellation of paths that results from adding an AV may result in conditional independence constraints between the AV and other variables that can be used to test the model.\nIn this paper, we generalize the results of Chen et al. (2016) and demonstrate how auxiliary variables can be utilized in generalized instrumental sets, which allow for conditioning to block spurious paths. We prove that, unlike auxiliary simple instrumental sets, this generalization strictly subsumes the g-HT algorithm. Additionally, we introduce quasi-instrumental sets, which utilize auxiliary variables to identify coefficients when partial experimental data is available. Quasi-instrumental sets are incorporated into our identification algorithm, allowing it to better address the problem of z-identification. To our knowledge, this algorithm is the first systematic method for tackling zidentification in linear systems. We also demonstrate how auxiliary instrumental sets and quasi-instrumental sets can be used to derive over-identifying constraints, which can be used to test the model specification against data. Moreover, we prove that these overidentifying constraints subsume conditional independence constraints among auxiliary variables. Lastly, we discuss related work, showing how auxiliary IVs are able to unite a variety of disparate methods under a single framework."}, {"heading": "2. Preliminaries", "text": "The causal graph or path diagram of an SEM is a graph, G = (V,D,B), where V are nodes or vertices, D directed edges, and B bidirected edges. The nodes represent model variables. Directed eges encode the direction of causal-\nity, and for each coefficient \u039bij 6= 0, an edge is drawn from xi to xj . Each directed edge, therefore, is associated with a coefficient in the SEM, which we will often refer to as its structural coefficient. Additionally, when it is clear from context, we may abuse notation slightly and use coefficients and directed edges interchangeably. The error terms, ui, are not shown explicitly in the graph. However, a bidirected edge between two nodes indicates that their corresponding error terms may be statistically dependent while the lack of a bidirected edge indicates that the error terms are independent.\nWe will use standard graph terminology with Pa(y) denoting the parents of y, Anc(y) denoting the ancestors of Y , De(y) denoting the descendants of y, and Sib(y) denoting the siblings of y, the variables that are connected to y via a bidirected edge. He(E) denotes the heads of a set of directed edges, E, while Ta(E) denotes the tails. Additionally, for a node v, the set of edges for which He(E) = v is denoted Inc(v). Lastly, we will utilize d-separation (Pearl, 2009).\nWe will use \u03c3(x, y|W ) to denote the partial covariance between two random variables, x and y, given a set of variables, W , and \u03c3(x, y|W )G as the partial covariance between random variables x and y given W implied by the graph G. We will assume without loss of generality that the model variables have been standardized to mean 0 and variance 1.\nDefinition 1. For a given unblocked (given the empty set) path, \u03c0, from x to y, Left(\u03c0) is the set of nodes, if any, that has a directed edge leaving it in the direction of x in addition to x. Right(\u03c0) is the set of nodes, if any, that has a directed edge leaving it in the direction of y in addition to y.\nFor example, consider the path \u03c0 = x \u2190 vL1 \u2190 ... \u2190 vLk \u2190 vT \u2192 vRj \u2192 ... \u2192 vR1 \u2192 y. In this case, Left(\u03c0) = \u222aki=1vLi \u222a {x, vT } and Right(\u03c0) = \u222a j i=1v R i \u222a {y, vT }. vT is a member of both Right(\u03c0) and Left(\u03c0).\nDefinition 2. A set of paths, \u03c01, ..., \u03c0n, has no sided intersection if for all \u03c0i, \u03c0j \u2208 {\u03c01, ..., \u03c0n} such that \u03c0i 6= \u03c0j , Left(\u03c0i)\u2229Left(\u03c0j)=Right(\u03c0i)\u2229Right(\u03c0j) = \u2205.\nWright\u2019s rules (Wright, 1921) allow us to equate the modelimplied covariance, \u03c3(x, y)M , between any pair of variables, x and y, to the sum of products of parameters along unblocked paths between x and y.5 Let \u03a0 =\n5Wright\u2019s rules characterize the relationship between the covariance matrix and model parameters. Therefore, any question about identification using the covariance matrix can be decided by studying the solutions for this system of equations. However, since these equations are polynomials and not linear, it can be very difficult to analyze identification of models using Wright\u2019s rules.\n{\u03c01, \u03c02, ..., \u03c0k} denote the unblocked paths between x and y, and let pi be the product of structural coefficients along path \u03c0i. Then the covariance between variables x and y is\u2211 i pi.\nLastly, we define auxiliary variables and the augmented graph. Definition 3 (Auxiliary Variable). Given a linear SEM with graph G and a set of edges E whose coefficient values are known, an auxiliary variable is a variable, z\u2217 = z \u2212\u2211 i eiti, where {e1, ..., ek} \u2286 E\u2229 Inc(z) and ti = Ta(ei) for all i \u2208 {1, ..., k}.\nIf not otherwise specified, z\u2217 refers to the auxiliary variable, z \u2212 c1t1 \u2212 ... \u2212 cltl, where {c1, ..., cl} are the coefficients of E \u2229 Inc(z) and E is the set of directed edges whose coefficient values are known. In other words, z\u2217 is the auxiliary variable for z where as many known coefficients are subtracted out as possible. Chen et al. (2016) demonstrated that the covariance between any auxiliary variables and model variables can be computed using Wright\u2019s rules on the augmented graph, defined below. Definition 4. (Chen et al., 2016) Let M be a linear SEM with graph G and a set of directed edges E such that their coefficient values are known. The E-augmented model, ME+, includes all variables and structural equations of M in addition to new auxiliary variables, y\u22171 , ...y \u2217 k, one for each variable in He(E) = {y1, ..., yk} such that the structural equation for y\u2217i is y \u2217 i = yi \u2212 \u039bXiyiT ti , where Xi = Ta(E) \u2229 Pa(yi), for all i \u2208 {1, ..., k}. The corresponding augmented graph is denoted GE+.\nFor example, consider Figure 1a. If the value of \u03b2 is known, we can generate an auxiliary variable x\u2217 = x\u2212\u03b2t. The \u03b2-augmented graph G\u03b2+ is depicted in Figure 1b. In some cases, x\u2217 allows the identification of coefficients and testable implications using existing methods when x could not, due to the fact that the back-door paths from x to y that go through \u03b2 cancel with the back-door paths from x\u2217 to y that go through \u2212\u03b2. This can be seen by expressing the covariance of x\u2217 and y in terms of the model parameters using Wright\u2019s rules."}, {"heading": "3. Auxiliary and Quasi-Instrumental Sets", "text": "Two, perhaps the most common, methods for estimating causal effects are OLS regression and two-stage leastsquares (2SLS) regression. Both of these methods assume that the underlying causal relationships between variables are linear, in addition to other causal assumptions that guarantee identification. The single-door criterion (Pearl, 2009) graphically characterizes when the assumptions sufficient to estimate a causal effect using regression are satisfied in a linear SEM. Similarly, Brito and Pearl (2002a) gave a graphical characterization for when a variable z\nqualifies as an IV so that 2SLS regression provides a consistent estimate of the causal effect. In this section, we give a graphical criterion for when AVs can be utilized in generalized instrumental sets, which extends both the singledoor criterion and IVs. Additionally, we introduce quasiinstrumental sets, which utilize AVs to better address the problem of z-identification.\nFirst, we give a simple graphical criterion for when an AV would be conditionally independent of another variable, which will allow us to incorporate AVs into instrumental sets, as well as other identification and model testing methods that require the ability to detect conditional independence in the graph.\nTheorem 1. Given a linear SEM with graphG, whereE \u2286 Inc(z) is a set of edges whose coefficient values are known, if W \u222a {y} does not contain descendants of z and GE\u2212 represents the graph G with the edges for E removed, then (z\u2217 |= y|W )GE+ if and only if (z |= y|W )GE\u2212 .6\nProof. Proofs for all theorems and lemmas can be found in the Appendix (Chen et al., 2017).\nNext, we demonstrate how AVs can be incorporated into generalized instrumental sets, defined below.\n6The theorem disallows descendants of the generating variable in the conditioning set. At first glance, this may appear to limit the ability to block biasing paths among AVs. However, we conjecture that if z cannot be separated from y in G, then z\u2217 will almost surely not be independent of y given W , if W contains descendants of z. To illustrate, consider the example shown in Figure 1c. x\u2217 = x \u2212 \u03b2t is independent of y, as can be verified using Wright\u2019s rules, but x\u2217 is not independent of y given d! An intuitive explanation for this surprising result is that conditioning on d, a descendant of x, in Figure 1c induces correlation between the error term of x and t, since x acts as a \u201cvirtual collider\u201d. As a result, we have a \u201cvirtual path\u201d from x\u2217 to y, x\u2217 \u2190 x\u2190 ux \u2194 t\u2192 y. See Pearl (2009, p. 339) for a detailed discussion of virtual colliders.\nTheorem 2. (Brito and Pearl, 2002a) Given a linear model with graph G, the coefficients for a set of edges E = {(x1, y), ..., (xk, y)} are identified if there exists triplets (z1,W1, p1), ..., (zk,Wk, pk) such that for i = 1, ..., k,\n(i) (zi |= y|Wi)GE\u2212 , where W does not contain any descendants of y and GE\u2212 is the graph obtained by deleting the edges, E from G,\n(ii) pi is a path between zi and xi that is not blocked by Wi, and\n(iii) the set of paths, {p1, ..., pk} has no sided intersection.7\nIf the above conditions are satisfied, we say that Z is a generalized instrumental set for E or simply an instrumental set for E.8\nIn some cases, a variable z may not satisfy condition (i) above but an auxiliary variable z\u2217 does. For example, in Figure 1a, we cannot identify \u03b1 using Theorem 2. Blocking the path x \u2190 t \u2194 y by conditioning on t opens the path, x \u2194 t \u2194 y. Moreover, we cannot use t or s in an instrumental set due to the edges t \u2194 y and s \u2194 y. However, s is an IV for \u03b2, allowing us to generate an AV, x\u2217 = x \u2212 \u03b2 \u00b7 t1, as in Figure 1b. Now, \u03b1 can be identified using x\u2217 as an auxiliary instrument given w1.\nTheorem 1 tells us when (i) of Theorem 2 can be satisfied using an AV, z\u2217i . We simply check whether zi can be separated from y in GE\u222aEz\u2212, where Ez \u2286 Inc(zi) is the set of zi\u2019s edges whose coefficient values are known. When an instrumental set includes AVs, we call the set an auxiliary instrumental set or auxiliary IV set for short.\n7Brito and Pearl (2002a) provided an alternative statement of condition (iii). A proof that the two statement are, in fact, equivalent is given in the Appendix (Chen et al., 2017).\n8Note that when k = 1, z1 is an IV for (x1, y). Further, if z1 = x1, then x1 satisfies the single-door criterion for (x1, y).\nFigure 1a also demonstrates the importance of extending the simple auxiliary instrumental sets introduced by Chen et al. (2016) to allow for conditioning. \u03b1 can only be identified if we block the paths x \u2194 w1 \u2192 y and x \u2194 w1 \u2192 w2 \u2192 y by conditioning on w1.\nWhen knowledge of coefficient values are known a priori, it may be helpful to generate an AV from the outcome variable y. For example, in Figure 2a, \u03b1 cannot be identified. However, suppose that it is possible to run a surrogate experiment and randomize z. This experiment would allow us to estimate \u03b3 and generate the AV, Y \u2217 = Y \u2212 \u03b3Z. Now, z is not technically an instrument for \u03b1, but it can be shown that \u03b1 = rY \u2217Z.WrXZ . Chen et al. (2016) called such variables quasi-instrumental variables or quasi-IVs for short.\nInterestingly, while quasi-IVs are valuable for the problem of z-identification, they do no better than instrumental sets when applied to the standard identification problem, where no external knowledge of coefficient values is available. For example, consider again Figure 2a. In order to use z as a quasi-IV for \u03b1, we would first have to identify \u03b3 using an IV. If such a variable existed, say z\u2032, then we could have simply identified {\u03b1, \u03b3} using the IV set {z, z\u2032}.\nNext, we formally define quasi-instrumental sets or quasiIV sets for short. Note that auxiliary IV sets are also quasiIV sets.\nDefinition 5. Given a linear SEM with graph G, a set of edges EK whose coefficient values are known, and a set of structural coefficients \u03b1 = {\u03b11, \u03b12, ..., \u03b1k}, the set Z = {z1, ..., zk} is a quasi-instrumental set if there exist triples (z1,W1, p1), ..., (zk,Wj , pk) such that:\n(i) For i = 1, ..., k, either:\n(a) the elements of Wi are non-descendants of y, and (zi |= y|Wi)GE\u222aEy where Ey = EK \u2229 Inc(y).\n(b) the elements of Wi are non-descendants of zi and y, and (zi |= y|Wi)GE\u222aEzy where Ezy = EK \u2229 (Inc(z) \u222a Inc(y)).\n(ii) for i = 1, ..., k, pi is a path between zi and xi that is not blocked by Wi, where xi = He(\u03b1i), and\n(iii) the set of paths {p1, ..., pk} has no sided intersection\nTheorem 3. If Z\u2217 is a quasi-instrumental set for E, then E is identifiable.\nLastly, the following corollary provides a simple graphical condition for when a single variable or AV qualifies as a quasi-IV.\nCorollary 1. Given a linear SEM with graph G, z\u2217 is a quasi-IV for \u03b1 given W if W does not contain any descendants of z, and z is an IV for \u03b1 given W in GEz\u222aEy\u2212, where Ez \u2286 Inc(z) and Ey \u2286 Inc(y) are sets of edges whose coefficient values are known.\nAuxiliary and quasi-IV sets enable a bootstrapping procedure whereby complex models can be identified by iteratively identifying coefficients and using them to generate new auxiliary variables. For example, consider Figure 3a. First, we are able to identify b and c using IVs, but no other coefficients. Once b is identified, Corollary 1 tells us that e is identified using v\u22173 since v3 is an IV for e when the edge for b is removed (see Figure 3b). Now, the identification of e allows us to identify a and d using v\u22175 , since v5 is an IV for a and d when the edge for e is removed (see Figure 3c). This general strategy is the basis for our identification, zidentification, and model testing algorithm, described next."}, {"heading": "4. Identification and z-Identification Algorithm", "text": "In this section, we construct an identification algorithm that operationalizes the bootstrapping approach described in Section 3. First, we describe how to algorithmically find a quasi-instrumental set for a set of coefficients E, given a set of known coefficients, IDEdges.\nThe problem of finding generalized instrumental sets was addressed by van der Zander and Liskiewicz (2016). They provided an algorithm, TestGeneralIVs, that determines whether a given set Z is a generalized instrumental set for a set of edges, E, that runs in polynomial time if we bound the size of the coefficient set to be identified. More specifically, their algorithm has a running time of O((k!)2nk), where n is the number of variables in the graph and k = |E|.9\nOur method, TestQIS, given in the Appendix (Chen et al., 2017), generalizes TestGeneralIVs, for quasi-IV sets.\n9van der Zander and Liskiewicz (2016) also give an algorithm that tests whether Z is a simple conditional instrumental sets in O(nm) time. A simple conditional instrumental set is a generalized instrumental set where W1 =W2 = ... =Wk\nFindQIS, also given in the Appendix (Chen et al., 2017), searches for a quasi-IV set by checking all subsets of Z \u2286 (Anc(zi) \u222a Anc(y)) using TestQIS. It returns a quasi-IV set, as well as its conditioning sets, if one exists.\nIn some cases an instrumental set may not exist for C, but one exists for C \u2032 , where C \u2282 C \u2032 . Conversely, there may not be an instrumental set for C \u2032 , but there is one for C \u2282 C \u2032 . As a result, we may have to check all possible subsets of a variable\u2019s coefficients in order to determine whether a given subset is identifiable using auxiliary instrumental sets. This search can be simplified somewhat by noting that if E is a connected edge set (defined below) with no instrumental set, then there is no superset E \u2032 with an instrumental set. Definition 6. (Chen et al., 2014) For an arbitrary variable, V , let Pa1, Pa2, ..., Pak be the unique partition of Pa(V) such that any two parents are placed in the same subset, Pai, whenever they are connected by an unblocked path. A connected edge set with head V is a set of directed edges from Pai to V for some i \u2208 {1, 2, ..., k}.\nThe ID algorithm, called qID utilizes FindQIS to identify as many coefficients as possible in a given model with graph G. It iterates through each connected edge set and attempts to identify it using FindQIS. If it is unable to identify the connected edge set, it then attempts to identify subsets of the connected edge set. After the algorithm has attempted to identify each connected edge set, it again attempts to identify each unidentified connected edge set, since each newly identified coefficient may enable the identification of previously unidentifiable coefficients. This process is repeated until all coefficients have been identified or no new coefficients have been identified in the last iteration. The algorithm is polynomial if the degree of each node in the graph is bounded.\nOur algorithm identifies the model depicted in Figure 4b in the following way. First, let us assume that the connected edge sets are arbitrarily ordered, ({a}, {b, c, f}, {d}, {e}). Now, the first edge to be identified would be a using w1 as an IV. There is no auxiliary IV set for {b, c, f}, and we would attempt to find one for its subsets. We find that {b} is identified using {x} as an IV set with conditioning set\n{w1}. Now, {d} is identified using y\u2217 = y \u2212 bx, and e is identified using t\u22172. In the second iteration, we return to {b, c, f} and find that it is now identified using the auxiliary IV set, {x,w1, t\u22173}.\nAlgorithm 1 qID(G,\u03a3, IDEdges) Initialize: EdgeSets\u2190 all connected edge sets in G repeat\nfor all ES in EdgeSets such that ES 6\u2286 IDEdges do y \u2190 He(ES) for all E \u2286 ES such that E 6\u2286 IDEdges do\n(Z,W )\u2190 FindQIS(G,ES, IDEdges) if (Z,W ) 6=\u22a5 then\nIdentify ES using Z\u2217 as an auxiliary instrumental set in G(IDEdges\u2229Inc(Z))+\nIDEdges\u2190 IDEdges \u222a ES end if\nend for end for\nuntil All coefficients have been identified or no coefficients have been identified in the last iteration\nIn contrast, Figure 4b is not identified using simple instrumental sets and auxiliary variables. We cannot identify b without conditioning on w1, which means that the only coefficients identified using auxiliary simple instrumental sets is a. Since Chen et al. (2016) showed that any coefficient identified using the generalized half-trek criterion (g-HTC) can be identified using auxiliary variables and simple instrumental sets, we know that qID is able to identify coefficients and models that the g-HT algorithm is not. Moreover, qID will identify any coefficients that are identifiable using auxiliary variables and simple instrumental sets, giving us the following theorem.\nTheorem 4. Given an arbitrary linear causal model, if a set of coefficients is identifiable using the g-HT algorithm, then it is identifiable using qID. Additionally, there are models that are not identified using the g-HT algorithm, but identified using qID."}, {"heading": "5. Deriving Testable Implications using AVs", "text": "Theorem 1 also enables us to derive new vanishing partial correlation constraints that can be used to test the model. For example, in Figure 4a, \u03b1 can be identified using z1 as an instrument. Once \u03b1 is identified, we can generate the AV y\u2217 = y \u2212 \u03b1x = y \u2212 \u03c3(y,z1)\u03c3(x,z1)x, and Theorem 1 tells us that the correlation of z2 and y\u2217 should vanish. As a result, we can test the model specification by verifying that this constraint holds in the data.\nTheorem 1 also tells us that the correlation between z1 and y\u2217 should also vanish. However, upon closer inspection, we find that this implication does not actually constrain the covariance matrix:\n\u03c3(z1, y \u2217) =\u03c3(z1, y \u2212 \u03b1x)\n=\u03c3(z1, y)\u2212 \u03c3(y, z1)\n\u03c3(x, z1) \u03c3(z1, x) = 0.\nIn other words, our \u201ctestable implication\u201d that \u03c3(z1, y\u2217) = 0 is equivalent to stating \u03c3(z1, y)\u2212 \u03c3(z1, y) = 0\u2013a tautology! In contrast,\n\u03c3(z2, y \u2217) = \u03c3(z2, y)\u2212\n\u03c3(z1, y) \u03c3(x, z1) \u03c3(z2, x) = 0\ndoes provide a true testable implication.\nShpitser et al. (2009) noticed a similar phenomenon when deriving dormant independences in non-parametric models, and their explanation applies to conditional independence constraints among AVs as well. The idea is the following: When the model implies that two variables are conditionally independent, it relies on the modeled assumption that there is no edge between those variables. As a result, verifying that the constraint holds in data represents a test that this assumption is valid. However, unlike conditional independence constraints between model variables, conditional independence constraints among AVs rely upon the absence of certain edges in order to identify the coefficients\nnecessary to generate the AV. The key point is that this identification cannot rely on the same lack of edge whose existence we are trying to test!\nIn the above example, we identified \u03b1 using z1 as an IV. \u03c3(z2, y\n\u2217) = 0 follows from the lack of edge between z2 and y. However, even if this edge did exist, z\u2217 still equals z \u2212 \u03c3(y,z1)\u03c3(x,z1)x. In contrast, \u03c3(z1, y\n\u2217) = 0 follows from the lack of edge between z1 and y. The existence of this edge would disallow z1 as an instrument and z\u2217 = z \u2212 \u03b1x 6= z \u2212 \u03c3(y,z1)\u03c3(x,z1)x.\nAnother way to derive the constraint \u03c3(z2, y\u2217) = 0 is via overidentification. \u03b1 can be identified using either z1 or z2 and equating the corresponding expressions yields the constraint \u03c3(y,z1)\u03c3(x,z1) = \u03c3(y,z2) \u03c3(x,z2)\n, which is clearly equivalent to the previous constraint \u03c3(z2, y\u2217) = 0. In fact, we show (Theorem 6) that whenever a variable z cannot be separated from another variable y, but z\u2217 can be, the resulting AV conditional independence, if it is non-vacuous, is equivalent to an overidentifying constraint that can be derived using quasiIVs. As a result, all non-vacuous AV conditional independences are captured by overidentifying constraints derived using quasi-IVs!\nFirst, we give a sufficient condition for when a set of edges \u03b1 is overidentified.\nTheorem 5. Let Z be a quasi-IV set for structural coefficients \u03b1 = {\u03b11, ..., \u03b1k} and E be a set of known edges. If there exists a node s satisfying the conditions listed below, then \u03b1 is overidentified and we obtain the constraint .\n(i) s /\u2208 Z\n(ii) There exists an unblocked path between s and y including an edge in \u03b1\n(iii) There exists a conditioning set W that does not block the path p, such that either:\n(a) the elements ofW are non-descendants of y, and (s |= y|W )G\u03b1\u222aEy\u2212, where Ey = E \u2229 Inc(y)) (b) the elements of W are non-descendants of s and y, and (s |= y|W )G\u03b1\u222aEs\u222aEy\u2212 where Es = E \u2229 Inc(s).\nThe above theorem can be used to derive an overidentifying constraint for every variable that satisfies (i)-(iii) above. It can also be applied when \u03b1 is known a priori, yielding a z-overidentifying constraint. In this case, Z = \u2205 would be a quasi-IV set that trivially identifies \u03b1.\nThe following theorem states that non-vacuous AV conditional independence constraints are subsumed by quasi-IV overidentifying and z-overidentifying constraints.\nTheorem 6. Let z\u2217 = z \u2212 e1t1 \u2212 ... \u2212 ektk and suppose there does not exist W such that (z |= y|W )G. There exists W such that W \u2229De(z) = \u2205 and (z\u2217 |= y|W ) is nonvacuous if and only if y satisfies the conditions of Theorem 5 for E = {e1, ..., ek}.\nThe above theorem also applies when y is an AV, called y\u2217. In this case, we simply replace (z |= y|W )G with (z |= y\u2217|W )GEy+ , where Ey \u2286 Inc(y) is a set of edges whose coefficient values are known.\nAlgorithm 2 uses quasi-IV sets to output overidentifiying constraints in a graph given an optional set of identified edges. It uses isEIV, which is a slightly modified version of FindQIS that tests whether w fits the conditions of Theorem 6. Details of isEIV can be found in the Appendix (Chen et al., 2017).\nAlgorithm 2 Finds overidentifying constraints for G function CONSTRAINTFINDER(G,\u03a3,IDEdges)\nfor all ES \u2208 Edge Sets of G do (Z,W )\u2190 FINDQIS(ES,G,IDEdges) if (Z,W ) 6= \u22a5 then\nfor all w \u2208 V \\ Z \u222a {He(ES)} do if ISEIV(w,ES,G,IDEdges) then\nAdd constraint awA\u22121b = bw end if\nend for end if\nend for end function"}, {"heading": "6. Discussion and Related Work", "text": "In this section, we discuss how (single-variable) auxiliary IVs encompass a number of previous identification methods developed in economics (Hausman and Taylor, 1983), computer science (Chan and Kuroki, 2010), and epidemiology (Shardell, 2012).\nHausman and Taylor (1983) showed that if the equation for a given variable, z = \u03b21p1 + ...+ \u03b2kpk + uz , is identified, then the error term uz can be estimated and used as an instrument for other coefficients. In this case, the auxiliary variable z\u2217 = z \u2212 \u03b21p1 \u2212 ... \u2212 \u03b2kpk is equal to the error term uz . As a result, whenever the error term is estimable and can be used as an IV, we can also generate an auxiliary instrument. However, there are times when only some of the coefficients in an equation are identifiable, and as a result, the error term cannot be used as an instrument, but we can nevertheless generate an auxiliary instrument. As a result, auxiliary IVs strictly subsume error term IVs.\nChan and Kuroki (2010) gave sufficient conditions for when a descendant of x and a descendant of y could be\nused in analogous manner to IVs to identify the effect of x on y. In the context of AVs, this method is equivalent to generating an auxiliary instrument from the descendant by subtracting the total effect of x on the descendant or the total effect of y on the descendant (depending on whether the variable is a descendant of x or y). In this paper, we generated AVs by subtracting out direct effects, but clearly the work can be extended to subtracting out total effects. The benefit of AVs over these descendant IVs is that they can be generated from a variety of variables, not just descendants of x and y. Additionally, descendants of x or y can generate AVs from other total or direct effects, not just the effect of x or y on the descendant.\nThe notion of \u201csubtracting out a direct effect\u201d in order to turn a variable into an instrument was also noted by Shardell (2012) when attemping to identify the total effect of x on y. It was noticed that in certain cases, the violation of the independence restriction of a potential instrument z (i.e. z is not independent of the error term of y) could be remedied by identifying, using ordinary least squares regression, and then subtracting out the necessary direct effects on y. AVs generalize and operationalize this notion so that it can be used on arbitrary sets of known coefficient values and be utilized in conjunction with existing graphical methods for identification and enumeration of testable implications.\nAdditionally, as we have alluded to earlier, the highly algebraic, state-of-the-art g-HTC can also be understood in terms of auxiliary instruments. Identification using the gHTC is equivalent to identification using auxiliary simple instrumental sets.\nIn summary, auxiliary instruments are not only the basis for the most general identification algorithm yet devised, but they also unify disparate identification methods under a single framework. Moreover, AVs are directly applicable to the tasks of z-identification and model testing. Finally, they can, in principle, enhance any method for identification, model testing, or other tasks that relies on graphical separation."}, {"heading": "7. Conclusion", "text": "In this paper, we graphically characterized conditional independence among AVs, allowing us to demonstrate how they can help generalized instrumental sets in the problem of identification. We provided an algorithm that identifies more models than the g-HT algorithm, subsuming the stateof-the-art for identification in linear models. Additionally, we introduced quasi-IV sets, and constructed an algorithm that utilizes them to attack the problem of z-identification. Finally, we proved that AV conditional independences are subsumed by overidentifying constraints and gave an algorithm for deriving overidentifying constraints."}, {"heading": "Acknowledgements", "text": "We would like to thank Judea Pearl, Mathias Drton, Thomas Richardson, and Luca Weihs for helpful discussions. This research was supported in parts by grants from NSF #IIS-1302448 and #IIS-1527490 and ONR #N0001413-1-0153 and #N00014-13-1-0153."}], "year": 2017, "references": [{"title": "Causal inference by surrogate experiments: z-identifiability", "authors": ["E. BAREINBOIM", "J. PEARL"], "venue": "Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence (N. de Freitas and K. Murphy, eds.). AUAI Press, Corvallis, OR.", "year": 2012}, {"title": "Causal inference and the data-fusion problem", "authors": ["E. BAREINBOIM", "J. PEARL"], "venue": "Proceedings of the National Academy of Sciences 113 7345\u20137352.", "year": 2016}, {"title": "Identification, Equivalent Models, and Computer Algebra", "authors": ["P. BEKKER", "A. MERCKENS", "T. WANSBEEK"], "venue": "Statistical Modeling and Decision Science, Academic Press.", "year": 1994}, {"title": "Instrumental Variables", "authors": ["R. BOWDEN", "D. TURKINGTON"], "venue": "Cambridge University Press, Cambridge, England.", "year": 1984}, {"title": "Generalized instrumental variables", "authors": ["C. BRITO", "J. PEARL"], "venue": "Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference (A. Darwiche and N. Friedman, eds.). Morgan Kaufmann, San Francisco, 85\u201393.", "year": 2002}, {"title": "A graphical criterion for the identification of causal effects in linear models", "authors": ["C. BRITO", "J. PEARL"], "venue": "Proceedings of the Eighteenth National Conference on Artificial Intelligence. AAAI Press/The MIT Press, Menlo Park, CA, 533\u2013538.", "year": 2002}, {"title": "A new identification condition for recursive models with correlated errors", "authors": ["C. BRITO", "J. PEARL"], "venue": "Journal Structural Equation Modeling 9 459\u2013474.", "year": 2002}, {"title": "Graphical condition for identification in recursive SEM", "authors": ["C. BRITO", "J. PEARL"], "venue": "Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligence. AUAI Press, Corvallis, OR, 47\u201354.", "year": 2006}, {"title": "Using descendants as instrumental variables for the identification of direct causal effects in linear sems", "authors": ["H. CHAN", "M. KUROKI"], "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS).", "year": 2010}, {"title": "Identification and overidentification of linear structural equation models", "authors": ["B. CHEN"], "venue": "Advances In Neural Information Processing Systems. 1579\u20131587.", "year": 2016}, {"title": "Identification and model testing in linear structural equation models using auxiliary variables", "authors": ["B. CHEN", "D. KUMOR", "E. BAREINBOIM"], "venue": "arXiv preprint arXiv:1612.03451; Technical report R-27, Purdue AI Lab, Dept. of Computer Science, Purdue University. .", "year": 2017}, {"title": "Graphical tools for linear structural equation modeling", "authors": ["B. CHEN", "J. PEARL"], "venue": "Tech. Rep. R432, <http://ftp.cs.ucla.edu/pub/stat ser/r432.pdf>, Department of Computer Science, University of California, Los Angeles, CA. Forthcoming, Psychometrika.", "year": 2014}, {"title": "Incorporating knowledge into linear structural equation models using auxiliary variables", "authors": ["B. CHEN", "J. PEARL", "E. BAREINBOIM"], "venue": "Proceedings of the Twenty-fifth International Joint Conference on Artificial Intelligence (S. Kambhampati, ed.).", "year": 2016}, {"title": "Testable implications of linear structual equation models", "authors": ["B. CHEN", "J. TIAN", "J. PEARL"], "venue": "Proceedings of the Twenty-eighth AAAI Conference on Artificial Intelligence (C. E. Brodley and P. Stone, eds.). AAAI Press, Palo, CA.", "year": 2014}, {"title": "Generic identifiability of linear structural equation models by ancestor decomposition", "authors": ["M. DRTON", "L. WEIHS"], "venue": "Scandinavian Journal of Statistics n/a\u2013n/a10.1111/sjos.12227. URL http://dx.doi.org/10.1111/sjos.", "year": 2016}, {"title": "The Identification Problem in Econometrics", "authors": ["F. FISHER"], "venue": "McGraw-Hill, New York.", "year": 1966}, {"title": "Halftrek criterion for generic identifiability of linear structural equation models", "authors": ["R. FOYGEL", "J. DRAISMA", "M. DRTON"], "venue": "The Annals of Statistics 40 1682\u2013 1713.", "year": 2012}, {"title": "Identification in linear simultaneous equations models with covariance restrictions: an instrumental variables interpretation", "authors": ["J.A. HAUSMAN", "W.E. TAYLOR"], "venue": "Econometrica: Journal of the Econometric Society 1527\u20131549.", "year": 1983}, {"title": "Distinguishing cause from effect using observational data: methods and benchmarks", "authors": ["J.M. MOOIJ", "J. PETERS", "D. JANZING", "J. ZSCHEISCHLER", "B. SCH\u00d6LKOPF"], "venue": "Journal of Machine Learning Research 17 1\u2013102.", "year": 2016}, {"title": "Robustness of causal claims", "authors": ["J. PEARL"], "venue": "Proceedings of the Twentieth Conference Uncertainty in Artificial Intelligence (M. Chickering and J. Halpern, eds.). AUAI Press, Arlington, VA, 446\u2013453.", "year": 2004}, {"title": "Causality: Models, Reasoning, and Inference", "authors": ["J. PEARL"], "venue": "2nd ed. Cambridge University Press, New York.", "year": 2009}, {"title": "A necessary and sufficient identification rule for structural models estimated in practice", "authors": ["E.E. RIGDON"], "venue": "Multivariate Behavioral Research 30 359\u2013383.", "year": 1995}, {"title": "Methods to overcome violations of an instrumental variable assumption: Converting a confounder into an instrument", "authors": ["M. SHARDELL"], "venue": "Computational statistics & data analysis 56 2317\u20132333.", "year": 2012}, {"title": "A linear non-gaussian acyclic model for causal discovery", "authors": ["S. SHIMIZU", "P.O. HOYER", "A. HYV\u00c4RINEN", "A. KERMINEN"], "venue": "Journal of Machine Learning Research 7 2003\u20132030.", "year": 2006}, {"title": "Testing edges by truncations", "authors": ["I. SHPITSER", "T.S. RICHARDSON", "J.M. ROBINS"], "venue": "IJCAI.", "year": 2009}, {"title": "Causation, prediction, and search, vol", "authors": ["P. SPIRTES", "C.N. GLYMOUR", "R. SCHEINES"], "venue": "81. MIT press.", "year": 2000}, {"title": "Using path diagrams as a structural equation modelling tool", "authors": ["P. SPIRTES", "T. RICHARDSON", "C. MEEK", "R. SCHEINES", "C. GLYMOUR"], "venue": "Sociological Methods and Research 27 182\u2013225.", "year": 1998}, {"title": "Identifying direct causal effects in linear models", "authors": ["J. TIAN"], "venue": "Proceedings of the National Conference on Artificial Intelligence, vol. 20. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.", "year": 2005}, {"title": "A criterion for parameter identification in structural equation models", "authors": ["J. TIAN"], "venue": "Proceedings of the Twenty-Third Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-07). AUAI Press, Corvallis, Oregon.", "year": 2007}, {"title": "Parameter identification in a class of linear structural equation models", "authors": ["J. TIAN"], "venue": "Proceedings of the Twenty-First International Joint Conference on Artificial Intelligence (IJCAI-09).", "year": 2009}, {"title": "Searching for generalized instrumental variables", "authors": ["B. VAN DER ZANDER", "M. LISKIEWICZ"], "venue": "Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AISTATS-16)).", "year": 2016}, {"title": "Efficiently finding conditional instruments for causal inference", "authors": ["B. VAN DER ZANDER", "J. TEXTOR", "M. LISKIEWICZ"], "venue": "Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI 2015).", "year": 2015}, {"title": "Correlation and causation", "authors": ["S. WRIGHT"], "venue": "Journal of Agricultural Research 20 557\u2013585.", "year": 1921}, {"title": "On the identifiability of the post-nonlinear causal model", "authors": ["K. ZHANG", "A. HYV\u00c4RINEN"], "venue": "Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence. AUAI Press.", "year": 2009}], "id": "SP:be82c1ba1880270149ae74e92be9856cfba77709", "authors": [{"name": "Bryant Chen", "affiliations": []}, {"name": "Daniel Kumor", "affiliations": []}, {"name": "Elias Bareinboim", "affiliations": []}], "abstractText": "We developed a novel approach to identification and model testing in linear structural equation models (SEMs) based on auxiliary variables (AVs), which generalizes a widely-used family of methods known as instrumental variables. The identification problem is concerned with the conditions under which causal parameters can be uniquely estimated from an observational, noncausal covariance matrix. In this paper, we provide an algorithm for the identification of causal parameters in linear structural models that subsumes previous state-of-the-art methods. In other words, our algorithm identifies strictly more coefficients and models than methods previously known in the literature. Our algorithm builds on a graph-theoretic characterization of conditional independence relations between auxiliary and model variables, which is developed in this paper. Further, we leverage this new characterization for allowing identification when limited experimental data or new substantive knowledge about the domain is available. Lastly, we develop a new procedure for model testing using AVs.", "title": "Identification and Model Testing in Linear Structural Equation Models using Auxiliary Variables"}