0	42	Despite over 25 years of research in computational linguistics aimed at acquiring multiword lexicons using corpora statistics, and growing evidence that speakers process language primarily in terms of memorized sequences (Wray, 2008), the individual word nonetheless stubbornly remains the de facto standard processing unit for most research in modern NLP.
1	198	The potential of multiword knowledge to improve both the automatic processing of language as well as offer new understanding of human acquisition and usage of language is the primary motivator of this work.
4	21	A central challenge in building comprehensive multiword lexicons is paring down the huge space of possibilities without imposing restrictions which disregard a major portion of the multiword vocabulary of a language: allowing for diversity creates significant redundancy among statistically promising candidates.
41	27	The explanatory power of the whole lattice is defined simply as a product of the explainedness of the individual nodes.
43	26	The basis of the calculation of explainedness is the syntax-sensitive LPR association measure of Brooke et al. (2015), but it is calculated differently depending on the on/off status of the node as well as the status of the nodes in its vicinity.
44	46	Nodes are linked based on n-gram subsumption and corpus overlap relationships (see Figure 2), with “on” nodes typically explaining other nodes.
54	28	LPR is a measure of how predictable a word is in a lexical context, as compared to how predictable it is given only syntactic context (over the same span of words).
57	25	We use the same equation for gapped n-grams, with the caveat that quantities involving sequences which include the location where the gap occurs are derived from special gapped n-gram statistics.
60	25	For example, in the case of be keep ∗ under wraps (Figure 2), a general statistical metric might assign it a high score due to the strong association between keep and under or under and wraps, but minLPR is focused on the weaker relationship between be and the rest of the phrase.
63	32	For example, as detailed in Figure 2, the (gapped) n-gram keep ∗ under wraps would be connected “upwards” to the node keep everything under wraps and connected “downwards” to under wraps.
65	53	A third, undirected mechanism is overlapping, where nodes inhibit each other due to overlaps in the corpus (e.g., having both keep ∗ under wraps and be keep ∗ under as FS will be avoided).
68	26	Hard covering is based on the idea that, due to very similar counts, we can reasonably conclude that the presence of an n-gram in our statistics is a direct result of a subsuming (n+i)-gram.
69	33	In Figure 2, e.g., if we have 143 counts of keep ∗ under wraps and 152 counts of under wraps, the presence of keep ∗ under wraps almost completely explains under wraps, and we should consider these two n-grams as one.
88	23	We refer to this mechanism as “clearing” because it tends to clear away a variety of trivial uses of common FS which may have higher LPR due to the lexical and syntactic specificity of the FS.
94	18	For example, given that be keep ∗ under and keep ∗ under wraps often appear together (overlapping on the tokens keep ∗ under), we do not want both being selected as an FS, even in the case that both have high minLPR.
99	27	The effect of overlap is hyperbolic: small amounts of overlap have little effect, but nodes with significant overlap will effectively be forced to turn off.
101	31	, xN , which can be defined in terms of minLPR, the node interaction functions, and the FS status yi of each node in the lattice: expl(X) = |X|∏ i=1 yi · C−overlap(xi) + (−yi + 1) ·minLPR(xi)−cover(xi)·clear(xi) (1) When a node is off, its explainedness is the inverse of its minLPR, except if there are covering or clearing nodes which explain it by pushing the exponent of minLPR towards zero.
102	23	When the node is on, its explainedness is the inverse of a fixed cost hyperparameterC, though this cost is increased if it overlaps with other active nodes.
103	29	All else being equal, when minLPR(t) > C, a node will be selected as an FS, and so, independent of the node interactions, C can be viewed as the threshold for the minLPR association measure under a traditional approach to MWE identification.
114	22	Explainedness values are indicated by e. rev = relevant, aff = affected, curr = current function LOCALOPT(Ystart , x,Xrev , Xaff ) Ystart ← SET(Ystart , x,ON) Q← EMPTYQUEUE() ebest ← 0 Ybest ← NULL PUSH(Q,Ystart) repeat Ycurr ← POP(Q) ecurr ← CALCEXPL(Ycurr , Xaff ) for xrev in Xrev do Ynew ← SET(Ycurr , xrev ,OFF) enew ← CALCEXPL(Ynew , Xaff ) if enew > ecurr then PUSH(Q,Ynew ) if enew > ebest then: ebest ← enew Ybest ← Ynew until ISEMPTY(Q) return Ybest FREQUENCYSORTREVERSE(X) s← INITIALIZEALLOFF(X) repeat Changed ← FALSE for x in X do Xrev ← GETRELEVANT(x,X) Xaff ← GETAFFECTED(Xrev , X) Ynew ← LOCALOPT(Y, x,Xrev , Xaff ) if Ynew 6= Y then Y ← Ynew Changed ← TRUE until !Changed a competing node like keep ∗ under wraps from being correctly activated.
116	32	Since turned-off nodes have no direct effect on each other, only turned-on nodes above, below, or overlapping with the current node in the lattice need be considered.
118	51	Next, a search is carried out for the optimal configuration of the relevant nodes, starting from an ‘all-on’ state and iteratively considering new states with one relevant node turned off; the search continues as long as there is an improvement in explainedness.
143	21	One advantage of a type-based annotation approach, particularly with regards to annotation with a known subjective component, is that it is quite sensible to simply discard borderline cases, improving reliability at the cost of some representativeness.
158	20	The LPRseg method of Brooke et al. (2015) consistently outperforms simple ranking, and the lattice method proposed here does better still, with a margin that is fairly consistent across the languages.
162	25	In the contiguous FS test set for the BNC (Ta- ble 3), we found that both the LocalMaxs algorithm and the DP-seg method of Newman et al. (2012) were able to beat our other baseline methods with roughly similar F-scores, though both are well below our Lattice method.
175	23	We were interested in Croatian and Japanese in part because of their relatively free word order, and whether the handling of gaps would help with identifying FS in these languages.
183	44	Though some of the diversity can of course be attributed to noise, it is safe to say that most FS do not fall into the standard two-word syntactic categories used in MWE work, and therefore identifying them requires a much more general approach like the one presented here.
189	36	One striking thing about the non-English FS is how poorly they translate: many good FS in these languages become extremely awkward when translated into English.
203	29	In terms of the content of the resulting lexicon we believe the effect of this difference on FS extraction is modest, since much of the extra FS in Japanese would simply be single words in other languages (and considered trivially part of the FS lexicon).
207	60	Though it is indisputable that inflectional fixedness is part of the lexical information contained in an FS, in practice this sort of information can be efficiently derived post hoc from the corpus statistics.
