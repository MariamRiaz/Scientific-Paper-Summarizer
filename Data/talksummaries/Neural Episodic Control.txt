2	32	For example, in the Atari 2600 set of environments (Bellemare et al., 2013), deep Q-networks (Mnih et al., 2016) require more than 200 hours of gameplay in order to achieve scores similar to those a human player achieves after two hours (Lake et al., 2016).
3	31	The glacial learning speed of deep reinforcement learning has several plausible explanations and in this work we focus on addressing these: 1.
4	26	Stochastic gradient descent optimisation requires the use of small learning rates.
11	17	This can be fairly efficient if updates happen in reverse order in which the transitions occur.
12	15	However, in order to train on uncorrelated minibatches DQN-style, algorithms train on randomly selected transitions, and, in order to further stabilise training, require the use of a slowly updating target network further slowing down reward propagation.
16	14	Our work is in part inspired by the hypothesised role of the Hippocampus in decision making (Lengyel & Dayan, 2007; Blundell et al., 2016) and also by recent work on one-shot learning (Vinyals et al., 2016) and learning to remember rare events with neural networks (Kaiser et al., 2016).
21	21	This helps alleviate the typically slow weight updates of stochastic gradient descent applied to the whole network and is reminiscent of work on fast weights (Ba et al., 2016; Hinton & Plaut, 1987), although the architecture we present is quite different.
23	27	Instead, we elect to write all experiences to the memory, and allow it to grow very large compared to existing memory architectures (in contrast to Oh et al. (2015); Graves et al. (2016) where the memory is wiped at the end of each episode).
40	35	A3C (Mnih et al., 2016) is another well known deep reinforcement learning algorithm that is very different from DQN.
41	31	It is based upon a policy gradient, and learns both a policy and its associated value function, which is learned entirely on-policy (similar to the λ = 1 case of Q(λ)).
46	14	The memory module acts as an arbitrary association from keys to corresponding values, much like the dictionary data type found in programs.
53	13	After a DND is queried, a new key-value pair is written into the memory.
82	47	If the state is not already present Q(N)(st, a) is appended to Va and h is appended to Ka.
83	20	Note that our agent learns the value function in much the same way that a classic tabular Q-learning agent does, except that the Q-table grows with time.
84	15	We found that α could take on a high value, allowing repeatedly visited states with a stable representation to rapidly update their value function estimate.
87	22	Agent parameters are updated by minimising the L2 loss between the predicted Q value for a given action and the Q(N) estimate on randomly sampled mini-batches from a replay buffer.
91	42	Backpropagation updates the weights and biases of the convolutional embedding network and the keys and values of each actionspecific memory using gradients of this loss, using a lower learning rate than is used for updating pairs after queries.
92	48	We investigated whether neural episodic control allows for more data efficient learning in practice in complex domains.
93	85	As a problem domain we chose the Atari Learning Environment(ALE; Bellemare et al., 2013).
94	46	We tested our method on the 57 Atari games used by Schaul et al. (2015a), which form an interesting set of tasks as they contain diverse challenges such as sparse rewards and vastly different magnitudes of scores across games.
98	24	We also compare to two algorithms incorporating λ returns (Sutton, 1988) aiming at more data efficiency by faster propagation of credit assignments, namely Q∗(λ) (Harutyunyan et al., 2016) and Retrace(λ) (Munos et al., 2016).
103	48	In our implementation of MFEC we used random projections as an embedding function, since in the original publication it obtained better performance on the Atari games tested.
104	24	In terms of hyperparameters for NEC, we chose the same convolutional architecture as DQN, and store up to 5 × 105 memories per action.
112	54	(5) Intuitively, when all neighbours are far away we want to avoid putting all weight onto one data point.
113	12	A Gaussian kernel, for example, would exponentially suppress all neighbours except for the closest one.
116	30	In order to determine whether an key corresponding to a given state is already present in the table we store a hash of the observation for each state and check whether the hash is present when inserting.
126	26	Across most games, NEC is significantly faster at learning in the initial phase (see also Table 1), only comparable to MFEC, which also uses an episodic-like Q-function.
172	37	At the core of NEC is a memory structure: a Differentiable Neural Dictionary (DND), one for each potential action.
173	13	NEC inserts recent state representations paired with corresponding value functions into the appropriate DND.
175	38	We speculate that NEC learns faster through a combination of three features of the agent: the memory architecture (DND), the use of N -step Q estimates, and a state representation provided by a convolutional neural network.
