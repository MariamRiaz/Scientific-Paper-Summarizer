{"sections": [{"text": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2306\u20132312, Austin, Texas, November 1-5, 2016. c\u00a92016 Association for Computational Linguistics"}, {"heading": "1 Introduction", "text": "Discovering the discourse relation between two sentences is crucial to understanding the meaning of a coherent text, and also beneficial to many downstream NLP applications, such as question answering and machine translation. Implicit discourse relation recognition (DRRimp) remains a challenging task due to the absence of strong surface clues like discourse connectives (e.g. but). Most work resorts to large amounts of manually designed features (Soricut and Marcu, 2003; Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Rutherford and Xue, 2014), or distributed features learned via neural network models (Braud and Denis, 2015; Zhang et al., 2015; Ji and Eisenstein, 2015). The above methods usually suffer from limited labeled data.\nMarcu and Echihabi (2002) attempt to create labeled implicit data automatically by removing connectives from explicit instances, as additional training data. These data are usually called as syn-\n\u2217Corresponding author.\nthetic implicit data (hereafter SynData). However, Sporleder and Lascarides (2008) argue that SynData has two drawbacks: 1) meaning shifts in some cases when removing connectives, and 2) a different word distribution with the real implicit data. They also show that using SynData directly degrades the performance. Recent work seeks to derive valuable information from SynData while filtering noise, via domain adaptation (Braud and Denis, 2014; Ji et al., 2015), classifying connectives (Rutherford and Xue, 2015) or multi-task learning (Lan et al., 2013; Liu et al., 2016), and shows promising results.\nDifferent from previous work, we propose to construct bilingually-constrained synthetic implicit data (called BiSynData) for DRRimp, which can alleviate the drawbacks of SynData. Our method is inspired by the findings that a discourse instance expressed implicitly in one language may be expressed explicitly in another. For example, Zhou and Xue\n2306\n(2012) show that the connectives in Chinese omit much more frequently than those in English with about 82.0% vs. 54.5%. Li et al. (2014a) further argue that there are about 23.3% implicit/explicit mismatchs between Chinese/English instances. As illustrated in Figure 1, a Chinese implicit instance where the connective \u00b4 is absent, is translated into an English explicit one with the connective but. Intuitively, the Chinese instance is a real implicit one which can be signaled by but. Hence, it could potentially serve as additional training data for the Chinese DRRimp, avoiding the different word distribution problem of SynData. Meanwhile, for the English explicit instance, it is very likely that removing but would not lose any information since its Chinese counterpart \u00b4 can be omitted. Therefore it could be used for the English DRRimp, alleviating the meaning shift problem of SynData.\nWe extract our BiSynData from a ChineseEnglish sentence-aligned corpus (Section 2). Then we design a multi-task neural network model to incorporate the BiSynData (Section 3). Experimental results, on both the English PDTB (Prasad et al., 2008) and Chinese CDTB (Li et al., 2014b), show that BiSynData is more effective than SynData used in previous work (Section 4). Finally, we review the related work (Section 5) and draw conclusions (Section 6)."}, {"heading": "2 BiSynData", "text": "Formally, given a Chinese-English sentence pair (Sch, Sen), we try to find an English explicit instance (Arg1en, Arg2en, Connen) in Sen1, and a Chinese implicit instance (Arg1ch, Arg2ch) in Sch, where (Arg1en, Arg2en, Connen) is the translation of (Arg1ch, Arg2ch). In most cases, discourse relations should be preserved during translating, so the connective Connen is potentially a strong indicator of the discourse relation between not only Arg1en and Arg2en, but also Arg1ch and Arg2ch. Therefore, we can construct two synthetic implicit instances labeled by Connen, denoted as \u3008(Arg1en, Arg2en), Connen\u3009 and \u3008(Arg1ch, Arg2ch), Connen\u3009, respectively. We refer to these synthetic instances as BiSynData be-\n1In our experiments, we use the pdtb-parser toolkit (Lin et al., 2014) to identify English explicit instances.\ncause they are constructed according to the bilingual implicit/explicit mismatch.\nIn our experiments, we extract our BiSynData from a combined corpus (FBIS and HongKong Law), with about 2.38 million Chinese-English sentence pairs. We generate 30,032 synthetic English instances and the same number of Chinese instances, with 80 connectives, as our BiSynData. Table 1 lists the top 10 most frequent connectives in our BiSynData, which are roughly consistent with the statistics of Chinese/English implicit/explicit mismatches in (Li et al., 2014a). According to connectives and their related relations in the PDTB, in most cases, and and also indicate the Expansion relation, if and because the Contigency relation, before the Temporal relation, and but the Comparison relation. Connectives as, when, while and since are ambiguous. For example, while can indicate the Comparison or Temporal relation. Overall, our constructed BiSynData covers all four main discourse relations defined in the PDTB.\nWith our BiSynData, we define two connective classification tasks: 1) given (Arg1en, Arg2en) to predict the connective Connen, and 2) given (Arg1ch, Arg2ch) to predict Connen. We incorporate the first task to help the English DRRimp, and the second for the Chinese DRRimp. It is worthy to note that we use English connectives themselves as classification labels rather than mapping them to relations in both tasks."}, {"heading": "3 Multi-Task Neural Network Model", "text": "We design a Multi-task Neural Network Model (denoted as MTN ), which incorporates a connective classification task on BiSynData (auxiliary task) to benefit DRRimp (main task). In general, the more related two tasks are, the more powerful a multi-task learning method will be. In the current problem, the\n2307\ntwo tasks are essentially the same, just with different output labels. Therefore, as illustrated in Figure 2, MTN shares parameters in all feature layers (L1-L3) and uses two separate classifiers in the classifier layer (L4). For each task, given an instance (Arg1, Arg2), MTN simply averages embeddings of words to represent arguments, as vArg1 and vArg2 . These two vectors are then concatenated and transformed through two non-linear hidden layers. Finally, the corresponding softmax layer is used to perform classification.\nMTN ignores the word order in arguments and uses two hidden layers to capture the interactions between two arguments. The idea behind MTN is borrowed from (Iyyer et al., 2015), where a deep averaging network achieves close to the state-ofthe-art performance on text classification. Though MTN is simple, it is easy to train and efficient on both memory and computational cost. In addition, the simplicity of MTN allows us to focus on measuring the quality of BiSynData.\nWe use the cross-entropy loss function and minibatch AdaGrad (Duchi et al., 2011) to optimize parameters. Pre-trained word embeddings are fixed. We find that fine-tuning word embeddings during training leads to severe overfitting in our experiments. Following Liu et al. (2016), we alternately use two tasks to train the model, one task per epoch. For tasks on both the PDTB and CDTB, we use the same hyper-parameters. The dimension of word embedding is 100. We set the size of L2 to 200, and L3 to 100. ReLU is used as the non-linear function. Different learning rates 0.005 and 0.001 are used in the main and auxiliary tasks, respectively. To avoid overfitting, we randomly drop out 20% words\nin each argument following Iyyer et al. (2015). All hyper-parameters are tuned on the development set."}, {"heading": "4 Experiments", "text": "We evaluate our method on both the English PDTB and Chinese CDTB data sets. We tokenize English data and segment Chinese data using the Stanford CoreNLP toolkit (Manning et al., 2014). The English/Chinese Gigaword corpus (3rd edition) is used to train the English/Chinese word embeddings via word2vec (Mikolov et al., 2013), respectively. Due to the skewed class distribution of test data (see Section 4.1), we use the macro-averaged F1 for performance evaluation."}, {"heading": "4.1 On the PDTB", "text": "Following Rutherford and Xue (2015), we perform a 4-way classification on the top-level discourse relations: Temporal (Temp), Comparison (Comp), Contigency (Cont) and Expansion (Expa). Sections 2-20 are used as training set, sections 0-1 as development set and sections 21-22 as test set. The training/test set contains 582/55 instances for Temp, 1855/145 for Comp, 3235/273 for Cont and 6673/538 for Expa. The top 20 most frequent connectives in our BiSynData are considered in the auxiliary task, with 28,013 synthetic English instances in total.\nTable 2 shows the results of MTN combining our BiSynData (denoted as MTNbi) on the PDTB.\n2308\nSTN means we train MTN with only the main task. On the macro F1, MTNbi gains an improvement of 4.17% over STN . The improvement is significant under one-tailed t-test (p<0.05). A closer look into the results shows that MTNbi performs better across all relations, on the precision, recall and F1 score, except a little drop on the recall of Cont. The reason for the recall drop of Cont is not clear. The greatest improvement is observed on Comp, up to 6.36% F1 score. The possible reason is that only while is ambiguous about Comp and Temp, while as, when and since are all ambiguous about Temp and Cont, among top 10 connectives in our BiSynData. Meanwhile the amount of labeled data for Comp is relatively small. Overall, using BiSynData under our multi-task model achieves significant improvements on the English DRRimp. We believe the reasons for the improvements are twofold: 1) the added synthetic English instances from our BiSynData can alleviate the meaning shift problem, and 2) a multi-task learning method is helpful for addressing the different word distribution problem between implicit and explicit data.\nConsidering some of the English connectives (e.g., while) are highly ambiguous, we compare our method with ones that uses only unambiguous connectives. Specifically, we first discard as, when, while and since in top 20 connectives, and get 22,999 synthetic instances. Then, we leverage these instances in two different ways: 1) using them in our multi-task model as above, and 2) using them as additional training data directly after mapping unambiguous connectives into relations. Both methods using only unambiguous connectives do not achieve better performance. One possible reason is that these synthetic instances become more unbalanced after discarding ones with ambiguous connectives.\nWe also compare MTNbi with recent systems using additional training data. Rutherford and Xue (2015) select explicit instances that are similar to the implicit ones via connective classification, to enrich the training data. Liu et al. (2016) use a multi-task model with three auxiliary tasks: 1) conn: connective classification on explicit instances, 2) exp: relation classification on the labeled explicit instances in the PDTB, and 3) rst: relation classification on the labeled RST corpus (William and Thompson,\n1988), which defines different discourse relations with that in the PDTB. The results are shown in Table 3. Although Liu et al. (2016) achieve the stateof-the-art performance (Line 5), they use two additional labeled corpora. We can find that MTNbi (Line 6) yields better results than those systems incorporating SynData (Line 1, 2 and 3), or even the labeled RST (Line 4). These results confirm that BiSynData can indeed alleviate the disadvantages of SynData effectively."}, {"heading": "4.2 On the CDTB", "text": "Four top-level relations are defined in the CDTB, including Transition (Tran), Causality (Caus), Explanation (Expl) and Coordination (Coor). We use instances in the first 50 documents as test set, second 50 documents as development set and remaining 400 documents as training set. We conduct a 3-way classification because of only 39 instances for Tran. The training/test set contains 682/95 instances for Caus, 1143/126 for Expl and 2300/347 for Coor. The top 20 most frequent connectives (excluding and)2 in our BiSynData are considered in the auxiliary task, with 13,899 synthetic Chinese instances in total. The results are shown in Table 4. Compared with STN , MTNbi raises the macro F1 from 55.44% to 58.28%. The improvement is significant under one-tailed t-test (p<0.05). Therefore, BiSynData is also helpful for the Chinese DRRimp.\nBecause of no reported results on the CDTB, we use MTN with two different auxiliary tasks as baselines: 1) exp: relation classification on the labeled\n2Including and degrades the performance slightly. A possible reason is that and can be related to both the Expl and Coor relations in the CDTB, and instances marked by and account for about half of our BiSynData.\n2309\nexplicit instances in the CDTB, including 466 instances for Caus, 201 for Expl and 974 for Coor. 2) conn: connective classification on explicit instances from the Xinhua part of the Chinese Gigaword corpus. We collect explicit instances with the top 20 most frequent Chinese connectives and sample 20,000 instances for the experiment. Both exp and conn can be considered as tasks on SynData. The results in Table 5 show that MTN incorporating BiSynData (Line 3) performs better than using SynData (Line 1 and 2), for the task on the CDTB."}, {"heading": "5 Related Work", "text": "One line of research related to DRRimp tries to take advantage of explicit discourse data. Zhou et al. (2010) predict the absent connectives based on a language model. Using these predicted connectives as features is proven to be helpful. Biran and McKeown (2013) aggregate word-pair features that are collected around the same connectives, which can effectively alleviate the feature sparsity problem. More recently, Braud and Denis (2014) and Ji et al. (2015) consider explicit data from a different domain, and use domain adaptation methods to explore the effect of them. Rutherford and Xue (2015) propose to gather weakly labeled data from explicit instances via connective classification, which are\nused as additional training data directly. Lan et al. (2013) and Liu et al. (2016) combine explicit and implicit data using multi-task learning models and gain improvements. Different from all the above work, we construct additional training data from a bilingual corpus.\nMulti-task neural networks have been successfully used for many NLP tasks. For example, Collobert et al. (2011) jointly train models for the Partof-Speech tagging, chunking, named entity recognition and semantic role labeling using convolutional network. Liu et al. (2015) successfully combine the tasks of query classification and ranking for web search using a deep multi-task neural network. Luong et al. (2016) explore multi-task sequence to sequence learning for constituency parsing, image caption generation and machine translation."}, {"heading": "6 Conclusion", "text": "In this paper, we introduce bilingually-constrained synthetic implicit data (BiSynData), which are generated based on the bilingual implicit/explicit mismatch, into implicit discourse relation recognition for the first time. On both the PDTB and CDTB, using BiSynData as the auxiliary task significantly improves the performance of the main task. We also show that BiSynData is more beneficial than the synthetic implicit data typically used in previous work. Since the lack of labeled data is a major challenge for implicit discourse relation classification, our proposed BiSynData can enrich the training data and then benefit future work."}, {"heading": "Acknowledgments", "text": "We would like to thank all the reviewers for their constructive and helpful suggestions on this paper. This work is partially supported by the Natural Science Foundation of China (Grant Nos. 61573294, 61303082, 61672440), the Ph.D. Programs Foundation of Ministry of Education of China (Grant No. 20130121110040), the Fund of Research Project of Tibet Autonomous Region of China (Grant No. Z2014A18G2-13), and the Natural Science Foundation of Fujian Province (Grant No. 2016J05161).\n2310"}], "year": 2016, "references": [{"title": "Aggregated Word Pair Features for Implicit Discourse Relation Disambiguation", "authors": ["Or Biran", "Kathleen McKeown."], "venue": "Proceedings of ACL (Volume 2: Short Papers), pages 69\u201373, Sofia, Bulgaria.", "year": 2013}, {"title": "Combining Natural and Artificial Examples to Improve Implicit Discourse Relation Identification", "authors": ["Chlo\u00e9 Braud", "Pascal Denis."], "venue": "Proceedings of COLING, pages 1694\u20131705, Dublin, Ireland.", "year": 2014}, {"title": "Comparing Word Representations for Implicit Discourse Relation Classification", "authors": ["Chlo\u00e9 Braud", "Pascal Denis."], "venue": "Proceedings of EMNLP, pages 2201\u2013 2211, Lisbon, Portugal.", "year": 2015}, {"title": "Natural Language Processing (Almost) from Scratch", "authors": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research, 12(1):2493\u2013 2537.", "year": 2011}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "authors": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "The Journal of Machine Learning Research, 12:2121\u20132159.", "year": 2011}, {"title": "Deep Unordered Composition Rivals Syntactic Methods for Text Classification", "authors": ["Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daum\u00e9 III."], "venue": "Proceedings of ACL-IJCNLP, pages 1681\u2013 1691, Beijing, China.", "year": 2015}, {"title": "One Vector is Not Enough: Entity-Augmented Distributed Semantics for Discourse Relations", "authors": ["Yangfeng Ji", "Jacob Eisenstein."], "venue": "Transactions of the Association for Computational Linguistics, volume 3, pages 329\u2013344.", "year": 2015}, {"title": "Closing the Gap: Domain Adaptation from Explicit to Implicit Discourse Relations", "authors": ["Yangfeng Ji", "Gongbo Zhang", "Jacob Eisenstein."], "venue": "Proceedings of EMNLP, pages 2219\u20132224, Lisbon, Portugal.", "year": 2015}, {"title": "Leveraging Synthetic Discourse Data via Multi-task Learning for Implicit Discourse Relation Recognition", "authors": ["Man Lan", "Yu Xu", "Zhengyu Niu."], "venue": "Proceedings of ACL, pages 476\u2013485, Sofia, Bulgaria.", "year": 2013}, {"title": "Cross-lingual Discourse Relation Analysis: A Corpus Study and a Semi-supervised Classification System", "authors": ["Junyi Jessy Li", "Marine Carpuat", "Ani Nenkova."], "venue": "Proceedings of COLING : Technical Papers, pages 577\u2013587, Dublin, Ireland.", "year": 2014}, {"title": "Building Chinese Discourse Corpus with Connective-driven Dependency Tree Structure", "authors": ["Yancui Li", "Wenhe Feng", "Jing Sun", "Fang Kong", "Guodong Zhou."], "venue": "Proceedings of EMNLP, pages 2105\u20132114, Doha, Qatar.", "year": 2014}, {"title": "Recognizing Implicit Discourse Relations in the Penn", "authors": ["Ziheng Lin", "Min-Yen Kan", "Hwee Tou Ng"], "year": 2009}, {"title": "A PDTB-styled End-to-end Discourse Parser", "authors": ["Ziheng Lin", "Hwee Tou Ng", "Min-Yen Kan."], "venue": "Natural Language Engineering, 20(02):151\u2013184.", "year": 2014}, {"title": "Representation Learning Using Multi-task Deep Neural Networks for Semantic Classification and Information Retrieval", "authors": ["Xiaodong Liu", "Jianfeng Gao", "Xiaodong He", "Li Deng", "Kevin Duh", "Ye-Yi Wang."], "venue": "Proceedings of NAACL, pages 912\u2013921, Denver, Col-", "year": 2015}, {"title": "Implicit Discourse Relation Classification via Multi-Task Neural Networks", "authors": ["Yang Liu", "Sujian Li", "Xiaodong Zhang", "Zhifang Sui."], "venue": "Proceedings of AAAI, pages 2750\u20132756, Arizona, USA.", "year": 2016}, {"title": "Using Entity Features to Classify Implicit Discourse Relations", "authors": ["Annie Louis", "Aravind Joshi", "Rashmi Prasad", "Ani Nenkova."], "venue": "Proceedings of SIGDIAL, pages 59\u201362, PA, USA.", "year": 2010}, {"title": "Multi-task Sequence to Sequence Learning", "authors": ["Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "Proceedings of ICLR, pages 1\u201310, San Juan, Puerto Rico.", "year": 2016}, {"title": "The Stanford CoreNLP Natural Language Processing Toolkit", "authors": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."], "venue": "Proceedings of ACL (System Demonstrations), pages 55\u201360, Maryland, USA.", "year": 2014}, {"title": "An Unsupervised Approach to Recognizing Discourse Relations", "authors": ["Daniel Marcu", "Abdessamad Echihabi."], "venue": "Proceedings of ACL, pages 368\u2013375, PA, USA.", "year": 2002}, {"title": "Efficient Estimation of Word Representations in Vector Space", "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "CoRR, abs/1301.3781.", "year": 2013}, {"title": "Automatic Sense Prediction for Implicit Discourse Relations in Text", "authors": ["Emily Pitler", "Annie Louis", "Ani Nenkova."], "venue": "Proceedings of ACL-IJCNLP, pages 683\u2013691, PA, USA.", "year": 2009}, {"title": "The Penn Discourse TreeBank 2.0", "authors": ["Rashmi Prasad", "Nikhil Dinesh", "Alan Lee", "Eleni Miltsakaki", "Livio Robaldo", "Aravind Joshi", "Bonnie Webber"], "venue": "In Proceedings of LREC,", "year": 2008}, {"title": "Discovering Implicit Discourse Relations Through Brown Cluster Pair Representation and Coreference Patterns", "authors": ["Attapol T. Rutherford", "Nianwen Xue."], "venue": "Proceedings of EACL, pages 645\u2013654, Gothenburg, Sweden.", "year": 2014}, {"title": "Improving the Inference of Implicit Discourse Relations via Classifying Explicit Discourse Connectives", "authors": ["Attapol Rutherford", "Nianwen Xue."], "venue": "Proceedings of NAACL, pages 799\u2013808, Denver, Colorado.", "year": 2015}, {"title": "Sentence Level Discourse Parsing Using Syntactic and Lexical Information", "authors": ["Radu Soricut", "Daniel Marcu."], "venue": "Proceedings of NAACL, pages 149\u2013156, PA, USA.", "year": 2003}, {"title": "Using Automatically Labelled Examples to Classify Rhetorical Relations: An Assessment", "authors": ["Caroline Sporleder", "Alex Lascarides."], "venue": "Natural Language Engineering, 14(3):369\u2013416.", "year": 2008}, {"title": "Rhetorical structure theory: Towards a Functional Theory of Text Organization", "authors": ["Mann William", "Sandra Thompson."], "venue": "Text, 8(3):243\u2013281.", "year": 1988}, {"title": "Shallow Convolutional Neural Network for Implicit Discourse Relation Recognition", "authors": ["Biao Zhang", "Jinsong Su", "Deyi Xiong", "Yaojie Lu", "Hong Duan", "Junfeng Yao."], "venue": "Proceedings of EMNLP, pages 2230\u2013 2235, Lisbon, Portugal.", "year": 2015}, {"title": "PDTB-style discourse annotation of Chinese text", "authors": ["Yuping Zhou", "Nianwen Xue."], "venue": "Proceedings of ACL, pages 69\u201377, Jeju Island, Korea.", "year": 2012}, {"title": "Predicting Discourse Connectives for Implicit Discourse Relation Recognition", "authors": ["Zhi-Min Zhou", "Yu Xu", "Zheng-Yu Niu", "Man Lan", "Jian Su", "Chew Lim Tan."], "venue": "Proceedings of COLING, pages 1507\u20131514, PA, USA.", "year": 2010}], "id": "SP:63ecc6f01da9ee7b9a1b2e26f8455c113f29c4e7", "authors": [{"name": "Yidong Chen", "affiliations": []}, {"name": "Changxing Wu", "affiliations": []}, {"name": "Xiaodong Shi", "affiliations": []}, {"name": "Yidong Cheng", "affiliations": []}, {"name": "Yanzhou Huang", "affiliations": []}, {"name": "Jinsong Su", "affiliations": []}], "abstractText": "To alleviate the shortage of labeled data, we propose to use bilingually-constrained synthetic implicit data for implicit discourse relation recognition. These data are extracted from a bilingual sentence-aligned corpus according to the implicit/explicit mismatch between different languages. Incorporating these data via a multi-task neural network model achieves significant improvements over baselines, on both the English PDTB and Chinese CDTB data sets.", "title": "Bilingually-constrained Synthetic Data for Implicit Discourse Relation Recognition"}