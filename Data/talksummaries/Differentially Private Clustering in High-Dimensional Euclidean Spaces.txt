26	1	While some prior works have explored the possibility of non-private clustering in the context of sparse data (Barger & Feldman, 2016), much remains unknown in the private setting.
37	1	The candidate set can be potentially applied to other problems and is of independent interest.
38	1	• We empirically compare our algorithm with the nonprivate k-means++ algorithm and four strong private baseilnes.
39	1	Across all datasets, our algorithm is competitive with k-means++ and significantly outperforms the private baselines, especially for large dimensional data.
42	1	Private Candidate Set: Our algorithm uses a novel private technique that recursively subdivides low-dimensional Euclidean spaces to construct a set of candidate points containing good centers.
49	1	As a lot of data points will be gathered around an optimal center, we can put candidate centers at each cube containing many points during partition.
51	1	From Candidate Set to Private Clustering: Our private clustering algorithm then follows the technique of local swapping on the discrete set of candidate centers, inspired by the work of Gupta et al. (2010) for k-median clustering.
68	1	For a vector v, v[i] denotes the ith entry of v. We denote by M(X) the output of an algorithm with input dataset X .
72	1	We will frequently denote the clustering loss in problem (1) on the centers z1, z2, ..., zk by L(z1, z2, ..., zk).
74	1	Problem Setup: We use the following definition of differential privacy: Definition 1 ( -Differential Privacy).
75	1	A randomized algorithmM with output range O is -differentially private if for any given set S ⊆ O and two datasets X ∈ Rd×n and Y = [X; z] for any z ∈ Rd, we have e− Pr[M(X) ∈ S] ≤ Pr[M(Y ) ∈ S] ≤ e Pr[M(X) ∈ S].
80	1	Given bounded data points x1, x2, ..., xn ∈ Rd as input, how can we efficiently output k centers z1, z2, ..., zk such that the algorithm is -differentially private and the clustering loss is at most polylog(n) × OPT + poly(d, log n, k, 1 )?
89	1	Our algorithm works by repeatedly applying a recursive discretization of the space with random shifts.
90	1	It is worth noticing that direct extension of previous methods such as (Matoušek, 2000) lead to arbitrarily bad quality.
92	1	We first describe our subroutine of private discretization, a private recursive division procedure: We start with a cube containing all the data points, privately decide whether to partition the current cubes based on number of data points they contain, and stop when there are few points in each cube.
103	1	The following theorem uses tail bounds for the exponential distribution and the union bound to upper bound the number of points in each cube not subdivided by Algorithm 1.
117	1	In contrast, our following Algorithm 2 efficiently constructs an (O(log3 n),O(kpolylog(n)))-approximate candidate set of size polynomial in n. Since Algorithm 2 only sees the private data through repeated application of Algorithm 1, we obtain the following privacy guarantee using standard composition theorems.
145	1	Private Discrete Clustering In this section, we propose a differentially private k-means algorithm in the discrete spaces.
149	1	The privacy guarantee is straightforward using the basic composition theorem over T rounds of the algorithm, and an additional exponential mechanism that selects the best one.
161	1	, xn in Rd and suppose thatA(S) is an differentially private algorithm that operates on sets of points in Rd.
167	1	[y1, y2, · · · , yn] = 1√dG[x1, x2, · · · , xn].
168	1	{u1, u2, · · · , uk} = localswap ( {yi}ni=1, C, 6T , δ ) .
169	1	Sj = {i : j = argminl ‖yi − ul‖}, j = 1, 2, · · · , k. sj = max { |Sj |+ Lap ( 24T ) , 1 } .
193	1	By a composition argument, we have the privacy of Algorithm 5.
203	1	6.2. k-Median Clustering We can also easily modify our algorithms to adapt to kmedian problem.
240	1	Across all values of k and all datasets, our algorithm is competitive with non-private k-means++ and always outperforms SuLQ k-means.
244	3	In this paper, we propose efficient algorithms for -private k-means and k-median clustering in Rd that achieves clustering loss at most O ( log3 n ) OPT + poly ( k, d, log n, 1 ) and O ( log 3 2 n ) OPT + poly ( k, d, log n, 1 ) , respectively.
245	21	We also study the scenario where the data points are s-sparse and show that the k-means clustering loss can be even smaller, namely, O ( log3 n ) OPT + poly ( k, s, log d, log n, 1 ) .
246	136	Results of this type advance the state-of-the-art approaches in the high-dimensional Euclidean spaces.
247	133	Our method of constructing candidate set can be potentially applied to other problems, which might be of independent interest more broadly.
