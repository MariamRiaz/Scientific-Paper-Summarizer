0	116	Co-reference resolution requires models to cluster mentions that refer to the same physical entities.
1	6	The models based on neural networks typically require different levels of semantic representations of input sentences.
2	21	The models usually need to calculate the representations of word spans, or mentions, given pre-trained character and wordlevel embeddings (Turian et al., 2010; Pennington et al., 2014) before predicting antecedents.
3	59	The mention-level embeddings are used to make coreference decisions, typically by scoring mention pairs and making links (Lee et al., 2017; Clark and Manning, 2016a; Wiseman et al., 2016).
5	11	Articles and conversations include more than one sentences.
6	25	Considering the accuracy and efficiency of co-reference resolution models, the encoder LSTM usually processes input sentences separately as a batch (Lee et al., 2017).
8	9	For example, pronouns are often linked to entities mentioned in other sentences, while their initial word vectors lack dependency information.
11	3	An input article or conversation can be too long for a single LSTM cell to memorize.
15	1	Borrowing the idea of an external memory module from Sukhbaatar et al. (2015), an external memory block containing syntactic and semantic information from context sentences is added to the standard LSTM model.
17	7	Experiments showed that this approach improved the performance of co-reference resolution models.
35	17	To improve the word representation learning model for better co-reference resolution performance, we propose two word representation models that learn cross-sentence dependency.
37	3	We name this method linear sentence linking (LSL).
41	9	In the second LSTM layer, the initial state of the forward LSTM of si is initialized as −→ S i = [ −→c 20; [−→s i−1;←−s i−1]] while the backward state is initialized as ←− S i = [ ←−c 20; [−→s i−1;←−s i−1]] where ci0 stands for the initial cell of the ith layer, and x stands for the final output of the LSTMs in first layer.
44	5	To collect richer knowledge from neighbor sentences, we propose a long short-term recurrent memory module and an attention mechanism to improve sentence linking.
46	4	We present the input embeddings of the j-th word in the i-th sentence with xi,j .
50	2	We design an LSTM module with cross-sentence attention for capturing cross-sentence dependency.
52	3	Considering input word xi,t in the ith sentence and all words from the previous sentence Xi−1 = [xi−1,1, xi−1,2, .
54	14	αj = ecj∑ k e ck (1) ck = fc([xi,t;ht−1;xi−1,k] T ) (2) With the attention distribution α, we can get a vector summarizing related information from si−1, vi−1 = ∑ j αj · xi−1,j (3) The model decides if it needs to pay more attention on the current input or cross-sentence information with a context gate.
55	7	gt = σ(fg([xi,t;ht−1; vi−1] T )) (4) x̂i,t = gt · xi,t + (1− gt) · vi−1 (5) σ(·) stands for the Sigmoid function.
56	17	The word representation of the target word is calculated as hi,t = flstm(x̂i,t, hi,t−1, ci,t−1) (6) where flstm stands for standard LSTM update described in section 3.2.1.
73	15	Comparing with the baseline model that achieved 67.2% F1 score, the ASL model improved the performance by 0.6% and achieved 67.8% average F1.
77	19	Examples for comparing the performance of the ASL model and the baseline are shown in Table 2.
78	17	Each example contains two continuous sentences with co-references distritubed in different sentences.
80	34	Spans in green are ASL predictions, and spans in red are baseline predictions.
81	48	A prediction on “-” means that no mention is predicted as a co-reference.
82	29	Table 2 shows that the baseline model, which does not consider cross-sentence dependency, has difficulty in learning the semantics of pronouns whose co-references are not in the same sentence.
84	117	In the first example, “it” is not semantically similar with “SMS” in GloVe without any context, and in this case, “it” and “SMS” are in different sentences.
86	24	This difficulty makes the co-reference resolution model either prediction a wrong antecedent mention, or cannot find any co-reference.
88	10	With the proposed context gate, ASL takes knowledge from context sentences if local inputs are not informative enough.
89	16	Based on word represents enhanced with cross-sentence dependency, the co-reference scoring model can make better predictions.
