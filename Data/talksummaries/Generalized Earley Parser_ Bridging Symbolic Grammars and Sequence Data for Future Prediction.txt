3	12	Such tasks often exhibit non-Markovian and compositional properties, which should be captured by a top-down prediction algorithm.
5	20	Context-free grammars are natural choices to model such reasoning processes, and it is one step closer to Turing machines than Markov models (e.g., Hidden Markov Models) in the Chomsky hierarchy.
6	9	However, it is not clear how to directly use symbolic gram- ... ... open microwave take food put into microwave Parse Tree Future prediction ...Sequenceinput data Classi er raw output Final output Grammar Generalized Earley Parser microwave food Figure 1.
9	52	Future predictions are then made based on the grammar.
10	24	mars to parse and label sequence data.
11	33	Traditional grammar parsers take symbolic sentences as inputs instead of noisy sequence data like videos or audios.
12	25	The data has to be i) segmented and ii) labeled to apply existing grammar parsers to.
13	80	One naive solution is to first segment and label the data using a classifier and thus generating a label sentence.
15	16	But this is apparently non-optimal, since the grammar rules are not considered in the classification process.
16	13	It may not even be possible to parse this label sentence, because they are very often grammatically incorrect.
18	37	Specifically, we propose a generalized Earley parser based on the Earley parser (Earley, 1970).
19	36	The algorithm finds the optimal segmentation and label sentence according to both a symbolic grammar and a classifier output of probabilities of labels for each frame as shown in Figure 1.
21	10	The difficulty of achieving this optimality lies in the joint optimization of both the grammar structure and the parsing likelihood of the output label sentence.
22	24	For example, an expectation-maximization-type algorithm will not work well since i) there is no guarantee for optimality, and ii) any grammatically incorrect sentence has a grammar prior of probability 0.
23	9	The algorithm can easily get stuck in local minimums and fail to find a grammatically correct solution.
26	58	Specifically, a heuristic search is performed on the prefix tree expanded according to the grammar, where the path from the root to a node represents a partial sentence (prefix).
27	25	By carefully defining the heuristic as a prefix probability computed based on the classifier output, we can efficiently search through the tree to find the optimal label sentence.
28	18	The generalized Earley parser has four major advantages.
29	22	i) The inference process highly integrates a high-level grammar with an underlying classifier; the grammar gives guidance for segmenting and labeling the sequence data.
30	12	ii) It can be applied to any classifier outputs.
31	14	iii) It generates a grammar parse tree for data sequence that is highly explainable.
35	11	Comparisons show that our method significantly outperforms state-of-the-art methods on future activity prediction.
36	18	The second dataset Watch-n-Patch (Wu et al., 2015) is designed for “action patching”, which includes daily activities that have action forgotten by people.
37	12	Experiments on the second dataset show the robustness of our method on noisy data.
38	12	Both experiments show that the generalized Earley parser performs particularly well on prediction tasks, primarily due to its non-Markovian property.
40	13	•We design a parsing algorithm for symbolic context-free grammars.
41	11	It directly operates on sequence data to obtain the optimal segmentation and labels.
43	25	•We formulate an objective for future prediction for both grammar induction and classifier training.
45	8	In formal language theory, a context-free grammar is a type of formal grammar, which contains a set of production rules that describe all possible sentences in a given formal language.
48	9	Σ is a finite set of terminal symbols that represent actual words in a language, which cannot be further expanded.
