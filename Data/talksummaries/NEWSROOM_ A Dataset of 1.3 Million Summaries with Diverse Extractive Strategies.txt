3	21	Access to large-scale high-quality data is an essential prerequisite for making substantial progress in summarization.
4	36	In this paper, we present NEWSROOM, a dataset with 1.3 million news articles and human-written summaries.
17	32	Our analysis focuses on a key dimension, extractivenss and abstractiveness: extractive summaries frequently borrow words and phrases from their source text, while abstractive summaries describe the contents of articles primarily using new language.
18	30	We develop measures designed to quantify extractiveness and use these measures to subdivide the data into extractive, mixed, and abstractive subsets, as shown in Figure 1, displaying the broad set of summarization techniques practiced by different publishers.
19	34	Finally, we analyze the performance of three summarization models as baselines for NEWSROOM to better understand the challenges the dataset poses.
27	43	Datasets produced for the Document Understanding Conference (DUC)1 are small, high-quality datasets developed to evaluate summarization systems (Harman and Over, 2004; Dang, 2006).
32	32	DUC summaries are often used in conjunction with larger training datasets, including Gigaword (Rush et al., 2015; Chopra et al., 2016), CNN / Daily Mail (Nallapati et al., 2017; Paulus et al., 2017; See et al., 2017), or Daily Mail alone (Nallapati et al., 2016b; Cheng and Lapata, 2016).
37	44	These systems are trained on Gigaword to recreate headlines given the first sentence of an article.
38	108	When used this way, Gigaword’s simulated summaries are shorter than most natural summary text.
41	19	It consists of carefully curated articles from a single source, The New York Times.
42	48	The corpus contains several hundred thousand articles written between 1987–2007 that have paired summaries.
43	20	The summaries were written for the corpus by library scientists, rather than at the time of publication.
49	43	This different usage makes comparisons between systems using these data challenging.
61	21	The publisher sites we crawled were selected using a combination of Alexa.com top overall sites, as well as Alexa’s top news sites.3 We supplemented the lists with older lists published by Google of the highest-traffic sites on the Web.4 We excluded sites such as Reddit that primarily aggregate rather than produce content, as well as publisher sites that proved to have few or no articles with summary metadata available, or have articles primarily in languages other than English.
95	34	We examine summarization strategies using three measures that capture the degree of text overlap between the summary and article, and the rate of compression of the information conveyed.
109	21	Extractive Fragment Density The density measure quantifies how well the word sequence of a summary can be described as a series of extractions.
120	24	Publication Diversity Each NEWSROOM publication shows a unique distribution of summaries mixing extractive and abstractive strategies in varying amounts.
131	30	CNN / Daily Mail shows higher coverage and density than all other datasets and publishers in our data.
133	38	We train and evaluate several summarization systems to understand the challenges of NEWSROOM and its usefulness for training systems.
148	44	We evaluate three instances of this model by varying the training data: (1) Pointer-C: trained on the CNN / Daily Mail dataset; (2) Pointer-N: trained on the NEWSROOM dataset; and (3) Pointer-S: trained on a random subset of NEWSROOM training data the same size as the CNN / Daily Mail training.
176	35	While human evaluation is still uncommon in summarization work, developing a benchmark dataset presents an opportunity for developing an accompanying protocol for human evaluation.
181	49	Evaluation was performed on 60 summaries, 20 from each extractive NEWSROOM subset.
184	48	Table 5 shows the mean score given to each system under each of the four dimensions, as well as the mean overall score (rightmost column).
186	49	However, the extractive oracle designed to maximize n-gram based evaluation performed worse than the majority of sys- tems under human evaluation.
187	20	While the fully abstractive Abs-N model performed very poorly under automatic evaluation, it fared slightly better when scored by humans.
189	27	TextRank generates full sentences extracted from the article, and raters preferred TextRank primarily for its fluency and coherence.
190	39	The pointer-generator models do not have this advantage, and raters did not find the pointer-generator models to be as syntactically sound as TextRank.
192	22	We present NEWSROOM, a dataset of articles and their summaries written in the newsrooms of online publications.
195	61	We show that the dataset’s diversity of summaries presents a new challenge to summarization systems.
196	32	Finally, we find that using NEWSROOM to train an existing state-of-art mixed-strategy summarization model results in performance improvements on out-ofdomain data.
