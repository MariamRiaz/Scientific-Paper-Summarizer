3	37	We aim at closing this gap for morphological surface segmentation, the task of splitting a word into the surface forms of its smallest meaning-bearing units, its morphemes.
5	38	To illustrate how segmentation helps understanding unknown multiplemorpheme words, consider an example in this paper’s language of writing: even if the word unconditionally did not appear in a given training corpus, its meaning could still be derived from a combination of its morphs un, condition, al and ly.
11	12	We experiment on four polysynthetic Mexican languages: Mexicanero, Nahuatl, Wixarika and Yorem Nokki (details in §2).
12	20	The datasets we use are, as far as we know, the first computer-readable datasets annotated for morphological segmentation in those languages.
19	10	Polysynthetic languages are morphologically rich languages which are highly synthetic, i.e., single words can be composed of many individual morphemes.
20	62	In extreme cases, entire sentences consist of only one single token, whereupon “every argument of a predicate must be expressed by morphology on the word that contains that assigner” (Baker, 2006).
24	106	Mexicanero is a Western Peripheral Nahuatl variant, spoken in the Mexican state of Durango by approximately one thousand people.
25	14	This dialect is isolated from the rest of the other branches and has a strong process of Spanish stem incorporation, while also having borrowed some suffixes from that language (Vanhove et al., 2012).
37	10	An example for a word in the language is: ne|p+|ti|kuye|kai – I was sick Like Nahuatl, it has an SOV syntax, with heavy agglutination on the verb.
63	58	The first part of our model is a bidirectional recurrent neural network (RNN) which encodes the input sequence, i.e., the sequence of characters of a given word w = w1, w2, .
69	14	, ct−1, w) is computed using an attention mechanism and an output softmax layer over Σ ∪ S. A more detailed description of the general attention-based encoder-decoder architecture can be found in the original paper by Bahdanau et al. (2015).
80	32	We denote models trained with multi-task training using unlabeled corpus data as MTT-U and models trained with multi-task training using random strings as MTT-R.
81	53	A second option to make use of unlabeled data or random strings is to extend the available training data with new examples made from those.
82	13	The main question to answer here is how to include the new data into the existing datasets.
94	33	Training on onemokokowaya 7→ onemokokowaya will make the model learn not to segment words which consist of the morphemes o, ne,mo, kokowa, ya, which should ultimately hurt performance.
97	25	Using random strings, the difference between the multi-task and the data augmentation approaches is less obvious: Real morphemes should appear rarely enough in the created random character sequences to avoid the negative effect which we expect for corpus words.
100	22	For the multi-task training and data augmentation using unlabeled data, we use (unsegmented) words from a parallel corpus collected by Gutierrez-Vasques et al. (2016) for Nahuatl and the closely related Mexicanero.
121	17	Optimizing the amount of auxiliary task data.
122	52	The performance of our neural segmentation model in dependence of the amount of auxiliary task training data can be seen in Figure 1.
123	13	As a general tendency across all languages, adding more data seems better, particularly for the autoencoding task with random strings.
130	22	In the case of random strings, again, adding more training data seems to help more.
139	61	While we use this metric because it is common for segmentation tasks, it is not ideal for our models since those are not guaranteed to preserve the input character sequence.
140	33	We handle this problem as follows: In order to compare borders, we identify them by the position of their preceding letter, i.e., if in both the model’s guess and the gold solution a segment border appears after the second character, it counts as correct.
143	32	The test results also give an answer to our first research question: The neural model S2S performs on par with CRF, the strongest baseline, for all languages but Nahuatl.
152	14	Assuming that the effect of training a language model using unlabeled data and erroneously learning to not segment words are working against each other for MTT-U, this might explain why MTT-U is best for Mexicanero and the gap between MTT-U and MTT-R is smaller for Nahuatl than for Yorem Nokki and Wixarika.
157	38	While CRF is overall the strongest baseline for our considered languages, our methods outperform it by up to 0.0214 accuracy or 0.0147 F1 for Mexicanero, 0.0322 accuracy or 0.0229 F1 for Wixarika and 0.0505 accuracy or 0.0340 F1 for Yorem Nokki.
158	26	This shows the effectiveness of our fortified neural models for minimal-resource morphological segmentation.
159	30	We now want to investigate the performance of one single model trained on all languages at once.
160	139	This is done in analogy to the multi-task training described in §5.1.
161	37	We treat segmentation in each language as a separate task and train an attentionbased encoder-decoder model on maximizing the joint log-likelihood: L(θ)= ∑ Li∈L ∑ (w,c)∈TLi log pθ (c | e(w)) (4) TLi denotes the segmentation training data in language Li and L is the set of our languages.
