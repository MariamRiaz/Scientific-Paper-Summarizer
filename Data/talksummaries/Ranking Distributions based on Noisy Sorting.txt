6	40	Our model is inspired by the idea of a data-generating process in the form of a noisy sorting procedure (Biernacki & Jacques, 2013), that is, the idea that a ranking is produced as the result of a sorting process, in which comparisons are not deterministic but dependant on chance.
12	35	In an experimental study, we assess the performance of our models in terms of goodness of fit on a large number of real-world data sets.
14	32	In the next section, we introduce notation and recall the basic families of probability distributions on rankings.
21	17	With each ranking ⇡, we associate an ordering ⇡ 1, where ⇡ 1(j) is the index of the item on position j.
23	13	For example, ⇡ = [2, 3, 1], ⇡ = (3, 1, 2), as well as the function ⇡ defined by ⇡(1) = 2,⇡(2) = 1,⇡(3) = 1, all denote the ranking in which o3 is at the top, o1 in the middle, and o2 on the last position.
62	13	The former corresponds to the “correct” ranking, i.e., the mode of the distribution, and p 2 [0.5, 1] is the noise parameter that controls the peakedness of the distribution.
63	30	More specifically, the following assumption is made: A sorting algorithm (insertion sort) is run on an initial ordering ⇡, and whenever two items oi and oj are compared, the “right” outcome (consistent with ⌧ ) is produced with probability p (hence the “wrong” outcome with probability 1 p).
67	25	(4) This model can also be written as follows: P( |P) = 1 C 0(P) X ⇡2SK P( |⇡,P) , = 1 C 0(P) X ⇡2SK KY i=1 Y j 6=i pi,j d ,⇡i,j , (5) where P is a K ⇥K matrix P = [pi,j ]1i,jK with entries pi,j = pi,j(⌧, p) = ⇢ p if ⌧(oi) < ⌧(oj) 1 p if ⌧(oi) > ⌧(oj) .
69	27	Moreover, for rankings ,⇡ 2 SK , D ,⇡ = ⇥ d ,⇡i,j ⇤ 1i,jK is a binary matrix with entries d ,⇡i,j = 1 if the sorting algorithm, given ⇡ as initial ordering and producing as output, has compared oi to oj with a win for oi, and to 0 otherwise.
73	20	Our model is a modification of (5), which looks very similar at first sight: Instead of averaging out the influence of the initial ranking ⇡ in an additive way, by aggregating the probabilities P( |⇡, ⌧, p) with an arithmetic mean, we apply the product as an aggregation function: PA( | ⌧, p) / Y ⇡2SK PA( |⇡, ⌧, p) , where A is the underlying sorting algorithm.
77	11	There are different motivations for the above modification.
80	14	The product is a conjunctive aggregation function (Grabisch et al., 2009), and combining probabilities in a conjunctive way is in agreement with standard (deterministic) sorting, where the “correct” output ordering is obtained regardless of the initial ordering ⇡, that is, as a result for all initial orderings ⇡.
81	9	Therefore, we call our model the Conjunctive Noisy Sorting (CNS) model.
100	14	Note that the key quantity in the model is D , which we shall compute in a closed form when insertion sort is used as sorting algorithm, and characterize recursively when quick sort is used.
143	12	It is easy to see that object oi and oj are not compared if there is a third object ok, which is between oj and oi with respect to id , and also between oj and oi in ⇡; Figure 1 illustrates such a configuration of objects.
162	11	In the second case, either oi or oj is chosen as pivot, in which case they will be compared.
166	10	Given a set of observations { 1, .
168	25	We tackle the problem with simple hill-climbing search for ⌧ in the discrete space SK , initialized with the Borda ranking (i.e., sorting items according to their average rank in the data).
169	11	The neighborhood of an ordering is defined as the set of all orderings that can be obtained by a swap of two adjacent items.
170	12	For a fixed ⌧ , the optimization problem (13) reduces to a simple one-dimensional problem: max p nX `=1 2 4 X ⌧(i)<⌧(j) d `,⌧i,j log p+ X ⌧(i)>⌧(j) d `,⌧i,j log(1 p) 3 5 n logC(P⌧ ) s. t. p 2 [0.5, 1] (14) This problem is convex (the distribution belongs to the exponential family) and can be solved numerically, for example by means of the golden section method.
171	14	In each iteration of the algorithm, the best candidate solution (⌧, p) in the neighborhood of the current best solution is adopted, and the search stops if no improvement is possible anymore.
172	13	The GCNS model (11) is parametrized by P. Here, the maximum likelihood (ML) principle cannot be applied directly, because the normalizing factor C(P) in (12) cannot be written in a closed form in terms of the model parameters.
173	21	Therefore, we opt for using the generalized iterative scaling (GIS) procedure (Darroch & Ratcliff, 1972), an iterative method for estimating the probabilities in a log-linear model.
181	12	According to (Darroch & Ratcliff, 1972), P0 in (16) is the (unconstrained) ML estimate for P. In our case, however, the constraints pi,j + pj,i = 1 in (15) need to be taken into account.
208	11	Our models have an intuitive interpretation, exhibit convenient mathematical properties, and seem to fit empirical data very well.
209	16	For two sorting algorithms, insertion sort and quick sort, we developed parameter estimation techniques based on a closed-form expression of the likelihood function for the former, and a recursive characterization of it for the latter.
210	42	Experimentally, insertion sort leads to better performance.
211	15	In future work, we plan to consider other sorting algorithms, such as merge sort and heap sort.
212	39	Another direction worth to investigate is the analysis of algebraic properties of our models using tools from computational algebraic geometry (Geiger et al., 2006); such properties may simplify the handling of the model and help to further improve efficiency of parameter estimation.
213	28	Last but not least, we are also interested in using the model for other machine learning problems, in which distributions on rankings are needed, such as learning to rank (Ailon et al., 2005; Ailon, 2008; Cao et al., 2007) and multi-armed bandits (Busa-Fekete & H¨ullermeier, 2014; Sz¨or´enyi et al., 2015).
