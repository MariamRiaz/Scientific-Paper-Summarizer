0	11	The conditional gradient algorithm (Frank & Wolfe, 1956) (also known as Frank-Wolfe) is historically the earliest algorithm for solving general constrained convex optimization problems.
3	19	Many different algorithm variants, such as the composite variant (Harchaoui et al., 2015), the online and stochastic variants (Hazan & Kale, 2012) (Hazan, 2016) (Hazan & Luo, 2016), faster variants over special types of convex domains, i.e. spectrahedron (Garber, 2016) and polytope (Garber & Hazan, 2016), have also been proposed.
5	28	In comparison to this situation is the popularity of the variants of its gradient descent and dual averaging counterparts—distributed online gradient descent (D-OGD) (Ram et al., 2010) (Yan et al., 2013) and distributed online dual averaging (D-ODA) (Duchi et al., 2012) (Hosseini et al., 2013) (Lee et al., 2015), which have been successfully applied in handling large-scale streaming data in decentralized computational architectures (e.g., sensor networks and smart phones).
6	16	Despite the success of these algorithms, the projection operation required in them still limits their further applicability in many settings of practical interests.
7	33	For example, in matrix learning (Dudı́k et al., 2012), multiclass classification (Hazan & Luo, 2016) and many other related problems, where the convex domain is the set of all matrices with bounded nuclear norm, the projection operation amounts to computing the full singular value decomposition (SVD) of a matrix, too expensive an operation that does not meet the need of locally light computation in distributed online learning.
11	21	To fill this gap, in this work, we present the distributed online conditional gradient (D-OCG) algorithm as the desired variant.
22	10	Then the adversary replies each learner’s decision with a convex loss function ft,i : K → R and each learner suffers the loss ft,i(xi(t)).
24	12	The goal of the learners is to generate a sequence of decision points xi(t), i ∈ V so that the regret with respect to each learner i regarding any fixed decision x ∈ K in hindsight, RT (xi,x) = n∑ j=1 T∑ t=1 (ft,j(xi(t))− ft,j(x)), is sublinear in T .
31	11	The communication between learners is modeled by a doubly stochastic symmetric matrix P , which satisfies (1) Pij > 0 only if (i, j) ∈ E (i 6= j) or i = j; (2) ∀ i ∈ V , ∑n j=1 Pij = ∑ j∈N(i) Pij = 1 and ∀ j ∈ V ,∑n i=1 Pij = ∑ i∈N(j) Pij = 1.
33	8	The standard online conditional gradient algorithm (Hazan & Kale, 2012) (Hazan, 2016) eschews the computational expensive projection operation by using a simple linear optimization step instead and is thus much more efficient for many computationally intensive tasks.
35	9	In this section, we first present the proposed distributed online conditional gradient algorithm, and then give the theoretical analysis of its regret bound.
36	9	Algorithm 3 Distributed Online Conditional Gradient (DOCG) 1: Input: convex set K, maximum round number T , 2: parameters {ηi} and{σt,i}, ∀ i ∈ V 3: Initialize: xi(1) ∈ K, zi(1) = 0, ∀ i ∈ V 4: for t = 1, · · · , T do 5: The adversary reveals ft,i, ∀ i ∈ V 6: Compute subgradients gi(t) ∈ ∂ft,i(xi(t)), ∀ i ∈ V 7: for Each Learner i ∈ V do 8: Ft,i(x) = ηi 〈zi(t) , x〉+ ‖x− x1(1)‖2 9: vi(t) = arg minx∈K {〈∇Ft,i(xi(t)) , x〉} 10: xi(t+ 1) = xi(t) + σt,i(vi(t)− xi(t)) 11: zi(t+ 1) = ∑ j∈N(i) pijzj(t)+gi(t) 12: end for 13: end for
38	22	(1) The regret bound for D-OCG is in the similar order O(T 3/4) to that of its centralized variant OCG (Hazan, 2016).
39	11	(2) Since the connectivity of a graph is captured by its spectral gap value 1−σ2(P ) (Duchi et al., 2012) (Colin et al., 2016): the better the connectivity of a graph is, the larger the spectral gap value will be, it is easy to verify that this theorem captures the intuition that the D-OCG’s regret bound will be larger on larger graphs (the regret bound will be larger when the node size n is larger for all T ) and will be smaller on ”well-connected” graphs than on ”poorly connected” graphs (the regret bound will be smaller when the spectral gap value is larger for certain large T ).
115	17	To evaluate the performance of the proposed D-OCG algorithm, we conduct simulation experiments for a popular machine learning problem: multiclass classification.
118	7	Then the adversary reveals the true class labels yi(t) and each learner i suffers a convex multivariate logistic loss ft,i(Xi(t)) = log ( 1+ ∑ ` 6=yi(t) exp(xT` ei(t)− xTyi(t)ei(t)) ) .
122	18	Network Topology To investigate the influence of network topology, we conduct our experiments on three types of graphs, which represent different levels of connectivity.
123	7	This represents the highest level of connectivity in our experiments: all nodes are connected to each other.
124	15	This represents the lowest level of connectivity in our experiments: each node has only two immediate neighbors.
126	6	We tune the parameters k = 4 and p = 0.3 to achieve an intermediate level of connectivity in our experiments.
127	23	Compared Algorithms To evaluate the performance benefit of D-OCG over its counterparts with projection operation, we compare it with two classic algorithms: DOGD (Yan et al., 2013) and D-ODA (Hosseini et al., 2013).
128	28	To verify that performing online conditional gradient in the distributed setting does not lose much quality compared with that in the centralized setting, we also compare DOCG with OCG, i.e. D-OCG with 1 node.
133	26	From the results shown in Figure 1, we can clearly observe that DOCG is significantly faster than both D-OGD and D-ODA, which illustrates the necessity and usefulness of using conditional gradient in distributed online learning.
136	34	First, the average losses decrease more slowly on larger graphs than on smaller graphs, which nicely confirms our theoretical results.
138	7	We finally test the influence of network topology on the algorithm’s performance.
140	53	As shown in Figure 2(b), graphs with better connectivity lead to slightly faster convergence, which illustrates good agreement of empirical results with our theoretical predictions.
141	9	In this paper, we propose the distributed online conditional gradient algorithm for projection-free distributed online learning in networks.
142	27	We give detailed analysis of the regret bound for the proposed algorithm, which depends on both the network size and the network topology.
143	10	We evaluate the efficacy of the proposed algorithm on two real-world datasets for a multiclass classification task and find that it runs significantly faster than the counterpart algorithms with projection.
144	32	The theoretical results regarding the regret bound for different graphs have also been verified.
