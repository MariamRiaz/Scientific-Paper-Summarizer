0	51	Sequence-to-sequence (seq2seq) models (Cho et al., 2014; Sutskever et al., 2014) for grammatical error correction (GEC) have drawn growing attention (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017; Schmaltz et al., 2017; Sakaguchi et al., 2017; Chollampatt and Ng, 2018) in recent years.
3	85	Limited by the size of training data, the models with millions of parameters may not be well generalized.
5	31	Second, the seq2seq models usually cannot perfectly correct a sentence with many grammatical errors through single-round seq2seq inference, as shown in Figure 1(b) and 1(c), because some errors in a sentence may make the context strange, which confuses the models to correct other errors.
6	25	To address the above-mentioned limitations in model learning and inference, this paper proposes a novel fluency boost learning and inference mechanism, illustrated in Figure 2.
19	31	The fluency boost sentence pairs will be used as training instances in subsequent training epochs, which helps expand the training set and accordingly benefits model learning; (b) fluency boost inference allows an error correction model to correct a sentence incrementally through multi-round seq2seq inference until its fluency score stops increasing.
21	54	Specifically, we call the generated errorcorrected sentence pairs fluency boost sentence pairs because the sentence in the target side always improves fluency over that in the source side.
32	21	For model inference, an output sequence xo = (xo1, · · · , xoi , · · · , xoL) is selected through beam search, which maximizes the following equation: P (xo|xr) = L∏ i=1 P (xoi |xr,xo<i;Θcrt) (2)
41	27	Back-boost learning borrows the idea from back translation (Sennrich et al., 2016) in NMT, referring to training a backward model (we call it error generation model, as opposed to error correction model) that is used to convert a fluent sentence to a less fluent sentence with errors.
48	20	Then, disfluency candidates of a correct sentence xc can be derived: Dback(xc) = {xok |xok ∈ Yn(xc;Θgen) ∧ f(xc) f(xok) ≥ σ} (5) Algorithm 1 Back-boost learning 1: Train error generation model Θgen with S̃∗; 2: for each sentence pair (xr,xc) ∈ S do 3: Compute Dback(xc) according to Eq (5); 4: end for 5: for each training epoch t do 6: S ′ ← ∅; 7: Derive a subset St by randomly sampling |S∗| ele- ments from S; 8: for each (xr,xc) ∈ St do 9: Establish a fluency boost pair (x′,xc) by ran- domly sampling x′ ∈ Dback(xc); 10: S ′ ← S ′ ∪ {(x′,xc)}; 11: end for 12: Update error correction model Θcrt with S∗ ∪ S ′; 13: end for where Dback(xc) denotes the disfluency candidate set for xc in back-boost learning.
57	28	For self-boost learning, given an error corrected pair (xr,xc), an error correction model Θcrt first predicts n-best outputs xo1 , · · · ,xon for the raw sentence xr.
62	22	As introduced above, back- and self-boost learning generate disfluency candidates from different perspectives to create more fluency boost sentence pairs to benefit training the error correction model.
63	41	Intuitively, the more diverse disfluency candidates generated, the more helpful for training an error correction model.
65	40	Algorithm 3 Dual-boost learning 1: for each (xr,xc) ∈ S do 2: Ddual(xc)← ∅; 3: end for 4: S ′ ← ∅; S ′′ ← ∅; 5: for each training epoch t do 6: Update error correction model Θcrt with S∗ ∪ S ′; 7: Update error generation model Θgen with S̃∗ ∪ S ′′; 8: S ′ ← ∅; S ′′ ← ∅; 9: Derive a subset St by randomly sampling |S∗| ele- ments from S; 10: for each (xr,xc) ∈ St do 11: Update Ddual(xc) according to Eq (7); 12: Establish a fluency boost pair (x′,xc) by ran- domly sampling x′ ∈ Ddual(xc); 13: S ′ ← S ′ ∪ {(x′,xc)}; 14: Establish a reversed fluency boost pair (xc,x′′) by randomly sampling x′′ ∈ Ddual(xc); 15: S ′′ ← S ′′ ∪ {(xc,x′′)}; 16: end for 17: end for As Figure 3(c) shows, disfluency candidates in dual-boost learning are from both the error generation model and the error correction model : Ddual(xc) = Ddual(xc) ∪ {xok |xok ∈ Yn(xr;Θcrt) ∪ Yn(xc;Θgen) ∧ f(xc) f(xok) ≥ σ} (7) Moreover, the error correction model and the error generation model are dual and both of them are dynamically updated, which improves each other: the disfluency candidates produced by error generation model can benefit training the error correction model, while the disfluency candidates created by error correction model can be used as training data for the error generation model.
70	21	As we discuss in Section 1, some sentences with multiple grammatical errors usually cannot be perfectly corrected through normal seq2seq inference which does only single-round inference.
72	65	The characteristic allows us to edit a sentence more than once through multi-round model inference, which motivates our fluency boost inference.
74	101	Specifically, an error correction seq2seq model first takes a raw sentence xr as an input and outputs a hypothesis xo1 .
77	116	As previous studies (Ji et al., 2017), we use the public Lang-8 Corpus (Mizumoto et al., 2011; Tajiri et al., 2012), Cambridge Learner Corpus (CLC) (Nicholls, 2003) and NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) as our original error-corrected training data.
78	55	Table 1 shows the stats of the datasets.
80	51	The native data we use for fluency boost learning is English Wikipedia that contains 61,677,453 sentences.
81	26	We use CoNLL-2014 shared task dataset with original annotations (Ng et al., 2014), which contains 1,312 sentences, as our main test set for evaluation.
84	67	We set up experiments in order to answer the following questions: • Whether is fluency boost learning mechanism helpful for training the error correction model, and which of the strategies (back-boost, selfboost, dual-boost) is the most effective?
89	21	The vocabulary sizes of the encoder and decoder are 100,000 and 50,000 respectively.
90	37	The models’ parameters are uniformly initialized in [-0.1,0.1].
91	73	We train the models with an Adam optimizer with a learning rate of 0.0001 up to 40 epochs with batch size = 128.
93	26	For fluency boost learning, we generate disfluency candidates from 10-best outputs.
98	216	Table 2 compares the performance of seq2seq error correction models with different learning and inference methods.
99	146	By comparing by row, one can observe that our fluency boost learning approaches improve the performance over normal seq2seq learning, especially on the recall metric, since the fluency boost learning approaches generate a variety of grammatically incorrect sentences, allowing the error correction model to learn to correct much more sentences than the conventional learning strategy.
100	97	Among the proposed three fluency boost learning strategies, dual-boost achieves the best result in most cases because it produces more diverse incorrect sentences (average |Ddual| ≈ 9.43) than either back-boost (avg |Dback| ≈ 1.90) or self-boost learning (avg |Dself | ≈ 8.10).
113	43	In contrast, fluency boost inference additionally edits 23 sentences during the second round inference, improving F0.5 from 52.59 to 52.72.
114	45	Now, we answer the last question raised in Section 5.2 by testing if our approaches achieve the stateof-the-art result.
