{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rV5hbLb4xGLA",
        "outputId": "7e15f611-6baf-4898-a366-2fcc5e21a8f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.22.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.9.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import sklearn.model_selection as ms\n",
        "import sklearn.preprocessing as p\n",
        "\n",
        "import tensorflow as tf\n",
        "import transformers as trfs\n",
        "import os"
      ],
      "metadata": {
        "id": "IgypwEi9xqqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device = tf.test.gpu_device_name()\n",
        "if device != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xud1CmDqxwR_",
        "outputId": "b795a9aa-0353-4fdf-a75d-f882782e0696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QY_7ibn9xyXq",
        "outputId": "712ea456-85af-4268-fc10-eef8804235e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Sep 24 16:04:15 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    36W / 300W |   4709MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Max length of encoded string(including special tokens such as [CLS] and [SEP]):\n",
        "MAX_SEQUENCE_LENGTH = 512\n",
        "\n",
        "# BERT model with lowercase chars only:\n",
        "PRETRAINED_MODEL_NAME = 'bert-base-uncased'\n",
        "\n",
        "tokenizer = trfs.DistilBertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
        "\n",
        "def batch_encode(X, tokenizer):\n",
        "    return tokenizer.batch_encode_plus(\n",
        "    X,\n",
        "    max_length=MAX_SEQUENCE_LENGTH, # set the length of the sequences\n",
        "    add_special_tokens=True, # add [CLS] and [SEP] tokens\n",
        "    return_attention_mask=True,\n",
        "    return_token_type_ids=True, # not needed for this type of ML task\n",
        "    pad_to_max_length=True,\n",
        "    truncation = True, # add 0 pad tokens to the sequences less than max_length\n",
        "    return_tensors='tf'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_e97cppx74S",
        "outputId": "767a5f5b-36f3-4dcb-b4e1-32ee58339ec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
            "The class this function is called from is 'DistilBertTokenizer'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = tf.keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/model/sum2/summodel\")\n",
        "\n",
        "# Check its architecture\n",
        "new_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkR0-QshyAtl",
        "outputId": "ed6fb183-089b-4ea0-e76b-9ea773ee9224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " bert (Custom>TFBertMainLayer)  {'pooler_output': (  109482240   ['input_ids[0][0]',              \n",
            "                                None, 768),                       'attention_mask[0][0]',         \n",
            "                                 'last_hidden_state               'input_1[0][0]']                \n",
            "                                ': (None, 512, 768)                                               \n",
            "                                }                                                                 \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 2)            1538        ['bert[0][1]']                   \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,483,778\n",
            "Trainable params: 1,538\n",
            "Non-trainable params: 109,482,240\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cdir =\"/content/drive/MyDrive/Colab Notebooks/data/testdata\""
      ],
      "metadata": {
        "id": "vkZwpE6Dx2JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTPutgvLT0Ye",
        "outputId": "27beee0c-7644-4d01-ce97-168155420e44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.2.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score) (3.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.21.6)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->rouge-score) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk->rouge-score) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->rouge-score) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->rouge-score) (7.1.2)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24955 sha256=b8c5146de7ccdc4279a0bfe9b901c3dad910f03e07ef71c1e8fe96fb21435eb2\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/ac/6b/38096e3c5bf1dc87911e3585875e21a3ac610348e740409c76\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "from statistics import mean\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1'])\n",
        "results = {'precision': [], 'recall': [], 'fmeasure': []}\n",
        "for subdir, dirs, files in os.walk(cdir):\n",
        "  for file in files:\n",
        "    f = os.path.join(subdir, file)\n",
        "    print(f)\n",
        "    df = pd.read_csv(f, delimiter=',', encoding= \"utf-8\")\n",
        "    trueset=[]\n",
        "    for index, row in df.iterrows():\n",
        "      if (row[\"label2\"] == 1):\n",
        "        trueset.append(row[\"0\"])\n",
        "    print(trueset)\n",
        "    xtest = df[\"0\"].values\n",
        "    ytest = df[\"label2\"]\n",
        "    xtest2 = batch_encode(xtest, tokenizer)\n",
        "    y_pred = new_model.predict(xtest2.values(), batch_size=64, verbose=1)\n",
        "    row_index=[]\n",
        "    for i in enumerate(y_pred):\n",
        "      if i[1][1] >= 0.50:\n",
        "        row_index.append(i[0])\n",
        "    #print(row_index)\n",
        "    predsumm = []\n",
        "    for j in enumerate(xtest):\n",
        "      #print(j)\n",
        "      if j[0] in row_index:\n",
        "        predsumm.append(j[1])\n",
        "\n",
        "    # for each of the hypothesis and reference documents pair\n",
        "    for (h, r) in zip(trueset, predsumm):\n",
        "        # computing the ROUGE\n",
        "        score = scorer.score(h, r)\n",
        "        # separating the measurements\n",
        "        precision, recall, fmeasure = score['rouge1']\n",
        "        # add them to the proper list in the dictionary\n",
        "        results['precision'].append(precision)\n",
        "        results['recall'].append(recall)\n",
        "        results['fmeasure'].append(fmeasure)\n",
        "\n",
        "    print(results)\n",
        "    for st,vals in results.items():\n",
        "        print(\"Average for {} is {}\".format(st,mean(vals)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIQvfgqaP9jn",
        "outputId": "cbae10fd-d77e-4326-91ec-e6f87aa41a59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/data/testdata/Black-Box Variational Inference for Stochastic Differential Equations.csv\n",
            "['A stochastic differential equation (SDE) defines a diffusion process, which evolves randomly over time, by describing its instantaneous behaviour.', 'As such, SDEs are powerful modelling tools used extensively in fields such as econometrics (Black & Scholes, 1973; Eraker, 2001), biology (Gillespie, 2000; Golightly & Wilkinson, 2011), physics (van Kampen, 2007) and epidemiology (Fuchs, 2013).\\n', 'It is only possible to work with analytic solutions to SDEs in special cases.', 'Therefore it is common to use a numerical approximation, such as the Euler-Maruyama scheme.', 'Here the diffusion process is defined only on a grid of time points, and the transition density between successive diffusion states is approximated as Gaussian.', 'The RNN learns how to supply Gaussian state transitions between successive time points which closely match those for the intractable conditioned diffusion process.\\n', 'In\\nthis case, (1) defines a diffusion process.', 'Such processes are always Markovian (i.e. memoryless).\\n', 'We further assume partial noisy observations of the latent process.', 'In the simplest case, these times are equally spaced, separated by a time-step of ∆t.', 'Upon choosing a prior density p(θ), Bayesian inference proceeds via the parameter posterior p(θ|y), or alternatively the joint posterior p(θ, x|y).\\n', 'This motivates us to use a variational approximation in which the diffusion matrix is not constrained to follow (9), and instead is allowed to shrink.', 'When variational inference outputs a good match to the posterior distribution, importance sampling (IS) (see e.g. Robert, 2004) can correct remaining inaccuracies and provide near-exact posterior inference.', 'To express x with a non-centred parameterisation, let 2 ∼ N(0, Ipm) be the flattened vector of (z1, z2, . . .', 'So the network just discussed forms a cell of an overall recurrent neural network (RNN) structure for q(x|θ;φx).', 'It can be interpreted as constraining the variational approximation based on prior beliefs about positivity of diffusion paths.', 'Exploratory work showed that the RNN produces a much better approximation of the conditioned process with these features as input rather than simply xτi , y, θ and τi.\\n', 'Many of these violate the assumptions used by existing diffusion bridge constructs (Whitaker et al., 2017b).\\n', 'Lotka-Volterra models describe simple predator-prey population dynamics combining three types of event: prey reproduction, predation (in which prey are consumed and predators have the resources to reproduce) and predator death.', 'Table 1 shows the resulting ESS values.', 'Convergence takes 2 hours, and importance sampling with 500,000 iterations produces an ESS of 635.4.', 'Constant population size is assumed.', 'Model comparison The two models produce visually similar diffusion paths, but close inspection shows some differences.', 'A better estimate of the parameter posteriors would allow formal model comparison based on importance sampling evidence estimates.', 'This performs inference for a broad class of SDEs with minimal tuning requirements.', 'Approximate parameter inference is also possible, with our results recovering known parameters for synthetic data (Section 5.1), and previous results for real data (Section 5.2), using only a few hours of computation for a desktop PC.', 'An interesting future direction is develop choices of q(x|θ;φx) more efficent than standard RNNs, to further reduce computing time and enable real-time applications of this methodology.']\n",
            "156/156 [==============================] - 115s 739ms/step\n",
            "{'precision': [0.021739130434782608, 0.021739130434782608, 0.0, 0.07692307692307693, 0.16, 0.09523809523809523, 0.10810810810810811, 0.0, 0.13636363636363635, 0.057692307692307696, 0.08, 0.16666666666666666, 0.08108108108108109, 0.125, 0.0, 0.03508771929824561, 0.1346153846153846, 0.08108108108108109, 0.16666666666666666, 0.038461538461538464, 0.058823529411764705, 0.030303030303030304, 0.05, 0.0, 0.07017543859649122, 0.06818181818181818, 0.13043478260869565], 'recall': [0.05263157894736842, 0.030303030303030304, 0.0, 0.26666666666666666, 0.32, 0.17391304347826086, 0.5, 0.0, 0.3, 0.1875, 0.17391304347826086, 0.28, 0.10344827586206896, 0.25, 0.0, 0.1111111111111111, 0.25, 0.1875, 0.21875, 0.14285714285714285, 0.058823529411764705, 0.2, 0.125, 0.0, 0.3076923076923077, 0.07894736842105263, 0.20689655172413793], 'fmeasure': [0.03076923076923077, 0.02531645569620253, 0.0, 0.11940298507462686, 0.21333333333333335, 0.12307692307692307, 0.17777777777777778, 0.0, 0.18749999999999997, 0.08823529411764705, 0.1095890410958904, 0.20895522388059704, 0.09090909090909091, 0.16666666666666666, 0.0, 0.05333333333333334, 0.17500000000000002, 0.11320754716981132, 0.18918918918918917, 0.060606060606060615, 0.058823529411764705, 0.052631578947368425, 0.07142857142857144, 0.0, 0.11428571428571428, 0.07317073170731707, 0.16]}\n",
            "Average for precision is 0.07386600822841681\n",
            "Average for recall is 0.16762791296122864\n",
            "Average for fmeasure is 0.09863734364730062\n",
            "/content/drive/MyDrive/Colab Notebooks/data/testdata/Coordinated Multi-Agent Imitation Learning.csv\n",
            "['Motivated by these challenges, we study the problem of imitation learning for multiple coordinating agents from demonstrations.', 'The second is a challenging task of learning multiple policies for team defense in professional soccer, using a large training set1 of play sequences illustrated by Figure 1.', 'We show that learning a good latent structure to encode implicit coordination yields significantly superior imitation performance compared to conventional baselines.', 'In coordinated multi-agent imitation learning, we have K agents acting in coordination to achieve a common goal (or sequence of goals).', 'Importantly, we assume the identity (or indexing) of the K experts may change from one demonstration to another.', 'Our ultimate goal is to learn a (largely) decentralized policy, but for clarity we first present the problem of learning a fully centralized multi-agent policy.', 'The goal is to minimize imitation loss:\\nLimitation “ E~s„d~π r`p~πp~sqqs ,\\nwhere d~π denotes the distribution of states experienced by joint policy ~π and ` is the imitation loss defined over the demonstrations (e.g., squared loss for deterministic policies, or cross entropy for stochastic policies).\\n', 'This requirement applies for both centralized and decentralized policy learning, and is often implicitly assumed in prior work on multi-agent learning.', 'Note that barring extensive annotations or some heuristic rulebased definitions, it is unnatural to quantitatively define what makes a player “left defender”.', 'To motivate our notion of role, let’s first consider the simplest indexing mechanism: one could equate role to agent identity.', 'However, a key challenge in learning policies directly is that the roles are undefined, unobserved, and could change dynamically within the same sequence.', 'We formulate the indexing mechanism as an assignment function A which maps the unstructured set U and some probabilistic structured model q to an indexed set of trajectory A rearranged from U , i.e.,\\nA : tU1, .., UKu ˆ q ÞÑ rA1, .., AKs ,\\nwhere the set tA1, .., AKu ” tU1, .., UKu.', 'We view q as a latent variable model that infers the role assignments for each set of demonstrations.', 'For efficient training, we employ alternating stochastic optimization (Hoffman et al., 2013; Johnson & Willsky, 2014; Beal, 2003) on the same mini-batches.', 'We interleave the three components described above into a complete learning algorithm.', 'We slightly abuse notations and overload θ for the natural parameters of global parameter θ in the exponential family.', 'Assuming the usual conjugacy in the exponential family, the stochastic natural gradient takes a convenient form (line 2 of Algo 3, and derivation in Appendix), where tpz, xq is the vector of sufficient statistics, b is a vector of scaling factors adjusting for the relative size of the minibatches.', 'Now solving the linear assignment problem for cost matrix M , we obtain the matching A from role k̄ to index k, such that the total cost per agent is minimized.', 'In both versions, training was done using the same data aggregation scheme and batch training was conducted using the same random forests configuration.\\n', 'Figure 4 compares the test performance of our method versus unstructured multi-agent imitation learning.', 'Note that we even gave the unstructured baseline some advantage over our method, by forcing the prey to select the moves last after all predators make decisions (effectively making the prey slower).', 'Without this advantage, the unstructured policies fail to capture the prey almost 100% of the time.', 'RoboCup, the robotic and simulation soccer platform, is perhaps the most popular testbed for multi-agent reinforcement learning research to date (Stone, 2016).', 'In this experiment, we aim to learn multi-agent policies for team soccer defense, based on tracking data from real-life professional soccer (Bialkowski et al., 2014).\\n', 'As LSTMs generally require fixed-length input sequences, we further chunk each trajectory into sub-sequences of length 50, with overlapping window of 25 time steps.', 'The structured model component is learned via stochastic variational inference on a continuous HMM, where the perstate emission distribution is a mixture of Gaussians.']\n",
            "163/163 [==============================] - 121s 741ms/step\n",
            "{'precision': [0.021739130434782608, 0.021739130434782608, 0.0, 0.07692307692307693, 0.16, 0.09523809523809523, 0.10810810810810811, 0.0, 0.13636363636363635, 0.057692307692307696, 0.08, 0.16666666666666666, 0.08108108108108109, 0.125, 0.0, 0.03508771929824561, 0.1346153846153846, 0.08108108108108109, 0.16666666666666666, 0.038461538461538464, 0.058823529411764705, 0.030303030303030304, 0.05, 0.0, 0.07017543859649122, 0.06818181818181818, 0.13043478260869565, 0.09259259259259259, 0.09523809523809523, 0.06666666666666667, 0.13157894736842105, 0.14285714285714285, 0.07317073170731707, 0.12903225806451613, 0.08064516129032258, 0.046511627906976744, 0.05263157894736842, 0.07792207792207792, 0.14814814814814814, 0.0967741935483871, 0.06382978723404255, 0.02564102564102564, 0.08433734939759036, 0.2391304347826087, 0.057692307692307696, 0.1111111111111111, 0.03225806451612903, 0.1320754716981132, 0.05263157894736842, 0.11363636363636363, 0.08163265306122448, 0.05128205128205128, 0.20512820512820512], 'recall': [0.05263157894736842, 0.030303030303030304, 0.0, 0.26666666666666666, 0.32, 0.17391304347826086, 0.5, 0.0, 0.3, 0.1875, 0.17391304347826086, 0.28, 0.10344827586206896, 0.25, 0.0, 0.1111111111111111, 0.25, 0.1875, 0.21875, 0.14285714285714285, 0.058823529411764705, 0.2, 0.125, 0.0, 0.3076923076923077, 0.07894736842105263, 0.20689655172413793, 0.29411764705882354, 0.2857142857142857, 0.09523809523809523, 0.22727272727272727, 0.3888888888888889, 0.11538461538461539, 0.08333333333333333, 0.22727272727272727, 0.18181818181818182, 0.14285714285714285, 0.2608695652173913, 0.1702127659574468, 0.3333333333333333, 0.13636363636363635, 0.16666666666666666, 0.4117647058823529, 0.22448979591836735, 0.1, 0.17391304347826086, 0.13333333333333333, 0.21875, 0.1875, 0.21739130434782608, 0.14814814814814814, 0.07692307692307693, 0.3333333333333333], 'fmeasure': [0.03076923076923077, 0.02531645569620253, 0.0, 0.11940298507462686, 0.21333333333333335, 0.12307692307692307, 0.17777777777777778, 0.0, 0.18749999999999997, 0.08823529411764705, 0.1095890410958904, 0.20895522388059704, 0.09090909090909091, 0.16666666666666666, 0.0, 0.05333333333333334, 0.17500000000000002, 0.11320754716981132, 0.18918918918918917, 0.060606060606060615, 0.058823529411764705, 0.052631578947368425, 0.07142857142857144, 0.0, 0.11428571428571428, 0.07317073170731707, 0.16, 0.1408450704225352, 0.14285714285714285, 0.0784313725490196, 0.16666666666666666, 0.208955223880597, 0.08955223880597016, 0.10126582278481013, 0.11904761904761903, 0.07407407407407407, 0.07692307692307693, 0.12000000000000001, 0.1584158415841584, 0.15000000000000002, 0.08695652173913043, 0.044444444444444446, 0.13999999999999999, 0.23157894736842108, 0.07317073170731708, 0.13559322033898305, 0.05194805194805195, 0.1647058823529412, 0.0821917808219178, 0.1492537313432836, 0.10526315789473684, 0.06153846153846155, 0.25396825396825395]}\n",
            "Average for precision is 0.08450071412364958\n",
            "Average for recall is 0.18605366044707863\n",
            "Average for fmeasure is 0.11077086063280622\n",
            "/content/drive/MyDrive/Colab Notebooks/data/testdata/Deep Tensor Convolution on Multicores.csv\n",
            "['Even in the case of the most successful distributed frameworks for ConvNets (Abadi et al., 2016), GPU memory management is largely unresolved.', 'The TensorFlow authors propose two partial solutions warranting further investigation: (a) re-computing versus storing large tensors; and (b) transferring long-lived tensors from GPU to host CPU memory.', 'Consider the 1-dimension convolution s = g∗d, where the kernel and data vectors are of length G and D. This problem can be rephrased as one of polynomial multiplication by introducing the associated polynomials d(x), g(x) and s(x) = g(x)d(x), where the coefficients si = ∑ k gi−kdk of xi are the solution to the desired convolution.', 'With respect to Algorithm 1, Step (1) is implemented by the kernel and data trans-\\nforms, Step (2) by their transformed element-wise product and Step (3) by the final inverse transform.\\n', 'Lavin has published code that automates this procedure using the Cook-Toom algorithm to produce transformed kernels and data both of length D (Lavin, 2016).', 'Inappropriate selection of m(x) would yield matrices of polynomials (degree > 1) that require considerably more scalar multiplications to compute.\\n', 'Below we present an alternative approach to fast convolution that removes the need to hand-craft minimal algorithms.', 'Instead of crafting a minimal algorithm, we show how relaxed memory constraints and efficient sparse linear algebra of CPU systems can be leveraged to amortize transform costs.', '(3)\\nIn our case U is sparse and X is dense, so we implement (3) such that U is traversed in the outermost two loops.', 'It is straightforward to show that (1) is a special case of (4) by considering the following equivalence:\\nY = X ×n U⇔ Y(n) = UX(n),\\nwhere the matrix X(n) is the mode-n major unfolding of tensor X (Kolda & Bader, 2009).', 'In the 1-dimensional case, X(1) is simply x and thus X ×1 U = Ux.', 'The reasons are two-fold: (a) the matrix multiplication cost can be amortized across a larger number of kernels and channels due to relaxed memory constraints; and (b) CPUs are able to directly leverage the sparse structure of these matrices for further acceleration.', 'The game changes when one considers these approaches in the context of a ConvNet layer with multiple channels and kernels.', 'Without loss of generality, assume the numbers of kernels and channels are both equal to M .', 'As the inverse transform can be applied once over the reduced output and the data transform once across all kernels, the required number of multiplications is just 4M2 + 24M (versus 6M2 for Winograd).', 'Looking beyond a single core, a recent review demonstrates poor multicore scalability across all major ConvNet frameworks (Shi et al., 2016a).', 'To solve this problem, recall that D is one of many small, overlapping tiles that span the full-size feature map.', ', DN captures a single (x, y) coordinate in the earlier G′ D′ element-wise product, which is fused with the channelwise reduction into end-to-end matrix multiplications:\\ncomputations\\nmemory accesses =\\n2DNMTK\\nDN (MT +MK) =\\n2TK\\nT +K .\\n', 'Single-core utilization is just one dimension of performance optimization.', 'To avoid memory contention and other concurrency issues we adopt the Cilk Plus work-stealing scheduler supported by GCC 4.8 (Blumofe et al., 1996; Robison, 2013), simply applying its fork-join primitive to all for-loops with no iteration dependencies.', 'The number of tiles T per thread is empirically tuned to simultaneously maximize L3 cache utilization (T cannot be too large) and compute-to-memory ratio (T cannot be too small).\\n', 'We observe that even this simple parallelization scheme yields near-optimal linear scalability.', 'Scalability is measured across a single convolution layer for a 1024 × 1024 image with kernels of size 4 × 4.', 'To avoid NUMA issues relating to expensive inter-chip communication, we spawn independent instances for each CPU in our 4-socket sharedmemory server such that all 18 threads in Figure 3 are bound to a single chip.', 'The only study we could find presenting thorough CPU benchmarking is that of Shi et al., comparing the throughput of Caffe, CNTK, Tensorflow and Torch for the AlexNet and ResNet architectures (Shi et al., 2016a).', 'They adopt an ear-\\nlier version of TensorFlow that uses the Eigen 3.2 library (no AVX/FMA support), and otherwise use the default framework-specific implementations of convolution rather than linking to optimized packages such as Intel MKL.\\n', 'We benchmark 2D ConvNet performance against two popular frameworks: TensorFlow, using the newer Eigen 3.3 library (with AVX support); and Caffe, compiled to use Intel’s optimized MKL library.', 'An important innovation of our approach is that it is batch size-agnostic, making it suitable for single-image autoregressive models common in generative modelling and deep reinforcement learning.\\n']\n",
            "189/189 [==============================] - 140s 741ms/step\n",
            "{'precision': [0.021739130434782608, 0.021739130434782608, 0.0, 0.07692307692307693, 0.16, 0.09523809523809523, 0.10810810810810811, 0.0, 0.13636363636363635, 0.057692307692307696, 0.08, 0.16666666666666666, 0.08108108108108109, 0.125, 0.0, 0.03508771929824561, 0.1346153846153846, 0.08108108108108109, 0.16666666666666666, 0.038461538461538464, 0.058823529411764705, 0.030303030303030304, 0.05, 0.0, 0.07017543859649122, 0.06818181818181818, 0.13043478260869565, 0.09259259259259259, 0.09523809523809523, 0.06666666666666667, 0.13157894736842105, 0.14285714285714285, 0.07317073170731707, 0.12903225806451613, 0.08064516129032258, 0.046511627906976744, 0.05263157894736842, 0.07792207792207792, 0.14814814814814814, 0.0967741935483871, 0.06382978723404255, 0.02564102564102564, 0.08433734939759036, 0.2391304347826087, 0.057692307692307696, 0.1111111111111111, 0.03225806451612903, 0.1320754716981132, 0.05263157894736842, 0.11363636363636363, 0.08163265306122448, 0.05128205128205128, 0.20512820512820512, 0.09803921568627451, 0.03389830508474576, 0.24324324324324326, 0.07142857142857142, 0.1111111111111111, 0.023255813953488372, 0.08333333333333333, 0.05357142857142857, 0.0, 0.11428571428571428, 0.07692307692307693, 0.15942028985507245, 0.1111111111111111, 0.05, 0.11538461538461539, 0.0, 0.0975609756097561, 0.09090909090909091, 0.024390243902439025, 0.11428571428571428, 0.09375, 0.07142857142857142, 0.034482758620689655, 0.09523809523809523, 0.15384615384615385, 0.06896551724137931, 0.07526881720430108, 0.09259259259259259], 'recall': [0.05263157894736842, 0.030303030303030304, 0.0, 0.26666666666666666, 0.32, 0.17391304347826086, 0.5, 0.0, 0.3, 0.1875, 0.17391304347826086, 0.28, 0.10344827586206896, 0.25, 0.0, 0.1111111111111111, 0.25, 0.1875, 0.21875, 0.14285714285714285, 0.058823529411764705, 0.2, 0.125, 0.0, 0.3076923076923077, 0.07894736842105263, 0.20689655172413793, 0.29411764705882354, 0.2857142857142857, 0.09523809523809523, 0.22727272727272727, 0.3888888888888889, 0.11538461538461539, 0.08333333333333333, 0.22727272727272727, 0.18181818181818182, 0.14285714285714285, 0.2608695652173913, 0.1702127659574468, 0.3333333333333333, 0.13636363636363635, 0.16666666666666666, 0.4117647058823529, 0.22448979591836735, 0.1, 0.17391304347826086, 0.13333333333333333, 0.21875, 0.1875, 0.21739130434782608, 0.14814814814814814, 0.07692307692307693, 0.3333333333333333, 0.22727272727272727, 0.06896551724137931, 0.14516129032258066, 0.09375, 0.16, 0.05, 0.16666666666666666, 0.1111111111111111, 0.0, 0.09302325581395349, 0.1875, 0.2558139534883721, 0.25, 0.1875, 0.18181818181818182, 0.0, 0.19047619047619047, 0.13157894736842105, 0.1, 0.0975609756097561, 0.0967741935483871, 0.15384615384615385, 0.05263157894736842, 0.10810810810810811, 0.22857142857142856, 0.10256410256410256, 0.23333333333333334, 0.1724137931034483], 'fmeasure': [0.03076923076923077, 0.02531645569620253, 0.0, 0.11940298507462686, 0.21333333333333335, 0.12307692307692307, 0.17777777777777778, 0.0, 0.18749999999999997, 0.08823529411764705, 0.1095890410958904, 0.20895522388059704, 0.09090909090909091, 0.16666666666666666, 0.0, 0.05333333333333334, 0.17500000000000002, 0.11320754716981132, 0.18918918918918917, 0.060606060606060615, 0.058823529411764705, 0.052631578947368425, 0.07142857142857144, 0.0, 0.11428571428571428, 0.07317073170731707, 0.16, 0.1408450704225352, 0.14285714285714285, 0.0784313725490196, 0.16666666666666666, 0.208955223880597, 0.08955223880597016, 0.10126582278481013, 0.11904761904761903, 0.07407407407407407, 0.07692307692307693, 0.12000000000000001, 0.1584158415841584, 0.15000000000000002, 0.08695652173913043, 0.044444444444444446, 0.13999999999999999, 0.23157894736842108, 0.07317073170731708, 0.13559322033898305, 0.05194805194805195, 0.1647058823529412, 0.0821917808219178, 0.1492537313432836, 0.10526315789473684, 0.06153846153846155, 0.25396825396825395, 0.136986301369863, 0.045454545454545456, 0.18181818181818182, 0.08108108108108107, 0.13114754098360656, 0.031746031746031744, 0.1111111111111111, 0.07228915662650602, 0.0, 0.10256410256410256, 0.1090909090909091, 0.1964285714285714, 0.15384615384615383, 0.07894736842105264, 0.14117647058823532, 0.0, 0.12903225806451615, 0.10752688172043011, 0.03921568627450981, 0.10526315789473684, 0.09523809523809523, 0.0975609756097561, 0.041666666666666664, 0.10126582278481013, 0.18390804597701152, 0.08247422680412371, 0.11382113821138212, 0.12048192771084339]}\n",
            "Average for precision is 0.08439829888153083\n",
            "Average for recall is 0.16922574707292393\n",
            "Average for fmeasure is 0.10693824719290819\n",
            "/content/drive/MyDrive/Colab Notebooks/data/testdata/Efficient softmax approximation for GPUs.csv\n",
            "['This paper considers strategies to learn parametric models for language modeling with very large vocabularies.', 'This problem is key to natural language processing, with applications in machine translation (Schwenk et al., 2012; Sutskever et al., 2014; Vaswani et al., 2013) or automatic speech recognition (Graves et al., 2013; Hinton et al., 2012).', 'This potentially makes parametric models prohibitively slow to train on corpora with very large vocabulary.', 'In particular, smoothed N-gram models (Bahl et al., 1983; Katz, 1987; Kneser & Ney, 1995) achieve good performance in practice (Mikolov et al., 2011a), especially when they are associated with cache models (Kuhn & De Mori, 1990).', 'In the simplest case, this probability is represented by a 2-layer neural network acting on an input xt ∈ VN , defined as the concatenation of the one-hot representation of the N previous words, wt−N+1, . . .', 'In neural language modeling, predicting the probability of the next word requires computing scores for every word in the vocabulary and to normalize them to form a probability distribution.', 'If each class contains √ k words, the computational cost is reduced from O(dk) to O(d √ k).', 'We observe that the computation time g(k) is constant for low values of k, until a certain inflection point k0 ≈ 50, and then becomes affine for values k > k0.', 'We observe the same behavior when measuring the timings as a function of the batch size B, i.e., it is inefficient to matrix-multiplication when one of the dimensions is small.', 'Similarly, clusters comprising only rare words have a low probabilty p and a shrinking batch size of p B, which also lead to iniffient matrix-multiplication.', 'For instance, one may define the head would only contain 20% of the vocabulary (covering for 87% on PennTree Bank).', 'The tail cluster will then contain the rest of the vocabulary, made of kt = k − kh words and covering for pt = 1 − ph of the overall distribution.', 'The computation time corresponding to the matrix multiplication of the root is equal to g(kh +1, B), while the computation time for the tail of the distribution is equal to g(kt, ptB), where B is the\\nbatch size.', 'We thus obtain the overall computation time\\nC = g(kh + 1, B) + g(kt, ptB).\\n', 'We can then find the size of the head cluster kh which minimizes the computation time C. We plot the value of C as a function of kh in Figure 2, for the word distribution of the Bulgarian Europarl dataset.', 'Like in Chen et al. (2015), we exploit this observation to further reduce the computational time of our classifier.', 'Moreover, we observe that using more than 5 clusters does not lead to significant gains in computational time (a couple of milliseconds at best).', 'In practice, we thus decide to use a small number of clusters (between 2 and 5), as it usually lead to slightly better perplexity, and we empirically determine the best speed/perplexity compromise on training data.', 'Our method is compared to: (1) the full softmax, (2) the hierarchical softmax with frequency binning (HSM freq) and similarity-based binning (HSM sim), (3) importance sampling (Bengio et al., 2003b; Bengio & Senécal, 2008) and (4) the differentiated softmax (Chen et al., 2015).', 'For the negative sampling method, we used a number of samples equal to 20% of the\\nsize of the vocabulary (Chen et al., 2015).', 'Comparison with the state of the art.', 'Our approach is the only one to approach the result of the full soft-max (below by 3 points of perplexity), while being the fastest.', 'We also note that for models of similar size, we achieve similar perplexity than the method introduced by Jozefowicz et al. (2016).', 'As far as we know, ours the first method to achieve a perplexity lower than 50 on a single GPU.', 'In all our experiments on GPU, our method consistently maintains a low perplexity while enjoying a speed-up going from 2× to 10× compared to the exact model.', 'This type of speed-up allows to deal with extremely large corpora in\\nreasonable time and without the need of a large number of GPUs.', 'We believe our approach to be general enough to be applied to other parallel computing architectures and other losses, as well as to other domains where the distributions of the class are unbalanced.']\n",
            "228/228 [==============================] - 169s 740ms/step\n",
            "{'precision': [0.021739130434782608, 0.021739130434782608, 0.0, 0.07692307692307693, 0.16, 0.09523809523809523, 0.10810810810810811, 0.0, 0.13636363636363635, 0.057692307692307696, 0.08, 0.16666666666666666, 0.08108108108108109, 0.125, 0.0, 0.03508771929824561, 0.1346153846153846, 0.08108108108108109, 0.16666666666666666, 0.038461538461538464, 0.058823529411764705, 0.030303030303030304, 0.05, 0.0, 0.07017543859649122, 0.06818181818181818, 0.13043478260869565, 0.09259259259259259, 0.09523809523809523, 0.06666666666666667, 0.13157894736842105, 0.14285714285714285, 0.07317073170731707, 0.12903225806451613, 0.08064516129032258, 0.046511627906976744, 0.05263157894736842, 0.07792207792207792, 0.14814814814814814, 0.0967741935483871, 0.06382978723404255, 0.02564102564102564, 0.08433734939759036, 0.2391304347826087, 0.057692307692307696, 0.1111111111111111, 0.03225806451612903, 0.1320754716981132, 0.05263157894736842, 0.11363636363636363, 0.08163265306122448, 0.05128205128205128, 0.20512820512820512, 0.09803921568627451, 0.03389830508474576, 0.24324324324324326, 0.07142857142857142, 0.1111111111111111, 0.023255813953488372, 0.08333333333333333, 0.05357142857142857, 0.0, 0.11428571428571428, 0.07692307692307693, 0.15942028985507245, 0.1111111111111111, 0.05, 0.11538461538461539, 0.0, 0.0975609756097561, 0.09090909090909091, 0.024390243902439025, 0.11428571428571428, 0.09375, 0.07142857142857142, 0.034482758620689655, 0.09523809523809523, 0.15384615384615385, 0.06896551724137931, 0.07526881720430108, 0.09259259259259259, 0.05128205128205128, 0.07547169811320754, 0.05172413793103448, 0.05, 0.15789473684210525, 0.24444444444444444, 0.0847457627118644, 0.18421052631578946, 0.12162162162162163, 0.06, 0.05454545454545454, 0.06818181818181818, 0.03636363636363636, 0.0, 0.1724137931034483, 0.06493506493506493, 0.09523809523809523, 0.07692307692307693, 0.1875, 0.14285714285714285, 0.07317073170731707, 0.15, 0.08823529411764706, 0.0, 0.10810810810810811, 0.1111111111111111, 0.13043478260869565], 'recall': [0.05263157894736842, 0.030303030303030304, 0.0, 0.26666666666666666, 0.32, 0.17391304347826086, 0.5, 0.0, 0.3, 0.1875, 0.17391304347826086, 0.28, 0.10344827586206896, 0.25, 0.0, 0.1111111111111111, 0.25, 0.1875, 0.21875, 0.14285714285714285, 0.058823529411764705, 0.2, 0.125, 0.0, 0.3076923076923077, 0.07894736842105263, 0.20689655172413793, 0.29411764705882354, 0.2857142857142857, 0.09523809523809523, 0.22727272727272727, 0.3888888888888889, 0.11538461538461539, 0.08333333333333333, 0.22727272727272727, 0.18181818181818182, 0.14285714285714285, 0.2608695652173913, 0.1702127659574468, 0.3333333333333333, 0.13636363636363635, 0.16666666666666666, 0.4117647058823529, 0.22448979591836735, 0.1, 0.17391304347826086, 0.13333333333333333, 0.21875, 0.1875, 0.21739130434782608, 0.14814814814814814, 0.07692307692307693, 0.3333333333333333, 0.22727272727272727, 0.06896551724137931, 0.14516129032258066, 0.09375, 0.16, 0.05, 0.16666666666666666, 0.1111111111111111, 0.0, 0.09302325581395349, 0.1875, 0.2558139534883721, 0.25, 0.1875, 0.18181818181818182, 0.0, 0.19047619047619047, 0.13157894736842105, 0.1, 0.0975609756097561, 0.0967741935483871, 0.15384615384615385, 0.05263157894736842, 0.10810810810810811, 0.22857142857142856, 0.10256410256410256, 0.23333333333333334, 0.1724137931034483, 0.13333333333333333, 0.10810810810810811, 0.2, 0.05555555555555555, 0.16216216216216217, 0.3793103448275862, 0.2777777777777778, 0.23333333333333334, 0.28125, 0.11538461538461539, 0.15, 0.1111111111111111, 0.05, 0.0, 0.125, 0.2631578947368421, 0.16666666666666666, 0.1111111111111111, 0.06818181818181818, 0.4583333333333333, 0.42857142857142855, 0.24, 0.13636363636363635, 0.0, 0.14285714285714285, 0.28, 0.18181818181818182], 'fmeasure': [0.03076923076923077, 0.02531645569620253, 0.0, 0.11940298507462686, 0.21333333333333335, 0.12307692307692307, 0.17777777777777778, 0.0, 0.18749999999999997, 0.08823529411764705, 0.1095890410958904, 0.20895522388059704, 0.09090909090909091, 0.16666666666666666, 0.0, 0.05333333333333334, 0.17500000000000002, 0.11320754716981132, 0.18918918918918917, 0.060606060606060615, 0.058823529411764705, 0.052631578947368425, 0.07142857142857144, 0.0, 0.11428571428571428, 0.07317073170731707, 0.16, 0.1408450704225352, 0.14285714285714285, 0.0784313725490196, 0.16666666666666666, 0.208955223880597, 0.08955223880597016, 0.10126582278481013, 0.11904761904761903, 0.07407407407407407, 0.07692307692307693, 0.12000000000000001, 0.1584158415841584, 0.15000000000000002, 0.08695652173913043, 0.044444444444444446, 0.13999999999999999, 0.23157894736842108, 0.07317073170731708, 0.13559322033898305, 0.05194805194805195, 0.1647058823529412, 0.0821917808219178, 0.1492537313432836, 0.10526315789473684, 0.06153846153846155, 0.25396825396825395, 0.136986301369863, 0.045454545454545456, 0.18181818181818182, 0.08108108108108107, 0.13114754098360656, 0.031746031746031744, 0.1111111111111111, 0.07228915662650602, 0.0, 0.10256410256410256, 0.1090909090909091, 0.1964285714285714, 0.15384615384615383, 0.07894736842105264, 0.14117647058823532, 0.0, 0.12903225806451615, 0.10752688172043011, 0.03921568627450981, 0.10526315789473684, 0.09523809523809523, 0.0975609756097561, 0.041666666666666664, 0.10126582278481013, 0.18390804597701152, 0.08247422680412371, 0.11382113821138212, 0.12048192771084339, 0.07407407407407407, 0.0888888888888889, 0.0821917808219178, 0.052631578947368425, 0.16, 0.29729729729729726, 0.12987012987012986, 0.20588235294117646, 0.169811320754717, 0.07894736842105263, 0.08, 0.08450704225352113, 0.042105263157894736, 0.0, 0.14492753623188406, 0.10416666666666667, 0.12121212121212123, 0.09090909090909093, 0.09999999999999999, 0.2178217821782178, 0.125, 0.1846153846153846, 0.10714285714285714, 0.0, 0.12307692307692308, 0.1590909090909091, 0.1518987341772152]}\n",
            "Average for precision is 0.08775625276358086\n",
            "Average for recall is 0.1719136395198202\n",
            "Average for fmeasure is 0.10961173264217473\n",
            "/content/drive/MyDrive/Colab Notebooks/data/testdata/Fine-grained Coordinated Cross-lingual Text Stream Alignment for Endless Language Knowledge Acquisition.csv\n",
            "['Coordinated text streams (Wang et al., 2007) refer to the text streams that are topically related and indexed by the same set of time points.', 'Previous studies (Wang et al., 2007; Hu et al., 2012) on coordinated text stream focus on discovering and aligning common topic patterns across languages.', 'Despite their contributions to applications like cross-lingual information retrieval and topic analysis, such a coarse-grained topic-level alignment framework inevitably overlooks many useful fine-grained alignment knowledge.', 'In addition to (a) bi-lingual word translations, we can also discover (b) polysemous and multi-referential words if one Chinese word is aligned to multiple English words, (c) synonymous and co-referential word pairs if two Chinese words are aligned to the same English word, and (d) entity phrases (e.g.,阿布扎比 in Figure 1)\\ninject Dezhou Texas ASEAN Abu Dhabi\\nChinese\\nEnglish\\n(a) (b) (c) (d)\\n', 'In order to acquire language knowledge for Natural Language Processing (NLP) applications, we study fine-grained cross-lingual text stream alignment.', 'Instead of directly turning massive, unstructured data streams into structured knowledge (D2K), we adopt a new Data-to-Network-toKnowledge (D2N2K) paradigm, based on the following observations: (i) most information units are not independent, instead they are interconnected or interacting, forming massive networks; (ii) if information networks can be constructed across multiple languages, they may bring tremendous power to make knowledge mining algorithms more scalable and effective because we can employ the graph structures to acquire and propagate knowledge.\\n', 'Based on the motivations, we employ a promising text stream representation – Burst Information Networks (BINets) (Ge et al., 2016a), which can be easily constructed without rich language resources, as media to display the most important information units and illustrate their connections in the text streams.', 'With the BINet representation, we propose a simple yet effective network decipherment algorithm for aligning cross-lingual text streams, which can take advantage of the coburst characteristic of cross-lingual text streams and easily incorporate prior knowledge and rich clues for fast and accurate network decipherment.\\n', 'For example, in Figure 2, each node in a BINet is a bursty word with one of its burst periods, representing an important information unit in a text stream.', 'To decipher the Chinese BINet, our approach first focuses on the nodes in the English BINet in Figure 2 as the candidates because they co-burst with the Chinese nodes.', 'Then, we decipher some nodes based on prior knowledge (the green node), the pronunciation similarity clue (the orange nodes) or literal translation similarity clue (the blue node).', 'These deciphered nodes will serve as neighbor clues to decipher their adjacent nodes (the red node) which will then be used for further decipherment (e.g., decipher the yellow node) through knowledge propagation across the network, as the dashed arrows in Figure 2 show.\\n', 'The main contributions of this paper are:\\n• We propose a promising framework to mine knowledge from inexhaustible coordinated cross-lingual text streams through finegrained alignment, exploring a paradigm for language knowledge acquisition.\\n', 'We define Gc = 〈Vc, Ec,ωc,πc〉 and Ge = 〈Ve, Ee,ωe,πe〉 as the Chinese BINet and English BINet respectively.', 'Pronunciation Inspired by previous work on name translation mining (e.g., (Schafer III, 2006; Sproat et al., 2006; Ji, 2009)), for a node e ∈ Cand(c), if its pronunciation is similar to c, then e is likely to be the translation of c. For a Chinese node c and an English node e, we define Sp as its scaled pronunciation score to measure their pronunciation similarity whose range is [0, 1]: Sp ∈', 'Even though “澳洲网球公开赛(Australian Open)” is not in the bi-lingual lexicon, “Australian” and “open” are in the lexicon and their Chinese translations are “澳洲的(Australian)” and “公开(open)” respectively.', 'If we literally translate “Australian Open” word by word, we will get “澳洲的公 开” which has long common subsequences with the Chinese node “澳洲网球公开赛(Australian Open)”, inferring that “Australian Open” is likely to be the translation of “澳洲网球公开赛”.\\n3Pinyin is the official romanization system for Chinese.', 'Motivated by this observation, for a candidate e ∈ Cand(c), we first extract its possible Chinese translations C(e) from the bilingual lexicon.', 'By analyzing a node’s neighbors, we can learn useful topic-level knowledge to decipher the node.', 'If we know “Serena Williams”, “Australian Open” and “Zheng Jie” are the counterpart of ‘威廉”, “澳洲网球公开赛” and “郑洁” respectively, we can infer “Justine Henin” is likely to be the counterpart of “艾宁”, which can be further used as a clue to decipher its neighbors such as “外卡(wildcard)” through knowledge propagation.\\n', 'We define Sb as the burst correlation score:\\nSb = sw(c) · sw(e)\\n‖sw(c)‖1 + ‖sw(e)‖1 − sw(c) · sw(e) (2)\\nwhere w(v) denotes the word of the node v and sw denotes the burst sequence of the word w in which each entry is a binary variable indicating if w bursts at a moment throughout the time frame.', 'We define the overall (credibility) score as the linear combination of the clues introduced above:\\nScore(c, e) = ηSp + λSt + γSn + δSb (3)\\nwhere Sp, St, Sn and Sb are the scores that measure the value/reliability of the pronunciation, translation, neighbor and burst correlation clues respectively, and η, λ, γ and δ are hyperparameters for adjusting their weights.\\n', 'Algorithm 1 Graph-based Decipherment 1: For the determined pair 〈c, e〉 based on the prior knowl-\\nedge, Score(c, e)← 1.0 2: For other undermined pairs 〈c, e〉, initialize Score(c, e)\\naccording to Eq (4); 3: while True (until ∆Conf(Gc, Ge) ≤ 0.0001) do 4: for each undetermined pair 〈c, e〉 do 5: Compute new score according to Eq (3); 6: update(c, e) = min(1.0, new score) 7: end for 8: for each undetermined pair 〈c, e〉 do 9: Score(c, e)← update(c, e)\\n10: end for 11: end while\\n∆Conf(Gc, Ge) in the 3rd line of Algorithm 1 is the difference between the network decipherment confidence score at the current iteration and that at its previous iteration.', 'Our seed bi-lingual lexicon is released by (Zens and Ney, 2004), containing 81,990 Chinese word entries, each of which has an English translation.', 'For top 100 mined pairs with the highest confidence scores (i.e., the score in Eq (3)), the accuracy is 98%.', 'Word/entity translations are the main knowledge that can be derived from our alignment results by extracting word pairs from the aligned cross-lingual node pairs.', 'Table 2 compares our approach to representative bilingual lexicon extraction approaches.']\n",
            "195/195 [==============================] - 144s 739ms/step\n",
            "{'precision': [0.021739130434782608, 0.021739130434782608, 0.0, 0.07692307692307693, 0.16, 0.09523809523809523, 0.10810810810810811, 0.0, 0.13636363636363635, 0.057692307692307696, 0.08, 0.16666666666666666, 0.08108108108108109, 0.125, 0.0, 0.03508771929824561, 0.1346153846153846, 0.08108108108108109, 0.16666666666666666, 0.038461538461538464, 0.058823529411764705, 0.030303030303030304, 0.05, 0.0, 0.07017543859649122, 0.06818181818181818, 0.13043478260869565, 0.09259259259259259, 0.09523809523809523, 0.06666666666666667, 0.13157894736842105, 0.14285714285714285, 0.07317073170731707, 0.12903225806451613, 0.08064516129032258, 0.046511627906976744, 0.05263157894736842, 0.07792207792207792, 0.14814814814814814, 0.0967741935483871, 0.06382978723404255, 0.02564102564102564, 0.08433734939759036, 0.2391304347826087, 0.057692307692307696, 0.1111111111111111, 0.03225806451612903, 0.1320754716981132, 0.05263157894736842, 0.11363636363636363, 0.08163265306122448, 0.05128205128205128, 0.20512820512820512, 0.09803921568627451, 0.03389830508474576, 0.24324324324324326, 0.07142857142857142, 0.1111111111111111, 0.023255813953488372, 0.08333333333333333, 0.05357142857142857, 0.0, 0.11428571428571428, 0.07692307692307693, 0.15942028985507245, 0.1111111111111111, 0.05, 0.11538461538461539, 0.0, 0.0975609756097561, 0.09090909090909091, 0.024390243902439025, 0.11428571428571428, 0.09375, 0.07142857142857142, 0.034482758620689655, 0.09523809523809523, 0.15384615384615385, 0.06896551724137931, 0.07526881720430108, 0.09259259259259259, 0.05128205128205128, 0.07547169811320754, 0.05172413793103448, 0.05, 0.15789473684210525, 0.24444444444444444, 0.0847457627118644, 0.18421052631578946, 0.12162162162162163, 0.06, 0.05454545454545454, 0.06818181818181818, 0.03636363636363636, 0.0, 0.1724137931034483, 0.06493506493506493, 0.09523809523809523, 0.07692307692307693, 0.1875, 0.14285714285714285, 0.07317073170731707, 0.15, 0.08823529411764706, 0.0, 0.10810810810810811, 0.1111111111111111, 0.13043478260869565, 0.1111111111111111, 0.02, 0.044444444444444446, 0.11764705882352941, 0.05263157894736842, 0.2391304347826087, 0.02857142857142857, 0.17543859649122806, 0.07586206896551724, 0.14035087719298245, 0.04838709677419355, 0.09523809523809523, 0.15625, 0.05555555555555555, 0.19607843137254902, 0.07692307692307693, 0.1794871794871795, 0.1111111111111111, 0.125, 0.21052631578947367, 0.24242424242424243, 0.2857142857142857, 0.09090909090909091, 0.06779661016949153, 0.11627906976744186, 0.17391304347826086, 0.02127659574468085], 'recall': [0.05263157894736842, 0.030303030303030304, 0.0, 0.26666666666666666, 0.32, 0.17391304347826086, 0.5, 0.0, 0.3, 0.1875, 0.17391304347826086, 0.28, 0.10344827586206896, 0.25, 0.0, 0.1111111111111111, 0.25, 0.1875, 0.21875, 0.14285714285714285, 0.058823529411764705, 0.2, 0.125, 0.0, 0.3076923076923077, 0.07894736842105263, 0.20689655172413793, 0.29411764705882354, 0.2857142857142857, 0.09523809523809523, 0.22727272727272727, 0.3888888888888889, 0.11538461538461539, 0.08333333333333333, 0.22727272727272727, 0.18181818181818182, 0.14285714285714285, 0.2608695652173913, 0.1702127659574468, 0.3333333333333333, 0.13636363636363635, 0.16666666666666666, 0.4117647058823529, 0.22448979591836735, 0.1, 0.17391304347826086, 0.13333333333333333, 0.21875, 0.1875, 0.21739130434782608, 0.14814814814814814, 0.07692307692307693, 0.3333333333333333, 0.22727272727272727, 0.06896551724137931, 0.14516129032258066, 0.09375, 0.16, 0.05, 0.16666666666666666, 0.1111111111111111, 0.0, 0.09302325581395349, 0.1875, 0.2558139534883721, 0.25, 0.1875, 0.18181818181818182, 0.0, 0.19047619047619047, 0.13157894736842105, 0.1, 0.0975609756097561, 0.0967741935483871, 0.15384615384615385, 0.05263157894736842, 0.10810810810810811, 0.22857142857142856, 0.10256410256410256, 0.23333333333333334, 0.1724137931034483, 0.13333333333333333, 0.10810810810810811, 0.2, 0.05555555555555555, 0.16216216216216217, 0.3793103448275862, 0.2777777777777778, 0.23333333333333334, 0.28125, 0.11538461538461539, 0.15, 0.1111111111111111, 0.05, 0.0, 0.125, 0.2631578947368421, 0.16666666666666666, 0.1111111111111111, 0.06818181818181818, 0.4583333333333333, 0.42857142857142855, 0.24, 0.13636363636363635, 0.0, 0.14285714285714285, 0.28, 0.18181818181818182, 0.16, 0.041666666666666664, 0.06896551724137931, 0.05970149253731343, 0.09523809523809523, 0.13924050632911392, 0.044444444444444446, 0.21739130434782608, 0.3793103448275862, 0.26666666666666666, 0.1111111111111111, 0.09090909090909091, 0.15625, 0.14285714285714285, 0.1388888888888889, 0.1111111111111111, 0.16666666666666666, 0.2608695652173913, 0.23529411764705882, 0.08695652173913043, 0.12698412698412698, 0.18181818181818182, 0.031746031746031744, 0.16, 0.23809523809523808, 0.3076923076923077, 0.09090909090909091], 'fmeasure': [0.03076923076923077, 0.02531645569620253, 0.0, 0.11940298507462686, 0.21333333333333335, 0.12307692307692307, 0.17777777777777778, 0.0, 0.18749999999999997, 0.08823529411764705, 0.1095890410958904, 0.20895522388059704, 0.09090909090909091, 0.16666666666666666, 0.0, 0.05333333333333334, 0.17500000000000002, 0.11320754716981132, 0.18918918918918917, 0.060606060606060615, 0.058823529411764705, 0.052631578947368425, 0.07142857142857144, 0.0, 0.11428571428571428, 0.07317073170731707, 0.16, 0.1408450704225352, 0.14285714285714285, 0.0784313725490196, 0.16666666666666666, 0.208955223880597, 0.08955223880597016, 0.10126582278481013, 0.11904761904761903, 0.07407407407407407, 0.07692307692307693, 0.12000000000000001, 0.1584158415841584, 0.15000000000000002, 0.08695652173913043, 0.044444444444444446, 0.13999999999999999, 0.23157894736842108, 0.07317073170731708, 0.13559322033898305, 0.05194805194805195, 0.1647058823529412, 0.0821917808219178, 0.1492537313432836, 0.10526315789473684, 0.06153846153846155, 0.25396825396825395, 0.136986301369863, 0.045454545454545456, 0.18181818181818182, 0.08108108108108107, 0.13114754098360656, 0.031746031746031744, 0.1111111111111111, 0.07228915662650602, 0.0, 0.10256410256410256, 0.1090909090909091, 0.1964285714285714, 0.15384615384615383, 0.07894736842105264, 0.14117647058823532, 0.0, 0.12903225806451615, 0.10752688172043011, 0.03921568627450981, 0.10526315789473684, 0.09523809523809523, 0.0975609756097561, 0.041666666666666664, 0.10126582278481013, 0.18390804597701152, 0.08247422680412371, 0.11382113821138212, 0.12048192771084339, 0.07407407407407407, 0.0888888888888889, 0.0821917808219178, 0.052631578947368425, 0.16, 0.29729729729729726, 0.12987012987012986, 0.20588235294117646, 0.169811320754717, 0.07894736842105263, 0.08, 0.08450704225352113, 0.042105263157894736, 0.0, 0.14492753623188406, 0.10416666666666667, 0.12121212121212123, 0.09090909090909093, 0.09999999999999999, 0.2178217821782178, 0.125, 0.1846153846153846, 0.10714285714285714, 0.0, 0.12307692307692308, 0.1590909090909091, 0.1518987341772152, 0.13114754098360656, 0.02702702702702703, 0.05405405405405405, 0.07920792079207921, 0.06779661016949153, 0.176, 0.034782608695652174, 0.1941747572815534, 0.12643678160919541, 0.1839080459770115, 0.06741573033707865, 0.09302325581395349, 0.15625, 0.08, 0.16260162601626016, 0.09090909090909093, 0.1728395061728395, 0.15584415584415584, 0.16326530612244897, 0.12307692307692308, 0.16666666666666666, 0.2222222222222222, 0.047058823529411764, 0.09523809523809523, 0.15625, 0.2222222222222222, 0.034482758620689655]}\n",
            "Average for precision is 0.09433876072781984\n",
            "Average for recall is 0.16798116518394254\n",
            "Average for fmeasure is 0.11201458410916\n",
            "/content/drive/MyDrive/Colab Notebooks/data/testdata/Graph Convolutional Encoders for Syntax-aware Neural Machine Translation.csv\n",
            "['Neural machine translation (NMT) is one of success stories of deep learning in natural language processing, with recent NMT systems outperforming traditional phrase-based approaches on many language pairs (Sennrich et al., 2016a).', 'State-ofthe-art NMT systems rely on sequential encoderdecoders (Sutskever et al., 2014; Bahdanau et al., 2015) and lack any explicit modeling of syntax or any hierarchical structure of language.', 'Despite some successes, techniques explored so far either incorporate syntactic information in NMT models in a relatively indirect way (e.g., multi-task learning (Luong et al., 2015a; Nadejde et al., 2017; Eriguchi et al., 2017; Hashimoto and Tsuruoka, 2017)) or may be too restrictive in modeling the interface between syntax and the translation task (e.g., learning representations of linguistic phrases (Eriguchi et al., 2016)).', 'Our goal is to automatically incorporate information about syntactic neighborhoods of source words into these feature vectors, and, thus, potentially improve quality of the translation output.', 'We will now describe the Graph Convolutional Networks (GCNs) of Kipf and Welling (2016).', 'For a comprehensive overview of alternative GCN architectures see Gilmer et al. (2017).\\n', 'In each GCN layer, information flows along edges of the graph; in other words, each node receives messages from all its immediate neighbors.', 'When multiple GCN layers are stacked, information about larger neighborhoods gets integrated.', 'The model might not only benefit from this teleporting capability however; also the nature of the relations between words (i.e. dependency relation types) may be useful, and the GCN exploits this information (see §2.3 for details).\\n', 'In all experiments we obtain translations using a greedy decoder, i.e. we select the output token with the highest probability at each time step.\\n', 'From a vocabulary of 26 types, we generate random sequences of 3-10 tokens.', 'We then randomly permute them, pointing every token to its original predecessor with a label sampled from a set of 5 labels.', 'Figure 3 shows that the mean values of the bias terms of gates (i.e. b̂) for real and fake edges are far apart, suggesting that the GCN learns to distinguish them.', 'A gate-less model would not understand which of the two outgoing arcs is fake and which is genuine, because only biases b would then be label-dependent.', 'Although using label-specific matrices W would also help, this would not scale to the real scenario (see §2.3).', 'The English sides of the corpora are tokenized and parsed into dependency\\n6http://www.statmt.org/wmt16/translation-task.html\\ntrees by SyntaxNet,7 using the pre-trained Parsey McParseface model.8 The Czech and German sides are tokenized using the Moses tokenizer.9 Sentence pairs where either side is longer than 50 words are filtered out after tokenization.\\n', 'For Czech and German, to deal with rare words and phenomena such as inflection and compounding, we learn byte-pair encodings (BPE) as described by Sennrich et al. (2016b).', 'We use 256 units for word embeddings, 512 units for GRUs (800 for En-De full data set experiment), and 512 units for convolutional layers (or equivalently, 512 ‘channels’).', 'We report results for 2-layer GCNs, as we find them most effective (see ablation studies below).\\n', 'We provide three baselines, each with a different encoder: a bag-of-words encoder, a convolutional encoder with window size w = 5, and a BiRNN.', 'We report (cased) BLEU results (Papineni et al., 2002) using multi-bleu, as well as Kendall τ reordering scores.10', 'We expected the BoW+GCN model to make easy gains over this baseline, which is indeed what happens.', 'While it is difficult to obtain high absolute BLEU scores on this dataset, we can still see similar relative improvements.', 'How many GCN layers do we need?', 'One explanation may be that syntactic parses are noisier for longer sentences, and this prevents us from obtaining extra improvements with GCNs.\\n', 'We have presented a simple and effective approach to integrating syntax into neural machine translation models and have shown consistent BLEU4 improvements for two challenging language pairs: English-German and English-Czech.', 'Since GCNs are capable of encoding any kind of graph-based structure, in future work we would like to go be-\\nyond syntax, by using semantic annotations such as SRL and AMR, and co-reference chains.']\n",
            "28/28 [==============================] - 20s 724ms/step\n",
            "{'precision': [0.021739130434782608, 0.021739130434782608, 0.0, 0.07692307692307693, 0.16, 0.09523809523809523, 0.10810810810810811, 0.0, 0.13636363636363635, 0.057692307692307696, 0.08, 0.16666666666666666, 0.08108108108108109, 0.125, 0.0, 0.03508771929824561, 0.1346153846153846, 0.08108108108108109, 0.16666666666666666, 0.038461538461538464, 0.058823529411764705, 0.030303030303030304, 0.05, 0.0, 0.07017543859649122, 0.06818181818181818, 0.13043478260869565, 0.09259259259259259, 0.09523809523809523, 0.06666666666666667, 0.13157894736842105, 0.14285714285714285, 0.07317073170731707, 0.12903225806451613, 0.08064516129032258, 0.046511627906976744, 0.05263157894736842, 0.07792207792207792, 0.14814814814814814, 0.0967741935483871, 0.06382978723404255, 0.02564102564102564, 0.08433734939759036, 0.2391304347826087, 0.057692307692307696, 0.1111111111111111, 0.03225806451612903, 0.1320754716981132, 0.05263157894736842, 0.11363636363636363, 0.08163265306122448, 0.05128205128205128, 0.20512820512820512, 0.09803921568627451, 0.03389830508474576, 0.24324324324324326, 0.07142857142857142, 0.1111111111111111, 0.023255813953488372, 0.08333333333333333, 0.05357142857142857, 0.0, 0.11428571428571428, 0.07692307692307693, 0.15942028985507245, 0.1111111111111111, 0.05, 0.11538461538461539, 0.0, 0.0975609756097561, 0.09090909090909091, 0.024390243902439025, 0.11428571428571428, 0.09375, 0.07142857142857142, 0.034482758620689655, 0.09523809523809523, 0.15384615384615385, 0.06896551724137931, 0.07526881720430108, 0.09259259259259259, 0.05128205128205128, 0.07547169811320754, 0.05172413793103448, 0.05, 0.15789473684210525, 0.24444444444444444, 0.0847457627118644, 0.18421052631578946, 0.12162162162162163, 0.06, 0.05454545454545454, 0.06818181818181818, 0.03636363636363636, 0.0, 0.1724137931034483, 0.06493506493506493, 0.09523809523809523, 0.07692307692307693, 0.1875, 0.14285714285714285, 0.07317073170731707, 0.15, 0.08823529411764706, 0.0, 0.10810810810810811, 0.1111111111111111, 0.13043478260869565, 0.1111111111111111, 0.02, 0.044444444444444446, 0.11764705882352941, 0.05263157894736842, 0.2391304347826087, 0.02857142857142857, 0.17543859649122806, 0.07586206896551724, 0.14035087719298245, 0.04838709677419355, 0.09523809523809523, 0.15625, 0.05555555555555555, 0.19607843137254902, 0.07692307692307693, 0.1794871794871795, 0.1111111111111111, 0.125, 0.21052631578947367, 0.24242424242424243, 0.2857142857142857, 0.09090909090909091, 0.06779661016949153, 0.11627906976744186, 0.17391304347826086, 0.02127659574468085, 0.08695652173913043, 0.05555555555555555, 0.1388888888888889, 0.13636363636363635, 0.23809523809523808, 0.07936507936507936, 0.047619047619047616, 0.02127659574468085, 0.09090909090909091, 0.01818181818181818, 0.034482758620689655, 0.11428571428571428, 0.11363636363636363, 0.06666666666666667, 0.034482758620689655, 0.06818181818181818, 0.10204081632653061, 0.029411764705882353, 0.043478260869565216, 0.034482758620689655, 0.05660377358490566, 0.01694915254237288, 0.020833333333333332, 0.0, 0.029850746268656716, 0.13043478260869565, 0.13043478260869565], 'recall': [0.05263157894736842, 0.030303030303030304, 0.0, 0.26666666666666666, 0.32, 0.17391304347826086, 0.5, 0.0, 0.3, 0.1875, 0.17391304347826086, 0.28, 0.10344827586206896, 0.25, 0.0, 0.1111111111111111, 0.25, 0.1875, 0.21875, 0.14285714285714285, 0.058823529411764705, 0.2, 0.125, 0.0, 0.3076923076923077, 0.07894736842105263, 0.20689655172413793, 0.29411764705882354, 0.2857142857142857, 0.09523809523809523, 0.22727272727272727, 0.3888888888888889, 0.11538461538461539, 0.08333333333333333, 0.22727272727272727, 0.18181818181818182, 0.14285714285714285, 0.2608695652173913, 0.1702127659574468, 0.3333333333333333, 0.13636363636363635, 0.16666666666666666, 0.4117647058823529, 0.22448979591836735, 0.1, 0.17391304347826086, 0.13333333333333333, 0.21875, 0.1875, 0.21739130434782608, 0.14814814814814814, 0.07692307692307693, 0.3333333333333333, 0.22727272727272727, 0.06896551724137931, 0.14516129032258066, 0.09375, 0.16, 0.05, 0.16666666666666666, 0.1111111111111111, 0.0, 0.09302325581395349, 0.1875, 0.2558139534883721, 0.25, 0.1875, 0.18181818181818182, 0.0, 0.19047619047619047, 0.13157894736842105, 0.1, 0.0975609756097561, 0.0967741935483871, 0.15384615384615385, 0.05263157894736842, 0.10810810810810811, 0.22857142857142856, 0.10256410256410256, 0.23333333333333334, 0.1724137931034483, 0.13333333333333333, 0.10810810810810811, 0.2, 0.05555555555555555, 0.16216216216216217, 0.3793103448275862, 0.2777777777777778, 0.23333333333333334, 0.28125, 0.11538461538461539, 0.15, 0.1111111111111111, 0.05, 0.0, 0.125, 0.2631578947368421, 0.16666666666666666, 0.1111111111111111, 0.06818181818181818, 0.4583333333333333, 0.42857142857142855, 0.24, 0.13636363636363635, 0.0, 0.14285714285714285, 0.28, 0.18181818181818182, 0.16, 0.041666666666666664, 0.06896551724137931, 0.05970149253731343, 0.09523809523809523, 0.13924050632911392, 0.044444444444444446, 0.21739130434782608, 0.3793103448275862, 0.26666666666666666, 0.1111111111111111, 0.09090909090909091, 0.15625, 0.14285714285714285, 0.1388888888888889, 0.1111111111111111, 0.16666666666666666, 0.2608695652173913, 0.23529411764705882, 0.08695652173913043, 0.12698412698412698, 0.18181818181818182, 0.031746031746031744, 0.16, 0.23809523809523808, 0.3076923076923077, 0.09090909090909091, 0.12121212121212122, 0.1, 0.07575757575757576, 0.23076923076923078, 0.35714285714285715, 0.38461538461538464, 0.13043478260869565, 0.08333333333333333, 0.07894736842105263, 0.04, 0.14285714285714285, 0.18181818181818182, 0.15625, 0.10714285714285714, 0.05, 0.05084745762711865, 0.1724137931034483, 0.034482758620689655, 0.058823529411764705, 0.04, 0.15789473684210525, 0.05555555555555555, 0.05, 0.0, 0.09090909090909091, 0.09375, 0.16666666666666666], 'fmeasure': [0.03076923076923077, 0.02531645569620253, 0.0, 0.11940298507462686, 0.21333333333333335, 0.12307692307692307, 0.17777777777777778, 0.0, 0.18749999999999997, 0.08823529411764705, 0.1095890410958904, 0.20895522388059704, 0.09090909090909091, 0.16666666666666666, 0.0, 0.05333333333333334, 0.17500000000000002, 0.11320754716981132, 0.18918918918918917, 0.060606060606060615, 0.058823529411764705, 0.052631578947368425, 0.07142857142857144, 0.0, 0.11428571428571428, 0.07317073170731707, 0.16, 0.1408450704225352, 0.14285714285714285, 0.0784313725490196, 0.16666666666666666, 0.208955223880597, 0.08955223880597016, 0.10126582278481013, 0.11904761904761903, 0.07407407407407407, 0.07692307692307693, 0.12000000000000001, 0.1584158415841584, 0.15000000000000002, 0.08695652173913043, 0.044444444444444446, 0.13999999999999999, 0.23157894736842108, 0.07317073170731708, 0.13559322033898305, 0.05194805194805195, 0.1647058823529412, 0.0821917808219178, 0.1492537313432836, 0.10526315789473684, 0.06153846153846155, 0.25396825396825395, 0.136986301369863, 0.045454545454545456, 0.18181818181818182, 0.08108108108108107, 0.13114754098360656, 0.031746031746031744, 0.1111111111111111, 0.07228915662650602, 0.0, 0.10256410256410256, 0.1090909090909091, 0.1964285714285714, 0.15384615384615383, 0.07894736842105264, 0.14117647058823532, 0.0, 0.12903225806451615, 0.10752688172043011, 0.03921568627450981, 0.10526315789473684, 0.09523809523809523, 0.0975609756097561, 0.041666666666666664, 0.10126582278481013, 0.18390804597701152, 0.08247422680412371, 0.11382113821138212, 0.12048192771084339, 0.07407407407407407, 0.0888888888888889, 0.0821917808219178, 0.052631578947368425, 0.16, 0.29729729729729726, 0.12987012987012986, 0.20588235294117646, 0.169811320754717, 0.07894736842105263, 0.08, 0.08450704225352113, 0.042105263157894736, 0.0, 0.14492753623188406, 0.10416666666666667, 0.12121212121212123, 0.09090909090909093, 0.09999999999999999, 0.2178217821782178, 0.125, 0.1846153846153846, 0.10714285714285714, 0.0, 0.12307692307692308, 0.1590909090909091, 0.1518987341772152, 0.13114754098360656, 0.02702702702702703, 0.05405405405405405, 0.07920792079207921, 0.06779661016949153, 0.176, 0.034782608695652174, 0.1941747572815534, 0.12643678160919541, 0.1839080459770115, 0.06741573033707865, 0.09302325581395349, 0.15625, 0.08, 0.16260162601626016, 0.09090909090909093, 0.1728395061728395, 0.15584415584415584, 0.16326530612244897, 0.12307692307692308, 0.16666666666666666, 0.2222222222222222, 0.047058823529411764, 0.09523809523809523, 0.15625, 0.2222222222222222, 0.034482758620689655, 0.10126582278481013, 0.07142857142857142, 0.09803921568627452, 0.1714285714285714, 0.2857142857142857, 0.13157894736842105, 0.06976744186046512, 0.03389830508474576, 0.08450704225352113, 0.024999999999999998, 0.05555555555555555, 0.14035087719298245, 0.13157894736842105, 0.0821917808219178, 0.04081632653061224, 0.05825242718446602, 0.12820512820512822, 0.03174603174603175, 0.049999999999999996, 0.03703703703703704, 0.08333333333333333, 0.025974025974025976, 0.029411764705882353, 0.0, 0.0449438202247191, 0.10909090909090909, 0.14634146341463414]}\n",
            "Average for precision is 0.09058765692715504\n",
            "Average for recall is 0.15980914644596986\n",
            "Average for fmeasure is 0.10734213880698718\n",
            "/content/drive/MyDrive/Colab Notebooks/data/testdata/Human Centered NLP with User-Factor Adaptation.csv\n",
            "['Knowing who wrote a piece of text can help to better understand it.', 'For instance, knowing the age and gender groups of authors has been shown to improve document classification (Hovy, 2015) and sentiment analysis (Volkova et al., 2013).\\n', 'However, putting people into discrete groups (e.g. age groups, binary gender) often relies on arbitrary boundaries which may not correspond to meaningful changes in language use.', 'A wealth of psychological research suggests people should not be characterized as discrete types (or domains) but rather as mixtures of continuous factors (McCrae\\nand Costa Jr., 1989; Ruscio and Ruscio, 2000; Widiger and Samuel, 2005).\\n', 'Here, we ask how one can adapt NLP models to real-valued human factors – continuous valued attributes that capture fine-grained differences between users (e.g. real-valued age, continuous gender scores).', 'Our approach composes user factor information with the linguistic features, similar to feature augmentation (Daumé III, 2007), a widely used domain adaptation technique which allows for easy integration with most feature-based learning models.', 'Since relevant user information often is not explicitly available, we use a background of tweets from the user to infer user factors.', 'The discrete adaptation method is a direct application of this idea, where the training and test instances are mapped into domains based on some grouping that we induce from the user factors.', 'For instances from domain i, the original features are copied over to feature set i + 1.', 'Figure 1 illustrates the advantage of continuous adaptation for a single feature — whether the current instance contains an intensifier — using sarcasm detection as an example.', 'The colored shapes show the feature values for instances from four users, with green squares representing “sarcastic” tweets and yellow circles representing “not sarcastic” ones.', 'The model is unable to distinguish between sarcastic and non-sarcastic tweets in the no adaptation and discrete adaptation case.', 'As with discrete adaptation, learning then proceeds unmodified with these augmented instances.\\n', 'The augmented training data (trainaug) is thus associated with the features x of the tweet, the task labels y, and the user information u.', 'We learn factors from a user’s background language, or past tweets2.', 'Adaptation yields better results for sarcasm and stance, semantic tasks where we’d expect user preferences to be an important factor.', 'Stance, however, is the one task where discrete works better for most factors.', 'As we show in Section 5.4, demographics and personality scores are themselves highly predictive of stances on issues; we believe this makes the simpler binning approach more reliable than the continuous model.\\n', '(iii) Both known and latent factors are helpful: Sarcasm benefits from age, gender and personality based adaptations, while stance benefits from personality.', 'However, adapting to random factors typically lowered results, suggesting that models using more features but not more information naturally take a hit.', 'Performance improves with d first and then tapers off; its best is +3.4 at d=5 and 7.', 'One way to use the factors is to add them as direct features to the instances, without adaptation.', 'For stance, however, we see that while there is an improvement over the baseline, it is not as large as that from adaptation.', 'Increase in the number of adjectives is a positive indicator of sarcasm for women (high gender scores) but is a negative indicator for men (low gender scores).', 'User-factor adaptation can model these interaction relationships when direct features alone may not.', 'To understand the advantage of continuous latent adaptation, we look at how well discrete and continuous factor representations capture meaningful information about users.']\n",
            "164/164 [==============================] - 122s 742ms/step\n",
            "{'precision': [0.021739130434782608, 0.021739130434782608, 0.0, 0.07692307692307693, 0.16, 0.09523809523809523, 0.10810810810810811, 0.0, 0.13636363636363635, 0.057692307692307696, 0.08, 0.16666666666666666, 0.08108108108108109, 0.125, 0.0, 0.03508771929824561, 0.1346153846153846, 0.08108108108108109, 0.16666666666666666, 0.038461538461538464, 0.058823529411764705, 0.030303030303030304, 0.05, 0.0, 0.07017543859649122, 0.06818181818181818, 0.13043478260869565, 0.09259259259259259, 0.09523809523809523, 0.06666666666666667, 0.13157894736842105, 0.14285714285714285, 0.07317073170731707, 0.12903225806451613, 0.08064516129032258, 0.046511627906976744, 0.05263157894736842, 0.07792207792207792, 0.14814814814814814, 0.0967741935483871, 0.06382978723404255, 0.02564102564102564, 0.08433734939759036, 0.2391304347826087, 0.057692307692307696, 0.1111111111111111, 0.03225806451612903, 0.1320754716981132, 0.05263157894736842, 0.11363636363636363, 0.08163265306122448, 0.05128205128205128, 0.20512820512820512, 0.09803921568627451, 0.03389830508474576, 0.24324324324324326, 0.07142857142857142, 0.1111111111111111, 0.023255813953488372, 0.08333333333333333, 0.05357142857142857, 0.0, 0.11428571428571428, 0.07692307692307693, 0.15942028985507245, 0.1111111111111111, 0.05, 0.11538461538461539, 0.0, 0.0975609756097561, 0.09090909090909091, 0.024390243902439025, 0.11428571428571428, 0.09375, 0.07142857142857142, 0.034482758620689655, 0.09523809523809523, 0.15384615384615385, 0.06896551724137931, 0.07526881720430108, 0.09259259259259259, 0.05128205128205128, 0.07547169811320754, 0.05172413793103448, 0.05, 0.15789473684210525, 0.24444444444444444, 0.0847457627118644, 0.18421052631578946, 0.12162162162162163, 0.06, 0.05454545454545454, 0.06818181818181818, 0.03636363636363636, 0.0, 0.1724137931034483, 0.06493506493506493, 0.09523809523809523, 0.07692307692307693, 0.1875, 0.14285714285714285, 0.07317073170731707, 0.15, 0.08823529411764706, 0.0, 0.10810810810810811, 0.1111111111111111, 0.13043478260869565, 0.1111111111111111, 0.02, 0.044444444444444446, 0.11764705882352941, 0.05263157894736842, 0.2391304347826087, 0.02857142857142857, 0.17543859649122806, 0.07586206896551724, 0.14035087719298245, 0.04838709677419355, 0.09523809523809523, 0.15625, 0.05555555555555555, 0.19607843137254902, 0.07692307692307693, 0.1794871794871795, 0.1111111111111111, 0.125, 0.21052631578947367, 0.24242424242424243, 0.2857142857142857, 0.09090909090909091, 0.06779661016949153, 0.11627906976744186, 0.17391304347826086, 0.02127659574468085, 0.08695652173913043, 0.05555555555555555, 0.1388888888888889, 0.13636363636363635, 0.23809523809523808, 0.07936507936507936, 0.047619047619047616, 0.02127659574468085, 0.09090909090909091, 0.01818181818181818, 0.034482758620689655, 0.11428571428571428, 0.11363636363636363, 0.06666666666666667, 0.034482758620689655, 0.06818181818181818, 0.10204081632653061, 0.029411764705882353, 0.043478260869565216, 0.034482758620689655, 0.05660377358490566, 0.01694915254237288, 0.020833333333333332, 0.0, 0.029850746268656716, 0.13043478260869565, 0.13043478260869565, 0.043478260869565216, 0.09259259259259259, 0.05555555555555555, 0.11363636363636363, 0.09523809523809523, 0.07936507936507936, 0.047619047619047616, 0.1702127659574468, 0.06060606060606061, 0.05454545454545454, 0.08620689655172414, 0.11428571428571428, 0.022727272727272728, 0.15555555555555556, 0.0, 0.11363636363636363, 0.04081632653061224, 0.058823529411764705, 0.17391304347826086, 0.0, 0.07547169811320754, 0.03389830508474576, 0.020833333333333332, 0.10810810810810811, 0.0, 0.17391304347826086], 'recall': [0.05263157894736842, 0.030303030303030304, 0.0, 0.26666666666666666, 0.32, 0.17391304347826086, 0.5, 0.0, 0.3, 0.1875, 0.17391304347826086, 0.28, 0.10344827586206896, 0.25, 0.0, 0.1111111111111111, 0.25, 0.1875, 0.21875, 0.14285714285714285, 0.058823529411764705, 0.2, 0.125, 0.0, 0.3076923076923077, 0.07894736842105263, 0.20689655172413793, 0.29411764705882354, 0.2857142857142857, 0.09523809523809523, 0.22727272727272727, 0.3888888888888889, 0.11538461538461539, 0.08333333333333333, 0.22727272727272727, 0.18181818181818182, 0.14285714285714285, 0.2608695652173913, 0.1702127659574468, 0.3333333333333333, 0.13636363636363635, 0.16666666666666666, 0.4117647058823529, 0.22448979591836735, 0.1, 0.17391304347826086, 0.13333333333333333, 0.21875, 0.1875, 0.21739130434782608, 0.14814814814814814, 0.07692307692307693, 0.3333333333333333, 0.22727272727272727, 0.06896551724137931, 0.14516129032258066, 0.09375, 0.16, 0.05, 0.16666666666666666, 0.1111111111111111, 0.0, 0.09302325581395349, 0.1875, 0.2558139534883721, 0.25, 0.1875, 0.18181818181818182, 0.0, 0.19047619047619047, 0.13157894736842105, 0.1, 0.0975609756097561, 0.0967741935483871, 0.15384615384615385, 0.05263157894736842, 0.10810810810810811, 0.22857142857142856, 0.10256410256410256, 0.23333333333333334, 0.1724137931034483, 0.13333333333333333, 0.10810810810810811, 0.2, 0.05555555555555555, 0.16216216216216217, 0.3793103448275862, 0.2777777777777778, 0.23333333333333334, 0.28125, 0.11538461538461539, 0.15, 0.1111111111111111, 0.05, 0.0, 0.125, 0.2631578947368421, 0.16666666666666666, 0.1111111111111111, 0.06818181818181818, 0.4583333333333333, 0.42857142857142855, 0.24, 0.13636363636363635, 0.0, 0.14285714285714285, 0.28, 0.18181818181818182, 0.16, 0.041666666666666664, 0.06896551724137931, 0.05970149253731343, 0.09523809523809523, 0.13924050632911392, 0.044444444444444446, 0.21739130434782608, 0.3793103448275862, 0.26666666666666666, 0.1111111111111111, 0.09090909090909091, 0.15625, 0.14285714285714285, 0.1388888888888889, 0.1111111111111111, 0.16666666666666666, 0.2608695652173913, 0.23529411764705882, 0.08695652173913043, 0.12698412698412698, 0.18181818181818182, 0.031746031746031744, 0.16, 0.23809523809523808, 0.3076923076923077, 0.09090909090909091, 0.12121212121212122, 0.1, 0.07575757575757576, 0.23076923076923078, 0.35714285714285715, 0.38461538461538464, 0.13043478260869565, 0.08333333333333333, 0.07894736842105263, 0.04, 0.14285714285714285, 0.18181818181818182, 0.15625, 0.10714285714285714, 0.05, 0.05084745762711865, 0.1724137931034483, 0.034482758620689655, 0.058823529411764705, 0.04, 0.15789473684210525, 0.05555555555555555, 0.05, 0.0, 0.09090909090909091, 0.09375, 0.16666666666666666, 0.15384615384615385, 0.19230769230769232, 0.07407407407407407, 0.1388888888888889, 0.0625, 0.14705882352941177, 0.13636363636363635, 0.25, 0.125, 0.12, 0.2, 0.2, 0.08333333333333333, 0.2916666666666667, 0.0, 0.23809523809523808, 0.15384615384615385, 0.06060606060606061, 0.18181818181818182, 0.0, 0.21052631578947367, 0.1111111111111111, 0.043478260869565216, 0.14814814814814814, 0.0, 0.17391304347826086], 'fmeasure': [0.03076923076923077, 0.02531645569620253, 0.0, 0.11940298507462686, 0.21333333333333335, 0.12307692307692307, 0.17777777777777778, 0.0, 0.18749999999999997, 0.08823529411764705, 0.1095890410958904, 0.20895522388059704, 0.09090909090909091, 0.16666666666666666, 0.0, 0.05333333333333334, 0.17500000000000002, 0.11320754716981132, 0.18918918918918917, 0.060606060606060615, 0.058823529411764705, 0.052631578947368425, 0.07142857142857144, 0.0, 0.11428571428571428, 0.07317073170731707, 0.16, 0.1408450704225352, 0.14285714285714285, 0.0784313725490196, 0.16666666666666666, 0.208955223880597, 0.08955223880597016, 0.10126582278481013, 0.11904761904761903, 0.07407407407407407, 0.07692307692307693, 0.12000000000000001, 0.1584158415841584, 0.15000000000000002, 0.08695652173913043, 0.044444444444444446, 0.13999999999999999, 0.23157894736842108, 0.07317073170731708, 0.13559322033898305, 0.05194805194805195, 0.1647058823529412, 0.0821917808219178, 0.1492537313432836, 0.10526315789473684, 0.06153846153846155, 0.25396825396825395, 0.136986301369863, 0.045454545454545456, 0.18181818181818182, 0.08108108108108107, 0.13114754098360656, 0.031746031746031744, 0.1111111111111111, 0.07228915662650602, 0.0, 0.10256410256410256, 0.1090909090909091, 0.1964285714285714, 0.15384615384615383, 0.07894736842105264, 0.14117647058823532, 0.0, 0.12903225806451615, 0.10752688172043011, 0.03921568627450981, 0.10526315789473684, 0.09523809523809523, 0.0975609756097561, 0.041666666666666664, 0.10126582278481013, 0.18390804597701152, 0.08247422680412371, 0.11382113821138212, 0.12048192771084339, 0.07407407407407407, 0.0888888888888889, 0.0821917808219178, 0.052631578947368425, 0.16, 0.29729729729729726, 0.12987012987012986, 0.20588235294117646, 0.169811320754717, 0.07894736842105263, 0.08, 0.08450704225352113, 0.042105263157894736, 0.0, 0.14492753623188406, 0.10416666666666667, 0.12121212121212123, 0.09090909090909093, 0.09999999999999999, 0.2178217821782178, 0.125, 0.1846153846153846, 0.10714285714285714, 0.0, 0.12307692307692308, 0.1590909090909091, 0.1518987341772152, 0.13114754098360656, 0.02702702702702703, 0.05405405405405405, 0.07920792079207921, 0.06779661016949153, 0.176, 0.034782608695652174, 0.1941747572815534, 0.12643678160919541, 0.1839080459770115, 0.06741573033707865, 0.09302325581395349, 0.15625, 0.08, 0.16260162601626016, 0.09090909090909093, 0.1728395061728395, 0.15584415584415584, 0.16326530612244897, 0.12307692307692308, 0.16666666666666666, 0.2222222222222222, 0.047058823529411764, 0.09523809523809523, 0.15625, 0.2222222222222222, 0.034482758620689655, 0.10126582278481013, 0.07142857142857142, 0.09803921568627452, 0.1714285714285714, 0.2857142857142857, 0.13157894736842105, 0.06976744186046512, 0.03389830508474576, 0.08450704225352113, 0.024999999999999998, 0.05555555555555555, 0.14035087719298245, 0.13157894736842105, 0.0821917808219178, 0.04081632653061224, 0.05825242718446602, 0.12820512820512822, 0.03174603174603175, 0.049999999999999996, 0.03703703703703704, 0.08333333333333333, 0.025974025974025976, 0.029411764705882353, 0.0, 0.0449438202247191, 0.10909090909090909, 0.14634146341463414, 0.06779661016949153, 0.125, 0.06349206349206349, 0.125, 0.07547169811320754, 0.10309278350515463, 0.07058823529411765, 0.20253164556962025, 0.0816326530612245, 0.075, 0.12048192771084339, 0.14545454545454545, 0.03571428571428572, 0.2028985507246377, 0.0, 0.15384615384615383, 0.06451612903225806, 0.05970149253731344, 0.17777777777777776, 0.0, 0.11111111111111112, 0.05194805194805195, 0.028169014084507043, 0.125, 0.0, 0.17391304347826086]}\n",
            "Average for precision is 0.0886502068536133\n",
            "Average for recall is 0.15630672078201685\n",
            "Average for fmeasure is 0.10547640563487526\n",
            "/content/drive/MyDrive/Colab Notebooks/data/testdata/Improved Semantic-Aware Network Embedding with Fine-Grained Word Alignment.csv\n",
            "['Networks are ubiquitous, with prominent examples including social networks (e.g., Facebook, Twitter) or citation networks of research papers (e.g., arXiv).', 'Alternatively, network embedding (i.e., network representation learning) has been considered, representing each vertex of a network with a low-dimensional vector that preserves information on its similarity rel-\\native to other vertices.', 'Although these methods have demonstrated performance gains over structure-only network embeddings, the relationship between text sequences for a pair of vertices is accounted for solely by comparing their sentence embeddings.', 'However, as shown in Figure 1, to assess the similarity between two research papers, a more effective strategy would compare and align (via localweighting) individual important words (keywords) within a pair of abstracts, while information from other words (e.g., stop words) that tend to be less relevant can be effectively ignored (downweighted).', 'Given a pair of sentences, our model first aligns each word within one sentence with keywords from the other sentence (adaptively up-weighted via an attention mechanism), producing a set of fine-grained matching vectors.', 'To learn these embeddings, we specify an objective that leverages the information from both W and T , denoted as\\nL = ∑ e∈E Lstruct(e) + Ltext(e) + Ljoint(e) , (1)\\nwhere Lstruct, Ltext and Ljoint denote structure, text, and joint structure-text training losses, respectively.', 'For a vertex pair {vi, vj} weighted by wij , Lstruct(vi, vj) in (1) is defined as (Tang et al., 2015)\\nLstruct(vi, vj) = wij log p(his|hjs) , (2)\\nwhere p(his|hjs) denotes the conditional probability between structural embeddings for vertices {vi, vj}.', 'Note that structural embeddings, hs, are treated directly as parameters, while the text embeddings ht are learned based on the text sequences associated with vertices.\\n', 'We first introduce our base model, which reweights the importance of individual words within a text sequence in the context of the edge being considered.', 'Thus, while inferring the adaptive textual embedding for sentence ta, we propose reweighting the importance of each word in ta to explicitly account for its alignment with sentence tb.', 'Further, the word-by-context embedding for sequence ta is obtained by taking the weighted average over all word embeddings\\nha = ∑Ma i=1αix (i) a .', 'As illustrated in Figure 2, given two input embedding matrices Xa and Xb, we first compute the affinity matrix A ∈ RMb×Ma , whose elements represent the affinity scores corresponding to all word pairs between sequences ta and tb\\nA = XTb Xa .', '(9)\\nSubsequently, we compute the context-aware matrix for sequence tb as\\nAb = softmax(A) , X̃b = XbAb , (10)\\nwhere the softmax(·) function is applied columnwise to A, and thus Ab contains the attention weights (importance scores) across sequence tb (columns), which account for each word in sequence ta (rows).', 'To abstract the word-by-word alignments, we compare x(i)a with x̃ (i) b , for i = 1, 2, ...,Ma, to obtain the corresponding matching vector\\nm(i)a = falign ( x(i)a , x̃ (i) b ) , (11)\\nwhere falign(·) represents the alignment function.', 'Subsequently, matching vectors from (11) are aggregated to produce the final textual embedding hat for sequence ta as\\nhat = faggregate ( m(1)a ,m (2) a , ...,m (Ma) a ) , (12)\\nwhere faggregate denotes the aggregation function, which we specify as the max-pooling pooling operation.', 'Although these aggregation functions are simple and invariant to the order of words in input sentences, they have been demonstrated to be highly effective in relational reasoning (Parikh et al., 2016; Santoro et al., 2017).', 'The following three real-world datasets are employed for quantitative evaluation: (i) Cora, a standard paper citation network that contains 2,277 machine learning papers (vertices) grouped into 7 categories and connected by 5,214 citations (edges) (ii) HepTh, another citation network of 1,038 papers with abstract information and 1,990 citations; (iii) Zhihu, a network of 10,000 active users from Zhihu, the largest Q&A website in China, where 43,894 vertices and descriptions of the Q&A topics are available.', 'We set the batch size as 128, and the model is trained using Adam (Kingma and Ba, 2014), with a learning rate of 1× 10−3 for all parameters.', 'We experiment with three variants for our WANE model: (i) WANE: where the word embeddings of each text sequence are simply average to obtain the sentence representations, similar to (Joulin et al., 2016; Shen et al., 2018c).', 'Similar to Tu et al. (2017), we generate the global embedding for each vertex by taking the average over its context-aware embeddings with all other connected vertices.', 'Moreover, WANEww consistently outperforms other competitive semantic-aware models on a wide range of labeled proportions, suggesting that explicitly capturing word-by-word alignment features is not only use-\\nful for vertex-pair-based tasks, such as link prediction, but also results in better global embeddings which are required for vertex classification tasks.', 'Motivated by the observation in Wang and Jiang (2017) that the advantages of different functions to match two vectors vary from task to task, we further explore the choice of alignment and aggregation functions in our WANE-ww model.', 'In terms of the aggregation functions, we compare (one-layer) CNN, mean-pooling, and maxpooling operations to accumulate the matching vectors.', 'Embedding visualization To visualize the learned network representations, we further employ t-SNE to map the low-dimensional vectors of the vertices to a 2-D embedding space.', 'As shown in Figure 4 where each point indicates one paper (vertex), and the color of each point indicates the category it belongs to, the embeddings of the same label are indeed very close in the 2-D plot, while those with different labels are relatively farther from each other.', 'Case study The proposed word-by-word alignment mechanism can be used to highlight the most informative words (and the corresponding matching features) wrt the relationship between vertices.', 'It can be observed that matched key words, e.g., ‘MCMC’, ‘convergence’, between the text sequences are indeed assigned higher values in the matching vectors.', 'These words would be selected preferentially by the final max-pooling aggregation operation.']\n",
            "3/3 [==============================] - 2s 608ms/step\n",
            "{'precision': [0.021739130434782608, 0.021739130434782608, 0.0, 0.07692307692307693, 0.16, 0.09523809523809523, 0.10810810810810811, 0.0, 0.13636363636363635, 0.057692307692307696, 0.08, 0.16666666666666666, 0.08108108108108109, 0.125, 0.0, 0.03508771929824561, 0.1346153846153846, 0.08108108108108109, 0.16666666666666666, 0.038461538461538464, 0.058823529411764705, 0.030303030303030304, 0.05, 0.0, 0.07017543859649122, 0.06818181818181818, 0.13043478260869565, 0.09259259259259259, 0.09523809523809523, 0.06666666666666667, 0.13157894736842105, 0.14285714285714285, 0.07317073170731707, 0.12903225806451613, 0.08064516129032258, 0.046511627906976744, 0.05263157894736842, 0.07792207792207792, 0.14814814814814814, 0.0967741935483871, 0.06382978723404255, 0.02564102564102564, 0.08433734939759036, 0.2391304347826087, 0.057692307692307696, 0.1111111111111111, 0.03225806451612903, 0.1320754716981132, 0.05263157894736842, 0.11363636363636363, 0.08163265306122448, 0.05128205128205128, 0.20512820512820512, 0.09803921568627451, 0.03389830508474576, 0.24324324324324326, 0.07142857142857142, 0.1111111111111111, 0.023255813953488372, 0.08333333333333333, 0.05357142857142857, 0.0, 0.11428571428571428, 0.07692307692307693, 0.15942028985507245, 0.1111111111111111, 0.05, 0.11538461538461539, 0.0, 0.0975609756097561, 0.09090909090909091, 0.024390243902439025, 0.11428571428571428, 0.09375, 0.07142857142857142, 0.034482758620689655, 0.09523809523809523, 0.15384615384615385, 0.06896551724137931, 0.07526881720430108, 0.09259259259259259, 0.05128205128205128, 0.07547169811320754, 0.05172413793103448, 0.05, 0.15789473684210525, 0.24444444444444444, 0.0847457627118644, 0.18421052631578946, 0.12162162162162163, 0.06, 0.05454545454545454, 0.06818181818181818, 0.03636363636363636, 0.0, 0.1724137931034483, 0.06493506493506493, 0.09523809523809523, 0.07692307692307693, 0.1875, 0.14285714285714285, 0.07317073170731707, 0.15, 0.08823529411764706, 0.0, 0.10810810810810811, 0.1111111111111111, 0.13043478260869565, 0.1111111111111111, 0.02, 0.044444444444444446, 0.11764705882352941, 0.05263157894736842, 0.2391304347826087, 0.02857142857142857, 0.17543859649122806, 0.07586206896551724, 0.14035087719298245, 0.04838709677419355, 0.09523809523809523, 0.15625, 0.05555555555555555, 0.19607843137254902, 0.07692307692307693, 0.1794871794871795, 0.1111111111111111, 0.125, 0.21052631578947367, 0.24242424242424243, 0.2857142857142857, 0.09090909090909091, 0.06779661016949153, 0.11627906976744186, 0.17391304347826086, 0.02127659574468085, 0.08695652173913043, 0.05555555555555555, 0.1388888888888889, 0.13636363636363635, 0.23809523809523808, 0.07936507936507936, 0.047619047619047616, 0.02127659574468085, 0.09090909090909091, 0.01818181818181818, 0.034482758620689655, 0.11428571428571428, 0.11363636363636363, 0.06666666666666667, 0.034482758620689655, 0.06818181818181818, 0.10204081632653061, 0.029411764705882353, 0.043478260869565216, 0.034482758620689655, 0.05660377358490566, 0.01694915254237288, 0.020833333333333332, 0.0, 0.029850746268656716, 0.13043478260869565, 0.13043478260869565, 0.043478260869565216, 0.09259259259259259, 0.05555555555555555, 0.11363636363636363, 0.09523809523809523, 0.07936507936507936, 0.047619047619047616, 0.1702127659574468, 0.06060606060606061, 0.05454545454545454, 0.08620689655172414, 0.11428571428571428, 0.022727272727272728, 0.15555555555555556, 0.0, 0.11363636363636363, 0.04081632653061224, 0.058823529411764705, 0.17391304347826086, 0.0, 0.07547169811320754, 0.03389830508474576, 0.020833333333333332, 0.10810810810810811, 0.0, 0.17391304347826086, 0.034482758620689655, 0.10714285714285714, 0.15789473684210525, 0.1111111111111111, 0.058823529411764705, 0.12962962962962962, 0.18867924528301888], 'recall': [0.05263157894736842, 0.030303030303030304, 0.0, 0.26666666666666666, 0.32, 0.17391304347826086, 0.5, 0.0, 0.3, 0.1875, 0.17391304347826086, 0.28, 0.10344827586206896, 0.25, 0.0, 0.1111111111111111, 0.25, 0.1875, 0.21875, 0.14285714285714285, 0.058823529411764705, 0.2, 0.125, 0.0, 0.3076923076923077, 0.07894736842105263, 0.20689655172413793, 0.29411764705882354, 0.2857142857142857, 0.09523809523809523, 0.22727272727272727, 0.3888888888888889, 0.11538461538461539, 0.08333333333333333, 0.22727272727272727, 0.18181818181818182, 0.14285714285714285, 0.2608695652173913, 0.1702127659574468, 0.3333333333333333, 0.13636363636363635, 0.16666666666666666, 0.4117647058823529, 0.22448979591836735, 0.1, 0.17391304347826086, 0.13333333333333333, 0.21875, 0.1875, 0.21739130434782608, 0.14814814814814814, 0.07692307692307693, 0.3333333333333333, 0.22727272727272727, 0.06896551724137931, 0.14516129032258066, 0.09375, 0.16, 0.05, 0.16666666666666666, 0.1111111111111111, 0.0, 0.09302325581395349, 0.1875, 0.2558139534883721, 0.25, 0.1875, 0.18181818181818182, 0.0, 0.19047619047619047, 0.13157894736842105, 0.1, 0.0975609756097561, 0.0967741935483871, 0.15384615384615385, 0.05263157894736842, 0.10810810810810811, 0.22857142857142856, 0.10256410256410256, 0.23333333333333334, 0.1724137931034483, 0.13333333333333333, 0.10810810810810811, 0.2, 0.05555555555555555, 0.16216216216216217, 0.3793103448275862, 0.2777777777777778, 0.23333333333333334, 0.28125, 0.11538461538461539, 0.15, 0.1111111111111111, 0.05, 0.0, 0.125, 0.2631578947368421, 0.16666666666666666, 0.1111111111111111, 0.06818181818181818, 0.4583333333333333, 0.42857142857142855, 0.24, 0.13636363636363635, 0.0, 0.14285714285714285, 0.28, 0.18181818181818182, 0.16, 0.041666666666666664, 0.06896551724137931, 0.05970149253731343, 0.09523809523809523, 0.13924050632911392, 0.044444444444444446, 0.21739130434782608, 0.3793103448275862, 0.26666666666666666, 0.1111111111111111, 0.09090909090909091, 0.15625, 0.14285714285714285, 0.1388888888888889, 0.1111111111111111, 0.16666666666666666, 0.2608695652173913, 0.23529411764705882, 0.08695652173913043, 0.12698412698412698, 0.18181818181818182, 0.031746031746031744, 0.16, 0.23809523809523808, 0.3076923076923077, 0.09090909090909091, 0.12121212121212122, 0.1, 0.07575757575757576, 0.23076923076923078, 0.35714285714285715, 0.38461538461538464, 0.13043478260869565, 0.08333333333333333, 0.07894736842105263, 0.04, 0.14285714285714285, 0.18181818181818182, 0.15625, 0.10714285714285714, 0.05, 0.05084745762711865, 0.1724137931034483, 0.034482758620689655, 0.058823529411764705, 0.04, 0.15789473684210525, 0.05555555555555555, 0.05, 0.0, 0.09090909090909091, 0.09375, 0.16666666666666666, 0.15384615384615385, 0.19230769230769232, 0.07407407407407407, 0.1388888888888889, 0.0625, 0.14705882352941177, 0.13636363636363635, 0.25, 0.125, 0.12, 0.2, 0.2, 0.08333333333333333, 0.2916666666666667, 0.0, 0.23809523809523808, 0.15384615384615385, 0.06060606060606061, 0.18181818181818182, 0.0, 0.21052631578947367, 0.1111111111111111, 0.043478260869565216, 0.14814814814814814, 0.0, 0.17391304347826086, 0.045454545454545456, 0.18181818181818182, 0.1935483870967742, 0.09433962264150944, 0.08571428571428572, 0.1590909090909091, 0.2222222222222222], 'fmeasure': [0.03076923076923077, 0.02531645569620253, 0.0, 0.11940298507462686, 0.21333333333333335, 0.12307692307692307, 0.17777777777777778, 0.0, 0.18749999999999997, 0.08823529411764705, 0.1095890410958904, 0.20895522388059704, 0.09090909090909091, 0.16666666666666666, 0.0, 0.05333333333333334, 0.17500000000000002, 0.11320754716981132, 0.18918918918918917, 0.060606060606060615, 0.058823529411764705, 0.052631578947368425, 0.07142857142857144, 0.0, 0.11428571428571428, 0.07317073170731707, 0.16, 0.1408450704225352, 0.14285714285714285, 0.0784313725490196, 0.16666666666666666, 0.208955223880597, 0.08955223880597016, 0.10126582278481013, 0.11904761904761903, 0.07407407407407407, 0.07692307692307693, 0.12000000000000001, 0.1584158415841584, 0.15000000000000002, 0.08695652173913043, 0.044444444444444446, 0.13999999999999999, 0.23157894736842108, 0.07317073170731708, 0.13559322033898305, 0.05194805194805195, 0.1647058823529412, 0.0821917808219178, 0.1492537313432836, 0.10526315789473684, 0.06153846153846155, 0.25396825396825395, 0.136986301369863, 0.045454545454545456, 0.18181818181818182, 0.08108108108108107, 0.13114754098360656, 0.031746031746031744, 0.1111111111111111, 0.07228915662650602, 0.0, 0.10256410256410256, 0.1090909090909091, 0.1964285714285714, 0.15384615384615383, 0.07894736842105264, 0.14117647058823532, 0.0, 0.12903225806451615, 0.10752688172043011, 0.03921568627450981, 0.10526315789473684, 0.09523809523809523, 0.0975609756097561, 0.041666666666666664, 0.10126582278481013, 0.18390804597701152, 0.08247422680412371, 0.11382113821138212, 0.12048192771084339, 0.07407407407407407, 0.0888888888888889, 0.0821917808219178, 0.052631578947368425, 0.16, 0.29729729729729726, 0.12987012987012986, 0.20588235294117646, 0.169811320754717, 0.07894736842105263, 0.08, 0.08450704225352113, 0.042105263157894736, 0.0, 0.14492753623188406, 0.10416666666666667, 0.12121212121212123, 0.09090909090909093, 0.09999999999999999, 0.2178217821782178, 0.125, 0.1846153846153846, 0.10714285714285714, 0.0, 0.12307692307692308, 0.1590909090909091, 0.1518987341772152, 0.13114754098360656, 0.02702702702702703, 0.05405405405405405, 0.07920792079207921, 0.06779661016949153, 0.176, 0.034782608695652174, 0.1941747572815534, 0.12643678160919541, 0.1839080459770115, 0.06741573033707865, 0.09302325581395349, 0.15625, 0.08, 0.16260162601626016, 0.09090909090909093, 0.1728395061728395, 0.15584415584415584, 0.16326530612244897, 0.12307692307692308, 0.16666666666666666, 0.2222222222222222, 0.047058823529411764, 0.09523809523809523, 0.15625, 0.2222222222222222, 0.034482758620689655, 0.10126582278481013, 0.07142857142857142, 0.09803921568627452, 0.1714285714285714, 0.2857142857142857, 0.13157894736842105, 0.06976744186046512, 0.03389830508474576, 0.08450704225352113, 0.024999999999999998, 0.05555555555555555, 0.14035087719298245, 0.13157894736842105, 0.0821917808219178, 0.04081632653061224, 0.05825242718446602, 0.12820512820512822, 0.03174603174603175, 0.049999999999999996, 0.03703703703703704, 0.08333333333333333, 0.025974025974025976, 0.029411764705882353, 0.0, 0.0449438202247191, 0.10909090909090909, 0.14634146341463414, 0.06779661016949153, 0.125, 0.06349206349206349, 0.125, 0.07547169811320754, 0.10309278350515463, 0.07058823529411765, 0.20253164556962025, 0.0816326530612245, 0.075, 0.12048192771084339, 0.14545454545454545, 0.03571428571428572, 0.2028985507246377, 0.0, 0.15384615384615383, 0.06451612903225806, 0.05970149253731344, 0.17777777777777776, 0.0, 0.11111111111111112, 0.05194805194805195, 0.028169014084507043, 0.125, 0.0, 0.17391304347826086, 0.0392156862745098, 0.1348314606741573, 0.17391304347826086, 0.10204081632653061, 0.06976744186046513, 0.14285714285714285, 0.20408163265306123]}\n",
            "Average for precision is 0.08950770644369475\n",
            "Average for recall is 0.15573257262080817\n",
            "Average for fmeasure is 0.10613472555631116\n",
            "/content/drive/MyDrive/Colab Notebooks/data/testdata/JointGAN_ Multi-Domain Joint Distribution Learning with  Generative Adversarial Nets.csv\n",
            "['This work was done when Yunchen Pu, Zhe Gan and Yizhe Zhang were Ph.D. students at Duke University.', 'There has been recent interest in employing GAN ideas to learn conditional distributions for two random variables.', 'Example applications include generative models with (stochastic) latent variables (Mescheder et al., 2017; Tao et al., 2018), and conditional data synthesis (Isola et al., 2017; Reed et al., 2016), when both domains consist of observed pairs of random variables.\\n', 'In this paper we focus on learning the joint distribution of multiple random variables using adversarial training.', 'For the case of two random variables, conditional GAN (Mirza & Osindero, 2014) and Triangle GAN (Gan et al., 2017a) have been utilized for this task in the case that paired data are available.', 'These models are unified as the joint distribution matching problem by Li et al. (2017a).', 'However, in all previous approaches the joint distributions are not fully learned, i.e., the model only learns to sample from the conditional distributions, assuming access to the marginal distributions, which are typically instantiated as empirical samples from each individual domain (see Figure 1(b) for illustration).', 'The resulting model may then be employed in several distinct applications: (i) synthesis of draws from any of the marginals; (ii) synthesis of draws from the conditionals when other random variables are observed, i.e., imputation; (iii) or we may simultaneously draw all random variables from the joint distribution.\\n', 'Unlike existing models, the proposed framework learns marginals and conditionals simultaneously.', 'As in GAN, p( 2) and p( ′2) are two simple distributions that provide the stochasticity when generating y given x, and vice versa.', 'By “supervised” it is meant that we have access to joint empirical data (x,y), and by “unsupervised” it is meant that we have access to empirical draws ofx and y, but not paired observations (x,y) from the joint distribution.', 'Compared with using multiple binary classifiers, this design is more principled in that we avoid multiple critics resulting in possibly conflicting (real vs. synthetic) assessments.\\n', 'In this case, the discriminator becomes a 4-class classifier.', 'One must have access to all the six instantiations of these models, if the goal is to be able to generate (impute) samples from all conditionals.', 'Adam (Kingma & Ba, 2014) with learning rate 0.0002 is utilized for optimization of the JointGAN objectives.', 'We use relevance score to evaluate the quality and relevance of two generated images.', 'The relevance score is calculated as the cosine similarity between two images that are embedded into a shared latent space, which are learned via training a ranking model (Huang et al., 2013).', 'It demonstrates that this metric correlates well with the quality of generated image pairs.', 'Since generating realistic text using GAN itself is a challenging task, in this work, we train our model on pairs of caption features and images.', 'The caption features are obtained from a pretrained word-level CNN-LSTM autoencoder (Gan et al., 2017b), which aims to achieve a one-to-one mapping between the captions and the features.', 'We then train JointGAN based on the caption features and their corresponding images (the paired data for training JointGAN use CNN-generated text\\nfeatures, which avoids issues of training GAN for text generation).', 'Finally to visualize the results, we use the pretrained LSTM decoder to decode the generated features back to captions.', 'We employ StackGAN-stage-I (Zhang et al., 2017a) for generating images from caption features while a CNN is\\nutilized to generate caption features from images.', 'The results show high-quality and diverse image generation, and strong coherent relationship between each pair of the caption feature and image.', 'It demonstrates the robustness of our model, in that it not only generates realistic multidomain images but also handles well different datasets such as caption feature and image pairs.', 'We propose JointGAN, a new framework for multi-domain joint distribution learning.', 'The joint distribution is learned via decomposing it into the product of a marginal and a conditional distribution(s), each learned via adversarial training.', 'JointGAN allows interesting applications since it provides freedom to draw samples from various marginalized or conditional distributions.', 'We consider joint analysis of two and three domains, and demonstrate that JointGAN achieves significantly better results than a two-step baseline model, both qualitatively and quantitatively.']\n",
            "179/179 [==============================] - 133s 740ms/step\n",
            "{'precision': [0.021739130434782608, 0.021739130434782608, 0.0, 0.07692307692307693, 0.16, 0.09523809523809523, 0.10810810810810811, 0.0, 0.13636363636363635, 0.057692307692307696, 0.08, 0.16666666666666666, 0.08108108108108109, 0.125, 0.0, 0.03508771929824561, 0.1346153846153846, 0.08108108108108109, 0.16666666666666666, 0.038461538461538464, 0.058823529411764705, 0.030303030303030304, 0.05, 0.0, 0.07017543859649122, 0.06818181818181818, 0.13043478260869565, 0.09259259259259259, 0.09523809523809523, 0.06666666666666667, 0.13157894736842105, 0.14285714285714285, 0.07317073170731707, 0.12903225806451613, 0.08064516129032258, 0.046511627906976744, 0.05263157894736842, 0.07792207792207792, 0.14814814814814814, 0.0967741935483871, 0.06382978723404255, 0.02564102564102564, 0.08433734939759036, 0.2391304347826087, 0.057692307692307696, 0.1111111111111111, 0.03225806451612903, 0.1320754716981132, 0.05263157894736842, 0.11363636363636363, 0.08163265306122448, 0.05128205128205128, 0.20512820512820512, 0.09803921568627451, 0.03389830508474576, 0.24324324324324326, 0.07142857142857142, 0.1111111111111111, 0.023255813953488372, 0.08333333333333333, 0.05357142857142857, 0.0, 0.11428571428571428, 0.07692307692307693, 0.15942028985507245, 0.1111111111111111, 0.05, 0.11538461538461539, 0.0, 0.0975609756097561, 0.09090909090909091, 0.024390243902439025, 0.11428571428571428, 0.09375, 0.07142857142857142, 0.034482758620689655, 0.09523809523809523, 0.15384615384615385, 0.06896551724137931, 0.07526881720430108, 0.09259259259259259, 0.05128205128205128, 0.07547169811320754, 0.05172413793103448, 0.05, 0.15789473684210525, 0.24444444444444444, 0.0847457627118644, 0.18421052631578946, 0.12162162162162163, 0.06, 0.05454545454545454, 0.06818181818181818, 0.03636363636363636, 0.0, 0.1724137931034483, 0.06493506493506493, 0.09523809523809523, 0.07692307692307693, 0.1875, 0.14285714285714285, 0.07317073170731707, 0.15, 0.08823529411764706, 0.0, 0.10810810810810811, 0.1111111111111111, 0.13043478260869565, 0.1111111111111111, 0.02, 0.044444444444444446, 0.11764705882352941, 0.05263157894736842, 0.2391304347826087, 0.02857142857142857, 0.17543859649122806, 0.07586206896551724, 0.14035087719298245, 0.04838709677419355, 0.09523809523809523, 0.15625, 0.05555555555555555, 0.19607843137254902, 0.07692307692307693, 0.1794871794871795, 0.1111111111111111, 0.125, 0.21052631578947367, 0.24242424242424243, 0.2857142857142857, 0.09090909090909091, 0.06779661016949153, 0.11627906976744186, 0.17391304347826086, 0.02127659574468085, 0.08695652173913043, 0.05555555555555555, 0.1388888888888889, 0.13636363636363635, 0.23809523809523808, 0.07936507936507936, 0.047619047619047616, 0.02127659574468085, 0.09090909090909091, 0.01818181818181818, 0.034482758620689655, 0.11428571428571428, 0.11363636363636363, 0.06666666666666667, 0.034482758620689655, 0.06818181818181818, 0.10204081632653061, 0.029411764705882353, 0.043478260869565216, 0.034482758620689655, 0.05660377358490566, 0.01694915254237288, 0.020833333333333332, 0.0, 0.029850746268656716, 0.13043478260869565, 0.13043478260869565, 0.043478260869565216, 0.09259259259259259, 0.05555555555555555, 0.11363636363636363, 0.09523809523809523, 0.07936507936507936, 0.047619047619047616, 0.1702127659574468, 0.06060606060606061, 0.05454545454545454, 0.08620689655172414, 0.11428571428571428, 0.022727272727272728, 0.15555555555555556, 0.0, 0.11363636363636363, 0.04081632653061224, 0.058823529411764705, 0.17391304347826086, 0.0, 0.07547169811320754, 0.03389830508474576, 0.020833333333333332, 0.10810810810810811, 0.0, 0.17391304347826086, 0.034482758620689655, 0.10714285714285714, 0.15789473684210525, 0.1111111111111111, 0.058823529411764705, 0.12962962962962962, 0.18867924528301888, 0.07142857142857142, 0.05128205128205128, 0.04878048780487805, 0.04878048780487805, 0.06349206349206349, 0.05172413793103448, 0.08163265306122448, 0.16666666666666666, 0.020833333333333332, 0.08571428571428572, 0.16666666666666666, 0.03488372093023256, 0.0297029702970297, 0.11538461538461539, 0.03488372093023256, 0.05319148936170213, 0.046511627906976744, 0.05714285714285714, 0.16216216216216217, 0.1, 0.11428571428571428, 0.0, 0.045454545454545456, 0.09836065573770492, 0.13513513513513514, 0.05714285714285714, 0.05102040816326531, 0.0, 0.05], 'recall': [0.05263157894736842, 0.030303030303030304, 0.0, 0.26666666666666666, 0.32, 0.17391304347826086, 0.5, 0.0, 0.3, 0.1875, 0.17391304347826086, 0.28, 0.10344827586206896, 0.25, 0.0, 0.1111111111111111, 0.25, 0.1875, 0.21875, 0.14285714285714285, 0.058823529411764705, 0.2, 0.125, 0.0, 0.3076923076923077, 0.07894736842105263, 0.20689655172413793, 0.29411764705882354, 0.2857142857142857, 0.09523809523809523, 0.22727272727272727, 0.3888888888888889, 0.11538461538461539, 0.08333333333333333, 0.22727272727272727, 0.18181818181818182, 0.14285714285714285, 0.2608695652173913, 0.1702127659574468, 0.3333333333333333, 0.13636363636363635, 0.16666666666666666, 0.4117647058823529, 0.22448979591836735, 0.1, 0.17391304347826086, 0.13333333333333333, 0.21875, 0.1875, 0.21739130434782608, 0.14814814814814814, 0.07692307692307693, 0.3333333333333333, 0.22727272727272727, 0.06896551724137931, 0.14516129032258066, 0.09375, 0.16, 0.05, 0.16666666666666666, 0.1111111111111111, 0.0, 0.09302325581395349, 0.1875, 0.2558139534883721, 0.25, 0.1875, 0.18181818181818182, 0.0, 0.19047619047619047, 0.13157894736842105, 0.1, 0.0975609756097561, 0.0967741935483871, 0.15384615384615385, 0.05263157894736842, 0.10810810810810811, 0.22857142857142856, 0.10256410256410256, 0.23333333333333334, 0.1724137931034483, 0.13333333333333333, 0.10810810810810811, 0.2, 0.05555555555555555, 0.16216216216216217, 0.3793103448275862, 0.2777777777777778, 0.23333333333333334, 0.28125, 0.11538461538461539, 0.15, 0.1111111111111111, 0.05, 0.0, 0.125, 0.2631578947368421, 0.16666666666666666, 0.1111111111111111, 0.06818181818181818, 0.4583333333333333, 0.42857142857142855, 0.24, 0.13636363636363635, 0.0, 0.14285714285714285, 0.28, 0.18181818181818182, 0.16, 0.041666666666666664, 0.06896551724137931, 0.05970149253731343, 0.09523809523809523, 0.13924050632911392, 0.044444444444444446, 0.21739130434782608, 0.3793103448275862, 0.26666666666666666, 0.1111111111111111, 0.09090909090909091, 0.15625, 0.14285714285714285, 0.1388888888888889, 0.1111111111111111, 0.16666666666666666, 0.2608695652173913, 0.23529411764705882, 0.08695652173913043, 0.12698412698412698, 0.18181818181818182, 0.031746031746031744, 0.16, 0.23809523809523808, 0.3076923076923077, 0.09090909090909091, 0.12121212121212122, 0.1, 0.07575757575757576, 0.23076923076923078, 0.35714285714285715, 0.38461538461538464, 0.13043478260869565, 0.08333333333333333, 0.07894736842105263, 0.04, 0.14285714285714285, 0.18181818181818182, 0.15625, 0.10714285714285714, 0.05, 0.05084745762711865, 0.1724137931034483, 0.034482758620689655, 0.058823529411764705, 0.04, 0.15789473684210525, 0.05555555555555555, 0.05, 0.0, 0.09090909090909091, 0.09375, 0.16666666666666666, 0.15384615384615385, 0.19230769230769232, 0.07407407407407407, 0.1388888888888889, 0.0625, 0.14705882352941177, 0.13636363636363635, 0.25, 0.125, 0.12, 0.2, 0.2, 0.08333333333333333, 0.2916666666666667, 0.0, 0.23809523809523808, 0.15384615384615385, 0.06060606060606061, 0.18181818181818182, 0.0, 0.21052631578947367, 0.1111111111111111, 0.043478260869565216, 0.14814814814814814, 0.0, 0.17391304347826086, 0.045454545454545456, 0.18181818181818182, 0.1935483870967742, 0.09433962264150944, 0.08571428571428572, 0.1590909090909091, 0.2222222222222222, 0.10526315789473684, 0.11764705882352941, 0.05128205128205128, 0.11764705882352941, 0.12121212121212122, 0.2, 0.0851063829787234, 0.22448979591836735, 0.09090909090909091, 0.25, 0.14634146341463414, 0.12, 0.3, 0.23076923076923078, 0.17647058823529413, 0.35714285714285715, 0.0625, 0.2857142857142857, 0.24, 0.0625, 0.12121212121212122, 0.0, 0.07692307692307693, 0.2727272727272727, 0.1724137931034483, 0.16666666666666666, 0.20833333333333334, 0.0, 0.07407407407407407], 'fmeasure': [0.03076923076923077, 0.02531645569620253, 0.0, 0.11940298507462686, 0.21333333333333335, 0.12307692307692307, 0.17777777777777778, 0.0, 0.18749999999999997, 0.08823529411764705, 0.1095890410958904, 0.20895522388059704, 0.09090909090909091, 0.16666666666666666, 0.0, 0.05333333333333334, 0.17500000000000002, 0.11320754716981132, 0.18918918918918917, 0.060606060606060615, 0.058823529411764705, 0.052631578947368425, 0.07142857142857144, 0.0, 0.11428571428571428, 0.07317073170731707, 0.16, 0.1408450704225352, 0.14285714285714285, 0.0784313725490196, 0.16666666666666666, 0.208955223880597, 0.08955223880597016, 0.10126582278481013, 0.11904761904761903, 0.07407407407407407, 0.07692307692307693, 0.12000000000000001, 0.1584158415841584, 0.15000000000000002, 0.08695652173913043, 0.044444444444444446, 0.13999999999999999, 0.23157894736842108, 0.07317073170731708, 0.13559322033898305, 0.05194805194805195, 0.1647058823529412, 0.0821917808219178, 0.1492537313432836, 0.10526315789473684, 0.06153846153846155, 0.25396825396825395, 0.136986301369863, 0.045454545454545456, 0.18181818181818182, 0.08108108108108107, 0.13114754098360656, 0.031746031746031744, 0.1111111111111111, 0.07228915662650602, 0.0, 0.10256410256410256, 0.1090909090909091, 0.1964285714285714, 0.15384615384615383, 0.07894736842105264, 0.14117647058823532, 0.0, 0.12903225806451615, 0.10752688172043011, 0.03921568627450981, 0.10526315789473684, 0.09523809523809523, 0.0975609756097561, 0.041666666666666664, 0.10126582278481013, 0.18390804597701152, 0.08247422680412371, 0.11382113821138212, 0.12048192771084339, 0.07407407407407407, 0.0888888888888889, 0.0821917808219178, 0.052631578947368425, 0.16, 0.29729729729729726, 0.12987012987012986, 0.20588235294117646, 0.169811320754717, 0.07894736842105263, 0.08, 0.08450704225352113, 0.042105263157894736, 0.0, 0.14492753623188406, 0.10416666666666667, 0.12121212121212123, 0.09090909090909093, 0.09999999999999999, 0.2178217821782178, 0.125, 0.1846153846153846, 0.10714285714285714, 0.0, 0.12307692307692308, 0.1590909090909091, 0.1518987341772152, 0.13114754098360656, 0.02702702702702703, 0.05405405405405405, 0.07920792079207921, 0.06779661016949153, 0.176, 0.034782608695652174, 0.1941747572815534, 0.12643678160919541, 0.1839080459770115, 0.06741573033707865, 0.09302325581395349, 0.15625, 0.08, 0.16260162601626016, 0.09090909090909093, 0.1728395061728395, 0.15584415584415584, 0.16326530612244897, 0.12307692307692308, 0.16666666666666666, 0.2222222222222222, 0.047058823529411764, 0.09523809523809523, 0.15625, 0.2222222222222222, 0.034482758620689655, 0.10126582278481013, 0.07142857142857142, 0.09803921568627452, 0.1714285714285714, 0.2857142857142857, 0.13157894736842105, 0.06976744186046512, 0.03389830508474576, 0.08450704225352113, 0.024999999999999998, 0.05555555555555555, 0.14035087719298245, 0.13157894736842105, 0.0821917808219178, 0.04081632653061224, 0.05825242718446602, 0.12820512820512822, 0.03174603174603175, 0.049999999999999996, 0.03703703703703704, 0.08333333333333333, 0.025974025974025976, 0.029411764705882353, 0.0, 0.0449438202247191, 0.10909090909090909, 0.14634146341463414, 0.06779661016949153, 0.125, 0.06349206349206349, 0.125, 0.07547169811320754, 0.10309278350515463, 0.07058823529411765, 0.20253164556962025, 0.0816326530612245, 0.075, 0.12048192771084339, 0.14545454545454545, 0.03571428571428572, 0.2028985507246377, 0.0, 0.15384615384615383, 0.06451612903225806, 0.05970149253731344, 0.17777777777777776, 0.0, 0.11111111111111112, 0.05194805194805195, 0.028169014084507043, 0.125, 0.0, 0.17391304347826086, 0.0392156862745098, 0.1348314606741573, 0.17391304347826086, 0.10204081632653061, 0.06976744186046513, 0.14285714285714285, 0.20408163265306123, 0.0851063829787234, 0.07142857142857141, 0.05, 0.06896551724137931, 0.08333333333333333, 0.0821917808219178, 0.08333333333333333, 0.19130434782608696, 0.03389830508474576, 0.1276595744680851, 0.15584415584415584, 0.05405405405405404, 0.05405405405405406, 0.15384615384615388, 0.058252427184466014, 0.09259259259259259, 0.05333333333333333, 0.09523809523809522, 0.1935483870967742, 0.07692307692307693, 0.11764705882352942, 0.0, 0.05714285714285715, 0.14457831325301204, 0.15151515151515152, 0.0851063829787234, 0.08196721311475412, 0.0, 0.05970149253731344]}\n",
            "Average for precision is 0.08703690465063019\n",
            "Average for recall is 0.15538034438489304\n",
            "Average for fmeasure is 0.10383409566753996\n",
            "/content/drive/MyDrive/Colab Notebooks/data/testdata/Knowledge transfer between speakers for personalised dialogue management.csv\n",
            "['Partially observable Markov decision processes (POMDP) (Young et al., 2013) are a popular framework to model dialogue management as a reinforcement learning (RL) problem.', 'In a POMDP, a state tracker (Thomson and Young, 2010)(Williams, 2014) maintains a distribution over possible user goals (states), called the belief state, and RL methods (Sutton and Barto,\\n1998) are used to optimize a metric called cumulative reward, a score that combines dialogue success rate and dialogue length.', 'Taking this idea into dialogue management, if a similarity metric is defined between different speakers, this metric can be used to select which data from the source speakers is used to train the model, and even to weight the influence of the data from each speaker in the model.', 'This paper investigates knowledge transfer between speakers in the context of a spoken environmental control system personalised for speakers with dysarthria (Christensen et al., 2013), where the ASR is adapted as speaker specific data is gathered (Christensen et al., 2012), thus improving the ASR performance with usage.', 'Section 3 presents the experimental setup of the environmental control system and the different dysarthric simulated users, as well as the different features used to define the speaker similarities.', 'The objective of a POMDP based dialogue manager is to find the policy π(b) = a that maximizes the expected cumulative reward ci defined as the sum of immediate rewards from time step i until the dialogue is finished, where a ∈ A is the action taken by the manager, and the belief state b is a probability distribution over a discrete set of states S .', 'If the random variables qt are assumed to have a joint Gaussian distribution with zero mean and ∆Q(bi, ai) ∼ N (0, σ2), the system can be modelled as a GP (Rasmussen and Williams, 2005), with the covariance matrix determined by a kernel function defined independently over the belief and the action space (Engel et al., 2005):\\nki,j = k((bi, ai), (bj , aj))', 'In the same way, when this kernel is used to compute the covariance vector between a new test point and the set Zt, as the new point x∗ = (b∗, a∗) lies in the belief-action space, it is redefined as z∗ = (b∗, a∗,b∗+1, a∗+1) with b∗+1 and a∗+1 set to default values.', 'Redefining the belief-action set of points Xt as the set of temporal difference points Zt also simplifies the selection of data points (e.g. to select inducing\\n2Take into account that |Zt| = |Xt| − 1\\npoints in sparse models), because the dependency between consecutive points is well defined.\\n', 'Once the posterior for any new belief-action point can be computed with eq. 7 or eq. 8, the policy π(b) = a can be computed as the action a that maximizes the Q-function from the current belief state b∗, but in order to avoid getting stuck in a local optimum, an exploration-exploitation approach should be taken.', 'et al. (2013 a) proposes to use the source points to train a prior GP, and use its posterior as mean function for the GP trained with the target points.', 'A third approach combines the two previous methods, using a portion of the transfer points to train a GP for the prior mean function, while the rest is used to initialise the set Zt of the GP that will be updated with target points.', 'The most straightforward way is to select the most similar points to the speaker from the transferred points.', 'The system has a vocabulary of 36 commands and is organised in a tree setup where each node in the tree represents either a device (e.g. “TV”), a property of that device (e.g. “channel”), or actions that trigger some change in one of the devices (e.g. “one”, child of “channel”, will change the TV to channel one).', 'Each non-terminal node in the tree is modelled as an independent POMDP where the state set S is the set of possible goals of the node and the action setA is the set of actions associated with each goal plus an “ask” action, which requests the user to repeat his last command.', 'The performance of DTC-int is way below the other two metrics, suggesting that the information given by intelligibility assessments is a weak feature for source speaker selection (as it is done by humans, it might be very noisy).', 'DTC-randspk selects the ordering of the speakers from whom the data is transferred at random, and has a much worse performance than the similarity based method, but DTC-allspk selects the 1000 source points from all the speakers, selecting 1000 points at random from the pool of 4200 points and, as it can be seen, the reward obtained by this method is slightly better than with DTC-iv, even if the success rate is lower.', 'This might be because weighting the data does a kind of data selection, as the data points from source speakers closer to the target will have more influence than the further ones, while transferring points from all the speakers covers a bigger part of the belief-action space.', 'Finally iv-allspk-hyb plots the performance of the hybrid model when selecting the data from all the speakers and weighting it with the i-vector based similarity.', 'Even if it is computationally cheaper, it outperforms iv-allspk after 100 dialogues, suggesting that with a good similarity metric and data selection method, the hybrid model in section 3.3 is the best option to take.', 'When transferring knowledge between speakers in a GP-RL based policy, weighting the data by using a similarity metric between speakers, and to a lesser extent, selecting the data using this similarity, improves the performance of the dialogue manager.', 'By defining a kernel between temporal difference points and interpreting the Q-function as a GP regression problem where data points are in the TD space, sparse methods that allow the selection of the subset of inducing points such as DTC can be applied.', 'We showed that using part of the transferred data to train a prior GP for the mean function,\\nand the rest to initialize the set of points of the GP, improves the performance of each of these approaches.', 'More computationally efficient ways to transfer the data could be studied.\\n', 'Of the three metrics based on speaker features tested (speaker intelligibility, i-vectors and ASR accuracy), i-vectors outperformed the rest.', 'This suggest that i-vectors are a potentially good feature for speaker specific dialogue management and could be used in other tasks such as state tracking.']\n",
            "58/58 [==============================] - 42s 731ms/step\n",
            "{'precision': [0.021739130434782608, 0.021739130434782608, 0.0, 0.07692307692307693, 0.16, 0.09523809523809523, 0.10810810810810811, 0.0, 0.13636363636363635, 0.057692307692307696, 0.08, 0.16666666666666666, 0.08108108108108109, 0.125, 0.0, 0.03508771929824561, 0.1346153846153846, 0.08108108108108109, 0.16666666666666666, 0.038461538461538464, 0.058823529411764705, 0.030303030303030304, 0.05, 0.0, 0.07017543859649122, 0.06818181818181818, 0.13043478260869565, 0.09259259259259259, 0.09523809523809523, 0.06666666666666667, 0.13157894736842105, 0.14285714285714285, 0.07317073170731707, 0.12903225806451613, 0.08064516129032258, 0.046511627906976744, 0.05263157894736842, 0.07792207792207792, 0.14814814814814814, 0.0967741935483871, 0.06382978723404255, 0.02564102564102564, 0.08433734939759036, 0.2391304347826087, 0.057692307692307696, 0.1111111111111111, 0.03225806451612903, 0.1320754716981132, 0.05263157894736842, 0.11363636363636363, 0.08163265306122448, 0.05128205128205128, 0.20512820512820512, 0.09803921568627451, 0.03389830508474576, 0.24324324324324326, 0.07142857142857142, 0.1111111111111111, 0.023255813953488372, 0.08333333333333333, 0.05357142857142857, 0.0, 0.11428571428571428, 0.07692307692307693, 0.15942028985507245, 0.1111111111111111, 0.05, 0.11538461538461539, 0.0, 0.0975609756097561, 0.09090909090909091, 0.024390243902439025, 0.11428571428571428, 0.09375, 0.07142857142857142, 0.034482758620689655, 0.09523809523809523, 0.15384615384615385, 0.06896551724137931, 0.07526881720430108, 0.09259259259259259, 0.05128205128205128, 0.07547169811320754, 0.05172413793103448, 0.05, 0.15789473684210525, 0.24444444444444444, 0.0847457627118644, 0.18421052631578946, 0.12162162162162163, 0.06, 0.05454545454545454, 0.06818181818181818, 0.03636363636363636, 0.0, 0.1724137931034483, 0.06493506493506493, 0.09523809523809523, 0.07692307692307693, 0.1875, 0.14285714285714285, 0.07317073170731707, 0.15, 0.08823529411764706, 0.0, 0.10810810810810811, 0.1111111111111111, 0.13043478260869565, 0.1111111111111111, 0.02, 0.044444444444444446, 0.11764705882352941, 0.05263157894736842, 0.2391304347826087, 0.02857142857142857, 0.17543859649122806, 0.07586206896551724, 0.14035087719298245, 0.04838709677419355, 0.09523809523809523, 0.15625, 0.05555555555555555, 0.19607843137254902, 0.07692307692307693, 0.1794871794871795, 0.1111111111111111, 0.125, 0.21052631578947367, 0.24242424242424243, 0.2857142857142857, 0.09090909090909091, 0.06779661016949153, 0.11627906976744186, 0.17391304347826086, 0.02127659574468085, 0.08695652173913043, 0.05555555555555555, 0.1388888888888889, 0.13636363636363635, 0.23809523809523808, 0.07936507936507936, 0.047619047619047616, 0.02127659574468085, 0.09090909090909091, 0.01818181818181818, 0.034482758620689655, 0.11428571428571428, 0.11363636363636363, 0.06666666666666667, 0.034482758620689655, 0.06818181818181818, 0.10204081632653061, 0.029411764705882353, 0.043478260869565216, 0.034482758620689655, 0.05660377358490566, 0.01694915254237288, 0.020833333333333332, 0.0, 0.029850746268656716, 0.13043478260869565, 0.13043478260869565, 0.043478260869565216, 0.09259259259259259, 0.05555555555555555, 0.11363636363636363, 0.09523809523809523, 0.07936507936507936, 0.047619047619047616, 0.1702127659574468, 0.06060606060606061, 0.05454545454545454, 0.08620689655172414, 0.11428571428571428, 0.022727272727272728, 0.15555555555555556, 0.0, 0.11363636363636363, 0.04081632653061224, 0.058823529411764705, 0.17391304347826086, 0.0, 0.07547169811320754, 0.03389830508474576, 0.020833333333333332, 0.10810810810810811, 0.0, 0.17391304347826086, 0.034482758620689655, 0.10714285714285714, 0.15789473684210525, 0.1111111111111111, 0.058823529411764705, 0.12962962962962962, 0.18867924528301888, 0.07142857142857142, 0.05128205128205128, 0.04878048780487805, 0.04878048780487805, 0.06349206349206349, 0.05172413793103448, 0.08163265306122448, 0.16666666666666666, 0.020833333333333332, 0.08571428571428572, 0.16666666666666666, 0.03488372093023256, 0.0297029702970297, 0.11538461538461539, 0.03488372093023256, 0.05319148936170213, 0.046511627906976744, 0.05714285714285714, 0.16216216216216217, 0.1, 0.11428571428571428, 0.0, 0.045454545454545456, 0.09836065573770492, 0.13513513513513514, 0.05714285714285714, 0.05102040816326531, 0.0, 0.05, 0.0, 0.11764705882352941, 0.11320754716981132, 0.14285714285714285, 0.06944444444444445, 0.125, 0.19148936170212766, 0.14102564102564102, 0.08823529411764706, 0.05970149253731343, 0.23333333333333334, 0.1487603305785124, 0.17142857142857143, 0.16216216216216217, 0.24444444444444444, 0.1836734693877551, 0.22641509433962265, 0.1282051282051282, 0.1, 0.125, 0.10869565217391304, 0.19718309859154928, 0.15, 0.027777777777777776, 0.027777777777777776, 0.09722222222222222], 'recall': [0.05263157894736842, 0.030303030303030304, 0.0, 0.26666666666666666, 0.32, 0.17391304347826086, 0.5, 0.0, 0.3, 0.1875, 0.17391304347826086, 0.28, 0.10344827586206896, 0.25, 0.0, 0.1111111111111111, 0.25, 0.1875, 0.21875, 0.14285714285714285, 0.058823529411764705, 0.2, 0.125, 0.0, 0.3076923076923077, 0.07894736842105263, 0.20689655172413793, 0.29411764705882354, 0.2857142857142857, 0.09523809523809523, 0.22727272727272727, 0.3888888888888889, 0.11538461538461539, 0.08333333333333333, 0.22727272727272727, 0.18181818181818182, 0.14285714285714285, 0.2608695652173913, 0.1702127659574468, 0.3333333333333333, 0.13636363636363635, 0.16666666666666666, 0.4117647058823529, 0.22448979591836735, 0.1, 0.17391304347826086, 0.13333333333333333, 0.21875, 0.1875, 0.21739130434782608, 0.14814814814814814, 0.07692307692307693, 0.3333333333333333, 0.22727272727272727, 0.06896551724137931, 0.14516129032258066, 0.09375, 0.16, 0.05, 0.16666666666666666, 0.1111111111111111, 0.0, 0.09302325581395349, 0.1875, 0.2558139534883721, 0.25, 0.1875, 0.18181818181818182, 0.0, 0.19047619047619047, 0.13157894736842105, 0.1, 0.0975609756097561, 0.0967741935483871, 0.15384615384615385, 0.05263157894736842, 0.10810810810810811, 0.22857142857142856, 0.10256410256410256, 0.23333333333333334, 0.1724137931034483, 0.13333333333333333, 0.10810810810810811, 0.2, 0.05555555555555555, 0.16216216216216217, 0.3793103448275862, 0.2777777777777778, 0.23333333333333334, 0.28125, 0.11538461538461539, 0.15, 0.1111111111111111, 0.05, 0.0, 0.125, 0.2631578947368421, 0.16666666666666666, 0.1111111111111111, 0.06818181818181818, 0.4583333333333333, 0.42857142857142855, 0.24, 0.13636363636363635, 0.0, 0.14285714285714285, 0.28, 0.18181818181818182, 0.16, 0.041666666666666664, 0.06896551724137931, 0.05970149253731343, 0.09523809523809523, 0.13924050632911392, 0.044444444444444446, 0.21739130434782608, 0.3793103448275862, 0.26666666666666666, 0.1111111111111111, 0.09090909090909091, 0.15625, 0.14285714285714285, 0.1388888888888889, 0.1111111111111111, 0.16666666666666666, 0.2608695652173913, 0.23529411764705882, 0.08695652173913043, 0.12698412698412698, 0.18181818181818182, 0.031746031746031744, 0.16, 0.23809523809523808, 0.3076923076923077, 0.09090909090909091, 0.12121212121212122, 0.1, 0.07575757575757576, 0.23076923076923078, 0.35714285714285715, 0.38461538461538464, 0.13043478260869565, 0.08333333333333333, 0.07894736842105263, 0.04, 0.14285714285714285, 0.18181818181818182, 0.15625, 0.10714285714285714, 0.05, 0.05084745762711865, 0.1724137931034483, 0.034482758620689655, 0.058823529411764705, 0.04, 0.15789473684210525, 0.05555555555555555, 0.05, 0.0, 0.09090909090909091, 0.09375, 0.16666666666666666, 0.15384615384615385, 0.19230769230769232, 0.07407407407407407, 0.1388888888888889, 0.0625, 0.14705882352941177, 0.13636363636363635, 0.25, 0.125, 0.12, 0.2, 0.2, 0.08333333333333333, 0.2916666666666667, 0.0, 0.23809523809523808, 0.15384615384615385, 0.06060606060606061, 0.18181818181818182, 0.0, 0.21052631578947367, 0.1111111111111111, 0.043478260869565216, 0.14814814814814814, 0.0, 0.17391304347826086, 0.045454545454545456, 0.18181818181818182, 0.1935483870967742, 0.09433962264150944, 0.08571428571428572, 0.1590909090909091, 0.2222222222222222, 0.10526315789473684, 0.11764705882352941, 0.05128205128205128, 0.11764705882352941, 0.12121212121212122, 0.2, 0.0851063829787234, 0.22448979591836735, 0.09090909090909091, 0.25, 0.14634146341463414, 0.12, 0.3, 0.23076923076923078, 0.17647058823529413, 0.35714285714285715, 0.0625, 0.2857142857142857, 0.24, 0.0625, 0.12121212121212122, 0.0, 0.07692307692307693, 0.2727272727272727, 0.1724137931034483, 0.16666666666666666, 0.20833333333333334, 0.0, 0.07407407407407407, 0.0, 0.12, 0.12244897959183673, 0.1276595744680851, 0.1724137931034483, 0.125, 0.140625, 0.19298245614035087, 0.0625, 0.06896551724137931, 0.4666666666666667, 0.4090909090909091, 0.3333333333333333, 0.1, 0.20754716981132076, 0.23076923076923078, 0.15789473684210525, 0.20833333333333334, 0.17857142857142858, 0.21621621621621623, 0.2564102564102564, 0.3181818181818182, 0.39473684210526316, 0.18181818181818182, 0.09523809523809523, 0.2692307692307692], 'fmeasure': [0.03076923076923077, 0.02531645569620253, 0.0, 0.11940298507462686, 0.21333333333333335, 0.12307692307692307, 0.17777777777777778, 0.0, 0.18749999999999997, 0.08823529411764705, 0.1095890410958904, 0.20895522388059704, 0.09090909090909091, 0.16666666666666666, 0.0, 0.05333333333333334, 0.17500000000000002, 0.11320754716981132, 0.18918918918918917, 0.060606060606060615, 0.058823529411764705, 0.052631578947368425, 0.07142857142857144, 0.0, 0.11428571428571428, 0.07317073170731707, 0.16, 0.1408450704225352, 0.14285714285714285, 0.0784313725490196, 0.16666666666666666, 0.208955223880597, 0.08955223880597016, 0.10126582278481013, 0.11904761904761903, 0.07407407407407407, 0.07692307692307693, 0.12000000000000001, 0.1584158415841584, 0.15000000000000002, 0.08695652173913043, 0.044444444444444446, 0.13999999999999999, 0.23157894736842108, 0.07317073170731708, 0.13559322033898305, 0.05194805194805195, 0.1647058823529412, 0.0821917808219178, 0.1492537313432836, 0.10526315789473684, 0.06153846153846155, 0.25396825396825395, 0.136986301369863, 0.045454545454545456, 0.18181818181818182, 0.08108108108108107, 0.13114754098360656, 0.031746031746031744, 0.1111111111111111, 0.07228915662650602, 0.0, 0.10256410256410256, 0.1090909090909091, 0.1964285714285714, 0.15384615384615383, 0.07894736842105264, 0.14117647058823532, 0.0, 0.12903225806451615, 0.10752688172043011, 0.03921568627450981, 0.10526315789473684, 0.09523809523809523, 0.0975609756097561, 0.041666666666666664, 0.10126582278481013, 0.18390804597701152, 0.08247422680412371, 0.11382113821138212, 0.12048192771084339, 0.07407407407407407, 0.0888888888888889, 0.0821917808219178, 0.052631578947368425, 0.16, 0.29729729729729726, 0.12987012987012986, 0.20588235294117646, 0.169811320754717, 0.07894736842105263, 0.08, 0.08450704225352113, 0.042105263157894736, 0.0, 0.14492753623188406, 0.10416666666666667, 0.12121212121212123, 0.09090909090909093, 0.09999999999999999, 0.2178217821782178, 0.125, 0.1846153846153846, 0.10714285714285714, 0.0, 0.12307692307692308, 0.1590909090909091, 0.1518987341772152, 0.13114754098360656, 0.02702702702702703, 0.05405405405405405, 0.07920792079207921, 0.06779661016949153, 0.176, 0.034782608695652174, 0.1941747572815534, 0.12643678160919541, 0.1839080459770115, 0.06741573033707865, 0.09302325581395349, 0.15625, 0.08, 0.16260162601626016, 0.09090909090909093, 0.1728395061728395, 0.15584415584415584, 0.16326530612244897, 0.12307692307692308, 0.16666666666666666, 0.2222222222222222, 0.047058823529411764, 0.09523809523809523, 0.15625, 0.2222222222222222, 0.034482758620689655, 0.10126582278481013, 0.07142857142857142, 0.09803921568627452, 0.1714285714285714, 0.2857142857142857, 0.13157894736842105, 0.06976744186046512, 0.03389830508474576, 0.08450704225352113, 0.024999999999999998, 0.05555555555555555, 0.14035087719298245, 0.13157894736842105, 0.0821917808219178, 0.04081632653061224, 0.05825242718446602, 0.12820512820512822, 0.03174603174603175, 0.049999999999999996, 0.03703703703703704, 0.08333333333333333, 0.025974025974025976, 0.029411764705882353, 0.0, 0.0449438202247191, 0.10909090909090909, 0.14634146341463414, 0.06779661016949153, 0.125, 0.06349206349206349, 0.125, 0.07547169811320754, 0.10309278350515463, 0.07058823529411765, 0.20253164556962025, 0.0816326530612245, 0.075, 0.12048192771084339, 0.14545454545454545, 0.03571428571428572, 0.2028985507246377, 0.0, 0.15384615384615383, 0.06451612903225806, 0.05970149253731344, 0.17777777777777776, 0.0, 0.11111111111111112, 0.05194805194805195, 0.028169014084507043, 0.125, 0.0, 0.17391304347826086, 0.0392156862745098, 0.1348314606741573, 0.17391304347826086, 0.10204081632653061, 0.06976744186046513, 0.14285714285714285, 0.20408163265306123, 0.0851063829787234, 0.07142857142857141, 0.05, 0.06896551724137931, 0.08333333333333333, 0.0821917808219178, 0.08333333333333333, 0.19130434782608696, 0.03389830508474576, 0.1276595744680851, 0.15584415584415584, 0.05405405405405404, 0.05405405405405406, 0.15384615384615388, 0.058252427184466014, 0.09259259259259259, 0.05333333333333333, 0.09523809523809522, 0.1935483870967742, 0.07692307692307693, 0.11764705882352942, 0.0, 0.05714285714285715, 0.14457831325301204, 0.15151515151515152, 0.0851063829787234, 0.08196721311475412, 0.0, 0.05970149253731344, 0.0, 0.1188118811881188, 0.11764705882352941, 0.1348314606741573, 0.09900990099009903, 0.125, 0.16216216216216214, 0.16296296296296295, 0.07317073170731707, 0.06399999999999999, 0.31111111111111117, 0.2181818181818182, 0.22641509433962265, 0.12371134020618559, 0.22448979591836737, 0.20454545454545456, 0.18604651162790695, 0.15873015873015872, 0.12820512820512822, 0.15841584158415842, 0.15267175572519084, 0.2434782608695652, 0.21739130434782605, 0.04819277108433734, 0.043010752688172046, 0.14285714285714288]}\n",
            "Average for precision is 0.09150781474736636\n",
            "Average for recall is 0.15984732580152028\n",
            "Average for fmeasure is 0.10841555132023777\n",
            "/content/drive/MyDrive/Colab Notebooks/data/testdata/Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders.csv\n",
            "['Instead, we get around the problem by applying the notion of depth, which has been wildly successful in improving the performance of neural networks, to direct memorization.', 'Therefore, as depth increases the specifics of the connectivity matters less and the network automatically becomes more stable with respect to the random choices made during construction.', 'but, as we saw, a slightly more complex model in the form of a network of support-limited lookup tables does significantly better than chance and is closer to the standard algorithms on a number of binary classification problems from MNIST and CIFAR-10.', 'Although direct memorization with a lookup table obviously does not generalize, we find that introducing depth in the form of a network of support-limited lookup tables leads to generalization that is significantly above chance and closer to those obtained by standard learning algorithms on several tasks derived from MNIST and CIFAR-10.', 'We observe that the weight differences between the standard and the attention model correlate with the differences between the standard and multitask model by a Pearson’s r of 0.346, averaged across datasets, with a standard deviation of 0.315; on individual datasets, correlation coefficient is as\\n5For the multi-task models, this analysis disregards those dimensions that do not correspond to classes in the main task.\\nhigh as 96.', 'Multi-task learning was shown to be effective for a variety of NLP tasks, such as POS tagging, chunking, named entity recognition (Collobert et al., 2011) or sentence compression (Klerke et al., 2016).', 'We believe that this analysis is a valuable contribution to the understanding of MTL approaches also beyond spelling normalization, and we are confident that our observations will stimulate further research into the relationship between MTL and attention.\\n', 'TransH (Wang et al., 2014b) introduced the relation-specific hyperplane to translate the entities.', 'In addition to that, we explored using automatically generated dictionaries as a shortcut to practical unsupervised learning.', 'In this regard, we propose a novel bidirectional filter generation mechanism to allow interactions between sentence pairs while constructing context-aware representations.\\n', 'To elucidate the role of different parts (modules) in our AdaQA model, we implement several model variants for comparison: (i) a “vanilla” CNN model that independently encodes two sentence representations for matching; (ii) a self-adaptive ACNN-based model where the question/answer sentence generates adaptive filters only to convolve with the input itself; (iii) a one-way ACNN model where only the answer sentence representation is extracted with adaptive filters, which\\nare generated conditioned on the question; (iv) a two-way AdaQA model as described in Section 2.4, where both sentences are adaptively encoded, with filters generated conditioned on the other sequence; (v) considering that the proposed filter generation mechanism is complementary to the attention layer typically employed in sequence matching tasks (see Section 3.4), we experiment with another model variant that combines the proposed context-aware filter generation mechanism with the multi-perspective attention layer introduced in (Wang et al., 2017b).\\n', 'Each dyadic conversation can be represented via three random variables: the dialog context c (context window size k − 1), the response utterance x (the kth utterance) and a latent variable z, which is used to capture the latent distribution over the valid responses.', '= p(x|z, c)p(z|c) and our goal is to use deep neural networks (parametrized by θ) to approximate p(z|c) and p(x|z, c).', 'We refer to pθ(z|c) as the prior network and pθ(x, |z, c) as the\\nresponse decoder.', 'Since we assume z follows isotropic Gaussian distribution, the recognition network qφ(z|x, c) ∼ N (µ, σ2I) and the prior network pθ(z|c) ∼ N (µ′, σ′2I), and then we have:\\n[ µ\\nlog(σ2)\\n', '+Eqφ(z|c,x,y)[log p(y|z, c)] (4)\\nSince now the reconstruction of y is a part of the loss function, kgCVAE can more efficiently encode y-related information into z than discovering it only based on the surface-level x and c. Another advantage of kgCVAE is that it can output a highlevel label (e.g. dialog act) along with the wordlevel responses, which allows easier interpretation of the model’s outputs.', 'A straightforward VAE with RNN decoder fails to encode meaningful information in z due to the vanishing latent variable problem (Bowman et al., 2015).', 'Bowman et al., (2015) proposed two solutions: (1) KL annealing: gradually increasing the weight of the KL term from 0 to 1 during training; (2) word drop decoding: setting a certain percentage of the target words to 0.', 'We used Glove embedding described in Section 4 and denote the average method as A-bow and extrema method as E-bow.\\n3.', 'On the contrary, the responses from the baseline model are limited to local n-gram variations and share a similar prefix, i.e. ”I’m”.', 'The kgCVAE model is also able to generate various ways of back-channeling.', 'To compare with past work (Bowman et al., 2015), we conducted the same language modelling (LM) task on Penn Treebank using VAE.', 'Last but not least, our experiments showed that the conclusions drawn from LM using VAE also apply to training CVAE/kgCVAE, so we used BOW loss together with KLA for all previous experiments.', 'In turn, the output of this novel neural dialog model will be easier to explain and control by humans.', 'All of the above suggest a promising research direction.', 'Variational Lower Bound for kgCVAE We assume that even with the presence of linguistic feature y regarding x, the prediction of xbow still only depends on the z and c. Therefore, we have:\\nL(θ, φ;x, c, y) = −KL(qφ(z|x, c, y)‖Pθ(z|c))', 'Secondly, for each dialog context in the test set, we retrieved the 10 nearest neighbors from the training set and treated the responses from the training set as candidate reference responses.', 'Thirdly, we further sampled 240 context-responses pairs from 5481 pairs in the total test set and post-processed the selected candidate responses by two human computational linguistic experts who were told to give a binary label for each candidate response about whether the response is appropriate regarding its dialog context.']\n",
            "68/68 [==============================] - 50s 732ms/step\n",
            "{'precision': [0.021739130434782608, 0.021739130434782608, 0.0, 0.07692307692307693, 0.16, 0.09523809523809523, 0.10810810810810811, 0.0, 0.13636363636363635, 0.057692307692307696, 0.08, 0.16666666666666666, 0.08108108108108109, 0.125, 0.0, 0.03508771929824561, 0.1346153846153846, 0.08108108108108109, 0.16666666666666666, 0.038461538461538464, 0.058823529411764705, 0.030303030303030304, 0.05, 0.0, 0.07017543859649122, 0.06818181818181818, 0.13043478260869565, 0.09259259259259259, 0.09523809523809523, 0.06666666666666667, 0.13157894736842105, 0.14285714285714285, 0.07317073170731707, 0.12903225806451613, 0.08064516129032258, 0.046511627906976744, 0.05263157894736842, 0.07792207792207792, 0.14814814814814814, 0.0967741935483871, 0.06382978723404255, 0.02564102564102564, 0.08433734939759036, 0.2391304347826087, 0.057692307692307696, 0.1111111111111111, 0.03225806451612903, 0.1320754716981132, 0.05263157894736842, 0.11363636363636363, 0.08163265306122448, 0.05128205128205128, 0.20512820512820512, 0.09803921568627451, 0.03389830508474576, 0.24324324324324326, 0.07142857142857142, 0.1111111111111111, 0.023255813953488372, 0.08333333333333333, 0.05357142857142857, 0.0, 0.11428571428571428, 0.07692307692307693, 0.15942028985507245, 0.1111111111111111, 0.05, 0.11538461538461539, 0.0, 0.0975609756097561, 0.09090909090909091, 0.024390243902439025, 0.11428571428571428, 0.09375, 0.07142857142857142, 0.034482758620689655, 0.09523809523809523, 0.15384615384615385, 0.06896551724137931, 0.07526881720430108, 0.09259259259259259, 0.05128205128205128, 0.07547169811320754, 0.05172413793103448, 0.05, 0.15789473684210525, 0.24444444444444444, 0.0847457627118644, 0.18421052631578946, 0.12162162162162163, 0.06, 0.05454545454545454, 0.06818181818181818, 0.03636363636363636, 0.0, 0.1724137931034483, 0.06493506493506493, 0.09523809523809523, 0.07692307692307693, 0.1875, 0.14285714285714285, 0.07317073170731707, 0.15, 0.08823529411764706, 0.0, 0.10810810810810811, 0.1111111111111111, 0.13043478260869565, 0.1111111111111111, 0.02, 0.044444444444444446, 0.11764705882352941, 0.05263157894736842, 0.2391304347826087, 0.02857142857142857, 0.17543859649122806, 0.07586206896551724, 0.14035087719298245, 0.04838709677419355, 0.09523809523809523, 0.15625, 0.05555555555555555, 0.19607843137254902, 0.07692307692307693, 0.1794871794871795, 0.1111111111111111, 0.125, 0.21052631578947367, 0.24242424242424243, 0.2857142857142857, 0.09090909090909091, 0.06779661016949153, 0.11627906976744186, 0.17391304347826086, 0.02127659574468085, 0.08695652173913043, 0.05555555555555555, 0.1388888888888889, 0.13636363636363635, 0.23809523809523808, 0.07936507936507936, 0.047619047619047616, 0.02127659574468085, 0.09090909090909091, 0.01818181818181818, 0.034482758620689655, 0.11428571428571428, 0.11363636363636363, 0.06666666666666667, 0.034482758620689655, 0.06818181818181818, 0.10204081632653061, 0.029411764705882353, 0.043478260869565216, 0.034482758620689655, 0.05660377358490566, 0.01694915254237288, 0.020833333333333332, 0.0, 0.029850746268656716, 0.13043478260869565, 0.13043478260869565, 0.043478260869565216, 0.09259259259259259, 0.05555555555555555, 0.11363636363636363, 0.09523809523809523, 0.07936507936507936, 0.047619047619047616, 0.1702127659574468, 0.06060606060606061, 0.05454545454545454, 0.08620689655172414, 0.11428571428571428, 0.022727272727272728, 0.15555555555555556, 0.0, 0.11363636363636363, 0.04081632653061224, 0.058823529411764705, 0.17391304347826086, 0.0, 0.07547169811320754, 0.03389830508474576, 0.020833333333333332, 0.10810810810810811, 0.0, 0.17391304347826086, 0.034482758620689655, 0.10714285714285714, 0.15789473684210525, 0.1111111111111111, 0.058823529411764705, 0.12962962962962962, 0.18867924528301888, 0.07142857142857142, 0.05128205128205128, 0.04878048780487805, 0.04878048780487805, 0.06349206349206349, 0.05172413793103448, 0.08163265306122448, 0.16666666666666666, 0.020833333333333332, 0.08571428571428572, 0.16666666666666666, 0.03488372093023256, 0.0297029702970297, 0.11538461538461539, 0.03488372093023256, 0.05319148936170213, 0.046511627906976744, 0.05714285714285714, 0.16216216216216217, 0.1, 0.11428571428571428, 0.0, 0.045454545454545456, 0.09836065573770492, 0.13513513513513514, 0.05714285714285714, 0.05102040816326531, 0.0, 0.05, 0.0, 0.11764705882352941, 0.11320754716981132, 0.14285714285714285, 0.06944444444444445, 0.125, 0.19148936170212766, 0.14102564102564102, 0.08823529411764706, 0.05970149253731343, 0.23333333333333334, 0.1487603305785124, 0.17142857142857143, 0.16216216216216217, 0.24444444444444444, 0.1836734693877551, 0.22641509433962265, 0.1282051282051282, 0.1, 0.125, 0.10869565217391304, 0.19718309859154928, 0.15, 0.027777777777777776, 0.027777777777777776, 0.09722222222222222, 0.1794871794871795, 0.09722222222222222, 0.2857142857142857, 0.09, 0.391304347826087, 0.07894736842105263, 0.13559322033898305, 0.0, 0.02564102564102564, 0.04819277108433735, 0.2830188679245283, 0.11290322580645161, 0.1111111111111111, 0.07272727272727272, 0.056818181818181816, 0.35135135135135137, 0.07692307692307693, 0.06896551724137931, 0.1282051282051282, 0.11764705882352941, 0.06060606060606061, 0.07407407407407407, 0.23333333333333334, 0.06, 0.03571428571428571, 0.25, 0.0547945205479452, 0.21666666666666667], 'recall': [0.05263157894736842, 0.030303030303030304, 0.0, 0.26666666666666666, 0.32, 0.17391304347826086, 0.5, 0.0, 0.3, 0.1875, 0.17391304347826086, 0.28, 0.10344827586206896, 0.25, 0.0, 0.1111111111111111, 0.25, 0.1875, 0.21875, 0.14285714285714285, 0.058823529411764705, 0.2, 0.125, 0.0, 0.3076923076923077, 0.07894736842105263, 0.20689655172413793, 0.29411764705882354, 0.2857142857142857, 0.09523809523809523, 0.22727272727272727, 0.3888888888888889, 0.11538461538461539, 0.08333333333333333, 0.22727272727272727, 0.18181818181818182, 0.14285714285714285, 0.2608695652173913, 0.1702127659574468, 0.3333333333333333, 0.13636363636363635, 0.16666666666666666, 0.4117647058823529, 0.22448979591836735, 0.1, 0.17391304347826086, 0.13333333333333333, 0.21875, 0.1875, 0.21739130434782608, 0.14814814814814814, 0.07692307692307693, 0.3333333333333333, 0.22727272727272727, 0.06896551724137931, 0.14516129032258066, 0.09375, 0.16, 0.05, 0.16666666666666666, 0.1111111111111111, 0.0, 0.09302325581395349, 0.1875, 0.2558139534883721, 0.25, 0.1875, 0.18181818181818182, 0.0, 0.19047619047619047, 0.13157894736842105, 0.1, 0.0975609756097561, 0.0967741935483871, 0.15384615384615385, 0.05263157894736842, 0.10810810810810811, 0.22857142857142856, 0.10256410256410256, 0.23333333333333334, 0.1724137931034483, 0.13333333333333333, 0.10810810810810811, 0.2, 0.05555555555555555, 0.16216216216216217, 0.3793103448275862, 0.2777777777777778, 0.23333333333333334, 0.28125, 0.11538461538461539, 0.15, 0.1111111111111111, 0.05, 0.0, 0.125, 0.2631578947368421, 0.16666666666666666, 0.1111111111111111, 0.06818181818181818, 0.4583333333333333, 0.42857142857142855, 0.24, 0.13636363636363635, 0.0, 0.14285714285714285, 0.28, 0.18181818181818182, 0.16, 0.041666666666666664, 0.06896551724137931, 0.05970149253731343, 0.09523809523809523, 0.13924050632911392, 0.044444444444444446, 0.21739130434782608, 0.3793103448275862, 0.26666666666666666, 0.1111111111111111, 0.09090909090909091, 0.15625, 0.14285714285714285, 0.1388888888888889, 0.1111111111111111, 0.16666666666666666, 0.2608695652173913, 0.23529411764705882, 0.08695652173913043, 0.12698412698412698, 0.18181818181818182, 0.031746031746031744, 0.16, 0.23809523809523808, 0.3076923076923077, 0.09090909090909091, 0.12121212121212122, 0.1, 0.07575757575757576, 0.23076923076923078, 0.35714285714285715, 0.38461538461538464, 0.13043478260869565, 0.08333333333333333, 0.07894736842105263, 0.04, 0.14285714285714285, 0.18181818181818182, 0.15625, 0.10714285714285714, 0.05, 0.05084745762711865, 0.1724137931034483, 0.034482758620689655, 0.058823529411764705, 0.04, 0.15789473684210525, 0.05555555555555555, 0.05, 0.0, 0.09090909090909091, 0.09375, 0.16666666666666666, 0.15384615384615385, 0.19230769230769232, 0.07407407407407407, 0.1388888888888889, 0.0625, 0.14705882352941177, 0.13636363636363635, 0.25, 0.125, 0.12, 0.2, 0.2, 0.08333333333333333, 0.2916666666666667, 0.0, 0.23809523809523808, 0.15384615384615385, 0.06060606060606061, 0.18181818181818182, 0.0, 0.21052631578947367, 0.1111111111111111, 0.043478260869565216, 0.14814814814814814, 0.0, 0.17391304347826086, 0.045454545454545456, 0.18181818181818182, 0.1935483870967742, 0.09433962264150944, 0.08571428571428572, 0.1590909090909091, 0.2222222222222222, 0.10526315789473684, 0.11764705882352941, 0.05128205128205128, 0.11764705882352941, 0.12121212121212122, 0.2, 0.0851063829787234, 0.22448979591836735, 0.09090909090909091, 0.25, 0.14634146341463414, 0.12, 0.3, 0.23076923076923078, 0.17647058823529413, 0.35714285714285715, 0.0625, 0.2857142857142857, 0.24, 0.0625, 0.12121212121212122, 0.0, 0.07692307692307693, 0.2727272727272727, 0.1724137931034483, 0.16666666666666666, 0.20833333333333334, 0.0, 0.07407407407407407, 0.0, 0.12, 0.12244897959183673, 0.1276595744680851, 0.1724137931034483, 0.125, 0.140625, 0.19298245614035087, 0.0625, 0.06896551724137931, 0.4666666666666667, 0.4090909090909091, 0.3333333333333333, 0.1, 0.20754716981132076, 0.23076923076923078, 0.15789473684210525, 0.20833333333333334, 0.17857142857142858, 0.21621621621621623, 0.2564102564102564, 0.3181818181818182, 0.39473684210526316, 0.18181818181818182, 0.09523809523809523, 0.2692307692307692, 0.25925925925925924, 0.25925925925925924, 0.18181818181818182, 0.16981132075471697, 0.2535211267605634, 0.09090909090909091, 0.21621621621621623, 0.0, 0.058823529411764705, 0.18181818181818182, 0.0967741935483871, 0.16279069767441862, 0.14285714285714285, 0.21052631578947367, 0.15625, 0.17105263157894737, 0.25, 0.10526315789473684, 0.21739130434782608, 0.16, 0.15384615384615385, 0.09090909090909091, 0.21212121212121213, 0.15789473684210525, 0.2222222222222222, 0.13043478260869565, 0.12903225806451613, 0.2549019607843137], 'fmeasure': [0.03076923076923077, 0.02531645569620253, 0.0, 0.11940298507462686, 0.21333333333333335, 0.12307692307692307, 0.17777777777777778, 0.0, 0.18749999999999997, 0.08823529411764705, 0.1095890410958904, 0.20895522388059704, 0.09090909090909091, 0.16666666666666666, 0.0, 0.05333333333333334, 0.17500000000000002, 0.11320754716981132, 0.18918918918918917, 0.060606060606060615, 0.058823529411764705, 0.052631578947368425, 0.07142857142857144, 0.0, 0.11428571428571428, 0.07317073170731707, 0.16, 0.1408450704225352, 0.14285714285714285, 0.0784313725490196, 0.16666666666666666, 0.208955223880597, 0.08955223880597016, 0.10126582278481013, 0.11904761904761903, 0.07407407407407407, 0.07692307692307693, 0.12000000000000001, 0.1584158415841584, 0.15000000000000002, 0.08695652173913043, 0.044444444444444446, 0.13999999999999999, 0.23157894736842108, 0.07317073170731708, 0.13559322033898305, 0.05194805194805195, 0.1647058823529412, 0.0821917808219178, 0.1492537313432836, 0.10526315789473684, 0.06153846153846155, 0.25396825396825395, 0.136986301369863, 0.045454545454545456, 0.18181818181818182, 0.08108108108108107, 0.13114754098360656, 0.031746031746031744, 0.1111111111111111, 0.07228915662650602, 0.0, 0.10256410256410256, 0.1090909090909091, 0.1964285714285714, 0.15384615384615383, 0.07894736842105264, 0.14117647058823532, 0.0, 0.12903225806451615, 0.10752688172043011, 0.03921568627450981, 0.10526315789473684, 0.09523809523809523, 0.0975609756097561, 0.041666666666666664, 0.10126582278481013, 0.18390804597701152, 0.08247422680412371, 0.11382113821138212, 0.12048192771084339, 0.07407407407407407, 0.0888888888888889, 0.0821917808219178, 0.052631578947368425, 0.16, 0.29729729729729726, 0.12987012987012986, 0.20588235294117646, 0.169811320754717, 0.07894736842105263, 0.08, 0.08450704225352113, 0.042105263157894736, 0.0, 0.14492753623188406, 0.10416666666666667, 0.12121212121212123, 0.09090909090909093, 0.09999999999999999, 0.2178217821782178, 0.125, 0.1846153846153846, 0.10714285714285714, 0.0, 0.12307692307692308, 0.1590909090909091, 0.1518987341772152, 0.13114754098360656, 0.02702702702702703, 0.05405405405405405, 0.07920792079207921, 0.06779661016949153, 0.176, 0.034782608695652174, 0.1941747572815534, 0.12643678160919541, 0.1839080459770115, 0.06741573033707865, 0.09302325581395349, 0.15625, 0.08, 0.16260162601626016, 0.09090909090909093, 0.1728395061728395, 0.15584415584415584, 0.16326530612244897, 0.12307692307692308, 0.16666666666666666, 0.2222222222222222, 0.047058823529411764, 0.09523809523809523, 0.15625, 0.2222222222222222, 0.034482758620689655, 0.10126582278481013, 0.07142857142857142, 0.09803921568627452, 0.1714285714285714, 0.2857142857142857, 0.13157894736842105, 0.06976744186046512, 0.03389830508474576, 0.08450704225352113, 0.024999999999999998, 0.05555555555555555, 0.14035087719298245, 0.13157894736842105, 0.0821917808219178, 0.04081632653061224, 0.05825242718446602, 0.12820512820512822, 0.03174603174603175, 0.049999999999999996, 0.03703703703703704, 0.08333333333333333, 0.025974025974025976, 0.029411764705882353, 0.0, 0.0449438202247191, 0.10909090909090909, 0.14634146341463414, 0.06779661016949153, 0.125, 0.06349206349206349, 0.125, 0.07547169811320754, 0.10309278350515463, 0.07058823529411765, 0.20253164556962025, 0.0816326530612245, 0.075, 0.12048192771084339, 0.14545454545454545, 0.03571428571428572, 0.2028985507246377, 0.0, 0.15384615384615383, 0.06451612903225806, 0.05970149253731344, 0.17777777777777776, 0.0, 0.11111111111111112, 0.05194805194805195, 0.028169014084507043, 0.125, 0.0, 0.17391304347826086, 0.0392156862745098, 0.1348314606741573, 0.17391304347826086, 0.10204081632653061, 0.06976744186046513, 0.14285714285714285, 0.20408163265306123, 0.0851063829787234, 0.07142857142857141, 0.05, 0.06896551724137931, 0.08333333333333333, 0.0821917808219178, 0.08333333333333333, 0.19130434782608696, 0.03389830508474576, 0.1276595744680851, 0.15584415584415584, 0.05405405405405404, 0.05405405405405406, 0.15384615384615388, 0.058252427184466014, 0.09259259259259259, 0.05333333333333333, 0.09523809523809522, 0.1935483870967742, 0.07692307692307693, 0.11764705882352942, 0.0, 0.05714285714285715, 0.14457831325301204, 0.15151515151515152, 0.0851063829787234, 0.08196721311475412, 0.0, 0.05970149253731344, 0.0, 0.1188118811881188, 0.11764705882352941, 0.1348314606741573, 0.09900990099009903, 0.125, 0.16216216216216214, 0.16296296296296295, 0.07317073170731707, 0.06399999999999999, 0.31111111111111117, 0.2181818181818182, 0.22641509433962265, 0.12371134020618559, 0.22448979591836737, 0.20454545454545456, 0.18604651162790695, 0.15873015873015872, 0.12820512820512822, 0.15841584158415842, 0.15267175572519084, 0.2434782608695652, 0.21739130434782605, 0.04819277108433734, 0.043010752688172046, 0.14285714285714288, 0.21212121212121213, 0.14141414141414144, 0.2222222222222222, 0.1176470588235294, 0.3076923076923077, 0.08450704225352113, 0.16666666666666669, 0.0, 0.035714285714285705, 0.07619047619047621, 0.14423076923076922, 0.13333333333333333, 0.125, 0.1081081081081081, 0.08333333333333333, 0.23008849557522124, 0.11764705882352941, 0.08333333333333333, 0.16129032258064516, 0.13559322033898305, 0.08695652173913043, 0.08163265306122448, 0.22222222222222224, 0.08695652173913043, 0.06153846153846154, 0.1714285714285714, 0.07692307692307691, 0.23423423423423423]}\n",
            "Average for precision is 0.0955896253253638\n",
            "Average for recall is 0.1606386168261746\n",
            "Average for fmeasure is 0.11083422115360113\n",
            "/content/drive/MyDrive/Colab Notebooks/data/testdata/Meta-Learning by Adjusting Priors Based on Extended PAC-Bayes Theory.csv\n",
            "['In summary, the evaluation phase of LOLS collects training data for a cost-sensitive classifier, where the\\ninputs (states), outputs (actions), and costs are obtained by interacting with the environment.', 'back.5\\n5Our implementation uses a slightly faster method which accumulates an “undo list” of changes that it makes to the chart to quickly revert the modified chart to the original roll-in state.\\n', 'In the case of parsing, given a performance criterion and a good baseline policy for that criterion, the learner consistently manages to find a higher-reward policy.', 'While conventional automatic speech recognition (ASR) systems have a long history and have recently made great strides thanks to the revival of deep neural networks (DNNs), their reliance on highly supervised training paradigms has essentially restricted their application to the major languages of the world, accounting for a small fraction of the more than 7,000 human languages spoken worldwide (Lewis et al., 2016).', 'Proceedings of NAACL-HLT 2018, pages 1090–1100 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics\\nMultilingual topic models enable document analysis across languages through coherent multilingual summaries of the data. However, there is no standard and effective metric to evaluate the quality of multilingual topics. We introduce a new intrinsic evaluation of multilingual topic models that correlates well with human judgments of multilingual topic coherence as well as performance in downstream applications. Importantly, we also study evaluation for low-resource languages. Because standard metrics fail to accurately measure topic quality when robust external resources are unavailable, we propose an adaptation model that improves the accuracy and reliability of these metrics in low-resource settings.', 'For a problem P, the\\nalgorithm must keep a subset of the rows of A and, upon reading the full input, may use a black-box solver to compute an approximate solution to P with only the subset of rows stored.', 'Bayesian inference entails specifying a prior distribution Π over f1(.) and f0(.), i.e.\\nf0, f1 ∼ Π(φ̄β0 , φ̄β1), (4)\\nwhere φ̄βw = {φkβw} ∞ k=1, w ∈ {0, 1}, are complete orthonormal bases (indexed by a parameter βw > 0) with respect to Lebesgue measure in X , fw = ∑ k f̄ k w ·φkβw , and f̄kw = ⟨fw, φkβw⟩.', 'The proposed LSPEs (often significantly) outperform all spectral initializers in terms of visual quality as well as the N-MSE.', 'While much previous work employs a pipelined approach to both POS tagging for dependency parsing and predicate detection for SRL, we take a multi-task learning (MTL) approach (Caruana,\\n3Usually the head emits a tree, but we do not enforce it here.\\n1993), sharing the parameters of earlier layers in our SRL model with a joint POS and predicate detection objective.', 'Note this requires Q ≥ √ K, leading to a speedup bounded by√\\nK. Similarly, for computing the active set A in step 4 of Algorithm 2, we can compute an appropriate threshold τ2 using the properties of the loss function.', 'It is interesting to examine the constants that multiply Z2 in some of the obtained asymptotic variance expressions for the different tricks.', 'Firstly for this choice of A we can construct an explicit, symplectic, leapfrog-like integration scheme which is important for developing an efficient HMC sampler as discussed in Section 3.1.', 'We work within the framework of Meta-Learning / Learning-toLearn / Inductive Transfer (Thrun & Pratt, 1997; Vilalta & Drissi, 2002) 1 in which a ‘meta-learner’ extracts knowledge from several observed tasks to facilitate the learning of new tasks by a ‘base-learner’ (see Figure 1).', 'The performance is evaluated when learning related new tasks (which are unavailable to the meta-learner) .\\n', 'Thus, new tasks can be learned using fewer examples than learning from scratch (e.g., Yosinski et al. (2014)).', 'The main contributions of this work are the following.', 'In the common setting for learning, a set of independent samples, S = {zi}mi=1, from a space of examples Z , is given, each sample drawn from an unknown probability distribution D, namely zi ∼ D. We will use the notation S ∼ Dm to denote the distribution over the full sample.', 'By choosing Q that minimizes the bound we obtain a learning algorithm with generalization guarantees.', 'and Si ∼ Dmii , i = 1, ..., n.\\nNotice that the transfer error (2) is bounded by the empirical multi-task error (3) plus two complexity terms.', 'This term converges to zero if infinite number of tasks is observed from the task environment (n→∞).', 'The first step differs from Pentina & Lampert (2014).', 'Therefore our bound is better adjusted the observed data set.\\n', 'In section A.1 we use McAllester’s bound (Theorem 1), which is tighter than the lemma used in Pentina & Lampert (2014).', 'Notice that Q appears in the bound (4) in two forms (i) divergence from the hyper-prior D(Q||P) and (ii) expectations over P ∼ Q.\\nBy setting the hyper-prior as zero-mean isotropic Gaussian, P = N ( 0, κ2PINP×NP ) , where κP > 0 is another constant, we get a simple form for the KL-divergence term, D(Qθ||P) = 12κ2P ‖θ‖ 2 2 .', 'Note that the hyper-prior acts as a regularization term which prefers solutions with small L2 norm.\\n', '(8)\\nThe optimization process is illustrated in Figure 2.', 'First, we define the hypothesis class H as a family of functions parameterized by a weight vector{ hw : w ∈ Rd } .', 'Given this parameterization, the posterior and prior are distributions over Rd.\\n']\n",
            "248/248 [==============================] - 184s 740ms/step\n",
            "{'precision': [0.021739130434782608, 0.021739130434782608, 0.0, 0.07692307692307693, 0.16, 0.09523809523809523, 0.10810810810810811, 0.0, 0.13636363636363635, 0.057692307692307696, 0.08, 0.16666666666666666, 0.08108108108108109, 0.125, 0.0, 0.03508771929824561, 0.1346153846153846, 0.08108108108108109, 0.16666666666666666, 0.038461538461538464, 0.058823529411764705, 0.030303030303030304, 0.05, 0.0, 0.07017543859649122, 0.06818181818181818, 0.13043478260869565, 0.09259259259259259, 0.09523809523809523, 0.06666666666666667, 0.13157894736842105, 0.14285714285714285, 0.07317073170731707, 0.12903225806451613, 0.08064516129032258, 0.046511627906976744, 0.05263157894736842, 0.07792207792207792, 0.14814814814814814, 0.0967741935483871, 0.06382978723404255, 0.02564102564102564, 0.08433734939759036, 0.2391304347826087, 0.057692307692307696, 0.1111111111111111, 0.03225806451612903, 0.1320754716981132, 0.05263157894736842, 0.11363636363636363, 0.08163265306122448, 0.05128205128205128, 0.20512820512820512, 0.09803921568627451, 0.03389830508474576, 0.24324324324324326, 0.07142857142857142, 0.1111111111111111, 0.023255813953488372, 0.08333333333333333, 0.05357142857142857, 0.0, 0.11428571428571428, 0.07692307692307693, 0.15942028985507245, 0.1111111111111111, 0.05, 0.11538461538461539, 0.0, 0.0975609756097561, 0.09090909090909091, 0.024390243902439025, 0.11428571428571428, 0.09375, 0.07142857142857142, 0.034482758620689655, 0.09523809523809523, 0.15384615384615385, 0.06896551724137931, 0.07526881720430108, 0.09259259259259259, 0.05128205128205128, 0.07547169811320754, 0.05172413793103448, 0.05, 0.15789473684210525, 0.24444444444444444, 0.0847457627118644, 0.18421052631578946, 0.12162162162162163, 0.06, 0.05454545454545454, 0.06818181818181818, 0.03636363636363636, 0.0, 0.1724137931034483, 0.06493506493506493, 0.09523809523809523, 0.07692307692307693, 0.1875, 0.14285714285714285, 0.07317073170731707, 0.15, 0.08823529411764706, 0.0, 0.10810810810810811, 0.1111111111111111, 0.13043478260869565, 0.1111111111111111, 0.02, 0.044444444444444446, 0.11764705882352941, 0.05263157894736842, 0.2391304347826087, 0.02857142857142857, 0.17543859649122806, 0.07586206896551724, 0.14035087719298245, 0.04838709677419355, 0.09523809523809523, 0.15625, 0.05555555555555555, 0.19607843137254902, 0.07692307692307693, 0.1794871794871795, 0.1111111111111111, 0.125, 0.21052631578947367, 0.24242424242424243, 0.2857142857142857, 0.09090909090909091, 0.06779661016949153, 0.11627906976744186, 0.17391304347826086, 0.02127659574468085, 0.08695652173913043, 0.05555555555555555, 0.1388888888888889, 0.13636363636363635, 0.23809523809523808, 0.07936507936507936, 0.047619047619047616, 0.02127659574468085, 0.09090909090909091, 0.01818181818181818, 0.034482758620689655, 0.11428571428571428, 0.11363636363636363, 0.06666666666666667, 0.034482758620689655, 0.06818181818181818, 0.10204081632653061, 0.029411764705882353, 0.043478260869565216, 0.034482758620689655, 0.05660377358490566, 0.01694915254237288, 0.020833333333333332, 0.0, 0.029850746268656716, 0.13043478260869565, 0.13043478260869565, 0.043478260869565216, 0.09259259259259259, 0.05555555555555555, 0.11363636363636363, 0.09523809523809523, 0.07936507936507936, 0.047619047619047616, 0.1702127659574468, 0.06060606060606061, 0.05454545454545454, 0.08620689655172414, 0.11428571428571428, 0.022727272727272728, 0.15555555555555556, 0.0, 0.11363636363636363, 0.04081632653061224, 0.058823529411764705, 0.17391304347826086, 0.0, 0.07547169811320754, 0.03389830508474576, 0.020833333333333332, 0.10810810810810811, 0.0, 0.17391304347826086, 0.034482758620689655, 0.10714285714285714, 0.15789473684210525, 0.1111111111111111, 0.058823529411764705, 0.12962962962962962, 0.18867924528301888, 0.07142857142857142, 0.05128205128205128, 0.04878048780487805, 0.04878048780487805, 0.06349206349206349, 0.05172413793103448, 0.08163265306122448, 0.16666666666666666, 0.020833333333333332, 0.08571428571428572, 0.16666666666666666, 0.03488372093023256, 0.0297029702970297, 0.11538461538461539, 0.03488372093023256, 0.05319148936170213, 0.046511627906976744, 0.05714285714285714, 0.16216216216216217, 0.1, 0.11428571428571428, 0.0, 0.045454545454545456, 0.09836065573770492, 0.13513513513513514, 0.05714285714285714, 0.05102040816326531, 0.0, 0.05, 0.0, 0.11764705882352941, 0.11320754716981132, 0.14285714285714285, 0.06944444444444445, 0.125, 0.19148936170212766, 0.14102564102564102, 0.08823529411764706, 0.05970149253731343, 0.23333333333333334, 0.1487603305785124, 0.17142857142857143, 0.16216216216216217, 0.24444444444444444, 0.1836734693877551, 0.22641509433962265, 0.1282051282051282, 0.1, 0.125, 0.10869565217391304, 0.19718309859154928, 0.15, 0.027777777777777776, 0.027777777777777776, 0.09722222222222222, 0.1794871794871795, 0.09722222222222222, 0.2857142857142857, 0.09, 0.391304347826087, 0.07894736842105263, 0.13559322033898305, 0.0, 0.02564102564102564, 0.04819277108433735, 0.2830188679245283, 0.11290322580645161, 0.1111111111111111, 0.07272727272727272, 0.056818181818181816, 0.35135135135135137, 0.07692307692307693, 0.06896551724137931, 0.1282051282051282, 0.11764705882352941, 0.06060606060606061, 0.07407407407407407, 0.23333333333333334, 0.06, 0.03571428571428571, 0.25, 0.0547945205479452, 0.21666666666666667, 0.14705882352941177, 0.20512820512820512, 0.19672131147540983, 0.07317073170731707, 0.25, 0.1794871794871795, 0.08333333333333333, 0.047619047619047616, 0.11956521739130435, 0.16129032258064516, 0.1702127659574468, 0.05, 0.06976744186046512, 0.109375, 0.0847457627118644, 0.06666666666666667, 0.25, 0.08163265306122448, 0.08888888888888889, 0.02040816326530612, 0.0196078431372549, 0.015625, 0.06, 0.21739130434782608, 0.06666666666666667, 0.08571428571428572, 0.09615384615384616, 0.023809523809523808], 'recall': [0.05263157894736842, 0.030303030303030304, 0.0, 0.26666666666666666, 0.32, 0.17391304347826086, 0.5, 0.0, 0.3, 0.1875, 0.17391304347826086, 0.28, 0.10344827586206896, 0.25, 0.0, 0.1111111111111111, 0.25, 0.1875, 0.21875, 0.14285714285714285, 0.058823529411764705, 0.2, 0.125, 0.0, 0.3076923076923077, 0.07894736842105263, 0.20689655172413793, 0.29411764705882354, 0.2857142857142857, 0.09523809523809523, 0.22727272727272727, 0.3888888888888889, 0.11538461538461539, 0.08333333333333333, 0.22727272727272727, 0.18181818181818182, 0.14285714285714285, 0.2608695652173913, 0.1702127659574468, 0.3333333333333333, 0.13636363636363635, 0.16666666666666666, 0.4117647058823529, 0.22448979591836735, 0.1, 0.17391304347826086, 0.13333333333333333, 0.21875, 0.1875, 0.21739130434782608, 0.14814814814814814, 0.07692307692307693, 0.3333333333333333, 0.22727272727272727, 0.06896551724137931, 0.14516129032258066, 0.09375, 0.16, 0.05, 0.16666666666666666, 0.1111111111111111, 0.0, 0.09302325581395349, 0.1875, 0.2558139534883721, 0.25, 0.1875, 0.18181818181818182, 0.0, 0.19047619047619047, 0.13157894736842105, 0.1, 0.0975609756097561, 0.0967741935483871, 0.15384615384615385, 0.05263157894736842, 0.10810810810810811, 0.22857142857142856, 0.10256410256410256, 0.23333333333333334, 0.1724137931034483, 0.13333333333333333, 0.10810810810810811, 0.2, 0.05555555555555555, 0.16216216216216217, 0.3793103448275862, 0.2777777777777778, 0.23333333333333334, 0.28125, 0.11538461538461539, 0.15, 0.1111111111111111, 0.05, 0.0, 0.125, 0.2631578947368421, 0.16666666666666666, 0.1111111111111111, 0.06818181818181818, 0.4583333333333333, 0.42857142857142855, 0.24, 0.13636363636363635, 0.0, 0.14285714285714285, 0.28, 0.18181818181818182, 0.16, 0.041666666666666664, 0.06896551724137931, 0.05970149253731343, 0.09523809523809523, 0.13924050632911392, 0.044444444444444446, 0.21739130434782608, 0.3793103448275862, 0.26666666666666666, 0.1111111111111111, 0.09090909090909091, 0.15625, 0.14285714285714285, 0.1388888888888889, 0.1111111111111111, 0.16666666666666666, 0.2608695652173913, 0.23529411764705882, 0.08695652173913043, 0.12698412698412698, 0.18181818181818182, 0.031746031746031744, 0.16, 0.23809523809523808, 0.3076923076923077, 0.09090909090909091, 0.12121212121212122, 0.1, 0.07575757575757576, 0.23076923076923078, 0.35714285714285715, 0.38461538461538464, 0.13043478260869565, 0.08333333333333333, 0.07894736842105263, 0.04, 0.14285714285714285, 0.18181818181818182, 0.15625, 0.10714285714285714, 0.05, 0.05084745762711865, 0.1724137931034483, 0.034482758620689655, 0.058823529411764705, 0.04, 0.15789473684210525, 0.05555555555555555, 0.05, 0.0, 0.09090909090909091, 0.09375, 0.16666666666666666, 0.15384615384615385, 0.19230769230769232, 0.07407407407407407, 0.1388888888888889, 0.0625, 0.14705882352941177, 0.13636363636363635, 0.25, 0.125, 0.12, 0.2, 0.2, 0.08333333333333333, 0.2916666666666667, 0.0, 0.23809523809523808, 0.15384615384615385, 0.06060606060606061, 0.18181818181818182, 0.0, 0.21052631578947367, 0.1111111111111111, 0.043478260869565216, 0.14814814814814814, 0.0, 0.17391304347826086, 0.045454545454545456, 0.18181818181818182, 0.1935483870967742, 0.09433962264150944, 0.08571428571428572, 0.1590909090909091, 0.2222222222222222, 0.10526315789473684, 0.11764705882352941, 0.05128205128205128, 0.11764705882352941, 0.12121212121212122, 0.2, 0.0851063829787234, 0.22448979591836735, 0.09090909090909091, 0.25, 0.14634146341463414, 0.12, 0.3, 0.23076923076923078, 0.17647058823529413, 0.35714285714285715, 0.0625, 0.2857142857142857, 0.24, 0.0625, 0.12121212121212122, 0.0, 0.07692307692307693, 0.2727272727272727, 0.1724137931034483, 0.16666666666666666, 0.20833333333333334, 0.0, 0.07407407407407407, 0.0, 0.12, 0.12244897959183673, 0.1276595744680851, 0.1724137931034483, 0.125, 0.140625, 0.19298245614035087, 0.0625, 0.06896551724137931, 0.4666666666666667, 0.4090909090909091, 0.3333333333333333, 0.1, 0.20754716981132076, 0.23076923076923078, 0.15789473684210525, 0.20833333333333334, 0.17857142857142858, 0.21621621621621623, 0.2564102564102564, 0.3181818181818182, 0.39473684210526316, 0.18181818181818182, 0.09523809523809523, 0.2692307692307692, 0.25925925925925924, 0.25925925925925924, 0.18181818181818182, 0.16981132075471697, 0.2535211267605634, 0.09090909090909091, 0.21621621621621623, 0.0, 0.058823529411764705, 0.18181818181818182, 0.0967741935483871, 0.16279069767441862, 0.14285714285714285, 0.21052631578947367, 0.15625, 0.17105263157894737, 0.25, 0.10526315789473684, 0.21739130434782608, 0.16, 0.15384615384615385, 0.09090909090909091, 0.21212121212121213, 0.15789473684210525, 0.2222222222222222, 0.13043478260869565, 0.12903225806451613, 0.2549019607843137, 0.16666666666666666, 0.23529411764705882, 0.4444444444444444, 0.046875, 0.09166666666666666, 0.17073170731707318, 0.07017543859649122, 0.2, 0.18032786885245902, 0.2564102564102564, 0.36363636363636365, 0.12903225806451613, 0.06818181818181818, 0.4375, 0.2631578947368421, 0.3333333333333333, 0.14, 0.26666666666666666, 0.16, 0.058823529411764705, 0.125, 0.1, 0.13636363636363635, 0.15625, 0.17647058823529413, 0.3333333333333333, 0.25, 0.09090909090909091], 'fmeasure': [0.03076923076923077, 0.02531645569620253, 0.0, 0.11940298507462686, 0.21333333333333335, 0.12307692307692307, 0.17777777777777778, 0.0, 0.18749999999999997, 0.08823529411764705, 0.1095890410958904, 0.20895522388059704, 0.09090909090909091, 0.16666666666666666, 0.0, 0.05333333333333334, 0.17500000000000002, 0.11320754716981132, 0.18918918918918917, 0.060606060606060615, 0.058823529411764705, 0.052631578947368425, 0.07142857142857144, 0.0, 0.11428571428571428, 0.07317073170731707, 0.16, 0.1408450704225352, 0.14285714285714285, 0.0784313725490196, 0.16666666666666666, 0.208955223880597, 0.08955223880597016, 0.10126582278481013, 0.11904761904761903, 0.07407407407407407, 0.07692307692307693, 0.12000000000000001, 0.1584158415841584, 0.15000000000000002, 0.08695652173913043, 0.044444444444444446, 0.13999999999999999, 0.23157894736842108, 0.07317073170731708, 0.13559322033898305, 0.05194805194805195, 0.1647058823529412, 0.0821917808219178, 0.1492537313432836, 0.10526315789473684, 0.06153846153846155, 0.25396825396825395, 0.136986301369863, 0.045454545454545456, 0.18181818181818182, 0.08108108108108107, 0.13114754098360656, 0.031746031746031744, 0.1111111111111111, 0.07228915662650602, 0.0, 0.10256410256410256, 0.1090909090909091, 0.1964285714285714, 0.15384615384615383, 0.07894736842105264, 0.14117647058823532, 0.0, 0.12903225806451615, 0.10752688172043011, 0.03921568627450981, 0.10526315789473684, 0.09523809523809523, 0.0975609756097561, 0.041666666666666664, 0.10126582278481013, 0.18390804597701152, 0.08247422680412371, 0.11382113821138212, 0.12048192771084339, 0.07407407407407407, 0.0888888888888889, 0.0821917808219178, 0.052631578947368425, 0.16, 0.29729729729729726, 0.12987012987012986, 0.20588235294117646, 0.169811320754717, 0.07894736842105263, 0.08, 0.08450704225352113, 0.042105263157894736, 0.0, 0.14492753623188406, 0.10416666666666667, 0.12121212121212123, 0.09090909090909093, 0.09999999999999999, 0.2178217821782178, 0.125, 0.1846153846153846, 0.10714285714285714, 0.0, 0.12307692307692308, 0.1590909090909091, 0.1518987341772152, 0.13114754098360656, 0.02702702702702703, 0.05405405405405405, 0.07920792079207921, 0.06779661016949153, 0.176, 0.034782608695652174, 0.1941747572815534, 0.12643678160919541, 0.1839080459770115, 0.06741573033707865, 0.09302325581395349, 0.15625, 0.08, 0.16260162601626016, 0.09090909090909093, 0.1728395061728395, 0.15584415584415584, 0.16326530612244897, 0.12307692307692308, 0.16666666666666666, 0.2222222222222222, 0.047058823529411764, 0.09523809523809523, 0.15625, 0.2222222222222222, 0.034482758620689655, 0.10126582278481013, 0.07142857142857142, 0.09803921568627452, 0.1714285714285714, 0.2857142857142857, 0.13157894736842105, 0.06976744186046512, 0.03389830508474576, 0.08450704225352113, 0.024999999999999998, 0.05555555555555555, 0.14035087719298245, 0.13157894736842105, 0.0821917808219178, 0.04081632653061224, 0.05825242718446602, 0.12820512820512822, 0.03174603174603175, 0.049999999999999996, 0.03703703703703704, 0.08333333333333333, 0.025974025974025976, 0.029411764705882353, 0.0, 0.0449438202247191, 0.10909090909090909, 0.14634146341463414, 0.06779661016949153, 0.125, 0.06349206349206349, 0.125, 0.07547169811320754, 0.10309278350515463, 0.07058823529411765, 0.20253164556962025, 0.0816326530612245, 0.075, 0.12048192771084339, 0.14545454545454545, 0.03571428571428572, 0.2028985507246377, 0.0, 0.15384615384615383, 0.06451612903225806, 0.05970149253731344, 0.17777777777777776, 0.0, 0.11111111111111112, 0.05194805194805195, 0.028169014084507043, 0.125, 0.0, 0.17391304347826086, 0.0392156862745098, 0.1348314606741573, 0.17391304347826086, 0.10204081632653061, 0.06976744186046513, 0.14285714285714285, 0.20408163265306123, 0.0851063829787234, 0.07142857142857141, 0.05, 0.06896551724137931, 0.08333333333333333, 0.0821917808219178, 0.08333333333333333, 0.19130434782608696, 0.03389830508474576, 0.1276595744680851, 0.15584415584415584, 0.05405405405405404, 0.05405405405405406, 0.15384615384615388, 0.058252427184466014, 0.09259259259259259, 0.05333333333333333, 0.09523809523809522, 0.1935483870967742, 0.07692307692307693, 0.11764705882352942, 0.0, 0.05714285714285715, 0.14457831325301204, 0.15151515151515152, 0.0851063829787234, 0.08196721311475412, 0.0, 0.05970149253731344, 0.0, 0.1188118811881188, 0.11764705882352941, 0.1348314606741573, 0.09900990099009903, 0.125, 0.16216216216216214, 0.16296296296296295, 0.07317073170731707, 0.06399999999999999, 0.31111111111111117, 0.2181818181818182, 0.22641509433962265, 0.12371134020618559, 0.22448979591836737, 0.20454545454545456, 0.18604651162790695, 0.15873015873015872, 0.12820512820512822, 0.15841584158415842, 0.15267175572519084, 0.2434782608695652, 0.21739130434782605, 0.04819277108433734, 0.043010752688172046, 0.14285714285714288, 0.21212121212121213, 0.14141414141414144, 0.2222222222222222, 0.1176470588235294, 0.3076923076923077, 0.08450704225352113, 0.16666666666666669, 0.0, 0.035714285714285705, 0.07619047619047621, 0.14423076923076922, 0.13333333333333333, 0.125, 0.1081081081081081, 0.08333333333333333, 0.23008849557522124, 0.11764705882352941, 0.08333333333333333, 0.16129032258064516, 0.13559322033898305, 0.08695652173913043, 0.08163265306122448, 0.22222222222222224, 0.08695652173913043, 0.06153846153846154, 0.1714285714285714, 0.07692307692307691, 0.23423423423423423, 0.15625, 0.2191780821917808, 0.27272727272727276, 0.05714285714285714, 0.13414634146341461, 0.17500000000000002, 0.0761904761904762, 0.07692307692307693, 0.1437908496732026, 0.19801980198019803, 0.23188405797101447, 0.07207207207207207, 0.0689655172413793, 0.175, 0.1282051282051282, 0.1111111111111111, 0.1794871794871795, 0.125, 0.1142857142857143, 0.030303030303030304, 0.03389830508474576, 0.02702702702702703, 0.08333333333333333, 0.18181818181818182, 0.09677419354838708, 0.13636363636363638, 0.1388888888888889, 0.03773584905660377]}\n",
            "Average for precision is 0.09677763341485052\n",
            "Average for recall is 0.16375420312794223\n",
            "Average for fmeasure is 0.11207005053853211\n",
            "/content/drive/MyDrive/Colab Notebooks/data/testdata/Near-Optimal Design of Experiments via Regret Minimization.csv\n",
            "['Many research efforts have been spent on incorporating the monolingual corpora into machine translation, such as multi-task learning (Gulcehre et al., 2015; Zhang and Zong, 2016), back-translation (Sennrich et al., 2015), dual learning (He et al., 2016) and unsupervised machine translation with monolingual corpora only for both sides (Artetxe et al., 2017b; Lample et al., 2017; Yang et al., 2018).\\n', 'This completes the computation of the gradient and enables the application of gradient-based methods, i.e., BFGS to find a (locally) optimal estimate ŵ.', 'In this paper, we first discuss the robustness of graph-based Sybil detectors SybilRank and Integro and refine theoretically their security guarantees towards more realistic assumptions.', 'In this work, we employ a bigram model over output characters.', 'The answers to these questions are found in a single sentence in the narrative, although it is possible that the answer may change through the course of the narrative (e.g., “John’s new office is GHC122”).\\n', 'The Douban corpus is constructed in a similar way to the Ubuntu Corpus, except that its validation set contains 50k instances with 1:1 positive-negative ratios and the testing set of Douban corpus is consisted of 10k instances, where each context has 10 candidate responses, collected via a tiny invertedindex system (Lucene3), and labels are manually annotated.', 'Experimental design is an important problem in statistics and machine learning research (Pukelsheim, 2006).', 'Consider a linear regression model\\ny = Xβ0 +w, (1)\\nwhere X ∈ Rn×p is a pool of n design points, y is the response vector, β0 is a p-dimensional unknown regression model and w is a vector of i.i.d.', 'noise variables satisfying Ewi = 0 and Ew2i', 'As experiments are expensive and time-consuming,\\n*Author names listed in alphabetic order.', 'The experimental design problem can then be formulated as a combinatorial optimization problem:\\nS∗(k) = arg min S∈S(n,k) f(X>SXS), (2)\\nwhere S is either a set or a multi-set of size k, and XS ∈ Rk×p is formed by stacking the rows of X that are in S.', 'Under this setting, XS only contains distinct rows of the design pool X.\\nThe “with replacement” setting is classical in statistics literature, where the multiple measurements in y with respect to the same design point lead to different values with statistically independent noise.', 'Finally, it is worth pointing out that the “with replacement” setting is easier, because it can be reduced (in polynomial time) to the “without replacement” setting by replicating each row of X for k times.\\n', '[n] of size k, with objective value f(X>\\nŜ XŜ) being at most O(1) a constant\\ntimes the optimum.', 'If replacement (b = 1) or over-sampling (k > r) is allowed, the approximation ratio can be tightened to 1+ ε for arbitrarily small ε > 0.', 'In contrast, no polynomial-\\ntime algorithm has achieved O(1) approximation in the regime k = O(p) for non-submodular optimality criteria (e.g., A- and V-optimality) under the without replacement setting.\\n', '• Our algorithm works for any regular optimality criterion.', 'To the best of our knowledge, no known polynomial-time algorithm can achieve a (1 + ε) approximation for the D- and T-optimality criteria, or even an O(1) approximation for the E- and G-optimality criteria.', 'The key idea behind our proof of Theorem 1.1 is a regret minimization characterization of the least eigenvalue of positive semidefinite (PSD) matrices.', 'Pilanci & Wainwright (2016); Raskutti & Mahoney (2014); Woodruff (2014) considered fast methods for solving ordinary least squares (OLS) problems.']\n",
            "250/250 [==============================] - 185s 741ms/step\n",
            "{'precision': [0.021739130434782608, 0.021739130434782608, 0.0, 0.07692307692307693, 0.16, 0.09523809523809523, 0.10810810810810811, 0.0, 0.13636363636363635, 0.057692307692307696, 0.08, 0.16666666666666666, 0.08108108108108109, 0.125, 0.0, 0.03508771929824561, 0.1346153846153846, 0.08108108108108109, 0.16666666666666666, 0.038461538461538464, 0.058823529411764705, 0.030303030303030304, 0.05, 0.0, 0.07017543859649122, 0.06818181818181818, 0.13043478260869565, 0.09259259259259259, 0.09523809523809523, 0.06666666666666667, 0.13157894736842105, 0.14285714285714285, 0.07317073170731707, 0.12903225806451613, 0.08064516129032258, 0.046511627906976744, 0.05263157894736842, 0.07792207792207792, 0.14814814814814814, 0.0967741935483871, 0.06382978723404255, 0.02564102564102564, 0.08433734939759036, 0.2391304347826087, 0.057692307692307696, 0.1111111111111111, 0.03225806451612903, 0.1320754716981132, 0.05263157894736842, 0.11363636363636363, 0.08163265306122448, 0.05128205128205128, 0.20512820512820512, 0.09803921568627451, 0.03389830508474576, 0.24324324324324326, 0.07142857142857142, 0.1111111111111111, 0.023255813953488372, 0.08333333333333333, 0.05357142857142857, 0.0, 0.11428571428571428, 0.07692307692307693, 0.15942028985507245, 0.1111111111111111, 0.05, 0.11538461538461539, 0.0, 0.0975609756097561, 0.09090909090909091, 0.024390243902439025, 0.11428571428571428, 0.09375, 0.07142857142857142, 0.034482758620689655, 0.09523809523809523, 0.15384615384615385, 0.06896551724137931, 0.07526881720430108, 0.09259259259259259, 0.05128205128205128, 0.07547169811320754, 0.05172413793103448, 0.05, 0.15789473684210525, 0.24444444444444444, 0.0847457627118644, 0.18421052631578946, 0.12162162162162163, 0.06, 0.05454545454545454, 0.06818181818181818, 0.03636363636363636, 0.0, 0.1724137931034483, 0.06493506493506493, 0.09523809523809523, 0.07692307692307693, 0.1875, 0.14285714285714285, 0.07317073170731707, 0.15, 0.08823529411764706, 0.0, 0.10810810810810811, 0.1111111111111111, 0.13043478260869565, 0.1111111111111111, 0.02, 0.044444444444444446, 0.11764705882352941, 0.05263157894736842, 0.2391304347826087, 0.02857142857142857, 0.17543859649122806, 0.07586206896551724, 0.14035087719298245, 0.04838709677419355, 0.09523809523809523, 0.15625, 0.05555555555555555, 0.19607843137254902, 0.07692307692307693, 0.1794871794871795, 0.1111111111111111, 0.125, 0.21052631578947367, 0.24242424242424243, 0.2857142857142857, 0.09090909090909091, 0.06779661016949153, 0.11627906976744186, 0.17391304347826086, 0.02127659574468085, 0.08695652173913043, 0.05555555555555555, 0.1388888888888889, 0.13636363636363635, 0.23809523809523808, 0.07936507936507936, 0.047619047619047616, 0.02127659574468085, 0.09090909090909091, 0.01818181818181818, 0.034482758620689655, 0.11428571428571428, 0.11363636363636363, 0.06666666666666667, 0.034482758620689655, 0.06818181818181818, 0.10204081632653061, 0.029411764705882353, 0.043478260869565216, 0.034482758620689655, 0.05660377358490566, 0.01694915254237288, 0.020833333333333332, 0.0, 0.029850746268656716, 0.13043478260869565, 0.13043478260869565, 0.043478260869565216, 0.09259259259259259, 0.05555555555555555, 0.11363636363636363, 0.09523809523809523, 0.07936507936507936, 0.047619047619047616, 0.1702127659574468, 0.06060606060606061, 0.05454545454545454, 0.08620689655172414, 0.11428571428571428, 0.022727272727272728, 0.15555555555555556, 0.0, 0.11363636363636363, 0.04081632653061224, 0.058823529411764705, 0.17391304347826086, 0.0, 0.07547169811320754, 0.03389830508474576, 0.020833333333333332, 0.10810810810810811, 0.0, 0.17391304347826086, 0.034482758620689655, 0.10714285714285714, 0.15789473684210525, 0.1111111111111111, 0.058823529411764705, 0.12962962962962962, 0.18867924528301888, 0.07142857142857142, 0.05128205128205128, 0.04878048780487805, 0.04878048780487805, 0.06349206349206349, 0.05172413793103448, 0.08163265306122448, 0.16666666666666666, 0.020833333333333332, 0.08571428571428572, 0.16666666666666666, 0.03488372093023256, 0.0297029702970297, 0.11538461538461539, 0.03488372093023256, 0.05319148936170213, 0.046511627906976744, 0.05714285714285714, 0.16216216216216217, 0.1, 0.11428571428571428, 0.0, 0.045454545454545456, 0.09836065573770492, 0.13513513513513514, 0.05714285714285714, 0.05102040816326531, 0.0, 0.05, 0.0, 0.11764705882352941, 0.11320754716981132, 0.14285714285714285, 0.06944444444444445, 0.125, 0.19148936170212766, 0.14102564102564102, 0.08823529411764706, 0.05970149253731343, 0.23333333333333334, 0.1487603305785124, 0.17142857142857143, 0.16216216216216217, 0.24444444444444444, 0.1836734693877551, 0.22641509433962265, 0.1282051282051282, 0.1, 0.125, 0.10869565217391304, 0.19718309859154928, 0.15, 0.027777777777777776, 0.027777777777777776, 0.09722222222222222, 0.1794871794871795, 0.09722222222222222, 0.2857142857142857, 0.09, 0.391304347826087, 0.07894736842105263, 0.13559322033898305, 0.0, 0.02564102564102564, 0.04819277108433735, 0.2830188679245283, 0.11290322580645161, 0.1111111111111111, 0.07272727272727272, 0.056818181818181816, 0.35135135135135137, 0.07692307692307693, 0.06896551724137931, 0.1282051282051282, 0.11764705882352941, 0.06060606060606061, 0.07407407407407407, 0.23333333333333334, 0.06, 0.03571428571428571, 0.25, 0.0547945205479452, 0.21666666666666667, 0.14705882352941177, 0.20512820512820512, 0.19672131147540983, 0.07317073170731707, 0.25, 0.1794871794871795, 0.08333333333333333, 0.047619047619047616, 0.11956521739130435, 0.16129032258064516, 0.1702127659574468, 0.05, 0.06976744186046512, 0.109375, 0.0847457627118644, 0.06666666666666667, 0.25, 0.08163265306122448, 0.08888888888888889, 0.02040816326530612, 0.0196078431372549, 0.015625, 0.06, 0.21739130434782608, 0.06666666666666667, 0.08571428571428572, 0.09615384615384616, 0.023809523809523808, 0.3442622950819672, 0.10526315789473684, 0.09302325581395349, 0.0547945205479452, 0.17647058823529413, 0.06451612903225806, 0.047619047619047616, 0.15789473684210525, 0.0, 0.058823529411764705, 0.15151515151515152, 0.1, 0.21212121212121213, 0.07142857142857142, 0.038461538461538464, 0.043478260869565216, 0.0, 0.16666666666666666, 0.046511627906976744, 0.0], 'recall': [0.05263157894736842, 0.030303030303030304, 0.0, 0.26666666666666666, 0.32, 0.17391304347826086, 0.5, 0.0, 0.3, 0.1875, 0.17391304347826086, 0.28, 0.10344827586206896, 0.25, 0.0, 0.1111111111111111, 0.25, 0.1875, 0.21875, 0.14285714285714285, 0.058823529411764705, 0.2, 0.125, 0.0, 0.3076923076923077, 0.07894736842105263, 0.20689655172413793, 0.29411764705882354, 0.2857142857142857, 0.09523809523809523, 0.22727272727272727, 0.3888888888888889, 0.11538461538461539, 0.08333333333333333, 0.22727272727272727, 0.18181818181818182, 0.14285714285714285, 0.2608695652173913, 0.1702127659574468, 0.3333333333333333, 0.13636363636363635, 0.16666666666666666, 0.4117647058823529, 0.22448979591836735, 0.1, 0.17391304347826086, 0.13333333333333333, 0.21875, 0.1875, 0.21739130434782608, 0.14814814814814814, 0.07692307692307693, 0.3333333333333333, 0.22727272727272727, 0.06896551724137931, 0.14516129032258066, 0.09375, 0.16, 0.05, 0.16666666666666666, 0.1111111111111111, 0.0, 0.09302325581395349, 0.1875, 0.2558139534883721, 0.25, 0.1875, 0.18181818181818182, 0.0, 0.19047619047619047, 0.13157894736842105, 0.1, 0.0975609756097561, 0.0967741935483871, 0.15384615384615385, 0.05263157894736842, 0.10810810810810811, 0.22857142857142856, 0.10256410256410256, 0.23333333333333334, 0.1724137931034483, 0.13333333333333333, 0.10810810810810811, 0.2, 0.05555555555555555, 0.16216216216216217, 0.3793103448275862, 0.2777777777777778, 0.23333333333333334, 0.28125, 0.11538461538461539, 0.15, 0.1111111111111111, 0.05, 0.0, 0.125, 0.2631578947368421, 0.16666666666666666, 0.1111111111111111, 0.06818181818181818, 0.4583333333333333, 0.42857142857142855, 0.24, 0.13636363636363635, 0.0, 0.14285714285714285, 0.28, 0.18181818181818182, 0.16, 0.041666666666666664, 0.06896551724137931, 0.05970149253731343, 0.09523809523809523, 0.13924050632911392, 0.044444444444444446, 0.21739130434782608, 0.3793103448275862, 0.26666666666666666, 0.1111111111111111, 0.09090909090909091, 0.15625, 0.14285714285714285, 0.1388888888888889, 0.1111111111111111, 0.16666666666666666, 0.2608695652173913, 0.23529411764705882, 0.08695652173913043, 0.12698412698412698, 0.18181818181818182, 0.031746031746031744, 0.16, 0.23809523809523808, 0.3076923076923077, 0.09090909090909091, 0.12121212121212122, 0.1, 0.07575757575757576, 0.23076923076923078, 0.35714285714285715, 0.38461538461538464, 0.13043478260869565, 0.08333333333333333, 0.07894736842105263, 0.04, 0.14285714285714285, 0.18181818181818182, 0.15625, 0.10714285714285714, 0.05, 0.05084745762711865, 0.1724137931034483, 0.034482758620689655, 0.058823529411764705, 0.04, 0.15789473684210525, 0.05555555555555555, 0.05, 0.0, 0.09090909090909091, 0.09375, 0.16666666666666666, 0.15384615384615385, 0.19230769230769232, 0.07407407407407407, 0.1388888888888889, 0.0625, 0.14705882352941177, 0.13636363636363635, 0.25, 0.125, 0.12, 0.2, 0.2, 0.08333333333333333, 0.2916666666666667, 0.0, 0.23809523809523808, 0.15384615384615385, 0.06060606060606061, 0.18181818181818182, 0.0, 0.21052631578947367, 0.1111111111111111, 0.043478260869565216, 0.14814814814814814, 0.0, 0.17391304347826086, 0.045454545454545456, 0.18181818181818182, 0.1935483870967742, 0.09433962264150944, 0.08571428571428572, 0.1590909090909091, 0.2222222222222222, 0.10526315789473684, 0.11764705882352941, 0.05128205128205128, 0.11764705882352941, 0.12121212121212122, 0.2, 0.0851063829787234, 0.22448979591836735, 0.09090909090909091, 0.25, 0.14634146341463414, 0.12, 0.3, 0.23076923076923078, 0.17647058823529413, 0.35714285714285715, 0.0625, 0.2857142857142857, 0.24, 0.0625, 0.12121212121212122, 0.0, 0.07692307692307693, 0.2727272727272727, 0.1724137931034483, 0.16666666666666666, 0.20833333333333334, 0.0, 0.07407407407407407, 0.0, 0.12, 0.12244897959183673, 0.1276595744680851, 0.1724137931034483, 0.125, 0.140625, 0.19298245614035087, 0.0625, 0.06896551724137931, 0.4666666666666667, 0.4090909090909091, 0.3333333333333333, 0.1, 0.20754716981132076, 0.23076923076923078, 0.15789473684210525, 0.20833333333333334, 0.17857142857142858, 0.21621621621621623, 0.2564102564102564, 0.3181818181818182, 0.39473684210526316, 0.18181818181818182, 0.09523809523809523, 0.2692307692307692, 0.25925925925925924, 0.25925925925925924, 0.18181818181818182, 0.16981132075471697, 0.2535211267605634, 0.09090909090909091, 0.21621621621621623, 0.0, 0.058823529411764705, 0.18181818181818182, 0.0967741935483871, 0.16279069767441862, 0.14285714285714285, 0.21052631578947367, 0.15625, 0.17105263157894737, 0.25, 0.10526315789473684, 0.21739130434782608, 0.16, 0.15384615384615385, 0.09090909090909091, 0.21212121212121213, 0.15789473684210525, 0.2222222222222222, 0.13043478260869565, 0.12903225806451613, 0.2549019607843137, 0.16666666666666666, 0.23529411764705882, 0.4444444444444444, 0.046875, 0.09166666666666666, 0.17073170731707318, 0.07017543859649122, 0.2, 0.18032786885245902, 0.2564102564102564, 0.36363636363636365, 0.12903225806451613, 0.06818181818181818, 0.4375, 0.2631578947368421, 0.3333333333333333, 0.14, 0.26666666666666666, 0.16, 0.058823529411764705, 0.125, 0.1, 0.13636363636363635, 0.15625, 0.17647058823529413, 0.3333333333333333, 0.25, 0.09090909090909091, 0.3387096774193548, 0.24, 0.15384615384615385, 0.36363636363636365, 0.16216216216216217, 0.06896551724137931, 0.14285714285714285, 0.13953488372093023, 0.0, 0.15384615384615385, 0.09259259259259259, 0.09302325581395349, 0.2, 0.14285714285714285, 0.043478260869565216, 0.06060606060606061, 0.0, 0.1111111111111111, 0.08333333333333333, 0.0], 'fmeasure': [0.03076923076923077, 0.02531645569620253, 0.0, 0.11940298507462686, 0.21333333333333335, 0.12307692307692307, 0.17777777777777778, 0.0, 0.18749999999999997, 0.08823529411764705, 0.1095890410958904, 0.20895522388059704, 0.09090909090909091, 0.16666666666666666, 0.0, 0.05333333333333334, 0.17500000000000002, 0.11320754716981132, 0.18918918918918917, 0.060606060606060615, 0.058823529411764705, 0.052631578947368425, 0.07142857142857144, 0.0, 0.11428571428571428, 0.07317073170731707, 0.16, 0.1408450704225352, 0.14285714285714285, 0.0784313725490196, 0.16666666666666666, 0.208955223880597, 0.08955223880597016, 0.10126582278481013, 0.11904761904761903, 0.07407407407407407, 0.07692307692307693, 0.12000000000000001, 0.1584158415841584, 0.15000000000000002, 0.08695652173913043, 0.044444444444444446, 0.13999999999999999, 0.23157894736842108, 0.07317073170731708, 0.13559322033898305, 0.05194805194805195, 0.1647058823529412, 0.0821917808219178, 0.1492537313432836, 0.10526315789473684, 0.06153846153846155, 0.25396825396825395, 0.136986301369863, 0.045454545454545456, 0.18181818181818182, 0.08108108108108107, 0.13114754098360656, 0.031746031746031744, 0.1111111111111111, 0.07228915662650602, 0.0, 0.10256410256410256, 0.1090909090909091, 0.1964285714285714, 0.15384615384615383, 0.07894736842105264, 0.14117647058823532, 0.0, 0.12903225806451615, 0.10752688172043011, 0.03921568627450981, 0.10526315789473684, 0.09523809523809523, 0.0975609756097561, 0.041666666666666664, 0.10126582278481013, 0.18390804597701152, 0.08247422680412371, 0.11382113821138212, 0.12048192771084339, 0.07407407407407407, 0.0888888888888889, 0.0821917808219178, 0.052631578947368425, 0.16, 0.29729729729729726, 0.12987012987012986, 0.20588235294117646, 0.169811320754717, 0.07894736842105263, 0.08, 0.08450704225352113, 0.042105263157894736, 0.0, 0.14492753623188406, 0.10416666666666667, 0.12121212121212123, 0.09090909090909093, 0.09999999999999999, 0.2178217821782178, 0.125, 0.1846153846153846, 0.10714285714285714, 0.0, 0.12307692307692308, 0.1590909090909091, 0.1518987341772152, 0.13114754098360656, 0.02702702702702703, 0.05405405405405405, 0.07920792079207921, 0.06779661016949153, 0.176, 0.034782608695652174, 0.1941747572815534, 0.12643678160919541, 0.1839080459770115, 0.06741573033707865, 0.09302325581395349, 0.15625, 0.08, 0.16260162601626016, 0.09090909090909093, 0.1728395061728395, 0.15584415584415584, 0.16326530612244897, 0.12307692307692308, 0.16666666666666666, 0.2222222222222222, 0.047058823529411764, 0.09523809523809523, 0.15625, 0.2222222222222222, 0.034482758620689655, 0.10126582278481013, 0.07142857142857142, 0.09803921568627452, 0.1714285714285714, 0.2857142857142857, 0.13157894736842105, 0.06976744186046512, 0.03389830508474576, 0.08450704225352113, 0.024999999999999998, 0.05555555555555555, 0.14035087719298245, 0.13157894736842105, 0.0821917808219178, 0.04081632653061224, 0.05825242718446602, 0.12820512820512822, 0.03174603174603175, 0.049999999999999996, 0.03703703703703704, 0.08333333333333333, 0.025974025974025976, 0.029411764705882353, 0.0, 0.0449438202247191, 0.10909090909090909, 0.14634146341463414, 0.06779661016949153, 0.125, 0.06349206349206349, 0.125, 0.07547169811320754, 0.10309278350515463, 0.07058823529411765, 0.20253164556962025, 0.0816326530612245, 0.075, 0.12048192771084339, 0.14545454545454545, 0.03571428571428572, 0.2028985507246377, 0.0, 0.15384615384615383, 0.06451612903225806, 0.05970149253731344, 0.17777777777777776, 0.0, 0.11111111111111112, 0.05194805194805195, 0.028169014084507043, 0.125, 0.0, 0.17391304347826086, 0.0392156862745098, 0.1348314606741573, 0.17391304347826086, 0.10204081632653061, 0.06976744186046513, 0.14285714285714285, 0.20408163265306123, 0.0851063829787234, 0.07142857142857141, 0.05, 0.06896551724137931, 0.08333333333333333, 0.0821917808219178, 0.08333333333333333, 0.19130434782608696, 0.03389830508474576, 0.1276595744680851, 0.15584415584415584, 0.05405405405405404, 0.05405405405405406, 0.15384615384615388, 0.058252427184466014, 0.09259259259259259, 0.05333333333333333, 0.09523809523809522, 0.1935483870967742, 0.07692307692307693, 0.11764705882352942, 0.0, 0.05714285714285715, 0.14457831325301204, 0.15151515151515152, 0.0851063829787234, 0.08196721311475412, 0.0, 0.05970149253731344, 0.0, 0.1188118811881188, 0.11764705882352941, 0.1348314606741573, 0.09900990099009903, 0.125, 0.16216216216216214, 0.16296296296296295, 0.07317073170731707, 0.06399999999999999, 0.31111111111111117, 0.2181818181818182, 0.22641509433962265, 0.12371134020618559, 0.22448979591836737, 0.20454545454545456, 0.18604651162790695, 0.15873015873015872, 0.12820512820512822, 0.15841584158415842, 0.15267175572519084, 0.2434782608695652, 0.21739130434782605, 0.04819277108433734, 0.043010752688172046, 0.14285714285714288, 0.21212121212121213, 0.14141414141414144, 0.2222222222222222, 0.1176470588235294, 0.3076923076923077, 0.08450704225352113, 0.16666666666666669, 0.0, 0.035714285714285705, 0.07619047619047621, 0.14423076923076922, 0.13333333333333333, 0.125, 0.1081081081081081, 0.08333333333333333, 0.23008849557522124, 0.11764705882352941, 0.08333333333333333, 0.16129032258064516, 0.13559322033898305, 0.08695652173913043, 0.08163265306122448, 0.22222222222222224, 0.08695652173913043, 0.06153846153846154, 0.1714285714285714, 0.07692307692307691, 0.23423423423423423, 0.15625, 0.2191780821917808, 0.27272727272727276, 0.05714285714285714, 0.13414634146341461, 0.17500000000000002, 0.0761904761904762, 0.07692307692307693, 0.1437908496732026, 0.19801980198019803, 0.23188405797101447, 0.07207207207207207, 0.0689655172413793, 0.175, 0.1282051282051282, 0.1111111111111111, 0.1794871794871795, 0.125, 0.1142857142857143, 0.030303030303030304, 0.03389830508474576, 0.02702702702702703, 0.08333333333333333, 0.18181818181818182, 0.09677419354838708, 0.13636363636363638, 0.1388888888888889, 0.03773584905660377, 0.34146341463414637, 0.14634146341463414, 0.11594202898550725, 0.09523809523809523, 0.16901408450704225, 0.06666666666666667, 0.07142857142857142, 0.14814814814814817, 0.0, 0.0851063829787234, 0.11494252873563218, 0.09638554216867469, 0.2058823529411765, 0.09523809523809523, 0.04081632653061224, 0.05063291139240506, 0.0, 0.13333333333333333, 0.059701492537313425, 0.0]}\n",
            "Average for precision is 0.09676934390918102\n",
            "Average for recall is 0.16165443548792552\n",
            "Average for fmeasure is 0.11144084939775951\n",
            "/content/drive/MyDrive/Colab Notebooks/data/testdata/On Learning Sparsely Used Dictionaries from Incomplete Samples.csv\n",
            "['This is possibly because MoG is the only strategy that takes into account the numerical magnitudes of the numerals.', 'Unlike linear models which posit a latent variable per observation, VAEs introduce a mapping from observations to a distribution on the latent space; when parameterized by a deep neural network, this mapping is called the inference network.', 'In contrast, we provide rigorous polynomial time algorithms, together with error bounds on the estimation quality of A⇤.', 'Given a vector x 2 Rm and a subset S ✓ [m], we denote xS 2 Rm as a vector which equals x in indices belonging to S and equals zero elsewhere.', 'We seek an algorithm that provides a provably “good” estimate of A⇤.', 'To keep notation simple, in our convergence theorems below, whenever we discuss nearness, we simply replace the transformations ⇡ and in the above definition with the identity mapping ⇡(i) =', 'Each entry of the sample A⇤x⇤ is independently observed with constant probability ⇢ 2 (0, 1].\\n', 'Finally, we also require the sparsity k  O⇤(⇢ p n/ log n) throughout the paper.', 'This enables us to devise an algorithm similar to that of Arora et al. (2015) and obtain a descent property directly related to (the population parameter) A⇤.', 'The full procedure is detailed as Algorithm 1.\\n', 'We now analyze our proposed algorithm.', 'Suppose that the initial estimate A0 is ( , 2)- near to A ⇤ with = O⇤(1/ log n) and the sampling prob-\\nability satisfies ⇢ 1/(k + 1).', 'The sample complexity argument emerges when we control the concentration of bgs, detailed in Appendix C. Here, we separately discuss the encoding and update steps in Algorithm 1.\\n', 'Since we assume that the current estimate As is (columnwise) sufficiently close to A⇤, each s\\ni is approximately\\nequal to 1, and hence gs i ⇡ ⇢piqi(As•i A⇤•i), i.e., the gradient points in the desired direction.', 'Our main theoretical result (Theorem 5) summarizes its performance.\\n', 'A\\n⇤T ,iA ⇤ •W↵W + 1 ⇢ hA⇤ ,i, A⇤•ii 1 ↵i\\n 1 ⇢ A⇤T ,iA⇤•W↵W + ( 1 ⇢ A ⇤T ,iA ⇤ •i 1)↵i .\\n(4)\\n', 'because ↵W is ksparse sub-Gaussian.', 'our results match the previous bounds when ⇢ = 1.', 'This gives us a coarse estimate of A⇤•i.\\n', 'Using the result of Arora et al. (2015), if |P1| is p1 = eO(m), then we can estimate all the m dictionary atoms.', 'We corroborate our theory by demonstrating some representative numerical benefits of our proposed algorithms.', 'An implementation of our method is available online3.\\n', '(Since we can only estimate bA modulo a permutation and sign flip, the optimal column and sign matching is computed using the Hungarian algorithm.)\\n', 'Figure 1 shows our experimental results.', 'Here, sample size refers to the number of incomplete samples.', 'Our algorithms are able to recover the dictionary for ⇢ = 0.6, 0.8, 1.0.', 'For ⇢ = 0.4, we can observe a “phase transition” in sample complexity of successful recovery around p = 10, 000 samples.\\n']\n",
            "46/46 [==============================] - 34s 731ms/step\n",
            "{'precision': [0.021739130434782608, 0.021739130434782608, 0.0, 0.07692307692307693, 0.16, 0.09523809523809523, 0.10810810810810811, 0.0, 0.13636363636363635, 0.057692307692307696, 0.08, 0.16666666666666666, 0.08108108108108109, 0.125, 0.0, 0.03508771929824561, 0.1346153846153846, 0.08108108108108109, 0.16666666666666666, 0.038461538461538464, 0.058823529411764705, 0.030303030303030304, 0.05, 0.0, 0.07017543859649122, 0.06818181818181818, 0.13043478260869565, 0.09259259259259259, 0.09523809523809523, 0.06666666666666667, 0.13157894736842105, 0.14285714285714285, 0.07317073170731707, 0.12903225806451613, 0.08064516129032258, 0.046511627906976744, 0.05263157894736842, 0.07792207792207792, 0.14814814814814814, 0.0967741935483871, 0.06382978723404255, 0.02564102564102564, 0.08433734939759036, 0.2391304347826087, 0.057692307692307696, 0.1111111111111111, 0.03225806451612903, 0.1320754716981132, 0.05263157894736842, 0.11363636363636363, 0.08163265306122448, 0.05128205128205128, 0.20512820512820512, 0.09803921568627451, 0.03389830508474576, 0.24324324324324326, 0.07142857142857142, 0.1111111111111111, 0.023255813953488372, 0.08333333333333333, 0.05357142857142857, 0.0, 0.11428571428571428, 0.07692307692307693, 0.15942028985507245, 0.1111111111111111, 0.05, 0.11538461538461539, 0.0, 0.0975609756097561, 0.09090909090909091, 0.024390243902439025, 0.11428571428571428, 0.09375, 0.07142857142857142, 0.034482758620689655, 0.09523809523809523, 0.15384615384615385, 0.06896551724137931, 0.07526881720430108, 0.09259259259259259, 0.05128205128205128, 0.07547169811320754, 0.05172413793103448, 0.05, 0.15789473684210525, 0.24444444444444444, 0.0847457627118644, 0.18421052631578946, 0.12162162162162163, 0.06, 0.05454545454545454, 0.06818181818181818, 0.03636363636363636, 0.0, 0.1724137931034483, 0.06493506493506493, 0.09523809523809523, 0.07692307692307693, 0.1875, 0.14285714285714285, 0.07317073170731707, 0.15, 0.08823529411764706, 0.0, 0.10810810810810811, 0.1111111111111111, 0.13043478260869565, 0.1111111111111111, 0.02, 0.044444444444444446, 0.11764705882352941, 0.05263157894736842, 0.2391304347826087, 0.02857142857142857, 0.17543859649122806, 0.07586206896551724, 0.14035087719298245, 0.04838709677419355, 0.09523809523809523, 0.15625, 0.05555555555555555, 0.19607843137254902, 0.07692307692307693, 0.1794871794871795, 0.1111111111111111, 0.125, 0.21052631578947367, 0.24242424242424243, 0.2857142857142857, 0.09090909090909091, 0.06779661016949153, 0.11627906976744186, 0.17391304347826086, 0.02127659574468085, 0.08695652173913043, 0.05555555555555555, 0.1388888888888889, 0.13636363636363635, 0.23809523809523808, 0.07936507936507936, 0.047619047619047616, 0.02127659574468085, 0.09090909090909091, 0.01818181818181818, 0.034482758620689655, 0.11428571428571428, 0.11363636363636363, 0.06666666666666667, 0.034482758620689655, 0.06818181818181818, 0.10204081632653061, 0.029411764705882353, 0.043478260869565216, 0.034482758620689655, 0.05660377358490566, 0.01694915254237288, 0.020833333333333332, 0.0, 0.029850746268656716, 0.13043478260869565, 0.13043478260869565, 0.043478260869565216, 0.09259259259259259, 0.05555555555555555, 0.11363636363636363, 0.09523809523809523, 0.07936507936507936, 0.047619047619047616, 0.1702127659574468, 0.06060606060606061, 0.05454545454545454, 0.08620689655172414, 0.11428571428571428, 0.022727272727272728, 0.15555555555555556, 0.0, 0.11363636363636363, 0.04081632653061224, 0.058823529411764705, 0.17391304347826086, 0.0, 0.07547169811320754, 0.03389830508474576, 0.020833333333333332, 0.10810810810810811, 0.0, 0.17391304347826086, 0.034482758620689655, 0.10714285714285714, 0.15789473684210525, 0.1111111111111111, 0.058823529411764705, 0.12962962962962962, 0.18867924528301888, 0.07142857142857142, 0.05128205128205128, 0.04878048780487805, 0.04878048780487805, 0.06349206349206349, 0.05172413793103448, 0.08163265306122448, 0.16666666666666666, 0.020833333333333332, 0.08571428571428572, 0.16666666666666666, 0.03488372093023256, 0.0297029702970297, 0.11538461538461539, 0.03488372093023256, 0.05319148936170213, 0.046511627906976744, 0.05714285714285714, 0.16216216216216217, 0.1, 0.11428571428571428, 0.0, 0.045454545454545456, 0.09836065573770492, 0.13513513513513514, 0.05714285714285714, 0.05102040816326531, 0.0, 0.05, 0.0, 0.11764705882352941, 0.11320754716981132, 0.14285714285714285, 0.06944444444444445, 0.125, 0.19148936170212766, 0.14102564102564102, 0.08823529411764706, 0.05970149253731343, 0.23333333333333334, 0.1487603305785124, 0.17142857142857143, 0.16216216216216217, 0.24444444444444444, 0.1836734693877551, 0.22641509433962265, 0.1282051282051282, 0.1, 0.125, 0.10869565217391304, 0.19718309859154928, 0.15, 0.027777777777777776, 0.027777777777777776, 0.09722222222222222, 0.1794871794871795, 0.09722222222222222, 0.2857142857142857, 0.09, 0.391304347826087, 0.07894736842105263, 0.13559322033898305, 0.0, 0.02564102564102564, 0.04819277108433735, 0.2830188679245283, 0.11290322580645161, 0.1111111111111111, 0.07272727272727272, 0.056818181818181816, 0.35135135135135137, 0.07692307692307693, 0.06896551724137931, 0.1282051282051282, 0.11764705882352941, 0.06060606060606061, 0.07407407407407407, 0.23333333333333334, 0.06, 0.03571428571428571, 0.25, 0.0547945205479452, 0.21666666666666667, 0.14705882352941177, 0.20512820512820512, 0.19672131147540983, 0.07317073170731707, 0.25, 0.1794871794871795, 0.08333333333333333, 0.047619047619047616, 0.11956521739130435, 0.16129032258064516, 0.1702127659574468, 0.05, 0.06976744186046512, 0.109375, 0.0847457627118644, 0.06666666666666667, 0.25, 0.08163265306122448, 0.08888888888888889, 0.02040816326530612, 0.0196078431372549, 0.015625, 0.06, 0.21739130434782608, 0.06666666666666667, 0.08571428571428572, 0.09615384615384616, 0.023809523809523808, 0.3442622950819672, 0.10526315789473684, 0.09302325581395349, 0.0547945205479452, 0.17647058823529413, 0.06451612903225806, 0.047619047619047616, 0.15789473684210525, 0.0, 0.058823529411764705, 0.15151515151515152, 0.1, 0.21212121212121213, 0.07142857142857142, 0.038461538461538464, 0.043478260869565216, 0.0, 0.16666666666666666, 0.046511627906976744, 0.0, 0.05172413793103448, 0.1, 0.06976744186046512, 0.047619047619047616, 0.03571428571428571, 0.125, 0.06896551724137931, 0.0, 0.06666666666666667, 0.02, 0.0, 0.08333333333333333, 0.10256410256410256, 0.1774193548387097, 0.0, 0.024390243902439025, 0.0196078431372549, 0.022222222222222223, 0.05, 0.08139534883720931, 0.02040816326530612, 0.1, 0.08163265306122448, 0.0, 0.021739130434782608, 0.09523809523809523, 0.14285714285714285], 'recall': [0.05263157894736842, 0.030303030303030304, 0.0, 0.26666666666666666, 0.32, 0.17391304347826086, 0.5, 0.0, 0.3, 0.1875, 0.17391304347826086, 0.28, 0.10344827586206896, 0.25, 0.0, 0.1111111111111111, 0.25, 0.1875, 0.21875, 0.14285714285714285, 0.058823529411764705, 0.2, 0.125, 0.0, 0.3076923076923077, 0.07894736842105263, 0.20689655172413793, 0.29411764705882354, 0.2857142857142857, 0.09523809523809523, 0.22727272727272727, 0.3888888888888889, 0.11538461538461539, 0.08333333333333333, 0.22727272727272727, 0.18181818181818182, 0.14285714285714285, 0.2608695652173913, 0.1702127659574468, 0.3333333333333333, 0.13636363636363635, 0.16666666666666666, 0.4117647058823529, 0.22448979591836735, 0.1, 0.17391304347826086, 0.13333333333333333, 0.21875, 0.1875, 0.21739130434782608, 0.14814814814814814, 0.07692307692307693, 0.3333333333333333, 0.22727272727272727, 0.06896551724137931, 0.14516129032258066, 0.09375, 0.16, 0.05, 0.16666666666666666, 0.1111111111111111, 0.0, 0.09302325581395349, 0.1875, 0.2558139534883721, 0.25, 0.1875, 0.18181818181818182, 0.0, 0.19047619047619047, 0.13157894736842105, 0.1, 0.0975609756097561, 0.0967741935483871, 0.15384615384615385, 0.05263157894736842, 0.10810810810810811, 0.22857142857142856, 0.10256410256410256, 0.23333333333333334, 0.1724137931034483, 0.13333333333333333, 0.10810810810810811, 0.2, 0.05555555555555555, 0.16216216216216217, 0.3793103448275862, 0.2777777777777778, 0.23333333333333334, 0.28125, 0.11538461538461539, 0.15, 0.1111111111111111, 0.05, 0.0, 0.125, 0.2631578947368421, 0.16666666666666666, 0.1111111111111111, 0.06818181818181818, 0.4583333333333333, 0.42857142857142855, 0.24, 0.13636363636363635, 0.0, 0.14285714285714285, 0.28, 0.18181818181818182, 0.16, 0.041666666666666664, 0.06896551724137931, 0.05970149253731343, 0.09523809523809523, 0.13924050632911392, 0.044444444444444446, 0.21739130434782608, 0.3793103448275862, 0.26666666666666666, 0.1111111111111111, 0.09090909090909091, 0.15625, 0.14285714285714285, 0.1388888888888889, 0.1111111111111111, 0.16666666666666666, 0.2608695652173913, 0.23529411764705882, 0.08695652173913043, 0.12698412698412698, 0.18181818181818182, 0.031746031746031744, 0.16, 0.23809523809523808, 0.3076923076923077, 0.09090909090909091, 0.12121212121212122, 0.1, 0.07575757575757576, 0.23076923076923078, 0.35714285714285715, 0.38461538461538464, 0.13043478260869565, 0.08333333333333333, 0.07894736842105263, 0.04, 0.14285714285714285, 0.18181818181818182, 0.15625, 0.10714285714285714, 0.05, 0.05084745762711865, 0.1724137931034483, 0.034482758620689655, 0.058823529411764705, 0.04, 0.15789473684210525, 0.05555555555555555, 0.05, 0.0, 0.09090909090909091, 0.09375, 0.16666666666666666, 0.15384615384615385, 0.19230769230769232, 0.07407407407407407, 0.1388888888888889, 0.0625, 0.14705882352941177, 0.13636363636363635, 0.25, 0.125, 0.12, 0.2, 0.2, 0.08333333333333333, 0.2916666666666667, 0.0, 0.23809523809523808, 0.15384615384615385, 0.06060606060606061, 0.18181818181818182, 0.0, 0.21052631578947367, 0.1111111111111111, 0.043478260869565216, 0.14814814814814814, 0.0, 0.17391304347826086, 0.045454545454545456, 0.18181818181818182, 0.1935483870967742, 0.09433962264150944, 0.08571428571428572, 0.1590909090909091, 0.2222222222222222, 0.10526315789473684, 0.11764705882352941, 0.05128205128205128, 0.11764705882352941, 0.12121212121212122, 0.2, 0.0851063829787234, 0.22448979591836735, 0.09090909090909091, 0.25, 0.14634146341463414, 0.12, 0.3, 0.23076923076923078, 0.17647058823529413, 0.35714285714285715, 0.0625, 0.2857142857142857, 0.24, 0.0625, 0.12121212121212122, 0.0, 0.07692307692307693, 0.2727272727272727, 0.1724137931034483, 0.16666666666666666, 0.20833333333333334, 0.0, 0.07407407407407407, 0.0, 0.12, 0.12244897959183673, 0.1276595744680851, 0.1724137931034483, 0.125, 0.140625, 0.19298245614035087, 0.0625, 0.06896551724137931, 0.4666666666666667, 0.4090909090909091, 0.3333333333333333, 0.1, 0.20754716981132076, 0.23076923076923078, 0.15789473684210525, 0.20833333333333334, 0.17857142857142858, 0.21621621621621623, 0.2564102564102564, 0.3181818181818182, 0.39473684210526316, 0.18181818181818182, 0.09523809523809523, 0.2692307692307692, 0.25925925925925924, 0.25925925925925924, 0.18181818181818182, 0.16981132075471697, 0.2535211267605634, 0.09090909090909091, 0.21621621621621623, 0.0, 0.058823529411764705, 0.18181818181818182, 0.0967741935483871, 0.16279069767441862, 0.14285714285714285, 0.21052631578947367, 0.15625, 0.17105263157894737, 0.25, 0.10526315789473684, 0.21739130434782608, 0.16, 0.15384615384615385, 0.09090909090909091, 0.21212121212121213, 0.15789473684210525, 0.2222222222222222, 0.13043478260869565, 0.12903225806451613, 0.2549019607843137, 0.16666666666666666, 0.23529411764705882, 0.4444444444444444, 0.046875, 0.09166666666666666, 0.17073170731707318, 0.07017543859649122, 0.2, 0.18032786885245902, 0.2564102564102564, 0.36363636363636365, 0.12903225806451613, 0.06818181818181818, 0.4375, 0.2631578947368421, 0.3333333333333333, 0.14, 0.26666666666666666, 0.16, 0.058823529411764705, 0.125, 0.1, 0.13636363636363635, 0.15625, 0.17647058823529413, 0.3333333333333333, 0.25, 0.09090909090909091, 0.3387096774193548, 0.24, 0.15384615384615385, 0.36363636363636365, 0.16216216216216217, 0.06896551724137931, 0.14285714285714285, 0.13953488372093023, 0.0, 0.15384615384615385, 0.09259259259259259, 0.09302325581395349, 0.2, 0.14285714285714285, 0.043478260869565216, 0.06060606060606061, 0.0, 0.1111111111111111, 0.08333333333333333, 0.0, 0.15789473684210525, 0.16216216216216217, 0.3333333333333333, 0.25806451612903225, 0.08333333333333333, 0.21428571428571427, 0.125, 0.0, 0.037037037037037035, 0.125, 0.0, 0.16, 0.14285714285714285, 0.275, 0.0, 0.038461538461538464, 0.16666666666666666, 0.125, 0.2222222222222222, 0.30434782608695654, 0.07142857142857142, 0.375, 0.16666666666666666, 0.0, 0.3, 0.13333333333333333, 0.2], 'fmeasure': [0.03076923076923077, 0.02531645569620253, 0.0, 0.11940298507462686, 0.21333333333333335, 0.12307692307692307, 0.17777777777777778, 0.0, 0.18749999999999997, 0.08823529411764705, 0.1095890410958904, 0.20895522388059704, 0.09090909090909091, 0.16666666666666666, 0.0, 0.05333333333333334, 0.17500000000000002, 0.11320754716981132, 0.18918918918918917, 0.060606060606060615, 0.058823529411764705, 0.052631578947368425, 0.07142857142857144, 0.0, 0.11428571428571428, 0.07317073170731707, 0.16, 0.1408450704225352, 0.14285714285714285, 0.0784313725490196, 0.16666666666666666, 0.208955223880597, 0.08955223880597016, 0.10126582278481013, 0.11904761904761903, 0.07407407407407407, 0.07692307692307693, 0.12000000000000001, 0.1584158415841584, 0.15000000000000002, 0.08695652173913043, 0.044444444444444446, 0.13999999999999999, 0.23157894736842108, 0.07317073170731708, 0.13559322033898305, 0.05194805194805195, 0.1647058823529412, 0.0821917808219178, 0.1492537313432836, 0.10526315789473684, 0.06153846153846155, 0.25396825396825395, 0.136986301369863, 0.045454545454545456, 0.18181818181818182, 0.08108108108108107, 0.13114754098360656, 0.031746031746031744, 0.1111111111111111, 0.07228915662650602, 0.0, 0.10256410256410256, 0.1090909090909091, 0.1964285714285714, 0.15384615384615383, 0.07894736842105264, 0.14117647058823532, 0.0, 0.12903225806451615, 0.10752688172043011, 0.03921568627450981, 0.10526315789473684, 0.09523809523809523, 0.0975609756097561, 0.041666666666666664, 0.10126582278481013, 0.18390804597701152, 0.08247422680412371, 0.11382113821138212, 0.12048192771084339, 0.07407407407407407, 0.0888888888888889, 0.0821917808219178, 0.052631578947368425, 0.16, 0.29729729729729726, 0.12987012987012986, 0.20588235294117646, 0.169811320754717, 0.07894736842105263, 0.08, 0.08450704225352113, 0.042105263157894736, 0.0, 0.14492753623188406, 0.10416666666666667, 0.12121212121212123, 0.09090909090909093, 0.09999999999999999, 0.2178217821782178, 0.125, 0.1846153846153846, 0.10714285714285714, 0.0, 0.12307692307692308, 0.1590909090909091, 0.1518987341772152, 0.13114754098360656, 0.02702702702702703, 0.05405405405405405, 0.07920792079207921, 0.06779661016949153, 0.176, 0.034782608695652174, 0.1941747572815534, 0.12643678160919541, 0.1839080459770115, 0.06741573033707865, 0.09302325581395349, 0.15625, 0.08, 0.16260162601626016, 0.09090909090909093, 0.1728395061728395, 0.15584415584415584, 0.16326530612244897, 0.12307692307692308, 0.16666666666666666, 0.2222222222222222, 0.047058823529411764, 0.09523809523809523, 0.15625, 0.2222222222222222, 0.034482758620689655, 0.10126582278481013, 0.07142857142857142, 0.09803921568627452, 0.1714285714285714, 0.2857142857142857, 0.13157894736842105, 0.06976744186046512, 0.03389830508474576, 0.08450704225352113, 0.024999999999999998, 0.05555555555555555, 0.14035087719298245, 0.13157894736842105, 0.0821917808219178, 0.04081632653061224, 0.05825242718446602, 0.12820512820512822, 0.03174603174603175, 0.049999999999999996, 0.03703703703703704, 0.08333333333333333, 0.025974025974025976, 0.029411764705882353, 0.0, 0.0449438202247191, 0.10909090909090909, 0.14634146341463414, 0.06779661016949153, 0.125, 0.06349206349206349, 0.125, 0.07547169811320754, 0.10309278350515463, 0.07058823529411765, 0.20253164556962025, 0.0816326530612245, 0.075, 0.12048192771084339, 0.14545454545454545, 0.03571428571428572, 0.2028985507246377, 0.0, 0.15384615384615383, 0.06451612903225806, 0.05970149253731344, 0.17777777777777776, 0.0, 0.11111111111111112, 0.05194805194805195, 0.028169014084507043, 0.125, 0.0, 0.17391304347826086, 0.0392156862745098, 0.1348314606741573, 0.17391304347826086, 0.10204081632653061, 0.06976744186046513, 0.14285714285714285, 0.20408163265306123, 0.0851063829787234, 0.07142857142857141, 0.05, 0.06896551724137931, 0.08333333333333333, 0.0821917808219178, 0.08333333333333333, 0.19130434782608696, 0.03389830508474576, 0.1276595744680851, 0.15584415584415584, 0.05405405405405404, 0.05405405405405406, 0.15384615384615388, 0.058252427184466014, 0.09259259259259259, 0.05333333333333333, 0.09523809523809522, 0.1935483870967742, 0.07692307692307693, 0.11764705882352942, 0.0, 0.05714285714285715, 0.14457831325301204, 0.15151515151515152, 0.0851063829787234, 0.08196721311475412, 0.0, 0.05970149253731344, 0.0, 0.1188118811881188, 0.11764705882352941, 0.1348314606741573, 0.09900990099009903, 0.125, 0.16216216216216214, 0.16296296296296295, 0.07317073170731707, 0.06399999999999999, 0.31111111111111117, 0.2181818181818182, 0.22641509433962265, 0.12371134020618559, 0.22448979591836737, 0.20454545454545456, 0.18604651162790695, 0.15873015873015872, 0.12820512820512822, 0.15841584158415842, 0.15267175572519084, 0.2434782608695652, 0.21739130434782605, 0.04819277108433734, 0.043010752688172046, 0.14285714285714288, 0.21212121212121213, 0.14141414141414144, 0.2222222222222222, 0.1176470588235294, 0.3076923076923077, 0.08450704225352113, 0.16666666666666669, 0.0, 0.035714285714285705, 0.07619047619047621, 0.14423076923076922, 0.13333333333333333, 0.125, 0.1081081081081081, 0.08333333333333333, 0.23008849557522124, 0.11764705882352941, 0.08333333333333333, 0.16129032258064516, 0.13559322033898305, 0.08695652173913043, 0.08163265306122448, 0.22222222222222224, 0.08695652173913043, 0.06153846153846154, 0.1714285714285714, 0.07692307692307691, 0.23423423423423423, 0.15625, 0.2191780821917808, 0.27272727272727276, 0.05714285714285714, 0.13414634146341461, 0.17500000000000002, 0.0761904761904762, 0.07692307692307693, 0.1437908496732026, 0.19801980198019803, 0.23188405797101447, 0.07207207207207207, 0.0689655172413793, 0.175, 0.1282051282051282, 0.1111111111111111, 0.1794871794871795, 0.125, 0.1142857142857143, 0.030303030303030304, 0.03389830508474576, 0.02702702702702703, 0.08333333333333333, 0.18181818181818182, 0.09677419354838708, 0.13636363636363638, 0.1388888888888889, 0.03773584905660377, 0.34146341463414637, 0.14634146341463414, 0.11594202898550725, 0.09523809523809523, 0.16901408450704225, 0.06666666666666667, 0.07142857142857142, 0.14814814814814817, 0.0, 0.0851063829787234, 0.11494252873563218, 0.09638554216867469, 0.2058823529411765, 0.09523809523809523, 0.04081632653061224, 0.05063291139240506, 0.0, 0.13333333333333333, 0.059701492537313425, 0.0, 0.07792207792207792, 0.12371134020618559, 0.11538461538461539, 0.08040201005025124, 0.05, 0.15789473684210525, 0.08888888888888889, 0.0, 0.047619047619047616, 0.03448275862068966, 0.0, 0.1095890410958904, 0.11940298507462686, 0.21568627450980393, 0.0, 0.029850746268656723, 0.03508771929824561, 0.03773584905660377, 0.0816326530612245, 0.12844036697247707, 0.031746031746031744, 0.15789473684210528, 0.10958904109589039, 0.0, 0.04054054054054054, 0.1111111111111111, 0.16666666666666666]}\n",
            "Average for precision is 0.0939237134422598\n",
            "Average for recall is 0.16112306167113183\n",
            "Average for fmeasure is 0.10901132051711994\n",
            "/content/drive/MyDrive/Colab Notebooks/data/testdata/Parallel WaveNet_ Fast High-Fidelity Speech Synthesis.csv\n",
            "['Such structures are ubiquitous in real networks, such as those arising in social and biological sciences (Jeub et al., 2015; Leskovec et al., 2009).\\n', 'We note that in general, one would be better-suited to consider the matrix [ATA]1/2, so that all eigenvalues are guaranteed to be nonnegative, but we will see that in the random dot product graph, the model that is the focus of this paper, the top d eigenvalues of A are positive with high probability (see, for example, either Lemma 1 in Athreya et al. (2016) or Observation 2 in Levin et al. (2017), or refer to the technical report, (Levin et al., 2018)).\\n', 'Applying this approximation allows us to investigate the trade-offs between computational cost and classification accuracy, to which we now turn our attention.', 'Thus any proposed change in meaning is contingent on a particular model of word representation and the method used to measure change.', 'Thus, we have established that the shuffled condition is a suitable control for meaning change.\\n', 'The relation between frequency and meaning change can also be represented by a linear mixed effect model, with the benefit that this model enables the addition of more explanatory variables to the data.', 'Thus it is unsurprising that the follower network is least useful for socially-informed personalization.', 'The DUET model (Mitra et al., 2017) is a hybrid approach that combines signals from a local model for relevance matching and a distributed model for semantic matching.', 'Instead we assume that the statistician has the ability to sample from some arbitrary product space Pn+1 on Dn+1, can evaluate f arbitrarily (and in particular on the result of this sampling), and is interested in applying a privatising mechanism with the guarantee of random differential privacy (Definition 4).', 'Thompson sampling (TS) (Thompson, 1933) can be understood as a version of the previous framework in which the utility function is defined as U(yj |xj ,DI) = yj and the expectation in (1) is taken with respect to p(yj |xj ,θ)\\ninstead of p(yj |xj ,DI), with θ being a sample from the posterior p(θ|DI).', 'Operating at such a high temporal resolution is not problematic during network training, where the complete sequence of input samples is already available and—thanks to the convolutional structure of the network—can be processed in parallel.', 'Autoregressive networks model the joint distribution of highdimensional data as a product of conditional distributions using the probabilistic chain-rule:\\np(x) = ∏ t p(xt|x<t,θ),\\nwhere xt is the t-th variable of x and θ are the parameters of the autoregressive model.', 'WaveNet (van den Oord et al., 2016a) is a convolutional autoregressive model which produces all p(xt|x<t) in one forward pass, by making use of causal—or masked— convolutions (van den Oord et al., 2016c; Germain et al., 2015).', 'At generation time, however, the waveform has to be synthesised in a sequential fashion as xt must be sampled first in order to obtain x>t. Due to this nature, real time (or faster) synthesis with a fully autoregressive system is challenging.', 'Raw audio data is typically very high-dimensional (e.g. 16,000 samples per second for 16kHz audio), and contains complex, hierarchical structures spanning many thousands of time steps, such as words in speech or melodies in music.', 'Unlike previous versions of WaveNet (van den Oord et al., 2016a), where 8-bit (µ-law or PCM) audio was modelled with a 256-way categorical distribution, we increased the fidelity by modelling 16-bit audio.', 'For all normalizing flows the transformation f is chosen so that it is invertible and its Jacobian determinant is easy to compute.', 'When the KL divergence becomes zero, the student distribution has fully recovered the teacher’s distribution.', 'For every sample x we draw from the student pS we can compute all pT (xt|x<t) in parallel with the teacher and then evaluate H(pS(xt|z<t), pT (xt|x<t)) very efficiently by drawing multiple different samples xt from pS(xt|z<t) for each timestep.', 'This unbiased estimator has a much lower variance than naively evaluating the sample under the teacher with Equation 9.\\n', 'We parameterise the teacher’s output distribution pT (xt|x<t) as a mixture of logistics distribution (Salimans et al., 2017), which allows the loss term ln pT (xt|x<t) to be differentiable with respect to both xt and x<t.', 'As an example consider the simple case where we have audio from a white random noise source: the distribution at every timestep is N (0, 1), regardless of the samples at previous timesteps.', 'White noise has a very specific and perceptually recognizable sound: a continual hiss.', 'Training with Probability Density Distillation alone might not sufficiently constrain the student to generate high quality audio streams.', 'The first additional loss we propose is the power loss, which ensures that the power in different frequency bands of the speech are on average used as much as in human speech.', 'The power loss helps to avoid the student from collapsing to a high-entropy WaveNet-mode, such as whispering.\\n', 'The latter produced better results in our experiments.', '∣∣∣∣∣∣PT (c1))−γDKL(PS(c1)∣∣∣∣∣∣PT c2)), (17) which minimises the KL-divergence between the teacher and student when both are conditioned on the same information c1 (e.g., linguistic features, speaker ID, . . . ), but also maximises it for different conditioning pairs c1 6= c2.', 'The contrastive loss penalises waveforms that have high likelihood regardless of the conditioning vector.']\n",
            "36/36 [==============================] - 26s 735ms/step\n",
            "{'precision': [0.021739130434782608, 0.021739130434782608, 0.0, 0.07692307692307693, 0.16, 0.09523809523809523, 0.10810810810810811, 0.0, 0.13636363636363635, 0.057692307692307696, 0.08, 0.16666666666666666, 0.08108108108108109, 0.125, 0.0, 0.03508771929824561, 0.1346153846153846, 0.08108108108108109, 0.16666666666666666, 0.038461538461538464, 0.058823529411764705, 0.030303030303030304, 0.05, 0.0, 0.07017543859649122, 0.06818181818181818, 0.13043478260869565, 0.09259259259259259, 0.09523809523809523, 0.06666666666666667, 0.13157894736842105, 0.14285714285714285, 0.07317073170731707, 0.12903225806451613, 0.08064516129032258, 0.046511627906976744, 0.05263157894736842, 0.07792207792207792, 0.14814814814814814, 0.0967741935483871, 0.06382978723404255, 0.02564102564102564, 0.08433734939759036, 0.2391304347826087, 0.057692307692307696, 0.1111111111111111, 0.03225806451612903, 0.1320754716981132, 0.05263157894736842, 0.11363636363636363, 0.08163265306122448, 0.05128205128205128, 0.20512820512820512, 0.09803921568627451, 0.03389830508474576, 0.24324324324324326, 0.07142857142857142, 0.1111111111111111, 0.023255813953488372, 0.08333333333333333, 0.05357142857142857, 0.0, 0.11428571428571428, 0.07692307692307693, 0.15942028985507245, 0.1111111111111111, 0.05, 0.11538461538461539, 0.0, 0.0975609756097561, 0.09090909090909091, 0.024390243902439025, 0.11428571428571428, 0.09375, 0.07142857142857142, 0.034482758620689655, 0.09523809523809523, 0.15384615384615385, 0.06896551724137931, 0.07526881720430108, 0.09259259259259259, 0.05128205128205128, 0.07547169811320754, 0.05172413793103448, 0.05, 0.15789473684210525, 0.24444444444444444, 0.0847457627118644, 0.18421052631578946, 0.12162162162162163, 0.06, 0.05454545454545454, 0.06818181818181818, 0.03636363636363636, 0.0, 0.1724137931034483, 0.06493506493506493, 0.09523809523809523, 0.07692307692307693, 0.1875, 0.14285714285714285, 0.07317073170731707, 0.15, 0.08823529411764706, 0.0, 0.10810810810810811, 0.1111111111111111, 0.13043478260869565, 0.1111111111111111, 0.02, 0.044444444444444446, 0.11764705882352941, 0.05263157894736842, 0.2391304347826087, 0.02857142857142857, 0.17543859649122806, 0.07586206896551724, 0.14035087719298245, 0.04838709677419355, 0.09523809523809523, 0.15625, 0.05555555555555555, 0.19607843137254902, 0.07692307692307693, 0.1794871794871795, 0.1111111111111111, 0.125, 0.21052631578947367, 0.24242424242424243, 0.2857142857142857, 0.09090909090909091, 0.06779661016949153, 0.11627906976744186, 0.17391304347826086, 0.02127659574468085, 0.08695652173913043, 0.05555555555555555, 0.1388888888888889, 0.13636363636363635, 0.23809523809523808, 0.07936507936507936, 0.047619047619047616, 0.02127659574468085, 0.09090909090909091, 0.01818181818181818, 0.034482758620689655, 0.11428571428571428, 0.11363636363636363, 0.06666666666666667, 0.034482758620689655, 0.06818181818181818, 0.10204081632653061, 0.029411764705882353, 0.043478260869565216, 0.034482758620689655, 0.05660377358490566, 0.01694915254237288, 0.020833333333333332, 0.0, 0.029850746268656716, 0.13043478260869565, 0.13043478260869565, 0.043478260869565216, 0.09259259259259259, 0.05555555555555555, 0.11363636363636363, 0.09523809523809523, 0.07936507936507936, 0.047619047619047616, 0.1702127659574468, 0.06060606060606061, 0.05454545454545454, 0.08620689655172414, 0.11428571428571428, 0.022727272727272728, 0.15555555555555556, 0.0, 0.11363636363636363, 0.04081632653061224, 0.058823529411764705, 0.17391304347826086, 0.0, 0.07547169811320754, 0.03389830508474576, 0.020833333333333332, 0.10810810810810811, 0.0, 0.17391304347826086, 0.034482758620689655, 0.10714285714285714, 0.15789473684210525, 0.1111111111111111, 0.058823529411764705, 0.12962962962962962, 0.18867924528301888, 0.07142857142857142, 0.05128205128205128, 0.04878048780487805, 0.04878048780487805, 0.06349206349206349, 0.05172413793103448, 0.08163265306122448, 0.16666666666666666, 0.020833333333333332, 0.08571428571428572, 0.16666666666666666, 0.03488372093023256, 0.0297029702970297, 0.11538461538461539, 0.03488372093023256, 0.05319148936170213, 0.046511627906976744, 0.05714285714285714, 0.16216216216216217, 0.1, 0.11428571428571428, 0.0, 0.045454545454545456, 0.09836065573770492, 0.13513513513513514, 0.05714285714285714, 0.05102040816326531, 0.0, 0.05, 0.0, 0.11764705882352941, 0.11320754716981132, 0.14285714285714285, 0.06944444444444445, 0.125, 0.19148936170212766, 0.14102564102564102, 0.08823529411764706, 0.05970149253731343, 0.23333333333333334, 0.1487603305785124, 0.17142857142857143, 0.16216216216216217, 0.24444444444444444, 0.1836734693877551, 0.22641509433962265, 0.1282051282051282, 0.1, 0.125, 0.10869565217391304, 0.19718309859154928, 0.15, 0.027777777777777776, 0.027777777777777776, 0.09722222222222222, 0.1794871794871795, 0.09722222222222222, 0.2857142857142857, 0.09, 0.391304347826087, 0.07894736842105263, 0.13559322033898305, 0.0, 0.02564102564102564, 0.04819277108433735, 0.2830188679245283, 0.11290322580645161, 0.1111111111111111, 0.07272727272727272, 0.056818181818181816, 0.35135135135135137, 0.07692307692307693, 0.06896551724137931, 0.1282051282051282, 0.11764705882352941, 0.06060606060606061, 0.07407407407407407, 0.23333333333333334, 0.06, 0.03571428571428571, 0.25, 0.0547945205479452, 0.21666666666666667, 0.14705882352941177, 0.20512820512820512, 0.19672131147540983, 0.07317073170731707, 0.25, 0.1794871794871795, 0.08333333333333333, 0.047619047619047616, 0.11956521739130435, 0.16129032258064516, 0.1702127659574468, 0.05, 0.06976744186046512, 0.109375, 0.0847457627118644, 0.06666666666666667, 0.25, 0.08163265306122448, 0.08888888888888889, 0.02040816326530612, 0.0196078431372549, 0.015625, 0.06, 0.21739130434782608, 0.06666666666666667, 0.08571428571428572, 0.09615384615384616, 0.023809523809523808, 0.3442622950819672, 0.10526315789473684, 0.09302325581395349, 0.0547945205479452, 0.17647058823529413, 0.06451612903225806, 0.047619047619047616, 0.15789473684210525, 0.0, 0.058823529411764705, 0.15151515151515152, 0.1, 0.21212121212121213, 0.07142857142857142, 0.038461538461538464, 0.043478260869565216, 0.0, 0.16666666666666666, 0.046511627906976744, 0.0, 0.05172413793103448, 0.1, 0.06976744186046512, 0.047619047619047616, 0.03571428571428571, 0.125, 0.06896551724137931, 0.0, 0.06666666666666667, 0.02, 0.0, 0.08333333333333333, 0.10256410256410256, 0.1774193548387097, 0.0, 0.024390243902439025, 0.0196078431372549, 0.022222222222222223, 0.05, 0.08139534883720931, 0.02040816326530612, 0.1, 0.08163265306122448, 0.0, 0.021739130434782608, 0.09523809523809523, 0.14285714285714285, 0.09523809523809523, 0.26744186046511625, 0.07317073170731707, 0.07792207792207792, 0.06976744186046512, 0.2, 0.06382978723404255, 0.10843373493975904, 0.1375, 0.2608695652173913, 0.06521739130434782, 0.16666666666666666, 0.10344827586206896, 0.1095890410958904, 0.06, 0.17391304347826086, 0.06818181818181818, 0.0967741935483871, 0.08571428571428572, 0.04, 0.15625, 0.16666666666666666, 0.021739130434782608, 0.011764705882352941, 0.09722222222222222, 0.0, 0.0392156862745098, 0.10810810810810811, 0.03896103896103896], 'recall': [0.05263157894736842, 0.030303030303030304, 0.0, 0.26666666666666666, 0.32, 0.17391304347826086, 0.5, 0.0, 0.3, 0.1875, 0.17391304347826086, 0.28, 0.10344827586206896, 0.25, 0.0, 0.1111111111111111, 0.25, 0.1875, 0.21875, 0.14285714285714285, 0.058823529411764705, 0.2, 0.125, 0.0, 0.3076923076923077, 0.07894736842105263, 0.20689655172413793, 0.29411764705882354, 0.2857142857142857, 0.09523809523809523, 0.22727272727272727, 0.3888888888888889, 0.11538461538461539, 0.08333333333333333, 0.22727272727272727, 0.18181818181818182, 0.14285714285714285, 0.2608695652173913, 0.1702127659574468, 0.3333333333333333, 0.13636363636363635, 0.16666666666666666, 0.4117647058823529, 0.22448979591836735, 0.1, 0.17391304347826086, 0.13333333333333333, 0.21875, 0.1875, 0.21739130434782608, 0.14814814814814814, 0.07692307692307693, 0.3333333333333333, 0.22727272727272727, 0.06896551724137931, 0.14516129032258066, 0.09375, 0.16, 0.05, 0.16666666666666666, 0.1111111111111111, 0.0, 0.09302325581395349, 0.1875, 0.2558139534883721, 0.25, 0.1875, 0.18181818181818182, 0.0, 0.19047619047619047, 0.13157894736842105, 0.1, 0.0975609756097561, 0.0967741935483871, 0.15384615384615385, 0.05263157894736842, 0.10810810810810811, 0.22857142857142856, 0.10256410256410256, 0.23333333333333334, 0.1724137931034483, 0.13333333333333333, 0.10810810810810811, 0.2, 0.05555555555555555, 0.16216216216216217, 0.3793103448275862, 0.2777777777777778, 0.23333333333333334, 0.28125, 0.11538461538461539, 0.15, 0.1111111111111111, 0.05, 0.0, 0.125, 0.2631578947368421, 0.16666666666666666, 0.1111111111111111, 0.06818181818181818, 0.4583333333333333, 0.42857142857142855, 0.24, 0.13636363636363635, 0.0, 0.14285714285714285, 0.28, 0.18181818181818182, 0.16, 0.041666666666666664, 0.06896551724137931, 0.05970149253731343, 0.09523809523809523, 0.13924050632911392, 0.044444444444444446, 0.21739130434782608, 0.3793103448275862, 0.26666666666666666, 0.1111111111111111, 0.09090909090909091, 0.15625, 0.14285714285714285, 0.1388888888888889, 0.1111111111111111, 0.16666666666666666, 0.2608695652173913, 0.23529411764705882, 0.08695652173913043, 0.12698412698412698, 0.18181818181818182, 0.031746031746031744, 0.16, 0.23809523809523808, 0.3076923076923077, 0.09090909090909091, 0.12121212121212122, 0.1, 0.07575757575757576, 0.23076923076923078, 0.35714285714285715, 0.38461538461538464, 0.13043478260869565, 0.08333333333333333, 0.07894736842105263, 0.04, 0.14285714285714285, 0.18181818181818182, 0.15625, 0.10714285714285714, 0.05, 0.05084745762711865, 0.1724137931034483, 0.034482758620689655, 0.058823529411764705, 0.04, 0.15789473684210525, 0.05555555555555555, 0.05, 0.0, 0.09090909090909091, 0.09375, 0.16666666666666666, 0.15384615384615385, 0.19230769230769232, 0.07407407407407407, 0.1388888888888889, 0.0625, 0.14705882352941177, 0.13636363636363635, 0.25, 0.125, 0.12, 0.2, 0.2, 0.08333333333333333, 0.2916666666666667, 0.0, 0.23809523809523808, 0.15384615384615385, 0.06060606060606061, 0.18181818181818182, 0.0, 0.21052631578947367, 0.1111111111111111, 0.043478260869565216, 0.14814814814814814, 0.0, 0.17391304347826086, 0.045454545454545456, 0.18181818181818182, 0.1935483870967742, 0.09433962264150944, 0.08571428571428572, 0.1590909090909091, 0.2222222222222222, 0.10526315789473684, 0.11764705882352941, 0.05128205128205128, 0.11764705882352941, 0.12121212121212122, 0.2, 0.0851063829787234, 0.22448979591836735, 0.09090909090909091, 0.25, 0.14634146341463414, 0.12, 0.3, 0.23076923076923078, 0.17647058823529413, 0.35714285714285715, 0.0625, 0.2857142857142857, 0.24, 0.0625, 0.12121212121212122, 0.0, 0.07692307692307693, 0.2727272727272727, 0.1724137931034483, 0.16666666666666666, 0.20833333333333334, 0.0, 0.07407407407407407, 0.0, 0.12, 0.12244897959183673, 0.1276595744680851, 0.1724137931034483, 0.125, 0.140625, 0.19298245614035087, 0.0625, 0.06896551724137931, 0.4666666666666667, 0.4090909090909091, 0.3333333333333333, 0.1, 0.20754716981132076, 0.23076923076923078, 0.15789473684210525, 0.20833333333333334, 0.17857142857142858, 0.21621621621621623, 0.2564102564102564, 0.3181818181818182, 0.39473684210526316, 0.18181818181818182, 0.09523809523809523, 0.2692307692307692, 0.25925925925925924, 0.25925925925925924, 0.18181818181818182, 0.16981132075471697, 0.2535211267605634, 0.09090909090909091, 0.21621621621621623, 0.0, 0.058823529411764705, 0.18181818181818182, 0.0967741935483871, 0.16279069767441862, 0.14285714285714285, 0.21052631578947367, 0.15625, 0.17105263157894737, 0.25, 0.10526315789473684, 0.21739130434782608, 0.16, 0.15384615384615385, 0.09090909090909091, 0.21212121212121213, 0.15789473684210525, 0.2222222222222222, 0.13043478260869565, 0.12903225806451613, 0.2549019607843137, 0.16666666666666666, 0.23529411764705882, 0.4444444444444444, 0.046875, 0.09166666666666666, 0.17073170731707318, 0.07017543859649122, 0.2, 0.18032786885245902, 0.2564102564102564, 0.36363636363636365, 0.12903225806451613, 0.06818181818181818, 0.4375, 0.2631578947368421, 0.3333333333333333, 0.14, 0.26666666666666666, 0.16, 0.058823529411764705, 0.125, 0.1, 0.13636363636363635, 0.15625, 0.17647058823529413, 0.3333333333333333, 0.25, 0.09090909090909091, 0.3387096774193548, 0.24, 0.15384615384615385, 0.36363636363636365, 0.16216216216216217, 0.06896551724137931, 0.14285714285714285, 0.13953488372093023, 0.0, 0.15384615384615385, 0.09259259259259259, 0.09302325581395349, 0.2, 0.14285714285714285, 0.043478260869565216, 0.06060606060606061, 0.0, 0.1111111111111111, 0.08333333333333333, 0.0, 0.15789473684210525, 0.16216216216216217, 0.3333333333333333, 0.25806451612903225, 0.08333333333333333, 0.21428571428571427, 0.125, 0.0, 0.037037037037037035, 0.125, 0.0, 0.16, 0.14285714285714285, 0.275, 0.0, 0.038461538461538464, 0.16666666666666666, 0.125, 0.2222222222222222, 0.30434782608695654, 0.07142857142857142, 0.375, 0.16666666666666666, 0.0, 0.3, 0.13333333333333333, 0.2, 0.25, 0.26744186046511625, 0.13043478260869565, 0.2727272727272727, 0.4, 0.30303030303030304, 0.2, 0.32142857142857145, 0.21568627450980393, 0.21428571428571427, 0.08108108108108109, 0.13636363636363635, 0.07317073170731707, 0.19047619047619047, 0.07894736842105263, 0.11428571428571428, 0.13636363636363635, 0.1875, 0.058823529411764705, 0.10526315789473684, 0.23809523809523808, 0.21212121212121213, 0.07692307692307693, 0.05555555555555555, 0.21875, 0.0, 0.25, 0.09090909090909091, 0.21428571428571427], 'fmeasure': [0.03076923076923077, 0.02531645569620253, 0.0, 0.11940298507462686, 0.21333333333333335, 0.12307692307692307, 0.17777777777777778, 0.0, 0.18749999999999997, 0.08823529411764705, 0.1095890410958904, 0.20895522388059704, 0.09090909090909091, 0.16666666666666666, 0.0, 0.05333333333333334, 0.17500000000000002, 0.11320754716981132, 0.18918918918918917, 0.060606060606060615, 0.058823529411764705, 0.052631578947368425, 0.07142857142857144, 0.0, 0.11428571428571428, 0.07317073170731707, 0.16, 0.1408450704225352, 0.14285714285714285, 0.0784313725490196, 0.16666666666666666, 0.208955223880597, 0.08955223880597016, 0.10126582278481013, 0.11904761904761903, 0.07407407407407407, 0.07692307692307693, 0.12000000000000001, 0.1584158415841584, 0.15000000000000002, 0.08695652173913043, 0.044444444444444446, 0.13999999999999999, 0.23157894736842108, 0.07317073170731708, 0.13559322033898305, 0.05194805194805195, 0.1647058823529412, 0.0821917808219178, 0.1492537313432836, 0.10526315789473684, 0.06153846153846155, 0.25396825396825395, 0.136986301369863, 0.045454545454545456, 0.18181818181818182, 0.08108108108108107, 0.13114754098360656, 0.031746031746031744, 0.1111111111111111, 0.07228915662650602, 0.0, 0.10256410256410256, 0.1090909090909091, 0.1964285714285714, 0.15384615384615383, 0.07894736842105264, 0.14117647058823532, 0.0, 0.12903225806451615, 0.10752688172043011, 0.03921568627450981, 0.10526315789473684, 0.09523809523809523, 0.0975609756097561, 0.041666666666666664, 0.10126582278481013, 0.18390804597701152, 0.08247422680412371, 0.11382113821138212, 0.12048192771084339, 0.07407407407407407, 0.0888888888888889, 0.0821917808219178, 0.052631578947368425, 0.16, 0.29729729729729726, 0.12987012987012986, 0.20588235294117646, 0.169811320754717, 0.07894736842105263, 0.08, 0.08450704225352113, 0.042105263157894736, 0.0, 0.14492753623188406, 0.10416666666666667, 0.12121212121212123, 0.09090909090909093, 0.09999999999999999, 0.2178217821782178, 0.125, 0.1846153846153846, 0.10714285714285714, 0.0, 0.12307692307692308, 0.1590909090909091, 0.1518987341772152, 0.13114754098360656, 0.02702702702702703, 0.05405405405405405, 0.07920792079207921, 0.06779661016949153, 0.176, 0.034782608695652174, 0.1941747572815534, 0.12643678160919541, 0.1839080459770115, 0.06741573033707865, 0.09302325581395349, 0.15625, 0.08, 0.16260162601626016, 0.09090909090909093, 0.1728395061728395, 0.15584415584415584, 0.16326530612244897, 0.12307692307692308, 0.16666666666666666, 0.2222222222222222, 0.047058823529411764, 0.09523809523809523, 0.15625, 0.2222222222222222, 0.034482758620689655, 0.10126582278481013, 0.07142857142857142, 0.09803921568627452, 0.1714285714285714, 0.2857142857142857, 0.13157894736842105, 0.06976744186046512, 0.03389830508474576, 0.08450704225352113, 0.024999999999999998, 0.05555555555555555, 0.14035087719298245, 0.13157894736842105, 0.0821917808219178, 0.04081632653061224, 0.05825242718446602, 0.12820512820512822, 0.03174603174603175, 0.049999999999999996, 0.03703703703703704, 0.08333333333333333, 0.025974025974025976, 0.029411764705882353, 0.0, 0.0449438202247191, 0.10909090909090909, 0.14634146341463414, 0.06779661016949153, 0.125, 0.06349206349206349, 0.125, 0.07547169811320754, 0.10309278350515463, 0.07058823529411765, 0.20253164556962025, 0.0816326530612245, 0.075, 0.12048192771084339, 0.14545454545454545, 0.03571428571428572, 0.2028985507246377, 0.0, 0.15384615384615383, 0.06451612903225806, 0.05970149253731344, 0.17777777777777776, 0.0, 0.11111111111111112, 0.05194805194805195, 0.028169014084507043, 0.125, 0.0, 0.17391304347826086, 0.0392156862745098, 0.1348314606741573, 0.17391304347826086, 0.10204081632653061, 0.06976744186046513, 0.14285714285714285, 0.20408163265306123, 0.0851063829787234, 0.07142857142857141, 0.05, 0.06896551724137931, 0.08333333333333333, 0.0821917808219178, 0.08333333333333333, 0.19130434782608696, 0.03389830508474576, 0.1276595744680851, 0.15584415584415584, 0.05405405405405404, 0.05405405405405406, 0.15384615384615388, 0.058252427184466014, 0.09259259259259259, 0.05333333333333333, 0.09523809523809522, 0.1935483870967742, 0.07692307692307693, 0.11764705882352942, 0.0, 0.05714285714285715, 0.14457831325301204, 0.15151515151515152, 0.0851063829787234, 0.08196721311475412, 0.0, 0.05970149253731344, 0.0, 0.1188118811881188, 0.11764705882352941, 0.1348314606741573, 0.09900990099009903, 0.125, 0.16216216216216214, 0.16296296296296295, 0.07317073170731707, 0.06399999999999999, 0.31111111111111117, 0.2181818181818182, 0.22641509433962265, 0.12371134020618559, 0.22448979591836737, 0.20454545454545456, 0.18604651162790695, 0.15873015873015872, 0.12820512820512822, 0.15841584158415842, 0.15267175572519084, 0.2434782608695652, 0.21739130434782605, 0.04819277108433734, 0.043010752688172046, 0.14285714285714288, 0.21212121212121213, 0.14141414141414144, 0.2222222222222222, 0.1176470588235294, 0.3076923076923077, 0.08450704225352113, 0.16666666666666669, 0.0, 0.035714285714285705, 0.07619047619047621, 0.14423076923076922, 0.13333333333333333, 0.125, 0.1081081081081081, 0.08333333333333333, 0.23008849557522124, 0.11764705882352941, 0.08333333333333333, 0.16129032258064516, 0.13559322033898305, 0.08695652173913043, 0.08163265306122448, 0.22222222222222224, 0.08695652173913043, 0.06153846153846154, 0.1714285714285714, 0.07692307692307691, 0.23423423423423423, 0.15625, 0.2191780821917808, 0.27272727272727276, 0.05714285714285714, 0.13414634146341461, 0.17500000000000002, 0.0761904761904762, 0.07692307692307693, 0.1437908496732026, 0.19801980198019803, 0.23188405797101447, 0.07207207207207207, 0.0689655172413793, 0.175, 0.1282051282051282, 0.1111111111111111, 0.1794871794871795, 0.125, 0.1142857142857143, 0.030303030303030304, 0.03389830508474576, 0.02702702702702703, 0.08333333333333333, 0.18181818181818182, 0.09677419354838708, 0.13636363636363638, 0.1388888888888889, 0.03773584905660377, 0.34146341463414637, 0.14634146341463414, 0.11594202898550725, 0.09523809523809523, 0.16901408450704225, 0.06666666666666667, 0.07142857142857142, 0.14814814814814817, 0.0, 0.0851063829787234, 0.11494252873563218, 0.09638554216867469, 0.2058823529411765, 0.09523809523809523, 0.04081632653061224, 0.05063291139240506, 0.0, 0.13333333333333333, 0.059701492537313425, 0.0, 0.07792207792207792, 0.12371134020618559, 0.11538461538461539, 0.08040201005025124, 0.05, 0.15789473684210525, 0.08888888888888889, 0.0, 0.047619047619047616, 0.03448275862068966, 0.0, 0.1095890410958904, 0.11940298507462686, 0.21568627450980393, 0.0, 0.029850746268656723, 0.03508771929824561, 0.03773584905660377, 0.0816326530612245, 0.12844036697247707, 0.031746031746031744, 0.15789473684210528, 0.10958904109589039, 0.0, 0.04054054054054054, 0.1111111111111111, 0.16666666666666666, 0.13793103448275862, 0.26744186046511625, 0.09375, 0.1212121212121212, 0.1188118811881188, 0.24096385542168675, 0.0967741935483871, 0.16216216216216217, 0.16793893129770993, 0.23529411764705882, 0.07228915662650602, 0.15, 0.0857142857142857, 0.1391304347826087, 0.06818181818181818, 0.13793103448275862, 0.0909090909090909, 0.12765957446808507, 0.06976744186046513, 0.057971014492753624, 0.18867924528301888, 0.18666666666666665, 0.03389830508474576, 0.019417475728155338, 0.13461538461538464, 0.0, 0.06779661016949153, 0.09876543209876544, 0.06593406593406594]}\n",
            "Average for precision is 0.09455150893744342\n",
            "Average for recall is 0.16222615309649221\n",
            "Average for fmeasure is 0.1097345637095998\n",
            "/content/drive/MyDrive/Colab Notebooks/data/testdata/QuantTree_ Histograms for Change Detection in Multivariate Data Streams.csv\n",
            "['Second, it underlines the lack of robustness of neural networks and questions their ability to generalize in settings where the train and test distributions can be (slightly) different as is the case for the distributions of legitimate and adversarial examples.\\n', 'For this reason, and because it is useful for understanding system performance, most studies also report disfluency detection performance, which is measured in terms of the F1 score for detecting whether a word is in an edit region.', 'However, the intuition is that if the LP relaxation is fairly tight and the obtained dual solution is close to optimal, it can still provide useful information about the primal problem.', 'In each plot of Figure 2 we consider normalized relative losses at a specific cutoff time (1000 examples in the left plot, and all examples in the center and right), and for each method we plot the number of datasets where it achieves loss below a threshold, as a function of the threshold.', 'X,X(f −mX) , (4) σ2 = kx∗,x∗ − kx∗,XK−1X,XkX,x∗ , (5)\\nwhere kA,B denotes the scalar or vector of covariances for each pair of elements inA andB. In this work, the squared exponential kernel with Automatic Relevance Determination (ARD) (Williams & Rasmussen, 2005) with hyperparameters θGP is employed.', 'We then compare submodular maximization with cardinality constraint k = 3 on the reduced sets S (returned by the baselines) and that of the whole ground set Ω. To this end, we sample 20 users from the test group and compute their average values.', 'Our experiments (Section 5) show that QuantTree enables good detection performance in high dimensional streams.', 'We assume that a training set TR = {xi ∈ X , i = 1, . . .', 'Histograms: we define a histogram as:\\nh = {(Sk, π̂k)}k=1,...,K , (1) where the K subsets Sk ⊆ X form a partition of Rd, i.e.,⋃K k=1 Sk = Rd and Sj ∩ Si = ∅, for j 6=', 'We focus on HTs that are based on a test statistic Th defined over the histogram h, like for instance the Pearson statistic (Lehmann & Romano, 2006).', 'Thus, Th uniquely depends on {yk}k=1,...,K , where yk denotes the number of samples in W falling in Sk.', 'We detect a change in the incoming W when\\nTh(W ) = Th(y1, . . .', 'There are two important comments.', '7: Draw γ ∈ {0, 1} from a Bernoulli(0.5).', '8: if γ = 0 then 9: Define Sk = {x ∈', 'Here we describe QuantTree1, an algorithm to define histograms h through a recursive binary splitting of the input space X .', '[xn]i, i.e., the values of the i-th component for each xn ∈ Xk (lines 5).', ', B do 2: Draw from ψ0 a training set TRb of N samples.', '3: Use QuantTree to compute the histogram hb with K bins and target probabilities {πk}k over TR.\\n4: Draw a batch Wb containing ν points from φ0.', 'Indexes i and parameter γ are randomly chosen to add variability to the histogram construction.', 'Figure 1(a) shows a tree obtained from a bivariate Gaussian training set, defined by K = 4 bins, each having probability πk = N/4.', 'This result follows from Theorem 1, that is proved in Section 4.\\n', 'At first we generate B training sets {TRb}b=1,...,B , sampling N points from ψ0 and, for each training set, we build a histogram hb using QuantTree (lines 2-3).', 'Then, for each hb we generate a batch Wb of ν points drawn from ψ0, and compute the value of the statistic tb = Th(Wb) (lines 4-5).', 'Finally, we estimate τ (line 7) from the set TB = {t1, . . .', ', tB} as the 1 − α quantile of the empirical distribution of Th over the generated batches, i.e.\\nτ = min { t ∈ TB : #{v ∈ TB : v > t} ≤ αB } , (5)\\nwhere #A denotes the cardinality of a set A.\\nTo take full advantage of the distribution-free nature of the procedure, we set ψ0 to a univariate uniform distribution\\nU(0, 1).', 'In contrast, thresholds defined by Algorithm 2 hold also in case of limited sample size, since they are not based on an asymptotic result.\\n', 'These two statistics will be used for our experiments in Section 5, using thresholds reported in Table 1 for different values of N , K, ν and choosing πk = 1/K, k = 1, . . .']\n",
            "239/239 [==============================] - 177s 740ms/step\n",
            "{'precision': [0.021739130434782608, 0.021739130434782608, 0.0, 0.07692307692307693, 0.16, 0.09523809523809523, 0.10810810810810811, 0.0, 0.13636363636363635, 0.057692307692307696, 0.08, 0.16666666666666666, 0.08108108108108109, 0.125, 0.0, 0.03508771929824561, 0.1346153846153846, 0.08108108108108109, 0.16666666666666666, 0.038461538461538464, 0.058823529411764705, 0.030303030303030304, 0.05, 0.0, 0.07017543859649122, 0.06818181818181818, 0.13043478260869565, 0.09259259259259259, 0.09523809523809523, 0.06666666666666667, 0.13157894736842105, 0.14285714285714285, 0.07317073170731707, 0.12903225806451613, 0.08064516129032258, 0.046511627906976744, 0.05263157894736842, 0.07792207792207792, 0.14814814814814814, 0.0967741935483871, 0.06382978723404255, 0.02564102564102564, 0.08433734939759036, 0.2391304347826087, 0.057692307692307696, 0.1111111111111111, 0.03225806451612903, 0.1320754716981132, 0.05263157894736842, 0.11363636363636363, 0.08163265306122448, 0.05128205128205128, 0.20512820512820512, 0.09803921568627451, 0.03389830508474576, 0.24324324324324326, 0.07142857142857142, 0.1111111111111111, 0.023255813953488372, 0.08333333333333333, 0.05357142857142857, 0.0, 0.11428571428571428, 0.07692307692307693, 0.15942028985507245, 0.1111111111111111, 0.05, 0.11538461538461539, 0.0, 0.0975609756097561, 0.09090909090909091, 0.024390243902439025, 0.11428571428571428, 0.09375, 0.07142857142857142, 0.034482758620689655, 0.09523809523809523, 0.15384615384615385, 0.06896551724137931, 0.07526881720430108, 0.09259259259259259, 0.05128205128205128, 0.07547169811320754, 0.05172413793103448, 0.05, 0.15789473684210525, 0.24444444444444444, 0.0847457627118644, 0.18421052631578946, 0.12162162162162163, 0.06, 0.05454545454545454, 0.06818181818181818, 0.03636363636363636, 0.0, 0.1724137931034483, 0.06493506493506493, 0.09523809523809523, 0.07692307692307693, 0.1875, 0.14285714285714285, 0.07317073170731707, 0.15, 0.08823529411764706, 0.0, 0.10810810810810811, 0.1111111111111111, 0.13043478260869565, 0.1111111111111111, 0.02, 0.044444444444444446, 0.11764705882352941, 0.05263157894736842, 0.2391304347826087, 0.02857142857142857, 0.17543859649122806, 0.07586206896551724, 0.14035087719298245, 0.04838709677419355, 0.09523809523809523, 0.15625, 0.05555555555555555, 0.19607843137254902, 0.07692307692307693, 0.1794871794871795, 0.1111111111111111, 0.125, 0.21052631578947367, 0.24242424242424243, 0.2857142857142857, 0.09090909090909091, 0.06779661016949153, 0.11627906976744186, 0.17391304347826086, 0.02127659574468085, 0.08695652173913043, 0.05555555555555555, 0.1388888888888889, 0.13636363636363635, 0.23809523809523808, 0.07936507936507936, 0.047619047619047616, 0.02127659574468085, 0.09090909090909091, 0.01818181818181818, 0.034482758620689655, 0.11428571428571428, 0.11363636363636363, 0.06666666666666667, 0.034482758620689655, 0.06818181818181818, 0.10204081632653061, 0.029411764705882353, 0.043478260869565216, 0.034482758620689655, 0.05660377358490566, 0.01694915254237288, 0.020833333333333332, 0.0, 0.029850746268656716, 0.13043478260869565, 0.13043478260869565, 0.043478260869565216, 0.09259259259259259, 0.05555555555555555, 0.11363636363636363, 0.09523809523809523, 0.07936507936507936, 0.047619047619047616, 0.1702127659574468, 0.06060606060606061, 0.05454545454545454, 0.08620689655172414, 0.11428571428571428, 0.022727272727272728, 0.15555555555555556, 0.0, 0.11363636363636363, 0.04081632653061224, 0.058823529411764705, 0.17391304347826086, 0.0, 0.07547169811320754, 0.03389830508474576, 0.020833333333333332, 0.10810810810810811, 0.0, 0.17391304347826086, 0.034482758620689655, 0.10714285714285714, 0.15789473684210525, 0.1111111111111111, 0.058823529411764705, 0.12962962962962962, 0.18867924528301888, 0.07142857142857142, 0.05128205128205128, 0.04878048780487805, 0.04878048780487805, 0.06349206349206349, 0.05172413793103448, 0.08163265306122448, 0.16666666666666666, 0.020833333333333332, 0.08571428571428572, 0.16666666666666666, 0.03488372093023256, 0.0297029702970297, 0.11538461538461539, 0.03488372093023256, 0.05319148936170213, 0.046511627906976744, 0.05714285714285714, 0.16216216216216217, 0.1, 0.11428571428571428, 0.0, 0.045454545454545456, 0.09836065573770492, 0.13513513513513514, 0.05714285714285714, 0.05102040816326531, 0.0, 0.05, 0.0, 0.11764705882352941, 0.11320754716981132, 0.14285714285714285, 0.06944444444444445, 0.125, 0.19148936170212766, 0.14102564102564102, 0.08823529411764706, 0.05970149253731343, 0.23333333333333334, 0.1487603305785124, 0.17142857142857143, 0.16216216216216217, 0.24444444444444444, 0.1836734693877551, 0.22641509433962265, 0.1282051282051282, 0.1, 0.125, 0.10869565217391304, 0.19718309859154928, 0.15, 0.027777777777777776, 0.027777777777777776, 0.09722222222222222, 0.1794871794871795, 0.09722222222222222, 0.2857142857142857, 0.09, 0.391304347826087, 0.07894736842105263, 0.13559322033898305, 0.0, 0.02564102564102564, 0.04819277108433735, 0.2830188679245283, 0.11290322580645161, 0.1111111111111111, 0.07272727272727272, 0.056818181818181816, 0.35135135135135137, 0.07692307692307693, 0.06896551724137931, 0.1282051282051282, 0.11764705882352941, 0.06060606060606061, 0.07407407407407407, 0.23333333333333334, 0.06, 0.03571428571428571, 0.25, 0.0547945205479452, 0.21666666666666667, 0.14705882352941177, 0.20512820512820512, 0.19672131147540983, 0.07317073170731707, 0.25, 0.1794871794871795, 0.08333333333333333, 0.047619047619047616, 0.11956521739130435, 0.16129032258064516, 0.1702127659574468, 0.05, 0.06976744186046512, 0.109375, 0.0847457627118644, 0.06666666666666667, 0.25, 0.08163265306122448, 0.08888888888888889, 0.02040816326530612, 0.0196078431372549, 0.015625, 0.06, 0.21739130434782608, 0.06666666666666667, 0.08571428571428572, 0.09615384615384616, 0.023809523809523808, 0.3442622950819672, 0.10526315789473684, 0.09302325581395349, 0.0547945205479452, 0.17647058823529413, 0.06451612903225806, 0.047619047619047616, 0.15789473684210525, 0.0, 0.058823529411764705, 0.15151515151515152, 0.1, 0.21212121212121213, 0.07142857142857142, 0.038461538461538464, 0.043478260869565216, 0.0, 0.16666666666666666, 0.046511627906976744, 0.0, 0.05172413793103448, 0.1, 0.06976744186046512, 0.047619047619047616, 0.03571428571428571, 0.125, 0.06896551724137931, 0.0, 0.06666666666666667, 0.02, 0.0, 0.08333333333333333, 0.10256410256410256, 0.1774193548387097, 0.0, 0.024390243902439025, 0.0196078431372549, 0.022222222222222223, 0.05, 0.08139534883720931, 0.02040816326530612, 0.1, 0.08163265306122448, 0.0, 0.021739130434782608, 0.09523809523809523, 0.14285714285714285, 0.09523809523809523, 0.26744186046511625, 0.07317073170731707, 0.07792207792207792, 0.06976744186046512, 0.2, 0.06382978723404255, 0.10843373493975904, 0.1375, 0.2608695652173913, 0.06521739130434782, 0.16666666666666666, 0.10344827586206896, 0.1095890410958904, 0.06, 0.17391304347826086, 0.06818181818181818, 0.0967741935483871, 0.08571428571428572, 0.04, 0.15625, 0.16666666666666666, 0.021739130434782608, 0.011764705882352941, 0.09722222222222222, 0.0, 0.0392156862745098, 0.10810810810810811, 0.03896103896103896, 0.19298245614035087, 0.04918032786885246, 0.15217391304347827, 0.19672131147540983, 0.13333333333333333, 0.21428571428571427, 0.04, 0.06976744186046512, 0.13333333333333333, 0.07894736842105263, 0.06, 0.054945054945054944, 0.0, 0.03278688524590164, 0.03225806451612903, 0.06944444444444445, 0.04040404040404041, 0.06, 0.06976744186046512, 0.045454545454545456, 0.04, 0.024390243902439025, 0.046875, 0.07142857142857142, 0.019230769230769232, 0.23809523809523808, 0.05714285714285714, 0.07547169811320754], 'recall': [0.05263157894736842, 0.030303030303030304, 0.0, 0.26666666666666666, 0.32, 0.17391304347826086, 0.5, 0.0, 0.3, 0.1875, 0.17391304347826086, 0.28, 0.10344827586206896, 0.25, 0.0, 0.1111111111111111, 0.25, 0.1875, 0.21875, 0.14285714285714285, 0.058823529411764705, 0.2, 0.125, 0.0, 0.3076923076923077, 0.07894736842105263, 0.20689655172413793, 0.29411764705882354, 0.2857142857142857, 0.09523809523809523, 0.22727272727272727, 0.3888888888888889, 0.11538461538461539, 0.08333333333333333, 0.22727272727272727, 0.18181818181818182, 0.14285714285714285, 0.2608695652173913, 0.1702127659574468, 0.3333333333333333, 0.13636363636363635, 0.16666666666666666, 0.4117647058823529, 0.22448979591836735, 0.1, 0.17391304347826086, 0.13333333333333333, 0.21875, 0.1875, 0.21739130434782608, 0.14814814814814814, 0.07692307692307693, 0.3333333333333333, 0.22727272727272727, 0.06896551724137931, 0.14516129032258066, 0.09375, 0.16, 0.05, 0.16666666666666666, 0.1111111111111111, 0.0, 0.09302325581395349, 0.1875, 0.2558139534883721, 0.25, 0.1875, 0.18181818181818182, 0.0, 0.19047619047619047, 0.13157894736842105, 0.1, 0.0975609756097561, 0.0967741935483871, 0.15384615384615385, 0.05263157894736842, 0.10810810810810811, 0.22857142857142856, 0.10256410256410256, 0.23333333333333334, 0.1724137931034483, 0.13333333333333333, 0.10810810810810811, 0.2, 0.05555555555555555, 0.16216216216216217, 0.3793103448275862, 0.2777777777777778, 0.23333333333333334, 0.28125, 0.11538461538461539, 0.15, 0.1111111111111111, 0.05, 0.0, 0.125, 0.2631578947368421, 0.16666666666666666, 0.1111111111111111, 0.06818181818181818, 0.4583333333333333, 0.42857142857142855, 0.24, 0.13636363636363635, 0.0, 0.14285714285714285, 0.28, 0.18181818181818182, 0.16, 0.041666666666666664, 0.06896551724137931, 0.05970149253731343, 0.09523809523809523, 0.13924050632911392, 0.044444444444444446, 0.21739130434782608, 0.3793103448275862, 0.26666666666666666, 0.1111111111111111, 0.09090909090909091, 0.15625, 0.14285714285714285, 0.1388888888888889, 0.1111111111111111, 0.16666666666666666, 0.2608695652173913, 0.23529411764705882, 0.08695652173913043, 0.12698412698412698, 0.18181818181818182, 0.031746031746031744, 0.16, 0.23809523809523808, 0.3076923076923077, 0.09090909090909091, 0.12121212121212122, 0.1, 0.07575757575757576, 0.23076923076923078, 0.35714285714285715, 0.38461538461538464, 0.13043478260869565, 0.08333333333333333, 0.07894736842105263, 0.04, 0.14285714285714285, 0.18181818181818182, 0.15625, 0.10714285714285714, 0.05, 0.05084745762711865, 0.1724137931034483, 0.034482758620689655, 0.058823529411764705, 0.04, 0.15789473684210525, 0.05555555555555555, 0.05, 0.0, 0.09090909090909091, 0.09375, 0.16666666666666666, 0.15384615384615385, 0.19230769230769232, 0.07407407407407407, 0.1388888888888889, 0.0625, 0.14705882352941177, 0.13636363636363635, 0.25, 0.125, 0.12, 0.2, 0.2, 0.08333333333333333, 0.2916666666666667, 0.0, 0.23809523809523808, 0.15384615384615385, 0.06060606060606061, 0.18181818181818182, 0.0, 0.21052631578947367, 0.1111111111111111, 0.043478260869565216, 0.14814814814814814, 0.0, 0.17391304347826086, 0.045454545454545456, 0.18181818181818182, 0.1935483870967742, 0.09433962264150944, 0.08571428571428572, 0.1590909090909091, 0.2222222222222222, 0.10526315789473684, 0.11764705882352941, 0.05128205128205128, 0.11764705882352941, 0.12121212121212122, 0.2, 0.0851063829787234, 0.22448979591836735, 0.09090909090909091, 0.25, 0.14634146341463414, 0.12, 0.3, 0.23076923076923078, 0.17647058823529413, 0.35714285714285715, 0.0625, 0.2857142857142857, 0.24, 0.0625, 0.12121212121212122, 0.0, 0.07692307692307693, 0.2727272727272727, 0.1724137931034483, 0.16666666666666666, 0.20833333333333334, 0.0, 0.07407407407407407, 0.0, 0.12, 0.12244897959183673, 0.1276595744680851, 0.1724137931034483, 0.125, 0.140625, 0.19298245614035087, 0.0625, 0.06896551724137931, 0.4666666666666667, 0.4090909090909091, 0.3333333333333333, 0.1, 0.20754716981132076, 0.23076923076923078, 0.15789473684210525, 0.20833333333333334, 0.17857142857142858, 0.21621621621621623, 0.2564102564102564, 0.3181818181818182, 0.39473684210526316, 0.18181818181818182, 0.09523809523809523, 0.2692307692307692, 0.25925925925925924, 0.25925925925925924, 0.18181818181818182, 0.16981132075471697, 0.2535211267605634, 0.09090909090909091, 0.21621621621621623, 0.0, 0.058823529411764705, 0.18181818181818182, 0.0967741935483871, 0.16279069767441862, 0.14285714285714285, 0.21052631578947367, 0.15625, 0.17105263157894737, 0.25, 0.10526315789473684, 0.21739130434782608, 0.16, 0.15384615384615385, 0.09090909090909091, 0.21212121212121213, 0.15789473684210525, 0.2222222222222222, 0.13043478260869565, 0.12903225806451613, 0.2549019607843137, 0.16666666666666666, 0.23529411764705882, 0.4444444444444444, 0.046875, 0.09166666666666666, 0.17073170731707318, 0.07017543859649122, 0.2, 0.18032786885245902, 0.2564102564102564, 0.36363636363636365, 0.12903225806451613, 0.06818181818181818, 0.4375, 0.2631578947368421, 0.3333333333333333, 0.14, 0.26666666666666666, 0.16, 0.058823529411764705, 0.125, 0.1, 0.13636363636363635, 0.15625, 0.17647058823529413, 0.3333333333333333, 0.25, 0.09090909090909091, 0.3387096774193548, 0.24, 0.15384615384615385, 0.36363636363636365, 0.16216216216216217, 0.06896551724137931, 0.14285714285714285, 0.13953488372093023, 0.0, 0.15384615384615385, 0.09259259259259259, 0.09302325581395349, 0.2, 0.14285714285714285, 0.043478260869565216, 0.06060606060606061, 0.0, 0.1111111111111111, 0.08333333333333333, 0.0, 0.15789473684210525, 0.16216216216216217, 0.3333333333333333, 0.25806451612903225, 0.08333333333333333, 0.21428571428571427, 0.125, 0.0, 0.037037037037037035, 0.125, 0.0, 0.16, 0.14285714285714285, 0.275, 0.0, 0.038461538461538464, 0.16666666666666666, 0.125, 0.2222222222222222, 0.30434782608695654, 0.07142857142857142, 0.375, 0.16666666666666666, 0.0, 0.3, 0.13333333333333333, 0.2, 0.25, 0.26744186046511625, 0.13043478260869565, 0.2727272727272727, 0.4, 0.30303030303030304, 0.2, 0.32142857142857145, 0.21568627450980393, 0.21428571428571427, 0.08108108108108109, 0.13636363636363635, 0.07317073170731707, 0.19047619047619047, 0.07894736842105263, 0.11428571428571428, 0.13636363636363635, 0.1875, 0.058823529411764705, 0.10526315789473684, 0.23809523809523808, 0.21212121212121213, 0.07692307692307693, 0.05555555555555555, 0.21875, 0.0, 0.25, 0.09090909090909091, 0.21428571428571427, 0.275, 0.07894736842105263, 0.22580645161290322, 0.22641509433962265, 0.19607843137254902, 0.2857142857142857, 0.06666666666666667, 0.2727272727272727, 0.16216216216216217, 0.23076923076923078, 0.14285714285714285, 0.38461538461538464, 0.0, 0.2222222222222222, 0.125, 0.25, 0.23529411764705882, 0.23076923076923078, 0.1111111111111111, 0.21428571428571427, 0.08333333333333333, 0.08333333333333333, 0.1935483870967742, 0.1111111111111111, 0.1, 0.1724137931034483, 0.08333333333333333, 0.12903225806451613], 'fmeasure': [0.03076923076923077, 0.02531645569620253, 0.0, 0.11940298507462686, 0.21333333333333335, 0.12307692307692307, 0.17777777777777778, 0.0, 0.18749999999999997, 0.08823529411764705, 0.1095890410958904, 0.20895522388059704, 0.09090909090909091, 0.16666666666666666, 0.0, 0.05333333333333334, 0.17500000000000002, 0.11320754716981132, 0.18918918918918917, 0.060606060606060615, 0.058823529411764705, 0.052631578947368425, 0.07142857142857144, 0.0, 0.11428571428571428, 0.07317073170731707, 0.16, 0.1408450704225352, 0.14285714285714285, 0.0784313725490196, 0.16666666666666666, 0.208955223880597, 0.08955223880597016, 0.10126582278481013, 0.11904761904761903, 0.07407407407407407, 0.07692307692307693, 0.12000000000000001, 0.1584158415841584, 0.15000000000000002, 0.08695652173913043, 0.044444444444444446, 0.13999999999999999, 0.23157894736842108, 0.07317073170731708, 0.13559322033898305, 0.05194805194805195, 0.1647058823529412, 0.0821917808219178, 0.1492537313432836, 0.10526315789473684, 0.06153846153846155, 0.25396825396825395, 0.136986301369863, 0.045454545454545456, 0.18181818181818182, 0.08108108108108107, 0.13114754098360656, 0.031746031746031744, 0.1111111111111111, 0.07228915662650602, 0.0, 0.10256410256410256, 0.1090909090909091, 0.1964285714285714, 0.15384615384615383, 0.07894736842105264, 0.14117647058823532, 0.0, 0.12903225806451615, 0.10752688172043011, 0.03921568627450981, 0.10526315789473684, 0.09523809523809523, 0.0975609756097561, 0.041666666666666664, 0.10126582278481013, 0.18390804597701152, 0.08247422680412371, 0.11382113821138212, 0.12048192771084339, 0.07407407407407407, 0.0888888888888889, 0.0821917808219178, 0.052631578947368425, 0.16, 0.29729729729729726, 0.12987012987012986, 0.20588235294117646, 0.169811320754717, 0.07894736842105263, 0.08, 0.08450704225352113, 0.042105263157894736, 0.0, 0.14492753623188406, 0.10416666666666667, 0.12121212121212123, 0.09090909090909093, 0.09999999999999999, 0.2178217821782178, 0.125, 0.1846153846153846, 0.10714285714285714, 0.0, 0.12307692307692308, 0.1590909090909091, 0.1518987341772152, 0.13114754098360656, 0.02702702702702703, 0.05405405405405405, 0.07920792079207921, 0.06779661016949153, 0.176, 0.034782608695652174, 0.1941747572815534, 0.12643678160919541, 0.1839080459770115, 0.06741573033707865, 0.09302325581395349, 0.15625, 0.08, 0.16260162601626016, 0.09090909090909093, 0.1728395061728395, 0.15584415584415584, 0.16326530612244897, 0.12307692307692308, 0.16666666666666666, 0.2222222222222222, 0.047058823529411764, 0.09523809523809523, 0.15625, 0.2222222222222222, 0.034482758620689655, 0.10126582278481013, 0.07142857142857142, 0.09803921568627452, 0.1714285714285714, 0.2857142857142857, 0.13157894736842105, 0.06976744186046512, 0.03389830508474576, 0.08450704225352113, 0.024999999999999998, 0.05555555555555555, 0.14035087719298245, 0.13157894736842105, 0.0821917808219178, 0.04081632653061224, 0.05825242718446602, 0.12820512820512822, 0.03174603174603175, 0.049999999999999996, 0.03703703703703704, 0.08333333333333333, 0.025974025974025976, 0.029411764705882353, 0.0, 0.0449438202247191, 0.10909090909090909, 0.14634146341463414, 0.06779661016949153, 0.125, 0.06349206349206349, 0.125, 0.07547169811320754, 0.10309278350515463, 0.07058823529411765, 0.20253164556962025, 0.0816326530612245, 0.075, 0.12048192771084339, 0.14545454545454545, 0.03571428571428572, 0.2028985507246377, 0.0, 0.15384615384615383, 0.06451612903225806, 0.05970149253731344, 0.17777777777777776, 0.0, 0.11111111111111112, 0.05194805194805195, 0.028169014084507043, 0.125, 0.0, 0.17391304347826086, 0.0392156862745098, 0.1348314606741573, 0.17391304347826086, 0.10204081632653061, 0.06976744186046513, 0.14285714285714285, 0.20408163265306123, 0.0851063829787234, 0.07142857142857141, 0.05, 0.06896551724137931, 0.08333333333333333, 0.0821917808219178, 0.08333333333333333, 0.19130434782608696, 0.03389830508474576, 0.1276595744680851, 0.15584415584415584, 0.05405405405405404, 0.05405405405405406, 0.15384615384615388, 0.058252427184466014, 0.09259259259259259, 0.05333333333333333, 0.09523809523809522, 0.1935483870967742, 0.07692307692307693, 0.11764705882352942, 0.0, 0.05714285714285715, 0.14457831325301204, 0.15151515151515152, 0.0851063829787234, 0.08196721311475412, 0.0, 0.05970149253731344, 0.0, 0.1188118811881188, 0.11764705882352941, 0.1348314606741573, 0.09900990099009903, 0.125, 0.16216216216216214, 0.16296296296296295, 0.07317073170731707, 0.06399999999999999, 0.31111111111111117, 0.2181818181818182, 0.22641509433962265, 0.12371134020618559, 0.22448979591836737, 0.20454545454545456, 0.18604651162790695, 0.15873015873015872, 0.12820512820512822, 0.15841584158415842, 0.15267175572519084, 0.2434782608695652, 0.21739130434782605, 0.04819277108433734, 0.043010752688172046, 0.14285714285714288, 0.21212121212121213, 0.14141414141414144, 0.2222222222222222, 0.1176470588235294, 0.3076923076923077, 0.08450704225352113, 0.16666666666666669, 0.0, 0.035714285714285705, 0.07619047619047621, 0.14423076923076922, 0.13333333333333333, 0.125, 0.1081081081081081, 0.08333333333333333, 0.23008849557522124, 0.11764705882352941, 0.08333333333333333, 0.16129032258064516, 0.13559322033898305, 0.08695652173913043, 0.08163265306122448, 0.22222222222222224, 0.08695652173913043, 0.06153846153846154, 0.1714285714285714, 0.07692307692307691, 0.23423423423423423, 0.15625, 0.2191780821917808, 0.27272727272727276, 0.05714285714285714, 0.13414634146341461, 0.17500000000000002, 0.0761904761904762, 0.07692307692307693, 0.1437908496732026, 0.19801980198019803, 0.23188405797101447, 0.07207207207207207, 0.0689655172413793, 0.175, 0.1282051282051282, 0.1111111111111111, 0.1794871794871795, 0.125, 0.1142857142857143, 0.030303030303030304, 0.03389830508474576, 0.02702702702702703, 0.08333333333333333, 0.18181818181818182, 0.09677419354838708, 0.13636363636363638, 0.1388888888888889, 0.03773584905660377, 0.34146341463414637, 0.14634146341463414, 0.11594202898550725, 0.09523809523809523, 0.16901408450704225, 0.06666666666666667, 0.07142857142857142, 0.14814814814814817, 0.0, 0.0851063829787234, 0.11494252873563218, 0.09638554216867469, 0.2058823529411765, 0.09523809523809523, 0.04081632653061224, 0.05063291139240506, 0.0, 0.13333333333333333, 0.059701492537313425, 0.0, 0.07792207792207792, 0.12371134020618559, 0.11538461538461539, 0.08040201005025124, 0.05, 0.15789473684210525, 0.08888888888888889, 0.0, 0.047619047619047616, 0.03448275862068966, 0.0, 0.1095890410958904, 0.11940298507462686, 0.21568627450980393, 0.0, 0.029850746268656723, 0.03508771929824561, 0.03773584905660377, 0.0816326530612245, 0.12844036697247707, 0.031746031746031744, 0.15789473684210528, 0.10958904109589039, 0.0, 0.04054054054054054, 0.1111111111111111, 0.16666666666666666, 0.13793103448275862, 0.26744186046511625, 0.09375, 0.1212121212121212, 0.1188118811881188, 0.24096385542168675, 0.0967741935483871, 0.16216216216216217, 0.16793893129770993, 0.23529411764705882, 0.07228915662650602, 0.15, 0.0857142857142857, 0.1391304347826087, 0.06818181818181818, 0.13793103448275862, 0.0909090909090909, 0.12765957446808507, 0.06976744186046513, 0.057971014492753624, 0.18867924528301888, 0.18666666666666665, 0.03389830508474576, 0.019417475728155338, 0.13461538461538464, 0.0, 0.06779661016949153, 0.09876543209876544, 0.06593406593406594, 0.2268041237113402, 0.06060606060606061, 0.1818181818181818, 0.21052631578947367, 0.15873015873015875, 0.24489795918367344, 0.05, 0.11111111111111109, 0.14634146341463417, 0.11764705882352941, 0.08450704225352111, 0.09615384615384616, 0.0, 0.05714285714285715, 0.05128205128205128, 0.10869565217391305, 0.06896551724137932, 0.09523809523809523, 0.08571428571428572, 0.07500000000000001, 0.05405405405405406, 0.037735849056603765, 0.07547169811320754, 0.08695652173913043, 0.03225806451612903, 0.19999999999999998, 0.06779661016949153, 0.09523809523809525]}\n",
            "Average for precision is 0.0937002352893879\n",
            "Average for recall is 0.16308033638909633\n",
            "Average for fmeasure is 0.10926657563498036\n",
            "/content/drive/MyDrive/Colab Notebooks/data/testdata/Reinforcement Learning in Multi-Party Trading Dialog.csv\n",
            "['In the remainder of the paper, we first discuss the previous work on adversarial examples.', 'Our core corpus is Switchboard-NXT (Calhoun et al., 2010), a subset of the Switchboard corpus (Godfrey and Holliman, 1993): 2,400 telephone conversations between strangers; 642 of these were hand-annotated with syntactic parses and further augmented with richer layers of annotation facilitated by the NITE XML toolkit (Calhoun et al., 2010).', 'Most previous work on integrating prosody and parsing has used the Switchboard corpus, but it is still difficult to compare results because of differences in constraints, objectives and the use of constituent vs. dependency structure, as discussed further in Section 6.', '(8) outperforms (1), (2) and (3), showing that combining phraselevel information from different granularities can further improve performance.\\n', 'Furthermore, the alignment matrix of a phrase can be refined even if the phrase division does not change between layers.', 'This yields the following expression for the squared error as a function of c:\\ng(c) = K∑ k=1 (c− vkik) 2 + K∑ k=1 ik−1∑ i=1', 'However, this comes at the cost of maximizing the utility of each user on 2000 movies.', 'The second case is more involved, since one needs to calculate all initial orders ⇡ 2 SK in which a pair of objects, say oi and oj , are discordant and compared to each other in the course of the sorting procedure.', 'In this paper, as a first study on multi-party negotiation, we apply RL to a multi-party trading scenario where the dialog system (learner) trades with one, two, or three other agents.', 'We experiment with different RL algorithms and reward functions.', 'To the best of our knowledge this is the first study that applies RL to multi-party (more than two participants) negotiation dialog management.', 'Section 3 describes our multi-party trading domain.', 'In our experiments, there are three types of items: apple, orange, and grape, and each trader may like, hate, or feel neutral about each type of fruit.', 'At the end of the dialog the trader earns 100 points for each fruit that he likes, 0 points for each fruit that he is neutral to, and -100 points for each fruit that he hates.', 'Furthermore, all traders can get a big payoff for having a fruit salad, i.e., the trader earns 500 additional points if he ends up with one fruit of each type.', 'Thus even hated fruits may sometimes be beneficial, but only if they can be part of a fruit salad.', 'Thus the outcome for a trader otr is calculated by Equation (1).\\n', 'The trader simulators are used as negotiation partners of the learner for both training and evaluating the learner’s policy (see Section 5).', 'Note that we use two kinds of rewards.\\n', 'The first type of reward is based on Equation (3).', 'In this case, the learner is rewarded based on its outcome only at the end of the dialog.', 'The dialog state is represented by binary variables (or features).', 'More concretely, this policy selects an action based on the following steps:\\n1.', 'Initially the learner has (O apples, 2 oranges, 1 grape) and Agent 1 has (1 apple, 0 oranges, 1 grape).', 'Agent 1 apple orange 0\\napple grape 0 orange apple 1 orange grape 0 grape apple 0 grape orange 0\\nAgent who Fruit type Number of fruits possesses fruits (used as feature) apple 0 learner orange 2\\ngrape 1 apple 1\\nAgent 1 orange 0 grape 1\\neach item is represented by a card), given the role of the trader (Rich, Middle, Poor) and how many items there can be in the hand.', 'A plan is a sequence of trades (one item in hand for one item out of hand) that will lead to the goal.', 'The outcome will be the hand that results from the end state, or the state before the trade that fails.', 'More specifically, there are 9 different setups:\\nH: 2-party dialog, where the trader simulator follows a hand-crafted policy.', 'HxR: 3-party dialog, where one trader simulator follows a hand-crafted policy and the other one follows a random policy.']\n",
            "376/376 [==============================] - 279s 742ms/step\n",
            "{'precision': [0.021739130434782608, 0.021739130434782608, 0.0, 0.07692307692307693, 0.16, 0.09523809523809523, 0.10810810810810811, 0.0, 0.13636363636363635, 0.057692307692307696, 0.08, 0.16666666666666666, 0.08108108108108109, 0.125, 0.0, 0.03508771929824561, 0.1346153846153846, 0.08108108108108109, 0.16666666666666666, 0.038461538461538464, 0.058823529411764705, 0.030303030303030304, 0.05, 0.0, 0.07017543859649122, 0.06818181818181818, 0.13043478260869565, 0.09259259259259259, 0.09523809523809523, 0.06666666666666667, 0.13157894736842105, 0.14285714285714285, 0.07317073170731707, 0.12903225806451613, 0.08064516129032258, 0.046511627906976744, 0.05263157894736842, 0.07792207792207792, 0.14814814814814814, 0.0967741935483871, 0.06382978723404255, 0.02564102564102564, 0.08433734939759036, 0.2391304347826087, 0.057692307692307696, 0.1111111111111111, 0.03225806451612903, 0.1320754716981132, 0.05263157894736842, 0.11363636363636363, 0.08163265306122448, 0.05128205128205128, 0.20512820512820512, 0.09803921568627451, 0.03389830508474576, 0.24324324324324326, 0.07142857142857142, 0.1111111111111111, 0.023255813953488372, 0.08333333333333333, 0.05357142857142857, 0.0, 0.11428571428571428, 0.07692307692307693, 0.15942028985507245, 0.1111111111111111, 0.05, 0.11538461538461539, 0.0, 0.0975609756097561, 0.09090909090909091, 0.024390243902439025, 0.11428571428571428, 0.09375, 0.07142857142857142, 0.034482758620689655, 0.09523809523809523, 0.15384615384615385, 0.06896551724137931, 0.07526881720430108, 0.09259259259259259, 0.05128205128205128, 0.07547169811320754, 0.05172413793103448, 0.05, 0.15789473684210525, 0.24444444444444444, 0.0847457627118644, 0.18421052631578946, 0.12162162162162163, 0.06, 0.05454545454545454, 0.06818181818181818, 0.03636363636363636, 0.0, 0.1724137931034483, 0.06493506493506493, 0.09523809523809523, 0.07692307692307693, 0.1875, 0.14285714285714285, 0.07317073170731707, 0.15, 0.08823529411764706, 0.0, 0.10810810810810811, 0.1111111111111111, 0.13043478260869565, 0.1111111111111111, 0.02, 0.044444444444444446, 0.11764705882352941, 0.05263157894736842, 0.2391304347826087, 0.02857142857142857, 0.17543859649122806, 0.07586206896551724, 0.14035087719298245, 0.04838709677419355, 0.09523809523809523, 0.15625, 0.05555555555555555, 0.19607843137254902, 0.07692307692307693, 0.1794871794871795, 0.1111111111111111, 0.125, 0.21052631578947367, 0.24242424242424243, 0.2857142857142857, 0.09090909090909091, 0.06779661016949153, 0.11627906976744186, 0.17391304347826086, 0.02127659574468085, 0.08695652173913043, 0.05555555555555555, 0.1388888888888889, 0.13636363636363635, 0.23809523809523808, 0.07936507936507936, 0.047619047619047616, 0.02127659574468085, 0.09090909090909091, 0.01818181818181818, 0.034482758620689655, 0.11428571428571428, 0.11363636363636363, 0.06666666666666667, 0.034482758620689655, 0.06818181818181818, 0.10204081632653061, 0.029411764705882353, 0.043478260869565216, 0.034482758620689655, 0.05660377358490566, 0.01694915254237288, 0.020833333333333332, 0.0, 0.029850746268656716, 0.13043478260869565, 0.13043478260869565, 0.043478260869565216, 0.09259259259259259, 0.05555555555555555, 0.11363636363636363, 0.09523809523809523, 0.07936507936507936, 0.047619047619047616, 0.1702127659574468, 0.06060606060606061, 0.05454545454545454, 0.08620689655172414, 0.11428571428571428, 0.022727272727272728, 0.15555555555555556, 0.0, 0.11363636363636363, 0.04081632653061224, 0.058823529411764705, 0.17391304347826086, 0.0, 0.07547169811320754, 0.03389830508474576, 0.020833333333333332, 0.10810810810810811, 0.0, 0.17391304347826086, 0.034482758620689655, 0.10714285714285714, 0.15789473684210525, 0.1111111111111111, 0.058823529411764705, 0.12962962962962962, 0.18867924528301888, 0.07142857142857142, 0.05128205128205128, 0.04878048780487805, 0.04878048780487805, 0.06349206349206349, 0.05172413793103448, 0.08163265306122448, 0.16666666666666666, 0.020833333333333332, 0.08571428571428572, 0.16666666666666666, 0.03488372093023256, 0.0297029702970297, 0.11538461538461539, 0.03488372093023256, 0.05319148936170213, 0.046511627906976744, 0.05714285714285714, 0.16216216216216217, 0.1, 0.11428571428571428, 0.0, 0.045454545454545456, 0.09836065573770492, 0.13513513513513514, 0.05714285714285714, 0.05102040816326531, 0.0, 0.05, 0.0, 0.11764705882352941, 0.11320754716981132, 0.14285714285714285, 0.06944444444444445, 0.125, 0.19148936170212766, 0.14102564102564102, 0.08823529411764706, 0.05970149253731343, 0.23333333333333334, 0.1487603305785124, 0.17142857142857143, 0.16216216216216217, 0.24444444444444444, 0.1836734693877551, 0.22641509433962265, 0.1282051282051282, 0.1, 0.125, 0.10869565217391304, 0.19718309859154928, 0.15, 0.027777777777777776, 0.027777777777777776, 0.09722222222222222, 0.1794871794871795, 0.09722222222222222, 0.2857142857142857, 0.09, 0.391304347826087, 0.07894736842105263, 0.13559322033898305, 0.0, 0.02564102564102564, 0.04819277108433735, 0.2830188679245283, 0.11290322580645161, 0.1111111111111111, 0.07272727272727272, 0.056818181818181816, 0.35135135135135137, 0.07692307692307693, 0.06896551724137931, 0.1282051282051282, 0.11764705882352941, 0.06060606060606061, 0.07407407407407407, 0.23333333333333334, 0.06, 0.03571428571428571, 0.25, 0.0547945205479452, 0.21666666666666667, 0.14705882352941177, 0.20512820512820512, 0.19672131147540983, 0.07317073170731707, 0.25, 0.1794871794871795, 0.08333333333333333, 0.047619047619047616, 0.11956521739130435, 0.16129032258064516, 0.1702127659574468, 0.05, 0.06976744186046512, 0.109375, 0.0847457627118644, 0.06666666666666667, 0.25, 0.08163265306122448, 0.08888888888888889, 0.02040816326530612, 0.0196078431372549, 0.015625, 0.06, 0.21739130434782608, 0.06666666666666667, 0.08571428571428572, 0.09615384615384616, 0.023809523809523808, 0.3442622950819672, 0.10526315789473684, 0.09302325581395349, 0.0547945205479452, 0.17647058823529413, 0.06451612903225806, 0.047619047619047616, 0.15789473684210525, 0.0, 0.058823529411764705, 0.15151515151515152, 0.1, 0.21212121212121213, 0.07142857142857142, 0.038461538461538464, 0.043478260869565216, 0.0, 0.16666666666666666, 0.046511627906976744, 0.0, 0.05172413793103448, 0.1, 0.06976744186046512, 0.047619047619047616, 0.03571428571428571, 0.125, 0.06896551724137931, 0.0, 0.06666666666666667, 0.02, 0.0, 0.08333333333333333, 0.10256410256410256, 0.1774193548387097, 0.0, 0.024390243902439025, 0.0196078431372549, 0.022222222222222223, 0.05, 0.08139534883720931, 0.02040816326530612, 0.1, 0.08163265306122448, 0.0, 0.021739130434782608, 0.09523809523809523, 0.14285714285714285, 0.09523809523809523, 0.26744186046511625, 0.07317073170731707, 0.07792207792207792, 0.06976744186046512, 0.2, 0.06382978723404255, 0.10843373493975904, 0.1375, 0.2608695652173913, 0.06521739130434782, 0.16666666666666666, 0.10344827586206896, 0.1095890410958904, 0.06, 0.17391304347826086, 0.06818181818181818, 0.0967741935483871, 0.08571428571428572, 0.04, 0.15625, 0.16666666666666666, 0.021739130434782608, 0.011764705882352941, 0.09722222222222222, 0.0, 0.0392156862745098, 0.10810810810810811, 0.03896103896103896, 0.19298245614035087, 0.04918032786885246, 0.15217391304347827, 0.19672131147540983, 0.13333333333333333, 0.21428571428571427, 0.04, 0.06976744186046512, 0.13333333333333333, 0.07894736842105263, 0.06, 0.054945054945054944, 0.0, 0.03278688524590164, 0.03225806451612903, 0.06944444444444445, 0.04040404040404041, 0.06, 0.06976744186046512, 0.045454545454545456, 0.04, 0.024390243902439025, 0.046875, 0.07142857142857142, 0.019230769230769232, 0.23809523809523808, 0.05714285714285714, 0.07547169811320754, 0.10526315789473684, 0.14754098360655737, 0.15217391304347827, 0.01639344262295082, 0.08, 0.125, 0.0, 0.13953488372093023, 0.08888888888888889, 0.02631578947368421, 0.12, 0.01098901098901099, 0.03571428571428571, 0.08196721311475409, 0.03225806451612903, 0.027777777777777776, 0.020202020202020204, 0.08, 0.09302325581395349, 0.030303030303030304, 0.08, 0.024390243902439025, 0.0078125, 0.023809523809523808, 0.15384615384615385, 0.16666666666666666, 0.08571428571428572, 0.018867924528301886, 0.10204081632653061], 'recall': [0.05263157894736842, 0.030303030303030304, 0.0, 0.26666666666666666, 0.32, 0.17391304347826086, 0.5, 0.0, 0.3, 0.1875, 0.17391304347826086, 0.28, 0.10344827586206896, 0.25, 0.0, 0.1111111111111111, 0.25, 0.1875, 0.21875, 0.14285714285714285, 0.058823529411764705, 0.2, 0.125, 0.0, 0.3076923076923077, 0.07894736842105263, 0.20689655172413793, 0.29411764705882354, 0.2857142857142857, 0.09523809523809523, 0.22727272727272727, 0.3888888888888889, 0.11538461538461539, 0.08333333333333333, 0.22727272727272727, 0.18181818181818182, 0.14285714285714285, 0.2608695652173913, 0.1702127659574468, 0.3333333333333333, 0.13636363636363635, 0.16666666666666666, 0.4117647058823529, 0.22448979591836735, 0.1, 0.17391304347826086, 0.13333333333333333, 0.21875, 0.1875, 0.21739130434782608, 0.14814814814814814, 0.07692307692307693, 0.3333333333333333, 0.22727272727272727, 0.06896551724137931, 0.14516129032258066, 0.09375, 0.16, 0.05, 0.16666666666666666, 0.1111111111111111, 0.0, 0.09302325581395349, 0.1875, 0.2558139534883721, 0.25, 0.1875, 0.18181818181818182, 0.0, 0.19047619047619047, 0.13157894736842105, 0.1, 0.0975609756097561, 0.0967741935483871, 0.15384615384615385, 0.05263157894736842, 0.10810810810810811, 0.22857142857142856, 0.10256410256410256, 0.23333333333333334, 0.1724137931034483, 0.13333333333333333, 0.10810810810810811, 0.2, 0.05555555555555555, 0.16216216216216217, 0.3793103448275862, 0.2777777777777778, 0.23333333333333334, 0.28125, 0.11538461538461539, 0.15, 0.1111111111111111, 0.05, 0.0, 0.125, 0.2631578947368421, 0.16666666666666666, 0.1111111111111111, 0.06818181818181818, 0.4583333333333333, 0.42857142857142855, 0.24, 0.13636363636363635, 0.0, 0.14285714285714285, 0.28, 0.18181818181818182, 0.16, 0.041666666666666664, 0.06896551724137931, 0.05970149253731343, 0.09523809523809523, 0.13924050632911392, 0.044444444444444446, 0.21739130434782608, 0.3793103448275862, 0.26666666666666666, 0.1111111111111111, 0.09090909090909091, 0.15625, 0.14285714285714285, 0.1388888888888889, 0.1111111111111111, 0.16666666666666666, 0.2608695652173913, 0.23529411764705882, 0.08695652173913043, 0.12698412698412698, 0.18181818181818182, 0.031746031746031744, 0.16, 0.23809523809523808, 0.3076923076923077, 0.09090909090909091, 0.12121212121212122, 0.1, 0.07575757575757576, 0.23076923076923078, 0.35714285714285715, 0.38461538461538464, 0.13043478260869565, 0.08333333333333333, 0.07894736842105263, 0.04, 0.14285714285714285, 0.18181818181818182, 0.15625, 0.10714285714285714, 0.05, 0.05084745762711865, 0.1724137931034483, 0.034482758620689655, 0.058823529411764705, 0.04, 0.15789473684210525, 0.05555555555555555, 0.05, 0.0, 0.09090909090909091, 0.09375, 0.16666666666666666, 0.15384615384615385, 0.19230769230769232, 0.07407407407407407, 0.1388888888888889, 0.0625, 0.14705882352941177, 0.13636363636363635, 0.25, 0.125, 0.12, 0.2, 0.2, 0.08333333333333333, 0.2916666666666667, 0.0, 0.23809523809523808, 0.15384615384615385, 0.06060606060606061, 0.18181818181818182, 0.0, 0.21052631578947367, 0.1111111111111111, 0.043478260869565216, 0.14814814814814814, 0.0, 0.17391304347826086, 0.045454545454545456, 0.18181818181818182, 0.1935483870967742, 0.09433962264150944, 0.08571428571428572, 0.1590909090909091, 0.2222222222222222, 0.10526315789473684, 0.11764705882352941, 0.05128205128205128, 0.11764705882352941, 0.12121212121212122, 0.2, 0.0851063829787234, 0.22448979591836735, 0.09090909090909091, 0.25, 0.14634146341463414, 0.12, 0.3, 0.23076923076923078, 0.17647058823529413, 0.35714285714285715, 0.0625, 0.2857142857142857, 0.24, 0.0625, 0.12121212121212122, 0.0, 0.07692307692307693, 0.2727272727272727, 0.1724137931034483, 0.16666666666666666, 0.20833333333333334, 0.0, 0.07407407407407407, 0.0, 0.12, 0.12244897959183673, 0.1276595744680851, 0.1724137931034483, 0.125, 0.140625, 0.19298245614035087, 0.0625, 0.06896551724137931, 0.4666666666666667, 0.4090909090909091, 0.3333333333333333, 0.1, 0.20754716981132076, 0.23076923076923078, 0.15789473684210525, 0.20833333333333334, 0.17857142857142858, 0.21621621621621623, 0.2564102564102564, 0.3181818181818182, 0.39473684210526316, 0.18181818181818182, 0.09523809523809523, 0.2692307692307692, 0.25925925925925924, 0.25925925925925924, 0.18181818181818182, 0.16981132075471697, 0.2535211267605634, 0.09090909090909091, 0.21621621621621623, 0.0, 0.058823529411764705, 0.18181818181818182, 0.0967741935483871, 0.16279069767441862, 0.14285714285714285, 0.21052631578947367, 0.15625, 0.17105263157894737, 0.25, 0.10526315789473684, 0.21739130434782608, 0.16, 0.15384615384615385, 0.09090909090909091, 0.21212121212121213, 0.15789473684210525, 0.2222222222222222, 0.13043478260869565, 0.12903225806451613, 0.2549019607843137, 0.16666666666666666, 0.23529411764705882, 0.4444444444444444, 0.046875, 0.09166666666666666, 0.17073170731707318, 0.07017543859649122, 0.2, 0.18032786885245902, 0.2564102564102564, 0.36363636363636365, 0.12903225806451613, 0.06818181818181818, 0.4375, 0.2631578947368421, 0.3333333333333333, 0.14, 0.26666666666666666, 0.16, 0.058823529411764705, 0.125, 0.1, 0.13636363636363635, 0.15625, 0.17647058823529413, 0.3333333333333333, 0.25, 0.09090909090909091, 0.3387096774193548, 0.24, 0.15384615384615385, 0.36363636363636365, 0.16216216216216217, 0.06896551724137931, 0.14285714285714285, 0.13953488372093023, 0.0, 0.15384615384615385, 0.09259259259259259, 0.09302325581395349, 0.2, 0.14285714285714285, 0.043478260869565216, 0.06060606060606061, 0.0, 0.1111111111111111, 0.08333333333333333, 0.0, 0.15789473684210525, 0.16216216216216217, 0.3333333333333333, 0.25806451612903225, 0.08333333333333333, 0.21428571428571427, 0.125, 0.0, 0.037037037037037035, 0.125, 0.0, 0.16, 0.14285714285714285, 0.275, 0.0, 0.038461538461538464, 0.16666666666666666, 0.125, 0.2222222222222222, 0.30434782608695654, 0.07142857142857142, 0.375, 0.16666666666666666, 0.0, 0.3, 0.13333333333333333, 0.2, 0.25, 0.26744186046511625, 0.13043478260869565, 0.2727272727272727, 0.4, 0.30303030303030304, 0.2, 0.32142857142857145, 0.21568627450980393, 0.21428571428571427, 0.08108108108108109, 0.13636363636363635, 0.07317073170731707, 0.19047619047619047, 0.07894736842105263, 0.11428571428571428, 0.13636363636363635, 0.1875, 0.058823529411764705, 0.10526315789473684, 0.23809523809523808, 0.21212121212121213, 0.07692307692307693, 0.05555555555555555, 0.21875, 0.0, 0.25, 0.09090909090909091, 0.21428571428571427, 0.275, 0.07894736842105263, 0.22580645161290322, 0.22641509433962265, 0.19607843137254902, 0.2857142857142857, 0.06666666666666667, 0.2727272727272727, 0.16216216216216217, 0.23076923076923078, 0.14285714285714285, 0.38461538461538464, 0.0, 0.2222222222222222, 0.125, 0.25, 0.23529411764705882, 0.23076923076923078, 0.1111111111111111, 0.21428571428571427, 0.08333333333333333, 0.08333333333333333, 0.1935483870967742, 0.1111111111111111, 0.1, 0.1724137931034483, 0.08333333333333333, 0.12903225806451613, 0.4, 0.16981132075471697, 0.17073170731707318, 0.05555555555555555, 0.3, 0.2413793103448276, 0.0, 0.15, 0.12121212121212122, 0.2222222222222222, 0.25, 0.125, 0.07407407407407407, 0.1388888888888889, 0.03225806451612903, 0.10526315789473684, 0.16666666666666666, 0.17391304347826086, 0.5, 0.2, 0.2222222222222222, 0.1, 0.07692307692307693, 0.05, 0.1095890410958904, 0.30434782608695654, 0.15, 0.05, 0.23809523809523808], 'fmeasure': [0.03076923076923077, 0.02531645569620253, 0.0, 0.11940298507462686, 0.21333333333333335, 0.12307692307692307, 0.17777777777777778, 0.0, 0.18749999999999997, 0.08823529411764705, 0.1095890410958904, 0.20895522388059704, 0.09090909090909091, 0.16666666666666666, 0.0, 0.05333333333333334, 0.17500000000000002, 0.11320754716981132, 0.18918918918918917, 0.060606060606060615, 0.058823529411764705, 0.052631578947368425, 0.07142857142857144, 0.0, 0.11428571428571428, 0.07317073170731707, 0.16, 0.1408450704225352, 0.14285714285714285, 0.0784313725490196, 0.16666666666666666, 0.208955223880597, 0.08955223880597016, 0.10126582278481013, 0.11904761904761903, 0.07407407407407407, 0.07692307692307693, 0.12000000000000001, 0.1584158415841584, 0.15000000000000002, 0.08695652173913043, 0.044444444444444446, 0.13999999999999999, 0.23157894736842108, 0.07317073170731708, 0.13559322033898305, 0.05194805194805195, 0.1647058823529412, 0.0821917808219178, 0.1492537313432836, 0.10526315789473684, 0.06153846153846155, 0.25396825396825395, 0.136986301369863, 0.045454545454545456, 0.18181818181818182, 0.08108108108108107, 0.13114754098360656, 0.031746031746031744, 0.1111111111111111, 0.07228915662650602, 0.0, 0.10256410256410256, 0.1090909090909091, 0.1964285714285714, 0.15384615384615383, 0.07894736842105264, 0.14117647058823532, 0.0, 0.12903225806451615, 0.10752688172043011, 0.03921568627450981, 0.10526315789473684, 0.09523809523809523, 0.0975609756097561, 0.041666666666666664, 0.10126582278481013, 0.18390804597701152, 0.08247422680412371, 0.11382113821138212, 0.12048192771084339, 0.07407407407407407, 0.0888888888888889, 0.0821917808219178, 0.052631578947368425, 0.16, 0.29729729729729726, 0.12987012987012986, 0.20588235294117646, 0.169811320754717, 0.07894736842105263, 0.08, 0.08450704225352113, 0.042105263157894736, 0.0, 0.14492753623188406, 0.10416666666666667, 0.12121212121212123, 0.09090909090909093, 0.09999999999999999, 0.2178217821782178, 0.125, 0.1846153846153846, 0.10714285714285714, 0.0, 0.12307692307692308, 0.1590909090909091, 0.1518987341772152, 0.13114754098360656, 0.02702702702702703, 0.05405405405405405, 0.07920792079207921, 0.06779661016949153, 0.176, 0.034782608695652174, 0.1941747572815534, 0.12643678160919541, 0.1839080459770115, 0.06741573033707865, 0.09302325581395349, 0.15625, 0.08, 0.16260162601626016, 0.09090909090909093, 0.1728395061728395, 0.15584415584415584, 0.16326530612244897, 0.12307692307692308, 0.16666666666666666, 0.2222222222222222, 0.047058823529411764, 0.09523809523809523, 0.15625, 0.2222222222222222, 0.034482758620689655, 0.10126582278481013, 0.07142857142857142, 0.09803921568627452, 0.1714285714285714, 0.2857142857142857, 0.13157894736842105, 0.06976744186046512, 0.03389830508474576, 0.08450704225352113, 0.024999999999999998, 0.05555555555555555, 0.14035087719298245, 0.13157894736842105, 0.0821917808219178, 0.04081632653061224, 0.05825242718446602, 0.12820512820512822, 0.03174603174603175, 0.049999999999999996, 0.03703703703703704, 0.08333333333333333, 0.025974025974025976, 0.029411764705882353, 0.0, 0.0449438202247191, 0.10909090909090909, 0.14634146341463414, 0.06779661016949153, 0.125, 0.06349206349206349, 0.125, 0.07547169811320754, 0.10309278350515463, 0.07058823529411765, 0.20253164556962025, 0.0816326530612245, 0.075, 0.12048192771084339, 0.14545454545454545, 0.03571428571428572, 0.2028985507246377, 0.0, 0.15384615384615383, 0.06451612903225806, 0.05970149253731344, 0.17777777777777776, 0.0, 0.11111111111111112, 0.05194805194805195, 0.028169014084507043, 0.125, 0.0, 0.17391304347826086, 0.0392156862745098, 0.1348314606741573, 0.17391304347826086, 0.10204081632653061, 0.06976744186046513, 0.14285714285714285, 0.20408163265306123, 0.0851063829787234, 0.07142857142857141, 0.05, 0.06896551724137931, 0.08333333333333333, 0.0821917808219178, 0.08333333333333333, 0.19130434782608696, 0.03389830508474576, 0.1276595744680851, 0.15584415584415584, 0.05405405405405404, 0.05405405405405406, 0.15384615384615388, 0.058252427184466014, 0.09259259259259259, 0.05333333333333333, 0.09523809523809522, 0.1935483870967742, 0.07692307692307693, 0.11764705882352942, 0.0, 0.05714285714285715, 0.14457831325301204, 0.15151515151515152, 0.0851063829787234, 0.08196721311475412, 0.0, 0.05970149253731344, 0.0, 0.1188118811881188, 0.11764705882352941, 0.1348314606741573, 0.09900990099009903, 0.125, 0.16216216216216214, 0.16296296296296295, 0.07317073170731707, 0.06399999999999999, 0.31111111111111117, 0.2181818181818182, 0.22641509433962265, 0.12371134020618559, 0.22448979591836737, 0.20454545454545456, 0.18604651162790695, 0.15873015873015872, 0.12820512820512822, 0.15841584158415842, 0.15267175572519084, 0.2434782608695652, 0.21739130434782605, 0.04819277108433734, 0.043010752688172046, 0.14285714285714288, 0.21212121212121213, 0.14141414141414144, 0.2222222222222222, 0.1176470588235294, 0.3076923076923077, 0.08450704225352113, 0.16666666666666669, 0.0, 0.035714285714285705, 0.07619047619047621, 0.14423076923076922, 0.13333333333333333, 0.125, 0.1081081081081081, 0.08333333333333333, 0.23008849557522124, 0.11764705882352941, 0.08333333333333333, 0.16129032258064516, 0.13559322033898305, 0.08695652173913043, 0.08163265306122448, 0.22222222222222224, 0.08695652173913043, 0.06153846153846154, 0.1714285714285714, 0.07692307692307691, 0.23423423423423423, 0.15625, 0.2191780821917808, 0.27272727272727276, 0.05714285714285714, 0.13414634146341461, 0.17500000000000002, 0.0761904761904762, 0.07692307692307693, 0.1437908496732026, 0.19801980198019803, 0.23188405797101447, 0.07207207207207207, 0.0689655172413793, 0.175, 0.1282051282051282, 0.1111111111111111, 0.1794871794871795, 0.125, 0.1142857142857143, 0.030303030303030304, 0.03389830508474576, 0.02702702702702703, 0.08333333333333333, 0.18181818181818182, 0.09677419354838708, 0.13636363636363638, 0.1388888888888889, 0.03773584905660377, 0.34146341463414637, 0.14634146341463414, 0.11594202898550725, 0.09523809523809523, 0.16901408450704225, 0.06666666666666667, 0.07142857142857142, 0.14814814814814817, 0.0, 0.0851063829787234, 0.11494252873563218, 0.09638554216867469, 0.2058823529411765, 0.09523809523809523, 0.04081632653061224, 0.05063291139240506, 0.0, 0.13333333333333333, 0.059701492537313425, 0.0, 0.07792207792207792, 0.12371134020618559, 0.11538461538461539, 0.08040201005025124, 0.05, 0.15789473684210525, 0.08888888888888889, 0.0, 0.047619047619047616, 0.03448275862068966, 0.0, 0.1095890410958904, 0.11940298507462686, 0.21568627450980393, 0.0, 0.029850746268656723, 0.03508771929824561, 0.03773584905660377, 0.0816326530612245, 0.12844036697247707, 0.031746031746031744, 0.15789473684210528, 0.10958904109589039, 0.0, 0.04054054054054054, 0.1111111111111111, 0.16666666666666666, 0.13793103448275862, 0.26744186046511625, 0.09375, 0.1212121212121212, 0.1188118811881188, 0.24096385542168675, 0.0967741935483871, 0.16216216216216217, 0.16793893129770993, 0.23529411764705882, 0.07228915662650602, 0.15, 0.0857142857142857, 0.1391304347826087, 0.06818181818181818, 0.13793103448275862, 0.0909090909090909, 0.12765957446808507, 0.06976744186046513, 0.057971014492753624, 0.18867924528301888, 0.18666666666666665, 0.03389830508474576, 0.019417475728155338, 0.13461538461538464, 0.0, 0.06779661016949153, 0.09876543209876544, 0.06593406593406594, 0.2268041237113402, 0.06060606060606061, 0.1818181818181818, 0.21052631578947367, 0.15873015873015875, 0.24489795918367344, 0.05, 0.11111111111111109, 0.14634146341463417, 0.11764705882352941, 0.08450704225352111, 0.09615384615384616, 0.0, 0.05714285714285715, 0.05128205128205128, 0.10869565217391305, 0.06896551724137932, 0.09523809523809523, 0.08571428571428572, 0.07500000000000001, 0.05405405405405406, 0.037735849056603765, 0.07547169811320754, 0.08695652173913043, 0.03225806451612903, 0.19999999999999998, 0.06779661016949153, 0.09523809523809525, 0.16666666666666666, 0.15789473684210525, 0.16091954022988506, 0.02531645569620253, 0.12631578947368421, 0.16470588235294117, 0.0, 0.14457831325301204, 0.10256410256410256, 0.047058823529411764, 0.16216216216216217, 0.020202020202020204, 0.04819277108433735, 0.10309278350515463, 0.03225806451612903, 0.04395604395604395, 0.03603603603603604, 0.1095890410958904, 0.15686274509803924, 0.052631578947368425, 0.11764705882352941, 0.03921568627450981, 0.014184397163120567, 0.03225806451612903, 0.128, 0.2153846153846154, 0.10909090909090909, 0.027397260273972605, 0.14285714285714285]}\n",
            "Average for precision is 0.09224052460392967\n",
            "Average for recall is 0.16346490081293427\n",
            "Average for fmeasure is 0.1081693273392644\n",
            "/content/drive/MyDrive/Colab Notebooks/data/testdata/Sample-efficient Actor-Critic Reinforcement Learning with Supervised Data for Dialogue Management.csv\n",
            "['Besides the fact that learning machines have often singularities (Watanabe, 2009) (|I(Θ)| = 0, not full rank) characterized by plateaux in gradient learning, computing/estimating the FIM of a large neuron system (e.g. one with millions of parameters, Szegedy, Christian et al. 2015) is very challenging due to the finiteness of data, and the huge number D(D+1)2 of matrix coefficients to evaluate.', 'The idea is to accurately describe the information geometry (IG) in a subsystem of the large learning system, which is invariant to the scaling up and structural change of the global system, so that the local machinery, including optimization, can be discussed regardless of the other parts.\\n', 'A recent series of efforts (Montavon & Müller, 2012; Raiko et al., 2012; Desjardins et al., 2015) are gearing towards a parametric approach to applying natural gradient, which memorizes and learns a geometry.', 'We extract a local component from a large neural system, and define its relative Fisher information metric that describes accurately this small component, and is invariant to the other parts of the system.', 'This analysis comes with a guarantee that the probability of overestimating the true number of datasets with effect is upper bounded by a predefined constant.', 'That is, the wost-case distortion is the ratio of the maximal expansion and the minimal contraction of distances.', 'Some\\nimplementations work around this limitation by simultaneously evaluating the branching function at all nodes as the vector is streamed but this increases the prediction costs from logarithmic to linear which might not be acceptable.\\n', 'The idea is to raise each data likelihood to a weight and to infer the weights along with the hidden patterns.', 'Kalai & Sastry (2009) and Kakade et al. (2011) investigated the low-dimensional SIMs with monotone transfers, and they proposed perceptron-type algorithms to estimate both f∗ and θ∗, with provable guarantees on prediction error.', 'The second part aims to mitigate the cold start issue by using demonstration data to pre-train an RL model.', 'This can be solved by applying either value-based\\nor policy-based methods.', 'Following the Policy Gradient Theorem (Sutton et al., 2000), the gradient of the parameters given the objective function has the form:\\n∇θJ(θ) =', 'Sample-efficiency can be improved by utilising experience replay (ER) (Lin, 1992), where minibatches of dialogue experiences are randomly sampled from a replay pool P to train the model.', 'This increases learning efficiency by re-using past samples in multiple updates whilst ensuring stability by reducing the data correlation.', 'However, setting the rate low enough to avoid occasional large destabilising updates is not conducive to fast learning.\\n', 'In addition to maximising the cumulative reward J(θ), the optimisation is also subject to a Kullback-Leibler (KL) divergence limit between the updated policy θ and an average policy θa to ensure safety.', 'This implies ∆θNG = w = F (θ)−1∇θJ(θ) and it is called the natural gradient.', 'This problem can be mitigated by an off-line corpus of demonstration data to bootstrap a policy.', 'This data may come from a WoZ collection or from interactions between users and an existing policy.', 'It can be used in three ways: A: Pre-train the model, B: Initialise a supervised replay buffer Psup, and C: a combination of the two.\\n', '(A) For model pre-training, the objective is to ‘mimic’ the response behaviour from the corpus.', 'A policy trained by SL on a fixed dataset may not generalise well.', 'The learned parameters of the pre-trained model in method A above might distribute differently from the optimal RL policy and this may cause some performance drop in early stages while learning an RL policy from this model.', 'DQN often suffers from over-estimation on Qvalues as the max operator is used to select an action as well as to evaluate it.', 'Double DQN (DDQN) (Van Hasselt et al., 2016) is thus used to de-couple the action selection and Q-value estimation to achieve better performance.', 'In this case, the user intent is perfectly captured in the dialogue belief state without noise.\\n', 'The total return of each dialogue was set to 1(D)− 0.05× T , where T is the dialogue length and 1(D) is the success indicator for dialogue', 'All deep RL models (A2C, TRACER, eNACER and DQN) contained two hidden layers of size 130 and 50.']\n",
            "149/149 [==============================] - 110s 738ms/step\n",
            "{'precision': [0.021739130434782608, 0.021739130434782608, 0.0, 0.07692307692307693, 0.16, 0.09523809523809523, 0.10810810810810811, 0.0, 0.13636363636363635, 0.057692307692307696, 0.08, 0.16666666666666666, 0.08108108108108109, 0.125, 0.0, 0.03508771929824561, 0.1346153846153846, 0.08108108108108109, 0.16666666666666666, 0.038461538461538464, 0.058823529411764705, 0.030303030303030304, 0.05, 0.0, 0.07017543859649122, 0.06818181818181818, 0.13043478260869565, 0.09259259259259259, 0.09523809523809523, 0.06666666666666667, 0.13157894736842105, 0.14285714285714285, 0.07317073170731707, 0.12903225806451613, 0.08064516129032258, 0.046511627906976744, 0.05263157894736842, 0.07792207792207792, 0.14814814814814814, 0.0967741935483871, 0.06382978723404255, 0.02564102564102564, 0.08433734939759036, 0.2391304347826087, 0.057692307692307696, 0.1111111111111111, 0.03225806451612903, 0.1320754716981132, 0.05263157894736842, 0.11363636363636363, 0.08163265306122448, 0.05128205128205128, 0.20512820512820512, 0.09803921568627451, 0.03389830508474576, 0.24324324324324326, 0.07142857142857142, 0.1111111111111111, 0.023255813953488372, 0.08333333333333333, 0.05357142857142857, 0.0, 0.11428571428571428, 0.07692307692307693, 0.15942028985507245, 0.1111111111111111, 0.05, 0.11538461538461539, 0.0, 0.0975609756097561, 0.09090909090909091, 0.024390243902439025, 0.11428571428571428, 0.09375, 0.07142857142857142, 0.034482758620689655, 0.09523809523809523, 0.15384615384615385, 0.06896551724137931, 0.07526881720430108, 0.09259259259259259, 0.05128205128205128, 0.07547169811320754, 0.05172413793103448, 0.05, 0.15789473684210525, 0.24444444444444444, 0.0847457627118644, 0.18421052631578946, 0.12162162162162163, 0.06, 0.05454545454545454, 0.06818181818181818, 0.03636363636363636, 0.0, 0.1724137931034483, 0.06493506493506493, 0.09523809523809523, 0.07692307692307693, 0.1875, 0.14285714285714285, 0.07317073170731707, 0.15, 0.08823529411764706, 0.0, 0.10810810810810811, 0.1111111111111111, 0.13043478260869565, 0.1111111111111111, 0.02, 0.044444444444444446, 0.11764705882352941, 0.05263157894736842, 0.2391304347826087, 0.02857142857142857, 0.17543859649122806, 0.07586206896551724, 0.14035087719298245, 0.04838709677419355, 0.09523809523809523, 0.15625, 0.05555555555555555, 0.19607843137254902, 0.07692307692307693, 0.1794871794871795, 0.1111111111111111, 0.125, 0.21052631578947367, 0.24242424242424243, 0.2857142857142857, 0.09090909090909091, 0.06779661016949153, 0.11627906976744186, 0.17391304347826086, 0.02127659574468085, 0.08695652173913043, 0.05555555555555555, 0.1388888888888889, 0.13636363636363635, 0.23809523809523808, 0.07936507936507936, 0.047619047619047616, 0.02127659574468085, 0.09090909090909091, 0.01818181818181818, 0.034482758620689655, 0.11428571428571428, 0.11363636363636363, 0.06666666666666667, 0.034482758620689655, 0.06818181818181818, 0.10204081632653061, 0.029411764705882353, 0.043478260869565216, 0.034482758620689655, 0.05660377358490566, 0.01694915254237288, 0.020833333333333332, 0.0, 0.029850746268656716, 0.13043478260869565, 0.13043478260869565, 0.043478260869565216, 0.09259259259259259, 0.05555555555555555, 0.11363636363636363, 0.09523809523809523, 0.07936507936507936, 0.047619047619047616, 0.1702127659574468, 0.06060606060606061, 0.05454545454545454, 0.08620689655172414, 0.11428571428571428, 0.022727272727272728, 0.15555555555555556, 0.0, 0.11363636363636363, 0.04081632653061224, 0.058823529411764705, 0.17391304347826086, 0.0, 0.07547169811320754, 0.03389830508474576, 0.020833333333333332, 0.10810810810810811, 0.0, 0.17391304347826086, 0.034482758620689655, 0.10714285714285714, 0.15789473684210525, 0.1111111111111111, 0.058823529411764705, 0.12962962962962962, 0.18867924528301888, 0.07142857142857142, 0.05128205128205128, 0.04878048780487805, 0.04878048780487805, 0.06349206349206349, 0.05172413793103448, 0.08163265306122448, 0.16666666666666666, 0.020833333333333332, 0.08571428571428572, 0.16666666666666666, 0.03488372093023256, 0.0297029702970297, 0.11538461538461539, 0.03488372093023256, 0.05319148936170213, 0.046511627906976744, 0.05714285714285714, 0.16216216216216217, 0.1, 0.11428571428571428, 0.0, 0.045454545454545456, 0.09836065573770492, 0.13513513513513514, 0.05714285714285714, 0.05102040816326531, 0.0, 0.05, 0.0, 0.11764705882352941, 0.11320754716981132, 0.14285714285714285, 0.06944444444444445, 0.125, 0.19148936170212766, 0.14102564102564102, 0.08823529411764706, 0.05970149253731343, 0.23333333333333334, 0.1487603305785124, 0.17142857142857143, 0.16216216216216217, 0.24444444444444444, 0.1836734693877551, 0.22641509433962265, 0.1282051282051282, 0.1, 0.125, 0.10869565217391304, 0.19718309859154928, 0.15, 0.027777777777777776, 0.027777777777777776, 0.09722222222222222, 0.1794871794871795, 0.09722222222222222, 0.2857142857142857, 0.09, 0.391304347826087, 0.07894736842105263, 0.13559322033898305, 0.0, 0.02564102564102564, 0.04819277108433735, 0.2830188679245283, 0.11290322580645161, 0.1111111111111111, 0.07272727272727272, 0.056818181818181816, 0.35135135135135137, 0.07692307692307693, 0.06896551724137931, 0.1282051282051282, 0.11764705882352941, 0.06060606060606061, 0.07407407407407407, 0.23333333333333334, 0.06, 0.03571428571428571, 0.25, 0.0547945205479452, 0.21666666666666667, 0.14705882352941177, 0.20512820512820512, 0.19672131147540983, 0.07317073170731707, 0.25, 0.1794871794871795, 0.08333333333333333, 0.047619047619047616, 0.11956521739130435, 0.16129032258064516, 0.1702127659574468, 0.05, 0.06976744186046512, 0.109375, 0.0847457627118644, 0.06666666666666667, 0.25, 0.08163265306122448, 0.08888888888888889, 0.02040816326530612, 0.0196078431372549, 0.015625, 0.06, 0.21739130434782608, 0.06666666666666667, 0.08571428571428572, 0.09615384615384616, 0.023809523809523808, 0.3442622950819672, 0.10526315789473684, 0.09302325581395349, 0.0547945205479452, 0.17647058823529413, 0.06451612903225806, 0.047619047619047616, 0.15789473684210525, 0.0, 0.058823529411764705, 0.15151515151515152, 0.1, 0.21212121212121213, 0.07142857142857142, 0.038461538461538464, 0.043478260869565216, 0.0, 0.16666666666666666, 0.046511627906976744, 0.0, 0.05172413793103448, 0.1, 0.06976744186046512, 0.047619047619047616, 0.03571428571428571, 0.125, 0.06896551724137931, 0.0, 0.06666666666666667, 0.02, 0.0, 0.08333333333333333, 0.10256410256410256, 0.1774193548387097, 0.0, 0.024390243902439025, 0.0196078431372549, 0.022222222222222223, 0.05, 0.08139534883720931, 0.02040816326530612, 0.1, 0.08163265306122448, 0.0, 0.021739130434782608, 0.09523809523809523, 0.14285714285714285, 0.09523809523809523, 0.26744186046511625, 0.07317073170731707, 0.07792207792207792, 0.06976744186046512, 0.2, 0.06382978723404255, 0.10843373493975904, 0.1375, 0.2608695652173913, 0.06521739130434782, 0.16666666666666666, 0.10344827586206896, 0.1095890410958904, 0.06, 0.17391304347826086, 0.06818181818181818, 0.0967741935483871, 0.08571428571428572, 0.04, 0.15625, 0.16666666666666666, 0.021739130434782608, 0.011764705882352941, 0.09722222222222222, 0.0, 0.0392156862745098, 0.10810810810810811, 0.03896103896103896, 0.19298245614035087, 0.04918032786885246, 0.15217391304347827, 0.19672131147540983, 0.13333333333333333, 0.21428571428571427, 0.04, 0.06976744186046512, 0.13333333333333333, 0.07894736842105263, 0.06, 0.054945054945054944, 0.0, 0.03278688524590164, 0.03225806451612903, 0.06944444444444445, 0.04040404040404041, 0.06, 0.06976744186046512, 0.045454545454545456, 0.04, 0.024390243902439025, 0.046875, 0.07142857142857142, 0.019230769230769232, 0.23809523809523808, 0.05714285714285714, 0.07547169811320754, 0.10526315789473684, 0.14754098360655737, 0.15217391304347827, 0.01639344262295082, 0.08, 0.125, 0.0, 0.13953488372093023, 0.08888888888888889, 0.02631578947368421, 0.12, 0.01098901098901099, 0.03571428571428571, 0.08196721311475409, 0.03225806451612903, 0.027777777777777776, 0.020202020202020204, 0.08, 0.09302325581395349, 0.030303030303030304, 0.08, 0.024390243902439025, 0.0078125, 0.023809523809523808, 0.15384615384615385, 0.16666666666666666, 0.08571428571428572, 0.018867924528301886, 0.10204081632653061, 0.12307692307692308, 0.2112676056338028, 0.0967741935483871, 0.13636363636363635, 0.20689655172413793, 0.07792207792207792, 0.0, 0.1111111111111111, 0.1076923076923077, 0.05, 0.02857142857142857, 0.11363636363636363, 0.10294117647058823, 0.046511627906976744, 0.07142857142857142, 0.11538461538461539, 0.0625, 0.011363636363636364, 0.028037383177570093, 0.17307692307692307, 0.1111111111111111, 0.019417475728155338, 0.11267605633802817, 0.0784313725490196, 0.10526315789473684, 0.021739130434782608, 0.125, 0.04878048780487805], 'recall': [0.05263157894736842, 0.030303030303030304, 0.0, 0.26666666666666666, 0.32, 0.17391304347826086, 0.5, 0.0, 0.3, 0.1875, 0.17391304347826086, 0.28, 0.10344827586206896, 0.25, 0.0, 0.1111111111111111, 0.25, 0.1875, 0.21875, 0.14285714285714285, 0.058823529411764705, 0.2, 0.125, 0.0, 0.3076923076923077, 0.07894736842105263, 0.20689655172413793, 0.29411764705882354, 0.2857142857142857, 0.09523809523809523, 0.22727272727272727, 0.3888888888888889, 0.11538461538461539, 0.08333333333333333, 0.22727272727272727, 0.18181818181818182, 0.14285714285714285, 0.2608695652173913, 0.1702127659574468, 0.3333333333333333, 0.13636363636363635, 0.16666666666666666, 0.4117647058823529, 0.22448979591836735, 0.1, 0.17391304347826086, 0.13333333333333333, 0.21875, 0.1875, 0.21739130434782608, 0.14814814814814814, 0.07692307692307693, 0.3333333333333333, 0.22727272727272727, 0.06896551724137931, 0.14516129032258066, 0.09375, 0.16, 0.05, 0.16666666666666666, 0.1111111111111111, 0.0, 0.09302325581395349, 0.1875, 0.2558139534883721, 0.25, 0.1875, 0.18181818181818182, 0.0, 0.19047619047619047, 0.13157894736842105, 0.1, 0.0975609756097561, 0.0967741935483871, 0.15384615384615385, 0.05263157894736842, 0.10810810810810811, 0.22857142857142856, 0.10256410256410256, 0.23333333333333334, 0.1724137931034483, 0.13333333333333333, 0.10810810810810811, 0.2, 0.05555555555555555, 0.16216216216216217, 0.3793103448275862, 0.2777777777777778, 0.23333333333333334, 0.28125, 0.11538461538461539, 0.15, 0.1111111111111111, 0.05, 0.0, 0.125, 0.2631578947368421, 0.16666666666666666, 0.1111111111111111, 0.06818181818181818, 0.4583333333333333, 0.42857142857142855, 0.24, 0.13636363636363635, 0.0, 0.14285714285714285, 0.28, 0.18181818181818182, 0.16, 0.041666666666666664, 0.06896551724137931, 0.05970149253731343, 0.09523809523809523, 0.13924050632911392, 0.044444444444444446, 0.21739130434782608, 0.3793103448275862, 0.26666666666666666, 0.1111111111111111, 0.09090909090909091, 0.15625, 0.14285714285714285, 0.1388888888888889, 0.1111111111111111, 0.16666666666666666, 0.2608695652173913, 0.23529411764705882, 0.08695652173913043, 0.12698412698412698, 0.18181818181818182, 0.031746031746031744, 0.16, 0.23809523809523808, 0.3076923076923077, 0.09090909090909091, 0.12121212121212122, 0.1, 0.07575757575757576, 0.23076923076923078, 0.35714285714285715, 0.38461538461538464, 0.13043478260869565, 0.08333333333333333, 0.07894736842105263, 0.04, 0.14285714285714285, 0.18181818181818182, 0.15625, 0.10714285714285714, 0.05, 0.05084745762711865, 0.1724137931034483, 0.034482758620689655, 0.058823529411764705, 0.04, 0.15789473684210525, 0.05555555555555555, 0.05, 0.0, 0.09090909090909091, 0.09375, 0.16666666666666666, 0.15384615384615385, 0.19230769230769232, 0.07407407407407407, 0.1388888888888889, 0.0625, 0.14705882352941177, 0.13636363636363635, 0.25, 0.125, 0.12, 0.2, 0.2, 0.08333333333333333, 0.2916666666666667, 0.0, 0.23809523809523808, 0.15384615384615385, 0.06060606060606061, 0.18181818181818182, 0.0, 0.21052631578947367, 0.1111111111111111, 0.043478260869565216, 0.14814814814814814, 0.0, 0.17391304347826086, 0.045454545454545456, 0.18181818181818182, 0.1935483870967742, 0.09433962264150944, 0.08571428571428572, 0.1590909090909091, 0.2222222222222222, 0.10526315789473684, 0.11764705882352941, 0.05128205128205128, 0.11764705882352941, 0.12121212121212122, 0.2, 0.0851063829787234, 0.22448979591836735, 0.09090909090909091, 0.25, 0.14634146341463414, 0.12, 0.3, 0.23076923076923078, 0.17647058823529413, 0.35714285714285715, 0.0625, 0.2857142857142857, 0.24, 0.0625, 0.12121212121212122, 0.0, 0.07692307692307693, 0.2727272727272727, 0.1724137931034483, 0.16666666666666666, 0.20833333333333334, 0.0, 0.07407407407407407, 0.0, 0.12, 0.12244897959183673, 0.1276595744680851, 0.1724137931034483, 0.125, 0.140625, 0.19298245614035087, 0.0625, 0.06896551724137931, 0.4666666666666667, 0.4090909090909091, 0.3333333333333333, 0.1, 0.20754716981132076, 0.23076923076923078, 0.15789473684210525, 0.20833333333333334, 0.17857142857142858, 0.21621621621621623, 0.2564102564102564, 0.3181818181818182, 0.39473684210526316, 0.18181818181818182, 0.09523809523809523, 0.2692307692307692, 0.25925925925925924, 0.25925925925925924, 0.18181818181818182, 0.16981132075471697, 0.2535211267605634, 0.09090909090909091, 0.21621621621621623, 0.0, 0.058823529411764705, 0.18181818181818182, 0.0967741935483871, 0.16279069767441862, 0.14285714285714285, 0.21052631578947367, 0.15625, 0.17105263157894737, 0.25, 0.10526315789473684, 0.21739130434782608, 0.16, 0.15384615384615385, 0.09090909090909091, 0.21212121212121213, 0.15789473684210525, 0.2222222222222222, 0.13043478260869565, 0.12903225806451613, 0.2549019607843137, 0.16666666666666666, 0.23529411764705882, 0.4444444444444444, 0.046875, 0.09166666666666666, 0.17073170731707318, 0.07017543859649122, 0.2, 0.18032786885245902, 0.2564102564102564, 0.36363636363636365, 0.12903225806451613, 0.06818181818181818, 0.4375, 0.2631578947368421, 0.3333333333333333, 0.14, 0.26666666666666666, 0.16, 0.058823529411764705, 0.125, 0.1, 0.13636363636363635, 0.15625, 0.17647058823529413, 0.3333333333333333, 0.25, 0.09090909090909091, 0.3387096774193548, 0.24, 0.15384615384615385, 0.36363636363636365, 0.16216216216216217, 0.06896551724137931, 0.14285714285714285, 0.13953488372093023, 0.0, 0.15384615384615385, 0.09259259259259259, 0.09302325581395349, 0.2, 0.14285714285714285, 0.043478260869565216, 0.06060606060606061, 0.0, 0.1111111111111111, 0.08333333333333333, 0.0, 0.15789473684210525, 0.16216216216216217, 0.3333333333333333, 0.25806451612903225, 0.08333333333333333, 0.21428571428571427, 0.125, 0.0, 0.037037037037037035, 0.125, 0.0, 0.16, 0.14285714285714285, 0.275, 0.0, 0.038461538461538464, 0.16666666666666666, 0.125, 0.2222222222222222, 0.30434782608695654, 0.07142857142857142, 0.375, 0.16666666666666666, 0.0, 0.3, 0.13333333333333333, 0.2, 0.25, 0.26744186046511625, 0.13043478260869565, 0.2727272727272727, 0.4, 0.30303030303030304, 0.2, 0.32142857142857145, 0.21568627450980393, 0.21428571428571427, 0.08108108108108109, 0.13636363636363635, 0.07317073170731707, 0.19047619047619047, 0.07894736842105263, 0.11428571428571428, 0.13636363636363635, 0.1875, 0.058823529411764705, 0.10526315789473684, 0.23809523809523808, 0.21212121212121213, 0.07692307692307693, 0.05555555555555555, 0.21875, 0.0, 0.25, 0.09090909090909091, 0.21428571428571427, 0.275, 0.07894736842105263, 0.22580645161290322, 0.22641509433962265, 0.19607843137254902, 0.2857142857142857, 0.06666666666666667, 0.2727272727272727, 0.16216216216216217, 0.23076923076923078, 0.14285714285714285, 0.38461538461538464, 0.0, 0.2222222222222222, 0.125, 0.25, 0.23529411764705882, 0.23076923076923078, 0.1111111111111111, 0.21428571428571427, 0.08333333333333333, 0.08333333333333333, 0.1935483870967742, 0.1111111111111111, 0.1, 0.1724137931034483, 0.08333333333333333, 0.12903225806451613, 0.4, 0.16981132075471697, 0.17073170731707318, 0.05555555555555555, 0.3, 0.2413793103448276, 0.0, 0.15, 0.12121212121212122, 0.2222222222222222, 0.25, 0.125, 0.07407407407407407, 0.1388888888888889, 0.03225806451612903, 0.10526315789473684, 0.16666666666666666, 0.17391304347826086, 0.5, 0.2, 0.2222222222222222, 0.1, 0.07692307692307693, 0.05, 0.1095890410958904, 0.30434782608695654, 0.15, 0.05, 0.23809523809523808, 0.12307692307692308, 0.3191489361702128, 0.09090909090909091, 0.18181818181818182, 0.24, 0.3157894736842105, 0.0, 0.3333333333333333, 0.21212121212121213, 0.1, 0.07692307692307693, 0.22727272727272727, 0.2413793103448276, 0.1, 0.2222222222222222, 0.1875, 0.21428571428571427, 0.058823529411764705, 0.17647058823529413, 0.34615384615384615, 0.375, 0.15384615384615385, 0.21052631578947367, 0.16666666666666666, 0.24, 0.0625, 0.3793103448275862, 0.2222222222222222], 'fmeasure': [0.03076923076923077, 0.02531645569620253, 0.0, 0.11940298507462686, 0.21333333333333335, 0.12307692307692307, 0.17777777777777778, 0.0, 0.18749999999999997, 0.08823529411764705, 0.1095890410958904, 0.20895522388059704, 0.09090909090909091, 0.16666666666666666, 0.0, 0.05333333333333334, 0.17500000000000002, 0.11320754716981132, 0.18918918918918917, 0.060606060606060615, 0.058823529411764705, 0.052631578947368425, 0.07142857142857144, 0.0, 0.11428571428571428, 0.07317073170731707, 0.16, 0.1408450704225352, 0.14285714285714285, 0.0784313725490196, 0.16666666666666666, 0.208955223880597, 0.08955223880597016, 0.10126582278481013, 0.11904761904761903, 0.07407407407407407, 0.07692307692307693, 0.12000000000000001, 0.1584158415841584, 0.15000000000000002, 0.08695652173913043, 0.044444444444444446, 0.13999999999999999, 0.23157894736842108, 0.07317073170731708, 0.13559322033898305, 0.05194805194805195, 0.1647058823529412, 0.0821917808219178, 0.1492537313432836, 0.10526315789473684, 0.06153846153846155, 0.25396825396825395, 0.136986301369863, 0.045454545454545456, 0.18181818181818182, 0.08108108108108107, 0.13114754098360656, 0.031746031746031744, 0.1111111111111111, 0.07228915662650602, 0.0, 0.10256410256410256, 0.1090909090909091, 0.1964285714285714, 0.15384615384615383, 0.07894736842105264, 0.14117647058823532, 0.0, 0.12903225806451615, 0.10752688172043011, 0.03921568627450981, 0.10526315789473684, 0.09523809523809523, 0.0975609756097561, 0.041666666666666664, 0.10126582278481013, 0.18390804597701152, 0.08247422680412371, 0.11382113821138212, 0.12048192771084339, 0.07407407407407407, 0.0888888888888889, 0.0821917808219178, 0.052631578947368425, 0.16, 0.29729729729729726, 0.12987012987012986, 0.20588235294117646, 0.169811320754717, 0.07894736842105263, 0.08, 0.08450704225352113, 0.042105263157894736, 0.0, 0.14492753623188406, 0.10416666666666667, 0.12121212121212123, 0.09090909090909093, 0.09999999999999999, 0.2178217821782178, 0.125, 0.1846153846153846, 0.10714285714285714, 0.0, 0.12307692307692308, 0.1590909090909091, 0.1518987341772152, 0.13114754098360656, 0.02702702702702703, 0.05405405405405405, 0.07920792079207921, 0.06779661016949153, 0.176, 0.034782608695652174, 0.1941747572815534, 0.12643678160919541, 0.1839080459770115, 0.06741573033707865, 0.09302325581395349, 0.15625, 0.08, 0.16260162601626016, 0.09090909090909093, 0.1728395061728395, 0.15584415584415584, 0.16326530612244897, 0.12307692307692308, 0.16666666666666666, 0.2222222222222222, 0.047058823529411764, 0.09523809523809523, 0.15625, 0.2222222222222222, 0.034482758620689655, 0.10126582278481013, 0.07142857142857142, 0.09803921568627452, 0.1714285714285714, 0.2857142857142857, 0.13157894736842105, 0.06976744186046512, 0.03389830508474576, 0.08450704225352113, 0.024999999999999998, 0.05555555555555555, 0.14035087719298245, 0.13157894736842105, 0.0821917808219178, 0.04081632653061224, 0.05825242718446602, 0.12820512820512822, 0.03174603174603175, 0.049999999999999996, 0.03703703703703704, 0.08333333333333333, 0.025974025974025976, 0.029411764705882353, 0.0, 0.0449438202247191, 0.10909090909090909, 0.14634146341463414, 0.06779661016949153, 0.125, 0.06349206349206349, 0.125, 0.07547169811320754, 0.10309278350515463, 0.07058823529411765, 0.20253164556962025, 0.0816326530612245, 0.075, 0.12048192771084339, 0.14545454545454545, 0.03571428571428572, 0.2028985507246377, 0.0, 0.15384615384615383, 0.06451612903225806, 0.05970149253731344, 0.17777777777777776, 0.0, 0.11111111111111112, 0.05194805194805195, 0.028169014084507043, 0.125, 0.0, 0.17391304347826086, 0.0392156862745098, 0.1348314606741573, 0.17391304347826086, 0.10204081632653061, 0.06976744186046513, 0.14285714285714285, 0.20408163265306123, 0.0851063829787234, 0.07142857142857141, 0.05, 0.06896551724137931, 0.08333333333333333, 0.0821917808219178, 0.08333333333333333, 0.19130434782608696, 0.03389830508474576, 0.1276595744680851, 0.15584415584415584, 0.05405405405405404, 0.05405405405405406, 0.15384615384615388, 0.058252427184466014, 0.09259259259259259, 0.05333333333333333, 0.09523809523809522, 0.1935483870967742, 0.07692307692307693, 0.11764705882352942, 0.0, 0.05714285714285715, 0.14457831325301204, 0.15151515151515152, 0.0851063829787234, 0.08196721311475412, 0.0, 0.05970149253731344, 0.0, 0.1188118811881188, 0.11764705882352941, 0.1348314606741573, 0.09900990099009903, 0.125, 0.16216216216216214, 0.16296296296296295, 0.07317073170731707, 0.06399999999999999, 0.31111111111111117, 0.2181818181818182, 0.22641509433962265, 0.12371134020618559, 0.22448979591836737, 0.20454545454545456, 0.18604651162790695, 0.15873015873015872, 0.12820512820512822, 0.15841584158415842, 0.15267175572519084, 0.2434782608695652, 0.21739130434782605, 0.04819277108433734, 0.043010752688172046, 0.14285714285714288, 0.21212121212121213, 0.14141414141414144, 0.2222222222222222, 0.1176470588235294, 0.3076923076923077, 0.08450704225352113, 0.16666666666666669, 0.0, 0.035714285714285705, 0.07619047619047621, 0.14423076923076922, 0.13333333333333333, 0.125, 0.1081081081081081, 0.08333333333333333, 0.23008849557522124, 0.11764705882352941, 0.08333333333333333, 0.16129032258064516, 0.13559322033898305, 0.08695652173913043, 0.08163265306122448, 0.22222222222222224, 0.08695652173913043, 0.06153846153846154, 0.1714285714285714, 0.07692307692307691, 0.23423423423423423, 0.15625, 0.2191780821917808, 0.27272727272727276, 0.05714285714285714, 0.13414634146341461, 0.17500000000000002, 0.0761904761904762, 0.07692307692307693, 0.1437908496732026, 0.19801980198019803, 0.23188405797101447, 0.07207207207207207, 0.0689655172413793, 0.175, 0.1282051282051282, 0.1111111111111111, 0.1794871794871795, 0.125, 0.1142857142857143, 0.030303030303030304, 0.03389830508474576, 0.02702702702702703, 0.08333333333333333, 0.18181818181818182, 0.09677419354838708, 0.13636363636363638, 0.1388888888888889, 0.03773584905660377, 0.34146341463414637, 0.14634146341463414, 0.11594202898550725, 0.09523809523809523, 0.16901408450704225, 0.06666666666666667, 0.07142857142857142, 0.14814814814814817, 0.0, 0.0851063829787234, 0.11494252873563218, 0.09638554216867469, 0.2058823529411765, 0.09523809523809523, 0.04081632653061224, 0.05063291139240506, 0.0, 0.13333333333333333, 0.059701492537313425, 0.0, 0.07792207792207792, 0.12371134020618559, 0.11538461538461539, 0.08040201005025124, 0.05, 0.15789473684210525, 0.08888888888888889, 0.0, 0.047619047619047616, 0.03448275862068966, 0.0, 0.1095890410958904, 0.11940298507462686, 0.21568627450980393, 0.0, 0.029850746268656723, 0.03508771929824561, 0.03773584905660377, 0.0816326530612245, 0.12844036697247707, 0.031746031746031744, 0.15789473684210528, 0.10958904109589039, 0.0, 0.04054054054054054, 0.1111111111111111, 0.16666666666666666, 0.13793103448275862, 0.26744186046511625, 0.09375, 0.1212121212121212, 0.1188118811881188, 0.24096385542168675, 0.0967741935483871, 0.16216216216216217, 0.16793893129770993, 0.23529411764705882, 0.07228915662650602, 0.15, 0.0857142857142857, 0.1391304347826087, 0.06818181818181818, 0.13793103448275862, 0.0909090909090909, 0.12765957446808507, 0.06976744186046513, 0.057971014492753624, 0.18867924528301888, 0.18666666666666665, 0.03389830508474576, 0.019417475728155338, 0.13461538461538464, 0.0, 0.06779661016949153, 0.09876543209876544, 0.06593406593406594, 0.2268041237113402, 0.06060606060606061, 0.1818181818181818, 0.21052631578947367, 0.15873015873015875, 0.24489795918367344, 0.05, 0.11111111111111109, 0.14634146341463417, 0.11764705882352941, 0.08450704225352111, 0.09615384615384616, 0.0, 0.05714285714285715, 0.05128205128205128, 0.10869565217391305, 0.06896551724137932, 0.09523809523809523, 0.08571428571428572, 0.07500000000000001, 0.05405405405405406, 0.037735849056603765, 0.07547169811320754, 0.08695652173913043, 0.03225806451612903, 0.19999999999999998, 0.06779661016949153, 0.09523809523809525, 0.16666666666666666, 0.15789473684210525, 0.16091954022988506, 0.02531645569620253, 0.12631578947368421, 0.16470588235294117, 0.0, 0.14457831325301204, 0.10256410256410256, 0.047058823529411764, 0.16216216216216217, 0.020202020202020204, 0.04819277108433735, 0.10309278350515463, 0.03225806451612903, 0.04395604395604395, 0.03603603603603604, 0.1095890410958904, 0.15686274509803924, 0.052631578947368425, 0.11764705882352941, 0.03921568627450981, 0.014184397163120567, 0.03225806451612903, 0.128, 0.2153846153846154, 0.10909090909090909, 0.027397260273972605, 0.14285714285714285, 0.12307692307692308, 0.2542372881355932, 0.09375000000000001, 0.15584415584415584, 0.22222222222222224, 0.12499999999999999, 0.0, 0.16666666666666666, 0.14285714285714288, 0.06666666666666667, 0.041666666666666664, 0.15151515151515152, 0.14432989690721648, 0.06349206349206349, 0.10810810810810811, 0.14285714285714285, 0.09677419354838708, 0.019047619047619046, 0.04838709677419355, 0.23076923076923078, 0.17142857142857143, 0.034482758620689655, 0.14678899082568805, 0.10666666666666666, 0.14634146341463414, 0.03225806451612903, 0.18803418803418803, 0.08]}\n",
            "Average for precision is 0.09205688485240877\n",
            "Average for recall is 0.16560683367493123\n",
            "Average for fmeasure is 0.10875718124325222\n",
            "/content/drive/MyDrive/Colab Notebooks/data/testdata/Tight Regret Bounds for Bayesian Optimization in One Dimension.csv\n",
            "['Binary and multi-class classification We use a set of binary classification tasks (see Table 1) that covers various types of sentence classification, including sentiment analysis (MR, SST), question-type (TREC), product reviews (CR), subjectivity/objectivity (SUBJ) and opinion polarity (MPQA).', '(Manning et al., 2014) and the VADER sentiment analyzer (Hutto and Gilbert, 2014).', 'The importance of the different features reflect the nature of the different communities.', 'The intuition is\\nthat, when we want to reduce the (embedding) parameters and still learn a joint representation, two different sentiment effects need to be separated in different vector spaces.', 'The key observation for multiplication is that a mixture of rank-1 tensors can be treated as a probability distribution\\nover the rank-1 tensors with expectation equal to the true function, by considering the weight of each rank-1 term wk as its probability.\\n', 'Most recently, the Transformer model (Vaswani et al., 2017), which is based solely on a selfattention mechanism (Parikh et al., 2016) and feed-forward connections, has further advanced the field of NMT, both in terms of translation quality and speed of convergence.\\n', 'The authors in (Gribonval et al., 2015b; Vainsencher et al., 2011) derive sample complexity bounds for the rate of convergence towards 0 of the absolute difference in (3) when Φ(x) = ||x||22, D is a general constraint set, and g(a) ranges from the lp-norms and characteristic functions of compact sets to the indicator function of non-negative vectors or k-sparse vectors.', 'Lau et al. (2015, 2016) explored a number of unsupervised models for predicting acceptability, including n-gram language models, Bayesian HMMs, LDA-based models, and a simple recurrent network language model.', 'The same levels of annotations are available as for the RIKEN corpus.', 'For every function in ΓC,Br , every sigmoidal function σ, every probability measure, and every k ≥ 1, there exists a linear combination of\\nsigmoidal functions fk(x) of the form\\nfk(x) = k∑ j=1 αjσ(y T j x + θj), (15)\\nsuch that ∫ Br (f(x)− fk(x))2µ(dx) ≤', 'This model is updated using (typically noisy) samples, which are selected to steer towards the function maximum.\\n', ', f(xT )) and noisy samples y = (y1, . . .', ', yT )) is known as the maximum information gain.', 'Here I(f ;y) denotes the mutual information (Cover & Thomas, 2001) between the function values and noisy samples, and O∗(·) denotes asymptotic notation up to logarithmic factors.\\n', 'The guarantee (2) ensures sub-linear cumulative regret for many kernels of interest.', 'We show that the best possible cumulative regret behaves as Θ∗( √ T ) under mild assumptions on the kernel, thus identifying both cases where (2) is nearoptimal, and cases where it is strictly suboptimal.', 'Under mild technical assumptions on the kernel, satisfied (for example) by the SE kernel and Matérn-ν kernel with ν > 2, the best possible cumulative regret of noisy BO in one dimension behaves as Ω( √ T ) and O( √ T log T ).\\n', 'Our results have several important implications:\\n• To our knowledge, our lower bound is the first of any kind in the noisy Bayesian setting, and is tight up to a√\\nlog T factor under our technical assumptions.\\n', '• On the other hand, our upper bound establishes that the upper bound of (Srinivas et al., 2010) for the Matérn-ν kernel, namely O∗(T ν+2 2ν+2 ), is strictly suboptimal for\\nν > 2.', 'For example, if ν = 3, then this is O∗(T 0.625), as opposed to our upper bound of O∗(T 0.5).', 'Our upper bound is stated formally in Section 3, and its technical assumptions are given in Section 2.1.', 'The kernel k is stationary, depending on its inputs (x, x′) only through τ = x− x′;\\n2.', 'Next, we give some high-probability assumptions on the random function f itself.\\nAssumption 2.', 'There exists a constant δ1 ∈ (0, 1) such that, with probability at least 1− δ1, we have the following:\\n1.', 'This implies that f is c1-Lipschitz continuous, and f ′ is c2-Lipschitz continuous.\\n']\n",
            "237/237 [==============================] - 176s 743ms/step\n",
            "{'precision': [0.021739130434782608, 0.021739130434782608, 0.0, 0.07692307692307693, 0.16, 0.09523809523809523, 0.10810810810810811, 0.0, 0.13636363636363635, 0.057692307692307696, 0.08, 0.16666666666666666, 0.08108108108108109, 0.125, 0.0, 0.03508771929824561, 0.1346153846153846, 0.08108108108108109, 0.16666666666666666, 0.038461538461538464, 0.058823529411764705, 0.030303030303030304, 0.05, 0.0, 0.07017543859649122, 0.06818181818181818, 0.13043478260869565, 0.09259259259259259, 0.09523809523809523, 0.06666666666666667, 0.13157894736842105, 0.14285714285714285, 0.07317073170731707, 0.12903225806451613, 0.08064516129032258, 0.046511627906976744, 0.05263157894736842, 0.07792207792207792, 0.14814814814814814, 0.0967741935483871, 0.06382978723404255, 0.02564102564102564, 0.08433734939759036, 0.2391304347826087, 0.057692307692307696, 0.1111111111111111, 0.03225806451612903, 0.1320754716981132, 0.05263157894736842, 0.11363636363636363, 0.08163265306122448, 0.05128205128205128, 0.20512820512820512, 0.09803921568627451, 0.03389830508474576, 0.24324324324324326, 0.07142857142857142, 0.1111111111111111, 0.023255813953488372, 0.08333333333333333, 0.05357142857142857, 0.0, 0.11428571428571428, 0.07692307692307693, 0.15942028985507245, 0.1111111111111111, 0.05, 0.11538461538461539, 0.0, 0.0975609756097561, 0.09090909090909091, 0.024390243902439025, 0.11428571428571428, 0.09375, 0.07142857142857142, 0.034482758620689655, 0.09523809523809523, 0.15384615384615385, 0.06896551724137931, 0.07526881720430108, 0.09259259259259259, 0.05128205128205128, 0.07547169811320754, 0.05172413793103448, 0.05, 0.15789473684210525, 0.24444444444444444, 0.0847457627118644, 0.18421052631578946, 0.12162162162162163, 0.06, 0.05454545454545454, 0.06818181818181818, 0.03636363636363636, 0.0, 0.1724137931034483, 0.06493506493506493, 0.09523809523809523, 0.07692307692307693, 0.1875, 0.14285714285714285, 0.07317073170731707, 0.15, 0.08823529411764706, 0.0, 0.10810810810810811, 0.1111111111111111, 0.13043478260869565, 0.1111111111111111, 0.02, 0.044444444444444446, 0.11764705882352941, 0.05263157894736842, 0.2391304347826087, 0.02857142857142857, 0.17543859649122806, 0.07586206896551724, 0.14035087719298245, 0.04838709677419355, 0.09523809523809523, 0.15625, 0.05555555555555555, 0.19607843137254902, 0.07692307692307693, 0.1794871794871795, 0.1111111111111111, 0.125, 0.21052631578947367, 0.24242424242424243, 0.2857142857142857, 0.09090909090909091, 0.06779661016949153, 0.11627906976744186, 0.17391304347826086, 0.02127659574468085, 0.08695652173913043, 0.05555555555555555, 0.1388888888888889, 0.13636363636363635, 0.23809523809523808, 0.07936507936507936, 0.047619047619047616, 0.02127659574468085, 0.09090909090909091, 0.01818181818181818, 0.034482758620689655, 0.11428571428571428, 0.11363636363636363, 0.06666666666666667, 0.034482758620689655, 0.06818181818181818, 0.10204081632653061, 0.029411764705882353, 0.043478260869565216, 0.034482758620689655, 0.05660377358490566, 0.01694915254237288, 0.020833333333333332, 0.0, 0.029850746268656716, 0.13043478260869565, 0.13043478260869565, 0.043478260869565216, 0.09259259259259259, 0.05555555555555555, 0.11363636363636363, 0.09523809523809523, 0.07936507936507936, 0.047619047619047616, 0.1702127659574468, 0.06060606060606061, 0.05454545454545454, 0.08620689655172414, 0.11428571428571428, 0.022727272727272728, 0.15555555555555556, 0.0, 0.11363636363636363, 0.04081632653061224, 0.058823529411764705, 0.17391304347826086, 0.0, 0.07547169811320754, 0.03389830508474576, 0.020833333333333332, 0.10810810810810811, 0.0, 0.17391304347826086, 0.034482758620689655, 0.10714285714285714, 0.15789473684210525, 0.1111111111111111, 0.058823529411764705, 0.12962962962962962, 0.18867924528301888, 0.07142857142857142, 0.05128205128205128, 0.04878048780487805, 0.04878048780487805, 0.06349206349206349, 0.05172413793103448, 0.08163265306122448, 0.16666666666666666, 0.020833333333333332, 0.08571428571428572, 0.16666666666666666, 0.03488372093023256, 0.0297029702970297, 0.11538461538461539, 0.03488372093023256, 0.05319148936170213, 0.046511627906976744, 0.05714285714285714, 0.16216216216216217, 0.1, 0.11428571428571428, 0.0, 0.045454545454545456, 0.09836065573770492, 0.13513513513513514, 0.05714285714285714, 0.05102040816326531, 0.0, 0.05, 0.0, 0.11764705882352941, 0.11320754716981132, 0.14285714285714285, 0.06944444444444445, 0.125, 0.19148936170212766, 0.14102564102564102, 0.08823529411764706, 0.05970149253731343, 0.23333333333333334, 0.1487603305785124, 0.17142857142857143, 0.16216216216216217, 0.24444444444444444, 0.1836734693877551, 0.22641509433962265, 0.1282051282051282, 0.1, 0.125, 0.10869565217391304, 0.19718309859154928, 0.15, 0.027777777777777776, 0.027777777777777776, 0.09722222222222222, 0.1794871794871795, 0.09722222222222222, 0.2857142857142857, 0.09, 0.391304347826087, 0.07894736842105263, 0.13559322033898305, 0.0, 0.02564102564102564, 0.04819277108433735, 0.2830188679245283, 0.11290322580645161, 0.1111111111111111, 0.07272727272727272, 0.056818181818181816, 0.35135135135135137, 0.07692307692307693, 0.06896551724137931, 0.1282051282051282, 0.11764705882352941, 0.06060606060606061, 0.07407407407407407, 0.23333333333333334, 0.06, 0.03571428571428571, 0.25, 0.0547945205479452, 0.21666666666666667, 0.14705882352941177, 0.20512820512820512, 0.19672131147540983, 0.07317073170731707, 0.25, 0.1794871794871795, 0.08333333333333333, 0.047619047619047616, 0.11956521739130435, 0.16129032258064516, 0.1702127659574468, 0.05, 0.06976744186046512, 0.109375, 0.0847457627118644, 0.06666666666666667, 0.25, 0.08163265306122448, 0.08888888888888889, 0.02040816326530612, 0.0196078431372549, 0.015625, 0.06, 0.21739130434782608, 0.06666666666666667, 0.08571428571428572, 0.09615384615384616, 0.023809523809523808, 0.3442622950819672, 0.10526315789473684, 0.09302325581395349, 0.0547945205479452, 0.17647058823529413, 0.06451612903225806, 0.047619047619047616, 0.15789473684210525, 0.0, 0.058823529411764705, 0.15151515151515152, 0.1, 0.21212121212121213, 0.07142857142857142, 0.038461538461538464, 0.043478260869565216, 0.0, 0.16666666666666666, 0.046511627906976744, 0.0, 0.05172413793103448, 0.1, 0.06976744186046512, 0.047619047619047616, 0.03571428571428571, 0.125, 0.06896551724137931, 0.0, 0.06666666666666667, 0.02, 0.0, 0.08333333333333333, 0.10256410256410256, 0.1774193548387097, 0.0, 0.024390243902439025, 0.0196078431372549, 0.022222222222222223, 0.05, 0.08139534883720931, 0.02040816326530612, 0.1, 0.08163265306122448, 0.0, 0.021739130434782608, 0.09523809523809523, 0.14285714285714285, 0.09523809523809523, 0.26744186046511625, 0.07317073170731707, 0.07792207792207792, 0.06976744186046512, 0.2, 0.06382978723404255, 0.10843373493975904, 0.1375, 0.2608695652173913, 0.06521739130434782, 0.16666666666666666, 0.10344827586206896, 0.1095890410958904, 0.06, 0.17391304347826086, 0.06818181818181818, 0.0967741935483871, 0.08571428571428572, 0.04, 0.15625, 0.16666666666666666, 0.021739130434782608, 0.011764705882352941, 0.09722222222222222, 0.0, 0.0392156862745098, 0.10810810810810811, 0.03896103896103896, 0.19298245614035087, 0.04918032786885246, 0.15217391304347827, 0.19672131147540983, 0.13333333333333333, 0.21428571428571427, 0.04, 0.06976744186046512, 0.13333333333333333, 0.07894736842105263, 0.06, 0.054945054945054944, 0.0, 0.03278688524590164, 0.03225806451612903, 0.06944444444444445, 0.04040404040404041, 0.06, 0.06976744186046512, 0.045454545454545456, 0.04, 0.024390243902439025, 0.046875, 0.07142857142857142, 0.019230769230769232, 0.23809523809523808, 0.05714285714285714, 0.07547169811320754, 0.10526315789473684, 0.14754098360655737, 0.15217391304347827, 0.01639344262295082, 0.08, 0.125, 0.0, 0.13953488372093023, 0.08888888888888889, 0.02631578947368421, 0.12, 0.01098901098901099, 0.03571428571428571, 0.08196721311475409, 0.03225806451612903, 0.027777777777777776, 0.020202020202020204, 0.08, 0.09302325581395349, 0.030303030303030304, 0.08, 0.024390243902439025, 0.0078125, 0.023809523809523808, 0.15384615384615385, 0.16666666666666666, 0.08571428571428572, 0.018867924528301886, 0.10204081632653061, 0.12307692307692308, 0.2112676056338028, 0.0967741935483871, 0.13636363636363635, 0.20689655172413793, 0.07792207792207792, 0.0, 0.1111111111111111, 0.1076923076923077, 0.05, 0.02857142857142857, 0.11363636363636363, 0.10294117647058823, 0.046511627906976744, 0.07142857142857142, 0.11538461538461539, 0.0625, 0.011363636363636364, 0.028037383177570093, 0.17307692307692307, 0.1111111111111111, 0.019417475728155338, 0.11267605633802817, 0.0784313725490196, 0.10526315789473684, 0.021739130434782608, 0.125, 0.04878048780487805, 0.03333333333333333, 0.09090909090909091, 0.05454545454545454, 0.0975609756097561, 0.06451612903225806, 0.17307692307692307, 0.20833333333333334, 0.0851063829787234, 0.024390243902439025, 0.0975609756097561, 0.06, 0.0, 0.019230769230769232, 0.1111111111111111, 0.021052631578947368, 0.1282051282051282, 0.06557377049180328, 0.0967741935483871, 0.16326530612244897, 0.04424778761061947, 0.03508771929824561, 0.06521739130434782, 0.008620689655172414, 0.08333333333333333, 0.02564102564102564], 'recall': [0.05263157894736842, 0.030303030303030304, 0.0, 0.26666666666666666, 0.32, 0.17391304347826086, 0.5, 0.0, 0.3, 0.1875, 0.17391304347826086, 0.28, 0.10344827586206896, 0.25, 0.0, 0.1111111111111111, 0.25, 0.1875, 0.21875, 0.14285714285714285, 0.058823529411764705, 0.2, 0.125, 0.0, 0.3076923076923077, 0.07894736842105263, 0.20689655172413793, 0.29411764705882354, 0.2857142857142857, 0.09523809523809523, 0.22727272727272727, 0.3888888888888889, 0.11538461538461539, 0.08333333333333333, 0.22727272727272727, 0.18181818181818182, 0.14285714285714285, 0.2608695652173913, 0.1702127659574468, 0.3333333333333333, 0.13636363636363635, 0.16666666666666666, 0.4117647058823529, 0.22448979591836735, 0.1, 0.17391304347826086, 0.13333333333333333, 0.21875, 0.1875, 0.21739130434782608, 0.14814814814814814, 0.07692307692307693, 0.3333333333333333, 0.22727272727272727, 0.06896551724137931, 0.14516129032258066, 0.09375, 0.16, 0.05, 0.16666666666666666, 0.1111111111111111, 0.0, 0.09302325581395349, 0.1875, 0.2558139534883721, 0.25, 0.1875, 0.18181818181818182, 0.0, 0.19047619047619047, 0.13157894736842105, 0.1, 0.0975609756097561, 0.0967741935483871, 0.15384615384615385, 0.05263157894736842, 0.10810810810810811, 0.22857142857142856, 0.10256410256410256, 0.23333333333333334, 0.1724137931034483, 0.13333333333333333, 0.10810810810810811, 0.2, 0.05555555555555555, 0.16216216216216217, 0.3793103448275862, 0.2777777777777778, 0.23333333333333334, 0.28125, 0.11538461538461539, 0.15, 0.1111111111111111, 0.05, 0.0, 0.125, 0.2631578947368421, 0.16666666666666666, 0.1111111111111111, 0.06818181818181818, 0.4583333333333333, 0.42857142857142855, 0.24, 0.13636363636363635, 0.0, 0.14285714285714285, 0.28, 0.18181818181818182, 0.16, 0.041666666666666664, 0.06896551724137931, 0.05970149253731343, 0.09523809523809523, 0.13924050632911392, 0.044444444444444446, 0.21739130434782608, 0.3793103448275862, 0.26666666666666666, 0.1111111111111111, 0.09090909090909091, 0.15625, 0.14285714285714285, 0.1388888888888889, 0.1111111111111111, 0.16666666666666666, 0.2608695652173913, 0.23529411764705882, 0.08695652173913043, 0.12698412698412698, 0.18181818181818182, 0.031746031746031744, 0.16, 0.23809523809523808, 0.3076923076923077, 0.09090909090909091, 0.12121212121212122, 0.1, 0.07575757575757576, 0.23076923076923078, 0.35714285714285715, 0.38461538461538464, 0.13043478260869565, 0.08333333333333333, 0.07894736842105263, 0.04, 0.14285714285714285, 0.18181818181818182, 0.15625, 0.10714285714285714, 0.05, 0.05084745762711865, 0.1724137931034483, 0.034482758620689655, 0.058823529411764705, 0.04, 0.15789473684210525, 0.05555555555555555, 0.05, 0.0, 0.09090909090909091, 0.09375, 0.16666666666666666, 0.15384615384615385, 0.19230769230769232, 0.07407407407407407, 0.1388888888888889, 0.0625, 0.14705882352941177, 0.13636363636363635, 0.25, 0.125, 0.12, 0.2, 0.2, 0.08333333333333333, 0.2916666666666667, 0.0, 0.23809523809523808, 0.15384615384615385, 0.06060606060606061, 0.18181818181818182, 0.0, 0.21052631578947367, 0.1111111111111111, 0.043478260869565216, 0.14814814814814814, 0.0, 0.17391304347826086, 0.045454545454545456, 0.18181818181818182, 0.1935483870967742, 0.09433962264150944, 0.08571428571428572, 0.1590909090909091, 0.2222222222222222, 0.10526315789473684, 0.11764705882352941, 0.05128205128205128, 0.11764705882352941, 0.12121212121212122, 0.2, 0.0851063829787234, 0.22448979591836735, 0.09090909090909091, 0.25, 0.14634146341463414, 0.12, 0.3, 0.23076923076923078, 0.17647058823529413, 0.35714285714285715, 0.0625, 0.2857142857142857, 0.24, 0.0625, 0.12121212121212122, 0.0, 0.07692307692307693, 0.2727272727272727, 0.1724137931034483, 0.16666666666666666, 0.20833333333333334, 0.0, 0.07407407407407407, 0.0, 0.12, 0.12244897959183673, 0.1276595744680851, 0.1724137931034483, 0.125, 0.140625, 0.19298245614035087, 0.0625, 0.06896551724137931, 0.4666666666666667, 0.4090909090909091, 0.3333333333333333, 0.1, 0.20754716981132076, 0.23076923076923078, 0.15789473684210525, 0.20833333333333334, 0.17857142857142858, 0.21621621621621623, 0.2564102564102564, 0.3181818181818182, 0.39473684210526316, 0.18181818181818182, 0.09523809523809523, 0.2692307692307692, 0.25925925925925924, 0.25925925925925924, 0.18181818181818182, 0.16981132075471697, 0.2535211267605634, 0.09090909090909091, 0.21621621621621623, 0.0, 0.058823529411764705, 0.18181818181818182, 0.0967741935483871, 0.16279069767441862, 0.14285714285714285, 0.21052631578947367, 0.15625, 0.17105263157894737, 0.25, 0.10526315789473684, 0.21739130434782608, 0.16, 0.15384615384615385, 0.09090909090909091, 0.21212121212121213, 0.15789473684210525, 0.2222222222222222, 0.13043478260869565, 0.12903225806451613, 0.2549019607843137, 0.16666666666666666, 0.23529411764705882, 0.4444444444444444, 0.046875, 0.09166666666666666, 0.17073170731707318, 0.07017543859649122, 0.2, 0.18032786885245902, 0.2564102564102564, 0.36363636363636365, 0.12903225806451613, 0.06818181818181818, 0.4375, 0.2631578947368421, 0.3333333333333333, 0.14, 0.26666666666666666, 0.16, 0.058823529411764705, 0.125, 0.1, 0.13636363636363635, 0.15625, 0.17647058823529413, 0.3333333333333333, 0.25, 0.09090909090909091, 0.3387096774193548, 0.24, 0.15384615384615385, 0.36363636363636365, 0.16216216216216217, 0.06896551724137931, 0.14285714285714285, 0.13953488372093023, 0.0, 0.15384615384615385, 0.09259259259259259, 0.09302325581395349, 0.2, 0.14285714285714285, 0.043478260869565216, 0.06060606060606061, 0.0, 0.1111111111111111, 0.08333333333333333, 0.0, 0.15789473684210525, 0.16216216216216217, 0.3333333333333333, 0.25806451612903225, 0.08333333333333333, 0.21428571428571427, 0.125, 0.0, 0.037037037037037035, 0.125, 0.0, 0.16, 0.14285714285714285, 0.275, 0.0, 0.038461538461538464, 0.16666666666666666, 0.125, 0.2222222222222222, 0.30434782608695654, 0.07142857142857142, 0.375, 0.16666666666666666, 0.0, 0.3, 0.13333333333333333, 0.2, 0.25, 0.26744186046511625, 0.13043478260869565, 0.2727272727272727, 0.4, 0.30303030303030304, 0.2, 0.32142857142857145, 0.21568627450980393, 0.21428571428571427, 0.08108108108108109, 0.13636363636363635, 0.07317073170731707, 0.19047619047619047, 0.07894736842105263, 0.11428571428571428, 0.13636363636363635, 0.1875, 0.058823529411764705, 0.10526315789473684, 0.23809523809523808, 0.21212121212121213, 0.07692307692307693, 0.05555555555555555, 0.21875, 0.0, 0.25, 0.09090909090909091, 0.21428571428571427, 0.275, 0.07894736842105263, 0.22580645161290322, 0.22641509433962265, 0.19607843137254902, 0.2857142857142857, 0.06666666666666667, 0.2727272727272727, 0.16216216216216217, 0.23076923076923078, 0.14285714285714285, 0.38461538461538464, 0.0, 0.2222222222222222, 0.125, 0.25, 0.23529411764705882, 0.23076923076923078, 0.1111111111111111, 0.21428571428571427, 0.08333333333333333, 0.08333333333333333, 0.1935483870967742, 0.1111111111111111, 0.1, 0.1724137931034483, 0.08333333333333333, 0.12903225806451613, 0.4, 0.16981132075471697, 0.17073170731707318, 0.05555555555555555, 0.3, 0.2413793103448276, 0.0, 0.15, 0.12121212121212122, 0.2222222222222222, 0.25, 0.125, 0.07407407407407407, 0.1388888888888889, 0.03225806451612903, 0.10526315789473684, 0.16666666666666666, 0.17391304347826086, 0.5, 0.2, 0.2222222222222222, 0.1, 0.07692307692307693, 0.05, 0.1095890410958904, 0.30434782608695654, 0.15, 0.05, 0.23809523809523808, 0.12307692307692308, 0.3191489361702128, 0.09090909090909091, 0.18181818181818182, 0.24, 0.3157894736842105, 0.0, 0.3333333333333333, 0.21212121212121213, 0.1, 0.07692307692307693, 0.22727272727272727, 0.2413793103448276, 0.1, 0.2222222222222222, 0.1875, 0.21428571428571427, 0.058823529411764705, 0.17647058823529413, 0.34615384615384615, 0.375, 0.15384615384615385, 0.21052631578947367, 0.16666666666666666, 0.24, 0.0625, 0.3793103448275862, 0.2222222222222222, 0.024390243902439025, 0.23076923076923078, 0.23076923076923078, 0.13333333333333333, 0.044444444444444446, 0.21428571428571427, 0.15625, 0.12903225806451613, 0.08333333333333333, 0.08, 0.17647058823529413, 0.0, 0.125, 0.14814814814814814, 0.15384615384615385, 0.15625, 0.10256410256410256, 0.08333333333333333, 0.24242424242424243, 0.22727272727272727, 0.10526315789473684, 0.1875, 0.06666666666666667, 0.25, 0.07142857142857142], 'fmeasure': [0.03076923076923077, 0.02531645569620253, 0.0, 0.11940298507462686, 0.21333333333333335, 0.12307692307692307, 0.17777777777777778, 0.0, 0.18749999999999997, 0.08823529411764705, 0.1095890410958904, 0.20895522388059704, 0.09090909090909091, 0.16666666666666666, 0.0, 0.05333333333333334, 0.17500000000000002, 0.11320754716981132, 0.18918918918918917, 0.060606060606060615, 0.058823529411764705, 0.052631578947368425, 0.07142857142857144, 0.0, 0.11428571428571428, 0.07317073170731707, 0.16, 0.1408450704225352, 0.14285714285714285, 0.0784313725490196, 0.16666666666666666, 0.208955223880597, 0.08955223880597016, 0.10126582278481013, 0.11904761904761903, 0.07407407407407407, 0.07692307692307693, 0.12000000000000001, 0.1584158415841584, 0.15000000000000002, 0.08695652173913043, 0.044444444444444446, 0.13999999999999999, 0.23157894736842108, 0.07317073170731708, 0.13559322033898305, 0.05194805194805195, 0.1647058823529412, 0.0821917808219178, 0.1492537313432836, 0.10526315789473684, 0.06153846153846155, 0.25396825396825395, 0.136986301369863, 0.045454545454545456, 0.18181818181818182, 0.08108108108108107, 0.13114754098360656, 0.031746031746031744, 0.1111111111111111, 0.07228915662650602, 0.0, 0.10256410256410256, 0.1090909090909091, 0.1964285714285714, 0.15384615384615383, 0.07894736842105264, 0.14117647058823532, 0.0, 0.12903225806451615, 0.10752688172043011, 0.03921568627450981, 0.10526315789473684, 0.09523809523809523, 0.0975609756097561, 0.041666666666666664, 0.10126582278481013, 0.18390804597701152, 0.08247422680412371, 0.11382113821138212, 0.12048192771084339, 0.07407407407407407, 0.0888888888888889, 0.0821917808219178, 0.052631578947368425, 0.16, 0.29729729729729726, 0.12987012987012986, 0.20588235294117646, 0.169811320754717, 0.07894736842105263, 0.08, 0.08450704225352113, 0.042105263157894736, 0.0, 0.14492753623188406, 0.10416666666666667, 0.12121212121212123, 0.09090909090909093, 0.09999999999999999, 0.2178217821782178, 0.125, 0.1846153846153846, 0.10714285714285714, 0.0, 0.12307692307692308, 0.1590909090909091, 0.1518987341772152, 0.13114754098360656, 0.02702702702702703, 0.05405405405405405, 0.07920792079207921, 0.06779661016949153, 0.176, 0.034782608695652174, 0.1941747572815534, 0.12643678160919541, 0.1839080459770115, 0.06741573033707865, 0.09302325581395349, 0.15625, 0.08, 0.16260162601626016, 0.09090909090909093, 0.1728395061728395, 0.15584415584415584, 0.16326530612244897, 0.12307692307692308, 0.16666666666666666, 0.2222222222222222, 0.047058823529411764, 0.09523809523809523, 0.15625, 0.2222222222222222, 0.034482758620689655, 0.10126582278481013, 0.07142857142857142, 0.09803921568627452, 0.1714285714285714, 0.2857142857142857, 0.13157894736842105, 0.06976744186046512, 0.03389830508474576, 0.08450704225352113, 0.024999999999999998, 0.05555555555555555, 0.14035087719298245, 0.13157894736842105, 0.0821917808219178, 0.04081632653061224, 0.05825242718446602, 0.12820512820512822, 0.03174603174603175, 0.049999999999999996, 0.03703703703703704, 0.08333333333333333, 0.025974025974025976, 0.029411764705882353, 0.0, 0.0449438202247191, 0.10909090909090909, 0.14634146341463414, 0.06779661016949153, 0.125, 0.06349206349206349, 0.125, 0.07547169811320754, 0.10309278350515463, 0.07058823529411765, 0.20253164556962025, 0.0816326530612245, 0.075, 0.12048192771084339, 0.14545454545454545, 0.03571428571428572, 0.2028985507246377, 0.0, 0.15384615384615383, 0.06451612903225806, 0.05970149253731344, 0.17777777777777776, 0.0, 0.11111111111111112, 0.05194805194805195, 0.028169014084507043, 0.125, 0.0, 0.17391304347826086, 0.0392156862745098, 0.1348314606741573, 0.17391304347826086, 0.10204081632653061, 0.06976744186046513, 0.14285714285714285, 0.20408163265306123, 0.0851063829787234, 0.07142857142857141, 0.05, 0.06896551724137931, 0.08333333333333333, 0.0821917808219178, 0.08333333333333333, 0.19130434782608696, 0.03389830508474576, 0.1276595744680851, 0.15584415584415584, 0.05405405405405404, 0.05405405405405406, 0.15384615384615388, 0.058252427184466014, 0.09259259259259259, 0.05333333333333333, 0.09523809523809522, 0.1935483870967742, 0.07692307692307693, 0.11764705882352942, 0.0, 0.05714285714285715, 0.14457831325301204, 0.15151515151515152, 0.0851063829787234, 0.08196721311475412, 0.0, 0.05970149253731344, 0.0, 0.1188118811881188, 0.11764705882352941, 0.1348314606741573, 0.09900990099009903, 0.125, 0.16216216216216214, 0.16296296296296295, 0.07317073170731707, 0.06399999999999999, 0.31111111111111117, 0.2181818181818182, 0.22641509433962265, 0.12371134020618559, 0.22448979591836737, 0.20454545454545456, 0.18604651162790695, 0.15873015873015872, 0.12820512820512822, 0.15841584158415842, 0.15267175572519084, 0.2434782608695652, 0.21739130434782605, 0.04819277108433734, 0.043010752688172046, 0.14285714285714288, 0.21212121212121213, 0.14141414141414144, 0.2222222222222222, 0.1176470588235294, 0.3076923076923077, 0.08450704225352113, 0.16666666666666669, 0.0, 0.035714285714285705, 0.07619047619047621, 0.14423076923076922, 0.13333333333333333, 0.125, 0.1081081081081081, 0.08333333333333333, 0.23008849557522124, 0.11764705882352941, 0.08333333333333333, 0.16129032258064516, 0.13559322033898305, 0.08695652173913043, 0.08163265306122448, 0.22222222222222224, 0.08695652173913043, 0.06153846153846154, 0.1714285714285714, 0.07692307692307691, 0.23423423423423423, 0.15625, 0.2191780821917808, 0.27272727272727276, 0.05714285714285714, 0.13414634146341461, 0.17500000000000002, 0.0761904761904762, 0.07692307692307693, 0.1437908496732026, 0.19801980198019803, 0.23188405797101447, 0.07207207207207207, 0.0689655172413793, 0.175, 0.1282051282051282, 0.1111111111111111, 0.1794871794871795, 0.125, 0.1142857142857143, 0.030303030303030304, 0.03389830508474576, 0.02702702702702703, 0.08333333333333333, 0.18181818181818182, 0.09677419354838708, 0.13636363636363638, 0.1388888888888889, 0.03773584905660377, 0.34146341463414637, 0.14634146341463414, 0.11594202898550725, 0.09523809523809523, 0.16901408450704225, 0.06666666666666667, 0.07142857142857142, 0.14814814814814817, 0.0, 0.0851063829787234, 0.11494252873563218, 0.09638554216867469, 0.2058823529411765, 0.09523809523809523, 0.04081632653061224, 0.05063291139240506, 0.0, 0.13333333333333333, 0.059701492537313425, 0.0, 0.07792207792207792, 0.12371134020618559, 0.11538461538461539, 0.08040201005025124, 0.05, 0.15789473684210525, 0.08888888888888889, 0.0, 0.047619047619047616, 0.03448275862068966, 0.0, 0.1095890410958904, 0.11940298507462686, 0.21568627450980393, 0.0, 0.029850746268656723, 0.03508771929824561, 0.03773584905660377, 0.0816326530612245, 0.12844036697247707, 0.031746031746031744, 0.15789473684210528, 0.10958904109589039, 0.0, 0.04054054054054054, 0.1111111111111111, 0.16666666666666666, 0.13793103448275862, 0.26744186046511625, 0.09375, 0.1212121212121212, 0.1188118811881188, 0.24096385542168675, 0.0967741935483871, 0.16216216216216217, 0.16793893129770993, 0.23529411764705882, 0.07228915662650602, 0.15, 0.0857142857142857, 0.1391304347826087, 0.06818181818181818, 0.13793103448275862, 0.0909090909090909, 0.12765957446808507, 0.06976744186046513, 0.057971014492753624, 0.18867924528301888, 0.18666666666666665, 0.03389830508474576, 0.019417475728155338, 0.13461538461538464, 0.0, 0.06779661016949153, 0.09876543209876544, 0.06593406593406594, 0.2268041237113402, 0.06060606060606061, 0.1818181818181818, 0.21052631578947367, 0.15873015873015875, 0.24489795918367344, 0.05, 0.11111111111111109, 0.14634146341463417, 0.11764705882352941, 0.08450704225352111, 0.09615384615384616, 0.0, 0.05714285714285715, 0.05128205128205128, 0.10869565217391305, 0.06896551724137932, 0.09523809523809523, 0.08571428571428572, 0.07500000000000001, 0.05405405405405406, 0.037735849056603765, 0.07547169811320754, 0.08695652173913043, 0.03225806451612903, 0.19999999999999998, 0.06779661016949153, 0.09523809523809525, 0.16666666666666666, 0.15789473684210525, 0.16091954022988506, 0.02531645569620253, 0.12631578947368421, 0.16470588235294117, 0.0, 0.14457831325301204, 0.10256410256410256, 0.047058823529411764, 0.16216216216216217, 0.020202020202020204, 0.04819277108433735, 0.10309278350515463, 0.03225806451612903, 0.04395604395604395, 0.03603603603603604, 0.1095890410958904, 0.15686274509803924, 0.052631578947368425, 0.11764705882352941, 0.03921568627450981, 0.014184397163120567, 0.03225806451612903, 0.128, 0.2153846153846154, 0.10909090909090909, 0.027397260273972605, 0.14285714285714285, 0.12307692307692308, 0.2542372881355932, 0.09375000000000001, 0.15584415584415584, 0.22222222222222224, 0.12499999999999999, 0.0, 0.16666666666666666, 0.14285714285714288, 0.06666666666666667, 0.041666666666666664, 0.15151515151515152, 0.14432989690721648, 0.06349206349206349, 0.10810810810810811, 0.14285714285714285, 0.09677419354838708, 0.019047619047619046, 0.04838709677419355, 0.23076923076923078, 0.17142857142857143, 0.034482758620689655, 0.14678899082568805, 0.10666666666666666, 0.14634146341463414, 0.03225806451612903, 0.18803418803418803, 0.08, 0.028169014084507043, 0.13043478260869568, 0.08823529411764705, 0.11267605633802817, 0.05263157894736842, 0.1914893617021277, 0.17857142857142858, 0.10256410256410256, 0.037735849056603765, 0.08791208791208793, 0.08955223880597014, 0.0, 0.03333333333333334, 0.12698412698412698, 0.037037037037037035, 0.14084507042253522, 0.08, 0.08955223880597014, 0.1951219512195122, 0.07407407407407408, 0.05263157894736842, 0.09677419354838708, 0.015267175572519083, 0.125, 0.03773584905660377]}\n",
            "Average for precision is 0.09115296529580753\n",
            "Average for recall is 0.164148713022173\n",
            "Average for fmeasure is 0.10771124403314801\n",
            "/content/drive/MyDrive/Colab Notebooks/data/testdata/A strong baseline for question relevancy ranking.csv\n",
            "['Community question-answer fora are great resources, collecting answers to frequently and lessfrequently asked questions on specific topics, but these are often not moderated and contain many irrelevant answers.', 'Community Question Answering (CQA), cast as a question relevancy ranking problem, was the topic of two shared tasks at SemEval 2016-17.', 'This is a non-trivial retrieval task, typically evaluated using mean average precision (MAP).', 'We present a strong baseline for this task, on par with or surpassing state-of-the-art systems.\\n', 'We use a question-answer dataset as auxiliary task; but we also experiment with datasets for pairwise classification tasks such as natural language inference and fake news detection.', 'This simple, easy-totrain model is on par or better than state-of-theart systems for question relevancy ranking.', 'Natural Language Inference Natural Language Inference (NLI), consists in predicting ENTAILMENT, CONTRADICTION or NEUTRAL, given a hypothesis and a premise.', 'We use the MNLI dataset as opposed to the SNLI data (Bowman et al., 2015; Nangia et al., 2017), since it contains different genres.', 'This task has been used before in a multi-task setting as a way to utilize general information about pairwise relations (Augenstein et al., 2018).', 'Formally, the FNC task consists in, given a headline and the body of\\n2http://www.fakenewschallenge.org/\\ntext which can be from the same news article or not, classify the stance of the body of text relative to what is claimed in the headline.', 'For the SemEval-16 data, our multitask MLP architecture with a question-answer auxiliary task performed best on all metrics, except accuracy, where the multi-task MLP using all auxiliary tasks performed best.', 'For a more direct comparison, we also train a more expressive model than the simple MTLbased model we propose.', 'For this model, we input sequences of embedded words (using pre-trained word embeddings) from each query into independent BiLSTM blocks and output a vector representation for each query.', 'We then concatenate the vector representations with the similarity features from our MTL model and feed it into a dense layer and a classification layer.', 'This way we can evaluate the usefulness of the flexible, expressive LSTM network directly (as our MTL model becomes an ablation instance of the full, more complex architecture).', 'We use the same dropout regularization and SGD values as for the MLP.', 'Tuning all parameters on the development data, we do not manage to outperform our proposed model, however.', 'See lines MTL-LSTM-SIM in Table 1 for results.', 'We show that simple feature engineering, combined with an auxiliary task and a simple feedfor-\\nward neural architecture is appropriate for a small dataset and manages to beat the baseline and the best performing systems for the Semeval task of question relevancy ranking.', 'We observe that introducing pairwise classification tasks leads to significant improvements in performance and a more stable model.', 'Overall, our simple model introduces a new strong baseline which is particularly useful when there is a lack of labeled data.']\n",
            "14/14 [==============================] - 10s 697ms/step\n",
            "{'precision': [0.021739130434782608, 0.021739130434782608, 0.0, 0.07692307692307693, 0.16, 0.09523809523809523, 0.10810810810810811, 0.0, 0.13636363636363635, 0.057692307692307696, 0.08, 0.16666666666666666, 0.08108108108108109, 0.125, 0.0, 0.03508771929824561, 0.1346153846153846, 0.08108108108108109, 0.16666666666666666, 0.038461538461538464, 0.058823529411764705, 0.030303030303030304, 0.05, 0.0, 0.07017543859649122, 0.06818181818181818, 0.13043478260869565, 0.09259259259259259, 0.09523809523809523, 0.06666666666666667, 0.13157894736842105, 0.14285714285714285, 0.07317073170731707, 0.12903225806451613, 0.08064516129032258, 0.046511627906976744, 0.05263157894736842, 0.07792207792207792, 0.14814814814814814, 0.0967741935483871, 0.06382978723404255, 0.02564102564102564, 0.08433734939759036, 0.2391304347826087, 0.057692307692307696, 0.1111111111111111, 0.03225806451612903, 0.1320754716981132, 0.05263157894736842, 0.11363636363636363, 0.08163265306122448, 0.05128205128205128, 0.20512820512820512, 0.09803921568627451, 0.03389830508474576, 0.24324324324324326, 0.07142857142857142, 0.1111111111111111, 0.023255813953488372, 0.08333333333333333, 0.05357142857142857, 0.0, 0.11428571428571428, 0.07692307692307693, 0.15942028985507245, 0.1111111111111111, 0.05, 0.11538461538461539, 0.0, 0.0975609756097561, 0.09090909090909091, 0.024390243902439025, 0.11428571428571428, 0.09375, 0.07142857142857142, 0.034482758620689655, 0.09523809523809523, 0.15384615384615385, 0.06896551724137931, 0.07526881720430108, 0.09259259259259259, 0.05128205128205128, 0.07547169811320754, 0.05172413793103448, 0.05, 0.15789473684210525, 0.24444444444444444, 0.0847457627118644, 0.18421052631578946, 0.12162162162162163, 0.06, 0.05454545454545454, 0.06818181818181818, 0.03636363636363636, 0.0, 0.1724137931034483, 0.06493506493506493, 0.09523809523809523, 0.07692307692307693, 0.1875, 0.14285714285714285, 0.07317073170731707, 0.15, 0.08823529411764706, 0.0, 0.10810810810810811, 0.1111111111111111, 0.13043478260869565, 0.1111111111111111, 0.02, 0.044444444444444446, 0.11764705882352941, 0.05263157894736842, 0.2391304347826087, 0.02857142857142857, 0.17543859649122806, 0.07586206896551724, 0.14035087719298245, 0.04838709677419355, 0.09523809523809523, 0.15625, 0.05555555555555555, 0.19607843137254902, 0.07692307692307693, 0.1794871794871795, 0.1111111111111111, 0.125, 0.21052631578947367, 0.24242424242424243, 0.2857142857142857, 0.09090909090909091, 0.06779661016949153, 0.11627906976744186, 0.17391304347826086, 0.02127659574468085, 0.08695652173913043, 0.05555555555555555, 0.1388888888888889, 0.13636363636363635, 0.23809523809523808, 0.07936507936507936, 0.047619047619047616, 0.02127659574468085, 0.09090909090909091, 0.01818181818181818, 0.034482758620689655, 0.11428571428571428, 0.11363636363636363, 0.06666666666666667, 0.034482758620689655, 0.06818181818181818, 0.10204081632653061, 0.029411764705882353, 0.043478260869565216, 0.034482758620689655, 0.05660377358490566, 0.01694915254237288, 0.020833333333333332, 0.0, 0.029850746268656716, 0.13043478260869565, 0.13043478260869565, 0.043478260869565216, 0.09259259259259259, 0.05555555555555555, 0.11363636363636363, 0.09523809523809523, 0.07936507936507936, 0.047619047619047616, 0.1702127659574468, 0.06060606060606061, 0.05454545454545454, 0.08620689655172414, 0.11428571428571428, 0.022727272727272728, 0.15555555555555556, 0.0, 0.11363636363636363, 0.04081632653061224, 0.058823529411764705, 0.17391304347826086, 0.0, 0.07547169811320754, 0.03389830508474576, 0.020833333333333332, 0.10810810810810811, 0.0, 0.17391304347826086, 0.034482758620689655, 0.10714285714285714, 0.15789473684210525, 0.1111111111111111, 0.058823529411764705, 0.12962962962962962, 0.18867924528301888, 0.07142857142857142, 0.05128205128205128, 0.04878048780487805, 0.04878048780487805, 0.06349206349206349, 0.05172413793103448, 0.08163265306122448, 0.16666666666666666, 0.020833333333333332, 0.08571428571428572, 0.16666666666666666, 0.03488372093023256, 0.0297029702970297, 0.11538461538461539, 0.03488372093023256, 0.05319148936170213, 0.046511627906976744, 0.05714285714285714, 0.16216216216216217, 0.1, 0.11428571428571428, 0.0, 0.045454545454545456, 0.09836065573770492, 0.13513513513513514, 0.05714285714285714, 0.05102040816326531, 0.0, 0.05, 0.0, 0.11764705882352941, 0.11320754716981132, 0.14285714285714285, 0.06944444444444445, 0.125, 0.19148936170212766, 0.14102564102564102, 0.08823529411764706, 0.05970149253731343, 0.23333333333333334, 0.1487603305785124, 0.17142857142857143, 0.16216216216216217, 0.24444444444444444, 0.1836734693877551, 0.22641509433962265, 0.1282051282051282, 0.1, 0.125, 0.10869565217391304, 0.19718309859154928, 0.15, 0.027777777777777776, 0.027777777777777776, 0.09722222222222222, 0.1794871794871795, 0.09722222222222222, 0.2857142857142857, 0.09, 0.391304347826087, 0.07894736842105263, 0.13559322033898305, 0.0, 0.02564102564102564, 0.04819277108433735, 0.2830188679245283, 0.11290322580645161, 0.1111111111111111, 0.07272727272727272, 0.056818181818181816, 0.35135135135135137, 0.07692307692307693, 0.06896551724137931, 0.1282051282051282, 0.11764705882352941, 0.06060606060606061, 0.07407407407407407, 0.23333333333333334, 0.06, 0.03571428571428571, 0.25, 0.0547945205479452, 0.21666666666666667, 0.14705882352941177, 0.20512820512820512, 0.19672131147540983, 0.07317073170731707, 0.25, 0.1794871794871795, 0.08333333333333333, 0.047619047619047616, 0.11956521739130435, 0.16129032258064516, 0.1702127659574468, 0.05, 0.06976744186046512, 0.109375, 0.0847457627118644, 0.06666666666666667, 0.25, 0.08163265306122448, 0.08888888888888889, 0.02040816326530612, 0.0196078431372549, 0.015625, 0.06, 0.21739130434782608, 0.06666666666666667, 0.08571428571428572, 0.09615384615384616, 0.023809523809523808, 0.3442622950819672, 0.10526315789473684, 0.09302325581395349, 0.0547945205479452, 0.17647058823529413, 0.06451612903225806, 0.047619047619047616, 0.15789473684210525, 0.0, 0.058823529411764705, 0.15151515151515152, 0.1, 0.21212121212121213, 0.07142857142857142, 0.038461538461538464, 0.043478260869565216, 0.0, 0.16666666666666666, 0.046511627906976744, 0.0, 0.05172413793103448, 0.1, 0.06976744186046512, 0.047619047619047616, 0.03571428571428571, 0.125, 0.06896551724137931, 0.0, 0.06666666666666667, 0.02, 0.0, 0.08333333333333333, 0.10256410256410256, 0.1774193548387097, 0.0, 0.024390243902439025, 0.0196078431372549, 0.022222222222222223, 0.05, 0.08139534883720931, 0.02040816326530612, 0.1, 0.08163265306122448, 0.0, 0.021739130434782608, 0.09523809523809523, 0.14285714285714285, 0.09523809523809523, 0.26744186046511625, 0.07317073170731707, 0.07792207792207792, 0.06976744186046512, 0.2, 0.06382978723404255, 0.10843373493975904, 0.1375, 0.2608695652173913, 0.06521739130434782, 0.16666666666666666, 0.10344827586206896, 0.1095890410958904, 0.06, 0.17391304347826086, 0.06818181818181818, 0.0967741935483871, 0.08571428571428572, 0.04, 0.15625, 0.16666666666666666, 0.021739130434782608, 0.011764705882352941, 0.09722222222222222, 0.0, 0.0392156862745098, 0.10810810810810811, 0.03896103896103896, 0.19298245614035087, 0.04918032786885246, 0.15217391304347827, 0.19672131147540983, 0.13333333333333333, 0.21428571428571427, 0.04, 0.06976744186046512, 0.13333333333333333, 0.07894736842105263, 0.06, 0.054945054945054944, 0.0, 0.03278688524590164, 0.03225806451612903, 0.06944444444444445, 0.04040404040404041, 0.06, 0.06976744186046512, 0.045454545454545456, 0.04, 0.024390243902439025, 0.046875, 0.07142857142857142, 0.019230769230769232, 0.23809523809523808, 0.05714285714285714, 0.07547169811320754, 0.10526315789473684, 0.14754098360655737, 0.15217391304347827, 0.01639344262295082, 0.08, 0.125, 0.0, 0.13953488372093023, 0.08888888888888889, 0.02631578947368421, 0.12, 0.01098901098901099, 0.03571428571428571, 0.08196721311475409, 0.03225806451612903, 0.027777777777777776, 0.020202020202020204, 0.08, 0.09302325581395349, 0.030303030303030304, 0.08, 0.024390243902439025, 0.0078125, 0.023809523809523808, 0.15384615384615385, 0.16666666666666666, 0.08571428571428572, 0.018867924528301886, 0.10204081632653061, 0.12307692307692308, 0.2112676056338028, 0.0967741935483871, 0.13636363636363635, 0.20689655172413793, 0.07792207792207792, 0.0, 0.1111111111111111, 0.1076923076923077, 0.05, 0.02857142857142857, 0.11363636363636363, 0.10294117647058823, 0.046511627906976744, 0.07142857142857142, 0.11538461538461539, 0.0625, 0.011363636363636364, 0.028037383177570093, 0.17307692307692307, 0.1111111111111111, 0.019417475728155338, 0.11267605633802817, 0.0784313725490196, 0.10526315789473684, 0.021739130434782608, 0.125, 0.04878048780487805, 0.03333333333333333, 0.09090909090909091, 0.05454545454545454, 0.0975609756097561, 0.06451612903225806, 0.17307692307692307, 0.20833333333333334, 0.0851063829787234, 0.024390243902439025, 0.0975609756097561, 0.06, 0.0, 0.019230769230769232, 0.1111111111111111, 0.021052631578947368, 0.1282051282051282, 0.06557377049180328, 0.0967741935483871, 0.16326530612244897, 0.04424778761061947, 0.03508771929824561, 0.06521739130434782, 0.008620689655172414, 0.08333333333333333, 0.02564102564102564, 0.057692307692307696, 0.07547169811320754, 0.023255813953488372, 0.05714285714285714, 0.05263157894736842, 0.08888888888888889, 0.022727272727272728, 0.09090909090909091, 0.08620689655172414, 0.16129032258064516, 0.10416666666666667, 0.13793103448275862, 0.11904761904761904, 0.10526315789473684, 0.06451612903225806, 0.046511627906976744, 0.10810810810810811, 0.01818181818181818, 0.11320754716981132, 0.03125], 'recall': [0.05263157894736842, 0.030303030303030304, 0.0, 0.26666666666666666, 0.32, 0.17391304347826086, 0.5, 0.0, 0.3, 0.1875, 0.17391304347826086, 0.28, 0.10344827586206896, 0.25, 0.0, 0.1111111111111111, 0.25, 0.1875, 0.21875, 0.14285714285714285, 0.058823529411764705, 0.2, 0.125, 0.0, 0.3076923076923077, 0.07894736842105263, 0.20689655172413793, 0.29411764705882354, 0.2857142857142857, 0.09523809523809523, 0.22727272727272727, 0.3888888888888889, 0.11538461538461539, 0.08333333333333333, 0.22727272727272727, 0.18181818181818182, 0.14285714285714285, 0.2608695652173913, 0.1702127659574468, 0.3333333333333333, 0.13636363636363635, 0.16666666666666666, 0.4117647058823529, 0.22448979591836735, 0.1, 0.17391304347826086, 0.13333333333333333, 0.21875, 0.1875, 0.21739130434782608, 0.14814814814814814, 0.07692307692307693, 0.3333333333333333, 0.22727272727272727, 0.06896551724137931, 0.14516129032258066, 0.09375, 0.16, 0.05, 0.16666666666666666, 0.1111111111111111, 0.0, 0.09302325581395349, 0.1875, 0.2558139534883721, 0.25, 0.1875, 0.18181818181818182, 0.0, 0.19047619047619047, 0.13157894736842105, 0.1, 0.0975609756097561, 0.0967741935483871, 0.15384615384615385, 0.05263157894736842, 0.10810810810810811, 0.22857142857142856, 0.10256410256410256, 0.23333333333333334, 0.1724137931034483, 0.13333333333333333, 0.10810810810810811, 0.2, 0.05555555555555555, 0.16216216216216217, 0.3793103448275862, 0.2777777777777778, 0.23333333333333334, 0.28125, 0.11538461538461539, 0.15, 0.1111111111111111, 0.05, 0.0, 0.125, 0.2631578947368421, 0.16666666666666666, 0.1111111111111111, 0.06818181818181818, 0.4583333333333333, 0.42857142857142855, 0.24, 0.13636363636363635, 0.0, 0.14285714285714285, 0.28, 0.18181818181818182, 0.16, 0.041666666666666664, 0.06896551724137931, 0.05970149253731343, 0.09523809523809523, 0.13924050632911392, 0.044444444444444446, 0.21739130434782608, 0.3793103448275862, 0.26666666666666666, 0.1111111111111111, 0.09090909090909091, 0.15625, 0.14285714285714285, 0.1388888888888889, 0.1111111111111111, 0.16666666666666666, 0.2608695652173913, 0.23529411764705882, 0.08695652173913043, 0.12698412698412698, 0.18181818181818182, 0.031746031746031744, 0.16, 0.23809523809523808, 0.3076923076923077, 0.09090909090909091, 0.12121212121212122, 0.1, 0.07575757575757576, 0.23076923076923078, 0.35714285714285715, 0.38461538461538464, 0.13043478260869565, 0.08333333333333333, 0.07894736842105263, 0.04, 0.14285714285714285, 0.18181818181818182, 0.15625, 0.10714285714285714, 0.05, 0.05084745762711865, 0.1724137931034483, 0.034482758620689655, 0.058823529411764705, 0.04, 0.15789473684210525, 0.05555555555555555, 0.05, 0.0, 0.09090909090909091, 0.09375, 0.16666666666666666, 0.15384615384615385, 0.19230769230769232, 0.07407407407407407, 0.1388888888888889, 0.0625, 0.14705882352941177, 0.13636363636363635, 0.25, 0.125, 0.12, 0.2, 0.2, 0.08333333333333333, 0.2916666666666667, 0.0, 0.23809523809523808, 0.15384615384615385, 0.06060606060606061, 0.18181818181818182, 0.0, 0.21052631578947367, 0.1111111111111111, 0.043478260869565216, 0.14814814814814814, 0.0, 0.17391304347826086, 0.045454545454545456, 0.18181818181818182, 0.1935483870967742, 0.09433962264150944, 0.08571428571428572, 0.1590909090909091, 0.2222222222222222, 0.10526315789473684, 0.11764705882352941, 0.05128205128205128, 0.11764705882352941, 0.12121212121212122, 0.2, 0.0851063829787234, 0.22448979591836735, 0.09090909090909091, 0.25, 0.14634146341463414, 0.12, 0.3, 0.23076923076923078, 0.17647058823529413, 0.35714285714285715, 0.0625, 0.2857142857142857, 0.24, 0.0625, 0.12121212121212122, 0.0, 0.07692307692307693, 0.2727272727272727, 0.1724137931034483, 0.16666666666666666, 0.20833333333333334, 0.0, 0.07407407407407407, 0.0, 0.12, 0.12244897959183673, 0.1276595744680851, 0.1724137931034483, 0.125, 0.140625, 0.19298245614035087, 0.0625, 0.06896551724137931, 0.4666666666666667, 0.4090909090909091, 0.3333333333333333, 0.1, 0.20754716981132076, 0.23076923076923078, 0.15789473684210525, 0.20833333333333334, 0.17857142857142858, 0.21621621621621623, 0.2564102564102564, 0.3181818181818182, 0.39473684210526316, 0.18181818181818182, 0.09523809523809523, 0.2692307692307692, 0.25925925925925924, 0.25925925925925924, 0.18181818181818182, 0.16981132075471697, 0.2535211267605634, 0.09090909090909091, 0.21621621621621623, 0.0, 0.058823529411764705, 0.18181818181818182, 0.0967741935483871, 0.16279069767441862, 0.14285714285714285, 0.21052631578947367, 0.15625, 0.17105263157894737, 0.25, 0.10526315789473684, 0.21739130434782608, 0.16, 0.15384615384615385, 0.09090909090909091, 0.21212121212121213, 0.15789473684210525, 0.2222222222222222, 0.13043478260869565, 0.12903225806451613, 0.2549019607843137, 0.16666666666666666, 0.23529411764705882, 0.4444444444444444, 0.046875, 0.09166666666666666, 0.17073170731707318, 0.07017543859649122, 0.2, 0.18032786885245902, 0.2564102564102564, 0.36363636363636365, 0.12903225806451613, 0.06818181818181818, 0.4375, 0.2631578947368421, 0.3333333333333333, 0.14, 0.26666666666666666, 0.16, 0.058823529411764705, 0.125, 0.1, 0.13636363636363635, 0.15625, 0.17647058823529413, 0.3333333333333333, 0.25, 0.09090909090909091, 0.3387096774193548, 0.24, 0.15384615384615385, 0.36363636363636365, 0.16216216216216217, 0.06896551724137931, 0.14285714285714285, 0.13953488372093023, 0.0, 0.15384615384615385, 0.09259259259259259, 0.09302325581395349, 0.2, 0.14285714285714285, 0.043478260869565216, 0.06060606060606061, 0.0, 0.1111111111111111, 0.08333333333333333, 0.0, 0.15789473684210525, 0.16216216216216217, 0.3333333333333333, 0.25806451612903225, 0.08333333333333333, 0.21428571428571427, 0.125, 0.0, 0.037037037037037035, 0.125, 0.0, 0.16, 0.14285714285714285, 0.275, 0.0, 0.038461538461538464, 0.16666666666666666, 0.125, 0.2222222222222222, 0.30434782608695654, 0.07142857142857142, 0.375, 0.16666666666666666, 0.0, 0.3, 0.13333333333333333, 0.2, 0.25, 0.26744186046511625, 0.13043478260869565, 0.2727272727272727, 0.4, 0.30303030303030304, 0.2, 0.32142857142857145, 0.21568627450980393, 0.21428571428571427, 0.08108108108108109, 0.13636363636363635, 0.07317073170731707, 0.19047619047619047, 0.07894736842105263, 0.11428571428571428, 0.13636363636363635, 0.1875, 0.058823529411764705, 0.10526315789473684, 0.23809523809523808, 0.21212121212121213, 0.07692307692307693, 0.05555555555555555, 0.21875, 0.0, 0.25, 0.09090909090909091, 0.21428571428571427, 0.275, 0.07894736842105263, 0.22580645161290322, 0.22641509433962265, 0.19607843137254902, 0.2857142857142857, 0.06666666666666667, 0.2727272727272727, 0.16216216216216217, 0.23076923076923078, 0.14285714285714285, 0.38461538461538464, 0.0, 0.2222222222222222, 0.125, 0.25, 0.23529411764705882, 0.23076923076923078, 0.1111111111111111, 0.21428571428571427, 0.08333333333333333, 0.08333333333333333, 0.1935483870967742, 0.1111111111111111, 0.1, 0.1724137931034483, 0.08333333333333333, 0.12903225806451613, 0.4, 0.16981132075471697, 0.17073170731707318, 0.05555555555555555, 0.3, 0.2413793103448276, 0.0, 0.15, 0.12121212121212122, 0.2222222222222222, 0.25, 0.125, 0.07407407407407407, 0.1388888888888889, 0.03225806451612903, 0.10526315789473684, 0.16666666666666666, 0.17391304347826086, 0.5, 0.2, 0.2222222222222222, 0.1, 0.07692307692307693, 0.05, 0.1095890410958904, 0.30434782608695654, 0.15, 0.05, 0.23809523809523808, 0.12307692307692308, 0.3191489361702128, 0.09090909090909091, 0.18181818181818182, 0.24, 0.3157894736842105, 0.0, 0.3333333333333333, 0.21212121212121213, 0.1, 0.07692307692307693, 0.22727272727272727, 0.2413793103448276, 0.1, 0.2222222222222222, 0.1875, 0.21428571428571427, 0.058823529411764705, 0.17647058823529413, 0.34615384615384615, 0.375, 0.15384615384615385, 0.21052631578947367, 0.16666666666666666, 0.24, 0.0625, 0.3793103448275862, 0.2222222222222222, 0.024390243902439025, 0.23076923076923078, 0.23076923076923078, 0.13333333333333333, 0.044444444444444446, 0.21428571428571427, 0.15625, 0.12903225806451613, 0.08333333333333333, 0.08, 0.17647058823529413, 0.0, 0.125, 0.14814814814814814, 0.15384615384615385, 0.15625, 0.10256410256410256, 0.08333333333333333, 0.24242424242424243, 0.22727272727272727, 0.10526315789473684, 0.1875, 0.06666666666666667, 0.25, 0.07142857142857142, 0.20689655172413793, 0.18181818181818182, 0.07142857142857142, 0.1111111111111111, 0.10714285714285714, 0.21052631578947367, 0.05, 0.375, 0.2, 0.11363636363636363, 0.15151515151515152, 0.21052631578947367, 0.1724137931034483, 0.32, 0.07142857142857142, 0.15384615384615385, 0.23529411764705882, 0.1, 0.13953488372093023, 0.16666666666666666], 'fmeasure': [0.03076923076923077, 0.02531645569620253, 0.0, 0.11940298507462686, 0.21333333333333335, 0.12307692307692307, 0.17777777777777778, 0.0, 0.18749999999999997, 0.08823529411764705, 0.1095890410958904, 0.20895522388059704, 0.09090909090909091, 0.16666666666666666, 0.0, 0.05333333333333334, 0.17500000000000002, 0.11320754716981132, 0.18918918918918917, 0.060606060606060615, 0.058823529411764705, 0.052631578947368425, 0.07142857142857144, 0.0, 0.11428571428571428, 0.07317073170731707, 0.16, 0.1408450704225352, 0.14285714285714285, 0.0784313725490196, 0.16666666666666666, 0.208955223880597, 0.08955223880597016, 0.10126582278481013, 0.11904761904761903, 0.07407407407407407, 0.07692307692307693, 0.12000000000000001, 0.1584158415841584, 0.15000000000000002, 0.08695652173913043, 0.044444444444444446, 0.13999999999999999, 0.23157894736842108, 0.07317073170731708, 0.13559322033898305, 0.05194805194805195, 0.1647058823529412, 0.0821917808219178, 0.1492537313432836, 0.10526315789473684, 0.06153846153846155, 0.25396825396825395, 0.136986301369863, 0.045454545454545456, 0.18181818181818182, 0.08108108108108107, 0.13114754098360656, 0.031746031746031744, 0.1111111111111111, 0.07228915662650602, 0.0, 0.10256410256410256, 0.1090909090909091, 0.1964285714285714, 0.15384615384615383, 0.07894736842105264, 0.14117647058823532, 0.0, 0.12903225806451615, 0.10752688172043011, 0.03921568627450981, 0.10526315789473684, 0.09523809523809523, 0.0975609756097561, 0.041666666666666664, 0.10126582278481013, 0.18390804597701152, 0.08247422680412371, 0.11382113821138212, 0.12048192771084339, 0.07407407407407407, 0.0888888888888889, 0.0821917808219178, 0.052631578947368425, 0.16, 0.29729729729729726, 0.12987012987012986, 0.20588235294117646, 0.169811320754717, 0.07894736842105263, 0.08, 0.08450704225352113, 0.042105263157894736, 0.0, 0.14492753623188406, 0.10416666666666667, 0.12121212121212123, 0.09090909090909093, 0.09999999999999999, 0.2178217821782178, 0.125, 0.1846153846153846, 0.10714285714285714, 0.0, 0.12307692307692308, 0.1590909090909091, 0.1518987341772152, 0.13114754098360656, 0.02702702702702703, 0.05405405405405405, 0.07920792079207921, 0.06779661016949153, 0.176, 0.034782608695652174, 0.1941747572815534, 0.12643678160919541, 0.1839080459770115, 0.06741573033707865, 0.09302325581395349, 0.15625, 0.08, 0.16260162601626016, 0.09090909090909093, 0.1728395061728395, 0.15584415584415584, 0.16326530612244897, 0.12307692307692308, 0.16666666666666666, 0.2222222222222222, 0.047058823529411764, 0.09523809523809523, 0.15625, 0.2222222222222222, 0.034482758620689655, 0.10126582278481013, 0.07142857142857142, 0.09803921568627452, 0.1714285714285714, 0.2857142857142857, 0.13157894736842105, 0.06976744186046512, 0.03389830508474576, 0.08450704225352113, 0.024999999999999998, 0.05555555555555555, 0.14035087719298245, 0.13157894736842105, 0.0821917808219178, 0.04081632653061224, 0.05825242718446602, 0.12820512820512822, 0.03174603174603175, 0.049999999999999996, 0.03703703703703704, 0.08333333333333333, 0.025974025974025976, 0.029411764705882353, 0.0, 0.0449438202247191, 0.10909090909090909, 0.14634146341463414, 0.06779661016949153, 0.125, 0.06349206349206349, 0.125, 0.07547169811320754, 0.10309278350515463, 0.07058823529411765, 0.20253164556962025, 0.0816326530612245, 0.075, 0.12048192771084339, 0.14545454545454545, 0.03571428571428572, 0.2028985507246377, 0.0, 0.15384615384615383, 0.06451612903225806, 0.05970149253731344, 0.17777777777777776, 0.0, 0.11111111111111112, 0.05194805194805195, 0.028169014084507043, 0.125, 0.0, 0.17391304347826086, 0.0392156862745098, 0.1348314606741573, 0.17391304347826086, 0.10204081632653061, 0.06976744186046513, 0.14285714285714285, 0.20408163265306123, 0.0851063829787234, 0.07142857142857141, 0.05, 0.06896551724137931, 0.08333333333333333, 0.0821917808219178, 0.08333333333333333, 0.19130434782608696, 0.03389830508474576, 0.1276595744680851, 0.15584415584415584, 0.05405405405405404, 0.05405405405405406, 0.15384615384615388, 0.058252427184466014, 0.09259259259259259, 0.05333333333333333, 0.09523809523809522, 0.1935483870967742, 0.07692307692307693, 0.11764705882352942, 0.0, 0.05714285714285715, 0.14457831325301204, 0.15151515151515152, 0.0851063829787234, 0.08196721311475412, 0.0, 0.05970149253731344, 0.0, 0.1188118811881188, 0.11764705882352941, 0.1348314606741573, 0.09900990099009903, 0.125, 0.16216216216216214, 0.16296296296296295, 0.07317073170731707, 0.06399999999999999, 0.31111111111111117, 0.2181818181818182, 0.22641509433962265, 0.12371134020618559, 0.22448979591836737, 0.20454545454545456, 0.18604651162790695, 0.15873015873015872, 0.12820512820512822, 0.15841584158415842, 0.15267175572519084, 0.2434782608695652, 0.21739130434782605, 0.04819277108433734, 0.043010752688172046, 0.14285714285714288, 0.21212121212121213, 0.14141414141414144, 0.2222222222222222, 0.1176470588235294, 0.3076923076923077, 0.08450704225352113, 0.16666666666666669, 0.0, 0.035714285714285705, 0.07619047619047621, 0.14423076923076922, 0.13333333333333333, 0.125, 0.1081081081081081, 0.08333333333333333, 0.23008849557522124, 0.11764705882352941, 0.08333333333333333, 0.16129032258064516, 0.13559322033898305, 0.08695652173913043, 0.08163265306122448, 0.22222222222222224, 0.08695652173913043, 0.06153846153846154, 0.1714285714285714, 0.07692307692307691, 0.23423423423423423, 0.15625, 0.2191780821917808, 0.27272727272727276, 0.05714285714285714, 0.13414634146341461, 0.17500000000000002, 0.0761904761904762, 0.07692307692307693, 0.1437908496732026, 0.19801980198019803, 0.23188405797101447, 0.07207207207207207, 0.0689655172413793, 0.175, 0.1282051282051282, 0.1111111111111111, 0.1794871794871795, 0.125, 0.1142857142857143, 0.030303030303030304, 0.03389830508474576, 0.02702702702702703, 0.08333333333333333, 0.18181818181818182, 0.09677419354838708, 0.13636363636363638, 0.1388888888888889, 0.03773584905660377, 0.34146341463414637, 0.14634146341463414, 0.11594202898550725, 0.09523809523809523, 0.16901408450704225, 0.06666666666666667, 0.07142857142857142, 0.14814814814814817, 0.0, 0.0851063829787234, 0.11494252873563218, 0.09638554216867469, 0.2058823529411765, 0.09523809523809523, 0.04081632653061224, 0.05063291139240506, 0.0, 0.13333333333333333, 0.059701492537313425, 0.0, 0.07792207792207792, 0.12371134020618559, 0.11538461538461539, 0.08040201005025124, 0.05, 0.15789473684210525, 0.08888888888888889, 0.0, 0.047619047619047616, 0.03448275862068966, 0.0, 0.1095890410958904, 0.11940298507462686, 0.21568627450980393, 0.0, 0.029850746268656723, 0.03508771929824561, 0.03773584905660377, 0.0816326530612245, 0.12844036697247707, 0.031746031746031744, 0.15789473684210528, 0.10958904109589039, 0.0, 0.04054054054054054, 0.1111111111111111, 0.16666666666666666, 0.13793103448275862, 0.26744186046511625, 0.09375, 0.1212121212121212, 0.1188118811881188, 0.24096385542168675, 0.0967741935483871, 0.16216216216216217, 0.16793893129770993, 0.23529411764705882, 0.07228915662650602, 0.15, 0.0857142857142857, 0.1391304347826087, 0.06818181818181818, 0.13793103448275862, 0.0909090909090909, 0.12765957446808507, 0.06976744186046513, 0.057971014492753624, 0.18867924528301888, 0.18666666666666665, 0.03389830508474576, 0.019417475728155338, 0.13461538461538464, 0.0, 0.06779661016949153, 0.09876543209876544, 0.06593406593406594, 0.2268041237113402, 0.06060606060606061, 0.1818181818181818, 0.21052631578947367, 0.15873015873015875, 0.24489795918367344, 0.05, 0.11111111111111109, 0.14634146341463417, 0.11764705882352941, 0.08450704225352111, 0.09615384615384616, 0.0, 0.05714285714285715, 0.05128205128205128, 0.10869565217391305, 0.06896551724137932, 0.09523809523809523, 0.08571428571428572, 0.07500000000000001, 0.05405405405405406, 0.037735849056603765, 0.07547169811320754, 0.08695652173913043, 0.03225806451612903, 0.19999999999999998, 0.06779661016949153, 0.09523809523809525, 0.16666666666666666, 0.15789473684210525, 0.16091954022988506, 0.02531645569620253, 0.12631578947368421, 0.16470588235294117, 0.0, 0.14457831325301204, 0.10256410256410256, 0.047058823529411764, 0.16216216216216217, 0.020202020202020204, 0.04819277108433735, 0.10309278350515463, 0.03225806451612903, 0.04395604395604395, 0.03603603603603604, 0.1095890410958904, 0.15686274509803924, 0.052631578947368425, 0.11764705882352941, 0.03921568627450981, 0.014184397163120567, 0.03225806451612903, 0.128, 0.2153846153846154, 0.10909090909090909, 0.027397260273972605, 0.14285714285714285, 0.12307692307692308, 0.2542372881355932, 0.09375000000000001, 0.15584415584415584, 0.22222222222222224, 0.12499999999999999, 0.0, 0.16666666666666666, 0.14285714285714288, 0.06666666666666667, 0.041666666666666664, 0.15151515151515152, 0.14432989690721648, 0.06349206349206349, 0.10810810810810811, 0.14285714285714285, 0.09677419354838708, 0.019047619047619046, 0.04838709677419355, 0.23076923076923078, 0.17142857142857143, 0.034482758620689655, 0.14678899082568805, 0.10666666666666666, 0.14634146341463414, 0.03225806451612903, 0.18803418803418803, 0.08, 0.028169014084507043, 0.13043478260869568, 0.08823529411764705, 0.11267605633802817, 0.05263157894736842, 0.1914893617021277, 0.17857142857142858, 0.10256410256410256, 0.037735849056603765, 0.08791208791208793, 0.08955223880597014, 0.0, 0.03333333333333334, 0.12698412698412698, 0.037037037037037035, 0.14084507042253522, 0.08, 0.08955223880597014, 0.1951219512195122, 0.07407407407407408, 0.05263157894736842, 0.09677419354838708, 0.015267175572519083, 0.125, 0.03773584905660377, 0.09022556390977444, 0.10666666666666667, 0.03508771929824561, 0.07547169811320754, 0.07058823529411765, 0.125, 0.03125000000000001, 0.14634146341463414, 0.12048192771084339, 0.13333333333333333, 0.1234567901234568, 0.16666666666666666, 0.1408450704225352, 0.15841584158415842, 0.06779661016949153, 0.07142857142857142, 0.14814814814814817, 0.030769230769230767, 0.125, 0.05263157894736842]}\n",
            "Average for precision is 0.09064777219049787\n",
            "Average for recall is 0.16427725080718214\n",
            "Average for fmeasure is 0.10744831480529154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "from statistics import mean\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'])\n",
        "results = {'precision': [], 'recall': [], 'fmeasure': []}\n",
        "for subdir, dirs, files in os.walk(cdir):\n",
        "  for file in files:\n",
        "    f = os.path.join(subdir, file)\n",
        "    print(f)\n",
        "    df = pd.read_csv(f, delimiter=',', encoding= \"utf-8\")\n",
        "    trueset=[]\n",
        "    for index, row in df.iterrows():\n",
        "      if (row[\"label2\"] == 1):\n",
        "        trueset.append(row[\"0\"])\n",
        "    print(trueset)\n",
        "    xtest = df[\"0\"].values\n",
        "    ytest = df[\"label2\"]\n",
        "    xtest2 = batch_encode(xtest, tokenizer)\n",
        "    y_pred = new_model.predict(xtest2.values(), batch_size=64, verbose=1)\n",
        "    row_index=[]\n",
        "    for i in enumerate(y_pred):\n",
        "      if i[1][1] >= 0.50:\n",
        "        row_index.append(i[0])\n",
        "    #print(row_index)\n",
        "    predsumm = []\n",
        "    for j in enumerate(xtest):\n",
        "      #print(j)\n",
        "      if j[0] in row_index:\n",
        "        predsumm.append(j[1])\n",
        "\n",
        "    # for each of the hypothesis and reference documents pair\n",
        "    for (h, r) in zip(trueset, predsumm):\n",
        "        # computing the ROUGE\n",
        "        score = scorer.score(h, r)\n",
        "        # separating the measurements\n",
        "        precision, recall, fmeasure = score['rouge1']\n",
        "        # add them to the proper list in the dictionary\n",
        "        results['precision'].append(precision)\n",
        "        results['recall'].append(recall)\n",
        "        results['fmeasure'].append(fmeasure)\n",
        "\n",
        "    print(results)\n",
        "    for st,vals in results.items():\n",
        "        print(\"Average for {} is {}\".format(st,mean(vals)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2XhirIPyMK6",
        "outputId": "c0da377f-511c-45be-dba9-3b2a24d2d543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/data/testdata/Black-Box Variational Inference for Stochastic Differential Equations.csv\n",
            "['A stochastic differential equation (SDE) defines a diffusion process, which evolves randomly over time, by describing its instantaneous behaviour.', 'As such, SDEs are powerful modelling tools used extensively in fields such as econometrics (Black & Scholes, 1973; Eraker, 2001), biology (Gillespie, 2000; Golightly & Wilkinson, 2011), physics (van Kampen, 2007) and epidemiology (Fuchs, 2013).\\n', 'It is only possible to work with analytic solutions to SDEs in special cases.', 'Therefore it is common to use a numerical approximation, such as the Euler-Maruyama scheme.', 'Here the diffusion process is defined only on a grid of time points, and the transition density between successive diffusion states is approximated as Gaussian.', 'The RNN learns how to supply Gaussian state transitions between successive time points which closely match those for the intractable conditioned diffusion process.\\n', 'In\\nthis case, (1) defines a diffusion process.', 'Such processes are always Markovian (i.e. memoryless).\\n', 'We further assume partial noisy observations of the latent process.', 'In the simplest case, these times are equally spaced, separated by a time-step of ∆t.', 'Upon choosing a prior density p(θ), Bayesian inference proceeds via the parameter posterior p(θ|y), or alternatively the joint posterior p(θ, x|y).\\n', 'This motivates us to use a variational approximation in which the diffusion matrix is not constrained to follow (9), and instead is allowed to shrink.', 'When variational inference outputs a good match to the posterior distribution, importance sampling (IS) (see e.g. Robert, 2004) can correct remaining inaccuracies and provide near-exact posterior inference.', 'To express x with a non-centred parameterisation, let 2 ∼ N(0, Ipm) be the flattened vector of (z1, z2, . . .', 'So the network just discussed forms a cell of an overall recurrent neural network (RNN) structure for q(x|θ;φx).', 'It can be interpreted as constraining the variational approximation based on prior beliefs about positivity of diffusion paths.', 'Exploratory work showed that the RNN produces a much better approximation of the conditioned process with these features as input rather than simply xτi , y, θ and τi.\\n', 'Many of these violate the assumptions used by existing diffusion bridge constructs (Whitaker et al., 2017b).\\n', 'Lotka-Volterra models describe simple predator-prey population dynamics combining three types of event: prey reproduction, predation (in which prey are consumed and predators have the resources to reproduce) and predator death.', 'Table 1 shows the resulting ESS values.', 'Convergence takes 2 hours, and importance sampling with 500,000 iterations produces an ESS of 635.4.', 'Constant population size is assumed.', 'Model comparison The two models produce visually similar diffusion paths, but close inspection shows some differences.', 'A better estimate of the parameter posteriors would allow formal model comparison based on importance sampling evidence estimates.', 'This performs inference for a broad class of SDEs with minimal tuning requirements.', 'Approximate parameter inference is also possible, with our results recovering known parameters for synthetic data (Section 5.1), and previous results for real data (Section 5.2), using only a few hours of computation for a desktop PC.', 'An interesting future direction is develop choices of q(x|θ;φx) more efficent than standard RNNs, to further reduce computing time and enable real-time applications of this methodology.']\n",
            "156/156 [==============================] - 116s 741ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-87ca4279206c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# separating the measurements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmeasure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rouge1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m# add them to the proper list in the dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'rouge1'"
          ]
        }
      ]
    }
  ]
}