0	32	Discourse relations (e.g., contrast and causality) support a set of sentences to form a coherent text.
1	55	Automatically recognizing discourse relations can help many downstream tasks such as question answering and automatic summarization.
2	34	Despite great progress in classifying explicit discourse relations where the discourse connectives (e.g., “because”, “but”) explicitly exist in the text, implicit discourse relation recognition remains a challenge due to the absence of discourse connectives.
4	30	To some extent, these methods simulate the single-pass reading process that a person quickly skim the text through one-pass reading and directly collect important clues for understanding the text.
5	32	Although single-pass reading plays a crucial role when we just want the general meaning and do not necessarily need to understand every single point of the text, it is not enough for tackling tasks that need a deep analysis of the text.
6	59	In contrast with single-pass reading, repeated reading involves the process where learners repeatedly read the text in detail with specific learning aims, and has the potential to improve readers’ reading fluency and comprehension of the text (National Institute of Child Health and Human Development, 2000; LaBerge and Samuels, 1974).
8	53	Now, let us check one real example to elaborate the necessity of using repeated reading in discourse parsing.
9	59	Arg-1 : the use of 900 toll numbers has been expanding rapidly in recent years Arg-2 : for a while, high-cost pornography lines and services that tempt children to dial (and redial) movie or music information earned the service a somewhat sleazy image (Comparison - wsj 2100) To identify the “Comparison” relation between 1224 the two arguments Arg-1 and Arg-2, the most crucial clues mainly lie in some content, like “expanding rapidly” in Arg-1 and “earned the service a somewhat sleazy image” in Arg-2, since there exists a contrast between the semantic meanings of these two text spans.
11	77	In such case, we follow the repeated reading strategy, where we obtain the general meaning through reading the arguments for the first time, re-read them later and gradually pay close attention to the key content.
12	20	Recently, some approaches simulating repeated reading have witnessed their success in different tasks.
13	19	These models mostly combine the attention mechanism that has been originally designed to solve the alignment problem in machine translation (Bahdanau et al., 2014) and the external memory which can be read and written when processing the objects (Sukhbaatar et al., 2015).
15	15	In computation vision, Yang et al. (2015) pointed out that repeatedly giving attention to different regions of an image could gradually lead to more precise image representations.
16	18	Inspired by these recent work, for discourse parsing, we propose a model that aims to repeatedly read an argument pair and gradually focus on more fine-grained parts after grasping the global information.
18	112	In the general level, we capture the general representations of each argument based on two bidirectional long short-term memory (LSTM) models.
28	19	On each attention level, an external short-term memory is used to store what has been learned from previous passes and guide which words should be focused on.
29	20	To pinpoint the useful parts of the arguments, the attention mechanism is used to predict a probability distribution over each word, indicating to what degree each word should be concerned.
72	18	Finally, we use the newest representation derived from the top attention level to recognize the discourse relations.
77	26	Also, we use different learning rates λ and λe to train the neural network parameters Θ and the word embeddings Θe referring to (Ji and Eisenstein, 2015).
93	21	To this end, we implement a baseline model (LSTM with no attention) which directly applies the mean pooling operation over LSTM output vectors of two arguments without any attention mechanism.
104	17	However, when adding the third attention level, the performance does not promote much and almost reaches its plateau.
113	25	• Zhang2015: Zhang et al. (2015) proposed to use shallow convolutional neural networks to model two arguments respectively.
124	37	As for the performance on each discourse relation, with respect to the F1 measure, we can see that our NNMA model can achieve the best results on the “Expansion”, “Expansion+EntRel” and “Temporal” relations and competitive results on the “Contingency” relation .
137	50	At the same time, we visualize the attention levels of some example argument pairs which are analyzed by the three-level NNMA.
138	45	To illustrate the kth attention level, we get its attention weights a1k and a2k which reflect the contribution of each word and then depict them by a row of color-shaded grids in Figure 2.
139	34	We can see that the NNMA model focuses on different words on different attention levels.
141	129	It seems that NNMA tries to find some clues (e.g. “moscow could be suspended” in Arg-2a; “won the business” in Arg-1b; “with great aplomb he considers not only” in Arg-2c) for recognizing the discourse relation on the 1st level, looking closely at other words (e.g. “misuse of psychiatry against dissenters” in Arg-2a; “a third party that” in Arg-1b; “and support of hitler” in Arg-2c) on the 2nd level, and then reconsider the arguments, focus on some specific words (e.g. “moscow could be suspended” in Arg-2a; “has not only hurt” in Arg-2b) and make the final decision on the last level.
168	67	To this end, we for the first time propose to imitate the repeated reading strategy and dynamically exploit efficient features through several passes of reading.
169	57	Following this idea, we design neural networks with multiple levels of attention (NNMA), where the general level and the attention levels represent the first and subsequent passes of reading.
170	25	With the help of external short-term memories, NNMA can gradually update the arguments representations on each attention level and fix attention on some specific words which provide effective clues to discourse relation recognition.
171	78	We conducted experiments on PDTB and the evaluation results show that our model can achieve the state-of-the-art performance on recognizing the implicit discourse relations.
