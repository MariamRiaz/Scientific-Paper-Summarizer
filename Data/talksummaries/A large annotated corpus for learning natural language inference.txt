6	33	Our ultimate objective is to provide an empirical evaluation of learning-centered approaches to NLI, advancing the case for NLI as a tool for the evaluation of domain-general approaches to semantic representation.
9	41	To address this, this paper introduces the Stanford Natural Language Inference (SNLI) corpus, a collection of sentence pairs labeled for entailment, contradiction, and semantic independence.
14	31	In this paper, we use this corpus to evaluate 632 a variety of models for natural language inference, including rule-based systems, simple linear classifiers, and neural network-based models.
16	65	We further evaluate the LSTM model by taking advantage of its ready support for transfer learning, and show that it can be adapted to an existing NLI challenge task, yielding the best reported performance by a neural network model and approaching the overall state of the art.
17	134	To date, the primary sources of annotated NLI corpora have been the Recognizing Textual Entailment (RTE) challenge tasks.1 These are generally high-quality, hand-labeled data sets, and they have stimulated innovative logical and statistical models of natural language reasoning, but their small size (fewer than a thousand examples each) limits their utility as a testbed for learned distributed representations.
18	65	The data for the SemEval 2014 task called Sentences Involving Compositional Knowledge (SICK) is a step up in terms of size, but only to 4,500 training examples, and its partly automatic construction introduced some spurious patterns into the data (Marelli et al. 2014a, §6).
19	60	The Denotation Graph entailment set (Young et al., 2014) contains millions of examples of entailments between sentences and artificially constructed short phrases, but it was labeled using fully automatic methods, and is noisy enough that it is probably suitable only as a source of supplementary training data.
20	38	Outside the domain of sentence-level entailment, Levy et al. (2014) introduce a large corpus of semi-automatically annotated entailment examples between subject–verb– object relation triples, and the second release of the Paraphrase Database (Pavlick et al., 2015) includes automatically generated entailment annotations over a large corpus of pairs of words and short phrases.
22	61	For an example of the pitfalls surrounding entity coreference, consider the sentence pair A boat sank in the Pacific Ocean and A boat sank in the Atlantic Ocean.
23	134	The pair could be labeled as a contradiction if one assumes that the two sentences refer to the same single event, but could also be reasonably labeled as neutral if that assumption is not made.
25	77	If we opt not to assume that events are coreferent, then we will only ever find contradictions between sentences that make broad universal assertions, but if we opt to assume coreference, new counterintuitive predictions emerge.
26	39	For example, Ruth Bader Ginsburg was appointed to the US Supreme Court and I had a sandwich for lunch today would unintuitively be labeled as a contradiction, rather than neutral, under this assumption.
29	151	This kind of indeterminacy of label can be resolved only once the questions of coreference are resolved.
33	26	Third, a subset of the resulting sentences were sent to a validation task aimed at providing a highly reliable set of annotations over the same data, and at identifying areas of inferential uncertainty.
34	26	We used Amazon Mechanical Turk for data collection.
37	112	Below the instructions were three fields for each of three requested sentences, corresponding to our entailment, neutral, and contradiction labels, a fourth field (marked optional) for reporting problems, and a link to an FAQ page.
39	50	It warned about disallowed techniques (e.g., reusing the same sentence for many different prompts, which we saw in a few cases), provided guidance concerning sentence length and 2 Issues of coreference are not completely solved, but greatly mitigated.
40	261	For example, with the premise sentence A dog is lying in the grass, a worker could safely assume that the dog is the most prominent thing in the photo, and very likely the only dog, and build contradicting sentences assuming reference to the same dog.
42	21	About 2,500 workers contributed.
46	34	We observed that while premise sentences varied considerably in length, hypothesis sentences tended to be as 3 We additionally include about 4k sentence pairs from a pilot study in which the premise sentences were instead drawn from the VisualGenome corpus (under construction; visualgenome.org).
52	49	In order to measure the quality of our corpus, and in order to construct maximally useful testing and development sets, we performed an additional round of validation for about 10% of our data.
53	29	This validation phase followed the same basic form as the Mechanical Turk labeling task used to label the SICK entailment data: we presented workers with pairs of sentences in batches of five, and asked them to choose a single label for each pair.
58	28	If any one of the three labels was chosen by at least three of the five annotators, it was LHS RHS 0 0 0 1 1 39 1 2 42 1011 1/2/00 3 156 7980 3 4 1095 29471 4 5 3882 61196 5 6 12120 74094 6 7 26514 93600 7 8 37434 85851 8 9 44028 61359 9 10 49245 46711 10 11 50919 33241 11 12 48363 22844 12 13 43314 15994 13 14 38121 11047 14 15 33183 7601 5 16 27621 5312 16 17 23250 3732 17 18 20247 2631 18 19 18513 1878 19 20 16386 1325 20 21 13746 911 21 22 12066 642 22 23 9183 449 23 24 7131 357 24 25 6198 217 25 26 5007 168 26 27 3963 138 27 28 3438 84 28 29 2631 67 29 30 1959 46 30 31 1956 26 31 32 1434 31 32 33 1086 23 33 34 912 16 34 35 897 19 35 36 774 8 36 37 453 12 37 38 618 4 38 39 291 5 39 40 330 2 40 41 249 4 41 42 180 2 42 43 225 1 43 44 162 1 44 45 108 1 48 46 87 1 51 47 60 2 55 48 36 1 56 49 90 1 60 50 21 1 62 51 66 52 51 53 36 54 24 55 63 56 18 57 15 58 6 59 27 60 6 61 3 62 3 63 3 64 6 65 3 66 3 67 6 68 6 69 18 70 15 71 3 72 15 73 3 75 15 79 3 82 15 0 10,000 20,000 30,000 40,000 50,000 60,000 70,000 80,000 90,000 100,000 0 5 10 15 20 25 30 35 40 N um be r of se nt en ce s Sentence length (tokens) Premise Hypothesis Figure 2: The distribution of sentence length.
60	22	While these unlabeled examples are included in the corpus distribution, they are unlikely to be helpful for the standard NLI classification task, and we do not include them in either training or evaluation in the experiments that we discuss in this paper.
76	47	In par- ticular, since it is dramatically larger than any existing corpus of comparable quality, we expect it to be suitable for training parameter-rich models like neural networks, which have not previously been competitive at this task.
78	135	In this section, we explore the performance of three classes of models which could scale readily: (i) models from a well-known NLI system, the Excitement Open Platform; (ii) variants of a strong but simple feature-based classifier model, which makes use of both unlexicalized and lexicalized features, and (iii) distributed representation models, including a baseline model and neural network sequence models.
79	49	The first class of models is from the Excitement Open Platform (EOP, Padó et al. 2014; Magnini et al. 2014)—an open source platform for RTE research.
99	28	Cross-bigrams: for every pair of bigrams across the premise and hypothesis which share a POS tag on the second word, an indicator feature over the two bigrams.
108	20	To focus specifically on the strengths of these models at producing informative sentence representations, we use sentence embedding as an intermediate step in the NLI classification task: each model must produce a vector representation of each of the two sentences without using any context from the other sentence, and the two resulting vectors are then passed to a neural network classifier which predicts the label for the pair.
110	23	Our neural network classifier, depicted in Figure 3 (and based on a one-layer model in Bowman et al. 2015), is simply a stack of three 200d tanh layers, with the bottom layer taking the concatenated sentence representations as input and the top layer feeding a softmax classifier, all trained jointly with the sentence embedding model itself.
