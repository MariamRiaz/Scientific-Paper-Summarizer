0	31	Collecting a large amount of labeled data is a critical bottleneck in real-world machine learning applications due to the laborious manual annotation.
1	34	In contrast, unlabeled data can often be collected automatically and abundantly, e.g., by a web crawler.
2	12	This has led to the development of various semi-supervised classification algorithms over the past decades.
3	10	To leverage unlabeled data in training, most of the existing semi-supervised classification methods rely on particular assumptions on the data distribution (Chapelle et al., 2006).
4	79	For example, themanifold assumption supposes that samples are distributed on a low-dimensional manifold in the data space (Belkin et al., 2006).
5	57	In the existing framework, such a distributional assumption is encoded as a reg- ularizer for training a classifier and biases the classifier toward a better one under the assumption.
6	25	However, if such a distributional assumption contradicts the data distribution, the bias behaves adversely, and the performance of the obtained classifier becomes worse than the one obtained with supervised classification (Cozman et al., 2003; Sokolovska et al., 2008; Li & Zhou, 2015; Krijthe & Loog, 2017).
7	41	Recently, classification from positive and unlabeled data (PU classification) has been gathering growing attention (Elkan & Noto, 2008; du Plessis et al., 2014; 2015; Jain et al., 2016), which trains a classifier only from positive and unlabeled data without negative data.
8	10	In PU classification, the unbiased risk estimators proposed in du Plessis et al. (2014; 2015) utilize unlabeled data for risk evaluation, implying that label information is directly extracted from unlabeled data without restrictive distributional assumptions, unlike existing semi-supervised classification methods that utilize unlabeled data for regularization.
9	25	Furthermore, theoretical analysis (Niu et al., 2016) showed that PU classification (or its counterpart, NU classification, classification from negative and unlabeled data) is likely to outperform classification from positive and negative data (PN classification, i.e., ordinary supervised classification) depending on the number of positive, negative, and unlabeled samples.
11	122	In this paper, we propose a novel semi-supervised classification approach by considering convex combinations of the risk functions of PN, PU, and NU classification.
12	158	Without any distributional assumption, we theoretically show that the confidence term of the generalization error bounds decreases at the optimal parametric rate with respect to the number of positive, negative, and unlabeled samples, and the variance of the proposed risk estimator is almost always smaller than the plain PN risk function given an infinite number of unlabeled samples.
13	93	Through experiments, we analyze the behavior of the proposed approach and demonstrate the usefulness of the proposed semi-supervised classification methods.
14	37	In this section, we first introduce the notation commonly used in this paper and review the formulations of PN, PU, and NU classification.
15	71	Let random variables x ∈ Rd and y ∈ {+1,−1} be equipped with probability density p(x, y), where d is a positive integer.
16	36	Let us consider a binary classification problem from x to y, given three sets of samples called the positive (P), negative (N), and unlabeled (U) data: XP := {xPi }nPi=1 i.i.d.∼ pP(x) := p(x | y = +1), XN := {xNi }nNi=1 i.i.d.∼ pN(x) := p(x | y = −1), XU := {xUi }nUi=1 i.i.d.∼ p(x) := θPpP(x) + θNpN(x), where θP := p(y = +1), θN := p(y = −1) are the class-prior probabilities for the positive and negative classes such that θP + θN = 1.
21	57	In standard supervised classification (PN classification), we have both positive and negative data, i.e., fully labeled data.
23	59	The risk in PN classification (the PN risk) is defined as RPN(g) := θP EP[ℓ(g(x))] + θN EN[ℓ(−g(x))] = θPRP(g) + θNRN(g), (1) which is equal to R(g), but p(x, y) is not included.
24	12	If we use the hinge loss function ℓH(m) := max(0, 1 −m), the PN risk coincides with the risk of the support vector machine (Vapnik, 1995).
25	38	In PU classification, we do not have labeled data for the negative class, but we can use unlabeled data drawn from marginal density p(x).
26	89	The goal of PU classification is to train a classifier using only positive and unlabeled data.
27	36	The basic approach to PU classification is to discriminate P and U data (Elkan & Noto, 2008).
