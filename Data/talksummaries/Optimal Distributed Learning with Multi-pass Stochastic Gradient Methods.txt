1	21	The aim is to learn a function which can be used to predict future outputs given the corresponding inputs.
2	24	The quality of a predictor is often measured in terms of the mean-squared error.
5	21	Nonparametric approaches, which can adapt their complexity to the problem at hand, are key to good results.
7	50	It is based on choosing a RKHS as the hypothesis space in the design of learning algorithms.
8	48	With an appropriate reproducing kernel, RKHS can be used to approximate any smooth function.
9	42	1 Laboratory for Information and Inference Systems, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland.
10	10	The classical algorithms to perform learning task are regularized algorithms, such as KRR, kernel principal component regression (KPCR), and more generally, spectral regularization algorithms (SRA).
11	14	From the point of view of inverse problems, such approaches amount to solving an empirical, linear operator equation with the empirical covariance operator replaced by a regularized one (Engl et al., 1996; Bauer et al., 2007; Gerfo et al., 2008).
14	17	Another type of algorithms to perform learning tasks is based on iterative procedure (Engl et al., 1996).
15	39	In this kind of algorithms, an empirical objective function is optimized in an iterative way with no explicit constraint or penalization, and the regularization against overfitting is realized by early-stopping the empirical procedure.
16	61	Statistical results on generalization error and the regularization roles of the number of iterations/passes have been investigated in (Zhang & Yu, 2005; Yao et al., 2007) for gradient methods (GM, also known as Landweber algorithm in inverse problems), in (Caponnetto, 2006; Bauer et al., 2007) for accelerated gradient methods (AGM, known as ν-methods in inverse problems) in (Blanchard & Krämer, 2010) for conjugate gradient methods (CGM), and in (Lin & Rosasco, 2017b) for (multi-pass) SGM.
17	60	Statistical results have been well studied for these algorithms; however, these algorithms suffer from computational burdens at least of order O(N2) due to the nonlinearity of kernel methods, where N is the sample size.
18	106	Indeed, a standard execution of KRR requiresO(N2) in space andO(N3) in time, while SGM after T -iterations requires O(N) in space and O(NT ) (or T 2) in time.
19	3	Such approaches would be prohibitive when dealing with large-scale learning problems, especially in the case where data cannot be stored on a single machine.
20	55	These thus motivate one to study distributed learning algorithms (Mcdonald et al., 2009; Zhang et al., 2012).
21	98	The basic idea of distributed learning is very simple: randomly divide a dataset of size N into m subsets of equal size, compute an independent estimator using a fixed algorithm on each subset, and then average the local solutions into a global predictor.
22	40	Interestingly, distributed learning technique has been successfully combined with KRR (Zhang et al., 2015; Lin et al., 2017) and more generally, SRA (Guo et al., 2017; Blanchard & Mücke, 2016), and it has been shown that statistical results on generalization error can be retained provided that the number of partitioned subsets is not too large.
23	77	Moreover, it was highlighted (Zhang et al., 2015) that distributed KRR not only allows one to handle large datasets that restored on multiple machines, but also leads to a substantial reduction in computational complexity versus the standard approach of performing KRR on all N samples.
25	9	The algorithm is a combination of distributed learning technique and (multi-pass) SGM: it randomly partitions a dataset of size N into m subsets of equal size, computes an independent estimator by SGM for each subset, and then averages the local solutions into a global predictor.
29	15	For example, without considering any benign properties of the studied problem such as the regularity of the regression function (Smale & Zhou, 2007; Caponnetto & De Vito, 2007) and a capacity assumption on the RKHS (Zhang, 2005; Caponnetto & De Vito, 2007), even implementing on a single machine, distributed SGM has an optimal convergence rate of order O(N−1/2), with a computational complexity O(N) in space and O(N3/2) in time, compared with O(N) in space and O(N2) in time of classic SGM performing on all N samples, or O(N3/2) in space andO(N2) in time of distributed KRR.
31	34	The proof of the main results is based on a similar error decomposition from (Lin & Rosasco, 2017b), which decomposes the excess risk into three terms: bias, sample and computational variance.
32	120	The error decomposition allows one to study distributed GM and distributed SGM simultaneously.
33	45	Different to those in (Lin & Rosasco, 2017b) which rely heavily on the intrinsic relationship of GM with the square loss, in this paper, an integral operator approach (Smale & Zhou, 2007; Caponnetto & De Vito, 2007) is used, combining with some novel and refined analysis.
34	60	As a byproduct, we derive optimal statistical results on generalization error for nondistributed SGM, which improve on the results in (Lin & Rosasco, 2017b).
35	103	Note also that we can extend our analysis to distributed SRA, and get better statistical results than those from (Zhang et al., 2015; Guo et al., 2017).
