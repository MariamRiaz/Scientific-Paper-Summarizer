27	24	We present the experimental details and results in Section 6, and conclude the paper with discussions of future work in Section 7.
48	23	The problem with video frame prediction originates from modeling pixels directly in a sequence-to-sequence manner and attempting to generate frames in a recurrent fashion.
49	30	Current state-of-the-art approaches recurrently observe the predicted frames, which causes rapidly increasing error accumulation through time.
50	71	Our objective is to avoid having to observe generated future frames at all during the full video prediction procedure.
57	16	The proposed algorithm makes it possible to decompose the task of video frame prediction to sub-tasks of future highlevel structure prediction and structure-conditioned frame generation.
58	15	By doing so, we remove the recursive dependency of generated frames which have caused the compound errors of pixel-level prediction in previous methods, which allows us to perform very long-term video prediction.
91	60	Training our network to transform an input image into a target image that is too close in image space can lead to suboptimal parameters being learned due to the simplicity of such task that requires only changing a few pixels.
95	17	To train our network, we use the compound loss from Dosovitskiy & Brox (2016).
97	32	The image loss intuitively guides our network towards a rough blurry pixel-leven frame prediction that reflects most details of the target image.
103	16	is the discriminator network in adversarial loss.
106	47	The discriminator loss is defined by LDisc = − logD ([pt+n,xt+n]) − 0.5 log (1−D ([pt+n, x̂t+n])) − 0.5 log (1−D ([pt+n,xt])) , (9) while optimizing our generator with respect to the adversarial loss, the mismatch-aware term sends a stronger signal to our generator resulting in higher quality image generation, and network optimization.
129	17	The Penn Action dataset is composed of 2326 video sequences of 15 different actions and 13 human joint annotations for each sequence.
130	21	To train our image generator, we use the standard train split provided in the dataset.
133	27	Our pose predictor is trained to observe 10 inputs and predict 32 steps, and tested on predicting up to 64 steps (some videos’ groundtruth end before 64 steps).
134	26	Our image generator is trained to make single random jumps within 30 steps into the future.
141	30	The remainder of the human actions contain more complicated non-linear motion, which is much more complicated to predict.
144	47	To see whether the generated videos contain actions that can fool a CNN trained for action recognition, we train a Two-Stream CNN on the PennAction dataset.
150	15	Pixel-level evaluation and control experiments.
159	48	We evaluate on a single clip from each test video that starts at the exact middle of the video to make sure there is motion occurring.
160	33	We collected a total of 2203 comparisons (1086 against convolutional LSTM and 1117 against optical flow baseline) from 71 unique workers.
161	20	As shown in Table 3, the videos generated by our network are perceptually higher quality and reflect a reasonable future compared to the baselines on average.
162	96	Unexpectedly, our network does not perform well on videos where the action involves minimal motion, such as sitting, sitting down, eating, taking a photo, and waiting.
163	23	These actions usually involve the person staying still or making very unnoticeable motion which can result in a static prediction (by convolutional LSTM and/or optical flow) making frames look far more realistic than the prediction from our network.
170	167	The highest PSNR scores are again achieved when the exact future pose is used to generate the video frames; however, there is an even larger gap compared to the results in Section 6.1.
171	33	Due to space constraints, we ask the reader to please refer to the supplementary material for more detailed quantitative and qualitative analysis.
175	16	At the same time, an important open research question would be how to automatically learn such structures without domain knowledge.
176	21	Another limitation of this work is that it generates a single future trajectory.
177	53	For an agent to make a better estimation of what the future looks like, we would need more than one generated future.
178	17	Future work will involve the generation of many futures given using a probabilistic sequence model.
179	15	Finally, our model does not handle background motion.
