0	110	Discriminative language modeling (DLM) helps ASR systems to discriminate between acoustically similar word sequences in the process of choosing the most accurate transcription of an utterance.
1	51	DLM characterizes and learns from ASR errors by comparing the reference transcription of the utterance and the candidate hypotheses generated by the ASR system.
2	21	Although previous studies based on this supervised setting have been successful (Roark et al., 2007; Arısoy et al., 2009; Arısoy et al., 2012; Sak et al., 2012), they require large amounts of transcribed speech data and a well-trained in-domain ASR system, both of which are hard to obtain.
5	17	In this approach, first a confusion model (CM) is estimated from supervised data.
10	32	Although being able to simulate the output of the ASR system allows us to generate as much output as we need for the DLM training, there is not always enough text data that is in the same domain as the ASR system.
42	13	To generate the hypotheses, we follow the three-step finite state transducer based pipeline given in Çelebi et al. (2012) and summarized by the following composition sequence: sample(N -best(prune(W◦LW◦CM)◦LM–1◦GM)) In the first step of the pipeline, we use the confusion model transducer (CM) to generate all possible confusions that the ASR system can make for a given reference sentenceW .
43	14	We consider syllable, morph and word based confusion models, and convert W to these units using the lexicon LW .
45	13	As the output of the first step may include many implausible sequences, the second step converts them to morphs using LM–1 and reweights them with a morph-based language model GM to favor the meaningful sequences.
50	15	The third step, called sampling, involves picking a subset of the hypotheses from a larger set with broad variety.
51	21	This step is done in order to pick samples so as to make sure that they include error variety instead of just high scoring hypotheses.
60	12	As the learning algorithm, we apply the WER-sensitive perceptron algorithm proposed by Sak et al. (2011b), which has been shown to perform better for reranking ASR hypotheses as it minimizes an objective function based on the WER rather than the number of misclassifications.
61	22	We employ DLM on a Turkish broadcast news transcription data set (Arısoy et al., 2009), which comprises disjoint training (105356 sentences), held-out (1947 sentences) and test (1784 sentences) subsets consisting of ASR outputs represented as N -best lists.
63	28	For semi-supervised ex- periments, we use the first half of the training subset (t1: 53992 sentences, 965K morphs) to learn the confusion models, and the reference transcriptions of the second half (t2: 51364 sentences, 935K morphs) to generate in-domain simulated n-best lists to be compared against OOD simulated ones.
66	56	For OOD data, we use a data set of 10.8M sentences (140M morphs) from newspaper articles downloaded from the Internet (Sak et al., 2011a).
67	44	To calculate the perplexity of OOD sentences for selection, we use a language model trained over the reference transcripts and 50-best lists of t1 and t2.
68	24	We start our experiments with 500K randomly selected OOD sentences, or RAND-500K.
69	45	We run the simulation pipeline with four sampling methods, three confusion and three language models, giving 36 experiments in total.
70	24	We choose among the proposed sampling approaches and confusion models using a rank-based comparison as done by Dikici et al. (2012).
74	13	This shows that ASRdist-50 gives the best WER reduction on OOD data, which is also true for in-domain data (Çelebi et al., 2012).
75	30	Doing the same rank-based comparison for the CMs this time, we observe that the syllable and morph-based models have the same average rank of 1.5, whereas the word-based model has 2.8.
89	10	Note also the slightly high value of KL distance for t2, which can be attributed to the i pilog( pi 1/V ) = log(V ) − H(p), where V = 61294 and H(p) is the entropy of p. relatively low number of unique morphs (types).
90	41	In this section, we compare the results for in-domain data with the results for four OOD data sets in Table 3.
93	9	According to Table 3, even though 50K OOD sentences yield worse results than the same amount of in-domain sentences, as the size of OOD data set increases, the amount of WER reduction increases and surpasses the level obtained by using in-domain data.
96	24	Then we go one step further and expand the BOTTOM data set to 1M sentences and we observe WER of 22.1% on the held-out set.
102	14	When combined with the real hypotheses from t1, RAND500K achieves the same level of WER reduction as the simulated hypotheses from t2 on the heldout set.
107	18	Next, we combine the in-domain real hypotheses from t1, simulated hypotheses from t2 and simulated ones from the OOD data sets.
111	11	We observe that ASRdist-50 sampling method and syllable-based CMs yield the best results with the OOD data.
112	54	Moreover, selecting OOD sentences randomly rather than using perplexity-based methods is enough to achieve the best WER reduction.
113	59	We also observe that simulated hypotheses from the OOD data is almost as good as in-domain simulated hypotheses or even real ones.
114	12	As a future work, we will increase the size of the OOD data and examine other methods like relative entropy based OOD selection.
