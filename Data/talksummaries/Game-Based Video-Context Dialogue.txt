2	20	Current dialogue tasks are usually focused on the textual or verbal context (conversation history).
3	61	In terms of multimodal dialogue, speechbased spoken dialogue systems have been widely explored (Eckert et al., 1997; Singh et al., 2000; Young, 2000; Janin et al., 2003; Celikyilmaz et al., 2017; Wen et al., 2015; Su et al., 2016; Mrkšić et al., 2016), as well as work on gesture and haptics based dialogue (Johnston et al., 2002; Cassell, 1999; Foster et al., 2008).
4	19	In order to address the additional advantage of using visually-grounded context knowledge in dialogue, recent work introduced the visual dialogue task (Das et al., 2017; de Vries et al., 2017; Mostafazadeh et al., 2017).
5	23	However, the visual context in these tasks is lim- ited to one static image.
6	28	Moreover, the interactions are between two speakers with fixed roles (one asks questions and the other answers).
7	47	Several situations of real-world dialogue among humans involve more ‘dynamic’ visual context, i.e., video-style information of the world moving around us (both spatially and temporally).
10	24	Our dataset is based on live-broadcast soccer (FIFA18) game videos from the ‘Twitch.tv’ live video streaming platform, along with the spontaneous, many-speaker live chats about the game.
11	38	This challenging testbed allows us to develop dialogue models where the generated response is required to be relevant to the temporal and spatial events in the live video, as well as be relevant to the chat history (with potential impact towards videogrounded applications such as personal assistants, intelligent tutors, and human-robot collaboration).
39	21	First, we use 20-sec context windows to extract the video clips and users utterances in this time frame, and use it as our video and chat contexts, resp.
41	58	Now, out of these potential responses, to only allow the response that has at least some good coherence and relevance with the chat context’s topic, we choose the first (earliest) response that has high similarity with some other utterance in this response window (using 0.5 BLEU-4 threshold, based on manual inspection).2 Human Quality Evaluation of Data Filtering Process: To evaluate the quality of the responses that result from our filtering process described above, we performed an anonymous (randomly shuffled w/o identity) human comparison between the response selected by our filtering process vs. the first response from the response window without any filtering, based on relevance w.r.t.
44	39	In order to make the above procedure safe and to make the dataset more challenging, we also discourage frequent responses (top-20 most-frequent generic utterances) unless no other response satisfies the similarity condition, hence suppressing the frequent responses.3 If we couldn’t find any utterance based on the multi-response matching procedure described above, then we just consider the first utterance in the 10-second window as the response.4 We also make sure that the chat context window has at least 4 utterances, otherwise we exclude that context window and also the corresponding response window from the dataset.
45	20	After all this processing, our final resulting dataset contains 10, 510 samples in training, 2, 153 samples in validation, and 2, 780 samples in test.5
47	21	As shown, the average chat context length in the dataset is around 68 words, and the average response length is 6.3 words.
51	33	4 presents the top-20 frequent words (excluding stop words) and their corresponding frequency in our Twitch-FIFA dataset.
54	23	Let v = {v1, v2, .., vm} be the video context frames, u = {u1, u2, .., un} be the textual chat (utterance) context tokens, and r = {r1, r2, .., rk} be response tokens generated (or retrieved).
71	26	We use bidirectional attention flow mechanisms (Seo et al., 2017) between the video and chat contexts, between the video context and the response, as well as between the chat context and the response, hence enabling attention flow across all three modalities, as shown in Fig.
102	31	For our discriminative models, we simply rerank the given responses (in a candidate list of size 10, based on 9 negative examples; more details below) in the order of the probability score each response gets from the model.
126	42	Starting with a simple sequenceto-sequence attention model with video only, chat only, and both video and chat encoders, the recall@k scores are better than all the simple baselines.
131	47	significantly better than nonBiDAF model on both METEOR (p < 0.01) and ROUGE-L (p < 0.02) metrics.
133	28	Finally, we also perform human evaluation to compare our top two generative models, i.e., the video+chat seq2seq with attention and its extension with BiDAF (Sec.
134	23	4.3), based on a 100-sized sample.
136	73	We then ask two annotators (for 50 task instances each) to score the responses of these two models based on relevance.
137	23	Note that the human evaluators were familiar with Twitch FIFA-18 video games and also the Twitch’s unique set of chat mannerisms and emotes.
138	74	As shown in Table 5, our BiDAF based generative model performs better than the non-BiDAF one, which is already quite a strong video+chat encoder model with attention.
139	18	We also compare the effect of different negative training triples that we discussed in Sec.
140	50	Table 6 shows the comparison between one negative training triple (with just a negative response) vs. three negative training triples (one with negative video context, one with negative chat context, and another with negative response), showing that using the 3-negative examples setup is substantially better.
141	23	Table 7 shows the performance comparison between the classification loss and max-margin loss on our TriDAF with self-attention discriminative model (Sec.
142	31	We observe that max-margin loss performs better than the classification loss, which is intuitive because max-margin loss tries to differentiate between positive and negative training example triples.
145	32	Finally, we show some interesting output examples from both our discriminative and generative models as shown in Fig.
146	25	9 visualizes that our models can learn some correct attention alignments from the generated output response word to the appropriate (goal-related) video frames as well as chat context words.
