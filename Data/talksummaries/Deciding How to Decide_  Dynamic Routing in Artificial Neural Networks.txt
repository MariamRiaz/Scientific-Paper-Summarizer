1	12	Additionally, different difficult decisions may require different expertise—an avid birder may know very little about identifying cars.
4	90	Dynamic routing is also performed in the primate visual system: spatial information is processed somewhat separately from object identity information (Goodale & Milner, 1992), and faces and other behaviorally-relevant stimuli ellicit responses in anatomically distinct, specialized regions (Moeller et al., 2008; Kornblith et al., 2013).
6	15	With this in mind, we propose a mechanism for introducing cascaded evaluation to arbitrary feedforward ANNs, focusing on the task of object recognition as a proof of concept.
9	42	Additionally, we propose and evaluate methods for appropriating regularization and optimization techniques developed for statically-routed networks.
21	22	We compute dj as the argmax of the score vector sj , a learned function of the last feature vector computed before reaching j. We’ll refer to this rule for generating d from s as the inference routing policy.
25	11	When dj = 0, the signal is propagated through the top sink, and the bottom sink is inactive.
29	21	At every junction j, the score vector sj is computed by a small routing network operating on the last-computed global descriptor.
44	10	(3) In our experiments, the training routing policy samples d̂ such that Pr(d̂j = i) = softmax(sj/τ)i, (4) where τ is the network “temperature”: a scalar hyperparameter that decays over the course of training, converging the training routing policy towards the inference routing policy.
45	18	Alternatively, we can attempt to learn to predict the expected utility of making every routing decision.
53	10	To support both frequently- and infrequently-used layers, we regularize subnetworks as they are activated by d̂, instead of regularizing the entire network directly.
59	17	With this setup, if we use a constant learning rate for every layer in the network, then layers through which the policy routes examples more frequently will receive larger parameter updates, since they contribute more to the expected cost.
67	11	(16) So, for every layer `, we can scale the learning rate by ‖p`‖−1, and the variance of the weight updates will be similar thoughout the network.
75	15	The weights of the final layers of routing networks are zero-initialized, and we initialize all other weights using the Xavier initialization method (Glorot & Bengio, 2010).
81	15	We augment our data using an approach that is popular for use with CIFAR-10 (Lin et al., 2013) (Srivastava et al., 2015) (Clevert et al., 2015).
84	123	We compare approaches to dynamic routing by training 153 networks to classify small images, varying the policy-learning strategy, regularization strategy, optimization strategy, architecture, cost of computation, and details of the task.
85	11	The results of these experiments are reported in Fig.
86	10	Our code is available via GitLab.
88	82	Our dataset includes the classes “0”, “1”, “2”, “3”, and “4” from MNIST and “airplane”, “automobile”, “deer”, “horse”, and “frog” from CIFAR-10 (see Fig.
90	55	For a given computational budget, dynamically-routed networks achieve higher accuracy rates than architecturematched statically-routed baselines (networks composed of the first n columns of the architecture illustrated in Fig.
91	61	Additionally, dynamically-routed networks tend to avoid routing data along deep paths at the beginning of training (see Fig.
92	16	This is possibly because the error surfaces of deeper networks are more complicated, or because deeper paths are less stable—changing the parameters in any component layer to better classify images routed along other, overlapping paths may decrease performance.
95	57	Actor networks perform better than critic networks, possibly because critic networks are forced to learn a potentially-intractable auxilliary task (i.e. it’s easier to decide who to call to fix your printer than it is to predict exactly how quickly and effectively everyone you know would fix it).
96	20	Actor networks also consistently achieve higher peak accuracy rates than comparable statically-routed networks, across experiments.
97	21	4 8 40k 80k E p o ch In d ex kcpt = 0 4 8 kcpt = 1×10−9 4 8 kcpt = 2×10−9 4 8 kcpt = 4×10−9 0.0 0.2 0.4 0.6 0.8 1.0 Layer Index Figure 8.
98	18	Dataflow over the course of training.
99	23	The heatmaps illustrate the fraction of validation images classified at every terminal node in the bottom four networks in Fig.
101	20	Although actor networks may be more performant, critic networks are more flexible.
102	43	Since critic networks don’t require E[cinf(ν, d̂)] to be a differentiable function of d̂, they can be trained by sampling d̂, saving memory, and they support a wider selection of training routing policies (e.g. -greedy) and cinf definitions.
104	53	Although these networks do not perform as well as the original pragmatic critic networks, they still outperform comparable statically-routed networks.
105	22	Based on our experiments with the hybrid dataset, regularizing d̂, as described in section 4.4, discourages networks from routing data along deep paths, reducing peak accuracy.
