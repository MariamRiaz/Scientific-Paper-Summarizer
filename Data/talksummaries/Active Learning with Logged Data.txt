0	29	We consider learning a classifier from logged data.
3	11	An example is predicting the efficacy of a treatment as a function of patient characteristics based on observed data.
4	21	Doctors may assign the treatment to patients based on some predetermined rule; recording these patient outcomes produces a logged dataset where outcomes are observed conditioned on the doctors’ assignment.
5	19	A second example is recidivism prediction, where the goal is to predict whether a convict will re-offend.
6	9	Judges use their own predefined policy to grant parole, and if parole is granted, then an outcome (reoffense or not) is observed.
9	29	Consequently, empirical risk minimization (ERM) on the logged data leads to classifiers that may be highly suboptimal on the population.
12	8	A final approach, typically used in clinical trials, is controlled random experimentation – essentially, ignore the logged data, and record outcomes for fresh examples drawn from the population.
13	20	This approach is expensive due to the high cost of trials, and wasteful since it ignores the observed data.
25	15	We prove that our algorithm is statistically consistent, and has a lower label requirement than simple active learning that uses the logged data as a warm start.
28	11	This confirms that active learning to combine logged data with carefully chosen labeled data may indeed yield performance gains.
29	13	Instances are drawn from an instance space X and a label space Y = {0, 1}.
32	8	For simplicity, we assumeH is a finite set, but our results can be generalized to VC-classes by standard arguments (Vapnik & Chervonenkis, 1971).
34	8	The logged data are generated from m examples {(Xt, Yt)}mt=1 drawn i.i.d.
38	14	From the algorithm’s perspective, we assume it knows the logging policy Q0, and only observes instances {Xt}mt=1, decisions of the policy {Zt}mt=1, and revealed labels {Yt | Zt = 1}mt=1.
43	16	The goal of the algorithm is to learn a classifier h ∈ H from observed logged data and online data.
46	19	In this work, we are interested in the situation where n is about the same as or less than m.
47	29	Our algorithm is based on Disagreement-Based Active Learning (DBAL) which has rigorous theoretical guarantees and can be implemented practically (see (Hanneke et al., 2014) for a survey, and (Hanneke & Yang, 2015; Huang et al., 2015) for some recent developments).
52	6	For each instance X ∈ Tk, if it falls into the disagreement region Dk, then the algorithm queries for its label; otherwise, observing that all classifiers in Vk have the same prediction on X , its label is not queried.
53	12	The queried labels are then used to update future candidate sets.
75	6	In the classical active learning setting without logged data, standard impor- tance sampling can give satisfactory performance guarantees (Beygelzimer et al., 2009; 2010; Huang et al., 2015).
76	19	KEY IDEA 3: A DEBIASING QUERY STRATEGY The logging policy Q0 introduces bias into the logged data: some examples may be underrepresented since Q0 chooses to reveal their labels with lower probability.
86	15	Applying both methods requires that the query policy and consequently the importance weights in the error estimator are updated with observed examples in each iteration.
106	7	The algorithm uses T (0)0 to construct an initial candidate set, and uses Sk := T (k) 0 ∪ Tk in iteration k. Algorithm 1 uses the disagreement-based active learning framework.
109	8	It only queries for labels inside the disagreement region Dk+1.
112	7	The algorithm only observes T̃k and S̃k.
132	8	From Theorem 1 and 3 and some algebra, our algorithm requires Õ ( νθ̃ · (ν+ 2 log |H| δ −mξ̃K) ) labels.
143	6	We now empirically validate our theoretical results by comparing our algorithm with a few alternatives on several datasets and logging policies.
147	10	We consider the overall performance of our algorithm against two natural baselines: standard passive learning (PASSIVE) and the disagreement-based active learning algorithm with warm start (DBALW).
149	47	We do not compare with the standard disagreement-based active learning that ignores the logged data since the contribution of warm start is clear: it always results in a smaller initial candidate set, and thus leads to less label queries.
152	19	This algorithm only uses only our first key idea – warm start.
