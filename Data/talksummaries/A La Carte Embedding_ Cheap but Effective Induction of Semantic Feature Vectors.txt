5	50	We propose an alternative, novel solution via à la carte embedding, a method which bootstraps existing high-quality word vectors to learn a feature representation in the same semantic space via a linear transformation of the average word embeddings in the feature’s available contexts.
6	41	This can be seen as a shallow extension of the distributional hypothesis (Harris, 1954), “a feature is characterized by the words in its context,” rather than the computationally more-expensive “a feature is characterized by the features in its context” that has been used implicitly by past work (Rothe and Schütze, 2015; Logeswaran and Lee, 2018).
13	67	For word embeddings, the approach is an easy way to get a good vector for a new word from its definition or a few examples in context.
14	19	For feature embeddings, the method can embed anything that does not need labeling (such as a bigram) or occurs in an annotated corpus (such as a word-sense).
15	85	Our document embeddings, constructed directly using à la carte n-gram vectors, compete well with recent deep neural representations; this provides further evidence that simple methods can outperform modern deep learning on many NLP benchmarks (Arora et al., 2017; Mu and Viswanath, 2018; Arora et al., 2018a,b; Pagliardini et al., 2018).
29	56	We begin by assuming a large text corpus CV consisting of contexts c of words w in a vocabulary V , with the contexts themselves being sequences of words in V (e.g. a fixed-size window around the word or feature).
31	26	Our goal is to construct a good embedding vf ∈ Rd of a text feature f given a set Cf of contexts it occurs in.
35	20	A naive first approach to construct feature embeddings using context is additive, i.e. taking the average over all contexts of a feature f of the average word vector in each context: vadditivef = 1 |Cf | ∑ c∈Cf 1 |c| ∑ w∈c vw (1) This formulation reflects the training of commonly used embeddings, which employs additive composition to represent the context (Mikolov et al., 2013; Pennington et al., 2014).
36	32	It has proved successful in the bag-of-embeddings approach to sentence representation (Wieting et al., 2016; Arora et al., 2017), which can compete with LSTM representations, and has also been given theoretical justification as the maximum a posteriori (MAP) context vector under a generative model related to popular embedding objectives (Arora et al., 2016).
38	25	The additive approach has some limitations because the set of all word vectors is seen to share a few common directions.
42	63	We now note that removing the component along the top few principal directions is tantamount to multiplying the additive composition by a fixed (but data-dependent) matrix.
46	43	After learning the matrix, we can embed any text feature in the same semantic space as the word embeddings via the following expression: vf = Av additive f = A  1 |Cf | ∑ c∈Cf ∑ w∈c vw  (3) Note that A is fixed for a given corpus and set of pretrained word embeddings and so does not need to be re-computed to embed different features or feature types.
47	29	Algorithm 1: The basic à la carte feature embedding induction method.
49	104	Data: vocabulary V , corpus CV , vectors vw ∈ Rd ∀ w ∈ V , feature f , corpus Cf of contexts of f Result: feature embedding vf ∈ Rd 1 for w ∈ V do 2 let Cw ⊂ CV be the subcorpus of contexts of w 3 uw ← 1|Cw| ∑ c∈Cw ∑ w′∈c vw′ // compute each word’s context embedding uw 4 A← argmin A∈Rd×d ∑ w∈V ‖vw −Auw‖22 // compute context-to-feature transform A 5 uf ← 1|Cf | ∑ c∈Cf ∑ w∈c vw // compute feature’s context embedding uf 6 vf ← Auf // transform feature’s context embedding Theoretical Justification: As shown by Arora et al. (2018b, Theorem 1), the approximation (2) holds exactly in expectation for some matrix A when contexts c ∈ C are generated by sampling a context vector vc ∈ Rd from a zero-mean Gaussian with fixed covariance and drawing |c| words using P(w|vc) ∝ exp〈vc,vw〉.
52	90	We observe that the best linear transform A can recover vectors with mean cosine similarity as high as 0.9 or more with the embeddings used to learn it, thus also justifying the method empirically.
54	24	In practice we may wish to modify the regression step in an attempt to learn a better transformation matrix A.
56	19	A more useful modification is to weight each point by some non-decreasing function α of each word’s corpus count cw, i.e. to solve A = argmin A∈Rd×d ∑ w∈V α(cw)‖vw −Auw‖22 (4) where uw is the additive context embedding.
66	26	However, none of these datasets can be used directly to measure the effect of word frequency on embedding quality, which would help us understand the data requirements of our approach.
72	21	In CRW, the first word in every pair is the more frequent word and occurs in the subcorpus, while the second word occurs in the 255 sampled contexts but not in the subcorpus.
76	19	We report the Spearman ρ (as described above) at each sample size, averaged over 100 trials obtained by shuffling each rare word’s 255 contexts.
80	46	However, if we train word2vec embeddings from scratch on the subcorpus together with the sampled contexts we achieve a Spearman correlation of 0.45; this gap between word2vec and our method shows that there remains room for even better approaches for fewshot learning of word embeddings.
81	65	We now evaluate our work directly on the tasks posed by Herbelot and Baroni (2017), who developed simple datasets and methods to “simulate the process by which a competent speaker encounters a new word in known contexts.” The general goal will be to construct embeddings of new concepts in the same semantic space as a known embedding vocabulary using contextual information consisting of definitions or example sentences.
85	28	Using 259,376 word2vec embeddings trained on Wikipedia as the base vectors, Herbelot and Baroni (2017) heavily modify the skip-gram algorithm to successfully learn on one definition, creating the nonce2vec system.
92	26	The desired nonce embeddings is then evaluated via the correlation of its cosine similar- ity with the embeddings of several other words, with ratings provided by human judges.
103	37	Synset Embeddings: We use SemCor (Langone et al., 2004), a subset of the Brown Corpus (BC) (Francis and Kucera, 1979) annotated using PWN synsets.
104	55	However, because the corpus is quite small we use GloVe trained on Wikipedia instead of on BC itself.
106	420	Then we set the context embedding us of each synset s to be the average sum of word embeddings representation over all sentences in SemCor containing s. Finally, we apply the à la carte transform to get the synset embedding vs = Aus.
107	28	Sense Disambiguation: To determine the sense of a word w given its context c, we convert c into a vector using the à la carte transform A on the sum of its word embeddings and return the synset s of w whose embedding vs is most similar to this vector.
108	41	We try two different synset embeddings: those induced from SemCor as above and those obtained by embedding a synset using its gloss, or PWN-provided definition, in the same way as a nonce in Section 4.2.
110	63	As shown in Table 2, synset embeddings induced from SemCor alone beat MFS overall, largely due to good noun results.
