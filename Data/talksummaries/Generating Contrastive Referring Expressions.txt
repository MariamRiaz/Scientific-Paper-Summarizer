0	19	Interactive natural language generation (NLG) systems face the task of detecting when they have been misunderstood, and reacting appropriately to fix the problem.
1	121	For instance, even when the system generated a semantically correct referring expression (RE), the user may still misunderstand it, i.e. resolve it to a different object from the one the system intended.
2	16	In an interactive setting, such as a dialogue system or a pedestrian navigation system, the system can try to detect such misunderstandings – e.g. by predicting what the hearer understood from their behavior (Engonopoulos et al., 2013) – and to produce further utterances which resolve the misunderstanding and get the hearer to identify the intended object after all.
4	45	Say that we originally described an object b as “the blue button”, but the hearer approaches a button b′ which is green, thus providing evidence that they misunderstood the RE to mean b′.
5	16	In this case, we would like to say “no, the BLUE button”, with the contrastive focus realized by an appropriate pitch accent on “BLUE”.
8	44	We start from the modeling assumption that misunderstandings arise because the RE rs the system uttered was corrupted by a noisy channel into an RE ru which the user “heard” and then resolved correctly; in the example above, we assume the user literally heard “the green button”.
11	15	We evaluate our system empirically on REs from the GIVE Challenge (Koller et al., 2010) and the TUNA Challenge (van der Sluis et al., 2007), and show that the contrastive REs generated by our system are preferred over a number of baselines.
15	16	In Section 5, we present the corruption model and show how to use it to reconstruct ru.
59	32	We call an RE that uses contrastive focus to highlight the difference between the misunderstood and the intended object, a contrastive RE.
63	14	This algorithm assumes a synchronous grammar which relates strings with the sets of objects they refer to.
96	14	Finally, let L be the set of referring expressions that the IF would resolve to ou, i.e. the set of candidates for ru.
105	60	Each run of the automaton on a string w corresponds to a specific edit sequence that transforms rs intow, and the sum of transition weights of the run is the cost of that edit sequence.
107	20	It has a state qi for every position i in rs; the start state is q0 and the final state is q|rs|.
116	25	The resulting grammar G′ uses nonterminal symbols of the form Nb,A,〈qi,qk〉, where b, A are as in Section 4, and qi, qk indicate that the string derived by this nonterminal was generated by editing the substring of rs from position i to k. Let Nb,A → a be a production rule of G with a word a on the right-hand side; as explained above, b is the object to which the subtree should refer, and A is the set of objects to which the subtree actually might refer.
124	26	This rule deletes the substrings from positions h to i and j to k from rs; thus we assign it the cost ((i− h) + (k − j))C, i.e. the cost of the corresponding transitions.
128	22	6 shows an example tree for the G′ we obtain from the automaton in Fig.
129	21	We can read the string w = “the yellow button above the window” off of the leaves; by construction, this is an RE for ou.
136	33	We are now ready to generate a contrastive RE from rs and s∗.
148	43	We call this strategy Shortening and define it as follows.
150	20	So if rs is “the [blue button] [above the window]” and s∗ = K SyellowKKKK, corresponding to a rsE of “the [BLUE button] [above the window]”, then the RE would be “the [BLUE button]”.
153	23	To test whether our algorithm for contrastive REs assigns contrastive focus correctly, we evaluated it against several baselines in crowdsourced pairwise comparison overhearer experiments.
156	49	We performed a first experiment with scenes from the GIVE Challenge, while a second experiment replaced these scenes with stimuli from the “People” domain of the TUNA Reference Corpus (van der Sluis et al., 2007).
157	17	This corpus consists of photographs of men annotated with nine attributes, such as whether the We wanted our player to select the person circled in green: So we told them: the light haired old man in a suit looking straight.
187	26	While the Shortening strategy is numerically preferred over both baselines, the difference is not significant, and it is significantly worse than the Emphasis strategy.
208	16	We would expect a shorter RE to always be preferred, following the Gricean Maxim of Quantity (Grice, 1975).
211	14	In this paper, we have presented an algorithm for generating contrastive feedback for a hearer who has misunderstood a referring expression.
217	42	Furthermore, we could use this data to refine the costs c(D), c(Ia) etc.
218	12	for the edit operations, possibly assigning different costs to different edit operations.
219	62	Finally, it would be interesting to combine our algorithm with a speech synthesis system.
220	57	In this way, we will be able to express focus with actual pitch accents, in contrast to the typographic approximation we made here.
