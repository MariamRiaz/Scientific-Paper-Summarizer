1	13	Though neural networks are successful in practice, their theoretical properties are not yet well understood.
2	15	Specifically, there are two intriguing empirical observations that existing theories cannot explain.
3	47	• Optimization: Despite the highly non-convex nature of the objective function, simple first-order algorithms like stochastic gradient descent are able to minimize the training loss of neural networks.
4	43	Researchers have conjectured that the use of over-parametrization (Livni et al., 2014; Safran and Shamir, 2017) is the primary reason why local search algorithms can achieve low training error.
5	6	The intuition is over-parametrization alters the loss function to have a large manifold of globally optimal solutions, which in turn allows local search algorithms to more easily find a global optimal solution.
9	25	In this paper, we provide new theoretical insights into the optimization landscape and generalization ability of overparametrized neural networks.
11	15	(1) In the above x ∈ Rd is the input vector, W ∈ Rd×k with wj ∈ Rd denotes the j-th row of W and ai’s are the weights in the second layer.
14	12	In our setting, we fix the second layer to be a = (1, .
17	4	Though quadratic activations are rarely used in practice, stacking multiple such two-layer blocks can be used to simulate higher-order polynomial neural networks and sigmodial activated neural networks (Livni et al., 2014; Soltani and Hegde, 2017).
19	4	For gradient descent we use the following update W←W − η n∑ i=1 ∇W` (f(xi), yi) where η is the step size.
25	3	We analyze two kinds of over-parameterization.
26	34	First we show that for k ≥ d, then all local minima in Problem (2) is global and all saddle points are strict.
27	12	This properties together with recent algorithmic advances in non-convex optimization (Lee et al., 2016) imply gradient descent can find a globally optimal solution with random initialization.
28	31	This is a minor generalization of results in (Soltanolkotabi et al., 2017) which only includes `2 loss, and (Haeffele and Vidal, 2015; Haeffele et al., 2014) which only include k ≥ d+ 1.
29	15	Second, we consider another form of over-parametrization, k(k + 1) 2 > n. This condition on the amount of over-parameterization is much milder than k ≥ n, a condition used in many previous papers (Nguyen and Hein, 2017a;b).
30	21	Further in practice, k(k + 1)/2 > n is a much milder requirement than k ≥ d, since if k ≈ √ 2n and n << d2 then k << d. In this setting, we consider the perturbed version of the Problem (2): min W LC (W) = 1 n n∑ i=1 ` (f(xi), yi) + λ 2 ‖W‖2F + 〈C,W>W〉 (3) where C is a random positive semidefinite matrix with arbitrarily small Frobenius norm.
31	3	We show that if k(k+1)2 > n, Problem (3) also has the desired properties that all local minima are global and all saddle points are strict with probability 1.
32	25	Since C has small Frobenius norm, the optimal value of Problem (3) is very close to that of Problem (2).
33	9	See Section 3 for the precise statement.
76	11	For a matrix M, we denote ‖M‖2 the spectral norm and ‖M‖F the Frobenius norm.
78	3	We use Σ (M) to denote the set of matrices with Frobenius norm bounded by M and Σ1 (1) to denote the set of rank-1 matrices with spectral norm bounded by 1.
80	57	In this paper, we characterize the landscape of overparameterized neural networks.
84	4	If W∗ is a saddle point, then for all neighborhood O around W∗, there is a W ∈ O such that L (W) < L (W∗).
85	11	Ideally, we would like a loss function that satisfies the following two geometric properties.
86	4	Property 2.1 (All local minima are global).
87	3	If W∗ is a local minimum of L (·) it is also the global minimum, i.e., W∗ ∈ argminWL (W).
88	14	Property 2.2 (All saddles are strict).
89	6	At a saddle point Ws, there is a direction U ∈ Rk×d such that vect (U)>∇2L (Ws) vect (U) < 0.
91	3	Lastly, standard applications of Rademacher complexity theory will be used to derive generalization bounds.
95	4	In this section we present our main results on explaining why over-parametrization helps local search algorithms find a global optimal solution.
