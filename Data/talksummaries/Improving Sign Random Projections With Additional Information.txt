2	22	Today, this probability is also used for distance based learning algorithms given high dimensional data.
3	10	Consider a data matrix Xn×p with n observations, and p features.
13	10	An example of this could be image classification, where a 1024 × 1024 pixel image would mean working with vectors in Rp, p = 1048576.
18	9	We set sgn(x) = 0 when x < 0 for convenience of notation in Figures 2 and 8.
40	8	Deming examined the computation of maximum likelihood estimates for a multinomial distribution when the marginal probabilities are known.
41	10	His result was used by Li to estimate two way associations in word data by making use of information known at the margins.
43	15	We denote θxi,xj to be the angle between the vectors xi,xj .
48	29	Consider the vectors v1 = (v11, .
59	14	We mention that the denominators of the fractions in dldp3 are only equal to zero when pi = 0, for i = 1, 2, 3, 4.
72	7	We have that I(p3) = E [ − d 2l dp23 ] = E[n1]π2 (θx2e + π(p3 − 1))2 + E[n4]π2 (θx1e + π(p3 − 1))2 + E[n2]π2 (θx1e + θx2e + π(p3 − 1)) 2 + E[n3] p23 (15) = kp1π 2 (θx2e + π(p3 − 1))2 + kp4π 2 (θx1e + π(p3 − 1))2 + kp2π 2 (θx1e + θx2e + π(p3 − 1)) 2 + kp3 p23 (16) = k p1 + k p2 + k p3 + k p4 (17) = k ( 1 p1 + 1 p2 + 1 p3 + 1 p4 ) (18) and hence Var[p̂3] = 1 k ( 1 p1 + 1 p2 + 1 p3 + 1 p4 ) (19) We note that Var[p̂3] tends to 0 when any pi tends to 0.
89	11	The four faces of the tetrahedron correspond to the cases when Var[θ̂] = 0 and the center ( π 2 , π 2 , π 2 ) corresponds to Var[θ̂]max = Var[θ̂orig]max = π 2 4k .
93	11	Figure 4 shows some plots of Var[θ̂orig] − Var[θ̂] over the valid region of (θx1e, θx2e) for some fixed θ’s.
94	13	We can see that the variance reduction always occurs at the boundary and no significant variance reductions near the center.
105	25	Finding all angles between e and xi, 1 ≤ i ≤ n takes O(np) time, which is the same as the cost of normalizing or scaling data.
109	12	Hence, the overall computational complexity of our algorithm isO(n2s+npk+n2k) which isO(min{n2k, n2s}).
110	7	Our estimator only requires the probability of the random hyperplane separating the two vectors xi,xj .
111	7	Any derivatives of SRP which have a different probability can still utilize this estimator.
114	11	The Gisette dataset has n = 13, 500 observations, and p = 5, 000 parameters.
119	9	, 3008} of our random matrix over 100 simulations for both datasets.
136	12	To give some intuition to magnitude of the right and left graphs of Figure 5 and Figure 6, we show the exact values of the RMSE estimates and 3 standard deviations at k = 1024 in Figure 7.
140	17	Suppose we repeat the process in building our estimator and generate vectors e1, e2.
141	59	We can compute and store the angles θxiej for all 1 ≤ i ≤ n, 1 ≤ j ≤ 2, and compute V = sgn(XR),ve1 = sgn(e T 1 R),ve2 = sgn(eT2 R).
142	30	We then have a 2× 2× 2 contingency table, which we give in Figure 8 in terms of n1 to n8.
145	16	Suppose we have three vectors x1,x2,x3, with angles θij being the angle between xi,xj .
179	8	Minwise hashing - b-bit minwise hashing (Li & König, 2010) is used to estimate the resemblance of binary vectors, while weighted minwise hashing algorithms (Shrivastava, 2016; Ioffe, 2010) are used to estimate the Jaccard similarity between vectors.
180	32	We can add an extra information e that is a binary vector (or weighted vector) to better improve the estimates of the resemblance (or Jaccard similarity).
181	7	Conditional random sampling - Conditional random sampling (Church et al., 2006) is a local sampling strategy which is used to estimate Hamming distances and χ2 distances between vectors, amongst other distances.
185	39	As we are only counting the margins, we can set e1 to be an existing vector xs, and compute the respective angles θxs,xi , s 6= i.
186	34	While we have demonstrated an estimator with good performance, we feel that the idea behind the construction of this estimator (and subsequent estimators) is more important.
187	25	We hope this idea can be extended to many other practical applications as well.
