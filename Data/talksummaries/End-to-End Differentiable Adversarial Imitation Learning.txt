0	43	Learning a policy from scratch is often difficult.
3	10	Imitation is needed for several reasons: Automation (in case the expert is human), distillation (e.g., if the expert is too expensive to run in realtime (Rusu et al., 2015)), and initialization (using an expert policy as an initial solution).
5	23	Our goal is to train a new policy π which imitates πE without access to the original reward signal rE that was used by the expert.
6	62	There are two main approaches to solve imitation problems.
7	16	The first, known as Behavioral Cloning (BC), directly learns the conditional distribution of actions over states p(a|s) in a supervised learning fashion (Pomerleau, 1991).
9	30	However, BC has its downsides as well.
10	11	Contrary to temporal difference methods (Sutton, 1988) that integrate information over time, BC methods are trained using single time-step state-action pairs {st, at}.
11	22	Therefore, an agent trained using BC is unaware of how his choice of actions affects the future state distribution, which makes it susceptible to compounding errors (Ross & Bagnell, 2010; Ross et al., 2011).
13	15	The second approach to imitation is comprised of two stages.
15	15	(1) After reconstructing a reward signal r̂, the second step is to train a policy that maximizes the discounted cumulative expected reward: EπR = Eπ [∑T t=0 γ tr̂t ] .
17	101	A different strategy could be to recover a sparser reward signal (a more well-defined problem) and enrich it by hand.
19	16	Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) is a recent method for training generative models.
20	36	It uses a second neural network (D) to guide the generative model (G) towards producing patterns similar to those of the expert (see illustration in Figure 1).
32	6	In this section, we review the mathematical formulation of Markov Decision Processes, as well as previous approaches to imitation learning.
46	33	SMILe starts with an initial policy π0 that blindly follows the expert’s action choice.
50	119	In this game, player G produces patterns (denoted as x), and the second one (D) judges their authenticity.
51	10	It does so by solving a binary classification problem where G’s patterns are labeled as 0, and expert patterns are labeled as 1.
85	73	The first relates to the state distribution: what is the likelihood of encountering state s under the distribution induced by πE vs. the one induced by π?
87	8	We reach the conclusion that effective learning requires the learner to be mindful of two effects.
88	6	First, how its choice of actions stands against the expert?
103	11	Gumbel-Softmax provides a differentiable approximation of the hard sampling procedure in the Gumbel-Max trick, by replacing the argmax operation with a softmax: asoftmax = exp [ 1 τ (gi + log π(ai|s)) ]∑k j=1 exp [ 1 τ (gj + log π(ai|s)) ] , where τ is a ”temperature” hyper-parameter that trades bias with variance.
107	7	This leads to low variance gradients, but at the cost of a high bias (asoftmax 6= aargmax).
109	15	We solve this by applying argmax over asoftmax, but use the continuous approximation in the backward pass by using the estimation: ∇θaargmax ≈ ∇θasoftmax.
110	8	So far we showed the changes necessary to use the exact partial derivative ∇aD.
111	11	Incorporating the use of ∇sD as well is a more involved and constitutes the crux of this work.
114	14	On the contrary, in the model-based approach, st can be written as a function of the previous state and action: st = f(st−1, at−1), where f is the forward model.
118	11	Figure 3 summarizes this idea.
119	110	We showed that a good approach for imitation requires: (a) to use a model, and (b) to process multi-step transitions.
120	6	This setup was previously suggested by ShalevShwartz et al. (2016) and Heess et al. (2015), who built a multi-step computation graph for describing the familiar policy gradient objective, which in our case is given by: J(θ) = E [∑ t=0 γ tD(st, at) ∣∣θ].
121	16	To show how to differentiate J(θ) over a trajectory of (s, a, s′) transitions, we rely on the results of Heess et al. (2015): Js = Ep(a|s)Ep(s′|s,a) [ Ds +Daπs + γJ ′ s′(fs + faπs) ] , (12) Jθ = Ep(a|s)Ep(s′|s,a) [ Daπθ + γ(J ′ s′faπθ + J ′ θ) ] .
