4	15	The connection between neural networks and kernel machines has long been studied (Neal, 1994).
8	10	Another reason may be that some researchers may hold the intuitive (but not necessarily principled) view that the Central Limit Theorem (CLT) should somehow apply.
11	7	We investigate the consequences that weight initialization has on the equivalent kernel at the beginning of training.
13	8	The derived kernels also allow us to analyze the loss of information as an input is propagated through the network, offering a complementary view to the shattered gradient problem (Balduzzi et al., 2017).
14	30	Consider a fully connected (FC) feedforward neural network with m inputs and a hidden layer with n neurons.
16	18	Further assume that the biases are 0, as is common when initializing neural network parameters.
17	46	For any two inputs x,y ∈ Rm propagated through the network, the dot product in the hidden layer is 1 n h(x) · h(y) = 1 n n∑ i=1 σ(wi · x)σ(wi · y), (1) where h(·) denotes the n dimensional vector in the hidden layer and wi ∈ Rm is the weight vector into the ith neuron.
18	29	Assuming an infinite number of hidden neurons, the sum in (1) has an interpretation as an inner product in feature space, which corresponds to the kernel of a Hilbert space.
39	26	The kernel (2) has previously been evaluated for a number of choices of f and σ (Williams, 1997; Roux & Bengio, 2007; Cho & Saul, 2009; Pandey & Dukkipati, 2014a;b).
43	8	Figure 1 shows a plot of the normalized Arc-Cosine Kernel.
44	15	One might ask how the equivalent kernel changes for a different choice of weight distribution.
47	18	The kernel can also be used to choose good weights for initialization.
52	9	We then extend this result using the same technique to the case where σ is LReLU.
55	29	The equivalent kernel of the network is (3).
57	35	With the conditions in Proposition 1 and inputs x,y ∈ Rm the equivalent kernel of the network is the solution to the Initial Value Problem (IVP) k′′(θ0) + k(θ0) = F (θ0), k ′(π) = 0, k(π) = 0, (4) where θ0 ∈ (0, π) is the angle between the inputs x and y.
86	7	One could similarly up-sample the image.
95	27	Then σ(W(m) · x(m))σ(W(m) · y(m)) D−→ σ(Z1)σ(Z2), where D−→ denotes convergence in distribution and Z = (Z1, Z2) T is a Gaussian random vector with covari- ance matrix E[W 2i ] [ ‖x‖2 ‖x‖‖y‖ cos θ0 ‖x‖‖y‖ cos θ0 ‖y‖2 ] and 0 mean.
96	28	Every Z(m) = (W(m) · x(m),W(m) · y(m))T has the same mean and covariance matrix as Z. Convergence in distribution is a weak form of convergence, so we cannot expect in general that all kernels should converge asymptotically.
98	17	We first present the ReLU case.
102	27	Suppose the random weights come from a spherical Gaussian with E[Wi] = 0 and finite variance E[W 2i ] with PDF gm.
114	23	As β → ∞ this distribution converges pointwise to the uniform distribution on [−α, α].
120	13	Without skip connections, the gradients of deep networks approach white noise as they are backpropagated through the network, making them difficult to train.
122	21	As m → ∞, the angle between two inputs in the jth layer of a LReLU network random weights with E[W ] = 0 and E|W 3| < ∞ approaches cos θj = 1 1+a2 ( (1−a)2 π ( sin θj−1+(π−θj−1) cos θj−1 ) +2a cos θ0 ) .
133	7	Corollary 8 implies that for this deep network, the angle between any two signals at a deep layer approaches 0.
145	26	Contrary to the shattered gradients analysis, which applies to gradient based optimizers, our analysis relates to any optimizers that initialize weights from some distribution satisfying conditions in Proposition 4 or Corollary 7.
146	15	Since information is lost during signal propagation, the network’s output shares little information with the input.
147	12	An optimizer that tries to relate inputs, outputs and weights through a suitable cost function will be “blind” to relationships between inputs and outputs.
160	8	We have considered universal properties of MLPs with weights coming from a large class of distributions.
161	17	We have theoretically and empirically shown that the equivalent kernel for networks with an infinite number of hidden ReLU neurons and all rotationally-invariant weight distributions is the Arc-Cosine Kernel.
164	15	The kernel converges to a fixed point, showing that information is lost as signals propagate through the network.
