0	21	A document summary can be regarded as domainspecific or general-purpose, using the specificity as classification criterion (Hovy and Lin, 1998).
1	15	We can, also, look at this criterion from language angle: language-specific or language-independent summarization.
2	27	Language-independent systems can handle more than one language.
7	12	In the task’s pilot, there were seven languages covering news texts: Arabic, Czech, English, French, Greek, Hebrew and Hindi, where each system has to participate for at least two languages.
59	6	Figure 1 represents the general architecture of AllSummarizer1.
60	9	This is the language-dependent part, which can be found in many information retrieval (IR) works.
67	7	Concerning stop-word elimination, we use precompiled word-lists available on the web.
69	41	Each text contains many topics, where a topic is a set of sentences having some sort of relationship between each other.
74	8	To generate topics, we use a simple algorithm (see algorithm 1) which uses cosine similarity and a clustering threshold th to cluster n sentences.
75	6	Algorithm 1: clustering method Data: Pre-processed sentences Result: clusters of sentences (C) foreach sentence Si / i = 1 to n do Ci += Si ; // Ci: ith cluster foreach sentence Sj / j = i + 1 to n do Sim = cosine similarity(Si, Sj) ; if sim > th then Ci += Sj ; end end C += Ci ; end foreach cluster Ci / i=n to 1 do foreach cluster Cj / j=i-1 to 1 do if Ci is included in Cj then C -= Ci ; break ; end end end
87	15	We use 5 statistical features to score the sentences: unigram term frequency (TFU), bigram term frequency (TFB), sentence position (Pos) and sentence length (Rleng, PLeng).
90	16	Each category has a probability to occur in a cluster, which is the number of its appearance in this cluster divided by all cluster’s terms, as shown in equation 4.
113	31	The clustering threshold is used with sentences’ similarities to decide if two sentences are similar or not.
114	5	Our idea is to use statistic measures over those similarities to estimate the clustering threshold.
120	7	For each document (or topic in multi-document), we generated summaries using the 8 measures of th, and different combinations of the scoring features.
121	13	Then, we calculated the average ROUGE-2 score for each language.
128	12	To compare our system to others participated systems, we followed these steps (for every evaluation metric): • For each system, calculate the average scores of all used languages.
131	9	• Then, we calculate the relative improvement using the averages oursystem−othersystemothersystem .
139	5	Looking at these results, our system took the fifth place out of seven participants.
143	11	Table 4 shows a comparison between our system and the other systems in multi-document task, using the relative improvement.
144	6	We used the parameters fixed for single document summarization to see if the same parameters are applicable for both single and multi-document summarizations.
145	8	Looking to the results, our system took the seventh place out of ten participants.
149	18	So, we consider the input text as a set of topics, where a sentence can belong to many topics.
150	16	We calculated how much a sentence can represent all the topics.
151	84	Then, the score is used to reorder the sentences and extract the first non redundant ones.
153	6	Compared to other systems, it affords fair results, but more improvements have to be done in the future.
156	18	We fixed the parameters (threshold and features) based on the average score of ROUGE-2 of all training documents.
157	6	Further investigations must be done to estimate these parameters for each document based on statistical criteria.
158	32	We want to investigate the effect of the preprocessing step and the clustering methods on the resulted summaries.
159	16	Finally, readability remains a challenge for extractive methods, especially when we want to use a multilingual method.
