0	56	Networks are ubiquitous, with prominent examples including social networks (e.g., Facebook, Twitter) or citation networks of research papers (e.g., arXiv).
3	21	Alternatively, network embedding (i.e., network representation learning) has been considered, representing each vertex of a network with a low-dimensional vector that preserves information on its similarity rel- ative to other vertices.
13	16	Although these methods have demonstrated performance gains over structure-only network embeddings, the relationship between text sequences for a pair of vertices is accounted for solely by comparing their sentence embeddings.
14	19	However, as shown in Figure 1, to assess the similarity between two research papers, a more effective strategy would compare and align (via localweighting) individual important words (keywords) within a pair of abstracts, while information from other words (e.g., stop words) that tend to be less relevant can be effectively ignored (downweighted).
17	25	Given a pair of sentences, our model first aligns each word within one sentence with keywords from the other sentence (adaptively up-weighted via an attention mechanism), producing a set of fine-grained matching vectors.
32	19	To learn these embeddings, we specify an objective that leverages the information from both W and T , denoted as L = ∑ e∈E Lstruct(e) + Ltext(e) + Ljoint(e) , (1) where Lstruct, Ltext and Ljoint denote structure, text, and joint structure-text training losses, respectively.
33	27	For a vertex pair {vi, vj} weighted by wij , Lstruct(vi, vj) in (1) is defined as (Tang et al., 2015) Lstruct(vi, vj) = wij log p(his|hjs) , (2) where p(his|hjs) denotes the conditional probability between structural embeddings for vertices {vi, vj}.
34	22	To leverage the textual information in T , similar text-specific and joint structure-text training objectives are also defined Ltext(vi, vj) = wijα1 log p(hit|h j t ) , (3) Ljoint(vi, vj) = wijα2 log p(hit|hjs) (4) + wijα3 log p(h i s|h j t ) , (5) where p(hit|h j t ) and p(h i t|hjs) (or p(his|h j t )) denote the conditional probability for a pair of text embeddings and text embedding given structure embedding (or vice versa), respectively, for vertices {vi, vj}.
36	18	Note that structural embeddings, hs, are treated directly as parameters, while the text embeddings ht are learned based on the text sequences associated with vertices.
44	10	We first introduce our base model, which reweights the importance of individual words within a text sequence in the context of the edge being considered.
49	17	Thus, while inferring the adaptive textual embedding for sentence ta, we propose reweighting the importance of each word in ta to explicitly account for its alignment with sentence tb.
50	24	The weight αi, corresponding to the i-th word in ta, is generated as: αi = exp(tanh(W1cb +W2x (i) a ))∑Ma j=1 exp(tanh(W1cb +W2x (j) a )) , (7) where W1 and W2 are model parameters and cb = ∑Mb i=1 x b i is the context vector of sequence tb, obtained by simply averaging over all the word embeddings in the sequence, similar to fastText (Joulin et al., 2016).
51	11	Further, the word-by-context embedding for sequence ta is obtained by taking the weighted average over all word embeddings ha = ∑Ma i=1αix (i) a .
58	55	As illustrated in Figure 2, given two input embedding matrices Xa and Xb, we first compute the affinity matrix A ∈ RMb×Ma , whose elements represent the affinity scores corresponding to all word pairs between sequences ta and tb A = XTb Xa .
59	22	(9) Subsequently, we compute the context-aware matrix for sequence tb as Ab = softmax(A) , X̃b = XbAb , (10) where the softmax(·) function is applied columnwise to A, and thus Ab contains the attention weights (importance scores) across sequence tb (columns), which account for each word in sequence ta (rows).
62	43	To abstract the word-by-word alignments, we compare x(i)a with x̃ (i) b , for i = 1, 2, ...,Ma, to obtain the corresponding matching vector m(i)a = falign ( x(i)a , x̃ (i) b ) , (11) where falign(·) represents the alignment function.
65	25	Subsequently, matching vectors from (11) are aggregated to produce the final textual embedding hat for sequence ta as hat = faggregate ( m(1)a ,m (2) a , ...,m (Ma) a ) , (12) where faggregate denotes the aggregation function, which we specify as the max-pooling pooling operation.
67	11	Although these aggregation functions are simple and invariant to the order of words in input sentences, they have been demonstrated to be highly effective in relational reasoning (Parikh et al., 2016; Santoro et al., 2017).
88	14	The following three real-world datasets are employed for quantitative evaluation: (i) Cora, a standard paper citation network that contains 2,277 machine learning papers (vertices) grouped into 7 categories and connected by 5,214 citations (edges) (ii) HepTh, another citation network of 1,038 papers with abstract information and 1,990 citations; (iii) Zhihu, a network of 10,000 active users from Zhihu, the largest Q&A website in China, where 43,894 vertices and descriptions of the Q&A topics are available.
93	12	We set the batch size as 128, and the model is trained using Adam (Kingma and Ba, 2014), with a learning rate of 1× 10−3 for all parameters.
102	18	We experiment with three variants for our WANE model: (i) WANE: where the word embeddings of each text sequence are simply average to obtain the sentence representations, similar to (Joulin et al., 2016; Shen et al., 2018c).
114	12	Similar to Tu et al. (2017), we generate the global embedding for each vertex by taking the average over its context-aware embeddings with all other connected vertices.
116	85	Moreover, WANEww consistently outperforms other competitive semantic-aware models on a wide range of labeled proportions, suggesting that explicitly capturing word-by-word alignment features is not only use- ful for vertex-pair-based tasks, such as link prediction, but also results in better global embeddings which are required for vertex classification tasks.
122	11	Motivated by the observation in Wang and Jiang (2017) that the advantages of different functions to match two vectors vary from task to task, we further explore the choice of alignment and aggregation functions in our WANE-ww model.
126	17	In terms of the aggregation functions, we compare (one-layer) CNN, mean-pooling, and maxpooling operations to accumulate the matching vectors.
129	21	Embedding visualization To visualize the learned network representations, we further employ t-SNE to map the low-dimensional vectors of the vertices to a 2-D embedding space.
131	22	As shown in Figure 4 where each point indicates one paper (vertex), and the color of each point indicates the category it belongs to, the embeddings of the same label are indeed very close in the 2-D plot, while those with different labels are relatively farther from each other.
133	14	Case study The proposed word-by-word alignment mechanism can be used to highlight the most informative words (and the corresponding matching features) wrt the relationship between vertices.
135	19	It can be observed that matched key words, e.g., ‘MCMC’, ‘convergence’, between the text sequences are indeed assigned higher values in the matching vectors.
136	60	These words would be selected preferentially by the final max-pooling aggregation operation.
