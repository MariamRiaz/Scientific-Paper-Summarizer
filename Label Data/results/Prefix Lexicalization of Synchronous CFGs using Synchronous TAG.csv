0,1,label2,summary_sentences
"Deep neural networks achieve near-human accuracy on many perception tasks (He et al., 2016; Amodei et al., 2015).",1. Introduction,[0],[0]
"However, they lack robustness to small alterations of the inputs at test time (Szegedy et al., 2014).",1. Introduction,[0],[0]
"Indeed when presented with a corrupted image that is barely distinguishable from a legitimate one by a human, they can predict incorrect labels, with high-confidence.",1. Introduction,[0],[0]
"An adversary can design such so-called adversarial examples, by adding a small perturbation to a legitimate input to maximize the likelihood of an incorrect class under constraints on the magnitude of the perturbation (Szegedy et al., 2014; Goodfellow et al., 2015; Moosavi-Dezfooli et al., 2015; Pa-
1Facebook AI Research.",1. Introduction,[0],[0]
"Correspondence to: Moustapha Cisse <moustaphacisse@fb.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"pernot et al., 2016a).",1. Introduction,[0],[0]
"In practice, for a significant portion of inputs, a single step in the direction of the gradient sign is sufficient to generate an adversarial example (Goodfellow et al., 2015) that is even transferable from one network to another one trained for the same problem but with a different architecture (Liu et al., 2016; Kurakin et al., 2016).
",1. Introduction,[0],[0]
The existence of transferable adversarial examples has two undesirable corollaries.,1. Introduction,[0],[0]
"First, it creates a security threat for production systems by enabling black-box attacks (Papernot et al., 2016a).",1. Introduction,[0],[0]
"Second, it underlines the lack of robustness of neural networks and questions their ability to generalize in settings where the train and test distributions can be (slightly) different as is the case for the distributions of legitimate and adversarial examples.
",1. Introduction,[0.9600212741681879],['This is because the size increase depends on both the number of prefix lexicalized trees in the intermediate grammars (which grows with the proportion of lexicalized rules) and the number of productions which need to be lexicalized (which shrinks as the proportion of prefix lexicalized rules increases).']
"Whereas the earliest works on adversarial examples already suggested that their existence was related to the magnitude of the hidden activations gradient with respect to their inputs (Szegedy et al., 2014), they also empirically assessed that standard regularization schemes such as weight decay or training with random noise do not solve the problem (Goodfellow et al., 2015; Fawzi et al., 2016).",1. Introduction,[0],[0]
The current mainstream approach to improving the robustness of deep networks is adversarial training.,1. Introduction,[0],[0]
"It consists in generating adversarial examples on-line using the current network’s parameters (Goodfellow et al., 2015; Miyato et al., 2015; Moosavi-Dezfooli et al., 2015; Szegedy et al., 2014; Kurakin et al., 2016) and adding them to the training data.",1. Introduction,[0],[0]
"This data augmentation method can be interpreted as a robust optimization procedure (Shaham et al., 2015).
",1. Introduction,[0],[0]
"In this paper, we introduce Parseval networks, a layerwise regularization method for reducing the network’s sensitivity to small perturbations by carefully controlling its global Lipschitz constant.",1. Introduction,[0],[0]
"Since the network is a composition of functions represented by its layers, we achieve increased robustness by maintaining a small Lipschitz constant (e.g., 1) at every hidden layer; be it fully-connected, convolutional or residual.",1. Introduction,[0],[0]
"In particular, a critical quantity governing the local Lipschitz constant in both fully connected and convolutional layers is the spectral norm of the weight matrix.",1. Introduction,[0],[0]
"Our main idea is to control this norm by parameterizing the network with parseval tight frames (Kovačević & Chebira, 2008), a generalization of orthogonal matrices.
",1. Introduction,[0],[0]
"The idea that regularizing the spectral norm of each weight
matrix could help in the context of robustness appeared as early as (Szegedy et al., 2014), but no experiment nor algorithm was proposed, and no clear conclusion was drawn on how to deal with convolutional layers.",1. Introduction,[0],[0]
"Previous work, such as double backpropagation (Drucker & Le Cun, 1992) has also explored jacobian normalization as a way to improve generalization.",1. Introduction,[0],[0]
Our contribution is twofold.,1. Introduction,[0],[0]
"First, we provide a deeper analysis which applies to fully connected networks, convolutional networks, as well as Residual networks (He et al., 2016).",1. Introduction,[0],[0]
"Second, we propose a computationally efficient algorithm and validate its effectiveness on standard benchmark datasets.",1. Introduction,[0],[0]
"We report results on MNIST, CIFAR-10, CIFAR-100 and Street View House Numbers (SVHN), in which fully connected and wide residual networks were trained (Zagoruyko & Komodakis, 2016) with Parseval regularization.",1. Introduction,[0],[0]
"The accuracy of Parseval networks on legitimate test examples matches the state-of-the-art, while the results show notable improvements on adversarial examples.",1. Introduction,[0],[0]
"Besides, Parseval networks train significantly faster than their vanilla counterpart.
",1. Introduction,[0],[0]
"In the remainder of the paper, we first discuss the previous work on adversarial examples.",1. Introduction,[0],[0]
"Next, we give formal definitions of the adversarial examples and provide an analysis of the robustness of deep neural networks.",1. Introduction,[0],[0]
"Then, we introduce Parseval networks and its efficient training algorithm.",1. Introduction,[0],[0]
Section 5 presents experimental results validating the model and providing several insights.,1. Introduction,[0],[0]
"Early papers on adversarial examples attributed the vulnerability of deep networks to high local variations (Szegedy et al., 2014; Goodfellow et al., 2015).",2. Related work,[0],[0]
"Some authors argued that this sensitivity of deep networks to small changes in their inputs is because neural networks only learn the discriminative information sufficient to obtain good accuracy rather than capturing the true concepts defining the classes (Fawzi et al., 2015; Nguyen et al., 2015).
",2. Related work,[0],[0]
"Strategies to improve the robustness of deep networks include defensive distillation (Papernot et al., 2016b), as well as various regularization procedures such as contractive networks (Gu & Rigazio, 2015).",2. Related work,[0],[0]
"However, the bulk of recent proposals relies on data augmentation (Goodfellow et al., 2015; Miyato et al., 2015; Moosavi-Dezfooli et al., 2015; Shaham et al., 2015; Szegedy et al., 2014; Kurakin et al., 2016).",2. Related work,[0],[0]
It uses adversarial examples generated online during training.,2. Related work,[0],[0]
"As we shall see in the experimental section, regularization can be complemented with data augmentation; in particular, Parseval networks with data augmentation appear more robust than either data augmentation or Parseval networks considered in isolation.",2. Related work,[0],[0]
"We consider a multiclass prediction setting, where we have Y classes in Y = {1, ..., Y }.",3. Robustness in Neural Networks,[0],[0]
"A multiclass classifier is a function ĝ : (x ∈ RD,W ∈ W) 7→ argmaxȳ∈Y",3. Robustness in Neural Networks,[0],[0]
"gȳ(x,W ), where W are the parameters to be learnt, and gȳ(x,W ) is the score given to the (input, class) pair (x, ȳ) by a function g :",3. Robustness in Neural Networks,[0],[0]
RD × W → RY .,3. Robustness in Neural Networks,[0],[0]
"We take g to be a neural network, represented by a computation graph G = (N , E), which is a directed acyclic graph with a single root node, and each node n ∈ N takes values in Rd (n) out and is a function of its children in the graph, with learnable parameters W (n):
n : x 7→ φ(n) ( W (n), ( n′(x) )",3. Robustness in Neural Networks,[0],[0]
"n′:(n,n′)∈E ) .",3. Robustness in Neural Networks,[0],[0]
"(1)
The function g we want to learn is the root of G. The training data ((xi, yi))mi=1 ∈",3. Robustness in Neural Networks,[0],[0]
"(X × Y)m is an i.i.d. sample of D, and we assume X ⊂",3. Robustness in Neural Networks,[0],[0]
RD is compact.,3. Robustness in Neural Networks,[0],[0]
"A function ` : RY × Y → R measures the loss of g on an example (x, y); in a single-label classification setting for instance, a common choice for ` is the log-loss:
` ( g(x,W ), y ) = −gy(x,W ) + log (∑ ȳ∈Y egȳ(x,W ) ) .",3. Robustness in Neural Networks,[0],[0]
"(2)
The arguments that we develop below depend only on the Lipschitz constant of the loss, with respect to the norm of interest.",3. Robustness in Neural Networks,[0],[0]
"Formally, we assume that given a p-norm of interest ‖.‖p, there is a constant λp such that
∀z, z′ ∈ RY ,∀ȳ ∈ Y, |`(z, ȳ)−`(z′, ȳ)| ≤",3. Robustness in Neural Networks,[0],[0]
"λp‖z−z′‖p .
",3. Robustness in Neural Networks,[0],[0]
"For the log-loss of (2), we have λ2 ≤ √
2 and λ∞ ≤ 2.",3. Robustness in Neural Networks,[0],[0]
"In the next subsection, we define adversarial examples and the generalization performance of the classifier.",3. Robustness in Neural Networks,[0],[0]
"Then, we make the relationship between robustness to adversarial examples and the lipschitz constant of the networks.",3. Robustness in Neural Networks,[0],[0]
"Given an input (train or test) example (x, y), an adversarial example is a perturbation of the input pattern x̃ = x",3.1. Adversarial examples,[0],[0]
"+ δx where δx is small enough so that x̃ is nearly undistinguishable from x (at least from the point of view of a human annotator), but has the network predict an incorrect label.",3.1. Adversarial examples,[0],[0]
"Given the network parameters and structure g(.,W ) and a p-norm, the adversarial example is formally defined as
x̃ = argmax x̃:‖x̃−x‖p≤
` ( g(x̃,W ), y ) , (3)
where represents the strength of the adversary.",3.1. Adversarial examples,[0],[0]
"Since the optimization problem above is non-convex, Shaham et al. (2015) propose to take the first order taylor expansion of x 7→ `(g(x,W ), y) to compute δx by solving
x̃ = argmax x̃:‖x̃−x‖p≤
( ∇x`(g(x,W ), y) )",3.1. Adversarial examples,[0],[0]
T (x̃− x) .,3.1. Adversarial examples,[0],[0]
"(4)
If p = ∞, then x̃ = x + sign(∇x`(g(x,W ), y)).",3.1. Adversarial examples,[0],[0]
This is the fast gradient sign method.,3.1. Adversarial examples,[0],[0]
"For the case p = 2, we obtain x̃ = x + ∇x`(g(x,W ), y).",3.1. Adversarial examples,[0],[0]
"A more involved method is the iterative fast gradient sign method, in which several gradient steps of (4) are performed with a smaller stepsize to obtain a local minimum of (3).",3.1. Adversarial examples,[0],[0]
"In the context of adversarial examples, there are two different generalization errors of interest:
L(W )",3.2. Generalization with adversarial examples,[0],[0]
"= E (x,y)∼D
[ `(g(x,W ), y) ] ,
Ladv(W,p, ) =",3.2. Generalization with adversarial examples,[0],[0]
"E (x,y)∼D
[ max
x̃:‖x̃−x‖p≤ `(g(x̃,W ), y)
] .
",3.2. Generalization with adversarial examples,[0],[0]
"By definition, L(W )",3.2. Generalization with adversarial examples,[0],[0]
"≤ Ladv(W,p, ) for every p and >0.",3.2. Generalization with adversarial examples,[0],[0]
"Reciprocally, denoting by λp and Λp the Lipschitz constant (with respect to ‖.‖p) of ` and g respectively, we have:
Ladv(W,p, ) ≤ L(W )",3.2. Generalization with adversarial examples,[0],[0]
+,3.2. Generalization with adversarial examples,[0],[0]
"E
(x,y)∼D
[ max
x̃:‖x̃−x‖p≤ |`(g(x̃,W ), y)− `(g(x,W ), y)| ] ≤ L(W ) +",3.2. Generalization with adversarial examples,[0],[0]
"λpΛp .
",3.2. Generalization with adversarial examples,[0],[0]
This suggests that the sensitivity to adversarial examples can be controlled by the Lipschitz constant of the network.,3.2. Generalization with adversarial examples,[0.9517254368839941],"['We also show the percentage of productions which are already prefix lexicalized in G. ated by linked nonterminals in the original grammar will still be generated by linked nonterminals in the final grammar, so no reordering information is lost or added.6 This result holds despite the fact that our transformation is only applicable to chainfree grammars: chain rules cannot introduce any reorderings, since by definition they involve only a single pair of linked nonterminals.']"
"In the robustness framework of (Xu & Mannor, 2012), the Lipschitz constant also controls the difference between the average loss on the training set and the generalization performance.",3.2. Generalization with adversarial examples,[0],[0]
"More precisely, let us denote by Cp(X , γ) the covering number of X using γ-balls for ‖.‖p.",3.2. Generalization with adversarial examples,[0],[0]
"Using M = supx,W,y `(g(x,W ), y), Theorem 3 of (Xu & Mannor, 2012) implies that for every δ ∈ (0, 1), with probability 1− δ over the i.i.d. sample ((xi, yi)mi=1, we have:
L(W )",3.2. Generalization with adversarial examples,[0],[0]
≤ 1 m m∑ i=1,3.2. Generalization with adversarial examples,[0],[0]
"`(g(xi,W ), yi)
+ λpΛpγ",3.2. Generalization with adversarial examples,[0],[0]
"+M
√ 2Y Cp(X , γ2 )",3.2. Generalization with adversarial examples,[0],[0]
"ln(2)− 2 ln(δ)
m .
",3.2. Generalization with adversarial examples,[0],[0]
"Since covering numbers of a p-norm ball in RD increases exponentially with RD, the bound above suggests that it is critical to control the Lipschitz constant of g, for both good generalization and robustness to adversarial examples.",3.2. Generalization with adversarial examples,[0],[0]
"From the network structure we consider (1), for every node n ∈",3.3. Lipschitz constant of neural networks,[0],[0]
"N , we have (see below for the definition of Λ(n,n ′) p ):
",3.3. Lipschitz constant of neural networks,[0],[0]
"‖n(x)− n(x̃)‖p ≤ ∑
n′:(n,n′)∈E
Λ(n,n ′) p ‖n′(x)− n′(x̃)‖p ,
for any Λ(n,n ′)
p that is greater than the worst case variation of n with respect to a change in its input n′(x).",3.3. Lipschitz constant of neural networks,[0],[0]
"In particular we can take for Λ(n,n ′)",3.3. Lipschitz constant of neural networks,[0],[0]
"p any value greater than the
supremum over x0 ∈ X of the Lipschitz constant for ‖.‖p of the function (1n′′ = n′ is 1 if n′′ = n′ and 0 otherwise):
x 7→ φ(n) ( W (n), ( n′′(x0+1n ′′ = n′(x−x0)) )",3.3. Lipschitz constant of neural networks,[0],[0]
"n′′:(n,n′′)∈E ) .
",3.3. Lipschitz constant of neural networks,[0],[0]
"The Lipschitz constant of n, denoted by Λ(n)p satisfies:
Λ(n)p ≤ ∑
n′:(n,n′)∈E
Λ(n,n ′) p Λ (n′) p (5)
",3.3. Lipschitz constant of neural networks,[0],[0]
"Thus, the Lipschitz constant of the network g can grow exponentially with its depth.",3.3. Lipschitz constant of neural networks,[0],[0]
"We now give the Lipschitz constants of standard layers as a function of their parameters:
Linear layers: For layer n(x) = W (n)n′(x) where n′ is the unique child of n in the graph, the Lipschitz constant for ‖.‖p is, by definition, the matrix norm of W (n) induced by ‖.‖p, which is usually denoted ‖W (n)‖p and defined by
‖W (n)‖p = sup z:‖z‖p=1 ‖W (n)z‖p .
",3.3. Lipschitz constant of neural networks,[0],[0]
"Then Λ(n)2 = ‖W (n)‖2Λ (n′) 2 , where ‖W (n)‖2, called the spectral norm of W (n), is the maximum singular value of W (n).",3.3. Lipschitz constant of neural networks,[0],[0]
"We also have Λ(n)∞ = ‖W (n)‖∞Λ(n ′) ∞ , where
‖W (n)‖∞ = maxi ∑ j |W (n)",3.3. Lipschitz constant of neural networks,[0],[0]
ij,3.3. Lipschitz constant of neural networks,[0],[0]
| is the maximum 1-norm of the rows.,3.3. Lipschitz constant of neural networks,[0],[0]
"W (n).
",3.3. Lipschitz constant of neural networks,[0],[0]
"Convolutional layers: To simplify notation, let us consider convolutions on 1D inputs without striding, and we take the width of the convolution to be 2k + 1 for k ∈ N. To write convolutional layers in the same way as linear layers, we first define an unfolding operator U , which prepares the input z, denoted by U(z).",3.3. Lipschitz constant of neural networks,[0],[0]
"If the input has length T with din inputs channels, the unfolding operator maps z",3.3. Lipschitz constant of neural networks,[0],[0]
"For a convolution of the unfolding of z considered as a T × (2k + 1)din matrix, its j-th column is:
Uj(z) =",3.3. Lipschitz constant of neural networks,[0],[0]
"[zj−k; ...; zj+k] ,
where “;” is the concatenation along the vertical axis (each zi is seen as a column din-dimensional vector), and zi",3.3. Lipschitz constant of neural networks,[0],[0]
= 0,3.3. Lipschitz constant of neural networks,[0],[0]
if i is out of bounds (0-padding).,3.3. Lipschitz constant of neural networks,[0],[0]
"A convolutional layer with dout output channels is then defined as
n(x) =",3.3. Lipschitz constant of neural networks,[0],[0]
"W (n) ∗ n′(x) = W (n)U(n′(x)) ,
where W (n) is a dout × (2k + 1)din matrix.",3.3. Lipschitz constant of neural networks,[0],[0]
"We thus have Λ
(n) 2 ≤ ‖W‖2‖U(n′(x))‖2.",3.3. Lipschitz constant of neural networks,[0],[0]
"Since U is a linear operator that essentially repeats its input (2k + 1) times, we have ‖U(n′(x))",3.3. Lipschitz constant of neural networks,[0],[0]
− U(n′(x̃))‖22 ≤ (2k + 1)‖n′(x),3.3. Lipschitz constant of neural networks,[0],[0]
"− n′(x̃)‖22, so that Λ(n)2 ≤ √ 2k + 1‖W‖2Λ(n ′) 2 .",3.3. Lipschitz constant of neural networks,[0],[0]
"Also, ‖U(n′(x))",3.3. Lipschitz constant of neural networks,[0],[0]
"− U(n′(x̃))‖∞ = ‖n′(x) − n′(x̃)‖∞, and so for a convolutional layer, Λ(n)∞ ≤ ‖W (n)‖∞Λ(n ′) ∞ .
",3.3. Lipschitz constant of neural networks,[0],[0]
"Aggregation layers/transfer functions: Layers that perform the sum of their inputs, as in Residual Netowrks (He et al., 2016), fall in the case where the values Λ(n,n ′) p in (5) come into play.",3.3. Lipschitz constant of neural networks,[0],[0]
"For a node n that sums its inputs, we have Λ (n,n′) p = 1, and thus Λ (n) p ≤ ∑ n′:(n,n′)∈E Λ (n′) p .",3.3. Lipschitz constant of neural networks,[0],[0]
"If n is a tranfer function layer (e.g., an element-wise application of ReLU)",3.3. Lipschitz constant of neural networks,[0],[0]
"we can check that Λ(n)p ≤ Λ(n ′) p , where n′ is the input node, as soon as the Lipschitz constant of the transfer function (as a function R→ R) is ≤ 1.",3.3. Lipschitz constant of neural networks,[0],[0]
"Parseval regularization, which we introduce in this section, is a regularization scheme to make deep neural networks robust, by constraining the Lipschitz constant (5) of each hidden layer to be smaller than one, assuming the Lipschitz constant of children nodes is smaller than one.",4. Parseval networks,[0],[0]
"That way, we avoid the exponential growth of the Lipschitz constant, and a usual regularization scheme (i.e., weight decay) at the last layer then controls the overall Lipschitz constant of the network.",4. Parseval networks,[0],[0]
"To enforce these constraints in practice, Parseval networks use two ideas: maintaining orthonormal rows in linear/convolutional layers, and performing convex combinations in aggregation layers.",4. Parseval networks,[0],[0]
"Below, we first explain the rationale of these constraints and then describe our approach to efficiently enforce the constraints during training.",4. Parseval networks,[0],[0]
"Orthonormality of weight matrices: For linear layers, we need to maintain the spectral norm of the weight matrix at 1.",4.1. Parseval Regularization,[0],[0]
Computing the largest singular value of weight matrices is not practical in an SGD setting unless the rows of the matrix are kept orthogonal.,4.1. Parseval Regularization,[0],[0]
"For a weight matrix W ∈ Rdout×din with dout ≤ din, Parseval regularization maintains WTW",4.1. Parseval Regularization,[0],[0]
"≈ Idout×dout , where I refers to the identity matrix.",4.1. Parseval Regularization,[0],[0]
"W is then approximately a Parseval tight frame (Kovačević & Chebira, 2008), hence the name of Parseval networks.",4.1. Parseval Regularization,[0],[0]
"For convolutional layers, the matrix W ∈ Rdout×(2k+1)din is constrained to be a Parseval tight frame (with the notations of the previous section), and the output is rescaled by a factor (2k + 1)−1/2.",4.1. Parseval Regularization,[0],[0]
"This maintains all singular values of W to (2k+ 1)−1/2, so that Λ
(n) 2 ≤ Λ (n′) 2 where n
′ is the input node.",4.1. Parseval Regularization,[0],[0]
"More generally, keeping the rows of weight matrices orthogonal makes it possible to control both the spectral norm and the ‖.‖∞ of a weight matrix through the norm of its individual rows.",4.1. Parseval Regularization,[0],[0]
Robustness for ‖.‖∞ is achieved by rescaling the rows so that their 1-norm is smaller than 1.,4.1. Parseval Regularization,[0],[0]
"For now, we only experimented with constraints on the 2-norm of the rows, so we aim for robustness in the sense of ‖.‖2.
",4.1. Parseval Regularization,[0],[0]
Remark 1 (Orthogonality is required).,4.1. Parseval Regularization,[0],[0]
"Without orthogonality, constraints on the 2-norm of the rows of weight ma-
trices are not sufficient to control the spectral norm.",4.1. Parseval Regularization,[0],[0]
"Parseval networks are thus fundamentally different from weight normalization (Salimans & Kingma, 2016).
",4.1. Parseval Regularization,[0],[0]
Aggregation Layers:,4.1. Parseval Regularization,[0],[0]
"In parseval networks, aggregation layers do not make the sum of their inputs, but rather take a convex combination of them:
n(x) = ∑
n′:(n,n′)∈E
α(n,n ′)n′(x)
with ∑ n′:(n,n′)∈E α (n,n′) = 1 and α(n,n ′) ≥ 0.",4.1. Parseval Regularization,[0],[0]
"The parameters α(n,n ′) are learnt, but using (5), these constraint guarantee that Λ(n)p ≤ 1 as soon as the children satisfy the inequality for the same p-norm.",4.1. Parseval Regularization,[0],[0]
Orthonormality constraints: The first significant difference between Parseval networks and its vanilla counterpart is the orthogonality constraint on the weight matrices.,4.2. Parseval Training,[0],[0]
"This requirement calls for an optimization algorithm on the manifold of orthogonal matrices, namely the Stiefel manifold.",4.2. Parseval Training,[0],[0]
"Optimization on matrix manifolds is a well-studied topic (see (Absil et al., 2009) for a comprehensive survey).",4.2. Parseval Training,[0],[0]
The simplest first-order geometry approaches consist in optimizing the unconstrained function of interest by moving in the direction of steepest descent (given by the gradient of the function) while at the same time staying on the manifold.,4.2. Parseval Training,[0],[0]
"To guarantee that we remain in the manifold after every parameter update, we need to define a retraction operator.",4.2. Parseval Training,[0],[0]
"There exist several pullback operators for embedded submanifolds such as the Stiefel manifold based for example on Cayley transforms (Absil et al., 2009).",4.2. Parseval Training,[0],[0]
"However, when learning the parameters of neural networks, these methods are computationally prohibitive.",4.2. Parseval Training,[0],[0]
"To overcome this difficulty, we use an approximate operator derived from the following layer-wise regularizer of weight matrices to ensure their parseval tightness (Kovačević & Chebira, 2008):
Rβ(Wk) =",4.2. Parseval Training,[0],[0]
"β
2 ‖W>k Wk − I‖22.
Optimizing Rβ(Wk) to convergence after every gradient descent step (w.r.t the main objective) guarantees us to stay on the desired manifold but this is an expensive procedure.",4.2. Parseval Training,[0],[0]
"Moreover, it may result in parameters that are far from the ones obtained after the main gradient update.",4.2. Parseval Training,[0],[0]
"We use two approximations to make the algorithm more efficient: First, we only do one step of descent on the function Rα(Wk).",4.2. Parseval Training,[0],[0]
The gradient of this regularization term is∇WkRβ(Wk) =,4.2. Parseval Training,[0],[0]
β(WkW > k − I)Wk.,4.2. Parseval Training,[0],[0]
"Consequently, after every main update we perform the following secondary update:
Wk ← (1 + β)Wk",4.2. Parseval Training,[0],[0]
"− βWkW>k Wk.
Algorithm 1 Parseval Training Θ = {Wk,αk}Kk=1, e← 0",4.2. Parseval Training,[0],[0]
"while e ≤ E do
Sample a minibatch {(xi, yi)}Bi=1.",4.2. Parseval Training,[0],[0]
"for k ∈ {1, . . .",4.2. Parseval Training,[0],[0]
",K} do
Compute the gradient:",4.2. Parseval Training,[0],[0]
"GWk ← ∇Wk`(Θ, {(xi, yi)}), Gαk ← ∇αk`(Θ, {(xi, yi)}).",4.2. Parseval Training,[0],[0]
Update the parameters: Wk ←Wk − ·GWk αk ← αk − ·Gαk .,4.2. Parseval Training,[0],[0]
"if hidden layer then
Sample a subset S of rows of Wk.",4.2. Parseval Training,[0],[0]
Projection: WS ← (1 + β)WS,4.2. Parseval Training,[0],[0]
− βWSW>S WS .,4.2. Parseval Training,[0],[0]
αk ← argminγ∈∆K−1‖αK,4.2. Parseval Training,[0],[0]
"− γ‖22
e← e+ 1.
Optionally, instead of updating the whole matrix, one can randomly select a subset S of rows and perform the update from Eq.",4.2. Parseval Training,[0],[0]
(4.2) on the submatrix composed of rows indexed by S. This sampling based approach reduces the overall complexity to O(|S|2d).,4.2. Parseval Training,[0],[0]
"Provided the rows are carefully sampled, the procedure is an accurate Monte Carlo approximation of the regularizer loss function (Drineas et al., 2006).",4.2. Parseval Training,[0],[0]
"The optimal sampling probabilities, also called statistical leverages are approximately equal if we start from an orthogonal matrix and (approximately) stay on the manifold throughout the optimization since they are proportional to the eigenvalues of W (Mahoney et al., 2011).",4.2. Parseval Training,[0],[0]
"Therefore, we can sample a subset of columns uniformly at random when applying this projection step.
",4.2. Parseval Training,[0],[0]
"While the full update does not result in an increased overhead for convolutional layers, the picture can be very different for large fully connected layers making the sampling approach computationally more appealing for such layers.",4.2. Parseval Training,[0],[0]
We show in the experiments that the weight matrices resulting from this procedure are (quasi)-orthogonal.,4.2. Parseval Training,[0],[0]
"Also, note that quasi-orthogonalization procedures similar to the one described here have been successfully used previously in the context of learning overcomplete representations with independent component analysis (Hyvärinen & Oja, 2000).
",4.2. Parseval Training,[0],[0]
"Convexity constraints in aggregation layers: In Parseval networks, aggregation layers output a convex combination of their inputs instead of e.g., their sum as in Residual networks (He et al., 2016).",4.2. Parseval Training,[0],[0]
"For an aggregation node n of the network, let us denote by α = (α(n,n
′))n′:(n,n′)∈E the K-size vector of coefficients used for the convex combination output by the layer.",4.2. Parseval Training,[0],[0]
"To ensure that the Lipschitz constant at the node n is such that Λ(n)p ≤ 1, the constraints of 4.1 call for a euclidean projection of α onto the positive simplex after a gradient update:
α∗ = argmin γ∈∆K−1 ‖α− γ‖22 ,
where ∆K−1 = {γ ∈ RK |1>γ = 1,γ ≥ 0}.",4.2. Parseval Training,[0],[0]
"This is a well studied problem (Michelot, 1986; Pardalos & Kovoor, 1990; Duchi et al., 2008; Condat, 2016).",4.2. Parseval Training,[0],[0]
"Its solution is of the form: α∗i = max(0, αi − τ(α)), with τ : RK → R the unique function satisfying ∑ i(xi − τ(α))",4.2. Parseval Training,[0],[0]
= 1 for every x ∈ RK .,4.2. Parseval Training,[0],[0]
"Therefore, the solution essentially boils down to a soft thresholding operation.",4.2. Parseval Training,[0],[0]
If we denote α1 ≥ α2 ≥ . . .,4.2. Parseval Training,[0],[0]
"αK the sorted coefficients and k(α) = max{k ∈ (1, . . .",4.2. Parseval Training,[0],[0]
",K)|1+kαk > ∑ j≤k αj}, the optimal thresholding is given by (Duchi et al., 2008):
τ(α) =",4.2. Parseval Training,[0],[0]
"( ∑ j≤k(α) αj)− 1 k(α)
",4.2. Parseval Training,[0],[0]
"Consequently, the complexity of the projection is O(K log(K))",4.2. Parseval Training,[0],[0]
since it is only dominated by the sorting of the coefficients and is typically cheap because aggregation nodes will only have few children in practice (e.g. 2).,4.2. Parseval Training,[0],[0]
"If the number of children is large, there exist efficient linear time algorithms for finding the optimal thresholding τ(α) (Michelot, 1986; Pardalos & Kovoor, 1990; Condat, 2016).",4.2. Parseval Training,[0],[0]
"In this work, we use the method detailed above (Duchi et al., 2008) to perform the projection of the coefficient α after every gradient update step.",4.2. Parseval Training,[0],[0]
"We evaluate the effectiveness of Parseval networks on well-established image classification benchmark datasets namely MNIST, CIFAR-10, CIFAR-100 (Krizhevsky, 2009) and Street View House Numbers (SVHN) (Netzer et al.).",5. Experimental evaluation,[0],[0]
We train both fully connected networks and wide residual networks.,5. Experimental evaluation,[0],[0]
"The details of the datasets, the models, and the training routines are summarized below.",5. Experimental evaluation,[0],[0]
CIFAR.,5.1. Datasets,[0],[0]
Each of the CIFAR datasets is composed of 60K natural scene color images of size 32 × 32 split between 50K training images and 10K test images.,5.1. Datasets,[0],[0]
CIFAR-10 and CIFAR-100 have respectively 10 and 100 classes.,5.1. Datasets,[0],[0]
"For these two datasets, we adopt the following standard preprocessing and data augmentation scheme (Lin et al., 2013;",5.1. Datasets,[0],[0]
"He et al., 2016; Huang et al., 2016a; Zagoruyko & Komodakis, 2016): Each training image is first zero-padded with 4 pixels on each side.",5.1. Datasets,[0],[0]
The resulting image is randomly cropped to produce a new 32 × 32 image which is subsequently horizontally flipped with probability 0.5.,5.1. Datasets,[0],[0]
We also normalize every image with the mean and standard deviation of its channels.,5.1. Datasets,[0],[0]
"Following the same practice as (Huang et al., 2016a), we initially use 5K images from the training as a validation set.",5.1. Datasets,[0],[0]
"Next, we train de novo the best model on the full set of 50K images and report the results on the test set.",5.1. Datasets,[0],[0]
SVHN The Street View House Number dataset is a set of 32× 32 color digit images officially split into 73257 training images and 26032 test images.,5.1. Datasets,[0],[0]
"Following common practice (Zagoruyko & Komodakis, 2016; He et al., 2016; Huang et al., 2016a;b), we randomly sample 10000 images from the available extra set of about 600K images as a validation set and combine the rest of the pictures with the official training set.",5.1. Datasets,[0],[0]
We divide the pixel values by 255 as a preprocessing step and report the test set performance of the best performing model on the validation set.,5.1. Datasets,[0],[0]
ConvNet Models.,5.2. Models and Implementation details,[0],[0]
"For the CIFAR and SVHN datasets, we trained wide residual networks (Zagoruyko & Komodakis, 2016) as they perform on par with standard resnets (He et al., 2016) while being faster to train thanks to a reduced depth.",5.2. Models and Implementation details,[0],[0]
We used wide resnets of depth 28 and width 10 for both CIFAR-10 and CIFAR-100.,5.2. Models and Implementation details,[0],[0]
For SVHN we used wide resnet of depth 16 and width 4.,5.2. Models and Implementation details,[0],[0]
"For each architecture, we compare Parseval networks with the vanilla model trained with standard regularization both in the adversarial and the non-adversarial training settings.
",5.2. Models and Implementation details,[0],[0]
ConvNet Training.,5.2. Models and Implementation details,[0],[0]
We train the networks with stochastic gradient descent using a momentum of 0.9.,5.2. Models and Implementation details,[0],[0]
"On CIFAR datasets, the initial learning rate is set to 0.1 and scaled by a factor of 0.2 after epochs 60, 120 and 160, for a total number of 200 epochs.",5.2. Models and Implementation details,[0],[0]
We used mini-batches of size 128.,5.2. Models and Implementation details,[0],[0]
"For SVHN, we trained the models with mini-batches of size 128 for 160 epochs starting with a learning rate of 0.01 and decreasing it by a factor of 0.1 at epochs 80 and 120.",5.2. Models and Implementation details,[0],[0]
"For all the vanilla models, we applied by default weight decay regularization (with parameter λ = 0.0005) together with batch normalization and dropout since this combination resulted in better accuracy and increased robustness in preliminary experiments.",5.2. Models and Implementation details,[0],[0]
"The dropout rate use
is 0.3 for CIFAR and 0.4 for SVHN.",5.2. Models and Implementation details,[0],[0]
"For Parseval regularized models, we choose the value of the retraction parameter to be β = 0.0003 for CIFAR datasets and β = 0.0001 for SVHN based on the performance on the validation set.",5.2. Models and Implementation details,[0],[0]
"In all cases, We also adversarially trained each of the models on CIFAR-10 and CIFAR-100 following the guidelines in (Goodfellow et al., 2015; Shaham et al., 2015; Kurakin et al., 2016).",5.2. Models and Implementation details,[0],[0]
"In particular, we replace 50% of the examples of every minibatch by their adversarially perturbed version generated using the one-step method to avoid label leaking (Kurakin et al., 2016).",5.2. Models and Implementation details,[0],[0]
"For each mini-batch, the magnitude of the adversarial perturbation is obtained by sampling from a truncated Gaussian centered at 0 with standard deviation 2.
Fully Connected Model.",5.2. Models and Implementation details,[0],[0]
We also train feedforward networks composed of 4 fully connected hidden layers of size 2048 and a classification layer.,5.2. Models and Implementation details,[0],[0]
The input to these networks are images unrolled into a C × 1024 dimensional vector where C is the number of channels.,5.2. Models and Implementation details,[0],[0]
We used these models on MNIST and CIFAR-10 mainly to demonstrate that the proposed approach is also useful on non-convolutional networks.,5.2. Models and Implementation details,[0],[0]
We compare a Parseval networks to vanilla models with and without weight decay regularization.,5.2. Models and Implementation details,[0],[0]
"For adversarially trained models, we follow the guidelines previously described for the convolutional networks.
",5.2. Models and Implementation details,[0],[0]
Fully Connected Training.,5.2. Models and Implementation details,[0],[0]
We train the models with SGD and divide the learning rate by two every 10 epochs.,5.2. Models and Implementation details,[0],[0]
We use mini-batches of size 100 and train the model for 50 epochs.,5.2. Models and Implementation details,[0],[0]
We chose the hyperparameters on the validation set and retrain the model on the union of the training and validation sets.,5.2. Models and Implementation details,[0],[0]
"The hyperparameters are β, the size of the row subset S, the learning rate and its decrease rate.",5.2. Models and Implementation details,[0],[0]
Using a subset S of 30% of all the rows of each of weight matrix for the retraction step worked well in practice.,5.2. Models and Implementation details,[0],[0]
We first validate that Parseval training (Algorithm 1) indeed yields (near)-orthonormal weight matrices.,5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
"To do so, we analyze the spectrum of the weight matrices of the different models by plotting the histograms of their singular values, and compare these histograms for Parseval networks to networks trained using standard SGD with and without weight decay (SGD-wd and SGD).
",5.3.1. (QUASI)-ORTHOGONALITY.,[0.9514549657981178],"['Our transformation preserves all of the alignments generated by SCFG, and retains properties such as O(n3k) parsing complexity for grammars of rank k. We plan to verify whether rank-k PL-RSTAG is more powerful than rank-k SCFG in future work, and to reduce the rank of the transformed grammar if possible.']"
The histograms representing the distribution of singular values at layers 1 and 4 for the fully connected network (using S = 30%) trained on the dataset CIFAR-10 are shown in Fig. 2 (the figures for convolutional networks are similar).,5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
The singular values obtained with our method are tightly concentrated around 1.,5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
"This experiment confirms that the weight matrices produced by the proposed opti-
mization procedure are (almost) orthonormal.",5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
"The distribution of the singular values of the weight matrices obtained with SGD has a lot more variance, with nearly as many small values as large ones.",5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
"Adding weight decay to standard SGD leads to a sparse spectrum for the weight matrices, especially in the higher layers of the network suggesting a low-rank structure.",5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
"This observation has motivated recent work on compressing deep neural networks (Denton et al., 2014).",5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
"We evaluate the robustness of the models to adversarial noise by generating adversarial examples from the test set, for various magnitudes of the noise vector.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Following common practice (Kurakin et al., 2016), we use the fast gradient sign method to generate the adversarial examples (using ‖.‖∞, see Section 3.1).",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Since these adversarial examples transfer from one network to the other, the fast gradient sign method allows to benchmark the network for reasonable settings where the opponent does not know the network.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
We report the accuracy of each model as a function of the magnitude of the noise.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"To make the results easier to interpret, we compute the corresponding Signal to Noise Ratio (SNR).",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"For an input x and perturbation δx, the SNR is defined as SNR(x, δx) = 20 log10 ‖x‖2 ‖δx‖2 .",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"We show some adversarial examples in Fig. 1.
Fully Connected Nets.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
Figure 3 depicts a comparison of Parseval and vanilla networks with and without adversarial training at various noise levels.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"On both MNIST and CIFAR-10, Parseval networks consistently outperforms weight decay regularization.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"In addition, it is as robust as
adversarial training (SGD-wd-da) on CIFAR-10.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Combining Parseval Networks and adversarial training results in the most robust method on MNIST.
ResNets.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Table 1 summarizes the results of our experiments with wide residual Parseval and vanilla networks on CIFAR-10, CIFAR-100 and SVHN.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"In the table, we denote Parseval(OC) the Parseval network with orthogonality constraint and without using a convex combination in aggregation layers.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
Parseval indicates the configuration where both of the orthogonality and convexity constraints are used.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
We first observe that Parseval networks outperform vanilla ones on all datasets on the clean examples and match the state of the art performances on CIFAR-10 (96.28%) and SVHN (98.44%).,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"On CIFAR-100, when we use Parseval wide Resnet of depth 40 instead of 28, we achieve an accuracy of 81.76%.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"In comparison, the best performance achieved by a vanilla wide resnet (Zagoruyko & Komodakis, 2016) and a pre-activation resnet (He et al., 2016) are respectively 81.12% and 77.29%.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Therefore, our proposal is a useful regularizer for legitimate examples.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Also note that in most cases, Parseval networks combining both the orthogonality constraint and the convexity constraint is superior to use the orthogonality constraint solely.
",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0.9532009692823771],"['Like the original GNF transformation for CFGs our construction at most cubes the grammar size, though when applied to the kinds of synchronous grammars used in machine translation the size is merely squared.']"
The results presented in the table validate our most important claim: Parseval networks significantly improve the robustness of vanilla models to adversarial examples.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"When no adversarial training is used, the gap in accuracy be-
tween the two methods is significant (particularly in the high noise scenario).",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"For an SNR value of 40, the best Parseval network achieves 55.41% accuracy while the best vanilla model is at 44.62%.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"When the models are adversarially trained, Parseval networks remain superior to vanilla models in most cases.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Interestingly, adversarial training only slightly improves the robustness of Parseval networks in low noise setting (e.g. SNR values of 45-50) and sometimes even deteriorates it (e.g. on CIFAR-10).",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"In contrast, combining adversarial training and Parseval networks is an effective approach in the high noise setting.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"This result suggests that thanks to the particular form of regularizer (controlling the Lipschitz constant of the network), Parseval networks achieves robustness to adversarial examples located in the immediate vicinity of each data point.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0.9594785364375814],['This highlights the need for future work to examine the generative power of rank-k PL-RSTAG relative to rankk SCFG in the interest of reducing the rank of the transformed grammar.']
"Therefore, adversarial training only helps for adversarial examples found further away from the legitimate patterns.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"This observation holds consistently across all our datasets.
",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Better use of capacity Given the distribution of singular values observed in Figure 2, we want to analyze the intrinsic dimensionality of the representation learned by the different networks at every layer.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"To that end, we use the local covariance dimension (Dasgupta & Freund, 2008) which can be measured from the covariance matrix of the data.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"For each layer k of the fully connected network, we compute the activation’s empirical covariance matrix 1 n",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"∑n i=1 φk(x)φk(x)
",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
>,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
and obtain its sorted eigenvalues σ1 ≥ · · · ≥ σd.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"For each method and each layer, we select the smallest integer p such that ∑p i=1",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
σi ≥ 0.99 ∑d i=1,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
σi.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
This gives us the number of dimensions that we need to explain 99% of the covariance.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"We can also compute the same quantity for the examples of each class, by only considering in the empirical estimation of the covariance of the examples xi such that yi = c. Table 2 report these numbers for all examples and the per-class average on CIFAR-10.
Table 2 shows that the local covariance dimension of all the data is consistently higher for Parseval networks than all the other approaches at any layer of the network.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"SGDwd-da contracts all the data in very low dimensional spaces at the upper levels of the network by using only 0.4% of the total dimension (layer 3 and 4) while Parseval networks use about 81% and 56% at of the whole dimension respectively
in the same layers.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"This is intriguing given that SGD-wd-da also increases the robustness of the network, apparently not in the same way as Parseval networks.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"For the average local covariance dimension of the classes, SGD-wd-da contracts each class into the same dimensionality as it contracts all the data at the upper layers of the network.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"For Parseval, the data of each class is contracted in about 30% and 19% of the overall dimension.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"These results suggest that Parseval contracts the data of each class in a lower dimensional manifold (compared to the intrinsic dimensionality of the whole data) hence making classification easier.
faster convergence Parseval networks converge significantly faster than vanilla networks trained with batch normalization and dropout as depicted by figure 4.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Thanks to the orthogonalization step following each gradient update, the weight matrices are well conditioned at each step during the optimization.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
We hypothesize this is the main explanation of this phenomenon.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"For convolutional networks (resnets), the faster convergence is not obtained at the expense of larger wall-time since the cost of the projection step is negligible compared to the total cost of the forward pass on modern GPU architecture thanks to the small size of the filters.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"We introduced Parseval networks, a new approach for learning neural networks that are intrinsically robust to adversarial noise.",6. Conclusion,[0],[0]
We proposed an algorithm that allows us to optimize the model efficiently.,6. Conclusion,[0],[0]
Empirical results on three classification datasets with fully connected and wide residual networks illustrate the performance of our approach.,6. Conclusion,[0],[0]
"As a byproduct of the regularization we propose, the model trains faster and makes a better use of its capacity.",6. Conclusion,[0],[0]
Further investigation of this phenomenon is left to future work.,6. Conclusion,[0],[0]
"The authors would like to thank M.A. Ranzato, Y. Tian, A. Bordes and F. Perronnin for their valuable feedback on this work.",Acknowledgements,[0],[0]
"We introduce Parseval networks, a form of deep neural networks in which the Lipschitz constant of linear, convolutional and aggregation layers is constrained to be smaller than 1.",abstractText,[0],[0]
Parseval networks are empirically and theoretically motivated by an analysis of the robustness of the predictions made by deep neural networks when their input is subject to an adversarial perturbation.,abstractText,[0],[0]
"The most important feature of Parseval networks is to maintain weight matrices of linear and convolutional layers to be (approximately) Parseval tight frames, which are extensions of orthogonal matrices to non-square matrices.",abstractText,[0],[0]
We describe how these constraints can be maintained efficiently during SGD.,abstractText,[0],[0]
"We show that Parseval networks match the state-of-the-art in terms of accuracy on CIFAR-10/100 and Street View House Numbers (SVHN), while being more robust than their vanilla counterpart against adversarial examples.",abstractText,[0],[0]
"Incidentally, Parseval networks also tend to train faster and make a better usage of the full capacity of the networks.",abstractText,[0],[0]
Parseval Networks: Improving Robustness to Adversarial Examples,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2331–2336, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics
We recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve a new state of the art for constituency Penn Treebank parsing — 93.8 F1 on section 23, using 2-21 as training, 24 as development, plus tri-training. When trees are converted to Stanford dependencies, UAS and LAS are 95.9% and 94.1%.",text,[0],[0]
"Recent work on deep learning syntactic parsing models has achieved notably good results, e.g., Dyer et al. (2016) with 92.4 F1 on Penn Treebank constituency parsing and Vinyals et al. (2015) with 92.8 F1.",1 Introduction,[0],[0]
"In this paper we borrow from the approaches of both of these works and present a neural-net parse reranker that achieves very good results, 93.8 F1, with a comparatively simple architecture.
",1 Introduction,[0],[0]
In the remainder of this section we outline the major difference between this and previous work — viewing parsing as a language modeling problem.,1 Introduction,[0],[0]
Section 2 looks more closely at three of the most relevant previous papers.,1 Introduction,[0],[0]
"We then describe our exact model (Section 3), followed by the experimental setup and results (Sections 4 and 5).",1 Introduction,[0],[0]
"Formally, a language model (LM) is a probability distribution over strings of a language:
P (x) = P (x1, · · · , xn)
=
n∏
t=1
P (xt|x1, · · · , xt−1), (1)
where x is a sentence and t indicates a word position.",1.1 Language Modeling,[0],[0]
"The efforts in language modeling go into computing P (xt|x1, · · · , xt−1), which as described next is useful for parsing as well.",1.1 Language Modeling,[0],[0]
"A generative parsing model parses a sentence (x) into its phrasal structure (y) according to
argmax y′∈Y(x)
P (x,y′),
where Y(x) lists all possible structures of x.",1.2 Parsing as Language Modeling,[0],[0]
"If we think of a tree (x,y) as a sequence (z) (Vinyals et
2331
al., 2015) as illustrated in Figure 1, we can define a probability distribution over (x,y) as follows:
P (x,y) = P (z) = P (z1, · · · , zm)
= m∏
t=1
P (zt|z1, · · · , zt−1), (2)
which is equivalent to Equation (1).",1.2 Parsing as Language Modeling,[0],[0]
"We have reduced parsing to language modeling and can use language modeling techniques of estimating P (zt|z1, · · · , zt−1) for parsing.",1.2 Parsing as Language Modeling,[0],[0]
We look here at three neural net (NN) models closest to our research along various dimensions.,2 Previous Work,[0],[0]
"The first (Zaremba et al., 2014) gives the basic language modeling architecture that we have adopted, while the other two (Vinyals et al., 2015; Dyer et al., 2016) are parsing models that have the current best results in NN parsing.",2 Previous Work,[0],[0]
"The LSTM-LM of Zaremba et al. (2014) turns (x1, · · · , xt−1) into ht, a hidden state of an LSTM (Hochreiter and Schmidhuber, 1997; Gers et al., 2003; Graves, 2013), and uses ht to guess xt:
P (xt|x1, · · · , xt−1) = P (xt|ht) = softmax(Wht)[xt],
where W is a parameter matrix and [i] indexes ith element of a vector.",2.1 LSTM-LM,[0],[0]
"The simplicity of the model makes it easily extendable and scalable, which has inspired a character-based LSTM-LM that works well for many languages (Kim et al., 2016) and an ensemble of large LSTM-LMs for English with astonishing perplexity of 23.7 (Jozefowicz et al., 2016).",2.1 LSTM-LM,[0.951705971596098],"['GNF has a variety of theoretical and practical applications, including for example the proofs of the famous theorems due to Shamir and Chomsky-Schützenberger (Shamir, 1967; Chomsky and Schützenberger, 1963; Autebert et al., 1997).']"
"In this paper, we build a parsing model based on the LSTM-LM of Zaremba et al. (2014).",2.1 LSTM-LM,[0],[0]
"Vinyals et al. (2015) observe that a phrasal structure (y) can be expressed as a sequence and build a machine translation parser (MTP), a sequence-tosequence model, which translates x into y using a
conditional probability:
P (y|x)",2.2 MTP,[0],[0]
"= P (y1, · · · , yl|x)
= l∏
t=1
P (yt|x, y1, · · · , yt−1),
where the conditioning event (x, y1, · · · , yt−1) is modeled by an LSTM encoder and an LSTM decoder.",2.2 MTP,[0],[0]
"The encoder maps x into he, a set of vectors that represents x, and the decoder obtains a summary vector (h′t) which is concatenation of the decoder’s hidden state (hdt ) and weighted sum of word representations ( ∑n i=1 αih e i ) with an alignment vector (α).",2.2 MTP,[0],[0]
Finally the decoder predicts yt given h′t.,2.2 MTP,[0],[0]
"Inspired by MTP, our model processes sequential trees.",2.2 MTP,[0],[0]
"Recurrent Neural Network Grammars (RNNG), a generative parsing model, defines a joint distribution over a tree in terms of actions the model takes to generate the tree (Dyer et al., 2016):
P (x,y) = P (a) =
m∏
t=1
P (at|a1, · · · , at−1), (3)
where a is a sequence of actions whose output precisely matches the sequence of symbols in z, which implies Equation (3) is the same as Equation (2).",2.3 RNNG,[0],[0]
"RNNG and our model differ in how they compute the conditioning event (z1, · · · , zt−1): RNNG combines hidden states of three LSTMs that keep track of actions the model has taken, an incomplete tree the model has generated and words the model has generated whereas our model uses one LSTM’s hidden state as shown in the next section.",2.3 RNNG,[0],[0]
"Our model, the model of Zaremba et al. (2014) applied to sequential trees and we call LSTM-LM from now on, is a joint distribution over trees:
P (x,y)",3 Model,[0],[0]
"= P (z) = m∏
t=1
P (zt|z1, · · · , zt−1)
=
m∏
t=1
P (zt|ht)
= m∏
t=1
softmax(Wht)[zt],
where ht is a hidden state of an LSTM.",3 Model,[0],[0]
"Due to lack of an algorithm that searches through an exponentially large phrase-structure space, we use an n-best parser to reduce Y(x) to Y ′(x), whose size is polynomial, and use LSTM-LM to find y that satisfies
argmax y′∈Y ′(x)
P (x,y′).",3 Model,[0],[0]
(4),3 Model,[0],[0]
"The model has three LSTM layers with 1,500 units and gets trained with truncated backpropagation through time with mini-batch size 20 and step size 50.",3.1 Hyper-parameters,[0],[0]
"We initialize starting states with previous minibatch’s last hidden states (Sutskever, 2013).",3.1 Hyper-parameters,[0],[0]
"The forget gate bias is initialized to be one (Jozefowicz et al., 2015) and the rest of model parameters are sampled from U(−0.05, 0.05).",3.1 Hyper-parameters,[0],[0]
"Dropout is applied to non-recurrent connections (Pham et al., 2014) and gradients are clipped when their norm is bigger than 20 (Pascanu et al., 2013).",3.1 Hyper-parameters,[0],[0]
"The learning rate is 0.25 · 0.85max( −15, 0) where is an epoch number.",3.1 Hyper-parameters,[0],[0]
"For simplicity, we use vanilla softmax over an entire vocabulary as opposed to hierarchical softmax (Morin and Bengio, 2005) or noise contrastive estimation (Gutmann and Hyvärinen, 2012).",3.1 Hyper-parameters,[0],[0]
"We describe datasets we use for evaluation, detail training and development processes.1",4 Experiments,[0],[0]
"We use the Wall Street Journal (WSJ) of the Penn Treebank (Marcus et al., 1993) for training (2-21), development (24) and testing (23) and millions of auto-parsed “silver” trees (McClosky et al., 2006; Huang et al., 2010; Vinyals et al., 2015) for tritraining.",4.1 Data,[0],[0]
"To obtain silver trees, we parse the entire section of the New York Times (NYT) of the fifth Gigaword (Parker et al., 2011) with a product of eight Berkeley parsers (Petrov, 2010)2 and ZPar (Zhu et al., 2013) and select 24 million trees on which both parsers agree (Li et al., 2014).",4.1 Data,[0],[0]
"We do not resample trees to match the sentence length distribution of the NYT to that of the WSJ (Vinyals et
1The code and trained models used for experiments are available at github.com/cdg720/emnlp2016.
",4.1 Data,[0],[0]
2We use the reimplementation by Huang et al. (2010).,4.1 Data,[0],[0]
"al., 2015) because in preliminary experiments Charniak parser (Charniak, 2000) performed better when trained on all of 24 million trees than when trained on resampled two million trees.
",500 97.0 91.8 40.0,[0],[0]
"Given x, we produce Y ′(x), 50-best trees, with Charniak parser and find y with LSTM-LM as Dyer et al. (2016) do with their discriminative and generative models.3",500 97.0 91.8 40.0,[0],[0]
"We unk words that appear fewer than 10 times in the WSJ training (6,922 types) and drop activations with probability 0.7.",4.2.1 Supervision,[0],[0]
"At the beginning of each epoch, we shuffle the order of trees in the training data.",4.2.1 Supervision,[0],[0]
Both perplexity and F1 of LSTM-LM (G) improve and then plateau (Figure 2).,4.2.1 Supervision,[0],[0]
"Perplexity, the
3Dyer et al. (2016)’s discriminative model performs comparably to Charniak (89.8 vs. 89.7).
model’s training objective, nicely correlates with F1, what we care about.",4.2.1 Supervision,[0],[0]
Training takes 12 hours (37 epochs) on a Titan X. We also evaluate our model with varying n-best trees including optimal 51-best trees that contain gold trees (51o).,4.2.1 Supervision,[0],[0]
"As shown in Table 1, the LSTM-LM (G) is robust given sufficiently large n, i.e. 50, but does not exhibit its full capacity because of search errors in Charniak parser.",4.2.1 Supervision,[0],[0]
We address this problem in Section 5.3.,4.2.1 Supervision,[0],[0]
"We unk words that appear at most once in the training (21,755 types).",4.2.2 Semi-supervision,[0],[0]
"We drop activations with probability 0.45, smaller than 0.7, thanks to many silver trees, which help regularization.",4.2.2 Semi-supervision,[0],[0]
"We train LSTM-LM (GS) on the WSJ and a different set of 400,000 NYT trees for each epoch except for the last one during which we use the WSJ only.",4.2.2 Semi-supervision,[0],[0]
Training takes 26 epochs and 68 hours on a Titan X. LSTMLM (GS) achieves 92.5 F1 on the development.,4.2.2 Semi-supervision,[0],[0]
"As shown in Table 2, with 92.6 F1 LSTM-LM (G) outperforms an ensemble of five MTPs (Vinyals et al., 2015) and RNNG (Dyer et al., 2016), both of which are trained on the WSJ only.",5.1 Supervision,[0],[0]
"We compare LSTM-LM (GS) to two very strong semi-supervised NN parsers: an ensemble of five MTPs trained on 11 million trees of the highconfidence corpus4 (HC) (Vinyals et al., 2015); and an ensemble of six one-to-many sequence models
4The HC consists of 90,000 gold trees, from the WSJ, English Web Treebank and Question Treebank, and 11 million silver trees, whose sentence length distribution matches that of the WSJ, parsed and agreed on by Berkeley parser and ZPar.
trained on the HC and 4.5 millions of EnglishGerman translation sentence pairs (Luong et al., 2016).",5.2 Semi-supervision,[0],[0]
We also compare LSTM-LM (GS) to best performing non-NN parsers in the literature.,5.2 Semi-supervision,[0],[0]
Parsers’ parsing performance along with their training data is reported in Table 3.,5.2 Semi-supervision,[0],[0]
LSTM-LM (GS) outperforms all the other parsers with 93.1 F1.,5.2 Semi-supervision,[0],[0]
"Due to search errors – good trees are missing in 50-best trees – in Charniak (G), our supervised and semi-supervised models do not exhibit their full potentials when Charniak (G) provides Y ′(x).",5.3 Improved Semi-supervision,[0],[0]
"To mitigate the search problem, we tri-train Charniak (GS) on all of 24 million NYT trees in addition to the WSJ, to yield Y ′(x).",5.3 Improved Semi-supervision,[0],[0]
"As shown in Table 3, both LSTM-LM (G) and LSTM-LM (GS) are affected by the quality of Y ′(x).",5.3 Improved Semi-supervision,[0],[0]
"A single LSTM-LM (GS) together with Charniak (GS) reaches 93.6 and an ensemble of eight LSTM-LMs (GS) with Charniak (GS) achieves a new state of the art, 93.8 F1.",5.3 Improved Semi-supervision,[0],[0]
"When trees are converted to Stanford dependencies,5 UAS and LAS are 95.9% and 94.1%,6 more than 1% higher than those of the state of the art dependency parser (Andor et al., 2016).",5.3 Improved Semi-supervision,[0],[0]
"Why an indirect method (converting trees to dependencies) is more accurate than a direct one (dependency parsing) remains unanswered (Kong and Smith, 2014).",5.3 Improved Semi-supervision,[0],[0]
The generative parsing model we presented in this paper is very powerful.,6 Conclusion,[0],[0]
"In fact, we see that a generative parsing model, LSTM-LM, is more effective than discriminative parsing models (Dyer et al., 2016).",6 Conclusion,[0],[0]
"We suspect building large models with character embeddings would lead to further improvement as in language modeling (Kim et al., 2016; Jozefowicz et al., 2016).",6 Conclusion,[0],[0]
We also wish to develop a complete parsing model using the LSTMLM framework.,6 Conclusion,[0],[0]
"We thank the NVIDIA corporation for its donation of a Titan X GPU, Tstaff of Computer Science
5Version 3.3.0.",Acknowledgments,[0],[0]
6We use the CoNLL evaluator available through the CoNLL website: ilk.uvt.nl/conll/software/eval.pl.,Acknowledgments,[0],[0]
"Following the convention, we ignore punctuation.
at Brown University for setting up GPU machines and David McClosky for helping us train Charniak parser on millions trees.",Acknowledgments,[0],[0]
"We recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve a new state of the art for constituency Penn Treebank parsing — 93.8 F1 on section 23, using 2-21 as training, 24 as development, plus tri-training.",abstractText,[0],[0]
"When trees are converted to Stanford dependencies, UAS and LAS are 95.9% and 94.1%.",abstractText,[0],[0]
Parsing as Language Modeling,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 69–81 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"While parsing has become a relatively mature technology for written text, parser performance on conversational speech lags behind.",1 Introduction,[0],[0]
"Speech poses challenges for parsing: transcripts may contain errors and lack punctuation; even perfect transcripts can be difficult to handle because of disfluencies (restarts, repetitions, and self-corrections), filled pauses (“um”, “uh”), interjections (“like”), parentheticals (“you know”, “I mean”), and sentence fragments.",1 Introduction,[0],[0]
"Some of these phenomena can be handled in standard grammars, but disfluencies typically require extensions of the model.",1 Introduction,[0],[0]
"Different approaches have been explored in both constituency parsing (Charniak and Johnson, 2001; Johnson and Charniak, 2004) and dependency parsing (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014).
∗Equal Contribution.
",1 Introduction,[0],[0]
"Despite these challenges, speech carries helpful extra information – beyond the words – associated with the prosodic structure of an utterance and encoded via variation in timing and intonation.",1 Introduction,[0],[0]
"Speakers pause in locations that are correlated with syntactic structure (Grosjean et al., 1979), and listeners use prosodic structure in resolving syntactic ambiguities (Price et al., 1991).",1 Introduction,[0],[0]
"Prosodic cues also signal disfluencies by marking the interruption point (Shriberg, 1994).",1 Introduction,[0],[0]
"However, most speech parsing systems in practice take little advantage of these cues.",1 Introduction,[0],[0]
"Our study focuses on this last challenge, aiming to incorporate prosodic cues in a neural parser, handling disfluencies as constituents via a neural attention mechanism.
",1 Introduction,[0],[0]
"A challenge of incorporating prosody in parsing is that multiple acoustic cues interact to signal prosodic structure, including pauses, lengthening, fundamental frequency modulation, and spectral shape.",1 Introduction,[0],[0]
"These cues also vary with the phonetic segment, emphasis, emotion and speaker, so feature extraction typically involves multiple time windows and normalization techniques.",1 Introduction,[0],[0]
"The most successful constituent parsers have mapped these features to prosodic boundary posteriors by using labeled training data (Kahn et al., 2005; Hale et al., 2006; Dreyer and Shafran, 2007).",1 Introduction,[0.9519213059419953],"['This work investigates the formal properties of prefix lexicalized synchronous grammars as employed by Watanabe et al. (2006) and Siahbani et al. (2013), which have received little theoretical attention compared to non-synchronous prefix lexicalized grammars.']"
The approach proposed here takes advantage of advances in neural networks to automatically learn a good feature representation without the need to explicitly represent prosodic constituents.,1 Introduction,[0],[0]
"To narrow the scope of this work and facilitate error analysis, our experiments use known transcripts and sentence segmentation.
",1 Introduction,[0],[0]
Our work offers the following contributions.,1 Introduction,[0],[0]
We introduce a framework for directly integrating acoustic-prosodic features with text in a neural encoder-decoder parser that does not require handannotated prosodic structure.,1 Introduction,[0],[0]
"We demonstrate improvements in constituent parsing of conversational
69
speech over a high-quality text-only parser and provide analyses showing where prosodic features help and that assessment of their utility is affected by human transcription errors.",1 Introduction,[0],[0]
Our model maps a sequence of word-level input features to a linearized parse output sequence.,2 Task and Model Description,[0],[0]
"The word-level input feature vector consists of the concatenation of (learnable) word embeddings ei and several types of acoustic-prosodic features, described in Section 2.3.",2 Task and Model Description,[0],[0]
"We assume the availability of a training treebank of conversational speech (in our case, SwitchboardNXT (Calhoun et al., 2010)) and corresponding constituent parses.",2.1 Task Setup,[0],[0]
The transcriptions are preprocessed by removing punctuation and lower-casing all text to better mimic the speech recognition setting.,2.1 Task Setup,[0],[0]
"Following Vinyals et al. (2015), the parse trees are linearized, and pre-terminals are normalized as “XX” (see Appendix A.1).",2.1 Task Setup,[0],[0]
Our attention-based encoder-decoder model is similar to the one used by Vinyals et al. (2015).,2.2 Encoder-Decoder Parser,[0],[0]
"The encoder is a deep long short-term memory recurrent neural network (LSTM-RNN) (Hochreiter and Schmidhuber, 1997) that reads in a word-level inputs,1 represented as a sequence of vectors x = (x1, · · · ,xTs), and outputs high-level features h = (h1, · · · ,hTs) where hi = LSTM(xi,hi−1).2
",2.2 Encoder-Decoder Parser,[0],[0]
"The parse decoder is also a deep LSTM-RNN that predicts the linearized parse sequence y = (y1, · · · , yTo) as follows:
P (y|x) = To∏
t=1
P (yt|h,y<t)
",2.2 Encoder-Decoder Parser,[0],[0]
"In attention-based models, the posterior distribution of the output yt at time step t is given by:
P (yt|h,y<t) =",2.2 Encoder-Decoder Parser,[0],[0]
softmax(W s[ct;dt],2.2 Encoder-Decoder Parser,[0],[0]
"+ bs),
where vector bs and matrix W s are learnable parameters; ct is referred to as a context vector that summarizes the encoder’s output h; and dt is the
1As in Vinyals et al. (2015)",2.2 Encoder-Decoder Parser,[0],[0]
"the input sequence is processed in reverse order, as shown in Figure 1.
",2.2 Encoder-Decoder Parser,[0],[0]
2For brevity we omit the LSTM equations.,2.2 Encoder-Decoder Parser,[0],[0]
"The details can be found, e.g., in Zaremba et al. (2014).
",2.2 Encoder-Decoder Parser,[0],[0]
"decoder hidden state at time step t, which captures the previous output sequence context y<t.
uit = v >",2.2 Encoder-Decoder Parser,[0],[0]
tanh(W 1hi,2.2 Encoder-Decoder Parser,[0],[0]
"+W 2dt + ba)
",2.2 Encoder-Decoder Parser,[0],[0]
"αt = softmax(ut) ct = Ts∑
i=1
αtihi
where vectors v, ba and matrices W 1, W 2 are learnable parameters; ut and αt are the attention score and attention weight vector, respectively, for decoder time step t.
The above attention mechanism is only contentbased, i.e., it is only dependent on hi, dt.",2.2 Encoder-Decoder Parser,[0],[0]
"It is not location-aware, i.e., it does not consider the “location” of the previous attention vector.",2.2 Encoder-Decoder Parser,[0],[0]
"For parsing conversational text, location awareness is beneficial since disfluent structures can have duplicate words/phrases that may “confuse” the attention mechanism.
",2.2 Encoder-Decoder Parser,[0],[0]
"In order to make the model location-aware, the attention mechanism takes into account the previous attention weight vector αt−1.",2.2 Encoder-Decoder Parser,[0],[0]
"In particular, we use the attention mechanism proposed by Chorowski et al. (2015), in which αt−1 is represented via a feature vector f t = F ∗αt−1, where F ∈ Rk×r represents k learnable convolution filters of width r. The filters are used for performing 1-D convolution over αt−1 to extract k features f ti for each time step i of the input sequence.",2.2 Encoder-Decoder Parser,[0],[0]
"The extracted features are then incorporated in the alignment score calculation as:
uit = v >",2.2 Encoder-Decoder Parser,[0],[0]
tanh(W 1hi,2.2 Encoder-Decoder Parser,[0],[0]
+W 2dt,2.2 Encoder-Decoder Parser,[0],[0]
"+W ff ti + ba)
where W f is another learnable parameter matrix.",2.2 Encoder-Decoder Parser,[0],[0]
"Finally, the decoder state dt is computed as dt = LSTM([ỹt−1; ct−1],dt−1), where ỹt−1 is the embedding vector corresponding to the previous output symbol yt−1.",2.2 Encoder-Decoder Parser,[0],[0]
"As we will see in Sec. 4.1, the location-aware attention mechanism is especially useful for handling disfluencies.",2.2 Encoder-Decoder Parser,[0],[0]
"In previous work using encoder-decoder models for parsing (Vinyals et al., 2015; Luong et al., 2016), vector xi is simply the word embedding ei of the word at position i of the input sentence.",2.3 Acoustic-Prosodic Features,[0],[0]
"For parsing conversational speech, we can incorporate acousticprosodic features.",2.3 Acoustic-Prosodic Features,[0],[0]
"Here we explore four types of features widely used in computational models of prosody: pauses, duration lengthening, fundamental frequency, and energy.",2.3 Acoustic-Prosodic Features,[0],[0]
"Since prosodic cues are
at sub- and multi-word time scales, they are integrated with the encoder-decoder using different mechanisms.
",2.3 Acoustic-Prosodic Features,[0],[0]
All features are extracted from transcriptions that are time-aligned at the word level.3,2.3 Acoustic-Prosodic Features,[0],[0]
We use time alignments associated with the corpus to be consistent with other studies.,2.3 Acoustic-Prosodic Features,[0],[0]
"In a small number of cases, the time alignment for a particular word boundary is missing.",2.3 Acoustic-Prosodic Features,[0],[0]
Some cases are due to tokenization.,2.3 Acoustic-Prosodic Features,[0],[0]
"For example, contractions, such as don’t in the original transcript, are treated as separated words for the parser (do and n’t), and the internal word boundary time is missing.",2.3 Acoustic-Prosodic Features,[0],[0]
"In such cases, these internal times are estimated.",2.3 Acoustic-Prosodic Features,[0],[0]
"In other cases, there are transcription mismatches that lead to missing time alignments, where we cannot estimate times.",2.3 Acoustic-Prosodic Features,[0],[0]
"For the roughly 1% of sentences where time alignments are missing, we simply back off to the text-based parser.
Pause.",2.3 Acoustic-Prosodic Features,[0],[0]
"The pause feature vector pi for word i is the concatenation of pre-word pause feature ppre,i and post-word pause feature ppost,i, where each subvector is a learned embedding for 6 pause categories: no pause, missing, 0 < p ≤ 0.05 s, 0.05 s < p ≤ 0.2 s, 0.2 < p ≤ 1 s, and p > 1 s (including turn boundaries).",2.3 Acoustic-Prosodic Features,[0],[0]
The bins are chosen based on the observed distribution (see Appendix A.1).,2.3 Acoustic-Prosodic Features,[0],[0]
"We did not use (real-valued) pause duration directly, for two main reasons: (1) to handle missing time alignments; and (2) duration of pause does
3The assumption of known word alignments is standard for prosodic feature extraction in many spoken language processing studies.",2.3 Acoustic-Prosodic Features,[0],[0]
"Time alignments can be obtained as a by-product of recognition or from forced alignment.
not matter beyond a threshold (e.g. p > 1 s).
",2.3 Acoustic-Prosodic Features,[0],[0]
Word duration.,2.3 Acoustic-Prosodic Features,[0],[0]
"Both word duration and wordfinal duration lengthening are strong cues to prosodic phrase boundaries (Wightman et al., 1992; Pate and Goldwater, 2013).",2.3 Acoustic-Prosodic Features,[0],[0]
"The word duration feature δi is computed as the actual word duration divided by the mean duration of the word, clipped to a maximum value of 5.",2.3 Acoustic-Prosodic Features,[0],[0]
The sample mean is used for frequent words (count ≥ 15).,2.3 Acoustic-Prosodic Features,[0],[0]
For infrequent words we estimate the mean as the sum over the sample means for the phonemes in the word’s dictionary pronunciation.,2.3 Acoustic-Prosodic Features,[0],[0]
"We refer to the manually defined prosodic feature pair of pi and δi as φi.
Fundament frequency (f0) and Energy (E) contours (f0/E).",2.3 Acoustic-Prosodic Features,[0],[0]
We use a CNN to automatically learn the mapping from the time series of f0/E features to a word-level vector.,2.3 Acoustic-Prosodic Features,[0],[0]
"The contour features are extracted from 25-ms frames with 10-ms hops using Kaldi (Povey et al., 2011).",2.3 Acoustic-Prosodic Features,[0],[0]
"Three f0 features are used: warped Normalized Cross Correlation Function (NCCF), log-pitch with Probability of Voicing (",2.3 Acoustic-Prosodic Features,[0],[0]
"POV)-weighted mean subtraction over a 1.5-second window, and the estimated derivative (delta) of the raw log pitch.",2.3 Acoustic-Prosodic Features,[0],[0]
"Three energy features are extracted from the Kaldi 40-mel-frequency filter bank features: Etotal, the log of total energy normalized by dividing by the speaker side’s max total energy; Elow, the log of total energy in the lower 20 mel-frequency bands, normalized by total energy, and Ehigh, the log of total energy in the higher 20 mel-frequency bands, normalized by total energy.",2.3 Acoustic-Prosodic Features,[0],[0]
"Multi-band energy features are used as a
simple mechanism to capture articulatory strengthening at prosodic constituent onsets (Fourgeron and Keating, 1997).
",2.3 Acoustic-Prosodic Features,[0],[0]
Figure 1 summarizes the feature learning approach.,2.3 Acoustic-Prosodic Features,[0],[0]
The f0 and E features are processed at the word level: each sequence of frames corresponding to a time-aligned word (and potentially its surrounding context) is convolved with N filters of m sizes (a total of mN filters).,2.3 Acoustic-Prosodic Features,[0],[0]
The motivation for the multiple filter sizes is to enable the computation of features that capture information on different time scales.,2.3 Acoustic-Prosodic Features,[0],[0]
"For each filter, we perform a 1-D convolution over the 6-dimensional f0/E features with a stride of 1.",2.3 Acoustic-Prosodic Features,[0],[0]
"Each filter output is max-pooled, resulting in mN -dimensional speech features si.",2.3 Acoustic-Prosodic Features,[0],[0]
"Our overall acoustic-prosodic feature vector is the concatenation of pi, δi, and si in various combinations.",2.3 Acoustic-Prosodic Features,[0],[0]
"Our core corpus is Switchboard-NXT (Calhoun et al., 2010), a subset of the Switchboard corpus (Godfrey and Holliman, 1993): 2,400 telephone conversations between strangers; 642 of these were hand-annotated with syntactic parses and further augmented with richer layers of annotation facilitated by the NITE XML toolkit (Calhoun et al., 2010).",3.1 Dataset,[0],[0]
"Our sentence segmentations and syntactic trees are based on the annotations from the Treebank set, with a few manual corrections from the NXT release.",3.1 Dataset,[0],[0]
"This core dataset consists of 100K sentences, totaling 830K tokens forming a vocabulary of 13.5K words.",3.1 Dataset,[0],[0]
"We use the time alignments available from NXT, which is based on a corrected word transcript that occasionally differs from the Treebank, leading to some missing time alignments.",3.1 Dataset,[0],[0]
"We follow the sentence boundaries defined by the parsed data available,4 and the data split (90% train; 5% dev; 5% test) defined by related work done on Switchboard (Charniak and Johnson, 2001; Kahn et al., 2005; Honnibal and Johnson, 2014).",3.1 Dataset,[0],[0]
"The standard evaluation metric for constituent parsing is the parseval metric which uses bracketing precision, recall, and F1, as in the canonical implementation of EVALB.5",3.2 Evaluation Metrics and Baselines,[0],[0]
"For written text, punc-
4Note that these sentence units can be inconsistent with other layers of Switchboard annotations, such as slash units.
",3.2 Evaluation Metrics and Baselines,[0],[0]
"5http://nlp.cs.nyu.edu/evalb/
tuation is sometimes represented as part of the sequence and impacts the final score, but for speech the punctuation is not explicitly available so it does not contribute to the score.",3.2 Evaluation Metrics and Baselines,[0],[0]
Another challenge of transcribed speech is the presence of disfluencies.,3.2 Evaluation Metrics and Baselines,[0],[0]
"Speech repairs are indicated under “EDITED” nodes in Switchboard parse trees, which include structure under these nodes that is not of interest for simple text clean-up.",3.2 Evaluation Metrics and Baselines,[0],[0]
"Therefore, some studies report flattened-edit parseval F1 scores (“flatF1”), which is parseval computed on trees where the structure under edit nodes has been eliminated so that all leaves are immediate children.",3.2 Evaluation Metrics and Baselines,[0],[0]
"We report both scores for the baseline text-only model showing that the differences are small, then use the standard parseval F1 score for most results.6
Disfluencies are particularly problematic for statistical parsers, as explained by Charniak and Johnson (2001), and some systems incorporate a separate disfluency detection stage.",3.2 Evaluation Metrics and Baselines,[0],[0]
"For this reason, and because it is useful for understanding system performance, most studies also report disfluency detection performance, which is measured in terms of the F1 score for detecting whether a word is in an edit region.",3.2 Evaluation Metrics and Baselines,[0],[0]
"Our approach does not involve a separate disfluency detection stage, but identifies disfluencies implicitly via the parse structure.",3.2 Evaluation Metrics and Baselines,[0],[0]
"Consequently, the disfluency detection results are not competitive with work that directly optimize for disfluency detection.",3.2 Evaluation Metrics and Baselines,[0],[0]
"We report disfluency detection scores primarily as a diagnostic.
",3.2 Evaluation Metrics and Baselines,[0],[0]
"Most previous work on integrating prosody and parsing has used the Switchboard corpus, but it is still difficult to compare results because of differences in constraints, objectives and the use of constituent vs. dependency structure, as discussed further in Section 6.",3.2 Evaluation Metrics and Baselines,[0],[0]
The most relevant prior studies (on constituent parsing) that we compare to are a bit old.,3.2 Evaluation Metrics and Baselines,[0],[0]
The text-only result from our neural parser represents a stronger baseline and is important for decoupling the impact of prosody vs. the parsing framework.,3.2 Evaluation Metrics and Baselines,[0.9558316075873768],"['Table 1 shows the actual size increase on a variety of grammars: here |G| is the size of the initial grammar, |H| is the size after applying our transformation, and the increase is expressed as a power of the original grammar size.']"
Both the encoder and decoder are 3-layer deep LSTM-RNNs with 256 hidden units in each layer.,3.3 Model Training and Inference,[0],[0]
"For the location-aware attention, the convolution operation uses 5 filters of width 40 each.",3.3 Model Training and Inference,[0],[0]
"We use 512-dimensional embedding vectors to repre-
6A variant of the “flat-F1” score is used in (Charniak and Johnson, 2001; Kahn et al., 2005), which uses a relaxed edited node precision and recall but also ignores filled pauses.
sent words and linearized parsing symbols, such as “(S”.7
A number of configurations are explored for the acoustic-prosodic features, tuning based on dev set parsing performance.",3.3 Model Training and Inference,[0],[0]
"Pause embeddings are tuned over {4, 16, 32} dimensions.",3.3 Model Training and Inference,[0],[0]
"For the CNN, we try different configurations of filter widths w ∈ {",3.3 Model Training and Inference,[0],[0]
"[10, 25, 50], [5, 10, 25, 50]} and number of filters N ∈ {16, 32, 64, 128} for each filter width.8 These filter size combinations are chosen to capture f0 and energy phenomena on various levels: w = 5, 10 for sub-word, w = 25 for word, and w = 50 for word and extended context.",3.3 Model Training and Inference,[0],[0]
Our best model uses 32-dimensional pause embeddings and N = 32 filters of widthsw =,3.3 Model Training and Inference,[0],[0]
"[5, 10, 25, 50], which corresponds to m = 4 and 128 filters.
",3.3 Model Training and Inference,[0],[0]
"For optimization we use Adam (Kingma and Ba, 2014) with a minibatch size of 64.",3.3 Model Training and Inference,[0],[0]
"The initial learning rate is 0.001 which is decayed by a factor of 0.9 whenever training loss, calculated after every 500 updates, degrades relative to the worst of its previous 3 values.",3.3 Model Training and Inference,[0],[0]
All models are trained for up to 50 epochs with early stopping.,3.3 Model Training and Inference,[0],[0]
"For regularization, dropout with 0.3 probability is applied on the output of all LSTM layers (Pham et al., 2014).
",3.3 Model Training and Inference,[0],[0]
"For inference, we use a greedy decoder to generate the linearized parse.",3.3 Model Training and Inference,[0],[0]
The output token with maximum posterior probability is chosen at every time step and fed as input in the next time step.,3.3 Model Training and Inference,[0],[0]
The decoder stops upon producing the end-of-sentence symbol.,3.3 Model Training and Inference,[0],[0]
"We use TensorFlow (Abadi et al., 2015) to implement all models.9",3.3 Model Training and Inference,[0],[0]
"7The number of layers, dimension of hidden units, dimension of embedding, and convolutional attention filter parameters of the text-only parser were explored in earlier experiments on the development set and then fixed as described.
8Note that a filter of width 10 has size 6 × 10, since the features are of dimension 6.
",4.1 Text-only Results,[0],[0]
"9Our code resources can be found in Appendix A.1.
",4.1 Text-only Results,[0],[0]
"We first show our results on the model using only text (i.e. xi = ei) to establish a strong baseline, on top of which we can add acousticprosodic features.",4.1 Text-only Results,[0],[0]
We experiment with the contentonly attention model used by Vinyals et al. (2015) and the content+location attention of Chorowski et al. (2015).,4.1 Text-only Results,[0],[0]
"For comparison with previous nonneural models, we use a high-quality latent-variable parser, the Berkeley parser (Petrov et al., 2006), retrained on our Switchboard data.",4.1 Text-only Results,[0],[0]
Table 1 compares the three text-only models.,4.1 Text-only Results,[0],[0]
"In terms of F1, the content+location attention beats the Berkeley parser by about 2.5% and content-only attention by about 4.5%.",4.1 Text-only Results,[0],[0]
"Flat-F1 scores for both encoder-decoder models is lower than their corresponding F1 scores, suggesting that the encoder-decoder models do well on predicting the internal structure of EDIT nodes while the reverse is true for the Berkeley parser.
",4.1 Text-only Results,[0],[0]
"To explain the gains of content+location attention over content-only attention, we compare their scores on fluent (without EDIT nodes) and disfluent sentences, shown in Table 1.",4.1 Text-only Results,[0],[0]
It is clear that most of the gains for content+location attention are from disfluent sentences.,4.1 Text-only Results,[0],[0]
"A possible explanation is the presence of duplicate words or phrases in disfluent sentences, which can be problematic for a contentonly attention model.",4.1 Text-only Results,[0.958273718262827],"['A grammar in GNF is said to be prefix lexicalized, because the prefix of every production is a lexical item.']"
"Since our best model is the content+location attention model, we will henceforth refer to it as the “CL-attn” text-only model.",4.1 Text-only Results,[0],[0]
"All models using acoustic-prosodic features are extensions of this model, which provides a strong text-only baseline.",4.1 Text-only Results,[0],[0]
"We extend our CL-attn model with the three kinds of acoustic-prosodic features: pause (p), word duration (δ), and CNN mappings of fundamental frequency (f0) and energy (E) features (f0/E-CNN).
",4.2 Adding Acoustic-Prosodic Features,[0],[0]
The results of several model configurations on our dev set are presented in Table 2.,4.2 Adding Acoustic-Prosodic Features,[0],[0]
"First, we note that adding any combination of acoustic-prosodic features (individually or in sets) improves performance over the text-only baseline.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"However, certain combinations of acoustic-prosodic features are not always better than their subsets.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
The text + p + δ + f0/E-CNN model that uses all three types of features has the best performance with a gain of 0.7% over the already-strong text-only baseline.,4.2 Adding Acoustic-Prosodic Features,[0],[0]
"We will henceforth refer to the text + p + δ + f0/E-CNN model as our “best model”.
",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"As a robustness check, we report results of averaging 10 runs on the CL-attn text-only and the best model in Table 3.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"We performed a bootstrap test (Efron and Tibshirani, 1993) that simulates 105 random test draws on the models giving median performance on the dev set.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
These median models gave a statistically significant difference between the text-only and best model (p-value < 0.02).,4.2 Adding Acoustic-Prosodic Features,[0],[0]
"Additionally, a simple t-test over the two sets of 10 results also shows statistical significance p-value < 0.03.
",4.2 Adding Acoustic-Prosodic Features,[0],[0]
Table 4 presents the results on the test set.,4.2 Adding Acoustic-Prosodic Features,[0],[0]
"Again, adding the acoustic-prosodic features improves over the text-only baseline.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"The gains are statistically significant for the best model with p-value < 0.02, again using a bootstrap test with simulated 105 random test draws on the two models.
",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"Table 5 includes results from prior studies that compare systems using text alone with ones that incorporate prosody, given hand transcripts and sentence segmentation.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"It is difficult to compare systems directly, because of the many differences in the experimental set-up.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"For example, the original Charniak and Johnson (2001) result (reporting F=85.9 for parsing and F=78.2 for disfluencies) leverages punctuation in the text stream, which is not realistic for speech transcripts and not used in most other work.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"Our work benefits from more text training material than others, but others benefit from gold part-of-speech tags.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
Kahn et al. (2005) use a modified sentence segmentation.,4.2 Adding Acoustic-Prosodic Features,[0],[0]
There are probably minor differences in handling of word fragments and scoring edit regions.,4.2 Adding Acoustic-Prosodic Features,[0],[0]
"Thus, this table primarily shows that our framework leads to more benefits from sentence-internal prosodic cues than others have obtained.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
Effect of sentence length.,5 Analysis,[0],[0]
"Figure 2 shows performance differences between our best model and the text-only model for varying sentence lengths.
",5 Analysis,[0],[0]
"Both models do worse on longer sentences, as expected since the corresponding parse trees tend to be more complex.",5 Analysis,[0],[0]
The performance difference between our best model and the text-only model increases with sentence length.,5 Analysis,[0],[0]
"This is likely because longer sentences more often have multiple prosodic phrases and disfluencies.
",5 Analysis,[0],[0]
Effect of disfluencies.,5 Analysis,[0],[0]
"Table 6 presents parse scores on the subsets of fluent and disfluent sentences, showing that the performance gain is in the disfluent set (65% of the dev set sentences).",5 Analysis,[0],[0]
"Because sentence boundaries are given, and so many fluent sentences in spontaneous speech are short, there is less potential for benefit from prosody in the fluent set.
",5 Analysis,[0],[0]
Types of errors.,5 Analysis,[0],[0]
"We use the Berkeley Parser Analyzer (Kummerfeld et al., 2012) to compare the types of errors made by the different parsers.10 Table 7 presents the relative error reductions over the text-only baseline achieved by the text + p model and our best model for disfluent sentences.",5 Analysis,[0],[0]
The two models differ in the types of error reductions they provide.,5 Analysis,[0],[0]
"Including pause information gives largest improvements on PP attachment and Modifier at-
10This analysis omits the 1% of the sentences that did not have timing information.
tachment errors.",5 Analysis,[0],[0]
"Adding the remaining acousticprosodic features helps to correct more types of attachment errors, especially VP and NP attachment.",5 Analysis,[0],[0]
Figure 3 demonstrates one case where the pause feature helps in correcting a PP attachment error made by a text-only parser.,5 Analysis,[0],[0]
"Other interesting examples (see Appendix A.2) suggest that the learned f0/E features help reduce NP attachment errors where the audio reveals a prominent word at the constituent boundary, even though there is no pause at that word.
",5 Analysis,[0],[0]
Effect of transcription errors.,5 Analysis,[0],[0]
The results and analyses so far have assumed that we have reliable transcripts.,5 Analysis,[0],[0]
"In fact, the original transcripts contained errors, and the Treebank annotators used these without reference to audio files.",5 Analysis,[0],[0]
"Mississippi State University (MS-State) ran a clean-up project
that produced more accurate word transcripts and time alignments (Deshmukh et al., 1998).",5 Analysis,[0],[0]
"The NXT corpus provides reconciliation between Treebank and MS-State transcripts in terms of annotating missed/extra/substituted words, but parses were not re-annotated.",5 Analysis,[0],[0]
The transcript errors mean that the acoustic signal is inconsistent with the “gold” parse tree.,5 Analysis,[0],[0]
"Below are some examples of “fluent” sentences (according to the Treebank transcripts) with transcription errors, for which prosodic features “hurt” parsing.",5 Analysis,[0],[0]
Words that transcribers missed are in brackets and those inserted are underlined.,5 Analysis,[0],[0]
S1: and because <,5 Analysis,[0],[0]
uh> like if your spouse died <all of a sudden you be> all alone it ’d be nice to go someplace with people similar to you to have friends S2: uh uh <i have had>,5 Analysis,[0],[0]
"my wife ’s picked up a couple of things saying uh boy if we could refinish that ’d be a beautiful piece of furniture
Multi-syllable errors are especially problematic, leading to serious inconsistencies between the text and the acoustic signal.",5 Analysis,[0],[0]
"Further, the missed words lead to an incorrect attachment in the “gold” parse in S1 and a missing restart edit in S2.",5 Analysis,[0],[0]
"Indeed, for sentences with consecutive transcript errors, which we expect to impact the prosodic features, there is a statistically significant (p-value < 0.05) negative effect on parsing with prosody.",5 Analysis,[0],[0]
"Not included in this analysis are sentence boundary errors, which also change the “gold” parse.",5 Analysis,[0],[0]
"Thus, prosody may be more useful than results here indicate.",5 Analysis,[0],[0]
"Related work on parsing conversational speech has mainly addressed four problems: speech recognition errors, unknown sentence segmentation, disfluencies, and integrating prosodic cues.",6 Related Work,[0],[0]
"Our work addresses the last two problems, which involve studies based on hand-transcribed text and known sentence boundaries, as in much speech parsing work.",6 Related Work,[0],[0]
The related studies are thus the focus of this discussion.,6 Related Work,[0],[0]
"We describe studies using the Switchboard corpus, since it has dominated work in this area, being the largest source of treebanked English spontaneous speech.
",6 Related Work,[0],[0]
"One major challenge of parsing conversational speech is the presence of disfluencies, which are grammatical and prosodic interruptions.",6 Related Work,[0],[0]
Disfluencies include repetitions (‘I am +,6 Related Work,[0],[0]
"I am’), repairs (‘I am +",6 Related Work,[0],[0]
"we are’), and restarts (‘What I",6 Related Work,[0],[0]
+,6 Related Work,[0],[0]
"Today is the...’), where the ‘+’ corresponds to an interruption point.",6 Related Work,[0],[0]
"Repairs often involve parallel grammatical
constructions, but they can be more complex, involving hedging, clarifications, etc.",6 Related Work,[0],[0]
"Charniak and Johnson (Charniak and Johnson, 2001; Johnson and Charniak, 2004) demonstrated that disfluencies are different in character than other constituents and that parsing performance improves from combining a PCFG parser with a separate module for disfluency detection via parse rescoring.",6 Related Work,[0],[0]
Our approach does not use a separate disfluency detection module; we hypothesized that the location-sensitive attention model helps handle these differences based on analysis of the text-only results (Table 1).,6 Related Work,[0],[0]
"However, more explicit modeling of disfluency pattern match characteristics in a dependency parser (Honnibal and Johnson, 2014) leads to better disfluency detection performance (F = 84.1 vs. 76.7 for our text only model).",6 Related Work,[0],[0]
"Pattern match features also benefit a neural model for disfluency detection alone (F = 87.0) (Zayats et al., 2016), and similar gains are observed by formulating disfluency detection in a transition-based framework (F = 87.5) (Wang et al., 2017).",6 Related Work,[0.9533151667476949],"['By using prefix lexicalized synchronous context-free grammars (SCFGs), Watanabe et al. (2006) and Siahbani et al. (2013) obtain asymptotic and empirical speed improvements on a machine translation task.']"
"Experiments with oracle disfluencies as features improve the CL-attn text-only parsing performance from 87.85 to 89.38 on the test set, showing that more accurate disfluency modeling is a potential area of improvement.
",6 Related Work,[0],[0]
"It is well known that prosodic features play a role in human resolution of syntactic ambiguities, with more than two decades of studies seeking to incorporate prosodic features in parsing.",6 Related Work,[0],[0]
"A series of studies looked at constituent parsing informed by the presence (or likelihood) of prosodic breaks at word boundaries (Kahn et al., 2004, 2005; Hale et al., 2006; Dreyer and Shafran, 2007).",6 Related Work,[0],[0]
"Our approach improves over performance of these systems using raw acoustic features, without the need for handlabeling prosodic breaks.",6 Related Work,[0],[0]
"The gain is in part due to the improved text-based parser, but the incremental benefit of prosody here is similar to that in these prior studies.",6 Related Work,[0],[0]
"(In prior work using acoustic feature directly (Gregory et al., 2004), prosody actually degraded performance.)",6 Related Work,[0],[0]
"Our analyses of the impact of prosody also extends prior work.
",6 Related Work,[0],[0]
"Prosody is also known to provide useful cues to sentence boundaries (Liu et al., 2006), and automatic sentence segmentation performance has been shown to have a significant impact on parsing performance (Kahn and Ostendorf, 2012).",6 Related Work,[0],[0]
"In our study, sentence boundaries are given so as to focus on the role of prosody in resolving sentenceinternal parse ambiguity, for which prior work had
obtained smaller gains.",6 Related Work,[0],[0]
"Studies have also shown that parsing lattices or confusion networks can improve ASR performance (Kahn and Ostendorf, 2012; Yoshikawa et al., 2016).",6 Related Work,[0],[0]
"Our analysis of performance degradation for the system with prosody when the gold transcript and associated parse are in error suggests that prosody may have benefits for parsers operating on alternative ASR hypotheses.
",6 Related Work,[0],[0]
The results we compare to in Section 4 are relatively old.,6 Related Work,[0],[0]
"More recent parsing results on spontaneous speech involve dependency parsers using only text (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016), with the exception of a study on unsupervised dependency parsing (Pate and Goldwater, 2013).",6 Related Work,[0],[0]
"With the recent success of transition-based neural approaches in dependency parsing, researchers have adapted transition-based ideas to constituent parsing (Zhu et al., 2013; Watanabe and Sumita, 2015; Dyer et al., 2016).",6 Related Work,[0],[0]
"These approaches have not yet been used with speech, to our knowledge, but we expect it to be straightforward to extend our prosody integration framework to these systems, both for dependency and constituency parsing.",6 Related Work,[0],[0]
We have presented a framework for directly integrating acoustic-prosodic features with text in a neural encoder-decoder parser that does not require hand-annotated prosodic structure.,7 Conclusion,[0],[0]
"On conversational sentences, we obtained strong results when including word-level acoustic-prosodic features over using only transcriptions.",7 Conclusion,[0],[0]
"The acousticprosodic features provide the largest gains when sentences are disfluent or long, and analysis of error types shows that these features are especially helpful in repairing attachment errors.",7 Conclusion,[0],[0]
"In cases where prosodic features hurt performance, we observe a statistically significant negative effect caused by imperfect human transcriptions that make the “ground truth” parse tree and the acoustic signal inconsistent, which suggests that there is more to be gained from prosody than observed in prior studies.",7 Conclusion,[0],[0]
"We thus plan to investigate aligning the Treebank and MS-State versions of Switchboard for future work.
",7 Conclusion,[0],[0]
"Here, we assumed known sentence boundaries and hand transcripts, leaving open the question of whether increased benefits from prosody can be gained by incorporating sentence segmentation in parsing and/or in parsing ASR lattices.",7 Conclusion,[0],[0]
"Most prior work using prosody in parsing has been on con-
stituent parsing, since prosodic cues tend to align with constituent boundaries.",7 Conclusion,[0],[0]
"However, it remains an open question as to whether dependency, constituency or other parsing frameworks are better suited to leveraging prosody.",7 Conclusion,[0],[0]
"Our study builds on a parser that uses reverse order text processing, since it provides a stronger text-only baseline.",7 Conclusion,[0],[0]
"However, the prosody modeling component relies only on a 1 second lookahead of the current word (for pause binning), so it could be easily incorporated in an incremental parser.",7 Conclusion,[0],[0]
We thank the anonymous reviewers for their helpful feedback.,Acknowledgement,[0],[0]
"We also thank Pranava Swaroop Madhyastha, Hao Tang, Jon Cai, Hao Cheng, and Navdeep Jaitly for their help with initial discussions and code setup.",Acknowledgement,[0],[0]
"This research was partially funded by a Google Faculty Research Award to Mohit Bansal, Karen Livescu, and Kevin Gimpel; and NSF grant no. IIS-1617176.",Acknowledgement,[0],[0]
The opinions expressed in this work are those of the authors and do not necessarily reflect the views of the funding agency.,Acknowledgement,[0],[0]
"A.1 Miscellany
Our main model code is available at https://github.com/shtoshni92/ speech_parsing.",A Appendix,[0],[0]
Most of the data preprocessing code is available at https://github. com/trangham283/seq2seq_parser/ tree/master/src/data_preps.,A Appendix,[0],[0]
"Part of our data preprocessing pipeline also uses https: //github.com/syllog1sm/swbd_tools.
",A Appendix,[0],[0]
Table 8 shows statistics of our Switchboard dataset.,A Appendix,[0],[0]
"As defined, for example, in (Charniak and Johnson, 2001; Honnibal and Johnson, 2014), the splits are: conversations sw2000 to sw3000 for training, sw4500 to sw4936 for validation (dev), and sw4000 to sw4153 for evaluation (test).",A Appendix,[0],[0]
"In addition, previous work has reserved sw4154 to sw4500 for “future use” (dev2), but we added this set to our training set.",A Appendix,[0],[0]
"That is, all of our models are trained on Switchboard conversations sw2000 to sw3000 as well as sw4154 to sw4500.
",A Appendix,[0],[0]
Figure 4 illustrates the data preprocessing step.,A Appendix,[0],[0]
"On the decoder end, we also use a post-processing step that merges the original sentence with the decoder output to obtain the standard constituent tree representation.",A Appendix,[0],[0]
"During inference, in rare cases (and virtually none as our models converge), the decoder does not generate a valid parse sequence, due to the mismatch in brackets and/or the mismatch in the number of pre-terminals and terminals, i.e., num(XX) 6= num(tokens).",A Appendix,[0],[0]
"In such cases, we simply add/remove brackets from either end of the parse, or add/remove pre-terminal symbols XX in the middle of the parse to match the number of input tokens.
",A Appendix,[0],[0]
Figure 5 shows the distribution of pause durations in our training data.,A Appendix,[0],[0]
"Our pause buckets of
0 < p ≤ 0.05 s, 0.05 s < p ≤ 0.2 s, 0.2 < p ≤ 1 s, and p > 1 s described in the main paper were based on this distribution of pause lengths.
",A Appendix,[0],[0]
"Table 9 shows the comprehensive error counts in all error categories defined in the Berkeley Parse Analyzer (Kummerfeld et al., 2012) in both the fluent and disfluent subsets.
",A Appendix,[0],[0]
"A.2 Tree Examples In figures 6, 7, and 8, we follow node correction notations as in (Kummerfeld et al., 2012).",A Appendix,[0],[0]
"In particular, missing nodes are marked in blue on the gold tree, extra nodes are marked red in the predicted tree, and yellow nodes denote crossing.",A Appendix,[0],[0]
"In conversational speech, the acoustic signal provides cues that help listeners disambiguate difficult parses.",abstractText,[0],[0]
"For automatically parsing spoken utterances, we introduce a model that integrates transcribed text and acoustic-prosodic features using a convolutional neural network over energy and pitch trajectories coupled with an attention-based recurrent neural network that accepts text and prosodic features.",abstractText,[0],[0]
"We find that different types of acoustic-prosodic features are individually helpful, and together give statistically significant improvements in parse and disfluency detection F1 scores over a strong text-only baseline.",abstractText,[0],[0]
"For this study with known sentence boundaries, error analyses show that the main benefit of acousticprosodic features is in sentences with disfluencies, attachment decisions are most improved, and transcription errors obscure gains from prosody.",abstractText,[0],[0]
Parsing Speech: A Neural Approach to Integrating Lexical and Acoustic-Prosodic Information,title,[0],[0]
"General treebank analyses are graph structured, but parsers are typically restricted to tree structures for efficiency and modeling reasons. We propose a new representation and algorithm for a class of graph structures that is flexible enough to cover almost all treebank structures, while still admitting efficient learning and inference. In particular, we consider directed, acyclic, one-endpoint-crossing graph structures, which cover most long-distance dislocation, shared argumentation, and similar tree-violating linguistic phenomena. We describe how to convert phrase structure parses, including traces, to our new representation in a reversible manner. Our dynamic program uniquely decomposes structures, is sound and complete, and covers 97.3% of the Penn English Treebank. We also implement a proofof-concept parser that recovers a range of null elements and trace types.",text,[0],[0]
"Many syntactic representations use graphs and/or discontinuous structures, such as traces in Government and Binding theory and f-structure in Lexical Functional Grammar (Chomsky 1981; Kaplan and Bresnan 1982).",1 Introduction,[0],[0]
"Sentences in the Penn Treebank (PTB, Marcus et al. 1993) have a core projective tree structure and trace edges that represent control structures, wh-movement and more.",1 Introduction,[0],[0]
"However, most parsers and the standard evaluation metric ignore these edges and all null elements.",1 Introduction,[0],[0]
"By leaving out parts of the structure, they fail to provide key relations to downstream tasks such as question answering.",1 Introduction,[0],[0]
"While there has been work on capturing
some parts of this extra structure, it has generally either been through post-processing on trees (Johnson 2002; Jijkoun 2003; Campbell 2004; Levy and Manning 2004; Gabbard et al. 2006) or has only captured a limited set of phenomena via grammar augmentation (Collins 1997; Dienes and Dubey 2003; Schmid 2006; Cai et al. 2011).
",1 Introduction,[0],[0]
We propose a new general-purpose parsing algorithm that can efficiently search over a wide range of syntactic phenomena.,1 Introduction,[0],[0]
"Our algorithm extends a non-projective tree parsing algorithm (Pitler et al. 2013; Pitler 2014) to graph structures, with improvements to avoid derivational ambiguity while maintaining an O(n4) runtime.",1 Introduction,[0],[0]
"Our algorithm also includes an optional extension to ensure parses contain a directed projective tree of non-trace edges.
",1 Introduction,[0],[0]
Our algorithm cannot apply directly to constituency parses–it requires lexicalized structures similar to dependency parses.,1 Introduction,[0],[0]
We extend and improve previous work on lexicalized constituent representations (Shen et al. 2007; Carreras et al. 2008; Hayashi and Nagata 2016) to handle traces.,1 Introduction,[0],[0]
"In this form, traces can create problematic structures such as directed cycles, but we show how careful choice of head rules can minimize such issues.
We implement a proof-of-concept parser, scoring 88.1 on trees in section 23 and 70.6 on traces.",1 Introduction,[0],[0]
"Together, our representation and algorithm cover 97.3% of sentences, far above the coverage of projective tree parsers (43.9%).",1 Introduction,[0],[0]
"This work builds on two areas: non-projective tree parsing, and parsing with null elements.
",2 Background,[0],[0]
"Non-projectivity is important in syntax for rep-
441
Transactions of the Association for Computational Linguistics, vol. 5, pp.",2 Background,[0],[0]
"441–454, 2017.",2 Background,[0],[0]
Action Editor: Marco Kuhlmann.,2 Background,[0],[0]
"Submission batch: 4/2017; Published 11/2017.
",2 Background,[0],[0]
c©2017 Association for Computational Linguistics.,2 Background,[0],[0]
"Distributed under a CC-BY 4.0 license.
",2 Background,[0],[0]
"resenting many structures, but inference over the space of all non-projective graphs is intractable.",2 Background,[0],[0]
"Fortunately, in practice almost all parses are covered by well-defined subsets of this space.",2 Background,[0],[0]
"For dependency parsing, recent work has defined algorithms for inference within various subspaces (GómezRodrı́guez and Nivre 2010; Pitler et al. 2013).",2 Background,[0],[0]
We build upon these algorithms and adapt them to constituency parsing.,2 Background,[0],[0]
"For constituency parsing, a range of formalisms have been developed that are mildlycontext sensitive, such as CCG (Steedman 2000), LFG (Kaplan and Bresnan 1982), and LTAG (Joshi and Schabes 1997).
",2 Background,[0],[0]
"Concurrently with this work, Cao et al. (2017) also proposed a graph version of Pitler et al. (2013)’s One-Endpoint Crossing (1-EC) algorithm.",2 Background,[0],[0]
"However, Cao’s algorithm does not consider the direction of edges1 and so it could produce cycles, or graphs with multiple root nodes.",2 Background,[0],[0]
"Their algorithm also has spurious ambiguity, with multiple derivations of the same parse structure permitted.",2 Background,[0],[0]
"One advantage of their algorithm is that by introducing a new item type it can handle some cases of the Locked-Chain we define below (specifically, when N is even), though in practise they also restrict their algorithm to ignore such cases.",2 Background,[0],[0]
"They also show that the class of graphs they generate corresponds to the 1-EC pagenumber-2 space, a property that applies to this work as well2.
",2 Background,[0],[0]
Parsing with Null Elements in the PTB has taken two general approaches.,2 Background,[0],[0]
"The first broadly effective system was Johnson (2002), which post-processed the output of a parser, inserting extra elements.",2 Background,[0],[0]
"This was effective for some types of structure, such as null complementizers, but had difficulty with long distance dependencies.",2 Background,[0],[0]
The other common approach has been to thread a trace through the tree structure on the non-terminal symbols.,2 Background,[0],[0]
"Collins (1997)’s third model used this approach to recover wh-traces, while Cai et al. (2011) used it to recover null pronouns, and others have used it for a range of movement types (Dienes and Dubey 2003; Schmid 2006).",2 Background,[0],[0]
"These approaches have the disadvantage that each
1 To produce directed edges, their parser treats the direction as part of the edge label.
",2 Background,[0],[0]
2,2 Background,[0],[0]
This is a topological space with two half-planes sharing a boundary.,2 Background,[0],[0]
"All edges are drawn on one of the two half-planes and each half-plane contains no crossings.
",2 Background,[0],[0]
additional trace dramatically expands the grammar.,2 Background,[0],[0]
Our representation is similar to LTAG-Spinal (Shen et al. 2007) but has the advantage that it can be converted back into the PTB representation.,2 Background,[0],[0]
Hayashi and Nagata (2016) also incorporated null elements into a spinal structure but did not include a representation of co-indexation.,2 Background,[0],[0]
"In related work, dependency parsers have been used to assist in constituency parsing, with varying degrees of representation design, but only for trees (Hall, Nivre, and Nilsson 2007; Hall and Nivre 2008; FernándezGonzález and Martins 2015; Kong et al. 2015).
",2 Background,[0],[0]
"Kato and Matsubara (2016) described a new approach, modifying a transition-based parser to recover null elements and traces, with strong results, but using heuristics to determine trace referents.",2 Background,[0],[0]
"Our algorithm is a dynamic program, similar at a high level to CKY (Kasami 1966;",3 Algorithm,[0],[0]
Younger 1967; Cocke 1969).,3 Algorithm,[0],[0]
The states of our dynamic program (items) represent partial parses.,3 Algorithm,[0],[0]
"Usually in CKY, items are defined as covering the n words in a sentence, starting and ending at the spaces between words.",3 Algorithm,[0],[0]
"We follow Eisner (1996), defining items as covering the n−1 spaces in a sentence, starting and ending on words, as shown in Figure 1.",3 Algorithm,[0],[0]
"This means that we process each word’s left and right dependents separately, then combine the two halves.
",3 Algorithm,[0],[0]
"We use three types of items: (1) a single edge, linking two words, (2) a continuous span, going from one word to another, representing all edges linking pairs of words within the span, (3) a span (as defined in 2) plus an additional word outside the span, enabling the inclusion of edges between that word and words in the span.
",3 Algorithm,[0],[0]
"Within the CKY framework, the key to defining our algorithm is a set of rules that specify which items are allowed to combine.",3 Algorithm,[0],[0]
"From a bottom-up perspective, a parse is built in a series of steps, which come in three types: (1) adding an edge to an item, (2) combining two items that have non-overlapping adjacent spans to produce a new item with a larger span, (3) combining three items, similarly to (2).
",3 Algorithm,[0],[0]
"Example: To build intuition for the algorithm, we will describe the derivation in Figure 1.",3 Algorithm,[0],[0]
"Note, item sub-types (I, X, and N) are defined below, and in-
cluded here for completeness.",3 Algorithm,[0],[0]
"(1) We initialize with spans of width one, going between adjacent words, e.g. between ROOT and We. ∅",3 Algorithm,[0],[0]
"7→ I0,1 (2) Edges can be introduced in exactly two ways, either by linking the two ends of a span, e.g. like– running, or by linking one end of a span with a word outside the span, e.g. like–. (which in this case forms a new item that has a span and an external word).
",3 Algorithm,[0],[0]
"I2,3 ∧ like–running 7→ I2,3 I3,4 ∧ like–. 7→ X3,4,2
(3) We add a second edge to one of the items.",3 Algorithm,[0],[0]
"I1,2 ∧ running–We 7→ X1,2,3 (4) Now that all the edges to We have been added, the two items either side of it are combined to form an item that covers it.",3 Algorithm,[0],[0]
"I0,1 ∧ X1,2,3 7→",3 Algorithm,[0],[0]
"N0,2,3 (5) We add an edge, creating a crossing because We is an argument of a word to the right of like.",3 Algorithm,[0],[0]
"N0,2,3 ∧ ROOT–like 7→ N0,2,3 (7) We use a ternary rule to combine three adjacent items.",3 Algorithm,[0],[0]
"In the process we create another crossing.
",3 Algorithm,[0],[0]
"N0,2,3 ∧ I2,3 ∧ X3,4,2 7→ I0,6",3 Algorithm,[0],[0]
"Notation Vertices are p, q, etc.",3.1 Algorithm definition,[0],[0]
"Continuous ranges are [pq], [pq), (pq], or (pq), where the brackets indicate inclusion, [ ], or exclusion, ( ), of each endpoint.",3.1 Algorithm definition,[0],[0]
A span [pq] and vertex o that are part of the same item are [pq.o].,3.1 Algorithm definition,[0],[0]
"Two vertices and an arrow indicate an edge, ~pq.",3.1 Algorithm definition,[0],[0]
"Two vertices without an arrow are an edge in either direction, pq. Ranges and/or vertices connected by a dash define a set of edges, e.g. the
set of edges between o and (pq) is o–(pq) (in some places we will also use this to refer to an edge from the set, rather than the whole set).",3.1 Algorithm definition,[0],[0]
"If there is a path from p to q, q is reachable from p.
Item Types As shown in Figure 1, our items start and end on words, fully covering the spaces in between.",3.1 Algorithm definition,[0],[0]
"Earlier we described three item types: an edge, a span, and a span plus an external vertex.",3.1 Algorithm definition,[0],[0]
"Here we define spans more precisely as I , and divide the span plus an external point case into five types differing in the type of edge crossing they contain: p qI , Interval A span for which there are no edges sr :",3.1 Algorithm definition,[0],[0]
r ∈ (pq) and s /∈,3.1 Algorithm definition,[0],[0]
"[pq].
o X , Exterval An interval and either op or oq, where",3.1 Algorithm definition,[0],[0]
o /∈,3.1 Algorithm definition,[0],[0]
[pq].,3.1 Algorithm definition,[0],[0]
"B, Both A span and vertex [pq.o], for which there are no edges sr : r ∈ (pq) and s /∈",3.1 Algorithm definition,[0],[0]
"[pq] ∪ o. Edges o–[pq] may be crossed by pq, p–(pq) or q–(pq), and at least one crossing of the second and third types occurs.",3.1 Algorithm definition,[0],[0]
Edges o–(pq) may not be crossed by (pq)–(pq) edges.,3.1 Algorithm definition,[0],[0]
"L, Left Same as B, but o–(pq) edges may only cross p–(pq] edges.",3.1 Algorithm definition,[0],[0]
"R, Right Symmetric with L. N , Neither An interval and a vertex [pq.o], with at least one o–(pq) edge, which can be crossed by pq, but no other [pq]–[pq] edges.
",3.1 Algorithm definition,[0],[0]
Items are further specified as described in Alg.,3.1 Algorithm definition,[0],[0]
1.,3.1 Algorithm definition,[0],[0]
"Most importantly, for each pair of o, p, and q in an item, the rules specify whether one is a parent of the other, and if they are directly linked by an edge.
",3.1 Algorithm definition,[0],[0]
"For an item H with span [ij], define covered(H) as (ij), and define visible(H) as {i, j}.",3.1 Algorithm definition,[0],[0]
"When an external vertex x is present, it is in visible(H).",3.1 Algorithm definition,[0],[0]
"Call the union of multiple such sets covered(F,G,H), and visible(F,G,H).
",3.1 Algorithm definition,[0],[0]
Deduction Rules To make the deduction rules manageable,3.1 Algorithm definition,[0],[0]
", we use templates to define some constraints explicitly, and then use code to generate the rules.",3.1 Algorithm definition,[0],[0]
"During rule generation, we automatically apply additional constraints to prevent rules that would leave a word in the middle of a span without a parent or that would form a cycle (proven possible below).",3.1 Algorithm definition,[0],[0]
Algorithm 1 presents the explicit constraints.,3.1 Algorithm definition,[0],[0]
"Once expanded, these give rules that specify all properties for each item (general type, external vertex position
Algorithm 1 Dynamic program for Lock-Free, One-Endpoint Crossing, Directed, Acyclic graph parsing.",3.1 Algorithm definition,[0],[0]
Adding Edges: Consider a span [lr] and vertex x /∈,3.1 Algorithm definition,[0],[0]
[lr].,3.1 Algorithm definition,[0],[0]
"Edges between l and r can be added to items I , N , L, R, and B (making L̂ and N̂ in those cases).",3.1 Algorithm definition,[0],[0]
"Edges between l and x can be added to items I (forming an X), R, and N .",3.1 Algorithm definition,[0],[0]
"Edges between r and x can be added to items I (forming an X), L, and N .",3.1 Algorithm definition,[0],[0]
"The l–r edge cannot be added after another edge, and N items cannot get both l–x and r–x edges.",3.1 Algorithm definition,[0],[0]
Combining Items: In the rules below the following notation is used: For this explanation items are T,3.1 Algorithm definition,[0],[0]
[lr crl clr] and T,3.1 Algorithm definition,[0],[0]
[lrx crl cxl clr cxr clx crx].,3.1 Algorithm definition,[0],[0]
T is the type of item.,3.1 Algorithm definition,[0],[0]
Multiple letters indicate any of those types are allowed.,3.1 Algorithm definition,[0],[0]
"For the next three types of notation, if an item does not have a mark, either option is valid.",3.1 Algorithm definition,[0],[0]
˙ T and T : indicate the number of edges between the external vertex and the span: one or more than one respectively.,3.1 Algorithm definition,[0],[0]
·T and T · indicate the position of the external vertex relative to the item’s span (left or right respectively).,3.1 Algorithm definition,[0],[0]
T̂ indicates for N and L that ∀p ∈,3.1 Algorithm definition,[0],[0]
(ij)∃rs : i≤r<p<s≤j.,3.1 Algorithm definition,[0],[0]
"In (11) and (12) it is optional, but true for output iff true for input.",3.1 Algorithm definition,[0],[0]
"l, r, and x: the position of the left end of the span, the right end, and the external vertex, respectively.",3.1 Algorithm definition,[0],[0]
"crl, cxl, etc: connectivity of each pair of visible vertices, from the first subscript to the second.",3.1 Algorithm definition,[0],[0]
"Using crl as an example, these can be .",3.1 Algorithm definition,[0],[0]
"(unconstrained), d ( ~rl must exist), p (l is reachable from r, but ~rl does not exist), n",3.1 Algorithm definition,[0],[0]
"(l is not reachable from r), d (= p ∨ n), n",3.1 Algorithm definition,[0],[0]
(= d ∨ p).,3.1 Algorithm definition,[0],[0]
Note:,3.1 Algorithm definition,[0],[0]
"In the generated rules every value is d, p, or n, leading to multiple rules per template below.
I[ij nd]← max   (Init) j = i+1 (1) I[i i+1 nn] I[i+1 j nn] maxk∈(i,j)   (2) I[ik nd] I[kj",3.1 Algorithm definition,[0],[0]
..],3.1 Algorithm definition,[0],[0]
(3) BLRN · [ikj nndddd] I[kj ..],3.1 Algorithm definition,[0],[0]
"maxl∈(k,j){ (4) RN ·",3.1 Algorithm definition,[0],[0]
[ikl nndddd] I[kl ..] ·LNX[ljk .d..d.] (5) BLRN · [ikl nndddd] I[kl ..] I[lj ..],3.1 Algorithm definition,[0],[0]
"maxl∈(i,k){ (6) I[il n.] ·LN [lki .d.dnn]",3.1 Algorithm definition,[0],[0]
"·N : [kjl ddd.d.]
(7) RNX· [ilk nn.ddd] I[lk ..]",3.1 Algorithm definition,[0],[0]
"·LN :: [kjl .d..d.]
B· [ijx nndddd]← maxk∈(i,j)  
(8) L̂N̂ ·",3.1 Algorithm definition,[0],[0]
[ikx nn.ddd] R·,3.1 Algorithm definition,[0],[0]
[kjx ...d.d] (9) L̂N̂ ·,3.1 Algorithm definition,[0],[0]
[ikx nn.ddd] N ·,3.1 Algorithm definition,[0],[0]
[kjx d.dd.d] (10) L̂N̂ ·,3.1 Algorithm definition,[0],[0]
[ikx nn.ddd] N ·,3.1 Algorithm definition,[0],[0]
"[kjx d.dd.d]
˙ L̂[ijx dddddd]←",3.1 Algorithm definition,[0],[0]
"maxk∈(i,j){
(11) X[ikx .d.dnn] · L̂N̂",3.1 Algorithm definition,[0],[0]
[kji .d.ddd] (12) X[ikx .d.ddd,3.1 Algorithm definition,[0],[0]
],3.1 Algorithm definition,[0],[0]
· L̂N̂,3.1 Algorithm definition,[0],[0]
"[kji .d.ddd]
L :",3.1 Algorithm definition,[0],[0]
"[ijx dddddd]← maxk∈(i,j)   (13) LN",3.1 Algorithm definition,[0],[0]
[ikx .d.ddd],3.1 Algorithm definition,[0],[0]
·N,3.1 Algorithm definition,[0],[0]
[kji dddddd] (14) LN,3.1 Algorithm definition,[0],[0]
[ikx .d.ddd],3.1 Algorithm definition,[0],[0]
·N,3.1 Algorithm definition,[0],[0]
[kji dddddd] (15) L[ikx .d.ddd] I[kj ..],3.1 Algorithm definition,[0],[0]
(16) L[ikx .d.ddd] I[kj ..],3.1 Algorithm definition,[0],[0]
(17) N,3.1 Algorithm definition,[0],[0]
[ikx dddddd] I[kj ..],3.1 Algorithm definition,[0],[0]
(18) N,3.1 Algorithm definition,[0],[0]
[ikx dddddd] I[kj ..],3.1 Algorithm definition,[0],[0]
(19) N,3.1 Algorithm definition,[0],[0]
[ikx dddddd] I[kj ..,3.1 Algorithm definition,[0],[0]
"]
(20) N",3.1 Algorithm definition,[0],[0]
"[ikx dddddd] I[kj ..]
N :",3.1 Algorithm definition,[0],[0]
"[ijx dddddd]← maxk∈(i,j)   (21) ·N [ikx dddddd] I[kj ..]",3.1 Algorithm definition,[0],[0]
(22) ·N [ikx dddddd] I[kj ..] (23) I[ik ..] N ·,3.1 Algorithm definition,[0],[0]
[kjx dddddd] (24) I[ik ..] N ·,3.1 Algorithm definition,[0],[0]
"[kjx dddddd]
˙ N",3.1 Algorithm definition,[0],[0]
"[ijx dddddd]← maxk∈(i,j)   (25) ·X[ikx .d.ddd] I[kj ..] (26) ·X[ikx .d.ddd] I[kj ..] (27) I[ik ..]",3.1 Algorithm definition,[0],[0]
X· [kjx .d.ddd] (28) I[ik ..],3.1 Algorithm definition,[0],[0]
"X· [kjx .d.ddd]
I[ij pn], ·B[ijx ddnndd], R : [ijx dddddd], and ˙ R[ijx dddddd] are symmetric with cases above.
",3.1 Algorithm definition,[0],[0]
"relative to the item spans, connectivity of every pair of vertices in each item, etc).
",3.1 Algorithm definition,[0],[0]
The final item for n vertices is an interval where the left end has a parent.,3.1 Algorithm definition,[0],[0]
For parsing we assume there is a special root word at the end of the sentence.,3.1 Algorithm definition,[0],[0]
Definition 1.,3.2 Properties,[0],[0]
"A graph is One-Endpoint Crossing if, when drawn with vertices along the edge of a halfplane and edges drawn in the open half-plane above, for any edge e, all edges that cross e share a vertex.",3.2 Properties,[0],[0]
"Let that vertex be Pt(e).
",3.2 Properties,[0],[0]
"Aside from applying to graphs, this is the same as
Pitler et al. (2013)’s 1-EC tree definition.
",3.2 Properties,[0],[0]
Definition 2.,3.2 Properties,[0],[0]
"A Locked-Chain (shown in Fig. 2) is formed by a set of consecutive vertices in order from 0 to N , where N > 3, with edges {(0, N−1), (1, N)} ∪ {(i, i+2)∀i ∈",3.2 Properties,[0],[0]
"[0, N−2]}.",3.2 Properties,[0],[0]
Definition 3.,3.2 Properties,[0],[0]
"A graph is Lock-Free if it does not contain edges that form a Locked-Chain.
",3.2 Properties,[0],[0]
"Note that in practice, most parse structures satisfy 1-EC, and the Locked-Chain structure does not occur in the PTB when using our head rules.
",3.2 Properties,[0],[0]
Theorem 1.,3.2 Properties,[0],[0]
"For the space of Lock-Free OneEndpoint Crossing graphs, the algorithm is sound, complete and gives unique decompositions.
",3.2 Properties,[0],[0]
Our proof is very similar in style and structure to Pitler et al. (2013).,3.2 Properties,[0],[0]
"The general approach is to consider the set of structures an item could represent, and divide them into cases based on properties of the internal structure.",3.2 Properties,[0],[0]
"We then show how each case can be decomposed into items, taking care to ensure all the properties that defined the case are satisfied.",3.2 Properties,[0],[0]
Uniqueness follows from having no ambiguity in how a given structure could be decomposed.,3.2 Properties,[0],[0]
"Completeness and soundness follow from the fact that our rules apply equally well in either direction, and so our top-down decomposition implies a bottom-up formation.",3.2 Properties,[0],[0]
"To give intuition for the proof, we show the derivation of one rule below.",3.2 Properties,[0],[0]
The complete proof can be found in Kummerfeld (2016).,3.2 Properties,[0],[0]
"We do not include it here due to lack of space.
",3.2 Properties,[0],[0]
"We do provide the complete set of rule templates in Algorithm 1, and in the proof of Lemma 2 we show that the case in which an item cannot be decomposed occurs if and only if the graph contains a Locked-Chain.",3.2 Properties,[0],[0]
"To empirically check our rule generation code, we checked that our parser uniquely decomposes all 1-EC parses in the PTB and is unable to decompose the rest.
",3.2 Properties,[0],[0]
"Note that by using subsets of our rules, we can restrict the space of structures we generate, giving parsing algorithms for projective DAGs, projective trees (Eisner 1996), or 1-EC trees (Pitler et al. 2013).",3.2 Properties,[0],[0]
"Versions of these spaces with undirected edges could also be easily handled with the same approach.
",3.2 Properties,[0],[0]
p qs t Derivation of rule (4) in Algorithm 1:,3.2 Properties,[0],[0]
"This rule applies to intervals with the substructure shown, and with no parent in this item for p.",3.2 Properties,[0],[0]
They have at least one p–(pq) edge (otherwise rule 1 applies).,3.2 Properties,[0],[0]
"The longest p–(pq) edge, ps, is crossed (otherwise rule 2 applies).",3.2 Properties,[0],[0]
Let C be the set of (ps)–(sq) edges (note: these cross ps).,3.2 Properties,[0],[0]
"Either all of the edges in C have a common endpoint t ∈ (sq), or if |C| = 1 let t be the endpoint in (sq) (otherwise rule 6 or 7 applies).",3.2 Properties,[0],[0]
Let D be the set of s–(tq) edges.,3.2 Properties,[0],[0]
|D| > 0,3.2 Properties,[0],[0]
"(otherwise rule 3 or 5 applies).
",3.2 Properties,[0],[0]
We will break this into three items.,3.2 Properties,[0],[0]
"First, (st)–(tq] edges would violate the 1-EC property and (st)–[ps) edges do not exist by construction.",3.2 Properties,[0],[0]
"Therefore, the middle item is an Interval [st], the left item is [ps.t], and the right item is [tq.s] (since |C| > 0 and |D| > 0).",3.2 Properties,[0],[0]
"The left item can be either
an N or R, but not an L or B because that would violate the 1-EC property for the C edges.",3.2 Properties,[0],[0]
"The right item can be an X , L, or N , but not an R or B because that would violate the 1-EC property for the D edges.",3.2 Properties,[0],[0]
"We will require edge ps to be present in the first item, and not allow pt.",3.2 Properties,[0],[0]
"To avoid a spurious ambiguity, we also prevent the first or third items from having st (which could otherwise occur in any of the three items).",3.2 Properties,[0],[0]
"Now we have broken down the original item into valid sub-items, and we have ensured that those sub-items contain all of the structure used to define the case in a unique way.
",3.2 Properties,[0],[0]
"Now we will further characterize the nature of the Lock-Free restriction to the space of graphs.
",3.2 Properties,[0],[0]
Lemma 1.,3.2 Properties,[0],[0]
No edge in a Locked-Chain in a 1-EC graph is crossed by edges that are not part of it.,3.2 Properties,[0],[0]
Proof.,3.2 Properties,[0],[0]
"First, note that: Pt((0, N−1))",3.2 Properties,[0],[0]
"= N , Pt((1, N))",3.2 Properties,[0],[0]
"= 0, and {Pt((i, i+2))",3.2 Properties,[0],[0]
= i+1,3.2 Properties,[0],[0]
∀i ∈,3.2 Properties,[0],[0]
"[0, N−2]} Call the set {(i, i+2)∀i ∈",3.2 Properties,[0],[0]
"[0, N−2]}, the chain.
Consider an edge e that crosses an edge f in a Locked-Chain.",3.2 Properties,[0],[0]
"Let ein be the end of e that is between the two ends of f , and eout be the other end.",3.2 Properties,[0],[0]
"One of e’s endpoints is at Pt(f), and Pt(e) is an endpoint of f .",3.2 Properties,[0],[0]
"There are three cases:
(i) f",3.2 Properties,[0],[0]
"= (1, N).",3.2 Properties,[0],[0]
"Here, eout = Pt(f) = 0, and ein ∈ (1, N).",3.2 Properties,[0],[0]
"For all vertices v ∈ (1, N) there is an edge g in the chain such that v is between the endpoints of g. Therefore, e will cross such an edge g. To satisfy the 1-EC property, g must share an endpoint with f , which means g is either (1, 3) or (N−2, N).",3.2 Properties,[0],[0]
"In the first case, the 1-EC property forces e = (0, 2), and in the second e = (0, N−1), both of which are part of the Locked-Chain.
(ii) f = (0, N−1), symmetrical with (i).",3.2 Properties,[0],[0]
"(iii) f = (i, i+2), for some i ∈",3.2 Properties,[0],[0]
"[0, N−2].",3.2 Properties,[0],[0]
"Here, ein = Pt(f) = i+1.",3.2 Properties,[0],[0]
"We can assume e does not cross (0, N−1) or (1, N), as those cases are covered by (i).",3.2 Properties,[0],[0]
"As in (i), e must cross another edge in the chain, and that edge must share an endpoint with f .
",3.2 Properties,[0],[0]
"This forces e to be either (i−1, i+1) or (i+1, i+3) (excluding one or both if they cross (0, N−1) or (1, N)), which are both in the Locked-Chain.
",3.2 Properties,[0],[0]
Our rules define a unique way to decompose almost any item into a set of other items.,3.2 Properties,[0],[0]
"The exception is B, which in some cases can not be divided into two items (i.e. has no valid binary division).
",3.2 Properties,[0],[0]
Lemma 2.,3.2 Properties,[0],[0]
A B[ij.x] has no valid binary division if and only if the graph has a Locked-Chain.,3.2 Properties,[0],[0]
Proof.,3.2 Properties,[0],[0]
Consider the k and l that give the longest ik and lj edges in a B with no valid binary division (at least one edge of each type must exist by definition).,3.2 Properties,[0],[0]
"No vertex in (ik) or (jl) is a valid split point, as they would all require one of the items to have two external vertices.
",3.2 Properties,[0],[0]
"Now, consider p ∈",3.2 Properties,[0],[0]
[kj].,3.2 Properties,[0],[0]
"If there is no edge l1r1, where i ≤ l1 < p < r1 ≤ j, then p would be a valid split point.",3.2 Properties,[0],[0]
"Therefore, such an edge must exist.",3.2 Properties,[0],[0]
"Consider l1, either l1 ∈ (ik) or there is an edge l2c, where i ≤ l2",3.2 Properties,[0],[0]
< l1 < c ≤ j (by the same logic as for l1r1).,3.2 Properties,[0],[0]
"Similarly, either r1 ∈ (jl) or there is an edge cr2 (it must be c to satisfy 1-EC).",3.2 Properties,[0],[0]
"We can also apply this logic to edges l2c and cr2, giving edges l3l1 and r1r3.",3.2 Properties,[0],[0]
This pattern will terminate when it reaches lu ∈ (ik) and rv ∈ (jl) with edges lulu−2 and rv−2rv.,3.2 Properties,[0],[0]
"Note that k = lu−1 and l = rv−1, to satisfy 1-EC.
",3.2 Properties,[0],[0]
"Since it is a B, there must be at least two x–(ij) edges.",3.2 Properties,[0],[0]
"To satisfy 1-EC, these end at lu−1 and rv−1.
",3.2 Properties,[0],[0]
"Let x be to the right (the left is symmetrical), and call i = 0, j = N−1, and x = N .",3.2 Properties,[0],[0]
"Comparing with the Locked-Chain definition, we have all the edges except one: 0 to N−1.",3.2 Properties,[0],[0]
"However, that edge must be present in the overall graph, as all B items start with an ij edge (see rules 3 and 5 in Algorithm 1).",3.2 Properties,[0],[0]
"Therefore, if there is no valid split point for a B, the overall graph must contain a Locked-Chain.
",3.2 Properties,[0],[0]
"Now, for a graph that contains a Locked-Chain, consider the items that contain the Locked-Chain.",3.2 Properties,[0],[0]
"Grouping them by their span [ij], there are five valid options:",3.2 Properties,[0],[0]
"[0, N−1], [1, N ], [0, N ], (i ≤ 0",3.2 Properties,[0],[0]
"∧ j > N ), and (i < 0",3.2 Properties,[0],[0]
∧ j ≥ N ).,3.2 Properties,[0],[0]
"Items of the last three types would be divided by our rules into smaller items, one of which contains the whole Locked-Chain.",3.2 Properties,[0],[0]
"The first two are Bs of the type discussed above.
",3.2 Properties,[0],[0]
"Now we will prove that our code to generate rules from the templates can guarantee a DAG is formed.
",3.2 Properties,[0],[0]
Lemma 3.,3.2 Properties,[0],[0]
"For any item H , ∀v ∈ covered(H) ∃u ∈ visible(H) : v is reachable from u. Proof.",3.2 Properties,[0],[0]
"This is true for initial items because covered(H) = ∅. To apply induction, consider adding edges and combing items.",3.2 Properties,[0],[0]
The lemma clearly remains true when adding an edge.,3.2 Properties,[0],[0]
"Consider combining items E, F , G to form H[ij.x], and assume the lemma is true for E, F , and G (the binary case is similar).",3.2 Properties,[0],[0]
"Since all vertices are reachable from visible(E,F,G), we only need to ensure that ∀v ∈",3.2 Properties,[0],[0]
"visible(E,F,G) ∃u ∈",3.2 Properties,[0],[0]
visible(H) : v is reachable from u.,3.2 Properties,[0],[0]
"The connectivity between all pairs {(u, v) | u ∈ visible(H), v ∈",3.2 Properties,[0],[0]
"visible(E,F,G)} can be inferred from the item definitions, and so this requirement can be enforced in rule generation.
",3.2 Properties,[0],[0]
Lemma 4.,3.2 Properties,[0],[0]
The final item is a directed acyclic graph.,3.2 Properties,[0],[0]
Proof.,3.2 Properties,[0],[0]
"First, consider acyclicity.",3.2 Properties,[0],[0]
Initial items do not contain any edges and so cannot contain a cycle.,3.2 Properties,[0],[0]
"For induction, there are two cases:
(i) Adding an Edge ~pq to an item H: Assume that H does not contain any cycles.",3.2 Properties,[0],[0]
"~pq will create a cycle if and only if p is reachable from q. By construction, p and q ∈ visible(H), and so the item definition contains whether p is reachable from q.
(ii)",3.2 Properties,[0],[0]
"Combining Items: Assume that in isolation, none of the items being combined contain cycles.",3.2 Properties,[0],[0]
"Therefore, a cycle in the combined item must be composed of paths in multiple items.",3.2 Properties,[0],[0]
A path in one item can only continue in another item by passing through a visible vertex.,3.2 Properties,[0],[0]
"Therefore, a cycle would have to be formed by a set of paths between visible vertices.",3.2 Properties,[0],[0]
"But the connectivity of every pair of visible vertices is specified in the item definitions.
",3.2 Properties,[0],[0]
"In both cases, rules that create a cycle can be excluded during rule generation.
",3.2 Properties,[0],[0]
"By induction, the items constructed by our algorithm do not contain cycles.",3.2 Properties,[0],[0]
"Together with Lemma 3 and the final item definition, this means the final structure is an acyclic graph with all vertices reachable from vertex n.
Next, we will show two properties that give intuition for the algorithm.",3.2 Properties,[0],[0]
"Specifically, we will prove which rules add edges that are crossed in the final derivation.
",3.2 Properties,[0],[0]
Lemma 5.,3.2 Properties,[0],[0]
An edge ij added to I[ij] is not crossed.,3.2 Properties,[0],[0]
Proof.,3.2 Properties,[0],[0]
"First, we will show three properties of any pair of items in a derivation (using [ij.x] and [kl.y]).
",3.2 Properties,[0],[0]
(1) It is impossible for either i <,3.2 Properties,[0],[0]
k,3.2 Properties,[0],[0]
< j < l or k,3.2 Properties,[0],[0]
< i,3.2 Properties,[0],[0]
<,3.2 Properties,[0],[0]
"l < j, i.e., items cannot have partially overlapping spans.",3.2 Properties,[0],[0]
"As a base case, the final item is an interval spanning all vertices, and so no other item can partially overlap with it.",3.2 Properties,[0],[0]
"Now assume it is true for an item H and consider the rules in reverse, breaking H up.",3.2 Properties,[0],[0]
"By construction, each rule divides H into items with spans that are adjacent, overlapping only at their visible vertices.",3.2 Properties,[0],[0]
"Since the new items are nested within H , they do not overlap with any items H did not overlap with.",3.2 Properties,[0],[0]
"By induction, no pair of items have partially overlapping spans.
",3.2 Properties,[0],[0]
(2) For items with nested spans (i ≤ k,3.2 Properties,[0],[0]
"< l ≤ j), y ∈",3.2 Properties,[0],[0]
[ij]∪{x}.,3.2 Properties,[0],[0]
"Following the argument for the previous case, the [ij.x] item must be decomposed into a set of items that includes [kl.y].",3.2 Properties,[0],[0]
"Now, consider how those items are combined.",3.2 Properties,[0],[0]
"The rules that start with an item with an external vertex produce an item that either has the same external vertex, or with the external vertex inside the span of the new item.",3.2 Properties,[0],[0]
"Therefore, y must either be equal to x or inside [ij].
(3) For items without nested spans, x /∈",3.2 Properties,[0],[0]
(kl).,3.2 Properties,[0],[0]
Assume x ∈,3.2 Properties,[0],[0]
(kl) for two items without nested spans.,3.2 Properties,[0],[0]
"None of the rules combine such a pair of items, or allow one to be extended so that the other is nested within it.",3.2 Properties,[0],[0]
"However, all items are eventually combined to complete the derivation.",3.2 Properties,[0],[0]
"By contradiction, x /∈ (kl).
",3.2 Properties,[0],[0]
"Together, these mean that given an interval H with span [ij], and another item G, either ∀v ∈ visible(G), v ∈",3.2 Properties,[0],[0]
"[ij] or ∀v ∈ visible(G),",3.2 Properties,[0],[0]
v /∈,3.2 Properties,[0],[0]
(ij).,3.2 Properties,[0],[0]
"Since edges are only created between visible vertices, no edge can cross edge ij.
Lemma 6.",3.2 Properties,[0],[0]
"All edges aside from those considered in Lemma 5 are crossed.
",3.2 Properties,[0],[0]
Proof.,3.2 Properties,[0],[0]
"First, consider an edge ij added to an item [ij.x] of type B, L, R, or N. This edge is crossed by all x–(ij) edges, and in these items |x–(ij)| ≥ 1 by definition.",3.2 Properties,[0],[0]
"Note, by the same argument as Lemma 5, the edge is not crossed later in the derivation.
",3.2 Properties,[0],[0]
"Second, consider adding e ∈ {xi, xj}, to H , an item with [ij] or [ij.x], forming an item G[ij.x].",3.2 Properties,[0],[0]
"Note, e does not cross any edges in H .",3.2 Properties,[0],[0]
Let E(F,3.2 Properties,[0],[0]
[kl.y]) be the set of y–[kl] edges in some item F .,3.2 Properties,[0],[0]
Note that e ∈ E(G).,3.2 Properties,[0],[0]
"We will show how this set of edges is affected by the rules and what that implies for e. Consider each input item A[kl.y] in each
rule, with output item C. Every item A falls into one of four categories: (1) ∀f ∈ E(A), f is crossed by an edge in another of the rule’s input items, (2) E(A) ⊆ E(C), (3) A∧ kl 7→ C",3.2 Properties,[0],[0]
"and there are no ky or ly edges in A, (4)",3.2 Properties,[0],[0]
"A contains edge kl and there are no ky or ly edges in A.
Cases 2-4 are straightforward to identify.",3.2 Properties,[0],[0]
"For an example of the first case, consider the rightmost item in rule 4.",3.2 Properties,[0],[0]
"The relevant edges are k–(lj] (by construction, kl is not present).",3.2 Properties,[0],[0]
"Since the leftmost item is either an R or N, |l–(ik)| ≥ 1.",3.2 Properties,[0],[0]
Since i < k,3.2 Properties,[0],[0]
"< l < j, all k–(lj] edges will cross all l–[ik) edges.",3.2 Properties,[0],[0]
"Therefore applying this rule will cross all k–(lj] edges in the rightmost item.
",3.2 Properties,[0],[0]
"Initially, e is not crossed and e ∈ E(G).",3.2 Properties,[0],[0]
"For each rule application, edges in E(A) are either crossed (1 and 3), remain in the set E(C)",3.2 Properties,[0],[0]
"(2), or must already be crossed (4).",3.2 Properties,[0],[0]
Since the final item is an interval and E(Interval) =,3.2 Properties,[0],[0]
"∅, there must be a subsequent rule that is not in case 2.",3.2 Properties,[0],[0]
Therefore e will be crossed.,3.2 Properties,[0],[0]
"Our algorithm is based on Pitler et al. (2013), which had the crucial idea of One-Endpoint crossing and a complete decomposition of the tree case.",3.3 Comparison with Pitler et al. (2013),[0],[0]
"Our changes and extensions provide several benefits:
Extension to graphs: By extending to support multiple parents while preventing cycles, we substantially expand the space of generatable structures.
",3.3 Comparison with Pitler et al. (2013),[0],[0]
Uniqueness:,3.3 Comparison with Pitler et al. (2013),[0],[0]
By avoiding derivational ambiguity we reduce the search space and enable efficient summing as well as maxing.,3.3 Comparison with Pitler et al. (2013),[0],[0]
Most of the cases in which ambiguity arises in Pitler et al. (2013)’s algorithm are due to symmetry that is not explicitly broken.,3.3 Comparison with Pitler et al. (2013),[0],[0]
"For example, the rule we worked through in the previous section defined t ∈ (sq)",3.3 Comparison with Pitler et al. (2013),[0],[0]
when |C| = 1.,3.3 Comparison with Pitler et al. (2013),[0],[0]
"Picking t ∈ (ps) would also lead to a valid set of rules, but allowing either creates a spurious ambiguity.",3.3 Comparison with Pitler et al. (2013),[0],[0]
"This ambiguity is resolved by tracking whether there is only one edge to the external vertex or more than one, and requiring more than one in rules 6 and 7.",3.3 Comparison with Pitler et al. (2013),[0],[0]
"Other changes include ensuring equivalent structures cannot be represented by multiple item types and enforcing a unique split point in B items.
",3.3 Comparison with Pitler et al. (2013),[0],[0]
"More concise algorithm definition: By separating edge creation from item merging, and defining our rules via a combination of templates and code, we are able to define our algorithm more concisely.",3.3 Comparison with Pitler et al. (2013),[0],[0]
Edge labels can be added by calculating either the sum or max over edge types when adding each edge.,3.4.1 Edge Labels and Word Labels,[0],[0]
"Word labels (e.g., POS Tags) must be added to the state, specifying a label for each visible word (p, q and o).",3.4.1 Edge Labels and Word Labels,[0],[0]
This state expansion is necessary to ensure agreement when combining items.,3.4.1 Edge Labels and Word Labels,[0],[0]
"Our algorithm constrains the space of graph structures, but we also want to ensure that our parse contains a projective tree of non-trace edges.
",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"To ensure every word gets one and only one structural parent, we add booleans to the state, indicating whether p, q and o have structural parents.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"When adding edges, a structural edge cannot be added if a word already has a structural parent.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"When combining items, no word can receive more than one structural parent, and words that will end up in the middle of the span must have exactly one.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"Together, these constraints ensure we have a tree.
",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"To ensure the tree is projective, we need to prevent structural edges from crossing.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"Crossing edges are introduced in two ways, and in both we can avoid structural edges crossing by tracking whether there are structural o–[pq] edges.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"Such edges are present if a rule adds a structural op or oq edge, or if a rule combines an item with structural o–[pq] edges and o will still be external in the item formed by the rule.
",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"For adding edges, every time we add a pq edge in the N , L, R and B items we create a crossing with all o–(pq) edges.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"We do not create a crossing with oq or op, but our ordering of edge creation means these are not present when we add a pq edge, so tracking structural o–[pq] edges gives us the information we need to prevent two structural edges crossing.
",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"For combining items, in Lemma 6 we showed that during combinations, o–[pq] edges in each pair of items will cross.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"As a result, knowing whether any o–[pq] edge is structural is sufficient to determine whether two structural edges will cross.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"Consider a sentence with n tokens, and let E and S be the number of edge types and word labels in our grammar respectively.
",3.5 Complexity,[0],[0]
"Parses without word or edge labels: Rules have up to four positions, leading to complexity of O(n4).",3.5 Complexity,[0],[0]
"Note, there is also an important constant–once our templates are expanded, there are 49,292 rules.
",3.5 Complexity,[0],[0]
"With edge labels: When using a first-order model, edge labels only impact the rules for edge creation, leading to a complexity of O(n4 + En2).
",3.5 Complexity,[0],[0]
"With word labels: Since we need to track word labels in the state, we need to adjust every n by a factor of S, leading to O(S4n4 + ES2n2).",3.5 Complexity,[0],[0]
Our algorithm relies on the assumption that we can process the dependents to the left and right of a word independently and then combine the two halves.,4 Parse Representation,[0],[0]
"This means we need lexicalized structures, which the PTB does not provide.",4 Parse Representation,[0],[0]
We define a new representation in which each non-terminal symbol is associated with a specific word (the head).,4 Parse Representation,[0],[0]
"Unlike dependency parsing, we retain all the information required to reconstruct the constituency parse.
",4 Parse Representation,[0],[0]
"Our approach is related to Carreras et al. (2008) and Hayashi and Nagata (2016), with three key differences: (1) we encode non-terminals explicitly, rather than implicitly through adjunction operations, which can cause ambiguity, (2) we add representations of null elements and co-indexation, (3) we modify head rules to avoid problematic structures.
",4 Parse Representation,[0],[0]
Figure 3 shows a comparison of the PTB representation and ours.,4 Parse Representation,[0],[0]
"We add lexicalization, assigning each non-terminal to a word.",4 Parse Representation,[0],[0]
"The only other changes are visual notation, with non-terminals moved to be directly above the words to more clearly show the distinction between spines and edges.
",4 Parse Representation,[0],[0]
Spines:,4 Parse Representation,[0],[0]
"Each word is assigned a spine, shown im-
mediately above the word.",4 Parse Representation,[0],[0]
"A spine is the ordered set of non-terminals that the word is the head of, e.g. SVP for like.",4 Parse Representation,[0],[0]
"If a symbol occurs more than once in a spine, we use indices to distinguish instances.
",4 Parse Representation,[0],[0]
"Edges: An edge is a link between two words, with a label indicating the symbols it links in the child and parent spines.",4 Parse Representation,[0],[0]
"In our figures, edge labels are indicated by where edges start and end.
",4 Parse Representation,[0],[0]
"Null Elements: We include each null element in the spine of its parent, unlike Hayashi and Nagata (2016), who effectively treated null elements as words, assigning them independent spines.",4 Parse Representation,[0],[0]
"We also considered encoding null elements entirely on edges but found this led to poorer performance.
",4 Parse Representation,[0],[0]
Co-indexation:,4 Parse Representation,[0],[0]
"The treebank represents movement with index pairs on null elements and nonterminals, e.g. *1 and NP1 in Figure 3.",4 Parse Representation,[0],[0]
"We represent co-indexation with edges, one per reference, going from the null element to the non-terminal.",4 Parse Representation,[0],[0]
There are three special cases of co-indexation: (1) It is possible for trace edges to have the same start and end points as a non-trace edge.,4 Parse Representation,[0],[0]
We restrict this case to allow at most one trace edge.,4 Parse Representation,[0],[0]
This decreases edge coverage in the training set by 0.006%.,4 Parse Representation,[0],[0]
"(2) In some cases the reference non-terminal only spans a null element, e.g. the WHNP in Figure 4a.",4 Parse Representation,[0],[0]
For these we use a reversed edge to avoid creating a cycle.,4 Parse Representation,[0],[0]
"Figure 4a shows a situation where the trace edge links two positions in the same spine, which we assign with the spine during parsing.",4 Parse Representation,[0],[0]
(3) For parallel constructions the treebank coindexes arguments that fulfill the same roles (Fig. 4b).,4 Parse Representation,[0],[0]
These are distinct from the previous cases because neither index is on a null element.,4 Parse Representation,[0],[0]
"We considered two options: add edges from the repetition
to the referent (middle), or add edges from the repetition to the parent of the first occurrence (bottom).",4 Parse Representation,[0],[0]
"Option two produces fewer non-1-EC structures and explicitly represents all predicates, but only implicitly captures the original structure.",4 Parse Representation,[0],[0]
Prior work on parsing with spines has used radjunction to add additional non-terminals to spines.,4.1 Avoiding Adjunction Ambiguity,[0],[0]
"This introduces ambiguity, because edges modifying the same spine from different sides may not have a unique order of application.",4.1 Avoiding Adjunction Ambiguity,[0],[0]
We resolve this issue by using more articulated spines with the complete set of non-terminals.,4.1 Avoiding Adjunction Ambiguity,[0],[0]
"We found that 0.045% of spine instances in the development set are not observed in training, though in 70% of those cases an equivalent spine sans null elements is observed in training.",4.1 Avoiding Adjunction Ambiguity,[0],[0]
"To construct the spines, we lexicalize with head rules that consider the type of each non-terminal and its children.",4.2 Head Rules,[0],[0]
Different heads often represent more syntactic or semantic aspects of the phrase.,4.2 Head Rules,[0],[0]
"For trees, all head rules generate valid structures.",4.2 Head Rules,[0],[0]
"For graphs, head rules influence the creation of two problematic structures:
Cycles: These arise when the head chosen for a phrase is also an argument of another word in the phrase.",4.2 Head Rules,[0],[0]
Figure 5a shows a cycle between which and proposed.,4.2 Head Rules,[0],[0]
"We resolve this by changing the head of an SBAR to be an S rather than a Wh-noun phrase.
",4.2 Head Rules,[0],[0]
"One-Endpoint Crossing Violations: Figure 5b shows an example, with the trace from CEO to Page crossing two edges with no endpoints in common.",4.2 Head Rules,[0],[0]
We resolve this case by changing the head for VPs to be a child VP rather than an auxiliary.,4.2 Head Rules,[0],[0]
Algorithm Coverage: In Table 1 we show the impact of design decisions for our representation.,5 Results,[0],[0]
The percentages indicate how many sentences in the training set are completely recoverable by our algorithm.,5 Results,[0],[0]
"Each row shows the outcome of an addition to the previous row, starting from no traces at all, going to our representation with the head rules of Carreras et al. (2008), then changing the head rules, reversing null-null edges, and changing the target of edges in parallel constructions.",5 Results,[0],[0]
"The largest gain comes from changing the head rules, which is unsurprising since Carreras et al. (2008)’s rules were designed for trees (any set of rules form valid structures for trees).
",5 Results,[0],[0]
"Problematic Structures: Of the sentences we do not cover, 54% contain a cycle, 45% contain a 1- EC violation, and 1% contain both.",5 Results,[0],[0]
"To understand these problematic sentences, we manually inspected a random sample of twenty parses that contained a cycle and twenty parses with a 1-EC violation (these forty are 6% of all problematic parses, enough to identify the key remaining challenges).
",5 Results,[0],[0]
"For the cycles, eleven cases related to sentences containing variations of NP said interposed between two parts of a single quote.",5 Results,[0],[0]
A cycle was present because the top node of the parse was co-indexed with a null argument of said while said was an argument of the head word of the quote.,5 Results,[0],[0]
"The remaining cases were all instances of pseudo-attachment, which the treebank uses to show that non-adjacent constituents are related (Bies et al. 1995).",5 Results,[0],[0]
"These cases were split between use of Expletive (5) and Interpret Constituent Here (4) traces.
",5 Results,[0],[0]
It was more difficult to determine trends for cases where the parse structure has a 1-EC violation.,5 Results,[0],[0]
"The same three cases, Expletive, Interpret Constituent Here, and NP said accounted for half of the issues.",5 Results,[0],[0]
We implemented a parser with a first-order model using our algorithm and representation.,5.1 Implementation,[0],[0]
"Code for the parser, for conversion to and from our representation, and for our metrics is available3.",5.1 Implementation,[0],[0]
"Our parser uses a linear discriminative model, with features based on McDonald et al. (2005).",5.1 Implementation,[0],[0]
"We train
3 https://github.com/jkkummerfeld/ 1ec-graph-parser
with an online primal subgradient approach (Ratliff et al. 2007) as described by Kummerfeld, BergKirkpatrick, et al. (2015), with parallel lock-free sparse updates.
",5.1 Implementation,[0],[0]
"Loss Function: We use a weighted Hamming distance for loss-augmented decoding, as it can be efficiently decomposed within our dynamic program.",5.1 Implementation,[0],[0]
Calculating the loss for incorrect spines and extra edges is easy.,5.1 Implementation,[0],[0]
"For missing edges, we add when a deduction rule joins two spans that cover an end of the edge, since if it does not exist in one of those items it is not going to be created in future.",5.1 Implementation,[0],[0]
"To avoid double counting we subtract when combining two halves that contain the two ends of a gold edge4.
",5.1 Implementation,[0],[0]
Inside–Outside Calculations:,5.1 Implementation,[0],[0]
"Assigning scores to edges is simple, as they are introduced in a single item in the derivation.",5.1 Implementation,[0],[0]
"Spines must be introduced in multiple items (left, right, and external positions) and must be assigned a score in every case to avoid ties in beams.",5.1 Implementation,[0],[0]
"We add the score every time the spine is introduced and then subtract when two items with a spine in common are combined.
",5.1 Implementation,[0],[0]
Algorithm rule pruning: Many 1-EC structures are not seen in our data.,5.1 Implementation,[0],[0]
"We keep only the rules used in gold training parses, reducing the set of 49,292 from the general algorithm to 627 (including rules for both adding arcs and combining items).",5.1 Implementation,[0],[0]
"Almost every template in Algorithm 1 generates some unnecessary rules, and no items of type B are needed.
",5.1 Implementation,[0],[0]
"4 One alternative is to count half of it on each end, removing the need for subtraction later.",5.1 Implementation,[0],[0]
"Another is to add it during the combination step.
",5.1 Implementation,[0],[0]
"The remaining rules still have high coverage of the development set, missing only 15 rules, each applied once (out of 78,692 rule applications).",5.1 Implementation,[0],[0]
"By pruning in this way, we are considering the intersection of 1-EC graphs and the true space of structures used in language.
",5.1 Implementation,[0],[0]
"Chart Pruning: To improve speed we use beams and cube pruning (Chiang 2007), discarding items based on their Viterbi inside score.",5.1 Implementation,[0],[0]
We divide each beam into sub-beams based on aspects of the state.,5.1 Implementation,[0],[0]
"This ensures diversity and enables consideration of only compatible items during binary and ternary compositions.
",5.1 Implementation,[0],[0]
"Coarse to Fine Pruning: Rather than parsing immediately with the full model we use several passes with progressively richer structure (Goodman 1997): (1) Projective parsing without traces or spines, and simultaneously a trace classifier, (2) Non-projective parsing without spines, and simultaneously a spine classifier, (3) Full structure parsing.",5.1 Implementation,[0],[0]
"Each pass prunes using parse max-marginals and classifier scores, tuned on the development set.",5.1 Implementation,[0],[0]
The third pass also prunes spines that are not consistent with any unpruned edge from the second pass.,5.1 Implementation,[0],[0]
"For the spine classifier we use a bidirectional LSTM tagger, implemented in DyNet (Neubig et al. 2017).
",5.1 Implementation,[0],[0]
Speed: Parsing took an average of 8.6 seconds per sentence for graph parsing and 0.5 seconds when the parser is restricted to trees5.,5.1 Implementation,[0],[0]
"Our algorithm is also amenable to methods such as semi-supervised and adaptive supertagging, which can improve the speed of a parser after training (Kummerfeld, Roesner, et al. 2010; Lewis and Steedman 2014).
",5.1 Implementation,[0],[0]
Tree Accuracy:,5.1 Implementation,[0],[0]
"On the standard tree-metric, we score 88.1.",5.1 Implementation,[0],[0]
"Using the same non-gold POS tags as input, Carreras et al. (2008) score 90.9, probably due to their second-order features and head rules tuned for performance6.",5.1 Implementation,[0],[0]
"Shifting to use their head rules, we score 88.9.",5.1 Implementation,[0],[0]
"Second-order features could be added to our model through the use of forest reranking, an improvement that would be orthogonal to this paper’s contributions.
",5.1 Implementation,[0],[0]
We can also evaluate on spines and edges.,5.1 Implementation,[0],[0]
"Since their system produces regular PTB trees, we con-
5 Using a single core of an Amazon EC2 m4.2xlarge instance (2.4 GHz Xeon CPU and 32 Gb of RAM).
",5.1 Implementation,[0],[0]
"6 Previous work has shown that the choice of head can significantly impact accuracy (Schwartz et al. 2012).
vert its output to our representation and compare its results with our system using their head rules.",5.1 Implementation,[0],[0]
"We see slightly lower accuracy for our system on both spines (94.0 vs. 94.3) and edges (90.4 vs. 91.1).
",5.1 Implementation,[0],[0]
Trace Accuracy: Table 2 shows results using Johnson (2002)’s trace metric.,5.1 Implementation,[0],[0]
"Our parser is competitive with previous work that has highly-engineered models: Johnson’s system has complex non-local features on tree fragments, and similarly Kato and Matsubara (K&M 2016) consider complete items in the stack of their transition-based parser.",5.1 Implementation,[0],[0]
On co-indexation our results fall between Johnson and K&M.,5.1 Implementation,[0],[0]
"Converting to our representation, our parser has higher precision than K&M on trace edges (84.1 vs. 78.1) but lower recall (59.5 vs. 71.3).",5.1 Implementation,[0],[0]
"One modeling challenge we observed is class imbalance: of the many places a trace could be added, only a small number are correct, and so our model tends to be conservative (as shown by the P/R tradeoff).",5.1 Implementation,[0],[0]
We propose a representation and algorithm that cover 97.3% of graph structures in the PTB.,6 Conclusion,[0],[0]
"Our algorithm is O(n4), uniquely decomposes parses, and enforces the property that parses are composed of a core tree with additional traces and null elements.",6 Conclusion,[0],[0]
A proof of concept parser shows that our algorithm can be used to parse and recover traces.,6 Conclusion,[0],[0]
"We thank Greg Durrett for advice on parser implementation and debugging, and the action editor and anonymous reviewers for their helpful feedback.",Acknowledgments,[0],[0]
"This research was partially supported by a General Sir John Monash Fellowship and the Office of Naval
Research under MURI Grant",Acknowledgments,[0],[0]
No. N000140911081.,Acknowledgments,[0],[0]
"General treebank analyses are graph structured, but parsers are typically restricted to tree structures for efficiency and modeling reasons.",abstractText,[0],[0]
"We propose a new representation and algorithm for a class of graph structures that is flexible enough to cover almost all treebank structures, while still admitting efficient learning and inference.",abstractText,[0],[0]
"In particular, we consider directed, acyclic, one-endpoint-crossing graph structures, which cover most long-distance dislocation, shared argumentation, and similar tree-violating linguistic phenomena.",abstractText,[0],[0]
"We describe how to convert phrase structure parses, including traces, to our new representation in a reversible manner.",abstractText,[0],[0]
"Our dynamic program uniquely decomposes structures, is sound and complete, and covers 97.3% of the Penn English Treebank.",abstractText,[0],[0]
We also implement a proofof-concept parser that recovers a range of null elements and trace types.,abstractText,[0],[0]
Parsing with Traces: An O(n) Algorithm and a Structural Representation,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2411–2420 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"During the last decade, social media have become extremely popular, on which billions of usergenerated contents are posted every day.",1 Introduction,[0],[0]
Many users have been writing about their thoughts and lives on the go.,1 Introduction,[0],[0]
"The massive unstructured data from social media provides valuable information for a variety of applications such as stock prediction (Bollen et al., 2011), public health analysis (Wilson and Brownstein, 2009; Paul and Dredze, 2011), real-time event detection (Sakaki et al., 2010), and so on.",1 Introduction,[0],[0]
"The quality of these applications is highly impacted by the performance of natural language processing tasks.
∗Corresponding author.
",1 Introduction,[0],[0]
Part-of-speech (POS) tagging is one of the most important natural language processing tasks.,1 Introduction,[0],[0]
"It has also been widely used in the social media analysis systems (Ritter et al., 2012; Lamb et al., 2013; Kiritchenko et al., 2014).",1 Introduction,[0],[0]
Most stateof-the-art POS tagging approaches are based on supervised methods.,1 Introduction,[0],[0]
"Hence, they usually require a large amount of annotated data to train models.",1 Introduction,[0],[0]
Many datasets have been constructed for POS tagging task.,1 Introduction,[0],[0]
"Because newswire articles are carefully edited, benchmarks usually use them for annotation (Marcus et al., 1993).",1 Introduction,[0],[0]
"However, usergenerated contents on social media are usually informal and contain many nonstandard lexical items.",1 Introduction,[0],[0]
"Moreover, the difference in domains between training data and evaluation data may heavily impact the performance of approaches based on supervised methods (Caruana and NiculescuMizil, 2006).",1 Introduction,[0],[0]
"Hence, most POS tagging methods cannot achieve the same performance as reported on newswire domain when applied on Twitter (Owoputi et al., 2013).
",1 Introduction,[0],[0]
"To perform the Twitter POS tagging task, some approaches have been proposed to perform the task.",1 Introduction,[0],[0]
"Gimpel et al. (2011) manually annotated 1,827 tweets and carefully studied various fea-
2411
tures.",1 Introduction,[0],[0]
"Ritter et al. (2011) also constructed a labeled dataset, which contained 787 tweets, to empirically evaluate the performance of supervised methods on Twitter.",1 Introduction,[0],[0]
Owoputi et al. (2013) incorporated word clusters into the feature sets and further improved the performance.,1 Introduction,[0],[0]
"From these works, we can observe that the size of the training data was much smaller than the newswire domain’s.
",1 Introduction,[0],[0]
"Besides the challenge of lack of training data, the frequent use of out-of-vocabulary words also makes this problem difficult to address.",1 Introduction,[0],[0]
"Social media users often use informal ways of expressing their ideas and often spell words phonetically (e.g., “2mor” for “tomorrow”).",1 Introduction,[0],[0]
"In addition, they also make extensive use of emoticons and abbreviations (e.g., “:-)” for smiling emotion and “LOL” for laughing out loud).",1 Introduction,[0],[0]
"Moreover, new symbols, abbreviations, and words are constantly being created.",1 Introduction,[0],[0]
"Figure 1 shows an example of tagged Tweet.
",1 Introduction,[0],[0]
"To tackle the challenges posed by the lack of training data and the out-of-vocabulary words, in this paper, we propose a novel recurrent neural network, which we call Target Preserved Adversarial Neural Network (TPANN) to perform the task.",1 Introduction,[0],[0]
"It can make use of a large quantity of annotated data from other resourcerich domains, unlabeled in-domain data, and a small amount of labeled in-domain data.",1 Introduction,[0],[0]
All of these datasets can be easily obtained.,1 Introduction,[0],[0]
"To make use of unlabeled data, motivated by the work of Goodfellow et al. (2014) and Chen et al. (2016), the proposed method extends the bi-directional long short-term memory recurrent neural network (bi-LSTM) with an adversarial predictor.",1 Introduction,[0],[0]
"To overcome the defect that adversarial networks can merely learn the common features, we propose to use an autoencoder only acting on target dataset to preserve its own specific features.",1 Introduction,[0],[0]
"For tackling the out-of-vocabulary problem, the proposed method also incorporates a character level convolutional neutral network to leverage subword information.
",1 Introduction,[0],[0]
"The contributions of this work are as follows:
• We propose to incorporate large scale unlabeled in-domain data, out-of-domain labeled data, and in-domain labeled data for Twitter part-of-speech tagging task.
",1 Introduction,[0],[0]
"• We introduce a novel recurrent neural network, which can learn domain-invariant rep-
resentations through in-domain and out-ofdomain data and construct a cross domain POS tagger through the learned representations.",1 Introduction,[0],[0]
"The proposed method also tries to preserve the specific features of target domain.
",1 Introduction,[0],[0]
• Experimental results demonstrate that the proposed method can lead to better performance in most of cases on three different datasets.,1 Introduction,[0],[0]
"In this work, we propose a novel recurrent neural network, Target Preserved Adversarial Neural Network (TPANN), to learn common features between resource-rich domain and target domain, simultaneously to preserve target domain-specific features.",2 Approach,[0],[0]
It extends the bi-directional LSTM with adversarial network and autoencoder.,2 Approach,[0],[0]
The architecture of TPANN is illustrated in Figure 2.,2 Approach,[0],[0]
"The model consists of four components: Feature Extractor, POS Tagging Classifier, Domain Discriminator and Target Domain Autoencoder.",2 Approach,[0],[0]
"In the following sections, we will detail each part of the proposed architecture and training methods.",2 Approach,[0],[0]
"The feature extractor F adopts CNN to extract character embedding features, which can tackle the out-of-vocabulary word problem effectively.",2.1 Feature Extractor,[0],[0]
"To incorporate word embedding features, we concatenate word embedding to character embedding as the input of bi-LSTM on the next layer.",2.1 Feature Extractor,[0],[0]
"Utilizing a bi-LSTM to model sentences, F can extract sequential relations and context information.
",2.1 Feature Extractor,[0],[0]
"We denote the input sentence as x and the i-th word as xi. xi ∈ S(x) and xi ∈ T (x) represent input samples are from source domain and target domain, respectively.",2.1 Feature Extractor,[0],[0]
We denote the parameters of F as θf .,2.1 Feature Extractor,[0],[0]
"Let V be the vocabulary of words, and C be the vocabulary of characters.",2.1 Feature Extractor,[0],[0]
d is the dimensionality of character embedding then Q ∈ Rd×|C| is the representation matrix of vocabulary.,2.1 Feature Extractor,[0],[0]
We assume that word xi ∈ V is made up of a sequence of characters Ci =,2.1 Feature Extractor,[0],[0]
"[c1, c2, . . .",2.1 Feature Extractor,[0],[0]
", cl], where l is the max length of word and every word will be padded to this length.",2.1 Feature Extractor,[0],[0]
"Then Ci ∈ Rd×l would be the inputs of CNN.
",2.1 Feature Extractor,[0],[0]
"We apply a narrow convolution between Ci and filter H ∈ Rd×k, where k is the width of the filter.
",2.1 Feature Extractor,[0],[0]
After that we add a bias and apply nonlinearity to obtain a feature map mi ∈ Rl−k+1.,2.1 Feature Extractor,[0],[0]
"Specifically, the j-th element of mi is given by:
ik[j] = tanh(〈Ci[∗, j : j + k",2.1 Feature Extractor,[0],[0]
"− 1],H〉+ b), (1)
where Ck[∗, j : j + k",2.1 Feature Extractor,[0],[0]
− 1] is the j-to-(j + k,2.1 Feature Extractor,[0],[0]
"− 1)-th column of Ci and 〈A,B〉 = Tr(ABT ) is the Frobenius inner product.",2.1 Feature Extractor,[0],[0]
"We then apply a max-over-time pooling operation (Collobert et al., 2011) over the feature map.",2.1 Feature Extractor,[0],[0]
CNN uses multiple filters with varying widths to obtain the feature vector ~ci for word xi.,2.1 Feature Extractor,[0],[0]
"Then, the character-level feature vector ~ci is concatenated to the word embedding ~wi to form the input of bi-LSTM on the next layer.",2.1 Feature Extractor,[0],[0]
The word embedding ~w is pretrained on 30 million tweets.,2.1 Feature Extractor,[0],[0]
"Then, the hidden states h of bi-LSTM turn into the features that will be transfered to P , Q andR, i.e. F(x) = h.",2.1 Feature Extractor,[0],[0]
POS tagging classifier P and domain discriminator Q take F(x) as input.,2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
They are standard feed-forward networks with a softmax layer for classification.,2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"P predicts POS tagging label to get classification capacity, and Q discriminates domain label to make F(x) domain-invariant.
",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
The POS tagging classifier P maps the feature vector F(xi) to its label.,2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
We denote the parameters of this mapping as θy.,2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"The POS tagging
classifier is trained on Ns samples from the source domain with the cross entropy loss:
",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
Ltask =,2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
− Ns∑ i=1,2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"yi ∗ log ŷi, (2)
where yi is the one-hot vector of POS tagging label corresponding to xi ∈ S(x), ŷi is the output of top softmax layer:",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
ŷi = P(F(xi)).,2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"During the training time, The parameters θf and θy are optimized to minimize the classification loss Ltask.",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"This ensures that P(F(xi)) can make accurate prediction on the source domain.
",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"Conversely, domain discriminator maps the same hidden states h to the domain labels with parameters θd.",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"The domain discriminator aims to discriminate the domain label with following loss function: Ltype = − Ns+Nt∑
i=1",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
{di log d̂i+(1−di),2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"log(1− d̂i)}, (3)
where di is the ground truth domain label for sample",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"i, d̂i is the output of top layer: d̂i = Q(F(xi)).",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
Nt meansNt samples from the target domain.,2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"The domain discriminator is trained towards a saddle point of the loss function through minimizing the loss over θd while maximizing the loss over θf (Ganin et al., 2016).",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"Optimizing θf ensures that the domain discriminator can’t discriminate
the domain, i.e., the feature extractor finds the common features between the two domains.",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"Through training adversarial networks, we can obtain domain-invariant features hcommon, but it will weaken some domain-specific features which are useful for POS tagging classification.",2.3 Target Domain Autoencoder,[0],[0]
"Merely obtaining domain invariant features would therefore limit the classification ability.
",2.3 Target Domain Autoencoder,[0],[0]
"Our model tries to tackle this defect by introducing domain-specific autoencoder R, which attempts to reconstruct target domain data.",2.3 Target Domain Autoencoder,[0],[0]
"Inspired by (Sutskever et al., 2014) but different from (Dai and Le, 2015), we treat the feature extractor F as encoder.",2.3 Target Domain Autoencoder,[0],[0]
"In addition, we combine the last hidden states of the forward LSTM and backward LSTM in F as the initial state h0(dec) of the decoder LSTM.",2.3 Target Domain Autoencoder,[0],[0]
"Hence, we don’t need to reverse the order of words of the input sentences and the model avoids the difficulty of ”establish communication” between the input and the output (Sutskever et al., 2014).
",2.3 Target Domain Autoencoder,[0],[0]
"Similar to (Zhang et al., 2016), we use h0(dec) and embedding vector of the previous word as the inputs of the decoder, but in a computationally more efficient manner by computing previous word representation.",2.3 Target Domain Autoencoder,[0.955084845742299],"['Following Siahbani et al. (2013), we refer to the left half of a synchronous production as the source side, and the right half as the target side; this terminology captures the intuition that synchronous grammars model translational equivalence between a source phrase and its translation into a target language.']"
"We assume that (x̂1, · · · , x̂T ) is the output sequence.",2.3 Target Domain Autoencoder,[0],[0]
"zt is the t-th word representation: zt = MLP (ht), and MLP is the multiple perceptron function.",2.3 Target Domain Autoencoder,[0],[0]
Hidden state ht = LSTM([h0(dec) :,2.3 Target Domain Autoencoder,[0],[0]
"zt−1], ht−1), where [· : ·] is the concatenation operation.",2.3 Target Domain Autoencoder,[0],[0]
"We estimate the conditional probability p(x̂1, · · · , x̂T |h0(dec))",2.3 Target Domain Autoencoder,[0],[0]
"as follows:
p(x̂1, · · · , x̂T |h0(dec))",2.3 Target Domain Autoencoder,[0],[0]
"= T∏
t=1
p(x̂t|h0(dec), z1, · · · , zt−1), (4)
where each p(x̂t|h0(dec), z1, · · · , zt−1) distribution is computed with softmax over all the words in the vacabulary.
",2.3 Target Domain Autoencoder,[0],[0]
"Our aim is to minimize the following loss function with respect to parameters θr:
Ltarget = − Nt∑ i=1",2.3 Target Domain Autoencoder,[0],[0]
"xi ∗ log x̂i, (5)
where xi is the one-hot vector of i-th word.",2.3 Target Domain Autoencoder,[0],[0]
"This makes h0(dec) learn an undercomplete and most salient sentence representation of target domain
data.",2.3 Target Domain Autoencoder,[0],[0]
"When the adversarial networks try to optimize the hidden representation to common representation hcommon, The target domain autoencoder counteracts a tendency of the adversarial network to erase target domain features by optimizing the common representation to be informative on the target-domain data.",2.3 Target Domain Autoencoder,[0],[0]
"Our model can be trained end-to-end with standard back-propagation, which we will detail in this section.
",2.4 Training,[0],[0]
"Our ultimate training goal is to minimize the total loss function with parameters {θf , θy, θr, θd} as follows:
Ltotal = αLtask + βLtarget + γLtype, (6) where α, β, γ are the weights to balance the effects of P ,R and Q.
For obtaining domain-invariant representation hcommon, inspired by (Ganin and Lempitsky, 2015), we introduce a special gradient reversal layer (GRL), which does nothing during forward propagation, but negates the gradients if it receives backward propagation, i.e. g(F(x))",2.4 Training,[0],[0]
= F(x) but ∇g(F(x)),2.4 Training,[0],[0]
= −λ∇F(x).,2.4 Training,[0],[0]
"We insert the GRL between F and Q, which can run standard Stochastic Gradient Descent with respect to θf and θd.",2.4 Training,[0],[0]
The parameter −λ drives the parameters θf not to amplify the dissimilarity of features when minimize Ltpye.,2.4 Training,[0],[0]
"So by introducing a GRL, F can drive its parameters θf to extract hidden representations that help the POS tagging classification and hamper the domain discrimination.
",2.4 Training,[0],[0]
"In order to preserve target domain-specific features, we only optimize the autoencoder on target domain data for reconstruction tasks.
",2.4 Training,[0],[0]
"Through above procedures, the model can learn the common features between domains, simultaneously preserve target domain-specific features.",2.4 Training,[0],[0]
"Finally, we can update the parameters as follows:
θf = θf − µ(α∂L i task
∂θf + β",2.4 Training,[0],[0]
"∂Litarget ∂θf − γ · λ∂L i type ∂θf )
θy = θy − µ · α∂L i task
∂θy
θr = θr − µ · β ∂Litarget ∂θr θd = θd − µ · γ ∂Litype ∂θd ,
(7)
where µ is the learning rate.",2.4 Training,[0],[0]
"Because the size of the WSJ is more than 100 times that of the labeled Twitter dataset, if we directly train the model with the combined dataset, the final results are much worse than those using two training steps.",2.4 Training,[0],[0]
"So, we adopt adversarial training on WSJ and unlabeled Twitter dataset at the first step, then use a small number of in-domain labeled data to fine-tune the parameters with a low learning rate.",2.4 Training,[0],[0]
"In this section, we will detail the datasets used for experiments and experimental setup.",3 Experiments,[0],[0]
"The methods proposed in this work incorporate out-of-domain labeled data from resource-rich domains, large scale unlabeled in-domain data, and a small number of labeled in-domain data.",3.1 Datasets,[0],[0]
The datasets used in this work are as follows: Labeled out-of-domain data.,3.1 Datasets,[0],[0]
"We use a standard benchmark dataset for adversarial POS tagging, namely the Wall Street Journal (WSJ) data from the Penn TreeBank v3 (Marcus et al., 1993), sections 0-24 for the out-of-domain data.",3.1 Datasets,[0],[0]
Labeled in-domain data.,3.1 Datasets,[0],[0]
"For training and evaluating POS tagging approaches, we compare the proposed method with other approaches on three benchmarks: RIT-Twitter (Ritter et al., 2011), NPSCHAT (Forsyth, 2007), and ARKTwitter (Gimpel et al., 2011).",3.1 Datasets,[0],[0]
Unlabeled in-domain data.,3.1 Datasets,[0],[0]
"For training the adversarial network, we need to use a dataset that has large scale unlabeled tweets.",3.1 Datasets,[0],[0]
"Hence, in this work, we construct large scale unlabeled data (UNL), from Twitter using its API.
",3.1 Datasets,[0],[0]
The detailed data statistics of the datasets used in this work are listed in Table 1.,3.1 Datasets,[0],[0]
"We select both state-of-the-art and classic methods for comparison, as follows:
• Stanford POS",3.2 Experimental Setup,[0],[0]
Tagger:,3.2 Experimental Setup,[0],[0]
"Stanford POS Tagger is a widely used tool for newswire domains (Toutanova et al., 2003).",3.2 Experimental Setup,[0],[0]
"In this work, we train it using two different sets, the WSJ (sections 0-18) and a WSJ, IRC, and Twitter mixed corpus.",3.2 Experimental Setup,[0],[0]
"We use StanfordWSJ and Stanford-MIX to represent them, respectively.
",3.2 Experimental Setup,[0],[0]
"• T-POS: T-Pos (Ritter et al., 2011) adopts the Conditional Random Fields and clustering algorithm to perform the task.",3.2 Experimental Setup,[0],[0]
"It was trained from a mixture of hand-annotated tweets and existing POS-labeled data.
",3.2 Experimental Setup,[0],[0]
• GATE,3.2 Experimental Setup,[0],[0]
"Tagger: GATE tagger (Derczynski et al., 2013) is based on vote-constrained bootstrapping with unlabeled data.",3.2 Experimental Setup,[0],[0]
"It combines cases where available taggers use different tagsets.
",3.2 Experimental Setup,[0],[0]
"• ARK Tagger: ARK tagger (Owoputi et al., 2013) is a system that reports the best accuracy on the RIT dataset.",3.2 Experimental Setup,[0],[0]
"It uses unsupervised word clustering and a variety of lexical features.
",3.2 Experimental Setup,[0],[0]
• bi-LSTM:,3.2 Experimental Setup,[0],[0]
"Bidirectional Long Short-Term Memory (LSTM) networks have been widely used in a variety of sequence labeling tasks (Graves and Schmidhuber, 2005).",3.2 Experimental Setup,[0],[0]
"In this work, we evaluate it at character level, word level, and combining them together.",3.2 Experimental Setup,[0],[0]
bi-LSTM (word level) uses one layer of bi-LSTM to extract word-level features and adopts a random initialization method to transform words to vectors.,3.2 Experimental Setup,[0],[0]
"bi-LSTM (character level) represents a method that combines bi-LSTM and CNN-based character embedding, a similar approach with character-aware neural network described in (Kim et al., 2015) to handle the out-ofvocabulary words.",3.2 Experimental Setup,[0],[0]
"bi-LSTM (word level pretrain) architecture is the same as that of bi-LSTM(word level) but adopts word2vec tool (Mikolov et al., 2013) to vectorize.",3.2 Experimental Setup,[0],[0]
"bi-LSTM (combine) concatenates word to character features.
",3.2 Experimental Setup,[0],[0]
The hyper-parameters used for our model are as follows.,3.2 Experimental Setup,[0],[0]
AdaGrad optimizer trained with crossentropy loss is used with 0.1 as the default learning rate.,3.2 Experimental Setup,[0],[0]
The dimensionality of word embedding is set to 200.,3.2 Experimental Setup,[0],[0]
The dimensionality for random initialized character embedding is set to 25.,3.2 Experimental Setup,[0],[0]
We adopt a bi-LSTM for encoding with each layer consisting of 250 hidden neurons.,3.2 Experimental Setup,[0],[0]
We set three layers of standard LSTM for decoding.,3.2 Experimental Setup,[0],[0]
Each LSTM layer consists of 500 hidden neurons.,3.2 Experimental Setup,[0],[0]
Adam optimizer trained with cross-entropy loss is used to fine-tune with 0.0001 as the default learning rate.,3.2 Experimental Setup,[0],[0]
Finetuning is run for 100 epochs using early stop.,3.2 Experimental Setup,[0],[0]
"In this section, we will report experimental results and a detailed analysis of the results for the three different datasets.",4 Results and Discussion,[0],[0]
"The RIT-Twitter is split into training, development and evaluation sets (RIT-Train, RIT-Dev, RITTest).",4.1 Evaluation on RIT-Twitter,[0],[0]
"The splitting method is shown in (Derczynski et al., 2013), and the dataset statistics are listed in Table 1.",4.1 Evaluation on RIT-Twitter,[0],[0]
Table 2 shows the results of our method and other approaches on the RIT-Twitter dataset.,4.1 Evaluation on RIT-Twitter,[0],[0]
"RIT-Twitter uses the PTB tagset with several Twitter-specific tags: retweets, @usernames, hashtags, and urls.",4.1 Evaluation on RIT-Twitter,[0],[0]
"Since words in these
categories can be tagged almost perfectly using simple regular expressions, similar to (Owoputi et al., 2013), we use regular expressions to tags these words appropriately for all systems.
",4.1 Evaluation on RIT-Twitter,[0],[0]
"From the results of the Stanford-WSJ, we can observe that the newswire domain is different from Twitter.",4.1 Evaluation on RIT-Twitter,[0],[0]
"Although the token-level accuracy of the Stanford POS Tagger is higher than 97.0% on the PTB dataset, its performance on Twitter drops sharply to 73.37%.",4.1 Evaluation on RIT-Twitter,[0],[0]
"By incorporating some indomain labeled data for training, the accuracy of Stanford POS Tagger can reach up to 83.14%.",4.1 Evaluation on RIT-Twitter,[0],[0]
"Taking a variety of linguistic features and many other resources into consideration, the T-POS, GATE tagger, and ARK tagger can achieve better performance.
",4.1 Evaluation on RIT-Twitter,[0],[0]
"The second part of Table 2 shows the results of the bi-LSTM based methods, which are trained on the RIT-Train dataset.",4.1 Evaluation on RIT-Twitter,[0],[0]
"According to the results of word level, we can see that word2vec can provide valuable information.",4.1 Evaluation on RIT-Twitter,[0],[0]
"The pre-trained word vectors in bi-LSTM(word level pretrain) give almost 10% higher accuracy than bi-LSTM(word level).
",4.1 Evaluation on RIT-Twitter,[0],[0]
"Comparing the character-level bi-LSTM with word-level bi-LSTM with random initialization, we can observe that the character-level method can achieve better performance than the word-level method.",4.1 Evaluation on RIT-Twitter,[0],[0]
"bi-LSTM(combine) combines word with character features, as described in Section 2.1,
which achieves the best results at 89.48% in the bi-LSTM based baseline systems and shows that the morphological features and pre-trained word vectors are both useful for POS tagging.
",4.1 Evaluation on RIT-Twitter,[0],[0]
"The third part of Table 2 shows the results of our methods incorporating out-of-domain labeled data, in-domain unlabeled data, and in-domain labeled data.",4.1 Evaluation on RIT-Twitter,[0],[0]
"Putting everything together, our model can achieve 90.92% on this dataset.",4.1 Evaluation on RIT-Twitter,[0],[0]
"Compared with the architecture without an adversarial model, our method is almost 1% better.",4.1 Evaluation on RIT-Twitter,[0],[0]
It demonstrates that adversarial networks can significantly help with tasks of this nature.,4.1 Evaluation on RIT-Twitter,[0],[0]
"Through introducing the autoencoder in target domain, we can preserve domain-specific features for better performance.",4.1 Evaluation on RIT-Twitter,[0],[0]
"Compared with the ARK tagger, which achieves the previous best result on this dataset, our model is also 0.52% better, the error reduction rate is more than 5.5%.
",4.1 Evaluation on RIT-Twitter,[0],[0]
"To better understand why adversarial networks can help transfer domains from newswire to Twitter, in this work we also followed the method Ganin and Lempitsky (2015) used to visualize the outputs of LSTM with tSNE (Van Der Maaten, 2013).",4.1 Evaluation on RIT-Twitter,[0],[0]
Figure 3 shows the visualization results.,4.1 Evaluation on RIT-Twitter,[0],[0]
"From the figure, we can see that the adversary in our method makes the two distributions of features much more similar, which means that the outputs of bi-LSTM are domain-invariant.",4.1 Evaluation on RIT-Twitter,[0],[0]
"Hence, the PTB training data can provide much more help than directly combining PTB and RIT-Train together.",4.1 Evaluation on RIT-Twitter,[0],[0]
"IRC, which contains Internet relay room messages from 2006, is a medium of online conversational text.",4.2 Evaluation on NPSChat,[0],[0]
Its content is very similar to tweets.,4.2 Evaluation on NPSChat,[0],[0]
"We evaluate the proposed method on the NPSChat corpus (Forsyth, 2007), a PTB-part-of-speech annotated dataset of IRC.
",4.2 Evaluation on NPSChat,[0],[0]
"We compared our method with a tagger in the same setup as experiments with (Forsyth, 2007).",4.2 Evaluation on NPSChat,[0],[0]
The training part contains 90% of the data.,4.2 Evaluation on NPSChat,[0],[0]
The testing part contains the other 10%.,4.2 Evaluation on NPSChat,[0],[0]
Table 3 shows the results of the ARK Tagger and our method.,4.2 Evaluation on NPSChat,[0],[0]
"We used PTB, unlabeled Twitter, and the training part of NPSChat to train our model.",4.2 Evaluation on NPSChat,[0],[0]
"From the results, we can see that our model achieved 94.1% accuracy.",4.2 Evaluation on NPSChat,[0],[0]
"This is significantly better than the result Forsyth (2007) reported, which was 90.8%.",4.2 Evaluation on NPSChat,[0],[0]
"They trained their tagger with a mix of several POS-annotated corpora (12K from Twitter, 40K from IRC, and 50K from PTB).",4.2 Evaluation on NPSChat,[0],[0]
"Our method also outperforms state-of-the-art results 93.4%±0.3%, which was achieved by the ARK Tagger with various external corpus and features, e.g., Brown clustering, PTB, Freebase lists of celebrities, and video games.",4.2 Evaluation on NPSChat,[0],[0]
"ARK-Twitter data contains an entire dataset consisting of a number of tweets sampled from one particular day (October 27, 2010) described in (Gimpel et al., 2011).",4.3 Evaluation on ARK-Twitter,[0],[0]
This part is used for training.,4.3 Evaluation on ARK-Twitter,[0],[0]
"They also created another dataset, which consists of 547 tweets, for evaluation (DAILY547).",4.3 Evaluation on ARK-Twitter,[0],[0]
"This dataset consists of one random English tweet from every day between January 1, 2011 and June 30, 2012.",4.3 Evaluation on ARK-Twitter,[0],[0]
"The distribution of training data may be slightly different from the testing data, for example a substantial fraction of the messages in the training data are about a basketball game.",4.3 Evaluation on ARK-Twitter,[0],[0]
"Since ARK-Twitter uses a different tagset with PTB, we manually construct a table to link tags for the two datasets.
",4.3 Evaluation on ARK-Twitter,[0],[0]
Table 4 shows the results of different methods on this dataset.,4.3 Evaluation on ARK-Twitter,[0],[0]
"From the results, we can see that our method can achieve a better result than (Gimpel et al., 2011).",4.3 Evaluation on ARK-Twitter,[0],[0]
"However, the performance of our method is worse than the ARK Tagger.",4.3 Evaluation on ARK-Twitter,[0],[0]
"Through analyzing the errors, we find that 16.7% errors occurr between nouns and proper nouns.",4.3 Evaluation on ARK-Twitter,[0],[0]
"Since our method do not include any ontology or knowledge, proper nouns can not be easily detected.",4.3 Evaluation on ARK-Twitter,[0],[0]
"However, the ATK Tagge add a tokenlevel name list feature.",4.3 Evaluation on ARK-Twitter,[0],[0]
"The name list is useful for proper nouns recognition, which fires on names from many sources, such as Freebase lists of celebrities, the Moby Words list of US Locations, proper names from Mark Kantrowitz’s name corpus and so on.",4.3 Evaluation on ARK-Twitter,[0],[0]
"So, our model is also competitive when lacking of manual feature knowledge.",4.3 Evaluation on ARK-Twitter,[0],[0]
Part-of-Speech tagging is an important preprocessing step and can provide valuable information for various natural language processing tasks.,5 Related Work,[0],[0]
"In recent years, deep learning algorithms have been successfully used for POS tagging.",5 Related Work,[0],[0]
A number of approaches have been proposed and have achieved some progress.,5 Related Work,[0],[0]
"Santos and Guimaraes (2015) proposed
using a character-based convolutional neural network to perform the POS tagging problem.",5 Related Work,[0],[0]
"Bi-LSTMs with word, character or unicode byte embedding were also introduced to achieve the POS tagging and named entity recognition tasks (Plank et al., 2016; Chiu and Nichols, 2015; Ma and Hovy, 2016).",5 Related Work,[0],[0]
"In this work, we study the problem from a domain adaption perspective.",5 Related Work,[0],[0]
"Inspired by these works, we also propose to use character-level methods to handle out-ofvocabulary words and bi-LSTMs to model the sequence relations.
",5 Related Work,[0],[0]
"Adversarial networks were successfully used for image generation (Goodfellow et al., 2014; Dosovitskiy et al., 2015; Denton et al., 2015), domain adaption (Tzeng et al., 2014; Ganin et al., 2016), and semi-supervised learning (Denton et al., 2016).",5 Related Work,[0],[0]
"The key idea of adversarial networks for domain adaption is to construct invariant features by optimizing the feature extractor as an adversary against the domain classifier (Zhang et al., 2017).
",5 Related Work,[0],[0]
Sequence autoencoder reads the input sequence into a vector and then tries to reconstruct it.,5 Related Work,[0],[0]
Dai and Le (2015) used the model on a number of different tasks and verified its validity.,5 Related Work,[0],[0]
"Li et al. (2015) introduced the model to hierarchically build an embedding for a paragraph, showing that the model was able to encode texts to preserve syntactic, semantic, and discourse coherence.
",5 Related Work,[0],[0]
"In this work, we incorporate adversarial networks with autoencoder to obtain domaininvariant features and keep domain-specific features.",5 Related Work,[0],[0]
Our model is more suitable for target domain tasks.,5 Related Work,[0],[0]
"In this work, we propose a novel adversarial neural network to address the POS tagging problem.",6 Conclusion,[0],[0]
"Besides learning common representations between source domain and target domain, it can simultaneously preserve specific features of target domain.",6 Conclusion,[0],[0]
"The proposed method leverages newswire resources and large scale in-domain unlabeled data to help POS tagging classification on Twitter, which has a few of labeled data.",6 Conclusion,[0],[0]
We evaluate the proposed method and several state-ofthe-art methods on three different corpora.,6 Conclusion,[0],[0]
"In most of the cases, the proposed method can achieve better performance than previous methods.",6 Conclusion,[0],[0]
"Experimental results demonstrate that the proposed
method can make full use of these resources, which can be easily obtained.",6 Conclusion,[0],[0]
The authors wish to thank the anonymous reviewers for their helpful comments.,Acknowledgments,[0],[0]
"This work was partially funded by National Natural Science Foundation of China (No. 61532011, 61473092, and 61472088) and STCSM (No.16JC1420401).",Acknowledgments,[0],[0]
"In this work, we study the problem of partof-speech tagging for Tweets.",abstractText,[0],[0]
"In contrast to newswire articles, Tweets are usually informal and contain numerous out-ofvocabulary words.",abstractText,[0],[0]
"Moreover, there is a lack of large scale labeled datasets for this domain.",abstractText,[0],[0]
"To tackle these challenges, we propose a novel neural network to make use of out-of-domain labeled data, unlabeled in-domain data, and labeled indomain data.",abstractText,[0],[0]
"Inspired by adversarial neural networks, the proposed method tries to learn common features through adversarial discriminator.",abstractText,[0],[0]
"In addition, we hypothesize that domain-specific features of target domain should be preserved in some degree.",abstractText,[0],[0]
"Hence, the proposed method adopts a sequence-to-sequence autoencoder to perform this task.",abstractText,[0],[0]
Experimental results on three different datasets show that our method achieves better performance than state-of-the-art methods.,abstractText,[0],[0]
Part-of-Speech Tagging for Twitter with Adversarial Neural Networks,title,[0],[0]
"This paper is about weighted correlation clustering (Bansal et al., 2004), a combinatorial optimization problem whose feasible solutions are all clusterings of a graph, and whose objective function is a sum of weights w0, w1 : E → R+0 defined on the edgesE of the graph.",1. Introduction,[0],[0]
"The weightw0e is added to the sum if the nodes {u, v} = e ∈",1. Introduction,[0],[0]
"E are in the same cluster, and the weight w1e is added to the sum if these nodes are in distinct clusters.",1. Introduction,[0],[0]
"The problem consists in finding a clustering of minimum weight.
",1. Introduction,[0],[0]
"Weighted correlation clustering has found applications in the fields of network analysis (Cesa-Bianchi et al., 2012) and, more recently, computer vision (Kappes et al., 2011; Keuper et al., 2015; Insafutdinov et al., 2016; Beier et al., 2017; Tang et al., 2017), partly due to its key property that the number of clusters is not fixed, constrained or penalized in the problem statement but is instead defined by the (any)
1Max Planck Institute for Informatics, Saarbrücken, Germany 2Saarland University, Saarbrücken, Germany 3Bosch Center for AI, Renningen, Germany 4University of Tübingen, Germany.",1. Introduction,[0],[0]
"Correspondence to: Jan-Hendrik Lange <jlange@mpi-inf.mpg.de>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
solution.",1. Introduction,[0],[0]
"Weighted correlation clustering in general graphs is hard to solve exactly and hard to approximate (Demaine et al., 2006).",1. Introduction,[0],[0]
Remarkable progress has been made toward algorithms that find feasible solutions by approximations or heuristics (cf. Section 2).,1. Introduction,[0],[0]
"Yet, the computation of lower bounds remains challenging for large instances (Swoboda & Andres, 2017).
",1. Introduction,[0],[0]
"We make the following contributions: Firstly, in order to reduce instances in size, we establish partial optimality conditions on the graph and weights that can be checked combinatorially in polynomial time and determine the values of some variables in an optimal solution.",1. Introduction,[0],[0]
"By applying these conditions recursively, we reduce an instance in size without restricting the quality of solutions.",1. Introduction,[0],[0]
"For series-parallel graphs, our algorithm solves weighted correlation clustering exactly and in linear time, as we show.",1. Introduction,[0],[0]
"For general graphs, we demonstrate its effectiveness empirically.
",1. Introduction,[0],[0]
"Secondly, in order to compute lower bounds to the optimal objective value efficiently, we define an algorithm that outputs a heuristic solution to a packing problem that is the dual of a reformulation of weighted correlation clustering.",1. Introduction,[0],[0]
"Empirically, this algorithm is shown to exhibit a run-time/tightness trade-off that is different from both the cutting plane algorithm of Kappes et al. (2015) and the message passing algorithm of Swoboda & Andres (2017), both of which solve a canonical linear program relaxation of weighted correlation clustering.
",1. Introduction,[0],[0]
"Thirdly, toward the goal of obtaining primal feasible solutions, we define a transformation of the weights w.r.t.",1. Introduction,[0],[0]
our heuristic solution to the dual problem.,1. Introduction,[0],[0]
This transformation is again a heuristic and is motivated by complementary slackness.,1. Introduction,[0],[0]
"Empirically, local search algorithms are shown to find feasible solutions of lower original weight when applied to instances with transformed weights.
",1. Introduction,[0],[0]
"In the supplementary material, we provide additional results that were omitted from the main paper for the sake of space.",1. Introduction,[0],[0]
Implementations of our algorithms are provided on GitHub.,1. Introduction,[0],[0]
Weighted correlation clustering has a long history in the field of combinatorial optimization.,2. Related Work,[0],[0]
"Grötschel & Wakabayashi (1989) state an equivalent problem for complete graphs and
devise a branch-and-cut algorithm for solving this problem exactly.",2. Related Work,[0],[0]
"The polyhedral geometry of its feasible set is studied by Grötschel & Wakabayashi (1990); Deza et al. (1990; 1992), in the case of general graphs by Chopra & Rao (1993); Chopra (1994) and, for a more general problem, by Horňáková et al. (2017).",2. Related Work,[0],[0]
"For uniform absolute edge costs, Bansal et al. (2004) coined the name correlation clustering, established NP-hardness and the first approximation results.",2. Related Work,[0],[0]
The connection between correlation clustering in general weighted graphs and weighted multicuts was made by Demaine et al. (2006) who thus established APX-hardness and obtained anO(log|V |) approximation algorithm for the problem.,2. Related Work,[0],[0]
"Further hardness results and improved approximation algorithms for particular classes of graphs are due to Charikar et al. (2005); Chawla et al. (2006; 2015); Ailon et al. (2012); Klein et al. (2015).
",2. Related Work,[0],[0]
Approximation algorithms are typically based on rounding the solution of a linear or semi-definite program relaxation.,2. Related Work,[0],[0]
"Due to its importance, tailored algorithms for solving the linear program relaxation more efficiently than standard methods have been proposed by Yarkony et al. (2012; 2015); Swoboda & Andres (2017).",2. Related Work,[0],[0]
"Complementary to these lower bounds, a variety of fast primal heuristics have been developed to tackle large instances (Beier et al., 2014; Pan et al., 2015; Levinkov et al., 2017).",2. Related Work,[0],[0]
"Although it has been observed that, in practice, heuristic solutions are often good, it remains difficult for large instances to determine non-trivial bounds on their optimality gap.
",2. Related Work,[0],[0]
"Partial optimality concepts have been developed and exploited successfully for node labeling problems that arise from pseudo-Boolean optimization and from maximum a-posteriori inference in Markov Random Fields, cf.",2. Related Work,[0],[0]
"(Shekhovtsov, 2014; Swoboda et al., 2016).",2. Related Work,[0],[0]
"Transferring this knowledge to weighted correlation clustering is nontrivial, due to the different nature of the problem.",2. Related Work,[0],[0]
Two partial optimality conditions for weighted correlation clustering are established by Alush & Goldberger (2012) and are here generalized.,2. Related Work,[0],[0]
"Weighted correlation clustering is a combinatorial optimization problem whose feasible solutions are all clusterings of a graph.
",3.1. Weighted Correlation Clustering,[0],[0]
"Let G = (V,E) be a simple graph.",3.1. Weighted Correlation Clustering,[0],[0]
"We call a partition Π of V a clustering if every S ∈ Π induces a connected subgraph (cluster) of G. For any clustering Π of G, we denote by E0Π the set of those edges whose nodes are in the same cluster, and by E1Π the (complementary) set of those edges whose
nodes are in distinct clusters:
E0Π = {uv ∈ E",3.1. Weighted Correlation Clustering,[0],[0]
"| ∃S ∈ Π : u ∈ S and v ∈ S}, (1) E1Π = E",3.1. Weighted Correlation Clustering,[0],[0]
\,3.1. Weighted Correlation Clustering,[0],[0]
"E0Π. (2)
",3.1. Weighted Correlation Clustering,[0],[0]
"The set of edges E1Π is known as the multicut of G that corresponds to the clustering Π.
Definition 1.",3.1. Weighted Correlation Clustering,[0],[0]
"For any graph G = (V,E) and any w0, w1 : E → R+0 , the instance of weighted correlation clustering w.r.t.",3.1. Weighted Correlation Clustering,[0],[0]
"G, w0 and w1 is the optimization problem
min Π ∑ e∈E0Π w0e + ∑ e∈E1Π w1e .",3.1. Weighted Correlation Clustering,[0],[0]
(3),3.1. Weighted Correlation Clustering,[0],[0]
Weighted correlation clustering is commonly stated in the form of a binary program whose feasible solutions are the incidence vectors of the multicuts of the graph.,3.2. Minimum Cost Multicut,[0],[0]
The incidence vector xΠ ∈,3.2. Minimum Cost Multicut,[0],[0]
"{0, 1}E corresponding to the multicut induced by Π is defined as
xΠe = { 1 if e ∈",3.2. Minimum Cost Multicut,[0],[0]
"E1Π 0 else.
",3.2. Minimum Cost Multicut,[0],[0]
"(4)
Definition 2.",3.2. Minimum Cost Multicut,[0],[0]
"For any graph G = (V,E) and any c : E → R, the instance of the minimum cost multicut problem w.r.t.",3.2. Minimum Cost Multicut,[0],[0]
"G and c is the binary program
min Π ∑ e∈E ce x Π e .",3.2. Minimum Cost Multicut,[0],[0]
"(5)
The minimizers of an instance of weighted correlation clustering (Def. 1) coincide with the minimizers of the instance of minimum cost multicut (Def. 2) with c = w1−w0, since
min Π ∑ e∈E0Π w0e + ∑ e∈E1Π w1e (6)
",3.2. Minimum Cost Multicut,[0],[0]
= min Π ∑ e∈E ( w0e (1− xΠe ) +,3.2. Minimum Cost Multicut,[0],[0]
"w1e xΠe ) (7)
= ∑ e∈E
w0e︸ ︷︷ ︸ const.",3.2. Minimum Cost Multicut,[0],[0]
"+ min Π
∑ e∈E (w1e − w0e)︸ ︷︷ ︸ ce xΠe .",3.2. Minimum Cost Multicut,[0],[0]
(8),3.2. Minimum Cost Multicut,[0],[0]
"By taking the convex hull of multicut incidence vectors
MC(G) := conv{xΠ | Π clustering of G}, (9)
the minimum cost multicut problem (Def. 2) can be written as the integer linear programming problem
min x∈MC(G) ∑ e∈E ce xe.",3.3. Linear Program Relaxation,[0],[0]
"(PMC)
The set MC(G) is called multicut polytope of G (Chopra & Rao, 1993).",3.3. Linear Program Relaxation,[0],[0]
"As the minimum cost multicut problem is NP-hard, a full description of the multicut polytope in terms of its facets is impractical.",3.3. Linear Program Relaxation,[0],[0]
"For practical purposes a linear programming (LP) relaxation of PMC is derived as follows.
",3.3. Linear Program Relaxation,[0],[0]
"Denote by C(G) the set of all simple cycles of G. For any cycle C ∈ C(G), we write EC for the edge set of C. It is straight-forward to check the fact that any multicut incidence vector xΠ satisfies the system of linear inequalities
∀C ∈ C(G) ∀f",3.3. Linear Program Relaxation,[0],[0]
"∈ EC : xf ≤ ∑
e∈EC\{f}
xe , (10)
the so-called cycle inequalities (Chopra & Rao, 1993).",3.3. Linear Program Relaxation,[0],[0]
"Therefore, the standard linear programming relaxation is given by the program
min x∈CYC(G) ∑ e∈E ce xe (PCYC)
",3.3. Linear Program Relaxation,[0],[0]
"whose feasible set
CYC(G) :",3.3. Linear Program Relaxation,[0],[0]
= { x ∈,3.3. Linear Program Relaxation,[0],[0]
"[0, 1]E ∣∣x satisfies (10)} (11) is also known as the cycle relaxation of MC(G).",3.3. Linear Program Relaxation,[0],[0]
"The problem PCYC is practical, because the cycle inequalities in (10) can be separated in polynomial time.",3.3. Linear Program Relaxation,[0],[0]
"The lower bounds thus obtained can serve to solve (small) instances of the minimum cost multicut problem by branch-and-cut because the cycle relaxation has no integer vertices except the incidence vectors of multicuts, according to Lemma 1.
",3.3. Linear Program Relaxation,[0],[0]
Lemma 1 (Chopra & Rao (1993)).,3.3. Linear Program Relaxation,[0],[0]
"For any graph G = (V,E), it holds that MC(G) = CYC(G) ∩ ZE .
",3.3. Linear Program Relaxation,[0],[0]
A reference algorithm that we use for the experiments in Section 7 further exploits the fact that a cycle inequality in (10) defines a facet of MC(G) iff the associated cycle is chordless.,3.3. Linear Program Relaxation,[0],[0]
"For the presentation of this paper, we employ an alternative (integer) linear programming formulation in terms of covering cycles, which was similarly considered, e.g., by Demaine et al. (2006) for the combinatorial problem and by Charikar et al. (2005) in connection with the LP relaxation for complete graphs.",3.4. Cycle Covering Formulation,[0],[0]
"We rewrite the feasible set of the general LP relaxation relative to the cost vector c. Therefore, let G and c be fixed.
",3.4. Cycle Covering Formulation,[0],[0]
We call an edge e ∈ E repulsive if ce < 0,3.4. Cycle Covering Formulation,[0],[0]
and we call it attractive if ce > 0.,3.4. Cycle Covering Formulation,[0],[0]
"Note that we may w.l.o.g. remove all edges e ∈ E with ce = 0, since the choice of xe is irrelevant to the objective.",3.4. Cycle Covering Formulation,[0],[0]
"We write E = E+ ∪ E− where E+, E− collect all attractive and repulsive edges, respectively.
",3.4. Cycle Covering Formulation,[0],[0]
We call a cycle of G conflicted w.r.t.,3.4. Cycle Covering Formulation,[0],[0]
"(G, c) if it contains precisely one repulsive edge.",3.4. Cycle Covering Formulation,[0],[0]
"We denote by C−(G, c) ⊆ C(G) the set of all such cycles.
",3.4. Cycle Covering Formulation,[0],[0]
We consider the relaxation of CYC(G) that is constrained only by conflicted cycles.,3.4. Cycle Covering Formulation,[0],[0]
"More specifically, we consider the system
∀C ∈ C−(G, c), f ∈ EC ∩",3.4. Cycle Covering Formulation,[0],[0]
E− :,3.4. Cycle Covering Formulation,[0],[0]
"xf ≤ ∑
e∈EC\{f}
xe
(12)
of only those linear inequalities of (10) for which the edge on the left-hand side is repulsive and all other edges are attractive.",3.4. Cycle Covering Formulation,[0],[0]
"Defining
CYC−(G, c) := { x ∈",3.4. Cycle Covering Formulation,[0],[0]
"[0, 1]E ∣∣ x satisfies (12)} (13) and replacing CYC(G) by CYC−(G, c) in PCYC has no effect on the solutions, due to the following lemma, a weaker form of which was also given by Yarkony et al. (2015).",3.4. Cycle Covering Formulation,[0],[0]
Lemma 2.,3.4. Cycle Covering Formulation,[0],[0]
"For any c : E → R it holds that
min x∈CYC(G)",3.4. Cycle Covering Formulation,[0],[0]
"c>x = min x∈CYC−(G,c) c",3.4. Cycle Covering Formulation,[0],[0]
">x (14)
and
min x∈MC(G)",3.4. Cycle Covering Formulation,[0],[0]
"c>x = min x∈CYC−(G,c)∩ZE c>x. (15)
",3.4. Cycle Covering Formulation,[0],[0]
Proof.,3.4. Cycle Covering Formulation,[0],[0]
Let x∗ be an optimal solution to the right-hand side of (14).,3.4. Cycle Covering Formulation,[0],[0]
We show that x∗ satisfies all cycle inequalities (10) by contradiction.,3.4. Cycle Covering Formulation,[0],[0]
"To this end, suppose there exists a cycle C ∈ C(G) and f ∈ EC such that
x∗f > ∑
e∈EC\{f}
x∗e.
",3.4. Cycle Covering Formulation,[0],[0]
"If any edge g ∈ EC \ {f} is repulsive, then increasing x∗g would lower the objective.",3.4. Cycle Covering Formulation,[0],[0]
"Since x
∗ is optimal, there must be a conflicted cycle C ′",3.4. Cycle Covering Formulation,[0],[0]
"with g ∈ EC′ such that x∗g = ∑ e∈EC′\{g}
x∗e .",3.4. Cycle Covering Formulation,[0],[0]
Note that this means f /∈ EC′ .,3.4. Cycle Covering Formulation,[0],[0]
We write C4C ′ for the cycle obtained from the symmetric difference of EC and EC′ .,3.4. Cycle Covering Formulation,[0],[0]
"Apparently, the cycle C4C ′ has one repulsive edge less and f ∈ EC4C′ .",3.4. Cycle Covering Formulation,[0],[0]
"Therefore, by repeating the argument, we may w.l.o.g. assume that all edges in EC \ {f} are attractive.",3.4. Cycle Covering Formulation,[0],[0]
"Now assume that f is attractive as well, then decreasing x∗f would lower the objective.",3.4. Cycle Covering Formulation,[0],[0]
"Therefore, since x
∗ is optimal, there is a conflicted cycle C ′ with f ∈ EC′ and g ∈ EC′ ∩ E− such that
x∗g = x ∗",3.4. Cycle Covering Formulation,[0],[0]
"f + ∑ e∈EC′\{f,g} x∗e
> ∑
e∈EC\{f}
x∗e + ∑
e∈EC′\{f,g}
x∗e
≥ ∑
e∈EC4C′\{g}
x∗e.
",3.4. Cycle Covering Formulation,[0],[0]
Note that C4C ′ is a conflicted cycle.,3.4. Cycle Covering Formulation,[0],[0]
"Thus, we conclude that x∗ violates an inequality of (12) and hence cannot be feasible.",3.4. Cycle Covering Formulation,[0],[0]
"This concludes the proof of (14), the argument for (15) is analogous.
",3.4. Cycle Covering Formulation,[0],[0]
"With the help of Lemma 2, we formulate PMC as a set covering problem: Definition 3.",3.4. Cycle Covering Formulation,[0],[0]
"For any graph G = (V,E) and any c ∈ RE , we call
min x̂∈SC(G,c) ∑ e∈E |ce| x̂e (PSC)
with SC(G, c) the convex hull of all x̂ ∈ ZE that satisfy the system
∀C ∈ C−(G, c) : ∑ e∈EC",3.4. Cycle Covering Formulation,[0],[0]
"x̂e ≥ 1 (16)
∀e ∈ E : x̂e ≥ 0 (17)
the set covering problem w.r.t.",3.4. Cycle Covering Formulation,[0],[0]
"conflicted cycles, and we call SC(G, c) the set covering polyhedron w.r.t.",3.4. Cycle Covering Formulation,[0],[0]
conflicted cycles.,3.4. Cycle Covering Formulation,[0],[0]
Lemma 3.,3.4. Cycle Covering Formulation,[0],[0]
"For any graph G = (V,E) and any c ∈ RE , we have
min x∈CYC−(G,c)∩ZE ∑ e∈E ce xe
= Ltriv + min x̂∈SC(G,c) ∑ e∈E |ce| x̂e (18)
with
Ltriv = ∑ e∈E− ce (19)
the sum of negative edge costs (a trivial lower bound to the optimal value of PMC).
",3.4. Cycle Covering Formulation,[0],[0]
Proof.,3.4. Cycle Covering Formulation,[0],[0]
We define x̂ via x̂e,3.4. Cycle Covering Formulation,[0],[0]
:= xe for any attractive edge e ∈ E+,3.4. Cycle Covering Formulation,[0],[0]
and x̂e,3.4. Cycle Covering Formulation,[0],[0]
:= 1− xe for any repulsive edge e ∈,3.4. Cycle Covering Formulation,[0],[0]
"E−. Since any conflicted cycle C ∈ C−(G, c) has precisely one repulsive edge, all conflicted cycle inequalities (12) become covering inequalities.",3.4. Cycle Covering Formulation,[0],[0]
"In this section, we study partial optimality for PMC.",4. Partial Optimality,[0],[0]
"More precisely, we establish conditions on an edge e ∈ E which guarantee that xe assumes one particular value, either 0 or 1, in at least one optimal solution (weak persistency).",4. Partial Optimality,[0],[0]
"Fixations to 0 are of particular interest as they can be implemented as edge contractions (with subsequent merging of parallel edges), which effectively reduce the size of a given instance of the problem.",4. Partial Optimality,[0],[0]
"As a corollary, we obtain an algorithm that solves weighted correlation clustering problems on seriesparallel graphs in linear time.",4. Partial Optimality,[0],[0]
A direct consequence from Lemma 3 is that we may disregard all edges that are not contained in any conflicted cycle.,4.1. Basic Conditions,[0],[0]
There are (at least) two ways this can happen: 1.,4.1. Basic Conditions,[0],[0]
"An edge e ∈ E is not contained in any cycle at all, that is, e is a bridge.",4.1. Basic Conditions,[0],[0]
2.,4.1. Basic Conditions,[0],[0]
"The endpoints of a repulsive edge e = {u, v} ∈",4.1. Basic Conditions,[0],[0]
"E− belong to different components of G+ = (V,E+).",4.1. Basic Conditions,[0],[0]
"In both cases, for any optimal solution x∗ of PMC, it holds that x∗e = 0",4.1. Basic Conditions,[0],[0]
"if e is attractive, and x ∗ e",4.1. Basic Conditions,[0],[0]
= 1 if e is repulsive.,4.1. Basic Conditions,[0],[0]
"Thus, we can restrict the instance of the problem to the maximal components ofG that are connected in G+ and biconnected in G. This was also observed by Alush & Goldberger (2012).
",4.1. Basic Conditions,[0],[0]
"Below, we establish more general partial optimality conditions.",4.1. Basic Conditions,[0],[0]
"To this end, we need the following notation.",4.1. Basic Conditions,[0],[0]
"A cut of G is a bipartition B = (S1, S2) of the nodes V , i.e. V = S1 ∪̇S2.",4.1. Basic Conditions,[0],[0]
The edge set of the cut B is denoted by EB = {uv ∈ E,4.1. Basic Conditions,[0],[0]
"| u ∈ S1, v ∈ S2}.",4.1. Basic Conditions,[0],[0]
Definition 4.,4.2. Dominant Edges,[0],[0]
"Let G = (V,E) be any graph and let c ∈ RE .",4.2. Dominant Edges,[0],[0]
An edge f ∈ E is called dominant attractive iff cf > 0,4.2. Dominant Edges,[0],[0]
"and there exists a cut B with f ∈ EB such that
cf ≥ ∑
e∈EB\{f}
|ce| .",4.2. Dominant Edges,[0],[0]
"(20)
An edge f ∈",4.2. Dominant Edges,[0],[0]
E− is called dominant repulsive iff cf < 0,4.2. Dominant Edges,[0],[0]
"and there exists a cut B with f ∈ EB such that
|cf | ≥ ∑
e∈EB∩E+ ce.",4.2. Dominant Edges,[0],[0]
"(21)
",4.2. Dominant Edges,[0],[0]
"An edge is called dominant iff it is dominant attractive or dominant repulsive.
",4.2. Dominant Edges,[0],[0]
Lemma 4.,4.2. Dominant Edges,[0],[0]
"Let G = (V,E) be any graph and let c ∈ RE .
",4.2. Dominant Edges,[0],[0]
(i),4.2. Dominant Edges,[0],[0]
"If f ∈ E is dominant attractive, then x∗f = 0 in at least one optimal solution x∗ of PMC.
(ii) If f ∈ E is dominant repulsive, then x∗f = 1 in at least one optimal solution x∗ of PMC.
",4.2. Dominant Edges,[0],[0]
Proof.,4.2. Dominant Edges,[0],[0]
(i),4.2. Dominant Edges,[0],[0]
We use the set covering formulation of PMC.,4.2. Dominant Edges,[0],[0]
Suppose f ∈ E+ is dominant and x̂∗f = 1 in an optimal solution x̂∗ of PSC.,4.2. Dominant Edges,[0],[0]
"Every conflicted cycle that contains f also contains some edge e ∈ EB , since B is a cut.",4.2. Dominant Edges,[0],[0]
"Therefore, the vector x̂ ∈ {0, 1}E defined by
x̂e =  0",4.2. Dominant Edges,[0],[0]
"if e = f 1 if e ∈ EB , e 6= f x̂∗e else
is a feasible solution to PSC.",4.2. Dominant Edges,[0],[0]
"It has the same objective value as x̂∗, since f is dominant and x̂∗ is optimal.
",4.2. Dominant Edges,[0],[0]
(ii) Suppose f ∈,4.2. Dominant Edges,[0],[0]
E− is dominant and x̂∗f,4.2. Dominant Edges,[0],[0]
= 1 in an optimal solution x̂∗ of PSC,4.2. Dominant Edges,[0],[0]
.,4.2. Dominant Edges,[0],[0]
Every conflicted cycle that contains f also contains some edge e ∈ EB ∩,4.2. Dominant Edges,[0],[0]
"E+, since B is a cut and every conflicted cycle contains only one repulsive edge.",4.2. Dominant Edges,[0],[0]
"Then the vector x̂ ∈ {0, 1}E defined by x̂f = 0, x̂e = 1 for all e ∈ EB ∩ E+ and x̂e = x̂∗e elsewhere is a feasible solution to PSC.",4.2. Dominant Edges,[0],[0]
"It has the same objective value as x̂∗, since f is dominant and x̂∗ is optimal.
",4.2. Dominant Edges,[0],[0]
"Lemma 4 generalizes the basic conditions discussed in Section 4.1, since each edge f ∈ E that is not contained in any conflicted cycle is also dominant.",4.2. Dominant Edges,[0],[0]
"Dominance of edges can be decided in polynomial time, by computing minimum st-cuts in G for a suitable choice of capacities.",4.2. Dominant Edges,[0],[0]
"In practice, the required computational effort may be mitigated by constructing a cut tree of G (Gomory & Hu, 1961).",4.2. Dominant Edges,[0],[0]
"The practically most relevant cuts can even be checked in linear time, which we discuss in the following section.",4.2. Dominant Edges,[0],[0]
"In practice, it is expected that dominant edges are more likely to be found in cuts that are relatively sparse.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"We discuss two special cases of sparse cuts that are of particular interest, due to the following reasons.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"First, they can be checked in linear time, which gives rise to a fast preprocessing algorithm.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"Second, we show that our techniques solve PMC to optimality if G is series-parallel.
",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
Two-edge cuts.,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"Suppose B is a two-edge cut of G, i.e. EB = {e, f} for two edges e, f ∈",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
E.,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"Apparently, according to (20) and (21), at least one of them must be dominant.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"Further, it is guaranteed that we can simplify the instance by edge deletions or contractions.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"To see this, distinguish the following cases.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"If both e and f are repulsive, then both of them are dominant and we can delete them, as they are not contained in any conflicted cycle.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"If f is dominant attractive, we can contract f .",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"Finally, if f is dominant repulsive and e is attractive, then we can switch the signs of their coefficients and redefine xf := 1 − xf as well as xe := 1 − xe.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"Since |EB | = 2, this operation does not change the set of conflicted cycles of G and thus is valid (while only adding a constant to the objective).",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"Afterwards, the edge f is dominant attractive and we can contract f .",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"The two-edge cuts of G can be found in linear time, by computing the 3-edge-connected components of G, cf.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"(Mehlhorn et al., 2017).
",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
Single-node cuts.,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"For any v ∈ V , let Bv = ({v}, V \ {v}) denote the cut that is induced by v. Whether EBv contains a dominant edge is easily decided by considering all edges incident to v. Moreover, if deg v = 2, then Bv is also a two-edge cut and we can apply the operation described in
Algorithm 1 Single-Node Cut Preprocessing input G = (V,E), c :",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"E → R
1: Initialize objective value offset ∆ = 0.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
2: Initialize a queue Q = V .,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
3: while Q 6= ∅,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
do 4: Extract a vertex v ∈ Q. 5: if deg v = 1 then 6: Get neighbor u ∈ V .,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"7: if cuv ≥ 0 then 8: Set xuv = 0 and contract uv ∈ E. 9: else
10: Set xuv = 1, ∆ = ∆ + cuv and delete uv ∈ E. 11: end if 12: else if deg v = 2 then 13: Get neighbors u,w ∈ V with |cuv| ≥ |cwv|.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
14: if uv ∈ E+ then 15: Set xuv = 0 and contract uv ∈ E. 16: else if uv ∈,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
E− and wv ∈,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
E− then 17: Adjust offset ∆ = ∆ + cuv + cwv .,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"18: Set xuv = xwv = 1 and delete uv,wv ∈",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
E. 19: else if uv ∈,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
E− and wv ∈ E+ then 20:,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
Adjust offset ∆ = ∆ + cuv + cwv .,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"21: Redefine xuv = 1− xuv , xwv = 1− xwv and cuv = −cuv , cwv = −cwv .",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
22: Set xuv = 0 and contract uv ∈ E. 23: end if 24: else if ∃f ∈ Bv dominant attractive then 25: Set xf = 0 and contract f ∈,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
E. 26: end if 27:,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
Add to Q all vertices u /∈,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
Q whose neighborhood was changed.,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"28: end while 29: return (G, c), x,∆
the last paragraph.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
Updating the graph and applying these techniques recursively as specified in Algorithm 1 takes linear time.,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"This has the following theoretical consequence.
",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
Corollary 1.,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"If G has treewidth at most 2, then Algorithm 1 can be implemented to solve PMC exactly in O(|V |) time.
",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
Proof.,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
Place the vertices of G into buckets of ascending degree and always pick a vertex of minimal degree.,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
Every graph of treewidth 2 has a vertex v with deg v ≤ 2.,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"Since Algorithm 1 only contracts or deletes edges, fixing the variables according to Lemma 4, the updated graph still has treewidth at most 2.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"The number of nodes decreases by 1 in every iteration, hence the algorithm terminates in time O(|E|) = O(|V |) and outputs an optimal solution.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"In this section, we define an algorithm for computing lower bounds for PMC.",5. Dual Lower Bounds,[0],[0]
"This algorithm exploits the structure of
Algorithm 2 Iterative Cycle Packing (ICP) input G = (V,E), c :",5. Dual Lower Bounds,[0],[0]
"E → R
1: Initialize we = |ce| for all e ∈ E and y = 0, L = Ltriv. 2: for ` = 3 . . .",5. Dual Lower Bounds,[0],[0]
"|E| do 3: while ∃C ∈ C−(G, c) : |EC | ≤ ` do 4: Pick C ∈ C−(G, c) such that |EC | ≤",5. Dual Lower Bounds,[0],[0]
"`. 5: Compute yC = mine∈EC we.
6: Redefine we = { we − yC",5. Dual Lower Bounds,[0],[0]
if e ∈ EC we else.,5. Dual Lower Bounds,[0],[0]
7: Increase lower bound L = L+ yC .,5. Dual Lower Bounds,[0],[0]
"8: Remove all edges e ∈ E with we = 0 from G. 9: end while
10: if C−(G, c) = ∅ then 11: return y, L 12: end if 13: end for
the reformulation PSC.",5. Dual Lower Bounds,[0],[0]
"It computes a heuristic solution to the dual of its LP relaxation.
",5. Dual Lower Bounds,[0],[0]
"The LP relaxation (up to the constant Ltriv) of problem PSC is given by
min ∑ e∈E |ce|x̂e (22)
subject to ∑ e∈EC x̂e ≥ 1 ∀C ∈ C−(G, c) (23)
",5. Dual Lower Bounds,[0],[0]
"x̂e ≥ 0 ∀e ∈ E .
",5. Dual Lower Bounds,[0],[0]
"The corresponding dual program reads
max ∑
C∈C−(G,c)
yC (24)
subject to ∑
C: e∈EC
yC ≤ |ce| ∀e ∈ E (25)
yC ≥ 0 ∀C ∈ C−(G, c) .
",5. Dual Lower Bounds,[0],[0]
"A heuristic solution of (24), and thus a lower bound for (22), is found by Algorithm 2 that we call Iterative Cycle Packing (ICP).",5. Dual Lower Bounds,[0],[0]
It works as follows:,5. Dual Lower Bounds,[0],[0]
"Firstly, it chooses a conflicted cycle C and increases yC as much as possible.",5. Dual Lower Bounds,[0],[0]
"Secondly, it decreases the weights we (initially |ce|) of all edges e ∈ EC by yC and removes all edges of zero weight.",5. Dual Lower Bounds,[0],[0]
"These steps are repeated until there are no conflicted cycles left.
",5. Dual Lower Bounds,[0],[0]
Implementation details.,5. Dual Lower Bounds,[0],[0]
"The absolute running time of ICP as well as the quality of the output lower bounds depends on the choice of cycles C. We pursue the following strategy that we found to perform well empirically in both aspects: In each iteration of the main loop, we choose a repulsive edge e = uv ∈",5. Dual Lower Bounds,[0],[0]
"E− such that u and v are in the same connected component of G+ = (V,E+).",5. Dual Lower Bounds,[0],[0]
"Then, we find a conflicted cycle containing e by searching for a shortest path
(in terms of hop distance) from u to v in G+.",5. Dual Lower Bounds,[0],[0]
"We apply this search for conflicted cycles in rounds of increasing cycle length, using breadth-first search with an early termination criterion based on the hop distance.",5. Dual Lower Bounds,[0],[0]
We also maintain and periodically update a component labeling of G+ in order to to reduce the number of redundant shortest path searches.,5. Dual Lower Bounds,[0],[0]
"In this section, we exploit the dual solution in primal algorithms.",6. Re-weighting for Primal Algorithms,[0],[0]
"The motivation is due to complementary slackness, which is made explicit in the following lemma.
",6. Re-weighting for Primal Algorithms,[0],[0]
Lemma 5.,6. Re-weighting for Primal Algorithms,[0],[0]
"Assume the primal LP (22) is tight, i.e., its optimal solution x̂∗ also solves PSC, and the solution output by ICP solves the dual (24) optimally.",6. Re-weighting for Primal Algorithms,[0],[0]
"Then, for every e ∈ E with positive residual weight we > 0, it holds that x̂∗e = 0.
",6. Re-weighting for Primal Algorithms,[0],[0]
Proof.,6. Re-weighting for Primal Algorithms,[0],[0]
"If we > 0, the constraint (25) at e ∈ E is inactive at the optimal dual solution.",6. Re-weighting for Primal Algorithms,[0],[0]
"Thus, x̂∗e = 0 in the optimal primal solution, by complementary slackness.
",6. Re-weighting for Primal Algorithms,[0],[0]
"Of course, the assumption of Lemma 5 is too strong for practical purposes.",6. Re-weighting for Primal Algorithms,[0],[0]
"However, the intuition is that if the LP relaxation is fairly tight and the obtained dual solution is close to optimal, it can still provide useful information about the primal problem.",6. Re-weighting for Primal Algorithms,[0],[0]
"More specifically, the weights we output by ICP can be interpreted as an indication of how likely the primal variable x̂e is zero in an optimal solution.",6. Re-weighting for Primal Algorithms,[0],[0]
"In order to make use of this information, we propose to shift the weights of the primal problem to a convex combination λ|ce|+ (1− λ)we of the original and residual weights, for a suitable choice of λ ∈ (0, 1).",6. Re-weighting for Primal Algorithms,[0],[0]
Experiments in Section 7 show that this shift can guide primal heuristics toward better feasible solutions to the original problem.,6. Re-weighting for Primal Algorithms,[0],[0]
"In this section, we study partial optimality, dual lower bounds and re-weightings empirically, for all instances of
the weighted correlation clustering problem from Kappes et al. (2015) and Leskovec et al. (2010).
Instances.",7. Experiments,[0],[0]
"From Kappes et al. (2015), we consider all three collections of instances: Image Segmentation contains instances w.r.t.",7. Experiments,[0],[0]
planar superpixel adjacency graphs of photographs.,7. Experiments,[0],[0]
Knott-3D contains instances w.r.t.,7. Experiments,[0],[0]
non-planar supervoxel adjacency graphs of volume images taken by a serial sectioning electron microscope.,7. Experiments,[0],[0]
Modularity Clustering contains instances w.r.t.,7. Experiments,[0],[0]
complete graphs.,7. Experiments,[0],[0]
"In all three collections, the edge costs ce are fractional and non-uniform.",7. Experiments,[0],[0]
"For all these instances, except one in the collection Modularity Clustering, optimal solutions are accessible and are computed here as a reference.",7. Experiments,[0],[0]
"From Leskovec et al. (2010), we consider directed graphs of the social networks Epinions and Slashdot, each with more than half a million edges labeled either +1 or −1.",7. Experiments,[0],[0]
"Instances of the minimum cost multicut problem are defined here by removing the orientation of edges, by deleting all self-loops, and by replacing parallel edges by a single edge with the sum of their costs1.",7. Experiments,[0],[0]
"In order to study the partial optimality conditions of Section 4 empirically, we process the above instances as follows: First, we remove all edges of cost 0, all bridges, as well as all repulsive edges whose nodes belong to distinct connected components of G+.",7.1. Partial Optimality,[0],[0]
"Second, we check for every v ∈ V whether the cut Bv = ({v}, V \ {v}) induces dominant edges.",7.1. Partial Optimality,[0],[0]
"If we find dominant attractive edges or vertices of degree ≤ 2, we perform contractions and deletions according to Alg. 1.",7.1. Partial Optimality,[0],[0]
"Both steps are repeated until no further edges can be removed or contracted.
",7.1. Partial Optimality,[0],[0]
"After the main reduction step, which takes linear time and is thus very fast, we further check all remaining edges uv ∈ E for dominance in any (general) uv-cut.",7.1. Partial Optimality,[0],[0]
"To this end, we construct a cut tree of G with the help of Gusfield’s algorithm (Gusfield, 1990), which takes |V | − 1 max-flow computations.",7.1. Partial Optimality,[0],[0]
"Despite the increased computational effort, we only found a small number of additional dominant attractive edges and thus could only perform few further contractions.",7.1. Partial Optimality,[0],[0]
"However, we found a significant number of additional dom-
1This results in 2703 edges of cost 0 for Epinions, and 1949 such edges for Slashdot.
inant repulsive edges.
",7.1. Partial Optimality,[0],[0]
The effect of our method in the total number of nodes and edges is shown in Table 1.,7.1. Partial Optimality,[0],[0]
We also report the number of remaining edges that are not dominant repulsive.,7.1. Partial Optimality,[0],[0]
It can be seen from this table that the numbers are effectively reduced.,7.1. Partial Optimality,[0],[0]
"This is explained, firstly, by the sparsity of the graphs and, secondly, by the non-uniformity of the costs.",7.1. Partial Optimality,[0],[0]
"From the comparison to the number of remaining non-persistent variables when only the criteria of Alush & Goldberger (2012) are applied, it can be seen that our more general criteria reveal considerably more persistency.
",7.1. Partial Optimality,[0],[0]
It may be expected that optimization methods benefit in terms of runtime from the reduced size of the instances.,7.1. Partial Optimality,[0],[0]
"On the instances of Kappes et al. (2015), we found the effect to be insignificant due to their small original size.",7.1. Partial Optimality,[0],[0]
"On Epinions and Slashdot, however, the runtime of the local search algorithm GAEC+KLj (cf. Section 7.3) decreased by more than 70%.",7.1. Partial Optimality,[0],[0]
"For completeness, we provide the numbers in the supplements.",7.1. Partial Optimality,[0],[0]
"In order to put into perspective the dual lower bounds output by Iterative Cycle Packing (ICP) as described in Section 5, we compare this algorithm, firstly, to the cutting plane algorithm for PCYC of Kappes et al. (2015), with Gurobi for solving the LPs (denoted here by LP) and, secondly, to the message passing algorithm of Swoboda & Andres (2017), applied to PCYC, with code and parameter settings kindly provided by the authors (denoted here by MPC).
",7.2. Dual Lower Bounds,[0],[0]
Results are shown in Figure 1 and Table 2.,7.2. Dual Lower Bounds,[0],[0]
"It can be seen from the figure and the table that, for the large and hard instances Epinions and Slashdot, ICP converges at under 102 seconds, outputting lower bounds that are matched and exceeded by MPC at around 103 seconds.",7.2. Dual Lower Bounds,[0],[0]
"It can be seen from Table 2 that the situation is similar for the smaller instances: The lower bounds output by ICP are a bit worse than those output by LP or MPC (here compared to the best optimal solution known) but are obtained faster (by as much as three orders of magnitude for Knott-3D-450).
",7.2. Dual Lower Bounds,[0],[0]
"It is known from Kappes et al. (2015) that their instances can be solved faster than their LP relaxations by means of branch-and-cut, separating only integer infeasible points
·105
Epinions
·105
Slashdot
by cycle inequalities using BFS (instead of Dijkstra’s algorithm), and resorting to the strong (undisclosed) cuts of Gurobi for cutting off fractional solutions.",7.2. Dual Lower Bounds,[0],[0]
We restrict our comparison here to algorithms that seek to solve the LP relaxation PCYC.,7.2. Dual Lower Bounds,[0],[0]
This is justified by the fact that size ultimately renders integer linear programming intractable.,7.2. Dual Lower Bounds,[0],[0]
We conclude that ICP is capable of computing non-trivial lower bounds fast.,7.2. Dual Lower Bounds,[0],[0]
"In order to study the re-weighting described in Section 6, we measure its effect on heuristic algorithms for finding feasible solutions.",7.3. Re-weighting,[0],[0]
"To this end, we employ the implementations of Levinkov et al. (2017) of Greedy Additive Edge Contraction (GAEC), an algorithm that starts from singleton clusters and greedily contracts attractive edges with maximum nonnegative cost, and of KLj, the well-known Kernighan-Lin heuristic for graph partitioning that recursively improves an initial clustering by splitting, merging or exchanging nodes between neighboring clusters.
",7.3. Re-weighting,[0],[0]
"A comparison between the feasible solutions found by applying the heuristics GAEC and GAEC+KLj to original instances, on the one hand, and to instances re-weighted by ICP with λ = 12 , on the other hand, can be found in Table
3.",7.3. Re-weighting,[0],[0]
"Note that we only re-weight the input to GAEC and let KLj run with original weights, starting from the solution returned by GAEC, as we found this approach to be advantageous.",7.3. Re-weighting,[0],[0]
It can be seen from Table 3 that our re-weighting consistently improves the gap.,7.3. Re-weighting,[0],[0]
"On average, it is slightly less effective than the reparameterization with the more accurate dual solutions obtained from MPC, as proposed by Swoboda & Andres (2017).",7.3. Re-weighting,[0],[0]
A more detailed comparison is provided in the supplements.,7.3. Re-weighting,[0],[0]
"We have established partial optimality conditions, a heuristic lower bound and a heuristic re-weighting for instances of the weighted correlation clustering problem.",8. Conclusion,[0],[0]
We have shown advantages of each of these constructions empirically.,8. Conclusion,[0],[0]
Checking a subset of our partial optimality conditions recursively gives a fast combinatorial algorithm that efficiently reduces the size of problem instances.,8. Conclusion,[0],[0]
"Conceptually, it solves the problem for series-parallel graphs to optimality, in linear time.",8. Conclusion,[0],[0]
Our dual heuristic algorithm provides nontrivial lower bounds and valuable dual information fast.,8. Conclusion,[0],[0]
"For future work, it is relevant to examine if more sophisticated dual solvers such as MPC benefit from a “warm-start” that transforms and exploits the heuristic dual solution.",8. Conclusion,[0],[0]
Weighted correlation clustering is hard to solve and hard to approximate for general graphs.,abstractText,[0],[0]
Its applications in network analysis and computer vision call for efficient algorithms.,abstractText,[0],[0]
"To this end, we make three contributions: We establish partial optimality conditions that can be checked efficiently, and doing so recursively solves the problem for series-parallel graphs to optimality, in linear time.",abstractText,[0],[0]
"We exploit the packing dual of the problem to compute a heuristic, but non-trivial lower bound faster than that of a canonical linear program relaxation.",abstractText,[0],[0]
We introduce a re-weighting with the dual solution by which efficient local search algorithms converge to better feasible solutions.,abstractText,[0],[0]
The effectiveness of our methods is demonstrated empirically on a number of benchmark instances.,abstractText,[0],[0]
Partial Optimality and Fast Lower Bounds for Weighted Correlation Clustering,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3719–3728 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3719",text,[0],[0]
Many interpretation methods for neural networks explain the model’s prediction as a counterfactual: how does the prediction change when the input is modified?,1 Introduction,[0],[0]
"Adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015) highlight the instability of neural network predictions by showing how small perturbations to the input dramatically change the output.
",1 Introduction,[0],[0]
"A common, non-adversarial form of model interpretation is feature attribution: features that are crucial for predictions are highlighted in a heatmap.",1 Introduction,[0],[0]
One can measure a feature’s importance by input perturbation.,1 Introduction,[0],[0]
"Given an input for text classification, a word’s importance can be measured by the difference in model confidence before and after that word is removed from the input—the word is important if confidence decreases significantly.",1 Introduction,[0],[0]
"This is the leave-one-out method (Li et al., 2016b).",1 Introduction,[0],[0]
"Gradients can also measure feature importance; for example, a feature is influential to the prediction if its gradient is a large positive value.",1 Introduction,[0],[0]
"Both perturbation and gradient-based methods can generate heatmaps, implying that the model’s prediction is highly influenced by the highlighted, important words.
",1 Introduction,[0],[0]
"Instead, we study how the model’s prediction is influenced by the unimportant words.",1 Introduction,[0],[0]
"We use input reduction, a process that iteratively removes the unimportant words from the input while maintaining the model’s prediction.",1 Introduction,[0],[0]
"Intuitively, the words remaining after input reduction should be important for prediction.",1 Introduction,[0],[0]
"Moreover, the words
should match the leave-one-out method’s selections, which closely align with human perception (Li et al., 2016b; Murdoch et al., 2018).",1 Introduction,[0],[0]
"However, rather than providing explanations of the original prediction, our reduced examples more closely resemble adversarial examples.",1 Introduction,[0],[0]
"In Figure 1, the reduced input is meaningless to a human but retains the same model prediction with high confidence.",1 Introduction,[0],[0]
"Gradient-based input reduction exposes pathological model behaviors that contradict what one expects based on existing interpretation methods.
",1 Introduction,[0],[0]
"In Section 2, we construct more of these counterintuitive examples by augmenting input reduction with beam search and experiment with three tasks: SQUAD (Rajpurkar et al., 2016) for reading comprehension, SNLI (Bowman et al., 2015) for textual entailment, and VQA (Antol et al., 2015) for visual question answering.",1 Introduction,[0],[0]
Input reduction with beam search consistently reduces the input sentence to very short lengths—often only one or two words—without lowering model confidence on its original prediction.,1 Introduction,[0],[0]
"The reduced examples appear nonsensical to humans, which we verify with crowdsourced experiments.",1 Introduction,[0],[0]
"In Section 3, we draw connections to adversarial examples and confidence calibration; we explain why the observed pathologies are a consequence of the overconfidence of neural models.",1 Introduction,[0],[0]
This elucidates limitations of interpretation methods that rely on model confidence.,1 Introduction,[0],[0]
"In Section 4, we encourage high model uncertainty on reduced examples with entropy regularization.",1 Introduction,[0],[0]
"The pathological model behavior under input reduction is mitigated, leading to more reasonable reduced examples.",1 Introduction,[0],[0]
"To explain model predictions using a set of important words, we must first define importance.",2 Input Reduction,[0],[0]
"After defining input perturbation and gradient-based approximation, we describe input reduction with these importance metrics.",2 Input Reduction,[0],[0]
Input reduction drastically shortens inputs without causing the model to change its prediction or significantly decrease its confidence.,2 Input Reduction,[0],[0]
Crowdsourced experiments confirm that reduced examples appear nonsensical to humans: input reduction uncovers pathological model behaviors.,2 Input Reduction,[0],[0]
"Ribeiro et al. (2016) and Li et al. (2016b) define importance by seeing how confidence changes when a feature is removed; a natural approximation is to use the gradient (Baehrens et al., 2010; Simonyan et al., 2014).",2.1 Importance from Input Gradient,[0],[0]
We formally define these importance metrics in natural language contexts and introduce the efficient gradient-based approximation.,2.1 Importance from Input Gradient,[0],[0]
"For each word in an input sentence, we measure its importance by the change in the confidence of the original prediction when we remove that word from the sentence.",2.1 Importance from Input Gradient,[0],[0]
"We switch the sign so that when the confidence decreases, the importance value is positive.
",2.1 Importance from Input Gradient,[0],[0]
"Formally, let x = 〈x1, x2, . . .",2.1 Importance from Input Gradient,[0],[0]
"xn〉 denote the input sentence, f(y |x)",2.1 Importance from Input Gradient,[0],[0]
"the predicted probability of label y, and y = argmaxy′ f(y
′ |x)",2.1 Importance from Input Gradient,[0],[0]
the original predicted label.,2.1 Importance from Input Gradient,[0],[0]
"The importance is then
g(xi",2.1 Importance from Input Gradient,[0],[0]
| x) = f(y |x)− f(y |x−i).,2.1 Importance from Input Gradient,[0],[0]
"(1)
To calculate the importance of each word in a sentence with n words, we need n forward passes of the model, each time with one of the words left out.",2.1 Importance from Input Gradient,[0],[0]
"This is highly inefficient, especially for longer sentences.",2.1 Importance from Input Gradient,[0],[0]
"Instead, we approximate the importance value with the input gradient.",2.1 Importance from Input Gradient,[0],[0]
"For each word in the sentence, we calculate the dot product of its word embedding and the gradient of the output with respect to the embedding.",2.1 Importance from Input Gradient,[0],[0]
The importance of n words can thus be computed with a single forward-backward pass.,2.1 Importance from Input Gradient,[0],[0]
"This gradient approximation has been used for various interpretation methods for natural language classification models (Li et al., 2016a; Arras et al., 2016); see Ebrahimi et al. (2017) for further details on the derivation.",2.1 Importance from Input Gradient,[0],[0]
We use this approximation in all our experiments as it selects the same words for removal as an exhaustive search (no approximation).,2.1 Importance from Input Gradient,[0],[0]
Instead of looking at the words with high importance values—what interpretation methods commonly do—we take a complementary approach and study how the model behaves when the supposedly unimportant words are removed.,2.2 Removing Unimportant Words,[0],[0]
"Intuitively, the important words should remain after the unimportant ones are removed.
",2.2 Removing Unimportant Words,[0],[0]
Our input reduction process iteratively removes the unimportant words.,2.2 Removing Unimportant Words,[0],[0]
"At each step, we remove the word with the lowest importance value until the model changes its prediction.",2.2 Removing Unimportant Words,[0],[0]
"We experi-
ment with three popular datasets: SQUAD (Rajpurkar et al., 2016) for reading comprehension, SNLI (Bowman et al., 2015) for textual entailment, and VQA (Antol et al., 2015) for visual question answering.",2.2 Removing Unimportant Words,[0],[0]
"We describe each of these tasks and the model we use below, providing full details in the Supplement.
",2.2 Removing Unimportant Words,[0],[0]
"In SQUAD, each example is a context paragraph and a question.",2.2 Removing Unimportant Words,[0],[0]
The task is to predict a span in the paragraph as the answer.,2.2 Removing Unimportant Words,[0],[0]
We reduce only the question while keeping the context paragraph unchanged.,2.2 Removing Unimportant Words,[0],[0]
"The model we use is the DRQA Document Reader (Chen et al., 2017).
",2.2 Removing Unimportant Words,[0],[0]
"In SNLI, each example consists of two sentences: a premise and a hypothesis.",2.2 Removing Unimportant Words,[0],[0]
"The task is to predict one of three relationships: entailment, neutral, or contradiction.",2.2 Removing Unimportant Words,[0],[0]
We reduce only the hypothesis while keeping the premise unchanged.,2.2 Removing Unimportant Words,[0],[0]
"The model we use is Bilateral Multi-Perspective Matching (BIMPM) (Wang et al., 2017).
",2.2 Removing Unimportant Words,[0],[0]
"In VQA, each example consists of an image and a natural language question.",2.2 Removing Unimportant Words,[0],[0]
We reduce only the question while keeping the image unchanged.,2.2 Removing Unimportant Words,[0],[0]
"The model we use is Show, Ask, Attend, and Answer (Kazemi and Elqursh, 2017).
",2.2 Removing Unimportant Words,[0],[0]
"During the iterative reduction process, we ensure that the prediction does not change (exact same span for SQUAD); consequently, the model accuracy on the reduced examples is identical to the original.",2.2 Removing Unimportant Words,[0],[0]
The predicted label is used for input reduction and the ground-truth is never revealed.,2.2 Removing Unimportant Words,[0],[0]
"We use the validation set for all three tasks.
",2.2 Removing Unimportant Words,[0],[0]
Most reduced inputs are nonsensical to humans (Figure 2) as they lack information for any reasonable human prediction.,2.2 Removing Unimportant Words,[0],[0]
"However, models make confident predictions, at times even more confident than the original.
",2.2 Removing Unimportant Words,[0],[0]
"To find the shortest possible reduced inputs (potentially the most meaningless), we relax the requirement of removing only the least important word and augment input reduction with beam search.",2.2 Removing Unimportant Words,[0],[0]
"We limit the removal to the k least important words, where k is the beam size, and decrease the beam size as the remaining input is shortened.1",2.2 Removing Unimportant Words,[0],[0]
We empirically select beam size five as it produces comparable results to larger beam sizes with reasonable computation cost.,2.2 Removing Unimportant Words,[0],[0]
"The requirement of maintaining model prediction is unchanged.
",2.2 Removing Unimportant Words,[0],[0]
"1We set beam size to max(1,min(k, L − 3))",2.2 Removing Unimportant Words,[0],[0]
"where k is maximum beam size and L is the current length of the input sentence.
",2.2 Removing Unimportant Words,[0],[0]
"With beam search, input reduction finds extremely short reduced examples with little to no decrease in the model’s confidence on its original predictions.",2.2 Removing Unimportant Words,[0],[0]
Figure 3 compares the length of input sentences before and after the reduction.,2.2 Removing Unimportant Words,[0],[0]
"For all three tasks, we can often reduce the sentence to only one word.",2.2 Removing Unimportant Words,[0],[0]
Figure 4 compares the model’s confidence on original and reduced inputs.,2.2 Removing Unimportant Words,[0],[0]
"On SQUAD and SNLI the confidence decreases slightly, and on VQA the confidence even increases.",2.2 Removing Unimportant Words,[0],[0]
"On the reduced examples, the models retain their original predictions despite short input lengths.",2.3 Humans Confused by Reduced Inputs,[0],[0]
"The following experiments examine whether these predictions are justified or pathological, based on how humans react to the reduced inputs.
",2.3 Humans Confused by Reduced Inputs,[0],[0]
"For each task, we sample 200 examples that are correctly classified by the model and generate their reduced examples.",2.3 Humans Confused by Reduced Inputs,[0],[0]
"In the first setting, we compare the human accuracy on original and reduced examples.",2.3 Humans Confused by Reduced Inputs,[0],[0]
"We recruit two groups of crowd workers and task them with textual entailment, reading comprehension, or visual question answering.",2.3 Humans Confused by Reduced Inputs,[0],[0]
We show one group the original inputs and the other the reduced.,2.3 Humans Confused by Reduced Inputs,[0],[0]
"Humans are no longer able to give
the correct answer, showing a significant accuracy loss on all three tasks (compare Original and Reduced in Table 1).
",2.3 Humans Confused by Reduced Inputs,[0],[0]
The second setting examines how random the reduced examples appear to humans.,2.3 Humans Confused by Reduced Inputs,[0],[0]
"For each of the original examples, we generate a version where words are randomly removed until the length matches the one generated by input reduction.",2.3 Humans Confused by Reduced Inputs,[0],[0]
We present the original example along with the two reduced examples and ask crowd workers their preference between the two reduced ones.,2.3 Humans Confused by Reduced Inputs,[0],[0]
"The workers’ choice is almost fifty-fifty (the vs. Random in Table 1): the reduced examples appear almost random to humans.
",2.3 Humans Confused by Reduced Inputs,[0],[0]
These results leave us with two puzzles: why are the models highly confident on the nonsensical reduced examples?,2.3 Humans Confused by Reduced Inputs,[0],[0]
"And why, when the leave-oneout method selects important words that appear reasonable to humans, the input reduction process selects ones that are nonsensical?",2.3 Humans Confused by Reduced Inputs,[0],[0]
"Having established the incongruity of our definition of importance vis-à-vis human judgements, we now investigate possible explanations for these results.",3 Making Sense of Reduced Inputs,[0],[0]
We explain why model confidence can empower methods such as leave-one-out to generate reasonable interpretations but also lead to pathologies under input reduction.,3 Making Sense of Reduced Inputs,[0],[0]
We attribute these results to two issues of neural models.,3 Making Sense of Reduced Inputs,[0],[0]
"Neural models are overconfident in their predictions (Guo et al., 2017).",3.1 Model Overconfidence,[0],[0]
One explanation for overconfidence is overfitting: the model overfits the negative log-likelihood loss during training by learning to output low-entropy distributions over classes.,3.1 Model Overconfidence,[0],[0]
Neural models are also overconfident on examples outside the training data distribution.,3.1 Model Overconfidence,[0],[0]
"As Goodfellow et al. (2015) observe for image classification, samples from pure noise can sometimes trigger highly confident predictions.",3.1 Model Overconfidence,[0],[0]
"These socalled rubbish examples are degenerate inputs that
a human would trivially classify as not belonging to any class but for which the model predicts with high confidence.",3.1 Model Overconfidence,[0],[0]
Goodfellow et al. (2015) argue that the rubbish examples exist for the same reason that adversarial examples do: the surprising linear nature of neural models.,3.1 Model Overconfidence,[0],[0]
"In short, the confidence of a neural model is not a robust estimate of its prediction uncertainty.
",3.1 Model Overconfidence,[0],[0]
"Our reduced inputs satisfy the definition of rubbish examples: humans have a hard time making predictions based on the reduced inputs (Table 1), but models make predictions with high confidence (Figure 4).",3.1 Model Overconfidence,[0],[0]
"Starting from a valid example, input reduction transforms it into a rubbish example.
",3.1 Model Overconfidence,[0],[0]
"The nonsensical, almost random results are best explained by looking at a complete reduction path (Figure 5).",3.1 Model Overconfidence,[0],[0]
"In this example, the transition from valid to rubbish happens immediately after the first step: following the removal of “Broncos”, humans can no longer determine which team the question is asking about, but model confidence remains high.",3.1 Model Overconfidence,[0],[0]
"Not being able to lower its confidence on rubbish examples—as it is not trained to do so— the model neglects “Broncos” and eventually the process generates nonsensical results.
",3.1 Model Overconfidence,[0],[0]
"In this example, the leave-one-out method will not highlight “Broncos”.",3.1 Model Overconfidence,[0],[0]
"However, this is not a failure of the interpretation method but of the model itself.",3.1 Model Overconfidence,[0],[0]
"The model assigns a low importance to “Broncos” in the first step, causing it to be removed—leave-one-out would be able to expose this particular issue by not highlighting “Broncos”.",3.1 Model Overconfidence,[0],[0]
"However, in cases where a similar issue only appear after a few unimportant words are removed, the leave-one-out method would fail to expose the unreasonable model behavior.
",3.1 Model Overconfidence,[0],[0]
Input reduction can expose deeper issues of model overconfidence and stress test a model’s uncertainty estimation and interpretability.,3.1 Model Overconfidence,[0],[0]
"So far, we have seen that the output of a neural model is sensitive to small changes in its input.",3.2 Second-order Sensitivity,[0],[0]
"We call this first-order sensitivity, because interpretation based on input gradient is a first-order Taylor expansion of the model near the input (Simonyan et al., 2014).",3.2 Second-order Sensitivity,[0],[0]
"However, the interpretation also shifts drastically with small input changes (Figure 6).",3.2 Second-order Sensitivity,[0],[0]
"We call this second-order sensitivity.
",3.2 Second-order Sensitivity,[0],[0]
"The shifting heatmap suggests a mismatch between the model’s first- and second-order sensi-
tivities.",3.2 Second-order Sensitivity,[0],[0]
"The heatmap shifts when, with respect to the removed word, the model has low first-order sensitivity but high second-order sensitivity.
",3.2 Second-order Sensitivity,[0],[0]
Similar issues complicate comparable interpretation methods for image classification models.,3.2 Second-order Sensitivity,[0],[0]
"For example, Ghorbani et al. (2017) modify image inputs so the highlighted features in the interpretation change while maintaining the same prediction.",3.2 Second-order Sensitivity,[0],[0]
"To achieve this, they iteratively modify the input to maximize changes in the distribution of feature importance.",3.2 Second-order Sensitivity,[0],[0]
"In contrast, the shifting heatmap we observe occurs by only removing the least impactful features without a targeted optimization.",3.2 Second-order Sensitivity,[0],[0]
They also speculate that the steepest gradient direction for the first- and secondorder sensitivity values are generally orthogonal.,3.2 Second-order Sensitivity,[0],[0]
"Loosely speaking, the shifting heatmap suggests that the direction of the smallest gradient value can sometimes align with very steep changes in second-order sensitivity.
",3.2 Second-order Sensitivity,[0],[0]
"When explaining individual model predictions, the heatmap suggests that the prediction is made based on a weighted combination of words, as in a linear model, which is not true unless the model is indeed taking a weighted sum such as in a DAN (Iyyer et al., 2015).",3.2 Second-order Sensitivity,[0],[0]
"When the model composes representations by a non-linear combination of words, a linear interpretation oblivious to second-order sensitivity can be misleading.",3.2 Second-order Sensitivity,[0],[0]
The previous section explains the observed pathologies from the perspective of overconfidence: models are too certain on rubbish examples when they should not make any prediction.,4 Mitigating Model Pathologies,[0],[0]
Human experiments in Section 2.3 confirm that the reduced examples fit the definition of rubbish examples.,4 Mitigating Model Pathologies,[0],[0]
"Hence, a natural way to mitigate the pathologies is to maximize model uncertainty on the reduced examples.",4 Mitigating Model Pathologies,[0],[0]
"To maximize model uncertainty on reduced examples, we use the entropy of the output distribution as an objective.",4.1 Regularization on Reduced Inputs,[0],[0]
"Given a model f trained on a dataset (X ,Y), we generate reduced examples using input reduction for all training examples X .",4.1 Regularization on Reduced Inputs,[0],[0]
"Beam search often yields multiple reduced versions with the same minimum length for each input x, and we collect all of these versions together to form X̃ as the “negative” example set.
",4.1 Regularization on Reduced Inputs,[0],[0]
Let H (·) denote the entropy and f(y |x) denote the probability of the model predicting y given x.,4.1 Regularization on Reduced Inputs,[0],[0]
"We fine-tune the existing model to simultaneously maximize the log-likelihood on regular examples and the entropy on reduced examples:∑ (x,y)∈(X ,Y) log(f(y |x))",4.1 Regularization on Reduced Inputs,[0],[0]
+ λ,4.1 Regularization on Reduced Inputs,[0],[0]
"∑ x̃∈X̃ H (f(y | x̃)) , (2) where hyperparameter λ controls the trade-off between the two terms.",4.1 Regularization on Reduced Inputs,[0],[0]
"Similar entropy regularization is used by Pereyra et al. (2017), but not in
combination with input reduction; their entropy term is calculated on regular examples rather than reduced examples.",4.1 Regularization on Reduced Inputs,[0],[0]
"On regular examples, entropy regularization does no harm to model accuracy, with a slight increase for SQUAD (Accuracy in Table 2).
",4.2 Regularization Mitigates Pathologies,[0],[0]
"After entropy regularization, input reduction produces more reasonable reduced inputs (Figure 7).",4.2 Regularization Mitigates Pathologies,[0],[0]
"In the SQUAD example from Figure 1, the reduced question changed from “did” to “spend Astor money on ?”",4.2 Regularization Mitigates Pathologies,[0],[0]
after fine-tuning.,4.2 Regularization Mitigates Pathologies,[0],[0]
The average length of reduced examples also increases across all tasks (Reduced length in Table 2).,4.2 Regularization Mitigates Pathologies,[0],[0]
"To verify that model overconfidence is indeed mitigated— that the reduced examples are less “rubbish” compared to before fine-tuning—we repeat the human experiments from Section 2.3.
",4.2 Regularization Mitigates Pathologies,[0],[0]
Human accuracy increases across all three tasks (Table 3).,4.2 Regularization Mitigates Pathologies,[0],[0]
"We also repeat the vs. Random experiment: we re-generate the random examples to match the lengths of the new reduced examples from input reduction, and find humans now prefer the reduced examples to random ones.",4.2 Regularization Mitigates Pathologies,[0],[0]
"The increase in both human performance and preference suggests that the reduced examples are more reasonable; model pathologies have been mitigated.
",4.2 Regularization Mitigates Pathologies,[0],[0]
"While these results are promising, it is not clear whether our input reduction method is necessary to achieve them.",4.2 Regularization Mitigates Pathologies,[0],[0]
"To provide a baseline, we finetune models using inputs randomly reduced to the same lengths as the ones generated by input reduction.",4.2 Regularization Mitigates Pathologies,[0],[0]
This baseline improves neither the model accuracy on regular examples nor interpretability under input reduction (judged by lengths of reduced examples).,4.2 Regularization Mitigates Pathologies,[0],[0]
Input reduction is effective in generating negative examples to counter model overconfidence.,4.2 Regularization Mitigates Pathologies,[0],[0]
"Rubbish examples have been studied in the image domain (Goodfellow et al., 2015; Nguyen et al., 2015), but to our knowledge not for NLP.",5 Discussion,[0],[0]
Our input reduction process gradually transforms a valid input into a rubbish example.,5 Discussion,[0],[0]
"We can often determine which word’s removal causes the transition to occur—for example, removing “Broncos” in Figure 5.",5 Discussion,[0],[0]
"These rubbish examples are particularly interesting, as they are also adversarial: the difference from a valid example is small, unlike image rubbish examples generated from pure noise which are far outside the training data distribution.
",5 Discussion,[0],[0]
"The robustness of NLP models has been studied extensively (Papernot et al., 2016; Jia and Liang, 2017; Iyyer et al., 2018; Ribeiro et al., 2018), and most studies define adversarial examples similar to the image domain: small perturbations to the input lead to large changes in the output.",5 Discussion,[0],[0]
"HotFlip (Ebrahimi et al., 2017) uses a gradient-based approach, similar to image adversarial examples, to flip the model prediction by perturbing a few characters or words.",5 Discussion,[0],[0]
"Our work and Belinkov and Bisk (2018) both identify cases where noisy
user inputs become adversarial by accident: common misspellings break neural machine translation models; we show that incomplete user input can lead to unreasonably high model confidence.
",5 Discussion,[0],[0]
Other failures of interpretation methods have been explored in the image domain.,5 Discussion,[0],[0]
"The sensitivity issue of gradient-based interpretation methods, similar to our shifting heatmaps, are observed by Ghorbani et al. (2017) and Kindermans et al. (2017).",5 Discussion,[0],[0]
They show that various forms of input perturbation—from adversarial changes to simple constant shifts in the image input—cause significant changes in the interpretation.,5 Discussion,[0],[0]
"Ghorbani et al. (2017) make a similar observation about secondorder sensitivity, that “the fragility of interpretation is orthogonal to fragility of the prediction”.
",5 Discussion,[0],[0]
Previous work studies biases in the annotation process that lead to datasets easier than desired or expected which eventually induce pathological models.,5 Discussion,[0],[0]
We attribute our observed pathologies primarily to the lack of accurate uncertainty estimates in neural models trained with maximum likelihood.,5 Discussion,[0],[0]
"SNLI hypotheses contain artifacts that allow training a model without the premises (Gururangan et al., 2018); we apply input reduction at test time to the hypothesis.",5 Discussion,[0],[0]
"Similarly, VQA images are surprisingly unimportant for training a model; we reduce the question.",5 Discussion,[0],[0]
"The recent SQUAD 2.0 (Rajpurkar et al., 2018) augments the original reading comprehension task with an uncertainty modeling requirement, the goal being to make the task more realistic and challenging.
",5 Discussion,[0],[0]
Section 3.1 explains the pathologies from the overconfidence perspective.,5 Discussion,[0],[0]
"One explanation for overconfidence is overfitting: Guo et al. (2017) show that, late in maximum likelihood training,
the model learns to minimize loss by outputting low-entropy distributions without improving validation accuracy.",5 Discussion,[0],[0]
"To examine if overfitting can explain the input reduction results, we run input reduction using DRQA model checkpoints from every training epoch.",5 Discussion,[0],[0]
"Input reduction still achieves similar results on earlier checkpoints, suggesting that better convergence in maximum likelihood training cannot fix the issues by itself—we need new training objectives with uncertainty estimation in mind.",5 Discussion,[0],[0]
We use the reduced examples generated by input reduction to regularize the model and improve its interpretability.,5.1 Methods for Mitigating Pathologies,[0],[0]
"This resembles adversarial training (Goodfellow et al., 2015), where adversarial examples are added to the training set to improve model robustness.",5.1 Methods for Mitigating Pathologies,[0],[0]
"The objectives are different: entropy regularization encourages high uncertainty on rubbish examples, while adversarial training makes the model less sensitive to adversarial perturbations.
",5.1 Methods for Mitigating Pathologies,[0],[0]
Pereyra et al. (2017) apply entropy regularization on regular examples from the start of training to improve model generalization.,5.1 Methods for Mitigating Pathologies,[0],[0]
"A similar method is label smoothing (Szegedy et al., 2016).",5.1 Methods for Mitigating Pathologies,[0],[0]
"In comparison, we fine-tune a model with entropy regularization on the reduced examples for better uncertainty estimates and interpretations.
",5.1 Methods for Mitigating Pathologies,[0],[0]
"To mitigate overconfidence, Guo et al. (2017) propose post-hoc fine-tuning a model’s confidence with Platt scaling.",5.1 Methods for Mitigating Pathologies,[0],[0]
This method adjusts the softmax function’s temperature parameter using a small held-out dataset to align confidence with accuracy.,5.1 Methods for Mitigating Pathologies,[0],[0]
"However, because the output is calibrated using the entire confidence distribution, not individual values, this does not reduce overconfidence on specific inputs, such as the reduced examples.",5.1 Methods for Mitigating Pathologies,[0],[0]
"To highlight the erratic model predictions on short examples and provide a more intuitive demonstration, we present paired-input tasks.",5.2 Generalizability of Findings,[0],[0]
"On these tasks, the short lengths of reduced questions and hypotheses obviously contradict the necessary number of words for a human prediction (further supported by our human studies).",5.2 Generalizability of Findings,[0],[0]
"We also apply input reduction to single-input tasks including sentiment analysis (Maas et al., 2011) and Quizbowl (BoydGraber et al., 2012), achieving similar results.
",5.2 Generalizability of Findings,[0],[0]
"Interestingly, the reduced examples transfer to other architectures.",5.2 Generalizability of Findings,[0],[0]
"In particular, when we feed fifty reduced SNLI inputs from each class—generated with the BIMPM model (Wang et al., 2017)—through the Decomposable Attention Model (Parikh et al., 2016),2 the same prediction is triggered 81.3% of the time.",5.2 Generalizability of Findings,[0],[0]
"We introduce input reduction, a process that iteratively removes unimportant words from an input while maintaining a model’s prediction.",6 Conclusion,[0],[0]
"Combined with gradient-based importance estimates often used for interpretations, we expose pathological behaviors of neural models.",6 Conclusion,[0],[0]
"Without lowering model confidence on its original prediction, an input sentence can be reduced to the point where it appears nonsensical, often consisting of one or two words.",6 Conclusion,[0],[0]
"Human accuracy degrades when shown the reduced examples instead of the original, in contrast to neural models which maintain their original predictions.
",6 Conclusion,[0],[0]
We explain these pathologies with known issues of neural models: overconfidence and sensitivity to small input changes.,6 Conclusion,[0],[0]
The nonsensical reduced examples are caused by inaccurate uncertainty estimates—the model is not able to lower its confidence on inputs that do not belong to any label.,6 Conclusion,[0],[0]
"The second-order sensitivity is another issue why gradient-based interpretation methods may fail to align with human perception: a small change in the input can cause, at the same time, a minor change in the prediction but a large change in the interpretation.",6 Conclusion,[0],[0]
Input reduction perturbs the input multiple times and can expose deeper issues of model overconfidence and oversensitivity that other methods cannot.,6 Conclusion,[0],[0]
"Therefore, it can be used to stress test the interpretability of a model.
",6 Conclusion,[0],[0]
"Finally, we fine-tune the models by maximizing entropy on reduced examples to mitigate the deficiencies.",6 Conclusion,[0],[0]
"This improves interpretability without sacrificing model accuracy on regular examples.
",6 Conclusion,[0],[0]
"To properly interpret neural models, it is important to understand their fundamental characteristics: the nature of their decision surfaces, robustness against adversaries, and limitations of their training objectives.",6 Conclusion,[0],[0]
We explain fundamental difficulties of interpretation due to pathologies in neural models trained with maximum likelihood.,6 Conclusion,[0],[0]
"Our
2http://demo.allennlp.org/ textual-entailment
work suggests several future directions to improve interpretability: more thorough evaluation of interpretation methods, better uncertainty and confidence estimates, and interpretation beyond bagof-word heatmap.",6 Conclusion,[0],[0]
Feng was supported under subcontract to Raytheon BBN Technologies by DARPA award HR0011-15-C-0113.,Acknowledgments,[0],[0]
JBG is supported by NSF Grant IIS1652666.,Acknowledgments,[0],[0]
"Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsor.",Acknowledgments,[0],[0]
"The authors would like to thank Hal Daumé III, Alexander M. Rush, Nicolas Papernot, members of the CLIP lab at the University of Maryland, and the anonymous reviewers for their feedback.",Acknowledgments,[0],[0]
"One way to interpret neural model predictions is to highlight the most important input features—for example, a heatmap visualization over the words in an input sentence.",abstractText,[0],[0]
"In existing interpretation methods for NLP, a word’s importance is determined by either input perturbation—measuring the decrease in model confidence when that word is removed—or by the gradient with respect to that word.",abstractText,[0],[0]
"To understand the limitations of these methods, we use input reduction, which iteratively removes the least important word from the input.",abstractText,[0],[0]
This exposes pathological behaviors of neural models: the remaining words appear nonsensical to humans and are not the ones determined as important by interpretation methods.,abstractText,[0],[0]
"As we confirm with human experiments, the reduced examples lack information to support the prediction of any label, but models still make the same predictions with high confidence.",abstractText,[0],[0]
"To explain these counterintuitive results, we draw connections to adversarial examples and confidence calibration: pathological behaviors reveal difficulties in interpreting neural models trained with maximum likelihood.",abstractText,[0],[0]
"To mitigate their deficiencies, we fine-tune the models by encouraging high entropy outputs on reduced examples.",abstractText,[0],[0]
Fine-tuned models become more interpretable under input reduction without accuracy loss on regular examples.,abstractText,[0],[0]
Pathologies of Neural Models Make Interpretations Difficult,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1805–1816, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Recent progress in NLP has given rise to the field of personality profiling - automated classification of personality traits based on written, verbal and multimodal behavior of an individual.",1 Introduction,[0],[0]
"This research builds upon findings from classical personality psychology and has applications in a wide range of areas from medicine (suicide prevention) across security (forensics, paedophile detection, cyberbullying) to marketing and sales (recommendation systems, target group profiles).",1 Introduction,[0],[0]
"The gold standard labels for an objective evaluation of personality are mostly obtained by means of personality tests of the Five Factor Model (FFM) (McCrae and Costa, 1987; Goldberg, 1990), which is wellknown and widely accepted in psychology and other research fields.",1 Introduction,[0],[0]
"The FFM defines personality
along five bipolar scales: Extraversion (sociable vs. reserved), Emotional stability (secure vs. neurotic), Agreeableness (friendly vs. unsympathic), Conscientiousness (organized vs. careless) and Openness to experience (insightful vs. unimaginative).",1 Introduction,[0],[0]
"Psychologists have shown that these five personality traits are stable across individual lifespan, demographical and cultural differences (John and Srivastava, 1999) and affect many life aspects.",1 Introduction,[0],[0]
"(Terracciano et al., 2008; Rentfrow et al., 2011).
",1 Introduction,[0],[0]
"It has been shown that the personality traits of readers impact their literature preferences (Tirre and Dixit, 1995; Mar et al., 2009).",1 Introduction,[0],[0]
"Psychology researchers also found that perceived similarity is predictive of interpersonal attraction (Montoya et al., 2008; Byrne, 1961; Chartrand and Bargh, 1999).",1 Introduction,[0],[0]
"More explicitly, recent research (Kaufman and Libby, 2012) shows that readers of a narrative develop more favorable attitudes and less stereotype application towards a character, if his difference (e.g. racial) is revealed only later in the story.",1 Introduction,[0],[0]
We therefore hypothesize that readers might have a preference for reading novels depicting fictional characters that are similar to themselves.,1 Introduction,[0],[0]
Finding a direct link between reader’s and protagonist’s personality traits would advance the development of content-based recommendation systems.,1 Introduction,[0],[0]
"As a first step to explore this hypothesis further, it needs to be determined if we are able to construct a personality profile of a fictional character in a similar way as it is done for humans, and which aspects of personality profiling can be exploited to automatize such procedure.
",1 Introduction,[0],[0]
"In this paper, we open this research topic by presenting a novel collaboratively built dataset of fictional character personality in Section 3, which we make available on our website.1 Framing the personality prediction as a text classification task, we incorporate features of both lexical-
1https://www.ukp.tu-darmstadt.de/data/ personality-profiling/
1805
resource-based and vector space semantics, including WordNet and VerbNet sense-level information and vectorial word representations.",1 Introduction,[0],[0]
"We evaluate three machine learning models based on the speech (Section 4), actions (Section 5) and predicatives (Section 6) of the protagonists, and show that especially on the direct speech and action data the lexical-semantic features significantly outperform the baselines.",1 Introduction,[0],[0]
Qualitative analysis reveals that the most predictive features correspond to reported findings in psychology and NLP.,1 Introduction,[0],[0]
"Research in the the area of content-based recommendation systems have shown that incorporating semantic information is valuable for the user and leads to measurable improvements (Passant, 2010; Di Noia et al., 2012; Heitmann and Hayes, 2010).",2 Related work,[0],[0]
De Clercq et al. (2014) incorporated semantic frames from FrameNet into the recommendation system for books.,2 Related work,[0],[0]
They represent the plot of each book with a sequence of ca.,2 Related work,[0],[0]
200 semantic frames,2 Related work,[0],[0]
and has shown that the frame information (such as Killing - Revenge - Death) outperforms the bag-of-words approach.,2 Related work,[0],[0]
Recent NLP experiments begin to reveal the importance of entitycentric models in a variety of tasks.,2 Related work,[0],[0]
"Chambers (2013) show improvement in event schema induction by learning entity-centric rules (e.g., a victim is likely to be a person).",2 Related work,[0],[0]
"Bamman et al. (2014) and Smith et al. (2013) present latent variable models for unsupervised learning of latent character types in movie plot summaries and in English novels, taking authorial style into account.",2 Related work,[0],[0]
"However, even the state-of-the-art NLP work rather describes personas of fictional characters by their role in the story - e.g., action hero, valley girl, best friend, villain etc. - or by their relations to other characters, such as mother or daughter (Elson et al., 2010; Kokkinakis and Malm, 2011), rather than by their inner preferences and motivations.",2 Related work,[0],[0]
It is important to note here that determining a personality of a character is a very different task from determining its role in the story.,2 Related work,[0],[0]
"Psychological understanding of personality, in contrast to role attribution requires a certain detached objectivity - even outright villains may have traits considered desirable in real life.",2 Related work,[0],[0]
"For example, the devil has in many tales a very high aspiration level, appearing highly conscientious and agreeable.",2 Related work,[0],[0]
"We hypothesize that these deeper personality aspects are
those which drive reader’s affiliation to the character, thus deserve to be examined closer.
",2 Related work,[0],[0]
"Also literary scholars formulate ad hoc personality descriptions for their experiments, for example to test hypotheses from evolutionary psychology (Johnson et al., 2011) or examine fictional portrayals of physicists (Dotson, 2009).",2 Related work,[0],[0]
"These descriptions are usually adjusted to the experiment focus (e.g. emotions, relationships, ambitions).",2 Related work,[0],[0]
"As McCrae et al. () point out, a standard set of personality traits, that encompass the full range of characteristics found in all characters in literature (p.77), is needed for a better comparison.
",2 Related work,[0],[0]
Hence we base our present study primarily on the previous NLP research on personality prediction of human individuals.,2 Related work,[0],[0]
"Correlations between lexical and stylistic aspects of text and the five FFM personality traits of the author have been found in numerous experiments, with extraversion receiving the most attention (Pennebaker and King, 1999; Dewaele and Furnham, 1999; Gill and Oberlander, 2002; Mehl et al., 2006; Aran and Gatica-Perez, 2013; Lepri et al., 2010).",2 Related work,[0],[0]
"The LIWC lexicon (Pennebaker et al., 2001) established its position as a powerful mean of such analysis.
",2 Related work,[0],[0]
"The first machine learning experiments in this area were conducted by Argamon et al. (2005), Oberlander and Nowson (2006) and Mairesse et al. (2007).",2 Related work,[0],[0]
"Researchers predicted the five personality traits of the authors of stream-ofconscientiousness essays, blog posts and recorded conversation snippets.",2 Related work,[0],[0]
"Given balanced data sets, Mairesse et al. (2007) report binary classification accuracy of 50-56% on extraversion in text and 47-57% in speech, using word ngrams, LIWC, MRC psycholinguistic database (Coltheart, 1981) and prosodic features.",2 Related work,[0],[0]
Additional improvement is reported when the extraversion was labeled by external judges rather than by self-testing.,2 Related work,[0],[0]
"Extended studies on larger datasets achieve accuracies around 55% (Nowson, 2007; Estival et al., 2007).",2 Related work,[0],[0]
"More recent work in this area focuses on the personality prediction in social networks (Kosinski et al., 2013; Kosinski et al., 2014) and multimodal personality prediction (Biel and Gatica-Perez, 2013; Aran and Gatica-Perez, 2013).",2 Related work,[0],[0]
"These trends emphasized the correlation of network features and audiovisual features with extraversion, giving rise to the Workshop on Computational Personality Recognition (for an overview see (Celli et al., 2013; Celli et al., 2014).",2 Related work,[0],[0]
"Traditionally, the gold standard for this supervised classification task is obtained by the means of personality questionnaires, used for the Five-Factor Model, taken by each of the individuals assessed.",3 Data set construction,[0],[0]
This poses a challenge for fictional characters.,3 Data set construction,[0],[0]
"However, strong correlations have been found between the self-reported and perceived personality traits (Mehl et al., 2006).",3 Data set construction,[0],[0]
Our gold standard benefits from the fact that readers enjoy discussing the personality of their favourite book character online.,3 Data set construction,[0],[0]
"A popular layman instrument for personality classification is the Myers-Brigggs Type Indicator (Myers et al., 1985), shortly MBTI, which sorts personal preferences into four opposite pairs, or dichotomies, such as Thinking vs. Feeling or Judging vs. Perceiving.",3 Data set construction,[0],[0]
"While the MBTI validity has been questioned by the research community (Pittenger, 2005), the Extraversion scale is showing rather strong validity and correlation to similar trait in the Five-Factor Model (McCrae and Costa, 1989; MacDonald et al., 1994).",3 Data set construction,[0],[0]
"Our study hence focuses on the Extraversion scale.
",3 Data set construction,[0],[0]
"Our data was collected from the collaboratively constructed Personality Databank2 where the readers can vote if a book character is, among other aspects, introverted or extraverted.",3 Data set construction,[0],[0]
"While the readers used codes based on the MBTI typology, they did not apply the MBTI assessment strategies.",3 Data set construction,[0],[0]
There was no explicit annotation guideline and the interpretation was left to readers’ intuition and knowledge.3,3 Data set construction,[0],[0]
This approach of gold standard collection has several obvious drawbacks.,3 Data set construction,[0],[0]
"First, the question is posed as dichotomic, while in reality the extraversion is a normally distributed trait in human population (Goldberg, 1990).",3 Data set construction,[0],[0]
"Second, users can view the vote of previous participants, which may influence their decision.",3 Data set construction,[0],[0]
"While we address both of these issues in our ongoing data collection project based on the Five-Factor Model, we consider them acceptable for this study due to the exploratory character of our pilot research.
",3 Data set construction,[0],[0]
"We have collected extraversion ratings for 298 book characters, of which 129 (43%) are rather extraverted and 166 (56%) rather introverted.",3 Data set construction,[0],[0]
"Rated
2http://www.mbti-databank.com/ 3MBTI defines extraversion as “getting energy from active involvement in events, having a lot of different activities, enjoying being around people.”",3 Data set construction,[0],[0]
"In the NEO Five-Factor Inventory (Costa and McCrae, 1992), underlying facets of extraversion are warmth, gregariousness, assertiveness, activity, excitement seeking and positive emotion.
",3 Data set construction,[0],[0]
"characters come from a wide range of novels that the online users are familiar with, often covering classical literature which is part of the high school syllabus, as well as the most popular modern fiction, such as the Harry Potter series, Twilight, Star Wars or A Game of Thrones.",3 Data set construction,[0],[0]
A sample of the most rated introverts and extraverts is given in table 1.,3 Data set construction,[0],[0]
The rating distribution in our data is strongly Ushaped.,3 Data set construction,[0],[0]
"The percentage agreement of voters in our data is 84.9%, calculated as:
P = 1 N N∑ i=1",3 Data set construction,[0],[0]
k∑ j=1 nij(nij,3 Data set construction,[0],[0]
"− 1) n(n− 1)
where k = 2 (introvert, extravert), N is the number of book characters and n the number of votes per character.",3 Data set construction,[0],[0]
Voters on the website were anonymous and cannot be uniquely identified for additional corrections.,3 Data set construction,[0],[0]
"There is no correlation between the extraversion and the gender of the character.
",3 Data set construction,[0],[0]
Our set of English e-books covered 220 of the characters from our gold standard.,3 Data set construction,[0],[0]
"We have built three systems to assess the following:
1.",3 Data set construction,[0],[0]
Direct speech: Does the style and content of character’s utterances predict his extraversion in a similar way as it was shown for living individuals?,3 Data set construction,[0],[0]
"2. Actions: Is the behavior, of which a character is an agent, predictive for extraversion?",3 Data set construction,[0],[0]
3.,3 Data set construction,[0],[0]
"Predicatives and adverbs: Are the explicit (John was an exhibitionist) or implicit (John shouted abruptly) descriptions of the character in the book predictive for extraversion?
",3 Data set construction,[0],[0]
In the next three sections we present the experimental settings and results for each of the systems.,3 Data set construction,[0],[0]
"The system for the direct speech resembles the most to the previous systems developed for author personality profiling, e.g. on stream of consciousness essays (Mairesse et al., 2007) or social media posts (Celli et al., 2013) and therefore provides the best opportunity for comparison between human individuals and fictional characters.",4 Direct speech of fictional characters,[0],[0]
"On top of the comparison to previous research, we exploit the sense links between WordNet and VerbNet to extract additional features - an approach which is novel for this type of task.",4 Direct speech of fictional characters,[0],[0]
"We process the book text using freely available components of the DKPro framework (Gurevych et al., 2007).",4.1 Extraction and assignment of speech,[0],[0]
The most challenging task in building the direct speech data set is assigning to the direct speech utterance the correct speaker.,4.1 Extraction and assignment of speech,[0],[0]
"We benefit from the epub format of the e-books which defines a paragraph structure in such a way, that only the indirect speech chunk immediately surrounding the direct speech can be considered:
<p> John turned to Harry.",4.1 Extraction and assignment of speech,[0],[0]
"""Let’s go,"" he said.</p>
Given the large amount of text available in the books we focus on precision and discard all utterances with no explicit speaker (i.e., 30-70% of the utterances, dependent on the book), as the performance of current systems on such utterance types is still fairly low (O’Keefe et al., 2012; He et al., 2013; Iosif and Mishra, 2014).",4.1 Extraction and assignment of speech,[0],[0]
"Similarly, conventional coreference resolution systems did not perform well on this type of data and were therefore not used in the final setup.",4.1 Extraction and assignment of speech,[0],[0]
"We adapt the Stanford Named Entity Recognizer(Finkel et al., 2005) to consider titles (Mr., Mrs., Sir...) as a part of the name and to treat the first person I as a named entity.",4.1 Extraction and assignment of speech,[0],[0]
"However, identifying only the named entity PERSON in this way is not sufficient.",4.1 Extraction and assignment of speech,[0],[0]
"On our evaluation sample consisting of A Game of Thrones and Pride and Prejudice books (the former annotated by us, the latter by He et al. (2013)), 20% of utterances with explicit named speaker were not recognized.",4.1 Extraction and assignment of speech,[0],[0]
"Of those correctly identified as a Person in the adjacent indirect speech, 17% were not the speakers.",4.1 Extraction and assignment of speech,[0],[0]
"Therefore we implemented a
custom heuristics (Algorithm 1), which additionally benefits from the WordNet semantic classes of verbs, enriching the speaker detection by grabbing the nouns .",4.1 Extraction and assignment of speech,[0],[0]
"With this method we retrieve 89% of known speakers, of which 92% is assigned correctly.",4.1 Extraction and assignment of speech,[0],[0]
"Retrieved names are grouped based on string overlap (e.g. Ser Jaime and Jaime Lannister), excluding the match on last name, and corrected for non-obvious groupings (such as Margaret and Peggy).",4.1 Extraction and assignment of speech,[0],[0]
"Algorithm 1 Assign speaker 1: nsubj← subjects in adjacent indirect speech 2: if count(nsubj(i) = PERSON) = 1 then speaker ←
nsubj 3: else if count(nsubj(i) = PERSON) ≥ 1 then
speaker ← the nearest one to directSpeech 4: else if directSpeech preceded by
VERB.COMMUNICATION then speaker ← the preceding noun(s) 5: else if directSpeech followed by VERB.COMMUNICATION then speaker ← the following noun(s) 6: else if directSpeech followed by gap & VERB.COMMUNICATION then speaker ← the noun(s) in gap 7: else if directSpeech preceded by gap & VERB.COMMUNICATION then speaker ← the noun(s) in gap return speaker
Our experimental data consists of usable direct speech sets of 175 characters - 80 extraverts (E) and 95 introverts (I) - containing 289 274 words in 21 857 utterances (on average 111 utterances for E and 136 for I, as I are often central in books).4",4.1 Extraction and assignment of speech,[0],[0]
All speech utterances of one book character are represented as one instance in our system.,4.2 Classification approach for direct speech,[0],[0]
"We use the leave-one-out classification setup due to the relatively small dataset size, using the support vector machines (SVM-SMO) classifier, which performs well on comparable tasks (Celli et al., 2013).",4.2 Classification approach for direct speech,[0],[0]
"The classification is performed through the DKPro TC Framework (Daxenberger et al., 2014).
",4.2 Classification approach for direct speech,[0],[0]
"Lexical features As a bottom-up approach we use the 1000 most frequent word uni-, bi- and trigrams, 1000 dependency word pairs, 1000 character trigrams and 500 most frequent verbs, adverbs, adjectives and interjections as binary features.
",4.2 Classification approach for direct speech,[0],[0]
"Semantic features Since the top-down approach, i.e. not focusing on individual words, has
4The data set size is comparable to ongoing personality profiling challenges - see http://pan.webis.de
been found more suitable for the personality profiling task on smaller data sets (Celli et al., 2013), we aim on capturing additional phenomena on a higher level of abstraction.",4.2 Classification approach for direct speech,[0],[0]
The main part of our features is extracted on sense level.,4.2 Classification approach for direct speech,[0],[0]
"We use the most frequent sense of WordNet (Miller, 1995) to annotate all verbs in the direct speech (a simple but well performing approach for books).",4.2 Classification approach for direct speech,[0],[0]
"We then label the disambiguated verbs with their semantic field given in WordNet (WordNet defines 14 semantic classes of verbs which group verbs by their semantic field) and we measure frequency and occurence of each of these classes (e.g. cognition, communication, motion, perception)5.",4.2 Classification approach for direct speech,[0],[0]
"Additionally, we use the lexical-semantic resource UBY (Gurevych et al., 2012) to access the WordNet and VerbNet information, and to exploit the VerbNet sense-level links which connects WordNet senses with the corresponding 273 main VerbNet classes (Kipper-Schuler, 2005).",4.2 Classification approach for direct speech,[0],[0]
"These are more fine-grained (e.g. pay, conspire, neglect, discover) than the WordNet semantic fields.",4.2 Classification approach for direct speech,[0],[0]
"WordNet covered 90% and VerbNet 86% of all the verb occurences.
",4.2 Classification approach for direct speech,[0],[0]
"On word level, we extract 81 additional features using the Linguistic Inquiry and Word Count (LIWC) tools (Pennebaker et al., 2001), which consists of lexicons related to psychological processes (cognitive, perceptual, social, biological, affective) and personal concerns (achievement, religion, death...) and other categories such as fillers, disfluencies or swear words6.",4.2 Classification approach for direct speech,[0],[0]
"Additionally, since emotion detection has been found predictive in previous personality work (Mohammad and Kiritchenko, 2013), we measure overall positive and negative sentiment expressed per character, using SentiWordNet (Esuli and Sebastiani, 2006) and NRC Emotion Lexicon (Mohammad and Turney, 2010) for the word lookup, inverting sentiment scores for negated dependency sub-tree given by the Stanford Parser.
Stylistic features Features of this group capture the syntactic and stylistic properties of the utterances of a character, disregarding the content.",4.2 Classification approach for direct speech,[0],[0]
"Starting from the surfacial properties, we measure the sentence, utterance and word length, including the proportion of words shorter than 4 or longer than 6 letters, frequency of each punctuation mark,
5https://wordnet.princeton.edu/man/ lexnames.5WN.html
6For complete overview refer to www.liwc.net
and endings of each adjective as per Corney et al. (2002).",4.2 Classification approach for direct speech,[0],[0]
"On the syntax level we measure the frequency of each part of speech as well as the 500 most frequent part-of-speech bi-, tri- and quadrigrams, and the frequency of each dependency obtained from the Stanford Parser.",4.2 Classification approach for direct speech,[0],[0]
"We additionally capture the frequency of superlatives, comparatives and modal verbs, the proportion of verbs in present, past and future tense, and the formality of the language as per the part-of-speech-based formality coefficient (Heylighen and Dewaele, 2002), and measure the average depth of the parse trees.
",4.2 Classification approach for direct speech,[0],[0]
"Word embeddings as features Since vector space semantics has been beneficial for predicting author’s personality in previous work (Neuman and Cohen, 2014), we use a pre-trained word vector model created by the GloVe algorithm (Pennington et al., 2014) on English Wikipedia.",4.2 Classification approach for direct speech,[0],[0]
GloVe employs a global log-bilinear regression model that combines the advantages of the global matrix factorization and local context window methods.,4.2 Classification approach for direct speech,[0],[0]
"We assign the resulting 300-dimensional vectors to the words in character’s direct speech, excluding stopwords, and calculate an average vector for each character.",4.2 Classification approach for direct speech,[0],[0]
"We calculate for each test character the cosine similarity to the mean vector of extravert, resp.",4.2 Classification approach for direct speech,[0],[0]
"introvert, in the training data, and to each character in the training set individually using the DL4J NLP package7.",4.2 Classification approach for direct speech,[0],[0]
We consider both the final scalar outcome and the difference of each of the individual vector dimensions as features.,4.2 Classification approach for direct speech,[0],[0]
"Table 2 shows the precision, recall, F1-score and accuracy for extraversion and introversion as a weighted average of the two class values.
",4.3 Classification results on direct speech,[0],[0]
"7http://deeplearning4j.org/
Similarly to previous research (Mairesse et al., 2007; Celli et al., 2013), the bottom-up word based approach is outperformed by top-down semantic approaches which employ a more abstract feature representation.",4.3 Classification results on direct speech,[0],[0]
"As in previous work, LIWC features exhibit good performance.",4.3 Classification results on direct speech,[0],[0]
"However, the highest performance is achieved employing the VerbNet verb classes with WordNet wordsense disambiguation.",4.3 Classification results on direct speech,[0],[0]
Also stylistic features contribute substantially to the classification despite the mixture of genres in our book corpus - especially frequencies of modal verbs and part-ofspeech ratios were particularly informative.,4.3 Classification results on direct speech,[0],[0]
"The most predictive features from each group are listed in Table 3 together with their correlation merit (Hall, 1999), and compared with previous work in Table 4.
",4.3 Classification results on direct speech,[0],[0]
"In accordance with the experiments of Pennebaker and King (1999), we observe more frequent exclusions (e.g. without, but), hedging and negation expressed by introverts, and inclusion (e.g. with, and) by extraverts.",4.3 Classification results on direct speech,[0],[0]
"Extraverts talk more in first person plural, use more back-channels and interjections, and talk more about aspects related to their body.",4.3 Classification results on direct speech,[0],[0]
"Introverts show more rationalization through insight words and more factual speech using less pronouns.
",4.3 Classification results on direct speech,[0],[0]
"Additionally, the semantic features in Table 3 confirm the broad psychological characteristics of both types in general, i.e., for introverts the rationalization, uncertainty and preference for individual or rather static activities, and for extraverts their spontaneity, talkativeness and preference for motion.",4.3 Classification results on direct speech,[0],[0]
"Furthermore, we observe certain directness in extraverts’ speech - note the predictive words fat and dirty and frequent descriptions of body functions.
",4.3 Classification results on direct speech,[0],[0]
Discussion Exploiting the links between lexicalsemantic resources (performing WordNet wordsense disambiguation and using VerbNet verb classes linked to the disambiguated senses) was particularly beneficial for this task.,4.3 Classification results on direct speech,[0],[0]
"WordNet semantic fields for verbs alone are too coarsegrained to capture the nuances in direct speech, and experiments with fine-grained VerbNet classes without WSD resulted in noisy labels.",4.3 Classification results on direct speech,[0],[0]
"We did not confirm the previously reported findings on emotional polarity - we observe that the genre of the books (e.g. love romance vs horror story) have blurred the subtle differences between individual characters, unfortunately the dataset size did not allow for genre distinctions.",4.3 Classification results on direct speech,[0],[0]
"Furthermore, a perceived extravert in our case can be a pure villain (Draco Malfoy, Joffrey Baratheon...) as well as a friendly companion (Gimli, Ron Weasley...), while the evil extravert types are possibly rarer in the experiments on human writing, or are more likely to fit under the MBTI definition of extraversion than FFM facets.",4.3 Classification results on direct speech,[0],[0]
"Another potential cause, based on the error analysis, is the different target of the same sentiment for extraverts and introverts.",4.3 Classification results on direct speech,[0],[0]
"For example, the ngram ”I fear” is highly predictive for an introvert in our data while extraverts would rather use formulations to imply that others should fear.",4.3 Classification results on direct speech,[0],[0]
"Similarly to Nowson et al. (2005), we did not find any difference in the formality measure of Heylighen and Dewaele (2002).",4.3 Classification results on direct speech,[0],[0]
"Neither we did in the complexity of sentences as per the parse tree depth
and sentence length.",4.3 Classification results on direct speech,[0],[0]
It is probable that these aspects were also impacted by our broad variety of author style (F. Dostoyevsky vs J. K. Rowling).,4.3 Classification results on direct speech,[0],[0]
"Our basic vector-based features carried no useful information in our case, in contrast to the personality research of Neuman and Cohen (2014).",4.3 Classification results on direct speech,[0],[0]
We observed that the factual content of the stories contributed to the character similarity measure more than the subtle personality differences.,4.3 Classification results on direct speech,[0],[0]
"While psycholinguists and consequenlty NLP researchers analyzed the relation between speech, resp.",5 Actions of fictional characters,[0],[0]
"writing, and personality of an individual, psychologists often evaluate extraversion through behavioral personality questionnaries (Costa and McCrae, 1992; Goldberg et al., 2006).",5 Actions of fictional characters,[0],[0]
We hypothesize that similar behavior shall be predictive for extraversion of fictional characters as perceived by the readers.,5 Actions of fictional characters,[0],[0]
"For our purpose we define actions as the subject, verb and context of a sentence, where the subject is a named entity Person and the context is either a direct object in relation dobj to the verb or a first child of the adjacent verb phrase in a parse tree.",5.1 Action extraction,[0],[0]
"After grouping the actions per character, the subject name is removed.",5.1 Action extraction,[0],[0]
"For example, a sample of actions of the character Eddard Stark of Game of Thrones would be: X paused a moment, X studied his face, X changed his mind, X unrolled the paper, X said etc., visualized in Figure 1.",5.1 Action extraction,[0],[0]
"We obtained 22 030 actions for 205 characters (102 E, 116 I), with on average 100 actions for E and 101 for I. Note that also actions for those characters who do not talk enough in the books (often first-person perspectives) could be used.",5.1 Action extraction,[0],[0]
In the system based on actions we use only a subset of the features described in 4.2.,5.2 Action classification setup,[0],[0]
From the lexical features we focus on the 500 most frequent verbs and dependency word pairs.,5.2 Action classification setup,[0],[0]
"Semantic features are used the same way as in 4.2, profiting from LIWC, WordNet, Verbnet and the sentiment lexicons.",5.2 Action classification setup,[0],[0]
Word embedding vectors for book characters are in this case computed by taking only the verbs into account rather than all content words.,5.2 Action classification setup,[0],[0]
"From the stylistic features we use the part-ofspeech bigrams and trigrams, verb modality and verb tense.",5.2 Action classification setup,[0],[0]
"Table 5 shows the performance of the classification models based on the protagonists’ actions, using different feature groups.",5.3 Classification results on actions,[0],[0]
"The overall performance is higher than for the direct speech model.
",5.3 Classification results on actions,[0],[0]
"Due to the lack of previous NLP experiments on this task, we compare our features to the actions measured in the International Personality Item Pool (Goldberg et al., 2006), frequently used personality assesment questionnaire (Table 6).
",5.3 Classification results on actions,[0],[0]
The most predictive features of this model capture the activity and excitement seeking facets of extraversion.,5.3 Classification results on actions,[0],[0]
"Stylistic features reflect the complexity difference of the verb phrases (John jumped vs. John thought about it), extraverts being characterized by plain verbs.",5.3 Classification results on actions,[0],[0]
Semantic features exhibit higher precision than stylistic ones.,5.3 Classification results on actions,[0],[0]
"Sense-linked semantic classes of VerbNet demonstrate the preference of extraverts for being active and expressing themselves - they jump, fight, shout, run in and run out, eat and drink, see and hear and get easily bored.",5.3 Classification results on actions,[0],[0]
"Extraverts in books also
often bring or hold something.",5.3 Classification results on actions,[0],[0]
"Introverts, on the other hand, seem to favor slow movements - while they are thinking, reflecting, creating, looking for explanations and find out solutions, they tend to lie down, sit or walk, eventually even sleep or snooze.",5.3 Classification results on actions,[0],[0]
"The uncertainty typical for introverts is also notable in their actions, as they often hope or wish for something they might like to do.",5.3 Classification results on actions,[0],[0]
"Additionally, semantic classes Social and Family, reported as correlated to extraversion by Pennebaker and King (1999) and not confirmed in our first model, became predictive in protaonists’ actions.",5.3 Classification results on actions,[0],[0]
"Also in this task, the VerbNet classes brought significant improvement in performance.",5.4 Discussion,[0],[0]
"The classification model based on actions outperforms not only the direct speech model, but also the state-of-the-art systems predicting authors’ extraversion from the stream-of-consciousness essays (Mairesse et al., 2007; Celli et al., 2013; Neuman and Cohen, 2014).",5.4 Discussion,[0],[0]
"While surely not directly comparable, this result hints to the fact that the personality is easier to detect from behavior than from person’s verbal expression.",5.4 Discussion,[0],[0]
"This would correspond to the findings of Mairesse et al. (2007), Biel and Gatica-Perez (2013) and Aran and Gatica-Perez (2013) on multimodal data sets.",5.4 Discussion,[0],[0]
Our third extraversion prediction system is subordinate to how fictional characters are described and to the manners in which they behave.,6 Predicatives of fictional characters,[0],[0]
We are not aware of a previous NLP work predicting extraversion using descriptive adjectives of the persons in question.,6 Predicatives of fictional characters,[0],[0]
We thus juxtapose the most predictive features of our system to the adjectival extraversion markers developed by Goldberg (1992).,6 Predicatives of fictional characters,[0],[0]
In this setup we extract predicatives of the named entities PERSON in the books - relations amod (angry John) and cop (John was smart).,6.1 Extraction of descriptive properties,[0],[0]
"As these explicit statements are very sparse in modern novels, we additionally include adverbial modifiers (advmod) related to person’s actions (John said angrily).",6.1 Extraction of descriptive properties,[0],[0]
"We extract data for 205 characters, with on average 43 words per character.",6.1 Extraction of descriptive properties,[0],[0]
"This system uses similar set of lexical, semantic and vectorial features similarly as in 5.2, this time with the focus on adjectives, nouns and adverbs instead of verbs.",6.2 Classification setup,[0],[0]
"Stylistic and VerbNet features are hence not included, word vectors are as in 4.2.",6.2 Classification setup,[0],[0]
Table 7 reports on the performance of individual feature groups.,6.3 Classification results on descriptions,[0],[0]
"With only few words per character semantic lexicons are less powerful than ngrams.
",6.3 Classification results on descriptions,[0],[0]
Table 8 displays the most predictive features in our system contrasted to the adjectival markers.,6.3 Classification results on descriptions,[0],[0]
All our systems had issues with characters rated by less than five readers and with protagonists with low agreement.,6.4 Discussion on errors,[0],[0]
"Other challenges arise from authorial style, age of the novel and speech individuality of characters (e.g. Yoda).",6.4 Discussion on errors,[0],[0]
"Varied length of information for different characters poses issues in measuring normally distributed features (e.g. ratio of jumping verbs), being in shorter texts less reliable.",6.4 Discussion on errors,[0],[0]
"Ongoing and future work on this task addresses the limitations of these initial experiments, especially the data set size and the gold standard quality.",6.4 Discussion on errors,[0],[0]
Extending the data will also enable us to examine different book genres as variables for the personality distribution and feature impact.,6.4 Discussion on errors,[0],[0]
"It will be worth examining the relations between characters, since we observed certain patterns in our data, such as the main introvert character supported by his best friend extravert.",6.4 Discussion on errors,[0],[0]
"Additionally, we want to verify if the system in Section 6 is overly optimistic due to the data size.",6.4 Discussion on errors,[0],[0]
"Automated personality profiling of fictional characters, based on rigorous models from personality psychology, has a potential to impact numerous domains.",7 Conclusion and future work,[0],[0]
We framed it as a text classification problem and presented a novel collaboratively built dataset of fictional personality.,7 Conclusion and future work,[0],[0]
"We incor-
porate features of both lexical resource-based and vectorial semantics, including WordNet and VerbNet sense-level information and vectorial word representations.",7 Conclusion and future work,[0],[0]
"In models based on the speech and actions of the protagonists, we demonstrated that the sense-linked lexical-semantic features significantly outperform the baselines.",7 Conclusion and future work,[0],[0]
The most predictive features correspond to the reported findings in personality psychology and NLP experiments on human personality.,7 Conclusion and future work,[0],[0]
"Our systems based on actions and appearance of characters demonstrate higher performance than systems based on direct speech, which is in accordance with recent research on personality in social networks (Kosinski et al., 2014; Biel and Gatica-Perez, 2013), revealing the importance of the metadata.",7 Conclusion and future work,[0],[0]
"We have shown that exploiting the links between lexical resources to leverage more accurate semantic information can be beneficial for this type of tasks, oriented to actions performed by the entity.",7 Conclusion and future work,[0],[0]
"However, the human annotator agreement in our task stays high above the performance achieved.",7 Conclusion and future work,[0],[0]
"Considering that most of the sucessful novels were produced as movies, we cannot exclude that our annotators based their decision on the multimodal representation of the protagonists.",7 Conclusion and future work,[0],[0]
"In the future we aim on collecting a more detail and rigorous gold standard through gamification and expanding our work on all five personality traits from the FiveFactor Model and their facets, and ultimately extend our system to a semi-supervised model dealing with notably larger amount of data.",7 Conclusion and future work,[0],[0]
"We also plan to examine closer the differences between perceived human and fictional personality, and the relationship between the personality of the reader and the characters.",7 Conclusion and future work,[0],[0]
This work has been supported by the Volkswagen Foundation as part of the Lichtenberg Professorship Program under grant No. I/82806 and by the German Research Foundation under grant No. GU 798/14-1.,Acknowledgments,[0],[0]
Additional support was provided by the German Federal Ministry of Education and Research (BMBF) as a part of the Software Campus program under the promotional reference 01-S12054 and by the German Institute for Educational Research (DIPF).,Acknowledgments,[0],[0]
"We also warmly thank Holtzbrinck Digital GmbH for providing a substantial part of the e-book resources, and the EMNLP reviewers for their helpful comments.",Acknowledgments,[0],[0]
This study focuses on personality prediction of protagonists in novels based on the Five-Factor Model of personality.,abstractText,[0],[0]
We present and publish a novel collaboratively built dataset of fictional character personality and design our task as a text classification problem.,abstractText,[0],[0]
"We incorporate a range of semantic features, including WordNet and VerbNet sense-level information and word vector representations.",abstractText,[0],[0]
"We evaluate three machine learning models based on the speech, actions and predicatives of the main characters, and show that especially the lexical-semantic features significantly outperform the baselines.",abstractText,[0],[0]
The most predictive features correspond to reported findings in personality psychology.,abstractText,[0],[0]
Personality Profiling of Fictional Characters using Sense-Level Links between Lexical Resources,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 700–705 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
700",text,[0],[0]
Query auto-completion (QAC) is a feature used by search engines that provides a list of suggested queries for the user as they are typing.,1 Introduction,[0],[0]
"For instance, if the user types the prefix “mete” then the system might suggest “meters” or “meteorite” as completions.",1 Introduction,[0],[0]
"This feature can save the user time and reduce cognitive load (Cai et al., 2016).
",1 Introduction,[0],[0]
"Most approaches to QAC are extensions of the Most Popular Completion (MPC) algorithm (BarYossef and Kraus, 2011).",1 Introduction,[0],[0]
MPC suggests completions based on the most popular queries in the training data that match the specified prefix.,1 Introduction,[0],[0]
"One way to improve MPC is to consider additional signals such as temporal information (Shokouhi and Radinsky, 2012; Whiting and Jose, 2014) or information gleaned from a users’ past queries (Shokouhi, 2013).",1 Introduction,[0],[0]
"This paper deals with the latter of those two signals, i.e. personalization.",1 Introduction,[0],[0]
"Personalization relies on the fact that query likelihoods are drastically different among different people depending on their needs and interests.
",1 Introduction,[0],[0]
"Recently, Park and Chiba (2017) suggested a significantly different approach to QAC.",1 Introduction,[0],[0]
"In their
work, completions are generated from a character LSTM language model instead of by ranking completions retrieved from a database, as in the MPC algorithm.",1 Introduction,[0],[0]
"This approach is able to complete queries whose prefixes were not seen during training and has significant memory savings over having to store a large query database.
",1 Introduction,[0],[0]
"Building on this work, we consider the task of personalized QAC, advancing current methods by combining the obvious advantages of personalization with the effectiveness of a language model in handling rare and previously unseen prefixes.",1 Introduction,[0],[0]
The model must learn how to extract information from a user’s past queries and use it to adapt the generative model for that person’s future queries.,1 Introduction,[0],[0]
"To do this, we leverage recent advances in contextadaptive neural language modeling.",1 Introduction,[0],[0]
"In particular, we make use of the recently introduced FactorCell model that uses an embedding vector to additively transform the weights of the language model’s recurrent layer with a low-rank matrix (Jaech and Ostendorf, 2017).",1 Introduction,[0],[0]
"By allowing a greater fraction of the weights to change during personalization, the FactorCell model has advantages over the traditional approach to adaptation of concatenating a context vector to the input of the LSTM (Mikolov and Zweig, 2012).
",1 Introduction,[0],[0]
"Table 1 provides an anecdotal example from
the trained FactorCell model to demonstrate the intended behavior.",1 Introduction,[0],[0]
The table shows the top five completions for the prefix “ba” in a cold start scenario and again after the user has completed five sports related queries.,1 Introduction,[0],[0]
"In the warm start scenario, the “baby names” and “babiesrus” completions no longer appear in the top five and have been replaced with “basketball” and “baseball”.
",1 Introduction,[0],[0]
The novel aspects of this work are the application of an adaptive language model to the task of QAC personalization and the demonstration of how RNN language models can be adapted to contexts (users) not seen during training.,1 Introduction,[0],[0]
An additional contribution is showing that a richer adaptation framework gives added gains with added data.,1 Introduction,[0],[0]
"Adaptation depends on learning an embedding for each user, which we discuss in Section 2.1, and then using that embedding to adjust the weights of the recurrent layer, discussed in Section 2.2.",2 Model,[0],[0]
"During training, we learn an embedding for each of the users.",2.1 Learning User Embeddings,[0],[0]
We think of these embeddings as holding latent demographic factors for each user.,2.1 Learning User Embeddings,[0],[0]
"Users who have less than 15 queries in the training data (around half the users but less than 13% of the queries) are grouped together as a single entity, user1, leaving k users.",2.1 Learning User Embeddings,[0],[0]
"The user embeddings matrix Uk×m, wherem is the user embedding size, is learned via back-propagation as part of the end-toend model.",2.1 Learning User Embeddings,[0],[0]
"The embedding for an individual user is the ith row of U and is denoted by ui.
",2.1 Learning User Embeddings,[0],[0]
It is important to be able to apply the model to users that are not seen during training.,2.1 Learning User Embeddings,[0],[0]
This is done by online updating of the user embeddings during evaluation.,2.1 Learning User Embeddings,[0],[0]
"When a new person, userk+1 is seen, a new row is added to U and initialized to u1.",2.1 Learning User Embeddings,[0],[0]
Each person’s user embedding is updated via back-propagation every time they select a query.,2.1 Learning User Embeddings,[0],[0]
"When doing online updating of the user embeddings, the rest of the model parameters (everything except U) are frozen.",2.1 Learning User Embeddings,[0],[0]
We consider three model architectures which differ only in the method for adapting the recurrent layer.,2.2 Recurrent Layer Adaptation,[0],[0]
"First is the unadapted LM, analogous to the model from Park and Chiba (2017), which does no personalization.",2.2 Recurrent Layer Adaptation,[0],[0]
"The second architecture was
introduced by Mikolov and Zweig (2012) and has been used multiple times for LM personalization (Wen et al., 2013; Huang et al., 2014; Li et al., 2016).",2.2 Recurrent Layer Adaptation,[0],[0]
It works by concatenating a user embedding to the character embedding at every step of the input to the recurrent layer.,2.2 Recurrent Layer Adaptation,[0],[0]
Jaech and Ostendorf (2017) refer to this model as the ConcatCell and show that it is equivalent to adding a term Vu to adjust the bias of the recurrent layer.,2.2 Recurrent Layer Adaptation,[0],[0]
"The hidden state of a ConcatCell with embedding size e and hidden state size h is given in Equation 1 where σ is the activation function, wt is the character embedding, ht−1 is the previous hidden state, and W ∈ Re+h×h and b ∈",2.2 Recurrent Layer Adaptation,[0],[0]
"Rh are the recurrent layer weight matrix and bias vector.
",2.2 Recurrent Layer Adaptation,[0],[0]
"ht = σ([wt, ht−1]W + b+Vu) (1)
",2.2 Recurrent Layer Adaptation,[0],[0]
Adapting just the bias vector is a significant limitation.,2.2 Recurrent Layer Adaptation,[0],[0]
"The FactorCell model, (Jaech and Ostendorf, 2017), remedies this by letting the user embedding transform the weights of the recurrent layer via the use of a low-rank adaptation matrix.",2.2 Recurrent Layer Adaptation,[0],[0]
"The FactorCell uses a weight matrix W′ = W +A that has been additively transformed by a personalized low-rank matrix A. Because the FactorCell weight matrix W′ is different for each user (See Equation 2), it allows for a much stronger adaptation than what is possible using the more standard ConcatCell model.1
ht = σ([wt, ht−1]W ′ + b) (2)
The low-rank adaptation matrix A is generated by taking the product between a user’s m dimensional embedding and left and right bases tensors, ZL ∈ Rm×e+h×r and ZR ∈ Rr×h×m",2.2 Recurrent Layer Adaptation,[0],[0]
"as so,
A = (ui ×1 ZL)(ZR ×3 ui) (3)
where ×i denotes the mode-i tensor product.",2.2 Recurrent Layer Adaptation,[0],[0]
The above product selects a user specific adaptation matrix by taking a weighted combination of the m rank r matrices held between ZL and ZR.,2.2 Recurrent Layer Adaptation,[0],[0]
"The rank, r, is a hyperparameter which controls the degree of personalization.",2.2 Recurrent Layer Adaptation,[0],[0]
"Our experiments make use of the AOL Query data collected over three months in 2006 (Pass et al., 2006).",3 Data,[0],[0]
"The first six of the ten files were used for
1In the case of an LSTM, W′ is extended to incorporate all of the gates.
training.",3 Data,[0],[0]
"This contains approximately 12 million queries from 173,000 users for an average of 70 queries per user (median 15).",3 Data,[0],[0]
"A set of 240,000 queries from those same users (2% of the data) was reserved for tuning and validation.",3 Data,[0],[0]
"From the remaining files, one million queries from 30,000 users are used to test the models on a disjoint set of users.",3 Data,[0],[0]
The vocabulary consists of 79 characters including special start and stop tokens.,4.1 Implementation Details,[0],[0]
Models were trained for six epochs.,4.1 Implementation Details,[0],[0]
"The Adam optimizer is used during training with a learning rate of 10−3 (Kingma and Ba, 2014).",4.1 Implementation Details,[0],[0]
"When updating the user embeddings during evaluation, we found that it is easier to use an optimizer without momentum.",4.1 Implementation Details,[0],[0]
"We use Adadelta (Zeiler, 2012) and tune the online learning rate to give the best perplexity on a held-out set of 12,000 queries, having previously verified that perplexity is a good indicator of performance on the QAC task.2
",4.1 Implementation Details,[0],[0]
"The language model is a single-layer characterlevel LSTM with coupled input and forget gates and layer normalization (Melis et al., 2018; Ba et al., 2016).",4.1 Implementation Details,[0],[0]
We do experiments on two model configurations: small and large.,4.1 Implementation Details,[0],[0]
The small models use an LSTM hidden state size of 300 and 20 dimensional user embeddings.,4.1 Implementation Details,[0],[0]
The large models use a hidden state size of 600 and 40 dimensional user embeddings.,4.1 Implementation Details,[0],[0]
Both sizes use 24 dimensional character embeddings.,4.1 Implementation Details,[0],[0]
"For the small sized models, we experimented with different values of the FactorCell rank hyperparameter between 30 and 50 dimensions finding that bigger rank is better.",4.1 Implementation Details,[0],[0]
The large sized models used a fixed value of 60 for the rank hyperparemeter.,4.1 Implementation Details,[0],[0]
"During training only and due to limited computational resources, queries are truncated to a length of 40 characters.
",4.1 Implementation Details,[0],[0]
Prefixes are selected uniformly at random with the constraint that they contain at least two characters in the prefix and that there is at least one character in the completion.,4.1 Implementation Details,[0],[0]
"To generate completions using beam search, we use a beam width of 100 and a branching factor of 4.",4.1 Implementation Details,[0],[0]
"Results are reported using mean reciprocal rank (MRR), the standard method of evaluating QAC systems.",4.1 Implementation Details,[0],[0]
"It is the mean of the reciprocal rank of the true completion in the
2Code at http://github.com/ajaech/query completion
top ten proposed completions.",4.1 Implementation Details,[0],[0]
"The reciprocal rank is zero if the true completion is not in the top ten.
",4.1 Implementation Details,[0],[0]
Neural models are compared against an MPC baseline.,4.1 Implementation Details,[0],[0]
"Following Park and Chiba (2017), we remove queries seen less than three times from the MPC training data.",4.1 Implementation Details,[0],[0]
Table 2 compares the performance of the different models against the MPC baseline on a test set of one million queries from a user population that is disjoint with the training set.,4.2 Results,[0],[0]
Results are presented separately for prefixes that are seen or unseen in the training data.,4.2 Results,[0],[0]
"Consistent with prior work, the neural models do better than the MPC baseline.",4.2 Results,[0],[0]
The personalized models are both better than the unadapted one.,4.2 Results,[0],[0]
"The FactorCell model is the best overall in both the big and small sized experiments, but the gain is mainly for the seen prefixes.
",4.2 Results,[0],[0]
Figure 1 shows the relative improvement in MRR over an unpersonalized model versus the number of queries seen per user.,4.2 Results,[0],[0]
"Both the Factor-
Cell and the ConcatCell show continued improvement as more queries from each user are seen, and the FactorCell outperforms the ConcatCell by an increasing margin over time.",4.2 Results,[0],[0]
"In the long run, we expect that the system will have seen many queries from most users.",4.2 Results,[0],[0]
"Therefore, the right side of Figure 1, where the relative gain of FactorCell is up to 2% better than that of the ConcatCell, is more indicative of the potential of these models for active users.",4.2 Results,[0],[0]
"Since the data was collected over a limited time frame and half of all users have fifteen or fewer queries, the results in Table 2 do not reflect the full benefit of personalization.
",4.2 Results,[0],[0]
Figure 2 shows the MRR for different prefix and query lengths.,4.2 Results,[0],[0]
We find that longer prefixes help the model make longer completions and (more obviously) shorter completions have higher MRR.,4.2 Results,[0],[0]
"Comparing the personalized model against the unpersonalized baseline, we see that the biggest gains are for short queries and prefixes of length one or two.
",4.2 Results,[0],[0]
We found that one reason why the FactorCell outperforms the ConcatCell is that it is able to pick up sooner on the repetitive search behaviors that some users have.,4.2 Results,[0],[0]
This commonly happens for navigational queries where someone searches for the name of their favorite website once or more per day.,4.2 Results,[0],[0]
At the extreme tail there are users who search for nothing but free online poker.,4.2 Results,[0],[0]
"Both models do well on these highly predictable users but the FactorCell is generally a bit quicker to adapt.
",4.2 Results,[0],[0]
We conducted case studies to better understand what information is represented in the user embeddings and what makes the FactorCell different from the ConcatCell.,4.2 Results,[0],[0]
From a cold start user embedding we ran two queries and allowed the model to update the user embedding.,4.2 Results,[0],[0]
"Then, we ranked
the most frequent 1,500 queries based on the ratio of their likelihood from before and after updating the user embeddings.
",4.2 Results,[0],[0]
"Tables 3 and 4 show the queries with the highest relative likelihood of the adapted vs. unadapted models after two related search queries: “high school softball” and “math homework help” for Table 3, and “Prada handbags” and “Versace eyewear” for Table 4.",4.2 Results,[0],[0]
"In both cases, the FactorCell model examples are more semantically coherent than the ConcatCell examples.",4.2 Results,[0],[0]
"In the first case, the FactorCell model identifies queries that a high school student might make, including entertainment sources and a celebrity entertainer popular with that demographic.",4.2 Results,[0],[0]
"In the second case, the FactorCell model chooses retailers that carry woman’s apparel and those that sell home goods.",4.2 Results,[0],[0]
"While these companies’ brands are not as luxurious as Prada or Versace, most of the top luxury brand names do not appear in the top 1,500 queries and our model may not be capable of being that specific.",4.2 Results,[0],[0]
There is no obvious semantic connection between the highest likelihood ratio phrases for the ConcatCell; it seems to be focusing more on orthography than semantics (e.g. “home” in the first example)..,4.2 Results,[0],[0]
Not shown are the queries which experienced the greatest decrease in likelihood.,4.2 Results,[0],[0]
"For the “high school” case, these included searches for travel agencies and airline tickets— websites not targeted towards the high school age demographic.",4.2 Results,[0],[0]
"While the standard implementation of MPC can not handle unseen prefixes, there are variants which do have that ability.",5 Related Work,[0],[0]
"Park and Chiba (2017) find that the neural LM outperforms MPC even when MPC has been augmented with the approach from Mitra and Craswell (2015) for handling rare
prefixes.",5 Related Work,[0],[0]
"There has also been work on personalizing MPC (Shokouhi, 2013; Cai et al., 2014).",5 Related Work,[0],[0]
We did not compare against these specific models because our goal was to show how personalization can improve the already-proven generative neural model approach.,5 Related Work,[0],[0]
"RNN’s have also previously been used for the related task of next query suggestion (Sordoni et al., 2015).
",5 Related Work,[0],[0]
Our results are not directly comparable to Park and Chiba (2017) or Mitra and Craswell (2015) due to differences in the partitioning of the data and the method for selecting random prefixes.,5 Related Work,[0],[0]
Prior work partitions the data by time instead of by user.,5 Related Work,[0],[0]
"Splitting by users is necessary in order to properly test personalization over longer time ranges.
",5 Related Work,[0],[0]
Wang et al. (2018) show how spelling correction can be integrated into an RNN language model query auto-completion system and how the completions can be generated in real time using a GPU.,5 Related Work,[0],[0]
"Our method of updating the model during evaluation resembles work on dynamic evaluation for language modeling (Krause et al., 2017), but differs in that only the user embeddings (latent demographic factors) are updated.",5 Related Work,[0],[0]
Our experiments show that the LSTM model can be improved using personalization.,6 Conclusion and Future Work,[0],[0]
The method of adapting the recurrent layer clearly matters and we obtained an advantage by using the FactorCell model.,6 Conclusion and Future Work,[0],[0]
The reason the FactorCell does better is in part attributable to having two to three times as many parameters in the recurrent layer as either the ConcatCell or the unadapted models.,6 Conclusion and Future Work,[0],[0]
"By design, the adapted weight matrix W′ only needs to be computed at most once per query and is reused many thousands of times during beam search.",6 Conclusion and Future Work,[0],[0]
"As a result, for a given latency budget, the FactorCell
model outperforms the Mikolov and Zweig (2012) model for LSTM adaptation.
",6 Conclusion and Future Work,[0],[0]
"The cost for updating the user embeddings is similar to the cost of the forward pass and depends on the size of the user embedding, hidden state size, FactorCell rank, and query length.",6 Conclusion and Future Work,[0],[0]
"In most cases there will be time between queries for updates, but updates can be less frequent to reduce computational costs.
",6 Conclusion and Future Work,[0],[0]
We also showed that language model personalization can be effective even on users who are not seen during training.,6 Conclusion and Future Work,[0],[0]
The benefits of personalization are immediate and increase over time as the system continues to leverage the incoming data to build better user representations.,6 Conclusion and Future Work,[0],[0]
The approach can easily be extended to include time as an additional conditioning factor.,6 Conclusion and Future Work,[0],[0]
We leave the question of whether the results can be improved by combining the language model with MPC for future work.,6 Conclusion and Future Work,[0],[0]
Query auto-completion is a search engine feature whereby the system suggests completed queries as the user types.,abstractText,[0],[0]
"Recently, the use of a recurrent neural network language model was suggested as a method of generating query completions.",abstractText,[0],[0]
We show how an adaptable language model can be used to generate personalized completions and how the model can use online updating to make predictions for users not seen during training.,abstractText,[0],[0]
The personalized predictions are significantly better than a baseline that uses no user information.,abstractText,[0],[0]
Personalized Language Model for Query Auto-Completion,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2019–2025, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Technologies are increasingly personalized, accommodating their behavior for each user.",1 Introduction,[0],[0]
Such personalization is done through user modeling where the goal is to “get to know” the user.,1 Introduction,[0],[0]
"To that end, personalization is based on users’ attributes, such as demographics (gender, age etc.), personalities, and preferences.",1 Introduction,[0],[0]
"For example, in Information Retrieval, results are customized according to the user’s information and search history (Speretta and Gauch, 2005), performance of Automatic Speech Recognition substantially improves when adapted to a specific speaker (Neumeyer et al., 1995), and Targeted Advertising makes use of the user’s location and prior purchases (Kölmel and Alexakis, 2002).
",1 Introduction,[0],[0]
Personalization in machine translation has a somewhat different nature.,1 Introduction,[0],[0]
"Providers of MT tools and services offer means to “customize” or “personalize” the translation engine for each client, mostly through domain adaptation techniques, and a great deal of effort is made to make the human-involved translation process more efficient (see Section 2.2).",1 Introduction,[0],[0]
"Most of the focus, though, goes to customization for companies or professional translators.",1 Introduction,[0],[0]
"We argue that Personalized Machine Translation (PMT below) should and can take the next step and directly address individual end-users.
∗This work was done while the first author was at Xerox Research Centre Europe.
",1 Introduction,[0],[0]
The difficulty to objectively determine whether one (automatic) translation is better than another has been repeatedly revealed in the MT literature.,1 Introduction,[0],[0]
"Our conjecture is that one reason is individual preferences, to which we refer as Translational Preferences (TP).",1 Introduction,[0],[0]
"TP come into play both when the alternative translations are all correct, and when each of them is wrong in a different way.",1 Introduction,[0],[0]
"In the former case, a preference may be a stylistic choice, and in the latter, a matter of comprehension or a selection of the least intolerable error in one’s opinion.",1 Introduction,[0],[0]
"For instance, one user may prefer shorter sentences than others; she may favor a more formal style, while another would rather have it casual.",1 Introduction,[0],[0]
A user could be fine with some reordering errors but be more picky concerning punctuations.,1 Introduction,[0],[0]
"One user will not be bothered if some words are left untranslated (perhaps because the source language belongs to the same language family as the target language that he speaks), while another will find it utterly displeasing.",1 Introduction,[0],[0]
"Such differences may be the result of the type of translation system being employed (e.g. syntax- vs. phrased-based), the specific training data or many other factors.",1 Introduction,[0],[0]
"On the user’s side, a preference may be attributed, for example, to her mother tongue, her age or her personality.
",1 Introduction,[0],[0]
"Two aspects of end-user PMT may be considered: (i) Personalized translation of texts written by a specific user, and (ii) PMT to provide better translations for a specific reader.",1 Introduction,[0],[0]
"In this work we address the second task, aiming to identify translations each user is more likely to prefer.1 Specifically, we consider a setting where at least two MT systems are available, and the goal is to predict which of the translation systems the user would choose, assuming we have no knowledge about her preference between them.",1 Introduction,[0],[0]
"Benchmarking the systems in advance with respect to a reference set, or estimating the quality of the translations (Specia et al., 2009) are viable alternatives for translation selection; these, however, are not personalized to the target user.",1 Introduction,[0],[0]
"Instead, we employ a user-user Collaborative Filtering approach, common in Recommender Systems, which we map to the TP prediction task.
",1 Introduction,[0],[0]
"We assess this approach using a collection of user rankings of MT systems from a shared translation task
1In (Mirkin et al., 2015)",1 Introduction,[0],[0]
"we investigate the first task, assessing whether the author’s demographic and personality traits are preserved over machine translation.
2019
(see Section 3).",1 Introduction,[0],[0]
"Our results show that the personalized method modestly, but consistently, outperforms several other approaches that rank the systems in general, disregarding the specific user.",1 Introduction,[0],[0]
We consider this as an indication that user feedback can be employed towards a more personalized approach to machine translation.,1 Introduction,[0],[0]
"Collaborative filtering (CF) is a common approach employed by recommender systems for suggesting to users items, such as books or movies.",2.1 Collaborative filtering,[0],[0]
"A recommender system may simply suggest to all users the most popular items; often, however, the recommendations are personalized for each individual user to fit her taste or preferences.",2.1 Collaborative filtering,[0],[0]
User-user CF relies on community preferences.,2.1 Collaborative filtering,[0],[0]
"The idea is to recommend to the user items that are liked by users similar to her, as manifested, for example, by high rating.",2.1 Collaborative filtering,[0],[0]
Similar users are those that agree with the current user on previously-rated items.,2.1 Collaborative filtering,[0],[0]
"In k-nearest-neighbors CF, a user is typically represented by a vector of her preferences, where each entry of the vector is, e.g., a rating of a movie.",2.1 Collaborative filtering,[0],[0]
k similar users are then identified by measuring the similarity between the users’ vectors.,2.1 Collaborative filtering,[0],[0]
"Cosine similarity is a popular function for that purpose, and we also use it in our work (Resnick et al., 1994; Sarwar et al., 2001; Ricci et al., 2011).",2.1 Collaborative filtering,[0],[0]
"An alternative to cosine, Pearson’s correlation coefficient (Pearson, 1895), allows addressing different rating patterns across users.",2.1 Collaborative filtering,[0],[0]
"In comparison to cosine, here vector entries are normalized with respect to the user’s average rating.",2.1 Collaborative filtering,[0],[0]
"In our case, such normalization is not very meaningful since the entries of the users vectors represent comparisons rather than absolute ratings, as will be made clear in Section 4.",2.1 Collaborative filtering,[0],[0]
"Nevertheless, we have experimented with Pearson correlation as well, and found no advantage in using it instead of cosine.",2.1 Collaborative filtering,[0],[0]
"Various means of customization and personalization are available, in both academic and commercial MT.","2.2 Customization, personalization and adaptation in MT",[0],[0]
"Many of them target the company, rather than the individual user, and much of the effort is invested in designing tools for professional translators, aiming to improve their productivity, through intelligent Computer Aided Translation (CAT).
","2.2 Customization, personalization and adaptation in MT",[0],[0]
"Domain adaptation methods are commonly used to adapt to the topic, the genre and even the style of the translated material.","2.2 Customization, personalization and adaptation in MT",[0],[0]
"Using the company’s own corpora is one of the simplest techniques to do so, but many more approaches have been proposed, including dataselection (Axelrod et al., 2011; Gascó","2.2 Customization, personalization and adaptation in MT",[0],[0]
"et al., 2012; Mirkin and Besacier, 2014), mixture models (Foster and Kuhn, 2007) and table fill-up (Bisazza et al., 2011).","2.2 Customization, personalization and adaptation in MT",[0],[0]
"Clients can utilize their own glossaries (Federico et al., 2014), corpora (parallel or monolingual) and translation memories (TM), either shared or private ones
(Caskey and Maskey, 2014; Federico et al., 2014).","2.2 Customization, personalization and adaptation in MT",[0],[0]
"Through Adaptive and Interactive MT (Nepveu et al., 2004), the system learns from the translator’s edits, in order to avoid repeating errors that have already been corrected.","2.2 Customization, personalization and adaptation in MT",[0],[0]
"Post-editions can continuously be added to the translator’s TM or be used as additional training material, for tighter adaptation to the domain of interest, through batch or incremental training.","2.2 Customization, personalization and adaptation in MT",[0],[0]
Many tasks that require annotation by humans are affected by the annotator and not only by the item being judged.,2.3 User preferences in MT,[0],[0]
"Metrics for inter-rater reliability or interannotator agreement, such as Cohen’s Kappa (Cohen, 1960), help measuring the extent to which annotators disagree.",2.3 User preferences in MT,[0],[0]
"Disagreement may be due to untrained or inattentive annotators, a result of a task that is not well defined, or when there is no obvious “truth”.",2.3 User preferences in MT,[0],[0]
Such is the case with the evaluation of translation quality – it is not always straightforward to tell whether one translation is better than another.,2.3 User preferences in MT,[0],[0]
A single sentence can be translated in multiple correct ways.,2.3 User preferences in MT,[0],[0]
The decision becomes even harder when the translations are automatically produced and are imperfect: Is one error worse than another?,2.3 User preferences in MT,[0],[0]
The answer is in the eye of the beholder.,2.3 User preferences in MT,[0],[0]
"MT papers regularly report rather low Kappa levels, even when measured on simpler tasks, such as short segments (Macháček and Bojar, 2015).
",2.3 User preferences in MT,[0],[0]
Turchi et al. (2013) refer to the issue of “subjectivity” of human annotators.,2.3 User preferences in MT,[0],[0]
"They address the task of binary classification of “good” vs. “bad” translations, and show that relying on human annotation for training a binary quality estimator is less effective than using automatically-generated labels.",2.3 User preferences in MT,[0],[0]
This subjectivity is exactly what we are after.,2.3 User preferences in MT,[0],[0]
"We treat it as a preference, trying to identify the systems or specific translations which the user subjectively prefers.
Kichhoff et al. (2012) analyze user preferences with respect to MT errors.",2.3 User preferences in MT,[0],[0]
"They show that some types, e.g. word order errors, are the most dis-preferred by users, and that this is a more important factor than the number of errors.",2.3 User preferences in MT,[0],[0]
"While very relevant for our research, their analysis is aggregated over all users participating in the study, and is not focusing on individuals’ preferences.",2.3 User preferences in MT,[0],[0]
"In this work we used the data provided for the MT Shared Task in the 2013 Workshop on Statistical Machine Translation (WMT) (Bojar et al., 2013).2",3 Data,[0],[0]
"This data was of a particularly large scale, with crowdsourced human judges, either volunteer researchers or paid Amazon Turkers.",3 Data,[0],[0]
"For each source sentence, a judge was presented with the source sentence itself, a reference translation, and the outputs of five machine translation systems.",3 Data,[0],[0]
"The five systems were randomly selected from the pool of participating systems,
2http://www.statmt.org/wmt13/ translation-task.html
and were anonymized and randomly-ordered when presented to the judge.",3 Data,[0],[0]
"The judge had to rank the translations, with ties allowed (i.e. two system can receive the same ranking).",3 Data,[0],[0]
"Hence, each annotation point provided with 10 pairwise rankings between systems.",3 Data,[0],[0]
"Translations of 10 language pairs were assessed, with 11 to 19 systems for each pair.",3 Data,[0],[0]
"In total, over 900K non-tied pairwise rankings were collected.",3 Data,[0],[0]
"The Turkers’ annotation included a control task for quality assurance, rejecting Turkers failing more than 50% of the control points.",3 Data,[0],[0]
The inter-annotator score showed on average a fair to moderate level of agreement.,3 Data,[0],[0]
"Our method, denoted CTP (Collaborative Translational Preferences), is based on a k-nearest-neighbors approach for user-user CF.",4 Translational preferences with collaborative filtering,[0],[0]
"That is, we predict the translational preferences of a user based on those of similar users.",4 Translational preferences with collaborative filtering,[0],[0]
"In our setting, a user preference is the choice between two translation systems – which system’s translations does the user prefer.",4 Translational preferences with collaborative filtering,[0],[0]
"Given two systems (or models of the same system) we wish to predict which one the user would prefer, without assuming the user has ever expressed her preference between these two specific systems.",4 Translational preferences with collaborative filtering,[0],[0]
"It is important to emphasize that the method presented here considers the users’ overall preferences of systems, and does not regard the specific sentence that is being translated.",4 Translational preferences with collaborative filtering,[0],[0]
In future work we intend to make use of this information as well.,4 Translational preferences with collaborative filtering,[0],[0]
"As mentioned in Section 3, each annotation consists of a ranking of five systems.",4.1 Representation,[0],[0]
"From that, we extract pairwise rankings for every pair of systems that were ranked for a given language pair.",4.1 Representation,[0],[0]
"For each user u ∈ U (where U are all users who annotated the language pair), we create a user-preference vector, pu, that contains an entry for each pair of translation systems.",4.1 Representation,[0],[0]
"Denoting the set of systems with S, we have |S|·(|S|−1)2 system pairs.",4.1 Representation,[0],[0]
"E.g., for Czech-English, with 11 participating systems, the user vector size is 55.",4.1 Representation,[0],[0]
"Each entry (i, j) of the vector is assigned the following value:
pu (i,j) =
w",4.1 Representation,[0],[0]
"(i,j) u − l(i,j)u w",4.1 Representation,[0],[0]
"(i,j) u +",4.1 Representation,[0],[0]
"l (i,j) u
(1)
where w(i,j)u and l (i,j) u are the number of wins and loses of system si vs. system sj as judged by user u.3
With this representation, a user vector contains values between −1 (if si always lost to sj) and 1 (if si always won).",4.1 Representation,[0],[0]
"If the user always ranked the two systems identically, the value is 0, and if she has never evaluated the pair, the entry is regarded as a missing value (NA).",4.1 Representation,[0],[0]
"Altogether, we have a matrix of users by system pairs, as depicted in Figure 1.
",4.1 Representation,[0],[0]
3We have also considered including ties in the denominator of the equation; discarding them was found superior.,4.1 Representation,[0],[0]
"Given a user preference to predict for a pair of systems (si, sj), we compute the similarity between pu and each one of pu′ for all other u′ ∈ U .",4.2 Finding similar users,[0],[0]
In our experiments we used cosine as the similarity measure.,4.2 Finding similar users,[0],[0]
The k most-similar-users (MSU ) are then selected.,4.2 Finding similar users,[0],[0]
"To be included in MSU (u), we require that u and u′ have judged at least 2 common system pairs.",4.2 Finding similar users,[0],[0]
"Given the similarity scores, to predict the user’s preference for the target system pair, we compute a weighted average of the predictions of the users in MSU (u).
",4.3 Preference prediction,[0],[0]
We include in the average only users with similarity scores above a certain positive threshold (0.05).,4.3 Preference prediction,[0],[0]
We then require that a minimum number of users meet the above criteria of common annotations and minimum similarity (we used 5).,4.3 Preference prediction,[0],[0]
"If not enough such similar users are found, we turn to a fallback, where we use the non-weighted average preference across all users (AVPF presented in Section 5).4 The prediction is then the sign of the weighted average.",4.3 Preference prediction,[0],[0]
"A positive value means si is the preferred system; a negative one means it is sj , and a zero is a draw.",4.3 Preference prediction,[0],[0]
"In our evaluation we compare this prediction to the sign of the actual preference of the user, pu(i,j).",4.3 Preference prediction,[0],[0]
"Formally, CTP computes the following prediction function f for a given user u and a system pair (si, sj):
fCTP(u)(i,j) = sign( ∑ u′ pu′",4.3 Preference prediction,[0],[0]
"(i,j) · sim(u, u′)∑
u′ sim(u, u′) )",4.3 Preference prediction,[0],[0]
"(2)
where u′ ∈ MSU (u) are the most similar users (the nearest neighbors) of u; pu′ (i,j) are the preferences of user u′ for (si, sj) and sim(u, u′) is the similarity score between the two users.5",4.3 Preference prediction,[0],[0]
"In our experiments we try to predict which one of two translation systems would be preferred by a given user.
4The fallback was used 0.1% of the times.",5.1 Evaluation methodology,[0],[0]
5The denominator is not required as long as we predict only the sign since all used similarity scores are positive.,5.1 Evaluation methodology,[0],[0]
"We keep it in order to obtain a normalized score that can be used for other decisions, e.g. ranking multiple systems.
",5.1 Evaluation methodology,[0],[0]
"We evaluate our method, as well as several other prediction functions, when compared with the user’s pairwise system preference according to the annotation – pu
(i,j), shown in Equation 1.",5.1 Evaluation methodology,[0],[0]
"For each user this is an aggregated figure over all her pairwise rankings for the pair, determining the preferred system as the one chosen by the user (i.e. ranked higher) more times.
",5.1 Evaluation methodology,[0],[0]
We conduct a leave-one-out experiment.,5.1 Evaluation methodology,[0],[0]
"For each language pair, we iterate over all non-NA entries in the user-preferences matrix, remove the entry and try to predict it.",5.1 Evaluation methodology,[0],[0]
"User similarity scores are re-computed for each evaluation point, to ensure they do not consider the target pair.",5.1 Evaluation methodology,[0],[0]
"The “gold” preference is positive when the user prefers si, negative when she prefers sj and 0 when she has no preference between them.",5.1 Evaluation methodology,[0],[0]
"Hence, each of the assessed methods is measured by the accuracy of predicting the sign of the preference.",5.1 Evaluation methodology,[0],[0]
"We compare CTP to the following prediction methods:
Always i (ALI)",5.2 Non-personalized methods,[0],[0]
This is a naı̈ve baseline showing the score when always predicting that system i wins.,5.2 Non-personalized methods,[0],[0]
"Note that the baseline is not simply 50% due to ties.
",5.2 Non-personalized methods,[0],[0]
Average rank (RANK),5.2 Non-personalized methods,[0],[0]
"Here, two systems are compared by the average of their rankings across all annotations (r ∈ {1, 2, 3, 4, 5}):
fRANK(u)(i,j) = sign(rj − ri) (3) rj and ri are the average ranks of sj and si respectively.",5.2 Non-personalized methods,[0],[0]
"Since a smaller value of r corresponds to a higher rank, we subtract the rank of si from sj and not the other way around.",5.2 Non-personalized methods,[0],[0]
"This way, if for instance, si is ranked on averaged higher than sj , the prediction would be positive, as desired.
",5.2 Non-personalized methods,[0],[0]
Expected (EXPT),5.2 Non-personalized methods,[0],[0]
"This metric, proposed by Koehn (2012) and used by Bojar et al. (2013) in order to rank the participating systems in the WMT benchmark, compares the expected wins of the two systems.",5.2 Non-personalized methods,[0],[0]
"Its intuition is explained as follows: “If the system is compared against a randomly picked opposing system, on a randomly picked sentence, by a randomly picked judge, what is the probability that its translation is ranked higher?”",5.2 Non-personalized methods,[0],[0]
"The expected wins of si, e(si), is the probability of si to win when compared to another system, estimated as the total number of wins of si relative to the total number of comparisons involving it, excluding ties, and normalized by the total number of systems excluding si, |{sk}|:
e(i)",5.2 Non-personalized methods,[0],[0]
"= 1 |{sk}| ∑ k 6=i
w(i,k)
w(i,k) + l(i,k) (4)
where w(i,k) and l(i,k) are summed over all users.",5.2 Non-personalized methods,[0],[0]
"The preference prediction is therefore:
fEXPT(u)(i,j) = sign(e(i)− e(j)) (5)
RANK and EXPT predict preferences based on a system’s performance in general, when compared to all other systems.",5.2 Non-personalized methods,[0],[0]
"We propose an additional prediction function for comparison which uses only the information concerning the system pair under consideration.
",5.2 Non-personalized methods,[0],[0]
Average user preference (AVPF),5.2 Non-personalized methods,[0],[0]
This method takes into account only the specific system pair and averages the user preferences for the pair.,5.2 Non-personalized methods,[0],[0]
"Formally:
fAVPF(u)(i,j) = sign( ∑ u′ p (i,j) u′
|{u′}| ) (6)
where u′ 6= u, and {u′} are all users except u.",5.2 Non-personalized methods,[0],[0]
"This method can be viewed as a non-personalized version of CTP, with two differences:
(1) It considers all users, and not only similar ones.",5.2 Non-personalized methods,[0],[0]
"(2) It does not weight the preferences of the other
users by their similarity to the target user.",5.2 Non-personalized methods,[0],[0]
Table 1 shows the results of an experiment comparing the performance of the various methods in terms of prediction accuracy.,5.3 Results,[0],[0]
"Figure 2 shows the micro-average scores, when giving each of the 97,412 test points an equal weight in the average.",5.3 Results,[0],[0]
"CTP outperforms all others for 9 out of 10 language pairs, and in the overall microaveraged results.",5.3 Results,[0],[0]
"The difference between CTP and each of the other metrics was found statistically significance with p < 5 · 10−6 at worse, as measured with a paired Wilcoxon signed rank test (Wilcoxon, 1945) on the predictions of the two methods.",5.3 Results,[0],[0]
"The significance test captures in this case the fact that the methods disagreed in many more cases than is visible by the score difference.
",5.3 Results,[0],[0]
"Our method was found superior to all others also when computing macro-average, taking the average of the scores of each language pair, as well as when the ties are included in the computation of pu.
",5.3 Results,[0],[0]
The parameters with which the above results were obtained are found within the method’s description in Section 4.,5.3 Results,[0],[0]
"Yet, in our experiments, CTP turned out to be rather insensitive to their values.",5.3 Results,[0],[0]
In this experiment we used a global set of parameters and did not tune them for each language pair separately.,5.3 Results,[0],[0]
It is reasonable to assume that such tuning would improve results.,5.3 Results,[0],[0]
"For instance, choosing k, the number of users to include in the average, depends on the total number of users.",5.3 Results,[0],[0]
"E.g., for en-es, where there are only 57 users in total, reducing k’s value from 50 to 25, improves results of CTP from 62.6% to 63.2%, higher than all other methods (whose scores are not affected).
",5.3 Results,[0],[0]
"Specifically in comparison to AVPF, weighting by the similarity scores was found to be a more significant factor than selecting a small subset of the users.",5.3 Results,[0],[0]
"This may not come as a surprise, since less similar users that are added to MSU (u) have a smaller impact on the final decision since their weight in the average is smaller.
",5.3 Results,[0],[0]
"One weakness of CTP, as well as of other methods, is that it poorly predict ties.",5.3 Results,[0],[0]
"In the above experiment, approximately 13.5% of the preferences were 0, none of them was correctly identified.",5.3 Results,[0],[0]
Our analysis showed that numerical accuracy is not the main cause; setting any prediction that is smaller than some values of |ε| to 0 was not found helpful.,5.3 Results,[0],[0]
"Arguably, ties need not be predicted, since if the user has no preference between two systems, any choice is just as good.",5.3 Results,[0],[0]
"Still, we believe that better ties prediction could lead to general improvement of our method and we wish to address it in future work.",5.3 Results,[0],[0]
We addressed the task of predicting user preference with respect to MT output via a collaborative filtering approach whose prediction is based on preferences of similar users.,6 Discussion,[0],[0]
This method predicts TP better than a set of non-personalized methods.,6 Discussion,[0],[0]
"The gain is modest in absolute numbers, but the results are highly statistically significant and stable over parameter values.
",6 Discussion,[0],[0]
We consider this work as a step towards more personalized MT.,6 Discussion,[0],[0]
This line of research can be extended in multiple ways.,6 Discussion,[0],[0]
"First and foremost, as mentioned, we did not consider the actual content of the sentences, but rather identified a general preference for one system over another.",6 Discussion,[0],[0]
"It is plausible, however, that one system is better – from the user’s perspective – at translating one type of text, while another is preferred for other texts.",6 Discussion,[0],[0]
"Taking the actual texts into account seems therefore es-
sential.",6 Discussion,[0],[0]
Content-based methods for recommender systems may be useful for this purpose.,6 Discussion,[0],[0]
"Another factor that may be affecting preferences is translation quality: when compared translations are all poor, preferences play a less significant role.",6 Discussion,[0],[0]
"Hence, it may be informative to assess TP prediction separately across different levels of translation quality.
",6 Discussion,[0],[0]
Large parallel corpora are typically required for training reasonable statistical translation models.,6 Discussion,[0],[0]
"Yet, parallel corpora, and even more so in-domain ones, are hard to gather.",6 Discussion,[0],[0]
"It is virtually impossible to find a user-specific parallel corpus, and methods for monolingual domain adaptation are easier to envisage if one wishes to address author-aware PMT (the first PMT task mentioned in Section 1).",6 Discussion,[0],[0]
"Collecting user feedback is another challenge, especially since most endusers do not speak the source language.",6 Discussion,[0],[0]
"For that and other reasons, it currently seems more feasible to collect preference information from professional translators, explicitly or implicitly.",6 Discussion,[0],[0]
"Yet, in this research we aim at end-users rather than translators whose preferences are often driven by the ease of correction more than anything else.",6 Discussion,[0],[0]
"We believe that one way to tackle this issue is to exploit other kinds of feedback, from which we can infer user preferences and similarity.",6 Discussion,[0],[0]
Online MT providers are recently collecting end-user feedback for their proposed translations which may be useful for TP prediction.,6 Discussion,[0],[0]
"For instance, in early 2015 Facebook introduced a feature letting users rate (Bing) translations, and Google Translate asks for suggested improvements.",6 Discussion,[0],[0]
We are hopeful that such data becomes publicly available.,6 Discussion,[0],[0]
"Nevertheless, it remains unlikely to obtain feedback from each and every user.",6 Discussion,[0],[0]
"A potential direction for both corpora and feedback collection is personalizing models and identifying preferences for groups of users based on socio-demographic traits, such as gender, age or mother tongue, or based on (e.g. Big 5) personality traits.",6 Discussion,[0],[0]
These can even be inferred by automatically analyzing user texts.,6 Discussion,[0],[0]
We wish to thank Hervé Déjean and the EMNLP reviewers for their valuable feedback on this work.,Acknowledgments,[0],[0]
"Machine Translation (MT) has advanced in recent years to produce better translations for clients’ specific domains, and sophisticated tools allow professional translators to obtain translations according to their prior edits.",abstractText,[0],[0]
We suggest that MT should be further personalized to the end-user level – the receiver or the author of the text – as done in other applications.,abstractText,[0],[0]
"As a step in that direction, we propose a method based on a recommender systems approach where the user’s preferred translation is predicted based on preferences of similar users.",abstractText,[0],[0]
"In our experiments, this method outperforms a set of non-personalized methods, suggesting that user preference information can be employed to provide better-suited translations for each user.",abstractText,[0],[0]
Personalized Machine Translation: Predicting Translational Preferences,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 208–215 New Orleans, Louisiana, June 1 - 6, 2018. c©2017 Association for Computational Linguistics",text,[0],[0]
"Predicting the next characters or words following a prefix has had multiple uses from helping handicapped people (Swiffin et al., 1987) to, more recently, helping search engine users (Cai et al., 2016).",1 Introduction,[0],[0]
"In practice, most search engines today use query auto completion (QAC) systems, consisting of suggesting queries as users type in the search box (Fiorini et al., 2017).",1 Introduction,[0],[0]
"The task suffers from high dimensionality, because the number of possible solutions increases as the length of the target query increases.",1 Introduction,[0],[0]
"Historically, the query prediction task has been addressed by relying on query logs, particularly the popularity of past queries (BarYossef and Kraus, 2011; Lu et al., 2009).",1 Introduction,[0],[0]
"The idea is to rely on the wisdom of the crowd, as popular
queries matching a typed prefix are more likely to be the user’s intent.
",1 Introduction,[0],[0]
"This traditional approach is usually referred to as MostPopularCompletion (MPC)(Bar-Yossef and Kraus, 2011).",1 Introduction,[0],[0]
"However, the performance of MPC is skewed: it is very high for popular queries and very low for rare queries.",1 Introduction,[0],[0]
"At the extreme, MPC simply cannot predict a query it has never seen.",1 Introduction,[0],[0]
"This becomes a bigger problem in academic search (Lankinen et al., 2016), where systems are typically less used, with a wider range of possible queries.",1 Introduction,[0],[0]
"Recent advances in deep learning, particularly in semantic modeling (Mitra and Craswell, 2015) and neural language modeling (Park and Chiba, 2017) showed promising results for predicting rare queries.",1 Introduction,[0],[0]
"In this work, we propose to improve the state-of-the-art approaches in neural QAC by integrating personalization and time sensitivity information as well as addressing current MPC limitations by diversifying the suggestions, thus approaching a production-ready architecture.",1 Introduction,[0],[0]
"While QAC has been well studied, the field has recently started to shift towards deep learningbased models, which can be categorized into two main classes: semantic models (using Convolutional Neural Nets, or CNNs) (Mitra and Craswell, 2015) and language models (using Recurrent Neural Nets, or RNNs) (Park and Chiba, 2017).",2.1 Neural query auto completion,[0],[0]
"Both approaches are frequently used in natural language processing in general (Kim et al., 2016) and tend to capture different features.",2.1 Neural query auto completion,[0],[0]
"In this work, we focus on RNNs as they provide a flexible solution to generate text, even when it is not previously seen in the training data.
",2.1 Neural query auto completion,[0],[0]
"Yet, recent work in this field (Park and Chiba, 2017) suffers from some limitations.",2.1 Neural query auto completion,[0],[0]
"Most importantly, the probability estimates for full queries
208
are directly correlated to the length of the suggestions, consequently favoring shorter queries in some cases and hampering some predictions (Park and Chiba, 2017).",2.1 Neural query auto completion,[0],[0]
"By appending these results to MPC’s and re-ranking the list with LambdaMART (Burges, 2010) in another step as suggested in previous work (Mitra and Craswell, 2015), they achieve state-of-the-art performance in neural query auto completion at the cost of a higher complexity and more computation time.",2.1 Neural query auto completion,[0],[0]
"Still, these preliminary approaches have yet to integrate standards in QAC, e.g. query personalization (Koutrika and Ioannidis, 2005; Margaris et al., 2018) and time sensitivity (Cai et al., 2014).",2.2 Context information,[0],[0]
This integration has to differ from traditional approaches by taking full advantage of neural language modeling.,2.2 Context information,[0],[0]
"For example, neural language models could be refined to capture interests of some users as well as their actual language or query formulation.",2.2 Context information,[0],[0]
"The same can apply to timesensitivity, where the probability of queries might change over time (e.g. for queries such as “tv guide”, or “weather”).",2.2 Context information,[0],[0]
"Furthermore, the feasibility of these approaches in real-world settings has not been demonstrated, even more so on specialized domains.
",2.2 Context information,[0],[0]
"By addressing these issues, we make the following contributions in this work compared to the previous approaches:
• We propose a more straightforward architecture with improved scalability;
• Our method integrates user information when available as well as time-sensitivity;
• We propose to use a balanced beam search for ensuring diversity;
• We test on a second dataset and compare the generalizability of different methods in a specialized domain;
•",2.2 Context information,[0],[0]
"Our method achieves stronger performance than the state of the art on both datasets.
",2.2 Context information,[0],[0]
"Finally, our source code is made available in a public repository1.",2.2 Context information,[0],[0]
"This allows complete reproducibility of our results and future comparisons.
1https://github.com/ncbi-nlp/NQAC",2.2 Context information,[0],[0]
"The justification of using a neural language model for the task of predicting queries is that it has been proven to perform well to generate text that has never been seen in the training data (Sutskever et al., 2011).",3.1 Personalized neural Language Model,[0],[0]
"Particularly, character-level models work with a finer granularity.",3.1 Personalized neural Language Model,[0],[0]
"That is, if a given prefix has not been seen in the training data (e.g. a novel or incomplete word), the model can use the information shared across similar prefixes to make a prediction nonetheless.
",3.1 Personalized neural Language Model,[0],[0]
Recurrent Neural Network,3.1 Personalized neural Language Model,[0],[0]
The difficulty of predicting queries given a prefix is that the number of candidates explodes as the query becomes longer.,3.1 Personalized neural Language Model,[0],[0]
"RNNs allow to represent each character (or word) of a sequence as a cell state, therefore reducing the dimensionality of the task.",3.1 Personalized neural Language Model,[0],[0]
"However, they also introduce the vanishing gradient problem during backpropagation, preventing them from learning long-term dependencies.",3.1 Personalized neural Language Model,[0],[0]
"Both gated recurrent units (GRU) (Cho et al., 2014) and long-short term memory cells (LSTMs) solve this limitation — albeit with a different approach — and are increasingly used.",3.1 Personalized neural Language Model,[0],[0]
"In preliminary experiments, we tried various forms of RNNs: vanilla RNNs, GRUs and LSTMs.",3.1 Personalized neural Language Model,[0],[0]
"GRUs performed similarly to LSTM with a smaller computational complexity due to fewer parameters to learn as was previously observed (Jozefowicz et al., 2015).
",3.1 Personalized neural Language Model,[0],[0]
Word embedded character-level Neural Language Model,3.1 Personalized neural Language Model,[0],[0]
"The main novelty in (Park and Chiba, 2017) is to combine a character-level neural language model with a word-embedded space character.",3.1 Personalized neural Language Model,[0],[0]
"The incentive is that character-level neural language models benefit from a finer granularity for predictions but they lack the semantic understanding words-level models provide, and vice versa.",3.1 Personalized neural Language Model,[0],[0]
"Therefore, they encode text sequences using one-hot encoding of characters, character embedding and pre-trained word embedding (using word2vec (Mikolov et al., 2013)) of the previous word when a space character is encountered.",3.1 Personalized neural Language Model,[0],[0]
"Our preliminary results showed that the character embedding does not bring much to the learning, so we traded it with the context feature vectors below to save some computation time while enriching the model with additional, diverse information.
",3.1 Personalized neural Language Model,[0],[0]
"User representation We make the assumption that the way a user types a query is a function of their actual language/vocabulary, but also a function of their interests.",3.1 Personalized neural Language Model,[0],[0]
"Therefore, a language model could capture these user characteristics to better predict the query, if we feed the learner with the information.",3.1 Personalized neural Language Model,[0],[0]
"Each query qi is a set of words such that qi = {w1, ..., wn}.",3.1 Personalized neural Language Model,[0],[0]
"U is a column matrix and a user u ∈ U is characterized by the union of words in their k past queries, i.e. Qu = ∪ki=1qi.",3.1 Personalized neural Language Model,[0],[0]
"The objective is to reduce, for each user, the vocabulary used in their queries to a vector of a dimensionality d of choice, or Qu → Rd.",3.1 Personalized neural Language Model,[0],[0]
"We chose d = 30, in order to stay in the same computation order of previous work using character embedding (Park and Chiba, 2017).",3.1 Personalized neural Language Model,[0],[0]
"To this end, we adapted the approach PV-DBOW detailed in (Le and Mikolov, 2014).",3.1 Personalized neural Language Model,[0],[0]
"That is, at each training iteration, a random word wi is sampled from Qu.",3.1 Personalized neural Language Model,[0],[0]
"The model is trained by maximizing the probability of predicting the user u given the word wi, i.e.:
1 |U | ∑
u∈U
∑
wi∈Qu log P (u|wi).",3.1 Personalized neural Language Model,[0],[0]
"(1)
The resulting vectors are stored for each user ID and are used as input for the neural net (NN) (see Architecture section).
",3.1 Personalized neural Language Model,[0],[0]
"Time representation As an example, in the background data (see Section 4.1), the query “tv guide” appears 1,682 times and it is vastly represented in evening and nights.",3.1 Personalized neural Language Model,[0],[0]
"For this reason, we propose to integrate time features in the language model.",3.1 Personalized neural Language Model,[0],[0]
"While there has been more elaborated approaches to model it in the past (Shokouhi and Radinsky, 2012), we instead propose a straightforward encoding and leave the rest of the work to the neural net.",3.1 Personalized neural Language Model,[0],[0]
"For each query, we look at the time it was issued, consisting of hour x , minute y and second z, and we derive the following features:
sin
( 2π(3600x+ 60y + z)
86400
) ,
cos
( 2π(3600x+ 60y + z)
86400
) .
",3.1 Personalized neural Language Model,[0],[0]
"(2)
This encoding has the benefit of belonging to [−1, 1], which is a range comparable to the rest of the features.",3.1 Personalized neural Language Model,[0],[0]
"It is also capable to model cyclic data, which is important particularly around boundaries (e.g. considering a query at 11:55PM
and another at 00:05AM).",3.1 Personalized neural Language Model,[0],[0]
"We proceed the same way to encode weekdays and we end up with four time features.
",3.1 Personalized neural Language Model,[0],[0]
Overall architecture An overview of the architecture is proposed in Figure 1.,3.1 Personalized neural Language Model,[0],[0]
"The input of our neural language model is a concatenation of the vectors defined above, for each character and for each query in the training set.",3.1 Personalized neural Language Model,[0],[0]
"We use zeropadding after the “\n” character to keep the sequence length consistent, and the NN learns to recognize it.",3.1 Personalized neural Language Model,[0],[0]
"We feed this input vector into 2 layers of 1024 GRUs2, each followed by a dropout layer (with a dropout rate of 50%) to prevent overfitting.",3.1 Personalized neural Language Model,[0],[0]
Each GRU cell is activated with ReLu(x) =,3.1 Personalized neural Language Model,[0],[0]
x+ and gradients are clipped to a norm of 0.5 to avoid gradient exploding problems.,3.1 Personalized neural Language Model,[0],[0]
"The output of the second dropout layer is fed to a temporal softmax layer, which allows to make predictions at each state.",3.1 Personalized neural Language Model,[0],[0]
"The softmax function returns the probability P (ci|c1, ..., ci−1) of the character ci given the previous characters of the sequence, which is then used to calculate the loss function by comparing it to the next character in the target query.",3.1 Personalized neural Language Model,[0],[0]
"Instead of using the objective denoted in (Park and Chiba, 2017), we minimize the loss L defined as the average cross entropy of this probability with the reference probability P̂ (ci) across all queries, that is
L =
− 1|Q| ∑
q∈Q
|q|−1∑
i=1
P̂ (ci+1)× log P (ci+1|c1, ..., ci).
",3.1 Personalized neural Language Model,[0],[0]
"(3)
Q is the set of queries in the training dataset, |Q| is the total number of queries in the set and |q| is the number of characters in the query q. Convergence stabilizes around 5-10 epochs for the AOL dataset (depending on the model) and 15-20 epochs for the biomedical specialized dataset (see Section 4.1).",3.1 Personalized neural Language Model,[0],[0]
"The straightforward approach for decoding the most likely output sequence — in this case, a suffix given a prefix — is to use a greedy approach.",3.2 Balanced diverse beam search,[0],[0]
"That is, we feed the prefix into the trained NN and pick the most likely output at every step, until the sequence is complete.",3.2 Balanced diverse beam search,[0],[0]
"This approach has a high
2It was reported that using more cells may not help the prediction while hurting computation (Park and Chiba, 2017).
chance to output a locally optimal sequence and a common alternative is to use a beam search instead.",3.2 Balanced diverse beam search,[0],[0]
"We propose to improve the beam search by adding a greedy heuristic within it, in order to account for the diversity in the results.",3.2 Balanced diverse beam search,[0],[0]
"A similar suggestion has been made in (Vijayakumar et al., 2016), and our proposition differs by rebalancing the probabilities after diversity was introduced.",3.2 Balanced diverse beam search,[0],[0]
"In (Vijayakumar et al., 2016), at every step the most likely prediction is not weighted while all others are, by greedily comparing them.",3.2 Balanced diverse beam search,[0],[0]
This approach effectively always prefers the most likely character over all other alternatives at each step.,3.2 Balanced diverse beam search,[0],[0]
"The first result will thus be the same as the local optimum using a greedy approach, which becomes problematic for QAC where order is critical.",3.2 Balanced diverse beam search,[0],[0]
"By rebalancing the probability of the most likely suggestion with the average diversity weight given to other suggestions, we make sure probabilities stay uniform yet suggestions are diverse.",3.2 Balanced diverse beam search,[0],[0]
We use a normalized Levenshtein distance to assess the diversity.,3.2 Balanced diverse beam search,[0],[0]
"The AOL query logs (Pass et al., 2006) are commonly used to evaluate the quality of QAC systems.",4.1 Dataset,[0],[0]
"We rely on a background dataset for the
NN; training and validation datasets for lambdaMART integrations; and a test dataset for evaluations.",4.1 Dataset,[0],[0]
"Some adaptations are done to the AOL background dataset as in (Park and Chiba, 2017), such as removing the queries appearing less than 3 times or longer that 100 characters.",4.1 Dataset,[0],[0]
"For each query in the training, validation and test datasets, we use all possible prefixes starting after the first word as in (Shokouhi, 2013).",4.1 Dataset,[0],[0]
"We use the sets from (Park and Chiba, 2017) available online, enriched with user and time information provided in the original AOL dataset.",4.1 Dataset,[0],[0]
"In addition, we evaluate the systems on a second real-world dataset from a production search engine in the biomedical domain, PubMed (Fiorini et al., 2017; Lu, 2011; Mohan et al., 2018), that was created in the same manner.",4.1 Dataset,[0],[0]
"The biomedical dataset consists of 8,490,317 queries.",4.1 Dataset,[0],[0]
"The sizes of training, validation and test sets are comparable to those used for the AOL dataset.",4.1 Dataset,[0],[0]
Systems are evaluated using the traditional Mean Reciprocal Rank (MRR) metric.,4.2 Evaluation,[0],[0]
This metric assesses the quality of suggestions by identifying the rank of the real query in the suggestions given one of its prefixes.,4.2 Evaluation,[0],[0]
"We also tested PMRR as introduced in (Park and Chiba, 2017) and observed the same trends in results as MRR, so we do not show them due to space limitation.",4.2 Evaluation,[0],[0]
"Given the set of prefixes
P in the test dataset, MRR is defined as follows:
MRR = 1 |Q| ∑
r∈P
1
rp , (4)
where rp represent the rank of the match.",4.2 Evaluation,[0],[0]
Paired t-tests measure the significance of score variations among systems and are reported in the Results section.,4.2 Evaluation,[0],[0]
We also evaluate prediction time as this is an important parameter for building production systems.,4.2 Evaluation,[0],[0]
"The prediction time is averaged over 10 runs on the test set, on the same hardware for all models.",4.2 Evaluation,[0],[0]
We do not evaluate throughput but rather compare the time required by all approaches to process one prefix.,4.2 Evaluation,[0],[0]
"We implemented the method in (Park and Chiba, 2017) and used their best-performing model as a baseline.",4.3 Systems and setups,[0],[0]
"We also compare our results to the standard MPC (Bar-Yossef and Kraus, 2011).",4.3 Systems and setups,[0],[0]
"For our method, we evaluate several incremental versions, starting with NQAC which follows the architecture detailed above but with the word embeddings and the one-hot encoding of characters only.",4.3 Systems and setups,[0],[0]
We add the subscript U when the language model is enriched with user vectors and T when it integrates time features.,4.3 Systems and setups,[0],[0]
We append +D to indicate the use of the diverse beam search to predict queries instead of a standard beam search.,4.3 Systems and setups,[0],[0]
"Finally, we also study the impact of adding MPC and LambdaMART (+MPC, +λMART).",4.3 Systems and setups,[0],[0]
A summary of the results is presented in Table 1.,5 Results,[0],[0]
"Interestingly, our simple NQAC model performs similarly to the state-of-the-art on this dataset, called Neural Query Language Model (NQLM), on all queries.",5 Results,[0],[0]
It is significantly less good for seen queries (-5.6%) and significantly better for unseen queries (+4.2%).,5 Results,[0],[0]
"Although GRUs have less expressive power than LSTMs, their smaller number of parameters to train allowed them to better converge than all LSTM models we tested, including that of (Park and Chiba, 2017).",5 Results,[0],[0]
NQAC also benefits from a significantly better scalability (28% faster than NQLM) and thus seems more appropriate for production systems.,5 Results,[0],[0]
"When we enrich the language model with user information, it becomes better for seen queries (+1.9%) while being about as fast.",5 Results,[0],[0]
"Adding time sensitivity does not yield significant improvements on this
dataset overall, but improves significantly the performance for seen queries (+1.7%).",5 Results,[0],[0]
Relying on the diverse beam search significantly hurts the processing time (39% longer) while not providing significantly better performance.,5 Results,[0],[0]
Our integration of MPC differs from previous studies.,5 Results,[0],[0]
"We noticed that for Web search, MPC performs extremely well and is computationally cheap (0.24 seconds).",5 Results,[0],[0]
"On the other hand, all neural QAC systems are better for unseen queries but struggle to stay under a second of processing time.",5 Results,[0],[0]
"Since identifying if a query has been seen or not is done in constant time, we route the query either to MPC or to NQACUT and we note the overall performance as NQACUT+MPC.",5 Results,[0],[0]
This method provides a significant improvement over NQLM (+6.7%) overall while being faster on average.,5 Results,[0],[0]
"Finally, appending NQACUT ’s results to MPC’s and reranking the list with LambdaMART provides the best results on this dataset, but at the expense of greater computational cost (+60%).
",5 Results,[0],[0]
"While NQACUT+MPC appears clearly as the best compromise between performance and quality for the AOL dataset, the landscape changes drastically on the biomedical dataset and the quality drops significantly for all systems.",5 Results,[0],[0]
"This shows the potential difficulties associated with real-world systems, which particularly occur in specialized domains.",5 Results,[0],[0]
"In this case, the drop in performance is mostly due to the fact that biomedical queries are longer and it becomes more difficult for models to predict the entire query accurately only with the first keywords.",5 Results,[0],[0]
"While the generated queries make sense and are relevant candidates, the chance for generative models to predict the exact target query diminishes as the target query is longer because of combinatorial explosion.",5 Results,[0],[0]
"This is even more true when the target queries are diverse as in specialized domains (Islamaj Dogan et al., 2009; Névéol et al., 2011).",5 Results,[0],[0]
"For example, for the prefix “breast cancer”, there are 1169 diverse suffixes in a single day of logs used for training.",5 Results,[0],[0]
"These include “local recurrence”, “nodular prognosis”, “hormone receptor”, “circulating cells”, “family history”, “chromosome 4p16” or “herceptin review”, to cite only a few.",5 Results,[0],[0]
"Hence, while the model predicts plausible queries, it is a lot more difficult to predict the one the user intended.",5 Results,[0],[0]
"The target query length also has an impact on prediction time, as roughly twice the time is needed for Web searches.",5 Results,[0],[0]
"MPC is the exception, however, it per-
forms poorly even on seen queries (0.165).",5 Results,[0],[0]
This observation suggests that more elaborate models are specifically needed for specialized domains.,5 Results,[0],[0]
"On this dataset, NQAC does not perform as well as NQLM",5 Results,[0],[0]
and it seems this time that the higher number of parameters in NQLM is more appropriate for the task.,5 Results,[0],[0]
"Still, user information helps significantly for seen queries (+23%), probably because some users frequently check the same queries to keep up-to-date.",5 Results,[0],[0]
Time sensitivity seems to help significantly unseen queries (+21%) while significantly hurting the quality for seen queries (-47%).,5 Results,[0],[0]
Diversity is significantly helpful on this dataset (+19%) and provides a balance in performance for both seen and unseen queries.,5 Results,[0],[0]
"NQACUT+MPC yields the best overall MRR score for this dataset, and LambdaMART is unable to learn how to rerank the suggestions, thus decreasing the score.",5 Results,[0],[0]
"From these results, we draw several conclusions.",5 Results,[0],[0]
"First, MPC performs very well on seen queries for Web searches and it should be used on them.",5 Results,[0],[0]
"For unseen queries, the NQACUT model we propose achieves a sub-second state-of-the-art performance.",5 Results,[0],[0]
"Second, it is clear that the field of application will affect many of the decisions when designing a QAC system.",5 Results,[0],[0]
"On a specialized domain, the task is more challenging: fast approaches like MPC perform too poorly while more elaborate approaches do not meet production requirements.",5 Results,[0],[0]
"NQACU performs best on seen queries, NQACUT on unseen queries.",5 Results,[0],[0]
"Finally, NQACUT+D provides an equilibrium between the two at a greater computational cost.",5 Results,[0],[0]
Its overall MRR is similar to that of NQACUT+MPC but it is less redundant (see Table 2).,5 Results,[0],[0]
"Particularly, the system seems not to be limited anymore by the higher probability associ-
ated with shorter suggestions (e.g. “www google”, a form of “www google com”), thus bringing more diversity.",5 Results,[0],[0]
This aspect can be more useful for specialized domains where the range of possible queries is broader.,5 Results,[0],[0]
"Finally, we found that a lot more data was needed for the biomedical domain than for general Web search.",5 Results,[0],[0]
"After about a million queries, NQAC suggests meaningful and plausible queries for both datasets.",5 Results,[0],[0]
"However, for the biomedical dataset, the loss needs more epochs to stabilize than for the AOL dataset, mainly due to the combinatorial explosion mentioned above.",5 Results,[0],[0]
"To the best of our knowledge, we proposed the first neural language model that integrates user information and time sensitivity for query auto completion with a focus on scalability for real-world systems.",6 Conclusions and future work,[0],[0]
Personalization is provided through pretrained user vectors based on their past queries.,6 Conclusions and future work,[0],[0]
"By incorporating this information and by adapting the architecture, we were able to achieve stateof-the-art performance in neural query auto completion without relying on re-ranking, making this approach significantly more scalable in practice.
",6 Conclusions and future work,[0],[0]
"We studied multiple variants, their benefits and drawbacks for various use cases.",6 Conclusions and future work,[0],[0]
"We also demonstrate the utility of this method for specialized domains such as biomedicine, where the query diversity and vocabulary are broader and MPC fails to provide the same performance as in Web search.",6 Conclusions and future work,[0],[0]
We also found that user information and diversity improve the performance significantly more than for Web search engines.,6 Conclusions and future work,[0],[0]
"To allow readers to easily reproduce, evaluate and improve our models, we provide all the code on a public repository.",6 Conclusions and future work,[0],[0]
"The handling of time-sensitivity may benefit from a more elaborate integration, for example sessionbased rather than absolute time.",6 Conclusions and future work,[0],[0]
"Also, we evaluated our approaches on a general search setup for both datasets, while searches in the biomedical domain commonly contain fields (i.e. authors, title, abstract, etc.) which adds to the difficulty.",6 Conclusions and future work,[0],[0]
"The choice of a diversity metric is also important and could be faster or more efficient (e.g., using word embeddings to diversify the semantics of the suggestions).",6 Conclusions and future work,[0],[0]
These limitations warrant further work and we leave them as perspectives.,6 Conclusions and future work,[0],[0]
"This research was supported by the Intramural Research Program of the NIH, National Library of Medicine.",Acknowledgement,[0],[0]
"Query auto completion (QAC) systems are a standard part of search engines in industry, helping users formulate their query.",abstractText,[0],[0]
"Such systems update their suggestions after the user types each character, predicting the user’s intent using various signals — one of the most common being popularity.",abstractText,[0],[0]
"Recently, deep learning approaches have been proposed for the QAC task, to specifically address the main limitation of previous popularity-based methods: the inability to predict unseen queries.",abstractText,[0],[0]
"In this work we improve previous methods based on neural language modeling, with the goal of building an end-to-end system.",abstractText,[0],[0]
We particularly focus on using real-world data by integrating user information for personalized suggestions when possible.,abstractText,[0],[0]
We also make use of time information and study how to increase diversity in the suggestions while studying the impact on scalability.,abstractText,[0],[0]
"Our empirical results demonstrate a marked improvement on two separate datasets over previous best methods in both accuracy and scalability, making a step towards neural query auto-completion in production search engines.",abstractText,[0],[0]
Personalized neural language models for real-world query auto completion,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 706–711 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
706",text,[0],[0]
"Contextual, or ‘data-to-text’ natural language generation is one of the core tasks in natural language processing and has a considerable impact on various fields (Gatt and Krahmer, 2017).",1 Introduction,[0],[0]
"Within the field of recommender systems, a promising application is to estimate (or generate) personalized reviews that a user would write about a product, i.e., to discover their nuanced opinions about each of its individual aspects.",1 Introduction,[0],[0]
"A successful model could work (for instance) as (a) a highly-nuanced recommender system that tells users their likely reaction to a product in the form of text fragments; (b) a writing tool that helps users ‘brainstorm’ the review-writing process; or (c) a querying system that facilitates personalized natural lan-
guage queries (i.e., to find items about which a user would be most likely to write a particular phrase).",1 Introduction,[0],[0]
"Some recent works have explored the review generation task and shown success in generating cohesive reviews (Dong et al., 2017; Ni et al., 2017; Zang and Wan, 2017).",1 Introduction,[0],[0]
"Most of these works treat the user and item identity as input; we seek a system with more nuance and more precision by allowing users to ‘guide’ the model via short phrases, or auxiliary data such as item specifications.",1 Introduction,[0],[0]
"For example, a review writing assistant might allow users to write short phrases and expand these key points into a plausible review.
",1 Introduction,[0],[0]
"Review text has been widely studied in traditional tasks such as aspect extraction (Mukherjee and Liu, 2012; He et al., 2017), extraction of sentiment lexicons (Zhang et al., 2014), and aspectaware sentiment analysis (Wang et al., 2016; McAuley et al., 2012).",1 Introduction,[0],[0]
These works are related to review generation since they can provide prior knowledge to supervise the generative process.,1 Introduction,[0],[0]
"We are interested in exploring how such knowledge (e.g. extracted aspects) can be used in the review generation task.
",1 Introduction,[0],[0]
"In this paper, we focus on designing a review generation model that is able to leverage both user and item information as well as auxiliary, textual input and aspect-aware knowledge.",1 Introduction,[0],[0]
"Specifically, we study the task of expanding short phrases into complete, coherent reviews that accurately reflect the opinions and knowledge learned from those phrases.
",1 Introduction,[0],[0]
"These short phrases could include snippets provided by the user, or manifest aspects about the items themselves (e.g. brand words, technical specifications, etc.).",1 Introduction,[0],[0]
"We propose an encoderdecoder framework that takes into consideration three encoders (a sequence encoder, an attribute encoder, and an aspect encoder), and one decoder.",1 Introduction,[0],[0]
"The sequence encoder uses a gated recurrent unit
(GRU) network to encode text information; the attribute encoder learns a latent representation of user and item identity; finally, the aspect encoder finds an aspect-aware representation of users and items, which reflects user-aspect preferences and item-aspect relationships.",1 Introduction,[0],[0]
The aspect-aware representation is helpful to discover what each user is likely to discuss about each item.,1 Introduction,[0],[0]
"Finally, the output of these encoders is passed to the sequence decoder with an attention fusion layer.",1 Introduction,[0],[0]
The decoder attends on the encoded information and biases the model to generate words that are consistent with the input phrases and words belonging to the most relevant aspects.,1 Introduction,[0],[0]
"Review generation belongs to a large body of work on data-to-text natural language generation (Gatt and Krahmer, 2017), which has applications including summarization (See et al., 2017), image captioning (Vinyals et al., 2015), and dialogue response generation (Xing et al., 2017; Li et al., 2016; Ghosh et al., 2017), among others.",2 Related Work,[0],[0]
"Among these, review generation is characterized by the need to generate long sequences and estimate high-order interactions between users and items.
",2 Related Work,[0],[0]
Several approaches have been recently proposed to tackle these problems.,2 Related Work,[0],[0]
Dong et al. (2017) proposed an attribute-to-sequence (Attr2Seq) method to encode user and item identities as well as rating information with a multi-layer perceptron and a decoder then generates reviews conditioned on this information.,2 Related Work,[0],[0]
"They also used an attention mechanism to strengthen the alignment between
output and input attributes.",2 Related Work,[0],[0]
Ni et al. (2017) trained a collaborative-filtering generative concatenative network to jointly learn the tasks of review generation and item recommendation.,2 Related Work,[0],[0]
"Zang and Wan (2017) proposed a hierarchical structure to generate long reviews; they assume each sentence is associated with an aspect score, and learn the attention between aspect scores and sentences during training.",2 Related Work,[0],[0]
"Our approach differs from these mainly in our goal of incorporating auxiliary textual information (short phrases, product specifications, etc.) into the generative process, which facilitates the generation of higher-fidelity reviews.
",2 Related Work,[0],[0]
"Another line of work related to review generation is aspect extraction and opinion mining (Park et al., 2015; Qiu et al., 2017; He et al., 2017; Chen et al., 2014).",2 Related Work,[0],[0]
"In this paper, we argue that the extra aspect (opinion) information extracted using these previous works can effectively improve the quality of generated reviews.",2 Related Work,[0],[0]
We propose a simple but effective way to combine aspect information into the generative model.,2 Related Work,[0],[0]
We describe the review generation task as follows.,3 Approach,[0],[0]
"Given a user u, item i, several short phrases {d1, d2, ..., dM}, and a group of extracted aspects {A1, A2, ..., Ak}, our goal is to generate a review (w1, w2, ..., wT) that maximizes the probability P (w1:T|u, i, d1:M).",3 Approach,[0],[0]
"To solve this task, we propose a method called ExpansionNet which contains two parts: 1) three encoders to leverage the input phrases and aspect information; and 2) a decoder with an attention fusion layer to generate sequences and align the generation with the input
sources.",3 Approach,[0],[0]
The model structure is shown in Figure 1.,3 Approach,[0],[0]
"Our sequence encoder is a two-layer bi-directional GRU, as is commonly used in sequence-tosequence (Seq2Seq) models (Cho et al., 2014).","3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
"Input phrases first pass a word embedding layer, then go through the GRU one-by-one and finally yield a sequence of hidden states {e1, e2..., eL}.","3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
"In the case of multiple phrases, these share the same sequence encoder and have different lengths L. To simplify notation, we only consider one input phrase in this section.
","3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
The attribute encoder and aspect encoder both consist of two embedding layers and a projection layer.,"3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
"For the attribute encoder, we define two general embedding layers Eu ∈ R|U|×m and Ei ∈ R|I|×m to obtain the attribute latent factors γu and γi; for the aspect encoder, we use two aspect-aware embedding layers E ′","3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
u ∈ R|U|×k,"3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
and E ′,"3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
i ∈ R|I|×k to obtain aspect-aware latent factors βu,"3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
and βi.,"3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
"Here |U|, |I|, m and k are the number of users, number of items, the dimension of attributes, and the number of aspects, respectively.","3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
"After the embedding layers, the attribute and aspect-aware latent factors are concatenated and fed into a projection layer with tanh activation.","3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
"The outputs are calculated as:
γu = Eu(u), γi = Ei(i) (1) βu = E ′ u(u), βi = E ′ i(i) (2)
u = tanh(Wu[γu; γi] + bu) (3)
v = tanh(Wv[βu;βi] + bv) (4)
where Wu ∈","3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
"Rn×2m,","3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
"bu ∈ Rn, Wv ∈ Rn×2k, bv ∈ Rn are learnable parameters and n is the dimensionality of the hidden units in the decoder.","3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
The decoder is a two-layer GRU that predicts the target words given the start token.,3.2 Decoder with attention fusion layer,[0],[0]
The hidden state of the decoder is initialized using the sum of the three encoders’ outputs.,3.2 Decoder with attention fusion layer,[0],[0]
The hidden state at time-step t is updated via the GRU unit based on the previous hidden state and the input word.,3.2 Decoder with attention fusion layer,[0],[0]
"Specifically:
h0 = eL + u+ v (5)
ht = GRU(wt,ht−1), (6)
where h0 ∈ Rn is the decoder’s initial hidden state and ht ∈",3.2 Decoder with attention fusion layer,[0],[0]
"Rn is the hidden state at time-step t.
To fully exploit the encoder-side information, we apply an attention fusion layer to summarize the output of each encoder and jointly determine the final word distribution.",3.2 Decoder with attention fusion layer,[0],[0]
"For the sequence encoder, the attention vector is defined as in many other applications (Bahdanau et al., 2014; Luong et al., 2015):
a1t = L∑ j=1 α1tjej (7)
α1tj = exp(tanh(v 1 α >",3.2 Decoder with attention fusion layer,[0],[0]
"(W 1α[ej ;ht] + b 1 α)))/Z,
(8)
where a1t ∈",3.2 Decoder with attention fusion layer,[0],[0]
"Rn is the attention vector on the sequence encoder at time-step t, α1tj is the attention score over the encoder hidden state ej and decoder hidden state ht, and Z is a normalization term.
",3.2 Decoder with attention fusion layer,[0],[0]
"For the attribute encoder, the attention vector is calculated as:
a2t = ∑ j∈u,i α2tjγj (9)
α2tj = exp(tanh(v 2 α >",3.2 Decoder with attention fusion layer,[0],[0]
"(W 2α[γj ;ht] + b 2 α)))/Z,
(10)
where a2t ∈",3.2 Decoder with attention fusion layer,[0],[0]
"Rn is the attention vector on the attribute encoder, and α2tj is the attention score between the attribute latent factor γj and decoder hidden state ht.
",3.2 Decoder with attention fusion layer,[0],[0]
"Inspired by the copy mechanism (Gu et al., 2016; See et al., 2017), we design an attention vector that estimates the probability that each aspect will be discussed in the next time-step:
sui =Ws[βu;βi] + bs (11) a3t = tanh(W 3 α[sui; et;ht] + b 3 α), (12)
where sui ∈ Rk is the aspect importance considering the interaction between u and i, et is the decoder input after embedding layer at time-step t, and a3t ∈",3.2 Decoder with attention fusion layer,[0],[0]
"Rk is a probability vector to bias each aspect at time-step t. Finally, the first two attention vectors are concatenated with the decoder hidden state at time-step t and projected to obtain the output word distribution Pv.",3.2 Decoder with attention fusion layer,[0],[0]
The attention scores from the aspect encoder are then directly added to the aspect words in the final word distribution.,3.2 Decoder with attention fusion layer,[0],[0]
"The output probability for word w at time-step t is given by:
Pv(wt) =",3.2 Decoder with attention fusion layer,[0],[0]
tanh(W,3.2 Decoder with attention fusion layer,[0],[0]
[ht;a 1 t ;a 2 t ] + b),3.2 Decoder with attention fusion layer,[0],[0]
"(13)
P (wt) = Pv(wt) + a 3 t [k] · 1wt∈Ak , (14)
where wt is the target word at time-step t, a3t",3.2 Decoder with attention fusion layer,[0],[0]
"[k] is the probability that aspect k will be discussed at time-step t, Ak represents all words belonging to aspect k and 1wt∈Ak is a binary variable indicating whether wt belongs to aspect k.
During inference, we use greedy decoding by choosing the word with maximum probability, denoted as yt = argmaxwtsoftmax(P (wt)).",3.2 Decoder with attention fusion layer,[0],[0]
Decoding finishes when an end token is encountered.,3.2 Decoder with attention fusion layer,[0],[0]
"We consider a real world dataset from Amazon Electronics (McAuley et al., 2015) to evaluate our model.",4 Experiments,[0],[0]
"We convert all text into lowercase, add start and end tokens to each review, and perform tokenization using NLTK.1",4 Experiments,[0],[0]
"We discard reviews with length greater than 100 tokens and consider a vocabulary of 30,000 tokens.",4 Experiments,[0],[0]
"After preprocessing, the dataset contains 182,850 users, 59,043 items, and 992,172 reviews (sparsity 99.993%), which is much sparser than the datasets used in previous works (Dong et al., 2017; Ni et al., 2017).",4 Experiments,[0],[0]
"On average, each review contains 49.32 tokens as well as a short-text summary of 4.52 tokens.",4 Experiments,[0],[0]
"In our experiments, the basic ExpansionNet uses these summaries as input phrases.",4 Experiments,[0],[0]
"We split the dataset into training (80%), validation (10%) and test sets (10%).",4 Experiments,[0],[0]
All results are reported on the test set.,4 Experiments,[0],[0]
"We use the method2 in (He et al., 2017) to extract 15 aspects and consider the top 100 words from each aspect.",4.1 Aspect Extraction,[0],[0]
Table 2 shows 10 inferred aspects and representative words (inferred aspects are manually labeled).,4.1 Aspect Extraction,[0],[0]
"ExpansionNet calculates an attention score based on the user and item aspect-aware representation, then determines how much these representative words are biased in the output word distribution.
",4.1 Aspect Extraction,[0],[0]
1 https://www.nltk.org/ 2 https://github.com/ruidan/ Unsupervised-Aspect-Extraction,4.1 Aspect Extraction,[0],[0]
We use PyTorch3 to implement our model.4 Parameter settings are shown in Table 1.,4.2 Experiment Details,[0],[0]
"For the attribute encoder and aspect encoder, we set the dimensionality to 64 and 15 respectively.",4.2 Experiment Details,[0],[0]
"For both the sequence encoder and decoder, we use a 2- layer GRU with hidden size 512.",4.2 Experiment Details,[0],[0]
We also add dropout layers before and after the GRUs.,4.2 Experiment Details,[0],[0]
The dropout rate is set to 0.1.,4.2 Experiment Details,[0],[0]
"During training, the input sequences of the same source (e.g. review, summary) inside each batch are padded to the same length.",4.2 Experiment Details,[0],[0]
"We evaluate the model on six automatic metrics (Table 3): Perplexity, BLEU-1/BLEU-4, ROUGEL and Distinct-1/2 (percentage of distinct unigrams and bi-grams) (Li et al., 2016).",4.3 Performance Evaluation,[0],[0]
"We compare
3 http://pytorch.org/docs/master/index.html 4 https://github.com/nijianmo/textExpansion
against three baselines: Rand (randomly choose a review from the training set), GRU-LM (the GRU decoder works alone as a language model) and a state-of-the-art model Attr2Seq that only considers user and item attribute (Dong et al., 2017).",4.3 Performance Evaluation,[0],[0]
"ExpansionNet (with summary, item title, attribute and aspect as input) achieves significant improvements over Attr2Seq on all metrics.",4.3 Performance Evaluation,[0],[0]
"As we add more input information, the model continues to obtain better results, except for the ROUGE-L metric.",4.3 Performance Evaluation,[0],[0]
"This proves that our model can effectively learn from short input phrases and aspect information and improve the correctness and diversity of generated results.
",4.3 Performance Evaluation,[0],[0]
Figure 2 presents a sample generation result.,4.3 Performance Evaluation,[0],[0]
"ExpansionNet captures fine-grained item information (e.g. that the item is a tablet), which Attr2Seq fails to recognize.",4.3 Performance Evaluation,[0],[0]
"Moreover, given a phrase like “easy to use” in the summary, ExpansionNet generates reviews containing the same text.",4.3 Performance Evaluation,[0],[0]
This demonstrates the possibility of using our model in an assistive review generation scenario.,4.3 Performance Evaluation,[0],[0]
"Finally, given extra aspect information, the model successfully estimates that the screen would be an important aspect (i.e., for the current user and item); it generates phrases such as “screen is very respon-
sive” about the aspect “screen” which is also covered in the real (ground-truth) review (“display is beautiful”).
",4.3 Performance Evaluation,[0],[0]
We are also interested in seeing how the aspectaware representation can find related aspects and bias the generation to discuss more about those aspects.,4.3 Performance Evaluation,[0],[0]
We analyze the average number of aspects in real and generated reviews and show on average how many aspects in real reviews are covered in generated reviews.,4.3 Performance Evaluation,[0],[0]
We consider a review as covering an aspect if any of the aspect’s representative words exists in the review.,4.3 Performance Evaluation,[0],[0]
"As shown in Table 4, Attr2Seq tends to cover more aspects in generation, many of which are not discussed in real reviews.",4.3 Performance Evaluation,[0],[0]
"On the other hand, ExpansionNet better captures the distribution of aspects that are discussed in real reviews.",4.3 Performance Evaluation,[0],[0]
"In this paper, we focus on the problem of building assistive systems that can help users to write reviews.",abstractText,[0],[0]
"We cast this problem using an encoder-decoder framework that generates personalized reviews by expanding short phrases (e.g. review summaries, product titles) provided as input to the system.",abstractText,[0],[0]
We incorporate aspect-level information via an aspect encoder that learns ‘aspect-aware’ user and item representations.,abstractText,[0],[0]
An attention fusion layer is applied to control generation by attending on the outputs of multiple encoders.,abstractText,[0],[0]
Experimental results show that our model is capable of generating coherent and diverse reviews that expand the contents of input phrases.,abstractText,[0],[0]
"In addition, the learned aspectaware representations discover those aspects that users are more inclined to discuss and bias the generated text toward their personalized aspect preferences.",abstractText,[0],[0]
Personalized Review Generation by Expanding Phrases and Attending on Aspect-Aware Representations,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2204–2213 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
2204",text,[0],[0]
"Despite much recent success in natural language processing and dialogue research, communication between a human and a machine is still in its infancy.",1 Introduction,[0],[0]
It is only recently that neural models have had sufficient capacity and access to sufficiently large datasets that they appear to generate meaningful responses in a chit-chat setting.,1 Introduction,[0],[0]
"Still, conversing with such generic chit-chat models for even a short amount of time quickly exposes their weaknesses (Serban et al., 2016; Vinyals and Le, 2015).
",1 Introduction,[0],[0]
"Common issues with chit-chat models include: (i) the lack of a consistent personality (Li et al., 2016a) as they are typically trained over many dialogs each with different speakers, (ii) the lack of an explicit long-term memory as they are typically trained to produce an utterance given only the recent dialogue history (Vinyals and Le, 2015);
1Work done while at Facebook AI Research.
and (iii) a tendency to produce non-specific answers like “I don’t know” (Li et al., 2015).",1 Introduction,[0],[0]
Those three problems combine to produce an unsatisfying overall experience for a human to engage with.,1 Introduction,[0],[0]
"We believe some of those problems are due to there being no good publicly available dataset for general chit-chat.
",1 Introduction,[0],[0]
"Because of the low quality of current conversational models, and because of the difficulty in evaluating these models, chit-chat is often ignored as an end-application.",1 Introduction,[0],[0]
"Instead, the research community has focused on task-oriented communication, such as airline or restaurant booking (Bordes and Weston, 2016), or else single-turn information seeking, i.e. question answering (Rajpurkar et al., 2016).",1 Introduction,[0],[0]
"Despite the success of the latter, simpler, domain, it is well-known that a large quantity of human dialogue centers on socialization, personal interests and chit-chat (Dunbar et al., 1997).",1 Introduction,[0],[0]
"For example, less than 5% of posts on Twitter are questions, whereas around 80% are about personal emotional state, thoughts or activities, authored by so called “Meformers” (Naaman et al., 2010).
",1 Introduction,[0],[0]
"In this work we make a step towards more engaging chit-chat dialogue agents by endowing them with a configurable, but persistent persona, encoded by multiple sentences of textual description, termed a profile.",1 Introduction,[0],[0]
"This profile can be stored in a memory-augmented neural network and then used to produce more personal, specific, consistent and engaging responses than a persona-free model, thus alleviating some of the common issues in chit-chat models.",1 Introduction,[0],[0]
"Using the same mechanism, any existing information about the persona of the dialogue partner can also be used in the same way.",1 Introduction,[0],[0]
"Our models are thus trained to both ask and answer questions about personal topics, and the resulting dialogue can be used to build a model of the persona of the speaking partner.
",1 Introduction,[0],[0]
"To support the training of such models, we
present the PERSONA-CHAT dataset, a new dialogue dataset consisting of 164,356 utterances between crowdworkers who were randomly paired and each asked to act the part of a given provided persona (randomly assigned, and created by another set of crowdworkers).",1 Introduction,[0],[0]
The paired workers were asked to chat naturally and to get to know each other during the conversation.,1 Introduction,[0],[0]
"This produces interesting and engaging conversations that our agents can try to learn to mimic.
",1 Introduction,[0],[0]
"Studying the next utterance prediction task during dialogue, we compare a range of models: both generative and ranking models, including Seq2Seq models and Memory Networks (Sukhbaatar et al., 2015) as well as other standard retrieval baselines.",1 Introduction,[0],[0]
We show experimentally that in either the generative or ranking case conditioning the agent with persona information gives improved prediction of the next dialogue utterance.,1 Introduction,[0],[0]
"The PERSONA-CHAT dataset is designed to facilitate research into alleviating some of the issues that traditional chitchat models face, and with the aim of making such models more consistent and engaging, by endowing them with a persona.",1 Introduction,[0],[0]
"By comparing against chit-chat models built using the OpenSubtitles and Twitter datasets, human evaluations show that our dataset provides more engaging models, that are simultaneously capable of being fluent and consistent via conditioning on a persistent, recognizable profile.",1 Introduction,[0],[0]
"Traditional dialogue systems consist of building blocks, such as dialogue state tracking components and response generators, and have typically been applied to tasks with labeled internal dialogue state and precisely defined user intent (i.e., goal-oriented dialogue), see e.g. (Young, 2000).",2 Related Work,[0],[0]
"The most successful goal-oriented dialogue systems model conversation as partially observable Markov decision processes (POMDPs) (Young et al., 2013).",2 Related Work,[0],[0]
All those methods typically do not consider the chit-chat setting and are more concerned with achieving functional goals (e.g. booking an airline flight) than displaying a personality.,2 Related Work,[0],[0]
"In particular, many of the tasks and datasets available are constrained to narrow domains (Serban et al., 2015).
",2 Related Work,[0],[0]
"Non-goal driven dialogue systems go back to Weizenbaum’s famous program ELIZA (Weizenbaum, 1966), and hand-coded systems have con-
tinued to be used in applications to this day.",2 Related Work,[0],[0]
"For example, modern solutions that build an openended dialogue system to the Alexa challenge combine hand-coded and machine-learned elements (Serban et al., 2017a).",2 Related Work,[0],[0]
"Amongst the simplest of statistical systems that can be used in this domain, that are based on data rather than handcoding, are information retrieval models (Sordoni et al., 2015), which retrieve and rank responses based on their matching score with the recent dialogue history.",2 Related Work,[0],[0]
"We use IR systems as a baseline in this work.
",2 Related Work,[0],[0]
End-to-end neural approaches are a class of models which have seen growing recent interest.,2 Related Work,[0],[0]
"A popular class of methods are generative recurrent systems like seq2seq applied to dialogue (Sutskever et al., 2014; Vinyals and Le, 2015; Sordoni et al., 2015; Li et al., 2016b; Serban et al., 2017b).",2 Related Work,[0],[0]
"Rooted in language modeling, they are able to produce syntactically coherent novel responses, but their memory-free approach means they lack long-term coherence and a persistent personality, as discussed before.",2 Related Work,[0],[0]
"A promising direction, that is still in its infancy, to fix this issue is to use a memory-augmented network instead (Sukhbaatar et al., 2015; Dodge et al., 2015) by providing or learning appropriate memories.
",2 Related Work,[0],[0]
Serban et al. (2015) list available corpora for training dialogue systems.,2 Related Work,[0],[0]
"Perhaps the most relevant to learning chit-chat models are ones based on movie scripts such as OpenSubtitles and Cornell Movie-Dialogue Corpus, and dialogue from web platforms such as Reddit and Twitter, all of which have been used for training neural approaches (Vinyals and Le, 2015; Dodge et al., 2015; Li et al., 2016b; Serban et al., 2017b).",2 Related Work,[0],[0]
Naively training on these datasets leads to models with the lack of a consistent personality as they will learn a model averaged over many different speakers.,2 Related Work,[0],[0]
"Moreover, the data does little to encourage the model to engage in understanding and maintaining knowledge of the dialogue partner’s personality and topic interests.
",2 Related Work,[0],[0]
"According to Serban et al. (2015)’s survey, personalization of dialogue systems is “an important task, which so far has not received much attention”.",2 Related Work,[0],[0]
"In the case of goal-oriented dialogue some work has focused on the agent being aware of the human’s profile and adjusting the dialogue accordingly, but without a personality to the agent itself (Lucas et al., 2009; Joshi et al., 2017).",2 Related Work,[0],[0]
"For
the chit-chat setting, the most relevant work is (Li et al., 2016a).",2 Related Work,[0],[0]
"For each user in the Twitter corpus, personas were captured via distributed embeddings (one per speaker) to encapsulate individual characteristics such as background information and speaking style, and they then showed using those vectors improved the output of their seq2seq model for the same speaker.",2 Related Work,[0],[0]
"Their work does not focus on attempting to engage the other speaker by getting to know them, as we do here.",2 Related Work,[0],[0]
"For that reason, our focus is on explicit profile information, not hard-to-interpret latent variables.
",2 Related Work,[0],[0]
3,2 Related Work,[0],[0]
"The PERSONA-CHAT Dataset
The aim of this work is to facilitate more engaging and more personal chit-chat dialogue.",2 Related Work,[0],[0]
"The PERSONA-CHAT dataset is a crowd-sourced dataset, collected via Amazon Mechanical Turk, where each of the pair of speakers condition their dialogue on a given profile, which is provided.
",2 Related Work,[0],[0]
"The data collection consists of three stages: (i) Personas: we crowdsource a set of 1155 possible personas, each consisting of at least 5 profile sentences, setting aside 100 never seen before personas for validation, and 100 for test.
",2 Related Work,[0],[0]
"(ii) Revised personas: to avoid modeling that takes advantage of trivial word overlap, we crowdsource additional rewritten sets of the same 1155 personas, with related sentences that are rephrases, generalizations or specializations, rendering the task much more challenging.
",2 Related Work,[0],[0]
"(iii) Persona chat: we pair two Turkers and assign them each a random (original) persona from the pool, and ask them to chat.",2 Related Work,[0],[0]
"This resulted in a dataset of 164,356 utterances over 10,981 dialogs, 15,705 utterances (968 dialogs) of which are set aside for validation, and 15,119 utterances (1000 dialogs) for test.
",2 Related Work,[0],[0]
"The final dataset and its corresponding data collection source code, as well as models trained on the data, are all available open source in ParlAI2.
",2 Related Work,[0],[0]
"In the following, we describe each data collection stage and the resulting tasks in more detail.",2 Related Work,[0],[0]
"We asked the crowdsourced workers to create a character (persona) description using 5 sentences, providing them only a single example:
2https://github.com/facebookresearch/ ParlAI/tree/master/projects/personachat
“I am a vegetarian.",3.1 Personas,[0],[0]
I like swimming.,3.1 Personas,[0],[0]
My father used to work for Ford.,3.1 Personas,[0],[0]
My favorite band is Maroon5.,3.1 Personas,[0],[0]
"I got a new job last month, which is about advertising design.”
",3.1 Personas,[0],[0]
"Our aim was to create profiles that are natural and descriptive, and contain typical topics of human interest that the speaker can bring up in conversation.",3.1 Personas,[0],[0]
"Because the personas are not the real profiles of the Turkers, the dataset does not contain personal information (and they are told specifically not to use any).",3.1 Personas,[0],[0]
"We asked the workers to make each sentence short, with a maximum of 15 words per sentence.",3.1 Personas,[0],[0]
"This is advantageous both for humans and machines: if they are too long, crowdsourced workers are likely to lose interest, and for machines the task could become more difficult.
",3.1 Personas,[0],[0]
Some examples of the personas collected are given in Table 1 (left).,3.1 Personas,[0],[0]
"A difficulty when constructing dialogue datasets, or text datasets in general, is that in order to encourage research progress, the task must be carefully constructed so that is neither too easy nor too difficult for the current technology (Voorhees et al., 1999).",3.2 Revised Personas,[0],[0]
"One issue with conditioning on textual personas is that there is a danger that humans will, even if asked not to, unwittingly repeat profile information either verbatim or with significant word overlap.",3.2 Revised Personas,[0],[0]
"This may make any subsequent machine learning tasks less challenging, and the solutions will not generalize to more difficult tasks.",3.2 Revised Personas,[0],[0]
"This has been a problem in some recent datasets: for example, the dataset curation technique used for the well-known SQuAD dataset suffers from this word overlap problem to a certain extent (Chen et al., 2017).
",3.2 Revised Personas,[0],[0]
"To alleviate this problem, we presented the original personas we collected to a new set of crowdworkers and asked them to rewrite the sentences so that a new sentence is about “a related characteristic that the same person may have”, hence the revisions could be rephrases, generalizations or specializations.",3.2 Revised Personas,[0],[0]
"For example “I like basketball” can be revised as “I am a big fan of Michael Jordan” not because they mean the same thing but because the same persona could contain both.
",3.2 Revised Personas,[0],[0]
"In the revision task, workers are instructed not to trivially rephrase the sentence by copying the original words.",3.2 Revised Personas,[0],[0]
"However, during the entry stage if a non-stop word is copied we issue a warning,
and ask them to rephrase, guaranteeing that the instructions are followed.",3.2 Revised Personas,[0],[0]
"For example, “My father worked for Ford.”",3.2 Revised Personas,[0],[0]
"can be revised to “My dad worked in the car industry”, but not “My dad was employed by Ford.”",3.2 Revised Personas,[0],[0]
"due to word overlap.
",3.2 Revised Personas,[0],[0]
Some examples of the revised personas collected are given in Table 1 (right).,3.2 Revised Personas,[0],[0]
"After collecting personas, we then collected the dialogues themselves, conditioned on the personas.",3.3 Persona Chat,[0],[0]
"For each dialogue, we paired two random crowdworkers, and gave them the instruction that they will chit-chat with another worker, while playing the part of a given character.",3.3 Persona Chat,[0],[0]
"We then provide them with a randomly chosen persona from our pool, different to their partners.",3.3 Persona Chat,[0],[0]
"The instructions are on
purpose quite terse and simply ask them to “chat with the other person naturally and try to get to know each other”.",3.3 Persona Chat,[0],[0]
"In an early study we noticed the crowdworkers tending to talk about themselves (their own persona) too much, so we also added the instructions “both ask questions and answer questions of your chat partner” which seemed to help.",3.3 Persona Chat,[0],[0]
We also gave a bonus for high quality dialogs.,3.3 Persona Chat,[0],[0]
"The dialog is turn-based, with a maximum of 15 words per message.",3.3 Persona Chat,[0],[0]
"We again gave instructions to not trivially copy the character descriptions into the messages, but also wrote explicit code sending them an error if they tried to do so, using simple string matching.",3.3 Persona Chat,[0],[0]
We define a minimum dialogue length which is randomly between 6 and 8 turns each for each dialogue.,3.3 Persona Chat,[0],[0]
An example dialogue from the dataset is given in Table 2.,3.3 Persona Chat,[0],[0]
"We focus on the standard dialogue task of predicting the next utterance given the dialogue history, but consider this task both with and without the profile information being given to the learning agent.",3.4 Evaluation,[0],[0]
"Our goal is to enable interesting directions for future research, where chatbots can for instance have personalities, or imputed personas could be used to make dialogue more engaging to the user.
",3.4 Evaluation,[0],[0]
"We consider this in four possible scenarios: conditioning on no persona, your own persona, their persona, or both.",3.4 Evaluation,[0],[0]
"These scenarios can be tried using either the original personas, or the revised ones.",3.4 Evaluation,[0],[0]
"We then evaluate the task using three metrics: (i) the log likelihood of the correct sequence, measured via perplexity, (ii) F1 score, and (iii) next utterance classification loss, following Lowe et al. (2015).",3.4 Evaluation,[0],[0]
"The latter consists of choosing N random distractor responses from other dialogues (in our setting, N=19) and the model selecting the best response among them, resulting in a score of one if the model chooses the correct response, and zero otherwise (called hits@1 in the experiments).",3.4 Evaluation,[0],[0]
We consider two classes of model for next utterance prediction: ranking models and generative models.,4 Models,[0],[0]
Ranking models produce a next utterance by considering any utterance in the training set as a possible candidate reply.,4 Models,[0],[0]
"Generative models generate novel sentences by conditioning on the dialogue history (and possibly, the persona), and then generating the response word-by-word.",4 Models,[0],[0]
"Note one can still evaluate the latter as ranking models by computing the probability of generating a given candidate, and ranking candidates by those scores.",4 Models,[0],[0]
"We first consider two baseline models, an IR baseline (Sordoni et al., 2015) and a supervised embedding model, Starspace (Wu et al., 2017)3.",4.1 Baseline ranking models,[0],[0]
"While there are many IR variants, we adopt the simplest one: find the most similar message in the (training) dataset and output the response from that exchange.",4.1 Baseline ranking models,[0],[0]
Similarity is measured by the tfidf weighted cosine similarity between the bags of words.,4.1 Baseline ranking models,[0],[0]
"Starspace is a recent model that also performs information retrieval but by learning the
3github.com/facebookresearch/StarSpace
similarity between the dialog and the next utterance by optimizing the embeddings directly for that task using the margin ranking loss and k-negative sampling.",4.1 Baseline ranking models,[0],[0]
"The similarity function sim(q, c′) is the cosine similarity of the sum of word embeddings of the query q and candidate c′.",4.1 Baseline ranking models,[0],[0]
"Denoting the dictionary of D word embeddings as W which is a D× d matrix, where Wi indexes the ith word (row), yielding its d-dimensional embedding, it embeds the sequences q and c′.
In both methods, IR and StarSpace, to incorporate the profile we simply concatenate it to the query vector bag of words.",4.1 Baseline ranking models,[0],[0]
"Both the previous models use the profile information by combining it with the dialogue history, which means those models cannot differentiate between the two when deciding on the next utterance.",4.2 Ranking Profile Memory Network,[0],[0]
"In this model we instead use a memory network with the dialogue history as input, which then performs attention over the profile to find relevant lines from the profile to combine with the input, and then finally predicts the next utterance.",4.2 Ranking Profile Memory Network,[0],[0]
"We use the same representation and loss as in the Starspace model, so without the profile, the two models are identical.",4.2 Ranking Profile Memory Network,[0],[0]
"When the profile is available attention is performed by computing the similarity of the input q with the profile sentences pi, computing the softmax, and taking the weighted sum:
q+ = q+ ∑ sipi, si = Softmax(sim(q, pi))",4.2 Ranking Profile Memory Network,[0],[0]
where Softmax(zi) = ezi/ ∑ j e zj .,4.2 Ranking Profile Memory Network,[0],[0]
"One can then rank the candidates c′ using sim(q+, c′).",4.2 Ranking Profile Memory Network,[0],[0]
"One can also perform multiple “hops” of attention over the profile rather than one, as shown here, although that did not bring significant gains in our parameter sweeps.",4.2 Ranking Profile Memory Network,[0],[0]
"The key-value (KV) memory network (Miller et al., 2016) was proposed as an improvement to the memory network by performing attention over keys and outputting the values (instead of the same keys as in the original), which can outperform memory networks dependent on the task and definition of the key-value pairs.",4.3 Key-Value Profile Memory Network,[0],[0]
"Here, we apply this model to dialogue, and consider the keys as dialog histories (from the training set), and the values as the next dialogue utterances, i.e., the replies from the speaking partner.",4.3 Key-Value Profile Memory Network,[0],[0]
"This allows the model
to have a memory of past dialogues that it can directly use to help influence its prediction for the current conversation.",4.3 Key-Value Profile Memory Network,[0],[0]
"The model we choose is identical to the profile memory network just described in the first hop over profiles, while in the second hop, q+ is used to attend over the keys and output a weighted sum of values as before, producing q++.",4.3 Key-Value Profile Memory Network,[0],[0]
"This is then used to rank the candidates c′ using sim(q++, c′) as before.",4.3 Key-Value Profile Memory Network,[0],[0]
As the set of (key-value) pairs is large this would make training very slow.,4.3 Key-Value Profile Memory Network,[0],[0]
In our experiments we simply trained the profile memory network and used the same weights from that model and applied this architecture at test time instead.,4.3 Key-Value Profile Memory Network,[0],[0]
"Training the model directly would presumably give better results, however this heuristic already proved beneficial compared to the original network.",4.3 Key-Value Profile Memory Network,[0],[0]
The input sequence x is encoded by applying het = LSTMenc(xt | het−1).,4.4 Seq2Seq,[0],[0]
"We use GloVe (Pennington et al., 2014) for our word embeddings.",4.4 Seq2Seq,[0],[0]
"The final hidden state, het , is fed into the decoder LSTMdec as the initial state hd0.",4.4 Seq2Seq,[0],[0]
"For each time step t, the decoder then produces the probability of a word j occurring in that place via the softmax, i.e.,
p(yt,j = 1 | yt−1, . . .",4.4 Seq2Seq,[0],[0]
", y1) = exp(wjh d t )",4.4 Seq2Seq,[0],[0]
"∑K
j′=1 exp(wj′h d t ) .
",4.4 Seq2Seq,[0],[0]
The model is trained via negative log likelihood.,4.4 Seq2Seq,[0],[0]
"The basic model can be extended to include persona information, in which case we simply prepend it to the input sequence x, i.e., x = ∀p ∈ P ||",4.4 Seq2Seq,[0],[0]
"x, where || denotes concatenation.",4.4 Seq2Seq,[0],[0]
"For the OpenSubtitles and Twitter datasets trained in Section 5.2 we found training a language model (LM), essentially just the decoder part of this model, worked better and we report that instead.",4.4 Seq2Seq,[0],[0]
"Finally, we introduce a generative model that encodes each of the profile entries as individual memory representations in a memory network.",4.5 Generative Profile Memory Network,[0],[0]
"As before, the dialogue history is encoded via LSTMenc, the final state of which is used as the initial hidden state of the decoder.",4.5 Generative Profile Memory Network,[0],[0]
"Each entry pi = 〈pi,1, . . .",4.5 Generative Profile Memory Network,[0],[0]
", pi,n〉 ∈ P is then encoded via f(pi)",4.5 Generative Profile Memory Network,[0],[0]
"=∑|pi|
j αipi,j .",4.5 Generative Profile Memory Network,[0],[0]
"That is, we weight words by their inverse term frequency: αi = 1/(1 + log(1 + tf)) where tf is computed from the GloVe index via
Zipf’s law4.",4.5 Generative Profile Memory Network,[0],[0]
Let F be the set of encoded memories.,4.5 Generative Profile Memory Network,[0],[0]
"The decoder now attends over the encoded profile entries, i.e., we compute the mask at, context ct and next input x̂t as:
at = softmax(FWah d t ),
ct = a ᵀ tF ; x̂t = tanh(Wc[ct−1, xt]).
",4.5 Generative Profile Memory Network,[0],[0]
"If the model has no profile information, and hence no memory, it becomes equivalent to the Seq2Seq model.",4.5 Generative Profile Memory Network,[0],[0]
"We first report results using automated evaluation metrics, and subsequently perform an extrinsic evaluation where crowdsourced workers perform a human evaluation of our models.",5 Experiments,[0],[0]
The main results are reported in Table 3.,5.1 Automated metrics,[0],[0]
"Overall, the results show the following key points:
Persona Conditioning Most models improve significantly when conditioning prediction on their own persona at least for the original (non-revised) versions, which is an easier task than the revised ones which have no word overlap.",5.1 Automated metrics,[0],[0]
"For example, the Profile Memory generation model has improved perplexity and hits@1 compared to Seq2Seq, and all the ranking algorithms (IR baseline, Starspace and Profile Memory Networks) obtain improved hits@1.
",5.1 Automated metrics,[0],[0]
Ranking vs. Generative.,5.1 Automated metrics,[0],[0]
Ranking models are far better than generative models at ranking.,5.1 Automated metrics,[0],[0]
"This is perhaps obvious as that is the metric they are optimizing, but still the performance difference is quite stark.",5.1 Automated metrics,[0],[0]
"It may be that the word-based probability which generative models use works well, but is not calibrated well enough to give a sentencebased probability which ranking requires.",5.1 Automated metrics,[0],[0]
"Human evaluation is also used to compare these methods, which we perform in Sec. 5.2.
",5.1 Automated metrics,[0],[0]
Ranking Models.,5.1 Automated metrics,[0],[0]
"For the ranking models, the IR baseline is outperformed by Starspace due to its learnt similarity metric, which in turn is outperformed by Profile Memory networks due to the attention mechanism over the profiles (as all other parts of the models are the same).",5.1 Automated metrics,[0],[0]
"Finally KV Profile Memory networks outperform Profile Memory Networks in the no persona case due to the ability to consider neighboring dialogue history and next
4tf = 1e6 ∗ 1/(idx1.07)
utterance pairs in the training set that are similar to the current dialogue, however when using persona information the performance is similar.
",5.1 Automated metrics,[0],[0]
Revised Personas.,5.1 Automated metrics,[0],[0]
Revised personas are much harder to use.,5.1 Automated metrics,[0],[0]
We do however still see some gain for the Profile Memory networks compared to none (0.354 vs. 0.318 hits@1).,5.1 Automated metrics,[0],[0]
"We also tried two variants of training: with the original personas in the training set or the revised ones, a comparison of which is shown in Table 6 of the Appendix.",5.1 Automated metrics,[0],[0]
"Training on revised personas helps, both for test examples that are in original form or revised form, likely due to the model be forced to learn more than simple word overlap, forcing the model to generalize more (i.e., learn semantic similarity of differing phrases).
",5.1 Automated metrics,[0],[0]
Their Persona.,5.1 Automated metrics,[0],[0]
"We can also condition a model on the other speaker’s persona, or both personas at once, the results of which are in Tables 5 and 6 in the Appendix.",5.1 Automated metrics,[0],[0]
Using “Their persona” has less impact on this dataset.,5.1 Automated metrics,[0],[0]
We believe this is because most speakers tend to focus on themselves when it comes to their interests.,5.1 Automated metrics,[0],[0]
It would be interesting how often this is the case in other datasets.,5.1 Automated metrics,[0],[0]
Certainly this is skewed by the particular instructions one could give to the crowdworkers.,5.1 Automated metrics,[0],[0]
"For example if we gave the instructions “try not to talk about yourself, but about the other’s interests’ likely these metrics would change.",5.1 Automated metrics,[0],[0]
"As automated metrics are notoriously poor for evaluating dialogue (Liu et al., 2016)",5.2 Human Evaluation,[0],[0]
we also perform human evaluation using crowdsourced workers.,5.2 Human Evaluation,[0],[0]
The procedure is as follows.,5.2 Human Evaluation,[0],[0]
We perform almost exactly the same setup as in the dataset collection process itself as in Section 3.3.,5.2 Human Evaluation,[0],[0]
"In that setup, we paired two Turkers and assigned them each a random (original) persona from the collected pool, and asked them to chat.",5.2 Human Evaluation,[0],[0]
"Here, from the Turker’s point of view everything looks the same except instead of being paired with a Turker they are paired with one of our models instead (they do not know this).",5.2 Human Evaluation,[0],[0]
"In this setting, for both the Turker and the model, the personas come from the test set pool.
",5.2 Human Evaluation,[0],[0]
"After the dialogue, we then ask the Turker some additional questions in order to evaluate the quality of the model.",5.2 Human Evaluation,[0],[0]
"We ask them to evaluate fluency, engagingness and consistency (scored between 1- 5).",5.2 Human Evaluation,[0],[0]
"Finally, we measure the ability to detect the other speaker’s profile by displaying two possible profiles, and ask which is more likely to be the profile of the person the Turker just spoke to.",5.2 Human Evaluation,[0],[0]
"More details of these measures are given in the Appendix.
",5.2 Human Evaluation,[0],[0]
"The results are reported in Table 4 for the best performing generative and ranking models, in both the No Persona and Self Persona categories, 100 dialogues each.",5.2 Human Evaluation,[0],[0]
We also evaluate the scores of human performance by replacing the chatbot with a human (another Turker).,5.2 Human Evaluation,[0],[0]
This effectively gives us upper bound scores which we can aim for with our models.,5.2 Human Evaluation,[0],[0]
"Finally, and importantly, we compare our models trained on PERSONA-CHAT with chit-chat models trained with the Twitter and OpenSubtitles datasets (2009 and 2018 versions) instead, following Vinyals and Le (2015).",5.2 Human Evaluation,[0],[0]
"Example chats from a few of the models are shown in the Appendix in Tables 7, 8, 9, 10, 11 and 12.
",5.2 Human Evaluation,[0],[0]
"Firstly, we see a difference in fluency, engagingness and consistency between all PERSONACHAT models and the models trained on OpenSubtitles and Twitter.",5.2 Human Evaluation,[0],[0]
"PERSONA-CHAT is a resource that is particularly strong at providing training data for the beginning of conversations, when the two speakers do not know each other, focusing on asking and answering questions, in contrast to other resources.",5.2 Human Evaluation,[0],[0]
"We also see suggestions of more subtle differences between the models, although these differences are obscured by the high variance of
the human raters’ evaluations.",5.2 Human Evaluation,[0],[0]
"For example, in both the generative and ranking model cases, models endowed with a persona can be detected by the human conversation partner, as evidenced by the persona detection accuracies, whilst maintaining fluency and consistency compared to their nonpersona driven counterparts.
",5.2 Human Evaluation,[0],[0]
"Finding the balance between fluency, engagement, consistency, and a persistent persona remains a strong challenge for future research.",5.2 Human Evaluation,[0],[0]
"Two tasks could naturally be considered using PERSONACHAT: (1) next utterance prediction during dialogue, and (2) profile prediction given dialogue history.",5.3 Profile Prediction,[0],[0]
"The main study of this work has been Task 1, where we have shown the use of profile information.",5.3 Profile Prediction,[0],[0]
"Task 2, however, can be used to extract such information.",5.3 Profile Prediction,[0],[0]
"While a full study is beyond the scope of this paper, we conducted some preliminary experiments, the details of which are in Appendix D. They show (i) human speaker’s profiles can be predicted from their dialogue with high accuracy (94.3%, similar to human performance in Table 4) or even from the model’s dialogue (23% with KV Profile Memory) showing the model is paying attention to the human’s interests.",5.3 Profile Prediction,[0],[0]
"Further, the accuracies clearly improve with further dialogue, as shown in Table 14.",5.3 Profile Prediction,[0],[0]
Combining Task 1 and Task 2 into a full system is an exciting area of future research.,5.3 Profile Prediction,[0],[0]
"In this work we have introduced the PERSONACHAT dataset, which consists of crowd-sourced dialogues where each participant plays the part of an assigned persona; and each (crowd-sourced) persona has a word-distinct paraphrase.",6 Conclusion & Discussion,[0],[0]
"We test various baseline models on this dataset, and show that models that have access to their own personas in addition to the state of the dialogue are scored as more consistent by annotators, although not more engaging.",6 Conclusion & Discussion,[0],[0]
"On the other hand, we show that models trained on PERSONA-CHAT (with or without personas) are more engaging than models trained on dialogue from other resources (movies, Twitter).
",6 Conclusion & Discussion,[0],[0]
We believe PERSONA-CHAT will be a useful resource for training components of future dialogue systems.,6 Conclusion & Discussion,[0],[0]
"Because we have paired human generated profiles and conversations, the data aids the construction of agents that have consistent per-
sonalities and viewpoints.",6 Conclusion & Discussion,[0],[0]
"Furthermore, predicting the profiles from a conversation moves chitchat tasks in the direction of goal-directed dialogue, which has metrics for success.",6 Conclusion & Discussion,[0],[0]
"Because we collect paraphrases of the profiles, they cannot be trivially matched; indeed, we believe the original and rephrased profiles are interesting as a semantic similarity dataset in their own right.",6 Conclusion & Discussion,[0],[0]
"We hope that the data will aid training agents that can ask questions about users’ profiles, remember the answers, and use them naturally in conversation.",6 Conclusion & Discussion,[0],[0]
"Chit-chat models are known to have several problems: they lack specificity, do not display a consistent personality and are often not very captivating.",abstractText,[0],[0]
In this work we present the task of making chit-chat more engaging by conditioning on profile information.,abstractText,[0],[0]
"We collect data and train models to (i) condition on their given profile information; and (ii) information about the person they are talking to, resulting in improved dialogues, as measured by next utterance prediction.",abstractText,[0],[0]
"Since (ii) is initially unknown, our model is trained to engage its partner with personal topics, and we show the resulting dialogue can be used to predict profile information about the interlocutors.",abstractText,[0],[0]
"Personalizing Dialogue Agents: I have a dog, do you have pets too?",title,[0],[0]
Machine Translation (MT) is a flagship of the recent successes and advances in the field of natural language processing.,1 Introduction,[0],[0]
"Its practical applications and use as a testbed for sequence transduction algorithms have spurred renewed interest in this topic.
",1 Introduction,[0],[0]
"While recent advances have reported near human-level performance on several language
†Sorbonne Universités, UPMC Univ Paris 06, CNRS, UMR 7606, LIP6, F-75005, Paris, France.
1https://github.com/facebookresearch/ UnsupervisedMT
pairs using neural approaches (Wu et al., 2016; Hassan et al., 2018), other studies have highlighted several open challenges (Koehn and Knowles, 2017; Isabelle et al., 2017; Sennrich, 2017).",1 Introduction,[0],[0]
A major challenge is the reliance of current learning algorithms on large parallel corpora.,1 Introduction,[0],[0]
"Unfortunately, the vast majority of language pairs have very little, if any, parallel data: learning algorithms need to better leverage monolingual data in order to make MT more widely applicable.
",1 Introduction,[0],[0]
"While a large body of literature has studied the use of monolingual data to boost translation performance when limited supervision is available, two recent approaches have explored the fully unsupervised setting (Lample et al., 2018; Artetxe et al., 2018), relying only on monolingual corpora in each language, as in the pioneering work by Ravi and Knight (2011).",1 Introduction,[0],[0]
"While there are subtle technical differences between these two recent works, we identify several common principles underlying their success.
",1 Introduction,[0],[0]
"First, they carefully initialize the MT system with an inferred bilingual dictionary.",1 Introduction,[0],[0]
"Second, they leverage strong language models, via training the sequence-to-sequence system (Sutskever et al., 2014; Bahdanau et al., 2015) as a denoising autoencoder (Vincent et al., 2008).",1 Introduction,[0],[0]
"Third, they turn the unsupervised problem into a supervised one by automatic generation of sentence pairs via back-translation (Sennrich et al., 2015a), i.e., the source-to-target model is applied to source sentences to generate inputs for training the targetto-source model, and vice versa.",1 Introduction,[0],[0]
"Finally, they constrain the latent representations produced by the encoder to be shared across the two languages.",1 Introduction,[0],[0]
"Empirically, these methods achieve remarkable results considering the fully unsupervised setting; for instance, about 15 BLEU points on the WMT’14 English-French benchmark.
",1 Introduction,[0],[0]
"The first contribution of this paper is a model
ar X
iv :1
80 4.
07 75
5v 2
[ cs
.C",1 Introduction,[0],[0]
"L
] 1
3 A
ug 2
01 8
that combines these two previous neural approaches, simplifying the architecture and loss function while still following the above mentioned principles.",1 Introduction,[0],[0]
The resulting model outperforms previous approaches and is both easier to train and tune.,1 Introduction,[0],[0]
"Then, we apply the same ideas and methodology to a traditional phrase-based statistical machine translation (PBSMT) system (Koehn et al., 2003).",1 Introduction,[0],[0]
"PBSMT models are well-known to outperform neural models when labeled data is scarce because they merely count occurrences, whereas neural models typically fit hundred of millions of parameters to learn distributed representations, which may generalize better when data is abundant but is prone to overfit when data is scarce.",1 Introduction,[0],[0]
"Our PBSMT model is simple, easy to interpret, fast to train and often achieves similar or better results than its NMT counterpart.",1 Introduction,[0],[0]
"We report gains of up to +10 BLEU points on widely used benchmarks when using our NMT model, and up to +12 points with our PBSMT model.",1 Introduction,[0],[0]
"Furthermore, we apply these methods to distant and low-resource languages, like EnglishRussian, English-Romanian and English-Urdu, and report competitive performance against both semi-supervised and supervised baselines.",1 Introduction,[0],[0]
"Learning to translate with only monolingual data is an ill-posed task, since there are potentially many ways to associate target with source sentences.",2 Principles of Unsupervised MT,[0],[0]
"Nevertheless, there has been exciting progress towards this goal in recent years, as discussed in the related work of Section 5.",2 Principles of Unsupervised MT,[0],[0]
"In this sec-
tion, we abstract away from the specific assumptions made by each prior work and instead focus on identifying the common principles underlying unsupervised MT.
",2 Principles of Unsupervised MT,[0],[0]
"We claim that unsupervised MT can be accomplished by leveraging the three components illustrated in Figure 1: (i) suitable initialization of the translation models, (ii) language modeling and (iii) iterative back-translation.",2 Principles of Unsupervised MT,[0],[0]
"In the following, we describe each of these components and later discuss how they can be better instantiated in both a neural model and phrase-based model.
",2 Principles of Unsupervised MT,[0],[0]
Initialization:,2 Principles of Unsupervised MT,[0],[0]
"Given the ill-posed nature of the task, model initialization expresses a natural prior over the space of solutions we expect to reach, jump-starting the process by leveraging approximate translations of words, short phrases or even sub-word units (Sennrich et al., 2015b).",2 Principles of Unsupervised MT,[0],[0]
"For instance, Klementiev et al. (2012) used a provided bilingual dictionary, while Lample et al. (2018) and Artetxe et al. (2018) used dictionaries inferred in an unsupervised way (Conneau et al., 2018; Artetxe et al., 2017).",2 Principles of Unsupervised MT,[0],[0]
"The motivating intuition is that while such initial “word-by-word” translation may be poor if languages or corpora are not closely related, it still preserves some of the original semantics.
",2 Principles of Unsupervised MT,[0],[0]
Language Modeling:,2 Principles of Unsupervised MT,[0],[0]
"Given large amounts of monolingual data, we can train language models on both source and target languages.",2 Principles of Unsupervised MT,[0],[0]
"These models express a data-driven prior about how sentences should read in each language, and they improve the quality of the translation models by per-
Algorithm 1:",2 Principles of Unsupervised MT,[0],[0]
"Unsupervised MT 1 Language models: Learn language models Ps and Pt
over source and target languages; 2 Initial translation models: Leveraging Ps and Pt,
learn two initial translation models, one in each direction: P (0)s→t and P (0) t→s;
3 for k=1 to N do 4 Back-translation: Generate source and target
sentences using the current translation models, P (k−1) t→s and P (k−1) s→t , factoring in language
models, Ps and Pt; 5 Train new translation models P (k)s→t and P (k) t→s
using the generated sentences and leveraging Ps and Pt;
6 end
forming local substitutions and word reorderings.
",2 Principles of Unsupervised MT,[0],[0]
Iterative Back-translation:,2 Principles of Unsupervised MT,[0],[0]
"The third principle is back-translation (Sennrich et al., 2015a), which is perhaps the most effective way to leverage monolingual data in a semi-supervised setting.",2 Principles of Unsupervised MT,[0],[0]
Its application in the unsupervised setting is to couple the source-to-target translation system with a backward model translating from the target to source language.,2 Principles of Unsupervised MT,[0],[0]
The goal of this model is to generate a source sentence for each target sentence in the monolingual corpus.,2 Principles of Unsupervised MT,[0],[0]
"This turns the daunting unsupervised problem into a supervised learning task, albeit with noisy source sentences.",2 Principles of Unsupervised MT,[0],[0]
"As the original model gets better at translating, we use the current model to improve the back-translation model, resulting in a coupled system trained with an iterative algorithm (He et al., 2016).",2 Principles of Unsupervised MT,[0],[0]
"Equipped with the three principles detailed in Section 2, we now discuss how to effectively combine them in the context of a NMT model (Section 3.1) and PBSMT model (Section 3.2).
",3 Unsupervised MT systems,[0],[0]
"In the reminder of the paper, we denote the space of source and target sentences by S and T , respectively, and the language models trained on source and target monolingual datasets by Ps and Pt, respectively.",3 Unsupervised MT systems,[0],[0]
"Finally, we denote by Ps→t and Pt→s the translation models from source to target and vice versa.",3 Unsupervised MT systems,[0],[0]
An overview of our approach is given in Algorithm 1.,3 Unsupervised MT systems,[0],[0]
"We now introduce a new unsupervised NMT method, which is derived from earlier work by Artetxe et al. (2018) and Lample et al. (2018).",3.1 Unsupervised NMT,[0],[0]
"We first discuss how the previously mentioned
three key principles are instantiated in our work, and then introduce an additional property of the system, the sharing of internal representations across languages, which is specific and critical to NMT.",3.1 Unsupervised NMT,[0],[0]
"From now on, we assume that a NMT model consists of an encoder and a decoder.",3.1 Unsupervised NMT,[0],[0]
"Section 4 gives the specific details of this architecture.
",3.1 Unsupervised NMT,[0],[0]
"Initialization: While prior work relied on bilingual dictionaries, here we propose a more effective and simpler approach which is particularly suitable for related languages.2 First, instead of considering words, we consider byte-pair encodings (BPE) (Sennrich et al., 2015b), which have two major advantages: they reduce the vocabulary size and they eliminate the presence of unknown words in the output translation.",3.1 Unsupervised NMT,[0],[0]
"Second, instead of learning an explicit mapping between BPEs in the source and target languages, we define BPE tokens by jointly processing both monolingual corpora.",3.1 Unsupervised NMT,[0],[0]
"If languages are related, they will naturally share a good fraction of BPE tokens, which eliminates the need to infer a bilingual dictionary.",3.1 Unsupervised NMT,[0],[0]
"In practice, we i) join the monolingual corpora, ii) apply BPE tokenization on the resulting corpus, and iii) learn token embeddings (Mikolov et al., 2013) on the same corpus, which are then used to initialize the lookup tables in the encoder and decoder.
Language Modeling: In NMT, language modeling is accomplished via denoising autoencoding, by minimizing:
Llm = Ex∼S",3.1 Unsupervised NMT,[0],[0]
[− logPs→s(x|C(x))],3.1 Unsupervised NMT,[0],[0]
+ Ey∼T [− logPt→t(y|C(y))],3.1 Unsupervised NMT,[0],[0]
"(1)
where C is a noise model with some words dropped and swapped as in Lample et al. (2018).",3.1 Unsupervised NMT,[0],[0]
"Ps→s andPt→t are the composition of encoder and decoder both operating on the source and target sides, respectively.
",3.1 Unsupervised NMT,[0],[0]
Back-translation: Let us denote by u∗(y) the sentence in the source language inferred from y ∈ T such that u∗(y) = argmaxPt→s(u|y).,3.1 Unsupervised NMT,[0],[0]
"Similarly, let us denote by v∗(x) the sentence in the target language inferred from x ∈ S such that v∗(x) = argmaxPs→t(v|x).",3.1 Unsupervised NMT,[0],[0]
"The pairs (u∗(y), y) and (x, v∗(x))) constitute automatically-generated parallel sentences which, following the back-translation principle, can be
2For unrelated languages, we need to infer a dictionary to properly initialize the embeddings (Conneau et al., 2018).
",3.1 Unsupervised NMT,[0],[0]
"used to train the two MT models by minimizing the following loss:
Lback = Ey∼T [− logPs→t(y|u∗(y))",3.1 Unsupervised NMT,[0],[0]
],3.1 Unsupervised NMT,[0],[0]
+ Ex∼S,3.1 Unsupervised NMT,[0],[0]
[− logPt→s(x|v∗(x))].,3.1 Unsupervised NMT,[0],[0]
"(2)
Note that when minimizing this objective function we do not back-prop through the reverse model which generated the data, both for the sake of simplicity and because we did not observe improvements when doing so.",3.1 Unsupervised NMT,[0],[0]
"The objective function minimized at every iteration of stochastic gradient descent, is simply the sum of Llm in Eq. 1 and Lback in Eq. 2.",3.1 Unsupervised NMT,[0],[0]
"To prevent the model from cheating by using different subspaces for the language modeling and translation tasks, we add an additional constraint which we discuss next.
",3.1 Unsupervised NMT,[0],[0]
"Sharing Latent Representations: A shared encoder representation acts like an interlingua, which is translated in the decoder target language regardless of the input source language.",3.1 Unsupervised NMT,[0],[0]
"This ensures that the benefits of language modeling, implemented via the denoising autoencoder objective, nicely transfer to translation from noisy sources and eventually help the NMT model to translate more fluently.",3.1 Unsupervised NMT,[0],[0]
"In order to share the encoder representations, we share all encoder parameters (including the embedding matrices since we perform joint tokenization) across the two languages to ensure that the latent representation of the source sentence is robust to the source language.",3.1 Unsupervised NMT,[0],[0]
"Similarly, we share the decoder parameters across the two languages.",3.1 Unsupervised NMT,[0],[0]
"While sharing the encoder is critical to get the model to work, sharing the decoder simply induces useful regularization.",3.1 Unsupervised NMT,[0],[0]
"Unlike prior work (Johnson et al., 2016), the first token of the decoder specifies the language the module is operating with, while the encoder does not have any language identifier.",3.1 Unsupervised NMT,[0],[0]
"In this section, we discuss how to perform unsupervised machine translation using a PhraseBased Statistical Machine Translation (PBSMT) system (Koehn et al., 2003) as the underlying backbone model.",3.2 Unsupervised PBSMT,[0],[0]
"Note that PBSMT models are known to perform well on low-resource language pairs, and are therefore a potentially good alternative to neural models in the unsupervised setting.
",3.2 Unsupervised PBSMT,[0],[0]
"When translating from x to y, a PBSMT system scores y according to: argmaxy P (y|x) = argmaxy P (x|y)P (y), where P (x|y) is derived
from so called “phrase tables”, and P (y) is the score assigned by a language model.
",3.2 Unsupervised PBSMT,[0],[0]
"Given a dataset of bitexts, PBSMT first infers an alignment between source and target phrases.",3.2 Unsupervised PBSMT,[0],[0]
"It then populates phrase tables, whose entries store the probability that a certain n-gram in the source/target language is mapped to another ngram in the target/source language.
",3.2 Unsupervised PBSMT,[0],[0]
"In the unsupervised setting, we can easily train a language model on monolingual data, but it is less clear how to populate the phrase tables, which are a necessary component for good translation.",3.2 Unsupervised PBSMT,[0],[0]
"Fortunately, similar to the neural case, the principles of Section 2 are effective to solve this problem.
",3.2 Unsupervised PBSMT,[0],[0]
Initialization: We populate the initial phrase tables (from source to target and from target to source) using an inferred bilingual dictionary built from monolingual corpora using the method proposed by Conneau et al. (2018).,3.2 Unsupervised PBSMT,[0],[0]
"In the following, we will refer to phrases as single words, but the very same arguments trivially apply to longer ngrams.",3.2 Unsupervised PBSMT,[0],[0]
"Phrase tables are populated with the scores of the translation of a source word to:
p(tj |si) = e 1 T cos(e(tj),We(si))∑
k e 1 T cos(e(tk),We(si))
, (3)
where tj is the j-th word in the target vocabulary and si is the i-th word in the source vocabulary, T is a hyper-parameter used to tune the peakiness of the distribution3, W is the rotation matrix mapping the source embeddings into the target embeddings (Conneau et al., 2018), and e(x) is the embedding of x.
Language Modeling: Both in the source and target domains we learn smoothed n-gram language models using KenLM (Heafield, 2011), although neural models could also be considered.",3.2 Unsupervised PBSMT,[0],[0]
"These remain fixed throughout training iterations.
",3.2 Unsupervised PBSMT,[0],[0]
"Iterative Back-Translation: To jump-start the iterative process, we use the unsupervised phrase tables and the language model on the target side to construct a seed PBSMT.",3.2 Unsupervised PBSMT,[0],[0]
We then use this model to translate the source monolingual corpus into the target language (back-translation step).,3.2 Unsupervised PBSMT,[0],[0]
"Once the data has been generated, we train a PBSMT in supervised mode to map the generated data back to the original source sentences.",3.2 Unsupervised PBSMT,[0],[0]
"Next, we perform
3We set T = 30 in all our experiments, following the setting of Smith et al. (2017).
",3.2 Unsupervised PBSMT,[0],[0]
both generation and training process but in the reverse direction.,3.2 Unsupervised PBSMT,[0],[0]
"We repeat these steps as many times as desired (see Algorithm 2 in Section A).
",3.2 Unsupervised PBSMT,[0],[0]
"Intuitively, many entries in the phrase tables are not correct because the input to the PBSMT at any given point during training is noisy.",3.2 Unsupervised PBSMT,[0],[0]
"Despite that, the language model may be able to fix some of these mistakes at generation time.",3.2 Unsupervised PBSMT,[0],[0]
"As long as that happens, the translation improves, and with that also the phrase tables at the next round.",3.2 Unsupervised PBSMT,[0],[0]
"There will be more entries that correspond to correct phrases, which makes the PBSMT model stronger because it has bigger tables and it enables phrase swaps over longer spans.",3.2 Unsupervised PBSMT,[0],[0]
We first describe the datasets and experimental protocol we used.,4 Experiments,[0],[0]
"Then, we compare the two proposed unsupervised approaches to earlier attempts, to semi-supervised methods and to the very same models but trained with varying amounts of labeled data.",4 Experiments,[0],[0]
We conclude with an ablation study to understand the relative importance of the three principles introduced in Section 2.,4 Experiments,[0],[0]
"We consider five language pairs: English-French, English-German, English-Romanian, EnglishRussian and English-Urdu.",4.1 Datasets and Methodology,[0],[0]
"The first two pairs are used to compare to recent work on unsupervised MT (Artetxe et al., 2018; Lample et al., 2018).",4.1 Datasets and Methodology,[0],[0]
"The last three pairs are instead used to test our PBSMT unsupervised method on truly low-resource pairs (Gu et al., 2018) or unrelated languages that do not even share the same alphabet.
",4.1 Datasets and Methodology,[0],[0]
"For English, French, German and Russian, we use all available sentences from the WMT monolingual News Crawl datasets from years 2007 through 2017.",4.1 Datasets and Methodology,[0],[0]
"For Romanian, the News Crawl dataset is only composed of 2.2 million sentences, so we augment it with the monolingual data from WMT’16, resulting in 2.9 million sentences.",4.1 Datasets and Methodology,[0],[0]
"In Urdu, we use the dataset of Jawaid et al. (2014), composed of about 5.5 million monolingual sentences.",4.1 Datasets and Methodology,[0],[0]
"We report results on newstest 2014 for en− fr, and newstest 2016 for en− de, en− ro and en− ru.",4.1 Datasets and Methodology,[0],[0]
"For Urdu, we use the LDC2010T21 and LDC2010T23 corpora each with about 1800 sentences as validation and test sets, respectively.
",4.1 Datasets and Methodology,[0],[0]
"We use Moses scripts (Koehn et al., 2007) for tokenization.",4.1 Datasets and Methodology,[0],[0]
"NMT is trained with 60,000 BPE
codes.",4.1 Datasets and Methodology,[0],[0]
"PBSMT is trained with true-casing, and by removing diacritics from Romanian on the source side to deal with their inconsistent use across the monolingual dataset (Sennrich et al., 2016).",4.1 Datasets and Methodology,[0],[0]
Both the NMT and PBSMT approaches require either cross-lingual BPE embeddings (to initialize the shared lookup tables) or n-gram embeddings (to initialize the phrase table).,4.2 Initialization,[0],[0]
"We generate embeddings using fastText (Bojanowski et al., 2017) with an embedding dimension of 512, a context window of size 5 and 10 negative samples.",4.2 Initialization,[0],[0]
"For NMT, fastText is applied on the concatenation of source and target corpora, which results in crosslingual BPE embeddings.
",4.2 Initialization,[0],[0]
"For PBSMT, we generate n-gram embeddings on the source and target corpora independently, and align them using the MUSE library (Conneau et al., 2018).",4.2 Initialization,[0],[0]
"Since learning unique embeddings of every possible phrase would be intractable, we consider the most frequent 300,000 source phrases, and align each of them to its 200 nearest neighbors in the target space, resulting in a phrase table of 60 million phrase pairs which we score using the formula in Eq. 3.
",4.2 Initialization,[0],[0]
"In practice, we observe a small but significant difference of about 1 BLEU point using a phrase table of bigrams compared to a phrase table of unigrams, but did not observe any improvement using longer phrases.",4.2 Initialization,[0],[0]
"Table 1 shows an extract of a French-English unsupervised phrase table, where we can see that unigrams are correctly aligned to bigrams, and vice versa.",4.2 Initialization,[0],[0]
The next subsections provide details about the architecture and training procedure of our models.,4.3 Training,[0],[0]
"In this study, we use NMT models built upon LSTM (Hochreiter and Schmidhuber, 1997) and Transformer (Vaswani et al., 2017) cells.",4.3.1 NMT,[0],[0]
For the LSTM model we use the same architecture as in Lample et al. (2018).,4.3.1 NMT,[0],[0]
"For the Transformer, we use 4 layers both in the encoder and in the decoder.",4.3.1 NMT,[0],[0]
"Following Press and Wolf (2016), we share all lookup tables between the encoder and the decoder, and between the source and the target languages.",4.3.1 NMT,[0],[0]
The dimensionality of the embeddings and of the hidden layers is set to 512.,4.3.1 NMT,[0],[0]
"We used the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 10−4, β1 = 0.5, and a batch size of 32.",4.3.1 NMT,[0],[0]
"At decoding time, we generate greedily.",4.3.1 NMT,[0],[0]
The PBSMT uses Moses’ default smoothed ngram language model with phrase reordering disabled during the very first generation.,4.3.2 PBSMT,[0],[0]
PBSMT is trained in a iterative manner using Algorithm 2.,4.3.2 PBSMT,[0],[0]
"At each iteration, we translate 5 million sentences randomly sampled from the monolingual dataset in the source language.",4.3.2 PBSMT,[0],[0]
"Except for initialization, we use phrase tables with phrases up to length 4.",4.3.2 PBSMT,[0],[0]
"Moses’ implementation of PBSMT has 15 hyperparameters, such as relative weighting of each scoring function, word penalty, etc.",4.4 Model selection,[0],[0]
"In this work, we consider two methods to set these hyperparameters.",4.4 Model selection,[0],[0]
"We either set them to their default values in the toolbox, or we set them using a small validation set of parallel sentences.",4.4 Model selection,[0],[0]
"It turns out
that with only 100 labeled sentences in the validation set, PBSMT would overfit to the validation set.",4.4 Model selection,[0],[0]
"For instance, on en → fr, PBSMT tuned on 100 parallel sentences obtains a BLEU score of 26.42 on newstest 2014, compared to 27.09 with default hyper-parameters, and 28.02 when tuned on the 3000 parallel sentences of newstest 2013.",4.4 Model selection,[0],[0]
"Therefore, unless otherwise specified, all PBSMT models considered in the paper use default hyperparameter values, and do not use any parallel resource whatsoever.
",4.4 Model selection,[0],[0]
"For the NMT, we also consider two model selection procedures: an unsupervised criterion based on the BLEU score of a “round-trip” translation (source → target → source and target → source → target) as in Lample et al. (2018), and crossvalidation using a small validation set with 100 parallel sentences.",4.4 Model selection,[0],[0]
"In our experiments, we found the unsupervised criterion to be highly correlated with the test metric when using the Transformer model, but not always for the LSTM.",4.4 Model selection,[0],[0]
"Therefore, unless otherwise specified, we select the best LSTM models using a small validation set of 100 parallel sentences, and the best Transformer models with the unsupervised criterion.",4.4 Model selection,[0],[0]
The results reported in Table 2 show that our unsupervised NMT and PBSMT systems largely outperform previous unsupervised baselines.,4.5 Results,[0],[0]
We report large gains on all language pairs and directions.,4.5 Results,[0],[0]
"For instance, on the en → fr task, our unsupervised PBSMT obtains a BLEU score of 28.1, outperforming the previous best result by more than 11 BLEU points.",4.5 Results,[0],[0]
"Even on a more complex task like en → de, both PBSMT and NMT surpass the baseline score by more than 10 BLEU
points.",4.5 Results,[0],[0]
"Even before iterative back-translation, the PBSMT model significantly outperforms previous approaches, and can be trained in a few minutes.
",4.5 Results,[0],[0]
Table 3 illustrates the quality of the PBSMT model during the iterative training process.,4.5 Results,[0],[0]
"For instance, the fr → en model obtains a BLEU score of 17.5 at iteration 0 – i.e. after the unsupervised phrase table construction – while it achieves a score of 27.2 at iteration 4.",4.5 Results,[0],[0]
This highlights the importance of multiple back-translation iterations.,4.5 Results,[0],[0]
The last rows of Table 3 also show that we get additional gains by further tuning the NMT model on the data generated by PBSMT (PBSMT + NMT).,4.5 Results,[0],[0]
We simply add the data generated by the unsupervised PBSMT system to the back-translated data produced by the NMT model.,4.5 Results,[0],[0]
"By combining PBSMT and NMT, we achieve BLEU scores of 20.2 and 25.2 on the challenging en → de and de → en translation tasks.",4.5 Results,[0],[0]
"While we also tried bootstraping the PBSMT model with back-translated data generated by a NMT model (NMT + PBSMT), this did not improve over PBSMT alone.
",4.5 Results,[0],[0]
"Next, we compare to fully supervised models.",4.5 Results,[0],[0]
Figure 2 shows the performance of the same architectures trained in a fully supervised way using parallel training sets of varying size.,4.5 Results,[0],[0]
"The unsupervised PBSMT model achieves the same performance as its supervised counterpart trained on more than 100,000 parallel sentences.
",4.5 Results,[0],[0]
This is confirmed on low-resource languages.,4.5 Results,[0],[0]
"In particular, on ro → en, our unsupervised PBSMT model obtains a BLEU score of 23.9, outperforming Gu et al. (2018)’s method by 1 point, despite its use of 6,000 parallel sentences, a seed dictionary, and a multi-NMT system combining par-
allel resources from 5 different languages.",4.5 Results,[0],[0]
"On Russian, our unsupervised PBSMT model obtains a BLEU score of 16.6 on ru→ en, showing that this approach works reasonably well on distant languages.",4.5 Results,[0],[0]
"Finally we train on ur → en, which is both low resource and distant.",4.5 Results,[0],[0]
"In a supervised mode, PBSMT using the noisy and outof-domain 800,000 parallel sentences from Tiedemann (2012) achieves a BLEU score of 9.8.",4.5 Results,[0],[0]
"Instead, our unsupervised PBSMT system achieves 12.3 BLEU using only a validation set of 1800 sentences to tune Moses hyper-parameters.",4.5 Results,[0],[0]
"In Figure 3 we report results from an ablation study, to better understand the importance of the three principles when training PBSMT.",4.6 Ablation Study,[0],[0]
"This study shows that more iterations only partially compensate for lower quality phrase table initialization (Left), language models trained over less data (Middle) or less monolingual data (Right).",4.6 Ablation Study,[0],[0]
"Moreover, the influence of the quality of the language model becomes more prominent as we iterate.",4.6 Ablation Study,[0],[0]
"These findings suggests that better initialization methods and more powerful language models may further improve our results.
",4.6 Ablation Study,[0],[0]
We perform a similar ablation study for the NMT system (see Appendix).,4.6 Ablation Study,[0],[0]
"We find that backtranslation and auto-encoding are critical components, without which the system fails to learn.",4.6 Ablation Study,[0],[0]
"We also find that initialization of embeddings is very important, and we gain 7 BLEU points compared to prior work (Artetxe et al., 2018; Lample et al., 2018) by learning BPE embeddings over the concatenated monolingual corpora.",4.6 Ablation Study,[0],[0]
A large body of literature has studied using monolingual data to boost translation performance when limited supervision is available.,5 Related Work,[0],[0]
"This limited supervision is typically provided as a small set of parallel sentences (Sennrich et al., 2015a; Gulcehre et al., 2015; He et al., 2016; Gu et al., 2018; Wang et al., 2018); large sets of parallel sentences in related languages (Firat et al., 2016; Johnson et al., 2016; Chen et al., 2017; Zheng et al., 2017); cross-lingual dictionaries (Klementiev et al., 2012; Irvine and Callison-Burch, 2014, 2016); or comparable corpora (Munteanu et al., 2004; Irvine and Callison-Burch, 2013).
",5 Related Work,[0],[0]
"Learning to translate without any form of supervision has also attracted interest, but is challenging.",5 Related Work,[0],[0]
"In their seminal work, Ravi and Knight (2011) leverage linguistic prior knowledge to reframe the unsupervised MT task as deciphering and demonstrate the feasibility on short sentences with limited vocabulary.",5 Related Work,[0],[0]
"Earlier work by Carbonell et al. (2006) also aimed at unsupervised MT, but leveraged a bilingual dictionary to seed the translation.",5 Related Work,[0],[0]
"Both works rely on a language model on the target side to correct for translation fluency.
",5 Related Work,[0],[0]
"Subsequent work (Klementiev et al., 2012; Irvine and Callison-Burch, 2014, 2016) relied on bilingual dictionaries, small parallel corpora of several thousand sentences, and linguistically motivated features to prune the search space.",5 Related Work,[0],[0]
Irvine and Callison-Burch (2014) use monolingual data to expand phrase tables learned in a supervised setting.,5 Related Work,[0],[0]
"In our work we also expand phrase tables, but we initialize them with an inferred bilingual n-gram dictionary, following work from the connectionist community aimed at improving PBSMT with neural models (Schwenk, 2012; Kalchbrenner and Blunsom, 2013; Cho et al., 2014).
",5 Related Work,[0],[0]
"In recent years back-translation has become a
popular method of augmenting training sets with monolingual data on the target side (Sennrich et al., 2015a), and has been integrated in the “dual learning” framework of He et al. (2016) and subsequent extensions (Wang et al., 2018).",5 Related Work,[0],[0]
"Our approach is similar to the dual learning framework, except that in their model gradients are backpropagated through the reverse model and they pretrain using a relatively large amount of labeled data, whereas our approach is fully unsupervised.
",5 Related Work,[0],[0]
"Finally, our work can be seen as an extension of recent studies (Lample et al., 2018; Artetxe et al., 2018; Yang et al., 2018) on fully unsupervised MT with two major contributions.",5 Related Work,[0],[0]
"First, we propose a much simpler and more effective initialization method for related languages.",5 Related Work,[0],[0]
"Second, we abstract away three principles of unsupervised MT and apply them to a PBSMT, which even outperforms the original NMT.",5 Related Work,[0],[0]
"Moreover, our results show that the combination of PBSMT and NMT achieves even better performance.",5 Related Work,[0],[0]
"In this work, we identify three principles underlying recent successes in fully unsupervised MT and show how to apply these principles to PBSMT and NMT systems.",6 Conclusions and Future Work,[0],[0]
"We find that PBSMT systems often outperform NMT systems in the fully unsupervised setting, and that by combining these systems we can greatly outperform previous approaches from the literature.",6 Conclusions and Future Work,[0],[0]
"We apply our approach to several popular benchmark language pairs, obtaining state of the art results, and to several low-resource and under-explored language pairs.
",6 Conclusions and Future Work,[0],[0]
"It’s an open question whether there are more effective instantiations of these principles or other principles altogether, and under what conditions our iterative process is guaranteed to converge.",6 Conclusions and Future Work,[0],[0]
Future work may also extend to the semisupervised setting.,6 Conclusions and Future Work,[0],[0]
"In this Appendix, we report the detailed algorithm for unsupervised PBSMT, a detailed ablation study using NMT and conclude with some example translations.
",A Supplemental Material,[0],[0]
"Algorithm 2: Unsupervised PBSMT 1 Learn bilingual dictionary using Conneau
et al. (2018); 2 Populate phrase tables using Eq. 3 and learn a
language model to build P (0)s→t;
3 Use P (0)s→t to translate the source monolingual
dataset, yielding D(0)t ; 4 for i=1 to N do 5 Train model P (i)t→s using D (i−1) t ; 6 Use P (i)t→s to translate the target monolingual dataset, yielding D(i)s ; 7 Train model P (i)s→t using D (i) s ; 8 Use P (i)s→t to translate the source monolingual dataset, yielding D(i)t ; 9 end
A.1 NMT Ablation study In Table 4 we report results from an ablation study we performed for NMT using the Transformer architecture.",A Supplemental Material,[0],[0]
"All results are for the en→ fr task.
",A Supplemental Material,[0],[0]
"First, we analyze the effect of different initialization methods for the embedding matrices.",A Supplemental Material,[0],[0]
"If we switch from BPE tokens to words, BLEU drops by 4 points.",A Supplemental Material,[0],[0]
"If we used BPE but train embeddings in each language independently and then map them via MUSE (Conneau et al., 2018), BLEU drops by 3 points.",A Supplemental Material,[0],[0]
"Finally, compared to the word aligned procedure used by Lample et al. (2018), based on words and MUSE, the gain is about 7 points.",A Supplemental Material,[0],[0]
"To stress the importance of initialization, we also report performance using random initialization of BPE embeddings.",A Supplemental Material,[0],[0]
"In this case, convergence is much slower and to a much lower accuracy, achieving a BLEU score of 10.5.
",A Supplemental Material,[0],[0]
"The table also demonstrates the critical importance of the auto-encoding and back-translation terms in the loss, and the robustness of our approach to choice of architectures.
",A Supplemental Material,[0],[0]
"A.2 Qualitative study Table 5 shows example translations from the French-English newstest 2014 dataset at different
iterations of the learning algorithm for both NMT and PBSMT models.",A Supplemental Material,[0],[0]
"Prior to the first iteration of back-translation, using only the unsupervised phrase table, the PBSMT translations are similar to word-by-word translations and do not respect the syntax of the target language, yet still contain most of the semantics of the original sentences.",A Supplemental Material,[0],[0]
"As we increase the number of epochs in NMT and as we iterate for PBSMT, we observe a continuous improvement in the quality of the unsupervised translations.",A Supplemental Material,[0],[0]
"Interestingly, in the second example, both the PBSMT and NMT models fail to adapt to the polysemy of the French word “langue”, which can be translated as “tongue” or “language” in English.",A Supplemental Material,[0],[0]
"These translations were both present in the unsupervised phrase table, but the conditional probability of “language” to be the correct translation of “langue” was very high compared to the one of “tongue”: P (language|langue) = 0.92, while P (tongue|langue) = 0.0005.",A Supplemental Material,[0],[0]
"As a comparison, the phrase table of a Moses model trained in a supervised way contains P (language|langue) = 0.633, P (tongue|langue) = 0.0076, giving a higher probability for “langue” to be properly translated.",A Supplemental Material,[0],[0]
"This underlines the importance of the initial unsupervised phrase alignment procedure, as it was shown in Figure 1.
",A Supplemental Material,[0],[0]
"Finally, in Table 6 we report a random subset of test sentences translated from Russian to English, showing that the model mostly retains the semantics, while making some mistakes in the grammar as well as in the choice of words and entities.",A Supplemental Material,[0],[0]
"In Table 7, we show examples of translations from German to English with PBSMT, NMT, and PBSMT+NMT to show how the combination of these models performs better than them individually.
",A Supplemental Material,[0],[0]
"German→ English
Source Flüchtlinge brauchen Unterkünfte : Studie warnt vor Wohnungslücke PBSMT Refugees need accommodation : Study warns Wohnungslücke NMT Refugees need forestry study to warn housing gap PBSMT+NMT",A Supplemental Material,[0],[0]
"Refugees need accommodation : Study warns of housing gap Reference Refugees need accommodation : study warns of housing gap
Source Konflikte : Mehrheit unterstützt die Anti-IS-Ausbildungsmission PBSMT Conflict : Majority supports Anti-IS-Ausbildungsmission NMT Tensions support majority anti-IS-recruiting mission PBSMT+NMT",A Supplemental Material,[0],[0]
"Tensions : Majority supports the anti-IS-recruitment mission Reference Conflicts : the majority support anti ISIS training mission
Source Roboterautos : Regierung will Vorreiterrolle für Deutschland PBSMT Roboterautos : Government will step forward for Germany NMT Robotic cars will pre-reiterate government for Germany PBSMT+NMT",A Supplemental Material,[0],[0]
"Robotic cars : government wants pre-orders for Germany Reference Robot cars : Government wants Germany to take a leading role
Source Pfund steigt durch beschleunigtes Lohnwachstum im Vereinigten Königreich PBSMT Pound rises as UK beschleunigtes Lohnwachstum .",A Supplemental Material,[0],[0]
NMT £ rises through rapid wage growth in the U.S. PBSMT+NMT,A Supplemental Material,[0],[0]
"Pound rises by accelerating wage growth in the United Kingdom Reference Pound rises as UK wage growth accelerates
Source 46 Prozent sagten , dass sie die Tür offen lassen , um zu anderen Kandidaten zu wechseln .",A Supplemental Material,[0],[0]
PBSMT 52 per cent said that they left the door open to them to switch to other candidates .,A Supplemental Material,[0],[0]
NMT 46 percent said that they would let the door open to switch to other candidates .,A Supplemental Material,[0],[0]
PBSMT+NMT,A Supplemental Material,[0],[0]
46 percent said that they left the door open to switch to other candidates .,A Supplemental Material,[0],[0]
"Reference 46 percent said they are leaving the door open to switching candidates .
",A Supplemental Material,[0],[0]
"Source Selbst wenn die Republikaner sich um einen anderen Kandidaten sammelten , schlägt Trump noch fast jeden .",A Supplemental Material,[0],[0]
"PBSMT Even if the Republicans a way to other candidates collected , beats Trump , yet almost everyone .",A Supplemental Material,[0],[0]
NMT,A Supplemental Material,[0],[0]
"Even if Republicans are to donate to a different candidate , Trump takes to almost every place . PBSMT+NMT",A Supplemental Material,[0],[0]
"Even if Republicans gather to nominate another candidate , Trump still beats nearly everyone .",A Supplemental Material,[0],[0]
"Reference Even if Republicans rallied around another candidate , Trump still beats almost everyone .
",A Supplemental Material,[0],[0]
"Source Ich glaube sicher , dass es nicht genügend",A Supplemental Material,[0],[0]
"Beweise gibt , um ein Todesurteil zu rechtfertigen .",A Supplemental Material,[0],[0]
PBSMT I think for sure that there was not enough evidence to justify a death sentence .,A Supplemental Material,[0],[0]
NMT I believe it ’s not sure there ’s enough evidence to justify a executions .,A Supplemental Material,[0],[0]
PBSMT+NMT,A Supplemental Material,[0],[0]
I sure believe there is not enough evidence to justify a death sentence .,A Supplemental Material,[0],[0]
"Reference I certainly believe there was not enough evidence to justify a death sentence .
",A Supplemental Material,[0],[0]
"Source Auch wenn der Laden gut besucht ist , ist es nicht schwer , einen Parkplatz zu finden .",A Supplemental Material,[0],[0]
"PBSMT Even if the store visited it is , it is not hard to find a parking lot .",A Supplemental Material,[0],[0]
"NMT To be sure , the shop is well visited , but it ’s not a troubled driveway .",A Supplemental Material,[0],[0]
PBSMT+NMT,A Supplemental Material,[0],[0]
"Even if the shop is well visited , it is not hard to find a parking lot .",A Supplemental Material,[0],[0]
"Reference While the store can get busy , parking is usually not hard to find .
",A Supplemental Material,[0],[0]
"Source Die Suite , in dem der kanadische Sänger wohnt , kostet am Tag genau so viel , wie ihre Mama Ewa",A Supplemental Material,[0],[0]
im halben Jahr verdient .,A Supplemental Material,[0],[0]
"PBSMT The suite in which the Canadian singer grew up , costs on the day so much as their mum Vera in half year earned .",A Supplemental Material,[0],[0]
"NMT The Suite , in which the Canadian singer lived , costs day care exactly as much as her mom Ewa earned during the decade .",A Supplemental Material,[0],[0]
PBSMT+NMT,A Supplemental Material,[0],[0]
"The suite , in which the Canadian singer lived , costs a day precisely as much as her mom Ewa earned in half .",A Supplemental Material,[0],[0]
"Reference The suite where the Canadian singer is staying costs as much for one night as her mother Ewa earns in six months .
",A Supplemental Material,[0],[0]
"Source Der durchschnittliche BMI unter denen , die sich dieser Operation unterzogen , sank von 31 auf",A Supplemental Material,[0],[0]
"24,5 bis",A Supplemental Material,[0],[0]
Ende des fünften Jahres in dieser Studie .,A Supplemental Material,[0],[0]
PBSMT The average BMI among those who undergoing this operation decreased by 22 to 325 by the end of fifth year in this study .,A Supplemental Material,[0],[0]
NMT,A Supplemental Material,[0],[0]
"The average BMI among those who undergo surgery , sank from 31 to 24,500 by the end of the fifth year in that study .",A Supplemental Material,[0],[0]
PBSMT+NMT,A Supplemental Material,[0],[0]
The average BMI among those who undergo this surgery fell from 31 to 24.5 by the end of the fifth year in this study .,A Supplemental Material,[0],[0]
Reference,A Supplemental Material,[0],[0]
"The average BMI among those who had surgery fell from 31 to 24.5 by the end of their fifth year in the study .
",A Supplemental Material,[0],[0]
"Source Die 300 Plakate sind von Künstlern , die ihre Arbeit im Museum für grausame Designs in Banksys Dismaland ausgestellt haben .",A Supplemental Material,[0],[0]
PBSMT The 250 posters are by artists and their work in the museum for gruesome designs in Dismaland Banksys have displayed .,A Supplemental Material,[0],[0]
NMT,A Supplemental Material,[0],[0]
The 300 posters are posters of artists who have their work Museum for real-life Designs in Banksys and Dismalausgestellt .,A Supplemental Material,[0],[0]
PBSMT+NMT,A Supplemental Material,[0],[0]
The 300 posters are from artists who have displayed their work at the Museum of cruel design in Banksys Dismaland .,A Supplemental Material,[0],[0]
Reference,A Supplemental Material,[0],[0]
"The 300 posters are by artists who exhibited work at the Museum of Cruel Designs in Banksy ’s Dismaland .
",A Supplemental Material,[0],[0]
Source Bis zum Ende des Tages gab es einen weiteren Tod :,A Supplemental Material,[0],[0]
"Lamm nahm sich das Leben , als die Polizei ihn einkesselte .",A Supplemental Material,[0],[0]
PBSMT At the end of the day it ’s a further death :,A Supplemental Material,[0],[0]
Lamb took out life as police him einkesselte .,A Supplemental Material,[0],[0]
NMT,A Supplemental Material,[0],[0]
By the end of the day there was another death : Lamm emerged this year as police were trying to looseless him .,A Supplemental Material,[0],[0]
PBSMT+NMT,A Supplemental Material,[0],[0]
"By the end of the day , there ’s another death : Lamm took out life as police arrived to him .",A Supplemental Material,[0],[0]
"Reference By the end of the day , there would be one more death : Lamb took his own life as police closed in on him .
",A Supplemental Material,[0],[0]
"Source Chaos folgte an der Grenze , als Hunderte von Migranten sich in einem Niemandsland ansammelten und serbische Beamte mit Empörung reagierten .",A Supplemental Material,[0],[0]
"PBSMT Chaos followed at the border , as hundreds of migrants in a frontier ansammelten and Serb officers reacted with outrage .",A Supplemental Material,[0],[0]
NMT Chaos followed an avalanche as hundreds of thousands of immigrants fled to a mandsdom in answerage and Serbian officers responded with outrage . PBSMT+NMT,A Supplemental Material,[0],[0]
Chaos followed at the border as hundreds of immigrants gathered in a bush town and Serb officers reacted with outrage .,A Supplemental Material,[0],[0]
"Reference Chaos ensued at the border , as hundreds of migrants piled up in a no man ’s land , and Serbian officials reacted with outrage .
",A Supplemental Material,[0],[0]
"Source ” Zu unserer Reise gehörten viele dunkle Bahn- und Busfahrten , ebenso Hunger , Durst , Kälte und Angst ” , schrieb sie .",A Supplemental Material,[0],[0]
"PBSMT ” To our trip included many of the dark rail and bus rides , such as hunger , thirst , cold and fear , ” she wrote .",A Supplemental Material,[0],[0]
NMT ”,A Supplemental Material,[0],[0]
"During our trip , many included dark bus and bus trips , especially hunger , Durst , and cold fear , ” she wrote . PBSMT+NMT ”",A Supplemental Material,[0],[0]
"Our trip included many dark rail and bus journeys , as well as hunger , thirst , cold and fear , ” she wrote .",A Supplemental Material,[0],[0]
"Reference ” Our journey involved many dark train and bus rides , as well as hunger , thirst , cold and fear , ” she wrote .
",A Supplemental Material,[0],[0]
Table 7: Unsupervised translations: German-English.,A Supplemental Material,[0],[0]
"Examples of translations on the German-English pair of newstest 2016 using the PBSMT, NMT, and PBSMT+NMT.",A Supplemental Material,[0],[0]
"Machine translation systems achieve near human-level performance on some languages, yet their effectiveness strongly relies on the availability of large amounts of parallel sentences, which hinders their applicability to the majority of language pairs.",abstractText,[0],[0]
This work investigates how to learn to translate when having access to only large monolingual corpora in each language.,abstractText,[0],[0]
"We propose two model variants, a neural and a phrase-based model.",abstractText,[0],[0]
"Both versions leverage a careful initialization of the parameters, the denoising effect of language models and automatic generation of parallel data by iterative back-translation.",abstractText,[0],[0]
"These models are significantly better than methods from the literature, while being simpler and having fewer hyper-parameters.",abstractText,[0],[0]
"On the widely used WMT’14 English-French and WMT’16 German-English benchmarks, our models respectively obtain 28.1 and 25.2 BLEU points without using a single parallel sentence, outperforming the state of the art by more than 11 BLEU points.",abstractText,[0],[0]
"On low-resource languages like English-Urdu and English-Romanian, our methods achieve even better results than semisupervised and supervised approaches leveraging the paucity of available bitexts.",abstractText,[0],[0]
Our code for NMT and PBSMT is publicly available.1,abstractText,[0],[0]
Phrase-Based & Neural Unsupervised Machine Translation,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 559–564 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
559",text,[0],[0]
Extractive question answering (QA) is the task of selecting an answer phrase (span) to a question given an evidence document.,1 Introduction,[0],[0]
"Due to the easiness of evaluation (compared to generative QA) and the fine-grainess of the answer (compared to sentence-level QA), it has become one of the most popular QA tasks, driven by massive new datasets such as SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017).",1 Introduction,[0],[0]
"Current QA models heavily rely on explicitly learning the interaction between the evidence document and the question using neural attention mechanisms (Wang and Jiang, 2017; Xiong et al., 2017; Seo et al., 2017; Lee et al., 2016, inter alia), in which the model is fully aware of the question before or as it reads the document.",1 Introduction,[0],[0]
"As a result, despite significant advances, they have not led to the standalone representation of document discourse which is never-
∗Most work done during internship with Google AI.
",1 Introduction,[0],[0]
theless a key goal of research in reading comprehension.,1 Introduction,[0],[0]
"Furthermore, QA models that condition the document representation on a question have the practical scalability downside that the entire model should be re-applied on the same document for every question.
",1 Introduction,[0],[0]
"In this paper, we formalize a modular variant of the QA task, Phrase Indexed Question Answering (PIQA), that enforces complete independence between document encoder and question encoder (Figure 1).",1 Introduction,[0],[0]
"In PIQA, all documents are processed independently of any question to generate phrase index vectors (blue nodes in the figure) for each answer candidate (left boxes in the figure).",1 Introduction,[0],[0]
"Similarly, the questions are independently mapped to query vectors (red nodes in figure).",1 Introduction,[0],[0]
"Then, at inference time, the answer is obtained by retrieving the nearest indexed phrase vector to the query vector.",1 Introduction,[0],[0]
"Hence the algorithms aimed at tackling PIQA have the inherent benefit of modularity and scalability compared to current QA systems.
",1 Introduction,[0],[0]
"The task setup is analogous to how documents or sentences are retrieved in modern search engines via similarity search algorithms (Shrivastava and Li, 2015).",1 Introduction,[0],[0]
"Nevertheless, there is a key distinction that search engines index each document by its content, while PIQA requires one to index each phrase in documents by its context.
",1 Introduction,[0],[0]
We formally define the PIQA problem and provide baseline models for the new task.,1 Introduction,[0],[0]
"Our experiments show that the constraint introduced
by PIQA leads to meaningful standalone document representations and practical scalability advantage, demonstrating the significance of the new task.",1 Introduction,[0],[0]
"Moreover, there is still a large gap between the baselines and the unconstrained state of the art, showing that the task is yet far from being solved.",1 Introduction,[0],[0]
We have set up a leaderboard1for PIQA challenge and invite the research community to participate.,1 Introduction,[0],[0]
We currently support SQuAD and plan to expand to other datasets as well.,1 Introduction,[0],[0]
Reading comprehension.,2 Related Work,[0],[0]
"Massive reading comprehension question answering datasets (Hermann et al., 2015; Hill et al., 2016; Dhingra et al., 2017; Dunn et al., 2017) have driven a large number of successful neural approaches (Kadlec et al., 2016; Hu et al., 2017, inter alia).",2 Related Work,[0],[0]
"Choi et al. (2017); Chen et al. (2017); Clark and Gardner (2017); Min et al. (2018) tackled large-scale QA by using a fast, coarse model (e.g. TF-IDF) to retrieve few documents or sentences and then using a slower, accurate model to obtain the answer.",2 Related Work,[0],[0]
Salant and Berant (2018) proposed to minimize (but not prohibit) the influence of question when modeling the document.,2 Related Work,[0],[0]
"Similarly to ours, Lee et al. (2016) proposed to explicitly learn the representation for each answer candidate (phrase) in the document, but it was conditioned (dependent) on the question.",2 Related Work,[0],[0]
Sentence retrieval.,2 Related Work,[0],[0]
"A closely related task to ours is that of retrieving a sentence/paragraph in a corpus that answers the question (Tay et al., 2017).",2 Related Work,[0],[0]
A comprehensive survey for neural approaches in information retrieval literature is discussed in Mitra and Craswell (2017).,2 Related Work,[0],[0]
"We note that our problem is focused on phrasal answer extraction, which presents a unique challenge over sentence retrieval—the need for context-based representation as opposed to the content-based representation in the sentence-retrieval literature.",2 Related Work,[0],[0]
Language representation.,2 Related Work,[0],[0]
"Recently there has been a growing interest in developing natural language representations that can be transferred across tasks (Vendrov et al., 2016; Wieting et al., 2016; Conneau et al., 2017, inter alia).",2 Related Work,[0],[0]
"In particular, SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2017) encourage architectures that first encode the hypothesis and the premise independently before a comparator neu-
1nlp.cs.washington.edu/piqa
ral network is applied.",2 Related Work,[0],[0]
Our proposed problem shares similar traits but has a stronger constraint that only inner product comparison is allowed and one needs to model phrases instead of complete sentences.,2 Related Work,[0],[0]
Extractive question answering is the task of obtaining the answer â to a questionQ = {q1 . . .,3 Phrase-Indexed Question Answering,[0],[0]
qn} given an evidence document D = {d1 . . .,3 Phrase-Indexed Question Answering,[0],[0]
"dm}, where the answer â = (s, e) indicates the start and end of a span in the document.",3 Phrase-Indexed Question Answering,[0],[0]
The task is often formulated as learning the probabilistic distribution of the answer given the question and the document.,3 Phrase-Indexed Question Answering,[0],[0]
"In existing literature (Section 2), the distribution is mainly featurized by Pr(a|Q,D) ∝ exp(Fθ(Q,D, a)) where Fθ could be any realvalued scoring function parameterized by θ.",3 Phrase-Indexed Question Answering,[0],[0]
"Once θ is learned, the prediction â is obtained by
â = argmax a Fθ(Q,D, a).",3 Phrase-Indexed Question Answering,[0],[0]
"(1)
So far, most competitive designs of Fθ(Q,D, a) make use of attention connections between the words in Q and D. As a result, these models cannot yield a query independent representation of the document D. It is subsequently not possible to independently assess the document understanding capability of the model.",3 Phrase-Indexed Question Answering,[0],[0]
"Furthermore, Fθ(Q,D, a) needs to be re-computed for the entire document for every new question.",3 Phrase-Indexed Question Answering,[0],[0]
"We believe that this inefficiency precludes all current models as the candidates for end-to-end QA systems.
",3 Phrase-Indexed Question Answering,[0],[0]
We propose a new task—,3 Phrase-Indexed Question Answering,[0],[0]
Phrase-Indexed Question Answering (PIQA)—that addresses these issues.,3 Phrase-Indexed Question Answering,[0],[0]
"We enforce the decomposability of Fθ into two exclusive functions Gθ(Q), Hθ(D, a) ∈ Rk.",3 Phrase-Indexed Question Answering,[0],[0]
"The answer distribution is then modeled by Pr(a|Q,D) ∝ exp(Gθ(Q) •",3 Phrase-Indexed Question Answering,[0],[0]
"Hθ(D, a)), where • is the inner product.",3 Phrase-Indexed Question Answering,[0],[0]
"The prediction is obtained by
â = argmax a
Gθ(Q) •Hθ(D, a).",3 Phrase-Indexed Question Answering,[0],[0]
"(2)
",3 Phrase-Indexed Question Answering,[0],[0]
"In this setting, the document encoder Hθ learns models the document independently of the question.",3 Phrase-Indexed Question Answering,[0],[0]
"Successful question answering models that follow the structure of PIQA will have two important advantages over current QA models: full document comprehension and scalablity.
",3 Phrase-Indexed Question Answering,[0],[0]
Full document comprehension.,3 Phrase-Indexed Question Answering,[0],[0]
"Language understanding ability is widely associated with learning a good standalone representation of text (or its
components such as phrases) independent of the end task (Bowman et al., 2015).",3 Phrase-Indexed Question Answering,[0],[0]
"Under PIQA constraints, the document encoder Hθ learns the representation of the answer candidate phrases a in the document D independent of the question.",3 Phrase-Indexed Question Answering,[0],[0]
"In order to correctly answer questions, these phrase representations (index vectors) need to correctly encode their meaning with respect to their context.",3 Phrase-Indexed Question Answering,[0],[0]
"Therefore, PIQA constraint enforces evaluating research in document comprehension and phrase representation learning.
",3 Phrase-Indexed Question Answering,[0],[0]
Scalability.,3 Phrase-Indexed Question Answering,[0],[0]
"Models that adhere to the PIQA constraint only need to be run once for each document, regardless of the number of questions asked.",3 Phrase-Indexed Question Answering,[0],[0]
"To answer a question, the model then just needs to encode the question and compare it to each of the answer candidates via the inner product in Equation 2.",3 Phrase-Indexed Question Answering,[0],[0]
"Implemented naively, computing a single inner product for each answer candidate is more efficient than building a new document encoding; after the documents are pre-encoded, Equation 2 is O(k) time per word where k is the vector size (most neural models require O(k2) per word for matrix multiplications).
",3 Phrase-Indexed Question Answering,[0],[0]
"More importantly, PIQA also permits an approximate solution in sublinear time using asymmetric locality-sensitive hashing (aLSH) (Shrivastava and Li, 2014, 2015), through which Equation 2 can be approximated for N answer candidates with O(kNρ logN) time, where ρ < 1 is a function of the approximation factor and the properties of the hash functions.",3 Phrase-Indexed Question Answering,[0],[0]
"We argue that this type of approach will be essential for the development of real world QA systems, where the number of potential answers N is extremely large.",3 Phrase-Indexed Question Answering,[0],[0]
"We introduce several baselines for PIQA that are motivated by related literature.
",4 Baseline Models,[0],[0]
"For all (neural) baselines, we represent the words in D and Q with one of three embedding mechanisms: CharCNN (Kim, 2014) +",4 Baseline Models,[0],[0]
"GloVe (Pennington et al., 2014), and ELMo (Peters et al., 2018).",4 Baseline Models,[0],[0]
"We follow the majority of the related literature and apply bidirectional LSTMs (Hochreiter and Schmidhuber, 1997) to these embeddings to build the context-aware representations of the document D = {d1 . .",4 Baseline Models,[0],[0]
.dm},4 Baseline Models,[0],[0]
and question Q = {q1 . .,4 Baseline Models,[0],[0]
".qn}, where the forward & backward LSTM outputs are concatenated to get a single word representation, i.e. di,qi ∈ R2k
where k is the hidden state size of LSTMs.",4 Baseline Models,[0],[0]
PIQA disallows cross-attention between document and question.,4 Baseline Models,[0],[0]
"However, we can still benefit from self-attention, which has become crucial for machine translation (Vaswani et al., 2017) and QA (Huang et al., 2018; Yu et al., 2018).",4 Baseline Models,[0],[0]
"In all of our baselines, each variable-length question is collapsed into a fixed length vector via the sum qSA",4 Baseline Models,[0],[0]
= ∑ i uiqi where u = {u1 . . .,4 Baseline Models,[0],[0]
un} is a vector containing a single weight for each word in the question.,4 Baseline Models,[0],[0]
"Similarly, we experiment with document side self attention to represent each document word dj as a weighted sum of itself and all neighboring words dSAj = ∑ i h j idj .",4 Baseline Models,[0],[0]
"The weight vectors u and hj are calculated as
u = softmaxi(w >qi)
hj = softmaxi(Rθ(D, j)",4 Baseline Models,[0],[0]
>,4 Baseline Models,[0],[0]
"Kθ(D, i))
where Rθ, and Kθ are trainable neural networks with the same ouptut size, and w ∈",4 Baseline Models,[0],[0]
R2k is a trainable weight vector.,4 Baseline Models,[0],[0]
We use independent BiLSTMs with hidden state size k (i.e. the output size is 2k) to model both Rθ and Kθ.,4 Baseline Models,[0],[0]
"That is, Rθ(D, j) is the j-th output of BiLSTM on top of D, and we similarly define Kθ with unshared parameters.
",4 Baseline Models,[0],[0]
"For all (neural) baselines, the question is represented using the concatenation of two copies of qSA, one that should have high inner product with the vector for the answer’s start span and another that should have high inner product with the vector for the answer’s end.",4 Baseline Models,[0],[0]
"Thus, Equation 2’s Gθ(Q) =",4 Baseline Models,[0],[0]
"[q SA s ,q SA e ] where the subscripts s (start) and e (end) imply that different sets of parameters were used.",4 Baseline Models,[0],[0]
"Now we define several baselines.
",4 Baseline Models,[0],[0]
LSTM baseline.,4 Baseline Models,[0],[0]
"An answer candidate a = (s, e) is represented using the LSTM outputs at its endpoints: from Equation 2, Hθ(D, (s, e))",4 Baseline Models,[0],[0]
=,4 Baseline Models,[0],[0]
"[ds,de] ∈ R4k and Gθ(Q) =",4 Baseline Models,[0],[0]
"[qSAs ,qSAe ] ∈ R4k.
LSTM+SA baseline.",4 Baseline Models,[0],[0]
"The LSTM outputs are augmented with the endpoint representations that come out of the document’s self-attention (SA): Hθ(D, (s, e))",4 Baseline Models,[0],[0]
=,4 Baseline Models,[0],[0]
"[ds,d SA s ,de,d SA e ] ∈ R8k and Gθ(Q) =",4 Baseline Models,[0],[0]
"[q SA s1 ,q SA s2 ,q SA e1 ,q SA e2 ] ∈ R8k.
TF-IDF.",4 Baseline Models,[0],[0]
"We lastly include a purely TF-IDFbased model, where each answer candidate phrase is associated with a bag of neighbor words within a distance of 7.",4 Baseline Models,[0],[0]
Then the BOW vector is normalized via TF-IDF and indexed.,4 Baseline Models,[0],[0]
"When the query comes in, its TF-IDF vector is queried on the indexed phrases to yield the answer.
",4 Baseline Models,[0],[0]
"For training the (neural) models, we minimize the negative log probability of getting the correct answer: the loss function for each example (D,Q, a∗) is L(θ) =",4 Baseline Models,[0],[0]
"− log Pr(a∗|D,Q) where a∗ is the correct answer.",4 Baseline Models,[0],[0]
We impose the independence restrictions from PIQA on the Stanford Question Answering Dataset2.,5 Experiments,[0],[0]
We only consider answer spans with length ≤ 7.,5 Experiments,[0],[0]
"We use the hidden state size (k) of 128, which results in a 512D (4k) and 1024D (8k) vector for each phrase in LSTM and LSTM+SA, respectively.",5 Experiments,[0],[0]
"The default embedding model is CharCNN concatenated with 200D GloVe, with an option to append ELMo vectors following the same setup for SQuAD experiments discussed in Peters et al. (2018).",5 Experiments,[0],[0]
"We use a batch size of 64 and train for 20 epochs with the default Adam optimizer (Kingma and Ba, 2015), and take the best model on the validation set during training.
Results.",5 Experiments,[0],[0]
Table 1 shows the results for the PIQA baselines (top) and the unconstrained state of the art (bottom).,5 Experiments,[0],[0]
"First, the TF-IDF model performs poorly, which signifies the limitations of traditional document retrieval models for the task.",5 Experiments,[0],[0]
"Second, we note that the addition of self-attention makes a significant impact on results, improving F1 by 2.6%.",5 Experiments,[0],[0]
"Next, we see that adding ELMo gives 3.7% and 2.9% improvement on F1 for LSTM and LSTM+SA models, respectively.",5 Experiments,[0],[0]
"Lastly, the best PIQA baseline model is 11.7% higher than the first (unconstrained) baseline model (Rajpurkar et al., 2016) and 26.6% lower than the state of the art (Yu et al., 2018).",5 Experiments,[0],[0]
"This gives us a reasonable starting point of the new task and a significant gap
2PIQA paradigm can be also extended to other extractive QA datasets.
to close for future work.
",5 Experiments,[0],[0]
Phrase representations.,5 Experiments,[0],[0]
"Since PIQA models encode all answer candidates into the same space, we expect similar answer candidates to have high inner products with one another.",5 Experiments,[0],[0]
"Table 2 shows pairs of answer candidates that come from different documents in SQuAD, but that have similar encodings (high inner product).",5 Experiments,[0],[0]
We observe that phrase representations learned through the PIQA task capture different interesting characteristics of the phrases.,5 Experiments,[0],[0]
"In all three rows, we can see that the phrase pairs seem to fit into natural categories: national, or multi-national organizational constructs; mechanical engines; and mechanical properties, respectively.",5 Experiments,[0],[0]
This suggests that the model has learned interesting typing information above the word level.,5 Experiments,[0],[0]
The second and third rows also indicate that the model has learned a rich representation of context.,5 Experiments,[0],[0]
"This is particularly obvious in the third row where the two phrases are lexically dissimilar, but preceded by the similar contexts ‘primarily accomplished through’ and ‘directly derived from’.",5 Experiments,[0],[0]
"We believe that this analysis, while not complete, points toward exciting future lines of work in learning highly contextualized phrase representations through question answering.
Scalability.",5 Experiments,[0],[0]
"PIQA can also gain massive execution time speedups once the documents are preencoded: in our simple benchmark on a consumergrade CPU and NumPy (for LSTM+SA model, 1024D vectors), one can easily perform exact search over 1 million document words per second.",5 Experiments,[0],[0]
"BiDAF (Seo et al., 2017), an open-sourced and relatively light QA model reaching 77.5% F1 (66.5% EM), can process less than 1k document words per second with an equivalent computing power (after pre-encoding the document as much as possible), which is more than 1,000x slower.3
3The difference will be even higher with a dedicated similarity search package such as Faiss (Johnson et al., 2017) or approximate search (Section 3).
",5 Experiments,[0],[0]
It is also important to consider the memory cost for storing a vector representation of each of the answer candidates.,5 Experiments,[0],[0]
We train an independent single-layer perceptron classifier that predicts whether the phrase encoding is likely to be a good one.,5 Experiments,[0],[0]
"By varying a threshold on the score assigned by this classifier, we can filter answer candidates prior to storage.",5 Experiments,[0],[0]
Figure 2 illustrates the trade-off between accuracy and memory (measured in mean number of vectors per document word) resulting from this filtering procedure for the LSTM+SA model.,5 Experiments,[0],[0]
We observe that 1.3 vectors (candidates) per word on average reaches > 98% of the model’s F1 accuracy.,5 Experiments,[0],[0]
"This is equivalent to 5.2 KB per word with 1024D (4 KB) float vectors, or around 15 TB for the entire English Wikipedia (3 billion words).",5 Experiments,[0],[0]
Future work will also involve creating a better classifier (i.e. improving the trade-off curve in Figure 2) for determining which phrase vectors to store.,5 Experiments,[0],[0]
"We introduced Phrase-Indexed Question Answering (PIQA), a new variant of the extractive question answering task that requires documents and question encoded completely independently and that they only interact each other via inner product.",6 Conclusion and Future Work,[0],[0]
We argued that building a question-agnostic document encoder for question answering should be an important consideration for those in the QA community with the research goal of learning a model that reads and comprehends documents.,6 Conclusion and Future Work,[0],[0]
"Furthermore, the imposed constraint of the task implies a sublinear scalability benefit.",6 Conclusion and Future Work,[0],[0]
"Given that SQuAD models have recently outperformed hu-
mans, PIQA formulation motivates a new challenge for which we hope that the community’s effort gradually closes the gap between our constrained baselines and the unconstrained models.",6 Conclusion and Future Work,[0],[0]
"This research was supported by ONR (N0001418-1-2826), NSF (IIS 1616112), Allen Distinguished Investigator Award, and gifts from Google, Allen Institute for AI, Amazon, and Bloomberg.",Acknowledgments,[0],[0]
We thank the anonymous reviewers for their helpful comments.,Acknowledgments,[0],[0]
We formalize a new modular variant of current question answering tasks by enforcing complete independence of the document encoder from the question encoder.,abstractText,[0],[0]
This formulation addresses a key challenge in machine comprehension by requiring a standalone representation of the document discourse.,abstractText,[0],[0]
It additionally leads to a significant scalability advantage since the encoding of the answer candidate phrases in the document can be pre-computed and indexed offline for efficient retrieval.,abstractText,[0],[0]
"We experiment with baseline models for the new task, which achieve a reasonable accuracy but significantly underperform unconstrained QA models.",abstractText,[0],[0]
"We invite the QA research community to engage in Phrase-Indexed Question Answering (PIQA, pika) for closing the gap.",abstractText,[0],[0]
The leaderboard is at: nlp.cs.washington.,abstractText,[0],[0]
Phrase-Indexed Question Answering: A New Challenge for Scalable Document Comprehension,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3729–3738 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3729",text,[0],[0]
"Following the success of word embeddings (Bengio et al., 2003; Mikolov et al., 2013), one of NLP’s next challenges has become the hunt for universal sentence encoders.",1 Introduction,[0],[0]
"The goal is to learn a general-purpose sentence encoding model on a large corpus, which can be readily transferred to other tasks.",1 Introduction,[0],[0]
"The learned sentence representations are able to generalize to unseen combination of words, which makes them highly desirable for
downstream NLP tasks, especially for those with relatively small datasets.
",1 Introduction,[0],[0]
"Previous models for sentence encoding typically rely on Recurrent Neural Networks (RNNs) (Hochreiter and Schmidhuber, 1997; Chung et al., 2014) or Convolutional Neural Networks (CNNs) (Kalchbrenner et al., 2014; dos Santos and Gatti, 2014; Kim, 2014; Mou et al., 2016) to produce context-aware representation.",1 Introduction,[0],[0]
"RNNs encode a sentence by reading words in sequential order, they are capable of learning long-term dependencies but are hard to parallelize and not time-efficient.",1 Introduction,[0],[0]
"CNNs focus on local or positioninvariant dependencies but do not perform well on many tasks (Shen et al., 2017).
",1 Introduction,[0],[0]
"Fully attention-based neural networks have attracted wide interest recently, because they can model both dependencies while being more parallelizable and requiring significantly less time to train.",1 Introduction,[0],[0]
"Vaswani et al. (2017) proposed the multihead attention to project a sentence to multiple semantic subspaces, then apply self-attention in each subspace and concatenate the attention results.",1 Introduction,[0],[0]
"Shen et al. (2017) proposed the directional self-attention, they apply forward and backward masks to the alignment score matrix to encode temporal order information, and computed attention at feature level to select the features that can best describe the word’s meaning in given context.",1 Introduction,[0],[0]
"Effective as their models are, the memory required to store the alignment scores of all the token pairs grows quadratically with the sentence length.",1 Introduction,[0],[0]
"Furthermore, the syntactic property that is intrinsic to natural language is not considered at all.
",1 Introduction,[0],[0]
"Language is inherently tree structured, and the meaning of a sentence comes largely from composing the meanings of subtrees (Chomsky, 1957).",1 Introduction,[0],[0]
"Previous syntactic tree-based sentence encoders (Socher et al., 2013; Tai et al., 2015) mainly rely on recursive networks.",1 Introduction,[0],[0]
"Although the composition-
ality can be explicitly modeled, their models need expensive recursion computation and are hard to be trained by batched gradient descent methods.
",1 Introduction,[0],[0]
"In this paper, we propose the Phrase-level SelfAttention Networks (PSAN), for RNN/CNN-free sentence encoding, it inherits all the advantages of fully attention-based models while requires much less memory consumption.",1 Introduction,[0],[0]
"In addition, syntactic information can be incorporated into the model more easily.",1 Introduction,[0],[0]
"In our model, every sentence is split into multiple phrases based on parse tree, selfattention is performed at the phrase level instead of the sentence level, thus the memory consumption reduces rapidly as the number of phrases increases.",1 Introduction,[0],[0]
"Furthermore, a gated memory component is employed to refine word representations hierarchically by incorporating longer-term context dependencies.",1 Introduction,[0],[0]
"As a result, syntactic information can be integrated into the model without expensive recursion computation.",1 Introduction,[0],[0]
"At last, multi-dimensional attention is applied on the refined word representations to obtain the final sentence representation.
",1 Introduction,[0],[0]
"Following Conneau et al. (2017), we trained our sentence encoder on the SNLI (Bowman et al., 2015) dataset, and evaluate the quality of the obtained universal sentence representations on a wide range of transfer tasks.",1 Introduction,[0],[0]
"The SNLI dataset is extremely suitable for training sentence encoders because it is the largest high-quality humanannotated dataset that involves reasoning about the semantic relationships within sentences.
",1 Introduction,[0],[0]
"The main contributions of our work can be summarized as follows:
• We propose the Phrase-level Self-Attention mechanism (PSA) for contextualization.",1 Introduction,[0],[0]
"The memory consumption can be reduced because self-attention is performed at the phrase level instead of the sentence level.
",1 Introduction,[0],[0]
"• A gated memory updating mechanism is proposed to refine each word representation hierarchically by incorporating different levels of contextual information along the parse tree.
",1 Introduction,[0],[0]
• Our proposed PSAN model outperforms the state-of-the-art supervised sentence encoders on a wide range of transfer tasks with significantly less memory consumption.,1 Introduction,[0],[0]
"In this section, we introduce the Phrase-level SelfAttention Networks (PSAN) for sentence encod-
ing.",2 Proposed Model,[0],[0]
A phrase is a group of words that carry a specific idiomatic meaning and function as a constituent in the syntax of a sentence.,2 Proposed Model,[0],[0]
Words in a phrase are syntactically and semantically related to each other.,2 Proposed Model,[0],[0]
"Therefore, it can be advantageous to learn a context-aware representation inside a phrase while filtering out information from outside the phrase using self-attention mechanism.",2 Proposed Model,[0],[0]
"In an attempt to better utilize the tree structure which is intrinsic to language, we propose the gated memory updating mechanism to combine different levels of context information.",2 Proposed Model,[0],[0]
"At last, an attention mechanism is utilized to summarize all the token representations into a fixed-length sentence vector.",2 Proposed Model,[0],[0]
The phrase structure organizes words into nested constituents which can be successively divided into their parts as we move down the constituencybased parse trees.,2.1 Phrase Division,[0],[0]
One phrase division shows only one aspect of context dependency.,2.1 Phrase Division,[0],[0]
"In order to capture different levels of context dependencies, we can split a sentence at different granularities.",2.1 Phrase Division,[0],[0]
"The number of levels T is a hyper-parameter to be tuned.
",2.1 Phrase Division,[0],[0]
"We can break down the nodes at T different layers in the parse tree to capture T levels of context dependencies1, as illustrated in Figure 1.",2.1 Phrase Division,[0],[0]
This is the core component of our model.,2.2 Phrase-level Self-Attention,[0],[0]
It aims to learn a context-aware representation for each token inside a phrase.,2.2 Phrase-level Self-Attention,[0],[0]
"In order to filter out information that is semantically or syntactically distant, self-attention is performed at the phrase level instead of the sentence level.
",2.2 Phrase-level Self-Attention,[0],[0]
"Similar to directional self-attention network (DiSAN) (Shen et al., 2017), Phrase-level SelfAttention uses multi-dimensional attention to compute the alignment score for each dimension of token embedding.",2.2 Phrase-level Self-Attention,[0],[0]
"Therefore, it can select the features that can best describe a word’s specific meaning in any given context.
",2.2 Phrase-level Self-Attention,[0],[0]
"Given a phrase P ∈ Rl×d represented as a sequence of word embeddings [p1, . . .",2.2 Phrase-level Self-Attention,[0],[0]
",pl], where l is the length of the phrase and d is the dimension of word embedding representation, we first compute the alignment score for each token pair in the
1To avoid the situation that the produced phrases are too small, a phrase will not be further divided if its length is smaller than 4.
phrase: aij = σ",2.2 Phrase-level Self-Attention,[0],[0]
"( W a1pi +W a2pj + b a ) +Mij
Mij = { 0, i 6= j −∞, i = j
(1)
where σ (·) is an activation function, W a1,W a2 ∈ Rd×d and ba ∈ Rd are parameters to be learned, and M is a diagonal-diabled mask (Hu et al., 2017) that aims to prevent a word from being aligned with itself.
",2.2 Phrase-level Self-Attention,[0],[0]
"The output of the attention mechanism is a weighted sum of embeddings from all tokens for each token in the phrase:
p̃i = l∑ j=1",2.2 Phrase-level Self-Attention,[0],[0]
"[ exp (aij)∑l k=1 exp (aik) pj ] (2)
where means point-wise product.",2.2 Phrase-level Self-Attention,[0],[0]
"Note that the alignment score for each token pair is a vector rather than a scalar in the multi-dimensional attention.
",2.2 Phrase-level Self-Attention,[0],[0]
The final output of Phrase-level Self-Attention is obtained by comparing each input representation with its attention-weighted counterpart.,2.2 Phrase-level Self-Attention,[0],[0]
We use a comparison function based on absolute difference and element-wise multiplication which was similar to Wang and Jiang (2016).,2.2 Phrase-level Self-Attention,[0],[0]
"This comparison function has the advantage of measuring the semantic similarity or relatedness of two sequences.
",2.2 Phrase-level Self-Attention,[0],[0]
ci = σ,2.2 Phrase-level Self-Attention,[0],[0]
"(W c [|pi − p̃i| ;pi p̃i] + bc) (3)
where",2.2 Phrase-level Self-Attention,[0],[0]
W c ∈ Rd×2d,2.2 Phrase-level Self-Attention,[0],[0]
and ba ∈ Rd are parameters to be learned.,2.2 Phrase-level Self-Attention,[0],[0]
"ci is the representation for the i-th word in the phrase that captures local dependencies within the phrase.
",2.2 Phrase-level Self-Attention,[0],[0]
"At last, we put together the Phrase-level SelfAttention results for non-overlapping phrases from the same phrase division of a sentence.",2.2 Phrase-level Self-Attention,[0],[0]
For the t-th phrase division we can get C(t) =,2.2 Phrase-level Self-Attention,[0],[0]
"[c1, . .",2.2 Phrase-level Self-Attention,[0],[0]
.,2.2 Phrase-level Self-Attention,[0],[0]
", cls ], the phrase-level self-attention results for the sentence from the t-th layer split, where ls is the sentence length.",2.2 Phrase-level Self-Attention,[0],[0]
Above describes the Phrase-level Self-Attention (PSA) for one split of the parse tree.,2.3 Gated Memory Updating,[0],[0]
The parse tree can be split at different granularities.,2.3 Gated Memory Updating,[0],[0]
We propose a novel gated memory updating mechanism to refine each word representation hierarchically with longer-term dependencies captured in a larger granularity.,2.3 Gated Memory Updating,[0],[0]
"Inspired by the idea of adaptive gate in highway networks (Srivastava et al., 2015), our memory mechanism add a gate to original memory networks (Weston et al., 2014; Sukhbaatar et al., 2015).",2.3 Gated Memory Updating,[0],[0]
"This gate has the ability to determine the importance of the new input and the original memory in the memory updating.
C(t) = PSA ( M (t−1) ) G(t) = sigmoid ( W g",2.3 Gated Memory Updating,[0],[0]
[ M (t−1);C(t) ],2.3 Gated Memory Updating,[0],[0]
"+ bg
) M (t) = G(t) σ",2.3 Gated Memory Updating,[0],[0]
( Wm [ M (t−1);C(t) ],2.3 Gated Memory Updating,[0],[0]
"+ bm
) (4)
whereW g,Wm ∈ Rd×2d and bg,",2.3 Gated Memory Updating,[0],[0]
bm ∈ Rd are parameters to be learned.,2.3 Gated Memory Updating,[0],[0]
"Note that in order to share representation power and to reduce the number of parameters, the parameters of gated memory updating are shared among different layers.",2.3 Gated Memory Updating,[0],[0]
"In this layer, self-attention mechanism is employed to summarize the refined representation of a sentence into a fixed-length vector.",2.4 Sentence Summarization,[0],[0]
The selfattention mechanism can explore the dependencies among tokens within the whole sentence.,2.4 Sentence Summarization,[0],[0]
"As a result, global dependencies can also be incorporated in the model.
",2.4 Sentence Summarization,[0],[0]
ei =W e2σ ( W e1m (T ),2.4 Sentence Summarization,[0],[0]
i + b e1 ) +,2.4 Sentence Summarization,[0],[0]
"be2
v = l∑ i=1",2.4 Sentence Summarization,[0],[0]
[ exp (ei)∑l j=1 exp (ej) m(T ),2.4 Sentence Summarization,[0],[0]
"i ] (5) where W g,Wm ∈ Rd×d and bg, bm ∈ Rd are parameters to be learned.",2.4 Sentence Summarization,[0],[0]
"After this step, the refined context-aware sentence representation is compressed into a fixed-length vector.",2.4 Sentence Summarization,[0],[0]
"In this section, we conduct a plethora of experiments to study the effectiveness of the PSAN model.",3 Experiments,[0],[0]
"Following Conneau et al. (2017), we train our sentence encoder using the SNLI dataset, and evaluate it across a variety of NLP tasks including sentence classification, natural language inference and sentence textual similarity.",3 Experiments,[0],[0]
"300-dimensional GloVe (Pennington et al., 2014) word embeddings (Common Crawl, uncased) are used to represent words.",3.1 Model Configuration,[0],[0]
"Following Parikh et al. (2016), out-of-vocabulary words are hashed to one of 128 random embeddings initialized by uniform distribution between (-0.05, 0.05).",3.1 Model Configuration,[0],[0]
All the word embeddings remain fixed during training.,3.1 Model Configuration,[0],[0]
Hidden dimension d is set to 300.,3.1 Model Configuration,[0],[0]
"All other parameters are initialized with Glorot normal initialization (Glorot and Bengio, 2010).",3.1 Model Configuration,[0],[0]
"Activation function σ (·) is ELU (Clevert et al., 2015) if not specified.",3.1 Model Configuration,[0],[0]
Minibatch size is set to 16.,3.1 Model Configuration,[0],[0]
The number of levels T is fixed to 3 in all of our experiments.,3.1 Model Configuration,[0],[0]
"The syntactic parse trees of SNLI are provided within the corpus. parse trees for all test corpus are produced by the Stanford PCFG Parser 3.5.2 (Klein and Manning, 2003), the same parser that produced parse trees for the SNLI dataset.
",3.1 Model Configuration,[0],[0]
"To train the model, Adadelta optimizer (Zeiler, 2012) with a learning rate of 0.75 is used on the SNLI dataset.",3.1 Model Configuration,[0],[0]
"The dropout (Srivastava et al., 2014) rate and L2 regularization weight decay factor γ are set to 0.5 and 5e-5.",3.1 Model Configuration,[0],[0]
"To test the model, the SentEval toolkit (Conneau and Kiela, 2018) is used as the evaluation pipeline for fairer comparison.",3.1 Model Configuration,[0],[0]
"Natural language inference (NLI) is a fundamental task in the field of natural language processing that involves reasoning about the semantic relationship between two sentences, which makes it a suitable task to train sentence encoding models.
",3.2 Training Setting,[0],[0]
"We conduct experiments on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015).",3.2 Training Setting,[0],[0]
"The dataset has 570k human-annotated sentence pairs, each labeled with one of the following pre-defined relationships: Entailment (the premise entails the hypothesis), Contradiction (they contradict each other) and Neutral (they are irrelevant).",3.2 Training Setting,[0],[0]
"Following previous work (Bowman et al., 2015; Mou et al., 2016), we remove the instances which annotators can not reach consensus on.",3.2 Training Setting,[0],[0]
"In this way we get 549367/9842/9824 sentence pairs for train/validation/test set.
",3.2 Training Setting,[0],[0]
"Following the siamese architecture (Bromley et al., 1993), we apply PSAN to both the premise and the hypothesis with their parameters tied.",3.2 Training Setting,[0],[0]
vp and vh are fixed-length vector representations for the premise and the hypothesis respectively.,3.2 Training Setting,[0],[0]
"The final sentence-pair representation is formed by concatenating the original vectors with the absolute difference and element-wise multiplication between them:
vinp =",3.2 Training Setting,[0],[0]
[ vp;vh; ∣∣∣vp − vh∣∣∣ ;vp vh] (6),3.2 Training Setting,[0],[0]
"At last, we feed the sentence-pair representation vinp into a two layer feed-forward network and use a softmax layer to make the classification.",3.2 Training Setting,[0],[0]
This is the de facto scheme for sentence encoders trained on SNLI.,3.2 Training Setting,[0],[0]
"(Mou et al., 2016; Liu et al., 2016; Shen et al., 2017)",3.2 Training Setting,[0],[0]
"To show the modeling capacity and robustness of our proposed model, we evaluate our model across a wide range of tasks that can be solved purely based on the encoded semantics.",3.3 Evaluation Setting,[0],[0]
"The set of tasks
was selected based on what appears to be the community consensus regarding the appropriate evaluations for universal sentence representations.",3.3 Evaluation Setting,[0],[0]
"To facilitate comparison, we use the same sentence evaluation tool as Conneau et al. (2017) to automate evaluation on all the tasks mentioned in this paper.
",3.3 Evaluation Setting,[0],[0]
"The transfer tasks used in evaluation can be concluded in the following classes: sentence classification (MR, CR, MPQA, SUBJ, SST2, SST5, TREC), natural language inference (SICKE, SICK-R), semantic relatedness (STS14) and paraphrase detection (MRPC).",3.3 Evaluation Setting,[0],[0]
Table 1 presents some statistics about the datasets 2.,3.3 Evaluation Setting,[0],[0]
"We compare our model with the following supervised sentence encoders:
• BiLSTM-Max (Conneau et al., 2017) is a simple but effective baseline that performs max-pooling over a bi-directional LSTM.
",3.4 Baselines,[0],[0]
"• AdaSent (Zhao et al., 2015) forms a hierarchy of representations from words to phrases and then to sentences through recursive gated local composition of adjacent segments.
",3.4 Baselines,[0],[0]
"• TBCNN (Mou et al., 2015) is a tree-based CNN model where convolution is applied over the parse tree.
",3.4 Baselines,[0],[0]
"2For further information on the datasets, please refer to Conneau et al. (2017).
",3.4 Baselines,[0],[0]
"• DiSAN (Shen et al., 2017) is composed of a directional self-attention block with temporal order encoded, and a multi-dimensional attention that compresses the sequence into a vector representation.",3.4 Baselines,[0],[0]
Experiment results of our model and four baselines are shown in Table 2.,4.1 Overall Performance,[0],[0]
Micro and macro accuracies are two composite indicators for evaluating transfer performance of tasks whose metric is classification accuracy.,4.1 Overall Performance,[0],[0]
Macro accuracy is the proportion of true results in the population of instances from all tasks.,4.1 Overall Performance,[0],[0]
"Micro accuracy is the arithmetic mean of dev accuracies for each task.
",4.1 Overall Performance,[0],[0]
"PSAN achieves the state-of-the-art performance
with considerably fewer parameters, outperforming a RNN-based model, a CNN-based model, a fully attention-based model and a model that utilize syntactic information.",4.1 Overall Performance,[0],[0]
"Especially when compared with previous best model BiLSTM-Max, PSAN can outperform their model with only 5% of their parameter numbers, demonstrating the effectiveness of our model at extracting semantically important information from a sentence.
",4.1 Overall Performance,[0],[0]
"In Table 3, we compare our model with baseline sentence encoders in each transfer task.",4.1 Overall Performance,[0],[0]
PSAN can consistently outperform the baselines in almost every task considered.,4.1 Overall Performance,[0],[0]
"On the SICK dataset, which can be seen as an out-domain version of SNLI, our model can outperform the baselines by a large margin, demonstrating the semantic relationship learned on the SNLI can be well transfered to other domains.",4.1 Overall Performance,[0],[0]
"On the STS14 dataset, where sentence vectors can be more directly measured by the cosine distance, our model can also achieve the stateof-the-art performance, indicating that our learned sentence representations are of high quality.",4.1 Overall Performance,[0],[0]
"For thorough comparison, we implement seven extra baselines to analyze the improvements con-
tributed by each part of our PSAN model:
• PSA on the first/second/third layer only only uses the Phrase-level Self-Attention at the first/second/third layer of phrase division.
",4.2 Ablation Study,[0],[0]
"• w/o PSA applies self-attention at the sentence level and uses the gated memory updating mechanism to refine each token representation hierarchically.
",4.2 Ablation Study,[0],[0]
"• w/o syntactic division divides each sentence equally into small blocks, and applies PSA within each block.",4.2 Ablation Study,[0],[0]
"The number of blocks equals the number of phrases in that layer.
",4.2 Ablation Study,[0],[0]
"• w/o gated memory updating concatenates the outputs of Phrase-level Self-Attention from three layers of phrase division and feeds the result to a feed-forward layer.
",4.2 Ablation Study,[0],[0]
"• w/o both applies self-attention at the sentence level, and uses sentence summarization to summarize the attention results into a fixed length vector.
",4.2 Ablation Study,[0],[0]
The results are listed in Table 4.,4.2 Ablation Study,[0],[0]
"We can see that (2) performs best among (1), (2) and (3), demonstrating that the second layer split is more expressive, because the number of words per phrase in the second layer is the most suitable.",4.2 Ablation Study,[0],[0]
"It is neither too small to capture context dependencies, nor too large to filter out irrelevant noise.",4.2 Ablation Study,[0],[0]
"(8) outperforms (1), (2) and (3), showing that combining phraselevel information from different granularities can further improve performance.
",4.2 Ablation Study,[0],[0]
We also experiment on models where the alignment matrix is calculated at the sentence level or at the syntactic-irrelevant block level.,4.2 Ablation Study,[0],[0]
"(5) performs quite well, showing that hierarchical refinement on smaller units can bring about reasonable
performance gain.",4.2 Ablation Study,[0],[0]
"(8) outperforms (4) and (5), demonstrating syntactic information helps in sentence representation.
",4.2 Ablation Study,[0],[0]
"When comparing (6) with (8), we can tell that gated memory updating is a better method when used to refine token representation along the parse tree.",4.2 Ablation Study,[0],[0]
"We assume that memory updating resembles the tree structure of language in that larger phrase is composed in the knowledge of how smaller phrases are composed inside it.
",4.2 Ablation Study,[0],[0]
"Comparing (7) with (1), (2) and (3), we can find that performing self-attention at the phrase level is generally better than at the sentence level, indicating that reducing attention context into phrase level can effectively filter out words that are syntactically and semantically distant, thus focusing on the interaction with important words.",4.2 Ablation Study,[0],[0]
"Comparing (7) with (4), we can draw the conclusion that memory updating is effective even when the inputs to each layer are the same.",4.2 Ablation Study,[0],[0]
"Long-term dependencies are typically hard to capture for sequential models like RNNs (Bengio et al., 1994; Hochreiter and Schmidhuber, 1997).",4.3 Analysis of Sentence Length,[0],[0]
We conduct experiments to see how performance changes as the sentence length increases.,4.3 Analysis of Sentence Length,[0],[0]
"In Figure 2, we show the relationship between classification accuracy and the average length of sentence pair on the SNLI dataset.",4.3 Analysis of Sentence Length,[0],[0]
Sentence-level SelfAttention (w/o PSA model described in subsection 4.2) is used as a baseline for our model.,4.3 Analysis of Sentence Length,[0],[0]
"PSAN
outperforms Sentence-level Self-Attention model consistently for longer sentences of length 14 to 20.",4.3 Analysis of Sentence Length,[0],[0]
This demonstrates that incorporating syntactic information by performing self-attention at the phrase level and refining each word’s representation hierarchically can help to capture long-term dependencies across words in a sentence.,4.3 Analysis of Sentence Length,[0],[0]
We conduct experiments to analyze the memory consumption reduction resulted from Phrase-level Self-Attention.,4.4 Analysis of Memory Consumption,[0],[0]
"To this end, we re-implement two fully attention-based models (Vaswani et al., 2017; Shen et al., 2017) on the TREC dataset.",4.4 Analysis of Memory Consumption,[0],[0]
"To make fair comparison, the dimensions of sentence vectors are set to 300, the same number as our model.",4.4 Analysis of Memory Consumption,[0],[0]
Table 5 lists the results.,4.4 Analysis of Memory Consumption,[0],[0]
"Our PSAN model can outperform the other two fully attention-based models, while being more memory efficient.",4.4 Analysis of Memory Consumption,[0],[0]
reducing more than 20% of memory consumption.,4.4 Analysis of Memory Consumption,[0],[0]
"In order to analyze the attention changing process and the importance of each word in the sentence vector, we visualize the attention scores in the alignment matrix of each layer in Phraselevel Self-Attention and sentence summarization layer.",4.5 Visualization and Case Study,[0],[0]
"To facilitate the visualization of the multidimension attention vector, we use the l2 norm of the attention vector for representation.
",4.5 Visualization and Case Study,[0],[0]
"In Figure 3, we can see that, the difference in attention weights between semantically important and unimportant words gets larger as the context becomes larger.",4.5 Visualization and Case Study,[0],[0]
This implies that token representation can be gradually refined by the gated memory updating mechanism.,4.5 Visualization and Case Study,[0],[0]
"Furthermore, the alignment matrix of a phrase can be refined even if the phrase division does not change between layers.",4.5 Visualization and Case Study,[0],[0]
"For instance, the word “girl” gets larger attention weight in the second layer division than in the first layer.",4.5 Visualization and Case Study,[0],[0]
"This demonstrates that the memory
updating mechanism can gradually pick out important words for sentence representation.",4.5 Visualization and Case Study,[0],[0]
"Finally, nouns and verbs dominate the attention weights, while stop words like “a” and “its”, contribute little to the final sentence representation, this indicates that PSAN can effectively pick out semantically important words that are most representative for the meaning of the whole sentence.",4.5 Visualization and Case Study,[0],[0]
"Recently, self-attention mechanism has been successfully applied to the field of sentence encoding, it utilizes the attention mechanism to relate elements at different positions from a single sentence.",5 Related Work,[0],[0]
"Due to its direct access to each token representation, both long-term and local dependencies can be modeled flexibly.",5 Related Work,[0],[0]
Liu et al. (2016) leveraged the average-pooled word representation to attend words appear in the sentence itself.,5 Related Work,[0],[0]
"Cheng et al. (2016) proposed the LSTMN model for machine reading, an attention vector is produced for each of its hidden states during the recurrent iteration, thus empowering the recurrent network with stronger memorization capability and the ability to discover relations among tokens.",5 Related Work,[0],[0]
Lin et al. (2017) obtained a fixed-size sentence embedding matrix by introducing self-attention.,5 Related Work,[0],[0]
"Different from the feature-level attention used in our model, their attention mechanism extracted different aspects of the sentence into multiple vector representations, and utilized a penalization term to encourage the diversity of different attention results.
",5 Related Work,[0],[0]
Syntactic information can be useful for understanding a natural language sentence.,5 Related Work,[0],[0]
"Many previous researches utilized syntactic information to build sentence encoder from composing the mean-
ings of subtrees.",5 Related Work,[0],[0]
"Tree-LSTM (Tai et al., 2015; Zhu et al., 2015) composed its hidden state from an input vector and the hidden states of arbitrarily many child units.",5 Related Work,[0],[0]
"In Tree-based CNN (Mou et al., 2015, 2016), a set of subtree feature detectors slide over the parse tree of a sentence, and a max-pooling layer is utilized to aggregate information along different parts of the tree.
",5 Related Work,[0],[0]
"Apart from the models that use parse information, there have been several researches that aimed to learn the hierarchical latent structure of text by recursively composing words into sentence representation.",5 Related Work,[0],[0]
"Among them, neural tree indexer (Munkhdalai and Yu, 2017b) utilized LSTM or attentive node composition function to construct full n-ary tree for input text.",5 Related Work,[0],[0]
"Gumbel TreeLSTM (Choi et al., 2018) used Straight-Through Gumbel-Softmax estimator to decide the parent node among candidates dynamically.",5 Related Work,[0],[0]
A major drawback of these models is that the recursion computation can be expensive and hard to be processed in batches.,5 Related Work,[0],[0]
"We propose the Phrase-level Self-Attention Networks (PSAN), a fully attention-based model that can utilize syntactic information for universal sentence encoding.",6 Conclusion,[0],[0]
"By applying self-attention at the phrase level, we can filter out distant and unrelated words and focus on modeling interaction between semantically and syntactically important words, a gated memory updating mechanism is utilized to incorporate different levels of contextual information along the parse tree.",6 Conclusion,[0],[0]
Empirical results on a wide range of transfer tasks demonstrate the effectiveness of our model.,6 Conclusion,[0],[0]
Our work is supported by National Natural Science Foundation of China under Grant No.61433015 and the National Key Research and Development Program of China under Grant No.2017YFB1002101.,Acknowledgments,[0],[0]
The corresponding authors of this paper are Houfeng Wang.,Acknowledgments,[0],[0]
Universal sentence encoding is a hot topic in recent NLP research.,abstractText,[0],[0]
"Attention mechanism has been an integral part in many sentence encoding models, allowing the models to capture context dependencies regardless of the distance between elements in the sequence.",abstractText,[0],[0]
Fully attention-based models have recently attracted enormous interest due to their highly parallelizable computation and significantly less training time.,abstractText,[0],[0]
"However, the memory consumption of their models grows quadratically with sentence length, and the syntactic information is neglected.",abstractText,[0],[0]
"To this end, we propose Phrase-level Self-Attention Networks (PSAN) that perform self-attention across words inside a phrase to capture context dependencies at the phrase level, and use the gated memory updating mechanism to refine each word’s representation hierarchically with longer-term context dependencies captured in a larger phrase.",abstractText,[0],[0]
"As a result, the memory consumption can be reduced because the self-attention is performed at the phrase level instead of the sentence level.",abstractText,[0],[0]
"At the same time, syntactic information can be easily integrated in the model.",abstractText,[0],[0]
"Experiment results show that PSAN can achieve the state-ofthe-art transfer performance across a plethora of NLP tasks including sentence classification, natural language inference and sentence textual similarity.",abstractText,[0],[0]
Phrase-level Self-Attention Networks for Universal Sentence Encoding,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 114–120 New Orleans, Louisiana, June 1 - 6, 2018. c©2017 Association for Computational Linguistics",text,[0],[0]
"Neural machine translation (NMT) (Bahdanau et al., 2014; Sutskever et al., 2014) has recently achieved remarkable performance improving fluency and adequacy over phrase-based machine translation and is being deployed in commercial settings (Koehn and Knowles, 2017).",1 Introduction,[0],[0]
"However, this comes at a cost of slow decoding speeds compared to phrase-based and syntax-based SMT (see section 3).
",1 Introduction,[0],[0]
NMT models are generally trained using 32-bit floating point values.,1 Introduction,[0],[0]
"At training time, multiple sentences can be processed in parallel leveraging graphical processing units (GPUs) to good advantage since the data is processed in batches.",1 Introduction,[0],[0]
"This is also true for decoding for non-interactive applications such as bulk document translation.
",1 Introduction,[0],[0]
Why is fast execution on CPUs important?,1 Introduction,[0],[0]
"First, CPUs are cheaper than GPUs.",1 Introduction,[0],[0]
Fast CPU computation will reduce commercial deployment costs.,1 Introduction,[0],[0]
"Second, for low-latency applications such as speech-to-speech translation (Neubig et al.,
∗A piece of eight was a Spanish dollar that was divided into 8 reales, also known as Real de a Ocho.
2017a), it is important to translate individual sentences quickly enough so that users can have an application experience that responds seamlessly.",1 Introduction,[0],[0]
"Translating individual sentences with NMT requires many memory bandwidth intensive matrixvector or matrix-narrow matrix multiplications (Abdelfattah et al., 2016).",1 Introduction,[0],[0]
"In addition, the batch size is 1 and GPUs do not have a speed advantage over CPUs due to the lack of adequate parallel work (as evidenced by increasingly difficult batching scenarios in dynamic frameworks (Neubig et al., 2017b)).
",1 Introduction,[0],[0]
Others have successfully used low precision approximations to neural net models.,1 Introduction,[0],[0]
Vanhoucke et al. (2011) explored 8-bit quantization for feedforward neural nets for speech recognition.,1 Introduction,[0],[0]
Devlin (2017) explored 16-bit quantization for machine translation.,1 Introduction,[0],[0]
In this paper we show the effectiveness of 8-bit decoding for models that have been trained using 32-bit floating point values.,1 Introduction,[0],[0]
"Results show that 8-bit decoding does not hurt the fluency or adequacy of the output, while producing results up to 4-6x times faster.",1 Introduction,[0],[0]
"In addition, implementation is straightforward and we can use the models as is without altering training.
",1 Introduction,[0],[0]
"The paper is organized as follows: Section 2 reviews the attentional model of translation to be sped up, Section 3 presents our 8-bit quantization in our implementation, Section 4 presents automatic measurements of speed and translation quality plus human evaluations, Section 5 discusses the results and some illustrative examples, Section 6 describes prior work, and Section 7 concludes the paper.",1 Introduction,[0],[0]
"Our translation system implements the attentional model of translation (Bahdanau et al., 2014) consisting of an encoder-decoder network with an at-
114
tention mechanism.",2 The Attentional Model of Translation,[0],[0]
"The encoder uses a bidirectional GRU recurrent neural network (Cho et al., 2014) to encode a source sentence x = (x1, ..., xl), where xi is the embedding vector for the ith word and l is the sentence length.",2 The Attentional Model of Translation,[0],[0]
"The encoded form is a sequence of hidden states h = (h1, ..., hl) where each hi is computed as follows
hi =",2 The Attentional Model of Translation,[0],[0]
[←− hi−→ hi ] =,2 The Attentional Model of Translation,[0],[0]
"[←− f (xi, ←− h i+1)−→ f (xi, −→ h i−1) ] , (1)
where −→ h0 = ←− h0 = 0.",2 The Attentional Model of Translation,[0],[0]
Here ←− f and −→ f are GRU cells.,2 The Attentional Model of Translation,[0],[0]
"Given h, the decoder predicts the target translation y by computing the output token sequence (y1, ...ym), where m is the length of the sequence.",2 The Attentional Model of Translation,[0],[0]
"At each time t, the probability of each token yt from a target vocabulary is
p(yt|h, yt−1..y1)",2 The Attentional Model of Translation,[0],[0]
"= g(st, yt−1, Ht), (2)
where g is a two layer feed-forward network over the embedding of the previous target word (yt−1), the decoder hidden state (st), and the weighted sum of encoder states h",2 The Attentional Model of Translation,[0],[0]
"(Ht), followed by a softmax to predict the probability distribution over the output vocabulary.
",2 The Attentional Model of Translation,[0],[0]
"We compute st with a two layer GRU as
s′t = r(st−1, y ∗ t−1) (3)
and st = q(s ′ t, Ht), (4) where s′t is an intermediate state and s0 = ←− h0.",2 The Attentional Model of Translation,[0],[0]
The two GRU units r and q together with the attention constitute the conditional GRU layer of Sennrich et al. (2017).,2 The Attentional Model of Translation,[0],[0]
"Ht is computed as
Ht =
[∑l i=1(αt,i · ←− h i)∑l
i=1(αt,i · −→ h i)
] , (5)
",2 The Attentional Model of Translation,[0],[0]
"where αt,i are the elements of αt which is the output vector of the attention model.",2 The Attentional Model of Translation,[0],[0]
"This is computed with a two layer feed-forward network
α′t = v(tanh(w(hi) + u(s ′ t−1))), (6)
where w and u are weight matrices, and v is another matrix resulting in one real value per encoder",2 The Attentional Model of Translation,[0],[0]
state hi.,2 The Attentional Model of Translation,[0],[0]
"αt is then the softmax over α′t.
",2 The Attentional Model of Translation,[0],[0]
"We train our model using a program written using the Theano framework (Bastien et al.,
2012).",2 The Attentional Model of Translation,[0],[0]
"Generally models are trained with batch sizes ranging from 64 to 128 and unbiased Adam stochastic optimizer (Kingma and Ba, 2014).",2 The Attentional Model of Translation,[0],[0]
We use an embedding size of 620 and hidden layer sizes of 1000.,2 The Attentional Model of Translation,[0],[0]
We select model parameters according to the best BLEU score on a held-out development set over 10 epochs.,2 The Attentional Model of Translation,[0],[0]
Our translation engine is a C++ implementation.,3 8-bit Translation,[0],[0]
"The engine is implemented using the Eigen matrix library, which provides efficient matrix operations.",3 8-bit Translation,[0],[0]
Each CPU core translates a single sentence at a time.,3 8-bit Translation,[0],[0]
"The same engine supports both batch and interactive applications, the latter making single-sentence translation latency important.",3 8-bit Translation,[0],[0]
"We report speed numbers as both words per second (WPS) and words per core second (WPCS), which is WPS divided by the number of cores running.",3 8-bit Translation,[0],[0]
"This gives us a measure of overall scaling across many cores and memory buses as well as the single-sentence speed.
",3 8-bit Translation,[0],[0]
"Phrase-based SMT systems, such as (Tillmann, 2006), for English-German run at 170 words per core second (3400 words per second) on a 20 core Xeon 2690v2 system.",3 8-bit Translation,[0],[0]
"Similarly, syntax-based SMT systems, such as (Zhao and Al-onaizan, 2008), for the same language pair run at 21.5 words per core second (430 words per second).
",3 8-bit Translation,[0],[0]
"In contrast, our NMT system (described in Section 2) with 32-bit decoding runs at 6.5 words per core second (131 words per second).",3 8-bit Translation,[0],[0]
"Our goal is to increase decoding speed for the NMT system to what can be achieved with phrase-based systems while maintaining the levels of fluency and adequacy that NMT offers.
",3 8-bit Translation,[0],[0]
Benchmarks of our NMT decoder unsurprisingly show matrix multiplication as the number one source of compute cycles.,3 8-bit Translation,[0],[0]
In Table 1 we see that more than 85% of computation is spent in Eigen’s matrix and vector multiply routines (Eigen matrix vector product and Eigen matrix multiply).,3 8-bit Translation,[0],[0]
"It dwarfs the costs of the transcendental function computations as well as the bias additions.
",3 8-bit Translation,[0],[0]
"Given this distribution of computing time, it makes sense to try to accelerate the matrix operations as much as possible.",3 8-bit Translation,[0],[0]
One approach to increasing speed is to quantize matrix operations.,3 8-bit Translation,[0],[0]
"Replacing 32-bit floating point math operations with 8-bit integer approximations in neural nets has been shown to give speedups and similar ac-
curacy (Vanhoucke et al., 2011).",3 8-bit Translation,[0],[0]
"We chose to apply similar optimization to our translation system, both to reduce memory traffic as well as increase parallelism in the CPU.
",3 8-bit Translation,[0],[0]
Our 8-bit matrix multiply routine uses a naive implementation with no blocking or copy.,3 8-bit Translation,[0],[0]
"The code is implemented using Intel SSE4 vector instructions and computes 4 rows at a time, similar to (Devlin, 2017).",3 8-bit Translation,[0],[0]
Simplicity led to implementing 8-bit matrix multiplication with the results being placed into a 32-bit floating point result.,3 8-bit Translation,[0],[0]
This has the advantage of not needing to know the scale of the result.,3 8-bit Translation,[0],[0]
"In addition, the output is a vector or narrow matrix, so little extra memory bandwidth is consumed.
",3 8-bit Translation,[0],[0]
"Multilayer matrix multiply algorithms result in significantly faster performance than naive algorithms (Goto and Geijn, 2008).",3 8-bit Translation,[0],[0]
"This is due to the fact that there are O(N3) math operations on O(N2) elements when multiplying NxN matrices, therefore it is worth significant effort to minimize memory operations while maximizing math operations.",3 8-bit Translation,[0],[0]
"However, when multiplying an NxN matrix by an NxP matrix where P is very small (<10), memory operations dominate and performance does not benefit from the complex algorithm.",3 8-bit Translation,[0],[0]
"When decoding single sentences, we typically set our beam size to a value less than 8 following standard practice in this kind of systems (Koehn and Knowles, 2017).",3 8-bit Translation,[0],[0]
"We actually find that at such small values of P, the naive algorithm is a bit faster.
",3 8-bit Translation,[0],[0]
Table 2 shows the profile after converting the matrix routines to 8-bit integer computation.,3 8-bit Translation,[0],[0]
There is only one entry for matrix-matrix and matrix-vector multiplies since they are handled by the same routine.,3 8-bit Translation,[0],[0]
"After conversion, tanh and sigmoid still consume less than 7% of CPU time.",3 8-bit Translation,[0],[0]
"We decided not to convert these operations to integer in light of that fact.
",3 8-bit Translation,[0],[0]
"It is possible to replace all the operations with 8-bit approximations (Wu et al., 2016), but this makes implementation more complex, as the scale of the result of a matrix multiplication must be known to correctly output 8-bit numbers without dangerous loss of precision.
",3 8-bit Translation,[0],[0]
"Assuming we have 2 matrices of size 1000x1000 with a range of values [−10, 10], the individual dot products in the result could be as large as 108.",3 8-bit Translation,[0],[0]
"In practice with neural nets, the scale of the result is similar to that of the input matrices.",3 8-bit Translation,[0],[0]
"So if we scale the result to [−127, 127] assuming the worst case, the loss of precision will give us a matrix full of zeros.",3 8-bit Translation,[0],[0]
"The choices are to either scale the result of the matrix multiplication with a reasonable value, or to store the result as floating point.",3 8-bit Translation,[0],[0]
"We opted for the latter.
8-bit computation achieves 32.3 words per core second (646 words per second), compared to the 6.5 words per core second (131 words per second) of the 32-bit system (both systems load parameters from the same model).",3 8-bit Translation,[0],[0]
This is even faster than the syntax-based system that runs at 21.5 words per core second (430 words per second).,3 8-bit Translation,[0],[0]
"Table 3 summarizes running speeds for the phrase-based SMT system, syntax-based system and NMT with 32-bit decoding and 8-bit decoding.",3 8-bit Translation,[0],[0]
"To demonstrate the effectiveness of approximating the floating point math with 8-bit integer computation, we show automatic evaluation results
on several models, as well as independent human evaluations.",4 Measurements,[0],[0]
"We report results on Dutch-English, English-Dutch, Russian-English, German-English and English-German models.",4 Measurements,[0],[0]
Table 4 shows training data sizes and vocabulary sizes.,4 Measurements,[0],[0]
All models have 620 dimension embeddings and 1000 dimension hidden states.,4 Measurements,[0],[0]
Here we report automatic results comparing decoding results on 32-bit and 8-bit implementations.,4.1 Automatic results,[0],[0]
"As others have found (Wu et al., 2016), 8-bit implementations impact quality very little.
",4.1 Automatic results,[0],[0]
"In Table 6, we compared automatic scores and speeds for Dutch-English, English-Dutch, Russian-English, German-English and EnglishGerman models on news data.",4.1 Automatic results,[0],[0]
"The EnglishGerman model was run with both a single model (1x) and an ensemble of two models (2x) (Freitag et al., 2017).",4.1 Automatic results,[0],[0]
"Table 5 gives the number of sentences and average sentence length for the test sets used.
",4.1 Automatic results,[0],[0]
Speed is reported in words per core second (WPCS).,4.1 Automatic results,[0],[0]
This gives us a better sense of the speed of individual engines when deployed on multicore systems with all cores performing translations.,4.1 Automatic results,[0],[0]
Total throughput is simply the product of WPCS and the number of cores in the machine.,4.1 Automatic results,[0],[0]
The reported speed is the median of 9 runs to ensure consistent numbers.,4.1 Automatic results,[0],[0]
"The results show that we see a 4-6x speedup over 32-bit floating point de-
coding.",4.1 Automatic results,[0],[0]
German-English shows the largest deficit for the 8-bit mode versus the 32-bit mode.,4.1 Automatic results,[0],[0]
The German-English test set only includes 168 sentences so this may be a spurious difference.,4.1 Automatic results,[0],[0]
These automatic results suggest that 8-bit quantization can be done without perceptible degradation.,4.2 Human evaluation,[0],[0]
"To confirm this, we carried out a human evaluation experiment.
",4.2 Human evaluation,[0],[0]
"In Table 7, we show the results of performing human evaluations on some of the same language pairs in the previous section.",4.2 Human evaluation,[0],[0]
An independent native speaker of the language being translated to/from different than English (who is also proficient in English) scored 100 randomly selected sentences.,4.2 Human evaluation,[0],[0]
The sentences were shuffled during the evaluation to avoid evaluator bias towards different runs.,4.2 Human evaluation,[0],[0]
"We employ a scale from 0 to 5, with 0 being unintelligible and 5 being perfect translation.
",4.2 Human evaluation,[0],[0]
"The Table shows that the automatic scores shown in the previous section are also sustained
by humans.",4.2 Human evaluation,[0],[0]
8-bit decoding is as good as 32-bit decoding according to the human evaluators.,4.2 Human evaluation,[0],[0]
Having a faster NMT engine with no loss of accuracy is commercially useful.,5 Discussion,[0],[0]
"In our deployment scenarios, it is the difference between an interactive user experience that is sluggish and one that is not.",5 Discussion,[0],[0]
"Even in batch mode operation, the same throughput can be delivered with 1/4 the hardware.
",5 Discussion,[0],[0]
"In addition, this speedup makes it practical to deploy small ensembles of models.",5 Discussion,[0],[0]
"As shown above in the En-De model in Table 6, an ensemble can deliver higher accuracy at the cost of a 2x slowdown.",5 Discussion,[0],[0]
"This work makes it possible to translate with higher quality while still being at least twice as fast as the previous baseline.
",5 Discussion,[0],[0]
"As the numbers reported in Section 4 demonstrate, 8-bit and 32-bit decoding have similar average quality.",5 Discussion,[0],[0]
"As expected, the outputs produced by the two decoders are not identical.",5 Discussion,[0],[0]
"In fact, on a run of 166 sentences of De-En translation, only 51 were identical between the two.",5 Discussion,[0],[0]
"In addition, our human evaluation results and the automatic scoring suggest that there is no specific degradation by the 8-bit decoder compared to the 32-bit decoder.",5 Discussion,[0],[0]
"In order to emphasize these claims, Table 8 shows several examples of output from the two systems for a German-English system.",5 Discussion,[0],[0]
"Table 9 shows 2 more examples from a Dutch-English system.
",5 Discussion,[0],[0]
"In general, there are minor differences without any loss in adequacy or fluency due to 8-bit decoding.",5 Discussion,[0],[0]
"Sentence 2 in Table 8 shows a spelling error (“predictated”) in the 32-bit output due to re-
assembly of incorrect subword units.1",5 Discussion,[0],[0]
"Reducing the resources required for decoding neural nets in general and neural machine translation in particular has been the focus of some attention in recent years.
",6 Related Work,[0],[0]
Vanhoucke et al. (2011) explored accelerating convolutional neural nets with 8-bit integer decoding for speech recognition.,6 Related Work,[0],[0]
They demonstrated that low precision computation could be used with no significant loss of accuracy.,6 Related Work,[0],[0]
"Han et al. (2015) investigated highly compressing image classification neural networks using network pruning, quantization, and Huffman coding so as to fit completely into on-chip cache, seeing significant improvements in speed and energy efficiency while keeping accuracy losses small.
",6 Related Work,[0],[0]
"Focusing on machine translation, Devlin (2017) implemented 16-bit fixed-point integer math to speed up matrix multiplication operations, seeing a 2.59x improvement.",6 Related Work,[0],[0]
They show competitive BLEU scores on WMT English-French NewsTest2014 while offering significant speedup.,6 Related Work,[0],[0]
"Similarly, (Wu et al., 2016) applies 8-bit end-toend quantization in translation models.",6 Related Work,[0],[0]
They also show that automatic metrics do not suffer as a result.,6 Related Work,[0],[0]
"In this work, quantization requires modification to model training to limit the size of matrix outputs.",6 Related Work,[0],[0]
"In this paper, we show that 8-bit decoding for neural machine translation runs up to 4-6x times faster than a similar optimized floating point implementation.",7 Conclusions and Future Work,[0],[0]
We show that the quality of this approximation is similar to that of the 32-bit version.,7 Conclusions and Future Work,[0],[0]
"We also show that it is unnecessary to modify the training procedure to produce models compatible with 8- bit decoding.
",7 Conclusions and Future Work,[0],[0]
"To conclude, this paper shows that 8-bit decoding is as good as 32-bit decoding both in automatic measures and from a human perception perspective, while it improves latency substantially.
",7 Conclusions and Future Work,[0],[0]
In the future we plan to implement a multilayered matrix multiplication that falls back to the naive algorithm for matrix-panel multiplications.,7 Conclusions and Future Work,[0],[0]
This will provide speed for batch decoding for applications that can take advantage of it.,7 Conclusions and Future Work,[0],[0]
"We also
1In order to limit the vocabulary, we use BPE subword units (Sennrich et al., 2016) in all models.
plan to explore training with low precision for faster experiment turnaround time.
",7 Conclusions and Future Work,[0],[0]
Our results offer hints of improved accuracy rather than just parity.,7 Conclusions and Future Work,[0],[0]
Other work has used training as part of the compression process.,7 Conclusions and Future Work,[0],[0]
We would like to see if training quantized models changes the results for better or worse.,7 Conclusions and Future Work,[0],[0]
Neural machine translation has achieved levels of fluency and adequacy that would have been surprising a short time ago.,abstractText,[0],[0]
"Output quality is extremely relevant for industry purposes, however it is equally important to produce results in the shortest time possible, mainly for latency-sensitive applications and to control cloud hosting costs.",abstractText,[0],[0]
In this paper we show the effectiveness of translating with 8-bit quantization for models that have been trained using 32-bit floating point values.,abstractText,[0],[0]
Results show that 8-bit translation makes a non-negligible impact in terms of speed with no degradation in accuracy and adequacy.,abstractText,[0],[0]
Pieces of Eight: 8-bit Neural Machine Translation,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3233–3242 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3233",text,[0],[0]
The 20 Question Game (Q20 Game) is a classic game that requires deductive reasoning and creativity.,1 Introduction,[0],[0]
"At the beginning of the game, the answerer thinks of a target object and keeps it concealed.",1 Introduction,[0],[0]
"Then the questioner tries to figure out the target object by asking questions about it, and the answerer answers each question with a simple “Yes”, “No” or “Unknown”, honestly.",1 Introduction,[0],[0]
The questioner wins the game if the target object is found within 20 questions.,1 Introduction,[0],[0]
"In a Q20 game system, the
∗The work was done when the first author was an intern in Microsoft XiaoIce team.
",1 Introduction,[0],[0]
"user is considered as the answerer while the system itself acts as the questioner which requires a good question selection strategy to win the game.
",1 Introduction,[0],[0]
"As a game with the hype read your mind, Q20 has been played since the 19th century, and was brought to screen in the 1950s by the TV show Twenty Questions.",1 Introduction,[0],[0]
"Burgener’s program (Burgener, 2006) further popularized Q20 as an electronic game in 1988, and modern virtual assistants like Microsoft XiaoIce and Amazon Alexa also incorporate this game into their system to demonstrate their intelligence.
",1 Introduction,[0],[0]
"However, it is not easy to design the algorithm to construct a Q20 game system.",1 Introduction,[0],[0]
"Although the decision tree based method seems like a natural fit to the Q20 game, it typically require a well defined Knowledge Base (KB) that contains enough information about each object, which is usually not available in practice.",1 Introduction,[0],[0]
"Burgener (2006) instead uses a object-question relevance table as the pivot for question and object selection, which does not depend on an existing KB.",1 Introduction,[0],[0]
Wu et al. (2018) further improve the relevance table with a lot of engineering tricks.,1 Introduction,[0],[0]
"Since these table-based methods greedily select questions and the model parameters are only updated by rules, their models are very sensitive to noisy answers from users, which is common in the real-world Q20 games.",1 Introduction,[0],[0]
"Zhao and Maxine (2016) utilizes a value-based Reinforcement Learning (RL) model to improve the generalization ability but still relies on the existing KB.
",1 Introduction,[0],[0]
"In this paper, we formulate the process of question selction in the game as a Markov Decision Process (MDP), and further propose a novel policy-based RL framework to learn the optimal policy of question selection in the Q20 game.",1 Introduction,[0],[0]
"Our questioner agent maintains a probability distribution over all objects to model the confidence of the target object, and updates the confidence based on answers from the user.",1 Introduction,[0],[0]
At each time-step.,1 Introduction,[0],[0]
"the agent uses a policy network πθ(a|s) to take in
the confidence vector and output a question distribution for selecting the next question.",1 Introduction,[0],[0]
"To solve the problem that there is no immediate reward for each selected question, we also propose to employ a RewardNet to estimate the appropriate immediate reward at each time-step, which is further used to calculate the long-term return to train our RL model.",1 Introduction,[0],[0]
"Our RL framework makes the agent robust to noisy answers since the model parameters are fully learnable and the question distribution from πθ(a|s) provides us with a principled way to sample questions, which enables the agent to jump out of the local optimum caused by incorrect answers and also introduces more randomness during training to improve the model generalization ability.",1 Introduction,[0],[0]
"Furthermore, the ability to sample questions, compared to greedy selection, also improves the diversity of the questions asked by our agent, which is crucial for user experience.
",1 Introduction,[0],[0]
Our contributions can be summarized as follows: (1) We propose a novel RL framework to learn the optimal policy of question selection in the Q20 game without any dependencies on the existing KBs of target objects.,1 Introduction,[0],[0]
Our trained agent is robust to noisy answers and has a good diversity in its selected questions.,1 Introduction,[0],[0]
"(2) To make the reward more meaningful, we also propose a novel neural network on reward function approximation to deliver the appropriate immediate rewards at each time-step.",1 Introduction,[0],[0]
(3) Extensive experiments show that our RL method clearly outperforms a highly engineered baseline in the real-world Q20 games where noisy answers are common.,1 Introduction,[0],[0]
"Besides, our RL method is also competitive to that baseline on a noise-free simulation environment.",1 Introduction,[0],[0]
"In this section, we first describe our RL framework for playing the Q20 game, which is shown in the
Fig. 1.",2 Method,[0],[0]
The user in our system is the answerer who thinks of a target object otgt in the object set O at the beginning of the game.,2 Method,[0],[0]
Our policy-based agent acts as the questioner that can ask 20 questions to figure out what exactly otgt is.,2 Method,[0],[0]
"Specifically, an internal state vector s is maintained by our agent, which describes the confidence about otgt.",2 Method,[0],[0]
"At each time-step t, the agent picks up the promising action (select a question) according to the policy πθ(a|st), and transits from the state st to the next state st+1 after receiving the answer (“Yes”/“No”/“Unknown”) from the user.",2 Method,[0],[0]
"The historical trajectories 〈st, at, rt+1, st+1〉 are stored in a replay memory which enables the agent to be trained on previously observed data by sampling from it.",2 Method,[0],[0]
"Note that only when a guess is made about otgt at the end of game can the agent receive a reward signal, which makes it unable to distinguish the importance of each selected question.",2 Method,[0],[0]
"Therefore, we design a RewardNet to learn the more informative reward at each time-step and thus lead the agent to achieve the better performance.
",2 Method,[0],[0]
"In the rest of this section, we first describe how to formulate the Q20 game into a RL framework, and then introduce the RewardNet.",2 Method,[0],[0]
"Finally, we will demonstrate our training procedure in detail.",2 Method,[0],[0]
"In the Q20 game, the goal of our agent is to figure out the object otgt that the user thinks of at the beginning of game by asking 20 questions.",2.1 Modeling of the Q20 Game,[0],[0]
We formulate the process of question selection as a finite Markov Decision Process (MDP) which can be solved with RL.,2.1 Modeling of the Q20 Game,[0],[0]
"A tuple 〈S,A, P ,R, γ〉 is defined to represent the MDP, where S is the continuous state space, A = {a1, a2, · · · , am} is the set of all available actions, P(St+1 = s′|St = s,At = a) is the transition probability matrix,R(s, a) is the reward function and γ ∈",2.1 Modeling of the Q20 Game,[0],[0]
"[0, 1] is the discount factor used to calculate the long-time return.",2.1 Modeling of the Q20 Game,[0],[0]
"In the RL framework, at each time-step t, the agent takes an action at under the state st according to the policy πθ(a|st).",2.1 Modeling of the Q20 Game,[0],[0]
"After interacting with the environment, the agent receives a reward scalar rt+1 and transits to the next state st+1, then another time-step begins.",2.1 Modeling of the Q20 Game,[0],[0]
"All these trajectories 〈st, at, rt+1, st+1〉 in a game constitute an episode which is an instance of the finite MDP.",2.1 Modeling of the Q20 Game,[0],[0]
"The long-time return Gt of the time-step t is calculated as follows:
Gt = T∑ k=0 γkrt+k+1 (1)
",2.1 Modeling of the Q20 Game,[0],[0]
"In the following parts, we describe each component of RL corresponding to the Q20 game.
Environment.",2.1 Modeling of the Q20 Game,[0],[0]
The major component of our environment is the user in the Q20 game who decides the target object otgt and answers questions from the agent.,2.1 Modeling of the Q20 Game,[0],[0]
"Besides, the environment also needs to deliver the reward based on the outcome of the game and store historical data into the replay memory (see Fig. 1).
Action.",2.1 Modeling of the Q20 Game,[0],[0]
"Since the agent interacts with the user by asking questions, the action at ∈ A taken by our agent refers to selecting the question qat at timestep t, andA is the set of the indices to all available questions in the Q20 game.
State.",2.1 Modeling of the Q20 Game,[0],[0]
"In our method, we use the state st to keep track of the current confidence of target object otgt.",2.1 Modeling of the Q20 Game,[0],[0]
Specifically st ∈ R|O| and ∑n i=1,2.1 Modeling of the Q20 Game,[0],[0]
"st,i = 1, where O = {o1, o2, · · · , on} represents the set of all the objects that can be chosen by the user.",2.1 Modeling of the Q20 Game,[0],[0]
"Therefore, the state st is a probability distribution over all the objects and st,i is the confidence that the object oi is the target object otgt at time-step t.
The initial state s0 can either be a uniform distribution or initialized by the prior knowledge.",2.1 Modeling of the Q20 Game,[0],[0]
We observe that users typically prefer to choose popular objects which are more concerned by the public.,2.1 Modeling of the Q20 Game,[0],[0]
"For example, the founder of Tesla Inc. and the designer of SpaceX, “Elon Musk”, is more likely to be chosen compared to a CEO of a new startup.",2.1 Modeling of the Q20 Game,[0],[0]
"Motivated by this, we could use the yearly retrieval frequency C(oi) of object oi on a commercial search engine to calculate the initial state s0, where s0,i = C(oi) /",2.1 Modeling of the Q20 Game,[0],[0]
"∑n j=1C(oj).
",2.1 Modeling of the Q20 Game,[0],[0]
Transition Dynamics.,2.1 Modeling of the Q20 Game,[0],[0]
"In our method, the transition dynamics is deterministic.",2.1 Modeling of the Q20 Game,[0],[0]
"Given the object set O and the question set A, we collect the normalized probabilities of the answer over “Yes”, “No” and “Unknown” for each object-question pair.",2.1 Modeling of the Q20 Game,[0],[0]
"And the rule of state transition is define as:
st+1 = st α (2)
where α depends on the answer xt to the question qat which is selected by the agent at the step t:
α =  ",2.1 Modeling of the Q20 Game,[0],[0]
"[R(1, at), . . .",2.1 Modeling of the Q20 Game,[0],[0]
", R(|O|, at)], xt = Y es
[W (1, at), . . .",2.1 Modeling of the Q20 Game,[0],[0]
",W (|O|, at)], xt =",2.1 Modeling of the Q20 Game,[0],[0]
"No [U(1, at), . . .",2.1 Modeling of the Q20 Game,[0],[0]
", U(|O|, at)], xt = Unk
(3) where O is the object set and for each objectquestion pair (oi, qj), R(i, j) and W (i, j) are cal-
culated as follows:
R(i, j) = Cyes(i, j) + δ
Cyes(i, j) +",2.1 Modeling of the Q20 Game,[0],[0]
"Cno(i, j) +",2.1 Modeling of the Q20 Game,[0],[0]
"Cunk(i, j) + λ
W (i, j) = Cno(i, j) + δ
Cyes(i, j) + Cno(i, j) +",2.1 Modeling of the Q20 Game,[0],[0]
"Cunk(i, j) + λ
(4)
R(i, j) and W (i, j) are probabilities of answering “Yes” and “No” to question qj with respect to the object oi respectively.",2.1 Modeling of the Q20 Game,[0],[0]
"Cyes(i, j), Cno(i, j) and Cunk(i, j) are frequencies of answering “Yes”, “No” and “Unknown” to question qj with respect to the object oi.",2.1 Modeling of the Q20 Game,[0],[0]
δ,2.1 Modeling of the Q20 Game,[0],[0]
and λ are smoothing parameters.,2.1 Modeling of the Q20 Game,[0],[0]
"Then the probability of answering “Unknown” to question qj with respect to the object oi is:
U(i, j) = 1−R(i, j)−W (i, j) (5)
",2.1 Modeling of the Q20 Game,[0],[0]
"In this way, the confidence st,i that the object oi is the target object otgt is updated following the user’s answer xt to the selected question qat at the time-step t.
Policy Network.",2.1 Modeling of the Q20 Game,[0],[0]
We directly parameterize the policy πθ(a|st) with a neural network which maps the state st to a probability distribution over all available actions: πθ(a|st) = P[a|st; θ].,2.1 Modeling of the Q20 Game,[0],[0]
The parameters θ are updated to maximize the expected return which is received from the environment.,2.1 Modeling of the Q20 Game,[0],[0]
"Instead of learning a greedy policy in value-based methods like DQN, the policy network is able to learn a stochastic policy which can increase the diversity of questions asked by our agent and potentially make the agent more robust to noisy answers in the real-world Q20 game.",2.1 Modeling of the Q20 Game,[0],[0]
The policy πθ(a|s) is modeled by a Multi-Layer Perceptron (MLP) and the output layer is normalized by using a masked softmax function to avoid selecting the question that has been asked before.,2.1 Modeling of the Q20 Game,[0],[0]
Because asking the same question twice does not provide extra information about otgt in a game.,2.1 Modeling of the Q20 Game,[0],[0]
"For most reinforcement learning applications, it is always a critical part to design reward functions, especially when the agent needs to precisely take actions in a complex task.",2.2 Problem of Direct Reward,[0],[0]
"A good reward function can improve the learning efficiency and help the agent achieve better performances.
",2.2 Problem of Direct Reward,[0],[0]
"In the Q20 game, however, the immediate reward rt of selecting question qat is unknown at the time-step t (t < T ) because each selected question is just answered with a simple “Yes”, “No” or
“Unknown” and there is no extra information provided by user.",2.2 Problem of Direct Reward,[0],[0]
Only when the game ends (t = T ) can the agent receive a reward signal of win or loss.,2.2 Problem of Direct Reward,[0],[0]
So we intuitively consider the direct reward: rT = 30 and −30 for the win and loss respectively while rt = 0 for all t < T .,2.2 Problem of Direct Reward,[0],[0]
"Unfortunately, the direct reward is not discriminative because the agent receives the same immediate reward rt = 0 (t < T ) for selecting both good and bad questions.",2.2 Problem of Direct Reward,[0],[0]
"For example, if the otgt is “Donald Trump”, then selecting question (a) “Is your role the American president?” should receive more immediate reward rt than selecting question (b) “Has your role been married?”.",2.2 Problem of Direct Reward,[0],[0]
"The reason is that as for the otgt, question (a) is more relevant and can narrow down the searching space to a greater extent.
",2.2 Problem of Direct Reward,[0],[0]
"Therefore, it is necessary to design a better reward function to estimate a non-zero immediate reward rt, and make the long-time return Gt =∑T
k=0",2.2 Problem of Direct Reward,[0],[0]
γ krt+k+1 more informative.,2.2 Problem of Direct Reward,[0],[0]
"To solve the problem of the direct reward, we propose a reward function which employs a neural network to estimate a non-zero immediate reward rt at each time-step.",2.3 Reward Function Approximation by Neural Network,[0],[0]
"So that Gt can be more informative, which thus leads to a better trained questioner agent.
",2.3 Reward Function Approximation by Neural Network,[0],[0]
"The reward function takes the state-action pair (st, at) as input and outputs the corresponding immediate reward rt+1.",2.3 Reward Function Approximation by Neural Network,[0],[0]
"In our method, we use a MLP with sigmoid output to learn the appropriate immediate reward during training, and this network is referred as RewardNet.",2.3 Reward Function Approximation by Neural Network,[0],[0]
"In each episode, the long-term return Gt is used as a surrogate indicator of rt+1 to train our RewardNet with the following loss function:
L1(σ) =",2.3 Reward Function Approximation by Neural Network,[0],[0]
"(R(st, at;σ)− sigmoid(Gt))2 (6)
where σ is the network parameters.",2.3 Reward Function Approximation by Neural Network,[0],[0]
Here we apply the sigmoid function on Gt so as to prevent Gt from growing too large.,2.3 Reward Function Approximation by Neural Network,[0],[0]
"Besides, we also use the replay memory to store both old and recent experiences, and then train the network by sampling mini-batches from it.",2.3 Reward Function Approximation by Neural Network,[0],[0]
"The training process based on the experience replay technique can decorrelate the sample data and thus make the training of the RewardNet more efficient.
",2.3 Reward Function Approximation by Neural Network,[0],[0]
"Furthermore, since the target object otgt can be obtained at the end of each episode, we can
use the extra information provided by otgt to estimate a better immediate reward rt.",2.3 Reward Function Approximation by Neural Network,[0],[0]
"To capture the relevance between the selected questions and otgt in an episode, we further propose a objectaware RewardNet which takes the 〈st, at, otgt〉 tuple as input and produces corresponding rt+1 as output.",2.3 Reward Function Approximation by Neural Network,[0],[0]
The detailed training algorithm is shown in Algo.,2.3 Reward Function Approximation by Neural Network,[0],[0]
"1.
",2.3 Reward Function Approximation by Neural Network,[0],[0]
Algorithm 1: Training Object-Aware RewardNet 1 Initialize replay memory D1 to capacity N1 2 Initialize RewardNet with random weights σ 3 for episode i← 1 to Z do 4,2.3 Reward Function Approximation by Neural Network,[0],[0]
"User chooses object oi from O 5 Initialize temporary set S1 and S2 6 Play with policy πθ(at|st), and store (st, at) in S1, where t ∈",2.3 Reward Function Approximation by Neural Network,[0],[0]
"[0, T ] 7 rT ← 30 or −30 for a win or loss 8 for (st, at) in S1 do 9 Get rt+1 from RewardNet
10 Store (st, at, rt+1) tuple in S2 11 for (st, at, rt+1) in S2 do 12 Gt ← ∑T k=0",2.3 Reward Function Approximation by Neural Network,[0],[0]
"γ
krt+k+1 13 r′t+1 ← sigmoid(Gt) 14 Store (st, at, oi, r′t+1) in D1 15 if len(D1) > K1 then 16 Sample mini-batch from D1 17 Update σ with loss L1(σ) in Eq. 6",2.3 Reward Function Approximation by Neural Network,[0],[0]
"We train the policy network using REINFORCE (Williams, 1992) algorithm and the corresponding loss function is defined as follows:
L2(θ) = −Eπθ",2.4 Training the Policy-Based Agent,[0],[0]
[log πθ(at|st)(Gt − bt)],2.4 Training the Policy-Based Agent,[0],[0]
"(7)
where the baseline bt is a estimated value of the expected future reward at the state st, which is produced by a value network Vη(st).",2.4 Training the Policy-Based Agent,[0],[0]
"Similarly, the value network Vη(st) is modeled as a MLP which takes the state st as input and outputs a real value as the expected return.",2.4 Training the Policy-Based Agent,[0],[0]
"By introducing the baseline bt for the policy gradient, we can reduce the variance of gradients and thus make the training process of policy network more stable.",2.4 Training the Policy-Based Agent,[0],[0]
"The network parameters η are updated by minimizing the loss function below:
L3(η) = (Vη(st)−Gt)2 (8)
Note that, in our method, both the RewardNet and the value network Vη(st) approximate the reward during training.",2.4 Training the Policy-Based Agent,[0],[0]
But the difference lies in that the RewardNet is designed to estimate a appropriate non-zero reward rt and further derive the more informative return Gt while Vη(st) aims to learn a baseline bt to reduce the variance of policy gradients.,2.4 Training the Policy-Based Agent,[0],[0]
We combine both of two networks to improve the gradients for our policy network and thus lead to a better agent.,2.4 Training the Policy-Based Agent,[0],[0]
The training procedure is described in Algo.,2.4 Training the Policy-Based Agent,[0],[0]
"2.
",2.4 Training the Policy-Based Agent,[0],[0]
Algorithm 2: Training the Agent 1 Initialize replay memory D2 to capacity N2 2 Initialize policy net π with random weights θ 3 Initialize value net V with random weights η 4,2.4 Training the Policy-Based Agent,[0],[0]
"Initialize RewardNet with random weights σ 5 for episode i← 1 to Z do 6 Rollout, collect rewards, and save the history in S2 (4-10 in Algo.",2.4 Training the Policy-Based Agent,[0],[0]
"1) 7 for (st, at, rt+1) in S2 do 8 Gt ← ∑T k=0",2.4 Training the Policy-Based Agent,[0],[0]
"γ
krt+k+1 9 Update RewardNet (13-17 in
Algo.",2.4 Training the Policy-Based Agent,[0],[0]
"1) 10 Store (st, at, Gt) in D2 11 if len(D2) > K2 then 12 Sample mini-batch from D2 13 Update η with loss L3 in Eq. 8 14 Update θ with loss L2 in Eq. 7",2.4 Training the Policy-Based Agent,[0],[0]
We use a user simulator to train our questioner agent and test the agent with the simulated answerer and real users.,3 Experimental Setup,[0],[0]
"Specifically, our experiments answer three questions: (1) Is our method more robust in real-world Q20 games, compared to the methods based on relevance table?",3 Experimental Setup,[0],[0]
(Section. 4.2),3 Experimental Setup,[0],[0]
And how does it perform in the simulation environment?,3 Experimental Setup,[0],[0]
(Section. 4.1) (2) Does our RewardNet help in the training process?,3 Experimental Setup,[0],[0]
"(Section. 4.3) (3) How the winning rate grows with the number of questions, and whether it is possible to stop earlier?",3 Experimental Setup,[0],[0]
(Section. 4.4),3 Experimental Setup,[0],[0]
Training the RL agent is challenging because the agent needs to continuously interact with the environment.,3.1 User Simulator,[0],[0]
"To speed up the training process of the proposed RL model, we construct a user simulator
which has enough prior knowledge to choose objects and answer questions selected by the agent.
",3.1 User Simulator,[0],[0]
"We collect 1,000 famous people and 500 questions for them.",3.1 User Simulator,[0],[0]
"Besides, for every person-question pair in our dataset, a prior frequency distribution over “Yes”, “No” and “Unknown” is also collected from thousands of real users.",3.1 User Simulator,[0],[0]
"For example, as for “Donald Trump”, question (a) “Is your role the American president?” is answered with “Yes” for 9,500 times, “No” for 50 times and “Unknown” for 450 times.",3.1 User Simulator,[0],[0]
"We use Eq.4 and 5 to construct three matrices R,W,U ∈ R|O|∗|A| (|O| = 1000, |A| = 500) which are used for state transition in the Section. 2.1.",3.1 User Simulator,[0],[0]
"Then given the object oi and question qj , the user simulator answers “Yes”, “No” and “Unknown” whenR(i, j),W (i, j), andU(i, j) has the max value among them respectively.
",3.1 User Simulator,[0],[0]
"Constructed by the prior knowledge, the simulator can give noise-free answer in most cases.",3.1 User Simulator,[0],[0]
"Because the prior frequency distribution for each person-question pair is collected from thousands of users with the assumption that most of them do not lie when answering questions in the Q20 game.
",3.1 User Simulator,[0],[0]
"In an episode, the simulator randomly samples a person following the object distribution s0, which is generated from the object popularity (see the state part of Section. 2.1), as the target object.",3.1 User Simulator,[0],[0]
Then the agent gives a guess when the number of selected questions reaches 20.,3.1 User Simulator,[0],[0]
"After that, the simulator check the agent’s answer and return a reward signal of win or loss.",3.1 User Simulator,[0],[0]
There is only one chance for the agent to guess in an episode.,3.1 User Simulator,[0],[0]
The win and loss reward are 30 and -30 respectively.,3.1 User Simulator,[0],[0]
"While the architectures of the policy network, RewardNet and value network can vary in different scenarios, in this paper, we simply use the MLP with one hidden layer of size 1,000 for all of them, but with different parameters.",3.2 Implementation Details,[0],[0]
"These networks take in the state vector directly, which is a probability distribution over all objects.",3.2 Implementation Details,[0],[0]
The RewardNet further takes in the one-hot vector of action at.,3.2 Implementation Details,[0],[0]
"Based on the input of RewardNet, the objectaware RewardNet takes one more target object otgt as the feature which is also a one-hot vector.
",3.2 Implementation Details,[0],[0]
"We use the ADAM optimizer (Kingma and Ba, 2014) with the learning rate 1e-3 for policy network and 1e-2 for both RewardNet and value network.",3.2 Implementation Details,[0],[0]
The discounted factor γ for calculating the long-term return is 0.99.,3.2 Implementation Details,[0],[0]
"The model was trained up
to 2,000,000 steps (2,00,000 games) and the policy network was evaluated every 5,000 steps.",3.2 Implementation Details,[0],[0]
"Each evaluation records the agent’s performance with a greedy policy for 2,000 independent episodes.",3.2 Implementation Details,[0],[0]
"The 2,000 target objects for these 2,000 episodes are randomly selected following the distribution s0, which is generated from the object popularity and kept the same for all the training settings.",3.2 Implementation Details,[0],[0]
"We compare our RL method with the entropybased model proposed by Wu et al. (2018), which utilizes the real-world answers to each objectquestion pair to calculate an object-question relevance matrix with the entropy-based method.",3.3 Competitor,[0],[0]
The relevance matrix is then used for question ranking and object ranking via carefully designed formulas and engineering tricks.,3.3 Competitor,[0],[0]
"Since this method is shown to be effective in their production environment, we consider it to be a strong baseline to our proposed RL model.",3.3 Competitor,[0],[0]
"We first evaluate our agent and the entropy-based baseline (referred to as EntropyModel, see Section. 3.3) by using the simulated user (Section. 3.1).",4.1 Simulated Evaluation,[0],[0]
"To investigate which initialization strategy of the state s0 is better (see the state part of Section. 2.1), we further evaluate two variants of our model: the agent with uniform distribution s0 (RL uniform) and the agent with the distribution s0 initialized by the prior knowledge on the object popularity (RL popularity).
",4.1 Simulated Evaluation,[0],[0]
"Fig. 2 shows the curves on the win rate of these methods evaluated on 2,000 independent episodes
with respect to the number of training steps.",4.1 Simulated Evaluation,[0],[0]
"Note that, the EntropyModel only needs to update its statistics during training and has already accumulated a significant number of data since it has been run for over a year in their production environment.",4.1 Simulated Evaluation,[0],[0]
"Therefore, only a small fraction of its statistics can be changed, which leads to a small rise at the beginning of training, and its win rate remains at around 95% afterwards.
",4.1 Simulated Evaluation,[0],[0]
"On the other hand, both our RL models continuously improve the win rate with the growing number of interactions with the user simulator, and they achieve 50% win rate after around 20,000 steps.",4.1 Simulated Evaluation,[0],[0]
"As we can see, although the s0 initialized with the prior knowledge of object popularity keeps consistent with the object selection strategy of the simulator, the agent with uniform distribution s0 (RL uniform) still performs clearly better than the agent with s0 based on the prior knowledge (RL popularity).",4.1 Simulated Evaluation,[0],[0]
The reason is that the former can explore the Q20 game environment more fully.,4.1 Simulated Evaluation,[0],[0]
The prior knowledge based s0 helps the agent narrow down the candidate space more quickly when the target object is a popular object.,4.1 Simulated Evaluation,[0],[0]
"However, it also becomes misleading when the target object is not popular and makes the agent even harder to correct the confidence of the target object.",4.1 Simulated Evaluation,[0],[0]
"On the contrary, the uniform distribution s0 makes the agent keep track of the target object only based on the user’s answers.",4.1 Simulated Evaluation,[0],[0]
"And the superior performance of the RL uniform indicates that our question selection policy is highly effective, which means it is not necessary to use the RL popularity to increase the win rate of hot objects in the game.
",4.1 Simulated Evaluation,[0],[0]
"As shown in Fig. 2, RL uniform achieves win rate 94% which is very close to EntropyModel.",4.1 Simulated Evaluation,[0],[0]
"Compared to our RL method, EntropyModel needs more user data to calculate their entropybased relevance matrix and involves many engineering tricks.",4.1 Simulated Evaluation,[0],[0]
The fact that RL uniform is competitive to EntropyModel in the noise-free simulation environment indicates that our RL method is very cost-effective: it makes use of user data more efficiently and is easier to implement.,4.1 Simulated Evaluation,[0],[0]
"To further investigate the performance of our RL method in the real-world Q20 game where noisy answers are common, we also conduct an human evaluation experiment.",4.2 Human Evaluation,[0],[0]
"Specifically, we let real
users to play the game with EntropyModel and RL uniform for 1,000 times respectively.",4.2 Human Evaluation,[0],[0]
"In the real-world Q20 game, users sometimes make mistakes when they answer the questions during the game.",4.2 Human Evaluation,[0],[0]
"For example, as for the target object “Donald Trump”, question (a) “Is your role the American president?” is sometimes answered with “No” or “Unknown” by real users.",4.2 Human Evaluation,[0],[0]
"On the contrary, the simulator hardly makes such mistakes since we have provided it with enough prior knowledge.",4.2 Human Evaluation,[0],[0]
As shown in Table.,4.2 Human Evaluation,[0],[0]
"1, RL uniform outperforms EntropyModel by about 4.5% on win rate in the real-world Q20 games.",4.2 Human Evaluation,[0],[0]
It shows that our RL method is more robust to noisy answers than EntropyModel.,4.2 Human Evaluation,[0],[0]
"Specifically, the robustness of our RL method to the noise is shown in the following two aspects.",4.2 Human Evaluation,[0],[0]
"First, compared to the rulebased statistics update in EntropyModel, our RL model can be trained by modern neural network optimizers in a principled way, which results in the better generalization ability of our model.",4.2 Human Evaluation,[0],[0]
"Secondly, different from the EntropyModel selecting the top-ranked question at each time-step, RL uniform samples a question following its question probability distribution πθ(a|s), which enables our agent to jump out of the local optimum caused by incorrect answers from users.",4.2 Human Evaluation,[0],[0]
"And since more randomness is introduced by sampling from the question probability distribution during training, it also improves the tolerance of our model towards the unexpected question sequences.
",4.2 Human Evaluation,[0],[0]
"Besides, we also find some interesting cases during human evaluation.",4.2 Human Evaluation,[0],[0]
"Sometimes, the RL agent selects a few strange questions which seems to be not that much relevant to the chosen object, but it can still find the correct answer at the end of game.",4.2 Human Evaluation,[0],[0]
"This situation is caused by the fact that our method samples questions based on the output of policy net, rather than greedy selection during training.",4.2 Human Evaluation,[0],[0]
We find that this phenomenon increases the user experience since it makes the agent more unpredictable to the users.,4.2 Human Evaluation,[0],[0]
"To investigate the effectiveness of our RewardNet (Section. 2.3), we further evaluate three variants of our model in the simulation environment: the model trained with with direct reward, RewardNet, and object-aware RewardNet, which are referred to as DirectReward, RewardNet, and ObjectRewardNet respectively.",4.3 The Effectiveness of RewardNet,[0],[0]
"They are all trained with the uniform distribution s0.
",4.3 The Effectiveness of RewardNet,[0],[0]
"As shown in Fig. 3, DirectReward converges in the early steps and has a relatively poor performance with the win rate 89%.",4.3 The Effectiveness of RewardNet,[0],[0]
Both RewardNet and ObjectRewardNet achieve the better performance with a win rate of 94% after convergence.,4.3 The Effectiveness of RewardNet,[0],[0]
"This clear improvement shows that the more informative long-term return, calculated with the immediate reward delivered by our RewardNet method, significantly helps the training of the agent.
",4.3 The Effectiveness of RewardNet,[0],[0]
"Furthermore, as shown in Fig. 3, we can also see that ObjectRewardNet learns faster than RewardNet in the early steps.",4.3 The Effectiveness of RewardNet,[0],[0]
"This indicates that ObjectRewardNet can estimate the immediate reward more quickly with the extra information provided by the target object, which leads to the faster convergence of the agent.",4.3 The Effectiveness of RewardNet,[0],[0]
"In this section, we investigate how the win rate grows with the number of asked questions and whether a early-stop strategy can be adopted in the game.",4.4 Win Rate Regarding Question Numbers,[0],[0]
"We use the user simulator to play the game with the RL uniform agent and two settings are taken into account: the simulator samples the target object following the uniform object distribution (UnifSimulator), and samples following
the prior object distribution based on the object popularity (PopSimulator).",4.4 Win Rate Regarding Question Numbers,[0],[0]
"We perform 1,000 simulations for each number of questions, and the win rate curve is shown in Fig. 4.
",4.4 Win Rate Regarding Question Numbers,[0],[0]
As we can see that UnifSimulator achieves the win rate of 80% with only 14 questions in both settings.,4.4 Win Rate Regarding Question Numbers,[0],[0]
And the flat curves in the region after 18 questions indicate that the game can be early stopped with the almost same win rate at step 18.,4.4 Win Rate Regarding Question Numbers,[0],[0]
"Since a lower win rate is acceptable sometimes, other early-stop strategies can also be derived for the better user experience with the trade-off between the win rate and game steps.
",4.4 Win Rate Regarding Question Numbers,[0],[0]
"Besides, the fact that RL uniform performs similarly under both settings actually shows that our RL method is robust to different objects.",4.4 Win Rate Regarding Question Numbers,[0],[0]
It also performs well on infrequent objects where we may have the limited user data for constructing a well-tuned state transition dynamics.,4.4 Win Rate Regarding Question Numbers,[0],[0]
"When our agent is playing the game with real users, we select two cases from records.",4.5 Case Study,[0],[0]
"In the first case, the person that the user chooses is Cristiano Ronaldo, the famous football player.",4.5 Case Study,[0],[0]
"As we can see in Tab. 2, our agent can still figure out the target person while No.17 and No.19 questions are answered wrong by the user, which indicates our agent is robust to noisy answers.",4.5 Case Study,[0],[0]
"In the second case, the chosen person is Napoleon Bonaparte who was the French Emperor.",4.5 Case Study,[0],[0]
"Although there are some other candidates satisfied the constraints, the target person can be figured out because of the people popularity, which is shown in Tab. 3.",4.5 Case Study,[0],[0]
Q20.,5 Related Work,[0],[0]
"The Q20 game is popularized as an electronic game by the program of Robin Burgener in 1988 (Burgener, 2006), which uses a objectquestion relevance table to rank questions and target objects.",5 Related Work,[0],[0]
"Wu et al. (Wu et al., 2018) improves the relevance table with entropy-based metrics, and uses complicated engineering tricks to make it perform quite well in their production environment.",5 Related Work,[0],[0]
"These table-based methods use rules to update parameters, which makes them easily affected by noisy answers.",5 Related Work,[0],[0]
"Besides, Zhao and Maxine (2016) also explores Q20 in their dialogue state tracking research.",5 Related Work,[0],[0]
"However, they only use a small toy Q20 setting where the designed questions are about 6 person attributes in the Knowledge Base (KB).",5 Related Work,[0],[0]
"Since their method relies on the KB for narrowing down the scope of target object, it is not applicable to real-world Q20 games where a welldefined object KB is often unavailable.",5 Related Work,[0],[0]
"Compared to previous approaches, our RL method is robust to the answer noise and does not rely on the KB.
",5 Related Work,[0],[0]
Deep Reinforcement Learning.,5 Related Work,[0],[0]
"DRL has witnessed great success in playing complex games like Atari games (Mnih et al., 2015) , Go (Silver et al., 2016), and etc.",5 Related Work,[0],[0]
"In the natural language processing (NLP), DRL is also used to play text-based games (Narasimhan et al., 2015), and used to handle fundamental NLP tasks like machine translation (He et al., 2016) and machine comprehension (Hu et al., 2017) as well.",5 Related Work,[0],[0]
Our Q20 game lies in the intersection of the field of game and NLP.,5 Related Work,[0],[0]
"In this work, we propose a policy-based RL model that acts as the questioner in the Q20 game, and it exhibits the superior performance in our human evaluation.
",5 Related Work,[0],[0]
Natural Language Games.,5 Related Work,[0],[0]
"In the literature, there are some works focusing on solving and generating English riddles (De Palma and Weiner, 1992; Binsted, 1996) and Chinese character riddles (Tan et al., 2016).",5 Related Work,[0],[0]
"Compared to riddles, the Q20 game is a sequential decision process which requires careful modeling of this property.",5 Related Work,[0],[0]
"In this paper, we propose a policy-based RL method to solve the question selection problem in the Q20 Game.",6 Conclusions,[0],[0]
"Instead of using the direct reward, we further propose an object-aware RewardNet to estimate the appropriate non-zero reward and
thus make the long-time return more informative.",6 Conclusions,[0],[0]
"Compared to previous approaches, our RL method is more robust to the answer noise which is common in the real-world Q20 game.",6 Conclusions,[0],[0]
"Besides, our RL agent can also ask various questions and does not require the existing KB and complicated engineering tricks.",6 Conclusions,[0],[0]
"The experiments on a noisy-free simulation environment show that our RL method is competitive to an entropy-based engineering system, and clearly outperforms it on the human evaluation where noisy answers are common.
",6 Conclusions,[0],[0]
"As for the future work, we plan to explore methods to use machine reading to automatically construct the state transition dynamics from corpora like Wikipedia.",6 Conclusions,[0],[0]
"In this way, we can further build an end-to-end framework for the large-scale Q20 games in the real world.",6 Conclusions,[0],[0]
We gratefully thank the anonymous reviewers for their insightful comments and suggestions on the earlier version of this paper.,Acknowledgement,[0],[0]
The first author also thanks the Microsoft for providing resources for the research.,Acknowledgement,[0],[0]
The 20 Questions (Q20) game is a well known game which encourages deductive reasoning and creativity.,abstractText,[0],[0]
"In the game, the answerer first thinks of an object such as a famous person or a kind of animal.",abstractText,[0],[0]
Then the questioner tries to guess the object by asking 20 questions.,abstractText,[0],[0]
"In a Q20 game system, the user is considered as the answerer while the system itself acts as the questioner which requires a good strategy of question selection to figure out the correct object and win the game.",abstractText,[0],[0]
"However, the optimal policy of question selection is hard to be derived due to the complexity and volatility of the game environment.",abstractText,[0],[0]
"In this paper, we propose a novel policy-based Reinforcement Learning (RL) method, which enables the questioner agent to learn the optimal policy of question selection through continuous interactions with users.",abstractText,[0],[0]
"To facilitate training, we also propose to use a reward network to estimate the more informative reward.",abstractText,[0],[0]
"Compared to previous methods, our RL method is robust to noisy answers and does not rely on the Knowledge Base of objects.",abstractText,[0],[0]
Experimental results show that our RL method clearly outperforms an entropy-based engineering system and has competitive performance in a noisyfree simulation environment.,abstractText,[0],[0]
Playing 20 Question Game with Policy-Based Reinforcement Learning,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 92–102 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Every public speech involving a large audience can be seen as a game of coordination (Asch, 1951): at each moment, each individual member of the audience must decide in a split second whether to applaud at what has just been said.",1 Introduction,[0],[0]
"Applause is a potentially risky action: if an individual spontaneously claps but no one joins in, they suffer some negative social cost; the game is to judge from their own private information and content of the speech whether the rest of the audience will applaud at the same time they do.
",1 Introduction,[0],[0]
"Because of this cost, audiences respond to several interacting factors in a speaker’s behavior: a.) the content of the message; b.) their delivery (so that changes in pitch, duration and gaze signal salient moments for which applause may be licensed); and c.) the verbal design of the message—those rhetorical strategies that speakers use to signal that applause is welcome (Atkinson, 1984; Heritage and Greatbatch, 1986).
",1 Introduction,[0],[0]
"In this work, we attempt to model all three of these dimensions in developing a computational model for applause.",1 Introduction,[0],[0]
"While past work has focused on these elements in isolation (Guerini et al., 2015; Liu et al., 2017) or for related problems such as laughter detection (Purandare and Litman, 2006; Chen and Lee, 2017; Bertero and Fung, 2016), we find that developing a holistic model encompassing all three aspects yields the most robust predictor of applause.
",1 Introduction,[0],[0]
"We focus on political speeches, and in particular those at campaign rallies, which lend themselves well to analysis of rhetorical strategies for several reasons.",1 Introduction,[0],[0]
"First, the speakers at these events prioritize maintaining the crowd’s attention (Strangert, 2005).",1 Introduction,[0],[0]
"Motivated to drum up excitement and fervor among their supporters that they hope will carry beyond the event and into the voting booth, speakers pull out their strongest rhetorical tactics.",1 Introduction,[0],[0]
"Second, campaign speeches usually consist of a series of self-contained messages that can be fully expressed within a few utterances (Heritage and Greatbatch, 1986), yielding a well-defined observation of a complete rhetorical strategy.",1 Introduction,[0],[0]
"Lastly, these speeches are delivered by a single speaker to a partisan crowd, and clapping, cheering, and other responses are invited and expected.
",1 Introduction,[0],[0]
"We focus in particular in this work on operationalizating the verbal design of the speech; in so doing, one contribution we make is operationalizing the concepts of tension and release.",1 Introduction,[0],[0]
"Writers and performers often communicate with their audience on a fundamental level by building up tension, and then, at the proper time, delivering a satisfying release.",1 Introduction,[0],[0]
"These simple but pervasive concepts structure our experience of different modes of communication used throughout everyday life, including music (Madsen and Fredrickson, 1993), literature (Rabkin, 1973) and film (Carroll, 1996).
",1 Introduction,[0],[0]
"Tension in music can be built up by harmonic
92
movement away from a tonal center; release then comes with a return to that established tonic (Hindemith, 1937).",1 Introduction,[0],[0]
"One form of tension in literature is realized as suspense (Barthes and Duisit, 1975; Vorderer et al., 1996; Algee-Hewitt, 2016), in which a reader’s knowledge of events is uncertain (either because those events take place in the narrative future or are withheld from narration), and released when that knowledge is revealed.",1 Introduction,[0],[0]
"In film, sudden changes in camera perspective create graphic tension, which is then released as the shot returns to a stable position (Bordwell, 2013).",1 Introduction,[0],[0]
"Often, it is the confluence of multiple sources of tension that mark the climax of a narrative (Hume, 2017).",1 Introduction,[0],[0]
"We draw on each of these strands of work in operationalizing tension and release as a rhetorical strategy.
",1 Introduction,[0],[0]
"In this work, we make the following contributions:
• We collect a new dataset of text and audio from 310 speeches from campaign events leading up to the 2016 U.S presidential election with associated tags for over 19,000 instances of audience applause.
",1 Introduction,[0],[0]
"• We introduce new textual and acoustic features inspired by tension and release, combine and compare them with features used in previous work, and deploy those features in a logistic regression model and in an LSTM to predict when applause is likely to occur.",1 Introduction,[0],[0]
"Code, data, and trained models are openly available to the public at https://github.com/ jrgillick/Applause/.",1 Introduction,[0],[0]
"Heritage and Greatbatch (1986) conduct an extensive analysis of nearly 500 speeches from British political party conferences, manually associating each of over 2000 instances of applause with coded message types (e.g. External Attacks or Statements of Approval), rhetorical devices (e.g. Contrast/Antithesis or Headline-Punchline), and performance factors (e.g. speech stress or body language).",2.1 Rhetoric and Response,[0],[0]
"They find most of these factors to be positively correlated with applause; one especially striking result is over two thirds of observed instances of applause can be explained through a set of seven rhetorical devices (including contrast,
pursuit, position taking, and “the 3-part list”).",2.1 Rhetoric and Response,[0],[0]
"Though each device is different, a common feature of most of these techniques is that they are not always carried out within a single sentence or utterance; they often depend on the relationship between a series of utterances or phrases.",2.1 Rhetoric and Response,[0],[0]
We argue in this work that some of these relationships can be characterized and subsequently operationalized within models as tension and release.,2.1 Rhetoric and Response,[0],[0]
Recent work from Guerini et al. (2015) and Liu et al. (2017) approaches the task of applause prediction by looking at textual features of the individual sentences that immediately precede audience applause.,2.2 Predicting Applause,[0],[0]
"Both follow the methodology proposed by Danescu-Niculescu-Mizil et al. (2012) in constructing a data set for binary classification, which is composed of sentences that generated applause, each paired with a single nearby sentence from the same document that did not lead to applause.
",2.2 Predicting Applause,[0],[0]
"Guerini et al. (2015) examine a set of features designed to capture aspects of euphony, or “the inherent pleasantness of the sounds of words” that might make an utterance memorable or persuasive—such as rhyme, alliteration, homogeneity, and plosives.",2.2 Predicting Applause,[0],[0]
"On the CORPS dataset (Guerini et al., 2013), which consists of the text of several thousand political speeches dating from 1917 to 2011, they define persuasive sentences as those that preceded annotations of either applause or laughter.
",2.2 Predicting Applause,[0],[0]
"Liu et al. (2017), working with a corpus of TED talks, use logistic regression to predict applause from sentences using a combination of features: euphony (again from Guerini et al. (2015)), linguistic style markers derived from membership in LIWC categories, markers of emotional expression derived from membership in the NRC Emotion Lexicon, mentions of names, rhetorical questions (string matching for “?”), expressions of gratitude (matching a handcrafted list of word stems including “thank∗” and “grateful∗”), and expressions seeking applause (matching the pattern “applau∗”).",2.2 Predicting Applause,[0],[0]
Liu et al. (2017) also report that adding the same features for earlier sentences beyond the final sentence that preceded the applause caused the prediction accuracy to go down.,2.2 Predicting Applause,[0],[0]
"Chen and Lee (2017) and Bertero and Fung (2016) run similar binary classification experiments but pre-
dict laughter as opposed to applause.",2.2 Predicting Applause,[0],[0]
Bertero and Fung (2016) analyze punchlines from the TV sitcom “The Big Bang Theory” and report 70% accuracy using an LSTM.,2.2 Predicting Applause,[0],[0]
"They touch briefly on the notion of tension and release in humor, as punchlines typically depend on a previous line as a setup in order to be funny.",2.2 Predicting Applause,[0],[0]
"In this work, we focus on a new data set of campaign speeches from the 2016 U.S. presidential race, which we obtain from the public domain broadcasts of C-SPAN.",3.1 Corpus Acquisition,[0],[0]
"We downloaded about 500 speeches from presidential candidates, vice presidential candidates, or former presidents, collecting audio files and transcripts that were tagged in the categories “Campaign 2016” and “Speech” and which took place between 12/01/2015 and 12/01/2016.",3.1 Corpus Acquisition,[0],[0]
"We then excluded events that took place outside of a traditional campaign speech setting (e.g. town hall events) or events that contained multiple speakers without a speaker identification tied to the transcript, which yielded a final set of 310 speeches from 16 speakers.",3.1 Corpus Acquisition,[0],[0]
"Because different types of events have different social norms around when and whether applause is appropriate (Atkinson, 1984; Heritage and Greatbatch, 1986), we control for these factors to some degree by restricting our dataset to events in similar settings and within a single year.",3.1 Corpus Acquisition,[0],[0]
"As a point of comparison, the C-SPAN dataset contains 62 instances of applause per speech on average, whereas the CORPS data (Guerini et al., 2013) contains 13.",3.1 Corpus Acquisition,[0],[0]
"Since our C-SPAN data originates in video, we have access to the audio information of a speech event, which we employ both for feature extraction and for automatically identifying when applause occurs.",3.2 Applause Detection in Audio,[0],[0]
"Following Clement and McLaughlin (2016), we train an acoustic model using a set of poetry readings from the PennSound archive to distinguish applause from speech.",3.2 Applause Detection in Audio,[0],[0]
We used logistic regression on the standard set of MFCC features and found similar results on the PennSound data to the reported classification accuracy of 99.4%.,3.2 Applause Detection in Audio,[0],[0]
"In a manual inspection of 100 applause segments from 5 different speeches in the C-SPAN corpus, our applause detector achieved 92% preci-
sion, 90% recall, and 91% F1 score.",3.2 Applause Detection in Audio,[0],[0]
"Due to variation in the nature of applause in a crowd (sometimes we observe examples of isolated clapping and cheering, mixed laughter and applause, or applause interrupting the speaker), some ambiguity is inherent among the labels.
",3.2 Applause Detection in Audio,[0],[0]
"We also measure the applause by first running the speeches through the audio source separation algorithm from Chandna et al. (2017), which was trained to separate voice from music, and then measuring the RMSE loudness of the separated non-vocal track.",3.2 Applause Detection in Audio,[0],[0]
"We found that the separation worked well, qualitatively matching with the results from the applause detection classifier.",3.2 Applause Detection in Audio,[0],[0]
"To match the identified segments of applause in the audio files with the relevant text from the transcriptions, we ran forced alignment using the Kaldi Toolkit (Povey et al., 2011).",3.3 Forced Alignment,[0],[0]
"Since the CSPAN transcripts are sourced from uncorrected closed captioning, the text contains a number of misspellings and paraphrases, which we handled by discarding the 12% of words for which forced alignment failed.",3.3 Forced Alignment,[0],[0]
"Though these transcriptions are not as accurate as what we would find in professionally transcribed datasets, previous work has shown that it is possible to achieve good accuracy in downstream tasks even with high error rates in transcription (Peskin et al., 1993; Novotney and Callison-Burch, 2010).",3.3 Forced Alignment,[0],[0]
"Moreover, the caliber of transcripts derived from closed captioning is representative of the data that would be available in real time for practical use at future speech events.
",3.3 Forced Alignment,[0],[0]
"To estimate the accuracy of the closed captions, we manually transcribed selections from 5 speeches in the C-SPAN data totaling about 25 minutes and 2250 words, finding 30.9% WER relative to the reference transcriptions in our sample.",3.3 Forced Alignment,[0],[0]
"Many of the errors are due to omitted words and phrases in the closed captions, which may occur as a result of transcribers’ inability to keep up with the pace of fast speeches; in this sample, the closed caption texts contained 17% fewer words than our gold standard transcriptions.
",3.3 Forced Alignment,[0],[0]
"After finding the alignments, we segmented out a list of utterances by defining a minimum period of silence between words.",3.3 Forced Alignment,[0],[0]
"Since many of the transcripts do not have punctuation, we find that dividing the text into utterances yielded qualitatively more coherent units than sentence boundary detec-
tion.",3.3 Forced Alignment,[0],[0]
"Dividing into utterances is also conducive to building a dataset for binary classification, since every pause by the speaker yields an opportunity for applause.",3.3 Forced Alignment,[0],[0]
"We chose a pause length of 0.7 seconds, but in future work we might be able to improve our models by adapting this threshold to the rate of speech in order to maintain consistent phrase sizes across different speakers.",3.3 Forced Alignment,[0],[0]
"Given this set of utterances, we paired each utterance with a “positive” or “negative” label, determined by whether applause occurred within 1.5 seconds of the end of the utterance.",3.3 Forced Alignment,[0],[0]
"All of these preprocessing choices were made during the corpus preparation phase, prior to any experimental evaluation.
",3.3 Forced Alignment,[0],[0]
"Table 1 provides summary statistics for the number of speakers, speeches, utterances, and acts of applause in our data.",3.3 Forced Alignment,[0],[0]
"In our models, we draw features from previous work on applause or humor prediction and then supplement them with a new set of features inspired by the ideas of tension and release and by the rhetorical strategies of Heritage and Greatbatch (1986).",4 Models,[0],[0]
LIWC.,4.1 Features adapted from existing work,[0],[0]
"Features for membership in 73 LIWC categories proved to be the most effective for applause prediction in TED talks (Liu et al., 2017).
",4.1 Features adapted from existing work,[0],[0]
Euphony.,4.1 Features adapted from existing work,[0],[0]
We adopt the 4 features for “euphony” defined by Guerini et al. (2015):,4.1 Features adapted from existing work,[0],[0]
"Rhyme, Alliteration, Homogeneity, and Plosives.
",4.1 Features adapted from existing work,[0],[0]
Lexical.,4.1 Features adapted from existing work,[0],[0]
Guerini et al. (2015) find n-grams to be highly predictive of both applause and laughter.,4.1 Features adapted from existing work,[0],[0]
"We operationalize these features with bigrams, including in our model all bigrams that appear at least 5 times in the corpus.
Embeddings.",4.1 Features adapted from existing work,[0],[0]
Bertero and Fung (2016) use sentence embeddings learned from a CNN encoder as input to an LSTM.,4.1 Features adapted from existing work,[0],[0]
"We adopt this feature for use in our neural models, encoding phrases using the Skip-Thought model of Kiros et al. (2015).
Acoustic.",4.1 Features adapted from existing work,[0],[0]
Purandare and Litman (2006) use a set of features intended to capture elements of prosody in a model for humor prediction in television dialogue.,4.1 Features adapted from existing work,[0],[0]
"These features include the mean, max, min, range, and standard deviation values in an utterance’s pitch (F0) and energy (RMS), along with features for internal silence and for tempo.",4.1 Features adapted from existing work,[0],[0]
"We compute the F0 statistics with Reaper (Talkin, 2015) and the energy statistics with Librosa (McFee et al., 2015).",4.1 Features adapted from existing work,[0],[0]
Repeated Words.,4.2.1 Repetition,[0],[0]
Rhetorical strategies such as “The 3-part List” and “Contrast” rely on repetition to drive home important points.,4.2.1 Repetition,[0],[0]
"We capture this phenomenon by computing the proportion of words in each utterance that also appear in the immediately preceding phrase.
",4.2.1 Repetition,[0],[0]
Longest Common Subsequence.,4.2.1 Repetition,[0],[0]
"Repeating an entire phrase, especially one with a politically charged topic, serves to build tension through the notion of “theme and variation” as is often realized
in music (Cope, 2005); an example of this phenomenon in our data can be found in the following passage:
We will not allow the party of Lincoln and Reagan to fall into the hands of a con artist.",4.2.1 Repetition,[0],[0]
We will not allow the next president of the United States to be a socialist like Bernie Sanders.,4.2.1 Repetition,[0],[0]
"And we will not allow the next president of the United States to be someone under FBI investigation like Hillary Clinton.
",4.2.1 Repetition,[0],[0]
"[Marco Rubio, Mar. 1, 2016]
We calculate this theme and variation by measuring the longest common subsequence between adjacent phrases.",4.2.1 Repetition,[0],[0]
"Delta features (local approximations to derivatives) are commonly used in speech recognition and audio classification systems (Povey et al., 2011).",4.2.2 Deltas,[0],[0]
"In a discourse, either highly similar or drastically different neighboring pairs of utterances may indicate dramatic moments.",4.2.2 Deltas,[0],[0]
"We operationalize these features by explicitly adding a delta measurement for every feature in our model, which captures the difference between every feature at time t and the same feature at time t − 1.",4.2.2 Deltas,[0],[0]
"For K-dimensional vector embeddings, we calculate deltas as their cosine distance.",4.2.2 Deltas,[0],[0]
"Rhetorical Structure Theory (RST) provides a foundation for describing the ways in which functional components of a text combine to form a coherent whole (Thompson and Mann, 1987).",4.2.3 RST,[0],[0]
At the core of RST is a categorization system consisting of relations between elementary discourse units (EDUs).,4.2.3 RST,[0],[0]
"Relations between units are typically hierarchical (a nucleus and a satellite), but can also be defined between equally significant units (two nuclei).
",4.2.3 RST,[0],[0]
"A typical RST tree can be seen below, where the sentence “He won’t win, but I’ll vote for him anyway”, he said is decomposed into three elementary discourse units (EDUs); those discourse units form the leaves of a tree with intermediate structure between subphrases and labeled edges along each branch.
",4.2.3 RST,[0],[0]
"ATTRIBUTION
CONTRAST
“He won’t win,
but I’ll vote for him anyway”
he said.
",4.2.3 RST,[0],[0]
"Some of the rhetorical strategies defined by Heritage and Greatbatch (1986), such as “Contrast,” map directly to RST relations, while others do not have a clear one-to-one mapping but are qualitatively similar in their descriptions.",4.2.3 RST,[0],[0]
"While RST has been used with success for classification problems in the past (Ji and Smith, 2017; Bhatia et al., 2015), it has not yet been employed in existing models for applause prediction.",4.2.3 RST,[0],[0]
"In our work, we parse the rhetorical structure of the extracted sequence of phrases using the RST parser of Ji and Eisenstein (2014).",4.2.3 RST,[0],[0]
"From the structure of this RST tree, we extract two classes of features.
RST label.",4.2.3 RST,[0],[0]
"First, we operationalize the rhetorical category for an individual elementary discourse unit.",4.2.3 RST,[0],[0]
While the span of text within a single EDU is implicated in several rhetorical relations throughout the tree (as He won’t win bears a CONTRAST relationship with,4.2.3 RST,[0],[0]
but I’ll vote for him anyway and is part of the ATTRIBUTION relationship with he,4.2.3 RST,[0],[0]
"said), each EDU bears exactly one leaf relationship with the rest of the tree—here, He won’t win is a nucleus of a CONTRAST relationship, but I’ll vote for him anyway is also a nucleus of a CONTRAST relationship, and he said is the satellite of an ATTRIBUTION relationship.
",4.2.3 RST,[0],[0]
"We featurize a sentence as the set of all such typed relationships that EDUs within it hold; each typed relationship is the conjunction of the label (e.g., CONTRAST, ATTRIBUTION) and directionality (Nucleus, Satellite).
",4.2.3 RST,[0],[0]
Rhetorical phrase closures.,4.2.3 RST,[0],[0]
"In order to further operationalize the notion of predictability of applause, we measure the number of rhetorical phrases that a given discourse segment brings to closure.",4.2.3 RST,[0],[0]
"We can illustrate this with figure 1, which presents a sample RST tree with only the spans annotated (i.e., without RST labels or nucleus/satellite directed edges).",4.2.3 RST,[0],[0]
"This tree spans 10 elementary discourse units; each non-terminal node is annotated with the span of the subtree
rooted at that node (so the root spans all ten EDUs, while its left child spans only the first five).",4.2.3 RST,[0],[0]
"The final discourse unit (EDU 10) is the final EDU in three rhetorical phrases (those spanning EDUs 9-10, 6-10 and the entire discourse 1-10).",4.2.3 RST,[0],[0]
"We might hypothesize that the greater number of discourse phrases that a given discourse unit closes, the stronger the signal it provides that applause is licensed (and hence the greater likelihood to be followed by applause empirically).",4.2.3 RST,[0],[0]
"For a sentence with multiple discourse units, we featurize this value as the maximum number of rhetorical phrases closed by any unit it contains.",4.2.3 RST,[0],[0]
"We present two experiments to uncover the degree to which we are able to predict applause from different operationalizations of a politician’s campaign speech: one in which have access to a politician’s previous speeches, and can learn their specific nuances and stock phrases used to solicit applause; and another in which we seek to uncover the broader rhetorical strategies common to multiple speakers.
",5 Experiments,[0],[0]
"We refer to the following sets of features when we summarize results:
• Guerini.",5 Experiments,[0],[0]
"Euphony features from Guerini et al. (2015).
",5 Experiments,[0],[0]
• Liu.,5 Experiments,[0],[0]
"LIWC features and additional matchers for handcrafted regular expressions from Liu et al. (2017)
• Audio.",5 Experiments,[0],[0]
All acoustic features described in §4.1 above.,5 Experiments,[0],[0]
•,5 Experiments,[0],[0]
Combined.,5 Experiments,[0],[0]
"Combination of features from
Guerini, Liu, and Audio.
• Tension.",5 Experiments,[0],[0]
"Combination of RST (§4.2.3), repetition (§4.2.1), and delta features (§4.2.2).",5 Experiments,[0],[0]
• N-gram.,5 Experiments,[0],[0]
Bigram features.,5 Experiments,[0],[0]
• Skip-Thought.,5 Experiments,[0],[0]
"4800 dimensional Skip-
Thought embeddings.",5 Experiments,[0],[0]
"Access to a politician’s previous speeches provides a great deal of evidence for understanding their rhetorical strategies for soliciting applause; speakers often give variations of the same speech at different campaign events, and rely on a fixed set of stock phrases (e.g., “Yes, We Can,” “Make America Great Again”) and general strategies to solicit reactions (Lu, 1999; Miller, 1939; Petrow and Sullivan, 2007).",5.1 Intra-speaker validation,[0],[0]
"To model this, we attempt to predict a speaker’s likelihood of applause using only information from their own speeches.
",5.1 Intra-speaker validation,[0],[0]
"We use logistic regression with `2 regularization for this experiment, with hyperparameters chosen through cross-validation on the training data.",5.1 Intra-speaker validation,[0],[0]
"We run 10-fold cross validation for each speaker, and leave-one-out cross validation for those speakers with fewer than 10 speeches (we exclude Rick Santorum from this experiment because we have only one speech from him), with whole speeches divided across folds so that no utterances from the same speech ever appear in both training and test sets.",5.1 Intra-speaker validation,[0],[0]
Reported results aggregate the predictions across all speakers to calculate the final accuracies.,5.1 Intra-speaker validation,[0],[0]
"We choose utterances (or sequences of utterances) that directly precede applause as positive examples, pairing each one with a negative example randomly chosen from the same speech.",5.1 Intra-speaker validation,[0],[0]
"Since we use different amounts of data for each speaker, we are not able to compare accuracies across all speakers, but we can see that some speakers are significantly easier to model: for example, our best model reaches 0.719 accuracy on Bernie Sanders but only 0.660 on Donald Trump.
",5.1 Intra-speaker validation,[0],[0]
"Table 2 summarizes the results, comparing across different combinations of features as well as across a scope of a single phrase or multiple phrases.",5.1 Intra-speaker validation,[0],[0]
All feature combinations are scoped over a single utterance unless otherwise noted.,5.1 Intra-speaker validation,[0],[0]
"At the same time, many of the strategies identified by Heritage and Greatbatch (1986) are gener-
alized rhetorical devices used to solicit applause; we should expect then that a model trained on a fixed set of speakers should be able to generalize to speakers not in the training data.",5.2 Inter-speaker validation,[0],[0]
"To test this more realistic scenario, we performed Kfold cross-validation on all of the speakers in our dataset, holding out one speaker in turn for each fold (so that the same speaker did not appear in the training and test partitions).
",5.2 Inter-speaker validation,[0],[0]
"In this experiment, we use both logistic regression and neural models (sharing training data between speakers has the added benefit of allowing us enough data to reasonably train a neural model).",5.2 Inter-speaker validation,[0],[0]
All logistic regression models were trained in the same way is in the intra-speaker case.,5.2 Inter-speaker validation,[0],[0]
Our feedforward and LSTM models use a hidden state size of 100 for models including phrase embeddings (4800 dimensions) and a hidden state of size 25 for models without phrase embeddings.,5.2 Inter-speaker validation,[0],[0]
"All LSTM models use a standard formulation of attention (Bahdanau et al., 2014), and all neural models are trained with dropout (Srivastava et al., 2014) and the ADAM optimizer (Kingma and Ba, 2014).",5.2 Inter-speaker validation,[0],[0]
"We implemented the models using Keras (Chollet et al., 2015) and Tensorflow (Abadi et al., 2016).
",5.2 Inter-speaker validation,[0],[0]
"Table 3 summarizes these results, and table 4 shows the coefficients for the most significant features.",5.2 Inter-speaker validation,[0],[0]
"Each of the feature classes we operationalize offers some ability to recognize what Heritage and Greatbatch (1986) term the “projectability” of applause—the ability of an audience to see an applaudable moment on the horizon.
Audio.",6 Analysis,[0],[0]
"Perhaps not surprising in retrospect is the ability of acoustic features (only summary statistics of the pitch and energy) to solicit applause:
higher pitch and energy, and a broader pitch range are all predictive of applause; while past work has focused on textual indicators of applause, these results suggest that how a message is delivered is equally important.
",6 Analysis,[0],[0]
Lexical.,6 Analysis,[0],[0]
"The use of explicit n-grams improves performance significantly in the intra-speaker setting, where they are able to capture stock phrases employed by the same speaker at different events.",6 Analysis,[0],[0]
"N-grams are also predictive across different speakers, though the performance gains are not as high in the inter-speaker setting.
",6 Analysis,[0],[0]
"The strongest bigrams predictive of applause include moral declaratives like should not (e.g., “and billionaires should not be able to buy elections”",6 Analysis,[0],[0]
"[Bernie Sanders]), right to (“you have a right to be angry”",6 Analysis,[0],[0]
"[Marco Rubio]), and should be (“They should be ashamed of that kind of behavior”",6 Analysis,[0],[0]
"[Hillary Clinton]); call-outs to the audience such as this room (“Love the people in this room”
[Donald Trump]) and listening to (“our campaign is listening to our Latino brothers and sisters”",6 Analysis,[0],[0]
"[Bernie Sanders]); and politically charged topics such as political revolution, equal pay, immigration reform, planned parenthood, campaign contributors and police officers.
LIWC.",6 Analysis,[0],[0]
"Among broader lexical category features, we see the LIWC FOCUSFUTURE category strongly indicative of applause; this category includes auxilaries like will, going, gonna (including conjunctions I’ll) and future-oriented verbs like anticipate; also important are categories of BODY (including heart, hands, brain) and REWARD (including succeed, optimism, great).
",6 Analysis,[0],[0]
Rhetorical.,6 Analysis,[0],[0]
"While RST features were not as predictive for applause as other (likely correlated) features, we still see a strong alignment between the RST features most associated with applause and those rhetorical devices outlined by Heritage and Greatbatch (1986): in particular, a clear relationship between applause and the RST category of ANTITHESIS (a contrastive relation between two discourse units with a clear nucleus and satellite, rather than two equal nuclei) and PURPOSE (a relation between a discourse unit that must take place in order for another to be realized).",6 Analysis,[0],[0]
"As expected, phrases that close more discourse units tend to be more predictive of applause.
Contextual.",6 Analysis,[0],[0]
"Though lexical features from the final utterance significantly outweigh the effects of previous context in the intra-speaker setting, in the inter-speaker case we leveraged gains from longterm context in the LSTM to reach a similar level of performance attained from the lexical features,
but without access to lexical cues provided by the n-grams at all.",6 Analysis,[0],[0]
"This result suggests that the improved performance in the intra-speaker setting may be largely due to the presence of specific words and catch-phrases; the other stylistic features are more easily generalized to new speakers.
7 “Please clap”
As a further measure of out-of-sample validity, we can analyze the predictions we make for the single example where a speaker wears his communicative intent on his sleeve.",6 Analysis,[0],[0]
"On February 2, 2016, presidential candidate Jeb Bush spoke to a crowd in New Hampshire a week before their state primary.",6 Analysis,[0],[0]
"His speech ended with the following:
So here’s my pledge to you.",6 Analysis,[0],[0]
"[I] will be a commander-in-chief who will have the back of the military, I won’t trash talk, I won’t be a divider-in-chief or an agitator-in-chief, I won’t be out there blowharding talking a big game without backing it up; I think the next President needs to be a lot quieter but send a signal that we’re prepared to act in the national security interests of this country to get back in the business of creating a more peaceful world . . . . . . . . .",6 Analysis,[0],[0]
"Please clap.
",6 Analysis,[0],[0]
"[Jeb Bush, Feb 2, 2016]1
Bush’s admonition to the audience (“please clap”) earned criticism in news coverage at the time (Benen, 2016), but also presents us with a rare insight into a speaker’s true rhetorical intention; in this case, Bush was soliciting applause and was vocal about not being able to do so.
",6 Analysis,[0],[0]
Does our model recover this true intention?,6 Analysis,[0],[0]
"Indeed it does; while the opening So here’s my pledge to you is predicted to not solicit applause (with applause probability of 24.8%), the segment that ends with peaceful world is strongly predicted to have been followed by applause",6 Analysis,[0],[0]
(with an applause probability of 94.5%).,6 Analysis,[0],[0]
"The strongest features are again lexical (this country, commander in chief ), a LIWC focus on the future (elicited by will), and an RST PURPOSE relation (evoked by to get back in the business of creating a more peaceful world).
",6 Analysis,[0],[0]
1Video of this speech can be found at: https://www.,6 Analysis,[0],[0]
youtube.com/watch?v=DdCYMvaUcrA,6 Analysis,[0],[0]
"We present in this work a new dataset for the analysis of political rhetoric derived from the public campaign speeches of politicians during the 2016 United States presidential election, along with empirical results assessing the performance of different operationalizations of rhetoric derived from the theoretical work of Heritage and Greatbatch (1986) and others in order to measure and predict the occurrence of applause.",8 Conclusion,[0],[0]
"We introduce several new features designed to capture elements of tension and release in public performance, including rhetorical contrast, closure, repetition and movement across speech segments; while each of these features in isolation is able to predict applause to varying degree and comport with our prior understanding of their utility, we find that lexicalized features are among the strongest source of information in determining applause; while audiences react to many dimensions of a speaker’s style, the words they use—as slogan, stock phrases, and indicators of more complex rhetorical functions like moral valuations and imperatives—matter most.
",8 Conclusion,[0],[0]
"As detailed in previous work (Liu et al., 2017; Haider et al., 2017; Clement and McLaughlin, 2016), understanding and identifying climactic moments in speeches can be useful for a variety of reasons, including learning to give better talks, automatically summarizing videos and transcripts, and analyzing social dynamics within crowds.",8 Conclusion,[0],[0]
"One additional interesting application of this work is to bring to the surface occasions where a speaker uses typical applause-seeking devices but does not receive applause (the “Please Clap” moments); we leave to future work identifying the reverse, when speakers receive applause without invoking common techniques (for example, to identify instances of claques paid to clap).",8 Conclusion,[0],[0]
Many thanks to the anonymous reviewers for their helpful feedback.,9 Acknowledgments,[0],[0]
The research reported in this article was supported by a UC Berkeley Fellowship for Graduate Study to J.G. and by resources provided by NVIDIA.,9 Acknowledgments,[0],[0]
This work examines the rhetorical techniques that speakers employ during political campaigns.,abstractText,[0],[0]
We introduce a new corpus of speeches from campaign events in the months leading up to the 2016 U.S. presidential election and develop new models for predicting moments of audience applause.,abstractText,[0],[0]
"In contrast to existing datasets, we tackle the challenge of working with transcripts that derive from uncorrected closed captioning, using associated audio recordings to automatically extract and align labels for instances of audience applause.",abstractText,[0],[0]
"In prediction experiments, we find that lexical features carry the most information, but that a variety of features are predictive, including prosody, long-term contextual dependencies, and theoretically motivated features designed to capture rhetorical techniques.",abstractText,[0],[0]
Please Clap: Modeling Applause in Campaign Speeches,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1763–1775 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1763",text,[0],[0]
Computing the co-occurrence strength between two linguistic expressions is a fundamental task in natural language processing (NLP).,1 Introduction,[0],[0]
"For example, in collocation extraction (Manning and Schütze, 1999), word bigrams are collected from corpora and then strongly co-occurring bigrams (e.g., “New York”) are found.",1 Introduction,[0],[0]
"In dialogue response selection (Lowe et al., 2015), pairs comprising a context and its response sentence are collected from dialogue corpora and the goal is to rank the candidate responses for each given context sentence.",1 Introduction,[0],[0]
"In either case, a set of linguistic expression pairs D = {(xi, yi)}ni=1 is first collected and then the co-occurrence strength of a (new) pair (x, y) is computed.
",1 Introduction,[0],[0]
"Pointwise mutual information (PMI) (Church and Hanks, 1989) is frequently used to model the co-occurrence strength of linguistic expression pairs.",1 Introduction,[0],[0]
There are two typical types of PMI estimation (computation) method.,1 Introduction,[0],[0]
"One is a countingbased estimator using maximum likelihood estimation, sometimes with smoothing techniques, for example,
P̂MIMLE(x, y;D)= log n · c(x, y)∑ y′c(x, y ′) ∑ x′c(x ′, y) ,
(1)
where c(x, y) denotes the frequency of the pair (x, y) in given dataD. This is easy to compute and is commonly used to measure co-occurrence between words, such as in collocation extraction1; however, when data D is sparse, i.e., when x or y is a phrase or sentence, this approach is unrealistic.",1 Introduction,[0],[0]
The second method uses recurrent neural networks (RNNs).,1 Introduction,[0],[0]
"Li et al. (2016) proposed to em1 In collocation extraction, simple counting c(x, y) ∝",1 Introduction,[0],[0]
"P̂(x, y), rather than PMI, ranks undesirable function-word pairs (e.g., “of the”) higher (Manning and Schütze, 1999).
",1 Introduction,[0],[0]
ploy PMI to suppress dull responses for utterance generation in dialogue systems2.,1 Introduction,[0],[0]
"They estimated P(y) and P(y|x) using RNN language models and estimated PMI as follows:
P̂MIRNN(x, y;D) = log P̂RNN(y|x) P̂RNN(y) .",1 Introduction,[0],[0]
"(2)
This way of estimating PMI is applicable to sparse language expressions; however, learning RNN language models is computationally costly.
",1 Introduction,[0],[0]
"To eliminate this trade-off between robustness to data sparsity and learning time, in this study we propose a new kernel-based co-occurrence measure, which we call the pointwise Hilbert–Schmidt independence criterion (PHSIC) (see Table 1).",1 Introduction,[0],[0]
"Our contributions are as follows: • We formalize PHSIC, which is derived from
HSIC (Gretton et al., 2005), a kernel-based dependence measure, in the same way that PMI is derived from mutual information (Section 3).",1 Introduction,[0],[0]
• We give an intuitive explanation why PHSIC is robust to data sparsity.,1 Introduction,[0],[0]
"PHSIC is a “smoothed variant of PMI”, which allows various similarity metrics to be plugged in as kernels (Section 4).",1 Introduction,[0],[0]
"• We propose fast estimators of PHSIC, which are reduced to a simple and fast matrix calculation regardless of whether we use linear or nonlinear kernels (Section 5).",1 Introduction,[0],[0]
"• We empirically confirmed the effectiveness of PHSIC, i.e., its robustness to data sparsity and learning time, in two different types of experiment, a dialogue response selection task and a data selection task for machine translation (Section 6).",1 Introduction,[0],[0]
"Let X and Y denote random variables on X and Y , respectively.",2 Problem Setting,[0],[0]
"In this paper, we deal with the tasks of taking a set of linguistic expression pairs
D = {(xi, yi)}ni=1 ∼i.i.d.",2 Problem Setting,[0],[0]
"PXY , (3)
which is regarded as a set of i.i.d.",2 Problem Setting,[0],[0]
"samples drawn from a joint distribution PXY , and then measuring the “co-occurrence strength” for each given pair (x, y) ∈ X × Y .",2 Problem Setting,[0],[0]
"Such tasks include collocation extraction and dialogue response selection (Section 1).
",2 Problem Setting,[0],[0]
2,2 Problem Setting,[0],[0]
"In dialogue response selection or generation, a simple conditional probability P̂(y|x), rather than PMI, ranks dull responses (e.g., “I don’t know.”) higher (Li et al., 2016).",2 Problem Setting,[0],[0]
"In this section, we give the formal definition of PHSIC, a new kernel-based co-occurrence measure.",3 Pointwise HSIC,[0],[0]
We show a summary of this section in Table 2.,3 Pointwise HSIC,[0],[0]
"Intuitively, PHSIC is a “kernelized variant of PMI.”",3 Pointwise HSIC,[0],[0]
"As a preliminary step, we introduce the simple concept of dependence (see Dependence Measure in Table 2).",3.1 Dependence Measure,[0],[0]
Recall that random variables X and Y are independent if and only if the joint probability density PXY and the product of the marginals PXPY are equivalent.,3.1 Dependence Measure,[0],[0]
"Therefore, we can measure the dependence between random variables X and Y via the difference between PXY and PXPY .
",3.1 Dependence Measure,[0],[0]
"Both the mutual information and the Hilbert– Schmidt independence criterion, to be described below, are such dependence measures.",3.1 Dependence Measure,[0],[0]
"We briefly review the well-known mutual information and PMI (see MI & PMI in Table 2).
",3.2 MI and PMI,[0],[0]
"The mutual information (MI)3 between two random variables X and Y is defined by
MI(X,Y ) := KL[PXY ‖PXPY ] (4)
(Cover and Thomas, 2006), where KL[·‖·] denotes the Kullback–Leibler (KL) divergence.",3.2 MI and PMI,[0],[0]
"Thus, MI(X,Y ) is the degree of dependence between X and Y measured by the KL divergence between PXY and PXPY .
",3.2 MI and PMI,[0],[0]
"Here, by definition of the KL divergence, MI can be represented in the form of the expectation over PXY , i.e., the summation over all possible pairs (x, y) ∈ X×Y:
MI(X,Y ) =",3.2 MI and PMI,[0],[0]
"E (x,y)
[ log PXY (x, y)
PX(x)PY",3.2 MI and PMI,[0],[0]
"(y)
] .",3.2 MI and PMI,[0],[0]
"(5)
The shaded part in Equation (5) is actually the pointwise mutual information (PMI) (Church and Hanks, 1989):
PMI(x, y;X,Y )",3.2 MI and PMI,[0],[0]
":= log PXY (x, y)
",3.2 MI and PMI,[0],[0]
PX(x)PY,3.2 MI and PMI,[0],[0]
(y) .,3.2 MI and PMI,[0],[0]
"(6)
Therefore, PMI(x, y) can be thought of as the contribution of (x, y) to MI(X,Y ).
3 Conventionally, mutual information is denoted by I(X;Y ); in this paper, however, for notational consistency, mutual information is denoted by MI(X,Y ).",3.2 MI and PMI,[0],[0]
"As seen in the previous section, PMI can be derived from MI.",3.3 HSIC and PHSIC,[0],[0]
"Here, we consider replacing MI with the Hilbert–Schmidt independence criterion (HSIC).",3.3 HSIC and PHSIC,[0],[0]
"Then, in analogy with the relationship between PMI and MI, we derive PHSIC from HSIC (see HSIC & PHSIC in Table 2).
",3.3 HSIC and PHSIC,[0],[0]
Let k :,3.3 HSIC and PHSIC,[0],[0]
"X × X → R and ` : Y × Y → R denote positive definite kernels on X and Y , respectively (intuitively, they are similarity functions between linguistic expressions).",3.3 HSIC and PHSIC,[0],[0]
"The Hilbert– Schmidt independence criterion (HSIC) (Gretton et al., 2005), a kernel-based dependence measure, is defined by
HSIC(X,Y; k, `) :=MMD2k,`[PXY ,PXPY ], (7)
where MMD[·, ·] denotes the maximum mean discrepancy (MMD) (Gretton et al., 2012), which measures the difference between random variables on a kernel-induced feature space.",3.3 HSIC and PHSIC,[0],[0]
"Thus, HSIC(X,Y ; k, `) is the degree of dependence between X and Y measured by the MMD between PXY and PXPY , while MI is measured by the KL divergence (Equation (4)).
",3.3 HSIC and PHSIC,[0],[0]
"Analogous to MI in Equation (5), HSIC can be represented in the form of the expectation on PXY by a simple deformation:
HSIC(X,Y ; k, `)
= E (x,y)
",3.3 HSIC and PHSIC,[0],[0]
"[ (φ(x)−mX)>CXY (ψ(y)−mY ) ] (8)
= E (x,y)
[ E (x′,y′)",3.3 HSIC and PHSIC,[0],[0]
"[k̃(x, x′)˜̀(y, y′)]",3.3 HSIC and PHSIC,[0],[0]
"], (9)
where
φ(x) := k(x, ·), ψ(y) := `(y, ·), (10)
mX := Ex[φ(x)], mY := Ey[ψ(y)], (11)
",3.3 HSIC and PHSIC,[0],[0]
"CXY := E (x,y)
",3.3 HSIC and PHSIC,[0],[0]
"[ (φ(x)−mX)(ψ(y)−mY )> ] , (12)
k̃(x, x′)",3.3 HSIC and PHSIC,[0],[0]
":= k(x, x′)−Ex′",3.3 HSIC and PHSIC,[0],[0]
"[k(x, x′)]",3.3 HSIC and PHSIC,[0],[0]
"−Ex[k(x, x′)]",3.3 HSIC and PHSIC,[0],[0]
"+Ex,x′ [k(x, x′)].",3.3 HSIC and PHSIC,[0],[0]
"(13)
At first glance, these equations are somewhat complicated; however, the estimators of PHSIC we actually use are reduced to a simple matrix calculation in Section 5.",3.3 HSIC and PHSIC,[0],[0]
"Unlike MI in Equation (5), HSIC has two representations: Equation (8) is the representation in feature space and Equation (9) is the representation in data space.
",3.3 HSIC and PHSIC,[0],[0]
"Similar to the relationship between MI and PMI (Section 3.2), we define the pointwise Hilbert– Schmidt independence criterion (PHSIC) by the shaded parts in Equations (8) and (9):
PHSIC(x, y;X,Y, k, `)
:= (φ(x)−mX)>CXY (ψ(y)−mY ) (14)
",3.3 HSIC and PHSIC,[0],[0]
"= E (x′,y′)",3.3 HSIC and PHSIC,[0],[0]
"[k̃(x, x′)˜̀(y, y′)] .",3.3 HSIC and PHSIC,[0],[0]
"(15) Namely, PHSIC(x, y) is defined as the contribution of (x, y) to HSIC(X,Y ).
",3.3 HSIC and PHSIC,[0],[0]
"In summary, we define PHSIC such that “MI:PMI = HSIC:PHSIC” holds (see Table 2).",3.3 HSIC and PHSIC,[0],[0]
"This section gives an intuitive explanation for the first feature of PHSIC, i.e., the robustness to data sparsity, using Table 3.",4 PHSIC as Smoothed PMI,[0],[0]
"In short, we show that PHSIC is a “smoothed variant of PMI.”
",4 PHSIC as Smoothed PMI,[0],[0]
"First, the maximum likelihood estimator of PMI
in Equation (1) can be rewritten as P̂MI(x, y;D)= log n ·",4 PHSIC as Smoothed PMI,[0],[0]
"∑
iI[x=xi ∧ y=yi]∑ iI[x=xi] ∑ iI[y=yi] , (16)
where I[condition] = 1 if the condition is true and I[condition] = 0 otherwise.",4 PHSIC as Smoothed PMI,[0],[0]
"According to Equation (16), P̂MI(x, y) is the amount computed by repeating the following operation (see the first row in Table 3):
collate the given (x, y) and the observed (xi, yi) in D in order, and add the scores if (x, y) and (xi, yi) match exactly or deduct the scores if either the x side or the y side (but nor both) matches.
",4 PHSIC as Smoothed PMI,[0],[0]
"Moreover, an estimator of PHSIC in data space (Equation (15)) is
P̂HSIC(x, y;D, k, `)=",4 PHSIC as Smoothed PMI,[0],[0]
"1n ∑ i ̂̃ k(x, xi)̂˜̀(y, yi) ,
(17)
where ̂̃k(·, ·) and ̂̀̃(·, ·) are similarity functions centered on the data4.",4 PHSIC as Smoothed PMI,[0],[0]
"According to Equation (17), P̂HSIC(x, y) is the amount computed by repeating the following operation (see the second row in Table 3):
collate the given (x, y) and the observed (xi, yi) in D in order, and add the scores if the similarities on the x and y sides are both
higher (both ̂̃k(x, xi) > 0 and ̂̀̃(y, yi) > 0",4 PHSIC as Smoothed PMI,[0],[0]
"hold)5 or deduct the scores if the similarities on either the x or y sides are similar but those on the other side are not similar.
",4 PHSIC as Smoothed PMI,[0],[0]
"4 To be exact, ̂̃k(x, x′)",4 PHSIC as Smoothed PMI,[0],[0]
":= k(x, x′)",4 PHSIC as Smoothed PMI,[0],[0]
"− 1 n ∑n j=1 k(x, xj)",4 PHSIC as Smoothed PMI,[0],[0]
− 1 n ∑n i=1,4 PHSIC as Smoothed PMI,[0],[0]
"k(xi, x ′) + 1 n2 ∑n i=1",4 PHSIC as Smoothed PMI,[0],[0]
"∑n j=1 k(xi, xj), which is an estimator of the centered kernel k̃(x, x′) in Equation (13).",4 PHSIC as Smoothed PMI,[0],[0]
5,4 PHSIC as Smoothed PMI,[0],[0]
"In addition, the scores are added if the similarity on the x
side and that on the y side are both lower, that is, if ̂̃k(x, xi) < 0 and ̂̀̃(y, yi) < 0 hold.
",4 PHSIC as Smoothed PMI,[0],[0]
"As described above, when comparing the estimators of PMI and PHSIC from the viewpoint of “methods of matching the given (x, y) and the observed (xi, yi),” it is understood that PMI matches them in an exact manner, while PHSIC smooths the matching using kernels (similarity functions).
",4 PHSIC as Smoothed PMI,[0],[0]
"With this mechanism, even for completely unknown pairs, it is possible to estimate the cooccurrence strength by referring to observed pairs through the kernels.",4 PHSIC as Smoothed PMI,[0],[0]
"Therefore, PHSIC is expected to be robust to data sparsity and can be applied to phrases and sentences.
",4 PHSIC as Smoothed PMI,[0],[0]
"Available Kernels for PHSIC In NLP, a variety of similarity functions (i.e., positive definite kernels) are available.",4 PHSIC as Smoothed PMI,[0],[0]
"We can freely utilize such resources, such as cosine similarity between sentence embeddings.",4 PHSIC as Smoothed PMI,[0],[0]
"For a more detailed discussion, see Appendix A.",4 PHSIC as Smoothed PMI,[0],[0]
"Recall that we have two types of empirical estimator of PMI, the maximum likelihood estimator (Equation (1)) and the RNN-based estimator (Equation (2)).",5 Empirical Estimators of PHSIC,[0],[0]
"In this section, we describe how to rapidly estimate PHSIC from data.",5 Empirical Estimators of PHSIC,[0],[0]
"When using the linear kernel or cosine similarity (e.g., cosine similarity between sentence embeddings), PHSIC can be efficiently estimated in feature space (Section 5.1).",5 Empirical Estimators of PHSIC,[0],[0]
"When using a nonlinear kernel such as the Gaussian kernel, PHSIC can also be estimated efficiently in data space via a simple matrix decomposition (Section 5.2).",5 Empirical Estimators of PHSIC,[0],[0]
"When using the linear kernel or cosine similarity, the estimator of PHSIC in feature space (14) is as follows:
P̂HSICfeature(x, y;D, k, `)
=",5.1 Estimation Using Linear Kernel or Cosine,[0],[0]
"(φ(x)−φ(x))>ĈXY (ψ(y)−ψ(y)) , (18)
where
φ(x) = { x (k(x, x′) = x>x′)",5.1 Estimation Using Linear Kernel or Cosine,[0],[0]
"x/‖x‖ (k(x, x′) = cos(x, x′)) , (19)
φ(x) := 1
n n∑ i=1 φ(xi), ψ(y)",5.1 Estimation Using Linear Kernel or Cosine,[0],[0]
":= 1 n n∑ i=1 ψ(yi), (20)
",5.1 Estimation Using Linear Kernel or Cosine,[0],[0]
"ĈXY := 1
n n∑ i=1 φ(xi)ψ(yi)",5.1 Estimation Using Linear Kernel or Cosine,[0],[0]
>,5.1 Estimation Using Linear Kernel or Cosine,[0],[0]
− φ(x)ψ(y)>.,5.1 Estimation Using Linear Kernel or Cosine,[0],[0]
"(21)
Generally in kernel methods, a feature map φ(·) induced by a kernel k(·, ·) is unknown or highdimensional and it is difficult to compute estimated values in feature space6.",5.1 Estimation Using Linear Kernel or Cosine,[0],[0]
"However, when we use the linear kernel or cosine similarity, feature maps can be explicitly determined (Equation (19)).
",5.1 Estimation Using Linear Kernel or Cosine,[0],[0]
"Computational Cost When learning Equation (18) with feature maps φ : X → Rd and ψ : Y → Rd, computing the vectors φ(x), ψ(y) ∈ Rd and the matrix ĈXY ∈ Rd×d takes O(nd2) time and O(nd) space (linear in the size of the input, n).",5.1 Estimation Using Linear Kernel or Cosine,[0],[0]
"When estimating PHSIC(x, y), computing φ(x), ψ(y) ∈ Rd and Equation (18) takes O(d2) time (constant; does not depend on the size of the input, n).",5.1 Estimation Using Linear Kernel or Cosine,[0],[0]
"When using a nonlinear kernel such as the Gaussian kernel, it is necessary to estimate PHSIC in data space.",5.2 Estimation Using Nonlinear Kernels,[0],[0]
"Using a simple matrix decomposition, this can be achieved with the same computational cost as the estimation in feature space.",5.2 Estimation Using Nonlinear Kernels,[0],[0]
See Appendix B for a detailed derivation.,5.2 Estimation Using Nonlinear Kernels,[0],[0]
"In this section, we provide empirical evidence for the greater effectiveness of PHSIC than PMI, i.e., a very short learning time and robustness to data sparsity.",6 Experiments,[0],[0]
"Among the many potential applications of PHSIC, we choose two fundamental scenarios, (re-)ranking/classification and data selection.",6 Experiments,[0],[0]
"• In the ranking/classification scenario (measuring
the co-occurrence strength of new data pairs with reference to observed pairs), PHSIC is applied
6 One of the characteristics of kernel methods is that an intractable estimation in feature space is replaced with an efficient estimation in data space.
as a criterion for the dialogue response selection task (Section 6.2).",6 Experiments,[0],[0]
•,6 Experiments,[0],[0]
"In the data selection/filtering scenario (ordering the entire set of observed data pairs according to the co-occurrence strength), PHSIC is also applied as a criterion for data selection in the context of machine translation (Section 6.3).",6 Experiments,[0],[0]
"To take advantage of recent developments in representation learning, we used several pre-trained models for encoding sentences into vectors and several kernels between these vectors for PHSIC.
Encoders",6.1 PHSIC Settings,[0],[0]
"As sentence encorders, we used two pre-trained models without fine-tuning.",6.1 PHSIC Settings,[0],[0]
"First, the sum of the word vectors effectively represents a sentence (Mikolov et al., 2013a):
x= ∑ w∈xvec(w), y= ∑ w∈yvec(w).",6.1 PHSIC Settings,[0],[0]
"(22)
For vec(·), we used the pre-trained fastText model7, which is a high-accuracy and popular word embedding model (Bojanowski et al., 2017); models in 157 languages are publicly distributed (Grave et al., 2018).",6.1 PHSIC Settings,[0],[0]
"Second, we also used a DNN-based sentence encoder, called the universal sentence encoder (Cer et al., 2018), which utilizes the deep averaging network (DAN) (Iyyer et al., 2015).",6.1 PHSIC Settings,[0],[0]
"The pre-trained model for English sentences we used is publicly available8.
Kernels",6.1 PHSIC Settings,[0],[0]
"As kernels between these vectors, we used cosine similarity (cos)
k(x,x′) = cos(x,x′) (23)
and the Gaussian kernel (also known as the radial basis function kernel; RBF kernel)
k(x,x′) = exp ( −‖x− x
′‖22 2σ2
) , (24)
and similarly for `(y,y′).",6.1 PHSIC Settings,[0],[0]
"The experiments are ran with hyperparameter σ = 1.0 for the RBF kernel, and d = 100 for incomplete Cholesky decomposition (for more detail, see Section B).",6.1 PHSIC Settings,[0],[0]
"In the first experiment, we applied PHSIC as a ranking criterion of the task of dialogue response 7 https://fasttext.cc/docs/en/english-vectors.",6.2 Ranking: Dialogue Response Selection,[0],[0]
"html, https://fasttext.cc/docs/en/crawl-vectors.",6.2 Ranking: Dialogue Response Selection,[0],[0]
"html 8 https://www.tensorflow.org/hub/modules/google/ universal-sentence-encoder/1
selection (Lowe et al., 2015); in the task, pairs comprising a context (previous utterance sequence) and its response are collected from dialogue corpora and the goal is to rank the candidate responses for each given context sentence.
",6.2 Ranking: Dialogue Response Selection,[0],[0]
"The task entails sentence sequences (very sparse linguistic expressions); moreover, Li et al. (2016) pointed out that (RNN-based) PMI has a positive impact on suppressing dull responses (e.g., “I don’t know.”) in dialogue systems.",6.2 Ranking: Dialogue Response Selection,[0],[0]
"Therefore, PHSIC, another co-occurrence measure, is also expected to be effective for this.",6.2 Ranking: Dialogue Response Selection,[0],[0]
"With this setting, where the validity of PMI is confirmed, we investigate whether PHSIC can replace RNN-based PMI in terms of both learning time and robustness to data sparsity.
",6.2 Ranking: Dialogue Response Selection,[0],[0]
"Experimental Settings
Dataset For the training data, we gathered approximately 5× 105 reply chains from Twitter, following Sordoni et al. (2015)9.",6.2 Ranking: Dialogue Response Selection,[0],[0]
"In addition, we randomly selected {103, 104, 105} reply chains from that dataset.",6.2 Ranking: Dialogue Response Selection,[0],[0]
"Using these small subsets, we confirmed the effect of the difference in the size of the training set (data sparseness) on the learning time and predictive performance.
",6.2 Ranking: Dialogue Response Selection,[0],[0]
"For validation and test data, we used a small (approximately 2000 pairs each) but highly reliable dataset created by Sordoni et al. (2015)10, which consists only of conversations given high scores by human annotators.",6.2 Ranking: Dialogue Response Selection,[0],[0]
"Therefore, this set was not expected to include dull responses.
",6.2 Ranking: Dialogue Response Selection,[0],[0]
"For each dataset, we converted each contextmessage-response triple into a context-response pair by concatenating the context and message following Li et al. (2016).",6.2 Ranking: Dialogue Response Selection,[0],[0]
"In addition, to convert the test set (positive examples) to ten-choice multiplechoice questions, we shuffled the combinations of context and response to generate pseudo-negative examples.
",6.2 Ranking: Dialogue Response Selection,[0],[0]
Evaluation Metrics,6.2 Ranking: Dialogue Response Selection,[0],[0]
"We adopted the following evaluation metrics for the task: (i) ROC-AUC (the area under the receiver operating characteristic curve), (ii) MRR (the mean reciprocal rank), and (iii) Recall@{1,2}.
9",6.2 Ranking: Dialogue Response Selection,[0],[0]
"We collected tweets after 2017 for our training set to avoid duplication with the test set, which contains tweets from the year 2012.",6.2 Ranking: Dialogue Response Selection,[0],[0]
10 https://www.microsoft.com/en-us/download/,6.2 Ranking: Dialogue Response Selection,[0],[0]
"details.aspx?id=52375
Experimental Procedure",6.2 Ranking: Dialogue Response Selection,[0],[0]
"We used the following procedure: (i) train the model with a set of context-response pairs D = {(xi, yi)}ni=1; (ii) for each context sentence x in the test data, rank the candidate responses {yj}10j=1 by the model; and (iii) report three evaluation metrics.
",6.2 Ranking: Dialogue Response Selection,[0],[0]
"Baseline Measures As baseline measures, both (1) an RNN language model P̂RNN(y) (Mikolov et al., 2010) and (2) a conditional RNN language model P̂RNN(y|x) (Sutskever et al., 2014) were trained, and (3) PMI based on these language models, RNN-PMI, was also used for experiments (see Equation (2)).",6.2 Ranking: Dialogue Response Selection,[0],[0]
"We trained these models with all combinations of the following settings: (a) the number of dimensions of the hidden layers being 300 or 1200 and (b) the initialization of the embedding layer being random (uniform on [−0.1, 0.1]) or fastText.",6.2 Ranking: Dialogue Response Selection,[0],[0]
"For more detailed settings, see Appendix C.
Experimental Results Learning Time Table 4 shows the experimental results of the learning time11.",6.2 Ranking: Dialogue Response Selection,[0],[0]
"Regardless of the size of the training set n, the learning time for
11 The computing environment was as follows: (i) CPU: Xeon E5-1650-v3 (3.5 GHz, 6 Cores); (ii) GPU: GTX 1080 (8 GB).
",6.2 Ranking: Dialogue Response Selection,[0],[0]
PHSIC is much shorter than that of the RNN-based method.,6.2 Ranking: Dialogue Response Selection,[0],[0]
"For example, even when the size of the training set n is 5× 105, PHSIC is approximately 1400–4000 times faster than RNN-based PMI.",6.2 Ranking: Dialogue Response Selection,[0],[0]
"This is because the estimators of PHSIC are reduced to a deterministic and efficient matrix calculation (Section 5), whereas neural network-based models involve the sequential optimization of parameters via gradient descent methods.
",6.2 Ranking: Dialogue Response Selection,[0],[0]
Robustness to Data Sparsity Table 5 shows the experimental results of the predictive performance.,6.2 Ranking: Dialogue Response Selection,[0],[0]
"When the size of the training data is small (n=103, 104), that is, when the data is extremely sparse, the predictive performance of PHSIC hardly deteriorates while that of PMI rapidly decays as the number of data decreases.",6.2 Ranking: Dialogue Response Selection,[0],[0]
This indicates that PHSIC is more robust to data sparsity than RNN-based PMI owing to the effect of kernels.,6.2 Ranking: Dialogue Response Selection,[0],[0]
"Moreover, PHSIC with the simple cosine kernel outperforms the RNN-based model regardless of the number of data, while the learning time of PHSIC is thousands of times shorter than those of the baseline methods (Section 6.2).
",6.2 Ranking: Dialogue Response Selection,[0],[0]
Additionally we report Spearman’s rank correlation coefficient between models to verify whether PHSIC shows similar behavior to PMI.,6.2 Ranking: Dialogue Response Selection,[0],[0]
See Appendix D for more detail.,6.2 Ranking: Dialogue Response Selection,[0],[0]
The aim of our second experiment was to demonstrate that PHSIC is also beneficial as a criterion of data selection.,6.3 Data Selection for Machine Translation,[0],[0]
"To achieve this, we attempted to apply PHSIC to a parallel corpus filtering task that has been intensively discussed in recent (neural) machine translation (MT, NMT) studies.",6.3 Data Selection for Machine Translation,[0],[0]
"This task was first adopted as a shared task in the third conference on machine translation (WMT 2018)12.
",6.3 Data Selection for Machine Translation,[0],[0]
"Several existing parallel corpora, especially those automatically gathered from large-scale text data, such as the Web, contain unacceptable amounts of noisy (low-quality) sentence pairs that greatly affect the translation quality.",6.3 Data Selection for Machine Translation,[0],[0]
"Therefore, the development of an effective method for parallel corpus filtering would potentially have a large influence on the MT community; discarding such noisy pairs may improve the translation quality and shorten the training time.
",6.3 Data Selection for Machine Translation,[0],[0]
"We expect PHSIC to give low scores to exceptional sentence pairs (misalignments or missing 12 http://www.statmt.org/wmt18/ parallel-corpus-filtering.html
translations) during the selection process because PHSIC assigns low scores to pairs that are highly inconsistent with other pairs (see Section 4).",6.3 Data Selection for Machine Translation,[0],[0]
"Note that applying RNN-based PMI to a parallel corpus selection task is unprofitable since obtaining RNNbased PMI also has an identical computational cost for training a sequence-to-sequence model for MT, and thus, we cannot expect a reduction of the total training time.
",6.3 Data Selection for Machine Translation,[0],[0]
"Experimental Settings
Dataset",6.3 Data Selection for Machine Translation,[0],[0]
"We used the ASPEC-JE corpus13, which is an official dataset used for the MT-evaluation shared task held in the fourth workshop on Asian translation (WAT 2017)14 (Nakazawa et al., 2017).",6.3 Data Selection for Machine Translation,[0],[0]
ASPEC-JE consists of approximately three million (3M) Japanese–English parallel sentences from scientific paper abstracts.,6.3 Data Selection for Machine Translation,[0],[0]
"As discussed by Kocmi et al. (2017), ASPEC-JE contains many low-quality parallel sentences that have the potential to significantly degrade the MT quality.",6.3 Data Selection for Machine Translation,[0],[0]
"In fact, they empirically revealed that using only the reliable part of the training parallel corpus significantly improved the translation quality.",6.3 Data Selection for Machine Translation,[0],[0]
"Therefore, ASPEC-JE is a suitable dataset for evaluating the data selection ability.
",6.3 Data Selection for Machine Translation,[0],[0]
"Model For our data selection evaluation, we selected the Transformer architecture (Vaswani et al., 2017) as our baseline NMT model, which is widelyused in the NMT community and known as one of the current state-of-the-art architectures.",6.3 Data Selection for Machine Translation,[0],[0]
"We utilized fairseq15, a publicly available tool for neural sequence-to-sequence models, for building our models.
",6.3 Data Selection for Machine Translation,[0],[0]
"Experimental Procedure We used the following procedure for this evaluation: (1) rank all parallel sentences in a given parallel corpus according to each criterion, (2) extract the top K ranked parallel sentences, (3) train the NMT model using the extracted parallel sentences, and (4) evaluate the translation quality of the test data using a typical MT automatic evaluation measure, i.e., BLEU (Papineni et al., 2002)16.",6.3 Data Selection for Machine Translation,[0],[0]
In our experiments we evaluated PHSIC with K = 0.5M and 1M.,6.3 Data Selection for Machine Translation,[0],[0]
"16 We used multi-bleu.perl in the Moses tool (https:// github.com/moses-smt/mosesdecoder).
",15 https://github.com/pytorch/fairseq,[0],[0]
"Baseline Measure As a baseline measure, we utilize a publicly available script17 of fast align (Dyer et al., 2013), which is one of the state-of-theart word aligner.",15 https://github.com/pytorch/fairseq,[0],[0]
"We firstly used the fast align for the training set D = {(xi, yi)}i to obtain the word alignment between each sentence pair (xi, yi), i.e., a set of aligned word pairs with its probabilities.",15 https://github.com/pytorch/fairseq,[0],[0]
"We then computed the co-occurrence score of (xi, yi) with sentence-length normalization, i.e., the average log probability of aligned word pairs.
",15 https://github.com/pytorch/fairseq,[0],[0]
Experimental Results Table 6 shows the results of our data selection evaluation.,15 https://github.com/pytorch/fairseq,[0],[0]
It is common knowledge in NMT that more data gives better performance in general.,15 https://github.com/pytorch/fairseq,[0],[0]
"However, we observed that PHSIC successfully extracted beneficial parallel sentences from the noisy parallel 17 https://github.com/clab/fast align
corpus; the result using 1M data extracted from the 3M corpus by PHSIC was almost the same as that using 3M data (the decrease in the BLEU score was only 0.07), whereas that by random extraction reduced the BLEU score by 1.20.
",15 https://github.com/pytorch/fairseq,[0],[0]
This was actually a surprising result because PHSIC utilizes only monolingual similarity measures (kernels) without any other language resources.,15 https://github.com/pytorch/fairseq,[0],[0]
This indicates that PHSIC can be applied to a language pair poor in parallel resources.,15 https://github.com/pytorch/fairseq,[0],[0]
"In addition, the surface form and grammatical characteristics between English and Japanese are extremely different18; therefore, we expect that PHSIC will work well regardless of the similarity of the language pair.",15 https://github.com/pytorch/fairseq,[0],[0]
"Dependence Measures Measuring independence or dependence (correlation) between two random variables, i.e., estimating dependence from a set of paired data, is a fundamental task in statistics and a very wide area of data science.",7 Related Work,[0],[0]
"To measure the complex nonlinear dependence that real data has, we have several choices.
",7 Related Work,[0],[0]
"First, information-theoretic MI (Cover and Thomas, 2006) and its variants (Suzuki et al., 2009; Reshef et al., 2011) are the most commonly used dependence measures.",7 Related Work,[0],[0]
"However, to the best of our knowledge, there is no practical method of computing MIs for large-multi class high-dimensional 18",7 Related Work,[0],[0]
"For example, word order; English is an SVO (subject-verbobject) language and Japanese is an SOV (subject-object-verb) language.
",7 Related Work,[0],[0]
"(having a complex generative model) discrete data, such as sparse linguistic data.
",7 Related Work,[0],[0]
"Second, several kernel-based dependence measures have been proposed for measuring nonlinear dependence (Akaho, 2001; Bach and Jordan, 2002; Gretton et al., 2005).",7 Related Work,[0],[0]
"The reason why kernelbased dependence measures work well for real data is that they do not explicitly estimate densities, which is difficult for high-dimensional data.",7 Related Work,[0],[0]
"Among them, HSIC (Gretton et al., 2005) is popular because it has a simple estimation method, which is used for various tasks such as feature selection (Song et al., 2012), dimensionality reduction (Fukumizu et al., 2009), and",7 Related Work,[0],[0]
"unsupervised object matching (Quadrianto et al., 2009; Jagarlamudi et al., 2010).",7 Related Work,[0],[0]
"We follow this line.
",7 Related Work,[0],[0]
"Co-occurrence Measures First, In NLP, PMI (Church and Hanks, 1989) and its variants (Bouma, 2009) are the de facto co-occurrence measures between dense linguistic expressions, such as words (Bouma, 2009) and simple narrative-event expressions (Chambers and Jurafsky, 2008).",7 Related Work,[0],[0]
"In recent years, positive PMI (PPMI) has played an important role as a component of word vectors (Levy and Goldberg, 2014).
",7 Related Work,[0],[0]
"Second, there are several studies in which the pairwise ranking problem has been solved by using deep neural networks (DNNs) in NLP.",7 Related Work,[0],[0]
Li et al. (2016) proposed a PMI estimation using RNN language models; this was used as a baseline model in our experiments (see Section 6.2).,7 Related Work,[0],[0]
"Several studies have used DNN-based binary classifiers modeling P(C = positive | (x, y)) to solve the given ranking problem directly (Hu et al., 2014; Yin et al., 2016; Mueller and Thyagarajan, 2016) (these networks are sometimes called Siamese neural networks).",7 Related Work,[0],[0]
Our study focuses on comparing co-occurrence measures.,7 Related Work,[0],[0]
"It is unknown whether Siamese NNs capture the co-occurrence strength; therefore we did not deal with Siamese NNs in this paper.
",7 Related Work,[0],[0]
"Finally, to the best of our knowledge, Yokoi et al. (2017)’s paper is the first study that suggested converting HSIC to a pointwise measure.",7 Related Work,[0],[0]
"The present study was inspired by their suggestion; here, we have (i) provided a formal definition (population) of PHSIC; (ii) analyzed the relationship between PHSIC and PMI; (iii) proposed linear-time estimation methods; and (iv) experimentally verified the computation speed and robustness to data sparsity of PHSIC for practical applications.",7 Related Work,[0],[0]
"The NLP community has commonly employed PMI to estimate the co-occurrence strength between linguistic expressions; however, existing PMI estimators have a high computational cost when applied to sparse linguistic expressions (Section 1).",8 Conclusion,[0],[0]
"We proposed a new kernel-based co-occurrence measure, the pointwise Hilbert– Schmidt independent criterion (PHSIC).",8 Conclusion,[0],[0]
"As well as defining PMI as the contribution to mutual information, PHSIC is defined as the contribution to HSIC; PHSIC is intuitively a “kernelized variant of PMI” (Section 3).",8 Conclusion,[0],[0]
PHSIC can be applied to sparse linguistic expressions owing to the mechanism of smoothing by kernels.,8 Conclusion,[0],[0]
"Comparing the estimators of PMI and PHSIC, PHSIC can be interpreted as a smoothed variant of PMI, which allows various similarity metrics to be plugged in as kernels (Section 4).",8 Conclusion,[0],[0]
"In addition, PHSIC can be estimated in linear time owing to the efficient matrix calculation, regardless of whether we use linear or nonlinear kernels (Section 5).",8 Conclusion,[0],[0]
We conducted a ranking task for dialogue systems and a data selection task for machine translation (Section 6).,8 Conclusion,[0],[0]
"The experimental results show that (i) the learning of PHSIC was completed thousands of times faster than that of the RNN-based PMI while outperforming it in ranking accuracy (Section 6.2); and (ii) even when using a nonlinear kernel, PHSIC can be applied to a large dataset.",8 Conclusion,[0],[0]
"Moreover, PHSIC reduces the amount of training data to one third without sacrificing the output translation quality (Section 6.3).
",8 Conclusion,[0],[0]
"Future Work Using the PHSIC estimator in feature space (Equation (18)), we can generate the most appropriate ψ(y) for a given φ(x) (uniquely, up to scale).",8 Conclusion,[0],[0]
"That is, if a DNN-based sentence decoder is used, y (a sentence) can be restored from ψ(y) (a feature vector) so that generative models of strong co-occurring sentences can be realized.",8 Conclusion,[0],[0]
We are grateful to anonymous reviewers for their helpful comments.,Acknowledgments,[0],[0]
"We also thank Weihua Hu for useful discussions, Kenshi Yamaguchi for collecting data, and Paul Reisert for proofreading.",Acknowledgments,[0],[0]
"This work was supported in part by JSPS KAKENHI Grant Number JP15H01702 and JST CREST Grant Number JPMJCR1513, Japan.",Acknowledgments,[0],[0]
"Similarity between Sentence Vectors A variety of vector representations of phrases and sentences based on the distributional hypothesis have recently been proposed, including sentence encoders (Kiros et al., 2015; Dai and Le, 2015; Iyyer et al., 2015; Hill et al., 2016; Cer et al., 2018) and the sum of word embeddings; it is known as additive compositionality (Mitchell and Lapata, 2010; Mikolov et al., 2013a; Wieting et al., 2015) that we can express the meaning of phrases and sentences well with the sum of word vectors (e.g., word2vec (Mikolov et al., 2013b), GloVe (Pennington et al., 2014), and fastText (Bojanowski et al., 2017)).",A Available Kernels for PHSIC,[0],[0]
"Note that various pre-trained models of sentence encoders and word embeddings have also been made available.
",A Available Kernels for PHSIC,[0],[0]
"The cosine of these vectors, which is a positive definite kernel, can be used as a convenient and highly accurate similarity function between phrases or sentences.",A Available Kernels for PHSIC,[0],[0]
"Other major kernels can also be used, such as the RBF kernel, the Laplacian kernel, and polynomial kernels.
",A Available Kernels for PHSIC,[0],[0]
"Structured Kernels Various structured kernels for NLP, such as tree kernels, which capture fine structure of sentences such as syntax, were devised in the support vector machine era (Collins and Duffy, 2002; Bunescu and Mooney, 2006; Moschitti, 2006).
",A Available Kernels for PHSIC,[0],[0]
"Combinations We can freely combine the previously mentioned kernels because the sum and the product of positive definite kernels are also positive definite kernels (Shawe-Taylor and Cristianini, 2004, Proposition 3.22).",A Available Kernels for PHSIC,[0],[0]
"Although estimators of HSIC and PHSIC depend on kernels k, ` and data D, hereinafter, we use the following notation for the sake of simplicity:
ĤSIC(X,Y ) := ĤSIC(X,Y ;D, k, `), (25)
P̂HSIC(x, y) := P̂HSIC(x, y;D, k, `).",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"(26)
Naı̈ve Estimation Fist, an estimator of PHSIC in the data space (15) is
P̂HSICkernel(x, y)=(k",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
− k)>,B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"( 1nH)(`− `), (27)
where k := (k(x, x1), . . .",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
", k(x, xn))",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"> ∈ Rn, so as `; and vector k := 1nK1 denotes empirical mean of {ki}ni=1, so as `.",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"This estimation has a
large computational cost.",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"When learning, computing the vectors k, ` takes O(n2) time and O(n) space.",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"When estimating PHSIC, computing k, ` and multiplying the matrix 1nH takes O(n) time.
",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
Fast Estimation via Incomplete Cholesky Decomposition Equation (27) has a large computational cost because it is necessary to construct the Gram matrices K and L ∈ Rn×n.,B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"In kernel methods, several methods have been proposed for approximating Gram matrices at low cost without constructing them explicitly, such as incomplete Cholesky decomposition (Fine and Scheinberg, 2001).
",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"By incomplete Cholesky decomposition, from data points {x1, . . .",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
", xn} ⊆ X and a positive definite kernel",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
k,B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
:,B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"X × X → R, a matrix A = (a1, . . .",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
",an)
> ∈ Rn×d (d n) can be obtained with O(nd2) time complexity.",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"This makes it possible to approximate the Gram matrix K by vectors ai ∈ Rd without configuring the entire of K:
a>",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"i aj ≈ k(xi, xj) (",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
28) AA>,B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"≈ K. (29)
",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"Also, for HSIC, an efficient approximation method utilizing incomplete Cholesky decomposition has been proposed (Gretton et al., 2005, Lemma 2):
ĤSICICD(X,Y ) = 1 n2 ‖(HA)>B‖2F, (30)
where A = (a1, . . .",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
",an)> ∈ Rn×d is a matrix satisfying AA>",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"≈ K computed via incomplete Cholesky decomposition, so as B (BB> ≈ L).",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"Equation (30) can be represented in the form of the expectation on data points:
ĤSICICD(X,Y )= 1
n n∑ i=1",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
[ (ai−a)>ĈICD(bi−b) ],B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"(31)
ĈICD :",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
= 1 n,B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"(HA)>B ∈ Rd×d, (32)
where vector a := 1nA >",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"1 ∈ Rd denotes empirical mean of {ai}ni=1, so as b := 1nB >1.
",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"Recall that PHSIC(x, y) is the contribution of (x, y) to HSIC(X,Y ) (see Section 3.3); PHSIC then can be efficiently estimated by the shaded part of Equation (31):
P̂HSICICD(x, y)= (a−a)>ĈICD(b−b) .",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"(33)
Here, the vector a ∈ Rd corresponding to the new x can be calculated by “performing from halfway”
on the incomplete Cholesky decomposition algorithm.",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"Let x(1), . . .",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
", x(d) denote the dominant xis adopted during decomposition algorithm.",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
"The jth element of a can be computed as follows:
a[j]= [ k(x, x(j))− j−1∑ m=1 a[m]Ajm ] /Ajj , (34)
so as b ∈ Rd corresponding to the new y.",B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
The estimation via incomplete Cholesky decomposition (33) is extremely efficient compared to the naive estimation (27); Equation (33)’s computational complexity is equivalent to the estimation in the feature space (18).,B Derivation of Fast PHSIC Estimation in Data Space,[0],[0]
Detailed settings for learning RNNs used in this research are as follows.,C Detailed Settings for Learning RNNs,[0],[0]
"• Hidden layers: single layer LSTMs (Hochreiter
and Schmidhuber, 1997) •",C Detailed Settings for Learning RNNs,[0],[0]
"Vocabulary: words with a frequency: 10 or more
(n = 5× 105), 2 or more (otherwise) •",C Detailed Settings for Learning RNNs,[0],[0]
"Dropout rate: 0.1 (300-dim), 0.3 (1200-dim) •",C Detailed Settings for Learning RNNs,[0],[0]
"Batch size: 64 • Max epoch number: 5 (n = 5× 105), 30 (other-
wise) • Deep learning framework: Chainer (Tokui et al.,
2015)",C Detailed Settings for Learning RNNs,[0],[0]
Table D shows Spearman’s rank correlation coefficient (Spearman’s ρ) between the co-occurrence scores on the test set computed by the models in the dialogue response selection task (Section 6.2).,D Correlation Between Models in Dialogue Response Selection Task,[0],[0]
"This shows that the behavior of RNN-based PMI and
PHSIC are considerably different.",D Correlation Between Models in Dialogue Response Selection Task,[0],[0]
"Furthermore, interestingly, the behavior of PHSICs using different kernels is also different.",D Correlation Between Models in Dialogue Response Selection Task,[0],[0]
Possible reasons for these observations are as follows: (1) the difference in the dependence measures (MI or HSIC) on which each model is based; (2) the validity or numerical stability of estimating PMI with RNN language models; and (3) differences in the behavior of PHSIC originating from differences in the plugged in kernels.,D Correlation Between Models in Dialogue Response Selection Task,[0],[0]
A more detailed analysis of the compatibility between tasks and measures (or kernels) is attractive future work.,D Correlation Between Models in Dialogue Response Selection Task,[0],[0]
"In this paper, we propose a new kernel-based co-occurrence measure that can be applied to sparse linguistic expressions (e.g., sentences) with a very short learning time, as an alternative to pointwise mutual information (PMI).",abstractText,[0],[0]
"As well as deriving PMI from mutual information, we derive this new measure from the Hilbert–Schmidt independence criterion (HSIC); thus, we call the new measure the pointwise HSIC (PHSIC).",abstractText,[0],[0]
"PHSIC can be interpreted as a smoothed variant of PMI that allows various similarity metrics (e.g., sentence embeddings) to be plugged in as kernels.",abstractText,[0],[0]
"Moreover, PHSIC can be estimated by simple and fast (linear in the size of the data) matrix calculations regardless of whether we use linear or nonlinear kernels.",abstractText,[0],[0]
"Empirically, in a dialogue response selection task, PHSIC is learned thousands of times faster than an RNNbased PMI while outperforming PMI in accuracy.",abstractText,[0],[0]
"In addition, we also demonstrate that PHSIC is beneficial as a criterion of a data selection task for machine translation owing to its ability to give high (low) scores to a consistent (inconsistent) pair with other pairs.",abstractText,[0],[0]
Pointwise HSIC: A Linear-Time Kernelized Co-occurrence Norm for Sparse Linguistic Expressions,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 469–476 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
469",text,[0],[0]
"Many recent state-of-the-art models for constituency parsing are transition based, decomposing production of each parse tree into a sequence of action decisions (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017; Stern et al., 2017), building on a long line of work in transition-based parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Henderson, 2004; Zhang and Clark, 2011; Chen and Manning, 2014; Andor et al., 2016; Kiperwasser and Goldberg, 2016).
",1 Introduction,[0],[0]
"However, models of this type, which decompose structure prediction into sequential decisions, can be prone to two issues (Ranzato et al., 2016; Wiseman and Rush, 2016).",1 Introduction,[0],[0]
"The first is exposure bias: if, at training time, the model only observes
states resulting from correct past decisions, it will not be prepared to recover from its own mistakes during prediction.",1 Introduction,[0],[0]
"Second is the loss mismatch between the action-level loss used at training and any structure-level evaluation metric, for example F1.
",1 Introduction,[0],[0]
"A large family of techniques address the exposure bias problem by allowing the model to make mistakes and explore incorrect states during training, supervising actions at the resulting states using an expert policy (Daumé III et al., 2009; Ross et al., 2011; Choi and Palmer, 2011; Chang et al., 2015); these expert policies are typically referred to as dynamic oracles in parsing (Goldberg and Nivre, 2012; Ballesteros et al., 2016).",1 Introduction,[0],[0]
"While dynamic oracles have produced substantial improvements in constituency parsing performance (Coavoux and Crabbé, 2016; Cross and Huang, 2016; Stern et al., 2017; González and Gómez-Rodrı́guez, 2018), they must be custom designed for each transition system.
",1 Introduction,[0],[0]
"To address the loss mismatch problem, another line of work has directly optimized for structurelevel cost functions (Goodman, 1996; Och, 2003).",1 Introduction,[0],[0]
"Recent methods applied to models that produce output sequentially commonly use policy gradient (Auli and Gao, 2014; Ranzato et al., 2016; Shen et al., 2016) or beam search (Xu et al., 2016; Wiseman and Rush, 2016; Edunov et al., 2017) at training time to minimize a structured cost.",1 Introduction,[0],[0]
"These methods also reduce exposure bias through exploration but do not require an expert policy for supervision.
",1 Introduction,[0],[0]
"In this work, we apply a simple policy gradient method to train four different state-of-theart transition-based constituency parsers to maximize expected F1.",1 Introduction,[0],[0]
"We compare against training with a dynamic oracle (both to supervise exploration and provide loss-augmentation) where one is available, including a novel dynamic oracle that we define for the top-down transition system of
Dyer et al. (2016).",1 Introduction,[0],[0]
"We find that while policy gradient usually outperforms standard likelihood training, it typically underperforms the dynamic oracle-based methods – which provide direct, model-aware supervision about which actions are best to take from arbitrary parser states.",1 Introduction,[0],[0]
"However, a substantial fraction of each dynamic oracle’s performance gain is often recovered using the model-agnostic policy gradient method.",1 Introduction,[0],[0]
"In the process, we obtain new state-of-the-art results for single-model discriminative transition-based parsers trained on the English PTB (92.6 F1), French Treebank (83.5 F1), and Penn Chinese Treebank Version 5.1 (87.0 F1).",1 Introduction,[0],[0]
"The transition-based parsers we use all decompose production of a parse tree y for a sentence x into a sequence of actions (a1, . . .",2 Models,[0],[0]
"aT ) and resulting states (s1, . . .",2 Models,[0],[0]
sT+1).,2 Models,[0],[0]
"Actions at are predicted sequentially, conditioned on a representation of the parser’s current state st and parameters θ:
p(y|x; θ) = T∏ t=1 p(at",2 Models,[0],[0]
"| st; θ) (1)
We investigate four parsers with varying transition systems and methods of encoding the current state and sentence: (1) the discriminative Recurrent Neural Network Grammars (RNNG) parser of Dyer et al. (2016), (2) the In-Order parser of Liu and Zhang (2017), (3) the Span-Based parser of Cross and Huang (2016), and (4) the Top-Down parser of Stern et al. (2017).1 We refer to the original papers for descriptions of the transition systems and model parameterizations.",2 Models,[0],[0]
"Likelihood training without exploration maximizes Eq. 1 for trees in the training corpus, but may be prone to exposure bias and loss mismatch (Section 1).",3 Training Procedures,[0],[0]
"Dynamic oracle methods are known to improve on this training procedure for a variety of parsers (Coavoux and Crabbé, 2016; Cross and Huang, 2016; Stern et al., 2017; González and Gómez-Rodrı́guez, 2018), supervising exploration
1Stern et al. (2017) trained their model using a nonprobabilistic, max-margin objective.",3 Training Procedures,[0],[0]
"For comparison to the other models and to allow training with policy gradient, we create a locally-normalized probabilistic variant of their model by applying a softmax function to the predicted scores for each action.
during training by providing the parser with the best action to take at each explored state.",3 Training Procedures,[0],[0]
We describe how policy gradient can be applied as an oracle-free alternative.,3 Training Procedures,[0],[0]
"We then compare to several variants of dynamic oracle training which focus on addressing exposure bias, loss mismatch, or both.",3 Training Procedures,[0],[0]
"Given an arbitrary cost function ∆ comparing structured outputs (e.g. negative labeled F1, for trees), we use the risk objective:
R(θ) =",3.1 Policy Gradient,[0],[0]
N∑ i=1,3.1 Policy Gradient,[0],[0]
"∑ y p(y | x(i); θ)∆(y,y(i))
which measures the model’s expected cost over possible outputs y for each of the training examples (x(1),y(1)), . . .",3.1 Policy Gradient,[0],[0]
", (x(N),y(N)).
",3.1 Policy Gradient,[0],[0]
"Minimizing a risk objective has a long history in structured prediction (Povey and Woodland, 2002; Smith and Eisner, 2006; Li and Eisner, 2009; Gimpel and Smith, 2010) but often relies on the cost function decomposing according to the output structure.",3.1 Policy Gradient,[0],[0]
"However, we can avoid any restrictions on the cost using reinforcement learning-style approaches (Xu et al., 2016; Shen et al., 2016; Edunov et al., 2017) where cost is ascribed to the entire output structure – albeit at the expense of introducing a potentially difficult credit assignment problem.
",3.1 Policy Gradient,[0],[0]
"The policy gradient method we apply is a simple variant of REINFORCE (Williams, 1992).",3.1 Policy Gradient,[0],[0]
"We perform mini-batch gradient descent on the gradient of the risk objective:
∇R(θ) =",3.1 Policy Gradient,[0],[0]
N∑ i=1,3.1 Policy Gradient,[0],[0]
"∑ y p(y|x(i))∆(y,y(i))∇ log p(y|x(i); θ)
",3.1 Policy Gradient,[0],[0]
≈ N∑ i=1,3.1 Policy Gradient,[0],[0]
"∑ y∈Y(x(i)) ∆(y,y(i))∇ log p(y|x(i); θ)
where Y(x(i)) is a set of k candidate trees obtained by sampling from the model’s distribution for sentence x(i).",3.1 Policy Gradient,[0],[0]
"We use negative labeled F1 for ∆.
To reduce the variance of the gradient estimates, we standardize ∆ using its running mean and standard deviation across all candidates used so far throughout training.",3.1 Policy Gradient,[0],[0]
"Following Shen et al. (2016), we also found better performance when including the gold tree y(i) in the set of k candidates Y(x(i)), and do so for all experiments reported here.2
2Including the gold tree in the set of candidates does bias",3.1 Policy Gradient,[0],[0]
"For a given parser state st, a dynamic oracle defines an action a∗(st) which should be taken to incrementally produce the best tree still reachable from that state.3
Dynamic oracles provide strong supervision for training with exploration, but require custom design for a given transition system.",3.2 Dynamic Oracle Supervision,[0],[0]
"Cross and Huang (2016) and Stern et al. (2017) defined optimal (with respect to F1) dynamic oracles for their respective transition systems, and below we define a novel dynamic oracle for the top-down system of RNNG.
",3.2 Dynamic Oracle Supervision,[0],[0]
"In RNNG, tree production occurs in a stackbased, top-down traversal which produces a leftto-right linearized representation of the tree using three actions: OPEN a labeled constituent (which fixes the constituent’s span to begin at the next word in the sentence which has not been shifted), SHIFT the next word in the sentence to add it to the current constituent, or CLOSE the current constituent (which fixes its span to end after the last word that has been shifted).",3.2 Dynamic Oracle Supervision,[0],[0]
"The parser stores opened constituents on the stack, and must therefore close them in the reverse of the order that they were opened.
",3.2 Dynamic Oracle Supervision,[0],[0]
"At a given parser state, our oracle does the following:
1.",3.2 Dynamic Oracle Supervision,[0],[0]
"If there are any open constituents on the stack which can be closed (i.e. have had a word shifted since being opened), check the topmost of these (the one that has been opened most recently).",3.2 Dynamic Oracle Supervision,[0],[0]
"If closing it would produce a constituent from the the gold tree that has not yet been produced (which is determined by the constituent’s label, span beginning position, and the number of words currently shifted), or if the constituent could not be closed at a later position in the sentence to produce a constituent in the gold tree, return CLOSE.
",3.2 Dynamic Oracle Supervision,[0],[0]
"the estimate of the risk objective’s gradient; however since in the parsing tasks we consider, the gold tree has constant and minimal cost, augmenting with the gold is equivalent to jointly optimizing the standard likelihood and risk objectives, using an adaptive scaling factor for each objective that is dependent on the cost for the trees that have been sampled from the model.",3.2 Dynamic Oracle Supervision,[0],[0]
"We found that including the gold candidate in this manner outperformed initial experiments that first trained a model using likelihood training and then fine-tuned using unbiased policy gradient.
",3.2 Dynamic Oracle Supervision,[0],[0]
"3More generally, an oracle can return a set of such actions that could be taken from the current state, but the oracles we use select a single canonical action.
2.",3.2 Dynamic Oracle Supervision,[0],[0]
"Otherwise, if there are constituents in the gold tree which have not yet been opened in the parser state, with span beginning at the next unshifted word, OPEN the outermost of these.
",3.2 Dynamic Oracle Supervision,[0],[0]
3.,3.2 Dynamic Oracle Supervision,[0],[0]
"Otherwise, SHIFT the next word.
",3.2 Dynamic Oracle Supervision,[0],[0]
"While we do not claim that this dynamic oracle is optimal with respect to F1, we find that it still helps substantially in supervising exploration (Section 5).
",3.2 Dynamic Oracle Supervision,[0],[0]
"Likelihood Training with Exploration Past work has differed on how to use dynamic oracles to guide exploration during oracle training (Ballesteros et al., 2016; Cross and Huang, 2016; Stern et al., 2017).",3.2 Dynamic Oracle Supervision,[0],[0]
"We use the same sample-based method of generating candidate sets Y as for policy gradient, which allows us to control the dynamic oracle and policy gradient methods to perform an equal amount of exploration.",3.2 Dynamic Oracle Supervision,[0],[0]
"Likelihood training with exploration then maximizes the sum of the log probabilities for the oracle actions for all states composing the candidate trees:
LE(θ) =",3.2 Dynamic Oracle Supervision,[0],[0]
N∑ i=1,3.2 Dynamic Oracle Supervision,[0],[0]
"∑ y∈Y(x(i)) ∑ s∈y log p(a∗(s) | s)
where a∗(s) is the dynamic oracle’s action for state s.
Softmax Margin Softmax margin loss (Gimpel and Smith, 2010; Auli and Lopez, 2011) addresses loss mismatch by incorporating task cost into the training loss.",3.2 Dynamic Oracle Supervision,[0],[0]
"Since trees are decomposed into a sequence of local action predictions, we cannot use a global cost, such as F1, directly.",3.2 Dynamic Oracle Supervision,[0],[0]
"As a proxy, we rely on the dynamic oracles’ action-level supervision.
",3.2 Dynamic Oracle Supervision,[0],[0]
"In all models we consider, action probabilities (Eq. 1) are parameterized by a softmax function
pML(a",3.2 Dynamic Oracle Supervision,[0],[0]
| st; θ) ∝,3.2 Dynamic Oracle Supervision,[0],[0]
"exp(z(a, st, θ))
for some state–action scoring function z.",3.2 Dynamic Oracle Supervision,[0],[0]
"The softmax-margin objective replaces this by
pSMM (a | st; θ) ∝",3.2 Dynamic Oracle Supervision,[0],[0]
"exp(z(a, st, θ) + ∆(a, a∗t ))",3.2 Dynamic Oracle Supervision,[0],[0]
"(2) We use ∆(a, a∗t )",3.2 Dynamic Oracle Supervision,[0],[0]
= 0,3.2 Dynamic Oracle Supervision,[0],[0]
if a = a ∗ t and 1 otherwise.,3.2 Dynamic Oracle Supervision,[0],[0]
"This can be viewed as a “soft” version of the maxmargin objective used by Stern et al. (2017) for training without exploration, but retains a locallynormalized model that we can use for samplingbased exploration.
",3.2 Dynamic Oracle Supervision,[0],[0]
"Softmax Margin with Exploration Finally, we train using a combination of softmax margin loss augmentation and exploration.",3.2 Dynamic Oracle Supervision,[0],[0]
"We perform the same sample-based candidate generation as for policy gradient and likelihood training with exploration, but use Eq. 2 to compute the training loss for candidate states.",3.2 Dynamic Oracle Supervision,[0],[0]
"For those parsers that have a dynamic oracle, this provides a means of training that more directly provides both exploration and cost-aware losses.",3.2 Dynamic Oracle Supervision,[0],[0]
We compare the constituency parsers listed in Section 2 using the above training methods.,4 Experiments,[0],[0]
"Our experiments use the English PTB (Marcus et al., 1993), French Treebank (Abeillé et al., 2003), and Penn Chinese Treebank (CTB) Version 5.1 (Xue et al., 2005).
",4 Experiments,[0],[0]
"Training To compare the training procedures as closely as possible, we train all models for a given parser in a given language from the same randomly-initialized parameter values.
",4 Experiments,[0],[0]
"We train two different versions of the RNNG model: one model using size 128 for the LSTMs and hidden states (following the original work), and a larger model with size 256.",4 Experiments,[0],[0]
"We perform evaluation using greedy search in the Span-Based and Top-Down parsers, and beam search with beam size 10 for the RNNG and In-Order parsers.",4 Experiments,[0],[0]
"We found that beam search improved performance for these two parsers by around 0.1-0.3 F1 on the development sets, and use it at inference time in every setting for these two parsers.
",4 Experiments,[0],[0]
"In our experiments, policy gradient typically requires more epochs of training to reach performance comparable to either of the dynamic oraclebased exploration methods.",4 Experiments,[0],[0]
"Figure 1 gives a typical learning curve, for the Top-Down parser on English.",4 Experiments,[0],[0]
"We found that policy gradient is also more sensitive to the number of candidates sampled per
sentence than either of the other exploration methods, with best performance on the development set usually obtained with k = 10 for k ∈ {2, 5, 10} (where k also counts the sentence’s gold tree, included in the candidate set).",4 Experiments,[0],[0]
"See Appendix A in the supplemental material for the values of k used.
",4 Experiments,[0],[0]
"Tags, Embeddings, and Morphology We largely follow previous work for each parser in our use of predicted part-of-speech tags, pretrained word embeddings, and morphological features.
",4 Experiments,[0],[0]
All parsers use predicted part-of-speech tags as part of their sentence representations.,4 Experiments,[0],[0]
"For English and Chinese, we follow the setup of Cross and Huang (2016): training the Stanford tagger (Toutanova et al., 2003) on the training set of each parsing corpus to predict development and test set tags, and using 10-way jackknifing to predict tags for the training set.
",4 Experiments,[0],[0]
"For French, we use the predicted tags and morphological features provided with the SPMRL dataset (Seddah et al., 2014).",4 Experiments,[0],[0]
We modified the publicly released code for all parsers to use predicted morphological features for French.,4 Experiments,[0],[0]
"We follow the approach outlined by Cross and Huang (2016) and Stern et al. (2017) for representing morphological features as learned embeddings, and use the same dimensions for these embeddings as in their papers.",4 Experiments,[0],[0]
"For RNNG and In-Order, we similarly use 10-dimensional learned embeddings for each morphological feature, feeding them as LSTM inputs for each word alongside the word and part-of-speech tag embeddings.
",4 Experiments,[0],[0]
"For RNNG and the In-Order parser, we use the same word embeddings as the original papers for English and Chinese, and train 100-dimensional word embeddings for French using the structured skip-gram method of Ling et al. (2015) on French Wikipedia.",4 Experiments,[0],[0]
Table 1 compares parser F1 by training procedure for each language.,5 Results and Discussion,[0],[0]
"Policy gradient improves upon likelihood training in 14 out of 15 cases, with improvements of up to 1.5 F1.",5 Results and Discussion,[0],[0]
"One of the three dynamic oracle-based training methods – either likelihood with exploration, softmax margin (SMM), or softmax margin with exploration – obtains better performance than policy gradient in 10 out of 12 cases.",5 Results and Discussion,[0],[0]
"This is perhaps unsurprising given the strong supervision provided by the dynamic oracles and the credit assignment problem faced by
policy gradient.",5 Results and Discussion,[0],[0]
"However, a substantial fraction of this performance gain is recaptured by policy gradient in most cases.
",5 Results and Discussion,[0],[0]
"While likelihood training with exploration using a dynamic oracle more directly addresses exploration bias, and softmax margin training more directly addresses loss mismatch, these two phenomena are still entangled, and the best dynamic oracle-based method to use varies.",5 Results and Discussion,[0],[0]
The effectiveness of the oracle method is also likely to be influenced by the nature of the dynamic oracle available for the parser.,5 Results and Discussion,[0],[0]
"For example, the oracle for RNNG lacks F1 optimality guarantees, and softmax margin without exploration often underperforms likelihood for this parser.",5 Results and Discussion,[0],[0]
"However, exploration improves softmax margin training across all parsers and conditions.
",5 Results and Discussion,[0],[0]
"Although results from likelihood training are mostly comparable between RNNG-128 and the larger model RNNG-256 across languages, policy gradient and likelihood training with exploration both typically yield larger improvements in the larger models, obtaining 92.6 F1 for English and 86.0 for Chinese (using likelihood training with exploration), although results are slightly higher for the policy gradient and dynamic oracle-based methods for the smaller model on French (including 83.5 with softmax margin with exploration).",5 Results and Discussion,[0],[0]
"Finally, we observe that policy gradient also provides large improvements for the In-Order parser, where a dynamic oracle has not been defined.
",5 Results and Discussion,[0],[0]
"We note that although some of these results (92.6 for English, 83.5 for French, 87.0 for Chinese) are state-of-the-art for single model, discriminative transition-based parsers, other work on constituency parsing achieves better performance through other methods.",5 Results and Discussion,[0],[0]
"Techniques that combine multiple models or add semi-supervised data (Vinyals et al., 2015; Dyer et al., 2016; Choe and Charniak, 2016; Kuncoro et al., 2017; Liu and Zhang, 2017; Fried et al., 2017) are orthogonal to, and could be combined with, the singlemodel, fixed training data methods we explore.",5 Results and Discussion,[0],[0]
"Other recent work (Gaddy et al., 2018; Kitaev and Klein, 2018) obtains comparable or stronger performance with global chart decoders, where training uses loss augmentation provided by an oracle.",5 Results and Discussion,[0],[0]
"By performing model-optimal global inference, these parsers likely avoid the exposure bias problem of the sequential transition-based parsers we investigate, at the cost of requiring a chart decoding procedure for inference.
",5 Results and Discussion,[0],[0]
"Overall, we find that although optimizing for F1 in a model-agnostic fashion with policy gradient typically underperforms the model-aware expert supervision given by the dynamic oracle training methods, it provides a simple method for consistently improving upon static oracle likelihood training, at the expense of increased training costs.",5 Results and Discussion,[0],[0]
DF is supported by a Huawei / Berkeley AI fellowship.,Acknowledgments,[0],[0]
"This research used the Savio computational cluster provided by the Berkeley Research Computing program at the University of California, Berkeley.",Acknowledgments,[0],[0]
"Dynamic oracles provide strong supervision for training constituency parsers with exploration, but must be custom defined for a given parser’s transition system.",abstractText,[0],[0]
We explore using a policy gradient method as a parser-agnostic alternative.,abstractText,[0],[0]
"In addition to directly optimizing for a tree-level metric such as F1, policy gradient has the potential to reduce exposure bias by allowing exploration during training; moreover, it does not require a dynamic oracle for supervision.",abstractText,[0],[0]
"On four constituency parsers in three languages, the method substantially outperforms static oracle likelihood training in almost all settings.",abstractText,[0],[0]
"For parsers where a dynamic oracle is available (including a novel oracle which we define for the transition system of Dyer et al. (2016)), policy gradient typically recaptures a substantial fraction of the performance gain afforded by the dynamic oracle.",abstractText,[0],[0]
Policy Gradient as a Proxy for Dynamic Oracles in Constituency Parsing,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2442–2452 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
2442",text,[0],[0]
"Semantic parsing from denotations (SpFD) is the problem of mapping text to executable formal representations (or program) in a situated environment and executing them to generate denotations (or answer), in the absence of access to correct representations.",1 Introduction,[0],[0]
"Several problems have been handled within this framework, including question answering (Berant et al., 2013; Iyyer et al., 2017) and instructions for robots (Artzi and Zettlemoyer, 2013; Misra et al., 2015).
",1 Introduction,[0],[0]
Consider the example in Figure 1.,1 Introduction,[0],[0]
"Given the question and a table environment, a semantic parser maps the question to an executable program, in this case a SQL query, and then executes the query on the environment to generate the answer England.",1 Introduction,[0],[0]
"In the SpFD setting, the training data does not contain the correct programs.",1 Introduction,[0],[0]
"Thus, the existing learning approaches for SpFD perform two steps for every training example, a search step that explores the space of programs
Question: what nation scored the most points
Index Name Nation Points Games Pts/game 1 Karen Andrew England 44 5 8.8 2 Daniella Waterman England 40 5 8 3 Christelle Le Duff France 33 5 6.6 4 Charlotte Barras England 30 5 6 5",1 Introduction,[0],[0]
"Naomi Thomas Wales 25 5 5
Select Nation Where Points is Maximum
Program:
",1 Introduction,[0],[0]
"Answer:
Environment:
England
Figure 1: An example of semantic parsing from denotations.",1 Introduction,[0],[0]
"Given the table environment, map the question to an executable program that evaluates to the answer.
and finds suitable candidates, and an update step that uses these programs to update the model.",1 Introduction,[0],[0]
"Figure 2 shows the two step training procedure for the above example.
",1 Introduction,[0],[0]
"In this paper, we address two key challenges in model training for SpFD by proposing a novel learning framework, improving both the search and update steps.",1 Introduction,[0],[0]
"The first challenge, the existence of spurious programs, lies in the search step.",1 Introduction,[0],[0]
"More specifically, while the success of the search step relies on its ability to find programs that are semantically correct, we can only verify if the program can generate correct answers, given that no gold programs are presented.",1 Introduction,[0],[0]
"The search step is complicated by spurious programs, which happen to evaluate to the correct answer but do not represent accurately the meaning of the natural language question.",1 Introduction,[0],[0]
"For example, for the environment in Figure 1, the program Select Nation",1 Introduction,[0],[0]
Where Name = Karen Andrew is spurious.,1 Introduction,[0],[0]
"Selecting spurious programs as positive examples can greatly affect the performance of semantic parsers as these programs generally do not gen-
eralize to unseen questions and environments.",1 Introduction,[0],[0]
"The second challenge, choosing a learning algorithm, lies in the update step.",1 Introduction,[0],[0]
"Because of the unique indirect supervision setting of SpFD, the quality of the learned semantic parser is dictated by the choice of how to update the model parameters, often determined empirically.",1 Introduction,[0],[0]
"As a result, several families of learning methods, including maximum marginal likelihood, reinforcement learning and margin based methods have been used.",1 Introduction,[0],[0]
"How to effectively explore different model choices could be crucial in practice.
",1 Introduction,[0],[0]
Our contributions in this work are twofold.,1 Introduction,[0],[0]
"To address the first challenge, we propose a policy shaping (Griffith et al., 2013) method that incorporates simple, lightweight domain knowledge, such as a small set of lexical pairs of tokens in the question and program, in the form of a critique policy (§ 3).",1 Introduction,[0],[0]
"This helps bias the search towards the correct program, an important step to improve supervision signals, which benefits learning regardless of the choice of algorithm.",1 Introduction,[0],[0]
"To address the second challenge, we prove that the parameter update step in several algorithms are similar and can be viewed as special cases of a generalized update equation (§ 4).",1 Introduction,[0],[0]
The equation contains two variable terms that govern the update behavior.,1 Introduction,[0],[0]
Changing these two terms effectively defines an infinite class of learning algorithms where different values lead to significantly different results.,1 Introduction,[0],[0]
"We study this effect and propose a novel learning framework that improves over existing methods.
",1 Introduction,[0],[0]
"We evaluate our methods using the sequential question answering (SQA) dataset (Iyyer et al., 2017), and show that our proposed improvements to the search and update steps consistently enhance existing approaches.",1 Introduction,[0],[0]
The proposed algorithm achieves new state-of-the-art and outperforms existing parsers by 5.0%.,1 Introduction,[0],[0]
"We give a formal problem definition of the semantic parsing task, followed by the general learning framework for solving it.",2 Background,[0],[0]
The problem discussed in this paper can be formally defined as follows.,2.1 The Semantic Parsing Task,[0],[0]
"Let X be the set of all possible questions, Y programs (e.g., SQL-like queries), T tables (i.e., the structured data in this work) and Z answers.",2.1 The Semantic Parsing Task,[0],[0]
We further assume access to an executor : Y ⇥ T !,2.1 The Semantic Parsing Task,[0],[0]
"Z , that given a program y 2 Y and a table t 2 T , generates an answer (y, t) 2 Z .",2.1 The Semantic Parsing Task,[0],[0]
We assume that the executor and all tables are deterministic and the executor can be called as many times as possible.,2.1 The Semantic Parsing Task,[0],[0]
"To facilitate discussion in the following sections, we define an environment function et : Y !",2.1 The Semantic Parsing Task,[0],[0]
"Z , by applying the executor to the program as et(y) = (y, t).
",2.1 The Semantic Parsing Task,[0],[0]
"Given a question x and an environment et, our aim is to generate a program y⇤ 2 Y and then execute it to produce the answer et(y⇤).",2.1 The Semantic Parsing Task,[0],[0]
"Assume that for any y 2 Y , the score of y being a correct program for x is score✓(y, x, t), parameterized by ✓.",2.1 The Semantic Parsing Task,[0],[0]
"The inference task is thus:
y⇤ = arg max y2Y score✓(y, x, t) (1)
",2.1 The Semantic Parsing Task,[0],[0]
"As the size of Y is exponential to the length of the program, a generic search procedure is typically employed for Eq.",2.1 The Semantic Parsing Task,[0],[0]
"(1), as efficient dynamic algorithms typically do not exist.",2.1 The Semantic Parsing Task,[0],[0]
"These search procedures generally maintain a beam of program states sorted according to some scoring function, where each program state represents an incomplete program.",2.1 The Semantic Parsing Task,[0],[0]
The search then generates a new program state from an existing state by performing an action.,2.1 The Semantic Parsing Task,[0],[0]
"Each action adds a set of tokens (e.g., Nation) and keyword (e.g., Select) to a
program state.",2.1 The Semantic Parsing Task,[0],[0]
"For example, in order to generate the program in Figure 1, the DynSP parser (Iyyer et al., 2017) will take the first action as adding the SQL expression Select Nation.",2.1 The Semantic Parsing Task,[0],[0]
Notice that score✓ can be used in either probabilistic or nonprobabilistic models.,2.1 The Semantic Parsing Task,[0],[0]
"For probabilistic models, we assume that it is a Boltzmann policy, meaning that p✓(y | x, t) / exp{score✓(y, x, t)}.",2.1 The Semantic Parsing Task,[0],[0]
"Learning a semantic parser is equivalent to learning the parameters ✓ in the scoring function, which is a structured learning problem, due to the large, structured output space Y .",2.2 Learning,[0],[0]
Structured learning algorithms generally consist of two major components: search and update.,2.2 Learning,[0],[0]
"When the gold programs are available during training, the search procedure finds a set of high-scoring incorrect programs.",2.2 Learning,[0],[0]
These programs are used by the update step to derive loss for updating parameters.,2.2 Learning,[0],[0]
"For example, these programs are used for approximating the partition-function in maximum-likelihood objective (Liang et al., 2011) and finding set of programs causing margin violation in margin based methods (Daumé III and Marcu, 2005).",2.2 Learning,[0],[0]
"Depending on the exact algorithm being used, these two components are not necessarily separated into isolated steps.",2.2 Learning,[0],[0]
"For instance, parameters can be updated in the middle of search (e.g., Huang et al., 2012).
",2.2 Learning,[0],[0]
"For learning semantic parsers from denotations, where we assume only answers are available in a training set {(xi, ti, zi)}Ni=1 of N examples, the basic construction of the learning algorithms remains the same.",2.2 Learning,[0],[0]
"However, the problems that search needs to handle in SpFD is more challenging.",2.2 Learning,[0],[0]
"In addition to finding a set of high-scoring incorrect programs, the search procedure also needs to guess the correct program(s) evaluating to the gold answer zi.",2.2 Learning,[0],[0]
"This problem is further complicated by the presence of spurious programs, which generate the correct answer but are semantically incompatible with the question.",2.2 Learning,[0],[0]
"For example, although all programs in Figure 2 evaluate to the same answer, only one of them is correct.",2.2 Learning,[0],[0]
The issue of the spurious programs also affects the design of model update.,2.2 Learning,[0],[0]
"For instance, maximum marginal likelihood methods treat all the programs that evaluate to the gold answer equally, while maximum margin reward networks use model score to break tie and pick one of the
programs as the correct reference.",2.2 Learning,[0],[0]
"Given a training example (x, t, z), the aim of the search step is to find a set K(x, t, z) of programs consisting of correct programs that evaluate to z and high-scoring incorrect programs.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
The search step should avoid picking up spurious programs for learning since such programs typically do not generalize.,3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"For example, in Figure 2, the spurious program Select Nation Where Index is Min will evaluate to an incorrect answer if the indices of the first two rows are swapped1.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"This problem is challenging since among the programs that evaluate to the correct answer, most of them are spurious.
",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"The search step can be viewed as following an exploration policy b✓(y|x, t, z) to explore the set of programs Y .",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"This exploration is often performed by beam search and at each step, we either sample from b✓ or take the top scoring programs.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"The set K(x, t, z) is then used by the update step for parameter update.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"Most search strategies use an exploration policy which is based on the score function, for example b✓(y|x, t, z) / exp{score✓(y, t)}.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"However, this approach can suffer from a divergence phenomenon whereby the score of spurious programs picked up by the search in the first epoch increases, making it more likely for the search to pick them up in the future.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"Such divergence issues are common with latent-variable learning and often require careful initialization to overcome (Rose, 1998).",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"Unfortunately such initialization schemes are not applicable for deep neural networks which form the model of most successful semantic parsers today (Jia and Liang, 2016; Misra and Artzi, 2016; Iyyer et al., 2017).",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"Prior work, such as ✏-greedy exploration (Guu et al., 2017), has reduced the severity of this problem by introducing random noise in the search procedure to avoid saturating the search on high-scoring spurious programs.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"However, random noise need not bias the search towards the correct program(s).",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"In this paper, we introduce a simple policy-shaping method to guide the search.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"This approach allows incorporating prior knowledge in the exploration policy and can bias the search away from spurious programs.
",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"1This transformation preserves the answer of the question.
",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"Algorithm 1 Learning a semantic parser from denotation using generalized updates.
",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"Input: Training set {(xi, ti, zi}Ni=1 (see Section 2), learning rate µ and stopping epoch T (̃see Section 4).",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"Definitions: score✓(y, x, t) is a semantic parsing model parameterized by ✓.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
ps(y,3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"| x, t) is the policy used for exploration and search(✓, x, t, z, ps) generates candidate programs for updating parameters (see Section 3).",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
is the generalized update (see Section 4).,3 Addressing Spurious Programs: Policy Shaping,[0],[0]
Output: Model parameters ✓.,3 Addressing Spurious Programs: Policy Shaping,[0],[0]
1: » Iterate over the training data.,3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"2: for t = 1 to T , i = 1 to N do 3: » Find candidate programs using the shaped policy.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"4: K = search(✓, xi, ti, zi, ps) 5: » Compute generalized gradient updates 6: ✓ = ✓ + µ (K) 7: return ✓
Policy Shaping Policy shaping is a method to introduce prior knowledge into a policy (Griffith et al., 2013).",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"Formally, let the current behavior policy be b✓(y|x, t, z) and a predefined critique policy, the prior knowledge, be pc(y|x, t).",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"Policy shaping defines a new shaped behavior policy pb(y|x, t) given by:
pb(y|x, t) = b✓(y|x, t, z)pc(y|x, t)P
y02Y",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"b✓(y 0|x, t, z)pc(y0|x, t)
.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"(2)
Using the shaped policy for exploration biases the search towards the critique policy’s preference.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"We next describe a simple critique policy that we use in this paper.
",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
Lexical Policy Shaping We qualitatively observed that correct programs often contains tokens which are also present in the question.,3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"For example, the correct program in Figure 2 contains the token Points, which is also present in the question.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"We therefore, define a simple surface form similarity feature match(x, y) that computes the ratio of number of non-keyword tokens in the program y that are also present in the question",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"x.
However, surface-form similarity is often not enough.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"For example, both the first and fourth program in Figure 2 contain the token Points but only the fourth program is correct.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"Therefore, we also use a simple co-occurrence feature that triggers on frequently co-occurring pairs of tokens in the program and instruction.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"For example, the token most is highly likely to co-occur with a correct program containing the keyword Max.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
This happens for the example in Figure 2.,3 Addressing Spurious Programs: Policy Shaping,[0],[0]
Similarly the token not may co-occur with the keyword NotEqual.,3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"We assume access to a lexicon ⇤ = {(wj , !j)}kj=1",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"containing
k lexical pairs of tokens and keywords.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"Each lexical pair (w, !) maps the token w in a text to a keyword !",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
in a program.,3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"For a given program y and question x, we define a co-occurrence score as co_occur(y, x) = P (w,!)2⇤ {w 2 x ^ !",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
2 y}}.,3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"We define critique score critique(y, x) as the sum of the match and co_occur scores.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"The critique policy is given by:
pc(y|x, t) / exp",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"(⌘ ⇤ critique(y, x)) , (3)
where ⌘ is a single scalar hyper-parameter denoting the confidence in the critique policy.",3 Addressing Spurious Programs: Policy Shaping,[0],[0]
"Given the set of programs generated by the search step, one can use many objectives to update the parameters.",4 Addressing Update Strategy Selection: Generalized Update Equation,[0],[0]
"For example, previous work have utilized maximum marginal likelihood (Krishnamurthy et al., 2017; Guu et al., 2017), reinforcement learning (Zhong et al., 2017; Guu et al., 2017) and margin based methods (Iyyer et al., 2017).",4 Addressing Update Strategy Selection: Generalized Update Equation,[0],[0]
"It could be difficult to choose the suitable algorithm from these options.
",4 Addressing Update Strategy Selection: Generalized Update Equation,[0],[0]
"In this section, we propose a principle and general update equation such that previous update algorithms can be considered as special cases to this equation.",4 Addressing Update Strategy Selection: Generalized Update Equation,[0],[0]
Having a general update is important for the following reasons.,4 Addressing Update Strategy Selection: Generalized Update Equation,[0],[0]
"First, it allows us to understand existing algorithms better by examining their basic properties.",4 Addressing Update Strategy Selection: Generalized Update Equation,[0],[0]
"Second, the generalized update equation also makes it easy to implement and experiment with various different algorithms.",4 Addressing Update Strategy Selection: Generalized Update Equation,[0],[0]
"Moreover, it provides a framework that enables the development of new variations or extensions of existing learning methods.
",4 Addressing Update Strategy Selection: Generalized Update Equation,[0],[0]
"In the following, we describe how the commonly used algorithms are in fact very similar – their update rules can all be viewed as special cases of the proposed generalized update equation.",4 Addressing Update Strategy Selection: Generalized Update Equation,[0],[0]
Algorithm 1 shows the meta-learning framework.,4 Addressing Update Strategy Selection: Generalized Update Equation,[0],[0]
"For every training example, we first find a set of candidates using an exploration policy (line 4).",4 Addressing Update Strategy Selection: Generalized Update Equation,[0],[0]
We use the program candidates to update the parameters (line 6).,4 Addressing Update Strategy Selection: Generalized Update Equation,[0],[0]
"We briefly describe three algorithms: maximum marginalized likelihood, policy gradient and maximum margin reward.
",4.1 Commonly Used Learning Algorithms,[0],[0]
"Maximum Marginalized Likelihood The maximum marginalized likelihood method maximizes the log-likelihood of the training data by marginalizing over the set of programs.
",4.1 Commonly Used Learning Algorithms,[0],[0]
"JMML = log p(zi|xi, ti) = log X
y2Y p(zi|y, ti)p(y|xi, ti) (4)
",4.1 Commonly Used Learning Algorithms,[0],[0]
"Because an answer is deterministically computed given a program and a table, we define p(z | y, t) as 1 or 0 depending upon whether the y evaluates to z given t, or not.",4.1 Commonly Used Learning Algorithms,[0],[0]
"Let Gen(z, t) ✓ Y be the set of compatible programs that evaluate to z given the table t.",4.1 Commonly Used Learning Algorithms,[0],[0]
"The objective can then be expressed as:
JMML = log X
y2Gen(zi,ti)
p(y|xi, ti) (5)
",4.1 Commonly Used Learning Algorithms,[0],[0]
"In practice, the summation over Gen(.) is approximated by only using the compatible programs in the set K generated by the search step.
",4.1 Commonly Used Learning Algorithms,[0],[0]
Policy Gradient Methods Most reinforcement learning approaches for semantic parsing assume access to a reward function R : Y ⇥X ⇥Z !,4.1 Commonly Used Learning Algorithms,[0],[0]
"R, giving a scalar reward R(y, z) for a given program y and the correct answer z.2",4.1 Commonly Used Learning Algorithms,[0],[0]
"We can further assume without loss of generality that the reward is always in [0, 1].",4.1 Commonly Used Learning Algorithms,[0],[0]
"Reinforcement learning approaches maximize the expected reward JRL:
JRL = X
y2Y p(y|xi, ti)R(y, zi) (6)
JRL is hard to approximate using numerical integration since the reward for all programs may not be known a priori.",4.1 Commonly Used Learning Algorithms,[0],[0]
Policy gradient methods solve this by approximating the derivative using a sample from the policy.,4.1 Commonly Used Learning Algorithms,[0],[0]
"When the search space is large, the policy may fail to sample a correct program, which can greatly slow down the learning.",4.1 Commonly Used Learning Algorithms,[0],[0]
"Therefore, off-policy methods are sometimes introduced to bias the sampling towards high-reward yielding programs.",4.1 Commonly Used Learning Algorithms,[0],[0]
"In those methods, an additional exploration policy u(y|xi, ti, zi) is used to improve sampling.",4.1 Commonly Used Learning Algorithms,[0],[0]
"Importance weights are used to make the gradient unbiased (see Appendix for derivation).
",4.1 Commonly Used Learning Algorithms,[0],[0]
2This is essentially a contextual bandit setting.,4.1 Commonly Used Learning Algorithms,[0],[0]
Guu et al. (2017) also used this setting.,4.1 Commonly Used Learning Algorithms,[0],[0]
A general reinforcement learning setting requires taking a sequence of actions and receiving a reward for each action.,4.1 Commonly Used Learning Algorithms,[0],[0]
"For example, a program can be viewed as a sequence of parsing actions, where each action can get a reward.",4.1 Commonly Used Learning Algorithms,[0],[0]
"We do not consider the general setting here.
",4.1 Commonly Used Learning Algorithms,[0],[0]
"Maximum Margin Reward For every training example (xi, ti, zi), the maximum margin reward method finds the highest scoring program yi that evaluates to zi, as the reference program, from the set K of programs generated by the search.",4.1 Commonly Used Learning Algorithms,[0],[0]
With a margin function : Y⇥Y⇥Z !,4.1 Commonly Used Learning Algorithms,[0],[0]
"R and reference program y, the set of programs V that violate the margin constraint can thus be defined as:
V = {y0 | y0 2 Y and score✓(y, x, t)  score✓(y0, x, t)",4.1 Commonly Used Learning Algorithms,[0],[0]
+,4.1 Commonly Used Learning Algorithms,[0],[0]
"(y, y0, z)}, (7)
where (y, y0, z) = R(y, z)",4.1 Commonly Used Learning Algorithms,[0],[0]
"R(y0, z).",4.1 Commonly Used Learning Algorithms,[0],[0]
"Similarly, the program that most violates the constraint can be written as:
ȳ = arg max y02Y
{score✓(y0, x, t) +",4.1 Commonly Used Learning Algorithms,[0],[0]
"(y, y0, z)
score✓(y, x, t)}",4.1 Commonly Used Learning Algorithms,[0],[0]
"(8)
The most-violation margin objective (negative margin loss) is thus defined as:
JMMR = max{0, score✓(ȳ, xi, ti) score✓(yi, xi, ti) +",4.1 Commonly Used Learning Algorithms,[0],[0]
"(yi, ȳ, zi)}
Unlike the previous two learning algorithms, margin methods only update the score of the reference program and the program that violates the margin.",4.1 Commonly Used Learning Algorithms,[0],[0]
"Although the algorithms described in §4.1 seem very different on the surface, the gradients of their loss functions can in fact be described in the same generalized form, given in Eq.",4.2 Generalized Update Equation,[0],[0]
(9)3.,4.2 Generalized Update Equation,[0],[0]
"In addition to the gradient of the model scoring function, this equation has two variable terms, w(·), q(·).",4.2 Generalized Update Equation,[0],[0]
"We call the first term w(y, x, t, z) intensity, which is a positive scalar value and the second term q(y|x, t) the competing distribution, which is a probability distribution over programs.",4.2 Generalized Update Equation,[0],[0]
"Varying them makes the equation equivalent to the update rule of the algorithms we discussed, as shown in Table 1.",4.2 Generalized Update Equation,[0],[0]
"We also consider meritocratic update policy which uses a hyperparameter to sharpen or smooth the intensity of maximum marginal likelihood (Guu et al., 2017).
",4.2 Generalized Update Equation,[0],[0]
"Intuitively, w(y, x, t, z) defines the positive part of the update equation, which defines how aggressively the update favors program y. Likewise, q(y|x, t) defines the negative part of the learning
3See Appendix for the detailed derivation.
",4.2 Generalized Update Equation,[0],[0]
"Generalized Update Equation:
(K) = X
y2K w(y, x, t, z)
0 @r✓score✓(y, x, t) X
y02Y",4.2 Generalized Update Equation,[0],[0]
"q(y0|x, t)r✓score✓(y0, x, t)
1
A (9)
algorithm, namely how aggressively the update penalizes the members of the program set.
",4.2 Generalized Update Equation,[0],[0]
"The generalized update equation provides a tool for better understanding individual algorithm, and helps shed some light on when a particular method may perform better.
",4.2 Generalized Update Equation,[0],[0]
"Intensity versus Search Quality In SpFD, the effectiveness of the algorithms for SpFD is closely related to the quality of the search results given that the gold program is not available.",4.2 Generalized Update Equation,[0],[0]
"Intuitively, if the search quality is good, the update algorithm could be aggressive on updating the model parameters.",4.2 Generalized Update Equation,[0],[0]
"When the search quality is poor, the algorithm should be conservative.
",4.2 Generalized Update Equation,[0],[0]
The intensity w(·) is closely related to the aggressiveness of the algorithm.,4.2 Generalized Update Equation,[0],[0]
"For example, the maximum marginal likelihood is less aggressive given that it produces a non-zero intensity over all programs in the program set K that evaluate to the correct answer.",4.2 Generalized Update Equation,[0],[0]
"The intensity for a particular correct program y is proportional to its probability p(y|x, t).",4.2 Generalized Update Equation,[0],[0]
"Further, meritocratic update becomes more aggressive as becomes larger.
",4.2 Generalized Update Equation,[0],[0]
"In contrast, REINFORCE and maximum margin reward both have a non-zero intensity only on a single program in K.",4.2 Generalized Update Equation,[0],[0]
"This value is 1.0 for maximum margin reward, while for reinforcement learning, this value is the reward.",4.2 Generalized Update Equation,[0],[0]
"Maximum margin reward therefore updates most aggressively in favor of its selection while maximum marginal
likelihood tends to hedge its bet.",4.2 Generalized Update Equation,[0],[0]
"Therefore, the maximum margin methods should benefit the most when the search quality improves.
",4.2 Generalized Update Equation,[0],[0]
Stability The general equation also allows us to investigate the stability of a model update algorithm.,4.2 Generalized Update Equation,[0],[0]
"In general, the variance of update direction can be high, hence less stable, if the model update algorithm has peaky competing distribution, or it puts all of its intensity on a single program.",4.2 Generalized Update Equation,[0],[0]
"For example, REINFORCE only samples one program and puts non-zero intensity only on that program, so it could be unstable depending on the sampling results.
",4.2 Generalized Update Equation,[0],[0]
The competing distribution affects the stability of the algorithm.,4.2 Generalized Update Equation,[0],[0]
"For example, maximum margin reward penalizes only the most violating program and is benign to other incorrect programs.",4.2 Generalized Update Equation,[0],[0]
"Therefore, the MMR algorithm could be unstable during training.
",4.2 Generalized Update Equation,[0],[0]
New Model Update Algorithm,4.2 Generalized Update Equation,[0],[0]
The general equation provides a framework that enables the development of new variations or extensions of existing learning methods.,4.2 Generalized Update Equation,[0],[0]
"For example, in order to improve the stability of the MMR algorithm, we propose a simple variant of maximum margin reward, which penalizes all violating programs instead of only the most violating one.",4.2 Generalized Update Equation,[0],[0]
"We call this approach maximum margin average violation reward (MAVER), which is included in Table 1 as well.",4.2 Generalized Update Equation,[0],[0]
"Given that MAVER effectively considers
more negative examples during each update, we expect that it is more stable compared to the MMR algorithm.",4.2 Generalized Update Equation,[0],[0]
We describe the setup in §5.1 and results in §5.2.,5 Experiments,[0],[0]
"Dataset We use the sequential question answering (SQA) dataset (Iyyer et al., 2017) for our experiments.",5.1 Setup,[0],[0]
"SQA contains 6,066 sequences and each sequence contains up to 3 questions, with 17,553 questions in total.",5.1 Setup,[0],[0]
The data is partitioned into training (83%) and test (17%) splits.,5.1 Setup,[0],[0]
We use 4/5 of the original train split as our training set and the remaining 1/5 as the dev set.,5.1 Setup,[0],[0]
We evaluate using exact match on answer.,5.1 Setup,[0],[0]
"Previous state-of-theart result on the SQA dataset is 44.7% accuracy, using maximum margin reward learning.
",5.1 Setup,[0],[0]
"Semantic Parser Our semantic parser is based on DynSP (Iyyer et al., 2017), which contains a set of SQL actions, such as adding a clause (e.g., Select Column) or adding an operator (e.g., Max).",5.1 Setup,[0],[0]
"Each action has an associated neural network module that generates the score for the action based on the instruction, the table and the list of past actions.",5.1 Setup,[0],[0]
"The score of the entire program is given by the sum of scores of all actions.
",5.1 Setup,[0],[0]
We modified DynSP to improve its representational capacity.,5.1 Setup,[0],[0]
We refer to the new parser as DynSP++.,5.1 Setup,[0],[0]
"Most notably, we included new features and introduced two additional parser actions.",5.1 Setup,[0],[0]
See Appendix 8.2 for more details.,5.1 Setup,[0],[0]
"While these improvements help us achieve state-of-the-art results, the majority of the gain comes from the learning contributions described in this paper.
",5.1 Setup,[0],[0]
"Hyperparameters For each experiment, we train the model for 30 epochs.",5.1 Setup,[0],[0]
We find the optimal stopping epoch by evaluating the model on the dev set.,5.1 Setup,[0],[0]
We then train on train+dev set till the stopping epoch and evaluate the model on the held-out test set.,5.1 Setup,[0],[0]
Model parameters are trained using stochastic gradient descent with learning rate of 0.1.,5.1 Setup,[0],[0]
We set the hyperparameter ⌘ for policy shaping to 5.,5.1 Setup,[0],[0]
All hyperparameters were tuned on the dev set.,5.1 Setup,[0],[0]
We use 40 lexical pairs for defining the co-occur score.,5.1 Setup,[0],[0]
"We used common English superlatives (e.g., highest, most) and comparators (e.g., more, larger) and did not fit the lexical pairs based on the dataset.
",5.1 Setup,[0],[0]
"Given the model parameter ✓, we use a base exploration policy defined in (Iyyer et al., 2017).",5.1 Setup,[0],[0]
"This exploration policy is given by b✓(y | x, t, z) / exp( · R(y, z) + score",5.1 Setup,[0],[0]
"✓(y, ✓, z)).",5.1 Setup,[0],[0]
"R(y, z) is the reward function of the incomplete program y, given the answer z.",5.1 Setup,[0],[0]
"We use a reward function R(y, z) given by the Jaccard similarity of the gold answer z and the answer generated by the program y.",5.1 Setup,[0],[0]
"The value of is set to infinity, which essentially is equivalent to sorting the programs based on the reward and using the current model score for tie breaking.",5.1 Setup,[0],[0]
"Further, we prune all syntactically invalid programs.",5.1 Setup,[0],[0]
"For more details, we refer the reader to (Iyyer et al., 2017).",5.1 Setup,[0],[0]
Table 2 contains the dev and test results when using our algorithm on the SQA dataset.,5.2 Results,[0],[0]
We observe that margin based methods perform better than maximum likelihood methods and policy gradient in our experiment.,5.2 Results,[0],[0]
Policy shaping in general improves the performance across different algorithms.,5.2 Results,[0],[0]
"Our best test results outperform previous SOTA by 5.0%.
",5.2 Results,[0],[0]
"Policy Gradient vs Off-Policy Gradient REINFORCE, a simple policy gradient method, achieved extremely poor performance.",5.2 Results,[0],[0]
This likely due to the problem of exploration and having to sample from a large space of programs.,5.2 Results,[0],[0]
This is further corroborated from observing the much superior performance of off-policy policy gradient methods.,5.2 Results,[0],[0]
"Thus, the sampling policy is an important factor to consider for policy gradient methods.
",5.2 Results,[0],[0]
The Effect of Policy Shaping We observe that the improvement due to policy shaping is 6.0% on the SQA dataset for MAVER and only 1.3% for maximum marginal likelihood.,5.2 Results,[0],[0]
"We also observe that as increases, the improvement due to policy shaping for meritocratic update increases.",5.2 Results,[0],[0]
"This supports our hypothesis that aggressive updates of margin based methods is beneficial when the search method is more accurate as compared to maximum marginal likelihood which hedges its bet between all programs that evaluate to the right answer.
",5.2 Results,[0],[0]
"Stability of MMR In Section 4, the general update equation helps us point out that MMR could be unstable due to the peaky competing distribution.",5.2 Results,[0],[0]
MAVER was proposed to increase the stability of the algorithm.,5.2 Results,[0],[0]
"To measure stability, we cal-
culate the mean absolute difference of the development set accuracy between successive epochs during training, as it indicates how much an algorithm’s performance fluctuates during training.",5.2 Results,[0],[0]
"With this metric, we found mean difference for MAVER is 0.57% where the mean difference for MMR is 0.9%.",5.2 Results,[0],[0]
"This indicates that MAVER is in fact more stable than MMR.
",5.2 Results,[0],[0]
Other variations We also analyze other possible novel learning algorithms that are made possible due to generalized update equations.,5.2 Results,[0],[0]
Table 3 reports development results using these algorithms.,5.2 Results,[0],[0]
"By mixing different intensity scalars and competing distribution from different algorithms, we can create new variations of the model update algorithm.",5.2 Results,[0],[0]
"In Table 3, we show that by mixing the MMR’s intensity and MML’s competing distribution, we can create an algorithm that outperform MMR on the development set.
",5.2 Results,[0],[0]
"Policy Shaping helps against Spurious Programs In order to better understand if policy shaping helps bias the search away from spurious programs, we analyze 100 training examples.",5.2 Results,[0],[0]
We look at the highest scoring program in the beam at the end of training using MAVER.,5.2 Results,[0],[0]
"Without policy shaping, we found that 53 programs were spurious while using policy shaping this number came down to 23.",5.2 Results,[0],[0]
"We list few examples of spurious program errors corrected by policy shaping in Table 4.
Policy Shaping vs Model Shaping Critique policy contains useful information that can bias the search away from spurious programs.",5.2 Results,[0],[0]
"Therefore, one can also consider making the critique policy as part of the model.",5.2 Results,[0],[0]
We call this model shaping.,5.2 Results,[0],[0]
We define our model to be the shaped policy and train and test using the new model.,5.2 Results,[0],[0]
"Using MAVER updates, we found that the dev accuracy dropped to 37.1%.",5.2 Results,[0],[0]
We conjecture that the strong prior in the critique policy can hinder generalization in model shaping.,5.2 Results,[0],[0]
Semantic Parsing from Denotation Mapping natural language text to formal meaning representation was first studied by Montague (1970).,6 Related Work,[0],[0]
"Early work on learning semantic parsers rely on labeled formal representations as the supervision signals (Zettlemoyer and Collins, 2005, 2007; Zelle and Mooney, 1993).",6 Related Work,[0],[0]
"However, because getting access to gold formal representation generally requires expensive annotations by an expert, distant supervision approaches, where semantic parsers are learned from denotation only, have become the main learning paradigm (e.g., Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Berant et al., 2013; Iyyer et al., 2017; Krishnamurthy et al., 2017).",6 Related Work,[0],[0]
"Guu et al. (2017) studied the problem of spurious programs and considered adding noise to diversify the search procedure and introduced meritocratic updates.
",6 Related Work,[0],[0]
"Reinforcement Learning Algorithms Reinforcement learning algorithms have been applied to various NLP problems including dialogue (Li et al., 2016), text-based games (Narasimhan et al., 2015), information extraction (Narasimhan et al., 2016), coreference resolution (Clark and Man-
ning, 2016), semantic parsing (Guu et al., 2017) and instruction following (Misra et al., 2017).",6 Related Work,[0],[0]
Guu et al. (2017) show that policy gradient methods underperform maximum marginal likelihood approaches.,6 Related Work,[0],[0]
Our result on the SQA dataset supports their observation.,6 Related Work,[0],[0]
"However, we show that using off-policy sampling, policy gradient methods can provide superior performance to maximum marginal likelihood methods.
",6 Related Work,[0],[0]
Margin-based Learning Margin-based methods have been considered in the context of SVM learning.,6 Related Work,[0],[0]
"In the NLP literature, margin based learning has been applied to parsing (Taskar et al., 2004; McDonald et al., 2005), text classification (Taskar et al., 2003), machine translation (Watanabe et al., 2007) and semantic parsing (Iyyer et al., 2017).",6 Related Work,[0],[0]
Kummerfeld et al. (2015) found that max-margin based methods generally outperform likelihood maximization on a range of tasks.,6 Related Work,[0],[0]
Previous work have studied connections between margin based method and likelihood maximization for supervised learning setting.,6 Related Work,[0],[0]
We show them as special cases of our unified update equation for distant supervision learning.,6 Related Work,[0],[0]
"Similar to this work, Lee et al. (2016) also found that in the context of supervised learning, margin-based algorithms which update all violated examples perform better than the one that only updates the most violated example.
",6 Related Work,[0],[0]
"Latent Variable Modeling Learning semantic parsers from denotation can be viewed as a latent variable modeling problem, where the program is the latent variable.",6 Related Work,[0],[0]
"Probabilistic latent variable models have been studied using EM-algorithm and its variant (Dempster et al., 1977).",6 Related Work,[0],[0]
"The graphical model literature has studied latent variable learning on margin-based methods (Yu and Joachims, 2009) and probabilistic models (Quattoni et al., 2007).",6 Related Work,[0],[0]
"Samdani et al. (2012) studied various vari-
ants of EM algorithm and showed that all of them are special cases of a unified framework.",6 Related Work,[0],[0]
Our generalized update framework is similar in spirit.,6 Related Work,[0],[0]
"In this paper, we propose a general update equation from semantic parsing from denotation and propose a policy shaping method for addressing the spurious program challenge.",7 Conclusion,[0],[0]
"For the future, we plan to apply the proposed learning framework to more semantic parsing tasks and consider new methods for policy shaping.",7 Conclusion,[0],[0]
"We thank Ryan Benmalek, Alane Suhr, Yoav Artzi, Claire Cardie, Chris Quirk, Michel Galley and members of the Cornell NLP group for their valuable comments.",8 Acknowledgements,[0],[0]
We are also grateful to Allen Institute for Artificial Intelligence for the computing resource support.,8 Acknowledgements,[0],[0]
This work was initially started when the first author interned at Microsoft Research.,8 Acknowledgements,[0],[0]
"Semantic parsing from denotations faces two key challenges in model training: (1) given only the denotations (e.g., answers), search for good candidate semantic parses, and (2) choose the best model update algorithm.",abstractText,[0],[0]
We propose effective and general solutions to each of them.,abstractText,[0],[0]
"Using policy shaping, we bias the search procedure towards semantic parses that are more compatible to the text, which provide better supervision signals for training.",abstractText,[0],[0]
"In addition, we propose an update equation that generalizes three different families of learning algorithms, which enables fast model exploration.",abstractText,[0],[0]
"When experimented on a recently proposed sequential question answering dataset, our framework leads to a new state-of-theart model that outperforms previous work by 5.0% absolute on exact match accuracy.",abstractText,[0],[0]
Policy Shaping and Generalized Update Equations for Semantic Parsing from Denotations,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 784–792 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1073",text,[0],[0]
"The basic idea of distributional semantics, i.e. determining the meaning of a word based on its co-occurrence with other words, is derived from the empiricists – Harris (1954) and Firth (1957).",1.1 Distributional semantics,[0],[0]
"John R. Firth drew attention to the contextdependent nature of meaning especially with his
1The dataset is obtainable at: http://zil.ipipan.waw.pl/Scwad/CDSCorpus
famous maxim “You shall know a word by the company it keeps” (Firth, 1957, p. 11).
",1.1 Distributional semantics,[0],[0]
"Nowadays, distributional semantics models are estimated with various methods, e.g. word embedding techniques (Bengio et al., 2003, 2006; Mikolov et al., 2013).",1.1 Distributional semantics,[0],[0]
"To ascertain the purport of a word, e.g. bath, you can use the context of other words that surround it.",1.1 Distributional semantics,[0],[0]
"If we assume that the meaning of this word expressed by its lexical context is associated with a distributional vector, the distance between distributional vectors of two semantically similar words, e.g bath and shower, should be smaller than between vectors representing semantically distinct words, e.g. bath and tree.",1.1 Distributional semantics,[0],[0]
"Based on empirical observations that distributional vectors encode certain aspects of word meaning, it is expected that similar aspects of the meaning of phrases and sentences can also be represented with vectors obtained via composition of distributional word vectors.",1.2 Compositional distributional semantics,[0],[0]
The idea of semantic composition is not new.,1.2 Compositional distributional semantics,[0],[0]
It is well known as the principle of compositionality:2,1.2 Compositional distributional semantics,[0],[0]
“The meaning of a compound expression is a function of the meaning of its parts and of the way they are syntactically combined.”,1.2 Compositional distributional semantics,[0],[0]
"(Janssen, 2012, p. 19).
",1.2 Compositional distributional semantics,[0],[0]
"Modelling the meaning of textual units larger than words using compositional and distributional information is the main subject of compositional distributional semantics (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012, to name a few studies).",1.2 Compositional distributional semantics,[0],[0]
"The fundamental principles of compositional distributional semantics, henceforth referred to as CDS, are mainly propagated with papers written on the topic.",1.2 Compositional distributional semantics,[0],[0]
"Apart from the papers, it was the SemEval-2014 Shared Task 1
2As the principle of compositionality is attributed to Gottlob Frege, it is often called Frege’s principle.
784
(Marelli et al., 2014) that essentially contributed to the expansion of CDS and increased an interest in this domain.",1.2 Compositional distributional semantics,[0],[0]
The goal of the task was to evaluate CDS models of English in terms of semantic relatedness and entailment on proper sentences from the SICK corpus.,1.2 Compositional distributional semantics,[0],[0]
"The SICK corpus (Bentivogli et al., 2014) consists of 10K pairs of English sentences containing multiple lexical, syntactic, and semantic phenomena.",1.3 The SICK corpus,[0],[0]
"It builds on two external data sources – the 8K ImageFlickr dataset (Rashtchian et al., 2010) and SemEval-2012 Semantic Textual Similarity dataset (Agirre et al., 2012).",1.3 The SICK corpus,[0],[0]
"Each sentence pair is human-annotated for relatedness in meaning and entailment.
",1.3 The SICK corpus,[0],[0]
The relatedness score corresponds to the degree of semantic relatedness between two sentences and is calculated as the average of ten human ratings collected for this sentence pair on the 5-point Likert scale.,1.3 The SICK corpus,[0],[0]
"This score indicates the extent to which the meanings of two sentences are related.
",1.3 The SICK corpus,[0],[0]
"The entailment relation between two sentences, in turn, is labelled with entailment, contradiction, or neutral.",1.3 The SICK corpus,[0],[0]
"According to the SICK guidelines, the label assigned by the majority of human annotators is selected as the valid entailment label.",1.3 The SICK corpus,[0],[0]
"Studying approaches to various natural language processing (henceforth NLP) problems, we have observed that the availability of language resources (e.g. training or testing data) stimulates the development of NLP tools and the estimation of NLP models.",1.4 Motivation and organisation of the paper,[0],[0]
English is undoubtedly the most prominent in this regard and English resources are the most numerous.,1.4 Motivation and organisation of the paper,[0],[0]
"Therefore, NLP methods are mostly designed for English and tested on English data, even if there is no guarantee that they are universal.",1.4 Motivation and organisation of the paper,[0],[0]
"In order to verify whether an NLP algorithm is adequate, it is not enough to evaluate it solely for English.",1.4 Motivation and organisation of the paper,[0],[0]
It is also valuable to have high-quality resources for languages typologically different to English.,1.4 Motivation and organisation of the paper,[0],[0]
"Hence, we aim at building datasets for the evaluation of CDS models in languages other than English, which are often underresourced.",1.4 Motivation and organisation of the paper,[0],[0]
"We strongly believe that the availability of test data will encourage development of CDS models in these languages and allow to better test the universality of CDS methods.
",1.4 Motivation and organisation of the paper,[0],[0]
"We start with a high-quality dataset for Polish, which is a completely different language than English in at least two dimensions.",1.4 Motivation and organisation of the paper,[0],[0]
"First, it is a rather under-resourced language in contrast to the resource-rich English.",1.4 Motivation and organisation of the paper,[0],[0]
"Second, it is a fusional language with a relatively free word order in contrast to the isolated English with a relatively fixed word order.",1.4 Motivation and organisation of the paper,[0],[0]
"If some heuristics is tested on e.g. Polish, the evaluation results can be approximately generalised to other Slavic languages.",1.4 Motivation and organisation of the paper,[0],[0]
"We hope the Slavic NLP community will be interested in designing and evaluating methods of semantic modelling for Slavic languages.
",1.4 Motivation and organisation of the paper,[0],[0]
The procedure of building an evaluation dataset for validating compositional distributional semantics models of Polish generally builds on steps designed to assemble the SICK corpus (described in Section 1.3) because we aim at building an evaluation dataset which is comparable to the SICK corpus.,1.4 Motivation and organisation of the paper,[0],[0]
"However, the implementation of particular building steps significantly differs from the original SICK design assumptions, which is caused by both lack of necessary extraneous resources for Polish (see Section 2.1) and the need for Polish-specific transformation rules (see Section 2.2).",1.4 Motivation and organisation of the paper,[0],[0]
"Furthermore, the rules of arranging sentences into pairs (see Section 2.3) are defined anew taking into account the characteristic of data and bidirectional entailment annotations, since an entailment relation between two sentences must not be symmetric.",1.4 Motivation and organisation of the paper,[0],[0]
"Even if our assumptions of annotating sentence pairs coincide with the SICK principles to a certain extent (see Section 3.1), the annotation process differs from the SICK procedure, in particular by introducing an element of human verification of correctness of automatically transformed sentences (see Section 3.2) and some additional post-corrections (see Section 3.3).",1.4 Motivation and organisation of the paper,[0],[0]
"Finally, a summary of the dataset is provided in Section 4.1 and the dataset evaluation is given in Section 4.2.",1.4 Motivation and organisation of the paper,[0],[0]
"The first step of building the SICK corpus consisted in the random selection of English sentence pairs from existing datasets (Rashtchian et al., 2010; Agirre et al., 2012).",2.1 Selection and description of images,[0],[0]
"Since we are not aware of accessibility of analogous resources for Polish, we have to select images first and then describe the selected images.
",2.1 Selection and description of images,[0],[0]
"Images are selected from the 8K ImageFlickr
dataset (Rashtchian et al., 2010).",2.1 Selection and description of images,[0],[0]
At first we wanted to take only these images the descriptions of which were selected for the SICK corpus.,2.1 Selection and description of images,[0],[0]
"However, a cursory check shows that these images are quite homogeneous, with a predominant number of dogs depictions.",2.1 Selection and description of images,[0],[0]
"Therefore, we independently extract 1K images and split them into 46 thematic groups (e.g. children, musical instruments, motorbikes, football, dogs).",2.1 Selection and description of images,[0],[0]
The numbers of images within individual thematic groups vary from 6 images in the volleyball and telephoning groups to 94 images in the various people group.,2.1 Selection and description of images,[0],[0]
"The second largest groups are children and dogs with 50 images each.
",2.1 Selection and description of images,[0],[0]
The chosen images are given to two authors who independently of each other formulate their descriptions based on a short instruction.,2.1 Selection and description of images,[0],[0]
The authors are instructed to write one single sentence (with a sentence predicate) describing the action in a displayed image.,2.1 Selection and description of images,[0],[0]
They should not describe an imaginable context or an interpretation of what may lie behind the scene in the picture.,2.1 Selection and description of images,[0],[0]
"If some details in the picture are not obvious, they should not be described either.",2.1 Selection and description of images,[0],[0]
"Furthermore, the authors should avoid multiword expressions, such as idioms, metaphors, and named entities, because those are not compositional linguistic phenomena.",2.1 Selection and description of images,[0],[0]
"Finally, descriptions should contain Polish diacritics and proper punctuation.",2.1 Selection and description of images,[0],[0]
"The second step of building the SICK corpus consisted in pre-processing extracted sentences, i.e. normalisation and expansion (Bentivogli et al., 2014, p. 3–4).",2.2 Transformation of descriptions,[0],[0]
"Since the authors of Polish descriptions are asked to follow the guidelines (presented in Section 2.1), the normalisation step is not essential for our data.",2.2 Transformation of descriptions,[0],[0]
"The expansion step, in turn, is implemented and the sentences provided by the authors are lexically and syntactically transformed in order to obtain derivative sentences with similar, contrastive, or neutral meanings.",2.2 Transformation of descriptions,[0],[0]
"The following transformations are implemented:
1.",2.2 Transformation of descriptions,[0],[0]
"dropping conjunction concerns sentences with coordinated predicates sharing a subject, e.g. Rowerzysta odpoczywa i obserwuje morze.",2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
‘A cyclist is resting and watching the sea.’).,2.2 Transformation of descriptions,[0],[0]
"The finite form of one of the coordinated predicates is transformed into:
• an active adjectival participle, e.g. Odpoczywający rowerzysta obserwuje
morze.",2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
‘A resting cyclist is watching the sea.’) or Obserwujący morze rowerzysta odpoczywa.,2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
"‘A cyclist, who is watching the sea, is resting.’), • a contemporary adverbial participle,
e.g. Rowerzysta, odpoczywając, obserwuje morze.",2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
"‘A cyclist is watching the sea, while resting.’)",2.2 Transformation of descriptions,[0],[0]
"or Rowerzysta odpoczywa, obserwując morze.",2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
"‘A cyclist is resting, while watching the sea.’).
",2.2 Transformation of descriptions,[0],[0]
"2. removing conjunct in adjuncts, i.e. the deletion of one of coordinated elements of an adjunct, e.g. Mały, ale zwinny kot miauczy.",2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
‘A small but agile cat miaows.’) can be changed into either Mały kot miauczy.,2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
‘A small cat miaows.’) or Zwinny kot miauczy.,2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
"‘An agile cat miaows.’).
",2.2 Transformation of descriptions,[0],[0]
3.,2.2 Transformation of descriptions,[0],[0]
"passivisation, e.g. Człowiek ujeżdża byka.",2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
‘A man is breaking a bull in.’) can be transformed into Byk jest ujeżdżany przez człowieka.,2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
"‘A bull is being broken in by a man.’).
",2.2 Transformation of descriptions,[0],[0]
"4. removing adjuncts, e.g. Dwa białe króliki siedzą na trawie.",2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
‘Two small rabbits are sitting on the grass.’) can be changed into Króliki,2.2 Transformation of descriptions,[0],[0]
siedzą.,2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
"‘The rabbits are sitting.’).
",2.2 Transformation of descriptions,[0],[0]
"5. swapping relative clause for participles, i.e. a relative clause swaps with a participle (and vice versa), e.g. Kobieta przytula psa, którego trzyma na smyczy.",2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
‘A woman hugs a dog which she keeps on a leash.’).,2.2 Transformation of descriptions,[0],[0]
"The relative clause is interchanged for a participle construction, e.g. Kobieta przytula trzymanego na smyczy psa.",2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
"‘A woman hugs a dog kept on a leash.’).
6.",2.2 Transformation of descriptions,[0],[0]
"negation, e.g. Mężczyźni w turbanach na głowach siedzą na słoniach.",2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
‘Men in turbans on their heads are sitting on elephants.’) can be transformed into Nikt nie siedzi na słoniach.,2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
"‘Nobody is sitting on elephants.’), Żadni mężczyźni w turbanach na głowach nie siedzą na słoniach.",2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
"‘No men in turbans on their heads are sitting on elephants.’), and Mężczyźni w turbanach na głowach nie siedzą na słoniach.",2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
"‘Men in turbans on their heads are not sitting on elephants.’).
",2.2 Transformation of descriptions,[0],[0]
7.,2.2 Transformation of descriptions,[0],[0]
constrained mixing of dependents from various sentences,2.2 Transformation of descriptions,[0],[0]
", e.g. Dwoje dzieci siedzi na wielbłądach w pobliżu wysokich gór.",2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
‘Two children are sitting on camels near high mountains.’) can be changed into Dwoje dzieci siedzi przy zastawionym stole w pobliżu wysokich gór.,2.2 Transformation of descriptions,[0],[0]
(Eng.,2.2 Transformation of descriptions,[0],[0]
"‘Two children are sitting at the table laid with food near high mountains.’).
",2.2 Transformation of descriptions,[0],[0]
"The first five transformations are designed to produce sentences with a similar meaning, the sixth transformation outputs sentences with a contradictory meaning, and the seventh transformation should generate sentences with a neutral (or unrelated) meaning.",2.2 Transformation of descriptions,[0],[0]
"All transformations are performed on the dependency structures of input sentences (Wróblewska, 2014).
",2.2 Transformation of descriptions,[0],[0]
Some of the transformations are very productive (e.g. mixing dependents).,2.2 Transformation of descriptions,[0],[0]
"Other, in turn, are sparsely represented in the output (e.g. dropping conjunction).",2.2 Transformation of descriptions,[0],[0]
The number of transformed sentences randomly selected to build the dataset is in the second column of Table 1.,2.2 Transformation of descriptions,[0],[0]
The final step of building the SICK corpus consisted in arranging normalised and expanded sentences into pairs.,2.3 Data ensemble,[0],[0]
"Since our data diverges from SICK data, the process of arranging Polish sentences into pairs also differs from pairing in the SICK corpus.",2.3 Data ensemble,[0],[0]
The general idea behind the pair-ensembling procedure was to introduce sentence pairs with different levels of relatedness into the dataset.,2.3 Data ensemble,[0],[0]
"Apart from pairs connecting two sentences originally written by humans (as described in Section 2.1), there are also pairs in which an original sentence is connected with
a transformed sentence.",2.3 Data ensemble,[0],[0]
"For each of the 1K images, the following 10 pairs are constructed (for A being the set of all sentences originally written by the first author, B being the set of all sentences originally written by the second author, a ∈ A and b ∈ B being the original descriptions of the picture):
1.",2.3 Data ensemble,[0],[0]
"(a,b)
2.",2.3 Data ensemble,[0],[0]
"(a,a1), where a1 ∈ t(a), and t(a) is the set of all transformations of the sentence a
3.",2.3 Data ensemble,[0],[0]
"(b,b1), where b1 ∈ t(b)
4.",2.3 Data ensemble,[0],[0]
"(a,b2), where b2 ∈ t(b)
5.",2.3 Data ensemble,[0],[0]
"(b,a2), where a2 ∈ t(a)
6.",2.3 Data ensemble,[0],[0]
"(a,a3), where a3 ∈ t(a′),a′ ∈",2.3 Data ensemble,[0],[0]
"A, T (a′) = T (a),a′ 6= a, for T (a) being the thematic group3 of a
7.",2.3 Data ensemble,[0],[0]
"(b,b3), where b3 ∈ t(b′),b′ ∈ B, T (b′)",2.3 Data ensemble,[0],[0]
"= T (b),b′ 6= b
8.",2.3 Data ensemble,[0],[0]
"(a,a4), where a4 ∈ A, T (a4) 6=",2.3 Data ensemble,[0],[0]
"T (a)4
9.",2.3 Data ensemble,[0],[0]
"(b,b4), where b4 ∈ B, T (b4) 6=",2.3 Data ensemble,[0],[0]
"T (b)
10.",2.3 Data ensemble,[0],[0]
"(a,a5), where a5 ∈",2.3 Data ensemble,[0],[0]
"t(a),a5 6=",2.3 Data ensemble,[0],[0]
"a1 for 50% images, (b,b5) (analogously) for other 50%.5
For each sentence pair (a,b) created according to this procedure, its reverse (b,a) is also included in our corpus.",2.3 Data ensemble,[0],[0]
"As a result, the working set consists of 20K sentence pairs.",2.3 Data ensemble,[0],[0]
The degree of semantic relatedness between two sentences is calculated as the average of all human ratings on the Likert scale with the range from 0 to 5.,3.1 Annotation assumptions,[0],[0]
"Since we do not want to excessively influence
3The thematic group of a sentence a corresponds to the thematic group of an image being the source of a (as described in Section 2.1).
4The pairs (a,a4) of the same authors’ descriptions of two images from different thematic groups are expected to be unrelated.",3.1 Annotation assumptions,[0],[0]
"The same applies to (b,b4).
",3.1 Annotation assumptions,[0],[0]
5A repetition of point 2 with a restriction that a different pair is created (pairs of very related sentences are expected).,3.1 Annotation assumptions,[0],[0]
"We alternate between authors A and B to obtain equal author proportions in the final ensemble of pairs.
",3.1 Annotation assumptions,[0],[0]
"the annotations, the guidelines given to annotators are mainly example-based:6
• 5 (very related): Kot siedzi na płocie.",3.1 Annotation assumptions,[0],[0]
(Eng.,3.1 Annotation assumptions,[0],[0]
‘A cat is sitting on the fence.’),3.1 Annotation assumptions,[0],[0]
vs. Na płocie jest duży kot.,3.1 Annotation assumptions,[0],[0]
(Eng.,3.1 Annotation assumptions,[0],[0]
"‘There is a large cat on the fence.’),
• 1–4 (more or less related): Kot siedzi na płocie.",3.1 Annotation assumptions,[0],[0]
(Eng.,3.1 Annotation assumptions,[0],[0]
‘A cat is sitting on the fence.’),3.1 Annotation assumptions,[0],[0]
vs. Kot nie siedzi na płocie.,3.1 Annotation assumptions,[0],[0]
(Eng.,3.1 Annotation assumptions,[0],[0]
‘A cat is not sitting on the fence.’); Kot siedzi na płocie.,3.1 Annotation assumptions,[0],[0]
(Eng.,3.1 Annotation assumptions,[0],[0]
‘A cat is sitting on the fence.’),3.1 Annotation assumptions,[0],[0]
vs. Właściciel dał kotu chrupki.,3.1 Annotation assumptions,[0],[0]
(Eng.,3.1 Annotation assumptions,[0],[0]
‘The owner gave kibble to his cat.’),3.1 Annotation assumptions,[0],[0]
; Kot siedzi na płocie.,3.1 Annotation assumptions,[0],[0]
(Eng.,3.1 Annotation assumptions,[0],[0]
‘A cat is sitting on the fence.’),3.1 Annotation assumptions,[0],[0]
vs. Kot miauczy pod płotem.,3.1 Annotation assumptions,[0],[0]
(Eng.,3.1 Annotation assumptions,[0],[0]
"‘A cat miaows by the fence.’).
",3.1 Annotation assumptions,[0],[0]
• 0 (unrelated): Kot siedzi na płocie.,3.1 Annotation assumptions,[0],[0]
(Eng.,3.1 Annotation assumptions,[0],[0]
‘A cat is sitting on the fence.’),3.1 Annotation assumptions,[0],[0]
vs. Zaczął padać deszcz.,3.1 Annotation assumptions,[0],[0]
(Eng.,3.1 Annotation assumptions,[0],[0]
"‘It started to rain.’).
",3.1 Annotation assumptions,[0],[0]
"Apart from these examples, there is a note in the annotation guidelines indicating that the degree of semantic relatedness is not equivalent to the degree of semantic similarity.",3.1 Annotation assumptions,[0],[0]
"Semantic similarity is only a special case of semantic relatedness, semantic relatedness is thus a more general term than the other one.
",3.1 Annotation assumptions,[0],[0]
"Polish entailment labels correspond directly to the SICK labels (i.e. entailment, contradiction, neutral).",3.1 Annotation assumptions,[0],[0]
The entailment label assigned by the majority of human judges is selected as the gold label.,3.1 Annotation assumptions,[0],[0]
"The entailment labels are defined as follows:
• a wynika z b (b entails a) – if a situation or an event described by sentence b occurs, it is recognised that a situation or an event described by a occurs as well, i.e. a and b refer to the same event or the same situation,
• a jest zaprzeczeniem b (a is the negation of b) – if a situation or an event described by b occurs, it is recognised that a situation or an event described by a may not occur at the same time,
6We realise that the boundary between semantic perception of a sentence by various speakers is fuzzy (it depends on speakers’ education, origin, age, etc.).",3.1 Annotation assumptions,[0],[0]
"It was thus our wellthought-out decision to draw only general annotation frames and to enable annotators to rely on their feel for language.
",3.1 Annotation assumptions,[0],[0]
• a jest neutralne wobec b,3.1 Annotation assumptions,[0],[0]
(a is neutral to b) – the truth of a situation described by a cannot be determined on the basis of b.,3.1 Annotation assumptions,[0],[0]
"Similar to the SICK corpus, each Polish sentence pair is human-annotated for semantic relatedness and entailment by 3 human judges experienced in Polish linguistics.7 Since for each annotated pair (a,b), its reverse (b,a) is also subject to annotation, the entailment relation is in practice determined ‘in both directions’ for 10K sentence pairs.",3.2 Annotation procedure,[0],[0]
"For the task of relatedness annotation, the order of sentences within pairs seems to be irrelevant, we can thus assume to obtain 6 relatedness scores for 10K unique pairs.
",3.2 Annotation procedure,[0],[0]
"Since the transformation process is fully automatic and to a certain extent based on imperfect dependency parsing, we cannot ignore errors in the transformed sentences.",3.2 Annotation procedure,[0],[0]
"In order to avoid annotating erroneous sentences, the annotation process is divided into two stages:
1.",3.2 Annotation procedure,[0],[0]
"a sentence pair is sent to a judge with the leader role, who is expected to edit and to correct the transformed sentence from this pair before annotation, if necessary,
2.",3.2 Annotation procedure,[0],[0]
"the verified and possibly enhanced sentence pair is sent to the other two judges, who can only annotate it.
",3.2 Annotation procedure,[0],[0]
The leader judges should correct incomprehensible and ungrammatical sentences with a minimal number of necessary changes.,3.2 Annotation procedure,[0],[0]
Unusual sentences which could be accepted by Polish speakers should not be modified.,3.2 Annotation procedure,[0],[0]
"Moreover, the modified sentence may not be identical with the other sentence in the pair.",3.2 Annotation procedure,[0],[0]
"The classification and statistics of distinct corrections made by the leader judges are provided in Table 2.
",3.2 Annotation procedure,[0],[0]
A strict classification of error types is quite hard to provide because some sentences contain more than one error.,3.2 Annotation procedure,[0],[0]
We thus order the error types from the most serious errors (i.e. ‘sense’ errors) to the redundant corrections (i.e. ‘other’ type).,3.2 Annotation procedure,[0],[0]
"If a sentence contains several errors, it is qualified for the higher order error type.
",3.2 Annotation procedure,[0],[0]
"In the case of sentences with ‘sense’ errors, the need for correction is uncontroversial and
7Our annotators have relatively strong linguistic background.",3.2 Annotation procedure,[0],[0]
"Five of them have PhD in linguistics, five are PhD students, one is a graduate, and one is an undergraduate.
arises from an internal logical contradiction.8",3.2 Annotation procedure,[0],[0]
"The sentences with ‘semantic’ changes are syntactically correct, but deemed unacceptable by the leader annotators from the semantic or pragmatic point of view.9 The ‘grammatical’ errors mostly concern missing agreement.10 The majority of ‘word order’ corrections are unnecessary, but we found some examples which can be classified as actual word or phrase order errors.11 The correction of punctuation consists in adding or deleting a comma.12 The sentences in the ‘other’ group, in turn, could as well have been left unchanged because they are proper Polish sentences, but were apparently considered odd by the leader annotators.
",3.2 Annotation procedure,[0],[0]
8An example of ‘sense’ error: the sentence Chłopak w zielonej bluzie,3.2 Annotation procedure,[0],[0]
i czapce zjeżdża na rolkach na leżąco.,3.2 Annotation procedure,[0],[0]
(Eng.,3.2 Annotation procedure,[0],[0]
‘A boy in a green sweatshirt and a cap roller-skates downhill in a lying position.’) is corrected into Chłopak w zielonej bluzie,3.2 Annotation procedure,[0],[0]
i czapce zjeżdża na rolkach.,3.2 Annotation procedure,[0],[0]
(Eng.,3.2 Annotation procedure,[0],[0]
"‘A boy in a green sweatshirt and a cap roller-skates downhill.’).
",3.2 Annotation procedure,[0],[0]
9An example of ‘semantic’ correction: the sentence Dziewczyna trzyma w pysku patyk.,3.2 Annotation procedure,[0],[0]
(Eng.,3.2 Annotation procedure,[0],[0]
‘A girl holds a stick in her muzzle.’) is corrected into Dziewczyna trzyma w ustach patyk.,3.2 Annotation procedure,[0],[0]
(Eng.,3.2 Annotation procedure,[0],[0]
"‘A girl holds a stick in her mouth.’).
",3.2 Annotation procedure,[0],[0]
10An example of ‘grammatical’ error: the sentence Grupasg.nom uśmiechających się ludzi tańcząpl.,3.2 Annotation procedure,[0],[0]
(Eng.,3.2 Annotation procedure,[0],[0]
*‘A group of smiling people are dancing.’) is corrected into Grupasg.nom uśmiechających się ludzi tańczysg .,3.2 Annotation procedure,[0],[0]
(Eng.,3.2 Annotation procedure,[0],[0]
"‘A group of smiling people is dancing.’).
",3.2 Annotation procedure,[0],[0]
"11An example of word order error: the sentence Samochód, który jest uszkodzony, koloru białego stoi na lawecie dużego auta.",3.2 Annotation procedure,[0],[0]
(lit.,3.2 Annotation procedure,[0],[0]
"‘A car that is damaged, of the white color stands on the trailer of a large car.’, Eng.",3.2 Annotation procedure,[0],[0]
‘A white car that is damaged is standing on the trailer of a large car.’),3.2 Annotation procedure,[0],[0]
"is corrected into Samochód koloru białego, który jest uszkodzony, stoi na lawecie dużego auta.
12An example of punctuation correction: the wrong comma in the sentence Nad brzegiem wody, stoją dwaj mężczyźni z wędkami.",3.2 Annotation procedure,[0],[0]
(lit.,3.2 Annotation procedure,[0],[0]
"‘On the water’s edge, two men are standing with rods.’; Eng.",3.2 Annotation procedure,[0],[0]
"‘Two men with rods are standing on the water’s edge.’) should be deleted, i.e. Nad brzegiem wody stoją dwaj mężczyźni z wędkami.",3.2 Annotation procedure,[0],[0]
During the annotation process it came out that sentences accepted by some human annotators are unacceptable for other annotators.,3.3 Impromptu post-corrections,[0],[0]
We thus decided to garner annotators’ comments and suggestions for improving sentences.,3.3 Impromptu post-corrections,[0],[0]
"After validation of these suggestions by an experienced linguist, it turns out that most of these proposals concern punctuation errors (e.g. missing comma) and typos in 312 distinct sentences.",3.3 Impromptu post-corrections,[0],[0]
These errors are fixed directly in the corpus because they should not impact the annotations of sentence pairs.,3.3 Impromptu post-corrections,[0],[0]
The other suggestions concern more significant changes in 29 distinct sentences (mostly minor grammatical or semantic problems overlooked by the leader annotators).,3.3 Impromptu post-corrections,[0],[0]
The annotations of pairs with modified sentences are resent to the annotators so that they can verify and update them.,3.3 Impromptu post-corrections,[0],[0]
Tables 3 and 4 summarise the annotations of the resulting 10K sentence pairs corpus.,4.1 Corpus statistics,[0],[0]
"Table 3 aggregates the occurrences of 6 possible relatedness scores, calculated as the mean of all 6 individual annotations, rounded to an integer.
",4.1 Corpus statistics,[0],[0]
Table 4 shows the number of the particular entailment labels in the corpus.,4.1 Corpus statistics,[0],[0]
"Since each sentence pair is annotated for entailment in both directions, the final entailment label is actually a pair of two labels:
• entailment+neutral points to ‘one-way’ entailment,
• contradiction+neutral points to ‘one-way’ contradiction,
• entailment+entailment, contradiction+contradiction, and neutral+neutral point to equivalence.
",4.1 Corpus statistics,[0],[0]
"While the actual corpus labels are ordered in the sense that there is a difference between e.g. entailment+neutral and neutral+entailment (the entailment occurs in different directions), we treat all labels as unordered for the purpose of this summary (e.g. entailment+neutral covers neutral+entailment as well, representing the same type of relation between two sentences).",4.1 Corpus statistics,[0],[0]
"The standard measure of inter-annotator agreement in various natural language labelling tasks is Cohen’s kappa (Cohen, 1960).",4.2 Inter-annotator agreement,[0],[0]
"However, this coefficient is designed to measure agreement between two annotators only.",4.2 Inter-annotator agreement,[0],[0]
"Since there are three annotators of each pair of ordered sentences, we decided to apply Fleiss’ kappa13 (Fleiss, 1971) designed for measuring agreement between multiple raters who give categorical ratings to a fixed number of items.",4.2 Inter-annotator agreement,[0],[0]
"An additional advantage of this measure is that different items can be rated by different human judges, which doesn’t impact measurement.",4.2 Inter-annotator agreement,[0],[0]
"The normalised Fleiss’ measure of inter-annotator agreement is:
κ = P̄",4.2 Inter-annotator agreement,[0],[0]
"− P̄e 1− P̄e
where the quantity P̄ − P̄e measures the degree of agreement actually attained in excess of chance, while “[t]he quantity 1 − P̄e measures the degree of agreement attainable over and above what would be predicted by chance” (Fleiss, 1971, p. 379).
",4.2 Inter-annotator agreement,[0],[0]
We recognise Fleiss’ kappa as particularly useful for measuring inter-annotator agreement with respect to entailment labelling in our evaluation dataset.,4.2 Inter-annotator agreement,[0],[0]
"First, there are more than two raters.",4.2 Inter-annotator agreement,[0],[0]
"Second, entailment labels are categorically.",4.2 Inter-annotator agreement,[0],[0]
"Measured
13As Fleiss’ kappa is actually the generalisation of Scott’s π (Scott, 1955), it is sometimes referred to as Fleiss’ multi-π, cf.",4.2 Inter-annotator agreement,[0],[0]
"Artstein and Poesio (2008).
with Fleiss’ kappa, there is an inter-annotator agreement of κ = 0.734 for entailment labels in Polish evaluation dataset, which is quite satisfactory as for a semantic labelling task.
",4.2 Inter-annotator agreement,[0],[0]
"Relative to semantic relatedness, the distinction in meaning of two sentences made by human judges is often very subtle.",4.2 Inter-annotator agreement,[0],[0]
This is also reflected in the inter-annotator agreement scores measured with Fleiss’ kappa.,4.2 Inter-annotator agreement,[0],[0]
Inter-annotator agreement measured for six semantic relatedness groups corresponding to points on the Likert scale is quite low: κ = 0.337.,4.2 Inter-annotator agreement,[0],[0]
"If we measure interannotator agreement for three classes corresponding to the three relatedness groups from the annotation guidelines (see Section 3.1), i.e. <0>, <1, 2, 3, 4>, and <5>, the Fleiss’ score is significantly higher: κ = 0.543.",4.2 Inter-annotator agreement,[0],[0]
"Hence, we conclude that Fleiss’ kappa is not a reliable measure of inter-annotator agreement in relation to relatedness scores.",4.2 Inter-annotator agreement,[0],[0]
"Therefore, we decided to use Krippendorff’s α instead.
",4.2 Inter-annotator agreement,[0],[0]
"Krippendorff’s α (Krippendorff, 1980, 2013) is a coefficient appropriate for measuring the interannotator agreement of a dataset which is annotated with multiple judges and characterised by different magnitudes of disagreement and missing values.",4.2 Inter-annotator agreement,[0],[0]
"Krippendorff proposes distance metrics suitable for various scales: binary, nominal, interval, ordinal, and ratio.",4.2 Inter-annotator agreement,[0],[0]
"In ordinal measurement14 the attributes can be rank-ordered, but distances between them do not have any meaning.",4.2 Inter-annotator agreement,[0],[0]
"Measured with Krippendorff’s ordinal α, there is an inter-annotator agreement of α = 0.780 for relatedness scores in the Polish evaluation dataset, which is quite satisfactory as well.",4.2 Inter-annotator agreement,[0],[0]
"Hence, we conclude that our dataset is a reliable resource for the purpose of evaluating compositional distributional semantics model of Polish.",4.2 Inter-annotator agreement,[0],[0]
The goal of this paper is to present the procedure of building a Polish evaluation dataset for the validation of compositional distributional semantics models.,5 Conclusions,[0],[0]
"As we aim at building an evalua-
14Nominal measurement is useless for measuring agreement between relatedness scores (α = 0.340 is the identical value as Fleiss’ kappa, since all disagreements are considered equal).",5 Conclusions,[0],[0]
"We also test interval measurement, in which the distance between the attributes does have meaning and an average of an interval variable is computed.",5 Conclusions,[0],[0]
"The interval score measured for relatedness annotations is quite high α = 0.785, but we doubt whether the distance between relatedness scores is meaningful in this case.
tion dataset which is comparable to the SICK corpus, the general assumptions of our procedure correspond to the design principles of the SICK corpus.",5 Conclusions,[0],[0]
"However, the procedure of building the SICK corpus cannot be adapted without modifications.",5 Conclusions,[0],[0]
"First, the Polish seed-sentences have to be written based on the images which are selected from 8K ImageFlickr dataset and split into thematic groups, since usable datasets are not publicly available.",5 Conclusions,[0],[0]
"Second, since the process of transforming sentences seems to be language-specific, the linguistic transformation rules appropriate for Polish have to be defined from scratch.",5 Conclusions,[0],[0]
"Third, the process of arranging Polish sentences into pairs is defined anew taking into account the data characteristic and bidirectional entailment annotations.",5 Conclusions,[0],[0]
The discrepancies relative to the SICK procedure also concern the annotation process itself.,5 Conclusions,[0],[0]
"Since an entailment relation between two sentences must not be symmetric, each sentence pair is annotated for entailment in both directions.",5 Conclusions,[0],[0]
"Furthermore, we introduce an element of human verification of correctness of automatically transformed sentences and some additional post-corrections.
",5 Conclusions,[0],[0]
The presented procedure of building a dataset was tested on Polish.,5 Conclusions,[0],[0]
"However, it is very likely that the annotation framework will work for other Slavic languages (e.g. Czech with an excellent dependency parser).
",5 Conclusions,[0],[0]
"The presented procedure results in building the Polish test corpus of relatively high quality, confirmed by the inter-annotator agreement coefficients of κ = 0.734 (measured with Fleiss’ kappa) for entailment labels and of α = 0.780 (measured with Krippendorff’s ordinal alpha) for relatedness scores.",5 Conclusions,[0],[0]
"We would like to thank the reliable and tenacious annotators of our dataset: Alicja DziedzicRawska, Bożena Itoya, Magdalena Król, Anna Latusek, Justyna Małek, Małgorzata Michalik, Agnieszka Norwa, Małgorzata Szajbel-Keck, Alicja Walichnowska, Konrad Zieliński, and some other.",Acknowledgments,[0],[0]
The research presented in this paper was supported by SONATA 8 grant no 2014/15/D/HS2/03486 from the National Science Centre Poland.,Acknowledgments,[0],[0]
The paper presents a procedure of building an evaluation dataset1.,abstractText,[0],[0]
for the validation of compositional distributional semantics models estimated for languages other than English.,abstractText,[0],[0]
"The procedure generally builds on steps designed to assemble the SICK corpus, which contains pairs of English sentences annotated for semantic relatedness and entailment, because we aim at building a comparable dataset.",abstractText,[0],[0]
"However, the implementation of particular building steps significantly differs from the original SICK design assumptions, which is caused by both lack of necessary extraneous resources for an investigated language and the need for language-specific transformation rules.",abstractText,[0],[0]
"The designed procedure is verified on Polish, a fusional language with a relatively free word order, and contributes to building a Polish evaluation dataset.",abstractText,[0],[0]
The resource consists of 10K sentence pairs which are human-annotated for semantic relatedness and entailment.,abstractText,[0],[0]
The dataset may be used for the evaluation of compositional distributional semantics models of Polish.,abstractText,[0],[0]
Polish evaluation dataset for compositional distributional semantics models,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 667–672 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
667",text,[0],[0]
"The standard approach to multilingual NLP is to design a single architecture, but tune and train a separate model for each language.",1 Introduction,[0],[0]
"While this method allows for customizing the model to the particulars of each language and the available data, it also presents a problem when little data is available: extensive language-specific annotation is required.",1 Introduction,[0],[0]
"The reality is that most languages have very little annotated data for most NLP tasks.
",1 Introduction,[0],[0]
"Ammar et al. (2016a) found that using training data from multiple languages annotated with Universal Dependencies (Nivre et al., 2016), and represented using multilingual word vectors, outperformed monolingual training.",1 Introduction,[0],[0]
"Inspired by this, we apply the idea of training one model on multiple languages—which we call polyglot training— to PropBank-style semantic role labeling (SRL).",1 Introduction,[0],[0]
"We train several parsers for each language in the CoNLL 2009 dataset (Hajič et al., 2009): a tra-
I think Peter even made some deals with the gorillas .",1 Introduction,[0],[0]
O,1 Introduction,[0],[0]
O A0 AM-ADV O,1 Introduction,[0],[0]
"O A1 AM-ADV O O
Pero el suizo difícilmente atacará a Rominger en la montaña .",1 Introduction,[0],[0]
O,1 Introduction,[0],[0]
"O arg0-agt argM-adv O O arg1-pat argM-loc O O
Četrans oslovil sedm velkých evropských výrobců nákladních automobilů.",1 Introduction,[0],[0]
O O RSTR RSTR RSTR,1 Introduction,[0],[0]
"O O PAT
Figure 1:",1 Introduction,[0],[0]
"Example predicate-argument structures from English, Spanish, and Czech.",1 Introduction,[0],[0]
"Note that the argument labels are different in each language.
ditional monolingual version, and variants which additionally incorporate supervision from English portion of the dataset.",1 Introduction,[0],[0]
"To our knowledge, this is the first multilingual SRL approach to combine supervision from several languages.
",1 Introduction,[0],[0]
"The CoNLL 2009 dataset includes seven different languages, allowing study of trends across the same.",1 Introduction,[0],[0]
"Unlike the Universal Dependencies dataset, however, the semantic label spaces are entirely language-specific, making our task more challenging.",1 Introduction,[0],[0]
"Nonetheless, the success of polyglot training in this setting demonstrates that sharing of statistical strength across languages does not depend on explicit alignment in annotation conventions, and can be done simply through parameter sharing.",1 Introduction,[0],[0]
"We show that polyglot training can result in better labeling accuracy than a monolingual parser, especially for low-resource languages.",1 Introduction,[0],[0]
We find that even a simple combination of data is as effective as more complex kinds of polyglot training.,1 Introduction,[0],[0]
We include a breakdown into label categories of the differences between the monolingual and polyglot models.,1 Introduction,[0],[0]
Our findings indicate that polyglot training consistently improves label accuracy for common labels.,1 Introduction,[0],[0]
"We evaluate our system on the semantic role labeling portion of the CoNLL-2009 shared task (Hajič et al., 2009), on all seven languages, namely Catalan, Chinese, Czech, English, German, Japanese and Spanish.",2 Data,[0],[0]
"For each language, certain tokens in each sentence in the dataset are marked as predicates.",2 Data,[0],[0]
"Each predicate takes as arguments other words in the same sentence, their relationship marked by labeled dependency arcs.",2 Data,[0],[0]
"Sentences may contain no predicates.
",2 Data,[0],[0]
"Despite the consistency of this format, there are significant differences between the training sets across languages.1 English uses PropBank role labels (Palmer et al., 2005).",2 Data,[0],[0]
"Catalan, Chinese, English, German, and Spanish include (but are not limited to) labels such as “arg0-agt” (for “agent”) or “A0” that may correspond to some degree to each other and to the English roles.",2 Data,[0],[0]
"Catalan and Spanish share most labels (being drawn from the same source corpus, AnCora; Taulé et al., 2008), and English and German share some labels.",2 Data,[0],[0]
"Czech and Japanese each have their own distinct sets of argument labels, most of which do not have clear correspondences to English or to each other.
",2 Data,[0],[0]
"We also note that, due to semi-automatic projection of annotations to construct the German dataset, more than half of German sentences do not include labeled predicate and arguments.",2 Data,[0],[0]
"Thus while German has almost as many sentences as Czech, it has by far the fewest training examples (predicate-argument structures); see Table 1.
",2 Data,[0],[0]
"1This is expected, as the datasets were annotated independently under diverse formalisms and only later converted into CoNLL format (Hajič et al., 2009).",2 Data,[0],[0]
"Given a sentence with a marked predicate, the CoNLL 2009 shared task requires disambiguation of the sense of the predicate, and labeling all its dependent arguments.",3 Model,[0],[0]
"The shared task assumed predicates have already been identified, hence we do not handle the predicate identification task.
",3 Model,[0],[0]
Our basic model adapts the span-based dependency SRL model of He et al. (2017).,3 Model,[0],[0]
This adaptation treats the dependent arguments as argument spans of length 1.,3 Model,[0],[0]
"Additionally, BIO consistency constraints are removed from the original model— each token is tagged simply with the argument label or an empty tag.",3 Model,[0],[0]
"A similar approach has also been proposed by Marcheggiani et al. (2017).
",3 Model,[0],[0]
The input to the model consists of a sequence of pretrained embeddings for the surface forms of the sentence tokens.,3 Model,[0],[0]
Each token embedding is also concatenated with a vector indicating whether the word is a predicate or not.,3 Model,[0],[0]
"Since the part-ofspeech tags in the CoNLL 2009 dataset are based on a different tagset for each language, we do not use these.",3 Model,[0],[0]
Each training instance consists of the annotations for a single predicate.,3 Model,[0],[0]
"These representations are then passed through a deep, multilayer bidirectional LSTM (Graves, 2013; Hochreiter and Schmidhuber, 1997) with highway connections (Srivastava et al., 2015).
",3 Model,[0],[0]
"We use the hidden representations produced by the deep biLSTM for both argument labeling and predicate sense disambiguation in a multitask setup; this is a modification to the models of He et al. (2017), who did not handle predicate senses, and of Marcheggiani et al. (2017), who used a separate model.",3 Model,[0],[0]
"These two predictions are made independently, with separate softmaxes over different last-layer parameters; we then combine the losses for each task when training.",3 Model,[0],[0]
"For predicate sense disambiguation, since the predicate has been identified, we choose from a small set of valid predicate senses as the tag for that token.",3 Model,[0],[0]
This set of possible senses is selected based on the training data: we map from lemmatized tokens to predicates and from predicates to the set of all senses of that predicate.,3 Model,[0],[0]
"Most predicates are only observed to have one or two corresponding senses, making the set of available senses at test time quite small (less than five senses/predicate on average across all languages).",3 Model,[0],[0]
"If a particular lemma was not observed in training, we heuristically predict it as the first sense of that predicate.",3 Model,[0],[0]
"For Czech and
Japanese, the predicate sense annotation is simply the lemmatized token of the predicate, giving a one-to-one predicate-“sense” mapping.
",3 Model,[0],[0]
"For argument labeling, every token in the sentence is assigned one of the argument labels, or NULL if the model predicts it is not an argument to the indicated predicate.",3 Model,[0],[0]
We use pretrained word embeddings as input to the model.,3.1 Monolingual Baseline,[0],[0]
"For each of the shared task languages, we produced GloVe vectors (Pennington et al., 2014) from the news, web, and Wikipedia text of the Leipzig Corpora Collection (Goldhahn et al., 2012).2 We trained 300-dimensional vectors, then reduced them to 100 dimensions with principal component analysis for efficiency.",3.1 Monolingual Baseline,[0],[0]
"In the first polyglot variant, we consider multilingual sharing between each language and English by using pretrained multilingual embeddings.",3.2 Simple Polyglot Sharing,[0],[0]
This polyglot model is trained on the union of annotations in the two languages.,3.2 Simple Polyglot Sharing,[0],[0]
"We use stratified sampling to give the two datasets equal effective weight in training, and we ensure that every training instance is seen at least once per epoch.
",3.2 Simple Polyglot Sharing,[0],[0]
Pretrained multilingual embeddings.,3.2 Simple Polyglot Sharing,[0],[0]
"The basis of our polyglot training is the use of pretrained multilingual word vectors, which allow representing entirely distinct vocabularies (such as the tokens of different languages) in a shared representation space, allowing crosslingual learning (Klementiev et al., 2012).",3.2 Simple Polyglot Sharing,[0],[0]
"We produced multilingual embeddings from the monolingual embeddings using the method of Ammar et al. (2016b): for each non-English language, a small crosslingual dictionary and canonical correlation analysis was used to find a transformation of the non-English vectors into the English vector space (Faruqui and Dyer, 2014).
",3.2 Simple Polyglot Sharing,[0],[0]
"Unlike multilingual word representations, argument label sets are disjoint between language pairs, and correspondences are not clearly defined.",3.2 Simple Polyglot Sharing,[0],[0]
"Hence, we use separate label representations for each language’s labels.",3.2 Simple Polyglot Sharing,[0],[0]
"Similarly, while (for example) ENG:look and SPA:mira may be semantically connected, the senses look.01 and
2For English we used the vectors provided on the GloVe website nlp.stanford.edu/projects/glove/.
mira.01 may not correspond.",3.2 Simple Polyglot Sharing,[0],[0]
"Hence, predicate sense representations are also language-specific.",3.2 Simple Polyglot Sharing,[0],[0]
"In the second variant, we concatenate a language ID vector to each multilingual word embedding and predicate indicator feature in the input representation.",3.3 Language Identification,[0],[0]
This vector is randomly initialized and updated in training.,3.3 Language Identification,[0],[0]
"These additional parameters provide a small degree of language-specificity in the model, while still sharing most parameters.",3.3 Language Identification,[0],[0]
This third variant takes inspiration from the “frustratingly easy” architecture of Daume III (2007) for domain adaptation.,3.4 Language-Specific LSTMs,[0],[0]
"In addition to processing every example with a shared biLSTM as in previous models, we add language-specific biLSTMs that are trained only on the examples belonging to one language.",3.4 Language-Specific LSTMs,[0],[0]
"Each of these languagespecific biLSTMs is two layers deep, and is combined with the shared biSLTM in the input to the third layer.",3.4 Language-Specific LSTMs,[0],[0]
This adds a greater degree of languagespecific processing while still sharing representations across languages.,3.4 Language-Specific LSTMs,[0],[0]
It also uses the language identification vector and multilingual word vectors in the input.,3.4 Language-Specific LSTMs,[0],[0]
We present our results in Table 2.,4 Experiments,[0],[0]
"We observe that simple polyglot training improves over monolingual training, with the exception of Czech, where we observe no change in performance.",4 Experiments,[0],[0]
"The languages with the fewest training examples (German, Japanese, Catalan) show the most improvement, while large-dataset languages such as Czech or Chinese see little or no improvement (Figure 2).
",4 Experiments,[0],[0]
"The language ID model performs inconsistently; it is better than the simple polyglot model in some cases, including Czech, but not in all.",4 Experiments,[0],[0]
"The language-specific LSTMs model performs best on a few languages, such as Catalan and Chinese, but worst on others.",4 Experiments,[0],[0]
"While these results may reflect differences between languages in the optimal amount of crosslingual sharing, we focus on the simple polyglot results in our analysis, which sufficiently demonstrate that polyglot training can improve performance over monolingual training.
",4 Experiments,[0],[0]
"We also report performance of state-of-the-art systems in each of these languages, all of which make explicit use of syntactic features, Marcheg-
giani et al. (2017) excepted.",4 Experiments,[0],[0]
"While this results in better performance on many languages, our model has the advantage of not relying on a syntactic parser, and is hence more applicable to languages with lower resources.",4 Experiments,[0],[0]
"However, the results suggest that syntactic information is critical for strong performance on German, which has the fewest predicates and thus the least semantic annotation for a semantics-only model to learn from.",4 Experiments,[0],[0]
"Nevertheless, our baseline is on par with the best published scores for Chinese, and it shows strong performance on most languages.
",4 Experiments,[0],[0]
Label-wise results.,4 Experiments,[0],[0]
"Table 3 gives the F1 scores for individual label categories in the Catalan and Spanish datasets, as an illustration of the larger trend.",4 Experiments,[0],[0]
"In both languages, we find a small but consistent improvement in the most common label categories (e.g., arg1 and argM ).",4 Experiments,[0],[0]
"Less common label categories are sensitive to small changes in performance; they have the largest changes in F1 in absolute value, but without a consistent direction.",4 Experiments,[0],[0]
"This could be attributed to the addition of English data, which improves learning of representations that are useful for the most common labels, but is essentially a random perturbation for the rarer ones.",4 Experiments,[0],[0]
"This pattern is seen across languages, and consistently results in overall gains from polyglot training.
",4 Experiments,[0],[0]
"One exception is in Czech, where polyglot training reduces accuracy on several common argument labels, e.g., PAT and LOC.",4 Experiments,[0],[0]
"While the effect sizes are small (consistent with other languages), the overall F1 score on Czech decreases slightly in the polyglot condition.",4 Experiments,[0],[0]
"It may be that the Czech dataset is too large to make use of the comparatively small amount of English data, or that differences in the annotation schemes prevent
effective crosslingual transfer.",4 Experiments,[0],[0]
Future work on language pairs that do not include English could provide further insights.,4 Experiments,[0],[0]
"Catalan and Spanish, for example, are closely related and use the same argument label set (both being drawn from the AnCora corpus) which would allow for sharing output representations as well as input tokens and parameters.
",4 Experiments,[0],[0]
Polyglot English results.,4 Experiments,[0],[0]
"For each language pair, we also evaluated the simple polyglot model on the English test set from the CoNLL 2009 shared task (Table 4).",4 Experiments,[0],[0]
"English SRL consistently benefits from polyglot training, with an increase of 0.25–0.7 absolute F1 points, depending on the language.",4 Experiments,[0],[0]
"Surprisingly, Czech provides the smallest improvement, despite the large amount of data added; the absence of crosslingual transfer in both directions for the English-Czech case, breaking the pattern seen in other languages, could therefore be due to differences in annotation rather than questions of dataset size.
",4 Experiments,[0],[0]
Labeled vs. unlabeled F1.,4 Experiments,[0],[0]
Table 5 provides unlabeled F1 scores for each language pair.,4 Experiments,[0],[0]
"As can be seen here, the unlabeled F1 improvements are generally positive but small, indicating that polyglot training can help both in structure prediction and labeling of arguments.",4 Experiments,[0],[0]
"The pattern of seeing the largest improvements on the languages with the smallest datasets generally holds here: the largest F1 gains are in German and Catalan, followed by Japanese, with minimal or no improvement elsewhere.",4 Experiments,[0],[0]
Recent improvements in multilingual SRL can be attributed to neural architectures.,5 Related Work,[0],[0]
"Swayamdipta et al. (2016) present a transition-based stack LSTM model that predicts syntax and semantics jointly, as a remedy to the reliance on pipelined models.",5 Related Work,[0],[0]
Guo et al. (2016) and Roth and Lapata (2016) use deep biLSTM architectures which use syntactic information to guide the composition.,5 Related Work,[0],[0]
"Marcheggiani et al. (2017) use a simple LSTM model over word tokens to tag semantic dependencies, like our model.",5 Related Work,[0],[0]
"Their model predicts a token’s label based on the combination of the token vector and the predicate vector, and saw benefits from using POS tags, both improvements that could be added to our model.",5 Related Work,[0],[0]
"Marcheggiani and Titov (2017) apply the recently-developed graph
convolutional networks to SRL, obtaining state of the art results on English and Chinese.",5 Related Work,[0],[0]
"All of these approaches are orthogonal to ours, and might benefit from polyglot training.
",5 Related Work,[0],[0]
Other polyglot models have been proposed for semantics.,5 Related Work,[0],[0]
Richardson et al. (2018) train on multiple (natural language)-(programming language) pairs to improve a model that translates API text into code signature representations.,5 Related Work,[0],[0]
"Duong et al. (2017) treat English and German semantic parsing as a multi-task learning problem and saw improvement over monolingual baselines, especially for small datasets.",5 Related Work,[0],[0]
"Most relevant to our work is Johannsen et al. (2015), which trains a polyglot
model for frame-semantic parsing.",5 Related Work,[0],[0]
"In addition to sharing features with multilingual word vectors, they use them to find word translations of target language words for additional lexical features.",5 Related Work,[0],[0]
"In this work, we have explored a straightforward method for polyglot training in SRL: use multilingual word vectors and combine training data across languages.",6 Conclusion,[0],[0]
"This allows sharing without crosslingual alignments, shared annotation, or parallel data.",6 Conclusion,[0],[0]
"We demonstrate that a polyglot model can outperform a monolingual one for semantic analysis, particularly for languages with less data.",6 Conclusion,[0],[0]
"We thank Luke Zettlemoyer, Luheng He, and the anonymous reviewers for helpful comments and feedback.",Acknowledgments,[0],[0]
This research was supported in part by the Defense Advanced Research Projects Agency (DARPA) Information Innovation Office (I2O) under the Low Resource Languages for Emergent Incidents (LORELEI) program issued by DARPA/I2O under contract HR001115C0113 to BBN.,Acknowledgments,[0],[0]
Views expressed are those of the authors alone.,Acknowledgments,[0],[0]
"Previous approaches to multilingual semantic dependency parsing treat languages independently, without exploiting the similarities between semantic structures across languages.",abstractText,[0],[0]
"We experiment with a new approach where we combine resources from a pair of languages in the CoNLL 2009 shared task (Hajič et al., 2009) to build a polyglot semantic role labeler.",abstractText,[0],[0]
"Notwithstanding the absence of parallel data, and the dissimilarity in annotations between languages, our approach results in an improvement in SRL performance on multiple languages over a monolingual baseline.",abstractText,[0],[0]
Analysis of the polyglot model shows it to be advantageous in lower-resource settings.,abstractText,[0],[0]
Polyglot Semantic Role Labeling,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2183–2192, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics
Language resources that systematically organize paraphrases for binary relations are of great value for various NLP tasks and have recently been advanced in projects like PATTY, WiseNet and DEFIE. This paper presents a new method for building such a resource and the resource itself, called POLY. Starting with a very large collection of multilingual sentences parsed into triples of phrases, our method clusters relational phrases using probabilistic measures. We judiciously leverage fine-grained semantic typing of relational arguments for identifying synonymous phrases. The evaluation of POLY shows significant improvements in precision and recall over the prior works on PATTY and DEFIE. An extrinsic use case demonstrates the benefits of POLY for question answering.",text,[0],[0]
Motivation.,1 Introduction,[0],[0]
Information extraction from text typically yields relational triples: a binary relation along with its two arguments.,1 Introduction,[0],[0]
"Often the relation is expressed by a verb phrase, and the two arguments are named entities.",1 Introduction,[0],[0]
We refer to the surface form of the relation in a triple as a relational phrase.,1 Introduction,[0],[0]
"Repositories of relational phrases are an asset for a variety of tasks, including information extraction, textual entailment, and question answering.
",1 Introduction,[0],[0]
This paper presents a new method for systematically organizing a large set of such phrases.,1 Introduction,[0],[0]
"We aim to construct equivalence classes of synonymous phrases, analogously to how WordNet organizes
unary predicates as noun-centric synsets (aka. semantic types).",1 Introduction,[0],[0]
"For example, the following relational phrases should be in the same equivalence class: sings in, is vocalist in, voice in denoting a relation between a musician and a song.
",1 Introduction,[0],[0]
State of the Art and its Limitations.,1 Introduction,[0],[0]
"Starting with the seminal work on DIRT (Lin and Pantel, 2001), there have been various attempts on building comprehensive resources for relational phrases.",1 Introduction,[0],[0]
"Recent works include PATTY (Nakashole et al., 2012), WiseNet (Moro and Navigli, 2012) and DEFIE (Bovi et al., 2015).",1 Introduction,[0],[0]
Out of these DEFIE is the cleanest resource.,1 Introduction,[0],[0]
"However, the equivalence classes tend to be small, prioritizing precision over recall.",1 Introduction,[0],[0]
"On the other hand, PPDB (Ganitkevitch et al., 2013) offers the largest repository of paraphrases.",1 Introduction,[0],[0]
"However, the paraphrases are not relation-centric and they are not semantically typed.",1 Introduction,[0],[0]
"So it misses out on the opportunity of using types to distinguish identical phrases with different semantics, for example, performance in with argument types musician and song versus performance in with types athlete and competition.
",1 Introduction,[0],[0]
Our Approach.,1 Introduction,[0],[0]
"We start with a large collection of relational triples, obtained by shallow information extraction.",1 Introduction,[0],[0]
"Specifically, we use the collection of Faruqui and Kumar (2015), obtained by combining the OLLIE tool with Google Translate and projecting multilingual sentences back to English.",1 Introduction,[0],[0]
"Note that the task addressed in that work is relational triple extraction, which is orthogonal to our problem of organizing the relational phrases in these triples into synonymy sets.
",1 Introduction,[0],[0]
"We canonicalize the subject and object arguments
2183
of triples by applying named entity disambiguation and word sense disambiguation wherever possible.",1 Introduction,[0],[0]
"Using a knowledge base of entity types, we can then infer prevalent type signatures for relational phrases.",1 Introduction,[0],[0]
"Finally, based on a suite of judiciously devised probabilistic distance measures, we cluster phrases in a type-compatible way using a graph-cut technique.",1 Introduction,[0],[0]
The resulting repository contains ca.,1 Introduction,[0],[0]
"1 Million relational phrases, organized into ca.",1 Introduction,[0],[0]
"160,000 clusters.
",1 Introduction,[0],[0]
Contribution.,1 Introduction,[0],[0]
"Our salient contributions are: i) a novel method for constructing a large repository of relational phrases, based on judicious clustering and type filtering; ii) a new linguistic resource, coined POLY, of relational phrases with semantic typing, organized in equivalence classes; iii) an intrinsic evaluation of POLY, demonstrating its high quality in comparison to PATTY and DEFIE; iv) an extrinsic evaluation of POLY, demonstrating its benefits for question answering.",1 Introduction,[0],[0]
The POLY resource is publicly available 1.,1 Introduction,[0],[0]
Our approach consists of two stages: relational phrase typing and relational phrase clustering.,2 Method Overview,[0],[0]
"In Section 3, we explain how we infer semantic types of the arguments of a relational phrase.",2 Method Overview,[0],[0]
"In Section 4, we present the model for computing synonyms of relational phrases (i.e., paraphrases) and organizing them into clusters.
",2 Method Overview,[0],[0]
A major asset for our approach is a large corpus of multilingual sentences from the work of Faruqui and Kumar (2015).,2 Method Overview,[0],[0]
That dataset contains sentences from Wikipedia articles in many languages.,2 Method Overview,[0],[0]
"Each sentence has been processed by an Open Information Extraction method (Banko et al., 2007), specifically the OLLIE tool (Mausam et al., 2012), which produces a triple of surface phrases that correspond to a relational phrase candidate and its two arguments (subject and object).",2 Method Overview,[0],[0]
"Each non-English sentence has been translated into English using Google Translate, thus leveraging the rich statistics that Google has obtained from all kinds of parallel multilingual texts.",2 Method Overview,[0],[0]
"Altogether, the data from Faruqui and Kumar (2015) provides 135 million triples in 61 languages and in English (from the translations of the corresponding sentences).",2 Method Overview,[0],[0]
"This is the noisy input to our
1www.mpi-inf.mpg.de/yago-naga/poly/
method.",2 Method Overview,[0],[0]
"Figure 1 shows two Spanish sentences, the extracted triples of Spanish phrases, the sentences’ translations to English, and the extracted triples of English phrases.
",2 Method Overview,[0],[0]
"The figure shows that identical phrases in the foreign language - “fue filmado por” - may be translated into different English phrases: “was shot by” vs. “was filmed by”, depending on the context in the respective sentences.",2 Method Overview,[0],[0]
This is the main insight that our method builds on.,2 Method Overview,[0],[0]
The two resulting English phrases have a certain likelihood of being paraphrases of the same relation.,2 Method Overview,[0],[0]
"However, this is an uncertain hypotheses only, given the ambiguity of language, the noise induced by machine translation and the potential errors of the triple extraction.",2 Method Overview,[0],[0]
"Therefore, our method needs to de-noise these input phrases and quantify to what extent the the relational phrases are indeed synonymous.",2 Method Overview,[0],[0]
We discuss this in Sections 3 and 4.,2 Method Overview,[0],[0]
This section explains how we assign semantic types to relational phrases.,3 Relation Typing,[0],[0]
"For example, the relational phrase wrote could be typed as <author> wrote <paper>, as one candidate.",3 Relation Typing,[0],[0]
The typing helps us to disambiguate the meaning of the relational phrase and later find correct synonyms.,3 Relation Typing,[0],[0]
The relational phrase shot could have synonyms directed or killed with a gun.,3 Relation Typing,[0],[0]
"However, they represent different senses of the phrase shot.",3 Relation Typing,[0],[0]
"With semantic typing, we can separate these two meanings and determine that <person> shot <person> is a synonym of <person> killed with a gun <person>, whereas <director> shot<movie> is a synonym of<director> directed <movie>.
",3 Relation Typing,[0],[0]
"Relation typing has the following steps: argument extraction, argument disambiguation, argument typing and type filtering.",3 Relation Typing,[0],[0]
The output is a set of candidate types for the left and right arguments of each English relational phrase.,3 Relation Typing,[0],[0]
"For the typing of a relational phrase, we have to determine words in the left and right arguments that give cues for semantic types.",3.1 Argument Extraction,[0],[0]
"To this end, we identify named entities, whose types can be looked up in a knowledge base, and the head words of common
noun phrases.",3.1 Argument Extraction,[0],[0]
"As output, we produce a ranked list of entity mentions and common nouns.
",3.1 Argument Extraction,[0],[0]
"To create this ranking, we perform POS tagging and noun phrase chunking using Stanford CoreNLP",3.1 Argument Extraction,[0],[0]
"(Manning et al., 2014) and Apache OpenNLP 2.",3.1 Argument Extraction,[0],[0]
"For head noun extraction, we use the YAGO Javatools3 and a set of manually crafted regular expressions.",3.1 Argument Extraction,[0],[0]
"Since the input sentences result from machine translation, we could not use dependency parsing, because sentences are often ungrammatical.
",3.1 Argument Extraction,[0],[0]
"Finally, we extract all noun phrases which contain the same head noun.",3.1 Argument Extraction,[0],[0]
"These noun phrases are then sorted according to their lengths.
",3.1 Argument Extraction,[0],[0]
"For example, for input phrase contemporary British director who also created “Inception”, our method would yield contemporary British director, British director, director in decreasing order.",3.1 Argument Extraction,[0],[0]
The second step is responsible for the disambiguation of the noun phrase and named entity candidates.,3.2 Argument Disambiguation,[0],[0]
"We use the YAGO3 knowledge base (Mahdisoltani et al., 2015) for named entities, and WordNet (Fellbaum, 1998) for noun phrases.",3.2 Argument Disambiguation,[0],[0]
"We proceed in the ranking order of the phrases from the first step.
",3.2 Argument Disambiguation,[0],[0]
"Candidate senses are looked up in YAGO3 and WordNet, respectively, and each candidate is scored.",3.2 Argument Disambiguation,[0],[0]
"The scores are based on:
• Frequency count prior: This is the number of Wikipedia incoming links for named entities in YAGO3, or the frequency count of noun phrase senses in WordNet.
",3.2 Argument Disambiguation,[0],[0]
"• Wikipedia prior: We increase scores of YAGO3 entities whose URL strings (i.e., Wikipedia titles) occur in the Wikipedia page from which the triple was extracted.
",3.2 Argument Disambiguation,[0],[0]
"2opennlp.apache.org/ 3mpi-inf.mpg.de/yago-naga/javatools/
•",3.2 Argument Disambiguation,[0],[0]
Translation prior: We boost the scores of senses whose translations occur in the original input sentence.,3.2 Argument Disambiguation,[0],[0]
"For example, the word stage is disambiguated as opera stage rather than phase, because the original German sentence contains the word Bühne",3.2 Argument Disambiguation,[0],[0]
(German word for a concert stage) and not Phase.,3.2 Argument Disambiguation,[0],[0]
"The translations of word senses are obtained from Universal WordNet (de Melo and Weikum, 2009).
",3.2 Argument Disambiguation,[0],[0]
We prefer WordNet noun phrases over YAGO3 named entities since noun phrases have lower type ambiguity (fewer possible types).,3.2 Argument Disambiguation,[0],[0]
"The final score of a sense s is:
score(s) = αfreq(s)+βwiki(s)+γtrans(s) (1)
where freq(s) is the frequency count of s, and wiki(s) and trans(s) equal maximal frequency count if the Wikipedia prior and Translation prior conditions hold (and otherwise set to 0).",3.2 Argument Disambiguation,[0],[0]
"α, β, γ are tunable hyper-parameters (set using withheld data).
",3.2 Argument Disambiguation,[0],[0]
"Finally, from the list of candidates, we generate a disambiguated argument: either a WordNet synset or a YAGO3 entity identifier.",3.2 Argument Disambiguation,[0],[0]
"In the third step of relation typing, we assign candidate types to the disambiguated arguments.",3.3 Argument Typing,[0],[0]
"To this end, we query YAGO3 for semantic types (incl. transitive hypernyms) for a given YAGO3 or WordNet identifier.
",3.3 Argument Typing,[0],[0]
The type system used in POLY consists of a subset of the WordNet noun hierarchy.,3.3 Argument Typing,[0],[0]
"We restrict ourselves to 734 types, chosen semi-automatically as follows.",3.3 Argument Typing,[0],[0]
We selected the 1000 most frequent WordNet types in YAGO3 (incl. transitive hypernyms).,3.3 Argument Typing,[0],[0]
"Redundant and non-informative types were filtered out by the following technique: all types were organized into a directed acyclic graph (DAG), and
we removed a type when the frequency count of some of its children was higher than 80% of the parent’s count.",3.3 Argument Typing,[0],[0]
"For example, we removed type trainer since more than 80% of trainers in YAGO3 are also coaches.",3.3 Argument Typing,[0],[0]
"In addition, we manually removed a few non-informative types (e.g. expressive style).
",3.3 Argument Typing,[0],[0]
"As output, we obtain lists of semantic types for the two arguments of each relational phrase.",3.3 Argument Typing,[0],[0]
"In the last step, we filter types one more time.",3.4 Type Filtering,[0],[0]
"This time we filter candidate types separately for each distinct relational phrase, in order to choose the most suitable specific type signature for each phrase.",3.4 Type Filtering,[0],[0]
"This choice is made by type tree pruning.
",3.4 Type Filtering,[0],[0]
"For each relational phrase, we aggregate all types of the left arguments and all types of the right arguments, summing up their their frequency counts.",3.4 Type Filtering,[0],[0]
"This information is organized into a DAG, based on type hypernymy.",3.4 Type Filtering,[0],[0]
"Then we prune types as follows (similarly to Section 3.3): i) remove a parent type when the relative frequency count of one of the children types is larger than 80% of the parent’s count; ii) remove a child type when its relative frequency count is smaller than 20% of the parent’s count.
",3.4 Type Filtering,[0],[0]
For each of the two arguments of the relational phrase we allow only those types which are left after the pruning.,3.4 Type Filtering,[0],[0]
"The final output is a set of relational phrases where each has a set of likely type signatures (i.e., pairs of types for the relation’s arguments).",3.4 Type Filtering,[0],[0]
The second stage of POLY addresses the relation clustering.,4 Relation Clustering,[0],[0]
"The algorithm takes semantically typed relational phrases as input, quantifies the semantic similarity between relational phrases, and organizes them into clusters of synonyms.",4 Relation Clustering,[0],[0]
The key insight that our approach hinges on is that synonymous phrases have similar translations in a different language.,4 Relation Clustering,[0],[0]
"In our setting, two English phrases are semantically similar if they were translated from the same relational phrases in a foreign language and their argument types agree (see Figure 1 for an example).",4 Relation Clustering,[0],[0]
Similarities between English phrases are cast into edge weights of a graph with phrases as nodes.,4 Relation Clustering,[0],[0]
This graph is then partitioned to obtain clusters.,4 Relation Clustering,[0],[0]
The phrase similarities in POLY are based on probabilistic measures.,4.1 Probabilistic Similarity Measures,[0],[0]
"We use the notation:
• F : a set of relational phrases from a foreign language F
• E: a set of translations of relational phrases from language F to English
• c(f, e): no. of times of translating relational phrase f ∈ F into relational phrase e ∈ E
• c(f), c(e): frequency counts for relational phrase f ∈ F and its translation e ∈ E
• p(e|f) = c(f,e)c(f) : (estimator for the) probability of translating f ∈ F into e ∈ E
• p(f |e) = c(f,e)c(e) : (estimator for the) probability of e ∈ E being a translation of f ∈ F
We define:
p(e1|e2) = ∑
f
p(e1|f) ∗ p(f |e2) (2)
as the probability of generating relational phrase e1 ∈ E from phrase e2 ∈",4.1 Probabilistic Similarity Measures,[0],[0]
"E. Finally we define:
support(e1, e2) =",4.1 Probabilistic Similarity Measures,[0],[0]
"∑
f∈F c(f, e1) ∗",4.1 Probabilistic Similarity Measures,[0],[0]
"c(f, e2) (3)
confidence(e1, e2)",4.1 Probabilistic Similarity Measures,[0],[0]
"= 2
1 p(e1|e2) + 1 p(e2|e1)
(4)
",4.1 Probabilistic Similarity Measures,[0],[0]
Confidence is the final similarity measure used in POLY.,4.1 Probabilistic Similarity Measures,[0],[0]
We use the harmonic mean in Equation 4 to dampen similarity scores that have big differences in their probabilities in Equation 2.,4.1 Probabilistic Similarity Measures,[0],[0]
"Typically, pairs e1, e2 with such wide gaps in their probabilities come from subsumptions, not synonymous phrases.",4.1 Probabilistic Similarity Measures,[0],[0]
"Finally, we compute the support and confidence for every pair of English relational phrases which have a common source phrase of translation.",4.1 Probabilistic Similarity Measures,[0],[0]
"We prune phrase pairs with low support (below a threshold), and rank the remaining pairs by confidence.",4.1 Probabilistic Similarity Measures,[0],[0]
"To compute clusters of relational phrases, we use modularity-based graph partitioning.",4.2 Graph Clustering,[0],[0]
"Specifically, we use the partitioning algorithm of Blondel et al. (2008).",4.2 Graph Clustering,[0],[0]
"The resulting clusters (i.e., subgraphs) are
then ranked by their weighted graph density multiplied by the graph size (Equation 5).",4.2 Graph Clustering,[0],[0]
"The example of a cluster is shown in Table 1.
∑ (ei,ej)∈E sim(ei, ej)
|V",4.2 Graph Clustering,[0],[0]
| ∗ |V,4.2 Graph Clustering,[0],[0]
− 1| ∗ |V,4.2 Graph Clustering,[0],[0]
| (5),4.2 Graph Clustering,[0],[0]
"For the experimental evaluation, we primarily chose triples from the German language (and their English translations).",5 Evaluation,[0],[0]
"With about 23 million triples, German is the language with the largest number of extractions in the dataset, and there are about 2.5 million distinct relational phrases from the German-toEnglish translation.",5 Evaluation,[0],[0]
"The POLY method is implemented using Apache Spark, so it scales out to handle such large inputs.
",5 Evaluation,[0],[0]
"After applying the relation typing algorithm, we obtain around 10 million typed relational phrases.",5 Evaluation,[0],[0]
"If we ignored the semantic types, we would have about 950,000 distinct phrases.",5 Evaluation,[0],[0]
"On this input data, POLY detected 1,401,599 pairs of synonyms.",5 Evaluation,[0],[0]
"The synonyms were organized into 158,725 clusters.
",5 Evaluation,[0],[0]
"In the following, we present both an intrinsic evaluation and an extrinsic use case.",5 Evaluation,[0],[0]
"For the intrinsic evaluation, we asked human annotators to judge whether two typed relational phrases are synonymous or not.",5 Evaluation,[0],[0]
We also studied source languages other than German.,5 Evaluation,[0],[0]
"In addition, we compared POLY against PATTY (Nakashole et al., 2012) and DEFIE (Bovi et al., 2015) on the relation paraphrasing task.",5 Evaluation,[0],[0]
"For the extrinsic evaluation, we considered a simple question answering system and studied to what extent similarities between typed relational phrases can contribute to answering more questions.",5 Evaluation,[0],[0]
"To assess the precision of the discovered synonymy among relational phrases (i.e., clusters of para-
phrases), we sampled POLY’s output.",5.1 Precision of Synonyms,[0],[0]
We assessed the 250 pairs of synonyms with the highest similarity scores.,5.1 Precision of Synonyms,[0],[0]
"We also assessed a sample of 250 pairs of synonyms, randomly drawn from POLY’s output.
",5.1 Precision of Synonyms,[0],[0]
These pairs of synonyms were shown to several human annotators to check their correctness.,5.1 Precision of Synonyms,[0],[0]
"Relational phrases were presented by showing the semantic types, the textual representation of the relational phrase and sample sentences where the phrase was found.",5.1 Precision of Synonyms,[0],[0]
The annotators were asked whether two relational phrases have the same meaning or not.,5.1 Precision of Synonyms,[0],[0]
"They could also abstain.
",5.1 Precision of Synonyms,[0],[0]
"The results of this evaluation are shown in Table 2 with (lower bounds and upper bounds of) the 0.95-confidence Wilson score intervals (Brown et al., 2001).",5.1 Precision of Synonyms,[0],[0]
"This evaluation task had good interannotator agreement, with Fleiss’ Kappa around 0.6.",5.1 Precision of Synonyms,[0],[0]
"Table 3 shows anecdotal examples of synonymous pairs of relational phrases.
",5.1 Precision of Synonyms,[0],[0]
These results show that POLY’s quality is comparable with state-of-the-art baselines resources.,5.1 Precision of Synonyms,[0],[0]
"WiseNet (Moro and Navigli, 2012) is reported to have precision of 0.85 for 30,000 clusters.",5.1 Precision of Synonyms,[0],[0]
This is also the only prior work where the precision of synonymy of semantically typed relational phrases was evaluated.,5.1 Precision of Synonyms,[0],[0]
The other systems did not report that measure.,5.1 Precision of Synonyms,[0],[0]
"However, they performed the evaluation of subsumption, entailment or hypernymy relationships which are related to synonymy.",5.1 Precision of Synonyms,[0],[0]
Subsumptions in PATTY have precision of 0.83 for top 100 and 0.75 for a random sample.,5.1 Precision of Synonyms,[0],[0]
Hypernyms in RELLY are reported to have precision of 0.87 for top 100 and 0.78 for a random sample.,5.1 Precision of Synonyms,[0],[0]
"DEFIE performed separate evaluations for hypernyms generated directly from WordNet (precision 0.87) and hypernyms obtained through a substring generalization algorithm (precision 0.9).
",5.1 Precision of Synonyms,[0],[0]
Typical errors in the paraphrase discovery of POLY come from incorrect translations or extraction errors.,5.1 Precision of Synonyms,[0],[0]
"For example, heard and belongs to were clustered together because they were translated from the
same semantically ambiguous German word gehört.",5.1 Precision of Synonyms,[0],[0]
An example for extraction errors is that took and participated in were clustered together because took was incorrectly extracted from a sentence with the phrase took part in.,5.1 Precision of Synonyms,[0],[0]
"Other errors are caused by swapped order of arguments in a triple (i.e., mistakes in detecting passive form) and incorrect argument disambiguation.",5.1 Precision of Synonyms,[0],[0]
"To compare POLY with the closest competitors PATTY and DEFIE, we designed an experiment along the lines of the evaluation of Information Retrieval systems (e.g. TREC benchmarks).",5.2 Comparison to Competitors,[0],[0]
"First, we randomly chose 100 semantically typed relational phrases with at least three words (to focus on the more interesting multi-word case, rather than single verbs).",5.2 Comparison to Competitors,[0],[0]
These relational phrases had to occur in all three resources.,5.2 Comparison to Competitors,[0],[0]
"For every relational phrase we retrieved synonyms from all of the systems, forming a pool of candidates.",5.2 Comparison to Competitors,[0],[0]
"Next, to remove minor syntactic variations of the same phrase, the relational phrases were lemmatized.",5.2 Comparison to Competitors,[0],[0]
"In addition, we removed all leading prepositions, modal verbs, and adverbs.
",5.2 Comparison to Competitors,[0],[0]
We manually evaluated the correctness of the remaining paraphrase candidates for each of the 100 phrases.,5.2 Comparison to Competitors,[0],[0]
Precision was computed as the ratio of the correct synonyms by one system to the number of all synonyms provided by that system.,5.2 Comparison to Competitors,[0],[0]
"Recall was computed as the ratio of the number of correct synonyms by one system to the number of all correct synonyms in the candidate pool from all three systems.
",5.2 Comparison to Competitors,[0],[0]
The results are presented in Table 4.,5.2 Comparison to Competitors,[0],[0]
All results are macro-averaged over the 100 sampled phrases.,5.2 Comparison to Competitors,[0],[0]
We performed a paired t-test for precision and recall of POLY against each of the systems and obtained p-values below 0.05.,5.2 Comparison to Competitors,[0],[0]
"POLY and DEFIE of-
fer much higher diversity of synonyms than PATTY.",5.2 Comparison to Competitors,[0],[0]
"However, DEFIE’s synonyms often do not fit the semantic type signature of the given relational phrase and are thus incorrect.",5.2 Comparison to Competitors,[0],[0]
"For example, was assumed by was found to be a synonym of <group> was acquired by <group>.",5.2 Comparison to Competitors,[0],[0]
"PATTY, on the other hand, has higher recall due to its variety of prepositions attached to relational phrases; however, these also include spurious phrases, leading to lower precision.",5.2 Comparison to Competitors,[0],[0]
"For example, succeeded in was found to be a synonym of <person> was succeeded by <leader>.",5.2 Comparison to Competitors,[0],[0]
"Overall, POLY achieves much higher precision and recall than both of these baselines.",5.2 Comparison to Competitors,[0],[0]
"To evaluate the influence of different components, we performed an ablation study.",5.3 Ablation Study,[0],[0]
"We consider versions of POLY where Wikipedia prior and Translation prior (Section 3.2) are disregarded (− disambiguation), where the type system (Section 3.3) was limited to the 100 most frequent YAGO types (Type system 100) or to the 5 top-level types from the YAGO hierarchy (Type system 5), or where the type filtering parameter (Section 3.4) was set to 70% or 90% (Type filtering 0.7/0.9).",5.3 Ablation Study,[0],[0]
"The evaluation was done on random samples of 250 pairs of synonyms.
",5.3 Ablation Study,[0],[0]
Table 5 shows the results with the 0.95-confidence Wilson score intervals.,5.3 Ablation Study,[0],[0]
"Without our argument disambiguation techniques, the precision drops heavily.",5.3 Ablation Study,[0],[0]
"When weakening the type system, our tech-
niques for argument typing and type filtering are penalized, resulting in lower precision.",5.3 Ablation Study,[0],[0]
So we see that all components of the POLY architecture are essential for achieving high-quality output.,5.3 Ablation Study,[0],[0]
Lowering the type-filtering threshold yields results with comparable precision.,5.3 Ablation Study,[0],[0]
"However, increasing the threshold results in a worse noise filtering procedure.",5.3 Ablation Study,[0],[0]
"In addition to paraphrases derived from German, we evaluated the relational phrase synonymy derived from a few other languages with lower numbers of extractions.",5.4 Evaluation with Other Languages,[0],[0]
"We chose French, Hindi, and Russian (cf.",5.4 Evaluation with Other Languages,[0],[0]
"(Faruqui and Kumar, 2015)).",5.4 Evaluation with Other Languages,[0],[0]
"The results are presented in Table 6, again with the 0.95-confidence Wilson score intervals.
",5.4 Evaluation with Other Languages,[0],[0]
Synonyms derived from French have similar quality as those from German.,5.4 Evaluation with Other Languages,[0],[0]
This is plausible as one would assume that French and German have similar quality in translation to English.,5.4 Evaluation with Other Languages,[0],[0]
Synonyms derived from Russian and Hindi have lower precision due to the lower translation quality.,5.4 Evaluation with Other Languages,[0],[0]
"The precision for Hindi is lower, as the Hindi input corpus has much fewer sentences than for the other languages.",5.4 Evaluation with Other Languages,[0],[0]
"As an extrinsic use case for the POLY resource, we constructed a simple Question Answering (QA) system over knowledge graphs such as Freebase, and determined the number of questions for which the
system can find a correct answer.",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
We followed the approach presented by Fader et al. (2014).,5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"The system consists of question parsing, query rewriting and database look-up stages.",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"We disregard the stage of ranking answer candidates, and merely test whether the system could return the right answer (i.e., would return with the perfect ranking).
",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"In the question parsing stage, we use 10 highprecision parsing operators by Fader et al. (2014), which map questions (e.g., Who invented papyrus?)",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"to knowledge graph queries (e.g., (?x, invented, papyrus)).",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"Additionally, we map question words to semantic types.",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"For example, the word who is mapped to person, where to location, when to abstract entity and the rest of the question words are mapped to type entity.
",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
We harness synonyms and hyponyms of relational phrases to paraphrase the predicate of the query.,5.5 Extrinsic Evaluation: Question Answering,[0],[0]
The paraphrases must be compatible with the semantic type of the question word.,5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"In the end, we use the original query, as well as found paraphrases, to query a database of subject, predicate, object triples.",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"As the knowledge graph for this experiment we used the union of collections: a triples database from OpenIE (Fader et al., 2011), Freebase (Bollacker et al., 2008), Probase (Wu et al., 2012) and NELL (Carlson et al., 2010).",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"In total, this knowledge graph contained more than 900 Million triples.
",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"We compared six systems for paraphrasing semantically typed relational phrases:
• Basic: no paraphrasing at all, merely using the originally generated query.
• DEFIE: using the taxonomy of relational phrases by Bovi et al. (2015).
",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"• PATTY: using the taxonomy of relational phrases by Nakashole et al. (2012).
",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"• RELLY: using the subset of the PATTY taxonomy with additional entailment relationships between phrases (Grycner et al., 2015).
• POLY DE: using synonyms of relational phrases derived from the German language.
",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"• POLY ALL: using synonyms of relational phrases derived from the 61 languages.
",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"Since DEFIE’s relational phrases are represented by BabelNet (Navigli and Ponzetto, 2012) word sense identifiers, we generated all possible lemmas for
each identifier.",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"We ran the paraphrase-enhanced QA system for three benchmark sets of questions:
• TREC: the set of questions used for the evaluation of information retrieval QA systems (Voorhees and Tice, 2000)
",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"• WikiAnswers: a random subset of questions from WikiAnswers (Fader et al., 2013).
• WebQuestions: the set of questions about Freebase entities (Berant et al., 2013).
",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"From these question sets, we kept only those questions which can be parsed by one of the 10 question parsing templates and have a correct answer in the gold-standard ground truth.",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"In total, we executed 451 questions for TREC, 516 for WikiAnswers and 1979 for WebQuestions.
",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"For every question, each paraphrasing system generates a set of answers.",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
We measured for how many questions we could obtain at least one correct answer.,5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"Table 7 shows the results.
",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
The best results were obtained by POLY ALL.,5.5 Extrinsic Evaluation: Question Answering,[0],[0]
We performed a paired t-test for the results of POLY DE and POLY ALL against all other systems.,5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"The differences between POLY ALL and the other systems are statistically significant with pvalue below 0.05.
",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"Additionally, we evaluated paraphrasing systems which consist of combination of all of the described datasets and all of the described datasets without POLY.",5.5 Extrinsic Evaluation: Question Answering,[0],[0]
The difference between these two versions suggest that POLY contains many paraphrases which are available in none of the competing resources.,5.5 Extrinsic Evaluation: Question Answering,[0],[0]
"Knowledge bases (KBs) contribute to many NLP tasks, including Word Sense Disambiguation (Moro et al., 2014), Named Entity Disambiguation (Hoffart et al., 2011), Question Answering (Fader et al., 2014), and Textual Entailment (Sha et al., 2015).",6 Related Work,[0],[0]
"Widely used KBs are DBpedia (Lehmann et al., 2015), Freebase (Bollacker et al., 2008), YAGO (Mahdisoltani et al., 2015), Wikidata (Vrandecic and Krötzsch, 2014) and the Google Knowledge Vault (Dong et al., 2014).",6 Related Work,[0],[0]
"KBs have rich information about named entities, but are pretty sparse on relations.",6 Related Work,[0],[0]
"In the latter regard, manually created resources such as WordNet (Fellbaum, 1998), VerbNet (Kipper et al., 2008) or FrameNet (Baker et al., 1998) are much richer, but still face the limitation of labor-intensive input and human curation.
",6 Related Work,[0],[0]
The paradigm of Open Information Extraction (OIE) was developed to overcome the weak coverage of relations in automatically constructed KBs.,6 Related Work,[0],[0]
OIE methods process natural language texts to produce triples of surface forms for the arguments and relational phrase of binary relations.,6 Related Work,[0],[0]
"The first large-scale approach along these lines, TextRunner (Banko et al., 2007), was later improved by ReVerb (Fader et al., 2011) and OLLIE (Mausam et al., 2012).",6 Related Work,[0],[0]
"The focus of these methods has been on verbal phrases as relations, and there is little effort to determine lexical synonymy among them.
",6 Related Work,[0],[0]
"The first notable effort to build up a resource for relational paraphrases is DIRT (Lin and Pantel, 2001), based on Harris’ Distributional Hypothesis to cluster syntactic patterns.",6 Related Work,[0],[0]
"RESOLVER (Yates and Etzioni, 2009) introduced a probabilistic relational model for predicting synonymy.",6 Related Work,[0],[0]
Yao et al. (2012) incorporated latent topic models to resolve the ambiguity of relational phrases.,6 Related Work,[0],[0]
"Other probabilistic approaches employed matrix factorization for finding entailments between relations (Riedel et al., 2013; Petroni et al., 2015) or used probabilistic graphical models to find clusters of relations (Grycner et al., 2014).",6 Related Work,[0],[0]
"All of these approaches rely on the cooccurrence of the arguments of the relation.
",6 Related Work,[0],[0]
"Recent endeavors to construct large repositories of relational paraphrases are PATTY, WiseNet and DEFIE.",6 Related Work,[0],[0]
"PATTY (Nakashole et al., 2012) devised a sequence mining algorithm to extract relational
phrases with semantic type signatures, and organized them into synonymy sets and hypernymy hierarchies.",6 Related Work,[0],[0]
"WiseNet (Moro and Navigli, 2012) tapped Wikipedia categories for a similar way of organizing relational paraphrases.",6 Related Work,[0],[0]
"DEFIE (Bovi et al., 2015) went even further and used word sense disambiguation, anchored in WordNet, to group phrases with the same meanings.
",6 Related Work,[0],[0]
Translation models have previously been used for paraphrase detection.,6 Related Work,[0],[0]
Barzilay and McKeown (2001) utilized multiple English translations of the same source text for paraphrase extraction.,6 Related Work,[0],[0]
Bannard and Callison-Burch (2005) used the bilingual pivoting method on parallel corpora for the same task.,6 Related Work,[0],[0]
"Similar methods were performed at a much bigger scale by the Paraphrase Database (PPDB) project (Pavlick et al., 2015).",6 Related Work,[0],[0]
"Unlike POLY, the focus of these projects was not on paraphrases of binary relations.",6 Related Work,[0],[0]
"Moreover, POLY considers the semantic type signatures of relations, which is missing in PPDB.
",6 Related Work,[0],[0]
Research on OIE for languages other than English has received little attention.,6 Related Work,[0],[0]
Kim et al. (2011) uses Korean-English parallel corpora for cross-lingual projection.,6 Related Work,[0],[0]
Gamallo et al. (2012) developed an OIE system for Spanish and Portuguese using rules over shallow dependency parsing.,6 Related Work,[0],[0]
The recent work of Faruqui and Kumar (2015) extracted relational phrases from Wikipedia in 61 languages using crosslingual projection.,6 Related Work,[0],[0]
"Lewis and Steedman (2013) clustered semantically equivalent English and French phrases, based on the arguments of relations.",6 Related Work,[0],[0]
"We presented POLY, a method for clustering semantically typed English relational phrases using a multilingual corpus, resulting in a repository of semantically typed paraphrases with high coverage and precision.",7 Conclusions,[0],[0]
"Future work includes jointly processing all 61 languages in the corpus, rather than considering them pairwise, to build a resource for all languages.",7 Conclusions,[0],[0]
The POLY resource is publicly available at www.mpi-inf.mpg.de/yago-naga/poly/.,7 Conclusions,[0],[0]
"Language resources that systematically organize paraphrases for binary relations are of great value for various NLP tasks and have recently been advanced in projects like PATTY, WiseNet and DEFIE.",abstractText,[0],[0]
"This paper presents a new method for building such a resource and the resource itself, called POLY.",abstractText,[0],[0]
"Starting with a very large collection of multilingual sentences parsed into triples of phrases, our method clusters relational phrases using probabilistic measures.",abstractText,[0],[0]
We judiciously leverage fine-grained semantic typing of relational arguments for identifying synonymous phrases.,abstractText,[0],[0]
The evaluation of POLY shows significant improvements in precision and recall over the prior works on PATTY and DEFIE.,abstractText,[0],[0]
An extrinsic use case demonstrates the benefits of POLY for question answering.,abstractText,[0],[0]
POLY: Mining Relational Paraphrases from Multilingual Sentences,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 35–45 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
A basic but highly important challenge in natural language understanding is being able to populate a knowledge base with relational facts contained in a piece of text.,1 Introduction,[0],[0]
"For the text shown in Figure 1, the system should extract triples, or equivalently, knowledge graph edges, such as hPenner, per:spouse, Lisa Dillmani.",1 Introduction,[0],[0]
"Combining such extractions, a system can produce a knowledge graph of relational facts between persons, organizations, and locations in the text.",1 Introduction,[0],[0]
"This task involves entity recognition, mention coreference and/or entity linking, and relation extraction; we focus on the
most challenging “slot filling” task of filling in the relations between entities in the text.
",1 Introduction,[0],[0]
Organized relational knowledge in the form of “knowledge graphs” has become an important knowledge resource.,1 Introduction,[0],[0]
"These graphs are now extensively used by search engine companies, both to provide information to end-users and internally to the system, as a way to understand relationships.",1 Introduction,[0],[0]
"However, up until now, automatic knowledge extraction has proven sufficiently difficult that most of the facts in these knowledge graphs have been built up by hand.",1 Introduction,[0],[0]
"It is therefore a key challenge to show that NLP technology can effectively contribute to this important problem.
",1 Introduction,[0],[0]
"Existing work on relation extraction (e.g., Zelenko et al., 2003; Mintz et al., 2009; Adel et al., 2016) has been unable to achieve sufficient recall or precision for the results to be usable versus hand-constructed knowledge bases.",1 Introduction,[0],[0]
"Supervised training data has been scarce and, while techniques like distant supervision appear to be a promising way to extend knowledge bases at low cost, in practice the training data has often been too noisy for reliable training of relation extraction systems (Angeli et al., 2015).",1 Introduction,[0],[0]
"As a result most systems fail to make correct extractions even in apparently straightforward cases like Figure 1,
35
where the best system at the NIST TAC Knowledge Base Population (TAC KBP) 2015 evaluation failed to recognize the relation between Penner and Dillman.1 Consequently most automatic systems continue to make heavy use of hand-written rules or patterns because it has been hard for machine learning systems to achieve adequate precision or to generalize as well across text types.",1 Introduction,[0],[0]
"We believe machine learning approaches have suffered from two key problems: (1) the models used have been insufficiently tailored to relation extraction, and (2) there has been insufficient annotated data available to satisfy the training of data-hungry models, such as deep learning models.
",1 Introduction,[0],[0]
This work addresses both of these problems.,1 Introduction,[0],[0]
"We propose a new, effective neural network sequence model for relation classification.",1 Introduction,[0],[0]
Its architecture is better customized for the slot filling task: the word representations are augmented by extra distributed representations of word position relative to the subject and object of the putative relation.,1 Introduction,[0],[0]
This means that the neural attention model can effectively exploit the combination of semantic similarity-based attention and positionbased attention.,1 Introduction,[0],[0]
"Secondly, we markedly improve the availability of supervised training data by using Mechanical Turk crowd annotation to produce a large supervised training dataset (Table 1), suitable for the common relations between people, organizations and locations which are used in the TAC KBP evaluations.",1 Introduction,[0],[0]
"We name this dataset the TAC Relation Extraction Dataset (TACRED), and will make it available through the Linguistic Data Consortium (LDC) in order to respect copyrights on the underlying text.
",1 Introduction,[0],[0]
Combining these two gives a system with markedly better slot filling performance.,1 Introduction,[0],[0]
"This is
1Note: former spouses count as spouses in the ontology.
shown not only for a relation classification task on the crowd-annotated data but also for the incorporation of the resulting classifiers into a complete cold start knowledge base population system.",1 Introduction,[0],[0]
"On TACRED, our system achieves a relation classification F1 score that is 7.9% higher than that of a strong feature-based classifier, and 3.5% higher than that of the best previous neural architecture that we re-implemented.",1 Introduction,[0],[0]
"When this model is used in concert with a pattern-based system on the TAC KBP 2015 Cold Start Slot Filling evaluation data, the system achieves an F1 score of 26.7%, which exceeds the previous state-of-the-art by 4.5% absolute.",1 Introduction,[0],[0]
While this performance certainly does not solve the knowledge base population problem – achieving sufficient recall remains a formidable challenge – this is nevertheless notable progress.,1 Introduction,[0],[0]
"Existing work on neural relation extraction (e.g., Zeng et al., 2014; Nguyen and Grishman, 2015; Zhou et al., 2016) has focused on convolutional neural networks (CNNs), recurrent neural networks (RNNs), or their combination.",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"While these models generally work well on the datasets they are tested on, as we will show, they often fail to generalize to the longer sentences that are common in real-world text (such as in TAC KBP).
",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"We believe that existing model architectures suffer from two problems: (1) Although modern sequence models such as Long Short-Term Memory (LSTM) networks have gating mechanisms to control the relative influence of each individual word to the final sentence representation (Hochreiter and Schmidhuber, 1997), these controls are not explicitly conditioned on the entire sentence being classified; (2) Most existing work either
does not explicitly model the positions of entities (i.e., subject and object) in the sequence, or models the positions only within a local region.
",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"Here, we propose a new neural sequence model with a position-aware attention mechanism over an LSTM network to tackle these challenges.",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"This model can (1) evaluate the relative contribution of each word after seeing the entire sequence, and (2) base this evaluation not only on the semantic information of the sequence, but also on the global positions of the entities within the sequence.
",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
We formalize the relation extraction task as follows: Let X =,2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"[x1, ..., xn] denote a sentence, where xi is the i-th token.",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"A subject entity s and an object entity o are identified in the sentence, corresponding to two non-overlapping consecutive spans:",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
Xs =,2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"[xs1 , xs1+1, . . .",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
", xs2 ] and Xo =",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"[xo1 , xo1+1, . . .",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
", xo2 ].",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"Given the sentence X and the positions of s and o, the goal is to predict a relation r 2 R (R is the set of relations) that holds between s and o or no relation otherwise.
",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"Inspired by the position encoding vectors used in Collobert et al. (2011) and Zeng et al. (2014), we define a position sequence relative to the subject entity [ps1, ..., p s n], where
psi = 8>",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
<,2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
>,2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
": i s1, i < s1 0, s1  i  s2 i s2, i > s2
(1)
",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"Here s1, s2 are the starting and ending indices of the subject entity respectively, and psi 2 Z can be viewed as the relative distance of token xi to the subject entity.",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"Similarly, we obtain a position sequence [po1, ..., p o n] relative to the object entities.
",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
Let x,2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
=,2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"[x1, ...,xn] be word embeddings of the sentence, obtained using an embedding matrix E. Similarly, we obtain position embedding vectors ps =",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"[ps1, ...,p s n] and p o =",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"[po1, ...,p",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
o n,2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
] using a shared position embedding matrix P respectively.,2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"Next, as shown in Figure 2, we obtain hidden state representations of the sentence by feeding x into an LSTM:
{h1, ...,hn} = LSTM({x1, ...,xn}) (2)
We define a summary vector q =",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"hn (i.e., the output state of the LSTM).",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
This summary vector encodes information about the entire sentence.,2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"Then for each hidden state hi, we calculate an attention weight ai as:
ui = v> tanh(Whhi + Wqq+
Wspsi +",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"Wop o i ) (3)
ai = exp(ui)Pn
j=1 exp(uj) (4)
",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"Here Wh,Wq 2 Rda⇥d, Ws,Wo 2 Rda⇥dp and v 2 Rda are learnable parameters of the network, where d is the dimension of hidden states, dp is the dimension of position embeddings, and da is the size of attention layer.",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"Additional parameters of the network include embedding matrices E 2 R|V|⇥d and P 2 R(2L 1)⇥dp , where V is the vocabulary and L is the maximum sentence length.
",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
We regard attention weight ai as the relative contribution of the specific word to the sentence representation.,2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"The final sentence representation z is computed as:
z = Xn
i=1",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"aihi (5)
z is later fed into a fully-connected layer followed by a softmax layer for relation classification.
",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"Note that our model significantly differs from the attention mechanism in Bahdanau et al. (2015) and Zhou et al. (2016) in our use of the summary vector and position embeddings, and the way our attention weights are computed.",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"An intuitive way to understand the model is to view the attention calculation as a selection process, where the goal is to select relevant contexts over irrelevant ones.
",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
"Here the summary vector (q) helps the model to base this selection on the semantic information of the entire sentence (rather than on each word only), while the position vectors (psi and p o i ) provides important spatial information between each word and the entities.",2 A Position-aware Neural Sequence Model Suitable for Relation Extraction,[0],[0]
Previous research has shown that slot filling systems can greatly benefit from supervised data.,3 The TAC Relation Extraction Dataset,[0],[0]
"For example, Angeli et al. (2014b) showed that even a small amount of supervised data can boost the end-to-end F1 score by 3.9% on the TAC KBP tasks.",3 The TAC Relation Extraction Dataset,[0],[0]
"However, existing relation extraction datasets such as the SemEval-2010 Task 8 dataset (Hendrickx et al., 2009) and the Automatic Content Extraction (ACE) (Strassel et al., 2008) dataset are less useful for this purpose.",3 The TAC Relation Extraction Dataset,[0],[0]
"This is mainly because: (1) these datasets are relatively small for effectively training high-capacity models (see Table 2), and (2) they capture very different types of relations.",3 The TAC Relation Extraction Dataset,[0],[0]
"For example, the SemEval dataset focuses on semantic relations (e.g., CauseEffect, Component-Whole) between two nominals.
",3 The TAC Relation Extraction Dataset,[0],[0]
"One can further argue that it is easy to obtain a large amount of training data using distant supervision (Mintz et al., 2009).",3 The TAC Relation Extraction Dataset,[0],[0]
"In practice, however, due to the large amount of noise in the induced data, training relation extractors that perform well becomes very difficult.",3 The TAC Relation Extraction Dataset,[0],[0]
"For example, Riedel et al. (2010) show that up to 31% of the distantly supervised labels are wrong when creating training data from aligning Freebase to newswire text.
",3 The TAC Relation Extraction Dataset,[0],[0]
"To tackle these challenges, we collect a large supervised dataset TACRED, targeted towards the TAC KBP relations.
",3 The TAC Relation Extraction Dataset,[0],[0]
Data collection.,3 The TAC Relation Extraction Dataset,[0],[0]
We create TACRED based on query entities and annotated system responses in the yearly TAC KBP evaluations.,3 The TAC Relation Extraction Dataset,[0],[0]
"In each year of the TAC KBP evaluation (2009–2015), 100 entities (people or organizations) are given as queries,
for which participating systems should find associated relations and object entities.",3 The TAC Relation Extraction Dataset,[0],[0]
We make use of Mechanical Turk to annotate each sentence in the source corpus that contains one of these query entities.,3 The TAC Relation Extraction Dataset,[0],[0]
"For each sentence, we ask crowd workers to annotate both the subject and object entity spans and the relation types.
",3 The TAC Relation Extraction Dataset,[0],[0]
Dataset stratification.,3 The TAC Relation Extraction Dataset,[0],[0]
"In total we collect 119,474 examples.",3 The TAC Relation Extraction Dataset,[0],[0]
"We stratify TACRED across years in which the TAC KBP challenge was run, and use examples corresponding to query entities from 2009 to 2012 as training split, 2013 as development split, and 2014 as test split.",3 The TAC Relation Extraction Dataset,[0],[0]
"We reserve the TAC KBP 2015 evaluation data for running slot filling evaluations, as presented in Section 4.",3 The TAC Relation Extraction Dataset,[0],[0]
"Detailed statistics are given in Table 3.
Discussion.",3 The TAC Relation Extraction Dataset,[0],[0]
Table 1 presents sampled examples from TACRED.,3 The TAC Relation Extraction Dataset,[0],[0]
"Compared to existing datasets, TACRED has four advantages.",3 The TAC Relation Extraction Dataset,[0],[0]
"First, it contains an order of magnitude more relation instances (Table 2), enabling the training of expressive models.",3 The TAC Relation Extraction Dataset,[0],[0]
"Second, we reuse the entity and relation types of the TAC KBP tasks.",3 The TAC Relation Extraction Dataset,[0],[0]
We believe these relation types are of more interest to downstream applications.,3 The TAC Relation Extraction Dataset,[0],[0]
"Third, we fully annotate all negative instances that appear in our data collection process, to ensure that models trained on TACRED are not biased towards predicting false positives on realworld text.",3 The TAC Relation Extraction Dataset,[0],[0]
"Lastly, the average sentence length in TACRED is 36.2, compared to 19.1 in the SemEval dataset, reflecting the complexity of contexts in which relations occur in real-world text.
",3 The TAC Relation Extraction Dataset,[0],[0]
"Due to space constraints, we describe the data collection and validation process, system interfaces, and more statistics and examples of TACRED in the supplementary material.",3 The TAC Relation Extraction Dataset,[0],[0]
We will make TACRED publicly available through the LDC.,3 The TAC Relation Extraction Dataset,[0],[0]
"In this section we evaluate the effectiveness of our proposed model and TACRED on improving slot
filling systems.",4 Experiments,[0],[0]
"Specifically, we run two sets of experiments: (1) we evaluate model performance on the relation extraction task using TACRED, and (2) we evaluate model performance on the TAC KBP 2015 cold start slot filling task, by training the models on TACRED.",4 Experiments,[0],[0]
"We compare our model against the following baseline models for relation extraction and slot filling:
TAC KBP 2015 winning system.",4.1 Baseline Models,[0],[0]
"To judge our proposed model against a strong baseline, we compare against Stanford’s top performing system on the TAC KBP 2015 cold start slot filling task (Angeli et al., 2015).",4.1 Baseline Models,[0],[0]
At the core of this system are two relation extractors: a pattern-based extractor and a logistic regression (LR) classifier.,4.1 Baseline Models,[0],[0]
"The pattern-based system uses a total of 4,528 surface patterns and 169 dependency patterns.",4.1 Baseline Models,[0],[0]
The logistic regression model was trained on approximately 2 million bootstrapped examples (using a small annotated dataset and high-precision pattern system output) that are carefully tuned for TAC KBP slot filling evaluation.,4.1 Baseline Models,[0],[0]
"It uses a comprehensive feature set similar to the MIML-RE system for relation extraction (Surdeanu et al., 2012), including lemmatized n-grams, sequence NER tags and POS tags, positions of entities, and various features over dependency paths, etc.
",4.1 Baseline Models,[0],[0]
Convolutional neural networks.,4.1 Baseline Models,[0],[0]
We follow the 1-dimensional CNN architecture by Nguyen and Grishman (2015) for relation extraction.,4.1 Baseline Models,[0],[0]
"This model learns a representation of the input sentence, by first running a series of convolutional operations on the sentence with various filters, and then feeding the output into a max-pooling layer to reduce the dimension.",4.1 Baseline Models,[0],[0]
The resulting representation is then fed into a fully-connected layer followed by a softmax layer for relation classification.,4.1 Baseline Models,[0],[0]
"As an extension, positional embeddings are also introduced into this model to better capture the relative position of each word to the subject and object entities and were shown to achieve improved results.",4.1 Baseline Models,[0],[0]
"We use “CNN-PE” to represent the CNN model with positional embeddings.
",4.1 Baseline Models,[0],[0]
Dependency-based recurrent neural networks.,4.1 Baseline Models,[0],[0]
"In dependency-based neural models, shortest dependency paths between entities are often used as input to the neural networks.",4.1 Baseline Models,[0],[0]
"The intuition is to eliminate tokens that are potentially less relevant
to the classification of the relation.",4.1 Baseline Models,[0],[0]
"For the example in Figure 1, the shortest dependency path between the two entities is:
[Penner] survived!",4.1 Baseline Models,[0],[0]
brother !,4.1 Baseline Models,[0],[0]
wife!,4.1 Baseline Models,[0],[0]
"[Lisa Dillman]
We follow the SDP-LSTM model proposed by Xu et al. (2015b).",4.1 Baseline Models,[0],[0]
"In this model, each shortest dependency path is divided into two separate sub-paths from the subject entity and the object entity to the lowest common ancestor node.",4.1 Baseline Models,[0],[0]
"Each sub-path is fed into an LSTM network, and the resulting hidden units at each word position are passed into a max-over-time pooling layer to form the output of this sub-path.",4.1 Baseline Models,[0],[0]
"Outputs from the two sub-paths are then concatenated to form the final representation.
",4.1 Baseline Models,[0],[0]
"In addition to the above models, we also compare our proposed model against an LSTM sequence model without attention mechanism.",4.1 Baseline Models,[0],[0]
We map words that occur less than 2 times in the training set to a special <UNK> token.,4.2 Implementation Details,[0],[0]
"We use the pre-trained GloVe vectors (Pennington et al., 2014) to initialize word embeddings.",4.2 Implementation Details,[0],[0]
"For all the LSTM layers, we find that 2-layer stacked LSTMs generally work better than one-layer LSTMs.",4.2 Implementation Details,[0],[0]
"We minimize cross-entropy loss over all 42 relations using AdaGrad (Duchi et al., 2011).",4.2 Implementation Details,[0],[0]
We apply Dropout with p = 0.5 to CNNs and LSTMs.,4.2 Implementation Details,[0],[0]
During training we also find a word dropout strategy to be very effective: we randomly set a token to be <UNK> with a probability p.,4.2 Implementation Details,[0],[0]
"We set p to be 0.06 for the SDP-LSTM model and 0.04 for all other models.
",4.2 Implementation Details,[0],[0]
Entity masking.,4.2 Implementation Details,[0],[0]
We replace each subject entity in the original sentence with a special <NER>SUBJ token where <NER> is the corresponding NER signature of the subject as provided in TACRED.,4.2 Implementation Details,[0],[0]
We do the same processing for object entities.,4.2 Implementation Details,[0],[0]
"This processing step helps (1) provide a model with entity type information, and (2) prevent a model from overfitting its predictions to specific entities.
",4.2 Implementation Details,[0],[0]
Multi-channel augmentation.,4.2 Implementation Details,[0],[0]
"Instead of using only word vectors as input to the network, we augment the input with part-of-speech (POS) and named entity recognition (NER) embeddings.",4.2 Implementation Details,[0],[0]
We run Stanford CoreNLP,4.2 Implementation Details,[0],[0]
"(Manning et al., 2014) to obtain the POS and NER annotations.
",4.2 Implementation Details,[0],[0]
We describe our model hyperparameters and training in detail in the supplementary material.,4.2 Implementation Details,[0],[0]
We first evaluate all models on TACRED.,4.3 Evaluation on TACRED,[0],[0]
We train each model for 5 separate runs with independent random initializations.,4.3 Evaluation on TACRED,[0],[0]
For each run we perform early stopping using the dev set.,4.3 Evaluation on TACRED,[0],[0]
"We then select the run (among 5) that achieves the median F1 score on the dev set, and report its test set performance.
",4.3 Evaluation on TACRED,[0],[0]
Table 4 summarizes our results.,4.3 Evaluation on TACRED,[0],[0]
"We observe that all neural models achieve higher F1 scores than the logistic regression and patterns systems, which demonstrates the effectiveness of neural models for relation extraction.",4.3 Evaluation on TACRED,[0],[0]
"Although positional embeddings help increase the F1 by around 2% over the plain CNN model, a simple (2-layer) LSTM model performs surprisingly better than CNN and dependency-based models.",4.3 Evaluation on TACRED,[0],[0]
"Lastly, our proposed position-aware mechanism is very effective and achieves an F1 score of 65.4%, with an absolute increase of 3.9% over the best baseline neural model (LSTM) and 7.9% over the baseline logistic regression system.",4.3 Evaluation on TACRED,[0],[0]
"We also run an ensemble of our position-aware attention model which takes majority votes from 5 runs with random initializations and it further pushes the F1 score up by 1.6%.
",4.3 Evaluation on TACRED,[0],[0]
We find that different neural architectures show a different balance between precision and recall.,4.3 Evaluation on TACRED,[0],[0]
CNN-based models tend to have higher precision; RNN-based models have better recall.,4.3 Evaluation on TACRED,[0],[0]
This can be explained by noting that the filters in CNNs are essentially a form of “fuzzy n-gram patterns”.,4.3 Evaluation on TACRED,[0],[0]
"Second, we evaluate the slot filling performance of all models using the TAC KBP 2015 cold start slot filling task (Ellis et al., 2015).",4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
"In this task, about 50k newswire and Web forum documents are selected as the evaluation corpus.",4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
A slot filling system is asked to answer a series of queries with two-hop slots (Figure 3):,4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
"The first slot asks about fillers of a relation with the query entity as the subject (Mike Penner), and we term this a hop-0 slot; the second slot asks about fillers with the system’s hop-0 output as the subject, and we term this a hop-1 slot.",4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
"System predictions are then evaluated against gold annotations, and micro-averaged precision, recall and F1 scores are calculated at the hop-0 and hop-1 levels.",4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
"Lastly hop-all scores are calculated by combining hop-0 and hop-1 scores.2
Evaluating relation extraction systems on slot filling is particularly challenging in that: (1) Endto-end cold start slot filling scores conflate the performance of all modules in the system (i.e., entity recognizer, entity linker and relation extractor).",4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
(2) Errors in hop-0 predictions can easily propagate to hop-1 predictions.,4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
"To fairly evaluate each relation extraction model on this task, we use Stanford’s 2015 slot filling system as our basic pipeline.3 It is a very strong baseline specifically tuned for TAC KBP evaluation and ranked top in the 2015 evaluation.",4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
"We then plug in the corresponding relation extractor trained on TACRED, keeping all other modules unchanged.
",4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
Table 5 presents our results.,4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
"We find that: (1) by only training our logistic regression model on TACRED (in contrast to on the 2 million bootstrapped examples used in the 2015 Stanford system) and combining it with patterns, we obtain a higher hop-0 F1 score than the 2015 Stanford sys-
2In the TAC KBP cold start slot filling evaluation, a hop-1 slot is transferred to a pseudo-slot which is treated equally as a hop-0 slot.",4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
"Hop-all precision, recall and F1 are then calculated by combining these pseudo-slot predictions and hop-0 predictions.
",4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
3This system uses the fine-grained NER system in Stanford CoreNLP,4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
"(Manning et al., 2014) for entity detection and the Illinois Wikifier (Ratinov et al., 2011) for entity linking.
",4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
"tem, and a similar hop-all F1; (2) our proposed position-aware attention model substantially outperforms the 2015 Stanford system on all hop-0, hop-1 and hop-all F1 scores.",4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
"Combining it with the patterns, we achieve a hop-all F1 of 26.7%, an absolute improvement of 4.5% over the previous state-of-the-art result.",4.4 Evaluation on TAC KBP Slot Filling,[0],[0]
Model ablation.,4.5 Analysis,[0],[0]
Table 6 presents the results of an ablation test of our position-aware attention model on the development set of TACRED.,4.5 Analysis,[0],[0]
"The entire attention mechanism contributes about 1.5% F1, where the position-aware term in Eq.",4.5 Analysis,[0],[0]
"(3) alone contributes about 1% F1 score.
",4.5 Analysis,[0],[0]
Impact of negative examples.,4.5 Analysis,[0],[0]
"Figure 4 shows how the slot filling evaluation scores change as we change the amount of negative (i.e., no relation) training data provided to our proposed model.",4.5 Analysis,[0],[0]
"We find that: (1) At hop-0 level, precision increases as we provide more negative examples, while recall stays almost unchanged.",4.5 Analysis,[0],[0]
F1 score keeps increasing.,4.5 Analysis,[0],[0]
"(2) At hop-all level, F1 score increases by
about 10% as we change the amount of negative examples from 20% to 100%.
",4.5 Analysis,[0],[0]
Performance by sentence length.,4.5 Analysis,[0],[0]
Figure 5 shows performance on varying sentence lengths.,4.5 Analysis,[0],[0]
We find that: (1) Performance of all models degrades substantially as the sentences get longer.,4.5 Analysis,[0],[0]
"(2) Compared to the baseline Logistic Regression model, all neural models handle long sentences better.",4.5 Analysis,[0],[0]
"(3) Compared to CNN-PE model, RNNbased models are more robust on long sentences, and notably SDP-LSTM model is least sensitive to sentence length.",4.5 Analysis,[0],[0]
"(4) Our proposed model achieves equal or better results on sentences of all lengths, except for sentences with more than 60 tokens where SDP-LSTM model achieves the best result.
",4.5 Analysis,[0],[0]
Improvement by slot types.,4.5 Analysis,[0],[0]
We calculate the F1 score for each slot type and compare the improvement from using our proposed model across slot types.,4.5 Analysis,[0],[0]
"When compared with the CNN-PE model, our position-aware attention model achieves improved F1 scores on 30 out of the 41 slot types, with the top 5 slot types being org:members, per:country of death, org:shareholders, per:children and per:religion.",4.5 Analysis,[0],[0]
"When compared with SDP-LSTM model, our model achieves improved F1 scores on 26 out of the 41 slot types, with the top 5 slot types being org:political/religious affiliation, per:country of death, org:alternate names, per:religion and per:alternate names.",4.5 Analysis,[0],[0]
"We observe that slot types with relatively sparse training examples tend to be improved by using the position-aware attention model.
",4.5 Analysis,[0],[0]
Attention visualization.,4.5 Analysis,[0],[0]
"Lastly, Figure 6 shows the visualization of attention weights assigned by our model on sampled sentences from the development set.",4.5 Analysis,[0],[0]
"We find that the model learns to pay more attention to words that are informative for the relation (e.g., “graduated from”, “niece” and “chairman”), though it still makes mistakes (e.g., “refused to name the three”).",4.5 Analysis,[0],[0]
"We also observe that the model tends to put a lot of weight onto object entities, as the object NER signatures are very informative to the classification of relations.",4.5 Analysis,[0],[0]
Relation extraction.,5 Related Work,[0],[0]
"There are broadly three main lines of work on relation extraction: first, fully-supervised approaches (Zelenko et al., 2003; Bunescu and Mooney, 2005), where a statisti-
cal classifier is trained on an annotated dataset; second, distant supervision (Mintz et al., 2009; Surdeanu et al., 2012), where a training set is formed by projecting the relations in an existing knowledge base onto textual instances that contain the entities that the relation connects; and third, Open IE (Fader et al., 2011; Mausam et al., 2012), which views its goal as producing subject-relationobject triples and expressing the relation in text.
",5 Related Work,[0],[0]
Slot filling and knowledge base population.,5 Related Work,[0],[0]
"The most widely-known effort to evaluate slot filling and KBP systems is the yearly TAC KBP slot filling tasks, starting from 2009 (McNamee and Dang, 2009).",5 Related Work,[0],[0]
"Participants in slot filling tasks usually make use of hybrid systems that combine patterns, Open IE, distant supervision and supervised systems for relation extraction (Kisiel et al., 2015; Finin et al., 2015; Zhang et al., 2016).
",5 Related Work,[0],[0]
Datasets for relation extraction.,5 Related Work,[0],[0]
"Popular general-domain datasets include the ACE dataset (Strassel et al., 2008) and the SemEval-2010 task 8 dataset (Hendrickx et al., 2009).",5 Related Work,[0],[0]
"In addition, the BioNLP Shared Tasks (Kim et al., 2009) are yearly efforts on creating datasets and evaluations for biomedical information extraction systems.
",5 Related Work,[0],[0]
Deep learning models for relation extraction.,5 Related Work,[0],[0]
"Many deep learning models have been proposed for relation extraction, with a focus on end-to-end training using CNNs (Zeng et al., 2014; Nguyen and Grishman, 2015) and RNNs (Zhang et al., 2015).",5 Related Work,[0],[0]
"Other popular approaches include using CNN or RNN over dependency paths between entities (Xu et al., 2015a,b), augmenting RNNs with different components (Xu et al., 2016; Zhou et al., 2016), and combining RNNs and CNNs (Vu et al., 2016; Wang et al., 2016).",5 Related Work,[0],[0]
Adel et al. (2016) compares the performance of CNN models against traditional approaches on slot filling using a portion of the TAC KBP evaluation data.,5 Related Work,[0],[0]
"We introduce a state-of-the-art position-aware neural sequence model for relation extraction, as well as TACRED, a large-scale, crowd-sourced dataset that is orders of magnitude larger than previous relation extraction datasets.",6 Conclusion,[0],[0]
Our proposed model outperforms a strong feature-based classifier and all baseline neural models.,6 Conclusion,[0],[0]
"In combination with the new dataset, it improves the state-of-the-
art hop-all F1 on the TAC KBP 2015 slot filling task by 4.5% absolute.",6 Conclusion,[0],[0]
We thank the anonymous reviewers for their helpful suggestions.,Acknowledgments,[0],[0]
We gratefully acknowledge the support of the Allen Institute for Artificial Intelligence and the support of the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract,Acknowledgments,[0],[0]
No. FA8750-13-2-0040.,Acknowledgments,[0],[0]
"Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the DARPA, AFRL, or the US government.",Acknowledgments,[0],[0]
Organized relational knowledge in the form of “knowledge graphs” is important for many applications.,abstractText,[0],[0]
"However, the ability to populate knowledge bases with facts automatically extracted from documents has improved frustratingly slowly.",abstractText,[0],[0]
This paper simultaneously addresses two issues that have held back prior work.,abstractText,[0],[0]
"We first propose an effective new model, which combines an LSTM sequence model with a form of entity position-aware attention that is better suited to relation extraction.",abstractText,[0],[0]
"Then we build TACRED, a large (119,474 examples) supervised relation extraction dataset, obtained via crowdsourcing and targeted towards TAC KBP relations.",abstractText,[0],[0]
The combination of better supervised data and a more appropriate high-capacity model enables much better relation extraction performance.,abstractText,[0],[0]
"When the model trained on this new dataset replaces the previous relation extraction component of the best TAC KBP 2015 slot filling system, its F1 score increases markedly from 22.2% to 26.7%.",abstractText,[0],[0]
Position-aware Attention and Supervised Data Improve Slot Filling,title,[0],[0]
There are many cases in Bayesian modeling where a certain choice of prior distribution allows for computationally simple or tractable inference.,1. Introduction,[0],[0]
"For example,
• Conjugate priors yield posteriors with a known parametric form and therefore allow for non-iterative, exact inference (Diaconis et al., 1979).
",1. Introduction,[0],[0]
"• Certain priors yield models with tractable conditional or marginal distributions, which allows efficient approximate inference algorithms to be applied (e.g. Gibbs sampling (Smith & Roberts, 1993), sampling
1Carnegie Mellon University, Machine Learning Department, Pittsburgh, USA 2CMU School of Computer Science.",1. Introduction,[0],[0]
"Correspondence to: Willie Neiswanger <willie@cs.cmu.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
in collapsed models (Teh et al., 2006), or mean-field variational methods (Wang & Blei, 2013)).
",1. Introduction,[0],[0]
"• Simple parametric priors allow for computationally cheap density queries, maximization, and sampling, which can reduce costs in iterative inference algorithms (e.g. Metropolis-Hastings (Metropolis et al., 1953), gradient-based MCMC (Neal, 2011), or sequential Monte Carlo (Doucet et al., 2000)).
",1. Introduction,[0],[0]
"For these reasons, one might hope to infer a result under a convenient-but-unrealistic prior, and afterwards, attempt to correct the result.",1. Introduction,[0],[0]
"More generally, given an inference result (under a convenient prior or otherwise), one might wish to incorporate updated prior information, or see a result under different prior assumptions, without having to re-run a costly inference algorithm.
",1. Introduction,[0],[0]
"This leads to the main question of this paper: for a given model, is it possible to use any convenient false prior to infer a false posterior, and afterwards, given any target prior of interest, efficiently and accurately infer the associated target posterior?
",1. Introduction,[0],[0]
One potential strategy involves sampling from the false posterior and reweighting these samples via importance sampling (IS).,1. Introduction,[0],[0]
"However, depending on the chosen target prior—both its parametric form and similarity to the false prior—the resulting inference can be inaccurate due to high or infinite variance IS estimates (demonstrated in Sec. 2.1).
",1. Introduction,[0],[0]
We instead aim to devise a method that yields accurate inferences for arbitrary target priors.,1. Introduction,[0],[0]
"Furthermore, like IS, we want to make use of the pre-inferred false posterior, without simply running standard inference algorithms on the target posterior.",1. Introduction,[0],[0]
"Note that most standard inference algorithms are iterative and data-dependent: parameter updates at each iteration involve data, and the computational cost or quality of each update depends on the amount of data used.",1. Introduction,[0],[0]
"Hence, running inference algorithms directly on the target posterior can be costly (especially given a large amount of data or many target priors of interest) and defeats the purpose of using a convenient false prior.
",1. Introduction,[0],[0]
"In this paper, we propose prior swapping, an iterative, dataindependent method for generating accurate posterior samples under arbitrary target priors.",1. Introduction,[0],[0]
"Prior swapping uses the pre-inferred false posterior to perform efficient updates that
do not depend on the data, and thus proceeds very quickly.",1. Introduction,[0],[0]
"We therefore advocate breaking difficult inference problems into two easier steps: first, do inference using the most computationally convenient prior for a given model, and then, for all future priors of interest, use prior swapping.
",1. Introduction,[0],[0]
"In the following sections, we demonstrate the pitfalls of using IS, describe the proposed prior swapping methods for different types of false posterior inference results (e.g. exact or approximate density functions, or samples) and give theoretical guarantees for these methods.",1. Introduction,[0],[0]
"Finally, we show empirical results on heavy-tailed and sparsity priors in Bayesian generalized linear models, and relational priors over components in mixture and topic models.",1. Introduction,[0],[0]
"Suppose we have a dataset of n vectors xn = {x1, . . .",2. Methodology,[0],[0]
", xn},",2. Methodology,[0],[0]
"xi ∈ Rp, and we have chosen a family of models with the likelihood function L(θ|xn) = p(xn|θ), parameterized by θ ∈ Rd.",2. Methodology,[0],[0]
"Suppose we have a prior distribution over the space of model parameters θ, with probability density function (PDF) π(θ).",2. Methodology,[0],[0]
"The likelihood and prior define a joint model with PDF p(θ, xn) = π(θ)L(θ|xn).",2. Methodology,[0],[0]
"In Bayesian inference, we are interested in computing the posterior (conditional) distribution of this joint model, with PDF
p(θ|xn) = π(θ)L(θ|x n)∫
π(θ)L(θ|xn) dθ .",2. Methodology,[0],[0]
"(1)
Suppose we’ve chosen a different prior distribution πf (θ), which we refer to as a false prior (while we refer to π(θ) as the target prior).",2. Methodology,[0],[0]
"We can now define a new posterior
pf (θ|xn) =",2. Methodology,[0],[0]
"πf (θ)L(θ|xn)∫ πf (θ)L(θ)|xn) dθ
(2)
which we refer to as a false posterior.
",2. Methodology,[0],[0]
"We are interested in the following task: given a false posterior inference result (i.e. samples from pf (θ|xn), or some exact or approximate PDF), choose an arbitrary target prior π(θ) and efficiently sample from the associated target posterior p(θ|xn)—or, more generally, compute an expectation µh = Ep [h(θ)] for some test function h(θ) with respect to the target posterior.",2. Methodology,[0],[0]
"We begin by describing an initial strategy, and existing work in a related task known as prior sensitivity analysis.
",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
Suppose we have T false posterior samples {θ̃t}Tt=1 ∼ pf (θ|xn).,2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"In importance sampling (IS), samples from an importance distribution are used to estimate the expectation of a test function with respect to a target distribution.",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"A straightforward idea is to use the false posterior as an
importance distribution, and compute the IS estimate
µ̂ISh = T∑ t=1 w(θ̃t)h(θ̃t) (3)
where the weight function w(θ) ∝ p(θ|x n) pf (θ|xn) ∝ π(θ) πf (θ)
, and the T weights are normalized to sum to one.
",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
IS-based methods have been developed for the task of prior sensitivity analysis (PSA).,2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"In PSA, the goal is to determine how the posterior varies over a sequence of priors (e.g. over a parameterized family of priors π(θ; γi), i = 0, 1, . . .).",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"Existing work has proposed inferring a single posterior under prior π(θ; γ0), and then using IS methods to infer further posteriors in the sequence (Besag et al., 1995; Hastings, 1970; Bornn et al., 2010).
",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"This strategy is effective when subsequent priors are similar enough, but breaks down when two priors are sufficiently dissimilar, or are from ill-matched parametric families, which we illustrate in an example below.
",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"Note that, in general for IS, as T → ∞, µ̂ISh → µh almost surely.",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"However, IS estimates can still fail in practice if µ̂ISh has high or infinite variance.",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"If so, the variance of the weights w(θ̃t) will be large (a problem often referred to as weight degeneracy), which can lead to inaccurate estimates.",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"In our case, the variance of µ̂ISh is only finite if
Epf [ h(θ)2 π(θ)2
πf (θ)2
] ∝",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"Ep [ h(θ)2 π(θ)
πf (θ)
]",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"<∞. (4)
",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"For a broad class of h, this is satisfied",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"if there existsM ∈ R such that π(θ)πf (θ) < M, ∀θ (Geweke, 1989)",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
.,2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"Given some preinferred pf (θ|xn) with false prior πf (θ), the accuracy of IS thus depends on the target prior of interest.",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"For example, if π(θ) has heavier tails than πf (θ), the variance of µ̂ISh will be infinite for many h. Intuitively, we expect the variance to be higher for π that are more dissimilar to πf .
",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
We show a concrete example of this in Fig. 1.,2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"Consider a normal model for data xn ∼ N (θ, 1), with a standard normal false prior πf (θ) = N (θ|0, 1).",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"This yields a closedform false posterior (due to the conjugate πf ), which is also normal.",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"Suppose we’d like to estimate the posterior expectation under a Laplace target prior, with mean 10 and variance 1, for test function h(θ) = θ (i.e. an estimate of the target posterior mean).",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"We draw T false posterior samples {θ̃t}Tt=1 ∼ pf (θ|xn), compute weights w(θ̃t) and IS estimate µ̂ISh , and compare it with the true expectation µh.
",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
We see in Fig. 1 that |µh,2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"− µ̂ISh | slows significantly as T increases, and maintains a high error even as T is made very large.",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
We can analyze this issue theoretically.,2.1. Importance Sampling and Prior Sensitivity,[0],[0]
Suppose we want |µh,2.1. Importance Sampling and Prior Sensitivity,[0],[0]
− µ̂ISh | < δ.,2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"Since we know pf (θ|xn) is normal, we can compute a lower bound on the number of false posterior samples T that would be needed for
the expected estimate to be within δ of µh.",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"Namely, if pf (θ|xn) = N (θ|m, s2), in order for |µh",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
− Epf,2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"[µ̂ISh ]| < δ, we’d need
T ≥ exp { 1
2s2 (|µh −m|",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"− δ)2
} .
",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"In the example in Fig. 1, we have m = 1, s2 = 0.25, and µh = 7.9892.",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"Hence, for |µh",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
− Epf,2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"[µ̂ISh ]| < 1, we’d need T > 1031 samples (see appendix for full details of this analysis).",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"Note that this bound actually has nothing to do with the parametric form of π(θ)—it is based solely on the normal false posterior, and its distance to the target posterior mean µh.",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"However, even if this distance was small, the importance estimate would still have infinite variance due to the Laplace target prior.",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"Further, note that the situation can significantly worsen in higher dimensions, or if the false posterior has a lower variance.",2.1. Importance Sampling and Prior Sensitivity,[0],[0]
"We’d like a method that will work well even when false and target priors πf (θ) and π(θ) are significantly different, or are from different parametric families, with performance that does not worsen (in accuracy nor computational complexity) as the priors are made more dissimilar.
",2.2. Prior Swapping,[0],[0]
"Redoing inference for each new target posterior can be very costly, especially when the data size n is large, because the per-iteration cost of most standard inference algorithms scales with n, and many iterations may be needed for accurate inference.",2.2. Prior Swapping,[0],[0]
This includes both MCMC and sequential monte carlo (SMC) algorithms (i.e. repeated-ISmethods that infer a sequence of distributions).,2.2. Prior Swapping,[0],[0]
"In SMC, the per-iteration cost still scales with n, and the variance estimates can still be infinite if subsequent distributions are ill-matched.
",2.2. Prior Swapping,[0],[0]
"Instead, we aim to leverage the inferred false posterior to more-efficiently compute any future target posterior.",2.2. Prior Swapping,[0],[0]
We begin by defining a prior swap density ps(θ).,2.2. Prior Swapping,[0],[0]
"Suppose for now that a false posterior inference algorithm has returned a density function p̃f (θ) (we will give more details on p̃f
later; assume for now that it is either equal to pf (θ|xn) or approximates it).",2.2. Prior Swapping,[0],[0]
"We then define the prior swap density as
ps(θ) ∝",2.2. Prior Swapping,[0],[0]
"p̃f (θ)π(θ)
πf (θ) .",2.2. Prior Swapping,[0],[0]
"(5)
Note that if p̃f (θ) = pf (θ|xn), then ps(θ) = p(θ|xn).",2.2. Prior Swapping,[0],[0]
"However, depending on how we represent p̃f (θ), ps(θ) can have a much simpler analytic representation than p(θ|xn), which is typically defined via a likelihood function (i.e. a function of the data) and causes inference algorithms to have costs that scale with the data size n. Specifically, we will only use low-complexity p̃f (θ) that can be evaluated in constant time with respect to the data size",2.2. Prior Swapping,[0],[0]
"n.
Our general strategy is to use ps(θ) as a surrogate for p(θ|xn) in standard MCMC or optimization procedures, to yield data-independent algorithms with constant cost per iteration.",2.2. Prior Swapping,[0],[0]
"Intuitively, the likelihood information is captured by the false posterior—we make use of this instead of the likelihood function, which is costly to evaluate.
More concretely, at each iteration in standard inference algorithms, we must evaluate a data-dependent function associated with the posterior density.",2.2. Prior Swapping,[0],[0]
"For example, we evaluate a function proportional to p(θ|xn) in Metropolis-Hastings (MH) (Metropolis et al., 1953), and ∇θ log p(θ|xn) in gradient-based MCMC methods (such as Langevin dynamics (LD) (Rossky et al., 1978) and Hamiltonian Monte Carlo (HMC) (Neal, 2011)) and in optimization procedures that yield a MAP point estimate.",2.2. Prior Swapping,[0],[0]
"In prior swapping, we instead evaluate ps(θ) in MH, or ∇θ log ps(θ) in LD, HMC, or gradient optimization to a MAP estimate (see appendix for algorithm pseudocode).",2.2. Prior Swapping,[0],[0]
"Here, each iteration only requires evaluating a few simple analytic expressions, and thus has O(1) complexity with respect to data size.
",2.2. Prior Swapping,[0],[0]
"We demonstrate prior swapping on our previous example (using a normal false prior and Laplace target prior) in Fig. 2, where we have a closed-form (normal PDF) p̃f (θ).",2.2. Prior Swapping,[0],[0]
"To do prior swapping, we run a Metropolis-Hastings algorithm on the target density ps(θ).",2.2. Prior Swapping,[0],[0]
"Note that drawing each
sample in this Markov chain does not involve the data xn, and can be done in constant time with respect to n (which we can see by viewing the wall time for different T ).",2.2. Prior Swapping,[0],[0]
"In Fig. 2, we draw T samples {θt}Tt=1 ∼ ps(θ), compute a sample estimate µ̂PSh = 1 T ∑T t=1 θt, and compare it with the true value µh.",2.2. Prior Swapping,[0],[0]
We see that µ̂PSh converges to µh after a relatively small number of samples T.,2.2. Prior Swapping,[0],[0]
The previous method is only applicable if our false posterior inference result is a PDF p̃f (θ) (such as in closed-form inference or variational approximations).,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Here, we develop prior swapping methods for the setting where we only have access to samples {θ̃t} Tf t=1 ∼ pf (θ|xn).",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"We propose the following procedure:
1.",2.3. Prior Swapping with False Posterior Samples,[0],[0]
Use {θ̃t} Tf t=1 to form an estimate p̃f (θ) ≈ pf (θ|xn).,2.3. Prior Swapping with False Posterior Samples,[0],[0]
2.,2.3. Prior Swapping with False Posterior Samples,[0],[0]
Sample from ps(θ) ∝,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"π(θ)p̃f (θ)πf (θ) with prior swapping, as before.
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Note that, in general, ps(θ) only approximates p(θ|xn).",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"As a final step, after sampling from ps(θ), we can:
3.",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Apply a correction to samples from ps(θ).
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"We will describe two methods for applying a correction to ps samples—one involving importance sampling, and one involving semiparametric density estimation.",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Additionally, we will discuss forms for p̃f (θ), guarantees about these forms, and how to optimize the choice of p̃f (θ).",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"In particular, we will argue why (in constrast to the initial IS strategy) these methods do not fail when p(θ|xn) and pf (θ|xn) are very dissimilar or have ill-matching parametric forms.
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
Prior swap importance sampling.,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Our first proposal for applying a correction to prior swap samples involves IS: after estimating some p̃f (θ), and sampling {θt}Tt=1 ∼ ps(θ), we can treat {θt}Tt=1 as importance samples, and compute the IS estimate
µ̂PSish = T∑ t=1 w(θt)h(θt) (6)
where the weight function is now
w(θ) ∝ p(θ|x n) ps(θ) ∝",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"pf (θ|x n) p̃f (θ) (7)
and the weights are normalized so that ∑T t=1 w(θt) = 1.
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
The key difference between this and the previous IS strategy is the weight function.,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Recall that, previously, an accurate estimate depended on the similarity between π(θ) and πf (θ); both the distance to and parametric form of π(θ) could produce high or infinite variance estimates.",2.3. Prior Swapping with False Posterior Samples,[0],[0]
This was an issue because we wanted the procedure to work well for any π(θ).,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Now, however, the performance depends on the similarity between p̃f (θ) and pf (θ|xn)—and by using the false posterior samples, we can estimate a p̃f (θ) that well approximates pf (θ|xn).",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Additionally, we can prove that certain choices of p̃f (θ) guarantee a finite variance IS estimate.",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Note that the variance of µ̂PSish is only finite if
Epf [ h(θ)2 pf (θ|xn)2
p̃f (θ)2
] ∝",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Ep [ h(θ)2
pf (θ|xn) p̃f (θ)
]",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"<∞.
To bound this, it is sufficient to show that there exists M ∈ R such that pf (θ|x
n) p̃f (θ)
< M for all θ (assuming a test function h(θ) with finite variance) (Geweke, 1989).",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"To satisfy this condition, we will propose a certain parametric family p̃αf (θ).",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Note that, to maintain a prior swapping procedure with O(1) cost, we want a p̃αf (θ) that can be evaluated in constant time.",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"In general, a p̃αf (θ) with fewer terms will yield a faster procedure.",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"With these in mind, we propose the following family of densities.
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
Definition.,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"For a parameter α = (α1, . . .",2.3. Prior Swapping with False Posterior Samples,[0],[0]
", αk), αj ∈",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Rp, k > 0, let density p̃αf (θ) satisfy
p̃αf (θ) ∝",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"πf (θ) k∏ j=1 p(αj |θ)n/k (8)
where p(αj |θ) denotes the model conditional PDF.
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"The number of terms in p̃αf (θ) (and cost to evaluate) is determined by the parameter k. Note that this family is
inspired by the true form of the false posterior pf (θ|xn).",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"However, p̃αf (θ) has constant-time evaluation, and we can estimate its parameter α using samples {θ̃t} Tf t=1 ∼ pf (θ|xn).",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Furthermore, we have the following guarantees.",2.3. Prior Swapping with False Posterior Samples,[0],[0]
Theorem 2.1.,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"For any α = (α1, . . .",2.3. Prior Swapping with False Posterior Samples,[0],[0]
", αk) ⊂",2.3. Prior Swapping with False Posterior Samples,[0],[0]
Rp and k > 0,2.3. Prior Swapping with False Posterior Samples,[0],[0]
let p̃αf (θ) be defined as in Eq.,2.3. Prior Swapping with False Posterior Samples,[0],[0]
(8).,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Then, there existsM > 0",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"such that pf (θ|x n)
p̃αf (θ) < M , for all θ ∈ Rd.
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
Corollary 2.1.1.,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"For {θt}Tt=1 ∼ pαs (θ) ∝ p̃αf (θ)π(θ) πf (θ) , w(θt) = pf (θt|xn) p̃αf (θt) (∑T r=1",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"pf (θr|xn) p̃αf (θr) )−1 , and test function that satisfies Varp [h(θ)]",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"< ∞, the variance of IS estimate µ̂PSish = ∑T t=1 h(θt)w(θt) is finite.
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Proofs for these theorems are given in the appendix.
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
Note that we do not know the normalization constant for p̃αf (θ).,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"This is not an issue for its use in prior swapping, since we only need access to a function proportional to pαs (θ) ∝",2.3. Prior Swapping with False Posterior Samples,[0],[0]
p̃αf (θ)π(θ)πf (θ)−1 in most MCMC algorithms.,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"However, we still need to estimate α, which is an issue because the unknown normalization constant is a function of α.",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Fortunately, we can use the method of score matching (Hyvärinen, 2005) to estimate α given a density such as p̃αf (θ) with unknown normalization constant.
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Once we have found an optimal parameter α∗, we draw samples from pα ∗ s (θ) ∝ p̃α ∗ f (θ)π(θ)πf (θ) −1, compute weights for these samples (Eq. (7)), and compute the IS estimate µ̂PSish .",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"We give pseudocode for the full prior swap importance sampling procedure in Alg. 1.
Algorithm 1:",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Prior Swap Importance Sampling
Input: False posterior samples {θ̃t} Tf t=1 ∼ pf (θ|xn).
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
Output: IS estimate µ̂PSish .,2.3. Prior Swapping with False Posterior Samples,[0],[0]
1 Score matching: estimate α∗ using {θ̃t} Tf t=1.,2.3. Prior Swapping with False Posterior Samples,[0],[0]
2,2.3. Prior Swapping with False Posterior Samples,[0],[0]
Prior swapping: sample {θt}Tt=1 ∼ pα ∗ s (θ) ∝,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"p̃α ∗ f (θ)π(θ) πf (θ) .
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"3 Importance sampling: compute µ̂PSish = ∑T t=1 h(θt)w(θt).
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
Semiparametric prior swapping.,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"In the previous method, we chose a parametric form for p̃αf (θ); in general, even the optimal α will yield an inexact approximation to pf (θ|xn).",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Here, we aim to incorporate methods that return an increasingly exact estimate p̃f (θ) when given more false posterior samples {θ̃t} Tf t=1.
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
One idea is to use a nonparametric kernel density estimate p̃npf (θ) and plug this into p np s (θ) ∝,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"p̃ np f (θ)π(θ)πf (θ)
−1.",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"However, nonparametric density estimates can yield inaccurate density tails and fare badly in high dimensions.",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"To help mitigate these problems, we turn to a semiparametric estimate, which begins with a parametric estimate, and
adjusts it as samples are generated.",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"In particular, we use a density estimate that can be viewed as the product of a parametric density estimate and a nonparametric correction function (Hjort & Glad, 1995).",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"This density estimate is consistent as the number of samples Tf → ∞. Instead of (or in addition to) correcting prior swap samples with importance sampling, we can correct them by updating the nonparametric correction function as we continue to generate false posterior samples.
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Given Tf samples {θ̃t} Tf t=1 ∼ pf (θ|xn), we write the semiparametric false posterior estimate as
p̃spf (θ) = 1
Tf Tf∑ t=1
[ 1
bd K
( ‖θ − θ̃t‖
b
) p̃αf (θ)
p̃αf (θ̃t)
] , (9)
where K denotes a probability density kernel, with bandwidth b, where b→ 0 as Tf →∞ (see (Wasserman, 2006) for details on probability density kernels and bandwidth selection).",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"The semiparametric prior swap density is then
psps (θ) ∝",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"p̃spf (θ)π(θ)
πf (θ) =
1
Tf Tf∑ t=1
K ( ‖θ−θ̃t‖
b )",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"p̃αf (θ)π(θ)
p̃αf (θ̃t)πf (θ)b d
∝",2.3. Prior Swapping with False Posterior Samples,[0],[0]
[pαs (θ)]  1 Tf Tf∑ t=1,2.3. Prior Swapping with False Posterior Samples,[0],[0]
K,2.3. Prior Swapping with False Posterior Samples,[0],[0]
(,2.3. Prior Swapping with False Posterior Samples,[0],[0]
‖θ−θ̃t‖ b ) p̃αf (θ̃t)  .,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"(10) Hence, the prior swap density psps (θ) is proportional to the product of two densities: the parametric prior swap density pαs (θ), and a correction density.",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"To estimate expectations with respect to psps (θ), we can follow Alg.",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"1 as before, but replace the weight function in the final IS estimate with
w(θ) ∝",2.3. Prior Swapping with False Posterior Samples,[0],[0]
p sp s (θ) pαs (θ) ∝,2.3. Prior Swapping with False Posterior Samples,[0],[0]
1,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Tf Tf∑ t=1
K ( ‖θ−θ̃t‖
b ) p̃αf (θ̃t) .",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"(11)
One advantage of this strategy is that computing the weights doesn’t require the data—it thus has constant cost with respect to data size n (though its cost does increase with the number of false posterior samples Tf ).",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Additionally, as in importance sampling, we can prove that this procedure yields an exact estimate of E[h(θ)], asymptotically, as Tf → ∞ (and we can provide an explicit bound on the rate at which psps (θ) converges to p(θ|xn)).",2.3. Prior Swapping with False Posterior Samples,[0],[0]
We do this by showing that psps (θ) is consistent for p(θ|xn).,2.3. Prior Swapping with False Posterior Samples,[0],[0]
Theorem 2.2.,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"Given false posterior samples {θ̃t} Tf t=1 ∼ pf (θ|xn) and b T−1/(4+d)f , the estimator psps is consistent for p(θ|xn), i.e. its mean-squared error satisfies
sup p(θ|xn)
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"E [∫
(psps (θ)− p(θ|xn))",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"2 dθ
] <
c
T 4/(4+d)",2.3. Prior Swapping with False Posterior Samples,[0],[0]
"f
for some c > 0 and 0 < b ≤ 1.
",2.3. Prior Swapping with False Posterior Samples,[0],[0]
The proof for this theorem is given in the appendix.,2.3. Prior Swapping with False Posterior Samples,[0],[0]
"We show empirical results on Bayesian generalized linear models (including linear and logistic regression) with sparsity and heavy tailed priors, and on latent factor models (including mixture models and topic models) with relational priors over factors (e.g. diversity-encouraging, agglomerate-encouraging, etc.).",3. Empirical Results,[0],[0]
"We aim to demonstrate empirically that prior swapping efficiently yields correct samples and, in some cases, allows us to apply certain inference algorithms to more-complex models than was previously possible.",3. Empirical Results,[0],[0]
"In the following experiments, we will refer to the following procedures:
• Target posterior inference: some standard inference algorithm (e.g. MCMC) run on p(θ|xn).
",3. Empirical Results,[0],[0]
"• False posterior inference: some standard inference algorithm run on pf (θ|xn).
",3. Empirical Results,[0],[0]
"• False posterior IS: IS using samples from pf (θ|xn).
",3. Empirical Results,[0],[0]
"• Prior swap exact: prior swapping with closed-form p̃f (θ) = pf (θ|xn).
",3. Empirical Results,[0],[0]
• Prior swap parametric: prior swapping with parametric p̃αf (θ) given by Eq.,3. Empirical Results,[0],[0]
"(8).
",3. Empirical Results,[0],[0]
"• Prior swap IS: correcting samples from p̃αf (θ) with IS.
",3. Empirical Results,[0],[0]
"• Prior swap semiparametric: correcting samples from p̃αf (θ) with the semiparametric estimate IS procedure.
",3. Empirical Results,[0],[0]
"To assess performance, we choose a test function h(θ), and compute the Euclidean distance between µh = Ep[h(θ)] and some estimate µ̂h returned by a procedure.",3. Empirical Results,[0],[0]
We denote this performance metric by posterior error = ‖µh − µ̂h‖2.,3. Empirical Results,[0],[0]
"Since µh is typically not available analytically, we run a single chain of MCMC on the target posterior for one million steps, and use these samples as ground truth to compute µh.",3. Empirical Results,[0],[0]
"For timing plots, to assess error of a method at a given time point, we collect samples drawn before this time point, remove the first quarter as burn in, and add the time it takes to compute any of the corrections.",3. Empirical Results,[0],[0]
Sparsity-encouraging regularizers have gained a high level of popularity over the past decade due to their ability to produce models with greater interpretability and parsimony.,3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"For example, the L1 norm has been used to induce sparsity with great effect (Tibshirani, 1996), and has been shown to be equivalent to a mean-zero independent Laplace prior (Tibshirani, 1996; Seeger, 2008).",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"In a Bayesian setting, inference given a sparsity prior can be difficult, and often requires a computationally intensive method (such as MH or
HMC) or posterior approximations (e.g. expectation propagation (Minka, 2001)) that make factorization or parametric assumptions (Seeger, 2008; Gerwinn et al., 2010).",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"We propose a cheap yet accurate solution: first get an inference result with a more-tractable prior (such as a normal prior), and then use prior swapping to quickly convert the result to the posterior given a sparsity prior.
",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"Our first set of experiments are on Bayesian linear regression models, which we can write as yi =",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"Xiθ + , ∼ N (0, σ2), θ ∼ π, i = 1,...,n.",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"For π, we compute results on Laplace, Student’s t, and VerySparse (with PDF VerySparse(σ) = ∏d i=1 1 2σ exp{−|θi|
0.4/σ} (Seeger, 2008)) priors.",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"Here, a normal πf is conjugate and allows for exact false posterior inference.",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"Our second set of experiments are on Bayesian logistic regression models, which we write as yi ∼ Bern(pi), pi = logistic(Xiθ), θ ∼ π, i = 1,...,n.",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"which we will pair with both heavy tailed priors and a hierarchical target prior π = N (0, α−1I), α ∼ Gamma(γ, 1).",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"For these experiments, we also use a normal πf .",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"However, this false prior is no longer conjugate, and so we use MCMC to sample from pf (θ|xn).
",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"For linear regression, we use the YearPredictionMSD data set*, (n = 515345, d = 90), in which regression is used to predict the year associated with a a song, and for logistic regression we use the MiniBooNE particle identification data set†, (n = 130065, d = 50), in which binary classification is used to distinguish particles.
",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"In Fig. 3, we compare prior swapping and IS methods, in order to show that the prior swapping procedures yield accurate posterior estimates, and to compare their speeds of convergence.",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
We plot posterior error vs. wall time for each method’s estimate of the posterior mean Ep[h(θ)],3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"= Ep[θ] for two sparsity target priors (Laplace and VerySparse), for both linear and logistic regression.",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"In linear regression (only), since the normal conjugate πf allows us to compute a closed form pf (θ|xn), we can run the prior swap exact method, where p̃f (θ) = pf (θ|xn).",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"However, we can also sample from pf (θ|xn) to compute p̃α ∗
f (θ), and therefore compare methods such as prior swap parametric and the two correction methods.",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"In logistic regression, we do not have a closed form pf (θ|xn); here, we only compare the methods that make use of samples from pf (θ|xn).",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"In Fig. 3, we see that the prior swapping methods (particularly prior swap IS) quickly converge to nearly zero posterior error.",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"Additionally, in linear regression, we see that prior swap parametric, using p̃f (θ) = p̃α ∗
f (θ), yields similar posterior error as prior swap exact, which uses p̃f (θ) = p(θ|xn).
",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"*https://archive.ics.uci.edu/ml/datasets/ YearPredictionMSD
†https://archive.ics.uci.edu/ml/datasets/ MiniBooNE+particle+identification
In Fig. 4, we show how prior swapping can be used for fast inference in Bayesian linear models with sparsity or heavy-tailed priors.",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"We plot the time needed to first compute the false posterior (via exact inference) and then run prior swapping (via the MH procedure) on some target posterior, and compare this with the MH algorithm run directly on the target posterior.",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
In (a) and (b) we show convergence plots and see that prior swapping performs faster inference (by a few orders of magnitude) than direct MH.,3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"In plot (b) we reduce the variance of the target prior; while this hurts the accuracy of false posterior IS, prior swapping still quickly converges to zero error.",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"In (c) we show 1-d density marginals as we increase the prior sparsity, and in (d) we show prior swapping results for various sparsity priors.
",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"In the appendix, we also include results on logistic regression with the hierarchical target prior, as well as results for synthetic data where we are able to compare timing and posterior error as we tune n and d.",3.1. Sparsity Inducing and Heavy Tailed Priors in Bayesian Generalized Linear Models,[0],[0]
"Many latent variable models in machine learning—such as mixture models, topic models, probabilistic matrix factorization, and others—involve a set of latent factors (e.g. components or topics).",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"Often, we’d like to use priors that encourage interesting behaviors among the factors.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"For example, we might want dissimilar factors through a diversity-promoting prior (Kwok & Adams, 2012; Xie et al., 2016) or for the factors to show some sort of sparsity pattern (Mayrink et al., 2013; Knowles & Ghahramani, 2011).",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"Inference in such models is often computationally expensive or designed on a case-by-case basis (Xie et al., 2016; Knowles & Ghahramani, 2011).
",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"However, when conjugate priors are placed over the factor parameters, collapsed Gibbs sampling can be applied.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"In this method, the factor parameters are integrated out, leaving only a subset of variables; on these, the conditional
(a) (b)
",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"Collapsed Gibbs Prior Swapping
W al
",3.2. Priors over Factors in Latent Variable Models,[0],[0]
l t,3.2. Priors over Factors in Latent Variable Models,[0],[0]
"im
e (s
ec o
n d
s)
0
0.5
1
1.5
2
2.5
3x 10 4
1.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
southern 2.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
northern 3.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
region 4.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
western 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
eastern 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
"south
Topic 6
Topic Model: False Posterior via Collapsed Gibbs
C lu
st er : G eo",3.2. Priors over Factors in Latent Variable Models,[0],[0]
gr,3.2. Priors over Factors in Latent Variable Models,[0],[0]
ap h y C,3.2. Priors over Factors in Latent Variable Models,[0],[0]
"lu st er : F am il y
1.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
west 2.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
south 3.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
coast 4.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
north 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
east 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
western Topic 171 1.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
north 2.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
asia 3.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
south 4.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
western 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
southern 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
eastern,3.2. Priors over Factors in Latent Variable Models,[0],[0]
"Topic 285
1.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
father 2.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
family 3.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
brother 4. born 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
son 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
children,3.2. Priors over Factors in Latent Variable Models,[0],[0]
Topic 11 1.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
children 2.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
daughter 3. born 4.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
son 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
family 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
father Topic 243 1. born 2. died 3.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
father 4.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
years 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
family 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
lived Topic 280 1. born 2.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
parents 3.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
studied 4. moved 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
age 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
year,3.2. Priors over Factors in Latent Variable Models,[0],[0]
"Topic 306
1.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
north 2.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
west 3.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
east 4.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
south 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
eastern 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
"western
Topic 353
Topic Model: Target Posterior via Prior Swapping
1.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
brother 2.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
sister 3.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
younger 4.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
older 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
youngest 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
sisters,3.2. Priors over Factors in Latent Variable Models,[0],[0]
Topic 11 1.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
husband 2.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
marriage 3.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
wife 4.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
death 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
marry 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
children,3.2. Priors over Factors in Latent Variable Models,[0],[0]
Topic 243 1.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
school 2.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
college 3.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
graduated 4.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
studies 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
university 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
fellow,3.2. Priors over Factors in Latent Variable Models,[0],[0]
Topic 306Topic 280 1.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
important 2.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
stayed 3.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
wrote 4. travelled 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
started 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
"died
1.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
territory 2.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
region 3.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
regions 4.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
provinces 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
capital 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
"territories
Topic 6
T er
ri to
ri es 1.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"bay
2.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
south 3.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
coast 4.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
area 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
land 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
"sea
Topic 171
C oa
st
1.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
america 2.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
europe 3.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
asia 4.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
world 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
countries 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
"africa
Topic 285
C ou
n tr
ie s 1.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"side
2.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
east 3.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
bordered 4.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
west 5.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
middle 6.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
"border
Topic 353
B or
d er
Si b
li n
gs
M ar
ri ag
e
B io
gr ap
h y
Sc h
oo",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"l
Pr io
r sw
ap p
in g
fo r
di ve
rs e
to p
ic s.
= D
iv er
se (0
.5 )
",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"(c)
Relational target priors (over factors )
−3 −2 −1 0 1 2 3 4
−3
−2
−1
0
1
2
3
4
−3 −2 −1 0 1 2 3 4
−3
−2
−1
0
1
2
3
4
−3 −2 −1 0 1 2 3 4
−3
−2
−1
0
1
2
3
4
−3 −2 −1 0 1 2 3 4
−3
−2
−1
0
1
2
3
4
−3 −2 −1 0 1 2 3 4
−3
−2
−1
0
1
2
3
4
−3 −2 −1 0 1 2 3 4
−3
−2
−1
0
1
2
3
4
−3 −2 −1 0 1 2 3 4
−3
−2
−1
0
1
2
3
4
−3 −2 −1 0 1 2 3 4
−3
−2
−1
0
1
2
3
4
−3 −2 −1 0 1 2 3 4
−3
−2
−1
0
1
2
3
4 3. Chain(0.1)
7. SparseChain(0.1) 8.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"Origin+Diverse(0.1)6. SparseAgglom(0.1)
",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"False-Posterior 1. Origin(0.1) 2. Agglom(0.1) 4. Diverse(0.1)
5.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"SparseOrigin(0.1)Wall time (seconds)
",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"Mixture Model: False Posterior via Collapsed Gibbs, Target Posterior via Prior Swapping
Figure 5.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
Latent factor models: (a) Prior swapping results for relational target priors (defined in (b)) over components in a mixture model.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
(c) Prior swapping with a diversity-promoting target prior on an LDA topic model (Simple English Wikipedia corpus) to separate redundant topic clusters; the top 6 words per topic are shown.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
"In (a, c)",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"we show wall times for the initial inference and prior swapping.
distributions can be computed analytically, which allows for Gibbs sampling over these variables.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"Afterwards, samples of the collapsed factor parameters can be computed.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"Hence, we propose the following strategy: first, assign a prior for the factor parameters that allows for collapsed Gibbs sampling; afterwards, reconstruct the factor samples and apply prior swapping for more complex relational priors over the factors.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"We can thus perform convenient inference in the collapsed model, yet apply more-sophisticated priors to variables in the uncollapsed model.
",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"We first show results on a Gaussian mixture model (GMM), written xi ∼ N (µzi ,Σzi), zi ∼ Dir(α), {µm}Mm=1 ∼ π, i = 1,...,n.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
Using a normal πf over {µm}Mm=1 allows for collapsed Gibbs sampling.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
"We also show results on a topic model (latent Dirichlet allocation (LDA) (Blei et al., 2003)) for text data (for the form of this model, see (Blei et al., 2003; Wang & Blei, 2011)).",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"Here, using a Dirichlet πf over topics allows for collapsed Gibbs sampling.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"For mixture models, we generate synthetic data from the above model (n=10,000, d=2, M=9), and for topic models, we use the Simple English Wikipedia‡ corpus (n=27,443 documents, vocab=10,192 words), and set M=400 topics.
‡https://simple.wikipedia.org/
In Fig. 5, we show results for mixture and topic models.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"In (a) we show inferred posteriors over GMM components for a number of relational target priors, which we define in (b).",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"In (c), we apply the diversity-promoting target prior to LDA, to separate redundant topics.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
"Here, we show two topic clusters (“geography” and “family”) in pf (θ|xn), which are separated into distinct, yet thematically-similar, topics after prior swapping.",3.2. Priors over Factors in Latent Variable Models,[0],[0]
In (a) and (c) we also show wall times of the inference methods.,3.2. Priors over Factors in Latent Variable Models,[0],[0]
"Given some false posterior inference result, and an arbitrary target prior, we have studied methods to accurately compute the associated target posterior (or expectations with respect to it), and to do this efficiently by leveraging the pre-inferred result.",4. Conclusion,[0],[0]
We have argued and shown empirically that this strategy is effective even when the false and target posteriors are quite dissimilar.,4. Conclusion,[0],[0]
"We believe that this strategy shows promise to allow a wider range of (and possibly less-costly) inference alorithms to be applied to certain models, and to allow updated or new prior information to be more-easily incorporated into models without re-incurring the full costs of standard inference algorithms.",4. Conclusion,[0],[0]
"While Bayesian methods are praised for their ability to incorporate useful prior knowledge, in practice, convenient priors that allow for computationally cheap or tractable inference are commonly used.",abstractText,[0],[0]
"In this paper, we investigate the following question: for a given model, is it possible to compute an inference result with any convenient false prior, and afterwards, given any target prior of interest, quickly transform this result into the target posterior?",abstractText,[0],[0]
A potential solution is to use importance sampling (IS).,abstractText,[0],[0]
"However, we demonstrate that IS will fail for many choices of the target prior, depending on its parametric form and similarity to the false prior.",abstractText,[0],[0]
"Instead, we propose prior swapping, a method that leverages the pre-inferred false posterior to efficiently generate accurate posterior samples under arbitrary target priors.",abstractText,[0],[0]
"Prior swapping lets us apply less-costly inference algorithms to certain models, and incorporate new or updated prior information “post-inference”.",abstractText,[0],[0]
"We give theoretical guarantees about our method, and demonstrate it empirically on a number of models and priors.",abstractText,[0],[0]
Post-Inference Prior Swapping,title,[0],[0]
"We study the design of practically useful, theoretically wellfounded, general-purpose algorithms for the contextual bandits (CBs) problem.",1. Introduction,[0],[0]
"In this setting, the learner repeatedly receives context, then selects an action, resulting in a received reward.",1. Introduction,[0],[0]
"The aim is to learn a policy, a mapping from contexts to actions, to maximize the long-term cumulative reward.",1. Introduction,[0],[0]
"For instance, a news portal must repeatedly choose articles to present to each user to maximize clicks.",1. Introduction,[0],[0]
"Here, the context is information about the user, the actions are the articles, and the reward might be indicator of a click.",1. Introduction,[0],[0]
"We refer the reader to an ICML 2017 tutorial (http://hunch.net/
˜
rwil/) for further examples.
CB algorithms can be put into two groups.",1. Introduction,[0],[0]
"Some methods (Langford & Zhang, 2008; Agarwal et al., 2014) are
1Cornell University.",1. Introduction,[0],[0]
Work performed while the author was an intern at Microsoft Research.,1. Introduction,[0],[0]
2Microsoft Research 3University of Southern California.,1. Introduction,[0],[0]
"Correspondence to: Dylan J. Foster <djf244@cornell.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
agnostic in the sense that they are provably effective for any given policy class and data distribution.,1. Introduction,[0],[0]
"In contrast, realizability-based approaches such as LinUCB and variants (Chu et al., 2011; Li et al., 2017; Filippi et al., 2010) or Thompson sampling (Thompson, 1933) assume the data is generated from a particular parametrized family of models.",1. Introduction,[0],[0]
"Computationally tractable realizability-based algorithms are only known for specific model families, such as when the conditional reward distributions come from a generalized linear model.
",1. Introduction,[0],[0]
The two groups of approaches seem to have different advantages and disadvantages.,1. Introduction,[0],[0]
"Empirically, in the contextual semibandit setting, Krishnamurthy et al. (2016) found that the realizability-based LinUCB approach outperforms all agnostic baselines using a linear policy class.",1. Introduction,[0],[0]
"However, the agnostic approaches were able to overcome this shortcoming by using a more powerful policy class.",1. Introduction,[0],[0]
"Computationally, previous realizability-based approaches have been limited by their reliance on either closed-form confidence bounds (as in LinUCB variants), or the ability to efficiently sample from and frequently update the posterior (as in Thompson sampling).",1. Introduction,[0],[0]
"Agnostic approaches, on the other hand, typically assume an oracle for cost-sensitive classification, which is computationally intractable in the worst case, but often practically feasible for many natural policy classes.
",1. Introduction,[0],[0]
"In this paper, we aim to develop techniques that combine the best of both of these approaches.",1. Introduction,[0],[0]
"To this end, in Section 3, we propose computationally efficient and practical realizability-based algorithms for arbitrary model classes.",1. Introduction,[0],[0]
"As is often done in agnostic approaches, we assume the availability of an oracle which reduces to a standard learning setting and knows how to efficiently leverage the structure of the model class.",1. Introduction,[0],[0]
"Specifically, we require access to a leastsquares regression oracle over the model class that we use for predicting rewards given contexts.",1. Introduction,[0],[0]
"Since regression can often be solved efficiently, the availability of such an oracle is a far more reasonable assumption than the cost-sensitive classification oracle usually assumed, which typically must solve NP-hard problems.",1. Introduction,[0],[0]
"In fact, for this reason, even the classification oracles are typically approximated by regression oracles in practice (see, e.g., Beygelzimer & Langford, 2009).",1. Introduction,[0],[0]
"Our main algorithmic components here are motivated by and adapted from a recent work of Krishnamurthy et al. (2017) on cost-sensitive active learning.
",1. Introduction,[0],[0]
"In Section 4, we prove that our algorithms are effective in achieving low regret under certain distributional assumptions.",1. Introduction,[0],[0]
"Specifically, we show that our methods enjoy low regret so long as certain quantities like the disagreement coefficient (Hanneke, 2014; Krishnamurthy et al., 2017) are bounded, or when some other distributional coefficients inspired by Bastani & Bayati (2015) are well-behaved.",1. Introduction,[0],[0]
"As a special consequence, we obtain nearly dimension-free results for sparse linear bandits in high dimensions.
",1. Introduction,[0],[0]
"Finally, in Section 5, we conduct a very extensive empirical evaluation of our algorithms on a number of datasets and against both realizability-based and agnostic baselines.",1. Introduction,[0],[0]
"In this test of practical effectiveness, we find that our approach gives comparable or superior results in nearly all cases, and we also validate the distributional assumptions required for low-regret guarantees on these datasets.",1. Introduction,[0],[0]
We consider the following contextual bandit protocol.,2. Preliminaries,[0],[0]
"Contexts are drawn from an arbitrary space, x ∈ X , actions are from a finite set, a ∈",2. Preliminaries,[0],[0]
"A ∶= {1, . . .",2. Preliminaries,[0],[0]
",K}, for some fixed K, and reward vectors are from a bounded set, r ∈",2. Preliminaries,[0],[0]
"[0,1]K , with component r(a) denoting the reward for action a ∈ A.",2. Preliminaries,[0],[0]
We consider an i.i.d.,2. Preliminaries,[0],[0]
"setting where there is a fixed and unknown distribution D over the context-reward pairs (x, r), with DX denoting its marginal over X .",2. Preliminaries,[0],[0]
"At each round t = 1,2, . . .",2. Preliminaries,[0],[0]
", T , nature samples (xt, rt) according to D and reveals xt to the learner.",2. Preliminaries,[0],[0]
The learner chooses an action at ∈ A and observes the reward rt(at).,2. Preliminaries,[0],[0]
The learner aims to maximize its reward and compete with strategies that model the expected reward E[r(a)  ,2. Preliminaries,[0],[0]
"x, a] via functions f ∶ X ×A → [0,1].",2. Preliminaries,[0],[0]
"We consider mappings f from a given class F , such as linear predictors or regression trees.",2. Preliminaries,[0],[0]
"The main assumption this paper follows is that the class F is rich enough to contain a predictor that perfectly predicts the expected reward of any action under any context, that is: Assumption 1 (Realizability).",2. Preliminaries,[0],[0]
There is a predictor f ∈ F such that E[r(a)  ,2. Preliminaries,[0],[0]
"x, a] = f(x, a) ∀x ∈ X , a ∈ A.",2. Preliminaries,[0],[0]
"This assumption is used by essentially all regression-based contextual bandit algorithms (Chu et al., 2011; Filippi et al., 2010; Russo & Van Roy, 2013; Li et al., 2017).",2. Preliminaries,[0],[0]
"Given a predictor f ∈ F , the associated optimal strategy ⇡f ∶ X → A, called a policy, picks the action with the highest predicted reward, i.e., ⇡f(x) ∶= argmaxa∈A f(x, a) (ties broken arbitrarily).",2. Preliminaries,[0],[0]
"Using ⇡ ∶= ⇡f to denote an optimal policy, the learner aims to minimize its regret
RegT = ∑Tt=1",2. Preliminaries,[0],[0]
"rt(⇡(xt)) −∑Tt=1 rt(at), which compares the accumulated rewards between the optimal policy and the learner’s strategy.",2. Preliminaries,[0],[0]
"The classic Exp4
algorithm (Auer et al., 2002b) achieves an optimal regret bound of order O(TK ln F ) (for any finite F), but the computational complexity is unfortunately linear in F .",2. Preliminaries,[0],[0]
"Regression Oracle To overcome the computational obstacle, our algorithms reduce the contextual bandit problem to weighted least-squares regression.",2. Preliminaries,[0],[0]
"Abstracting the computational complexity, we assume access to a weighted least-squares regression oracle over the predictor class F , which takes any set H of weighted examples (w,x, a, y) ∈ R+ ×X ×A ×",2. Preliminaries,[0],[0]
"[0,1] as input, and outputs the predictor with the smallest weighted squared loss: ORACLE(H) = argminf∈F ∑(w,x,a,y)∈H w(f(x, a)− y)2.",2. Preliminaries,[0],[0]
"As mentioned, such regression tasks are very common in machine learning practice and the availability of such oracle is thus a very mild assumption.",2. Preliminaries,[0],[0]
The high-level idea of our algorithms is the following.,3. Algorithms,[0],[0]
"As data is collected, we maintain a subset of F , referred to as the version space, that only contains f ∈ F with small squared loss on observed data.",3. Algorithms,[0],[0]
"For a new example, we construct a confidence interval for the expected reward of each action based on this version space.",3. Algorithms,[0],[0]
"Finally, with these confidence intervals, we either optimistically pick the action with the highest upper bound, similar to UCB and LinUCB, or randomize among all actions that are potentially the best.
",3. Algorithms,[0],[0]
"The challenge here is to maintain such version spaces and compute upper and lower confidence bounds efficiently, and we show that this can be done using a binary search together with a small number of regression oracle calls.
",3. Algorithms,[0],[0]
"More formally, we define the upper and lower bounds on the expected reward with respect to a subset F ′ ⊆ F as HIGHF ′(x, a) =max
f∈F ′ f(x, a), LOWF ′(x, a) = minf∈F ′ f(x, a).",3. Algorithms,[0],[0]
"Our algorithms will induce the confidence bounds by instantiating these quantities using the version space as F ′. To reduce computation, our algorithms update on a doubling epoch schedule.",3. Algorithms,[0],[0]
There are M = O(logT ) epochs and each epoch m begins at time ⌧m = 2m−1.,3. Algorithms,[0],[0]
"At epoch m our algorithms (implicitly) construct a version space Fm ⊆ F , and then select an action based on the reward ranges defined by HIGHFm(x, a) and LOWFm(x, a) for each time t that falls into epoch m. Specifically, we consider two algorithm variants: the first one uniformly at random picks from actions that are plausible to be the best (see lines 6-7 in Algorithm 1); the second one simply behaves optimistically and picks the action with the highest upper bound (see line 9 in Algorithm 2).",3. Algorithms,[0],[0]
"For technical reasons, the optimistic variant also performs pure exploration in the first few epochs to warm-start the algorithm.
",3. Algorithms,[0],[0]
Algorithm 1 REGCB.ELIMINATION 1:,3. Algorithms,[0],[0]
"Input: square-loss tolerance m 2: for epoch m = 1, . . .",3. Algorithms,[0],[0]
",M do 3:",3. Algorithms,[0],[0]
"Fm ←
 ∏a∈A Ĝm( m, a) (OPTION I)F̂m( m) (OPTION II)
4: for time t = ⌧m, . . .",3. Algorithms,[0],[0]
", ⌧m+1 − 1 do 5: Receive xt and define At as: 6: {a ∶ HIGHFm(xt, a) ≥maxa′∈A LOWFm(xt, a′)}.",3. Algorithms,[0],[0]
7: Sample at ∼ Unif (At) and receive rt(at).,3. Algorithms,[0],[0]
"8: end for 9: end for
To construct these version spaces, we further introduce the following least-squares notation for any m ≥ 2:
• R̂m(f)",3. Algorithms,[0],[0]
"= 1⌧m−1 ∑s<⌧mf(xs, as) − rs(as)2, • F̂m( ) = f ∈ F  R̂m(f) −minf∈F R̂m(f) ≤ ,
and also let F̂ 1 ( ) = F for any .",3. Algorithms,[0],[0]
"With this notation Fm is simply set to F̂( m) for some m, and HIGHFm and LOWFm recover the confidence bounds in UCB (Auer et al., 2002a) and LinUCB (Chu et al., 2011) for appropriate m.
Product Classes",3. Algorithms,[0],[0]
"Sometimes it is desirable to have a product predictor class, that is, F = GA, where G ∶ X →",3. Algorithms,[0],[0]
"[0,1] is a “base class” and each f ∈ F , described by a K-tuple(ga)a∈A where ga ∈ G, predicts according to f(x, a) = ga(x).",3. Algorithms,[0],[0]
"Similar to the general case, we define: • R̂m(g, a) = 1⌧m−1 ∑s<⌧m(g(xs)",3. Algorithms,[0],[0]
"− rs(as))21{as = a}, • Ĝm( , a) = g ∈ G  R̂m(g, a) −ming∈G R̂m(g, a) ≤ , and let Ĝ
1 ( , a) = G for any .",3. Algorithms,[0],[0]
In this case we constructFm as∏a∈A,3. Algorithms,[0],[0]
"Ĝm( m, a) for some tolerance parameter m. Our two procedures are described in Algorithms 1 and 2.",3. Algorithms,[0],[0]
Algorithms 1 and 2 hinge on the computation of the bounds HIGHFm and LOWFm .,3.1. Efficient Reward-Range Computation,[0],[0]
"This can be carried out efficiently via a small number of calls to the regression oracle.
",3.1. Efficient Reward-Range Computation,[0],[0]
"Specifically, to calculate the confidence bounds for a given pair (x, a), we augment the data set Hm with a single example (x, a, r) with a weight w.",3.1. Efficient Reward-Range Computation,[0],[0]
For the upper bound HIGHFm we use r = 2; for the lower bound r = −1,3.1. Efficient Reward-Range Computation,[0],[0]
(these values are chosen as they lie outside the reward range).,3.1. Efficient Reward-Range Computation,[0],[0]
"By changing the weight w, we trade-off the loss on this single example against that on the history Hm.",3.1. Efficient Reward-Range Computation,[0],[0]
"The binary search over w identifies—up to a given precision—the weight w at which
Algorithm 2 REGCB.OPTIMISTIC 1:",3.1. Efficient Reward-Range Computation,[0],[0]
"Input: square-loss tolerance m
number of warm-start epochs M 0
2: for time t = 1, . . .",3.1. Efficient Reward-Range Computation,[0],[0]
", ⌧M0 − 1 do 3: Receive xt, play at ∼ Unif (A), and receive rt(at).",3.1. Efficient Reward-Range Computation,[0],[0]
"4: end for 5: for epoch m =M
0 , . . .",3.1. Efficient Reward-Range Computation,[0],[0]
",M do 6: Fm ← F̂m( m).",3.1. Efficient Reward-Range Computation,[0],[0]
"7: for time t = ⌧m, . . . ,",3.1. Efficient Reward-Range Computation,[0],[0]
"⌧m+1 − 1 do 8: Receive xt. 9: Select at = argmaxa∈A HIGHFm(xt, a).
10: Receive rt(at).",3.1. Efficient Reward-Range Computation,[0],[0]
"11: end for 12: end for
the empirical regret on Hm is exactly the desired tolerance , with the corresponding prediction on x, a yielding HIGHF̂m( )(x, a) or LOWF̂m( )(x, a) (see Algorithm 3).",3.1. Efficient Reward-Range Computation,[0],[0]
In Appendix A.1 we prove that this strategy works as intended and in O(log(1↵)) iterations computes the confidence bounds up to a precision of ↵.,3.1. Efficient Reward-Range Computation,[0],[0]
Theorem 1.,3.1. Efficient Reward-Range Computation,[0],[0]
"Let Hm = {(xs, as, rs(as))}⌧m−1s=1 .",3.1. Efficient Reward-Range Computation,[0],[0]
"If the function class F is convex and closed under pointwise convergence, then the calls
zHIGH ← BINSEARCH(HIGH, (x, a),Hm, ,↵) zLOW",3.1. Efficient Reward-Range Computation,[0],[0]
"← BINSEARCH(LOW, (x, a),Hm, ,↵)
terminate after O(log(1↵))",3.1. Efficient Reward-Range Computation,[0],[0]
"oracle invocations and HIGHF̂m( )(x, a) − zHIGH ≤ ↵, LOWF̂m( )(x, a) − zLOW ≤ ↵.
",3.1. Efficient Reward-Range Computation,[0],[0]
"Compared to the procedure from Krishnamurthy et al. (2017), Algorithm 3 is much simpler and achieves an exponential improvement in terms of oracle calls, namely O(log(1↵))",3.1. Efficient Reward-Range Computation,[0],[0]
"as opposed to O(1↵), when F is convex.",3.1. Efficient Reward-Range Computation,[0],[0]
"Compared to oracles for cost-sensitive classification, convexity is not a strong assumption for regression oracles.",3.1. Efficient Reward-Range Computation,[0],[0]
"When F is not convex, reward bounds can be computed in O(1↵) oracle calls (see Krishnamurthy et al. 2017).",3.1. Efficient Reward-Range Computation,[0],[0]
In this section we provide regret guarantees for RegCB (Algorithm 1 and Algorithm 2).,4. Regret Guarantees,[0],[0]
"Note that RegCB is not minimax optimal: while it can obtain OKT logF  regret or even logarithmic regret under certain distributional assumptions, which we describe shortly, for some instances it can make as many as F  mistakes, which is suboptimal: Proposition 1.",4. Regret Guarantees,[0],[0]
"For every ✏ ∈ (0,1] and N ∈ N there exists a class of reward predictors satisfying Assumption 1 with
Algorithm 3 BINSEARCH 1: Input: bound type ∈ {LOW,HIGH}, target pair (x, a)
history H , radius > 0, precision ↵ > 0 2:",4. Regret Guarantees,[0],[0]
"Based on bound type: r←2 if HIGH and r←−1 if LOW 3: Let R(f) ∶= ∑(x′,a′,r′)∈H(f(x′, a′)",4. Regret Guarantees,[0],[0]
"− r′)2 4: Let R̃(f,w) ∶= R(f) + w
2 (f(x, a)",4. Regret Guarantees,[0],[0]
"− r)2 5: wL ← 0, wH",4. Regret Guarantees,[0],[0]
"← ↵
//",4. Regret Guarantees,[0],[0]
"Invoke oracle twice 6: fL ← argminf∈F R̃(f,wL), zL ← fL(x, a) 7: fH ← argminf∈F R̃(f,wH), zH",4. Regret Guarantees,[0],[0]
"← fH(x, a) 8:",4. Regret Guarantees,[0],[0]
"Rmin ← R(fL) 9: ← ↵ (r − zL)3
10: while zH",4. Regret Guarantees,[0],[0]
− zL >,4. Regret Guarantees,[0],[0]
↵ and wH,4. Regret Guarantees,[0],[0]
"−wL > do 11: w ← (wH +wL)2
//",4. Regret Guarantees,[0],[0]
Invoke oracle.,4. Regret Guarantees,[0],[0]
"12: f ← argmin ˜f∈F R̃( ˜f,w), z ← f(x, a) 13: if R(f) ≥",4. Regret Guarantees,[0],[0]
"Rmin + then 14: wH ← w, zH",4. Regret Guarantees,[0],[0]
"← z 15: else 16: wL ← w, zL",4. Regret Guarantees,[0],[0]
"← z 17: end if 18: end while 19: return zH.
F  = N + 1 and a distribution for which both Algorithms 1 and 2 have regret lower bounded by (1−✏) ⋅minN, ⌦̃(T ).",4. Regret Guarantees,[0],[0]
Proposition 1 is proved in Appendix A.2.,4. Regret Guarantees,[0],[0]
The proof builds on a well-known albeit rather pathological instance.,4. Regret Guarantees,[0],[0]
"In contrast, our strong empirical results in the following section show that such instances are not encountered in practice.",4. Regret Guarantees,[0],[0]
"In order to understand the typical behavior of such algorithms, prior works have considered structural assumptions such as finite eluder dimension (Russo & Van Roy, 2013) or disagreement coefficients (Hanneke, 2014; Krishnamurthy et al., 2017).",4. Regret Guarantees,[0],[0]
"In the next two subsections, we use similar ideas to analyze the regret incurred by our algorithm.",4. Regret Guarantees,[0],[0]
"We assume that HIGHFm and LOWFm are computed exactly, but extension to the approximate case is straightforward.",4. Regret Guarantees,[0],[0]
"Disagreement coefficients come from the active learning literature (Hanneke, 2014), and roughly assume that given a set of functions which fit the historical data well, the probability that these functions make differing predictions on a new example is small.",4.1. Disagreement-based Analysis,[0],[0]
"This rules out the bad case of Proposition 1, where a near-optimal predictor significantly disagrees from the others on each context.",4.1. Disagreement-based Analysis,[0],[0]
"Our development in this subsection largely follows Krishnamurthy et al. (2017), with appropriate modifications to translate from active learning to contextual bandits.",4.1. Disagreement-based Analysis,[0],[0]
"We begin with a formal definition of the disagreement coefficient.
",4.1. Disagreement-based Analysis,[0],[0]
Definition 1 (Disagreement Coefficient).,4.1. Disagreement-based Analysis,[0],[0]
"The disagreement coefficient for F (with respect to DX ) is defined as ✓ 0
∶= sup >0,"">0 "" PrDX x ∈",4.1. Disagreement-based Analysis,[0],[0]
"Dis(F("")) and
∃a ∈ AF("")(x) ∶WF("")(x, a) > .",4.1. Disagreement-based Analysis,[0],[0]
"Here F("") is the set of all predictors f whose greedy policies have regret at most "", Dis(F(""))",4.1. Disagreement-based Analysis,[0],[0]
"is the set of x’s where the greedy policies of at least two functions in F("") choose different actions, AF(x) = f∈Fargmaxa∈A",4.1. Disagreement-based Analysis,[0],[0]
"f(x, a), and WF(x, a) is the difference between the upper and lower bounds HIGHF(x, a) − LOWF(x, a).",4.1. Disagreement-based Analysis,[0],[0]
Formal definitions of these quantities can be found in Appendix A.3.,4.1. Disagreement-based Analysis,[0],[0]
"Informally, the disagreement coefficient is small if on most contexts either all f ∈ F("") choose the same action according to their greedy policies or all actions chosen by those policies have a low range of predicted rewards.
",4.1. Disagreement-based Analysis,[0],[0]
The following theorem provides regret bounds in terms of the disagreement coefficient.,4.1. Disagreement-based Analysis,[0],[0]
"In all theorems we use Õ to suppress polynomial terms in logT , logK, and log(1 ), where is the failure probability.",4.1. Disagreement-based Analysis,[0],[0]
"Moreover, all results can be improved to be logarithmic (in T ) under the standard Massart noise condition (see the appendix for the details).",4.1. Disagreement-based Analysis,[0],[0]
Theorem 2.,4.1. Disagreement-based Analysis,[0],[0]
"With m = (M−m+1)C ⌧m−1 and C = 16 log 2GKT 2 , Algorithm 1 with Option I incurs RegT = Õ T 34 (log G) 14√✓
0 K with probability at least 1 − .",4.1. Disagreement-based Analysis,[0],[0]
"See Theorem 5 in Appendix A.3 for the full version of this theorem, which applies to infinite classes and additionally obtains faster rates under the Massart noise condition.
",4.1. Disagreement-based Analysis,[0],[0]
"Discussion Theorem 2 critically uses the product class structure, specifically the fact that the set At computed by the algorithm coincides with the disagreement set AFm(xt) for t ∈ {⌧m, . . .",4.1. Disagreement-based Analysis,[0],[0]
", ⌧m+1 − 1}.",4.1. Disagreement-based Analysis,[0],[0]
"This is true for product classes, but not necessarily for general (non-product) predictor classes.",4.1. Disagreement-based Analysis,[0],[0]
"Computing the disagreement set efficiently for non-product classes is a challenge for future work.
",4.1. Disagreement-based Analysis,[0],[0]
"While bounding the disagreement coefficients a priori often requires strong assumptions on the model class and the distribution, the size of disagreement set can be easily checked empirically under the product class assumption, and we include this diagnostic in our experimental results.
",4.1. Disagreement-based Analysis,[0],[0]
"Finally, while the disagreement coefficient enables the analysis of Algorithm 1, it is not obvious how to use it to analyze Algorithm 2.",4.1. Disagreement-based Analysis,[0],[0]
"Our analysis crucially requires that any plausibly optimal action a be chosen with a reasonable probability, something which the optimistic algorithm fails to ensure.",4.1. Disagreement-based Analysis,[0],[0]
"The disagreement-based analysis of Theorem 2 is not entirely satisfying, because even for linear predictors (e.g.,
as in LinUCB, Chu et al. 2011), fairly strong assumptions on DX (e.g., log-concavity) are required to bound the disagreement coefficient ✓
0",4.2. Moment-based Analysis,[0],[0]
"(Hanneke, 2014).",4.2. Moment-based Analysis,[0],[0]
"To generalize the analysis to richer than linear classes without distributional assumptions on the contexts, prior work has used the notion of eluder dimension (Russo & Van Roy, 2013).",4.2. Moment-based Analysis,[0],[0]
"It remains challenging, however, to show examples with a small eluder dimension beyond linearly parameterized functions.",4.2. Moment-based Analysis,[0],[0]
"In addition, taking the worst-case over all histories, as in eluder dimension, is overly pessimistic for i.i.d. contextual bandits.
",4.2. Moment-based Analysis,[0],[0]
"To address the shortcomings of both the disagreement-based analysis as well as eluder dimension for i.i.d. settings, we define two new distributional properties which we will use to analyze the regret of both of our algorithms.",4.2. Moment-based Analysis,[0],[0]
Definition 2 (Surprise bound).,4.2. Moment-based Analysis,[0],[0]
"The surprise bound L
1 > 0 is the smallest constant such that for all f ∈ F , x ∈ X , and a ∈ A, the gap (f(x, a) − f(x, a))2 is at most
L 1 ⋅Ex′∼DX",4.2. Moment-based Analysis,[0],[0]
"Ea′∼Unif(A)f(x′, a′)",4.2. Moment-based Analysis,[0],[0]
"− f(x′, a′)2 .",4.2. Moment-based Analysis,[0],[0]
"The surprise bound is small if functions with a small expected squared error relative to f (under a uniform choice of actions) do not encounter a much larger squared error on any single context-action pair.
",4.2. Moment-based Analysis,[0],[0]
"The second quantity, which we call the implicit exploration coefficient (or IEC) relates the expected regression error under actions chosen by the optimal policy to the worst-case error on any other context-action pair.",4.2. Moment-based Analysis,[0],[0]
"For ∈ (0,1] define U (a) = {x  f(x, a) ≥ f(x, a′) + ∀a′ ≠ a}.",4.2. Moment-based Analysis,[0],[0]
Definition 3,4.2. Moment-based Analysis,[0],[0]
(Implicit exploration coefficient—IEC).,4.2. Moment-based Analysis,[0],[0]
"For any ∈ (0,1], the implicit exploration coefficient L
2, > 0 is the smallest constant such that for all f ∈ F , x ∈ X , and a ∈ A, the gap (f(x, a) − f(x, a))2 is at most L 2, Ex′∼DX",4.2. Moment-based Analysis,[0],[0]
"Ea′∼Unif(A)1x′ ∈ U (a′) (1) ⋅ f(x′, a′)",4.2. Moment-based Analysis,[0],[0]
"− f(x′, a′)2.",4.2. Moment-based Analysis,[0],[0]
"We now make two remarks about these definitions and their impact on the performance of Algorithms 1 and 2.
",4.2. Moment-based Analysis,[0],[0]
"• By definition, L 2, is non-decreasing in .",4.2. Moment-based Analysis,[0],[0]
"For Al-
gorithm 1 we can simply use = 0, for which L 2,0 is defined by replacing the right-hand side of (1) with L2,0K Ex∼DX",4.2. Moment-based Analysis,[0],[0]
"[(f(x,⇡(x))",4.2. Moment-based Analysis,[0],[0]
"− f(x,⇡(x)))2].",4.2. Moment-based Analysis,[0],[0]
"The analysis of Algorithm 2 requires > 0, and this must be used to tune the algorithm’s warm-start period.
",4.2. Moment-based Analysis,[0],[0]
"• We always have L 1 ≤ L 2,0, but L1 may be much
smaller.",4.2. Moment-based Analysis,[0],[0]
"L 1 appears in the regret bound of Algorithm 2, but not Algorithm 1.
",4.2. Moment-based Analysis,[0],[0]
"We now state the regret bound for Algorithm 1 with a general class F , and employ the shorthand C ′",4.2. Moment-based Analysis,[0],[0]
"= 16 log 2F T 2 .
",4.2. Moment-based Analysis,[0],[0]
Theorem 3.,4.2. Moment-based Analysis,[0],[0]
"With m = (M−m+1)C′ ⌧m−1 , Algorithm 1 with Option II incurs RegT = Õ",4.2. Moment-based Analysis,[0],[0]
"TL2,0 log F  with probability at least 1 − .",4.2. Moment-based Analysis,[0],[0]
We now move on to describe the performance guarantee for Algorithm 2.,4.2. Moment-based Analysis,[0],[0]
"Because this optimistic strategy does not explore as readily as the elimination-based strategy of Algorithm 1, the analysis requires both that (i) the IEC L
2, be invoked for some > 0 and (ii) that the algorithm use a warm-start period whose size grows as 1 2.",4.2. Moment-based Analysis,[0],[0]
Theorem 4.,4.2. Moment-based Analysis,[0],[0]
"With m = (M−m+1)C′ ⌧m−1 and M0 = 2 + log
2
 1 + (2M+3)L1C′ 2  for any ∈ (0,1), Algorithm 2
incurs RegT = Õ L1 logF  2 +TL2, log F  with probability at least 1 − .",4.2. Moment-based Analysis,[0],[0]
"As Algorithm 2 requires a warm start, the regret bounds of Theorem 4 are always worse than those of Theorem 3.",4.2. Moment-based Analysis,[0],[0]
"Appendix A.4 contains full versions of these theorems, Theorem 6 and Theorem 7, which obtain faster rates under the Massart noise condition and apply to infinite classes.
",4.2. Moment-based Analysis,[0],[0]
"Linear classes For concreteness, let us discuss the regret of both algorithms in a linear setting with a fixed feature map ∶ X ×A → Rd and F = {(x, a) w",4.2. Moment-based Analysis,[0],[0]
"(x, a) w ∈W} for some W ⊆ Rd (e.g., as in LinUCB).",4.2. Moment-based Analysis,[0],[0]
"In the basic ` 2 -bounded case, L 1 and L 2, can be bounded in terms of the minimum eigenvalues of Ex[ (x, a) (x, a)] and Ex1{x ∈ U (a)} (x, a) (x, a), respectively.",4.2. Moment-based Analysis,[0],[0]
When predictors are s-sparse we can instead obtain bounds in terms of (A) ∶= minw≠0∶ w0≤2swAw,4.2. Moment-based Analysis,[0],[0]
"ww, the minimum restricted eigenvalue for 2s-sparse predictors (Raskutti et al., 2010).",4.2. Moment-based Analysis,[0],[0]
"For Algorithm 1 this yields a near dimensionindependent bound on RegT of Õ sKT log d  Ex (x,⇡(x))",4.2. Moment-based Analysis,[0],[0]
"(x,⇡(x))",4.2. Moment-based Analysis,[0],[0]
.1,4.2. Moment-based Analysis,[0],[0]
"This improves upon the moment matrix conditions of Bastani & Bayati (2015), although our algorithm requires nonconvex optimization oracles.2 Note that without the scaling with K as in our result, a √ d dependence is unavoidable (Abbasi-Yadkori et al., 2012).",4.2. Moment-based Analysis,[0],[0]
"The result highlights the strengths of our analysis in the best case compared with eluder dimension, which does not adapt to sparsity structures.",4.2. Moment-based Analysis,[0],[0]
"On the other hand, for the standard LinUCB setting, our result is inferior by at least a factor of K.
Discussion Our analysis is influenced by the results of Bastani & Bayati (2015) for the (high-dimensional) linear setting, but extends to general classes F , and when applied to Algorithm 1 with linear classes, the assumed bound on
1See Proposition 3, Lemma 9, and Theorem 3 in the appendix.",4.2. Moment-based Analysis,[0],[0]
"2Also, since the class F is non-convex, this requires the slower
binary search algorithm of Krishnamurthy et al. (2017).
",4.2. Moment-based Analysis,[0],[0]
"L 2, is weaker than their “diversity condition”.",4.2. Moment-based Analysis,[0],[0]
"Similar assumptions have been used to analyze purely greedy linear contextual bandits (Bastani et al., 2017; Kannan et al., 2018); our assumptions are strictly weaker.",4.2. Moment-based Analysis,[0],[0]
We compared our new algorithms with existing oracle-based alternatives.,5. Experiments,[0],[0]
"In addition to showing that RegCB3 has strong empirical performance, our experiments provide a more extensive empirical study of oracle-based contextual bandit algorithms than any past works (e.g., Agarwal et al., 2014, Krishnamurthy et al., 2016).",5. Experiments,[0],[0]
"Description of the datasets, benchmark algorithms, and oracle configurations, as well as further experimental results are included in Appendix B.
Datasets We begin with 10 datasets with full reward information and simulate bandit feedback by withholding the rewards for actions not selected by the algorithm.",5. Experiments,[0],[0]
"We use two large-scale learning-to-rank datasets, Microsoft MSLRWEB30k (mslr) (Qin & Liu, 2010) and Yahoo!",5. Experiments,[0],[0]
"Learning to Rank Challenge V2.0 (yahoo) (Chapelle & Chang, 2011), which were previously used to evaluate contextual semibandits (Krishnamurthy et al., 2016).",5. Experiments,[0],[0]
"We also use eight classification datasets from the UCI repository (Lichman, 2013), summarized in Table 1 of Appendix B.1.
",5. Experiments,[0],[0]
"The ranking datasets have natural rewards (relevances), but the rewards for the classification datasets always have multiclass structure (1 for the correct action and 0 for all others).",5. Experiments,[0],[0]
"To ensure that we evaluate the full generality of the CB setting, we create eight “noisy” UCI datasets by sampling new rewards for the datasets according to a noisy reward matrix model described in Appendix B. This yields additional 8 datasets for a total of 18.",5. Experiments,[0],[0]
"On each dataset we consider several replicates obtained by randomly permuting examples and, on noisy UCI, also randomly generating rewards.",5. Experiments,[0],[0]
"All the methods are evaluated on the same set of replicates.
",5. Experiments,[0],[0]
"Algorithms We evaluate Algorithms 1 and 2 against three baselines, all based on various optimization-oracle assumptions.",5. Experiments,[0],[0]
"First two are agnostic baselines, ✏-Greedy (Langford & Zhang, 2008) and the minimax-optimal ILOVETOCONBANDITS (ILTCB) strategy of Agarwal et al. (2014).4
✏-Greedy and ILTCB both assume cost-sensitive classification oracles and come with theoretical guarantees.",5. Experiments,[0],[0]
"The third baseline is a bootstrapping-based exploration strategy of Dimakopoulou et al. (2017) (Bootstrap-TS), which uses bootstrapping to estimate confidence intervals and then performs Thompson sampling to select an action based on the intervals.",5. Experiments,[0],[0]
"This algorithm represents a computationally
3RegCB refers collectively to both Algorithms 1 and 2.",5. Experiments,[0],[0]
"4We use an implementation available at https://github.
com/akshaykr/oracle_cb, which was also used by Krishnamurthy et al. (2016).
tractable alternative to Thompson sampling as it works in the regression-oracle model we consider here, but it does not have a theoretical analysis.5
Note that the LinUCB algorithm (Chu et al., 2011; AbbasiYadkori et al., 2011), which is a natural baseline as well, coincides with our Algorithm 2 (with a linear oracle), so we only plot the performance of RegCB with a linear oracle.
",5. Experiments,[0],[0]
"All of the algorithms update on an epoch schedule with epoch lengths of 2i2, which is a theoretically rigorous choice for each algorithm.",5. Experiments,[0],[0]
"Oracles We consider two baseline predictor classes F : ` 2
- regularized linear functions (Linear) and gradient-boosted depth-5 regression trees (GB5) (Friedman, 2001).",5. Experiments,[0],[0]
"For the regularized linear class, Algorithm 2 is equivalent to LinUCB on an epoch schedule.6 See Appendix B.3 for details.
",5. Experiments,[0],[0]
"When running both RegCB variants with the GB5 oracle, we use a simple heuristic to substantially speed up the computation.",5. Experiments,[0],[0]
"At the beginning of each epoch m, we find the best regression-tree ensemble on the dataset so far (i.e., with respect to R̂m).",5. Experiments,[0],[0]
"Throughout the epoch, we keep the structure of the ensemble fixed and in each call to ORACLE(H) we only re-optimize the predictions in leaves.",5. Experiments,[0],[0]
"This can be solved in closed form, similar to LinUCB, so the full binary search procedure (Algorithm 3) does not need to be run.
",5. Experiments,[0],[0]
Parameter Tuning We evaluate each algorithm for eight exponentially spaced parameter values across five replicates.,5. Experiments,[0],[0]
"For ✏-Greedy we tune the constant ✏, and for ILTCB we tune a certain smoothing parameter (see Appendix B).",5. Experiments,[0],[0]
For Algorithms 1 and 2 we set m = for all m and tune .,5. Experiments,[0],[0]
For Algorithm 2 we use a warm start of 0.,5. Experiments,[0],[0]
"We tune a confidence parameter similar to for Bootstrap-TS.
",5. Experiments,[0],[0]
"Evaluation Each dataset is split into “training data”, for which algorithm receives one example at a time and must predict online, and a holdout validation set.",5. Experiments,[0],[0]
Validation is performed by simulating the algorithm’s predictions on examples from the holdout set without allowing the algorithm to incorporate these examples.,5. Experiments,[0],[0]
"We also plot the validation reward of a “supervised” baseline obtained by training the oracle (either Linear or GB5) on the entire training set at once (including rewards for all actions).
",5. Experiments,[0],[0]
For Algorithms 1 and 2 we show average reward at various numbers of training examples for the best fixed parameter value in each dataset.,5. Experiments,[0],[0]
"For the baselines, we take the pointwise maximum of the average reward across all parameter values for each number of examples.",5. Experiments,[0],[0]
"Thus,
5It is not known how to implement the standard formulation of Thompson sampling for contextual bandits (e.g., Russo & Van Roy 2013) with optimization oracles.
",5. Experiments,[0],[0]
"6More precisely, it is equivalent to the well-known OFUL variant of LinUCB (Abbasi-Yadkori et al., 2011).
",5. Experiments,[0],[0]
"the curves for our methods correspond to an actual run of the algorithm, while the baselines are an upper envelope aggregating multiple parameter values.
",5. Experiments,[0],[0]
"Results: Performance Figure 1 shows average reward of each algorithm on a holdout validation set for three representative datasets, letter from UCI, letter-noise (the variant with simulated rewards), and yahoo.
",5. Experiments,[0],[0]
"RegCB (both Algorithms 1 and 2) outperforms all baselines on the unmodified UCI datasets (e.g., letter in Figure 1).",5. Experiments,[0],[0]
"On the noisy variants (e.g., letter+N in Figure 1), the performance of the ILTCB and Bootstrap-TS benchmarks improves significantly, with Bootstrap-TS slightly edging out the rest of the algorithms.",5. Experiments,[0],[0]
"On the yahoo ranking dataset (Figure 1, right), the ordering of the algorithms in performance is similar to noisy UCI datasets.
",5. Experiments,[0],[0]
"Validation performance plots for all datasets are in Appendix B. Overall, RegCB methods and Bootstrap-TS generally dominate the field.",5. Experiments,[0],[0]
"While Bootstrap-TS can outperform RegCB methods when using GB5 models, the gap is typically quite small.",5. Experiments,[0],[0]
"For linear models, RegCB methods generally outperform Bootstrap-TS, hinting that the approximate binary search might be hurting RegCB with GB5 models.",5. Experiments,[0],[0]
"We also observe that when RegCB methods outperform Bootstrap-TS, the gap is often quite large.",5. Experiments,[0],[0]
"We will see further evidence of this behavior in the next set of results.
",5. Experiments,[0],[0]
"Results: Aggregate Performance To rigorously draw conclusions about overall performance, Figure 2 aggregates performance across all datasets.",5. Experiments,[0],[0]
"We compute “normalized relative loss” for each algorithm by rescaling the validation reward (computed as in Figure 1) so that, at each round, the best performing algorithm has loss 0 and the worst has loss 1.",5. Experiments,[0],[0]
"In each plot of Figure 2 we consider normalized relative losses at a specific cutoff time (1000 examples in the left plot, and all examples in the center and right), and for each method we plot the number of datasets where it achieves loss below a threshold, as a function of the threshold.",5. Experiments,[0],[0]
"Thus, curves towards top left corner correspond to methods that achieve lower relative loss on more datasets.",5. Experiments,[0],[0]
"The intercept at loss 0 is the number of datasets where an algorithm is the best, and the intercept at 0.99 is the number of datasets where the it is not the worst (so the distance from top is the number of datasets where it is the worst).",5. Experiments,[0],[0]
"Solid lines are runs with GB5 and dashed lines are with the Linear oracle.
",5. Experiments,[0],[0]
"The aggregate performance with the GB5 oracle across all datasets can be briefly summarized as follows: RegCB always beats ✏-Greedy and ILTCB, but sometimes loses out to Bootstrap-TS, and Bootstrap-TS itself sometimes underperforms relative to the other baselines, especially on the UCI datasets.",5. Experiments,[0],[0]
"Even when RegCB is not the best, it is almost always within 20% of the best.",5. Experiments,[0],[0]
"The elimination and optimistic variants of RegCB have comparable performance,
with elimination performing slightly better in aggregate.
",5. Experiments,[0],[0]
"The RegCB algorithms with the GB5 oracle also dominate the ✏-Greedy, ILTCB, and Bootstrap-TS baselines when they are equipped with Linear oracles (the dashed lines in Figure 2).",5. Experiments,[0],[0]
"When the RegCB algorithms use the Linear oracle they also dominate the baselines with the Linear oracle across all datasets, including Bootstrap-TS.7",5. Experiments,[0],[0]
"This suggests that the gap between RegCB and Bootstrap-TS for GB5 may be due to the approximation of fixing the ensemble structure in each epoch, as noted earlier.
",5. Experiments,[0],[0]
Results: Confidence Width,5. Experiments,[0],[0]
The analysis of RegCB relies on assumptions on D (disagreement coefficient or moment parameters) that are not easy to verify.,5. Experiments,[0],[0]
"The main role of these parameters is to control the rate at which confidence width WFm(xt, a) = HIGHFm(xt, a) − LOWFm(xt, a) used in RegCB shrinks, since small widths imply that the algorithm makes good decisions and thus has low regret.
",5. Experiments,[0],[0]
"To investigate whether the width indeed shrinks empirically, we compute WFm(xt, a) on each dataset for Algorithm 2 and Bootstrap-TS, where a is the “optimistic” action with highest upper confidence bound under each algorithm.",5. Experiments,[0],[0]
"Finally for both Algorithm 2 and Bootstrap-TS we compute the size of the “disagreement set” At, defined in Algorithm 1, which measures how many actions the algorithm thinks are plausibly best.8
Figure 3 shows width and disagreement for a representative sample of datasets under the GB5 oracle; the remaining datasets are in Appendix B. The figure suggests that our distributional assumptions are reasonable for real-world datasets.",5. Experiments,[0],[0]
"In particular, for our algorithm, the width decays roughly as T −13 for letter and T −12 for letter+N and yahoo.",5. Experiments,[0],[0]
"Interestingly, the best hyper-parameter setting for Bootstrap-TS on letter yields low but essentially constant (i.e., not shrinking) width, and obtains a poor validation reward in Figure 1 (left).",5. Experiments,[0],[0]
"This suggests that while the Bootstrap-TS confidence intervals are small, they may not be faithful in the sense of containing f(x, a).",5. Experiments,[0],[0]
This work serves as a starting point for what we hope will be a fruitful line of research on oracle-efficient contextual bandit algorithms in realizability-based settings.,6. Conclusion and Discussion,[0],[0]
"We have shown that the RegCB family of algorithms have strong empirical performance and enjoy nice theoretical properties.
7The aggregate plots for RegCB with the Linear oracle can be found in Appendix B along with additional aggregate plots.
8This set is well-defined for both RegCB-Opt and Bootstrap-TS even through neither algorithm instantiates it explicitly.",6. Conclusion and Discussion,[0],[0]
"For the yahoo and mslr datasets this At is technically a lower bound on the true disagreement set size AFm(xt) because our classesF do not have product structure on these datasets—see Section 4.1.
",6. Conclusion and Discussion,[0],[0]
"These results suggest several compelling future directions.
",6. Conclusion and Discussion,[0],[0]
"First, is there a regression oracle–based algorithm that achieves the optimal Õ(KT log F ) regret?",6. Conclusion and Discussion,[0],[0]
"For example, is it possible to oraclize regressor elimination of Agarwal et al. (2012)?
Second, given the competitive empirical performance of
Bootstrap-TS, are there reasonable assumptions as in Section 4 under which it can be analyzed?",6. Conclusion and Discussion,[0],[0]
"There is recent work in this direction for linear models (Lu & Van Roy, 2017).
",6. Conclusion and Discussion,[0],[0]
"Finally, randomizing uniformly or putting all the mass on the optimistic choice are two extreme cases of choosing amongst the plausibly optimal actions.",6. Conclusion and Discussion,[0],[0]
Are there better randomization schemes that lead to stronger regret guarantees?,6. Conclusion and Discussion,[0],[0]
We thank Akshay Krishnamurthy and Alberto Bietti for helpful discussions.,Acknowledgements,[0],[0]
A major challenge in contextual bandits is to design general-purpose algorithms that are both practically useful and theoretically well-founded.,abstractText,[0],[0]
We present a new technique that has the empirical and computational advantages of realizabilitybased approaches combined with the flexibility of agnostic methods.,abstractText,[0],[0]
"Our algorithms leverage the availability of a regression oracle for the valuefunction class, a more realistic and reasonable oracle than the classification oracles over policies typically assumed by agnostic methods.",abstractText,[0],[0]
Our approach generalizes both UCB and LinUCB to far more expressive possible model classes and achieves low regret under certain distributional assumptions.,abstractText,[0],[0]
"In an extensive empirical evaluation, we find that our approach typically matches or outperforms both realizability-based and agnostic baselines.",abstractText,[0],[0]
Practical Contextual Bandits with Regression Oracles,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 439–443 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"The success of automatic image captioning (Farhadi et al., 2010; Mitchell et al., 2012; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015) demonstrates compellingly that end-to-end statistical models can align visual information with language.",1 Introduction,[0],[0]
"However, high-quality captions are not merely true, but also pragmatically informative in the sense that they highlight salient properties and help distinguish their inputs from similar images.",1 Introduction,[0],[0]
"Captioning systems trained on single images struggle to be pragmatic in this sense, producing either very general or hyper-specific descriptions.
",1 Introduction,[0],[0]
"In this paper, we present a neural image captioning system1 that is a pragmatic speaker as defined by the Rational Speech Acts (RSA) model (Frank and Goodman, 2012; Goodman and Stuhlmüller,
1The code is available at https://github.com/ reubenharry/Recurrent-RSA
2013).",1 Introduction,[0],[0]
"Given a set of images, of which one is the target, its objective is to generate a natural language expression which identifies the target in this context.",1 Introduction,[0],[0]
"For instance, the literal caption in Figure 1 could describe both the target and the top two distractors, whereas the pragmatic caption mentions something that is most salient of the target.",1 Introduction,[0],[0]
"Intuitively, the RSA speaker achieves this by reasoning not only about what is true but also about what it’s like to be a listener in this context trying to identify the target.
",1 Introduction,[0],[0]
"This core idea underlies much work in referring expression generation (Dale and Reiter, 1995; Monroe and Potts, 2015; Andreas and Klein, 2016; Monroe et al., 2017) and image captioning (Mao et al., 2016a; Vedantam et al., 2017), but these models do not fully confront the fact that the agents must reason about all possible utterances, which is intractable.",1 Introduction,[0],[0]
"We fully address this problem by implementing RSA at the level of characters rather than the level of utterances or words: the neural language model emits individual characters, choosing them to balance pragmatic informativeness with overall well-formedness.",1 Introduction,[0],[0]
"Thus, the agents reason not about full utterances, but rather only about all possible character choices, a very small space.",1 Introduction,[0],[0]
"The result is that the information encoded recurrently in the neural model allows us
439
to obtain global pragmatic effects from local decisions.",1 Introduction,[0],[0]
"We show that such character-level RSA speakers are more effective than literal captioning systems at the task of helping a reader identify the target image among close competitors, and outperform word-level RSA captioners in both efficiency and accuracy.",1 Introduction,[0],[0]
"In applying RSA to image captioning, we think of captioning as a kind of reference game.",2 Bayesian Pragmatics for Captioning,[0],[0]
"The speaker and listener are in a shared context consisting of a set of images W , the speaker is privately assigned a target image w⇤ 2 W , and the speaker’s goal is to produce a caption that will enable the listener to identify w⇤.",2 Bayesian Pragmatics for Captioning,[0],[0]
U is the set of possible utterances.,2 Bayesian Pragmatics for Captioning,[0],[0]
"In its simplest form, the literal speaker is a conditional distribution S0(u|w) assigning equal probability to all true utterances u 2 U and 0 to all others.",2 Bayesian Pragmatics for Captioning,[0],[0]
"The pragmatic listener L0 is then defined in terms of this literal agent and a prior P (w) over possible images:
L0(w|u) / S0(u|w) ⇤ P (w)P
w02W S0(u|w0) ⇤ P (w0) (1)
",2 Bayesian Pragmatics for Captioning,[0],[0]
"The pragmatic speaker S1 is then defined in terms of this pragmatic listener, with the addition of a rationality parameter ↵ > 0",2 Bayesian Pragmatics for Captioning,[0],[0]
governing how much it takes into account the L0 distribution when choosing utterances.,2 Bayesian Pragmatics for Captioning,[0],[0]
"P (u) is here taken to be a uniform distribution over U :
S1(u|w) / L0(w|u",2 Bayesian Pragmatics for Captioning,[0],[0]
),2 Bayesian Pragmatics for Captioning,[0],[0]
"↵ ⇤ P (u)P
u02U L0(w|u0)↵ ⇤ P (u0) (2)
",2 Bayesian Pragmatics for Captioning,[0],[0]
"As a result of this back-and-forth, the S1 speaker is reasoning not merely about what is true, but rather about a listener reasoning about a literal speaker who reasons about truth.
",2 Bayesian Pragmatics for Captioning,[0],[0]
"To illustrate, consider the pair of images 2a and 2b in Figure 2.",2 Bayesian Pragmatics for Captioning,[0],[0]
"Suppose that U = {bus, red bus}.",2 Bayesian Pragmatics for Captioning,[0],[0]
Then the literal speaker S0 is equally likely to produce bus and red bus when the left image 2a is the target.,2 Bayesian Pragmatics for Captioning,[0],[0]
"However, L0 breaks this symmetry; because red bus is false of the right bus, L0(2a|bus) = 13 and L0(2b|bus) = 23 .",2 Bayesian Pragmatics for Captioning,[0],[0]
"The S1 speaker therefore ends up favoring red bus when trying to convey 2a, so that S1(red bus|2a) = 34 and S1(bus|2a)",2 Bayesian Pragmatics for Captioning,[0],[0]
= 14 .,2 Bayesian Pragmatics for Captioning,[0],[0]
"To apply the RSA model to image captioning, we first train a neural model with a CNN-RNN architecture (Karpathy and Fei-Fei, 2015; Vinyals et al., 2015).",3 Applying Bayesian Pragmatics to a Neural Semantics,[0],[0]
The trained model can be considered an S0-style distribution P (caption|image) on top of which further listeners and speakers can be built.,3 Applying Bayesian Pragmatics to a Neural Semantics,[0],[0]
"(Unlike the idealized S0 described above, a neural S0 will assign some probability to untrue utterances.)
",3 Applying Bayesian Pragmatics to a Neural Semantics,[0],[0]
"The main challenge for this application is that the space of utterances (captions) U will be very large for any suitable captioning system, making the calculation of S1 intractable due to its normalization over all utterances.",3 Applying Bayesian Pragmatics to a Neural Semantics,[0],[0]
"The question, therefore, is how best to approximate this inference.",3 Applying Bayesian Pragmatics to a Neural Semantics,[0],[0]
"The solution employed by Monroe et al. (2017) and Andreas and Klein (2016) is to sample a small subset of probable utterances from the S0, as an approximate prior upon which exact inference can be performed.",3 Applying Bayesian Pragmatics to a Neural Semantics,[0],[0]
"While tractable, this approach has the shortcoming of only considering a small part of the true prior, which potentially decreases the extent to which pragmatic reasoning will be able to apply.",3 Applying Bayesian Pragmatics to a Neural Semantics,[0],[0]
"In particular, if a useful caption never appears in the sampled prior, it cannot appear in the posterior.",3 Applying Bayesian Pragmatics to a Neural Semantics,[0],[0]
"Inspired by the success of the “emittorsuppressor” method of Vedantam et al. (2017), we propose an incremental version of RSA.",3.1 Step-Wise Inference,[0],[0]
"Rather than performing a single inference over utterances, we perform an inference for each step of the unrolling of the utterance.
",3.1 Step-Wise Inference,[0],[0]
"We use a character-level LSTM, which defines a distribution over characters P (u|pc, image), where pc (“partial caption”) is a string of char-
acters constituting the caption so far and u is the next character of the caption.",3.1 Step-Wise Inference,[0],[0]
"This is now our S0: given a partially generated caption and an image, it returns a distribution over which character should next be added to the caption.",3.1 Step-Wise Inference,[0],[0]
"The advantage of using a character-level LSTM over a word-level one is that U is much smaller for the former (⇡30 vs. ⇡20, 000), making the ensuing RSA model much more efficient.
",3.1 Step-Wise Inference,[0],[0]
"We use this S0 to define an L0 which takes a partial caption and a new character, and returns a distribution over images.",3.1 Step-Wise Inference,[0],[0]
"The S1, in turn, given a target image w⇤, performs an inference over the set of possible characters to determine which is best with respect to the listener choosing w⇤.
",3.1 Step-Wise Inference,[0],[0]
"At timestep t of the unrolling, the listener L0 takes as its prior over images the L0 posterior from timestep (t 1).",3.1 Step-Wise Inference,[0],[0]
"The idea is that as we proceed with the unrolling, the L0 priors on which image is being referred to may change, which in turn should affect the speaker’s actions.",3.1 Step-Wise Inference,[0],[0]
"For instance, the speaker, having made the listener strongly in favor of the target image, is less compelled to continue being pragmatic.",3.1 Step-Wise Inference,[0],[0]
"In our incremental RSA, speaker models take both a target image and a partial caption pc.",3.2 Model Definition,[0],[0]
"Thus, S0 is a neurally trained conditional distribution St0(u|w, pct), where t is the current timestep of the unrolling and u is a character.
",3.2 Model Definition,[0],[0]
"We define the Lt0 in terms of the S t 0 as follows, where ip is a distribution over images representing the L0 prior:
Lt0(w|u, ipt, pct) / St0(u|w, pct) ⇤ ipt(w) (3)
",3.2 Model Definition,[0],[0]
"Given an St0 and L t 0, we define S t 1 and L t 1 as:
St1(u|w, ipt, pct) / St0(u|w, pct) ⇤ Lt0(w|u, ipt, pct)↵ (4)
Lt1(w|u, ipt, pct) / Lt0(w|u, ipt, pct) ⇤ St0(u|w, pct) (5)
",3.2 Model Definition,[0],[0]
"Unrolling To perform greedy unrolling (though in practice we use a beam search) for either S0 or S1, we initialize the state as a partial caption pc0 consisting of only the start token and a uniform prior over the images ip0.",3.2 Model Definition,[0],[0]
"Then, for t > 0, we use our incremental speaker model S0 or S1 to
generate a distribution over the subsequent character St(u|w, ipt, pct), and add the character u with highest probability density to pct, giving us pct+1.",3.2 Model Definition,[0],[0]
"We then run our listener model L1 on u, to obtain a distribution ipt+1 = L t 1(w|u, ipt, pct) over images that the L0 can use at the next timestep.",3.2 Model Definition,[0],[0]
"This incremental approach keeps the inference itself very simple, while placing the complexity of the model in the recurrent nature of the unrolling.2 While our S0 is character-level, the same incremental RSA model works for a word-level S0, giving rise to a word-level S1.",3.2 Model Definition,[0],[0]
"We compare character and word S1s in section 4.2.
",3.2 Model Definition,[0],[0]
"As well as being incremental, these definitions of St1 and L t 1 differ from the typical RSA described in section 2 in that St1 and L t 1 draw their priors from St0 and L t 0 respectively.",3.2 Model Definition,[0],[0]
This generalizes the scheme put forward for S1 by Andreas and Klein (2016).,3.2 Model Definition,[0],[0]
The motivation is to have Bayesian speakers who are somewhat constrained by the S0 language model.,3.2 Model Definition,[0],[0]
"Without this, other methods are needed to achieve English-like captions, as in Vedantam et al. (2017), where their equivalent of the S1 is combined in a weighted sum with the S0.",3.2 Model Definition,[0],[0]
"Qualitatively, Figures 1 and 2 show how the S1 captions are more informative than the S0, as a result of pragmatic considerations.",4 Evaluation,[0],[0]
"To demonstrate the effectiveness of our method quantitatively, we implement an automatic evaluation.",4 Evaluation,[0],[0]
"To evaluate the success of S1 as compared to S0, we define a listener Leval(image|caption) / PS0(caption|image), where PS0(caption|image) is the total probability of S0 incrementally generating caption given image.",4.1 Automatic Evaluation,[0],[0]
"In other words, Leval uses Bayes’ rule to obtain from S0 the posterior probability of each image w given a full caption u.
The neural S0 used in the definition of Leval must be trained on separate data to the neural S0 used for the S1 model which produces captions, since otherwise this S1 production model effectively has access to the system evaluating it.",4.1 Automatic Evaluation,[0],[0]
"As Mao et al. (2016b) note, “a model might ‘com-
2The move from standard to incremental RSA can be understood as a switching of the order of two operations; instead of unrolling a character-level distribution into a sentence level one and then applying pragmatics, we apply pragmatics and then unroll.",4.1 Automatic Evaluation,[0],[0]
"This generalizes to any recursive generation of utterances.
",4.1 Automatic Evaluation,[0],[0]
municate’ better with itself using its own language than with others”.,4.1 Automatic Evaluation,[0],[0]
"In evaluation, we therefore split the training data in half, with one part for training the S0 used in the caption generation model S1 and one part for training the S0 used in the caption evaluation model Leval.
",4.1 Automatic Evaluation,[0],[0]
"We say that the caption succeeds as a referring expression if the target has more probability mass under the distribution Leval(image|caption) than any distractor.
",4.1 Automatic Evaluation,[0],[0]
"Dataset We train our production and evaluation models on separate sets consisting of regions in the Visual Genome dataset (Krishna et al., 2017) and full images in MSCOCO (Chen et al., 2015).",4.1 Automatic Evaluation,[0],[0]
"Both datasets consist of over 100,000 images of common objects and scenes.",4.1 Automatic Evaluation,[0],[0]
"MSCOCO provides captions for whole images, while Visual Genome provides captions for regions within images.
",4.1 Automatic Evaluation,[0],[0]
Our test sets consist of clusters of 10 images.,4.1 Automatic Evaluation,[0],[0]
"For a given cluster, we set each image in it as the target, in turn.",4.1 Automatic Evaluation,[0],[0]
We use two test sets.,4.1 Automatic Evaluation,[0],[0]
"Test set 1 (TS1) consists of 100 clusters of images, 10 for each of the 10 most common objects in Visual Genome.3 Test set 2 (TS2) consists of regions in Visual Genome images whose ground truth captions have high word overlap, an indicator that they are similar.",4.1 Automatic Evaluation,[0],[0]
We again select 100 clusters of 10.,4.1 Automatic Evaluation,[0],[0]
"Both test sets have 1,000 items in total (10 potential target images for each of 100 clusters).
",4.1 Automatic Evaluation,[0],[0]
"Captioning System Our neural image captioning system is a CNN-RNN architecture4 adapted to use a character-based LSTM for the language model.
",4.1 Automatic Evaluation,[0],[0]
"Hyperparameters We use a beam search with width 10 to produce captions, and a rationality parameter of ↵ = 5.0 for the S1.",4.1 Automatic Evaluation,[0],[0]
"As shown in Table 1, the character-level S1 obtains higher accuracy (68% on TS1 and 65.9% on TS2) than the S0 (48.9% on TS1 and 47.5% on TS2), demonstrating that S1 is better than S0 at referring.
",4.2 Results,[0],[0]
"Advantage of Incremental RSA We also observe that 66% percent of the times in which the S1 caption is referentially successful and the S0
3Namely, man, person, woman, building, sign, table, bus, window, sky, and tree.
",4.2 Results,[0],[0]
"4https://github.com/yunjey/ pytorch-tutorial/tree/master/tutorials/ 03-advanced/image_captioning
caption is not, for a given image, the S1 caption is not one of the top 50 S0 captions, as generated by the beam search unrolling at S0.",4.2 Results,[0],[0]
"This means that in these cases the non-incremental RSA method of Andreas and Klein (2016) could not have generated the S1 caption, if these top 50 S0 captions were the support of the prior over utterances.
",4.2 Results,[0],[0]
Comparison to Word-Level RSA We compare the performance of our character-level model to a word-level model.5,4.2 Results,[0],[0]
"This model is incremental in precisely the way defined in section 3.2, but uses a word-level LSTM so that u 2 U are words and U is a vocabulary of English.",4.2 Results,[0],[0]
"It is evaluated with an Leval model that also operates on the word level.
",4.2 Results,[0],[0]
"Though the word S0 performs better on both test sets than the character S0, the character S1 outperforms the word S1, demonstrating the advantage of a character-level model for pragmatic behavior.",4.2 Results,[0],[0]
"We conjecture that the superiority of the characterlevel model is the result of the increased number of decisions where pragmatics can be taken into account, but leave further examination for future research.
",4.2 Results,[0],[0]
Variants of the Model We further explore the effect of two design decisions in the characterlevel model.,4.2 Results,[0],[0]
"First, we consider a variant of S1 which has a prior over utterances determined by an LSTM language model trained on the full set of captions.",4.2 Results,[0],[0]
This achieves an accuracy of 67.2% on TS1.,4.2 Results,[0],[0]
"Second, we consider our standard S1 but with unrolling such that the L0 prior is drawn uniformly at each timestep rather than determined by the L0 posterior at the previous step.",4.2 Results,[0],[0]
This achieves an accuracy of 67.4% on TS1.,4.2 Results,[0],[0]
"This suggests that neither this change of S1 nor L0 priors has a large effect on the performance of the model.
",4.2 Results,[0],[0]
"5Here, we use greedy unrolling, for reasons of efficiency due to the size of U for the word-level model, and set ↵ = 1.0 from tuning on validation data.",4.2 Results,[0],[0]
"For comparison, we note that greedy character-level S1 achieves an accuracy of 61.2% on TS1.",4.2 Results,[0],[0]
We show that incremental RSA at the level of characters improves the ability of the neural image captioner to refer to a target image.,5 Conclusion,[0],[0]
"The incremental approach is key to combining RSA with language models: as utterances become longer, it becomes exponentially slower, for a fixed n, to subsample n% of the utterance distribution and then perform inference (non-incremental approach).",5 Conclusion,[0],[0]
"Furthermore, character-level RSA yields better results than word-level RSA and is far more efficient.",5 Conclusion,[0],[0]
"Many thanks to Hiroto Udagawa and Poorvi Bhargava, who were involved in early versions of this project.",Acknowledgments,[0],[0]
This material is based in part upon work supported by the Stanford Data Science Initiative and by the NSF under Grant No. BCS-1456077.,Acknowledgments,[0],[0]
This work is also supported by a Sloan Foundation Research Fellowship to Noah Goodman.,Acknowledgments,[0],[0]
We combine a neural image captioner with a Rational Speech Acts (RSA) model to make a system that is pragmatically informative: its objective is to produce captions that are not merely true but also distinguish their inputs from similar images.,abstractText,[0],[0]
Previous attempts to combine RSA with neural image captioning require an inference which normalizes over the entire set of possible utterances.,abstractText,[0],[0]
"This poses a serious problem of efficiency, previously solved by sampling a small subset of possible utterances.",abstractText,[0],[0]
"We instead solve this problem by implementing a version of RSA which operates at the level of characters (“a”,“b”,“c”, . . . )",abstractText,[0],[0]
during the unrolling of the caption.,abstractText,[0],[0]
We find that the utterance-level effect of referential captions can be obtained with only characterlevel decisions.,abstractText,[0],[0]
"Finally, we introduce an automatic method for testing the performance of pragmatic speaker models, and show that our model outperforms a non-pragmatic baseline as well as a word-level RSA captioner.",abstractText,[0],[0]
Pragmatically Informative Image Captioning with Character-Level Inference,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 172–181 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
172",text,[0],[0]
"Coreference resolution, identifying mentions that refer to the same entities, is an important NLP problem.",1 Introduction,[0],[0]
"Resolving coreference is critical for many downstream applications, such as reading comprehension, translation, and text summarization.",1 Introduction,[0],[0]
"Identifying a mention depends not only on its lexicons but also its contexts, and requires representations of all the entities before the mention.",1 Introduction,[0],[0]
This is still a challenging task for the approaches based on the cutting-edge word2vec-like lexical representation.,1 Introduction,[0],[0]
"For example, it is hard to identify the mention “he” between two entities “Tom” and “Jerry” because they have almost the same word embeddings.
",1 Introduction,[0],[0]
"A number of datasets have been proposed to study the coreference resolution problem, such as MUC (Hirschman and Chinchor, 1997), ACE
(Doddington et al., 2004), and OntoNotes (Pradhan et al., 2012).",1 Introduction,[0],[0]
"The most popular one is OntoNotes, and recent work on coreference resolution (Clark and Manning, 2016a,b; Lee et al., 2017; Peters et al., 2018) evaluated their models on it.",1 Introduction,[0],[0]
"Other datasets were rarely studied after OntoNotes was published.
",1 Introduction,[0],[0]
"Previous work (Sadat Moosavi and Strube, 2017) suggests that the overlap between training and test sets makes significant impact on the performance of current coreference resolvers.",1 Introduction,[0],[0]
"In OntoNotes, which has relatively low training-test overlap, this impact is mixed together with the core challenges of coreference resolution.",1 Introduction,[0],[0]
"For example, consider the failure of referencing “them” to “the wounded” in “..., the wounded were carried off so fast and it was difficult to count them”.",1 Introduction,[0],[0]
It is hard to tell whether the algorithm can succeed if the currently low-frequency phrase “the wounded” has not been seen enough times in the training set.,1 Introduction,[0],[0]
"From a machine learning perspective, high overlap is needed to ensure that the training and test datasets have similar statistics.
",1 Introduction,[0],[0]
"Another limitation of OntoNotes is that it only has annotations for non-singleton mentions, while singleton mentions are not annotated.",1 Introduction,[0],[0]
"Most of the algorithms for coreference resolution have two steps: mention detection and mention clustering (Wiseman et al., 2016; Clark and Manning, 2016a,b).",1 Introduction,[0],[0]
"The lack of singleton mention annotations makes training and evaluation of mention detectors more difficult.
",1 Introduction,[0],[0]
"To address both limitations of OntoNotes, we build a new dataset, PreCo.",1 Introduction,[0],[0]
"To alleviate the negative impact of low training-test overlap, we restrict the data domain and collect a sufficient amount of data to achieve a relatively high training-test overlap.",1 Introduction,[0],[0]
"Restricting the data domain is a common way to enable better studies of unsolved NLP tasks, such as language modeling (Hill et al., 2015) and
visual question answering (Johnson et al., 2017).",1 Introduction,[0],[0]
"We select our data from English reading comprehension tests for middle and high school Chinese students, which has several advantages.",1 Introduction,[0],[0]
"On one hand, the vocabulary size is appropriate.",1 Introduction,[0],[0]
The English vocabulary of a typical Chinese high school student contains about 3000 commonly used words.,1 Introduction,[0],[0]
"This is similar to the vocabulary of a preschool English-speaking child (Wikipedia, 2018).",1 Introduction,[0],[0]
Most words from the English tests are in this limited vocabulary.,1 Introduction,[0],[0]
"On the other hand, it is practical to collect enough data of this type from the Internet.",1 Introduction,[0],[0]
"With 12.5M words, PreCo is about 10 times larger than OntoNotes.",1 Introduction,[0],[0]
"Large scale datasets, e.g. ImageNet (Deng et al., 2009), SQuAD (Rajpurkar et al., 2016), have played an important role for driving computer vision and NLP forward.
",1 Introduction,[0],[0]
We use the rate of out-of-vocabulary (OOV) words between training and test sets to measure their overlap.,1 Introduction,[0],[0]
"PreCo shows much higher trainingtest overlap than OntoNotes by having an OOV rate of 0.8%, which is about 1/3 of OntoNotes’s 2.1%.",1 Introduction,[0],[0]
"At the same time, PreCo presents a good challenge for coreference resolution research since its documents are in the open domain and have various writing styles.",1 Introduction,[0],[0]
"We test a state-of-the-art
system (Peters et al., 2018) on PreCo and get an F1 score of 81.5.",1 Introduction,[0],[0]
"However, a modest human performance (87.9, which will be described in 4.1 ) is much higher, verifying there remain challenges.
",1 Introduction,[0],[0]
"To help training and evaluation of mention detection, we annotate singleton mentions in PreCo.",1 Introduction,[0],[0]
"Besides singleton mentions, we follow most other annotation rules of OntoNotes to label the new dataset.",1 Introduction,[0],[0]
"We show that in a state-of-the-art coreference resolution system (Peters et al., 2018), we can improve the model performance from 77.3 to 81.6 F1 on a training set of 2.5K PreCo documents by using an oracle mention detector, and the remaining gap of 18.4 F1 to the perfect 100 F1 can only be reduced by improving mention clustering.",1 Introduction,[0],[0]
"This indicates that future work should concern more about mention clustering than mention detection.
",1 Introduction,[0],[0]
"The advantages of our proposed dataset over existing ones in coreference resolution can be summarized as follows:
•",1 Introduction,[0],[0]
"Its OOV rate is about 1/3 of OntoNotes.
",1 Introduction,[0],[0]
"• It has about 10 times larger corpus size than OntoNotes.
",1 Introduction,[0],[0]
• It has annotated singleton mentions.,1 Introduction,[0],[0]
Existing Datasets.,2 Related Work,[0],[0]
"The first two resources for coreference resolution study were MUC-6 and MUC-7 (Hirschman and Chinchor, 1997).",2 Related Work,[0],[0]
"The MUC datasets are too small for training and testing, containing a total of 127 documents with 65K words.",2 Related Work,[0],[0]
"The next standard dataset was ACE (Doddington et al., 2004) which has a much larger corpus of 1M words.",2 Related Work,[0],[0]
But its annotations are restricted to a small subset of entities and are less consistent.,2 Related Work,[0],[0]
"OntoNotes (Pradhan et al., 2012) was presented to overcome those limitations.",2 Related Work,[0],[0]
"Machine learning based approaches, especially deep learning based, benefitted from this well annotated and large-scale (1.3M words) dataset.",2 Related Work,[0],[0]
"Continuous research on OntoNotes over the past 6 years improved performance by 10 F1 score (Durrett and Klein, 2013; Peters et al., 2018).",2 Related Work,[0],[0]
"Datasets after OntoNotes, such as WikiCoref (Ghaddar and Langlais, 2016), are seldom studied.",2 Related Work,[0],[0]
"Therefore, we mainly compare PreCo with OntoNotes in this paper.",2 Related Work,[0],[0]
"With a much larger scale, PreCo builds on the advantages of OntoNotes.",2 Related Work,[0],[0]
"Some of these existing datasets also have corpus in other languages, but we just focus on coreference resolution in English.
",2 Related Work,[0],[0]
Out-of-domain Evaluation.,2 Related Work,[0],[0]
"(Sadat Moosavi and Strube, 2017) show that if coreference resolvers mainly rely on lexical representation, as it is the case in state-of-the-art ones, they are weak at generalizing to unseen domains.",2 Related Work,[0],[0]
"Even in the seen domains, the low degree of overlap for non-pronominal mentions between the training and test sets cause serious deterioration of coreference resolution performance.",2 Related Work,[0],[0]
"As a conclusion, (Sadat Moosavi and Strube, 2017) suggested that out-of-domain evaluation is a must in the literature.",2 Related Work,[0],[0]
"But we think the problem can be relieved by expanding the training data for the target domains to increase overlap, so that the field can pay more attention to the other challenges of coreference resolution.
",2 Related Work,[0],[0]
Data Simplification.,2 Related Work,[0],[0]
Many simplified datasets were built to enable better study on unsolved tasks.,2 Related Work,[0],[0]
Such simplifications can guide researchers to the core problems and make data collection easier.,2 Related Work,[0],[0]
"For example, (Hill et al., 2015) introduced the Children’s Book Test to distinguish the task of predicting syntactic function words from that of predicting low-frequency words for language model.",2 Related Work,[0],[0]
"The dataset helped them to develop a generalizable model with explicit memory representations.
",2 Related Work,[0],[0]
The reading comprehension dataset SQuAD,2 Related Work,[0],[0]
"(Rajpurkar et al., 2016) imposes the constraint that every answer is always a segment of the input text.",2 Related Work,[0],[0]
"This constraint benefits both labeling and evaluation of the dataset, which has significant influences in terms of benchmarks.",2 Related Work,[0],[0]
"Similarly, the reinforcement learning literature develops algorithms by studying games instead of the real world environment (Mnih et al., 2013).",2 Related Work,[0],[0]
"We hope that, with high training-test overlap, PreCo can serve as a valuable resource for research on coreference resolution.",2 Related Work,[0],[0]
We discuss the data collection and annotation in this section.,3 Dataset Creation,[0],[0]
The overview of the process is shown in Figure 2.,3 Dataset Creation,[0],[0]
We crawl English tests from several web sites.,3.1 Corpus Collection,[0],[0]
The web pages often contain the full English tests in a lot of formats.,3.1 Corpus Collection,[0],[0]
We build an annotation website and hire annotators to manually extract the relevant contents.,3.1 Corpus Collection,[0],[0]
"We have a total of 80 part-time Chinese annotators, most of whom are university students.",3.1 Corpus Collection,[0],[0]
They are required to have a minimum score in standard English tests.,3.1 Corpus Collection,[0],[0]
"During annotation training, the annotators read the annotation rules, and take several practice tasks, in which they annotate sample articles, and their results are compared with ground truth side by side for them to study.",3.1 Corpus Collection,[0],[0]
"Before formal annotation, the annotators will need to pass an assessment.
",3.1 Corpus Collection,[0],[0]
"Some data cleaning is done during annotation, such as unifying paragraph separators, etc.",3.1 Corpus Collection,[0],[0]
The questions with answers in these tests are also extracted for future research.,3.1 Corpus Collection,[0],[0]
"Finally, we use NLTK’s sentence and word tokenizer (Bird et al., 2009) to tokenize the crawled text.
",3.1 Corpus Collection,[0],[0]
"In addition to having annotators manually clean the data, we also use heuristic rules to further clean the data.",3.1 Corpus Collection,[0],[0]
"For example, in some cases the whitespaces between two words are missing.",3.1 Corpus Collection,[0],[0]
We use a spell checker to identify and correct most of these cases.,3.1 Corpus Collection,[0],[0]
"We also use heuristic rules to fix some sentence partition boundaries, e.g., to make sure opening quotes are placed at the beginning of a sentence, instead of being wrongly placed at the end of a previous sentence (closing quotes are handled similarly).
",3.1 Corpus Collection,[0],[0]
"In addition to the crawled data, we include the
documents from the RACE dataset (Lai et al., 2017).",3.1 Corpus Collection,[0],[0]
"RACE is a reading comprehension dataset from English tests for middle and high school Chinese students, which has similar types of data sources as PreCo.",3.1 Corpus Collection,[0],[0]
"About 2/3 of PreCo documents are from the RACE dataset.
",3.1 Corpus Collection,[0],[0]
"Since documents are from several data sources, we want to remove duplicated documents, and documents that are not exactly the same but have a high rate of repetitions.",3.1 Corpus Collection,[0],[0]
The similarity of two documents D1 and D2 is estimated using the bagof-words model.,3.1 Corpus Collection,[0],[0]
Assume S1 and S2 are bag-ofwords multisets to represent the two documents.,3.1 Corpus Collection,[0],[0]
"The similarity between D1 and D2 is defined as max( |S1∩S2||S1| , |S1∩S2| |S2| ).",3.1 Corpus Collection,[0],[0]
"If the similarity between two documents are larger than 0.9, we remove the shorter one.",3.1 Corpus Collection,[0],[0]
This process is referred as deduplicate in Figure 2.,3.1 Corpus Collection,[0],[0]
The dataset has a total of 38K documents.,3.2 Data Partition,[0],[0]
"We use 500 documents for the development set, 500 documents for the test set, and the rest 37K documents for the training set.",3.2 Data Partition,[0],[0]
The development and test documents were randomly selected from RACE’s development and test sets.,3.2 Data Partition,[0],[0]
We manually annotate coreferences on these documents.,3.3 Coreference Annotation and Refinement,[0],[0]
"The annotation rules are slightly different from OntoNotes (Pradhan et al., 2012).",3.3 Coreference Annotation and Refinement,[0],[0]
We modify some of the rules to make the definition of coreference more consistent and easier to be understood by the annotators.,3.3 Coreference Annotation and Refinement,[0],[0]
"The major differences
are listed in Table 1.",3.3 Coreference Annotation and Refinement,[0],[0]
"Figure 1 shows an example document in PreCo with annotations.
",3.3 Coreference Annotation and Refinement,[0],[0]
"Good quality control of annotation is essential, since the rules are complicated and coreference resolution depends on meticulous reading of the whole document over and over.",3.3 Coreference Annotation and Refinement,[0],[0]
"We found that annotators get low recall and insufficient precision mainly because of negligence, as opposed to the lack of annotation rules or other ambiguities.",3.3 Coreference Annotation and Refinement,[0],[0]
"For example, two co-referred mentions could be far apart and require careful searches, and an annotator may miss it.",3.3 Coreference Annotation and Refinement,[0],[0]
Therefore we further refine annotations as shown in Figure 3.,3.3 Coreference Annotation and Refinement,[0],[0]
"Annotators can think about the complicated inconsistent cases when merging annotations, and the voting process will fix some errors while preserving the mentions and coreferences that are found only once by individual annotators.
",3.3 Coreference Annotation and Refinement,[0],[0]
The quality of different annotation processes is shown in Table 2.,3.3 Coreference Annotation and Refinement,[0],[0]
OntoNotes took 2 individual annotations for each document and got an adjudicated version based on them.,3.3 Coreference Annotation and Refinement,[0],[0]
"Taking the adjudicated version as ground truth, the average MUC score (Vilain et al., 1995) 1 of individual annota-
1MUC score is one of the metrics to evaluate the quality of coreference resolution.
tions is 89.6, and the inter-annotator MUC score is 83.0.",3.3 Coreference Annotation and Refinement,[0],[0]
The corresponding numbers for PreCo are 85.3 and 77.5.,3.3 Coreference Annotation and Refinement,[0],[0]
The actual gap of individual annotation quality between OntoNotes and PreCo is not as large as it looks like.,3.3 Coreference Annotation and Refinement,[0],[0]
"Note that, OntoNotes’s two individual coreference annotations of each document are based on the same syntactic annotations of the document, so they could be more consistent than PreCo’s which are annotated on raw text.",3.3 Coreference Annotation and Refinement,[0],[0]
"Therefore, if we want to fairly compare PreCo with OntoNotes, we should take into account OntoNotes’s inter-annotator consistency of syntactic parsing annotations.",3.3 Coreference Annotation and Refinement,[0],[0]
"As it has a rough upper bound of 98.5 F1 score according to the reannotation of English Treebank on OntoNotes by the principal annotator a year after the original annotation (Weischedel et al., 2011), we could infer that the individual annotation quality of PreCo is quite close to OntoNotes.
",3.3 Coreference Annotation and Refinement,[0],[0]
Labeling the whole dataset is costly because each annotation from scratch or comparison takes an average of about 10 minutes.,3.3 Coreference Annotation and Refinement,[0],[0]
Prompts from an algorithm do not help since they do not speed up the annotation much but instead introduce biases.,3.3 Coreference Annotation and Refinement,[0],[0]
We observed some biases when using an algorithm to help annotation.,3.3 Coreference Annotation and Refinement,[0],[0]
"We have two models, M1 and M2, and we have a test set T which is annotated manually, and a test set T ′ which uses prompts from model M1 to help annotation.",3.3 Coreference Annotation and Refinement,[0],[0]
"While M1 and M2 have similar performance on T , M1’s performance is much higher than M2’s on T ′, which shows the biases.
",3.3 Coreference Annotation and Refinement,[0],[0]
"Because of limited annotation resources, we have only finished the refinements on the devel-
opment and test sets with the process shown in Figure 3.",3.3 Coreference Annotation and Refinement,[0],[0]
"We refine the training set annotations as follows: for each document, two annotators annotate it separately, and a third annotator compares and merges the two annotations.",3.3 Coreference Annotation and Refinement,[0],[0]
We use a training set of 2.5K documents to quantify the impact of this annotation refinement to model performance.,3.3 Coreference Annotation and Refinement,[0],[0]
"Table 3 shows the model performances of the training set that is annotated once, and the training set of the merged annotation.",3.3 Coreference Annotation and Refinement,[0],[0]
The performance difference is quite significant.,3.3 Coreference Annotation and Refinement,[0],[0]
"Furthermore, the difference is consistent with Table 2: the “AB-merge” model has a similar precision as the “Once” model, but it has a much higher recall.",3.3 Coreference Annotation and Refinement,[0],[0]
It indicates that a further refinement of the training set such as DEF-voting could be essential.,3.3 Coreference Annotation and Refinement,[0],[0]
A more interesting question is: how to make the definition of coreference more consistent and executable?,3.3 Coreference Annotation and Refinement,[0],[0]
We leave it as future work.,3.3 Coreference Annotation and Refinement,[0],[0]
Table 4 shows some properties of OntoNotes and PreCo.,3.4 Dataset Properties,[0],[0]
"As intended, PreCo has a lower OOV rate than OntoNotes.",3.4 Dataset Properties,[0],[0]
"For a training set with vocabulary V and a test set with n tokens [t1, t2, ..., tn], ignoring the tokens with non-alphabetic characters, the OOV rate is defined by:∑
i o(ti)
n ,where o(ti) = { 0 if ti ∈ V 1 if ti /∈ V
The OOV rate can be extended to the rate of lowfrequency words which also indicates the trainingtest overlap, by simply replacing V in the definition above with the non-low-frequency vocabulary of the training set.",3.4 Dataset Properties,[0],[0]
We find that the OOV rate is consistent to the rates of low-frequency words in different levels.,3.4 Dataset Properties,[0],[0]
"So we use the OOV rate for convenience.
",3.4 Dataset Properties,[0],[0]
"In PreCo, about 50.8% of the mentions are singleton mentions.",3.4 Dataset Properties,[0],[0]
Figure 4 shows the distribution of cluster sizes within non-singleton clusters.,3.4 Dataset Properties,[0],[0]
The distribution is similar between OntoNotes and PreCo.,3.4 Dataset Properties,[0],[0]
"To verify our assumption that PreCo embodies the core challenges of coreference, we evaluate a strong baseline coreference resolver on it.",4 Analysis,[0],[0]
"Specifically, we (i) estimate the room for improvement of the baseline system to show that the dataset is challenging, (ii) study the impact of training-test overlap to model performance and error analysis to show the advantages of PreCo, and (iii) quan-
titatively evaluate the mention detector to understand the bottlenecks of the coreference resolution system.",4 Analysis,[0],[0]
"We use the end-to-end neural coreference resolver, E2E-Coref (Lee et al., 2017), enhanced by the deep contextualized word representations (Peters et al., 2018) as the baseline system, and we refer to this system as EE2E-Coref.",4.1 Baseline Performance,[0],[0]
"This is the state-ofthe-art model on OntoNotes, achieving a test average F1 score of 70.4, which is the main evaluation metric for coreference resolution.",4.1 Baseline Performance,[0],[0]
"The metric is computed by averaging the F1 of MUC, B3, and CEAFφ4, which are three metrics of coreference resolution that have different focuses.
",4.1 Baseline Performance,[0],[0]
Our implementation EE2E-Coref2 gets 81.5 Avg.,4.1 Baseline Performance,[0],[0]
F1 score on PreCo.,4.1 Baseline Performance,[0],[0]
"We follow the setting of most hyperparameters on OntoNotes and do gridsearch for the decay parameter of the learning rate and the size of the hidden layers on the development set, since these two hyperparameters are relatively sensitive to the scale of the training data.",4.1 Baseline Performance,[0],[0]
"The F1 score increment from OntoNotes to PreCo is probably due to the higher overlap between the training and test sets in PreCo.
",4.1 Baseline Performance,[0],[0]
"2It gets an F1 score of 70.0±0.3 on OntoNotes, slightly lower than the F1 score reported in the original paper.
",4.1 Baseline Performance,[0],[0]
We demonstrate three typical error cases made by EE2E-Coref on PreCo in Table 5.,4.1 Baseline Performance,[0],[0]
"Coreference resolution in these cases requires good understanding of multiple sentences, which is an open problem in NLP.",4.1 Baseline Performance,[0],[0]
"A capable entity representation for “them”, “another one” or “Dr. Watson” may help to resolve these error cases.",4.1 Baseline Performance,[0],[0]
We also compare the performance of EE2E-Coref with human performance to estimate the room for improvement on PreCo.,4.1 Baseline Performance,[0],[0]
"As described in Section 3.4, human annotators get low recall mostly due to negligence.",4.1 Baseline Performance,[0],[0]
So we use the AB-merge annotation to estimate human’s ability on coreference resolution.,4.1 Baseline Performance,[0],[0]
"The gap of performance between model and human is 6.4 F1 score, from 81.5 to 87.9.",4.1 Baseline Performance,[0],[0]
"The actual gap
is larger, since AB-merge still has some missed coreference annotations due to negligence.",4.1 Baseline Performance,[0],[0]
This shows that the dataset is challenging and encourages future research.,4.1 Baseline Performance,[0],[0]
"The error cases show the challenges as well.
",4.1 Baseline Performance,[0],[0]
Note that PreCo is not a general purpose dataset.,4.1 Baseline Performance,[0],[0]
"Our motivation of designing PreCo is to make it easier to improve coreference resolution algorithms, e.g., to make error analysis easier.",4.1 Baseline Performance,[0],[0]
It is not a goal of PreCo to generalize well on corpus from other domains.,4.1 Baseline Performance,[0],[0]
"Furthermore, we find that there are a certain amount of annotation errors in the development and test sets.",4.1 Baseline Performance,[0],[0]
"We suggest that researchers working on PreCo should be careful about these errors, especially after a model gets F1 score beyond 90.0.",4.1 Baseline Performance,[0],[0]
Training-test overlap makes significant impact on error analysis.,4.2 Impact of Training-test Overlap,[0],[0]
"Consider an error case of coreference resolution, if there are low-frequency words in the related mentions, then it will be hard to tell whether the algorithm can succeed if the words has not been seen enough times in the training set.",4.2 Impact of Training-test Overlap,[0],[0]
We call an error case LFW if there are low-frequency words3 in its related mentions4.,4.2 Impact of Training-test Overlap,[0],[0]
"Therefore, the lower LFW rate a training set contains, the more precisely it may expose the drawbacks of the algorithm.
",4.2 Impact of Training-test Overlap,[0],[0]
"To study the impact of training-test overlap, actually, the training-dev overlap, we pick different subsets from the training data and evaluate the models trained on them.",4.2 Impact of Training-test Overlap,[0],[0]
"At first, we control overlap by picking different sizes of the training data randomly.",4.2 Impact of Training-test Overlap,[0],[0]
"Figure 5(a) shows that, as the training data size grows, the OOV rate, which is the overlap indicator, decreases and the F1 score of EE2ECoref increases significantly.",4.2 Impact of Training-test Overlap,[0],[0]
"Figure 5(b) shows that when training set size increases, the OOV rate and the LFW rate drop together.",4.2 Impact of Training-test Overlap,[0],[0]
"Then, to remove the impact of data size, we pick training sets which have a fixed size but different overlaps with the development set vocabulary.",4.2 Impact of Training-test Overlap,[0],[0]
The OOV rates and F1 scores of these subsets are shown in Figure 5(c).,4.2 Impact of Training-test Overlap,[0],[0]
"This experiment verifies the positive cor-
3In our experiments, a word is defined as low-frequency if it appears in the training set less than 10 times.
",4.2 Impact of Training-test Overlap,[0],[0]
"4There are 3 kinds of error cases of coreference resolution: false-new, false-link and wrong-link.",4.2 Impact of Training-test Overlap,[0],[0]
"In our experiments, the related mentions include: the current mention in all 3 kinds of cases, the nearest gold antecedent in false-new and wrong-link and the false referred antecedent in false-link and wrong-link.
relation between training-dev overlap and coreference resolution performance suggested by (Sadat Moosavi and Strube, 2017).",4.2 Impact of Training-test Overlap,[0],[0]
"Figure 5(d) shows that for training sets with the same size, the OOV rate and the LFW rate also drop together.
",4.2 Impact of Training-test Overlap,[0],[0]
We observe that the training set of 2.5K documents in Figure 5(a) has a higher model performance than all the training sets in Figure 5(c).,4.2 Impact of Training-test Overlap,[0],[0]
This is not expected.,4.2 Impact of Training-test Overlap,[0],[0]
"One hypothesis is that the lower performance in Figure 5(c) is due to the smaller diversity of these training sets, which are selected to have certain training-dev OOV rates.
",4.2 Impact of Training-test Overlap,[0],[0]
The training-dev LFW rate of OntoNotes is 34.8%.,4.2 Impact of Training-test Overlap,[0],[0]
"As a comparison, the number for PreCo is 12.3%.",4.2 Impact of Training-test Overlap,[0],[0]
A subset of PreCo with a similar token number to OntoNotes has a LFW rate of 33.0%.,4.2 Impact of Training-test Overlap,[0],[0]
This indicates that research of coreference algorithms on PreCo will be much more efficient than on OntoNotes.,4.2 Impact of Training-test Overlap,[0],[0]
"Even if we can ignore the LFW error cases, there are others related to low-frequency word senses, phrases and sentence structures, which are hard to filter out.",4.2 Impact of Training-test Overlap,[0],[0]
They will also obscure the error analysis.,4.2 Impact of Training-test Overlap,[0],[0]
"It is reasonable to believe that training-dev overlap impacts the rate
of these error cases in a similar way to impact LFW rate.",4.2 Impact of Training-test Overlap,[0],[0]
"Since most coreference systems consist of a mention detection module and a mention clustering module, an important question is: with a perfect mention detection module, what is the model performance on coreference resolution?",4.3 Mention Detection,[0],[0]
"The answer would help us understand the bottlenecks of the entire system, by quantifying the impact of the mention detection module on the final F1 score.",4.3 Mention Detection,[0],[0]
"(Lee et al., 2017) gave an answer by taking ground truth non-singleton mentions as the input of the coreference resolver for both training and evaluation, assuming that the perfect mention detector can also make perfect anaphoricity decisions, e.g., to decide whether a mention should be linked to an antecedent.",4.3 Mention Detection,[0],[0]
"But this assumption can be violated since mention detectors usually take local information but anaphoricity decisions usually need more context, nearly as much as entity identification.",4.3 Mention Detection,[0],[0]
"The anaphoricity decisions should be made in the mention clustering module.
",4.3 Mention Detection,[0],[0]
We argue that a better way to answer the question is to take all ground truth mentions (including singletons) for coreference.,4.3 Mention Detection,[0],[0]
This operation is not feasible in OntoNotes since it does not have annotations for singleton mentions.,4.3 Mention Detection,[0],[0]
We do this on PreCo and the results are shown in Table 6.,4.3 Mention Detection,[0],[0]
There is an obvious difference between the F1 scores achieved with all gold mentions and non-singleton gold mentions.,4.3 Mention Detection,[0],[0]
"Therefore, the room for improvement by better mention detection is not as enormous as suggested in (Lee et al., 2017).",4.3 Mention Detection,[0],[0]
The major challenge remained in coreference resolution is mention clustering.,4.3 Mention Detection,[0],[0]
"In this paper, we propose a large-scale coreference resolution dataset to overcome the limitations of existing ones.",5 Conclusion,[0],[0]
"Our dataset, PreCo, features higher training-test overlap, about 10 times larger scale than previous datasets, and singleton mention annotations.",5 Conclusion,[0],[0]
"By evaluating a state-of-the-art coreference resolver, we show that there is a wide gap between the model and human performance, which demonstrated challenges of the dataset.",5 Conclusion,[0],[0]
We verified the expectation that PreCo’s higher trainingtest overlap helps research on coreference resolution.,5 Conclusion,[0],[0]
"For the first time, we quantified the impact of mention detector to the entire system, thanks to our singleton mention annotations.",5 Conclusion,[0],[0]
"We make the dataset public, and hope it will stimulate further research on coreference resolution.",5 Conclusion,[0],[0]
"We introduce PreCo, a large-scale English dataset for coreference resolution.",abstractText,[0],[0]
"The dataset is designed to embody the core challenges in coreference, such as entity representation, by alleviating the challenge of low overlap between training and test sets and enabling separated analysis of mention detection and mention clustering.",abstractText,[0],[0]
"To strengthen the training-test overlap, we collect a large corpus of 38K documents and 12.5M words which are mostly from the vocabulary of Englishspeaking preschoolers.",abstractText,[0],[0]
"Experiments show that with higher training-test overlap, error analysis on PreCo is more efficient than the one on OntoNotes, a popular existing dataset.",abstractText,[0],[0]
"Furthermore, we annotate singleton mentions making it possible for the first time to quantify the influence that a mention detector makes on coreference resolution performance.",abstractText,[0],[0]
The dataset is freely available at https:// preschool-lab.github.io/PreCo/.,abstractText,[0],[0]
PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution,title,[0],[0]
"Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 11–20, Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics",text,[0],[0]
Natural language understanding (NLU) requires analysis beyond the sentence-level.,1 Introduction,[0],[0]
"For example, an entity may be mentioned multiple times in a discourse, participating in various events, where each event may itself be referenced elsewhere in the text.",1 Introduction,[0],[0]
"Traditionally the task of coreference resolution has been defined as finding those entity mentions within a single document that co-refer, while crossdocument coreference resolution considers a wider discourse context across many documents, yet still pertains strictly to entities.
",1 Introduction,[0],[0]
"Predicate argument alignment, or entity-event cross-document coreference resolution, enlarges the set of possible co-referent elements to include the mentions of situations in which entities participate.",1 Introduction,[0],[0]
"This expanded definition drives practitioners towards a more complete model of NLU, where systems must not only consider who is mentioned, but also what happened.",1 Introduction,[0],[0]
"However, despite the drive towards an expanded notion of discourse, models typically are formulated with strong notions of localindependence: viewing a multi-document task as one limited to individual pairs of sentences.",1 Introduction,[0],[0]
"This creates a mis-match between the goals of such work – considering entire documents – with the systems – consider individual sentences.
",1 Introduction,[0],[0]
"In this work, we consider a system that takes a document level view in considering coreference for entities and predictions: the task of predicate argument linking.",1 Introduction,[0],[0]
"We treat this task as a global inference problem, leveraging multiple sources of semantic information identified at the document level.",1 Introduction,[0],[0]
"Global inference for this problem is mostly unexplored, with the exception of Lee et al. (2012) (discussed in § 8).",1 Introduction,[0],[0]
"Especially novel here is the use of document-level temporal constraints on events, representing a next step forward on the path to full understanding.
",1 Introduction,[0],[0]
Our approach avoids the pitfalls of local inference while still remaining fast and exact.,1 Introduction,[0],[0]
"We use the pairwise features of a very strong predicate argument aligner (Wolfe et al., 2013) (competitive with the state-of-the-art (Roth, 2014)), and add quadratic factors that constrain local decisions based on global document information.",1 Introduction,[0],[0]
These global factors lead to superior performance compared to the previous state-of-the-art.,1 Introduction,[0],[0]
We release both our code and data.1,1 Introduction,[0],[0]
Consider the two sentences from the document pair shown in Figure 1.,2 Model,[0],[0]
"These sentences describe the same event, although with different details.",2 Model,[0],[0]
"The source sentence has four predicates and four arguments, while the target has three predicates and three arguments.",2 Model,[0],[0]
"In this case, one of the predicates from each sentence aligns, as do three of the arguments.",2 Model,[0],[0]
We also show additional information potentially helpful to determining alignments: temporal relations between the predicates.,2 Model,[0],[0]
"The goal of predicate argument alignment is to assign these links indicating coreferent predicates and arguments across a document pair (Roth and Frank, 2012).
",2 Model,[0],[0]
"Previous work by Wolfe et al. (2013) formulated
1https://github.com/hltcoe/parma2
11
Figure 1: An example analysis and predicate argument alignment task between a source and target document.",2 Model,[0],[0]
"Predicates appear as hollow ovals, have blue mentions, and are aligned considering their arguments (dashed lines).",2 Model,[0],[0]
"Arguments, in black diamonds with green mentions, represent a document-level entity (coreference chain), and are aligned using their predicate structure and mention-level features.",2 Model,[0],[0]
The alignment choices appear in the middle in red.,2 Model,[0],[0]
"Temporal relation information is lifted into the global inference over alignments.
",2 Model,[0],[0]
"this as a binary classification problem: given a pair of arguments or predicates, construct features and score the pair, where scores above threshold indicate links.",2 Model,[0],[0]
"A binary classification framework has advantages: it’s fast since individual decisions can be made quickly, but it comes at the cost of global information across links.",2 Model,[0],[0]
The result may be links that conflict in their interpretation of the document.,2 Model,[0],[0]
"Figure 1 makes clear that jointly considering all links at once can aid individual decisions, for example, by including temporal ordering of predicates.
",2 Model,[0],[0]
The global nature of this task is similar to word alignment for machine translation (MT).,2 Model,[0],[0]
"Many systems consider alignment links between words individually, selecting the best link for each word independently of the other words in the sentence.",2 Model,[0],[0]
"Just as with an independent linking strategy in predicate argument alignment, this can lead to inconsistencies in the output.",2 Model,[0],[0]
"Lacoste-Julien et al. (2006) introduced a model that jointly resolved word alignments based on the introduction of quadratic variables, factors that depend on two alignment decisions which characterize patterns that span word-word links.",2 Model,[0],[0]
"Their approach achieved improved results even in the presence of little training data.
",2 Model,[0],[0]
We present a global predicate argument alignment model based on considering quadratic interactions between alignment variables to captures patterns we expect in coherent discourse.,2 Model,[0],[0]
"We introduce factors which are comprised of a binary variable, multiple quadratic constraints on that variable, and features that determine the cost associated with that variable in order to characterize the dependence between alignment decisions.
",2 Model,[0],[0]
"While the mathematical framework we use is similar to Lacoste-Julien et al. (2006), predicate argument alignment greatly differs from word alignment; thus our joint factors are based on different sources of regularity.",2 Model,[0],[0]
"Word alignment favors monotonicity in word order, but this effect is very weak in predicate argument alignment: aligned items can be spread throughout a document, and are often nested, gapped, or shuffled.",2 Model,[0],[0]
"Instead, we encode assumptions about consistency of temporal relations between coreferent events, coherence between predicates and arguments that appear in both documents, and fertility (to prevent over-alignment).",2 Model,[0],[0]
"We also note that our setting has much less data than typical word alignment tasks, as well as richer features that utilize semantic resources.
",2 Model,[0],[0]
"Notation An alignment between an item indexed by i in the source document and j in the target document is represented by variable zij ∈ {0, 1}, where zij = 1 indicates that items i and j are aligned.",2 Model,[0],[0]
"In some cases, we will explicitly indicate when the two items are predicates as zpij ; an argument alignment will be zaij .",2 Model,[0],[0]
"We represent all alignments for a document pair as matrix z.
For clarity, we omit any variable representing observed data when discussing feature functions; alignment variables are endowed with this information.",2 Model,[0],[0]
"For each pair of items we use “local” feature functions f(·) and corresponding parameters w, which capture the similarity between two items without the context of other alignments.
sij = w · f(zij) (1)
where sij is the score of linking items i and j. Using only local features, our system would greedily select alignments.",2 Model,[0],[0]
To capture global aspects we add joint factors that capture effects between alignment variables.,2 Model,[0],[0]
Each joint factor φ is comprised of a constrained binary variable zφ associated with features f(φ) that indicates when the factor is active.,2 Model,[0],[0]
"Together with parameters w these form additional scores sφ for the objective:
sφ = w · f(φ) (2)
The full linear scoring function on alignments sums over both local similarity and joint factors:∑
ij sijzij + ∑ φ∈Φ sφzφ.",2 Model,[0],[0]
"(3)
Lastly, it is convenient to describe the local feature functions and their corresponding alignment variable as factors with no constraints, and we will do so when describing the full score function.",2 Model,[0],[0]
"Local factors encode features based on the mention pair, which include a wide variety of similarity measures, e.g. whether two headwords appear as synonyms in WordNet, gender agreement based on possessive pronouns.",3 Local Factors,[0],[0]
"We adopt the features of Wolfe et al. (2013), a strong baseline system
which doesn’t use global inference.2 These features are built on top of a variety of semantic resources (PPDB (Ganitkevitch et al., 2013), WordNet (Miller, 1995), FrameNet (Baker et al., 1998)) and methods for comparing mentions (tree edit distance (Yao et al., 2013), string transducer (Andrews et al., 2012)).",3 Local Factors,[0],[0]
"Our goal is to develop joint factors that improve over the feature rich local factors baseline by considering global information.
",4 Joint Factors,[0],[0]
Fertility A common mistake when making independent classification decisions is to align many source items to a single target item.,4 Joint Factors,[0],[0]
"While each link looks promising on its own, they clearly cannot all be right.",4 Joint Factors,[0],[0]
"Empirically, the training set reveals that many to one alignments are uncommon; thus many to one predictions are likely errors.",4 Joint Factors,[0],[0]
"We add a fertility factor for predicates and arguments, where fertility is defined as the number of links to an item.",4 Joint Factors,[0],[0]
Higher fertilities are undesired and are thus penalized.,4 Joint Factors,[0],[0]
"Formally, for matrix z, the fertility of a row i or column j is the sum of that row or column.",4 Joint Factors,[0],[0]
"We discuss fertility in terms of rows below.
",4 Joint Factors,[0],[0]
We include two types of fertility factors.,4 Joint Factors,[0],[0]
"First, factor φfert1 distinguishes between rows with at least one link from those with none.",4 Joint Factors,[0],[0]
"For row i, we add one instance of the linear factor φfert1 with constraints
zφfert1 ≥ zij ∀j (4)
",4 Joint Factors,[0],[0]
"The cost associated with zφfert1 , which we will refer to as sfert1, will be incurred any time an item is mentioned in both documents.",4 Joint Factors,[0],[0]
"For data sets with many singletons, sfert1 more strongly penalizes nonsingleton rows, reflecting this pattern in the training data.",4 Joint Factors,[0],[0]
"We make sfert1 parametric, where the features of the φfert1 factor allow us to learn different weights for predicates and arguments, as well as the size of the row, i.e. number of items in the pairing.
",4 Joint Factors,[0],[0]
"The second fertility factory φfert2 considers items with a fertility greater than one, penalizing items for having too many links.",4 Joint Factors,[0],[0]
"Its binary variable has the
2Some features inspect the apparent predicate argument structure, based on things like dependency parses, but the model may not inspect more than one of its own decisions (joint factors) while scoring an alignment.
",4 Joint Factors,[0],[0]
"quadratic constraints:
zφfert2 ≥ zijzik ∀j < k",4 Joint Factors,[0],[0]
"(5)
",4 Joint Factors,[0],[0]
"This factor penalizes rows that have fertility of at least two, but does not distinguish beyond that.",4 Joint Factors,[0],[0]
"An alternative would be to introduce a factor for every pair of variables in a row, each with one constraint.",4 Joint Factors,[0],[0]
This would heavily penalize fertilities greater than two.,4 Joint Factors,[0],[0]
"We found that the resulting quadratic program took longer to solve and gave worse results.
",4 Joint Factors,[0],[0]
"Since documents have been processed to identify in-document coreference chains, we do not expect multiple arguments from a source document to align to a single target item.",4 Joint Factors,[0],[0]
"For this reason, we expect φfert2 for arguments to have a large negative weight.",4 Joint Factors,[0],[0]
"In contrast, since predicates do not form chains, we may have multiple source predicates for one target.
",4 Joint Factors,[0],[0]
We note an important difference between our fertility factor compared with Lacoste-Julien et al. (2006).,4 Joint Factors,[0],[0]
"We parameterize fertility for only two cases (1 and 2) whereas they consider fertility factors from 2 to D. We do not parameterize fertilities higher than two because they are not common in our dataset and come at a high computational cost.
",4 Joint Factors,[0],[0]
"The features f(φ) for both φfert1 and φfert2 are an intercept feature (which always fires), indicator features for whether this row corresponds to an argument or a predicate, and a discretized feature for how many alignments are in this row.
",4 Joint Factors,[0],[0]
Predicate Argument Structure We expect structure among links that involve a predicate and its associated arguments.,4 Joint Factors,[0],[0]
"Therefore, we add joint factors that consider a predicate and its associated alignments: the predicate argument structure.",4 Joint Factors,[0],[0]
"We determine this structure from a dependency parse, though the idea is general to any semantic binding, e.g. FrameNet or Propbank style parses.",4 Joint Factors,[0],[0]
"Given a coherent discourse, there are several expected types of patterns in the PAS; we add factors for these.
",4 Joint Factors,[0],[0]
"Predicate-centric We begin with a predicatecentric factor, which views scores an alignment between predicates based on their arguments, i.e. the two predicates share the same arguments.",4 Joint Factors,[0],[0]
"Ideally, two predicates can only align when their arguments are coreferent.",4 Joint Factors,[0],[0]
"However, in practice we may incorrectly resolve argument links, or there may be
implicit arguments that do not appear as syntactic dependencies of the predicate trigger.",4 Joint Factors,[0],[0]
"Therefore, we settle for a weaker condition, that there should be some overlap in the arguments of two coreferent predicates.
",4 Joint Factors,[0],[0]
"For every predicate alignment zpij , we add a factor φpsa whose score spsa is a penalty for having no argument overlap; predicates share arguments (psa).",4 Joint Factors,[0],[0]
"To constrain the variable of φpsa, we add a quadratic constraint that considers every possible pair of argument alignments that might overlap:
zφpsa",4 Joint Factors,[0],[0]
"≥ zpij ( 1− max
k∈args(pi) l∈args(pj)
zakl )
(6)
where args(pi) finds the indices of all arguments governed by the predicate pi.
",4 Joint Factors,[0],[0]
Entity-centric We expect similar behavior from arguments (entities).,4 Joint Factors,[0],[0]
"If an entity appears in two documents, it is likely that this entity will be mentioned in the context of a common predicate, i.e. arguments share predicates (asp).",4 Joint Factors,[0],[0]
"For a given argument alignment zaij we add quadratic constraints so that zφasp represents a penalty for two arguments not sharing a single predicate:
zφasp ≥ zaij ( 1− max
k∈preds(ai) l∈preds(aj)
zpkl )
(7)
where preds(ai) finds the indices of all predicates that govern any mention of argument ai.
",4 Joint Factors,[0],[0]
"The features f(φ) for both psa and asp are an intercept feature and a bucketed count of the size of args(pi)× args(pj) or preds(ai)×preds(aj) respectively.
",4 Joint Factors,[0],[0]
"Temporal Information Temporal ordering, in contrast to textual ordering, can indicate when predicates cannot align: we expect aligned predicates in both documents to share the same temporal relations.",4 Joint Factors,[0],[0]
"SemEval 2013 included a task on predicting temporal relations between events (UzZaman et al., 2013).",4 Joint Factors,[0],[0]
"Many systems produced partial relations of events in a document based on lexical aspect and tense, as well as discourse connectives like “during” or “after”.",4 Joint Factors,[0],[0]
"We obtain temporal relations with CAEVO, a state-of-the-art sieve-based system (Chambers et al., 2014).
",4 Joint Factors,[0],[0]
"TimeML (Pustejovsky et al., 2003), the format for specifying temporal relations, defines relations between predicates (e.g. immediately before and simultaneous), each with an inverse (e.g. immediately after and simultaneous respectively).",4 Joint Factors,[0],[0]
We will refer to a relation as R and its inverse as R−1.,4 Joint Factors,[0],[0]
"Suppose we had pa and pb in the source document, px and py in the target document, and paR1pb, pxR2py.",4 Joint Factors,[0],[0]
"Given this configuration the following alignments conflict with the in-doc relations:
zax zby zay zbx In-Doc Relations",4 Joint Factors,[0],[0]
"* * 1 1 R1 = R2 1 1 * * R1 = R−12
where 1 means there is a link and * means there is a link or no link (wildcard).",4 Joint Factors,[0],[0]
"The simplest example that fits this pattern is: ‘a before b’, ‘x before y’, ‘a corefers with y’, and ‘b corefers with x’ implies a conflict.
",4 Joint Factors,[0],[0]
We introduce a factor that penalizes these conflicting configurations.,4 Joint Factors,[0],[0]
"In every instance where the predicted temporal relation for a pair of predicate alignments matches one of the conflict patterns above, we add a factor using zφtemp :
zφtemp ≥ zayzbx if paR1pb, pxR2py, R1 = R2 zφtemp ≥ zaxzby if paR1pb, pxR2py, R1 = R−12
(8)
Thus sφtemp is the cost of disagreeing with the indoc temporal relations.",4 Joint Factors,[0],[0]
This is a general technique for incorporating relational information into coreference decisions.,4 Joint Factors,[0],[0]
"It only requires specifying when two relations are incompatible, e.g. spouseOf and siblingOf are incompatible relations (in most states).",4 Joint Factors,[0],[0]
"We leave this for future work.
",4 Joint Factors,[0],[0]
"Since CAEVO gives each relation prediction a probability, we incorporate this into the feature by indicating the probability of a conflict not arising:
f(φtemp) = log ( 1− p(R1)p(R2) + )",4 Joint Factors,[0],[0]
"(9)
avoids large negative values since CAEVO probabilities are not perfectly calibrated.",4 Joint Factors,[0],[0]
"We use = 0.1, allowing feature values of at most −2.3.",4 Joint Factors,[0],[0]
Summary The objective is a linear function over binary variables.,4 Joint Factors,[0],[0]
"There is a local similarity score
def train(alignments): w = init_weights() working_set = set() while True: xi = solve_ILP(w, working_set) c = most_violated_constraint(w, alignments) working_set.add(c) if hinge(c, w) <",4 Joint Factors,[0],[0]
"xi: break
coefficient on every alignment variable, and a joint factor similarity score on every quadratic variable.",4 Joint Factors,[0],[0]
These quadratic variables are constrained by products of the original alignment variables.,4 Joint Factors,[0],[0]
Decoding an alignment requires solving this quadratically constrained integer program; in practice is can be solved quickly without relations.,4 Joint Factors,[0],[0]
Learning We use the supervised structured SVM formulation of Joachims et al. (2009).,5 Inference,[0],[0]
"As is common in structure prediction we use margin rescaling and 1 slack variable, with the structural SVM objective:
min w ||w||22 +",5 Inference,[0],[0]
"Cξ
s.t. ξ ≥ 0
ξ +",5 Inference,[0],[0]
N∑ i=1,5 Inference,[0],[0]
w · f(zi) ≥,5 Inference,[0],[0]
N∑ i=1,5 Inference,[0],[0]
w · f(ẑi),5 Inference,[0],[0]
"+ ∆(zi, ẑi)
",5 Inference,[0],[0]
"∀ẑi ∈ Zi (10)
where Zi is the set of all possible alignments that have the same shape as zi.
",5 Inference,[0],[0]
"The score function for an alignment uses three types of terms: weights, features, and alignment variables.",5 Inference,[0],[0]
"When we decode, we take the product of the weights and the features to get the costs for the ILP (e.g. sφ = w · f(φ)).",5 Inference,[0],[0]
"When we optimize our SVM objective, we take the product of the alignment variables and the features to get modified features for the SVM:
f(z) = ∑ ij zijf(zij) + ∑ φ∈Φ zφf(φ) (11)
Since we cannot iterate over the exponentially many margin constraints, we solve for this optimization using the cutting-plane learning algorithm.",5 Inference,[0],[0]
"This algorithm repeatedly asks the “separation oracle” for the most violated SVM constraint, which finds this constraint by solving:
arg max ẑ1...ẑN ∑ i w · f(ẑi) + ∆(zi, ẑi) (12)
subject to the constraints defined by the joint factors.",5 Inference,[0],[0]
"When the separation oracle returns a constraint that is not violated or is already in the working set, then we have a guarantee that we solved the original SVM problem with exponentially many constraints.",5 Inference,[0],[0]
"This is the most time-consuming aspect of learning, but since the problem decomposes over document alignments, we cache solutions on a per document alignment basis.",5 Inference,[0],[0]
"With caching, we only call the separation oracle around 100-300 times.
",5 Inference,[0],[0]
"We implement the separation oracle using an ILP solver, CPLEX,3 due to complexity of the discrete optimization problem: there are 2m n possible alignments for and m×n alignment grid.",5 Inference,[0],[0]
"In practice this is solved very efficiently, taking less than a third of a second per document alignment on average.",5 Inference,[0],[0]
"We would like ∆ to be F1, but we need a decomposable loss to include it in a linear objective (Taskar et al., 2003).",5 Inference,[0],[0]
"Instead, we use Hamming loss as a surrogate, as in Lacoste-Julien et al. (2006).
",5 Inference,[0],[0]
"Our training data is heavily biased towards negative examples, performing poorly on F1 since precision and recall are unbalanced.",5 Inference,[0],[0]
"We use an asymmetric version of Hamming loss that incurs cFP cost for predicting an alignment for two unaligned
3http://www-01.ibm.com/software/ commerce/optimization/cplex-optimizer/
items and cFN for predicting no alignment for two aligned items.",5 Inference,[0],[0]
"We fixed cFP = 1 and tuned cFN ∈ {1, 2, 3, 4} on dev data.",5 Inference,[0],[0]
"Additionally we found it useful to tune the scale of the loss function across {12 , 1, 2, 4}.",5 Inference,[0],[0]
"Previous work, such as Joachims et al. (2009), use a hand-chosen constant for the scale of the Hamming loss, but we observe some sensitivity in this parameter and choose to optimize it.
",5 Inference,[0],[0]
"Decoding Following Wolfe et al. (2013), we tune the threshold for classification τ on dev data to maximize F1 (via linesearch).",5 Inference,[0],[0]
For SVMs τ is typically fixed at 0: this is not necessarily good practice when your training loss differs from test loss (Hamming vs F1).,5 Inference,[0],[0]
In our case this extra parameter is worth allocating a portion of training data to enable tuning.,5 Inference,[0],[0]
"Tuning τ addresses the same problem as using an asymmetric Hamming loss, but we found that doing both led to better results.4",5 Inference,[0],[0]
"Since we are using a global scoring function rather than a set of classifications, τ is implemented as a test-time unary factor on every alignment.",5 Inference,[0],[0]
Data We consider two datasets for evaluation.,6 Experiments,[0],[0]
The first is a cross-document entity and event coreference resolution dataset called the Extended Event Coref Bank (EECB) created by Lee et al. (2012) and based on a corpus from Bejan and Harabagiu (2010).,6 Experiments,[0],[0]
The dataset contains clusters of news articles taken from Google News with annotations about coreference over entities and events.,6 Experiments,[0],[0]
"Following the procedure of Wolfe et al. (2013), we select the first document in every cluster and pair it with every other document in the cluster.
",6 Experiments,[0],[0]
The second dataset (RF) comes from Roth and Frank (2012).,6 Experiments,[0],[0]
"The dataset contains pairs of news articles that describe the same news story, and are annotated for predicate links between the document pairs.",6 Experiments,[0],[0]
"Due to the lack of annotated arguments, we can only report predicate linking performance and the psa and asp factors do not apply.",6 Experiments,[0],[0]
"Lastly, the size of the RF data should be noted as it is much smaller than EECB:",6 Experiments,[0],[0]
"the test set has 60 document pairs and the dev set has 10 document pairs.
4Only tuning τ performed almost as well as tuning τ and the Hamming loss, but not tuning τ performed much worse than only tuning the Hamming loss at train time.
",6 Experiments,[0],[0]
Both datasets are annotated with parses and indocument coreference labels provided by the toolset of Napoles et al. (2012)5 and are available with our code release.,6 Experiments,[0],[0]
"Due to the small data size, we use kfold cross validation for both datasets.",6 Experiments,[0],[0]
We choose k = 10 for RF due to its very small size (more folds give more training examples) and k = 5 on EECB to save computation time (amount of training data in EECB is less of a concern).,6 Experiments,[0],[0]
Hyperparameters were chosen by hand using using cross validation on the EECB dataset using F1 as the criteria (rather than Hamming).,6 Experiments,[0],[0]
"Figures report averages across these folds.
",6 Experiments,[0],[0]
"Systems Following Roth and Frank (2012) and Wolfe et al. (2013) we include a Lemma baseline for identifying alignments which will align any two predicates or arguments that have the same lemmatized head word.6 The Local baseline uses the same features as Wolfe et al., but none of our joint factors.",6 Experiments,[0],[0]
"In addition to running our joint model with all factors, we measure the efficacy of each individual factor by evaluating each with the local features.
",6 Experiments,[0],[0]
"For evaluation we use a generous version of F1 that is defined for alignment labels composed of sure, Gs, and possible links, Gp and the system’s proposed links H (following Cohn et al. (2008), Roth and Frank (2012) and Wolfe et al. (2013)).
",6 Experiments,[0],[0]
"P = |H ∩Gp| |H| R = |H ∩Gs| |Gs| F = 2PR P +R
Note that the EECB data does not have a sure and possible distinction, so Gs = Gp, resulting in standard F1.",6 Experiments,[0],[0]
"In addition to F1, we separately measure predicate and argument F1 to demonstrate where our model makes the largest improvements.
",6 Experiments,[0],[0]
We performed a one-sided paired-bootstrap test where the null hypothesis was that the joint model was no better than the Local baseline (described in Koehn (2004)).,6 Experiments,[0],[0]
"Cases where p < 0.05 are bolded.
",6 Experiments,[0],[0]
5https://github.com/cnap/anno-pipeline 6The lemma baseline is obviously sensitive to the lemmatizer used.,6 Experiments,[0],[0]
"We used the Stanford CoreNLP lemmatizer (Manning et al., 2014) and found it yielded slightly better results than previously reported as the lemma baseline (Roth and Frank, 2012), so we used it for all systems to ensure fairness and that the baseline is as strong as it could be.",6 Experiments,[0],[0]
Results for EECB and RF are reported in Table 7.,7 Results,[0],[0]
"As previously reported, using just local factors (features on pairs) improves over lemma baselines (Wolfe et al., 2013).",7 Results,[0],[0]
The joint factors make statistically significant gains over local factors in almost all experiments.,7 Results,[0],[0]
Fertility factors provide the largest improvements from any single constraint.,7 Results,[0],[0]
"A fertility penalty actually allows the pairwise weights to be more optimistic in that they can predict more alignments for reasonable pairs, allowing the fertility penalty to ensure only the best is chosen.",7 Results,[0],[0]
"This penalty also prevents the “garbage collecting” effect that arises for instances that have rare features (Brown et al., 1993).
",7 Results,[0],[0]
"Temporal constraints are relatively sparse, appearing just 2.8 times on average.",7 Results,[0],[0]
"Nevertheless, it was very helpful across all experiments, though only statistically significantly on the RF dataset.",7 Results,[0],[0]
This is one of the first results to demonstrate benefits of temporal relations affecting an downstream task.,7 Results,[0],[0]
"Perhaps surprisingly, these improvements result from a a temporal relation system that has relatively poor absolute performance.",7 Results,[0],[0]
"Despite this, improvements are possibly due to the orthogonal nature of temporal information; no other feature captures this signal.",7 Results,[0],[0]
"This suggests that future work on temporal relation prediction may yield further improvements and deserves more attention as a useful feature for semantic tasks in NLP.
",7 Results,[0],[0]
The predicate-centric factors improved performance significantly on both datasets.,7 Results,[0],[0]
"For the predicate-centric factor, when a predicate was aligned there is a 72.3% chance that there was at least one argument aligned as well, compared to only 14.1% of case of non-aligned predicates.",7 Results,[0],[0]
"As mentioned before, the reason the former number isn’t 100% is primarily due to implicit arguments and errors in argument identification.",7 Results,[0],[0]
"The argument-centric features helped almost as much as the predicate-centric version, but the improvements were not significant on the EECB dataset.",7 Results,[0],[0]
"Running the same diagnostic as the predicate-centric feature reveals similar support: in 57.1% of the cases where an argument was aligned, at least one predicate it partook in was aligned too, compared to 7.6% of cases for non-aligned arguments.",7 Results,[0],[0]
"Both the
EECB F1 P R Arg F1 Arg P Arg R",7 Results,[0],[0]
"Pred F1 Pred P Pred R
Lemma 68.1 79.3 * 59.6 61.7 79.1 * 50.6 75.0 87.3 * 65.7",7 Results,[0],[0]
Local 73.0 75.8 70.5 67.7 76.3 60.8 78.7 81.4 76.2 +Fertility 77.1 * 83.9 * 71.3 66.6 80.9 * 56.6 82.8 * 87.4 * 78.7 * +,7 Results,[0],[0]
"Predicate-centric 74.1 * 80.7 * 68.6 67.4 81.6 * 57.3 79.7 * 85.0 * 75.1 +Argument-centric 73.7 81.2 * 67.5 66.8 83.0 * 55.9 79.3 85.1 * 74.3 +Temporal 73.7 78.2 * 69.7 67.9 80.6 * 58.7 79.0 82.1 76.1 +All Factors 77.5 * 86.3 * 70.3 65.8 83.1 * 54.5 83.7 * 89.7 * 78.4 *
RF",7 Results,[0],[0]
"Pred F1 Pred P Pred R
Lemma 52.4 47.6 58.2 * Local 58.1 63.5 53.6 +Fertility 60.0 57.4 62.4 *",7 Results,[0],[0]
+Predicate-centric NA NA NA +Argument-centric NA NA NA +Temporal 59.0 57.4 60.6 *,7 Results,[0],[0]
"+All factors 59.4 56.9 62.2 *
Figure 3: Cross validation results for EECB (above) (Lee et al., 2012) and RF (left) (Roth and Frank, 2012).",7 Results,[0],[0]
"Statistically significant improvements from Local marked * (p < 0.05 using a one-sided pairedbootstrap test) and best results are bolded.
predicate- and argument-centric improve similarly across both predicates and arguments on EECB.
",7 Results,[0],[0]
"While each of the joint factors all improve over the baselines on RF, the full model with all the joint factors does not perform as well as with some factors excluded.",7 Results,[0],[0]
"Specifically, the fertility model performs the best.",7 Results,[0],[0]
"We attribute this small gap to lack of training data (RF only contains 64 training document pairs in our experiments), as this is not a problem on the larger EECB dataset.
",7 Results,[0],[0]
"Additionally, the joint models seem to trade precision for recall on the RF dataset compared to the Local baseline.",7 Results,[0],[0]
"Note that both models are tuned to maximize F1, so this tells you more about the shape of the ROC curve as opposed to either models’ ability to achieve either high precision or recall.",7 Results,[0],[0]
"Since we don’t see this behavior on the EECB corpus, it is more likely that this is a property of the data than the model.",7 Results,[0],[0]
"The task of predicate argument linking was introduced by Roth and Frank (2012), who used a graph parameterized by a small number of semantic features to express similarities between predicates and used min-cuts to produce an alignment.",8 Related Work,[0],[0]
"This was followed by Wolfe et al. (2013), who gave a locallyindependent, feature-rich log-linear model that utilized many lexical semantic resources, similar to the
sort employed in RTE challenges.",8 Related Work,[0],[0]
Lee et al. (2012) considered a similar problem but sought to produce clusters of entities and events rather than an alignment between two documents with the goal of improving coreference resolution.,8 Related Work,[0],[0]
They used features which consider previous event and entity coreference decisions to make future coreference decisions in a greedy manner.,8 Related Work,[0],[0]
"This differs from our model which is built on non-greedy joint inference, but much of the signal indicating when two mentions corefer or are aligned is similar.
",8 Related Work,[0],[0]
"In the context of in-document coreference resolution, Recasens et al. (2013) sought to overcome the problem of opaque mentions7 by finding highprecision paraphrases of entities by pivoting off verbs mentioned in similar documents.",8 Related Work,[0],[0]
"We address the issue of opaque mentions not by building a paraphrase table, but by jointly reasoning about entities that participate in coreferent events (c.f. §4); the approaches are complementary.
",8 Related Work,[0],[0]
In this work we incorporate ordering information of events.,8 Related Work,[0],[0]
"Though we consider it an upstream task, there is a line of work trying to predict temporal relations between events (Pustejovsky et al., 2003; Mani et al., 2006; Chambers et al., 2014).",8 Related Work,[0],[0]
"Our results indicate this is a useful source of information, one of the first results to show an improvement from this
7A lexically disparate description of an entity.
type of system (Glavaš and Šnajder, 2013).",8 Related Work,[0],[0]
"We utilize an ILP to improve upon a pipelined system, similar to Roth and Yih (2004), but our work differs in that we do not use piecewise-trained classifiers.",8 Related Work,[0],[0]
Our local similarity scores are calibrated according to a global objective by propagating the gradient back from the loss to every parameter in the model.,8 Related Work,[0],[0]
"When using piecewise training, local classifiers must focus more on recall (in the spirit of Weiss and Taskar (2010)) than they would for an ordinary classification task with no global objective.",8 Related Work,[0],[0]
Our method trains classifiers jointly with a global convex objective.,8 Related Work,[0],[0]
"While our training procedure requires decoding an integer program, the parameters we learn are globally optimal.",8 Related Work,[0],[0]
"We presented a max-margin quadratic cost model for predicate argument alignment, seeking to exploit discourse level semantic features to improve on previous, locally independent approaches.",9 Conclusion,[0],[0]
"Our model includes factors that consider fertility of predicates and arguments, the predicate argument structure present in coherent discourses, and soft constraints on predicate coreference determined by a temporal relation classifier.",9 Conclusion,[0],[0]
We have shown that this model significantly improves upon prior work which uses extensive lexical resources but without the benefit of joint inference.,9 Conclusion,[0],[0]
"Additionally, this is one of the first demonstrations of the benefits of temporal relation identification.",9 Conclusion,[0],[0]
"Overall, this work demonstrates the benefits of considering global document information as part of natural language understanding.
",9 Conclusion,[0],[0]
"Future work should extend the problem formulation of predicate argument alignment to consider incremental linking: starting with a pair of documents, perform linking, and then continue to add in documents over time.",9 Conclusion,[0],[0]
"This problem formulation would capture the evolution of a breaking news story, which closely matches the type of data (news articles) considered in this work (EECB and RF datasets).",9 Conclusion,[0],[0]
"This formulation ties into existing work on news summarization, topic detection and tracking, an multi-document NLU.",9 Conclusion,[0],[0]
"This goes hand with work on better intra-document relation prediction methods, such as the temporal relation model used in this work, to lead to better joint linking decisions.",9 Conclusion,[0],[0]
We present a joint model for predicate argument alignment.,abstractText,[0],[0]
"We leverage multiple sources of semantic information, including temporal ordering constraints between events.",abstractText,[0],[0]
"These are combined in a max-margin framework to find a globally consistent view of entities and events across multiple documents, which leads to improvements over a very strong local baseline.",abstractText,[0],[0]
Predicate Argument Alignment using a Global Coherence Model,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 450–455 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
450
Because obtaining training data is often the most difficult part of an NLP or ML project, we develop methods for predicting how much data is required to achieve a desired test accuracy by extrapolating results from systems trained on a small pilot training dataset. We model how accuracy varies as a function of training size on subsets of the pilot data, and use that model to predict how much training data would be required to achieve the desired accuracy. We introduce a new performance extrapolation task to evaluate how well different extrapolations predict system accuracy on larger training sets. We show that details of hyperparameter optimisation and the extrapolation models can have dramatic effects in a document classification task. We believe this is an important first step in developing methods for estimating the resources required to meet specific engineering performance targets.",text,[0],[0]
An engineering discipline should be able to predict the cost of a project before the project is started.,1 Introduction,[0],[0]
"Because training data is often the most expensive part of an NLP or ML project, it is important to estimate how much training data required for a system to achieve a target accuracy.",1 Introduction,[0],[0]
"Unfortunately our field only offers fairly impractical advice, e.g., that more data increases accuracy (Banko and Brill, 2001); we currently have no practical methods for estimating how much data or what quality of data is required to achieve a target accuracy goal.",1 Introduction,[0],[0]
"Imagine if bridge construction was planned the way we build our systems!
",1 Introduction,[0],[0]
"Our long-term goal is to develop practical methods for designing systems that achieve target performance specifications, including identifying the amount of training data that the system will require.",1 Introduction,[0],[0]
This paper starts to address this goal by introducing an extrapolation methodology that predicts a system’s accuracy on a larger dataset from its performance on subsets of much smaller pilot data.,1 Introduction,[0],[0]
These extrapolations allow us to estimate how much training data a system will require to achieve a target accuracy.,1 Introduction,[0],[0]
"We focus on a specific task (document classification) using a specific system (the fastText classifier of Joulin et al. (2016)), and leave to future work to determine if our approach and results generalise to other tasks and systems.
",1 Introduction,[0],[0]
We introduce an accuracy extrapolation task that can be used to evaluate different extrapolation models.,1 Introduction,[0],[0]
We describe three well-known extrapolation models and evaluate them on a document classification dataset.,1 Introduction,[0],[0]
"On our development data the biased power-law method with binomial item weighting performs best, so we propose it should be a baseline for future research.",1 Introduction,[0],[0]
"We demonstrate the importance of hyperparameter optimisation on each different-sized data subset (rather than just optimising on the largest data subset) and item weighting, and show that these can have a dramatic impact on extrapolation, especially from small pilot data sets.",1 Introduction,[0],[0]
"The data and code for all experiments in this paper, including the R code for the graphics, is available from http://web.",1 Introduction,[0],[0]
science.mq.edu.au/˜mjohnson.,1 Introduction,[0],[0]
"Power analysis (Cohen, 1992) is widely-used statistical technique (e.g., in biomedical trials) for predicting the number of measurements required in an experimental design; we aim to develop sim-
ilar techniques for NLP and ML systems.",2 Related work,[0],[0]
There is a large body of research on the relationship between training data size and system performance.,2 Related work,[0],[0]
Geman et al. (1992) decompose the squared error of a model into a bias term (due to model errors) and a variance term (due to statistical noise).,2 Related work,[0],[0]
"Bias does not vary with training data size n, but the error due to variance should decrease as O(1/√n) if the training observations are independent (Domingos, 2000a,b).",2 Related work,[0],[0]
"The power-law models used in this paper have been investigated many times in prior literature (Haussler et al., 1996; Mukherjee et al., 2003; Figueroa et al., 2012; Beleites et al., 2013; Hajian-Tilaki, 2014; Cho et al., 2015).",2 Related work,[0],[0]
"Sun et al. (2017), Barone et al. (2017) and the concurrent unpublished work by Hestness et al. (2017) point out that these power-law models describe modern ML and NLP systems quite well, including complex deep-learning systems, so we expect our results to generalise to these systems.
",2 Related work,[0],[0]
This paper differs from prior work in that we explicitly focus on the task of extrapolating system performance from small pilot data.,2 Related work,[0],[0]
"We introduce a new evaluation task to compare the effectiveness of different models for this extrapolation, and demonstrate the importance of per-subset hyperparameter optimisation and item weighting, which prior work did not investigate.",2 Related work,[0],[0]
"We are given a system whose accuracy on a large dataset we wish to predict, but only a smaller pilot dataset is available.",3 Models for extrapolating pilot data,[0],[0]
"We train the system on different-sized subsets of the pilot dataset, and use the results of those training runs to estimate how the system’s accuracy varies as a function of training data size.
",3 Models for extrapolating pilot data,[0],[0]
"We focus on predicting the minimum error rate e(n) that the system can achieve on a dataset of size n after hyperparameter optimisation (where the error rate is 1−accuracy for a classifier) given a pilot dataset of size m n (in the task below, m = n/2 or m = n/10).",3 Models for extrapolating pilot data,[0],[0]
"We investigate three different extrapolation models of e(n) in this paper:
• Power law: ê(n) = bnc • Inverse square-root: ê(n) =",3 Models for extrapolating pilot data,[0],[0]
a+ bn−1/2 •,3 Models for extrapolating pilot data,[0],[0]
"Biased power law: ê(n) = a+ bnc
Here ê(n) is the estimate of e(n), and a, b and c are adjustable parameters that are estimated based on the system’s performance on the pilot dataset.
",3 Models for extrapolating pilot data,[0],[0]
"The inverse square-root curve is what one would expect if the error is distributed according to a Bias-Variance decomposition (Geman et al., 1992) with a constant bias term a and a variance term that asymptotically follows the Central Limit Theorem.",3 Models for extrapolating pilot data,[0],[0]
We fit these models using weighted least squares regression.,3 Models for extrapolating pilot data,[0],[0]
"Each data point or item in the regression is the result of a run of the system on a subset of the pilot dataset.
",3 Models for extrapolating pilot data,[0],[0]
"Assuming that the underlying system has adjustable hyperparameters, the question arises: how should the hyperparameters be set?",3 Models for extrapolating pilot data,[0],[0]
"The computationally least demanding approach is to optimise the system’s hyperparameters on the full pilot dataset, and use these hyperparameters for all the runs on subsets of the pilot dataset.",3 Models for extrapolating pilot data,[0],[0]
"An alternative, computationally more demanding approach is to optimise the system’s hyperparameters separately on each of the subsets of the pilot dataset.",3 Models for extrapolating pilot data,[0],[0]
"Figure 1 shows an example where optimising the hyperparameters just on the full pilot dataset is clearly in-
ferior to optimising the hyperparameters on each subset of the pilot dataset.",3 Models for extrapolating pilot data,[0],[0]
"We show below that the more demanding approach of optimising on each subset is superior, especially when extrapolating from small pilot datasets.
",3 Models for extrapolating pilot data,[0],[0]
We also investigate how details of the regression fit affect the regression accuracy ê(n).,3 Models for extrapolating pilot data,[0],[0]
"We experimented with several link functions (we used the default Gaussian link here), but found that these had less impact than adjusting the item weights in the regression.",3 Models for extrapolating pilot data,[0],[0]
"Runs with smaller training sets presumably have higher variance, and since our goal is to extrapolate to larger datasets, it is reasonable to place more weight on items corresponding to larger datasets.",3 Models for extrapolating pilot data,[0],[0]
"We investigated three item weighting functions in regression:
• constant weights (1), • linear weights (n), and • binomial weights (n/e(1− e))
",3 Models for extrapolating pilot data,[0],[0]
"Linear weights are motivated by the assumption that the item variance follows the Central Limit Theorem, while the binomial weights are motivated by the assumption that item variance follows a binomial distribution (see the Supplemental Materials for further discussion).",3 Models for extrapolating pilot data,[0],[0]
"As Figure 2 makes clear, linear weights and binomial weights generally produce more accurate extrapolations than constant weights, so we use binomial weights in our evaluation in Table 2.",3 Models for extrapolating pilot data,[0],[0]
We used the fastText document classifier and the document classification corpora distributed with it; see Joulin et al. (2016) for full details.,4 A performance extrapolation task,[0],[0]
FastText’s speed and evaluation scripts make it easy to do the experiments described below.,4 A performance extrapolation task,[0],[0]
We fitted our extrapolation models to the fastText document classifier results on the 8 corpora distributed with the fastText classifier.,4 A performance extrapolation task,[0],[0]
"These corpora contain labelled documents for a document classification task, and come randomised and divided into training and test sections.",4 A performance extrapolation task,[0],[0]
"All our results are on these test sections.
",4 A performance extrapolation task,[0],[0]
The corpora were divided into development and evaluation corpora (each with train and test splits) as shown in table 1.,4 A performance extrapolation task,[0],[0]
"We use the amazon review polarity, sogou news, yahoo answers and yelp review full corpora as our test set (so these are only used in the final evaluation), while the ag news, dbpedia, amazon review full and
yelp review polarity were used as development corpora.",4 A performance extrapolation task,[0],[0]
"The development and evaluation sets contain document collections of roughly similar sizes and complexities, but no attempt was made to accurately “balance” the development and evaluation corpora.
",4 A performance extrapolation task,[0],[0]
"We trained the fastText classifier on 13 differently-sized prefixes of each training set that are approximately logarithmically spaced over two orders of magnitude (i.e., varying from 1⁄100 to all of the training corpus).",4 A performance extrapolation task,[0],[0]
"To explore the effect of hyperparameter tuning on extrapolation, for each prefix of each training set we trained a classifier on each of 1,079 different hyperparameter settings, varying the n-gram length, learning rate, dimensionality of the hidden units and the loss function (the fastText classifier crashed on 17 hyperparameter combinations; we did not investigate why).",4 A performance extrapolation task,[0],[0]
"We re-ran the entire process 8 times on randomlyshuffled versions of each training corpus.
",4 A performance extrapolation task,[0],[0]
"As expected, the minimum error configuration invariably requires the full training data.",4 A performance extrapolation task,[0],[0]
When extrapolating from subsets of a smaller pilot set (we explored pilot sets consisting of 0.1 and 0.5 of the full training data) there are two plausible ways of performing hyperparameter optimisation.,4 A performance extrapolation task,[0],[0]
"Ideally, one would optimise the hyperparameters for each subset of the pilot data considered (we selected the best-performing hyperparameters using grid search).",4 A performance extrapolation task,[0],[0]
"However, if one is not working with computationally efficient algorithms like fastText, one might be tempted to only optimise the hyperparameters once on all the pilot data, and use the hyperparameters optimised on all the pilot data when calculating the error rate on subsets of that pilot data.",4 A performance extrapolation task,[0],[0]
"As figure 2 and table 2 make clear, selecting the optimal hyperparameters for each subset of the pilot data generally produces better extrapolation results.",4 A performance extrapolation task,[0],[0]
Figure 1 shows how different ways of choosing hyperparameters can affect extrapolation.,4 A performance extrapolation task,[0],[0]
"As that figure shows, hyperparameters optimised on 50% of the training data perform very badly on 1% of the training data.",4 A performance extrapolation task,[0],[0]
"As figure 2 shows, this can lead simpler extrapolation models such as the power-law to dramatically underestimate the error on the full dataset.",4 A performance extrapolation task,[0],[0]
"Interestingly, more complex extrapolation models, such as the extended power-law model, often do much better.
",4 A performance extrapolation task,[0],[0]
"Based on the development corpora results presented in Figures 1 and 2, we choose the biased power law model (ê(n) = a+ bnc) with binomial
item weights (n/e(1− e)) as the model to evaluate on the evaluation corpora.
",4 A performance extrapolation task,[0],[0]
We evaluate an extrapolation by calculating the root-mean-square (RMS) of the relative residuals ê/e,4 A performance extrapolation task,[0],[0]
"− 1, where e is the minimum error achieved by the classifier with any hyperparameter setting when trained on the full training set, and ê is the predicted error made by the extrapolation model
from the pilot dataset.1
Unsurprisingly, Table 2 shows that extrapolation is more accurate from larger pilot datasets; increasing the size of the pilot dataset 5 times re-
1We use relative residuals because the residuals themselves vary greatly from corpus to corpus, and we use RMS to penalise large extrapolation errors.",4 A performance extrapolation task,[0],[0]
"We admit that RMS relative residuals is probably not a close approximation to the extrapolation loss in real applications, and we hope future work will develop more realistic loss functions.
duces the RMS relative residuals by a factor of 10.",4 A performance extrapolation task,[0],[0]
"It also clearly shows that it valuable to perform hyperparameter optimisation on all subsets of the pilot dataset, not just on the whole pilot data.",4 A performance extrapolation task,[0],[0]
"Interestingly, Table 2 shows that the RMS difference between the two approaches to hyperparameter setting is greater when the pilot data is larger.",4 A performance extrapolation task,[0],[0]
"This makes sense; the hyperparameters that are optimal on a large pilot dataset may be far from optimal on a very small subset (this is clearly visible in Figure 1, where the items deviating most are those for the = 0.5 pilot data and hyperparameter choice).",4 A performance extrapolation task,[0],[0]
"This paper introduced an extrapolation methodology for predicting accuracy on large dataset from a small pilot dataset, applied it to a document classification system, and identified the biased powerlaw model with binomial weights as a good baseline extrapolation model.",5 Conclusions and Future Work,[0],[0]
This only scratches the surface of performance extrapolation tasks.,5 Conclusions and Future Work,[0],[0]
"We hope that teams with greater computational resources will study the extrapolation task for computationally more-demanding systems, including popular deep learning models.",5 Conclusions and Future Work,[0],[0]
"The power-law models should be considered baselines for more sophisticated extrapolation models, which might exploit more information than just accuracy on subsets of the pilot data.
",5 Conclusions and Future Work,[0],[0]
"We hope this work will spur the development of better methods for estimating the resources needed to build an NLP or ML system to meet a specification, as we believe this is essential for any mature engineering field.",5 Conclusions and Future Work,[0],[0]
We would like to thank the anonymous reviewers for their insightful comments and suggestions.,Acknowledgments,[0],[0]
"This research was supported by a Google award through the Natural Language Understanding Focused Program, and under the Australian Research Council’s Discovery Projects funding scheme (project number DP160102156).",Acknowledgments,[0],[0]
"Because obtaining training data is often the most difficult part of an NLP or ML project, we develop methods for predicting how much data is required to achieve a desired test accuracy by extrapolating results from systems trained on a small pilot training dataset.",abstractText,[0],[0]
"We model how accuracy varies as a function of training size on subsets of the pilot data, and use that model to predict how much training data would be required to achieve the desired accuracy.",abstractText,[0],[0]
We introduce a new performance extrapolation task to evaluate how well different extrapolations predict system accuracy on larger training sets.,abstractText,[0],[0]
We show that details of hyperparameter optimisation and the extrapolation models can have dramatic effects in a document classification task.,abstractText,[0],[0]
We believe this is an important first step in developing methods for estimating the resources required to meet specific engineering performance targets.,abstractText,[0],[0]
Predicting accuracy on large datasets from smaller pilot data,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 541–551 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1050",text,[0],[0]
"The influence of a speaker’s native language on learning and performance in a foreign language, also known as cross-linguistic transfer, has been studied for several decades in linguistics and psychology (Odlin, 1989; Martohardjono and Flynn, 1995; Jarvis and Pavlenko, 2008; Berkes and Flynn, 2012; Alonso, 2015).",1 Introduction,[0],[0]
"The growing availably of learner corpora has also sparked interest in cross-linguistic influence phenomena in NLP, where studies have explored the task of Native Language Identification (NLI) (Tetreault et al., 2013), as well as analysis of textual features in relation to the author’s native language (Jarvis and Crossley, 2012; Swanson and Charniak, 2013; Malmasi and Dras, 2014).",1 Introduction,[0],[0]
"Despite these advances,
1The experimental data collected in this study will be made publicly available.
",1 Introduction,[0],[0]
the extent and nature of first language influence in second language processing remains far from being established.,1 Introduction,[0],[0]
"Crucially, most prior work on this topic focused on production, while little is currently known about cross-linguistic influence in language comprehension.
",1 Introduction,[0],[0]
"In this work, we present a novel framework for studying cross-linguistic influence in language comprehension using eyetracking for reading and free-form native English text.",1 Introduction,[0],[0]
"We collect and analyze English newswire reading data from 182 participants, including 145 English as Second Language (ESL) learners from four different native language backgrounds: Chinese, Japanese, Portuguese and Spanish, as well as 37 native English speakers.",1 Introduction,[0],[0]
"Each participant reads 156 English sentences, half of which are shared across all participants, and the remaining half are individual to each participant.",1 Introduction,[0],[0]
"All the sentences are manually annotated with part-of-speech (POS) tags and syntactic dependency trees.
",1 Introduction,[0],[0]
"We then introduce the task of Native Language Identification from Reading (NLIR), which requires predicting a subject’s native language from gaze while reading text in a second language.",1 Introduction,[0],[0]
"Focusing on ESL participants and using a log-linear classifier with word fixation times normalized for reading speed as features, we obtain 71.03 NLIR accuracy in the shared sentences regime.",1 Introduction,[0],[0]
"We further demonstrate that NLIR can be generalized effectively to the individual sentences regime, in which each subject reads a different set of sentences, by grouping fixations according to linguistically motivated clustering criteria.",1 Introduction,[0],[0]
"In this regime, we obtain an NLIR accuracy of 51.03.
",1 Introduction,[0],[0]
"Further on, we provide classification and feature analyses, suggesting that the signal underlying NLIR is likely to be related to linguistic characteristics of the respective native languages.",1 Introduction,[0],[0]
"First, drawing on previous work on ESL production, we
541
observe that classifier uncertainty in NLIR correlates with global linguistic similarities across native languages.",1 Introduction,[0],[0]
"In other words, the more similar are the languages, the more similar are the reading patterns of their native speakers in English.",1 Introduction,[0],[0]
"Second, we perform feature analysis across native and non-native English speakers, and discuss structural and lexical factors that could potentially drive some of the non-native reading patterns in each of our native languages.",1 Introduction,[0],[0]
"Taken together, our results provide evidence for a systematic influence of native language properties on reading, and by extension, on online processing and comprehension in a second language.
",1 Introduction,[0],[0]
"To summarize, we introduce a novel framework for studying cross-linguistic influence in language learning by using eyetracking for reading free-form English text.",1 Introduction,[0],[0]
We demonstrate the utility of this framework in the following ways.,1 Introduction,[0],[0]
"First, we obtain the first NLIR results, addressing both the shared and the individual textual input scenarios.",1 Introduction,[0],[0]
"We further show that reading preserves linguistic similarities across native languages of ESL readers, and perform feature analysis, highlighting key distinctive reading patterns in each native language.",1 Introduction,[0],[0]
"The proposed framework complements and extends production studies, and can inform linguistic inquiry on cross-linguistic influence.
",1 Introduction,[0],[0]
This paper is structured as follows.,1 Introduction,[0],[0]
In section 2 we present the data and our experimental setup.,1 Introduction,[0],[0]
Section 3 describes our approach to NLIR and summarizes the classification results.,1 Introduction,[0],[0]
We analyze cross-linguistic influence in reading in section 4.,1 Introduction,[0],[0]
In section 4.1 we examine NLIR classification uncertainty in relation to linguistic similarities between native languages.,1 Introduction,[0],[0]
In section 4.2 we discuss several key fixation features associated with different native languages.,1 Introduction,[0],[0]
"Section 5 surveys related work, and section 6 concludes.",1 Introduction,[0],[0]
"Participants
We recruited 182 adult participants.",2 Experimental Setup,[0],[0]
"Of those, 37 are native English speakers and 145 are ESL learners from four native language backgrounds: Chinese, Japanese, Portuguese and Spanish.",2 Experimental Setup,[0],[0]
All the participants in the experiment are native speakers of only one language.,2 Experimental Setup,[0],[0]
"The ESL speakers were tested for English proficiency using the grammar and listening sections of the Michigan English test (MET), which consist of 50 multiple choice ques-
tions.",2 Experimental Setup,[0],[0]
The English proficiency score was calculated as the number of correctly answered questions on these modules.,2 Experimental Setup,[0],[0]
The majority of the participants scored in the intermediate-advanced proficiency range.,2 Experimental Setup,[0],[0]
Table 1 presents the number of participants and the mean English proficiency score for each native language group.,2 Experimental Setup,[0],[0]
"Additionally, we collected metadata on gender, age, level of education, duration of English studies and usage, time spent in English speaking countries and proficiency in any additional language spoken.
",2 Experimental Setup,[0],[0]
"Reading Materials
We utilize 14,274 randomly selected sentences from the Wall Street Journal part of the Penn Treebank (WSJ-PTB) (Marcus et al., 1993).",2 Experimental Setup,[0],[0]
"To support reading convenience and measurement precision, the maximal sentence length was set to 100 characters, leading to an average sentence length of 11.4 words.",2 Experimental Setup,[0],[0]
Word boundaries are defined as whitespaces.,2 Experimental Setup,[0],[0]
"From this sentence pool, 78 sentences (900 words) were presented to all participants (henceforth shared sentences) and the remaining 14,196 sentences were split into 182 individual batches of 78 sentences (henceforth individual sentences, averaging 880 words per batch).
",2 Experimental Setup,[0],[0]
"All the sentences include syntactic annotations from the Universal Dependency Treebank project (UDT) (McDonald et al., 2013).",2 Experimental Setup,[0],[0]
"The annotations include PTB POS tags (Santorini, 1990), Google universal POS tags (Petrov et al., 2012) and dependency trees.",2 Experimental Setup,[0],[0]
"The dependency annotations of the UDT are converted automatically from the manual phrase structure tree annotations of the WSJ-PTB.
",2 Experimental Setup,[0],[0]
"Gaze Data Collection
Each participant read 157 sentences.",2 Experimental Setup,[0],[0]
The first sentence was presented to familiarize participants with the experimental setup and was discarded during analysis.,2 Experimental Setup,[0],[0]
"The following 156 sentences consisted of 78 shared and 78 individual sen-
tences.",2 Experimental Setup,[0],[0]
The shared and the individual sentences were mixed randomly and presented to all participants in the same order.,2 Experimental Setup,[0],[0]
"The experiment was divided into three parts, consisting of 52 sentences each.",2 Experimental Setup,[0],[0]
"Participants were allowed to take a short break between experimental parts.
",2 Experimental Setup,[0],[0]
Each sentence was presented on a blank screen as a one-liner.,2 Experimental Setup,[0],[0]
"The text appeared in Times font, with font size 23.",2 Experimental Setup,[0],[0]
"To encourage attentive reading, upon completion of sentence reading participants answered a simple yes/no question about its content, and were subsequently informed if they answered the question correctly.",2 Experimental Setup,[0],[0]
"Both the sentences and the questions were triggered by a 300ms gaze on a fixation target (fixation circle for sentences and the letter “Q” for questions) which appeared on a blank screen and was co-located with the beginning of the text in the following screen.
",2 Experimental Setup,[0],[0]
"Throughout the experiment, participants held a joystick with buttons for indicating completion of sentence reading and answering the comprehension questions.",2 Experimental Setup,[0],[0]
"Eye-movement of participants’ dominant eye was recorded using a desktop mount Eyelink 1000 eyetracker, at a sampling rate of 1000Hz.",2 Experimental Setup,[0],[0]
Further details on the experimental setup are provided in appendix A.,2 Experimental Setup,[0],[0]
Our first goal is to determine whether the native language of ESL learners can be decoded from their gaze patterns while reading English text.,3 Native Language Identification from Reading,[0],[0]
"We address this question in two regimes, corresponding to our division of reading input into shared and individual sentences.",3 Native Language Identification from Reading,[0],[0]
"In the shared regime, all the participants read the same set of sentences.",3 Native Language Identification from Reading,[0],[0]
"Normalizing over the reading input, this regime facilitates focusing on differences in reading behavior across readers.",3 Native Language Identification from Reading,[0],[0]
"In the individual regime, we use the individual batches from our data to address the more challenging variant of the NLIR task in which the reading material given to each participant is different.",3 Native Language Identification from Reading,[0],[0]
"We seek to utilize features that can provide robust, simple and interpretable characterizations of reading patterns.",3.1 Features,[0],[0]
"To this end, we use speed normalized fixation duration measures over word sequences.
",3.1 Features,[0],[0]
"Fixation Measures We utilize three measures of word fixation duration:
• First Fixation duration (FF) Duration of the first fixation on a word.
",3.1 Features,[0],[0]
"• First Pass duration (FP) Time spent from first entering a word to first leaving it (including re-fixations within the word).
",3.1 Features,[0],[0]
• Total Fixation duration (TF),3.1 Features,[0],[0]
"The sum of all fixation times on a word.
",3.1 Features,[0],[0]
"We experiment with fixations over unigram, bigram and trigram sequences seqi,k = wi, ..., wi+k−1, k ∈ {1, 2, 3}, where for each metric M ∈ {FF, FP, TF} the fixation time for a sequence Mseqi,k is defined as the sum of fixations on individual tokens Mw in the sequence2.
Mseqi,k = ∑
w′∈seqi,k Mw′ (1)
",3.1 Features,[0],[0]
"Importantly, we control for variation in reading speeds across subjects by normalizing each subjects’s sequence fixation times.",3.1 Features,[0],[0]
"For each metric M and sequence seqi,k we normalize the sequence fixation time Mseqi,k relative to the subject’s sequence fixation times in the textual context of the sequence.",3.1 Features,[0],[0]
The context C is defined as the sentence in which the sequence appears for the Words in Fixed Context feature-set and the entire textual input for the Syntactic and Information clusters feature-sets (see definitions of feature-sets below).,3.1 Features,[0],[0]
"The normalization term SM,C,k is accordingly defined as the metric’s fixation time per sequence of length k in the context:
SM,C,k = 1 |C| ∑
seqk∈C Mseqk (2)
",3.1 Features,[0],[0]
"We then obtain a normalized fixation time Mnormseqi,k as:
Mnormseqi,k = Mseqi,k SM,C,k
(3)
2Note that for bigrams and trigrams, one could also measure FF and FP for interest regions spanning the sequence, instead, or in addition to summing these fixation times over individual tokens.
",3.1 Features,[0],[0]
Feature Types,3.1 Features,[0],[0]
"We use the above presented speed normalized fixation metrics to extract three feature-sets, Words in Fixed Context (WFC), Syntactic Clusters (SC) and Information Clusters (IC).",3.1 Features,[0],[0]
WFC is a token-level feature-set that presupposes a fixed textual input for all participants.,3.1 Features,[0],[0]
It is thus applicable only in the shared sentences regime.,3.1 Features,[0],[0]
SC and IC are typelevel features which provide abstractions over sequences of words.,3.1 Features,[0],[0]
"Crucially, they can also be applied when participants read different sentences.
",3.1 Features,[0],[0]
• Words in Fixed Context (WFC),3.1 Features,[0],[0]
The WFC features capture fixation times on word sequences in a specific sentence.,3.1 Features,[0],[0]
"This featureset consists of FF, FP and TF times for each of the 900 unigram, 822 bigram, and 744 trigram word sequences comprising the shared sentences.",3.1 Features,[0],[0]
The fixation times of each metric are normalized for each participant relative to their fixations on sequences of the same length in the surrounding sentence.,3.1 Features,[0],[0]
"As noted above, the WFC feature-set is not applicable in the individual regime, as it requires identical sentences for all participants.
",3.1 Features,[0],[0]
"• Syntactic Clusters (SC) CS features are average globally normalized FF, FP and TF times for word sequences clustered by our three types of syntactic labels: universal POS, PTB POS, and syntactic relation labels.",3.1 Features,[0],[0]
An example of such a feature is the average of speed-normalized TF times spent on the PTB POS bigram sequence DT NN.,3.1 Features,[0],[0]
We take into account labels that appear at least once in the reading input of all participants.,3.1 Features,[0],[0]
"On the four non-native languages, considering all three label types, we obtain 104 unigram, 636 bigram and 1,310 trigram SC features per fixation metric in the shared regime, and 56 unigram, 95 bigram and 43 trigram SC features per fixation metric in the individual regime.
",3.1 Features,[0],[0]
"• Information Clusters (IC) We also obtain average FF, FP and TF for words clustered according to their length, measured in number of characters.",3.1 Features,[0],[0]
"Word length was previously shown to be a strong predictor of information content (Piantadosi et al., 2011).",3.1 Features,[0],[0]
"As such, it provides an alternative abstraction to the syntactic clusters, combining both syntactic and lexical information.",3.1 Features,[0],[0]
"As with SC features, we take into account features that ap-
pear at least once in the textual input of all participants.",3.1 Features,[0],[0]
"For our set of non-native languages, we obtain for each fixation metric 15 unigram, 21 bigram and 23 trigram IC features in the shared regime, and 12 unigram, 18 bigram and 18 trigram IC features in the individual regime.",3.1 Features,[0],[0]
"Notably, this feature-set is very compact, and differently from the syntactic clusters, does not rely on the availability of external annotations.
",3.1 Features,[0],[0]
"In each feature-set, we perform a final preprocessing step for each individual feature, in which we derive a zero mean unit variance scaler from the training set feature values, and apply it to transform both the training and the test values of the feature to Z scores.",3.1 Features,[0],[0]
"The experiments are carried out using a log-linear model:
p(y|x; θ) = exp(θ · f(x, y))∑ y′∈Y exp(θ · f(x, y′))
(4)
where y is the reader’s native language, x is the reading input and θ are the model parameters.",3.2 Model,[0],[0]
"The classifier is trained with gradient descent using LBFGS (Byrd et al., 1995).",3.2 Model,[0],[0]
"In table 2 we report 10-fold cross-validation results on NLIR in the shared and the individual experimental regimes for native speakers of Chinese, Japanese, Portuguese and Spanish.",3.3 Experimental Results,[0],[0]
We introduce two baselines against which we compare the performance of our feature-sets.,3.3 Experimental Results,[0],[0]
The majority baseline selects the native language with the largest number of participants.,3.3 Experimental Results,[0],[0]
"The random clusters baseline clusters words into groups randomly, with the number of groups set to the number of syntactic categories in our data.
",3.3 Experimental Results,[0],[0]
"In the shared regime, WFC fixations yield the highest classification rates, substantially outperforming the cluster feature-sets and the two baselines.",3.3 Experimental Results,[0],[0]
"The strongest result using this featureset, 71.03, is obtained by combining unigram, bigram and trigram fixation times.",3.3 Experimental Results,[0],[0]
"In addition to this outcome, we note that training binary classifiers in this setup yields accuracies ranging from 68.49 for the language pair Portuguese and Spanish, to 93.15 for Spanish and Japanese.",3.3 Experimental Results,[0],[0]
"These results confirm the effectiveness of the shared input
regime for performing reliable NLIR, and suggest a strong native language signal in non-native reading fixation times.
SC features yield accuracies of 45.52 to 58.62 on the shared sentences, while IC features exhibit weaker performance in this regime, with accuracies of 41.38 to 46.21.",3.3 Experimental Results,[0],[0]
"Both results are well above chance, but lower than WFC fixations due to the information loss imposed by the clustering step.",3.3 Experimental Results,[0],[0]
"Crucially, both feature-sets remain effective in the individual input regime, with 43.45 to 48.97 accuracy for SC features and 32.41 to 38.62 accuracy for IC features.",3.3 Experimental Results,[0],[0]
"The strongest result in the individual regime is 51.03, obtained by concatenating IC and SC features over unigrams.",3.3 Experimental Results,[0],[0]
"We also note that using this setup in a binary classification scheme yields results ranging from chance level 49.31 for Portuguese versus Spanish, to 84.93 on Spanish versus Japanese.
",3.3 Experimental Results,[0],[0]
"Generally, we observe that adding bigram and trigram fixations in the shared regime leads to performance improvements compared to using unigram features only.",3.3 Experimental Results,[0],[0]
"This trend does not hold for the individual sentences, presumably due to a combination of feature sparsity and context variation in this regime.",3.3 Experimental Results,[0],[0]
"We also note that IC and SC features tend to perform better together than in separation, suggesting that the information encoded using these feature-sets is to some extent complementary.
",3.3 Experimental Results,[0],[0]
The generalization power of our cluster based feature-sets has both practical and theoretical consequences.,3.3 Experimental Results,[0],[0]
"Practically, they provide useful abstractions for performing NLIR over arbitrary textual input.",3.3 Experimental Results,[0],[0]
"That is, they enable performing this task using any textual input during both training and testing phases.",3.3 Experimental Results,[0],[0]
"Theoretically, the effectiveness of linguistically motivated features in discerning native languages suggests that linguistic factors
play an important role in the ESL reading process.",3.3 Experimental Results,[0],[0]
The analysis presented in the following sections will further explore this hypothesis.,3.3 Experimental Results,[0],[0]
"As mentioned in the previous section, the ability to perform NLIR in general, and the effectiveness of linguistically motivated features in particular, suggest that linguistic factors in the native and second languages are pertinent to ESL reading.",4 Analysis of Cross-Linguistic Influence in ESL Reading,[0],[0]
"In this section we explore this hypothesis further, by analyzing classifier uncertainty and the features learned in the NLIR task.",4 Analysis of Cross-Linguistic Influence in ESL Reading,[0],[0]
"Previous work in NLP suggested a link between textual patterns in ESL production and linguistic similarities of the respective native languages (Nagata and Whittaker, 2013; Nagata, 2014; Berzak et al., 2014, 2015).",4.1 Preservation of Linguistic Similarity,[0],[0]
"In particular, Berzak et al. (2014) has demonstrated that NLI classification uncertainty correlates with similarities between languages with respect to their typological features.",4.1 Preservation of Linguistic Similarity,[0],[0]
"Here, we extend this framework and examine if preservation of native language similarities in ESL production is paralleled in reading.
",4.1 Preservation of Linguistic Similarity,[0],[0]
Similarly to Berzak et al.,4.1 Preservation of Linguistic Similarity,[0],[0]
"(2014) we define the classification uncertainty for a pair of native languages y and y′ in our data collection D, as the average probability assigned by the NLIR classifier to one language given the other being the true native language.",4.1 Preservation of Linguistic Similarity,[0],[0]
This approach provides a robust measure of classification confusion that does not rely on the actual performance of the classifier.,4.1 Preservation of Linguistic Similarity,[0],[0]
"We interpret the classifier uncertainty as a similarity measure between the respective languages and de-
note it as English Reading Similarity ERS.
ERSy,y′",4.1 Preservation of Linguistic Similarity,[0],[0]
"=
∑ (x,y)∈Dy p(y′|x;θ)+ ∑ (x,y′)∈Dy′ p(y|x;θ)
|Dy |+|Dy′ | (5)
We compare these reading similarities to the linguistic similarities between our native languages.",4.1 Preservation of Linguistic Similarity,[0],[0]
"To approximate these similarities, we utilize feature vectors from the URIEL Typological Compendium (Littel et al., 2016) extracted using the lang2vec tool (Littell et al., 2017).",4.1 Preservation of Linguistic Similarity,[0],[0]
"URIEL aggregates, fuses and normalizes typological, phylogenetic and geographical information about the world’s languages.
",4.1 Preservation of Linguistic Similarity,[0],[0]
"We obtain all the 103 available morphosyntactic features in URIEL, which are derived from the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013), Syntactic Structures of the World’s Languages (SSWL) (Collins and Kayne, 2009) and Ethnologue (Lewis et al., 2015).",4.1 Preservation of Linguistic Similarity,[0],[0]
Missing feature values are completed with a KNN classifier.,4.1 Preservation of Linguistic Similarity,[0],[0]
"We also extract URIEL’s 3,718 language family features derived from Glottolog (Hammarström et al., 2015).",4.1 Preservation of Linguistic Similarity,[0],[0]
Each of these features represents membership in a branch of Glottolog’s world language tree.,4.1 Preservation of Linguistic Similarity,[0],[0]
"Truncating features with the same value for all our languages, we remain with 76 features, consisting of 49 syntactic features and 27 family tree features.",4.1 Preservation of Linguistic Similarity,[0],[0]
"The linguistic similarity LS between a pair of languages y and y′ is then determined by the cosine similarity of their URIEL feature vectors.
",4.1 Preservation of Linguistic Similarity,[0],[0]
"LSy,y′ =",4.1 Preservation of Linguistic Similarity,[0],[0]
"vy · vy′ ‖vy‖‖vy′‖
(6)
",4.1 Preservation of Linguistic Similarity,[0],[0]
Figure 1 presents the URIEL based linguistic similarities for our set of non-native languages against the average NLIR classification uncertainties on the cross-validation test samples.,4.1 Preservation of Linguistic Similarity,[0],[0]
The results presented in this figure are based on the unigram IC+SC feature-set in the individual sentences regime.,4.1 Preservation of Linguistic Similarity,[0],[0]
"We also provide a graphical illustration of the language similarities for each measure, using the Ward clustering algorithm (Ward Jr, 1963).",4.1 Preservation of Linguistic Similarity,[0],[0]
We observe a correlation between the two measures which is also reflected in similar hierarchies in the two language trees.,4.1 Preservation of Linguistic Similarity,[0],[0]
"Thus, linguistically motived features in English reveal linguistic similarities across native languages.",4.1 Preservation of Linguistic Similarity,[0],[0]
"This outcome supports the hypothesis that English
reading differences across native languages are related to linguistic factors.
",4.1 Preservation of Linguistic Similarity,[0],[0]
"We note that while comparable results are obtained for the IC and SC feature-sets, together and in separation in the shared regime, WFC features in the shared regime do not exhibit a clear uncertainty distinction when comparing across the pairs Japanese and Spanish, Japanese and Portuguese, Chinese and Spanish, and Chinese and Portuguese.",4.1 Preservation of Linguistic Similarity,[0],[0]
"Instead, this feature-set yields very low uncertainty, and correspondingly very high performance ranging from 90.41 to 93.15, for all four language pairs.",4.1 Preservation of Linguistic Similarity,[0],[0]
"Our framework enables not only native language classification, but also exploratory analysis of native language specific reading patterns in English.",4.2 Feature Analysis,[0],[0]
The basic question that we examine in this respect is on which features do readers of different native language groups spend more versus less time.,4.2 Feature Analysis,[0],[0]
We also discuss several potential relations of the observed reading time differences to usage patterns and grammatical errors committed by speakers of our four native languages in production.,4.2 Feature Analysis,[0],[0]
"We obtain this information by extracting grammatical error counts from the CLC FCE corpus (Yannakoudakis et al., 2011), and from the ngram frequency analysis in Nagata and Whittaker (2013).
",4.2 Feature Analysis,[0],[0]
"In order to obtain a common benchmark for reading time comparisons across non-native speakers, in this analysis we also consider our group of native English speakers.",4.2 Feature Analysis,[0],[0]
"In this context, we train four binary classifiers that discern each of the non-native groups from native English speakers based on TF times over unigram PTB POS tags in the shared regime.",4.2 Feature Analysis,[0],[0]
The features with the strongest positive and negative weights learned by these classifiers are presented in table 3.,4.2 Feature Analysis,[0],[0]
"These features serve as a reference point for selecting the case studies discussed below.
",4.2 Feature Analysis,[0],[0]
"Interestingly, some of the reading features that are most predictive of each native language lend themselves to linguistic interpretation with respect to structural factors.",4.2 Feature Analysis,[0],[0]
"For example, in Japanese and Chinese we observe shorter reading times for determiners (DT), which do not exist in these languages.",4.2 Feature Analysis,[0],[0]
"Figure 2a presents the mean TF times for determiners in all five native languages, suggesting that native speakers of Portuguese and Spanish, which do have determiners, do not exhibit reduced reading times on this structure compared to natives.",4.2 Feature Analysis,[0],[0]
"In ESL production, missing determiner errors are the most frequent error for native speakers of Japanese and third most common error for native speakers of Chinese.
",4.2 Feature Analysis,[0],[0]
"In figure 2b we present the mean TF reading times for pronouns (PRP), where we also see shorter reading times by natives of Japanese and Chinese as compared to English natives.",4.2 Feature Analysis,[0],[0]
In both languages pronouns can be omitted both in object and subject positions.,4.2 Feature Analysis,[0],[0]
"Portuguese and Spanish, in which pronoun omission is restricted to the subject position present similar albeit weaker tendency.
",4.2 Feature Analysis,[0],[0]
"In figure 2c we further observe that differently from natives of Chinese and Japanese, native speakers of Portuguese and Spanish spend more time on NN+POS in head final possessives such as “the public’s confidence”.",4.2 Feature Analysis,[0],[0]
"While similar constructions exist in Chinese and Japanese, the NN+POS combination is expressed in Portuguese and Spanish as a head initial NN of NN.",4.2 Feature Analysis,[0],[0]
"This form exists in English (e.g. “the confidence of the public”) and is preferred by speakers of these languages in ESL writing (Nagata and Whittaker, 2013).",4.2 Feature Analysis,[0],[0]
"As an additional baseline for this construction, we provide the TF times for NN in figure 2d.",4.2 Feature Analysis,[0],[0]
"There, relative to English natives, we observe longer reading times for Japanese and Chinese and comparable times for Portuguese and Spanish.
",4.2 Feature Analysis,[0],[0]
"The reading times of NN in figure 2d also give
rise to a second, potentially competing interpretation of differences in ESL reading times, which highlights lexical rather than structural factors.",4.2 Feature Analysis,[0],[0]
"According to this interpretation, increased reading times of nouns are the result of substantially smaller lexical sharing with English by Chinese and Japanese as compared to Spanish and Portuguese.",4.2 Feature Analysis,[0],[0]
"Given the utilized speed normalization, lexical effects on nouns could in principle account for reduced reading times on determiners and pronouns.",4.2 Feature Analysis,[0],[0]
"Conversely, structural influence leading to reduced reading times on determiners and pronouns could explain longer dwelling on nouns.",4.2 Feature Analysis,[0],[0]
A third possibility consistent with the observed reading patterns would allow for both structural and lexical effects to impact second language reading.,4.2 Feature Analysis,[0],[0]
"Importantly, in each of these scenarios, ESL reading patterns are related to linguistic factors of the reader’s native language.
",4.2 Feature Analysis,[0],[0]
"We note that the presented analysis is preliminary in nature, and warrants further study in future research.",4.2 Feature Analysis,[0],[0]
"In particular, reading times and classifier learned features may in some cases differ between the shared and the individual regimes.",4.2 Feature Analysis,[0],[0]
"In the examples presented above, similar results are obtained in the individual sentences regime for DT, PRP and NN.",4.2 Feature Analysis,[0],[0]
"The trend for the NN+POS construction, however, diminishes in that setup with similar reading times for all languages.",4.2 Feature Analysis,[0],[0]
"On the other hand, one of the strongest features for predicting Portuguese and Spanish in the individual regime are longer reading times for prepositions (IN), an outcome that holds in the shared regime only relative to Chinese and Japanese, but not relative to native speakers of English.
",4.2 Feature Analysis,[0],[0]
"Despite these caveats, our results suggest that reading patterns can potentially be related to linguistic factors of the reader’s native language.",4.2 Feature Analysis,[0],[0]
"This analysis can be extended in various ways, such as inclusion of additional feature types and fixation metrics, as well as utilization of other comparative methodologies.",4.2 Feature Analysis,[0],[0]
"Combined with evidence from language production, this line of investigation can be instrumental for informing linguistic theory of cross-linguistic influence.",4.2 Feature Analysis,[0],[0]
"Eyetracking and second language reading Second language reading has been studied using eyetracking, with much of the work focusing on processing of syntactic ambiguities and analysis
of specific target word classes such as cognates (Dussias, 2010; Roberts and Siyanova-Chanturia, 2013).",5 Related Work,[0],[0]
"In contrast to our work, such studies typically use controlled, rather than free-form sentences.",5 Related Work,[0],[0]
Investigation of global metrics in freeform second language reading was introduced only recently by Cop et al. (2015).,5 Related Work,[0],[0]
"This study compared ESL and native reading of a novel by native speakers of Dutch, observing longer sentence reading times, more fixations and shorter saccades in ESL reading.",5 Related Work,[0],[0]
"Differently from this study, our work focuses on comparison of reading patterns between different native languages.",5 Related Work,[0],[0]
"We also analyze a related, but different metric, namely speed normalized fixation durations on word sequences.
",5 Related Work,[0],[0]
Eyetracking for NLP tasks Recent work in NLP has demonstrated that reading gaze can serve as a valuable supervision signal for standard NLP tasks.,5 Related Work,[0],[0]
"Prominent examples of such work include POS tagging (Barrett and Søgaard, 2015a; Barrett et al., 2016), syntactic parsing (Barrett and Søgaard, 2015b) and sentence compression (Klerke et al., 2016).",5 Related Work,[0],[0]
"Our work also tackles a traditional NLP task with free-form text, but differs from this line of research in that it addresses this task only in comprehension.",5 Related Work,[0],[0]
"Furthermore, while these studies use gaze recordings of native readers, our work focuses on non-native readers.
",5 Related Work,[0],[0]
"NLI in production NLI was first introduced in Koppel et al. (2005) and has been drawing considerable attention in NLP, including a recent shared-task challenge with 29 participating teams (Tetreault et al., 2013).",5 Related Work,[0],[0]
"NLI has also been driving much of the work on identification of native language related features in writing (Tsur and Rappoport, 2007; Jarvis and Crossley, 2012; Brooke and Hirst, 2012; Tetreault et al., 2012; Swanson and Charniak, 2013, 2014; Malmasi and Dras, 2014; Bykh and Meurers, 2016).",5 Related Work,[0],[0]
"Several studies have also linked usage patterns and grammatical errors in production to linguistic properties of the writer’s native language (Nagata and Whittaker, 2013; Nagata, 2014; Berzak et al., 2014, 2015).",5 Related Work,[0],[0]
Our work departs from NLI in writing and introduces NLI and related feature analysis in reading.,5 Related Work,[0],[0]
"We present a novel framework for studying crosslinguistic influence in multilingualism by measuring gaze fixations during reading of free-form En-
glish text.",6 Conclusion and Outlook,[0],[0]
We demonstrate for the first time that this signal can be used to determine a reader’s native language.,6 Conclusion and Outlook,[0],[0]
The effectiveness of linguistically motivated criteria for fixation clustering and our subsequent analysis suggest that the ESL reading process is affected by linguistic factors.,6 Conclusion and Outlook,[0],[0]
"Specifically, we show that linguistic similarities between native languages are reflected in similarities in ESL reading.",6 Conclusion and Outlook,[0],[0]
"We also identify several key features that characterize reading in different native languages, and discuss their potential connection to structural and lexical properties of the native langauge.",6 Conclusion and Outlook,[0],[0]
"The presented results demonstrate that eyetracking data can be instrumental for developing predictive and explanatory models of second language reading.
",6 Conclusion and Outlook,[0],[0]
"While this work is focused on NLIR from fixations, our general framework can be used to address additional aspects of reading, such as analysis of saccades and gaze trajectories.",6 Conclusion and Outlook,[0],[0]
"In future work, we also plan to explore the role of native and second language writing system characteristics in second language reading.",6 Conclusion and Outlook,[0],[0]
"More broadly, our methodology introduces parallels with production studies in NLP, creating new opportunities for integration of data, methodologies and tasks between production and comprehension.",6 Conclusion and Outlook,[0],[0]
"Furthermore, it holds promise for formulating language learning theory that is supported by empirical findings in naturalistic setups across language processing domains.",6 Conclusion and Outlook,[0],[0]
"We thank Amelia Smith, Emily Weng, Run Chen and Lila Jansen for contributions to stimuli preparation and data collection.",Acknowledgements,[0],[0]
"We also thank Andrei Barbu, Guy Ben-Yosef, Yen-Ling Kuo, Roger Levy, Jonathan Malmaud, Karthik Narasimhan and the anonymous reviewers for valuable feedback on this work.",Acknowledgements,[0],[0]
"This material is based upon work supported by the Center for Brains, Minds, and Machines (CBMM), funded by NSF STC award CCF-1231216.",Acknowledgements,[0],[0]
"Eyetracking Setup We use a 44.5x30cm screen with 1024x768px resolution to present the reading materials, and a desktop mount Eyelink 1000 eyetracker (1000Hz) to record gaze.",A Supplemental Material,[0],[0]
"The screen, eyetracker camera and chinrest are horizontally aligned on a table surface.",A Supplemental Material,[0],[0]
"The screen center (x=512, y=384) is 79cm away from the center of
the forehead bar, and 13cm below it.",A Supplemental Material,[0],[0]
The eyetracker camera knob is 65cm away from forehead bar.,A Supplemental Material,[0],[0]
"Throughout the experiment participants hold a joystick with a button for indicating sentence completion, and two buttons for answering yes/no questions.",A Supplemental Material,[0],[0]
"We record gaze of the participant’s dominant eye.
",A Supplemental Material,[0],[0]
"Text Parameters All the textual material in the experiment is presented using Times font, normal style, with font size 23.",A Supplemental Material,[0],[0]
"In our setup, this corresponds to 0.36 degrees (11.3px) average lower case letter width, and 0.49 degrees (15.7px) average upper case letter width.",A Supplemental Material,[0],[0]
"We chose a nonmonospace font, as such fonts are generally more common in reading.",A Supplemental Material,[0],[0]
"They are also more compact compared to monospace fonts, allowing to substantially increase the upper limit for sentence length.
",A Supplemental Material,[0],[0]
"Calibration We use 3H line calibration with point repetition on the central horizontal line (y=384), using 16px outer circle, 6px inner circle, fixation points.",A Supplemental Material,[0],[0]
"At least three calibrations are performed during the experiment, one at the beginning of each experimental section.",A Supplemental Material,[0],[0]
We also recalibrate upon failure to produce a 300ms fixation on any fixation trigger preceding a sentence or a question within 4 seconds after its appearance.,A Supplemental Material,[0],[0]
The mean validation error for calibrations across subjects is 0.146 degrees (std 0.038).,A Supplemental Material,[0],[0]
A fundamental question in language learning concerns the role of a speaker’s first language in second language acquisition.,abstractText,[0],[0]
We present a novel methodology for studying this question: analysis of eye-movement patterns in second language reading of free-form text.,abstractText,[0],[0]
"Using this methodology, we demonstrate for the first time that the native language of English learners can be predicted from their gaze fixations when reading English.",abstractText,[0],[0]
"We provide analysis of classifier uncertainty and learned features, which indicates that differences in English reading are likely to be rooted in linguistic divergences across native languages.",abstractText,[0],[0]
The presented framework complements production studies and offers new ground for advancing research on multilingualism.1,abstractText,[0],[0]
Predicting Native Language from Gaze,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 659–664 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
659",text,[0],[0]
"The data generated from online news consumption constitutes a rich resource, which allows us to explore the relation between news content and user opinions and behaviors.",1 Introduction,[0],[0]
"In order to stay in business, newspapers need to pay attention to this information.",1 Introduction,[0],[0]
"For example, what headlines do users click on, and why?",1 Introduction,[0],[0]
"With the volume of news being consumed online today, there is great interest in addressing this problem algorithmically.",1 Introduction,[0],[0]
"We collaborate with a large Danish newspaper, who gave us access to several years’ worth of headlines, and the number of clicks generated by readers.
",1 Introduction,[0],[0]
"We aggregate the viewing logs to classify headlines as popular or unpopular, and build models to predict those classifications.",1 Introduction,[0],[0]
We use an expanded version of the dataset investigated by Hardt and Rambow (2017).,1 Introduction,[0],[0]
"That work found that bag-ofword models based on headlines did indeed have predictive value concerning viewing behavior, although models based on the article body were more accurate.",1 Introduction,[0],[0]
"As Hardt and Rambow noted, this is somewhat paradoxical: how can a model based
on the article text be better at predicting clicks?",1 Introduction,[0],[0]
"After all, the choice to click on an article must be based on the headline alone – the article is only seen after the clicking decision is made.",1 Introduction,[0],[0]
"Hardt and Rambow speculate that “it is possible that the headline on its own gives readers a lot of semantic information which we are not capturing with our features, but which the whole article does provide.",1 Introduction,[0],[0]
So human readers can “imagine” the article before they read it and implicitly base their behavior on their expectation.”,1 Introduction,[0],[0]
"(Hardt and Rambow, 2017)
",1 Introduction,[0],[0]
"In other words, readers are able to anticipate the contents of an article in advance from a headline, because of the linguistic and world knowledge that they bring to bear when assessing the headline.",1 Introduction,[0],[0]
"If we can incorporate this “future” knowledge into a prediction model, we are likely to improve performance.
",1 Introduction,[0],[0]
"We test this hypothesis by defining ways to model aspects of the lexical, structural, and topical knowledge of human news readers:
• Lexical – Word Embeddings: we provide our models with pretrained word embeddings from large datasets.",1 Introduction,[0],[0]
"This models aspects of the rich lexical information and association that human readers bring to bear in reading a headline.
",1 Introduction,[0],[0]
"• Structural – POS Tagging: part of speech information is a basic component of structural linguistic knowledge, reflected in the structure of common headline templates such as “Can X do Y?” or “You will not believe what happened when X”.
",1 Introduction,[0],[0]
"• Topical – Section Prediction: Each article is labeled with a section (sports, politics, etc).",1 Introduction,[0],[0]
We include a task which predicts the section of a headline.,1 Introduction,[0],[0]
"This models the ability of a news reader to understand the most salient
and interesting topical material in a headline text.
",1 Introduction,[0],[0]
"We use a multi-task learning (MTL) setup (Caruana, 1993), which provides a natural framework to test the above hypotheses: one of the first uses of MTL was to include the outcome of future diagnostic tests into a prediction task (Caruana et al., 1996).
",1 Introduction,[0],[0]
"We explore the effect of pretrained word embeddings, and the effects of auxiliary tasks involving POS tagging and section prediction.",1 Introduction,[0],[0]
"We find that the combination of all of these factors results in substantial improvements over the baseline and the previous work, which used a single-task system.",1 Introduction,[0],[0]
"We also build logistic regression models, both for word and character n-grams.",1 Introduction,[0],[0]
"The wordbased models have the advantage that the predictiveness of individual words can be examined.
",1 Introduction,[0],[0]
"While the word n-gram models have performance comparable to the baseline neural net, the character n-gram model has higher performance, competing with the best MTL result.",1 Introduction,[0],[0]
"This finding is in line with the results from Zhang et al. (2015).
",1 Introduction,[0],[0]
Our results indicate that MTL can indeed provide the tools to implement prediction processes that involve expectations about the future.,1 Introduction,[0],[0]
"Given the successful integration of two auxiliary tasks, we see this as a promising starting point for future research.",1 Introduction,[0],[0]
"However, the performance parity with the character model underscores the fact that simple model architectures still have a place.",1 Introduction,[0],[0]
"Our findings, in line with other current work (Benton et al., 2017), shine light on the question of auxiliary task selection and their interaction, and highlight that MTL results should be rigorously tested.
",1 Introduction,[0],[0]
"A good predictive model is a powerful diagnostic tool for editors, allowing them to select proposed headlines.",1 Introduction,[0],[0]
"However, journalism is a creative production process, so detection is only part of the application.",1 Introduction,[0],[0]
We also want to be able to give strategic advice to headline writers.,1 Introduction,[0],[0]
"To this end, we report an analysis of common n-gram features in the word-based logistic regression model, that provide some insights into successful headline patterns.
",1 Introduction,[0],[0]
Contributions We explore an MTL architecture with two auxiliary tasks for headline popularity prediction.,1 Introduction,[0],[0]
"We show how aspects of lexical, structural, and topical knowledge are all relevant for headline popularity.",1 Introduction,[0],[0]
The positive results reported here provide a fruitful basis for further development of MTL models for news data.,1 Introduction,[0],[0]
"We also ana-
lyze lexical features that are predictive of headline popularity.",1 Introduction,[0],[0]
News Data,2 Data,[0],[0]
The present work is based on a significantly expanded and cleaned version of the dataset used by Hardt and Rambow (2017).,2 Data,[0],[0]
This dataset includes Jyllands-Posten articles and logs.,2 Data,[0],[0]
Jyllands-Posten is a major Danish newspaper (and became known to an international audience over the cartoon controversy).,2 Data,[0],[0]
The data covers a period from July 2015 through July 2017.,2 Data,[0],[0]
"We removed any articles from before July 2015, when the viewing logs began, since these older articles have unreliable numbers of clicks.",2 Data,[0],[0]
"The resulting dataset consists of 82,532 articles and a total of 281,005,390 user views.",2 Data,[0],[0]
"We furthermore extracted the news section each article belongs to (sports, politics, etc.) from the URL.
",2 Data,[0],[0]
"We bin the articles by numbers of clicks into 2 bins, thus defining a classification task: is the article in the top 50% of clicks or not?",2 Data,[0],[0]
"The data is divided into 80% training, and 10% each development and test data.
",2 Data,[0],[0]
"Figure 1 shows the top headline on the JyllandsPosten web site for August 27, 2018.",2 Data,[0],[0]
"Our data does not include information such as the position of a headline on the page, and possible associated graphical material.
",2 Data,[0],[0]
Additional Data,2 Data,[0],[0]
"In addition to the news data from JP, we obtained a corpus of 100 million words of Danish text from the Society for Danish Language and Literature, or DSL (Jørg Asmussen, 2018).",2 Data,[0],[0]
"This corpus was collected from diverse
sources over a period from 1990 to 2010.",2 Data,[0],[0]
"The corpus has been automatically annotated for part of speech and lemmatization, and we use this for our POS tagging task.",2 Data,[0],[0]
"We also downloaded the Danish Wikipedia, which consists of approximately 49 million words of Danish text.",2 Data,[0],[0]
"We use these corpora in conjunction with the JP article texts to induce pre-trained Danish word-embeddings.
",2 Data,[0],[0]
Data Statement A. CURATION,2 Data,[0],[0]
RATIONALE,2 Data,[0],[0]
"The dataset is collected by Jyllands-Posten as part of a general strategy to understand user behavior and preferences with respect to the news content on the site.
",2 Data,[0],[0]
B. LANGUAGE VARIETY,2 Data,[0],[0]
"The data is Danish (da-DK).
",2 Data,[0],[0]
"C. SPEAKER DEMOGRAPHIC The text is produced by professional journalists.
",2 Data,[0],[0]
"D. ANNOTATOR DEMOGRAPHIC There is no manual annotation of the text.
",2 Data,[0],[0]
E. SPEECH SITUATION,2 Data,[0],[0]
"The texts were produced from July 2015 until July 2017; the intended audience is Danish news consumers.
",2 Data,[0],[0]
F. TEXT CHARACTERISTICS,2 Data,[0],[0]
"The text is standard, mainstream Danish journalism.",2 Data,[0],[0]
"Our task is to predict which articles get the most user clicks, based on the headline alone.",3 Models,[0],[0]
"We report results using logistic regression and a neural network, using MTL.
Logistic Regression We define the following features for logistic regression models:
1. n",3 Models,[0],[0]
"-chars: sequences of n characters, with n ranging from 2 to 6 in all experiments.
",3 Models,[0],[0]
"2. word unigrams: tfidf scores for all word unigrams
3.",3 Models,[0],[0]
"word bigrams: tfidf scores for all word bigrams
GRU Neural Network While the task is classification, which could be done with a feed-forward model, we want a sequential architecture, so that we can incorporate POS tagging as an auxiliary task, adding POS output at each time step.
",3 Models,[0],[0]
"Based on good results in recent work (Lee and Dernoncourt, 2016), (Liu et al., 2016), we choose a Recurrent Neural Network architecture and after
a series of experiments on the training and validation set, we obtained the best results using GRU (Gated Recurrent Unit) units.
",3 Models,[0],[0]
"Each layer k consists of two sets of units, labeled fw and bw that process the sequence forwards and backwards respectively, so that information from the whole sequence is available on every timestep t.",3 Models,[0],[0]
The two directions’ activations are concatenated and fed to a fully-connected softmax (for multi-class classification) or sigmoid layer (for binary classification) to get the output probability ykt of the task associated with layer k.,3 Models,[0],[0]
"So that higher level tasks can benefit, we embed the output probabilities using the fully connected label embedding LE layer, a technique used on similar scenarios (Rønning et al., 2018).",3 Models,[0],[0]
"The embedded label gets concatenated with the GRU output to get the activation akt that gets fed in the next layer, or the final fully connected prediction layer, as presented in figure 2.
",3 Models,[0],[0]
"In the sequential auxiliary task, i.e. POS tagging, this is done for every timestep, while for the classification tasks the prediction is made on the final timestep.
",3 Models,[0],[0]
"For regularization, we apply dropout on every layer of our network.
",3 Models,[0],[0]
"Auxiliary Tasks In our setup, we use two auxiliary tasks:
1.",3 Models,[0],[0]
"POS tagging: we include POS tagging using the DSL dataset on the first recurrent layer of the GRU.
2.",3 Models,[0],[0]
"Section prediction: we include classification into one of the 227 sections of the Jyllands-
Posten website.",3 Models,[0],[0]
"The output for this task is based on the penultimate recurrent layer.
",3 Models,[0],[0]
"Hyper-parameters and Training We perform a grid search to find the best hyper-parameters for a single-task model (i.e., without any auxiliary tasks) and then keep those settings for all our experiments.",3 Models,[0],[0]
"We settle on a model with hidden size H = 112 and Nk = 3 layers, respectively.",3 Models,[0],[0]
"The dropout probability p = 0.3 gave best results for both models.
",3 Models,[0],[0]
"We train the model for 10 epochs using Adam optimizer with the default parameters, clipping the gradient updates so that their norm is not higher than 5.",3 Models,[0],[0]
"We train the different tasks sequentially for each epoch, with the lower level (POS tagging) first and the popularity prediction last.",3 Models,[0],[0]
"Additionally, we decay the learning rate by a factor of 0.9 after each epoch.",3 Models,[0],[0]
"While this is not common with adaptive methods such as Adam, it performed better.",3 Models,[0],[0]
We stop training if the accuracy on the development set stops improving.,3 Models,[0],[0]
Tables 1 and 2 report accuracy for logistic regression and neural classifiers.,4 Results,[0],[0]
"We also give the best score from Hardt and Rambow (2017) for comparison purposes (note, though, that the data sets are not identical and can therefore not be directly compared).",4 Results,[0],[0]
We observe a substantial improvement over the baseline GRU when incorporating the pre-trained embeddings and both auxiliary tasks.,4 Results,[0],[0]
"It seems that pretrained embeddings and MTL act at least partly as regularizers, as these models trained for more epochs without overfitting.",4 Results,[0],[0]
"Interestingly, we observe a similar improvement over the word-based logistic regression models with a character n-gram model.",4 Results,[0],[0]
"Our main focus in this paper is on MTL as a framework to explore the lexical, structural and topical knowledge involved in users’ selection of headlines.",5 Analysis and Discussion,[0],[0]
"However, recognizing a popular headline and giving advice on how to write one are not the same: we want to provide editors and journalists with insights as to what constructions are likely to attract more eyeballs.
",5 Analysis and Discussion,[0],[0]
One way to explore this is to examine individual words and their contribution to predictiveness.,5 Analysis and Discussion,[0],[0]
"Table 3 displays the top 20 unigrams based on their
coefficients in the logistic regression model.",5 Analysis and Discussion,[0],[0]
For each unigram we provide a translation (if needed) and a comment.,5 Analysis and Discussion,[0],[0]
We classify several unigrams as Deictic-reference.,5 Analysis and Discussion,[0],[0]
"This follows Blom and Hansen (2015), who suggest that headline ”clickbait” often relies on forward-looking expressions, such as ”This”, as in, e.g., ”This is how you should eat an avocado”.",5 Analysis and Discussion,[0],[0]
"Here, ”this” is a referring expression, but the reader understands that the antecedent will be found in the article body.",5 Analysis and Discussion,[0],[0]
Several of these top unigrams are names that are of specific topical interest in areas such as sports and politics.,5 Analysis and Discussion,[0],[0]
"Others mention topics of more general interest (Researchers, dead, found).",5 Analysis and Discussion,[0],[0]
"The second person pronoun is also on the list – in general, it was found that second person pronouns are far more predictive of popularity than first or third person pronouns.",5 Analysis and Discussion,[0],[0]
"Finally, several unigrams identify sections of the newspaper of particular interest (car, weather, analysis, and satire).",5 Analysis and Discussion,[0],[0]
"Prediction of news headline popularity is an increasingly important problem, as news consumption has moved online.",6 Related Work,[0],[0]
"The insights and models described here might well be applicable to related problems of interest: for example, Balakrishnan and Parekh (2014) and Jaidka et al. (2018) study the problem of predicting clicks on email subject lines.
",6 Related Work,[0],[0]
Subramanian et al. (2018) show that a regression-based multitask approach can increase performance for the classification prediction of popularity.,6 Related Work,[0],[0]
"Their work looks at the popularity of online petitions, but the methodology applies to our subject as well, and ties in with the approaches taken in this project.
",6 Related Work,[0],[0]
"Benton et al. (2017) caution that in order to evaluate MTL results properly, we need to take the number of parameters into account.",6 Related Work,[0],[0]
"Our results to some extent support this finding, by showing that a simpler linear model can fare equally well on the task.
",6 Related Work,[0],[0]
"The choice of auxiliary tasks greatly influences the performance of MTL architectures, prompting several recent investigations into the selection process (Alonso and Plank, 2017; Bingel and Søgaard, 2017).",6 Related Work,[0],[0]
"However, it is still unclear whether these tasks serve as mere regularizers, or whether they can also impart some additional information.",6 Related Work,[0],[0]
We presented an exploratory approach to predicting newspaper article popularity from headlines alone.,7 Conclusion,[0],[0]
"Using pre-trained embeddings and a MTL setup, we are able to incorporate rich structural and semantic knowledge into the task and substantially improve performance.",7 Conclusion,[0],[0]
"While the results are encouraging and allow the exploration of further auxiliary tasks (for example article word prediction), we find that a simple character-based n-
gram model performs competitively.",7 Conclusion,[0],[0]
"These findings highlight two aspects: 1) For any application of MTL, this is a strong case for comparing the results to non-deep models.",7 Conclusion,[0],[0]
"While it is comparatively easy to show an improvement over the basic STL model, there might be other simple models that are competitive.",7 Conclusion,[0],[0]
2),7 Conclusion,[0],[0]
"The selection of auxiliary tasks greatly influences the performance, even beyond simple regularization, and in a non-linear way.",7 Conclusion,[0],[0]
"It does, however, provide us with a tool to test human intuitions about task interactions and the importance of certain problem aspects.",7 Conclusion,[0],[0]
Thanks to A. Michele Colombo for help with the data and experiments.,Acknowledgments,[0],[0]
"We also thank JyllandsPosten for giving us access to the data, and to DSL for data for embeddings and POS annotations.",Acknowledgments,[0],[0]
"Newspapers need to attract readers with headlines, anticipating their readers’ preferences.",abstractText,[0],[0]
"These preferences rely on topical, structural, and lexical factors.",abstractText,[0],[0]
We model each of these factors in a multi-task GRU network to predict headline popularity.,abstractText,[0],[0]
"We find that pre-trained word embeddings provide significant improvements over untrained embeddings, as do the combination of two auxiliary tasks, newssection prediction and part-of-speech tagging.",abstractText,[0],[0]
"However, we also find that performance is very similar to that of a simple Logistic Regression model over character n-grams.",abstractText,[0],[0]
"Feature analysis reveals structural patterns of headline popularity, including the use of forward-looking deictic expressions and second person pronouns.",abstractText,[0],[0]
Predicting News Headline Popularity with Syntactic and Semantic Knowledge Using Multi-Task Learning,title,[0],[0]
"Proceedings of the SIGDIAL 2018 Conference, pages 130–139, Melbourne, Australia, 12-14 July 2018. c©2018 Association for Computational Linguistics
130",text,[0],[0]
"Co-located, face-to-face spoken dialogue is the primary and basic setting where humans learn their first language (Fillmore, 1981) partly because dialogue participants (i.e., caregiver and child) can denote objects in their shared environment which is an important developmental step in child language acquisition (McCune, 2008).",1 Introduction,[0],[0]
"This setting motivates human-robot interaction tasks where robots acquire semantic meanings of words, and where part of the semantic representation of those words is grounded (Harnad, 1990) somehow in the physical world (e.g., the semantics of the word red is grounded in perception of color vision).",1 Introduction,[0],[0]
"Language grounding for robots has received increased attention (Bansal et al., 2017) and language learning is an essential aspect to robots that learn about their environment and how to interact naturally with humans.
",1 Introduction,[0],[0]
"However, humans who interact with robots often assign anthropomorphic characteristics to
robots depending on how they perceive those robots; for example stereotypical gender (Eyssel and Hegel, 2012), social categorizations (Eyssel and Kuchenbrandt, 2012) stereotypical roles (Tay et al., 2014), as well as intelligence, interpretability, and sympathy (Novikova et al., 2017).",1 Introduction,[0],[0]
"This has implications for the kinds of tasks that we ask our robots to do and the settings in which robots perform those tasks, including tasks where language grounding and acquisition is either a direct or indirect goal.",1 Introduction,[0],[0]
"It is important not to assume that humans will perceive the robot in the “correct” way; rather, the age and academic level appropriateness needs to be monitored, particularly in a grounding and first-language acquisition task.",1 Introduction,[0],[0]
The obvious follow-up question here is: Do robots need to acquire language as human children do?,1 Introduction,[0],[0]
"Certainly, enough functional systems exist that show how language can be acquired in many ways.",1 Introduction,[0],[0]
"The motivation here, however, is that those systems could be missing something in the language acquisition process that children receive because of the way they are perceived by human dialogue partners.",1 Introduction,[0],[0]
"We cannot tell until we have a robot that is shown as being perceived as a child (current work) and use that robot for language learning tasks (future work).
",1 Introduction,[0],[0]
"We hypothesize in this paper that how a robot looks and acts will not only affect how humans perceive that robot’s intelligence, but it will also affect how humans perceive that robot’s age and academic level.",1 Introduction,[0],[0]
"In particular, we explore how humans perceive three different systems: two embodied robots, and one a spoken dialogue system (explained in Section 3).",1 Introduction,[0],[0]
"We show through an experiment that human perception of robots, particularly in how they perceive the robots’ intelligence, age, and academic level, is due to how the robot appears, but also due to how the robot uses speech to interact.",1 Introduction,[0],[0]
"Several areas of research play into this work including seminal (Roy and Reiter, 2005) and recent work in grounded semantic learning in various tasks and settings, notably learning descriptions of the immediate environment (Walter et al., 2014); navigation (Kollar et al., 2010); nouns, adjectives, and relational spatial descriptions (Kennington and Schlangen, 2015); spatial operations (Bisk et al., 2018), and verbs (She and Chai, 2016).",2 Related Work,[0],[0]
"Previous work has also focused on multimodal aspects of human-robot interaction, including grounded semantics (Thomason et al., 2016), engagement (Bohus and Horvitz, 2009), and establishing common ground (Chai et al., 2014).",2 Related Work,[0],[0]
"Others have explored how robots are perceived differently by different human age groups such as the elderly (Kiela et al., 2015), whereas we are focused on the perceived age of the robot by human dialogue partners.",2 Related Work,[0],[0]
"Moreover, though we do not design our robots for deliberate affective grounding (i.e., the coordination effect of building common understanding of what behaviors can be exhibited, and how beahvior is interpreted emotionally) as in Jung (2017), we hypothesize that how our robots behave effects how they are perceived.
",2 Related Work,[0],[0]
"Kiela et al. (2015) compared tutoring sequences in parent-child and human-robot interactions with varying verbal and demonstrative behaviors, and Lyon et al. (2016) brought together several areas of research relating to language acquisition in robotics.",2 Related Work,[0],[0]
"We differ from this previous work in that we do not explcitely tell our participants to interact with the robots as they would a child, effectively removing the assumption that participants will treat robots in an age-appropriate way.",2 Related Work,[0],[0]
"Another important difference to their work is that we opted not to use an anthropomorphically realistic child robot because such robots often make people feel uncomfortable (Eberle, 2009).",2 Related Work,[0],[0]
"Our work is similar in some ways to, but different from work in paralinguistics where recognition of age given linguistic features is a common task (Schuller et al., 2013) in that we are make use of exra-linguistic features.",2 Related Work,[0],[0]
Our work primarily builds off of Novikova et al. (2017) who used multimodal features derived from the human participants to predict perceived likability and intelligence of a robot.,2 Related Work,[0],[0]
We use similar multimodal features to predict the perceived age and academic level.,2 Related Work,[0],[0]
"An important difference to their work is that we designed
the experiment with three robots to vary the appearance and two language settings to vary the behavior and linguistic factors of the robots.",2 Related Work,[0],[0]
The primary goal of our experiment is to determine what factors play into how humans perceive a robot’s age and academic level.,3 Experiment,[0],[0]
"We used the three following robotic systems in our experiment:
• Kobuki Base Robot with a Microsoft Kinect on top (denoted as KOBUKI)
",3 Experiment,[0],[0]
"• Anki Cozmo robot (denoted as COZMO)
• Non-physical “robot” (i.e., a non-embodied spoken dialogue system) which was just a camera and speaker (denoted as SDS)
Robot Appearance The COZMO has a head and animated eyes and is noticeably smaller than the KOBUKI.",3 Experiment,[0],[0]
"The robots did not move during the experiments, though they were clearly activated (e.g., the KOBUKI had a small light and COZMO’s eyes were visible and moved at random intervals, which is the default setting).",3 Experiment,[0],[0]
Figure 1 shows the KOBUKI and COZMO robots as seen by the participants.,3 Experiment,[0],[0]
"We chose these three robots because they were available to us and we assume that, based solely on appearance, participants would perceive the robots differently.",3 Experiment,[0],[0]
"We chose a spoken dialogue system (SDS) as one of the “robots” because we wanted to explore how participants perceive a system that is unembodied in direct comparison to embodied systems.
",3 Experiment,[0],[0]
Robot Behavior The COZMO robot has a builtin speaker with a young-sounding synthetic voice.,3 Experiment,[0],[0]
We used two adult voices for the KOBUKI and SDS robots from the Amazon Polly system (the Joey and Joanna voices) which we played on a small speaker.1 We vary the language setting of the robots by assigning each robot one of two possible settings: high and low.,3 Experiment,[0],[0]
"In the high setting,
1https://aws.amazon.com/polly/
the following responses were possible: sure; okay;",3 Experiment,[0],[0]
yeah;,3 Experiment,[0],[0]
"oh; I see; uh huh; (where the robot repeats a word spoken by the participant) and any combination of those responses in a single uttered response; and for the low setting, the following responses were possible: yes; okay; uh; (where the robot repeats a word spoken by the participant).",3 Experiment,[0],[0]
"In the high setting, the robot would produce responses more often than in the low setting.",3 Experiment,[0],[0]
"These responses are characteristic of different levels of feedback; the high setting contains feedback strategies that signaled understanding to the participant, whereas the low setting only signaled phonetic receipt.",3 Experiment,[0],[0]
"This corresponds to previous work (Stubbe, 1998) which investigated various feedback strategies employed in human-human dialogue termed neutral minimal responses (corresponding to our low setting) and supportive minimal responses (corresponding to our high setting).
",3 Experiment,[0],[0]
"With this setup, there are 6 possible settings: high and low for each of the three robots.",3 Experiment,[0],[0]
"Our hypothesis is that participants will perceive KOBUKI as older and more intelligent than COZMO overall (in both high and low settings) despite being less anthropomorphic, perceive COZMO as very young in the low setting, and that SDS will be perceived as older than COZMO in the high setting, but similar to COZMO in the low setting.",3 Experiment,[0],[0]
The experimenter gave each participant consent and instruction forms to complete before the experiment.,3.1 Task and Participants,[0],[0]
"The participant was then given three colored pentomino puzzle tiles and a sheet of paper with three goal shapes (example in Figure 2), each composed from the corresponding tiles.",3.1 Task and Participants,[0],[0]
The experimenter instructed the participant to sit at a table where they would see a robot.,3.1 Task and Participants,[0],[0]
Their task was to explain to the robot how to use the tiles to construct the three goal shapes and tell the robot the name of each shape.,3.1 Task and Participants,[0],[0]
"The experimenter did
not specify how to accomplish this task or give examples of the kinds of things that the robot might understand.",3.1 Task and Participants,[0],[0]
"The experimenter then left the room, leaving the participant with the robot to complete the task.",3.1 Task and Participants,[0],[0]
"The robots only responded verbally in the low/high setting as explained above and their responses were controlled by the experimenter (i.e., in a Wizard-of-Oz paradigm).",3.1 Task and Participants,[0],[0]
The robots produced no physical movement.,3.1 Task and Participants,[0],[0]
"When the participant completed each task, they uttered a keyword (i.e., done), then the experimenter returned and administered a questionnaire.",3.1 Task and Participants,[0],[0]
"This process was followed for each of the three robots.
",3.1 Task and Participants,[0],[0]
"The following aspects of the experiment were randomly assigned to each participant: the order of robot presentation, the puzzle tiles and corresponding goal shapes for each robot, the language setting (i.e., high or low) which remained the same for all three robot interactions for each participant, and for KOBUKI and SDS the adult voice (either Joey or Joanna).",3.1 Task and Participants,[0],[0]
"We recruited 21 Englishspeaking participants (10 Female, 11 Male), most of whom were students of Boise State University.",3.1 Task and Participants,[0],[0]
The interaction generally took about 30 minutes; participants received $5 for their participation.,3.1 Task and Participants,[0],[0]
We recorded the interactions with a camera that captured the face and a microphone that captured the speech of each participant.,3.2 Data Collection,[0],[0]
"We automatically transcribed the speech using the Google Speech API (we manually checked an accented female
voice which achieved an estimated WER of 30.0) and segmented transcriptions into sentences after 1 second of detected silence, which is a longer pause duration than the average pause duration for adult-adult conversation (though adults tend to take longer pauses when interacting with children (DePaulo and Coleman, 1986)).",3.2 Data Collection,[0],[0]
"This resulted in video, audio, and transcriptions for each participant, for each robot interaction.",3.2 Data Collection,[0],[0]
"We also collected 58 questionnaires (we had to remove several because they were missing data; i.e., some participants did not answer some of the questionnaire questions), one for each robot interaction, from each participant.",3.2 Data Collection,[0],[0]
"Using the data collected from the experiment, we derived subjective measures from the questionnaires and we derived a number of objective measures from the video, audio, and transcriptions.",4 Data Analysis,[0],[0]
"In this section, we explain what methods we used to derive and analyze those measures.
",4 Data Analysis,[0],[0]
"Emotion Features Using the video feed of the participants, we extracted an image of the participants’ faces every 5 seconds.",4 Data Analysis,[0],[0]
"We used the Microsoft Emotion API for processing these images to calculate an average distribution over 8 possible emotion categories for each image: happiness, sadness, surprise, anger, fear, contempt, disgust, and neutral.",4 Data Analysis,[0],[0]
"Figure 4 shows an example of face snapshots taken from the video in the task setting and the corresponding distributions over the emotions as produced by the API.
",4 Data Analysis,[0],[0]
"Prosodic Features From the audio, we calculated the average fundamental frequency of speech (F0) of the participant over the entire interaction
between the participant and the robot for each robot setting.
",4 Data Analysis,[0],[0]
"Linguistic Features Using the automatically transcribed text, we follow directly from Novikova et al. (2017) to derive several linguistic measures, with the exception that we did not derive dialoguerelated features because, though our robots were engaging in a kind of dialogue with the participants, they weren’t taking the floor in a dialogue turn; i.e., our robots were only providing feedback to signal either phonetic receipt or semantic understanding (low and high settings, respectively).",4 Data Analysis,[0],[0]
"We used the Lexical Complexity Analyser (Lu, 2009, 2012), which yields several measures, two of which we leverage here: lexical diversity (LD) and the mean segmented type-token ratio (MSTTR), both of which measure diversity of tokens; the latter averaging the diversity over segments of a given length (for all measures, higher values denote more respective diversity and sophistication in the measured text).",4 Data Analysis,[0],[0]
"The Complexity Analyser also produces a lexical sophistication (LS) measure, also known as lexical rareness which is the proportion of lexical word types that are not common (i.e., not the 2,000 most frequent words in the British National Corpus).
",4 Data Analysis,[0],[0]
"For syntactic variation, we applied the D-Level Analyser (Lu, 2009) using the D-Level scale (Lu, 2014).",4 Data Analysis,[0],[0]
"This tool builds off of the Stanford Partof-Speech Tagger (Toutanova and Manning, 2000) and the Collins Parser (Collins, 2003) and produces a scaled analysis.",4 Data Analysis,[0],[0]
"The D-Level scale counts utterances belonging to one of 8 levels (Levels 0- 7), where lower levels such as 0-1 include simple or incomplete sentences; the higher the level, the more complex the syntactic structure.",4 Data Analysis,[0],[0]
"We report each of these levels along with a mean level.
",4 Data Analysis,[0],[0]
"Godspeed Questionnaire We used the Godspeed Questionnaire (Bartneck et al., 2009) which consists of 21 pairs of contrasting characteristics in areas of anthropomorphism (e.g., artificial vs. lifelike), likability (e.g., unfriendly vs. friendly), intelligence (e.g., incompetent vs. competent), and interpretabilitiy (e.g., confusing vs. clear) each with a 5-point scaling between them.",4 Data Analysis,[0],[0]
"In addition to those questions, we included the following:
• Have you ever interacted with a robot before participating in this study?
",4 Data Analysis,[0],[0]
"• If you could give the robot you interacted
with a human age, how old would you say it was?
• What level of education would be appropriate for the robot you interacted with?
",4 Data Analysis,[0],[0]
"For the question asking about human age, answers could be selected from a set of binned ranges (under 2 years, 2-5, 6-12, 13-17, 18-24, 25- 34, 35 and older), and for the education question, answers could be selected from preschool, kindergarten, 1-12 (each grade could be selected), undergraduate, graduate, post-graduate.",4 Data Analysis,[0],[0]
"In this section, we analyze the results of the data for the emotional, prosodic, and linguistic measures.",4.1 Analysis,[0],[0]
We also provide correlations between those measures and the Godspeed Questionnaire.,4.1 Analysis,[0],[0]
"At the end of this section, we provide a discussion of the overall analysis.
",4.1 Analysis,[0],[0]
"Emotion Analysis The most common emotional response as produced by the MS Emotions API was neutral for all settings, ranging from 73- 87% (avg 81%).",4.1 Analysis,[0],[0]
"The next most common emotions were happiness (avg 11.1%), sadness (avg 3.7%), surprise (2%), and contempt (avg 1%).",4.1 Analysis,[0],[0]
We show in Figure 5 the average distribution over those four emotions for all of our settings.,4.1 Analysis,[0],[0]
"All other emotions were negligible.
",4.1 Analysis,[0],[0]
Prosodic Analysis Table 1 shows the the average F0 scores for each setting.,4.1 Analysis,[0],[0]
"In general, in the low linguistic setting participants averaged a higher F0 across all robots.",4.1 Analysis,[0],[0]
This was the case also for individual robots.,4.1 Analysis,[0],[0]
"By a wide margin, COZMO
averaged a higher F0 than the other two robots under both low and high settings.
",4.1 Analysis,[0],[0]
Linguistic Analysis Table 2 shows the results of the linguistic analysis.,4.1 Analysis,[0],[0]
"The LD (lexical diversity) scores show that, on average, participants used more LD in the high settings.",4.1 Analysis,[0],[0]
Figure 6 shows the results of the D-Level analysis.,4.1 Analysis,[0],[0]
"Level0 (i.e., short utterances) was by far the most common level which accounted for 66% of all utterances for all participants.",4.1 Analysis,[0],[0]
"The second most common was Level7, the level representing the most complex types of utterances.",4.1 Analysis,[0],[0]
"This is no surprise, as Level7 accounts for longer utterances above some threshold; i.e., all utterances of a certain length and complexity or higher fit under Level7.",4.1 Analysis,[0],[0]
"The low setting had a Level7 value of 17%, and the high setting had a Level7 value of 11%.",4.1 Analysis,[0],[0]
"This may seem surprising, but it follows previous research which has shown that, when a speaker receives fewer responses, they draw out their turns, which result longer utterances (Stubbe, 1998).
",4.1 Analysis,[0],[0]
"Questionnaire Analysis We calculated (Spearman) correlations between the prosodic, emotional, and linguistic features, and the questionnaire responses with the low/high settings and the robot settings.",4.1 Analysis,[0],[0]
Table 3 shows the results where the correlation had a strength of 0.5 or higher.,4.1 Analysis,[0],[0]
"Fig-
ures 7 and 8 respectively show the age groups and academic years that the participants perceived for each robot in each setting.",4.1 Analysis,[0],[0]
"Overall, participants assigned low age and academic level to all robots when they produced feedback that did not signal semantic understanding (i.e., the low setting).",4.1 Analysis,[0],[0]
"They also assigned a lower age and academic level to COZMO for all settings (with the exception of one 10th grade assignment).
",4.1 Analysis,[0],[0]
Our results confirm the Novikova et al. (2017) result which showed a strong correlation between F0 and knowledgeable.,4.1 Analysis,[0],[0]
"Interestingly, F0 only correlated knowledge with the physical robots and the SDS robot in the low setting.",4.1 Analysis,[0],[0]
"There is more to the F0 correlations: F0 in the low setting correlates with conscious, in the high setting correlates with natural and human-like, and in the COZMO robot setting with lifelike.",4.1 Analysis,[0],[0]
"There were some correlations with age and academic level: LS in the high setting correlated with the robot being perceived as age 18-24 and when interacting with COZMO, a higher F0 correlated with a perception of COZMO being 6-12 years old and in the 4th grade.",4.1 Analysis,[0],[0]
"Lexical diversity correlates with sadness and contempt, which indicates that participants use more diverse language (i.e., they continue speaking) when they are frustrated with the interaction (Stubbe, 1998); particularly in the high setting when they expect more from the robots.",4.1 Analysis,[0],[0]
"However, they increase their LS also in the high setting because they perceive the robot as more intelligent.
",4.1 Analysis,[0],[0]
"Discussion Taken together, the emotional, prosodic, and linguistic analyses show that participants treated the low setting with a higher average F0, less linguistic complexity, and a greater display of happiness in their facial emotions.",4.1 Analysis,[0],[0]
"This is useful knowledge: the way a robot
speaks has an impact on the perception of that robot by the human users, regardless of whether or not that robot is embodied.",4.1 Analysis,[0],[0]
"Moreover, despite the fact that the robots only produced feedback as the only system behavior, the participants tended to assign a younger age and academic level to the COZMO robot.",4.1 Analysis,[0],[0]
There were subtle differences in how the participants perceived the KOBUKI and SDS robots.,4.1 Analysis,[0],[0]
"In general, the participants seemed to perceive the SDS as being older and as having a higher academic level in the emotion, prosodic, and linguistic modalities, though those differences were small.",4.1 Analysis,[0],[0]
This leads us to postulate that anthropomorphic physical features do not automatically denote intelligence in the same way as perceived ability to comprehend language.,4.1 Analysis,[0],[0]
"In general, participants assigned younger ages and lower academic levels for the low setting, and higher ones for the high setting.",4.1 Analysis,[0],[0]
"Moreover, participants generally assigned COZMO lower ages, including the most for Under 2 years.",4.1 Analysis,[0],[0]
Of note is that no participant assigned COZMO an age of above 6-12 years for either of the low/high settings.,4.1 Analysis,[0],[0]
"The highest assigned academic level was undergrad, which was never assigned to COZMO.",4.1 Analysis,[0],[0]
The KOBUKI and SDS robots were both variously assigned comparable older ages and average academic levels under all settings.,4.1 Analysis,[0],[0]
"Using the measures we derived from the collected data, we attempted to determine if we could predict the perceived age and academic level of the robots.",5 Prediction Tasks,[0],[0]
"We used the emotional features (happiness, sadness, surprise, anger, fear, contempt, disgust, and neutral), the prosody (F0 average), and the linguistic features (LS, LD, MSTTR) to predict
both the age and the academic level as separate classification tasks.",5 Prediction Tasks,[0],[0]
"We also predict intelligence, likability, and interpretability in order to compare to previous work.",5 Prediction Tasks,[0],[0]
"Data & Task For predicting both age and academic level, we used the 58 data points from the participants for each interaction with each robot and applied those points to a 5-fold cross validation.",5.1 Predicting the Perceived Age & Academic Level of Robots,[0],[0]
We used a logistic regression classifier to perform the classification using the Python scikitlearn library.,5.1 Predicting the Perceived Age & Academic Level of Robots,[0],[0]
"We report accuracy for our metric.
Age We ran the cross validation for two different settings when predicting age.",5.1 Predicting the Perceived Age & Academic Level of Robots,[0],[0]
"In particular, we varied the labels that could be classified.",5.1 Predicting the Perceived Age & Academic Level of Robots,[0],[0]
"We conducted a first task which treated all of the 7 possible outcomes for age as individual labels (i.e., under 2 years, 2-5, 6-12, 13-17, 18-24, 25-34, 35 and older) and a second task splitting at age 18 (i.e., younger than 18 is one label; 18 & older is the other label).",5.1 Predicting the Perceived Age & Academic Level of Robots,[0],[0]
"The respective random baselines are 14% and 50%.
",5.1 Predicting the Perceived Age & Academic Level of Robots,[0],[0]
"Academic Levels Similar to age, we ran the cross validation for two different settings when predicting for perceived academic level.",5.1 Predicting the Perceived Age & Academic Level of Robots,[0],[0]
"The first task treated all of the 14 possible outcomes for academic level as individual labels (preschool, kindergarten, 1-11, undergraduate; we leave out graduate and post-graduate because they were never selected in the questionnaires, nor was 12th grade), the second task treated treated preschool and beyond preschool as a binary classification task.",5.1 Predicting the Perceived Age & Academic Level of Robots,[0],[0]
"The respective random baselines are 7% and 50%.
",5.1 Predicting the Perceived Age & Academic Level of Robots,[0],[0]
Results The results of this prediction task are in Table 4.,5.1 Predicting the Perceived Age & Academic Level of Robots,[0],[0]
"As might be expected, when attempting to predict using many labels, the classification task is challenging with so little data.",5.1 Predicting the Perceived Age & Academic Level of Robots,[0],[0]
"However, the classifiers beat their respective random baselines.",5.1 Predicting the Perceived Age & Academic Level of Robots,[0],[0]
"When classifying for age, the best performing task was a binary task splitting on 18 years at 87%, effectively making it a classifier that can determine if a human user perceives the robot as an adult or as a minor.",5.1 Predicting the Perceived Age & Academic Level of Robots,[0],[0]
The best performing task for the academic level classification was treating preschool and above preschool as a binary classifier.,5.1 Predicting the Perceived Age & Academic Level of Robots,[0],[0]
"Though the data is sparse, these classifiers give us useful information: a robot can use these classifiers to determine if they are perceived as an adult by human dialogue partners, and, more importantly for our purposes, as a preschool aged child, which is the age range in which we are interested for language acquisition tasks.",5.1 Predicting the Perceived Age & Academic Level of Robots,[0],[0]
"Data & Task To directly compare with Novikova et al. (2017), we also predicted perceived intelligence, likability, and interpretability using a ridge regression classifier (which is optimized to reduced standard error) while considering only certain subsets of out our feature set.","5.2 Predicting Intelligence, Likability, and Interpretability",[0],[0]
"We evaluated when only considering emo-
tional features, prosody, non-linguistic (in our case, emotions and prosody), linguistic, and all combined features.","5.2 Predicting Intelligence, Likability, and Interpretability",[0],[0]
Our metric was root mean square error (RMSE).,"5.2 Predicting Intelligence, Likability, and Interpretability",[0],[0]
"We average the RMSE over a 5-fold cross-validation.
","5.2 Predicting Intelligence, Likability, and Interpretability",[0],[0]
Results Table 5 shows the results of this prediction task.,"5.2 Predicting Intelligence, Likability, and Interpretability",[0],[0]
"We found that likability is predicted best by prosody, perceived intelligence is predicted best by linguistic features, and interpretability is predicted best by also using linguistic features.","5.2 Predicting Intelligence, Likability, and Interpretability",[0],[0]
"One big difference between our experiment data and that of previous work is that we did not consider dialogue features (e.g., number of turns, speech duration, number of self-repetitions, etc.), which they termed as non-linguistic features.","5.2 Predicting Intelligence, Likability, and Interpretability",[0],[0]
"Those features were important in predicting perceived intelligence and interpretability in their work; here, linguistic and prosodic features were the most effective in predicting all three human perceptions of the robots.","5.2 Predicting Intelligence, Likability, and Interpretability",[0],[0]
This confirms the work of Novikova et al. (2017) that linguistic features are a good predictor of interpretability.,"5.2 Predicting Intelligence, Likability, and Interpretability",[0],[0]
"In this paper, we have investigated how human dialogue partners perceive the age and academic level of three robotic systems, two of which were embodied (albeit not particularly anthropomorphically), and one unembodied spoken dialogue system.",6 Discussion & Conclusion,[0],[0]
"We collected data from participants as they interacted with the three robotic systems then derived prosodic, emotional, and linguistic features from that participant data, and found that those features correlate with certain age and academic perceptions of those robots, as well as a number of other subjective measures from the Godspeed Questionnaire.",6 Discussion & Conclusion,[0],[0]
"This work confirms what previous work has shown: that humans tend to perceive robots differently depending on different factors; in our case, varying the look and spo-
ken reposes determined how the human participants perceived the age and academic levels, as well as intelligence, likability, and interpretability of those robots.",6 Discussion & Conclusion,[0],[0]
"We were then able to use these features to automatically predict perceived age (i.e., adult or minor), perceived academic level (i.e., preschool or above) and perceived intelligence, likability, and interpretabilitiy.",6 Discussion & Conclusion,[0],[0]
"One important result of our experiment was that human dialogue partners perceive the unembodied robot (i.e., SDS) in similar ways to embodied robots; that is, the way a robot or system speaks (i.e., in our case, produces feedback by signaling either phonetic receipt or semantic understanding) is as important to human perceptions of intelligence and likability as visual characteristics.
",6 Discussion & Conclusion,[0],[0]
"We cannot not simply assume that human dialogue partners would treat a robot as they would a child, which is an important aspect of tasks with realistic first-language acquisition settings.",6 Discussion & Conclusion,[0],[0]
The work presented here shows that those interacting with a robot like COZMO will more likely treat COZMO as a learning child instead of as an adult.,6 Discussion & Conclusion,[0],[0]
"This is an important result because for future work we plan on using the COZMO robot as a platform for first language acquisition research, where the setting will be more similar to first language acquisition in humans than common language grounding tasks.",6 Discussion & Conclusion,[0],[0]
"The COZMO robot is an afforable way for reseachers to couple spoken dialogue systems with a robotic system; it has a Python SDK which allows researchers to access its sensors (including a color camera) and control its wheel and arm movements, as well as its speech and animated face.",6 Discussion & Conclusion,[0],[0]
"Our results show that human users generally like COZMO, find COZMO lifelike, competent, and intelligent; i.e., COZMO may be treated as a child, but it has potential to learn.
",6 Discussion & Conclusion,[0],[0]
"In future work, we will apply a model of grounded semantics in a co-located dialogue setting where COZMO can learn the semantics of words as it interacts with human dialogue partners.
",6 Discussion & Conclusion,[0],[0]
Acknowledgements This work was supported in part by the Boise State University HERC program.,6 Discussion & Conclusion,[0],[0]
"We would like to thank the anonymous reviewers for their comments, Hoda Mehrpouyan for use of her Kobuki robot, and the Mary Ellen Ryder Linguistics Lab at Boise State University for use of their lab for the data collection.",6 Discussion & Conclusion,[0],[0]
This work was approved by Boise State University IRB 131-SB17-043.,6 Discussion & Conclusion,[0],[0]
"When interacting with robots in a situated spoken dialogue setting, human dialogue partners tend to assign anthropomorphic and social characteristics to those robots.",abstractText,[0],[0]
"In this paper, we explore the age and educational level that human dialogue partners assign to three different robotic systems, including an un-embodied spoken dialogue system.",abstractText,[0],[0]
We found that how a robot speaks is as important to human perceptions as the way the robot looks.,abstractText,[0],[0]
"Using the data from our experiment, we derived prosodic, emotional, and linguistic features from the participants to train and evaluate a classifier that predicts perceived intelligence, age, and education level.",abstractText,[0],[0]
Predicting Perceived Age: Both Language Ability and Appearance are Important,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1741–1751 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1741",text,[0],[0]
"Semantic graphs, such as WordNet (Fellbaum, 1998), encode the structural qualities of language as a representation of human knowledge.",1 Introduction,[0],[0]
"On the local level, they describe connections between specific semantic concepts, or synsets, through individual edges representing relations such as hypernymy (‘is-a’) or meronymy (‘is-part-of’); on the global level, they encode emergent regular properties in the induced relation graphs.",1 Introduction,[0],[0]
"Local properties have been subject to extensive study in recent years via the task of relation prediction, where individual edges are found based mostly on
distributional methods that embed synsets and relations into a vector space (e.g. Socher et al., 2013; Bordes et al., 2013; Toutanova and Chen, 2015; Neelakantan et al., 2015).",1 Introduction,[0],[0]
"In contrast, while the structural regularity and significance of global aspects of semantic graphs is well-attested (Sigman and Cecchi, 2002), global properties have rarely been used in prediction settings.",1 Introduction,[0],[0]
"In this paper, we show how global semantic graph features can facilitate in local tasks such as relation prediction.
",1 Introduction,[0],[0]
"To motivate this approach, consider the hypothetical hypernym graph fragments in Figure 1: in (a), the semantic concept (synset) ‘catamaran’ has
a single hypernym, ‘boat’.",1 Introduction,[0],[0]
This is a typical property across a standard hypernym graph.,1 Introduction,[0],[0]
"In (b), the synset ‘cat’ has two hypernyms, an unlikely event.",1 Introduction,[0],[0]
"While a local relation prediction model might mistake the relation between ‘cat’ and ‘boat’ to be plausible, for whatever reason, a high-order graphstructure-aware model should be able to discard it based on the knowledge that a synset should not have more than one hypernym.",1 Introduction,[0],[0]
"In (c), an impossible situation arises: a cycle in the hypernym graph leads each of the participating synsets to be predicted by transitivity as its own hypernym, contrary to the relation’s definition.",1 Introduction,[0],[0]
"However, a purely local model has no explicit mechanism for rejecting such an outcome.
",1 Introduction,[0],[0]
"In this paper, we examine the effect of global graph properties on the link structure via the WordNet relation prediction task.",1 Introduction,[0],[0]
"Our hypothesis is that features extracted from the entire graph can help constrain local predictions to structurally sound ones (Guo et al., 2007).",1 Introduction,[0],[0]
"Such features are often manifested as aggregate counts of small subgraph structures, known as motifs, such as the number of nodes with two or more outgoing edges, or the number of cycles of length 3.",1 Introduction,[0],[0]
"Returning to the example in Figure 1, each of these features will be affected when graphs (b) and (c) are evaluated, respectively.
",1 Introduction,[0],[0]
"To estimate weights on local and global graph features, we build on the Exponential Random Graph Model (ERGM), a log-linear model over networks utilizing global graph features (Holland and Leinhardt, 1981).",1 Introduction,[0],[0]
"In ERGMs, the likelihood of a graph is computed by exponentiating a weighted sum of the features, and then normalizing over all possible graphs.",1 Introduction,[0],[0]
"This normalization term grows exponentially in the number of nodes, and in general cannot be decomposed into smaller parts.",1 Introduction,[0],[0]
"Approximations are therefore necessary to fit ERGMs on graphs with even a few dozen nodes, and the largest known ERGMs scale only to thousands of nodes (Schmid and Desmarais, 2017).",1 Introduction,[0],[0]
"This is insufficient for WordNet, which has an order of 105 nodes.
",1 Introduction,[0],[0]
We extend the ERGM framework in several ways.,1 Introduction,[0],[0]
"First, we replace the maximum likelihood objective with a margin-based objective, which compares the observed network against alternative networks; we call the resulting model the MaxMargin Markov Graph Model (M3GM), drawing on ideas from structured prediction (Taskar
et al., 2004).",1 Introduction,[0],[0]
"The gradient of this loss is approximated by importance sampling over candidate negative edges, using a local relational model as a proposal distribution.",1 Introduction,[0],[0]
"The complexity of each epoch of estimation is thus linear in the number of edges, making it possible to scale up to the 105 nodes in WordNet.1 Second, we address the multi-relational nature of semantic graphs, by incorporating a combinatorial set of labeled motifs.",1 Introduction,[0],[0]
"Finally, we link graph-level relational features with distributional information, by combining the M3GM with a dyad-level model over word sense embeddings.
",1 Introduction,[0],[0]
"We train M3GM as a re-ranker, which we apply to a a strong local-feature baseline on the WN18RR dataset (Dettmers et al., 2018).",1 Introduction,[0],[0]
This yields absolute improvements of 3-4 points on all commonly-used metrics.,1 Introduction,[0],[0]
"Model inspection reveals that M3GM assigns importance to features from all relations, and captures some interesting inter-relational properties that lend insight into the overall structure of WordNet.2",1 Introduction,[0],[0]
Relational prediction in semantic graphs.,2 Related Work,[0],[0]
Recent approaches to relation prediction in semantic graphs generally start by embedding the semantic concepts into a shared space and modeling relations by some operator that induces a score for an embedding pair input.,2 Related Work,[0],[0]
"We use several of these techniques as base models (Nickel et al., 2011; Bordes et al., 2013; Yang et al., 2014); detailed description of these methods is postponed to Section 3.2.",2 Related Work,[0],[0]
Socher et al. (2013) generalize over the approach of Nickel et al. (2011) by using a bilinear tensor which assigns multiple parameters for each relation; Shi and Weninger (2017) project the node embeddings in a translational model similar to that of Bordes et al. (2013); Dettmers et al. (2018) apply a convolutional neural network by reshaping synset embeddings to 2-dimensional matrices.,2 Related Work,[0],[0]
"None of these embedding-based approaches incorporate structural information; in general, improvements in embedding-based methods are expected to be complementary to our approach.
",2 Related Work,[0],[0]
"1Although in principle the number of edges could grow quadratically with the number of nodes, Steyvers and Tenenbaum (2005) show that semantic graphs like WordNet tend to be very sparse, so that the number of observed edges grows roughly linearly with the number of nodes.
",2 Related Work,[0],[0]
2Our code is available at http://www.github.,2 Related Work,[0],[0]
"com/yuvalpinter/m3gm.
",2 Related Work,[0],[0]
"Some recent works compose single edges into more intricate motifs, such as Guu et al. (2015), who define a task of path prediction and compose various functions to solve it.",2 Related Work,[0],[0]
They find that compositionalized bilinear models perform best on WordNet.,2 Related Work,[0],[0]
Minervini et al. (2017) train link-prediction models against an adversary that produces examples which violate structural constraints such as symmetry and transitivity.,2 Related Work,[0],[0]
"Another line of work builds on local neighborhoods of relation interactions and automatic detection of relations from syntactically parsed text (Riedel et al., 2013; Toutanova et al., 2015).",2 Related Work,[0],[0]
Schlichtkrull et al. (2017) use Graph Convolutional Networks to predict relations while considering high-order neighborhood properties of the nodes in question.,2 Related Work,[0],[0]
"In general, these methods aggregate information over local neighborhoods, but do not explicitly model structural motifs.
",2 Related Work,[0],[0]
"Our model introduces interaction features between relations (e.g., hypernyms and meronyms) for the goal of relation prediction.",2 Related Work,[0],[0]
"To our knowledge, this is the first time that relation interaction is explicitly modeled into a relation prediction task.",2 Related Work,[0],[0]
"Within the ERGM framework, Lu et al. (2010) train a limited set of combinatory path features for social network link prediction.
",2 Related Work,[0],[0]
Scaling exponential random graph models.,2 Related Work,[0],[0]
The problem of approximating the denominator of the ERGM probability has been an active research topic for several decades.,2 Related Work,[0],[0]
Two common approximation methods exist in the literature.,2 Related Work,[0],[0]
"In Maximum Pseudolikelihood Estimation (MPLE; Strauss and Ikeda, 1990), a graph’s probability is decomposed into a product of the probability for each edge, which in turn is computed based on the ERGM feature difference between the graph excluding the edge and the full graph.",2 Related Work,[0],[0]
"Monte Carlo Maximum Likelihood Estimation (MCMLE; Snijders, 2002) follows a sampling logic, where a large number of graphs is randomly generated from the overall space under the intuition that the sum of their scores would give a good approximation for the total score mass.",2 Related Work,[0],[0]
"The probability for the observed graph is then estimated following normalization conditioned on the sampling distribution, and its precision increases as more samples are gathered.",2 Related Work,[0],[0]
"Recent work found that applying a parametric bootstrap can increase the reliability of MPLE, while retaining its superiority in training speed (Schmid
and Desmarais, 2017).",2 Related Work,[0],[0]
"Despite this result, we opted for an MCMLE-based approach for M3GM, mainly due to the ability to keep the number of edges constant in each sampled graph.",2 Related Work,[0],[0]
"This property is important in our setup, since local edge scores added or removed to the overall graph score can occasionally dominate the objective function, giving unintended importance to the overall edge count.",2 Related Work,[0],[0]
"Consider a graph G = (V,E), where V is a set of vertices and E = {(si, ti)}|E|i=1 is a set of directed edges.",3 Max-Margin Markov Graph Models,[0],[0]
"The ERGM scoring function defines a probability over G|V |, the set of all graphs with |V | nodes.",3 Max-Margin Markov Graph Models,[0],[0]
"This probability is defined as a loglinear function,
PERGM(G) ∝",3 Max-Margin Markov Graph Models,[0],[0]
ψERGM(G),3 Max-Margin Markov Graph Models,[0],[0]
"= exp ( θT f(G) ) , (1)
where f is a feature function, from graphs to a vector of feature counts.",3 Max-Margin Markov Graph Models,[0],[0]
Features are typically counts of motifs — small subgraph structures — as described in the introduction.,3 Max-Margin Markov Graph Models,[0],[0]
"The vector θ is the parameter to estimate.
",3 Max-Margin Markov Graph Models,[0],[0]
"In this section we discuss our adaptation of this model to the domain of semantic graphs, leveraging their idiosyncratic properties.",3 Max-Margin Markov Graph Models,[0],[0]
"Semantic graphs are composed of multiple relation types, which the feature space needs to accommodate; their nodes are linguistic constructs (semantic concepts) associated with complex interpretations, which can benefit the graph representation through incorporating their embeddings in Rd into a new scoring model.",3 Max-Margin Markov Graph Models,[0],[0]
We then present our M3GM framework to perform reliable and efficient parameter estimation on the new model.,3 Max-Margin Markov Graph Models,[0],[0]
"Based on common practice in ERGM feature extraction (e.g., Morris et al., 2008), we select the following graph features as a basis:
• Total edge count;
• Number of cycles of length k, for k ∈ {2, 3};
• Number of nodes with exactly k outgoing (incoming) edges, for k ∈ {1, 2, 3};
• Number of nodes with at least k outgoing (incoming) edges, for k ∈ {1, 2, 3};
• Number of paths of length 2;
• Transitivity: the proportion of length-2 paths u → v → w where an edge u → w also exists.
",3.1 Graph Motifs as Features,[0],[0]
"Semantic graphs are multigraphs, where multiple relationships (hypernymy, meronymy, derivation, etc.) are overlaid atop a common set of nodes.",3.1 Graph Motifs as Features,[0],[0]
"For each relation r in the relation inventory R, we denote its edge set as Er, and redefine E = ⋃ r∈REr, the union of all labeled edges.",3.1 Graph Motifs as Features,[0],[0]
"Some relations do not produce a connected graph, while others may coincide with each other frequently, possibly in regular but intricate patterns: for example, derivation relations tend to occur between synsets in the higher, more abstract levels of the hypernym graph.",3.1 Graph Motifs as Features,[0],[0]
We represent this complexity by expanding the feature space to include relation-sensitive combinatory motifs.,3.1 Graph Motifs as Features,[0],[0]
"For each feature template from the basis list above, we extract features for all possible combinations of relation types existing in the graph.",3.1 Graph Motifs as Features,[0],[0]
"Depending on the feature type, these could be relation singletons, pairs, or triples; they may be order-sensitive or order-insensitive.",3.1 Graph Motifs as Features,[0],[0]
"For example:
• A combinatory ‘transitivity’ feature will be extracted for the proportion of paths
u hypernym−−−−−−→ v meronym−−−−−−→ w where an edge u has part−−−−−→ w also exists.
",3.1 Graph Motifs as Features,[0],[0]
"• A combinatory ‘2-outgoing’ feature will be extracted for the number of nodes with exactly one derivation and one has part.
",3.1 Graph Motifs as Features,[0],[0]
"The number of features thus scales in O(|R|K) for a feature basis which involves up to K edges in any feature, and so our 17 basis features (with K = 3) generate a combinatory feature set with roughly 3,000 features for the 11-relation version of WordNet used in our experiments (see Section 4.1).",3.1 Graph Motifs as Features,[0],[0]
"In classical ERGM application domains such as social media or biological networks, nodes tend to have little intrinsic distinction, or at least little meaningful intrinsic information that may be extracted prior to applying the model.",3.2 Local Score Component,[0],[0]
"In semantic graphs, however, the nodes represent synsets, which are associated with information that is both valuable to predicting the graph structure and approximable using unsupervised techniques such as embedding into a common d-dimensional vector
space based on copious amounts of available data.",3.2 Local Score Component,[0],[0]
We thus modify the traditional scoring function from eq.,3.2 Local Score Component,[0],[0]
"(1) to include node-specific information, by introducing a relation-specific association operator A(r) : V × V → R:
ψERGM+(G) =
= exp θT f(G) +∑ r∈R ∑ (s,t)∈Er A(r)(s, t)  .",3.2 Local Score Component,[0],[0]
"(2)
The association operator generalizes various models from the relation prediction literature:
TransE (Bordes et al., 2013) embeds each relation r into a vector in the shared space, representing a ‘difference’ between sources and targets, to compute the association score under a translational objective,
A(r)TRANSE(s, t) =",3.2 Local Score Component,[0],[0]
−‖es,3.2 Local Score Component,[0],[0]
"+ er − et‖.
BiLin (Nickel et al., 2011) embeds relations into full-rank matrices, computing the score by a bilinear multiplication,
A(r)BILIN(s, t) = e T s Wret.
",3.2 Local Score Component,[0],[0]
"DistMult (Yang et al., 2014) is a special case of BiLin where the relation matrices are diagonal, reducing the computation to a ternary dot product,
A(r)DISTMULT(s, t) = 〈es, er, et〉 = d∑
i=1
esi eri eti .",3.2 Local Score Component,[0],[0]
"The probabilistic formulation of ERGM requires the computation of a normalization term that sums over all possible graphs with a given number of nodes, GN .",3.3 Parameter Estimation,[0],[0]
"The set of such graphs grows at a rate that is super-exponential in the number of nodes, making exact computation intractable even for networks that are orders of magnitude smaller than semantic graphs like WordNet.",3.3 Parameter Estimation,[0],[0]
"One solution is to approximate probability using a variant of the Monte Carlo Maximum Likelihood Estimation (MCMLE) produce,
logP (G)",3.3 Parameter Estimation,[0],[0]
≈ logψ(G)− log |G|V ||,3.3 Parameter Estimation,[0],[0]
"M M∑ G̃∼G|V | ψ(G̃),
(3)
where M is the number of networks G̃ sampled from G|V |, the space of all (multirelational) edge sets on nodes V .",3.3 Parameter Estimation,[0],[0]
"Each G̃ is referred to as a negative sample, and the goal of estimation is to assign low scores to these samples, in comparison with the score assigned to the observed network G.
Network samples can be obtained using edgewise negative sampling.",3.3 Parameter Estimation,[0],[0]
"For each edge s r−→ t in the training network G, we remove it temporarily and consider T alternative edges, keeping the source s and relation r constant, and sampling a target t̃ from a proposal distribution Q. Every such substitution produces a new graph G̃,
G̃ =G ∪ {s r−→ t̃} \ {s r−→ t}.",3.3 Parameter Estimation,[0],[0]
"(4)
Large-margin objective.",3.3 Parameter Estimation,[0],[0]
"Rather than approximating the log probability, as in MCMLE estimation, we propose a margin loss objective: the log score for each negative sample G̃ should be below the log score for G by a margin of at least 1.",3.3 Parameter Estimation,[0],[0]
"This motivates the hinge loss,
L(Θ, G̃;G) =",3.3 Parameter Estimation,[0],[0]
"( 1− logψERGM+(G)
",3.3 Parameter Estimation,[0],[0]
+ logψERGM+(G̃) ),3.3 Parameter Estimation,[0],[0]
"+ , (5)
where (x)+ = max(0, x).",3.3 Parameter Estimation,[0],[0]
Recall that the scoring function ψERGM+ includes both the local association score for the alternative edge and the global graph features for the resulting graph.,3.3 Parameter Estimation,[0],[0]
"However, it is not necessary to recompute all association scores; we need only subtract the association score for the deleted edge s r−→ t, and add the association score for the sampled edge s r−→ t̃.
The overall loss function is the sum over N = |E|×T negative samples, {G̃(i)}Ni=1, plus an L2 regularizer on the model parameters,
L(Θ;G) = λ||Θ||22+ N∑ i=1",3.3 Parameter Estimation,[0],[0]
"L(Θ, G̃(i)).",3.3 Parameter Estimation,[0],[0]
"(6)
Proposal distribution.",3.3 Parameter Estimation,[0],[0]
"The proposal distribution Q used to sample negative edges is defined to be proportional to the local association scores of edges not present in the training graph:
Q(t̃ | s, r,G) ∝
{ 0 s
r−→ t̃ ∈ G A(r)(s, t̃) s r−→ t̃ /∈ G .",3.3 Parameter Estimation,[0],[0]
"(7)
By preferring edges that have high association scores, the negative sampler helps push the M3GM parameters away from likely false positives.",3.3 Parameter Estimation,[0],[0]
"We evaluate M3GM on the relation graph edge prediction task.3 Data for this task consists of a set of labeled edges, i.e. tuples of the form (s, r, t), where s and t denote source and target entities, respectively.",4 Relation Prediction,[0],[0]
"Given an edge from an evaluation set, two prediction instances are created by hiding the source and target side, in turn.",4 Relation Prediction,[0],[0]
"The predictor is then evaluated on its ability to predict the hidden entity, given the other entity and the relation type.4",4 Relation Prediction,[0],[0]
"A popular relation prediction dataset for WordNet is the subset curated as WN18 (Bordes et al., 2013, 2014), containing 18 relations for about 41,000 synsets extracted from WordNet 3.0.",4.1 WN18RR Dataset,[0],[0]
"It has been noted that this dataset suffers from considerable leakage: edges from reciprocal relations such as hypernym / hyponym appear in one direction in the training set and in the opposite direction in dev / test (Socher et al., 2013; Dettmers et al., 2018).",4.1 WN18RR Dataset,[0],[0]
This allows trivial rule-based baselines to achieve high performance.,4.1 WN18RR Dataset,[0],[0]
"To alleviate this concern, Dettmers et al. (2018) released the WN18RR set, removing seven relations altogether.",4.1 WN18RR Dataset,[0],[0]
"However, even this dataset retains four symmetric relation types: also see, derivationally related form, similar to, and verb group.",4.1 WN18RR Dataset,[0],[0]
These symmetric relations can be exploited by defaulting to a simple rulebased predictor.,4.1 WN18RR Dataset,[0],[0]
"We report the following metrics, common in ranking tasks and in relation prediction in particular: MR, the Mean Rank of the desired entity; MRR, Mean Reciprocal Rank, the main evaluation metric; and H@k, the proportion of Hits (true entities) found in the top k of the lists, for k ∈ {1, 10}.",4.2 Metrics,[0],[0]
"Unlike some prior work, we do not type-restrict the possible relation predictions (so, e.g., a verb group link may select a noun, and that would count against the model).",4.2 Metrics,[0],[0]
"We evaluate a single-rule baseline, three association models, and two variants of the M3GM re-
3Sometimes referred to as Knowledge Base Completion, e.g. in Socher et al. (2013).
",4.3 Systems,[0],[0]
"4We follow prior work in excluding the following from the ranked lists: the known entity (no self loops); entities from the training set which fit the instance; other entities in the evaluation set.
",4.3 Systems,[0],[0]
ranker trained on top of the best-performing association baseline.,4.3 Systems,[0],[0]
We include a single-rule baseline that predicts a relation between s and t in the evaluation set if the same relation was encountered between t and s in the training set.,4.3.1 RULE,[0],[0]
All other models revert to this baseline for the four symmetric relations.,4.3.1 RULE,[0],[0]
The next group of systems compute local scores for entity-relation triplets.,4.3.2 Association Models,[0],[0]
"They all encode entities into embeddings e. Each of these systems, in addition to being evaluated as a baseline, is also used for computing association scores in M3GM, both in the proposal distribution (see Section 3.3) and for creating lists to be re-ranked (see below): TRANSE, BILIN, DISTMULT.",4.3.2 Association Models,[0],[0]
"For detailed descriptions, see Section 3.2.",4.3.2 Association Models,[0],[0]
The M3GM is applied as a re-ranker.,4.3.3 Max-Margin Markov Graph Model,[0],[0]
"For each relation and source (target), the top K candidate targets (sources) are retrieved based on the local association scores.",4.3.3 Max-Margin Markov Graph Model,[0],[0]
"Each candidate edge is introduced into the graph, and the score ψERGM+(G) is used to re-rank the top-K list.
",4.3.3 Max-Margin Markov Graph Model,[0],[0]
"We add a variant to this protocol where the graph score and association score are weighted by α and 1 − α, repsectively, before being summed.",4.3.3 Max-Margin Markov Graph Model,[0],[0]
"We tune a separate αr for each relation type, using the development set’s mean reciprocal rank (MRR).",4.3.3 Max-Margin Markov Graph Model,[0],[0]
"These hyperparameter values offer further insight into where the M3GM signal benefits relation prediction most (see Section 6).
",4.3.3 Max-Margin Markov Graph Model,[0],[0]
"Since we do not apply the model to the symmetric relations (scored by the RULE baseline), they are excluded from the sampling protocol described in eq.",4.3.3 Max-Margin Markov Graph Model,[0],[0]
"(5), although their edges do contribute to the combinatory graph feature vector f .
",4.3.3 Max-Margin Markov Graph Model,[0],[0]
Our default setting backpropagates loss into only the graph weight vector θ.,4.3.3 Max-Margin Markov Graph Model,[0],[0]
We experiment with a model variant which backpropagates into the association model and synset embeddings as well.,4.3.3 Max-Margin Markov Graph Model,[0],[0]
"For the association component of our model, we require embedding representations for WordNet synsets.",4.4 Synset Embeddings,[0],[0]
"While unsupervised word embedding techniques go a long way in representing wordforms (Collobert et al., 2011; Mikolov et al., 2013;
Pennington et al., 2014), they are not immediately applicable to the semantically-precise domain of synsets.",4.4 Synset Embeddings,[0],[0]
"We explore two methods of transforming pre-trained word embeddings into synset embeddings.
",4.4 Synset Embeddings,[0],[0]
Averaging.,4.4 Synset Embeddings,[0],[0]
"A straightforward way of using word embeddings to create synset embeddings is to collect the words representing the synset as surface form within the WordNet dataset and average their embeddings (Socher et al., 2013).",4.4 Synset Embeddings,[0],[0]
"We apply this method to pre-trained GloVe embeddings (Pennington et al., 2014) and pre-trained FastText embeddings (Bojanowski et al., 2017), averaging over the set of all wordforms in all lemmas for each synset, and performing a caseinsensitive query on the embedding dictionary.",4.4 Synset Embeddings,[0],[0]
"For example, the synset ‘determine.v.01’ lists the following lemmas: ‘determine’, ‘find’, ‘find out’, ‘ascertain’.",4.4 Synset Embeddings,[0],[0]
"Its vector is initialized as
1 5 (edetermine + 2 · efind + eout + eascertain).
",4.4 Synset Embeddings,[0],[0]
AutoExtend retrofitting + Mimick.,4.4 Synset Embeddings,[0],[0]
"AutoExtend is a method developed specifically for embedding WordNet synsets (Rothe and Schütze, 2015), in which pre-trained word embeddings are retrofitted to the tripartite relation graph connecting wordforms, lemmas, and synsets.",4.4 Synset Embeddings,[0],[0]
The resulting synset embeddings occupy the same space as the word embeddings.,4.4 Synset Embeddings,[0],[0]
"However, some WordNet senses are not represented in the underlying set of pre-trained word embeddings.5 To handle these cases, we trained a character-based model called MIMICK, which learns to predict embeddings for out-of-vocabulary items based on their spellings (Pinter et al., 2017).",4.4 Synset Embeddings,[0],[0]
"We do not modify the spelling conventions of WordNet synsets before passing them to Mimick, so e.g. ‘mask.n.02’ (the second synset corresponding to ‘mask’ as a noun) acts as the input character sequence as is.
",4.4 Synset Embeddings,[0],[0]
Random initialization.,4.4 Synset Embeddings,[0],[0]
"In preliminary experiments, we attempted training the association models using randomly-initialized embeddings.",4.4 Synset Embeddings,[0],[0]
These proved to be substantially weaker than distributionally-informed embeddings and we do not report their performance in the results section.,4.4 Synset Embeddings,[0],[0]
"We view this finding as strong evidence to support the necessity of a distributional signal in a typelevel semantic setup.
",4.4 Synset Embeddings,[0],[0]
5We use the out-of-the-box vectors supplied in http:// www.cis.lmu.de/˜sascha/AutoExtend.,4.4 Synset Embeddings,[0],[0]
"Following tuning experiments, we train the association models on synset embeddings with d = 300, using a negative log-likelihood loss function over 10 negative samples and iterating over symmetric relations once every five epochs.",4.5 Setup,[0],[0]
"We optimize the loss using AdaGrad with η = 0.01, and perform early stopping based on the development set mean reciprocal rank.",4.5 Setup,[0],[0]
M3GM is trained in four epochs using AdaGrad with η = 0.1.,4.5 Setup,[0],[0]
"We set M3GM’s rerank list size K = 100 and, following tuning, the regularization parameter λ = 0.01 and negative sample count per edge T = 10.",4.5 Setup,[0],[0]
"Our models are all implemented in DyNet (Neubig et al., 2017).",4.5 Setup,[0],[0]
Table 1 presents the results on the development set.,5 Results,[0],[0]
"Lines 1-3 depict the results for local models using averaged FastText embedding initialization, showing that the best performance in terms of MRR and top-rank hits is achieved by TRANSE.",5 Results,[0],[0]
"Mean Rank does not align with the other metrics; this is an interpretable tradeoff, as both BILIN and DISTMULT have an inherent preference for correlated synset embeddings, giving a stronger fallback for cases where the relation embedding is completely off, but allowing less freedom for separating strong cases from correlated false positives, compared to a translational objective.
",5 Results,[0],[0]
Effect of global score.,5 Results,[0],[0]
There is a clear advantage to re-ranking the top local candidates using the score signal from the M3GM model (line 4).,5 Results,[0],[0]
These results are further improved when the graph score is weighted against the association component per relation (line 5).,5 Results,[0],[0]
"We obtain similar improvements when re-ranking the predictions from DISTMULT and BILIN.
",5 Results,[0],[0]
"The M3GM training procedure is not useful in fine-tuning the association model via backpropagation: this degrades the association scores for true edges in the evaluation set, dragging the reranked results along with them to about a 2-point drop relative to the untuned variant.
",5 Results,[0],[0]
"Table 2 shows that our main results transfer onto the test set, with even a slightly larger margin.",5 Results,[0],[0]
"This could be the result of the greater edge density of the combined training and dev graphs, which enhance the global coherence of the graph structure captured by M3GM features.",5 Results,[0],[0]
"To support this theory, we tested the M3GM model trained on only the training set, and its test set performance was roughly one point worse on all metrics, as compared with the model trained on the training+dev data.
",5 Results,[0],[0]
Synset embedding initialization.,5 Results,[0],[0]
We trained association models initialized on AutoExtend+Mimick vectors (see Section 4.4).,5 Results,[0],[0]
"Their performance, inferior to averaged FastText vectors by about 1-2 MRR points on the dev set, is somewhat at odds with findings from previous experiments on WordNet (Guu et al., 2015).",5 Results,[0],[0]
"We believe the decisive factor in our result is the size of the training corpus used to create FastText embeddings, along with the increase in resulting vocabulary coverage.",5 Results,[0],[0]
"Out of 124,819 lemma tokens participating in 41,105 synsets, 118,051 had embeddings available (94.6%; type-level coverage 88.1%).",5 Results,[0],[0]
Only 530 synsets (1.3%) finished this initialization process with no embedding and were assigned random vectors.,5 Results,[0],[0]
"AutoExtend, fit for embeddings from Mikolov et al. (2013) which were trained on a smaller corpus, offers a weaker signal: 13,377 synsets (32%) had no vector and needed Mimick initialization.",5 Results,[0],[0]
"As a consequence of the empirical experiment, we aim to find out what M3GM has learned about WordNet.",6 Graph Analysis,[0],[0]
Table 3 presents a sample of topweighted motifs.,6 Graph Analysis,[0],[0]
"Lines 1 and 2 demonstrate that the model prefers a broad scattering of targets for the member meronym and has part relations6, which are flat and top-downwards hierarchical, respectively, while line 4 shows that a multitude of unique hypernyms is undesired, as expected from a bottom-upwards hierarchical relation.",6 Graph Analysis,[0],[0]
"Line 5 enforces the asymmetry of the hypernym relation.
",6 Graph Analysis,[0],[0]
"Lines 3, 6, and 7 hint at deeper interactions between the different relation types.",6 Graph Analysis,[0],[0]
"Line 3 shows that the model assigns positive weights to hypernyms which have derivationally-related forms, suggesting that the derivational equivalence classes in the graph tend to exist in the higher, more abstract levels of the hypernym hierarchy, as noted in Section 3.1.",6 Graph Analysis,[0],[0]
"Line 6 captures a semantic conflict: synsets located in the lower, specific levels of the graph can be specified either as instances of abstract concepts7, or as members of less specific concrete classes, but not as both.",6 Graph Analysis,[0],[0]
"Line 7 may have captured a nodal property – since part of is a relation which holds between nouns, and verb group holds between verbs, this negative weight assignment may be the manifestation of a part-of-speech uniqueness constraint.",6 Graph Analysis,[0],[0]
"In addition, in features 3 and 7 we see the importance of symmetric relations (here derivationally related form
6Example edges: ‘America’ → ‘American’, ‘face’ → ‘mouth’, respectively.
",6 Graph Analysis,[0],[0]
"7Example instance hypernym edge: ‘Rome’→ ‘national capital’.
and verb group, respectively), which manage to be represented in the graph model despite not being directly trained on.
",6 Graph Analysis,[0],[0]
Table 4 presents examples of relation targets successfully re-ranked thanks to these features.,6 Graph Analysis,[0],[0]
"The first false connection created a new unique hypernym, ‘garden lettuce’, downgraded by the graph score through incrementing the count of negatively-weighted feature 4.",6 Graph Analysis,[0],[0]
"In the second case, ‘vienna’ was brought from rank 10 to rank 1 since it incremented the count for the positivelyweighted feature 2, whereas all targets ranked above it by the local model were already has parts, mostly of ‘europe’.
",6 Graph Analysis,[0],[0]
"The αr values weighing the importance of M3GM scores in the overall function, found per relation through grid search over the development set, are presented in Table 5.",6 Graph Analysis,[0],[0]
"It appears that for all but two relations, the best-performing model preferred the signal from the graph features to that from the association model (αr > 0.5).",6 Graph Analysis,[0],[0]
"Based on the surface properties of the different relation graphs, the decisive factor seems to be that synset domain topic of and has part pertain mostly to very common concepts, offering good local signal from the synset embeddings, whereas the rest include many long-tail, low-frequency synsets that require help from global features to detect regularity.",6 Graph Analysis,[0],[0]
"This paper presents a novel method for reasoning about semantic graphs like WordNet, combining the distributional coherence between individual entity pairs with the structural coherence of network motifs.",7 Conclusion,[0],[0]
"Applied as a re-ranker, this method substantially improves performance on link prediction.",7 Conclusion,[0],[0]
"Our analysis of results from Table 3, lines 6 and 7, suggests that adding graph motifs which qualify their adjacent nodes in terms of syntactic function or semantic category may prove useful.
",7 Conclusion,[0],[0]
"From a broader perspective, M3GM can do more as a probabilistic model than predict individual edges.",7 Conclusion,[0],[0]
"For example, consider the problem of linking a new entity into a semantic graph, given only the vector embedding.",7 Conclusion,[0],[0]
"This task involves adding multiple edges simultaneously, while maintaining structural coherence.",7 Conclusion,[0],[0]
"Our model is capable of scoring bundles of new edges, and in future work, we plan to explore the possibility of combining M3GM with a search algorithm, to automatically extend existing knowledge graphs by linking in one or more new entities.
",7 Conclusion,[0],[0]
We also plan to explore multilingual applications.,7 Conclusion,[0],[0]
"To some extent, the structural parameters estimated by M3GM are not specific to English: for example, hypernymy cannot be symmetric in any language.",7 Conclusion,[0],[0]
"If the structural parameters estimated from English WordNet are transferable to other languages, then the combination of M3GM and multilingual word embeddings could facilitate the creation and extension of large-scale semantic resources across many languages (Fellbaum and Vossen, 2012; Bond and Foster, 2013; Lafourcade, 2007).",7 Conclusion,[0],[0]
We would like to thank the anonymous reviewers for their helpful comments.,Acknowledgments,[0],[0]
"We discussed fast motif-counting algorithms with Polo Chau and Oded Green, and received early feedback from Jordan Boyd-Graber, Erica Briscoe, Martin Hyatt, Bryan Leslie Lee, Martha Palmer, and Oren Tsur.",Acknowledgments,[0],[0]
This research was funded by the Defense Threat Research Agency under award HDTRA115-1-0019.,Acknowledgments,[0],[0]
"Semantic graphs, such as WordNet, are resources which curate natural language on two distinguishable layers.",abstractText,[0],[0]
"On the local level, individual relations between synsets (semantic building blocks) such as hypernymy and meronymy enhance our understanding of the words used to express their meanings.",abstractText,[0],[0]
"Globally, analysis of graph-theoretic properties of the entire net sheds light on the structure of human language as a whole.",abstractText,[0],[0]
"In this paper, we combine global and local properties of semantic graphs through the framework of Max-Margin Markov Graph Models (M3GM), a novel extension of Exponential Random Graph Model (ERGM) that scales to large multi-relational graphs.",abstractText,[0],[0]
"We demonstrate how such global modeling improves performance on the local task of predicting semantic relations between synsets, yielding new state-ofthe-art results on the WN18RR dataset, a challenging version of WordNet link prediction in which “easy” reciprocal cases are removed.",abstractText,[0],[0]
"In addition, the M3GM model identifies multirelational motifs that are characteristic of wellformed lexical semantic ontologies.",abstractText,[0],[0]
Predicting Semantic Relations using Global Graph Properties,title,[0],[0]
The problem of learning dynamics – where an agent learns a model of how its actions will affect its state and that of its environment – is a key open problem in robotics and reinforcement learning.,1. Introduction,[0],[0]
"An agent equipped with a dynamics model can leverage model-predictive control or modelbased reinforcement learning (RL) to perform a wide variety of tasks, whose exact nature need not be known in advance, and without additional access to the environment.
",1. Introduction,[0],[0]
"In contrast with model-free RL, which seeks to directly learn a policy (mapping from states to actions) in order to accomplish a specific task, learning dynamics has the advantage that dynamics models can be learned without taskspecific supervision.",1. Introduction,[0],[0]
"Since dynamics models are decoupled from any particular task, they can be reused across different
1University of California, Berkeley 2OpenAI.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Nikhil Mishra <nmishra@berkeley.edu>.
Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
tasks in the same environment.",1. Introduction,[0],[0]
"Additionally, learning differentiable dynamics models (such as those based on neural networks) enables the use of end-to-end backpropagationbased methods for policy and trajectory optimization that are much more efficient than model-free methods.
",1. Introduction,[0],[0]
"Typical approaches to dynamics learning build a one-step model of the dynamics, predicting the next state as a function of the current state and the current action.",1. Introduction,[0],[0]
"However, when chained successively for many timesteps into the future, the predictions from a one-step model tend to diverge from the true dynamics, either due to the accumulation of small errors or deviation from the regime represented by the data the model was trained on.",1. Introduction,[0],[0]
"Any learned dynamics model is only valid under the distribution of states and actions represented by its training data, and one-step models make no attempt to deal with the fact that they cannot make accurate predictions far outside this distribution.
",1. Introduction,[0],[0]
"When the true dynamics are stochastic, or the sensory measurements noisy or unreliable, these problems are only exacerbated.",1. Introduction,[0],[0]
"Moreover, the dynamics may be inherently difficult to learn: bifurcations such as collisions induce sharp changes in state that are hard to model with certainty when looking at a single timestep.",1. Introduction,[0],[0]
"There may also be hysteresis effects such as gear backlash in robots, or high-order dynamics in hydraulic robot actuators and human muscles that require looking at a history of past states.
",1. Introduction,[0],[0]
We present a novel approach to learning dynamics based on a deep generative model over temporal segments: we wish to model the distribution over possible future state trajectories conditioned on planned future actions and a history of past states and actions.,1. Introduction,[0],[0]
"By considering an entire segment of future states, our approach can model both uncertainty and complex interactions (like collisions) holistically over a segment, even if it makes small errors at individual timesteps.",1. Introduction,[0],[0]
"We also model a prior over action segments using a similar generative model, which can be used to ensure that the action distribution explored during planning is the same as the one the model was trained on.",1. Introduction,[0],[0]
"We show that that our method makes better predictions over long horizons than one-step models, is robust to stochastic dynamics and measurements, and can be used in a variety of control settings while only considering actions from the regime where the model is valid.",1. Introduction,[0],[0]
"A number of options are available for representation of learned dynamics models, including linear functions (Mordatch et al., 2016; Yip & Camarillo, 2014), Gaussian processes (Boedecker et al., 2014; Ko & Fox, 2009; Deisenroth & Rasmussen, 2011), predictive state representations (PSRs) (Littman et al., 2002; Rosencrantz et al., 2004), and deep neural networks (Punjani & Abbeel, 2015; Fragkiadaki et al., 2015; Agrawal et al., 2016).",2. Related Work,[0],[0]
"Linear functions are efficient to evaluate and solve controls for, but have limited expressive power.",2. Related Work,[0],[0]
"Gaussian processes (Williams & Rasmussen, 1996) provide uncertainty estimates, but scaling them to large datasets remains a challenge (Shen et al.; Lawrence et al., 2003).",2. Related Work,[0],[0]
"PSRs and variants make multi-step predictions, but suffer from the same scalability challenges as Gaussian processes.",2. Related Work,[0],[0]
"Our method combines the expressiveness and scalability of neural networks with the ability to provide sampling and uncertainty estimates, modeling entire segments to improve stability and robustness.
",2. Related Work,[0],[0]
"An alternative is to learn dynamics models in an online fashion, constantly adapting the model based on an incoming stream of observed states and actions (Fu et al., 2016; Mordatch et al., 2016; Yip & Camarillo, 2014; Lenz et al., 2015).",2. Related Work,[0],[0]
"However, such approaches are slow to adapt to rapidly-changing dynamics modes (such as those arising when making or breaking contact) and may be problematic when applied on robots performing rapid motions.
",2. Related Work,[0],[0]
Several approaches exist to improve the stability of models that make sequential predictions.,2. Related Work,[0],[0]
Abbeel & Ng (2004) and Venkatraman et al. (2015) consider alternative loss functions that improve robustness over long prediction horizons.,2. Related Work,[0],[0]
Bengio et al. (2015) and Venkatraman et al. (2016) also use simple curricula for a similar effect.,2. Related Work,[0],[0]
"While they all consider multi-step prediction losses, they only do so in the context of training models that are intrinsically one-step.
",2. Related Work,[0],[0]
"Existing methods for video prediction (Finn & Levine, 2016; Oh et al., 2015) look at a history of previous states and actions to predict the next frame; we take this a step further by modeling a distribution over an entire segment of future states that is also conditioned on future actions.",2. Related Work,[0],[0]
"In this work, we focus on demonstrating the benefits of a probabilistic segment-based approach; these methods could easily be incorporated with ours to learn dynamics from images, but we leave this to future work.
",2. Related Work,[0],[0]
Watter et al. (2015) and Johnson et al. (2016) use variational autoencoders to learn a low-dimensional latent-space representation of image observations.,2. Related Work,[0],[0]
"Finn et al. (2016) takes a similar approach, but without the variational aspect.",2. Related Work,[0],[0]
"These works utilized autoencoders as a means of dimensionality reduction (rather than for temporal coherence like we do) to enable the use of existing control algorithms based on locally-linear one-step dynamics models.
",2. Related Work,[0],[0]
"Temporally-extended actions were shown to be effective in reinforcement learning, such as the options framework (Sutton et al., 1999b) or sequencing of sub-plans (Vezhnevets et al., 2016).",2. Related Work,[0],[0]
Considering entire trajectories as opposed to single timesteps can also lead to simpler control policies.,2. Related Work,[0],[0]
"For example, there are effective and simple manually-designed control laws (Raibert, 1986), (Pratt et al., 2006) that formulate optimal actions as a function of the entire future trajectory rather than a single future state.",2. Related Work,[0],[0]
Suppose we have a non-linear dynamical system with states xt and actions ut.,3. Segment-Based Dynamics Model,[0],[0]
"The conventional approach to learning dynamics is to learn a function xt+1 = f(xt, ut) using an approximator such as a neural network (possibly recurrent).
",3. Segment-Based Dynamics Model,[0],[0]
"We consider a more general formulation of the problem, which is depicted in Figure 1: given segments (of lengthH) of past states X− = {xt−H , . . .",3. Segment-Based Dynamics Model,[0],[0]
", xt−1} and actions U− = {ut−H , . . .",3. Segment-Based Dynamics Model,[0],[0]
", ut−1}, we wish to predict the entire segment of future states X+ = {xt, . . .",3. Segment-Based Dynamics Model,[0],[0]
", xt+H} that would result from taking actions U+ = {ut, . . .",3. Segment-Based Dynamics Model,[0],[0]
", ut+H−1}.",3. Segment-Based Dynamics Model,[0],[0]
"Treating these four temporal segments as random variables, then we wish to learn the conditional distribution P (X+|X−, U−, U+).",3. Segment-Based Dynamics Model,[0],[0]
"We introduce dependency on past actions U− to support dynamics with delayed or filtered actions.
",3. Segment-Based Dynamics Model,[0],[0]
"With this in mind, we propose the use of a deep conditional variational autoencoder (Kingma & Welling, 2014): our encoder will learn the distribution Q(x)(Z|X+, X−, U−, U+) over latent codes Z, and our decoder will learn to reconstruct X+ from X−, U−, U+ and a sample from Z, modeling the distribution P (x)(X+|X−, U−, U+, Z).",3. Segment-Based Dynamics Model,[0],[0]
"Note that the random variable Z is a vector that describes an entire segment of states, X+.",3. Segment-Based Dynamics Model,[0],[0]
"After training is complete, we can discard the
encoder, and the decoder will allow us to predict the future state trajectory X̂+ using X−, U−, U+, as desired, sampling latent codes from an enforced prior P (Z) = N (0, I).",3. Segment-Based Dynamics Model,[0],[0]
"Empirically, we observe that having the encoder model Q(x)(Z|X+) instead of Q(x)(Z|X+, X−, U−, U+) gives equivalent performance, and so we take this approach in all of our experiments for simplicity.",3. Segment-Based Dynamics Model,[0],[0]
In the previous section we discussed a conditional variational autoencoder whose generative path serves as a stochastic dynamics model.,3.1. Model Architecture and Training,[0],[0]
Here we will expand on some of the architectural details.,3.1. Model Architecture and Training,[0],[0]
A diagram of the entire training setup is shown in Figure 2.,3.1. Model Architecture and Training,[0],[0]
"For more details of the architectures used in our experiments, see Appendix A.
The encoder network Q(x) explicitly parametrizes a Gaussian distribution over latent codes Z with diagonal covariance.",3.1. Model Architecture and Training,[0],[0]
"It consists of a stack of 1D-convolutional layers, whose output is flattened and projected into a single vector containing a mean µZ and variance σ2Z .",3.1. Model Architecture and Training,[0],[0]
We then sample z ∼ N,3.1. Model Architecture and Training,[0],[0]
"(µZ , σ2Z) in a differentiable manner using the reparametrization trick (Kingma & Welling, 2014).
",3.1. Model Architecture and Training,[0],[0]
The decoder network P (x) seeks to model a distribution over a segment of states P (X+),3.1. Model Architecture and Training,[0],[0]
"= P (xt, . . .",3.1. Model Architecture and Training,[0],[0]
", xt+H).",3.1. Model Architecture and Training,[0],[0]
"The causal nature of this segment (a particular timestep is only
affected by the ones that occur before it)",3.1. Model Architecture and Training,[0],[0]
"suggests that an autoregressive model with dilated convolutions is appropriate, similar to architectures previously used for modeling audio (van den Oord et al., 2016a) and image (van den Oord et al., 2016b) data.",3.1. Model Architecture and Training,[0],[0]
"Like these works, we use layers with the following activation function:
tanh(Wf,k ∗ s+ V Tf,kz) σ(Wg,k ∗",3.1. Model Architecture and Training,[0],[0]
"s+ V Tg,kz) (1)
where ∗ denotes convolution, denotes elementwise multiplication, σ(·) is the sigmoid function, s is the input to the layer, z is a latent code sampled from the output of the decoder, and W,V are network weights to be learned.",3.1. Model Architecture and Training,[0],[0]
"We found that residual layers and skip connections between layers give slightly better performance but are not essential.
",3.1. Model Architecture and Training,[0],[0]
"We train the model parameters end-to-end, minimizing the l2-loss between X+ and its reconstruction X̂+, along with the KL-divergence of the latent codeZ ∼ N (µZ , σ2Z) from N (0, I) similarly to Kingma & Welling (2014).",3.1. Model Architecture and Training,[0],[0]
"Once we have learned a dynamics model, we want to utilize it in order to accomplish different tasks, each of which can be expressed as reward function r(xt, ut).",4. Control with Segment-Based Models,[0],[0]
"Trajectory optimization and policy optimization are two settings where a dynamics model would commonly be used, and provide meaningful ways with which to evaluate a dynamics model.",4. Control with Segment-Based Models,[0],[0]
"In trajectory optimization, we wish to find a sequence of actions that can be applied to accomplish a particular instance of a task.",4.1. Trajectory Optimization,[0],[0]
"Specifically, given a reward function r, we want to maximize the sum of rewards along the trajectory that results from applying the actions u1, . . .",4.1. Trajectory Optimization,[0],[0]
", uT , beginning from an initial state x0.",4.1. Trajectory Optimization,[0],[0]
"This can be summarized by the following optimization problem:
max u1,...,uT E",4.1. Trajectory Optimization,[0],[0]
"[ T∑ t=1 r(xt, ut) ] with xt ∼ P (x)(xt|x0:t−1, u1:t)
(2)
where r(xt, ut) is the reward received at time t, and X = {x1, . . .",4.1. Trajectory Optimization,[0],[0]
", xT } is the sequence of states that would result from taking actions U = {u1, . . .",4.1. Trajectory Optimization,[0],[0]
", uT } from initial state x0, under dynamics model P (x).",4.1. Trajectory Optimization,[0],[0]
The expectation is taken over state trajectories sampled from the model.,4.1. Trajectory Optimization,[0],[0]
"If we attempt to solve the optimization problem as posed in (2), the solution will often attempt to apply action sequences outside the manifold where the dynamics model
is valid: these actions come from a very different distribution than the action distribution of the training data.",4.2. Latent Action Priors,[0],[0]
"This can be problematic: the optimization may find actions that achieve high rewards under the model (by exploiting it in a regime where it is invalid) but that do not accomplish the goal when they are executed in the real environment.
",4.2. Latent Action Priors,[0],[0]
"To mitigate this problem, we propose the use of another conditional variational autoencoder, this one over segments of actions.",4.2. Latent Action Priors,[0],[0]
"In particular, given sequences of past actions U− = {ut−H , . . .",4.2. Latent Action Priors,[0],[0]
", ut−1}, and future actions U+ = {ut, . . .",4.2. Latent Action Priors,[0],[0]
", ut+H}, we wish to model the the conditional distribution P (U+|U−).",4.2. Latent Action Priors,[0],[0]
"The encoder learnsQ(u)(Z|U+) and the decoder learns P (u)(U+|Z,U−).",4.2. Latent Action Priors,[0],[0]
We condition on U− to support temporal coherence in the generated action sequence.,4.2. Latent Action Priors,[0],[0]
"Like the dynamics model introduced in Section 3.1, the encoder uses 1D-convolutional layers, and the decoder is autoregressive, with dilated causal convolutions.",4.2. Latent Action Priors,[0],[0]
"The latent space that this autoencoder learns describes a prior over actions that can be used when planning with a dynamics model; hence we refer to this autoencoder over action sequences as a latent action prior.
",4.2. Latent Action Priors,[0],[0]
"To incorporate a latent action prior, we divide an action sequence U = {u1, . . .",4.2. Latent Action Priors,[0],[0]
", uT } into segments U1, . . .",4.2. Latent Action Priors,[0],[0]
"UK of length H (where K is determined such that T = HK, and U0 = 0).",4.2. Latent Action Priors,[0],[0]
"Then we can generate action sequences that are similar to the ones in our training set by sampling different latent codes z1, . . .",4.2. Latent Action Priors,[0],[0]
", zK and using the decoder to sample from P (u)(Uk|Uk−1, zk),∀k = 1, . . .",4.2. Latent Action Priors,[0],[0]
",K. The optimization problem posed in (2) can then be expressed as:
max z1,...,zK E",4.2. Latent Action Priors,[0],[0]
"[ T∑ t=1 r(xt, ut) ] with xt ∼ P (x)(xt|x0:t−1, u1:t)
",4.2. Latent Action Priors,[0],[0]
"ut ∼ P (u)(ut|u1:t−1, z1:K)
(3)
where the actions u1, . . .",4.2. Latent Action Priors,[0],[0]
", uT and states x1, . . .",4.2. Latent Action Priors,[0],[0]
", xT are generated by the latent action prior and dynamics model (see Figure 3 for an illustration).",4.2. Latent Action Priors,[0],[0]
"Since the dynamics model is differentiable, the above optimization problem can be solved end-to-end with backpropagation.",4.2. Latent Action Priors,[0],[0]
"While it is still nonconvex, we are optimizing over fewer variables, and the possible action sequences that are explored will be from the same distribution as the model’s training data.",4.2. Latent Action Priors,[0],[0]
"Moreover, the gradients of the rewards with respect to the latent codes are likely to have stronger signal than those with respect to a single action.",4.2. Latent Action Priors,[0],[0]
"We used Adam (Kingma & Ba, 2015) with step size 0.01 to perform this optimization and found that it generally converged in around 100 iterations.",4.2. Latent Action Priors,[0],[0]
"Trajectory optimization enables an agent to accomplish a single instance of a task, but more often, we are interested
in policy optimization, where the agent learns a policy that dictates optimal behavior in order to accomplish the task in the general case.",4.3. Policy Optimization,[0],[0]
"In particular, a policy is a learned function (with parameters θ) that defines a conditional distribution over actions given states, denoted πθ(u|x).",4.3. Policy Optimization,[0],[0]
"The value of a policy is defined as the expected sum of discounted rewards when acting under the policy, and can be expressed as:
η(θ) =",4.3. Policy Optimization,[0],[0]
"E [ ∞∑ t=1 γt · r(xt, ut) ]
(4)
where the actions are sampled from πθ(ut|xt), and γ is a discount factor.",4.3. Policy Optimization,[0],[0]
"The goal of policy optimization is to maximize the value of the policy with respect to its parameters.
",4.3. Policy Optimization,[0],[0]
"The class of algorithms known as policy gradient methods (Sutton et al., 1999a; Peters & Schaal, 2006) attempt to solve this optimization problem without considering a dynamics model.",4.3. Policy Optimization,[0],[0]
"They execute a policy πθ to get samples x1, u1, r1, . . .",4.3. Policy Optimization,[0],[0]
", xT , uT , rT from the environment, and then update θ to get an improved policy, relying on likelihood ratio methods (Williams, 1992) to estimate the gradient ∂η∂θ because they cannot directly compute the derivatives of the rewards r(xt, ut) with respect to the actions u1, . . .",4.3. Policy Optimization,[0],[0]
", ut.
Model-based policy optimization can be more efficient than traditional policy-gradient methods, because the gradient ∂η ∂θ can be computed directly by backpropagation through a differentiable model.",4.3. Policy Optimization,[0],[0]
"However, its success hinges on the
accuracy of the dynamics model, as the optimization can exploit flaws in the model in the same way as discussed in Section 4.2.",4.3. Policy Optimization,[0],[0]
Heess et al. (2015) use a model-based approach where a one-step dynamics model is learned jointly with a policy in an online manner.,4.3. Policy Optimization,[0],[0]
"To evaluate the robustness of our models, we experiment with learning policies offline, where the dynamics model is learned through unsupervised exploration of the environment, and no environment interaction is allowed beyond this exploration.
",4.3. Policy Optimization,[0],[0]
"Instead of a one-step policy of the form πθ(ut|xt), we also explored using a segment-based policy πθ(Z|X−, U−) that generates actions using latent action priorP (u) as follows:
X−, U− = {xt−H , . . .",4.3. Policy Optimization,[0],[0]
", xt−1}, {ut−H , . . .",4.3. Policy Optimization,[0],[0]
", ut−1} sample Z ∼ πθ(Z|X−, U−)
{ut, . . .",4.3. Policy Optimization,[0],[0]
", ut+H} = U+ ∼ P (u)(U+|Z,U−) and then acts according to action ut.",4.3. Policy Optimization,[0],[0]
The resulting policy will learn to accomplish the task while only considering actions for which the dynamics model is valid.,4.3. Policy Optimization,[0],[0]
"In terms of the options framework (Sutton et al., 1999b), we can think of this policy as considering a continuous spectrum of options, all of which are consistent with both past observed states and actions, and the data distribution under which the dynamics model makes good predictions.",4.3. Policy Optimization,[0],[0]
Our experiments investigate the following questions: (i) How well do segment-based models predict dynamics?,5. Experiments,[0],[0]
(ii) How does prediction accuracy transfer to control applications?,5. Experiments,[0],[0]
How does this scale with the difficulty of the task and stochasticity in the dynamics?,5. Experiments,[0],[0]
(iii) How is this affected by the use of latent action priors?,5. Experiments,[0],[0]
(iv) Is there any meaning or structure encoded by the latent space learned by the dynamics model?,5. Experiments,[0],[0]
"In order for a dynamics model to be versatile enough for use in control settings, the training data needs to contain a variety of actions that explore a diverse subset of the state space.",5.1. Environments,[0],[0]
Efficient exploration strategies are an open problem in reinforcement learning and are not the focus of this work.,5.1. Environments,[0],[0]
"With this in mind, we base our experiments on a simulated 2-DOF arm moving in a plane (as implemented in the Reacher environment in OpenAI Gym), because performing random actions in this environment results in sufficient exploration.",5.1. Environments,[0],[0]
We consider the following environments throughout our experiments (illustrated in Figure 4): (i),5.1. Environments,[0],[0]
"The basic, unmodified Reacher environment.",5.1. Environments,[0],[0]
"(ii) A version containing an obstacle that the arm can collide with: the obstacle cannot move, but its position is randomly chosen at the start of each episode.",5.1. Environments,[0],[0]
"(iii) A version in which the arm can push a damped cylin-
drical object around the arena.
",5.1. Environments,[0],[0]
"While the segment length and dimensionality of the latent space could be varied, we found that these values were reasonable choices for these environments.",5.1. Environments,[0],[0]
"As the segment length approaches 1, the model degenerates into a one-step model, and for longer segments, its performance plateaus because the states towards the end of the segment become independent of those at the beginning.",5.1. Environments,[0],[0]
"Likewise, we observed that this latent-space dimensionality was a good trade-off between expressiveness and information density.",5.1. Environments,[0],[0]
"We compare our method against the following baselines: (i) A one-step model: a learned function xt+1 = f(xt, ut), where f is a fully-connected neural network.",5.2. Baselines,[0],[0]
"It is trained using a one-step-prediction l2-loss on tuples (xt, ut, xt+1).",5.2. Baselines,[0],[0]
(ii) A one-step model that is rolled out several timesteps at training time.,5.2. Baselines,[0],[0]
"The model is still a learned function xt+1 = f(xt, ut), but it is trained with a multi-step prediction loss, over a horizon of length 2H .",5.2. Baselines,[0],[0]
"While this does not increase the model’s expressive power, we expect it to be more robust to the accumulation of small errors (e.g., Venkatraman et al. (2015); Abbeel & Ng (2004)).",5.2. Baselines,[0],[0]
"(iii) An LSTM model, which can store information about the past in a hidden state ht: xt+1, ht+1 = f(xt, ut, ht), and is trained with the same multi-step prediction loss (also over a horizon of 2H).",5.2. Baselines,[0],[0]
"We expect that the LSTM can learn fairly complex dynamics, but the hidden state dependencies can make trajectory and policy optimization more difficult.",5.2. Baselines,[0],[0]
"After learning a dynamics model, we evaluate it on a test set of held-out trajectories by computing the average loglikelihood of the test data under the model.
",5.3.1. DYNAMICS PREDICTION,[0],[0]
"For our method, we do this by obtaining samples from the model, fitting a Gaussian to the samples, and determining the log-likelihood of the true trajectory under the fitted Gaussian.",5.3.1. DYNAMICS PREDICTION,[0],[0]
"Since the baseline methods do not express uncertainty, but are trained using l2-loss, we interpret their predictions as the mean of a Gaussian distribution whose variance is constant across all state dimensions and timesteps (since minimizing l2-loss is equivalent to maximizing this log-likelihood).",5.3.1. DYNAMICS PREDICTION,[0],[0]
"We then fit the value of the variance constant to maximize the log-likelihood on the test set.
",5.3.1. DYNAMICS PREDICTION,[0],[0]
Figure 5 compares our method to the baselines in each environment.,5.3.1. DYNAMICS PREDICTION,[0],[0]
"The values reported are log-likelihoods per timestep, averaged over a test set of 1000 trajectories.",5.3.1. DYNAMICS PREDICTION,[0],[0]
"Our model and the LSTM are competitive in the basic environment (and both substantially better than the one-step models), but the LSTM’s performance degrades on more challenging environments with collisions.
",5.3.1. DYNAMICS PREDICTION,[0],[0]
"Basic Pushing Object With Obstacle
Environment
32.16
26.55
17.95
28.44
16.42
11.96
18.49
13.02 11.36
19.93
13.98 12.55
Ours LSTM One Step One Step, rolled out
Figure 5.",5.3.1. DYNAMICS PREDICTION,[0],[0]
Prediction quality of our method compared to several baselines in a range of environments.,5.3.1. DYNAMICS PREDICTION,[0],[0]
The reported values are the average log-likelihood per timestep on a test set (higher is better).,5.3.1. DYNAMICS PREDICTION,[0],[0]
"Our method significantly outperforms the baseline methods, even in environments with complex dynamics such as collisions.",5.3.1. DYNAMICS PREDICTION,[0],[0]
"Next, we compare our method to the baselines on trajectory and policy optimization.",5.3.2. CONTROL1,[0],[0]
"Of interest is both the actual reward achieved in the environment, and the difference between the true reward and the expected reward under the model.",5.3.2. CONTROL1,[0],[0]
"If a control algorithm exploits the model to predict unrealistic behavior, then the latter will be large.
",5.3.2. CONTROL1,[0],[0]
We consider two tasks: (i),5.3.2. CONTROL1,[0],[0]
Reaching Task: the arm must move its end effector to a desired position.,5.3.2. CONTROL1,[0],[0]
"The reward function is the negative distance between the end effector and the target position, minus a quadratic penalty on applying large torques.
1 Videos of our experimental results can be seen here: https://sites.google.com/site/temporalsegmentmodels/.
(ii) Pushing Task: the arm must push a cylindrical object to the desired position.",5.3.2. CONTROL1,[0],[0]
"Like in the reaching task, the reward function is the negative distance between the object and the target, again minus a penalty on large torques.
",5.3.2. CONTROL1,[0],[0]
The trajectory-optimization results are summarized in Figure 6.,5.3.2. CONTROL1,[0],[0]
"For each task and dynamics model, we sampled 100 target positions uniformly at random, solved the optimization problem as described in (2) or (3), and then executed the action sequences in the environment in open loop.",5.3.2. CONTROL1,[0],[0]
"Under each model, the optimization finds actions that achieve similar model-predicted rewards, but the baselines suffer from large discrepancies between model prediction and the true dynamics.",5.3.2. CONTROL1,[0],[0]
"Qualitatively, we notice that, on the pushing task, the optimization exploits the LSTM and onestep models to predict unrealistic state trajectories, such as the object moving without being touched or the arm passing through the object instead of colliding with it.",5.3.2. CONTROL1,[0],[0]
"Our model consistently performs better, and, with a latent action prior, the true execution closely matches the model’s prediction.",5.3.2. CONTROL1,[0],[0]
"When it makes inaccurate predictions, it respects physical invariants, such as objects staying still unless they are touched, or not penetrating each other when they collide.
",5.3.2. CONTROL1,[0],[0]
"Reaching Pushing
7.23 7.917.38
12.15
8.15
16.06
8.69
19.65
7.89
19.01
Negative Reward in Environment
Reaching Pushing
0.21 0.131.01
4.30 1.65
8.44
2.27
11.71
1.58
11.47
Discrepancy between Model and Environment
Ours, with latent prior Ours, without latent prior LSTM
One Step One Step, rolled out
Figure 6.",5.3.2. CONTROL1,[0],[0]
Trajectory optimization on the reaching and pushing tasks.,5.3.2. CONTROL1,[0],[0]
"The top plot reports the negative reward from open-loop execution of the returned action sequences (lower is better, averaged over 100 trials), and the bottom shows the difference between true reward and model-predicted reward.",5.3.2. CONTROL1,[0],[0]
"Our model, with a latent action prior, achieves both the best in-environment performance and the smallest discrepancy between environment and model.
",5.3.2. CONTROL1,[0],[0]
Figure 7 depicts the results from policy-optimization (Section 4.3) in the form of learning curves for each task and dynamics model.,5.3.2. CONTROL1,[0],[0]
See Appendix A for model architectures and hyperparameters.,5.3.2. CONTROL1,[0],[0]
"For comparison, we also plot the performance of a traditional policy gradient method.",5.3.2. CONTROL1,[0],[0]
"Although this method and ours eventually achieve similar performance, ours does so much more efficiently, learning the policy offline with fewer samples from the model than the traditional method needed from the environment.",5.3.2. CONTROL1,[0],[0]
"To explore the effects of stochastic dynamics and delayed actions, we consider two more modifications of the Reacher environment, one in which there is considerable Gaussian noise in the state observations (σ = 0.25 on data in the range [−1,+1]), and one in which actions are delayed: they do not take effect for τ = 5 timesteps after they are applied.",5.3.3. SENSORY NOISE AND DELAYED ACTIONS,[0],[0]
"These challenges commonly arise in realworld robotics applications (Atkeson et al., 2016), and so it is important to be able to learn a useful dynamics model in either setting.",5.3.3. SENSORY NOISE AND DELAYED ACTIONS,[0],[0]
"For both the noisy-state and delayedaction environments, we learn a dynamics model with each method, and then use it to learn a policy for the reaching task.",5.3.3. SENSORY NOISE AND DELAYED ACTIONS,[0],[0]
Figure 8 displays the resulting learning curves.,5.3.3. SENSORY NOISE AND DELAYED ACTIONS,[0],[0]
"Our dynamics model performs much better than the baselines, both with and without an action prior.",5.3.3. SENSORY NOISE AND DELAYED ACTIONS,[0],[0]
"Notably, using the LSTM model results in a substantially worse policy than ours even though its prediction accuracy is only slightly lower.",5.3.3. SENSORY NOISE AND DELAYED ACTIONS,[0],[0]
"Because our model operates over segments, it implicitly learns to filter noisy observations.",5.3.3. SENSORY NOISE AND DELAYED ACTIONS,[0],[0]
"This removes the need to explicitly apply and tune a filtering process, as is traditionally done.",5.3.3. SENSORY NOISE AND DELAYED ACTIONS,[0],[0]
"Variational autoencoders are known for learning lossy latent codes that preserve high-level semantics of the data, leaving the decoder to determine the low-level details.",5.3.4. ANALYSIS OF LATENT SPACE,[0],[0]
"As a result, we are curious to see whether our dynamics model learns a latent space that possesses similar properties.
",5.3.4. ANALYSIS OF LATENT SPACE,[0],[0]
"Applied to dynamics data, one might expect a latent code to provide an overall description of what happens in the state trajectory X+ it encodes.",5.3.4. ANALYSIS OF LATENT SPACE,[0],[0]
"Alternatively, per the argument made by Chen et al. (2016), it is also conceivable that the decoder would ignore the latent code entirely, because the segments X−, U−, U+ provide better information than Z about X+.",5.3.4. ANALYSIS OF LATENT SPACE,[0],[0]
"However, we observe that our model does learn a meaningful latent space: one that encodes uncertainty about the future.",5.3.4. ANALYSIS OF LATENT SPACE,[0],[0]
"A particular latent code corresponds to a particular future within the space of possible ones consistent with the given X−, U−, U+.
",5.3.4. ANALYSIS OF LATENT SPACE,[0],[0]
"When the dynamics are simple and deterministic (such as in the original Reacher environment), the model does express certainty by ignoring the latent code.",5.3.4. ANALYSIS OF LATENT SPACE,[0],[0]
"With stochas-
ticity (such as in the previous section), it provides a spread of reasonable state trajectories.",5.3.4. ANALYSIS OF LATENT SPACE,[0],[0]
"Interestingly, when the dynamics are deterministic but complex, the model also uses the latent codes to express uncertainty.",5.3.4. ANALYSIS OF LATENT SPACE,[0],[0]
"This can occur regarding the orientations and velocities of objects immediately following a collision, as illustrated in Figure 9.",5.3.4. ANALYSIS OF LATENT SPACE,[0],[0]
"Our earlier experiments demonstrated the benefits of a latent action prior: by only considering actions for which the dynamics model is valid, the discrepancy between the model and the true dynamics is minimized, resulting in higher rewards achieved in the actual environment.
",5.3.5. EFFECT OF LATENT ACTION PRIOR,[0],[0]
"In this section, we qualitatively examine how the actions returned by control algorithms differ as a consequence of the latent action prior.",5.3.5. EFFECT OF LATENT ACTION PRIOR,[0],[0]
An example is illustrated in Figure 10.,5.3.5. EFFECT OF LATENT ACTION PRIOR,[0],[0]
"In the training data, the actions that the agent takes are smooth, random torques, and we observe that when we use an action prior, solutions from trajectory optimization look similar.",5.3.5. EFFECT OF LATENT ACTION PRIOR,[0],[0]
"We contrast this with the solutions from optimizing directly over actions, which are sharp and discontinuous, unlike anything the dynamics model has seen before.",5.3.5. EFFECT OF LATENT ACTION PRIOR,[0],[0]
"This lets us infer that the baselines perform poorly on the pushing task (as shown in Figure 6) because of large discrepancies between the model prediction and the true execution.
0",5.3.5. EFFECT OF LATENT ACTION PRIOR,[0],[0]
"20 40 60 80 100
Timestep
Sample of Actions from Training Data",5.3.5. EFFECT OF LATENT ACTION PRIOR,[0],[0]
"We presented a novel approach to dynamics learning based on temporal segments, using a variational autoencoder to learn the distribution over future state trajectories conditioned on past states, past actions, and planned future actions.",6. Conclusion and Future Work,[0],[0]
"We also introduced the latent action prior, a variational autoencoder that models a prior over action segments, and showed how it can be used to perform control using actions from the same distribution as a dynamics model’s training data.",6. Conclusion and Future Work,[0],[0]
"Finally, through experiments involving trajectory optimization and model-based policy optimization, we showed that the resulting method can model complex phenomena such as collisions, is robust to sensory noise and action delays, and learns a meaningful latent space that expresses uncertainty about the future.
",6. Conclusion and Future Work,[0],[0]
"The most prominent direction for future work that we plan to explore, is the data collection procedure.",6. Conclusion and Future Work,[0],[0]
"In our experiments, correlated random actions resulted in sufficient exploration for the tasks we considered and allowed us to demonstrate the benefits of a segment-based approach.",6. Conclusion and Future Work,[0],[0]
"However, incorporating a more sophisticated exploration strategy to gather data (in an iterative procedure, potentially using the model’s predictions to guide exploration) would allow us to tackle a more diverse set of environments, both simulated and real-world.",6. Conclusion and Future Work,[0],[0]
The action prior and segmentbased policy could be used as a starting point for hierarchical reinforcement-learning algorithms.,6. Conclusion and Future Work,[0],[0]
Leveraging existing work on few-shot learning could help finetune a dynamics model during the policy learning process.,6. Conclusion and Future Work,[0],[0]
"Such approaches could yield significant advances in reinforcement learning, improving both sample efficiency and knowledge transfer between related tasks.",6. Conclusion and Future Work,[0],[0]
Work done at Berkeley was supported in part by an ONR PECASE award.,Acknowledgements,[0],[0]
We introduce a method for learning the dynamics of complex nonlinear systems based on deep generative models over temporal segments of states and actions.,abstractText,[0],[0]
"Unlike dynamics models that operate over individual discrete timesteps, we learn the distribution over future state trajectories conditioned on past state, past action, and planned future action trajectories, as well as a latent prior over action trajectories.",abstractText,[0],[0]
Our approach is based on convolutional autoregressive models and variational autoencoders.,abstractText,[0],[0]
"It makes stable and accurate predictions over long horizons for complex, stochastic systems, effectively expressing uncertainty and modeling the effects of collisions, sensory noise, and action delays.",abstractText,[0],[0]
"The learned dynamics model and action prior can be used for end-to-end, fully differentiable trajectory optimization and model-based policy optimization, which we use to evaluate the performance and sample-efficiency of our method.",abstractText,[0],[0]
Prediction and Control with Temporal Segment Models,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 193–199 New Orleans, Louisiana, June 1 - 6, 2018. c©2017 Association for Computational Linguistics",text,[0],[0]
"In the last decades, media and news business underwent a fundamental shift, from one-directional to bi-directional communication between users on the one side and journalists on the other.",1 Exploding Comment Threads,[0],[0]
"The use of social media, blogs, and the possibility to immediately share, like, and comment digital content transformed readers into active and powerful agents in the media business.",1 Exploding Comment Threads,[0],[0]
"This shift from passive “consumers” to active “agents” deeply impacts both media and communication science and has many positive aspects.
",1 Exploding Comment Threads,[0],[0]
"However, the possibilities and powers can also be misused.",1 Exploding Comment Threads,[0],[0]
"Pressure groups, lobbyists, trolls, and others are effectively trying to influence discussions according to their (very different) interests.",1 Exploding Comment Threads,[0],[0]
An easy approach consists in burying unwanted arguments or simply destroying a discussion by blowing it up.,1 Exploding Comment Threads,[0],[0]
"After such an attack, readers have to crawl through hundreds of nonsense and meaningless comments to extract meaningful and interesting arguments.",1 Exploding Comment Threads,[0],[0]
"Blowing up a thread can be
achieved by injecting provocative (but not necessarily off-topic) arguments into discussions.",1 Exploding Comment Threads,[0],[0]
"Bystanders are completing the goal of the destroyers, and they do so often unknowingly: with each — often well-intentioned — reaction to the provocation, they make it more difficult for others to follow the actual argumentation path and/or tree.
",1 Exploding Comment Threads,[0],[0]
"It is costly in terms of working power and time to keep the discussion area of a news site clean from attacks like that, and to watch the compliance of users (“netiquette”).",1 Exploding Comment Threads,[0],[0]
"As a reaction, many large online media sites worldwide closed their discussion areas or downsized them significantly (prominent examples of the last years are the Internet Movie Database, Bloomberg or the USAmerican National Public Radio).",1 Exploding Comment Threads,[0],[0]
"Other news provider and media sites, including us, take a different approach: A team of editors reads and filters comments on a 24/7-basis.",1 Exploding Comment Threads,[0],[0]
This results in a huge workload with several thousand reader comments published each day.,1 Exploding Comment Threads,[0],[0]
"In its lifetime, an article receives between less than ten and more than 1500 comments; typical are about 100 to 150 comments.",1 Exploding Comment Threads,[0],[0]
"The number of published comments
193
presumably depends to a large extent on time, weather, and season as well as for each article on subject, length, style of writing, and author, among others.
Being able to predict which articles will receive high comment volume would be beneficial at two positions in the newsroom:
1.",1 Exploding Comment Threads,[0],[0]
"for the news director to schedule the publication of news stories, and
2.",1 Exploding Comment Threads,[0],[0]
"for scheduling team sizes and guiding the focus of the comment moderators and editors.
",1 Exploding Comment Threads,[0],[0]
Figure 1 gives an overview of how comment volume prediction can be integrated into the workflow of a modern online news site.,1 Exploding Comment Threads,[0],[0]
The incoming news articles are ranked based on the estimated number of comments they will attract.,1 Exploding Comment Threads,[0],[0]
The news director takes these numbers into account in the decision process when to schedule which article for publication.,1 Exploding Comment Threads,[0],[0]
"This can balance the distribution of highly controversial topics across a day, giving not only readers and commenters the possibility to engage in each single one, but also distribute the moderation workload for comment editors evenly.",1 Exploding Comment Threads,[0],[0]
"Further, knowing which articles will receive many comments can help in the moderation process.",1 Exploding Comment Threads,[0],[0]
"Guiding the main focus of attention of moderators towards controversial topics not only facilitates efficient moderation, but also improves the quality of a comment thread.",1 Exploding Comment Threads,[0],[0]
"Our experience has shown that moderators entering the online discussion at an early stage can help keeping the discussion focused and fruitful.
",1 Exploding Comment Threads,[0],[0]
"In this paper, we study the task of identifying the weekly top 10% articles with the highest comment volume.",1 Exploding Comment Threads,[0],[0]
We consider a new real-world dataset of 7 million news comments collected over more than nine years.,1 Exploding Comment Threads,[0],[0]
"In order to enrich our dataset and increase its meaningfulness, we propose to transfer a classifier trained on the Englishlanguage Yahoo News Annotated Comments Corpus (Napoles et al., 2017b) to our Germanlanguage dataset and leverage the additional class labels for comments in a post-publication prediction scenario.",1 Exploding Comment Threads,[0],[0]
"Experiments show that our logistic regression model based on article metadata, linguistic, and topical features outperforms state-ofthe-art approaches significantly.",1 Exploding Comment Threads,[0],[0]
"Our contributions are summarized as (1) a transfer learning approach to learn early comments’ characteristics, (2) an analysis of a new 7-million-comment dataset and
(3) an improvement of F1-score by 81% compared to state-of-the-art in predicting most commented articles.",1 Exploding Comment Threads,[0],[0]
Related work on newsroom assistants focuses on comment volume prediction for pre-publication and post-publication scenarios.,2 Related Work,[0],[0]
"By the nature of news articles, the attention span after article publication is short and in practice post-publication prediction is valuable only within a short time frame.",2 Related Work,[0],[0]
Tsagkias et al. (2009) classify online newspaper articles using random forests.,2 Related Work,[0],[0]
"First, they classify whether an article will receive any comments at all.",2 Related Work,[0],[0]
"Second, they classify articles as receiving a high or low amount of comments.",2 Related Work,[0],[0]
The authors find that the second task is much harder and that predicting the actual number of comments is practically infeasible.,2 Related Work,[0],[0]
"Badari et al. (2012) conclude the same, analyzing Twitter activity as a popularity indicator for news:",2 Related Work,[0],[0]
Predicting popularity as a regression task results in large errors.,2 Related Work,[0],[0]
"Therefore, the authors predict classes of popularity by binning the absolute numbers (1-20, 20-100, 100-2400 received tweets).",2 Related Work,[0],[0]
"However, predicting the number of received tweets includes modeling both, the user behavior and the platform, which is problematic.",2 Related Work,[0],[0]
"It is part of a platform’s business secrets how content is internally ranked and distributed to users, making it hard to distinguish cause and effect from the outside.",2 Related Work,[0],[0]
"In our scenario, we even see no benefit in predicting the exact number of comments.",2 Related Work,[0],[0]
"Instead, we predict which articles belong to the weekly top 10% articles with the highest comment volume, which is one of the tasks defined by Tsagkias et al. (2009).
",2 Related Work,[0],[0]
"In a post-publication scenario, Tsagkias et al. (2010) consider the comments received within the first ten hours after article publication.",2 Related Work,[0],[0]
"Based on this feature, they propose a linear model to predict the final number of comments.",2 Related Work,[0],[0]
"Comparing comment behavior at eight online news platforms, they observe seasonal trends.",2 Related Work,[0],[0]
Tatar et al. (2011) consider the shorter time frame of five hours after article publication to predict article popularity.,2 Related Work,[0],[0]
They also use a linear model and find that neither adding publication time and article category to the feature set nor extending the dataset from three months to two years improves prediction results.,2 Related Work,[0],[0]
"Their survey on popularity prediction for web content summarizes features with good predictive capabilities
and lists fields of application for popularity prediction (Tatar et al., 2012).
",2 Related Work,[0],[0]
Rizos et al. (2016) focus on user comments to predict a discussion’s controversiality.,2 Related Work,[0],[0]
"They extract a comment tree and a user graph from the discussion and investigate for example comment count, number of users, and vote score.",2 Related Work,[0],[0]
"The demonstrated improvement of popularity prediction with this limited, focused features motivates us to further explore content-based features of comments in our work.
",2 Related Work,[0],[0]
"Recently, research on deep learning (Nobata et al., 2016; Pavlopoulos et al., 2017) addresses (semi-) automation of the entire moderation task, but we see several issues that prevent us from putting these approaches into practice.",2 Related Work,[0],[0]
"First, the accuracy of these methods is not high enough.",2 Related Work,[0],[0]
"For example, reported recall (0.79) and precision (0.77) at the task of abusive language detection (Nobata et al., 2016) are not sufficient for use in production.",2 Related Work,[0],[0]
"With this recall, an algorithm would let pass every fifth inappropriate comment (containing hate speech, derogatory statements, or profanity), which is not acceptable.",2 Related Work,[0],[0]
Pavlopoulos et al. (2017) address this problem by letting human moderators review comments that an algorithm could not classify with high confidence.,2 Related Work,[0],[0]
"Second, acceptance of these kind of black-box solutions is still limited in the community and the models lack comprehensibility.",2 Related Work,[0],[0]
"A compromise can be (ensemble) decision trees, because they achieve comparable results and can give reasons for their decisions (Kennedy et al., 2017).",2 Related Work,[0],[0]
"Still, moderators and users do not feel comfortable with machines deciding which comments are allowed to be published – not least because of fear of concealed censorship or bias.",2 Related Work,[0],[0]
"For each news article, we want to predict whether it belongs to the weekly top 10% articles with the highest comment volume.",3 Predicting High Comment Volume,[0],[0]
We chose this relative amount to account for seasonal fluctuations and also to even out periods with low news worthiness.,3 Predicting High Comment Volume,[0],[0]
"This traditional classification setting enables us to use established methods, such as logistic regression, to solve the task and provide explanations on why a particular article will receive many comments or not.
",3 Predicting High Comment Volume,[0],[0]
"As a baseline to compare against, we implemented a random forest model with features from
Tsagkias et al. (2009).",3 Predicting High Comment Volume,[0],[0]
For our approach we extend this feature set and categorize the features into five groups.,3 Predicting High Comment Volume,[0],[0]
"Our metadata features consist of article publication time, day of the week, and whether the article is promoted on our Facebook page.",3 Predicting High Comment Volume,[0],[0]
We consider temperature and humidity during the hour of publication1 and the number of “competing articles” as context features.,3 Predicting High Comment Volume,[0],[0]
Competing articles is the number of similar articles and the total number of articles published by our newspaper in the same hour.,3 Predicting High Comment Volume,[0],[0]
These articles compete for readers and user comments.,3 Predicting High Comment Volume,[0],[0]
Figure 2 visualizes how the number of received comments is not affected by the significantly higher number of published articles on Thursdays.,3 Predicting High Comment Volume,[0],[0]
The publication peek on Thursdays is caused by articles that are published in our weekly printed edition and at the same time published online one-to-one.,3 Predicting High Comment Volume,[0],[0]
"Further, we incorporate publisher information, such as genre, department, and which news agency served as a source for the article.",3 Predicting High Comment Volume,[0],[0]
"We include these features in order to study their impact and performance at comment volume prediction tasks and not in order to focus on engineering complex features.
",3 Predicting High Comment Volume,[0],[0]
"In addition, we propose to leverage the article content itself.",3 Predicting High Comment Volume,[0],[0]
"Starting with headline features, we use ngrams of length one to three as well as author provided keywords for the article.",3 Predicting High Comment Volume,[0],[0]
"To capture topical information in the body, we rely on topic modeling and document embedding besides traditional bag-of-word (BOW) features.",3 Predicting High Comment Volume,[0],[0]
These guarantee that we also grasp some semantic representations of the articles.,3 Predicting High Comment Volume,[0],[0]
"To this end, topic distributions, document embeddings, and word n-grams serve as semantic representa-
1as obtained for three large German cities, Berlin, Hamburg, and Frankfurt from http://www.dwd.de/
tions of articles.",3 Predicting High Comment Volume,[0],[0]
"In order to model topics of news article bodies, we apply standard latent Dirichlet allocation (Blei et al., 2003).",3 Predicting High Comment Volume,[0],[0]
"For the document embedding, we use a Doc2Vec implementation that downsamples higher-frequency words for the composition (Mikolov et al., 2013).",3 Predicting High Comment Volume,[0],[0]
"We choose the vector length, number of topics, and window size based on F1-score evaluation on a validation set.
",3 Predicting High Comment Volume,[0],[0]
"Despite recent advances of deep neural networks for natural language processing, there is a reason to focus on other models: For the application in newsrooms and the integration in semiautomatic processes, comprehensibility of the prediction results is very important.",3 Predicting High Comment Volume,[0],[0]
A black-box model — even if it achieved better performance — is not helpful in this scenario.,3 Predicting High Comment Volume,[0],[0]
Human moderators need to understand why the number of comments is predicted to be high or low.,3 Predicting High Comment Volume,[0],[0]
"This comprehensibility issue justifies the application of decision trees and regression models, which allow to trace back predictions to their decisive factors.",3 Predicting High Comment Volume,[0],[0]
"Table 1 lists precision, recall, and F1-score for the prediction of weekly top 10% articles with the highest comment volume.",3 Predicting High Comment Volume,[0],[0]
"Especially the bag-of-words (BOW) and the topics of the article body, but also headline keywords and publisher metadata achieve
higher F1-score than the metadata features.",3 Predicting High Comment Volume,[0],[0]
"The highest precision is achieved with the binary feature whether an article is promoted on Facebook, whereas author and competing articles achieve the highest recall.",3 Predicting High Comment Volume,[0],[0]
Whether the first comment is a provocative question in disagreement with the article or an offtopic statement influences the route of further conversation.,3.1 Automatic Translation of Comments,[0],[0]
"We assume that this assumption holds not only for social networks (Berry and Taylor, 2017), but also for comment sections at news websites.",3.1 Automatic Translation of Comments,[0],[0]
"Therefore, we consider the tone and sentiment of the first comments received shortly after article publication as an additional feature.",3.1 Automatic Translation of Comments,[0],[0]
Typical layouts of news websites (including ours) list comments in chronological order and show only the first few comments to readers below an article.,3.1 Automatic Translation of Comments,[0],[0]
Pagination hides later received comments and most users do not click through dozens of pages to read through all comments.,3.1 Automatic Translation of Comments,[0],[0]
"As a consequence, early comments attract a lot more attention and, with their tone and sentiment, influence comment volume to a larger extent.",3.1 Automatic Translation of Comments,[0],[0]
"Presumably, articles that receive controversial comments in the first few minutes after publication are more likely to receive a high number of comments in total.
",3.1 Automatic Translation of Comments,[0],[0]
"To classify comments as controversial or engaging, we need to train a supervised classification algorithm, which takes thousands of annotated comments.",3.1 Automatic Translation of Comments,[0],[0]
"Such training corpora exist, if at all, mostly for English comments, while our comments are written in German.",3.1 Automatic Translation of Comments,[0],[0]
We propose to apply machine translation to overcome this language barrier:,3.1 Automatic Translation of Comments,[0],[0]
"Given a German comment, we automatically translate it into English.",3.1 Automatic Translation of Comments,[0],[0]
"From a classifier that has been trained on an annotated English dataset, we can derive automatic annotations for the translated comment.",3.1 Automatic Translation of Comments,[0],[0]
"The derived annotations serve as another feature for our actual task of comment volume prediction.
",3.1 Automatic Translation of Comments,[0],[0]
We reimplemented the classifier by Napoles et al. (2017a) and train on their English dataset.,3.1 Automatic Translation of Comments,[0],[0]
"The considered annotations consist of 12 binary labels: addressed audience (reply to a particular user or broadcast message to a general audience), agreement/disagreement with previous comment, informative, mean, controversial, persuasive, off-topic regarding the corresponding news article, neutral, positive, negative, and mixed sentiment.",3.1 Automatic Translation of Comments,[0],[0]
"We au-
tomatically translate all comments in our German dataset into English using the DeepL translation service2.",3.1 Automatic Translation of Comments,[0],[0]
"For the translated comments, we automatically generate annotations based on Napoles et al.’s classifier.",3.1 Automatic Translation of Comments,[0],[0]
"Thereby, we transfer the knowledge that the classifier learned on English training data to our German dataset despite its different language.",3.1 Automatic Translation of Comments,[0],[0]
"This approach builds on the similar content style of both corpora, which is described in the next section.",3.1 Automatic Translation of Comments,[0],[0]
We consider two datasets that both contain user comments received by news articles with similar topics.,4 Dataset,[0],[0]
"First, our German 7-million-comment dataset, which we call Zeit Online Comment Corpus (ZOCC)3 and second, the English 10kcomment Yahoo News Annotated Comments Corpus (YNACC) (Napoles et al., 2017b).",4 Dataset,[0],[0]
"ZOCC consists of roughly 200,000 online news articles published between 2008 and 2017 and 7 million associated user comments in German.",4 Dataset,[0],[0]
"Out of 174,699 users in total, 60% posted more than one comment, 23% more than 10 comments and 7% more than 100 comments.",4 Dataset,[0],[0]
"For both, articles and comments, extensive metadata is available, such as author list, department, publication date, and tags (for articles) and user name, parent comment (if posted in response), and number of recommendations by other users (for comments).",4 Dataset,[0],[0]
"Not surprisingly, ZOCC is following a popularity growth with an increasing number of articles and comments over time.",4 Dataset,[0],[0]
"While our newspaper published roughly 1,300 articles per month in 2010 and each article received roughly 20 comments on average, we nowadays publish roughly 1,500 articles per month, each receiving 110 comments on average.",4 Dataset,[0],[0]
"As both corpora’s articles and comments cover a similar time span of several years and many different departments, they deal with a broad range of topics.",4 Dataset,[0],[0]
"While the majority of articles in YNACC is
2https://deepl.com 3http://www.zeit.de/
about economy, ZOCC’s major department is politics.",4 Dataset,[0],[0]
"More than 50% of the comments in ZOCC are posted in response to articles in the politics department, whereas in YNACC culture, society, and economy share an almost equal amount of around 20% each and politics on forth rank with 12%.",4 Dataset,[0],[0]
"On average, an article in ZOCC receives 90% of its comments within 48 hours, while it takes 61 hours for an article in YNACC.",4 Dataset,[0],[0]
"Despite their slight differences, both corpora cover most popular departments, which motivates the idea to transfer a classifier trained on YNACC to ZOCC.",4 Dataset,[0],[0]
"For YNACC, Napoles et al. propose a machine learning approach to automatically identify engaging, respectful, and informative conversations (2017a).",4 Dataset,[0],[0]
"By identifying weekly top 10% articles with the highest comment volume, we focus on a different task.",4 Dataset,[0],[0]
"Nonetheless, both corpora, ZOCC and YNACC, have similar properties: both corpora contain user comments posted in reaction to news articles across similar time span and similar topics.",4 Dataset,[0],[0]
"However, only the much smaller YNACC provides detailed annotations regarding, for example, comments’ tone and sentiment.",4 Dataset,[0],[0]
"We compare to the approach by Tsagkias et al. and evaluate on the same task (Tsagkias et al., 2009, 2010).",5 Evaluation,[0],[0]
"Therefore, we consider a binary classification task, which is to identify the weekly top 10% articles with the largest comment volume.",5 Evaluation,[0],[0]
Table 3 lists our final evaluation results on the hold-out test set.,5 Evaluation,[0],[0]
"We choose F1-score as our evaluation metric, since precision and recall are equally relevant in our scenario.",5 Evaluation,[0],[0]
"On the one hand, we want to achieve high recall so that no important article and its discussion is overlooked.",5 Evaluation,[0],[0]
"On the other hand, we have limited resources and cannot afford to moderate each and every discussion.",5 Evaluation,[0],[0]
A high precision is crucial so that our moderators focus only on articles that need their attention.,5 Evaluation,[0],[0]
"All experiments are conducted using time-wise split with years 2014 to 2016 for training, January 2017 to March 2017 for validation, and April 2017 for testing.",5 Evaluation,[0],[0]
"We find that our additional article and metadata features, but also the automatically annotated first comments outperform the baseline.",5 Evaluation,[0],[0]
"Due to the diversity of the different features, their combination further improves the prediction results.",5 Evaluation,[0],[0]
"In comparison to the approach by Tsagkias et al., we finally achieve an 81% larger F1-score.",5 Evaluation,[0],[0]
"With another experiment, we study the classification error introduced by translation.",5.1 Automatically Translated Comments,[0],[0]
"Therefore, we train two classifiers with the approach by Napoles et al.:",5.1 Automatically Translated Comments,[0],[0]
"First, we train and test a classifier on the original, English YNACC.",5.1 Automatically Translated Comments,[0],[0]
"Second, we automatically translate all comments in YNACC from English into German and use this translated data for training and testing of the second classifier.",5.1 Automatically Translated Comments,[0],[0]
"Comparing these two classifiers, we find that both precision and recall slightly decrease after translation, as shown in Table 4.",5.1 Automatically Translated Comments,[0],[0]
"Based on this result, we can assume that the translation of German comments into English introduces only a small error.",5.1 Automatically Translated Comments,[0],[0]
"Although YNACC and ZOCC differ in language, we can transfer a classifier that has been trained on YNACC to ZOCC.",5.1 Automatically Translated Comments,[0],[0]
"For each article, we use the labels assigned to the first four comments, which are visible on the first comment page below an article.",5.1 Automatically Translated Comments,[0],[0]
The first four comments are typically received within very few minutes after article publication.,5.1 Automatically Translated Comments,[0],[0]
"As a baseline feature for comparison, we use the number of comments4 received in a short time span after article publication.",5.2 Number of Early Comments,[0],[0]
"Annotated first page comments, but also article and metadata features significantly outperform the baseline until 32 minutes after article publication.",5.2 Number of Early Comments,[0],[0]
"After 32 minutes, the number of received comments outperforms every single feature (but not the combination of all our features).",5.2 Number of Early Comments,[0],[0]
This is because the difference between final number of comments and so far received comments converges over time.,5.2 Number of Early Comments,[0],[0]
"In this paper, we studied the task of predicting the weekly top 10% articles with the highest comment volume.",6 Conclusions,[0],[0]
This prediction helps to schedule the publication of news stories and supports moderation teams in focusing on article discussions that require most likely their attention.,6 Conclusions,[0],[0]
"Our supervised classification approach is based on a combination of metadata and content-based features, such as article body and topics.",6 Conclusions,[0],[0]
"Further, we automatically translate German comments into English to make use of a classifier pre-trained on English data: We classify the tone and sentiment of comments received in the first minutes after article publication, which improves prediction even further.",6 Conclusions,[0],[0]
On a 7-million-comment real-world dataset our approach outperforms the current state-of-theart by over 81% larger F1-score.,6 Conclusions,[0],[0]
"We hope that our prediction will help to reduce the number of cases where newspapers have no other choice but to close down a discussion section because of limited moderation resources.
",6 Conclusions,[0],[0]
"4To allow for non-linear correlations, we pass the number of comments as an absolute count and a squared count.",6 Conclusions,[0],[0]
The overwhelming success of the Web and mobile technologies has enabled millions to share their opinions publicly at any time.,abstractText,[0],[0]
But the same success also endangers this freedom of speech due to closing down of participatory sites misused by individuals or interest groups.,abstractText,[0],[0]
We propose to support manual moderation by proactively drawing the attention of our moderators to article discussions that most likely need their intervention.,abstractText,[0],[0]
"To this end, we predict which articles will receive a high number of comments.",abstractText,[0],[0]
"In contrast to existing work, we enrich the article with metadata, extract semantic and linguistic features, and exploit annotated data from a foreign language corpus.",abstractText,[0],[0]
Our logistic regression model improves F1-scores by over 80% in comparison to state-of-the-art approaches.,abstractText,[0],[0]
"1 Exploding Comment Threads In the last decades, media and news business underwent a fundamental shift, from one-directional to bi-directional communication between users on the one side and journalists on the other.",abstractText,[0],[0]
"The use of social media, blogs, and the possibility to immediately share, like, and comment digital content transformed readers into active and powerful agents in the media business.",abstractText,[0],[0]
This shift from passive “consumers” to active “agents” deeply impacts both media and communication science and has many positive aspects.,abstractText,[0],[0]
"However, the possibilities and powers can also be misused.",abstractText,[0],[0]
"Pressure groups, lobbyists, trolls, and others are effectively trying to influence discussions according to their (very different) interests.",abstractText,[0],[0]
An easy approach consists in burying unwanted arguments or simply destroying a discussion by blowing it up.,abstractText,[0],[0]
"After such an attack, readers have to crawl through hundreds of nonsense and meaningless comments to extract meaningful and interesting arguments.",abstractText,[0],[0]
Blowing up a thread can be 1.,abstractText,[0],[0]
Prediction for the Newsroom: Which Articles Will Get the Most Comments?,title,[0],[0]
Shape constraints like monotonicity and convexity arise naturally in many real-world regression and classification tasks.,1. Introduction,[0],[0]
"For example, holding all other variables fixed, a practitioner might assume that the price of a house is a decreasing function of neighborhood crime rate, that an individual’s utility function is concave in income level, or that phenotypes such as height or the likelihood of contracting a disease are monotonic in certain genetic effects.
",1. Introduction,[0],[0]
Parametric models like linear regression implicity impose monotonicity constraints at the cost of strong assumptions on the true underlying function.,1. Introduction,[0],[0]
"On the other hand, nonparametric techniques like kernel regression impose weak assumptions, but do not guarantee monotonicity or convexity in their predictions.",1. Introduction,[0],[0]
"Shape-constrained nonparametric regression methods attempt to offer the best of both worlds, allowing practitioners to dispense with parametric assumptions while retaining many of their appealing properties.
",1. Introduction,[0],[0]
"However, classical approaches to nonparametric regression under shape constraints suffer from the curse of dimensionality (Han & Wellner, 2016; Han et al., 2017).",1. Introduction,[0],[0]
"Some methods have been developed to mitigate this issue under assumptions like additivity, where the true function f is assumed to have the form f(x) = ∑ j fj(xj) + c, where a subset of the component fj’s are shapeconstrained (Chen & Samworth, 2016; Pya & Wood, 2015; Xu et al., 2016).",1. Introduction,[0],[0]
"But in many real-world settings, the lack of interaction terms among the predictors can be too restrictive.
",1. Introduction,[0],[0]
"Approaches from the machine learning community like random forests, gradient boosted trees, and deep learning methods have been shown to exhibit outstanding empirical performance on highdimensional tasks.",1. Introduction,[0],[0]
"But these methods do not guarantee monotonicity or convexity.
∗Department of Statistics, The University of Chicago †Department of Statistics, University of Illinois at Urbana-Champaign ‡Department of Statistics and Data Science, Yale University
ar X
iv :1
80 5.
",1. Introduction,[0],[0]
"06 43
9v 1
[ st
at .M
L ]
1 6
M ay
2 01
8
In this paper, we propose two methods for high-dimensional shape-constrained regression and classification.",1. Introduction,[0],[0]
"These methods blend the performance of machine learning methods with the classical least-squares approach to nonparametric shape-constrained regression.
",1. Introduction,[0],[0]
"In Section (2.1), we describe black box reshaping, which takes any pre-trained prediction rule and reshapes it on a set of test inputs to enforce shape constraints.",1. Introduction,[0],[0]
"In the case of monotonicity constraints, we develop an efficient algorithm to compute the estimator.",1. Introduction,[0],[0]
"Section (2.2) presents a second method designed specifically to reshape random forests (Breiman, 2001).",1. Introduction,[0],[0]
This approach reshapes each individual decision tree based on its split rules and estimated leaf values.,1. Introduction,[0],[0]
"Again, in the case of monotonicity constraints, we present another efficient reshaping algorithm.",1. Introduction,[0],[0]
We apply our methods to four datasets in Section (3) and show that they enforce the pre-specified shape constraints without sacrificing accuracy.,1. Introduction,[0],[0]
"In the context of monotonicity constraints, the black box reshaping method is related to the method of rearrangements (Chernozhukov et al., 2009, 2010).",1.1. Related Work,[0],[0]
The rearrangement operation takes a pretrained prediction rule and sorts its predictions to enforce monotonicity.,1.1. Related Work,[0],[0]
"In higher dimensions, the rearranged estimator is the average of one-dimensional rearrangements.",1.1. Related Work,[0],[0]
"In contrast, this paper focuses on isotonization of prediction values, jointly reshaping multiple dimensions in tandem.",1.1. Related Work,[0],[0]
"It would be interesting to explore adaptive procedures that average rearranged and isotonized predictions in future work.
",1.1. Related Work,[0],[0]
Monotonic decision trees have previously been studied in the context of classification.,1.1. Related Work,[0],[0]
"Several methods require that the training data satisfy monotonicity constraints (Makino et al., 1996; Potharst & Feelders, 2002), a relatively strong assumption in the presence of noise.",1.1. Related Work,[0],[0]
"The methods we propose here do not place any restrictions on the training data.
",1.1. Related Work,[0],[0]
"Another class of methods augment the score function for each split to incorporate the degree of non-monotonicity introduced by that split (Ben-David, 1995; González et al., 2015).",1.1. Related Work,[0],[0]
"However, this approach does not guarantee monotonicity.",1.1. Related Work,[0],[0]
Feelders & Pardoel (2003) apply pruning algorithms to non-monotonic trees as a post-processing step in order to enforce monotonicity.,1.1. Related Work,[0],[0]
"For a comprehensive survey of estimating monotonic functions, see Gupta et al. (2016) .
",1.1. Related Work,[0],[0]
"A line of recent work has led to a method for learning deep monotonic models by alternating different types of monotone layers (You et al., 2017).",1.1. Related Work,[0],[0]
"Amos et al. (2017) propose a method for fitting neural networks whose predictions are convex with respect to a subset of predictors.
",1.1. Related Work,[0],[0]
Our methods differ from this work in several ways.,1.1. Related Work,[0],[0]
"First, our techniques can be used to enforce both monotonic and convex/concave relationships.",1.1. Related Work,[0],[0]
"Unlike pruning methods, neither approach presented here changes the structure of the original tree.",1.1. Related Work,[0],[0]
"Black box reshaping, described in Section (2.1), can be applied to any pre-trained prediction rule, giving practitioners the flexibility of picking the method of their choice.",1.1. Related Work,[0],[0]
And both methods guarantee that the intended shape constraints are satisfied on test data.,1.1. Related Work,[0],[0]
"In what follows, we say that a function f : Rd → R is monotone with respect to variables R ⊆",2. Prediction Rule Reshaping,[0],[0]
"[d] = {1, . . .",2. Prediction Rule Reshaping,[0],[0]
", d} if f(x) ≤ f(y) when xi ≤ yi for i ∈ R, and xi = yi otherwise.
",2. Prediction Rule Reshaping,[0],[0]
"Similarly, a function f is convex in R if for all x, y ∈ Rd and α ∈",2. Prediction Rule Reshaping,[0],[0]
"[0, 1], f(αx + (1 − α)y) ≤ αf(x) + (1− α)f(y) when xi = yi ∀i /∈",2. Prediction Rule Reshaping,[0],[0]
R.,2. Prediction Rule Reshaping,[0],[0]
Let f̂ : Rd → R denote an arbitrary prediction rule fit on a training set and assume we have a candidate set of shape constraints with respect to variablesR ⊆,2.1. Black Box Reshaping,[0],[0]
[d].,2.1. Black Box Reshaping,[0],[0]
"For example, we might require that the function be monotone increasing in each variable v ∈ R.
Let F denote the class of functions that satisfy the desired shape constraints on each predictor variable v ∈ R.",2.1. Black Box Reshaping,[0],[0]
"We aim to find a function f ∗ ∈ F that is close to f̂ in the L2 norm:
f ∗ = arg min f∈F ‖f",2.1. Black Box Reshaping,[0],[0]
"− f̂‖2 (2.1)
where the L2 norm is with respect to the uniform measure on a compact set containing the data.",2.1. Black Box Reshaping,[0],[0]
"We simplify this infinite-dimensional problem by only considering values of f̂ on certain fixed test points.
",2.1. Black Box Reshaping,[0],[0]
"Suppose we take a sequence t1, t2, . . .",2.1. Black Box Reshaping,[0],[0]
", tn of test points, each in Rd, that differ only in their v-th coordinate so that tik = t i′
k for all k 6=",2.1. Black Box Reshaping,[0],[0]
"v. These points can be ordered by their v-th coordinate, allowing us to consider shape constraints on the vector (f(t1), f(t2), ..., f(tn)) ∈",2.1. Black Box Reshaping,[0],[0]
Rn.,2.1. Black Box Reshaping,[0],[0]
"For instance, under a monotone-increasing constraint with respect to v, if t1v ≤ t2v ≤ · · · ≤ tnv , then we consider functions f such that (f(t1), f(t2), ..., f(tn)) is a monotone sequence.
",2.1. Black Box Reshaping,[0],[0]
"There is now the question of choosing a test point t as well as a sequence of values t1v, ..., t n v to plug into its v-th coordinate.",2.1. Black Box Reshaping,[0],[0]
"A natural choice is to use the observed data values as both test points and coordinate values.
",2.1. Black Box Reshaping,[0],[0]
"Denote Dn = {(x1, y1), . . .",2.1. Black Box Reshaping,[0],[0]
", (xn, yn)} as a set of observed values where yi is the response and xi ∈ Rd are the predictors.",2.1. Black Box Reshaping,[0],[0]
"From each xi, we construct a sequence of test points that can be ordered according to their v-th coordinate in the following way.",2.1. Black Box Reshaping,[0],[0]
"Let xi,k,v denote the observed vector xi with its v-th coordinate replaced by the v-th coordinate of xk, so that
xi,k,v = (xi1, x i 2, . . .",2.1. Black Box Reshaping,[0],[0]
", x i v−1, x k v , x i v+1, . . .",2.1. Black Box Reshaping,[0],[0]
", x i d).",2.1. Black Box Reshaping,[0],[0]
"(2.2)
",2.1. Black Box Reshaping,[0],[0]
"This process yields n points from xi that can be ordered by their v-th coordinate, xi,1,v, xi,2,v, . . .",2.1. Black Box Reshaping,[0],[0]
", xi,n,v. We then require (f(xi,1,v), f(xi,2,v), . . .",2.1. Black Box Reshaping,[0],[0]
", f(xi,n,v)) ∈",2.1. Black Box Reshaping,[0],[0]
"Sv where Sv ⊂ Rd is the appropriate convex cone that enforces the shape constraint for variable v ∈ R, for example the cone of monotone increasing or convex sequences.
",2.1. Black Box Reshaping,[0],[0]
"To summarize, for each coordinate v ∈ R and for each i ∈",2.1. Black Box Reshaping,[0],[0]
"[n], we:
1.",2.1. Black Box Reshaping,[0],[0]
Take the i-th observed data point xi as a test point.,2.1. Black Box Reshaping,[0],[0]
"2. Replace its v-th coordinate with the n observed v-th coordinates x1v, ...x n",2.1. Black Box Reshaping,[0],[0]
"v to produce
xi,1,v, xi,2,v, . . .",2.1. Black Box Reshaping,[0],[0]
", xi,n,v. 3.",2.1. Black Box Reshaping,[0],[0]
"Enforce the appropriate shape constraint on the vector of evaluated function values,
(f(xi,1,v), f(xi,2,v), . . .",2.1. Black Box Reshaping,[0],[0]
", f(xi,n,v)) ∈",2.1. Black Box Reshaping,[0],[0]
"Sv.
",2.1. Black Box Reshaping,[0],[0]
See Figure (1) for an illustration.,2.1. Black Box Reshaping,[0],[0]
"This leads to the following relaxation of (2.1):
f ∗ = arg min f∈Fn ‖f − f̂‖2 (2.3)
where Fn is the class of functions f such that (f(xi,1,v), f(xi,2,v), . . .",2.1. Black Box Reshaping,[0],[0]
", f(xi,n,v))",2.1. Black Box Reshaping,[0],[0]
∈,2.1. Black Box Reshaping,[0],[0]
Sv ⊂,2.1. Black Box Reshaping,[0],[0]
Rn for each v ∈ R and each,2.1. Black Box Reshaping,[0],[0]
i ∈,2.1. Black Box Reshaping,[0],[0]
[n].,2.1. Black Box Reshaping,[0],[0]
"In other words, we have relaxed the shape constraints on the function f , requiring the constraints to hold relative to the selected test points.",2.1. Black Box Reshaping,[0],[0]
"However, this optimization is still infinite dimensional.
",2.1. Black Box Reshaping,[0],[0]
We make the final transition to finite dimensions by changing the objective function to only consider values of f on the test points.,2.1. Black Box Reshaping,[0],[0]
"Letting Fi,k,v denote the value of f evaluated on test point xi,k,v, we relax (2.3) to obtain the solution F ∗ =",2.1. Black Box Reshaping,[0],[0]
"(F ∗i,k,v)v∈R,i∈[n],k∈[n] of the optimization:
arg min F
∑ i,k,v (Fi,k,v − f̂(xi,k,v))2
subject to (Fi,1,v, ..., Fi,n,v) ∈ (Sv)v∈R, ∀",2.1. Black Box Reshaping,[0],[0]
i ∈,2.1. Black Box Reshaping,[0],[0]
"[n] (2.4)
However, this leads to ill-defined predictions on the original data points x1, ..., xn, since for each
v, xi,i,v = xi, but we may obtain different values F ∗i,i,v for various v ∈ R.
We avoid this issue by adding a consistency constraint (2.7) to obtain our final black box reshaping optimization (BBOPT):
arg min F
∑ i,k,v (Fi,k,v − f̂(xi,k,v))2 (2.5)
subject to (Fi,1,v, ..., Fi,n,v) ∈",2.1. Black Box Reshaping,[0],[0]
"(Sv)v∈R, ∀",2.1. Black Box Reshaping,[0],[0]
i ∈,2.1. Black Box Reshaping,[0],[0]
"[n] (2.6) and Fi,i,v = Fi,i,w ∀ v, w ∈",2.1. Black Box Reshaping,[0],[0]
"R,∀",2.1. Black Box Reshaping,[0],[0]
i ∈,2.1. Black Box Reshaping,[0],[0]
"[n] (2.7)
We then take the reshaped predictions to be
f ∗(xi)",2.1. Black Box Reshaping,[0],[0]
"= F ∗i,i,v
for any v ∈ R.",2.1. Black Box Reshaping,[0],[0]
"Since the constraints depend on each xi independently, BBOPT decomposes into n optimization problems, one for each observed value.",2.1. Black Box Reshaping,[0],[0]
Note that the true response values yi are not used when reshaping.,2.1. Black Box Reshaping,[0],[0]
We could select optimal shape constraints on a held-out test set.,2.1. Black Box Reshaping,[0],[0]
"In this section, we present an efficient algorithm for solving BBOPT for the case when each Sv imposes monotonicity constraints.",2.1.1. Intersecting Isotonic Regression,[0],[0]
"Let R = |R| denote the number of monotonicity constraints.
",2.1.1. Intersecting Isotonic Regression,[0],[0]
"When reshaping with respect to only one predictor (R = 1), the consistency constraints (2.7) vanish, so the optimization decomposes into n isotonic regression problems.",2.1.1. Intersecting Isotonic Regression,[0],[0]
Each problem is efficiently solved in Θ(n) time with the pool adjacent violators algorithm (PAVA),2.1.1. Intersecting Isotonic Regression,[0],[0]
"(Ayer et al., 1955).
",2.1.1. Intersecting Isotonic Regression,[0],[0]
"For R > 1 monotonicity constraints, BBOPT gives rise to n independent intersecting isotonic regression problems.",2.1.1. Intersecting Isotonic Regression,[0],[0]
"The k-th problem corresponds to the k-th observed value xk; the “intersection"" is implied by the consistency constraints (2.7).",2.1.1. Intersecting Isotonic Regression,[0],[0]
"For each independent problem, our algorithm takes O(m logR) time, where m = n×R is the number of variables in each problem.
",2.1.1. Intersecting Isotonic Regression,[0],[0]
We first state the general problem.,2.1.1. Intersecting Isotonic Regression,[0],[0]
"Assume v1, v2, . . .",2.1.1. Intersecting Isotonic Regression,[0],[0]
", vK are each real-valued vectors with dimensions d1, d2, . . .",2.1.1. Intersecting Isotonic Regression,[0],[0]
", dK , respectively.",2.1.1. Intersecting Isotonic Regression,[0],[0]
Let ij ∈,2.1.1. Intersecting Isotonic Regression,[0],[0]
"{1, . . .",2.1.1. Intersecting Isotonic Regression,[0],[0]
", dj} denote an index in the j-th vector vj .",2.1.1. Intersecting Isotonic Regression,[0],[0]
"The intersecting isotonic regression problem (IISO) is:
minimize (v̂k)Kk=1 K∑ k=1 ‖v̂k",2.1.1. Intersecting Isotonic Regression,[0],[0]
− vk‖2 subject to v̂k1 ≤ v̂k2 ≤ · · ·,2.1.1. Intersecting Isotonic Regression,[0],[0]
"≤ v̂kdk , ∀ k ∈",2.1.1. Intersecting Isotonic Regression,[0],[0]
"[K] and v̂1i1 = v̂ 2 i2 = · · · = v̂KiK
(2.8)
",2.1.1. Intersecting Isotonic Regression,[0],[0]
First consider the simpler constrained isotonic regression problem with a single sequence v ∈,2.1.1. Intersecting Isotonic Regression,[0],[0]
"Rd,
Algorithm 1 IISO Algorithm 1.",2.1.1. Intersecting Isotonic Regression,[0],[0]
Apply PAVA to each of the 2K tails.,2.1.1. Intersecting Isotonic Regression,[0],[0]
2.,2.1.1. Intersecting Isotonic Regression,[0],[0]
Combine and sort the left and right tails separately.,2.1.1. Intersecting Isotonic Regression,[0],[0]
3.,2.1.1. Intersecting Isotonic Regression,[0],[0]
Find segment s∗ in between tail values where the derivative g′(η) changes sign.,2.1.1. Intersecting Isotonic Regression,[0],[0]
4.,2.1.1. Intersecting Isotonic Regression,[0],[0]
"Compute c∗, the minimizer of g(c) in segment s∗.
index",2.1.1. Intersecting Isotonic Regression,[0],[0]
i ∈,2.1.1. Intersecting Isotonic Regression,[0],[0]
"[d], and fixed value c ∈ R
minimize v̂ ‖v̂ − v‖2 subject to v̂1 ≤",2.1.1. Intersecting Isotonic Regression,[0],[0]
"v̂2 ≤ · · · ≤ v̂d and v̂i = c
(2.9)
",2.1.1. Intersecting Isotonic Regression,[0],[0]
Lemma 2.1.,2.1.1. Intersecting Isotonic Regression,[0],[0]
"The solution v∗ to (2.9) can be computed by using index i as a pivot and splitting v into its left and right tails, so that ` = (v1, v2, . . .",2.1.1. Intersecting Isotonic Regression,[0],[0]
", vi−1) and r = (vi+1, . . .",2.1.1. Intersecting Isotonic Regression,[0],[0]
", vd), then applying PAVA to obtain monotone tails ̂̀and",2.1.1. Intersecting Isotonic Regression,[0],[0]
"r̂. v∗ is obtained by setting elements of ̂̀and r̂ to
`∗k = min(̂̀k, c) r∗k",2.1.1. Intersecting Isotonic Regression,[0],[0]
"= max(r̂k, c)
(2.10)
and concatenating the resulting tails so that v∗ = (`∗, c, r∗) ∈ Rd.
",2.1.1. Intersecting Isotonic Regression,[0],[0]
We now explain the IISO Algorithm presented in Algorithm (1).,2.1.1. Intersecting Isotonic Regression,[0],[0]
"First divide each vector vj into two tails, the left tail `j and the right tail rj , using the intersection index ij as a pivot,
vj = (vj1, v j 2, . . .",2.1.1. Intersecting Isotonic Regression,[0],[0]
", v j (ij−1)︸ ︷︷ ︸
`j
, vjij , v j (ij+1) , vj(ij+2), . . .",2.1.1. Intersecting Isotonic Regression,[0],[0]
", v j dj︸ ︷︷ ︸
rj
).
resulting in 2K tails {`1, . . .",2.1.1. Intersecting Isotonic Regression,[0],[0]
", `K , r1, . . .",2.1.1. Intersecting Isotonic Regression,[0],[0]
", rK}.
",2.1.1. Intersecting Isotonic Regression,[0],[0]
"Step 1 of Algorithm (1) performs an unconstrained isotonic regression on each tail using PAVA to obtain 2K monotone tails {̂̀1, . . .",2.1.1. Intersecting Isotonic Regression,[0],[0]
", ̂̀K , r̂1, . . .",2.1.1. Intersecting Isotonic Regression,[0],[0]
", r̂K}.",2.1.1. Intersecting Isotonic Regression,[0],[0]
"This can be done in Θ(n) time, where n is the total number of elements across all vectors so that n = ∑K i=1",2.1.1. Intersecting Isotonic Regression,[0],[0]
"di.
",2.1.1. Intersecting Isotonic Regression,[0],[0]
"Given the monotone tails, we can write a closed-form expression for the IISO objective function in terms of the value at the point of intersection.
",2.1.1. Intersecting Isotonic Regression,[0],[0]
Let c be the value of the vectors at the point of intersection so that c = v̂1i1 = v̂ 2 i2 = · · ·,2.1.1. Intersecting Isotonic Regression,[0],[0]
= v̂KiK .,2.1.1. Intersecting Isotonic Regression,[0],[0]
"For a fixed c, we can solve IISO by applying Lemma (2.1) to each sequence separately.",2.1.1. Intersecting Isotonic Regression,[0],[0]
"This yields the following expression for the squared error as a function of c:
g(c) = K∑ k=1 (c− vkik) 2 + K∑ k=1 ik−1∑ i=1",2.1.1. Intersecting Isotonic Regression,[0],[0]
"(`ki −min(̂̀ki , c))2 + K∑ l=1 dl∑ j=il+1 (rlj −max(r̂lj, c))2 (2.11)
which is piecewise quadratic with knots at each ̂̀ki and r̂lj .",2.1.1. Intersecting Isotonic Regression,[0],[0]
Our goal is to find c? = minc g(c).,2.1.1. Intersecting Isotonic Regression,[0],[0]
"Note that g(c) is convex and differentiable.
",2.1.1. Intersecting Isotonic Regression,[0],[0]
"We proceed by computing the derivative of g at each knot, from smallest to largest, and finding the segment in which the sign of the derivative changes from negative to positive.",2.1.1. Intersecting Isotonic Regression,[0],[0]
"The minimizer c∗ will live in this segment.
",2.1.1. Intersecting Isotonic Regression,[0],[0]
Step 2 of Algorithm (1) merges the left and right sorted tails into two sorted lists.,2.1.1. Intersecting Isotonic Regression,[0],[0]
This can be done in O(n logK) time with a heap data structure.,2.1.1. Intersecting Isotonic Regression,[0],[0]
"Step 3 computes the derivative of the objective function g at each knot, from smallest to largest, searching for the segment in which the derivative changes sign.",2.1.1. Intersecting Isotonic Regression,[0],[0]
Step 4 computes the minimizer of g in the corresponding segment.,2.1.1. Intersecting Isotonic Regression,[0],[0]
"By updating the derivative incrementally and storing relevant side information, Steps 3 and 4 can be done in linear time.
",2.1.1. Intersecting Isotonic Regression,[0],[0]
The total time complexity is therefore O(n log(K)).,2.1.1. Intersecting Isotonic Regression,[0],[0]
"In this section, we describe a framework for reshaping a random forest to ensure monotonicity of its predictions in a subset of its predictor variables.",2.2. Reshaping Random Forests,[0],[0]
A similar method can be applied to ensure convexity.,2.2. Reshaping Random Forests,[0],[0]
"For both regression and probability trees (Malley et al., 2012), the prediction of the forest is an average of the prediction of each tree; it is therefore sufficient to ensure monotonicity or convexity of the trees.",2.2. Reshaping Random Forests,[0],[0]
"For the rest of this section, we focus on reshaping individual trees to enforce monotonicity.
",2.2. Reshaping Random Forests,[0],[0]
Our method is a two-step procedure.,2.2. Reshaping Random Forests,[0],[0]
The first step is to grow a tree in the usual way.,2.2. Reshaping Random Forests,[0],[0]
The second step is to reshape the leaf values to enforce monotonicity.,2.2. Reshaping Random Forests,[0],[0]
"We hope to explore the implications of combining these steps in future work.
",2.2. Reshaping Random Forests,[0],[0]
Let T (x) be a regression tree and R ⊆,2.2. Reshaping Random Forests,[0],[0]
[d] a set of predictor variables to be reshaped.,2.2. Reshaping Random Forests,[0],[0]
Let x ∈ Rd be an input point and denote the k-th coordinate of x as xk.,2.2. Reshaping Random Forests,[0],[0]
Assume v ∈,2.2. Reshaping Random Forests,[0],[0]
R is a predictor variable to be reshaped.,2.2. Reshaping Random Forests,[0],[0]
"The following thought experiment, illustrated in Figure (2), will motivate
our approach.
",2.2. Reshaping Random Forests,[0],[0]
"Imagine dropping x down T until it falls in its corresponding leaf, `1.",2.2. Reshaping Random Forests,[0],[0]
Let p1 be the closest ancestor node to `1 that splits on v and assume it has split rule {xv ≤ t1}.,2.2. Reshaping Random Forests,[0],[0]
"Holding all other coordinates constant, increasing xv until it is greater than t1 would create a new point that drops down to a different leaf `2 in the right subtree of p1.
",2.2. Reshaping Random Forests,[0],[0]
"If `1 and `2 both share another ancestor p2 farther up the tree with split rule {xv ≤ t2}, increasing xv beyond t2 would yield another leaf `3.",2.2. Reshaping Random Forests,[0],[0]
"Assume these leaves have no other shared ancestors that split on v. Denoting the value of leaf ` as µ`, in order to ensure monotonicity in v for this point x, we require µ`1 ≤ µ`2 ≤ µ`3 .
",2.2. Reshaping Random Forests,[0],[0]
We use this line of thinking to propose a framework for estimating monotonic random forests and describe two estimators that fall under this framework.,2.2. Reshaping Random Forests,[0],[0]
"Each leaf ` in a decision tree is a cell (or hyperrectangle) C` which is an intersection of intervals
C` = d⋂ j=1 {x : xj ∈ I`j}
When we split on a shape-constrained variable v with split-value t, each cell in the left subtree
is of the form Cl = C̄l ∩ {x : xv ≤ t} and each cell in the right subtree is of the form Cr = C̄r ∩ {x : xv > t}.
",2.2.1. Exact Estimator,[0],[0]
"For cells l in the left subtree and r in the right subtree, our goal is to constrain the corresponding leaf values µl ≤ µr only when C̄l ∩ C̄r 6= ∅.",2.2.1. Exact Estimator,[0],[0]
See Figure (3) for an illustration.,2.2.1. Exact Estimator,[0],[0]
"We must devise an algorithm to find the intersecting cells (l, r), and add each to a constraint set E. This can be done efficiently with an interval tree data structure.
",2.2.1. Exact Estimator,[0],[0]
"Assume there are n unique leaves appearing in E. The exact estimator is obtained by solving the following optimization:
min (µ̂`) n",2.2.1. Exact Estimator,[0],[0]
"`=1
n∑ `=1 (µ` − µ̂`)2
subject to µ̂i ≤ µ̂j ∀",2.2.1. Exact Estimator,[0],[0]
"(i, j) ∈ E (2.12)
where µ` is the original value of leaf `.
",2.2.1. Exact Estimator,[0],[0]
"This is an instance of L2 isotonic regression on a directed acyclic graph where each leaf value µ` is a node, and each constraint in E is an edge.",2.2.1. Exact Estimator,[0],[0]
"With n vertices and m edges, the fastest known exact algorithm for this problem has time complexity Θ(n4) (Spouge et al., 2003), and the fastest known δ-approximate algorithm has complexity O(m1.5 log2 n log n
δ ) (Kyng et al., 2015).
",2.2.1. Exact Estimator,[0],[0]
"With a corresponding change to the constraints in Equation (2.12), this approach extends naturally to convex regression trees.",2.2.1. Exact Estimator,[0],[0]
It can also be applied directly to probability trees for binary classification by reshaping the estimated probabilities in each leaf.,2.2.1. Exact Estimator,[0],[0]
"In this section, we propose an alternative estimator that can be more efficient to compute, depending on the tree structure.",2.2.2. Over-constrained Estimator,[0],[0]
"In our experiments below, we find that computing this estimator is always faster.
",2.2.2. Over-constrained Estimator,[0],[0]
Let Ep denote the set of constraints that arise between leaf values under a shape-constrained split node p.,2.2.2. Over-constrained Estimator,[0],[0]
"By adding additional constraints to Ep, we can solve (2.12) exactly for each shapeconstrained split node in O(np log np) time, where np is the number of leaves under p.
",2.2.2. Over-constrained Estimator,[0],[0]
"In this setting, each shape-constrained split node gives rise to an independent optimization involving its leaves.",2.2.2. Over-constrained Estimator,[0],[0]
"Due to transitivity, we can solve these optimizations sequentially in reverse (bottom-up) level-order on the tree.
",2.2.2. Over-constrained Estimator,[0],[0]
Let np denote the number of leaves under node p.,2.2.2. Over-constrained Estimator,[0],[0]
"For each node p that is split on a shapeconstrained variable, the over-constrained estimator solves the following max-min problem:
min (µ̂`) np `=1
np∑ `=1 (µ` − µ̂`)2
subject to max `∈left(p) µ̂` ≤ min r∈right(p) µ̂r
(2.13)
where left(p) denotes all leaves in the left subtree of p and right(p) denotes all leaves in the right subtree.
",2.2.2. Over-constrained Estimator,[0],[0]
"This is equivalent to adding an edge (`, r) toE for every pair of leaves such that ` is in left(p) and r is in right(p).",2.2.2. Over-constrained Estimator,[0],[0]
All such pairs do not necessarily exist in E for the exact estimator; see Figure (3).,2.2.2. Over-constrained Estimator,[0],[0]
"For each shape-constrained split, (2.13) is an instance of L2 isotonic regression on a complete directed bipartite graph.
",2.2.2. Over-constrained Estimator,[0],[0]
"For a given shape-constrained split node p, let ` = (`1, `2, . . .",2.2.2. Over-constrained Estimator,[0],[0]
", `n1) be the values of the leaves in its left subtree, and r = (r1, r2, . . .",2.2.2. Over-constrained Estimator,[0],[0]
", rn2) be the values of the leaves in its right subtree, indexed so that `1 ≤ · · · ≤ `n1 and r1 ≤ · · · ≤ rn2 .",2.2.2. Over-constrained Estimator,[0],[0]
"Then the max-min problem (2.13) is equivalent to:
min˜̀,r̃ n1∑ i=1",2.2.2. Over-constrained Estimator,[0],[0]
(`i − ˜̀i)2 + n2∑ i=1,2.2.2. Over-constrained Estimator,[0],[0]
"(ri − r̃i)2
subject to ˜̀1 ≤ ˜̀2 ≤ ...",2.2.2. Over-constrained Estimator,[0],[0]
"≤ ˜̀n1 ≤ r̃1 ≤ · · · ≤ r̃n2 (2.14)
",2.2.2. Over-constrained Estimator,[0],[0]
"The solution to this optimization is of the form ˜̀i = min(c, `i) and r̃i = max(c, ri), for some constant c.",2.2.2. Over-constrained Estimator,[0],[0]
"Given the two sorted vectors ` and r, the optimization becomes:
min c n1∑ i=1",2.2.2. Over-constrained Estimator,[0],[0]
"(`i −min(c, `i))2 + n2∑ i=1",2.2.2. Over-constrained Estimator,[0],[0]
"(ri −max(c, ri))2
This objective is convex and differentiable in c. Similar to the black box reshaping method, we can compute the derivatives at the values of the data and find where it flips sign, then compute the minimizer in the corresponding segment.",2.2.2. Over-constrained Estimator,[0],[0]
"This takes O(n) time where n = n1 + n2, the number of leaves under the shape-constrained split.",2.2.2. Over-constrained Estimator,[0],[0]
"With sorting, the over-constrained estimator can be computed in O(n log n) time for each shape-constrained split node.
",2.2.2. Over-constrained Estimator,[0],[0]
"We apply this procedure sequentially on the leaves of every shape-constrained node in reverse level-order on the tree.
3.",2.2.2. Over-constrained Estimator,[0],[0]
"Experiments
We apply the reshaping methods described above to two regression tasks and two binary classification tasks.",2.2.2. Over-constrained Estimator,[0],[0]
We show that reshaping allows us to enforce shape constraints without compromising predictive accuracy.,2.2.2. Over-constrained Estimator,[0],[0]
"For convenience, we use the acronyms in Table (1) to refer to each method.
",2.2.2. Over-constrained Estimator,[0],[0]
"The BB method was implemented in R, and the OC and EX methods were implemented in R and C++, extending the R package ranger (Wright & Ziegler, 2017).",2.2.2. Over-constrained Estimator,[0],[0]
"The exact estimator from Section (2.2.1) is computed using the MOSEK C++ package (ApS, 2017).
",2.2.2. Over-constrained Estimator,[0],[0]
"For binary classification, we use the probability tree implementation found in ranger, enforcing monotonicity of the probability of a positive classification with respect to the chosen predictors.",2.2.2. Over-constrained Estimator,[0],[0]
"For the purposes of these experiments, black box reshaping is applied to a traditional random forest.",2.2.2. Over-constrained Estimator,[0],[0]
"The random forest was fit with the default settings found in ranger.
",2.2.2. Over-constrained Estimator,[0],[0]
We apply 5-fold cross validation on all four tasks and present the results under the relevant performance metrics in Table (2).,2.2.2. Over-constrained Estimator,[0],[0]
"The diabetes dataset (Efron et al., 2004) consists of ten physiological baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements, for each of 442 patients.",3.1. Diabetes Dataset,[0],[0]
"The response is a quantitative measure of disease progression measured one year after baseline.
",3.1. Diabetes Dataset,[0],[0]
"Holding all other variables constant, we might expect disease progression to be monotonically increasing in body mass index (Ganz et al., 2014).",3.1. Diabetes Dataset,[0],[0]
"We estimate a random forest and apply our reshaping techniques, then make predictions for a random test subject as we vary the body mass index predictor variable.",3.1. Diabetes Dataset,[0],[0]
"The results shown in Figure (4a) illustrate the effect of reshaping on the predictions.
",3.1. Diabetes Dataset,[0],[0]
We use mean squared error to measure accuracy.,3.1. Diabetes Dataset,[0],[0]
The results in Table (2) indicate that the prediction accuracy of all four estimators is approximately the same.,3.1. Diabetes Dataset,[0],[0]
"In this section, the regression task is to predict real estate sales prices using property information.",3.2. Zillow Dataset,[0],[0]
"The data were obtained from Zillow, an online real estate database company.",3.2. Zillow Dataset,[0],[0]
"For each of 206,820 properties, we are given the list price, number of bedrooms and bathrooms, square footage, build decade, sale year, major renovation year (if any), city, and metropolitan area.",3.2. Zillow Dataset,[0],[0]
"The response is the actual sale price of the home.
",3.2. Zillow Dataset,[0],[0]
We reshape to enforce monotonicity of the sale price with respect to the list price.,3.2. Zillow Dataset,[0],[0]
"Due to the size of the constraint set, this problem becomes intractable for MOSEK; the results for the EX method are omitted.",3.2. Zillow Dataset,[0],[0]
"An interesting direction for future work is to investigate more efficient algorithms for this method.
",3.2. Zillow Dataset,[0],[0]
"Following reported results from Zillow, we use mean absolute percent error (MAPE) as our measure of accuracy.",3.2. Zillow Dataset,[0],[0]
"For an estimate ŷ of the true value y, the APE is |ŷ − y|/y.
",3.2. Zillow Dataset,[0],[0]
The results in Table (2) show that the performance across all estimators is indistinguishable.,3.2. Zillow Dataset,[0],[0]
We apply the reshaping techniques to the binary classification task found in the Adult dataset Lichman (2013).,3.3. Adult Dataset,[0],[0]
"The task is to predict whether an individual’s income is less than or greater than $50,000.",3.3. Adult Dataset,[0],[0]
"Following the experiments performed in Milani Fard et al. (2016) and You et al. (2017), we apply monotonic reshaping to four variables: capital gain, weekly hours of work, education
level, and the gender wage gap.
",3.3. Adult Dataset,[0],[0]
We illustrate the effect of reshaping on the predictions in Figure (4b).,3.3. Adult Dataset,[0],[0]
The results in Table (2) show that we achieve similar test set accuracy before and after reshaping the random forest.,3.3. Adult Dataset,[0],[0]
"Finally, we apply reshaping to classify whether an email is spam or not.",3.4. Spambase Dataset,[0],[0]
"The Spambase dataset (Lichman, 2013) contains 4,601 emails each with 57 predictors.",3.4. Spambase Dataset,[0],[0]
"There are 48 word frequency predictors, 6 character frequency predictors, and 3 predictors related to the number of capital letters appearing in the email.
",3.4. Spambase Dataset,[0],[0]
That data were collected by Hewlett-Packard labs and donated by George Forman.,3.4. Spambase Dataset,[0],[0]
"One of the predictors is the frequency of the word “george"", typically assumed to be an indicator of non-spam for this dataset.",3.4. Spambase Dataset,[0],[0]
"We reshape the predictions to enforce the probability of being classified as spam to be monotonically decreasing in the frequency of words “george"" and “hp"".
",3.4. Spambase Dataset,[0],[0]
The results in Table (2) again show similar performance across all methods.,3.4. Spambase Dataset,[0],[0]
We presented two strategies for prediction rule reshaping.,4. Discussion,[0],[0]
"We developed efficient algorithms to compute the reshaped estimators, and illustrated their properties on four datasets.",4. Discussion,[0],[0]
"Both approaches can be viewed as frameworks for developing more sophisticated reshaping techniques.
",4. Discussion,[0],[0]
There are several ways that this work can be extended.,4. Discussion,[0],[0]
"Extensions to the black box method include adaptively combining rearrangements and isotonization (Chernozhukov et al., 2009), and considering a weighted objective function to account for the distance between test points.
",4. Discussion,[0],[0]
"In general, the random forest reshaping method can be viewed as operating on pre-trained parameters of a specific model.",4. Discussion,[0],[0]
"Applying this line of thinking to gradient boosted trees, deep learning methods, and other machine learning techniques could yield useful variants of this approach.
",4. Discussion,[0],[0]
"And finally, while practitioners might require certain shape-constraints on their predictions, many scientific applications also require inferential quantities, such as confidence intervals and confidence bands.",4. Discussion,[0],[0]
"Developing inferential procedures for reshaped predictions, similar to Chernozhukov et al. (2010) for rearrangements and Athey et al. (2018) for random forests, would yield interpretable predictions along with useful measures of uncertainty.",4. Discussion,[0],[0]
"Research supported in part by ONR grant N00014-12-1-0762, NSF grants DMS-1513594 and DMS-1654076, and an Alfred P. Sloan fellowship.",5. Acknowledgments,[0],[0]
Two methods are proposed for high-dimensional shape-constrained regression and classification.,abstractText,[0],[0]
These methods reshape pre-trained prediction rules to satisfy shape constraints like monotonicity and convexity.,abstractText,[0],[0]
"The first method can be applied to any pre-trained prediction rule, while the second method deals specifically with random forests.",abstractText,[0],[0]
"In both cases, efficient algorithms are developed for computing the estimators, and experiments are performed to demonstrate their performance on four datasets.",abstractText,[0],[0]
We find that reshaping methods enforce shape constraints without compromising predictive accuracy.,abstractText,[0],[0]
Prediction Rule Reshaping,title,[0],[0]
"Sparse Spectrum Gaussian Processes (SSGPs) are a powerful tool for scaling Gaussian processes (GPs) to large datasets. Existing SSGP algorithms for regression assume deterministic inputs, precluding their use in many real-world robotics and engineering applications where accounting for input uncertainty is crucial. We address this problem by proposing two analytic moment-based approaches with closed-form expressions for SSGP regression with uncertain inputs. Our methods are more general and scalable than their standard GP counterparts, and are naturally applicable to multi-step prediction or uncertainty propagation. We show that efficient algorithms for Bayesian filtering and stochastic model predictive control can use these methods, and we evaluate our algorithms with comparative analyses and both real-world and simulated experiments.",text,[0],[0]
"The problem of prediction under uncertainty, appears in many fields of science and engineering that involve sequential prediction including state estimation (Ko & Fox, 2009; Deisenroth et al., 2012), time series prediction (Girard et al., 2003), stochastic process approximation (Archambeau et al., 2007), and planning and control (Deisenroth et al., 2015; Pan et al., 2015).",1. Introduction,[0],[0]
"In these problems, uncertainty can be found in both the predictive models and the model’s inputs.",1. Introduction,[0],[0]
"Formally, we are often interested in finding the probability density of a prediction y, given a distribution p(x) and a probabilistic model p(y|x).",1. Introduction,[0],[0]
"By marginal-
1Georgia Institute of Technology, Atlanta, Georgia, USA 2School of Aerospace Engineering 3School of Interactive Computing.",1. Introduction,[0],[0]
"Correspondence to: Yunpeng Pan <ypan37@gatech.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
ization,
p(y) = ∫",1. Introduction,[0],[0]
p(y|x)p(x) dx.,1. Introduction,[0],[0]
"(1)
Unfortunately, computing this integral exactly is often intractable.",1. Introduction,[0],[0]
"In this paper, we tackle a subfamily of (1) where: 1) the probabilistic model is learned from data and specified by a sparse spectrum representation of a Gaussian process (SSGP); and 2) the input x is normally distributed.",1. Introduction,[0],[0]
We show that analytic expressions of the moments of p(y) can be derived and that these are directly applicable to sequential prediction problems like filtering and control.,1. Introduction,[0],[0]
"Gaussian Process (GP) regression with uncertain inputs has been addressed by Candela et al. (2003); Girard et al. (2003), and extended to the multivariate outputs by Kuss (2006).",1.1. Related work,[0],[0]
"These methods have led to the development of many algorithms in reinforcement learning (Rasmussen & Kuss, 2004; Deisenroth et al., 2015), Bayesian filtering (Ko & Fox, 2009; Deisenroth et al., 2009), and smoothing (Deisenroth et al., 2012).",1.1. Related work,[0],[0]
"However, these approaches have two major limitations: 1) they are not directly applicable to large datasets, due to the polynomial time complexity for exact inference (Williams & Rasmussen, 2006); and 2) analytic moment expressions, when used, are restricted to squared exponential (SE) kernels (Kuss, 2006) and cannot be generalized to other kernels in a straightforward way.
",1.1. Related work,[0],[0]
"A common method for approximating large-scale kernel machines is through random Fourier features (Rahimi & Recht, 2007).",1.1. Related work,[0],[0]
The key idea is to map the input to a lowdimensional feature space yielding fast linear methods.,1.1. Related work,[0],[0]
"In the context of GP regression (GPR), this idea leads to the sparse spectrum GPR (SSGPR) algorithm (Lázaro-Gredilla et al., 2010).",1.1. Related work,[0],[0]
"SSGP has been extended in a number of ways for, e.g. incremental model learning (Gijsberts & Metta, 2013), and large-scale GPR (Dai et al., 2014; Yan et al., 2015).",1.1. Related work,[0],[0]
"However, to the best of our knowledge, prediction under uncertainty for SSGPs has not been explored.",1.1. Related work,[0],[0]
"Although there are several alternative approximations to exact GP inference including approximating the posterior distribution using inducing points, e.g., (Snelson & Ghahramani, 2006; Titsias, 2009; Cheng & Boots, 2016), comparing different GP approximations is not the focus of this paper.",1.1. Related work,[0],[0]
"We consider two key problems that are widely encountered in robotics and engineering: Bayesian filtering and stochastic model predictive control.
",1.2. Applications,[0],[0]
The goal of Bayesian filtering is to infer a hidden system state through the recursive application of Bayes’ rule.,1.2. Applications,[0],[0]
"Well-known frameworks for Bayesian filtering include unscented Kalman Filtering (UKF), particle filtering (PF), extended Kalman filtering (EKF), and assumed density filtering (ADF).",1.2. Applications,[0],[0]
"GP-based Bayesian filtering with SE kernels has been developed for these frameworks by (Ko & Fox, 2009; Deisenroth et al., 2009).",1.2. Applications,[0],[0]
"We extend this work with highly efficient SSGP-based EKF and ADF algorithms.
",1.2. Applications,[0],[0]
The goal of stochastic model predictive control (MPC) is to find finite horizon optimal control at each time instant.,1.2. Applications,[0],[0]
"Due to the high computational cost of GP inference and real-time optimization requirements in MPC, most GPbased control methods (Deisenroth et al., 2015; Pan & Theodorou, 2014; Kupcsik et al., 2014) are restricted to episodic reinforcement learning tasks.",1.2. Applications,[0],[0]
"To cope with this challenge, we present an SSGP-based MPC algorithm that is fast enough to perform probabilistic trajectory optimization and model adaptation on-the-fly.",1.2. Applications,[0],[0]
"• We propose two approaches to prediction under un-
certainty in SSGPs with closed-form expressions for the predictive distribution.",1.3. Our contributions,[0],[0]
"Compared to previous GP counterparts, our methods: 1) are more scalable, and 2) can be generalized to any continuous shift-invariant kernels with a Fourier feature representation.
",1.3. Our contributions,[0],[0]
"• We demonstrate successful applications of the proposed approaches by presenting scalable algorithms for 1) recursive Bayesian filtering and 2) stochastic model predictive control via probabilistic trajectory optimization.
",1.3. Our contributions,[0],[0]
The rest of the paper is organized as follows.,1.3. Our contributions,[0],[0]
"In §2, we give an introduction to SSGPs, which serves as our probabilistic model.",1.3. Our contributions,[0],[0]
Derivation and expressions of the two proposed prediction methods are detailed in §3.,1.3. Our contributions,[0],[0]
"Applications to filtering and control, and experimental results are presented in §4 and §5 respectively.",1.3. Our contributions,[0],[0]
Finally §6 concludes the paper.,1.3. Our contributions,[0],[0]
"Consider the task of learning the function f : Rd → R, given IID data D = {xi, yi}ni=1, with each pair related by
y = f(x) + , ∼ N (0, σ2n), (2)
where is IID additive Gaussian noise.",2. Sparse Spectral Representation of GPs,[0],[0]
"Gaussian process regression (GPR) is a principled way of performing Bayesian inference in function space, assuming that function f has a prior distribution f ∼ GP(m, k), with mean
function m : Rd → R and kernel",2. Sparse Spectral Representation of GPs,[0],[0]
k :,2. Sparse Spectral Representation of GPs,[0],[0]
"Rd × Rd → R. Without loss of generality, we assume m(x) = 0.",2. Sparse Spectral Representation of GPs,[0],[0]
"Exact GPR is challenging for large datasets due to its O(n3) time andO(n2) space complexity (Williams & Rasmussen, 2006), which is a direct consequence of having to store and invert an n× n Gram matrix.
",2. Sparse Spectral Representation of GPs,[0],[0]
"Random features can be used to form an unbiased approximation of continuous shift-invariant kernel functions, and are proposed as a general mechanism to accelerate largescale kernel machines (Rahimi & Recht, 2007), via explicitly mapping inputs to low-dimensional feature space.",2. Sparse Spectral Representation of GPs,[0],[0]
"Based on Bochner’s theorem, the Fourier transform of a continuous shift-invariant positive definite kernel k(x, x′) is a proper probability distribution p(ω), assuming k(x, x′) is properly scaled (Rahimi & Recht, 2007):
k(x, x′) = ∫",2. Sparse Spectral Representation of GPs,[0],[0]
p(ω)ejω T (x−x′),2. Sparse Spectral Representation of GPs,[0],[0]
"dω
= E(φω(x)φω(x ′)∗), ω ∼ p(ω),
(3)
where φω(x) = ejω T x, and we can see that k(x, x′) only depends on the lag vector separating x and x′:",2. Sparse Spectral Representation of GPs,[0],[0]
"x−x′. Equation (3) leads to an unbiased finite sample approximation of k: k(x, x′)",2. Sparse Spectral Representation of GPs,[0],[0]
"≈ 1m ∑ φωi(x)φωi(x
′)∗, where random frequencies {ωi}mi=1 are drawn IID from p(ω).",2. Sparse Spectral Representation of GPs,[0],[0]
"Utilizing the fact that φω can be replaced by sinusoidal functions since both p(ω) and k(x, x′) are reals, and",2. Sparse Spectral Representation of GPs,[0],[0]
"concatenating features {φωi}mi=1 into a succinct vector form, an approximation for k(x, x′) is expressed as
k(x, x′)",2. Sparse Spectral Representation of GPs,[0],[0]
"≈ φ(x)Tφ(x′), φ(x)",2. Sparse Spectral Representation of GPs,[0],[0]
=,2. Sparse Spectral Representation of GPs,[0],[0]
"[ φc(x) φs(x) ] , (4)
φci (x) = σk cos(ω T i x), φ s i (x) =",2. Sparse Spectral Representation of GPs,[0],[0]
"σk sin(ω T i x), ωi ∼ p(ω),
where σk is a scaling coefficient.",2. Sparse Spectral Representation of GPs,[0],[0]
"For the commonly used Squared Exponential (SE) kernel: k(x, x′) = σ2f exp(− 12‖x",2. Sparse Spectral Representation of GPs,[0],[0]
"− x ′‖2Λ−1), p(ω) = N (0,Λ −1) and σk = σf√ m
, where the coefficient σf and the diagonal matrix Λ are the hyperparameters, examples of kernels and corresponding spectral densities can be found in Table 1.
",2. Sparse Spectral Representation of GPs,[0],[0]
"In accordance with this feature map (4), Sparse Spectrum GPs are defined as follows
Definition 1.",2. Sparse Spectral Representation of GPs,[0],[0]
"Sparse Spectrum GPs (SSGPs) are GPs with kernels defined on the finite-dimensional and randomized feature map φ (4):
k(x, x′) = φ(x)Tφ(x′) + σ2nδ(x− x′), (5)
where the function δ is the Kronecker delta function.
",2. Sparse Spectral Representation of GPs,[0],[0]
"The second term in (5) accounts for the additive zero mean Gaussian noise in (2), if the goal is to learn the correlation between x and y directly as in our case of learning the probabilistic model p(y|x), instead of learning the latent function f .
",2. Sparse Spectral Representation of GPs,[0],[0]
"Because of the explicit finite-dimensional feature map (4), each SSGP is equivalent to a Gaussian distribution over the weights of features w ∈ R2m. Assuming that prior distribution of weights w is N (0, I) 1 and the feature map is fixed, after conditioning on the data D = {xi, yi}ni=1, the posterior distribution of w is 2
w ∼ N (α, σ2nA−1), (6) α = A−1ΦY, A = ΦΦT + σ2nI,
which can be derived through Bayesian linear regression.",2. Sparse Spectral Representation of GPs,[0],[0]
"In (6), the column vector Y and the matrix Φ are specified by the data D: Y =",2. Sparse Spectral Representation of GPs,[0],[0]
[ y1 . . .,2. Sparse Spectral Representation of GPs,[0],[0]
"yn ]T , Φ =[
φ(x1) . . .",2. Sparse Spectral Representation of GPs,[0],[0]
φ(xn) ] .,2. Sparse Spectral Representation of GPs,[0],[0]
"Consequently, the posterior distribution over the output y in (2) at a test point x is exactly Gaussian, in which the posterior variance explicitly captures the model uncertainty in prediction with input x:
p(y|x)",2. Sparse Spectral Representation of GPs,[0],[0]
"= N (αTφ(x), σ2n + σ2n‖φ(x)‖2A−1).",2. Sparse Spectral Representation of GPs,[0],[0]
"(7)
This Bayesian linear regression method for SSGP is proposed in Lázaro-Gredilla et al. (2010).",2. Sparse Spectral Representation of GPs,[0],[0]
"Its time complexity is O(nm2 +m3), which is significantly more efficient than standard GPR’s O(n3)",2. Sparse Spectral Representation of GPs,[0],[0]
"when m n.
Remark It’s worth noting that the methods proposed in this paper are not tied to specific algorithms for SSGP regression such as Bayesian linear regression (LázaroGredilla et al., 2010), but able to account for any SSGP with specified feature weights distribution (6), where posterior α and A can be computed by any means.",2. Sparse Spectral Representation of GPs,[0],[0]
"Variations on A include sparse approximations by a low rank plus diagonal matrix, or iterative solutions by optimization methods like doubly stochastic gradient descent (Dai et al., 2014).",2. Sparse Spectral Representation of GPs,[0],[0]
"Two methods for prediction under uncertainty are presented under two conditions: 1) the uncertain input is normally distributed: x ∼ N (µ,Σ), and 2) probabilistic models are in the form of (7) specified by SSGPs.",3. Prediction under Uncertainty,[0],[0]
"Despite these conditions, evaluating the integral in (1) is still intractable.",3. Prediction under Uncertainty,[0],[0]
"In this work, we approximate the true predictive distribution p(y) by a Gaussian distribution with moments that are analytically computed through: 1) exact moment matching, and 2) linearization of posterior mean function.",3. Prediction under Uncertainty,[0],[0]
"Closed-form expressions for predictive mean, variance, covariance, and input-prediction cross-covariance are derived.",3. Prediction under Uncertainty,[0],[0]
"We consider multivariate outputs by utilizing con-
1I is the identity matrix with proper size.",3. Prediction under Uncertainty,[0],[0]
The prior covariance is identity since E (f(x)f(x)),3. Prediction under Uncertainty,[0],[0]
= E ( φ(x)TwwTφ(x′) ),3. Prediction under Uncertainty,[0],[0]
"= φ(x)T E(wwT )φ(x′), and E (f(x)f(x′))",3. Prediction under Uncertainty,[0],[0]
"= φ(x)Tφ(x′) (see §2.2 in Rasmussen & Kuss (2004) for details.)
",3. Prediction under Uncertainty,[0],[0]
"2Conditioning on data D is omitted, e.g., in w|D, for simplicity in notation.
ditionally independent scalar models for each output dimension, i.e., assuming for outputs in different dimension ya and yb, p(ya, yb|x)",3. Prediction under Uncertainty,[0],[0]
= p(ya|x)p(yb|x).,3. Prediction under Uncertainty,[0],[0]
Discussions on this assumption can be found in Appendix §6.1.,3. Prediction under Uncertainty,[0],[0]
"For notational simplicity, we suppress the dependency of φ(x) on x, and treat y as a scalar by default.",3. Prediction under Uncertainty,[0],[0]
"We derive the closed-form expressions for exact moments: 1) the predictive mean E y, 2) the predictive variance Var y and covariance Cov(ya, yb), which in the multivariate case correspond to the diagonal and off-diagonal entries of the predictive covariance matrix, and 3) the cross-covariance between input and prediction Cov(x, y).
",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"Using the expressions for SSGP (4), (7), and the law of total expectation, the predictive mean becomes
E y = EE(y|x)",3.1. Exact moment matching (SSGP-EMM),[0],[0]
= E ( αTφ ),3.1. Exact moment matching (SSGP-EMM),[0],[0]
"= αT E
[ φc
φs
] , (8)
Eφci = σk E cos(ω T",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"i x),",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"Eφ s i = σk E sin(ω T i x),
where i = 1, . . .",3.1. Exact moment matching (SSGP-EMM),[0],[0]
",m, and in the nested expectation EE(y|x), the outer expectation is over the input distribution p(x)",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"= N (µ,Σ), and the inner expectation is over the conditional distribution p(y|x) (7).
",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"By observing (8), we see that the expectation of sinusoids under the Gaussian distribution is the key to computing the predictive mean.",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"Thus, we state the following proposition: Proposition 1.",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"The expectation of sinusoids over multivariate Gaussian distributions: x ∼ N (µ,Σ), x ∈ Rd, i.e., p(x) = (2π)− d 2 (det Σ)−
1 2 exp(− 12‖x",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"− µ‖ 2 Σ−1), can
be computed analytically:
E cos(ωTx) = exp(−1 2 ‖ω‖2Σ) cos(ωTµ), E sin(ωTx) = exp(−1 2 ‖ω‖2Σ) sin(ωTµ).
",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"To prove it, we invoke Euler’s formula to transform the lefthand-side to complex domain, apply identities involving quadratic exponentials, and then convert back to real numbers (see Appendix §3.2 for details).",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"In Proposition 1, the expectations depend on the mean and variance of the input Gaussian distribution.",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"Intuitively, after passing a Gaussian distributed input through a sinusoidal function, the expectation of the output is equal to passing the mean of the input through the sinusoid, and then scaling it by a constant exp(− 12‖ω‖ 2 Σ), which depends on the variance of the input.",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"Expectations are smaller with larger input variance due to the periodicity of sinusoids.
",3.1. Exact moment matching (SSGP-EMM),[0],[0]
The exact moments are then derived using Proposition 1.,3.1. Exact moment matching (SSGP-EMM),[0],[0]
"By the law of total variance, the predictive variance is
Var y = EVar(y|x) + VarE(y|x)",3.1. Exact moment matching (SSGP-EMM),[0],[0]
= σ2n,3.1. Exact moment matching (SSGP-EMM),[0],[0]
+ σ 2 nTr ( A−1Ψ ) +,3.1. Exact moment matching (SSGP-EMM),[0],[0]
"αTΨα− (E y)2, (9)
where Ψ is defined as the expectation of the outer product of feature vectors over input distribution p(x).",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"Specifically, we compute Ψ by applying the product-to-sum trigonometric identities:
E ( φφT ) =",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"Ψ =
[ Ψcc Ψcs
Ψsc Ψss
] ,
Ψccij = σ2k 2
( E ( cos(ωi + ωj)",3.1. Exact moment matching (SSGP-EMM),[0],[0]
Tx ) +,3.1. Exact moment matching (SSGP-EMM),[0],[0]
E,3.1. Exact moment matching (SSGP-EMM),[0],[0]
"( cos(ωi − ωj)Tx )) ,
Ψssij = σ2k 2
( E ( cos(ωi − ωj)Tx )",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"−E ( cos(ωi + ωj) Tx )) ,
Ψcsij = σ2k 2
( E ( sin(ωi + ωj) Tx )",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"−E ( sin(ωi − ωj)Tx )) ,
where Ψcc,Ψss,Ψcs are m × m matrices, and i, j = 1, . . .",3.1. Exact moment matching (SSGP-EMM),[0],[0]
",m, on whose terms Proposition 1 can be directly applied.
",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"Next, we derive the covariance for different output dimensions for multivariate prediction.",3.1. Exact moment matching (SSGP-EMM),[0],[0]
These correspond to the off-diagonal entries of the predictive covariance matrix.,3.1. Exact moment matching (SSGP-EMM),[0],[0]
"We show that, despite the conditional independence assumption for different outputs given a deterministic input, outputs become coupled with uncertain inputs.",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"Using the law of total covariance, the covariance is
Cov(ya, yb) =",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"Cov (E(ya|x),E(yb|x))",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"= E (E(ya|x),E(yb|x))−(E ya)(E yb) = αTaΨabαb",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"− (αTa Eφa)(αTb Eφb), (10)
where matrix Ψab is the expectation of the outer product of feature vectors corresponding to different feature maps φa, φb for outputs ya, yb, computed similarly as in (3.1) with corresponding random frequencies {ωi}, and the scaling coefficient σk (4).",3.1. Exact moment matching (SSGP-EMM),[0],[0]
Vectors αa and αb are the corresponding weight vectors for ya and yb (7).,3.1. Exact moment matching (SSGP-EMM),[0],[0]
"Compared to the expression for the variance of a single output in (9), the term E (Cov(ya|x)Cov(yb|x)) that is included in the law of total covariance is neglected due to the assumption of conditional independence of different outputs (§2), so (10) does not have the corresponding first two terms in (9).
",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"Finally, we compute the cross-covariance between input and each output dimension.",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"Invoking the law of total covariance:
Cov(x, y) = Cov(x,E(y|x))",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"= E (xE(y|x))− (Ex)(E y) = Υα− (E y)µ,
(11)
where matrix Υ is the expectation of the outer product of the input x and the feature vector φ(x) over input distribution x ∼ N (µ,Σ): E(xφT ) =",3.1. Exact moment matching (SSGP-EMM),[0],[0]
Υ = [ Υc1 . . .,3.1. Exact moment matching (SSGP-EMM),[0],[0]
Υ,3.1. Exact moment matching (SSGP-EMM),[0],[0]
c m Υ s 1 . . .,3.1. Exact moment matching (SSGP-EMM),[0],[0]
"Υ s m ] ,
Υci = σk E",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"( cos(ωTi x)x ) , Υsi = σk E ( cos(ωTi x)x ) ,
where r = √ 2ν‖x−x′‖2
` , Kν is a modified Bessel function,
and h = 2 dπ
d 2 Γ(ν+ d2 )(2ν) ν
Γ(ν)`2ν .
",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"where i = 1, . .",3.1. Exact moment matching (SSGP-EMM),[0],[0]
.,3.1. Exact moment matching (SSGP-EMM),[0],[0]
",m. We state the following proposition to compute each column in Υ consisting of expectations of the product sinusoidal functions and inputs.
",3.1. Exact moment matching (SSGP-EMM),[0],[0]
Proposition 2.,3.1. Exact moment matching (SSGP-EMM),[0],[0]
"The expectation of the multiplication of sinusoids and linear functions over multivariate Gaussian distributions: x ∼ N (µ,Σ), can be computed analytically:
E ( cos(ωTx)x )",3.1. Exact moment matching (SSGP-EMM),[0],[0]
=,3.1. Exact moment matching (SSGP-EMM),[0],[0]
( E cos(ωTx) ),3.1. Exact moment matching (SSGP-EMM),[0],[0]
"µ− (E(sin(ωTx))Σω,
E ( sin(ωTx)x )",3.1. Exact moment matching (SSGP-EMM),[0],[0]
=,3.1. Exact moment matching (SSGP-EMM),[0],[0]
( E sin(ωTx) ),3.1. Exact moment matching (SSGP-EMM),[0],[0]
µ+ ( E cos(ωTx) ),3.1. Exact moment matching (SSGP-EMM),[0],[0]
"Σω,
where the right-hand-side expectations have analytical expressions (Proposition 1).",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"To prove it, we find an expression for E ( aTx cos(ωTx) ) , for any a, through the complex domain trick used to prove Proposition 1.",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"Next, the result is extended to E ( x cos(ωTx) ) , by setting a to consist of indicator vectors (see Appendix §3.3 for details).",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"Applying Proposition 1 and 2, we complete the derivation of Cov(x, y) in (11).
",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"Remark In summary, SSGP-EMM computes the exact posterior moments.",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"This is equivalent to expectation propagation (Minka, 2001) by minimizing the Kullback-Leibler divergence between the true distribution and its Gaussian approximation with respect to the natural parameters.",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"SSGP-EMM’s computation complexity is O ( m2k2d2 ) , where m is the number of features, k is the output dimension, and d is the input dimension.",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"The most computationally demanding part is constructing matrices Ψab (10) for each output pair, where each requires O ( m2d2 ) .
",3.1. Exact moment matching (SSGP-EMM),[0],[0]
"Compared to the multivariate moment-matching approach for GPs (GP-EMM) (Girard et al., 2003; Kuss, 2006) with O ( n2k2d2 ) time complexity, SSGP-EMM is more efficient when m n. Moreover, our approach is applicable to any positive-definite continuous shift-invariant kernel with different spectral densities (see examples in Table 1), while previous approaches like GP-EMM (Kuss, 2006) are only derived for squared exponential (SE) or polynomial kernels.",3.1. Exact moment matching (SSGP-EMM),[0],[0]
Next we introduce a more computationally efficient but less accurate approach that avoids the computation of Ψab’s.,3.1. Exact moment matching (SSGP-EMM),[0],[0]
"An alternative approach to computing the exact moments of the predictive distribution is based on the linearization of the posterior mean function in (7) at the input mean µ:
m(x) = αTφ(x)",3.2. Linearization (SSGP-Lin),[0],[0]
"≈ m(µ) + αT Dφ(µ)︸ ︷︷ ︸ M (x− µ), (12)
where Dφ(µ) denotes taking the derivative of function φ at µ. Given the definition of φ in (4), Dφ can be found by chain rule: Dφci (x) = −σk",3.2. Linearization (SSGP-Lin),[0],[0]
sin(ωTi,3.2. Linearization (SSGP-Lin),[0],[0]
"x)ωTi , Dφsi (x) = σk cos(ω T",3.2. Linearization (SSGP-Lin),[0],[0]
i x)ω,3.2. Linearization (SSGP-Lin),[0],[0]
"T i .
",3.2. Linearization (SSGP-Lin),[0],[0]
"Utilizing the linearized posterior mean function (12), the predictive moments can be approximated.",3.2. Linearization (SSGP-Lin),[0],[0]
"The predictive mean approximation is
E y = EE(y|x)",3.2. Linearization (SSGP-Lin),[0],[0]
"≈ m(µ), (13)
and the predictive variance approximation is Var y = EVar(y|x)",3.2. Linearization (SSGP-Lin),[0],[0]
"+ VarE(y|x)
",3.2. Linearization (SSGP-Lin),[0],[0]
≈ Var(y|µ),3.2. Linearization (SSGP-Lin),[0],[0]
+ Var(αTMx),3.2. Linearization (SSGP-Lin),[0],[0]
= σ2n + σ,3.2. Linearization (SSGP-Lin),[0],[0]
"2 n‖φ(µ)‖2A−1 + α TMΣMTα.
(14)
and the approximate covariance between output dimension a and b is Cov(ya, yb) =",3.2. Linearization (SSGP-Lin),[0],[0]
"Cov (E(ya|x),E(yb|x))
",3.2. Linearization (SSGP-Lin),[0],[0]
= E ( αTaMa(x− µ)(x− µ)TMTb αb ),3.2. Linearization (SSGP-Lin),[0],[0]
"≈ αTaMaΣMTb αb, (15)
where Ma and Mb are defined as M in (12), except that they correspond to feature maps φa and φb.",3.2. Linearization (SSGP-Lin),[0],[0]
"Notice that the assumption of conditional independence between different outputs is invoked here again, cf., (10).
",3.2. Linearization (SSGP-Lin),[0],[0]
"Finally, the cross-covariance between the input and output can be approximated as
Cov(x, y) = Cov(x,E(y|x))",3.2. Linearization (SSGP-Lin),[0],[0]
≈ E ( (x− µ)(αTM(x− µ)) ),3.2. Linearization (SSGP-Lin),[0],[0]
"= αTMΣ
(16)
",3.2. Linearization (SSGP-Lin),[0],[0]
"Unlike SSGP-EMM, which computes exact moments (§3.1), this linearization-based approach SSGP-Lin computes an approximation of the predictive moments.",3.2. Linearization (SSGP-Lin),[0],[0]
"In contrast to SSGP-EMM’s O ( m2k2d ) computational complexity, the computation time of SSGP-Lin is reduced to O ( m2kd ) , as a direct consequence of avoiding the construction of Ψ (3.1) in SSGP-EMM (10), which makes SSGP-Lin more efficient than SSGP-EMM, especially when the output dimension is high.
",3.2. Linearization (SSGP-Lin),[0],[0]
Both SSGP-EMM and SSGP-Lin are applicable to a general family of kernels.,3.2. Linearization (SSGP-Lin),[0],[0]
"See Table 2 for a comparison between our methods and GP-EMM (Girard et al., 2003; Kuss, 2006).",3.2. Linearization (SSGP-Lin),[0],[0]
"In the next section, we compare these approaches in applications of filtering and control.",3.2. Linearization (SSGP-Lin),[0],[0]
We focus on the application of the proposed methods to Bayesian filtering and predictive control.,4. Applications,[0],[0]
"We begin by introducing Gauss-Markov models, which can be expressed by the following discrete-time nonlinear dynamical system:
xt+1 = f(xt, ut) + x t ,
x t ∼ N",4. Applications,[0],[0]
"(0,Σ x), (17)
yt = g(xt) + y t ,
",4. Applications,[0],[0]
"y t ∼ N (0,Σ y ), (18)
where xt ∈ Rd is state, ut ∈",4. Applications,[0],[0]
"Rr is control, yt ∈",4. Applications,[0],[0]
"Rk is observation or measurement, xt ∈ Rd is IID process noise,",4. Applications,[0],[0]
yt ∈,4. Applications,[0],[0]
"Rk is IID measurement noise, and subscript t denotes discrete time index.",4. Applications,[0],[0]
"We call the probabilistic models (17) and (18) the dynamics and observation models, and the corresponding deterministic functions f and g the dynamics and observation functions.
",4. Applications,[0],[0]
We consider scenarios where f and g are unknown but a dataset D =,4. Applications,[0],[0]
"( {(xt, ut), xt+1}n−1t=1 , {xt, yt}nt=1 ) is provided.",4. Applications,[0],[0]
"The probabilistic models specified by SSGPs can be learned from the dataset, and then used to model the dynamics and observation (17) (18).",4. Applications,[0],[0]
"More concretely, the dynamics model p(xt+1|xt, ut) is learned using state transition pairs {(xt, ut), xt+1}n−1t=1 , and the observation model p(yt|xt) is learned separately from state-observation pairs {xt, yt}nt=1.",4. Applications,[0],[0]
"The task of Bayesian filtering is to infer the posterior distribution of the current state of a dynamical system based on the current and past noisy observations, i.e., finding p(xt|t), where the notation xt|s denotes the random variable xt|y0, . . .",4.1. Bayesian filtering,[0],[0]
", ys.",4.1. Bayesian filtering,[0],[0]
"Due to the Markov property of the process x, i.e., xt|x0, . . .",4.1. Bayesian filtering,[0],[0]
", xt−1 = xt|xt−1, in Gauss-Markov models, p(xt|t) can be computed recursively through alternating prediction step and correction step.
4.1.1.",4.1. Bayesian filtering,[0],[0]
PREDICTION STEP (xt−1|t−1 → xt|t−1),4.1. Bayesian filtering,[0],[0]
"In the prediction step, xt−1|t−1 is propagated through the dynamics model p(xt|xt−1, ut−1):
p(xt|t−1) = ∫",4.1. Bayesian filtering,[0],[0]
"p(xt|xt−1, ut−1)p(xt−1|t−1) dxt−1,
which can be viewed as prediction under uncertainty (1).",4.1. Bayesian filtering,[0],[0]
"Suppose that p(xt−1|t−1) = N (µ̂t−1|t−1, Σ̂t−1|t−1), with learned SSGP representation for the dynamics, Gaussian approximations of the output: p(xt|t−1)",4.1. Bayesian filtering,[0],[0]
"≈ N (µ̂t|t−1, Σ̂t|t−1) can be obtained by either SSGP-EMM
(§3.1) using (8), (9) and (10), or SSGP-Lin (§3.2) using (13), (14) and (15).
",4.1. Bayesian filtering,[0],[0]
4.1.2.,4.1. Bayesian filtering,[0],[0]
CORRECTION STEP,4.1. Bayesian filtering,[0],[0]
(xt|t−1 → xt|t),4.1. Bayesian filtering,[0],[0]
"The correction step conditions xt|t−1 on the current observation yt using Bayes’ rule:
p(xt|t) = p(yt|xt|t−1)p(xt|t−1)∫ p(yt|xt|t−1)p(xt|t−1) dxt .",4.1. Bayesian filtering,[0],[0]
"(19)
In the preceding prediction step, we obtain p(xt|t−1)",4.1. Bayesian filtering,[0],[0]
"≈ N (µ̂t|t−1, Σ̂t−1|t−1), which serves as a prior on xt in this correction step.",4.1. Bayesian filtering,[0],[0]
"Due to the intractability of the integral in the denominator, to apply Bayes’ rule we first seek Gaussian approximations for the joint distribution, as in the previous work on Bayesian filtering relying on GPs (Deisenroth et al., 2009; Ko & Fox, 2009):",4.1. Bayesian filtering,[0],[0]
"[
xt|t−1 yt|t−1
] ∼ N ([ µ̂t|t−1 µ̂y ] , [ Σ̂t|t−1 Σ̂xy Σ̂Txy Σ̂y ]) , (20)
Invoking p(yt|t−1) = ∫ p(yt|xt|t−1)p(xt|t−1) dxt, the moments µ̂y , Σ̂y , and Σ̂xy in the joint Gaussian approximation can be computed as the predictive mean, predictive covariance, and input-prediction cross-covariance, for the observation model p(yt|xt) with input p(xt|t−1), using SSGPEMM or SSGP-Lin.",4.1. Bayesian filtering,[0],[0]
"Having all terms in (20) determined, we condition xt|t−1 exactly on current observation yt:
µ̂t|t = µ̂t|t−1 + Σ̂xyΣ̂ −1 y",4.1. Bayesian filtering,[0],[0]
"(y − µ̂y), Σ̂t|t = Σ̂t|t−1 − Σ̂xyΣ̂−1y Σ̂xy.",4.1. Bayesian filtering,[0],[0]
"(21)
This Gaussian approximation p(xt|t)",4.1. Bayesian filtering,[0],[0]
"≈ N (µ̂t|t, Σ̂t|t) is then used as input to the prediction step.",4.1. Bayesian filtering,[0],[0]
"Thus, we have shown that starting from p(x0) = N (µ0,Σ0), by consecutively applying prediction and correction steps presented above, we recursively obtain state estimates for xt|t−1 and xt|t.",4.1. Bayesian filtering,[0],[0]
"Rather than using a finite sample-based approximation such as in the GP-UKF (Ko & Fox, 2009), the Gaussian approximations of the full densities p(xt|t) and p(xt|t−1) are propagated.
",4.1. Bayesian filtering,[0],[0]
"Algorithm 1 SSGP-ADF and SSGP-EKF 1: Model learning: collect dataset D, and learn SSGP dy-
namics and observations models (§2.)",4.1. Bayesian filtering,[0],[0]
2: Initialization: set prior p(x0).,4.1. Bayesian filtering,[0],[0]
"3: for t = 1, . . .",4.1. Bayesian filtering,[0],[0]
do 4: Prediction: compute µ̂t|t−1 and Σ̂t|t−1 .,4.1. Bayesian filtering,[0],[0]
by either SSGP-EMM (§3.1) or SSGP-Lin (§3.2).,4.1. Bayesian filtering,[0],[0]
5: Measurement: make an observation yt.,4.1. Bayesian filtering,[0],[0]
6: Correction: compute µ̂t|t and Σ̂t|t according to (21) by either SSGP-EMM (§3.1) or SSGP-Lin (§3.2).,4.1. Bayesian filtering,[0],[0]
"7: end for
We summarize the resulting filtering algorithm SSGPADF (assumed density filtering) and SSGP-EKF (extended Kalman filtering), based on SSGP-EMM and SSGP-Lin, respectively, in Algorithm 1.",4.1. Bayesian filtering,[0],[0]
"These are analogs of GP-ADF (Deisenroth et al., 2009) and GP-EKF (Ko & Fox, 2009).",4.1. Bayesian filtering,[0],[0]
"The stochastic model predictive control (MPC) problem is to choose a control sequence that minimizes the expected cost, provided p(xt):
u?t+1:t+T = argmin ut+1:t+T
E ( h(xt+T ) +",4.2. Stochastic Model Predictive Control,[0],[0]
"i+T∑ i l(xt+i, ut+i) ) ,
at each time step, subject to stochastic system dynamics (17), where function h : Rd → R and l : Rd ×Rr",4.2. Stochastic Model Predictive Control,[0],[0]
→ R are the final and running cost respectively.,4.2. Stochastic Model Predictive Control,[0],[0]
"There are two main challenges to applying MPC in practice: 1) MPC requires an accurate dynamics model for multi-step prediction, and 2) online optimization is very computationally expensive.",4.2. Stochastic Model Predictive Control,[0],[0]
"For clarity in presentation, we will assume that the state is fully observable henceforth.
",4.2. Stochastic Model Predictive Control,[0],[0]
"Algorithm 2 MPC via probabilistic trajectory optimization (1-3: offline optimization, 4-8: online optimization)
1: Model learning: collect dataset D, and learn SSGP dynamics model (§2).",4.2. Stochastic Model Predictive Control,[0],[0]
"2: Initialization: set t = 0, and estimate p(x0).",4.2. Stochastic Model Predictive Control,[0],[0]
"3: Trajectory optimization: perform trajectory optimiza-
tion in belief space, obtain u?t+1:t+T .",4.2. Stochastic Model Predictive Control,[0],[0]
"4: repeat 5: Policy execution: apply one-step control u?t+1 to the system and move one step forward, update t = t+1.",4.2. Stochastic Model Predictive Control,[0],[0]
6: Model adaptation: incorporate new data and update SSGP dynamics model.,4.2. Stochastic Model Predictive Control,[0],[0]
"7: Trajectory optimization: perform re-optimization
with the updated model.",4.2. Stochastic Model Predictive Control,[0],[0]
"Initialize with the previously optimized trajectory and obtain new u?t+1:t+T .
8: until Task terminated",4.2. Stochastic Model Predictive Control,[0],[0]
We address the aforementioned challenges by employing a combination of prediction under uncertainty and trajectory optimization.,4.2.1. MPC VIA PROBABILISTIC TRAJECTORY OPTIMIZATION,[0],[0]
"More precisely, we use SSGP-EMM or SSGP-Lin to efficiently obtain approximate Gaussian distribution over trajectory of states and perform trajectory optimization in the resultant Gaussian belief space based on differential dynamic programming (DDP) (Abbeel et al., 2007; Tassa et al., 2007).",4.2.1. MPC VIA PROBABILISTIC TRAJECTORY OPTIMIZATION,[0],[0]
Note that DDP-related methods require computation of first and second order derivatives of the dynamics and cost.,4.2.1. MPC VIA PROBABILISTIC TRAJECTORY OPTIMIZATION,[0],[0]
Our analytic moment expressions provide a robust and efficient way to compute these derivatives.,4.2.1. MPC VIA PROBABILISTIC TRAJECTORY OPTIMIZATION,[0],[0]
"Details are omitted due to space limit, but they can be found in Appendix §4.
",4.2.1. MPC VIA PROBABILISTIC TRAJECTORY OPTIMIZATION,[0],[0]
"Within the SSGP framework, we may incrementally update the posterior distribution over the feature weights w (6) given a new sample without storing or inverting the matrix A explicitly, Instead we keep track of its upper triangular Cholesky factor A = RTR (Gijsberts & Metta, 2013).",4.2.1. MPC VIA PROBABILISTIC TRAJECTORY OPTIMIZATION,[0],[0]
"Given a new sample, a rank-1 update is applied to
the Cholesky factor R, which requires O(m2) time.",4.2.1. MPC VIA PROBABILISTIC TRAJECTORY OPTIMIZATION,[0],[0]
"To cope with time-varying systems and to make the method more adaptive, we employ a forgetting factor λ ∈ (0, 1), such that the impact of the previous samples decays exponentially in time (Ljung, 1998).
",4.2.1. MPC VIA PROBABILISTIC TRAJECTORY OPTIMIZATION,[0],[0]
"Our proposed MPC algorithm, summarized in Algorithm 2, is related to several algorithms and differs in both model and controller learning.",4.2.1. MPC VIA PROBABILISTIC TRAJECTORY OPTIMIZATION,[0],[0]
"First, SSGPs are more robust to modeling error than Locally Weighted Projection Regression (LWPR) used in iLQG-LD (Mitrovic et al., 2010).",4.2.1. MPC VIA PROBABILISTIC TRAJECTORY OPTIMIZATION,[0],[0]
"See a numerical comparison in (Gijsberts & Metta, 2013).",4.2.1. MPC VIA PROBABILISTIC TRAJECTORY OPTIMIZATION,[0],[0]
"Second, we efficiently propagate uncertainty in multi-step prediction which is crucial in MPC.",4.2.1. MPC VIA PROBABILISTIC TRAJECTORY OPTIMIZATION,[0],[0]
"In contrast, AGP-iLQR (Boedecker et al., 2014) drops the input uncertainty and uses subset of regressors (SoR-GP) which lacks a principled way to select reference points.",4.2.1. MPC VIA PROBABILISTIC TRAJECTORY OPTIMIZATION,[0],[0]
"In addition, PDDP (Pan & Theodorou, 2014) uses GPs which are computationally expensive for online optimization.",4.2.1. MPC VIA PROBABILISTIC TRAJECTORY OPTIMIZATION,[0],[0]
"Two deep neural networks are used for modeling in (Yamaguchi & Atkeson, 2016), which make it difficult to perform online incremental learning, as we do here.",4.2.1. MPC VIA PROBABILISTIC TRAJECTORY OPTIMIZATION,[0],[0]
"We consider a synthetic dynamical system with groundtruth dynamics f(x) = 12x+ 25x 1+x2 and observation g(x) = 6 sin(2x) with Σ x = 1.52 and Σ y = 1 in (17,18), in a similar setting to Deisenroth et al. (2009).",5.1.1. 1D ONE-STEP FILTERING,[0],[0]
"We compare the performance of four filters, SSGP-ADF, SSGP-EKF, GPADF (Deisenroth et al., 2009) and GP-EKF (Ko & Fox, 2009).",5.1.1. 1D ONE-STEP FILTERING,[0],[0]
All models are trained using 800 samples.,5.1.1. 1D ONE-STEP FILTERING,[0],[0]
"However, for SSGP models, only 10 random Fourier features of a SE kernel are used.",5.1.1. 1D ONE-STEP FILTERING,[0],[0]
Figure 1 illustrates the comparison of filtered state distribution of a typical realization.,5.1.1. 1D ONE-STEP FILTERING,[0],[0]
We evaluate the methods by computing NLx (the negative log-likelihood of the ground truth samples in the filtered distribution) and RMSE (root-mean-square error between filtered mean and ground truth samples).,5.1.1. 1D ONE-STEP FILTERING,[0],[0]
See Table 3 for a detailed comparison.,5.1.1. 1D ONE-STEP FILTERING,[0],[0]
Our methods SSGP-ADF and SSGPEKF are able to offer close performance with their full GP counterparts but with greatly reduced computational cost.,5.1.1. 1D ONE-STEP FILTERING,[0],[0]
See Appendix §6.2 for further discussions on the comparison between SSGP-ADF and SSGP-EKF.,5.1.1. 1D ONE-STEP FILTERING,[0],[0]
We next consider a state estimation task in high-speed autonomous driving on a dirt track (Figure 2a).,5.1.2. RECURSIVE FILTERING,[0],[0]
The goal is to recursively estimate the state of an autonomous rallycar given noisy measurements.,5.1.2. RECURSIVE FILTERING,[0],[0]
"The vehicle state consists of linear velocities (x and y), heading rate, and roll angle, in body frame.",5.1.2. RECURSIVE FILTERING,[0],[0]
Controls are steering and throttle.,5.1.2. RECURSIVE FILTERING,[0],[0]
Measurements are collected by wheel speed sensors.,5.1.2. RECURSIVE FILTERING,[0],[0]
This filtering task is challenging because of the complex nonlinear dynamics and the amount of noise in the measurements.,5.1.2. RECURSIVE FILTERING,[0],[0]
"We do not use any prior model of the car, but learn the model from ground truth estimates of vehicle state generated by integrating GPS and IMU data via iSAM2 (Kaess et al., 2012).",5.1.2. RECURSIVE FILTERING,[0],[0]
"50,000 samples are collected from wheel speed sensors and ground truth state estimates from iSAM2 for training.",5.1.2. RECURSIVE FILTERING,[0],[0]
"Because of the sample size, it is too computationally expensive to use GP-based filter such as GP-ADF (Deisenroth et al., 2009).",5.1.2. RECURSIVE FILTERING,[0],[0]
"Instead, we use SSGP-ADF to perform 1,200 recursive filtering steps which correspond to 30 seconds of high-speed driving.",5.1.2. RECURSIVE FILTERING,[0],[0]
"Filtered distributions using 80 features are shown in Figure 2b, and Figure 2c shows the mean and twice the standard deviation of NLx over six 30 seconds driving with different number of features.",5.1.2. RECURSIVE FILTERING,[0],[0]
"Surprisingly, only need a small number of features is necessary for satisfactory results.",5.1.2. RECURSIVE FILTERING,[0],[0]
We consider the Puma-560 robotic arm and quadrotor systems with dynamics model specified by SSGPs.,5.2.1. TRACKING A MOVING TARGET,[0],[0]
For both tasks the goal is to track a moving target.,5.2.1. TRACKING A MOVING TARGET,[0],[0]
"In addition, the true system dynamics vary online, which necessitates both online optimization and model update, as we do here.",5.2.1. TRACKING A MOVING TARGET,[0],[0]
See Appendix §5.2 for detailed task descriptions.,5.2.1. TRACKING A MOVING TARGET,[0],[0]
"Results in terms of cost l(xt, ut) are shown in Figure 4.",5.2.1. TRACKING A MOVING TARGET,[0],[0]
"Figure 4a shows that our methods outperform iLQG-LD (Mitrovic et al., 2010) and AGP-iLQR (Boedecker et al., 2014).",5.2.1. TRACKING A MOVING TARGET,[0],[0]
The similarities and differences between these methods have been discussed in §4.2.,5.2.1. TRACKING A MOVING TARGET,[0],[0]
Figure 4b shows that model update is necessary and more features could improve performance.,5.2.1. TRACKING A MOVING TARGET,[0],[0]
We study the control of an autonomous car during extreme operating conditions (powerslide).,5.2.2. AUTONOMOUS DRIFTING,[0],[0]
The task is to stabilize the vehicle to a specified steady-state using purely longitudinal control during high-speed cornering.,5.2.2. AUTONOMOUS DRIFTING,[0],[0]
This problem has been studied in Velenis et al. (2010) where the authors developed a LQR control scheme based on a physics-based dynamics model.,5.2.2. AUTONOMOUS DRIFTING,[0],[0]
"We apply our MPC algorithm to this task without any prior model knowledge and 2,500 data points generated by the model in Velenis et al. (2010).",5.2.2. AUTONOMOUS DRIFTING,[0],[0]
SSGP-Lin is used for multi-step prediction.,5.2.2. AUTONOMOUS DRIFTING,[0],[0]
"Results and comparison to Velenis et al. (2010) are illustrated in Figure 3.
0",5.2.2. AUTONOMOUS DRIFTING,[0],[0]
"100 200 300 400 500
0 100 200 300 400 500
0 100 200 300 400 500",5.2.2. AUTONOMOUS DRIFTING,[0],[0]
We introduced two analytic moment-based approaches to prediction under uncertainty in sparse spectrum Gaussian processes (SSGPs).,6. Discussion and Conclusion,[0],[0]
"Compared to their full GP counterparts, our methods are more general: they are applicable to any continuous shift-invariant kernel.",6. Discussion and Conclusion,[0],[0]
"They also scale to larger datasets by leveraging random features with frequencies sampled from the spectral density of a given kernel (see Table 1, 2).",6. Discussion and Conclusion,[0],[0]
"Although we adopt the name SSGP, our proposed methods are not tied to specific model learning methods such as linear Bayesian regression (LázaroGredilla et al., 2010).",6. Discussion and Conclusion,[0],[0]
"They can be applied to any SSGP with a specified feature weight distribution (6), and α and A can be computed via different approaches.",6. Discussion and Conclusion,[0],[0]
"For example, A can be iteratively computed by methods like doubly stochastic gradient descent (Dai et al., 2014).",6. Discussion and Conclusion,[0],[0]
We studied the application of the proposed methods to Bayesian filtering and model predictive control.,6. Discussion and Conclusion,[0],[0]
Our methods directly address the challenging aspects of these problems: model uncertainty and real-time execution constraints.,6. Discussion and Conclusion,[0],[0]
We evaluated our algorithms on real-world and simulated examples and showed that SSGP-EMM (§3.1) and SSGP-Lin (§3.2) are accurate alternatives to their full GP counterparts when learning from large amounts of data.,6. Discussion and Conclusion,[0],[0]
This work was supported by NSF NRI awards 1637758 and 1426945.,Acknowledgements,[0],[0]
Sparse Spectrum Gaussian Processes (SSGPs) are a powerful tool for scaling Gaussian processes (GPs) to large datasets.,abstractText,[0],[0]
"Existing SSGP algorithms for regression assume deterministic inputs, precluding their use in many real-world robotics and engineering applications where accounting for input uncertainty is crucial.",abstractText,[0],[0]
We address this problem by proposing two analytic moment-based approaches with closed-form expressions for SSGP regression with uncertain inputs.,abstractText,[0],[0]
"Our methods are more general and scalable than their standard GP counterparts, and are naturally applicable to multi-step prediction or uncertainty propagation.",abstractText,[0],[0]
"We show that efficient algorithms for Bayesian filtering and stochastic model predictive control can use these methods, and we evaluate our algorithms with comparative analyses and both real-world and simulated experiments.",abstractText,[0],[0]
Prediction under Uncertainty in Sparse Spectrum Gaussian Processes  with Applications to Filtering and Control,title,[0],[0]
"Let f : X → R be a black-box function defined on a hypercube X = ∏D d=1[ad, bd] where D ≥ 2.",1. Introduction,[0],[0]
Without loss of generality we assume that 0 ∈ X .,1. Introduction,[0],[0]
"The objective is to find a global minimizer
x∗ = argmin x∈X f(x).",1. Introduction,[0],[0]
"(1)
We assume, as in Preferential Bayesian Optimization (PBO, González et al., 2017), that f is not directly accessible.",1. Introduction,[0],[0]
"In PBO, queries to f can done in pairs of points x,x′ ∈ X , and the binary feedback indicates whether f(x) > f(x′).",1. Introduction,[0],[0]
"In contrast, in our work we assume that queries to f are be done
1Helsinki Institute for Information Technology HIIT, Department of Computer Science, Aalto University, Espoo, Finland 2Department of Applied Physics, Aalto University, Espoo, Finland 3The University of Manchester, UK.",1. Introduction,[0],[0]
"Correspondence to: Petrus Mikkola <petrus.mikkola@aalto.fi>, Samuel Kaski <samuel.kaski@aalto.fi>.
Proceedings of the 37 th International Conference on Machine Learning, Online, PMLR 119, 2020.",1. Introduction,[0],[0]
"Copyright 2020 by the author(s).
",1. Introduction,[0],[0]
Figure 1.,1. Introduction,[0],[0]
An illustration of a projective preferential query on molecular properties: in which rotation is the molecule most likely to bind to the surface.,1. Introduction,[0],[0]
"Here, x ∈ X describes the location and orientation of a molecule as a vector x = (X,Y, Z, a, b, c).",1. Introduction,[0],[0]
"In the figure, the projective preferential query (ξ,x) finds the optimal rotation along the horizontal plane, defined by the coordinate b.",1. Introduction,[0],[0]
"This corresponds to setting ξi = 0 to all coordinates i except the one corresponding to b, and by rotating the molecule the expert then gives the optimal value α∗ that corresponds to the optimal value for coordinate b.",1. Introduction,[0],[0]
"The other coordinates are kept fixed (these are determined by x).
over the projection onto a projection vector ξ ∈ Ξ ⊂ X .",1. Introduction,[0],[0]
"The feedback is the optimal scalar projection, that is, the length α∗ of the projection in the direction ξ.",1. Introduction,[0],[0]
"We assume that there are zero coordinates in ξ, and these coordinates are set to fixed values described by a reference vector x ∈ X .",1. Introduction,[0],[0]
"Formally, given a query (ξ,x), the feedback is obtained as a minimizer over the possible scalar projections,
α∗ = argmin α∈Iξ f(αξ + x), (2)
where Iξ ≡ {α ∈ R|αξ + x ∈ X}.
",1. Introduction,[0],[0]
What are then natural use cases for such projective preferential queries?,1. Introduction,[0],[0]
The main motivation comes from humans serving as the oracles.,1. Introduction,[0],[0]
"The form of the query enables efficient learning of user preferences over choice sets in which each choice has multiple attributes, and in particular over continuous choice sets.",1. Introduction,[0],[0]
"An important application is knowledge elicitation from trained professionals (doctors, physi-
ar X
iv :2
00 2.
03 11
3v 4
[ st
at .M
L ]
1 4
A ug
2 02
0
cists, etc.).",1. Introduction,[0],[0]
"For example, we may learn a material scientists preferences, that is, insight based on prior knowledge and experience, over molecular translations and orientations as a molecule adsorbs to a surface.",1. Introduction,[0],[0]
"In this case, a projective preferential query could correspond to finding an optimal rotation (see Figure 1), which the scientists can easily give by rotating the molecule in a visual interface.
",1. Introduction,[0],[0]
"Probabilistic preference learning is a relatively new topic in machine learning research but has a longer history in econometrics and psychometrics (McFadden, 1981; 2001; Stern, 1990; Thurstone, 1927).",1. Introduction,[0],[0]
"A wide range of applications of these models exists, for instance in computer graphics (Brochu et al., 2010), expert knowledge elicitation (Soufiani et al., 2013), revenue management systems of airlines (Carrier & Weatherford, 2015), rating systems, and almost any application that contains users’ preference modeling.",1. Introduction,[0],[0]
"An established probabilistic model is Thurstone-Mosteller (TM) model that measures a process of pairwise comparisons (Thurstone, 1927; Mosteller, 1951).",1. Introduction,[0],[0]
"In the preference learning context, the models based on the TM-model can be applied to learning preferences from pairwise comparison feedback (e.g. Chu & Ghahramani, 2005).",1. Introduction,[0],[0]
"An extension of this research into the interactive learning setting is studied by, among others, Brochu et al. (2008) and González et al. (2017).",1. Introduction,[0],[0]
All these approaches resort to pairwise feedbacks.,1. Introduction,[0],[0]
Koyama et al. (2017) proposed an extension of Bayesian optimization for learning optimal parameter values for visual design tasks by letting a user give feedback as a slider manipulation.,1. Introduction,[0],[0]
"They considered only two pairwise comparisons per slider since they tested to “included more sampling points” but they “did not observe any significant improvement in the optimization behavior”.
",1. Introduction,[0],[0]
A drawback of most preference learning frameworks is their incapability to handle high-dimensional input spaces.,1. Introduction,[0],[0]
"The underlying reason is a combinatorial explosion in the number of possible comparisons with respect to the number of dimensions D, O(K2D), given K grid-points per dimension.",1. Introduction,[0],[0]
This implies that a single pairwise comparison has low information content in high-dimensional spaces.,1. Introduction,[0],[0]
This problem was mitigated by González et al. (2017) by capturing the correlations among pairs of queries (duels).,1. Introduction,[0],[0]
"However, it is still difficult to scale that method to high-dimensional spaces, say higher than 2-dimensional (see Section 4).",1. Introduction,[0],[0]
"Furthermore, the numerical computations become infeasible in a high-dimensional setting, especially the optimization of an acquisition function or finding a Condorcet winner.
",1. Introduction,[0],[0]
"In this paper, we introduce a Bayesian framework, which we call Projective Preferential Bayesian Optimization (PPBO), that scales to high-dimensional input spaces.",1. Introduction,[0],[0]
A main reason is that the information content of a projective preferential query is much higher than that of a pairwise preferential query.,1. Introduction,[0],[0]
"A projective preferential query is equiv-
alent to infinite pairwise comparisons along a projection.",1. Introduction,[0],[0]
"An important consequence is that with projective preferential queries, the user’s workload in answering the queries will be considerably reduced.",1. Introduction,[0],[0]
Source code is available at https://github.com/AaltoPML/PPBO.,1. Introduction,[0],[0]
In this section we introduce a Bayesian framework capable of dealing with projective preferential data.,2. Learning preferences from projective preferential feedback,[0],[0]
"A central idea is to model the user’s utility function, that is f , as a Gaussian process as first proposed by Chu & Ghahramani (2005).",2. Learning preferences from projective preferential feedback,[0],[0]
"We extend this line of study to allow projective preferential queries, by deriving a tractable likelihood, proposing a method to approximate it, and introducing four acquisition criteria for enabling interactive learning in this setting.
",2. Learning preferences from projective preferential feedback,[0],[0]
"In this paper, for convenience, we will formulate the method for maximization instead of minimization as in (2), without loss of generality.",2. Learning preferences from projective preferential feedback,[0],[0]
"Our probabilistic model of user preferences is built upon the Thurstone’s law of comparative judgement (Thurstone, 1927).",2.1. Likelihood,[0],[0]
"A straightforward way to formalize this would be to assume pairwise comparisons are corrupted by Gaussian noise: x x′, if and only if f(x) + ε > f(x′) + ε′, where the latent function f is a utility function that characterizes user preferences described by the preference relation .",2.1. Likelihood,[0],[0]
The standard assumption is that ε and ε′ are identically and independently distributed Gaussians.,2.1. Likelihood,[0],[0]
"Here, we deviate slightly from this assumption:",2.1. Likelihood,[0],[0]
"Given two alternatives (αξ + x), (βξ+x) ∈ X , we assume that αξ+x βξ+x, if and only if f(αξ + x) +W (α) > f(βξ + x) +W (β), where W is a Gaussian white noise process with zeromean and autocorrelation E(W (t)W (t+ τ))",2.1. Likelihood,[0],[0]
"= σ2 if τ = 0, and zero otherwise.
",2.1. Likelihood,[0],[0]
"We would like to find the likelihood for an observation (α, (ξ,x)) that corresponds to uncountably infinite pairwise comparisons: αξ+x βξ+x for β 6= α.",2.1. Likelihood,[0],[0]
"For each comparison we condition on W (α) (more details in Supplementary material),
P (αξ + x βξ + x |W (α) = w) = 1− Φ ( f(βξ + x)−",2.1. Likelihood,[0],[0]
f(αξ,2.1. Likelihood,[0],[0]
"+ x)− w
σ
) ,
where Φ is the cumulative distribution function of the standard normal distribution.",2.1. Likelihood,[0],[0]
"For a single comparison we have
P (αξ + x βξ + x) = 1−",2.1. Likelihood,[0],[0]
"[Φ ∗ φ] ( f(βξ + x)− f(αξ + x)
σ
) ,
where φ is the probability density of the standard normal distribution and ∗ is the convolution operator.",2.1. Likelihood,[0],[0]
"For infinite comparisons, we first consider a finite number of comparisons m. By their independence, we have
P (αξ + x β1ξ",2.1. Likelihood,[0],[0]
+,2.1. Likelihood,[0],[0]
"x, ..., αξ + x βmξ + x)
= m∏ j=1 ( 1− [Φ ∗ φ] ( f(βjξ + x)− f(αξ + x) σ )) .
",2.1. Likelihood,[0],[0]
"By letting the number of points m in an increasing sequence β1, ..., βm of the partition of the interval Iξ\{α} to approach infinity, we can interpret this as a Volterra (product) integral
exp",2.1. Likelihood,[0],[0]
( − ∫ Iξ,2.1. Likelihood,[0],[0]
[Φ ∗ φ] (f(βξ + x)− f(αξ + x) σ ),2.1. Likelihood,[0],[0]
"dβ ) .
",2.1. Likelihood,[0],[0]
"The joint log-likelihood of a dataset D, denoted as L(D|f), takes the form
− N∑ i=1",2.1. Likelihood,[0],[0]
∫,2.1. Likelihood,[0],[0]
Iξi,2.1. Likelihood,[0],[0]
[Φ ∗ φ] (f(βξi + xi)−,2.1. Likelihood,[0],[0]
f(αiξi + xi) σ ),2.1. Likelihood,[0],[0]
dβ.,2.1. Likelihood,[0],[0]
"First, we introduce notation.",2.2. Prior,[0],[0]
"Assume that N projective preferential queries have been performed and gathered into a dataset D = {(αi, (ξi,xi))}Ni=1.",2.2. Prior,[0],[0]
"For every data instance (αi, (ξi,xi)), we also consider a sequence of pseudoobservations {(βijξ
i,xi)}mj=1.",2.2. Prior,[0],[0]
"Technically, the pseudoobservations are Monte-Carlo samples needed for integrating the likelihood.",2.2. Prior,[0],[0]
"The latent function values evaluated on those points are gathered into a vector,
f (i) ≡",2.2. Prior,[0],[0]
"( f(αiξi + xi), {f(βijξ i + xi)}mj=1 ) .
",2.2. Prior,[0],[0]
The latent function vector over all points is formed by concatenating over f ≡,2.2. Prior,[0],[0]
"(f (i))Ni=1.
",2.2. Prior,[0],[0]
"The user’s utility function f is modelled as a Gaussian process (Rasmussen & Williams, 2005).",2.2. Prior,[0],[0]
"GP model fits ideally to this objective, since it is flexible (non-parametric) and can conveniently handle uncertainty (predictive distributions can be derived analytically).",2.2. Prior,[0],[0]
"In particular, it allows us to have insight into those regions of the space X in which either we are uncertain about user preferences due to lack of data, or because the user gives inconsistent feedback.",2.2. Prior,[0],[0]
"A possible reason for the latter is that one of the preference axioms, transitivity or completeness, is violated in those regions.",2.2. Prior,[0],[0]
"A weak preference relation is complete if for all x,y ∈ X , either x y or y x holds.",2.2. Prior,[0],[0]
"That is, a user is able to reveal their preferences over all possible pairwise comparisons.",2.2. Prior,[0],[0]
"Similarly, is transitive if for any x,y, z ∈ X",2.2. Prior,[0],[0]
the following holds: (x y and y z) implies that,2.2. Prior,[0],[0]
x z.,2.2. Prior,[0],[0]
"That is, a user has consistent preferences.",2.2. Prior,[0],[0]
"This together
with the continuity of guarantees the existence of a realvalued continuous utility function that represents (Debreu, 1954).
",2.2. Prior,[0],[0]
"Thus, we assume as Chu & Ghahramani (2005), that the prior of the utility function follows a zero-mean Gaussian process,
p(f)",2.2. Prior,[0],[0]
"= 1
(2π) N 2 |Σ| 12 exp(−1 2 f>Σ−1f),
where the ijth-element of the covariance matrix is determined by a kernel k as Σij = k(xi,xj).",2.2. Prior,[0],[0]
"Throughout the paper, we assume the squared exponential kernel k(x,x) = σ2f exp( 1 −2l ‖x− x
′‖2), where the σf and l are hyperparameters.",2.2. Prior,[0],[0]
"For the sake of simplicity, we use the Laplace approximation for the posterior distribution.",2.3. Posterior,[0],[0]
"A maximum a posteriori (MAP) estimate is needed for that,
argmax f P(f |D) = argmax f (P(f)L(D|f))",2.3. Posterior,[0],[0]
= argmax f T,2.3. Posterior,[0],[0]
"(f),
where we denote the functional (log-scaled posterior)
T (f) ≡ −1",2.3. Posterior,[0],[0]
"2 f>Σ−1f
− N∑ i=1",2.3. Posterior,[0],[0]
∫,2.3. Posterior,[0],[0]
Iξi,2.3. Posterior,[0],[0]
[Φ ∗ φ] (f(βξi + xi)−,2.3. Posterior,[0],[0]
f(αiξi + xi) σ ),2.3. Posterior,[0],[0]
"dβ.
",2.3. Posterior,[0],[0]
The convolution Φ ∗ φ can be efficiently approximated by Gauss-Hermite quadrature.,2.3. Posterior,[0],[0]
"The outer integral is approximated as a Monte-Carlo integral,∫ Iξ",2.3. Posterior,[0],[0]
[Φ ∗ φ] (f(βξ + x)− f(αξ + x) σ ),2.3. Posterior,[0],[0]
"dβ
≈ `(Iξ) m m∑ j=1",2.3. Posterior,[0],[0]
"[Φ ∗ φ] (f(βjξ + x)− f(αξ + x) σ ) ,
where the pseudo-observations (βjξ)mj for j = 1, ...,m are sampled from a suitable distribution.",2.3. Posterior,[0],[0]
"Our choice is to use a family of truncated generalized normal (TGN) distributions, since it provides a continuous transformation from the uniform distribution to the truncated normal distribution, such that the locations of distributions can be specified.",2.3. Posterior,[0],[0]
The idea is to concentrate pseudo-observations more densely around the optimal value αξ as the number of queries increases.,2.3. Posterior,[0],[0]
"For more details, see Supplementary material.
",2.3. Posterior,[0],[0]
"For notational convenience, define
∆i,j(f) ≡",2.3. Posterior,[0],[0]
f(βijξ,2.3. Posterior,[0],[0]
i + xi)−,2.3. Posterior,[0],[0]
"f(αiξi + xi) σ .
",2.3. Posterior,[0],[0]
"If the domain is normalized to X = ∏D d=1[0, 1], and the projections are normalized to ξ = ξ‖ξ‖∞ , then `(Iξ) = 1.",2.3. Posterior,[0],[0]
"Hence, under this normalization, the functional T can be approximated as
T (f)",2.3. Posterior,[0],[0]
≈ −1 2 f>Σ−1f,2.3. Posterior,[0],[0]
− 1 m N∑ i=1,2.3. Posterior,[0],[0]
m∑ j=1,2.3. Posterior,[0],[0]
"[Φ ∗ φ] ( ∆i,j(f) ) .
",2.3. Posterior,[0],[0]
"The MAP estimate can be efficiently solved by a secondorder iterative optimization algorithm, since the gradient and the Hessian can be easily derived for T .
",2.3. Posterior,[0],[0]
The Laplace approximation of the posterior amounts to the second-order Taylor approximation of the log posterior around the MAP estimate.,2.3. Posterior,[0],[0]
"In the ordinary (non-log) scale, this reads P(f |D)",2.3. Posterior,[0],[0]
"≈ P(fMAP|D) exp ( − 1
2 (f − fMAP)>H(f − fMAP)
) ,
where the matrix H is the negative Hessian of the log-posterior at the MAP estimate, H ≡ −∇∇ log P(f |D)|f=fMAP =",2.3. Posterior,[0],[0]
Σ−1,2.3. Posterior,[0],[0]
"+ Λ.1 In other words, the posterior distribution is approximated as a multivariate normal distribution with mean fMAP and the covariance matrix (Σ−1 + Λ)−1.",2.3. Posterior,[0],[0]
"Based on the well-known properties of the multivariate Gaussian distribution, the predictive distribution of f is also Gaussian.",2.4. Predictive distribution,[0],[0]
"Given test locations (y(1), ...,y(M)), consider the N by M matrix K ≡",2.4. Predictive distribution,[0],[0]
"[k(y(j),x(i))]ij .",2.4. Predictive distribution,[0],[0]
"The predictive mean and the predictive covariance at test locations are (for more details see (Rasmussen & Williams, 2005) or (Chu & Ghahramani, 2005))
µpred = K >Σ−1fMAP Σpred = Σ ′ −K>(Σ + Λ−1)−1K,
where Σ′ is the covariance matrix of the test locations.",2.4. Predictive distribution,[0],[0]
"In this section, we discuss how to select the next projective preferential query (ξ,x).",3. Sequential learning by projective preferential query,[0],[0]
"We will choose the next query as a maximizer of an acquisition function α(ξ,x), for instance, we will consider a modified version of the expected
1We denote the partial derivatives matrix evaluated at MAP estimate as
Λ ≡ ∂ 2 ∂f∂f> 1 m N∑ i=1",3. Sequential learning by projective preferential query,[0],[0]
m∑ j=1,3. Sequential learning by projective preferential query,[0],[0]
"[Φ ∗ φ] ( ∆i,j(f) )∣∣∣∣ f=fMAP .
",3. Sequential learning by projective preferential query,[0],[0]
"improvement acquisition function (Jones et al., 1998).",3. Sequential learning by projective preferential query,[0],[0]
"The optimization (ξ,x)next = argmax(ξ,x) α(ξ,x) is carried out by using Bayesian optimization (more details in Supplementary material).
",3. Sequential learning by projective preferential query,[0],[0]
"If the oracle is a human, this allows us to learn user preferences in an iterative loop, making PPBO interactive.",3. Sequential learning by projective preferential query,[0],[0]
"However, this interesting special case, where f is a utility function of a human, also brings forth issues due to bounded rationality.",3. Sequential learning by projective preferential query,[0],[0]
"We apply here the following narrow definition of this more general concept (see Simon, 1990):",3. Sequential learning by projective preferential query,[0],[0]
"Bounded rationality is the idea that users give feedback that reflects their preferences, but within the limits of the information available to them and their mental capabilities.",3. Sequential learning by projective preferential query,[0],[0]
"If the oracle is a human, it is important to realize that the optimal next (ξ,x) is not solely the one which optimally balances the exploration-exploitation trade-off – as it is for a perfect oracle – but the optimal (ξ,x) takes also into account human cognitive capabilities and limitations.",3.1. The effects of bounded rationality on the optimal next query,[0],[0]
"For instance, the more there are non-zero coordinates in ξ, the greater the “cognitive burden” to a human user, and the harder it becomes to give useful feedback.",3.1. The effects of bounded rationality on the optimal next query,[0],[0]
"Thus, if there is a human in the loop, the choice of ξ should take into account both the optimization needs and what types of queries are convenient for the user.
",3.1. The effects of bounded rationality on the optimal next query,[0],[0]
"The projective preferential feedback (2) may not be singlevalued or even well-defined for all ξ ∈ Ξ, if the oracle is a human.",3.1. The effects of bounded rationality on the optimal next query,[0],[0]
"For instance, the user may not be able to explicate their preferences with respect to the dth attribute, that is, the preferences do not satisfy the completeness axiom.",3.1. The effects of bounded rationality on the optimal next query,[0],[0]
"Formally, this means that if ξ = ed (the dth-standard unit vector), then for some x ∈ X it holds that argmaxα∈Iξ f(αξ + x) is multi-valued, a random variable or not well-defined – depending on how we interpret the scenario in which the user should say “I do not know” but instead gives an arbitrary feedback.",3.1. The effects of bounded rationality on the optimal next query,[0],[0]
"Fortunately, this incompleteness can be easily handled when a GP is used for modelling f ; it just implies that the posterior variance is high along the dimension d.
A possible solution would be to allow the answer “I do not know”, and to design an acquisition function that is capable of discovering and avoiding those regions in the space X where the user gives inconsistent feedback due to any source of bounded rationality.",3.1. The effects of bounded rationality on the optimal next query,[0],[0]
This challenge is left for future research.,3.1. The effects of bounded rationality on the optimal next query,[0],[0]
"It is noteworthy that the acquisition function we introduce next, performed well in the user experiment covered in Section 5.",3.1. The effects of bounded rationality on the optimal next query,[0],[0]
"We define the expected improvement by projective preferential query at the nth-iteration by
EIn(ξ,x) ≡",3.2. Expected improvement by projective preferential query,[0],[0]
"En ( max {
max α∈Iξ
f(αξ + x)− µ∗n, 0 }) , (3)
where µ∗n denotes the highest value of the predictive posterior mean, and the expectation is conditioned on the data up to the nth-iteration.",3.2. Expected improvement by projective preferential query,[0],[0]
"The maximum over α models the anticipated feedback.
",3.2. Expected improvement by projective preferential query,[0],[0]
"EIn can be approximated as a Monte-Carlo integral (up to a multiplicative constant that does not depend on (ξ,x)),
1
K K∑ k=1 max { max α∈Iξ f̃k(αξ + x)− µ∗n, 0 } , (4)
where maxα∈Iξ f̃k(αξ + x) is approximated by using discrete2 Thompson sampling as described in (HernándezLobato et al., 2014).",3.2. Expected improvement by projective preferential query,[0],[0]
"Discrete Thompson sampling draws a finite sample from the GP posterior distribution, and then returns the maximum over the sample.",3.2. Expected improvement by projective preferential query,[0],[0]
"The steps needed to approximate EIn are summarized in Algorithm 1.
",3.2. Expected improvement by projective preferential query,[0],[0]
"Algorithm 1 Approximate EIn(ξ,x) input (ξ,x) and K ≥ 1, J ≥ 1
1.",3.2. Expected improvement by projective preferential query,[0],[0]
"Compute (Σ + Λ−1)−1 for k = 1, 2, ...,K do 2.",3.2. Expected improvement by projective preferential query,[0],[0]
Draw (βj)Jj=1 3.,3.2. Expected improvement by projective preferential query,[0],[0]
Draw ( f̃(βjξ +x) ),3.2. Expected improvement by projective preferential query,[0],[0]
"J j=1
from the predictive distribution N ( µpred,Σpred ) 4.",3.2. Expected improvement by projective preferential query,[0],[0]
zk ← maxj {( f̃(βjξ + x) ),3.2. Expected improvement by projective preferential query,[0],[0]
"J j=1
} end for
output 1K ∑K k=1 max {",3.2. Expected improvement by projective preferential query,[0],[0]
"zk − µ∗n, 0 }",3.2. Expected improvement by projective preferential query,[0],[0]
The bottlenecks are the first and the third steps.,3.2. Expected improvement by projective preferential query,[0],[0]
"In the third step, a predictive covariance matrix of size J×J needs to be computed, and then a sample from the multivariate normal distribution needs to be drawn.",3.2. Expected improvement by projective preferential query,[0],[0]
"Hence, the time complexity of Algorithm 1 is O(N3m3 +KN2m2J +KNmJ2 + KJ3), where the terms come from a matrix inversion (the first step), two matrix multiplications, and a Cholesky decomposition, respectively.",3.2. Expected improvement by projective preferential query,[0],[0]
"Recall that N,m,K and J refer to the number of observations, pseudo-observations, MonteCarlo samples, and grid points, respectively.
2Another alternative is to consider continuous Thompson sampling to draw a continuous sample path from the GP model, and then maximize it.",3.2. Expected improvement by projective preferential query,[0],[0]
The method is based on Bochner’s theorem and the equivalence between a Bayesian linear model with random features and a Gaussian process.,3.2. Expected improvement by projective preferential query,[0],[0]
"For more details see (HernándezLobato et al., 2014).",3.2. Expected improvement by projective preferential query,[0],[0]
In the experiments we use pure exploration and exploitation as baselines.,3.3. Pure exploitation and exploration,[0],[0]
"A natural interpretation of pure exploitation in our context is to select the next query (ξ,x) such that ξ + x = x∗ ≡ argmaxx′∈X µn(x′), where µn(x′) is the posterior mean of the GP model at location x′, given all the data so far Dn.
",3.3. Pure exploitation and exploration,[0],[0]
"We interpret pure exploration as maximization of the GP variance on a query (ξ,x) given the anticipated feedback.",3.3. Pure exploitation and exploration,[0],[0]
"That is, the pure explorative acquisition strategy maximizes the following acquisition function
Explore(ξ,x) ≡",3.3. Pure exploitation and exploration,[0],[0]
"Vn (
max α∈Iξ
f(αξ + x) ) .",3.3. Pure exploitation and exploration,[0],[0]
"(5)
In practice, (5) is approximated by Monte-Carlo integration and discrete Thompson sampling in the same vein as in Algorithm 1.",3.3. Pure exploitation and exploration,[0],[0]
"The fourth acquisition strategy corresponds to the interesting special case where ξ = ed (the dth-standard unit vector), and the coordinates d are rotated in a cyclical order for each query.",3.4. Preferential coordinate descent (PCD),[0],[0]
"The reference vector x = (x1, ..., xd−1, 0, xd+1, ..., xD) can be chosen in several ways, but it is natural to consider an exploitative strategy in which x is set to x∗ except for the dth-coordinate which is set to zero.",3.4. Preferential coordinate descent (PCD),[0],[0]
"We call this acquisition strategy Preferential Coordinate Descent (PCD), since PPBO with PCD acquisition is closely related to a coordinate descent algorithm that successively minimizes an objective function along coordinate directions.",3.4. Preferential coordinate descent (PCD),[0],[0]
"The PPBO method with PCD acquisition (PPBO-PCD) differs from the classical coordinate descent in two ways: First, PPBO-PCD assumes that direct function evaluations are not possible but instead projective preferential queries are.",3.4. Preferential coordinate descent (PCD),[0],[0]
"Second, it models the black-box function f (as a GP) whereas the classical coordinate descent does not.",3.4. Preferential coordinate descent (PCD),[0],[0]
"This makes PPBO-PCD able to take advantage of past queries from every one-dimensional optimization.
",3.4. Preferential coordinate descent (PCD),[0],[0]
"When comparing to other acquisition strategies, we show that PCD performs well in numerical experiments (when f is not a utility function but a numerical test function).",3.4. Preferential coordinate descent (PCD),[0],[0]
"This agrees with the results in the optimization literature; for instance: if f is pseudoconvex with continuous gradient, and X is compact and convex with “nice boundary”, then the coordinate descent algorithm converges to a global minimum (Spall, 2012, Corollary 3.1).",3.4. Preferential coordinate descent (PCD),[0],[0]
"However, PCD may not perform so well in high-dimensional spaces, since it cannot query in between the dimensions.",3.4. Preferential coordinate descent (PCD),[0],[0]
"For instance, the expected improvement by projective preferential query outperformed PCD on a 20D test function (see Section 4), since it allows to query arbitrary projections.",3.4. Preferential coordinate descent (PCD),[0],[0]
"In this section we demonstrate the efficiency of the PPBO method in high-dimensional spaces, and experiment with various acquisition strategies in numerical experiments on simulated functions.
",4. Numerical experiments,[0],[0]
The goal is to find a global minimum of a black-box function f by querying it either through (i) pairwise comparisons or (ii) projective preferential queries.,4. Numerical experiments,[0],[0]
"For (i) we use the PBO method of González et al. (2017), which is state of the art among Gaussian process preference learning frameworks that are based on pairwise comparisons.",4. Numerical experiments,[0],[0]
For (ii) we use the PPBO method as introduced in this paper.,4. Numerical experiments,[0],[0]
"The four different acquisition strategies introduced in Section 3 are compared against the baseline that samples a random (ξ,x).",4. Numerical experiments,[0],[0]
"For the PBO method, we consider a random and a duelingThompson sampling acquisition strategies.",4. Numerical experiments,[0],[0]
"In total seven different methods are compared: the expected improvement by projective preferential query (PPBO-EI), pure exploitation (PPBO-EXT), pure exploration (PPBO-EXR), preferential coordinate descent (PPBO-PCD), random (PPBO-RAND), and for the PBO; random (PBO-RAND) and a variant of dueling-Thompson sampling (PBO-DTS).",4. Numerical experiments,[0],[0]
"For more details, see Supplementary material.
",4. Numerical experiments,[0],[0]
"For f we consider four different test functions: Six-humpcamel2D, Hartmann6D, Levy10D and Ackley20D.3 We add a small Gaussian error term to the test function outputs.",4. Numerical experiments,[0],[0]
There are as many initial queries as there are dimensions in a test function.,4. Numerical experiments,[0],[0]
"The ith-initial query corresponds to ξ = ei, that is, to the ith-coordinate projection, and the reference vector x is uniformly random.",4. Numerical experiments,[0],[0]
We consider a total budget of 100 queries.,4. Numerical experiments,[0],[0]
"The results are depicted in Figure 2.4
PPBO-PCD obtained the best performance on three of the four test functions.",4. Numerical experiments,[0],[0]
"On the high-dimensional test function Ackley20D, PPBO-EI performed best.",4. Numerical experiments,[0],[0]
"Unsurprisingly, all PPBO variants clearly outperformed all PBO variants.",4. Numerical experiments,[0],[0]
"Since the performance gap between PPBO-RAND and PBO-RAND is so high, we conclude that from the optimization perspective; whenever a projective preferential query is possible, a PPBO-type of approach should be preferred to an approach that is based on pairwise comparisons.",4. Numerical experiments,[0],[0]
"However, we note that it is better to think of PPBO as a complement, not as a substitute for PBO.",4. Numerical experiments,[0],[0]
"In the applications, pairwise comparisons may be preferred, for instance, if they are more convenient to a user, and the underlying choice space is low-dimensional.
",4. Numerical experiments,[0],[0]
"To illustrate the low information content of pairwise comparisons, we ran a test on the Six-hump-camel2D function.
",4. Numerical experiments,[0],[0]
3https://www.sfu.ca/∼ssurjano/optimization.html 4All experiments of each test function were run on a computing infrastructure of 24x Xeon Gold 6148 2.40GHz cores and 72GB RAM.,4. Numerical experiments,[0],[0]
"The longest experiment (Ackley20D) took in total 24h.
",4. Numerical experiments,[0],[0]
"We trained a GP classifier with 2000 random queries (duels), and found a Condorcet winner (see González et al., 2017) by maximizing the soft-Copeland score (33 × 33 MC-samples used for the integration) by using Bayesian optimization (500 iterations with 10 optimization restarts).",4. Numerical experiments,[0],[0]
"This took 41 minutes on the 8th-gen Intel i5-CPU, and the distance to a true global minimizer was ‖xc − xtrue‖ = ‖(0.1770,−0.0488)− (0.0898,−0.7126)‖ ≈ 0.67, and the corresponding function value was 0.1052 compared to a true global minimum value −1.0316.",4. Numerical experiments,[0],[0]
"In contrast, PPBORAND reached this level of accuracy at the first queries, as seen from Figure 2.",4. Numerical experiments,[0],[0]
"In this section we demonstrate the capability of PPBO to correctly and efficiently encode user preferences from projective preferential feedback.
",5. User experiment,[0],[0]
We consider a material science problem of a single organic molecule adsorbing to an inorganic surface.,5. User experiment,[0],[0]
"This is a key step in understanding the structure at the interface between organic and inorganic films inside electronic devices, coatings, solar cells and other materials of technological relevance.",5. User experiment,[0],[0]
"The molecule can bind in different adsorption configurations, altering the electronic properties at the interface and affecting device performance.",5. User experiment,[0],[0]
Exploring the structure and property phase space of materials with accurate but costly computer simulations is a difficult task.,5. User experiment,[0],[0]
Our objective is to find the most stable surface adsorption configuration through human intuition and subsequent computer simulations.,5. User experiment,[0],[0]
"The optimal configuration is the one that minimises the computed adsorption energy.
",5. User experiment,[0],[0]
"Our test case is the adsorption of a non-symmetric, bulky molecule camphor on the flat surface of (111)-plane terminated Cu slab.",5. User experiment,[0],[0]
Some understanding of chemical bonding is required to infer correct adsorption configurations.,5. User experiment,[0],[0]
The user is asked to consider the adsorption structure as a function of molecular orientation and translation near the surface.,5. User experiment,[0],[0]
"These are represented with 6 physical variables: angles α, β, γ of molecular rotation around the X, Y, Z Cartesian axes (in the range",5. User experiment,[0],[0]
"[0, 360] deg.), and distances x, y, z of translation above the surface (with lattice vectors following the translational symmetry of the surface).",5. User experiment,[0],[0]
The internal structures of the molecule and surface were kept fixed since little structural deformation is expected with adsorption.,5. User experiment,[0],[0]
"A similar organic/inorganic model system and experiment scenario was previously employed to detect the most stable surface structures with autonomous BO, given the energies of sampled configurations (Todorović et al., 2019).
",5. User experiment,[0],[0]
"In this interactive experiment, the users encode their preferred adsorption geometry as a location in the 6- dimensional phase space.",5. User experiment,[0],[0]
"We employ the quantum-
mechanical atomistic simulation code FHI-aims (Blum et al., 2009) to i) compute the adsorption energy E of this preferred choice, and ii) optimise the structure from this initial position to find the nearest local energy minimum in phase space, E*.",5. User experiment,[0],[0]
"We also consider the number of optimization steps N needed to reach the nearest minimum as a measure of quality of the initial location.
",5. User experiment,[0],[0]
"There are four different test users: two materials science experts (human: a PhD student and an experienced researcher, both of them know the optimal solution), a non-expert (human), and a random bot (computer).",5. User experiment,[0],[0]
The hypothesis is that: the materials science experts should obtain structures associated with lower energy minimum points.,5. User experiment,[0],[0]
"We consider only coordinate projections, that is ξ ∈ {e1, ..., e6}.",5. User experiment,[0],[0]
"In other words, we let the user choose the optimal value for one dimension at a time.
",5. User experiment,[0],[0]
"The total number of queries was 24, of which 6 were initial queries.",5. User experiment,[0],[0]
"The ith-initial query corresponded to ξ = ei, that is, to the ith-coordinate projection.",5. User experiment,[0],[0]
The initial values for the reference coordinate vector x were fixed to the same value across all user sessions.,5. User experiment,[0],[0]
"For acquisition, we used the expected improvement by projective preferential query.",5. User experiment,[0],[0]
"Since we allowed only coordinate projections for ξ, we first selected ξn+1 =",5. User experiment,[0],[0]
"argmaxξ∈{e1,...,e6} ∫ EIn(ξ,x)dx,
and then, either xn+1",5. User experiment,[0],[0]
"= argmaxx µn(x) (EI-EXT), or the next xn+1 was drawn uniformly at random (EI-RAND).",5. User experiment,[0],[0]
"The computer bot gave random values to the queries; to provide some consistency to the bot, xn+1 was selected by maximizing a standard expected improvement function (EI-EI).",5. User experiment,[0],[0]
"The results are summarized in Table 1.
",5. User experiment,[0],[0]
Our first observation is that PPBO can distinguish between the choices made by a human and a computer bot.,5. User experiment,[0],[0]
"Human choices pinpoint atomic arrangements that are close to nearby local minima (small N), while the random bot’s choices are far less reasonable and require much subsequent computation to optimise structures.",5. User experiment,[0],[0]
"For all human users, the preferred molecular structures were placed somewhat high above the surface, which led to relatively high E values.",5. User experiment,[0],[0]
"With this query arrangement, it appears the z variable was the most difficult one to estimate visually.",5. User experiment,[0],[0]
"Human-preferred molecular orientations were favourable, so the structures were optimised quickly (few N steps).
",5. User experiment,[0],[0]
"The quality of user preference is best judged by the depth of the nearest energy basin, denoted by E*.",5. User experiment,[0],[0]
It describes the energy of the structure preferred by a user.,5. User experiment,[0],[0]
"Here, there is a marked divide by expertise.",5. User experiment,[0],[0]
"The structures refined from the choices of the bot and non-expert are local minima of adsorption, characterised by weak dispersive interactions.
",5. User experiment,[0],[0]
"The expert’s choice led to two low-energy structure types that compete for the global minimum, and feature strong chemical bonding of the O atom to the Cu surface.",5. User experiment,[0],[0]
"Thus, the data (Table 1, column E*: rows 1-4 versus rows 5-10) supports our hypothesis: the materials science experts do obtain structures associated with lower energy minimum points.
",5. User experiment,[0],[0]
The findings above demonstrate that the PPBO framework was able to encode the expert knowledge described via preferences.,5. User experiment,[0],[0]
"However, since there are only 10 samples, further work will be needed to validate the results.",5. User experiment,[0],[0]
"In this paper we have introduced a new Bayesian framework, PPBO, for learning user preferences from a special kind of feedback, which we call projective preferential feedback.",6. Conclusions,[0],[0]
The feedback is equivalent to a minimizer along a projection.,6. Conclusions,[0],[0]
Its form is especially applicable in a human-in-the-loop context.,6. Conclusions,[0],[0]
We demonstrated this in a user experiment in which the user gives the feedback as an optimal position or orientation of a molecule adsorbing to a surface.,6. Conclusions,[0],[0]
"PPBO was capable of encoding user preferences in this case.
",6. Conclusions,[0],[0]
"We demonstrated that PPBO can deal with high-dimensional spaces where existing preferential Bayesian optimization frameworks that are based on pairwise comparisons, such as IBO (Brochu, 2010) or PBO (González et al., 2017), have difficulties to operate.",6. Conclusions,[0],[0]
"In the numerical experiments, the performance gap between PPBO and PBO was so high that we conclude: whenever a projective preferential query is possible, a PPBO-type of approach is preferable from the optimization perspective.",6. Conclusions,[0],[0]
"However, we note that it is better to think of PPBO as a complement, not as a substitute for
PBO.",6. Conclusions,[0],[0]
"In the applications, pairwise comparisons may be preferred, for instance, if they are more convenient to a user.
",6. Conclusions,[0],[0]
"In summary, if it is possible to query a projective preferential query, then PPBO provides an efficient way for preference learning in high-dimensional problems.",6. Conclusions,[0],[0]
"In particular, PPBO can be used for efficient expert knowledge elicitation in highdimensional settings which are important in many fields.",6. Conclusions,[0],[0]
"This research was supported by the Academy of Finland (Flagship programme: Finnish Center for Artificial Intelligence FCAI; and grants 320181, 319264, 313195, 292334 and 316601).",Acknowledgements,[0],[0]
"Computational resources were provided by the CSC IT Center for Science, and the Aalto Science-IT Project.",Acknowledgements,[0],[0]
Bayesian optimization is an effective method for finding extrema of a black-box function.,abstractText,[0],[0]
We propose a new type of Bayesian optimization for learning user preferences in high-dimensional spaces.,abstractText,[0],[0]
"The central assumption is that the underlying objective function cannot be evaluated directly, but instead a minimizer along a projection can be queried, which we call a projective preferential query.",abstractText,[0],[0]
"The form of the query allows for feedback that is natural for a human to give, and which enables interaction.",abstractText,[0],[0]
This is demonstrated in a user experiment in which the user feedback comes in the form of optimal position and orientation of a molecule adsorbing to a surface.,abstractText,[0],[0]
"We demonstrate that our framework is able to find a global minimum of a high-dimensional black-box function, which is an infeasible task for existing preferential Bayesian optimization frameworks that are based on pairwise comparisons.",abstractText,[0],[0]
Projective Preferential Bayesian Optimization,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1160–1170 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1160",text,[0],[0]
"Greibach normal form (GNF; Greibach, 1965) is an important construction in formal language theory which allows every context-free grammar (CFG) to be rewritten so that the first character of each rule is a terminal symbol.",1 Introduction,[1.0],"['Greibach normal form (GNF; Greibach, 1965) is an important construction in formal language theory which allows every context-free grammar (CFG) to be rewritten so that the first character of each rule is a terminal symbol.']"
"A grammar in GNF is said to be prefix lexicalized, because the prefix of every production is a lexical item.",1 Introduction,[0],[0]
"GNF has a variety of theoretical and practical applications, including for example the proofs of the famous theorems due to Shamir and Chomsky-Schützenberger (Shamir, 1967; Chomsky and Schützenberger, 1963; Autebert et al., 1997).",1 Introduction,[0],[0]
"Other applications of prefix lexicalization include proving coverage of parsing algorithms (Gray and Harrison, 1972) and decidability of equivalence problems (Christensen et al., 1995).
",1 Introduction,[0],[0]
"By using prefix lexicalized synchronous context-free grammars (SCFGs), Watanabe et al. (2006) and Siahbani et al. (2013) obtain asymptotic and empirical speed improvements on a machine translation task.",1 Introduction,[0],[0]
"Using a prefix lexicalized grammar ensures that target sentences can be generated from left to right, which allows the use of beam search to constrain their decoder’s search space as it performs a left-to-right traversal of translation hypotheses.",1 Introduction,[0],[0]
"To achieve these results,
new grammars had to be heuristically constrained to include only prefix lexicalized productions, as there is at present no way to automatically convert an existing SCFG to a prefix lexicalized form.
",1 Introduction,[0],[0]
"This work investigates the formal properties of prefix lexicalized synchronous grammars as employed by Watanabe et al. (2006) and Siahbani et al. (2013), which have received little theoretical attention compared to non-synchronous prefix lexicalized grammars.",1 Introduction,[0],[0]
"To this end, we first prove that SCFG is not closed under prefix lexicalization.",1 Introduction,[0],[0]
"Our main result is that there is a method for prefix lexicalizing an SCFG by converting it to an equivalent grammar in a different formalism, namely synchronous tree-adjoining grammar (STAG) in regular form.",1 Introduction,[1.0],"['Our main result is that there is a method for prefix lexicalizing an SCFG by converting it to an equivalent grammar in a different formalism, namely synchronous tree-adjoining grammar (STAG) in regular form.']"
"Like the GNF transformation for CFGs, our method at most cubes the grammar size, but we show empirically that the size increase is only quadratic for grammars used in existing NLP tasks.",1 Introduction,[0],[0]
"The rank is at most doubled, and we maintain O(n3k) parsing complexity for grammars of rank k.",1 Introduction,[0],[0]
"We conclude that although SCFG does not have a prefix lexicalized normal form like GNF, our conversion to prefix lexicalized STAG offers a practical alternative.",1 Introduction,[0],[0]
"An SCFG is a tuple G = (N,Σ, P, S) where N is a finite nonterminal alphabet, Σ is a finite terminal alphabet, S ∈ N is a distinguished nonterminal called the start symbol, and P is a finite set of synchronous rules of the form
(1) 〈A1 → α1, A2 → α2〉
for some A1, A2 ∈ N and strings α1, α2 ∈ (N ∪ Σ)∗.1",2.1 SCFG,[0],[0]
"Every nonterminal which appears in α1
1A variant formalism exists which requires thatA1 = A2; this is called syntax-directed transduction grammar (Lewis and Stearns, 1968) or syntax-directed translation schemata (Aho and Ullman, 1969).",2.1 SCFG,[0],[0]
"This variant is weakly equivalent to SCFG, but SCFG has greater strong generative capacity (Crescenzi et al., 2015).
",2.1 SCFG,[0],[0]
"A 1
A↓ 2a
B 2
B↓ 1b
A
cA∗
B d 〈",2.1 SCFG,[0],[0]
"〉 〈 〉
〈 A A
a A ↓ 1
c ,
B 1
b B
d 〉
Figure 1: An example of synchronous rewriting in an STAG (left) and the resulting tree pair (right).
must be linked to exactly one nonterminal in α2, and vice versa.",2.1 SCFG,[0],[0]
"We write these links using numerical annotations, as in (2).
",2.1 SCFG,[0],[0]
"(2) 〈A→ A 1 B 2 , B → B 2 A 1 〉
An SCFG has rank k if no rule in the grammar contains more than k pairs of linked nodes.
",2.1 SCFG,[0],[0]
"In every step of an SCFG derivation, we rewrite one pair of linked nonterminals with a rule from P , in essentially the same way we would rewrite a single nonterminal in a non-synchronous CFG.",2.1 SCFG,[0],[0]
"For example, (3) shows linked A and B nodes being rewritten using (2): (3) 〈X 1",2.1 SCFG,[0],[0]
"A 2 , B 2 Y 1 〉 ⇒ 〈X 1 A 2 B 3 , B 3 A 2 Y 1 〉
Note how the 1 and 2 in rule (2) are renumbered to 2 and 3 during rewriting, to avoid an ambiguity with the 1 already present in the derivation.
",2.1 SCFG,[0],[0]
An SCFG derivation is complete when it contains no more nonterminals to rewrite.,2.1 SCFG,[0],[0]
A completed derivation represents a string pair generated by the grammar.,2.1 SCFG,[1.0],['A completed derivation represents a string pair generated by the grammar.']
"An STAG (Shieber, 1994) is a tuple G = (N,Σ, T, S) where N is a finite nonterminal alphabet, Σ is a finite terminal alphabet, S ∈ N is a distinguished nonterminal called the start symbol, and T is a finite set of synchronous tree pairs of the form
(4) 〈t1, t2〉
where t1 and t2 are elementary trees as defined in Joshi et al. (1975).",2.2 STAG,[0],[0]
A substitution site is a leaf node marked by ↓ which may be rewritten by another tree; a foot node is a leaf marked by ∗ that may be used to rewrite a tree-internal node.,2.2 STAG,[0],[0]
"Every substitution site in t1 must be linked to exactly one nonterminal in t2, and vice versa.",2.2 STAG,[0],[0]
"As in SCFG, we write these links using numbered annotations; rank is defined for STAG the same way as for SCFG.
",2.2 STAG,[0],[0]
"In every step of an STAG derivation, we rewrite one pair of linked nonterminals with a tree pair from T , using the same substitution and adjunction operations defined for non-synchronous TAG.",2.2 STAG,[0],[0]
"For example, Figure 1 shows linked A and B nodes being rewritten and the tree pair resulting from this operation.",2.2 STAG,[0],[0]
See Joshi et al. (1975) for details about the underlying TAG formalism.,2.2 STAG,[0],[0]
"We use synchronous production as a cover term for either a synchronous rule in an SCFG or a synchronous tree pair in an STAG.
",2.3 Terminology,[0],[0]
"Following Siahbani et al. (2013), we refer to the left half of a synchronous production as the source side, and the right half as the target side; this terminology captures the intuition that synchronous grammars model translational equivalence between a source phrase and its translation into a target language.",2.3 Terminology,[0],[0]
"Other authors refer to the two halves as the left and right components (Crescenzi et al., 2015) or, viewing the grammar as a transducer, the input and the output (Engelfriet et al., 2017).
",2.3 Terminology,[0],[0]
We call a grammar ε-free if it contains no productions whose source or target side produces only the empty string ε.,2.3 Terminology,[1.0],['We call a grammar ε-free if it contains no productions whose source or target side produces only the empty string ε.']
"Previous work (Watanabe et al., 2006; Siahbani et al., 2013) has shown that it is useful for the target side of a synchronous grammar to start with a terminal symbol.",2.4 Synchronous Prefix Lexicalization,[0],[0]
"For this reason, we define a synchronous grammar to be prefix lexicalized when the leftmost character of the target side2 of every synchronous production in that grammar is a terminal symbol.
",2.4 Synchronous Prefix Lexicalization,[0],[0]
"Formally, this means that every synchronous rule in a prefix lexicalized SCFG (PL-SCFG) is
2All of the proofs in this work admit a symmetrical variant which prefix lexicalizes the source side instead of the target.",2.4 Synchronous Prefix Lexicalization,[0],[0]
"We are not aware of any applications in NLP where sourceside prefix lexicalization is useful, so we do not address this case.
of the form
(5) 〈A1 → α1, A2 → aα2〉
whereA1, A2 ∈ N , α1, α2 ∈ (N∪Σ)∗ and a ∈ Σ.",2.4 Synchronous Prefix Lexicalization,[0],[0]
"Every synchronous tree pair in a prefix lexicalized STAG (PL-STAG) is of the form
(6)
",2.4 Synchronous Prefix Lexicalization,[0],[0]
"〈 A1
α1
, A2
aα2 〉 whereA1, A2 ∈ N , α1, α2 ∈",2.4 Synchronous Prefix Lexicalization,[0],[0]
(N∪Σ)∗ and a ∈ Σ.,2.4 Synchronous Prefix Lexicalization,[0],[0]
"We now prove that the class SCFG is not closed under prefix lexicalization.
",3 Closure under Prefix Lexicalization,[0],[0]
Theorem 1.,3 Closure under Prefix Lexicalization,[0],[0]
"There exists an SCFG which cannot be converted to an equivalent PL-SCFG.
",3 Closure under Prefix Lexicalization,[0],[0]
Proof.,3 Closure under Prefix Lexicalization,[0],[0]
"The SCFG in (7) generates the language L = {〈aibjci, bjai〉| i ≥ 0, j ≥ 1}, but this language cannot be generated by any PL-SCFG:
(7)
〈S → A 1 , S → A 1 〉 〈A→ aA 1 c, A→ A 1 a〉 〈A→ bB 1 , A→ bB 1 〉 〈A→ b, A→ b〉 〈B → bB 1 , B → bB 1 〉 〈B → b, B → b〉
Suppose, for the purpose of contradiction, that some PL-SCFG does generate L; call this grammar G. Then the following derivations must all be possible in G for some nontermials U, V,X, Y :
i)",3 Closure under Prefix Lexicalization,[0],[0]
"〈U 1 , V 1 〉 ⇒∗ 〈bkU 1 bm, bnV 1 bp〉, where k +m = n+ p and n ≥ 1
ii) 〈X 1 , Y 1 〉 ⇒∗ 〈aqX 1 cq, arY 1 as〉, where q = r + s and r ≥ 1
iii) 〈S 1 , S 1 〉 ⇒∗ 〈α1X 1 α2, bα3Y 1 α4〉, where α1, ..., α4 ∈ (N ∪ Σ)∗
iv) 〈X 1 , Y 1 〉 ⇒∗ 〈α5U 1 α6, α7V 1 α8〉, where α5, α6, α8 ∈",3 Closure under Prefix Lexicalization,[0],[0]
"(N ∪ Σ)∗, α7 ∈ Σ(N ∪ Σ)∗
i and ii follow from the same arguments used in the pumping lemma for (non-synchronous) context free languages (Bar-Hillel et al., 1961): strings in L can contain arbitrarily many as, bs, and cs, so there must exist some pumpable cycles
which generate these characters.",3 Closure under Prefix Lexicalization,[0],[0]
"In i, k + m = n + p because the final derived strings must contain an equal number of bs, and n ≥ 1 because G is prefix lexicalized; in ii the constraints on q, r and s follow likewise from L. iii follows from the fact that, in order to pump on the cycle in ii, this cycle must be reachable from the start symbol.",3 Closure under Prefix Lexicalization,[0],[0]
iv follows from the fact that a context-free production cannot generate a discontinuous span.,3 Closure under Prefix Lexicalization,[0],[0]
"Once the cycle in i has generated a b, it is impossible for ii to generate an a on one side of the b and a c on the other.",3 Closure under Prefix Lexicalization,[0],[0]
"Therefore i must always be derived strictly later than ii, as shown in iv.
",3 Closure under Prefix Lexicalization,[0],[0]
Now we obtain a contradiction.,3 Closure under Prefix Lexicalization,[0],[0]
"Given that G can derive all of i through iv, the following derivation is also possible: (8)
〈S 1 , S 1 〉 ⇒∗ 〈α1X 1 α2, bα3Y 1 α4〉 ⇒∗ 〈α1aqX 1 cqα2, bα3arY 1 asα4〉 ⇒∗ 〈α1aqα5U 1 α6cqα2, bα3arα7V 1 α8asα4〉 ⇒∗ 〈α1aqα5bkU 1 bmα6cqα2,
bα3a rα7b nV 1 bpα8a sα4〉
But since n, r ≥ 1, the target string derived this way contains an a before a b and does not belong to L.
This is a contradiction: if G is a PL-SCFG then it must generate i through iv, but if so then it also generates strings which do not belong to L. Thus no PL-SCFG can generate L, and SCFG must not be closed under prefix lexicalization.
",3 Closure under Prefix Lexicalization,[0],[0]
There also exist grammars which cannot be prefix lexicalized because they contain cyclic chain rules.,3 Closure under Prefix Lexicalization,[0],[0]
"If an SCFG can derive something of the form 〈X 1 , Y 1 〉 ⇒∗ 〈xX 1 , Y 1 〉, then it can generate arbitrarily many symbols in the source string without adding anything to the target string.",3 Closure under Prefix Lexicalization,[0],[0]
"Prefix lexicalizing the grammar would force it to generate some terminal symbol in the target string at each step of the derivation, making it unable to generate the original language where a source string may be unboundedly longer than its corresponding target.",3 Closure under Prefix Lexicalization,[0],[0]
We call an SCFG chain-free if it does not contain a cycle of chain rules of this form.,3 Closure under Prefix Lexicalization,[0],[0]
"The remainder of this paper focuses on chain-free grammars, like (7), which cannot be converted to PL-SCFG despite containing no such cycles.",3 Closure under Prefix Lexicalization,[0],[0]
"We now present a method for prefix lexicalizing an SCFG by converting it to an STAG.
",4 Prefix Lexicalization using STAG,[0.9929846019923724],['We have demonstrated a method for prefix lexicalizing an SCFG by converting it to an equivalent STAG.']
"SXA
〉
〈 SXA
SXA
〉
Theorem 2.",4 Prefix Lexicalization using STAG,[0],[0]
"Given a rank-k SCFG G which is εfree and chain-free, an STAGH exists such thatH is prefix lexicalized and L(G) = L(H).",4 Prefix Lexicalization using STAG,[0],[0]
"The rank of H is at most 2k, and |H| = O(|G|3).
",4 Prefix Lexicalization using STAG,[0],[0]
Proof.,4 Prefix Lexicalization using STAG,[0],[0]
"Let G = (N,Σ, P, S) be an ε-free, chainfree SCFG.",4 Prefix Lexicalization using STAG,[0],[0]
"We provide a constructive method for prefix lexicalizing the target side of G.
We begin by constructing an intermediate grammar GXA for each pair of nonterminals X,A ∈ N \",4 Prefix Lexicalization using STAG,[0.9795479000977884],"['We provide a constructive method for prefix lexicalizing the target side of G. We begin by constructing an intermediate grammar GXA for each pair of nonterminals X,A ∈ N \\ {S}.']"
{S}.,4 Prefix Lexicalization using STAG,[0],[0]
"For each pair X,A ∈ N",4 Prefix Lexicalization using STAG,[0],[0]
\,4 Prefix Lexicalization using STAG,[0],[0]
"{S}, GXA will be constructed to generate the language of sentential forms derivable from 〈X 1 , A 1 〉 via a target-side terminal leftmost derivation (TTLD).",4 Prefix Lexicalization using STAG,[0.9803193710984073],"['For each pair X,A ∈ N \\ {S}, GXA will be constructed to generate the language of sentential forms derivable from 〈X 1 , A 1 〉 via a target-side terminal leftmost derivation (TTLD).']"
"A TTLD is a derivation of the form in Figure 2, where the leftmost nonterminal in the target string is expanded until it produces a terminal symbol as the first character.",4 Prefix Lexicalization using STAG,[0],[0]
"We write 〈X 1 , A 1 〉 ⇒∗TTLD 〈u, v〉 to mean that 〈X 1 , A 1 〉 derives 〈u, v〉 by way of a TTLD; in this notation, LXA = {〈u, v〉|〈X 1 , A 1 〉 ⇒∗TTLD 〈u, v〉} is the language of sentential forms derivable from 〈X 1 , A 1 〉 via a TTLD.
",4 Prefix Lexicalization using STAG,[0],[0]
"Given X,A ∈ N \",4 Prefix Lexicalization using STAG,[0],[0]
"{S} we formally define GXA as an STAG over the terminal alphabet ΣXA = N ∪ Σ and nonterminal alphabet NXA = {YXA|Y ∈ N}, with start symbol SXA.",4 Prefix Lexicalization using STAG,[0],[0]
"NXA contains nonterminals indexed by XA to ensure that two intermediate grammars GXA and GY B do not interact as long as 〈X,",4 Prefix Lexicalization using STAG,[0],[0]
"A〉 6= 〈Y,B〉.
",4 Prefix Lexicalization using STAG,[0],[0]
"GXA contains four kinds of tree pairs: 3
• For each rule in G of the form 〈X → α1, A→ aα2〉, a ∈ Σ, αi ∈ (N∪Σ)∗, we add a tree pair of the form in Figure 3(a).
",4 Prefix Lexicalization using STAG,[0],[0]
"• For each rule in G of the form 〈Y → α1, B → aα2〉, a ∈ Σ, αi ∈ (N∪Σ)∗, Y,B ∈ N",4 Prefix Lexicalization using STAG,[0],[0]
\,4 Prefix Lexicalization using STAG,[0],[0]
"{S}, we add a tree pair of the form in Figure 3(b).
",4 Prefix Lexicalization using STAG,[0],[0]
"• For each rule in G of the form 〈Y → α1Z 1 β1, B → C 1 α2〉, Y, Z,B,C ∈ N",4 Prefix Lexicalization using STAG,[0],[0]
\,4 Prefix Lexicalization using STAG,[0],[0]
"{S}, αi, βi ∈ (N ∪ Σ)∗, we add a tree pair of the form in Figure 3(c).
",4 Prefix Lexicalization using STAG,[0],[0]
"As a special case, if Y = Z we collapse the root node and adjunction site to produce a tree pair of the following form:
(9)
〈 ZXA 1
α1ZXA ∗ β1
, CXA
α2BXA ↓ 1 〉 • For each rule in G of the form 〈X → α1Y 1 β1, A→ C 1 α2〉, Y,C ∈ N , αi, βi ∈ (N ∪ Σ)∗, we add a tree pair of the form in Figure 3(d).
",4 Prefix Lexicalization using STAG,[1.0000000631730193],"['As a special case, if Y = Z we collapse the root node and adjunction site to produce a tree pair of the following form: (9) 〈 ZXA 1 α1ZXA ∗ β1 , CXA α2BXA ↓ 1 〉 • For each rule in G of the form 〈X → α1Y 1 β1, A→ C 1 α2〉, Y,C ∈ N , αi, βi ∈ (N ∪ Σ)∗, we add a tree pair of the form in Figure 3(d).']"
"3In all cases, we assume that symbols inN (notNXA) retain any links they bore in the original grammar, even though they belong to the terminal alphabet in GXA and therefore do not participate in rewriting operations.",4 Prefix Lexicalization using STAG,[0],[0]
"In the final constructed grammar, these symbols will belong to the nonterminal alphabet again, and the links will function normally.
",4 Prefix Lexicalization using STAG,[0],[0]
"Figure 4 gives a concrete example of constructing an intermediate grammar tree pair on the basis of an SCFG rule.
",4 Prefix Lexicalization using STAG,[0],[0]
Lemma 1.,4 Prefix Lexicalization using STAG,[0],[0]
"GXA generates the language LXA.
Proof.",4 Prefix Lexicalization using STAG,[0],[0]
This can be shown by induction over derivations of increasing length.,4 Prefix Lexicalization using STAG,[0],[0]
"The proof is straightforward but very long, so we provide only a sketch; the complete proof is provided in the supplementary material.
",4 Prefix Lexicalization using STAG,[0],[0]
"As a base case, observe that a tree of the shape in Figure 3(a) corresponds straightforwardly to the derivation
(10) 〈X 1 , A 1 〉 ⇒ 〈α1, aα2〉
which is a TTLD starting from 〈X,A〉.",4 Prefix Lexicalization using STAG,[0],[0]
"By construction, therefore, every TTLD of the shape in (10) corresponds to some tree in GXA of shape 3(a); likewise every derivation inGXA comprising a single tree of shape 3(a) corresponds to a TTLD of the shape in (10).
",4 Prefix Lexicalization using STAG,[0],[0]
"As a second base case, note that a tree of the shape in Figure 3(b) corresponds to the last step of a TTLD like (11):
(11) 〈X 1 , A 1 〉 ⇒∗TTLD 〈uY 1 v,B 1 w〉 ⇒ 〈uα1v, aα2w〉
In the other direction, the last step of any TTLD of the shape in (11) will involve some rule of the shape 〈Y → α1, B → aα2〉; by construction GXA must contain a corresponding tree pair of shape 3(b).
",4 Prefix Lexicalization using STAG,[0],[0]
"Together, these base cases establish a one-toone correspondence between single-tree derivations in GXA and the last step of a TTLD starting from 〈X,A〉.
",4 Prefix Lexicalization using STAG,[0],[0]
"Now, assume that the last n steps of every TTLD starting from 〈X,A〉 correspond to some derivation over n trees in GXA, and vice versa.",4 Prefix Lexicalization using STAG,[0],[0]
"Then the last n + 1 steps of that TTLD will also correspond to some n+ 1 tree derivation in GXA, and vice versa.
",4 Prefix Lexicalization using STAG,[0],[0]
"To see this, consider the step n+ 1 steps before the end of the TTLD.",4 Prefix Lexicalization using STAG,[0],[0]
"This step may be in the middle of the derivation, or it may be the first step of the derivation.",4 Prefix Lexicalization using STAG,[0],[0]
"If it is in the middle, then this step must involve a rule of the shape
(12) 〈Y → α1Z 1 β1, B → C 1 α2〉
The existence of such a rule in G implies the existence of a corresponding tree in GXA of the shape in Figure 3(c).",4 Prefix Lexicalization using STAG,[0],[0]
Adding this tree to the existing n-tree derivation yields a new n + 1 tree derivation corresponding to the last n + 1 steps of the TTLD.4,4 Prefix Lexicalization using STAG,[0],[0]
"In the other direction, if the n+ 1th tree5 of a derivation in GXA is of the shape in Figure 3(c), then this implies the existence of a production in G of the shape in (12).",4 Prefix Lexicalization using STAG,[0],[0]
"By assumption the first n trees of the derivation in GXA correspond to some TTLD in G; by prepending the rule from (12) to this TTLD we obtain a new TTLD of length n + 1 which corresponds to the entire n + 1 tree derivation in GXA.
",4 Prefix Lexicalization using STAG,[0],[0]
"Finally, consider the case where the TTLD is only n + 1 steps long.",4 Prefix Lexicalization using STAG,[0],[0]
"The first step must involve a rule of the form
(13) 〈X → α1Y 1 β1, A→ C 1 α2〉
The existence of such a rule implies the existence of a corresponding tree in GXA of the shape in Figure 3(d).",4 Prefix Lexicalization using STAG,[0],[0]
Adding this tree to the derivation which corresponds to the last n steps of the TTLD yields a new n+1 tree derivation corresponding to the entire n+ 1 step TTLD.,4 Prefix Lexicalization using STAG,[0],[0]
"In the other direction, if the last tree of an n + 1 tree derivation in GA is of the shape in Figure 3(d), then this implies the
4It is easy to verify by inspection of Figure 3 that whenever one rule from G can be applied to the output of another rule, then the tree pairs in GXA which correspond to these rules can compose with one another.",4 Prefix Lexicalization using STAG,[0],[0]
"Thus we can add the new tree to the existing derivation and be assured that it will compose with one of the trees that is already present.
",4 Prefix Lexicalization using STAG,[0],[0]
"5Although trees in GXA may contain symbols from the nonterminal alphabet of G, these symbols belong to the terminal alphabet in GXA.",4 Prefix Lexicalization using STAG,[0],[0]
"Only nonterminals in NXA will be involved in this derivation, and by construction there is at most one such nonterminal per tree.",4 Prefix Lexicalization using STAG,[0],[0]
"Thus a well-formed derivation structure in GXA will never branch, and we can refer to the n+ 1th tree pair as the one which is at depth n in the derivation structure.
existence of a production inG of the shape in (13).",4 Prefix Lexicalization using STAG,[0],[0]
"By assumption the first n trees of the derivation in GXA correspond to some TTLD inG; by prepending the rule from (13) to this TTLD we obtain a new TTLD of length n + 1 which corresponds to the entire n+ 1 tree derivation in GXA.
",4 Prefix Lexicalization using STAG,[0],[0]
"Taken together, these cases establish a one-toone correspondence between derivations in GXA and TTLDs which start from 〈X,A〉; in turn they confirm that GXA generates the desired language LXA.
",4 Prefix Lexicalization using STAG,[0],[0]
"Once we have constructed an intermediate grammar GXA for each X,A ∈ N \ {S}, we obtain the final STAG H as follows:
1.",4 Prefix Lexicalization using STAG,[0],[0]
Convert the input SCFG G to an equivalent STAG.,4 Prefix Lexicalization using STAG,[0],[0]
"For each rule 〈A1 → α1, A2 → α2〉, where Ai ∈ N , αi ∈ (N ∪ Σ)∗, create a tree pair of the form
(14)
〈 A1
α1
, A2
α2 〉 where each pair of linked nonterminals in the original rule become a pair of linked substitution sites in the tree pair.",4 Prefix Lexicalization using STAG,[1.0000000743795219],"['For each rule 〈A1 → α1, A2 → α2〉, where Ai ∈ N , αi ∈ (N ∪ Σ)∗, create a tree pair of the form (14) 〈 A1 α1 , A2 α2 〉 where each pair of linked nonterminals in the original rule become a pair of linked substitution sites in the tree pair.']"
The terminal and nonterminal alphabets and start symbol are unchanged.,4 Prefix Lexicalization using STAG,[1.0],['The terminal and nonterminal alphabets and start symbol are unchanged.']
"Call the resulting STAG H .
2.",4 Prefix Lexicalization using STAG,[0],[0]
"For all X,A ∈ N \ {S}, add all of the tree pairs from the intermediate grammar GXA to the new grammar H .",4 Prefix Lexicalization using STAG,[1.0],"['For all X,A ∈ N \\ {S}, add all of the tree pairs from the intermediate grammar GXA to the new grammar H .']"
"Expand N to include the new nonterminal symbols in NXA.
3.",4 Prefix Lexicalization using STAG,[0],[0]
"For every X,A ∈ N , in all tree pairs where the target tree’s leftmost leaf is labeled with A and this node is linked to anX , replace this occurrence of A with SXA.",4 Prefix Lexicalization using STAG,[0],[0]
"Also replace the linked node in the source tree.
4.",4 Prefix Lexicalization using STAG,[0],[0]
"For every X,A ∈ N , let RXA be the set of all tree pairs rooted in SXA, and let TXA be the set of all tree pairs whose target tree’s leftmost leaf is labeled with SXA.",4 Prefix Lexicalization using STAG,[0],[0]
"For every 〈s, t〉 ∈ TXA and every 〈s′, t′〉 ∈ RXA, substitute or adjoin s′ and t′ into the linked SXA nodes in s and t, respectively.",4 Prefix Lexicalization using STAG,[0],[0]
"Add the derived trees to H .
5.",4 Prefix Lexicalization using STAG,[0],[0]
"For all X,A ∈ N , let TXA be defined as above.",4 Prefix Lexicalization using STAG,[0],[0]
"Remove all tree pairs in TXA from H .
6.",4 Prefix Lexicalization using STAG,[0],[0]
"For all X,A ∈ N , let RXA be defined as above.",4 Prefix Lexicalization using STAG,[0],[0]
"Remove all tree pairs in RXA from H .
",4 Prefix Lexicalization using STAG,[0],[0]
"We now claim that H generates the same language as the original grammar G, and all of the target trees in H are prefix lexicalized.
",4 Prefix Lexicalization using STAG,[0],[0]
The first claim follows directly from the construction.,4 Prefix Lexicalization using STAG,[0],[0]
Step 1 merely rewrites the grammar in a new formalism.,4 Prefix Lexicalization using STAG,[0],[0]
"From Lemma 1 it is clear that steps 2–3 do not change the generated language: the set of string pairs generable from a pair of SXA nodes is identical to the set generable from 〈X,A〉 in the original grammar.",4 Prefix Lexicalization using STAG,[0],[0]
"Step 4 replaces some nonterminals by all possible alternatives; steps 5– 6 then remove the trees which were used in step 4, but since all possible combinations of these trees have already been added to the grammar, removing them will not alter the language.
",4 Prefix Lexicalization using STAG,[0],[0]
The second claim follows from inspection of the tree pairs generated in Figure 3.,4 Prefix Lexicalization using STAG,[0],[0]
"Observe that, by construction, for all X,A ∈ N every target tree rooted in SXA is prefix lexicalized.",4 Prefix Lexicalization using STAG,[1.0],"['Observe that, by construction, for all X,A ∈ N every target tree rooted in SXA is prefix lexicalized.']"
"Thus the trees created in step 4 are all prefix lexicalized variants of non-lexicalized tree pairs; steps 5–6 then remove the non-lexicalized trees from the grammar.
",4 Prefix Lexicalization using STAG,[1.0000000010901324],['Thus the trees created in step 4 are all prefix lexicalized variants of non-lexicalized tree pairs; steps 5–6 then remove the non-lexicalized trees from the grammar.']
Figure 5 gives an example of this transformation applied to a small grammar.,4 Prefix Lexicalization using STAG,[0],[0]
"Note how A nodes at the left edge of the target trees end up rewritten as SAA nodes, as per step 4 of the transformation.",4 Prefix Lexicalization using STAG,[0],[0]
"Our conversion generates a subset of the class of prefix lexicalized STAGs in regular form, which we abbreviate to PL-RSTAG (regular form for TAG is defined in Rogers 1994).",5 Complexity & Formal Properties,[0],[0]
"This section discusses some formal properties of PL-RSTAG.
",5 Complexity & Formal Properties,[0],[0]
"Generative Capacity PL-RSTAG is weakly equivalent to the class of ε-free, chain-free SCFGs: this follows immediately from the proof that our transformation does not change the language generated by the input SCFG.",5 Complexity & Formal Properties,[0],[0]
"Note that every TAG in regular form generates a context-free language (Rogers, 1994).
",5 Complexity & Formal Properties,[0],[0]
Alignments and Reordering PL-RSTAG generates the same set of reorderings (alignments) as SCFG.,5 Complexity & Formal Properties,[0],[0]
"Observe that our transformation does not cause nonterminals which were linked in the original grammar to become unlinked, as noted for example in Figure 4.",5 Complexity & Formal Properties,[0],[0]
"Thus subtrees which are gener-
〈S → B 2 cA 1 , S → A 1 cB 2 〉 〈A→ B 2 cA 1 , A→ A 1 cB 2 〉 〈A→ a, A→ a〉 〈B → b, B → b〉
〈 S B ↓",5 Complexity & Formal Properties,[0],[0]
"1 c SAA
a
,
S
SAA
a
c B ↓",5 Complexity & Formal Properties,[0],[0]
"1
〉 〈 A B ↓ 1 c SAA
AAA 2
a
,
A
SAA
a AAA ↓ 2
c B ↓ 1 〉 〈 S
B ↓ 1 c SAA
AAA 2
a
,
S
SAA
a AAA ↓ 2
c B ↓ 1 〉 〈 A B ↓ 1 c SAA
a
,
A
SAA
a
c B ↓ 1
〉 〈 AAA 1
B ↓",5 Complexity & Formal Properties,[0],[0]
"2 c AAA∗
, AAA
c B ↓ 2 AAA ↓ 1 〉 〈
AAA
B ↓",5 Complexity & Formal Properties,[0],[0]
"1 c AAA∗
, AAA
c B ↓ 1
〉 〈 B
b
, B
b
〉 〈 A
a
, A
a 〉
Figure 5: An SCFG and the STAG which prefix lexicalizes it.",5 Complexity & Formal Properties,[0],[0]
"Non-productive trees have been omitted.
",5 Complexity & Formal Properties,[0],[0]
"Grammar |G| |H| % of G prefix lexicalized log|G|(|H|) Siahbani and Sarkar (2014a) (Zh-En) 18.5M 23.6T 63% 1.84 Example (7) 6 14 66% 1.47 ITG (10000 translation pairs) 10,003 170,000 99.97% 1.31
Table 1: Grammar sizes before and after prefix lexicalization, showing O(n2) size increase instead of the worst case O(n3).",5 Complexity & Formal Properties,[0],[0]
|G| and |H| give the grammar size before and after prefix lexicalization; log|G| |H| is the increase as a power of the initial size.,5 Complexity & Formal Properties,[0],[0]
"We also show the percentage of productions which are already prefix lexicalized in G.
ated by linked nonterminals in the original grammar will still be generated by linked nonterminals in the final grammar, so no reordering information is lost or added.6 This result holds despite the fact that our transformation is only applicable to chainfree grammars: chain rules cannot introduce any reorderings, since by definition they involve only a single pair of linked nonterminals.
",5 Complexity & Formal Properties,[0],[0]
Grammar Rank,5 Complexity & Formal Properties,[0],[0]
"If the input SCFG G has rank k,",5 Complexity & Formal Properties,[0],[0]
then the STAG H produced by our transformation has rank at most 2k.,5 Complexity & Formal Properties,[0],[0]
"To see this, observe that the construction of the intermediate grammars increases the rank by at most 1 (see Figure 3(b)).",5 Complexity & Formal Properties,[1.0],"['To see this, observe that the construction of the intermediate grammars increases the rank by at most 1 (see Figure 3(b)).']"
"When a prefix lexicalized tree is substituted at the left edge of a non-lexicalized tree, the link on the substitution site will be consumed, but up to k+ 1 new links will be introduced by the substituting tree, so that the final tree will have rank at most 2k.
",5 Complexity & Formal Properties,[1.0000000086366951],"['When a prefix lexicalized tree is substituted at the left edge of a non-lexicalized tree, the link on the substitution site will be consumed, but up to k+ 1 new links will be introduced by the substituting tree, so that the final tree will have rank at most 2k.']"
"In the general case, rank-k STAG is more powerful than rank-k SCFG; for example, a rank-4 SCFG is required to generate the reordering in 〈S → A 1 B 2 C 3 D 4 , S → C 3 A 1 D 4 B 2 〉 (Wu, 1997), but this reordering is captured by the
6Although we consume one link whenever we substitute a prefix lexicalized tree at the left edge of an unlexicalized tree, that link can still be remembered and used to reconstruct the reorderings which occurred between the two sentences.
",5 Complexity & Formal Properties,[0],[0]
"following rank-3 STAG:〈 S X
A ↓ 1 X 2
C ↓ 3
, S
C ↓ 3",5 Complexity & Formal Properties,[0],[0]
"A ↓ 1 X ↓ 2 〉 〈
X
B ↓",5 Complexity & Formal Properties,[0],[0]
"1 X∗ D ↓ 2
, X
D ↓ 2 B ↓ 1 〉 For this reason, we speculate that it is possible to further transform the grammars produced by our lexicalization in order to reduce their rank, but the details of this transformation remain as future work.
",5 Complexity & Formal Properties,[0],[0]
This potentially poses a solution to an issue raised by Siahbani and Sarkar (2014b).,5 Complexity & Formal Properties,[0],[0]
"On a Chinese-English translation task, they find that sentences like (15) involve reorderings which cannot be captured by a rank-2 prefix lexicalized SCFG: (15) Tā bǔchōng shuō ,",5 Complexity & Formal Properties,[0],[0]
liánhé zhèngfǔ,5 Complexity & Formal Properties,[0],[0]
"mùqián zhuàngkuàng wěndı̀ng ...
",5 Complexity & Formal Properties,[0],[0]
"He added that the coalition government is now in stable condition ...
",5 Complexity & Formal Properties,[0],[0]
"If rank-k PL-RSTAG is more powerful than rank-k
SCFG, using a PL-RSTAG here would permit capturing more reorderings without using grammars of higher rank.
",5 Complexity & Formal Properties,[0],[0]
"Parse Complexity Because the grammar produced is in regular form, each side can be parsed in time O(n3) (Rogers, 1994), for an overall parse complexity of O(n3k), where n is sentence length and k is the grammar rank.
",5 Complexity & Formal Properties,[0.9999999799907733],"['Parse Complexity Because the grammar produced is in regular form, each side can be parsed in time O(n3) (Rogers, 1994), for an overall parse complexity of O(n3k), where n is sentence length and k is the grammar rank.']"
Grammar Size and Experiments,5 Complexity & Formal Properties,[0],[0]
"If H is the PL-RSTAG produced by applying our transformation to an SCFG G, then H contains O(|G|3) elementary tree pairs, where |G| is the number of synchronous productions in G. When the set of nonterminalsN is small compared to |G|, a tighter bound is given by O(|G|2|N |2).
",5 Complexity & Formal Properties,[0],[0]
"Table 1 shows the actual size increase on a variety of grammars: here |G| is the size of the initial grammar, |H| is the size after applying our transformation, and the increase is expressed as a power of the original grammar size.",5 Complexity & Formal Properties,[0],[0]
"We apply our transformation to the grammar from Siahbani and Sarkar (2014a), which was created for a ChineseEnglish translation task known to involve complex reorderings that cannot be captured by PL-SCFG (Siahbani and Sarkar, 2014b).",5 Complexity & Formal Properties,[0],[0]
"We also consider the grammar in (7) and an ITG (Wu, 1997) containing 10,000 translation pairs, which is a grammar of the sort that has previously been used for word alignment tasks (cf Zhang and Gildea 2005).",5 Complexity & Formal Properties,[0],[0]
"We always observe an increase within O(|G|2) rather than the worst-case O(|G|3), because |N | is small relative to |G| in most grammars used for NLP tasks.
",5 Complexity & Formal Properties,[0],[0]
We also investigated how the proportion of prefix lexicalized rules in the original grammar affects the overall size increase.,5 Complexity & Formal Properties,[0],[0]
We sampled grammars with varying proportions of prefix lexicalized rules from the grammar in Siahbani and Sarkar (2014a); Table 2 shows the result of lexicalizing these samples.,5 Complexity & Formal Properties,[0],[0]
We find that the worst case size increase occurs when 50% of the original grammar is already prefix lexicalized.,5 Complexity & Formal Properties,[1.0],['We find that the worst case size increase occurs when 50% of the original grammar is already prefix lexicalized.']
This is because the size increase depends on both the number of prefix lexicalized trees in the intermediate grammars (which grows with the proportion of lexicalized rules) and the number of productions which need to be lexicalized (which shrinks as the proportion of prefix lexicalized rules increases).,5 Complexity & Formal Properties,[0],[0]
"At 50%, both factors contribute appreciably to the grammar size, analogous to how the function f(x) = x(1− x) takes
its maximum at x = 0.5.",5 Complexity & Formal Properties,[0],[0]
The LR decoding algorithm from Watanabe et al. (2006) relies on prefix lexicalized rules to generate a prefix of the target sentence during machine translation.,6 Applications,[0],[0]
"At each step, a translation hypothesis is expanded by rewriting the leftmost nonterminal in its target string using some grammar rule; the prefix of this rule is appended to the existing translation and the remainder of the rule is pushed onto a stack, in reverse order, to be processed later.",6 Applications,[0],[0]
"Translation hypotheses are stored in stacks according to the length of their translated prefix, and beam search is used to traverse these hypotheses and find a complete translation.",6 Applications,[0],[0]
"During decoding, the source side is processed by an Earley-style parser, with the dot moving around to process nonterminals in the order they appear on the target side.
",6 Applications,[0],[0]
"Since the trees on the target side of our transformed grammar are all of depth 1, and none of these trees can compose via the adjunction operation, they can be treated like context-free rules and used as-is in this decoding algorithm.",6 Applications,[0],[0]
"The only change required to adapt LR decoding to use a PL-RSTAG is to make the source side use a TAG parser instead of a CFG parser; an Earley-style parser for TAG already exists (Joshi and Schabes, 1997), so this is a minor adjustment.
",6 Applications,[0],[0]
"Combined with the transformation in Section 4, this suggests a method for using LR decoding without sacrificing translation quality.",6 Applications,[0],[0]
"Previously, LR decoding required the use of heuristically generated PL-SCFGs, which cannot model some reorderings (Siahbani and Sarkar, 2014a).",6 Applications,[0],[0]
"Now, an SCFG tailored for a translation task can be transformed directly to PL-RSTAG and used for decod-
ing; unlike a heuristically induced PL-SCFG, the transformed PL-RSTAG will generate the same language as the original SCFG which is known to handle more reorderings.
",6 Applications,[0],[0]
"Note that, since applying our transformation may double the rank of a grammar, this method may prove prohibitively slow.",6 Applications,[0],[0]
This highlights the need for future work to examine the generative power of rank-k PL-RSTAG relative to rankk SCFG in the interest of reducing the rank of the transformed grammar.,6 Applications,[0],[0]
Our work continues the study of TAGs and lexicalization (e.g. Joshi et al. 1975; Schabes and Waters 1993).,7 Related Work,[0],[0]
"Schabes and Waters (1995) show that TAG can strongly lexicalize CFG, whereas CFG only weakly lexicalizes itself; we show a similar result for SCFGs.",7 Related Work,[0],[0]
"Kuhlmann and Satta (2012) show that TAG is not closed under strong lexicalization, and Maletti and Engelfriet (2012) show how to strongly lexicalize TAG using simple context-free tree grammars (CFTGs).
",7 Related Work,[0],[0]
"Other extensions of GNF to new grammar formalisms include Dymetman (1992) for definite clause grammars, Fernau and Stiebe (2002) for CF valence grammars, and Engelfriet et al. (2017) for multiple CFTGs.",7 Related Work,[0],[0]
"Although multiple CFTG subsumes SCFG (and STAG), Engelfriet et al.’s result appears to guarantee only that some side of every synchronous production will be lexicalized, whereas our result guarantees that it is always the target side that will be prefix lexicalized.
",7 Related Work,[0],[0]
"Lexicalization of synchronous grammars was addressed by Zhang and Gildea (2005), but they consider lexicalization rather than prefix lexicalization, and they only consider SCFGs of rank 2.",7 Related Work,[0],[0]
"They motivate their results using a word alignment task, which may be another possible application for our lexicalization.
",7 Related Work,[0],[0]
"Analogous to our closure result, Aho and Ullman (1969) prove that SCFG does not admit a normal form with bounded rank like Chomsky normal form.
",7 Related Work,[0],[0]
Blum and Koch (1999) use intermediate grammars like our GXAs to transform a CFG to GNF.,7 Related Work,[0],[0]
"Another GNF transformation (Rosenkrantz, 1967) is used by Schabes and Waters (1995) to define Tree Insertion Grammars (which are also weakly equivalent to CFG).
",7 Related Work,[0],[0]
"We rely on Rogers (1994) for the claim that
our transformed grammars generate context-free languages despite allowing wrapping adjunction; an alternative proof could employ the results of Swanson et al. (2013), who develop their own context-free TAG variant known as osTAG.
",7 Related Work,[0],[0]
Kaeshammer (2013) introduces the class of synchronous linear context-free rewriting systems to model reorderings which cannot be captured by a rank-2 SCFG.,7 Related Work,[0],[0]
"In the event that rank-k PL-RSTAG is more powerful than rank-k SCFG, our work can be seen as an alternative approach to the same problem.
",7 Related Work,[0],[0]
"Finally, Nesson et al. (2008) present an algorithm for reducing the rank of an STAG on-the-fly during parsing; this presents a promising avenue for proving a smaller upper bound on the rank increase caused by our transformation.",7 Related Work,[0],[0]
We have demonstrated a method for prefix lexicalizing an SCFG by converting it to an equivalent STAG.,8 Conclusion and Future Work,[0],[0]
This process is applicable to any SCFG which is ε- and chain-free.,8 Conclusion and Future Work,[0],[0]
"Like the original GNF transformation for CFGs our construction at most cubes the grammar size, though when applied to the kinds of synchronous grammars used in machine translation the size is merely squared.",8 Conclusion and Future Work,[0],[0]
"Our transformation preserves all of the alignments generated by SCFG, and retains properties such as O(n3k) parsing complexity for grammars of",8 Conclusion and Future Work,[0],[0]
rank k.,8 Conclusion and Future Work,[0],[0]
"We plan to verify whether rank-k PL-RSTAG is more powerful than rank-k SCFG in future work, and to reduce the rank of the transformed grammar if possible.",8 Conclusion and Future Work,[0],[0]
We further plan to empirically evaluate our lexicalization on an alignment task and to offer a comparison against the lexicalization due to Zhang and Gildea (2005).,8 Conclusion and Future Work,[0],[0]
The authors wish to thank the anonymous reviewers for their helpful comments.,Acknowledgements,[0],[0]
The research was also partially supported by the Natural Sciences and Engineering Research Council of Canada (NSERC RGPIN-2018-06437 and RGPAS-2018-522574) to the second author.,Acknowledgements,[0],[0]
We dedicate this paper to the memory of Prof. Aravind Joshi; a short hallway conversation with him at ACL 2014 was the seed for this paper.,Acknowledgements,[0],[0]
"We show that an ε-free, chain-free synchronous context-free grammar (SCFG) can be converted into a weakly equivalent synchronous tree-adjoining grammar (STAG) which is prefix lexicalized.",abstractText,[0],[0]
"This transformation at most doubles the grammar’s rank and cubes its size, but we show that in practice the size increase is only quadratic.",abstractText,[0],[0]
"Our results extend Greibach normal form from CFGs to SCFGs and prove new formal properties about SCFG, a formalism with many applications in natural language processing.",abstractText,[0],[0]
Prefix Lexicalization of Synchronous CFGs using Synchronous TAG,title,[0],[0]
