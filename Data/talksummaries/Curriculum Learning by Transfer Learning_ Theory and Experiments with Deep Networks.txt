0	34	Biological organisms can learn to perform tasks (and often do) by observing a sequence of labeled events, just like supervised machine learning.
1	20	But unlike machine learning, in human learning supervision is often accompanied by a curriculum.
2	7	Thus the order of presented examples is rarely random when a human teacher teaches another human.
5	30	We focus here on curriculum learning based on ranking (or weighting as in (Bengio et al., 2009)) of the training examples, which is used to guide the order of presentation of examples to the learner.
6	17	Risking over simplification, the idea is to first present the learner primarily with examples of higher weight or rank, later to be followed by examples with lower weight or rank.
7	14	Ranking may be based on the difficulty of each training example as evaluated by the teacher, from easiest to the most difficult.
8	16	In Section 2 we investigate this strict definition of curriculum learning theoretically, in the context of stochastic gradient descent used to optimize the convex linear regression loss function.
9	64	We first define the (ideal) difficulty of a training point as its loss with respect to the optimal classifier.
10	2	We then prove that curriculum learning, when given the ranking of training points by their difficulty thus defined, is expected (probabilistically) to significantly speed up learning especially at the beginning of training.
11	69	This theoretical result is supported by empirical evidence obtained in the deep learning scenario of curriculum learning described in Section 3, where similar behavior is observed.
12	79	We also show that when the difficulty of the sampled training points is fixed, convergence is faster when sampling points that incur higher loss with respect to the current hypothesis as suggested in (Shrivastava et al., 2016).
13	28	This result is not always true when the difficulty of the sampled training points is not fixed.
15	11	In fact, the need for such supervision has rendered curriculum learning less useful in machine learning, since ranking by difficulty is hard to obtain.
17	8	For example, in visual object recognition it has been demonstrated that what makes an image difficult to a neural network classifier may not always match whatever makes it difficult to a human observer, an observation that has been taken advantage of in the recent work on adversarial examples (Szegedy et al., 2013).
18	17	Possibly, this is one of the reasons why curriculum learning is rarely used in practice (but see, e.g., Zaremba & Sutskever, 2014; Amodei et al., 2016; Jesson et al., 2017).
19	56	In the second part of this paper we focus on this question - how to rank (or weight) the training examples without the aid of a human teacher.
20	8	This is paramount when a human teacher cannot provide a reliable difficulty score for the task at hand, or when obtaining such a score by human teachers is too costly.
21	2	This question is also closely related to transfer learning: here we investigate the use of another classifier to provide the ranking of the training examples by their presumed difficulty.
26	9	This kind of transfer assumes that a powerful pre-trained network is only available at train time, and cannot be used at test time even for the computation of a test point’s representation.
27	44	This may be the case, for example, when the powerful network is too big to run on the target device.
28	53	One can no longer expect to have access to the transferred representation at test time, while ranking can be used at train time in order to improve the learning of the target smaller network (see related discussion of network compression in (Chen et al., 2015; Kim et al., 2015), for example).
29	45	In Section 3 we describe our method, an algorithm which uses the ranking to construct a schedule for the order of presentation of training examples.
30	53	In subsequent empirical evaluations we compare the performance of the method when using a curriculum which is based on different scheduling options, including 2 control conditions where difficult examples are presented first or when using arbitrary scheduling.
31	26	The main results of this empirical study can be summarized as follows: (i) Learning rate is always faster with curriculum learning, especially at the beginning of training.
32	57	(ii) Final generalization is sometimes improved with curriculum learning, especially when the conditions for learning are hard: the task is difficult, the network is small, or when strong regularization is enforced.
33	27	These results are consistent with prior art (see e.g. Bengio et al., 2009).
34	31	We start with some notations in Section 2.1, followed in Sections 2.2 by the rigorous analysis of curriculum learning when used to optimize the linear regression loss.
40	20	The difficulty of point x is measured by its minimal loss with respect to the set of optimal hypotheses {L(h̄(xi), yi)}.
