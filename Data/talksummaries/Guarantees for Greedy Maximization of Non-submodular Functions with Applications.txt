0	20	Many important problems, such as experimental design and sparse modeling, are naturally formulated as a subset selection problem, where a set function F (S) over a Kcardinality constraint is maximized, i.e., max S✓V,|S|K F (S), (P) where V = {v1, .
1	24	Specifically, in experimental design, the goal is to select a set of experiments to perform such that some statistical criterion is optimized.
2	23	This problem arises naturally in domains where performing experiments is costly.
6	22	For the case that F (S) Algorithm 1: The GREEDY Algorithm Input: Ground set V , set function F : 2V!R+, budget K S 0 ; for t = 1, .
8	41	This constant factor can be improved by refining the characterization of the objective using the curvature (Conforti & Cornuéjols, 1984; Vondrák, 2010; Iyer et al., 2013), which informally quantifies how close a submodular function is to being modular (i.e., F (S) and F (S) are submodular).
9	21	However, for many applications, including experimental design and sparse Gaussian processes (Lawrence et al., 2003), F (S) is in general not submodular (Krause et al., 2008) and the above guarantee does not hold.
14	15	Another important class of non-submodular set functions comes as the auxiliary function when optimizing a continuous function f(x) s.t.
15	17	combinatorial constraints, i.e., min x2C,supp(x)2I f(x), where supp(x) := {i | xi 6= 0} is the support set of x, C is a convex set, and I is the independent sets of the combinatorial structure.
16	53	One of the most popular ways to solve this problem is to use the GREEDY algorithm to maximize the auxiliary function F (S) : = max x2C,supp(x)✓S f(x).
17	36	This setting covers various important applications, to name a few, feature selection (Guyon & Elisseeff, 2003), sparse approximation (Das & Kempe, 2008; Krause & Cevher, 2010), sparse recovery (Candes et al., 2006), sparse M-estimation (Jain et al., 2014), linear programming (LP) with combinatorial constraints, and column subset selection (Altschuler et al., 2016).
20	13	In this paper, we combine and generalize the ideas of curvature and submodularity ratio to derive improved constant factor approximation guarantees of the GREEDY algorithm.
22	17	Furthermore, we bound these characteristics for important applications, rendering the usage of GREEDY a principled choice rather than a mere heuristic.
26	18	- Lastly, we experimentally validate our theory on several real-world applications.
27	21	It is worth noting that for the Bayesian A-optimality objective, GREEDY generates comparable solutions as the classically used semidefinite programming (SDP) based method, but is usually two orders of magnitude faster.
46	41	Sviridenko et al. (2013) presented a notion of curvature for monotone non-submodular functions.
51	25	Note that the classical total curvature is ↵total := 1 min i2V ⇢i(V\{i}) ⇢i(;) .
63	24	For the case ↵ = 0 (i.e., F (·) is supermodular), the approximation guarantee is lim ↵!0 1 ↵ (1 e ↵ ) = , which gives the first guarantee of greedily maximizing a nondecreasing supermodular function with bounded .
65	12	For the case ↵ = 1, we have a guarantee of (1 e ) (Das & Kempe, 2011).
75	18	Let P⌦⇤,SK 2 PK,↵, denote those problem instances with optimal solution ⌦⇤ and greedy solution SK .
76	45	We group all problem instances P K,↵, according to the set ⌦⇤ \ SK := {l1 = jm1 , l2 = jm2 , .
80	13	The main idea of the proof is to investigate the worst-case approximation ratio of each group of the problem instances P K,↵, ({l1, .
82	17	By studying the structures of these LPs, we will prove that the worst-case approximation ratio of all problem instances occurs when ⌦⇤ \SK = ;.
96	24	Assume that the optimal solution of the constructed LP is x⇤ 2 RK+ and that s = |⌦⇤ \ SK | 1.
102	13	One can prove that the LP objective decreases: Claim 2.
106	16	So the greedy solution has objective F (SK) 1 ↵  1 ⇣ K ↵ K ⌘ K F (⌦ ⇤ ) 1 ↵ (1 e ↵ )F (⌦⇤).
107	16	Tightness Result We demonstrate that the approximation guarantee in Theorem 1 is tight, i.e., for every submodularity ratio and every curvature ↵, there exist set functions that achieve the bound exactly.
113	43	The following lemma shows that it is generally nonsubmodular and non-supermodular.
116	37	One can see that ⇢ j1(;) = ⇠1 = ⇢ !1(;), so GREEDY can choose j1 in the first step.
121	35	We consider several important real-world applications and their corresponding objective functions.
131	46	Let us arrange a set S ✓ V of stimuli as a matrix X S : = [x v1 , .
