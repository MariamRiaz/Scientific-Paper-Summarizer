67	3	Our algorithms provide, in expectation, constant factor approximation guarantees, where they are robust to the (even adversarial) deletion of any d items from the set V. In our setting, an adversary might try to find a set of inputs for which our algorithms fail to provide good results.
71	2	This idea is useful because the adversary is not aware of the random bits of the algorithms, which makes the deletion probability of elements we have chosen negligible.
81	2	In the robust submodular maximization, we keep a large enough pool of elements with sufficient marginal values before adding or discarding them.
86	3	In this section we outline a centralized algorithm, called ROBUST-CORESET-CENTRALIZED, to find an (↵, d)-robust core-set.
93	5	We should first note that due to the deletion process, the relevant maximum singleton value is not 0 anymore, and it is 00 = maxe2V \D f({e}).
96	5	So ⌧⇤ = OPT2k could fall anywhere in the range [ d/2k, 0].
97	6	Unlike the deletion free case, the upper and lower limits of this range do not differ only by a multiplicative factor of k, thus a naive approach makes us try arbitrarily large number of different choices to find a good estimate of ⌧⇤.
103	2	Therefore, we consider all thresholds in the set T = {(1 + ✏)i| d2k  (1 + ✏) i  d}.
115	3	We also know that all elements added to A⌧ have similar marginal values and are interchangeable.
126	6	In many applications, the dataset does not fit in the main memory of a single machine or even the data itself arrives as a stream.
129	2	We first use the thresholding idea of Section 4.1 in order to find a core-set for V. Then we show that it is possible to find a good solution from this core-set when deletion happens.
182	3	In a wide range of applications, data can be represented as a kernel matrix K, which encodes the similarity between different items in the database.
183	10	In order to find a representative set S of cardinality k, a common objective function is f(S) = log det(I + ↵KS,S), (2) where KS,S is the principal sub-matrix of K indexed by S and ↵ > 0 is a regularization parameter (Herbrich et al., 2003; Seeger, 2004; Krause & Guestrin, 2005).
184	5	This function is monotone submodular.
190	2	To compare the effect of deletions on the performance of algorithms, we use two strategies to choose the deleted items: (i) classical greedy algorithm, and (ii) the stochastic greedy algorithm.
206	5	The joint distribution on (Y,X1, · · · , Xk), under the Naive Bayes assumption, is defined by p(y, x1, · · · , xk) = p(y) Q k i=1 p(xi|y).
228	2	Interestingly, we observe that the accuracies of classifiers which are trained on the features found by ROBUST-CENTRALIZED and ROBUST-STREAMING drop only by 0.2%.
239	13	For this algorithm, we first run stochastic greedy on each partitions to select Si = 6k items.
240	4	After deletion of D, we report f(GREEDY([Si \D)) as the final result.
241	30	Also, we normalize the utility of functions to the objective value of an instance of SG-DISTRIBUTED that knows the set of deleted items D in advance.
242	5	For deletions, we propose four different strategies: D1 randomly deletes 50% of items, D2 randomly deletes 80% of items, D3 deletes all men in the dataset, and D4 deletes all women.
245	4	Furthermore, we observe that the objective value of ROBUST-DISTRIBUTED in all scenarios is even better than our reference function for normalization (normalized objective values are larger than 1).
246	26	Each machine on average stores 209.3 (for k = 50) and 348.3 (for k = 100) items.
247	4	The standard deviations of memory complexities are 36.9 and 26.5, respectively.
248	3	To conclude, ROBUST-DISTRIBUTED enables us to robustly summarize a dataset of size 2,458,285 with storing only ⇡4500 items.
249	22	Our experimental results confirm that this core-set is robust to the deletion of even 80% of items.
250	10	In this paper, we considered the problem of deletion-robust submodular maximization.
251	33	We provided the first scalable and memory-efficient solutions in different optimization settings, namely, centralized, streaming, and distributed models of computation.
252	178	We rigorously proved that our methods enjoy constant factor approximations with respect to the optimum algorithm that is also aware of the deleted set of elements.
253	169	We showcased the effectiveness of our algorithms on real-word problems where part of data should be deleted due to privacy and fairness constraints.
