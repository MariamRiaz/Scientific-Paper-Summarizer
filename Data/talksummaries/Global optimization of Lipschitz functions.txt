2	26	Moreover, it can generally not be assumed that the function has good properties such as linearity or convexity.
3	21	This generic problem of sequentially optimizing the output of an unknown and potentially nonconvex function is often referred to as global optimization (Pintér, 1991), black-box optimization (Jones et al., 1998) or derivative-free optimization (Rios & Sahinidis, 2013).
8	57	Convergence properties of global optimization methods have been developed in the works of (Valko et al., 2013; Munos, 2014) under local smoothness assumptions, but, up to our knowledge, such properties have not been considered in the case where only the global smoothness of the function can be specified.
19	37	Let X ⊂ Rd be a compact and convex set with nonempty interior and let f : X → R be an unknown function which is only supposed to admit a maximum over its input domain X .
24	63	After n iterations, we consider that the algorithm returns an evaluation point Xı̂n with ı̂n ∈ arg mini=1...n f(Xi) which has recorded the highest evaluation.
33	17	In order to design efficient procedures, we first investigate the best performance that can be achieved by any algorithm over the class of Lipschitz functions.
40	21	The next definition introduces the notion of asymptotic convergence.
42	21	, Xn denotes a sequence of n evaluations points generated by the algorithm A over the function f .
45	65	Proposition 3 (CONSISTENCY NSC) A global optimization algorithmA is consistent over the set of Lipschitz functions if and only if ∀f ∈ ⋃k≥0 Lip(k), sup x∈X min i=1...n ‖Xi − x‖2 p−→ 0.
55	45	We start by casting a negative result stating that any algorithm can suffer, at any time, an arbitrarily large loss over the class of Lipschitz functions.
56	35	Proposition 5 Consider any global optimization algorithm A.
58	54	and δ ∈ (0, 1), there exists a function f̃ ∈ ⋃k≥0 Lip(k) only depending on (A,C, n, δ) for which we have with probability at least 1− δ, C ≤ max x∈X f̃(x)− max i=1...n f̃(Xi).
63	19	and the expectation is taken over a sequence of n evaluation points X1, .
68	29	In particular, we will see in the sequel that one can design: I) An algorithm with fixed constant k≥0 which achieves minimax efficiency and also presents exponentially decreasing rates over a large subset of functions, as opposed to space-filling methods (LIPO, Section 3).
76	21	Initialization: Let X1 ∼ U(X ) .....
86	75	Hence, we deduce from this lemma that the algorithm only evaluates the function over points that still have a chance to be maximizers of the unknown function.
88	17	− f(x) ≤ `(x?, x)} denotes the set of functions locally smooth around their maxima with regards to any semi-metric ` : X × X → R previously considered in (Munos, 2014), a straightforward derivation of Lemma 9 directly gives that the decision rule applied in Xt+1 would simply consists in testing whether maxi=1...t f(Xi) ≤ mini=1...t f(Xi) + `(Xt+1, Xi).
94	72	Then, for any f ∈ Lip(k) and n ∈ N?, we have that ∀y ∈ R, P ( max i=1...n f(Xi) ≥ y ) ≥ P ( max i=1...n f(X ′i) ≥ y ) where X1, .
97	34	Based on this result, one can easily derive a first finite-time bound on the difference between the value of the true maximum and its approximation.
99	28	and δ ∈ (0, 1), we have with probability at least 1− δ, max x∈X f(x)− max i=1...n f(Xi) ≤ k · diam(X ) · ( ln(1/δ) n ) 1 d .
103	35	Condition 1 (DECREASING RATE AROUND THE MAXIMUM) A function f : X → R is (κ, cκ)-decreasing around its maximum for some κ ≥ 0, cκ ≥ 0 if: 1.
104	19	This condition, already considered in the works of (Zhigljavsky & Pintér, 1991) and (Munos, 2014), captures how fast the function decreases around its maximum.
115	31	To compare our results, we thus considered DOO tuned with `(x, x′) = k ‖x− x′‖2 over X = [0, 1]d partitioned into a 2d-ary tree of hypercubes and with f belonging to the sets of globally smooth functions: (a) Lip(k), (b) Fκ= {f ∈ Lip(k) satisfying Condition 1 with cκ, κ ≥ 0} and (c) F ′κ = {f ∈ Fκ : ∃c2 > 0, f(x?)
116	18	The results of the comparison can be found in Table 1.
117	83	In addition to the novel lower bounds and the rate over Lip(k), we were able to obtain similar upper bounds as DOO over Fκ, uniformly better rates for the functions inF ′κ locally equivalent to ‖x?
119	32	Hence, when f is only known to be k-Lipschitz, one thus should expect the algorithm exploiting the global smoothness (LIPO) to perform asymptotically better or at least similarly to the one using the local smoothness (DOO) or no information (PRS).
120	33	However, keeping in mind that the constants are not necessarily optimal, it is also interesting to note that the term (k √ d/cκ) d appearing in both the exponential rates of LIPO and DOO tends to suggest that if f is also known to be locally smooth for some k` k, then one should expect an algorithm exploiting the local smoothness k` to be asymptotically faster than the one using the global smoothness k in the case where κ = 1.
122	26	The AdaLIPO algorithm (Algorithm 2) is an extension of LIPO which involves an estimate of the Lipschitz constant and takes as input a parameter p ∈ (0, 1) and a nondecreasing sequence of Lipschitz constant ki∈Z defining a meshgrid of R+ (i.e. such that ∀x > 0, ∃i ∈ Z with ki ≤ x ≤ ki+1).
126	77	Otherwise, if Bt+1 = 0, the algorithm exploits the previous evaluations by making an iteration of the LIPO algorithm with the smallest Lipschitz constant of the sequence k̂t which is associated with a subset of Lipschitz functions that probably contains f (step abbreviated in the algorithm by Xt+1 ∼ U(Xk̂t,t)).
127	24	Once an evaluation has been made, the Lipschitz constant estimate k̂t is updated.
