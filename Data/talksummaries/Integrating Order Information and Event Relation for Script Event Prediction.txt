4	35	Another typical use of event chain knowledge is to help infer what is likely to happen next given a previous event sequence in a scenario.
5	27	We investigate the modeling of stereotypical event chains, which is remotely similar to language modeling, but with events being more sparse and flexibly ordered than words.
7	86	Stereotypical knowledge about partially-ordered events, together with their participant roles such as “customer”, “waiter”, and “table”, is conventionally referred to as scripts (Schank et al., 1977).
8	12	NLP algorithms have been investigated for automatically inducing scripts from unstructured texts (Mooney and DeJong, 1985; Chambers and Jurafsky, 2008).
9	12	In particular, Chambers and Jurafsky (2008) made a first attempt to learn scripts from test inducing event 57 chains by grouping events based on their narrative coherence, calculated based on Pairwise Mutual Information (PMI).
10	38	Jans et al. (2012) showed that the method can be improved by calculating event relations using skip bi-gram probabilities, which explicitly model the temporal order of pairs event.
11	19	Jans et al. (2012)’s model is adopted by a line of subsequent methods on inducing event chains from text (Orr et al., 2014; Pichotta and Mooney, 2014; Rudinger et al., 2015).
13	23	Granroth-Wilding and Clark (2016) used a Siamese Network instead of PMI to calculate the coherence between two events.
17	18	These neural methods are consistent with the earlier statistical models in leveraging event-pair relations.
20	44	LSTMs capture significantly more order information compared to the methods of Granroth-Wilding and Clark (2016), Rudinger et al. (2015), and Modi (2016), which model the temporal order of only pairs of events.
26	16	To leverage the advantages of both methods, we propose to integrate chain temporal order information into event relation measuring.
82	12	First, given an event v(a0, a1, a2), a representation layer is used to compose the embeddings of v, a0, a1, and a2 into a single event vector e. Second, a LSTM is used to map a sequence of existing events e1, e2, ..., en−1 into a sequence of hidden vectors h1, h2, ..., hn−1, which encode the temporal order.
84	18	Third, hc is paired with h1, h2, ..., hn−1 individually, and passed to a dynamic memory network to learn the relatedness score s. s is used to denote the connectedness between the candidate subsequent event and the context event chain.
85	17	We learn vector representations of standard events by composing pre-trained word embeddings of its verb and arguments.
90	19	Denoting the embeddings of v, a0, a1, and a2 as e(v), e(a0), e(a1), and e(a2), respectively, the embedding of e is calculated using a tanh composition layer e(e) = tanh(W ve · e(v) +W 0e · e(a0)+ W 1e · e(a1) +W 2e · e(a2) + be) (1) Here W ve , W 0 e , W 1 e , W 2 e , and b are model parameters, which are randomly initialized and tuned during the training of the main network.
91	110	Given the embeddings of the existing chain of events e1, e2, ..., en−1, we use a standard LSTM (Hochreiter and Schmidhuber, 1997) without coupled input and forget gates or peephole connections to model the temporal order.
92	77	We obtain a sequence of hidden state vectors h1, h2, ..., hn−1 by recurrently feeding e(e1), e(e2), ..., e(en−1) as inputs to the LSTM, where hi = LSTM(e(ei), hi−1).
94	77	Now for each candidate next event ec, we obtain its vector representation e(ec) in the same way as for e1 to en−1.
96	28	With multiple next event candidates e1c , e 2 c , ..., e m c (m ∈ [1,∞]), m feature vectors are obtained, as shown in Figure 4, each being used as a basis for estimating the probability of the corresponding event candidate.
97	27	After obtaining the hidden states for events, we model event pair relations using these hidden state vectors.
101	44	Given the relation score si between hc and each existing event hi, the likelihood of ec given e1, e2, ..., en−1 can be calculated as the average of si: s = ∑n−1 i=1 si n− 1 (3) Weighting existing events.
102	19	The drawback of above approach is that it considers the contribution of each event on the chain is same.
110	52	Such as question answering (Sukhbaatar et al., 2015; Kumar et al., 2016) and reading comprehension (Hermann et al., 2015; Weston et al., 2015).
111	10	Our task is analogous to such semantic tasks in the sense that deep semantic information can be necessary for making the most rational inference.
113	123	Different from the previous researches, we use the memory network to model the event chain, refining the attention mechanism used to explore the pair-wise relation between events.
114	57	The memory model consists of multiple dynamic computational layers (hops).
115	62	For the first layer (hop 1), the weights α for existing events e1, e2, ..., en−1 can be calculated using the same attention mechanism as Eq.4 and Eq.5.
116	65	Given the weights α, we build a consolidated representation of context event chain e1, e2, ..., en−1 as a weighted sum of h1, h2, ..., hn−1: he = n−1∑ i−1 αi · hi (7) The event candidate hc and the new representation of the existing chain he can be further integrated to deduce a deeper representation of the full event chain hypothesis to the next layer (hop 2), denoted as v. v contains deeper semantic information compared with hc, which encode the temporal order of the event chain [h1, h2, ..., hn−1, hc] without differentiating the weights of each event.
121	20	Given a set of event chains, each with a goldstandard subsequent event and a number of nonsubsequent events, our training objective is to minimize the cross-entropy loss between the gold subsequent event and the set of non-subsequent events.
122	13	The loss function of event chain prediction is that: L(Θ) = N∑ i=1 (si − yi)2 + λ2 ||Θ|| 2 (11) where si is the relation score, yi is the label of the candidate (yi = 1 for positive sample, and yi = 0 for negative sample), Θ is the set of model parameters and λ is a parameter for L2 regularization.
