1	69	Two types of nonterminal symbols are used in translation rules: nonterminal X in ordinary SCFG rules and nonterminal S in glue rules that are specially introduced to concatenate nonterminal Xs in a monotonic manner.
2	44	The same generic symbol X for all ordinary nonterminals makes it difficult to distinguish and select proper translation rules.
3	25	In order to address this issue, researchers either use syntactic labels to annotate nonterminal Xs (Zollmann and Venugopal, 2006; Zollmann and Vogel, 2011; Li et al., 2012; Hanneman and Lavie, 2013), or employ syntactic information from parse trees to refine nonterminals with realvalued vectors (Venugopal et al., 2009; Huang et al., 2013).
4	24	In addition to syntactic knowledge, semantic structures are also leveraged to refine nonterminals (Gao and Vogel, 2011).
5	22	All these efforts focus on incorporating linguistic knowledge into hierarchical translation rules.
6	47	Unfortunately, syntactic or semantic parsers for many languages are not accessible due to the lack of labeled training data.
11	15	These representations have been used successfully in various NLP tasks.
12	15	However, there is no attempt to learn semantic representations for nonterminals from unlabeled data.
16	25	We further build a semantic nonterminal refinement model with semantic representations of nonterminals to compute similarities between phrasal substitutions and nonterminals.
17	24	In doing so, we want to enhance phrasal substitution and translation rule selection during decoding.
18	15	The big challenge here is that thousands of tar- 1391 get phrasal substitutions will be generated for one single nonterminal during decoding.
21	36	In the first method, we project representations of source phrases onto their target counterparts linearly/nonlinearly via a neural network.
71	17	We further feed these learned word embeddings to recursive autoencoders (RAE) (Socher et al., 2011) for learning phrase representations.
91	23	We propose two general approaches to obtain semantic vectors for nonterminals: a weighted mean value method and a minimum distance method.
112	23	Representations for nonterminals can be on either the source or target side.
122	48	Given a phrase vector representation ~p and nonterminalX semantic vector ~vx, Cosine Similarity (CS) is computed as: cos(~p, ~vx) = ~p · ~vx ‖~p‖‖ ~vx‖ (13) We set α for the Cosine Similarity between the glue rule and its corresponding phrase as follows: SeSim = { cos(~p, ~vx) hierarchical rules α glue rules (14) As for Euclidean Distance (ED), it is computed according to the following formula: dist(~p, ~vx) = √√√√ d∑ i=1 (pi − vxi)2 (15) and similarly we set β for glue rules: SeSim = { dist(~p, ~vx) hierarchical rules β glue rules (16)
134	63	In the two-pass decoding, we collect target phrase candidates from 100-best translations for each source sentence generated by the baseline in the first pass and learn vector representations for these target phrase candidates.
135	23	Then in the second pass, we decode source sentence with our target semantic nonterminal refinement model using learned target phrase vector representations.
136	43	If a target phrase appears in the collected set, the target-side semantic nonterminal refinement model will calculate the semantic similarity between the target phrase and the corresponding nonterminal on the target semantic space; otherwise the model will give a penalty.
137	18	This is because this phrase is not a desirable phrase as it is not used in 100-best translations.
140	48	In this section, we conducted a series of experiments on Chinese-to-English translation using large-scale bilingual training data, aiming at the following questions: 1.
154	37	The monolingual corpus, which was used to pre-train word embeddings, is extracted from the above parallel corpus in SMT.
155	22	To train vector representations for multi-word phrases, we randomly selected 1M bilingual sentences 5 as training set and used the unsupervised greedy RAE following (Socher et al., 2011).
167	17	Our first group of experiments were carried out to investigate which approach is more appropriate to learn semantic vectors for nonterminals.
169	28	In order to validate the effectiveness of the proposed approaches for learning nonterminal semantic vectors, we combined the minimum distance method (MD) with the Euclidean Distance (ED) because both of them are distance-based, and combined the weighted mean value method (MV) with the Cosine Similarity model (CS) as they belong to vector-based approaches.
191	37	These two models collectively achieve a gain of up to 1.16 BLEU points over the baseline and 0.41 BLEU points over Syn-Mis model on average, which is shown in Table 3.
194	36	As the semantic nonterminal refinement model is capable of selecting more semantically similar translation rules, it achieves statistically significant improvements over the baseline on Chinese-to-English translation.
195	83	Experiment results have shown that • Using (MV + CS) approach to learn semantic representations for nonterminals can achieve better performance than (MD + ED) in terms of BLEU scores.
199	33	For the future work, we are interested in learning bilingual representations (Lauly et al., 2014; Gouws et al., 2014) for nonterminals.
200	54	We also would like to extend our work by using more contextual lexical information to derive semantic vectors for nonterminals.
