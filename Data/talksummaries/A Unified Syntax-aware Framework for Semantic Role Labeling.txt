0	67	The purpose of semantic role labeling (SRL) is to derive the predicate-argument structure of each predicate in a sentence.
1	69	A popular formalism to represent the semantic predicate-argument structure is based on dependencies, namely dependency SRL, which annotates the heads of arguments rather than phrasal arguments.
2	38	Given a sentence (in Figure 1), SRL is generally decomposed A2 AM-TMP A0 Someone always makes you happy make.02 A1 Figure 1: An example of dependency-based SRL.
3	22	into multiple subtasks in pipeline framework, consisting of predicate identification (makes), predicate disambiguation (make.02), argument identification (e.g., Someone) and argument classification (Someone is A0 for the predicate makes).
4	16	SRL is beneficial to a wide range of natural language processing (NLP) tasks, including machine translation (Shi et al., 2016) and question answering (Berant et al., 2013; Yih et al., 2016).
16	77	In addition, as pointed out by He et al. (2017) for span SRL, the worse syntactic input will hurt performance if the syntactically-driven SRL model trusts syntactic information too much, and high-quality syntax can still make a large impact on SRL, which motivates us to investigate the effect of syntactic quality on dependency SRL.
17	22	In summary, our major contributions are as follows: • We propose a unified neural framework for dependency SRL to more effectively integrate syntactic information with multiple methods.
18	19	• Our SRL framework incorporated with syntax achieves the new state-of-the-art results on both English and Chinese CoNLL-2009 benchmarks.
22	39	In this work, we construct a general SRL framework for argument labeling.
23	49	As shown in Figure 2, our SRL framework includes three main modules, (1) BiLSTM encoder that directly takes sequential inputs, (2) MLP with highway connections for softmax output layer, and (3) an optional syntactic encoder that receives the outputs of the BiLSTM encoder and then let its own outputs integrate with the BiLSTM outputs through residual connections.
30	35	BiLSTM encoder We use bi-directional Long Short-term Memory neural network (BiLSTM) (Hochreiter and Schmidhuber, 1997) as the sentence encoder to model sequential inputs.
41	28	To integrate the syntactic information into sequential neural networks, we employ a syntactic encoder on top of the BiLSTM encoder.
47	16	Then, we will provide a brief introduction in subsequent subsections.
48	29	GCN (Kipf and Welling, 2017) is proposed to induce the representations of nodes in a graph based on the properties of their neighbors.
49	18	Given its effectiveness, Marcheggiani and Titov (2017) in- troduce a generalized version for the SRL task, namely syntactic GCN, and shows that syntactic GCN is effective in incorporating syntactic information into neural models.
51	16	Besides, it also models the information flows from a node to itself, namely, it assumes that a syntactic graph contains self-loop for each node.
67	19	Specifically, the main difference between TreeLSTM unit and the standard one is that the memory cell updating and the calculation of gating vectors are depended on multiple child units.
84	17	In addition, we use 300-dimensional ELMo embedding for English2.
86	45	The BiLSTM encoder consists of 4-layer BiLSTM with 512- dimensional hidden units.
96	13	More specifically, Tree-LSTM only considers information from arbitrary child units so that each node lacks of the information from parent.
98	44	For Chinese (Table 2), even though we use the same parameters as for English, our models are still comparable with the best reported results.
103	13	Effect of word representation In order to better understand how the enhanced word representation influences our model performance, we train our Syn-GCN model with different settings in input word embeddings.
110	16	To further investigate the impact of deep encoder, we perform our Syn-GCN, SA-LSTM and Tree-LSTM models with another alternative configuration, using the same encoder as (Marcheggiani and Titov, 2017) (M&T encoder for short), which removes the residual connections from our framework.
115	19	Nevertheless, the overall results show that applying deep encoder could receive higher gains.
118	17	To this end, we further carry out experiments on English test data with different syntactic inputs based on our Syn-GCN model.
127	16	Though not so surprised, these results show that our SRL component is even relatively stronger.
128	37	Third, when we adopt a syntactic parser with higher parsing accuracy, our SRL system will achieve a better performance.
130	36	It suggests that high-quality syntactic parse may indeed enhance SRL, which is consistent with the conclusion in (He et al., 2017).
154	70	This paper presents a unified neural framework for dependency-based SRL, effectively incorporating syntactic information by directly modeling syntax based on syntactic parse tree.
157	46	Our experiments specially show that giving an enlarged performance gap from syntax-agnostic to syntax-aware setting, SRL can be further promoted with the help of deep enhanced representation and effective methods of integrating syntax.
