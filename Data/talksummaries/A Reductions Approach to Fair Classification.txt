0	65	Over the past few years, the media have paid considerable attention to machine learning systems and their ability to inadvertently discriminate against minorities, historically disadvantaged populations, and other protected groups when allocating resources (e.g., loans) or opportunities (e.g., jobs).
1	40	In response to this scrutiny—and driven by ongoing debates and collaborations with lawyers, policy-makers, social scientists, and others (e.g., Barocas & Selbst, 2016)—machine learning researchers have begun to turn their attention to the topic of “fairness in machine learning,” and, in particular, to the design of fair classification and regression algorithms.
2	122	In this paper we study the task of binary classification subject to fairness constraints with respect to a pre-defined protected attribute, such as race or sex.
3	49	Previous work in this area can be divided into two broad groups of approaches.
6	75	The second group of approaches eliminate the restriction to specific classifier families and treat the underlying classification method as a “black box,” while implementing a wrapper that either works by pre-processing the data or post-processing the classifier’s predictions (e.g., Kamiran & Calders, 2012; Feldman et al., 2015; Hardt et al., 2016; Calmon et al., 2017).
8	49	In contrast, post-processing allows a wider range of fairness definitions and results in provable fairness guarantees.
9	37	However, it is not guaranteed to find the most accurate fair classifier, and requires test-time access to the protected attribute, which might not be available.
20	24	Our approach avoids partitioning the data and assumes access only to a classification algorithm rather than a transfer learning algorithm.
22	21	In the next section, we formalize our problem.
27	17	We consider a binary classification setting where the training examples consist of triples (X,A, Y ), whereX ∈ X is a feature vector, A ∈ A is a protected attribute, and Y ∈ {0, 1} is a label.
31	19	Our goal is to learn an accurate classifier h : X→ {0, 1} from some set (i.e., family) of classifiers H, such as linear threshold rules, decision trees, or neural nets, while satisfying some definition of fairness.
35	28	A classifier h satisfies demographic parity under a distribution over (X,A, Y ) if its prediction h(X) is statistically independent of the protected attribute A—that is, if P[h(X) = ŷ | A = a] = P[h(X) = ŷ] for all a, ŷ.
41	51	A classifier h satisfies equalized odds under a distribution over (X,A, Y ) if its prediction h(X) is conditionally independent of the protected attribute A given the label Y—that is, if P[h(X) = ŷ | A = a, Y = y] = P[h(X) = ŷ | Y = y] for all a, y, and ŷ.
42	75	We now show how each definition can be viewed as a special case of a general set of linear constraints of the form Mµ(h) ≤ c, (1) where matrix M ∈ R|K|×|J| and vector c ∈ R|K| describe the linear constraints, each indexed by k ∈ K, and µ(h) ∈ R|J| is a vector of conditional moments of the form µj(h) = E [ gj(X,A, Y, h(X)) ∣∣ Ej ] for j ∈ J, where gj : X×A× {0, 1} × {0, 1} → [0, 1] and Ej is an event defined with respect to (X,A, Y ).
44	38	In a binary classification setting, demographic parity can be expressed as a set of |A| equality constraints, each of the form E[h(X) |A = a] = E[h(X)].
58	17	The resulting classification error is err(Q) = ∑ h∈HQ(h) err(h) and the conditional moments are µ(Q) = ∑ h∈HQ(h)µ(h) (see Appendix A for the derivation).
59	23	Thus we seek to solve min Q∈∆ err(Q) subject to Mµ(Q) ≤ c, (3) where ∆ is the set of all distributions over H. In practice, we do not know the true distribution over (X,A, Y ) and only have access to a data set of training examples {(Xi, Ai, Yi)}ni=1.
72	25	We begin by introducing a Lagrange multiplier λk ≥ 0 for each of the |K| constraints, summarized as λ ∈ R|K|+ , and form the Lagrangian L(Q,λ) = êrr(Q) + λ> ( Mµ̂(Q)− ĉ ) .
106	16	Using the matrix M for demographic parity as described in Section 2, the cost-sensitive reduction for a vector of Lagrange multipliers λ uses costs C0i = 1{Yi 6= 0}, C1i = 1{Yi 6= 1}+ λAi pAi − ∑ a∈A λa, where pa := P̂[A = a] and λa := λ(a,+) − λ(a,−), effectively replacing two non-negative Lagrange multipliers by a single multiplier, which can be either positive or negative.
120	16	To bound the statistical error, we use the Rademacher complexity of the classifier family H, which we denote by Rn(H), where n is the number of training examples.
134	56	In other words, the solution returned by Algorithm 1 achieves the lowest feasible classification error on the true distribution up to the optimization error, which grows linearly with ν, and the statistical error, which grows as n−α.
170	18	• ProPublica’s COMPAS recidivism data (7,918 examples).
173	23	Here the task is to predict someone’s eventual passage of the bar exam, with race (restricted to white and black only) as the protected attribute.
175	28	Here the task is to predict whether or not someone has a prestigious occupation, with gender as the protected attribute.
178	42	We used the test examples to measure the classification error for each approach, as well as the violation of the desired fairness constraints, i.e., maxa ∣∣E[h(X) |A = a]− E[h(X)]∣∣ and maxa,y ∣∣E[h(X) | A = a, Y = y]− E[h(X) | Y = y]∣∣ for demographic parity and equalized odds, respectively.
179	40	We ran our reduction across a wide range of tradeoffs between the classification error and fairness constraints.
180	37	, 0.1} and for each value ran Algorithm 1 with ĉk = ε across all k. As expected, the returned randomized classifiers tracked the training Pareto frontier (see Figure 2 in Appendix D).
182	53	For all the data sets, the range of classification errors is much smaller than the range of constraint violations.
183	24	Almost all the approaches were able to substantially reduce or remove disparity without much impact on classifier accuracy.
188	38	As expected, post-processing yielded disparities that were statistically indistinguishable from zero, but the resulting classification error was sometimes higher than achieved by our reduction under a statistically indistinguishable disparity.
