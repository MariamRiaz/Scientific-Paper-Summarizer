0	45	In this paper, we are interested in solving the following stochastic optimization problem: min w∈K F (w) , Eξ[f(w; ξ)], (1) where ξ is a random variable, f(w; ξ) is a convex function of w, Eξ[·] is the expectation over ξ and K is a convex domain.
1	11	Let K∗ denote the optimal set of (1) and F∗ denote the optimal value.
2	15	Traditional stochastic subgradient (SSG) method updates the solution according to wt+1 = ΠK[wt − ηt∂f(wt; ξt)], (2) for t = 1, .
3	14	, T , where ξt is a sampled value of ξ at t-th iteration, ηt is a step size and ΠK[w] = arg minv∈K ‖w − v‖22 is a projection operator that projects a point into K. Previous studies have shown that under the following assumptions i) ‖∂f(w; ξ)‖2 ≤ G, ii) there exists w∗ ∈ K∗ such that ‖wt −w∗‖2 ≤ B for t = 1, .
7	14	Recently, there emerges a stream of studies on various variance reduction techniques to accelerate stochastic gradient method (Roux et al., 2012; Zhang et al., 2013; Johnson & Zhang, 2013; Xiao & Zhang, 2014; Defazio et al., 2014).
8	11	However, they all hinge on the smoothness assumption.
9	12	The proposed algorithms in this work tackle the issue of variance of stochastic subgradient without the smoothness assumption from another pespective.
11	23	This observation has also been leveraged in previous analysis to design faster convergence for stochastic convex optimization that use a strong or uniform convexity condition (Hazan & Kale, 2011; Juditsky & Nesterov, 2014) or a global growth condition (Ramdas & Singh, 2013) to control the distance of intermediate solutions to the optimal solution by their functional residuals.
13	10	In contrast, we develop a new theory only relying on the local growth condition to control the distance of intermediate solutions to the -optimal solution by their functional residuals but achieving a fast global convergence.
15	31	In addition, the present work will demonstrate the improved results and practicability of the proposed algorithms for many problems in machine learning, which is lacking in similar previous work.
34	11	Recall the notations K∗ and F∗ that denote the optimal set of (1) and the optimal value, respectively.
35	13	For the optimization problem in (1), we make the following assumption throughout the paper.
48	26	We quantify the functional local growth rate by measuring how fast the functional value increase when moving a point away from the optimal solution in the -sublevel set.
50	17	The inequality in (5) can be equivalently written as ‖w −w∗‖2 ≤ c(F (w)− F∗)θ, ∀w ∈ S , (6) where c = 1/λθ, which is called as local error bound condition in (Yang & Lin, 2016).
56	17	Strong convexity or uniform convexity condition implies LGC with θ = 1/2, but not vice versa.
62	12	The third observation shows that LGC could imply faster convergence than that induced by GGC.
74	13	The last observation is that the LGC is equivalent to a Kurdyka - Łojasiewicz inequality (KL), which was proved in (Bolte et al., 2015, Theorem 5).
84	11	To formally illustrate this, we consider the following stochastic subgradient update: wτ+1 = ΠK∩B(w1,D)[wτ − η∇f(wτ ; ξτ )].
108	16	It is worth mentioning that unlike traditional highprobability analysis of SSG that usually requires the domain to be bounded, the convergence analysis of ASSG does not rely on such a condition.
131	16	As a result, the iteration complexity of ASSG-r for achieving an 2 -optimal solution with a high probability 1 − δ is O(c2G2 log( 0/ ) log(1/δ)/ 2(1−θ)) provided β1 = O( 2c 2 0 2(1−θ) ).
140	9	Then Algorithm 3 ASSG with Restarting: RASSG 1: Input: w(0), K, D(1)1 , t1, 0 and ω ∈ (0, 1] 2: Set (1)0 = 0, η1 = 0/(3G 2) 3: for s = 1, 2, .
153	15	The total number of iterations of RASSG for obtaining 2 -optimal solution is upper bounded by TS = O(dlog2( 0 )e log(1/δ) G2B2̂1 2 ).
180	9	The huber loss function `(z, y) = `γ(z − y) has been used for robust regression.
193	14	We can consider an `1 constrained `p norm regression (Nyquist, 1983): min ‖w‖1≤s F (w) , 1 n n∑ i=1 (x>i w − yi)p, p ∈ 2N+.
206	10	In implementing the RASSG, we restart every 5 stages with t increased by a factor of 1.15, 2 and 2 respectively for hinge loss, Huber loss and robust regression.
209	28	The figures show that (i) ASSG can quickly converge to a certain level set determined implicitly by t; (ii) RASSG converges much faster than SSG to more accurate solutions; (iii) RASSG can gradually decrease the objective value.
210	26	Finally, we compare RASSG with state-of-art stochastic optimization algorithms for solving a finite-sum problem with a smooth piecewise quadratic loss (e.g., squared hinge loss, huber loss) and an `1 norm regularization.
216	16	The results show that RASSG converges faster than other three algorithms for the two tasks.
217	20	This is not surprising considering that RASSG, SAGA and SVRG++ suffer from an iteration complexity of Õ(1/ ), O(n/ ), and O(n log(1/ ) + 1/ ), respectively.
218	13	In this paper, we have proposed accelerated stochastic subgradient methods for solving general non-strongly convex stochastic optimization under the functional local growth condition.
