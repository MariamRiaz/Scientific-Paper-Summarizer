0	27	Broad-coverage knowledge bases (KBs) such as Freebase (Bollacker et al., 2008) and DBPedia (Auer et al., 2007) store a large amount of facts in the form of 〈head entity, relation, tail entity〉 triples (e.g. 〈The Matrix, country of film, Australia〉), which could support a wide range of reasoning and question answering applications.
1	69	The Knowledge Base Completion (KBC) task aims to predict the missing part of an incomplete triple, such as 〈Finding Nemo, country of film, ?〉, by reasoning from known facts stored in the KB.
2	41	As a most common approach (Wang et al., 2017), modeling entities and relations to operate in a low dimension vector space helps KBC, for three conceivable reasons.
4	37	This could imply that an entity (e.g. US) “type matches” a relation such as country of film.
11	18	In this paper, we investigate an alternative approach by training relation parameters jointly with an autoencoder (Figure 1).
13	16	We show this novel technique promotes parameter sharing between different relations, and drives them toward low dimension manifolds (Sec.6.2).
17	14	We evaluate our system using standard KBC datasets, achieving state-of-the-art on several of them (Sec.6.1), with strongly improved Mean Rank.
19	23	A knowledge base (KB) is a set T of triples of the form 〈h, r, t〉, where h, t ∈ E are entities and r ∈ R is a relation (e.g. 〈The Matrix, country of film, Australia〉).
23	14	The model we implement in this work represents entities h, t as d-dimension vectors uh,vt respectively, and relation r as a d×d matrix Mr.
25	14	This motivates us to use u>hMrvt as a natural parameter to model plausibility of 〈h, r, t〉, even in a low dimension space with d |E|.
59	98	In Sec.6.2, we further show that joint training drives relations toward a low dimension manifold.
64	29	After extensive pre-experiments, we have found some crucial settings for successful training.
69	32	We use Stochastic Gradient Descent (SGD) for optimization, and the common practice (Bottou, 2012) is to set the learning rate as α(τ) := η 1 + ηλτ .
71	23	In this work, in order to control the updates in detail to keep a balance, we modify (4) to use a a step counter τr for each relation r, counting “number of updates” instead of data points2.
75	24	The rule for setting η1, λ1 and η2, λ2 is that, η2 should be much smaller than η1, because η1, η2 control the magnitude of learning rates at the early stage of training, with the autoencoder still largely random and ∆2 not making much sense; on the other hand, one has to choose λ1 and λ2 such that ‖∆1‖/λ1 and ‖∆2‖/λ2 are at the same scale, because the learning rates approach 1/(λ1τr) and 1/(λ2τr) respectively, as the training proceeds.
82	28	Besides the tricks for joint training, we also found settings that significantly improve the base model on KBC, as briefly discussed below.
115	22	We evaluate on standard KBC datasets, including WN18 and FB15k (Bordes et al., 2013), WN18RR (Dettmers et al., 2018) and FB15k-237 (Toutanova and Chen, 2015).
121	19	For all datasets, we set the dimension d = 256 and c = 16, the SGD hyper-parameters η1 = 1/64, η2 = 2 −14 and λ1 = λ2 = 2−14.
123	20	We compare the base model (BASE) to our joint training with an autoencoder model (JOINT), and the base model with compositional training (BASE+COMP) to our joint model with compositional training (JOINT+COMP).
132	19	This is convincing because generally, joint training contributes with its regularizing effects, and drastic improvements are less expected3.
137	16	For re-experiments, we use Lin et al. (2015b)’s implementation4 of TransE (Bordes et al., 2013) and TransR, which represent relations as vector translations; and Nickel et al. (2016b)’s implementation5 of RESCAL (Nickel et al., 2011) and HolE, where RESCAL is most similar to the BASE model and HolE is a more parameter-efficient variant.
147	22	How does joint training affect relation matrices?
148	72	We address these questions by analyses showing that (i) the autoencoder learns sparse and interpretable codings of relations, (ii) the joint training drives relation matrices toward a low dimension manifold, and (iii) it helps discovering compositional constraints.
149	41	Sparse Coding and Interpretability Due to the ReLU function in (2), our autoencoder learns sparse coding, with most relations having large code values at only two or three dimensions.
151	24	In the first group of Figure 2, we show a small number of relations that are almost always assigned a near one-hot coding, regardless of initialization.
155	55	This kind of relation clustering also seems independent of initialization.
159	51	Low dimension manifold In order to visualize the relation matrices learned by our joint and base models, we use UMAP7 (McInnes and Healy, 2018) to embed Mr into a 2D plane8.
162	144	We can see that Figure 3a and Figure 3c are mostly similar, with high frequency relations scattered randomly around a low frequency cluster, suggesting that they come from various directions of a high dimension space, with frequent relations probably being pulled further by the training updates.
163	15	On the other hand, in Figure 3b and Figure 3d we found less frequent relations being clustered with frequent ones, and multiple traces of low dimension structures.
166	37	Compositional constraints In order to directly evaluate a model’s ability to find compositional constraints, we extracted from FB15k-237 a list of (r1/r2, r3) pairs such that r1/r2 matches r3.
