16	55	Furthermore, in order to reduce the dependence on the data in the target language, which may be low resource, we first learn the policy of active 595 learning on another language and then transfer it to the target language.
18	70	We use cross-lingual word embeddings to learn compatible data representations for both languages, such that the learned policy can be easily ported into the other language.
24	28	In Section 2, we briefly review some related work.
27	116	We conclude our work in Section 5.
42	125	We now show how active learning can be formalised as as a decision process, and then show how this allows for the active learning selection policy to be learned from data using deep reinforcement learning.
44	41	Active learning is a simple technique for labelling data, which involves first selecting some instances from an unlabelled dataset, which are then annotated by a human oracle, which is then repeated many times until a termination criterion is satisfied, e.g., the annotation budget is exhausted.
45	68	Most often the selection function is based on the pre- dictions of a trained model, which has been fit to the labelled dataset at each stage in the algorithm, where datapoints are selected based on the model’s predictive uncertainty (Lewis and Gale, 1994), or divergence in predictions over an ensemble (Seung et al., 1992).
46	88	The key idea of these methods is to find the instances on which the model is most likely to make errors, such that after their labelling and inclusion in the training set, the model becomes more robust to these types of errors on unseen data.
48	76	Accordingly, the state corresponds to the selected data for labelling and their labels, and each step in the active learning algorithm corresponds to a selection action, wherein the heuristic selects the next items from a pool.
50	50	Effectively the active learning heuristic is operating as a decision policy, a form of function taking as input the current state — comprising the labelled data, from which a model is trained — and a candidate unlabelled data point — e.g., the model uncertainty.
53	39	For simplicity, we make a streaming assumption, whereby unlabelled data (sentences) arrive in a stream (Lewis and Gale, 1994).2 As each instance arrives, an agent must decide the action to take, namely whether or not the instance should be manually annotated.
58	126	As illustrated in Figure 1 at each time, the agent observes the current state si which includes the sentence xi, and the learned model φ.
59	24	The agent selects a binary action ai, denoting whether to label xi, according to the policy π.
60	24	For ai = 1, the corresponding sentence is labelled and added to the labelled data, and the model pφ updated to include this new training point.
62	26	After termination a reward is computed based on the accuracy of the final model, φ.
65	35	We represent the state using a continuous vector, using the concatenation of the vector representation of xi, and outputs of the model pφ trained over the labelled data.
66	28	These outputs use both the predictive marginal distributions of the model on the instance, and a representation of the model’s confidence.
68	86	Content representation A key input to the agent is the content of the sentence, xi, which we encode using a convolutional neural network to arrive at a fixed sized vector representation, following Kim (2014).
69	23	This involves embedding each of the n words in the sentence to produce a matrix Xi = {xi,1, xi,2, · · · , xi,n}, after which a series of wide convolutional filters is applied, using multiple filters with different gram sizes.
72	33	Representation of marginals The prediction outputs of the training model, pφ(y|xi), are central to all active learning heuristics, and accordingly, we include this in our approach.
109	39	The parameters in the DQN are learnt using stochastic gradient descent, based on a regression objective to match the Q-values predicted by the DQN and the expected Q-values from the Bellman equation, ri + γmaxaQ(si+1, a; θ).
110	85	Following (Mnih et al., 2015), we use an experience replay memory M to store each transition (s, a, r, s′) as it is used in an episode, after which Algorithm 2 Active learning by policy transfer Input: unlabelled data D, budget B, policy π Output: Dl 1: Dl ← ∅ 2: φ← Random 3: for |Dl| 6= B and D not empty do 4: Randomly sample xi from the data pool D and construct the state si 5: The agent chooses an action ai according to ai = arg maxQπ(si, a) 6: if ai = 1 then 7: Obtain the annotation yi 8: Dl ← Dl + (xi,yi) 9: Update model φ based on Dl 10: end if 11: D ← D\xi 12: Receive a reward ri using held-out set 13: Update policy π 14: end for 15: return Dl we sample a mini-batch of transitions from the memory and then minimize the loss function: L(θ) = Es,a,r,s′ [( yi(r, s′)−Q(s, a; θ) )2] , (2) where yi(r, s′) = r + γmaxa′ Q(s′, a′; θi−1) is the target Q-value, based on the current parameters θi−1, and the expectation is over the minibatch.
135	24	We conduct experiments to validate the proposed active learning method in a cross-lingual setting, whereby an active learning policy trained on a source language is transferred to a target language.
136	39	We allow repeated active learning simulations on the source language, where annotated corpora are plentiful, to learn a policy, while for target languages we only permit a single episode, to mimic a language without existing resources.
163	49	Recall that in this setting there are no policy or model updates, as no heldout data is used, and all annotations arrive in a batch.
164	103	The model, however, is initialised with a NER tagger trained on a different language, which explains why the performance for all methods starts from around 40% rather than 0%.
166	85	Lastly, we report the results for all approaches in Table 2, based on training on the full 200 labelled sentences as selected under the different methods.
172	37	Based on this, we design an active learning algorithm as a policy based on deep reinforcement learning.
173	116	We show how these learned active learning policies can be transferred between languages, which we empirically show provides consistent and sizeable improvements over baseline methods, including traditional uncertainty sampling.
174	475	This holds true even in a very difficult cold-start setting, where no evaluation data is available, and there is no ability to react to annotations.
