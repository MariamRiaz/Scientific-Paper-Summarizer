2	60	In this paper we address the non-convex optimization problem of training a feedforward neural network.
3	20	This problem turns out to be very difficult as there can be exponentially many distinct local minima (Auer et al., 1996; Safran & Shamir, 2016).
4	16	It has been shown that the training of a network with a single neuron with a variety of activation functions turns out to be NP-hard (Sima, 2002).
5	50	In practice local search techniques like stochastic gradient descent or variants are used for training deep neural networks.
6	35	Surprisingly, it has been observed (Dauphin et al., 2014; Goodfellow et al., 2015) that in the training of stateof-the-art feedforward neural networks with sparse connectivity like convolutional neural networks (LeCun et al., 1990; Krizhevsky et al., 2012) or fully connected ones one does not encounter problems with suboptimal local minima.
8	56	On the theoretical side there have been several interesting developments recently, see e.g. (Brutzkus & Globerson, 2017; Lee et al., 2016; Poggio & Liao, 2017; Rister & Rubin, 2017; Soudry & Hoffer, 2017; Zhou & Feng, 2017).
12	32	While this is a highly non-trivial result as the optimization problem is non-convex, deep linear networks are not interesting in practice as one efficiently just learns a linear function.
13	20	In order to characterize the loss surface for general networks, an interesting approach has been taken by (Choromanska et al., 2015a).
14	18	By randomizing the nonlinear part of a feedforward network with ReLU activation function and making some additional simplifying assumptions, they can relate it to a certain spin glass model which one can analyze.
15	14	In this model the objective of local minima is close to the global optimum and the number of bad local minima decreases quickly with the distance to the global optimum.
17	12	It has recently been shown (Kawaguchi, 2016) that if some of these assumptions are dropped one basically recovers the result of the linear case, but the model is still unrealistic.
44	16	Let fk, gk : Rd → Rnk be the mappings from the input space to the feature space at layer k, which are defined as f0(x) = x, fk(x) = σ(gk(x)), gk(x) = W T k fk−1(x) + bk for every k ∈ [L], x ∈ Rd.
52	12	The idea of backpropagation is the core of our theoretical analysis.
53	14	Lemma 2.1 below shows well-known relations for feed-forward neural networks, which are used throughout the paper.
59	14	∇bkΦ = ∆Tk 1N ∀ k ∈ [L] Note that Lemma 2.1 does not apply to non-differentiable activation functions like the ReLU function, σReLU(x) = max{0, x}.
84	33	A typical example is the squared loss l(a) = a2 or the PseudoHuber loss (Hartley & Zisserman, 2004) given as lδ(a) = 2δ2( √ 1 + a2/δ2 − 1) which approximates a2 for small a and is linear with slope 2δ for large a.
85	24	But also nonconvex loss functions satisfy this requirement, e.g. the Blake-Zisserman, corrupted Gaussian and Cauchy functions (Hartley & Zisserman, 2004) (p.617-p.619).
86	21	As a motivation for our main result, we first analyze the case when the training samples are linearly independent, which requiresN ≤ d+1.
106	40	The third condition is trivially satisfied by a critical point and the requirement of full column rank of the weight matrices is similar to Theorem 3.4.
107	46	However, the first one may not be fulfilled since rank([Fk,1N ]) is dependent not only on the weights but also on the architecture.
108	45	The main difficulty of the proof of our following main theorem is to prove that this first condition holds under the rather simple requirement that nk ≥ N − 1 for a subset of all critical points.
112	10	When |S| = n, we write∇2f(x) ∈ Rn×n to denote the full Hessian matrix.
122	26	First of all we note that the full column rank condition of (Wl) L l=k+2 in Theorem 3.4, and 3.8 implicitly requires that nk+1 ≥ nk+2 ≥ .
125	44	Indeed, one can even argue that Theorem 3.8 gives an implicit justification as it hints on the fact that such networks are easy to train if one layer is sufficiently wide.
126	24	Note that Theorem 3.8 does not require fully nondegenerate critical points but non-degeneracy is only needed for some subset of variables that includes layer k + 1.
127	38	As a consequence of Theorem 3.8, we get directly a stronger result for non-degenerate local minima.
128	46	Corollary 3.9 Let Φ : P → R be defined as in (1) and let the Assumptions 3.2 hold.
129	19	Then every non-degenerate local minimum (W ∗l , b ∗ l ) L l=1 of Φ for which (W ∗ l ) L l=k+2 has full column rank, that is rank(W ∗l ) = nl, is a global minimum of Φ.
131	33	First, note that Theorem 3.8 is slightly weaker than Theorem 3.4 as it requires also non-degeneracy wrt to a set of variables including layer k + 1.
133	10	On the other hand it makes also very strong statements.
