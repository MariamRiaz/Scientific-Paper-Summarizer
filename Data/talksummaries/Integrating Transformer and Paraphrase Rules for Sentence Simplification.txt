40	11	As shown in Figure 1, our model based on the Transformer architecture works as follows: given a pair consisting a normal sentence I and a simple sentence O, the model learns the mapping from I to O.
57	35	The Deep Critic Sentence Simplification (DCSS) model is designed to apply rules identified by the Simple PPDB by introducing a new loss function.
60	103	For example, given a normal sentence in the training set, “the recipient of the kate greenaway medal”, the simplified sentence is “the winner of the kate greenaway medal.”, where “recipient” is simplified to “winner”, which is identified by Simple PPDB.
61	7	The major goal of the loss functions is to support the model in generating the simplified word “winner” while deterring the model from generating the word “recipient”.
62	50	Specifically, for an applicable simplification rule, our new loss function maximizes the probability of generating the simplified form (word “winner”) and meanwhile minimizes the probability of generating the original form (word “recipient”).
63	23	As in Equation 4, where wrule indicates the weight of the simplification rule provided by the Simple PPDB, once the model generates “recipient”, the model is criticized to generate word “winner”; when model predicts correctly with “winner”, the model is trained to minimize the probability of “recipient”.
64	10	In this way, the model avoids selecting normal words and instead becomes inclined to choose the simplified words.
65	28	Lcritic =  −wrulelogP (winner|I, θ) if model generates recipient wrulelogP (recipient|I, θ) if model generates winner (4) The Lcritic merely focuses on the words identified by the Simple PPDB and Lseq focuses on the entire vocabulary.
66	47	So, the model is trained in an end-to-end fashion by minimizing Lseq and Lcritic alternately.
67	21	DCSS, similar to the majority of neural network models, uses a piece of shared memory, i.e. the parameters, as the media to store the learned rules from the data.
68	24	As a result, it still focuses much more on rules that are frequently observed and ignores the rules observed infrequently.
69	21	However, infrequent rules are still important, particularly when the training data is limited.
70	139	In order to make full use of the rules in the knowledge base, we introduce the Deep Memory Augmented Sentence Simplification (DMASS) model.
75	11	Given the same example normal sentence “ the recipient of kate greenaway medal”, Simple PPDB determines that the word “recipient” should be simplified to “winner”.
83	23	As shown in Figure 2, current augmented memory contains three candidate rules for the word “recipient”, which indicates that it can be simplified into “winner”, “receiver” or “host”, respectively.
84	13	The current context vector c(1,j) is treated as a query to search for suitable rules by using Equation 5, where αri denotes the weight for i th rule, which is computed through the dot product between current context vector c(1,j) and ci.
86	16	αri = ei∑ j ej ei = exp(c(1,j) · ci) (5) ro = ∑ αri rr rr ∈ [rwinner, rreceiver, rhost] (6) Memory Update Module The task of the memory update module is to update the key and value vectors in the augmented memory.
88	35	If the augmented memory does not contain the key-value pair for the rule, c1,j and rwinner are appended to the memory.
89	14	If the augmented memory contains the key-value pair, the key vector is updated as the mean of current key vector and c1,j .
90	9	Similarly, the value vector is also updated as the mean of current value vector and rwinner.
106	32	FKGL measures the simplicity of a sentence without considering the ground truth simplification references and it correlates little with human judgment (Xu et al., 2016), so we also use another metric, SARI.
108	12	Specifically, it rewards addition operations where a word in the generated simplified sentence does not appear in the normal sentence but is mentioned in the reference sentences.
109	19	It also rewards words kept or deleted in both the simplified sentence and the reference sentences.
113	12	Because SARI rewards deleting and adding separately, we also include another metric to measure the correctness of lexical transformation, namely word simplification, verified by Simple PPDB.
127	39	We compare our results against the RNN/LSTMbased sentence simplification models.
129	57	Table 2 shows the experiment results where LxHy indicates a run with Transformer using x layers and y heads.
130	248	When compared with results of RNN/LSTM, our Transformer-based model performed better in terms of SARI and FKGL values.
131	59	In addition, with the increased number of layers or heads, the values of SARI and FKGL improve accordingly.
132	22	In the remainder of this section, we analyze the insights of these results in detail.
133	17	In our tasks, FKGL measures the sentence length and the word length as two factors for evaluating a simplified sentence.
