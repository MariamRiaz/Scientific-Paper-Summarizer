3	102	At each time step, it consumes one input token, updates its hidden state vector, and predicts the next token by generating a probability distribution over all permissible tokens.
4	96	The probability of an input string is simply obtained as the product of the predictions of the tokens constituting the string followed by a terminating token.
5	102	In this manner, each RNN defines a weighted language; i.e. a total function from strings to weights.
7	12	To this end, a specific architecture with 886 hidden units can simulate any Turing machine in real-time (i.e., each Turing machine step is simulated in a single time step).
15	54	For example, the weighted languages computed by weighted finite-state automata are closed under intersection (pointwise product) and union (pointwise sum), and the corresponding unweighted languages are closed under intersection, union, difference, and complementation (Droste et al., 2013).
16	16	Moreover, toolkits like OpenFST (Allauzen et al., 2007) and Carmel1 implement efficient algorithms on automata like minimization, intersection, finding the highestweighted path and the highest-weighted string.
18	12	For example, an RNN- 2261 based machine translation system should extract the highest-weighted output string (i.e., the most likely translation) generated by an RNN, (Sutskever et al., 2014; Bahdanau et al., 2014).
20	15	To facilitate the deployment of large RNNs onto limited memory devices (like mobile phones) minimization techniques would be beneficial.
23	20	Without a determination of the overall probability mass assigned to all finite strings, a fair comparison of language models with regard to perplexity is simply impossible.
24	56	The goal of this paper is to study the above problems for the mentioned ReLU-variant of RNNs.
80	15	The first two components are ReLU-thresholded to zero until t > 101, at which point they overwhelm the bias towards a turning all future predictions to $.
86	13	However, probabilistic context-free grammars obtained by training on a finite corpus using popular methods (such as expectation-maximization) are guaranteed to be consistent (Nederhof and Satta, 2006).
87	40	hs,t(2) = σ〈hs,t−1(2) + 1〉 hs,t(3) = σ〈hs,t−1(3) + 3hs,t−1(1)〉 Es,t($) = hs,t(3)− hs,t(2) Es,t(a) = hs,t(2) Now we distinguish two cases: Case 1: If hs,t(n) − hs,t(n′) = 0 for all t ∈ N, then hs,t(1) = 0 and hs,t(2) = t + 1 and hs,t(3) = 0.
88	24	Hence we have Es,t($) = −(t + 1) and Es,t(a) = t + 1.
89	58	In this case the termination probability E′s,t($) = e−(t+1) e−(t+1) + et+1 = 1 1 + e2(t+1) (i.e., the likelihood of predicting $) shrinks rapidly towards 0, so the RNN assigns less than 15% of the probability mass to the terminating sequences (i.e., the finite strings), so the RNN is inconsistent (see Lemma 15 in the appendix).
90	116	Case 2: Suppose that there exists a time point T ∈ N such that for all t ∈ N hs,t(n)− hs,t(n′) = { 1 if t = T 0 otherwise.
97	16	Let M be an arbitrary deterministic Turing machine.
105	42	Let M be an arbitrary deterministic Turing machine.
109	36	It follows that M halts if and only if R is consistent.
114	13	A two-layer RNN trained to a local optimum using Back-propagation-throughtime (BPTT) on a finite corpus is not necessarily consistent.
132	16	The best string problem for RNNs is undecidable.
138	50	The consistent best string problem for RNNs is decidable.
142	48	Next, we investigate the length |wmaxR | of the shortest string wmaxR of maximal weight in the weighted language R generated by a consistent RNN R in terms of its (binary storage) size |R|.
143	50	As already mentioned by Siegelmann and Sontag (1995) and evidenced here, only small precision rational numbers are needed in our constructions, so we assume that |R| ≤ c · |N |2 for a (reasonably small) constant c, where N is the set of neurons of R. We show that no computable bound on the length of the best string can exist, so its length can surpass all reasonable bounds.
144	18	Let f : N+ → N be the function with f(n) = max consistent RNN R |R|≤n |wmaxR | for all n ∈ N+.
146	19	In the previous section (before Theorem 6) we presented an RNN RM that simulates an arbitrary (single-track) Turing machine M with n states.
149	29	In the proof of Theorem 8 we have additionally seen that the length |wmaxR | of its best string exceeds the number TM of steps required to halt.
150	40	For every n ∈ N, let BB(n) be the n-th “Busy Beaver” number (Radó, 1962), which is BB(n) = max normalized n-state Turing machine M with 2 tape symbols that halts on empty input TM It is well-known that BB : N+ → N cannot be bounded by any computable function.
153	78	Identifying the best string of polynomial length in a consistent RNN is NP-complete.
155	15	, xm be m Boolean variables and F = k∧ i=1 ( `i1 ∨ `i2 ∨ `i3 ) , be a formula in conjunctive normal form, where `ij ∈ {x1, .
