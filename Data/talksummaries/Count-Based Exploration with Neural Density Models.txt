0	27	Exploration is the process by which an agent learns about its environment.
1	49	In the reinforcement learning framework, this involves reducing the agent’s uncertainty about the environment’s transition dynamics and attainable rewards.
5	94	The pseudocount is defined in terms of a density model ρ trained on the sequence of states experienced by an agent: N̂(x) = ρ(x)n̂(x), where n̂(x) can be thought of as a total pseudo-count computed from the model’s recoding probability ρ′(x), the probability of x computed immediately after training on x.
6	49	As a practical application the authors used the pseudocounts derived from the simple CTS density model (Bellemare et al., 2014) to incentivize exploration in Atari 2600 agents.
7	129	One of the main outcomes of their work was substantial empirical progress on the infamously hard game MONTEZUMA’S REVENGE.
8	34	Their method critically hinged on several assumptions regarding the density model: 1) the model should be learning-positive, i.e. the probability assigned to a state x should increase with training; 2) it should be trained online, using each sample exactly once; and 3) the effective model step-size should decay at a rate of n−1.
13	23	What role does the mixed Monte Carlo update play in successfully incentivizing exploration?
18	21	A CTS model can naturally be trained from sequentially presented, correlated data samples.
27	30	Let ρ be a density model on a finite space X , and ρn(x) the probability assigned by the model to x after being trained on a sequence of states x1, .
35	19	We consider a reinforcement learning (RL) agent interacting with an environment that provides observations and extrinsic rewards (see Sutton & Barto, 1998, for a thorough exposition of the RL framework).
40	20	In its simplest form, the model takes as input a 2D image and assigns to it a probability according to the product of location-dependent L-shaped filters, where the prediction of each filter is given by a CTS algorithm trained on past images.
69	26	See Appendix A for technical details.
77	33	To determine a suitable online learning rate schedule, we train the model on a sequence of 1M frames of experience of a random-policy agent.
87	85	In experiments comparing actual agent performance we empirically determined that in fact the constant learning rate 0.001, paired with a PG decay cn = c · n−1/2, obtains the best exploration results on hard exploration games like MONTEZUMA’S REVENGE, see Fig.
88	34	We find the model to be robust across 1-2 orders of magnitude for the value of c, and informally determine c = 0.1 to be a sensible configuration for achieving good results on a broad range of Atari 2600 games (see also Section 7).
89	43	Regarding (c), it is hard to ensure learning-positiveness for a deep neural model, and a negative PG can occur whenever the optimizer ‘overshoots’ a local loss minimum.
96	24	At each agent step, the density model receives a single frame, with which it simultaneously updates its parameters and outputs the PG.
98	54	The DQN-CTS agent we compare against is derived from the one in (Bellemare et al., 2016).
101	26	Unless stated otherwise, we always use the mixed Monte Carlo update (MMC) for the intrinsically motivated agents3, but regular Q-Learning for the baseline DQN.
104	21	On other hard exploration games (PRIVATE EYE; or VENTURE, appendix Fig.
112	28	The greatest gains from using either exploration bonus are observed in games categorized as hard exploration games in the ‘taxonomy of exploration’ in (Bellemare et al., 2016, reproduced in Appendix D), specifically in the most challenging sparse reward games (e.g. MONTEZUMA’S REVENGE, PRIVATE EYE, VENTURE).
113	19	Empirical practitioners know that techniques beneficial for one agent architecture often can be detrimental for a different algorithm.
114	26	To demonstrate the wide applicability of the PixelCNN exploration bonus, we also evaluate it with the more recent Reactor agent4 (Gruslys et al., 2017).
116	41	To reduce impact on computational efficiency of this agent, we sub-sample intrinsic rewards: we perform updates of the PixelCNN model and compute the reward bonus on (randomly chosen) 25% of all steps, leaving the agent’s reward unchanged on other steps.
120	30	It is further improved on a large fraction of games by the PixelCNN exploration reward, see Fig.
122	32	The effect of the exploration bonus is rather uniform, yielding improvements on a broad range of games.
123	22	In particular, Reactor-PixelCNN enjoys better sample efficiency (in terms of area under the curve, AUC) than vanilla Reactor.
129	31	The Retrace(λ) algorithm, on the other hand, has an effective horizon which depends on λ and, critically, the truncated importance sampling ratio.
133	44	However, we are not using the generative function of the models when computing an exploration bonus, and a better generative model does not necessarily give rise to better probability estimates (Theis et al., 2016).
134	54	9 we compare the PG produced by the two models throughout 5K training steps.
