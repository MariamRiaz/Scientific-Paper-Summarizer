{"sections": [{"text": "\u221a \u03b8%\u0303/n) where \u03b8\ndenotes freedom degree of the network parameters and %\u0303 = O(log(\u220fli=1 bi(ki \u2212 si + 1)/p) + log(bl+1)) encapsulates architecture parameters including the kernel size ki, stride si, pooling size p and parameter magnitude bi. To our best knowledge, this is the first generalization bound that only depends on O(log(\u220fl+1i=1 bi)), tighter than existing ones that all involve an exponential term likeO(\u220fl+1i=1 bi). Besides, we prove that for an arbitrary gradient descent algorithm, the computed approximate stationary point by minimizing empirical risk is also an approximate stationary point to the population risk. This well explains why gradient descent training algorithms usually perform sufficiently well in practice. Furthermore, we prove the one-to-one correspondence and convergence guarantees for the non-degenerate stationary points between the empirical and population risks. It implies that the computed local minimum for the empirical risk is also close to a local minimum for the population risk, thus ensuring the good generalization performance of CNNs."}, {"heading": "1. Introduction", "text": "Deep convolutional neural networks (CNNs) have been successfully applied to various fields, such as image classifica-\n1Department of Electrical & Computer Engineering (ECE), National University of Singapore, Singapore. Correspondence to: Pan Zhou <pzhou@u.nus.edu>, Jiashi Feng <elefjia@nus.edu.sg>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\ntion (Szegedy et al., 2015; He et al., 2016; Wang et al., 2017), speech recognition (Sainath et al., 2013; Abdel-Hamid et al., 2014), and classic games (Silver et al., 2016; Brown & Sandholm, 2017). However, theoretical analyses and understandings on deep CNNs still largely lag their practical applications. Recently, although many works establish theoretical understandings on deep feedforward neural networks (DNNs) from various aspects, e.g. (Neyshabur et al., 2015; Kawaguchi, 2016; Zhou & Feng, 2018; Tian, 2017; Lee et al., 2017), only a few (Sun et al., 2016; Kawaguchi et al., 2017; Du et al., 2017a;b) provide explanations on deep CNNs due to their more complex architectures and operations. Besides, these existing works all suffer certain discrepancy between their theories and practice. For example, the developed generalization error bound either exponentially grows along with the depth of a CNN model (Sun et al., 2016) or is data-dependent (Kawaguchi et al., 2017), and the convergence guarantees for optimization algorithms over CNNs are achieved by assuming an over-simplified CNN model consisting of only one non-overlapping convolutional layer (Du et al., 2017a;b).\nAs an attempt to explain the practical success of deep CNNs and mitigate the gap between theory and practice, this work aims to provide tighter data-independent generalization error bound and algorithmic optimization guarantees for the commonly used deep CNN models in practice. Specifically, we theoretically analyze the deep CNNs from following two aspects: (1) how their generalization performance varies with different network architecture choices and (2) why gradient descent based algorithms such as stochastic gradient descend (SGD) (Robbins & Monro, 1951), adam (Kingma & Ba, 2015) and RMSProp (Tieleman & Hinton, 2012), on minimizing empirical risk usually offer models with satisfactory performance. Moreover, we theoretically demonstrate the benefits of (stride) convolution and pooling operations, which are unique for CNNs, to the generalization performance, compared with feedforward networks.\nFormally, we consider a CNN model g(w;D) parameterized by w \u2208 Rd, consisting of l convolutional layers and one subsequent fully connected layer. It maps the input D \u2208 Rr0\u00d7c0 to an output vector v \u2208 Rdl+1 . Its i-th convolutional layer takes Z(i\u22121) \u2208 Rr\u0303i\u22121\u00d7c\u0303i\u22121\u00d7di\u22121 as input and outputs Z(i) \u2208 Rr\u0303i\u00d7c\u0303i\u00d7di through spatial convolution, non-linear activation and pooling operations sequentially.\nar X\niv :1\n80 5.\n10 76\n7v 1\n[ cs\n.L G\n] 2\n8 M\nay 2\n01 8\nHere r\u0303i \u00d7 c\u0303i and di respectively denote resolution and the number of feature maps. Specifically, the computation with the i-th convolutional layer is described as\nX(i)(:, :, j) = Z(i\u22121) ~W j(i) \u2208 Rri\u00d7ci ,\u2200j = 1, \u00b7 \u00b7 \u00b7 , di, Y(i) = \u03c31(X(i)) \u2208 Rri\u00d7ci\u00d7di , Z(i) = pool ( Y(i) ) \u2208 Rr\u0303i\u00d7c\u0303i\u00d7di ,\nwhere X(i)(:, :, j) denotes the j-th feature map output by the i-th layer; W j(i) \u2208 Rki\u00d7ki\u00d7di\u22121 denotes the j-th convolutional kernel of size ki\u00d7ki and there are in total di kernels in the i-th layer; ~, pool (\u00b7) and \u03c31(\u00b7) respectively denote the convolutional operation with stride si, pooling operation with window size p \u00d7 p without overlap and the sigmoid function. In particular, Z(0) = D is the input sample. The last layer is a fully connected one and formulated as\nu = W(l+1)z(l) \u2208 Rdl+1 and v = \u03c32(u) \u2208 Rdl+1 , where z(l) \u2208 Rr\u0303lc\u0303ldl is vectorization of the output Z(l) of the last convolutional layer; W(l+1) \u2208 Rdl+1\u00d7r\u0303lc\u0303ldl denotes the connection weight matrix; \u03c32(\u00b7) is a softmax activation function (for classification) and dl+1 is the class number.\nIn practice, a deep CNN model is trained by minimizing the following empirical risk in terms of squared loss on the training data pairs (D(i),y(i)) drawn from an unknown distribution D,\nQ\u0303n(w) , 1\nn\nn\u2211\ni=1\nf(g(w;D(i)),y(i)), (1)\nwhere f(g(w;D),y) = 12\u2016v \u2212 y\u201622 is the squared loss function. One can obtain the model parameter w\u0303 via SGD or its variants like adam and RMSProp. However, this empirical solution is different from the desired optimum w\u2217 that minimizes the population risk:\nQ(w) , E(D,y)\u223cD f(g(w;D),y). This raises an important question: why CNNs trained by minimizing the empirical risk usually perform well in practice, considering the high model complexity and nonconvexity? This work answers this question by (1) establishing the generalization performance guarantee for CNNs and (2) expounding why the computed solution w\u0303 by gradient descent based algorithms for minimizing the empirical risk usually performs sufficiently well in practice.\nTo be specific, we present three new theoretical guarantees for CNNs. First, we prove that the generalization error of deep CNNs decreases at the rate of O( \u221a \u03b8%\u0303/(2n)) where \u03b8 denotes parameter freedom degree1, and %\u0303 depends on the\n1We use the terminology of \u201cparameter freedom degree\u201d here for characterizing redundancy of parameters. For example, for a rank-r matrix A \u2208 Rm1\u00d7m2 , the parameter freedom degree in this work is r(m1 +m2 + 1) instead of the commonly used one r(m1 +m2 \u2212 r).\nnetwork architecture parameters including the convolutional kernel size ki, stride si, pooling size p, channel number di and parameter magnitudes. It is worth mentioning that our generalization error bound is the first one that does not exponentially grow with depth.\nSecondly, we prove that for any gradient descent based optimization algorithm, e.g. SGD, RMSProp or adam, if its output w\u0303 is an approximate stationary point of the empirical risk Q\u0303n(w), w\u0303 is also an approximate stationary point of the population risk Q(w). This result is important as it explains why CNNs trained by minimizing the empirical risk have good generalization performance on test samples. We achieve such results by analyzing the convergence behavior of the empirical gradient to its population counterpart.\nFinally, we go further and quantitatively bound the distance between w\u0303 and w\u2217. We prove that when the samples are sufficient, a non-degenerate stationary point wn of Q\u0303n(w) uniquely corresponds to a non-degenerate stationary point w\u2217 of the population risk Q(w), with a distance shrinking at the rate of O((\u03b2/\u03b6) \u221a d%\u0303/n) where \u03b2 also depends on the CNN architecture parameters (see Thereom 2). Here \u03b6 accounts for the geometric topology of non-degenerate stationary points. Besides, the corresponding pair (wn,w\u2217) shares the same geometrical property\u2014if one in (wn,w\u2217) is a local minimum or saddle point, so is the other one. All these results guarantee that for an arbitrary algorithm provided with sufficient samples, if the computed w\u0303 is close to the stationary point wn, then w\u0303 is also close to the optimum w\u2217 and they share the same geometrical property.\nTo sum up, we make multiple contributions to understand deep CNNs theoretically. To our best knowledge, this work presents the first theoretical guarantees on both generalization error bound without exponential growth over network depth and optimization performance for deep CNNs. We substantially extend prior works on CNNs (Du et al., 2017a;b) from the over-simplified single-layer models to the multi-layer ones, which is of more practical significance. Our generalization error bound is much tighter than the one derived from Rademacher complexity (Sun et al., 2016) and is also independent of data and specific training procedure, which distinguishes it from (Kawaguchi et al., 2017)."}, {"heading": "2. Related Works", "text": "Recently, many works have been devoted to explaining the remarkable success of deep neural networks. However, most works only focus on analyzing fully feedforward networks from aspects like generalization performance (Bartlett & Maass, 2003; Neyshabur et al., 2015), loss surface (Saxe et al., 2014; Dauphin et al., 2014; Choromanska et al., 2015; Kawaguchi, 2016; Nguyen & Hein, 2017; Zhou & Feng, 2018), optimization algorithm convergence (Tian, 2017; Li\n& Yuan, 2017) and expression ability (Eldan & Shamir, 2016; Soudry & Hoffer, 2017; Lee et al., 2017).\nThe literature targeting at analyzing CNNs is very limited, mainly because CNNs have much more complex architectures and computation. Among the few existing works, Du et al. (2017b) presented results for a simple and shallow CNN consisting of only one non-overlapping convolutional layer and ReLU activations, showing that gradient descent (GD) algorithms with weight normalization can converge to the global minimum. Similarly, Du et al. (2017a) also analyzed optimization performance of GD and SGD with nonGaussian inputs for CNNs with only one non-overlapping convolutional layer. By utilizing the kernel method, Zhang et al. (2017) transformed a CNN model into a single-layer convex model which has almost the same loss as the original CNN with high probability and proved that the transformed model has higher learning efficiency.\nRegarding generalization performance of CNNs, Sun et al. (2016) provided the Rademacher complexity of a deep CNN model which is then used to establish the generalization error bound. But the Rademacher complexity exponentially depends on the magnitude of total parameters per layer, leading to loose results. In contrast, the generalization error bound established in this work is much tighter, as discussed in details in Sec. 3. Kawaguchi et al. (2017) introduced two generalization error bounds of CNN, but both depend on a specific dataset as they involve the validation error or the intermediate outputs for the network model on a provided dataset. They also presented dataset-independent generalization error bound, but with a specific two-phase training procedure required, where the second phase need fix the states of ReLU activation functions. However, such two-phase training procedure is not used in practice.\nThere are also some works focusing on convergence behavior of nonconvex empirical risk of a single-layer model to the population risk. Our proof techniques essentially differ from theirs. For example, (Gonen & Shalev-Shwartz, 2017) proved that the empirical risk converges to the population risk for those nonconvex problems with no degenerated saddle points. Unfortunately, due to existence of degenerated saddle points in deep networks (Dauphin et al., 2014; Kawaguchi, 2016), their results are not applicable here. A very recent work (Mei et al., 2017) focuses on single-layer nonconvex problems but requires the gradient and Hessian of the empirical risk to be strong sub-Gaussian and subexponential respectively. Besides, it assumes a linearity property for the gradient which hardly holds in practice. Comparatively, our assumptions are much milder. We only assume magnitude of the parameters to be bounded. Furthermore, we also explore the parameter structures of optimized CNNs, i.e. the low-rankness property, and derive bounds matching empirical observations better. Finally, we analyze\nthe convergence rate of the empirical risk and generalization error of CNN which is absent in (Mei et al., 2017).\nOur work is also critically different from the recent work (Zhou & Feng, 2018) although we adopt a similar analysis road map with it. Zhou & Feng (2018) analyzed DNNs while this work focuses on CNNs with more complex architectures and operations which are more challenging and requires different analysis techniques. Besides, this work provides stronger results in the sense of several tighter bounds with much milder assumptions. (1) For nonlinear DNNs, Zhou & Feng (2018) assumed the input data to be Gaussian, while this work gets rid of such a restrictive assumption. (2) The generalization error bound O(r\u0302l+1 \u221a d/n) in (Zhou & Feng, 2018) exponentially depends on the upper magnitude bound r\u0302 of the weight matrix per layer and linearly depends on the total parameter number d, while ours is O( \u221a \u03b8%\u0303/n) which only depends on the logarithm term %\u0303 = log( \u220fl+1 i=1 bi) and the freedom degree \u03b8 of the network parameters, where bi and bl+1 respectively denote the upper magnitude bounds of each kernel per layer and the weight matrix of the fully connected layer. Note, the exponential term O(r\u0302l+1) in (Zhou & Feng, 2018) cannot be further improved due to their proof techniques. The results on empirical gradient and stationary point pairs in (Zhou & Feng, 2018) rely on O(r\u03022(l+1)), while ours is O(\u220fl+1i=1 bi) which only depends on bi instead of bi2. (3) This work explores the parameter structures, i.e. the low-rankness property, and derives tighter bounds as the parameter freedom degree \u03b8 is usually smaller than the total parameter number d."}, {"heading": "3. Generalization Performance of Deep CNNs", "text": "In this section, we present the generalization error bound for deep CNNs and reveal effects of different architecture parameters on their generalization performance, providing some principles on model architecture design. We derive these results by establishing uniform convergence of the empirical risk Q\u0303n(w) to its population one Q(w).\nWe start with explaining our assumptions. Similar to (Xu & Mannor, 2012; Tian, 2017; Zhou & Feng, 2018), we assume that the parameters of the CNN have bounded magnitude. But we get rid of the Gaussian assumptions on the input data, meaning our assumption is milder than the ones in (Tian, 2017; Soudry & Hoffer, 2017; Zhou & Feng, 2018). Assumption 1. The magnitudes of the j-th kernel W j(i) \u2208 Rki\u00d7ki\u00d7di\u22121 in the i-th convolutional layer and the weight matrix W(l+1) \u2208 Rdl+1\u00d7r\u0303lc\u0303ldl in the the fully connected layer are respectively bounded as follows\n\u2016W j(i)\u2016F \u2264bi (1\u2264j\u2264di; 1\u2264 i\u2264 l), \u2016W(l+1)\u2016F \u2264bl+1, where bi (1 \u2264 i \u2264 l) and bl+1 are positive constants.\nWe also assume that the entry value of the target output y\nalways falls in [0, 1], which can be achieved by scaling the entry value in y conveniently.\nIn this work, we also consider possible emerging structure of the learned parameters after training\u2014the parameters usually present redundancy and low-rank structures (Lebedev et al., 2014; Jaderberg et al., 2014) due to high model complexity. So we incorporate low-rankness of the parameters or more concretely the parameter matrix consisting of kernels per layer, into our analysis. Denoting by vec(A) the vectorization of a matrix A, we have Assumption 2.\nAssumption 2. Assume the matrices W\u0303(i) andW(l+1) obey\nrank(W\u0303(i)) \u2264 ai (1 \u2264 i \u2264 l) and rank(W(l+1)) \u2264 al+1,\nwhere W\u0303(i) = [vec(W 1(i)), vec(W 2 (i)), \u00b7 \u00b7 \u00b7 , vec(W di(i))] \u2208 Rk2i di\u22121\u00d7di denotes the matrix consisting of all kernels in the i-th layer (1 \u2264 i \u2264 l).\nThe parameter low-rankness can also be defined on kernels individually by using the tensor rank (Tucker, 1966; Zhou et al., 2017; Zhou & Feng, 2017). Our proof techniques are extensible to this case and similar results can be expected."}, {"heading": "3.1. Generalization Error Bound for Deep CNNs", "text": "We now proceed to establish generalization error bound for deep CNNs. Let S = {(D(1),y(1)), \u00b7 \u00b7 \u00b7 , (D(n),y(n))} denote the set of training samples i.i.d. drawn from D. When the optimal solution w\u0303 to problem (1) is computed by a deterministic algorithm, the generalization error is defined as g = \u2223\u2223Q\u0303n(w\u0303)\u2212Q(w\u0303) \u2223\u2223. But in practice, a CNN model is usually optimized by randomized algorithms, e.g. SGD. So we adopt the following generalization error in expectation. Definition 1. (Generalization error) (Shalev-Shwartz et al., 2010) Assume a randomized algorithm A is employed for optimization over training samples S = {(D(1),y(1)), \u00b7 \u00b7 \u00b7, (D(n),y(n))} \u223c D and w\u0303 = argminwQ\u0303n(w) is the empirical risk minimizer (ERM). Then if we have ES\u223cD \u2223\u2223EA(Q(w\u0303) \u2212 Q\u0303n(w\u0303)) \u2223\u2223 \u2264 k, the ERM is said to have generalization error with rate k under distribution D. We bound the generalization error in expectation for deep CNNs by first establishing uniform convergence of the empirical risk to its corresponding population risk, as stated in Lemma 1 with proof in Sec. D.1 in supplement. Lemma 1. Assume that in CNNs, \u03c31 and \u03c32 are respectively the sigmoid and softmax activation functions and the loss function f(g(w;D),y) is squared loss. Under Assumptions 1 and 2, if n \u2265 cf \u2032 l2(bl+1 +\u2211l\ni=1 dibi) 2 maxi \u221a rici/(\u03b8%\u03b5\n2) where cf \u2032 is a universal constant, then with probability at least 1\u2212 \u03b5, we have\nsup w\u2208\u2126\n\u2223\u2223\u2223Q\u0303n(w)\u2212Q(w) \u2223\u2223\u2223 \u2264\n\u221a \u03b8%+ log ( 4 \u03b5 )\n2n , (2)\nwhere the total freedom degree \u03b8 of the network is \u03b8 = al+1(dl+1 + r\u0303lc\u0303ldl + 1) + \u2211l i=1 ai(ki 2di\u22121 + di + 1) and % = \u2211l i=1log (\u221adibi(ki\u2212si+1) 4p ) + log(bl+1) + log ( n 128p2 ) .\nTo our best knowledge, this generalization error rate is the first one that grows linearly (in contrast to exponentially) with depth l without needing any special training procedure. This can be observed from the fact that our result only depends on O(\u2211li=1 log(bi)), rather than an exponential factor O(\u220fl+1i=1 bi) which appears in some existing works, e.g. the uniform convergence of the empirical risk in deep CNNs (Sun et al., 2016) and fully feedforward networks (Bartlett & Maass, 2003; Neyshabur et al., 2015; Zhou & Feng, 2018). This faster convergence rate is achieved by adopting similar analysis technique in (Mei et al., 2017; Zhou & Feng, 2018) but we derive tighter bounds on the related parameters featuring distributions of the empirical risk and its gradient, with milder assumptions. For instance, both (Zhou & Feng, 2018) and this work show that the empirical risk follows a sub-Gaussian distribution. But Zhou & Feng (2018) used Gaussian concentration inequality and thus need Lipschitz constant of loss which exponentially depends on the depth. In contrast, we use -net to decouple the dependence between input D and parameter w and then adopt Hoeffding\u2019s inequality, only requiring the constant magnitude bound of loss and geting rid of exponential term.\nBased on Lemma 1, we derive generalization error of deep CNNs in Theorem 1 with proof in Sec. D.2 in supplement.\nTheorem 1. Assume that in CNNs, \u03c31 and \u03c32 are respectively the sigmoid and softmax functions and the loss function f(g(w;D),y) is squared loss. Suppose Assumptions 1 and 2 hold. Then with probability at least 1\u2212 \u03b5, the generalization error of a deep CNN model is bounded as\nES\u223cD \u2223\u2223\u2223EA ( Q(w\u0303)\u2212 Q\u0303n(w\u0303) ) \u2223\u2223\u2223 \u2264\n\u221a \u03b8%+ log ( 4 \u03b5 )\n2n ,\nwhere \u03b8 and % are given in Lemma 1.\nBy inspecting Theorem 1, one can find that the generalization error diminishes at the rate of O (1/\u221an) (up to a log factor). Besides, Theorem 1 explicitly reveals the roles of network parameters in determining model generalization performance. Such transparent results form stark contrast to the works (Sun et al., 2016) and (Kawaguchi et al., 2017) (see more comparison in Sec. 3.2). Notice, our technique also applies to other third-order differentiable activation functions, e.g. tanh, and other losses, e.g. cross entropy, with only slight difference in the results.\nFirst, the freedom degree \u03b8 of network parameters, which depends on the network size and the redundancy in parameters, plays an important role in the generalization error bound. More specifically, to obtain smaller generalization\nerror, more samples are needed to train a deep CNN model having larger freedom degree \u03b8. As aforementioned, although the results in Theorem 1 are obtained under the low-rankness condition defined on the parameter matrix consisting of kernels per layer, they are easily extended to the (tensor) low-rankness defined on each kernel individually. The low-rankness captures common parameter redundancy in practice. For instance, (Lebedev et al., 2014; Jaderberg et al., 2014) showed that parameter redundancy exists in a trained network model and can be squeezed by low-rank tensor decomposition. The classic residual function (He et al., 2016; Zagoruyko & Komodakis, 2016) with three-layer bottleneck architecture (1\u00d7 1, 3\u00d7 3 and 1\u00d7 1 convs) has rank 1 in generalized block term decomposition (Chen et al., 2017; Cohen & Shashua, 2016). Similarly, inception networks (Szegedy et al., 2017) explicitly decomposes a convolutional kernel of large size into two separate convolutional kernels of smaller size (e.g. a 7\u00d7 7 kernel is replaced by two multiplying kernels of size 7\u00d71 and 1\u00d77). Employing these low-rank approximation techniques helps reduce the freedom degree and provides smaller generalization error. Notice, the low-rankness assumption only affects the freedom degree \u03b8. Without this assumption, \u03b8 will be replaced by the total parameter number of the network.\nFrom the factor %, one can observe that the kernel size ki and its stride si determine the generalization error but in opposite ways. Larger kernel size ki leads to larger generalization error, while larger stride si provides smaller one. This is because both larger kernel and smaller stride increase the model complexity, since larger kernel means more trainable parameters and smaller stride implies larger size of feature maps in the subsequent layer. Also, the pooling operation in the first l convolutional layers helps reduce the generalization error, as reflected by the factor 1/p in %.\nFurthermore, the number of feature maps (i.e. channels) di appearing in the \u03b8 and % also affects the generalization error. A wider network with larger di requires more samples for training such that it can generalize well. This is because (1) a larger di indicates more trainable parameters, which usually increases the freedom degree \u03b8, and (2) a larger di also requires larger kernels W j(i) with more channel-wise dimensions since there are more channels to convolve, leading to a larger magnitude bound bi for the kernel W j (i). Therefore, as suggested by Theorem 1, a thin network is more preferable than a fat network. Such an observation is consistent with other analysis works on the network expression ability (Eldan & Shamir, 2016; Lu et al., 2017) and the architecture-engineering practice, such as (He et al., 2016; Szegedy et al., 2015). By comparing contributions of the architecture and parameter magnitude to the generalization performance, we find that the generalization error usually depends on the network architecture parameters linearly or more heavily, and also on parameter magnitudes but\nwith a logarithm term log bi. This implies the architecture plays a more important role than the parameter magnitudes. Therefore, for achieving better generalization performance in practice, architecture engineering is indeed essential.\nFinally, by observing the factor %, we find that imposing certain regularization, such as \u2016w\u201622, on the trainable parameters is useful. The effectiveness of such a regularization will be more significant when imposing on the weight matrix of the fully connected layer due to its large size. Such a regularization technique, in deep learning literature, is well known as \u201cweight decay\u201d. This conclusion is consistent with other analysis works on the deep forward networks, such as (Bartlett & Maass, 2003; Neyshabur et al., 2015; Zhou & Feng, 2018)."}, {"heading": "3.2. Discussions", "text": "Sun et al. (2016) also analyzed generalization error bound in deep CNNs but employing different techniques. They proved that the Rademacher complexity Rm(F) of a deep CNN model with sigmoid activation functions is O(\u0303bx(2pb\u0303)l+1 \u221a log(r0c0)/ \u221a n) where F denotes the function hypothesis that maps the input data D to v \u2208 Rdl+1 by the analyzed CNN model. Here b\u0303x denotes the upper bound of the absolute entry values in the input datum D, i.e. b\u0303x \u2265 |Di,j | (\u2200i, j), and b\u0303 obeys b\u0303 \u2265 max{maxi \u2211di j=1 \u2016W j (i)\u20161, \u2016W(l+1)\u20161}. Sun et al. (2016) showed that with probability at least 1 \u2212 \u03b5, the difference between the empirical margin error err\u03b3e (g) (g \u2208 F) and the population margin error err\u03b3p(g) can be bounded as\nerr\u03b3p(g) \u2264 inf \u03b3>0\n[ err\u03b3e (g) +\n8dl+1(2dl+1 \u2212 1) \u03b3 Rm(F)\n+\n\u221a log log2(2/\u03b3)\nn +\n\u221a log(2/\u03b5)\nn\n] , (3)\nwhere \u03b3 controls the error margin since it obeys \u03b3 \u2265 vy \u2212 maxk 6=y vk and y denotes the label of v. However, the bound in Eqn. (3) is practically loose, sinceRm(F) involves the exponential factor (2b\u0303)l+1 which is usually very large. In this case,Rm(F) is extremely large. By comparison, the bound provided in our Theorem 1 only depends on\u2211l+1 i=1 log(bi) which avoids the exponential growth along with the depth l, giving a much tighter and more practically meaningful bound. The generalization error bounds in (Kawaguchi et al., 2017) either depend on a specific dataset or rely on restrictive and rarely used training procedure, while our Theorem 1 is independent of any specific dataset or training procedure, rendering itself more general. More importantly, the results in Theorem 1 make the roles of network parameters transparent, which could benefit understanding and architecture design of CNNs."}, {"heading": "4. Optimization Guarantees for Deep CNNs", "text": "Although deep CNNs are highly non-convex, gradient descent based algorithms usually perform quite well on optimizing the models in practice. After characterizing the roles of different network parameters for the generalization performance, here we present optimization guarantees for gradient descent based algorithms in training CNNs.\nSpecifically, in practice one usually adopts SGD or its variants, such as adam and RMSProp, to optimize the CNN models. Such algorithms usually terminate when the gradient magnitude decreases to a low level and the training hardly proceeds. This implies that the algorithms in fact compute an -approximate stationary point w\u0303 for the loss function Q\u0303n(w), i.e. \u2016\u2207wQ\u0303n(w\u0303)\u201622 \u2264 . Here we explore such a problem: by computing an -stationary point w\u0303 of the empirical risk Q\u0303n(w), can we also expect w\u0303 to be sufficiently good for generalization, or in other words expect that it is also an approximate stationary point for the population risk Q(w)? To answer this question, first we analyze the relationship between the empirical gradient\u2207wQ\u0303n(w) and its population counterpart\u2207wQ(w). Founded on this, we further establish convergence of the empirical gradient of the computed solution to its corresponding population gradient. Finally, we present the bounded distance between the computed solution w\u0303 and the optimum w\u2217.\nTo our best knowledge, this work is the first one that analyzes the optimization behavior of gradient descent based algorithms for training multi-layer CNN models with the commonly used convolutional and pooling operations."}, {"heading": "4.1. Convergence Guarantees on Gradients", "text": "Here we present guarantees on convergence of the empirical gradient to the population one in Theorem 2. As aforementioned, such results imply good generalization performance of the computed solution w\u0303 to the empirical risk Q\u0303n(w).\nTheorem 2. Assume that in CNNs, \u03c31 and \u03c32 respectively are the sigmoid and softmax functions and the loss function f(g(w;D),y) is squared loss. Suppose Assumptions 1 and 2 hold. Then the empirical gradient uniformly converges to the population gradient in Euclidean norm. More specifically, there exist universal constants cg\u2032 and cg such that if n \u2265 cg\u2032 l 2bl+1 2(bl+1+ \u2211l i=1 dibi) 2(r0c0d0) 4\nd40b1 8(d log(6)+\u03b8%)\u03b52 maxi(rici)\n, then\nsup w\u2208\u2126\n\u2225\u2225\u2225\u2207wQ\u0303n(w)\u2212\u2207wQ(w) \u2225\u2225\u2225\n2 \u2264cg\u03b2\n\u221a 2d+\u03b8%+log ( 4 \u03b5 )\n2n\nholds with probability at least 1 \u2212 \u03b5, where % is provided in Lemma 1. Here \u03b2 and d are defined as \u03b2 =[ rlcldl 8p2 + \u2211l i=1 bl+1 2di\u22121 8p2bi2di ri\u22121ci\u22121 \u220fl j=i djbj 2(kj\u2212sj+1)2 16p2 ]1/2 and d = r\u0303lc\u0303ldldl+1 + \u2211l i=1 ki 2di\u22121di, respectively.\nIts proof is given in Sec. D.3 in supplement. From Theorem 2, the empirical gradient converges to the population one at the rate of O(1/\u221an) (up to a log factor). In Sec. 3.1, we have discussed the roles of the network architecture parameters in %. Here we further analyze the effects of the network parameters on the optimization behavior through the factor \u03b2. The roles of the kernel size ki, the stride si, the pooling size p and the channel number di in \u03b2 are consistent with those in Theorem 1. The extra factor rici advocates not building such CNN networks with extremely large feature map sizes. The total number of parameters d is involved here instead of the degree of freedom because the gradient \u2207wQ\u0303n(w) may not have low-rank structures. Based on Theorem 2, we can further conclude that if the computed solution w\u0303 is an -approximate stationary point of the empirical risk, then it is also a 4 -approximate stationary point of the population risk. We state this result in Corollary 1 with proof in Sec. D.4 in supplement. Corollary 1. Suppose assumptions in Theorem 2 hold and we have n \u2265 (d% + log(4/\u03b5))\u03b22/ . Then if the solution w\u0303 computed by minimizing the empirical risk obeys \u2016\u2207Q\u0303n(w\u0303)\u201622 \u2264 , we have \u2016\u2207Q(w\u0303)\u201622 \u2264 4 with probability at least 1\u2212 \u03b5.\nCorollary 1 shows that by using full gradient descent algorithms to minimize the empirical risk, the computed approximate stationary point w\u0303 is also close to the desired stationary point w\u2217 of the population risk. This guarantee is also applicable to other stochastic gradient descent based algorithms, like SGD, adam and RMSProp, by applying recent results on obtaining -approximate stationary point for nonconvex problems (Ghadimi & Lan, 2013; Tieleman & Hinton, 2012; Kingma & Ba, 2015). Accordingly, the computed solution w\u0303 has guaranteed generalization performance on new data. It partially explains the success of gradient descent based optimization algorithms for CNNs."}, {"heading": "4.2. Convergence of Stationary Points", "text": "Here we go further and directly characterize the distance between stationary points in the empirical risk Q\u0303n(w) and its population counterpart Q(w). Compared with the results for the risk and gradient, the results on stationary points give more direct performance guarantees for CNNs. Here we only analyze the non-degenerate stationary points including local minimum/maximum and non-degenerate saddle points, as they are geometrically isolated and thus are unique in local regions. We first introduce some necessary definitions. Definition 2. (Non-degenerate stationary points and saddle points) (Gromoll & Meyer, 1969) A stationary point w is said to be a non-degenerate stationary point of Q(w) if\ninf i\n\u2223\u2223\u03bbi ( \u22072Q(w) )\u2223\u2223 \u2265 \u03b6,\nwhere \u03bbi ( \u22072Q(w) ) is the i-th eigenvalue of the Hessian\n\u22072Q(w) and \u03b6 is a positive constant. A stationary point is said to be a saddle point if the smallest eigenvalue of its Hessian\u22072Q(w) has a negative value.\nSuppose Q(w) has m non-degenerate stationary points which are denoted as {w(1), w(2), \u00b7 \u00b7 \u00b7 ,w(m)}. We have following results on the geometry of these stationary points in Theorem 3. The proof is given in Sec. D.5 in supplement.\nTheorem 3. Assume in CNNs, \u03c31 and \u03c32 are respectively the sigmoid and softmax activation functions and the loss f(g(w;D),y) is squared loss. Suppose Assumptions 1 and 2 hold. Then if n \u2265 ch max ( d+\u03b8% \u03b62 , l2bl+1 2(bl+1+ \u2211l i=1 dibi) 2(r0c0d0) 4\nd40b1 8d%\u03b52 maxi(rici)\n) where ch is a constant,\nfor k \u2208 {1, \u00b7 \u00b7 \u00b7 ,m}, there exists a non-degenerate stationary point w(k)n of Q\u0303n(w) which uniquely corresponds to the non-degenerate stationary point w(k) of Q(w) with probability at least 1\u2212 \u03b5. Moreover, with same probability the distance between w(k)n and w(k) is bounded as\n\u2016w(k)n \u2212w(k)\u20162\u2264 2cg\u03b2\n\u03b6\n\u221a 2d+\u03b8%+log ( 4 \u03b5 )\n2n , (1\u2264k\u2264m),\nwhere % and \u03b2 are given in Lemma 1 and Theorem 2, respectively.\nTheorem 3 shows that there exists exact one-to-one correspondence between the non-degenerate stationary points of the empirical risk Q\u0303n(w) and the popular risk Q(w) for CNNs, if the sample size n is sufficiently large. Moreover, the non-degenerate stationary point w(k)n of Q\u0303n(w) is very close to its corresponding non-degenerate stationary point w(k) of Q(w). More importantly, their distance shrinks at the rate of O (1/\u221an) (up to a log factor). The network parameters have similar influence on the distance bounds as explained in the above subsection. Compared with gradient convergence rate in Theorem 2, the convergence rate of corresponding stationary point pairs in Theorem 3 has an extra factor 1/\u03b6 that accounts for the geometric topology of non-degenerate stationary points, similar to other works like stochastic optimization analysis (Duchi & Ruan, 2016).\nFor degenerate stationary points to which the corresponding Hessian matrix has zero eigenvalues, one cannot expect to establish unique correspondence for stationary points in empirical and population risks, since they are not isolated anymore and may reside in flat regions. But Theorem 2 guarantees that the gradients of Q\u0303n(w) and Q(w) at these points are close. This implies a degenerate stationary point of Q(w) will also give a near-zero gradient for Q\u0303n(w), indicating it is also a stationary point for Q\u0303n(w).\nDu et al. (2017a;b) showed that for a simple and shallow CNN consisting of only one non-overlapping convolutional layer, (stochastic) gradient descent algorithms with weight\nnormalization can converge to the global minimum. In contrast to their simplified models, we analyze complex multi-layer CNNs with the commonly used convolutional and pooling operations. Besides, we provide results on both gradient and the distance between the computed solution and desired stationary points, which are applicable to arbitrary gradient descent based algorithms.\nNext, based on Theorem 3, we derive that the corresponding pair (w(k)n ,w(k)) in the empirical and population risks shares the same geometrical property stated in Corollary 2. Corollary 2. Suppose the assumptions in Theorem 3 hold. If any one in the pair (w(k)n ,w(k)) in Theorem 3 is a local minimum or saddle point, so is the other one.\nSee the proof of Corollary 2 in Sec. D.6 in supplement. Corollary 2 tells us that the corresponding pair, w(k)n and w(k), has the same geometric property. Namely, if either one in the pair is a local minimum or saddle point, so is the other one. This result is important for optimization. If the computed solution w\u0303 by minimizing the empirical risk Q\u0303n(w) is a local minimum, then it is also a local minimum of the population risk Q(w). Thus it partially explains why the computed solution w\u0303 can generalize well on new data. This property also benefits designing new optimization algorithms. For example, Saxe et al. (2014) and Kawaguchi (2016) pointed out that degenerate stationary points indeed exist for deep linear neural networks and Dauphin et al. (2014) empirically validated that saddle points are usually surrounded by high error plateaus in deep forward neural networks. So it is necessary to avoid the saddle points and find the local minimum of population risk. From Theorem 3, it is clear that one only needs to find the local minimum of empirical risk by using escaping saddle points algorithms, e.g. (Ge et al., 2015; Jin et al., 2017; Agarwal et al., 2017)."}, {"heading": "5. Comparison on DNNs And CNNs", "text": "Here we compare deep feedforward neural networks (DNNs) with deep CNNs from their generalization error and optimization guarantees to theoretically explain why CNNs are more preferable than DNNs, to some extent.\nBy assuming the input to be standard Gaussian N (0, \u03c42), Zhou & Feng (2018) proved that if n \u2265 18r2/(d\u03c42\u03b52 log(l+ 1)), with probability 1 \u2212 \u03b5, the generalization error of an (l + 1)-layer DNN model with sigmoid activation functions is bounded by n:\nn , cn\u03c4 \u221a\n(1 + crl) max i di\n\u221a d log(n(l+1)) + log(4/\u03b5)\nn ,\nwhere cn is a universal constant; di denotes the width of the i-th layer; d is the total parameter number of the network; cr = max(r\u03022/16, ( r\u03022/16 )l ) where r\u0302 upper bounds Frobenius norm of the weight matrix in each\nlayer. Recall that the generalization bound of CNN provided in this work is O( \u221a \u03b8%\u0303/(2n)), where %\u0303 =\u2211l\ni=1 log (\u221a dibi(ki \u2212 si + 1)/(4p) ) + log(bl+1).\nBy observing the above two generalization bounds, one can see when the layer number is fixed, CNN usually has smaller generalization error than DNN because: (1) CNN usually has much fewer parameters, i.e. smaller d, than DNN due to parameter sharing mechanism of convolutions. (2) The generalization error of CNN has a smaller factor than DNN in the network parameter magnitudes. Generalization error bound of CNN depends on a logarithm termO(log\u220fl+1i=1 bi) of the magnitude bi of each kernel/weight matrix, while the bound for DNN depends on O(r\u0302l+1). Since the kernel size is much smaller than that of the weight matrix in the fully connected layer by unfolding the convolutional layer, the upper magnitude bound bi (i = 1, \u00b7 \u00b7 \u00b7 , l) of each kernel is usually much smaller than r\u0302. (3) The pooling operation and the stride in convolutional operation in CNN also benefit its generalization performance. This is because the factor %\u0303 involves O(2(l + 1) log(1/p)) and (ki \u2212 si + 1) which also reduce the generalization error. Notice, by applying our analysis technique, it might be possible to remove the exponential term in DNN. But as mentioned above, the unique operations, e.g. convolution, pooling and striding, still benefit CNN, making it generalize better than DNN.\nBecause of the above factors, the empirical gradient in CNN converges to its population counterpart faster, as well as the paired non-degenerate stationary points for empirical risk and population risk. All these results guarantee that for an arbitrary gradient descent based algorithm, it is faster to compute an approximate stationary point or a local minimum in population risk of CNN compared with DNN."}, {"heading": "6. Proof of Roadmap", "text": "Here we briefly introduce the proof roadmap. Due to space limitation, all the proofs of our theoretical results are deferred to the supplement. Firstly, our analysis relies on bounding the gradient magnitude and the spectral norm of Hessian of the loss f(g(w;D),y). By considering multilayer architecture of CNN, we establish recursive relation of their magnitudes in the k and k + 1 layers (Lemmas 9 \u223c 14 in supplement) and get their overall magnitude upper bound.\nFor the uniform convergence supw\u2208\u2126 |Q\u0303n(w) \u2212 Q(w)| in Lemma 1, we resort to bound three distances: A1 = supw\u2208\u2126 |Q\u0303n(w) \u2212 Q\u0303n(wkw)|, A2 = supwkw\u2208\u0398 |Q\u0303n(wkw)\u2212EQ\u0303n(wkw)| and A3 = supw\u2208\u2126 |EQ\u0303n(wkw) \u2212EQ(w)|, where wkw belongs to the -net \u0398 of parameter domain \u2126. Using Markov inequality and Lipschitz property of loss, we can bound A1 and A3. To bound A2, we prove the empirical risk is sub-Gaussian. Considering the element wkw in -net \u0398 is independent of input D, we use Hoeffd-\ning\u2019s inequality to prove empirical risk at point wkw to be sub-Gaussian for any wkw in \u0398. By this decoupling of -net, our bound on A2 depends on the constant magnitude of loss and gets rid of exponential term. Combining these bounds together, we obtain the uniform convergence of empirical risk and can derive the generalization bound.\nWe use a similar decomposition and decoupling strategy mentioned above to bound gradient uniform convergence supw\u2208\u2126\u2016\u2207wQ\u0303n(w)\u2212\u2207wQ(w)\u20162 in Theorem 2. But here we need to bound gradient and spectral norm of Hessian.\nTo prove correspondence and bounded distance of stationary points, we define a set G= {w\u2208\u2126 : ||\u2207Q\u0303n(w)||\u2264 and infi |\u03bbi(\u22072Q\u0303n(w))| \u2265 \u03b6} where \u03bbi is the i-th eigenvalue of\u22072Q\u0303n(w). Then G is decomposed into countable components each of which has one or zero non-degenerate stationary point. Next we prove the uniform convergence between empirical and population Hessian by using a similar strategy as above. Combining uniform convergence of gradient and Hessian and the results in differential topology (Lemmas 4 & 5 in supplement), we obtain that for each component of G, if there is a unique non-degenerate stationary point in Q(w), then Q\u0303n(w) also has a unique one, and vice versa. This gives the one-to-one correspondence relation. Finally, the uniform convergence of gradient and Hessian can bound the distance between the corresponding points."}, {"heading": "7. Conclusion", "text": "In this work, we theoretically analyzed why deep CNNs can achieve remarkable success, from its generalization performance and the optimization guarantees of (stochastic) gradient descent based algorithms. We proved that the generalization error of deep CNNs can be bounded by a factor which depends on the network parameters. Moreover, we analyzed the relationship between the computed solution by minimizing the empirical risk and the optimum solutions in population risk from their gradient and their Euclidean distance. All these results show that with sufficient training samples, the generalization performance of deep CNN models can be guaranteed. Besides, these results also reveal that the kernel size ki, the stride si, the pooling size p, the channel number di and the freedom degree \u03b8 of the network parameters are critical to the generalization performance of deep CNNs. We also showed that the weight parameter magnitude is also important. These suggestions on network designs accord with the widely used network architectures."}, {"heading": "Acknowledgements", "text": "Jiashi Feng was partially supported by NUS startup R-263000-C08-133, MOE Tier-I R-263-000-C21-112, NUS IDS R-263-000-C67-646, ECRA R-263-000-C87-133 and MOE Tier-II R-263-000-D17-112."}, {"heading": "A Structure of This Document", "text": "This document gives some other necessary notations and preliminaries for our analysis in Sec. B.1 and provides auxiliary lemmas in Sec. B.2. Then in Sec. C, we present the technical lemmas for proving our final results and their proofs. Next, in Sec. D, we utilize these technical lemmas to prove our desired results. Finally, we give the proofs of other auxiliary lemmas in Sec. E.\nAs for the results in manuscript, the proofs of Lemma 1 and Theorem 1 in Sec. 3.1 in the manuscript are respectively provided in Sec. D.1 and Sec. D.2. As for the results in Sec. 4 in the manuscript, Sec. D.3 and D.4 present the proofs of Theorem 2 and Corollary 1, respectively. Finally, we respectively introduce the proofs of Theorem 3 and Corollary 2 in Sec. D.5 and D.6."}, {"heading": "B Notations and Preliminary Tools", "text": "Beyond the notations introduced in the manuscript, we need some other notations used in this document. Then we introduce several lemmas that will be used later."}, {"heading": "B.1 Notations", "text": "Throughout this document, we use \u3008\u00b7, \u00b7\u3009 to denote the inner product and use ~\u0303 to denote the convolution operation with stride 1. A\u2297C denotes the Kronecker product betweenA andC. Note thatA andC inA\u2297C can be matrices or vectors. For a matrixA \u2208 Rn1\u00d7n2 , we use \u2016A\u2016F = \u221a\u2211 i,jA 2 ij to denote its Frobenius norm, whereAij is the (i, j)-th entry of A. We use \u2016A\u2016op = maxi |\u03bbi(A)| to denote the operation norm of a matrixA \u2208 Rn1\u00d7n1 , where \u03bbi(A) denotes the i-th eigenvalue of the matrixA. For a 3-way tensorA \u2208 Rs\u00d7t\u00d7q , its operation norm is computed as\n\u2016A\u2016op = sup \u2016\u03bb\u20162\u22641\n\u2329 \u03bb\u2297 3 ,A \u232a = \u2211\ni,j,k\nAijk\u03bbi\u03bbj\u03bbk,\nwhereAijk denotes the (i, j, k)-th entry ofA.\nFor brevity, in this document we use f(w,D) to denote f(g(w;D),y) in the manuscript. Let w(i) = (w1(i); \u00b7 \u00b7 \u00b7 ;wdi(i)) \u2208 Rki2di\u22121di (i = 1, \u00b7 \u00b7 \u00b7 , l) be the parameter of the i-th layer where wk(i) = vec ( W k(i) ) \u2208 Rki2di\u22121 is the vectorization of W k(i). Similarly, let w(l+1) = vec ( W(l+1) ) \u2208 Rrlcldldl+1 . Then, we further define w = (w(1), \u00b7 \u00b7 \u00b7 ,w(l),w(l+1)) \u2208 Rrlcldldl+1+ \u2211l i=1 ki\n2di\u22121di which contains all the parameter in the network. Here we useW k(i) to denote the k-th kernel in\nthe i-th convolutional layer. For brevity, letW k,j(i) denotes the j-th slice ofW k (i), i.e. W k,j (i) = W k (i)(:, :, j).\nFor a matrixM \u2208 Rs\u00d7t, M\u0302 denotes the matrix which is obtained by rotating the matrixM by 180 degrees. Then we use \u03b4i to denote the gradient of f(w,D) w.r.t. X(i):\n\u03b4i = \u2207X(i)f(w,D) \u2208 Rri\u00d7ci\u00d7di , (i = 1, \u00b7 \u00b7 \u00b7 , l),\nBased on \u03b4i, we further define \u03b4\u0303i \u2208 R(r\u0303i\u22121\u2212ki+1)\u00d7(c\u0303i\u22121\u2212ki+1)\u00d7di . Each slice \u03b4\u0303ki+1 can be computed as follows. Firstly, let \u03b4\u0303ki+1 = \u03b4 k i+1. Then, we pad zeros of si \u2212 1 rows between the neighboring rows in \u03b4\u0303ki+1 and similarly we pad zeros of si \u2212 1 columns between the neighboring columns in \u03b4\u0303ki+1. Accordingly, the size of \u03b4\u0303 k i+1 is (si(ri \u2212 1) + 1)\u00d7 (si(ci \u2212 1) + 1). Finally, we pad zeros of width ki \u2212 1 around \u03b4\u0303ki+1 to obtain new \u03b4\u0303ki+1 \u2208 R(si(ri\u22121)+2ki\u22121)\u00d7(si(ci\u22121)+2ki\u22121). Note that since ri+1 = (r\u0303i \u2212 ki+1)/si+1 + 1 and ri+1 = (r\u0303i \u2212 ki+1)/si+1 + 1, we have si(ri \u2212 1) + 2ki \u2212 1 = r\u0303i\u22121 \u2212 ki + 1 and si(ci \u2212 1) + 2ki \u2212 1 = c\u0303i\u22121 \u2212 ki + 1. Then we define four operators G (\u00b7), Q (\u00b7), up (\u00b7) and vec(\u00b7), which are useful for explaining the following analysis.\nOperation G (\u00b7): It maps an arbitrary vector z \u2208 Rd into a diagonal matrix G (z) \u2208 Rd\u00d7d with its i-th diagonal entry equal to \u03c31(zi)(1\u2212 \u03c31(zi)) in which zi denotes the i-th entry of z. Operation Q (\u00b7): it maps a vector z \u2208 Rd into a matrix of size d2 \u00d7 d whose ((i \u2212 1)d + i, i) (i = 1, \u00b7 \u00b7 \u00b7 , d) entry equal to \u03c31(zi)(1\u2212 \u03c31(zi))(1\u2212 2\u03c31(zi)) and rest entries are all 0. Operation up (\u00b7): up (M) represents conducting upsampling on M \u2208 Rs\u00d7t\u00d7q. Let N = up (M) \u2208 Rps\u00d7pt\u00d7q. Specifically, for each sliceN(:, :, i) (i = 1, \u00b7 \u00b7 \u00b7 , q), we haveN(:, :, i) = up (M(:, :, i)). It actually upsamples each entryM(g, h, i) into a matrix of p2 same entries 1p2M(g, h, i).\nOperation vec(\u00b7): For a matrix A \u2208 Rs\u00d7t, vec(A) is defined as vec(A) = (A(:, 1); \u00b7 \u00b7 \u00b7 ;A(:, t)) \u2208 Rst that vectorizes A \u2208 Rs\u00d7t along its columns. If A \u2208 Rt\u00d7s\u00d7q is a 3-way tensor, vec(A) = [vec(A(:, :, 1)); vec(A(:, : , 2)), \u00b7 \u00b7 \u00b7 , vec(A(:, :, q))] \u2208 Rstq ."}, {"heading": "B.2 Auxiliary Lemmas", "text": "We first introduce Lemmas 2 and 3 which are respectively used for bounding the `2-norm of a vector and the operation norm of a matrix. Then we introduce Lemmas 4 and 5 which discuss the topology of functions. In Lemma 6, we introduce the Hoeffding\u2019s inequality which provides an upper bound on the probability that the sum of independent random variables deviates from its expected value. In Lemma 7, we provide the covering number of an -net for a low-rank matrix. Finally, several commonly used inequalities are presented in Lemma 8 for our analysis. Lemma 2. (Vershynin, 2012) For any vector x \u2208 Rd, its `2-norm can be bounded as\n\u2016x\u20162 \u2264 1\n1\u2212 sup\u03bb\u2208\u03bb \u3008\u03bb,x\u3009 .\nwhere \u03bb = {\u03bb1, . . . ,\u03bbkw} be an -covering net of Bd(1). Lemma 3. (Vershynin, 2012) For any symmetric matrixX \u2208 Rd\u00d7d, its operator norm can be bounded as\n\u2016X\u2016op \u2264 1\n1\u2212 2 sup\u03bb\u2208\u03bb |\u3008\u03bb,X\u03bb\u3009| .\nwhere \u03bb = {\u03bb1, . . . ,\u03bbkw} be an -covering net of Bd(1). Lemma 4. (Mei et al., 2017) Let D \u2286 Rd be a compact set with a C2 boundary \u2202D, and f, g : A\u2192 R be C2 functions defined on an open set A, with D \u2286 A. Assume that for all w \u2208 \u2202D and all t \u2208 [0, 1], t\u2207f(w) + (1 \u2212 t)\u2207g(w) 6= 0. Finally, assume that the Hessian \u22072f(w) is non-degenerate and has index equal to r for all w \u2208 D. Then the following properties hold:\n(1) If g has no critical point in D, then f has no critical point in D.\n(2) If g has a unique critical point w in D that is non-degenerate with an index of r, then f also has a unique critical point w\u2032 in D with the index equal to r.\nLemma 5. (Mei et al., 2017) Suppose that F (w) : \u0398\u2192 R is a C2 function where w \u2208 \u0398. Assume that {w(1), . . . , w(m)} is its non-degenerate critical points and let D = {w \u2208 \u0398 : \u2016\u2207F (w)\u20162 < and infi \u2223\u2223\u03bbi ( \u22072F (w) )\u2223\u2223 \u2265 \u03b6}. Then D can be decomposed into (at most) countably components, with each component containing either exactly one critical point, or no critical point. Concretely, there exist disjoint open sets {Dk}k\u2208N, with Dk possibly empty for k \u2265 m+ 1, such that\nD = \u222a\u221ek=1Dk . Furthermore, w(k) \u2208 Dk for 1 \u2264 k \u2264 m and each Di, k \u2265 m+ 1 contains no stationary points.\nLemma 6. (Hoeffding, 1963) Let x1, \u00b7 \u00b7 \u00b7 , xn be independent random variables where xi is bounded by the interval [ai, bi]. Suppose sn = 1n \u2211n i=1 xi, then the following properties hold:\nP (sn \u2212 Esn \u2265 t) \u2264 exp ( \u2212 2n\n2t2\u2211n i=1(bi \u2212 ai)2\n) .\nwhere t is an arbitrary positive value.\nLemma 7. (Candes & Plan, 2009) Let Sr = {X \u2208 Rn1\u00d7n2 : rank (X) \u2264 r, \u2016X\u2016F = b}. Then there exists an -net S\u0303r for the Frobenius norm obeying\n|S\u0303r| \u2264 ( 9b )r(n1+n2+1) .\nThen we give a lemma to summarize the properties of G (\u00b7), Q (\u00b7) and up (\u00b7) defined in Section B.1, the convolutional operation ~ defined in manuscript. Lemma 8. For G (\u00b7), Q (\u00b7) and up (\u00b7) defined in Section B.1, the convolutional operation ~ defined in manuscript, we have the following properties:\n(1) For arbitrary vector z, and arbitrary matricesM andN of proper sizes, we have\n\u2016G (z)M\u20162F \u2264 1\n16 \u2016M\u20162F and \u2016NG (z)\u20162F \u2264\n1\n16 \u2016N\u20162F .\n(2) For arbitrary vector z, and arbitrary matricesM andN of proper sizes, we have\n\u2016Q (z)M\u20162F \u2264 26\n38 \u2016M\u20162F and \u2016NQ (z)\u20162F \u2264\n26 38 \u2016N\u20162F .\n(3) For any tensorM \u2208 Rs\u00d7t\u00d7q , we have\n\u2016up (M)\u20162F \u2264 1\np2 \u2016M\u20162F .\n(4) For any kernelW \u2208 Rki\u00d7ki\u00d7di and \u03b4\u0303i+1 \u2208 R(r\u0303i\u22121\u2212ki+1)\u00d7(c\u0303i\u22121\u2212ki+1)\u00d7di defined in Sec. B.1, then we have\n\u2016\u03b4\u0303i+1~\u0303W \u20162F \u2264 (ki \u2212 si + 1)2\u2016W \u20162F \u2016\u03b4\u0303i+1\u20162F .\n(5) For softmax activation function \u03c32, we can bound the norm of difference between output v and its corresponding ont-hot label as\n0 \u2264 \u2016v \u2212 y\u201622 \u2264 2.\nIt should be pointed out that we defer the proof of Lemma 8 to Sec. E."}, {"heading": "C Technical Lemmas and Their Proofs", "text": "Here we present the key lemmas and theorems for proving our desired results. For brevity, in this document we use f(w,D) to denote f(g(w;D),y) in the manuscript.\nLemma 9. Suppose that the activation function \u03c31 is sigmoid and \u03c32 is softmax, and the loss function f(w,D) is squared loss. Then the gradient of f(w,D) with respect to w(j) can be written as\n\u2207W(l+1)f(w,D) =S(v \u2212 y)zT(l) \u2208 Rdl+1\u00d7r\u0303lc\u0303ldl , \u2207W k,j (i) f(w,D) =Zj(i\u22121)~\u0303\u03b4\u0303 k i \u2208 Rki\u00d7ki , (j = 1, \u00b7 \u00b7 \u00b7 , di\u22121; k = 1, \u00b7 \u00b7 \u00b7 , di; i = 1, \u00b7 \u00b7 \u00b7 , l),\nwhere \u03b4ki is the k-slice (i.e. \u03b4i(:, :, k)) of \u03b4i which is defined as\n\u03b4i = \u2207X(i)f(w,D) \u2208 Rri\u00d7ci\u00d7di , (i = 1, \u00b7 \u00b7 \u00b7 , l),\nand further satisfies\n\u03b4ji =up\n  di+1\u2211\nk=1\n\u03b4\u0303ki+1~\u0303W\u0302 k,j(i+1)   \u03c3\u20321(Xj(i)) \u2208 Rri\u00d7ci , (j = 1, \u00b7 \u00b7 \u00b7 , di; i = 1, \u00b7 \u00b7 \u00b7 , l \u2212 1),\nwhere the matrix W\u0302 k,j(i+1) \u2208 Rki+1\u00d7ki+1 is obtained by rotatingW k,j (i+1) with 180 degrees. Also, \u03b4l is computed as follows:\n\u2207uf(w,D) = S(v \u2212 y) \u2208 Rdl+1 , \u2207z(l)f(w,D) = (W(l+1))TS(v \u2212 y) \u2208 Rr\u0303lc\u0303ldl , \u2207Z(l)f(w,D)= reshape ( \u2207z(l)f(w,D) ) \u2208 Rr\u0303l\u00d7c\u0303l\u00d7dl , \u03b4l=\u03c3\u20321(X(l)) up ( \u2202f(w,D)\n\u2202Z(l)\n) \u2208 Rri\u00d7ci\u00d7di .\nwhere S = G (u).\nProof. We use chain rule to compute the gradient of f(w,D) with respect to Z(i). We first compute several basis gradient. According to the relationship betweenX(i),Y(i),Z(i) and f(w,D), we have\n\u2207uf(w,D) = S(v \u2212 y) \u2208 Rdl+1 , \u2207z(l)f(w,D) = (W(l+1))TS(v \u2212 y) \u2208 Rr\u0303lc\u0303ldl , \u2207Z(l)f(w,D) = reshape ( \u2207z(l)f(w,D) ) \u2208 Rr\u0303l\u00d7c\u0303l\u00d7dl ,\n\u2207X(l)f(w,D) = \u2202Y(l)\n\u2202X(l)\n\u2202Z(l)\n\u2202Y(l)\n\u2202f(w,D)\n\u2202Z(l) = \u03c3\u20321(X(l)) up\n( \u2202f(w,D)\n\u2202Z(l)\n) , \u03b4l \u2208 Rri\u00d7ci\u00d7di .\nThen we can further obtain\n\u03b4ji =up\n  di+1\u2211\nk=1\n\u03b4\u0303ki+1~\u0303W\u0302 k,j(i+1)   \u03c3\u20321(Xj(i)) \u2208 Rri\u00d7ci , (j = 1, \u00b7 \u00b7 \u00b7 , di; i = 1, \u00b7 \u00b7 \u00b7 , l \u2212 1).\nwhere W\u0302 k,j(i) denotes the j-th slice W\u0302 k (i)(:, : j) of W\u0302 k (i). Note, we clockwise rotate the matrix W k,j (i+1) by 180 degrees to obtain W\u0302 k,j(i+1). Finally, we can compute the gradient w.r.t. W(l+1) andW (i) (i = 1, \u00b7 \u00b7 \u00b7 , l):\n\u2207W(l+1)f(w,D) =S(v \u2212 y)zT(l) \u2208 Rdl+1\u00d7r\u0303lc\u0303ldl , \u2207W k,j (i) f(w,D) =Xj(i\u22121)~\u0303\u03b4\u0303 k i \u2208 Rki\u00d7ki , (j = 1, \u00b7 \u00b7 \u00b7 , di\u22121; k = 1, \u00b7 \u00b7 \u00b7 , di; i = 1, \u00b7 \u00b7 \u00b7 , l).\nThe proof is completed.\nLemma 10. Suppose that the activation function \u03c31 is sigmoid and \u03c32 is softmax, and the loss function f(w,D) is squared loss. Then the gradient of f(w,D) with respect to w(j) can be written as\n\u2016\u03b4l\u20162F \u2264 \u03d1\n16p2 bl+1\n2, \u2016\u03b4i\u20162F \u2264 di+1bi+1 2(ki+1 \u2212 si+1 + 1)2 16p2 \u2225\u2225\u03b4i+1 \u2225\u22252 F , \u2016\u03b4i\u20162F \u2264 \u03d1bl+1 2 16p2 l\u220f\ns=i+1\ndsbs 2(ks \u2212 ss + 1)2\n16p2 ,\nwhere \u03d1 = 1/8.\nProof. We first bound \u03b4l. By Lemma 9, we have\n\u2016\u03b4l\u20162F = \u2225\u2225\u2225\u2225\u03c3\u20321(X(l)) up ( \u2202f(w,D)\n\u2202Z(l)\n)\u2225\u2225\u2225\u2225 2\nF\n\u00ac \u2264 1\n16p2\n\u2225\u2225\u2225\u2225 \u2202f(w,D)\n\u2202Z(l)\n\u2225\u2225\u2225\u2225 2\nF\n= 1\n16p2\n\u2225\u2225\u2225\u2225 \u2202f(w,D)\n\u2202z(l)\n\u2225\u2225\u2225\u2225 2\n2\n\u2264 1 16p2\n\u2225\u2225(W(l+1))TS(v \u2212 y) \u2225\u22252\n2\n \u2264 \u03d1\n16p2 bl+1\n2,\nwhere \u00ac holds since the values of the entries in the tensor \u03c3\u20321(X(l)) \u2208 Rrl\u00d7cl\u00d7dl belong to [0, 1/4], and the up (\u00b7) operation does repeat one entry into p2 entries but the entry value becomes 1/p2 of the original entry value.  holds since we have \u2016S(v \u2212 y)\u201622 \u2264 1/8 in Lemma 8. Also by Lemma 9, we can further bound \u2016\u03b4ji \u20162F as follows:\n\u2016\u03b4i\u20162F = di\u2211\nj=1\n\u2225\u2225\u2225\u03b4ji \u2225\u2225\u2225 2\nF =\ndi\u2211\nj=1\n\u2225\u2225\u2225\u2225\u2225\u2225 up   di+1\u2211\nk=1\n\u03b4\u0303ki+1~\u0303W\u0302 k,j(i+1)   \u03c3\u20321(Xj(i)) \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 1 16p2\ndi\u2211\nj=1\n\u2225\u2225\u2225\u2225\u2225\u2225 di+1\u2211\nk=1\n\u03b4\u0303ki+1~\u0303W\u0302 k,j(i+1) \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 di+1 16p2\ndi\u2211\nj=1\ndi+1\u2211\nk=1\n\u2225\u2225\u2225\u03b4\u0303ki+1~\u0303W\u0302 k,j(i+1) \u2225\u2225\u2225 2 F \u2264 di+1(ki+1 \u2212 si+1 + 1) 2 16p2 di\u2211\nj=1\ndi+1\u2211\nk=1\n\u2225\u2225\u2225\u03b4\u0303ki+1 \u2225\u2225\u2225 2\nF\n\u2225\u2225\u2225W\u0302 k,j(i+1) \u2225\u2225\u2225 2\nF\n\u00ac = di+1(ki+1 \u2212 si+1 + 1)2\n16p2\ndi+1\u2211\nk=1\n\u2225\u2225\u03b4ki+1 \u2225\u22252 F \u2225\u2225\u2225W k(i+1) \u2225\u2225\u2225 2 F \u2264 di+1bi+1 2(ki+1 \u2212 si+1 + 1)2 16p2 \u2225\u2225\u03b4i+1 \u2225\u22252 F ,\nwhere W\u0302 k,j(i+1) denotes the j-th slice W\u0302 k (i+1)(:, :, j) of the tensor W\u0302 k (i+1). \u00ac holds since we rotate the matrix W k,j (i+1)\nby 180 degrees to obtain W\u0302 k(i+1), indicating \u2016W k,j (i+1)\u20162F = \u2016W\u0302 k,j (i+1)\u20162F and \u2211di j=1 \u2225\u2225\u2225W k,j(i+1) \u2225\u2225\u2225 2 F = \u2225\u2225\u2225W k(i+1) \u2225\u2225\u2225 2 F \u2264 bi+12. Accordingly, the above inequality gives\n\u2016\u03b4i\u20162F \u2264 di+1bi+1 2(ki+1 \u2212 si+1 + 1)2 16p2 \u2225\u2225\u03b4i+1 \u2225\u22252 F \u2264\u00b7 \u00b7 \u00b7\u2264\u2016\u03b4l\u20162F l\u220f\ns=i+1\ndsbs 2(ks \u2212 ss + 1)2\n16p2\n\u2264\u03d1bl+1 2\n16p2\nl\u220f\ns=i+1\ndsbs 2(ks \u2212 ss + 1)2\n16p2 .\nThe proof is completed.\nLemma 11. Suppose that the activation function \u03c31 is sigmoid and \u03c32 is softmax, and the loss function f(w,D) is squared loss. Then the gradient of f(w,D) with respect toW (l+1) and w can be respectively bounded as follows:\n\u2225\u2225\u2207W(l+1)f(w,D) \u2225\u22252 F \u2264 \u03d1r\u0303lc\u0303ldl and \u2016\u2207wf(w,D)\u201622 \u2264 \u03b22,\nwhere \u03d1 = 1/8 and \u03b2 , [ \u03d1r\u0303lc\u0303ldl + \u2211l i=1 \u03d1bl+1 2di\u22121 p2bi2di ri\u22121ci\u22121 \u220fl s=i dsbs 2(ks\u2212ss+1)2 16p2 ]1/2 .\nProof. By utilizing Lemma 10, we can bound\nl\u2211\ni=1\n\u2225\u2225\u2207w(i)f(w,D) \u2225\u22252 F = l\u2211\ni=1\ndi\u2211\nk=1\ndi\u22121\u2211\nj=1\n\u2225\u2225\u2225\u2207W k,j (i) f(w,D) \u2225\u2225\u2225 2 F = l\u2211\ni=1\ndi\u2211\nk=1\ndi\u22121\u2211\nj=1\n\u2225\u2225\u2225Zj(i\u22121)~\u0303\u03b4ki \u2225\u2225\u2225 2\nF\n\u00ac \u2264\nl\u2211\ni=1\ndi\u2211\nk=1\ndi\u22121\u2211\nj=1\n(ki \u2212 si + 1)2 \u2225\u2225\u2225Zj(i\u22121) \u2225\u2225\u2225 2\nF\n\u2225\u2225\u03b4ki \u2225\u22252 F\n \u2264\nl\u2211\ni=1\ndi\u2211\nk=1\ndi\u22121\u2211\nj=1\nr\u0303i\u22121c\u0303i\u22121(ki \u2212 si + 1)2 \u2225\u2225\u03b4ki \u2225\u22252 F\n\u2264 l\u2211\ni=1\nr\u0303i\u22121c\u0303i\u22121di\u22121(ki \u2212 si + 1)2 \u2016\u03b4i\u20162F\n\u2264 l\u2211\ni=1\nr\u0303i\u22121c\u0303i\u22121di\u22121(ki \u2212 si + 1)2 \u03d1bl+1\n2\n16p2\nl\u220f\ns=i+1\ndsbs 2(ks \u2212 ss + 1)2\n16p2\n= l\u2211\ni=1\n\u03d1bl+1 2di\u22121\np2bi 2di\nri\u22121ci\u22121\nl\u220f\ns=i\ndsbs 2(ks \u2212 ss + 1)2\n16p2 ,\nwhere \u00ac holds since \u2225\u2225\u2225Zj(i\u22121)~\u0303\u03b4ki \u2225\u2225\u2225 2\nF \u2264 (ki\u2212 si + 1)2\n\u2225\u2225\u2225Zj(i\u22121) \u2225\u2225\u2225 2\nF\n\u2225\u2225\u03b4ki \u2225\u22252 F\n; and  holds since the values of entries in Zj(i\u22121) belong to [0, 1].\nOn the other hand, we can bound\n\u2225\u2225\u2207W(l+1)f(w,D) \u2225\u22252 F = \u2225\u2225\u2225S(v \u2212 y)zT(l) \u2225\u2225\u2225 2 F \u2264 \u03d1r\u0303lc\u0303ldl.\nSo we can bound the `2 norm of the gradient as follows:\n\u2016\u2207wf(w,D)\u20162F = \u2225\u2225\u2207W(l+1)f(w,x) \u2225\u22252 F + l\u2211\ni=1\n\u2225\u2225\u2207w(i)f(w,D) \u2225\u22252 F\n\u2264\u03d1r\u0303lc\u0303ldl + l\u2211\ni=1\n\u03d1bl+1 2di\u22121\np2bi 2di\nri\u22121ci\u22121\nl\u220f\ns=i\ndsbs 2(ks \u2212 ss + 1)2\n16p2 .\nThe proof is completed.\nLemma 12. Suppose that the activation function \u03c31 is sigmoid and \u03c32 is softmax, and the loss function f(w,D) is squared loss. Then for both cases, the gradient of x(i) with respect to w(j) can be bounded as follows:\n\u2225\u2225\u2225\u2225\u2225\u2225 \u2202vec ( X(i) ) \u2202w(j) \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n= \u2225\u2225\u2225\u2225 \u2202x(i)\n\u2202w(j)\n\u2225\u2225\u2225\u2225 2\nF\n\u2264 diricir\u0303j\u22121c\u0303j\u22121dj\u22121(kj \u2212 sj + 1)2 i\u220f\ns=j+1\ndsbs 2(ks \u2212 ss + 1)2\n16p2\nand\nmax s \u2225\u2225\u2225\u2225\u2225\u2225 \u2202vec ( Xs(i) ) \u2202w(j) \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n= max s\n\u2225\u2225\u2225\u2225 \u2202xs(i)\n\u2202w(j)\n\u2225\u2225\u2225\u2225 2\nF\n\u2264 ricir\u0303j\u22121c\u0303j\u22121dj\u22121(kj \u2212 sj + 1)2 i\u220f\ns=j+1\ndsbs 2(ks \u2212 ss + 1)2\n16p2 .\nProof. For brevity, letXk(i)(s, t) denotes the (s, t)-th entry in the matrixX k (i) \u2208 Rri\u00d7ci . We also let \u03c6(i,m) =\n\u2202Xk(i)(s,t)\n\u2202X(m) \u2208\nRrm\u00d7cm\u00d7dm . So similar to Lemma 9, we have\n\u03c6q(i,m) =up\n  dm+1\u2211\nk=1\n\u03c6\u0303k(i,m+1)~\u0303W\u0302 k,q (m+1)   \u03c3\u20321(Xq(m)) \u2208 Rrm\u00d7cm , (q = 1, \u00b7 \u00b7 \u00b7 , dm),\nwhere the matrix W\u0302 k,q(m+1) \u2208 Rkm+1\u00d7km+1 is obtained by rotating W k,q (m+1) with 180 degrees. Then according to the relationship betweenX(j) andW(j), we can compute\n\u2202Xk(i)(s, t)\n\u2202W g,h(j) = Zh(j\u22121)~\u0303\u03c6 g (i,j) \u2208 Rkj\u00d7kj , (h = 1, \u00b7 \u00b7 \u00b7 , dj\u22121; g = 1, \u00b7 \u00b7 \u00b7 , dj).\nTherefore, we can further obtain \u2225\u2225\u2225\u2225\u2225 \u2202Xk(i)(s, t)\n\u2202w(j)\n\u2225\u2225\u2225\u2225\u2225 2\nF\n=\ndj\u2211\ng=1\ndj\u22121\u2211\nh=1\n\u2225\u2225\u2225\u2225\u2225 \u2202Xk(i)(s, t)\n\u2202W g,h(j)\n\u2225\u2225\u2225\u2225\u2225 2\nF\n=\ndj\u2211\ng=1\ndj\u22121\u2211\nh=1\n\u2225\u2225\u2225\u2225\u2225 \u2202Xk(i)(s, t)\n\u2202W g,h(j)\n\u2225\u2225\u2225\u2225\u2225 2\nF\n=\ndj\u2211\ng=1\ndj\u22121\u2211\nh=1\n\u2225\u2225\u2225Zh(j\u22121)~\u0303\u03c6g(i,j) \u2225\u2225\u2225 2\nF\n\u2264 dj\u2211\ng=1\ndj\u22121\u2211\nh=1\n(kj \u2212 sj + 1)2 \u2225\u2225\u2225Zh(j\u22121) \u2225\u2225\u2225 2\nF\n\u2225\u2225\u2225\u03c6g(i,j) \u2225\u2225\u2225 2\nF \u2264 (kj \u2212 sj + 1)2\n\u2225\u2225\u2225Z(j\u22121) \u2225\u2225\u2225 2\nF\n\u2225\u2225\u2225\u03c6(i,j) \u2225\u2225\u2225 2\nF\n\u2264r\u0303j\u22121c\u0303j\u22121dj\u22121(kj \u2212 sj + 1)2 \u2225\u2225\u2225\u03c6(i,j) \u2225\u2225\u2225 2\nF .\nOn the other hand, by Lemma 9, we can further bound \u2016\u03c6(i,j)\u20162F as follows:\n\u2225\u2225\u2225\u03c6(i,m) \u2225\u2225\u2225 2\nF =\ndm\u2211\nq=1\n\u2225\u2225\u2225\u03c6q(i,m) \u2225\u2225\u2225 2\nF =\ndm\u2211\nq=1\n\u2225\u2225\u2225\u2225\u2225\u2225 up   dm+1\u2211\nk=1\n\u03c6\u0303k(i,m+1)~\u0303W\u0302 k,q (m+1)   \u03c3\u20321(Xq(m)) \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 1 16p2\ndm\u2211\nq=1\n\u2225\u2225\u2225\u2225\u2225\u2225 dm+1\u2211\nk=1\n\u03c6\u0303k(i,m+1)~\u0303W\u0302 k,q (m+1) \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 dm+1 16p2\ndm\u2211\nq=1\ndm+1\u2211\nk=1\n\u2225\u2225\u2225\u03c6\u0303k(i,m+1)~\u0303W\u0302 k,q(m+1) \u2225\u2225\u2225 2\nF\n\u2264dm+1(km+1 \u2212 sm+1 + 1) 2\n16p2\ndm\u2211\nq=1\ndm+1\u2211\nk=1\n\u2225\u2225\u2225\u03c6\u0303k(i,m+1) \u2225\u2225\u2225 2\nF\n\u2225\u2225\u2225W\u0302 k,q(m+1) \u2225\u2225\u2225 2\nF\n\u00ac = dm+1(km+1 \u2212 sm+1 + 1)2\n16p2\ndm+1\u2211\nk=1\n\u2225\u2225\u2225\u03c6\u0303k(i,m+1) \u2225\u2225\u2225 2\nF\n\u2225\u2225\u2225W\u0302 k(m+1) \u2225\u2225\u2225 2\nF\n\u2264dm+1bm+1 2(km+1 \u2212 sm+1 + 1)2\n16p2\n\u2225\u2225\u2225\u03c6(i,m+1) \u2225\u2225\u2225 2\nF ,\nwhere \u00ac holds since \u2016W k,q(m+1)\u20162F = \u2016W\u0302 k,q (m+1)\u20162F . It further yields\n\u2225\u2225\u2225\u03c6(i,m) \u2225\u2225\u2225 2 F \u2264 \u2225\u2225\u2225\u03c6(i,i) \u2225\u2225\u2225 2 F i\u220f\ns=m+1\ndsbs 2(ks \u2212 ss + 1)2\n16p2 \u00ac =\ni\u220f\ns=m+1\ndsbs 2(ks \u2212 ss + 1)2\n16p2 .\nwhere \u00ac holds since we have \u2225\u2225\u2225\u03c6(i,i) \u2225\u2225\u2225 2\nF =\n\u2225\u2225\u2225\u2225 \u2202Xk(i)(s,t)\n\u2202X(i)\n\u2225\u2225\u2225\u2225 2\nF\n= 1.\nTherefore, we have \u2225\u2225\u2225\u2225\u2225 \u2202xk(i)\n\u2202w(j)\n\u2225\u2225\u2225\u2225\u2225 2\nF\n=\nri\u2211\ns=1\nci\u2211\nt=1\n\u2225\u2225\u2225\u2225\u2225 \u2202Xk(i)(s, t)\n\u2202w(j)\n\u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 ri\u2211\ns=1\nci\u2211\nt=1\nr\u0303j\u22121c\u0303j\u22121dj\u22121(kj \u2212 sj + 1)2 \u2225\u2225\u2225\u03c6(i,j) \u2225\u2225\u2225 2\nF\n=ricir\u0303j\u22121c\u0303j\u22121dj\u22121(kj \u2212 sj + 1)2 \u2225\u2225\u2225\u03c6(i,j) \u2225\u2225\u2225 2\nF\n\u2264ricir\u0303j\u22121c\u0303j\u22121dj\u22121(kj \u2212 sj + 1)2 i\u220f\ns=j+1\ndsbs 2(ks \u2212 ss + 1)2\n16p2 .\nIt further gives \u2225\u2225\u2225\u2225 \u2202x(i)\n\u2202w(j)\n\u2225\u2225\u2225\u2225 2\nF\n=\ndi\u2211\ns=1\n\u2225\u2225\u2225\u2225 \u2202xs(i)\n\u2202w(j)\n\u2225\u2225\u2225\u2225 2\nF\n\u2264 diricir\u0303j\u22121c\u0303j\u22121dj\u22121(kj \u2212 sj + 1)2 i\u220f\ns=j+1\ndsbs 2(ks \u2212 ss + 1)2\n16p2 .\nThe proof is completed.\nLemma 13. Suppose that the activation function \u03c31 is sigmoid and \u03c32 is softmax, and the loss function f(w,D) is squared loss. Then the gradient of \u03b4sl with respect to w(j) can be bounded as follows:\n\u2225\u2225\u2225\u2225 \u2202\u03b4sl \u2202w(j) \u2225\u2225\u2225\u2225 2\nF\n\u2264 \u03d1\u0303bl+1 2\n16p2\n\u2225\u2225\u2225W s(l+1) \u2225\u2225\u2225 2\nF dlrlclr\u0303j\u22121c\u0303j\u22121dj\u22121(kj \u2212 sj + 1)2\nl\u220f\ns=j+1\ndsbs 2(ks \u2212 ss + 1)2\n16p2\nand \u2225\u2225\u2225\u2225 \u2202\u03b4l \u2202w(j) \u2225\u2225\u2225\u2225 2\nF\n\u2264 \u03d1\u0303bl+1 4\n16p2 dlrlclr\u0303j\u22121c\u0303j\u22121dj\u22121(kj \u2212 sj + 1)2\nl\u220f\ns=j+1\ndsbs 2(ks \u2212 ss + 1)2\n16p2 ,\nwhere \u03d1\u0303 = 364 .\nProof. Assume thatW(l+1) = [W 1(l+1),W 2 (l+1), \u00b7 \u00b7 \u00b7 ,W dl(l+1)] whereW i(l+1) \u2208 Rdl+1\u00d7r\u0303lc\u0303l is a submatrix inW(l+1). Then we have v = \u03c32( \u2211dl k=1W k (l+1)z k (l)). For brevity, we further define a matrixG(k) as follows:\nG(k)= [ \u03c3\u20321 ( x(k) ) , \u03c3\u20321 ( x(k) ) , \u00b7 \u00b7 \u00b7 , \u03c3\u20321 ( x(k) ) \ufe38 \ufe37\ufe37 \ufe38\nrkck columns\n] \u2208 Rrkckdk\u00d7rkck ,\nThen we have\n\u2202\n\u2202z(l)\n( \u2202f(w,D)\n\u2202zs(l)\n) = [ (v \u2212 y)T \u2297 (W s(l+1))T ] Q (u)W(l+1) + (W s(l+1)) TG (u)G (u)W(l+1),\nwhere Q (u) is a matrix of size d2l+1 \u00d7 dl+1 whose (s, (s\u2212 1)dl+1 + s) entry equal to \u03c31(us)(1\u2212 \u03c31(us))(1\u2212 2\u03c31(us)) and rest entries are all 0. Accordingly, we have\n\u2225\u2225\u2225\u2225\u2225 \u2202\n\u2202z(l)\n( \u2202f(w,D)\n\u2202zs(l)\n)\u2225\u2225\u2225\u2225\u2225 2\nF\n\u22642 (\u2225\u2225\u2225 [ (v \u2212 y)T \u2297 (W s(l+1))T ] Q (u)W(l+1) \u2225\u2225\u2225 2 F + \u2225\u2225\u2225(W s(l+1))TG (u)G (u)W(l+1) \u2225\u2225\u2225 2 F ) \u22642 ( 26\n38 \u2016v \u2212 y\u20162F\n\u2225\u2225\u2225W s(l+1) \u2225\u2225\u2225 2\nF\n\u2225\u2225W(l+1) \u2225\u22252 F + 1\n162\n\u2225\u2225\u2225W s(l+1) \u2225\u2225\u2225 2\nF\n\u2225\u2225W(l+1) \u2225\u22252 F\n)\n\u00ac \u22642 ( 27\n38 +\n1\n162\n) bl+1 2 \u2225\u2225\u2225W s(l+1) \u2225\u2225\u2225 2\nF\n\u2264 3 64 bl+1\n2 \u2225\u2225\u2225W s(l+1) \u2225\u2225\u2225 2\nF ,\nwhere we have \u2016v \u2212 y\u20162F \u2264 2 by Lemma 8. Then by similar way, we can have\n\u2225\u2225\u2225\u2225 \u2202\u03b4sl \u2202w(j) \u2225\u2225\u2225\u2225 2\nF\n= \u2225\u2225\u2225\u2225\u2225 \u2202\n\u2202z(l)\n( \u2202f(w,D)\n\u2202zs(l)\n) \u2202z(l)\n\u2202yl \u2202yl \u2202x(l) \u2202x(l) \u2202w(j) \u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 3 64 \u2217 16p2 bl+1\n2 \u2225\u2225\u2225W s(l+1) \u2225\u2225\u2225 2\nF\n\u2225\u2225\u2225\u2225 \u2202x(l)\n\u2202w(j)\n\u2225\u2225\u2225\u2225 2\nF\n\u2264 3 64 \u2217 16p2 bl+1\n2 \u2225\u2225\u2225W s(l+1) \u2225\u2225\u2225 2\nF dlrlclr\u0303j\u22121c\u0303j\u22121dj\u22121(kj \u2212 sj + 1)2\nl\u220f\ns=j+1\ndsbs 2(ks \u2212 ss + 1)2\n16p2 .\nTherefore, we can further obtain:\n\u2225\u2225\u2225\u2225 \u2202\u03b4l \u2202w(j) \u2225\u2225\u2225\u2225 2\nF\n=\ndl+1\u2211\ns=1\n\u2225\u2225\u2225\u2225 \u2202\u03b4sl \u2202w(j) \u2225\u2225\u2225\u2225 2\nF\n\u2264 3 64 \u2217 16p2 bl+1\n4dlrlclr\u0303j\u22121c\u0303j\u22121dj\u22121(kj \u2212 sj + 1)2 l\u220f\ns=j+1\ndsbs 2(ks \u2212 ss + 1)2\n16p2 .\nThe proof is completed.\nLemma 14. Suppose that the activation function \u03c31 is sigmoid and \u03c32 is softmax, and the loss function f(w,D) is squared loss. Then the Hessian of f(w,x) with respect to w can be bounded as follows:\n\u2225\u2225\u22072wf(w,D) \u2225\u22252 F \u2264 O (\u03b32) ,\nwhere \u03b3 = ( \u03d1bl+1\n2d20 b14d21 l2r20c 2 0 [\u220fl s=1 dsbs 2(ks\u2212ss+1)2 8 \u221a 2p2 ]2)1/2 . With the same condition, we can bound the operation norm of\n\u22073wf(w,D). That is, there exists a universal constant \u03bd such that \u2225\u2225\u22073wf(w,D) \u2225\u2225 op \u2264 \u03bd.\nProof. From Lemma 9, we can further compute the Hessian matrix \u22072wf(w,D). Recall that wk(i) \u2208 Rki 2di\u22121 (k =\n1, \u00b7 \u00b7 \u00b7 , dl) is the vectorization of W k(i) \u2208 Rki\u00d7ki\u00d7di\u22121 , i.e. wk(i) = [ vec ( W 1(i)(:, : 1) ) ; \u00b7 \u00b7 \u00b7 ; vec ( W di(i)(:, :, di\u22121) )] .\nLet w(i) = [ w1(i); \u00b7 \u00b7 \u00b7 ;wdi(i) ] \u2208 Rki2di\u22121\u00d7di (i = 1, \u00b7 \u00b7 \u00b7 , l). Also, w(l+1) \u2208 Rr\u0303lc\u0303ldldl+1 is the vectorization of the weight matrixW(l+1). Then if 1 \u2264 i, j \u2264 l, we can have\n\u22022f(w,D)\n\u2202w(j)\u2202w k (i)\n=   \u22022f(w,D) \u2202w(j)\u2202w k,1 (i) \u22022f(w,D) \u2202w(j)\u2202w k,2 (i) ... \u22022f(w,D)\n\u2202w(j)\u2202w k,di\u22121 (i)\n  =   \u2202(vec(Z1(i\u22121)~\u0303\u03b4\u0303ki )) \u2202w(j) \u2202(vec(Z2(i\u22121)~\u0303\u03b4\u0303ki )) \u2202w(j) ... \u2202 ( vec ( Z di\u22121 (i\u22121)~\u0303\u03b4\u0303ki ))\n\u2202w(j)\n \n=   P1 ( \u03b4\u0303ki ) \u2202(vec(Z1(i\u22121))) \u2202w(j) + P2 ( Z1(i\u22121) ) \u2202(vec(\u03b4\u0303ki )) \u2202w(j) P1 ( \u03b4\u0303ki ) \u2202(vec(Z2(i\u22121))) \u2202w(j) + P2 ( Z2(i\u22121) ) \u2202(vec(\u03b4\u0303ki )) \u2202w(j) ...\nP1 ( \u03b4\u0303ki )\u2202(vec(Zdi\u22121 (i\u22121) )) \u2202w(j) + P2 ( Z di\u22121 (i\u22121) ) \u2202(vec(\u03b4\u0303ki )) \u2202w(j)\n  \u2208 Rki2di\u22121\u00d7kj2djdj\u22121 ,\n(4)\nwhere P1 ( \u03b4\u0303ki ) \u2208 Rki2\u00d7r\u0303i\u22121c\u0303i\u22121di\u22121 and P2 ( Z di\u22121 (i\u22121) ) \u2208 Rki2\u00d7(r\u0303i\u2212ki+1)(c\u0303i\u2212ki+1) satisfy: each row in P1 ( \u03b4\u0303ki ) contains\nthe vectorization of (\u03b4\u0303ki ) T at the right position and the remaining entries are 0s, and each row in P2 ( Z di\u22121 (i\u22121) ) is the submatrix in Zdi\u22121(i\u22121) that need to conduct inner product with \u03b4\u0303 k i in turn. Note that there are si\u2212 1 rows and columns between\neach neighboring nonzero entries inN which is decided by the definition of \u03b4\u0303i+1 in Sec. B.1. Accordingly, we have\n\u2225\u2225\u2225\u2225\u2225\u2225 P1 ( \u03b4\u0303ki )\u2202 ( vec ( Z di\u22121 (i\u22121) )) \u2202w(j) \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (ki \u2212 si + 1)2\u2016\u03b4\u0303ki \u20162F \u2225\u2225\u2225\u2225\u2225\u2225 \u2202 ( vec ( Z di\u22121 (i\u22121) )) \u2202w(j) \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n= (ki \u2212 si + 1)2\u2016\u03b4ki \u20162F \u2225\u2225\u2225\u2225\u2225\u2225 \u2202 ( vec ( Z di\u22121 (i\u22121) )) \u2202w(j) \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\nand\n\u2225\u2225\u2225\u2225\u2225\u2225 P2 ( Z di\u22121 (i\u22121) )\u2202 ( vec ( \u03b4\u0303ki )) \u2202w(j) \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 (ki \u2212 si + 1)2\u2016Zdi\u22121(i\u22121)\u20162F \u2225\u2225\u2225\u2225\u2225\u2225 \u2202 ( vec ( \u03b4\u0303ki )) \u2202w(j) \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n= (ki \u2212 si + 1)2\u2016Zdi\u22121(i\u22121)\u20162F \u2225\u2225\u2225\u2225\u2225 \u2202 ( vec ( \u03b4ki ))\n\u2202w(j)\n\u2225\u2225\u2225\u2225\u2225 2\nF\n.\nThen in order to bound\n\u2016\u22072wf(w,D)\u20162F = l+1\u2211\ni=1\nl+1\u2211\nj=1\n\u2225\u2225\u2225\u2225 \u22022f(w,D)\n\u2202w(j)\u2202w(i)\n\u2225\u2225\u2225\u2225 2\nF\n,\nwe try to bound each term separately. So we consider the following five cases: l \u2265 i \u2265 j, i \u2264 j \u2264 l, l + 1 = i > j, l + 1 = j > i and l + 1 = i = j.\nCase 1: l \u2265 i \u2265 j\nIn the following, we first consider the first case, i.e. i \u2265 j, and bound \u2225\u2225\u2225\u2225\u2225\u2225 \u2202 ( vec ( \u03b4\u0303ki )) \u2202w(j) \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n= \u2225\u2225\u2225\u2225\u2225 \u2202 ( vec ( \u03b4ki ))\n\u2202w(j)\n\u2225\u2225\u2225\u2225\u2225 2\nF\n= \u2225\u2225\u2225\u2225\u2225\u2225 \u2202 \u2202w(j) vec  up   di+1\u2211\ns=1\n\u03b4\u0303si+1~\u0303W\u0302 s,k(i+1)   \u03c3\u20321(Xk(i))   \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u00ac \u2264 2\n16 \u2225\u2225\u2225\u2225\u2225\u2225 \u2202 \u2202w(j) vec  up   di+1\u2211\ns=1\n\u03b4\u0303si+1~\u0303W\u0302 s,k(i+1)     \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n+ 2 \u2225\u2225\u2225\u2225\u2225\u2225 up   di+1\u2211\ns=1\n\u03b4\u0303si+1~\u0303W\u0302 s,k(i+1)   \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u2225\u2225\u2225\u2225\u2225 \u2202\u03c3\u20321(X k (i))\n\u2202w(j)\n\u2225\u2225\u2225\u2225\u2225 2\nF\n= 2\n16p2 \u2225\u2225\u2225\u2225\u2225\u2225 \u2202 \u2202w(j) vec   di+1\u2211\ns=1\n\u03b4\u0303si+1~\u0303W\u0302 s,k(i+1)   \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n+ 2\np2 \u2225\u2225\u2225\u2225\u2225\u2225 di+1\u2211\ns=1\n\u03b4\u0303si+1~\u0303W\u0302 s,k(i+1) \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u2225\u2225\u2225\u2225\u2225\u2225 \u2202vec ( \u03c3\u2032(xk(i)) ) \u2202xk(i) \u2202xk(i) \u2202w(j) \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n \u2264 2\n16p2 \u2225\u2225\u2225\u2225\u2225\u2225 \u2202 \u2202w(j) vec   di+1\u2211\ns=1\n\u03b4\u0303si+1~\u0303W\u0302 s,k(i+1)   \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n+ 2 \u00b7 26 38p2 \u2225\u2225\u2225\u2225\u2225\u2225 di+1\u2211\ns=1\n\u03b4\u0303si+1~\u0303W\u0302 s,k(i+1) \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u2225\u2225\u2225\u2225\u2225 \u2202Xk(i)\n\u2202w(j)\n\u2225\u2225\u2225\u2225\u2225 2\nF\n\u00ae \u22642di+1\n16p2 (ki+1\u2212si+1+1)2\ndi+1\u2211\ns=1\n\u2016W\u0302 s,k(i+1)\u20162F \u2225\u2225\u2225\u2225\u2225 \u2202\u03b4\u0303si+1 \u2202w(j) \u2225\u2225\u2225\u2225\u2225 2\nF\n+ 2 \u00b7 26 38p2 di+1(ki+1\u2212si+1+1)2 di+1\u2211\ns=1\n\u2225\u2225\u2225W\u0302 s,k(i+1) \u2225\u2225\u2225 2\nF\n\u2225\u2225\u2225\u03b4\u0303si+1 \u2225\u2225\u2225 2\nF \u2225\u2225\u2225\u2225\u2225 \u2202Xk(i)\n\u2202w(j)\n\u2225\u2225\u2225\u2225\u2225 2\nF\n\u00af = 2di+1 16p2\n(ki+1\u2212si+1+1)2 di+1\u2211\ns=1\n\u2016W s,k(i+1)\u20162F \u2225\u2225\u2225\u2225 \u2202\u03b4si+1 \u2202w(j) \u2225\u2225\u2225\u2225 2\nF\n+ 2 \u00b7 26 38p2 di+1(ki+1\u2212si+1+1)2 di+1\u2211\ns=1\n\u2225\u2225\u2225W s,k(i+1) \u2225\u2225\u2225 2\nF\n\u2225\u2225\u03b4si+1 \u2225\u22252 F \u2225\u2225\u2225\u2225\u2225 \u2202Xk(i)\n\u2202w(j)\n\u2225\u2225\u2225\u2225\u2225 2\nF\n.\n(5)\n\u00ac holds sinceXk(i) is independent onw(j) and the values of entries in \u03c3 \u2032 1(X k (i)) is not larger than 1/4 since for any constant a, \u03c3\u2032(a) = \u03c3(a)(1\u2212 \u03c3(a)) \u2264 1/4.  holds since for arbitrary tensorM , we have \u2016up (M)\u20162F \u2264 \u2016M\u20162F /p2 in Lemma 8, and we also have\n\u2225\u2225\u2225\u2225\u2225\u2225 \u2202vec ( \u03c3\u2032(xk(i)) ) \u2202xk(i) \u2202xk(i) \u2202w(j) \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n= \u2225\u2225\u2225\u2225\u2225Q ( xk(i) ) \u2202xk(i) \u2202w(j) \u2225\u2225\u2225\u2225\u2225 2\nF\n\u2264 2 6\n38 \u2225\u2225\u2225\u2225\u2225 \u2202xk(i)\n\u2202w(j)\n\u2225\u2225\u2225\u2225\u2225 2\nF\n= 26\n38 \u2225\u2225\u2225\u2225\u2225 \u2202Xk(i)\n\u2202w(j)\n\u2225\u2225\u2225\u2225\u2225 2\nF\n.\n\u00ae holds since we can just adopt similar strategy in Eqn. (4) to separate W\u0302 s,k(i+1) and the conclusion in Lemma 8; \u00af holds since the difference between \u03b4\u0303si+1 and \u03b4 s i+1 is that we pad 0 around \u03b4 s i+1 to obtain \u03b4\u0303 s i+1, indicating \u2016\u03b4si+1\u20162F = \u2016\u03b4\u0303si+1\u20162F .\nAccordingly, we can further bound \u2225\u2225\u2225\u2225\u2225 \u2202\u03b4\u0303i \u2202w(j) \u2225\u2225\u2225\u2225\u2225 2\nF\n=\ndi\u22121\u2211\nk=1\n\u2225\u2225\u2225\u2225\u2225 \u2202 ( vec ( \u03b4ki ))\n\u2202w(j)\n\u2225\u2225\u2225\u2225\u2225 2\nF\n\u22642di+1 16p2\n(ki+1\u2212si+1+1)2 di\u22121\u2211\nk=1\ndi+1\u2211\ns=1\n\u2016W s,k(i+1)\u20162F \u2225\u2225\u2225\u2225 \u2202\u03b4si+1 \u2202w(j) \u2225\u2225\u2225\u2225 2\nF\n+ 2 \u00b7 26 38p2 di+1(ki+1\u2212si+1+1)2 di\u22121\u2211\nk=1\ndi+1\u2211\ns=1\n\u2225\u2225\u2225W s,k(i+1) \u2225\u2225\u2225 2\nF\n\u2225\u2225\u03b4si+1 \u2225\u22252 F \u2225\u2225\u2225\u2225\u2225 \u2202Xk(i)\n\u2202w(j)\n\u2225\u2225\u2225\u2225\u2225 2\nF\n\u22642di+1 16p2\n(ki+1\u2212si+1+1)2 di+1\u2211\ns=1\n\u2016W s(i+1)\u20162F \u2225\u2225\u2225\u2225 \u2202\u03b4si+1 \u2202w(j) \u2225\u2225\u2225\u2225 2\nF\n+ 2 \u00b7 26 38p2 di+1(ki+1\u2212si+1+1)2 \u2225\u2225\u03b4i+1 \u2225\u22252 F max s \u2225\u2225\u2225W s(i+1) \u2225\u2225\u2225 2 F max k \u2225\u2225\u2225\u2225\u2225 \u2202Xk(i)\n\u2202w(j)\n\u2225\u2225\u2225\u2225\u2225 2\nF\n\u00ac \u22642di+1\n16p2 (ki+1\u2212si+1+1)2bi+12 \u2225\u2225\u2225\u2225 \u2202\u03b4i+1 \u2202w(j) \u2225\u2225\u2225\u2225 2\nF\n+ 2 \u00b7 26 38p2 di+1(ki+1\u2212si+1+1)2bi+12 \u2225\u2225\u03b4i+1 \u2225\u22252 F max k \u2225\u2225\u2225\u2225\u2225 \u2202Xk(i)\n\u2202w(j)\n\u2225\u2225\u2225\u2225\u2225 2\nF\n \u22642di+1\n16p2 (ki+1\u2212si+1+1)2bi+12 \u2225\u2225\u2225\u2225 \u2202\u03b4i+1 \u2202w(j) \u2225\u2225\u2225\u2225 2\nF\n+ \u03d1bl+1 2dj\u22121 3p2bj 2dj ricirj\u22121cj\u22121\nl\u220f\ns=j\ndsbs 2(ks \u2212 ss + 1)2\n16p2 ,\nwhere \u00ac holds since we have \u2016W s(i+1)\u2016F \u2264 rw;  holds due to the bounds of \u2225\u2225\u03b4i+1 \u2225\u22252 F and \u2225\u2225\u2225\u2225 \u2202Xk(i) \u2202w(j) \u2225\u2225\u2225\u2225 2\nF\nin Lemma 10\nand 12.\nThen, we can use the above recursion inequality to further obtain\n\u2225\u2225\u2225\u2225 \u2202\u03b4i \u2202w(j) \u2225\u2225\u2225\u2225 2\nF\n\u2264 [ i+1\u220f\ns=i+1\n2ds 16p2\n(ks\u2212ss+1)2bs2 ] 2di+2\n16p2 (ki+2\u2212si+2+1)2bi+22 \u2225\u2225\u2225\u2225 \u2202\u03b4i+2 \u2202w(j) \u2225\u2225\u2225\u2225 2\nF\n+ \u03d1bl+1 2dj\u22121 3p2bj 2dj rj\u22121cj\u22121ri+1ci+1\nl\u220f\ns=j\ndsbs 2(ks\u2212ss+1)2\n16p2\n \n+ \u03d1bl+1 2dj\u22121 3p2bj 2dj ricirj\u22121cj\u22121\nl\u220f\ns=j\ndsbs 2(ks \u2212 ss + 1)2\n16p2\n\u2264 [ l\u220f\ns=i+1\n2ds 16p2 (ks\u2212ss+1)2bs2 ]\u2225\u2225\u2225\u2225 \u2202\u03b4l \u2202w(j) \u2225\u2225\u2225\u2225 2\nF\n+ \u03d1bl+1 2dj\u22121 3p2bj 2dj rj\u22121cj\u22121\nl\u220f\ns=j\ndsbs 2(ks \u2212 ss + 1)2\n16p2\n[ rici + ri+1ci+1 [ i+1\u220f\ns=i+1\n2ds 16p2\n(ks\u2212ss+1)2bs2 ]\n+ri+2ci+2\n[ i+2\u220f\ns=i+1\n2ds 16p2\n(ks\u2212ss+1)2bs2 ] + \u00b7 \u00b7 \u00b7+ rlcl [ l\u220f\ns=i+1\n2ds 16p2\n(ks\u2212ss+1)2bs2 ]] .\nBy Lemma 13, we have\n\u2225\u2225\u2225\u2225 \u2202\u03b4l \u2202w(j) \u2225\u2225\u2225\u2225 2\nF\n\u2264 \u03d1\u0303bl+1 4dj\u22121\np2bj 2dj\ndlrlclrj\u22121cj\u22121\nl\u220f\ns=j\ndsbs 2(ks \u2212 ss + 1)2\n16p2 ,\nwhere \u03d1\u0303 = 364 . Thus, we can establish\n\u2225\u2225\u2225\u2225 \u2202\u03b4i \u2202w(j) \u2225\u2225\u2225\u2225 2\nF\n\u2264 \u03d1\u0303bl+1 2dj\u22121\np2bj 2dj\nrj\u22121cj\u22121\n[ \u03c4\n3 + bl+1\n2dlrlcl\nl\u220f\ns=i+1\ndsbs 2(ks \u2212 ss + 1)2\n16p2\n] l\u220f\ns=j\ndsbs 2(ks \u2212 ss + 1)2\n16p2 .\nwhere \u03c4 = rici + ri+1ci+1 [\u220fi+1 s=i+1 2ds 16p2 (ks\u2212ss+1)2bs 2 ] + \u00b7 \u00b7 \u00b7+ rlcl [\u220fl s=i+1 2ds 16p2 (ks\u2212ss+1)2bs 2 ] . It further gives the bound of \u2225\u2225\u2225 \u2202\n2f(w,x) \u2202w(j)\u2202w(i)\n\u2225\u2225\u2225 2\nF as follows:\n\u2225\u2225\u2225\u2225 \u22022f(w,D)\n\u2202w(j)\u2202w(i)\n\u2225\u2225\u2225\u2225 2\nF\n=\ndi\u22121\u2211\nk=1\n\u2225\u2225\u2225\u2225\u2225 \u22022f(w,D)\n\u2202w(j)\u2202w k (i)\n\u2225\u2225\u2225\u2225\u2225 2\nF\n=\ndi\u22121\u2211\nk=1\ndi\u2211\ns=1\n\u2225\u2225\u2225\u2225\u2225\u2225 P1 ( \u03b4ki )\u2202 ( vec ( Xs(i\u22121) )) \u2202w(j) + P2 ( Xs(i\u22121) )\u2202 ( vec ( \u03b4ki )) \u2202w(j) \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n\u22642 di\u22121\u2211\nk=1\ndi\u2211\ns=1\n  \u2225\u2225\u2225\u2225\u2225\u2225 P1 ( \u03b4ki )\u2202 ( vec ( Xs(i\u22121) )) \u2202w(j) \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n+ \u2225\u2225\u2225\u2225\u2225P2 ( Xs(i\u22121) )\u2202 ( vec ( \u03b4ki ))\n\u2202w(j)\n\u2225\u2225\u2225\u2225\u2225 2\nF\n \n\u22642(ki\u2212si+1)2 di\u22121\u2211\nk=1\ndi\u2211\ns=1\n  \u2225\u2225\u03b4ki \u2225\u22252 F \u2225\u2225\u2225\u2225\u2225\u2225 \u2202 ( vec ( Xs(i\u22121) )) \u2202w(j) \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n+ \u2225\u2225\u2225Xs(i\u22121) \u2225\u2225\u2225 2\nF \u2225\u2225\u2225\u2225\u2225 \u2202 ( vec ( \u03b4ki ))\n\u2202w(j)\n\u2225\u2225\u2225\u2225\u2225 2\nF\n \n\u22642(ki\u2212si+1)2  \u2016\u03b4i\u20162F \u2225\u2225\u2225\u2225\u2225\u2225 \u2202 ( vec ( X(i\u22121) )) \u2202w(j) \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n+ \u2225\u2225\u2225X(i\u22121) \u2225\u2225\u2225 2\nF \u2225\u2225\u2225\u2225 \u2202 (vec (\u03b4i)) \u2202w(j) \u2225\u2225\u2225\u2225 2\nF\n \n\u00ac \u226432\u03d1bl+1 2di\u22121 bi 2bj 2didj ri\u22121ci\u22121rj\u22121cj\u22121\nl\u220f\ns=i+1\ndsbs 2(ks \u2212 ss + 1)2\n16p2 + ri\u22121ci\u22121di\u22121 \u2225\u2225\u2225\u2225 \u2202 (vec (\u03b4i)) \u2202w(j) \u2225\u2225\u2225\u2225 2\nF\n \u2264O  \u03d1bl+1\n2di\u22121dj\u22121 bi 2bj 2didj ri\u22121ci\u22121rj\u22121cj\u22121\n  l\u220f\ns=j\ndsbs 2(ks \u2212 ss + 1)2\n16p2\n  [ l\u220f\ns=i\n2dsbs 2(ks \u2212 ss + 1)2\n16p2\n]  .\nwhere \u00ac holds because of Lemma 12, while  holds due to 13.\nCase 2: i \u2264 j \u2264 l\nSince \u2202 2f(w,D) \u2202w\u2202wT is symmetrical, we have \u2202 2f(w,D)\n\u2202wT (i) \u2202w(j)\n=\n( \u22022f(w,D)\n\u2202wT (j) \u2202w(i)\n)T (1 \u2264 i, j \u2264 l). Thus, it yields\n\u2225\u2225\u2225\u2225\u2225 \u22022f(w,D)\n\u2202wT(i)\u2202w(j)\n\u2225\u2225\u2225\u2225\u2225 2\nF\n= \u2225\u2225\u2225\u2225\u2225 \u22022f(w,D)\n\u2202wT(j)\u2202w(i)\n\u2225\u2225\u2225\u2225\u2225 2\nF\n.\nCase 3: l + 1 = i > j\nIn the following, we first consider the first case, i.e. cross entropy and softmax activation, and bound\n\u2225\u2225\u2225\u2225 \u22022f(w,D)\n\u2202w(j)\u2202w(l+1)\n\u2225\u2225\u2225\u2225 2\nF\n= \u2225\u2225\u2225\u2225\u2225 \u2202(v \u2212 y)zT(l)\n\u2202w(j)\n\u2225\u2225\u2225\u2225\u2225 2\nF\n= \u2225\u2225\u2225\u2225\u2225[Ir\u0303lc\u0303ldl \u2297 (v \u2212 y)] \u2202zT(l)\n\u2202x(l)\n\u2202x(l) \u2202w(j) + [ z(l) \u2297 Idl+1 ] \u2202v \u2202z(l) \u2202z(l) \u2202x(l) \u2202x(l) \u2202w(j) \u2225\u2225\u2225\u2225\u2225 2\nF\n= \u2225\u2225\u2225\u2225u\u0303p ( [Ir\u0303lc\u0303ldl \u2297 (v \u2212 y)] + [ z(l) \u2297 Idl+1 ] diag (\u03c3\u20322(u))W(l+1) ) G(l) \u2202x(l)\n\u2202w(j)\n\u2225\u2225\u2225\u2225 2\nF\n,\nwhereG(l) is defined as\nG(l)= [ \u03c3\u20321 ( x(l) ) , \u03c3\u20321 ( x(l) ) , \u00b7 \u00b7 \u00b7 , \u03c3\u20321 ( x(l) ) \ufe38 \ufe37\ufe37 \ufe38\nrlcl columns\n] \u2208 Rrlcldl\u00d7rlcl .\nThus, we can further obtain \u2225\u2225\u2225\u2225 \u22022f(w,D)\n\u2202w(j)\u2202w(l+1)\n\u2225\u2225\u2225\u2225 2\nF\n\u2264 1 16p2\n\u2225\u2225[Ir\u0303lc\u0303ldl \u2297 (v \u2212 y)] + [ z(l) \u2297 Idl+1 ] diag (\u03c3\u20322(u))W(l+1) \u2225\u22252 F \u2225\u2225\u2225\u2225 \u2202x(l)\n\u2202w(j)\n\u2225\u2225\u2225\u2225 2\nF\n\u2264 2 16p2\n( \u2016Ir\u0303lc\u0303ldl \u2297 (v \u2212 y)\u20162F + \u2225\u2225[z(l) \u2297 Idl+1 ] diag (\u03c3\u20322(u))W(l+1) \u2225\u22252 F )\u2225\u2225\u2225\u2225 \u2202x(l)\n\u2202w(j)\n\u2225\u2225\u2225\u2225 2\nF\n\u00ac \u2264 2\n16p2\n( \u2016Ir\u0303lc\u0303ldl \u2297 (v \u2212 y)\u20162F + \u2225\u2225z(l) \u2297 [ diag (\u03c3\u20322(u))W(l+1) ]\u2225\u22252 F )\u2225\u2225\u2225\u2225 \u2202x(l)\n\u2202w(j)\n\u2225\u2225\u2225\u2225 2\nF\n \u2264 1\n8p2 r\u0303lc\u0303ldl\n( 2 + 1\n16 bl+1\n2 ) dlrlclr\u0303j\u22121c\u0303j\u22121dj\u22121(kj \u2212 sj + 1)2 l\u220f\ns=j+1\ndsbs 2(ks \u2212 ss + 1)2\n16p2\n= 2dj\u22121 p4bj 2dj r2l c 2 l d 2 l rj\u22121cj\u22121\n( 2 + 1\n16 bl+1\n2\n) l\u220f\ns=j\ndsbs 2(ks \u2212 ss + 1)2\n16p2\nwhere \u00ac holds since for an arbitrary vector u \u2208 Rk and an arbitrary matrixM \u2208 Rk\u00d7k, we have (u\u2297 Ik)M = u\u2297M ;  holds since we use Lemma 12 and the assumption that \u2016W(l+1)\u20162F \u2264 bl+12. Now we consider the least square loss and softmax activation function. In such a case, we can further obtain:\n\u2225\u2225\u2225\u2225 \u22022f(w,D)\n\u2202w(j)\u2202w(l+1)\n\u2225\u2225\u2225\u2225 2\nF\n= \u2225\u2225\u2225\u2225\u2225 \u2202(v \u2212 y)G (u)zT(l)\n\u2202w(j)\n\u2225\u2225\u2225\u2225\u2225 2\nF\n= \u2225\u2225\u2225\u2225\u2225[Ir\u0303lc\u0303ldl \u2297 (v \u2212 y)] \u2202zT(l)\n\u2202x(l)\n\u2202x(l) \u2202w(j) + [ z(l) \u2297 (v \u2212 y) ] \u2202vec (G (u)) \u2202u \u2202u \u2202z(l) \u2202z(l) \u2202x(l) \u2202x(l) \u2202w(j) + [ z(l) \u2297 Idl+1 ] \u2202v \u2202z(l) \u2202z(l) \u2202x(l) \u2202x(l) \u2202w(j) \u2225\u2225\u2225\u2225\u2225 2\nF\n= \u2225\u2225\u2225\u2225u\u0303p ( [Ir\u0303lc\u0303ldl \u2297 (v \u2212 y)] + [ z(l) \u2297 (v \u2212 y) ] Q (u)W(l+1) + [ z(l) \u2297 Idl+1 ] Q (u)G (u)W(l+1) ) G(l) \u2202x(l)\n\u2202w(j)\n\u2225\u2225\u2225\u2225 2\nF\n.\nThus, we can further obtain\n\u2225\u2225\u2225\u2225 \u22022f(w,D)\n\u2202w(j)\u2202w(l+1)\n\u2225\u2225\u2225\u2225 2\nF\n\u2264 1 16p2\n\u2225\u2225[Ir\u0303lc\u0303ldl \u2297 (v \u2212 y)] + [ z(l) \u2297 (v \u2212 y) ] Q (u)W(l+1) + [ z(l) \u2297 Idl+1 ] Q (u)G (u)W(l+1) \u2225\u22252 F \u2225\u2225\u2225\u2225 \u2202x(l)\n\u2202w(j)\n\u2225\u2225\u2225\u2225 2\nF\n\u2264 3 16p2\n( \u2016Ir\u0303lc\u0303ldl \u2297 (v \u2212 y)\u20162F + \u2225\u2225[z(l) \u2297 (v \u2212 y) ] Q (u)W(l+1) \u2225\u22252 F + \u2225\u2225[z(l) \u2297 Idl+1 ] Q (u)G (u)W(l+1) \u2225\u22252 F )\u2225\u2225\u2225\u2225 \u2202x(l)\n\u2202w(j)\n\u2225\u2225\u2225\u2225 2\nF\n\u00ac \u2264 3\n16p2 r\u0303lc\u0303ldl\n( 2 + 3\n100 bl+1\n2 ) dlrlclr\u0303j\u22121c\u0303j\u22121dj\u22121(kj \u2212 sj + 1)2 l\u220f\ns=j+1\ndsbs 2(ks \u2212 ss + 1)2\n16p2\n= 3dj\u22121 p4bj 2dj rj\u22121cj\u22121d 2 l r 2 l c 2 l\n( 2 + 27\n38 bl+1\n2 + 26\n16 \u00b7 38 bl+1 2\n) l\u220f\ns=j\ndsbs 2(ks \u2212 ss + 1)2\n16p2 ,\nwhere \u00ac holds since we use Lemma 12 and the fact that \u2016W(l+1)\u20162F \u2264 bl+12. Case 4: i < j = l + 1\nSimilar to the Case 2, we also can have\n\u2225\u2225\u2225\u2225\u2225 \u22022f(w,D)\n\u2202wT(i)\u2202w(j)\n\u2225\u2225\u2225\u2225\u2225 2\nF\n= \u2225\u2225\u2225\u2225\u2225 \u22022f(w,D)\n\u2202wT(j)\u2202w(i)\n\u2225\u2225\u2225\u2225\u2225 2\nF\n.\nSo in this case, we can just directly use the bound in case 3 to bound \u2225\u2225\u2225\u2225 \u22022f(w,D)\n\u2202wT (i) \u2202w(j)\n\u2225\u2225\u2225\u2225 2\nF\n.\nCase 5: i = j = l + 1\nIn the following, we first consider the first case, i.e. i = l + 1, and bound\n\u2225\u2225\u2225\u2225 \u22022f(w,D)\n\u2202w(l+1)\u2202w(l+1)\n\u2225\u2225\u2225\u2225 2\nF\n= \u2225\u2225\u2225\u2225\u2225 \u2202(v \u2212 y)zT(l) \u2202w(l+1) \u2225\u2225\u2225\u2225\u2225 2\nF\n= \u2225\u2225\u2225\u2225 [ z(l) \u2297 Idl+1\n] \u2202v \u2202w(l+1)\n\u2225\u2225\u2225\u2225 2\nF\n= \u2225\u2225\u2225 [ z(l) \u2297 Idl+1 ] G (u) [ z(l) \u2297 Idl+1 ]T\u2225\u2225\u2225 2\nF\n\u00ac = \u2225\u2225\u2225\u2225 [ z(l) ( z(l) \u2297G (u) )T ]T \u2225\u2225\u2225\u2225 2\nF\n\u2264 \u2225\u2225z(l)\u20164F \u2016G (u) \u2225\u22252 F \u2264 1 16 r\u03032l c\u0303 2 l d 2 l dl+1,\nwhere \u00ac holds since for an arbitrary vector u \u2208 Rk and an arbitrary matrixM \u2208 Rk\u00d7k, we have (u\u2297 Ik)M = u\u2297M . Now we can bound \u2225\u2225\u2225\u2202\n2f(w,D) \u2202w\u2202w\n\u2225\u2225\u2225 2\nF as follows:\n\u2225\u2225\u2225\u2225 \u22022f(w,D)\n\u2202w\u2202w\n\u2225\u2225\u2225\u2225 2\nF\n= \u2225\u2225\u2225\u2225 \u22022f(w,D)\n\u2202w(l+1)\u2202w(l+1)\n\u2225\u2225\u2225\u2225 2\nF\n+ 2 l\u2211\nj=1\n\u2225\u2225\u2225\u2225 \u22022f(w,D)\n\u2202w(j)\u2202w(l+1)\n\u2225\u2225\u2225\u2225 2\nF\n+ 2 l\u2211\nj=1\nl\u2211\ni=j\n\u2225\u2225\u2225\u2225 \u22022f(w,D)\n\u2202w(j)\u2202w(i)\n\u2225\u2225\u2225\u2225 2\nF\n\u2264O ( l2\nk1 4 max1\u2264i,j\u2264l\nr\u0303ic\u0303irjcj bl+1\n4\n16p2\n( rw 2\n8 \u221a 2p2\n)2l\u22122 l\u220f\ns=1\n(dsks 2)2\n)\n\u2264O  \u03d1bl+1\n2d20 b1 4d21 l2r20c 2 0\n[ l\u220f\ns=1\ndsbs 2(ks \u2212 ss + 1)2\n8 \u221a 2p2\n]2  .\nOn the other hand, if the activation functions \u03c31 and \u03c32 are respectively sigmoid function and softmax function, f(w,D) is infinitely differentiable. Also \u03c3(a), \u03c3\u2032(a), \u03c3\u2032\u2032(a) and \u03c3\u2032\u2032\u2032(a) are all bounded. This means that \u22073wf(w,D) exists. Also since inputD and the parameter w are bounded, we can always find a universal constant \u03bd such that\n\u2016\u22073wf(w,D)\u2016op = sup \u2016\u03bb\u20162\u22641\n\u2329 \u03bb\u2297 3 ,\u22073wf(w,D) \u232a = \u2211\ni,j,k\n[\u22073wf(w,D)]ijk\u03bbi\u03bbj\u03bbk \u2264 \u03bd < +\u221e.\nThe proof is completed.\nLemma 15. Suppose that the activation function \u03c31 is sigmoid and \u03c32 is softmax, and the loss function f(w,D) is squared loss. Suppose Assumption 1 on the input dataD holds. Then for any t > 0, the objective f(w,x) obeys\nP\n( 1\nn\nn\u2211\ni=1\n( f(w,D(i))\u2212E(f(w,D(i))) ) >t ) \u2264 2 exp ( \u22122nt 2\n\u03b12\n) ,\nwhere \u03b1 = 1.\nProof. Since the input D(i) (i = 1, \u00b7 \u00b7 \u00b7 , n) are independent from each other, then the output f(w,D(i)) (i = 1, \u00b7 \u00b7 \u00b7 , n) are also independent. Meanwhile, when the loss is the square loss, we can easily bound 0 \u2264 f(w,D(i)) = 12\u2016v \u2212 y\u201622 \u2264 1, since the value of entries in v belongs to [0, 1] and y is a one-hot vector label of v.\nBesides, for arbitrary random variable x, |x\u2212 Ex| \u2264 |x|. So by Hoeffding\u2019s inequality in Lemma 6, we have\nP\n( 1\nn\nn\u2211\ni=1\n( f(w,D(i))\u2212E(f(w,D(i))) ) >t ) \u2264 exp ( \u22122nt 2\n\u03b12\n) ,\nwhere \u03b1 = 1. This means that 1n \u2211n i=1 ( f(w,D(i))\u2212E(f(w,D(i))) ) has exponential tails.\nLemma 16. Suppose that the activation function \u03c31 is sigmoid and \u03c32 is softmax, and the loss function f(w,D) is squared loss. Suppose Assumption 1 on the input dataD holds. Then for any t > 0 and arbitrary unit vector \u03bb \u2208 Sd\u22121, the gradient \u2207f(w,x) obeys\nP\n( 1\nn\nn\u2211\ni=1\n(\u2329 \u03bb,\u2207wf(w,D(i))\u2212ED\u223cD\u2207wf(w,D(i)) \u232a) >t ) \u2264 exp ( \u2212 nt 2\n2\u03b22\n) .\nwhere \u03b2 , [ \u03d1r\u0303lc\u0303ldl + \u2211l i=1 \u03d1bl+1 2di\u22121 p2bi2di ri\u22121ci\u22121 \u220fl s=i dsbs 2(ks\u2212ss+1)2 16p2 ]1/2 in which \u03d1 = 1/8.\nProof. Since the inputD(i) (i = 1, \u00b7 \u00b7 \u00b7 , n) are independent from each other, then the output\u2207wf(w,D(i)) (i = 1, \u00b7 \u00b7 \u00b7 , n) are also independent. Furthermore, for arbitrary vector x, \u2016x\u2212Ex\u201622 \u2264 \u2016x\u201622. Hence, for an arbitrary unit vector \u03bb \u2208 Sd\u22121 where d = r\u0303lc\u0303ldldl+1 + \u2211l i=1 ki 2di\u22121di, we have\n\u3008\u03bb,\u2207wf(w,D(i))\u2212 ED\u223cD\u2207wf(w,D(i))\u3009 \u2264\u2016\u03bb\u20162\u2016\u2207wf(w,D(i))\u2212 ED\u223cD\u2207wf(w,D(i))\u20162\n\u2264\u2016\u03bb\u20162\u2016\u2207wf(w,D(i))\u20162 \u00ac \u2264 \u03b2,\nwhere \u00ac holds since \u2016\u03bb\u20162 = 1 (\u03bb \u2208 Sd\u22121) and by Lemma 11, we have \u2016\u2207wf(w,D(i))\u2016 \u2264 \u03b2 where \u03b2 ,[ \u03d1r\u0303lc\u0303ldl + \u2211l i=1 \u03d1bl+1 2di\u22121 p2bi2di ri\u22121ci\u22121 \u220fl s=i dsbs 2(ks\u2212ss+1)2 16p2 ]1/2 in which \u03d1 = 1/8.\nThus, we can use Hoeffding\u2019s inequality in Lemma 6 to bound\nP\n( 1\nn\nn\u2211\ni=1\n(\u2329 \u03bb,\u2207wf(w,D(i))\u2212ED\u223cD\u2207wf(w,D(i)) \u232a) >t ) \u2264 exp ( \u2212 nt 2\n2\u03b22\n) .\nThe proof is completed.\nLemma 17. Suppose that the activation function \u03c31 is sigmoid and \u03c32 is softmax, and the loss function f(w,D) is squared loss. Suppose that Assumption 1 on the input data D and the parameter w holds. Then for any t > 0 and arbitrary unit vector \u03bb \u2208 Sd\u22121, the Hessian\u22072f(w,D) obeys\nP\n( 1\nn\nn\u2211\ni=1\n(\u2329 \u03bb, (\u22072wf(w,D(i))\u2212 ED\u223cD\u22072wf(w,D(i)))\u03bb \u232a) > t ) \u2264 2 exp ( \u2212nt 2\n2\u03b32\n) .\nwhere \u03b3 = ( \u03d1bl+1\n2d20 b14d21 l2r20c 2 0 [\u220fl s=1 dsbs 2(ks\u2212ss+1)2 8 \u221a 2p2 ]2)1/2 .\nProof. Since the inputD(i) (i = 1, \u00b7 \u00b7 \u00b7 , n) are independent from each other, then the output\u2207wf(w,D(i)) (i = 1, \u00b7 \u00b7 \u00b7 , n) are also independent. On the other hand, for arbitrary random matrix X , \u2016X \u2212 EX\u20162F \u2264 \u2016X\u20162F . Thus, for an arbitrary unit vector \u03bb \u2208 Sd\u22121 where d = r\u0303lc\u0303ldldl+1 + \u2211l i=1 ki 2di\u22121di, we have\n\u3008\u03bb, (\u22072wf(w,D(i))\u2212 ED\u223cD\u22072wf(w,D(i)))\u03bb\u3009 \u2264\u2016\u03bb\u20162\u2016(\u22072wf(w,D(i))\u2212 ED\u223cD\u22072wf(w,D(i)))\u03bb\u20162 \u2264\u2016\u22072wf(w,D(i))\u2212 ED\u223cD\u22072wf(w,D(i))\u2016op\u2016\u03bb\u20162 \u2264\u2016\u22072wf(w,D(i))\u2212 ED\u223cD\u22072wf(w,D(i))\u2016F \u2264\u2016\u22072wf(w,D(i))\u2016F \u00ac \u2264\u03b3,\nwhere \u00ac holds since \u2016\u03bb\u20162 = 1 (\u03bb \u2208 Sd\u22121) and by Lemma 14, we have \u2016\u22072wf(w,D(i))\u2016 \u2264 \u03b3 where \u03b3 =( \u03d1bl+1\n2d20 b14d21 l2r20c 2 0 [\u220fl s=1 dsbs 2(ks\u2212ss+1)2 8 \u221a 2p2 ]2)1/2 .\nThus, we can use Hoeffding\u2019s inequality in Lemma 6 to bound\nP\n( 1\nn\nn\u2211\ni=1\n(\u2329 \u03bb, (\u22072wf(w,D(i))\u2212 ED\u223cD\u22072wf(w,D(i)))\u03bb \u232a) > t ) \u2264 exp ( \u2212nt 2\n2\u03b32\n) .\nThe proof is completed.\nLemma 18. Suppose that the activation function \u03c31 is sigmoid and \u03c32 is softmax, and the loss function f(w,D) is squared loss. Suppose that Assumption 1 on the input dataD and the parameter w holds. Then the empirical Hessian converges uniformly to the population Hessian in operator norm. Specifically, there exit two universal constants cv\u2032 and cv such that if n \u2265 cv\u2032 \u03bd 2\nd%\u03b52 [\u220fl s=1 dsbs 2(ks\u2212ss+1)2 8 \u221a 2p2 ]\u22121 , then with probability at least 1\u2212\nsup w\u2208\u2126\n\u2225\u2225\u2225\u22072Q\u0303n(w)\u2212\u22072Q(w) \u2225\u2225\u2225\nop \u2264 cv\u03b3\n\u221a 2d+ \u03b8%+ log ( 4 \u03b5 )\n2n ,\nholds with probability at least 1 \u2212 \u03b5, where d = r\u0303lc\u0303ldldl+1 + \u2211l i=1 ki\n2di\u22121di, \u03b8 = al+1(dl+1 + r\u0303lc\u0303ldl \u2212 2al+1 + 1) + \u2211l i=1 ai(ki 2di + di\u22121 \u2212 2ai + 1), % = \u2211l i=1log (\u221a dibi(ki\u2212si+1) 4p ) + log(bl+1) + log ( n 128p2 ) , and\n\u03b3 =\n( \u03d1bl+1\n2d20 b14d21 l2r20c 2 0 [\u220fl s=1 dsbs 2(ks\u2212ss+1)2 8 \u221a 2p2 ]2)1/2 .\nProof. Recall that the weight of each kernel and the feature maps has magnitude bound separately, i.e. wk(i) \u2208 Bki\n2di\u22121(rw) (i = 1, \u00b7 \u00b7 \u00b7 , l; k = 1, \u00b7 \u00b7 \u00b7 , di) and w(l+1) \u2208 Br\u0303lc\u0303ldldl+1(bl+1). Since W\u0303(i) = [vec(W 1(i)), vec(W 2(i)), \u00b7 \u00b7 \u00b7 , vec(W di\u22121(i) )] \u2208 Rk 2 i di\u00d7di\u22121 , we have \u2016W\u0303(i)\u2016F \u2264 dibi.\nSo here we assume W\u0303(i, ) is the dibi /(bl+1 + \u2211l i=1 dibi)-covering net of the matrix W\u0303(i) which is the set of all parameters in the i-th layer. Then by Lemma 7, we have the covering number\nn i \u2264\n( 9(bl+1 + \u2211l i=1 dibi) )ai(ki2di+di\u22121\u22122ai+1) ,\nsince the rank of W\u0303(i) obeys rank(W\u0303(i)) \u2264 ai for 1 \u2264 i \u2264 l. For the last layer, we also can construct an bl+1 /(bl+1 +\u2211l i=1 dibi)-covering net for the weight matrixW(l+1). Here we have\nn l+1 \u2264\n( 9(bl+1 + \u2211l i=1 dibi) )al+1(dl+1+r\u0303lc\u0303ldl\u22122al+1+1) ,\nsince the rank of W(l+1) obeys rank(W(l+1)) \u2264 al+1. Finally, we arrange them together to construct a set \u0398 and claim that there is always an -covering net w in \u0398 for any parameter w. Accordingly, we have\n|\u0398| \u2264 l+1\u220f\ni=1\nn i=\n( 9(bl+1 + \u2211l i=1 dibi) )al+1(dl+1+r\u0303lc\u0303ldl\u22122al+1+1)+\u2211li=1 ai(ki2di+di\u22121\u22122ai+1) = ( 9(bl+1 + \u2211l i=1 dibi) )\u03b8 ,\nwhere \u03b8 = al+1(dl+1 + r\u0303lc\u0303ldl \u2212 2al+1 + 1) + \u2211l i=1 ai(ki\n2di + di\u22121 \u2212 2ai + 1) which is the total freedom degree of the network. So we can always find a vector wkw \u2208 \u0398 such that \u2016w \u2212wkw\u20162 \u2264 . Now we use the decomposition strategy to bound our goal:\n\u2225\u2225\u2225\u22072Q\u0303n(w)\u2212\u22072Q(w) \u2225\u2225\u2225\nop\n= \u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\n\u22072f(w,D(i))\u2212 ED\u223cD(\u22072f(w,D)) \u2225\u2225\u2225\u2225\u2225\nop\n= \u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\n( \u22072f(w,D(i))\u2212\u22072f(wkw ,D(i)) ) + 1\nn\nn\u2211\ni=1\n\u22072f(wkw ,D(i))\u2212 E(\u22072f(wkw ,D))\n+ ED\u223cD(\u22072f(wkw ,D))\u2212 ED\u223cD(\u22072f(w,D)) \u2225\u2225\u2225\u2225\u2225\nop\n\u2264 \u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\n( \u22072f(w,D(i))\u2212\u22072f(wkw ,D(i)) )\u2225\u2225\u2225\u2225\u2225 op + \u2225\u2225\u2225\u2225\u2225 1 n n\u2211 i=1 \u22072f(wkw ,D(i))\u2212 ED\u223cD(\u22072f(wkw ,D)) \u2225\u2225\u2225\u2225\u2225 op\n+ \u2225\u2225\u2225\u2225\u2225ED\u223cD(\u2207 2f(wkw ,D))\u2212 ED\u223cD(\u22072f(w,D)) \u2225\u2225\u2225\u2225\u2225 op .\nHere we also define four events E0, E1, E2 and E3 as\nE0 = { sup w\u2208\u2126 \u2225\u2225\u2225\u22072Q\u0303n(w)\u2212\u22072Q(w) \u2225\u2225\u2225 op \u2265 t } ,\nE1 =    supw\u2208\u2126 \u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\n( \u22072f(w,D(i))\u2212\u22072f(wkw ,D(i)) )\u2225\u2225\u2225\u2225\u2225 op \u2265 t 3    ,\nE2 =    supwkw\u2208\u0398 \u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\n\u22072f(wkw ,D(i))\u2212 ED\u223cD(\u22072f(wkw ,D)) \u2225\u2225\u2225\u2225\u2225\nop \u2265 t 3\n   ,\nE3 = { sup w\u2208\u2126 \u2225\u2225ED\u223cD(\u22072f(wkw ,D))\u2212 ED\u223cD(\u22072f(w,D)) \u2225\u2225 op \u2265 t 3 } .\nAccordingly, we have P (E0) \u2264 P (E1) + P (E2) + P (E3) .\nSo we can respectively bound P (E1), P (E2) and P (E3) to bound P (E0).\nStep 1. Bound P (E1): We first bound P (E1) as follows:\nP (E1) =P ( sup w\u2208\u2126 \u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\n( \u22072f(w,D(i))\u2212\u22072f(wkw ,D(i)) )\u2225\u2225\u2225\u2225\u2225 2 \u2265 t 3 )\n\u00ac \u22643 t ED\u223cD ( sup w\u2208\u2126 \u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\n( \u22072f(w,D(i))\u2212\u22072f(wkw ,D(i)) )\u2225\u2225\u2225\u2225\u2225 2 )\n\u22643 t ED\u223cD ( sup w\u2208\u2126 \u2225\u2225\u22072f(w,D)\u2212\u22072f(wkw ,D) \u2225\u2225 2 )\n\u22643 t ED\u223cD ( sup w\u2208\u2126 \u2225\u2225 1 n \u2211n i=1 ( \u22072f(w,D(i))\u2212\u22072f(wkw ,D(i)) )\u2225\u2225 2 \u2016w \u2212wkw\u20162 sup w\u2208\u2126 \u2016w \u2212wkw\u20162 )\n \u22643\u03bd\nt ,\nwhere \u00ac holds since by Markov inequality and  holds because of Lemma 14. Therefore, we can set\nt \u2265 6\u03bd \u03b5 .\nThen we can bound P(E1): P(E1) \u2264 \u03b5\n2 .\nStep 2. Bound P (E2): By Lemma 3, we know that for any matrixX \u2208 Rd\u00d7d, its operator norm can be computed as\n\u2016X\u2016op \u2264 1\n1\u2212 2 sup\u03bb\u2208\u03bb |\u3008\u03bb,X\u03bb\u3009| .\nwhere \u03bb = {\u03bb1, . . . ,\u03bbkw} be an -covering net of Bd(1).\nLet \u03bb1/4 be the 14 -covering net of B d(1), where d = r\u0303lc\u0303ldldl+1 + \u2211l i=1 ki 2di\u22121di. Recall that we use \u0398 to denote the -net of wkw and we have |\u0398| \u2264 \u220fl+1 i=1 n i = ( 3(bl+1+ \u2211l i=1 dibi) )\u03b8 . Then we can bound P (E2) as follows:\nP (E2) =P ( sup\nwkw\u2208\u0398\n\u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\n\u22072f(wkw ,D(i))\u2212 ED\u223cD(\u22072f(wkw ,D)) \u2225\u2225\u2225\u2225\u2225\n2\n\u2265 t 3\n)\n\u2264P (\nsup wkw\u2208\u0398,\u03bb\u2208\u03bb1/4 2\n\u2223\u2223\u2223\u2223\u2223 \u2329 \u03bb, ( 1 n n\u2211\ni=1\n\u22072f(wkw ,D(i))\u2212 ED\u223cD ( \u22072f(wkw ,D) ) ) \u03bb \u232a\u2223\u2223\u2223\u2223\u2223 \u2265 t 3 )\n\u226412d ( 3(bl+1 + \u2211l i=1 dibi) )\u03b8 sup wkw\u2208\u0398,\u03bb\u2208\u03bb1/4 P (\u2223\u2223\u2223\u2223 1 n n\u2211\ni=1\n\u2329 \u03bb, ( \u22072f(wkw ,D(i))\u2212 ED\u223cD ( \u22072f(wkw ,D) )) \u03bb \u232a\u2223\u2223\u2223\u2223\u2265 t\n6\n)\n\u00ac \u226412d\n( 3(bl+1 + \u2211l i=1 dibi) )\u03b8 2 exp ( \u2212 nt 2\n72\u03b32\n) ,\nwhere \u00ac holds since by Lemma 17, we have\nP\n( 1\nn\nn\u2211\ni=1\n(\u2329 \u03bb, (\u22072wf(w,D(i))\u2212 ED\u223cD\u22072wf(w,D(i)))\u03bb \u232a) > t ) \u2264 exp ( \u2212nt 2\n2\u03b32\n) .\nwhere \u03b3 = ( \u03d1bl+1\n2d20 b14d21 l2r20c 2 0 [\u220fl s=1 dsbs 2(ks\u2212ss+1)2 8 \u221a 2p2 ]2)1/2 .\nThus, if we set\nt \u2265\n\u221a\u221a\u221a\u221a72\u03b32 ( d log(12) + \u03b8 log ( 3(bl+1+ \u2211l i=1 dibi) ) + log ( 4 \u03b5 ))\nn ,\nthen we have P (E2) \u2264 \u03b5\n2 .\nStep 3. Bound P (E3): We first bound P (E3) as follows:\nP (E3) =P (\nsup w\u2208\u2126\n\u2225\u2225ED\u223cD(\u22072f(wkw ,D))\u2212 ED\u223cD(\u22072f(w,D)) \u2225\u2225 2 \u2265 t\n3\n)\n\u2264P ( ED\u223cD sup\nw\u2208\u2126\n\u2225\u2225(\u22072f(wkw ,D)\u2212\u22072f(w,D) \u2225\u2225 2 \u2265 t\n3\n)\n\u2264P (\nsup w\u2208\u2126\n\u2223\u2223 1 n \u2211n i=1 ( \u22072f(w,D(i))\u2212\u22072f(wkw ,D(i)) )\u2223\u2223 \u2016w \u2212wkw\u20162 sup w\u2208\u2126 \u2016w \u2212wkw\u20162 \u2265 t 3 )\n\u00ac \u2264P ( \u03bd \u2265 t\n3\n) ,\nwhere \u00ac holds because of Lemma 14. We set enough small such that \u03bd < t/3 always holds. Then it yields P (E3) = 0.\nStep 4. Final result: To ensure P(E0) \u2264 \u03b5, we just set = 36(bl+1+ \u2211l i=1 dibi)\n\u03d12nbl+1\n[\u220fl s=1 dsbs 2(ks\u2212ss+1)2 8 \u221a 2p2 ]\u2212 12 . Note that\n6 \u03bd \u03b5 > 3 \u03bd. Thus we can obtain\nt \u2265 max   6\u03bd\n\u03b5 ,\n\u221a\u221a\u221a\u221a72\u03b32 ( d log(12) + \u03b8 log ( 3(bl+1+ \u2211l i=1 dibi) ) + log ( 4 \u03b5 ))\nn\n  .\nThus, if n \u2265 cv\u2032 \u03bd 2\nd%\u03b52 [\u220fl s=1 dsbs 2(ks\u2212ss+1)2 8 \u221a 2p2 ]\u22121 where cv\u2032 is a constant, there exists a universal constant cv such that\nsup w\u2208\u2126\n\u2225\u2225\u2225\u22072Q\u0303n(w)\u2212\u22072Q(w) \u2225\u2225\u2225\nop \u2264c\u0302v\u03b3\n\u221a\u221a\u221a\u221ad log(12) + \u03b8 (\u2211l i=1 log (\u221a dsbs(ks\u2212ss+1) 4p ) + log(bl+1) + log ( n 128p2 )) + log ( 4 \u03b5 )\nn\n=cv\u03b3\n\u221a 2d+ \u03b8%+ log ( 4 \u03b5 )\n2n\nholds with probability at least 1 \u2212 \u03b5, where d = r\u0303lc\u0303ldldl+1 + \u2211l i=1 ki\n2di\u22121di, \u03b8 = al+1(dl+1 + r\u0303lc\u0303ldl \u2212 2al+1 + 1) + \u2211l i=1 ai(ki 2di + di\u22121 \u2212 2ai + 1), % = \u2211l i=1log (\u221a dibi(ki\u2212si+1) 4p ) + log(bl+1) + log ( n 128p2 ) , and\n\u03b3 =\n( \u03d1bl+1\n2d20 b14d21 l2r20c 2 0 [\u220fl s=1 dsbs 2(ks\u2212ss+1)2 8 \u221a 2p2 ]2)1/2 . The proof is completed."}, {"heading": "D Proofs of Main Theorems", "text": ""}, {"heading": "D.1 Proof of Lemma 1", "text": "Proof. Recall that the weight of each kernel and the feature maps has magnitude bound separately, i.e. wk(i) \u2208 Bki\n2di\u22121(rw) (i = 1, \u00b7 \u00b7 \u00b7 , l; k = 1, \u00b7 \u00b7 \u00b7 , di) and w(l+1) \u2208 Br\u0303lc\u0303ldldl+1(bl+1). Since W\u0303(i) = [vec(W 1(i)), vec(W 2(i)), \u00b7 \u00b7 \u00b7 , vec(W di\u22121(i) )] \u2208 Rk 2 i di\u00d7di\u22121 , we have \u2016W\u0303(i)\u2016F \u2264 dibi.\nSo here we assume W\u0303(i, ) is the dibi /(bl+1 + \u2211l i=1 dibi)-covering net of the matrix W\u0303(i) which is the set of all parameters in the i-th layer. Then by Lemma 7, we have the covering number\nn i \u2264\n( 9(bl+1 + \u2211l i=1 dibi) )ai(ki2di+di\u22121\u22122ai+1) ,\nsince the rank of W\u0303(i) obeys rank(W\u0303(i)) \u2264 ai for 1 \u2264 i \u2264 l. For the last layer, we also can construct an bl+1 /(bl+1 +\u2211l i=1 dibi)-covering net for the weight matrixW(l+1). Here we have\nn l+1 \u2264\n( 9(bl+1 + \u2211l i=1 dibi) )al+1(dl+1+r\u0303lc\u0303ldl\u22122al+1+1) ,\nsince the rank of W(l+1) obeys rank(W(l+1)) \u2264 al+1. Finally, we arrange them together to construct a set \u0398 and claim that there is always an -covering net w in \u0398 for any parameter w. Accordingly, we have\n|\u0398| \u2264 l+1\u220f\ni=1\nn i=\n( 9(bl+1 + \u2211l i=1 dibi) )al+1(dl+1+r\u0303lc\u0303ldl\u22122al+1+1)+\u2211li=1 ai(ki2di+di\u22121\u22122ai+1) = ( 9(bl+1 + \u2211l i=1 dibi) )\u03b8 ,\nwhere \u03b8 = al+1(dl+1 + r\u0303lc\u0303ldl \u2212 2al+1 + 1) + \u2211l i=1 ai(ki\n2di + di\u22121 \u2212 2ai + 1) which is the total freedom degree of the network. So we can always find a vector wkw \u2208 \u0398 such that \u2016w \u2212wkw\u20162 \u2264 . Now we use the decomposition strategy to bound our goal:\n\u2223\u2223\u2223Q\u0303n(w)\u2212Q(w) \u2223\u2223\u2223= \u2223\u2223\u2223\u2223\u2223 1 n n\u2211\ni=1\nf(w,D(i))\u2212 ED\u223cD(f(w,D)) \u2223\u2223\u2223\u2223\u2223\n= \u2223\u2223\u2223\u2223\u2223 1 n n\u2211\ni=1\n( f(w,D(i))\u2212f(wkw ,D(i)) ) + 1\nn\nn\u2211\ni=1\nf(wkw ,D (i))\u2212Ef(wkw ,D)+ED\u223cDf(wkw ,D)\u2212ED\u223cDf(w,D) \u2223\u2223\u2223\u2223\u2223\n\u2264 \u2223\u2223\u2223\u2223\u2223 1 n n\u2211\ni=1\n( f(w,D(i))\u2212f(wkw ,D(i)) )\u2223\u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2223 1 n n\u2211\ni=1\nf(wkw ,D (i))\u2212ED\u223cDf(wkw ,D) \u2223\u2223\u2223\u2223\u2223\n+ \u2223\u2223\u2223\u2223\u2223ED\u223cDf(wkw ,D)\u2212ED\u223cDf(w,D) \u2223\u2223\u2223\u2223\u2223.\nThen, we define four events E0, E1, E2 and E3 as\nE0 = { sup w\u2208\u2126 \u2223\u2223\u2223Q\u0303n(w)\u2212Q(w) \u2223\u2223\u2223 \u2265 t } ,\nE1 = { sup w\u2208\u2126 \u2223\u2223\u2223\u2223\u2223 1 n n\u2211\ni=1\n( f(w,D(i))\u2212 f(wkw ,x(i)) )\u2223\u2223\u2223\u2223\u2223 \u2265 t 3 } ,\nE2 =\n{ sup\nwkw\u2208\u0398\n\u2223\u2223\u2223\u2223\u2223 1 n n\u2211\ni=1\nf(wkw ,D (i))\u2212ED\u223cD(f(wkw ,D)) \u2223\u2223\u2223\u2223\u2223\u2265 t 3 } ,\nE3 = { sup w\u2208\u2126 \u2223\u2223\u2223\u2223\u2223ED\u223cD(f(wkw ,D))\u2212ED\u223cD(f(w,D)) \u2223\u2223\u2223\u2223\u2223\u2265 t 3 } .\nAccordingly, we have P (E0) \u2264 P (E1) + P (E2) + P (E3) .\nSo we can respectively bound P (E1), P (E2) and P (E3) to bound P (E0).\nStep 1. Bound P (E1): We first bound P (E1) as follows:\nP (E1) =P ( sup w\u2208\u2126 \u2223\u2223\u2223\u2223\u2223 1 n n\u2211\ni=1\n( f(w,D(i))\u2212 f(wkw ,D(i)) )\u2223\u2223\u2223\u2223\u2223 \u2265 t 3 )\n\u00ac \u22643 t ED\u223cD ( sup w\u2208\u2126 \u2223\u2223\u2223\u2223\u2223 1 n n\u2211\ni=1\n( f(w,D(i))\u2212 f(wkw ,D(i)) )\u2223\u2223\u2223\u2223\u2223 )\n\u22643 t ED\u223cD ( sup w\u2208\u2126 \u2223\u2223 1 n \u2211n i=1 ( f(w,D(i))\u2212 f(wkw ,D(i)) )\u2223\u2223 \u2016w \u2212wkw\u20162 sup w\u2208\u2126 \u2016w \u2212wkw\u20162 )\n\u22643 t ED\u223cD ( sup w\u2208\u2126 \u2225\u2225\u2225\u2207Q\u0303n(w,D) \u2225\u2225\u2225 2 ) ,\nwhere \u00ac holds since by Markov inequality, we have that for an arbitrary nonnegative random variable x, then\nP(x \u2265 t) \u2264 E(x) t .\nNow we only need to bound ED\u223cD ( supw\u2208\u2126 \u2225\u2225\u2225\u2207Q\u0303n(w,D) \u2225\u2225\u2225\n2\n) . Therefore, by Lemma 11, we have\nED\u223cD (\nsup w\u2208\u2126\n\u2225\u2225\u2225\u2207Q\u0303n(w,D) \u2225\u2225\u2225\n2\n) =ED\u223cD ( sup w\u2208\u2126 \u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\n\u2207f(w,D(i)) \u2225\u2225\u2225\u2225\u2225\n2\n) \u2264ED\u223cD ( sup w\u2208\u2126 \u2016\u2207f(w,D)\u20162 ) \u2264\u03b2.\nwhere \u03b2 , [ \u03d1r\u0303lc\u0303ldl + \u2211l i=1 \u03d1bl+1 2di\u22121 p2bi2di ri\u22121ci\u22121 \u220fl s=i dsbs 2(ks\u2212ss+1)2 16p2 ]1/2 in which \u03d1 = 1/8. Therefore, we have\nP (E1) \u2264 3 \u03b2\nt .\nWe further let\nt \u2265 6 \u03b2 \u03b5 .\nThen we can bound P(E1): P(E1) \u2264 \u03b5\n2 .\nStep 2. Bound P (E2): Recall that we use \u0398 to denote the index of wkw and we have |\u0398| \u2264 \u220fl+1 i=1 n\ni =( 9(bl+1+ \u2211l i=1 dibi)\n)\u03b8 . We can bound P (E2) as follows:\nP (E2) =P ( sup\nwkw\u2208\u0398\n\u2223\u2223\u2223\u2223\u2223 1 n n\u2211\ni=1\nf(wkw ,D (i))\u2212 ED\u223cD(f(wkw ,D)) \u2223\u2223\u2223\u2223\u2223 \u2265 t 3 )\n\u2264 ( 9(bl+1 + \u2211l i=1 dibi) )\u03b8 sup\nwkw\u2208\u0398 P\n(\u2223\u2223\u2223 1 n n\u2211\ni=1\nf(wkw ,D (i))\u2212 ED\u223cD(f(wkw ,D)) \u2223\u2223\u2223 \u2265 t 3\n)\n\u00ac \u2264 ( 9(bl+1 + \u2211l i=1 dibi) )\u03b8 2 exp ( \u22122nt 2\n\u03b12\n) ,\nwhere \u00ac holds because in Lemma 15, we have\nP\n( 1\nn\nn\u2211\ni=1\n( f(w,D(i))\u2212E(f(w,D(i))) ) >t ) \u2264 exp ( \u22122nt 2\n\u03b12\n) ,\nwhere \u03b1 = 1. Thus, if we set\nt \u2265\n\u221a\u221a\u221a\u221a\u03b12 ( \u03b8 log ( 9(bl+1+ \u2211l i=1 dibi) ) + log ( 4 \u03b5 ))\n2n ,\nthen we have P (E2) \u2264 \u03b5\n2 .\nStep 3. Bound P (E3): We first bound P (E3) as follows:\nP (E3) =P (\nsup w\u2208\u2126 \u2016ED\u223cD(f(wkw ,D))\u2212 ED\u223cD(f(w,D))\u20162 \u2265\nt\n3\n)\n=P (\nsup w\u2208\u2126 \u2016ED\u223cD (f(wkw ,D)\u2212 f(w,D)\u20162) \u2016w \u2212wkw\u20162 sup w\u2208\u2126 \u2016w \u2212wkw\u20162 \u2265 t 3\n)\n\u2264P ( ED\u223cD sup\nw\u2208\u2126 \u2016\u2207Qw(w,D)\u20162 \u2265\nt\n3\n)\n\u00ac \u2264P ( \u03b2 \u2265 t\n3\n) ,\nwhere \u00ac holds since we utilize Lemma 11. We set enough small such that \u03b2 < t/3 always holds. Then it yields P (E3) = 0.\nStep 4. Final result: To ensure P(E0) \u2264 \u03b5, we just set = 18p 2(bl+1+\n\u2211l i=1 dibi)\n\u03d12nbl+1\n[\u220fl s=1 dsbs 2(ks\u2212ss+1)2 16p2 ]\u2212 12 . Note that\n6 \u03b2 \u03b5 > 3 \u03b2 due to \u03b5 \u2264 1. Thus we can obtain\nt\u2265max   6 \u03b2\n\u03b5 ,\n\u221a\u221a\u221a\u221a\u03b12 ( \u03b8 log ( 9(bl+1+ \u2211l i=1 dibi) ) + log ( 4 \u03b5 ))\n2n\n  .\nBy comparing the values of \u03b1, we can observe that if n \u2265 cf \u2032 l 2(bl+1+\n\u2211l i=1 dibi) 2 maxi \u221a rici\n\u03b8%\u03b52 where cf \u2032 is a constant, there exists such a universal constant cf such that\nsup w\u2208\u2126\n\u2223\u2223\u2223Q\u0303n(w)\u2212Q(w) \u2223\u2223\u2223\u2264\u03b1\n\u221a\u221a\u221a\u221a\u03b8 (\u2211l i=1 log (\u221a dsbs(ks\u2212ss+1) 4p ) + log(bl+1)+log ( n 128p2 )) +log ( 4 \u03b5 )\n2n =\n\u221a \u03b8%+log ( 4 \u03b5 )\n2n\nholds with probability at least 1 \u2212 \u03b5, where \u03b8 = al+1(dl+1 + r\u0303lc\u0303ldl \u2212 2al+1 + 1) + \u2211l i=1 ai(ki\n2di + di\u22121 \u2212 2ai + 1), % = \u2211l i=1log (\u221a dibi(ki\u2212si+1) 4p ) + log(bl+1) + log ( n 128p2 ) , and \u03b1 = 1. The proof is completed."}, {"heading": "D.2 Proof of Theorem 1", "text": "Proof. By Lemma 1 in the manuscript, we know that if n \u2265 cf \u2032 l2(bl+1 + \u2211l i=1 dibi) 2 maxi \u221a rici/(\u03b8%\u03b5\n2) where cf \u2032 is a universal constant, then with probability at least 1\u2212 \u03b5, we have\nsup w\u2208\u2126\n\u2223\u2223\u2223Q\u0303n(w)\u2212Q(w) \u2223\u2223\u2223 \u2264\n\u221a \u03b8%+ log ( 4 \u03b5 )\n2n ,\nwhere the total freedom degree \u03b8 of the network is \u03b8 = al+1(dl+1 + r\u0303lc\u0303ldl + 1) + \u2211l i=1 ai(ki 2di\u22121 + di + 1) and % = \u2211l i=1log (\u221a dibi(ki\u2212si+1) 4p ) + log(bl+1) + log ( n 128p2 ) .\nThus based on such a result, we can derive the following generalization bound:\nES\u223cD \u2223\u2223\u2223EA(Q(w\u0303)\u2212 Q\u0303n(w\u0303)) \u2223\u2223\u2223 \u2264 ES\u223cD (\nsup w\u2208\u2126 \u2223\u2223\u2223Q\u0303n(w)\u2212Q(w) \u2223\u2223\u2223 ) \u2264 sup w\u2208\u2126 \u2223\u2223\u2223Q\u0303n(w)\u2212Q(w) \u2223\u2223\u2223 \u2264\n\u221a \u03b8%+ log ( 4 \u03b5 )\n2n .\nThus, the conclusion holds. The proof is completed."}, {"heading": "D.3 Proof of Theorem 2", "text": "Proof. Recall that the weight of each kernel and the feature maps has magnitude bound separately, i.e. wk(i) \u2208 Bki\n2di\u22121(rw) (i = 1, \u00b7 \u00b7 \u00b7 , l; k = 1, \u00b7 \u00b7 \u00b7 , di) and w(l+1) \u2208 Br\u0303lc\u0303ldldl+1(bl+1). Since W\u0303(i) = [vec(W 1(i)), vec(W 2(i)), \u00b7 \u00b7 \u00b7 , vec(W di\u22121(i) )] \u2208 Rk 2 i di\u22121\u00d7di , we have \u2016W\u0303(i)\u2016F \u2264 dibi. So here we assume W\u0303(i, ) is the dibi /(bl+1 + \u2211l i=1 dibi)-covering net of the matrix W\u0303(i) which is the set of all parameters in the i-th layer. Then by Lemma 7, we have the covering number\nn i \u2264\n( 9(bl+1 + \u2211l i=1 dibi) )ai(ki2di\u22121+di\u22122ai+1) ,\nsince the rank of W\u0303(i) obeys rank(W\u0303(i)) \u2264 ai for 1 \u2264 i \u2264 l. For the last layer, we also can construct an bl+1 /(bl+1 +\u2211l i=1 dibi)-covering net for the weight matrixW(l+1). Here we have\nn l+1 \u2264\n( 9(bl+1 + \u2211l i=1 dibi) )al+1(dl+1+r\u0303lc\u0303ldl\u22122al+1+1) ,\nsince the rank of W(l+1) obeys rank(W(l+1)) \u2264 al+1. Finally, we arrange them together to construct a set \u0398 and claim that there is always an -covering net w in \u0398 for any parameter w. Accordingly, we have\n|\u0398| \u2264 l+1\u220f\ni=1\nn i=\n( 9(bl+1 + \u2211l i=1 dibi) )al+1(dl+1+r\u0303lc\u0303ldl\u22122al+1+1)+\u2211li=1 ai(ki2di\u22121+di\u22122ai+1) = ( 9(bl+1 + \u2211l i=1 dibi) )\u03b8 ,\nwhere \u03b8 = al+1(dl+1 + r\u0303lc\u0303ldl \u2212 2al+1 + 1) + \u2211l i=1 ai(ki\n2di\u22121 + di \u2212 2ai + 1) which is the total freedom degree of the network. So we can always find a vector wkw \u2208 \u0398 such that \u2016w \u2212 wkw\u20162 \u2264 . Accordingly, we can decompose\u2225\u2225\u2225\u2207Q\u0303n(w)\u2212\u2207Q(w)\n\u2225\u2225\u2225 2 as\n\u2225\u2225\u2225\u2207Q\u0303n(w)\u2212\u2207Q(w) \u2225\u2225\u2225\n2\n= \u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\n\u2207f(w,D(i))\u2212 ED\u223cD(\u2207f(w,D)) \u2225\u2225\u2225\u2225\u2225\n2\n= \u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\n( \u2207f(w,D(i))\u2212\u2207f(wkw ,D(i)) ) + 1\nn\nn\u2211\ni=1\n\u2207f(wkw ,D(i))\u2212 ED\u223cD(\u2207f(wkw ,D))\n+ ED\u223cD(\u2207f(wkw ,D))\u2212 ED\u223cD(\u2207f(w,D)) \u2225\u2225\u2225\u2225\u2225\n2 \u2264 \u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\n( \u2207f(w,D(i))\u2212\u2207f(wkw ,D(i)) )\u2225\u2225\u2225\u2225\u2225 2 + \u2225\u2225\u2225\u2225\u2225 1 n n\u2211 i=1 \u2207f(wkw ,D(i))\u2212 ED\u223cD(\u2207f(wkw ,D)) \u2225\u2225\u2225\u2225\u2225 2\n+ \u2225\u2225\u2225\u2225\u2225ED\u223cD(\u2207f(wkw ,D))\u2212 ED\u223cD(\u2207f(w,D)) \u2225\u2225\u2225\u2225\u2225\n2\n.\nHere we also define four events E0, E1, E2 and E3 as\nE0 = { sup w\u2208\u2126 \u2225\u2225\u2225\u2207Q\u0303n(w)\u2212\u2207Q(w) \u2225\u2225\u2225 2 \u2265 t } ,\nE1 = { sup w\u2208\u2126 \u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\n( \u2207f(w,D(i))\u2212\u2207f(wkw ,D(i)) )\u2225\u2225\u2225\u2225\u2225 2 \u2265 t 3 } ,\nE2 =\n{ sup\nwkw\u2208\u0398\n\u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\n\u2207f(wkw ,D(i))\u2212 ED\u223cD(\u2207f(wkw ,D)) \u2225\u2225\u2225\u2225\u2225\n2\n\u2265 t 3\n} ,\nE3 = { sup w\u2208\u2126 \u2225\u2225\u2225\u2225\u2225ED\u223cD(\u2207f(wkw ,D))\u2212 ED\u223cD(\u2207f(w,D)) \u2225\u2225\u2225\u2225\u2225\n2\n\u2265 t 3\n} .\nAccordingly, we have P (E0) \u2264 P (E1) + P (E2) + P (E3) .\nSo we can respectively bound P (E1), P (E2) and P (E3) to bound P (E0).\nStep 1. Bound P (E1): We first bound P (E1) as follows:\nP (E1) =P ( sup w\u2208\u2126 \u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\n( \u2207f(w,D(i))\u2212\u2207f(wkw ,D(i)) )\u2225\u2225\u2225\u2225\u2225 2 \u2265 t 3 )\n\u00ac \u22643 t ED\u223cD ( sup w\u2208\u2126 \u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\n( \u2207f(w,D(i))\u2212\u2207f(wkw ,D(i)) )\u2225\u2225\u2225\u2225\u2225 2 )\n\u22643 t ED\u223cD ( sup w\u2208\u2126 \u2225\u2225 1 n \u2211n i=1 ( \u2207f(w,D(i))\u2212\u2207f(wkw ,D(i)) )\u2225\u2225 2 \u2016w \u2212wkw\u20162 sup w\u2208\u2126 \u2016w \u2212wkw\u20162 )\n\u22643 t ED\u223cD ( sup w\u2208\u2126 \u2225\u2225\u2225\u22072Q\u0303n(w,D) \u2225\u2225\u2225 2 ) ,\nwhere \u00ac holds since by Markov inequality, we have that for an arbitrary nonnegative random variable x, then P(x \u2265 t) \u2264 E(x) t . Now we only need to bound ED\u223cD ( supw\u2208\u2126 \u2225\u2225\u2225\u22072Q\u0303n(w,D) \u2225\u2225\u2225\n2\n) . Here we utilize Lemma 14 to achieve this goal:\nED\u223cD (\nsup w\u2208\u2126\n\u2225\u2225\u2225\u22072Q\u0303n(w,D) \u2225\u2225\u2225\n2\n) \u2264 ED\u223cD ( sup w\u2208\u2126 \u2225\u2225\u22072f(w,D)\u2212\u22072f(w\u2217,D) \u2225\u2225 2 ) \u2264 \u03b3.\nwhere \u03b3 = ( \u03d1bl+1\n2d20 b14d21 l2r20c 2 0 [\u220fl s=1 dsbs 2(ks\u2212ss+1)2 8 \u221a 2p2 ]2)1/2 . Therefore, we have\nP (E1) \u2264 3\u03b3\nt .\nWe further let t \u2265 6\u03b3\n\u03b5 .\nThen we can bound P(E1): P(E1) \u2264 \u03b5\n2 .\nStep 2. Bound P (E2): By Lemma 2, we know that for any vector x \u2208 Rd, its `2-norm can be computed as\n\u2016x\u20162 \u2264 1\n1\u2212 sup\u03bb\u2208\u03bb \u3008\u03bb,x\u3009 .\nwhere \u03bb = {\u03bb1, . . . ,\u03bbkw} be an -covering net of Bd(1).\nLet \u03bb be the 12 -covering net of B d(1), where d = r\u0303lc\u0303ldldl+1 + \u2211l i=1 ki 2di\u22121di. Recall that we use \u0398 to denote the index of wkw so that \u2016w \u2212wkw\u2016 \u2264 . Besides, |\u0398| \u2264 \u220fl+1 i=1 n i = ( 3(bl+1+ \u2211l i=1 dibi) )\u03b8 . Then we can bound P (E2) as follows:\nP (E2) =P ( sup\nwkw\u2208\u0398\n\u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\n\u2207f(wkw ,D(i))\u2212 ED\u223cD(\u2207f(wkw ,D)) \u2225\u2225\u2225\u2225\u2225\n2\n\u2265 t 3\n)\n=P ( sup\nwkw\u2208\u0398,\u03bb\u2208\u03bb1/2 2\n\u2329 \u03bb, 1\nn\nn\u2211\ni=1\n\u2207f(wkw ,D(i))\u2212 ED\u223cD (\u2207f(wkw ,D)) \u232a \u2265 t\n3\n)\n\u22646d ( 9(bl+1 + \u2211l i=1 dibi) )\u03b8 sup wkw\u2208\u0398,\u03bb\u2208\u03bb1/2 P ( 1 n n\u2211\ni=1\n\u2329 \u03bb,\u2207f(wkw ,D(i))\u2212 ED\u223cD (\u2207f(wkw ,D)) \u232a \u2265 t\n6\n)\n\u00ac \u22646d\n( 9(bl+1 + \u2211l i=1 dibi) )\u03b8 2 exp ( \u2212 nt 2\n72\u03b22\n) ,\nwhere \u00ac holds since by Lemma 16, we have\nP\n( 1\nn\nn\u2211\ni=1\n(\u2329 \u03bb,\u2207wf(w,D(i))\u2212ED\u223cD\u2207wf(w,D(i)) \u232a) >t ) \u2264 exp ( \u2212 nt 2\n2\u03b22\n) .\nwhere \u03b2 , [ \u03d1r\u0303lc\u0303ldl + \u2211l i=1 \u03d1bl+1 2di\u22121 p2bi2di ri\u22121ci\u22121 \u220fl s=i dsbs 2(ks\u2212ss+1)2 16p2 ]1/2 in which \u03d1 = 1/8.\nThus, if we set\nt\u2265\n\u221a\u221a\u221a\u221a72\u03b22 ( d log(6) + \u03b8 log ( 9(bl+1+ \u2211l i=1 dibi) ) + log ( 4 \u03b5 ))\nn ,\nthen we have P (E2) \u2264 \u03b5\n2 .\nStep 3. Bound P (E3): We first bound P (E3) as follows:\nP (E3) =P (\nsup w\u2208\u2126 \u2016E(\u2207f(wkw ,x))\u2212 ED\u223cD(\u2207f(w,x))\u20162 \u2265\nt\n3\n)\n=P (\nsup w\u2208\u2126 \u2016ED\u223cD (\u2207f(wkw ,x)\u2212\u2207f(w,x)\u20162) \u2016w \u2212wkw\u20162 sup w\u2208\u2126 \u2016w \u2212wkw\u20162 \u2265 t 3\n)\n\u2264P ( ED\u223cD sup\nw\u2208\u2126\n\u2225\u2225\u2225\u22072Q\u0303n(w,x) \u2225\u2225\u2225\n2 \u2265 t 3\n)\n\u2264P ( \u03b3 \u2265 t\n3\n) .\nWe set enough small such that \u03b3 < t/3 always holds. Then it yields P (E3) = 0.\nStep 4. Final result: Note that 6\u03b2 \u03b5 \u2265 3\u03b2 . Finally, to ensure P(E0) \u2264 \u03b5, we just set = 18p2(bl+1+ \u2211l i=1 dibi)\n\u03d12nbl+1\n[\u220fl s=i dsbs 2(ks\u2212ss+1)2 16p2 ]\u2212 12 .\nt \u2265 max   6\u03b3\n\u03b5 ,\n\u221a\u221a\u221a\u221a72\u03b22 ( d log(6) + \u03b8 log ( 9(bl+1+ \u2211l i=1 dibi) ) + log ( 4 \u03b5 ))\nn\n  .\nBy comparing the values of \u03b2 and \u03b3, we have if n \u2265 cg\u2032 l 2bl+1\n2(bl+1+ \u2211l i=1 dibi) 2(r0c0d0) 4\nd40b1 8(d log(6)+\u03b8%)\u03b52 maxi(rici) where cg\u2032 is a universal constant, then there exists a universal constant cg such that\nsup w\u2208\u2126\n\u2225\u2225\u2225\u2207wQ\u0303n(w)\u2212\u2207wQ(w) \u2225\u2225\u2225\n2 \u2264cg\u03b2\n\u221a\u221a\u221a\u221ad+ 1log(6)\u03b8 [(\u2211l i=1 log (\u221a dsbs(ks\u2212ss+1) 4p ) +log(bl+1)+log ( n 128p2 )) +log ( 4 \u03b5 )]\nn\n\u2264cg\u03b2\n\u221a d+ 12\u03b8%+ 1 2 log ( 4 \u03b5 )\nn ,\nholds with probability at least 1 \u2212 \u03b5, where d = r\u0303lc\u0303ldldl+1 + \u2211l i=1 ki 2di\u22121di, \u03b8 = al+1(dl+1 + r\u0303lc\u0303ldl + 1) + \u2211l i=1 ai(ki 2di\u22121 + di + 1), % = \u2211l i=1log (\u221a dibi(ki\u2212si+1) 4p ) + log(bl+1) + log ( n 128p2 ) , and \u03b2 , [ \u03d1r\u0303lc\u0303ldl + \u2211l i=1 \u03d1bl+1 2di\u22121 p2bi2di ri\u22121ci\u22121 \u220fl s=i dsbs 2(ks\u2212ss+1)2 16p2 ]1/2 in which \u03d1 = 1/8. The proof is completed."}, {"heading": "D.4 Proof of Corollary 1", "text": "Proof. By Theorem 2, we know that there exist universal constants cg\u2032 and cg such that if n \u2265 cg\u2032 l2bl+1 2(bl+1+ \u2211l i=1 dibi) 2(r0c0d0) 4\nd40b1 8(d log(6)+\u03b8%)\u03b52 maxi(rici)\n, then\nsup w\u2208\u2126\n\u2225\u2225\u2225\u2207wQ\u0303n(w)\u2212\u2207wQ(w) \u2225\u2225\u2225\n2 \u2264 cg\u03b2\n\u221a 2d+ \u03b8%+ log ( 4 \u03b5 )\n2n\nholds with probability at least 1 \u2212 \u03b5, where % is provided in Lemma 1. Here \u03b2 and d are defined as \u03b2 =[ rlcldl 8p2 + \u2211l i=1 bl+1 2di\u22121 8p2bi2di ri\u22121ci\u22121 \u220fl j=i djbj 2(kj\u2212sj+1)2 16p2 ]1/2 and d = r\u0303lc\u0303ldldl+1 + \u2211l i=1 ki 2di\u22121di, respectively.\nSo based on such a result, we can derive that if n \u2265 c2g(2d+ \u03b8%+ log(4/\u03b5))\u03b22/(2 ), then we have\n\u2016\u2207Q(w\u0303)\u20162 \u2264 \u2225\u2225\u2225\u2207wQ\u0303n(w\u0303) \u2225\u2225\u2225 2 + \u2225\u2225\u2225\u2207wQ\u0303n(w\u0303)\u2212\u2207wQ(w\u0303) \u2225\u2225\u2225 2 \u2264 \u221a + cg\u03b2\n\u221a 2d+ \u03b8%+ log ( 4 \u03b5 )\n2n \u2264 2\u221a .\nThus, we have \u2016\u2207Q(w\u0303)\u201622 \u2264 4 , which means that w\u0303 is a 4 -approximate stationary point in population risk with probability at least 1\u2212 \u03b5. The proof is completed."}, {"heading": "D.5 Proof of Theorem 3", "text": "Proof. Suppose that {w(1),w(2), \u00b7 \u00b7 \u00b7 ,w(m)} are the non-degenerate critical points ofQ(w). So for any w(k), it obeys\ninf i\n\u2223\u2223\u03bbki ( \u22072Q(w(k)) )\u2223\u2223 \u2265 \u03b6,\nwhere \u03bbki ( \u22072Q(w(k)) ) denotes the i-th eigenvalue of the Hessian \u22072Q(w(k)) and \u03b6 is a constant. We further define a\nset D = {w \u2208 Rd | \u2016\u2207Q(w)\u20162 \u2264 and infi |\u03bbi ( \u22072Q(w(k)) ) | \u2265 \u03b6}. According to Lemma 5, D = \u222a\u221ek=1Dk where each Dk is a disjoint component with w(k) \u2208 Dk for k \u2264 m and Dk does not contain any critical point ofQ(w) for k \u2265 m+ 1. On the other hand, by the continuity of\u2207Q(w), it yields \u2016\u2207Q(w)\u20162 = for w \u2208 \u2202Dk. Notice, we set the value of blow which is actually a function related to n.\nThen by utilizing Theorem 2, we let sample number n sufficient large such that\nsup w\u2208\u2126\n\u2225\u2225\u2225\u2207Q\u0303n(w)\u2212\u2207Q(w) \u2225\u2225\u2225\n2 \u2264 2\nholds with probability at least 1\u2212 \u03b5, where is defined as\n2 , cg\u03b2\n\u221a\u221a\u221a\u221ad log(6) + \u03b8 (\u2211l i=1 log (\u221a dsbs(ks\u2212ss+1) 4p ) + log(bl+1) + log ( n 128p2 )) + log ( 4 \u03b5 )\nn .\nThis further gives that for arbitrary w \u2208 Dk, we have\ninf w\u2208Dk\n\u2225\u2225\u2225t\u2207Q\u0303n(w) + (1\u2212 t)\u2207Q(w) \u2225\u2225\u2225\n2 = inf w\u2208Dk\n\u2225\u2225\u2225t ( \u2207Q\u0303n(w)\u2212\u2207Q(w) ) +\u2207Q(w) \u2225\u2225\u2225 2\n\u2265 inf w\u2208Dk \u2016\u2207Q(w)\u20162 \u2212 sup w\u2208Dk\nt \u2225\u2225\u2225\u2207Q\u0303n(w)\u2212\u2207Q(w) \u2225\u2225\u2225 2\n\u2265 2 . (6)\nSimilarly, by utilizing Lemma 18, let n be sufficient large such that\nsup w\u2208\u2126\n\u2225\u2225\u2225\u22072Q\u0303n(w)\u2212\u22072Q(w) \u2225\u2225\u2225\nop \u2264 \u03b6 2\nholds with probability at least 1\u2212 \u03b5, where \u03b6 satisfies\n\u03b6 2 \u2265 cv\u03b3\n\u221a d+ \u03b8%+ log ( 4 \u03b5 )\nn .\nAssume that b \u2208 Rd is a vector and satisfies bT b = 1. In this case, we can bound \u03bbki ( \u22072Q\u0303n(w) ) for arbitrary w \u2208 Dk as follows:\ninf w\u2208Dk\n\u2223\u2223\u2223\u03bbki ( \u22072Q\u0303n(w) )\u2223\u2223\u2223 = inf w\u2208Dk min bT b=1 \u2223\u2223\u2223bT\u22072Q\u0303n(w)b \u2223\u2223\u2223\n= inf w\u2208Dk min bT b=1\n\u2223\u2223\u2223bT ( \u22072Q\u0303n(w)\u2212\u22072Q(w) ) b+ bT\u22072Q(w)b \u2223\u2223\u2223\n\u2265 inf w\u2208Dk min bT b=1\n\u2223\u2223bT\u22072Q(w)b \u2223\u2223\u2212 min\nbT b=1\n\u2223\u2223\u2223bT ( \u22072Q\u0303n(w)\u2212\u22072Q(w) ) b \u2223\u2223\u2223\n\u2265 inf w\u2208Dk min bT b=1\n\u2223\u2223bT\u22072Q(w)b \u2223\u2223\u2212 max\nbT b=1\n\u2223\u2223\u2223bT ( \u22072Q\u0303n(w)\u2212\u22072Q(w) ) b \u2223\u2223\u2223\n= inf w\u2208Dk inf i |\u03bbki ( \u22072f(w(k),x) ) | \u2212 \u2225\u2225\u2225\u22072Q\u0303n(w)\u2212\u22072Q(w) \u2225\u2225\u2225 op\n\u2265\u03b6 2 .\n(7)\nThis means that in each set Dk,\u22072Q\u0303n(w) has no zero eigenvalues. Then, combine this and Eqn. (6), by Lemma 4 we know that if the population riskQ(w) has no critical point in Dk, then the empirical risk Q\u0303n(w) has also no critical point in Dk; otherwise it also holds.\nNow we bound the distance between the corresponding critical points ofQ(w) and Q\u0303n(w). Assume that in Dk,Q(w) has a unique critical point w(k) and Q\u0303n(w) also has a unique critical point w (k) n . Then, there exists t \u2208 [0, 1] such that for any z \u2208 \u2202Bd(1), we have \u2265\u2016\u2207Q(w(k)n )\u20162\n= max zT z=1\n\u3008\u2207Q(w(k)n ), z\u3009\n= max zT z=1 \u3008\u2207Q(w(k)), z\u3009+ \u3008\u22072Q(w(k) + t(w(k)n \u2212w(k)))(w(k)n \u2212w(k)), z\u3009 \u00ac \u2265 \u2329( \u22072Q(w(k)) )2 (w(k)n \u2212w(k)), (w(k)n \u2212w(k)) \u232a1/2\n \u2265\u03b6\u2016w(k)n \u2212w(k)\u20162,\nwhere \u00ac holds since \u2207Q(w(k)) = 0 and  holds since w(k) + t(w(k)n \u2212w(k)) is in Dk and for any w \u2208 Dk we have infi |\u03bbi ( \u22072Q(w) ) | \u2265 \u03b6. So if n \u2265 ch max ( l2bl+1 2(bl+1+ \u2211l i=1 dibi) 2(r0c0d0) 4\nd40b1 8d%\u03b52 maxi(rici)\n, d+\u03b8%\u03b62 ) where ch is a constant, then\n\u2016w(k)n \u2212w(k)\u20162 \u2264 2cg\u03b2\n\u03b6\n\u221a\u221a\u221a\u221ad log(6) + \u03b8 (\u2211l i=1 log (\u221a dsbs(ks\u2212ss+1) 4p ) + log(bl+1) + log ( n 128p2 )) + log ( 4 \u03b5 )\nn\nholds with probability at least 1\u2212 \u03b5."}, {"heading": "D.6 Proof of Corollary 2", "text": "Proof. By Theorem 3, we know that the non-degenerate stationary point w(k) in the m non-degenerate stationary points in population risk, denoted by {w(1),w(2), \u00b7 \u00b7 \u00b7 ,w(m)} uniquely corresponding to a non-degenerate stationary point w(k)n in the empirical risk.\nOn the other hand, for any w(k), it obeys\ninf i\n\u2223\u2223\u03bbki ( \u22072Q(w(k)) )\u2223\u2223 \u2265 \u03b6,\nwhere \u03bbki ( \u22072Q(w(k)) ) denotes the i-th eigenvalue of the Hessian \u22072Q(w(k)) and \u03b6 is a constant. We further define a\nset D = {w \u2208 Rd | \u2016\u2207Q(w)\u20162 \u2264 and infi |\u03bbi ( \u22072Q(w(k)) ) | \u2265 \u03b6}. According to Lemma 5, D = \u222a\u221ek=1Dk where each Dk is a disjoint component with w(k) \u2208 Dk for k \u2264 m and Dk does not contain any critical point ofQ(w) for k \u2265 m+ 1. Thenw(k)n also belong to the component Dk due to the unique corresponding relation betweenw(k) andw (k) n . Then from Eqn. (6) and (7), we know that if the assumptions in Theorem 3 hold, then for arbitrary w \u2208 Dk and t \u2208 (0, 1),\ninf w\u2208Dk\n\u2225\u2225\u2225t\u2207Q\u0303n(w) + (1\u2212 t)\u2207Q(w) \u2225\u2225\u2225\n2 \u2265 2 and inf w\u2208Dk\n\u2223\u2223\u2223\u03bbki ( \u22072Q\u0303n(w) )\u2223\u2223\u2223 \u2265 \u03b6 2 ,\nwhere and \u03b6 are constants. This means that in each set Dk,\u22072Q\u0303n(w) has no zero eigenvalues. Then, combine this and Eqn. (6), we can obtain that in Dk, ifQ(w) has a unique critical pointw(k) with non-degenerate index sk, then Q\u0303n(w) also has a unique critical point wn(k) in Dk with the same non-degenerate index sk. Namely, the number of negative eigenvalues of the Hessian matrices\u22072Q(w(k)) and \u22072Q(w(k)n ) are the same. This further gives that if one of the pair (w(k),w(k)n ) is a local minimum or saddle point, then another one is also a local minimum or a saddle point. The proof is completed."}, {"heading": "E Proof of Auxiliary Lemmas", "text": ""}, {"heading": "E.1 Proof of Lemma 8", "text": "Proof. (1) Since G (z) is a diagonal matrix and its diagonal values are upper bounded by \u03c31(zi)(1\u2212 \u03c31(z)) \u2264 1/4 where zi denotes the i-th entry of zi, we can conclude\n\u2016G (z)M\u20162F \u2264 1\n16 \u2016M\u20162F and \u2016NG (z)\u20162F \u2264\n1\n16 \u2016N\u20162F .\n(2) The operator Q (\u00b7) maps a vector z \u2208 Rd into a matrix of size d2 \u00d7 d whose ((i\u2212 1)d+ i, i) (i = 1, \u00b7 \u00b7 \u00b7 , d) entry equal to \u03c31(zi)(1\u2212 \u03c31(zi))(1\u2212 2\u03c31(zi)) and rest entries are all 0. This gives\n\u03c31(zi)(1\u2212 \u03c31(zi))(1\u2212 2\u03c31(zi)) = 1\n3 (3\u03c31(zi))(1\u2212 \u03c31(zi))(1\u2212 2\u03c31(zi))\n\u22641 3\n( 3\u03c31(zi) + 1\u2212 \u03c31(zi) + 1\u2212 2\u03c31(zi)\n3\n)3\n\u22642 3\n34 .\nThis means the maximal value in Q (z) is at most 2 3\n34 . Consider the structure in Q (z), we can obtain\n\u2016Q (z)M\u20162F \u2264 26\n38 \u2016M\u20162F and \u2016NQ (z)\u20162F \u2264\n26 38 \u2016N\u20162F .\n(3) up (M) represents conducting upsampling on M \u2208 Rs\u00d7t\u00d7q. Let N = up (M) \u2208 Rps\u00d7pt\u00d7q. Specifically, for each slice N(:, :, i) (i = 1, \u00b7 \u00b7 \u00b7 , q), we have N(:, :, i) = up (M(:, :, i)). It actually upsamples each entry M(g, h, i) into a matrix of p2 same entries 1p2M(g, h, i). So it is easy to obtain\n\u2016up (M)\u20162F \u2264 1\np2 \u2016M\u20162F .\n(4) Let M = W (:, :, i) and N = \u03b4\u0303i+1(:, : i). Assume that H = M ~N \u2208 Rm1\u00d7m2 , where m1 = r\u0303i\u22121 \u2212 2ki + 2 and m2 = c\u0303i\u22121 \u2212 2ki + 2. Then we have\n\u2016H\u20162F = m1\u2211\ni=1\nm2\u2211\nj=1\n|H(i, j)|2 = m1\u2211\ni=1\nm2\u2211\nj=1\n\u3008M\u2126i,j ,N\u30092 \u2264 m1\u2211\ni=1\nm2\u2211\nj=1\n\u2016M\u2126i,j\u20162F \u2016N\u20162F ,\nwhere \u2126i,j denotes the entry index ofM for the (i, j)-th convolution operation (i.e. computing theH(i, j)).\nSince for each convolution computing, each element in M is involved at most one time, we can claim that any element in M in \u2211m1 i=1 \u2211m2 j=1 \u2016M\u2126i,j\u20162F occurs at most (ki \u2212 si + 1)2 since there are si \u2212 1 rows and columns between each neighboring nonzero entries inN which is decided by the definition of \u03b4\u0303i+1 in Sec. B.1. Therefore, we have\nm1\u2211\ni=1\nm2\u2211\nj=1\n\u2016M\u2126i,j\u20162F \u2264 (ki \u2212 si + 1)2\u2016M\u20162F ,\nwhich further gives\n\u2016M~\u0303N\u20162F \u2264 (ki \u2212 si + 1)2\u2016M\u20162F \u2016N\u20162F .\nConsider all the slices in \u03b4\u0303i+1, we can obtain\n\u2016\u03b4\u0303i+1~\u0303W \u20162F \u2264 (ki \u2212 si + 1)2\u2016W \u20162F \u2016\u03b4\u0303i+1\u20162F .\n(5) Since for softmax activation function \u03c32, we have \u2211dl+1 i=1 vi = 1 (vi \u2265 0) and there is only one nonzero entry (i.e. 1) in y, we can obtain\n0 \u2264 \u2016v \u2212 y\u201622 = \u2016v\u201622 + \u2016y\u201622 \u2212 2\u3008v,y\u3009 = 2\u2212 2\u3008v,y\u3009 \u2264 2.\nThe proof is completed."}], "year": 2018, "references": [{"title": "Tight oracle bounds for low-rank matrix recovery from a minimal number of random measurements", "authors": ["E. Candes", "Y. Plan"], "venue": "IEEE TIT,", "year": 2009}, {"title": "Introduction to the non-asymptotic analysis of random matrices, compressed sensing", "authors": ["R. Vershynin"], "year": 2012}], "id": "SP:26d1112d0fb6e8133c6e6aab727599178136656c", "authors": [{"name": "Pan Zhou", "affiliations": []}, {"name": "Jiashi Feng", "affiliations": []}], "abstractText": "This work aims to provide understandings on the remarkable success of deep convolutional neural networks (CNNs) by theoretically analyzing their generalization performance and establishing optimization guarantees for gradient descent based training algorithms. Specifically, for a CNN model consisting of l convolutional layers and one fully connected layer, we prove that its generalization error is bounded by O( \u221a \u03b8%\u0303/n) where \u03b8 denotes freedom degree of the network parameters and %\u0303 = O(log(li=1 bi(ki \u2212 si + 1)/p) + log(bl+1)) encapsulates architecture parameters including the kernel size ki, stride si, pooling size p and parameter magnitude bi. To our best knowledge, this is the first generalization bound that only depends on O(log(\u220fl+1 i=1 bi)), tighter than existing ones that all involve an exponential term likeO(\u220fl+1 i=1 bi). Besides, we prove that for an arbitrary gradient descent algorithm, the computed approximate stationary point by minimizing empirical risk is also an approximate stationary point to the population risk. This well explains why gradient descent training algorithms usually perform sufficiently well in practice. Furthermore, we prove the one-to-one correspondence and convergence guarantees for the non-degenerate stationary points between the empirical and population risks. It implies that the computed local minimum for the empirical risk is also close to a local minimum for the population risk, thus ensuring the good generalization performance of CNNs.", "title": "Understanding Generalization and Optimization Performance of Deep CNNs"}