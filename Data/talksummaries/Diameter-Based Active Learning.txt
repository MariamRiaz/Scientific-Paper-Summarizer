29	12	That paper identifies a parameter called the splitting index ⇢ that captures the relevant geometry, and gives upper bounds on label complexity that are proportional to 1/⇢, as well as showing that this dependence is inevitable.
56	29	Given a sequence of data points x 1 , .
57	29	, x n and a target hypothesis h⇤, the induced version space is the set of hypotheses that are consistent with the target hypotheses on the sequence, i.e. {h 2 H : h(x i ) = h⇤(x i ) for all i = 1, .
58	25	The diameter of a set of hypotheses V ⇢ H is the maximal distance between any two hypotheses in V , i.e. diam(V ) := max h,h 02V d(h, h0).
59	25	Without any prior information, any hypothesis in the version space could be the target.
61	10	The splitting index roughly characterizes the number of queries required for an active learning algorithm to reduce the diameter of the version space below ✏.
62	24	While reducing the diameter of a version space V ⇢ H, we will sometimes identify pairs of hypotheses h, h0 2 V that are far apart and therefore need to be separated.
63	22	Given a set of edges E = {{h 1 , h0 1 }, .
64	16	, {h n , h0 n }} ⇢ H 2 , we say a data point x ⇢- splits E if querying x separates at least a ⇢ fraction of the pairs, that is, if max E+ x |, |E x  (1 ⇢)|E| where E+ x = E \ H+x 2 and similarly for E x .
67	18	The following theorem, due to Dasgupta (2005), bounds the sample complexity of active learning in terms of the splitting index.
69	17	Suppose H is a hypothesis class with splitting index (⇢, ✏, ⌧).
70	19	Then to learn a hypothesis with error ✏, (a) any active learning algorithm with  1/⌧ unlabeled samples must request at least 1/⇢ labels, and (b) if H has VC-dimension d, there is an active learning algorithm that draws ˜O(d/(⇢⌧) log2(1/✏)) unlabeled data points and requests ˜O((d/⇢) log2(1/✏)) labels.
71	12	Unfortunately, the only known algorithm satisfying (b) above is intractable for all but the simplest hypothesis classes: it constructs an ✏-covering of the hypothesis space and queries points which whittle away at the diameter of this covering.
74	9	We define the average diameter of a subset V ⇢ H as the expected distance between two hypotheses in V randomly drawn from ⇡, i.e. (V ) := E h,h 0⇠⇡| V [d(h, h0)] where ⇡| V is the conditional distribution induced by restricting ⇡ to V , that is, ⇡| V (h) = ⇡(h)/⇡(V ) for h 2 V .
75	36	Intuitively, a version space with very small average diameter ought to put high weight on hypotheses that are close to the true hypothesis.
84	9	We now turn to defining an average notion of splitting.
90	23	Let ⇡ be a probability measure over a hypothesis class H. If H has splitting index (⇢, ✏, ⌧), then it has average splitting index ( ⇢ 4dlog(1/✏)e , 2✏, ⌧).
93	10	Moreover, given access to samples from ⇡| V , we can easily estimate the quantities appearing in the definition of average splitting.
98	31	However, this definition does not satisfy a nice relationship with the splitting index.
102	16	If we draw ˜O(1/⌧) points from the data distribution then, with high probability, one of these will ⇢- average split V .
117	13	do Draw E0 ⇠ (⇡| V ) 2⇥m t and compute b t = 1 m t (E0) Draw E ⇠ (⇡| V ) 2⇥n t If 9x i s.t.
122	65	In SELECT, fix a round t and data point x 2 X that exactly ⇢-average splits V (that is, max{⇡| V (V + x ) 2 (V + x ), ⇡| V (V x ) 2 (V x )} = (1 ⇢) (V )).
181	9	We compared DBAL against the baseline passive learner as well as two other generic active learning strategies: CAL and QBC.
186	46	In each of our simulations, we drew our target h⇤ from the prior distribution.
191	10	In our simulations, both the prior distribution and the data distribution are uniform over the unit sphere.
192	62	Although there is no known method to exactly sample uniformly from the version space, Gilad-Bachrach et al. (2005) demonstrated that using samples generated by the hit-andrun Markov chain works well in practice.
195	14	Sparse monotone disjunctions A k-sparse monotone disjunction is a disjunction of k positive literals.
197	10	Bernoulli random variables with parameter p. The prior distribution is uniform over all k-sparse monotone disjunctions.
198	29	When k is constant, it is possible to sample from the prior restricted to the version space in expected polynomial time using rejection sampling.
199	18	The results of our simulations on k-sparse monotone disjunctions are in Figure 2.
