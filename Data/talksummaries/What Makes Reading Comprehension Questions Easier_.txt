0	9	Evaluating natural language understanding (NLU) systems is a long-established problem in AI (Levesque, 2014).
1	50	One approach to doing so is the machine reading comprehension (MRC) task, in which a system answers questions about given texts (Hirschman et al., 1999).
2	53	Although recent studies have made advances (Yu et al., 2018), it is still unclear to what precise extent questions require understanding of texts (Jia and Liang, 2017).
3	25	In this study, we examine MRC datasets and discuss what is needed to create datasets suit- able for the detailed testing of NLU.
4	43	Our motivation originates from studies that demonstrated unintended biases in the sourcing of other NLU tasks, in which questions contain simple patterns and systems can recognize these patterns to answer them (Gururangan et al., 2018; Mostafazadeh et al., 2017).
5	19	We conjecture that a situation similar to this occurs in MRC datasets.
7	37	Although the question, starting with when, requires an answer that is expressed as a moment in time, there is only one such expression (i.e., November 2014) in the given text (we refer to the text as the context).
8	24	In other words, the question has only a single candidate answer.
9	15	The system can solve it merely by recognizing the entity type required by when.
11	2	Therefore, this kind of question does not require a complex understanding of language—e.g., multiple-sentence reasoning, which is known as a more challenging task (Richardson et al., 2013).
12	5	In Section 3, we define two heuristics, namely entity-type recognition and attention.
13	135	We specifically analyze the differences in the performance of baseline systems for the following two configurations: (i) questions answerable or unanswerable with the first k tokens; and (ii) questions whose correct answer appears or does not appear in the context sentence that is most similar to the question (henceforth referred to as the most similar sentence).
14	97	Although similar heuristics are proposed by Weissenborn et al. (2017), ours are utilized for question filtering, rather than system development; Using these simple heuristics, we split each dataset into easy and hard subsets for further investigation of the baseline performance.
15	22	After conducting the experiments, we analyze the following two points in Section 4.
16	12	First, we consider which questions are valid for testing, i.e., reasonably solvable.
18	87	To investigate these two concerns, we manually annotate sample questions from each subset in terms of validity and required reasoning skills, such as word matching, knowledge inference, and multiple sentence reasoning.
19	24	We examine 12 recently proposed MRC datasets (Table 1), which include answer extraction, description, and multiple-choice styles.
21	24	For our baselines, we use two neural-based systems, namely, the Bidirectional Attention Flow (Seo et al., 2017) and the Gated-Attention Reader (Dhingra et al., 2017).
22	11	In Section 5, we describe the advantages and disadvantages of different question styles with regard to evaluating NLU systems.
23	25	We also interpret our heuristics for constructing realistic MRC datasets.
24	7	Our contributions are as follows: • This study is the first large-scale investigation across recent 12 MRC datasets with three question styles.
25	31	• We propose to employ simple heuristics to split each dataset into easy and hard subsets and examine the performance of two baseline models for each of the subsets.
26	95	• We manually annotate questions sampled from each subset with both validity and requisite reasoning skills to investigate which skills explain the difference between easy and hard questions.
27	39	We observed the following: • The baseline performances for the hard subsets remarkably degrade compared to those of entire datasets.
28	12	• Our annotation study shows that hard questions require knowledge inference and multiplesentence reasoning in comparison with easy questions.
29	44	• Compared to questions with answer extraction and description styles, multiple-choice questions tend to require a broader range of reasoning skills while exhibiting answerability, multiple answer candidates, and unambiguity.
31	32	They also emphasize the importance of considering simple answer-seeking heuristics when sourcing questions, in that a dataset could be easily biased unless such heuristics are employed.1
34	4	We employed the following two widely used baselines.
36	69	BiDAF models bi-directional attention between the context and question.
