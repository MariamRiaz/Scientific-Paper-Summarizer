0	33	Generative models defining joint distributions over parse trees and sentences are good theoretical models for interpreting natural language data, and appealing tools for tasks such as parsing, grammar induction and language modeling (Collins, 1999; Henderson, 2003; Titov and Henderson, 2007; Petrov and Klein, 2007; Dyer et al., 2016).
1	16	However, they often impose strong independence assumptions which restrict the use of arbitrary features for effective disambiguation.
2	18	Moreover, generative parsers are typically trained by maximizing the joint probability of the parse tree and the sentence—an objective that only indirectly relates to the goal of parsing.
3	38	At test time, these models require a relatively expensive recognition algorithm (Collins, 1999; Titov and Henderson, 2007) to recover the parse tree, but the parsing performance consistently lags behind their discriminative competitors (Nivre et al., 2007; Huang, 2008; Goldberg and Elhadad, 2010), which are directly trained to maximize the conditional probability of the parse tree given the sentence, where linear-time decoding algorithms exist (e.g., for transition-based parsers).
15	14	In this section we briefly describe Recurrent Neural Network Grammars (RNNGs; Dyer et al. 2016), a top-down transition-based algorithm for parsing and generation.
21	15	• SHIFT fetches the terminal in the front of the buffer and pushes it onto the top of the stack.
22	13	• REDUCE completes a subtree by repeatedly popping the stack until an open non-terminal is encountered.
24	35	The above transition system can be adapted with minor modifications to an algorithm that generates trees and sentences.
25	13	In generator transitions, there is no input buffer of unprocessed words but there is an output buffer for storing words that have been generated.
28	16	For illustration, we adopt the two RNNG variants introduced above with our customized features.
34	16	The decoder is a generative RNNG that models the joint probability p(x, y) of a latent parse tree y and an observed sentence x.
43	57	Our features4 are: 1) the stack embedding et obtained with a stack-LSTM that encodes the stack of the encoder; 2) the input buffer embedding it; we use a bidirectional LSTM to compose the input buffer and represent each word as a concatenation of forward and backward LSTM states; it is the representation of the word on top of the buffer; 3) to incorporate more global features and a more sophisticated look-ahead mechanism for the buffer, we also use an adaptive buffer embedding īt; the latter is computed by having the stack embedding et attend to all remaining embeddings on the buffer with the attention function in Vinyals et al. (2015); and 4) the parent non-terminal embedding nt.
45	17	Consider an auto-encoder whose encoder infers the latent parse tree and the decoder generates the observed sentence from the parse tree.5 The maximum likelihood estimate of the decoder parameters is determined by the log marginal likelihood of the sentence: log p(x) = log ∑ a p(x, a) (7) We follow expectation-maximization and variational inference techniques to construct an evidence lower bound of the above quantity (by Jensen’s Inequality), denoted as follows: log p(x) ≥ Eq(a|x) log p(x, a) q(a|x) = Lx (8) where p(x, a) = p(x|a)p(a) comes from the decoder or the generative model, and q(a|x) comes from the encoder or the recognition model.
49	20	We can directly maximize the log likelihood of the parse tree for the encoder output log q(a|x) and the decoder output log p(a): La = log q(a|x) + log p(a) (9) This supervised objective leverages extra information of labeled parse trees to regularize the distribution q(a|x) and p(a), and the final objective is: L = Lx + La (10) where Lx and La can be balanced with the task focus (e.g, language modeling or parsing).
51	20	Parsing In parsing, we are interested in the parse tree that maximizes the posterior p(a|x) (or the joint p(a, x)).
54	22	An alternative is to generate candidate trees by sampling from q(a|x), re-rank them with respect to the joint p(x, a) (which is proportional to the true posterior), and select the sample that maximizes the true posterior.
55	14	Language Modeling In language modeling, our goal is to compute the marginal probability p(x) = ∑ a p(x, a), which is typically intractable.
56	23	To approximate this quantity, we can use Equation (8) to compute a lower bound of the log likelihood log p(x) and then exponentiate it to get a pessimistic approximation of p(x).7 Another way of computing p(x) (without lower bounding) would be to use the variational approximation q(a|x) as the proposal distribution as in the importance sampler of Dyer et al. (2016).
64	19	We performed experiments on the English Penn Treebank dataset; we used sections 2–21 for training, 24 for validation, and 23 for testing.
69	36	Experimental results for constituency parsing and language modeling are shown in Tables 2 and 3, respectively.
73	26	We believe this is due to implementation disparities, such as the modeling of the reduce operation.
79	14	We proposed a framework that integrates a generative parser with a discriminative recognition model and showed how it can be instantiated with RNNGs.
80	29	We demonstrated that a unified framework, which relates to expectation maximization and variational inference, enables effective parsing and language modeling algorithms.
81	44	Evaluation on the English Penn Treebank, revealed that our framework obtains competitive performance on constituency parsing and state-of-the-art results on single-model language modeling.
82	21	In the future, we would like to perform grammar induction based on Equation (8), with gradient descent and posterior regularization techniques (Ganchev et al., 2010).
85	29	(Dyer et al., 2016) In this appendix we highlight the connections between importance sampling and variational inference, thereby comparing our method with Dyer et al. (2016).
87	26	The model evidence, or the marginal likelihood p(x) = ∑ a p(x, a) is often intractable to compute.
93	19	This proposal choice is close to optimal, since in a fully supervised setting a is also observed and the discriminative model can be trained to approximate the true posterior well.
94	19	We hypothesize that the performance of their importance sampler is dependent on this specific proposal distribution.
95	58	Besides, their training strategy does not generalize to an unsupervised setting.
