0,1,label2,summary_sentences
"Decades of research have been dedicated to heuristics for speeding up inference in natural language processing tasks, such as constituency parsing (Pauls and Klein, 2009; Caraballo and Charniak, 1998) and machine translation (Petrov et al., 2008; Xu et al., 2013).",1 Introduction,[0],[0]
"Such research is necessary because of a trend toward richer models, which improve accuracy at the cost of slower inference.",1 Introduction,[0],[0]
"For example, state-of-theart constituency parsers use grammars with millions of rules, while dependency parsers routinely use millions of features.",1 Introduction,[0],[0]
"Without heuristics, these parsers take minutes to process a single sentence.
",1 Introduction,[0],[0]
"To speed up inference, we will learn a pruning policy.",1 Introduction,[0],[0]
"During inference, the pruning policy is invoked to decide whether to keep or prune various parts of the search space, based on features of the input and (potentially) the state of the inference process.
",1 Introduction,[0],[0]
"Our approach searches for a policy with maximum end-to-end performance (reward) on training data, where the reward is a linear combination of problemspecific measures of accuracy and runtime, namely reward = accuracy−λ · runtime.",1 Introduction,[0],[0]
"The parameter λ ≥ 0
specifies the relative importance of runtime and accuracy.",1 Introduction,[0],[0]
"By adjusting λ, we obtain policies with different speed-accuracy tradeoffs.
",1 Introduction,[0],[0]
"For learning, we use Locally Optimal Learning to Search (LOLS) (Chang et al., 2015b), an algorithm for learning sequential decision-making policies, which accounts for the end-to-end performance of the entire decision sequence jointly.",1 Introduction,[0],[0]
"Unfortunately, executing LOLS naively in our setting is prohibitive because it would run inference from scratch millions of times under different policies, training examples, and variations of the decision sequence.",1 Introduction,[0],[0]
"Thus, this paper presents efficient algorithms for repeated inference, which are applicable to a wide variety of NLP tasks, including parsing, machine translation and sequence tagging.",1 Introduction,[0],[0]
"These algorithms, based on change propagation and dynamic programming, dramatically reduce time spent evaluating similar decision sequences by leveraging problem structure and sharing work among evaluations.
",1 Introduction,[0],[0]
We evaluate our approach by learning pruning heuristics for constituency parsing.,1 Introduction,[0],[0]
"In this setting, our approach is the first to account for end-to-end performance of the pruning policy, without making independence assumptions about the reward function, as in prior work (Bodenstab et al., 2011).",1 Introduction,[0],[0]
"In the larger context of learning-to-search for structured prediction, our work is unusual in that it learns to control a dynamic programming algorithm (i.e., graphbased parsing) rather than a greedy algorithm (e.g., transition-based parsing).",1 Introduction,[0],[0]
Our experiments show that accounting for end-to-end performance in training leads to better policies along the entire Pareto frontier of accuracy and runtime.,1 Introduction,[0],[0]
"A simple yet effective approach to speeding up parsing was proposed by Bodenstab et al. (2011), who trained a pruning policy π to classify whether or not spans of the input sentence w1 · · ·wn form plausible
263
Transactions of the Association for Computational Linguistics, vol. 5, pp.",2 Weighted CKY with pruning,[0],[0]
"263–278, 2017.",2 Weighted CKY with pruning,[0],[0]
Action Editor: Marco Kuhlmann.,2 Weighted CKY with pruning,[0],[0]
"Submission batch: 5/2016; Revision batch: 9/2016; Published 8/2017.
",2 Weighted CKY with pruning,[0],[0]
c©2017 Association for Computational Linguistics.,2 Weighted CKY with pruning,[0],[0]
"Distributed under a CC-BY 4.0 license.
constituents based on features of the input sentence.",2 Weighted CKY with pruning,[0],[0]
"These predictions enable a parsing algorithm, such as CKY, to skip expensive steps during its execution: unlikely constituents are pruned.",2 Weighted CKY with pruning,[0],[0]
"Only plausible constituents are kept, and the parser assembles the highest-scoring parse from the available constituents.
",2 Weighted CKY with pruning,[0],[0]
Alg. 1 provides pseudocode for weighted CKY with pruning.,2 Weighted CKY with pruning,[0],[0]
"Weighted CKY aims to find the highestscoring derivation (parse tree) of a given sentence, where a given grammar specifies a non-negative score for each derivation rule and a derivation’s score is the product of the scores of the rules it uses.1 CKY uses a dynamic programming strategy to fill in a three-dimensional array β, known as the chart.",2 Weighted CKY with pruning,[0],[0]
The score βikx is the score of the highest-scoring subderivation with fringe wi+1 . . .,2 Weighted CKY with pruning,[0],[0]
wk and root,2 Weighted CKY with pruning,[0],[0]
x.,2 Weighted CKY with pruning,[0],[0]
This value is computed by looping over the possible ways to assemble such a subderivation from smaller subderivations with scores βijy and βjkz (lines 17–22).,2 Weighted CKY with pruning,[0],[0]
"Additionally, we track a witness (backpointer) for each βikx, so that we can easily reconstruct the corresponding subderivation at line 23.",2 Weighted CKY with pruning,[0],[0]
"The chart is initialized with lexical grammar rules (lines 3–9), which derive words from grammar symbols.
",2 Weighted CKY with pruning,[0],[0]
"The key difference between pruned and unpruned CKY is an additional “if” statement (line 14), which queries the pruning policy π to decide whether to compute the several values βikx associated with a span (i, k).",2 Weighted CKY with pruning,[0],[0]
Note that width-1 and width-n spans are always kept because all valid parses require them.,2 Weighted CKY with pruning,[0],[0]
Bodenstab et al. (2011) train their pruning policy as a supervised classifier of spans.,3 End-to-end training,[0],[0]
"They derive direct supervision as follows: try to keep a span if it appears in the gold-standard parse, and prune it otherwise.",3 End-to-end training,[0],[0]
They found that using an asymmetric weighting scheme helped find the right balance between false positives and false negatives.,3 End-to-end training,[0],[0]
"Intuitively, failing to prune is only a slight slowdown, whereas pruning a good item can ruin the accuracy of the parse.
",3 End-to-end training,[0],[0]
"1As is common practice, we assume the grammar has been binarized.",3 End-to-end training,[0],[0]
"We focus on pre-trained grammars, leaving coadaptation of the grammar and pruning policy to future work.",3 End-to-end training,[0],[0]
"As indicated at lines 6 and 19, a rule’s score may be made to depend on the context in which that rule is applied (Finkel et al., 2008), although the pre-trained grammars in our present experiments are ordinary PCFGs for which this is not the case.
",3 End-to-end training,[0],[0]
"Algorithm 1 PARSE: Weighted CKY with pruning 1: Input: grammar G, sentence w, policy π
Output: completed chart β, derivation d 2: .",3 End-to-end training,[0],[0]
"Initialize chart 3: β := 0 4: for k := 1 to n : 5: for x such that (x→ wk) ∈ rules(G) : 6: s := G(x→ wk | w, k) 7: if s > βk−1,k,x : 8: βk−1,k,x := s 9: witness(k−1, k, x) := (k−1, k, wk)
10: for width := 2 to n : 11: for i := 0 to n− width : 12: k := i+ width .",3 End-to-end training,[0],[0]
"Current span is (i, k) 13: .",3 End-to-end training,[0],[0]
"Policy determines whether to fill in this span 14: if π(w, i, k) = prune : 15: continue 16: .",3 End-to-end training,[0],[0]
Fill in span by considering each split point j 17: for j := i+ 1 to k,3 End-to-end training,[0],[0]
"− 1 : 18: for (x→ y z) ∈ rules(G) : 19: s := βijy ·βjkz ·G(x→ y z | w, i, j, k) 20: if s > βikx : 21: βikx := s 22: witness(i, k, x) := (j, y, z) 23: d̂ := follow backpointers from (0, n,ROOT) 24: return (β, d̂)
",3 End-to-end training,[0],[0]
"Our end-to-end training approach improves upon asymmetric weighting by jointly evaluating the sequence of pruning decisions, measuring its effect on the test-time evaluation metric by actually running pruned CKY (Alg. 1).",3 End-to-end training,[0],[0]
"To estimate the value of a pruning policy π, we call PARSE(G,w(i), π) on each training sentence w(i), and apply the reward function, r = accuracy−λ · runtime.",3 End-to-end training,[0],[0]
"The empirical value of a policy is its average reward on the training set:
R(π) = 1 m
m∑
i=1
E",3 End-to-end training,[0],[0]
"[ r(PARSE(G,w(i), π)) ]",3 End-to-end training,[0],[0]
"(1)
The expectation in the definition may be dropped if PARSE, π, and r are all deterministic, as in our setting.2 Our definition of r depends on the user parameter λ ≥ 0, which specifies the amount of accuracy the user would sacrifice to save one unit of
2Parsers may break ties randomly or use Monte Carlo methods.",3 End-to-end training,[0],[0]
"The reward function r can be nondeterministic when it involves wallclock time or human judgments.
runtime.",3 End-to-end training,[0],[0]
"Training under a range of values for λ gives rise to policies covering a number of operating points along the Pareto frontier of accuracy and runtime.
",3 End-to-end training,[0],[0]
End-to-end training gives us a principled way to decide what to prune.,3 End-to-end training,[0],[0]
"Rather than artificially labeling each pruning decision as inherently good or bad, we evaluate its effect in the context of the particular sentence and the other pruning decisions.",3 End-to-end training,[0],[0]
"Actions that prune a gold constituent are not equally bad—some cause cascading errors, while others are “worked around” in the sense that the grammar still selects a mostly-gold parse.",3 End-to-end training,[0],[0]
"Similarly, actions that prune a non-gold constituent are not equally good—some provide more overall speedup (e.g., pruning narrow constituents prevents wider ones from being built), and some even improve accuracy by suppressing an incorrect but high-scoring parse.
",3 End-to-end training,[0],[0]
"More generally, the gold vs. non-gold distinction is not even available in NLP tasks where one is pruning potential elements of a latent structure, such as an alignment (Xu et al., 2013) or a finer-grained parse (Matsuzaki et al., 2005).",3 End-to-end training,[0],[0]
"Yet our approach can still be used in such settings, by evaluating the reward on the downstream task that the latent structure serves.
",3 End-to-end training,[0],[0]
Past work on optimizing end-to-end performance is discussed in §8.,3 End-to-end training,[0],[0]
"One might try to scale these techniques to learning to prune, but in this work we take a different approach.",3 End-to-end training,[0],[0]
"Given a policy, we can easily find small ways to improve it on specific sentences by varying individual pruning actions (e.g., if π currently prunes a span then try keeping it instead).",3 End-to-end training,[0],[0]
"Given a batch of improved action sequences (trajectories), the remaining step is to search for a policy which produces the improved trajectories.",3 End-to-end training,[0],[0]
"Conveniently, this can be reduced to a classification problem, much like the asymmetric weighting approach, except that the supervised labels and misclassification costs are not fixed across iterations, but rather are derived from interaction with the environment (i.e., PARSE and the reward function).",3 End-to-end training,[0.9510020642608018],"['The same framework we use here could be extended to video, enabling the learning of actions, verbs, environmental sounds, and the like.']"
"This idea is formalized as a learning algorithm called Locally Optimal Learning to Search (Chang et al., 2015b), described in §4.
",3 End-to-end training,[0],[0]
The counterfactual interventions we require— evaluating how reward would change if we changed one action—can be computed more efficiently using our novel algorithms (§5) than by the default strategy of running the parser repeatedly from scratch.,3 End-to-end training,[0],[0]
"The key is to reuse work among evaluations, which is
possible because LOLS only makes tiny changes.",3 End-to-end training,[0],[0]
Pruned inference is a sequential decision process.,4 Learning algorithm,[0],[0]
The process begins in an initial state s0.,4 Learning algorithm,[0],[0]
"In pruned CKY, s0 specifies the state of Alg.",4 Learning algorithm,[0],[0]
"1 at line 10, after the chart has been initialized from some selected sentence.",4 Learning algorithm,[0],[0]
"Next, the policy is invoked to choose action a0 = π(s0)—in",4 Learning algorithm,[0],[0]
our case at line 14—which affects what the parser does next.,4 Learning algorithm,[0],[0]
"Eventually the parser reaches some state s1 from which it calls the policy to choose action a1 = π(s1), and so on.",4 Learning algorithm,[0],[0]
"When the policy is invoked at state st, it selects action at based on features extracted from the current state st—a snapshot of the input sentence, grammar and parse chart at time t.3",4 Learning algorithm,[0],[0]
"We call the state-action sequence s0 a0 s1 a1 · · · sT a trajectory, where T is the trajectory length.",4 Learning algorithm,[0],[0]
"At the final state, the reward function is evaluated, r(sT ).
",4 Learning algorithm,[0],[0]
The LOLS algorithm for learning a policy is given in Alg.,4 Learning algorithm,[0],[0]
"2,4 with a graphical illustration in Fig. 1.",4 Learning algorithm,[0],[0]
"At a high level, LOLS alternates between evaluating and improving the current policy πi.
",4 Learning algorithm,[0],[0]
"The evaluation phase first samples a trajectory from πi, called a roll-in: s0 a0 s1 a1 · · · sT ∼ ROLL-IN(πi).",4 Learning algorithm,[0],[0]
"In our setting, s0 is derived from a randomly sampled training sentence, but the rest of the trajectory is then deterministically computed by πi given s0.",4 Learning algorithm,[0],[0]
"Then we revisit each state s in the roll-in (line 7), and try each available action ā∈A(s)",4 Learning algorithm,[0],[0]
"(line 9), executing πi thereafter—a rollout—to measure the resulting reward r̂[ā] (line 10).",4 Learning algorithm,[0],[0]
"Our parser is deterministic, so a single rollout is an unbiased, 0-variance estimate of the expected reward.",4 Learning algorithm,[0],[0]
"This process is repeated many times, yielding a large list Q̂i of pairs 〈s, r̂〉, where s is a state that was encountered in some roll-in and r̂ maps the possible actions A(s) in that state to their measured rewards.
",4 Learning algorithm,[0],[0]
"The improvement phase now trains a new policy πi+1 to try to choose high-reward actions, seeking a policy that will “on average” get high rewards r[πi+1(s)].",4 Learning algorithm,[0],[0]
"Good generalization is important: the policy must select high-reward actions even in states s that are not represented in Q̂i, in case they are
3Our experiments do not make use of the current state of the chart.",4 Learning algorithm,[0],[0]
"We discuss this decision in §8.
4Alg.",4 Learning algorithm,[0],[0]
"2 is simpler than in Chang et al. (2015b) because it omits oracle rollouts, which we do not use in our experiments.
",4 Learning algorithm,[0],[0]
Algorithm 2 LOLS algorithm for learning to prune.,4 Learning algorithm,[0],[0]
1: π1 := INITIALIZEPOLICY(. . . ),4 Learning algorithm,[0],[0]
2: for i := 1 to number of iterations : 3: .,4 Learning algorithm,[0],[0]
Evaluate: Collect dataset for πi 4: Q̂i := ∅ 5: for j := 1 to minibatch size : 6: s0 a0 s1 a1 · · · sT ∼ ROLL-IN(πi) .,4 Learning algorithm,[0],[0]
Sample 7: for t := 0 to T−1 : 8: .,4 Learning algorithm,[0],[0]
Intervene: Evaluate each action at st 9: for āt ∈ A(st) : .,4 Learning algorithm,[0],[0]
"Possible actions
10: r̂t[āt] ∼ ROLLOUT(πi, st, āt) 11: Q̂i.append(〈st, r̂t 〉) 12: .",4 Learning algorithm,[0],[0]
"Improve: Train with dataset aggregation
13: πi+1 ← TRAIN",4 Learning algorithm,[0],[0]
(,4 Learning algorithm,[0],[0]
"⋃i k=1 Q̂k )
14: .",4 Learning algorithm,[0],[0]
Finalize: Pick the best policy over all iterations 15: return argmaxi′ R(πi′) encountered when running the new policy πi+1 (or when parsing test sentences).,4 Learning algorithm,[0],[0]
"Thus, beyond just regularizing the training objective, we apply dataset aggregation (Ross et al., 2011): we take the training set to include not just Q̂i but also the examples from previous iterations (line 13).",4 Learning algorithm,[0],[0]
"This also ensures that the sequence of policies π1, π2, . .",4 Learning algorithm,[0],[0]
".will be “stable” (Ross and Bagnell, 2011) and will eventually converge.
",4 Learning algorithm,[0],[0]
"So line 13 seeks to find a good classifier πi+1 using a training set: a possible classifier π would receive from each training example 〈s, r̂〉 a reward of r̂[π(s)].",4 Learning algorithm,[0],[0]
"In our case, where A(s) = {keep, prune}, this cost-sensitive classification problem is equivalent to training an ordinary binary classifier, after converting each training example 〈s, r̂〉 to 〈s, argmaxa",4 Learning algorithm,[0],[0]
"r̂[a]〉 and giving this example a weight of |r̂t,keep− r̂t,prune|.",4 Learning algorithm,[0],[0]
"Our specific classifier is described in §6.
",4 Learning algorithm,[0],[0]
"In summary, the evaluation phase of LOLS collects training data for a cost-sensitive classifier, where the
inputs (states), outputs (actions), and costs are obtained by interacting with the environment.",4 Learning algorithm,[0],[0]
"LOLS concocts a training set and repeatedly revises it, similar to the well-known Expectation-Maximization algorithm.",4 Learning algorithm,[0],[0]
This enables end-to-end training of systems with discrete decisions and nondecomposable reward functions.,4 Learning algorithm,[0],[0]
LOLS gives us a principled framework for deriving (nonstationary) “supervision” even in tricky cases such as latent-variable inference (mentioned in §3).,4 Learning algorithm,[0],[0]
"LOLS has strong theoretical guarantees, though in pathological cases, it may take exponential time to converge (Chang et al., 2015b).
",4 Learning algorithm,[0],[0]
"The inner loop of the evaluation phase performs roll-ins, interventions and rollouts.",4 Learning algorithm,[0],[0]
Roll-ins ensure that the policy is (eventually) trained under the distribution of states it tends to encounter at test time.,4 Learning algorithm,[0],[0]
Interventions and rollouts force πi to explore the effect of currently disfavored actions.,4 Learning algorithm,[0],[0]
"Unlike most applications of LOLS and related algorithms, such as SEARN (Daumé III, 2006) and DAGGER (Ross et al., 2011), executing the policy is a major bottleneck in training.",5 Efficient rollouts,[0],[0]
"Because our dynamic programming parser explores many possibilities (unlike a greedy, transition-based decoder) its trajectories are quite long.",5 Efficient rollouts,[0],[0]
"This not only slows down each rollout: it means we must do more rollouts.
",5 Efficient rollouts,[0],[0]
"In our case, the trajectory has length T = n·(n+1)
2",5 Efficient rollouts,[0],[0]
− 1− n,5 Efficient rollouts,[0],[0]
"for a sentence of length n, where T is also the number of pruning decisions: one for each span other than the root and width-1 spans.",5 Efficient rollouts,[0],[0]
LOLS must then perform T rollouts on this example.,5 Efficient rollouts,[0],[0]
"This means that to evaluate policy πi, we must parse each sentence in the minibatch hundreds of times (e.g., 189 for n=20, 434 for n=30, and 779 for n=40).
",5 Efficient rollouts,[0],[0]
"We can regard each policy π as defining a pruning
mask m, an array that maps each of the T spans (i, k) to a decision mik (1 = keep, 0 = prune).",5 Efficient rollouts,[0],[0]
"Each rollout tries flipping a different bit in this mask.
",5 Efficient rollouts,[0],[0]
We could spend less time on each sentence by sampling only some of its T rollouts (see §6).,5 Efficient rollouts,[0],[0]
"Regardless, the rollouts we do on a given sentence are related: in this section we show how to get further speedups by sharing work among them.",5 Efficient rollouts,[0],[0]
"In §5.2, we leverage the fact that rollouts will be similar to one another (differing by a single pruning decision).",5 Efficient rollouts,[0],[0]
"In §5.3, we show that the reward of all T rollouts can be computed simultaneously by dynamic programming under some assumptions about the structure of the reward function (described later).",5 Efficient rollouts,[0],[0]
We found these algorithms to be crucial to training in a “reasonable” amount of time (see the empirical comparison in §7.2).,5 Efficient rollouts,[0],[0]
"It is convenient to present our efficient rollout algorithms in terms of the hypergraph structure of Alg. 1 (Klein and Manning, 2001; Huang, 2008; Li and Eisner, 2009; Eisner and Blatz, 2007).",5.1 Background: Parsing as hypergraphs,[0],[0]
A hypergraph describes the information flow among related quantities in a dynamic programming algorithm.,5.1 Background: Parsing as hypergraphs,[0],[0]
"Many computational tricks apply generically to hypergraphs.
",5.1 Background: Parsing as hypergraphs,[0],[0]
A hypergraph edge e (or hyperedge) is a “generalized arrow” e.head ≺ e.Tail with one output and a list of inputs.,5.1 Background: Parsing as hypergraphs,[0],[0]
"We regard each quantity βikx,mik, or G(. . .)",5.1 Background: Parsing as hypergraphs,[0],[0]
"in Alg. 1 as the value of a corresponding hypergraph vertex β̇ikx, ṁik, or Ġ(. . .).",5.1 Background: Parsing as hypergraphs,[0],[0]
"Thus, value(v̇) = v for any vertex v̇. Each ṁik’s value is computed by the policy π or chosen by a rollout intervention.",5.1 Background: Parsing as hypergraphs,[0],[0]
"Each Ġ’s value is given by the grammar.
",5.1 Background: Parsing as hypergraphs,[0],[0]
"Values of β̇ikx, by contrast, are computed at line 19 if k − i > 1.",5.1 Background: Parsing as hypergraphs,[0],[0]
"To record the dependence of βikx on other quantities, our hypergraph includes the hyperedge β̇ikx ≺",5.1 Background: Parsing as hypergraphs,[0],[0]
"(β̇ijy, β̇jkz, ṁik, ġ) for each 0 ≤",5.1 Background: Parsing as hypergraphs,[0],[0]
i < j < k ≤ n,5.1 Background: Parsing as hypergraphs,[0],[0]
"and (x→ y z) ∈ rules(G), where ġ denotes the vertex Ġ(x→ y z | w, i, j, k).
",5.1 Background: Parsing as hypergraphs,[0],[0]
"If k − i = 1, then values of βikx are instead computed at line 6, which does not access any other β values or the pruning mask.",5.1 Background: Parsing as hypergraphs,[0],[0]
"Thus our hypergraph includes the hyperedge vikx ≺(ġ) whenever i = k−1, 0 ≤",5.1 Background: Parsing as hypergraphs,[0],[0]
"i < k ≤ n, and (x→ wk) ∈ rules(G), with ġ = Ġ(x→ wk | w, k).
",5.1 Background: Parsing as hypergraphs,[0],[0]
"With this setup, the value βikx is the maximum score of any derivation of vertex β̇ikx (a tree rooted at β̇ikx, representing a subderivation), where the score
of a derivation is the product of its leaf values.",5.1 Background: Parsing as hypergraphs,[0],[0]
Alg. 1 computes it by considering hyperedges β̇ikx ≺ T and the previously computed values of the vertices in the tail T .,5.1 Background: Parsing as hypergraphs,[0],[0]
"For a vertex v̇, we write In(v̇) and Out(v̇) for its sets of incoming and outgoing hyperedges.",5.1 Background: Parsing as hypergraphs,[0],[0]
"Our algorithms follow these hyperedges implicitly, without the overhead of materializing or storing them.",5.1 Background: Parsing as hypergraphs,[0],[0]
"Change propagation is an efficient method for incrementally re-evaluating a computation under a change to its inputs (Acar and Ley-Wild, 2008; Filardo and Eisner, 2012).",5.2 Change propagation (CP),[0],[0]
"In our setting, each roll-in at Alg. 2 line 6 evaluates the reward r(PARSE(G, xi, π)) from (1), which involves computing an entire parse chart via Alg. 1.",5.2 Change propagation (CP),[0],[0]
"The inner loop at line 10 performs T interventions per roll-in, which ask how reward would have changed if one bit in the pruning maskm had been different.",5.2 Change propagation (CP),[0],[0]
"Rather than reparsing from scratch (T times) to determine this, we can simply adjust the initial roll-in computation (T times).
",5.2 Change propagation (CP),[0],[0]
CP is efficient when only a small fraction of the computation needs to be adjusted.,5.2 Change propagation (CP),[0],[0]
"In principle, flipping a single pruning bit can change up to 50% of the chart, so one might expect the bookkeeping overhead of CP to outweigh the gains.",5.2 Change propagation (CP),[0],[0]
"In practice, however, 90% of the interventions change < 10% of the β values in the chart.",5.2 Change propagation (CP),[0],[0]
"The reason is that βikx is a maximum over many quantities, only one of which “wins.”",5.2 Change propagation (CP),[0],[0]
"Changing a given βijy rarely affects this maximum, and so changes are unlikely to propagate from vertex β̇ijy to β̇ikx.",5.2 Change propagation (CP),[0],[0]
"Since changes are not very contagious, the “epidemic of changes” does not spread far.
",5.2 Change propagation (CP),[0],[0]
Alg. 3 provides pseudocode for updating the highest-scoring derivation found by Alg. 1.,5.2 Change propagation (CP),[0],[0]
"We remark that the RECOMPUTE is called only when we flip a bit from keep to prune, which removes hyperedges and potentially decreases vertex values.",5.2 Change propagation (CP),[0],[0]
"The reverse flip only adds hyperedges, which increases vertex values via a running max (lines 12–14).
",5.2 Change propagation (CP),[0],[0]
"After determining the effect of flipping a bit, we must restore the original chart before trying a different bit (the next rollout).",5.2 Change propagation (CP),[0],[0]
The simplest approach is to call Alg. 3 again to flip the bit,5.2 Change propagation (CP),[0],[0]
"back.5
5Our implementation uses a slightly faster method which accumulates an “undo list” of changes that it makes to the chart to quickly revert the modified chart to the original roll-in state.
",5.2 Change propagation (CP),[0],[0]
Algorithm 3 Change propagation algorithm 1: Global: Alg.,5.2 Change propagation (CP),[0],[0]
"1’s vertex values/witnesses (roll-in) 2: procedure CHANGE(v̇, v) 3: .",5.2 Change propagation (CP),[0],[0]
Change the value of a leaf vertex v̇ to v 4: value(v̇) := v ; witness(v̇) = LEAF 5: Q := ∅; Q.push(v̇) .,5.2 Change propagation (CP),[0],[0]
Work queue (“agenda”) 6: while Q 6= ∅ : .,5.2 Change propagation (CP),[0],[0]
Propagate until convergence 7: u̇,5.2 Change propagation (CP),[0],[0]
:= Q.pop() .,5.2 Change propagation (CP),[0],[0]
Narrower constituents first 8: if witness(u̇) = NULL : .,5.2 Change propagation (CP),[0],[0]
Value is unknown 9: RECOMPUTE(u̇) .,5.2 Change propagation (CP),[0],[0]
"Get value & witness
10: for e ∈ Out(u̇) : .",5.2 Change propagation (CP),[0],[0]
Propagate new value of u̇ 11: ṡ := e.head; s := ∏ u̇′∈e.,5.2 Change propagation (CP),[0],[0]
"Tail value(u̇
′) 12: if s > value(ṡ) : .",5.2 Change propagation (CP),[0],[0]
Increase value 13: value(ṡ) := s; witness(ṡ) := e 14: Q.push(ṡ) 15: else if witness(ṡ) = e and s < value(ṡ): 16: witness(ṡ) := NULL .Value,5.2 Change propagation (CP),[0],[0]
may decrease 17: Q.push(ṡ) .,5.2 Change propagation (CP),[0],[0]
"so, recompute upon pop 18: procedure RECOMPUTE(ṡ) 19: for e ∈ In(ṡ) : .",5.2 Change propagation (CP),[0],[0]
Max over incoming hyperedges 20: s := ∏ u̇∈e.,5.2 Change propagation (CP),[0],[0]
Tail value(u̇) 21: if s > value(ṡ) : 22: value(ṡ) = s; witness(ṡ) =,5.2 Change propagation (CP),[0],[0]
e,5.2 Change propagation (CP),[0],[0]
The naive rollout algorithm runs the parser T times— once for each variation of the pruning mask.,5.3 Dynamic programming (DP),[0],[0]
"The reader may be reminded of the finite difference approximation to the gradient of a function, which also measures the effects from perturbing each input value individually.",5.3 Dynamic programming (DP),[0],[0]
"In fact, for certain reward functions, the naive algorithm can be precisely regarded as computing a gradient—and thus we can use a more efficient algorithm, back-propagation, which finds the entire gradient vector of reward as fast (in the big-O sense) as computing the reward once.",5.3 Dynamic programming (DP),[0],[0]
"The overall algorithm is O(|E| + T ) where |E| is the total number of hyperedges, whereas the naive algorithm is O(|E′|·T ) where |E′| ≤ |E| is the maximum number of hyperedges actually visited on any rollout.
",5.3 Dynamic programming (DP),[0],[0]
What accuracy measure must we use?,5.3 Dynamic programming (DP),[0],[0]
Let r(d) denote the recall of a derivation d—the fraction of gold constituents that appear as vertices in the derivation.,5.3 Dynamic programming (DP),[0],[0]
"A simple accuracy metric would be 1-best recall, the recall r(d̂) of the highest-scoring derivation d̂ that was not pruned.",5.3 Dynamic programming (DP),[0],[0]
"In this section, we relax that to ex-
pected recall,6 r̄= ∑
d p(d)r(d).",5.3 Dynamic programming (DP),[0],[0]
"Here we interpret the pruned hypergraph’s values as an unnormalized probability distribution over derivations, where the probability p(d) =",5.3 Dynamic programming (DP),[0],[0]
p̃(d)/Z of a derivation is proportional to its score p̃(d) =,5.3 Dynamic programming (DP),[0],[0]
"∏ u̇∈leaves(d) value(u̇).
",5.3 Dynamic programming (DP),[0],[0]
"Though r̄ is not quite our evaluation metric, it captures more information about the parse forest, and so may offer some regularizing effect when used in a training criterion (see §7.1).",5.3 Dynamic programming (DP),[0],[0]
"In any case, r̄ is close to r(d̂) when probability mass is concentrated on a few derivations, which is common with heavy pruning.
",5.3 Dynamic programming (DP),[0],[0]
"We can re-express r̄ as r̃/Z, where
r̃ = ∑
d
p̃(d)r(d) Z = ∑
d
p̃(d) (2)
These can be efficiently computed by dynamic programming (DP), specifically by a variant of the inside algorithm (Li and Eisner, 2009).",5.3 Dynamic programming (DP),[0],[0]
"Since p̃(d) is a product of rule weights and pruning mask bits at d’s leaves (§5.1), each appearing at most once, both r̃ and Z vary linearly in any one of these inputs provided that all other inputs are held constant.",5.3 Dynamic programming (DP),[0],[0]
"Thus, the exact effect on r̃ or Z of changing an input mik can be found from the partial derivatives with respect to it.",5.3 Dynamic programming (DP),[0],[0]
"In particular, if we increased mik by ∆ ∈ {−1, 1} (to flip this bit), the new value of r̄ would be exactly
r̃ + ∆ · ∂r̃/∂mik",5.3 Dynamic programming (DP),[0],[0]
"Z + ∆ · ∂Z/∂mik
(3)
",5.3 Dynamic programming (DP),[0],[0]
It remains to compute these partial derivatives.,5.3 Dynamic programming (DP),[0],[0]
"All partials can be jointly computed by back-propagation, which equivalent to another dynamic program known as the outside algorithm (Eisner, 2016).
",5.3 Dynamic programming (DP),[0],[0]
"The inside algorithm only needs to visit the |E′| unpruned edges, but the outside algorithm must also visit some pruned edges, to determine the effect of “unpruning” them (changing their mik input from 0 to 1) by finding ∂r̃/∂mik and ∂Z/∂mik.",5.3 Dynamic programming (DP),[0],[0]
"On the other hand, these partials are 0 when some other input to the hyperedge is 0.",5.3 Dynamic programming (DP),[0],[0]
"This case is common when the hypergraph is heavily pruned (|E′| |E|), and means that back-propagation need not descend further through that hyperedge.
",5.3 Dynamic programming (DP),[0],[0]
"6In theory, we could anneal from expected to 1-best recall (Smith and Eisner, 2006).",5.3 Dynamic programming (DP),[0],[0]
"We experimented extensively with annealing but found it to be too numerically unstable for our purposes, even with high-precision arithmetic libraries.
",5.3 Dynamic programming (DP),[0],[0]
Note that the DP method computes only the accuracies of rollouts—not the runtimes.,5.3 Dynamic programming (DP),[0],[0]
"In this paper, we will combine DP with a very simple runtime measure that is trivial to roll out (see §7).",5.3 Dynamic programming (DP),[0],[0]
An alternative would be to use CP to roll out the runtimes.,5.3 Dynamic programming (DP),[0],[0]
"This is very efficient: to measure just runtime, CP only needs to update the record of which constituents or edges are built, and not their scores, so the changes are easier to compute than in §5.2, and peter out more quickly.
6 Parser details7
Setup: We use the standard English parsing setup: the Penn Treebank (Marcus et al., 1993) with the standard train/dev/test split, and standard tree normalization.8 For efficiency during training, we restrict the length of sentences to ≤ 40.",5.3 Dynamic programming (DP),[0],[0]
We do not restrict the length of test sentences.,5.3 Dynamic programming (DP),[0],[0]
"We experiment with two grammars: coarse, the “no frills” left-binarized treebank grammar, and fine, a variant of the Berkeley split-merge level-6 grammar (Petrov et al., 2006) as provided by Dunlop (2014, ch. 5).",5.3 Dynamic programming (DP),[0],[0]
The parsing algorithms used during training are described in §5.,5.3 Dynamic programming (DP),[0],[0]
"Our test-time parsing algorithm uses the left-child loop implementation of CKY (Dunlop et al., 2010).",5.3 Dynamic programming (DP),[0],[0]
All algorithms allow unary rules (though not chains).,5.3 Dynamic programming (DP),[0],[0]
"We evaluate accuracy at test time with the F1 score from the official EVALB script (Sekine and Collins, 1997).
",5.3 Dynamic programming (DP),[0],[0]
Training:,5.3 Dynamic programming (DP),[0],[0]
Note that we never retrain the grammar weights—we train only the pruning policy.,5.3 Dynamic programming (DP),[0],[0]
"To TRAIN our classifiers (Alg. 2 line 13), we use L2-regularized logistic regression, trained with L-BFGS optimization.",5.3 Dynamic programming (DP),[0],[0]
"We always rescale the example weights in the training set to sum to 1 (otherwise as LOLS proceeds, dataset aggregation overwhelms the regularizer).",5.3 Dynamic programming (DP),[0],[0]
"For the baseline (defined in next section), we determine the regularization coefficient by sweeping {2−11, 2−12, 2−13, 2−14, 2−15} and picking the best value (2−13) based on the dev frontier.",5.3 Dynamic programming (DP),[0],[0]
We re-used this regularization parameter for LOLS.,5.3 Dynamic programming (DP),[0],[0]
"The number of LOLS iterations is determined by a 6-day training-time limit9 (meaning some jobs run many
7Code for experiments is available at http://github.",5.3 Dynamic programming (DP),[0],[0]
"com/timvieira/learning-to-prune.
",5.3 Dynamic programming (DP),[0],[0]
8Data train/dev/test split (by section) 2–21 / 22 / 23.,5.3 Dynamic programming (DP),[0],[0]
"Normalization operations: Remove function tags, traces, spurious unary edges (X → X), and empty subtrees left by other operations.",5.3 Dynamic programming (DP),[0],[0]
"Relabel ADVP and PRT|ADVP tags to PRT.
",5.3 Dynamic programming (DP),[0],[0]
"9On the 7th day, LOLS rested and performance was good.
",5.3 Dynamic programming (DP),[0],[0]
fewer iterations than others).,5.3 Dynamic programming (DP),[0],[0]
For LOLS minibatch size we use 10K on the coarse grammar and 5K on the fine grammar.,5.3 Dynamic programming (DP),[0],[0]
"At line 15 of Alg. 2, we return the policy that maximized reward on development data, using the reward function from training.
",5.3 Dynamic programming (DP),[0],[0]
"Features: We use similar features to Bodenstab et al. (2011), but we have removed features that depend on part-of-speech tags.",5.3 Dynamic programming (DP),[0],[0]
"We use the following 16 feature templates for span (i, k) with 1 < k−i < N : bias, sentence length, boundary words, conjunctions of boundary words, conjunctions of word shapes, span shape, width bucket.",5.3 Dynamic programming (DP),[0],[0]
"Shape features map a word or phrase into a string of character classes (uppercase, lowercase, numeric, spaces); we truncate substrings of identical classes to length two; punctuation chars are never modified in any way.",5.3 Dynamic programming (DP),[0],[0]
"Width buckets use the following partition: 2, 3, 4, 5, [6, 10], [11, 20], [21,∞).",5.3 Dynamic programming (DP),[0],[0]
"We use feature hashing (Weinberger et al., 2009) with MurmurHash3 (Appleby, 2008) and project to 222 features.",5.3 Dynamic programming (DP),[0],[0]
"Conjunctions are taken at positions (i−1, i), (k, k+1), (i−1, k+1) and (i, k).",5.3 Dynamic programming (DP),[0],[0]
"We use special begin and end symbols when a template accesses positions beyond the sentence boundary.
",5.3 Dynamic programming (DP),[0],[0]
Hall et al. (2014) give examples motivating our feature templates and show experimentally that they are effective in multiple languages.,5.3 Dynamic programming (DP),[0],[0]
Boundary words are strong surface cues for phrase boundaries.,5.3 Dynamic programming (DP),[0],[0]
Span shape features are also useful as they (minimally) check for matched parentheses and quotation marks.,5.3 Dynamic programming (DP),[0],[0]
Reward functions and surrogates: Each user has a personal reward function.,7 Experimental design and results,[0],[0]
"In this paper, we choose to specify our true reward as accuracy − λ · runtime, where accuracy is given by labeled F1 percentage and runtime by mega-pushes (mpush), millions of calls per sentence to lines 6 and 19 of Alg. 1, which is in practice proportional to seconds per sentence (correlation > 0.95) and is more replicable.",7 Experimental design and results,[0],[0]
We evaluate accordingly (on test data)—but during LOLS training we approximate these metrics.,7 Experimental design and results,[0],[0]
"We compare:
• rCP (fast): Use change propagation (§5.2) to compute accuracy on a sentence as F1 of just that sentence, and to approximate runtime as ||β||0,
the number of constituents that were built.10
• rDP (faster): Use dynamic programming (§5.3) to approximate accuracy on a sentence as expected recall.11 This time we approximate runtime more crudely as ||m||0, the number of nonzeros in the pruning mask for the sentence (i.e., the number of spans whose constituents the policy would be willing to keep if they were built).
",7 Experimental design and results,[0],[0]
We use these surrogates because they admit efficient rollout algorithms.,7 Experimental design and results,[0],[0]
"Less important, they preserve the training objective (1) as an average over sentences.",7 Experimental design and results,[0],[0]
"(Our true F1 metric on a corpus cannot be computed in this way, though it could reasonably be estimated by averaging over minibatches of sentences in (1).)
",7 Experimental design and results,[0],[0]
"Controlled experimental design: Our baseline system is an adaptation of Bodenstab et al. (2011) to learning-to-prune, as described in §3 and §6.",7 Experimental design and results,[0],[0]
Our goal is to determine whether such systems can be improved by LOLS training.,7 Experimental design and results,[0],[0]
"We repeat the following design for both reward surrogates (rCP and rDP) and for both grammars (coarse and fine).
",7 Experimental design and results,[0],[0]
¬ We start by training a number of baseline models by sweeping the asymmetric weighting parameter.,7 Experimental design and results,[0],[0]
"For the coarse grammar we train 8 such models, and for the fine grammar 12.
 ",7 Experimental design and results,[0],[0]
"For each baseline policy, we estimate a value of λ for which that policy is optimal (among baseline policies) according to surrogate reward.12
10When using rCP, we speed up LOLS by doing≤ 2n rollouts per sentence of length n. We sample these uniformly without replacement from the T possible rollouts (§5), and compensate by upweighting the resulting training examples by T/(2n).
",7 Experimental design and results,[0],[0]
"11Considering all nodes in the binarized tree, except for the root, width-1 constituents, and children of unary rules.
",7 Experimental design and results,[0],[0]
"12We estimate λ by first fitting a parametric model yi = h(xi) , ymax · sigmoid(a · log(xi + c) + b) to the baseline runtime-accuracy measurements on dev data (shown in green in Fig. 2) by minimizing mean squared error.",7 Experimental design and results,[0],[0]
"We then use the fitted curve’s slope h′ to estimate each λi = h′(xi), where xi is the runtime of baseline i. The resulting choice of reward function y−λi",7 Experimental design and results,[0],[0]
"·x increases along the green arrow in Fig. 2, and is indeed maximized (subject to y ≤ h(x), and in the region where h is concave) at x = xi.",7 Experimental design and results,[0],[0]
"As a sanity check, notice since λi is a derivative of the function y = h(x), its units are in units of y (accuracy) per unit of x (runtime), as appropriate for use in the expression",7 Experimental design and results,[0],[0]
y,7 Experimental design and results,[0],[0]
− λi · x.,7 Experimental design and results,[0],[0]
"Indeed, this procedure will construct the same reward function regardless of the units we use to express x.",7 Experimental design and results,[0],[0]
"Our specific parametric model h is a sigmoidal curve, with
® For each baseline policy, we run LOLS with the same surrogate reward function (defined by λ) for which that baseline policy was optimal.",7 Experimental design and results,[0],[0]
We initialize LOLS by setting π0 to the baseline policy.,7 Experimental design and results,[0],[0]
"Furthermore, we include the baseline policy’s weighted training set Q̂0 in the ⋃ at line 13.
",7 Experimental design and results,[0],[0]
"Fig. 2 shows that LOLS learns to improve on the baseline, as evaluated on development data.
¯",7 Experimental design and results,[0],[0]
But do these surrogate reward improvements also improve our true reward?,7 Experimental design and results,[0],[0]
"For each baseline policy, we use dev data to estimate a value of λ for which that policy is optimal according to our true reward function.",7 Experimental design and results,[0],[0]
"We use blind test data to compare the baseline policy to its corresponding LOLS policy on this true reward function, testing significance with a paired permutation test.",7 Experimental design and results,[0],[0]
"The improvements hold up, as shown in Fig. 3.
",7 Experimental design and results,[0],[0]
"The rationale behind this design is that a user who actually wishes to maximize accuracy−λ·runtime, for some specific λ, could reasonably start by choosing the best baseline policy for this reward function, and then try to improve that baseline by running LOLS with the same reward function.",7 Experimental design and results,[0],[0]
"Our experiments show this procedure works for a range of λ values.
",7 Experimental design and results,[0],[0]
"In the real world, a user’s true objective might instead be some nonlinear function of runtime and accuracy.",7 Experimental design and results,[0],[0]
"For example, when accuracy is “good enough,” it may be more important to improve runtime, and vice-versa.",7 Experimental design and results,[0],[0]
LOLS could be used with such a nonlinear reward function as well.,7 Experimental design and results,[0],[0]
"In fact, a user does not even have to quantify their global preferences by writing down such a function.",7 Experimental design and results,[0],[0]
"Rather, they could select manually among the baseline policies, choosing one with an attractive speed-accuracy tradeoff, and then specify λ to indicate a local direction of desired improvement (like the green arrows in Fig. 2), modifying this direction periodically as LOLS runs.",7 Experimental design and results,[0],[0]
"As previous work has shown, learning to prune gives us excellent parsers with less than < 2% overhead
accuracy → ymax asymptotically as runtime → ∞. It obtains an excellent fit by placing accuracy and runtime on the loglogit scale—that is, log(xi + c) and logit(yi/ymax) transforms are used to convert our bounded random variables xi and yi to unbounded ones—and then assuming they are linearly related.
for deciding what to prune (i.e., pruning feature extraction and span classification).",7.1 Discussion,[0],[0]
"Even the baseline pruner has access to features unavailable to the grammar, and so it learns to override the grammar, improving an unpruned coarse parser’s accuracy from 61.1 to as high as 70.1% F1 on test data (i.e., beneficial search error).",7.1 Discussion,[0],[0]
"It is also 8.1x faster!13 LOLS simply does a better job at figuring out where to prune, raising accuracy 2.1 points to 72.2 (while maintaining a 7.4x speedup).",7.1 Discussion,[0],[0]
"Where pruning is more aggressive,
13We measure runtime as best of 10 runs (recommended by Dunlop (2014)).",7.1 Discussion,[0],[0]
"All parser timing experiments were performed on a Linux laptop with the following specs: Intel® Core™ i5-2540M 2.60GHz CPU, 8GB memory, 32K/256K/3072K L1/L2/L3 cache.",7.1 Discussion,[0],[0]
"Code is written in the Cython language.
",7.1 Discussion,[0],[0]
"LOLS has even more impact on accuracy.
",7.1 Discussion,[0],[0]
"Even on the fine grammar, where there is less room to improve accuracy, the most accurate LOLS system improves an unpruned parser by +0.16% F1 with a 8.6x speedup.",7.1 Discussion,[0],[0]
"For comparison, the most accurate baseline drops −0.03% F1 with a 9.7x speedup.
",7.1 Discussion,[0],[0]
"With the fine grammar, we do not see much improvement over the baseline in the accuracy > 85 regions.",7.1 Discussion,[0],[0]
This is because the supervision specified by asymmetric weighting is similar to what LOLS surmises via rollouts.,7.1 Discussion,[0],[0]
"However, in lower-accuracy regions we see that LOLS can significantly improve reward over its baseline policy.",7.1 Discussion,[0],[0]
"This is because the baseline supervision does not teach which plausible
constituents are “safest” to prune, nor can it learn strategies such as “skip all long sentences.”",7.1 Discussion,[0],[0]
"We discuss why LOLS does not help as much in the high accuracy regions further in §7.3.
",7.1 Discussion,[0],[0]
"In a few cases in Fig. 2, LOLS finds no policy that improves surrogate reward on dev data.",7.1 Discussion,[0],[0]
"In these cases, surrogate reward does improve slightly on training data (not shown), but early stopping just keeps the initial (baseline) policy since it is just as good on dev data.",7.1 Discussion,[0],[0]
"Adding a bit of additional random exploration might help break out of this initialization.
",7.1 Discussion,[0],[0]
"Interestingly, the rDP LOLS policies find higheraccuracy policies than the corresponding rCP policies, despite a greater mismatch in surrogate accuracy definitions.",7.1 Discussion,[0],[0]
"We suspect that rDP’s approach of trying to improve expected accuracy may provide a useful regularizing effect, which smooths out the reward signal and provides a useful bias (§5.3).
",7.1 Discussion,[0],[0]
"The most pronounced qualitative difference due to LOLS training is substantially lower rates of parse failure in the mid- to high- λ-range on both grammars
(not shown).",7.1 Discussion,[0],[0]
"Since LOLS does end-to-end training, it can advise the learner that a certain pruning decision catastrophically results in no parse being found.",7.1 Discussion,[0],[0]
Part of the contribution of this paper is faster algorithms for performing LOLS rollouts during training (§5).,7.2 Training speed and convergence,[0],[0]
"Compared to the naive strategy of running the parser from scratch T times, rCP achieves speedups of 4.9–6.6x on the coarse grammar and 1.9–2.4x on the fine grammar.",7.2 Training speed and convergence,[0],[0]
"rDP is even faster, 10.4–11.9x on coarse and 10.5–13.8x on fine.",7.2 Training speed and convergence,[0],[0]
"Most of the speedup comes from longer sentences, which take up most of the runtime for all methods.",7.2 Training speed and convergence,[0],[0]
Our new algorithms enable us to train on fairly long sentences (≤ 40).,7.2 Training speed and convergence,[0],[0]
"We note that our implementations of rCP and rDP are not as highly optimized as our test-time parser, so there may be room for improvement.
",7.2 Training speed and convergence,[0],[0]
Orthogonal to the cost per rollout is the number of training iterations.,7.2 Training speed and convergence,[0],[0]
"LOLS may take many steps to converge if trajectories are long (i.e., T is large)
because each iteration of LOLS training attempts to improve the current policy by a single action.",7.2 Training speed and convergence,[0],[0]
"In our setting, T is quite large (discussed extensively in §5), but we are able to circumvent slow convergence by initializing the policy (via the baseline method).",7.2 Training speed and convergence,[0],[0]
This means that LOLS can focus on fine-tuning a policy which is already quite good.,7.2 Training speed and convergence,[0],[0]
"In fact, in 4 cases, LOLS did not improve from its initial policy.
",7.2 Training speed and convergence,[0],[0]
We find that when λ is large—the cases where we get meaningful improvements because the initial policy is far from locally optimal—LOLS steadily and smoothly improves the surrogate reward on both training and development data.,7.2 Training speed and convergence,[0],[0]
"Because these are fast parsers, LOLS was able to run on the order of 10 (fine grammar) or 100 (coarse grammar) epochs within our 6-day limit; usually it was still improving when we terminated it.",7.2 Training speed and convergence,[0],[0]
"By contrast, for the slower and more accurate small-λ parsers (which completed fewer training epochs), LOLS still improves surrogate reward on training data, but without systematically improving on development data—often the reward on development fluctuates, and early stopping simply picks the best of this small set of “random” variants.",7.2 Training speed and convergence,[0],[0]
"In §3, we argued that LOLS gives a more appropriate training signal for pruning than the baseline method of consulting the gold parse, because it uses rollouts to measure the full effect of each pruning decision in the context of the other decisions made by the policy.
",7.3 Understanding the LOLS training signal,[0],[0]
"To better understand the results of our previous experiments, we analyze how often a rollout does determine that the baseline supervision for a span is suboptimal, and how suboptimal it is in those cases.
",7.3 Understanding the LOLS training signal,[0],[0]
We specifically consider LOLS rollouts that evaluate the rCP surrogate (because rDP is a cruder approximation to true reward).,7.3 Understanding the LOLS training signal,[0],[0]
"These rollouts Q̂i tell us what actions LOLS is trying to improve in its current policy πi for a given λ, although there is no guarantee that the learner in §4 will succeed at classifying Q̂i correctly (due to limited features, regularization, and the effects of dataset aggregation).
",7.3 Understanding the LOLS training signal,[0],[0]
We define regret of the baseline oracle.,7.3 Understanding the LOLS training signal,[0],[0]
"Let best(s) , argmaxaROLLOUT(π, s, a) and regret(s) , (ROLLOUT(π, s, best(s) − ROLLOUT(π, s, gold(s)))).",7.3 Understanding the LOLS training signal,[0],[0]
"Note that regret(s)≥0 for all s, and let diff(s) be the event that regret(s) > 0 strictly.",7.3 Understanding the LOLS training signal,[0],[0]
"We are interested in analyzing the expected regret over all gold and
non-gold spans, which we break down as
E[regret] = p(diff) (4) · ( p(gold | diff) · E[regret | gold, diff] + p(¬ gold | diff) · E[regret | ¬ gold, diff] )
where expectations are taken over s ∼ ROLL-IN(π).",7.3 Understanding the LOLS training signal,[0],[0]
"Empirical analysis of regret: To show where the benefit of the LOLS oracle comes from, Fig. 4 graphs the various quantities that enter into the definition (4) of baseline regret, for different π, λ, and grammar.",7.3 Understanding the LOLS training signal,[0],[0]
"The LOLS oracle evolves along with the policy π, since it identifies the best action given π.",7.3 Understanding the LOLS training signal,[0],[0]
"We thus evaluate the oracle baseline against two LOLS oracles: the one used at the start of LOLS training (derived from the initial policy π1 that was trained on baseline supervision), and the one obtained at the end (derived from the LOLS-trained policy π∗ selected by early stopping).",7.3 Understanding the LOLS training signal,[0],[0]
"These comparisons are shown by solid and dashed lines respectively.
",7.3 Understanding the LOLS training signal,[0],[0]
"Class imbalance (black curves): In all graphs, the aggregate curves primarily reflect the non-gold spans, since only 8% of spans are gold.
",7.3 Understanding the LOLS training signal,[0],[0]
"Gold spans (gold curves): The top graphs show that a substantial fraction of the gold spans should be pruned (whereas the baseline tries to keep them all), although the middle row shows that the benefit of pruning them is small.",7.3 Understanding the LOLS training signal,[0],[0]
"In most of these cases, pruning a gold span improves speed but leaves accuracy unchanged—because that gold span was missed anyway by the highest-scoring parse.",7.3 Understanding the LOLS training signal,[0],[0]
Such cases become both more frequent and more beneficial as λ increases and we prune more heavily.,7.3 Understanding the LOLS training signal,[0],[0]
"In a minority of cases, however, pruning a gold span also improves accuracy (through beneficial search error).
",7.3 Understanding the LOLS training signal,[0],[0]
"Non-gold spans (purple curves): Conversely, the top graphs show that a few non-gold spans should be kept (whereas the baseline tries to prune them all), and the middle row shows a large benefit from keeping them.",7.3 Understanding the LOLS training signal,[0],[0]
"They are needed to recover from catastrophic errors and get a mostly-correct parse.
",7.3 Understanding the LOLS training signal,[0],[0]
Coarse vs. fine (left vs. right):,7.3 Understanding the LOLS training signal,[0],[0]
"The two grammars differ mainly for small λ, and this difference comes especially from the top row.",7.3 Understanding the LOLS training signal,[0],[0]
"With a fine grammar and small λ, the baseline parses are more accurate, so LOLS has less room for improvement: fewer
gold spans go unused, and fewer non-gold spans are needed for recovery.
",7.3 Understanding the LOLS training signal,[0],[0]
"Effect of λ: Aggressive pruning (large λ) reduces accuracy, so its effect on the top row is similar to that of using a coarse grammar.",7.3 Understanding the LOLS training signal,[0],[0]
"Aggressive pruning also has an effect on the middle row: there is more benefit to be derived from pruning unused gold spans (surprisingly), and especially from keeping those non-gold spans that are helpful (presumably they enable recovery from more severe parse errors).",7.3 Understanding the LOLS training signal,[0],[0]
"These effects are considerably sharper with rDP reward (not shown here), which more smoothly evaluates the entire weighted pruned parse forest rather than trying to coordinate actions to ensure a good single 1-best tree; the baseline oracle is excellent at choosing the action that gets the better forest when the forest is mostly present (small λ) but not when it is mostly pruned (large λ).
",7.3 Understanding the LOLS training signal,[0],[0]
Effect on retraining the policy: The black lines in the bottom graphs show the overall regret (on training data) if we were to perfectly follow the baseline oracle rather than the LOLS oracle.,7.3 Understanding the LOLS training signal,[0],[0]
"In practice, retraining the policy to match the oracle will not match it perfectly in either case.",7.3 Understanding the LOLS training signal,[0],[0]
"Thus the baseline method has a further disadvantage: when it trains a policy, its training objective weights all gold or all non-gold examples equally, whereas LOLS invests greater effort in matching the oracle on those states where doing so would give greater downstream reward.",7.3 Understanding the LOLS training signal,[0],[0]
Our experiments have focused on using LOLS to improve a reasonable baseline.,8 Related work,[0],[0]
Fig. 5 shows that our resulting parser fits reasonably among state-of-the-art constituency parsers trained and tested on the Penn Treebank.,8 Related work,[0],[0]
These parsers include a variety of techniques that improve speed or accuracy.,8 Related work,[0],[0]
"Many are quite orthogonal to our work here—e.g., the SpMV method (which is necessary for Bodenstab’s parser to beat ours) is a set of cache-efficient optimizations (Dunlop, 2014) that could be added to our parser (just as it was added to Bodenstab’s), while Hall et al. (2014) and Fernández-González and Martins (2015) replace the grammar with faster scoring models that have more conditional independence.",8 Related work,[0],[0]
"Overall, other fast parsers could also be trained using LOLS, so that
they quickly find parses that are accurate, or at least helpful to the accuracy of some downstream task.
",8 Related work,[0],[0]
"Pruning methods14 can use classifiers not only to select spans but also to prune at other granularities (Roark and Hollingshead, 2008; Bodenstab et al., 2011).",8 Related work,[0],[0]
"Prioritization methods do not prune substructures, but instead delay their processing until they are needed—if ever (Caraballo and Charniak, 1998).
",8 Related work,[0],[0]
This paper focuses on learning pruning heuristics that have trainable parameters.,8 Related work,[0],[0]
"In the same way, Stoyanov and Eisner (2012) learn to turn off unneeded factors in a graphical model, and Jiang et al. (2012) and Berant and Liang (2015) train prioritization heuristics (using policy gradient).",8 Related work,[0],[0]
"In both of those 2012 papers, we explicitly sought to maximize accuracy − λ · runtime as we do here.",8 Related work,[0],[0]
"Some previous “coarse-to-fine” work does not optimize heuris-
14We focus here on parsing, but pruning is generally useful in structured prediction.",8 Related work,[0],[0]
"E.g., Xu et al. (2013) train a classifier to prune (latent) alignments in a machine translation system.
tics directly but rather derives heuristics for pruning (Charniak et al., 2006; Petrov and Klein, 2007; Weiss and Taskar, 2010; Rush and Petrov, 2012) or prioritization (Klein and Manning, 2003; Pauls and Klein, 2009) from a coarser version of the model.",8 Related work,[0],[0]
"Combining these automatic methods with LOLS would require first enriching their heuristics with trainable parameters, or parameterizing the coarse-to-fine hierarchy itself as in the “feature pruning” work of He et al. (2013) and Strubell et al. (2015).
",8 Related work,[0],[0]
Dynamic features are ones that depend on previous actions.,8 Related work,[0],[0]
"In our setting, a policy could in principle benefit from considering the full state of the chart at Alg. 1 line 14.",8 Related work,[0],[0]
"While coarse-to-fine methods implicitly use certain dynamic features, training with dynamic features is a fairly new goal that is challenging to treat efficiently.",8 Related work,[0],[0]
"It has usually been treated with some form of simple imitation learning, using a heuristic training signal much as in our baseline (Jiang, 2014; He et al., 2013).",8 Related work,[0],[0]
"LOLS would be a more principled way to train such features, but for efficiency, our present paper restricts to static features that only access the state via π(w, i, k).",8 Related work,[0],[0]
This permits our fast CP and DP rollout algorithms.,8 Related work,[0],[0]
"It also reduces the time and space cost of dataset aggregation.15
LOLS attempts to do end-to-end training of a sequential decision-making system, without falling back on black-box optimization tools (Och, 2003; Chung and Galley, 2012) that ignore the sequential structure.",8 Related work,[0],[0]
"In NLP, sequential decisions are more commonly trained with step-by-step supervision
15LOLS repeatedly evaluates actions given (w, i, k).",8 Related work,[0],[0]
"We consolidate the resulting training examples by summing their reward vectors r̂, so the aggregated dataset does not grow over time.
",8 Related work,[0],[0]
"(Kuhlmann et al., 2011), using methods such as local classification (Punyakanok and Roth, 2001) or beam search with early update (Collins and Roark, 2004).",8 Related work,[0],[0]
LOLS tackles the harder setting where the only training signal is a joint assessment of the entire sequence of actions.,8 Related work,[0],[0]
"It is an alternative to policy gradient, which does not scale well to our long trajectories because of high variance in the estimated gradient and because random exploration around (even good) pruning policies most often results in no parse at all.",8 Related work,[0],[0]
"LOLS uses controlled comparisons, resulting in more precise “credit assignment” and tighter exploration.
",8 Related work,[0],[0]
"We would be remiss not to note that current transition-based parsers—for constituency parsing (Zhu et al., 2013; Crabbé, 2015) as well as dependency parsing (Chen and Manning, 2014)—are both incredibly fast and surprisingly accurate.",8 Related work,[0],[0]
"This may appear to undermine the motivation for our work, or at least for its application to fast parsing.16",8 Related work,[0],[0]
"However, transition-based parsers do not produce marginal probabilities of substructures, which can be useful features for downstream tasks.",8 Related work,[0],[0]
"Indeed, the transitionbased approach is essentially greedy and so it may fail on tasks with more ambiguity than parsing.",8 Related work,[0],[0]
"Current transition-based parsers also require step-by-step supervision, whereas our method can also be used to train in the presence of incomplete supervision, latent structure, or indirect feedback.",8 Related work,[0],[0]
"Our method could also be used immediately to speed up dynamic programming methods for MT, synchronous parsing, parsing with non-context-free grammar formalisms, and other structured prediction problems for which transition systems have not (yet) been designed.",8 Related work,[0],[0]
We presented an approach to learning pruning policies that optimizes end-to-end performance on a userspecified speed-accuracy tradeoff.,9 Conclusions,[0],[0]
We developed two novel algorithms for efficiently measuring how varying policy actions affects reward.,9 Conclusions,[0],[0]
"In the case of parsing, given a performance criterion and a good baseline policy for that criterion, the learner consistently manages to find a higher-reward policy.",9 Conclusions,[0],[0]
"We hope this work inspires a new generation of fast and accurate structured prediction models with tunable runtimes.
16Of course, LOLS can also train transition-based parsers (Chang et al., 2015a), or even vary their beam width dynamically.",9 Conclusions,[0],[0]
This material is based in part on research sponsored by the National Science Foundation under Grant No. 0964681 and DARPA under agreement number FA8750-13-2-0017 (DEFT program).,Acknowledgments,[0],[0]
"We’d like to thank Nathaniel Wesley Filardo, Adam Teichert, Matt Gormley and Hal Daumé III for helpful discussions.",Acknowledgments,[0],[0]
"Finally, we thank TACL action editor Marco Kuhlmann and the anonymous reviewers and copy editor for suggestions that improved this paper.",Acknowledgments,[0],[0]
Pruning hypotheses during dynamic programming is commonly used to speed up inference in settings such as parsing.,abstractText,[0],[0]
"Unlike prior work, we train a pruning policy under an objective that measures end-to-end performance: we search for a fast and accurate policy.",abstractText,[0],[0]
"This poses a difficult machine learning problem, which we tackle with the LOLS algorithm.",abstractText,[0],[0]
"LOLS training must continually compute the effects of changing pruning decisions: we show how to make this efficient in the constituency parsing setting, via dynamic programming and change propagation algorithms.",abstractText,[0],[0]
"We find that optimizing end-to-end performance in this way leads to a better Pareto frontier—i.e., parsers which are more accurate for a given runtime.",abstractText,[0],[0]
Learning to Prune: Exploring the Frontier of Fast and Accurate Parsing,title,[0],[0]
Deep neural networks (DNNs) have been widely used for machine learning applications due to their powerful capacity for modeling complex input patterns.,1. Introduction,[0],[0]
"Despite their success, it has been shown that DNNs are prone to training set biases, i.e. the training set is drawn from a joint distribution p(x, y) that is different from the distribution p(xv, yv) of the evaluation set.",1. Introduction,[0],[0]
"This distribution mismatch could have many
1Uber Advanced Technologies Group, Toronto ON, CANADA 2Department of Computer Science, University of Toronto, Toronto ON, CANADA.",1. Introduction,[0],[0]
"Correspondence to: Mengye Ren <mren3@uber.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
different forms.",1. Introduction,[0],[0]
Class imbalance in the training set is a very common example.,1. Introduction,[0],[0]
"In applications such as object detection in the context of autonomous driving, the vast majority of the training data is composed of standard vehicles but models also need to recognize rarely seen classes such as emergency vehicles or animals with very high accuracy.",1. Introduction,[0],[0]
"This will sometime lead to biased training models that do not perform well in practice.
",1. Introduction,[0],[0]
Another popular type of training set bias is label noise.,1. Introduction,[0],[0]
"To train a reasonable supervised deep model, we ideally need a large dataset with high-quality labels, which require many passes of expensive human quality assurance (QA).",1. Introduction,[0],[0]
"Although coarse labels are cheap and of high availability, the presence of noise will hurt the model performance, e.g. Zhang et al. (2017) has shown that a standard CNN can fit any ratio of label flipping noise in the training set and eventually leads to poor generalization performance.
",1. Introduction,[0],[0]
"Training set biases and misspecification can sometimes be addressed with dataset resampling (Chawla et al., 2002), i.e. choosing the correct proportion of labels to train a network on, or more generally by assigning a weight to each example and minimizing a weighted training loss.",1. Introduction,[0],[0]
"The example weights are typically calculated based on the training loss, as in many classical algorithms such as AdaBoost (Freund & Schapire, 1997), hard negative mining (Malisiewicz et al., 2011), self-paced learning (Kumar et al., 2010), and other more recent work (Chang et al., 2017; Jiang et al., 2017).
",1. Introduction,[0],[0]
"However, there exist two contradicting ideas in training loss based approaches.",1. Introduction,[0],[0]
"In noisy label problems, we prefer examples with smaller training losses as they are more likely to be clean images; yet in class imbalance problems, algorithms such as hard negative mining (Malisiewicz et al., 2011) prioritize examples with higher training loss since they are more likely to be the minority class.",1. Introduction,[0],[0]
"In cases when the training set is both imbalanced and noisy, these existing methods would have the wrong model assumptions.",1. Introduction,[0],[0]
"In fact, without a proper definition of an unbiased test set, solving the training set bias problem is inherently ill-defined.",1. Introduction,[0],[0]
"As the model cannot distinguish the right from the wrong, stronger regularization can usually work surprisingly well in certain synthetic noise settings.",1. Introduction,[0],[0]
"Here we argue that in order to learn general forms of training set biases, it is necessary to have a small unbiased validation to guide training.",1. Introduction,[0],[0]
"It is actually
not uncommon to construct a dataset with two parts - one relatively small but very accurately labeled, and another massive but coarsely labeled.",1. Introduction,[0],[0]
"Coarse labels can come from inexpensive crowdsourcing services or weakly supervised data (Cordts et al., 2016; Russakovsky et al., 2015; Chen & Gupta, 2015).
",1. Introduction,[0],[0]
"Different from existing training loss based approaches, we follow a meta-learning paradigm and model the most basic assumption instead: the best example weighting should minimize the loss of a set of unbiased clean validation examples that are consistent with the evaluation procedure.",1. Introduction,[0],[0]
"Traditionally, validation is performed at the end of training, which can be prohibitively expensive if we treat the example weights as some hyperparameters to optimize; to circumvent this, we perform validation at every training iteration to dynamically determine the example weights of the current batch.",1. Introduction,[0.9504750622364233],"['Although we have trained our multimodal network to compute embeddings at the granularity of entire images and entire caption spectrograms, we can easily apply it in a more localized fashion.']"
"Towards this goal, we propose an online reweighting method that leverages an additional small validation set and adaptively assigns importance weights to examples in every iteration.",1. Introduction,[0],[0]
We experiment with both class imbalance and corrupted label problems and find that our approach significantly increases the robustness to training set biases.,1. Introduction,[0],[0]
The idea of weighting each training example has been well studied in the literature.,2. Related Work,[0],[0]
"Importance sampling (Kahn & Marshall, 1953), a classical method in statistics, assigns weights to samples in order to match one distribution to another.",2. Related Work,[0],[0]
"Boosting algorithms such as AdaBoost (Freund & Schapire, 1997), select harder examples to train subsequent classifiers.",2. Related Work,[0],[0]
"Similarly, hard example mining (Malisiewicz et al., 2011), downsamples the majority class and exploits the most difficult examples.",2. Related Work,[0],[0]
"Focal loss (Lin et al., 2017) adds a soft weighting scheme that emphasizes harder examples.
",2. Related Work,[0],[0]
Hard examples are not always preferred in the presence of outliers and noise processes.,2. Related Work,[0],[0]
Robust loss estimators typically downweigh examples with high loss.,2. Related Work,[0],[0]
"In selfpaced learning (Kumar et al., 2010), example weights are obtained through optimizing the weighted training loss encouraging learning easier examples first.",2. Related Work,[0],[0]
"In each step, the learning algorithm jointly solves a mixed integer program that iterates optimizing over model parameters and binary example weights.",2. Related Work,[0],[0]
"Various regularization terms on the example weights have since been proposed to prevent overfitting and trivial solutions of assigning weights to be all zeros (Kumar et al., 2010; Ma et al., 2017; Jiang et al., 2015).",2. Related Work,[0],[0]
Wang et al. (2017) proposed a Bayesian method that infers the example weights as latent variables.,2. Related Work,[0],[0]
"More recently, Jiang et al. (2017) proposed to use a meta-learning LSTM to output the weights of the examples based on the training loss.",2. Related Work,[0],[0]
"Reweighting examples is also related to curriculum learning (Bengio et al., 2009), where the model reweights
among many available tasks.",2. Related Work,[0],[0]
"Similar to self-paced learning, typically it is beneficial to start with easier examples.
",2. Related Work,[0],[0]
One crucial advantage of reweighting examples is robustness against training set bias.,2. Related Work,[0],[0]
"There has also been a multitude of prior studies on class imbalance problems, including using dataset resampling (Chawla et al., 2002; Dong et al., 2017), cost-sensitive weighting (Ting, 2000; Khan et al., 2015), and structured margin based objectives (Huang et al., 2016).",2. Related Work,[0],[0]
"Meanwhile, the noisy label problem has been thoroughly studied by the learning theory community (Natarajan et al., 2013; Angluin & Laird, 1988) and practical methods have also been proposed (Reed et al., 2014; Sukhbaatar & Fergus, 2014; Xiao et al., 2015; Azadi et al., 2016; Goldberger & Ben-Reuven, 2017; Li et al., 2017; Jiang et al., 2017; Vahdat, 2017; Hendrycks et al., 2018).",2. Related Work,[0],[0]
"In addition to corrupted data, Koh & Liang (2017); Muñoz-González et al. (2017) demonstrate the possibility of a dataset adversarial attack (i.e. dataset poisoning).
",2. Related Work,[0],[0]
"Our method improves the training objective through a weighted loss rather than an average loss and is an instantiation of meta-learning (Thrun & Pratt, 1998; Lake et al., 2017; Andrychowicz et al., 2016), i.e. learning to learn better.",2. Related Work,[0],[0]
"Using validation loss as the meta-objective has been explored in recent meta-learning literature for few-shot learning (Ravi & Larochelle, 2017; Ren et al., 2018; Lorraine & Duvenaud, 2018), where only a handful of examples are available for each class.",2. Related Work,[0],[0]
"Our algorithm also resembles MAML (Finn et al., 2017) by taking one gradient descent step on the meta-objective for each iteration.",2. Related Work,[0],[0]
"However, different from these meta-learning approaches, our reweighting method does not have any additional hyperparameters and circumvents an expensive offline training stage.",2. Related Work,[0],[0]
"Hence, our method can work in an online fashion during regular training.",2. Related Work,[0],[0]
"In this section, we derive our model from a meta-learning objective towards an online approximation that can fit into any regular supervised training.",3. Learning to Reweight Examples,[0],[0]
We give a practical implementation suitable for any deep network type and provide theoretical guarantees under mild conditions that our algorithm has a convergence rate of O(1/ 2).,3. Learning to Reweight Examples,[0],[0]
Note that this is the same as that of stochastic gradient descent (SGD).,3. Learning to Reweight Examples,[0],[0]
"Let (x, y) be an input-target pair, and {(xi, yi), 1 ≤",3.1. From a meta-learning objective to an online approximation,[0],[0]
i ≤ N} be the training set.,3.1. From a meta-learning objective to an online approximation,[0],[0]
"We assume that there is a small unbiased and clean validation set {(xvi , yvi ), 1 ≤ i ≤M}, and M N .",3.1. From a meta-learning objective to an online approximation,[0],[0]
"Hereafter, we will use superscript v to denote validation set and subscript i to denote the ith data.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"We also assume
that the training set contains the validation set; otherwise, we can always add this small validation set into the training set and leverage more information during training.
",3.1. From a meta-learning objective to an online approximation,[0],[0]
"Let Φ(x, θ) be our neural network model, and θ be the model parameters.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"We consider a loss function C(ŷ, y) to minimize during training, where ŷ = Φ(x, θ).
",3.1. From a meta-learning objective to an online approximation,[0],[0]
"In standard training, we aim to minimize the expected loss for the training set: 1N ∑N i=1",3.1. From a meta-learning objective to an online approximation,[0],[0]
"C(ŷi, yi) = 1 N ∑N i=1 fi(θ), where each input example is weighted equally, and fi(θ) stands for the loss function associating with data xi.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"Here we aim to learn a reweighting of the inputs, where we minimize a weighted loss:
θ∗(w) = arg min θ N∑ i=1",3.1. From a meta-learning objective to an online approximation,[0],[0]
"wifi(θ), (1)
with wi unknown upon beginning.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"Note that {wi}Ni=1 can be understood as training hyperparameters, and the optimal selection of w is based on its validation performance:
w∗ = arg min w,w≥0
1
M M∑ i=1",3.1. From a meta-learning objective to an online approximation,[0],[0]
fvi,3.1. From a meta-learning objective to an online approximation,[0],[0]
(θ ∗(w)).,3.1. From a meta-learning objective to an online approximation,[0],[0]
"(2)
It is necessary that wi ≥ 0 for all i, since minimizing the negative training loss can usually result in unstable behavior.
",3.1. From a meta-learning objective to an online approximation,[0],[0]
"Online approximation Calculating the optimal wi requires two nested loops of optimization, and every single loop can be very expensive.",3.1. From a meta-learning objective to an online approximation,[0],[0]
The motivation of our approach is to adapt online w through a single optimization loop.,3.1. From a meta-learning objective to an online approximation,[0],[0]
"For each training iteration, we inspect the descent direction of some training examples locally on the training loss surface and reweight them according to their similarity to the descent direction of the validation loss surface.
",3.1. From a meta-learning objective to an online approximation,[0],[0]
"For most training of deep neural networks, SGD or its variants are used to optimize such loss functions.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"At every step t of training, a mini-batch of training examples {(xi, yi), 1 ≤ i ≤ n} is sampled, where n is the mini-batch size, n N .",3.1. From a meta-learning objective to an online approximation,[0],[0]
Then the parameters are adjusted according to the descent direction of the expected loss on the mini-batch.,3.1. From a meta-learning objective to an online approximation,[0],[0]
"Let’s consider vanilla SGD:
θt+1 = θt − α∇
( 1
n n∑ i=1",3.1. From a meta-learning objective to an online approximation,[0],[0]
"fi(θt)
) , (3)
where α is the step size.
",3.1. From a meta-learning objective to an online approximation,[0],[0]
We want to understand what would be the impact of training example,3.1. From a meta-learning objective to an online approximation,[0],[0]
"i towards the performance of the validation set at training step t. Following a similar analysis to Koh & Liang (2017), we consider perturbing the weighting by i for each
training example in the mini- batch,
fi, (θ) = ifi(θ), (4)
θ̂t+1( ) = θt − α∇ n∑ i=1",3.1. From a meta-learning objective to an online approximation,[0],[0]
"fi, (θ) ∣∣∣ θ=θt .",3.1. From a meta-learning objective to an online approximation,[0],[0]
"(5)
We can then look for the optimal ∗ that minimizes the validation loss fv locally at step t:
∗t = arg min
1
M M∑ i=1",3.1. From a meta-learning objective to an online approximation,[0],[0]
fvi (θt+1( )).,3.1. From a meta-learning objective to an online approximation,[0],[0]
"(6)
Unfortunately, this can still be quite time-consuming.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"To get a cheap estimate of wi at step t, we take a single gradient descent step on a mini-batch of validation samples wrt. t, and then rectify the output to get a non-negative weighting:
ui,t = −η ∂
∂",3.1. From a meta-learning objective to an online approximation,[0],[0]
"i,t
1
m m∑ j=1 fvj (θt+1( )) ∣∣∣",3.1. From a meta-learning objective to an online approximation,[0],[0]
"i,t=0 , (7)
w̃i,t = max(ui,t, 0).",3.1. From a meta-learning objective to an online approximation,[0],[0]
"(8)
where η is the descent step size on .
",3.1. From a meta-learning objective to an online approximation,[0],[0]
"To match the original training step size, in practice, we can consider normalizing the weights of all examples in a training batch so that they sum up to one.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"In other words, we choose to have a hard constraint within the set {w : ‖w‖1 = 1} ∪ {0}.
wi,t = w̃i,t ( ∑ j w̃j,t) + δ",3.1. From a meta-learning objective to an online approximation,[0],[0]
"( ∑ j w̃j,t) , (9)
where δ(·) is to prevent the degenerate case when all wi’s in a mini-batch are zeros, i.e. δ(a) = 1 if a = 0, and equals to 0 otherwise.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"Without the batch-normalization step, it is possible that the algorithm modifies its effective learning rate of the training progress, and our one-step look ahead may be too conservative in terms of the choice of learning rate (Wu et al., 2018).",3.1. From a meta-learning objective to an online approximation,[0],[0]
"Moreover, with batch normalization, we effectively cancel the meta learning rate parameter η.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"In this section, we study how to compute wi,t in a multilayer perceptron (MLP) network.",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
One of the core steps is to compute the gradients of the validation loss wrt.,3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"the local perturbation , We can consider a multi-layered network where we have parameters for each layer θ = {θl}Ll=1, and at every layer, we first compute zl the pre-activation, a weighted sum of inputs to the layer, and afterwards we apply a non-linear activation function σ to obtain z̃l the post-activation:
zl = θ > l z̃l−1, (10)
z̃l = σ(zl).",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"(11)
During backpropagation, let gl be the gradients of loss wrt. zl, and the gradients wrt.",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
θl is given by z̃l−1g>l .,3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"We can further express the gradients towards as a sum of local dot products.
∂ ∂",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"i,t E [ fv(θt+1( )) ∣∣∣",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"i,t=0 ] ∝− 1
m m∑ j=1 ∂fvj (θ) ∂θ ∣∣∣",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
>,3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
θ=θt ∂fi(θ) ∂θ ∣∣∣ θ,3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"=θt
=− 1 m m∑ j=1 L∑ l=1",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"(z̃vj,l−1 >z̃i,l−1)(g v j,l >gi,l).
",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"(12)
Detailed derivations can be found in Supplementary Materials.",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
Eq. 12 suggests that the meta-gradient on is composed of the sum of the products of two terms: z>zv and g>gv.,3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"The first dot product computes the similarity between the training and validation inputs to the layer, while the second computes the similarity between the training and validation gradient directions.",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"In other words, suppose that a pair of training and validation examples are very similar, and they also provide similar gradient directions, then this training example is helpful and should be up-weighted, and conversely, if they provide opposite gradient directions, this training example is harmful and should be downweighed.",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"In an MLP and a CNN, the unnormalized weights can be calculated based on the sum of the correlations of layerwise activation gradients and input activations.",3.3. Implementation using automatic differentiation,[0],[0]
"In more general networks, we can leverage automatic differentiation techniques to compute the gradient of the validation loss wrt.",3.3. Implementation using automatic differentiation,[0],[0]
the example weights of the current batch.,3.3. Implementation using automatic differentiation,[0],[0]
"As shown in Figure 1, to get the gradients of the example weights, one needs to first unroll the gradient graph of the training batch, and then use backward-on-backward automatic differentiation to take a second order gradient
pass (see Step 5 in Figure 1).",3.3. Implementation using automatic differentiation,[0],[0]
We list detailed step-bystep pseudo-code in Algorithm 1.,3.3. Implementation using automatic differentiation,[0],[0]
"This implementation can be generalized to any deep learning architectures and can be very easily implemented using popular deep learning frameworks such as TensorFlow (Abadi et al., 2016).
",3.3. Implementation using automatic differentiation,[0],[0]
"Algorithm 1 Learning to Reweight Examples using Automatic Differentiation Require: θ0, Df , Dg , n, m Ensure: θT
1: for t = 0 ...",3.3. Implementation using automatic differentiation,[0],[0]
T,3.3. Implementation using automatic differentiation,[0],[0]
"− 1 do 2: {Xf , yf} ← SampleMiniBatch(Df , n) 3: {Xg, yg} ← SampleMiniBatch(Dg , m) 4: ŷf ← Forward(Xf , yf , θt) 5: ← 0; lf ← ∑n i=1",3.3. Implementation using automatic differentiation,[0],[0]
"iC(yf,i, ŷf,i) 6: ∇θt ← BackwardAD(lf , θt) 7: θ̂t ← θt − α∇θt 8: ŷg",3.3. Implementation using automatic differentiation,[0],[0]
"← Forward(Xg, yg, θ̂t) 9: lg ← 1m ∑m i=1",3.3. Implementation using automatic differentiation,[0],[0]
"C(yg,i, ŷg,i)
10: ∇ ← BackwardAD(lg, ) 11:",3.3. Implementation using automatic differentiation,[0],[0]
"w̃ ← max(−∇ , 0); w ← w̃∑
j w̃+δ( ∑ j w̃)
12: l̂f ← ∑n i=1",3.3. Implementation using automatic differentiation,[0],[0]
"wiC(yi, ŷf,i) 13: ∇θt",3.3. Implementation using automatic differentiation,[0],[0]
"← BackwardAD(l̂f , θt) 14: θt+1 ← OptimizerStep(θt,∇θt) 15: end for
Training time Our automatic reweighting method will introduce a constant factor of overhead.",3.3. Implementation using automatic differentiation,[0],[0]
"First, it requires two full forward and backward passes of the network on training and validation respectively, and then another backward on backward pass (Step 5 in Figure 1), to get the gradients to the example weights, and finally a backward pass to minimize the reweighted objective.",3.3. Implementation using automatic differentiation,[0],[0]
"In modern networks, a backwardon-backward pass usually takes about the same time as a forward pass, and therefore compared to regular training, our method needs approximately 3× training time; it is also possible to reduce the batch size of the validation pass for speedup.",3.3. Implementation using automatic differentiation,[0.9503591044012507],"['The model is trained to map entire image frames and entire spoken captions into a shared embedding space; however, as we will show, the trained network can then be used to localize patterns corresponding to words and phrases within the spectrogram, as well as visual objects within the image by applying it to small sub-regions of the image and spectrogram.']"
"We expect that it is worthwhile to spend the extra time to avoid the irritation of choosing early stopping, finetuning schedules, and other hyperparameters.",3.3. Implementation using automatic differentiation,[0],[0]
"Convergence results of SGD based optimization methods are well-known (Reddi et al., 2016).",3.4. Analysis: convergence of the reweighted training,[0],[0]
"However it is still meaningful to establish a convergence result about our method since it involves optimization of two-level objectives (Eq. 1, 2) rather than one, and we further make some firstorder approximation by introducing Eq. 7.",3.4. Analysis: convergence of the reweighted training,[0],[0]
"Here, we show theoretically that our method converges to the critical point of the validation loss function under some mild conditions, and we also give its convergence rate.",3.4. Analysis: convergence of the reweighted training,[0],[0]
"More detailed proofs can be found in the Supplementary Materials.
",3.4. Analysis: convergence of the reweighted training,[0],[0]
Definition 1.,3.4. Analysis: convergence of the reweighted training,[0],[0]
"A function f(x) : Rd → R is said to be Lipschitz-smooth with constant L if
‖∇f(x)−∇f(y)‖ ≤",3.4. Analysis: convergence of the reweighted training,[0],[0]
"L‖x− y‖,∀x, y ∈ Rd.
Definition 2. f(x) has σ-bounded gradients if ‖∇f(x)‖ ≤ σ for all x ∈ Rd.
",3.4. Analysis: convergence of the reweighted training,[0],[0]
"In most real-world cases, the high-quality validation set is really small, and thus we could set the mini-batch size m to be the same as the size of the validation set M .",3.4. Analysis: convergence of the reweighted training,[0],[0]
"Under this condition, the following lemma shows that our algorithm always converges to a critical point of the validation loss.",3.4. Analysis: convergence of the reweighted training,[0],[0]
"However, our method is not equivalent to training a model only on this small validation set.",3.4. Analysis: convergence of the reweighted training,[0],[0]
Because directly training a model on a small validation set will lead to severe overfitting issues.,3.4. Analysis: convergence of the reweighted training,[0],[0]
"On the contrary, our method can leverage useful information from a larger training set, and still converge to an appropriate distribution favored by this clean and balanced validation dataset.",3.4. Analysis: convergence of the reweighted training,[0],[0]
"This helps both generalization and robustness to biases in the training set, which will be shown in our experiments.
",3.4. Analysis: convergence of the reweighted training,[0],[0]
Lemma 1.,3.4. Analysis: convergence of the reweighted training,[0],[0]
"Suppose the validation loss function is Lipschitzsmooth with constant L, and the train loss function fi of training data xi have σ-bounded gradients.",3.4. Analysis: convergence of the reweighted training,[0],[0]
"Let the learning rate αt satisfies αt ≤ 2nLσ2 , where n is the training batch size.",3.4. Analysis: convergence of the reweighted training,[0],[0]
"Then, following our algorithm, the validation loss always monotonically decreases for any sequence of training batches, namely,
G(θt+1) ≤ G(θt), (13)
where G(θ) is the total validation loss
G(θ) = 1
M M∑ i=1",3.4. Analysis: convergence of the reweighted training,[0],[0]
fvi (θt+1( )).,3.4. Analysis: convergence of the reweighted training,[0],[0]
"(14)
Furthermore, in expectation, the equality in Eq. 13 holds only when the gradient of validation loss becomes 0 at some time step t, namely Et [G(θt+1)]",3.4. Analysis: convergence of the reweighted training,[0],[0]
"= G(θt) if and only if ∇G(θt) = 0, where the expectation is taking over possible training batches at time step t.
Moreover, we can prove the convergence rate of our method to be O(1/ 2).
",3.4. Analysis: convergence of the reweighted training,[0],[0]
Theorem 2.,3.4. Analysis: convergence of the reweighted training,[0],[0]
"Suppose G, fi and αt satisfy the aforementioned conditions, then Algorithm 1 achieves E",3.4. Analysis: convergence of the reweighted training,[0],[0]
[ ‖∇G(θt)‖2 ] ≤ in O(1/ 2) steps.,3.4. Analysis: convergence of the reweighted training,[0],[0]
"More specifically,
min 0<t<T
E [ ‖∇G(θt)‖2 ] ≤ C√
T , (15)
where C is some constant independent of the convergence process.",3.4. Analysis: convergence of the reweighted training,[0],[0]
"To test the effectiveness of our reweighting algorithm, we designed both class imbalance and noisy label settings, and a combination of both, on standard MNIST and CIFAR benchmarks for image classification using deep CNNs.",4. Experiments,[0],[0]
We use the standard MNIST handwritten digit classification dataset and subsample the dataset to generate a class imbalance binary classification task.,4.1. MNIST data imbalance experiments,[0],[0]
"We select a total of 5,000 images of size 28×28 on class 4 and 9, where 9 dominates the training data distribution.",4.1. MNIST data imbalance experiments,[0],[0]
We train a standard LeNet on this task and we compare our method with a suite of commonly used tricks for class imbalance: 1) PROPORTION weights each example by the inverse frequency 2),4.1. MNIST data imbalance experiments,[0],[0]
"RESAMPLE samples a class-balanced minibatch for each iteration 3) HARD MINING selects the highest loss examples from the majority class and 4) RANDOM is a random example weight baseline that assigns weights based on a rectified Gaussian distribution:
wrndi = max(zi, 0)∑",4.1. MNIST data imbalance experiments,[0],[0]
"i max(zi, 0) , where zi ∼ N (0, 1).",4.1. MNIST data imbalance experiments,[0],[0]
"(16)
To make sure that our method does not have the privilege of training on more data, we split the balanced validation set of 10 images directly from the training set.",4.1. MNIST data imbalance experiments,[0],[0]
"The network is trained with SGD with a learning rate of 1e-3 and mini-batch size of 100 for a total of 8,000 steps.
",4.1. MNIST data imbalance experiments,[0],[0]
Figure 2 plots the test error rate across various imbalance ratios averaged from 10 runs with random splits.,4.1. MNIST data imbalance experiments,[0],[0]
Note that our method significantly outperforms all the baselines.,4.1. MNIST data imbalance experiments,[0],[0]
"With class imbalance ratio of 200:1, our method only reports a small increase of error rate around 2%, whereas other methods suffer terribly under this setting.",4.1. MNIST data imbalance experiments,[0],[0]
"Compared with resampling and hard negative mining baselines, our approach does not throw away samples based on its class or training loss - as long as a sample is helpful towards the validation loss, it will be included as a part of the training loss.",4.1. MNIST data imbalance experiments,[0],[0]
Reweighting algorithm can also be useful on datasets where the labels are noisy.,4.2. CIFAR noisy label experiments,[0],[0]
"We study two settings of label noise here:
• UNIFORMFLIP: All label classes can uniformly flip to any other label classes, which is the most studied in the literature.",4.2. CIFAR noisy label experiments,[0],[0]
• BACKGROUNDFLIP:,4.2. CIFAR noisy label experiments,[0],[0]
All label classes can flip to a single background class.,4.2. CIFAR noisy label experiments,[0],[0]
This noise setting is very realistic.,4.2. CIFAR noisy label experiments,[0],[0]
"For instance, human annotators may not have recognized all the positive instances, while the
rest remain in the background class.",4.2. CIFAR noisy label experiments,[0],[0]
"This is also a combination of label imbalance and label noise since the background class usually dominates the label distribution.
",4.2. CIFAR noisy label experiments,[0],[0]
"We compare our method with prior work on the noisy label problem.
",4.2. CIFAR noisy label experiments,[0],[0]
"• REED, proposed by Reed et al. (2014), is a bootstrapping technique where the training target is a convex combination of the model prediction and the label.
",4.2. CIFAR noisy label experiments,[0],[0]
"• S-MODEL, proposed by Goldberger & Ben-Reuven (2017), adds a fully connected softmax layer after the regular classification output layer to model the noise transition matrix.
",4.2. CIFAR noisy label experiments,[0],[0]
"• MENTORNET, proposed by Jiang et al. (2017), is an RNN-based meta-learning model that takes in a sequence of loss values and outputs the example weights.",4.2. CIFAR noisy label experiments,[0],[0]
"We compare numbers reported in their paper with a base model that achieves similar test accuracy under 0% noise.
",4.2. CIFAR noisy label experiments,[0],[0]
"In addition, we propose two simple baselines: 1) RANDOM, which assigns weights according to a rectified Gaussian (see Eq. 16); 2) WEIGHTED, designed for BACKGROUNDFLIP, where the model knows the oracle noise ratio for each class and reweights the training loss proportional to the percentage of clean images of that label class.
",4.2. CIFAR noisy label experiments,[0],[0]
"Clean validation set For UNIFORMFLIP, we use 1,000 clean images in the validation set; for BACKGROUNDFLIP, we use 10 clean images per label class.",4.2. CIFAR noisy label experiments,[0],[0]
"Since our method uses information from the clean validation, for a fair comparison, we conduct an additional finetuning on the clean data based on the pre-trained baselines.",4.2. CIFAR noisy label experiments,[0],[0]
"We also study the effect on the size of the clean validation set in an ablation study.
",4.2. CIFAR noisy label experiments,[0],[0]
"Hyper-validation set For monitoring training progress and tuning baseline hyperparameters, we split out another
5,000 hyper-validation set from the 50,000 training images.",4.2. CIFAR noisy label experiments,[0],[0]
"We also corrupt the hyper-validation set with the same noise type.
",4.2. CIFAR noisy label experiments,[0],[0]
"Experimental details For REED model, we use the best β reported in Reed et al. (2014) (β = 0.8 for hard bootstrapping and β = 0.95 for soft bootstrapping).",4.2. CIFAR noisy label experiments,[0],[0]
"For the S-MODEL, we explore two versions to initialize the transition weights: 1) a smoothed identity matrix; 2) in background flip experiments we consider initializing the transition matrix with the confusion matrix of a pre-trained baseline model (S-MODEL +CONF).",4.2. CIFAR noisy label experiments,[0],[0]
"We find baselines can easily overfit the training noise, and therefore we also study early stopped versions of the baselines to provide a stronger comparison.",4.2. CIFAR noisy label experiments,[0],[0]
"In contrast, we find early stopping not necessary for our method.
",4.2. CIFAR noisy label experiments,[0],[0]
"To make our results comparable with the ones reported in MENTORNET and to save computation time, we exchange their Wide ResNet-101-10 with a Wide ResNet28-10 (WRN-28-10) (Zagoruyko & Komodakis, 2016) with dropout 0.3 as our base model in the UNIFORMFLIP experiments.",4.2. CIFAR noisy label experiments,[0],[0]
We find that test accuracy differences between the two base models are within 0.5% on CIFAR datasets under 0% noise.,4.2. CIFAR noisy label experiments,[0],[0]
"In the BACKGROUNDFLIP experiments, we use a ResNet-32 (He et al., 2016) as our base model.
",4.2. CIFAR noisy label experiments,[0],[0]
"We train the models with SGD with momentum, at an initial learning rate 0.1 and a momentum 0.9 with mini-batch size 100.",4.2. CIFAR noisy label experiments,[0],[0]
"For ResNet-32 models, the learning rate decays×0.1 at 40K and 60K steps, for a total of 80K steps.",4.2. CIFAR noisy label experiments,[0],[0]
"For WRN and early stopped versions of ResNet-32 models, the learning rate decays at 40K and 50K steps, for a total of 60K steps.",4.2. CIFAR noisy label experiments,[0],[0]
"Under regular 0% noise settings, our base ResNet-32 gets 92.5% and 68.1% classification accuracy on CIFAR-10 and 100, and the WRN-28-10 gets 95.5% and 78.2%.",4.2. CIFAR noisy label experiments,[0],[0]
"For the finetuning stage, we run extra 5K steps of training on the
CLEAN ONLY 15.90 ± 3.32 8.06 ± 0.76 BASELINE +FT 82.82 ± 0.93 54.23 ± 1.75 BASELINE +ES +FT 85.19 ± 0.46 55.22 ± 1.40 WEIGHTED +FT 85.98 ± 0.47 53.99 ± 1.62 S-MODEL +CONF +FT 81.90 ± 0.85 53.11 ± 1.33 S-MODEL +CONF +ES +FT 85.86 ± 0.63 55.75 ± 1.26
OURS 86.73 ± 0.48 59.30 ± 0.60
limited clean data.
",4.2. CIFAR noisy label experiments,[0],[0]
"We report the average test accuracy for 5 different random splits of clean and noisy labels, with 95% confidence interval in Table 1 and 2.",4.2. CIFAR noisy label experiments,[0],[0]
"The background classes for the 5 trials are [0, 1, 3, 5, 7] (CIFAR-10) and [7, 12, 41, 62, 85] (CIFAR-100).",4.2. CIFAR noisy label experiments,[0],[0]
"The first result that draws our attention is that “Random” performs surprisingly well on the UNIFORMFLIP benchmark, outperforming all historical methods that we compared.",4.3. Results and Discussion,[0],[0]
"Given that its performance is comparable with Baseline on BACKGROUNDFLIP and MNIST class imbalance, we hypothesize that random example weights act as a strong regularizer and under which the learning objective on UNIFORMFLIP is still consistent.
",4.3. Results and Discussion,[0],[0]
"Regardless of the strong baseline, our method ranks the top on both UNIFORMFLIP and BACKGROUNDFLIP, showing our method is less affected by the changes in the noise type.",4.3. Results and Discussion,[0],[0]
"On CIFAR-100, our method wins more than 3% compared to the state-of-the-art method.
",4.3. Results and Discussion,[0],[0]
Understanding the reweighting mechanism It is beneficial to understand how our reweighting algorithm contributes to learning more robust models during training.,4.3. Results and Discussion,[0],[0]
"First, we use a pre-trained model (trained at half of the total iterations without learning rate decay) and measure the example weight distribution of a randomly sampled batch of validation images, which the model has never seen.",4.3. Results and Discussion,[0],[0]
"As shown in the left figure of Figure 3, our model correctly
pushes most noisy images to zero weights.",4.3. Results and Discussion,[0],[0]
"Secondly, we conditioned the input mini-batch to be a single nonbackground class and randomly flip 40% of the images to the background, and we would like to see how well our model can distinguish clean and noisy images.",4.3. Results and Discussion,[0],[0]
"As shown in Figure 3 right, the model is able to reliably detect images that are flipped to the background class.
",4.3. Results and Discussion,[0],[0]
"Robustness to overfitting noise Throughout experimentation, we find baseline models can easily overfit to the noise in the training set.",4.3. Results and Discussion,[0],[0]
"For example, shown in Table 2, applying early stopping (“ES”) helps the classification performance of “S-Model” by over 10% on CIFAR-10.",4.3. Results and Discussion,[0],[0]
"Figure 6 compares the final confusion matrices of the baseline and the proposed algorithm, where a large proportion of noise transition probability is cleared in the final prediction.",4.3. Results and Discussion,[0],[0]
Figure 7 shows training curves on the BACKGROUNDFLIP experiments.,4.3. Results and Discussion,[0],[0]
"After the first learning rate decay, both “Baseline” and “SModel” quickly degrade their validation performance due to overfitting, while our model remains the same validation accuracy until termination.",4.3. Results and Discussion,[0],[0]
"Note that here “S-Model” knows the oracle noise ratio in each class, and this information is
not available in our method.
",4.3. Results and Discussion,[0],[0]
Impact of the noise level We would like to investigate how strongly our method can perform on a variety of noise levels.,4.3. Results and Discussion,[0],[0]
"Shown in Figure 5, our method only drops 6% accuracy when the noise ratio increased from 0% to 50%;
whereas the baseline has dropped more than 40%.",4.3. Results and Discussion,[0],[0]
"At 0% noise, our method only slightly underperforms baseline.",4.3. Results and Discussion,[0],[0]
"This is reasonable since we are optimizing on the validation set, which is strictly a subset of the full training set, and therefore suffers from its own subsample bias.
",4.3. Results and Discussion,[0],[0]
"Size of the clean validation set When the size of the clean validation set grows larger, fine-tuning on the validation set will be a reasonble approach.",4.3. Results and Discussion,[0],[0]
"Here, we make an attempt to explore the tradeoff and understand when fine-tuning becomes beneficial.",4.3. Results and Discussion,[0],[0]
Figure 4 plots the classification performance when we varied the size of the clean validation on BACKGROUNDFLIP.,4.3. Results and Discussion,[0],[0]
"Surprisingly, using 15 validation images for all classes only results in a 2% drop in performance, and the overall classification performance does not grow after having more than 100 validation images.",4.3. Results and Discussion,[0],[0]
"In comparison, we observe a significant drop in performance when only fine-tuning on these 15 validation images for the baselines, and the performance catches up around using 1,000 validation images (100 per class).",4.3. Results and Discussion,[0],[0]
"This phenomenon suggests that in our method the clean validation acts more like a regularizer rather than a data source for parameter finetuning, and potentially our method can be complementary with fine-tuning based method when the size of the clean set grows larger.",4.3. Results and Discussion,[0],[0]
"In this work, we propose an online meta-learning algorithm for reweighting training examples and training more robust deep learning models.",5. Conclusion,[0],[0]
"While various types of training set biases exist and manually designed reweighting objectives have their own bias, our automatic reweighting algorithm shows superior performance dealing with class imbalance, noisy labels, and both.",5. Conclusion,[0],[0]
Our method can be directly applied to any deep learning architecture and is expected to train end-to-end without any additional hyperparameter search.,5. Conclusion,[0],[0]
"Validating on every training step is a novel setting and we show that it has links with model regularization, which can be a fruitful future research direction.",5. Conclusion,[0],[0]
Deep neural networks have been shown to be very powerful modeling tools for many supervised learning tasks involving complex input patterns.,abstractText,[0],[0]
"However, they can also easily overfit to training set biases and label noises.",abstractText,[0],[0]
"In addition to various regularizers, example reweighting algorithms are popular solutions to these problems, but they require careful tuning of additional hyperparameters, such as example mining schedules and regularization hyperparameters.",abstractText,[0],[0]
"In contrast to past reweighting methods, which typically consist of functions of the cost value of each example, in this work we propose a novel meta-learning algorithm that learns to assign weights to training examples based on their gradient directions.",abstractText,[0],[0]
"To determine the example weights, our method performs a meta gradient descent step on the current mini-batch example weights (which are initialized from zero) to minimize the loss on a clean unbiased validation set.",abstractText,[0],[0]
"Our proposed method can be easily implemented on any type of deep network, does not require any additional hyperparameter tuning, and achieves impressive performance on class imbalance and corrupted label problems where only a small amount of clean validation data is available.",abstractText,[0],[0]
Learning to Reweight Examples for Robust Deep Learning,title,[0],[0]
"Many natural language processing (NLP) and computer vision problems necessitate predicting structured outputs such as labeled sequences, trees or general graphs (Smith, 2010; Nowozin & Lampert, 2011).",1. Introduction,[0],[0]
Such tasks require modeling both input-output relationships and the interactions between predicted outputs to capture correlations.,1. Introduction,[0],[0]
"Across the various structured prediction formulations (Lafferty et al., 2001; Taskar et al., 2003; Chang et al., 2012), prediction requires solving inference problems by searching for scoremaximizing output structures.",1. Introduction,[0],[0]
"The search space for inference is typically large (e.g., all parse trees), and grows with input size.",1. Introduction,[0],[0]
"Exhaustive search can be prohibitive and standard alternatives are either: (a) perform exact inference with a large computational cost or, (b) approximate inference to sacrifice accuracy in favor of time.
",1. Introduction,[0],[0]
"1School of Computing, University of Utah, Salt Lake City, Utah, USA.",1. Introduction,[0],[0]
"Correspondence to: Xingyuan Pan <xpan@cs.utah.edu>, Vivek Srikumar <svivek@cs.utah.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"In this paper, we focus on the computational cost of inference.",1. Introduction,[0],[0]
"We argue that naturally occurring problems have remarkable regularities across both inputs and outputs, and traditional formulations of inference ignore them.",1. Introduction,[0],[0]
"For example, parsing an n-word sentence will cost a standard head-driven lexical parser O(n5) time.",1. Introduction,[0],[0]
Current practice in NLP is to treat each new sentence as a fresh discrete optimization problem and pay the computational price each time.,1. Introduction,[0],[0]
"However, this practice is not only expensive, but also wasteful!",1. Introduction,[0],[0]
"We ignore the fact that slight changes to inputs often do not change the output, or even the sequence of steps taken to produce it.",1. Introduction,[0],[0]
"Moreover, not all outputs are linguistically meaningful structures; as we make more predictions, we should be able to learn to prune the output space.
",1. Introduction,[0],[0]
The motivating question that drives our work is: Can we design inference schemes that learn to make a trained structured predictor faster without sacrificing output quality?,1. Introduction,[0],[0]
"After training, the structured classifier can be thought as a black-box.",1. Introduction,[0],[0]
"Typically, once deployed, it is never modified over its lifetime of classifying new examples.",1. Introduction,[0],[0]
"Subsequently, we can view each prediction of the black-box classifier as an opportunity to learn how to navigate the output space more efficiently.",1. Introduction,[0],[0]
"Thus, if the classifier sees a previously encountered situation, it could make some decisions without needless computations.
",1. Introduction,[0],[0]
We formalize this intuition by considering the trained models as solving arbitrary integer linear programs (ILPs) for combinatorial inference.,1. Introduction,[0],[0]
"We train a second, inexpensive speedup classifier which acts as a heuristic for a searchbased inference algorithm that mimics the more expensive black-box classifier.",1. Introduction,[0],[0]
The speedup heuristic is a function that learns regularities among predicted structures.,1. Introduction,[0],[0]
"We present a mistake bound algorithm that, over the classifier’s lifetime, learns to navigate the feasible regions of the ILPs.",1. Introduction,[0],[0]
"By doing so, we can achieve a reduction in inference time.
",1. Introduction,[0],[0]
We further identify inference situations where the learned speedup heuristic alone can correctly label parts of the outputs without computing the corresponding input features.,1. Introduction,[0],[0]
"In such situations, the search algorithm can safely ignore parts of inputs if the corresponding outputs can be decided based on the sub-structures constructed so far.",1. Introduction,[0],[0]
"Seen this way, the speedup classifier can be seen as a statistical cache of past decisions made by the black-box classifier.
",1. Introduction,[0],[0]
We instantiate our strategy to the task of predicting entities and relations from sentences.,1. Introduction,[0],[0]
"Using an ILP based black-box classifier, we show that the trained speedup classifier mimics the reference inference algorithm to obtain improvements in running time, and also recovers its accuracy.",1. Introduction,[0],[0]
"Indeed, by learning to ignore input components when they will not change the prediction, we show that learned search strategy outperforms even greedy search in terms of speed.
",1. Introduction,[0],[0]
"To summarize, the main contribution of this paper is the formalization of the problem of learning to make structured output classifiers faster without sacrificing accuracy.",1. Introduction,[0],[0]
We develop a learning-to-search framework to train a speedup classifier with a mistake-bound guarantee and a sufficient condition to safely avoid computing input-based features.,1. Introduction,[0],[0]
"We show empirically on an entity-relation extraction task that we can learn a speedup classifier that is (a) faster than both the state-of-the-art Gurobi optimizer and greedy search, and (b) does not incur a loss in output quality.",1. Introduction,[0],[0]
"First, we will define the notation used in this paper with a running example that requires of identifying entity types and their relationships in text.",2. Notation and Preliminaries,[0.958267980478429],"['Once we have completed the grounding procedure, we are left with a small set of regions of interest in each image and caption spectrogram.']"
"The input to the problem consists of sentences such as:
Colin went back home in Ordon Village.
",2. Notation and Preliminaries,[0],[0]
"These inputs are typically preprocessed — here, we are given spans of text (underlined) corresponding to entities.",2. Notation and Preliminaries,[0],[0]
"We will denote such preprocessed inputs to the structured prediction problem as x.
We seek to produce a structure y ∈",2. Notation and Preliminaries,[0],[0]
"Yx (e.g., labeled trees, graphs) associated with these inputs.",2. Notation and Preliminaries,[0],[0]
"Here, Yx is the set of all possible output structures for the input x.",2. Notation and Preliminaries,[0],[0]
"In the example problem, our goal is to assign types to the entities and also label the relationships between them.",2. Notation and Preliminaries,[0],[0]
"Suppose our task has three types of entities: person, location and organization.",2. Notation and Preliminaries,[0],[0]
"A pair of entities can participate in one of five possible directed relations: Kill, LiveIn, WorkFor, LocatedAt and OrgBasedIn.",2. Notation and Preliminaries,[0],[0]
"Additionally, there is a special entity label NoEnt meaning a text span is not an entity, and a special relation label NoRel indicating that two spans are unrelated.",2. Notation and Preliminaries,[0],[0]
"Figure 1 shows a plausible structure for the example sentence as per this scheme.
",2. Notation and Preliminaries,[0],[0]
A standard way to model the prediction problem requires learning a model that scores all structures in Yx and searching for the score-maximizing structure.,2. Notation and Preliminaries,[0],[0]
"Linear models are commonly used as scoring functions, and require a feature vector characterizing input-output relationships Φ (x,y).",2. Notation and Preliminaries,[0],[0]
We will represent the model by a weight vector α.,2. Notation and Preliminaries,[0],[0]
"Every structure y associated with an input x is scored as the dot product α · Φ (x,y).",2. Notation and Preliminaries,[0],[0]
"The goal of prediction is to find the
structure y∗ that maximizes this score.",2. Notation and Preliminaries,[0],[0]
"That is,
y∗ = arg max y∈Yx α ·",2. Notation and Preliminaries,[0],[0]
"Φ (x,y) .",2. Notation and Preliminaries,[0],[0]
"(1)
Learning involves using training data to find the best weight vector α.
",2. Notation and Preliminaries,[0],[0]
"In general, the output structure y is a set of K categorical inference variables {y1, y2, · · · , yK} , each of which can take a value from a predefined set of n labels.",2. Notation and Preliminaries,[0],[0]
"That is, each yk ∈ y takes a value from {l1, l2, · · · , ln}.1 In our running example, the inference variables correspond to the four decisions that define the structure: the labels for the two entities, and the relations in each direction.",2. Notation and Preliminaries,[0],[0]
"The feature function Φ decomposes into a sum of features over each yk, each denoted by Φk, giving us the inference problem:
y∗ = arg max y∈Yx K∑",2. Notation and Preliminaries,[0],[0]
k=1 α ·,2. Notation and Preliminaries,[0],[0]
"Φk ( x, yk ) .",2. Notation and Preliminaries,[0],[0]
"(2)
The dependencies between the yk’s specify the nature of the output space.",2. Notation and Preliminaries,[0],[0]
Determining each yk in isolation greedily does not typically represent a viable inference strategy because constraints connecting the variables are ignored.,2. Notation and Preliminaries,[0],[0]
"In this spirit, the problem of finding the best structure can be viewed as a combinatorial optimization problem.
",2. Notation and Preliminaries,[0],[0]
"In this paper, we consider the scenario in which we have already trained a model α.",2. Notation and Preliminaries,[0],[0]
"We focus on solving the inference problem (i.e.,Eq.",2. Notation and Preliminaries,[0],[0]
(2)) efficiently.,2. Notation and Preliminaries,[0],[0]
We conjecture that it should be possible to observe a black-box inference algorithm over its lifetime to learn to predict faster without losing accuracy.,2. Notation and Preliminaries,[0],[0]
One common way to solve inference is by designing efficient dynamic programming algorithms that exploit problem structure.,2.1. Black-box Inference Mechanisms,[0],[0]
"While effective, this approach is limited to special cases where the problem admits efficient decoding, thus placing restrictions on factorization and feature design.
",2.1. Black-box Inference Mechanisms,[0],[0]
"In this paper, we seek to reason about the problem of predicting structures in the general case.",2.1. Black-box Inference Mechanisms,[0],[0]
"Since inference is essentially a combinatorial optimization problem, without loss
1We make this choice for simplicity of notation.",2.1. Black-box Inference Mechanisms,[0],[0]
"In general, K depends on the size of the input x, and categorical variables may take values from different label sets.
of generality, we can represent any inference problem as an integer linear programming (ILP) instance (Schrijver, 1998).",2.1. Black-box Inference Mechanisms,[0],[0]
To represent the inference task in Eq.,2.1. Black-box Inference Mechanisms,[0],[0]
"(2) as an ILP instance, we will define indicator variables of the form zki ∈ {0, 1}, which stands for the decision that the categorical variable yk is assigned the ith label among the n labels.",2.1. Black-box Inference Mechanisms,[0],[0]
"That is, zki = 1 if yk = li, and 0 otherwise.",2.1. Black-box Inference Mechanisms,[0],[0]
"Using this notation, we can write the cost of any structure y in terms of the indicators as
K∑ k=1 n∑ i=1",2.1. Black-box Inference Mechanisms,[0],[0]
cki,2.1. Black-box Inference Mechanisms,[0],[0]
z,2.1. Black-box Inference Mechanisms,[0],[0]
k,2.1. Black-box Inference Mechanisms,[0],[0]
i .,2.1. Black-box Inference Mechanisms,[0],[0]
"(3)
Here, cki is a stand in for −α ·",2.1. Black-box Inference Mechanisms,[0],[0]
"Φk (x, li), namely the cost (negative score) associated with this decision.2",2.1. Black-box Inference Mechanisms,[0],[0]
"In our example, suppose the first categorical variable y1 corresponds to the entity Colin, and it has possible labels {person,location, . . .",2.1. Black-box Inference Mechanisms,[0],[0]
}.,2.1. Black-box Inference Mechanisms,[0],[0]
"Then, assigning person to Colin would correspond to setting z11 = 1, and z 1 i = 0 for all i 6= 1.",2.1. Black-box Inference Mechanisms,[0],[0]
"Using the labels enumerated in §2, there will be 20 indicators for the four categorical decisions.
",2.1. Black-box Inference Mechanisms,[0],[0]
"Of course, arbitrary assignments to the indicators is not allowed.",2.1. Black-box Inference Mechanisms,[0],[0]
We can define the set of feasible structures using linear constraints.,2.1. Black-box Inference Mechanisms,[0],[0]
"Clearly, each categorical variable can take exactly one label, which can be expressed via:
n∑ i=1 zki = 1, for all k. (4)
",2.1. Black-box Inference Mechanisms,[0],[0]
"In addition, we can define the set of valid structures Yx using a collection of m linear constraints, the jth one of which can be written as
K∑ k=1 n∑ i=1",2.1. Black-box Inference Mechanisms,[0],[0]
"Akjiz k i = bj , for all j. (5)
",2.1. Black-box Inference Mechanisms,[0],[0]
These structural constraints characterize the interactions between the categorical variables.,2.1. Black-box Inference Mechanisms,[0],[0]
"For example, if a directed edge in our running example is labeled as LiveIn, then, its source and target must be a person and a location respectively.",2.1. Black-box Inference Mechanisms,[0],[0]
"While Eq.(5) only shows equality constraints, in practice, inequality constraints can also be included.
",2.1. Black-box Inference Mechanisms,[0],[0]
The inference problem in Eq.,2.1. Black-box Inference Mechanisms,[0],[0]
(2) is equivalent to the problem of minimizing the objective in Eq.,2.1. Black-box Inference Mechanisms,[0],[0]
(3) over the 0-1 indicator variables subject to the constraints in Eqs.,2.1. Black-box Inference Mechanisms,[0],[0]
"(4) and (5).
",2.1. Black-box Inference Mechanisms,[0],[0]
We should note the difference between the ability to write an inference problem as an ILP instance and actually solving it as one.,2.1. Black-box Inference Mechanisms,[0],[0]
"The former gives us the ability to reason about inference in general, and perhaps using other methods (such as Lagrangian relaxation (Lemaréchal, 2001)) for inference.",2.1. Black-box Inference Mechanisms,[0],[0]
"However, solving problems with industrial strength ILP
2The negation defines an equivalent minimization problem and makes subsequent description of the search framework easier.
solvers such as the Gurobi solver3 is competitive with other approaches in terms of inference time, even though they may not directly exploit problem structure.
",2.1. Black-box Inference Mechanisms,[0],[0]
"In this work, we use the general structure of the ILP inference formulation to develop the theory for speeding up inference.",2.1. Black-box Inference Mechanisms,[0],[0]
"In addition, because of its general applicability and fast inference speed, we use the Gurobi ILP solver as our black-box classifier, and learn a speedup heuristic to make even faster inference.",2.1. Black-box Inference Mechanisms,[0],[0]
Directly applying the black-box solver for the large output spaces may be impractical.,2.2. Inference as Search,[0],[0]
An alternative general purpose strategy for inference involves framing the maximization in Eq.,2.2. Inference as Search,[0],[0]
"(2) as a graph search problem.
",2.2. Inference as Search,[0],[0]
"Following Russell & Norvig (2003); Xu et al. (2009), a general graph search problem requires defining an initial search node I , a successor function s(·), and a goal test.",2.2. Inference as Search,[0],[0]
The successor function s(·) maps a search node to its successors.,2.2. Inference as Search,[0],[0]
The goal test determines whether a node is a goal node.,2.2. Inference as Search,[0],[0]
"Usually, each search step is associated with a cost function, and we seek to find a goal node with the least total cost.
",2.2. Inference as Search,[0],[0]
We can define the search problem corresponding to inference as follows.,2.2. Inference as Search,[0],[0]
"We will denote a generic search node in the graph as v, which corresponds to a set of partially assigned categorical variables.",2.2. Inference as Search,[0],[0]
"Specifically, we will define the search node v as a set of pairs {(k, i)}, each element of which specifies that the variable yk is assigned the ith label.",2.2. Inference as Search,[0],[0]
The initial search node I is the empty set since none of the variables has been assigned when the search begins.,2.2. Inference as Search,[0],[0]
"For a node v, its successors s(v) is a set of nodes, each containing one more assigned variable than v. A node is a goal node if all variables yk’s have been assigned.",2.2. Inference as Search,[0],[0]
"The size of any goal node is K, the number of categorical variables.
",2.2. Inference as Search,[0],[0]
"In our running example, at the start of search, we may choose to assign the first label l1 (person) to the variable y1 – the entity Colin – leading us to the successor {(1, 1)}.",2.2. Inference as Search,[0],[0]
Every search node specifies a partial or a full assignment to all the entities and relations.,2.2. Inference as Search,[0],[0]
"The goal test simply checks if we arrive at a full assignment, i.e., all the entity and relation candidates have been assigned a label.
",2.2. Inference as Search,[0],[0]
"Note that goal test does not test the quality of the node, it simply tests whether the search process is finished.",2.2. Inference as Search,[0],[0]
"The quality of the goal node is determined by the path cost from the initial node to the goal node, which is the accumulated cost of each step along the way.",2.2. Inference as Search,[0],[0]
The step cost for assigning label li to a variable yk is the same cki we defined for the ILP objective in Eq.,2.2. Inference as Search,[0],[0]
(3).,2.2. Inference as Search,[0],[0]
"Finding a shortest path in such a search space is equivalent to the original ILP problem
3http://www.gurobi.com
without the structural constraints in Eq.",2.2. Inference as Search,[0],[0]
(5).,2.2. Inference as Search,[0],[0]
The uniquelabel constraints in Eq.,2.2. Inference as Search,[0],[0]
"(4) are automatically satisfied by our formulation of the search process.
",2.2. Inference as Search,[0],[0]
"Indeed, solving inference without the constraints in Eq.(5) is trivial.",2.2. Inference as Search,[0],[0]
"For each categorical variable yk, we can pick the label li that has the lowest value of cki .",2.2. Inference as Search,[0],[0]
"This gives us two possible options for solving inference as search: We can (a) ignore the constraints that make inference slow to greedily predict all the labels, or, (b) enforce constraints at each step of the search, and only consider search nodes that satisfy all constraints.",2.2. Inference as Search,[0],[0]
"The first option is fast, but can give us outputs that are invalid.",2.2. Inference as Search,[0],[0]
"For example, we might get a structure that mandates that the person Colin lives in a person called Ordon Village.",2.2. Inference as Search,[0],[0]
"The second option will give us structurally valid outputs, but can be prohibitively slow.
",2.2. Inference as Search,[0],[0]
Various graph search algorithms can be used for performing inference.,2.2. Inference as Search,[0],[0]
"For efficiency, we can use beam search with a fixed beam width b.",2.2. Inference as Search,[0],[0]
When search begins the beam B0 contains only the initial node B0 =,2.2. Inference as Search,[0],[0]
[I].,2.2. Inference as Search,[0],[0]
"Following Collins & Roark (2004); Xu et al. (2009), we define the function BreadthExpand which takes the beam Bt at step t and generates the candidates Ct+1 for the next beam:
Ct+1 = BreadthExpand(Bt) = ∪v∈Bts(v)
The next beam is given by Bt+1 = Filter(Ct+1), where Filter takes top b nodes according to some priority function p(v).",2.2. Inference as Search,[0],[0]
"In the simplest case, the priority of a node v is the total path cost of reaching that node.",2.2. Inference as Search,[0],[0]
"More generally, the priority function can be informed not only by the path cost, but also by a heuristic function as in the popular A∗ algorithm.",2.2. Inference as Search,[0],[0]
"In the previous section, we saw that using a black-box ILP solver may be slower than greedy search which ignores constraints, but produces valid outputs.",3. Speeding up Structured Prediction,[0],[0]
"However, over its lifetime, a trained classifier predicts structures for a large number of inputs.",3. Speeding up Structured Prediction,[0],[0]
"While the number of unique inputs (e.g. sentences) may be large, the number of unique structures that actually occur among the predictions is not only finite, but also small.",3. Speeding up Structured Prediction,[0],[0]
"This observation was exploited by Srikumar et al. (2012); Kundu et al. (2013) for amortizing inference costs.
",3. Speeding up Structured Prediction,[0],[0]
"In this paper, we are driven by the need for an inference algorithm that learns regularities across outputs to become faster at producing structurally valid outputs.",3. Speeding up Structured Prediction,[0],[0]
"In order to do so, we will develop an inference-as-search scheme that inherits the speed of greedy search, but learns to produce structurally valid outputs.",3. Speeding up Structured Prediction,[0],[0]
"Before developing the algorithmic aspects of such an inference scheme, let us first see a proofof-concept for such a scheme.",3. Speeding up Structured Prediction,[0],[0]
Our goal is to incorporate the structural constraints from Eq.,3.1. Heuristics for Structural Validity,[0],[0]
(5) as a heuristic for greedy or beam search.,3.1. Heuristics for Structural Validity,[0],[0]
"To do so, at each step during search, we need to estimate how likely an assignment can lead to a constraint violation.",3.1. Heuristics for Structural Validity,[0],[0]
"This information can be characterized by using a heuristic function h(v), which will be used to evaluated a node v during search.
",3.1. Heuristics for Structural Validity,[0],[0]
The dual form the ILP in Eqs.,3.1. Heuristics for Structural Validity,[0],[0]
(3) to (5) help justify the idea of capturing constraint information using a heuristic function.,3.1. Heuristics for Structural Validity,[0],[0]
We treat the unique label constraints in Eq.,3.1. Heuristics for Structural Validity,[0],[0]
"(4) as defining the domain in which each 0-1 variable zki lives, and the only real constraints are given by Eq. (5).
Let uj represent the dual variable for the jth constraint.",3.1. Heuristics for Structural Validity,[0],[0]
"Thus, we obtain the Lagrangian4
L(z, u) =",3.1. Heuristics for Structural Validity,[0],[0]
K∑,3.1. Heuristics for Structural Validity,[0],[0]
k=1 n∑ i=1,3.1. Heuristics for Structural Validity,[0],[0]
cki z,3.1. Heuristics for Structural Validity,[0],[0]
k i,3.1. Heuristics for Structural Validity,[0],[0]
− m∑ j=1 uj (,3.1. Heuristics for Structural Validity,[0],[0]
K∑ k=1 n∑ i=1,3.1. Heuristics for Structural Validity,[0],[0]
Akjiz k i,3.1. Heuristics for Structural Validity,[0],[0]
"− bj )
",3.1. Heuristics for Structural Validity,[0],[0]
"= ∑ k,i cki",3.1. Heuristics for Structural Validity,[0],[0]
"−∑ j ujA k ji  zki +∑ j bjuj
",3.1. Heuristics for Structural Validity,[0],[0]
"The dual function θ(u) = minz L(z, u), where the minimization is over the domain of the z variables.
",3.1. Heuristics for Structural Validity,[0],[0]
Denote u∗ = arg max θ(u) as the solution to the dual problem.,3.1. Heuristics for Structural Validity,[0],[0]
"In the case of zero duality gap, the theory of Lagrangian relaxation (Lemaréchal, 2001) tells us that solving the following relaxed minimization problem will solve the original ILP:
min ∑ k,i cki",3.1. Heuristics for Structural Validity,[0],[0]
"−∑ j u∗jA k ji  zki (6)∑ i zki = 1, for all k (7) zki ∈ {0, 1}, for all k, i (8)
This new optimization problem does not have any structural constraints and can be solved greedily for each k if we know the optimal dual variables u∗.
To formulate the minimization in Eqs (6) to (8) as a search problem, we define the priority function p(v) for ranking the nodes as p(v) = g(v) + h∗(v), where the path cost g(v) and heuristic function h∗(v) are given by
g(v) = ∑
(k,i)∈v
cki , (9)
h∗(v) =",3.1. Heuristics for Structural Validity,[0],[0]
"− ∑
(k,i)∈v ∑ j Akjiu ∗ j (x).",3.1. Heuristics for Structural Validity,[0],[0]
"(10)
Since Eq. (6) is a minimization problem, smaller priority value p(v) means higher ranking during search.",3.1. Heuristics for Structural Validity,[0],[0]
"Note that
4We omit the ranges of the summation indices",3.1. Heuristics for Structural Validity,[0],[0]
"i, j, k hereafter.
even though heuristic function defined in this way is not always admissible, greedy search with ranking function p(v) will lead to the exact solution of Eqs.",3.1. Heuristics for Structural Validity,[0],[0]
(6) to (8).,3.1. Heuristics for Structural Validity,[0],[0]
"In practice, however, we do not have the optimal values for the dual variables u∗.",3.1. Heuristics for Structural Validity,[0],[0]
"Indeed, when Lagrangian relaxation is used for inference, the optmial dual variables are computed using subgradient optimization for each example because their value depends on the original input via the c’s.
",3.1. Heuristics for Structural Validity,[0],[0]
"Instead of performing expensive gradient based optimization for every input instance, we will approximate the heuristic function as a classifier that learns to prioritize structurally valid outputs.",3.1. Heuristics for Structural Validity,[0],[0]
"In this paper, we use a linear model based on a weight vector w to approximate the heuristic as
h(v) = −w · φ(v) (11)
",3.1. Heuristics for Structural Validity,[0],[0]
"For an appropriate choice of node features φ(v), the heuristic h(v) in Eq.(10) is indeed a linear function.5",3.1. Heuristics for Structural Validity,[0],[0]
"In other words, there exists a linear heuristic function that can guide graph search towards creating structurally valid outputs.
",3.1. Heuristics for Structural Validity,[0],[0]
"In this setting, the priority function p(v) for each node is determined by two components: the path cost g(v) from the initial node to the current node, and the learned heuristic cost h(v), which is an estimate of how good the current node is.",3.1. Heuristics for Structural Validity,[0],[0]
"Because the purpose of the heuristic is to help improve inference speed, we call φ(v) speedup features.",3.1. Heuristics for Structural Validity,[0],[0]
The speedup features can be different from the original model features in Eq.,3.1. Heuristics for Structural Validity,[0],[0]
(2).,3.1. Heuristics for Structural Validity,[0],[0]
In particular it can includes features for partial assignments made so far which were not available in the original model features.,3.1. Heuristics for Structural Validity,[0],[0]
"In this setting, the goal of speedup learning is to find suitable weight vector w over the black-box classifier’s lifetime.",3.1. Heuristics for Structural Validity,[0],[0]
"In this section, we will describe a mistake-bound algorithm to learn the weight vector w of the speedup classifier.",4. Learning the Speedup Classifier,[0],[0]
"The design of this algorithm is influenced by learning to search algorithms such as LaSO (Daumé III & Marcu, 2005; Xu et al., 2009).",4. Learning the Speedup Classifier,[0],[0]
"We assume that we have access to a trained black-box ILP solver called Solve, which can solve the structured prediction problems, and we have a large set of examples {xi}Ni=1.",4. Learning the Speedup Classifier,[0],[0]
Our goal is to use this set to train a speedup classifier to mimic the ILP solver while predicting structures for this set of examples.,4. Learning the Speedup Classifier,[0],[0]
"Subsequently, we can use the less expensive speedup influenced search procedure to replace the ILP solver.
",4. Learning the Speedup Classifier,[0],[0]
"To define the algorithm, we will need additional terminology.",4. Learning the Speedup Classifier,[0],[0]
"Given a reference solution y, we define a node v to be ygood, if it can possibly lead to the reference solution.",4. Learning the Speedup Classifier,[0],[0]
"If a node v is y-good, then the already assigned variables have the same labels as in the reference solution.",4. Learning the Speedup Classifier,[0],[0]
"We define a
5See supplementary material for an elaboration.
",4. Learning the Speedup Classifier,[0],[0]
"Algorithm 1 Learning a speedup classifier using examples {xi}Ni=1, and a black-box Solver Solve.
1: Initialize the speedup weight vector w← 0 2: for epoch = 1 . .",4. Learning the Speedup Classifier,[0],[0]
.M,4. Learning the Speedup Classifier,[0],[0]
do 3: for i = 1 . . .,4. Learning the Speedup Classifier,[0],[0]
N,4. Learning the Speedup Classifier,[0],[0]
do 4: y← Solve(xi) 5: Initialize the beam B ←,4. Learning the Speedup Classifier,[0],[0]
"[I] 6: while B is y-good and v̂ is not goal do 7: B ← Filter(BreadthExpand(B)) 8: end while 9: if B is not y-good then
10: v∗ ← SetGood(v̂) 11: w←",4. Learning the Speedup Classifier,[0],[0]
w + φ(v∗)− 1|B| ∑ v∈B φ(v) 12: else if v̂ is not y-good then 13: v∗ ← SetGood(v̂) 14: w←,4. Learning the Speedup Classifier,[0],[0]
"w + φ(v∗)− φ(v̂) 15: end if 16: end for 17: end for
beam B is y",4. Learning the Speedup Classifier,[0],[0]
-good if it contains at least one y-good node to represent the notion that search is still viable.,4. Learning the Speedup Classifier,[0],[0]
"We denote the first element (the highest ranked) in a beam by v̂. Finally, we define an operator SetGood, which takes a node that is not y-good, and return its corresponding y-good node by fixing the incorrect assignments according to the reference solution.",4. Learning the Speedup Classifier,[0],[0]
"The unassigned variables are still left unassigned by the SetGood operator.
",4. Learning the Speedup Classifier,[0],[0]
The speedup-learning algorithm is listed as Algorithm 1.,4. Learning the Speedup Classifier,[0],[0]
It begins by initializing the weight w to the zero vector.,4. Learning the Speedup Classifier,[0],[0]
We iterate over the examples for M epochs.,4. Learning the Speedup Classifier,[0],[0]
"For each example xi, we first solve inference using the ILP solver to obtain the reference structure y (line 4).",4. Learning the Speedup Classifier,[0],[0]
Next a breadth-expand search is performed (lines 5-8).,4. Learning the Speedup Classifier,[0],[0]
"Every time the beam B is updated, we check if the beam contains at least one ygood node that can possibly lead to the reference solution y. Search terminates if the beam is not y-good, or if the highest ranking node v̂ is a goal.",4. Learning the Speedup Classifier,[0],[0]
"If the beam is not y-good, we compute the corresponding y-good node v∗ from v̂, and perform a perceptron style update to the speedup weights (line 9-11).",4. Learning the Speedup Classifier,[0],[0]
"In other words, we update the weight vector by adding feature vector of φ(v∗), and subtracting the average feature vector of all the nodes in the beam.",4. Learning the Speedup Classifier,[0],[0]
Otherwise v̂ must be a goal node.,4. Learning the Speedup Classifier,[0],[0]
We then check if v̂ agrees with the reference solution (lines 12-15).,4. Learning the Speedup Classifier,[0],[0]
"If not, we perform a similar weight update, by adding the feature vector of φ(v∗), and subtracting φ(v̂).
",4. Learning the Speedup Classifier,[0],[0]
"Mistake bound Next, we show that the Algorithm 1 has a mistake bound.",4. Learning the Speedup Classifier,[0],[0]
"Let Rφ be a positive constant such that for every pair of nodes (v, v′), we have ‖φ(v)− φ(v′)‖ ≤ Rφ.",4. Learning the Speedup Classifier,[0],[0]
"Let Rg be a positive constant such that for every pair of
search nodes (v, v′), we have |g(v)− g(v′)| ≤",4. Learning the Speedup Classifier,[0],[0]
Rg .,4. Learning the Speedup Classifier,[0],[0]
"Finally we define the level margin of a weight vector w for a training set as
γ = min",4. Learning the Speedup Classifier,[0],[0]
"{(v,v′)}
",4. Learning the Speedup Classifier,[0],[0]
"w · ( φ(v)− φ(v′) ) (12)
",4. Learning the Speedup Classifier,[0],[0]
"Here, the set {(v, v′)} contains any pair such that v is ygood, v′ is not y-good, and v and v′ are at the same search level.",4. Learning the Speedup Classifier,[0],[0]
"The level margin denotes the minimum score gap between a y-good and a y-bad node at the same search level.
",4. Learning the Speedup Classifier,[0],[0]
The priority function used to rank the search nodes is defined as pw(v) = g(v)−w,4. Learning the Speedup Classifier,[0],[0]
·φ(v).,4. Learning the Speedup Classifier,[0],[0]
Smaller priority function value ranks higher during search.,4. Learning the Speedup Classifier,[0],[0]
With these definitions we have the following theorem: Theorem 1 (Speedup mistake bound).,4. Learning the Speedup Classifier,[0],[0]
"Given a training set such that there exists a weight vector w with level margin γ > 0 and ‖w‖ = 1, the speedup learning algorithm (Algorithm 1) will converge with a consistent weight vector after making no more than R2φ+2Rg
γ2 weight updates.
",4. Learning the Speedup Classifier,[0],[0]
Proof.,4. Learning the Speedup Classifier,[0],[0]
The complete proof is in the supplementary material of the paper.,4. Learning the Speedup Classifier,[0],[0]
"So far, we have shown that a structured prediction problem can be converted to a beam search problem.",4.1. Avoiding Computing the Input Features,[0],[0]
The priority function for ranking search nodes is determined by p(v) = g(v) + h(v).,4.1. Avoiding Computing the Input Features,[0],[0]
We have seen how the h function be trained to enforce structural constraints.,4.1. Avoiding Computing the Input Features,[0],[0]
"However, there are other opportunities for speeding up as well.
",4.1. Avoiding Computing the Input Features,[0],[0]
"Computing the path cost g(v) involves calculating the corresponding ILP coefficients, which in turn requires feature extraction using the original trained model.",4.1. Avoiding Computing the Input Features,[0],[0]
"This is usually a time-consuming step (Srikumar, 2017), thus motivating the question of whether we can avoid calculating them without losing accuracy.",4.1. Avoiding Computing the Input Features,[0],[0]
"If a search node is strongly preferred by the heuristic function, the path cost is unlikely to reverse the heuristic function’s decision.",4.1. Avoiding Computing the Input Features,[0],[0]
"In this case, we can rank the candidate search nodes with heuristic function only.
",4.1. Avoiding Computing the Input Features,[0],[0]
"Formally, given a fixed beam size b and the beam candidates Ct at step t from which we need to select the beam Bt, we can rank the nodes in Ct from smallest to largest according to the heuristic function value h(v).",4.1. Avoiding Computing the Input Features,[0],[0]
"Denote the bth smallest node as vb and the (b+1)th smallest node as vb+1, we define the heuristic gap ∆t as
∆t = h(vb+1)− h(vb).",4.1. Avoiding Computing the Input Features,[0],[0]
"(13)
If the beam Bt is selected from Ct only according to heuristic function, then ∆t is the gap between the last node in the beam and the first node outside the beam.",4.1. Avoiding Computing the Input Features,[0],[0]
"Next we define the path-cost gap δt as
δt = max v,v′∈Ct
(v − v′) (14)
",4.1. Avoiding Computing the Input Features,[0],[0]
With these definitions we immediately have the following theorem: Theorem 2.,4.1. Avoiding Computing the Input Features,[0],[0]
"Given the beam candidates Ct with heuristic gap ∆t and path-cost gap δt, if ∆t > δt, then using only heuristic function to select the beam Bt will have the same set of nodes selected as using the full priority function up to their ordering in the beam.
",4.1. Avoiding Computing the Input Features,[0],[0]
"If the condition of Theorem 2 holds, then we can rank the candidates using only heuristic function without calculating the path cost.",4.1. Avoiding Computing the Input Features,[0],[0]
This will further save computation time.,4.1. Avoiding Computing the Input Features,[0],[0]
"However, without actually calculating the path cost there is no way to determine the path-cost gap δt at each step.",4.1. Avoiding Computing the Input Features,[0],[0]
"In practice we can treat δt as an empirical parameter θ and define the following priority function
pθ(v) = { h(v), if ∆t > θ, g(v) + h(v), otherwise.
",4.1. Avoiding Computing the Input Features,[0],[0]
(15),4.1. Avoiding Computing the Input Features,[0],[0]
We empirically evaluate the speedup based inference scheme described in Section 4 on the problem of predicting entities and relations (i.e. our running example).,5. Experiments,[0],[0]
"In this task, we are asked to label each entity, and the relation between each pair of the entities.",5. Experiments,[0],[0]
"We assume the entity candidates are given, either from human annotators or from a preprocessing step.",5. Experiments,[0],[0]
"The goal of inference is to determine the types of the entity spans, and the relations between them, as opposed to identify entity candidates.",5. Experiments,[0],[0]
"The research questions we seek to resolve empirically are:
1.",5. Experiments,[0],[0]
Does using a learned speedup heuristic recover structurally valid outputs without paying the inference cost of the integer linear program solver?,5. Experiments,[0],[0]
2.,5. Experiments,[0],[0]
"Can we construct accurate outputs without always computing input features and using only the learned heuristic to guide search?
",5. Experiments,[0],[0]
The dataset we used is from the previous work by Roth & Yih (2004).,5. Experiments,[0],[0]
It contains 1441 sentences.,5. Experiments,[0],[0]
"Each sentence contains several entities with labels, and the labeled relations between every pair of entity.",5. Experiments,[0],[0]
"There are three types of entities, person, location and organization, and five types of relations, Kill, LiveIn, WorkFor, LocatedAt and OrgBasedIn.",5. Experiments,[0],[0]
"There are two constraints associated with each relation type, specifying the allowed source and target arguments.",5. Experiments,[0],[0]
"For example, if the relation label is LiveIn, the source entity must be person and the target entity must be location.",5. Experiments,[0],[0]
"There is also another kind of constraint which says for every pair of entities, they can not have a relation label in both directions between them, i.e., one of the direction must be labeled as NoRel.
",5. Experiments,[0.9541131594435452],"['Additionally, by collecting a second dataset of captions for our images in a different language, such as Spanish, our model could be extended to learn the acoustic correspondences for a given object category in both languages.']"
"We re-implemented the model from the original work using the same set of features as for the entity and relation scoring
functions.",5. Experiments,[0],[0]
"We used 70% of the labeled data to train an ILPbased inference scheme, which will become our black-box solver for learning the speedup classifier.",5. Experiments,[0],[0]
"The remaining 30% labeled data are held out for evaluations.
",5. Experiments,[0],[0]
"We use 29950 sentences from the Gigaword corpus (Graff et al., 2003) to train the speedup classifier.",5. Experiments,[0],[0]
"The entity candidates are extracted using the Stanford Named Entity Recognizer (Manning et al., 2014).",5. Experiments,[0],[0]
"We ignore the entity labels, however, since our task requires determining the type of the entities and relations.",5. Experiments,[0],[0]
"The features we use for the speedup classifiers are counts of the pairs of labels of the form (source label, relation label), (relation label, target label), and counts of the triples of labels of the form (source label, relation label, target label).",5. Experiments,[0],[0]
"We run Algorithm 1 over this unlabeled dataset, and evaluate the resulting speedup classifier on the held out test set.",5. Experiments,[0],[0]
"In all of our speedup search implementations, we first assign labels to the entities from left to right, then the relations among them.
",5. Experiments,[0],[0]
We evaluate the learned speedup classifier in terms of both accuracy and speed.,5. Experiments,[0],[0]
"The accuracy of the speedup classifier can be evaluated using three kinds of metrics: F-1 scores against gold labels, F-1 scores against the ILP solver’s prediction, and the validity ratio, which is the percentage of the predicted examples agreeing with all constraints.6",5. Experiments,[0],[0]
Our first set of experiments evaluates the impact of Algorithm 1.,5.1. Evaluation of Algorithm 1,[0],[0]
These results are shown in Table 1.,5.1. Evaluation of Algorithm 1,[0],[0]
We see the ILP solver achieves perfect entity and relation F-1 when compared with ILP model itself.,5.1. Evaluation of Algorithm 1,[0],[0]
It guarantees all constraints are satisfied.,5.1. Evaluation of Algorithm 1,[0],[0]
Its accuracy against gold label and its prediction time becomes the baselines of our speedup classifiers.,5.1. Evaluation of Algorithm 1,[0],[0]
We also provide two search baselines.,5.1. Evaluation of Algorithm 1,[0],[0]
The first search baseline just uses greedy search without any constraint considerations.,5.1. Evaluation of Algorithm 1,[0],[0]
"In this setting each label is assigned independently, since the step cost of assigning a label to an entity or a relation variable depends only on the corresponding coefficients in the ILP objectives.",5.1. Evaluation of Algorithm 1,[0],[0]
"In this case, a structured prediction problem becomes several independent multi-class classification problems.",5.1. Evaluation of Algorithm 1,[0],[0]
The prediction time is faster than ILP but the validity ratio is rather low (0.29).,5.1. Evaluation of Algorithm 1,[0],[0]
The second search baseline is greedy search with constraint satisfaction.,5.1. Evaluation of Algorithm 1,[0],[0]
The constraints are guaranteed to be satisfied by using the standard arc-consistency search.,5.1. Evaluation of Algorithm 1,[0],[0]
"The prediction takes much longer than the ILP solver (844 ms vs. 239 ms.).
",5.1. Evaluation of Algorithm 1,[0],[0]
We trained a speedup classifier with two different beam sizes.,5.1. Evaluation of Algorithm 1,[0],[0]
"Even with beam width b = 1, we are able to obtain > 95% validity ratio, and the prediction time is much faster
6All our experiments were conducted on a server with eight Intel i7 3.40 GHz cores and 16G memory.",5.1. Evaluation of Algorithm 1,[0],[0]
"We disabled multithreaded execution in all cases for a fair comparison.
than the ILP model.",5.1. Evaluation of Algorithm 1,[0],[0]
"Furthermore, we see that the F-1 score evaluated against gold labels is only slightly worse than ILP model.",5.1. Evaluation of Algorithm 1,[0],[0]
"With beam width b = 2, we recover the ILP model accuracy when evaluated against gold labels.",5.1. Evaluation of Algorithm 1,[0],[0]
The prediction time is still much less than the ILP solver.,5.1. Evaluation of Algorithm 1,[0],[0]
"In this section, we empirically verify the idea that we do not always need to compute the path cost, if the heuristic gap ∆t is large.",5.2. Experiments on Ignoring the Model Cost,[0],[0]
We use the evaluation function pθ(v) in Eq.,5.2. Experiments on Ignoring the Model Cost,[0],[0]
(15) with different values of θ to rank the search nodes.,5.2. Experiments on Ignoring the Model Cost,[0],[0]
"The results are given in Table 2.
",5.2. Experiments on Ignoring the Model Cost,[0],[0]
"For both beam widths, θ = 0 is the case in which the original model is completely ignored.",5.2. Experiments on Ignoring the Model Cost,[0],[0]
All the nodes are ranked using the speedup heuristic function only.,5.2. Experiments on Ignoring the Model Cost,[0],[0]
"Even though it has perfect validity ratio, the result is rather poor when evaluated on F-1 scores.",5.2. Experiments on Ignoring the Model Cost,[0],[0]
"When θ increases, the entity and relation F-1 scores quickly jump up, essentially getting back the same accuracy as the speedup classifiers in Table 1.",5.2. Experiments on Ignoring the Model Cost,[0],[0]
But the prediction time is lowered compared to the results from Table 1.,5.2. Experiments on Ignoring the Model Cost,[0],[0]
The idea of learning memo functions to make computation more efficient goes back to Michie (1968).,6. Discussion and Related Work,[0],[0]
"Speedup learning has been studied since the eighties in the context of general problem solving, where the goal is to learn a problem solver that becomes faster as opposed to becoming more accurate as it sees more data.",6. Discussion and Related Work,[0],[0]
Fern (2011) gives a broad survey of this area.,6. Discussion and Related Work,[0],[0]
"In this paper, we presented a variant of this idea that is more concretely applied to structured output prediction.
",6. Discussion and Related Work,[0],[0]
Efficient inference is a central topic in structured prediction.,6. Discussion and Related Work,[0],[0]
"In order to achieve efficiency, various strategies are adopted in the literature.",6. Discussion and Related Work,[0],[0]
Search based strategies are commonly used for this purpose and several variants abound.,6. Discussion and Related Work,[0],[0]
"The idea of framing a structured prediction problem as a search problem has been explored by several previous works (Collins & Roark, 2004; Daumé III & Marcu, 2005; Daumé III et al., 2009; Huang et al., 2012; Doppa et al., 2014).",6. Discussion and Related Work,[0],[0]
"It usually admits incorporating arbitrary features more easily than fully global structured prediction models like conditional random fields (Lafferty et al., 2001), structured perceptron (Collins, 2002), and structured support vector machines (Taskar et al., 2003; Tsochantaridis et al., 2004).",6. Discussion and Related Work,[0],[0]
"In such cases too, inference can be solved approximately using heuristic search.",6. Discussion and Related Work,[0],[0]
"Either a fixed beam size (Xu et al., 2009), or a dynamicallysized beam (Bodenstab et al., 2011) can be used.",6. Discussion and Related Work,[0],[0]
In our work we fix the beam size.,6. Discussion and Related Work,[0],[0]
The key difference from previous work is that our ranking function combines information from the trained model with the heuristic function which characterizes constraint information.,6. Discussion and Related Work,[0],[0]
"Closely related to the
work described in this paper are approaches that learn to prune the search space (He et al., 2014; Vieira & Eisner, 2016) and learn to select features (He et al., 2013).
",6. Discussion and Related Work,[0],[0]
Another line of recent related work focuses on discovering problem level regularities across the inference space.,6. Discussion and Related Work,[0],[0]
"These amortized inference schemes are designed using deterministic rules for discovering when a new inference problem can re-use previously computed solutions (Srikumar et al., 2012; Kundu et al., 2013) or in the context of a Bayesian network by learning a stochastic inverse network that generates outputs (Stuhlmüller et al., 2013).
",6. Discussion and Related Work,[0],[0]
"Our work is also related to the idea of imitation learning (Daumé III et al., 2009; Ross et al., 2011; Ross & Bagnell, 2014; Chang et al., 2015).",6. Discussion and Related Work,[0],[0]
"In this setting, we are given a reference policy, which may or may not be a good policy.",6. Discussion and Related Work,[0],[0]
"The goal of learning is to learn another policy to imitate the given policy, or even learn a better one.",6. Discussion and Related Work,[0],[0]
Learning usually proceeds in an online fashion.,6. Discussion and Related Work,[0],[0]
"However, imitation learning requires learning a new policy which is independent of the given reference policy, since during test time the reference policy is no longer available.",6. Discussion and Related Work,[0],[0]
"In our case, we can think of the black-box solver as a reference policy.",6. Discussion and Related Work,[0],[0]
"During prediction we always have this solver at our disposal, what we want is avoiding unnecessary calls to the solver.",6. Discussion and Related Work,[0],[0]
"Following recent successes in imitation learning, we expect that we can replace the linear heuristic function with a deep network to avoid feature design.
",6. Discussion and Related Work,[0],[0]
"Also related is the idea of knowledge distillation (Bucilă et al., 2006; Hinton et al., 2015; Kim & Rush, 2016), that seeks to train a student classifier (usually a neural network) to compress and mimic a larger teacher network, thus improve prediction speed.",6. Discussion and Related Work,[0],[0]
The primary difference with the speedup idea of this paper is that our goal is to be more efficient at constructing internally self-consistent structures without explicitly searching over the combinatorially large output space with complex constraints.,6. Discussion and Related Work,[0],[0]
"In this paper, we asked whether we can learn to make inference faster over the lifetime of a structured output classifier.",7. Conclusions,[0],[0]
"To address this question, we developed a search-based strategy that learns to mimic a black-box inference engine but is substantially faster.",7. Conclusions,[0],[0]
We further extended this strategy by identifying cases where the learned search algorithm can avoid expensive input feature extraction to further improve speed without losing accuracy.,7. Conclusions,[0],[0]
We empirically evaluated our proposed algorithms on the problem of extracting entities and relations from text.,7. Conclusions,[0],[0]
"Despite using an object-heavy JVM-based implementation of search, we showed that by exploiting regularities across the output space, we can outperform the industrial strength Gurobi integer linear program solver in terms of speed, while matching its accuracy.
",7. Conclusions,[0],[0]
Acknowledgments We thank the Utah NLP group members and the anonymous reviewers for their valuable feedback.,7. Conclusions,[0],[0]
Predicting structured outputs can be computationally onerous due to the combinatorially large output spaces.,abstractText,[0],[0]
"In this paper, we focus on reducing the prediction time of a trained black-box structured classifier without losing accuracy.",abstractText,[0],[0]
"To do so, we train a speedup classifier that learns to mimic a black-box classifier under the learning-to-search approach.",abstractText,[0],[0]
"As the structured classifier predicts more examples, the speedup classifier will operate as a learned heuristic to guide search to favorable regions of the output space.",abstractText,[0],[0]
We present a mistake bound for the speedup classifier and identify inference situations where it can independently make correct judgments without input features.,abstractText,[0],[0]
We evaluate our method on the task of entity and relation extraction and show that the speedup classifier outperforms even greedy search in terms of speed without loss of accuracy.,abstractText,[0],[0]
Learning to Speed Up Structured Output Prediction,title,[0],[0]
"In conventional ODE modelling coefficients of an equation driving the system state forward in time are estimated. However, for many complex systems it is practically impossible to determine the equations or interactions governing the underlying dynamics. In these settings, parametric ODE model cannot be formulated. Here, we overcome this issue by introducing a novel paradigm of nonparametric ODE modelling that can learn the underlying dynamics of arbitrary continuous-time systems without prior knowledge. We propose to learn non-linear, unknown differential functions from state observations using Gaussian process vector fields within the exact ODE formalism. We demonstrate the model’s capabilities to infer dynamics from sparse data and to simulate the system forward into future.",text,[0],[0]
Dynamical systems modelling is a cornerstone of experimental sciences.,1. Introduction,[0],[0]
"In biology, as well as in physics and chemistry, modelers attempt to capture the dynamical behavior of a given system or a phenomenon in order to improve its understanding and make predictions about its future state.",1. Introduction,[0],[0]
Systems of coupled ordinary differential equations (ODEs) are undoubtedly the most widely used models in science.,1. Introduction,[0],[0]
"Even simple ODE functions can describe complex dynamical behaviours (Hirsch et al., 2004).",1. Introduction,[0],[0]
"Typically, the dynamics are firmly grounded in physics with only a few parameters to be estimated from data.",1. Introduction,[0],[0]
"However, equally ubiquitous are the cases where the governing dynamics are partially or completely unknown.
",1. Introduction,[0],[0]
"We consider the dynamics of a system governed by multi-
*Equal contribution 1Aalto University, Finland 2Helsinki Institute of Information Technology HIIT, Finland.",1. Introduction,[0],[0]
Correspondence to: Markus Heinonen,1. Introduction,[0],[0]
<,1. Introduction,[0],[0]
"markus.o.heinonen@aalto.fi>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"variate ordinary differential functions:
ẋ(t) = dx(t)
dt = f(x(t)) (1)
where x(t) ∈ X",1. Introduction,[0],[0]
"= RD is the state vector of a Ddimensional dynamical system at time t, and the ẋ(t) ∈",1. Introduction,[0],[0]
Ẋ =,1. Introduction,[0],[0]
"RD is the first order time derivative of x(t) that drives the state x(t) forward, and where f :",1. Introduction,[0],[0]
RD → RD is the vector-valued derivative function.,1. Introduction,[0],[0]
"The ODE solution is determined by
x(t) = x0 + ∫",1. Introduction,[0],[0]
t 0,1. Introduction,[0],[0]
"f(x(τ))dτ, (2)
where we integrate the system state from an initial state x(0) = x0 for time t forward.",1. Introduction,[0],[0]
We assume that f(·) is completely unknown and we only observe one or several multivariate time series Y =,1. Introduction,[0],[0]
"(y1, . . .",1. Introduction,[0],[0]
",yN )",1. Introduction,[0],[0]
"T ∈ RN×D obtained from an additive noisy observation model at observation time points T = (t1, . . .",1. Introduction,[0],[0]
", tN ) ∈ RN ,
y(t)",1. Introduction,[0],[0]
= x(t) +,1. Introduction,[0],[0]
"εt, (3)
where εt ∼ N (0,Ω) follows a stationary zero-mean multivariate Gaussian distribution with diagonal noise variances Ω = diag(ω21 , . . .",1. Introduction,[0],[0]
", ω 2 D).",1. Introduction,[0],[0]
The observation time points do not need to be equally spaced.,1. Introduction,[0],[0]
"Our task is to learn the differential function f(·) given observations Y , with no prior knowledge of the ODE system.
",1. Introduction,[0],[0]
"There is a vast literature on conventional ODEs (Butcher, 2016) where a parametric form for function f(x;θ, t) is assumed to be known, and its parameters θ are subsequently optimised with least squares or Bayesian approach, where the expensive forward solution xθ(ti)",1. Introduction,[0],[0]
"=∫ ti 0
f(x(τ);θ, t)dτ is required to evaluate the system responses xθ(ti) from parameters θ against observations y(ti).",1. Introduction,[0],[0]
"To overcome the computationally intensive forward solution, a family of methods denoted as gradient matching (Varah, 1982; Ellner et al., 2002; Ramsay et al., 2007) have proposed to replace the forward solution by matching f(yi)",1. Introduction,[0],[0]
"≈ ẏi to empirical gradients ẏi of the data instead, which do not require the costly integration step.",1. Introduction,[0],[0]
"Recently several authors have proposed embedding a parametric differential function within a Bayesian or Gaussian process (GP) framework (Graepel, 2003; Calderhead et al., 2008;
Dondelinger et al., 2013; Wang and Barber, 2014; Macdonald, 2017) (see Macdonald et al. (2015) for a review).",1. Introduction,[0],[0]
"GPs have been successfully applied to model linear differential equations as they are analytically tractable (Gao et al., 2008; Raissi et al., 2017).
",1. Introduction,[0],[0]
"However, conventional ODE modelling can only proceed if a parametric form of the driving function f(·) is known.",1. Introduction,[0],[0]
"Recently, initial work to handle unknown or non-parametric ODE models have been proposed, although with various limiting approximations.",1. Introduction,[0],[0]
"Early works include spline-based smoothing and additive functions ∑D j fj(xj) to infer gene regulatory networks (De Hoon et al., 2002; Henderson and Michailidis, 2014).",1. Introduction,[0],[0]
"Äijö and Lähdesmäki (2009) proposed estimating the unknown nonlinear function with GPs using either finite time differences, or analytically solving the derivative function as a function of only time, ẋ(t) = f(t) (Äijö et al., 2013).",1. Introduction,[0],[0]
"In a seminal technical report of Heinonen and d’Alche Buc (2014) a full vector-valued kernel model f(x) was proposed, however using a gradient matching approximation.",1. Introduction,[0],[0]
"To our knowledge, there exists no model that can learn non-linear ODE functions ẋ(t) = f(x(t)) over the state x against the true forward solutions x(ti).
",1. Introduction,[0],[0]
"In this work we propose NPODE1: the first ODE model for learning arbitrary, and a priori completely unknown nonparametric, non-linear differential functions f : X → Ẋ from data in a Bayesian way.",1. Introduction,[0],[0]
"We do not use gradient matching or other approximative models, but instead propose to directly optimise the exact ODE system with the fully forward simulated responses against data.",1. Introduction,[0],[0]
"We parameterise our model as an augmented Gaussian process vector field with inducing points, while we propose sensitivity equations to efficiently compute the gradients of the system.",1. Introduction,[0],[0]
"Our model can forecast continuous-time systems arbitrary amounts to future, and we demonstrate the state-of-the-art performance in human motion datasets.",1. Introduction,[0],[0]
"The differential function f(x) to be learned defines a vector field2 f , that is, an assignment of a gradient vector f(x) ∈ RD to every state x ∈ RD.",2. Nonparametric ODE Model,[0],[0]
"We model the vector field as a vector-valued Gaussian process (Rasmussen and Williams, 2006)
f(x) ∼ GP(0,K(x,x′)), (4)
which defines a priori distribution over function values f(x) whose mean and covariances are
E[f(x)]",2. Nonparametric ODE Model,[0],[0]
"= 0 (5) cov[f(x), f(x′)] = K(x,x′), (6)
1The implementation is publicly available in http://www. github.com/cagatayyildiz/npode
2We use vector field and differential function interchangeably.
and where the kernel K(x,x′) ∈ RD×D is matrixvalued.",2. Nonparametric ODE Model,[0],[0]
A GP prior defines that for any collection of states X =,2. Nonparametric ODE Model,[0],[0]
"(x1, . . .",2. Nonparametric ODE Model,[0],[0]
",xN )",2. Nonparametric ODE Model,[0],[0]
"T ∈ RN×D, the function values F = (f(x1), . . .",2. Nonparametric ODE Model,[0],[0]
", f(xN ))",2. Nonparametric ODE Model,[0],[0]
"T ∈ RN×D follow a matrixvalued normal distribution,
p(F ) = N (vec(F )|0,K(X,X)), (7)
where K(X,X) =",2. Nonparametric ODE Model,[0],[0]
"(K(xi,xj))Ni,j=1 ∈",2. Nonparametric ODE Model,[0],[0]
"RND×ND is a block matrix of matrix-valued kernels K(xi,xj).",2. Nonparametric ODE Model,[0],[0]
"The key property of Gaussian processes is that they encode functions where similar states x,x′ induce similar differentials f(x), f(x′), and where the state similarity is defined by the kernel K(x,x′).
",2. Nonparametric ODE Model,[0],[0]
"In standard GP regression we would obtain the posterior of the vector field by conditioning the GP prior with the data (Rasmussen and Williams, 2006).",2. Nonparametric ODE Model,[0],[0]
In ODE models the conditional f(x)|Y of a vector field is intractable due to the integral mapping (2) between observed states y(ti) and differentials f(x).,2. Nonparametric ODE Model,[0],[0]
"Instead, we resort to augmenting the Gaussian process with a set of M inducing points z ∈ X and u ∈",2. Nonparametric ODE Model,[0],[0]
"Ẋ , such that f(z) = u (Quiñonero-Candela and
Rasmussen, 2005).",2. Nonparametric ODE Model,[0],[0]
"We choose to interpolate the differential function between the inducing points as (See Figure 1)
f(x) , Kθ(x, Z)Kθ(Z,Z) −1vec(U), (8)
which supports the function f(x) with inducing locations Z = (z1, . . .",2. Nonparametric ODE Model,[0],[0]
", zM ), inducing vectors U = (u1, . . .",2. Nonparametric ODE Model,[0],[0]
",uM ), and θ are the kernel parameters.",2. Nonparametric ODE Model,[0],[0]
"The function above corresponds to a vector-valued kernel function (Alvarez et al., 2012), or to a multi-task Gaussian process conditional mean without the variance term (Rasmussen and Williams, 2006).",2. Nonparametric ODE Model,[0],[0]
This definition is then compatible with the deterministic nature of the ODE formalism.,2. Nonparametric ODE Model,[0],[0]
"Due to universality of several kernels and kernel functions (Shawe-Taylor and Cristianini, 2004), we can represent arbitrary vector fields with appropriate inducing point and kernel choices.",2. Nonparametric ODE Model,[0],[0]
"The vector-valued kernel function (8) uses operator-valued kernels, which result in matrix-valued kernels Kθ(z, z′) ∈ RD×D for real valued states x, z, while the kernel matrix over data points becomes Kθ = (K(zi, zj))Mi,j=1 ∈ RMD×MD (See Alvarez et al. (2012) for a review).",2.1. Operator-valued Kernels,[0],[0]
"Most straightforward operator-valued kernel is the identity decomposable kernel Kdec(z, z′) = k(z, z′) ·",2.1. Operator-valued Kernels,[0],[0]
"ID, where the scalar Gaussian kernel
Kθ(z, z ′) =",2.1. Operator-valued Kernels,[0],[0]
σ2f exp −1 2 D∑ j=1 (zj − z′j)2,2.1. Operator-valued Kernels,[0],[0]
"`2j  (9) with differential variance σ2f and dimension-specific lengthscales ` = (`1, . . .",2.1. Operator-valued Kernels,[0],[0]
", `D) are expanded into a diagonal matrix of size D × D. We collect the kernel parameters as θ = (σf , `).
",2.1. Operator-valued Kernels,[0],[0]
We note that more complex kernels can also be considered given prior information of the underlying system characteristics.,2.1. Operator-valued Kernels,[0],[0]
"The divergence-free matrix-valued kernel induces vector fields that have zero divergence (Wahlström et al., 2013; Solin et al., 2015).",2.1. Operator-valued Kernels,[0],[0]
"Intuitively, these vector fields do not have sinks or sources, and every state always finally returns to itself after sufficient amount of time.",2.1. Operator-valued Kernels,[0],[0]
"Similarly, curl-free kernels induce curl-free vector fields that can contain sources or sinks, that is, trajectories can accelerate or decelerate.",2.1. Operator-valued Kernels,[0],[0]
"For theoretical treatment of vector field kernels, see (Narcowich and Ward, 1994; Bhatia et al., 2013; Fuselier and Wright, 2017).",2.1. Operator-valued Kernels,[0],[0]
"Non-stationary vector fields can be modeled with input-dependent lengthscales (Heinonen et al., 2016), while spectral kernels can represent stationary (Wilson et al., 2013) or non-stationary (Remes et al., 2017) recurring patterns in the differential function.",2.1. Operator-valued Kernels,[0],[0]
"We assume a Gaussian likelihood over the observations yi and the corresponding simulated responses x(ti) of Equation (2),
p(Y |x0, U, Z,ω) = N∏ i=1 N",2.2. Joint Model,[0],[0]
"(yi|x(ti),Ω), (10)
where x(ti) are forward simulated responses using the integral Equation (2) and differential Equation (8), and Ω = diag(ω21 . .",2.2. Joint Model,[0],[0]
.,2.2. Joint Model,[0],[0]
", ω 2 D) collects the dimension-specific noise variances.
",2.2. Joint Model,[0],[0]
"The inducing vectors have a Gaussian process prior
p(U |Z,θ) = N",2.2. Joint Model,[0],[0]
"(vec(U)|0,Kθ(Z,Z)).",2.2. Joint Model,[0],[0]
"(11)
",2.2. Joint Model,[0],[0]
"The model posterior is then
p(U,x0,θ,ω|Y ) ∝",2.2. Joint Model,[0],[0]
"p(Y |x0, U,ω)p(U |θ) = L, (12)
where we have for brevity omitted the dependency on the locations of the inducing points Z and also the parameter hyperpriors p(θ) and p(ω) since we assume them to be uniform, unless there is specific domain knowledge of the priors.
",2.2. Joint Model,[0],[0]
"The model parameters are the initial state x03, the inducing vectors U , the noise standard deviations ω = (ω1, . . .",2.2. Joint Model,[0],[0]
", ωD), and the kernel hyperparameters θ = (σf , `1, . . .",2.2. Joint Model,[0],[0]
", `D).",2.2. Joint Model,[0],[0]
"We apply a latent parameterisation using Cholesky decomposition LθLTθ = Kθ(Z,Z), which maps the inducing vectors to whitened domain (Kuss and Rasmussen, 2005)
",2.3. Noncentral Parameterisation,[0],[0]
"U = LθŨ , Ũ = L −1 θ U. (13)
",2.3. Noncentral Parameterisation,[0],[0]
The latent variables Ũ are projected on the kernel manifold Lθ to obtain the inducing vectors U .,2.3. Noncentral Parameterisation,[0],[0]
"This non-centered parameterisation (NCP) transforms the hierarchical posterior L of Equation (12) into a reparameterised form
p(x0, Ũ ,θ,ω|Y ) ∝",2.3. Noncentral Parameterisation,[0],[0]
"p(Y |x0, Ũ ,ω,θ)p(Ũ), (14)
where all variables to be optimised are decoupled, with the latent inducing vectors having a standard normal prior Ũ ∼ N (0, I).",2.3. Noncentral Parameterisation,[0],[0]
Optimizing Ũ and θ is now more efficient since they have independent contributions to the vector field via U = LθŨ .,2.3. Noncentral Parameterisation,[0],[0]
"The gradients of the whitened posterior can be retrieved analytically as (Heinonen et al., 2016)
",2.3. Noncentral Parameterisation,[0],[0]
∇Ũ logL = L T,2.3. Noncentral Parameterisation,[0],[0]
"θ∇U logL. (15)
3In case of multiple time-series, we will use one initial state for each time-series.
",2.3. Noncentral Parameterisation,[0],[0]
"Finally, we find a maximum a posteriori (MAP) estimate for the initial state x0, latent vector field Ũ , kernel parameters θ and noise variances ω by gradient ascent,
x0,MAP, ŨMAP,θMAP,ωMAP = arg max x0,Ũ ,θ,ω
logL, (16)
while keeping the inducing locations Z fixed on a sufficiently dense grid (See Figure 1).",2.3. Noncentral Parameterisation,[0],[0]
"The partial derivatives of the posterior with respect to noise parameters ω can be found analytically, while the derivative with respect to σf is approximated with finite differences.",2.3. Noncentral Parameterisation,[0],[0]
We select the optimal lengthscales ` by cross-validation.,2.3. Noncentral Parameterisation,[0],[0]
"The key term to carry out the MAP gradient ascent optimization is the likelihood
log p(Y |x0, Ũ ,ω)
that requires forward integration and computing the partial derivatives with respect to the whitened inducing vectors Ũ .",3. Sensitivity Equations,[0],[0]
Given Equation (15) we only need to compute the gradients with respect to the inducing vectors u = vec(U) ∈,3. Sensitivity Equations,[0],[0]
"RMD,
d log p(Y |x0,u,ω)",3. Sensitivity Equations,[0],[0]
"du
= N∑ s=1 d logN (ys|x(ts,u),Ω)",3. Sensitivity Equations,[0],[0]
"dx dx(ts,u) du .",3. Sensitivity Equations,[0],[0]
"(17)
",3. Sensitivity Equations,[0],[0]
"This requires computing the derivatives of the simulated system response x(t,u) against the vector field parameters u,
dx(t,u)
du ≡ S(t) ∈ RD×MD, (18)
which we denote by Sij(t)",3. Sensitivity Equations,[0],[0]
"= ∂x(t,u)i
∂uj , and expand the no-
tation to make the dependency of x on u explicit.",3. Sensitivity Equations,[0],[0]
"Approximating these with finite differences is possible in principle, but is highly inefficient and has been reported to cause unstability (Raue et al., 2013).",3. Sensitivity Equations,[0],[0]
"We instead turn to sensitivity equations for u and x0 that provide computationally efficient, analytical gradients S(t) (Kokotovic and Heller, 1967; Fröhlich et al., 2017).
",3. Sensitivity Equations,[0],[0]
"The solution for dx(t,u)du can be derived by differentiating the full nonparametric ODE system with respect to u by
d
du
dx(t,u)
dt =
d
du f(x(t,u)).",3. Sensitivity Equations,[0],[0]
"(19)
The sensitivity equation for the given system can be obtained by changing the order of differentiation on the left hand side and carrying out the differentiation on the right hand side.
",3. Sensitivity Equations,[0],[0]
"The resulting sensitivity equation can then be expressed in the form
Ṡ(t)︷ ︸︸ ︷",3. Sensitivity Equations,[0],[0]
"d
dt
dx(t,u)
du =
J(t)︷ ︸︸ ︷",3. Sensitivity Equations,[0],[0]
"∂f(x(t,u))
",3. Sensitivity Equations,[0],[0]
"∂x
S(t)︷ ︸︸ ︷",3. Sensitivity Equations,[0],[0]
"dx(t,u)
",3. Sensitivity Equations,[0],[0]
"du +
R(t)︷ ︸︸ ︷",3. Sensitivity Equations,[0],[0]
"∂f(x(t,u))
",3. Sensitivity Equations,[0],[0]
"∂u ,
(20)
",3. Sensitivity Equations,[0],[0]
"where J(t) ∈ RD×D, R(t), Ṡ(t) ∈ RD×MD (See Supplements for detailed specification).",3. Sensitivity Equations,[0],[0]
"For our nonparametric ODE system the sensitivity equation is fully determined by
J(t) = ∂K(x, Z)
∂x K(Z,Z)−1u",3. Sensitivity Equations,[0],[0]
"(21)
R(t) = K(x, Z)K(Z,Z)−1. (22)
",3. Sensitivity Equations,[0],[0]
The sensitivity equation provides us with an additional ODE system which describes the time evolution of the derivatives with respect to the inducing vectors S(t).,3. Sensitivity Equations,[0],[0]
"The sensitivities are coupled with the actual ODE system and, thus both systems x(t) and S(t) are concatenated as the new augmented state that is solved jointly by Equation (2) driven by the differentials ẋ(t) and Ṡ(t) (Leis and Kramer, 1988).",3. Sensitivity Equations,[0],[0]
The initial sensitivities are computed as S(0) = dx0du .,3. Sensitivity Equations,[0],[0]
"In our implementation, we merge x0 with u for sensitivity analysis to obtain the partial derivatives with respect to the initial state which is estimated along with the other parameters.",3. Sensitivity Equations,[0],[0]
"We use the CVODES solver from the SUNDIALS package (Hindmarsh et al., 2005) to solve the nonparametric ODE models and the corresponding gradients numerically.",3. Sensitivity Equations,[0],[0]
"The sensitivity equation based approach is superior to the finite differences approximation because we have exact formulation for the gradients of state over inducing points, which can be solved up to the numerical accuracy of the ODE solver.",3. Sensitivity Equations,[0],[0]
"As first illustration of the proposed nonparametric ODE method we consider three simulated differential systems: the Van der Pol (VDP), FitzHugh-Nagumo (FHN) and Lotka-Volterra (LV) oscillators of form
VDP : ẋ1 = x2 ẋ2 =",4. Simple Simulated Dynamics,[0],[0]
(1− x21)x2,4. Simple Simulated Dynamics,[0],[0]
− x1 FHN :,4. Simple Simulated Dynamics,[0],[0]
ẋ1 = 3(x1 − x31 3 + x2),4. Simple Simulated Dynamics,[0],[0]
"ẋ2 = 0.2− 3x1 − 0.2x2
3 LV :",4. Simple Simulated Dynamics,[0],[0]
ẋ1 = 1.5x1,4. Simple Simulated Dynamics,[0],[0]
− x1x2,4. Simple Simulated Dynamics,[0],[0]
"ẋ2 = −3x2 + x1x2.
",4. Simple Simulated Dynamics,[0],[0]
"In the conventional ODE case the coefficients of these equations can be inferred using standard statistical techniques if sufficient amount of time series data is available (Girolami, 2008; Raue et al., 2013).",4. Simple Simulated Dynamics,[0],[0]
"Our main goal is to infer unknown dynamics, that is, when these equations are unavailable and we instead represent the dynamics with a nonparametric
vector field of Equation (8).",4. Simple Simulated Dynamics,[0],[0]
"We use these simulated models to only illustrate our model behavior against the true dynamics.
",4. Simple Simulated Dynamics,[0],[0]
"We employ 25 data points from one cycle of noisy observation data from VDP and FHN models, and 25 data points from 1.7 cycles from the LV model with a noise variance of σ2n = 0.1
2.",4. Simple Simulated Dynamics,[0],[0]
"We learn the npODE model with five training sequences using M = 62 inducing locations on a fixed grid, and forecast between 4 and 8 future cycles starting from true initial state x0 at time 0.",4. Simple Simulated Dynamics,[0],[0]
Training takes approximately 100 seconds per oscillator.,4. Simple Simulated Dynamics,[0],[0]
"Figure 2 (bottom) shows the training datasets (grey regions), initial states, true trajectories (black lines) and the forecasted trajectory likelihoods (colored regions).",4. Simple Simulated Dynamics,[0],[0]
"The model accurately learns the dynamics from less than two cycles of data and can reproduce them reliably into future.
",4. Simple Simulated Dynamics,[0],[0]
Figure 2 (top) shows the corresponding true vector field (black arrows) and the estimated vector field (grey arrows).,4. Simple Simulated Dynamics,[0],[0]
"The vector field is a continuous function, which is plotted on a 8x8 grid for visualisation.",4. Simple Simulated Dynamics,[0],[0]
"In general the most difficult part of the system is learning the middle of the loop (as seen in the FHN model), and learning the most outermost regions (bottom left in the LV model).",4. Simple Simulated Dynamics,[0],[0]
"The model learns the
underlying differential f(x) accurately close to observed points, while making only few errors in the border regions with no data.",4. Simple Simulated Dynamics,[0],[0]
"Next, we illustrate how the model estimates realistic, unknown dynamics from noisy observations y(t1), . . .",5. Unknown System Estimation,[0],[0]
",y(tN ).",5. Unknown System Estimation,[0],[0]
"As in Section 4, we make no assumptions on the structure or form of the underlying system, and capture the underlying dynamics with the nonparameteric system alone.",5. Unknown System Estimation,[0],[0]
"We employ no subjective priors, and assume no inputs, controls or other sources of information.",5. Unknown System Estimation,[0],[0]
"The task is to infer the underlying dynamics f(x), and interpolate or extrapolate the state trajectory outside the observed data.
",5. Unknown System Estimation,[0],[0]
We use a benchmark dataset of human motion capture data from the Carnegie Mellon University motion capture (CMU mocap) database.,5. Unknown System Estimation,[0],[0]
"Our dataset contains 50-dimensional pose measurements y(ti) from humans walking, where each pose dimension records a measurement in different parts of the body during movement (Wang et al., 2008).",5. Unknown System Estimation,[0],[0]
We apply the preprocessing of Wang et al. (2008) by downsampling the datasets by a factor of four and centering the data.,5. Unknown System Estimation,[0],[0]
"This resulted in a total of 4303 datapoints spread across 43 trajec-
tories with on average 100 frames per trajectory.",5. Unknown System Estimation,[0],[0]
"In order to tackle the problem of dimensionality, we project the original dataset with PCA to a three dimensional latent space where the system is specified, following Damianou et al. (2011) and Wang et al. (2006).",5. Unknown System Estimation,[0],[0]
"We place M = 53 inducing vectors on a fixed grid, and optimize our model starting from 100 different initial values, which we set by perturbing the projected empirical differences y(ti)−y(ti−1) to the inducing vectors.",5. Unknown System Estimation,[0],[0]
We use an L-BFGS optimizer in Matlab.,5. Unknown System Estimation,[0],[0]
"The whole inference takes approximately few minutes per trajectory.
",5. Unknown System Estimation,[0],[0]
We evaluate the method with two types of experiments: imputing missing values and forecasting future cycles.,5. Unknown System Estimation,[0],[0]
"For the forecasting the first half of the trajectory is reserved for model training, and the second half is to be forecasted.",5. Unknown System Estimation,[0],[0]
"For imputation we remove roughly 20% of the frames from the middle of the trajectory, which are to be filled by the models.",5. Unknown System Estimation,[0],[0]
We perform model selection for lengthscales ` with crossvalidation split of 80/20.,5. Unknown System Estimation,[0],[0]
"We record the root mean square error (RMSE) over test points in the original feature space in both cases, where we reconstruct the original dimensions from the latent space trajectories.
",5. Unknown System Estimation,[0],[0]
"Due to the current lack of ODE methods suitable for this nonparametric inference task, we instead compare our method to the state-of-the-art state-space models where such problems have been previously considered (Wang et al., 2008).",5. Unknown System Estimation,[0],[0]
In a state-space or dynamical model a transition function x(tk+1) = g(x(tk)) moves the system forward in discrete steps.,5. Unknown System Estimation,[0],[0]
"With sufficiently high sampling rate, such models can estimate and forecast finite approximations of smooth dynamics.",5. Unknown System Estimation,[0],[0]
"In Gaussian process dynamical model (Wang et al., 2006; Frigola et al., 2014; Svensson et al., 2016)",5. Unknown System Estimation,[0],[0]
"a GP transition function is inferred in a latent space, which can be inferred with a standard GPLVM (Lawrence, 2004) or with a dependent GPLVM (Zhao and Sun, 2016).",5. Unknown System Estimation,[0],[0]
"In dynamical systems the transition function is replaced by a GP interpolation (Damianou et al., 2011).",5. Unknown System Estimation,[0],[0]
"The discrete time state-space models emphasize inference of a low-dimensional manifold as an explanation of the high-dimensional measurement trajectories.
",5. Unknown System Estimation,[0],[0]
"We compare our method to the dynamical model GPDM of Wang et al. (2006) and to the dynamical system VGPLVM of Damianou et al. (2011), where we directly apply the implementations provided by the authors at inverseprobability.com/vargplvm and dgp.",5. Unknown System Estimation,[0],[0]
toronto.edu/˜jmwang/gpdm.,5. Unknown System Estimation,[0],[0]
"Both methods optimize their latent spaces separately, and they are thus not directly comparable.",5. Unknown System Estimation,[0],[0]
"In the forecasting task we train all models with the first half of the trajectory, while forecasting the second half starting from the first frame.",5.1. Forecasting,[0],[0]
"The models are trained and forecasted
within a low-dimensional space, and subsequently projected back into the original space via inverting the PCA or with GPLVM mean predictions.",5.1. Forecasting,[0],[0]
"As all methods optimize their latent spaces separately, they are not directly comparable.",5.1. Forecasting,[0],[0]
"Thus, the mean errors are computed in the original highdimensional space.",5.1. Forecasting,[0],[0]
"Note that the low-dimensional representation necessarily causes some reconstruction errors.
",5.1. Forecasting,[0],[0]
Figure 3 illustrates the models on one of the trajectories 35 12.amc.,5.1. Forecasting,[0],[0]
"The top part (a) shows the training data in the PCA space for npODE, and optimized training data representation for GPDM and VGPLVM (black points).",5.1. Forecasting,[0],[0]
"The colored lines (npODE) and points (GPDM, VGPLVM) indicate the future forecast.",5.1. Forecasting,[0],[0]
The bottom part (b) shows the first 9 reconstructed original pose dimensions reconstructed from the latent forecasted trajectories.,5.1. Forecasting,[0],[0]
"The training data is shown in grey background, while test data is shown with circles.
",5.1. Forecasting,[0],[0]
"The VGPLVM has most trouble forecasting future points, and reverts quickly after training data to a value close to zero, failing to predict future points.",5.1. Forecasting,[0],[0]
"The GPDM model produces more realistic trajectories, but fails to predict any of the poses accurately.",5.1. Forecasting,[0],[0]
"Finally, npODE can accurately predict five poses, and still retains adequate performance on remaining poses, except for pose 2.
",5.1. Forecasting,[0],[0]
"Furthermore, Table 1 indicates that npODE is also best performing method on average over the whole dataset in the forecasting.",5.1. Forecasting,[0],[0]
In the imputation task we remove approximately 20% of the training data from the middle of the trajectory.,5.2. Imputation,[0],[0]
The goals are to learn a model with the remaining data and to forecast the missing values.,5.2. Imputation,[0],[0]
Figure 4 highlights the performance of the three models on the trajectory 07 07.amc.,5.2. Imputation,[0],[0]
"The top part (a) shows the training data (black points) in the PCA space (npODE) or optimized training locations in the latent space (GPDM, VGPLVM).",5.2. Imputation,[0],[0]
The middle part imputation is shown with colored points or lines.,5.2. Imputation,[0],[0]
"Interestingly both npODE and GPDM operate on cyclic representations, while VGPLVM is not cyclic.
",5.2. Imputation,[0],[0]
"The bottom panel (b) shows the first 9 reconstructed pose
dimensions from the three models.",5.2. Imputation,[0],[0]
"The missing values are shown in circles, while training points are shown with black dots.",5.2. Imputation,[0],[0]
"All models can accurately reproduce the overall trends, while npODE seems to fit slightly worse than the other methods.",5.2. Imputation,[0],[0]
The PCA projection causes the seemingly perfect fit of the npODE prediction (at the top) to lead to slightly warped reconstructions (at the bottom).,5.2. Imputation,[0],[0]
All methods mostly fit the missing parts as well.,5.2. Imputation,[0],[0]
Table 1 shows that on average the npODE and VGPLVM have approximately equal top performance on the imputing missing values task.,5.2. Imputation,[0],[0]
"We proposed the framework of nonparametric ODE model that can accurately learn arbitrary, nonlinear continuos-time dynamics from purely observational data without making assumptions of the underlying system dynamics.",6. Discussion,[0],[0]
We demonstrated that the model excels at learning dynamics that can be forecasted into the future.,6. Discussion,[0],[0]
"We consider this work as the
first in a line of studies of nonparametric ODE systems, and foresee several aspects as future work.",6. Discussion,[0],[0]
"Currently we do not handle non-stationary vector fields, that is time-dependent differentials ft(x).",6. Discussion,[0],[0]
"Furthermore, an interesting future avenue is the study of various vector field kernels, such as divergence-free, curl-free or spectral kernels (Remes et al., 2017).",6. Discussion,[0],[0]
"Finally, including inputs or controls to the system would allow precise modelling in interactive settings, such as robotics.
",6. Discussion,[0],[0]
"The proposed nonparametric ODE model operates along a continuous-time trajectory, while dynamic models such as hidden Markov models or state-space models are restricted to discrete time steps.",6. Discussion,[0],[0]
"These models are unable to consider system state at arbitrary times, for instance, between two successive timepoints.
",6. Discussion,[0],[0]
"Conventional ODE models have also been considered from the stochastic perspective with stochastic differential equation (SDE) models that commonly model the deterministic
system drift and diffusion processes separately leading to a distribution of trajectories p(x(t))",6. Discussion,[0],[0]
"(Archambeau et al., 2007; Garcı́a et al., 2017).",6. Discussion,[0],[0]
"As future work we will consider stochastic extensions of our nonparametric ODE model, as well as MCMC sampling of the inducing point posterior p(U |Y ), leading to trajectory distribution as well.
Acknowledgements.",6. Discussion,[0],[0]
The data used in this project was obtained from mocap.cs.cmu.edu.,6. Discussion,[0],[0]
The database was created with funding from NSF EIA-0196217.,6. Discussion,[0],[0]
"This work has been supported by the Academy of Finland Center of Excellence in Systems Immunology and Physiology, the Academy of Finland grants no. 284597, 311584, 313271, 299915.",6. Discussion,[0],[0]
In conventional ODE modelling coefficients of an equation driving the system state forward in time are estimated.,abstractText,[0],[0]
"However, for many complex systems it is practically impossible to determine the equations or interactions governing the underlying dynamics.",abstractText,[0],[0]
"In these settings, parametric ODE model cannot be formulated.",abstractText,[0],[0]
"Here, we overcome this issue by introducing a novel paradigm of nonparametric ODE modelling that can learn the underlying dynamics of arbitrary continuous-time systems without prior knowledge.",abstractText,[0],[0]
"We propose to learn non-linear, unknown differential functions from state observations using Gaussian process vector fields within the exact ODE formalism.",abstractText,[0],[0]
We demonstrate the model’s capabilities to infer dynamics from sparse data and to simulate the system forward into future.,abstractText,[0],[0]
Learning unknown ODE models with Gaussian processes,title,[0],[0]
"Translating words between languages, or more generally inferring bilingual dictionaries, is a long-studied research direction with applications including machine translation (Lample et al., 2017), multilingual word embeddings (Klementiev et al., 2012), and knowledge transfer to low resource languages (Guo et al., 2016).",1 Introduction,[0],[0]
"Research here has a long history under the guise of decipherment (Knight et al., 2006).",1 Introduction,[0],[0]
"Current contemporary methods have achieve effective word translation through theme-aligned corpora (Gouws et al., 2015), or seed dictionaries (Mikolov et al., 2013).
",1 Introduction,[0],[0]
"Mikolov et al. (2013) showed that monolingual word embeddings exhibit isomorphism across languages, and can be aligned with a simple linear transformation.",1 Introduction,[0],[0]
"Given two sets word vectors learned independently from monolingual corpora, and a dictionary of seed pairs to learn a linear transformation for alignment; they were able to
estimate a complete bilingual lexicon.",1 Introduction,[0.9522793449260147],"['For evaluation, we wish to assign a label to each cluster and cluster member, but this is not completely straightforward since each acoustic segment may capture part of a word, a whole word, multiple words, etc.']"
"Many studies have since followed this approach, proposing various improvements such as orthogonal mappings (Artetxe et al., 2016) and improved objectives (Lazaridou et al., 2015).
",1 Introduction,[0],[0]
Obtaining aligned corpora or bilingual seed dictionaries is nevertheless not straightforward for all language pairs.,1 Introduction,[0],[0]
"This has motivated a wave of very recent research into unsupervised word translation: inducing bilingual dictionaries given only monolingual word embeddings (Conneau et al., 2018; Zhang et al., 2017b,a; Artetxe et al., 2017).",1 Introduction,[0],[0]
"The most successful have leveraged ideas from Generative Adversarial Networks (GANs) (Goodfellow et al., 2014).",1 Introduction,[0],[0]
"In this approach the generator provides the cross-modal mapping, taking embeddings of dictionary words in one language and ‘generating’ their translation in another.",1 Introduction,[0],[0]
The discriminator tries to distinguish between this ‘fake’ set of translations and the true dictionary of embeddings in the target language.,1 Introduction,[0],[0]
"The two play a competitive game, and if the generator learns to fool the discriminator, then its cross-modal mapping should be capable of inducing a complete dictionary, as per Mikolov et al. (2013).
",1 Introduction,[0],[0]
"Despite these successes, such adversarial methods have a number of well-known drawbacks (Arjovsky et al., 2017):",1 Introduction,[0],[0]
"Due to the nature of their min-max game, adversarial training is very unstable, and they are prone to divergence.",1 Introduction,[0],[0]
"It is extremely hyper-parameter sensitive, requiring problem-specific tuning.",1 Introduction,[0],[0]
"Convergence is also hard to diagnose and does not correspond well to efficacy of the generator in downstream tasks (Hoshen and Wolf, 2018).
",1 Introduction,[0],[0]
"In this paper, we propose an alternative statistical dependency-based approach to unsupervised word translation.",1 Introduction,[0],[0]
"Specifically, we propose to search for the cross-lingual word pairing that maximizes statistical dependency in terms of squared
loss mutual information (SMI) (Yamada et al., 2015; Suzuki and Sugiyama, 2010).",1 Introduction,[0],[0]
"Compared to prior statistical dependency-based approaches such as Kernelized Sorting (KS) (Quadrianto et al., 2009) we advance: (i) through use of SMI rather than their Hilbert Schmidt Independence Criterion (HSIC) and (ii) through jointly optimising cross-modal pairing with representation learning within each view.",1 Introduction,[0],[0]
"In contrast to prior work that uses a fixed representation, by non-linearly projecting monolingual world vectors before matching, we learn a new embedding where statistical dependency is easier to establish.",1 Introduction,[0],[0]
"Our method: (i) achieves similar unsupervised translation performance to recent adversarial methods, while being significantly easier to train and (ii) clearly outperforms prior non-adversarial methods.",1 Introduction,[0],[0]
"Let dataset D contain two sets of unpaired monolingual word embeddings from two languages D = ({xi}ni=1, {yj}nj=1) where x,y ∈ Rd.",2.1 Deep Distribution Matching,[0],[0]
"Let π be a permutation function over {1, 2, . . .",2.1 Deep Distribution Matching,[0],[0]
",",2.1 Deep Distribution Matching,[0],[0]
"n}, and Π the corresponding permutation indicator matrix: Π ∈ {0, 1}n×n,Π1n",2.1 Deep Distribution Matching,[0],[0]
"= 1n, and Π>1n = 1n.",2.1 Deep Distribution Matching,[0],[0]
Where 1n is the n-dimensional vector with all ones.,2.1 Deep Distribution Matching,[0],[0]
"We aim to optimize for both the permutation Π (bilingual dictionary), and non-linear transformations gx(·) and gy(·) of the respective wordvectors, that maximize statistical dependency between the views.",2.1 Deep Distribution Matching,[0],[0]
While regularising by requiring the original word embedding information is preserved through reconstruction using decoders fx(·) and fy(·).,2.1 Deep Distribution Matching,[0],[0]
"Our overall loss function is:
min Θx,Θy ,Π Ω(D; Θx,Θy)︸ ︷︷ ︸ Regularizer −λDΠ(D; Θx,Θy)︸ ︷︷ ︸ Dependency ,
DΠ(D; Θx,Θy) = DΠ({gx(xi), gy(yπ(i))}ni=1),
Ω(D; Θx,Θy) = n∑ i=1",2.1 Deep Distribution Matching,[0],[0]
"‖xi − fx(gx(xi))‖22
+ ‖yi",2.1 Deep Distribution Matching,[0],[0]
− fy(gy(yi))‖22 +R(Θx),2.1 Deep Distribution Matching,[0],[0]
"+R(Θy).
(1)
where Θs parameterize the encoding and reconstruction transformations, R(·) is a regularizer (e.g., `2-norm and `1-norm), and DΠ(·, ·) is a statistical dependency measure.",2.1 Deep Distribution Matching,[0],[0]
"Crucially compared to prior methods such as matching CCA (Haghighi
et al., 2008), dependency measures such as SMI do not need comparable representations to get started, making the bootstrapping problem less severe.",2.1 Deep Distribution Matching,[0],[0]
Squared-Loss Mutual Information (SMI),2.2 Dependence Estimation,[0],[0]
"The squared loss mutual information between two random variables x and y is defined as (Suzuki and Sugiyama, 2010):
SMI = ∫∫ ( p(x,y) p(x)p(y)",2.2 Dependence Estimation,[0],[0]
"− 1 )2 p(x)p(y)dxdy,
which is the Pearson divergence (Pearson, 1900) from p(x,y) to p(x)p(y).",2.2 Dependence Estimation,[0],[0]
"The SMI is an f - divergence (Ali and Silvey, 1966).",2.2 Dependence Estimation,[0],[0]
"That is, it is a non-negative measure and is zero only if the random variables are independent.
",2.2 Dependence Estimation,[0],[0]
"To measure SMI from a set of samples we take a direct density ratio estimation approach (Suzuki and Sugiyama, 2010), which leads (Yamada et al., 2015) to the estimator:
ŜMI({(xi,yi)}ni=1)",2.2 Dependence Estimation,[0],[0]
"= 1 2n tr (diag (α̂)KL)− 1 2 ,
where K ∈ Rn×n and L ∈ Rn×n are the gram matricies for x and y respectively, and
Ĥ = 1
n2 (KK>) ◦",2.2 Dependence Estimation,[0],[0]
"(LL>),
ĥ = 1
n",2.2 Dependence Estimation,[0],[0]
"(K ◦L)1n, α̂ =
( Ĥ + λIn )−1 ĥ,
λ > 0 is a regularizer and In ∈ Rn×n is the identity matrix.
",2.2 Dependence Estimation,[0],[0]
"SMI for Matching SMI computes the dependency between two sets of variables, under an assumption of known correspondence.",2.2 Dependence Estimation,[0],[0]
In our application this corresponds to a measure of dependency between two aligned sets of monolingual wordvectors.,2.2 Dependence Estimation,[0],[0]
"To exploit SMI for matching, we introduce a permutation variable Π by replacing L→ Π>LΠ in the estimator:
ŜMI({(xi,yπ(i))}n1 )",2.2 Dependence Estimation,[0],[0]
= 1 2n tr ( diag (α̂Π)KΠ >LΠ ),2.2 Dependence Estimation,[0],[0]
"− 1 2 ,
that will enable optimizing Π to maximize SMI.",2.2 Dependence Estimation,[0],[0]
"To initialize Θx and Θy, we first independently estimate them using autoencoders.",2.3 Optimization of parameters,[0],[0]
Then we employ an alternative optimization on Eq.,2.3 Optimization of parameters,[0],[0]
"(1) for
(Θx,Θy) and Π until convergence.",2.3 Optimization of parameters,[0],[0]
We use 3 layer MLP neural networks for both f and g. Algorithm 1 summarises the steps.,2.3 Optimization of parameters,[0],[0]
"Optimization for Θx and Θy With fixed permutation matrix Π (or π), the objective function
min Θx,Θy
Ω(D; Θx,Θy)− λDΠ(D; Θx,Θy) (2)
is an autoencoder optimization with regularizer DΠ(·), and can be solved with backpropagation.",2.3 Optimization of parameters,[0],[0]
"Optimization for Π To find the permutation (word matching) Π that maximizes SMI given fixed encoding parameters Θx,Θy, we only need to optimize the dependency term DΠ in Eq.",2.3 Optimization of parameters,[0],[0]
(1).,2.3 Optimization of parameters,[0],[0]
"We employ the LSOM algorithm (Yamada et al., 2015).",2.3 Optimization of parameters,[0],[0]
"The estimator of SMI for samples {gx(xi), gy(yπ(i))}ni=1 encoded with gx, gy is:
ŜMI = 1 2n tr ( diag (α̂Θ,Π)KΘxΠ >LΘyΠ )",2.3 Optimization of parameters,[0],[0]
"− 1 2 .
",2.3 Optimization of parameters,[0],[0]
"Which leads to the optimization problem:
max Π∈{0,1}n×n
tr (
diag (α̂Θ,Π)KΘxΠ >LΘyΠ ) s.t. Π1n",2.3 Optimization of parameters,[0],[0]
"= 1n,Π>1n = 1n.",2.3 Optimization of parameters,[0],[0]
"(3)
Since the optimization problem is NP-hard, we iteratively solve the relaxed problem (Yamada et al., 2015):
",2.3 Optimization of parameters,[0],[0]
"Πnew = (1− η)Πold+
η argmax Π
tr ( diag ( α̂Θ,Πold ) KΘxΠ >LΘyΠ old ) ,
where 0 <",2.3 Optimization of parameters,[0],[0]
η ≤ 1 is a step size.,2.3 Optimization of parameters,[0],[0]
The optimization problem is a linear assignment problem (LAP).,2.3 Optimization of parameters,[0],[0]
"Thus, we can efficiently solve the algorithm by using the Hungarian method (Kuhn, 1955).",2.3 Optimization of parameters,[0],[0]
"To get discrete Π, we solve the last step by setting η = 1.
",2.3 Optimization of parameters,[0],[0]
"Intuitively, this can be seen as searching for the permutation Π for which the data in the two (initially unsorted views) have a matching withinview affinity (gram) matrix, where matching is defined by maximum SMI.",2.3 Optimization of parameters,[0],[0]
"In this section, we evaluate the efficacy of our proposed method against various state of the art methods for word translation.",3 Experiments,[0],[0]
Implementation Details,3 Experiments,[0],[0]
Our autoencoder consists of two layers with dropout and a tanh nonlinearity.,3 Experiments,[0],[0]
"We use polynomial kernel to compute
Algorithm 1 SMI-based unsupervised word translation Input: Unpaired word embeddings D = ({xi}ni=1, {yj}nj=1).
1: Init: weights Θx, Θy, permutation matrix Π. 2: while not converged do 3: Update Θx,Θy given Π: Backprop (2).",3 Experiments,[0],[0]
"4: Update Π given Θx,Θy: LSOM (3).",3 Experiments,[0],[0]
"5: end while
Output: Permutation Matrix Π. Params Θx, Θy.
the gram matrices K and L.",3 Experiments,[0],[0]
"For all pairs of languages, we fix the number of training epochs to 20.",3 Experiments,[0],[0]
All the word vectors are `2 unit normalized.,3 Experiments,[0],[0]
For CSLS we set the number of neighbors to 10.,3 Experiments,[0],[0]
"For optimizing Π at each epoch, we set the step size η = 0.75 and use 20 iterations.",3 Experiments,[0],[0]
"For the regularization R(Θ), we use the sum of the Frobenius norms of weight matrices.",3 Experiments,[0],[0]
"We train Θ using full batch gradient-descent, with learning rate 0.05.",3 Experiments,[0],[0]
"Datasets We performed experiments on the publicly available English-Italian, EnglishSpanish and English-Chinese datasets released by (Dinu and Baroni, 2015; Zhang et al., 2017b; Vulic and Moens, 2013).",3 Experiments,[0],[0]
We name this collective set of benchmarks BLI.,3 Experiments,[0],[0]
"We also conduct further experiments on a much larger recent public benchmark, MUSE (Conneau et al., 2018)1.",3 Experiments,[0],[0]
"Setting and Metrics We evaluate all methods in terms of Precision@1, following standard practice.",3 Experiments,[0],[0]
"We note that while various methods in the literature were initially presented as fully supervised (Mikolov et al., 2013), semi-supervised (using a seed dictionary)",3 Experiments,[0],[0]
"(Haghighi et al., 2008), or unsupervised (Zhang et al., 2017b), most of them can be straightforwardly adapted to run in any of these settings.",3 Experiments,[0],[0]
"Therefore we evaluate all methods both in the unsupervised setting in which we are primarily interested, and also the commonly evaluated semi-supervised setting with 500 seed pairs.",3 Experiments,[0],[0]
"Competitors: Non-Adversarial In terms of competitors that, like us, do not make use of GANs, we evaluate: Translation Matrix (Mikolov et al., 2013), which alternates between estimating a linear transformation by least squares and matching by nearest neighbour (NN).",3 Experiments,[0],[0]
"Multilingual Correlation (Faruqui and Dyer, 2014), and Matching CCA (Haghighi et al., 2008), which alternates between matching and estimat-
1https://github.com/facebookresearch/MUSE/
ing a joint linear subspace.",3 Experiments,[0],[0]
"Kernelized Sorting (Quadrianto et al., 2009), which directly uses HSIC-based statistical dependency to match heterogeneous data points.",3 Experiments,[0],[0]
"Self Training (Artetxe et al., 2017)",3 Experiments,[0],[0]
"A recent state of the art method that alternate between estimating an orthonormal transformation, and NN matching.
",3 Experiments,[0],[0]
"Competitors: Adversarial In terms of competitors that do make use of adversarial training, we compare: W-GAN and EMDOT (Zhang et al., 2017b) make use of adversarial learning using Wasserstein GAN and Earth Movers Distance respectively.",3 Experiments,[0],[0]
"GAN-NN (Conneau et al., 2018) uses adversarial learning to train an orthogonal transformation, along with some refinement steps and an improvement to the conventional NN matching procedure called ‘cross-domain similarity lo-
cal scaling’ (CSLS).",3 Experiments,[0],[0]
"Since this is a distinct step, we also evaluate our method with CSLS.
",3 Experiments,[0],[0]
"We use the provided code for GAN-NN and Self-Train, while re-implementing EDOT/WGAN to avoid dependency on theano.",3 Experiments,[0],[0]
Fully Unsupervised Table 1 presents comparative results for unsupervised word translation on BLI and MUSE.,3.1 Results,[0],[0]
From these we observe: (i),3.1 Results,[0],[0]
Our method (bottom) is consistently and significantly better than non-adversarial alternatives (top).,3.1 Results,[0],[0]
(ii),3.1 Results,[0],[0]
"Compared to adversarial alternatives Deep-SMI performs comparably.
",3.1 Results,[0],[0]
All methods generally perform better on the MUSE dataset than BLI.,3.1 Results,[0],[0]
"These differences are due to a few factors: MUSE is a significantly
larger dataset than BLI, benefitting methods that can exploit a large amount of training data.",3.1 Results,[0],[0]
"In the ground-truth annotation, BLI contains 1-1 translations while MUSE contains more realistic 1-many translations (if any correct translation is picked, a success is counted), making it easier to reach a higher score.
",3.1 Results,[0],[0]
Semi-supervised Results using a 500-word bilingual seed dictionary are presented in Table 2.,3.1 Results,[0],[0]
From these we observe: (i),3.1 Results,[0],[0]
"The conventional methods’ performances (top) jump up, showing that they are more competitive if at least some sparse data is available.",3.1 Results,[0],[0]
"(ii) Deep-SMI performance also improves, and still outperforms the classic methods significantly overall.",3.1 Results,[0],[0]
"(iii) Again, we perform comparably to the GAN methods.",3.1 Results,[0],[0]
Figure 1 shows the convergence process of DeepSMI.,3.2 Discussion,[0],[0]
"From this we see that: (i) Unlike the adversarial methods, our objective (Eq. (1)) improves smoothly over time, making convergence much easier to assess.",3.2 Discussion,[0],[0]
"(ii) Unlike the adversarial methods, our accuracy generally mirrors the model’s loss.",3.2 Discussion,[0],[0]
"In contrast, the various losses of the adversarial approaches do not well reflect translation accuracy, making model selection or early stopping a challenge in itself.",3.2 Discussion,[0],[0]
"Please compare our Figure 1 with Fig 3 in Zhang et al. (2017b), and Fig 2 in Conneau et al. (2018).
",3.2 Discussion,[0],[0]
There are two steps in our optimization: matching permutation Π and representation weights Θ.,3.2 Discussion,[0],[0]
"Although this is an alternating optimization, it is analogous to an EM-type algorithm optimizing latent variables (Π) and parameters (Θ).",3.2 Discussion,[0],[0]
"While local minima are a risk, every optimisation step for either variable reduces our objective Eq.",3.2 Discussion,[0],[0]
"(1).
",3.2 Discussion,[0],[0]
"There is no min-max game, so no risk of divergence as in the case of adversarial GAN-type methods.
",3.2 Discussion,[0],[0]
Our method can also be understood as providing an unsupervised Deep-CCA type model for relating heterogeneous data across two views.,3.2 Discussion,[0],[0]
"This is in contrast to the recently proposed unsupervised shallow CCA (Hoshen and Wolf, 2018), and conventional supervised Deep-CCA (Chang et al., 2018) that requires paired data for training; and using SMI rather than correlation as the optimisation objective.",3.2 Discussion,[0.9503802100082958],"['Previous work has presented algorithms for performing acoustic pattern discovery in continuous speech (Park and Glass, 2008; Jansen et al., 2010; Jansen and Van Durme, 2011) without the use of transcriptions or another modality, but those algorithms are limited in their ability to scale by their inherent O(n2) complexity, since they do an exhaustive comparison of the data against itself.']"
We have presented an effective approach to unsupervised word translation that performs comparably to adversarial approaches while being significantly easier to train and diagnose; as well as outperforming prior non-adversarial approaches.,4 Conclusion,[0],[0]
"Word translation, or bilingual dictionary induction, is an important capability that impacts many multilingual language processing tasks.",abstractText,[0],[0]
"Recent research has shown that word translation can be achieved in an unsupervised manner, without parallel seed dictionaries or aligned corpora.",abstractText,[0],[0]
"However, state of the art methods for unsupervised bilingual dictionary induction are based on generative adversarial models, and as such suffer from their well known problems of instability and hyperparameter sensitivity.",abstractText,[0],[0]
We present a statistical dependency-based approach to bilingual dictionary induction that is unsupervised – no seed dictionary or parallel corpora required; and introduces no adversary – therefore being much easier to train.,abstractText,[0],[0]
Our method performs comparably to adversarial alternatives and outperforms prior non-adversarial methods.,abstractText,[0],[0]
Learning Unsupervised Word Translations Without Adversaries,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 1024–1034 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
Learning word representations has become a fundamental problem in processing natural languages.,1 Introduction,[0],[0]
"These semantic representations, which map a word into a point in a linear space, have been widely applied in downstream applications, including named entity recognition (Guo et al., 2014), document ranking (Nalisnick et al., 2016), sentiment analysis (Irsoy and Cardie, 2014), question answering (Antol et al., 2015), and image captioning (Karpathy and Fei-Fei, 2015).
",1 Introduction,[0],[0]
"Over the past few years, various approaches have been proposed to learn word vectors (e.g., (Pennington et al., 2014; Mikolov et al., 2013a; Levy and Goldberg, 2014b; Ji et al., 2015)) based on co-occurrence information between words observed on the training corpus.",1 Introduction,[0],[0]
"The intuition behind this is to represent words with similar vectors if
they have similar contexts.",1 Introduction,[0],[0]
"To learn a good word embedding, most approaches assume a large collection of text is freely available, such that the estimation of word co-occurrences is accurate.",1 Introduction,[0],[0]
"For example, the Google Word2Vec model (Mikolov et al., 2013a) is trained on the Google News dataset, which contains around 100 billion tokens, and the GloVe embedding (Pennington et al., 2014) is trained on a crawled corpus that contains 840 billion tokens in total.",1 Introduction,[0],[0]
"However, such an assumption may not hold for low-resource languages such as Inuit or Sindhi, which are not spoken by many people or have not been put into a digital format.",1 Introduction,[0],[0]
"For those languages, usually, only a limited size corpus is available.",1 Introduction,[0],[0]
"Training word vectors under such a setting is a challenging problem.
",1 Introduction,[0],[0]
One key restriction of the existing approaches is that they often mainly rely on the word pairs that are observed to co-occur on the training data.,1 Introduction,[0],[0]
"When the size of the text corpus is small, most word pairs are unobserved, resulting in an extremely sparse co-occurrence matrix (i.e., most entries are zero)1.",1 Introduction,[0],[0]
"For example, the text82 corpus has about 17,000,000 tokens and 71,000 distinct words.",1 Introduction,[0],[0]
"The corresponding co-occurrence matrix has more than five billion entries, but only about 45,000,000 are non-zeros (observed on the training corpus).",1 Introduction,[0],[0]
"Most existing approaches, such as Glove and Skip-gram, cannot handle a vast number of zero terms in the co-occurrence matrix; therefore, they only sub-sample a small subset of zero entries during the training.
",1 Introduction,[0],[0]
"In contrast, we argue that the unobserved word pairs can provide valuable information for training a word embedding model, especially when the co-occurrence matrix is very sparse.",1 Introduction,[0],[0]
"Inspired
1Note that the zero term can mean either the pairs of words cannot co-occur or the co-occurrence is not observed in the training corpus.
2http://mattmahoney.net/dc/text8.zip
1024
by the success of Positive-Unlabeled Learning (PU-Learning) in collaborative filtering applications (Pan et al., 2008; Hu et al., 2008; Pan and Scholz, 2009; Qin et al., 2010; Paquet and Koenigstein, 2013; Hsieh et al., 2015), we design an algorithm to effectively learn word embeddings from both positive (observed terms) and unlabeled (unobserved/zero terms) examples.",1 Introduction,[0],[0]
"Essentially, by using the square loss to model the unobserved terms and designing an efficient update rule based on linear algebra operations, the proposed PULearning framework can be trained efficiently and effectively.
",1 Introduction,[0],[0]
We evaluate the performance of the proposed approach in English3 and other three resourcescarce languages.,1 Introduction,[0],[0]
"We collected unlabeled language corpora from Wikipedia and compared the proposed approach with popular approaches, the Glove and the Skip-gram models, for training word embeddings.",1 Introduction,[0],[0]
"The experimental results show that our approach significantly outperforms the baseline models, especially when the size of the training corpus is small.
",1 Introduction,[0],[0]
"Our key contributions are summarized below.
",1 Introduction,[0],[0]
"• We propose a PU-Learning framework for learning word embedding.
",1 Introduction,[0],[0]
"• We tailor the coordinate descent algorithm (Yu et al., 2017b) for solving the corresponding optimization problem.
",1 Introduction,[0],[0]
• Our experimental results show that PULearning improves the word embedding training in the low-resource setting.,1 Introduction,[0],[0]
Learning word vectors.,2 Related work,[0],[0]
"The idea of learning word representations can be traced back to Latent Semantic Analysis (LSA) (Deerwester et al., 1990) and Hyperspace Analogue to Language (HAL) (Lund and Burgess, 1996), where word vectors are generated by factorizing a worddocument and word-word co-occurrence matrix, respectively.",2 Related work,[0],[0]
"Similar approaches can also be extended to learn other types of relations between words (Yih et al., 2012; Chang et al., 2013) or entities (Chang et al., 2014).",2 Related work,[0],[0]
"However, due to the limitation of the use of principal component analysis,
3Although English is not a resource-scarce language, we simulate the low-resource setting in an English corpus.",2 Related work,[0],[0]
"In this way, we leverage the existing evaluation methods to evaluate the proposed approach.
",2 Related work,[0],[0]
these approaches are often less flexible.,2 Related work,[0],[0]
"Besides, directly factorizing the co-occurrence matrix may cause the frequent words dominating the training objective.
",2 Related work,[0],[0]
"In the past decade, various approaches have been proposed to improve the training of word embeddings.",2 Related work,[0],[0]
"For example, instead of factorizing the co-occurrence count matrix, Bullinaria and Levy (2007); Levy and Goldberg (2014b) proposed to factorize point-wise mutual information (PMI) and positive PMI (PPMI) matrices as these metrics scale the co-occurrence counts (Bullinaria and Levy, 2007; Levy and Goldberg, 2014b).",2 Related work,[0],[0]
"Skipgram model with negative-sampling (SGNS) and Continuous Bag-of-Words models (Mikolov et al., 2013b) were proposed for training word vectors on a large scale without consuming a large amount of memory.",2 Related work,[0],[0]
"GloVe (Pennington et al., 2014) is proposed as an alternative to decompose a weighted log co-occurrence matrix with a bias term added to each word.",2 Related work,[0],[0]
"Very recently, WordRank model (Ji et al., 2015) has been proposed to minimize a ranking loss which naturally fits the tasks requiring ranking based evaluation metrics.",2 Related work,[0],[0]
Stratos et al. (2015) also proposed CCA (canonical correlation analysis)-based word embedding which shows competitive performance.,2 Related work,[0],[0]
"All these approaches focus on the situations where a large text corpus is available.
",2 Related work,[0],[0]
"Positive and Unlabeled (PU) Learning: Positive and Unlabeled (PU) learning (Li and Liu, 2005) is proposed for training a model when the positive instances are partially labeled and the unlabeled instances are mostly negative.",2 Related work,[0],[0]
"Recently, PU learning has been used in many classification and collaborative filtering applications due to the nature of “implicit feedback” in many recommendation systems—users usually only provide positive feedback (e.g., purchases, clicks) and it is very hard to collect negative feedback.
",2 Related work,[0],[0]
"To resolve this problem, a series of PU matrix completion algorithms have been proposed (Pan et al., 2008; Hu et al., 2008; Pan and Scholz, 2009; Qin et al., 2010; Paquet and Koenigstein, 2013; Hsieh et al., 2015; Yu et al., 2017b).",2 Related work,[0],[0]
The main idea is to assign a small uniform weight to all the missing or zero entries and factorize the corresponding matrix.,2 Related work,[0],[0]
"Among them, Yu et al. (2017b) proposed an efficient algorithm for matrix factorization with PU-learning, such that the weighted matrix is constructed implicitly.",2 Related work,[0],[0]
"In this paper, we
W, C vocabulary of central and context words m,n vocabulary sizes k dimension of word vectors W,H m× k and n×",2 Related work,[0],[0]
"k latent matrices Cij weight for the (i, j) entry Aij value of the PPMI matrix Qij value of the co-occurrence matrix wi,hj i-th row of W and j-th row of H b, b̂ bias term λi, λj regularization parameters | · | the size of a set Ω Set of possible word-context pairs Ω+ Set of observed word-context pairs Ω− Set of unobserved word-context pairs
Table 1: Notations.
design a new approach for training word vectors by leveraging the PU-Learning framework and existing word embedding techniques.",2 Related work,[0],[0]
"To the best of our knowledge, this is the first work to train word embedding models using the PU-learning framework.",2 Related work,[0],[0]
"Similar to GloVe and other word embedding learning algorithms, the proposed approach consists of three steps.",3 PU-Learning for Word Embedding,[0],[0]
The first step is to construct a cooccurrence matrix.,3 PU-Learning for Word Embedding,[0],[0]
"Follow the literature (Levy and Goldberg, 2014a), we use the PPMI metric to measure the co-occurrence between words.",3 PU-Learning for Word Embedding,[0],[0]
"Then, in the second step, a PU-Learning approach is applied to factorize the co-occurrence matrix and generate word vectors and context vectors.",3 PU-Learning for Word Embedding,[0],[0]
"Finally, a post-processing step generates the final embedding vector for each word by combining the word vector and the context vector.
",3 PU-Learning for Word Embedding,[0],[0]
We summarize the notations used in this paper in Table 1 and describe the details of each step in the remainder of this section.,3 PU-Learning for Word Embedding,[0],[0]
Various metrics can be used for estimating the co-occurrence between words in a corpus.,3.1 Building the Co-Occurrence Matrix,[0],[0]
"PPMI metric stems from point-wise mutual information (PMI) which has been widely used as a measure of word association in NLP for various tasks (Church and Hanks, 1990).",3.1 Building the Co-Occurrence Matrix,[0],[0]
"In our case, each entry PMI(w, c) represents the relevant measure between a word w and a context word c by calculating the ratio between their joint probability (the
chance they appear together in a local context window) and their marginal probabilities (the chance they appear independently) (Levy and Goldberg, 2014b).",3.1 Building the Co-Occurrence Matrix,[0],[0]
"More specifically, each entry of PMI matrix can be defined by
PMI(w, c) = log P̂ (w, c)
P̂ (w) ·",3.1 Building the Co-Occurrence Matrix,[0],[0]
"P̂ (c) , (1)
where P̂ (w), P̂ (c) and P̂ (w, c) are the the frequency of word w, word c, and word pairs (w, c), respectively.",3.1 Building the Co-Occurrence Matrix,[0],[0]
"The PMI matrix can be computed based on the co-occurrence counts of word pairs, and it is an information-theoretic association measure which effectively eliminates the big differences in magnitude among entries in the cooccurrence matrix.
",3.1 Building the Co-Occurrence Matrix,[0],[0]
"Extending from the PMI metric, the PPMI metric replaces all the negative entries in PMI matrix by 0:
PPMI(w, c) = max(PMI(w, c), 0).",3.1 Building the Co-Occurrence Matrix,[0],[0]
"(2)
The intuition behind this is that people usually perceive positive associations between words (e.g. “ice” and “snow”).",3.1 Building the Co-Occurrence Matrix,[0],[0]
"In contrast, the negative association is hard to define (Levy and Goldberg, 2014b).",3.1 Building the Co-Occurrence Matrix,[0],[0]
"Therefore, it is reasonable to replace the negative entries in the PMI matrix by 0, such that the negative association is treated as “uninformative”.",3.1 Building the Co-Occurrence Matrix,[0],[0]
"Empirically, several existing works (Levy et al., 2015; Bullinaria and Levy, 2007) showed that the PPMI metric achieves good performance on various semantic similarity tasks.
",3.1 Building the Co-Occurrence Matrix,[0],[0]
"In practice, we follow the pipeline described in Levy et al. (2015) to build the PPMI matrix and apply several useful tricks to improve its quality.",3.1 Building the Co-Occurrence Matrix,[0],[0]
"First, we apply a context distribution smoothing mechanism to enlarge the probability of sampling a rare context.",3.1 Building the Co-Occurrence Matrix,[0],[0]
"In particular, all context counts are scaled to the power of α.4:
PPMIα(w, c) = max
( log P̂ (w, c)
P̂ (w)P̂α(c) , 0
)
P̂α(c) = #(c)α∑ c̄ #(c̄) α ,
where #(w) denotes the number of times word w appears.",3.1 Building the Co-Occurrence Matrix,[0],[0]
"This smoothing mechanism effectively
4Empirically, α = 0.75 works well (Mikolov et al., 2013b).
",3.1 Building the Co-Occurrence Matrix,[0],[0]
"alleviates PPMI’s bias towards rare words (Levy et al., 2015).
",3.1 Building the Co-Occurrence Matrix,[0],[0]
"Next, previous studies show that words that occur too frequent often dominate the training objective (Levy et al., 2015) and degrade the performance of word embedding.",3.1 Building the Co-Occurrence Matrix,[0],[0]
"To avoid this issue, we follow Levy et al. (2015) to sub-sample words with frequency more than a threshold twith a probability p defined as:
p = 1− √ t
P̂ (w) .",3.1 Building the Co-Occurrence Matrix,[0],[0]
We proposed a matrix factorization based word embedding model which aims to minimize the reconstruction error on the PPMI matrix.,3.2 PU-Learning for Matrix Factorization,[0],[0]
"The lowrank embeddings are obtained by solving the following optimization problem:
min W,H
∑
i,j∈Ω",3.2 PU-Learning for Matrix Factorization,[0],[0]
Cij(Aij −wTi hj,3.2 PU-Learning for Matrix Factorization,[0],[0]
"− bi − b̂j)2 + ∑
i
λi‖wi‖2 + ∑
j
λj‖hj‖2, (3)
where W and H are m× k and n× k latent matrices, representing words and context words, respectively.",3.2 PU-Learning for Matrix Factorization,[0],[0]
The first term in Eq.,3.2 PU-Learning for Matrix Factorization,[0],[0]
"(3) aims for minimizing reconstruction error, and the second and third terms are regularization terms.",3.2 PU-Learning for Matrix Factorization,[0],[0]
λi and λj are weights of regularization term.,3.2 PU-Learning for Matrix Factorization,[0],[0]
"They are hyperparameters that need to be tuned.
",3.2 PU-Learning for Matrix Factorization,[0],[0]
"The zero entries in co-occurrence matrix denote that two words never appear together in the current corpus, which also refers to unobserved terms.",3.2 PU-Learning for Matrix Factorization,[0],[0]
The unobserved term can be either real zero (two words shouldn’t be co-occurred even when we use very large corpus) or just missing in the small corpus.,3.2 PU-Learning for Matrix Factorization,[0],[0]
"In contrast to SGNS sub-sampling a small set of zero entries as negative samples, our model will try to use the information from all zeros.
",3.2 PU-Learning for Matrix Factorization,[0],[0]
"The set Ω includes all the |W| × |C| entries— both positive and zero entries:
Ω = Ω+ ∪ Ω−. (4)
Note that we define the positive samples Ω+ to be all the (w, c) pairs that appear at least one time in the corpus, and negative samples Ω− are word pairs that never appear in the corpus.
Weighting function.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"Eq (3) is very similar to the one used in previous matrix factorization approaches such as GloVe, but we propose a new way to set the weights Cij .",3.2 PU-Learning for Matrix Factorization,[0],[0]
"If we set equal weights for all the entries, then Cij = constant, and the model is very similar to conducting SVD for the PPMI matrix.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"Previous work has shown that this approach often suffers from poor performance (Pennington et al., 2014).",3.2 PU-Learning for Matrix Factorization,[0],[0]
"More advanced methods, such as GloVe, set non-uniform weights for observed entries to reflect their confidence.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"However, the time complexity of their algorithm is proportional to number of nonzero weights (|(i, j) | Cij 6= 0|), thus they have to set zero weights for all the unobserved entries (Cij = 0 for Ω−), or try to incorporate a small set of unobserved entries by negative sampling.
",3.2 PU-Learning for Matrix Factorization,[0],[0]
"We propose to set the weights for Ω+ and Ω−
differently using the following scheme:
Cij =   (Qij/xmax)",3.2 PU-Learning for Matrix Factorization,[0],[0]
"α, if Qij ≤ xmax, and (i, j) ∈ Ω+
1,",3.2 PU-Learning for Matrix Factorization,[0],[0]
"if Qij > xmax, and (i, j) ∈ Ω+",3.2 PU-Learning for Matrix Factorization,[0],[0]
"ρ, (i, j) ∈ Ω−
(5)
",3.2 PU-Learning for Matrix Factorization,[0],[0]
"Here xmax and α are re-weighting parameters, and ρ is the unified weight for unobserved terms.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"We will discuss them later.
",3.2 PU-Learning for Matrix Factorization,[0],[0]
"For entries in Ω+, we set the non-uniform weights as in GloVe (Pennington et al., 2014), which assigns larger weights to context word that appears more often with the given word, but also avoids overwhelming the other terms.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"For entries in Ω−, instead of setting their weights to be 0, we assign a small constant weight ρ.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"The main idea is from the literature of PU-learning (Hu et al., 2008; Hsieh et al., 2015): although missing entries are highly uncertain, they are still likely to be true 0, so we should incorporate them in the learning process but multiplying with a smaller weight according to the uncertainty.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"Therefore, ρ in (5) reflects how confident we are to the zero entries.
",3.2 PU-Learning for Matrix Factorization,[0],[0]
"In our experiments, we set xmax = 10, α = 3/4 according to (Pennington et al., 2014), and let ρ be a parameter to tune.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"Experiments show that adding weighting function obviously improves the performance especially on analogy tasks.
",3.2 PU-Learning for Matrix Factorization,[0],[0]
Bias term.,3.2 PU-Learning for Matrix Factorization,[0],[0]
"Unlike previous work on PU matrix completion (Yu et al., 2017b; Hsieh et al., 2015), we add the bias terms for word and context word
vectors.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"Instead of directly using w>i hj to approximate Aij , we use
Aij ≈ w>i hj + bi + b̂j .
Yu et al. (2017b) design an efficient columnwise coordinate descent algorithm for solving the PU matrix factorization problem; however, they do not consider the bias term in their implementations.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"To incorporate the bias term in (3), we propose the following training algorithm based on the coordinate descent approach.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"Our algorithm does not introduce much overhead compared to that in (Yu et al., 2017b).
",3.2 PU-Learning for Matrix Factorization,[0],[0]
"We augment each wi,hj ∈ Rk into the following (k + 2) dimensional vectors:
w′i =   wi1 ...",3.2 PU-Learning for Matrix Factorization,[0],[0]
"wik 1 bi   h′j =   hj1 ... hjk b̂j 1  
Therefore, for each word and context vector, we have the following equality
〈w′i,h′j〉 = 〈wi,hj〉+ bi + b̂j ,
which means the loss function in (3) can be written as
∑
i,j∈Ω Cij(Aij −w′>i h′j)2.
",3.2 PU-Learning for Matrix Factorization,[0],[0]
"Also, we denote W ′ =",3.2 PU-Learning for Matrix Factorization,[0],[0]
"[w′1,w ′ 2, . . .",3.2 PU-Learning for Matrix Factorization,[0],[0]
",w ′ n] > and H ′ =",3.2 PU-Learning for Matrix Factorization,[0],[0]
"[h′1,h ′ 2, . . .",3.2 PU-Learning for Matrix Factorization,[0],[0]
",h ′ n]",3.2 PU-Learning for Matrix Factorization,[0],[0]
>.,3.2 PU-Learning for Matrix Factorization,[0],[0]
"In the column-wise coordinate descent method, at each iteration we pick a t ∈ {1, . . .",3.2 PU-Learning for Matrix Factorization,[0],[0]
", (k+2)}, and update the t-th column of W ′ and H ′. The updates can be derived for the following two cases:
a. When t ≤ k, the elements in the t-th column is w1t, . . .",3.2 PU-Learning for Matrix Factorization,[0],[0]
", wnt and we can directly use the update rule derived in Yu et al. (2017b) to update them.
",3.2 PU-Learning for Matrix Factorization,[0],[0]
b.,3.2 PU-Learning for Matrix Factorization,[0],[0]
"When t = k + 1, we do not update the corresponding column of W ′ since the elements are all 1, and we use the similar coordinate descent update to update the k+ 1-th column of H ′ (corresponding to b̂1, . . .",3.2 PU-Learning for Matrix Factorization,[0],[0]
", b̂n).",3.2 PU-Learning for Matrix Factorization,[0],[0]
"When t = k+2, we do not update the corresponding column of H ′",3.2 PU-Learning for Matrix Factorization,[0],[0]
(they are all 1) and we update the k+ 2-th column of W ′,3.2 PU-Learning for Matrix Factorization,[0],[0]
"(corresponding to b1, . . .",3.2 PU-Learning for Matrix Factorization,[0],[0]
", bn) using coordinate descent.
",3.2 PU-Learning for Matrix Factorization,[0],[0]
"With some further derivations, we can show that the algorithm only requires O(nnz(A) + nk) time to update each column,5 so the overall complexity is O(nnz(A)k + nk2) time per epoch, which is only proportional to number of nonzero terms in A. Therefore, with the same time complexity as GloVe, we can utilize the information from all the zero entries in A instead of only sub-sampling a small set of zero entries.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"In the PU-Learning formulation, ρ represents the unified weight that assigned to the unobserved terms.",3.3 Interpretation of Parameters,[0],[0]
"Intuitively, ρ reflects the confidence on unobserved entries—larger ρmeans that we are quite certain about the zeroes, while small ρ indicates the many of unobserved pairs are not truly zero.",3.3 Interpretation of Parameters,[0],[0]
"When ρ = 0, the PU-Learning approach reduces to a model similar to GloVe, which discards all the unobserved terms.",3.3 Interpretation of Parameters,[0],[0]
"In practice, ρ is an important parameter to tune, and we find that ρ = 0.0625 achieves the best results in general.",3.3 Interpretation of Parameters,[0],[0]
"Regarding the other parameter, λ is the regularization term for preventing the embedding model from overfitting.",3.3 Interpretation of Parameters,[0],[0]
"In practice, we found the performance is not very sensitive to λ as long as it is resonably small.",3.3 Interpretation of Parameters,[0],[0]
"More discussion about the parameter setting can be found in Section 5.
",3.3 Interpretation of Parameters,[0],[0]
Post-processing of Word/Context Vectors The PU-Learning framework factorizes the PPMI matrix and generates two vectors for each word,3.3 Interpretation of Parameters,[0],[0]
"i, wi ∈ Rk and hi ∈ Rk.",3.3 Interpretation of Parameters,[0],[0]
The former represents the word when it is the central word and the latter represents the word when it is in context.,3.3 Interpretation of Parameters,[0],[0]
Levy et al. (2015) shows that averaging these two vectors (uavgi = wi + hi) leads to consistently better performance.,3.3 Interpretation of Parameters,[0],[0]
The same trick of constructing word vectors is also used in GloVe.,3.3 Interpretation of Parameters,[0],[0]
"Therefore, in the experiments, we evaluate all models with uavg.",3.3 Interpretation of Parameters,[0],[0]
Our goal in this paper is to train word embedding models for low-resource languages.,4 Experimental Setup,[0],[0]
"In this section, we describe the experimental designs to evaluate the proposed PU-learning approach.",4 Experimental Setup,[0],[0]
We first describe the data sets and the evaluation metrics.,4 Experimental Setup,[0],[0]
"Then, we provide details of parameter tuning.
",4 Experimental Setup,[0],[0]
5Here we assume m = n for the sake of simplicity.,4 Experimental Setup,[0],[0]
"And, nnz(A) denotes the number of nonzero terms in the matrix A.",4 Experimental Setup,[0],[0]
"We consider two widely used tasks for evaluating word embeddings, the word similarity task and the word analogy task.",4.1 Evaluation tasks,[0],[0]
"In the word similarity task, each question contains a word pairs and an annotated similarity score.",4.1 Evaluation tasks,[0],[0]
The goal is to predict the similarity score between two words based on the inner product between the corresponding word vectors.,4.1 Evaluation tasks,[0],[0]
"The performance is then measured by the Spearmans rank correlation coefficient, which estimates the correlation between the model predictions and human annotations.",4.1 Evaluation tasks,[0],[0]
"Following the settings in literature, the experiments are conducted on five data sets, WordSim353 (Finkelstein et al., 2001), WordSim Similarity (Zesch et al., 2008), WordSim Relatedness (Agirre et al., 2009), Mechanical Turk (Radinsky et al., 2011) and MEN (Bruni et al., 2012).
",4.1 Evaluation tasks,[0],[0]
"In the word analogy task, we aim at solving analogy puzzles like “man is to woman as king is to ?”, where the expected answer is “queen.”",4.1 Evaluation tasks,[0],[0]
"We consider two approaches for generating answers to the puzzles, namely 3CosAdd and 3CosMul (see (Levy and Goldberg, 2014a) for details).",4.1 Evaluation tasks,[0],[0]
"We evaluate the performances on Google analogy dataset (Mikolov et al., 2013a) which contains 8,860 semantic and 10,675 syntactic questions.",4.1 Evaluation tasks,[0],[0]
"For the analogy task, only the answer that exactly matches the annotated answer is counted as correct.",4.1 Evaluation tasks,[0],[0]
"As a result, the analogy task is more difficult than the similarity task because the evalu-
ation metric is stricter and it requires algorithms to differentiate words with similar meaning and find the right answer.
",4.1 Evaluation tasks,[0],[0]
"To evaluate the performances of models in the low-resource setting, we train word embedding models on Dutch, Danish, Czech and, English data sets collected from Wikipedia.",4.1 Evaluation tasks,[0],[0]
"The original Wikipedia corpora in Dutch, Danish, Czech and English contain 216 million, 47 million, 92 million, and 1.8 billion tokens, respectively.",4.1 Evaluation tasks,[0],[0]
"To simulate the low-resource setting, we sub-sample the Wikipedia corpora and create a subset of 64 million tokens for Dutch and Czech and a subset of 32 million tokens for English.",4.1 Evaluation tasks,[0],[0]
"We will demonstrate how the size of the corpus affects the performance of embedding models in the experiments.
",4.1 Evaluation tasks,[0],[0]
"To evaluate the performance of word embeddings in Czech, Danish, and Dutch, we translate the English similarity and analogy test sets to the other languages by using Google Cloud Translation API6.",4.1 Evaluation tasks,[0],[0]
"However, an English word may be translated to multiple words in another language (e.g., compound nouns).",4.1 Evaluation tasks,[0],[0]
We discard questions containing such words (see Table 3 for details).,4.1 Evaluation tasks,[0],[0]
"Because all approaches are compared on the same test set for each language, the comparisons are fair.",4.1 Evaluation tasks,[0],[0]
"We compare the proposed approach with two baseline methods, GloVe and SGNS.",4.2 Implementation and Parameter Setting,[0],[0]
"The imple-
6https://cloud.google.com/translate
mentations of Glove7 and SGNS8 and provided by the original authors, and we apply the default settings when appropriate.",4.2 Implementation and Parameter Setting,[0],[0]
The proposed PULearning framework is implemented based on Yu et al. (2017a).,4.2 Implementation and Parameter Setting,[0],[0]
"With the implementation of efficient update rules, our model requires less than 500 seconds to perform one iteration over the entire text8 corpus, which consists of 17 million tokens 9.",4.2 Implementation and Parameter Setting,[0],[0]
"All the models are implemented in C++.
",4.2 Implementation and Parameter Setting,[0],[0]
"We follow Levy et al. (2015)10 to set windows size as 15, minimal count as 5, and dimension of word vectors as 300 in the experiments.",4.2 Implementation and Parameter Setting,[0],[0]
Training word embedding models involves selecting several hyper-parameters.,4.2 Implementation and Parameter Setting,[0],[0]
"However, as the word embeddings are usually evaluated in an unsupervised setting (i.e., the evaluation data sets are not seen during the training), the parameters should not be tuned on each dataset.",4.2 Implementation and Parameter Setting,[0],[0]
"To conduct a fair comparison, we tune hyper-parameters on the text8 dataset.",4.2 Implementation and Parameter Setting,[0],[0]
"For GloVe model, we tune the discount parameters xmax and find that xmax = 10 per-
7https://nlp.stanford.edu/projects/glove 8https://code.google.com/archive/p/word2vec/ 9http://mattmahoney.net/dc/text8.zip
10https://bitbucket.org/omerlevy/hyperwords
forms the best.",4.2 Implementation and Parameter Setting,[0],[0]
SGNS has a natural parameter k which denotes the number of negative samples.,4.2 Implementation and Parameter Setting,[0],[0]
"Same as Levy et al. (2015), we found that setting k to 5 leads to the best performance.",4.2 Implementation and Parameter Setting,[0],[0]
"For the PU-learning model, ρ and λ are two important parameters that denote the unified weight of zero entries and the weight of regularization terms, respectively.",4.2 Implementation and Parameter Setting,[0],[0]
We tune ρ in a range from 2−1 to 2−14 and λ in a range from 20 to 2−10.,4.2 Implementation and Parameter Setting,[0],[0]
We analyze the sensitivity of the model to these hyper-parameters in the experimental result section.,4.2 Implementation and Parameter Setting,[0],[0]
The best performance of each model on the text8 dataset is shown in the Table 2.,4.2 Implementation and Parameter Setting,[0],[0]
It shows that PU-learning model outperforms two baseline models.,4.2 Implementation and Parameter Setting,[0],[0]
"We compared the proposed PU-Learning framework with two popular word embedding models – SGNS (Mikolov et al., 2013b) and Glove (Pennington et al., 2014) on English and three other languages.",5 Experimental Results,[0],[0]
The experimental results are reported in Table 4.,5 Experimental Results,[0],[0]
The results show that the proposed PULearning framework outperforms the two baseline approaches significantly in most datasets.,5 Experimental Results,[0],[0]
"This re-
sults confirm that the unobserved word pairs carry important information and the PU-Learning model leverages such information and achieves better performance.",5 Experimental Results,[0],[0]
"To better understand the model, we conduct detailed analysis as follows.
",5 Experimental Results,[0],[0]
"Performance v.s. Corpus size We investigate the performance of our algorithm with respect to different corpus size, and plot the results in Figure 1.",5 Experimental Results,[0],[0]
"The results in analogy task are obtained by 3CosMul method (Levy and Goldberg, 2014a).",5 Experimental Results,[0],[0]
"As the corpus size grows, the performance of all models improves, and the PU-learning model consistently outperforms other methods in all the tasks.",5 Experimental Results,[0],[0]
"However, with the size of the corpus increases, the difference becomes smaller.",5 Experimental Results,[0],[0]
"This is reasonable as when the corpus size increases the number of nonzero terms becomes smaller and the PU-learning approach is resemblance to Glove.
",5 Experimental Results,[0],[0]
Impacts of ρ and λ,5 Experimental Results,[0],[0]
"We investigate how sensitive the model is to the hyper-parameters, ρ and λ.",5 Experimental Results,[0],[0]
"Figure 2 shows the performance along with various values of λ and ρ when training on the text8 corpus, respectively.",5 Experimental Results,[0],[0]
Note that the x-axis is in log scale.,5 Experimental Results,[0],[0]
"When ρ is fixed, a big λ degrades the performance of the model significantly.",5 Experimental Results,[0],[0]
This is because when λ is too big the model suffers from underfitting.,5 Experimental Results,[0],[0]
"The model is less sensitive when λ is small and in general, λ = 2−11 achieves consistently good performance.
",5 Experimental Results,[0],[0]
"When λ is fixed, we observe that large ρ (e.g., ρ ≈ 2−4) leads to better performance.",5 Experimental Results,[0],[0]
"As ρ represents the weight assigned to the unobserved term, this result confirms that the model benefits from using the zero terms in the co-occurrences matrix.",5 Experimental Results,[0],[0]
"In this paper, we presented a PU-Learning framework for learning word embeddings of lowresource languages.",6 Conclusion,[0],[0]
"We evaluated the proposed approach on English and other three languages and showed that the proposed approach outperforms other baselines by effectively leveraging the information from unobserved word pairs.
",6 Conclusion,[0],[0]
"In the future, we would like to conduct experiments on other languages where available text corpora are relatively hard to obtain.",6 Conclusion,[0],[0]
"We are also interested in applying the proposed approach to domains, such as legal documents and clinical notes, where the amount of accessible data is small.",6 Conclusion,[0],[0]
"Besides, we plan to study how to leverage other information to facilitate the training of word embeddings under the low-resource setting.
",6 Conclusion,[0],[0]
"Acknowledge
This work was supported in part by National Science Foundation Grant IIS-1760523, IIS-1719097 and an NVIDIA Hardware Grant.",6 Conclusion,[0],[0]
Word embedding is a key component in many downstream applications in processing natural languages.,abstractText,[0],[0]
Existing approaches often assume the existence of a large collection of text for learning effective word embedding.,abstractText,[0],[0]
"However, such a corpus may not be available for some low-resource languages.",abstractText,[0],[0]
"In this paper, we study how to effectively learn a word embedding model on a corpus with only a few million tokens.",abstractText,[0],[0]
"In such a situation, the co-occurrence matrix is sparse as the co-occurrences of many word pairs are unobserved.",abstractText,[0],[0]
"In contrast to existing approaches often only sample a few unobserved word pairs as negative samples, we argue that the zero entries in the co-occurrence matrix also provide valuable information.",abstractText,[0],[0]
We then design a Positive-Unlabeled Learning (PU-Learning) approach to factorize the co-occurrence matrix and validate the proposed approaches in four different languages.,abstractText,[0],[0]
Learning Word Embeddings for Low-resource Languages by PU Learning,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4829–4833 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4829",text,[0],[0]
Co-reference resolution requires models to cluster mentions that refer to the same physical entities.,1 Introduction,[0],[0]
The models based on neural networks typically require different levels of semantic representations of input sentences.,1 Introduction,[0],[0]
"The models usually need to calculate the representations of word spans, or mentions, given pre-trained character and wordlevel embeddings (Turian et al., 2010; Pennington et al., 2014) before predicting antecedents.",1 Introduction,[0],[0]
"The mention-level embeddings are used to make coreference decisions, typically by scoring mention pairs and making links (Lee et al., 2017; Clark and Manning, 2016a; Wiseman et al., 2016).",1 Introduction,[0],[0]
"Long short-term memories (LSTMs) are often used to encode the syntactic and semantic information of input sentences.
",1 Introduction,[0],[0]
Articles and conversations include more than one sentences.,1 Introduction,[0],[0]
"Considering the accuracy and efficiency of co-reference resolution models, the encoder LSTM usually processes input sentences separately as a batch (Lee et al., 2017).",1 Introduction,[0],[0]
"The disadvantage of this method is that the models do not consider the dependency among words from different sentences, which plays a significant role in word representation learning and co-reference predicting.",1 Introduction,[0],[0]
"For example, pronouns are often linked to entities mentioned in other sentences, while their initial word vectors lack dependency information.",1 Introduction,[0],[0]
"As a result, a word representation model cannot learn an informative embedding of a pronoun without considering cross-sentence dependency in this case.
",1 Introduction,[0],[0]
It is also problematic if we encode the input document considering cross-sentence dependency and treat the entire document as one sentence.,1 Introduction,[0],[0]
An input article or conversation can be too long for a single LSTM cell to memorize.,1 Introduction,[0],[0]
"If the LSTM updates itself for too many steps, gradients will vanish or explode (Pascanu et al., 2013), and the coreference resolution model will be very difficult to optimize.",1 Introduction,[0],[0]
"Regarding the entire input corpus as one sequence instead of a batch also significantly increases the time complexity of the model.
",1 Introduction,[0],[0]
"To solve the problem that traditional LSTM encoders, which treat the input sentences as a batch, lack an ability to capture cross-sentence dependency, and to avoid the time complexity and difficulties of training the model concatenating all input sentences, we propose a cross-sentence encoder for end-to-end co-reference (E2E-CR).",1 Introduction,[0],[0]
"Borrowing the idea of an external memory module from Sukhbaatar et al. (2015), an external memory block containing syntactic and semantic information from context sentences is added to the standard LSTM model.",1 Introduction,[0],[0]
"With this context memory block, the proposed model is able to encode
input sentences as a batch, and also calculate the representations of input words by taking both target sentences and context sentences into consideration.",1 Introduction,[0],[0]
Experiments showed that this approach improved the performance of co-reference resolution models.,1 Introduction,[0],[0]
"A popular method of co-reference resolution is mention ranking (Durrett and Klein, 2013).",2.1 Co-reference Resolution,[0],[0]
"Reading each mention, the model calculates coreference scores for all antecedent mentions, and picks the mention with the highest positive score to be its co-reference.",2.1 Co-reference Resolution,[0],[0]
Many recent works are based on this approach.,2.1 Co-reference Resolution,[0],[0]
Durrett and Klein (2013) designed a set of feature templates to improve the mention-ranking model.,2.1 Co-reference Resolution,[0],[0]
Peng et al. (2015) proposed a mention-ranking model by jointly learning mention heads and co-references.,2.1 Co-reference Resolution,[0],[0]
Clark and Manning (2016a) proposed a reinforcement learning framework for the mention ranking approach.,2.1 Co-reference Resolution,[0],[0]
"Based on similar ideas but without using parsing features, the authors of Lee et al. (2017) proposed the current state-of-the-art model which uses neural networks to embed mentions and calculate mention and antecedent scores.",2.1 Co-reference Resolution,[0],[0]
"Lee et al. (2018) applied ELMo embeddings (Peters et al., 2018) to improve within-sentence dependency modeling and word representation learning.",2.1 Co-reference Resolution,[0],[0]
Wiseman et al. (2016) and Clark and Manning (2016b) proposed models using global entity-level features.,2.1 Co-reference Resolution,[0],[0]
"Distributed word embeddings has been used as the basic unit of language representation for over a decade (Bengio et al., 2003).",2.2 Language Representation Learning,[0],[0]
"Pre-trained word embeddings, for example GloVe (Pennington et al., 2014) and Skip-Gram (Mikolov et al., 2013) are widely used as the input of natural language processing models.
",2.2 Language Representation Learning,[0],[0]
"Long short-term memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) are widely used for sentence modeling.",2.2 Language Representation Learning,[0],[0]
"A single-layer LSTM network was applied in the previous state-of-theart co-reference model (Lee et al., 2017) to generate word and mention representations.",2.2 Language Representation Learning,[0],[0]
"To capture dependency of longer distances, Campos et al. (2017) proposed a recurrent model that outputs hidden states by skipping input tokens.
",2.2 Language Representation Learning,[0],[0]
"Recently, memory networks (Sukhbaatar et al.,
2015) have been applied in language modeling (Cheng et al., 2016; Tran et al., 2016).",2.2 Language Representation Learning,[0],[0]
"Applying an attention mechanism on memory cells, memory networks allow the model to focus on significant words or segments for classification and generation tasks.",2.2 Language Representation Learning,[0],[0]
"Previous works have shown that applying memory blocks in LSTMs also improves longdistance dependency extraction (Yogatama et al., 2018).",2.2 Language Representation Learning,[0],[0]
"To improve the word representation learning model for better co-reference resolution performance, we propose two word representation models that learn cross-sentence dependency.",3 Learning Cross-Sentence dependency,[0],[0]
"Instead of treating the entire input document as separate sentences and encode the sentences as a batch with an LSTM, the most direct way to consider cross-sentence dependency is to initialize LSTM states with the encodings of adjacent sentences.",3.1 Linear Sentence Linking,[0],[0]
"We name this method linear sentence linking (LSL).
",3.1 Linear Sentence Linking,[0],[0]
"In LSL, we encode input sentences with a 2- layer bidirectional LSTM.",3.1 Linear Sentence Linking,[0],[0]
"Give input sentences [s1, s2 . . .",3.1 Linear Sentence Linking,[0],[0]
"sn], the outputs of the first layer are [[−→s 1;←−s 1],",3.1 Linear Sentence Linking,[0],[0]
"[−→s 2;←−s 2], . . .",3.1 Linear Sentence Linking,[0],[0]
[−→s n;←−s n]].,3.1 Linear Sentence Linking,[0],[0]
"In the second LSTM layer, the initial state of the forward LSTM of si is initialized as
−→ S i =",3.1 Linear Sentence Linking,[0],[0]
"[ −→c 20; [−→s i−1;←−s i−1]]
while the backward state is initialized as
←−",3.1 Linear Sentence Linking,[0],[0]
"S i = [ ←−c 20; [−→s i−1;←−s i−1]]
where ci0 stands for the initial cell of the ith layer, and x stands for the final output of the LSTMs in first layer.",3.1 Linear Sentence Linking,[0],[0]
We then concatenate the outputs of the forward and backward LSTMs in the second layer as the word representations for coreference prediction.,3.1 Linear Sentence Linking,[0],[0]
It is difficult for LSTMs to embed enough information about a long sentence into a lowdimensional distributed vector.,3.2 Attentional Sentence Linking,[0],[0]
"To collect richer knowledge from neighbor sentences, we propose a long short-term recurrent memory module and an attention mechanism to improve sentence linking.
",3.2 Attentional Sentence Linking,[0],[0]
"To describe the architecture of the proposed model, we focus on adjacent input sentences si−1
and si.",3.2 Attentional Sentence Linking,[0],[0]
"We present the input embeddings of the j-th word in the i-th sentence with xi,j .",3.2 Attentional Sentence Linking,[0],[0]
"To solve the traditional recurrent neural networks, Hochreiter and Schmidhuber (1997) proposed the LSTM architecture.",3.2.1 Long Short-Term Memory RNNs,[0],[0]
"The detail of recurrent state updating in LSTMs ht = flstm(xt, ht−1, ct−1) is shown in following equations.
",3.2.1 Long Short-Term Memory RNNs,[0],[0]
it = σ(Wxixt +Whiht−1 + bi) ft = σ(Wxfxt,3.2.1 Long Short-Term Memory RNNs,[0],[0]
+Whfht−1 + bf ),3.2.1 Long Short-Term Memory RNNs,[0],[0]
ct = ft ct−1 + it tanh(Wxcxt +Whcht−1 + bc) ot = σ(Wxoxt +Whoht−1 + bo),3.2.1 Long Short-Term Memory RNNs,[0],[0]
"ht = ot tanh(ct)
where xt is the input embedding and ht is the output representation of the t-th word.",3.2.1 Long Short-Term Memory RNNs,[0],[0]
We design an LSTM module with cross-sentence attention for capturing cross-sentence dependency.,3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
We name this method attentional sentence linking (ASL).,3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
"Considering input word xi,t in the ith sentence and all words from the previous sentence Xi−1 =",3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
"[xi−1,1, xi−1,2, . . .",3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
", xi−1,m], we regard the matrix Xi−1 as an external memory module and calculate an attention on its cells, where each cell contains a word embedding.
",3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
"αj = ecj∑ k e ck (1)
ck = fc([xi,t;ht−1;xi−1,k] T ) (2)
",3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
"With the attention distribution α, we can get a vector summarizing related information from si−1,
vi−1 = ∑ j αj · xi−1,j (3)
",3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
"The model decides if it needs to pay more attention on the current input or cross-sentence information with a context gate.
",3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
"gt = σ(fg([xi,t;ht−1; vi−1] T )) (4)
x̂i,t = gt · xi,t + (1− gt) · vi−1 (5)
σ",3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
(·) stands for the Sigmoid function.,3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
"The word representation of the target word is calculated as
hi,t = flstm(x̂i,t, hi,t−1, ci,t−1) (6)
where flstm stands for standard LSTM update described in section 3.2.1.",3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
"In this work, we apply the mention-ranking endto-end co-reference resolution (E2E-CR) model proposed by Lee et al. (2017) for co-reference prediction.",3.3 Co-reference Prediction,[0],[0]
The word representations applied in E2ECR model is formed by concatenating pre-trained word embeddings and the outputs of LSTMs.,3.3 Co-reference Prediction,[0],[0]
"In our work, we represent words by concatenating pre-trained word embeddings and the outputs of LSL- and ASL-LSTMs.",3.3 Co-reference Prediction,[0],[0]
"We train and evaluate our model on the English corpus of the CoNLL-2012 shared task (Pradhan et al., 2012).",4 Experiments,[0],[0]
"We implement our model based on the published implementation of the baseline E2ECR model (Lee et al., 2017) 1.",4 Experiments,[0],[0]
Our implementation is also available online for reproducing the results reported in this paper 2.,4 Experiments,[0],[0]
"In this section, we first describe our hyperparameter setup, and then show the experimental results of previous work and our proposed models.",4 Experiments,[0],[0]
"In practice, the LSTM modules applied in our model have 200 output units.",4.1 Model and Hyperparameter Setup,[0],[0]
"In ASL, we calculate cross-sentence dependency using a multilayer perceptron with one hidden layer consisting of 150 hidden units.",4.1 Model and Hyperparameter Setup,[0],[0]
The initial learning rate is set as 0.001 and decays 0.001% every 100 steps.,4.1 Model and Hyperparameter Setup,[0],[0]
"The model is optimized with the Adam algorithm (Kingma and Ba, 2014).",4.1 Model and Hyperparameter Setup,[0],[0]
We randomly select up to 40 continuous sentences for training if the input is too long.,4.1 Model and Hyperparameter Setup,[0],[0]
"In co-reference prediction, we select 250 candidate antecedents as our baseline model.",4.1 Model and Hyperparameter Setup,[0],[0]
We evaluate our model on the test set of the CoNLL-2012 shared task.,4.2 Experiment Results and Discussion,[0],[0]
The performance of previous work and our model are shown in Table 1.,4.2 Experiment Results and Discussion,[0],[0]
"We mainly focus on the average F1 score of MUC, B3, and CEAF metrics.",4.2 Experiment Results and Discussion,[0],[0]
"Comparing with the baseline model that achieved 67.2% F1 score, the ASL model improved the performance by 0.6% and achieved 67.8% average F1.",4.2 Experiment Results and Discussion,[0],[0]
"Experiments
1https://github.com/kentonl/e2e-coref 2https://github.com/luohongyin/
coatt-coref
show that the models that consider cross-sentence dependency significantly outperform the baseline model, which encodes each sentence from the input document separately.
",4.2 Experiment Results and Discussion,[0],[0]
"Experiments also indicated that the ASL model has better performance than the LSL model, since it summarizes extracts context information with an attention mechanism instead of simply viewing sentence-level embeddings.",4.2 Experiment Results and Discussion,[0],[0]
"This gives the model a better ability to model cross-sentence dependency.
",4.2 Experiment Results and Discussion,[0],[0]
Examples for comparing the performance of the ASL model and the baseline are shown in Table 2.,4.2 Experiment Results and Discussion,[0],[0]
Each example contains two continuous sentences with co-references distritubed in different sentences.,4.2 Experiment Results and Discussion,[0],[0]
Underlined spans in bold are target mentions and annotated co-references.,4.2 Experiment Results and Discussion,[0],[0]
"Spans in
green are ASL predictions, and spans in red are baseline predictions.",4.2 Experiment Results and Discussion,[0],[0]
"A prediction on “-” means that no mention is predicted as a co-reference.
",4.2 Experiment Results and Discussion,[0],[0]
"Table 2 shows that the baseline model, which does not consider cross-sentence dependency, has difficulty in learning the semantics of pronouns whose co-references are not in the same sentence.",4.2 Experiment Results and Discussion,[0],[0]
The pretrained embeddings of pronouns are not informative enough.,4.2 Experiment Results and Discussion,[0],[0]
"In the first example, “it” is not semantically similar with “SMS” in GloVe without any context, and in this case, “it” and “SMS” are in different sentences.",4.2 Experiment Results and Discussion,[0],[0]
"As a result, if reading this two sentences separately, it is hard for the encoder to represent “it” with the semantics of “SMS”.",4.2 Experiment Results and Discussion,[0],[0]
"This difficulty makes the co-reference resolution model either prediction a wrong antecedent mention, or cannot find any co-reference.
",4.2 Experiment Results and Discussion,[0],[0]
"However, with ASL, the model learns the semantics of pronouns with an attention to words in other sentences.",4.2 Experiment Results and Discussion,[0],[0]
"With the proposed context gate, ASL takes knowledge from context sentences if local inputs are not informative enough.",4.2 Experiment Results and Discussion,[0],[0]
"Based on word represents enhanced with cross-sentence dependency, the co-reference scoring model can make better predictions.",4.2 Experiment Results and Discussion,[0],[0]
We proposed linear and attentional sentence linking models for learning word representations that captures cross-sentence dependency.,5 Conclusion and Future Work,[0],[0]
"Experiments showed that the embeddings learned by proposed models successfully improved the performance of the state-of-the-art co-reference resolution model, indicating that cross-sentence dependency plays an important role in semantic learning in articles and conversations consists of multiple sentences.",5 Conclusion and Future Work,[0],[0]
"It worth exploring if our model can improve the performance of other natural language processing
applications whose inputs contain multiple sentences, for example, reading comprehension, dialog generation, and sentiment analysis.",5 Conclusion and Future Work,[0],[0]
"In this work, we present a word embedding model that learns cross-sentence dependency for improving end-to-end co-reference resolution (E2E-CR).",abstractText,[0],[0]
"While the traditional E2ECR model generates word representations by running long short-term memory (LSTM) recurrent neural networks on each sentence of an input article or conversation separately, we propose linear sentence linking and attentional sentence linking models to learn crosssentence dependency.",abstractText,[0],[0]
Both sentence linking strategies enable the LSTMs to make use of valuable information from context sentences while calculating the representation of the current input word.,abstractText,[0],[0]
"With this approach, the LSTMs learn word embeddings considering knowledge not only from the current sentence but also from the entire input document.",abstractText,[0],[0]
"Experiments show that learning cross-sentence dependency enriches information contained by the word representations, and improves the performance of the co-reference resolution model compared with our baseline.",abstractText,[0],[0]
Learning Word Representations with Cross-Sentence Dependency for End-to-End Co-reference Resolution,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 506–517 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1047",text,[0],[0]
"Automatically discovering words and other elements of linguistic structure from continuous speech has been a longstanding goal in computational linguists, cognitive science, and other speech processing fields.",1.1 Problem Statement and Motivation,[0],[0]
"Practically all humans acquire language at a very early age, but this task has proven to be an incredibly difficult problem for computers.",1.1 Problem Statement and Motivation,[0],[0]
"While conventional automatic speech recognition (ASR) systems have a long history and have recently made great strides thanks to the revival of deep neural networks (DNNs), their reliance on highly supervised training paradigms has essentially restricted their application to the major languages of the world, accounting for a small fraction of the more than 7,000 human languages spoken worldwide (Lewis et al., 2016).",1.1 Problem Statement and Motivation,[0],[0]
"The main
reason for this limitation is the fact that these supervised approaches require enormous amounts of very expensive human transcripts.",1.1 Problem Statement and Motivation,[0],[0]
"Moreover, the use of the written word is a convenient but limiting convention, since there are many oral languages which do not even employ a writing system.",1.1 Problem Statement and Motivation,[0],[0]
"In constrast, infants learn to communicate verbally before they are capable of reading and writing - so there is no inherent reason why spoken language systems need to be inseparably tied to text.
",1.1 Problem Statement and Motivation,[0],[0]
The key contribution of this paper has two facets.,1.1 Problem Statement and Motivation,[0],[0]
"First, we introduce a methodology capable of not only discovering word-like units from continuous speech at the waveform level with no additional text transcriptions or conventional speech recognition apparatus.",1.1 Problem Statement and Motivation,[0.9567547742913286],"['This paves the way for creating a speech-to-speech translation model not only with absolutely zero need for any sort of text transcriptions, but also with zero need for directly parallel linguistic data or manual human translations.']"
"Instead, we jointly learn the semantics of those units via visual associations.",1.1 Problem Statement and Motivation,[0],[0]
"Although we evaluate our algorithm on an English corpus, it could conceivably run on any language without requiring any text or associated ASR capability.",1.1 Problem Statement and Motivation,[0],[0]
"Second, from a computational perspective, our method of speech pattern discovery runs in linear time.",1.1 Problem Statement and Motivation,[0],[0]
"Previous work has presented algorithms for performing acoustic pattern discovery in continuous speech (Park and Glass, 2008; Jansen et al., 2010; Jansen and Van Durme, 2011) without the use of transcriptions or another modality, but those algorithms are limited in their ability to scale by their inherent O(n2) complexity, since they do an exhaustive comparison of the data against itself.",1.1 Problem Statement and Motivation,[0],[0]
Our method leverages correlated information from a second modality - the visual domain - to guide the discovery of words and phrases.,1.1 Problem Statement and Motivation,[1.0],['Our method leverages correlated information from a second modality - the visual domain - to guide the discovery of words and phrases.']
"This enables our method to run in O(n) time, and we demonstrate it scalability by discovering acoustic patterns in over 522 hours of audio.",1.1 Problem Statement and Motivation,[1.0],"['This enables our method to run in O(n) time, and we demonstrate it scalability by discovering acoustic patterns in over 522 hours of audio.']"
"A sub-field within speech processing that has garnered much attention recently is unsupervised
506
speech pattern discovery.",1.2 Previous Work,[1.0000000278451047],['A sub-field within speech processing that has garnered much attention recently is unsupervised 506 speech pattern discovery.']
"Segmental Dynamic Time Warping (S-DTW) was introduced by Park and Glass (2008), which discovers repetitions of the same words and phrases in a collection of untranscribed acoustic data.",1.2 Previous Work,[0],[0]
"Many subsequent efforts extended these ideas (Jansen et al., 2010; Jansen and Van Durme, 2011; Dredze et al., 2010; Harwath et al., 2012; Zhang and Glass, 2009).",1.2 Previous Work,[0],[0]
"Alternative approaches based on Bayesian nonparametric modeling (Lee and Glass, 2012; Ondel et al., 2016) employed a generative model to cluster acoustic segments into phoneme-like categories, and related works aimed to segment and cluster either reference or learned phonemelike tokens into higher-level units (Johnson, 2008; Goldwater et al., 2009; Lee et al., 2015).
",1.2 Previous Work,[0],[0]
"While supervised object detection is a standard problem in the vision community, several recent works have tackled the problem of weaklysupervised or unsupervised object localization (Bergamo et al., 2014; Cho et al., 2015; Zhou et al., 2015; Cinbis et al., 2016).",1.2 Previous Work,[0],[0]
"Although the focus of this work is discovering acoustic patterns, in the process we jointly associate the acoustic patterns with clusters of image crops, which we demonstrate capture visual patterns as well.
",1.2 Previous Work,[0],[0]
The computer vision and NLP communities have begun to leverage deep learning to create multimodal models of images and text.,1.2 Previous Work,[0],[0]
"Many works have focused on generating annotations or text captions for images (Socher and Li, 2010; Frome et al., 2013; Socher et al., 2014; Karpathy et al., 2014; Karpathy and Li, 2015; Vinyals et al., 2015; Fang et al., 2015; Johnson et al., 2016).",1.2 Previous Work,[0],[0]
"One interesting intersection between word induction from phoneme strings and multimodal modeling of images and text is that of Gelderloos and Chrupaa (2016), who uses images to segment words within captions at the phoneme string level.",1.2 Previous Work,[0],[0]
"Other work has taken these ideas beyond text, and attempted to relate images to spoken audio captions directly at the waveform level (Roy, 2003; Harwath and Glass, 2015; Harwath et al., 2016).",1.2 Previous Work,[0],[0]
"The work of (Harwath et al., 2016) is the most similar to ours, in which the authors learned embeddings at the entire image and entire spoken caption level and then used the embeddings to perform bidirectional retrieval.",1.2 Previous Work,[0],[0]
"In this work, we go further by automatically segmenting and clustering the spoken captions into individual word-like units, as well as the images into object-like categories.",1.2 Previous Work,[1.0],"['In this work, we go further by automatically segmenting and clustering the spoken captions into individual word-like units, as well as the images into object-like categories.']"
"We employ a corpus of over 200,000 spoken captions for images taken from the Places205 dataset (Zhou et al., 2014), corresponding to over 522 hours of speech data.",2 Experimental Data,[1.0],"['We employ a corpus of over 200,000 spoken captions for images taken from the Places205 dataset (Zhou et al., 2014), corresponding to over 522 hours of speech data.']"
"The captions were collected using Amazon’s Mechanical Turk service, in which workers were shown images and asked to describe them verbally in a free-form manner.",2 Experimental Data,[1.0],"['The captions were collected using Amazon’s Mechanical Turk service, in which workers were shown images and asked to describe them verbally in a free-form manner.']"
"The data collection scheme is described in detail in Harwath et al. (2016), but the experiments in this paper leverage nearly twice the amount of data.",2 Experimental Data,[0],[0]
"For training our multimodal neural network as well as the pattern discovery experiments, we use a subset of 214,585 image/caption pairs, and we hold out a set of 1,000 pairs for evaluating the multimodal network’s retrieval ability.",2 Experimental Data,[0],[0]
"Because we lack ground truth text transcripts for the data, we used Google’s Speech Recognition public API to generate proxy transcripts which we use when analyzing our system.",2 Experimental Data,[0],[0]
"Note that the ASR was only used for analysis of the results, and was not involved in any of the learning.",2 Experimental Data,[0],[0]
"We first train a deep multimodal embedding network similar in spirit to the one described in Harwath et al. (2016), but with a more sophisticated architecture.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"The model is trained to map entire image frames and entire spoken captions into a shared embedding space; however, as we will show, the trained network can then be used to localize patterns corresponding to words and phrases within the spectrogram, as well as visual objects within the image by applying it to small sub-regions of the image and spectrogram.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"The model is comprised of two branches, one which takes as input images, and the other which takes as input spectrograms.",3 Audio-Visual Embedding Neural Networks,[1.0],"['The model is comprised of two branches, one which takes as input images, and the other which takes as input spectrograms.']"
"The image network is formed by taking the off-the-shelf VGG 16 layer network (Simonyan and Zisserman, 2014) and replacing the softmax classification layer with a linear transform which maps the 4096-dimensional activations of the second fully connected layer into our 1024-dimensional multimodal embedding space.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"In our experiments, the weights of this projection layer are trained, but the layers taken from the VGG network below it are kept fixed.",3 Audio-Visual Embedding Neural Networks,[0],[0]
The second branch of our network analyzes speech spectrograms as if they were black and white images.,3 Audio-Visual Embedding Neural Networks,[1.0],['The second branch of our network analyzes speech spectrograms as if they were black and white images.']
"Our spectrograms are computed using 40 log Mel
filterbanks with a 25ms Hamming window and a 10ms shift.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"The input to this branch always has 1 color channel and is always 40 pixels high (corresponding to the 40 Mel filterbanks), but the width of the spectrogram varies depending upon the duration of the spoken caption, with each pixel corresponding to approximately 10 milliseconds worth of audio.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"The architecture we use is entirely convolutional and shown below, where C denotes the number of convolutional channels, W is filter width, H is filter height, and S is pooling stride.
",3 Audio-Visual Embedding Neural Networks,[0],[0]
1.,3 Audio-Visual Embedding Neural Networks,[0],[0]
"Convolution: C=128, W=1, H=40, ReLU 2.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Convolution: C=256, W=11, H=1, ReLU 3.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Maxpool: W=3, H=1, S=2 4.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Convolution: C=512, W=17, H=1, ReLU 5.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Maxpool: W=3, H=1, S=2 6.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Convolution: C=512, W=17, H=1, ReLU 7.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Maxpool: W=3, H=1, S=2 8.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Convolution: C=1024, W=17, H=1, ReLU 9.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Meanpool over entire caption
10.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"L2 normalization In practice during training, we restrict the caption spectrograms to all be 1024 frames wide (i.e., 10sec of speech) by applying truncation or zero padding.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Additionally, both the images and spectrograms are mean normalized before training.",3 Audio-Visual Embedding Neural Networks,[1.0],"['Additionally, both the images and spectrograms are mean normalized before training.']"
"The overall multimodal network is formed by tying together the image and audio branches with a layer which takes both of their output vectors and computes an inner product between them, representing the similarity score between a given image/caption pair.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"We train the network to assign high scores to matching image/caption pairs, and lower scores to mismatched pairs.
",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Within a minibatch of B image/caption pairs, let Spj , j = 1, . . .",3 Audio-Visual Embedding Neural Networks,[0],[0]
", B denote the similarity score of the jth image/caption pair as output by the neural network.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Next, for each pair we randomly sample one impostor caption and one impostor image from the same minibatch.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Let Sij denote the similarity score between the jth caption and its impostor image, and Scj be the similarity score between the jth image and its impostor caption.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"The total loss for the entire minibatch is then computed as
L(θ) = B∑
j=1
[max(0, Scj − Spj + 1)
+ max(0,",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Sij − Spj + 1)] (1)
We train the neural network with 50 epochs of stochastic gradient descent using a batch size B =
128, a momentum of 0.9, and a learning rate of 1e5 which is set to geometrically decay by a factor between 2 and 5 every 5 to 10 epochs.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Although we have trained our multimodal network to compute embeddings at the granularity of entire images and entire caption spectrograms, we can easily apply it in a more localized fashion.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"In the case of images, we can simply take any arbitrary crop of an original image and resize it to 224x224 pixels.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"The audio network is even more trivial to apply locally, because it is entirely convolutional and the final mean pooling layer ensures that the output will be a 1024-dim vector no matter the extent of the input.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"The bigger question is where to locally apply the networks in order to discover meaningful acoustic and visual patterns.
",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"Given an image and its corresponding spoken audio caption, we use the term grounding to refer to extracting meaningful segments from the caption and associating them with an appropriate subregion of the image.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"For example, if an image depicted a person eating ice cream and its caption contained the spoken words “A person is enjoying some ice cream,” an ideal set of groundings would entail the acoustic segment containing the word “person” linked to a bounding box around the person, and the segment containing the word “ice cream” linked to a box around the ice cream.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
We use a constrained brute force ranking scheme to evaluate all possible groundings (with a restricted granularity) between an image and its caption.,4 Finding and Clustering Audio-Visual Caption Groundings,[1.0],['We use a constrained brute force ranking scheme to evaluate all possible groundings (with a restricted granularity) between an image and its caption.']
"Specifically, we divide the image into a grid, and extract all of the image crops whose boundaries sit on the grid lines.",4 Finding and Clustering Audio-Visual Caption Groundings,[1.0],"['Specifically, we divide the image into a grid, and extract all of the image crops whose boundaries sit on the grid lines.']"
"Because we are mainly interested in extracting regions of interest and not high precision object detection boxes, to keep the number of proposal regions under control we impose several restrictions.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"First, we use a 10x10 grid on each image regardless of its original size.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"Second, we define minimum and maximum aspect ratios as 2:3 and 3:2 so as not to introduce too much distortion and also to reduce the number of proposal boxes.",4 Finding and Clustering Audio-Visual Caption Groundings,[1.0],"['Second, we define minimum and maximum aspect ratios as 2:3 and 3:2 so as not to introduce too much distortion and also to reduce the number of proposal boxes.']"
"Third, we define a minimum bounding width as 30% of the original image width, and similarly a minimum height as 30% of the original image height.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"In practice, this results in a few thousand proposal regions per image.
",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"To extract proposal segments from the audio
caption spectrogram, we similarly define a 1-dim grid along the time axis, and consider all possible start/end points at 10 frame (pixel) intervals.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"We impose minimum and maximum segment length constraints at 50 and 100 frames (pixels), implying that our discovered acoustic patterns are restricted to fall between 0.5 and 1 second in duration.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"The number of proposal segments will vary depending on the caption length, and typically number in the several thousands.",4 Finding and Clustering Audio-Visual Caption Groundings,[1.0],"['The number of proposal segments will vary depending on the caption length, and typically number in the several thousands.']"
"Note that when learning groundings we consider the entire audio sequence, and do not incorporate the 10sec duration constraint imposed during training.
",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"Once we have extracted a set of proposed visual bounding boxes and acoustic segments for a given image/caption pair, we use our multimodal network to compute a similarity score between each unique image crop/acoustic segment pair.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"Each triplet of an image crop, acoustic segment, and similarity score constitutes a proposed grounding.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"A naive approach would be to simply keep the top N groundings from this list, but in practice we ran into two problems with this strategy.",4 Finding and Clustering Audio-Visual Caption Groundings,[1.0],"['A naive approach would be to simply keep the top N groundings from this list, but in practice we ran into two problems with this strategy.']"
"First, many proposed acoustic segments capture mostly silence due to pauses present in natural speech.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"We solve this issue by using a simple voice activity detector (VAD) which was trained on the TIMIT corpus(Garofolo et al., 1993).",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"If the VAD estimates that 40% or more of any proposed acoustic segment is silence, we discard that entire grounding.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
The second problem we ran into is the fact that the top of the sorted grounding list is dominated by highly overlapping acoustic segments.,4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"This makes sense, because highly informative content words will show up in many different groundings with slightly perturbed start or end times.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"To alleviate this issue, when evaluating a grounding from the top of the proposal list we compare the interval intersection over union (IOU) of its acoustic segment against all acoustic segments already accepted for further consideration.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"If the IOU exceeds a threshold of 0.1, we discard the new grounding and continue moving down the list.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"We stop accumulating groundings once the scores fall to below 50% of the top score in the “keep” list, or when 10 groundings have been added to the “keep” list.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"Figure 1 displays a pictorial example of our grounding procedure.
",4 Finding and Clustering Audio-Visual Caption Groundings,[1.0000000241594933],['Figure 1 displays a pictorial example of our grounding procedure.']
"Once we have completed the grounding procedure, we are left with a small set of regions of interest in each image and caption spectrogram.
",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
We use the respective branches of our multimodal network to compute embedding vectors for each grounding’s image crop and acoustic segment.,4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
We then employ k-means clustering separately on the collection of image embedding vectors as well as the collection of acoustic embedding vectors.,4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"The last step is to establish an affinity score between each image cluster I and each acoustic cluster A; we do so using the equation
Affinity(I,A) = ∑
i∈I
∑ a∈A",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"i>a · Pair(i,a) (2)
where i is an image crop embedding vector, a is an acoustic segment embedding vector, and Pair(i,a) is equal to 1",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"when i and a belong to the same grounding pair, and 0 otherwise.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"After clustering, we are left with a set of acoustic pattern clusters, a set of visual pattern clusters, and a set of linkages describing which acoustic clusters are associated with which image clusters.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"In the next section, we investigate these clusters in more detail.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"We trained our multimodal network on a set of 214,585 image/caption pairs, and vetted it with an image search (given caption, find image) and annotation (given image, find caption) task similar to the one used in Harwath et al. (2016); Karpathy et al. (2014); Karpathy and Li (2015).",5 Experiments and Analysis,[0],[0]
"The image annotation and search recall scores on a 1,000 image/caption pair held-out test set are shown in Table 1.",5 Experiments and Analysis,[0],[0]
"Also shown in this table are the scores
achieved by a model which uses the ASR text transcriptions for each caption instead of the speech audio.",5 Experiments and Analysis,[0],[0]
"The text captions were truncated/padded to 20 words, and the audio branch of the network was replaced with a branch with the following architecture:
1.",5 Experiments and Analysis,[0],[0]
"Word embedding layer of dimension 200
2.",5 Experiments and Analysis,[0],[0]
"Temporal Convolution: C=512, W=3, ReLU 3.",5 Experiments and Analysis,[0],[0]
"Temporal Convolution: C=1024, W=3 4.",5 Experiments and Analysis,[0],[0]
Meanpool over entire caption 5.,5 Experiments and Analysis,[0],[0]
"L2 normalization
One would expect that access to ASR hypotheses should improve the recall scores, but the performance gap is not enormous.",5 Experiments and Analysis,[0],[0]
"Access to the ASR hypotheses provides a relative improvement of approximately 21.8% for image search R@10 and 12.5% for annotation R@10 compared to using no transcriptions or ASR whatsoever.
",5 Experiments and Analysis,[0],[0]
"We performed the grounding and pattern clustering steps on the entire training dataset, which resulted in a total of 1,161,305 unique grounding pairs.",5 Experiments and Analysis,[1.0],"['We performed the grounding and pattern clustering steps on the entire training dataset, which resulted in a total of 1,161,305 unique grounding pairs.']"
"For evaluation, we wish to assign a label to each cluster and cluster member, but this is not completely straightforward since each acoustic segment may capture part of a word, a whole word, multiple words, etc.",5 Experiments and Analysis,[0],[0]
"Our strategy is to forcealign the Google recognition hypothesis text to the audio, and then assign a label string to each acoustic segment based upon which words it overlaps in time.",5 Experiments and Analysis,[0],[0]
"The alignments are created with the help of a Kaldi (Povey et al., 2011) speech recognizer
Table 3:",5 Experiments and Analysis,[0],[0]
Top 50 clusters with k = 500 sorted by increasing variance.,5 Experiments and Analysis,[0],[0]
"Legend: |Cc| is acoustic cluster size, |Ci| is associated image cluster size, Pur. is acoustic cluster purity, σ2 is acoustic cluster variance, and Cov. is acoustic cluster coverage.",5 Experiments and Analysis,[0],[0]
"A dash (-) indicates a cluster whose majority label is silence.
",5 Experiments and Analysis,[0.9999999840840383],['A dash (-) indicates a cluster whose majority label is silence.']
"Trans |Cc| |Ci| Pur. σ2 Cov. Trans |Cc| |Ci| Pur. σ2 Cov. - 1059 3480 0.70 0.26 - snow 4331 3480 0.85 0.26 0.45
desert 1936 2896 0.82 0.27 0.67 kitchen 3200 2990 0.88 0.28 0.76 restaurant 1921 2536 0.89 0.29 0.71 mountain 4571 2768 0.86 0.30 0.38
black 4369 2387 0.64 0.30 0.17 skyscraper 843 3205 0.84 0.30 0.84 bridge 1654 2025 0.84 0.30 0.25 tree 5303 3758 0.90 0.30 0.16 castle 1298 2887 0.72 0.31 0.74 bridge 2779 2025 0.81 0.32 0.41
- 2349 2165 0.31 0.33 - ocean 2913 3505 0.87 0.33 0.71 table 3765 2165 0.94 0.33 0.23 windmill 1458 3752 0.71 0.33 0.76 window 1890 2795 0.85 0.34 0.21 river 2643 3204 0.76 0.35 0.62 water 5868 3204 0.90 0.35 0.27 beach 1897 2964 0.79 0.35 0.64 flower 3906 2587 0.92 0.35 0.67 wall 3158 3636 0.84 0.35 0.23
sky 4306 6055 0.76 0.36 0.34 street 2602 2385 0.86 0.36 0.49 golf course 1678 3864 0.44 0.36 0.63 field 3896 3261 0.74 0.36 0.37
tree 4098 3758 0.89 0.36 0.13 lighthouse 1254 1518 0.61 0.36 0.83 forest 1752 3431 0.80 0.37 0.56 church 2503 3140 0.86 0.37 0.72 people 3624 2275 0.91 0.37 0.14 baseball 2777 1929 0.66 0.37 0.86 field 2603 3922 0.74 0.37 0.25 car 3442 2118 0.79 0.38 0.27
people 4074 2286 0.92 0.38 0.17 shower 1271 2206 0.74 0.38 0.82 people walking 918 2224 0.63 0.38 0.25 wooden 3095 2723 0.63 0.38 0.28
mountain 3464 3239 0.88 0.38 0.29 tree 3676 2393 0.89 0.39 0.11 - 1976 3158 0.28 0.39 - snow 2521 3480 0.79 0.39 0.24
water 3102 2948 0.90 0.39 0.14 rock 2897 2967 0.76 0.39 0.26 - 2918 3459 0.08 0.39 - night 3027 3185 0.44 0.39 0.59
station 2063 2083 0.85 0.39 0.62 chair 2589 2288 0.89 0.39 0.22 building 6791 3450 0.89 0.40 0.21 city 2951 3190 0.67 0.40 0.50
Figure 2:",5 Experiments and Analysis,[0.9847201889108492],['- 1059 3480 0.70 0.26 - snow 4331 3480 0.85 0.26 0.45 desert 1936 2896 0.82 0.27 0.67 kitchen 3200 2990 0.88 0.28 0.76 restaurant 1921 2536 0.89 0.29 0.71 mountain 4571 2768 0.86 0.30 0.38 black 4369 2387 0.64 0.30 0.17 skyscraper 843 3205 0.84 0.30 0.84 bridge 1654 2025 0.84 0.30 0.25 tree 5303 3758 0.90 0.30 0.16 castle 1298 2887 0.72 0.31 0.74 bridge 2779 2025 0.81 0.32 0.41 - 2349 2165 0.31 0.33 - ocean 2913 3505 0.87 0.33 0.71 table 3765 2165 0.94 0.33 0.23 windmill 1458 3752 0.71 0.33 0.76 window 1890 2795 0.85 0.34 0.21 river 2643 3204 0.76 0.35 0.62 water 5868 3204 0.90 0.35 0.27 beach 1897 2964 0.79 0.35 0.64 flower 3906 2587 0.92 0.35 0.67 wall 3158 3636 0.84 0.35 0.23 sky 4306 6055 0.76 0.36 0.34 street 2602 2385 0.86 0.36 0.49 golf course 1678 3864 0.44 0.36 0.63 field 3896 3261 0.74 0.36 0.37 tree 4098 3758 0.89 0.36 0.13 lighthouse 1254 1518 0.61 0.36 0.83 forest 1752 3431 0.80 0.37 0.56 church 2503 3140 0.86 0.37 0.72 people 3624 2275 0.91 0.37 0.14 baseball 2777 1929 0.66 0.37 0.86 field 2603 3922 0.74 0.37 0.25 car 3442 2118 0.79 0.38 0.27 people 4074 2286 0.92 0.38 0.17 shower 1271 2206 0.74 0.38 0.82 people walking 918 2224 0.63 0.38 0.25 wooden 3095 2723 0.63 0.38 0.28 mountain 3464 3239 0.88 0.38 0.29 tree 3676 2393 0.89 0.39 0.11 - 1976 3158 0.28 0.39 - snow 2521 3480 0.79 0.39 0.24 water 3102 2948 0.90 0.39 0.14 rock 2897 2967 0.76 0.39 0.26 - 2918 3459 0.08 0.39 - night 3027 3185 0.44 0.39 0.59 station 2063 2083 0.85 0.39 0.62 chair 2589 2288 0.89 0.39 0.22 building 6791 3450 0.89 0.40 0.21 city 2951 3190 0.67 0.40 0.50 Figure 2: Scatter plot of audio cluster purity weighted by log cluster size vs variance for k = 500 (least-squares line superimposed).']
"Scatter plot of audio cluster purity weighted by log cluster size vs variance for k = 500 (least-squares line superimposed).
based on the standard WSJ recipe and trained using the Google ASR hypothesis as a proxy for the transcriptions.",5 Experiments and Analysis,[0],[0]
Any word whose duration is overlapped 30% or more by the acoustic segment is included in the label string for the segment.,5 Experiments and Analysis,[0],[0]
We then employ a majority vote scheme to derive the overall cluster labels.,5 Experiments and Analysis,[0],[0]
"When computing the purity of a
cluster, we count a cluster member as matching the cluster label as long as the overall cluster label appears in the member’s label string.",5 Experiments and Analysis,[0],[0]
"In other words, an acoustic segment overlapping the words “the lighthouse” would receive credit for matching the overall cluster label “lighthouse”.",5 Experiments and Analysis,[0],[0]
A breakdown of the segments captured by two clusters is shown in Table 2.,5 Experiments and Analysis,[1.0],['A breakdown of the segments captured by two clusters is shown in Table 2.']
"We investigated some simple schemes for predicting highly pure clusters, and found that the empirical variance of the cluster members (average squared distance to the cluster centroid) was a good indicator.",5 Experiments and Analysis,[0],[0]
Figure 2 displays a scatter plot of cluster purity weighted by the natural log of the cluster size against the empirical variance.,5 Experiments and Analysis,[0],[0]
"Large, pure clusters are easily predicted by their low empirical variance, while a high variance is indicative of a garbage cluster.
",5 Experiments and Analysis,[0],[0]
"Ranking a set of k = 500 acoustic clusters by their variance, Table 3 displays some statistics for the 50 lowest-variance clusters.",5 Experiments and Analysis,[0],[0]
"We see that most of the clusters are very large and highly pure, and their labels reflect interesting object categories being identified by the neural network.",5 Experiments and Analysis,[0],[0]
"We additionally compute the coverage of each cluster by counting the total number of instances of the clus-
ter label anywhere in the training data, and then compute what fraction of those instances were captured by the cluster.",5 Experiments and Analysis,[0],[0]
"There are many examples of high coverage clusters, e.g. the “skyscraper” cluster captures 84% of all occurrences of the word “skyscraper”, while the “baseball” cluster captures 86% of all occurrences of the word “baseball”.",5 Experiments and Analysis,[0],[0]
"This is quite impressive given the fact that no conventional speech recognition was employed, and neither the multimodal neural network nor the grounding algorithm had access to the text transcripts of the captions.
",5 Experiments and Analysis,[0],[0]
"To get an idea of the impact of the k parameter as well as a variance-based cluster pruning threshold based on Figure 2, we swept k from 250 to 2000 and computed a set of statistics shown in Table 4.",5 Experiments and Analysis,[0],[0]
We compute the standard overall cluster purity evaluation metric in addition to the average coverage across clusters.,5 Experiments and Analysis,[0],[0]
"The table shows the natural tradeoff between cluster purity and redun-
dancy (indicated by the average cluster coverage) as k is increased.",5 Experiments and Analysis,[0],[0]
"In all cases, the variance-based cluster pruning greatly increases both the overall purity and average cluster coverage metrics.",5 Experiments and Analysis,[0],[0]
"We also notice that more unique cluster labels are discovered with a larger k.
Next, we examine the image clusters.",5 Experiments and Analysis,[0],[0]
"Figure 3 displays the 9 most central image crops for a set of 10 different image clusters, along with the majority-vote label of each image cluster’s associated audio cluster.",5 Experiments and Analysis,[0],[0]
"In all cases, we see that the image crops are highly relevant to their audio cluster label.",5 Experiments and Analysis,[0],[0]
"We include many more example image clusters in Appendix A.
In order to examine the semantic embedding space in more depth, we took the top 150 clusters from the same k = 500 clustering run described in Table 3 and performed t-SNE (van der Maaten and Hinton, 2008) analysis on the cluster centroid vectors.",5 Experiments and Analysis,[0],[0]
"We projected each centroid down to 2 di-
mensions and plotted their majority-vote labels in Figure 4.",5 Experiments and Analysis,[0],[0]
"Immediately we see that different clusters which capture the same label closely neighbor one another, indicating that distances in the embedding space do indeed carry information discriminative across word types (and suggesting that a more sophisticated clustering algorithm than kmeans would perform better).",5 Experiments and Analysis,[0],[0]
"More interestingly, we see that semantic information is also reflected in these distances.",5 Experiments and Analysis,[0],[0]
"The cluster centroids for “lake,” “river,” “body,” “water,” “waterfall,” “pond,” and “pool” all form a tight meta-cluster, as do “restaurant,” “store,” “shop,” and “shelves,” as well as “children,” “girl,” “woman,” and “man.”",5 Experiments and Analysis,[0.9789089317175927],"['The cluster centroids for “lake,” “river,” “body,” “water,” “waterfall,” “pond,” and “pool” all form a tight meta-cluster, as do “restaurant,” “store,” “shop,” and “shelves,” as well as “children,” “girl,” “woman,” and “man.” Many other semantic meta-clusters can be seen in Figure 4, suggesting that the embedding space is capturing information that is highly discriminative both acoustically and semantically.']"
"Many other semantic meta-clusters can be seen in Figure 4, suggesting that the embedding space is capturing information that is highly discriminative both acoustically and semantically.
",5 Experiments and Analysis,[0],[0]
"Because our experiments revolve around the discovery of word and object categories, a key question to address is the extent to which the supervision used to train the VGG network constrains or influences the kinds of objects learned.",5 Experiments and Analysis,[0],[0]
"Because the 1,000 object classes from the ILSVRC2012 task (Russakovsky et al., 2015) used to train the VGG network were derived from WordNet synsets (Fellbaum, 1998), we can measure the semantic similarity between the words
learned by our network and the ILSVRC2012 class labels by using synset similarity measures within WordNet.",5 Experiments and Analysis,[0],[0]
"We do this by first building a list of the 1,000 WordNet synsets associated with the ILSVRC2012 classes.",5 Experiments and Analysis,[0],[0]
"We then take the set of unique majority-vote labels associated with the discovered word clusters for k = 500, filtered by setting a threshold on their variance (σ2 ≤ 0.65) so as to get rid of garbage clusters, leaving us with 197 unique acoustic cluster labels.",5 Experiments and Analysis,[0],[0]
"We then look up each cluster label in WordNet, and compare all noun senses of the label to every ILSVRC2012 class synset according to the path similarity measure.",5 Experiments and Analysis,[0],[0]
"This measure describes the distance between two synsets in a hyponym/hypernym hierarchy, where a score of 1 represents identity and lower scores indicate less similarity.",5 Experiments and Analysis,[0],[0]
We retain the highest score between any sense of the cluster label and any ILSVRC2012 synset.,5 Experiments and Analysis,[0],[0]
"Of the 197 unique cluster labels, only 16 had a distance of 1 from any ILSVRC12 class, which would indicate an exact match.",5 Experiments and Analysis,[0],[0]
"A path similarity of 0.5 indicates one degree of separation in the hyponym/hypernym hierarchy - for example, the similarity between “desk” and “table” is 0.5.",5 Experiments and Analysis,[0],[0]
"47 cluster labels were found to have a similarity of 0.5 to some ILSVRC12 class, leaving 134 cluster labels whose highest similarity to any ILSVRC12 class was less than 0.5.",5 Experiments and Analysis,[0],[0]
"In
other words, more than two thirds of the highly pure pattern clusters learned by our network were dissimilar to all of the 1,000 ILSVRC12 classes used to pretrain the VGG network, indicating that our model is able to generalize far beyond the set of classes found in the ILSVRC12 data.",5 Experiments and Analysis,[0],[0]
We display the labels of the 40 lowest variance acoustic clusters labels along with the name and similarity score of their closest ILSVRC12 synset in Table 5.,5 Experiments and Analysis,[0],[0]
"In this paper, we have demonstrated that a neural network trained to associate images with the waveforms representing their spoken audio captions can successfully be applied to discover and
cluster acoustic patterns representing words or short phrases in untranscribed audio data.",6 Conclusions and Future Work,[0],[0]
"An analogous procedure can be applied to visual images to discover visual patterns, and then the two modalities can be linked, allowing the network to learn, for example, that spoken instances of the word “train” are associated with image regions containing trains.",6 Conclusions and Future Work,[0],[0]
"This is done without the use of a conventional automatic speech recognition system and zero text transcriptions, and therefore is completely agnostic to the language in which the captions are spoken.",6 Conclusions and Future Work,[0],[0]
"Further, this is done in O(n) time with respect to the number of image/caption pairs, whereas previous stateof-the-art acoustic pattern discovery algorithms which leveraged acoustic data alone run in O(n2) time.",6 Conclusions and Future Work,[0],[0]
"We demonstrate the success of our methodology on a large-scale dataset of over 214,000 image/caption pairs comprising over 522 hours of spoken audio data, which is to our knowledge the largest scale acoustic pattern discovery experiment ever performed.",6 Conclusions and Future Work,[0],[0]
"We have shown that the shared multimodal embedding space learned by our model is discriminative not only across visual object categories, but also acoustically and semantically across spoken words.
",6 Conclusions and Future Work,[0],[0]
The future directions in which this research could be taken are incredibly fertile.,6 Conclusions and Future Work,[0],[0]
"Because our method creates a segmentation as well as an alignment between images and their spoken captions, a generative model could be trained using these alignments.",6 Conclusions and Future Work,[0],[0]
"The model could provide a spoken caption for an arbitrary image, or even synthesize an image given a spoken description.",6 Conclusions and Future Work,[0],[0]
"Modeling improvements are also possible, aimed at the goal of incorporating both visual and acoustic localization into the neural network itself.",6 Conclusions and Future Work,[0],[0]
"The same framework we use here could be extended to video, enabling the learning of actions, verbs, environmental sounds, and the like.",6 Conclusions and Future Work,[0],[0]
"Additionally, by collecting a second dataset of captions for our images in a different language, such as Spanish, our model could be extended to learn the acoustic correspondences for a given object category in both languages.",6 Conclusions and Future Work,[0],[0]
"This paves the way for creating a speech-to-speech translation model not only with absolutely zero need for any sort of text transcriptions, but also with zero need for directly parallel linguistic data or manual human translations.",6 Conclusions and Future Work,[0],[0]
"beach cliff pool desert field
chair table staircase statue stone
church forest mountain skyscraper trees
waterfall windmills window city bridge
flowers man wall archway baseball
boat shelves cockpit girl children
building rock kitchen plant hallway",A Additional Cluster Visualizations,[1.0000000934661317],['beach cliff pool desert field chair table staircase statue stone church forest mountain skyscraper trees waterfall windmills window city bridge flowers man wall archway baseball boat shelves cockpit girl children building rock kitchen plant hallway']
"Given a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions.",abstractText,[0],[0]
"For example, our model is able to detect spoken instances of the words “lighthouse” within an utterance and associate them with image regions containing lighthouses.",abstractText,[0],[0]
"We do not use any form of conventional automatic speech recognition, nor do we use any text transcriptions or conventional linguistic annotations.",abstractText,[0],[0]
"Our model effectively implements a form of spoken language acquisition, in which the computer learns not only to recognize word categories by sound, but also to enrich the words it learns with semantics by grounding them in images.",abstractText,[0],[0]
Learning word-like units from joint audio-visual analysis,title,[0],[0]
