0	34	Reinforcement learning algorithms aim at learning policies for achieving target tasks by maximizing rewards provided by the environment.
1	19	In some scenarios, these rewards are supplied to the agent continuously, e.g. the running score in an Atari game (Mnih et al., 2015), or the distance between a robot arm and an object in a reaching task (Lillicrap et al., 2016).
7	21	Yet, the three-year-old has no trouble entertaining herself in that playground using what psychologists call intrinsic motivation (Ryan, 2000) or curiosity (Silvia, 2012).
11	24	Most formulations of intrinsic reward can be grouped into two broad classes: 1) encourage the agent to explore “novel” states (Bellemare et al., 2016; Lopes et al., 2012; Poupart et al., 2006) or, 2) encourage the agent to perform actions that reduce the error/uncertainty in the agent’s ability to predict the consequence of its own actions (i.e. its knowledge about the environment) (Houthooft et al., 2016; Mohamed & Rezende, 2015; Schmidhuber, 1991; 2010; Singh et al., 2005; Stadie et al., 2015).
21	26	However, we manage to escape most pitfalls of previous prediction approaches with the following key insight: we only predict those changes in the environment that could possibly be due to the actions of our agent or affect the agent, and ignore the rest.
22	35	That is, instead of making predictions in the raw sensory space (e.g. pixels), we transform the sensory input into a feature space where only the information relevant to the action performed by the agent is represented.
24	18	Since the neural network is only required to predict the action, it has no incentive to represent within its feature embedding space the factors of variation in the environment that do not affect the agent itself.
29	39	Curiosity helps an agent explore its environment in the quest for new knowledge (a desirable characteristic of exploratory behavior is that it should improve as the agent gains more knowledge).
30	38	Further, curiosity is a mechanism for an agent to learn skills that might be helpful in future scenarios.
31	19	In this paper, we evaluate the effectiveness of our curiosity formulation in all three of these roles.
46	24	θP is optimized to maximize the expected sum of rewards, max θP Eπ(st;θP )[Σtrt] (1) Unless specified otherwise, we use the notation π(s) to denote the parameterized policy π(s; θP ).
47	71	Our curiosity reward model can potentially be used with a range of policy learning methods; in the experiments discussed here, we use the asynchronous advantage actor critic policy gradient (A3C) (Mnih et al., 2016) for policy learning.
48	18	Our main contribution is in designing an intrinsic reward signal based on prediction error of the agent’s knowledge about its environment that scales to high-dimensional continuous state spaces like images, bypasses the hard problem of predicting pixels and is unaffected by the unpredictable aspects of the environment that do not affect the agent.
49	102	Making predictions in the raw sensory space (e.g. when st corresponds to images) is undesirable not only because it is hard to predict pixels directly, but also because some part of the input sensory space could be unpredictable and inconsequential to the agent, for e.g., the movement and location of tree leaves in a breeze in the environment.
51	34	A good feature space for curiosity should model (1) and (2) and be unaffected by (3).
52	44	The latter is because, if there is a source of variation that is inconsequential for the agent, then the agent has no incentive to know about it.
58	23	LI is modeled as soft-max loss across all possible actions when at is discrete.
59	26	The learned function g is also known as the inverse dynamics model and the tuple (st, at, st+1) required to learn g is obtained while the agent interacts with the environment using its current policy π(s).
60	35	Simultaneously with the inverse model g, we train another sub-module that takes as inputs at and φ(st) to predict the feature encoding of the state at time step t+ 1, φ̂(st+1) = f ( φ(st), at; θF ) (4) where φ̂(st+1) is the predicted estimate of φ(st+1).
63	19	The inverse model helps learn a feature space that encodes information relevant for predicting the agent’s actions only and the forward model makes this learned feature representation more predictable.
66	51	See Figure 2 for illustration of the formulation.
75	52	The agent is only provided a sparse terminal reward of +1 if it finds the vest and zero otherwise.
76	60	For generalization experiments, we pre-train on a different map with different random textures from (Dosovitskiy & Koltun, 2016) with 2100 step long episodes as there is no goal in pre-training.
81	102	This property makes the game particularly hard, e.g. to make a long jump over tall pipes or wide gaps, the agent needs to predict the same action up to 12 times in a row, introducing long-range dependencies.
86	28	Generalization is evaluated on a novel map with novel textures in VizDoom and on subsequent game levels in Mario.
88	36	We systematically varied the difficulty of this task and constructed “dense”, “sparse” and “very-sparse” reward (see Figure 4b) scenarios by varying the distance between the initial spawning location of the agent and the location of the goal.
94	29	One possible explanation of the inferior performance of ICM-pixels in comparison to ICM is that in every episode the agent is spawned in one out of seventeen rooms with different textures.
97	20	Note that ICM-pixels and ICM have similar convergence because, with a fixed spawning location of the agent, the ICM-pixels encounters the same textures at the starting of each episode which makes learning the pixel-prediction model easier as com- pared to the “dense” reward case.
98	31	Finally, in the “very sparse” reward case, both the A3C agent and ICM-pixels never succeed, while the ICM agent achieves a perfect score in 66% of the random runs.
99	31	This indicates that ICM is better suited than ICM-pixels and vanilla A3C for hard goal directed exploration tasks.
