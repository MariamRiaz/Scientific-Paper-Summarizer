0	93	Matrix completion method has been used in a wide range of applications such as collaborative filtering for recommendation (Koren et al., 2009), multi-label learning (Cabral et al., 2011) and clustering (Hsieh et al., 2012).
1	6	In these applications, every entry is modeled as the inner product between factors corresponding to the row and column variables.
2	30	For example, in movie recommendation, each row factor represents the latent representation of a user and each column factor represents the latent representation of a movie.
3	41	In many applications of significant interest, besides the partially observed matrix, side information, in the form of features, is also available.
4	10	These might correspond to demographic information (genders, occupation) for users or product information (genre, director) in a movie recommender system for example.
6	68	Formally, let L∗ ∈ Rd1×d2 be the unknown low-rank matrix with rank r, and let XL ∈ Rd1×n1 and XR ∈ Rd2×n2 be the known feature matrices with d1 ≥ n1 ≥ r and d2 ≥ n2 ≥ r. We assume the unknown rank-r matrix L∗ can be represented by XLM∗X>R for some unknown matrix M ∗ ∈ Rn1×n2 .
8	20	This inductive approach has been applied successfully in many applications including collaborative filtering (Abernethy et al., 2009; Menon et al., 2011; Chen et al., 2012), multi-label learning (Xu et al., 2013; Si et al., 2016), semi-supervised clustering (Yi et al., 2013; Si et al., 2016), gene-disease prediction (Natarajan & Dhillon, 2014) and blog recommendation (Shin et al., 2015).
11	18	Specifically, Xu et al. (2010) adapted the convex relaxation approach (Candès & Recht, 2009; Candès & Tao, 2010) and requires only O(rn log n log d)1 samples to recover the underlying matrix, which we believe is tight up to logarithmic factors.
12	3	However, the computational cost is usually high because they need to solve a nuclear norm minimization problem, which is inherently slow due to its high per-iteration complexity and non-strongly convex objective function (c.f.
14	59	On the other hand, Jain & Dhillon (2013) (also see Zhong et al. (2015)) proposed an algorithm which first does a spectral initialization to obtain a coarse estimate, then uses alternating minimization to refine the estimate.
15	69	Their algorithm has a locally linear rate of convergence but requires O(r3n2 log n log(1/ )) samples, which has an unsatisfactory quadratic dependency on n and cannot achieve exact recovery because sample complexity also depends on the target accuracy .
17	2	In this paper, we answer this question affirmatively.
18	1	Specifically, we propose a multi-phase gradient-based algorithm that converges to the underlying true matrix at a linear rate with sample complexity linearly depending on n and logarithmically depending on d. Our algorithm is a novel and highly nontrivial extension of Procrustes Flow (Tu et al., 2015) in which we add an additional phase to reduce the variance of gradient estimate, and therefore we call it MultiPhase Procrustes Flow.
19	20	The main challenges and technical insights are summarized in the following section.
21	16	A typical procedure is first to do a spectral initialization to obtain a coarse estimate, and then to use BurerMonteiro factorization (Burer & Monteiro, 2003) with projected gradient descent (a.k.a., Procrustes flow) on the partially observed entries to recover the underlying matrix, where the projection is introduced to control the variance of gradient descent (Tu et al., 2015; Zheng & Lafferty, 2016; Yi et al., 2016).
22	6	Our proposed algorithm also follows this framework.
23	3	However, direct adaptation does not achieve the desired statistical and computational rates.
24	9	Statistically, in the classical matrix completion setting, after the initialization phase, the variance of the gradient is at a smaller order than the magnitude of expected gradient for all iterations.
25	27	However, in our setting, because of limited samples, such uniform bound does not hold.
26	4	Computationally, the projection step in the inductive setting is more costly than that in the classical setting because we need to solve a convex quadratically-constrained-quadratic-programming (QCQP) problem (c.f.
27	5	Our first key observation is that the variance of the gradient converges to 0 at a faster rate than the magnitude of expectation of the gradient.
28	32	Therefore, if the iterate is close enough to the optimum, say in a ball with radius O(1/n) around the optimum, the desired uniform bound still holds.
29	32	Further, this observation also indicates when we are close to the optimum, projection step is not needed, i.e., vanilla gradient descent suffices.
30	16	Nevertheless, with limited samples, the spectral initialization cannot directly achieve this goal.
31	13	Our second key observation is that after a rough spectral initialization, if we use fresh samples to calculate the gradient at each iteration, the variance is still small compared with the expectation of the gradient.
32	13	In light of this, we add a new phase to the original algorithm where we use fresh samples to estimate the gradient at each iteration and use projected gradient descent to refine our estimation.
33	6	Though the projection is costly, we only need O(r log n) iterations to converge to a ball with radius O(1/n) around the optimum, since gradient descent in our problem enjoys a linear rate of convergence.
36	38	Denote the d × d identity matrix by Id.
39	20	Let ‖x‖2 be the `2 norm of a d-dimensional vector x ∈ Rd.
