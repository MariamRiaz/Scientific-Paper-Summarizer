0	28	An agent executing a sequence of instructions must address multiple challenges, including grounding the language to its observed environment, reasoning about discourse dependencies, and generating actions to complete high-level goals.
3	145	The third instruction requires resolving it to the rightmost beaker mentioned in the second instruction, and reasoning about the set of actions required to mix the colors in the beaker to brown.
4	27	In this paper, we describe a model and learning approach to map sequences of instructions to actions.
6	93	The majority of work on executing sequences of instructions focuses on mapping instructions to high-level formal representations, which are then evaluated to generate actions (e.g., Chen and Mooney, 2011; Long et al., 2016).
9	65	This requires resolving references without explicitly modeling them, and learning the sequences of actions required to complete high-level actions; for example, that mixing requires removing everything in the beaker and replacing with the same number of brown items.
10	21	A key challenge in executing sequences of instructions is considering contextual cues from both the history of the interaction and the state of the world.
12	22	The world state provides the set of objects the instruction may refer to, and implicitly determines the available actions.
14	21	Both types of contexts continuously change during an interaction.
16	25	We propose an attentionbased model that takes as input the current instruction, previous instructions, the initial world state, and the current state.
119	33	We compute the context vectors zck and z p k for the current instruction and previous instructions: zck = ATTEND(X c,hdk−1,W c) zpk = ATTEND(X p, [hdk−1, z c k],W p) , where hdk−1 is the decoder hidden state for step k− 1, and Xc and Xp are the sets of vector representations for the current instruction and previous instructions.
130	30	Otherwise, we execute the action ak to generate the next state sk+1, and update the agent context s̃k to s̃k+1 by appending the pair (sk, ak) to the execution ē and replacing the current state with sk+1.
131	21	The model parameters θ include: the embedding functions φI and φO; the recurrence parameters for −−−−−→ LSTME , ←−−−−− LSTME , and LSTMD; WC , WP , Wsb,1, Wsb,2, Wsc,1, Wsc,2, Wd, Wa, and bd; and the domain dependent parameters, including the parameters of the encoding function ENC and the action type, first argument, and second argument weights baT , ba1 , and ba2 .
132	28	We estimate the policy parameters θ using an exploration-based learning algorithm that maximizes the immediate expected reward.
133	42	Broadly speaking, during learning, we observe the agent behavior given the current policy, and for each visited state compute the expected immediate reward by observing rewards for all actions.
137	28	Reward The reward R(j)i : S × S × A → R is defined for each example j and instruction i: R (j) i (s, a, s ′) = P (j) i (s, a, s ′) + φ (j) i (s ′)− φ(j)i (s) , where s is a source state, a is an action, and s′ is a target state.4 P (j)i (s, a, s ′) is a problem reward and φ (j) i (s ′)− φ(j)i (s) is a shaping term.
151	38	We use a potential-based shaping term φ(j)i (s ′)− φ(j)i (s) (Ng et al., 1999), where φ (j) i (s) = −||s− g (j) i || computes the edit distance between the state s and the goal, measured over the objects in each state.
157	26	We iterate over the training data T times (line 1).
160	159	Given the sampled states visited, we compute the entropy (line 15) and observe the immediate reward for all actions (line 19) for each step.
169	142	In our algorithm, as the agent learns to roll out the correct POP prefix, it is then exposed to the reward for the first PUSH even though it likely sampled another POP.
180	23	State encodings are detailed in the Supplementary Material.
186	27	PUSH takes a beaker index and a color, and adds the color at the top of the beaker.
187	56	To encode a state, we encode each beaker with an RNN, and concatenate the last output with the beaker index embedding.
209	30	We also report supervised learning results (SUPERVISED) by heuristically generating correct executions and computing maximum-likelihood estimate using contextaction demonstration pairs.
235	22	This highlights the sensitivity of the model to the random effects of initialization, dropout, and ordering of training examples.
242	149	If the model picked the correct action, but the wrong beaker, we count it as a state reference.
245	32	Because the agent only has access to the world state after following this instruction, it does not observe what kind of item was previously removed, and cannot identify the item to add.
247	55	For example, the SCENE instruction person in green appears on the right end is ambiguous.
253	38	We propose a model to reason about contextdependent instructional language that display strong dependencies both on the history of the interaction and the state of the world.
257	28	However, it is particularly suitable to recovering from biases acquired early during learning, for example due to biased action spaces, which is likely to lead to incorrect blame assignment in neural network policies.
259	73	One possible direction for future work is to use an estimator to predict rewards for all actions, rather than observing them.
