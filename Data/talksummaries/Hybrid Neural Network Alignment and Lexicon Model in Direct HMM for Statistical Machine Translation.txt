0	9	The hidden Markov model (HMM) was first introduced to statistical machine translation for addressing the word alignment problem (Vogel et al., 1996).
2	71	In the conventional approach, the Bayes’ theorem is used and the HMM is applied to the inverse translation model Pr(eI1|fJ1 ) = Pr(eI1) · Pr(fJ1 |eI1) = ∑ aJ1 Pr(fJ1 , a J 1 |eI1) (1) In this case, as a part of a noisy channel model, the marginalisation becomes intractable for every e. This work proposes a novel concept focusing on direct HMM for Pr(eI1|fJ1 ), in which the alignment direction is from target to source positions.
3	22	This specific property allows us to introduce dependencies into the translation model that take the full source sentence into account.
5	43	The lexicon and alignment probabilities in the HMM are modeled using feedforward neural networks (FFNN) and they are trained jointly.
7	10	The experiments are conducted on the IWSLT 2016 German→English and BOLT Chinese→English translation tasks.
26	15	Then the model can be defined as: Pr(eI1|fJ1 ) = ∑ bI1 Pr(eI1, b I 1|fJ1 ) (2) Pr(eI1, b I 1|fJ1 ) = I∏ i=1 p(ei, bi|bi−11 , ei−11 , fJ1 ) = I∏ i=1 p(ei|bi1, ei−11 , fJ1 )︸ ︷︷ ︸ lexicon model · p(bi|bi−11 , ei−11 , fJ1 )︸ ︷︷ ︸ alignment model (3) Our feed-forward alignment model has the same architecture (Figure 1) as the one proposed in (Alkhouli et al., 2016).
30	13	For the lexicon model, we assume a similar dependence as in the alignment model with a shift, namely on the source words within a window centred on the aligned source word and n predecessor target words.
31	12	To overcome the high costs of the softmax function for large vocabularies, we adopt the class-factored output layer consisting of a class layer and a word layer (Goodman, 2001; Morin and Bengio, 2005).
32	22	The model in this case is defined as p(ei|bi1, ei−11 , fJ1 ) = p(ei|f bi+γmbi−γm , e i−1 i−n) = p(ei|c(ei), f bi+γmbi−γm , e i−1 i−n) · p(c(ei)|f bi+γm bi−γm , e i−1 i−n) (5) where c denotes a word mapping that assigns each target word to a single class, where the number of classes is chosen to be much smaller than the vocabulary size.
33	9	The lexicon model architecture is shown in Figure 2.
35	35	In the training of direct HMM including neural network-based models, the weights have to be updated along with the posterior probabilities calculated by the Baum-Welch algorithm.
36	90	Similar to the training procedure used in (BergKirkpatrick et al., 2010), we apply the EM algorithm and define the auxiliary function as Q(θ; θ̂) = ∑ bI1 p(bI1|fJ1 , eI1, θ) log p(eI1, bI1|fJ1 , θ̂) = ∑ bI1 p(bI1|fJ1 , eI1, θ) I∑ i=1 [log p(ei|fbi+γmbi−γm , e i−1 i−n, α̂) + log p(∆i|fbi−1+γmbi−1−γm , e i−1 i−n, β̂)] = ∑ i ∑ j pi(j|eI1, fJ1 , θ) log p(ei|f j+γmj−γm , e i−1 i−n, α̂) + ∑ i ∑ j′ ∑ j pi(j ′, j|eI1, fJ1 , θ) log p(∆i|f j ′+γm j′−γm , e i−1 i−n, β̂) (6) where θ̂ = {α̂, β̂}, j′ = bi−1 and pi(j|eI1, fJ1 , θ) = ∑ bI1:bi=j p(bI1|eI1, fJ1 , θ) (7) Then the parameters can be separated for lexicon model and alignment model: Q(θ; θ̂) = Qlex(θ; α̂) +Qalign(θ; β̂) (8) where ∂Qlex(θ, α̂) ∂α̂ = ∑ i ∑ j forward-backward algorithm︷ ︸︸ ︷ pi(j|eI1, fJ1 , θ) · ∂ ∂α̂ log p(ei|f j+γmj−γm , e i−1 i−n, α̂) ︸ ︷︷ ︸ backpropagation (9) ∂Qalign(θ, β̂) ∂β̂ = ∑ i ∑ j′ ∑ j forward-backward algorithm︷ ︸︸ ︷ pi(j ′, j|eI1, fJ1 , θ) · ∂ ∂β̂ log p(∆i|f j ′+γm j′−γm , e i−1 i−n, β̂) ︸ ︷︷ ︸ backpropagation (10) From Equations (9) and (10) we can observe that the marginalisation of hidden alignments ( ∑ j pi(j|eI1, fJ1 , θ)) is the only difference compared to the derivative of neural network training based on word-aligned data.
44	13	In the first epoch, for each sentence pair calculate and save the entire table of posterior probabilities pi(b|eI1, fJ1 ) (also pi(b ′, b|eI1, fJ1 ) for alignment model) using forward-backward algorithm based on the results of IBM-1 model.
45	8	Training neural network lexicon and alignment models based on the posterior probabilities.
46	13	From the second epoch onwards: (a) For each sentence pair, calculating the posterior probabilities based on the lexicon and alignment probabilities estimated by neural network models.
53	14	The experiments are conducted on the IWSLT 2016 German→English and BOLT Chinese→English translation tasks, which consist of 20M and 4M parallel sentence pairs respectively.
55	17	As an initial research of this topic, our new model is only applied for reranking n-best lists created by a phrase-based decoder.
60	24	Our direct HMM consists of a feed-forward neural network lexicon model with following configuration: • Five one-hot input vectors for source words and three for target words • Projection layer size 100 for each word • Two non-linear hidden layers with 1000 and 500 nodes respectively • A class-factored output layer with 1000 singleton classes dedicated to the most frequent words, and 1000 classes shared among the rest of the words.
61	17	and a feed-forward neural network alignment model with the same configuration as the lexicon model, except a small output layer with 201 nodes, which reflects that the aligned position can jump within the scope from −100 to 100 (Alkhouli et al., 2016).
73	18	During training a batch size of 50 is used.
74	10	More details about our neural machine translation system can be found in (Peter et al., 2016).
77	14	The direct HMM trained by the EM procedure with marginalizing the hidden alignments outperformed the same model trained on the word-aligned data.
80	15	This work aims to close the gap between the conventional word alignment models and the novel neural machine translation.
81	49	The proposed direct HMM consists of neural network-based alignment and lexicon models, both models are trained jointly and without any alignment information.
82	24	With the simple feed-forward neural network models, the HMM model already provides promising results and significantly improves the strong phrase-based translation system.
83	31	As future work, we are searching for alternatives to initialize the training instead of using IBM-1.
84	18	We will investigate recurrent model struc- tures, such as the LSTM representation for source and target word embeddings (Luong et al., 2015).
85	20	In addition to the network structure, we will implement a stand-alone decoder based on this novel model.
86	66	The first step would be to apply maximum approximation for the search problem as elucidated in (Yu et al., 2017).
87	22	Then we plan to investigate heuristics for marginalizing the hidden alignment during search.
