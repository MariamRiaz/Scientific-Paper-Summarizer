{"sections": [{"heading": "1. Introduction", "text": "Analyzing high dimensional, high volume data can be timeconsuming and resource intensive. Core data analysis, such as robust instances of regression, involve convex optimization tasks over large matrices, and do not naturally distribute or parallelize. In response to this, approximation algorithms have been proposed which follow a \u201csketch and solve\u201d paradigm: produce a reduced size representation of the data, and solve a version of the problem on this summary (Woodruff, 2014). It is then argued that the solution on the reduced data provides an approximation to the original problem on the original data. This paradigm is particularly attractive when the summarization can be computed efficiently on partial views of the full data\u2014for example, when it can be computed incrementally as the data arrives (streaming model) or assembled from summarizations of disjoint partitions of the data (distributed model) (Woodruff, 2014; Agarwal et al., 2012; Feldman et al., 2006). This\n*Equal contribution 1Department of Computer Science, University of Warwick, Coventry, UK 2School of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania, USA. Correspondence to: Charlie Dickens <c.dickens@warwick.ac.uk>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\ntemplate has been instantiated for a number of fundamental tasks in high dimensional linear algebra such as matrix multiplication, low rank approximation, and regression.\nOur understanding is well-established in the common case of the Euclidean norm, i.e., when distances are measured under the Minkowski p-norm for p = 2. Here, it suffices to choose a sketching matrix independent of the data\u2014where each entry is i.i.d. Gaussian, Rademacher, or more efficient variants of these. For other p values, less is known, but these are often needed to handle limitations of the 2-norm. For instance, p = 1 is widely used as it is extremely robust with respect to the presence of outliers while p > 2 can be used to detect outlying observations.\nWe continue the study of algorithms for `p norms on streaming and distributed data. A particular novelty of our results is that unlike previous distributed and streaming algorithms, they can all be implemented deterministically, i.e., our algorithms make no random choices. While in a number of settings randomized algorithms are highly beneficial, leading to massive computational savings, there are other applications which require extremely high reliability, for which one needs to obtain guaranteed performance across a large number of inputs. If one were to use a randomized algorithm, then it would need vanishingly small error probability; however, many celebrated algorithms in numerical linear algebra succeed with only constant probability. Another limitation of randomized algorithms was shown in (Hardt & Woodruff, 2013): if the input to a randomized sketch depends on the output of a preceding algorithm using the same sketch, then the randomized sketch can give an arbitrarily bad answer. Hence, such methods cannot handle adaptively chosen inputs. Thus, while randomized algorithms certainly have their place, the issues of high reliability and adaptivity motivate the development of deterministic methods for a number of other settings, for which algorithms are scarce.\nOur techniques can be viewed as a conceptual generalization of Liberty\u2019s Frequent Directions (in the 2-norm) (Liberty, 2013), which progressively computes an SVD on subsequent blocks of the input. This line of work (Liberty, 2013; Ghashami & Phillips, 2014; Ghashami et al., 2016; Ghashami et al., 2016) is the notable exception in numerical linear algebra, as it provides deterministic methods,\nalthough all such methods are specific to the 2-norm. Our core algorithm is similar in nature, but we require a very different technical analysis to argue that the basis transformation computed preserves the shape in the target p-norm.\nOur main application is to show how high dimensional regression and low rank approximation problems can be solved approximately and deterministically in the sketch and solve paradigm. The core of the summary is to find rows of the original matrix which have high leverage scores. That is, they contain a lot of information about the shape of the data. In the Euclidean norm, leverage scores correspond directly to row norms of an orthonormal basis. This is less straightforward for other `p norms, where the scores correspond to the row norms of so-called `p-well-conditioned bases. Moreover, while leverage scores are often used for sampling in randomized algorithms, we use them here in the context of fully deterministic algorithms.\nWe show how a superset of rows with high leverage scores can be found for arbitrary `p norms, based on only local information. This leads to efficient algorithms which identify rows with high (local) leverage scores within subsets of the data, and proceed hierarchically to collect a sufficient set of rows. These rows then allow us to solve regression problems: essentially, we solve the regression problem corresponding to just the retained input rows. We apply this technique to `p-regression and entrywise `p-low rank approximation. In particular, we use it to solve the `\u221e-regression problem with additive error in a stream. Note that the `\u221e problem reduces to finding a ball of minimum radius which covers the data, and global solutions are slow due to the need to solve a linear program. Instead, we show that only a subset of the data needs to be retained in the streaming model to compute accurate approximations. Given the relationship between the streaming model and the distributed model that we later define, this could be seen in the context of having data stored over multiple machines who could send \u2018important\u2019 rows of their data to a central coordinator in order to compute the approximation.\nSummary of Results. All our algorithms are deterministic polynomial time, and use significantly sublinear memory or communication in streaming and distributed models, respectively. We consider tall and thin n\u00d7 d matrices A for overconstrained regression so one should think of n d. We implement both deterministic and randomized variants of our algorithms. Section 3 presents an algorithm which returns rows of high \u2018importance\u2019 in a data matrix with additive error. This follows by storing a polynomial number (in d) of rows and using these to compute a well-conditioned basis. The key insight here is that rows of high norm in the full wellconditioned basis cannot have their norm decrease too much in a well-conditioned basis associated with a subblock; in\nfact they remain large up to a multiplicative poly(d) factor. Section 4 gives a method for computing a so-called `psubspace embedding of a data matrix in polynomial time. The space is n\u03b3 to obtain dO(1/\u03b3) distortion, for \u03b3 \u2208 (0, 1) a small constant. This result is then applied to `p-regression which is shown to have a poly(d) approximation factor with the same amount of space. Section 5 describes a deterministic algorithm which gives a poly(k)-approximation to the optimal low rank approximation problem in entrywise `1-norm. It runs in polynomial time for constant k. This method builds on prior work by derandomizing a subroutine from (Song et al., 2017). Section 6 describes an algorithm for computing an additiveerror solution to the `\u221e-regression problem, and shows a corresponding lower bound, showing that relative error solutions in this norm are not possible in sublinear space, even for randomized algorithms. Section 7 concludes with an empirical evaluation. More experiments, intermediate results, and formal proofs can be found in the Supplementary Material, as can results on approximate matrix multiplication.\nComparison to Related Work. There is a rich literature on algorithms for numerical linear algebra in general p-norms; most of which are randomized with the notable exception of Frequent Directions. The key contributions of our work for each of the problems considered and its relation to prior work is as follows:\nFinding high leverage rows: our algorithm is a single pass streaming algorithm and uses small space. We show that the global property of `p-leverage scores can be understood by considering only local statistics. Frequent Directions is the only comparable result to ours and outputs a summary of the rows only in the `2-norm. However, our method covers all p \u2265 1. Theorem 3.3 is the key result and is later used to prove Theorem 6.1 and approximate the `\u221e-regression problem.\nSubspace embedding, regression and `1 low-rank approximation: various approaches using row-sampling (Cohen & Peng, 2015; Dasgupta et al., 2008), and data oblivious methods such as low-distortion embeddings can solve regression in time proportional to the sparsity of the input matrix (Clarkson et al., 2013; Meng & Mahoney, 2013; Song et al., 2017; Woodruff & Zhang, 2013). However, despite the attractive running times and error guarantees of these works, they are all randomized and do not necessarily translate well to the streaming model of computation. Our contribution here is a fully deterministic algorithm that works for all p \u2265 1 in both streaming and distributed models. Randomized methods for `1 low-rank approximation have also been developed in (Song et al., 2017) and our result exploits a derandomized subroutine from this work to obtain a deterministic result which applies in both models."}, {"heading": "2. Preliminaries and Notation", "text": "We consider computing `p-leverage scores of a matrix, lowrank approximation, regression, and matrix multiplication. We assume the input is a matrix A \u2208 Rn\u00d7d and n d so rank(A) \u2264 d and the regression problems are overconstrained. Without loss of generality we may assume that the columns of the input matrix are linearly independent so that rank(A) = d. Throughout this paper we rely heavily on the notion of a well-conditioned basis for the column space of an input matrix, in the context of the entrywise p-norm which is \u2016A\u2016p = ( \u2211 i,j |Aij |p)1/p. Definition 2.1 (Well-conditioned basis). Let A \u2208 Rn\u00d7d have rank d. For p \u2208 [1,\u221e) let q = pp\u22121 be its dual norm. An n\u00d7 d matrix U is an (\u03b1, \u03b2, p)-well-conditioned basis for A if the column span of U is equal to that of A, \u2016U\u2016p \u2264 \u03b1, for all z \u2208 Rd, \u2016z\u2016q \u2264 \u03b2\u2016Uz\u2016p , and \u03b1, \u03b2, dO(1) are independent of n (Dasgupta et al., 2008).\nWe focus on the cases p < 2 and p > 2 because the deterministic p = 2 case is relatively straightforward. Indeed, for p = 2, ATA can be maintained incrementally as rows are added, allowing xTATAx to be computed for any vector x. So it is possible to find an exact `2 subspace embedding using O(d2) space in a stream and O(nd\u03c9\u22121) time (\u03c9 is the matrix multiplication constant). We adopt the convention that when p = 1 we take q =\u221e. Theorem 2.2 ((Dasgupta et al., 2008)). Let A be an n\u00d7 d matrix of rank d, let p \u2208 [1,\u221e) and let q be its dual norm. There exists an (\u03b1, \u03b2, p)-well-conditioned basis U for the column space of A such that:\n1. if p < 2 then \u03b1 = d 1 p+ 1 2 and \u03b2 = 1, 2. if p = 2 then \u03b1 = \u221a d and \u03b2 = 1, and 3. if p > 2 then \u03b1 = d 1 p+ 1 2 and \u03b2 = d 1 p\u2212 12 .\nMoreover,U can be computed in deterministic timeO(nd2+ nd5 log n) for p 6= 2 and O(nd2) if p = 2.\nWe freely use the fact that a well-conditioned basis U = AR can be efficiently computed for the given data matrix A. Details for the computation can be found in (Dasgupta et al., 2008) but this is done by computing a change of basis R such that U = AR is well-conditioned. Similarly, as R can be inverted we have the relation that UR\u22121 = A. Both methods are used so we adopt the convention that U = AR when writing a well-conditioned basis in terms of the input and US = A for the input in terms of the basis."}, {"heading": "2.1. Computation Models", "text": "Our algorithms operate under the streaming and distributed models of computation. In both settings an algorithm receives as input a matrix A \u2208 Rn\u00d7d. For a problem P, the\nalgorithm must keep a subset of the rows of A and, upon reading the full input, may use a black-box solver to compute an approximate solution to P with only the subset of rows stored. In both models we measure the summary size (storage), the update time which is the time taken to find the local summary, and the query time which is the time taken to compute an approximation to P using the summary.\nThe Streaming Model: The rows of A are given to the (centralized) algorithm one-by-one. Let b be the maximum number of rows that can be stored under the constraint that b is sublinear in n. The stored subset is used to compute local statistics which determine those rows to be kept or discarded from the stored set. Further rows are then appended and the process is repeated until the full matrix has been read. An approximation to the problem is then computed by solving P on the reduced subset of rows.\nThe Distributed Summary Model: Given a small constant \u03b3 \u2208 (0, 1), the input in the form of matrix A \u2208 Rn\u00d7d is partitioned into blocks among distributed compute nodes so that no block exceeds n\u03b3 rows. The computation then follows a tree structure: the initial blocks of the matrix form n1\u2212\u03b3 leaves of the compute tree. Each internal node merges and reduces its input from its child nodes. The first phase is for the leaf nodes l1, . . . , lm of the tree to reduce their input by computing a local summary on the block they receive as input. This is then sent to parent nodes p1, . . . , pm which merge and reduce the received rows until the space bound is reached. The resulting summaries are passed up the tree until we reach the root where a single summary of bounded size is obtained which can be used to compute an approximation to P. In total, there are O(1/\u03b3) levels in the tree. As the methods require only light synchronization (compute summary and return to coordinator), we do not model implementation issues relating to synchronization. Remark 2.3. The two models are quite close: the streaming model can be seen as a special case of the distributed model with only one participant who individually computes a summary, appends rows to the stored set, and reduces the new summary. This is represented as a deep binary tree, where each internal node has one leaf child. Likewise, the Distributed Summary Model can be implemented in a full streaming fashion over the entire binary tree. The experiments in Section 7 perform one round of merge-and-reduce in the distributed model to simulate the streaming approach."}, {"heading": "3. Finding Rows of High Leverage", "text": "This section is concerned with finding rows of high leverage from a matrix with respect to various p-norms. We conclude the section with an algorithm that returns rows of high leverage up to polynomial additive error. Definition 3.1. Let R be a change of basis matrix such that AR is a well-conditioned basis for the column space of A.\nThe (full) `p-leverage scores are defined as wi = \u2016eTi AR\u2016pp.\nNote that wi depends both on A and the choice of R, but we suppress this dependence in our notation. Next we present some basic facts about the `p leverage scores. Fact 1. By Definition 2.1 we have \u2211 i wi =\u2211\ni \u2016(AR)i\u2016pp \u2264 \u03b1p. Theorem 2.2 shows \u03b1 = poly(d). Define I = {i \u2208 [n] : wi > \u03c4\u2016AR\u2016pp} to be the index set of all rows whose `p leverage exceeds a \u03c4 fraction of \u2016AR\u2016pp, then: \u03b1p \u2265 \u2211 i wi \u2265 \u2211 i\u2208I wi \u2265 |I| \u00b7 \u03c4\u2016AR\u2016pp. Hence, |I| \u2264 \u03b1p/\u03c4\u2016AR\u2016pp = poly(d)/\u03c4 . So there are at most poly(d)/\u03c4 rows i for which wi \u2265 \u03c4\u2016AR\u2016pp. Fact 2. Definition 2.1 and Ho\u0308lder\u2019s inequality show that for any vector x we have |(ARx)i|p \u2264 \u03b2\u2016eTi AR\u2016pp \u00b7 \u2016ARx\u2016pp. Then \u03c4 \u2264 |eTi ARx|p/\u2016ARx\u2016pp \u2264 \u03b2wi. From this we deduce that if a row contributes at least a \u03c4 fraction of \u2016ARx\u2016pp then \u03c4 \u2264 wi\u03b2. That is, \u03c4 \u2264 wi for p \u2208 [1, 2] and \u03c4 \u2264 d1/2wi for p \u2208 (2,\u221e) by using Theorem 2.2. Definition 3.2. Let X be a matrix and Y be a subset of the rows of X . Define the local `p-leverage scores of Y with respect to X to be the leverage scores of rows Y found by computing a well-conditioned basis for Y rather than the whole matrix X .\nA key technical insight to proving Theorem 3.3 below is that rows of high leverage globally can be found by repeatedly finding rows of local high leverage. While relative `p row norms of a submatrix are at least as large as the full relative `p norms, it is not guaranteed that this property holds for leverage scores. This is because leverage scores are calculated from a well-conditioned basis for a matrix which need not be a well-conditioned basis for a block. However, we show that local `p leverage scores restricted to a coordinate subspace of a matrix basis do not decrease too much when compared to leverage scores in the original space. Let i be a row in A with local leverage score w\u0302i and global leverage score wi. Then w\u0302i \u2265 wi/ poly(d). The proof relies heavily on properties of the well-conditioned basis and details are given in the Supplementary Material, Lemma A.1. This lemma shows that local leverage scores can potentially drop in arbitrary `p norm, contrasting the behavior in `2. However, it is possible to find all rows exceeding a threshold globally by altering the local threshold. That is, to find all wi > \u03c4 globally we can find all local leverage scores exceeding an adjusted threshold w\u0302i > \u03c4/poly(d) to obtain a superset of all rows which exceed the global threshold. The price to pay for this is a poly(d) increase in space cost which, importantly, remains sublinear in n. Hence, we can gradually prune out rows of small leverage and keep only the most important rows of a matrix. Combining Lemmas A.1 and A.2 we can present the main theorem of the section.\nWe prove Theorem 3.3 by arguing the correctness of Algorithm 1 which reads A once only, row by row, and so\noperates in the streaming model of computation as follows. Let A\u2032 be the submatrix of A induced by the b block of poly(d)/\u03c4 rows. Upon storing A\u2032, we compute U , a local well-conditioned basis for A\u2032 and the local leverage scores with respect to U , w\u0302i(U) are calculated. Now, the local and global leverage scores can be related by Lemma A.1 as wi/poly(d) \u2264 w\u0302i so we can decide which rows to keep using an adjusted threshold. Any i for which the local leverage exceeds the adjusted threshold is kept in the sample and all other rows are deleted. The sample cannot be too large by properties of the well-conditioned basis and leverage scores so these kept rows can be appended to the next block which is read in before computing another well-conditioned basis and repeating in the same fashion. The proof of Theorem 3.3 is deferred to Appendix A. Theorem 3.3. Let \u03c4 > 0 be a fixed constant and let b denote a bound on the available space. There exists a deterministic algorithm, namely, Algorithm 1, which computes the `p-leverage scores of a matrix A \u2208 Rn\u00d7d with O(bd2 + bd5 log b) update time, poly(d) space, and returns all rows of A with `p leverage score satisfying wi \u2265 \u03c4/ poly(d).\n4. `p-Subspace Embeddings Under the assumptions of the Distributed Summary Model we present an algorithm which computes an `p-subspace embedding. By extension, this applies to both the distributed and streaming models of computation as described in Section 2.1. Two operations are needed for this model of computation: the merge and reduce steps. To reduce the input at each level a summary is computed by taking a block of input B (corresponding to a leaf node or a node higher up the tree) and computing a well-conditioned basis B = US. In particular, the summary is now the matrix S with U and B deleted. For the merge step, successive matrices S are concatenated until the space requirement is met. A further reduce step takes as input this concatenated matrix and the process is repeated. Further details, pseudocode, and proofs for this section are given in Appendix B. Definition 4.1. A matrix T is a relative error (c1, c2)-`p subspace embedding for the column space of a matrix A \u2208 Rn\u00d7d if there are constants c1, c2 > 0 so that for all x \u2208 Rd, c1\u2016Ax\u2016p \u2264 \u2016Tx\u2016p \u2264 c2\u2016Ax\u2016p. Theorem 4.2. Let A \u2208 Rn\u00d7d, p 6= 2,\u221e be fixed and fix a constant \u03b3 \u2208 (0, 1). Then there exists a one-pass deterministic algorithm which constructs a (1/dO(1/\u03b3), 1) relative error `p-subspace embedding in with O(n\u03b3d2 +n\u03b3d5 log n\u03b3) update time and O(n\u03b3d) space in the streaming and distributed models of computation.\nThe algorithm is used in a tree structure as follows: split inputA \u2208 Rn\u00d7d into n1\u2212\u03b3 blocks of size n\u03b3 , these form the leaves of the tree. For each block, a well-conditioned basis is\ncomputed and the change of basis matrix S \u2208 Rd\u00d7d is stored and passed to the next level of the tree. This is repeated until the concatenation of all the S matrices would exceed n\u03b3 . At this point, the concatenated S matrices form the parent node of the leaves in the tree and the process is repeated upon this node: this is the merge and reduce step of the algorithm. At every iteration of the merge-and-reduce steps it can be shown that a distortion of 1/d is introduced by using the summaries S. However, this can be controlled across all of the O(1/\u03b3) levels in the tree to give a deterministic relative error `p subspace embedding which requires only sublinear space and little communication. In addition, the subspace embedding can be used to achieve a deterministic relativeerror approximate regression result. The proof relies upon analyzing the merge-and-reduce behaviour across all nodes of the tree.\n`p-Regression Problem: Given matrix A \u2208 Rn\u00d7d and target vector b \u2208 Rn, find x\u0302 = argminx \u2016Ax\u2212 b\u2016p. Theorem 4.3. Let A \u2208 Rn\u00d7d, b \u2208 Rn, fix p 6= 2,\u221e and a constant \u03b3 > 0. The `p-regression problem can be solved deterministically in the streaming and distributed models with a (d + 1)O(1/\u03b3) = poly(d) relative error approximation factor. The update time is poly(n\u03b3(d + 1)) and O((1/\u03b3)n\u03b3(d + 1)) storage. The query time is poly(n\u03b3) for the cost of convex optimization."}, {"heading": "5. Low-Rank Approximation", "text": "`1-Low-Rank Approximation Problem: Given matrix A \u2208 Rn\u00d7d output a matrix B of rank k s.t., for constant k:\n\u2016A\u2212B\u20161 \u2264 poly(k) min A\u2032:rankk \u2016A\u2212A\u2032\u20161. (1)\nTheorem 5.1. Let A \u2208 Rn\u00d7d be the given data matrix and k be the (constant) target rank. Let \u03b3 > 0 be an arbitrary (small) constant. Then there exists a deterministic distributed and streaming algorithm (namely Algorithm 5 in Appendix C) which can output a solution to\nthe `1-Low Rank Approximation Problem with relative error poly(k) approximation factor, update time poly(n, d), space bounded by n\u03b3poly(d), and query time poly(n, d).\nThe key technique is similar to that of the previous section by using a tree structure with merge-and-reduce operations. For input A \u2208 Rn\u00d7d and constant \u03b3 > 0 partition A into n1\u2212\u03b3 groups of rows which form the leaves of the tree. The tree is defined as previously with the same \u2018merge\u2019 operation, but the \u2018reduce\u2019 step to summarize the data exploits a derandomization (subroutine Algorithm 4) of (Song et al., 2017) to compute an approximation to the optimal `1-lowrank approximation. Once this is computed, k of the rows in the summary are kept for later merge steps.\nThis process is continued with the successive k rows from n\u03b3 rows being \u2018merged\u2019 or added to the matrix until it has n\u03b3 rows. The process is repeated across all of the groups in the level and again on the successive levels on the tree from which it can be shown that the error does not propagate too much over the tree, thus giving the desired result."}, {"heading": "6. Application: `\u221e-Regression", "text": "Here we present a method for solving `\u221e-regression in a streaming fashion. Given input A and a target vector b, it is possible to achieve additive approximation error of the form \u03b5\u2016b\u2016p for arbitrarily large p. This contrasts with both Theorems 4.2 and 4.3 which achieve a relative error poly(d) approximation. Both of these theorems require that p is constant and not equal to the \u221e-norm. This restriction is due to a lower bound for `\u221e- regression showing that it cannot be approximated with relative error in sublinear space. The key to proving Theorem 6.1 below is using Theorem 3.3 to find high leverage rows and arguing that these are sufficient to give the claimed error guarantee.\nThe `\u221e-regression problem has been previously studied in the overdetermined case and can naturally be applied to curve-fitting under this norm. `\u221e-regression can be solved\nby linear programming (Sposito, 1976) and such a transformation allows the identification of outliers in the data. Also, if the errors are known to be distributed uniformly across an interval then `\u221e-regression estimator is the maximumlikelihood parameter choice (Hand, 1978). The same work argues that such uniform distributions on the errors often arise as round-off errors in industrial applications whereby the error is controlled or is small relative to the signal. There are further applications such as using `\u221e-regression to remove outliers prior to `2 regression in order to make the problem more robust (Shen et al., 2014). By applying `\u221e regression on subsets of the data an approximation to the Least Median of Squares (another robust form of regression) can be found. We now define the problem and proceed to show that it is possible to compute an approximate solution with additive error in `p-norm for arbitrarily large p.\nApproximate `\u221e-Regression problem: Given data A \u2208 Rn\u00d7d, target vector b \u2208 Rn, and error parameter \u03b5 > 0, compute an additive \u03b5\u2016b\u2016p error solution to:\nmin x\u2208Rd \u2016Ax\u2212 b\u2016\u221e = min x\u2208Rd [ max i |(Ax)i \u2212 bi| ] .\nTheorem 6.1. Let A \u2208 Rn\u00d7d, b \u2208 Rn and fix constants p \u2265 1, \u03b5 > 0 with p 6=\u221e. There exists a one-pass deterministic streaming algorithm which solves the `\u221e-regression problem up to an additive \u03b5\u2016b\u2016p error in dO(p)/\u03b5O(1) space, O(md5 + md2 logm) update time and Tsolve(m, d) query time.\nNote that Tsolve(m, d) query time is the time taken to solve the linear program associated with the above problem on a reduced instance size. Also, observe that Theorem 6.1 requires p < \u221e. This restriction is necessary to forbid relative error with respect to the infinity norm. Indeed, p can be an arbitrarily large constant, but for p = \u221e we can look for rows above an \u03b5/poly(d) threshold in the case when A is an all-ones column n-vector (so an n\u00d7 1 matrix). Then \u2016Ax\u2016\u221e = \u2016x\u2016\u221e since x is a scalar. Also, A is a wellconditioned basis for its own column span but the number of rows of leverage exceeding \u03b5/poly(d) = \u03b5 is n for a small constant \u03b5. This intuition allows us to prove the following theorem.\nTheorem 6.2. Any algorithm which outputs an \u03b5\u2016b\u2016\u221e relative error solution to the `\u221e-regression problem requires min { n, 2\u2126(d) } space."}, {"heading": "7. Experimental Evaluation", "text": "To validate our approach, we evaluate the use of high `p-leverage rows in order to approximate `\u221e-regression1, focusing particularly on the cases using `1 and `2 well-\n1Code available at https://github.com/c-dickens/ stream-summaries-high-lev-rows\nconditioned bases. It is straightforward to model `\u221e- regression as a linear program in the offline setting. We use this to measure the accuracy of our algorithm. The implementation is carried out in the single pass streaming model with a fixed space constraint, m, and threshold, \u03b1p/m for both conditioning methods to ensure the number of rows kept in the summary did not exceed m. Recall from Remark 2.3 that the single-pass streaming implementation is equivalent to the distributed model with only one participant applying merge-and-reduce, so this experiment can also be seen as a distributed computation with the merge step being the appending of new rows and the reduce step being the thresholding in the new well-conditioned basis.\nMethods. We analyze two instantiations of our methods based on how we find a well-conditioned basis and repeat over 5 independent trials with random permutations of the data. The methods are as follows:\nSPC3: We use an algorithm of Yang et al. (2013) to compute an `1-wcb. This method is randomized as it employs the Sparse Cauchy Transform and is only an `1-wellconditioned basis with constant probability We also implemented a check condition which showed that almost always, roughly 99% of the time, the randomized construction SPC3 would return a (d2.5, 1, 1)-well-conditioned basis. Thus, we bypassed this check in our experiment to ensure quick update times.\nOrth: In addition, we also used an orthonormal basis using the QR decomposition which is an `2-wcb. This method is fully deterministic and outputs a ( \u221a d, 1, 2)-well- conditioned basis.\nSample: A sample of the data is chosen uniformly at random and the retained summary has size exactly m.\nIdentity: No conditioning is performed. For a block B of the input, the surrogate scores wi(B) = \u2016eTi B\u201622/\u2016B\u20162F are used to determine which rows to keep. As the sum of these wi(B) is 1, we keep all rows which have wi(B) > 2/m. Since no more than m/2 of the rows can satisfy wi(B) > 2/m, the size of the stored subset of rows can be controlled and cannot grow too large.\nRemark 7.1. The Identity method keeps only the rows with high norm which contrasts our conditioning approach: if most of the mass of the block is concentrated on a few rows then these will appear heavy locally despite the possibility that they may correspond to previously seen or unimportant directions. In particular, if these heavy rows significantly outweigh the weight of some sparse directions in the data it is likely that the sparse directions will not be found at all. For instance, consider data X \u2208 Rn\u00d7d which is then augmented by appending the identity (and zeros) so that these are the only vectors in the new directions. That is, set X \u2032 = [X,0n\u00d7k;0k\u00d7d, Ik\u00d7k] and then\npermute the rows of X \u2032. The appended sparse vectors from Ik\u00d7k will have leverage of 1 so will be detected by the wellconditioned basis methods. However there is no guarantee that the Identity method will identify these directions if the entries in X significantly outweigh those in Ik\u00d7k. In addition, there is also no guarantee that using uniform sampling will identify these points, particularly when k is small compared to n and d. So while choosing to do no conditioning seems attractive, this example shows that doing so may not give any meaningful guarantees and hence we prefer the approach in Section 3. We compare only to these baselines as we are not aware of any other competing methods in the small memory regime for the `\u221e-regression problem.\nDatasets. We tested the methods on a subset of the US Census Data containing 5 million rows and 11 columns2 and YearPredictionMSD3 which has roughly 500,000 rows and 90 columns (although we focus on a fixed 50,000 row sample so that the LP for regression is tractable: see Figure 4c in the Supplementary Material, Appendix F). For the census dataset, space constraints between 50,000 and 500,000 rows were tested and for the YearPredictionsMSD data space budgets were tested between 2,500 and 25,000. The general behavior is roughly the same for both datasets so for brevity we primarily show the results for US Census Data, and defer corresponding plots for YearPredictionsMSD to Appendix F.\nResults on approximation error compared to storage Let f\u2217 denote the minimal value of the full regression obtained by x\u2217 and let x\u2032 be the output of the reduced problem. The approximate solution to the full problem is then f\u0302 = \u2016Ax\u2032 \u2212 b\u2016\u221e and approximation error is measured as f\u0302/f\u2217 \u2212 1 (note that f\u0302 \u2265 f\u2217). An error closer to 0 demonstrates that f\u0302 is roughly the same as f\u2217 so the optimal value is well-approximated. Figures 2a and 2b show that on both datasets the Identity method consistently performs poorly while Sample achieves comparable accuracy to the conditioning methods. Despite the simplicity of uniform sampling to keep a summary, the succeeding sections discuss the increased time and space costs of using such a sample and show that doing so is not favourable. Thus, neither of the baseline methods output a summary which can be used to approximate the regression problem both accurately and quickly, hence justifying our use of leverage scores. Our conditioning methods perform particularly well in the US Census Data data (Figure 2a) with Orth appearing to give the most accurate summary and SPC3 performing comparably well but with slightly more fluctuation: similar behaviour is observed in the YearPredictionMSD\n2http://www.census.gov/census2000/PUMS5. html\n3https://archive.ics.uci.edu/ml/datasets/ yearpredictionmsd\n(Figure 2b) data too. The conditioning methods are also seen to be robust to the storage constraint, give accurate performance across both datasets using significantly less storage than sampling, and give a better estimate in general than doing no conditioning.\nResults on Space Complexity. Recall that the space constraint is m rows and throughout the stream, after a local computation, the merge step concatenates more rows to the existing summary until the bound m is met, prior to computing the next reduction. During the initialization of the block A\u2032 by Algorithm 1, the number of stored rows is exactly m. However, we measure the maximum number of rows kept in a summary after every reduction step to understand how large the returned summary can grow. As seen in Figure 2c, Identity keeps the smallest summary but there is no reason to expect it has kept the most important rows. In contrast, if m is the bound on the summary size, then uniform sampling always returns a summary of size exactly m. However, we see that this is not optimal as both conditioning methods can return a set of rows which are pruned at every iteration to roughly half the size and contains only the most important rows in that block. Both conditioning methods exhibit similar behavior and are bounded between both Sample and Identity methods. Therefore, both of the conditioning methods respect the theoretical bound and, crucially, return a summary which is sublinear in the space constraint and hence a significantly smaller fraction of the input size.\nResults on Time Complexity. There are three time costs measured. The first is the update time taken to compute the local well-conditioned basis which is theoretically O(md2 +md5 logm) by Theorem 2.2. However, the two bases that we test are an orthonormal basis, computable in time O(md2) and the SPC3 transform which takes time O(nnz(B) logm) for a block B with m rows and nnz(B) non-zero entries. Figure 3a demonstrates that SPC3 is faster than Orth on this data in practice but this small absolute difference becomes negligible over the entirety of the stream as seen in Figure 3c. The query time in Figure 3b is roughly proportional to the summary size in all instances but here the conditioning methods perform noticeably better due to the smaller summary size that is returned as discussed in the previous section. However, as seen in Figure 4c, (Supplementary Material, Appendix F ) this disparity becomes hugely significant on higher dimensionality data due to the increased size summary retained by sampling, further justifying our approach of pruning rows at every stage. While Identity appears to have fast query time, this is due to the summary being smaller. Although it may seem that for smaller summaries more local bases need to be computed and this time could prohibitively increase over the stream, Figure 3c demonstrates that even using small blocks does not cause the overall time (to process the stream and pro-\nduce an approximate query) to increase too much. Hence, an approximation can be obtained which is highly accurate, and in total time faster than the brute force solver.\nExperimental Summary. While it might seem attractive not to perform any conditioning on the matrix and just pick heavy rows, our experiments show that this strategy is not effective in practice, and delivers poor accuracy. Although a simple sample of randomly chosen rows can be easily maintained, this appears less useful due to the increased time costs associated with larger summaries when conditioning methods output a similar estimate in less time over the entire stream. As the `\u221e-regression problems depend only on a few rows of the data there are cases when uniform sampling can perform well: if many of the critical rows look similar then there is a chance that uniform sampling will select some examples. In this case, the leverage of the important direction is divided across the repetitions, and so it is harder to ensure that desired direction is identified. Despite this potential drawback we have shown that both Orth and SPC3 can be used to find accurate summaries which perform robustly across each of the measures we have tested. It appears that SPC3 performs comparably\nto Orth; both are relatively quick to compute and admit accurate summaries in similar space. In particular, both conditioning methods return summaries which are a fraction of the space budget and hence highly sublinear in the input size, which give accurate approximations and are robust to the concatenation of new rows. All of these factors make the conditioning method fast in practice to both find the important rows in the data and then compute the reduced regression problem with high accuracy.\nDue to the problems in constructing summaries which can be used to solve regression quickly and accurately when using random sampling or no transformation, our methods are shown to be efficient and accurate alternatives. Our approach is vindicated both theoretically and practically: this is most clear in the U.S. Census dataset where small error can be achieved using a summary roughly 2% the size of the data. This also results in an overall speedup as solving the optimization on the reduced set is much faster than solving on the full problem. Such significant savings show that this general approach can be useful in large-scale applications."}, {"heading": "Acknowledgements", "text": "The work of G. Cormode and C. Dickens is supported by European Research Council grant ERC-2014-CoG 647557 and The Alan Turing Institute under the EPSRC grant EP/N510129/1. D. Woodruff would like to acknowledge the support by the National Science Foundation under Grant No. CCF-1815840."}], "year": 2018, "references": [{"title": "The fast cauchy transform and faster robust linear regression", "authors": ["K.L. Clarkson", "P. Drineas", "M. Magdon-Ismail", "M.W. Mahoney", "X. Meng", "D.P. Woodruff"], "venue": "In Proc. of the 24-th Annual SODA,", "year": 2013}, {"title": "`p row sampling by lewis weights", "authors": ["Cohen", "Michael B", "Peng", "Richard"], "venue": "In Proceedings of the forty-seventh annual ACM symposium on Theory of computing,", "year": 2015}, {"title": "Sampling algorithms and coresets for lp regression", "authors": ["A. Dasgupta", "P. Drineas", "B. Harb", "R. Kumar", "M.W. Mahoney"], "venue": "In Proc. of the 19th Annual SODA,", "year": 2008}, {"title": "On the complexity of processing massive, unordered, distributed data", "authors": ["Feldman", "Jon", "S. Muthukrishnan", "Sidiropoulos", "Anastasios", "Stein", "Cliff", "Svitkina", "Zoya"], "venue": "Technical Report CoRR abs/cs/0611108, ArXiV,", "year": 2006}, {"title": "Efficient Frequent Directions Algorithm for Sparse Matrices", "authors": ["M. Ghashami", "E. Liberty", "J.M. Phillips"], "venue": "ArXiv e-prints,", "year": 2016}, {"title": "Relative errors for deterministic low-rank matrix approximations", "authors": ["Ghashami", "Mina", "Phillips", "Jeff M"], "venue": "In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms,", "year": 2014}, {"title": "Frequent directions: Simple and deterministic matrix sketching", "authors": ["Ghashami", "Mina", "Liberty", "Edo", "Phillips", "Jeff M", "Woodruff", "David P"], "venue": "SIAM J. Comput.,", "year": 2016}, {"title": "Aspects of linear regression estimation under the criterion of minimizing the maximum absolute residual", "authors": ["Hand", "Michael Lawrence"], "year": 1978}, {"title": "On randomized one-round communication complexity", "authors": ["Kremer", "Ilan", "Nisan", "Noam", "Ron", "Dana"], "venue": "Computational Complexity,", "year": 1999}, {"title": "Communication Complexity", "authors": ["E. Kushilevitz", "N. Nisan"], "year": 1997}, {"title": "Simple and deterministic matrix sketching", "authors": ["Liberty", "Edo"], "venue": "In The 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "year": 2013}, {"title": "Low-distortion subspace embeddings in input-sparsity time and applications to robust linear regression", "authors": ["Meng", "Xiangrui", "Mahoney", "Michael W"], "venue": "In Symposium on Theory of Computing Conference,", "year": 2013}, {"title": "Fast approximate l\u221e minimization: speeding up robust regression", "authors": ["Shen", "Fumin", "Chunhua", "Hill", "Rhys", "van den Hengel", "Anton", "Tang", "Zhenmin"], "venue": "Computational Statistics & Data Analysis,", "year": 2014}, {"title": "Low rank approximation with entrywise `1-norm error", "authors": ["Z. Song", "D. Woodruff", "P. Zhong"], "venue": "In STOC,", "year": 2017}, {"title": "Sketching as a tool for numerical linear algebra", "authors": ["Woodruff", "David"], "venue": "Foundations and Trends in Theoretical Computer Science,", "year": 2014}, {"title": "Subspace embeddings and lp-regression using exponential random variables", "authors": ["Woodruff", "David", "Zhang", "Qin"], "venue": "JMLR: Workshop and Conference Proceedings,", "year": 2013}, {"title": "Quantile regression for large-scale applications", "authors": ["J. Yang", "X. Meng", "M.W. Mahoney"], "venue": "Proc. of the 30th ICML Conference, JMLR W&CP,", "year": 2013}], "id": "SP:4ab375d9177647e812306e7cb8d2234f717457c1", "authors": [{"name": "Graham Cormode", "affiliations": []}, {"name": "Charlie Dickens", "affiliations": []}, {"name": "David P. Woodruff", "affiliations": []}], "abstractText": "Work on approximate linear algebra has led to efficient distributed and streaming algorithms for problems such as approximate matrix multiplication, low rank approximation, and regression, primarily for the Euclidean norm `2. We study other `p norms, which are more robust for p < 2, and can be used to find outliers for p > 2. Unlike previous algorithms for such norms, we give algorithms that are (1) deterministic, (2) work simultaneously for every p \u2265 1, including p =\u221e, and (3) can be implemented in both distributed and streaming environments. We apply our results to `p-regression, entrywise `1-low rank approximation, and approximate matrix multiplication.", "title": "Leveraging Well-Conditioned Bases: Streaming and Distributed Summaries in Minkowski p-Norms"}