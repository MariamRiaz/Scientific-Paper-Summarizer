0	26	In such pro-drop languages as Japanese, Chinese and Italian, pronouns are frequently omitted in text.
1	20	For example, the subject of uketa (suffered) is unrealized in the following Japanese example (1): (1) sono-houkokusho-wa seifui-ga the report-TOP governmenti-SUBJ jouyaku-o teiketsushi (ϕi-ga) keizaitekini treaty-OBJ make iti-SUBJ economically higai-o uke-ta koto-o shitekishi-ta damage-OBJ suffer-PAST COMP point out-PAST The report pointed out that the governmenti agreed to a treaty and (iti) suffered economically.
2	22	The omitted argument is called a zero anaphor, which is represented using ϕ.
5	29	Identifying zero anaphoric relations is an essential task in developing such accurate NLP applications as information extraction and machine translation for pro-drop languages.
6	14	For example, in Japanese, 60% of subjects in newspaper articles are unrealized as zero anaphors (Iida et al., 2007).
7	47	This paper proposes a method for intra-sentential subject zero anaphora resolution, in which a zero anaphor and its antecedent appear in the same sentence and the zero anaphor must be a subject of a predicate, for Japanese.
8	38	We target subject zero anaphors because they represent 85% of the intrasentential zero anaphora in our data set (example (1) is such a case).
14	16	These methods use the constraints among possible zero anaphoric relations, such as “if a candidate antecedent is identified as the antecedent of a subject zero anaphor of a predicate, the candidate cannot be referred to by the object zero anaphor of the same predicate”, and determine an optimal set of zero anaphoric relations in an entire sentence while satisfying such constraints, using such optimization techniques as sentence-wise global learning (Ouchi et al., 2015) and integer linear programming (Iida and Poesio, 2011).
17	64	In our setting, for example, it actually obtained a precision of only 0.61, and even after attempting to obtain more reliable zero anaphoric relations by several modifications, we could only achieve 0.80 precision at extremely low recall levels (<0.01).
20	18	In our proposed method, we use a Multi-column Convolutional Neural Network (MCNN) (Cireşan et al., 2012), which is a variant of a Convolutional Neural Network (CNN) (LeCun et al., 1998).
21	32	AnMCNN has several independent columns, each of which has its own convolutional and pooling layers.
23	47	In this work, motivated by Centering Theory (Grosz et al., 1995) and other previous works, we exploit as distinct columns the word sequences obtained from the surface word sequence and the dependency tree of a target sentence in our MCNN.
24	30	Although the existing works also exploited such word sequences, they used only particular types of information from them as features based on the researchers’ linguistic insights.
59	16	In Step 1, we extract set of pairs ⟨predi, candi⟩ in which candidate antecedent candi is paired with predicate predi.
65	20	Note that zero anaphoric phenomena can be divided into two different referential phenomena: anaphoric (i.e., an antecedent precedes its zero anaphor) and cataphoric (i.e., a zero anaphor precedes its antecedent) cases.
67	24	Our MCNN simultaneously uses four column sets, as illustrated in Figure 1.
69	15	BASE The first column set consists of one column, which stores the word vectors of the bunsetsu phrases1 including either candi or predi.
73	30	We call this column set the SURFSEQ column set.
75	70	We extracted four partial dependency trees from the entire dependency tree of a target sentence: (a) the dependency path between predi and candi, (b) the sub-trees that depend on predi, (c) the sub-trees on which candi depends and (d) the remaining subtrees, which are illustrated in Figure 2.
81	23	Among the four column sets, the SURFSEQ column set was designed to introduce the clues based on Centering Theory, in which the antecedent for a given zero anaphor can basically be identified by the recency and saliency properties of a candidate antecedent.
122	23	The documents in the corpus were divided into five subsets, three of which were used as a training data set, one as a development data set, and one as a testing data set.
136	20	The first baseline is a single-column convolutional neural network in which the column includes the entire surface word sequence of a sentence.
137	28	To give the positions of predi and candi to the network, we concatenated to each word vector an additional 2-dimensional vector, where the first element is set to one if the corresponding word is predi, the second element is set to 1 if the corresponding word is candi, and otherwise they are set to 0.
139	16	The remaining two baselines are Ouchi et al. (2015)’s global optimization method and Iida et al. (2015)’s method based on subject sharing recognition.
152	43	This is because the zero anaphoric relations are exclusive; a zero anaphor does not refer to more than one antecedent.
161	33	Particularly, it got high precision in a wide range of recall levels (e.g., around 0.8 in precision at 0.25 in recall and around 0.7 in precision at 0.4 in recall), while the precision obtained by Ouchi’s method at 0.25 in recall was just around 0.65.
168	23	The results in Table 3 show that our MCNN-based method achieved better average precision than the single-column CNN baseline except the method that uses only the BASE column set for the cataphoric case.
169	27	The results also demonstrate that each column set consistently contributes to improving the average precision for both the anaphoric and cataphoric cases.
174	30	Our experimental results show that the proposed method achieved better precision than the strong baselines in a wide range of recall levels.
175	42	As future work, we plan to use our MCNN architecture for inter-sentential zero anaphora resolution and develop highly accurate NLP applications using our intra-sentential subject zero anaphora resolution method.
