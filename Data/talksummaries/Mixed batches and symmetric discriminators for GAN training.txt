0	5	Estimating generative models from unlabeled data is one of the challenges in unsupervised learning.
6	6	A discriminator network is trained to distinguish between generated samples and true samples in the observation space.
7	39	The generator, on the other hand, is trained to fool the discriminator.
8	23	In an idealized setting with unbounded capacity of both networks and infinite training data, the generator should converge to the distribution from which the training data has been sampled.
10	19	Consequently, it cannot directly detect discrepancies between the distribution of generated samples and global statistics of the training distribution, such as its moments or quantiles.
11	190	For instance, if the generator models a restricted part of the support of the target distribution very well, this can fool the discriminator at the level of individual samples, a phenomenon known as mode dropping.
12	42	In such a case there is little incentive for the generator to model other parts of the support of the target distribution.
13	4	A more thorough explanation of this effect can be found in (Salimans et al., 2016).
14	25	In order to access global distributional statistics, imagine a discriminator that could somehow take full probability distributions as its input.
17	7	The discriminator can compute statistics on those batches and detect discrepancies between the two distributions.
18	14	For instance, if a large batch exhibits only one mode from a multimodal distribution, the discriminator would notice the discrepancy right away.
19	3	Even though a single batch may not encompass all modes of the distribution, it will still convey more information about missing modes than an individual example.
20	131	Training the discriminator to discriminate “pure” batches with only real or only synthetic samples makes its task too easy, as a single bad sample reveals the whole batch as synthetic.
21	120	Instead, we introduce a “mixed” batch discrimination task in which the discriminator needs to predict the ratio of real samples in a batch.
22	3	1 This use of batches differs from traditional minibatch learning.
23	56	The batch is not used as a computational trick to increase parallelism, but as an approximate distribution, on which to compute global statistics.
24	145	A naive way of doing so would be to concatenate the samples in the batch, feeding the discriminator a single tensor containing all the samples.
25	25	However, this is parameterhungry, and the computed statistics are not automatically invariant to the order of samples in the batch.
26	30	To compute functions that depend on the samples only through their distribution, it is necessary to restrict the class of discriminator networks to permutation-invariant functions of the batch.
27	169	For this, we adapt and extend an architecture from McGregor (2007) to compute symmetric functions of the input.
28	31	We show this can be done with minimal modification to existing architectures, at a negligible computational overhead w.r.t.
29	70	In summary, our contributions are the following: • Naively training the discriminator to discriminate “pure” batches with only real or only synthetic samples makes its task way too easy.
34	69	• We apply these insights to GANs, with good experimental results, both qualitatively and quantitatively.
35	9	We believe that discriminating between distributions at the batch level provides an equally principled alternative to approaches to GANs based on duality formulas (Nowozin et al., 2016; Gulrajani et al., 2017; Arjovsky et al., 2017).
59	9	Using a batch of samples rather than individual samples as input to the discriminator can provide global statistics about the distributions of interest.
61	7	Adversarial learning (Goodfellow et al., 2014) can easily be extended to the batch discrimination case.
63	50	, xB)] + (1) Ez1,...,zB∼Z [log(1−D(G(z1), .
64	38	, G(zB)))] with D the empirical distribution over data, Z a distribution over the latent variable that is the input of the generator, G a pointwise generator and D a batch discriminator.1 This leads to a learning procedure similar to the usual GAN algorithm, except that the loss encourages the discriminator to output 1 when faced with an entire batch of real data, and 0 when faced with an entire batch of generated data.
