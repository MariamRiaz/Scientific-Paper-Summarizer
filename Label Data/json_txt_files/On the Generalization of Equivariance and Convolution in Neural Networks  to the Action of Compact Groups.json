{"sections": [{"text": "ar X\niv :1\n80 2.\n03 69\n0v 3\n[ st\nat .M\nL ]\n1 0\nN ov\n2 01\ntremely successful in the image recognition domain because they ensure equivariance to translations. There have been many recent attempts to generalize this framework to other domains, including graphs and data lying on manifolds. In this paper we give a rigorous, theoretical treatment of convolution and equivariance in neural networks with respect to not just translations, but the action of any compact group. Our main result is to prove that (given some natural constraints) convolutional structure is not just a sufficient, but also a necessary condition for equivariance to the action of a compact group. Our exposition makes use of concepts from representation theory and noncommutative harmonic analysis and derives new generalized convolution formulae."}, {"heading": "1. Introduction", "text": "One of the most successful neural network architectures is convolutional neural networks (CNNs) (LeCun et al., 1989). In the image recognition domain, where CNNs were originally conceived, convolution plays two crucial roles. First, it ensures that in any given layer, exactly the same filters are applied to each part of the image. Consequently, if the input image is translated, the activations of the network in each layer will translate the same way. This property is called equivariance (Cohen & Welling, 2016). Second, in conjunction with pooling, convolution ensures that each neuron\u2019s effective receptive field is a spatially contiguous domain. As we move higher in the network, these domains generally get larger, allowing the CNN to capture structure in images at multiple different scales.\n1Departments of Statistics and Computer Science, The University of Chicago 2Toyota Technological Institute at Chicago. Correspondence to: Risi Kondor <risi@cs.uchicago.edu>, Shubhendu Trivedi <shubhendu@ttic.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nRecently, there has been considerable interest in extending neural networks to more exotic types of data, such as graphs or functions on manifolds (Niepert et al., 2016; Defferrard et al., 2016; Duvenaud et al., 2015; Li et al., 2016; Cohen et al., 2018; Monti et al., 2017; Masci et al., 2015). In these domains, equivariance and multiscale structure are just as important as for images, but finding the right notion of convolution is not obvious.\nOn the other hand, mathematics does offer a sweeping generalization of convolution tied in deeply with some fundamental ideas of abstract algebra: if G is a compact group and f and g are two functionsG\u2192 C, then the convolution of f with g is defined\n(f \u2217 g)(u) = \u222b\nG\nf(uv\u22121) g(v) d\u00b5(v). (1)\nNote the striking similarity of this formula to the ordinary notion of convolution, except that in the argument of f , u \u2212 v has been replaced by the group operation uv\u22121, and integration is with respect to the Haar measure, \u00b5.\nThe goal of this paper is to relate (1) to the various looser notions of convolution used in the neural networks literature, and show that several practical neural networks implicitly already take advantange of the above group theoretic concept of convolution. In particular, we prove the following theorem (paraphrased here for simplicity).\nTheorem 1. A feed forward neural network N is equivariant to the action of a compact group G on its inputs if and only if each layer of N implements a generalized form of convolution derived from (1).\nTo the best of our knowledge, this is the first time that the connection between equivariance and convolution in neural networks has been stated at this level of generality. One of the technical challenges in proving our theorem is that the activations in each layer of a neural net correspond to functions on a sequence of spaces acted on byG (called homogeneous spaces or quotient spaces) rather than functions on G itself. This necessitates a discussion of group convolution that is rather more thoroughgoing than is customary in pure algebra.\nThis paper does not present any new algorithms or neural\nnetwork architectures. Rather, its goal is to provide the language for thinking about generalized notions of equivariance and convolution in neural networks, and thereby facilitate the development of future architectures for data with non-trivial symmetries. To avoid interruptions in the flow of our exposition, we first present the theory in its abstract form, and then illustrate it with examples in Section 6. For better understanding, the reader might choose to skip back and forth between these sections. One work that is close in spirit to the present paper but only considers discrete groups is (Ravanbakhsh et al., 2017)."}, {"heading": "2. Notation", "text": "In the following [a] will denote the set {1, 2, . . . , a}. Given a set X and a vector space V , LV (X ) will denote the space of functions {f : X \u2192 V }."}, {"heading": "3. Equivariance in neural networks", "text": "A feed-forward neural network consists of some number of \u201cneurons\u201d arranged in L+1 distinct layers. Layer \u2113 = 0 is the input layer, where data is presented to the network, while layer \u2113=L is where the output is read out. Each neuron n\u2113x (denoting neuron number x in layer \u2113) has an activation f \u2113x. For the input layer, the activations come directly from the data, whereas in higher layers they are computed via a simple function of the activations of the previous layer, such as f \u2113x = \u03c3 ( b\u2113x + \u2211 y w \u2113 x,y f \u2113\u22121 y ) . (2) Here, the {b\u2113x} bias terms and the {w\u2113x,y} weights are the network\u2019s learnable parameters, while \u03c3 is a fixed nonlinear function, such as the ReLU function \u03c3(z) =max(0, z). In the simplest case, each f \u2113x is a scalar, but, in the second half of the paper we consider neural networks with more general, vector or tensor valued activations.\nFor the purposes of the following discussion it is actually helpful to take a slightly more abstract view, and, instead of focusing on the individual activations, consider the activations in any given layer collectively as a function f \u2113 : X\u2113 \u2192 V\u2113, where X\u2113 is a set indexing the neurons and V\u2113 is a vector space. Omitting the bias terms in (2) for simplicity, each layer \u2113 = 1, 2, . . . , L can then just be thought of as implementing a linear transformation \u03c6\u2113 : LV\u2113\u22121(X\u2113\u22121) \u2192 LV\u2113(X\u2113) followed by the pointwise nonlinearity \u03c3. Our operational definition of neural networks for the rest of this paper will be as follows.\nDefinition 1. Let X0, . . . ,XL be a sequence of index sets, V0, . . . , VL vector spaces, \u03c61, . . . , \u03c6L linear maps \u03c6\u2113 : LV\u2113\u22121(X\u2113\u22121) \u2212\u2192 LV\u2113(X\u2113), and \u03c3\u2113 : V\u2113 \u2192 V\u2113 appropriate pointwise nonlinearities, such as the ReLU operator. The corresponding multilayer feed-forward neural network (MFF-NN) is then a\nsequence of maps f0 7\u2192 f1 7\u2192 f2 7\u2192 . . . 7\u2192 fL, where f\u2113(x) = \u03c3\u2113(\u03c6\u2113(f\u2113\u22121)(x)).\nIf we are interested in constructing a neural net for recognizing m \u00d7 m pixel images, it is tempting to take X0 = [m]\u00d7 [m] and define X1, . . . ,XL similarly. However, again for notational simplicity, we extend each of these index sets to the entire integer plane Z2, and simply assume that outside of the square region [m]\u00d7 [m], f0(x1, x2) = 0. A traditional convolutional neural network (CNN) is a network of this type where the \u03c6\u2113 functions are constrained to have the special form\n\u03c6\u2113(f\u2113\u22121)(x1, x2) = w\u2211\nu1=1\nw\u2211\nu2=1\nf\u2113\u22121(x1\u2212u1, x2\u2212u2) \u03c7\u2113(u1, u2). (3)\nThe above function is known as the discrete convolution of f \u2113\u22121 with the filter \u03c7, and is usually denoted f\u2113\u22121 \u2217 \u03c7\u2113. In most CNNs the width w of the filters is quite small, on the order of 3 \u223c 10, while the number of layers can be as small as 3 or as large as a few dozen.\nSome of the key features of CNNs are immediately apparent from the convolution formula (3):\n1. The number of parameters in CNNs is much smaller\nthan in general (fully connected) feed-forward networks, since we only have to learn the w2 numbers defining the \u03c7\u2113 filters rather than O((m\n2)2) weights. 2. (3) applies the same filter to every part of the image.\nTherefore, if the networks learns to recognize a certain feature, e.g., eyes, in one part of the image, then it will be able to do so in any other part as well. 3. Equivalently to the above, if the input image is trans-\nlated by any vector (t1, t2) (i.e., f 0\u2032(x1, x2) = f 0(x1\u2212 t1, x2 \u2212 t2), then all higher layers will translate in exactly the same way. This property is called equivariance (sometimes covariance) to translations.\nThe goal of the present paper is to understand the mathematical generalization of the above properties to other domains, such as graphs, manifolds, and so on."}, {"heading": "3.1. Group actions", "text": "The jumping off point to our analysis is the observation that the above is a special case of the following scenario.\n1. We have a set X and a function f : X \u2192 C. 2. We have a group G acting on X . This means that each g \u2208G has a corresponding transformation Tg : X \u2192 X , and for any g1, g2 \u2208G, Tg2g1 = Tg2 \u25e6 Tg1 . 3. The action of G on X extends to functions on X by Tg : f 7\u2192 f \u2032 f \u2032(Tg(x)) = f(x).\nIn the case of translation invariant image recognition, X = Z2, G is the group of integer translations, which is isomorphic to Z2 (note that this is a very special case, in general\nX and G are different objects), the action is\nT(t1,t2)(x1, x2) = (x1+ t1, x2+ t2) (t1, t2)\u2208Z2,\nand the corresponding (induced) action on functions is\nT : f 7\u2192 f \u2032 f \u2032(x1, x2) = f(x1\u2212 t1, x2\u2212 t2).\nWe give several other (more interesting) examples of group actions in Section 6, but for now continue with our abstract development. Also note that to simplify notation, in the following, where this does not cause confusion, we will simply write group actions as x 7\u2192 g(x) rather than the more cumbersome x 7\u2192 Tg(x). Most of the actions considered in this paper have the property that taking any x0 \u2208X , any other x\u2208X can be reached by the action of some g \u2208G, i.e., x= g(x0). This property is called transitivity, and if the action of G on X is transitive, we say that X is a homogeneous space of G."}, {"heading": "3.2. Equivariance", "text": "Equivariance is a concept that applies very broadly, whenever we have a group acting on a pair of spaces and there is a map from functions on one to functions on the other.\nDefinition 2. LetG be a group and X1,X2 be two sets with correspondingG-actions\nTg : X1 \u2192 X1, T \u2032g : X2 \u2192 X2.\nLet V1 and V2 be vector spaces, and T and T \u2032 be the induced actions of G on LV1(X1) and LV2(X2). We say that a (linear or non-linear) map \u03c6 : LV1(X1) \u2192 LV2(X2) is equivariant with the action of G (or G\u2013equivariant for short) if\n\u03c6(Tg(f)) = T \u2032 g(\u03c6(f)) \u2200f \u2208LV1(X1)\nfor any group element g \u2208G.\nEquivariance is represented graphically by a so-called commutative diagram, in our case\nLV1(X1) Tg //\n\u03c6\nLV1(X1)\n\u03c6\nLV2(X2) T \u2032 g // LV2(X2)\nWe are finally in a position to define the objects that we study in this paper, namely generalized equivariant neural networks.\nDefinition 3. Let N be a feed-forward neural network as defined in Definition 1, andG be a group that acts on each index spaceX0, . . . ,XL. Let T0,T1, . . . ,TL be the corresponding actions on LV0(X0), . . . , LVL(XL). We say that N is a G\u2013equivariant feed-forward network if, when the inputs are transformed f0 7\u2192 T0g(f0) (for any g \u2208 G), the activations of the other layers correspondingly transform as f\u2113 7\u2192 T\u2113g(f\u2113).\nIt is important to note how general the above framework is. In particular, we have not said whether G and X0, . . . ,XL are discrete or continuous. In any actual implementation of a neural network, the index sets would of course be finite. However, it has been observed before that in certain cases, specifically whenX0 is an object such as the sphere or other manifold which does not have a discretization that fully takes into account its symmetries, it is easier to describe the situation in terms of abstract \u201ccontinuous\u201d neural networks than seemingly simpler discrete ones (Cohen et al., 2018).\nNote also that invariance is a special case of equivariance, where Tg = id for all g. In fact, this is another major reason why equivariant architectures are so prevalent in the literature: any equivariant network can be turned into a G\u2013 invariant network simply by tacking on an extra layer that is equivariant in this degenerate sense (in practice, this often means either averaging or creating a histogram of the activations of the last layer). Nowhere is this more important than in graph learning, where it is a hard constraint that whatever representation is learnt by a neural network, it must be invariant to reordering the vertices. Today\u2019s state of the art solution to this problem are message passing networks (Gilmer et al., 2017), whose invariance behavior we discuss in section 6. Another architecture that achieves invariance by stacking equivariant layers followed by a final invariant one is that of scattering networks (Mallat, 2012)."}, {"heading": "4. Convolution on groups and quotient spaces", "text": "According to its usual definition in signal processing, the convolution of two functions f, g : R \u2192 R is\n(f \u2217 g)(x) = \u222b f(x\u2212y) g(y) dy. (4)\nIntuitively, we can think of f as a template and g as a modulating function (or the other way round, since convolution on R is commutative): we get f \u2217 g by a placing a \u201ccopy\u201d of f at each point on the x axis, but scaled by the value of g at that point, and superimposing the results. The discrete variant of (4) for f, g : Z \u2192 R is of course\n(f \u2217 g)(x) = \u2211\ny\u2208Z\nf(x\u2212 y) g(y), (5)\nand both the above formulae have natural generalizations to higher dimensions. In particular, (3) is just the two dimensional version of (5) with a limited width filter.\nWhat we are interested in for this paper, however, is the much broader generalization of convolution to the case when f and g are functions on a compact group G. As mentioned in the Introduction, this takes the form\n(f \u2217 g)(u) = \u222b\nG\nf(uv\u22121) g(v) d\u00b5(v). (6)\nNote that (6) only differs from (4) in that x\u2212y is replaced by the group operation uv\u22121, which is not surprising, since the group operation on R in fact is exactly (x, y) 7\u2192 x+y, and the \u201cinverse\u201d of y in the group sense is \u2212y. Furthermore, the Haar measure \u00b5makes an appearance. At this point, the main reason that we restrict ourselves to compact groups is because this guarantees that \u00b5 is essentially unique1. The discrete counterpart of (6) for countable (including finite) groups is\n(f \u2217 g)(u) = \u2211\nv\u2208G\nf(uv\u22121) g(v). (7)\nAll these definitions are standard and have deep connections to the algebraic properties of groups. In contrast, the various extensions of convolution to homogeneous spaces that we derive below are not often discussed in pure algebra."}, {"heading": "4.1. Convolution on quotient spaces", "text": "The major complication in neural networks is that X0, . . . ,XL (which are the spaces that the f0, . . . , fL activations are defined on) are homogeneous spaces of G, rather than being G itself. Fortunately, the strong connection between the structure of groups and their homogeneous spaces (see boxed text) allows generalizing convolution to this case as well. Note that from now on, to keep the exposition as simple as possible, we present our results assuming thatG is countable (or finite). The generalization to continuous groups is straightforward. We also allow all our functions to be complex valued, because representation theory itself, which is the workhorse behind our results, is easiest to formulate over C.\nDefinition 4. Let G be a finite or countable group, X and Y be (left or right) quotient spaces of G, f : X \u2192 C, and g : Y \u2192 C. We then define the convolution of f with g as\n(f \u2217 g)(u) = \u2211\nv\u2208G\nf\u2191G(uv\u22121) g\u2191G(v), u\u2208G. (8)\n1Non-compact groups would also cause trouble because their representation theory is much more involved. R2, which is the group behind traditional CCNs, is of course not compact. The reason that it is still amenable to our analysis (with small modifications) is that it belongs to one of a handful of families of exceptional non-compact groups that are easy to handle.\nESSENTIAL DEFINITIONS FOR QUOTIENT SPACES\nCertain connections between the structure of a group G and its homogeneous space X are crucial for our exposition. First, by definition, fixing an \u201corigin\u201d x0 \u2208X , any x\u2208X can be reached as x= g(x0) for some g \u2208G. This allows us to \u201cindex\u201d elements of X by elements of G. Since we use this mechanism so often, we introduce the shorthand [g ]X = g(x0), which hides the dependence on the (arbitrary) choice of x0. Second, elementary group theory tells us that the set of group elements that fix x0 actually form a subgroup H . By further elementary results (see Appendix), the set of group elements that map x0 7\u2192 x is a so-called left coset gH := {gh | h\u2208H }. The set of all such cosets forms the (left) quotient space G/H . Therefore, X can be identified with G/H . Now for each gH coset we may pick a coset representative g\u2032 \u2208 gH , and let x denote the representative of the coset of group elements that map x0 to x. Note that while the map g 7\u2192 [g ]G/H is well defined, the map x 7\u2192 x going in the opposite direction is more arbitary, since it depends on the choice of coset representatives. The right quotient space H\\G is similarly defined as the space of right cosets Hg := {hg | h\u2208H }. Furthermore, if K is another subgroup of G, we can talk about double cosets HgK = {hgk | h\u2208H, k \u2208K } and the corresponding space H\\G/K . Given f :G\u2192C, we define its projection to X =G/H\nf\u2193X : X \u2192 C f\u2193X (x) = 1 |H | \u2211\ng\u2208xH\nf(g).\nConversely, given f : X \u2192 C, we define the lifting of f to G f\u2191G : G\u2192 C f\u2191G(g) = f([g ]X ). Projection and lifting to/from right quotient spaces and double quotient spaces is defined analogously.\nThis definition includes X =G or Y =G as special cases, since any group is a quotient space of itself with respect to the trivial subgroup H = {e}.\nDefinition 4 hides the facts that depending on the choice of X and Y: (a) the summation might only have to extend over a quotient space of G rather than the entire group, (b) the result f \u2217g might have symmetries that effectively make it a function on a quotient space rather than G itself (this is exactly what the case will be in generalized convolutional networks). Therefore we now discuss three special cases.\nCASE I: X =G AND Y =G/H When f : G\u2192 C but g : G/H \u2192 C for some subgroup H of G, (8) reduces to\n(f \u2217 g)(u) = \u2211\nv\u2208G\nf(uv\u22121) g\u2191G(v).\nPlugging u\u2032 = uh into this formula (for any h \u2208 H) and changing the variable of summation to w := vh\u22121 gives\n(f \u2217 g)(u\u2032) = \u2211\nv\u2208G\nf(uhv\u22121) g\u2191G(v)\n= \u2211\nw\u2208G\nf(uw\u22121) g\u2191G(wh).\nHowever, since w and wh are in the same left H\u2013coset, g\u2191G(wh) = g\u2191G(w), so (f \u2217 g)(u\u2032) = (f \u2217 g)(u), i.e., f \u2217 g is constant on left H\u2013cosets. This makes it natural to interpret f \u2217 g as a function on G/H rather than the full group. Thus, we have the following definition.\nIf f : G\u2192C, and g : G/H\u2192C then f \u2217g : G/H \u2192 C with\n(f \u2217 g)(x) = \u2211\nv\u2208G\nf(xv\u22121) g([v ]G/H ). (9)\nCASE II: X =G/H AND Y =H\\G When f : G/H \u2192 C, but g : G\u2192 C, (8) reduces to\n(f \u2217 g)(u) = \u2211\nv\u2208G\nf\u2191G(uv\u22121) g(v). (10)\nThis time it is not f \u2217 g, but g that shows a spurious symmetry. Letting v\u2032 = hv (for any h\u2208H), by the right H\u2013invariance of f\u2191G, f\u2191G(uv\u2032\u22121) = f\u2191G(uv\u22121h\u22121) = f\u2191G(uv). Considering that any v can be uniquely written as v = hy, where y is the representative of one of its cosets, while h \u2208 H , we get that (10) factorizes in the form\n(f \u2217 g)(u) = \u2211\ny\u2208H\\G\nf\u2191G(uy\u22121) \u2211\nh\u2208H\ng(hy)\n= \u2211\ny\u2208H\\G\nf\u2191G(uy\u22121) g\u0303(y),\nwhere g\u0303(y) := \u2211\nh\u2208H g(hy). In other words, without loss of generality we can take g to be a function on H\\G rather than the full group.\nIf f : G/H \u2192 C, and g : H\\G\u2192 C, then f\u2217g : G\u2192 C with\n(f \u2217 g)(u) = |H | \u2211\ny\u2208H\\G\nf([uy\u22121 ]G/H) g(y). (11)\nCASE III: X =G/H AND Y =H\\G/K Finally, we consider the case when f : G/H \u2192 C and g : G/K \u2192 C for two subgroups H,K of G, which might or might not be the same. This combines features of the above two cases in the sense that, similarly to Case I, setting u\u2032 = uk for any k \u2208K and letting w = vk\u22121,\n(f \u2217 g)(u\u2032) = \u2211\nv\u2208G\nf\u2191G(u\u2032v\u22121) g\u2191G(v) =\n= \u2211\nv\u2208G\nf\u2191G(ukv\u22121) g\u2191G(v) = \u2211\nw\u2208G\nf\u2191G(uw\u22121) g\u2191G(wk)\n= \u2211\nw\u2208G\nf\u2191G(uw\u22121) g\u2191G(w) = (f \u2217 g)(u),\nshowing that f \u2217 g is right K\u2013invariant, and therefore can be regarded as a function G/K \u2192 C. At the same time, similarly to (10), letting v = hy,\n(f \u2217 g)(u) = \u2211\ny\u2208H\\G\nf\u2191G(uy\u22121) \u2211\nh\u2208H\ng\u2191G(hy)\n= \u2211\ny\u2208H\\G\nf\u2191G(uy\u22121) g\u0303(y),\nwhere g\u0303(y) := \u2211\nh\u2208H g(hy), which is left H\u2013invariant. Therefore, without loss of generality, we can take g to be a functionH\\G/K \u2192 C.\nIf f : G/H \u2192 C, and g : H\\G/K \u2192 C then we define the convolution of f with g as f \u2217 g : G/K \u2192 C with\n(f \u2217 g)(x) = |H | \u2211\ny\u2208H\\G\nf([xy\u22121 ]X ) g([y ]H\\G/K).\n(12)\nSince f 7\u2192 f \u2217 g is a map from one homogeneous space, X = G/H , to another homogeneous space, Y = H/K , it is this last defintion that will be of most relevance to us in constructing neural networks."}, {"heading": "4.2. Relationship to Fourier analysis", "text": "The nature of convolution on homogeneous spaces is further explicated by considering its form in Fourier space (see (Terras, 1999)). Recall that the Fourier transform of a function f on a countable group is defined\nf\u0302(\u03c1i) = \u2211\nu\u2208G\nf(u)\u03c1i(u), i = 0, 1, 2, . . . , (13)\nwhere \u03c10, \u03c11, . . . are matrix valued functions called irreducible representations or irreps of G (see the Appendix for details). As expected, the generalization of this to the case when f is a function on G/H , H\\G or H\\G/K is\nf\u0302(\u03c1i) = \u2211\nu\u2208G\n\u03c1i(u)f\u2191G(u), i = 1, 2, . . . .\nAnalogous formulae hold for continuous groups, involving integration with respect to the Haar measure.\nAt first sight it might be surprising that the Fourier transform of a function on a quotient space consists of the same number of matrices of the same sizes as the Fourier transform of a function on G itself, since G/H , H\\G or H\\G/K are smaller objects than G. This puzzle is resolved by the following proposition, which tells us that in the latter cases, the Fourier matrices have characteristic sparsity patterns.\nProposition 1. Let \u03c1 be an irrep of G, and assume that on restriction to H it decomposes into irreps of H in the form \u03c1|H = \u00b51 \u2295 \u00b52 \u2295 . . . \u2295 \u00b5k. Let f\u0302 be the Fourier transform of a function f : G/H \u2192 C. Then [f\u0302(\u03c1)]\u2217,j = 0 unless the block at column j in the decomposition of \u03c1|H is the trivial representation. Similarly, if f : H\\G \u2192 C, then [f\u0302(\u03c1)]i,\u2217 = 0 unless the block of \u03c1|H at row i is the trivial representation. Finally, if f : H\\G/K \u2192 C, then [f\u0302(\u03c1)]i,j = 0 unless the block of \u03c1|H at row i is the trivial representation of H and the block at column j in the decomposition of \u03c1|K is the trivial representation of K .\nSchematically, this proposition implies that in the three different cases, the Fourier matrices have three different forms of sparsity:\nG/K H\\G H\\G/K\nFortuitously, just like in the classical, Euclidean case, convolution also takes on a very nice form in the Fourier domain, even when f or g (or both) are defined on homogeneous spaces.\nProposition 2 (Convolution theorem on groups). Let G be a compact group, H and K subgroups of G, and f, g be complex valued functions on G, G/H , H\\G or H\\G/K . In any combination of these cases,\nf\u0302 \u2217g(\u03c1i) = f\u0302(\u03c1i) g\u0302(\u03c1i) (14)\nfor any given system of irreps RG = {\u03c10, \u03c11, . . .}.\nPlugging in matrices with the appropriate sparsity patterns into (19) now gives us an intuitive way of thinking about Cases I\u2013III above.\nCASE I: X =G AND Y =G/H Mutiplying a column sparse matrix with a dense matrix from the left gives a column sparse matrix with the same\npattern, therefore f \u2217 g is a function on G/H :    \nf\u0302 \u2217 g(\u03c1)\n=\n \n \nf\u0302(\u03c1)\n\u00d7\n \n \ng\u0302\u2191G(\u03c1)\n.\nCASE II: X =G/H AND Y =H\\G Multiplying a column sparse matrix from the right by another matrix picks out the corresponding rows of the second matrix. Therefore, if f is a function on G/H , then w.l.o.g. we can take g to be a function on H\\G.\n \n \nf\u0302 \u2217 g(\u03c1)\n=\n \n \nf\u0302\u2191G(\u03c1)\n\u00d7\n \n \ng\u0302\u2191G(\u03c1)\n.\nCASE III: f : G/H \u2192 C AND g : H\\G/K \u2192 C Finally, if f is a function on G/H , and we want to make f \u2217 g to be a function on G/K , then we should take g : H\\G/K:\n \n \nf\u0302 \u2217 g(\u03c1)\n=\n \n \nf\u0302\u2191G(\u03c1)\n\u00d7\n \n \ng\u0302\u2191G(\u03c1)\n."}, {"heading": "5. Main result: the connection between convolution and equivariance", "text": "We are finally in a position to define the notion of generalized convolutional networks, and state our main result connecting convolutions and equivariance.\nDefinition 5. Let G be a compact group and N an L+1 layer feed-forward network in which the i\u2019th index set is G/Hi for some subgroup Hi of G. We say that N is a G\u2013convolutional neural network (or G-CNN for short) if each of the linear maps \u03c61, . . . , \u03c6L in N is a generalized convolution (see Definition 4) of the form\n\u03c6\u2113(f\u2113\u22121) = f\u2113\u22121 \u2217 \u03c7\u2113\nwith some filter \u03c7\u2113 \u2208LV\u2113\u22121\u00d7V\u2113(H\u2113\u22121\\G/H\u2113).\nTheorem 1. Let G be a compact group and N be an L + 1 layer feed-forward neural network in which the \u2113\u2019th index set is of the form X\u2113 = G/H\u2113, where H\u2113 is some subgroup of G. Then N is equivariant to the action ofG in the sense of Definition 3 if and only if it is a G-CNN.\nProving this theorem in the forward direction is relatively easy and only requires some elementary facts about cosets and group actions.\nProof of Theorem 1 (forward direction). Assume that we translate f\u2113\u22121 by some group element g \u2208G and get f \u2032\u2113\u22121, i.e., f \u2032\u2113\u22121 = T \u2113\u22121 g (f\u2113\u22121), where f \u2032 \u2113\u22121(x) = f\u2113\u22121(g\n\u22121x). Then\n\u03c6\u2113(f \u2032 \u2113\u22121)(u) = (f \u2032 \u2113\u22121 \u2217 \u03c7\u2113)(u)\n= \u2211\nv\u2208G\nf \u2032\u2113\u22121([uv \u22121]X )\u03c7\u2113(v)\n= \u2211\nv\u2208G\nf\u2113\u22121(g \u22121([uv\u22121]X ))\u03c7\u2113(v).\nBy g\u22121([uv\u22121]X ) = [g \u22121uv\u22121]X this is further equal to\n\u2211\nv\u2208G\nf\u2113\u22121([g \u22121uv\u22121]X )\u03c7\u2113(v)\n= (f\u2113\u22121 \u2217 \u03c7\u2113)(g\u22121u) = \u03c6\u2113(f\u2113\u22121)(g\u22121u).\nTherefore, \u03c6\u2113(f\u2113\u22121) is equivariant with f\u2113\u22121. Since \u03c3\u2113 is a pointwise operator, so is f\u2113 = \u03c3\u2113(\u03c6\u2113(f\u2113\u22121)). By induction on \u2113, using the transitivity of equivariance, this implies that every layer of N is equivariant with layer 0. Note that this proof holds not only in the base case, when each f\u2113 is a function X \u2192 C, but also in the more general case when f\u2113 : X\u2113 \u2192 V\u2113 and the filters are \u03c7\u2113 : X\u2113 \u2192 V\u2113\u22121 \u00d7 V\u2113.\nProving the \u201conly if\u201d part of Theorem 1 is more technical, therefore we leave it to the Appendix."}, {"heading": "6. Examples of algebraic convolution in neural networks", "text": "We are not aware of any prior papers that have exposed the above algebraic theory of equivariance and convolution in its full generality. However, there are a few recent publications that implicitly exploit these ideas in specific contexts."}, {"heading": "6.1. Rotation equivariant networks", "text": "In image recognition applications it is a natural goal to achieve equivariance to both translation and rotation. The most common approach is to use CNNs, but with filters that are replicated at a certain number of rotational angles (typically multiples of 90 degrees), connected in such as a way as to achieve a generalization of equivariance called steerability. Steerability also has a group theoretic interpretation, which is most lucidly explained in (Cohen & Welling, 2017).\nThe recent papers (Marcos et al., 2017) and (Worrall et al., 2017) extend these architectures by considering continuous rotations at each point of the visual field. Thus, putting\naside the steerability aspect for now and only considering the behavior of the network at a single point, both these papers deal with the case where G = SO(2) (the two dimensional rotation group) and X is the circle S1. The group SO(2) is commutative, therefore its irreducible representations are one dimensional, and are, in fact, \u03c1j(\u03b8) = e 2\u03c0\u03b9j\u03b8 ,\nwhere \u03b9 = \u221a \u22121. While not calling it a group Fourier transform, Worrall et al. (2017) explicitly expand the local activations in this basis and scale them with weights, which, by virtue of Proposition 2, amounts to convolution on the group, as prescribed by our main theorem.\nThe form of the nonlinearity in (Worrall et al., 2017) is different from that prescribed in Definition 3, which leads to a coupling between the indices of the Fourier components in any path from the input layer to the output layer. This is compensated by what they call their \u201cequivariance condition\u201d, asserting that only Fourier components for which M = \u2211 \u2113 j\u2113 is the same may mix. This restores equivariance in the last layer, but analyzing it group theoretically is beyond the scope of the present paper."}, {"heading": "6.2. Spherical CNNs", "text": "Very close in spirit to our present exposition are the recent papers (Cohen et al., 2018; Kondor et al., 2018), which propose convolutional architectures for recognizing images painted on the sphere, satisfying equivariance with respect to rotations. Thus, in this case, G = SO(3), the group of three dimensional rotations, and X\u2113 is the sphere, S2. The case of rotations acting on the sphere is one of the textbook examples of continuous group actions. In particular, letting x0 be the North pole, we see that two-dimensional rotations in the x\u2013z plane fix x0, therefore, S 2 is identified with the quotient space SO(3)/SO(2). The irreducible representations of SO(3) are given by the so-called Wigner matrices. The \u2113\u2019th irreducible representation is 2\u2113+1 dimensional and of the form\n[\u03c1\u2113(\u03b8, \u03c6, \u03c8)]m,m\u2032 = e \u2212\u03b9m\u2032\u03c6 d\u2113m\u2032,m(\u03b8) e \u2212\u03b9m\u03c8,\nwhere m,m\u2032 \u2208 {\u2212\u2113, . . . , \u2113}, (\u03b8, \u03c6, \u03c8) are the Euler angles of the rotation and the d\u2113m\u2032,m(\u03b8) funcion is related to the spherical harmonics. It is immediately clear that on restriction to SO(2) (corresponding to \u03b8, \u03c6 = 0) only the middle column in each of these matrices reduces to the trivial representation of SO(2), therefore, by Proposition 1, in the case f : SO(3)/SO(2) \u2192 C, only the middle column of each f\u0302(\u03c1\u2113) matrix will be nonzero. In fact, up to constant scaling factors, the entries in that middle column are just the customary spherical harmonic expansion coefficients.\nCohen et al. (2018) explicitly make this connection between spherical harmonics and SO(3) Fourier transforms, and store the activations in terms of this representation. Moreover, just like in the present paper, they define convo-\nlution in terms of the noncommutative convolution theorem (Proposition 2), use pointwise nonlinearities, and prove that the resulting neural network is SO(3)\u2013equivariant. However, they do not prove the converse, i.e., that equivariance implies that the network must be convolutional. To apply the nonlinearity, the algorithm presented in (Cohen et al., 2018) requires repeated forward and backward SO(3) fast Fourier transforms. While this leads to a non-conventional architecture, the discussion echoes our observation that when dealing with continuous symmetries such as rotations, one must generalize to more abstract \u201ccontinuous\u201d neural networks, as afforded by Definition 3."}, {"heading": "6.3. Message passing neural networks", "text": "There has been considerable interest in extending the convolutional network formalism to learning from graphs (Niepert et al., 2016; Defferrard et al., 2016; Duvenaud et al., 2015), and the current consensus for approaching this problem is to use neural networks based on the message passing idea (Gilmer et al., 2017). Let G be a graph with n vertices. Message passing neural networks (MPNNs) are usually presented in terms of an iterative process, where in each round \u2113, each vertex v collects the labels of its neighbors w1, . . . , wk, and updates its own label f\u0303v according to a simple formula such as\nf\u0303 \u2113v = \u03a6 ( f\u0303 \u2113\u22121w1 + . . .+ f\u0303 \u2113\u22121 wk ) .\nAn equivalent way of seeing this process, however, is in terms of the \u201ceffective receptive fields\u201d S\u2113v of each vertex at round \u2113, i.e., the set of all vertices from which information can propagate to v by round \u2113.\nMPNNs can also be viewed as group convolutional networks. A receptive field of size k is just a subset {s1, . . . , sk} \u2286 {1, 2, . . . , n}, and the symmetric group Sn (the group of permutations of {1, 2, . . . , n}) acts on the set of such subsets transitively by {s1, . . . , sk} \u03c37\u2192 {\u03c3(s1), . . . , \u03c3(sk)} \u03c3 \u2208 Sn. Since permuting the n\u2212 k vertices not in S amongst themselves, as well as permuting the k vertices that are in S both leave S invariant, the stablizier of this action is Sn\u2212k \u00d7 Sk. Thus, the set of all k-subsets of vertices is identified with the quotient space X = Sn/(Sk \u00d7 Sn\u2212k), and the labeling function for k-element receptive fields is identified with fk : X \u2192 C. Effectively, this turns the MPNN into a generalized feed-forward network in the sense of Definition 3. Note that fk is a redundant representation of the labeling function because Sn/(Sk\u00d7Sn\u2212k) also includes subsets that do not correspond to contiguous neighborhoods. However this is not a problem because for such S we simply set fk(S) = 0. The key feature of the message passing formalism is that, by construction, it ensures that the f\u0303 \u2113v labels only depend\non the graph topology and are invariant to renumbering the vertices of G. In terms of our \u201ck\u2013subset network\u201d this means that each fk must be Sn\u2013equivariant. Thus, in contrast to the previous two examples, now each index set X\u2113 = Sn/(Sn\u2212\u2113 \u00d7 S\u2113) is different. The form of the corresponding convolutions LV\u2113\u22121(X\u2113\u22121) \u2192 LV\u2113(X\u2113) are best described in the Fourier domain. Unfortunately, the representation theory of symmetric groups is beyond the scope of the present paper (Sagan, 2001). We content ourselves by stating that the irreps of Sn are indexed by so-called integer partitions, (\u03bb1, . . . , \u03bbm), where \u03bb1 \u2265 . . . \u2265 \u03bbm and\u2211 i \u03bbi = n. Moreover, the structure of the Fourier transform of a function f : Sn/(Sn\u2212\u2113 \u00d7 S\u2113) dictated by Proposition 1 in this case is that each of the Fourier matrices are zero except for a single column in each of the f\u0302((n\u2212p, p)) components, where 0 \u2264 p \u2264 \u2113. The main theorem of our paper dictates that the linear map \u03c6\u2113 in each layer must be a convolution. In the case of Fourier matrices with such extreme sparsity structure, this means that each of the \u2113+1 Fourier matrices can be multiplied by a scalar, \u03c7\u2113p. These are the learnable parameters of the network. A real MPNN of course has multiple channels and various corresponding parameters, which could also be introduced in the k\u2013 subset network. The above observation about the form of \u03c7\u2113 is nonetheless interesting, because it at once implies that permutation equivariance is a severe constraint the significantly limits the form of the convolutional filters, yet the framework is still richer than traditional MPNNs where the labels of the neighbors are simply summed."}, {"heading": "7. Conclusions", "text": "Convolution has emerged as one of the key organizing principles of deep neural network architectures. Nonetheless, depending on their background, the word \u201cconvolution\u201d means different things to different researchers. The goal of this paper was to show that in the common setting when there is a group acting on the data that the architecture must be equivariant to, convolution has a specific mathematical meaning that has far reaching consequences: we proved that a feed forward network is equivariant to the group action if and only if it respects this notion of convolution.\nOur theory gives a clear prescription to practitioners on how to design neural networks for data with non-trivial symmetries, such as data on the sphere, etc.. In particular, we argue for Fourier space representations, similar to those that have appeared in (Worrall et al., 2017; Cohen et al., 2018; Kondor et al., 2018)), and, even more recently, since the submission of the original version of the present paper in (Thomas et al., 2018; Kondor, 2018; Weiler et al., 2018)."}, {"heading": "Acknowledgements", "text": "This work was supported in part by DARPA Young Faculty Award D16AP00112."}, {"heading": "A. Background from group and representation theory", "text": "For a more detailed background on representation theory, we point the reader to Serre, 1977.\nGroups. A group is a set G endowed with an operation G\u00d7G\u2192 G (usually denoted multiplicatively) obeying the following axioms:\nG1. for any g1, g2 \u2208G, g1g2 \u2208G (closure);\nG2. for any g1, g2, g3 \u2208G, g1(g2g3) = (g1g2)g3 (associativity); G3. there is a unique e\u2208G, called the identity ofG, such that eg = ge = g for any u\u2208G; G4. for any g \u2208G, there is a corresponding element g\u22121\u2208 G called the inverse of g, such that gg\u22121 = g\u22121g = e.\nWe do not require that the group operation be commutative, i.e., in general, g1g2 6= g2g1. Groups can be finite or infinite, countable or uncountable, compact or non-compact. While most of the results in this paper would generalize to any compact group, to keep the exposition as simple as possible, throughout we assume that G is finite or countably infinite. As usual, |G| will denote the size (cardinality) of G, sometimes also called the order of the group. A subset H of G is called a subgroup of G, denoted H \u2264 G, if H itself forms a group under the same operation as G, i.e., if for any g1, g2 \u2208H , g1g2 \u2208H .\nHomogeneous Spaces.\nDefinition 6. Let G be a group acting on a set X . We say that X is a homogeneous space of G if for any x, y \u2208 X , there is a g \u2208G such that y= g(x).\nThe significance of homogeneous spaces for our purposes is that once we fix the \u201corigin\u201d x0, the above correspondence between points in X and the group elements that map x0 to them allows to lift various operations on the homogeneous space to the group. Because expressions like g(x0) appear so often in the following, we introduce the shorthand [g]X :=g(x0). Note that this hides the dependency on the (arbitrary) choice of x0.\nFor some examples, we see that Z2 is a homogeneous space of itself with respect to the trivial action (i, j) 7\u2192 (g1+i, g2+ j), and the sphere is a homogeneous space of the rotation group with respect to the action:\nx 7\u2192 R(x) R(x) = Rx x\u2208S2, (15)\nOn the other hand, the entries of the adjacency matrix are not a homogeneous space of Sn with respect to\n(i, j) 7\u2192 (\u03c3(i), \u03c3(j)) \u03c3 \u2208 Sn. (16)\n, because if we take some (i, j) with i 6= j, then 16 can map it to any other (i\u2032, j\u2032) with i\u2032 6= j\u2032, but not to any of the diagonal elements, where i\u2032 = j\u2032. If we split the matrix into its \u201cdiagonal\u201d, and \u201coff-diagonal\u201d parts, individually these two parts are homogeneous spaces.\nRepresentations. A (finite dimensional) representation of a group G over a field F is a matrix-valued function \u03c1 : G \u2192 Fd\u03c1\u00d7d\u03c1 such that \u03c1(g1)\u03c1(g2) = \u03c1(g1g2) for any\ng1, g2 \u2208 G. In this paper, unless stated otherwise, we always assume that F = C. A representation \u03c1 is said to be unitary if \u03c1(g\u22121) = \u03c1(g)\u2020 for any g \u2208 G. One representation shared by every group is the trivial representation \u03c1tr that simply evaluates to the one dimensional matrix \u03c1tr(g) = (1) on every group element.\nEquivalence, reducibility and irreps. Two representations \u03c1 and \u03c1\u2032 of the same dimensionality d are said to be equivalent if for some invertible matrix Q \u2208 Cd\u00d7d, \u03c1(g) = Q\u22121\u03c1\u2032(g)Q for any g \u2208 G. A representation \u03c1 is said to be reducible if it decomposes into a direct sum of smaller representations in the form\n\u03c1(g)\n= Q\u22121 (\u03c11(g)\u2295\u03c12(g)) Q = Q\u22121 ( \u03c11(g) 0 0 \u03c12(g) ) Q \u2200 g \u2208G\nfor some invertible matrix Q \u2208 Cd\u03c1\u00d7d\u03c1 . We use RG to denote a complete set of inequivalent irreducible representations ofG. However, since this is quite a mouthful, in this paper we also use the alternative term system of irreps to refer to RG. Note that the choice of irreps in RG is far from unique, since each \u03c1 \u2208 RG can be replaced by an equivalent irrep Q\u22a4\u03c1(g)Q, where Q is any orthogonal matrix of the appropriate size.\nComplete reducibility and irreps. Representation theory takes on its simplest form when G is compact (and F = C). One of the reasons for this is that it is possible to prove (\u201ctheorem of complete reducibility\u201d) that any representation \u03c1 of a compact group can be reduced into a direct sum of irreducible ones, i.e.,\n\u03c1(g) = Q\u22121 ( \u03c1(1)(g)\u2295\u03c1(2)(g)\u2295 . . .\u2295 \u03c1(k)(g) ) Q, g \u2208G\n(17)\nfor some sequence \u03c1(1), \u03c1(2), . . . , \u03c1(k) of irreducible representations of G and some Q \u2208 Cd\u00d7d. In this sense, for compact groups, RG plays a role very similar to the primes in arithmetic. Fixing RG, the number of times that a particular \u03c1\u2032 \u2208 RG appears in (17) is a well-defined quantity called the multiplicity of \u03c1\u2032 in \u03c1, denoted m\u03c1(\u03c1\n\u2032). Compactness also has a number of other advantages:\n1. When G is compact, RG is a countable set, therefore we can refer to the individual irreps as \u03c11, \u03c12, . . .. (WhenG is finite, RG is not only countable but finite.) 2. The system of irreps of a compact group is essentially\nunique in the sense that if R\u2032G is any other system of irreps, then there is a bijection \u03c6 : RG \u2192 R\u2032G mapping each irrep \u03c1\u2208RG to an equivalent irrep \u03c6(\u03c1)\u2208R\u2032G. 3. When G is compact, RG can be chosen in such a way that each \u03c1\u2208R is unitary.\nRestricted representations. Given any representation \u03c1 of G and subgroup H \u2264 G, the restriction of \u03c1 to H is defined as the function \u03c1|H : H \u2192 Cd\u03c1\u00d7d\u03c1 , where \u03c1|H(h) = \u03c1(h) for all h \u2208 H . It is trivial to check that \u03c1|H is a representation of H , but, in general, it is not irreducible (even when \u03c1 itself is irreducible).\nFourier Transforms. In the Euclidean domain convolution and cross-correlation have close relationships with the Fourier transform\nf\u0302(k) = \u222b e\u22122\u03c0\u03b9kx f(x) dx, (18)\nwhere \u03b9 is the imaginary unit, \u221a \u22121. In particular, the Fourier transform of f \u2217 g is just the pointwise product of the Fourier transforms of f and g,\nf\u0302 \u2217 g(k) = f\u0302(k) g\u0302(k), (19)\nwhile cross-correlation is\nf\u0302 \u22c6 g(k) = f\u0302(k)\u2217 g\u0302(k). (20)\nThe concept of group representations (see Section A) allows generalizing the Fourier transform to any compact group. The Fourier transform of f : G \u2192 C is defined as:\nf\u0302(\u03c1i) =\n\u222b\nG\n\u03c1i(u) f(u) d\u00b5(u), i = 1, 2, . . . , (21)\nwhich, in the countable (or finite) case simplifies to\nf\u0302(\u03c1i) = \u2211\nu\u2208G\nf(u)\u03c1(u), i = 1, 2, . . . . (22)\nDespite R not being a compact group, (18) can be seen as a special case of (21), since e\u22122\u03c0\u03b9kx trivially obeys e\u22122\u03c0\u03b9k(x1+x2) = e\u22122\u03c0\u03b9kx1e\u22122\u03c0\u03b9kx2 , and the functions \u03c1k(x) = e \u22122\u03c0\u03b9kx are, in fact, the irreducible representations of R. The fundamental novelty in (21) and (22) compared to (18), however, is that since, in general (in particular, when G is not commutative), irreducible representations are matrix valued functions, each \u201cFourier compo-\nnent\u201d f\u0302(\u03c1) is now a matrix. In other respects, Fourier transforms on groups behave very similarly to classical Fourier transforms. For example, we have an inverse Fourier transform\nf(u) = 1 |G| \u2211\n\u03c1\u2208R\nd\u03c1 tr [ f(\u03c1)\u03c1(u)\u22121 ] ,\nand also an analog of the convolution theorem, which is stated in the main body of the paper."}, {"heading": "B. Convolution of vector valued functions", "text": "Since neural nets have multiple channels, we need to further extend equations 6-12 to vector/matrix valued functions. Once again, there are multiple cases to consider.\nDefinition 7. Let G be a finite or countable group, and X and Y be (left or right) quotient spaces of G. 1. If f : X \u2192 Cm, and g : Y \u2192 Cm, we define f\u2217g : G\u2192\nC with\n(f \u2217 g)(u) = \u2211\nv\u2208G\nf\u2191G(uv\u22121) \u00b7 g\u2191G(v), (23)\nwhere \u00b7 denotes the dot product. 2. If f : X \u2192 Cn\u00d7m, and g : Y \u2192 Cm, we define f \u2217 g : G\u2192 Cn with\n(f \u2217 g)(u) = \u2211\nv\u2208G\nf\u2191G(uv\u22121) \u00d7 g\u2191G(v), (24)\nwhere \u00d7 denotes the matrix/vector product. 3. If f : X \u2192 Cm, and g : Y \u2192 Cn\u00d7m, we define f \u2217 g : G\u2192 Cm with\n(f \u2217 g)(u) = \u2211\nv\u2208G\nf\u2191G(uv\u22121) \u00d7\u0303 g\u2191G(v), (25)\nwhere v\u00d7\u0303A denotes the \u201creverse matrix/vector product\u201d Av.\nSince in cases 2 and 3 the nature of the product is clear from the definition of f and g, we will omit the \u00d7 and \u00d7\u0303 symbols. The specializations of these formulae to the cases\nof Equations 6-12 are as to be expected."}, {"heading": "C. Proof of Proposition 1", "text": "Proposition 1 has three parts. To proceed with the proof, we introduce two simple lemmas.\nRecall that if H is a subgroup of G, a function f : G \u2192 C is called right H\u2013invariant if f(uh) = f(u) for all h\u2208H and all u\u2208G, and it is called left H\u2013invariant if f(hu) = f(u) for all h\u2208H and all u\u2208G. Lemma 1. Let H and K be two subgroups of a group G. Then\n1. If f : G/H \u2192 C, then f\u2191G : G \u2192 C is right H\u2013 invariant. 2. If f : H\\G \u2192 C, then f\u2191G : G \u2192 C is left H\u2013 invariant. 3. If f : K\\G/H \u2192 C, then f\u2191G : G \u2192 C is right H invariant and left K\u2013invariant.\nLemma 2. Let \u03c1 be an irreducible representation of a countable group G. Then \u2211 u\u2208G \u03c1(u) = 0 unless \u03c1 is the trivial representation, \u03c1tr(u) = (1).\nProof. Let us define the functions r\u03c1i,j(u) = [\u03c1(u)]i,j . Recall that for f, g : G \u2192 C, the inner product \u3008f, g\u3009 is\ndefined \u3008f, g\u3009 = \u2211u\u2208G f(u)\u2217g(u). The Fourier transform of a function f can then be written element-wise as [f\u0302(\u03c1)]i,j = \u3008r\u03c1i,j \u2217 , f\u3009. However, since the Fourier transform is a unitary transformation, for any \u03c1, \u03c1\u2032 \u2208RG, unless \u03c1 = \u03c1\u2032, i = i\u2032 and j = j\u2032, we must have \u3008r\u03c1i,j , r\u03c1 \u2032\ni\u2032,j\u2032\u3009 = 0. In particular, [\u2211 u\u2208G \u03c1(u) ] i,j\n= \u3008r\u03c1tr1,1, r\u03c1i,j\u3009 = 0, unless \u03c1 = \u03c1tr (and i= j =1).\nNow recall that given an irrep \u03c1 of G, the restriction of \u03c1 to H is \u03c1|H : H \u2192 Cd\u03c1\u00d7d\u03c1 , where \u03c1|H(h) = \u03c1(h) for all h \u2208 H . It is trivial to check that \u03c1|H is a representation of H , but, in general, it is not irreducible. Thus, by the Theorem of Complete Decomposability (see section A), it must decompose in the form \u03c1|H(h) = Q(\u00b51(h)\u2295\u00b52(h)\u2295 . . .\u2295 \u00b5k(h))Q\u2020 for some sequence \u00b51, . . . , \u00b5k of irreps of H and some unitary martrix Q. In the special case when the irreps of G and H are adapted to H \u2264 G, however, Q is just the unity.\nThis is essentially the case that we consider in Proposition 1. Now, armed with the above lemmas, we are in a position to prove Proposition 1.\nC.0.1. PROOF OF PART 1\nProof. The fact that any u \u2208 G can be written uniquely as u = gh where g is the representative of one of the gH cosets and h\u2208H immediately tells us that f\u0302(\u03c1) factors as f\u0302(\u03c1) = \u2211\nu\u2208G\nf\u2191G(u)\u03c1(u) = \u2211\nx\u2208G/H\n\u2211\nh\u2208H\nf\u2191G(xh)\u03c1(xh)\n= \u2211\nx\u2208G/H\n\u2211\nh\u2208H\nf(x)\u03c1(xh) = \u2211\nx\u2208G/H\n\u2211\nh\u2208H\nf(x)\u03c1(x)\u03c1(h)\n= \u2211\nx\u2208G/H\nf(x)\u03c1(x) [\u2211\nh\u2208H\n\u03c1(h) ] .\nHowever, \u03c1(h) = \u00b51(h) \u2295 \u00b52(h) \u2295 . . . \u2295 \u00b5k(h) for some sequence of irreps \u00b51, . . . , \u00b5k of H , so\n\u2211\nh\u2208H\n\u03c1(h) = [\u2211\nh\u2208H\n\u00b51(h) ] \u2295 [\u2211\nh\u2208H\n\u00b52(h) ] \u2295. . .\u2295 [\u2211\nh\u2208H\n\u00b5k(h) ] ,\nand by Lemma 2 each of the terms in this sum where \u00b5i is not the trivial representation (on H) is a zero matrix,\nzeroing out all the corresponding columns in f\u0302(\u03c1).\nC.0.2. PROOF OF PART 2\nProof. Analogous to the proof of part 1, using u = hg and\na factorization similar to that of f\u0302(\u03c1) in C.0.1 except that\u2211 h\u2208H \u03c1(h) will now multiply \u2211 x\u2208H\\G f(x)\u03c1(x) from the left.\nC.0.3. PROOF OF PART 3\nProof. Immediate from combining case 3 of Lemma 1 with Parts 1 and 2 of Proposition 1."}, {"heading": "D. Proof of Proposition 2", "text": "Proof. Let us assume that G is countable. Then\nf\u0302 \u2217g(\u03c1i) = \u2211\nu\u2208G\n[\u2211\nv\u2208G\nf(uv\u22121) g(v) ] \u03c1i(u)\n= \u2211\nu\u2208G\n\u2211\nv\u2208G\nf(uv\u22121) g(v)\u03c1i(uv \u22121)\u03c1i(v)\n= \u2211\nv\u2208G\n\u2211\nu\u2208G\nf(uv\u22121) g(v)\u03c1i(uv \u22121)\u03c1i(v)\n= \u2211\nv\u2208G\n[\u2211\nu\u2208G\nf(uv\u22121) \u03c1i(uv \u22121) ] g(v)\u03c1i(v)\n= \u2211\nv\u2208G\n[\u2211\nw\u2208G\nf(w) \u03c1i(w) ] g(v)\u03c1i(v)\n= [\u2211\nw\u2208G\nf(w) \u03c1i(w) ][\u2211\nv\u2208G\ng(v)\u03c1i(v) ]\n= f\u0302(\u03c1i) g\u0302(\u03c1i).\nThe continuous case is proved similarly but with integrals with respect Haar measure instead of sums."}, {"heading": "E. Proof of Theorem 1", "text": "E.1. Reverse Direction\nProving the \u201conly if\u201d part of Theorem 1 requires concepts from representation theory and the notion of generalized Fourier transforms (Section A)). We also need two versions of Schur\u2019s Lemma.\nLemma 3. (Schur\u2019s lemma I) Let {\u03c1(g) : U \u2192U}g\u2208G and {\u03c1\u2032(g) : V \u2192 V }g\u2208G be two irreducible representations of a compact group G. Let \u03c6 : U \u2192 V be a linear (not necessarily invertible) mapping that is equivariant with these representations in the sense that \u03c6(\u03c1(g)(u)) = \u03c1\u2032(g)(\u03c6(u)) for any u\u2208U . Then, unless \u03c6 is the zero map, \u03c1 and \u03c1\u2032 are equivalent representations.\nLemma 4. (Schur\u2019s lemma II) Let {\u03c1(g) : U \u2192 U}g\u2208G be an irreducible representation of a compact group G on a space U , and \u03c6 : U \u2192 U a linear map that commutes with each \u03c1(g) (i.e., \u03c1(g) \u25e6 \u03c6 = \u03c6 \u25e6 \u03c1(g) for any g \u2208 G). Then \u03c6 is a multiple of the identity.\nWe build up the proof through a sequence of lemmas.\nLemma 5. Let U and V be two vector spaces on which a compact group G acts by the linear actions {Tg : U \u2192\nU}g\u2208G and {T \u2032g : V\u2192V }g\u2208G, respectively. Let \u03c6 : U\u2192V be a linear map that is equivariant with the {Tg} and {T \u2032g} actions, and W be an irreducible subspace of U (with respect to {Tg}). Then Z =\u03c6(W ) is an irreducible subspace of V , and the restriction of {Tg} to W , as a representation, is equivalent with the restriction of {T \u2032g} to Z .\nProof. Assume for contradiction that Z is reducible, i.e., that it has a proper subspace Z \u2282 Z that is fixed by {T \u2032g} (in other words, T \u2032g(v) \u2208Z for all v \u2208Z and g \u2208G). Let v be any nonzero vector in Z , u \u2208 U be such that \u03c6(u) = v, and W = span {Tg(u) | g \u2208G }. Since W is irreducible, W cannot be a proper subspace of W , so W =W . Thus,\nZ = \u03c6(span {Tg(u) | g \u2208G }) = span{T \u2032g(\u03c6(u))|g \u2208G} = span{T \u2032g(v)|g \u2208G} \u2286 Z,\n(26)\ncontradicting our assumption. Thus, the restriction {Tg|W } of {Tg} to W and the restriction {T \u2032g|Z} of {T \u2032g} to Z are both irreducible representations, and \u03c6 : W \u2192 Z is a linear map that is equivariant with them. By Schur\u2019s lemma it follows that {Tg|W } and {T \u2032g|Z} are equivalent representations.\nLemma 6. Let U and V be two vector spaces on which a compact group G acts by the linear actions {Tg : U \u2192 U}g\u2208G and {T \u2032g : V \u2192 V }g\u2208G, and let U = U1 \u2295 U2 \u2295 . . . and V = V1 \u2295 V2 \u2295 . . . be the corresponding isotypic decompositions. Let \u03c6 : U \u2192 V be a linear map that is equivariant with the {Tg} and {T \u2032g} actions. Then \u03c6(Ui) \u2286 Vi for any i.\nProof. Let Ui = U 1 i \u2295 U2i \u2295 . . . be the decomposition of Ui into irreducible G\u2013modules, and V j i = \u03c6(U j i ). By Lemma 5, each V ji is an irreducible G\u2013module that is equivalent with U ji , hence V j i \u2286 Vi. Consequently, \u03c6(Ui) = \u03c6(U 1 i \u2295 U2i \u2295 . . .) \u2286 Vi.\nLemma 7. Let X = G/H and X \u2032 = G/K be two homogeneous spaces of a compact group G, let {Tg : L(X ) \u2192 L(X )}g\u2208G and {T\u2032g : L(X \u2032) \u2192 L(X \u2032)}g\u2208G be the corresponding translation actions, and let \u03c6 : L(X ) \u2192 L(X \u2032) be a linear map that is equivariant with these actions. Given f \u2208 L(X ) let f\u0302 denote its Fourier transform with respect to a specific choice of origin x0 \u2208 X and system or irreps RG = {\u03c11, \u03c12, . . .}. Similarly, f\u0302 \u2032 is the Fourier transform of f \u2032 \u2208L(X \u2032), with respect to some x\u20320 \u2208X \u2032 and the same system of irreps.\nNow if f \u2032 = \u03c6(f), then each Fourier component of f \u2032 is a linear function of the corresponding Fourier component of f , i.e., there is a sequence of linear maps {\u03a6i} such that f\u0302 \u2032(\u03c1i) = \u03a6i(f\u0302(\u03c1i)).\nProof. Let U1 \u2295 U2 \u2295 . . . and V1 \u2295 V2 \u2295 . . . be the isotypic decompositions of L(X ) and L(X \u2032) with respect to the {Tg} and {T\u2032g} actions. By our discussion in Section ??, each Fourier component f\u0302(\u03c1i) captures the part of f falling in the corresponding isotypic subspace Ui. Similarly, f\u0302 \u2032(\u03c1j) captures the part of f\n\u2032 falling in Vj . Lemma 6 tells us that because \u03c6 is equivariant with the translation actions, it maps each Ui to the corresponding isotypic Vi. Therefore, f\u0302 \u2032(\u03c1i) = \u03a6i(f\u0302(\u03c1i)) for some function \u03a6i. By the linearity of \u03c6, each \u03a6i must be linear.\nLemma 7 is a big step towards describing what form equivariant mappings take in Fourier space, but it doesn\u2019t yet fully pin down the individual \u03a6i maps. We now focus on a single pair of isotypics (Ui, Vi) and the corresponding map \u03a6i taking f\u0302(\u03c1i) 7\u2192 f\u0302 \u2032(\u03c1i). We will say that \u03a6i is an allowable map if there is some equivariant \u03c6 such\nthat \u03c6\u0302(f)(\u03c1i) = \u03a6i(f\u0302(\u03c1i)). Clearly, if \u03a61,\u03a62, . . . are individually allowable, then they are also jointly allowable.\nLemma 8. All linear maps of the form \u03a6i : M 7\u2192 MB where B \u2208C\u03b4\u00d7\u03b4 are allowable.\nProof. Recall that the {Tg} action takes f 7\u2192 fg , where fg(x) = f(g\u22121x). In Fourier space,\nf\u0302g(\u03c1i) = \u2211\nu\u2208G\n\u03c1i(u)f g\u2191G(u)\n= \u2211\nu\u2208G\n\u03c1i(u)f\u2191G(g\u22121u)\n= \u2211\nw\u2208G\n\u03c1i(gw)f\u2191G(w)\n= \u03c1i(g) \u2211\nw\u2208G\n\u03c1i(w)f\u2191G(w)\n= \u03c1i(g) f\u0302(\u03c1i). (27)\n(This is actually a general result called the (left) translation theorem.) Thus,\n\u03a6i ( T\u0302g(f)(\u03c1i) ) = \u03a6i ( \u03c1i(g)f\u0302(\u03c1i) ) = \u03c1i(g) f\u0302(\u03c1i)B.\nSimilarly, the {T\u2032g} action maps f\u0302 \u2032(\u03c1i) 7\u2192 g(\u03c1i)f\u0302 \u2032(\u03c1i), so\nT\u2032g ( \u03a6i(f\u0302(\u03c1i)) ) = T\u2032g ( f\u0302(\u03c1i)B ) = \u03c1i(g) f\u0302(\u03c1i)B.\nTherefore, \u03a6i is equivariant with the {T} and {T\u2032} actions.\nLemma 9. Let \u03a6i : M 7\u2192 BM for some B \u2208 C\u03b4\u00d7\u03b4. Then \u03a6i is not allowable unless B is a multiple of the identity. Moreover, this theorem also hold in the columnwise sense that if \u03a6i : M \u2192 M \u2032 such that [M \u2032]\u2217,j = Bj [M ]\u2217,j for some sequence of matrices B1, . . . , Bd, then \u03a6i is not allowable unless each Bj is a multiple of the identity.\nProof. Following the same steps as in the proof of Lemma 8, we now have\n\u03a6i ( T\u0302g(f)(\u03c1i) ) = B\u03c1i(g) f\u0302(\u03c1i),\nT\u2032g ( \u03a6i(f\u0302(\u03c1i)) ) = \u03c1i(g)Bf\u0302 (\u03c1i).\nHowever, by the second form of Schur\u2019s Lemma, we cannot have B\u03c1i(g) = \u03c1i(g)B for all g \u2208G, unless B is a multiple of the identity.\nLemma 10. \u03a6i is allowable if and only if it is of the form M 7\u2192MB for some B \u2208C\u03b4\u00d7\u03b4 .\nProof. For the \u201cif\u201d part of this lemma, see Lemma 8. For the \u201conly if\u201d part, note that the set of allowable \u03a6i form a subspace of all linear maps C\u03b4\u00d7\u03b4 \u2192 C\u03b4\u00d7\u03b4, and any allowable \u03a6i can be expressed in the form\n[\u03a6i(M)]a,b = \u2211\nc,d\n\u03b1a,b,c,dMc,d.\nBy Lemma 9, if a 6= c but b = d, then \u03b1a,b,c,d = 0. On the other hand, by Lemma 8 if a= c, then \u03b1a,b,c,d can take on any value, regardless of the values of b and d, as long as \u03b1a,b,a,d is constant across varying a.\nNow consider the remaining case a 6= c and b 6= d, and assume that \u03b1a,b,c,d 6= 0 while \u03a6i is still allowable. Then, by Lemma 8, it is possible to construct a second allowable map\u03a6\u2032i (namely one in which\u03b1 \u2032 a,d,a,b = 1 and\u03b1 \u2032 a,d,x,y = 0 for all (x, y) 6= (c, d)) such that in the composite map \u03a6\u2032\u2032i = \u03a6 \u2032 i \u25e6 \u03a6i, \u03b1\u2032\u2032a,d,c,d 6= 0. Thus, \u03a6\u2032\u2032i is not allowable. However, the composition of one allowable map with another allowable map is allowable, contradicting our assumption that \u03a6i is allowable.\nThus, we have established that if \u03a6i is allowable, then \u03b1a,b,c,d=0, unless a= c. To show that any allowable \u03a6i of the form M 7\u2192 MB, it remains to prove that additionally \u03b1a,b,a,d is constant across a. Assume for contradiction that \u03a6i is allowable, but for some (a, e, b, d) indices \u03b1a,b,a,d 6= \u03b1e,b,e,d. Now let \u03a60 be the allowable map that zeros out every column except column d (i.e., \u03b10x,d,x,d = 1 for all x, but all other coefficients are zero), and let \u03a6\u2032 be the allowable map that moves column b to column d (i.e., \u03b1\u2032x,d,x,b = 1 for any x, but all other coeffcients are zero). Since the composition of allowable maps is allowable, we expect \u03a6\u2032\u2032 = \u03a6\u2032 \u25e6 \u03a6 \u25e6 \u03a60 to be allowable. However \u03a6\u2032\u2032 is a map that falls under the purview of Lemma 9, yet \u03b1\u2032\u2032a,d,a,d 6= \u03b1\u2032\u2032e,d,e,d (i.e., Mj is not a multiple of the identity) creating a contradiction.\nProof of Theorem 1 (reverse direction). For simplicty we first prove the theorem assuming Y\u2113=C for each \u2113.\nSince N is a G-CNN, each of the mappings (\u03c3\u2113 \u25e6 \u03c6\u2113) : L(X\u2113\u22121) \u2192 L(X\u2113) is equivariant with the corresponding translation actions {T\u2113\u22121g }g\u2208G and {T\u2113g}g\u2208G. Since \u03c3\u2113 is a pointwise operator, this is equivalent to asserting that \u03c6\u2113 is equivariant with {T\u2113\u22121g }g\u2208G and {T\u2113g}g\u2208G. Letting X = X\u2113\u22121 and X \u2032 = X\u2113, Lemma 8 then tells us the the Fourier transforms of f\u2113\u22121 and \u03c6\u2113(f\u2113\u22121) are related by\n\u0302\u03c6\u2113(f\u2113\u22121)(\u03c1i) = \u03a6 ( f\u0302\u2113\u22121(\u03c1i) )\nfor some fixed set of linear maps \u03a61,\u03a62, . . .. Furthermore, by Lemma 10, each \u03a6i must be of the formM 7\u2192MBi for some appropriate matrix Bi \u2208 Cd\u03c1\u00d7d\u03c1 . If we then define \u03c7\u2113 as the inverse Fourier transform of (B1, B2, . . .), then by the convolution theorem (Proposition 2), \u03c6\u2113(f\u2113\u22121) = f\u2113\u22121 \u2217 \u03c7, confirming that N is a G-CNN. The extension of this result to the vector valued case, f\u2113 : X\u2113 \u2192 V\u2113, is straightforward."}], "year": 2018, "references": [{"title": "Group equivariant convolutional networks", "authors": ["T.S. Cohen", "M. Welling"], "venue": "Proceedings of International Conference on Machine Learning (ICML),", "year": 2016}, {"title": "Steerable CNNs", "authors": ["T.S. Cohen", "M. Welling"], "venue": "In International Conference on Learning Representations (ICLR),", "year": 2017}, {"title": "Convolutional neural networks on graphs with fast localized spectral filtering", "authors": ["M. Defferrard", "X. Bresson", "P. Vandergheynst"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "year": 2016}, {"title": "Convolutional networks on graphs for learning molecular fingerprints", "authors": ["D. Duvenaud", "D. Maclaurin", "J. Aguilera-Iparraguirre", "R. Gomez-Bombarelli", "T. Hirzel", "A. Aspuru-Guzik", "R.P. Adams"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "year": 2015}, {"title": "Neural message passing for quantum chemistry", "authors": ["J. Gilmer", "S.S. Schoenholz", "P.F. Riley", "O. Vinyals", "G.E. Dahl"], "venue": "Proceedings of International Conference on Machine Learning (ICML),", "year": 2017}, {"title": "N-body networks: a covariant hierarchical neural network architecture for learning atomic potentials", "authors": ["R. Kondor"], "venue": "ArXiv e-prints,", "year": 2018}, {"title": "Clebsch\u2013Gordan nets: a fully Fourier space spherical convolutional neural network", "authors": ["R. Kondor", "Z. Lin", "S. Trivedi"], "venue": "ArXiv e-prints,", "year": 2018}, {"title": "Backpropagation applied to handwritten zip code recognition", "authors": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural Computation,", "year": 1989}, {"title": "Gated graph sequence neural networks", "authors": ["Y. Li", "D. Tarlow", "M. Brockschmidt", "R. Zemel"], "venue": "In International Conference on Learning Representations (ICLR),", "year": 2016}, {"title": "Group invariant scattering", "authors": ["S. Mallat"], "venue": "Communications on Pure and Applied Mathematics,", "year": 2012}, {"title": "Rotation equivariant vector field networks", "authors": ["D. Marcos", "M. Volpi", "N. Komodakis", "D. Tuia"], "venue": "In International Conference on Computer Vision (ICCV),", "year": 2017}, {"title": "Geodesic convolutional neural networks on Riemannian manifolds", "authors": ["J. Masci", "D. Boscaini", "M.M. Bronstein", "P. Vandergheynst"], "venue": "International Conference on Computer Vision Workshop (ICCVW),", "year": 2015}, {"title": "Geometric deep learning on graphs and manifolds using mixture model cnns", "authors": ["F. Monti", "D. Boscaini", "J. Masci", "E. Rodol\u00e0", "J. Svoboda", "M.M. Bronstein"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "year": 2017}, {"title": "Learning convolutional neural networks for graphs", "authors": ["M. Niepert", "M. Ahmed", "K. Kutzkov"], "venue": "In Proceedings of International Conference on Machine Learning (ICML),", "year": 2016}, {"title": "Equivariance through parameter-sharing", "authors": ["S. Ravanbakhsh", "J. Schneider", "B. Poczos"], "venue": "In Proceedings of International Conference on Machine Learning (ICML),", "year": 2017}, {"title": "The Symmetric Group", "authors": ["B.E. Sagan"], "venue": "Graduate Texts in Mathematics. Springer,", "year": 2001}, {"title": "Fourier analysis on finite groups and applications, volume 43 of London Mathematical Society Student Texts", "authors": ["A. Terras"], "year": 1999}, {"title": "Tensor field networks: Rotation- and translation-equivariant neural networks for 3D point clouds", "authors": ["N. Thomas", "T. Smidt", "S.M. Kearnes", "L. Yang", "L. Li", "K. Kohlhoff", "P. Riley"], "venue": "Arxiv e-prints,", "year": 2018}, {"title": "3D steerable CNNs: Learning rotationally equivariant features in volumetric data", "authors": ["M. Weiler", "M. Geiger", "M. Welling", "W. Boomsma", "T. Cohen"], "venue": "ArXiv e-prints,", "year": 2018}, {"title": "Harmonic networks: Deep translation and rotation equivariance", "authors": ["D.E. Worrall", "S.J. Garbin", "D. Turmukhambetov", "G.J. Brostow"], "venue": "In Proceedings of International Conference on Computer Vision and Pattern Recognition", "year": 2017}], "id": "SP:4a09d38f594a6dd3043771353845a92ea5c44c69", "authors": [{"name": "Risi Kondor", "affiliations": []}, {"name": "Shubhendu Trivedi", "affiliations": []}], "abstractText": "Convolutional neural networks have been extremely successful in the image recognition domain because they ensure equivariance to translations. There have been many recent attempts to generalize this framework to other domains, including graphs and data lying on manifolds. In this paper we give a rigorous, theoretical treatment of convolution and equivariance in neural networks with respect to not just translations, but the action of any compact group. Our main result is to prove that (given some natural constraints) convolutional structure is not just a sufficient, but also a necessary condition for equivariance to the action of a compact group. Our exposition makes use of concepts from representation theory and noncommutative harmonic analysis and derives new generalized convolution formulae.", "title": "On the Generalization of Equivariance and Convolution in Neural Networks  to the Action of Compact Groups"}