{"sections": [{"heading": "1. Introduction", "text": "Since its development by Breiman (2001), random forest has proven to be both accurate and efficient for classification and regression problems. In regression setting, random forest will predict the conditional mean of a response variable by averaging predictions of a large number of regression trees. Later then, many other machine learning algorithms were developed upon random forest. Among them, robust versions of random forest have also been proposed using various methodologies. Besides the sampling idea (Breiman, 2001) which adds extra randomness, the other variations are mainly based on two ideas: (1) use more robust criterion to construct regression trees (Galimberti et al., 2007; Brence & Brown, 2006; Roy & Larocque, 2012); (2) choose more robust aggregation method (Meinshausen, 2006; Roy & Larocque, 2012; Tsymbal et al., 2006).\nMeinshausen (2006) generalized random forest to pre-\n1University of California at San Diego, San Diego, California, USA 2Zillow, Seattle, Washington, USA. Correspondence to: Alexander Hanbo Li <alexanderhanboli@gmail.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ndict quantiles by discovering that besides calculating the weighted mean of the observed response variables, one could also get information for the weighted distribution of observed response variables using the sets of local weights generated by random forest. This method is strongly connected to the adaptive nearest neighbors procedure (Lin & Jeon, 2006) which we will briefly review in section 1.2. Different from classical k-NN methods that rely on predefined distance metrics, the dissimilarities generated by random forest are data dependent and scale-invariant.\nAnother state-of-the-art algorithm AdaBoost (Freund & Schapire, 1995; Freund et al., 1996) has been generalized to be applicable to a large family of loss functions (Friedman, 2001; Mason et al., 1999; Li & Bradic, 2016). Recent development of more flexible boosting algorithms such as xgboost (Chen & Guestrin, 2016) have become the go-to forest estimators with tabular or matrix data. One way in which recent boosting algorithms have an advantage over the random forest is the ability to customize the loss function used to reduce the influence of outliers or optimize a metric more suited to the specific problem other than the mean squared error.\nIn this paper, we will propose a general framework for forest-type regression which can also be applied to a broad family of loss functions. It is claimed in (Meinshausen, 2006) that quantile random forest is another nonparametric approach which does not minimize an empirical loss. However, we will show in fact both random forest and quantile random forest estimators can be re-derived as regression methods using the squared error or quantile loss respectively in our framework. Inspired by the adaptive nearest neighbor viewpoint, we explore how random forest makes predictions using the local weights generated by ensemble of trees, and connect that with locally weighted regression (Fan & Gijbels, 1996; Tibshirani & Hastie, 1987; Staniswalis, 1989; Newey, 1994; Loader, 2006; Hastie & Loader, 1993). The intuition is that when predicting the target value (e.g. E[Y |X = x]) at point x, the observations closer to x should receive larger weights. Different from predefining a kernel, random forest assigns the weights data dependently and adaptively. After we illustrate the relation between random forest and local regression, we will use random forest weights to design other regression algo-\nrithms. By plugging robust loss functions like Huber loss and Tukey\u2019s redescending loss, we get forest-type regression methods that are more robust to outliers. Finally, motivated from the truncated squared error loss example, we will show that decreasing the number of nearest neighbors in random forest will also immediately improve its generalization performance.\nThe layout of this paper is as follows. In Section 1.1 and 1.2 we review random forest and adaptive nearest neighbors. Section 2 introduces the general framework of forest-type regression. In Section 3 we plug in robust regression loss functions to get robust forest algorithms. In Section 4 we motivate from the truncated squared error loss and investigate the importance of choosing right number of nearest neighbors. Finally, we test our robust forests in Section 5 and show that they are always superior to the traditional formulation in the presence of outliers in both synthetic and real data set."}, {"heading": "1.1. Random forest", "text": "Following the notation of Breiman (2001), let \u03b8 be the random parameter determining how a tree is grown, and data (X,Y ) \u2208 X \u00d7 Y . For each tree T (\u03b8), let L be the total number of leaves, and Rl denotes the rectangular subspace in X corresponding to the l-th leaf. Then for every x \u2208 X , there is exactly one leaf l such that x \u2208 Rl. Denote this leaf by l(x, \u03b8).\nFor each tree T (\u03b8), the prediction of a new data point X = x is the average of data values in leaf l(x, \u03b8), that is, Y\u0302 (x, \u03b8) = \u2211n j=1 w(Xi, x, \u03b8)Yi, where\nw(Xi, x, \u03b8) = 1I{Xi\u2208Rl(x,\u03b8)}\n#{j : Xj \u2208 Rl(x,\u03b8)} . (1)\nFinally, the conditional mean E[Y |X = x] is approximated by the averaged prediction of m trees, Y\u0302 (x) = m\u22121 \u2211m t=1 Y\u0302 (x, \u03b8t). After rearranging the terms, we can write the prediction of random forest as\nY\u0302 (x) = n\u2211 i=1 w(Xi, x)Yi, (2)\nwhere the averaged weight w(Xi, x) is defined as\nw(Xi, x) = 1\nm m\u2211 t=1 w(Xi, x, \u03b8t). (3)\nFrom equation (2), the prediction of the conditional expectation E[Y |X = x] is the weighted average of the response values of all observations. Furthermore, it is easy to show that \u2211n i=1 w(Xi, x) = 1."}, {"heading": "1.2. Adaptive nearest neighbors", "text": "Lin and Jeon (2006) studies the connection between random forest and adaptive nearest neighbor. They introduced the so-called potential nearest neighbors (PNN): A sample point xi is called a k-PNN to a target point x if there exists a monotone distance metric under which xi is among the k closest to x among all the sample points.\nTherefore, any k-NN method can be viewed as choosing k points from the k-PNNs according to some monotone metric. For example, under Euclidean metric, the classical k-NN algorithm sorts the observations by their Euclidean distances to the target point and outputs the k closest ones. This is equivalent to weighting the k-PNNs using inverse L2 distance.\nMore interestingly, they prove that those observations with positive weights (3) all belong to the k-PNNs (Lin & Jeon, 2006). Therefore, random forests is another weighted kPNN method, but it assigns weights to the observations different from any k-NN method under a pre-defined monotonic distance metric. In fact, the random forest weights are adaptive to the data if the splitting scheme is adaptive."}, {"heading": "2. General framework for forest-type regression", "text": "In this section, we generalize the classical random forest to a general forest-type regression (FTR) framework which is applicable to a broad family of loss functions. In Section 2.1, we motivate the framework by connecting random forest predictor with locally weighted regression. Then in Section 2.2, we formally propose the new forest-type regression framework. In Section 2.3, we rediscover the quantile random forest estimator by plugging the quantile loss function into our framework."}, {"heading": "2.1. Squared error and random forest", "text": "Classical random forest can be understood as an estimator of conditional mean E[Y |X]. As shown in (2), the estimator Y\u0302 (x) is weighted average of all response Yi\u2019s. This special form reminds us of the classical least squares regression, where the estimator is the sample mean. To be more precise, we rewrite (2) as\nn\u2211 i=1 w(Xi, x)(Yi \u2212 Y\u0302 (x)) = 0. (4)\nEquation (4) is the estimating equation (first order condition) of the locally weighted least squares regression (Ruppert & Wand, 1994):\nY\u0302 (x) = argmin \u03bb\u2208R n\u2211 i=1 w(Xi, x)(Yi \u2212 \u03bb)2 (5)\nIn classical local regression, the weight w(Xi, x) serves as a local metric between the target point x and observation Xi. Intuitively, observations closer to target x should be given more weights when predicting the response at x. One common choice of such local metric is kernel Kh(Xi, x) = K((Xi \u2212 x)/h). For example, the tricube kernel K(u) = (1 \u2212 |u|3)3 1I(|u| \u2264 1) will ignore the impact of observations outside a window centered at x and increase the weight of an observation when it is getting closer to x. The form of kernel-type local regression is as follows:\nargmin \u03bb\u2208R n\u2211 i=1 Kh(Xi \u2212 x)(Yi \u2212 \u03bb)2,\nThe random forest weight w(Xi, x) (3) defines a similar data dependent metric, which is constructed using the ensemble of regression trees. Using an adaptive splitting scheme, each tree chooses the most informative predictors from those at its disposal. The averaging process then assigns positive weights to these training responses, which are called voting points in (Lin & Jeon, 2006). Hence via the random forest voting mechanism, those observations close to the target point get assigned positive weights equivalent to a kernel functionality (Friedman et al., 2001)."}, {"heading": "2.2. Extension to general loss", "text": "Note that the formation (5) is just a special case when using squared error loss \u03c6(a, b) = (a\u2212b)2. In more general form, we have the following local regression problem:\nY\u0302 (x) = argmin s\u2208F n\u2211 i=1 w(Xi, x)\u03c6(s(Xi), Yi) (6)\nwhere w(Xi, x) is a local weight, F is a family of functions, and \u03c6(\u00b7) is a general loss. For example, when local weight is a kernel and F stands for polynomials of a certain degree, it reduces to local polynomial regression (Fan & Gijbels, 1996). Random forest falls into this framework with squared error loss, a family of constant functions and local weights (3) constructed from ensemble of trees.\nAlgorithm 1 Forest-type regression Step 1: Calculate local weights w(Xi, x) using ensemble or trees. Step 2: Choose a loss \u03c6(\u00b7, \u00b7) and a family F of function. Then do the locally weighted regression\nY\u0302 (x) = argmin s\u2208F n\u2211 i=1 w(Xi, x)\u03c6(Yi, s(Xi)).\nIn Algorithm 1, we summarize the forest-type regression as a general two-step method. Note that here we only focus on local weights generated by random forest, which\nuses ensemble of trees to recursively partition the covariate space X . However, there are many other data dependent dissimilarity measures that can potentially be used, such as k-NN, mp-dissimilarity (Aryal et al., 2014), shared nearest neighbors (Jarvis & Patrick, 1973), information-based similarity (Lin et al., 1998), mass-based dissimilarity (Ting et al., 2016), etc. And there are many other domain specific dissimilarity measures. To avoid distraction, we will only use random forest weights throughout the rest of this paper."}, {"heading": "2.3. Quantile loss and quantile random forest", "text": "Meinshausen (2006) proposed the quantile random forest which can extract the information of different quantiles rather than just predicting the average. It has been shown that quantile random forest is more robust than the classical random forest (Meinshausen, 2006; Roy & Larocque, 2012). In this section, we show quantile random forest estimator is also a special case of Algorithm 1. It is well known that the \u03c4 -th quantile of an (empirical) distribution is the constant that minimizes the (empirical) risk using \u03c4 - th quantile loss function \u03c1\u03c4 (z) = z(\u03c4 \u22121I{z<0}) (Koenker, 2005). Now let the loss function in Algorithm 1 be the quantile loss \u03c1\u03c4 (\u00b7), F be the family of constant functions, and w(Xi, x) be random forest weights (3). Solving the optimization problem\nY\u0302\u03c4 (x) = argmin \u03bb\u2208R n\u2211 i=1 w(Xi, x)\u03c1\u03c4 (Yi \u2212 \u03bb),\nwe get the corresponding first order condition\nn\u2211 i=1 w(Xi, x)(\u03c4 \u2212 1I {Yi \u2212 Y\u0302\u03c4 (x) < 0}) = 0.\nRecall that \u2211n i=1 w(Xi, x) = 1, hence, we have\nn\u2211 i=1 w(Xi, x) 1I {Yi < Y\u0302\u03c4 (x)} = \u03c4. (7)\nThe estimator Y\u0302\u03c4 (x) in (7) is exactly the same estimator proposed in (Meinshausen, 2006). In particular, when \u03c4 = 0.5, the equation \u2211n i=1 w(Xi, x) 1I {Yi < Y\u03020.5(x)} = 0.5 will give us the median estimator Y\u03020.5(x). Therefore, we have rediscovered quantile random forest from a totally different point of view as a local regression estimator with quantile loss function and random forest weights."}, {"heading": "3. Robust forest", "text": "From the framework 1, quantile random forest is insensitive to outliers because of the more robust loss function. In this section, we test our framework on other robust losses and proposed fixed-point method to solve the estimating\nequation. In Section 3.1 we choose the famous robust loss \u2013 (pseudo) Huber loss, and in Section 3.2, we further investigate a non-convex loss \u2013 Tukey\u2019s biweight."}, {"heading": "3.1. Huber loss", "text": "The Huber loss (Huber et al., 1964)\nH\u03b4(y) =\n{ 1 2y\n2 for |y| \u2264 \u03b4, \u03b4(|y| \u2212 12\u03b4) elsewhere\nis a well-known loss function used in robust regression. The penalty acts like squared error loss when the error is within [\u2212\u03b4, \u03b4] but becomes linear outside this range. In this way, it will penalize the outliers more lightly but still preserves more efficiency than absolute deviation when data is concentrated in the center and has light tails (e.g. Normal). By plugging Huber loss into the FTR framework 1, we get a robust counterpart of random forest. The estimating equation is\nn\u2211 i=1 wi(x) sign(Y\u0302 (x)\u2212 Yi) min(Y\u0302 (x)\u2212 Yi, \u03b4) = 0. (8)\nDirect optimization of (8) with local weights is hard, hence instead we will investigate the pseudo-Huber loss (see Figure 1),\nL\u03b4(y) = \u03b4 2 (\u221a 1 + (y \u03b4 )2 \u2212 1 ) which is a smooth approximation of Huber loss (Charbonnier et al., 1997). The estimating equation\nn\u2211 i=1 wpHi (x) ( Y\u0302pH(x)\u2212 Yi ) = 0. (9)\nis very similar to that of square error loss if we define a new weight\nwpHi (x) = wi(x)\u221a\n1 + ( Y\u0302pH(x)\u2212Yi\n\u03b4 )2 . (10) Then the (pseudo) Huber estimator can be expressed as\nY\u0302pH(x) =\n\u2211n i=1 w\npH i (x)Yi\u2211n\ni=1 w pH i (x)\n. (11)\nInformally, the estimator (11) can be viewed as a weighted average of all the responses Yi\u2019s. From (10), we know the new weight for pseudo-Huber loss has an extra scaling factor (\u221a\n1 + (\u03b4\u22121u)2 )\u22121\n(12)\nand hence will shrink more to zero whenever \u03b4\u22121|Y\u0302pH(x)\u2212 Yi| is large. The tuning parameter \u03b4 acts like a control of the level of robustness. A smaller \u03b4 will lead to more shrinkage on the weights of data that have responses far away from the estimator.\nThe estimating equation (9) can be solved by fix-point method which we propose in Algorithm 2. For notation simplicity, we will use wi,j to denote w(Xi, xj), where Xi is the i-th training point and xj is the j-th testing point. The convergence to the unique solution (if exists) is guaranteed by Lemma 1.\nLemma 1. Define\nK\u03b4(y) =\n\u2211n i=1\nwiYi\u221a 1+( y\u2212Yi\u03b4 )\n2\u2211n i=1\nwi\u221a 1+( y\u2212Yi\u03b4 ) 2 ,\nAlgorithm 2 pseudo-Huber loss (\u03b4)\nInput: Test points {xj}mj=1, initial guess {Y\u0302 (0)(xj)}, local weights wi,j , training responses {Yi}ni=1, and error tolerance 0. while > 0 do\n(a) Update the weights\nw (k) i,j = wi,j\u221a 1 + ( Y\u0302 (k\u22121)(xj)\u2212Yi\n\u03b4 )2 (b) Update the estimator\nY\u0302 (k)(xj) =\n\u2211n i=1 w\n(k) i,j Yi\u2211n\ni=1 w (k) i,j\n(c) Calculate error\n= 1\nm m\u2211 j=1 ( Y\u0302 k(xj)\u2212 Y\u0302 (k\u22121)(xj) )2 (d) k \u2190 k + 1\nend while Output the pseudo-Huber estimator:\nY\u0302pH(xj) = Y\u0302 (k)(xj)\nwhere \u2211n i=1 wi = 1. Let K = maxi=1,\u00b7\u00b7\u00b7 ,n |Yi|. Then Algorithm 2 can be written as Y\u0302 (k)(x) = K\u03b4(Y\u0302 (k\u22121)), and converges exponentially to a unique solution as long as \u03b4 > 2K.\nFrom Lemma 1, we know it is important to standardize the responses Yi so that \u03b4 will be of the same scale for different problems. In practice, we observe that one will not need to choose \u03b4 that satisfies the worst-case condition \u03b4 > K in order for convergence, but making \u03b4 too small does lead to slow convergence rate. For assigning the initial guess Y\u0302 (0), two simplest ways are to either take the random forest estimator we got or a constant vector equaling to the sample mean. Throughout the rest of this paper, we will choose the weights to be random forest weights (3)."}, {"heading": "3.2. Tukey\u2019s biweight", "text": "Non-convex function has played an important role in the context of robust regression (Huber, 2011; Hampel et al., 2011). Unlike convex losses, the penalization on the errors can be bounded and hence the contribution of outliers in the estimating equation will eventually vanish. Our forest regression framework 1 also incorporates the nonconvex losses which will show through the Tukey\u2019s biweight function T\u03b4(\u00b7) (Huber, 2011), which is an example\nof redescending loss whose derivative will vanish to zero as the input goes outside the interval [\u2212\u03b4, \u03b4]. It is defined in the following way:\nd\ndy T\u03b4(y) = y ( 1\u2212 y 2 \u03b42 )2 for |y| \u2264 \u03b4,\n0 elsewhere.\nSimilarly, by rearranging the estimating equation, we have\nY\u0302tukey(x) =\n\u2211n i=1 w\ntukey(Xi, x)Yi\u2211n i=1 w tukey(Xi, x)\nwhere\nwtukey(Xi, x) = w(Xi, x) max 1\u2212 ( Y\u0302tukey \u2212 Yi \u03b4 )2 , 0  with an extra scaling factor (see Figure 2)\nmax { 1\u2212 (u \u03b4 )2 , 0 } . (13)\nIn another word, the final estimator actually only depends on data with responses inside [\u2212\u03b4, \u03b4], and the importance of any data (Xi, Yi) will be shrinking to zero when |Y\u0302tukey(x)\u2212 Yi| gets closer to the boundary value \u03b4."}, {"heading": "4. Truncated squared loss and nearest neighbors", "text": "In this section, we will further use the framework 1 to investigate truncated squared error loss, and use this example to motivate the relation between random forest generalization performance and the number of adaptive nearest neighbors."}, {"heading": "4.1. Truncated squared error", "text": "For the truncated squared error loss\nS\u03b4(y) =\n{ 1 2y\n2 for |y| \u2264 \u03b4, 1 2\u03b4 2 elsewhere\nthe corresponding estimating equation is\u2211 |Y\u0302trunc(x)\u2212Yi|\u2264\u03b4 w(Xi, x)(Y\u0302trunc(x)\u2212 Yi) = 0.\nIf we define a new weight\nwtrunc(Xi, x) = w(Xi, x) 1I{|Y\u0302trunc(x)\u2212 Yi| \u2264 \u03b4}, (14)\nthen the estimator for truncated squared loss is\nY\u0302trunc(x) =\n\u2211n i=1 w\ntrunc(Xi, x)Yi\u2211n i=1 w trunc(Xi, x) . (15)\nThe estimator (15) is like a trimmed version of the random forest estimator (2). We first sort {Yi}ni=1 and trim off the responses where |Y\u0302trunc(x) \u2212 Yi| > \u03b4. Therefore, for any truncation level \u03b4, the estimator Y\u0302trunc(x) only depends on data satisfying |Y\u0302trunc(x) \u2212 Yi| \u2264 \u03b4 with the same local random forest weights (1)."}, {"heading": "4.2. Random Forest Nearest Neighbors", "text": "In classical random forest, all the data with positive weights (3) are included when calculating the final estimator Y\u0302 (x). However, from section 4.1, we know in order to achieve robustness, some of the data should be dropped out of consideration. For example, using the truncated squared error loss, we will only consider the data satisfying |Yi \u2212 Y\u0302trunc(x)| \u2264 \u03b4. In classical random forest, the criterion of tree split is to reduce the mean squared error, then in most cases, data points inside one terminal node will tend to have more similar responses. So informally larger |Y\u0302trim(x)\u2212Yi|will indicate smaller local weightw(Xi, x). Therefore, instead of solving for (15), we investigate a related estimator\nY\u0302wt(x) = \u2211 w(Xi,x)\u2265 w(Xi, x)Yi\u2211 w(Xi,x)\u2265 w(Xi, x)\n(16)\nwhere > 0 is a constant in (0, 1). Recall that in (Lin & Jeon, 2006), they show all the observations with positive weights are considered voting points for random forest estimator. However, (16) implies that we should drop observations with weights smaller than a threshold in order for the robustness. More formally, let \u03c3 be a permutation such that w(X\u03c3(1), x) \u2265 \u00b7 \u00b7 \u00b7 \u2265 w(X\u03c3(n0), x) > 0, then (2) is equivalent to\nY\u0302 (x) = n0\u2211 i=1 w(X\u03c3(i), x)Y\u03c3(i).\nThen we can define the k random forest nearest neighbors (k-RFNN) of x to be {X\u03c3(1), \u00b7 \u00b7 \u00b7 , X\u03c3(k)}, k \u2264 n0, and get predictor\nY\u0302k(x) = k\u2211 i=1 w\u0303(X\u03c3(i), x)Y\u03c3(i), (17)\nwhere w\u0303(X\u03c3(i), x) = w(X\u03c3(i), x)/ \u2211k j=1 w(X\u03c3(i), x). In the numerical experiments (Section 5.3), we will test the performance of the estimator (17) with different k, and show that by merely choosing the right number of nearest neighbors, one can largely improve the performance of classical random forest.\nShi and Horvath (2006) proposed a similar ensemble tree based nearest neighbor method. In their approach, if the observations Xi and Xj lie in the same leaf, then the similarity between them is increased by one. At the end, the similarities are normalized by dividing the total number of trees in the forest. Therefore, their weights (similarities) w(Xi, x) will be m\u22121 \u2211m t=1 1I{Xi\u2208Rl(x,\u03b8)} contrast to (3). So different from their approach, for random forest, the similarity between Xi and Xj will be increased by 1/#{p : Xp \u2208 Rl(Xi,\u03b8)} if they both lie in the same leaf l(Xi, \u03b8). This means the increment in the similarity also depends on the number of data points in the leaf."}, {"heading": "5. Experiments", "text": "In this section, we plug in the quantile loss, Huber loss and Tukey\u2019s biweight loss into the general forest framework and compare these algorithms with random forest. Unless otherwise stated, for both Huber and Tukey forest, the error tolerance is set to be 10\u22126, and every forest is an ensemble of 1000 trees with maximum terminal node size 10. The robust parameter \u03b4 are set to be 0.005 and 0.8 for Huber and Tukey forest, respectively."}, {"heading": "5.1. One dimensional toy example", "text": "We generate 1000 training data points from a Uniform distribution on [\u22125, 5] and another 1000 testing points from the same distribution. The true underlying model is Y = X2 + , \u223c N (0, 1). But on the training samples, we choose 20% of the data and add noise 2T2 to the responses, where T2 follows t-distribution with degree of freedom 2.\nIn Figure 3, we plot the true squared curve and different forest predictions. It is clear that Huber and Tukey forest achieve competitive robustness as quantile random forest, and can almost recover the true underlying distribution, but random forest is largely impacted by the outliers. We also repeat the experiments for 20 times, and report the average mean squared error (MSE), mean absolute deviation (MAD) and median absolute percentage error (MAPE) in Table 1."}, {"heading": "5.2. Multivariate example", "text": "We generate data from 10 dimensional Normal distribution, i.e. X \u223c N10(~0,\u03a3). Then we test out algorithms on following models.\n(1) Y = \u221110 i=1X 2 i + and \u223c N (0, 1), \u03a3 = I.\n(2) Y = \u221110 i=1X 2 i + and \u223c N (0, 1), \u03a3 = Toeplitz(\u03c1 = 0.7).\nThen for each model, we randomly choose \u03b7 proportion of the training samples and add noise 15T2 where T2 follows t-distribution with degree of freedom 2. The noise level \u03b7 \u2208 {0, 0.05, 0.1, 0.15, 0.2}. The results are summarized in Table 2 and 3. On the clean data, random forest still play the best, however, Huber forest\u2019s performance is also competitive and lose less efficiency than QRF and Tukey forest. On the noisy data, all three robust methods outperform random forest. Among them, Huber forest is most robust and stable."}, {"heading": "5.3. Nearest neighbors", "text": "In this section, we check how the number of adaptive nearest neighbors k in (17) will have impact on the performance of k-RFNN. We consider the same two models (1) and (2), and keep both training sample size and testing sample size to be 1000. The relations between MSE, MAD and the number of adaptive nearest neighbors are illustrated in Figure 4. Recall that k-RFNN with all 1000 neighbors is equivalent to random forest. From the figures, we clearly observe a kink at k = 15, which is much less than 1000."}, {"heading": "5.4. Real data", "text": "We take two regression datasets from UCI machine learning repository (Lichman, 2013), and one real estate dataset from OpenIntro. For each dataset, we randomly choose 2/3 observations for training and the rest for testing. MSE and MAD are reported by averaging over 20 trials. The results are presented in Table 4. To further test the robustness, we then repeat the experiment but add extra T2 noise to 20%\nof the standardized training data response variables everytime. The results are in Table 5. Robust forests outperform random forest in most of the cases except for Ames data sets, on which quantile random forest behaves poorly."}, {"heading": "6. Conclusion and discussion", "text": "The experimental results show that Huber forest, Tukey forest and quantile random forest are all much more robust than random forest in the presence of outliers. However, without outliers, Huber forest preserves more efficiency than the other two robust methods. We did not cross validate the parameter \u03b4 for different noise levels, so one would\nexpect even better performance after carefully tuning the parameter.\nBesides random forest weights, other data dependent similarities could also be used in Algorithm 1. We could also design loss functions which optimizes a metric for specific problems. The fixed-point method could be replaced by other more efficient algorithms. The framework could be easily extended to classification problems. All these will be potential future work."}, {"heading": "7. Appendix", "text": ""}, {"heading": "7.1. Proof of Lemma 1", "text": "Proof. Because Y\u0302 (k)(x) = K\u03b4(Y\u0302 (k\u22121)) which is a fixedpoint method, we only need to show \u2223\u2223\u2223K \u2032\u03b4(y)\u2223\u2223\u2223 < 1 in order for the existence and uniqueness of the solution. Define the normalized weight\nw\u0303i = wi\u221a\n1 + ( y\u2212Yi \u03b4\n)2 / n\u2211\ni=1 wi\u221a 1 + ( y\u2212Yi \u03b4 )2 , we have \u2211n i=1 w\u0303i = 1, and\n\u2223\u2223\u2223K \u2032\u03b4(y)\u2223\u2223\u2223 \u2264\n\u2223\u2223\u2223\u2223\u2223\u2223 n\u2211 i=1 w\u0303iYi  n\u2211 j=1 (1I(i = j)\u2212 w\u0303j) y \u2212 Yj \u03b42 + (y \u2212 Yj)2 \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 2\nn\u2211 i=1 w\u0303i |Yi| max i=1,\u00b7\u00b7\u00b7 ,n ( |y \u2212 Yi| \u03b42 + (y \u2212 Yi)2 )\n= 2 n\u2211 i=1 w\u0303i |Yi| 1 mini=1,\u00b7\u00b7\u00b7 ,n ( \u03b42 |y\u2212Yi| + |y \u2212 Yi| )\n\u2264 max i=1,\u00b7\u00b7\u00b7 ,n\n|Yi| 1\n\u03b4 .\nTherefore, \u2223\u2223\u2223K \u2032\u03b4(y)\u2223\u2223\u2223 < 12 if \u03b4 > 2 maxi=1,\u00b7\u00b7\u00b7 ,n |Yi| = 2K."}, {"heading": "Acknowledgements", "text": "We would like to thank Stan Humphrys and Zillow for supporting this research, as well as three anonymous referees for their insightful comments. Part of the implementation in this paper is based on Zillow code library."}], "year": 2017, "references": [{"title": "mp-dissimilarity: A data dependent dissimilarity measure", "authors": ["Aryal", "Sunil", "Ting", "Kai Ming", "Haffari", "Gholamreza", "Washio", "Takashi"], "venue": "In Data Mining (ICDM),", "year": 2014}, {"title": "Improving the robust random forest regression algorithm. Systems and Information Engineering Technical Papers, Department of Systems and Information Engineering, University of Virginia", "authors": ["Brence", "MAJ John R", "Brown", "Donald E"], "year": 2006}, {"title": "Deterministic edge-preserving regularization in computed imaging", "authors": ["Charbonnier", "Pierre", "Blanc-F\u00e9raud", "Laure", "Aubert", "Gilles", "Barlaud", "Michel"], "venue": "IEEE Transactions on image processing,", "year": 1997}, {"title": "Xgboost: A scalable tree boosting system", "authors": ["Chen", "Tianqi", "Guestrin", "Carlos"], "venue": "In Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "year": 2016}, {"title": "Local polynomial modelling and its applications: monographs on statistics and applied probability 66, volume 66", "authors": ["Fan", "Jianqing", "Gijbels", "Irene"], "year": 1996}, {"title": "A desicion-theoretic generalization of on-line learning and an application to boosting", "authors": ["Freund", "Yoav", "Schapire", "Robert E"], "venue": "In European conference on computational learning theory,", "year": 1995}, {"title": "Experiments with a new boosting algorithm", "authors": ["Freund", "Yoav", "Schapire", "Robert E"], "venue": "In icml,", "year": 1996}, {"title": "The elements of statistical learning, volume 1. Springer series in statistics", "authors": ["Friedman", "Jerome", "Hastie", "Trevor", "Tibshirani", "Robert"], "year": 2001}, {"title": "Greedy function approximation: a gradient boosting machine", "authors": ["Friedman", "Jerome H"], "venue": "Annals of statistics,", "year": 2001}, {"title": "Robust regression trees based on m-estimators", "authors": ["Galimberti", "Giuliano", "Pillati", "Marilena", "Soffritti", "Gabriele"], "year": 2007}, {"title": "Robust statistics: the approach based on influence functions, volume 114", "authors": ["Hampel", "Frank R", "Ronchetti", "Elvezio M", "Rousseeuw", "Peter J", "Stahel", "Werner A"], "year": 2011}, {"title": "Local regression: Automatic kernel carpentry", "authors": ["Hastie", "Trevor", "Loader", "Clive"], "venue": "Statistical Science, pp", "year": 1993}, {"title": "Robust estimation of a location parameter", "authors": ["Huber", "Peter J"], "venue": "The Annals of Mathematical Statistics,", "year": 1964}, {"title": "Clustering using a similarity measure based on shared near neighbors", "authors": ["Jarvis", "Raymond Austin", "Patrick", "Edward A"], "venue": "IEEE Transactions on computers,", "year": 1973}, {"title": "Boosting in the presence of outliers: adaptive classification with nonconvex loss functions", "authors": ["Li", "Alexander Hanbo", "Bradic", "Jelena"], "venue": "Journal of the American Statistical Association,", "year": 2016}, {"title": "An information-theoretic definition of similarity", "authors": ["Lin", "Dekang"], "venue": "In ICML,", "year": 1998}, {"title": "Random forests and adaptive nearest neighbors", "authors": ["Lin", "Yi", "Jeon", "Yongho"], "venue": "Journal of the American Statistical Association,", "year": 2006}, {"title": "Local regression and likelihood", "authors": ["Loader", "Clive"], "venue": "Springer Science & Business Media,", "year": 2006}, {"title": "Boosting algorithms as gradient descent", "authors": ["Mason", "Llew", "Baxter", "Jonathan", "Bartlett", "Peter L", "Frean", "Marcus R"], "venue": "In NIPS, pp", "year": 1999}, {"title": "Quantile regression forests", "authors": ["Meinshausen", "Nicolai"], "venue": "Journal of Machine Learning Research,", "year": 2006}, {"title": "Kernel estimation of partial means and a general variance estimator", "authors": ["Newey", "Whitney K"], "venue": "Econometric Theory,", "year": 1994}, {"title": "Robustness of random forests for regression", "authors": ["Roy", "Marie-H\u00e9l\u00e8ne", "Larocque", "Denis"], "venue": "Journal of Nonparametric Statistics,", "year": 2012}, {"title": "Multivariate locally weighted least squares regression", "authors": ["Ruppert", "David", "Wand", "Matthew P"], "venue": "The annals of statistics,", "year": 1994}, {"title": "Unsupervised learning with random forest predictors", "authors": ["Shi", "Tao", "Horvath", "Steve"], "venue": "Journal of Computational and Graphical Statistics,", "year": 2006}, {"title": "The kernel estimate of a regression function in likelihood-based models", "authors": ["Staniswalis", "Joan G"], "venue": "Journal of the American Statistical Association,", "year": 1989}, {"title": "Local likelihood estimation", "authors": ["Tibshirani", "Robert", "Hastie", "Trevor"], "venue": "Journal of the American Statistical Association,", "year": 1987}, {"title": "Dynamic integration with random forests", "authors": ["Tsymbal", "Alexey", "Pechenizkiy", "Mykola", "Cunningham", "P\u00e1draig"], "venue": "In European conference on machine learning,", "year": 2006}, {"title": "Modeling of strength of high-performance concrete using artificial neural networks", "authors": ["Yeh", "I-C"], "venue": "Cement and Concrete research,", "year": 1998}], "id": "SP:9f463fc391ba2f91432141bc1be7bbf482dd13a7", "authors": [{"name": "Alexander Hanbo Li", "affiliations": []}, {"name": "Andrew Martin", "affiliations": []}], "abstractText": "This paper introduces a new general framework for forest-type regression which allows the development of robust forest regressors by selecting from a large family of robust loss functions. In particular, when plugged in the squared error and quantile losses, it will recover the classical random forest (Breiman, 2001) and quantile random forest (Meinshausen, 2006). We then use robust loss functions to develop more robust foresttype regression algorithms. In the experiments, we show by simulation and real data that our robust forests are indeed much more insensitive to outliers, and choosing the right number of nearest neighbors can quickly improve the generalization performance of random forest.", "title": "Forest-type Regression with General Losses  and Robust Forest"}