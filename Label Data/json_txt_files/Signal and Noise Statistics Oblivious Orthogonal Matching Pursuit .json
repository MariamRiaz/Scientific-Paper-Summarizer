{"sections": [{"heading": "1. Introduction", "text": "This article deals with the estimation of the regression vector \u03b2 \u2208 Rp in the linear regression model y = X\u03b2 + w, where X \u2208 Rn\u00d7p is a known design matrix with unit Euclidean norm columns, w is the noise vector and y is the observation vector. Throughout this article, we assume that the entries of the noise w are independent, zero mean and Gaussian distributed with variance \u03c32. We consider the high dimensional and sample starved scenario of n < p or n p where classical techniques like ordinary least squares (OLS) are no longer applicable. This problem of estimating high dimensional vectors in sample starved scenarios is ill-posed even in the absence of noise unless strong structural assumptions are made on X and \u03b2. A widely used and practically valid assumption is sparsity. The vector \u03b2 \u2208 Rp is sparse if the support of \u03b2 given by S = supp(\u03b2) = {k : \u03b2k 6= 0} has cardinality k0 = card(S) p.\n*Equal contribution 1Department of Electrical Engineering, IIT Madras, India 2Department of Electrical Engineering, IIT Madras, India. Correspondence to: Sreejith Kallummil <sreejith.k.venugopal@gmail.com>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nA number of algorithms like least absolute shrinkage and selection operator (LASSO)(Tropp, 2006; Tibshirani, 1996), Dantzig selector (DS)(Candes & Tao, 2007), subspace pursuit (SP)(Dai & Milenkovic, 2009), OMP (Pati et al., 1993; Mallat & Zhang, 1993; Tropp, 2004; Cai & Wang, 2011), elastic net (Zou & Hastie, 2005) etc. are proposed to efficiently estimate \u03b2. Tuning the hyper parameters of aforementioned algorithms to achieve optimal performance require a priori knowledge of signal parameters like sparsity k0 or noise statistics like \u03c32 etc. Unfortunately, these parameters are rarely known a priori. To the best of our knowledge, no computationally efficient technique to estimate k0 is reported in open literature. However, limited success on the estimation of \u03c32 has been reported in literature (Dicker, 2014; Fan et al., 2012; Dicker & Erdogdu, 2016; Bayati et al., 2013). However, the performance of these \u03c32 estimates when used for tuning hyper parameters in LASSO, DS, OMP etc. are largely unknown. Generalised techniques for hyper parameter selection like cross validation (CV)(Arlot et al., 2010), re-sampling (Meinshausen & Bu\u0308hlmann, 2010) etc. are computationally challenging. Further, CV is reported to have poor variable selection behaviour(Chichignoud et al., 2016; Arlot et al., 2010). Indeed, algorithms that are oblivious to signal and noise statistics are also proposed in literature. This include algorithms inspired or related to LASSO like square root LASSO(Belloni et al., 2011), AV\u221e (Chichignoud et al., 2016), approximate message passing (Mousavi et al., 2013; Bayati et al., 2013) etc. and ridge regression inspired techniques like least squares adaptive thresholding (LAT), ridge adaptive thresholding (RAT)(Wang et al., 2016) etc. However, most of existing signal and noise statistics oblivious sparse recovery techniques have only large sample performance guarantees. Further, many of these techniques assume that design matrix X is sampled from a random ensemble, a condition which is rarely satisfied in practice."}, {"heading": "1.1. Contributions of this paper", "text": "This article present a novel technique called residual ratio thresholding (RRT) for finding a \u201cgood\u201d estimate of support S from the data dependent/adaptive sequence of supports generated by OMP. RRT is analytically shown to accomplish exact support recovery, (i.e., identifying S) under the same finite sample and deterministic constraints on X like\nrestricted isometry constants (RIC) or mutual coherence required by OMP with a priori knowledge of k0 or \u03c32. However, the signal to noise ratio (SNR=\u2016X\u03b2\u201622/n\u03c32) required for support recovery using RRT is slightly higher than that of OMP with a priori knowledge of k0 or \u03c32. This extra SNR requirement is shown to decrease with the increase in sample size n. RRT and OMP with a priori knowledge of k0 or \u03c32 are shown to be equivalent as n \u2192 \u221e in terms of the SNR required for support recovery. RRT involves a tuning parameter \u03b1 that can be set independent of ambient SNR or noise statistics. The hyper parameter \u03b1 in RRT have an interesting semantic interpretation of being the high SNR upper bound on support recovery error. Also RRT is asymptotically tuning free in the sense that a very wide range of \u03b1 deliver similar performances as n \u2192 \u221e. Numerical simulations indicate that RRT can deliver a highly competitive performance when compared to OMP having a priori knowledge of k0 or \u03c32, OMP with k0 estimated using CV and the recently proposed LAT algorithm. Further, RRT also delivered a highly competitive performance when applied to identify outliers in real data sets, an increasingly popular application of sparse estimation algorithms(Mitra et al., 2010; 2013).\nThe remainder of this article is organised as follows. In section 2 we discuss OMP algorithm. RRT algorithm is presented in Section 3. Section 4 presents theoretical performance guarantees for RRT. Section 5 presents numerical simulation results. All the proofs are provided in the supplementary material.\n1.2. Notations used \u2016x\u2016q = ( p\u2211 k=1 |xk|q ) 1 q is the lq norm of x \u2208 Rp. 0n is the n \u00d7 1 zero vector and In is the n \u00d7 n identity matrix. span(X) is the column space of X. X\u2020 = (XTX)\u22121XT is the Moore-Penrose pseudo inverse of X. XJ denotes the sub-matrix of X formed using the columns indexed by J . N (u,C) represents a Gaussian random vector (R.V) with mean u and covariance matrix C. B(a, b) denotes a Beta R.V with parameters a and b. a \u223c b implies that a and b are identically distributed. [p] represents the floor operator. \u03c6 represents the null set. For any two sets J1 and J2, J1/J2 denotes the set difference. a\nP\u2192 b represents the convergence of R.V a to R.V b in probability."}, {"heading": "2. Orthogonal Matching Pursuit (OMP)", "text": "OMP (Algorithm 1) starts with a null support estimate and in each iteration it adds that column index to the current support which is the most correlated with the previous residual rk\u22121, i.e., tk = arg max\nj |XTj rk\u22121|. Then a\nLS estimate of \u03b2 restricted to the current support Skomp is\nAlgorithm 1 Orthogonal matching pursuit Input: Observation y, matrix X Initialize Somp0 = \u03c6. k = 1 and residual r0 = y repeat\nIdentify the next column tk = arg max j |XTj rk\u22121| Expand current support Skomp = Sk\u22121omp \u222a tk Restricted LS estimate: \u03b2\u0302Skomp = X \u2020 Skomp y.\n\u03b2\u0302{1,...,p}/Skomp = 0p\u2212k.\nUpdate residual: rk = y \u2212X\u03b2\u0302 = (In \u2212Pk)y. Increment k \u2190 k + 1.\nuntil stopping condition (SC) is true Output: Support estimate S\u0302 = Skomp. Vector estimate \u03b2\u0302\ncomputed as an intermediate estimate of \u03b2 and this estimate is used to update the residual. Note that Pk in Algorithm 1 refers to XSkompX \u2020 Skomp , the projection matrix onto span(XSkomp). Since the residual r k is orthogonal to span(XSkomp), X T j r\nk = 0 for all j \u2208 Skomp. Consequently, tk+1 /\u2208 Skomp, i.e., the same index will not be selected in two different iterations. Hence, Sk+1omp \u2283 Skomp, i.e. the support sequence is monotonically increasing. The monotonicity of Skomp in turn implies that the residual norm \u2016rk\u20162 is a non increasing function of k, i.e, \u2016rk+1\u20162 \u2264 \u2016rk\u20162.\nMost of the theoretical properties of OMP are derived assuming a priori knowledge of true sparsity level k0 in which case OMP stops after exactly k0 iterations(Tropp, 2004; Wang, 2015). When k0 is not known, one has to rely on stopping conditions (SC) based on the properties of the residual rk as k varies. For example, one can stop OMP iterations once the residual power is too low compared to the expected noise power. Mathematically, when the noise w is l2 bounded, i.e., \u2016w\u20162 \u2264 2 for some a priori known 2, then OMP can be stopped if \u2016rk\u20162 \u2264 2. For a Gaussian noise vector w \u223c N (0n, \u03c32In), \u03c3 = \u03c3 \u221a n+ 2 \u221a n log(n) satisfies(Cai & Wang, 2011)\nP(\u2016w\u20162 \u2264 \u03c3) \u2265 1\u2212 1\nn , (1)\ni.e., Gaussian noise is l2 bounded with a very high probability. Consequently, one can stop OMP iterations in Gaussian noise once \u2016rk\u20162 \u2264 \u03c3 .\nA number of deterministic recovery guarantees are proposed for OMP. Among these guarantees the conditions based on RIC are the most popular. RIC of order j denoted by \u03b4j is defined as the smallest value of \u03b4 such that\n(1\u2212 \u03b4)\u2016b\u201622 \u2264 \u2016Xb\u201622 \u2264 (1 + \u03b4)\u2016b\u201622 (2)\nhold true for all b \u2208 Rp with \u2016b\u20160 = card(supp(b)) \u2264 j. A smaller value of \u03b4j implies that X act as a near orthogonal\nmatrix for all j sparse vectors b. Such a situation is ideal for the recovery of a j-sparse vector b using any sparse recovery technique. The latest RIC based support recovery guarantee using OMP is given in Lemma 1(Liu et al., 2017).\nLemma 1. OMP with k0 iterations or SC \u2016rk\u20162 \u2264 \u2016w\u20162 can recover any k0 sparse vector \u03b2 provided that \u03b4k0+1 < 1/ \u221a k0 + 1 and \u2016w\u20162 \u2264 omp =\n\u03b2min \u221a 1\u2212 \u03b4k0+1  1\u2212\u221ak0 + 1\u03b4k0+1 1 + \u221a 1\u2212 \u03b42k0+1 \u2212 \u221a k0 + 1\u03b4k0+1 . Since P(\u2016w\u20162 < \u03c3) \u2265 1\u2212 1/n when w \u223c N (0n, \u03c32In), it follows from Lemma 1 that OMP with k0 iterations or SC \u2016rk\u20162 \u2264 \u03c3 can recover any k0-sparse vector \u03b2 with probability greater than 1 \u2212 1/n provided that \u03b4k0+1 < 1/ \u221a k0 + 1 and \u03c3 \u2264 omp. Lemma 1 implies that OMP with a priori knowledge of k0 or \u03c32 can recover support S once the matrix satisfies the regularity condition \u03b4k0+1 < 1/ \u221a k0 + 1 and the SNR is high. It is also known that this RIC condition is worst case necessary. Consequently, Lemma 1 is one of the best deterministic guarantee for OMP available in literature. Note that the mutual incoherence condition given by \u00b5X = max\nj 6=k |XTj Xk| < 1/(2k0 \u2212 1)\nalso ensures exact support recovery at high SNR. Note that the a priori knowledge of k0 or \u03c32 required to materialise the recovery guarantees in Lemma 1 are not available in practical problems. Further, k0 and \u03c32 are very difficult to estimate. This motivates the proposed RRT algorithm which does not require a priori knowledge of k0 or \u03c32."}, {"heading": "3. Residual Ratio Thresholding (RRT)", "text": "RRT is a novel signal and noise statistics oblivious technique to estimate the support S based on the behaviour of the residual ratio statistic RR(k) = \u2016rk\u20162/\u2016rk\u22121\u20162 as k increases from k = 1 to a predefined value k = kmax > k0. As aforementioned, identifying the support using the behaviour of \u2016rk\u20162 requires a priori knowledge of \u03c32. However, as we will show in this section, support detection using RR(k) does not require a priori knowledge of \u03c32. Since the residual norms are non negative and non increasing, RR(k) always satisfy 0 \u2264 RR(k) \u2264 1."}, {"heading": "3.1. Minimal superset and implications", "text": "Consider running kmax > k0 iterations of OMP and let {Skomp}kmaxk=1 be the support sequence generated by OMP. Recall that Skomp is monotonically increasing.\nDefinition 1:- The minimal superset in the OMP support sequence {Skomp}kmaxk=1 is given by Skminomp , where kmin = min({k : S \u2286 Skomp}). When the set {k : S \u2286 Skomp} = \u03c6, we set kmin =\u221e and Skminomp = \u03c6.\nIn words, minimal superset is the smallest superset of support S present in a particular realization of the support estimate sequence {Skomp}kmaxk=1 . Note that both kmin and Skminomp are unobservable random variables. Since card(Skomp) = k, Skomp for k < k0 cannot satisfy S \u2286 Skomp and hence kmin \u2265 k0. Further, the monotonicity of Skomp implies that S \u2282 Skomp for all k \u2265 kmin. Case 1:- When kmin = k0, then Sk0omp = S and Skomp \u2283 S for k \u2265 k0, i.e., S is present in the solution path. Further, when kmin = k0, it is true that Skomp \u2286 S for k \u2264 k0. Case 2:- When k0 < kmin \u2264 kmax, then Skomp 6= S for all k and Sompk \u2283 S for k \u2265 kmin, i.e., S is not present in the solution path. However, a superset of S is present. Case 3:- When kmin = \u221e, then Skomp 6\u2287 S for all k, i.e., neither S nor a superset of S is present in {Skomp}kmaxk=1 . To summarize, exact support recovery using any OMP based scheme including the signal and noise statistics aware schemes is possible only if kmin = k0. Whenever kmin > k0, it is possible to estimate true support S without having any false negatives. However, one then has to suffer from false positives. When kmin =\u221e, any support in {Skomp}kmaxk=1 has to suffer from false negatives and all supports Skomp for k > k0 \u2212 1 has to suffer from false positives also. Note that the matrix and SNR conditions required for exact support recovery in Lemma 1 automatically implies that kmin = k0. We formulate the proposed RRT scheme assuming that kmin = k0.\n3.2. Behaviour of RR(k0)\nNext we consider the behaviour of residual ratio statistic at the k0 iteration, i.e., RR(k0) = \u2016rk0\u20162/\u2016rk0\u22121\u20162 under the assumption that \u2016w\u20162 \u2264 omp and \u03b4k0+1 < 1/ \u221a k0 + 1 which ensures kmin = k0 and Skomp \u2286 S for all k \u2264 k0. Since X\u03b2 = XS\u03b2S \u2208 span(XS), (In \u2212 Pk)X\u03b2 6= 0n if S 6\u2286 Skomp and (In \u2212 Pk)X\u03b2 = 0n if S \u2286 Skomp. This along with the monotonicity of Skomp implies the following. (In \u2212 Pk)X\u03b2 6= 0n for k < kmin = k0 and (In \u2212 Pk)X\u03b2 = 0n for k \u2265 kmin = k0. Thus rk = (In \u2212 Pk)y = (In \u2212 Pk)XS\u03b2S + (In \u2212 Pk)w for k < kmin = k0, whereas, rk = (In \u2212 Pk)w for k \u2265 kmin = k0. Consequently, at k = k0, the numerator \u2016rk0\u20162 ofRR(k0) contains contribution only from the noise term \u2016(In\u2212Pk0)w\u20162, whereas, the denominator \u2016rk0\u22121\u20162 in RR(k0) contain contributions from both the signal term i.e., (In\u2212Pk)XS\u03b2S and the noise term (In\u2212Pk)w. This behaviour of RR(k0) along with the fact that \u2016w\u20162\nP\u2192 0 as \u03c32 \u2192 0 implies the following theorem.\nTheorem 1. Assume that the matrix X satisfies the RIC constraint \u03b4k0+1 < 1/ \u221a k0 + 1 and kmax > k0. Then a). RR(kmin) P\u2192 0 as \u03c32 \u2192 0. b). lim \u03c32\u21920 P(kmin = k0) = 1.\nAlgorithm 2 Residual ratio thresholding Input: Observation y, matrix X Step 1: Run kmax iterations of OMP. Step 2: Compute RR(k) for k = 1, . . . , kmax. Step 3: Estimate kRRT = max{k : RR(k) \u2264 \u0393\u03b1RRT (k)} Output: Support estimate S\u0302 = SkRRTomp . Vector estimate \u03b2\u0302(SkRRTomp ) = X\n\u2020 SkRRTomp y, \u03b2\u0302({1, . . . , p}/SkRRTomp ) = 0p\u2212kRRT .\n3.3. Behaviour of RR(k) for k > kmin\nNext we discuss the behaviour of RR(k) for k > kmin. By the definition of kmin we have S \u2286 Skomp which implies that rk = (In \u2212 Pk)w for k \u2265 kmin. The absence of signal terms in numerator and the denominator of RR(k) = \u2016(In\u2212Pk)w\u20162\u2016(In\u2212Pk\u22121)w\u20162 for k > kmin implies that even when \u2016w\u20162 \u2192 0 or \u03c32 \u2192 0, RR(k) for k > kmin does not converge to zero. This behaviour of RR(k) for k > kmin is captured in Theorem 2 where we provide explicit \u03c32 or SNR independent lower bounds on RR(k) for k > kmin.\nTheorem 2. Let Fa,b(x) denotes the cumulative distribution function of a B(a, b) random variable. Then \u2200\u03c32 > 0,\n\u0393\u03b1RRT (k) = \u221a F\u22121n\u2212k\n2 ,0.5\n( \u03b1\nkmax(p\u2212 k + 1)\n) satisfies\nP(RR(k) > \u0393\u03b1RRT (k),\u2200k > kmin) \u2265 1\u2212 \u03b1. (3)\nTheorem 2 states that the residual ratio statistic RR(k) for k > kmin is lower bounded by the deterministic sequence {\u0393\u03b1RRT (k)} kmax k=kmin+1\nwith a high probability (for small values of \u03b1). Please note that kmin is itself a R.V. Note that the sequence \u0393\u03b1RRT (k) is dependent only on the matrix dimensions n and p. Further, Theorem 2 does not make any assumptions on the noise variance \u03c32 or the design matrix X. Theorem 2 is extremely non trivial considering the fact that the support estimate sequence {Skomp} kmax k=1 produced by OMP is adaptive and data dependent.\nLemma 2. The following important properties of \u0393\u03b1RRT (k) are direct consequences of the monotonicity of CDF and the fact that a Beta R.V take values only in [0, 1]. 1). \u0393\u03b1RRT (k) is defined only in the interval \u03b1 \u2208 [0, kmax(p\u2212 k + 1)]. 2). 0 \u2264 \u0393\u03b1RRT (k) \u2264 1. 3). \u0393\u03b1RRT (k) is a monotonically increasing function of \u03b1. 4). \u0393\u03b1RRT (k) = 0 when \u03b1 = 0 and \u0393 \u03b1 RRT (k) = 1 when \u03b1 = kmax(p\u2212 k + 1)."}, {"heading": "3.4. Residual ratio thresholding framework", "text": "From Theorem 1, it is clear that P(kmin = k0) and\nP(Sompk0 = S) increases with increasing SNR (or decreasing \u03c32), whereas, RR(kmin) decreases to zero with increasing SNR. At the same time, for small values of \u03b1 like \u03b1 = 0.01, RR(k) for k > kmin is lower bounded by \u0393\u03b1RRT (k) with a very high probability at all SNR. Hence, finding the last index k such that RR(k) \u2264 \u0393\u03b1RRT (k), i.e., kRRT = max{k : RR(k) \u2264 \u0393\u03b1RRT (k)} gives k0 and equivalently Sk0omp = S with a probability increasing with increasing SNR. This motivates the proposed signal and noise statistics oblivious RRT algorithm presented in Algorithm 2. Remark 1. An important aspect regarding the RRT in Algorithm 2 is the choice of kRRT when the set {k : RR(k) \u2264 \u0393\u03b1RRT (k)} = \u03c6. This situation happens only at very low SNR. When {k : RR(k) \u2264 \u0393\u03b1RRT (k)} = \u03c6 for a given value of \u03b1, we increase the value of \u03b1 to the smallest value \u03b1new > \u03b1 such that {k : RR(k) \u2264 \u0393\u03b1newRRT (k)} 6= \u03c6. Mathematically, we set kRRT = max{k : RR(k) < \u0393\u03b1newRRT (k)}, where \u03b1new = min\na>\u03b1 {a : {k : RR(k) \u2264 \u0393\u03b1RRT (k)} 6= \u03c6}.\nSince \u03b1 = p kmax gives \u0393\u03b1RRT (1) = 1 and RR(1) \u2264 1, a value of \u03b1new \u2264 pkmax always exists. \u03b1new can be easily computed by first pre-computing {\u0393aRRT (k)} kmax k=1 for say 100 prefixed values of a in the interval (\u03b1, pkmax]. Remark 2. RRT requires performing kmax iterations of OMP. All the quantities required for RRT including RR(k) and the final estimates can be computed while performing these kmax iterations itself. Consequently, RRT has complexity O(kmaxnp). As we will see later, a good choice of kmax is kmax = [0.5(n + 1)] which results in a complexity order O(n2p). This complexity is approximately n/k0 times higher than the O(npk0) complexity of OMP when k0 or \u03c32 are known a priori. This is the computational cost being paid for not knowing k0 or \u03c32 a priori. In contrast, L fold CV requires running (1\u2212 1/L)n iterations of OMP L times resulting in a O(L(1 \u2212 1/L)n2p) = O(Ln2p) complexity, i.e., RRT is L times computationally less complex than CV. Remark 3. RRT algorithm is developed only assuming that the support sequence generated by the sparse recovery algorithm is monotonically increasing. Apart from OMP, algorithms such as orthogonal least squares(Wen et al., 2017) and OMP with thresholding(Yang & de Hoog, 2015) also produce monotonic support sequences. RRT principle can be directly applied to operate these algorithms in a signal and noise statistics oblivious fashion."}, {"heading": "4. Analytical Results for RRT", "text": "In this section we present support recovery guarantees for RRT and compare it with the results available for OMP with a priori knowledge of k0 or \u03c32. The first result in this section deals with the finite sample and finite SNR performance for RRT.\nTheorem 3. Let kmax \u2265 k0 and suppose that the matrix X satisfies \u03b4k0+1 <\n1\u221a k0+1 . Then RRT can recover the true support S with probability greater than 1\u2212 1/n\u2212 \u03b1 provided that \u03c3 < min( omp, rrt), where\nrrt = \u0393\u03b1RRT (k0)\n\u221a 1\u2212 \u03b4k0\u03b2min\n1 + \u0393\u03b1RRT (k0) . (4)\nTheorem 3 implies that RRT can identify the support S at a higher SNR or lower noise level than that required by OMP with a priori knowledge of k0 and \u03c32. For small values of \u03b1 like \u03b1 = 0.01, the probability of exact support recovery, i.e., 1\u2212 \u03b1\u2212 1/n is similar to that of the 1\u2212 1/n probability of exact support recovery in Lemma 1. Also please note that the RRT framework does not impose any extra conditions on the design matrix X. Consequently, the only appreciable difference between RRT and OMP with a priori knowledge of k0 and \u03c32 is in the extra SNR required by RRT which is quantified next using the metric extra = omp/ rrt. Note that the larger the value of extra, larger should be the SNR or equivalently smaller should be the noise level required for RRT to accomplish exact support recovery. Substituting the values of omp and rrt and using the bound \u03b4k0 \u2264 \u03b4k0+1 gives\nextra \u2264 1 + 1\u0393\u03b1RRT (k0)\n1 +\n\u221a 1\u2212\u03b42k0+1\n1\u2212 \u221a k0+1\u03b4k0+1\n. (5)\nNote that\n\u221a 1\u2212\u03b42k0+1\n1\u2212 \u221a k0+1\u03b4k0+1\n= (\n1\u2212\u03b4k0+1 1\u2212 \u221a k0+1\u03b4k0+1 )\u221a 1+\u03b4k0+1 1\u2212\u03b4k0+1\n\u2265 1. Consequently,\nextra \u2264 0.5 ( 1 + 1\n\u0393\u03b1RRT (k0)\n) . (6)\nSince 0 \u2264 \u0393\u03b1RRT (k0) \u2264 1, it follows that 0.5 (\n1 + 1\u0393\u03b1RRT (k0)\n) is always greater than or equal to one.\nHowever, extra decreases with the increase in \u0393\u03b1RRT (k0). In particular, when \u0393\u03b1RRT (k0) = 1, there is no extra SNR requirement. Remark 4. RRT algorithm involves two hyper parameters viz. kmax and \u03b1. Exact support recovery using RRT requires only that kmax \u2265 k0. However, k0 is an unknown quantity. In our numerical simulations, we set kmax = min(p, [0.5(rank(X) + 1)]). This choice is motivated by the facts that k0 < [0.5(rank(X)+1)] is a necessary condition for exact support recovery using any sparse estimation algorithm(Elad, 2010) when n < p and min(n, p) is the maximum possible number of iterations in OMP. Since evaluating rank(X) requires extra computations, one can always use rank(X) \u2264 n to set kmax = min(p, [0.5(n+1)]). Please note that this choice of kmax is independent of the operating SNR, design matrix and the vector to be estimated and the user is not required to tune this parameter. Hence, \u03b1 is the only user specified hyper parameter in RRT algorithm."}, {"heading": "4.1. Large sample behaviour of RRT", "text": "Next we discuss the behaviour of RRT as n\u2192\u221e. From (6), it is clear that the extra SNR required for support recovery using RRT decreases with increasing \u0393\u03b1RRT (k0). However, by Lemma 2 increasing \u0393\u03b1RRT (k0) requires an increase in the value of \u03b1. However, increasing \u03b1 decreases the probability of support recovery given by 1 \u2212 \u03b1 \u2212 1/n. In other words, one cannot have exact support recovery using RRT at lower SNR without increasing the probability of error in the process. An answer to this conundrum is available in the large sample regime where it is possible to achieve both \u03b1 \u2248 0 and \u0393\u03b1RRT (k0) \u2248 1, i.e., no extra SNR requirement and no decrease in probability of support recovery. The following theorem states the conditions required for \u0393\u03b1RRT (k0) \u2248 1 for large values of n. Theorem 4. Define klim = lim\nn\u2192\u221e k0/n, plim =\nlim n\u2192\u221e log(p)/n and \u03b1lim = lim n\u2192\u221e log(\u03b1)/n. Let\nkmax = min(p, [0.5(n + 1)]). Then \u0393\u03b1RRT (k0) =\u221a F\u22121n\u2212k0\n2 ,0.5\n( \u03b1\nkmax(p\u2212 k0 + 1)\n) satisfies the following\nasymptotic limits. Case 1:-). lim\nn\u2192\u221e \u0393\u03b1RRT (k0) = 1, whenever klim < 0.5,\nplim = 0 and \u03b1lim = 0. Case 2:-). 0 < lim\nn\u2192\u221e \u0393\u03b1RRT (k0) < 1 if klim < 0.5,\n\u03b1lim = 0 and plim > 0. In particular, lim n\u2192\u221e \u0393\u03b1RRT (k0) = exp( \u2212plim1\u2212klim ). Case 3:- lim\nn\u2192\u221e \u0393\u03b1RRT (k0) = 0 if klim < 0.5, \u03b1lim = 0 and\nplim =\u221e.\nTheorem 4 states that all choices of (n, p, k0) satisfying plim = 0 and klim < 0.5 can result in lim\nn\u2192\u221e \u0393\u03b1RRT (k0) = 1\nprovided that the parameter \u03b1 satisfies \u03b1lim = 0. Note that \u03b1lim = 0 for a wide variety of \u03b1 including \u03b1 = constant, \u03b1 = 1/n\u03b4 for some \u03b4 > 0, \u03b1 = 1/ log(n) etc. It is interesting to see which (n, p, k0) scenario gives plim = 0 and klim < 0.5. Note that exact recovery in n < p scenario is possible only if k0 \u2264 [0.5(n + 1)]. Thus, the assumption klim < 0.5 will be satisfied in all interesting problem scenarios.\nRegime 1:- lim n\u2192\u221e \u0393\u03b1RRT (k0) = 1 in low dimensional regression problems with p fixed and n \u2192 \u221e or all (n, p) \u2192 (\u221e,\u221e) with lim\nn\u2192\u221e p/n \u2264 1.\nRegime 2:- lim n\u2192\u221e \u0393\u03b1RRT (k0) = 1 in high dimensional case with p increases sub exponentially with n as exp(n\u03b4) for some \u03b4 < 1 or p increases polynomially w.r.t n, i.e., p = n\u03b4 for some \u03b4 > 1. In both cases, plim = lim n\u2192\u221e log(n\u03b4)/n = 0 and plim = lim n\u2192\u221e log(exp(n\u03b4))/n = 0. Regime 3:- lim n\u2192\u221e \u0393\u03b1RRT (k0) = 1 in the extreme high dimensional case where (n, p, k0) \u2192 (\u221e,\u221e,\u221e) satisfy-\ning n \u2265 ck0 log(p) for some constant c > 0. Here plim = lim\nn\u2192\u221e log(p)/n \u2264 lim n\u2192\u221e\n1\nck0 = 0 and klim =\nlim n\u2192\u221e 1/c log(p) = 0. Note that the sampling regime n \u2248 2k0 log(p) is the best known asymptotic guarantee available for OMP(Fletcher & Rangan, 2012). Regime 4:- Consider a sampling regime where (n, p) \u2192 (\u221e,\u221e) such that k0 is fixed and n = ck0 log(p), i.e., p is exponentially increasing with n. Here plim = 1/(ck0) and klim = 0. Consequently, lim\nn\u2192\u221e \u0393\u03b1RRT (k0) = exp ( \u22121 ck0 ) <\n1. A good example of this sampling regime is (Tropp & Gilbert, 2007) where it was shown that OMP can recover a (not every) particular k0 dimensional signal from n random measurements (in noiseless case) when n = ck0 log(p). Note that c \u2264 20 for all k0 and c \u2248 4 for large k0. Even if we assume that only n = 4k0 log(p) measurements are sufficient for recovering a k0 sparse signal, we have lim n\u2192\u221e\n\u0393\u03b1RRT (k0) = exp(\u22120.125) = 0.9512 for k0 = 5 (i.e., extra \u2264 1.0257) and lim\nn\u2192\u221e \u0393\u03b1RRT (k0) = exp(\u22120.125) =\n0.9753 for k0 = 10 (i.e., extra \u2264 1.0127).\nNote that \u0393\u03b1RRT (k0)\u2192 1 as n\u2192\u221e implies that extra \u2192 1 and min( omp, rrt)\u2192 1. This asymptotic behaviour of \u0393\u03b1RRT (k0) and extra imply the large sample consistency of RRT as stated in the following theorem.\nTheorem 5. Suppose that the sample size n \u2192 \u221e such that the matrix X satisfies \u03b4k0+1 <\n1\u221a k0+1 , \u03c3 \u2264 omp and plim = 0. Then, a). OMP running k0 iterations and OMP with SC \u2016rk\u20162 \u2264 \u03c3 are large sample consistent, i.e.. lim\nn\u2192\u221e P(S\u0302 = S) = 1.\nb). RRT with hyper parameter \u03b1 satisfying lim n\u2192\u221e \u03b1 = 0 and \u03b1lim = 0 is also large sample consistent.\nTheorem 5 implies that at large sample sizes, RRT can accomplish exact support recovery under the same SNR and matrix conditions required by OMP with a priori knowledge of k0 or \u03c32. Theorem 5 has a very important corollary. Remark 5. Theorem 1 implies that all choices of \u03b1 satisfying \u03b1\u2192 0 and \u03b1lim = 0 deliver similar performances as n\u2192 \u221e. Note that the range of adaptations satisfying \u03b1 \u2192 0 and \u03b1lim = 0 include \u03b1 = 1/ log(n), \u03b1 = 1/n\u03b4 for \u03b4 > 0 etc. Since a very wide range of tuning parameters deliver similar results as n \u2192 \u221e, RRT is in fact asymptotically tuning free. Remark 6. Based on the large sample analysis of RRT, one can make the following guidelines on the choice of \u03b1. When the sample size n is large, one can choose \u03b1 as a function of n that satisfies both lim\nn\u2192\u221e \u03b1 = 0 and \u03b1lim = 0. Also since\nthe support recovery guarantees are of the form 1\u22121/n\u2212\u03b1, it does not make sense to choose a value of \u03b1 that decays to zero faster than 1/n. Hence, it is preferable to choose values of \u03b1 that decreases to zero slower than 1/n like\n\u03b1 = 1/ log(n), \u03b1 = 1/ \u221a n etc."}, {"heading": "4.2. A high SNR operational interpretation of \u03b1", "text": "Having discussed the large sample behaviour of RRT, we next discuss the finite sample and high SNR behaviour of RRT. Define the events support recovery error E = {S\u0302 6= S} and false positive F = card(S\u0302/S) > 0 and missed discovery or false negativeM = card(S/S\u0302) > 0. The following theorem characterizes the likelihood of these events as SNR increases to infinity or \u03c32 \u2192 0.\nTheorem 6. Let kmax > k0 and the matrix X satisfies \u03b4k0+1 < 1/ \u221a k0 + 1. Then, a). lim \u03c32\u21920\nP(M) = 0. b). lim\n\u03c32\u21920 P(E) = lim \u03c32\u21920 P(F) \u2264 \u03b1.\nTheorem 6 states that when the matrix X allows for exact support recovery in the noiseless or low noise situation, RRT will not suffer from missed discoveries. Under such favourable conditions, \u03b1 is a high SNR upper bound on both the probability of error and the probability of false positives. Please note that such explicit characterization of hyper parameters are not available for hyper parameters in Square root LASSO, RAT, LAT etc."}, {"heading": "5. Numerical Simulations", "text": "In this section, we provide extensive numerical simulations comparing the performance of RRT with state of art sparse recovery techniques. In particular, we compare the performance of RRT with OMP with k0 estimated using five fold CV and the least squares adaptive thresholding (LAT) proposed in (Wang et al., 2016). In synthetic data sets, we also compare RRT with OMP running exactly k0 itera-\ntions and OMP with SC \u2016rk\u20162 \u2264 \u03c3 \u221a n+ 2 \u221a n log(n)(Cai & Wang, 2011). These algorithms are denoted in Figures 1-4 by \u201cCV\u201d, \u201cLAT\u201d, \u201cOMP1\u201d and \u201cOMP2\u201d respectively. RRT1 and RRT2 represent RRT with parameter \u03b1 set to \u03b1 = 1/ log(n) and \u03b1 = 1/ \u221a n respectively. By Theorem 5, RRT1 and RRT2 are large sample consistent."}, {"heading": "5.1. Synthetic data sets", "text": "The synthetic data sets are generated as follows. We consider two models for the matrix X. Model 1 sample each entry of the design matrix X \u2208 Rn\u00d7p independently according toN (0, 1). Matrix X in Model 2 is formed by concatenating In with a n\u00d7 n Hadamard matrix Hn, i.e., X = [In,Hn]. This matrix guarantee exact support recovery using OMP at high SNR once k0 < 1+ \u221a n\n2 (Elad, 2010). The columns of X in both models are normalised to have unit l2-norm. Based on the choice of X and support S , we conduct 4 experiments. Experiments 1-2 involve matrix of model 1 with (n, p) given\nby (200, 300) and (200, 900) respectively with support S sampled randomly from the set {1, . . . , p}. Experiment 3 and 4 involve matrix of model 2 with (n = 128, p = 256). For experiment 3, support S is sampled randomly from the set {1, . . . , p}, whereas, in experiment 4, support S is fixed at {1, 2, . . . , k0}. The noise w is sampled according toN (0n, \u03c32In) with \u03c32 = 1. The non zero entries of \u03b2 are randomly assigned \u03b2j = \u00b11. Subsequently, these entries are scaled to achieve SNR = \u2016X\u03b2\u201622/n = 3. The number of non zero entries k0 in all experiments are fixed at six. We compare the algorithms in terms of the l2 error, the number of false positives and the number of false negatives produced in 100 runs of each experiment.\nFrom the box plots given in Figures 1-4, it is clear that RRT with both values of \u03b1 perform very similar to OMP1. They differ only in one run of experiment 3 where RRT1 and RRT2 suffer from a false negative. Further, RRT1 and RRT2 outperform CV and LAT in all the four experiments in terms of all the three metrics considered for evaluation. This is primarily because LAT and CV are more prone to make false positives, whereas RRT1 and RRT2 does not report any false positives. OMP2 consistently made false negatives which explains its poor performance in terms of l2 error. We have observed that once the SNR is made slightly higher, OMP2 delivers a performance similar to OMP1. Also note that RRT with two significantly different choices of \u03b1 viz. \u03b1 = 1/ \u221a n and \u03b1 = 1/ log(n) delivered similar performances. This observation is in agreement with the claim of asymptotic tuning freeness made in Remark 5. Similar trends are also visible in the simulation results presented in supplementary materials."}, {"heading": "5.2. Outlier detection in real data sets", "text": "We next consider the application of sparse estimation techniques including RRT to identify outliers in low dimensional or full column rank (i.e., n > p) real life data sets, an approach first considered in (Mitra et al., 2010; 2013). Consider a robust regression model of the form y = X\u03b2 + w + gout with usual interpretations for X, \u03b2 and w. The extra term gout \u2208 Rn represents the gross errors in the regression model that cannot be modelled using the distributional assumptions on w. Outlier detection problem in linear regression refers to the identification of the support Sg = supp(gout). Since X has full rank, one can always annihilate the signal component X\u03b2 by projecting onto a subspace orthogonal to span(X). This will result in a simple linear regression model of the form given by\ny\u0303 = (In \u2212XX\u2020)y = (In \u2212XX\u2020)gout + (In \u2212XX\u2020)w, (7) i.e., identifying Sg in robust regression is equivalent to a sparse support identification problem in linear regression. Even though this is a regression problem with n observa-\ntions and n variables, the design matrix (In\u2212XX\u2020) in (7) is rank deficient (i.e., rank(In\u2212XX\u2020) = n\u2212rank(X) < n). Hence, classical techniques based on LS are not useful for identifying Sg. Since card(Sg) and variance of w are unknown, we only consider the application of RRT, OMP with CV and LAT in detecting Sg . We consider four widely studied real life data sets and compare the outliers identified by these algorithms with the existing and widely replicated studies on these data sets. More details on these data sets are given in the supplementary materials. The outliers detected by the aforementioned algorithms and outliers reported in existing literature are tabulated in TABLE 1.\nAmong the four data sets considered, outliers detected by RRT and existing results are in consensus in two data sets viz. Stack loss and Stars data sets. In AR2000 data set, RRT identifies all the outliers. However, RRT also include observations 14 and 50 as outliers. These identifications can be potential false positives. In Brain and Body Weight data set, RRT agrees with the existing results in 4 observations. However, RRT misses two observations viz. 14 and 17 which are claimed to be outliers by existing results. LAT agrees with RRT in all data sets except the stack loss data set where it missed outlier indices 1 and 3. CV correctly identified all the outliers identified by other algorithms in all four data sets. However, it made lot of false positives in three data sets. To summarize, among all the three algorithms considered, RRT delivered an outlier detection performance which is the most similar to the results reported in literature."}, {"heading": "6. Conclusions", "text": "This article proposed a novel signal and noise statistics independent sparse recovery technique based on OMP called residual ratio thresholding and derived finite and large sample guarantees for the same. Numerical simulations in real and synthetic data sets demonstrates a highly competitive performance of RRT when compared to OMP with a priori knowledge of signal and noise statistics. The RRT technique developed in this article can be used to operate sparse recovery techniques that produce a monotonic sequence of support estimates in a signal and noise statistics oblivious fashion. However, the support estimate sequence generated by algorithms like LASSO, DS, SP etc. are not monotonic in nature. Hence, extending the concept of RRT to operate sparse estimation techniques that produce non monotonic support sequence in a signal and noise statistics oblivious fashion is an interesting direction of future research."}], "year": 2018, "references": [{"title": "A survey of cross-validation procedures for model selection", "authors": ["S. Arlot", "A Celisse"], "venue": "Statistics surveys,", "year": 2010}, {"title": "Robust diagnostic regression analysis", "authors": ["A. Atkinson", "M. Riani"], "venue": "Springer Science & Business Media,", "year": 2012}, {"title": "Estimating lasso risk and noise level", "authors": ["M. Bayati", "M.A. Erdogdu", "A. Montanari"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2013}, {"title": "Square-root lasso: pivotal recovery of sparse signals via conic programming", "authors": ["A. Belloni", "V. Chernozhukov", "L. Wang"], "year": 2011}, {"title": "Orthogonal matching pursuit for sparse signal recovery with noise", "authors": ["T.T. Cai", "L. Wang"], "venue": "IEEE Transactions on Information theory,", "year": 2011}, {"title": "The Dantzig selector: Statistical estimation when p is much larger than n", "authors": ["E. Candes", "T. Tao"], "venue": "The Annals of Statistics,", "year": 2007}, {"title": "A practical scheme and fast algorithm to tune the lasso with optimality guarantees", "authors": ["M. Chichignoud", "J. Lederer", "M.J. Wainwright"], "venue": "Journal of Machine Learning Research,", "year": 2016}, {"title": "Subspace pursuit for compressive sensing signal reconstruction", "authors": ["W. Dai", "O. Milenkovic"], "venue": "IEEE Transactions on Information Theory,", "year": 2009}, {"title": "Variance estimation in high-dimensional linear models", "authors": ["L.H. Dicker"], "venue": "Biometrika, 101(2):269\u2013284,", "year": 2014}, {"title": "Maximum likelihood for variance estimation in high-dimensional linear models", "authors": ["L.H. Dicker", "M.A. Erdogdu"], "venue": "In Artificial Intelligence and Statistics,", "year": 2016}, {"title": "Sparse and Redundant Representations: From Theory to Applications in Signal and Image Processing", "authors": ["M. Elad"], "year": 2010}, {"title": "Variance estimation using refitted cross-validation in ultra high dimensional regression", "authors": ["J. Fan", "S. Guo", "N. Hao"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "year": 2012}, {"title": "Orthogonal matching pursuit: A Brownian motion analysis", "authors": ["A.K. Fletcher", "S. Rangan"], "venue": "IEEE Transactions on Signal Processing,", "year": 2012}, {"title": "Some new results about sufficient conditions for exact support recovery of sparse signals via orthogonal matching pursuit", "authors": ["C. Liu", "Y. Fang", "J. Liu"], "venue": "IEEE Transactions on Signal Processing,", "year": 2017}, {"title": "Matching pursuits with timefrequency dictionaries", "authors": ["S.G. Mallat", "Z. Zhang"], "venue": "IEEE Transactions on signal processing,", "year": 1993}, {"title": "Stability selection", "authors": ["N. Meinshausen", "P. B\u00fchlmann"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "year": 2010}, {"title": "Robust regression using sparse learning for high dimensional parameter estimation problems", "authors": ["K. Mitra", "A. Veeraraghavan", "R. Chellappa"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing,", "year": 2010}, {"title": "Analysis of sparse regularization based robust regression approaches", "authors": ["K. Mitra", "A. Veeraraghavan", "R. Chellappa"], "venue": "IEEE Transactions on Signal Processing,", "year": 2013}, {"title": "Parameterless optimal approximate message passing", "authors": ["A. Mousavi", "A. Maleki", "R.G. Baraniuk"], "venue": "arXiv preprint arXiv:1311.0035,", "year": 2013}, {"title": "Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition", "authors": ["Y.C. Pati", "R. Rezaiifar", "P.S. Krishnaprasad"], "venue": "In Signals, Systems and Computers,", "year": 1993}, {"title": "Robust regression and outlier detection, volume 589", "authors": ["P.J. Rousseeuw", "A.M. Leroy"], "venue": "John wiley & sons,", "year": 2005}, {"title": "Regression shrinkage and selection via the lasso", "authors": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp", "year": 1996}, {"title": "Greed is good: Algorithmic results for sparse approximation", "authors": ["J.A. Tropp"], "venue": "IEEE Transactions on Information theory,", "year": 2004}, {"title": "Just relax: Convex programming methods for identifying sparse signals in noise", "authors": ["J.A. Tropp"], "venue": "IEEE transactions on information theory,", "year": 2006}, {"title": "Signal recovery from random measurements via orthogonal matching pursuit", "authors": ["J.A. Tropp", "A.C. Gilbert"], "venue": "IEEE Transactions on Information Theory,", "year": 2007}, {"title": "Support recovery with orthogonal matching pursuit in the presence of noise", "authors": ["J. Wang"], "venue": "IEEE Transactions on Information Theory,", "year": 2015}, {"title": "No penalty no tears: Least squares in high-dimensional linear models", "authors": ["X. Wang", "D. Dunson", "C. Leng"], "venue": "In International Conference on Machine Learning,", "year": 2016}, {"title": "Nearly optimal bounds for orthogonal least squares", "authors": ["J. Wen", "J. Wang", "Q. Zhang"], "venue": "IEEE Transactions on Signal Processing,", "year": 2017}, {"title": "Orthogonal matching pursuit with thresholding and its application in compressive sensing", "authors": ["M. Yang", "F. de Hoog"], "venue": "IEEE Transactions on Signal Processing,", "year": 2015}, {"title": "Regularization and variable selection via the elastic net", "authors": ["H. Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "year": 2005}], "id": "SP:4de4245a2988176d0b5b63e6b2e3506f156171b9", "authors": [{"name": "Sreejith Kallummil", "affiliations": []}, {"name": "Sheetal Kalyani", "affiliations": []}], "abstractText": "Orthogonal matching pursuit (OMP) is a widely used algorithm for recovering sparse high dimensional vectors in linear regression models. The optimal performance of OMP requires a priori knowledge of either the sparsity of regression vector or noise statistics. Both these statistics are rarely known a priori and are very difficult to estimate. In this paper, we present a novel technique called residual ratio thresholding (RRT) to operate OMP without any a priori knowledge of sparsity and noise statistics and establish finite sample and large sample support recovery guarantees for the same. Both analytical results and numerical simulations in real and synthetic data sets indicate that RRT has a performance comparable to OMP with a priori knowledge of sparsity and noise statistics.", "title": "Signal and Noise Statistics Oblivious Orthogonal Matching Pursuit "}