8	59	To realize such a classifier, it is necessary to learn domain-invariantly discriminative representations.
9	34	However, acquiring such representations is not easy because it is often difficult to collect a large number of labeled samples, and because samples from different domains have domain-specific characteristics.
10	4	In unsupervised domain adaptation, we try to train a classifier that works well on a target domain under the condition that we are provided labeled source samples and unlabeled target samples during training.
11	67	Most of the previously developed deep domain adaptation methods operate mainly under the assumption that the adaptation can be realized by matching the distribution of features from different domains.
12	81	These methods have been aimed at obtaining domain-invariant features by minimizing the divergence between domains, as well as a category loss on the source domain (Ganin & Lempitsky, 2014; Long et al., 2015b; 2016).
14	75	That is, even if the distributions are matched with the non-discriminative representations, the classifier may not work well on the target domain.
15	50	Because the direct learning discriminative representations for the target domain, in the absence of target labels, is considered very difficult, we propose assigning pseudo-labels to the target samples and training the target-specific networks as if they were true labels.
16	60	Co-training and tri-training (Zhou & Li, 2005) leverage multiple classifiers to artificially label unlabeled samples and retrain the classifiers.
17	21	However, such methods do not assume labeling samples from different domains.
18	43	Because our goal is to classify unlabeled target samples that have different characteristics from labeled source samples, we propose the use of asymmetric tri-training for unsupervised domain adaptation.
19	44	By asymmetric, we mean that we assign different roles to three different classifiers.
20	71	In this paper, we propose a novel tri-training method for unsupervised domain adaptation, where we assign pseudolabels to unlabeled samples, and train the neural networks utilizing these samples.
21	303	1, two networks are used to label unlabeled target samples, and the remaining network is trained using the pseudo-labeled target samples.
22	42	We evaluated our method using digit classification tasks, traffic sign classification tasks, and sentiment analysis tasks using the Amazon Review dataset, and demonstrated its state-of-the-art performance for nearly all of the conducted experiments.
23	38	In particular, for the adaptation scenario, MNISTâ†’SVHN, our method outperformed other methods by more than 10%.
52	25	We aim to construct a target- specific network by utilizing pseudo-labeled target samples.
53	38	Simultaneously, we expect two labeling networks to acquire target-discriminative representations and gradually increase the accuracy on the target domain.
54	2	Our proposed network structure is shown in Fig.
55	14	Here, F denotes a network that outputs shared features from among three different networks, and F1 and F2 classify the features generated from F .
56	12	Their predictions are utilized to provide pseudo-labels.
57	13	The classifier Ft classifies features generated from F , which is a target-specific network.
