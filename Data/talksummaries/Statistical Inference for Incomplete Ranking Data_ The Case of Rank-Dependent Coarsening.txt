7	28	The notion of “coarsening” is meant to indicate that an incomplete ranking can be associated with a set of complete rankings, namely the set of its consistent extensions—set-valued data of that kind is also called “coarse data” in statistics (Heitjan & Rubin, 1991; Gill et al., 1997).
8	26	The idea of coarsening is similar to the interpretation of partial rankings as “censored data” (Lebanon & Mao, 2008).
10	14	In addition to introducing a general statistical framework for analyzing incomplete ranking data (Section 3), we outline several problems and learning tasks to be addressed in this framework.
16	14	, aK} of K items ak, k ∈ [K] = {1, .
26	38	Modeling an incomplete observation τ by the set of linear extensions E(τ) reflects the idea that τ has been produced from an underlying complete ranking π by some “coarsening” or “imprecisiation” process, which essentially consists of omitting some of the items from the ranking.
27	40	E(τ) then corresponds to the set of all consistent extension π if nothing is known about the coarsening, except that it does not change the relative order of any items.
29	43	Statistical inference for this type of data requires a probabilistic model of the underlying data generating process, that is, a probability distribution on SK .
30	30	Recalling our idea of a coarsening process, it is natural to consider the data generating process as a two step procedure, in which a full ranking π is generated first and turned into an incomplete ranking τ afterward.
31	80	We model this assumption in terms of a distribution on SK × SK , which assigns a degree of probability to each pair (τ, π).
32	48	More specifically, we assume a parameterized distribution of the following form: pθ,λ(τ, π) = pθ(π) · pλ(τ |π) (1) Thus, while the generation of full rankings is determined by the distribution pθ : SK −→ [0, 1] , (2) the coarsening process is specified by a family of conditional probability distributions{ pλ(· |π) : π ∈ SK , λ ∈ Λ } , (3) where λ collects all parameters of these distributions; pθ,λ(τ, π) is the probability of producing the data (τ, π) ∈ SK × SK .
39	24	For example, the coarsening process may leave a ranking π unchanged whenever item a1 is on the last position, and remove a1 from π otherwise.
42	41	The assumption we make here is a property we call rank-dependent coarsening.
47	62	The assumption of rank-dependent coarsening can be seen as orthogonal to standard marginalization: while the latter projects a full ranking to a subset of items, the former projects a ranking to a subset of positions.
83	33	If rank-dependent coarsening is restricted to the generation of pairwise comparisons, the entire distribution pλ is specified by the set of K(K − 1)/2 probabilities { λi,j | 1 ≤ i < j ≤ K, λi,j ≥ 0, ∑ 1≤i<j≤K λi,j = 1 } , (7) where λi,j denotes the probability that the ranks i and j are selected.
89	17	Please note, however, that the marginals pi,j are not directly comparable with the qi,j , because the latter is a distribution on incomplete rankings ( ∑ i,j qi,j = 1) whereas the former is a set of marginal distributions (pi,j +pj,i = 1).
90	28	Instead, pi,j should be compared to q′i,j = qi,j/(qi,j + qj,i), which is the probability that, in the coarsened model, ai is observed as a winner, given it is paired with aj .
91	27	As an illustration, consider a concrete example with K = 3, θ = (14, 5, 1), and degenerate coarsening distribution specified by λ1,2 = 1 (top-2 selection).
92	38	One easily derives the probabilities of pairwise marginals (6) and coarsened (top-2) observations (8) as follows: i, j 1, 2 1, 3 2, 3 2, 1 3, 1 3, 2 pi,j 840 1140 1064 1140 950 1140 300 1140 76 1140 190 1140 qi,j 665 1140 133 1140 19 1140 266 1140 42 1140 15 1140 q′i,j 665 931 133 175 19 34 266 931 42 175 15 34 While the pi,j are completely coherent with a PL model (namely plθ with θ = (14, 5, 1)), the qi,j and q′i,j no longer are.
103	18	This leads us to the following questions: • Practical performance: What is the performance of a rank aggregation method in the finite sample setting, i.e., how close is the prediction π̂ to the ground truth π∗?
106	44	In this section, we discuss different rank aggregation methods that operate on pairwise data, categorized according to the some basic principles.
125	32	If the ci,j are interpreted as (weighted) preferences, the degree of inconsistency of ranking ai before aj is naturally quantified in terms of cj,i.
126	36	Starting from a formalization in terms of a graph whose nodes correspond to the items and whose edges are labeled with the weighted preferences, the weighted feedback arc set problem (Saab, 2001; Fomin et al., 2010) is to find the ranking that causes the lowest sum of penalties: π̂ = arg min π∈SK ∑ (i,j):π(i)<π(j) cj,i For the same reason as in the case of BTL, we also consider the FAS problem with edge weights given by relative winning frequencies p̂i,j and binary preferences I(p̂i,j > 1/2) instead of absolute frequencies ci,j ; we call the former approach FAS(R) and the latter FAS(B).
166	29	The results and observations for this series of experiments are quite similar to those for the PL model.
198	28	Theorem 5: Copeland ranking is consistent.
200	12	Our experimental results so far suggest that consistency does not only hold for Copeland and FAS, but also for most other methods (including BTL), and hence that rank-dependent coarsening is indeed somehow “good-natured”.
201	37	Anyway, for these cases, the proofs are still pending.
203	30	To this end, we proposed a suitable probabilistic model and introduced the property of rank-dependent coarsening, which can be seen as orthogonal to standard marginalization: while the latter projects a ranking to a subset of items, the former projects to a subset of ranks.
204	45	First experimental and theoretical results suggest that agnostic learning can be successful under rank-dependent coarsening: even if ignorance of the coarsening may lead to biased parameter estimates, the ranking task itself can still be solved properly.
206	30	Needless to say, this paper is only a first step.
207	12	Many questions are still open, for example regarding the consistency of ranking methods, not only for the specific setting considered here but even more so for generalizations thereof.
