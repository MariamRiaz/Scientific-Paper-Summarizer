0	49	Bayesian optimization (BO) has become a popular and effective way for black-box optimization of nonconvex, expensive functions in robotics, machine learning, computer vision, and many other areas of science and engineering (Brochu et al., 2009; Calandra et al., 2014; Krause & Ong, 2011; Lizotte et al., 2007; Snoek et al., 2012; Thornton et al., 2013; Wang et al., 2017).
1	21	In BO, a prior is posed on the (unknown) objective function, and the uncertainty given by the associated posterior is the basis for an acquisition function that guides the selection of the next point to query the function.
2	19	The selection of queries and hence the acquisition function is critical for the success of the method.
5	51	Particularly successful recent additions are entropy search (ES) (Hennig & Schuler, 2012) and predictive entropy search (PES) (Hernández-Lobato et al., 2014), which aim to maximize the mutual information between the queried points and the location of the global optimum.
9	11	In high dimensions, this estimation demands a large number of samples from the input space, which can quickly become inefficient.
10	22	We propose a twist to the viewpoint of ES and PES that retains the information-theoretic motivation and empirically successful query-efficiency of those methods, but at a much reduced computational cost.
11	14	The key insight is to replace the uncertainty about the arg max with the uncertainty about the maximum function value.
48	14	Our acquisition function is the gain in mutual information between the maximum y∗ and the next point we query, which can be approximated analytically by evaluating the entropy of the predictive distribution: αt(x) = I({x, y}; y∗ | Dt) (4) = H(p(y | Dt,x))− E[H(p(y | Dt,x, y∗))] (5) ≈ 1 K ∑ y∗∈Y∗ [ γy∗(x)ψ(γy∗(x)) 2Ψ(γy∗(x)) − log(Ψ(γy∗(x))) ] (6) where ψ is the probability density function and Ψ the cumulative density function of a normal distribution, and γy∗(x) = y∗−µt(x) σt(x) .
70	13	To sample more efficiently, we propose a O(M + |X̂| log 1δ )-time strategy, by approximating the CDF by a Gumbel distribution: P̂r[y∗ < z] ≈ G(a, b) = e−e − z−a b .
71	17	This choice is motivated by the Fisher-Tippett-Gnedenko theorem (Fisher, 1930), which states that the maximum of a set of i.i.d.
72	12	Gaussian variables is asymptotically described by a Gumbel distribution (see the appendix for further details).
86	15	Then we optimize f̃ with respect to its input to get a sample of the maximum of the function maxx∈X f̃(x).
89	17	The following methods are equivalent: 1.
97	47	Because g(u) is a monotonically decreasing function, maximizing g(γy∗(x)) is equivalent to minimizing γy∗(x).
98	45	The connection with EST directly leads to a bound on the simple regret of MES, when using only one sample of y∗.
100	26	Let F be the cumulative probability distribution for the maximum of any function f sampled from GP (µ, k) over the compact search space X ⊂ Rd, where k(x,x′) ≤ 1,∀x,x′ ∈ X.
105	68	As a full Bayesian treatment, we can also draw samples of the hyper-parameters using slice sampling (Vanhatalo et al., 2013), and then marginalize out the hyperparameters in our acquisition function in Eq.
110	12	In the past, AddGP has been used and analyzed for GP-UCB (Kandasamy et al., 2015), which assumed the high dimensional blackbox function is a summation of several disjoint lower dimensional functions.
111	10	Utilizing this special additive structure, we overcome the statistical problem of having insufficient data to recover a complex function, and the difficulty of optimizing acquisition functions in high dimensions.
122	13	Similar to the non-additive case, we may draw a posterior sample at ∼ N (νt,Σt) where νt = σ−2ΣtZtyt and Σt = (ZZ Tσ−2 + I)−1.
123	93	Let Bm = {i : φi(x) is active on xAm}.
124	49	The posterior sample for the function component f (m) is f̃ (m)(x) = (aBmt ) TφBm(xAm).
126	24	The algorithm for the additive max-value entropy search method (add-MES) is shown in Algorithm 2.
129	20	In this section, we probe the empirical performance of MES and add-MES on a variety of tasks.
132	51	We compare to methods from the entropy search family, i.e., ES and PES, and to other popular Bayesian optimization methods including GP-UCB (denoted by UCB), PI, EI and EST.
133	53	The parameter for GP-UCB was set according to Theorem 2 in (Srinivas et al., 2010); the parameter for PI was set to be the observation noise σ.
134	32	For the functions with unknown GP hyper-parameters, every 10 iterations, we learn the GP hyper-parameters using the same approach as was used by PES (Hernández-Lobato et al., 2014).
141	17	Similarly, we sample 100, 10, 1 y∗ values for MES-R and MES-G. We average the results on 100 functions sampled from the same Gaussian kernel with scale parameter 5.0 and bandwidth parameter 0.0625, and observation noise N (0, 0.012).
142	29	Figure 1 shows the simple and inference regrets.
143	51	For both regret measures, PES is very sensitive to the the number of x∗ sampled for the acquisition function: 100 samples lead to much better results than 10 or 1.
