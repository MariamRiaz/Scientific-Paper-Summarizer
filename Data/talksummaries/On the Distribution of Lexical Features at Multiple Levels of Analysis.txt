0	98	NLP for studying people has grown rapidly as more than one-third of the human population use social media actively.1 While traditional NLP tasks (e.g. POS tagging, parsing, sentiment analysis) mostly work at the word, sentence, or document level, the increased focus on social scientific applications has shifted attention to new levels of analysis (e.g. user-level and communitylevel) (Koppel et al., 2009; Sarawgi et al., 2011; Schwartz et al., 2013a; Coppersmith et al., 2014; Flekova et al., 2016).
1	87	Figure 1 shows the distribution of two unigrams, ‘the’ and ‘love’ at three levels of analysis.
2	29	While both words have zero counts in most messages, ‘the’ starts to look Normal across users, and both words are approximately Normal at the county level.
3	46	Methods performing optimally at the document level may suffer at the user or community level due to this shift in the distribution of lexical features.2 In this paper, we ask a fundamental statistical question: How does the shift in unit-of-analysis from document-level to user-or-community level shift lexical distributions in social media?3 The central limit theorem suggests that count data is better approximated by a Normal distribution as one increases the number of events, or as one aggregates more features (e.g. combining words using LDA topics or hand-built word sets).
4	15	However, we do not know how far towards a Normal these new levels of analysis bring us.
5	46	The question we ask harks back to work from pioneers in corpus-based computational linguistics, including Shannon (1948) who suggested that probabilistic distributions of ngrams could be used to solve a range of communications problems, and Mosteller and Wallace (1963) who found that a negative binomial distribution seemed to model unigram usage by authors of the Federalist Papers.
6	11	Numerous works have since continued the tradition of examining the distribution of lexical features.
8	21	Jansche 79 (2003) extended this line of work, observing lexical count data often display an extra probability mass concentrated at zero and suggesting ZeroInflated negative binomial distributions can capture this phenomenon better and are easier to implement than alternatives such as overdispersed binomial models.
14	41	Examining data at three different levels of analysis and across three different lexical feature types (unigrams, data-driven topics, and manual lexica), we seek to (1) visually characterize distributions, (2) empirically test which distributions best fit the data, and (3) evaluate classification models utilizing multiple distributions at each level.
16	25	We start with a set of about two million Twitter posts and supplemental information about the users: their ID, county, and gender.
18	19	We limit our data to users who have used at least 1000 words and counties that have at least 30 users and a total word count of 5000.
20	42	We consider three lexical features that are commonly used in NLP for social science: 1- grams (the top 10,000 most common unigrams found with happierFunTokenizing social media tokenizer), 2000 LDA topics downloaded from Schwartz et al. (2013b)), and lexica (64 categories from the linguistic inquiry and word count dictionary (Pennebaker et al., 2007)).
23	12	500 features were sampled from the top 20,000 unigrams 4, 2000 social media LDA topics (Schwartz et al., 2013a), and all 64 categories from the LIWC lexica (Pennebaker et al., 2007).
32	9	The first component is governed by a Bernoulli distribution that generates excess zeros, while the second component generates counts, some of which also could be zero (Jansche, 2003).
34	37	Following the central limit theorem, we seek to determine across the range levels of analysis and feature types, whether the distribution can be approximated by a Normal.
41	67	Table 1 shows the percentage of features in each level that were best fit from an underlying distribution of Normal, Log-Normal, or Power Law.
42	58	We see empirically that there is a trend toward Normal approximation moving from message to county level, as well as 1grams to lexica.
46	11	We consider three predictive tasks using a generative predictive model.
51	10	Gender Identification (User level): This task involves determining the gender of the author utilizing a previously described Twitter dataset (Volkova et al., 2013).
53	11	Ideology Classification (Community level): We utilized county voting records from 2012 along with a dataset of tweets mapped to counties.
54	11	This data consists of 2,175 counties with atleast 10,000 unigrams as is common in community level analyses (Eichstaedt et al., 2015).
56	26	Variable encoding for the classifiers varied from binary encoding of present or not (Bernoulli), to counts (Poisson, Zero-inflated Poisson), multivariate counts (Multinomial), and continuous relative frequencies (Normal).
57	15	All distributions have closed form MLE solutions except for Zero-Inflated Poisson, in which case we used LBFGS optimization to fit both of its parameters (Head and Zerner, 1985).
58	11	We report macro F1-score for each of the underlying distributions in Table 2.
60	18	We observe a similar pattern as that observed in the goodness of fit setting, with a shift in the best performing distribution from Bernoulli (which simply models if a feature exists or not) toward something more Gaussian (Poisson or Normal) as we move along from message-level to county-level analysis and from unigrams to lexica.
61	47	Specifically note that at higher levels of analysis (at user and county levels) as the distribution of features becomes closer to Normal, modeling features as Bernoulli is clearly sub-optimal where as at the message level modeling unigrams as a Bernoulli is superior.
66	25	We showed that the best-fit distribution depends on feature-type (i.e. unigram versus lexica) and the level of analysis (i.e. message-, user-, or community-level).
67	60	Following the central limit theorem, all user-level features were predominantly Log-normal, while a power law best fit unigrams at the message level and a Normal distribution best approximated lexica at the community level.
68	34	Finally, we demonstrated that predictive performance can also vary considerably by the level of analysis and feature-type, following a similar trend from Bernoulli distributions at the messagelevel to Poisson or Normal at the community-level.
69	168	Our results underscore the significance of the level of analysis for the ever-growing focus in NLP on social scientific problems which seek to not only better model words and documents but also the people and communities generating them.
