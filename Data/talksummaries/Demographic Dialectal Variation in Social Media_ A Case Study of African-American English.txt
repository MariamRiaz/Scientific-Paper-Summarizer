15	16	• Demonstrate racial disparity in the efficacy of NLP tools for language identification and dependency parsing—they perform poorly on this text, compared to text associated with white speakers (§4, §5).
16	15	• Improve language identification for U.S. on- line conversational text with a simple ensemble classifier using our demographicallybased distant supervision method, aiming to eliminate racial disparity in accuracy rates (§4.2).
17	12	• Provide a corpus of 830,000 tweets aligned with African-American demographics.
22	17	Our approach to identifying AAE-like text is to first harvest a set of messages from Twitter, cross-referenced against U.S. Census demographics (§2.1), then to analyze words against demographics with two alternative methods, a seedlist approach (§2.2) and a mixed-membership probabilistic model (§2.3).
28	5	(These are selected from a general archive of the “Gardenhose/Decahose” sample stream of public Twitter messages (Morstatter et al., 2013)).
29	9	Geolocated users are a particular sample of the userbase (Pavalanathan and Eisenstein, 2015), but we expect it is reasonable to compare users of different races within this group.
30	10	We look up the U.S. Census blockgroup geographic area that the message was sent in; blockgroups are one of the smallest geographic areas defined by the Census, typically containing a population of 600–3000 people.
31	12	We use race and ethnicity information for each blockgroup from the Census’ 2013 American Community Survey, defining four covariates: percentages of the population that are non-Hispanic whites, non-Hispanic blacks, Hispanics (of any race), and Asian.4 Finally, for each user u, we average the demographic values of all their messages in our dataset into a length-four vector π(census)u .
42	4	In early experiments, we constructed a corpus of 41,774 users (2.3 million messages) by first selecting the n = 100 highest-πw,AA terms occurring at least m = 3000 times across the data set, then collecting all tweets from frequent authors who have at least 10 tweets and frequently use these terms, defined as the case when at least p = 20% of their messages contain at least one of the seedlist terms.
43	6	Unfortunately, the n,m, p thresholds are ad-hoc.
50	5	be similar to their Census-associated demographic weights, and that every message has its own topic distribution.
54	6	Thus the model learns demographically-aligned language models for each demographic category.
56	15	A tweet written by an author in a highly AA neighborhood may be inferred to be non-AAE-aligned if it uses non-AAE-associated terms; as inference proceeeds, this information is used to learn sharper language models.
59	6	We observed convergence of the log-likelihood within 100 to 200 iterations, and ran for 300 total.7 We average together count tables from the last 50 Gibbs samples for analysis of posterior topic memberships at the word, message, and user level; for example, the posterior probability a particular user u uses topic k, P (z = k | u), can be calculated as the fraction of tokens with topic k within messages authored by u.
64	7	The formulation here has the advantage of fast inference with large vocabularies (since the partition function never has to be computed), and gives probabilistic admixture semantics at arbitrary levels of the data.
74	10	In order to remove the effects of non-English languages, and given uncertainty about what the model learned in the Hispanic and Asian-aligned demographic topics, we focused only on AA- and white-aligned language by imposing the additional constraint that each user’s combined posterior proportion of Hispanic or Asian language was less than 5%.
75	7	Our two resulting user corpora contain 830,000 and 7.3 million tweets, for which we are making their message IDs available for further research (in conformance with the Twitter API’s Terms of Service).
79	7	We begin by examining how much AA- and whitealigned lexical items diverge from a standard dictionary.
80	10	We used SCOWL’s largest wordlist with level 1 variants as our dictionary, totaling 627,685 words.8 We calculated, for each word w in the model’s vocabulary, the ratio rk(w) = p(w|z = k) p(w|z 6= k) where the p(.|.)
82	10	We selected heavily AA- and white-aligned words as those where rAA(w) ≥ 2 and rwhite(w) ≥ 2, respectively.
83	9	We find that while 58.2% of heavily white-aligned words were not in our dictionary, fully 79.1% of heavily AA-aligned words were not.
100	6	We further validate our model by verifying that it reproduces well-known AAE syntactic constructions, investigating three well-attested AAE aspectual or preverbal markers: habitual be, future gone, and completive done (Green, 2002).
111	5	We investigate this hypothesis in §4 and §5.
112	9	Language identification, the task of classifying the major world language in which a message is written, is a crucial first step in almost any web or social media text processing pipeline.
116	10	For Arabic dialect classification, work has developed corpora in both traditional and Romanized script (Cotterell et al., 2014; Malmasi et al., 2015) and tools that use n-gram and morphological analysis to identify code-switching between dialects and with English (Elfardy et al., 2014).
129	5	We sampled and annotated 50 tweets from the tweets classified as nonEnglish by each run.
136	117	Natural language processing tools can be improved to better support dialects; for example, Jørgensen et al. (2016) use domain adaptation methods to improve POS tagging on AAE corpora.
137	17	In this section, we contribute a fix to language identification to correctly identify AAE and other social media messages as English.
138	47	We observed that messages where our model infers a high probability of AAE, white-aligned, or “Hispanic”-aligned language almost always are written in English; therefore we construct a simple ensemble classifier by combining it with langid.py.
139	8	For a new message ~w, we predict its demographic-language proportions θ̂ via posterior inference with our trained model, given a symmetric α prior over demographic-topic proportions (see appendix for details).
