3	64	Uniform deviation bounds provide the answer.
4	39	Informally, they are the worst-case difference across all possible models between the empirical loss of a model and its expected loss.
6	13	In this paper, we consider the popular k-Means clustering problem and provide uniform deviation bounds based on weak assumptions on the underlying data generating distribution.
7	13	Traditional Vapnik-Chervonenkis theory provides tools to obtain uniform deviation bounds for binary concept classes such as classification using halfspaces (Vapnik & Chervonenkis, 1971).
9	20	In their seminal work, Pollard (1981) shows that k-Means clustering is strongly consistent, i.e., that the optimal cluster centers on a random sample converge almost surely to the optimal centers of the distribution under a weak assumption.
10	37	This has sparked a long line of research on cluster stability (Ben-David et al., 2006; Rakhlin & Caponnetto, 2007; Shamir & Tishby, 2007; 2008) which investigates the convergence of optimal parameters both asymptotically and for finite samples.
17	16	We provide a new framework to obtain uniform deviation bounds for unbounded loss functions.
33	11	For k-Means, such a result may be shown by bounding the deviation between the expected loss and the empirical error, i.e., X m (Q) E P ⇥ d(x,Q) 2 ⇤ , uniformly for all possible clusterings Q 2 Rd⇥k.
35	71	Uniform deviation bounds for k-Means A simple approach would be to bound the deviation by an absolute error ✏, i.e., to require that X m (Q) E P ⇥ d(x,Q) 2 ⇤  ✏ (1) uniformly for a set of possible solutions (Telgarsky & Dasgupta, 2013).
36	42	However, in this paper, we provide uniform deviation bounds of a more general form: For any distribution P and a sample of m = f(✏, , k, d, P ) points, we require that with probability at least 1 X m (Q) E P ⇥ d(x,Q) 2 ⇤  ✏ 2 2 + ✏ 2 E P ⇥ d(x,Q) 2 ⇤ (2) uniformly for all Q 2 Rd⇥k.
37	31	The terms on the right-hand side may be interpreted as follows: The first term based on the variance 2 corresponds to a scale-invariant, additive approximation error.
61	12	In contrast, we allow the deviation between the empirical and the expected quantization error to scale with E P ⇥ d(x,Q) 2 ⇤ .
63	10	Suppose that there exists a sample size m 2 N, an error tolerance ✏ 2 (0, 1) and a maximal failure probability 2 (0, 1) such that (2) holds for any distribution P .
65	22	Furthermore, with probability at least , the set X m of m independent samples from P consists of m copies of a point at one.
70	23	The kurtosis is the normalized fourth moment and is a scaleinvariant measure of the “tailedness” of a distribution.
83	17	is linear in the kurtosis ˆM 4 and the dimensionality d, nearlinear in the number of clusters k and 1 , and quadratic in 1 ✏ .
84	8	Intuitively, the bound may be interpreted as follows: ⌦ ⇣ ˆ M4 ✏ 2 ⌘ samples are required such that the guarantee holds for a single solution Q 2 Rd⇥k.
85	16	Informally, a generalization of the Vapnik Chervonenkis dimension for k-Means clustering may be bounded by O(dk log k) and measures the “complexity” of the learning problem.
86	15	The multiplicative dk log k + log 1 term intuitively extends the guarantee uniformly to all possible Q 2 Rd⇥k.
92	19	Telgarsky & Dasgupta (2013) bound this deviation by 2 O s M 2 p m ✓ dk log(Mdm) + log 1 ◆ + r 1 m 2 !
94	12	The key difference lies in how scales with the sample size m. While Telgarsky & Dasgupta (2013) show a rate of 2 O ⇣ m 14 ⌘ , we improve it to 2 O ⇣ m 12 ⌘ .
98	10	Let ✏ 2 (0, 1), 2 (0, 1) and k 2 N. Let P be any distribution on Rd with finite p-th order moment bound ˆM p <1 for p 2 {4, 8, .
102	11	Compared to the previous bound based on the kurtosis, Theorem 2 requires m 2 ⌦ 0 @p ˆ M p 4 p ✏ 2 ✓ dk log k + log 1 ◆ + ✓ 1 ◆ 8 p 1 A samples.
103	42	With higher order moment bounds, it is easier to achieve high probability results since the dependence on 1 is only of O ⇣ 1 8 p ⌘ compared to near linear for a kurtosis bound.
105	27	While the result only holds for p 2 {8, 12, 16, .
109	19	Telgarsky & Dasgupta (2013) require that there exists a bound M E P ⇥ d(x, µ) l ⇤ M, 1  l  p. Then, for m sufficiently large, is of O 0 @ s M 8 p m 1 4 p ✓ dk ln(M 4 p dm) + ln 1 ◆ + 2 p 4 m 3 4 2 p ✓ 1 ◆ 4 p 1 A .
110	11	In contrast, we obtain that, for m sufficiently large, 2 O 0 B@ vuutp ˆM p 4 p m ✓ dk log k + log 1 ◆ 1 CA.
111	9	While Telgarsky & Dasgupta (2013) only show a rate of O ⇣ m 12 ⌘ as p!1, we obtain a 2 O ⇣ m 12 ⌘ rate for all higher moment bounds.
113	62	By optimizing p in Theorem 2, we are able to show the following bound.
127	90	For m 12800 ⇣ 8 + R 4 4 ⌘ ✏ 2 ✓ 3 + 30k(d+ 4) log 6k + log 1 ◆ let X = {x 1 , x 2 , .
