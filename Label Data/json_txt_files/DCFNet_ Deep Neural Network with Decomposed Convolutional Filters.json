{"sections": [{"heading": "1. Introduction", "text": "Convolutional Neural Network (CNN) has become one of the most successful computational models in machine learning and artificial intelligence. Remarkable progress has been achieved in the design of successful CNN network structures, such as the VGG-Net (Simonyan & Zisserman, 2014), ResNet (He et al., 2016), and DenseNet (Huang et al., 2016). Less attention has been paid to the design of filter structures in CNNs. Filters, namely the weights in the convolutional layers, are one of the most important ingredients of a CNN model, as filters contain the actual model parameters learned from enormous amounts of data. Filters in CNNs are typically randomly initialized, and then updated using variants and extensions of gradient descent (\u201cback-propagation\u201d).\n1Duke University, Durham, North Carolina, USA. Work partially supported by NSF, DoD, NIH and AFOSR. Correspondence to: Xiuyuan Cheng <xiuyuan.cheng@duke.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nAs a result, trained CNN filters have no specific structures, which often leads to significant redundancy in the learned model (Denton et al., 2014; Han et al., 2015; Iandola et al., 2016). Filters with improved properties will have a direct impact on the accuracy and efficiency of CNN, and the theoretical analysis of filters is also of central importance to the mathematical understanding of deep networks.\nThis paper suggests to decompose convolutional filters in CNN into a truncated expansion with pre-fixed bases in the spatial domain, namely the Decomposed Convolutional Filters network (DCFNet), where the expansion coefficients remain learned from data. By representing the filters in terms of functional bases, which can come from prior data or task knowledge, rather than as pixel values, the number of trainable parameters is reduced to the expansion coefficients; and furthermore, regularity conditions can be imposed on the filters via the truncated expansion. For image classification tasks, we empirically observe that DCFNet is able to maintain the accuracy with a significant reduction in the number of parameters. Such observation holds even when random bases are used.\nIn particular, we adopt in DCFNet the leading FourierBessel (FB) bases (Abramowitz & Stegun, 1964), which correspond to the low-frequency components in the input. We experimentally observe the superior performance of DCFNet with FB bases (DCF-FB) in both image classification and denoising tasks. DCF-FB network reduces the response to the high-frequency components in the input, which are least stable under image variations such as deformation and often do not affect recognition after being\nsuppressed. Such an intuition is further supported by a mathematical analysis of the CNN representation, where we firstly develop a general result for the CNN representation stability when the input image undergoes a deformation, under proper boundedness conditions of the convolutional filters (Propositions 3.1, 3.3, 3.4). After imposing the DCF structure, we show that as long as the trainable expansion coefficients at each layer of a DCF-FB network satisfy a boundedness condition, the L-th-layer output is stable with respect to input deformation and the difference is bounded by the magnitude of the distortion (Theorems 3.7, 3.8).\nApart from FB bases, the DCFNet structure studied in this paper is compatible with general choices of bases, such as standard Fourier bases, wavelet bases, random bases and PCA bases. We numerically test several options in Section 4. The stability analysis for DCF-FB networks can be extended to other bases choices as well, based upon the general theory developed for CNN representation and using similar techniques.\nOur work is related to recent results on the topics of the usage of bases in deep networks, the model reduction of CNN, as well as the stability analysis of the deep representation. We review these connections in Section 1.1. Finally, though the current paper focuses on supervised networks for classification and recognition applications in image data, the introduced DCF layers are a generic concept and can potentially be used in reconstruction and generative models as well. We discuss possible extensions in the last section."}, {"heading": "1.1. Related works", "text": "Deep network with bases and representation stability. The usage of bases in deep networks has been previously studied, including wavelet bases, PCA bases, learned dictionary atoms, etc. Wavelets are a powerful tool in signal processing (Mallat, 2008) and have been shown to be the optimal basis for data representation under generic settings (Donoho & Johnstone, 1994). As a pioneering mathematical model of CNN, the scattering transform (Mallat, 2012; Bruna & Mallat, 2013; Sifre & Mallat, 2013) used pre-fixed weights in the network which are wavelet filters, and showed that the representation produced by a scattering network is stable with respect to certain variations in the input. The extension of the scattering transform has been studied in (Wiatowski & Bo\u0308lcskei, 2015; 2017) which includes a larger class of bases used in the network. Apart from wavelet, deep network with PCA bases has been studied in (Chan et al., 2015). Making a connection to dictionary learning (Aharon et al., 2006), (Papyan et al., 2016) studied deep networks in form of a cascade of convolutional sparse coding layers with theoretical analysis. Deep networks with random weights have been studied in (Giryes et al., 2016), with proved representation stability. The DCFNet studied in this\npaper incorporates structured pre-fixed bases combined by adapted expansion coefficients learned from data in a supervised way, and demonstrates comparable and even improved classification accuracy on image datasets. While the combination of fixed bases and learned coefficients has been studied in classical signal processing (Freeman et al., 1991; Mahalanobis et al., 1987), dictionary learning (Rubinstein et al., 2010) and computer vision (Henriques et al., 2013; Bertinetto et al., 2016), they were not designed with deep architectures in mind. Meanwhile, the representation stability of DCFNet is inherited thanks to the filter regularity imposed by the truncated bases decomposition.\nNetwork redundancy. Various approaches have been studied to suppress redundancy in the weights of trained CNNs, including model compression and sparse connections. In model compression, network pruning has been studied in (Han et al., 2015) and combined with quantization and Huffman encoding in (Han et al., 2016). (Chen et al., 2015) used hash functions to reduce model size without sacrificing generalization performance. Low-rank compression of filters in CNN has been studied in (Denton et al., 2014; Ioannou et al., 2015). (Iandola et al., 2016; Lin et al., 2014) explored model compression with specific CNN architectures, e.g., replacing regular filters with 1\u00d7 1 filters. Sparse connections in CNNs have been recently studied in (Ioannou et al., 2016; Anwar et al., 2017; Changpinyo et al., 2017). On the theoretical side, (Bo\u0308lcskei et al., 2017) showed that a sparsely-connected network can achieve certain asymptotic statistical optimality. The proposed DCFNet relates model redundancy compression to the regularity conditions imposed on the filters. In DCF-FB network, redundancy reduction is achieved by suppressing network response to the high-frequency components in the inputs."}, {"heading": "2. Decomposed Convolutional Filters", "text": ""}, {"heading": "2.1. Notations of CNN", "text": "The output at the l-th layer of a convolutional neural network (CNN) can be written as {x(l)(u, \u03bb)}u\u2208R2,\u03bb\u2208[Ml], where Ml is the number of channels in that layer and [M ] = {1, \u00b7 \u00b7 \u00b7 ,M} for any integer M . A CNN with L layers can be written as a mapping from {x(0)(u, \u03bb)}u\u2208R2,\u03bb\u2208[M0] to {x(L)(u, \u03bb)}u\u2208R2,\u03bb\u2208[ML], recursively defined via x(l)(u, \u03bb) = \u03c3(x(l)1\n2 (u, \u03bb) + b(l)(\u03bb)), \u03c3 being the nonlinear mapping, e.g., ReLU, and\nx (l) 1 2 (u, \u03bb) = Ml\u22121\u2211 \u03bb\u2032=1 \u222b W (l) \u03bb\u2032,\u03bb(v \u2032)x(l\u22121)(u+ v\u2032, \u03bb\u2032)dv\u2032. (1)\nThe filtersW (l)\u03bb\u2032,\u03bb(u) and the biases b (l) are the parameters of the CNN. In practice, both x(l)(u, \u03bb) and W (l)\u03bb\u2032,\u03bb(u) are discretized on a Cartesian grid, and the continuous convolution\nin (1) is approximated by its discrete analogue. Throughout the paper we use the continuous spatial variable u for simplicity. Very importantly, the filters W (l)\u03bb\u2032,\u03bb(u) are locally supported, e.g., on 3\u00d7 3 or 5\u00d7 5 image patches."}, {"heading": "2.2. Decomposition of convolutional filters", "text": "CNNs typically represent and store filters as vectors of the size of the local patches, which is equivalent to expanding the filters under the delta bases. Delta bases are not optimal for representing smooth functions. For example, regular functions have fast decaying coefficients under Fourier bases, and natural images have sparse representation under wavelet bases. DCF layers represent the convolutional filters as a truncated expansion under basis functions which are non-adapted through the training process, while adaption comes via the combination of such bases. Specifically, suppose that the convolutional filters W\u03bb\u2032,\u03bb(u) at certain layer, after a proper rescaling of the spatial variable (detailed in Section 3), are supported on the unit disk D in R2. Given a bases {\u03c8k}k of the space L2(D), the filters can be represented as\nW\u03bb\u2032,\u03bb(u) = K\u2211 k=1 (a\u03bb\u2032,\u03bb)k\u03c8k(u), (2)\nwhere K is the truncation. The decomposition (2) is illustrated in Figure 1, and conceptually, it can be viewed as a two-step scheme of a convolutional layer:\n1. (\u03a8-step) the input is convolved with each of the basis \u03c8k, k = 1, \u00b7 \u00b7 \u00b7 ,K, which are pre-fixed. The convolution for each input channel is independent from other channels, adding computational efficiency.\n2. (a-step) the intermediate output is linearly transformed by an effectively fully-connected weight matrix (a\u03bb\u2032,\u03bb)k mapping from index (\u03bb\u2032, k) to \u03bb, which is adapted to data.\nIn (2), \u03c8k can be any bases, and we numerically test on different choices in Section 4, including data-adapted bases and random bases. All experiments consistently show that the convolutional layers can be drastically decomposed and compressed with almost no reduction on the classification accuracy, and sometimes even using random bases gives strong performance. In particular, motivated by classical results of harmonic analysis, we use FB bases in DCFNet, with which the regularity of the filters W\u03bb\u2032,\u03bb can be imposed though constraining the magnitude the coefficients {(a\u03bb\u2032,\u03bb)k}k (Proposition 3.6). As an example, Gabor filters approximated using the leading FB bases are plotted in the right of Figure 2. In experiments, DCFNet with FB bases shows superior performance in image classification and denoising tasks compared to original CNN and other bases being tested (Section 4). Theoretically, Section 3 analyzes the representation stability of DCFNet with respect to input variations, which provides a theoretical explanation of the advantage of FB bases."}, {"heading": "2.3. Parameter and computation reduction", "text": "Suppose that the original convolutional layer is of size L\u00d7 L\u00d7M \u2032 \u00d7M , as shown in Figure 1, where typically L = 3, 5 and usually less than 11, M \u2032 and M grow from 3 (number of input channels) to a few hundreds in the deep layers in CNN. After switching to the DCFNet as in (2), there are M \u2032\u00d7M\u00d7K tunable parameters (a\u03bb\u2032,\u03bb)k. Thus the number of parameters in that layer is a factor KL2 smaller, which can be significant if K is allowed to be small, particularly when M \u2032 and M are large.\nThe theoretical computational complexity can be calculated directly. Suppose that the input and output activation is W \u00d7 W in spatial size, the original convolutional layer needs M \u2032W 2 \u00b7M(1 + 2L2) flops (the number of convolution operations is M \u2032M , each take 2L2W 2 flops, and the summation over channels take an extra W 2M \u2032M ). In contract, a DCF layer takesM \u2032W 2 \u00b72K(L2 +M) flops, (M \u2032K\nmany convolutions in the \u03a8 step, and 2KM \u2032MW 2 flops in the a step). Thus when M L2, the leading computation cost is KL2 of that of a regular CNN layer.\nThe reduction rate of KL2 in both model complexity and theoretical computational flops is confirmed on actual networks used in experiments, c.f. Table 3."}, {"heading": "3. Analysis of Representation Stability", "text": "The analysis in this section is firstly done for regular CNN and then the conditions on filters are reduced to generic conditions on learnt coefficients in a DCF Net. In the latter, the proof is for the Fourier-Bessel (FB) bases, and can be extended to other bases using similar techniques."}, {"heading": "3.1. Stable representation by CNN", "text": "We consider the spatial deformation operator denoted by D\u03c4 , where \u03c4 : R2 \u2192 R2 and is C2, \u03c1(u) = u\u2212 \u03c4(u), and\nD\u03c4x(u, \u03bb) = x(\u03c1(u), \u03bb), \u2200u, \u03bb.\nWe assume that the distortion is controlled, and specifically,\n(A0) |\u2207\u03c4 |\u221e = supu \u2016\u2207\u03c4(u)\u2016 < 15 , \u2016 \u00b7 \u2016 being the operator norm.\nThe choice of the constant 15 is purely technical. Thus \u03c1\u22121 exists, at least locally. Our goal is to control \u2016x(L)[D\u03c4x(0)]\u2212 x(L)[x(0)]\u2016, namely when the input undergoes a deformation the output at L-the layer is not severely changed. We achieve this in two steps: (1) We show that \u2016D\u03c4x(L)[x(0)] \u2212 x(L)[D\u03c4x(0)]\u2016 is bounded by the magnitude of deformation up to a constant proportional to the norm of the signal, c.f. Proposition 3.3. (2) We show that x(L) is stable under D\u03c4 when L is large, c.f. Proposition 3.4. To proceed, define the L2 norm of x(u, \u03bb) to be\n\u2016x\u20162 = 1 M \u2211 \u03bb\u2208[M ] 1 |\u2126| \u222b R2 |x(u, \u03bb)|2du, (3)\nwhere |\u2126|2 = (2 \u00b7 2J)2 is the area of the image-support domain, c.f. Figure 2. We assume that\n(A1) \u03c3 : R\u2192 R is non-expansive,\nwhich holds for ReLU. We also define the constants\nBl := max{sup \u03bb Ml\u22121\u2211 \u03bb\u2032=1 \u2016W (l)\u03bb\u2032,\u03bb\u20161, sup \u03bb\u2032 Ml\u22121 Ml Ml\u2211 \u03bb=1 \u2016W (l)\u03bb\u2032,\u03bb\u20161},\nCl := max{sup \u03bb Ml\u22121\u2211 \u03bb\u2032=1 \u2016|v||\u2207W (l)\u03bb\u2032,\u03bb(v)|\u20161,\nsup \u03bb\u2032 Ml\u22121 Ml Ml\u2211 \u03bb=1 \u2016|v||\u2207W (l)\u03bb\u2032,\u03bb(v)|\u20161}, (4)\nwhere \u2016|v||\u2207W (v)|\u20161 denotes \u222b R2 |v||\u2207W (v)|dv.\nFirstly, the following proposition shows that the layer-wise mapping is non-expansive whenever Bl \u2264 1, the proof of which is left to Supplementary Material (S.M.).\nProposition 3.1. In a CNN, under (A1), if Bl \u2264 1 for all l,\n(a) The mapping of the l-th convolutional layer (including \u03c3), denoted as x(l)[x(l\u22121)], is non-expansive, i.e., \u2016x(l)[x1] \u2212 x(l)[x2]\u2016 \u2264 \u2016x1 \u2212 x2\u2016 for arbitrary x1 and x2.\n(b) \u2016x(l)c \u2016 \u2264 \u2016x(l\u22121)c \u2016 for all l, where x(l)c (u, \u03bb) = x(l)(u, \u03bb)\u2212x(l)0 (\u03bb) is the centered version of x(l), x (l) 0 being the output at the l-th layer from a zero input at the bottom layer. As a result, \u2016x(l)c \u2016 \u2264 \u2016x(0)c \u2016 = \u2016x(0)\u2016.\nTo switch the operator D\u03c4 with the L-layer mapping x(L)[x(0)], the idea is to control the residual of the switching at each layer, which is the following lemma proved in S.M..\nLemma 3.2. In a CNN, under (A0) (A1), Bl, Cl as in (4),\n\u2016D\u03c4x(l)[x(l\u22121)]\u2212 x(l)[D\u03c4x(l\u22121)]\u2016 \u2264 4(Bl + Cl) \u00b7 |\u2207\u03c4 |\u221e\u2016x(l\u22121)c \u2016,\nwhere x(l)c is as in Proposition 3.1.\nWe thus impose the assumption on the filters to be\n(A2) For all l, Bl and Cl as in (4) are less than 1.\nThe assumption (A2) corresponds to a proper scaling of the convolutional filters so that the mapping in each convolutional layer is non-expansive (Proposition 3.1), and in practice, this can be qualitatively maintained by the standard normalization layers in CNN.\nNow we can bound the residual of a L-layer switching to be additive as L increases:\nProposition 3.3. In a CNN, under (A0), (A1), (A2),\n\u2016D\u03c4x(L)[x(0)]\u2212 x(L)[D\u03c4x(0)]\u2016 \u2264 8L|\u2207\u03c4 |\u221e\u2016x(0)\u2016. (5)\nProof is left to S.M. We remark that it is possible to derive a more technical bound in terms of the constants Bl, Cl without assuming (A2), using the same technique. We present the simplified result here.\nIn the later analysis of DCF Net, (A2) will be implied by a single condition on the bases expansion coefficients, c.f. (A2\u2019).\nTo be able to control \u2016D\u03c4x(L) \u2212 x(L)\u2016, we have the following proposition, proved in S.M.\nProposition 3.4. In a CNN, under (A1),\n\u2016D\u03c4x(l) \u2212 x(l)\u2016 \u2264 2|\u03c4 |\u221eDl\u2016x(l\u22121)c \u2016,\nwhere x(l)c is as in Proposition 3.1, and Dl := max{sup\u03bb \u2211Ml\u22121 \u03bb\u2032=1 \u2016\u2207W (l) \u03bb\u2032,\u03bb\u20161, sup\u03bb\u2032 Ml\u22121 Ml \u2211Ml \u03bb=1 \u2016\u2207W (l) \u03bb\u2032,\u03bb\u20161}.\nOne may notice that |\u03c4 |\u221e is not proportional to |\u2207\u03c4 |\u221e when the deformation happens on a large domain, e.g., a rotation. It turns out that the multi-scale architecture of CNN induces a decrease of the quantity Dl proportional to the inverse of the domain diameter, which compensate the increase of |\u03c4 |\u221e as scale grows, as long as the rescaled filters are properly bounded in integral. Thus a unified deformation theory can be derived for DCFNets, see next section."}, {"heading": "3.2. Multi-scale filters and Fourier Bessel (FB) bases", "text": "Due to the downsampling (\u201cpooling\u201d) in CNN, the support of the l-th layer filters W (l)\u03bb\u2032,\u03bb enlarges as l increases. Suppose that the input is supported on \u2126 which is a (2 \u00b7 2J) \u00d7 (2 \u00b7 2J) domain, and the CNN has L layers. In accordance with the 2\u00d7 2 pooling, we assume that W (l)\u03bb\u2032,\u03bb is supported on D(jl), vanishing on the boundary, where D(j) is a disk of radius 2j , j0 \u2264 \u00b7 \u00b7 \u00b7 \u2264 jL \u2264 J , and D(j0) is of size of patches at the smallest scale. Let {\u03c8k}k be a set of bases supported on the unit disk D(0), and we introduce the rescaled bases\n\u03c8j,k(u) = 2 \u22122j\u03c8k(2 \u2212ju), u \u2208 D(j),\nwhere the normalization 2\u22122j is introduced so that \u2016\u03c8j,k\u20161 = \u2016\u03c8k\u20161, where \u2016f\u20161 := \u222b R2 |f(u)|du. The multiscale filters and bases are illustrated in the left of Figure 2. By (2), we have that\nW (l) \u03bb\u2032,\u03bb(u) = \u2211 k (a (l) \u03bb\u2032,\u03bb)k\u03c8jl,k(u), u \u2208 D(jl). (6)\nWhile DCFNet is compatible with general choices of bases, we focus on the FB bases in this section as an example. FB bases \u03c8k are indexed by k = (m, q) where m and q are the angular and radial frequencies respectively. They are supported on the unit disk D = D(0), and in polar coordinates,\n\u03c8m,q(r, \u03b8) = cm,qJm(Rm,qr)e im\u03b8, r \u2208 [0, 1], \u03b8 \u2208 [0, 2\u03c0],\nwhere Jm is the Bessel function of the first kind, m are integers, q = 1, 2, \u00b7 \u00b7 \u00b7 , Rm,q is the q-th root of Jm, and cm,q is the normalizing constant s.t. \u3008\u03c8m,q, \u03c8m\u2032,q\u2032\u3009 =\u222b D \u03c8m,q(u)\u03c8 \u2217 m\u2032,q\u2032(u)du = \u03c0\u03b4m,m\u2032\u03b4q,q\u2032 . Furthermore, FB bases are eigenfunctions of the Dirichlet Laplacian on D, i.e., \u22124\u03c8k = \u00b5k\u03c8k, where \u00b5m,q = R2m,q. The eigenvalue \u00b5k grows as k increases (Weyl\u2019s law). Thus FB bases can be ordered by k so that \u00b5k increases, of which the leading few are shown in Table 1 and illustrated in Fig. 2. In principle, the frequency q and m should be truncated according to the Nyquist sampling rate. This truncation turned out to be\nk 1 2,3 4,5 6 7,8 9,10 11,12 13,14 m 0 1 2 0 3 1 4 2 q 1 1 1 2 1 2 1 2 \u00b5k 5.78 14.68 26.37 30.47 40.71 49.22 57.58 70.85\nTable 1. The angular frequency m, radial frequency q and Dirichlet eigenvalue \u00b5k of the first 14 Fourier-Bessel bases. Two k corresponds to one pair of (m, q) when m 6= 0 due to that both real and complex parts of the bases are used as real-valued bases.\nnot often used in our setting, due to the significant bases truncation in DCFNet.\nThe key technical quantities in the stability analysis of CNN are \u2016W (l)\u03bb\u2032,\u03bb\u20161 and \u2016|v||\u2207W (l) \u03bb\u2032,\u03bb(v)|\u20161, and with FB bases, these integrals are bounded by a \u00b5k-weighted L2-norm of a (l) \u03bb\u2032,\u03bb defined as \u2016a\u2016FB = ( \u2211 k \u00b5ka 2 k)\n1/2 for all l. The following lemma and proposition are proved in S.M. Lemma 3.5. Suppose that {\u03c8k} are FB bases, the function F (u) = \u2211 k ak\u03c8k(u) is smooth on the unit disk. Then 1\u221a \u03c0 \u2016\u2207F\u20162 = \u2016a\u2016FB , where \u00b5k are the eigenvalues of \u03c8k as eigenfunctions of the negative Dirichlet laplacian on the unit disk. As a result, \u2016\u2207F\u20161 \u2264 \u03c0\u2016a\u2016FB . Proposition 3.6. Using FB bases, \u2016|v||\u2207W (l)\u03bb\u2032,\u03bb(v)|\u20161 and \u2016W (l)\u03bb\u2032,\u03bb\u20161 are bounded by \u03c0\u2016a (l) \u03bb\u2032,\u03bb\u2016FB for all \u03bb\u2032, \u03bb and l.\nNotice that the boundedness of \u2016a\u2016FB implies a decay of |ak| at least as fast as \u00b5\u22121/2k . This justifies the truncation of the FB expansion to the leading few bases, which correspond to the low-frequency modes.\nProposition 3.6 implies that Bl and Cl are all bounded by Al defined as\nAl := \u03c0max{sup \u03bb Ml\u22121\u2211 \u03bb\u2032=1 \u2016a(l)\u03bb\u2032,\u03bb\u2016FB ,\nsup \u03bb\u2032 Ml\u22121 Ml Ml\u2211 \u03bb=1 \u2016a(l)\u03bb\u2032,\u03bb\u2016FB}.\nThen we introduce\n(A2\u2019) For all l, Al \u2264 1,\nand the result of Proposition 3.3 extends to DCFNet: Theorem 3.7. In a DCFNet with FB bases, under (A0),(A1), (A2\u2019), then\n\u2016D\u03c4x(L)[x(0)]\u2212 x(L)[D\u03c4x(0)]\u2016 \u2264 8L|\u2207\u03c4 |\u221e\u2016x(0)\u2016.\nCombined with Proposition 3.4, we have the following deformation stability bound, proved in S.M.: Theorem 3.8. In a DCFNet with FB bases, under (A0),(A1), (A2\u2019),\n\u2016x(L)[x(0)]\u2212 x(L)[D\u03c4x(0)]\u2016 \u2264 (8L|\u2207\u03c4 |\u221e + 2 \u00b7 2\u2212jL |\u03c4 |\u221e)\u2016x(0)\u2016. (7)"}, {"heading": "4. Experiments", "text": "In this section, we experimentally demonstrate that convolutional filters in CNN can be decomposed as a truncated expansion with pre-fixed bases, where the expansion coefficients remain learned from data. Though the number of trainable parameters are significantly reduced, the accuracy in tasks such as image classification and face verification is still maintained. Such empirical observations hold for data-independent Fourier-Bessel (FB) and random bases, and data-dependent PCA bases."}, {"heading": "4.1. Datasets", "text": "We perform an experimental evaluation on DCFNets using the following public datasets:\nMNIST. 28 \u00d7 28 grayscale images of digits from 0 to 9, with 60,000 training and 10,000 testing samples.\nSVHN. The Street View House Numbers (SVHN) dataset (Netzer et al., 2011) contains 32\u00d7 32 colored images of digits 0 to 9, with 73,257 training and 26,032 testing samples. The additional training images were not used.\nCIFAR10. The dataset (Krizhevsky, 2009) contains 32\u00d732 colored images from 10 object classes, with 50,000 training\nand 10,000 testing samples.\nVGG-Face. A large-scale face dataset, which contains about 2.6M face images from over 2.6K people (Parkhi et al., 2015). 1"}, {"heading": "4.2. Object classification", "text": "In our object classification experiments, we evaluate the DCFNet with three types of predefined bases: FourierBessel bases (DCF-FB), random bases which are generated by Gaussian vectors (DCF-RB), and PCA bases which are principal components of the convolutional filters in a pre-trained corresponding CNN model (DCF-PCA).\nThree CNN network architectures are used for classification, Conv-2 and Conv-3 shown in Table 2, and VGG-16 (Simonyan & Zisserman, 2014). To generate the corresponding DCFNet structure from CNN, each CNN conv layer is expended over a set of pre-defined bases, and the obtained trainable expansion coefficients are implemented as a 1\u00d7 1 conv layer. For example, a 5\u00d7 5\u00d7M \u2032 \u00d7M conv layer is expended over K 5 \u00d7 5 bases for trainable coefficients in a 1 \u00d7 1 \u00d7M \u2032K \u00d7M convolutional layer. K denotes the number of basis used, and we evaluate multiple K for different levels of parameter reduction. In order to be compatible with existing deep learning frameworks, pre-fixed bases are currently implemented as regular convolutional layers with zero learning rate. The additional memory cost incurred in such convenient implementation can be eliminated with a more careful implementation, as bases are pre-fixed and the addition across channels can be computed on the fly.\nThe classification accuracy using DCFNets on various datasets are shown in Table 3. We observe that, by using only 3 Fourier-Bessel (FB) bases, we already obtain comparable accuracy as the original full CNN models on all\n1The software is publicly available at https://github. com/xycheng/DCFNet.\ndatasets, while using 12% parameters for 5\u00d75 filters. When more FB bases are used, DCFNets outperform corresponding CNN models, still with significantly less parameters. As FB bases correspond to the low-frequency components in the inputs, DCF-FB network responds less to the highfrequency nuance details, which are often irrelevant for classification tasks. The superiority of DCF-FB network is further shown with less training data. For SVHN with 500 training samples, the testing accuracy (on a 50,000 testing set) of regular CNN and DCF-FB are 63.88% and 66.79% respectively. With 1000 training samples, the test accuracy are 73.53% v.s. 75.45%. Surprisingly, we observe that DCF with random bases also report acceptable performance.\nBoth the FB and random bases are data independent. For comparison purposes, we also evaluate DCFNets with data dependent PCA bases, which are principal components of corresponding convolutional filters in pre-trained CNN models. When the CNN model is pre-trained with all training data, PCA bases (pca-f) shows comparable performance as FB bases. However, the quality of the PCA bases (pca-s) degenerates, when only a randomly selected subset of the training set is used for the pre-training."}, {"heading": "4.3. Image denoising", "text": "To gain intuitions behind the superior classification performance of DCFNet, we conduct a set of \u201ctoy\u201d image denoising experiments on the SVHN image dataset. We take the first three 5 \u00d7 5 convolution blocks from the Conv-3 CNN network in Table 2, which is used in our SVHN object\nclassification experiments. We remove all pooling layers, and append at the end an FC-256 followed with a Euclidean loss layer. We then decompose each 5 \u00d7 5 conv layer in this CNN network over 3 random bases and 3 FB bases respectively, to produce DCF-RB and DCF-FB networks.\nWe use SVHN training images with their gray-scale version as labels to train all three networks to simply reconstruct an input image (in gray-scale). Figure 4 shows how three trained networks behave while reconstructing examples from the SVHN testing images. Without noise added to input images, Figure 4a, all three networks report decent reconstruction, while DCF-RB shows inferior to both CNN and DCF-FB. PSNR values indicate CNN often produces more precise reconstructions; however, those missing high-frequency components in DCF-FB reconstructions are mostly nuance details. With noise added as in figures 4b and 4c, DCF-FB produces significantly superior reconstruction over both CNN and DCF-RB, with about one tenth of the parameter number of CNN.\nThe above empirical observations clearly indicate that Fourier-Bessel bases, which correspond to the lowfrequency components in the inputs, enable DCF to ignore the high-frequency nuance details, which are often less stable under input variations, and mostly irrelevant for tasks such as classification. Such empirical observation provides good intuitions behind the superior classification performance of DCF, and is also consistent with the theoretical analysis on representation stability in Section 3."}, {"heading": "4.4. Face verification", "text": "We present a further evaluation of DCFNet on face verification tasks using \u201cvery deep\u201d network architectures, which comprise a long sequence of convolutional layers. In order to train such complex networks, we adopt a very large scale VGG-face (Parkhi et al., 2015) dataset, which contains about 2.6M face images from over 2.6K people.\nAs shown in Table 4, we adopt the VGG-Very-Deep-16 CNN architecture as detailed in (Parkhi et al., 2015) by modifying layer 32 and 35 to change output features from 4,096 dimension to 512. Such CNN network comprises 16 weight layers, and all except the last Fully-Connected (FC) layer utilize 3\u00d7 3 or 5\u00d7 5 filters.\nThe input to both CNN and DCFNet are face images of size 224 \u00d7 224 (with the average face image subtracted). As shown in Table 5, with FB bases, even only using 13 parameters at weight layers (K = 3 for 3 \u00d7 3, K = 8 for 5\u00d7 5), the DCFNet shows similar verification accuracy as the CNN structure on the challenging LFW benchmark. Note that our CNN model outperforms the VGG-face model in (Parkhi et al., 2015), and such improvement is mostly due to the smaller output dimension we adopted, as both models share similar architecture and are trained on the same face dataset."}, {"heading": "5. Conclusion and Discussion", "text": "The paper studies CNNs where the convolutional filters are represented as a truncated expansion under pre-fixed bases and the expansion coefficients are learned from labeled data. Experimentally, we observe that on various object recognition datasets the classification accuracy are maintained with a significant reduction of the number of parameters, and the performance of Fourier-Bessel (FB) bases is constantly superior. The truncated FB expansion in DCFNet can be viewed as a regularization of the filters. In other words, DCF-FB is less susceptible to the high-frequency components in the input, which are least stable under expected input variations and often do not affect recognition when suppressed. This interpretation is supported by image denoising experiments, where DCF-FB performs preferably over the original CNN and other basis options on noisy inputs. The stability of DCFNet representation is also proved theoretically, showing that the perturbation of the deep features with respect to input variations can be bounded under generic conditions on the decomposed filters.\nTo extend the work, firstly, DCF layers can be incorporated in networks for unsupervised learning, for which the denoising experiment serves as a first step. The stability analysis can be extended by testing the resilience to adversarial noise. Finally, more structures may be imposed across the channels, concurrently with the structures of the filters in space."}], "year": 2018, "references": [{"title": "Handbook of mathematical functions: with formulas, graphs, and mathematical tables, volume 55", "authors": ["Abramowitz", "Milton", "Stegun", "Irene A"], "venue": "Courier Corporation,", "year": 1964}, {"title": "rmk-svd: An algorithm for designing overcomplete dictionaries for sparse representation", "authors": ["Aharon", "Michal", "Elad", "Michael", "Bruckstein", "Alfred"], "venue": "IEEE Transactions on signal processing,", "year": 2006}, {"title": "Structured pruning of deep convolutional neural networks", "authors": ["Anwar", "Sajid", "Hwang", "Kyuyeon", "Sung", "Wonyong"], "venue": "ACM Journal on Emerging Technologies in Computing Systems (JETC),", "year": 2017}, {"title": "Staple: Complementary learners for real-time tracking", "authors": ["Bertinetto", "Luca", "Valmadre", "Jack", "Golodetz", "Stuart", "Miksik", "Ondrej", "Torr", "Philip HS"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "year": 2016}, {"title": "Optimal approximation with sparsely connected deep neural networks", "authors": ["B\u00f6lcskei", "Helmut", "Grohs", "Philipp", "Kutyniok", "Gitta", "Petersen"], "venue": "arXiv preprint arXiv:1705.01714,", "year": 2017}, {"title": "Invariant scattering convolution networks", "authors": ["Bruna", "Joan", "Mallat", "St\u00e9phane"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "year": 2013}, {"title": "Pcanet: A simple deep learning baseline for image classification", "authors": ["Chan", "Tsung-Han", "Jia", "Kui", "Gao", "Shenghua", "Lu", "Jiwen", "Zeng", "Zinan", "Ma", "Yi"], "venue": "IEEE Transactions on Image Processing,", "year": 2015}, {"title": "The power of sparsity in convolutional neural networks", "authors": ["Changpinyo", "Soravit", "Sandler", "Mark", "Zhmoginov", "Andrey"], "venue": "arXiv preprint arXiv:1702.06257,", "year": 2017}, {"title": "Compressing neural networks with the hashing trick", "authors": ["Chen", "Wenlin", "Wilson", "James", "Tyree", "Stephen", "Weinberger", "Kilian", "Yixin"], "venue": "In International Conference on Machine Learning,", "year": 2015}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "authors": ["Denton", "Emily L", "Zaremba", "Wojciech", "Bruna", "Joan", "LeCun", "Yann", "Fergus", "Rob"], "venue": "In NIPS,", "year": 2014}, {"title": "Ideal spatial adaptation by wavelet shrinkage. biometrika", "authors": ["Donoho", "David L", "Johnstone", "Jain M"], "year": 1994}, {"title": "The design and use of filters", "authors": ["Freeman", "William T", "Adelson", "Edward H"], "venue": "IEEE Transactions on Pattern analysis and machine intelligence,", "year": 1991}, {"title": "Deep neural networks with random gaussian weights: a universal classification strategy", "authors": ["Giryes", "Raja", "Sapiro", "Guillermo", "Bronstein", "Alexander M"], "venue": "IEEE Trans. Signal Processing,", "year": 2016}, {"title": "Learning both weights and connections for efficient neural network", "authors": ["Han", "Song", "Pool", "Jeff", "Tran", "John", "Dally", "William"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "authors": ["Han", "Song", "Mao", "Huizi", "Dally", "William J"], "venue": "International Conference on Learning Representations (ICLR),", "year": 2016}, {"title": "Deep residual learning for image recognition", "authors": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,", "year": 2016}, {"title": "Beyond hard negative mining: Efficient detector learning via block-circulant decomposition", "authors": ["Henriques", "Joao F", "Carreira", "Joao", "Caseiro", "Rui", "Batista", "Jorge"], "venue": "In Computer Vision (ICCV),", "year": 2013}, {"title": "Densely connected convolutional networks", "authors": ["Huang", "Gao", "Liu", "Zhuang", "Weinberger", "Kilian Q", "van der Maaten", "Laurens"], "venue": "arXiv preprint arXiv:1608.06993,", "year": 2016}, {"title": "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and <0.5mb model size", "authors": ["Iandola", "Forrest N", "Han", "Song", "Moskewicz", "Matthew W", "Ashraf", "Khalid", "Dally", "William J", "Keutzer", "Kurt"], "year": 2016}, {"title": "Training cnns with low-rank filters for efficient image classification", "authors": ["Ioannou", "Yani", "Robertson", "Duncan", "Shotton", "Jamie", "Cipolla", "Roberto", "Criminisi", "Antonio"], "venue": "arXiv preprint arXiv:1511.06744,", "year": 2015}, {"title": "Deep roots: Improving cnn efficiency with hierarchical filter groups", "authors": ["Ioannou", "Yani", "Robertson", "Duncan", "Cipolla", "Roberto", "Criminisi", "Antonio"], "venue": "arXiv preprint arXiv:1605.06489,", "year": 2016}, {"title": "Learning multiple layers of features from tiny images", "authors": ["Krizhevsky", "Alex"], "venue": "Technical report,", "year": 2009}, {"title": "Minimum average correlation energy filters", "authors": ["Mahalanobis", "Abhijit", "Kumar", "BVK Vijaya", "Casasent", "David"], "venue": "Applied Optics,", "year": 1987}, {"title": "A wavelet tour of signal processing: the sparse way", "authors": ["Mallat", "Stephane"], "venue": "Academic press,", "year": 2008}, {"title": "Group invariant scattering", "authors": ["Mallat", "St\u00e9phane"], "venue": "Communications on Pure and Applied Mathematics,", "year": 2012}, {"title": "Reading digits in natural images with unsupervised feature learning", "authors": ["Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning", "year": 2011}, {"title": "Convolutional neural networks analyzed via convolutional sparse coding", "authors": ["Papyan", "Vardan", "Romano", "Yaniv", "Elad", "Michael"], "year": 2016}, {"title": "Deep face recognition", "authors": ["O.M. Parkhi", "A. Vedaldi", "A. Zisserman"], "venue": "In British Machine Vision Conference,", "year": 2015}, {"title": "Double sparsity: Learning sparse dictionaries for sparse signal approximation", "authors": ["Rubinstein", "Ron", "Zibulevsky", "Michael", "Elad"], "venue": "IEEE Transactions on signal processing,", "year": 2010}, {"title": "Rotation, scaling and deformation invariant scattering for texture discrimination", "authors": ["Sifre", "Laurent", "Mallat", "St\u00e9phane"], "venue": "In CVPR, pp", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "authors": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1409.1556,", "year": 2014}, {"title": "Deep convolutional neural networks based on semi-discrete frames", "authors": ["Wiatowski", "Thomas", "B\u00f6lcskei", "Helmut"], "venue": "In Information Theory (ISIT),", "year": 2015}, {"title": "A mathematical theory of deep convolutional neural networks for feature extraction", "authors": ["Wiatowski", "Thomas", "B\u00f6lcskei", "Helmut"], "venue": "IEEE Transactions on Information", "year": 2017}], "id": "SP:4f9cc1283105ed535623e37942d7f8f0aa6c8593", "authors": [{"name": "Qiang Qiu", "affiliations": []}, {"name": "Xiuyuan Cheng", "affiliations": []}, {"name": "Robert Calderbank", "affiliations": []}, {"name": "Guillermo Sapiro", "affiliations": []}], "abstractText": "Filters in a Convolutional Neural Network (CNN) contain model parameters learned from enormous amounts of data. In this paper, we suggest to decompose convolutional filters in CNN as a truncated expansion with pre-fixed bases, namely the Decomposed Convolutional Filters network (DCFNet), where the expansion coefficients remain learned from data. Such a structure not only reduces the number of trainable parameters and computation, but also imposes filter regularity by bases truncation. Through extensive experiments, we consistently observe that DCFNet maintains accuracy for image classification tasks with a significant reduction of model parameters, particularly with Fourier-Bessel (FB) bases, and even with random bases. Theoretically, we analyze the representation stability of DCFNet with respect to input variations, and prove representation stability under generic assumptions on the expansion coefficients. The analysis is consistent with the empirical observations.", "title": "DCFNet: Deep Neural Network with Decomposed Convolutional Filters"}