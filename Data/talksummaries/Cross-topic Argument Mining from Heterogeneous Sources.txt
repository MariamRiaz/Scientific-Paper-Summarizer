3	61	These processes can be supported by argument mining (AM), a nascent area of natural language processing concerned with the automatic recognition and interpretation of arguments.
8	44	This may be due in part to the limitations of the underlying models and training resources, particularly as they relate to heterogeneous sources.
9	69	That is, most current approaches to AM are designed for use with particular text types, faring poorly when applied to new data (Daxenberger et al., 2017).
10	28	Indeed, as Habernal et al. (2014) observe, while there is a great diversity of perspectives on how arguments can be best characterized and modelled, there is no “one-size-fits-all” argumentation theory that applies to the variety of text sources found on the Web.
35	88	We define an argument as a span of text expressing evidence or reasoning that can be used to either support or oppose a given topic.
44	17	Note that while the fourth example expresses opposition to the topic, under our definition it is properly classified as a non-argument because it is a mere statement of stance that provides no evidence or reasoning.
46	29	We started by randomly selecting eight topics (see Table 2) from online lists of controversial topics.2 For each topic, we made a Google query for the topic name, removed results not archived by the Wayback Machine,3 and truncated the list to the top 50 results.
47	15	This resulted in a set of persistent, topic-relevant, largely polemical Web documents representing a range of genres and text types, including news reports, editorials, blogs, debate forums, and encyclopedia articles.
48	30	We preprocessed each document with Apache Tika (Mattmann and Zitting, 2011) to remove boilerplate text.
55	17	The data for this experiment consisted of 200 sentences randomly selected from each of our eight topics.
59	15	Inter-annotator agreement for our two experts, as measured by Cohen’s κ, was 0.721; this exceeds the commonly used threshold of 0.7 for assuming the results are reliable (Carletta, 1996).
60	24	We proceeded by having the two experts resolve their disagreements, resulting in a set of “expert” gold-standard annotations.
61	23	Similar gold standards were produced for the crowd annotations by applying the MACE denoising tool (Hovy et al., 2013); we tested various thresholds (1.0, 0.9, and 0.8) to discard instances that could be confidently assigned a gold label.
79	23	Since arguments need to be relevant to the given topic, we posit that providing topic information to the learner results in a more robust prediction capability in cross-topic setups.
80	24	Below, we present two models that integrate the topic, one that uses an attention mechanism and another that includes the topic vector directly in the LSTM cell.
97	32	The multi-task learning (mtl) and transfer learning (trl) models are able to make use of auxiliary data that can potentially improve the results on the main task.
117	19	For mtl+outer-att+corpus, we add the outer attention mechanism (see §4.1), modified for use with the mtl model, after each of the private RNNs, while additionally feeding it a second topic vector—the last hidden state of the shared RNN: m(t) = tanh(W rhr (t) +W shs +W pp) (10) fattention(hr (t),hs,p) = exp(wTmm(t))∑ t exp(wTmm(t)) (11) αt ∝ fattention(hr (t),hs,p) (12) s = n∑ t=1 hr (t)αt (13) whereW r ,W s, andW p are trainable weight matrices, hr (t) is the hidden state of the private bilstm at timestep t, hs is the last hidden state of the shared model, and p is the average of all word embeddings of topic words v1, .
119	30	To this end, we combine training (70%) and validation data (10%) of seven topics for training and parameter tuning, and use the test data (20%) of the eighth topic for testing.
122	15	As evaluation measures, we report the average macro F1, as well as the precision and the recall for the argument class (Parg, Rarg).
130	33	The results in Table 4 show that all our models outperform the baselines for two-label prediction.7 F1 for biclstm improves by 3.5 percentage points over the bilstm baseline and by 5.6 over lr-uni.
137	19	If no topic is provided (tr+bilstm+corpus), the transfer learning models are able to improve over the baseline bilstm.
157	22	To evaluate the performance of the models in datascarce scenarios, we gradually add target topic data to the training data and analyze the model performance on the target test set.
158	14	Figure 4 shows model performance (F1, Parg, and Rarg) on the “marijuana legalization” topicwhen adding different amounts of randomly sampled topic-specific data to the training data (x-axes).8 As the results show, the models that integrate the topic achieve higher recall when adding target topic data to the training data.
159	26	For bilstm, we observe a drastic difference when compared to the other models; the recall for arguments stays at around 30% and rises only when integrating more than 60% target topic data.
164	33	First, we introduced an annotation scheme suited to the information-seeking perspective of argument search and showed that it is cheaply but reliably applicable by untrained annotators to arbitrary Web texts.
165	22	Second, we presented a new corpus, including over 25,000 instances over eight topics, that allows for cross-topic experiments using heterogeneous text types.
166	62	Third, we conducted cross-topic experiments and showed that integrating topic information of arguments with our contextual BiLSTM leads to better generalization to unknown topics.
168	59	Finally, by gradually adding target topic data to our training set, we showed that, when available, even small amounts of target topic data (20%) have a strong positive influence on the recall of arguments.
170	35	An online argument search engine implementing our approach is now available for noncommercial use at https://www.argumentsearch.com/.
171	24	Furthermore, we are experimenting with language adaptation and plan to extend the tool to the German language.
