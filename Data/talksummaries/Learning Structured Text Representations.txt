22	12	Secondly, the inside-outside algorithm involves a dynamic programming process which is difficult to parallelize, making it impractical for modeling long documents.1 In this paper, we propose a new model for representing documents while automatically learning richer structural dependencies.
26	4	However, major operations in our approach can be parallelized efficiently on GPU computing hardware.
30	8	In this section, we describe how previous work uses the attention mechanism for representing individual sentences.
36	4	Despite successful application of the above attention mechanism in sentiment analysis (Cheng et al., 2016) and entailment recognition (Parikh et al., 2016), the structural information under consideration is shallow, limited to word-word dependencies.
39	13	Specifically, they normalize fij with a projective dependency tree using the inside-outside algorithm (Baker, 1979): fij = F (ui,uj) (4) a = inside-outside(f) (5) ri = n∑ j=1 aijuj (6) This process is differentiable, so the model can be trained end-to-end and learn structural information without relying on a parser.
53	6	We use a series of operations based on the MatrixTree Theorem (Tutte, 1984) to incorporate the struc- tural bias of non-projective dependency trees into the attention weights.
61	9	More formally, building a dependency tree amounts to finding latent variables zij for all i 6= j, where word i is the parent node of word j, under some global constraints, amongst which the single-head constraint is the most important, since it forces the structure to be a rooted tree.
64	5	Wa ∈ Rks∗ks is the weight for the bilinear transformation.
65	5	f ∈ Rn∗n can be viewed as a weighted adjacency matrix for a graph G with n nodes where each node corresponds to a word in a sentence.
66	6	We also calculate the root score f ri , indicating the unnormalized possibility of a node being the root: f ri = Wrdi (11) where Wr ∈ R1∗ks .
78	4	As illustrated in Figure 4, given a document with n sentences [s1, s2, · · · , sn], for each sentence si, the input is a sequence of word embeddings [ui1,ui2, · · · ,uim], where m is the number of tokens in si.
86	5	Let A denote a matrix depending on a real parameter x; assuming all component functions in A are differentiable, and A is invertible for all possible values, the gradient of A with respect respect to x is: dA−1 dx = −A−1dA dx A−1 (19) Multiplication of the three matrices and matrix inversion can be computed efficiently on modern parallel hardware architectures such as GPUs.
94	9	For this task we used the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), which contains premise-hypothesis pairs and target labels indicating their relation.
96	4	Sentence-level representations obtained by our model (with structured attention) were used to encode the premise and hypothesis by modifying the model of Parikh et al. (2016) as follows.
110	4	It is also worth noting that some models take structural information into account in the form of parse trees (Bowman et al., 2016; Chen et al., 2017).
123	4	This dataset contains restaurant reviews, each associated with human ratings on a scale from 1 (negative) to 5 (positive) which we used as gold labels for sentiment classification; we followed the preprocessing introduced in Tang et al. (2015a) and report experiments on their training, development, and testing partitions (80/10/10).
128	6	We include Czech in our experiments since it has more flexible word order compared to English, with non-projective dependency structures being more frequent.
134	6	In our experiments, we set the word embedding dimension to be 200 and the hidden size for the sentence-level and documentlevel LSTMs to 100 (the dimensions of the semantic and structure vectors were set to 75 and 25, respectively).
151	6	Sentence Trees We compared the dependency trees obtained from our model with those produced by a state-of-the-art dependency parser trained on the English Penn Treebank.
152	7	Table 5 presents various statistics on the depth of the trees produced by our model on the SNLI test set and the Stanford dependency parser (Manning et al., 2014).
165	5	For most datasets, documentlevel trees are not very deep, they mostly contain up to nodes of depth 3.
166	7	This is not surprising as the documents are relatively short (see Table 3) with the exception of debates which are longer and the induced trees more complex.
168	5	Unfortunately, our trees cannot be directly compared with the output of a discourse parser which typically involves a segmentation process splitting sentences into smaller units.
170	5	Figure 6 shows examples of document-level trees taken from Yelp and the Czech Movie dataset.
172	10	The second tree is non-projective, the edges connecting sentences 1 and 4 and 3 and 5 cross.
179	13	Experiments on sentence and document modeling tasks show that the representations learned by our model achieve competitive performance against strong comparison systems.
180	16	Analysis of the induced tree structures revealed that they are meaningful, albeit different from linguistics ones, without ever exposing the model to linguistic annotations or an external parser.
181	10	Directions for future work are many and varied.
182	100	Given appropriate training objectives (Linzen et al., 2016), it should be possible to induce linguistically meaningful dependency trees using the proposed attention mechanism.
183	109	We also plan to explore how document-level trees can be usefully employed in summarization, e.g., as a means to represent or even extract important content.
