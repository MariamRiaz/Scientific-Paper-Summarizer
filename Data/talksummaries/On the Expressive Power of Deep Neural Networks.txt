2	52	This includes even the fundamental question of neural network expressivity, how the architectural properties of a neural network (depth, width, layer type) affect the resulting functions it can compute, and its ensuing performance.
3	37	This is a foundational question, and there is a rich history of prior work addressing expressivity in neural networks.
5	35	Indeed, the very first results on this question take a highly theoretical approach, from using functional analysis to show universal approximation results (Hornik et al., 1989; Cybenko, 1989), to analysing expressivity via comparisons to boolean circuits (Maass et al., 1994) and studying network VC dimension (Bartlett et al., 1998).
6	19	While these results provided theoretically general conclusions, the shallow networks they studied are very different from the deep models that have proven so successful in recent years.
33	23	Given a neural network of a certain architecture A (some depth, width, layer types), we have an associated function, FA(x;W ), where x is an input and W represents all the parameters of the network.
34	44	Our goal is to understand how the behavior of FA(x;W ) changes asA changes, for values of W that we might encounter during training, and across inputs x.
36	30	Precisely quantifying the properties of FA(x;W ) over the entire input space is intractable.
37	21	As a tractable alternative, we study simple one dimensional trajectories through input space.
40	21	Armed with this notion of trajectories, we can begin to define measures of expressivity of a network FA(x;W ) over trajectories x(t).
42	17	Given a neural network with piecewise lin- ear activations (such as ReLU or hard tanh), the function it computes is also piecewise linear, a consequence of the fact that composing piecewise linear functions results in a piecewise linear function.
45	23	More precisely: Definition For fixed W , we say a neuron with piecewise linear region transitions between inputs x, x+ δ if its activation function switches linear region between x and x+δ.
48	17	Instead of just concentrating on the output neurons however, we can look at this pattern over the entire network.
49	22	We call this an activation patten: Definition We can defineAP(FA(x;W )) to be the activation pattern – a string of form {0, 1}num neurons (for ReLUs) and {−1, 0, 1}num neurons (for hard tanh) of the network encoding the linear region of the activation function of every neuron, for an input x and weights W .
51	24	As each distinct activation pattern corresponds to a different linear function of the input, this combinatorial measure captures how much more expressive A is over a simple linear mapping.
67	78	Regions in Input Space Given the corresponding function of a neural network FA(Rm;W ) with ReLU or hard tanh activations, the input space is partitioned into convex polytopes, with FA(Rm;W ) corresponding to a different linear function on each region.
71	61	We empirically tested the growth of the number of activations and transitions as we varied x along x(t) to understand their behavior.
72	37	We found that for bounded non linearities, especially tanh and hard-tanh, not only do we observe exponential growth with depth (as hinted at by the upper bound) but that the scale of parameter initialization also affects the observations (Figure 2).
74	16	This ‘dichotomies’ measure is discussed further in the Appendix, and also exhibits the same growth properties, Figure 14.
78	19	Bound on Growth of Trajectory Length Let FA(x ′,W ) be a ReLU or hard tanh random neural network and x(t) a one dimensional trajectory with x(t+ δ) having a non trival perpendicular component to x(t) for all t, δ (i.e, not a line).
79	25	Then defining z(d)(x(t)) = z(d)(t) to be the image of the trajectory in layer d of the network, we have (a) E [ l(z(d)(t)) ] ≥ O ( σw √ k√ k + 1 )d l(x(t)) for ReLUs (b) E [ l(z(d)(t)) ] ≥ O  σw√k√ σ2w + σ 2 b + k √ σ2w + σ 2 b d l(x(t)) for hard tanh That is, l(x(t) grows exponentially with the depth of the network, but the width only appears as a base (of the exponent).
90	19	(This is the case for e.g. large initialization scales).
91	22	Transitions proportional to trajectory length Let FAn,k be a hard tanh network with n hidden layers each of width k. And let g(k, σw, σb, n) = O  √k√ 1 + σ2b σ2w n Then T (FAn,k(x(t);W ) = O(g(k, σw, σb, n)) for W initialized with weight and bias scales σw, σb.
97	41	From the proof of Theorem 3, we saw that a perturbation to the input would grow exponentially in the depth of the network.
110	18	But note that even with a smaller weight initialization, weight norms increase during training, shown in Figure 9, pushing typically initialized networks into the exponential growth regime.
113	39	We see that the batch norm layers reduce trajectory length, helping stability.
114	15	Motivated by the fact that batch normalization decreases trajectory length and hence helps stability and generalization, we consider directly regularizing on trajectory length: we replace every batch norm layer used in the conv net in Figure 10 with a trajectory regularization layer.
125	29	There is also a natural connection between adversarial examples, (Goodfellow et al., 2014), and trajectory length: adversarial perturbations are only a small distance away in input space, but result in a large change in classification (the output layer).
126	19	Understanding how trajectories between the original input and an adversarial perturbation grow might provide insights into this phenomenon.
127	21	Another direction, partially explored in this paper, is regularizing based on trajectory length.
128	64	A very simple version of this was presented, but further performance gains might be achieved through more sophisticated use of this method.
