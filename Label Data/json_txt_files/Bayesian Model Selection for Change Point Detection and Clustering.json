{"sections": [{"heading": "1. Introduction", "text": "A classical estimation problem in many scientific inquiries is the well-studied change point detection problem where one tries to estimate when some properties of a sequence of random variables changes. This local property is of prime importance in many learning tasks such as signal segmentation (Abou-Elailah et al., 2016; Kim et al., 2009), change point detection in comparative genomics for early cancer diagnosis (Lai et al., 2005), and modeling and forecasting of changes in financial data (Lavielle & Teyssi\u00c3l\u0301re, 2006; Spokoiny, 2009).\nFor other applications, one needs more than this local answer and is interested in a more general overview of the time series where for instance earlier data samples behave like new ones creating a clustering effect. Examples of this are found in: electricity market data, where prices\n1KTH Royal Institute of Technology, Stockholm, Sweden. Correspondence to: Othmane Mazhar <othmane@kth.se>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nmight have different behavior corresponding to different price regimes that might reappear depending on some triggering events; signal partitioning with some parts of the signal sharing similar properties; and speech segmentation with different alternating sources. Generally speaking, it is of interest in these situations to determine not only the changes but also the clusters for a more precise description of the inhomogeneous time series.\nParametric models for solving the change point detection problem have been proposed in Cleynen & Lebarbier (2014) and Rigaill et al. (2012). However, in dealing with the change point and clustering problem we would naturally require that our solution does not assume any knowledge of the number of changes nor the actual number of clusters, as these numbers would evolve over time, so we expect new changes in the process to happen and new clusters to form as N , the number of samples, grows. Thus, any practical procedure should be able to estimate these numbers and also have adaptive guarantees with respect to how fast these numbers grow. Similar setups for change point detection have been the subject of study by Harchaoui & Capp\u00e9 (2007), Arlot et al. (2016) and Garreau & Arlot (2017) who use characteristic kernels for detecting changes in the distribution, while from a computational standpoint a more effective implementation has been proposed by Celisse et al. (2017). In this study, we will restrict ourselves to an iid (independent and identically distributed) Gaussian sequence model of the data with known variance, noting that the same study can be done using kernels and that the algorithm we develop can be effectively implemented using the same procedure as in (Celisse et al., 2017), as explained later in the paper.\nTwo other related lines of research, but which we do not explore here, are on-line algorithms for segmentation and L1-regularized segmentation. We refer the reader to (Tartakovsky et al., 2014) for an extensive review of on-line algorithms. Data segmentation using the L1-penalty was introduced by Rudin et al. (1992). The one-dimensional case, corresponding to the Fussed LASSO, has been studied in (Tibshirani et al., 2005) and (Rennie & Dobson, 1969) and an efficient algorithm has been proposed by Arnold & Tibshirani (2016). More recent results can be found in (Dalalyan et al., 2017) for the one-dimensional case and (H\u00fctter & Rigollet, 2016) for two-dimensional case.\nMain contribution: The generalized setting of change point detection while clustering the segments for sequences\nof data points does not seem to have been previously studied. In this work, we propose a two-pass dynamic programming algorithm for selecting an adequate model from a collection of candidate models. We motivate the choice of the algorithm computationally by showing that it runs in O(N2D + D4) time (where D is an upper bound on the number of change points), statistically by showing that it can be seen as an approximation of a computationally hard MAP optimization problem for which we can derive an oracle inequality that guarantees low sample complexity, consistency and adaptivity, and practically by testing the model on simulation data.\nStructure of the paper: In Section 2 we formulate the problem as one of nonparametric model selection from a family of models over all partitions of the data set. After some preliminaries and notations are given in Section 3, we propose in Section 4 a two-pass dynamic programming algorithm as a computationally effective relaxation of the optimization criterion and analyze its computational cost. We then put the model selection problem in a Bayesian framework in Section 5, and use a Laplace-type approximation to derive as optimization criterion the maximum a-posteriori probability. In Section 6 we derive an oracle inequality for the criterion that our algorithm is approximating, and study its properties. Experimental results showing that the clusters and segments can be effectively estimated are presented in Section 7 using simulation data."}, {"heading": "2. Problem formulation", "text": "Let Y be a measurable space and Y1, Y2, . . . , YN \u2208 Y denote random variables with distributions PYi . Our goal is on one hand to detect changes in the sequence of distribution measures (PYi) N i=1 and on the other hand to cluster the data points coming from the same process. Hence we put random variables between two consecutive changes in the same segment, and we think of random variables of the same segment or different segments as belonging to the same cluster if they are the realization of the same process.\nOne important case both in theory and in practice is the uniform constant design model were the Yis depend on deterministic variables uniformly spaced on a grid Xi = i for i \u2208 J1, NK := {1, . . . , N} through a regression function f\u2217 with an additive iid random noise ( i)Ni=1. Taking the distribution of the i\u2019s as N (0, \u03c32) with known variance, we end up with the following Gaussian sequence model:\nYi = f \u2217 i + i, for i \u2208 J1, NK. (1)\nHere we are placed in a regression setting of the form Y = f\u2217+ , where Y = [Y1 \u00b7 \u00b7 \u00b7 YN ]T , f\u2217 = [f\u22171 \u00b7 \u00b7 \u00b7 f\u2217N ]T and = [ 1 \u00b7 \u00b7 \u00b7 N ]T \u223c N (0, \u03c32IN ), and we are interested in estimating f\u2217 as a piecewise constant function that takes limited number of values.\nWe emphasize that it is unlikely that the data correspond exactly to a piecewise constant function plus independent\nrandom Gaussian noise and that we are in this low dimensional hidden structure exactly, yet there might exist a good sparse linear approximation. Hence our search is not for an exact model, rather we are trying to select the best model in a collection of candidates, as we explain in the next section."}, {"heading": "3. Preliminaries and notation", "text": "We would like to perform dimensionality reduction by exploiting the hidden structure on the data sequence Y1, Y2, . . . , YN . To do this we split it into different segments while also putting the segments sharing the same mean into the same cluster. Hence if we knew the clusters our problem reduces to fitting a constant to a set of observations over each cluster. Observe that if f\u2217 is constant over parts of J1, NK, then it determines a clustering of Y1, Y2, . . . , YN over the values where it is constant. Hence, we can think about the problem as, first determining the clustering of the Y1, Y2, . . . , YN which would result in a partition \u03c0 of J1, NK, and then choosing the best value of f\u0302 over each part as our estimate. So f\u2217 belong to the subspace F\u03c0: subspace of functions that are constant over the parts of the partition \u03c0.\nTo formalize this, let M be an index set over the collection of partitions \u03a0N of J1, NK; given m \u2208 M, denote by Fm the subspace of functions that are constant over the parts of \u03c0m. Our goal is two-fold: find m\u0302 as the index estimate of Fm\u0302, the subspace where the estimate of f\u2217 lives, and from Fm\u0302 compute f\u0302m\u0302 as our estimate. We represent a partition \u03c0 as an unordered collection of its subsets \u03c0 = {[1], [2], . . . , [|\u03c0|]} with [k] being the kth-equivalent class, -part or -cluster, and |\u03c0| the cardinality of the partition. Every part [k] can be seen as the union of segments [k] = {[k1], [k2], . . . , [k|[k]|]} where (ki) |[k]| i=1 is the collection of maximal intervals in [k] that we call segments of the kth-cluster. The last element in each segment [ki] is called a change point. We define d\u2032m := |\u03c0m|\u22121 = dim(Fm)\u22121 as the clustering dimension. Even though this choice might create some confusion it will be consistent the notations used in the proofs of sections 5 and 6. Also we define d\u2032\u2032m = |\u03c0m|0 := |\u222a d\u2032m+1 k=1 [k]| as the change point dimension.\nTo link partitions to subspaces let el := (0, . . . , 1, . . . , 0) be the lth-component of the standard orthonormal basis of RN , and define for a subset A of J1, NK the vector 1A :=\u2211 l\u2208A el. For [k], the k\nth cluster of \u03c0m, with a slight abuse of notation we define 1[k] := \u2211|[k]| i=1 eki , and observe that Fm = span{1[k] : k \u2208 \u03c0m}, which is consistent with the definition of the clustering dimension d\u2032m := |\u03c0m| \u2212 1 = dim(Fm)\u2212 1.\nWe define \u3008f\u2217\u3009 := span{f\u2217}, S1 \u2295 S2 as the direct sum of the two vector space S1 and S2, and S1 S2 as their direct difference. PS denotes the (orthogonal) projection operator onto the subspace S. We also define the partitions inclusion as m1 \u2282 m2 if Fm1 \u2282 Fm2 , or equivalently if\n\u03c0m2 is finer than \u03c0m1 . Example 1. Consider the signal f\u2217 of Figure 1, whose partition is\n\u03c0 = {[1]; [2]; [3]; [4]; [5]},\nwhere\n[1] = J615, 678K \u222a J821, 926K \u222a J1019, 1211K \u222a J1753, 2000K [2] = J1, 100K \u222a J679, 820K \u222a J1212, 1280K [3] = J101, 214K \u222a J505, 614K \u222a J926, 1018K \u222a J1281, 1600K [4] = J215, 504K [5] = J1601, 1752K.\nHence, d\u2032\u03c0 = 4 and d \u2032\u2032 \u03c0 = 12 for this signal.\nWe also denote by CNk the binomial coefficient that gives the number of ways, disregarding order, that k objects can be chosen from among N objects. This is given by\nCNk := N !\nk!(N \u2212 k)! . (2)\nThe Stirling numbers of the second kind, S(N, k), correspond to the number of ways to partition a set of N objects into k non-empty subsets, or, similarly, to the number of different equivalence relations with precisely k equivalence classes that can be defined on an set of N elements.\nWe are precisely interested in the case where the element set is J1, NK and the distance between every two elements in each equivalence class is at least 2; we denote the number of such equivalent classes by S2(N, k). S(N, k) and S2(N, k) satisfy the following recurrence relations:\nS(N, k) = S(N \u2212 1, k \u2212 1) + kS(N \u2212 1, k), N > k, S2(N, k) = S(N \u2212 1, k \u2212 1), N, k > 2. (3)\nFor the proofs of these results, we refer the reader to (Graham et al., 1988) and (Mohr & Porter, 2009)."}, {"heading": "4. Two-pass dynamic programming for change point detection and clustering", "text": "To solve the change point and clustering problem, a natural approach is to consider the minimization of a criterion of the form,\nCrit(m) = \u2016y \u2212 f\u0302m\u201622 + \u03c32K pen(m). (4)\nUniqueness, continuity and stability properties of similar criterion have been studied in (O. et al.), we restrict to a penalty term pen(d\u2032m, d \u2032\u2032 m) := pen(m) depending only on d\u2032 and d\u2032\u2032 and a multiplicative tuning parameter K. Indeed, as we shall see the penalty can be chosen such that the minimizer f\u0302m\u0302 of (4) behaves like an approximation to a maximum a-posteriori estimator (MAP), and also, the average expected risk 1NE[\u2016f\u0302m\u0302\u2212 f\n\u2217\u201622]\u2192 0 for a large class of signals f\u2217, namely, those corresponding to models with d\u2032 6 d\u2032\u2032 = o(N/ lnN), i.e., f\u2217 is a consistent estimator for those signals. The specific form of pen(m) will be derived in the next section, based on an oracle inequality that will guarantee consistency and adaptivity of our estimator.\nAlthough the estimator f\u0302m\u0302 enjoys good statistical properties, from a computational stand it would involve the exploration ofM. The setM is identified with the collection of all the partitions of J1, NK, whose number asymptotically behaves like O(NeN/ lnN), rendering the minimization of the criterion (4) computationally challenging. A way to bypass this issue for the change point only detection problem is via dynamic programming (Harchaoui & Capp\u00e9, 2007); this approach works in this simplified setup since there is a natural ordering for exploring the subproblems, which does not hold here. To overcome this, we will relax the criterion in such a way to create a subproblem ordering and thus derive a computationally feasible approximation. The proposed new method is outlined in Algorithm 1.\nLet y\u0304[k] := ( \u2211 i\u2208[k] Yi)/|[k]|, the average of the elements of Y in the [k]-th part. Notice that, given \u03c0m := {[1]; [2]; . . . ; [d\u2032m \u2212 1]}, we have\nPFm Y = d\u2032m\u22121\u2211 k=1 \u3008Y,1[k]\u3009 \u20161[k]\u20162 1[k] = d\u2032m\u22121\u2211 k=1 y\u0304[k]1[k].\nThe minimization of criterion (4) can then be equivalently written as\nmin m\u2208M Crit(m)\n= min m\u2208M\n{\u2016y \u2212 PFm Y \u201622 + \u03c32K pen(d\u2032m, d\u2032\u2032m)}\n= min 06d\u2032\n6d\u2032\u20326D  min|m|=d\u2032 |m|0=d\u2032\u2032 \u2016y \u2212 PFm Y \u201622 + \u03c32K pen(d\u2032, d\u2032\u2032)  ,\nAlgorithm 1 Two-Pass Dynamic Programming Algorithm input data points (yi)Ni=1, maximum number of changes D and\npenalty strength K. 1:\ny\u0304[k,l] :=\n\u2211l i=k Yi k \u2212 l + 1\nR[k,l] := l\u2211 i=k (yi \u2212 y\u0304[k,l])2, 1 6 k 6 l 6 N.\n2: for d = 1 to D do 3: use the dynamic programming recurrence in (9) and a\nbacktracking step to compute\nCd(N) := min |m\u0304|=d\n\u2016Y \u2212 PFm\u0304 Y \u2016 2, (5)\nm\u0303d \u2208 arg min |m\u0304|=d \u2016Y \u2212 PFm\u0304 Y \u2016 2.\n4: end for 5: for d = 1 to D do 6: m\u0303d =: {0 6 i1 < i2 < \u00b7 \u00b7 \u00b7 < id < N}\n(\u03b1k) d k=0 := (ik+1 \u2212 ik)d0, (i0 = 0, id+1 = N).\n7: sort (y\u0304[1,i1], y\u0304[i1+1,i2], . . . , y\u0304[id+1,N ]). 8: ( y\u0304(k) )d k=0 := ordered sequence of (y\u0304[ik+1,ik+1]) d k=0\n(\u03b1(k)) d k=0 := corresponding permuted (\u03b1k) d k=0 according to permutation \u03c6d.\n9: y\u0304(k,l) := \u2211l i=k \u03b1(i)y\u0304(i)\u2211l\u22121 i=k \u03b1(i) and R\u0304[k,l] := \u2211l i=k \u03b1(i)(y\u0304(i) \u2212\ny\u0304(k,l)) 2, 1 6 k 6 l 6 d.\n10: for \u03b4 = 1 to d do 11: use the dynamic programming recurrence in (10) and a\nbacktracking step to compute\nG(d,\u03b4) := min m\u2208My\u0304m\u0303,\u03b4\n\u2016PFm\u0304 Y \u2212 PFm PFm\u0304 Y \u2016 2, (6)\n\u02dc\u0303m(d,\u03b4) \u2208 arg min m\u2208My\u0304m\u0303\u03b4 \u2016PFm\u0304 Y \u2212 PFm PFm\u0304 Y \u2016 2.\n12: end for 13: end for 14: B(d,\u03b4) := Cd+G(d,\u03b4) +\u03c32K pen((d, \u03b4)), 1 6 \u03b4 6 d 6 D. 15: (d\u0302, \u03b4\u0302) := arg min\n16\u03b46d6D B(d,\u03b4).\n16: reconstruct m(d\u0302,\u03b4\u0302) from m\u0303d\u0302 and \u02dc\u0303m(d\u0302,\u03b4\u0302). output value of criterion Crit(m(d\u0302,\u03b4\u0302)) = B(d\u0302,\u03b4\u0302) and selected\nmodel for change points and clusters m(d\u0302,\u03b4\u0302).\nwhere D is a reasonable upper bound on the number of change points. As we shall see later, from a statistical point of view there is no need to explore all possible values of d\u2032 and d\u2032\u2032, since the statistical guarantees only hold in a regime where d\u2032 6 d\u2032\u2032 = o(N/ lnN).\nWe define \u03c0m\u0304 to be the partition having as elements all the segments of \u03c0m and instead of computing the minimum exactly we will take a greedy step by defining\nm\u0303 := arg min |m\u0304|=d\u2032\u2032\n\u2016Y \u2212 PFm\u0304 Y \u20162\nand defining Mm\u0303,d\u2032 := {m \u2208 M : m \u2282 m\u0303, |m| = d\u2032}, which can be identified with the collection of all partitions of J1, d\u2032\u2032K into d\u2032 sets. We restrict further this collection to partitions \u03c0 satisfying what we call the clustering property, which states that if I1, I2 and I are segments in some (possibly different) parts of \u03c0, then\n{I1, I2 \u2208 [k] y\u0304I1 6 y\u0304I 6 y\u0304I2 \u21d2 I \u2208 [k]. (7)\nThis sub-collection will be denoted asMy\u0304m\u0303,d\u2032 . Simply put, this property says that the partitions considered are those that respect the ordering of (y\u0304[ik+1,ik+1]) d\u2032\u2032\nk=0, since if two segments I1, I2 belong to [k], and the segment I satisfies y\u0304I1 6 y\u0304I 6 y\u0304I2 , then it should also be in cluster [k].\nThis leads to the following upper bound, whose detailed derivation is given in appendix B:\nmin m\u2208M Crit(m) 6 min 06d\u2032\u20326D { min |m|=d\u2032\u2032 \u2016Y \u2212 PFm Y \u20162\n+ min 06d\u20326d\u2032\u2032\nm\u2208My\u0304m\u0303,d\u2032\u2032\n\u2016PFm\u0303 Y \u2212 PFm PFm\u0303 Y \u20162\n+ \u03c32K pen(d\u2032, d\u2032\u2032) } .\nTherefore, we can define the following relaxation for the minimization of the criterion in (4):\nCritr(d \u2032\u2032) := min |m|=d\u2032\u2032 \u2016Y \u2212 PFm Y \u20162\n+ min 06d\u20326d\u2032\u2032\nm\u2208My\u0304m\u0303,d\u2032\u2032\n{ \u2016PFm\u0303 Y \u2212 PFm PFm\u0303 Y \u20162\n+ \u03c32K pen(d\u2032m, d \u2032\u2032 m)\n} . (8)\nand our algorithm computes min 06d\u2032\u20326D\nCritr(d \u2032\u2032) and returns\nm(d\u0302,\u03b4\u0302). From this last definition we observe that\nmin m\u2208M Crit(m) 6 Crit(m(d\u0302,\u03b4\u0302)) = min06d\u2032\u20326D Critr(d\n\u2032\u2032).\nThus, obtainingm(d\u0302,\u03b4\u0302) ensures making progress toward the minimization of Crit(m). The Two-Pass Dynamic Programming Algorithm 1 is aimed at doing this by computing the value of the minimum in (8) and returning a solution m\u0302 = m(d\u0302,\u03b4\u0302) in the following way:\nDetails of Main Steps in Algorithm 1\n\u2022 Step 3: It computes Cd(n) defined in (9) for all d and n to obtain Cd(N) for all d \u2208 J1, NK. It does so by using a dynamic programming algorithm that computes recursively for all 2 6 d 6 D and d 6 n 6 N the following recurrence, similar to the one in Hawkins (1976):\nC1(n) := R[1,n] (9)\nCd(n) := min i\u2208Jd,nK\n{Cd\u22121(i\u2212 1) +R[i,n]}, d > 2.\n\u2022 Step 7: For all values of d, it sorts the obtained segments according to their levels to yield ( y\u0304(k) )d 0 , and it keeps track\nof the segments\u2019 sizes as (\u03b1k)dk=0 = (ik+1 \u2212 ik)d0 .\n\u2022 Step 11: It runs a modified dynamic programming recurrence on ( y\u0304(k) )d 0\nthat uses weights according to the sizes (\u03b1(k)) d 0 . It does so using the following recurrence for all 1 6 \u03b4 6 t 6 d:\nG(t,1) := R\u0304[1,t], (10)\nG(t,\u03b4) := min i\u2208J\u03b4,tK\n{G(i\u22121,\u03b4\u22121) + R\u0304[i,t]}, \u03b4 > 2.\n\u2022 Step 15: It computes the minimum in (8) and finds for which model it is attained by solving the minimization problem:\n(d\u0302, \u03b4\u0302) := arg min 16\u03b46d6D B(d,\u03b4).\n\u2022 Step 16: It finally reconstructs m(d\u0302,\u03b4\u0302) from m\u0303d\u0302 and \u02dc\u0303m(d\u0302,\u03b4\u0302) using the permutation \u03c6(d\u0302).\nThis algorithm can be thought of as an efficient way to compute the relaxation in (8), based on solving the change point detection problem in (5) using the dynamic programming recurrence of (9), followed by a solving a clustering problem in (6) using the dynamic programming recurrence of (10).\nThe next theorem shows that Algorithm 1 correctly solves the minimization problem in (8) and explicits its time and space complexity. Theorem 4.1. Let (yi)Ni=1 \u2282 R, D \u2208 N and K > 0. Then,\n\u2022 for all 1 6 d 6 D,\nm\u0303d \u2208 arg min |m\u0304|=d \u2016Y \u2212 PFm\u0304 Y \u20162,\n\u2022 for all 1 6 \u03b4 6 d 6 D,\n\u02dc\u0303m(d,\u03b4) \u2208 arg min m\u2208My\u0304m\u0303\u03b4 \u2016PFm\u0304 Y \u2212 PFm PFm\u0304 Y \u20162.\nFurthermore, Algorithm 1 correctly solves the minimization problem in (8), with time and space complexity O(N3 +D4) and O(N2 +D3), respectively.\nProof. See Appendix B.\nThe time and space complexity can be improved to O(N2D +D4) and O(DN +D3), respectively. We refer the reader to the discussion after the proof in Appendix B for the derivation of this result. In this way we obtain a computationally feasible algorithm that finds the minimum in (8) and returns an approximation to the criterion in (4). In the next section, we will motivate the use of Algorithm 1 from a statistical point of view by showing that the minimization of criterion (4) can be viewed as an approximate maximum a-posteriori estimator."}, {"heading": "5. Model selection criterion for change point detection and clustering", "text": "In this part, we provide a derivation of the optimization criterion in (4). We start by proposing a Bayesian model selection scheme, which is later inverted to arrive at an integral form of the maximum a-posteriori probability (MAP) estimator. Then we use a Laplace approximation to derive turn the MAP into an optimization problem of the desired form.\nHere we show that the proposed selection criterion in (4) follows naturally from a Bayesian reasoning. For this, we model the data as being the outcome of the following sampling model. The observation Y is generated from a multivariate Gaussian of mean F and variance \u03c32IN as described by (1). For the random variable F , given that it belongs to a subspace Fm, we choose an absolutely continuous measure Ld\u2032m with respect to \u03bbd\u2032m , the Lebesgue measure on Rd\u2032m+1, such that dLd\u2032m = lf/md\u03bbd \u2032 m+1 =\nd\u2032m+1\u220f k=1 (lfk/md\u03bb) with lf1/m = \u00b7 \u00b7 \u00b7 = lfd\u2032m+1/m. Later we will see that the choice lf/m will not matter in comparison to the order of approximation, nevertheless we would like it to be a bounded continuous prior satisfying some additional conditions given in Lemma 2, even though we might be chosen as an improper prior. On the family of models M we impose a categorical distribution measure PM as prior, with a weight pm for model m. Thus, we obtain the following sampling model for the data1:\nY/F \u223c N (F, \u03c32IN ) F/m \u223c Ld \u2032 m (11)\nm \u223c PM = Categorical((pm)m\u2208M).\nSince Y , F and m are now random variables, it makes sense to compute \u00b5m/Y , the posterior distribution of m\n1Here and in the sequel, the dependence of pm and PM on the number of samples N is omitted, for simplicity of notation.\ngiven Y , and maximize it, to arrive at a MAP estimate of m given bellow.\npm/Y =\npm \u222b f\u2208Fm \u03c6N ( Y \u2212 f \u03c3 ) lf/m(f)df\n\u2211 m\u2032\u2208M pm\u2032 \u222b f \u2032\u2208Fm \u03c6N ( Y \u2212 f \u2032 \u03c3 ) lf/m\u2032(f \u2032)df \u2032 .\n(12)\nFor the complete derivation of the formula in 12 we refer you to appendix B.\nStarting from the a-posteriori distribution (12) we can derive an approximation for the MAP as follows:\npm/Y \u221d pm \u222b f\u2208Fm \u03c6N ( Y \u2212 f \u03c3 ) lf/m(f)df\n= pm d\u2032m+1\u220f k=1\n1\n(2\u03c0\u03c32) |[k]| 2\n(13)\n\u00b7 \u222b R exp ( \u2212 \u2016y[k] \u2212 fk1[k]\u201622 2\u03c32 ) lfk/m(fk)dfk.\nIn the last step of (13) we define y[k] as the vector obtained from the entries of y corresponding to cluster [k]. To obtain an approximation of the MAP estimate as a solution of a criterion of the form (4) we need the result of lemma 2 stated and proved in Appendix C using a Laplace approximation type of argument. We then obtain the following upper bound for the MAP for all K > 1 :\nCritMAP(m) 6 \u2016y \u2212 PFm y\u201622\n2\u03c32\n+K ( ln 1\npm +\n1 2 (d\u2032m + 1) ln N\nd\u2032m\n) +O(d\u2032m). (14)\nThe complete derivation of (14) can be found in Appendix C. Now we define our approximate MAP criterion as:\nCrit(m) = \u2016y \u2212 PFm y\u201622 + \u03c32K pen(m),\npen(m) = ( 2 ln 1\npm + (d\u2032m + 1) ln\nN\nd\u2032m\n) . (15)\nIn the next section, we finish the specification of the penalty term by providing the probabilities pm over the space of models. To do so we will exhibit an oracle inequality satisfied by the estimator that minimizes (4), and choose a probability mass function (pm) that gives a reasonable upper bound on the expected quadratic risk defined below."}, {"heading": "6. Oracle inequality and upper bound for the risk", "text": "The standard way of assessing the performance of a statistical algorithm is by comparing its performance to a reasonable oracle. For this we use as a measure of performance\nof an estimator f\u0302 the expected quadratic risk:\nRn(f\u0302) = E[\u2016f\u0302 \u2212 f\u2217\u201622].\nIn the case of the change point detection and clustering problem, the comparison should be non-asymptotic, reflecting our lack of knowledge about both the clustering dimension and the change point dimension. For this we state below a non-asymptotic oracle inequality for Crit(m) using an oracle with remainder of the form:\ninf m\u2208M {Rn(PFm y) + om(1)}.\nThis type of oracle has access to f\u2217 and chooses the m that minimizes the risk criterion up to a remainder term.\nTo derive this we finish the specification of Crit(m) by providing an appropriate prior pm. The intuition behind our choice is the following. Defining r\u0302m = \u2016y \u2212 f\u0302m\u201622 and pen(m) = 2\u03c32 ln 1pm + \u03c3 2(d\u2032m + 1) ln N d\u2032m\nwe see that the criterion (15) is of the form:\nCrit(m) = r\u0302m + pen(m).\nThe number of models in the family M having the same values of d\u2032m and d \u2032\u2032 m grows exponentially with those dimensions. Thus for fix d\u2032m and d \u2032\u2032 m we might find a model with low r\u0302m just because of randomness since some of them will deviate largely from their means, which would correspond to an over-fitting case, this was the problem of case with the traditional AIC type of estimators. Therefore, we need to penalize models of high dimensions more by taking into account the number of models with same dimensions. On the other hand we want this penalty to be as small as possible this way we give more importance to the fitting term r\u0302m. In particular we would prefer the term 2\u03c32 ln 1pm to stay close to \u03c3\n2(d\u2032+1) ln Nd\u2032 at least for values of d\u2032m close to d \u2032\u2032 m. Our choice for pm, useful inequalities and a complete discussion of the role of pm as a prior and tuning parameter for the risk can be found in Appendix D. From Lemmas 3 and 4, the following oracle inequality can be derived for f\u0302m\u0302:\nTheorem 6.1 (Oracle inequality for f\u0302m\u0302). With M restricted to models such that ed\u2032m 6 N and for the choice of K = 3a, pm as in 3, pen(m) as in 15 and m\u0302 \u2208 M corresponding to\nm\u0302 \u2208 arg min m\u2208M \u2016y \u2212 f\u0302m\u201622 + \u03c32K pen(m), (16)\nWe obtain for all a > 1,\nEf\u2217 [\u2016PFm\u0302 Y \u2212 f\u2217\u20162] 6\narg min m\u2208M\n{ a\na\u2212 1 Ef\u2217 [\u2016PFm Y \u2212 f\u2217\u20162]\n+ a2\u03c32\na\u2212 1\n( 7 + 3(d\u2032m + 1) ln N\nd\u2032m + 6 ln\n1\npm\n)} . (17)\nProof. See Appendix D.\nBy investigating the oracle inequality, one notices that for an optimal choice of a one has to make a trade-off between the performance of the oracle part and the bias part of the inequality. In general this trade-off is not possible to optimize since the value of the oracle part is not available to us and depends on the variance of the noise. In practice, one can use the SLOPE heuristic introduced in Lebarbier (2002) and described in Baudry et al. (2012) and in (Arlot & Massart, 2009). In our case, the value of the tuning parameter can be chosen independently of the variance of the noise and we can use the value of a for which we know that our estimator f\u0302m\u0302 will perform well.\nCorollary 6.1. For the set of models described in 6.1 with f\u2217 \u2208 Fm\u2217 the following properties hold:\n\u2022 Adaptation and Risk Upper bound: The following adaptive upper bound in terms of d\u2032m\u2217 and d \u2032\u2032 m\u2217 holds\nfor a = 2:\nEf\u2217 [\u2016PFm\u0302 Y \u2212 f \u2217\u20162] 6 4\u03c32 ( 7 + 3(d\u2032m\u2217 + 1) ln N\nd\u2032m\u2217\n+ 6 ( d\u2032m\u2217 ln[d \u2032\u2032 m\u2217e 13 6 ] + d\u2032\u2032m\u2217 ln[d \u2032 m\u2217e 2] + d\u2032\u2032m\u2217 ln N\nd\u2032\u2032m\u2217\n)) .\n\u2022 Consistency: If d\u2032\u2032m\u2217 = o(N/ lnN), then limN\u2192\u221eN \u22121Ef\u2217 [\u2016f\u0302m\u0302 \u2212 f\u2217\u20162] = 0.\nProof. See Appendix D.\nWe notice that the consistency condition d\u2032\u2032m\u2217 = o(N/ lnN) is within the restriction on the models in theorem 6.1, hence there is no loss of generality of having only models with ed\u2032m 6 N in M since for other models we cannot guarantee convergent mean square risk anyway. In the special case d\u2032m\u2217 = d \u2032\u2032 m\u2217 , i.e when the change point and clustering problem reduces to a change point only problem, Kernel methods have comparable accuracy (Celisse et al., 2017). The interesting case is when the numbers are different, we gain a logarithmic factor in accuracy with almost the same computational cost. In the next section, we validate these theoretical guarantees by a series of tests on simulated data to get a sense of how tight the oracle inequality is, which signals are difficult to estimate and how the algorithm behaves in practice."}, {"heading": "7. Experimental results", "text": "Consider first an experiment based data generated randomly according to the setup of (1) with the same change points of Example 1. This is considered to be an easy case since d\u2032m\u2217 = 4 < d \u2032\u2032 m\u2217 = 12 N = 2000, which is within the range of signals for which the consistency result of Corollary 6.1 holds.\nThe experiments in Figure 2 show that the algorithm is quite robust to the level of noise as measured by the signalto-noise ratio S/N = magnitude of smallest jump in f \u2217\n\u03c32 . We observe that the difference between the ground truth f\u2217 and f\u0302m\u0302 is quite small even for small S/N levels such as S/N = 0.5 and the change point locations do not vary appreciably; in fact, for this experiment, S/N = 0.3 seems to be the limiting case for which the algorithm performs well, and for lower values the risk upper-bound in Corollary 6.1 becomes loose when \u03c3 increases. Also, we note that an S/N of 0.5 is quite low for this kind of problems. In particular, algorithms relying on the L1-penalty such as Fussed LASSO do not achieve this kind of performance on the simpler task of change point only detection, while on the other hand, they are more computational efficient (Xin et al., 2014).\nFigure 3 illustrates a difficult case, where we reduced the number of observation by segment by scaling down the signal f\u2217 to a support of size N = 500. Now we are outside of the useful regime of Corollary 6.1 and we notice that the second segment J15, 53K is wider than what it should since the first change point at 25 was detected at 14; also the segment J206, 237K belongs to cluster [4] while it is actually in cluster [3] in the original signal f\u2217. Nevertheless we can observe an interesting property for segment J324, 346K, namely, that the end point 346 does not correspond to any real change point, yet this segment belongs to the optimal solution of the 1st dynamic programming pass. On the other hand the 2nd dynamic programming pass puts it in the same cluster [3] as J347, 399K, turning them into one single segment of cluster [3]. This behavior actually is the norm for the algorithm, where false changes are often detected in difficult signals in the 1st\ndynamic programming pass but are removed after the 2nd pass. These kinds of false discoveries are actually one of the weaknesses of many change point only detection algorithms like Fussed LASSO, and they have been studied in (Levy-leduc & Harchaoui, 2008), (Rinaldo, 2009) and (Rojas & Wahlberg, 2014). In the last experiment,\nwe run Algorithm 1 300 times with the parameter values d\u2032m\u2217 = 4 < d \u2032\u2032 m\u2217 = 12 N = 2000 and signal-to-noise ratio S/N = 1; Figure 4 summarizes the results. In the top histogram we notice that the algorithm successfully detects the change points most of the time; in fact, the achieved accuracy was number of change points correctly detectednumber of change points detected \u2248 0.8528. The\nmiddle histogram shows the placement of estimated clusters and the true values of the clusters; we observe that the true values lie in a small neighborhood of the estimated values for every cluster. In the bottom histogram we observe that the theoretical upper bound on the average mean square error \u2013in this case 12.1575\u2013 found in Corollary 6.1 is very conservative and most of the 300 estimates \u2013given by \u2016f\u0302m\u0302\u2212f\n\u2217\u20162 N \u2013 are significantly smaller."}, {"heading": "8. Conclusions", "text": "In this work, we considered a novel problem related to change point detection where we have to address the simultaneous task of segmenting and clustering the observed signal. Our approach has been to view this problem as a non-parametric model selection problem on the set of all possible partitions. We derived for this the computationally tractable Algorithm 1, that computes a relaxation of the penalized minimization of criterion (4), and we justified it from a statistical standpoint by showing that this minimization can be viewed as an approximate MAP. This approximate MAP estimate enjoys the properties of being adaptive and consistent in the sense of Corollary 6.1. We finally justified the use of Algorithm 1 by simulation data that shows some useful properties of the resulting estimate and validates the theoretical guarantees.\nOne extension of this work concerns developing a more complete analysis of Algorithm 1, to obtain consistency results on the number and locations of the change points and clusters. Another possible extension relates to the use of Algorithm 1 in the non-scalar case; this was already explored for change point only detection in (Arlot et al., 2016) through the use of characteristic kernels (Sriperumbudur et al., 2011). We believe that the same approach can be adopted here except that we cannot perform the sorting step; this can be overcome using a Kernel clustering algorithm (Filipponea et al., 2008) or a spectral version of it (Sch\u00f6lkopf et al., 1998) for the second stage. Finally, the remark after Figure 3 hints to the possibility of using a combined algorithm starting with the sparse solution of Fussed LASSO and running the 2nd dynamic programming pass of our algorithm as a way to boost the performance of Fussed LASSO to get rid of false discoveries. This would be still computationally attractive according to the comment after Theorem 4.1, since the solution of Fussed LASSO has a small number of changes."}], "year": 2018, "references": [{"title": "Detection of abrupt changes in spatial relationships in video sequences", "authors": ["A. Abou-Elailah", "V. Gouet-Brunet", "I. Bloch"], "venue": "In International Conference on Pattern Recognition Applications and Methods,", "year": 2016}, {"title": "Data-driven calibration of penalties for least-squares regression", "authors": ["S. Arlot", "P. Massart"], "venue": "Journal of Machine Learning Research,", "year": 2009}, {"title": "Efficient implementations of the generalized lasso dual path algorithm", "authors": ["T.B. Arnold", "R.J. Tibshirani"], "venue": "Journal of Computational and Graphical Statistics,", "year": 2016}, {"title": "Concentration Inequalities: A Nonasymptotic Theory of Independence", "authors": ["S. Boucheron", "G. Lugosi", "P. Massart"], "year": 2013}, {"title": "New efficient algorithms for multiple change-point detection with kernels", "authors": ["A. Celisse", "G. Marot", "M. Pierre-Jean", "G. Rigaill"], "year": 2017}, {"title": "Norms of Gaussian sample functions", "authors": ["B.S. Cirel\u2019son", "I.A. Ibragimov", "V.N. Sudakov"], "venue": "In Proceedings of the Third Japan-USSR Symposium on Probability Theory, pp", "year": 1976}, {"title": "Segmentation of the Poisson and negative binomial rate models: a penalized estimator", "authors": ["A. Cleynen", "E. Lebarbier"], "venue": "Probability and Statistics,", "year": 2014}, {"title": "Introduction to Algorithms, Third Edition", "authors": ["T.H. Cormen", "C.E. Leiserson", "R.L. Rivest", "Stein", "C. (eds"], "year": 2009}, {"title": "On the prediction performance of the Lasso", "authors": ["A.S. Dalalyan", "M. Hebiri", "J. Lederer"], "year": 2017}, {"title": "A survey of kernel and spectral methods for clustering", "authors": ["M. Filipponea", "F. Camastrab", "F. Masullia", "S. Rovetta"], "venue": "Journal of Machine Learning Research,", "year": 2008}, {"title": "Consistent change-point detection with kernels", "authors": ["D. Garreau", "S. Arlot"], "year": 2017}, {"title": "Algorithm Design: Foundations, Analysis, and Internet Examples", "authors": ["M.T. Goodrich", "Tamassia", "R. (eds"], "year": 2001}, {"title": "Retrospective multiple change-point estimation with kernels", "authors": ["Z. Harchaoui", "O. Capp\u00e9"], "venue": "IEEE/SP Workshop on Statistical Signal Processing (SSP \u201907),", "year": 2007}, {"title": "Point estimation of the parameters of piecewise regression models", "authors": ["Hawkins", "Douglas M"], "venue": "Journal of the Royal Statistical Society. Series C (Applied Statistics),", "year": 1976}, {"title": "Detection of abrupt changes in spatial relationships in video sequences", "authors": ["J. H\u00fctter", "P. Rigollet"], "venue": "In 29th Annual Conference on Learning Theory, PMLR", "year": 2016}, {"title": "Using labeled data to evaluate change detectors in a multivariate streaming environment", "authors": ["A.Y. Kim", "C. Marzban", "D.B. Percival", "W. Stuetzle"], "venue": "Signal Processing,", "year": 2009}, {"title": "Comparative analysis of algorithms for identifying amplifications and deletions in array", "authors": ["W.R. Lai", "M.D. Johnson", "R. Kucherlapati", "P.J. Park"], "venue": "CGH data. Bioinformatics,", "year": 2005}, {"title": "Detection of multiple change-points in multivariate time series", "authors": ["M. Lavielle", "G. Teyssi\u00c3\u013are"], "venue": "Lithuanian Mathematical Journal,", "year": 2006}, {"title": "Quelques approches pour la d\u2019etection de ruptures a horizon fini", "authors": ["E. Lebarbier"], "venue": "PhD thesis, FacultA\u0303l\u2019 des Sciences d\u2019Orsay (Essonne), Universite Paris-Sud,", "year": 2002}, {"title": "Catching change-points with lasso", "authors": ["C. Levy-leduc", "Z. Harchaoui"], "venue": "In Advances in Neural Information Processing Systems", "year": 2008}, {"title": "Concentration Inequalities and Model Selection", "authors": ["P. Massart"], "venue": "Springer-Verlag Berlin Heidelberg,", "year": 2003}, {"title": "Applications of chromatic polynomials involving Stirling numbers", "authors": ["A. Mohr", "T.D. Porter"], "venue": "Journal of Combinatorial Mathematics and Combinatorial Computing,", "year": 2009}, {"title": "Complexity penalized least squares estimators: Analytical results", "authors": ["A. Wittich", "G. Kempe", "Winkler", "Liebscher"], "venue": "Mathematische Nachrichten,", "year": 2005}, {"title": "On Stirling numbers of the second kind", "authors": ["B.C. Rennie", "A.J. Dobson"], "venue": "Journal of Combinatorial Theory,", "year": 1969}, {"title": "Exact posterior distributions and model selection criteria for multiple change-point detection problems", "authors": ["G. Rigaill", "E. Lebarbier", "S. Robin"], "venue": "Statistics and Computing,", "year": 2012}, {"title": "Properties and refinements of the fused lasso", "authors": ["A. Rinaldo"], "venue": "The Annals of Statistics,", "year": 2009}, {"title": "On change point detection using the fused lasso method", "authors": ["C.R. Rojas", "B. Wahlberg"], "year": 2014}, {"title": "Nonlinear total variation based noise removal algorithms", "authors": ["L.I. Rudin", "S. Osher", "E. Fatemi"], "venue": "Physica D: Nonlinear Phenomena,", "year": 1992}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "authors": ["B. Sch\u00f6lkopf", "A. Smola", "K.R. M\u00fcller"], "venue": "Neural Computation,", "year": 1998}, {"title": "Multiscale local change point detection with applications to value-at-risk", "authors": ["V. Spokoiny"], "venue": "The Annals of Statistics,", "year": 2009}, {"title": "Universality, characteristic kernels and rkhs embedding of measures", "authors": ["B.K. Sriperumbudur", "K. Fukumizu", "G.R.G. Lanckriet"], "venue": "Journal of Machine Learning Research,", "year": 2011}, {"title": "Hypothesis Testing and Changepoint Detection", "authors": ["A. Tartakovsky", "I.V. Nikiforov", "M. Basseville"], "venue": "Chapman and Hall, Monographs on Statistics and Applied Probability,", "year": 2014}, {"title": "Sparsity and smoothness via the fused lasso", "authors": ["R. Tibshirani", "M. Saunders", "S. Rosset", "J. Zhu", "K. Knight"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "year": 2005}, {"title": "Efficient generalized fused lasso and its application to the diagnosis of alzheimer\u00e2\u0102\u0179s disease", "authors": ["B. Xin", "Y. Kawahara\u00e2\u0102\u0103", "Y. Wang", "W. Gao"], "venue": "In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence,", "year": 2014}], "id": "SP:59831fdf67d2a058575295d00aad89321e42938a", "authors": [{"name": "Othmane Mazhar", "affiliations": []}, {"name": "Cristian R. Rojas", "affiliations": []}, {"name": "Carlo Fischione", "affiliations": []}, {"name": "Mohammad Reza Hesamzadeh", "affiliations": []}], "abstractText": "We address a generalization of change point detection with the purpose of detecting the change locations and the levels of clusters of a piecewise constant signal. Our approach is to model it as a nonparametric penalized least square model selection on a family of models indexed over the collection of partitions of the design points and propose a computationally efficient algorithm to approximately solve it. Statistically, minimizing such a penalized criterion yields an approximation to the maximum a-posteriori probability (MAP) estimator. The criterion is then analyzed and an oracle inequality is derived using a Gaussian concentration inequality. The oracle inequality is used to derive on one hand conditions for consistency and on the other hand an adaptive upper bound on the expected square risk of the estimator, which statistically motivates our approximation. Finally, we apply our algorithm to simulated data to experimentally validate the statistical guarantees and illustrate its behavior.", "title": "Bayesian Model Selection for Change Point Detection and Clustering"}