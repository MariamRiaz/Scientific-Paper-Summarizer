0	15	In scientific fields, scholars need to contextualize their contribution to help readers acquire an understanding of their research papers.
1	30	For this purpose, the related work section of an article serves as a pivot to connect prior domain knowledge, in which the innovation and superiority of current work are displayed by a comparison with previous studies.
2	25	While citation prediction can assist in drafting a reference collection (Nallapati et al., 2008), consuming all these papers is still a laborious job, where authors must read every source document carefully and locate the most relevant content cautiously.
3	47	As a solution in saving authors’ efforts, automatic related work summarization is essentially a topic-biased multi-document problem (Cong and Kan, 2010), which relies heavily on humanengineered features to retrieve snippets from the references.
6	27	To address the summarization alignment, former studies try to apply an attention mechanism to measure the saliency/novelty of each candidate word/sentence (Tan et al., 2017), with the aim of locating the most representative content to retain primary coverage.
9	15	Generally speaking, for a pair of documents, a larger lexical overlap often implies a higher similarity in their research backgrounds.
11	27	Take “DSSM”1 as an example, from viewpoint of the abstract similarity, those references investigating “Information Retrieval”, “Latent Semantic Model” or “Clickthrough Data Mining” could be of more importance in correlation and should be greatly sampled for the related work section.
12	18	But in reality, this article spends a bit larger chunk of texts (about 58%) to elaborate “Deep Learning” during the literature review, which is quite difficult for machines to grasp the contextual relevance therein.
13	15	In addition, other situations like emerging new concepts also suffer from the terminology variation or paraphrasing in varying degrees.
15	19	Over the recent past, there is a surge of interest in exploiting diverse relations to analyze bibliometrics, ranging from literature recommendation (Yu et al., 2015) to topic evolvement (Jensen et al., 2016).
16	33	In a graphical sense, interconnected papers transfer the credit among each other directly/indirectly through various patterns, such as paper citation, author collaboration, keyword association and releasing on series of venues, which constitutes the graphic context for outlining concerned topics.
18	17	Meanwhile, most existing solutions in mining heterogeneous graphs depend on the human supervision, e.g., hyperedge (Bu et al., 2010) and metapath (Swami et al., 2017).
21	42	Second, we develop a novel seq2seq summarizer for the automatic related work summarization, where a joint context-driven attention mechanism is proposed to measure the contextual relevance within both textual and graphic contexts.
43	14	Given an unedited paper t (target document) and its n-size reference collection Rt = {rt1:n}, we draw up a related work section for t by selecting sentences from Rt.
45	15	For each candidate sentence stj , once being visited, a label y t j ∈ {0, 1} will be determined synchronously based on whether or not this sentence should be covered into the output.
55	24	In this study, we propose an unsupervised approach to capture the connectivity diversity, by introducing an optimal EUD for navigating random walkers on the heterogeneous bibliography graph.
57	22	On this basis, a well-performing algorithm node2vec (Grover and Leskovec, 2016) is adopted to conduct an unsupervised random walk to vectorize every node ∀v∗ ∈ V into a d-dimensional embedding ϕ(v∗) ∈ Rd so that any edge ∀e∗ ∈ E can be calculated therefrom.
73	20	Specifically, the CNN deals with word-level texts to derive sentencelevel meanings, which are then taken as inputs to the RNN for handling longer-range dependency within lager units like a paragraph and even a whole paper.
87	34	Given Ht = {ht1:m}, this decoder returns the probability of ytj = 1 as below: Pr(ytj = 1 | Rt; St; θ) = sigmoid ( δ(htj , h̄ t j ) ) (8) h̄tj = m∑ i=1 aj,ih t i (9) where δ(htj , h̄ t j ) ∈ R denotes a fully connected layer with as input the concatenation of htj and h̄ t j , and aj,i ∈ [0, 1] is the attention weight indicating how much the supporting sentence sti contributes to extracting the candidate one stj .
88	16	Apart from saliency and novelty two traditional attention factors (Chen et al., 2016; Tan et al., 2017), we focus on the contextual relevance within both textual and graphic contexts to distinguish the relationship from near to far, as shown in Eq.
89	15	To be specific: 1) htTj Wsh t i represents the saliency of sti to s t j ; 2) −dtTj Wnhti indicates the novelty of sti to the dynamic output d t j ; 3) φ(t)TWthti denotes the relevance of s t i to t from the textual context; 4) ϕ(t)TWgϕ(hti ) refers to the relevance from the graphic context.
92	38	aj,i = h tT j Wsh t i # saliency −dtTj Wnhti # novelty +φ(t)TWth t i # relevance1 +ϕ(t)TWgϕ(h t i ) # relevance2 (10) dtj = j−1∑ i=1 Pr(ytj = 1 | Rt; St; θ)× hti (11) The basic idea behind our attention mechanism is as follows: if a supporting sentence more resembles a candidate one, or overlaps less with the dynamic output, or is more relevant to the target document, then it can provide more contextual information to facilitate current decision on being extracted or not, thereby taking a higher weight in the generated context vector.
96	16	Dataset We conduct experiments on a dataset2 created from the ACM digital library, where metadata and full texts are derived from PDF files.
101	24	On this basis, a total of 8,080 papers are selected to evaluate our approach, each containing more than 15 references found in the dataset and a related work section of at least 500 words.
124	17	From the top half, all scores appear a gradual upward trend with incorporation of saliency, novelty, relevance (from both textual and graphic contexts) and EUD into consideration one after another, which demonstrates the validity of our attention mechanism for summarizing related work sections.
125	23	To be specific, we further reach the following conclusions: 1) P.void vs. P.S vs. P.S+N: Both saliency and novelty are two effective factors to locate the required content for summaries, which is consistent with prior studies.
150	22	We develop a neural data-driven summarizer by leveraging the seq2seq paradigm, where a joint context-driven attention mechanism is proposed to measure the contextual relevance within full texts and a heterogeneous bibliography graph simultaneously.
152	101	In future work, an appealing direction is to organize the selected sentences in a logical fashion, e.g., by leveraging a topic hierarchy tree to determine the arrangement of the related work section (Cong and Kan, 2010).
153	213	We also would like to take the citation sentences of each reference into consideration, which is another concise and universal data source for scientific summarization (Chen and Hai, 2016; Cohan and Goharian, 2017).
154	192	At the end of this paper, we believe that extractive methods are by no means the final solutions for literature review generation due to plagiarism concerns, and we are going to put forward a fully abstractive version in further studies.
