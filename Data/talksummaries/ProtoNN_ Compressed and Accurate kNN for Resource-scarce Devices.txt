6	75	However, machine learning in IoT scenarios is so far limited to cloud-based predictions where large deep learning models are deployed to provide accurate predictions.
7	60	The sensors/embedded devices have limited compute/storage abilities and are tasked only with sensing and transmitting data to the cloud.
8	14	Such a solution does not take into account several practical concerns like privacy, bandwidth, latency and battery issues.
10	30	Consider a typical IoT device that has ≤ 32kB RAM and a 16MHz processor.
14	31	Moreover, they do not offer natural extensions to supervised learning problems other than the ones they were initially designed for.
15	37	In this paper, we propose a novel kNN based algorithm (ProtoNN) that can be deployed on the tiniest of devices, can handle general supervised learning problems, and can produce state-of-the-art accuracies with just ≈16kB of model size on many benchmark datasets.
17	28	However, kNN suffers from three issues which limit its applicability in practice, especially in the small devices setting: a) Poor accuracy: kNN is an ill-specified algorithm as it is not a priori clear which distance metric one should use to compare a given set of points.
18	26	Standard metrics like Euclidean distance, `1 distance etc.
20	61	b) Model size: kNN requires the entire training data for prediction, so its model size is too large for the IoT setting.
26	26	Finally, recent methods like Stochastic Neighborhood Compression (SNC) (Kusner et al., 2014) can decrease model size and prediction time by learning a small number of prototypes to represent the entire training dataset.
31	23	Moreover, we learn labels for each prototype to further boost accuracy.
38	44	We analyze ProtoNN in a simple binary classification setting where the data is sampled from a mixture of two wellseparated Gaussians, each Gaussian representing one class.
87	18	,bm] and the corresponding score vectors Z = [z1, .
106	10	Note that the sparsity constraints in the above objective gives us explicit control over the model size.
122	35	Step-size: Setting correct step-size is critical to convergence of SGD methods, especially for non-convex optimization problems.
124	11	Subsequent step sizes are selected as ηt = η0/t where η0 is the initial step-size.
130	18	In the other approach, we run k-means clustering in the transformed space on data points belonging to each class and pick the cluster centers as our prototypes.
144	19	the prototypes B while fixing projection matrix W and prototype label vectors Z.
163	17	Then, R with W = I and Z = [e1, e2] is a strongly convex function of B with condition number bounded by 20.
175	11	Hyperparameters: In all our experiments, we fix the no.
222	15	Right table of Figure 2 presents preliminary results on multilabel datasets.
223	14	Here, we compare ProtoNN with SLEEC, FastXML and DiSMEC (Babbar & Shölkopf, 2016), which learns a 1vsA linear-SVM in a distributed fashion.
224	184	ProtoNN almost matches the performance of all baselines with huge reduction in model size.
226	33	SNC doesn’t have such flexibility.
227	23	For example, it can’t be naturally extended to handle multilabel classification problems.
231	10	Figure 3 presents the results from this experiment on mnist binary dataset.
233	20	For SNC, we hard threshold the input transformation matrix so that it has sparsity 0.1.
238	67	Right Table: ProtoNN vs baselines on multilabel datasets.
241	17	On aloi our method is at most 2 slower than 1-vs-all while RBF-SVM is 115× slower.
250	30	Fixing m/L to a reasonable value such as 3-10 for medium L, 1-2 for large L typically gives good accuracies.
