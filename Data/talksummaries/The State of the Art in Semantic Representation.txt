0	47	Schemes for Semantic Representation of Text (SRT) aim to reflect the meaning of sentences and texts in a transparent way.
1	76	There has recently been an influx of proposals for semantic representations and corpora, e.g. GMB (Basile et al., 2012), AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013b) and Universal Decompositional Semantics (UDS; White et al., 2016).
2	54	Nevertheless, no detailed assessment of the relative merits of the different schemes has been carried out, nor their comparison to previous sentential analysis schemes, notably syntactic ones.
3	21	An understanding of the achievements and gaps of semantic analysis in NLP is crucial to its future prospects.
4	125	In this paper we begin to chart the various proposals for semantic schemes according to the content they support.
5	47	As not many semantic queries on texts can at present be answered with near human-like reliability without using manual symbolic annotation, we will mostly focus on schemes that represent semantic distinctions explicitly.1 We begin by discussing the goals of SRT in Section 2.
8	45	We find that despite the major differences in terms of formalism and interface with syntax, in terms of their content there is a great deal of convergence of SRT schemes.
36	21	Events (sometimes called frames, propositions or scenes) are the basic building blocks of argument structure representations.
37	23	An event includes a predicate (main relation, frame-evoking element), which is the main determinant of what the event is about.
38	21	It also includes arguments (participants, core elements) and secondary relations (modifiers, non-core elements).
88	23	Logical structure, including quantification, negation, coordination and their associated scope distinctions, is the cornerstone of semantic analysis in much of theoretical linguistics, and has attracted much attention in NLP as well.
91	19	A primary motivation for many semantic schemes is their ability to support inference and entailment.
99	68	SRL schemes diverge in their event types, the type of predicates they cover, their granularity, their cross-linguistic applicability, their organizing principles and their relation with syntax.
128	22	The PDT-TL results from an abstraction over PDT’s syntactic layers, and its close relation with syntax is apparent.
137	32	The Broad-coverage Semantic Dependency Parsing shared task and corpora (Oepen et al., 2014, 2015) include corpora annotated with the PDT-TL, and dependencies extracted from the HPSG grammars Enju (Miyao, 2006) and the LinGO English Reference Grammar (ERG; Flickinger, 2002).
139	17	While this approach provides powerful tools for inference, type checking, and mapping into external formal languages, it also often results in difficulties in abstracting away from some syntactic details.
140	72	For instance, the dependencies derived from ERG in the SDP corpus use the same label for different senses of the English possessive construction, regardless of whether they correspond to ownership (e.g., “John’s dog”) or to a different meaning, such as marking an argument of a nominal predicate (e.g., “John’s kick”).
141	18	OntoNotes is a useful resource with multiple inter-linked layers of annotation, borrowed from different schemes.
143	67	Some properties of the predicate, such as which nouns are eventive, are encoded as well.
145	39	The fundamental difference between the schemes is the extent that they abstract away from syntax.
146	46	For instance, AMR and UCCA abstract away from syntax as part of their design, while in most other schemes syntax and semantics are more tightly coupled.
147	16	Schemes also differ in other aspects discussed in Sections 5 and 6.
149	116	Determining how well an SRT scheme corresponds to human interpretation of a text is ideally carried out by asking annotators to make some semantic prediction or annotation according to pre-specified guidelines, and to compare this to the information extracted from the SRT.
150	57	Question Answering SRL (QASRL; He et al., 2015) is an SRL scheme which solicits nonexperts to answer mostly wh-questions, converting their output to an SRL annotation.
151	16	Hartshorne et al. (2013) and Reisinger et al. (2015) use crowdsourcing to elicit semantic role features, such as whether the argument was volitional in the described event, in order to evaluate proposals for semantic role sets.
153	52	Many semantic representations in NLP are defined with an application in mind, making this type of evaluation natural.
156	104	Another common criterion for evaluating a semantic scheme is invariance, where semantic analysis should be similar across paraphrases or translation pairs (Xue et al., 2014; Sulem et al., 2015).
157	135	For instance, most SRL schemes abstract away from the syntactic divergence between the sentences (1) “He gave a present to John” and (2) “It was John who was given a present” (although a complete analysis would reflect the difference of focus between them).
158	40	Importantly, these evaluation criteria also apply in cases where the representation is automatically induced, rather than manually defined.
160	23	Finally, where semantic schemes are induced through manual annotation (and not through au- tomated procedures), a common criterion for determining whether the guidelines are sufficiently clear, and whether the categories are well-defined is to measure agreement between annotators, by assigning them the same texts and measuring the similarity of the resulting structures.
