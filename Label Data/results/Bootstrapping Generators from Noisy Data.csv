0,1,label2,summary_sentences
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 908–916, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
Neural networks have proven to be highly effective at many tasks in natural language.,1 Introduction,[0],[0]
"For example, neural language models and joint language/translation models improve machine translation quality significantly (Vaswani et al., 2013; Devlin et al., 2014).",1 Introduction,[0],[0]
"However, neural networks can be complicated to design and train well.",1 Introduction,[0],[0]
"Many decisions need to be made, and performance can be highly dependent on making them correctly.",1 Introduction,[0],[0]
"Yet the optimal settings are non-obvious and can be laborious to find, often requiring an extensive grid search involving numerous experiments.
",1 Introduction,[0],[0]
"In this paper, we focus on the choice of the sizes of hidden layers.",1 Introduction,[0],[0]
"We introduce a method for automatically pruning out hidden layer units, by adding a sparsity-inducing regularizer that encourages units to deactivate if not needed, so that
they can be removed from the network.",1 Introduction,[0],[0]
"Thus, after training with more units than necessary, a network is produced that has hidden layers correctly sized, saving both time and memory when actually putting the network to use.
",1 Introduction,[0],[0]
"Using a neural n-gram language model (Bengio et al., 2003), we are able to show that our novel auto-sizing method is able to learn models that are smaller than models trained without the method, while maintaining nearly the same perplexity.",1 Introduction,[0],[0]
"The method has only a single hyperparameter to adjust (as opposed to adjusting the sizes of each of the hidden layers), and we find that the same setting works consistently well across different training data sizes, vocabulary sizes, and n-gram sizes.",1 Introduction,[0],[0]
"In addition, we show that incorporating these models into a machine translation decoder still results in large BLEU point improvements.",1 Introduction,[0],[0]
The result is that fewer experiments are needed to obtain models that perform well and are correctly sized.,1 Introduction,[0],[0]
Language models are often used in natural language processing tasks involving generation of text.,2 Background,[0],[0]
"For instance, in machine translation, the language model helps to output fluent translations, and in speech recognition, the language model helps to disambiguate among possible utterances.
",2 Background,[0],[0]
"Current language models are usually n-gram models, which look at the previous (n− 1) words to predict the nth word in a sequence, based on (smoothed) counts of n-grams collected from training data.",2 Background,[0],[0]
"These models are simple but very effective in improving the performance of natural language systems.
",2 Background,[0],[0]
"However, n-gram models suffer from some limitations, such as data sparsity and memory usage.",2 Background,[0],[0]
"As an alternative, researchers have begun exploring the use of neural networks for language modeling.",2 Background,[0],[0]
"For modeling n-grams, the most common approach is the feedforward network of Bengio et
908
al. (2003), shown in Figure 1.",2 Background,[0],[0]
"Each node represents a unit or “neuron,” which has a real valued activation.",2 Background,[0],[0]
The units are organized into real-vector valued layers.,2 Background,[0],[0]
The activations at each layer are computed as follows.,2 Background,[0],[0]
(We assume n = 3; the generalization is easy.),2 Background,[0],[0]
"The two preceding words, w1, w2, are mapped into lowerdimensional word embeddings,
x1",2 Background,[0],[0]
= A:w1 x2 =,2 Background,[0],[0]
"A:w2
then passed through two hidden layers,
y = f(B1x1 +",2 Background,[0],[0]
B2x2 + b) z,2 Background,[0],[0]
"= f(Cy + c)
where f is an elementwise nonlinear activation (or transfer) function.",2 Background,[0],[0]
"Commonly used activation functions are the hyperbolic tangent, logistic function, and rectified linear units, to name a few.",2 Background,[0],[0]
"Finally, the result is mapped via a softmax to an output probability distribution,
P (wn | w1 · · ·wn−1) ∝",2 Background,[0],[0]
exp([Dz + d]wn).,2 Background,[0],[0]
"The parameters of the model are A, B1, B2, b, C, c, D, and d, which are learned by minimizing the negative log-likelihood of the the training data using stochastic gradient descent (also known as backpropagation) or variants.
",2 Background,[0],[0]
"Vaswani et al. (2013) showed that this model, with some improvements, can be used effectively during decoding in machine translation.",2 Background,[0],[0]
"In this paper, we use and extend their implementation.",2 Background,[0],[0]
Our method is focused on the challenge of choosing the number of units in the hidden layers of a feed-forward neural network.,3 Methods,[0],[0]
"The networks used for different tasks require different numbers of units, and the layers in a single network also require different numbers of units.",3 Methods,[0],[0]
"Choosing too few units can impair the performance of the network, and choosing too many units can lead to overfitting.",3 Methods,[0],[0]
"It can also slow down computations with the network, which can be a major concern for many applications such as integrating neural language models into a machine translation decoder.
",3 Methods,[0],[0]
Our method starts out with a large number of units in each layer and then jointly trains the network while pruning out individual units when possible.,3 Methods,[0],[0]
"The goal is to end up with a trained network
that also has the optimal number of units in each layer.
",3 Methods,[0],[0]
We do this by adding a regularizer to the objective function.,3 Methods,[0],[0]
"For simplicity, consider a single layer without bias, y = f(Wx).",3 Methods,[0],[0]
Let L(W) be the negative log-likelihood of the model.,3 Methods,[0],[0]
"Instead of minimizing L(W) alone, we want to minimize L(W)",3 Methods,[0],[0]
"+ λR(W), where R(W) is a convex regularizer.",3 Methods,[0],[0]
"The `1 norm, R(W) = ‖W‖1 =∑
i,j |Wij |, is a common choice for pushing parameters to zero, which can be useful for preventing overfitting and reducing model size.",3 Methods,[0],[0]
"However, we are interested not only in reducing the number of parameters but the number of units.",3 Methods,[0],[0]
"To do this, we need a different regularizer.
",3 Methods,[0],[0]
"We assume activation functions that satisfy f(0) = 0, such as the hyperbolic tangent or rectified linear unit (f(x) = max{0, x}).",3 Methods,[0],[0]
"Then, if we push the incoming weights of a unit yi to zero, that is, Wij = 0 for all j (as well as the bias, if any: bi = 0), then yi = f(0) = 0 is independent of the previous layers and contributes nothing to subsequent layers.",3 Methods,[0],[0]
So the unit can be removed without affecting the network at all.,3 Methods,[0],[0]
"Therefore, we need a regularizer that pushes all the incoming connection weights to a unit together towards zero.
",3 Methods,[0],[0]
"Here, we experiment with two, the `2,1 norm and the `∞,1 norm.1 The `2,1 norm on a ma-
1In the notation `p,q , the subscript p corresponds to the norm over each group of parameters, and q corresponds to the norm over the group norms.",3 Methods,[0],[0]
"Contrary to more common usage, in this paper, the groups are rows, not columns.
",3 Methods,[0],[0]
"trix W is
R(W) =",3 Methods,[0],[0]
∑ i ‖Wi:‖2 = ∑ i ∑ j W 2ij  12 .,3 Methods,[0],[0]
"(1) (If there are biases bi, they should be included as well.)",3 Methods,[0],[0]
"This puts equal pressure on each row, but within each row, the larger values contribute more, and therefore there is more pressure on larger values towards zero.",3 Methods,[0],[0]
"The `∞,1 norm is
R(W) =",3 Methods,[0],[0]
∑ i ‖Wi:‖∞ = ∑ i max j |Wij |.,3 Methods,[0],[0]
"(2)
Again, this puts equal pressure on each row, but within each row, only the maximum value (or values) matter, and therefore the pressure towards zero is entirely on the maximum value(s).
",3 Methods,[0],[0]
Figure 2 visualizes the sparsity-inducing behavior of the two regularizers on a single row.,3 Methods,[0],[0]
Both have a sharp tip at the origin that encourages all the parameters in a row to become exactly zero.,3 Methods,[0],[0]
"However, this also means that sparsity-inducing regularizers are not differentiable at zero, making gradient-based optimization methods trickier to apply.",4 Optimization,[0],[0]
"The methods we use are discussed in detail elsewhere (Duchi et al., 2008; Duchi and Singer, 2009); in this section, we include a short description of these methods for completeness.",4 Optimization,[0],[0]
"Most work on learning with regularizers, including this work, can be thought of as instances of the proximal gradient method (Parikh and Boyd, 2014).",4.1 Proximal gradient method,[0],[0]
"Our objective function can be split into two parts, a convex and differentiable part (L) and a
convex but non-differentiable part (λR).",4.1 Proximal gradient method,[0],[0]
"In proximal gradient descent, we alternate between improving L alone and λR alone.",4.1 Proximal gradient method,[0],[0]
Let u be the parameter values from the previous iteration.,4.1 Proximal gradient method,[0],[0]
"We compute new parameter values w using:
v← u− η∇L(u) (3)
w← arg max w",4.1 Proximal gradient method,[0],[0]
( 1 2η ‖w − v‖2 + λR(w) ),4.1 Proximal gradient method,[0],[0]
"(4)
and repeat until convergence.",4.1 Proximal gradient method,[0],[0]
The first update is just a standard gradient descent update on L; the second is known as the proximal operator for λR and in many cases has a closed-form solution.,4.1 Proximal gradient method,[0],[0]
"In the rest of this section, we provide some justification for this method, and in Sections 4.2 and 4.3 we show how to compute the proximal operator for the `2 and `∞ norms.
",4.1 Proximal gradient method,[0],[0]
We can think of the gradient descent update (3) on L as follows.,4.1 Proximal gradient method,[0],[0]
"Approximate L around u by the tangent plane,
L̄(v) = L(u) +∇L(u)(v − u) (5)
and move v to minimize L̄, but don’t move it too far from u; that is, minimize
F (v) = 1",4.1 Proximal gradient method,[0],[0]
2η ‖v,4.1 Proximal gradient method,[0],[0]
"− u‖2 + L̄(v).
",4.1 Proximal gradient method,[0],[0]
"Setting partial derivatives to zero, we get
∂F ∂v = 1 η
(v − u) +∇L(u) = 0 v = u− η∇L(u).
",4.1 Proximal gradient method,[0],[0]
"By a similar strategy, we can derive the second step (4).",4.1 Proximal gradient method,[0],[0]
"Again we want to move w to minimize the objective function, but don’t want to move it too far from u; that is, we want to minimize:
G(w) = 1 2η ‖w",4.1 Proximal gradient method,[0],[0]
"− u‖2 + L̄(w) + λR(w).
",4.1 Proximal gradient method,[0],[0]
Note that we have not approximated R by a tangent plane.,4.1 Proximal gradient method,[0],[0]
We can simplify this by substituting in (3).,4.1 Proximal gradient method,[0],[0]
"The first term becomes
1 2η ‖w",4.1 Proximal gradient method,[0],[0]
− u‖2 = 1 2η ‖w,4.1 Proximal gradient method,[0],[0]
"− v − η∇L(u)‖2
= 1 2η ‖w − v‖2 −∇L(u)(w − v)
+ η
2 ‖∇L(u)‖2
and the second term becomes
L̄(w) = L(u) +∇L(u)(w − u) = L(u) +∇L(u)(w",4.1 Proximal gradient method,[0],[0]
"− v − η∇L(u)).
",4.1 Proximal gradient method,[0],[0]
"The ∇L(u)(w − v) terms cancel out, and we can ignore terms not involving w, giving
G(w) = 1 2η ‖w",4.1 Proximal gradient method,[0],[0]
"− v‖2 + λR(w) + const.
which is minimized by the update (4).",4.1 Proximal gradient method,[0],[0]
"Thus, we have split the optimization step into two easier steps: first, do the update for L (3), then do the update for λR (4).",4.1 Proximal gradient method,[0],[0]
The latter can often be done exactly (without approximating R by a tangent plane).,4.1 Proximal gradient method,[0],[0]
"We show next how to do this for the `2 and `∞ norms.
4.2 `2 and `2,1 regularization Since the `2,1 norm on matrices (1) is separable into the `2 norm of each row, we can treat each row separately.",4.1 Proximal gradient method,[0],[0]
"Thus, for simplicity, assume that we have a single row and want to minimize
G(w) = 1 2η ‖w − v‖2 +",4.1 Proximal gradient method,[0],[0]
"λ‖w‖+ const.
",4.1 Proximal gradient method,[0],[0]
"The minimum is either at w = 0 (the tip of the cone) or where the partial derivatives are zero (Figure 3):
∂G ∂w = 1 η (w − v) + λ w‖w‖",4.1 Proximal gradient method,[0],[0]
"= 0.
",4.1 Proximal gradient method,[0],[0]
"Clearly, w and v must have the same direction and differ only in magnitude, that is, w = α v‖v‖ .",4.1 Proximal gradient method,[0],[0]
"Substituting this into the above equation, we get the solution
α = ‖v‖",4.1 Proximal gradient method,[0],[0]
− ηλ.,4.1 Proximal gradient method,[0],[0]
"Therefore the update is
w = α v ‖v‖
α = max(0, ‖v‖ − ηλ).",4.1 Proximal gradient method,[0],[0]
"As above, since the `∞,1 norm on matrices (2) is separable into the `∞ norm of each row, we can treat each row separately; thus, we want to minimize
G(w) = 1 2η ‖w − v‖2 + λmax j |xj |+ const.
","4.3 `∞ and `∞,1 regularization",[0],[0]
"Intuitively, the solution can be characterized as: Decrease all of the maximal |xj | until the total decrease reaches ηλ or all the xj are zero.","4.3 `∞ and `∞,1 regularization",[0],[0]
"See Figure 4.
","4.3 `∞ and `∞,1 regularization",[0],[0]
"If we pre-sort the |xj | in nonincreasing order, it’s easy to see how to compute this: for ρ = 1, . . .","4.3 `∞ and `∞,1 regularization",[0],[0]
", n, see if there is a value ξ ≤ xρ","4.3 `∞ and `∞,1 regularization",[0],[0]
"such that decreasing all the x1, . . .","4.3 `∞ and `∞,1 regularization",[0],[0]
", xρ to ξ amounts to a total decrease of ηλ.","4.3 `∞ and `∞,1 regularization",[0],[0]
"The largest ρ for which this is possible gives the correct solution.
","4.3 `∞ and `∞,1 regularization",[0],[0]
"But this situation seems similar to another optimization problem, projection onto the `1-ball, which Duchi et al. (2008) solve in linear time without pre-sorting.","4.3 `∞ and `∞,1 regularization",[0],[0]
"In fact, the two problems can be solved by nearly identical algorithms, because they are convex conjugates of each other (Duchi and Singer, 2009; Bach et al., 2012).","4.3 `∞ and `∞,1 regularization",[0],[0]
"Intuitively, the `1 projection of v is exactly what is cut out by the `∞ proximal operator, and vice versa (Figure 4).
","4.3 `∞ and `∞,1 regularization",[0],[0]
Duchi et al.’s algorithm modified for the present problem is shown as Algorithm 1.,"4.3 `∞ and `∞,1 regularization",[0],[0]
It partitions the xj about a pivot element (line 6) and tests whether it and the elements to its left can be decreased to a value ξ such that the total decrease is δ (line 8).,"4.3 `∞ and `∞,1 regularization",[0],[0]
"If so, it recursively searches the right side; if not, the
left side.","4.3 `∞ and `∞,1 regularization",[0],[0]
"At the conclusion of the algorithm, ρ is set to the largest value that passes the test (line 13), and finally the new xj are computed (line 16) – the only difference from Duchi et al.’s algorithm.
","4.3 `∞ and `∞,1 regularization",[0],[0]
This algorithm is asymptotically faster than that of Quattoni et al. (2009).,"4.3 `∞ and `∞,1 regularization",[0],[0]
"They reformulate `∞,1 regularization as a constrained optimization problem (in which the `∞,1 norm is bounded by µ) and provide a solution inO(n log n) time.","4.3 `∞ and `∞,1 regularization",[0],[0]
"The method shown here is simpler and faster because it can work on each row separately.
","4.3 `∞ and `∞,1 regularization",[0],[0]
"Algorithm 1 Linear-time algorithm for the proximal operator of the `∞ norm.
1: procedure UPDATE(w, δ) 2: lo, hi← 1, n 3: s← 0 4:","4.3 `∞ and `∞,1 regularization",[0],[0]
"while lo ≤ hi do 5: select md randomly from lo, . . .","4.3 `∞ and `∞,1 regularization",[0],[0]
", hi 6: ρ← PARTITION(w, lo,md, hi) 7: ξ ← 1ρ","4.3 `∞ and `∞,1 regularization",[0],[0]
"( s+ ∑ρ i=lo |xi| − δ
) 8: if ξ ≤ |xρ| then 9: s← s+∑ρi=lo |xi|
10: lo← ρ+ 1 11: else 12: hi← ρ− 1 13: ρ← hi 14: ξ ← 1ρ (s− δ) 15: for i← 1, . . .","4.3 `∞ and `∞,1 regularization",[0],[0]
", n","4.3 `∞ and `∞,1 regularization",[0],[0]
"do 16: xi ← min(max(xi,−ξ), ξ) 17: procedure PARTITION(w, lo,md, hi) 18: swap xlo and xmd 19: i← lo + 1 20: for j ← lo + 1, . . .","4.3 `∞ and `∞,1 regularization",[0],[0]
", hi do 21: if xj ≥ xlo then 22: swap xi and xj 23: i← i+ 1 24: swap xlo and xi−1 25: return i− 1","4.3 `∞ and `∞,1 regularization",[0],[0]
"We evaluate our model using the open-source NPLM toolkit released by Vaswani et al. (2013), extending it to use the additional regularizers as described in this paper.2 We use a vocabulary size of 100k and word embeddings with 50 dimensions.",5 Experiments,[0],[0]
"We use two hidden layers of rectified linear units (Nair and Hinton, 2010).
",5 Experiments,[0],[0]
"2These extensions have been contributed to the NPLM project.
",5 Experiments,[0],[0]
"We train neural language models (LMs) on two natural language corpora, Europarl v7 English and the AFP portion of English Gigaword 5.",5 Experiments,[0],[0]
"After tokenization, Europarl has 56M tokens and Gigaword AFP has 870M tokens.",5 Experiments,[0],[0]
"For both corpora, we hold out a validation set of 5,000 tokens.",5 Experiments,[0],[0]
"We train each model for 10 iterations over the training data.
",5 Experiments,[0],[0]
Our experiments break down into three parts.,5 Experiments,[0],[0]
"First, we look at the impact of our pruning method on perplexity of a held-out validation set, across a variety of settings.",5 Experiments,[0],[0]
"Second, we take a closer look at how the model evolves through the training process.",5 Experiments,[0],[0]
"Finally, we explore the downstream impact of our method on a statistical phrase-based machine translation system.",5 Experiments,[0],[0]
"We first look at the impact that the `∞,1 regularizer has on the perplexity of our validation set.",5.1 Evaluating perplexity and network size,[0],[0]
The main results are shown in Table 1.,5.1 Evaluating perplexity and network size,[0],[0]
"For λ ≤ 0.01, the regularizer seems to have little impact: no hidden units are pruned, and perplexity is also not affected.",5.1 Evaluating perplexity and network size,[0],[0]
"For λ = 1, on the other hand, most hidden units are pruned – apparently too many, since perplexity is worse.",5.1 Evaluating perplexity and network size,[0],[0]
"But for λ = 0.1, we see that we are able to prune out many hidden units: up to half of the first layer, with little impact on perplexity.",5.1 Evaluating perplexity and network size,[0],[0]
"We found this to be consistent across all our experiments, varying n-gram size, initial hidden layer size, and vocabulary size.
",5.1 Evaluating perplexity and network size,[0],[0]
Table 2 shows the same information for 5-gram models trained on the larger Gigaword AFP corpus.,5.1 Evaluating perplexity and network size,[0],[0]
"These numbers look very similar to those on Europarl: again λ = 0.1 works best, and, counter to expectation, even the final number of units is similar.
",5.1 Evaluating perplexity and network size,[0],[0]
"Table 3 shows the result of varying the vocabulary size: again λ = 0.1 works best, and, although it is not shown in the table, we also found that the final number of units did not depend strongly on the vocabulary size.
",5.1 Evaluating perplexity and network size,[0],[0]
"Table 4 shows results using the `2,1 norm (Europarl corpus, 5-grams, 100k vocabulary).",5.1 Evaluating perplexity and network size,[0],[0]
"Since this is a different regularizer, there isn’t any reason to expect that λ behaves the same way, and indeed, a smaller value of λ seems to work best.",5.1 Evaluating perplexity and network size,[0],[0]
We also studied the evolution of the network over the training process to gain some insights into how the method works.,5.2 A closer look at training,[0],[0]
"The first question we want to
answer is whether the method is simply removing units, or converging on an optimal number of units.",5.2 A closer look at training,[0],[0]
"Figure 5 suggests that it is a little of both: if we start with too many units (900 or 1000), the method converges to the same number regardless of how many extra units there were initially.",5.2 A closer look at training,[0],[0]
"But if we start with a smaller number of units, the method still prunes away about 50 units.
",5.2 A closer look at training,[0],[0]
"Next, we look at the behavior over time of different regularization strengths λ.",5.2 A closer look at training,[0],[0]
"We found that not only does λ = 1 prune out too many units, it does so at the very first iteration (Figure 6, above), perhaps prematurely.",5.2 A closer look at training,[0],[0]
"By contrast, the λ = 0.1 run prunes out units gradually.",5.2 A closer look at training,[0],[0]
"By plotting these curves together with perplexity (Figure 6, below), we can see that the λ = 0.1 run is fitting the model and pruning it at the same time, which seems preferable to fitting without any pruning (λ =
0.01) or pruning first and then fitting (λ = 1).",5.2 A closer look at training,[0],[0]
"We can also visualize the weight matrix itself over time (Figure 7), for λ = 0.1.",5.2 A closer look at training,[0],[0]
"It is striking that although this setting fits the model and prunes it at the same time, as argued above, by the first iteration it already seems to have decided roughly how many units it will eventually prune.",5.2 A closer look at training,[0],[0]
We also looked at the impact of our method on statistical machine translation systems.,5.3 Evaluating on machine translation,[0],[0]
"We used the Moses toolkit (Koehn et al., 2007) to build a phrase based machine translation system with a traditional 5-gram LM trained on the target side of our bitext.",5.3 Evaluating on machine translation,[0],[0]
We augmented this system with neural LMs trained on the Europarl data and the Gigaword AFP data.,5.3 Evaluating on machine translation,[0],[0]
"Based on the results from the perplexity experiments, we looked at models both built with a λ = 0.1 regularizer, and without regularization (λ = 0).
",5.3 Evaluating on machine translation,[0],[0]
We built our system using the newscommentary dataset v8.,5.3 Evaluating on machine translation,[0],[0]
We tuned our model using newstest13 and evaluated using newstest14.,5.3 Evaluating on machine translation,[0],[0]
"After standard cleaning and tokenization, there were 155k parallel sentences in the newscommentary dataset, and 3,000 sentences each for the tuning and test sets.
",5.3 Evaluating on machine translation,[0],[0]
"Table 5 shows that the addition of a neural LM helps substantially over the baseline, with improvements of up to 2 BLEU.",5.3 Evaluating on machine translation,[0],[0]
"Using the Europarl model, the BLEU scores obtained without and with regularization were not significantly different (p ≥ 0.05), consistent with the negligible perplexity difference between these models.",5.3 Evaluating on machine translation,[0],[0]
"On the Gigaword AFP model, regularization did decrease the BLEU score by 0.3, consistent with the small perplexity increase of the regularized model.",5.3 Evaluating on machine translation,[0],[0]
"The decrease is statistically significant, but small compared with the overall benefit of adding a neural LM.",5.3 Evaluating on machine translation,[0],[0]
Researchers have been exploring the use of neural networks for language modeling for a long time.,6 Related Work,[0],[0]
Schmidhuber and Heil (1996) proposed a character n-gram model using neural networks which they used for text compression.,6 Related Work,[0],[0]
"Xu and Rudnicky (2000) proposed a word-based probability model using a softmax output layer trained using cross-entropy, but only for bigrams.",6 Related Work,[0],[0]
Bengio et al. (2003) defined a probabilistic word n-gram model and demonstrated improvements over conventional smoothed language models.,6 Related Work,[0],[0]
Mnih and Teh (2012) sped up training of log-bilinear language models through the use of noise-contrastive estimation (NCE).,6 Related Work,[0],[0]
"Vaswani et al. (2013) also used NCE to train the architecture of Bengio et al. (2003), and were able to integrate a largevocabulary language model directly into a machine translation decoder.",6 Related Work,[0],[0]
"Baltescu et al. (2014) describe a similar model, with extensions like a hierarchical softmax (based on Brown clustering) and direct n-gram features.
",6 Related Work,[0],[0]
"Beyond feed-forward neural network language models, researchers have explored using more complicated neural network architectures.",6 Related Work,[0],[0]
"RNNLM is an open-source implementation of a language model using recurrent neural networks (RNN) where connections between units can form directed cycles (Mikolov et al., 2011).",6 Related Work,[0],[0]
Sundermeyer et al. (2015) use the long-short term memory (LSTM) neural architecture to show a perplexity improvement over the RNNLM toolkit.,6 Related Work,[0],[0]
"In future work, we plan on exploring how our method could improve these more complicated neural models as well.
",6 Related Work,[0],[0]
Automatically limiting the size of neural networks is an old idea.,6 Related Work,[0],[0]
"The “Optimal Brain Damage” (OBD) technique (LeCun et al., 1989) computes a saliency based on the second derivative of the objective function with respect to each parameter.",6 Related Work,[0],[0]
"The parameters are then sorted by saliency, and the lowest-saliency parameters are pruned.",6 Related Work,[0],[0]
"The pruning process is separate from the training process, whereas regularization performs training and pruning simultaneously.",6 Related Work,[0],[0]
"Regularization in neural networks is also an old idea; for example, Nowland and Hinton (1992) mention both `22 and `0 regularization.",6 Related Work,[0],[0]
"Our method develops on this idea by using a mixed norm to prune units, rather than parameters.
",6 Related Work,[0],[0]
"Srivastava et al. introduce a method called dropout in which units are directly deactivated at random during training (Srivastava et al., 2014), which induces sparsity in the hidden unit activations.",6 Related Work,[0],[0]
"However, at the end of training, all units are reactivated, as the goal of dropout is to reduce overfitting, not to reduce network size.",6 Related Work,[0],[0]
"Thus, dropout and our method seem to be complementary.",6 Related Work,[0],[0]
"We have presented a method for auto-sizing a neural network during training by removing units using a `∞,1 regularizer.",7 Conclusion,[0],[0]
"This regularizer drives a unit’s input weights as a group down to zero, allowing the unit to be pruned.",7 Conclusion,[0],[0]
"We can thus prune units out of our network during training with minimal impact to held-out perplexity or downstream performance of a machine translation system.
",7 Conclusion,[0],[0]
"Our results showed empirically that the choice
of a regularization coefficient of 0.1 was robust to initial configuration parameters of initial network size, vocabulary size, n-gram order, and training corpus.",7 Conclusion,[0],[0]
"Furthermore, imposing a single regularizer on the objective function can tune all of the hidden layers of a network with one setting.",7 Conclusion,[0],[0]
"This reduces the need to conduct expensive, multi-dimensional grid searches in order to determine optimal sizes.
",7 Conclusion,[0],[0]
We have demonstrated the power and efficacy of this method on a feed-forward neural network for language modeling though experiments on perplexity and machine translation.,7 Conclusion,[0],[0]
"However, this method is general enough that it should be applicable to other domains, both inside natural language processing and outside.",7 Conclusion,[0],[0]
"As neural models become more pervasive in natural language processing, the ability to auto-size networks for fast experimentation and quick exploration will become increasingly important.",7 Conclusion,[0],[0]
"We would like to thank Tomer Levinboim, Antonios Anastasopoulos, and Ashish Vaswani for their helpful discussions, as well as the reviewers for their assistance and feedback.",Acknowledgments,[0],[0]
Neural networks have been shown to improve performance across a range of natural-language tasks.,abstractText,[0],[0]
"However, designing and training them can be complicated.",abstractText,[0],[0]
"Frequently, researchers resort to repeated experimentation to pick optimal settings.",abstractText,[0],[0]
"In this paper, we address the issue of choosing the correct number of units in hidden layers.",abstractText,[0],[0]
"We introduce a method for automatically adjusting network size by pruning out hidden units through `∞,1 and `2,1 regularization.",abstractText,[0],[0]
We apply this method to language modeling and demonstrate its ability to correctly choose the number of hidden units while maintaining perplexity.,abstractText,[0],[0]
We also include these models in a machine translation decoder and show that these smaller neural models maintain the significant improvements of their unpruned versions.,abstractText,[0],[0]
Auto-Sizing Neural Networks: With Applications to n-gram Language Models,title,[0],[0]
"There has been a staggering increase in progress on generative modeling in recent years, built largely upon fundamental advances such as generative adversarial networks (Goodfellow et al., 2014), variational inference (Kingma & Welling, 2013), and autoregressive density estimation (van den Oord et al., 2016c).",1. Introduction,[0],[0]
"These have led to breakthroughs in state-ofthe-art generation of natural images (Karras et al., 2017) and audio (van den Oord et al., 2016a), and even been used for unsupervised learning of disentangled representations (Higgins et al., 2017; Chen et al., 2016).",1. Introduction,[0],[0]
"These domains often have real-valued distributions with underlying metrics; that is, there is a domain-specific notion of similarity between data points.",1. Introduction,[0],[0]
"This similarity is ignored by the predominant work-horse of generative modeling, the Kullback-Leibler (KL) divergence.",1. Introduction,[0],[0]
"Progress is now being made towards algorithms that optimize with respect to these underlying metrics (Arjovsky et al., 2017; Bousquet et al., 2017).
",1. Introduction,[0],[0]
"*Equal contribution 1DeepMind, London, UK.",1. Introduction,[0],[0]
"Correspondence to: Georg Ostrovski <ostrovski@google.com>, Will Dabney <wdabney@google.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"In this paper, we present a novel approach to generative modeling, that, while strikingly different from existing methods, is grounded in the well-understood statistical methods of quantile regression.",1. Introduction,[0],[0]
"Unlike the majority of recent work, we approach generative modeling without the use of the KL divergence, and without explicitly approximating a likelihood model.",1. Introduction,[0],[0]
"Like GANs, in this way we produce an implicitly defined model, but unlike GANs our optimization procedure is inherently stable and lacks degenerate solutions which cause loss of diversity and mode collapse.
",1. Introduction,[0],[0]
"Much of the recent research on GANs has been focused on improving stability (Radford et al., 2015; Arjovsky et al., 2017; Daskalakis et al., 2017) and sample diversity (Gulrajani et al., 2017; Salimans et al., 2016; 2018).",1. Introduction,[0],[0]
"By stark contrast, methods such as PixelCNN (van den Oord et al., 2016b) readily produce high diversity, but due to their use of KL divergence are unable to make reasonable trade-offs between likelihood and perceptual similarity (Theis et al., 2015; Bellemare et al., 2017; Bousquet et al., 2017).
",1. Introduction,[0],[0]
"Our proposed method, autoregressive implicit quantile networks (AIQN), combines the benefits of both: a loss function that respects the underlying metric of the data leading to improved perceptual quality, and a stable optimization process leading to highly diverse samples.",1. Introduction,[0],[0]
"While there has been an increasing tendency towards complex architectures (Chen et al., 2017; Salimans et al., 2017) and multiple objective loss functions to overcome these challenges, AIQN is conceptually simple and does not rely on any special architecture or optimization techniques.",1. Introduction,[0],[0]
"Empirically it proves to be robust to hyperparameter variations and easy to optimize.
",1. Introduction,[0],[0]
"Our work is motivated by the recent advances achieved by reframing GANs in terms of optimal transport, leading to the Wasserstein GAN algorithm (Arjovsky et al., 2017), as well as work towards understanding the relationship between optimal transport and both GANs and VAEs (Bousquet et al., 2017).",1. Introduction,[0],[0]
"In agreement with these results, we focus on loss functions grounded in perceptually meaningful metrics.",1. Introduction,[0],[0]
"We build upon recent work in distributional reinforcement learning (Dabney et al., 2018a), which has begun to bridge the gap between approaches in reinforcement learning and unsupervised learning.",1. Introduction,[0],[0]
"Towards a practical algorithm we base our experimental results on Gated PixelCNN (van den Oord et al., 2016b), and show that using AIQN significantly im-
proves objective performance on CIFAR-10 and ImageNet 32x32 in terms of Fréchet Inception Distance (FID) and Inception score, as well as subjective perceptual quality in image samples and inpainting.",1. Introduction,[0],[0]
"We begin by establishing some notation, before turning to a review of three of the most prevalent methods for generative modeling.",2. Background,[0],[0]
"Calligraphic letters (e.g.X ) denote sets or spaces, capital letters (e.g. X) denote random variables, and lower case letters (e.g. x) indicate values.",2. Background,[0],[0]
"A probability distribution with random variable X ∈ X is denoted pX ∈P(X ), its cumulative distribution function (c.d.f.)",2. Background,[0],[0]
"FX , and inverse c.d.f. or quantile function QX = F−1X .",2. Background,[0],[0]
When probability distributions or quantile functions are parameterized by some θ,2. Background,[0],[0]
"we will write pθ or Qθ recognizing that here we do not view θ as a random variable.
",2. Background,[0],[0]
"Perhaps the simplest way to approach generative modeling of a random variableX ∈ X is by fixing some discretization ofX into n separate values, say x1, . . .",2. Background,[0],[0]
", xn ∈ X , and parameterize the approximate distribution with pθ(xi) ∝",2. Background,[0],[0]
exp(θi).,2. Background,[0],[0]
"This type of categorical parameterization is widely used, only slightly less commonly when X does not lend itself naturally to such a partitioning.",2. Background,[0],[0]
"Typically, the parameters θ are optimized to minimize the Kullback-Leibler (KL) divergence between observed values of X and the model pθ, θ∗ = arg minθDKL(pX‖pθ).",2. Background,[0],[0]
"However, this is only tractable whenX is a small discrete set or at best low-dimensional.",2. Background,[0],[0]
A common method for extending a generative model or density estimator to multivariate distributions is to factor the density as a product of scalarvalued conditional distributions.,2. Background,[0],[0]
"Let X = (X1, . . .",2. Background,[0],[0]
", Xn), then for any permutation of the dimensions σ :",2. Background,[0],[0]
"Nn → Nn,
pX(x) =",2. Background,[0],[0]
"n∏ i=1 pXσ(i)(xσ(i)|xσ(1), . . .",2. Background,[0],[0]
", xσ(i−1)).",2. Background,[0],[0]
"(1)
When the conditional density is modeled by a simple (e.g. Gaussian) base distribution, the ordering of the dimensions can be crucial (Papamakarios et al., 2017).",2. Background,[0],[0]
"However, it is common practice to choose an arbitrary ordering and rely upon a more powerful conditional model to avoid these problems.",2. Background,[0],[0]
"This class of models includes PixelRNN and PixelCNN (van den Oord et al., 2016c;b), MAF (Papamakarios et al., 2017), MADE (Germain et al., 2015), and many others.",2. Background,[0],[0]
"Fundamentally, all these approaches use the KL divergence as their loss function.
",2. Background,[0],[0]
"Another class of methods, generally known as latent variable methods, can bypass the need for autoregressive models using a different modeling assumption.",2. Background,[0],[0]
"Specifically, consider the Variational Autoencoder (VAE) (Kingma & Welling, 2013; Rezende et al., 2014), which represents
pθ as the marginalization over a latent random variable Z ∈ Z .",2. Background,[0],[0]
"The VAE is trained to maximize an approximate lower bound of the log-likelihood of the observations:
log pθ(x) ≥ −DKL(qθ(z|x)‖p(z))",2. Background,[0],[0]
+,2. Background,[0],[0]
E,2. Background,[0],[0]
"[log pθ(x|z)] .
",2. Background,[0],[0]
"Although VAEs are straightforward to implement and optimize, and effective at capturing structure in highdimensional spaces, they often miss fine-grained detail, resulting in blurry images.
",2. Background,[0],[0]
"Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) pose the problem of learning a generative model as a two-player zero-sum game between a discriminator D, attempting to distinguish between x ∼ pX (real data) and x ∼ pθ (generated data), and a generator G, attempting to generate data indistinguishable from real data.",2. Background,[0],[0]
"The generator is an implicit latent variable model that reparameterizes samples, typically from an isotropic Gaussian distribution, into values in X .",2. Background,[0],[0]
"The original formulation of GANs,
arg min G sup D",2. Background,[0],[0]
[ E X log(D(X)),2. Background,[0],[0]
"+ E Z log(1−D(G(Z))) ] ,
can be seen as minimizing a lower-bound on the JensenShannon divergence (Goodfellow et al., 2014; Bousquet et al., 2017).",2. Background,[0],[0]
"That is, even in the case of GANs we are often minimizing functions of the KL divergence1.
",2. Background,[0],[0]
"Many recent advances have come from principled combinations of these three fundamental methods (Makhzani et al., 2015; Dumoulin et al., 2016; Rosca et al., 2017).",2. Background,[0],[0]
"A common perspective in generative modeling is that the choice of model should encode existing metric assumptions about the domain, combined with a generic likelihoodfocused loss such as the KL divergence.",2.1. Distance Metrics and Loss Functions,[0],[0]
"Under this view, the KL’s general applicability and robust optimization properties make it a natural choice, and most implementations of the methods we reviewed in the previous section attempt to, at least indirectly, minimize a version of the KL.
",2.1. Distance Metrics and Loss Functions,[0],[0]
"On the other hand, as every model inevitably makes tradeoffs when constrained by capacity or limited training, it is desirable for its optimization goal to incentivize trade-offs prioritizing approximately correct solutions, when the data space is endowed with a metric supporting a meaningful (albeit potentially subjective) notion of approximation.",2.1. Distance Metrics and Loss Functions,[0],[0]
"It has been argued (Theis et al., 2015; Bousquet et al., 2017; Arjovsky et al., 2017; Bellemare et al., 2017) that the KL may not always be appropriate from this perspective, by making sub-optimal trade-offs between likelihood and similarity.
",2.1. Distance Metrics and Loss Functions,[0],[0]
"1The Jensen-Shannon divergence is the sum of KLs between distributions P,Q and their uniform mixture M = 0.5(P +Q): JSD(P ||Q)",2.1. Distance Metrics and Loss Functions,[0],[0]
"= 0.5(DKL(P ||M) +DKL(Q||M)).
",2.1. Distance Metrics and Loss Functions,[0],[0]
"Indeed, many limitations of existing models can be traced back to the use of KL, and the resulting trade-offs in approximate solutions it implies.",2.1. Distance Metrics and Loss Functions,[0],[0]
"For instance, its use appears to play a central role in one of the primary failure modes of VAEs, that of blurry samples.",2.1. Distance Metrics and Loss Functions,[0],[0]
"Zhao et al. (2017) argue that the Gaussian posterior pθ(x|z) implies an overly simple model, which, when unable to perfectly fit the data, is forced to average (thus creating blur), and is not incentivized by the KL towards an alternative notion of approximate solution.",2.1. Distance Metrics and Loss Functions,[0],[0]
"Theis et al. (2015) emphasized that an improvement of log-likelihood does not necessarily translate to higher perceptual quality, and that the KL loss is more likely to produce atypical samples than some other training criteria.
",2.1. Distance Metrics and Loss Functions,[0],[0]
"We offer an alternative perspective: a good model should encode assumptions about the data distribution, whereas a good loss should encode the notion of similarity, that is, the underlying metric on the data space.",2.1. Distance Metrics and Loss Functions,[0],[0]
"From this point of view, the KL corresponds to an actual absence of explicit underlying metric, with complete focus on probability.
",2.1. Distance Metrics and Loss Functions,[0],[0]
"The optimal transport metrics Wc, for underlying metric c(x, x′), and in particular the p-Wasserstein distance, when c is an Lp metric, have frequently been proposed as being well-suited replacements to KL (Bousquet et al., 2017; Genevay et al., 2017).",2.1. Distance Metrics and Loss Functions,[0],[0]
"Briefly, the advantages are (1) avoidance of mode collapse (no need to choose between spreading over modes or collapsing to a single mode as in KL), and (2) the ability to trade off errors and incentivize approximations that respect the underlying metric.
",2.1. Distance Metrics and Loss Functions,[0],[0]
"Recently, Arjovsky et al. (2017) introduced the Wasserstein
GAN, reposing the two-player game as the estimation of the gradient of the 1-Wasserstein distance between the data and generator distributions.",2.1. Distance Metrics and Loss Functions,[0],[0]
"They reframe this in terms of the dual form of the 1-Wasserstein, with the critic estimating a function f which maximally separates the two distributions.",2.1. Distance Metrics and Loss Functions,[0],[0]
"While this is an exciting line of work, it still faces limitations when the critic solution is approximate, i.e. when f∗ is not found before each update.",2.1. Distance Metrics and Loss Functions,[0],[0]
"In this case, due to insufficient training of the critic (Bellemare et al., 2017) or limitations of the function approximator, the gradient direction produced can be arbitrarily bad (Bousquet et al., 2017).
",2.1. Distance Metrics and Loss Functions,[0],[0]
"Thus, we are left with the question of how to minimize a distribution loss respecting an underlying metric.",2.1. Distance Metrics and Loss Functions,[0],[0]
"Recent work in distributional reinforcement learning has proposed the use of quantile regression as a method for minimizing the 1-Wasserstein in the univariate case when approximating using a mixture of Dirac functions (Dabney et al., 2018b).",2.1. Distance Metrics and Loss Functions,[0],[0]
"In this section, we review quantile regression as a method for estimating the quantile function of a distribution at specific points, i.e. its inverse cumulative distribution function.",2.2. Quantile Regression,[0],[0]
"This leads to recent work on approximating a distribution by a neural network approximation of its quantile function, acting as a reparameterization of a random sample from the uniform distribution.
",2.2. Quantile Regression,[0],[0]
"The quantile regression loss (Koenker & Hallock, 2001) for a quantile at τ ∈",2.2. Quantile Regression,[0],[0]
"[0, 1] and error u (positive for underestimation and negative for overestimation) is given by ρτ (u) =",2.2. Quantile Regression,[0],[0]
(τ − I{u ≤,2.2. Quantile Regression,[0],[0]
0})u.,2.2. Quantile Regression,[0],[0]
It is an asymmetric loss function penalizing underestimation by weight τ and overestimation by weight 1,2.2. Quantile Regression,[0],[0]
− τ .,2.2. Quantile Regression,[0],[0]
"For a given scalar distribution Z with c.d.f. FZ and a quantile τ , the inverse c.d.f. q = F−1Z (τ) minimizes the expected quantile regression loss Ez∼Z",2.2. Quantile Regression,[0],[0]
[ρτ (z − q)].,2.2. Quantile Regression,[0],[0]
Using this loss allows one to train a neural network to approximate a scalar distribution represented by its inverse c.d.f.,2.2. Quantile Regression,[0],[0]
"For this, the network can output a fixed grid of quantiles (Dabney et al., 2018b), with the respective quantile regression losses being applied to each output independently.",2.2. Quantile Regression,[0],[0]
"A more effective approach is to provide the desired quantile τ as an additional input to the network, and train it to output the corresponding value of F−1Z (τ).",2.2. Quantile Regression,[0],[0]
"The implicit quantile network (IQN) model (Dabney et al., 2018a) reparameterizes a sample τ ∼ U([0, 1]) through a deterministic function to produce samples from the underlying data distribution.",2.2. Quantile Regression,[0],[0]
These two methods can be seen to belong to the top-right and bottom-right categories in Figure 1.,2.2. Quantile Regression,[0],[0]
"An IQN Qθ can be trained by stochastic gradient descent on the quantile regression loss, with u = z −Qθ(τ) and training samples (z, τ) drawn from z ∼ Z and τ ∼ U([0, 1]).
",2.2. Quantile Regression,[0],[0]
"One drawback to the quantile regression loss is that gradients do not scale with the magnitude of the error, but instead with the sign of the error and the quantile weight τ .",2.2. Quantile Regression,[0],[0]
This increases gradient variance and can negatively impact the final model’s sample quality.,2.2. Quantile Regression,[0],[0]
"Increasing the batch size, and thus averaging over more values of τ , would have the effect of lowering this variance.",2.2. Quantile Regression,[0],[0]
"Alternatively, we can smooth the gradients as the model converges by allowing errors, under some threshold κ, to be scaled with their magnitude, reverting to an expectile loss.",2.2. Quantile Regression,[0],[0]
"This results in the Huber quantile loss (Huber, 1964; Dabney et al., 2018b):
ρκτ (u) =
{ |τ−I{u≤0}| 2κ u
2, if |u| ≤ κ, |τ −",2.2. Quantile Regression,[0],[0]
I{u ≤ 0}|(|u|,2.2. Quantile Regression,[0],[0]
"− 12κ), otherwise.",2.2. Quantile Regression,[0],[0]
(2),2.2. Quantile Regression,[0],[0]
"Let X = (X1, . . .",3. Autoregressive Implicit Quantiles,[0],[0]
", Xn) ∈",3. Autoregressive Implicit Quantiles,[0],[0]
X1 × · · · × Xn = X be an ndimensional random variable.,3. Autoregressive Implicit Quantiles,[0],[0]
"We begin by analyzing the effect of two naive applications of IQN to modeling the distribution of X .
",3. Autoregressive Implicit Quantiles,[0],[0]
"First, suppose we use the same quantile target, τ ∈",3. Autoregressive Implicit Quantiles,[0],[0]
"[0, 1], for every output dimension.",3. Autoregressive Implicit Quantiles,[0],[0]
"The only modification to IQN would be to output n dimensions instead of 1, the loss being applied to each output dimension independently.",3. Autoregressive Implicit Quantiles,[0],[0]
This is equivalent to assuming that the dimensions of X are comonotonic.,3. Autoregressive Implicit Quantiles,[0],[0]
"Two random variables are comonotonic if and only if they can be expressed as nondecreasing (deterministic) functions of a single random variable (Dhaene et al., 2006).",3. Autoregressive Implicit Quantiles,[0],[0]
Thus a joint quantile function for a comonotonic X can be written as F−1X (τ) =,3. Autoregressive Implicit Quantiles,[0],[0]
"(F−1X1 (τ), F −1 X2
(τ), . . .",3. Autoregressive Implicit Quantiles,[0],[0]
", F−1Xn(τ)).",3. Autoregressive Implicit Quantiles,[0],[0]
"While there are many interesting uses for comonotonic random variables, we believe this assumption is too strong to be useful more broadly.
",3. Autoregressive Implicit Quantiles,[0],[0]
"Second, one could use a separate value τi ∈",3. Autoregressive Implicit Quantiles,[0],[0]
"[0, 1] for each Xi, with the IQN being unchanged from the first case.",3. Autoregressive Implicit Quantiles,[0],[0]
This corresponds to making an independence assumption on the dimensions of X .,3. Autoregressive Implicit Quantiles,[0],[0]
"Again we would expect this to be an unreasonably restrictive modeling assumption for many domains, such as the case of natural images.
",3. Autoregressive Implicit Quantiles,[0],[0]
"Now, we turn to our proposed approach of extending IQN to multivariate distributions.",3. Autoregressive Implicit Quantiles,[0],[0]
We fix an ordering of the n dimensions.,3. Autoregressive Implicit Quantiles,[0],[0]
"If the density function pX is expressed as a product of conditional likelihoods, as in Equation 1, then the joint c.d.f. can be written as
FX(x) = P(X1 ≤",3. Autoregressive Implicit Quantiles,[0],[0]
"x1, . .",3. Autoregressive Implicit Quantiles,[0],[0]
.,3. Autoregressive Implicit Quantiles,[0],[0]
", Xn ≤ xn),
",3. Autoregressive Implicit Quantiles,[0],[0]
= n∏ i=1,3. Autoregressive Implicit Quantiles,[0],[0]
"FXi|Xi−1,...,X1(xi).
",3. Autoregressive Implicit Quantiles,[0],[0]
"Furthermore, for τjoint = ∏n i=1 τi, we can write the jointquantile function of X as
F−1X (τjoint) =",3. Autoregressive Implicit Quantiles,[0],[0]
"(F −1 X1 (τ1), . . .",3. Autoregressive Implicit Quantiles,[0],[0]
", F −1 Xn|Xn−1,...(τn)).
",3. Autoregressive Implicit Quantiles,[0],[0]
"This approach has been used previously by Koenker & Xiao (2006), who introduced a quantile autoregression model for quantile regression on time-series.
",3. Autoregressive Implicit Quantiles,[0],[0]
We propose to extend IQN to an autoregressive model of the above conditional form of a joint-quantile function.,3. Autoregressive Implicit Quantiles,[0],[0]
"Denoting X1:i = X1 × · · · × Xi, let X̃ := ⋃n i=0 X1:i be the space of ‘partial’ data points.",3. Autoregressive Implicit Quantiles,[0],[0]
We can define the autoregressive IQN as a deterministic functionQθ :,3. Autoregressive Implicit Quantiles,[0],[0]
X̃ ×,3. Autoregressive Implicit Quantiles,[0],[0]
"[0, 1]n → X̃ , mapping partial samples x̃ ∈ X̃ and quantile targets τi ∈",3. Autoregressive Implicit Quantiles,[0],[0]
"[0, 1] to estimates of F−1X .",3. Autoregressive Implicit Quantiles,[0],[0]
We can then train Qθ using a quantile regression loss (Equation 2).,3. Autoregressive Implicit Quantiles,[0],[0]
"For generation, one can iterate x1:i = Qθ(x1:i−1, τi), on a sequence of growing partial samples2 x1:i−1 and independently sampled τi ∼ U([0, 1]), for i = 1, . . .",3. Autoregressive Implicit Quantiles,[0],[0]
", n, to finally obtain a sample x = x1:n.",3. Autoregressive Implicit Quantiles,[0],[0]
"As previously mentioned, for the restricted model class of a uniform mixture of Diracs, quantile regression can be shown to minimize the 1-Wasserstein metric (Dabney et al., 2018b).",3.1. Quantile Regression and the Wasserstein,[0],[0]
"We extend this analysis for the case of arbitrary approximate quantile functions, and find that quantile regression minimizes a closely related divergence which we call quantile divergence, defined, for any distributions P and Q, as
q(P,Q) := ∫ 1 0",3.1. Quantile Regression and the Wasserstein,[0],[0]
[∫ F−1Q (τ),3.1. Quantile Regression and the Wasserstein,[0],[0]
"F−1P (τ) (FP (x)− τ)dx ] dτ.
",3.1. Quantile Regression and the Wasserstein,[0],[0]
"Indeed, the expected quantile loss of any parameterized quantile function Q̄θ equals, up to a constant, the quantile divergence between P and the distribution Qθ implicitly defined by Q̄θ:
E τ∼U([0,1])",3.1. Quantile Regression and the Wasserstein,[0],[0]
[ E z∼P [ρτ (z − Q̄θ(τ))],3.1. Quantile Regression and the Wasserstein,[0],[0]
"] = q(P,Qθ) + h(P ),
where h(P ) does not depend on Qθ.",3.1. Quantile Regression and the Wasserstein,[0],[0]
"Thus quantile regression minimizes the quantile divergence q(P,Qθ) and the sample gradient ∇θρτ (z − Q̄θ(τ))",3.1. Quantile Regression and the Wasserstein,[0],[0]
"(for τ ∼ U([0, 1]) and z ∼ P ) is an unbiased estimate of ∇θq(P,Qθ).",3.1. Quantile Regression and the Wasserstein,[0],[0]
See Appendix for proofs.,3.1. Quantile Regression and the Wasserstein,[0],[0]
"Although IQN does not directly model the log-likelihood of the data distribution, observe that we can still query the implied density at a point (Jones, 1992):
∂
∂τ F−1X (τ) =
1
pX(F −1 X (τ))
.
",3.2. Quantile Density Function,[0],[0]
"Indeed, this quantity, known as the sparsity function (Tukey, 1965) or the quantile-density function (Parzen, 1979) plays
2Throughout we understand x0 = x1:0 ∈ X1:0 to denote the ‘empty tuple’, and the function Qθ to map this to a single unconditional sample x1 = x1:1 = Qθ(x0, τ1).
",3.2. Quantile Density Function,[0],[0]
"a central role in the analysis of quantile regression models (Koenker, 1994).",3.2. Quantile Density Function,[0],[0]
"A common approach involves choosing a bandwidth parameter h and estimating this quantity through finite-differences around the value of interest as (F−1X (τ+h)−F−1X (τ−h))/2h (Siddiqui, 1960).",3.2. Quantile Density Function,[0],[0]
"However, as we have the full quantile function, the quantile-density function can be computed exactly using a single step of back-propagation to compute ∂F
−1(τ) ∂τ .",3.2. Quantile Density Function,[0],[0]
"As this only allows
querying the density given the value of τ , application to general likelihoods would require finding the value of τ that produces the closest approximation to the query point x.",3.2. Quantile Density Function,[0],[0]
"Though arguably too inefficient for training, this could potentially be used to interrogate the model.",3.2. Quantile Density Function,[0],[0]
"To test our proposed method, which is architecturally compatible with many generative model approaches, we wanted to compare and contrast IQN, that is quantile regression and quantile reparameterization, with a method trained with an explicit parameterization to minimize KL divergence.",4. PixelIQN,[0],[0]
"A natural choice for this was PixelCNN, specifically we build upon the Gated PixelCNN of van den Oord et al. (2016b).
",4. PixelIQN,[0],[0]
"The Gated PixelCNN takes as input an image x ∼ X , sampled from the training distribution at training time, and potentially all zeros or partially generated at generation time, as well as a location-dependent context s.",4. PixelIQN,[0],[0]
"The model consists of a number of residual layer blocks, whose structure is chosen to allow each output pixel to be a function of all preceding input pixels (in a raster-scan order).",4. PixelIQN,[0],[0]
"At its core, each layer block computes two gated activations of the form
y = tanh(Wk,f ∗ x+ Vk,f ∗ s) σ(Wk,g ∗ x+",4. PixelIQN,[0],[0]
"Vk,g ∗ s), with k the layer index, ∗ denoting convolution, and Vk,f and Vk,g being 1 × 1 convolution kernels.",4. PixelIQN,[0],[0]
"See Figure 2 for a full schematic depiction of a Gated PixelCNN layer
block.",4. PixelIQN,[0],[0]
"After a number of such layer blocks, the PixelCNN produces a final output layer with shape (n, n, 3, 256), with a softmax across the final dimension, corresponding to the approximate conditional likelihood for the value of each pixel-channel.",4. PixelIQN,[0],[0]
"That is, the conditional likelihood is the product of these individual autoregressive models,
p(x|s) = 3n2∏ i=1",4. PixelIQN,[0],[0]
"p(xi|x1, . . .",4. PixelIQN,[0],[0]
", xi−1, si).
",4. PixelIQN,[0],[0]
"Typically the location-dependent conditioning term was used to condition on class labels, but here, we will use it to condition on the sample point3 τ ∈",4. PixelIQN,[0],[0]
"[0, 1]3n2 .",4. PixelIQN,[0],[0]
"Thus, in addition to the input image x we input, in place of s, the sample points τ = (τ1, . . .",4. PixelIQN,[0],[0]
", τ3n2) to be reparameterized, with each τi ∼ U([0, 1]).",4. PixelIQN,[0],[0]
"Finally, our network outputs only the full sample image of shape (n, n, 3), without the need for an additional softmax layer.",4. PixelIQN,[0],[0]
Note that the number of τ values generated exactly corresponds to the number of random draws from softmax distributions in the original PixelCNN.,4. PixelIQN,[0],[0]
"We are simply changing the role of the randomness, from a draw at the output to a part of the input.
",4. PixelIQN,[0],[0]
"Architecturally, our proposed model, PixelIQN, is exactly the network given by van den Oord et al. (2016b), with the one exception that we output only a single value per pixel-channel and do not require the softmax activations.
",4. PixelIQN,[0],[0]
"In PixelCNN training is done by passing the training image through the network, and training each output softmax distribution using the KL divergence between the training image and the approximate distribution,∑
i
DKL(δxi , p(·|x1, . . .",4. PixelIQN,[0],[0]
", xi−1)).
",4. PixelIQN,[0],[0]
"For PixelIQN, the input is the training image x and a sample point τ ∼ U([0, 1]3n2).",4. PixelIQN,[0],[0]
"The output values Qx(τ) ∈ R3n 2 are interpreted as the approximate quantile function at τ , Qx(τ)i = QX(τi|xi−1, . . .), trained with a single step of quantile regression towards the observed sample",4. PixelIQN,[0],[0]
"x:∑
i
",4. PixelIQN,[0],[0]
"ρκτi(xi −QX(τi|xi−1, . .",4. PixelIQN,[0],[0]
.)).,4. PixelIQN,[0],[0]
"We begin by demonstrating PixelIQN on CIFAR-10 (Krizhevsky & Hinton, 2009).",4.1. CIFAR-10,[0],[0]
"For comparison, we train both a baseline Gated PixelCNN and a PixelIQN.",4.1. CIFAR-10,[0],[0]
"Both models correspond to the 15-layer network variant in (van den Oord et al., 2016b), see Appendix for detailed hyperparameters and training procedure.",4.1. CIFAR-10,[0],[0]
"The two methods have substantially different loss functions, so we performed a
3Conditioning on labels remains possible (see Section 4.2).
hyperparameter search using a short training run, with the same number (500) of hyperparameter configurations evaluated for both models.",4.1. CIFAR-10,[0],[0]
"For all results, we report full training runs using the best found hyperparameters in each case.",4.1. CIFAR-10,[0],[0]
"The evaluation metric used for the hyperparameter search was the Fréchet Inception Distance (FID) (Heusel et al., 2017), see Appendix for details.",4.1. CIFAR-10,[0],[0]
"In addition to FID, we report Inception score (Salimans et al., 2016) for both models.
",4.1. CIFAR-10,[0],[0]
Figure 4 (left) shows Inception score and FID for both models evaluated at several points throughout training.,4.1. CIFAR-10,[0],[0]
"The fully trained PixelCNN achieves an Inception score and FID of 4.6 and 65.9 respectively, while PixelIQN substantially outperforms it with an Inception score of 5.3 and FID of 49.5.",4.1. CIFAR-10,[0],[0]
"This also compares favorably with e.g. WGAN (Arjovsky et al., 2017), which reaches an Inception score of 3.8.",4.1. CIFAR-10,[0],[0]
"For subjective evaluations, we give samples from both models in Figure 3.",4.1. CIFAR-10,[0],[0]
Samples coming from PixelIQN are much more visually coherent.,4.1. CIFAR-10,[0],[0]
"Of note, the PixelIQN model achieves
a performance level comparable to that of the fully trained PixelCNN with only about one third the number of training updates (and about one third of the wall-clock time).",4.1. CIFAR-10,[0],[0]
"Next, we turn to the small ImageNet dataset (Russakovsky et al., 2015), first used for generative modeling in the PixelRNN work (van den Oord et al., 2016c).",4.2. ImageNet 32x32,[0],[0]
"Again, we evaluate using FID and Inception score.",4.2. ImageNet 32x32,[0],[0]
"For this much harder dataset, we base our PixelCNN and PixelIQN models on the larger 20-layer variant used in (van den Oord et al., 2016b).",4.2. ImageNet 32x32,[0],[0]
"Due to substantially longer training time for this model, we did not perform additional hyperparameter tuning, and mostly used the same hyperparameter values as in the previous sections for both models; details can be found in the Appendix.
",4.2. ImageNet 32x32,[0],[0]
Figure 4 shows Inception score and FID throughout training of PixelCNN and PixelIQN.,4.2. ImageNet 32x32,[0],[0]
"Again, PixelIQN substan-
tially outperforms the baseline in terms of final performance and sample complexity.",4.2. ImageNet 32x32,[0],[0]
"For final scores and a comparison to state-of-the-art GAN models, see Table 1.",4.2. ImageNet 32x32,[0],[0]
Figure 5 shows random (non-cherry-picked) samples from both models.,4.2. ImageNet 32x32,[0],[0]
"Compared to PixelCNN, PixelIQN samples appear to have superior quality with more global consistency and less ‘high-frequency noise’.
",4.2. ImageNet 32x32,[0],[0]
"In Figure 6, we show the inpainting performance of PixelIQN, by fixing the top half of a validation set image as input and sampling repeatedly from the model to generate different completions.",4.2. ImageNet 32x32,[0],[0]
We note that the model consistently generates plausible completions with significant diversity between different completion samples for the same input image.,4.2. ImageNet 32x32,[0],[0]
"Meanwhile, WGAN-GP has been seen to produce deterministic completions (Bellemare et al., 2017).
",4.2. ImageNet 32x32,[0],[0]
"Following (van den Oord et al., 2016b), we also trained a class-conditional PixelIQN variant, providing to the model the one-hot class label corresponding to a training image (in addition to a τ sample).",4.2. ImageNet 32x32,[0],[0]
"Samples from a class-conditional model can be expected to have higher visual quality, as the class label provides log2(1000)",4.2. ImageNet 32x32,[0],[0]
"≈ 10 bits of information, see Figure 7.",4.2. ImageNet 32x32,[0],[0]
"As seen in Figure 4 and Table 1, class conditioning also further improves Inception score and FID.",4.2. ImageNet 32x32,[0],[0]
"To generate each sample for the computation of these scores, we sample one of 1000 class labels randomly, then generate an image conditioned on this label via the trained model.
",4.2. ImageNet 32x32,[0],[0]
"Finally, motivated by the very long training time for the large PixelCNN model (approximately 1 day per 100K training steps, on 16 NVIDIA Tesla P100 GPUs), we also trained smaller 15-layer versions of the models (same as the ones used on CIFAR-10) on the small ImageNet dataset.",4.2. ImageNet 32x32,[0],[0]
"For comparison, these take approximately 12 hours for 100K training steps on a single P100 GPU, or less than 3 hours on 8 P100 GPUs.",4.2. ImageNet 32x32,[0],[0]
"As expected, little PixelCNN, while suitable
for the CIFAR-10 dataset, fails to achieve competitive scores on the ImageNet dataset, achieving Inception score 5.1 and FID 66.4.",4.2. ImageNet 32x32,[0],[0]
"Astonishingly, little PixelIQN on this dataset reaches Inception score 7.3 and FID 38.5, see Figure 4 (right).",4.2. ImageNet 32x32,[0],[0]
"It thereby not only outperforms the little PixelCNN, but also the larger 20-layer version!",4.2. ImageNet 32x32,[0],[0]
"This strongly supports the hypothesis that PixelCNN, and potentially many other models, are constrained not only by their model capacity, but crucially also by the sub-optimal trade-offs made by their log-likelihood training criterion, failing to align with perceptual or evaluation metrics.",4.2. ImageNet 32x32,[0],[0]
Most existing generative models for images belong to one of two classes.,5. Discussion and Conclusions,[0],[0]
"The first are likelihood-based models, trained with an elementwise KL reconstruction loss, which,
while perceptually meaningless, provides robust optimization properties and high sample diversity.",5. Discussion and Conclusions,[0],[0]
"The second are GANs, trained based on a discriminator loss, typically better aligned with a perceptual metric and enabling the generator to produce realistic, globally consistent samples.",5. Discussion and Conclusions,[0],[0]
"Their advantages come at the cost of a harder optimization problem, high parameter sensitivity, and most importantly, a tendency to collapse modes of the data distribution.
",5. Discussion and Conclusions,[0],[0]
"AIQNs are a new, fundamentally different, technique for generative modeling.",5. Discussion and Conclusions,[0],[0]
"By using a quantile regression loss instead of KL divergence, they combine some of the best properties of the two model classes.",5. Discussion and Conclusions,[0],[0]
"By their nature, they preserve modes of the learned distribution, while producing perceptually appealing high-quality samples.",5. Discussion and Conclusions,[0],[0]
The inevitable approximation trade-offs a generative model makes when constrained by capacity or insufficient training can vary significantly depending on the loss used.,5. Discussion and Conclusions,[0],[0]
"We argue that the proposed quantile regression loss aligns more effectively with a given metric and therefore makes subjectively more advantageous trade-offs.
",5. Discussion and Conclusions,[0],[0]
Devising methods for quantile regression over multidimensional outputs is an active area of research.,5. Discussion and Conclusions,[0],[0]
"New methods are continuing to be investigated (Carlier et al., 2016; Hallin & Miroslav, 2016), and a promising direction for future work is to find ways to use these to replace autoregressive models.",5. Discussion and Conclusions,[0],[0]
"One approach to reducing the computational burden of such models is to apply AIQN to the latent dimensions
of a VAE.",5. Discussion and Conclusions,[0],[0]
"Similar in spirit to Rosca et al. (2017), this would use the VAE to reduce the dimensionality of the problem and the AIQN to sample from the true latent distribution.",5. Discussion and Conclusions,[0],[0]
"In the Appendix we give preliminary results using such an technique, on CelebA 64× 64 (Liu et al., 2015).",5. Discussion and Conclusions,[0],[0]
"We have shown that IQN, computationally cheap and technically simple, can be readily applied to existing architectures, PixelCNN and VAE (Appendix), improving robustness and sampling quality of the underlying model.",5. Discussion and Conclusions,[0],[0]
"We demonstrated that PixelIQN produces more realistic, globally coherent samples, and improves Inception score and FID.
",5. Discussion and Conclusions,[0],[0]
We further point out that many recent advances in generative models could be easily combined with our proposed method.,5. Discussion and Conclusions,[0],[0]
"Recent algorithmic improvements to GANs such as mini-batch discrimination and progressive growing (Salimans et al., 2016; Karras et al., 2017), while not strictly necessary in our work, could be applied to further improve performance.",5. Discussion and Conclusions,[0],[0]
"PixelCNN++ (Salimans et al., 2017) is an architectural improvement of PixelCNN, with several beneficial modifications supported by experimental evidence.",5. Discussion and Conclusions,[0],[0]
"Although we have built upon the original Gated PixelCNN in this work, we believe all of these modifications to be compatible with our work, except for the use of a mixture of logistics in place of PixelCNN’s softmax.",5. Discussion and Conclusions,[0],[0]
"As we have entirely replaced this model component, this change does not map onto our model.",5. Discussion and Conclusions,[0],[0]
"Of note, the motivation behind this change closely mirrors our own, in looking for a loss that respects the underlying metric between examples.",5. Discussion and Conclusions,[0],[0]
"The recent PixelSNAIL model (Chen et al., 2017) achieves stateof-the-art modeling performance by enhancing PixelCNN with ELU nonlinearities, modified block structure, and an attention mechanism.",5. Discussion and Conclusions,[0],[0]
"Again, all of these are fully compatible with our work and should improve results further.
",5. Discussion and Conclusions,[0],[0]
"Finally, the implicit quantile formulation lifts a number of architectural restrictions of previous generative models.",5. Discussion and Conclusions,[0],[0]
"Most importantly, the reparameterization as an inverse c.d.f. allows to learn distributions over continuous ranges without pre-specified boundaries or quantization.",5. Discussion and Conclusions,[0],[0]
"This enables modeling continuous-valued variables, for example for generation of sound (van den Oord et al., 2016a), opening multiple interesting avenues for further investigation.",5. Discussion and Conclusions,[0],[0]
We would like to acknowledge the important role many of our colleagues at DeepMind played for this work.,Acknowledgements,[0],[0]
"We especially thank Aäron van den Oord and Sander Dieleman for invaluable advice on the PixelCNN model; Ivo Danihelka and Danilo J. Rezende for careful reading and insightful comments on an earlier version of the paper; Igor Babuschkin, Alexandre Galashov, Dominik Grewe, Jacob Menick, and Mihaela Rosca for technical help.",Acknowledgements,[0],[0]
"We introduce autoregressive implicit quantile networks (AIQN), a fundamentally different approach to generative modeling than those commonly used, that implicitly captures the distribution using quantile regression.",abstractText,[0],[0]
"AIQN is able to achieve superior perceptual quality and improvements in evaluation metrics, without incurring a loss of sample diversity.",abstractText,[0],[0]
The method can be applied to many existing models and architectures.,abstractText,[0],[0]
"In this work we extend the PixelCNN model with AIQN and demonstrate results on CIFAR-10 and ImageNet using Inception score, FID, non-cherrypicked samples, and inpainting results.",abstractText,[0],[0]
We consistently observe that AIQN yields a highly stable algorithm that improves perceptual quality while maintaining a highly diverse distribution.,abstractText,[0],[0]
Autoregressive Quantile Networks for Generative Modeling,title,[0],[0]
"There has been a staggering increase in progress on generative modeling in recent years, built largely upon fundamental advances such as generative adversarial networks (Goodfellow et al., 2014), variational inference (Kingma & Welling, 2013), and autoregressive density estimation (van den Oord et al., 2016c).",1. Introduction,[0],[0]
"These have led to breakthroughs in state-ofthe-art generation of natural images (Karras et al., 2017) and audio (van den Oord et al., 2016a), and even been used for unsupervised learning of disentangled representations (Higgins et al., 2017; Chen et al., 2016).",1. Introduction,[0],[0]
"These domains often have real-valued distributions with underlying metrics; that is, there is a domain-specific notion of similarity between data points.",1. Introduction,[0],[0]
"This similarity is ignored by the predominant work-horse of generative modeling, the Kullback-Leibler (KL) divergence.",1. Introduction,[0],[0]
"Progress is now being made towards algorithms that optimize with respect to these underlying metrics (Arjovsky et al., 2017; Bousquet et al., 2017).
",1. Introduction,[0],[0]
"*Equal contribution 1DeepMind, London, UK.",1. Introduction,[0],[0]
"Correspondence to: Georg Ostrovski <ostrovski@google.com>, Will Dabney <wdabney@google.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"In this paper, we present a novel approach to generative modeling, that, while strikingly different from existing methods, is grounded in the well-understood statistical methods of quantile regression.",1. Introduction,[0],[0]
"Unlike the majority of recent work, we approach generative modeling without the use of the KL divergence, and without explicitly approximating a likelihood model.",1. Introduction,[0],[0]
"Like GANs, in this way we produce an implicitly defined model, but unlike GANs our optimization procedure is inherently stable and lacks degenerate solutions which cause loss of diversity and mode collapse.
",1. Introduction,[0],[0]
"Much of the recent research on GANs has been focused on improving stability (Radford et al., 2015; Arjovsky et al., 2017; Daskalakis et al., 2017) and sample diversity (Gulrajani et al., 2017; Salimans et al., 2016; 2018).",1. Introduction,[0],[0]
"By stark contrast, methods such as PixelCNN (van den Oord et al., 2016b) readily produce high diversity, but due to their use of KL divergence are unable to make reasonable trade-offs between likelihood and perceptual similarity (Theis et al., 2015; Bellemare et al., 2017; Bousquet et al., 2017).
",1. Introduction,[0],[0]
"Our proposed method, autoregressive implicit quantile networks (AIQN), combines the benefits of both: a loss function that respects the underlying metric of the data leading to improved perceptual quality, and a stable optimization process leading to highly diverse samples.",1. Introduction,[0],[0]
"While there has been an increasing tendency towards complex architectures (Chen et al., 2017; Salimans et al., 2017) and multiple objective loss functions to overcome these challenges, AIQN is conceptually simple and does not rely on any special architecture or optimization techniques.",1. Introduction,[0],[0]
"Empirically it proves to be robust to hyperparameter variations and easy to optimize.
",1. Introduction,[0],[0]
"Our work is motivated by the recent advances achieved by reframing GANs in terms of optimal transport, leading to the Wasserstein GAN algorithm (Arjovsky et al., 2017), as well as work towards understanding the relationship between optimal transport and both GANs and VAEs (Bousquet et al., 2017).",1. Introduction,[0],[0]
"In agreement with these results, we focus on loss functions grounded in perceptually meaningful metrics.",1. Introduction,[0],[0]
"We build upon recent work in distributional reinforcement learning (Dabney et al., 2018a), which has begun to bridge the gap between approaches in reinforcement learning and unsupervised learning.",1. Introduction,[0],[0]
"Towards a practical algorithm we base our experimental results on Gated PixelCNN (van den Oord et al., 2016b), and show that using AIQN significantly im-
proves objective performance on CIFAR-10 and ImageNet 32x32 in terms of Fréchet Inception Distance (FID) and Inception score, as well as subjective perceptual quality in image samples and inpainting.",1. Introduction,[0],[0]
"We begin by establishing some notation, before turning to a review of three of the most prevalent methods for generative modeling.",2. Background,[0],[0]
"Calligraphic letters (e.g.X ) denote sets or spaces, capital letters (e.g. X) denote random variables, and lower case letters (e.g. x) indicate values.",2. Background,[0],[0]
"A probability distribution with random variable X ∈ X is denoted pX ∈P(X ), its cumulative distribution function (c.d.f.)",2. Background,[0],[0]
"FX , and inverse c.d.f. or quantile function QX = F−1X .",2. Background,[0],[0]
When probability distributions or quantile functions are parameterized by some θ,2. Background,[0],[0]
"we will write pθ or Qθ recognizing that here we do not view θ as a random variable.
",2. Background,[0],[0]
"Perhaps the simplest way to approach generative modeling of a random variableX ∈ X is by fixing some discretization ofX into n separate values, say x1, . . .",2. Background,[0],[0]
", xn ∈ X , and parameterize the approximate distribution with pθ(xi) ∝",2. Background,[0],[0]
exp(θi).,2. Background,[0],[0]
"This type of categorical parameterization is widely used, only slightly less commonly when X does not lend itself naturally to such a partitioning.",2. Background,[0],[0]
"Typically, the parameters θ are optimized to minimize the Kullback-Leibler (KL) divergence between observed values of X and the model pθ, θ∗ = arg minθDKL(pX‖pθ).",2. Background,[0],[0]
"However, this is only tractable whenX is a small discrete set or at best low-dimensional.",2. Background,[0],[0]
A common method for extending a generative model or density estimator to multivariate distributions is to factor the density as a product of scalarvalued conditional distributions.,2. Background,[0],[0]
"Let X = (X1, . . .",2. Background,[0],[0]
", Xn), then for any permutation of the dimensions σ :",2. Background,[0],[0]
"Nn → Nn,
pX(x) =",2. Background,[0],[0]
"n∏ i=1 pXσ(i)(xσ(i)|xσ(1), . . .",2. Background,[0],[0]
", xσ(i−1)).",2. Background,[0],[0]
"(1)
When the conditional density is modeled by a simple (e.g. Gaussian) base distribution, the ordering of the dimensions can be crucial (Papamakarios et al., 2017).",2. Background,[0],[0]
"However, it is common practice to choose an arbitrary ordering and rely upon a more powerful conditional model to avoid these problems.",2. Background,[0],[0]
"This class of models includes PixelRNN and PixelCNN (van den Oord et al., 2016c;b), MAF (Papamakarios et al., 2017), MADE (Germain et al., 2015), and many others.",2. Background,[0],[0]
"Fundamentally, all these approaches use the KL divergence as their loss function.
",2. Background,[0],[0]
"Another class of methods, generally known as latent variable methods, can bypass the need for autoregressive models using a different modeling assumption.",2. Background,[0],[0]
"Specifically, consider the Variational Autoencoder (VAE) (Kingma & Welling, 2013; Rezende et al., 2014), which represents
pθ as the marginalization over a latent random variable Z ∈ Z .",2. Background,[0],[0]
"The VAE is trained to maximize an approximate lower bound of the log-likelihood of the observations:
log pθ(x) ≥ −DKL(qθ(z|x)‖p(z))",2. Background,[0],[0]
+,2. Background,[0],[0]
E,2. Background,[0],[0]
"[log pθ(x|z)] .
",2. Background,[0],[0]
"Although VAEs are straightforward to implement and optimize, and effective at capturing structure in highdimensional spaces, they often miss fine-grained detail, resulting in blurry images.
",2. Background,[0],[0]
"Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) pose the problem of learning a generative model as a two-player zero-sum game between a discriminator D, attempting to distinguish between x ∼ pX (real data) and x ∼ pθ (generated data), and a generator G, attempting to generate data indistinguishable from real data.",2. Background,[0],[0]
"The generator is an implicit latent variable model that reparameterizes samples, typically from an isotropic Gaussian distribution, into values in X .",2. Background,[0],[0]
"The original formulation of GANs,
arg min G sup D",2. Background,[0],[0]
[ E X log(D(X)),2. Background,[0],[0]
"+ E Z log(1−D(G(Z))) ] ,
can be seen as minimizing a lower-bound on the JensenShannon divergence (Goodfellow et al., 2014; Bousquet et al., 2017).",2. Background,[0],[0]
"That is, even in the case of GANs we are often minimizing functions of the KL divergence1.
",2. Background,[0],[0]
"Many recent advances have come from principled combinations of these three fundamental methods (Makhzani et al., 2015; Dumoulin et al., 2016; Rosca et al., 2017).",2. Background,[0],[0]
"A common perspective in generative modeling is that the choice of model should encode existing metric assumptions about the domain, combined with a generic likelihoodfocused loss such as the KL divergence.",2.1. Distance Metrics and Loss Functions,[0],[0]
"Under this view, the KL’s general applicability and robust optimization properties make it a natural choice, and most implementations of the methods we reviewed in the previous section attempt to, at least indirectly, minimize a version of the KL.
",2.1. Distance Metrics and Loss Functions,[0],[0]
"On the other hand, as every model inevitably makes tradeoffs when constrained by capacity or limited training, it is desirable for its optimization goal to incentivize trade-offs prioritizing approximately correct solutions, when the data space is endowed with a metric supporting a meaningful (albeit potentially subjective) notion of approximation.",2.1. Distance Metrics and Loss Functions,[0],[0]
"It has been argued (Theis et al., 2015; Bousquet et al., 2017; Arjovsky et al., 2017; Bellemare et al., 2017) that the KL may not always be appropriate from this perspective, by making sub-optimal trade-offs between likelihood and similarity.
",2.1. Distance Metrics and Loss Functions,[0],[0]
"1The Jensen-Shannon divergence is the sum of KLs between distributions P,Q and their uniform mixture M = 0.5(P +Q): JSD(P ||Q)",2.1. Distance Metrics and Loss Functions,[0],[0]
"= 0.5(DKL(P ||M) +DKL(Q||M)).
",2.1. Distance Metrics and Loss Functions,[0],[0]
"Indeed, many limitations of existing models can be traced back to the use of KL, and the resulting trade-offs in approximate solutions it implies.",2.1. Distance Metrics and Loss Functions,[0],[0]
"For instance, its use appears to play a central role in one of the primary failure modes of VAEs, that of blurry samples.",2.1. Distance Metrics and Loss Functions,[0],[0]
"Zhao et al. (2017) argue that the Gaussian posterior pθ(x|z) implies an overly simple model, which, when unable to perfectly fit the data, is forced to average (thus creating blur), and is not incentivized by the KL towards an alternative notion of approximate solution.",2.1. Distance Metrics and Loss Functions,[0],[0]
"Theis et al. (2015) emphasized that an improvement of log-likelihood does not necessarily translate to higher perceptual quality, and that the KL loss is more likely to produce atypical samples than some other training criteria.
",2.1. Distance Metrics and Loss Functions,[0],[0]
"We offer an alternative perspective: a good model should encode assumptions about the data distribution, whereas a good loss should encode the notion of similarity, that is, the underlying metric on the data space.",2.1. Distance Metrics and Loss Functions,[0],[0]
"From this point of view, the KL corresponds to an actual absence of explicit underlying metric, with complete focus on probability.
",2.1. Distance Metrics and Loss Functions,[0],[0]
"The optimal transport metrics Wc, for underlying metric c(x, x′), and in particular the p-Wasserstein distance, when c is an Lp metric, have frequently been proposed as being well-suited replacements to KL (Bousquet et al., 2017; Genevay et al., 2017).",2.1. Distance Metrics and Loss Functions,[0],[0]
"Briefly, the advantages are (1) avoidance of mode collapse (no need to choose between spreading over modes or collapsing to a single mode as in KL), and (2) the ability to trade off errors and incentivize approximations that respect the underlying metric.
",2.1. Distance Metrics and Loss Functions,[0],[0]
"Recently, Arjovsky et al. (2017) introduced the Wasserstein
GAN, reposing the two-player game as the estimation of the gradient of the 1-Wasserstein distance between the data and generator distributions.",2.1. Distance Metrics and Loss Functions,[0],[0]
"They reframe this in terms of the dual form of the 1-Wasserstein, with the critic estimating a function f which maximally separates the two distributions.",2.1. Distance Metrics and Loss Functions,[0],[0]
"While this is an exciting line of work, it still faces limitations when the critic solution is approximate, i.e. when f∗ is not found before each update.",2.1. Distance Metrics and Loss Functions,[0],[0]
"In this case, due to insufficient training of the critic (Bellemare et al., 2017) or limitations of the function approximator, the gradient direction produced can be arbitrarily bad (Bousquet et al., 2017).
",2.1. Distance Metrics and Loss Functions,[0],[0]
"Thus, we are left with the question of how to minimize a distribution loss respecting an underlying metric.",2.1. Distance Metrics and Loss Functions,[0],[0]
"Recent work in distributional reinforcement learning has proposed the use of quantile regression as a method for minimizing the 1-Wasserstein in the univariate case when approximating using a mixture of Dirac functions (Dabney et al., 2018b).",2.1. Distance Metrics and Loss Functions,[0],[0]
"In this section, we review quantile regression as a method for estimating the quantile function of a distribution at specific points, i.e. its inverse cumulative distribution function.",2.2. Quantile Regression,[0],[0]
"This leads to recent work on approximating a distribution by a neural network approximation of its quantile function, acting as a reparameterization of a random sample from the uniform distribution.
",2.2. Quantile Regression,[0],[0]
"The quantile regression loss (Koenker & Hallock, 2001) for a quantile at τ ∈",2.2. Quantile Regression,[0],[0]
"[0, 1] and error u (positive for underestimation and negative for overestimation) is given by ρτ (u) =",2.2. Quantile Regression,[0],[0]
(τ − I{u ≤,2.2. Quantile Regression,[0],[0]
0})u.,2.2. Quantile Regression,[0],[0]
It is an asymmetric loss function penalizing underestimation by weight τ and overestimation by weight 1,2.2. Quantile Regression,[0],[0]
− τ .,2.2. Quantile Regression,[0],[0]
"For a given scalar distribution Z with c.d.f. FZ and a quantile τ , the inverse c.d.f. q = F−1Z (τ) minimizes the expected quantile regression loss Ez∼Z",2.2. Quantile Regression,[0],[0]
[ρτ (z − q)].,2.2. Quantile Regression,[0],[0]
Using this loss allows one to train a neural network to approximate a scalar distribution represented by its inverse c.d.f.,2.2. Quantile Regression,[0],[0]
"For this, the network can output a fixed grid of quantiles (Dabney et al., 2018b), with the respective quantile regression losses being applied to each output independently.",2.2. Quantile Regression,[0],[0]
"A more effective approach is to provide the desired quantile τ as an additional input to the network, and train it to output the corresponding value of F−1Z (τ).",2.2. Quantile Regression,[0],[0]
"The implicit quantile network (IQN) model (Dabney et al., 2018a) reparameterizes a sample τ ∼ U([0, 1]) through a deterministic function to produce samples from the underlying data distribution.",2.2. Quantile Regression,[0],[0]
These two methods can be seen to belong to the top-right and bottom-right categories in Figure 1.,2.2. Quantile Regression,[0],[0]
"An IQN Qθ can be trained by stochastic gradient descent on the quantile regression loss, with u = z −Qθ(τ) and training samples (z, τ) drawn from z ∼ Z and τ ∼ U([0, 1]).
",2.2. Quantile Regression,[0],[0]
"One drawback to the quantile regression loss is that gradients do not scale with the magnitude of the error, but instead with the sign of the error and the quantile weight τ .",2.2. Quantile Regression,[0],[0]
This increases gradient variance and can negatively impact the final model’s sample quality.,2.2. Quantile Regression,[0],[0]
"Increasing the batch size, and thus averaging over more values of τ , would have the effect of lowering this variance.",2.2. Quantile Regression,[0],[0]
"Alternatively, we can smooth the gradients as the model converges by allowing errors, under some threshold κ, to be scaled with their magnitude, reverting to an expectile loss.",2.2. Quantile Regression,[0],[0]
"This results in the Huber quantile loss (Huber, 1964; Dabney et al., 2018b):
ρκτ (u) =
{ |τ−I{u≤0}| 2κ u
2, if |u| ≤ κ, |τ −",2.2. Quantile Regression,[0],[0]
I{u ≤ 0}|(|u|,2.2. Quantile Regression,[0],[0]
"− 12κ), otherwise.",2.2. Quantile Regression,[0],[0]
(2),2.2. Quantile Regression,[0],[0]
"Let X = (X1, . . .",3. Autoregressive Implicit Quantiles,[0],[0]
", Xn) ∈",3. Autoregressive Implicit Quantiles,[0],[0]
X1 × · · · × Xn = X be an ndimensional random variable.,3. Autoregressive Implicit Quantiles,[0],[0]
"We begin by analyzing the effect of two naive applications of IQN to modeling the distribution of X .
",3. Autoregressive Implicit Quantiles,[0],[0]
"First, suppose we use the same quantile target, τ ∈",3. Autoregressive Implicit Quantiles,[0],[0]
"[0, 1], for every output dimension.",3. Autoregressive Implicit Quantiles,[0],[0]
"The only modification to IQN would be to output n dimensions instead of 1, the loss being applied to each output dimension independently.",3. Autoregressive Implicit Quantiles,[0],[0]
This is equivalent to assuming that the dimensions of X are comonotonic.,3. Autoregressive Implicit Quantiles,[0],[0]
"Two random variables are comonotonic if and only if they can be expressed as nondecreasing (deterministic) functions of a single random variable (Dhaene et al., 2006).",3. Autoregressive Implicit Quantiles,[0],[0]
Thus a joint quantile function for a comonotonic X can be written as F−1X (τ) =,3. Autoregressive Implicit Quantiles,[0],[0]
"(F−1X1 (τ), F −1 X2
(τ), . . .",3. Autoregressive Implicit Quantiles,[0],[0]
", F−1Xn(τ)).",3. Autoregressive Implicit Quantiles,[0],[0]
"While there are many interesting uses for comonotonic random variables, we believe this assumption is too strong to be useful more broadly.
",3. Autoregressive Implicit Quantiles,[0],[0]
"Second, one could use a separate value τi ∈",3. Autoregressive Implicit Quantiles,[0],[0]
"[0, 1] for each Xi, with the IQN being unchanged from the first case.",3. Autoregressive Implicit Quantiles,[0],[0]
This corresponds to making an independence assumption on the dimensions of X .,3. Autoregressive Implicit Quantiles,[0],[0]
"Again we would expect this to be an unreasonably restrictive modeling assumption for many domains, such as the case of natural images.
",3. Autoregressive Implicit Quantiles,[0],[0]
"Now, we turn to our proposed approach of extending IQN to multivariate distributions.",3. Autoregressive Implicit Quantiles,[0],[0]
We fix an ordering of the n dimensions.,3. Autoregressive Implicit Quantiles,[0],[0]
"If the density function pX is expressed as a product of conditional likelihoods, as in Equation 1, then the joint c.d.f. can be written as
FX(x) = P(X1 ≤",3. Autoregressive Implicit Quantiles,[0],[0]
"x1, . .",3. Autoregressive Implicit Quantiles,[0],[0]
.,3. Autoregressive Implicit Quantiles,[0],[0]
", Xn ≤ xn),
",3. Autoregressive Implicit Quantiles,[0],[0]
= n∏ i=1,3. Autoregressive Implicit Quantiles,[0],[0]
"FXi|Xi−1,...,X1(xi).
",3. Autoregressive Implicit Quantiles,[0],[0]
"Furthermore, for τjoint = ∏n i=1 τi, we can write the jointquantile function of X as
F−1X (τjoint) =",3. Autoregressive Implicit Quantiles,[0],[0]
"(F −1 X1 (τ1), . . .",3. Autoregressive Implicit Quantiles,[0],[0]
", F −1 Xn|Xn−1,...(τn)).
",3. Autoregressive Implicit Quantiles,[0],[0]
"This approach has been used previously by Koenker & Xiao (2006), who introduced a quantile autoregression model for quantile regression on time-series.
",3. Autoregressive Implicit Quantiles,[0],[0]
We propose to extend IQN to an autoregressive model of the above conditional form of a joint-quantile function.,3. Autoregressive Implicit Quantiles,[0],[0]
"Denoting X1:i = X1 × · · · × Xi, let X̃ := ⋃n i=0 X1:i be the space of ‘partial’ data points.",3. Autoregressive Implicit Quantiles,[0],[0]
We can define the autoregressive IQN as a deterministic functionQθ :,3. Autoregressive Implicit Quantiles,[0],[0]
X̃ ×,3. Autoregressive Implicit Quantiles,[0],[0]
"[0, 1]n → X̃ , mapping partial samples x̃ ∈ X̃ and quantile targets τi ∈",3. Autoregressive Implicit Quantiles,[0],[0]
"[0, 1] to estimates of F−1X .",3. Autoregressive Implicit Quantiles,[0],[0]
We can then train Qθ using a quantile regression loss (Equation 2).,3. Autoregressive Implicit Quantiles,[0],[0]
"For generation, one can iterate x1:i = Qθ(x1:i−1, τi), on a sequence of growing partial samples2 x1:i−1 and independently sampled τi ∼ U([0, 1]), for i = 1, . . .",3. Autoregressive Implicit Quantiles,[0],[0]
", n, to finally obtain a sample x = x1:n.",3. Autoregressive Implicit Quantiles,[0],[0]
"As previously mentioned, for the restricted model class of a uniform mixture of Diracs, quantile regression can be shown to minimize the 1-Wasserstein metric (Dabney et al., 2018b).",3.1. Quantile Regression and the Wasserstein,[0],[0]
"We extend this analysis for the case of arbitrary approximate quantile functions, and find that quantile regression minimizes a closely related divergence which we call quantile divergence, defined, for any distributions P and Q, as
q(P,Q) := ∫ 1 0",3.1. Quantile Regression and the Wasserstein,[0],[0]
[∫ F−1Q (τ),3.1. Quantile Regression and the Wasserstein,[0],[0]
"F−1P (τ) (FP (x)− τ)dx ] dτ.
",3.1. Quantile Regression and the Wasserstein,[0],[0]
"Indeed, the expected quantile loss of any parameterized quantile function Q̄θ equals, up to a constant, the quantile divergence between P and the distribution Qθ implicitly defined by Q̄θ:
E τ∼U([0,1])",3.1. Quantile Regression and the Wasserstein,[0],[0]
[ E z∼P [ρτ (z − Q̄θ(τ))],3.1. Quantile Regression and the Wasserstein,[0],[0]
"] = q(P,Qθ) + h(P ),
where h(P ) does not depend on Qθ.",3.1. Quantile Regression and the Wasserstein,[0],[0]
"Thus quantile regression minimizes the quantile divergence q(P,Qθ) and the sample gradient ∇θρτ (z − Q̄θ(τ))",3.1. Quantile Regression and the Wasserstein,[0],[0]
"(for τ ∼ U([0, 1]) and z ∼ P ) is an unbiased estimate of ∇θq(P,Qθ).",3.1. Quantile Regression and the Wasserstein,[0],[0]
See Appendix for proofs.,3.1. Quantile Regression and the Wasserstein,[0],[0]
"Although IQN does not directly model the log-likelihood of the data distribution, observe that we can still query the implied density at a point (Jones, 1992):
∂
∂τ F−1X (τ) =
1
pX(F −1 X (τ))
.
",3.2. Quantile Density Function,[0],[0]
"Indeed, this quantity, known as the sparsity function (Tukey, 1965) or the quantile-density function (Parzen, 1979) plays
2Throughout we understand x0 = x1:0 ∈ X1:0 to denote the ‘empty tuple’, and the function Qθ to map this to a single unconditional sample x1 = x1:1 = Qθ(x0, τ1).
",3.2. Quantile Density Function,[0],[0]
"a central role in the analysis of quantile regression models (Koenker, 1994).",3.2. Quantile Density Function,[0],[0]
"A common approach involves choosing a bandwidth parameter h and estimating this quantity through finite-differences around the value of interest as (F−1X (τ+h)−F−1X (τ−h))/2h (Siddiqui, 1960).",3.2. Quantile Density Function,[0],[0]
"However, as we have the full quantile function, the quantile-density function can be computed exactly using a single step of back-propagation to compute ∂F
−1(τ) ∂τ .",3.2. Quantile Density Function,[0],[0]
"As this only allows
querying the density given the value of τ , application to general likelihoods would require finding the value of τ that produces the closest approximation to the query point x.",3.2. Quantile Density Function,[0],[0]
"Though arguably too inefficient for training, this could potentially be used to interrogate the model.",3.2. Quantile Density Function,[0],[0]
"To test our proposed method, which is architecturally compatible with many generative model approaches, we wanted to compare and contrast IQN, that is quantile regression and quantile reparameterization, with a method trained with an explicit parameterization to minimize KL divergence.",4. PixelIQN,[0],[0]
"A natural choice for this was PixelCNN, specifically we build upon the Gated PixelCNN of van den Oord et al. (2016b).
",4. PixelIQN,[0],[0]
"The Gated PixelCNN takes as input an image x ∼ X , sampled from the training distribution at training time, and potentially all zeros or partially generated at generation time, as well as a location-dependent context s.",4. PixelIQN,[0],[0]
"The model consists of a number of residual layer blocks, whose structure is chosen to allow each output pixel to be a function of all preceding input pixels (in a raster-scan order).",4. PixelIQN,[0],[0]
"At its core, each layer block computes two gated activations of the form
y = tanh(Wk,f ∗ x+ Vk,f ∗ s) σ(Wk,g ∗ x+",4. PixelIQN,[0],[0]
"Vk,g ∗ s), with k the layer index, ∗ denoting convolution, and Vk,f and Vk,g being 1 × 1 convolution kernels.",4. PixelIQN,[0],[0]
"See Figure 2 for a full schematic depiction of a Gated PixelCNN layer
block.",4. PixelIQN,[0],[0]
"After a number of such layer blocks, the PixelCNN produces a final output layer with shape (n, n, 3, 256), with a softmax across the final dimension, corresponding to the approximate conditional likelihood for the value of each pixel-channel.",4. PixelIQN,[0],[0]
"That is, the conditional likelihood is the product of these individual autoregressive models,
p(x|s) = 3n2∏ i=1",4. PixelIQN,[0],[0]
"p(xi|x1, . . .",4. PixelIQN,[0],[0]
", xi−1, si).
",4. PixelIQN,[0],[0]
"Typically the location-dependent conditioning term was used to condition on class labels, but here, we will use it to condition on the sample point3 τ ∈",4. PixelIQN,[0],[0]
"[0, 1]3n2 .",4. PixelIQN,[0],[0]
"Thus, in addition to the input image x we input, in place of s, the sample points τ = (τ1, . . .",4. PixelIQN,[0],[0]
", τ3n2) to be reparameterized, with each τi ∼ U([0, 1]).",4. PixelIQN,[0],[0]
"Finally, our network outputs only the full sample image of shape (n, n, 3), without the need for an additional softmax layer.",4. PixelIQN,[0],[0]
Note that the number of τ values generated exactly corresponds to the number of random draws from softmax distributions in the original PixelCNN.,4. PixelIQN,[0],[0]
"We are simply changing the role of the randomness, from a draw at the output to a part of the input.
",4. PixelIQN,[0],[0]
"Architecturally, our proposed model, PixelIQN, is exactly the network given by van den Oord et al. (2016b), with the one exception that we output only a single value per pixel-channel and do not require the softmax activations.
",4. PixelIQN,[0],[0]
"In PixelCNN training is done by passing the training image through the network, and training each output softmax distribution using the KL divergence between the training image and the approximate distribution,∑
i
DKL(δxi , p(·|x1, . . .",4. PixelIQN,[0],[0]
", xi−1)).
",4. PixelIQN,[0],[0]
"For PixelIQN, the input is the training image x and a sample point τ ∼ U([0, 1]3n2).",4. PixelIQN,[0],[0]
"The output values Qx(τ) ∈ R3n 2 are interpreted as the approximate quantile function at τ , Qx(τ)i = QX(τi|xi−1, . . .), trained with a single step of quantile regression towards the observed sample",4. PixelIQN,[0],[0]
"x:∑
i
",4. PixelIQN,[0],[0]
"ρκτi(xi −QX(τi|xi−1, . .",4. PixelIQN,[0],[0]
.)).,4. PixelIQN,[0],[0]
"We begin by demonstrating PixelIQN on CIFAR-10 (Krizhevsky & Hinton, 2009).",4.1. CIFAR-10,[0],[0]
"For comparison, we train both a baseline Gated PixelCNN and a PixelIQN.",4.1. CIFAR-10,[0],[0]
"Both models correspond to the 15-layer network variant in (van den Oord et al., 2016b), see Appendix for detailed hyperparameters and training procedure.",4.1. CIFAR-10,[0],[0]
"The two methods have substantially different loss functions, so we performed a
3Conditioning on labels remains possible (see Section 4.2).
hyperparameter search using a short training run, with the same number (500) of hyperparameter configurations evaluated for both models.",4.1. CIFAR-10,[0],[0]
"For all results, we report full training runs using the best found hyperparameters in each case.",4.1. CIFAR-10,[0],[0]
"The evaluation metric used for the hyperparameter search was the Fréchet Inception Distance (FID) (Heusel et al., 2017), see Appendix for details.",4.1. CIFAR-10,[0],[0]
"In addition to FID, we report Inception score (Salimans et al., 2016) for both models.
",4.1. CIFAR-10,[0],[0]
Figure 4 (left) shows Inception score and FID for both models evaluated at several points throughout training.,4.1. CIFAR-10,[0],[0]
"The fully trained PixelCNN achieves an Inception score and FID of 4.6 and 65.9 respectively, while PixelIQN substantially outperforms it with an Inception score of 5.3 and FID of 49.5.",4.1. CIFAR-10,[0],[0]
"This also compares favorably with e.g. WGAN (Arjovsky et al., 2017), which reaches an Inception score of 3.8.",4.1. CIFAR-10,[0],[0]
"For subjective evaluations, we give samples from both models in Figure 3.",4.1. CIFAR-10,[0],[0]
Samples coming from PixelIQN are much more visually coherent.,4.1. CIFAR-10,[0],[0]
"Of note, the PixelIQN model achieves
a performance level comparable to that of the fully trained PixelCNN with only about one third the number of training updates (and about one third of the wall-clock time).",4.1. CIFAR-10,[0],[0]
"Next, we turn to the small ImageNet dataset (Russakovsky et al., 2015), first used for generative modeling in the PixelRNN work (van den Oord et al., 2016c).",4.2. ImageNet 32x32,[0],[0]
"Again, we evaluate using FID and Inception score.",4.2. ImageNet 32x32,[0],[0]
"For this much harder dataset, we base our PixelCNN and PixelIQN models on the larger 20-layer variant used in (van den Oord et al., 2016b).",4.2. ImageNet 32x32,[0],[0]
"Due to substantially longer training time for this model, we did not perform additional hyperparameter tuning, and mostly used the same hyperparameter values as in the previous sections for both models; details can be found in the Appendix.
",4.2. ImageNet 32x32,[0],[0]
Figure 4 shows Inception score and FID throughout training of PixelCNN and PixelIQN.,4.2. ImageNet 32x32,[0],[0]
"Again, PixelIQN substan-
tially outperforms the baseline in terms of final performance and sample complexity.",4.2. ImageNet 32x32,[0],[0]
"For final scores and a comparison to state-of-the-art GAN models, see Table 1.",4.2. ImageNet 32x32,[0],[0]
Figure 5 shows random (non-cherry-picked) samples from both models.,4.2. ImageNet 32x32,[0],[0]
"Compared to PixelCNN, PixelIQN samples appear to have superior quality with more global consistency and less ‘high-frequency noise’.
",4.2. ImageNet 32x32,[0],[0]
"In Figure 6, we show the inpainting performance of PixelIQN, by fixing the top half of a validation set image as input and sampling repeatedly from the model to generate different completions.",4.2. ImageNet 32x32,[0],[0]
We note that the model consistently generates plausible completions with significant diversity between different completion samples for the same input image.,4.2. ImageNet 32x32,[0],[0]
"Meanwhile, WGAN-GP has been seen to produce deterministic completions (Bellemare et al., 2017).
",4.2. ImageNet 32x32,[0],[0]
"Following (van den Oord et al., 2016b), we also trained a class-conditional PixelIQN variant, providing to the model the one-hot class label corresponding to a training image (in addition to a τ sample).",4.2. ImageNet 32x32,[0],[0]
"Samples from a class-conditional model can be expected to have higher visual quality, as the class label provides log2(1000)",4.2. ImageNet 32x32,[0],[0]
"≈ 10 bits of information, see Figure 7.",4.2. ImageNet 32x32,[0],[0]
"As seen in Figure 4 and Table 1, class conditioning also further improves Inception score and FID.",4.2. ImageNet 32x32,[0],[0]
"To generate each sample for the computation of these scores, we sample one of 1000 class labels randomly, then generate an image conditioned on this label via the trained model.
",4.2. ImageNet 32x32,[0],[0]
"Finally, motivated by the very long training time for the large PixelCNN model (approximately 1 day per 100K training steps, on 16 NVIDIA Tesla P100 GPUs), we also trained smaller 15-layer versions of the models (same as the ones used on CIFAR-10) on the small ImageNet dataset.",4.2. ImageNet 32x32,[0],[0]
"For comparison, these take approximately 12 hours for 100K training steps on a single P100 GPU, or less than 3 hours on 8 P100 GPUs.",4.2. ImageNet 32x32,[0],[0]
"As expected, little PixelCNN, while suitable
for the CIFAR-10 dataset, fails to achieve competitive scores on the ImageNet dataset, achieving Inception score 5.1 and FID 66.4.",4.2. ImageNet 32x32,[0],[0]
"Astonishingly, little PixelIQN on this dataset reaches Inception score 7.3 and FID 38.5, see Figure 4 (right).",4.2. ImageNet 32x32,[0],[0]
"It thereby not only outperforms the little PixelCNN, but also the larger 20-layer version!",4.2. ImageNet 32x32,[0],[0]
"This strongly supports the hypothesis that PixelCNN, and potentially many other models, are constrained not only by their model capacity, but crucially also by the sub-optimal trade-offs made by their log-likelihood training criterion, failing to align with perceptual or evaluation metrics.",4.2. ImageNet 32x32,[0],[0]
Most existing generative models for images belong to one of two classes.,5. Discussion and Conclusions,[0],[0]
"The first are likelihood-based models, trained with an elementwise KL reconstruction loss, which,
while perceptually meaningless, provides robust optimization properties and high sample diversity.",5. Discussion and Conclusions,[0],[0]
"The second are GANs, trained based on a discriminator loss, typically better aligned with a perceptual metric and enabling the generator to produce realistic, globally consistent samples.",5. Discussion and Conclusions,[0],[0]
"Their advantages come at the cost of a harder optimization problem, high parameter sensitivity, and most importantly, a tendency to collapse modes of the data distribution.
",5. Discussion and Conclusions,[0],[0]
"AIQNs are a new, fundamentally different, technique for generative modeling.",5. Discussion and Conclusions,[0],[0]
"By using a quantile regression loss instead of KL divergence, they combine some of the best properties of the two model classes.",5. Discussion and Conclusions,[0],[0]
"By their nature, they preserve modes of the learned distribution, while producing perceptually appealing high-quality samples.",5. Discussion and Conclusions,[0],[0]
The inevitable approximation trade-offs a generative model makes when constrained by capacity or insufficient training can vary significantly depending on the loss used.,5. Discussion and Conclusions,[0],[0]
"We argue that the proposed quantile regression loss aligns more effectively with a given metric and therefore makes subjectively more advantageous trade-offs.
",5. Discussion and Conclusions,[0],[0]
Devising methods for quantile regression over multidimensional outputs is an active area of research.,5. Discussion and Conclusions,[0],[0]
"New methods are continuing to be investigated (Carlier et al., 2016; Hallin & Miroslav, 2016), and a promising direction for future work is to find ways to use these to replace autoregressive models.",5. Discussion and Conclusions,[0],[0]
"One approach to reducing the computational burden of such models is to apply AIQN to the latent dimensions
of a VAE.",5. Discussion and Conclusions,[0],[0]
"Similar in spirit to Rosca et al. (2017), this would use the VAE to reduce the dimensionality of the problem and the AIQN to sample from the true latent distribution.",5. Discussion and Conclusions,[0],[0]
"In the Appendix we give preliminary results using such an technique, on CelebA 64× 64 (Liu et al., 2015).",5. Discussion and Conclusions,[0],[0]
"We have shown that IQN, computationally cheap and technically simple, can be readily applied to existing architectures, PixelCNN and VAE (Appendix), improving robustness and sampling quality of the underlying model.",5. Discussion and Conclusions,[0],[0]
"We demonstrated that PixelIQN produces more realistic, globally coherent samples, and improves Inception score and FID.
",5. Discussion and Conclusions,[0],[0]
We further point out that many recent advances in generative models could be easily combined with our proposed method.,5. Discussion and Conclusions,[0],[0]
"Recent algorithmic improvements to GANs such as mini-batch discrimination and progressive growing (Salimans et al., 2016; Karras et al., 2017), while not strictly necessary in our work, could be applied to further improve performance.",5. Discussion and Conclusions,[0],[0]
"PixelCNN++ (Salimans et al., 2017) is an architectural improvement of PixelCNN, with several beneficial modifications supported by experimental evidence.",5. Discussion and Conclusions,[0],[0]
"Although we have built upon the original Gated PixelCNN in this work, we believe all of these modifications to be compatible with our work, except for the use of a mixture of logistics in place of PixelCNN’s softmax.",5. Discussion and Conclusions,[0],[0]
"As we have entirely replaced this model component, this change does not map onto our model.",5. Discussion and Conclusions,[0],[0]
"Of note, the motivation behind this change closely mirrors our own, in looking for a loss that respects the underlying metric between examples.",5. Discussion and Conclusions,[0],[0]
"The recent PixelSNAIL model (Chen et al., 2017) achieves stateof-the-art modeling performance by enhancing PixelCNN with ELU nonlinearities, modified block structure, and an attention mechanism.",5. Discussion and Conclusions,[0],[0]
"Again, all of these are fully compatible with our work and should improve results further.
",5. Discussion and Conclusions,[0],[0]
"Finally, the implicit quantile formulation lifts a number of architectural restrictions of previous generative models.",5. Discussion and Conclusions,[0],[0]
"Most importantly, the reparameterization as an inverse c.d.f. allows to learn distributions over continuous ranges without pre-specified boundaries or quantization.",5. Discussion and Conclusions,[0],[0]
"This enables modeling continuous-valued variables, for example for generation of sound (van den Oord et al., 2016a), opening multiple interesting avenues for further investigation.",5. Discussion and Conclusions,[0],[0]
We would like to acknowledge the important role many of our colleagues at DeepMind played for this work.,Acknowledgements,[0],[0]
"We especially thank Aäron van den Oord and Sander Dieleman for invaluable advice on the PixelCNN model; Ivo Danihelka and Danilo J. Rezende for careful reading and insightful comments on an earlier version of the paper; Igor Babuschkin, Alexandre Galashov, Dominik Grewe, Jacob Menick, and Mihaela Rosca for technical help.",Acknowledgements,[0],[0]
"We introduce autoregressive implicit quantile networks (AIQN), a fundamentally different approach to generative modeling than those commonly used, that implicitly captures the distribution using quantile regression.",abstractText,[0],[0]
"AIQN is able to achieve superior perceptual quality and improvements in evaluation metrics, without incurring a loss of sample diversity.",abstractText,[0],[0]
The method can be applied to many existing models and architectures.,abstractText,[0],[0]
"In this work we extend the PixelCNN model with AIQN and demonstrate results on CIFAR-10 and ImageNet using Inception score, FID, non-cherrypicked samples, and inpainting results.",abstractText,[0],[0]
We consistently observe that AIQN yields a highly stable algorithm that improves perceptual quality while maintaining a highly diverse distribution.,abstractText,[0],[0]
Autoregressive Quantile Networks for Generative Modeling,title,[0],[0]
"We study the problem of attributing the prediction of a deep network to its input features.
",1. Motivation and Summary of Results,[0],[0]
Definition 1.,1. Motivation and Summary of Results,[0],[0]
"Formally, suppose we have a function F :",1. Motivation and Summary of Results,[0],[0]
"Rn → [0, 1] that represents a deep network, and an input x = (x1, . . .",1. Motivation and Summary of Results,[0],[0]
", xn) ∈ Rn.",1. Motivation and Summary of Results,[0],[0]
"An attribution of the prediction at input x relative to a baseline input x′ is a vector AF (x, x
′) =",1. Motivation and Summary of Results,[0],[0]
"(a1, . . .",1. Motivation and Summary of Results,[0],[0]
", an) ∈ Rn where ai is the contribution of xi to the prediction F (x).
",1. Motivation and Summary of Results,[0],[0]
"For instance, in an object recognition network, an attribution method could tell us which pixels of the image were responsible for a certain label being picked (see Figure 2).",1. Motivation and Summary of Results,[0],[0]
"The attribution problem was previously studied by various papers (Baehrens et al., 2010; Simonyan et al., 2013;
",1. Motivation and Summary of Results,[0],[0]
"*Equal contribution 1Google Inc., Mountain View, USA.",1. Motivation and Summary of Results,[0],[0]
Correspondence to: Mukund Sundararajan <mukunds@google.com,1. Motivation and Summary of Results,[0],[0]
">, Ankur Taly <ataly@google.com>.
",1. Motivation and Summary of Results,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Motivation and Summary of Results,[0],[0]
"Copyright 2017 by the author(s).
",1. Motivation and Summary of Results,[0],[0]
"Shrikumar et al., 2016; Binder et al., 2016; Springenberg et al., 2014).
",1. Motivation and Summary of Results,[0],[0]
"The intention of these works is to understand the inputoutput behavior of the deep network, which gives us the ability to improve it.",1. Motivation and Summary of Results,[0],[0]
"Such understandability is critical to all computer programs, including machine learning models.",1. Motivation and Summary of Results,[0],[0]
There are also other applications of attribution.,1. Motivation and Summary of Results,[0],[0]
They could be used within a product driven by machine learning to provide a rationale for the recommendation.,1. Motivation and Summary of Results,[0],[0]
"For instance, a deep network that predicts a condition based on imaging could help inform the doctor of the part of the image that resulted in the recommendation.",1. Motivation and Summary of Results,[0],[0]
This could help the doctor understand the strengths and weaknesses of a model and compensate for it.,1. Motivation and Summary of Results,[0],[0]
We give such an example in Section 6.2.,1. Motivation and Summary of Results,[0],[0]
Attributions could also be used by developers in an exploratory sense.,1. Motivation and Summary of Results,[0],[0]
"For instance, we could use a deep network to extract insights that could be then used in a rulebased system.",1. Motivation and Summary of Results,[0],[0]
"In Section 6.3, we give such an example.
",1. Motivation and Summary of Results,[0],[0]
A significant challenge in designing an attribution technique is that they are hard to evaluate empirically.,1. Motivation and Summary of Results,[0],[0]
"As we discuss in Section 4, it is hard to tease apart errors that stem from the misbehavior of the model versus the misbehavior of the attribution method.",1. Motivation and Summary of Results,[0],[0]
"To compensate for this shortcoming, we take an axiomatic approach.",1. Motivation and Summary of Results,[0],[0]
In Section 2 we identify two axioms that every attribution method must satisfy.,1. Motivation and Summary of Results,[0],[0]
Unfortunately most previous methods do not satisfy one of these two axioms.,1. Motivation and Summary of Results,[0],[0]
"In Section 3, we use the axioms to identify a new method, called integrated gradients.
",1. Motivation and Summary of Results,[0],[0]
"Unlike previously proposed methods, integrated gradients do not need any instrumentation of the network, and can be computed easily using a few calls to the gradient operation, allowing even novice practitioners to easily apply the technique.
",1. Motivation and Summary of Results,[0],[0]
"In Section 6, we demonstrate the ease of applicability over several deep networks, including two images networks, two text processing networks, and a chemistry network.",1. Motivation and Summary of Results,[0],[0]
"These applications demonstrate the use of our technique in either improving our understanding of the network, performing debugging, performing rule extraction, or aiding an end user in understanding the network’s prediction.
",1. Motivation and Summary of Results,[0],[0]
Remark 1.,1. Motivation and Summary of Results,[0],[0]
Let us briefly examine the need for the baseline in the definition of the attribution problem.,1. Motivation and Summary of Results,[0],[0]
"A common way for humans to perform attribution relies on counter-
ar X
iv :1
70 3.
01 36
5v 2
[ cs
.L",1. Motivation and Summary of Results,[0],[0]
"G
] 1
3 Ju
n 20
17
factual intuition.",1. Motivation and Summary of Results,[0],[0]
When we assign blame to a certain cause we implicitly consider the absence of the cause as a baseline for comparing outcomes.,1. Motivation and Summary of Results,[0],[0]
"In a deep network, we model the absence using a single baseline input.",1. Motivation and Summary of Results,[0],[0]
"For most deep networks, a natural baseline exists in the input space where the prediction is neutral.",1. Motivation and Summary of Results,[0],[0]
"For instance, in object recognition networks, it is the black image.",1. Motivation and Summary of Results,[0],[0]
"The need for a baseline has also been pointed out by prior work on attribution (Shrikumar et al., 2016; Binder et al., 2016).",1. Motivation and Summary of Results,[0],[0]
We now discuss two axioms (desirable characteristics) for attribution methods.,2. Two Fundamental Axioms,[0],[0]
We find that other feature attribution methods in literature break at least one of the two axioms.,2. Two Fundamental Axioms,[0],[0]
"These methods include DeepLift (Shrikumar et al., 2016; 2017), Layer-wise relevance propagation (LRP) (Binder et al., 2016), Deconvolutional networks (Zeiler & Fergus, 2014), and Guided back-propagation (Springenberg et al., 2014).",2. Two Fundamental Axioms,[0],[0]
"As we will see in Section 3, these axioms will also guide the design of our method.
Gradients.",2. Two Fundamental Axioms,[0],[0]
"For linear models, ML practitioners regularly inspect the products of the model coefficients and the feature values in order to debug predictions.",2. Two Fundamental Axioms,[0],[0]
"Gradients (of the output with respect to the input) is a natural analog of the model coefficients for a deep network, and therefore the product of the gradient and feature values is a reasonable starting point for an attribution method (Baehrens et al., 2010; Simonyan et al., 2013); see the third column of Figure 2 for examples.",2. Two Fundamental Axioms,[0],[0]
"The problem with gradients is that they break sensitivity, a property that all attribution methods should satisfy.",2. Two Fundamental Axioms,[0],[0]
An attribution method satisfies Sensitivity(a),2.1. Axiom: Sensitivity(a),[0],[0]
if for every input and baseline that differ in one feature but have different predictions then the differing feature should be given a non-zero attribution.,2.1. Axiom: Sensitivity(a),[0],[0]
"(Later in the paper, we will have a part (b) to this definition.)
",2.1. Axiom: Sensitivity(a),[0],[0]
Gradients violate Sensitivity(a):,2.1. Axiom: Sensitivity(a),[0],[0]
"For a concrete example, consider a one variable, one ReLU network, f(x) = 1 − ReLU(1−x).",2.1. Axiom: Sensitivity(a),[0],[0]
Suppose the baseline is x = 0 and the input is x = 2.,2.1. Axiom: Sensitivity(a),[0],[0]
"The function changes from 0 to 1, but because f becomes flat at x = 1, the gradient method gives attribution of 0 to x. Intuitively, gradients break Sensitivity because the prediction function may flatten at the input and thus have zero gradient despite the function value at the input being different from that at the baseline.",2.1. Axiom: Sensitivity(a),[0],[0]
"This phenomenon has been reported in previous work (Shrikumar et al., 2016).
",2.1. Axiom: Sensitivity(a),[0],[0]
"Practically, the lack of sensitivity causes gradients to focus on irrelevant features (see the “fireboat” example in Fig-
ure 2).
",2.1. Axiom: Sensitivity(a),[0],[0]
Other back-propagation based approaches.,2.1. Axiom: Sensitivity(a),[0],[0]
A second set of approaches involve back-propagating the final prediction score through each layer of the network down to the individual features.,2.1. Axiom: Sensitivity(a),[0],[0]
"These include DeepLift, Layer-wise relevance propagation (LRP), Deconvolutional networks (DeConvNets), and Guided back-propagation.",2.1. Axiom: Sensitivity(a),[0],[0]
"These methods differ in the specific backpropagation logic for various activation functions (e.g., ReLU, MaxPool, etc.).
",2.1. Axiom: Sensitivity(a),[0],[0]
"Unfortunately, Deconvolution networks (DeConvNets), and Guided back-propagation violate Sensitivity(a).",2.1. Axiom: Sensitivity(a),[0],[0]
This is because these methods back-propogate through a ReLU node only if the ReLU is turned on at the input.,2.1. Axiom: Sensitivity(a),[0],[0]
"This makes the method similar to gradients, in that, the attribution is zero for features with zero gradient at the input despite a non-zero gradient at the baseline.",2.1. Axiom: Sensitivity(a),[0],[0]
"We defer the specific counterexamples to Appendix B.
Methods like DeepLift and LRP tackle the Sensitivity issue by employing a baseline, and in some sense try to compute “discrete gradients” instead of (instantaeneous) gradients at the input.",2.1. Axiom: Sensitivity(a),[0],[0]
(The two methods differ in the specifics of how they compute the discrete gradient).,2.1. Axiom: Sensitivity(a),[0],[0]
"But the idea is that a large, discrete step will avoid flat regions, avoiding a breakage of sensitivity.",2.1. Axiom: Sensitivity(a),[0],[0]
"Unfortunately, these methods violate a different requirement on attribution methods.",2.1. Axiom: Sensitivity(a),[0],[0]
"Two networks are functionally equivalent if their outputs are equal for all inputs, despite having very different implementations.",2.2. Axiom: Implementation Invariance,[0],[0]
"Attribution methods should satisfy Implementation Invariance, i.e., the attributions are always identical for two functionally equivalent networks.",2.2. Axiom: Implementation Invariance,[0],[0]
"To motivate this, notice that attribution can be colloquially defined as assigning the blame (or credit) for the output to the input features.",2.2. Axiom: Implementation Invariance,[0],[0]
"Such a definition does not refer to implementation details.
",2.2. Axiom: Implementation Invariance,[0],[0]
"We now discuss intuition for why DeepLift and LRP break Implementation Invariance; a concrete example is provided in Appendix B.
First, notice that gradients are invariant to implementation.",2.2. Axiom: Implementation Invariance,[0],[0]
"In fact, the chain-rule for gradients ∂f∂g = ∂f ∂h · ∂h ∂g is essentially about implementation invariance.",2.2. Axiom: Implementation Invariance,[0],[0]
"To see this, think of g and f as the input and output of a system, and h being some implementation detail of the system.",2.2. Axiom: Implementation Invariance,[0],[0]
"The gradient of output f to input g can be computed either directly by ∂f∂g , ignoring the intermediate function h (implementation detail), or by invoking the chain rule via h.",2.2. Axiom: Implementation Invariance,[0],[0]
"This is exactly how backpropagation works.
",2.2. Axiom: Implementation Invariance,[0],[0]
Methods like LRP and DeepLift replace gradients with discrete gradients and still use a modified form of backpropagation to compose discrete gradients into attributions.,2.2. Axiom: Implementation Invariance,[0],[0]
"Un-
fortunately, the chain rule does not hold for discrete gradients in general.",2.2. Axiom: Implementation Invariance,[0],[0]
Formally f(x1)−f(x0)g(x1)−g(x0) 6=,2.2. Axiom: Implementation Invariance,[0],[0]
"f(x1)−f(x0) h(x1)−h(x0) · h(x1)−h(x0) g(x1)−g(x0) , and therefore these methods fail to satisfy implementation invariance.
",2.2. Axiom: Implementation Invariance,[0],[0]
"If an attribution method fails to satisfy Implementation Invariance, the attributions are potentially sensitive to unimportant aspects of the models.",2.2. Axiom: Implementation Invariance,[0],[0]
"For instance, if the network architecture has more degrees of freedom than needed to represent a function then there may be two sets of values for the network parameters that lead to the same function.",2.2. Axiom: Implementation Invariance,[0],[0]
"The training procedure can converge at either set of values depending on the initializtion or for other reasons, but the underlying network function would remain the same.",2.2. Axiom: Implementation Invariance,[0],[0]
It is undesirable that attributions differ for such reasons.,2.2. Axiom: Implementation Invariance,[0],[0]
We are now ready to describe our technique.,3. Our Method: Integrated Gradients,[0],[0]
"Intuitively, our technique combines the Implementation Invariance of Gradients along with the Sensitivity of techniques like LRP or DeepLift.
",3. Our Method: Integrated Gradients,[0],[0]
"Formally, suppose we have a function F :",3. Our Method: Integrated Gradients,[0],[0]
"Rn → [0, 1] that represents a deep network.",3. Our Method: Integrated Gradients,[0],[0]
"Specifically, let x ∈",3. Our Method: Integrated Gradients,[0],[0]
"Rn be the input at hand, and x′ ∈ Rn be the baseline input.",3. Our Method: Integrated Gradients,[0],[0]
"For image networks, the baseline could be the black image, while for text models it could be the zero embedding vector.
",3. Our Method: Integrated Gradients,[0],[0]
"We consider the straightline path (in Rn) from the baseline x′ to the input x, and compute the gradients at all points along the path.",3. Our Method: Integrated Gradients,[0],[0]
Integrated gradients are obtained by cumulating these gradients.,3. Our Method: Integrated Gradients,[0],[0]
"Specifically, integrated gradients are defined as the path intergral of the gradients along the straightline path from the baseline x′ to the input x.
The integrated gradient along the ith dimension for an input x and baseline x′ is defined as follows.",3. Our Method: Integrated Gradients,[0],[0]
"Here, ∂F (x)∂xi is the gradient of F (x) along the ith dimension.
",3. Our Method: Integrated Gradients,[0],[0]
IntegratedGradsi(x) ::= (xi−x ′,3. Our Method: Integrated Gradients,[0],[0]
i)× ∫ 1 α=0 ∂F (x′+α×(x−x′)),3. Our Method: Integrated Gradients,[0],[0]
"∂xi dα
(1)",3. Our Method: Integrated Gradients,[0],[0]
Axiom: Completeness.,3. Our Method: Integrated Gradients,[0],[0]
"Integrated gradients satisfy an
axiom called completeness that the attributions add up to the difference between the output of F at the input x and the baseline x′.",3. Our Method: Integrated Gradients,[0],[0]
This axiom is identified as being desirable by Deeplift and LRP.,3. Our Method: Integrated Gradients,[0],[0]
"It is a sanity check that the attribution method is somewhat comprehensive in its accounting, a property that is clearly desirable if the networks score is used in a numeric sense, and not just to pick the top label, for e.g., a model estimating insurance premiums from credit features of individuals.
",3. Our Method: Integrated Gradients,[0],[0]
"This is formalized by the proposition below, which instantiates the fundamental theorem of calculus for path integrals.
",3. Our Method: Integrated Gradients,[0],[0]
Proposition 1.,3. Our Method: Integrated Gradients,[0],[0]
"If F : Rn → R is differentiable almost everywhere 1 then
Σni=1IntegratedGradsi(x)",3. Our Method: Integrated Gradients,[0],[0]
"= F (x)− F (x′)
",3. Our Method: Integrated Gradients,[0],[0]
"For most deep networks, it is possible to choose a baseline such that the prediction at the baseline is near zero (F (x′)",3. Our Method: Integrated Gradients,[0],[0]
≈ 0).,3. Our Method: Integrated Gradients,[0],[0]
"(For image models, the black image baseline indeed satisfies this property.)",3. Our Method: Integrated Gradients,[0],[0]
"In such cases, there is an intepretation of the resulting attributions that ignores the baseline and amounts to distributing the output to the individual input features.
",3. Our Method: Integrated Gradients,[0],[0]
Remark 2.,3. Our Method: Integrated Gradients,[0],[0]
Integrated gradients satisfies Sensivity(a) because Completeness implies Sensivity(a) and is thus a strengthening of the Sensitivity(a) axiom.,3. Our Method: Integrated Gradients,[0],[0]
"This is because Sensitivity(a) refers to a case where the baseline and the input differ only in one variable, for which Completeness asserts that the difference in the two output values is equal to the attribution to this variable.",3. Our Method: Integrated Gradients,[0],[0]
Attributions generated by integrated gradients satisfy Implementation Invariance since they are based only on the gradients of the function represented by the network.,3. Our Method: Integrated Gradients,[0],[0]
Prior literature has relied on empirically evaluating the attribution technique.,4. Uniqueness of Integrated Gradients,[0],[0]
"For instance, in the context of an object recognition task, (Samek et al., 2015) suggests that we select the top k pixels by attribution and randomly vary their intensities and then measure the drop in score.",4. Uniqueness of Integrated Gradients,[0],[0]
"If the attribution method is good, then the drop in score should be large.",4. Uniqueness of Integrated Gradients,[0],[0]
"However, the images resulting from pixel perturbation could be unnatural, and it could be that the scores drop simply because the network has never seen anything like it in training.",4. Uniqueness of Integrated Gradients,[0],[0]
"(This is less of a concern with linear or logistic models where the simplicity of the model ensures that ablating a feature does not cause strange interactions.)
",4. Uniqueness of Integrated Gradients,[0],[0]
"A different evaluation technique considers images with human-drawn bounding boxes around objects, and computes the percentage of pixel attribution inside the box.",4. Uniqueness of Integrated Gradients,[0],[0]
"While for most objects, one would expect the pixels located on the object to be most important for the prediction, in some cases the context in which the object occurs may also contribute to the prediction.",4. Uniqueness of Integrated Gradients,[0],[0]
"The cabbage butterfly image from Figure 2 is a good example of this where the pixels on the leaf are also surfaced by the integrated gradients.
",4. Uniqueness of Integrated Gradients,[0],[0]
"Roughly, we found that every empirical evaluation technique we could think of could not differentiate between ar-
1Formally, this means the function F is continuous everywhere and the partial derivative of F along each input dimension satisfies Lebesgue’s integrability condition, i.e., the set of discontinuous points has measure zero.",4. Uniqueness of Integrated Gradients,[0],[0]
"Deep networks built out of Sigmoids, ReLUs, and pooling operators satisfy this condition.
tifacts that stem from perturbing the data, a misbehaving model, and a misbehaving attribution method.",4. Uniqueness of Integrated Gradients,[0],[0]
This was why we turned to an axiomatic approach in designing a good attribution method (Section 2).,4. Uniqueness of Integrated Gradients,[0],[0]
"While our method satisfies Sensitivity and Implementation Invariance, it certainly isn’t the unique method to do so.
",4. Uniqueness of Integrated Gradients,[0],[0]
We now justify the selection of the integrated gradients method in two steps.,4. Uniqueness of Integrated Gradients,[0],[0]
"First, we identify a class of methods called Path methods that generalize integrated gradients.",4. Uniqueness of Integrated Gradients,[0],[0]
We discuss that path methods are the only methods to satisfy certain desirable axioms.,4. Uniqueness of Integrated Gradients,[0],[0]
"Second, we argue why integrated gradients is somehow canonical among the different path methods.",4. Uniqueness of Integrated Gradients,[0],[0]
Integrated gradients aggregate the gradients along the inputs that fall on the straightline between the baseline and the input.,4.1. Path Methods,[0],[0]
"There are many other (non-straightline) paths that monotonically interpolate between the two points, and each such path will yield a different attribution method.",4.1. Path Methods,[0],[0]
"For instance, consider the simple case when the input is two dimensional.",4.1. Path Methods,[0],[0]
"Figure 1 has examples of three paths, each of which corresponds to a different attribution method.
",4.1. Path Methods,[0],[0]
"Formally, let γ = (γ1, . . .",4.1. Path Methods,[0],[0]
", γn) :",4.1. Path Methods,[0],[0]
"[0, 1] → Rn be a smooth function specifying a path in Rn from the baseline x′ to the input x, i.e., γ(0)",4.1. Path Methods,[0],[0]
"= x′ and γ(1) = x.
Given a path function γ, path integrated gradients are obtained by integrating the gradients along the path γ(α) for α ∈",4.1. Path Methods,[0],[0]
"[0, 1].",4.1. Path Methods,[0],[0]
"Formally, path integrated gradients along the ith dimension for an input x is defined as follows.
",4.1. Path Methods,[0],[0]
PathIntegratedGradsγi (x) ::= ∫ 1 α=0 ∂F (γ(α)) ∂γi(α) ∂γi(α) ∂α,4.1. Path Methods,[0],[0]
"dα
(2) where ∂F (x)∂xi is the gradient of F along the i
th dimension at x.
Attribution methods based on path integrated gradients are
collectively known as path methods.",4.1. Path Methods,[0],[0]
Notice that integrated gradients is a path method for the straightline path specified γ(α) = x′ + α× (x− x′) for α ∈,4.1. Path Methods,[0],[0]
"[0, 1].",4.1. Path Methods,[0],[0]
Remark 3.,4.1. Path Methods,[0],[0]
All path methods satisfy Implementation Invariance.,4.1. Path Methods,[0],[0]
"This follows from the fact that they are defined using the underlying gradients, which do not depend on the implementation.",4.1. Path Methods,[0],[0]
"They also satisfy Completeness (the proof is similar to that of Proposition 1) and Sensitvity(a) which is implied by Completeness (see Remark 2).
",4.1. Path Methods,[0],[0]
"More interestingly, path methods are the only methods that satisfy certain desirable axioms.",4.1. Path Methods,[0],[0]
"(For formal definitions of the axioms and proof of Proposition 2, see Friedman (Friedman, 2004).)
",4.1. Path Methods,[0],[0]
Axiom: Sensitivity(b).,4.1. Path Methods,[0],[0]
"(called Dummy in (Friedman, 2004))",4.1. Path Methods,[0],[0]
"If the function implemented by the deep network does not depend (mathematically) on some variable, then the attribution to that variable is always zero.
",4.1. Path Methods,[0],[0]
This is a natural complement to the definition of Sensitivity(a) from Section 2.,4.1. Path Methods,[0],[0]
"This definition captures desired insensitivity of the attributions.
",4.1. Path Methods,[0],[0]
Axiom: Linearity.,4.1. Path Methods,[0],[0]
"Suppose that we linearly composed two deep networks modeled by the functions f1 and f2 to form a third network that models the function a×f1+b×f2, i.e., a linear combination of the two networks.",4.1. Path Methods,[0],[0]
Then we’d like the attributions for a× f1 + b× f2 to be the weighted sum of the attributions for f1 and f2 with weights a and b respectively.,4.1. Path Methods,[0],[0]
"Intuitively, we would like the attributions to preserve any linearity within the network.",4.1. Path Methods,[0],[0]
Proposition 2.,4.1. Path Methods,[0],[0]
"(Theorem 1 (Friedman, 2004))",4.1. Path Methods,[0],[0]
"Path methods are the only attribution methods that always satisfy Implementation Invariance, Sensitivity(b), Linearity, and Completeness.",4.1. Path Methods,[0],[0]
Remark 4.,4.1. Path Methods,[0],[0]
"We note that these path integrated gradients have been used within the cost-sharing literature in economics where the function models the cost of a project as a function of the demands of various participants, and the attributions correspond to cost-shares.",4.1. Path Methods,[0],[0]
"Integrated gradients correspond to a cost-sharing method called AumannShapley (Aumann & Shapley, 1974).",4.1. Path Methods,[0],[0]
Proposition 2 holds for our attribution problem because mathematically the cost-sharing problem corresponds to the attribution problem with the benchmark fixed at the zero vector.,4.1. Path Methods,[0],[0]
(Implementation Invariance is implicit in the cost-sharing literature as the cost functions are considered directly in their mathematical form.),4.1. Path Methods,[0],[0]
"In this section, we formalize why the straightline path chosen by integrated gradients is canonical.",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"First, observe that it is the simplest path that one can define mathematically.
",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"Second, a natural property for attribution methods is to preserve symmetry, in the following sense.
",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
Symmetry-Preserving.,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
Two input variables are symmetric w.r.t.,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
a function if swapping them does not change the function.,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"For instance, x and y are symmetric w.r.t.",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
F,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"if and only if F (x, y) = F (y, x) for all values of x and y.",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"An attribution method is symmetry preserving, if for all inputs that have identical values for symmetric variables and baselines that have identical values for symmetric variables, the symmetric variables receive identical attributions.
",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"E.g., consider the logistic model Sigmoid(x1 + x2 + . . . ).",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
x1 and x2 are symmetric variables for this model.,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"For an input where x1 = x2 = 1 (say) and baseline where x1 = x2 = 0 (say), a symmetry preserving method must offer identical attributions to x1 and x2.
",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"It seems natural to ask for symmetry-preserving attribution methods because if two variables play the exact same role in the network (i.e., they are symmetric and have the same values in the baseline and the input) then they ought to receive the same attrbiution.
",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
Theorem 1.,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"Integrated gradients is the unique path method that is symmetry-preserving.
",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"The proof is provided in Appendix A.
Remark 5.",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"If we allow averaging over the attributions from multiple paths, then are other methods that satisfy all the axioms in Theorem 1.",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"In particular, there is the method by Shapley-Shubik (Shapley & Shubik, 1971) from the cost sharing literature, and used by (Lundberg & Lee, 2016; Datta et al., 2016) to compute feature attributions (though they were not studying deep networks).",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"In this method, the attribution is the average of those from n!",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
extremal paths; here n is the number of features.,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"Here each such path considers an ordering of the input features, and sequentially changes the input feature from its value at the baseline to its value at the input.",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
This method yields attributions that are different from integrated gradients.,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"If the function of interest is min(x1, x2), the baseline is x1 = x2 = 0, and the input is x1 = 1, x2 = 3, then integrated gradients attributes the change in the function value entirely to the critical variable x1, whereas Shapley-Shubik assigns attributions of 1/2 each; it seems somewhat subjective to prefer one result over the other.
",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"We also envision other issues with applying Shapley-Shubik to deep networks: It is computationally expensive; in an object recognition network that takes an 100X100 image as input, n is 10000, and n!",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
is a gigantic number.,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"Even if one samples few paths randomly, evaluating the attributions for a single path takes n calls to the deep network.",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"In contrast, integrated gradients is able to operate with 20 to 300 calls.",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"Further, the Shapley-Shubik computation visit
inputs that are combinations of the input and the baseline.",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
It is possible that some of these combinations are very different from anything seen during training.,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
We speculate that this could lead to attribution artifacts.,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
Selecting a Benchmark.,5. Applying Integrated Gradients,[0],[0]
A key step in applying integrated gradients is to select a good baseline.,5. Applying Integrated Gradients,[0],[0]
"We recommend that developers check that the baseline has a near-zero score— as discussed in Section 3, this allows us to interpret the attributions as a function of the input.",5. Applying Integrated Gradients,[0],[0]
But there is more to a good baseline:,5. Applying Integrated Gradients,[0],[0]
"For instance, for an object recogntion network it is possible to create an adversarial example that has a zero score for a given input label (say elephant), by applying a tiny, carefully-designed perturbation to an image with a very different label (say microscope) (cf.",5. Applying Integrated Gradients,[0],[0]
"(Goodfellow et al., 2015)).",5. Applying Integrated Gradients,[0],[0]
The attributions can then include undesirable artifacts of this adversarially constructed baseline.,5. Applying Integrated Gradients,[0],[0]
"So we would additionally like the baseline to convey a complete absence of signal, so that the features that are apparent from the attributions are properties only of the input, and not of the baseline.",5. Applying Integrated Gradients,[0],[0]
"For instance, in an object recognition network, a black image signifies the absence of objects.",5. Applying Integrated Gradients,[0],[0]
The black image isn’t unique in this sense—an image consisting of noise has the same property.,5. Applying Integrated Gradients,[0],[0]
"However, using black as a baseline may result in cleaner visualizations of “edge” features.",5. Applying Integrated Gradients,[0],[0]
"For text based networks, we have found that the allzero input embedding vector is a good baseline.",5. Applying Integrated Gradients,[0],[0]
"The action of training causes unimportant words tend to have small norms, and so, in the limit, unimportance corresponds to the all-zero baseline.",5. Applying Integrated Gradients,[0],[0]
"Notice that the black image corresponds to a valid input to an object recognition network, and is also intuitively what we humans would consider absence of signal.",5. Applying Integrated Gradients,[0],[0]
"In contrast, the all-zero input vector for a text network does not correspond to a valid input; it nevertheless works for the mathematical reason described above.
",5. Applying Integrated Gradients,[0],[0]
Computing Integrated Gradients.,5. Applying Integrated Gradients,[0],[0]
The integral of integrated gradients can be efficiently approximated via a summation.,5. Applying Integrated Gradients,[0],[0]
"We simply sum the gradients at points occurring at sufficiently small intervals along the straightline path from the baseline x′ to the input x.
IntegratedGradsapproxi (x) ::=
(xi − x′i)× Σmk=1 ∂F (x′+ k",5. Applying Integrated Gradients,[0],[0]
m×(x−x ′))),5. Applying Integrated Gradients,[0],[0]
"∂xi × 1m
(3)
",5. Applying Integrated Gradients,[0],[0]
Here m is the number of steps in the Riemman approximation of the integral.,5. Applying Integrated Gradients,[0],[0]
Notice that the approximation simply involves computing the gradient in a for loop which should be straightforward and efficient in most deep learning frameworks.,5. Applying Integrated Gradients,[0],[0]
"For instance, in TensorFlow, it amounts to calling tf.gradients in a loop over the set of inputs (i.e., x′ + km × (x − x ′) for k = 1, . . .",5. Applying Integrated Gradients,[0],[0]
",m), which
could also be batched.",5. Applying Integrated Gradients,[0],[0]
"In practice, we find that somewhere between 20 and 300 steps are enough to approximate the integral (within 5%); we recommend that developers check that the attributions approximately adds up to the difference beween the score at the input and that at the baseline (cf. Proposition 1), and if not increase the step-size m.",5. Applying Integrated Gradients,[0],[0]
The integrated gradients technique is applicable to a variety of deep networks.,6. Applications,[0],[0]
"Here, we apply it to two image models, two natural language models, and a chemistry model.",6. Applications,[0],[0]
"We study feature attribution in an object recognition network built using the GoogleNet architecture (Szegedy et al., 2014) and trained over the ImageNet object recognition dataset (Russakovsky et al., 2015).",6.1. An Object Recognition Network,[0],[0]
We use the integrated gradients method to study pixel importance in predictions made by this network.,6.1. An Object Recognition Network,[0],[0]
The gradients are computed for the output of the highest-scoring class with respect to pixel of the input image.,6.1. An Object Recognition Network,[0],[0]
"The baseline input is the black image, i.e., all pixel intensities are zero.
",6.1. An Object Recognition Network,[0],[0]
Integrated gradients can be visualized by aggregating them along the color channel and scaling the pixels in the actual image by them.,6.1. An Object Recognition Network,[0],[0]
Figure 2 shows visualizations for a bunch of images2.,6.1. An Object Recognition Network,[0],[0]
"For comparison, it also presents the corresponding visualization obtained from the product of the image with the gradients at the actual image.",6.1. An Object Recognition Network,[0],[0]
Notice that integrated gradients are better at reflecting distinctive features of the input image.,6.1. An Object Recognition Network,[0],[0]
Diabetic retinopathy (DR) is a complication of the diabetes that affects the eyes.,6.2. Diabetic Retinopathy Prediction,[0],[0]
"Recently, a deep network (Gulshan et al., 2016) has been proposed to predict the severity grade for DR in retinal fundus images.",6.2. Diabetic Retinopathy Prediction,[0],[0]
"The model has good predictive accuracy on various validation datasets.
",6.2. Diabetic Retinopathy Prediction,[0],[0]
"We use integrated gradients to study feature importance for this network; like in the object recognition case, the baseline is the black image.",6.2. Diabetic Retinopathy Prediction,[0],[0]
"Feature importance explanations are important for this network as retina specialists may use it to build trust in the network’s predictions, decide the grade for borderline cases, and obtain insights for further testing and screening.
",6.2. Diabetic Retinopathy Prediction,[0],[0]
Figure 3 shows a visualization of integrated gradients for a retinal fundus image.,6.2. Diabetic Retinopathy Prediction,[0],[0]
The visualization method is a bit different from that used in Figure 2.,6.2. Diabetic Retinopathy Prediction,[0],[0]
"We aggregate integrated gradients along the color channel and overlay them on the
2More examples can be found at https://github.com/ ankurtaly/Attributions
actual image in gray scale with positive attribtutions along the green channel and negative attributions along the red channel.",6.2. Diabetic Retinopathy Prediction,[0],[0]
Notice that integrated gradients are localized to a few pixels that seem to be lesions in the retina.,6.2. Diabetic Retinopathy Prediction,[0],[0]
The interior of the lesions receive a negative attribution while the periphery receives a positive attribution indicating that the network focusses on the boundary of the lesion.,6.2. Diabetic Retinopathy Prediction,[0],[0]
Automatically answering natural language questions (over semi-structured data) is an important problem in artificial intelligence (AI).,6.3. Question Classification,[0],[0]
"A common approach is to semantically parse the question to its logical form (Liang, 2016) using a set of human-authored grammar rules.",6.3. Question Classification,[0],[0]
An alternative approach is to machine learn an end-to-end model provided there is enough training data.,6.3. Question Classification,[0],[0]
An interesting question is whether one could peek inside machine learnt models to derive new rules.,6.3. Question Classification,[0],[0]
"We explore this direction for a sub-problem of semantic parsing, called question classification, using the method of integrated gradients.
",6.3. Question Classification,[0],[0]
The goal of question classification is to identify the type of answer it is seeking.,6.3. Question Classification,[0],[0]
"For instance, is the quesiton seeking a yes/no answer, or is it seeking a date?",6.3. Question Classification,[0],[0]
"Rules for solving this problem look for trigger phrases in the question, for e.g., a “when” in the beginning indicates a date seeking question.",6.3. Question Classification,[0],[0]
"We train a model for question classification using the the text categorization architecture proposed by (Kim, 2014) over the WikiTableQuestions dataset (Pasupat & Liang, 2015).",6.3. Question Classification,[0],[0]
We use integrated gradients to attribute predictions down to the question terms in order to identify new trigger phrases for answer type.,6.3. Question Classification,[0],[0]
"The baseline input is the all zero embedding vector.
",6.3. Question Classification,[0],[0]
Figure 4 lists a few questions with constituent terms highlighted based on their attribution.,6.3. Question Classification,[0],[0]
"Notice that the attributions largely agree with commonly used rules, for e.g., “how many” indicates a numeric seeking question.",6.3. Question Classification,[0],[0]
"In addition, attributions help identify novel question classification rules, for e.g., questions containing “total number” are seeking numeric answers.",6.3. Question Classification,[0],[0]
"Attributions also point out undesirable correlations, for e.g., “charles” is used as trigger for a yes/no question.",6.3. Question Classification,[0],[0]
"We applied our technique to a complex, LSTM-based Neural Machine Translation System (Wu et al., 2016).",6.4. Neural Machine Translation,[0],[0]
We attribute the output probability of every output token (in form of wordpieces) to the input tokens.,6.4. Neural Machine Translation,[0],[0]
Such attributions “align” the output sentence with the input sentence.,6.4. Neural Machine Translation,[0],[0]
"For
baseline, we zero out the embeddings of all tokens except the start and end markers.",6.4. Neural Machine Translation,[0],[0]
Figure 5 shows an example of such an attribution-based alignments.,6.4. Neural Machine Translation,[0],[0]
We observed that the results make intuitive sense.,6.4. Neural Machine Translation,[0],[0]
"E.g. “und” is mostly attributed to “and”, and “morgen” is mostly attributed to “morning”.",6.4. Neural Machine Translation,[0],[0]
We use 100 − 1000 steps (cf. Section 5) in the integrated gradient approximation; we need this because the network is highly nonlinear.,6.4. Neural Machine Translation,[0],[0]
"We apply integrated gradients to a network performing Ligand-Based Virtual Screening which is the problem of predicting whether an input molecule is active against a certain target (e.g., protein or enzyme).",6.5. Chemistry Models,[0],[0]
"In particular, we consider a network based on the molecular graph convolution architecture proposed by (Kearnes et al., 2016).
",6.5. Chemistry Models,[0],[0]
The network requires an input molecule to be encoded by hand as a set of atom and atom-pair features describing the molecule as an undirected graph.,6.5. Chemistry Models,[0],[0]
"Atoms are featurized using a one-hot encoding specifying the atom type (e.g., C, O, S, etc.), and atom-pairs are featurized by specifying either the type of bond (e.g., single, double, triple, etc.) between the atoms, or the graph distance between them.",6.5. Chemistry Models,[0],[0]
"The baseline input is obtained zeroing out the feature vectors for atom and atom-pairs.
",6.5. Chemistry Models,[0],[0]
We visualize integrated gradients as heatmaps over the the atom and atom-pair features with the heatmap intensity depicting the strength of the contribution.,6.5. Chemistry Models,[0],[0]
Figure 6 shows the visualization for a specific molecule.,6.5. Chemistry Models,[0],[0]
"Since integrated gradients add up to the final prediction score (see Proposition 1), the magnitudes can be use for accounting the contributions of each feature.",6.5. Chemistry Models,[0],[0]
"For instance, for the molecule in the figure, atom-pairs that have a bond between them cumulatively contribute to 46% of the prediction score, while all other pairs cumulatively contribute to only −3%.
",6.5. Chemistry Models,[0],[0]
Identifying Degenerate Features.,6.5. Chemistry Models,[0],[0]
"We now discuss how attributions helped us spot an anomaly in the W1N2 architecture in (Kearnes et al., 2016).",6.5. Chemistry Models,[0],[0]
"On applying the integrated gradients method to this network, we found that several atoms in the same molecule received identical attribution despite being bonded to different atoms.",6.5. Chemistry Models,[0],[0]
"This is surprising as one would expect two atoms with different neighborhoods to be treated differently by the network.
",6.5. Chemistry Models,[0],[0]
"On investigating the problem further, in the network architecture, the atoms and atom-pair features were not fully convolved.",6.5. Chemistry Models,[0],[0]
"This caused all atoms that have the same atom type, and same number of bonds of each type to contribute identically to the network.",6.5. Chemistry Models,[0],[0]
We already covered closely related work on attribution in Section 2.,7. Other Related work,[0],[0]
We mention other related work.,7. Other Related work,[0],[0]
"Over the last few years, there has been a vast amount work on demystifying the inner workings of deep networks.",7. Other Related work,[0],[0]
"Most of this work has been on networks trained on computer vision tasks, and deals with understanding what a specific neuron computes (Erhan et al., 2009; Le, 2013) and interpreting the representations captured by neurons during a prediction (Mahendran & Vedaldi, 2015; Dosovitskiy & Brox, 2015; Yosinski et al., 2015).",7. Other Related work,[0],[0]
"In contrast, we focus on understanding the network’s behavior on a specific input in terms of the base level input features.",7. Other Related work,[0],[0]
"Our technique quantifies the importance of each feature in the prediction.
",7. Other Related work,[0],[0]
"One approach to the attribution problem proposed first by (Ribeiro et al., 2016a;b), is to locally approximate the behavior of the network in the vicinity of the input being explained with a simpler, more interpretable model.",7. Other Related work,[0],[0]
An appealing aspect of this approach is that it is completely agnostic to the implementation of the network and satisfies implemenation invariance.,7. Other Related work,[0],[0]
"However, this approach does not guarantee sensitivity.",7. Other Related work,[0],[0]
"There is no guarantee that the local region explored escapes the “flat” section of the pre-
diction function in the sense of Section 2.",7. Other Related work,[0],[0]
The other issue is that the method is expensive to implement for networks with “dense” input like image networks as one needs to explore a local region of size proportional to the number of pixels and train a model for this space.,7. Other Related work,[0],[0]
"In contrast, our technique works with a few calls to the gradient operation.
",7. Other Related work,[0],[0]
"Attention mechanisms (Bahdanau et al., 2014) have gained popularity recently.",7. Other Related work,[0],[0]
"One may think that attention could be used a proxy for attributions, but this has issues.",7. Other Related work,[0],[0]
"For instance, in a LSTM that also employs attention, there are many ways for an input token to influence an output token: the memory cell, the recurrent state, and “attention”.",7. Other Related work,[0],[0]
Focussing only an attention ignores the other modes of influence and results in an incomplete picture.,7. Other Related work,[0],[0]
The primary contribution of this paper is a method called integrated gradients that attributes the prediction of a deep network to its inputs.,8. Conclusion,[0],[0]
"It can be implemented using a few calls to the gradients operator, can be applied to a variety of deep networks, and has a strong theoretical justification.
",8. Conclusion,[0],[0]
A secondary contribution of this paper is to clarify desirable features of an attribution method using an axiomatic framework inspired by cost-sharing literature from economics.,8. Conclusion,[0],[0]
"Without the axiomatic approach it is hard to tell whether the attribution method is affected by data artifacts, network’s artifacts or artifacts of the method.",8. Conclusion,[0],[0]
"The axiomatic approach rules out artifacts of the last type.
",8. Conclusion,[0],[0]
"While our and other works have made some progress on understanding the relative importance of input features in a deep network, we have not addressed the interactions between the input features or the logic employed by the network.",8. Conclusion,[0],[0]
So there remain many unanswered questions in terms of debugging the I/O behavior of a deep network.,8. Conclusion,[0],[0]
"We would like to thank Samy Bengio, Kedar Dhamdhere, Scott Lundberg, Amir Najmi, Kevin McCurley, Patrick Riley, Christian Szegedy, Diane Tang for their feedback.",ACKNOWLEDGMENTS,[0],[0]
We would like to thank Daniel Smilkov and Federico Allocati for identifying bugs in our descriptions.,ACKNOWLEDGMENTS,[0],[0]
"We would like to thank our anonymous reviewers for identifying bugs, and their suggestions to improve presentation.",ACKNOWLEDGMENTS,[0],[0]
Proof.,A. Proof of Theorem 1,[0],[0]
Consider a non-straightline path γ :,A. Proof of Theorem 1,[0],[0]
"[0, 1] → Rn from baseline to input.",A. Proof of Theorem 1,[0],[0]
"W.l.o.g., there exists t0 ∈",A. Proof of Theorem 1,[0],[0]
"[0, 1] such that for two dimensions i, j, γi(t0) > γj(t0).",A. Proof of Theorem 1,[0],[0]
"Let (t1, t2) be the maximum real open interval containing t0 such that γi(t) > γj(t) for all t in (t1, t2), and let a = γi(t1) = γj(t1), and b = γi(t2) = γj(t2).",A. Proof of Theorem 1,[0],[0]
Define function f,A. Proof of Theorem 1,[0],[0]
: x ∈,A. Proof of Theorem 1,[0],[0]
"[0, 1]n → R as 0 if min(xi, xj) ≤ a, as (b − a)2 if max(xi, xj) ≥ b, and as (xi − a)(xj − a) otherwise.",A. Proof of Theorem 1,[0],[0]
"Next we compute the attributions of f at x = 〈1, . . .",A. Proof of Theorem 1,[0],[0]
", 1〉n with baseline x′ = 〈0, . . .",A. Proof of Theorem 1,[0],[0]
", 0〉n.",A. Proof of Theorem 1,[0],[0]
"Note that xi and xj are symmetric, and should get identical attributions.",A. Proof of Theorem 1,[0],[0]
For t /∈,A. Proof of Theorem 1,[0],[0]
"[t1, t2], the function is a constant, and the attribution of f is zero to all variables, while for t ∈ (t1, t2), the integrand of attribution of f is γj(t)",A. Proof of Theorem 1,[0],[0]
"− a to xi, and γi(t)",A. Proof of Theorem 1,[0],[0]
"− a to xj , where the latter is always strictly larger by our choice of the interval.",A. Proof of Theorem 1,[0],[0]
"Integrating, it follows that xj gets a larger attribution than xi, contradiction.",A. Proof of Theorem 1,[0],[0]
"We show that the methods DeepLift and Layer-wise relevance propagation (LRP) break the implementation invariance axiom, and the Deconvolution and Guided backpropagation methods break the sensitivity axiom.
",B. Attribution Counter-Examples,[0],[0]
"Figure 7 provides an example of two equivalent networks
f(x1, x2) and g(x1, x2) for which DeepLift and LRP yield different attributions.
",B. Attribution Counter-Examples,[0],[0]
"First, observe that the networks f and g are of the form f(x1, x2) =",B. Attribution Counter-Examples,[0],[0]
"ReLU(h(x1, x2)) and f(x1, x2) = ReLU(k(x1, x2))",B. Attribution Counter-Examples,[0],[0]
"3, where
h(x1, x2) =",B. Attribution Counter-Examples,[0],[0]
"ReLU(x1)− 1− ReLU(x2) k(x1, x2)",B. Attribution Counter-Examples,[0],[0]
"= ReLU(x1 − 1)− ReLU(x2)
",B. Attribution Counter-Examples,[0],[0]
Note that h and k are not equivalent.,B. Attribution Counter-Examples,[0],[0]
They have different values whenever x1 < 1.,B. Attribution Counter-Examples,[0],[0]
But f and g are equivalent.,B. Attribution Counter-Examples,[0],[0]
"To prove this, suppose for contradiction that f and g are different for some x1, x2.",B. Attribution Counter-Examples,[0],[0]
Then it must be the case that ReLU(x1)− 1 6= ReLU(x1 − 1).,B. Attribution Counter-Examples,[0],[0]
"This happens only when x1 < 1, which implies that f(x1, x2) = g(x1, x2) = 0.
",B. Attribution Counter-Examples,[0],[0]
Now we leverage the above example to show that Deconvolution and Guided back-propagation break sensitivity.,B. Attribution Counter-Examples,[0],[0]
"Consider the network f(x1, x2) from Figure 7.",B. Attribution Counter-Examples,[0],[0]
"For a fixed value of x1 greater than 1, the output decreases linearly as x2 increases from 0 to x1 − 1.",B. Attribution Counter-Examples,[0],[0]
"Yet, for all inputs, Deconvolutional networks and Guided back-propagation results in zero attribution for x2.",B. Attribution Counter-Examples,[0],[0]
"This happens because for all inputs the back-propagated signal received at the node ReLU(x2) is negative and is therefore not back-propagated through the ReLU operation (per the rules of deconvolution and guided back-propagation; see (Springenberg et al., 2014) for details).",B. Attribution Counter-Examples,[0],[0]
"As a result, the feature x2 receives zero
3 ReLU(x) is defined as max(x, 0).
",B. Attribution Counter-Examples,[0],[0]
attribution despite the network’s output being sensitive to it.,B. Attribution Counter-Examples,[0],[0]
"We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works.",abstractText,[0],[0]
We identify two fundamental axioms— Sensitivity and Implementation Invariance that attribution methods ought to satisfy.,abstractText,[0],[0]
"We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods.",abstractText,[0],[0]
We use the axioms to guide the design of a new attribution method called Integrated Gradients.,abstractText,[0],[0]
Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator.,abstractText,[0],[0]
"We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.",abstractText,[0],[0]
1.,abstractText,[0],[0]
Motivation and Summary of Results We study the problem of attributing the prediction of a deep network to its input features.,abstractText,[0],[0]
Definition 1.,abstractText,[0],[0]
"Formally, suppose we have a function F :",abstractText,[0],[0]
R,abstractText,[0],[0]
→,abstractText,[0],[0]
"[0, 1] that represents a deep network, and an input x =",abstractText,[0],[0]
"(x1, . . .",abstractText,[0],[0]
", xn) ∈",abstractText,[0],[0]
R.,abstractText,[0],[0]
"An attribution of the prediction at input x relative to a baseline input x′ is a vector AF (x, x ′) =",abstractText,[0],[0]
"(a1, . . .",abstractText,[0],[0]
", an) ∈ R where ai is the contribution of xi to the prediction F (x).",abstractText,[0],[0]
"For instance, in an object recognition network, an attribution method could tell us which pixels of the image were responsible for a certain label being picked (see Figure 2).",abstractText,[0],[0]
"The attribution problem was previously studied by various papers (Baehrens et al., 2010; Simonyan et al., 2013; Equal contribution Google Inc., Mountain View, USA.",abstractText,[0],[0]
Correspondence to: Mukund Sundararajan <mukunds@google.com,abstractText,[0],[0]
">, Ankur Taly <ataly@google.com>.",abstractText,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",abstractText,[0],[0]
Copyright 2017 by the author(s).,abstractText,[0],[0]
"Shrikumar et al., 2016; Binder et al., 2016; Springenberg et al., 2014).",abstractText,[0],[0]
"The intention of these works is to understand the inputoutput behavior of the deep network, which gives us the ability to improve it.",abstractText,[0],[0]
"Such understandability is critical to all computer programs, including machine learning models.",abstractText,[0],[0]
There are also other applications of attribution.,abstractText,[0],[0]
They could be used within a product driven by machine learning to provide a rationale for the recommendation.,abstractText,[0],[0]
"For instance, a deep network that predicts a condition based on imaging could help inform the doctor of the part of the image that resulted in the recommendation.",abstractText,[0],[0]
This could help the doctor understand the strengths and weaknesses of a model and compensate for it.,abstractText,[0],[0]
We give such an example in Section 6.2.,abstractText,[0],[0]
Attributions could also be used by developers in an exploratory sense.,abstractText,[0],[0]
"For instance, we could use a deep network to extract insights that could be then used in a rulebased system.",abstractText,[0],[0]
"In Section 6.3, we give such an example.",abstractText,[0],[0]
A significant challenge in designing an attribution technique is that they are hard to evaluate empirically.,abstractText,[0],[0]
"As we discuss in Section 4, it is hard to tease apart errors that stem from the misbehavior of the model versus the misbehavior of the attribution method.",abstractText,[0],[0]
"To compensate for this shortcoming, we take an axiomatic approach.",abstractText,[0],[0]
In Section 2 we identify two axioms that every attribution method must satisfy.,abstractText,[0],[0]
Unfortunately most previous methods do not satisfy one of these two axioms.,abstractText,[0],[0]
"In Section 3, we use the axioms to identify a new method, called integrated gradients.",abstractText,[0],[0]
"Unlike previously proposed methods, integrated gradients do not need any instrumentation of the network, and can be computed easily using a few calls to the gradient operation, allowing even novice practitioners to easily apply the technique.",abstractText,[0],[0]
"In Section 6, we demonstrate the ease of applicability over several deep networks, including two images networks, two text processing networks, and a chemistry network.",abstractText,[0],[0]
"These applications demonstrate the use of our technique in either improving our understanding of the network, performing debugging, performing rule extraction, or aiding an end user in understanding the network’s prediction.",abstractText,[0],[0]
Remark 1.,abstractText,[0],[0]
Let us briefly examine the need for the baseline in the definition of the attribution problem.,abstractText,[0],[0]
A common way for humans to perform attribution relies on counterar X iv :1 70 3.,abstractText,[0],[0]
01,abstractText,[0],[0]
36 5v 2,abstractText,[0],[0]
[ cs .L G ] 1 3 Ju n 20 17 Axiomatic Attribution for Deep Networks factual intuition.,abstractText,[0],[0]
When we assign blame to a certain cause we implicitly consider the absence of the cause as a baseline for comparing outcomes.,abstractText,[0],[0]
"In a deep network, we model the absence using a single baseline input.",abstractText,[0],[0]
"For most deep networks, a natural baseline exists in the input space where the prediction is neutral.",abstractText,[0],[0]
"For instance, in object recognition networks, it is the black image.",abstractText,[0],[0]
"The need for a baseline has also been pointed out by prior work on attribution (Shrikumar et al., 2016; Binder et al., 2016).",abstractText,[0],[0]
2.,abstractText,[0],[0]
Two Fundamental Axioms We now discuss two axioms (desirable characteristics) for attribution methods.,abstractText,[0],[0]
We find that other feature attribution methods in literature break at least one of the two axioms.,abstractText,[0],[0]
"These methods include DeepLift (Shrikumar et al., 2016; 2017), Layer-wise relevance propagation (LRP) (Binder et al., 2016), Deconvolutional networks (Zeiler & Fergus, 2014), and Guided back-propagation (Springenberg et al., 2014).",abstractText,[0],[0]
"As we will see in Section 3, these axioms will also guide the design of our method.",abstractText,[0],[0]
Gradients.,abstractText,[0],[0]
"For linear models, ML practitioners regularly inspect the products of the model coefficients and the feature values in order to debug predictions.",abstractText,[0],[0]
"Gradients (of the output with respect to the input) is a natural analog of the model coefficients for a deep network, and therefore the product of the gradient and feature values is a reasonable starting point for an attribution method (Baehrens et al., 2010; Simonyan et al., 2013); see the third column of Figure 2 for examples.",abstractText,[0],[0]
"The problem with gradients is that they break sensitivity, a property that all attribution methods should satisfy.",abstractText,[0],[0]
2.1.,abstractText,[0],[0]
Axiom: Sensitivity(a),abstractText,[0],[0]
An attribution method satisfies Sensitivity(a),abstractText,[0],[0]
if for every input and baseline that differ in one feature but have different predictions then the differing feature should be given a non-zero attribution.,abstractText,[0],[0]
"(Later in the paper, we will have a part (b) to this definition.)",abstractText,[0],[0]
Gradients violate Sensitivity(a):,abstractText,[0],[0]
"For a concrete example, consider a one variable, one ReLU network, f(x) = 1 − ReLU(1−x).",abstractText,[0],[0]
Suppose the baseline is x = 0 and the input is x = 2.,abstractText,[0],[0]
"The function changes from 0 to 1, but because f becomes flat at x = 1, the gradient method gives attribution of 0 to x. Intuitively, gradients break Sensitivity because the prediction function may flatten at the input and thus have zero gradient despite the function value at the input being different from that at the baseline.",abstractText,[0],[0]
"This phenomenon has been reported in previous work (Shrikumar et al., 2016).",abstractText,[0],[0]
"Practically, the lack of sensitivity causes gradients to focus on irrelevant features (see the “fireboat” example in Figure 2).",abstractText,[0],[0]
Other back-propagation based approaches.,abstractText,[0],[0]
A second set of approaches involve back-propagating the final prediction score through each layer of the network down to the individual features.,abstractText,[0],[0]
"These include DeepLift, Layer-wise relevance propagation (LRP), Deconvolutional networks (DeConvNets), and Guided back-propagation.",abstractText,[0],[0]
"These methods differ in the specific backpropagation logic for various activation functions (e.g., ReLU, MaxPool, etc.).",abstractText,[0],[0]
"Unfortunately, Deconvolution networks (DeConvNets), and Guided back-propagation violate Sensitivity(a).",abstractText,[0],[0]
This is because these methods back-propogate through a ReLU node only if the ReLU is turned on at the input.,abstractText,[0],[0]
"This makes the method similar to gradients, in that, the attribution is zero for features with zero gradient at the input despite a non-zero gradient at the baseline.",abstractText,[0],[0]
"We defer the specific counterexamples to Appendix B. Methods like DeepLift and LRP tackle the Sensitivity issue by employing a baseline, and in some sense try to compute “discrete gradients” instead of (instantaeneous) gradients at the input.",abstractText,[0],[0]
(The two methods differ in the specifics of how they compute the discrete gradient).,abstractText,[0],[0]
"But the idea is that a large, discrete step will avoid flat regions, avoiding a breakage of sensitivity.",abstractText,[0],[0]
"Unfortunately, these methods violate a different requirement on attribution methods.",abstractText,[0],[0]
2.2.,abstractText,[0],[0]
"Axiom: Implementation Invariance Two networks are functionally equivalent if their outputs are equal for all inputs, despite having very different implementations.",abstractText,[0],[0]
"Attribution methods should satisfy Implementation Invariance, i.e., the attributions are always identical for two functionally equivalent networks.",abstractText,[0],[0]
"To motivate this, notice that attribution can be colloquially defined as assigning the blame (or credit) for the output to the input features.",abstractText,[0],[0]
Such a definition does not refer to implementation details.,abstractText,[0],[0]
"We now discuss intuition for why DeepLift and LRP break Implementation Invariance; a concrete example is provided in Appendix B. First, notice that gradients are invariant to implementation.",abstractText,[0],[0]
"In fact, the chain-rule for gradients ∂f ∂g = ∂f ∂h · ∂h ∂g is essentially about implementation invariance.",abstractText,[0],[0]
"To see this, think of g and f as the input and output of a system, and h being some implementation detail of the system.",abstractText,[0],[0]
"The gradient of output f to input g can be computed either directly by ∂f ∂g , ignoring the intermediate function h (implementation detail), or by invoking the chain rule via h.",abstractText,[0],[0]
This is exactly how backpropagation works.,abstractText,[0],[0]
Methods like LRP and DeepLift replace gradients with discrete gradients and still use a modified form of backpropagation to compose discrete gradients into attributions.,abstractText,[0],[0]
"UnAxiomatic Attribution for Deep Networks fortunately, the chain rule does not hold for discrete gradients in general.",abstractText,[0],[0]
Formally f(x1)−f(x0) g(x1)−g(x0) 6=,abstractText,[0],[0]
"f(x1)−f(x0) h(x1)−h(x0) · h(x1)−h(x0) g(x1)−g(x0) , and therefore these methods fail to satisfy implementation invariance.",abstractText,[0],[0]
"If an attribution method fails to satisfy Implementation Invariance, the attributions are potentially sensitive to unimportant aspects of the models.",abstractText,[0],[0]
"For instance, if the network architecture has more degrees of freedom than needed to represent a function then there may be two sets of values for the network parameters that lead to the same function.",abstractText,[0],[0]
"The training procedure can converge at either set of values depending on the initializtion or for other reasons, but the underlying network function would remain the same.",abstractText,[0],[0]
It is undesirable that attributions differ for such reasons.,abstractText,[0],[0]
3.,abstractText,[0],[0]
Our Method: Integrated Gradients We are now ready to describe our technique.,abstractText,[0],[0]
"Intuitively, our technique combines the Implementation Invariance of Gradients along with the Sensitivity of techniques like LRP or DeepLift.",abstractText,[0],[0]
"Formally, suppose we have a function F : R",abstractText,[0],[0]
→,abstractText,[0],[0]
"[0, 1] that represents a deep network.",abstractText,[0],[0]
"Specifically, let x ∈ R be the input at hand, and x′ ∈ R be the baseline input.",abstractText,[0],[0]
"For image networks, the baseline could be the black image, while for text models it could be the zero embedding vector.",abstractText,[0],[0]
"We consider the straightline path (in R) from the baseline x′ to the input x, and compute the gradients at all points along the path.",abstractText,[0],[0]
Integrated gradients are obtained by cumulating these gradients.,abstractText,[0],[0]
"Specifically, integrated gradients are defined as the path intergral of the gradients along the straightline path from the baseline x′ to the input x.",abstractText,[0],[0]
The integrated gradient along the i dimension for an input x and baseline x′ is defined as follows.,abstractText,[0],[0]
"Here, ∂F (x) ∂xi is the gradient of F (x) along the i dimension.",abstractText,[0],[0]
IntegratedGradsi(x) ::= (xi−x ′,abstractText,[0],[0]
i)× ∫ 1 α=0 ∂F (x′+α×(x−x′)),abstractText,[0],[0]
Axiomatic Attribution for Deep Networks,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1863–1873 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1863
We experiment on two structured NLP pipelines: syntactic-then-semantic dependency parsing, and semantic parsing followed by sentiment classification. We show that training with SPIGOT leads to a larger improvement on the downstream task than a modularly-trained pipeline, the straight-through estimator, and structured attention, reaching a new state of the art on semantic dependency parsing.",text,[0],[0]
Learning methods for natural language processing are increasingly dominated by end-to-end differentiable functions that can be trained using gradient-based optimization.,1 Introduction,[0],[0]
"Yet traditional NLP often assumed modular stages of processing that formed a pipeline; e.g., text was tokenized, then tagged with parts of speech, then parsed into a
phrase-structure or dependency tree, then semantically analyzed.",1 Introduction,[0],[0]
"Pipelines, which make “hard” (i.e., discrete) decisions at each stage, appear to be incompatible with neural learning, leading many researchers to abandon earlier-stage processing.
",1 Introduction,[0],[0]
"Inspired by findings that continue to see benefit from various kinds of linguistic or domain-specific preprocessing (He et al., 2017; Oepen et al., 2017; Ji and Smith, 2017), we argue that pipelines can be treated as layers in neural architectures for NLP tasks.",1 Introduction,[0],[0]
"Several solutions are readily available: • Reinforcement learning (most notably the
REINFORCE algorithm; Williams, 1992), and structured attention (SA; Kim et al., 2017).",1 Introduction,[0],[0]
These methods replace argmax with a sampling or marginalization operation.,1 Introduction,[0],[0]
"We note two potential downsides of these approaches: (i) not all argmax-able operations have corresponding sampling or marginalization methods that are efficient, and (ii) inspection of intermediate outputs, which could benefit error analysis and system improvement, is more straightforward for hard decisions than for posteriors.",1 Introduction,[0],[0]
"• The straight-through estimator (STE; Hin-
ton, 2012) treats discrete decisions as if they were differentiable and simply passes through gradients.",1 Introduction,[0],[0]
"While fast and surprisingly effective, it ignores constraints on the argmax problem, such as the requirement that every word has exactly one syntactic parent.",1 Introduction,[0],[0]
"We will find, experimentally, that the quality of intermediate representations degrades substantially under STE.
",1 Introduction,[0],[0]
"This paper introduces a new method, the structured projection of intermediate gradients optimization technique (SPIGOT; §2), which defines a proxy for the gradient of a loss function with respect to the input to argmax.",1 Introduction,[0],[0]
"Unlike STE’s gradient proxy, SPIGOT aims to respect the constraints
in the argmax problem.",1 Introduction,[0],[0]
"SPIGOT can be applied with any intermediate layer that is expressible as a constrained maximization problem, and whose feasible set can be projected onto.",1 Introduction,[0],[0]
"We show empirically that SPIGOT works even when the maximization and the projection are done approximately.
",1 Introduction,[0],[0]
"We offer two concrete architectures that employ structured argmax as an intermediate layer: semantic parsing with syntactic parsing in the middle, and sentiment analysis with semantic parsing in the middle (§3).",1 Introduction,[0],[0]
"These architectures are trained using a joint objective, with one part using data for the intermediate task, and the other using data for the end task.",1 Introduction,[0],[0]
"The datasets are not assumed to overlap at all, but the parameters for the intermediate task are affected by both parts of the training data.
",1 Introduction,[0],[0]
"Our experiments (§4) show that our architecture improves over a state-of-the-art semantic dependency parser, and that SPIGOT offers stronger performance than a pipeline, SA, and STE.",1 Introduction,[0],[0]
"On sentiment classification, we show that semantic parsing offers improvement over a BiLSTM, more so with SPIGOT than with alternatives.",1 Introduction,[0],[0]
Our analysis considers how the behavior of the intermediate parser is affected by the end task (§5).,1 Introduction,[0],[0]
Our code is open-source and available at https:// github.com/Noahs-ARK/SPIGOT.,1 Introduction,[0],[0]
Our aim is to allow a (structured) argmax layer in a neural network to be treated almost like any other differentiable function.,2 Method,[0],[0]
"This would allow us to place, for example, a syntactic parser in the middle of a neural network, so that the forward calculation simply calls the parser and passes the parse tree to the next layer, which might derive syntactic features for the next stage of processing.
",2 Method,[0],[0]
"The challenge is in the backward computation, which is key to learning with standard gradientbased methods.",2 Method,[0],[0]
"When its output is discrete as we assume here, argmax is a piecewise constant function.",2 Method,[0],[0]
"At every point, its gradient is either zero or undefined.",2 Method,[0],[0]
"So instead of using the true gradient, we will introduce a proxy for the gradient of the loss function with respect to the inputs to argmax, allowing backpropagation to proceed through the argmax layer.",2 Method,[0],[0]
Our proxy is designed as an improvement to earlier methods (discussed below) that completely ignore constraints on the argmax operation.,2 Method,[0],[0]
"It accomplishes this through a projec-
tion of the gradients.",2 Method,[0],[0]
"We first lay out notation, and then briefly review max-decoding and its relaxation (§2.1).",2 Method,[0],[0]
"We define SPIGOT in §2.2, and show how to use it to backpropagate through NLP pipelines in §2.3.
Notation.",2 Method,[0],[0]
"Our discussion centers around two tasks: a structured intermediate task followed by an end task, where the latter considers the outputs of the former (e.g., syntactic-then-semantic parsing).",2 Method,[0],[0]
"Inputs are denoted as x, and end task outputs as y. We use z to denote intermediate structures derived from x. We will often refer to the intermediate task as “decoding”, in the structured prediction sense.",2 Method,[0],[0]
"It seeks an output ẑ = argmaxz∈Z S from the feasible set Z , maximizing a (learned, parameterized) scoring function S for the structured intermediate task.",2 Method,[0],[0]
"L denotes the loss of the end task, which may or may not also involve structured predictions.",2 Method,[0],[0]
We use ∆k−1 = {p ∈,2 Method,[0],[0]
"Rk | 1>p = 1,p ≥ 0} to denote the (k − 1)-dimensional simplex.",2 Method,[0],[0]
"We denote the domain of binary variables as B = {0, 1}, and the unit interval as U =",2 Method,[0],[0]
"[0, 1].",2 Method,[0],[0]
"By projection of a vector v onto a set A, we mean the closest point in A to v, measured by Euclidean distance: projA(v) = argminv′∈A ‖v′ − v‖2.",2 Method,[0],[0]
"Decoding problems are typically decomposed into a collection of “parts”, such as arcs in a dependency tree or graph.",2.1 Relaxed Decoding,[0],[0]
"In such a setup, each element of z, zi, corresponds to one possible part, and zi takes a boolean value to indicate whether the part is included in the output structure.",2.1 Relaxed Decoding,[0],[0]
"The scoring function S is assumed to decompose into a vector s(x) of part-local, input-specific scores:
ẑ = argmax z∈Z S(x, z) =",2.1 Relaxed Decoding,[0],[0]
"argmax z∈Z
z>s(x) (1)
In the following, we drop s’s dependence on x for clarity.
",2.1 Relaxed Decoding,[0],[0]
"In many NLP problems, the output space Z can be specified by linear constraints (Roth and Yih, 2004):
",2.1 Relaxed Decoding,[0],[0]
A [ z ψ ],2.1 Relaxed Decoding,[0],[0]
"≤ b, (2)
where ψ are auxiliary variables (also scoped by argmax), together with integer constraints (typically, each zi ∈ B).
",2.1 Relaxed Decoding,[0],[0]
"The problem in Equation 1 can be NP-complete in general, so the {0, 1} constraints are often relaxed to [0, 1] to make decoding tractable (Martins et al., 2009).",2.1 Relaxed Decoding,[0],[0]
"Then the discrete combinatorial problem over Z is transformed into the optimization of a linear objective over a convex polytope P={p ∈ Rd |Ap≤b}, which is solvable in polynomial time (Bertsimas and Tsitsiklis, 1997).",2.1 Relaxed Decoding,[0],[0]
"This is not necessary in some cases, where the argmax can be solved exactly with dynamic programming.",2.1 Relaxed Decoding,[0],[0]
"We now view structured argmax as an activation function that takes a vector of input-specific partscores s and outputs a solution ẑ. For backpropagation, to calculate gradients for parameters of s, the chain rule defines:
∇sL = J ∇ẑL, (3) where the Jacobian matrix J = ∂ẑ∂s contains the derivative of each element of ẑ with respect to each element of s. Unfortunately, argmax is a piecewise constant function, so its Jacobian is either zero (almost everywhere) or undefined (in the case of ties).
",2.2 From STE to SPIGOT,[0],[0]
"One solution, taken in structured attention, is to replace the argmax with marginal inference and a softmax function, so that ẑ encodes probability distributions over parts (Kim et al., 2017; Liu and Lapata, 2018).",2.2 From STE to SPIGOT,[0],[0]
"As discussed in §1, there are two reasons to avoid this modification.",2.2 From STE to SPIGOT,[0],[0]
"Softmax can only be used when marginal inference is feasible, by sum-product algorithms for example (Eisner, 2016; Friesen and Domingos, 2016); in general marginal inference can be #P-complete.",2.2 From STE to SPIGOT,[0],[0]
"Further, a soft intermediate layer will be less amenable to inspection by anyone wishing to understand and improve the model.
",2.2 From STE to SPIGOT,[0],[0]
"In another line of work, argmax is augmented with a strongly-convex penalty on the solutions (Martins and Astudillo, 2016; Amos and Kolter, 2017; Niculae and Blondel, 2017; Niculae et al., 2018; Mensch and Blondel, 2018).",2.2 From STE to SPIGOT,[0],[0]
"However, their approaches require solving a relaxation even when exact decoding is tractable.",2.2 From STE to SPIGOT,[0],[0]
"Also, the penalty will bias the solutions found by the decoder, which may be an undesirable conflation of computational and modeling concerns.
",2.2 From STE to SPIGOT,[0],[0]
"A simpler solution is the STE method (Hinton, 2012), which replaces the Jacobian matrix in Equation 3 by the identity matrix.",2.2 From STE to SPIGOT,[0],[0]
"This method has been demonstrated to work well when used to “backpropagate” through hard threshold functions (Bengio et al., 2013; Friesen and Domingos, 2018) and categorical random variables (Jang et al., 2016; Choi et al., 2017).
",2.2 From STE to SPIGOT,[0],[0]
"Consider for a moment what we would do if ẑ were a vector of parameters, rather than intermediate predictions.",2.2 From STE to SPIGOT,[0],[0]
"In this case, we are seeking points in Z that minimize L; denote that set of minimizers by Z∗. Given ∇ẑL and step size η, we would update ẑ to be ẑ",2.2 From STE to SPIGOT,[0],[0]
"− η∇ẑL. This update, however, might not return a value in the feasible set Z , or even (if we are using a linear relaxation) the relaxed set P .
",2.2 From STE to SPIGOT,[0],[0]
SPIGOT therefore introduces a projection step that aims to keep the “updated” ẑ in the feasible set.,2.2 From STE to SPIGOT,[0],[0]
"Of course, we do not directly update ẑ; we continue backpropagation through s and onward to the parameters.",2.2 From STE to SPIGOT,[0],[0]
"But the projection step nonetheless alters the parameter updates in the way that our proxy for “∇sL” is defined.
",2.2 From STE to SPIGOT,[0],[0]
"The procedure is defined as follows:
p̂ = ẑ− η∇ẑL, (4a)",2.2 From STE to SPIGOT,[0],[0]
"z̃ = projP(p̂), (4b) ∇sL , ẑ−",2.2 From STE to SPIGOT,[0],[0]
"z̃. (4c)
",2.2 From STE to SPIGOT,[0],[0]
"First, the method makes an “update” to ẑ as if it contained parameters (Equation 4a), letting p̂ denote the new value.",2.2 From STE to SPIGOT,[0],[0]
"Next, p̂ is projected back onto the (relaxed) feasible set (Equation 4b), yielding a feasible new value z̃.",2.2 From STE to SPIGOT,[0],[0]
"Finally, the gradients with respect to s are computed by Equation 4c.
",2.2 From STE to SPIGOT,[0],[0]
"Due to the convexity of P , the projected point z̃ will always be unique, and is guaranteed to be no farther than p̂ from any point in Z∗ (Luenberger and Ye, 2015).1 Compared to STE, SPIGOT in-
1Note that this property follows from P’s convexity, and we do not assume the convexity of L.
volves a projection and limits ∇sL to a smaller space to satisfy constraints.",2.2 From STE to SPIGOT,[0],[0]
"See Figure 1 for an illustration.
",2.2 From STE to SPIGOT,[0],[0]
"When efficient exact solutions (such as dynamic programming) are available, they can be used.",2.2 From STE to SPIGOT,[0],[0]
"Yet, we note that SPIGOT does not assume the argmax operation is solved exactly.",2.2 From STE to SPIGOT,[0],[0]
"Using SPIGOT, we now devise an algorithm to “backpropagate” through NLP pipelines.",2.3 Backpropagation through Pipelines,[0],[0]
"In these pipelines, an intermediate task’s output is fed into an end task for use as features.",2.3 Backpropagation through Pipelines,[0],[0]
"The parameters of the complete model are divided into two parts: denote the parameters of the intermediate task model byφ (used to calculate s), and those in the end task model as θ.2 As introduced earlier, the end-task loss function to be minimized is L, which depends on both φ and θ.
Algorithm 1 describes the forward and backward computations.",2.3 Backpropagation through Pipelines,[0],[0]
"It takes an end task training pair 〈x,y〉, along with the intermediate task’s feasible set Z , which is determined by x. It first runs the intermediate model and decodes to get intermediate structure ẑ, just as in a standard pipeline.",2.3 Backpropagation through Pipelines,[0],[0]
"Then forward propagation is continued into the end-task model to compute loss L, using ẑ to define input features.",2.3 Backpropagation through Pipelines,[0],[0]
"Backpropagation in the endtask model computes ∇θL and ∇ẑL, and ∇sL is then constructed using Equations 4.",2.3 Backpropagation through Pipelines,[0],[0]
"Backpropagation then continues into the intermediate model, computing∇φL.
Due to its flexibility, SPIGOT is applicable to many training scenarios.",2.3 Backpropagation through Pipelines,[0],[0]
"When there is no 〈x, z〉 training data for the intermediate task, SPIGOT can be used to induce latent structures for the end-task (Yogatama et al., 2017; Kim et al., 2017; Choi et al., 2017, inter alia).",2.3 Backpropagation through Pipelines,[0],[0]
"When intermediate-task training data is available, one can use SPIGOT to adopt joint learning by minimizing an interpolation of L (on end-task data 〈x,y〉) and an intermediate-task loss function L̃ (on intermediate task data 〈x, z〉).",2.3 Backpropagation through Pipelines,[0],[0]
This is the setting in our experiments; note that we do not assume any overlap in the training examples for the two tasks.,2.3 Backpropagation through Pipelines,[0],[0]
"In this section we discuss how to compute approximate projections for the two intermediate tasks
2Nothing prohibits tying across pre-argmax parameters and post-argmax parameters; this separation is notationally convenient but not at all necessary.
",3 Solving the Projections,[0],[0]
"Algorithm 1 Forward and backward computation with SPIGOT. 1: procedure SPIGOT(x,y,Z) 2: Construct A, b such that Z = {p ∈",3 Solving the Projections,[0],[0]
Zd | Ap ≤ b} 3: P ← {p ∈ Rd | Ap ≤ b} .,3 Solving the Projections,[0],[0]
Relaxation 4: Forwardprop and compute sφ(x) 5: ẑ← argmaxz∈Z z>sφ(x) .,3 Solving the Projections,[0],[0]
"Intermediate decoding 6: Forwardprop and compute L given x, y, and ẑ 7: Backprop and compute∇θL and∇ẑL 8: z̃← projP(ẑ− η∇ẑL) .",3 Solving the Projections,[0],[0]
"Projection 9: ∇sL← ẑ− z̃ 10: Backprop and compute∇φL 11: end procedure
considered in this work, arc-factored unlabeled dependency parsing and first-order semantic dependency parsing.
",3 Solving the Projections,[0],[0]
"In early experiments we observe that for both tasks, projecting with respect to all constraints of their original formulations using a generic quadratic program solver was prohibitively slow.",3 Solving the Projections,[0],[0]
"Therefore, we construct relaxed polytopes by considering only a subset of the constraints.3",3 Solving the Projections,[0],[0]
"The projection then decomposes into a series of singly constrained quadratic programs (QP), each of which can be efficiently solved in linear time.
",3 Solving the Projections,[0],[0]
The two approximate projections discussed here are used in backpropagation only.,3 Solving the Projections,[0],[0]
"In the forward pass, we solve the decoding problem using the models’ original decoding algorithms.
",3 Solving the Projections,[0],[0]
Arc-factored unlabeled dependency parsing.,3 Solving the Projections,[0],[0]
"For unlabeled dependency trees, we impose [0, 1] constraints and single-headedness constraints.4
Formally, given a length-n input sentence, excluding self-loops, an arc-factored parser considers d = n(n − 1) candidate arcs.",3 Solving the Projections,[0],[0]
"Let i→j denote an arc from the ith token to the jth, and σ(i→j) denote its index.",3 Solving the Projections,[0],[0]
"We construct the relaxed feasible set by:
PDEP = p ∈",3 Solving the Projections,[0],[0]
Ud ∣∣∣∣∣∣ ∑,3 Solving the Projections,[0],[0]
i 6=j pσ(i→j) =,3 Solving the Projections,[0],[0]
"1,∀j  , (5) i.e., we consider each token j individually, and force single-headedness by constraining the number of arcs incoming to j to sum to 1.",3 Solving the Projections,[0],[0]
"Algorithm 2 summarizes the procedure to project onto PDEP.
",3 Solving the Projections,[0],[0]
"3A parallel work introduces an active-set algorithm to solve the same class of quadratic programs (Niculae et al., 2018).",3 Solving the Projections,[0],[0]
"It might be an efficient approach to solve the projections in Equation 4b, which we leave to future work.
4",3 Solving the Projections,[0],[0]
"It requires O(n2) auxiliary variables and O(n3) additional constraints to ensure well-formed tree structures (Martins et al., 2013).
",3 Solving the Projections,[0],[0]
"Line 3 forms a singly constrained QP, and can be solved in O(n) time (Brucker, 1984).
",3 Solving the Projections,[0],[0]
Algorithm 2 Projection onto the relaxed polytope PDEP for dependency tree structures.,3 Solving the Projections,[0],[0]
"Let bold σ(·→j) denote the index set of arcs incoming to j. For a vector v, we use vσ(·→j) to denote vector [vk]k∈σ(·→j).
1: procedure DEPPROJ(p̂) 2: for j = 1, 2, . . .",3 Solving the Projections,[0],[0]
", n",3 Solving the Projections,[0],[0]
"do 3: z̃σ(·→j) ← proj∆n−2 ( p̂σ(·→j) ) 4: end for 5: return z̃ 6: end procedure
First-order semantic dependency parsing.",3 Solving the Projections,[0],[0]
"Semantic dependency parsing uses labeled bilexical dependencies to represent sentence-level semantics (Oepen et al., 2014, 2015, 2016).",3 Solving the Projections,[0],[0]
"Each dependency is represented by a labeled directed arc from a head token to a modifier token, where the arc label encodes broadly applicable semantic relations.",3 Solving the Projections,[0],[0]
"Figure 2 diagrams a semantic graph from the DELPH-IN MRS-derived dependencies (DM), together with a syntactic tree.
",3 Solving the Projections,[0],[0]
"We use a state-of-the-art semantic dependency parser (Peng et al., 2017) that considers three types of parts: heads, unlabeled arcs, and labeled arcs.",3 Solving the Projections,[0],[0]
Let σ(i `→ j) denote the index of the arc from i to j with semantic role `.,3 Solving the Projections,[0],[0]
"In addition to [0, 1] constraints, we constrain that the predictions for labeled arcs sum to the prediction of their associated unlabeled arc:
PSDP { p ∈ Ud ∣∣∣∣∣∑ ` p σ(i `→j) = pσ(i→j), ∀i 6= j } .
(6)
",3 Solving the Projections,[0],[0]
This ensures that exactly one label is predicted if and only if its arc is present.,3 Solving the Projections,[0],[0]
The projection onto PSDP can be solved similarly to Algorithm 2.,3 Solving the Projections,[0],[0]
We drop the determinism constraint imposed by Peng et al. (2017) in the backward computation.,3 Solving the Projections,[0],[0]
"We empirically evaluate our method with two sets of experiments: using syntactic tree structures in semantic dependency parsing, and using semantic dependency graphs in sentiment classification.",4 Experiments,[0],[0]
"In this experiment we consider an intermediate syntactic parsing task, followed by seman-
… became dismayed at
poss arg1
arg2
’sG-2 connections arrested traffickersto drug
arg2 compound
root
arg2 arg1 arg2
tic dependency parsing as the end task.",4.1 Syntactic-then-Semantic Parsing,[0],[0]
"We first briefly review the neural network architectures for the two models (§4.1.1), and then introduce the datasets (§4.1.2) and baselines (§4.1.3).",4.1 Syntactic-then-Semantic Parsing,[0],[0]
Syntactic dependency parser.,4.1.1 Architectures,[0],[0]
"For intermediate syntactic dependencies, we use the unlabeled arc-factored parser of Kiperwasser and Goldberg (2016).",4.1.1 Architectures,[0],[0]
"It uses bidirectional LSTMs (BiLSTM) to encode the input, followed by a multilayerperceptron (MLP) to score each potential dependency.",4.1.1 Architectures,[0],[0]
"One notable modification is that we replace their use of Chu-Liu/Edmonds’ algorithm (Chu and Liu, 1965; Edmonds, 1967) with the Eisner algorithm (Eisner, 1996, 2000), since our dataset is in English and mostly projective.
",4.1.1 Architectures,[0],[0]
Semantic dependency parser.,4.1.1 Architectures,[0],[0]
We use the basic model of Peng et al. (2017) (denoted as NEURBOPARSER) as the end model.,4.1.1 Architectures,[0],[0]
"It is a first-order parser, and uses local factors for heads, unlabeled arcs, and labeled arcs.",4.1.1 Architectures,[0],[0]
NEURBOPARSER does not use syntax.,4.1.1 Architectures,[0],[0]
"It first encodes an input sentence with a two-layer BiLSTM, and then computes part scores with two-layer tanh-MLPs.",4.1.1 Architectures,[0],[0]
"Inference is conducted with AD3 (Martins et al., 2015).",4.1.1 Architectures,[0],[0]
"To add syntactic features to NEURBOPARSER, we concatenate a token’s contextualized representation to that of its syntactic head, predicted by the intermediate parser.",4.1.1 Architectures,[0],[0]
"Formally, given length-n input sentence, we first run a BiLSTM.",4.1.1 Architectures,[0],[0]
We use the concatenation of the two hidden representations hj =,4.1.1 Architectures,[0],[0]
[ −→ h j ; ←−,4.1.1 Architectures,[0],[0]
h j ] at each position j as the contextualized token representations.,4.1.1 Architectures,[0],[0]
"We then concatenate
hj with the representation of its head hHEAD(j) by
h̃j =",4.1.1 Architectures,[0],[0]
[hj ;hHEAD(j)],4.1.1 Architectures,[0],[0]
= hj ;∑ i 6=j ẑσ(i→j),4.1.1 Architectures,[0],[0]
"hi  , (7)
where ẑ ∈ Bn(n−1) is a binary encoding of the tree structure predicted by by the intermediate parser.",4.1.1 Architectures,[0],[0]
We then use h̃j anywhere hj would have been used in NEURBOPARSER.,4.1.1 Architectures,[0],[0]
"In backpropagation, we compute ∇ẑL with an automatic differentiation toolkit (DyNet; Neubig et al., 2017).
",4.1.1 Architectures,[0],[0]
"We note that this approach can be generalized to convolutional neural networks over graphs (Mou et al., 2015; Duvenaud et al., 2015; Kipf and Welling, 2017, inter alia), recurrent neural networks along paths (Xu et al., 2015; Roth and Lapata, 2016, inter alia) or dependency trees (Tai et al., 2015).",4.1.1 Architectures,[0],[0]
"We choose to use concatenations to control the model’s complexity, and thus to better understand which parts of the model work.
",4.1.1 Architectures,[0],[0]
"We refer the readers to Kiperwasser and Goldberg (2016) and Peng et al. (2017) for further details of the parsing models.
",4.1.1 Architectures,[0],[0]
Training procedure.,4.1.1 Architectures,[0],[0]
"Following previous work, we minimize structured hinge loss (Tsochantaridis et al., 2004) for both models.",4.1.1 Architectures,[0],[0]
"We jointly train both models from scratch, by randomly sampling an instance from the union of their training data at each step.",4.1.1 Architectures,[0],[0]
"In order to isolate the effect of backpropagation, we do not share any parameters between the two models.5 Implementation details are summarized in the supplementary materials.",4.1.1 Architectures,[0],[0]
"• For semantic dependencies, we use the
English dataset from SemEval 2015 Task 18 (Oepen et al., 2015).",4.1.2 Datasets,[0],[0]
"Among the three formalisms provided by the shared task, we consider DELPH-IN MRS-derived dependencies (DM) and Prague Semantic Dependencies (PSD).6 It includes §00–19 of the WSJ corpus as training data, §20 and §21 for development and in-domain test data, resulting in a 33,961/1,692/1,410 train/dev./test split, and
5 Parameter sharing has proved successful in many related tasks (Collobert and Weston, 2008; Søgaard and Goldberg, 2016; Ammar et al., 2016; Swayamdipta et al., 2016, 2017, inter alia), and could be easily combined with our approach.
",4.1.2 Datasets,[0],[0]
"6We drop the third (PAS) because its structure is highly predictable from parts-of-speech, making it less interesting.
",4.1.2 Datasets,[0],[0]
"1,849 out-of-domain test instances from the Brown corpus.7 •",4.1.2 Datasets,[0],[0]
"For syntactic dependencies, we use the Stanford Dependency (de Marneffe and Manning, 2008) conversion of the the Penn Treebank WSJ portion (Marcus et al., 1993).",4.1.2 Datasets,[0],[0]
"To avoid data leak, we depart from standard split and use §20 and §21 as development and test data, and the remaining sections as training data.",4.1.2 Datasets,[0],[0]
"The number of training/dev./test instances is 40,265/2,012/1,671.",4.1.2 Datasets,[0],[0]
We compare to the following baselines: • A pipelined system (PIPELINE).,4.1.3 Baselines,[0],[0]
"The pre-
trained parser achieves 92.9 test unlabeled attachment score (UAS).8
7The organizers remove, e.g., instances with cyclic graphs, and thus only a subset of the WSJ corpus is included.",4.1.3 Baselines,[0],[0]
"See Oepen et al. (2015) for details.
",4.1.3 Baselines,[0],[0]
8 Note that this number is not comparable to the parsing literature due to the different split.,4.1.3 Baselines,[0],[0]
"As a sanity check, we found in preliminary experiments that the same parser archi-
• Structured attention networks (SA; Kim et al., 2017).",4.1.3 Baselines,[0],[0]
"We use the inside-outside algorithm (Baker, 1979) to populate z with arcs’ marginal probabilities, use log-loss as the objective in training the intermediate parser.",4.1.3 Baselines,[0],[0]
"• The straight-through estimator (STE; Hinton,
2012), introduced in §2.2.",4.1.3 Baselines,[0],[0]
Table 1 compares the semantic dependency parsing performance of SPIGOT to all five baselines.,4.1.4 Empirical Results,[0],[0]
"FREDA3 (Peng et al., 2017) is a state-of-the-art variant of NEURBOPARSER that is trained using multitask learning to jointly predict three different semantic dependency graph formalisms.",4.1.4 Empirical Results,[0],[0]
"Like the basic NEURBOPARSER model that we build from, FREDA3 does not use any syntax.",4.1.4 Empirical Results,[0],[0]
"Strong DM performance is achieved in a more recent work by using joint learning and an ensemble (Peng et al., 2018), which is beyond fair comparisons to the models discussed here.
",4.1.4 Empirical Results,[0],[0]
We found that using syntactic information improves semantic parsing performance: using pipelined syntactic head features brings 0.5– 1.4% absolute labeled F1 improvement to NEURBOPARSER.,4.1.4 Empirical Results,[0],[0]
"Such improvements are smaller compared to previous works, where dependency path and syntactic relation features are included (Almeida and Martins, 2015; Ribeyre et al., 2015; Zhang et al., 2016), indicating the potential to get better performance by using more syntactic information, which we leave to future work.
",4.1.4 Empirical Results,[0],[0]
Both STE and SPIGOT use hard syntactic features.,4.1.4 Empirical Results,[0],[0]
"By allowing backpropation into the intermediate syntactic parser, they both consistently outperform PIPELINE.",4.1.4 Empirical Results,[0],[0]
"On the other hand, when marginal syntactic tree structures are used, SA outperforms PIPELINE only on the out-of-domain PSD test set, and improvements under other cases are not observed.
",4.1.4 Empirical Results,[0],[0]
"Compared to STE, SPIGOT outperforms STE on DM by more than 0.3% absolute labeled F1, both in-domain and out-of-domain.",4.1.4 Empirical Results,[0],[0]
"For PSD, SPIGOT achieves similar performance to STE on in-domain test set, but has a 0.5% absolute labeled F1 improvement on out-of-domain data, where syntactic parsing is less accurate.
",4.1.4 Empirical Results,[0],[0]
"tecture achieves 93.5 UAS when trained and evaluated with the standard split, close to the results reported by Kiperwasser and Goldberg (2016).",4.1.4 Empirical Results,[0],[0]
Our second experiment uses semantic dependency graphs to improve sentiment classification performance.,4.2 Semantic Dependencies for Sentiment Classification,[0],[0]
"We are not aware of any efficient algorithm that solves marginal inference for semantic dependency graphs under determinism constraints, so we do not include a comparison to SA.",4.2 Semantic Dependencies for Sentiment Classification,[0],[0]
"Here we use NEURBOPARSER as the intermediate model, as described in §4.1.1, but with no syntactic enhancements.
",4.2.1 Architectures,[0],[0]
Sentiment classifier.,4.2.1 Architectures,[0],[0]
We first introduce a baseline that does not use any structural information.,4.2.1 Architectures,[0],[0]
"It learns a one-layer BiLSTM to encode the input sentence, and then feeds the sum of all hidden states into a two-layer ReLU-MLP.
",4.2.1 Architectures,[0],[0]
"To use semantic dependency features, we concatenate a word’s BiLSTM-encoded representation to the averaged representation of its heads, together with the corresponding semantic roles, similarly to that in Equation 7.9 Then the concatenation is fed into an affine transformation followed by a ReLU activation.",4.2.1 Architectures,[0],[0]
"The rest of the model is kept the same as the BiLSTM baseline.
",4.2.1 Architectures,[0],[0]
Training procedure.,4.2.1 Architectures,[0],[0]
"We use structured hinge loss to train the semantic dependency parser, and log-loss for the sentiment classifier.",4.2.1 Architectures,[0],[0]
"Due to the discrepancy in the training data size of the two tasks (33K vs. 7K), we pre-train a semantic dependency parser, and then adopt joint training together with the classifier.",4.2.1 Architectures,[0],[0]
"In the joint training stage, we randomly sample 20% of the semantic dependency training instances each epoch.",4.2.1 Architectures,[0],[0]
Implementations are detailed in the supplementary materials.,4.2.1 Architectures,[0],[0]
"For semantic dependencies, we use the DM dataset introduced in §4.1.2.
We consider a binary classification task using the Stanford Sentiment Treebank (Socher et al., 2013).",4.2.2 Datasets,[0],[0]
It consists of roughly 10K movie review sentences from Rotten Tomatoes.,4.2.2 Datasets,[0],[0]
"The full dataset includes a rating on a scale from 1 to 5 for each constituent (including the full sentences), resulting in more than 200K instances.",4.2.2 Datasets,[0],[0]
"Following previous work (Iyyer et al., 2015), we only use full-sentence
9In a well-formed semantic dependency graph, a token may have multiple heads.",4.2.2 Datasets,[0],[0]
"Therefore we use average instead of the sum in Equation 7.
instances, with neutral instances excluded (3s) and the remaining four rating levels converted to binary “positive” or “negative” labels.",4.2.2 Datasets,[0],[0]
"This results in a 6,920/872/1,821 train/dev./test split.",4.2.2 Datasets,[0],[0]
Table 2 compares our SPIGOT method to three baselines.,4.2.3 Empirical Results,[0],[0]
"Pipelined semantic dependency predictions brings 0.9% absolute improvement in classification accuracy, and SPIGOT outperforms all baselines.",4.2.3 Empirical Results,[0],[0]
In this task STE achieves slightly worse performance than a fixed pre-trained PIPELINE.,4.2.3 Empirical Results,[0],[0]
We examine here how the intermediate model is affected by the end-task training signal.,5 Analysis,[0],[0]
"Is the endtask signal able to “overrule” intermediate predictions?
",5 Analysis,[0],[0]
We use the syntactic-then-semantic parsing model (§4.1) as a case study.,5 Analysis,[0],[0]
Table 3 compares a pipelined system to one jointly trained using SPIGOT.,5 Analysis,[0],[0]
"We consider the development set instances where both syntactic and semantic annotations are available, and partition them based on whether the two systems’ syntactic predictions agree (SAME), or not (DIFF).",5 Analysis,[0],[0]
"The second group includes sentences with much lower syntactic parsing accuracy (91.3 vs. 97.4 UAS), and SPIGOT further reduces this to 89.6.",5 Analysis,[0],[0]
"Even though these changes hurt syntactic parsing accuracy, they lead to a 1.1% absolute gain in labeled F1 for semantic parsing.",5 Analysis,[0],[0]
"Furthermore, SPIGOT has an overall less detrimental effect on the intermediate parser than STE: using SPIGOT, intermediate dev. parsing UAS drops to 92.5 from the 92.9 pipelined performance, while STE reduces it to 91.8.
",5 Analysis,[0],[0]
We then take a detailed look and categorize the changes in intermediate trees by their correlations with the semantic graphs.,5 Analysis,[0],[0]
"Specifically, when a modifier m’s head is changed from h to h′ in the
tree, we consider three cases: (a) h′ is a head of m in the semantic graph; (b) h′ is a modifier of m in the semantic graph; (c) h is the modifier of m in the semantic graph.",5 Analysis,[0],[0]
The first two reflect modifications to the syntactic parse that rearrange semantically linked words to be neighbors.,5 Analysis,[0],[0]
"Under (c), the semantic parser removes a syntactic dependency that reverses the direction of a semantic dependency.",5 Analysis,[0],[0]
"These cases account for 17.6%, 10.9%, and 12.8%, respectively (41.2% combined) of the total changes.",5 Analysis,[0],[0]
"Making these changes, of course, is complicated, since they often require other modifications to maintain well-formedness of the tree.",5 Analysis,[0],[0]
Figure 2 gives an example.,5 Analysis,[0],[0]
Joint learning in NLP pipelines.,6 Related Work,[0],[0]
"To avoid cascading errors, much effort has been devoted to joint decoding in NLP pipelines (Habash and Rambow, 2005; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Lewis et al., 2015; Zhang et al., 2015, inter alia).",6 Related Work,[0],[0]
"However, joint inference can sometimes be prohibitively expensive.",6 Related Work,[0],[0]
"Recent advances in representation learning facilitate exploration in the joint learning of multiple tasks by sharing parameters (Collobert and Weston, 2008; Blitzer et al., 2006; Finkel and Manning, 2010; Zhang and Weiss, 2016; Hashimoto et al., 2017, inter alia).
",6 Related Work,[0],[0]
Differentiable optimization.,6 Related Work,[0],[0]
"Gould et al. (2016) review the generic approaches to differentiation in bi-level optimization (Bard, 2010; Kunisch and Pock, 2013).",6 Related Work,[0],[0]
Amos and Kolter (2017) extend their efforts to a class of subdifferentiable quadratic programs.,6 Related Work,[0],[0]
"However, they both require that the intermediate objective has an invertible Hessian, limiting their application
in NLP.",6 Related Work,[0],[0]
"In another line of work, the steps of a gradient-based optimization procedure are unrolled into a single computation graph (Stoyanov et al., 2011; Domke, 2012; Goodfellow et al., 2013; Brakel et al., 2013).",6 Related Work,[0],[0]
This comes at a high computational cost due to the second-order derivative computation during backpropagation.,6 Related Work,[0],[0]
"Moreover, constrained optimization problems (like many NLP problems) often require projection steps within the procedure, which can be difficult to differentiate through (Belanger and McCallum, 2016; Belanger et al., 2017).",6 Related Work,[0],[0]
"We presented SPIGOT, a novel approach to backpropagating through neural network architectures that include discrete structured decisions in intermediate layers.",7 Conclusion,[0],[0]
"SPIGOT devises a proxy for the gradients with respect to argmax’s inputs, employing a projection that aims to respect the constraints in the intermediate task.",7 Conclusion,[0],[0]
"We empirically evaluate our method with two architectures: a semantic parser with an intermediate syntactic parser, and a sentiment classifier with an intermediate semantic parser.",7 Conclusion,[0],[0]
"Experiments show that SPIGOT achieves stronger performance than baselines under both settings, and outperforms stateof-the-art systems on semantic dependency parsing.",7 Conclusion,[0],[0]
Our implementation is available at https: //github.com/Noahs-ARK/SPIGOT.,7 Conclusion,[0],[0]
"We thank the ARK, Julian Michael, Minjoon Seo, Eunsol Choi, and Maxwell Forbes for their helpful comments on an earlier version of this work, and the anonymous reviewers for their valuable feedback.",Acknowledgments,[0],[0]
This work was supported in part by NSF grant IIS-1562364.,Acknowledgments,[0],[0]
"We introduce the structured projection of intermediate gradients optimization technique (SPIGOT), a new method for backpropagating through neural networks that include hard-decision structured predictions (e.g., parsing) in intermediate layers.",abstractText,[0],[0]
"SPIGOT requires no marginal inference, unlike structured attention networks (Kim et al., 2017) and some reinforcement learning-inspired solutions (Yogatama et al., 2017).",abstractText,[0],[0]
"Like socalled straight-through estimators (Hinton, 2012), SPIGOT defines gradient-like quantities associated with intermediate nondifferentiable operations, allowing backpropagation before and after them; SPIGOT’s proxy aims to ensure that, after a parameter update, the intermediate structure will remain well-formed.",abstractText,[0],[0]
"We experiment on two structured NLP pipelines: syntactic-then-semantic dependency parsing, and semantic parsing followed by sentiment classification.",abstractText,[0],[0]
"We show that training with SPIGOT leads to a larger improvement on the downstream task than a modularly-trained pipeline, the straight-through estimator, and structured attention, reaching a new state of the art on semantic dependency parsing.",abstractText,[0],[0]
Backpropagating through Structured Argmax using a SPIGOT,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 153–161 New Orleans, Louisiana, June 1 - 6, 2018. c©2017 Association for Computational Linguistics",text,[0],[0]
"Natural language understanding (NLU) is a key component of dialog systems for commercial personal digital assistants (PDAs) such as Amazon Alexa, Google Home, Microsoft Cortana and Apple Siri.",1 Introduction,[0],[0]
"The task of the NLU component is to map input user utterances into a semantic frame consisting of domain, intent and slots (Kurata et al., 2016).",1 Introduction,[0],[0]
"The semantic frame is used by the dialog manager for state tracking and action selection.
",1 Introduction,[0],[0]
"Slot tagging can be formulated as a sequence classification task where each input word in the user utterance must be classified as belonging to one of the slot types in a predefined schema (Sarikaya et al., 2016).",1 Introduction,[0],[0]
"In a standard NLU architecture, each new domain defines a new domainspecific schema for its slots.",1 Introduction,[0],[0]
"Figure 1 shows examples of annotated queries from three different domains relevant to a typical commercial digital
assistant.",1 Introduction,[0],[0]
"Since the schemas for different domains can vary, the usual strategy is to train a separate slot tagging model for each new domain.",1 Introduction,[0],[0]
"However, the number of domains increases rapidly as the PDAs are required to support new scenarios and training a separate slot tagging model for each new domain becomes prohibitively expensive in terms of annotation costs.
",1 Introduction,[0],[0]
"Even though different domains have different slot tagging schemas, some classes of slots appear across a number of domains, as suggested by the examples in Figure 1.",1 Introduction,[0],[0]
"Both travel and flight status have date and time related slots, and all three domains have the location slot.",1 Introduction,[0],[0]
Reusing annotated data for these common slots would allow us to train models with better accuracy using less data.,1 Introduction,[0],[0]
"However, since both the input distribution and the label distribution are different across domains, we must use domain adaptation methods to train on the joint data (Daume, 2007; Kim et al.,
153
2016c; Blitzer et al., 2006).",1 Introduction,[0],[0]
"In this data-driven adaptation approach, we build a repository of annotated data containing date, time, location and other reusable slots.",1 Introduction,[0],[0]
We then combine relevant data from the reusable repository with the domain specific data during model training.,1 Introduction,[0],[0]
"Figure 2(a) shows an example of this architecture where reusable date/time data is used for training travel domain.
",1 Introduction,[0],[0]
"A drawback of the data-driven adaptation approach is that as the repository of data for reusable slots grows, the training time for new domains increases.",1 Introduction,[0],[0]
"The training data for a new domain might be in the hundreds of samples, while the training data for the reusable slots might contain hundreds of thousands of samples.",1 Introduction,[0],[0]
"This increase in training time makes iterative refinement difficult in the initial design of new domains, which is when the ability to deploy new models quickly is crucial.
",1 Introduction,[0],[0]
"An alternative strategy is to use model-driven adaptation approaches (Kim et al., 2017b) as shown in Figure 2(b).",1 Introduction,[0],[0]
"Here, instead of retraining on the data for the reusable slots, we train “expert” models for these slots, and use the output of these models directly when training new domains.",1 Introduction,[0],[0]
"Using model-driven adaptation ensures that model training time is proportional to the data size of new
target domains, as opposed to the large data size for reusable slots, allowing for faster training.
",1 Introduction,[0],[0]
"In this paper, we present a model-driven adaptation approach for slot tagging called Bag of Experts (BoE).",1 Introduction,[0],[0]
"In Section 2, we first describe how this approach can be applied to two popular machine learning methods used for slot tagging: Long Short Term Memory (LSTM) and Conditional Random Fields (CRF) models.",1 Introduction,[0],[0]
"We then describe a dataset of 10 target domains and 2 reusable domains that we’ve collected for use in a commercial digital assistant, in Section 3.",1 Introduction,[0],[0]
"Using this data, we conduct experiments comparing the BoE models with their non-expert counterparts, and show that BoE models can lead to significant F1-score improvements.",1 Introduction,[0],[0]
The experimental setup is described in Section 4.1 and the results are discussed in Section 4.3.,1 Introduction,[0],[0]
This is followed by a survey of related work in Section 5 and the conclusion in Section 6.,1 Introduction,[0],[0]
"We first describe our LSTM and CRF models for slot tagging, followed by their BoE variants: LSTM-BoE and CRF-BoE. Tensorflow (Abadi et al., 2015) was used for implementing the LSTM models, while a custom C++ implementation was
used for the CRF models.",2 Approaches,[0],[0]
"For our LSTM model, we follow a standard bidirectional LSTM architecture (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016).",2.1 LSTM,[0],[0]
Let w1...,2.1 LSTM,[0],[0]
wn denote the input word sequence.,2.1 LSTM,[0],[0]
"For every input word wi, let fCi and b C i be the outputs of the forward and backward character level LSTMs respectively, and let mi be the word embedding (initialized either randomly or with pretrained embeddings).",2.1 LSTM,[0],[0]
"The input to the word level LSTMs, gi, is the concatenation of these three vectors:
gi =",2.1 LSTM,[0],[0]
"[f C i ; b C i ;mi]
where both fCi , b C i ∈",2.1 LSTM,[0],[0]
R25 and mi has the same dimensions as the pre-trained embeddings.,2.1 LSTM,[0],[0]
"The forward and backward word level LSTMs take gi as input and produce fWi and b W i , which are then concatenated to produce hi:
hi =",2.1 LSTM,[0],[0]
"[f W i , b W i ]
",2.1 LSTM,[0],[0]
"where fWi , b W i ∈ R100, making hi ∈ R200.",2.1 LSTM,[0],[0]
hi is then input to a dense feed forward layer with a softmax activation to predict the label probabilities for each word.,2.1 LSTM,[0],[0]
"We train using stochastic gradient descent with Adam (Kingma and Ba, 2015).",2.1 LSTM,[0],[0]
"To avoid overfitting, we also use dropout on top of mi and hi layers, with a default dropout keep probability of 0.8.",2.1 LSTM,[0],[0]
"We experiment with some variations
of this default LSTM architecture, the results are described in Section 4.2.",2.1 LSTM,[0],[0]
We now describe the LSTM Bag of Experts (LSTM-BoE) architecture.,2.2 LSTM-BoE,[0],[0]
Let e1...,2.2 LSTM-BoE,[0],[0]
ek ∈ E be the set of reusable expert domains.,2.2 LSTM-BoE,[0],[0]
"For each expert ej , we train a separate LSTM with the architecture described in Section 2.1.",2.2 LSTM-BoE,[0],[0]
"Let heji be the bi-directional word LSTM output for expert ej on word wi.
",2.2 LSTM-BoE,[0],[0]
"When training on a target domain, for each word wi, we first compute the character level LSTMs fCi , b C i similarly to Section 2.1.",2.2 LSTM-BoE,[0],[0]
"We then compute a BoE representation for this word as:
hE = ∑
ei∈E h ej",2.2 LSTM-BoE,[0],[0]
"i
The input to the word level LSTM for word wi in the target domain is now a concatenation of the character level LSTM outputs (fCi , b C i ), the word embedding mi, and hE :
gi =",2.2 LSTM-BoE,[0],[0]
[f C i ; b C i ;mi;h,2.2 LSTM-BoE,[0],[0]
"E ]
gi is then input to the word level LSTM for the target domain to produce hi in the same way as Section 2.1.",2.2 LSTM-BoE,[0],[0]
"This architecture is similar to the one presented in (Kim et al., 2017b), with the exception that in their architecture, hE is concatenated with the word level LSTM output hi for the target
domain.",2.2 LSTM-BoE,[0],[0]
"In our architecture, we add hE before the word-level LSTM in order to capture long-range dependencies of label prediction for a word on expert predictions for context words.",2.2 LSTM-BoE,[0],[0]
"Conditional Random Fields (CRF) are a popular family of models that have been proven to work well in a variety of sequence tagging NLP applications (Lafferty et al., 2001).",2.3 CRF,[0],[0]
"For our experiments, we use a standard linear-chain CRF architecture with n-gram and context features.
",2.3 CRF,[0],[0]
"In particular, for each token, we use unigram, bigram and trigram features, along with previous and next unigrams, bigrams, and trigrams for context length of up to 3 words.",2.3 CRF,[0],[0]
"We also use a skip bigram feature created by concatenating the current unigram and skip-one unigram.
",2.3 CRF,[0],[0]
We train our CRF using stochastic gradient descent with L1 regularization to prevent overfitting.,2.3 CRF,[0],[0]
"The L1 coefficient was set to 0.1 and we use a learning rate of 0.1 with exponential decay for learning rate scheduling (Tsuruoka et al., 2009).",2.3 CRF,[0],[0]
"Similar to the LSTM-BoE model, we first train a CRF model cj for each of the reusable expert domains ej ∈ E. When training on a target domain, for every query word wi, a one-hot label vector",2.4 CRF-BoE,[0],[0]
l j,2.4 CRF-BoE,[0],[0]
i is emitted by each expert CRF model cj .,2.4 CRF-BoE,[0],[0]
"The length of the label vector lji is the number of labels in the expert domain, with the value corresponding to the label predicted by cj for word wi set to 1, and values for all other labels set to 0.",2.4 CRF-BoE,[0],[0]
"For each word, the label vectors for all the expert CRF models are concatenated and provided as features for the target domain CRF training, along with the n-gram features.",2.4 CRF-BoE,[0],[0]
We built a dataset of 10 target domains for experimentation.,3.1 Target Domains,[0],[0]
Table 1 shows the list of domains as well as some statistics and example utterances.,3.1 Target Domains,[0],[0]
"We treated these as new domains - that is, we do not have real interaction data with users for these domains.",3.1 Target Domains,[0],[0]
"The annotated data is therefore prepared in two steps.
",3.1 Target Domains,[0],[0]
"First, utterances are obtained using crowdsourcing, where workers are provided with prompts for different intents of a domain and asked to generate
natural language utterances corresponding to those intents.",3.1 Target Domains,[0],[0]
"Next, the generated utterances are annotated by a different set of crowd workers, using the slot schema for each domain.",3.1 Target Domains,[0],[0]
"Inter-annotator agreement as well as manual inspection are used to ensure data quality in both stages.
",3.1 Target Domains,[0],[0]
The amount of data collected varies for each domain based on its complexity and business priority.,3.1 Target Domains,[0],[0]
Dataset size statistics for the data used in our experiments are presented in section 4.1.,3.1 Target Domains,[0],[0]
"Test and dev data are sampled at 10% of the total annotated data, with stratified sampling used in order to preserve the distribution of the intents.",3.1 Target Domains,[0],[0]
We experiment with two domains containing reusable slots: timex and location.,3.2 Reusable Domains,[0],[0]
"The timex domain consists of utterances containing the slots date, time and duration.",3.2 Reusable Domains,[0],[0]
"The location domain consists of utterances containing location, location type and place name slots.",3.2 Reusable Domains,[0],[0]
"Both of these types of slots appear in more than 20 of a set of 40 domains developed for use in our commercial personal assistant, making them ideal candidates for reuse.1
1Several other candidate reusable domains exist, including: the name domain containing the slot contact name; the number domain containing the slots rating, quantity and price; and the reference domain containing the slots ordinal (whose values include “first”, “second” or “third”) and order ref (with values such as “before” or “after”).",3.2 Reusable Domains,[0],[0]
"All of these slots appear in more than 25% of the available domains.
",3.2 Reusable Domains,[0],[0]
Data for these domains was sampled from the input utterances from our commercial digital assistant.,3.2 Reusable Domains,[0],[0]
Each reusable domain contains about a million utterances.,3.2 Reusable Domains,[0],[0]
There is no overlap between utterances in the target domains used for our experiments and utterances in the reusable domains.,3.2 Reusable Domains,[0],[0]
"The data for the reusable domains is sampled from other domains available to the digital assistant, not including our target domains.
",3.2 Reusable Domains,[0],[0]
Grouping the reusable slots into domains in this way provides additional opportunities for a commercial system: the trained reusable domain models can be used in other related products which need to identify time and location related entities.,3.2 Reusable Domains,[0],[0]
Models trained on the timex and location data have F1-scores of 96% and 89% respectively on test data from their respective domains.,3.2 Reusable Domains,[0],[0]
We want to verify if BoE models can improve slot tagging performance by using the information from reusable domains.,4.1 Experimental Setup,[0],[0]
"To simulate the low data scenario for the initial model training, we create three training datasets by sampling 2000, 1000 and 500 training examples from every domain.",4.1 Experimental Setup,[0],[0]
"We use stratified sampling to maintain the input distribution of the intents across the three training datasets.
",4.1 Experimental Setup,[0],[0]
"For each training dataset, we train the four models as described in Section 2 and compute the precision, recall and F1-score on the test data.",4.1 Experimental Setup,[0],[0]
Fixed seeds are used when training all models to make the results reproducible.,4.1 Experimental Setup,[0],[0]
"Table 3 summarizes these results, with only F1-scores reported to save space.",4.1 Experimental Setup,[0],[0]
We describe these results in Section 4.3.,4.1 Experimental Setup,[0],[0]
"Using the dev data set for the 10 domains, we experimented with using different pretrained embeddings, dropout probabilities and a CRF output layer in our LSTM architecture.",4.2 LSTM architecture variants,[0],[0]
The results are summarized in Table 2.,4.2 LSTM architecture variants,[0],[0]
"For each of the 10 domains, we trained using each variant with 10 different seeds, and computed the mean F1-score for each domain.",4.2 LSTM architecture variants,[0],[0]
"For comparing two variants, we computed the mean difference in the F1-scores over the 10 domains and its p-value.
",4.2 LSTM architecture variants,[0],[0]
"We tried word level Glove embeddings of 100, 200 and 300 dimensions as well as 500- dimensional word embeddings trained over the ut-
terances from our commercial PDA logs.",4.2 LSTM architecture variants,[0],[0]
"Both 100 and 200 dimensional Glove embeddings led to statistically significant improvements, but the word embeddings trained over our logs led to the biggest improvement.",4.2 LSTM architecture variants,[0],[0]
"We also tried using a CRF output layer (Lample et al., 2016) and different values of dropout keep probability, but none of them gave statistically significant improvements over the default model.",4.2 LSTM architecture variants,[0],[0]
"Based on this, we used PDA trained 500-dimensional word embeddings for our final experiments on test data.",4.2 LSTM architecture variants,[0],[0]
Table 3(a) shows the F1-scores obtained by the different methods for the training data set of 2000 training instances for each of the 10 domains.,4.3 Results and Discussion,[0],[0]
LSTM based models in general perform better than the CRF based models.,4.3 Results and Discussion,[0],[0]
The LSTM models have a statistically significant average improvement of 3.14 absolute F1-score over the CRF models.,4.3 Results and Discussion,[0],[0]
"The better performance of LSTM over CRF can be explained by the LSTM being able to use information over longer contexts to make predictions, while the CRF model is limited to at most the previous and next 3 words.
",4.3 Results and Discussion,[0],[0]
The results in Table 3(a) also show that both the CRF-BoE and LSTM-BoE outperform the basic CRF and LSTM models.,4.3 Results and Discussion,[0],[0]
LSTM-BoE has a statistically significant mean improvement of 1.92 points over LSTM.,4.3 Results and Discussion,[0],[0]
"CRF-BoE also shows an average improvement of 2.19 points over the CRF model, but the results are not statistically significant.",4.3 Results and Discussion,[0],[0]
"Looking at results for individual domains, the highest improvement for BoE models are seen for transportation and travel.",4.3 Results and Discussion,[0],[0]
"This can be explained by these domains having a high frequency of timex and location slots, as shown in Table 4.
",4.3 Results and Discussion,[0],[0]
"The shopping model shows a regression for BoE models, and a reason could be the low frequency of expert slots (Table 4).",4.3 Results and Discussion,[0],[0]
"However, low frequency of expert slots does not always mean that BoE methods can’t help, as shown by the improvement in the purchase domain.",4.3 Results and Discussion,[0],[0]
"Finally, for sports, social network and deals domains, the LSTM-BoE improves over LSTM, while CRFBoE does not improve over CRF.",4.3 Results and Discussion,[0],[0]
"Our hypothesis is that given the query patterns for these domains, the dense vector output used by LSTM-BoE is able to transfer some information, while the categorical label output used by CRF-BoE is not.
",4.3 Results and Discussion,[0],[0]
"Table 3(b) shows the results with 500 and 1000
training data instances.",4.3 Results and Discussion,[0],[0]
Note that the improvements are even higher for the experiments with smaller training data.,4.3 Results and Discussion,[0],[0]
"In particular, LSTM-BoE shows an improvement of 4.63 in absolute F1score over LSTM when training with 500 instances.",4.3 Results and Discussion,[0],[0]
"Thus, as we reduce the amount of training data in the target domain, the performance improvement from BoE models is even higher.
",4.3 Results and Discussion,[0],[0]
"As an example, in the purchase domain, the LSTM-BoE model achieves an F1-score of 70.66% with only 500 training instances, while even with 2000 training instances the CRF model achieves an F1-score of only 66.24%.",4.3 Results and Discussion,[0],[0]
Thus the LSTM-BoE model achieves better F1-score with only one-fourth the training data.,4.3 Results and Discussion,[0],[0]
"Similarly, for flight status, travel, and transportation domains, the LSTM-BoE model gets better performance with 500 training instances, compared to a CRF model with 2000 training instances.",4.3 Results and Discussion,[0],[0]
"The LSTMBoE architecture, therefore, allows us to reuse the domain experts to produce better performing mod-
els with much lower data annotation costs.",4.3 Results and Discussion,[0],[0]
"As the target domain training data increases, the contribution due to domain experts goes down, but more experimentation is needed to establish the threshold at which it is no longer useful to add experts.",4.3 Results and Discussion,[0],[0]
"Early methods for slot-tagging used rule-based approaches (Ward and Issar, 1994).",5 Related Work,[0],[0]
"Much of the later work on supervised learning focused on CRFs, for example (Sarikaya et al., 2016), or neural networks (Deoras and Sarikaya, 2013; Yao et al., 2013; Liu et al., 2015; Celikyilmaz and HakkaniTur, 2015).",5 Related Work,[0],[0]
"Unsupervised (or weakly-supervised) methods also were used for NLU tasks, primarily leveraging search query click logs (Hakkani-Tur et al., 2011a,b, 2013) and knowledge graphs (Tur et al., 2012; Heck and Hakkani-Tur, 2012; Heck et al., 2013); hybrid methods, for example as described in (Kim et al., 2015a; Celikyilmaz et al., 2015; Chen et al., 2016), also exist.",5 Related Work,[0],[0]
"Our approach
in this paper is a purely supervised one.",5 Related Work,[0],[0]
"Transfer learning is a vast area of research, with too many publications for an exhaustive list.",5 Related Work,[0],[0]
We discuss some of the recent work most relevant to our methods.,5 Related Work,[0],[0]
"In (Kim et al., 2015b), the slot labels from across different domains are mapped into a shared space using Canonical Correlation Analysis (CCA) and automatically-induced embeddings over the label space.",5 Related Work,[0],[0]
"These label representations allow mapping of label types between different domains, which makes it possible to apply standard data-driven domain adaptation approaches (Daume, 2007).",5 Related Work,[0],[0]
"They also introduce a model-driven adaptation technique based on training a hidden unit CRF (HUCRF) on the source domain, which is then used to initialize the training for the target domain.",5 Related Work,[0],[0]
"The limitation of this approach is that only one source domain can be used, while multiple experts can be used in the proposed BoE approach.
",5 Related Work,[0],[0]
"(Kim et al., 2016a) build a single, universal slot tagging model, and constrain the decoding process to subsets of slots for various domains; this process assumes that a mapping of slot tags in the new domain to the ones in the universal slot model has already been generated.",5 Related Work,[0],[0]
"A related work by (Kim et al., 2016b) directly predicts the required schema prior to performing the constrained decoding.",5 Related Work,[0],[0]
"These approaches are attractive because only one universal model needs to be trained, but do not work in cases when a new domain contains a mixture of new and existing slots.",5 Related Work,[0],[0]
"Our approach allows transfer of partial knowledge in such cases.
",5 Related Work,[0],[0]
"(Kim et al., 2016c) uses a neural version of the approach first described in (Daume, 2007), by using existing annotated data in a variety of domains
to adapt the slot tag models of new domains where the tag space is partly shared.",5 Related Work,[0],[0]
"The drawback of such data-driven domain adaptation is the increase in training time as more experts are added.
",5 Related Work,[0],[0]
"An expert-based adaptation, similar to the techniques applied in this paper, was first described in (Kim et al., 2017b).",5 Related Work,[0],[0]
"(Jaech et al., 2016) use multitask learning, training a bidirectional LSTM with character-level embeddings, trained jointly to produce slot tags for a number of travel-related domains.",5 Related Work,[0],[0]
"Finally, (Kim et al., 2017a) frame the problem of temporal shift in data of a single domain (and the related problem of bootstrapping a new domain with imperfectly-matched synthetic data) as one of domain adaptation, applying adversarial training approaches.
",5 Related Work,[0],[0]
A number of researchers also investigated bootstrapping NLU systems using zero-shot learning.,5 Related Work,[0],[0]
"(Dauphin et al., 2014; Kumar et al., 2017) both investigated domain classification; most relevant to us is the work by (Bapna et al., 2017), who studied full semantic frame tagging using zero-shot learning, by projecting the tags into a shared embedding space, similar to work done by (Kim et al., 2015b).",5 Related Work,[0],[0]
We experimented with Bag of Experts (BoE) architectures for CRF and LSTM based slot tagging models.,6 Conclusion,[0],[0]
Our experimental results over a set of 10 domains show that BoE architectures are able to use the information from reusable expert models to perform significantly better than their nonexpert counterparts.,6 Conclusion,[0],[0]
"In particular, the LSTM-BoE model shows a statistically significant improvement of 1.92% over the LSTM model on average when training with 2000 instances.",6 Conclusion,[0],[0]
"When training with 500 instances, the improvement of LSTM-BoE model over LSTM is even higher at 4.63%.",6 Conclusion,[0],[0]
"For multiple domains, an LSTM-BoE model trained on only 500 instances is able to outperform a baseline CRF model trained over 4 times the data.",6 Conclusion,[0],[0]
"Thus, the BoE approach produces high performing models for slot tagging at much lower annotation costs.",6 Conclusion,[0],[0]
We would like to thank Ahmed El Kholy for his comments and feedback on an earlier version of this paper.,Acknowledgments,[0],[0]
"Also, thanks to Kyle Williams and Zhaleh Feizollahi for their help with code and data collection.",Acknowledgments,[0],[0]
"Slot tagging, the task of detecting entities in input user utterances, is a key component of natural language understanding systems for personal digital assistants.",abstractText,[0],[0]
"Since each new domain requires a different set of slots, the annotation costs for labeling data for training slot tagging models increases rapidly as the number of domains grow.",abstractText,[0],[0]
"To tackle this, we describe Bag of Experts (BoE) architectures for model reuse for both LSTM and CRF based models.",abstractText,[0],[0]
"Extensive experimentation over a dataset of 10 domains drawn from data relevant to our commercial personal digital assistant shows that our BoE models outperform the baseline models with a statistically significant average margin of 5.06% in absolute F1score when training with 2000 instances per domain, and achieve an even higher improvement of 12.16% when only 25% of the training data is used.",abstractText,[0],[0]
Bag of Experts Architectures for Model Reuse in Conversational Language Understanding,title,[0],[0]
The stochastic multi-armed bandit (MAB) problem is a prominent framework for capturing the explorationexploitation tradeoff in online decision making and experiment design.,1. Introduction,[0],[0]
"The MAB problem proceeds in discrete sequential rounds, where in each round, the player pulls one
1 Department of Mathematics and Statistics, Lancaster University, Lancaster, UK 2 Department of Industrial Engineering and Operations Research, Columbia University, New York, NY, USA 3 DeepMind, London, UK 4 Department of Computing Science, University of Alberta, Edmonton, AB, Canada.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Ciara Pike-Burke <ciara.pikeburke@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
of the K possible arms.",1. Introduction,[0],[0]
"In the classic stochastic MAB setting, the player immediately observes stochastic feedback from the pulled arm in the form of a ‘reward’ which can be used to improve the decisions in subsequent rounds.",1. Introduction,[0],[0]
One of the main application areas of MABs is in online advertising.,1. Introduction,[0],[0]
"Here, the arms correspond to adverts, and the feedback would correspond to conversions, that is users buying a product after seeing an advert.",1. Introduction,[0],[0]
"However, in practice, these conversions may not necessarily happen immediately after the advert is shown, and it may not always be possible to assign the credit of a sale to a particular showing of an advert.",1. Introduction,[0],[0]
"A similar challenge is encountered in many other applications, e.g., in personalized treatment planning, where the effect of a treatment on a patient’s health may be delayed, and it may be difficult to determine which out of several past treatments caused the change in the patient’s health; or, in content design applications, where the effects of multiple changes in the website design on website traffic and footfall may be delayed and difficult to distinguish.
",1. Introduction,[0],[0]
"In this paper, we propose a new bandit model to handle online problems with such ‘delayed, aggregated and anonymous’ feedback.",1. Introduction,[0],[0]
"In our model, a player interacts with an environment ofK actions (or arms) in a sequential fashion.",1. Introduction,[0],[0]
At each time step the player selects an action which leads to a reward generated at random from the underlying reward distribution.,1. Introduction,[0],[0]
"At the same time, a nonnegative random integer-valued delay is also generated i.i.d.",1. Introduction,[0],[0]
from an underlying delay distribution.,1. Introduction,[0],[0]
Denoting this delay by τ ≥ 0,1. Introduction,[0],[0]
"and the index of the current round by t, the reward generated in round t will arrive at the end of the (t + τ)th round.",1. Introduction,[0],[0]
"At the end of each round, the player observes only the sum of all the rewards that arrive in that round.",1. Introduction,[0],[0]
"Crucially, the player does not know which of the past plays have contributed to this aggregated reward.",1. Introduction,[0],[0]
"We call this problem multi-armed bandits with delayed, aggregated anonymous feedback (MABDAAF).",1. Introduction,[0],[0]
"As in the standard MAB problem, in MABDAAF, the goal is to maximize the cumulative reward from T plays of the bandit, or equivalently to minimize the regret.",1. Introduction,[0],[0]
"The regret is the total difference between the reward of the optimal action and the actions taken.
",1. Introduction,[0],[0]
"If the delays are all zero, the MABDAAF problem reduces to the standard (stochastic) MAB problem, which has been studied considerably (e.g., Thompson, 1933; Lai & Robbins, 1985; Auer et al., 2002; Bubeck & Cesa-Bianchi,
2012).",1. Introduction,[0],[0]
"Compared to the MAB problem, the job of the player in our problem appears to be significantly more difficult since the player has to deal with (i) that some feedback from the previous pulls may be missing due to the delays, and (ii) that the feedback takes the form of the sum of an unknown number of rewards of unknown origin.
",1. Introduction,[0],[0]
"An easier problem is when the observations are delayed, but they are non-aggregated and non-anonymous: that is, the player has to only deal with challenge (i) and not (ii).",1. Introduction,[0],[0]
"Here, the player receives delayed feedback in the shape of action-reward pairs that inform the player of both the individual reward and which action generated it.",1. Introduction,[0],[0]
"This problem, which we shall call the (non-anonymous) delayed feedback bandit problem, has been studied by Joulani et al. (2013), and later followed up by Mandel et al. (2015) (for bounded delays).",1. Introduction,[0],[0]
"Remarkably, they show that compared to the standard (non-delayed) stochastic MAB setting, the regret will increase only additively by a factor that scales with the expected delay.",1. Introduction,[0],[0]
"For delay distributions with a finite expected delay, E[τ ], the worst case regret scales with O( √ KT log T + KE[τ ]).",1. Introduction,[0],[0]
"Hence, the price to pay for the delay in receiving the observations is negligible.",1. Introduction,[0],[0]
"QPM-D of Joulani et al. (2013) and SBD of Mandel et al. (2015) place received rewards into queues for each arm, taking one whenever a base bandit algorithm suggests playing the arm.",1. Introduction,[0],[0]
"Throughout, we take UCB1 (Auer et al., 2002) as the base algorithm in QPM-D. Joulani et al. (2013) also present a direct modification of the UCB1 algorithm.",1. Introduction,[0],[0]
All of these algorithms achieve the stated regret.,1. Introduction,[0],[0]
"None of them require any knowledge of the delay distributions, but they all rely heavily upon the non-anonymous nature of the observations.
",1. Introduction,[0],[0]
"While these results are encouraging, the assumption that the rewards are observed individually in a non-anonymous fashion is limiting for most practical applications with delays (e.g., recall the applications discussed earlier).",1. Introduction,[0],[0]
How big is the price to be paid for receiving only aggregated anonymous feedback?,1. Introduction,[0],[0]
Our main result is to prove that essentially there is no extra price to be paid provided that the value of the expected delay (or a bound on it) is available.,1. Introduction,[0],[0]
"In particular, this means that detailed knowledge of which action led to a particular delayed reward can be replaced by the much weaker requirement that the expected delay, or a bound on it, is known.",1. Introduction,[0],[0]
"Fig. 1 summarizes the relationship between the non-delayed, the delayed and the new problem
by showing the leading terms of the regret.",1. Introduction,[0],[0]
"In all cases, the dominant term is √ KT .",1. Introduction,[0],[0]
"Hence, asymptotically, the delayed, aggregated anonymous feedback problem is no more difficult than the standard multi-armed bandit problem.",1. Introduction,[0],[0]
We now consider what sort of algorithm will be able to achieve the aforementioned results for the MABDAAF problem.,1.1. Our Techniques and Results,[0],[0]
"Since the player only observes delayed, aggregated anonymous rewards, the first problem we face is how to even estimate the mean reward of individual actions.",1.1. Our Techniques and Results,[0],[0]
"Due to the delays and anonymity, it appears that to be able to estimate the mean reward of an action, the player wants to have played it consecutively for long stretches.",1.1. Our Techniques and Results,[0],[0]
"Indeed, if the stretches are sufficiently long compared to the mean delay, the observations received during the stretch will mostly consist of rewards of the action played in that stretch.",1.1. Our Techniques and Results,[0],[0]
"This naturally leads to considering algorithms that switch actions rarely and this is indeed the basis of our approach.
",1.1. Our Techniques and Results,[0],[0]
"Several popular MAB algorithms are based on choosing the action with the largest upper confidence bound (UCB) in each round (e.g., Auer et al., 2002; Cappé et al., 2013).",1.1. Our Techniques and Results,[0],[0]
UCB-style algorithms tend to switch arms frequently and will only play the optimal arm for long stretches if a unique optimal arm exists.,1.1. Our Techniques and Results,[0],[0]
"Therefore, for MABDAAF, we will consider alternative algorithms where arm-switching is more tightly controlled.",1.1. Our Techniques and Results,[0],[0]
The design of such algorithms goes back at least to the work of Agrawal et al. (1988) where the problem of bandits with switching costs was studied.,1.1. Our Techniques and Results,[0],[0]
The general idea of these rarely switching algorithms is to gradually eliminate suboptimal arms by playing arms in phases and comparing each arm’s upper confidence bound to the lower confidence bound of a leading arm at the end of each phase.,1.1. Our Techniques and Results,[0],[0]
"Generally, this sort of rarely switching algorithm switches arms onlyO(log T ) times.",1.1. Our Techniques and Results,[0],[0]
"We base our approach on one such algorithm, the so-called Improved UCB1 algorithm of Auer & Ortner (2010).
",1.1. Our Techniques and Results,[0],[0]
Using a rarely switching algorithm alone will not be sufficient for MABDAAF.,1.1. Our Techniques and Results,[0],[0]
"The remaining problem, and where the bulk of our contribution lies, is to construct appropri-
1The adjective “Improved” indicates that the algorithm improves upon the regret bounds achieved by UCB1.",1.1. Our Techniques and Results,[0],[0]
The improvement replaces log(T )/∆j,1.1. Our Techniques and Results,[0],[0]
by log(T∆2j )/∆j,1.1. Our Techniques and Results,[0],[0]
"in the regret bound.
ate confidence bounds and adjust the length of the periods of playing each arm to account for the delayed, aggregated anonymous feedback.",1.1. Our Techniques and Results,[0],[0]
"In particular, in the confidence bounds attention must be paid to fine details: it turns out that unless the variance of the observations is dealt with, there is a blow-up by a multiplicative factor of K. We avoid this by an improved analysis involving Freedman’s inequality (Freedman, 1975).",1.1. Our Techniques and Results,[0],[0]
"Further, to handle the dependencies between the number of plays of each arm and the past rewards, we combine Doob’s optimal skipping theorem (Doob, 1953) and Azuma-Hoeffding inequalities.",1.1. Our Techniques and Results,[0],[0]
Using a rarely switching algorithm for MABDAAF means we must also consider the dependencies between the elimination of arms in one phase and the corruption of observations in the next phase (ie. past plays can influence both whether an arm is still active and the corruption of its next plays).,1.1. Our Techniques and Results,[0],[0]
"We deal with this through careful algorithmic design.
",1.1. Our Techniques and Results,[0],[0]
"Using the above, we provide an algorithm that achieves worst case regret of O( √ KT logK + KE[τ ] log T ) using only knowledge of the expected delay, E[τ ].",1.1. Our Techniques and Results,[0],[0]
We then show that this regret can be improved by using a more careful martingale argument that exploits the fact that our algorithm is designed to remove most of the dependence between the corruption of future observations and elimination of arms.,1.1. Our Techniques and Results,[0],[0]
"Particularly, if the delays are bounded with known bound 0 ≤",1.1. Our Techniques and Results,[0],[0]
"d ≤ √ T/K, we can recover worst case regret ofO( √ KT logK+KE[τ ]), matching that of Joulani et al. (2013).",1.1. Our Techniques and Results,[0],[0]
"If the delays are unbounded but have known variance V(τ), we show that the problem independent regret can be reduced to O( √ KT logK +KE[τ ] +KV(τ)).",1.1. Our Techniques and Results,[0],[0]
We have already discussed several of the most relevant works to our own.,1.2. Related Work,[0],[0]
"However, there has also been other work looking at different flavors of the bandit problem with delayed (non-anonymous) feedback.",1.2. Related Work,[0],[0]
"For example, Neu et al. (2010) and Cesa-Bianchi et al. (2016) consider nonstochastic bandits with fixed constant delays; Dudik et al. (2011) look at stochastic contextual bandits with a constant delay and Desautels et al. (2014) consider Gaussian Process bandits with a bounded stochastic delay.",1.2. Related Work,[0],[0]
The general observation that delay causes an additive regret penalty in stochastic bandits and a multiplicative one in adversarial bandits is made in Joulani et al. (2013).,1.2. Related Work,[0],[0]
The empirical performance of K-armed stochastic bandit algorithms in delayed settings was investigated in Chapelle & Li (2011).,1.2. Related Work,[0],[0]
A further related problem is the ‘batched bandit’ problem studied by Perchet et al. (2016).,1.2. Related Work,[0],[0]
Here the player must fix a set of time points at which to collect feedback on all plays leading up to that point.,1.2. Related Work,[0],[0]
"Vernade et al. (2017) consider delayed Bernoulli bandits where some observations could also be censored (e.g., no conversion is ever actually observed if the delay exceeds some threshold) but require
complete knowledge of the delay distribution.",1.2. Related Work,[0],[0]
"Crucially, here and in all the aforementioned works, the feedback is always assumed to take the form of arm-reward pairs and knowledge of the assignment of rewards to arms underpins the suggested algorithms, rendering them unsuitable for MABDAAF.",1.2. Related Work,[0],[0]
"To the best of our knowledge, ours is the first work to develop algorithms to deal with delayed, aggregated anonymous feedback in the bandit setting.",1.2. Related Work,[0],[0]
The reminder of this paper is organized as follows: In the next section (Section 2) we give the formal problem definition.,1.3. Organization,[0],[0]
We present our algorithm in Section 3.,1.3. Organization,[0],[0]
"In Section 4, we discuss the performance of our algorithm under various delay assumptions; known expectation, bounded support with known bound and expectation, and known variance and expectation.",1.3. Organization,[0],[0]
This is followed by a numerical illustration of our results in Section 5.,1.3. Organization,[0],[0]
We conclude in Section 6.,1.3. Organization,[0],[0]
There are K > 1 actions or arms in the set A. Each action j ∈,2. Problem Definition,[0],[0]
A is associated with a reward distribution ζj and a delay distribution δj .,2. Problem Definition,[0],[0]
"The reward distribution is supported in [0, 1] and the delay distribution is supported on N .=",2. Problem Definition,[0],[0]
"{0, 1, . . .",2. Problem Definition,[0],[0]
}.,2. Problem Definition,[0],[0]
"We denote by µj the mean of ζj , µ∗ = µj∗ = maxj µj and define ∆j = µ∗ − µj to be the reward gap, that is the expected loss of reward each time action j is chosen instead of an optimal action.",2. Problem Definition,[0],[0]
"Let (Rl,j , τl,j)l∈N,j∈A be an infinite array of random variables defined on the probability space (Ω,Σ, P ) which are mutually independent.",2. Problem Definition,[0],[0]
"Further, Rl,j follows the distribution ζj and τl,j follows the distribution δj .",2. Problem Definition,[0],[0]
"The meaning of these random variables is that if the player plays action j at time l, a payoff of Rl,j will be added to the aggregated feedback that the player receives at the end of the (l + τl,j)th play.",2. Problem Definition,[0],[0]
"Formally, if Jl ∈ A denotes the action chosen by the player at time l = 1, 2, . . .",2. Problem Definition,[0],[0]
", then the observation received at the end of the tth play is
Xt = t∑ l=1",2. Problem Definition,[0],[0]
K∑ j=1,2. Problem Definition,[0],[0]
"Rl,j × I{l + τl,j = t, Jl = j}.
",2. Problem Definition,[0],[0]
"For the remainder, we will consider i.i.d. delays across arms.",2. Problem Definition,[0],[0]
"We also assume discrete delay distributions, although most results hold for continuous delays by redefining the event {τl,j = t− l} as {t− l− 1 < τl,j ≤ t− l} in Xt.",2. Problem Definition,[0],[0]
"In our analysis, we will sum over stochastic index sets.",2. Problem Definition,[0],[0]
For a stochastic index set I and random variables {Zn}n∈N we denote such sums as ∑ t∈I Zt .,2. Problem Definition,[0],[0]
= ∑ t∈N I{t ∈,2. Problem Definition,[0],[0]
"I} × Zt.
",2. Problem Definition,[0],[0]
Regret definition,2. Problem Definition,[0],[0]
"In most bandit problems, the regret is the cumulative loss due to not playing an optimal action.
",2. Problem Definition,[0],[0]
"In the case of delayed feedback, there are several possible ways to define the regret.",2. Problem Definition,[0],[0]
One option is to consider only the loss of the rewards received before horizon T (as in Vernade et al. (2017)).,2. Problem Definition,[0],[0]
"However, we will not use this definition.",2. Problem Definition,[0],[0]
"Instead, as in Joulani et al. (2013), we consider the loss of all generated rewards and define the (pseudo-)regret by
RT = T∑ t=1",2. Problem Definition,[0],[0]
(µ∗ − µJt) = Tµ∗,2. Problem Definition,[0],[0]
"− T∑ t=1 µJt .
",2. Problem Definition,[0],[0]
This includes the rewards received after the horizon T and does not penalize large delays as long as an optimal action is taken.,2. Problem Definition,[0],[0]
"This definition is natural since, in practice, the player should eventually receive all outstanding reward.
",2. Problem Definition,[0],[0]
"Lai & Robbins (1985) showed that the regret of any algorithm for the standard MAB problem must satisfy,
lim inf T→∞",2. Problem Definition,[0],[0]
"E[RT ] log(T )
",2. Problem Definition,[0],[0]
"≥ ∑
j:∆j>0
∆j",2. Problem Definition,[0],[0]
"KL(ζj , ζ∗) , (1)
where KL(ζj , ζ∗) is the KL-divergence between the reward distributions of arm j and an optimal arm.",2. Problem Definition,[0],[0]
Theorem 4 of Vernade et al. (2017) shows that the lower bound in (1) also holds for delayed feedback bandits with no censoring and their alternative definition of regret.,2. Problem Definition,[0],[0]
We therefore suspect (1) should hold for MABDAAF.,2. Problem Definition,[0],[0]
"However, due to the specific problem structure, finding a lower bound for MABDAAF is non-trivial and remains an open problem.
",2. Problem Definition,[0],[0]
"Assumptions on delay distribution For our algorithm for MABDAAF, we need some assumptions on the delay distribution.",2. Problem Definition,[0],[0]
"We assume that the expected delay, E[τ ], is bounded and known.",2. Problem Definition,[0],[0]
"This quantity is used in the algorithm.
",2. Problem Definition,[0],[0]
Assumption 1,2. Problem Definition,[0],[0]
"The expected delay E[τ ] is bounded and known to the algorithm.
",2. Problem Definition,[0],[0]
"We then show that under some further mild assumptions on the delay, we can obtain better algorithms with even more efficient regret guarantees.",2. Problem Definition,[0],[0]
"We consider two settings: delay distributions with bounded support, and bounded variance.
",2. Problem Definition,[0],[0]
Assumption 2 (Bounded support),2. Problem Definition,[0],[0]
There exists some constant d > 0 known to the algorithm,2. Problem Definition,[0],[0]
"such that the support of the delay distribution is bounded by d.
Assumption 3 (Bounded variance)",2. Problem Definition,[0],[0]
"The variance, V(τ), of the delay is bounded and known to the algorithm.
",2. Problem Definition,[0],[0]
In fact the known expected value and known variance assumption can be replaced by a ‘known upper bound’ on the expected value and variance respectively.,2. Problem Definition,[0],[0]
"However, for simplicity, in the remaining, we use E[τ ] and V(τ) directly.",2. Problem Definition,[0],[0]
The next sections provide algorithms and regret analysis for different combinations of the above assumptions.,2. Problem Definition,[0],[0]
Our algorithm is a phase-based elimination algorithm based on the Improved UCB algorithm by Auer & Ortner (2010).,3. Our Algorithm,[0],[0]
The general structure is as follows.,3. Our Algorithm,[0],[0]
"In each phase, each arm is played multiple times consecutively.",3. Our Algorithm,[0],[0]
"At the end of the phase, the observations received are used to update mean estimates, and any arm with an estimated mean below the best estimated mean by a gap larger than a ‘separation gap tolerance’ is eliminated.",3. Our Algorithm,[0],[0]
"This separation tolerance is decreased exponentially over phases, so that it is very small in later phases, eliminating all but the best arm(s) with high probability.",3. Our Algorithm,[0],[0]
"An alternative formulation of the algorithm is that at the end of a phase, any arm with an upper confidence bound lower than the best lower confidence bound is eliminated.",3. Our Algorithm,[0],[0]
"These confidence bounds are computed so that with high probability they are more (less) than the true mean, but within the separation gap tolerance.",3. Our Algorithm,[0],[0]
The phase lengths are then carefully chosen to ensure that the confidence bounds hold.,3. Our Algorithm,[0],[0]
"Here we assume that the horizon T is known, but we expect that this can be relaxed as in Auer & Ortner (2010).
",3. Our Algorithm,[0],[0]
"Algorithm overview Our algorithm, ODAAF, is given in Algorithm 1.",3. Our Algorithm,[0],[0]
"It operates in phases m = 1, 2, . .",3. Our Algorithm,[0],[0]
..,3. Our Algorithm,[0],[0]
"Define Am to be the set of active arms in phase m. The algorithm takes parameter nm which defines the number of samples of each active arm required by the end of phase m.
In Step 1 of phase m of the algorithm, each active arm j is played repeatedly for nm − nm−1 steps.",3. Our Algorithm,[0],[0]
We record all timesteps where arm j was played in the first m phases (excluding bridge periods) in the set Tj(m).,3. Our Algorithm,[0],[0]
The active arms are played in any arbitrary but fixed order.,3. Our Algorithm,[0],[0]
"In Step 2, the nm observations from timesteps in Tj(m) are averaged to obtain a new estimate X̄m,j of µj .",3. Our Algorithm,[0],[0]
"Arm j is eliminated if X̄m,j is further than ∆̃m from maxj′∈Am X̄m,j′ .
",3. Our Algorithm,[0],[0]
A further nuance in the algorithm structure is the ‘bridge period’ (see Figure 2).,3. Our Algorithm,[0],[0]
The algorithm picks an active arm j ∈ Am+1 to play in this bridge period for nm − nm−1 steps.,3. Our Algorithm,[0],[0]
"The observations received during the bridge period are discarded, and not used for computing confidence intervals.",3. Our Algorithm,[0],[0]
The significance of the bridge period is that it breaks the dependence between confidence intervals calculated in phasem and the delayed payoffs seeping into phasem+1.,3. Our Algorithm,[0],[0]
Without the bridge period this dependence would impair the validity of our confidence intervals.,3. Our Algorithm,[0],[0]
"However, we suspect that, in practice, it may be possible to remove it.
",3. Our Algorithm,[0],[0]
Choice of nm A key element of our algorithm design is the careful choice of nm.,3. Our Algorithm,[0],[0]
"Since nm determines the number of times each active (possibly suboptimal) arm is played, it clearly has an impact on the regret.",3. Our Algorithm,[0],[0]
"Furthermore, nm needs to be chosen so that the confidence bounds on the estimation error hold with given probability.",3. Our Algorithm,[0],[0]
"The main chal-
Algorithm 1 Optimism for Delayed, Aggregated Anonymous Feedback (ODAAF)",3. Our Algorithm,[0],[0]
"Input: A set of arms, A; a horizon, T ; choice of nm for
each phase m = 1, 2, . .",3. Our Algorithm,[0],[0]
..,3. Our Algorithm,[0],[0]
"Initialization: Set ∆̃1 = 1/2 (tolerance), the set of active
arms A1 = A. Let Ti(1) =",3. Our Algorithm,[0],[0]
"∅, i ∈",3. Our Algorithm,[0],[0]
"A, m = 1 (phase index), t = 1 (round index) while t ≤ T do
Step 1: Play arms.",3. Our Algorithm,[0],[0]
"for j ∈ Am do
Let Tj(m) =",3. Our Algorithm,[0],[0]
"Tj(m− 1) while |Tj(m)| ≤ nm and t ≤ T do
Play arm j, receive Xt.",3. Our Algorithm,[0],[0]
Add t to Tj(m).,3. Our Algorithm,[0],[0]
Increment t by 1. end while end for Step 2: Eliminate sub-optimal arms.,3. Our Algorithm,[0],[0]
"For every arm in j ∈ Am, compute X̄m,j as the average of observations at time steps t ∈ Tj(m).",3. Our Algorithm,[0],[0]
"That is,
X̄m,j = 1 |Tj(m)| ∑
t∈Tj(m)
",3. Our Algorithm,[0],[0]
"Xt .
",3. Our Algorithm,[0],[0]
"Construct Am+1 by eliminating actions j ∈ Am with
X̄m,j +",3. Our Algorithm,[0],[0]
"∆̃m < max j′∈Am X̄m,j′ .
",3. Our Algorithm,[0],[0]
Step 3:,3. Our Algorithm,[0],[0]
"Decrease Tolerance.
",3. Our Algorithm,[0],[0]
Set ∆̃m+1 = ∆̃m2 .,3. Our Algorithm,[0],[0]
Step 4: Bridge period.,3. Our Algorithm,[0],[0]
Pick an arm j ∈ Am+1 and play it νm = nm − nm−1 times while incrementing t ≤ T .,3. Our Algorithm,[0],[0]
Discard all observations from this period.,3. Our Algorithm,[0],[0]
Do not add t to Tj(m).,3. Our Algorithm,[0],[0]
"Increment phase index m.
end while
lenge is developing these confidence bounds from delayed, aggregated anonymous feedback.",3. Our Algorithm,[0],[0]
"Handling this form of feedback involves a credit assignment problem of deciding which samples can be used for a given arm’s mean estimation, since each sample is an aggregate of rewards from multiple previously played arms.",3. Our Algorithm,[0],[0]
This credit assignment problem would be hopeless in a passive learning setting without further information on how the samples were generated.,3. Our Algorithm,[0],[0]
"Our algorithm utilizes the power of active learning to design the phases in such a way that the feedback can be effectively ‘decensored’ without losing too many samples.
",3. Our Algorithm,[0],[0]
"A naive approach to defining the confidence bounds for delays bounded by a constant d ≥ 0 would be to observe that,∣∣∣∣ ∑
t∈Tj(m)\Tj(m−1)
",3. Our Algorithm,[0],[0]
Xt,3. Our Algorithm,[0],[0]
"− ∑
t∈Tj(m)\Tj(m−1)
Rt,j ∣∣∣∣ ≤ d,
since all rewards are in [0, 1].",3. Our Algorithm,[0],[0]
"Then we could use Hoeffding’s inequality to boundRt,Jt (see Appendix F) and select
nm = C1 log(T ∆̃
2 m)
∆̃2m + C2md ∆̃m
for some constants C1, C2.",3. Our Algorithm,[0],[0]
This corresponds to worst case regret of O( √ KT logK + K log(T )d).,3. Our Algorithm,[0],[0]
"For d E[τ ] and large T , this is significantly worse than that of Joulani et al. (2013).",3. Our Algorithm,[0],[0]
"In Section 4, we show that, surprisingly, it is possible to recover the same rate of regret as Joulani et al. (2013), but this requires a significantly more nuanced argument to get tighter confidence bounds and smaller nm.",3. Our Algorithm,[0],[0]
"In the next section, we describe this improved choice of nm for every phase m ∈ N and its implications on the regret, for each of the three cases mentioned previously: (i) Known and bounded expected delay (Assumption 1), (ii) Bounded delay with known bound and expected value (Assumptions 1 and 2), (iii) Delay with known and bounded variance and expectation (Assumptions 1 and 3).",3. Our Algorithm,[0],[0]
"In this section, we specify the choice of parameters nm and provide regret guarantees for Algorithm 1 for each of the three previously mentioned cases.",4. Regret Analysis,[0],[0]
"First, we consider the setting with the weakest assumption on delay distribution: we only assume that the expected delay, E[τ ], is bounded and known.",4.1. Known and Bounded Expected Delay,[0],[0]
No assumption on the support or variance of the delay distribution is made.,4.1. Known and Bounded Expected Delay,[0],[0]
"The regret analysis for this setting will not use the bridge period, so Step 4 of the algorithm could be omitted in this case.
",4.1. Known and Bounded Expected Delay,[0],[0]
"Choice of nm Here, we use Algorithm 1 with
nm = C1 log(T ∆̃
2 m) ∆̃2m + C2mE[τ ] ∆̃m (2)
for some large enough constants C1, C2.",4.1. Known and Bounded Expected Delay,[0],[0]
"The exact value of nm is given in Equation (14) in Appendix B.
Estimation of error bounds We bound the error between X̄m,j and µj by ∆̃m/2.",4.1. Known and Bounded Expected Delay,[0],[0]
"In order to do this we first bound the corruption of the observations received during timesteps Tj(m) due to delays.
",4.1. Known and Bounded Expected Delay,[0],[0]
Fix a phase m and arm j ∈ Am.,4.1. Known and Bounded Expected Delay,[0],[0]
Then the observations Xt in the period t ∈ Tj(m),4.1. Known and Bounded Expected Delay,[0],[0]
"\ Tj(m− 1) are composed of two types of rewards: a subset of rewards from plays of arm j in this period, and delayed rewards from some of the plays before this period.",4.1. Known and Bounded Expected Delay,[0],[0]
The expected value of observations from this period would be (nm − nm−1)µj but for the rewards entering and leaving this period due to delay.,4.1. Known and Bounded Expected Delay,[0],[0]
"Since the reward is bounded by 1, a simple observation is that expected discrepancy between the sum of observations in this period and the quantity (nm − nm−1)µj is bounded by the expected delay E[τ",4.1. Known and Bounded Expected Delay,[0],[0]
"],
E  ∑ t∈Tj(m)\Tj(m−1)",4.1. Known and Bounded Expected Delay,[0],[0]
(Xt − µj)  ≤ E[τ ].,4.1. Known and Bounded Expected Delay,[0],[0]
"(3) Summing this over phases ` = 1, . .",4.1. Known and Bounded Expected Delay,[0],[0]
".m gives a bound
|E[X̄m,j ]",4.1. Known and Bounded Expected Delay,[0],[0]
− µj | ≤ mE[τ ] |Tj(m)| = mE[τ ] nm .,4.1. Known and Bounded Expected Delay,[0],[0]
"(4)
Note that given the choice of nm in (2), the above is smaller than ∆̃m/2, when large enough constants are used.",4.1. Known and Bounded Expected Delay,[0],[0]
"Using this, along with concentration inequalities and the choice of nm from (2), we can obtain the following high probability bound.",4.1. Known and Bounded Expected Delay,[0],[0]
"A detailed proof is provided in Appendix B.1.
",4.1. Known and Bounded Expected Delay,[0],[0]
Lemma 1,4.1. Known and Bounded Expected Delay,[0],[0]
"Under Assumption 1 and the choice of nm given by (2), the estimates X̄m,j constructed by Algorithm 1 satisfy the following: For every fixed arm j and phase m, with probability 1− 3
T ∆̃2m , either j /∈",4.1. Known and Bounded Expected Delay,[0],[0]
"Am, or:
X̄m,j − µj ≤",4.1. Known and Bounded Expected Delay,[0],[0]
"∆̃m/2 .
",4.1. Known and Bounded Expected Delay,[0],[0]
"Regret bounds Using Lemma 1, we derive the following regret bounds in the current setting.
",4.1. Known and Bounded Expected Delay,[0],[0]
Theorem 2,4.1. Known and Bounded Expected Delay,[0],[0]
"Under Assumption 1, the expected regret of Algorithm 1 is upper bounded as
E[RT ] ≤ K∑ j=1 j 6=j∗ O ( log(T∆2j ) ∆j + log(1/∆j)E[τ ] ) .",4.1. Known and Bounded Expected Delay,[0],[0]
"(5)
Proof: Given Lemma 1, the proof of Theorem 2 closely follows the analysis of the Improved UCB algorithm of Auer & Ortner (2010).",4.1. Known and Bounded Expected Delay,[0],[0]
"Lemma 1 and the elimination condition in Algorithm 1 ensure that, with high probability, any suboptimal arm j will be eliminated by phase mj = log(1/∆j), thus incurring regret at most nmj∆j",4.1. Known and Bounded Expected Delay,[0],[0]
"We then substitute in nmj from (2), and sum over all suboptimal arms.",4.1. Known and Bounded Expected Delay,[0],[0]
A detailed proof is in Appendix B.2.,4.1. Known and Bounded Expected Delay,[0],[0]
"As in Auer & Ortner (2010), we avoid a union bound over all arms (which would result in an extra logK) by (i) reasoning about the regret of each arm individually, and (ii) bounding the regret resulting
from erroneously eliminating the optimal arm by carefully controlling the probability it is eliminated in each phase.
",4.1. Known and Bounded Expected Delay,[0],[0]
"Considering the worst-case values of ∆j (roughly √ K/T ), we obtain the following problem independent bound.
",4.1. Known and Bounded Expected Delay,[0],[0]
"Corollary 3 For any problem instance satisfying Assumption 1, the expected regret of Algorithm 1 satisfies
E[RT ] ≤",4.1. Known and Bounded Expected Delay,[0],[0]
O( √ KT log(K) +KE[τ ] log(T )).,4.1. Known and Bounded Expected Delay,[0],[0]
"If the delay is bounded by some constant d ≥ 0 and a single arm is played repeatedly for long enough, we can restrict the number of arms corrupting the observation",4.2. Delay with Bounded Support,[0],[0]
Xt at a given time t.,4.2. Delay with Bounded Support,[0],[0]
"In fact, if each arm j is played consecutively for more than d rounds, then at any time t ∈ Tj(m), the observation Xt will be composed of the rewards from at most two arms: the current arm j, and previous arm j′. Further, from the elimination condition, with high probability, arm j′ will have been eliminated if it is clearly suboptimal.",4.2. Delay with Bounded Support,[0],[0]
We can then recursively use the confidence bounds for arms j and j′ from the previous phase to bound |µj,4.2. Delay with Bounded Support,[0],[0]
− µj′ |.,4.2. Delay with Bounded Support,[0],[0]
"Below, we formalize this intuition to obtain a tighter bound on |X̄m,j − µj",4.2. Delay with Bounded Support,[0],[0]
|,4.2. Delay with Bounded Support,[0],[0]
"for every arm j and phase m, when each active arm is played a specified number of times per phase.
",4.2. Delay with Bounded Support,[0],[0]
"Choice of nm Here, we define,
nm = C1 log(T ∆̃
2 m) ∆̃2m + C2E[τ ]",4.2. Delay with Bounded Support,[0],[0]
"∆̃m (6)
+",4.2. Delay with Bounded Support,[0],[0]
"min { md, C3 log(T ∆̃ 2 m)
∆̃2m + C4mE[τ ] ∆̃m } for some large enough constants C1, C2, C3, C4 (see Appendix C, Equation (18) for the exact values).",4.2. Delay with Bounded Support,[0],[0]
"This choice of nm means that for large d, we essentially revert back to the choice of nm from (2) for the unbounded case, and we gain nothing by using the bound on the delay.",4.2. Delay with Bounded Support,[0],[0]
"However, if d is not large, the choice of nm in (6) is smaller than (2) since the second term now scales with E[τ ] rather than mE[τ ].
",4.2. Delay with Bounded Support,[0],[0]
"Estimation of error bounds In this setting, by the elimination condition and bounded delays, the expectation of each reward entering Tj(m) will be within ∆̃m−1 of µj , with high probability.",4.2. Delay with Bounded Support,[0],[0]
"Then, using knowledge of the upper bound of the support of τ , we can obtain a tighter bound and get an error bound similar to Lemma 1 with the smaller value of nm in (6).",4.2. Delay with Bounded Support,[0],[0]
We prove the following proposition.,4.2. Delay with Bounded Support,[0],[0]
"Since ∆̃m = 2−m, this is considerably tighter than (3).
",4.2. Delay with Bounded Support,[0],[0]
Proposition 4,4.2. Delay with Bounded Support,[0],[0]
"Assume ni − ni−1 ≥ d for phases i = 1, . . .",4.2. Delay with Bounded Support,[0],[0]
",m. Define Em−1 as the event that all arms j ∈",4.2. Delay with Bounded Support,[0],[0]
"Am satisfy error bounds |X̄m−1,j − µj | ≤ ∆̃m−1/2.",4.2. Delay with Bounded Support,[0],[0]
"Then, for
every arm j ∈",4.2. Delay with Bounded Support,[0],[0]
"Am,
E  ∑ t∈Tj(m)\Tj(m−1)",4.2. Delay with Bounded Support,[0],[0]
(Xt − µj) ∣∣∣∣Em−1  ≤ ∆̃m−1E[τ ].,4.2. Delay with Bounded Support,[0],[0]
Proof: (Sketch).,4.2. Delay with Bounded Support,[0],[0]
Consider a fixed arm j ∈ Am.,4.2. Delay with Bounded Support,[0],[0]
The expected value of the sum of observations Xt for t ∈ Tj(m),4.2. Delay with Bounded Support,[0],[0]
\ Tj(m− 1) would be (nm − nm−1)µj were it not for some rewards entering and leaving this period due to the delays.,4.2. Delay with Bounded Support,[0],[0]
"Because of the i.i.d. assumption on the delay, in expectation, the number of rewards leaving the period is roughly the same as the number of rewards entering this period, i.e., E[τ ].",4.2. Delay with Bounded Support,[0],[0]
(Conditioning on Em−1 does not effect this due to the bridge period).,4.2. Delay with Bounded Support,[0],[0]
"Since nm − nm−1 ≥ d, the reward coming into the period Tj(m)\Tj(m−1) can only be from the previous arm j′. All rewards leaving the period are from arm j. Therefore the expected difference between rewards entering and leaving the period is (µj − µj′)E[τ ].",4.2. Delay with Bounded Support,[0],[0]
"Then, if µj is close to µj′ , the total reward leaving the period is compensated by total reward entering.",4.2. Delay with Bounded Support,[0],[0]
"Due to the bridge period, even when j is the first arm played in phase m, j′ ∈ Am, so it was not eliminated in phase m − 1.",4.2. Delay with Bounded Support,[0],[0]
"By the elimination condition in Algorithm 1, if the error bounds |X̄m−1,j−µj | ≤ ∆̃m−1/2 are satisfied for all arms in Am, then |µj",4.2. Delay with Bounded Support,[0],[0]
− µj′ | ≤ ∆̃m−1.,4.2. Delay with Bounded Support,[0],[0]
"This gives the result.
",4.2. Delay with Bounded Support,[0],[0]
"Repeatedly using Proposition 4 we get,
m∑ i=1",4.2. Delay with Bounded Support,[0],[0]
E  ∑ t∈Tj(i)\Tj(i−1) (Xt − µj) ∣∣∣∣Ei−1  ≤ 2E[τ ] since ∑m i=1,4.2. Delay with Bounded Support,[0],[0]
"∆̃i−1 = ∑m−1 i=0 2
−i ≤ 2.",4.2. Delay with Bounded Support,[0],[0]
"Then, observe that P(ECi ) is small.",4.2. Delay with Bounded Support,[0],[0]
This bound is an improvement of a factor of m compared to (4).,4.2. Delay with Bounded Support,[0],[0]
"For the regret analysis, we derive a high probability version of the above result.",4.2. Delay with Bounded Support,[0],[0]
"Using this, and the choice of nm ≥ Ω ( log(T ∆̃2m)
",4.2. Delay with Bounded Support,[0],[0]
"∆̃2m + E[τ ] ∆̃m
) from (6), for
large enough constants, we derive the following lemma.",4.2. Delay with Bounded Support,[0],[0]
"A detailed proof is given in Appendix C.1.
",4.2. Delay with Bounded Support,[0],[0]
Lemma 5,4.2. Delay with Bounded Support,[0],[0]
"Under Assumptions 1 of known expected delay and 2 of bounded delays, and choice of nm given in (6), the estimates X̄m,j obtained by Algorithm 1 satisfy the following: For any arm j and phase m, with probability at least 1− 12
T ∆̃2m , either j /∈",4.2. Delay with Bounded Support,[0],[0]
"Am or
X̄m,j − µj ≤ ∆̃m/2.
",4.2. Delay with Bounded Support,[0],[0]
"Regret bounds We now give regret bounds for this case.
",4.2. Delay with Bounded Support,[0],[0]
Theorem 6,4.2. Delay with Bounded Support,[0],[0]
"Under Assumption 1 and bounded delay Assumption 2, the expected regret of Algorithm 1 satisfies
E[RT ]",4.2. Delay with Bounded Support,[0],[0]
"≤ K∑
j=1;j 6=j∗ O
( log(T∆2j )
∆j + E[τ",4.2. Delay with Bounded Support,[0],[0]
"]
+ min { d, log(T∆2j )
",4.2. Delay with Bounded Support,[0],[0]
"∆j + log(
1
∆j )",4.2. Delay with Bounded Support,[0],[0]
"E[τ ]
}) .
",4.2. Delay with Bounded Support,[0],[0]
Proof: (Sketch).,4.2. Delay with Bounded Support,[0],[0]
"Given Lemma 5, the proof is similar to that of Theorem 2.",4.2. Delay with Bounded Support,[0],[0]
"The full proof is in Appendix C.2.
",4.2. Delay with Bounded Support,[0],[0]
"Then, if d ≤ √
T logK K + E[τ ], we get the following
problem independent regret bound which matches that of Joulani et al. (2013).
",4.2. Delay with Bounded Support,[0],[0]
Corollary 7 For any problem instance satisfying Assumptions 1 and 2 with d ≤,4.2. Delay with Bounded Support,[0],[0]
"√ T logK K +E[τ ], the expected regret of Algorithm 1 satisfies
E[RT ] ≤",4.2. Delay with Bounded Support,[0],[0]
O( √ KT log(K) +KE[τ ]).,4.2. Delay with Bounded Support,[0],[0]
"If the delay is unbounded but well behaved in the sense that we know (a bound on) the variance, then we can obtain similar regret bounds to the bounded delay case.",4.3. Delay with Bounded Variance,[0],[0]
"Intuitively, delays from the previous phase will only corrupt observations in the current phase if their delays exceed the length of the bridge period.",4.3. Delay with Bounded Variance,[0],[0]
"We control this by using the bound on the variance to bound the tails of the delay distributions.
",4.3. Delay with Bounded Variance,[0],[0]
"Choice of nm Let V(τ) be the known variance (or bound on the variance) of the delay, as in Assumption 3.",4.3. Delay with Bounded Variance,[0],[0]
"Then, we use Algorithm 1 with the following value of nm,
nm = C1 log(T ∆̃2m)
∆̃2m + C2 E[τ ] + V(τ) ∆̃m
(7)
for some large enough constants C1, C2.",4.3. Delay with Bounded Variance,[0],[0]
"The exact value of nm is given in Appendix D, Equation (25).
",4.3. Delay with Bounded Variance,[0],[0]
"Regret bounds We get the following instance specific and problem independent regret bound in this case.
",4.3. Delay with Bounded Variance,[0],[0]
Theorem 8,4.3. Delay with Bounded Variance,[0],[0]
"Under Assumption 1 and Assumption 3 of known (bound on) the expectation and variance of the delay, and choice of nm from (7), the expected regret of Algorithm 1 can be upper bounded by,
E[RT ]",4.3. Delay with Bounded Variance,[0],[0]
"≤ K∑
j=1:µj 6=µ∗ O
( log(T∆2j )
∆j + E[τ ] + V(τ)
) .
",4.3. Delay with Bounded Variance,[0],[0]
Proof: (Sketch).,4.3. Delay with Bounded Variance,[0],[0]
See Appendix D.2.,4.3. Delay with Bounded Variance,[0],[0]
"We use Chebychev’s inequality to get a result similar to Lemma 5 and then use a similar argument to the bounded delay case.
",4.3. Delay with Bounded Variance,[0],[0]
"Corollary 9 For any problem instance satisfying Assumptions 1 and 3, the expected regret of Algorithm 1 satisfies
E[RT ] ≤",4.3. Delay with Bounded Variance,[0],[0]
"O( √ KT log(K) +KE[τ ] +KV(τ)).
",4.3. Delay with Bounded Variance,[0],[0]
"Remark If E[τ ] ≥ 1, then the delay penalty can be reduced to O(KE[τ ] +KV(τ)/E[τ ]) (see Appendix D).
",4.3. Delay with Bounded Variance,[0],[0]
"Thus, it is sufficient to know a bound on variance to obtain regret bounds similar to those in bounded delay case.",4.3. Delay with Bounded Variance,[0],[0]
Note that this approach is not possible just using knowledge of the expected delay since we cannot guarantee that the reward entering phase i is from an arm active in phase i− 1.,4.3. Delay with Bounded Variance,[0],[0]
"We compared the performance of our algorithm (under different assumptions) to QPM-D (Joulani et al., 2013) in various experimental settings.",5. Experimental Results,[0],[0]
"In these experiments, our aim was to investigate the effect of the delay on the performance of the algorithms.",5. Experimental Results,[0],[0]
"In order to focus on this, we used a simple setup of two arms with Bernoulli rewards and µ = (0.5, 0.6).",5. Experimental Results,[0],[0]
"In every experiment, we ran each algorithm to horizon T = 250000 and used UCB1 (Auer et al., 2002) as the base algorithm in QPM-D. The regret was averaged over 200 replications.",5. Experimental Results,[0],[0]
"For ease of reading, we define ODAAF to be our algorithm using only knowledge of the expected delay, with nm defined as in (2) and run without a bridge period, and ODAAF-B and ODAAF-V to be the versions of Algorithm 1 that use a bridge period and information on the bounded support and the finite variance of the delay to define nm as in (6) and (7) respectively.
",5. Experimental Results,[0],[0]
We tested the algorithms with different delay distributions.,5. Experimental Results,[0],[0]
"In the first case, we considered bounded delay distributions whereas in the second case, the delays were unbounded.",5. Experimental Results,[0],[0]
"In Fig. 3a, we plotted the ratios of the regret of ODAAF and ODAAF-B (with knowledge of d, the delay bound) to the regret of QPM-D. We see that in all cases the ratios converge to a constant.",5. Experimental Results,[0],[0]
"This shows that the regret of our algorithm is essentially of the same order as that of QPM-D. Our algorithm predetermines the number of times to play each active arm per phase (the randomness appears in whether an arm is active), so the jumps in the regret are it changing arm.",5. Experimental Results,[0],[0]
"This occurs at the same points in all replications.
",5. Experimental Results,[0],[0]
Fig.,5. Experimental Results,[0],[0]
3b shows a similar story for unbounded delays with mean E[τ ],5. Experimental Results,[0],[0]
= 50 (where N+ denotes the the half normal distribution).,5. Experimental Results,[0],[0]
The ratios of the regret of ODAAF and ODAAF-V (with knowledge of the delay variance) to the regret of QPM-D again converge to constants.,5. Experimental Results,[0],[0]
"Note that in this case, these constants, and the location of the jumps, vary with the delay distribution and V(τ).",5. Experimental Results,[0],[0]
"When the variance of the delay is small, it can be seen that using the variance information leads to improved performance.",5. Experimental Results,[0],[0]
"However, for exponential delays where V(τ) = E[τ ]2, the large variance causes nm to be large and so the suboptimal arm is played more, increasing the regret.",5. Experimental Results,[0],[0]
"In this case ODAAF-V had only just eliminated the suboptimal arm at time T .
",5. Experimental Results,[0],[0]
It can also be illustrated experimentally that the regret of our algorithms and that of QPM-D all increase linearly in E[τ ].,5. Experimental Results,[0],[0]
This is shown in Appendix E. We also provide an experimental comparison to Vernade et al. (2017) in Appendix E.,5. Experimental Results,[0],[0]
"We have studied an extension of the multi-armed bandit problem to bandits with delayed, aggregated anonymous feedback.",6. Conclusion,[0],[0]
"Here, a sum of observations is received after some stochastic delay and we do not learn which arms contributed to each observation.",6. Conclusion,[0],[0]
"In this more difficult setting, we have proven that, surprisingly, it is possible to develop an algorithm that performs comparably to those for the simpler delayed feedback bandits problem, where the assignment of rewards to plays is known.",6. Conclusion,[0],[0]
"Particularly, using only knowledge of the expected delay, our algorithm matches the worst case regret of Joulani et al. (2013) up to a logarithmic factor.",6. Conclusion,[0],[0]
"This logarithmic factors can be removed using an improved analysis and slightly more information about the delay; if the delay is bounded, we achieve the same worst case regret as Joulani et al. (2013), and for unbounded delays with known finite variance, we have an extra additive V(τ) term.",6. Conclusion,[0],[0]
We supported these claims experimentally.,6. Conclusion,[0],[0]
"Note that while our algorithm matches the order of regret of QPM-D, the constants are worse.",6. Conclusion,[0],[0]
"Hence, it is an open problem to find algorithms with better constants.",6. Conclusion,[0],[0]
CPB would like to thank the EPSRC funded EP/L015692/1 STOR-i centre for doctoral training and Sparx.,Acknowledgments,[0],[0]
We would like to thank the reviewers for their helpful comments.,Acknowledgments,[0],[0]
"We study a variant of the stochastic K-armed bandit problem, which we call “bandits with delayed, aggregated anonymous feedback”.",abstractText,[0],[0]
"In this problem, when the player pulls an arm, a reward is generated, however it is not immediately observed.",abstractText,[0],[0]
"Instead, at the end of each round the player observes only the sum of a number of previously generated rewards which happen to arrive in the given round.",abstractText,[0],[0]
"The rewards are stochastically delayed and due to the aggregated nature of the observations, the information of which arm led to a particular reward is lost.",abstractText,[0],[0]
"The question is what is the cost of the information loss due to this delayed, aggregated anonymous feedback?",abstractText,[0],[0]
"Previous works have studied bandits with stochastic, non-anonymous delays and found that the regret increases only by an additive factor relating to the expected delay.",abstractText,[0],[0]
"In this paper, we show that this additive regret increase can be maintained in the harder delayed, aggregated anonymous feedback setting when the expected delay (or a bound on it) is known.",abstractText,[0],[0]
"We provide an algorithm that matches the worst case regret of the non-anonymous problem exactly when the delays are bounded, and up to logarithmic factors or an additive variance term for unbounded delays.",abstractText,[0],[0]
"Bandits with Delayed, Aggregated Anonymous Feedback",title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3739–3748 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3739",text,[0],[0]
Single-document summarization methods can be divided into two categories: extractive and abstractive.,1 Introduction,[0],[0]
"Extractive summarization systems form summaries by selecting and copying text snippets from the document, while abstractive methods aim to generate concise summaries with paraphrasing.",1 Introduction,[0],[0]
"This work is primarily concerned with extractive
∗Equal contribution.
summarization.",1 Introduction,[0],[0]
"Though abstractive summarization methods have made strides in recent years, extractive techniques are still very attractive as they are simpler, faster, and more reliably yield semantically and grammatically correct sentences.
",1 Introduction,[0],[0]
"Many extractive summarizers work by selecting sentences from the input document (Luhn, 1958; Mihalcea and Tarau, 2004; Wong et al., 2008; Kågebäck et al., 2014; Yin and Pei, 2015; Cao et al., 2015; Yasunaga et al., 2017).",1 Introduction,[0],[0]
"Furthermore, a growing trend is to frame this sentence selection process as a sequential binary labeling problem, where binary inclusion/exclusion labels are chosen for sentences one at a time, starting from the beginning of the document, and decisions about later sentences may be conditioned on decisions about earlier sentences.",1 Introduction,[0],[0]
"Recurrent neural networks may be trained with stochastic gradient ascent to maximize the likelihood of a set of ground-truth binary label sequences (Cheng and Lapata, 2016; Nallapati et al., 2017).",1 Introduction,[0],[0]
"However, this approach has two well-recognized disadvantages.",1 Introduction,[0],[0]
"First, it suffers from exposure bias, a form of mismatch between training and testing data distributions which can hurt performance (Ranzato et al., 2015; Bahdanau et al., 2017; Paulus et al., 2018).",1 Introduction,[0],[0]
"Second, extractive labels must be generated by a heuristic, as summarization datasets do not generally include ground-truth extractive labels; the ultimate performance of models trained on such labels is thus fundamentally limited by the quality of the heuristic.
",1 Introduction,[0],[0]
"An alternative to maximum likelihood training
is to use reinforcement learning to train the model to directly maximize a measure of summary quality, such as the ROUGE score between the generated summary and a ground-truth abstractive summary (Wu and Hu, 2018).",1 Introduction,[0],[0]
"This approach has become popular because it avoids exposure bias, and directly optimizes a measure of summary quality.",1 Introduction,[0],[0]
"However, it also has a number of downsides.",1 Introduction,[0],[0]
"For one, the search space is quite large: for a document of length T , there are 2T possible extractive summaries.",1 Introduction,[0],[0]
This makes the exploration problem faced by the reinforcement learning algorithm during training very difficult.,1 Introduction,[0],[0]
"Another issue is that due to the sequential nature of selection, the model is inherently biased in favor of selecting earlier sentences over later ones, a phenomenon which we demonstrate empirically in Section 7.",1 Introduction,[0],[0]
"The first issue can be resolved to a degree using either a cumbersome maximum likelihood-based pre-training step (using heuristically-generated labels) (Wu and Hu, 2018), or placing a hard upper limit on the number of sentences selected.",1 Introduction,[0],[0]
"The second issue is more problematic, as it is inherent to the sequential binary labeling setting.
",1 Introduction,[0],[0]
"In the current work, we introduce BANDITSUM, a novel method for training neural network-based extractive summarizers with reinforcement learning.",1 Introduction,[0],[0]
"This method does away with the sequential binary labeling setting, instead formulating extractive summarization as a contextual bandit.",1 Introduction,[0],[0]
"This move greatly reduces the size of the space that must be explored, removes the need to perform supervised pre-training, and prevents systematically privileging earlier sentences over later ones.",1 Introduction,[0],[0]
"Although the strong performance of Lead-3 indicates that good sentences often occur early in the source article, we show in Sections 6 and 7 that the contextual bandit setting greatly improves model performance when good sentences occur late without sacrificing performance when good sentences occur early.
",1 Introduction,[0],[0]
"Under this reformulation, BANDITSUM takes the document as input and outputs an affinity for each of the sentences therein.",1 Introduction,[0],[0]
"An affinity is a real number in [0, 1] which quantifies the model’s propensity for including a sentence in the summary.",1 Introduction,[0],[0]
These affinities are then used in a process of repeated sampling-without-replacement which does not privilege earlier sentences over later ones.,1 Introduction,[0],[0]
"BANDITSUM is free to process the document as a whole before yielding affinities, which permits
affinities for different sentences in the document to depend on one another in arbitrary ways.",1 Introduction,[0],[0]
"In our technical section, we show how to apply policy gradient reinforcement learning methods to this setting.
",1 Introduction,[0],[0]
"The contributions of our work are as follows:
• We propose a theoretically grounded method, based on the contextual bandit formalism, for training neural network-based extractive summarizers with reinforcement learning.",1 Introduction,[0],[0]
"Based on this training method, we propose the BANDITSUM system for extractive summarization.
",1 Introduction,[0],[0]
"• We perform experiments demonstrating that BANDITSUM obtains state-of-the-art performance on a number of datasets and requires significantly fewer update steps than competing approaches.
",1 Introduction,[0],[0]
"• We perform human evaluations showing that in the eyes of human judges, summaries created by BANDITSUM are less redundant and of higher overall quality than summaries created by competing approaches.
",1 Introduction,[0],[0]
"• We provide evidence, in the form of experiments in which models are trained on subsets of the data, that the improved performance of BANDITSUM over competitors stems in part from better handling of summary-worthy sentences that come near the end of the document (see Section 7).",1 Introduction,[0],[0]
Extractive summarization has been widely studied in the past.,2 Related Work,[0],[0]
"Recently, neural network-based methods have been gaining popularity over classical methods (Luhn, 1958; Gong and Liu, 2001; Conroy and O’leary, 2001; Mihalcea and Tarau, 2004; Wong et al., 2008), as they have demonstrated stronger performance on large corpora.",2 Related Work,[0],[0]
Central to the neural network-based models is the encoderdecoder structure.,2 Related Work,[0],[0]
"These models typically use either a convolution neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Pei, 2015; Cao et al., 2015), a recurrent neural network (Chung et al., 2014; Cheng and Lapata, 2016; Nallapati et al., 2017), or a combination of the two (Narayan et al., 2018; Wu and Hu, 2018) to create sentence and document representations, using word embeddings (Mikolov et al., 2013; Pennington et al.,
2014) to represent words at the input level.",2 Related Work,[0],[0]
"These vectors are then fed into a decoder network to generate the output summary.
",2 Related Work,[0],[0]
"The use of reinforcement learning (RL) in extractive summarization was first explored by Ryang and Abekawa (2012), who proposed to use the TD(λ) algorithm to learn a value function for sentence selection.",2 Related Work,[0],[0]
Rioux et al. (2014) improved this framework by replacing the learning agent with another TD(λ) algorithm.,2 Related Work,[0],[0]
"However, the performance of their methods was limited by the use of shallow function approximators, which required performing a fresh round of reinforcement learning for every new document to be summarized.",2 Related Work,[0],[0]
"The more recent work of Paulus et al. (2018) and Wu and Hu (2018) use reinforcement learning in a sequential labeling setting to train abstractive and extractive summarizers, respectively, while Chen and Bansal (2018) combines both approaches, applying abstractive summarization to a set of sentences extracted by a pointer network (Vinyals et al., 2015) trained via REINFORCE.",2 Related Work,[0],[0]
"However, pre-training with a maximum likelihood objective is required in all of these models.
",2 Related Work,[0],[0]
The two works most similar to ours are Yao et al. (2018) and Narayan et al. (2018).,2 Related Work,[0],[0]
"Yao et al. (2018) recently proposed an extractive summarization approach based on deep Q learning, a type of reinforcement learning.",2 Related Work,[0],[0]
"However, their approach is extremely computationally intensive (a minimum of 10 days before convergence), and was unable to achieve ROUGE scores better than the best maximum likelihood-based approach.",2 Related Work,[0],[0]
"Narayan et al. (2018) uses a cascade of filters in order to arrive at a set of candidate extractive summaries, which we can regard as an approximation of the true action space.",2 Related Work,[0],[0]
They then use an approximation of a policy gradient method to train their neural network to select summaries from this approximated action space.,2 Related Work,[0],[0]
"In contrast, BANDITSUM samples directly from the true action space, and uses exact policy gradient parameter updates.",2 Related Work,[0],[0]
Our approach formulates extractive summarization as a contextual bandit which we then train an agent to solve using policy gradient reinforcement learning.,3 Extractive Summarization as a Contextual Bandit,[0],[0]
"A bandit is a decision-making formalization in which an agent repeatedly chooses one of several actions, and receives a reward based on
this choice.",3 Extractive Summarization as a Contextual Bandit,[0],[0]
"The agent’s goal is to quickly learn which action yields the most favorable distribution over rewards, and choose that action as often as possible.",3 Extractive Summarization as a Contextual Bandit,[0],[0]
"In a contextual bandit, at each trial, a context is sampled and shown to the agent, after which the agent selects an action and receives a reward; importantly, the rewards yielded by the actions may depend on the sampled context.",3 Extractive Summarization as a Contextual Bandit,[0],[0]
The agent must quickly learn which actions are favorable in which contexts.,3 Extractive Summarization as a Contextual Bandit,[0],[0]
"Contextual bandits are a subset of Markov Decision Processes in which every episode has length one.
",3 Extractive Summarization as a Contextual Bandit,[0],[0]
Extractive summarization may be regarded as a contextual bandit as follows.,3 Extractive Summarization as a Contextual Bandit,[0],[0]
"Each document is a context, and each ordered subset of a document’s sentences is a different action.",3 Extractive Summarization as a Contextual Bandit,[0],[0]
"Formally, assume that each context is a document d consisting of sentences s = (s1, . . .",3 Extractive Summarization as a Contextual Bandit,[0],[0]
", sNd), and that each action is a length-M sequence of unique sentence indices",3 Extractive Summarization as a Contextual Bandit,[0],[0]
"i = (i1, . . .",3 Extractive Summarization as a Contextual Bandit,[0],[0]
", iM ) where it ∈ {1, . . .",3 Extractive Summarization as a Contextual Bandit,[0],[0]
", Nd}, it 6=",3 Extractive Summarization as a Contextual Bandit,[0],[0]
"it′ for t 6= t′, and M is an integer hyper-parameter.",3 Extractive Summarization as a Contextual Bandit,[0],[0]
"For each i, the extractive summary induced by i is given by (si1 , . . .",3 Extractive Summarization as a Contextual Bandit,[0],[0]
", siM ).",3 Extractive Summarization as a Contextual Bandit,[0],[0]
"An action i taken in context d is given a reward R(i, a), where a is the gold-standard abstractive summary that is paired with document d, andR is a scalar reward function quantifying the degree of match between a and the summary induced by i.
A policy for extractive summarization is a neural network pθ(·|d), parameterized by a vector θ, which, for each input document d, yields a probability distribution over index sequences.",3 Extractive Summarization as a Contextual Bandit,[0],[0]
Our goal is to find parameters θ which cause pθ(·|d) to assign high probability to index sequences that induce extractive summaries that a human reader would judge to be of high-quality.,3 Extractive Summarization as a Contextual Bandit,[0],[0]
"We achieve this by maximizing the following objective function with respect to parameters θ:
J(θ) =",3 Extractive Summarization as a Contextual Bandit,[0],[0]
"E [R(i, a)] (1)
where the expectation is taken over documents d paired with gold-standard abstractive summaries a, as well as over index sequences i generated according to pθ(·|d).",3 Extractive Summarization as a Contextual Bandit,[0],[0]
"Ideally, we would like to maximize (1) using gradient ascent.",3.1 Policy Gradient Reinforcement Learning,[0],[0]
"However, the required gradient cannot be obtained using usual techniques (e.g. simple backpropagation) because i must be discretely sampled in order to compute R(i, a).
",3.1 Policy Gradient Reinforcement Learning,[0],[0]
"Fortunately, we can use the likelihood ratio gradient estimator from reinforcement learning and stochastic optimization (Williams, 1992; Sutton et al., 2000), which tells us that the gradient of this function can be computed as:
∇θJ(θ) = E",3.1 Policy Gradient Reinforcement Learning,[0],[0]
"[∇θ log pθ(i|d)R(i, a)]",3.1 Policy Gradient Reinforcement Learning,[0],[0]
"(2)
where the expectation is taken over the same variables as (1).
",3.1 Policy Gradient Reinforcement Learning,[0],[0]
"Since we typically do not know the exact document distribution and thus cannot evaluate the expected value in (2), we instead estimate it by sampling.",3.1 Policy Gradient Reinforcement Learning,[0],[0]
"We found that we obtained the best performance when, for each update, we first sample one document/summary pair (d, a), then sample B index sequences i1, . . .",3.1 Policy Gradient Reinforcement Learning,[0],[0]
", iB from pθ(·|d), and finally take the empirical average:
∇θJ(θ)",3.1 Policy Gradient Reinforcement Learning,[0],[0]
"≈ 1
B B∑ b=1",3.1 Policy Gradient Reinforcement Learning,[0],[0]
"∇θ log pθ(ib|d)R(ib, a) (3)
",3.1 Policy Gradient Reinforcement Learning,[0],[0]
"This overall learning algorithm can be regarded as an instance of the REINFORCE policy gradient algorithm (Williams, 1992).",3.1 Policy Gradient Reinforcement Learning,[0],[0]
There are many possible choices for the structure of pθ(·|d); we opt for one that avoids privileging early sentences over later ones.,3.2 Structure of pθ(·|d),[0],[0]
"We first decompose pθ(·|d) into two parts: πθ, a deterministic function which contains all the network’s parameters, and µ, a probability distribution parameterized by the output of πθ.",3.2 Structure of pθ(·|d),[0],[0]
"Concretely:
pθ(·|d) = µ(·|πθ(d)) (4)
",3.2 Structure of pθ(·|d),[0],[0]
"Given an input document d, πθ outputs a realvalued vector of sentence affinities whose length is equal to the number of sentences in the document (i.e. πθ(d) ∈ RNd) and whose elements fall in the range [0, 1].",3.2 Structure of pθ(·|d),[0],[0]
"The t-th entry π(d)t may be roughly interpreted as the network’s propensity to include sentence st in the summary of d.
",3.2 Structure of pθ(·|d),[0],[0]
"Given sentence affinities πθ(d), µ implements a process of repeated sampling-withoutreplacement.",3.2 Structure of pθ(·|d),[0],[0]
"This proceeds by repeatedly normalizing the set of affinities corresponding to sentences that have not yet been selected, thereby obtaining a probability distribution over unselected sentences, and sampling from that distribution to obtain a new sentence to include.",3.2 Structure of pθ(·|d),[0],[0]
"This normalizeand-sample step is repeated M times, yielding M unique sentences to include in the summary.
",3.2 Structure of pθ(·|d),[0],[0]
"At each step of sampling-without-replacement, we also include a small probability of sampling uniformly from all remaining sentences.",3.2 Structure of pθ(·|d),[0],[0]
"This is used to achieve adequate exploration during training, and is similar to the -greedy technique from reinforcement learning.
",3.2 Structure of pθ(·|d),[0],[0]
"Under this sampling scheme, we have the following expression for pθ(i|d):
M∏ j=1
(
Nd − j + 1 + (1− )π(d)ij z(d)− ∑j−1",3.2 Structure of pθ(·|d),[0],[0]
"k=1 π(d)ik
) (5)
where z(d) = ∑
t π(d)t.",3.2 Structure of pθ(·|d),[0],[0]
"For index sequences that have length different from M , or that contain duplicate indices, we have pθ(i|d) = 0.",3.2 Structure of pθ(·|d),[0],[0]
"Using this expression, it is straightforward to use automatic differentiation software to compute ∇θ log pθ(i|d), which is required for the gradient estimate in (3).",3.2 Structure of pθ(·|d),[0],[0]
"Our sample-based gradient estimate can have high variance, which can slow the learning.",3.3 Baseline for Variance Reduction,[0],[0]
"One potential cause of this high variance can be seen by inspecting (3), and noting that it basically acts to change the probability of a sampled index sequence to an extent determined by the reward R(i, a).",3.3 Baseline for Variance Reduction,[0],[0]
"However, since ROUGE scores are always positive, the probability of every sampled index sequence is increased, whereas intuitively, we would prefer to decrease the probability of sequences that receive a comparatively low reward, even if it is positive.",3.3 Baseline for Variance Reduction,[0],[0]
"This can be remedied by the introduction of a so-called baseline which is subtracted from all rewards.
",3.3 Baseline for Variance Reduction,[0],[0]
"Using a baseline r, our sample-based estimate of∇θJ(θ) becomes:
1
B B∑ i=1",3.3 Baseline for Variance Reduction,[0],[0]
"∇θ log pθ(ib|d)(R(ib, a)− r) (6)
",3.3 Baseline for Variance Reduction,[0],[0]
"It can be shown that the introduction of r does not bias the gradient estimator and can significantly reduce its variance if chosen appropriately (Sutton et al., 2000).
",3.3 Baseline for Variance Reduction,[0],[0]
"There are several possibilities for the baseline, including the long-term average reward and the average reward across different samples for one document-summary pair.",3.3 Baseline for Variance Reduction,[0],[0]
"We choose an approach known as self-critical reinforcement learning, in which the test-time performance of the current model is used as the baseline (Ranzato et al., 2015;
Rennie et al., 2017; Paulus et al., 2018).",3.3 Baseline for Variance Reduction,[0],[0]
"More concretely, after sampling the document-summary pair (d, a), we greedily generate an index sequence using the current parameters θ:
igreedy =",3.3 Baseline for Variance Reduction,[0],[0]
argmax,3.3 Baseline for Variance Reduction,[0],[0]
"i
pθ(i|d) (7)
and calculate the baseline for the current update as r = R(igreedy, a).",3.3 Baseline for Variance Reduction,[0],[0]
This baseline has the intuitively satisfying property of only increasing the probability of a sampled label sequence when the summary it induces is better than what would be obtained by greedy decoding.,3.3 Baseline for Variance Reduction,[0],[0]
"A final consideration is a concrete choice for the reward function R(i, a).",3.4 Reward Function,[0],[0]
"Throughout this work we use:
R(i, a) = 1
3 (ROUGE-1f (i, a) +
ROUGE-2f (i, a) +",3.4 Reward Function,[0],[0]
"ROUGE-Lf (i, a)).",3.4 Reward Function,[0],[0]
"(8)
The above reward function optimizes the average of all the ROUGE variants (Lin, 2004) while balancing precision and recall.",3.4 Reward Function,[0],[0]
"In this section, we discuss the concrete instantiations of the neural network πθ that we use in our experiments.",4 Model,[0],[0]
"We break πθ up into two components: a document encoder fθ1, which outputs a sequence of sentence feature vectors (h1, . . .",4 Model,[0],[0]
", hNd) and a decoder gθ2 which yields sentence affinities:
h1, . . .",4 Model,[0],[0]
", hNd = fθ1(d) (9)
πθ(d) = gθ2(h1, . . .",4 Model,[0],[0]
", hNd) (10)
Encoder.",4 Model,[0],[0]
"Features for each sentence in isolation are first obtained by applying a word-level Bidirectional Recurrent Neural Network (BiRNN) to the embeddings for the words in the sentence, and averaging the hidden states over words.",4 Model,[0],[0]
A separate sentence-level BiRNN is then used to obtain a representations hi for each sentence in the context of the document.,4 Model,[0],[0]
Decoder.,4 Model,[0],[0]
"A multi-layer perceptron is used to map from the representation ht of each sentence through a final sigmoid unit to yield sentence affinities πθ(d).
",4 Model,[0],[0]
"The use of a bidirectional recurrent network in the encoder is crucial, as it allows the network to
process the document as a whole, yielding representations for each sentence that take all other sentences into account.",4 Model,[0],[0]
"This procedure is necessary to deal with some aspects of summary quality such as redundancy (avoiding the inclusion of multiple sentences with similar meaning), which requires the affinities for different sentences to depend on one another.",4 Model,[0],[0]
"For example, to avoid redundancy, if the affinity for some sentence is high, then sentences which express similar meaning should have low affinities.",4 Model,[0],[0]
"In this section, we discuss the setup of our experiments.",5 Experiments,[0],[0]
We first discuss the corpora that we used and our evaluation methodology.,5 Experiments,[0],[0]
"We then discuss the baseline methods against which we compared, and conclude with a detailed overview of the settings of the model parameters.",5 Experiments,[0],[0]
"Three datasets are used for our experiments: the CNN, the Daily Mail, and combined CNN/Daily Mail (Hermann et al., 2015; Nallapati et al., 2016).",5.1 Corpora,[0],[0]
"We use the standard split of Hermann et al. (2015) for training, validating, and testing and the same setting without anonymization on the three corpus as See et al. (2017).",5.1 Corpora,[0],[0]
"The Daily Mail corpus has 196,557 training documents, 12,147 validation documents and 10,397 test documents; while the CNN corpus has 90,266/1,220/1,093 documents, respectively.",5.1 Corpora,[0],[0]
"The models are evaluated based on ROUGE (Lin, 2004).",5.2 Evaluation,[0],[0]
We obtain our ROUGE scores using the standard pyrouge package1 for the test set evaluation and a faster python implementation of the ROUGE metric2 for training and evaluating on the validation set.,5.2 Evaluation,[0],[0]
"We report the F1 scores of ROUGE1, ROUGE-2, and ROUGE-L, which compute the uniform, bigram, and longest common subsequence overlapping with the reference summaries.",5.2 Evaluation,[0],[0]
"We compare BANDITSUM with other extractive methods including: the Lead-3 model, SummaRuNNer (Nallapati et al., 2017), Refresh
1https://pypi.python.org/pypi/pyrouge/ 0.1.3
2We use the modified version based on https:// github.com/pltrdy/rouge
(Narayan et al., 2018), RNES (Wu and Hu, 2018), DQN (Yao et al., 2018), and NN-SE (Cheng and Lapata, 2016).",5.3 Baselines,[0],[0]
The Lead-3 model simply produces the leading three sentences of the document as the summary.,5.3 Baselines,[0],[0]
"We use 100-dimensional Glove embeddings (Pennington et al., 2014) as our embedding initialization.",5.4 Model Settings,[0],[0]
"We do not limit the sentence length, nor the maximum number of sentences per document.",5.4 Model Settings,[0],[0]
"We use one-layer BiLSTM for word-level RNN, and two-layers BiLSTM for sentence-level RNN.",5.4 Model Settings,[0],[0]
The hidden state dimension is 200 for each direction on all LSTMs.,5.4 Model Settings,[0],[0]
"For the decoder, we use a feedforward network with one hidden layer of dimension 100.
",5.4 Model Settings,[0],[0]
"During training, we use Adam (Kingma and Ba, 2015) as the optimizer with the learning rate of 5e−5, beta parameters (0, 0.999), and a weight decay of 1e−6, to maximize the objective function defined in equation (1).",5.4 Model Settings,[0],[0]
We employ gradient clipping of 1 to regularize our model.,5.4 Model Settings,[0],[0]
"At each iteration, we sample B = 20 times to estimate the gradient defined in equation 3.",5.4 Model Settings,[0],[0]
"For our system, the reported performance is obtained within two epochs of training 3.
",5.4 Model Settings,[0],[0]
"At the test time, we pick sentences sorted by the predicted probabilities until the length limit is reached.",5.4 Model Settings,[0],[0]
The full-length ROUGE F1 score is used as the evaluation metric.,5.4 Model Settings,[0],[0]
"For M , the number of sentences selected per summary, we use a value of 3, based on our validation results as well as on the settings described in Nallapati et al. (2017).",5.4 Model Settings,[0],[0]
"In this section, we present quantitative results from the ROUGE evaluation and qualitative results based on human evaluation.",6 Experiment Results,[0],[0]
"In addition, we demonstrate the stability of our RL model by comparing the validation curve of BANDITSUM with SummaRuNNer (Nallapati et al., 2017) trained with a maximum likelihood objective.",6 Experiment Results,[0],[0]
"We present the results of comparing BANDITSUM to several baseline algorithms4 on the CNN/Daily
3Our code can be found at https://github.com/ yuedongP/summarization_RL
4 Due to different pre-processing methods and different numbers of selected sentences, several papers report different Lead scores (Narayan et al., 2018; See et al., 2017).",6.1 Rouge Evaluation,[0],[0]
"We use
Mail corpus in Tables 1 and 2.",6.1 Rouge Evaluation,[0],[0]
"Compared to other extractive summarization systems, BANDITSUM achieves performance that is significantly better than two RL-based approaches, Refresh (Narayan et al., 2018) and DQN (Yao et al., 2018), as well as SummaRuNNer, the state-of-the-art maximum liklihood-based extractive summarizer (Nallapati et al., 2017).",6.1 Rouge Evaluation,[0],[0]
"BANDITSUM performs a little better than RNES (Wu and Hu, 2018) in terms of ROUGE-1 and slightly worse in terms of ROUGE2.",6.1 Rouge Evaluation,[0],[0]
"However, RNES requires pre-training with the maximum likelihood objective on heuristicallygenerated extractive labels; in contrast, BANDITSUM is very light-weight and converges significantly faster.",6.1 Rouge Evaluation,[0],[0]
"We discuss the advantage of framing the extractive summarization based on the contextual bandit (BANDITSUM) over the sequential binary labeling setting (RNES) in the discussion Section 7.
",6.1 Rouge Evaluation,[0],[0]
"We also noticed that different choices for the policy gradient baseline (see Section 3.3) in BANDITSUM affect learning speed, but do not significantly affect asymptotic performance.",6.1 Rouge Evaluation,[0],[0]
"Models trained with an average reward baseline learned most quickly, while models trained with three different baselines (greedy, average reward in a
the test set provided by Narayan et al. (2018).",6.1 Rouge Evaluation,[0],[0]
"Since their Lead score is a combination of Lead-3 for CNN and Lead4 for Daily Mail, we recompute the Lead-3 scores for both CNN and Daily Mail with the preprocessing steps used in See et al. (2017).",6.1 Rouge Evaluation,[0],[0]
"Additionally, our results are not directly comparable to results based on the anonymized dataset used by Nallapati et al. (2017).
batch, average global reward) all perform roughly the same after training for one epoch.",6.1 Rouge Evaluation,[0],[0]
Models trained without a baseline were found to underperform other baseline choices by about 2 points of ROUGE score on average.,6.1 Rouge Evaluation,[0],[0]
We also conduct a qualitative evaluation to understand the effects of the improvements introduced in BANDITSUM on human judgments of the generated summaries.,6.2 Human Evaluation,[0],[0]
"To assess the effect of training with RL rather than maximum likelihood, in the first set of human evaluations we compare BANDITSUM with the state-of-the-art maximum likelihood-based model SummaRuNNer.",6.2 Human Evaluation,[0],[0]
"To evaluate the importance of using an exact, rather than approximate, policy gradient to optimize ROUGE scores, we perform another human evaluation comparing BANDITSUM and Refresh, an RL-based method that uses the an approximation of the policy gradient.
",6.2 Human Evaluation,[0],[0]
We follow a human evaluation protocol similar to the one used in Wu and Hu (2018).,6.2 Human Evaluation,[0],[0]
"Given a set of N documents, we ask K volunteers to evaluate the summaries extracted by both systems.",6.2 Human Evaluation,[0],[0]
"For each document, a reference summary, and a pair of randomly ordered extractive summaries (one generated by each of the two models) is presented to the volunteers.",6.2 Human Evaluation,[0],[0]
"They are asked to compare and rank the extracted summaries along three dimensions: overall, coverage, and non-redundancy.
",6.2 Human Evaluation,[0],[0]
"To compare with SummaRuNNer, we randomly sample 57 documents from the test set of Daily-
Mail and ask 5 volunteers to evaluate the extracted summaries.",6.2 Human Evaluation,[0],[0]
"While comparing with Refresh, we use the 20 documents (10 CNN and 10 DailyMail) provided by Narayan et al. (2018) to 4 volunteers.",6.2 Human Evaluation,[0],[0]
Tables 3 and 4 show the results of human evaluation in these two settings.,6.2 Human Evaluation,[0],[0]
BANDITSUM is shown to be better than Refresh and SummaRuNNer in terms of overall quality and nonredundancy.,6.2 Human Evaluation,[0],[0]
"These results indicate that the use of the true policy gradient, rather than the approximation used by Refresh, improves overall quality.",6.2 Human Evaluation,[0],[0]
"It is interesting to observe that, even though BANDITSUM does not have an explicit redundancy avoidance mechanism, it actually outperforms the other systems on non-redundancy.",6.2 Human Evaluation,[0],[0]
Reinforcement learning methods are known for sometimes being unstable during training.,6.3 Learning Curve,[0],[0]
"However, this seems to be less of a problem for BANDITSUM, perhaps because it is formulated as a contextual bandit rather than a sequential labeling problem.",6.3 Learning Curve,[0],[0]
"We show this by comparing the validation curves generated by BANDITSUM and the state-of-the-art maximum likelihood-based model – SummaRuNNer (Nallapati et al., 2017) (Figure 1).
",6.3 Learning Curve,[0],[0]
"From Figure 1, we observe that BANDITSUM converges significantly more quickly to good results than SummaRuNNer.",6.3 Learning Curve,[0],[0]
"Moreover, there is less variance in the performance of BANDITSUM.
",6.3 Learning Curve,[0],[0]
One possible reason is that extractive summarization does not have well-defined supervised labels.,6.3 Learning Curve,[0],[0]
There exists a mismatch between the provided labels and human-generated abstractive summaries.,6.3 Learning Curve,[0],[0]
"Hence, the gradient, computed from the maximum likelihood loss function, is not optimizing the evaluation metric of interest.",6.3 Learning Curve,[0],[0]
"Another important message is that both models are still far from the estimated upper bound5, which shows that there is still significant room for improvement.",6.3 Learning Curve,[0],[0]
"On CNN/Daily mail dataset, our model’s timeper-epoch is about 25.5 hours on a TITAN Xp.",6.4 Run Time,[0],[0]
"We trained the model for 3 epochs, which took about 76 hours in total.",6.4 Run Time,[0],[0]
"For comparison, DQN took about 10 days to train on a GTX 1080 (Yao et al., 2018).",6.4 Run Time,[0],[0]
"Refresh took about 12 hours on a single GPU to train (Narayan et al., 2018).",6.4 Run Time,[0],[0]
Note that this figure does not take into account the significant time required by Refresh for pre-computing ROUGE scores.,6.4 Run Time,[0],[0]
"We conjecture that the contextual bandit (CB) setting is a more suitable framework for modeling extractive summarization than the sequential binary labeling setting, especially in the cases when good summary sentences appear later in the document.",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"The intuition behind this is that models based on the sequential labeling setting are affected by the order of the decisions, which biases towards selecting sentences that appear earlier in the document.",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"By contrast, our CB-based RL model has more flexibility and freedom to explore the search space, as it samples the sentences without replacement based on the affinity scores.",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"Note that although we do not explicitly make the selection decisions in a sequential fashion, the sequential information about dependencies between sentences is implicitly embedded in the affinity scores, which are produced by bidirectional RNNs.
",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"We provide empirical evidence for this conjecture by comparing BANDITSUM to the sequential RL model proposed by Wu and Hu (2018) (Figure 2) on two subsets of the data: one with good
5The supervised labels for the upper bound estimation are obtained using the heuristic described in Nallapati et al. (2017).
summary sentences appearing early in the article, while the other contains articles where good summary sentences appear late.",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"Specifically, we construct two evaluation datasets by selecting the first 50 documents (Dearly, i.e., best summary occurs early) and the last 50 documents (Dlate, i.e., best summary occurs late) from a sample of 1000 documents that is ordered by the average extractive label index idx.",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"Given an article with n sentences indexed from 1, . . .",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
", n and a greedy extractive labels set with three sentences (i, j, k)6, the average index for the extractive label is computed by idx= (i+ j + k)/3n.
",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"Given these two subsets of the data, three different models (BANDITSUM, RNES and RNES3) are trained and evaluated on each of the two datasets without extractive labels.",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"Since the original sequential RL model (RNES) is unstable without supervised pre-training, we propose the RNES3 model that is limited to select no more then three sentences.",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"Starting with random initializations without supervised pre-training, we train each model ten times for 100 epochs and plot the learning curve of the average ROUGE-F1 score computed based on the trained model in Figure 2.",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"We can clearly see that BANDITSUM finds a better so-
6For each document, a length-3 extractive summary with near-optimal ROUGE score is selected following the heuristic proposed by Nallapati et al. (2017).
lution more quickly than RNES and RNES3 on both datasets.",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"Moreover, it displays a significantly speed-up in the exploration and finds the best solution when good summary sentences appeared later in the document (Dlate).",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"In this work, we presented a contextual bandit learning framework, BANDITSUM , for extractive summarization, based on neural networks and reinforcement learning algorithms.",8 Conclusion,[0],[0]
BANDITSUM does not require sentence-level extractive labels and optimizes ROUGE scores between summaries generated by the model and abstractive reference summaries.,8 Conclusion,[0],[0]
"Empirical results show that our method performs better than or comparable to state-of-the-art extractive summarization models which must be pre-trained on extractive labels, and converges using significantly fewer update steps than competing approaches.",8 Conclusion,[0],[0]
"In future work, we will explore the direction of adding an extra coherence reward (Wu and Hu, 2018) to improve the quality of extracted summaries in terms of sentence discourse relation.",8 Conclusion,[0],[0]
The research was supported in part by Natural Sciences and Engineering Research Council of Canada (NSERC).,Acknowledgements,[0],[0]
The authors would like to thank Compute Canada for providing the computational resources.,Acknowledgements,[0],[0]
"In this work, we propose a novel method for training neural networks to perform singledocument extractive summarization without heuristically-generated extractive labels.",abstractText,[0],[0]
"We call our approach BANDITSUM as it treats extractive summarization as a contextual bandit (CB) problem, where the model receives a document to summarize (the context), and chooses a sequence of sentences to include in the summary (the action).",abstractText,[0],[0]
A policy gradient reinforcement learning algorithm is used to train the model to select sequences of sentences that maximize ROUGE score.,abstractText,[0],[0]
"We perform a series of experiments demonstrating that BANDITSUM is able to achieve ROUGE scores that are better than or comparable to the state-of-the-art for extractive summarization, and converges using significantly fewer update steps than competing approaches.",abstractText,[0],[0]
"In addition, we show empirically that BANDITSUM performs significantly better than competing approaches when good summary sentences appear late in the source document.",abstractText,[0],[0]
BANDITSUM: Extractive Summarization as a Contextual Bandit,title,[0],[0]
The advancement of modern society is driven by the development of Integrated Circuits (IC).,1. Introduction,[0],[0]
"Unlike the digital circuits where the design flow is already highly automated, the automation of analog circuit design is still a challenging problem.
",1. Introduction,[0],[0]
"Traditionally, the design parameters of analog circuits like widths and lengths of transistors are manually calculated by designers with their experience and the understanding of the design specifications.",1. Introduction,[0],[0]
"However, due to the progress
1State Key Lab of ASIC and System, School of Microelectronics, Fudan University, Shanghai, China 2Department of Electrical Engineering, University of Texas at Dallas, Richardson, TX, U.S.A. Correspondence to: Fan Yang <yangfan@fudan.edu.cn>, Xuan Zeng <xzeng@fudan.edu.cn>.
Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"of IC manufacture technology forecasted by Moore’s law, the circuit devices become more and more complicated, and the parasitic effect of the circuits can no longer be ignored.",1. Introduction,[0],[0]
"On the other hand, the demands for high-performance, lowpower analog circuits are increasing.",1. Introduction,[0],[0]
It is much more difficult to meet the performance and time-to-market requirements with manual circuit design.,1. Introduction,[0],[0]
"Automated analog circuit design has thus attracted much research interest in the past decade (Rutenbar et al., 2007).
",1. Introduction,[0],[0]
The analog circuit design automation problems can be formulated as optimization problems.,1. Introduction,[0],[0]
"The aim is to find the optimal design parameters that provide the best circuit performance, which can be represented by a figure of merit (FOM) real-valued function.",1. Introduction,[0],[0]
"Prior works about analog circuit optimization include offline model-based approaches (Colleran et al., 2003; Daems et al., 2003; Wang et al., 2014) and simulation-based approaches.",1. Introduction,[0],[0]
The offline model-based methods try to build global models of the FOM via manual calculation or regression with simulated data and then optimize the cheap-to-evaluate models.,1. Introduction,[0],[0]
The problem with this approach is that the accurate models are usually hard to get.,1. Introduction,[0],[0]
"For example, in Wang et al. (2014), 100,000 randomly simulated points are used to train a sparse polynomial model for an amplifier circuit with ten design parameters.
",1. Introduction,[0],[0]
"Simulation-based methods, instead, treat the performances of the circuits as black-box functions.",1. Introduction,[0],[0]
The performances are obtained from circuit simulations.,1. Introduction,[0],[0]
Global optimization algorithms are directly applied to the black-box functions.,1. Introduction,[0],[0]
"For simulation-based circuit optimization methods, metaheuristic algorithms (Phelps et al., 2000; Liu et al., 2009) are widely used.",1. Introduction,[0],[0]
"Although these algorithms can explore the whole design space, they have relatively low convergence rate.",1. Introduction,[0],[0]
"When the circuit simulation takes a long time, both model-based and simulation-based approaches can be very time-consuming.
",1. Introduction,[0],[0]
"In recent years, the Gaussian process (GP) (Rasmussen, 2006) model has been introduced for the automated design of analog circuits to reduce the required number of circuit simulations.",1. Introduction,[0],[0]
"In Liu et al. (2014), GP is combined with differential evolution algorithm.",1. Introduction,[0],[0]
"Recently, Bayesian optimization (BO) (Shahriari et al., 2016) algorithm has also been applied for analog circuit optimization.",1. Introduction,[0],[0]
"In Lyu et al. (2017),
Bayesian optimization algorithm is firstly introduced for the single- and multi-objective optimization of general analog circuits and has shown to be much more efficient compared with other simulation-based approaches.",1. Introduction,[0],[0]
"In Wang et al. (2017), Bayesian optimization algorithm is combined with adaptive Monte-Carlo sampling to optimize the yield of analog circuits and static random-access memory (SRAM).
",1. Introduction,[0],[0]
Bayesian optimization algorithm is a well-studied algorithm and has demonstrated to be promising for the automated design of analog circuits.,1. Introduction,[0],[0]
"However, the standard Bayesian optimization algorithm is sequential.",1. Introduction,[0],[0]
It chooses only one point at each iteration by optimizing the acquisition function.,1. Introduction,[0],[0]
It is often desirable to select a batch of points at each iteration.,1. Introduction,[0],[0]
"The sequential property of Bayesian optimization limits its further applications in multi-core computer systems.
",1. Introduction,[0],[0]
Bayesian optimization algorithm has been extended to enable batch selection.,1. Introduction,[0],[0]
"Some prior works, like the qEI (Chevalier & Ginsbourger, 2013), qKG (Wu & Frazier, 2016) and parallel predictive entropy search (PPES) (Shah & Ghahramani, 2015) approaches, consider to search for the optimal batch selection for a specific acquisition function.",1. Introduction,[0],[0]
"These methods usually involve some approximations or MonteCarlo sampling, and thus scale poorly as the batch size increases.",1. Introduction,[0],[0]
"Other works, including the simulation matching (SM) (Azimi et al., 2010) method, the batch-UCB (BUCB, BLCB for minimization problems) (Desautels et al., 2014) method, the parallel UCB with pure exploration (GP-UCBPE) (Contal et al., 2013) method, and the local penalization (LP) (González et al., 2016) method, adopted the greedy strategies that select individual points until the batch is filled.
",1. Introduction,[0],[0]
All the batch Bayesian optimization algorithms mentioned above choose to use single acquisition function.,1. Introduction,[0],[0]
"And except for the SM method (Azimi et al., 2010) and LP method (González et al., 2016) which can use arbitrary acquisition function, other parallelization methods rely on a specific acquisition function.",1. Introduction,[0],[0]
"The UCB acquisition function must be used for BUCB and GP-UCB-PE, and the knowledge gradient (KG) acquisition function must be used for the qKG algorithm.",1. Introduction,[0],[0]
"As is stated in Hoffman et al. (2011), no single acquisition function can always outperform other acquisition functions.",1. Introduction,[0],[0]
"Relying on one acquisition function may result in poor performance.
",1. Introduction,[0],[0]
"In this paper, we propose to parallelize Bayesian optimization algorithm via the Multi-objective ACquisition Ensemble (MACE).",1. Introduction,[0],[0]
The proposed MACE method exploits the disagreement between different acquisition functions to enable batch selection.,1. Introduction,[0],[0]
"At each iteration, after the GP model is updated, multiple acquisition functions are selected.",1. Introduction,[0],[0]
We then perform multi-objective optimization to find the Pareto front (PF) of the acquisition functions.,1. Introduction,[0],[0]
The PF represents the best trade-off between these acquisition functions.,1. Introduction,[0],[0]
"When batch
evaluations are possible, we can sample multiple points on the PF to accelerate the optimization.
",1. Introduction,[0],[0]
"The MACE algorithm is tested using several analytical benchmark functions and two real-world analog circuits, including an operational amplifier with ten design parameters and a class-E power amplifier with twelve design parameters.",1. Introduction,[0],[0]
"The BLCB method (Desautels et al., 2014), local penalization method with expected improvement acquisition function (EI-LP) (González et al., 2016), qEI (Chevalier & Ginsbourger, 2013) and qKG (Wu & Frazier, 2016) methods are compared with MACE.",1. Introduction,[0],[0]
The proposed MACE method achieved competitive performance when compared with the state-of-the-art algorithms listed in the paper.,1. Introduction,[0],[0]
"In this section, we will present the problem formulation of analog circuit optimization, and review the background of Gaussian process regression and Bayesian optimization.",2. Background,[0],[0]
"When designing integrated circuits, the designers have to decide what circuit topology to use and then set the corrsponding design parameters.",2.1. Problem Formulation,[0],[0]
"In this work, we handle the scenarios where the topology of the analog circuit is fixed.",2.1. Problem Formulation,[0],[0]
"This is practical as there are usually a lot of classical topologies for a given design task, so unlike digital circuits, choosing appropriate topology is relatively easy.
",2.1. Problem Formulation,[0],[0]
"Once the circuit topology is fixed, the designer has to choose the appropriate design parameters according to the specifications and the circuit device model.",2.1. Problem Formulation,[0],[0]
What we want to do is automatically searching for the optimal design parameters.,2.1. Problem Formulation,[0],[0]
"This problem can then be formulated as a bound-constrained black-box optimization problem:
minimize FOM(x), (1)
where x ∈ D is the vector of design variables, FOM(x) is the objective constructed from the design specifications, the FOM(x) can be deterministric or noisy depending on the design specifications.",2.1. Problem Formulation,[0],[0]
"Given the design parameters x, the FOM value can be obtained by commercial circuit simulators like HSPICE or Spectre.",2.1. Problem Formulation,[0],[0]
"The objective function FOM(x) in (1) can be approximated by Gaussian process (GP) model (Rasmussen, 2006).",2.2. Gaussian Process Regression,[0],[0]
The GP model is the most commonly used model for Bayesian optimization.,2.2. Gaussian Process Regression,[0],[0]
The advantage of GP is that it provides a well-calibrated uncertainty of prediction.,2.2. Gaussian Process Regression,[0],[0]
"GP is characterized by a mean function m(x) and a covariance function k(x,x′).",2.2. Gaussian Process Regression,[0],[0]
"In this work, we use squared-exponential ARD kernel (Rasmussen, 2006), and a constant mean function
m(x) = µ0 for all our experiments.",2.2. Gaussian Process Regression,[0],[0]
"By default, we assume the objective function evaluations are influenced by i.i.d. noise t ∼ N(0, σ2n) and set the noise level σ2n as a hyperparameter.",2.2. Gaussian Process Regression,[0],[0]
"The introduction of the i.i.d noise also helps to improve the numerical stability.
",2.2. Gaussian Process Regression,[0],[0]
"Denote the training set as {X,y} where X = {x1, . . .",2.2. Gaussian Process Regression,[0],[0]
",xN} and y = {y1, . .",2.2. Gaussian Process Regression,[0],[0]
.,2.2. Gaussian Process Regression,[0],[0]
", yN}, given a new data point x, the prediction of f(x) is not a scalar value, but a predictive distribution
f(x) ∼ N(µ(x), σ2(x)), (2)
where µ(x) and σ2(x) can be expressed as
µ(x) = µ0 + k(x, X)[K +",2.2. Gaussian Process Regression,[0],[0]
σ 2 nI] −1(y,2.2. Gaussian Process Regression,[0],[0]
"− µ0) σ2(x) = k(x,x)− k(x, X)[K + σ2nI]−1k(X,x),
(3)",2.2. Gaussian Process Regression,[0],[0]
"where k(x, X) = (k(x,x1), . .",2.2. Gaussian Process Regression,[0],[0]
.,2.2. Gaussian Process Regression,[0],[0]
", k(x,xN ))",2.2. Gaussian Process Regression,[0],[0]
"T and k(X,x) = k(x, X)T .",2.2. Gaussian Process Regression,[0],[0]
"The µ(x) can be viewed as the prediction of the function value, while the σ2(x) is a measure of uncertainty of the prediction.",2.2. Gaussian Process Regression,[0],[0]
"Bayesian optimization (Shahriari et al., 2016) was proposed for the optimization of expensive black-box functions.",2.3. Bayesian Optimization,[0],[0]
"It consists of two essential ingredients, i.e., the probabilistic surrogate models and the acquisition functions.",2.3. Bayesian Optimization,[0],[0]
The probabilistic surrogate models provide predictions with uncertainties.,2.3. Bayesian Optimization,[0],[0]
The acquisition functions make use of the predictive distribution to explore the state space.,2.3. Bayesian Optimization,[0],[0]
"The procedure of Bayesian optimization is summarized in Algorithm 1.
",2.3. Bayesian Optimization,[0],[0]
"Algorithm 1 Bayesian Optimization Require: Number of initial sampling points Ninit, number
of iterations Niter 1: Randomly sample Ninit points in the design space 2:",2.3. Bayesian Optimization,[0],[0]
"Construct initial GP model 3: for t = 1, 2, . . .",2.3. Bayesian Optimization,[0],[0]
", Niter do 4: Construct the acquisition function 5: Find xt that optimizes the acquisition function 6: Sample yt = f(xt) 7: Update probabilistic surrogate model 8: end for 9: Return best f(x) recorded during iterations
In Bayesian optimization described in Algorithm 1, the acquisition function is used to balance the exploration and exploitation during the optimization.",2.3. Bayesian Optimization,[0],[0]
The acquisition function considers both the predictive value and the uncertainty.,2.3. Bayesian Optimization,[0],[0]
There are a lot of existing acquisition functions.,2.3. Bayesian Optimization,[0],[0]
"Examples include the lower confidence bound (LCB), the probability of improvement (PI), and the expected improvement (EI).
",2.3. Bayesian Optimization,[0],[0]
"The LCB function is defined as follows:
LCB(x) = µ(x)− κσ(x), (4)
where the µ(x) and the σ(x) are the predictive value and uncertainty of GP defined in (3), κ is a parameter that balances the exploitation and exploration.
",2.3. Bayesian Optimization,[0],[0]
"Following the suggestion of (Srinivas et al., 2010; Brochu et al., 2010), the κ in (4) is defined as:
κ = √ ντt τt",2.3. Bayesian Optimization,[0],[0]
"= 2 log(t d/2+2π2/3δ),
(5)
where t is the number of current iteration, ν and δ are two user-defined parameters.",2.3. Bayesian Optimization,[0],[0]
"We fix ν = 0.5 and δ = 0.05 in this paper for the proposed MACE algorithm and our implementation of the BLCB algorithm.
",2.3. Bayesian Optimization,[0],[0]
"The PI and EI functions are defined as
PI(x) = Φ(λ) EI(x) = σ(x)(λΦ(λ) + φ(λ))",2.3. Bayesian Optimization,[0],[0]
λ,2.3. Bayesian Optimization,[0],[0]
= τ,2.3. Bayesian Optimization,[0],[0]
− ξ,2.3. Bayesian Optimization,[0],[0]
"− µ(x)
σ(x) ,
(6)
where τ is the current best value objective value, and ξ is a small positive jitter to improvement the ability of exploration.",2.3. Bayesian Optimization,[0],[0]
The Φ(.) and φ(.) functions are the CDF and PDF functions of normal distribution.,2.3. Bayesian Optimization,[0],[0]
"In our implementation of the MACE algorithm, we fix ξ = 1e-3.
",2.3. Bayesian Optimization,[0],[0]
"There are also other acquisition functions, like the knowledge gradient (Scott et al., 2011) function, predictive entropy search (Hernández-Lobato et al., 2014), and the max-value entropy search(Wang & Jegelka, 2017).",2.3. Bayesian Optimization,[0],[0]
"A portfolio of several acquisition functions is also possible (Hoffman et al., 2011).",2.3. Bayesian Optimization,[0],[0]
We will present the proposed batch Bayesian optimization algorithm in this section.,3. Proposed Batch Bayesian Optimization Algorithm,[0],[0]
"Unlike single-objective optimization, there are multiple objectives to optimize in multi-objective optimization problems(Marler & Arora, 2004).",3.1. Multi-objective Optimization,[0],[0]
"The multi-objective optimization problem is formulated as
minimize f1(x), . . .",3.1. Multi-objective Optimization,[0],[0]
", fm(x).",3.1. Multi-objective Optimization,[0],[0]
"(7)
The multiple objectives to be optimized can be conflicting so that it is usually impossible to find a single solution that is the optimum of all objectives.",3.1. Multi-objective Optimization,[0],[0]
"The goal of multi-objective optimization algorithms is to approximate the Pareto front of
the objectives.",3.1. Multi-objective Optimization,[0],[0]
A solution x1 is said to dominate x2 if ∀i ∈ {1 . .,3.1. Multi-objective Optimization,[0],[0]
".m}, fi(x1) ≤ fi(x2) and ∃j ∈ {1 . .",3.1. Multi-objective Optimization,[0],[0]
".m}, fj(x1) < fj(x2).",3.1. Multi-objective Optimization,[0],[0]
A design is Pareto-optimal if it is not dominated by any other point in the design space and dominates at least one point.,3.1. Multi-objective Optimization,[0],[0]
"The whole set of the Pareto-optimal points in the design space is called the Pareto set, and the set of Pareto-optimal points in the objective space is called the Pareto front.",3.1. Multi-objective Optimization,[0],[0]
"It is often unlikely to get the whole Pareto front as there might be infinite points on the Paret front, multi-objective optimization algorithms try to find a set of evenly distributed solutions that approximate the true Pareto front.
",3.1. Multi-objective Optimization,[0],[0]
"There exist many mature multi-objective optimization algorithms, like the non-dominated sorting based genetic algorithm (NSGA-II) (Deb et al., 2002), and the multiobjective evolutionary algorithm based on decomposition (MOEA/D) (Zhang & Li, 2007).",3.1. Multi-objective Optimization,[0],[0]
"In this paper, the multi-objective optimization based on differential evolution (DEMO) (Robič & Filipič, 2005) is used to solve multiobjective optimization problems, but other multi-objective optimization algorithms can also be applied.",3.1. Multi-objective Optimization,[0],[0]
"Each acquisition function represents a unique selection strategy, different acquisition functions may not agree with each other about where to sample the next point.",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"For example, the value of LCB function always decreases as the σ(x) increases.",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"However, for the PI function, when σ(x) increases, the value of PI would decrease when µ(x) < τ , and increase when µ(x) > τ .",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"For the EI function, if the function is noiseless, the values of EI function at already sampled points would always be worse than the EI values at any
unsampled locations, while this property does not hold for the LCB function.
",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"Algorithm 2 Multi-objective Acquisition Ensemble Algorithm Require: Number of initial sampling points Ninit, number
of iterations Niter, batch size B. 1: Randomly sample Ninit points in the design space 2:",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"Construct initial GP model 3: for t = 1, 2, . . .",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
", Niter do 4: Construct the LCB, EI and PI functions according to (4) and (6) 5: Find the Pareto front of LCB, EI, PI functions using the DEMO algorithm 6: Randomly sample B points x1, . . .",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
",xB from the Pareto-optimal points 7: Evaluate x1, . . .",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
",xB to get y1 = f(x1), . . .",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
", yB = f(xB) 8: Update the GP model 9: end for
10: Return best f(x) recorded during iterations
With multi-objective optimization, the best trade-off between acquisition functions can be captured by the Pareto front of these acquisition functions.",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"We can then sample on the Pareto front to obtain multiple candidate points for the objective function evaluations.
",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
The proposed MACE algorithm is described in Algorithm 2.,3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"In the proposed MACE algorithm, the LCB, EI, and PI acquisition functions are selected.",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
Other acquisition functions like KG and PES can also be incorporated into the MACE framework.,3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"In each iteration, the following multi-objective
optimization problem is constructed:
minimize LCB(x), − EI(x), − PI(x).",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"(8)
Then the DEMO multi-objective optimization algorithm (Robič & Filipič, 2005) is applied to solve the multiobjective problem in (8).",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"Once the Pareto front of LCB, EI and PI is obtained, the candidate points are then randomly sampled from the Pareto front.
",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"In Figure 1, we illustrate the proposed MACE algorithm using an example of a real-world amplifier circuit.",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"The optimization objective is to maximize the phase margin (PM) of the amplifier, so the FOM is defined as FOM(x) = −PM(x).",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
The width of one of its transistor is the design variable.,3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
We sweep the width of the transistor and perform HSPICE simulations to get the FOM values.,3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
The curve of FOM values is plotted in Figure 1(a) (the blue line).,3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
Several points are randomly sampled from the FOM curve to train the GP model.,3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"The LCB, EI, PI functions and the Pareto front of the acquisition functions are plotted in Figure 1(b).",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"We can see from Figure 1(b) that the optimal locations of the three acquisition functions are different, while their best trade-off is captured by the Pareto front.",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
The Pareto set that represents the best trade-off between the three acquisition functions is the interval,3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"[43, 50.4], as plotted in Figure 1(a).",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
The candidate points for the next batch of evaluations are randomly sampled from the Pareto set.,3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
The proposed MACE algorithm1 was tested using eight benchmark functions and two real-world analog circuits.,4. Experimental Results,[0],[0]
"Four state-of-the-art parallel Bayesian optimization methods were compared, including the BLCB algorithm (Desautels et al., 2014), the local penalization method with EI acquisition function (EI-LP) (González et al., 2016), the qEI and qKG methods (Chevalier & Ginsbourger, 2013; Wu & Frazier, 2016).",4. Experimental Results,[0],[0]
"2
For the MACE, BLCB, and EI-LP method, the ARD squared-exponential kernel is used and the GP models are fitted by maximum likelihood estimations (MLE); for the qKG and qEI methods, the ARD Matern52 kernels are used, and the GP hyperparameters are integrated via MCMC sampling.",4. Experimental Results,[0],[0]
"The Matern52 kernel and MCMC integration are the default strategies of the qKG and qEI implementations and it is unclear in the documentation about how to change the GP settings.
",4. Experimental Results,[0],[0]
1Available at https://github.com/Alaya-in-Matrix/MACE 2We implemented the BLCB algorithm as the available open source implementations only allow discrete input.,4. Experimental Results,[0],[0]
"For the EI-LP method, the code is downloaded from https://github.com/SheffieldML/GPyOpt.",4. Experimental Results,[0],[0]
The code for qEI and qKG is downloaded from https://github.com/wujian16/CornellMOE.,4. Experimental Results,[0],[0]
"We tested the MACE algorithm and other parallel BO methods using eight commonly used benchmark functions, as summarized in Table 1.
",4.1. Benchmark Problems,[0],[0]
"For all functions except the two 10D functions, we set the number of initial random sampling to Ninit = 20 and the number of iterations toNiter = 45.",4.1. Benchmark Problems,[0],[0]
"Batch size is set toB = 4, the total number of function evaluations is Ninit +B × Niter.",4.1. Benchmark Problems,[0],[0]
"For the 10D Ackley and 10D Rosenbrock functions, we set Ninit = 100 and Niter = 175.",4.1. Benchmark Problems,[0],[0]
"The experiments were repeated ten times to average the random fluctuations.
",4.1. Benchmark Problems,[0],[0]
We also ran the MACE algorithm in sequential mode and compared with the EI and LCB acquisition functions.,4.1. Benchmark Problems,[0],[0]
"The sequential EI and LCB based Bayesian optimization are implemented by setting the batch size B = 1 for EI-LP and BLCB respectively.
",4.1. Benchmark Problems,[0],[0]
"The mean convergence plots of the tested algorithms on the benchmark functions are given in Figure 2, the statistics of the final regrets are listed in Table 2.",4.1. Benchmark Problems,[0],[0]
"As can be seen in Figure 2 and Table 2, when running in sequential mode, the MACE algorithm is competitive with the LCB and EI acquisition functions.",4.1. Benchmark Problems,[0],[0]
"The sequential MACE (MACE-1) algorithm gave better performances than the sequential EI (EI-1) and sequential LCB (LCB-1) algorithms in the Eggholder, Branin, Hartmann6, Ackley10, and Rosenbrock10 functions.",4.1. Benchmark Problems,[0],[0]
"Also, the parallel MACE (MACE-4) gave the best performances among all the tested algorithms for six out of the eight benchmark functions, and has shown dramatic speedup compared to the sequential MACE.",4.1. Benchmark Problems,[0],[0]
"We also performed additional experiments with varied batch sizes, the detail of those experimental results can be seen in the supplementary materials.",4.1. Benchmark Problems,[0],[0]
"We report the time spent on the ten-dimensional Rosenbrock function optimization with B = 4 as a measure of the algorithm overhead, for the ten-dimensional Rosenbrock function, it took MACE about 11 hours to finish all the Niter = 175 iterations, the BLCB algorithm took about five hours, for the EI-LP algorithm, it took only one hour to finish the optimization.",4.1. Benchmark Problems,[0],[0]
"The overheads for qEI and qKG are much larger, it took more than two days for qKG and qEI to
finish the optimization of the ten-dimensional Rosenbrock function.",4.1. Benchmark Problems,[0],[0]
"The operational amplifier (Wang et al., 2014) shown in Figure 3 is used to test Bayesian optimization algorithms.",4.2. Operational Amplifier,[0],[0]
The circuit is designed using the 180nm process.,4.2. Operational Amplifier,[0],[0]
"It has 10 design parameters, including the lengths and widths of transistors, the resistance of the resistors and the capacitance of the capacitors.",4.2. Operational Amplifier,[0],[0]
"The circuit is simulated using the commercial HSPICE circuit simulator.
",4.2. Operational Amplifier,[0],[0]
"We want to maximize the gain, unit gain frequency (UGF) and the phase margin (PM) for this amplifier.",4.2. Operational Amplifier,[0],[0]
"The Figure of Merit FOM is constructed as
FOM = −1.2× gain − 10×UGF − 1.6× PM .
",4.2. Operational Amplifier,[0],[0]
"For this circuit, we compared the MACE algorithm with the BLCB and EI-LP algorithms.",4.2. Operational Amplifier,[0],[0]
"The qKG and qEI are not compared as the computation of qEI and qKG acquisition functions become very slow for the ten-dimensional functions.
",4.2. Operational Amplifier,[0],[0]
We run the algorithms in sequential mode and batch mode.,4.2. Operational Amplifier,[0],[0]
"For the batch mode, the batch size is set to B = 4.",4.2. Operational Amplifier,[0],[0]
"The number of initial random sampling is set to Ninit = 100, and the number of iterations is set to Niter = 100.
",4.2. Operational Amplifier,[0],[0]
The mean convergence plot for the sequential and batch runs are given in Figure 4.,4.2. Operational Amplifier,[0],[0]
The mean and standard deviation of the final optimized FOM values are listed in Table 3.,4.2. Operational Amplifier,[0],[0]
"As can be seen, on average, the batch MACE algorithm had the fastest convergence rate compared with the sequential MACE algorithm and other parallel algorithms.",4.2. Operational Amplifier,[0],[0]
"It should also be noted that the final optimized FOM values given by MACE-4 have very small deviation (0.105) compared with other algorithms.
",4.2. Operational Amplifier,[0],[0]
Table 2.,4.2. Operational Amplifier,[0],[0]
"Statistics of the regrets of the benchmark functions
Eggholder Branin Alpine1 Hartmann6
MACE-1 87.65±75.83 1.05e-5±1.31e-5 2.66305±1.05844 0.0646869±0.0621189 LCB-1 153.9±112.8 6.86e-5±1.13e-4 5.66812±1.76973 0.125565±0.122684 EI-1",4.2. Operational Amplifier,[0],[0]
172.8±132.2 1.62e-2±1.63e-2 2.46061±1.56079 0.110561±0.146809,4.2. Operational Amplifier,[0],[0]
MACE-4,4.2. Operational Amplifier,[0],[0]
46.38±40.89 4.62e-6±6.64e-6 0.903805±0.835209 0.0275738±0.052254 BLCB-4 56.86±35.91 4.32e-5±6.33e-5 1.8843±0.938873 0.06447±0.0621176 EI-LP-4 44.68±56.45,4.2. Operational Amplifier,[0],[0]
"2.11e-2±1.84e-2 1.0059±0.456865 0.0540446±0.0558557 qKG-4 106.4±67.64 2.65e-1±2.70e-1 3.01513±1.13414 0.47134±0.18939 qEI-4 72.13±52.08 3.29e-4±1.14e-3 2.7074±1.05145 0.186088±0.116323
Ackley2 Rosenbrock2 Ackley10",4.2. Operational Amplifier,[0],[0]
"Rosenbrock10
MACE-1 1.71474±1.12154 0.026173±0.051189 3.1348±0.447874 499.697±300.899 LCB-1 1.624±0.926437 0.0201124±0.0205367 3.14797±0.519164 517.944±288.955 EI-1 1.0136±0.985858",4.2. Operational Amplifier,[0],[0]
13.5508±9.52734 18.8006±0.652136 1367.08±637.507 MACE-4 1.07906±0.886466 0.00095416±0.00093729 2.56439±0.535488 158.116±50.0024 BLCB-4 1.40051±1.02849 0.00191986±0.00180895 3.27543±0.735501 406.819±127.351 EI-LP-4 0.284265±0.24634 2.73645±2.05923,4.2. Operational Amplifier,[0],[0]
18.2682±0.608564 721.351±327.365 qKG-4 5.59394±1.80595 5.03976±3.72014 18.197±0.764103 705.112±412.762,4.2. Operational Amplifier,[0],[0]
qEI-4 2.87373±1.02405 10.1881±15.0432,4.2. Operational Amplifier,[0],[0]
"18.3686±0.501869 655.208±340.954
Vin
C0
C1
Vdd1=2.5V
Vg
Vdd2=1.8V
M4
M3
M2
M1
L3 C2
C3
Vout
RLC1
L1
Cc
R0
R1
L2
Figure 5.",4.2. Operational Amplifier,[0],[0]
Schematic of the power amplifier,4.2. Operational Amplifier,[0],[0]
The class-E power amplifier shown in Figure 5 is used to test Bayesian optimization algorithms.,4.3. Class-E Power Amplifier,[0],[0]
"The circuit is designed using the 180nm process with 12 design parameters, the circuit is simulated by the commercial HSPICE circuit simulator to get its performances.
",4.3. Class-E Power Amplifier,[0],[0]
"For this power amplifier, we aim to maximize the power added efficiency (PAE) and the output power (Pout), the Figure of Merit FOM is constructed as
FOM = −3× PAE − Pout .
",4.3. Class-E Power Amplifier,[0],[0]
"The MACE, BLCB, and EI-LP algorithms were tested in both sequential and batch modes.",4.3. Class-E Power Amplifier,[0],[0]
The number of initial sampling is Ninit = 100.,4.3. Class-E Power Amplifier,[0],[0]
"The number of iterations is
Niter = 100.",4.3. Class-E Power Amplifier,[0],[0]
The batch size is set to B = 4.,4.3. Class-E Power Amplifier,[0],[0]
"The total number of HSPICE simulations is 500 for each batch run and 200 for each sequential run.
",4.3. Class-E Power Amplifier,[0],[0]
The optimization results of the class-E power amplifier are given in Figure 6 and Table 4.,4.3. Class-E Power Amplifier,[0],[0]
We can see that the MACE outperformed the BLCB and EI-LP in both sequential and batch mode.,4.3. Class-E Power Amplifier,[0],[0]
"For the batch runs, the MACE converges fastest among the three algorithms, while the sequential MACE (MACE-1) has comparable performance as the batch EI-LP (EI-LP-4) method.",4.3. Class-E Power Amplifier,[0],[0]
"In this paper, a batch Bayesian optimization algorithm is proposed for the automation of analog circuit design.",5. Conclusion,[0],[0]
The parallelization is achieved via the multi-objective ensemble of acquisition functions.,5. Conclusion,[0],[0]
"In each iteration, the candidate points are sampled from the Pareto front of multiple acquisition functions.",5. Conclusion,[0],[0]
"We compared the proposed MACE algorithm using analytical benchmark functions and real-world circuits, it is shown that the MACE algorithm is competitive compared with the state-of-the-art methods listed in the paper.",5. Conclusion,[0],[0]
"This research was supported partly by the National Major Science and Technology Special Project of China (2017ZX01028101-003), partly by National Key Research and Development Program of China 2016YFB0201304, partly by National Natural Science Foundation of China (NSFC) research projects 61774045, 61574044, 61474026, 61574046, 61674042, and 61628402 and partly by the Recruitment Program of Global Experts (the Thousand Talents Plan).",Acknowledgements,[0],[0]
Bayesian optimization methods are promising for the optimization of black-box functions that are expensive to evaluate.,abstractText,[0],[0]
"In this paper, a novel batch Bayesian optimization approach is proposed.",abstractText,[0],[0]
The parallelization is realized via a multi-objective ensemble of multiple acquisition functions.,abstractText,[0],[0]
"In each iteration, the multi-objective optimization of the multiple acquisition functions is performed to search for the Pareto front of the acquisition functions.",abstractText,[0],[0]
The batch of inputs are then selected from the Pareto front.,abstractText,[0],[0]
The Pareto front represents the best trade-off between the multiple acquisition functions.,abstractText,[0],[0]
Such a policy for batch Bayesian optimization can significantly improve the efficiency of optimization.,abstractText,[0],[0]
The proposed method is compared with several state-of-the-art batch Bayesian optimization algorithms using analytical benchmark functions and real-world analog integrated circuits.,abstractText,[0],[0]
The experimental results show that the proposed method is competitive compared with the state-of-the-art algorithms.,abstractText,[0],[0]
Batch Bayesian Optimization via Multi-objective Acquisition Ensemble for Automated Analog Circuit Design,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1853–1862 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1853",text,[0],[0]
"Representing words using dense and real-valued vectors, aka word embeddings, has become the cornerstone for many natural language processing (NLP) tasks, such as document classification (Sebastiani, 2002), parsing (Huang et al., 2012), discourse relation recognition (Lei et al., 2017) and named entity recognition (Turian et al., 2010).",1 Introduction,[0],[0]
"Word embeddings can be learned by optimizing that words occurring in similar contexts have similar embeddings, i.e. the well-known distributional hypothesis (Harris, 1954).",1 Introduction,[0],[0]
"A representative method is skip-gram (SG) (Mikolov et al., 2013a,b), which realizes the hypothesis using a
∗The first two authors contributed equally to this paper and share the first-authorship.
",1 Introduction,[0],[0]
shallow neural network model.,1 Introduction,[0],[0]
"The other family of methods is count-based, such as GloVe (Pennington et al., 2014) and LexVec (Salle et al., 2016a,b), which exploit low-rank models such as matrix factorization (MF) to learn embeddings by reconstructing the word co-occurrence statistics.
",1 Introduction,[0],[0]
"By far, most state-of-the-art embedding methods rely on SGD and negative sampling for optimization.",1 Introduction,[0],[0]
"However, the performance of SGD is highly sensitive to the sampling distribution and the number of negative samples (Chen et al., 2018; Yuan et al., 2016), as shown in Figure 1.",1 Introduction,[0],[0]
"Essentially, sampling is biased, making it difficult to converge to the same loss with all examples, regardless of how many update steps have been taken.",1 Introduction,[0],[0]
"Moreover, SGD exhibits dramatic fluctuation and suffers from overshooting on local minimums (Ruder, 2016).",1 Introduction,[0],[0]
"These drawbacks of SGD can be attributed to its one-sample learning scheme, which updates parameters based on one training sample in each step.
",1 Introduction,[0],[0]
"To address the above-mentioned limitations of SGD, a natural solution is to perform exact (full) batch learning.",1 Introduction,[0],[0]
"In contrast to SGD, batch learning does not involve any sampling procedure and computes the gradient over all training samples.",1 Introduction,[0],[0]
"As such, it can easily converge to a better optimum in a more stable way.",1 Introduction,[0],[0]
"Nevertheless, a well-known
difficulty in applying full batch learning lies in the expensive computational cost for large-scale data.",1 Introduction,[0],[0]
"Taking the word embedding learning as an example, if the vocabulary size is |V |, then evaluating the loss function and computing the full gradient takes O(|V |2k) time, where k is the embedding size.",1 Introduction,[0],[0]
"This high complexity is unaffordable in practice, since |V |2 can easily reach billion level or even higher.
",1 Introduction,[0],[0]
"In this paper, we introduce AllVec, an exact and efficient word embedding method based on full batch learning.",1 Introduction,[0],[0]
"To address the efficiency challenge in learning from all training samples, we devise a regression-based loss function for word embedding, which allows fast optimization with memorization strategies.",1 Introduction,[0],[0]
"Specifically, the acceleration is achieved by reformulating the expensive loss over all negative samples using a partition and a decouple operation.",1 Introduction,[0],[0]
"By decoupling and caching the bottleneck terms, we succeed to use all samples for each parameter update in a manageable time complexity which is mainly determined by the positive samples.",1 Introduction,[0],[0]
"The main contributions of this work are summarized as follows:
• We present a fine-grained weighted least square loss for learning word embeddings.",1 Introduction,[0],[0]
"Unlike GloVe, it explicitly accounts for all negative samples and reweights them with a frequency-aware strategy.
",1 Introduction,[0],[0]
• We propose an efficient and exact optimization algorithm based on full batch gradient optimization.,1 Introduction,[0],[0]
"It has a comparable time complexity with SGD, but being more effective and stable due to the consideration of all samples in each parameter update.
",1 Introduction,[0],[0]
"• We perform extensive experiments on several benchmark datasets and tasks to demonstrate the effectiveness, efficiency, and convergence property of our AllVec method.",1 Introduction,[0],[0]
"Mikolov et al. (2013a,b) proposed the skip-gram model to learn word embeddings.",2.1 Skip-gram with Negative Sampling,[0],[0]
"SG formulates the problem as a predictive task, aiming at predicting the proper context c for a target word w within a local window.",2.1 Skip-gram with Negative Sampling,[0],[0]
"To speed up the training process, it applies the negative sampling (Mikolov et al., 2013b) to approximate the full softmax.",2.1 Skip-gram with Negative Sampling,[0],[0]
"That is,
each positive (w, c) pair is trained with n randomly sampled negative pairs (w,wi).",2.1 Skip-gram with Negative Sampling,[0],[0]
"The sampled loss function of SG is defined as
LSGwc =log σ(UwŨ T c )+ n∑ i=1",2.1 Skip-gram with Negative Sampling,[0],[0]
"Ewi∼Pn(w) log σ(−UwŨ T wi)
where Uw and Ũc denote the k-dimensional embedding vectors for word w and context c. Pn(w) is the distribution from which negative context wi is sampled.
",2.1 Skip-gram with Negative Sampling,[0],[0]
"Plenty of research has been done based on SG, such as the use of prior knowledge from another source (Kumar and Araki, 2016; Liu et al., 2015a; Bollegala et al., 2016), incorporating word type information (Cao and Lu, 2017; Niu et al., 2017), character level n-gram models (Bojanowski et al., 2016; Joulin et al., 2016) and jointly learning with topic models like LDA (Shi et al., 2017; Liu et al., 2015b).",2.1 Skip-gram with Negative Sampling,[0],[0]
Mikolov et al. (2013b) showed that the unigram distribution raised to the 3/4th power as Pn(w) significantly outperformed both the unigram and the uniform distribution.,2.2 Importance of the Sampling Distribution,[0],[0]
This suggests that the sampling distribution (of negative words) has a great impact on the embedding quality.,2.2 Importance of the Sampling Distribution,[0],[0]
"Furthermore, Chen et al. (2018) and Guo et al. (2018) recently found that replacing the original sampler with adaptive samplers could result in better performance.",2.2 Importance of the Sampling Distribution,[0],[0]
The adaptive samplers are used to find more informative negative examples during the training process.,2.2 Importance of the Sampling Distribution,[0],[0]
"Compared with the original word-frequency based sampler, adaptive samplers adapt to both the target word and the current state of the model.",2.2 Importance of the Sampling Distribution,[0],[0]
They also showed that the finegrained samplers not only speeded up the convergence but also significantly improved the embedding quality.,2.2 Importance of the Sampling Distribution,[0],[0]
"Similar observations were also found in other fields like collaborative filtering (Yuan et al., 2016).",2.2 Importance of the Sampling Distribution,[0],[0]
"While being effective, it is proven that negative sampling is a biased approximation and does not converges to the same loss as the full softmax — regardless of how many update steps have been taken (Bengio and Senécal, 2008; Blanc and Rendle, 2017).",2.2 Importance of the Sampling Distribution,[0],[0]
"Another line of research is the count-based embedding, such as GloVe (Pennington et al., 2014).",2.3 Count-based Embedding Methods,[0],[0]
"GloVe performs a biased MF on the word-context co-occurrence statistics, which is a common ap-
proach in the field of collaborative filtering (Koren, 2008).",2.3 Count-based Embedding Methods,[0],[0]
"However, GloVe only formulates the loss on positive entries of the co-occurrence matrix, meaning that negative signals about wordcontext co-occurrence are discarded.",2.3 Count-based Embedding Methods,[0],[0]
"A remedy solution is LexVec (Salle et al., 2016a,b) which integrates negative sampling into MF.",2.3 Count-based Embedding Methods,[0],[0]
"Some other methods (Li et al., 2015; Stratos et al., 2015; Ailem et al., 2017) also use MF to approximate the word-context co-occurrence statistics.",2.3 Count-based Embedding Methods,[0],[0]
"Although predictive models and count-based models seem different at first glance, Levy and Goldberg (2014) proved that SG with negative sampling is implicitly factorizing a shifted pointwise mutual information (PMI) matrix, which means that the two families of embedding models resemble each other to a certain degree.
",2.3 Count-based Embedding Methods,[0],[0]
Our proposed method departs from all above methods by using the full batch gradient optimizer to learn from all (positive and negative) samples.,2.3 Count-based Embedding Methods,[0],[0]
We propose a fast learning algorithm to show that such batch learning is not “heavy” even with tens of billions of training examples.,2.3 Count-based Embedding Methods,[0],[0]
"In this work, we adopt the regression loss that is commonly used in count-based models (Pennington et al., 2014; Stratos et al., 2015; Ailem et al., 2017) to perform matrix factorization on word cooccurrence statistics.",3 AllVec Loss,[0],[0]
"As highlighted, to retain the modeling fidelity, AllVec eschews using any sampling but optimizes the loss on all positive and negative word-context pairs.
",3 AllVec Loss,[0],[0]
"Given a word w and a symmetric window of win contexts, the set of positive contexts can be obtained by sliding through the corpus.",3 AllVec Loss,[0],[0]
"Let c denote a specific context, Mwc be the number of cooccurred (w, c) pairs in the corpus within the window.",3 AllVec Loss,[0],[0]
"Mwc=0 means that the pair (w, c) has never been observed, i.e. the negative signal.",3 AllVec Loss,[0],[0]
"rwc is the association coefficient between w and c, which is calculated from Mwc.",3 AllVec Loss,[0],[0]
"Specifically, we use r+wc to denote the ground truth value for positive (w, c) pairs and a constant value r−(e.g., 0 or -1) for negative ones since there is no interaction between w and c in negative pairs.",3 AllVec Loss,[0],[0]
"Finally, with all positive and negative pairs considered, a regular loss function can be given as Eq.(1), where V is the vocabulary and S is the set of positive pairs.",3 AllVec Loss,[0],[0]
"α+wc and α−wc represent the weight for positive and negative
(w, c) pairs, respectively.",3 AllVec Loss,[0],[0]
"L = ∑
(w,c)∈S α+wc(r + wc − UwŨTc )2︸ ︷︷ ︸
LP + ∑ (w,c)∈(V×V )",3 AllVec Loss,[0],[0]
"\S
α−wc(r − − UwŨTc )2︸ ︷︷ ︸
LN
(1)
When it comes to r+wc, there are several choices.",3 AllVec Loss,[0],[0]
"For example, GloVe applies the log of Mwc with bias terms for w and c. However, research from Levy and Goldberg (2014) showed that the SG model with negative sampling implicitly factorizes a shifted PMI matrix.",3 AllVec Loss,[0],[0]
"The PMI value for a (w, c) pair can be defined as
PMIwc = log P (w, c)
P (w)P (c) = log MwcM∗∗ Mw∗M∗c",3 AllVec Loss,[0],[0]
"(2)
where ‘*’ denotes the summation of all corresponding indexes (e.g., Mw∗= ∑ c∈V Mwc).",3 AllVec Loss,[0],[0]
"Inspired by this connection, we set r+wc as the positive point-wise mutual information (PPMI) which has been commonly used in the NLP literature (Stratos et al., 2015; Levy and Goldberg, 2014).",3 AllVec Loss,[0],[0]
"Sepcifically, PPMI is the positive version of PMI by setting the negative values to zero.",3 AllVec Loss,[0],[0]
"Finally, r+wc is defined as
r+wc = PPMIwc = max(PMIwc, 0) (3)",3 AllVec Loss,[0],[0]
"Regarding α+wc, we follow the design in GloVe, where it is defined as
α+wc =
{ (Mwc/xmax) ρ",3.1 Weighting Strategies,[0],[0]
"Mwc < xmax
1 Mwc ≥ xmax (4)
",3.1 Weighting Strategies,[0],[0]
"As for the weight for negative instances α−wc, considering that there is no interaction between w and negative c, we set α−wc as α − c (or α − w), which means that the weight is determined by the word itself rather than the word-context interaction.",3.1 Weighting Strategies,[0],[0]
Note that either α−wc = α − c or α − wc = α,3.1 Weighting Strategies,[0],[0]
− w does not influence the complexity of AllVec learning algorithm described in the next section.,3.1 Weighting Strategies,[0],[0]
"The design of α−c is inspired by the frequency-based oversampling scheme in skip-gram and missing data reweighting in recommendation (He et al., 2016).",3.1 Weighting Strategies,[0],[0]
The intuition is that a word with high frequency is more likely to be a true negative context word if there is no observed word-context interactions.,3.1 Weighting Strategies,[0],[0]
"Hence, to effectively differentiate the positive and negative examples, we assign a higher weight for the negative examples that have a higher word fre-
quency, and a smaller weight for infrequent words.",3.1 Weighting Strategies,[0],[0]
"Formally, α−wc is defined as
α−wc = α − c = α0 M δ∗c∑",3.1 Weighting Strategies,[0],[0]
"c∈V M δ ∗c
(5)
where α0 can be seen as a global weight to control the overall importance of negative samples.",3.1 Weighting Strategies,[0],[0]
α0 = 0 means that no negative information is utilized in the training.,3.1 Weighting Strategies,[0],[0]
The exponent δ is used for smoothing the weights.,3.1 Weighting Strategies,[0],[0]
"Specially, δ = 0 means a uniform weight for all negative examples and δ = 1 means that no smoothing is applied.",3.1 Weighting Strategies,[0],[0]
"Once specifying the loss function, the main challenge is how to perform an efficient optimization for Eq.(1).",4 Fast Batch Gradient Optimization,[0],[0]
"In the following, we develop a fast batch gradient optimization algorithm that is based on a partition reformulation for the loss and a decouple operation for the inner product.",4 Fast Batch Gradient Optimization,[0],[0]
"As can be seen, the major computational cost in Eq.(1) lies in the term LN , because the size of (V×V ) \S is very huge, which typically contains over billions of negative examples.",4.1 Loss Partition,[0],[0]
"To this end, we show our first key design that separates the loss of negative samples into the difference between the loss on all samples and that on positive samples1.",4.1 Loss Partition,[0],[0]
"The loss partition serves as the prerequisite for the efficient computation of full batch gradients.
",4.1 Loss Partition,[0],[0]
"LN= ∑ w∈V ∑ c∈V α−c (r −−UwŨTc )2− ∑ (w,c)∈S α−c (r −− UwŨTc )2 (6)
By replacing LN in Eq.(1) with Eq.(6), we can obtain a new loss function with a more clear structure.",4.1 Loss Partition,[0],[0]
We further simplify the loss function by merging the terms on positive examples.,4.1 Loss Partition,[0],[0]
"Finally, we achieve a reformulated loss
L = ∑ w∈V ∑ c∈V α−c (r −−UwŨTc ) 2
︸ ︷︷ ︸ LA
+ ∑
(w,c)∈S
(α+wc − α−c )(∆− UwŨTc ) 2
︸ ︷︷ ︸ L
P ′
+C (7)
where ∆ =",4.1 Loss Partition,[0],[0]
(α+wcr + wc − α−c r−)/(α+wc − α−c ).,4.1 Loss Partition,[0],[0]
It can be seen that the new loss function consists of two components: the loss LA on the whole V ×V training examples and LP ′ on positive examples.,4.1 Loss Partition,[0],[0]
"The major computation now lies in LA which has
1The idea here is similar to that used in (He et al., 2016; Li et al., 2016) for a different problem.
",4.1 Loss Partition,[0],[0]
a time complexity of O(k|V |2).,4.1 Loss Partition,[0],[0]
"In the following, we show how to reduce the huge volume of computation by a simple mathematical decouple.",4.1 Loss Partition,[0],[0]
"To clearly show the decouple operation, we rewrite LA as L̃A by omitting the constant term α−c (r
−)2.",4.2 Decouple,[0],[0]
"Note that uwd and ũcd denote the d-th element in Uw and Ũc, respectively.
",4.2 Decouple,[0],[0]
"L̃A = ∑ w∈V ∑ c∈V α−c k∑ d=0 uwdũcd k∑ d′=0 uwd′ ũcd′
− 2r− ∑ w∈V ∑ c∈V α−c k∑",4.2 Decouple,[0],[0]
"d=0 uwdũcd
(8)
Now we show our second key design that is based on a decouple manipulation for the inner product operation.",4.2 Decouple,[0],[0]
"Interestingly, we observe that the summation operator and elements in Uw and Ũc can be rearranged by the commutative property (Dai et al., 2007), as shown below.
",4.2 Decouple,[0],[0]
L̃A = k∑ d=0 k∑ d′=0 ∑ w∈V uwduwd′,4.2 Decouple,[0],[0]
"∑ c∈V α−c ũcdũcd′
− 2r− k∑
d=0 ∑ w∈V uwd ∑ c∈V α−c ũcd
(9)
",4.2 Decouple,[0],[0]
"An important feature in Eq.(9) is that the original inner product terms are disappeared, while in the new equation ∑ c∈V α",4.2 Decouple,[0],[0]
− c ũcdũcd′ and ∑ c∈V α,4.2 Decouple,[0],[0]
− c ũcd are “constant” values relative to uwduwd′ and uwd respectively.,4.2 Decouple,[0],[0]
This means that they can be pre-calculated before training in each iteration.,4.2 Decouple,[0],[0]
"Specifically, we define pwdd′ , p c",4.2 Decouple,[0],[0]
"dd′ , q w d and q c d as the pre-calculated terms
pwdd′",4.2 Decouple,[0],[0]
=,4.2 Decouple,[0],[0]
∑,4.2 Decouple,[0],[0]
"w∈V uwduwd′ q w d = ∑ w∈V uwd
pcdd′ = ∑ c∈V α−c ũcdũcd′ q c",4.2 Decouple,[0],[0]
d = ∑ c∈V α−c ũcd (10),4.2 Decouple,[0],[0]
Then the computation of L̃A can be simplified to∑k d=0 ∑k d′=0 p w dd′p,4.2 Decouple,[0],[0]
c,4.2 Decouple,[0],[0]
dd′,4.2 Decouple,[0],[0]
"− 2r−qwd qcd.
",4.2 Decouple,[0],[0]
"It can be seen that the time complexity to compute all pwdd′ is O(|V |k2), and similarly, O(|V |k2) for pcdd′ andO(|V |k) for qwd and qcd.",4.2 Decouple,[0],[0]
"With all terms pre-calculated before each iteration, the time complexity of computing L̃A is justO(k2).",4.2 Decouple,[0],[0]
"As a result, the total time complexity of computing LA is decreased toO(2|V |k2+2|V |k+k2)",4.2 Decouple,[0],[0]
"≈ O(2|V |k2), which is much smaller than the originalO(k|V |2).",4.2 Decouple,[0],[0]
"Moreover, it’s worth noting that our efficient computation for L̃A is strictly equal to its original value, which means AllVec does not introduce any approximation in evaluating the loss function.
",4.2 Decouple,[0],[0]
"Finally, we can derive the batch gradients for
uwd and ũcd as ∂L
∂uwd = k∑ d′=0 uwd′p c dd′",4.2 Decouple,[0],[0]
"− ∑ c∈I+w Λ · ũcd − r−qcd
∂L
∂ũcd = k∑ d′=0 ũcd′p w dd′α",4.2 Decouple,[0],[0]
"− c− ∑ w∈I+c Λ · uwd − r−α−c qwd (11) where I+w denotes the set of positive contexts for w, I+c denotes the set of positive words for c and Λ = (α+wc−α−c )(∆−UwŨTc ).",4.2 Decouple,[0],[0]
"Algorithm 1 shows the training procedure of AllVec.
",4.2 Decouple,[0],[0]
"Algorithm 1 AllVec learning Input: corpus Γ, win, α0, δ, iter, learning rate η Output: embedding matrices U and Ũ
1: Build vocabulary V from Γ 2:",4.2 Decouple,[0],[0]
"Obtain all positive (w, c) and Mwc from Γ 3: Compute all r+wc, α + wc and α − c 4: Initialize U and Ũ 5: for i = 1, ..., iter do 6: for d ∈ {0, .., k} do 7: Compute and store qcd .O(|V |k) 8: for d′ ∈ {0, .., k} do 9: Compute and store pcdd′ .O(|V",4.2 Decouple,[0],[0]
"|k 2) 10: end for 11: end for 12: for w ∈ V do 13: Compute Λ .O(|S|k) 14: for d ∈ {0, .., k} do 15: Update uwd .O(|S|k + |V |k2) 16: end for 17: end for 18: Repeat 6-17 for ũcd .O(2|S|k+2|V |k2) 19: end for",4.2 Decouple,[0],[0]
"In the following, we show that AllVec can achieve the same time complexity with negative sampling based SGD methods.
",4.3 Time Complexity Analysis,[0],[0]
"Given the sample size n, the total time complexity for SG is O((n + 1)|S|k), where n + 1 denotes n negative samples and 1 positive example.",4.3 Time Complexity Analysis,[0],[0]
"Regarding the complexity of AllVec, we can see that the overall complexity of Algorithm 1 is O(4|S|k + 4|V |k2).
",4.3 Time Complexity Analysis,[0],[0]
"For the ease of discussion, we denote c as the average number of positive contexts for a word in the training corpus, i.e. |S| = c|V | (c ≥ 1000 in most cases).",4.3 Time Complexity Analysis,[0],[0]
"We then obtain the ratio
4|S|k + 4|V |k2
(n+ 1)|S|k =
4
n+ 1 (1 +
k c ) (12)
where k is typically set from 100 to 300 (Mikolov et al., 2013a; Pennington et al., 2014), resulting in k ≤ c.",4.3 Time Complexity Analysis,[0],[0]
"Hence, we can give the lower and upper bound for the ratio:
4
n+1",4.3 Time Complexity Analysis,[0],[0]
"<
",4.3 Time Complexity Analysis,[0],[0]
"4|S|k+4|V |k2
(n+1)|S|k = 4 n+1 (1+ k c )≤ 8 n+1",4.3 Time Complexity Analysis,[0],[0]
"(13)
",4.3 Time Complexity Analysis,[0],[0]
The above analysis suggests that the complexity of AllVec is same as that of SGD with negative sample size between 3 and 7.,4.3 Time Complexity Analysis,[0],[0]
"In fact, considering that c is much larger than k in most datasets, the major cost of AllVec comes from the part 4|S|k (see Section 5.4 for details), which is linear with respect to the number of positive samples.",4.3 Time Complexity Analysis,[0],[0]
"We conduct experiments on three popular evaluation tasks, namely word analogy (Mikolov et al., 2013a), word similarity (Faruqui and Dyer, 2014) and QVEC (Tsvetkov et al., 2015).
",5 Experiments,[0],[0]
Word analogy task.,5 Experiments,[0],[0]
"The task aims to answer questions like, “a is to b as c is to ?”.",5 Experiments,[0],[0]
"We adopt the Google testbed2 which contains 19, 544 such questions in two categories: semantic and syntactic.",5 Experiments,[0],[0]
"The semantic questions are usually analogies about people or locations, like “king is to man as queen is to ?”, while the syntactic questions focus on forms or tenses, e.g., “swimming is to swim as running to ?”.
",5 Experiments,[0],[0]
Word similarity tasks.,5 Experiments,[0],[0]
"We perform evaluation on six datasets, including MEN (Bruni et al., 2012), MC (Miller and Charles, 1991), RW (Luong et al., 2013), RG (Rubenstein and Goodenough, 1965), WS-353 Similarity (WSim) and Relatedness (WRel) (Finkelstein et al., 2001).",5 Experiments,[0],[0]
"We compute the spearman rank correlation between the similarity scores calculated based on the trained embeddings and human labeled scores.
QVEC.",5 Experiments,[0],[0]
QVEC is an intrinsic evaluation metric of word embeddings based on the alignment to features extracted from manually crafted lexical resources.,5 Experiments,[0],[0]
"QVEC has shown strong correlation with the performance of embeddings in several semantic tasks (Tsvetkov et al., 2015).
",5 Experiments,[0],[0]
"We compare AllVec with the following word embedding methods.
",5 Experiments,[0],[0]
"• SG: This is the original skip-gram model with SGD and negative sampling (Mikolov et al., 2013a,b).",5 Experiments,[0],[0]
"• SGA: This is the skip-gram model with an
adaptive sampler (Chen et al., 2018).",5 Experiments,[0],[0]
"2https://code.google.com/archive/p/word2vec/
For all baselines, we use the original implementation released by the authors.",5 Experiments,[0],[0]
"We evaluate the performance of AllVec on four real-world corpora, namely Text83, NewsIR4, Wiki-sub and Wiki-all.",5.1 Datasets and Experimental Setup,[0],[0]
Wiki-sub is a subset of 2017 Wikipedia dump5.,5.1 Datasets and Experimental Setup,[0],[0]
"All corpora have been pre-processed by a standard pipeline (i.e. removing non-textual elements, lowercasing and tokenization).",5.1 Datasets and Experimental Setup,[0],[0]
"Table 1 summarizes the statistics of these corpora.
",5.1 Datasets and Experimental Setup,[0],[0]
"To obtain Mwc for positive (w, c) pairs, we follow GloVe where word pairs that are xwords apart contribute 1/x to Mwc.",5.1 Datasets and Experimental Setup,[0],[0]
The window size is set as win = 8.,5.1 Datasets and Experimental Setup,[0],[0]
"Regarding α+wc, we set xmax = 100 and ρ = 0.75.",5.1 Datasets and Experimental Setup,[0],[0]
"For a fair comparison, the embedding size k is set as 200 for all models and corpora.",5.1 Datasets and Experimental Setup,[0],[0]
"AllVec can be easily trained by AdaGrad (Zeiler, 2012) like GloVe or Newton-like (Bayer et al., 2017; Bradley et al., 2011) second order methods.",5.1 Datasets and Experimental Setup,[0],[0]
"For models based on negative sampling (i.e. SG, SGA and LexVec), the sample size is set as n = 25 for Text8, n = 10 for NewsIR and n = 5 for Wiki-sub and Wiki-all.",5.1 Datasets and Experimental Setup,[0],[0]
The setting is also suggested by Mikolov et al. (2013b).,5.1 Datasets and Experimental Setup,[0],[0]
Other detailed hyper-parameters are reported in Table 2.,5.1 Datasets and Experimental Setup,[0],[0]
We present results on the word analogy task in Table 2.,5.2 Accuracy Comparison,[0],[0]
"As shown, AllVec achieves the highest total accuracy (Tot.) in all corpora, particu-
3http://mattmahoney.net/dc/text8.zip 4http://research.signalmedia.co/newsir16/signal-
dataset.html 5https://dumps.wikimedia.org/enwiki/
larly in smaller corpora (Text8 and NewsIR).",5.2 Accuracy Comparison,[0],[0]
"The reason is that in smaller corpora the number of positive (w, c) pairs is very limited, thus making use of negative examples will bring more benefits.",5.2 Accuracy Comparison,[0],[0]
"Similar reason also explains the poor accuracy of GloVe in Text8, because GloVe does not consider negative samples.",5.2 Accuracy Comparison,[0],[0]
"Even in the very large corpus (Wiki-all), ignoring negative samples still results in sub-optimal performance.
",5.2 Accuracy Comparison,[0],[0]
"Our results also show that SGA achieves better performance than SG, which demonstrates the importance of a good sampling strategy.",5.2 Accuracy Comparison,[0],[0]
"However, regardless what sampler (except the full softmax sampling) is utilized and how many updates are taken, sampling is still a biased approach.",5.2 Accuracy Comparison,[0],[0]
"AllVec achieves the best performance because it is trained on the whole batch data for each parameter update rather than a fraction of sampled data.
",5.2 Accuracy Comparison,[0],[0]
Another interesting observation is AllVec performs better in semantic tasks in general.,5.2 Accuracy Comparison,[0],[0]
"The reason is that our model utilizes global co-occurrence statistics, which capture more semantic signals than syntactic signals.",5.2 Accuracy Comparison,[0],[0]
"While both AllVec and GloVe use global contexts, AllVec performs much better than GloVe in syntactic tasks.",5.2 Accuracy Comparison,[0],[0]
"We argue that the main reason is because AllVec can distill useful signals from negative examples, while GloVe simply ignores all negative information.",5.2 Accuracy Comparison,[0],[0]
"By contrast, local-window based methods, such as SG and SGA, are more effective to capture local sentence features, resulting in good performance on syntactic analogies.",5.2 Accuracy Comparison,[0],[0]
"However, Rekabsaz et al. (2017) argues that these local-window based methods may suffer from the topic shifting issue.
",5.2 Accuracy Comparison,[0],[0]
Table 3 and Table 4 provide results in the word similarity and QVEC tasks.,5.2 Accuracy Comparison,[0],[0]
"We can see that AllVec achieves the best performance in most tasks, which admits the advantage of batch learning with all samples.",5.2 Accuracy Comparison,[0],[0]
"Interestingly, although GloVe performs well in semantic analogy tasks, it shows extremely worse results in word similarity and QVEC.",5.2 Accuracy Comparison,[0],[0]
The reason shall be the same as that it performs poorly in syntactic tasks.,5.2 Accuracy Comparison,[0],[0]
"In this subsection, we investigate the impact of the proposed weighting scheme for negative (context) words.",5.3 Impact of α−c,[0],[0]
We show the performance change of word analogy tasks on NewsIR in Figure 2 by tuning α0 and δ.,5.3 Impact of α−c,[0],[0]
"Results in other corpora show similar trends thus are omitted due to space limitation.
",5.3 Impact of α−c,[0],[0]
"Table 2: Results (“Tot.” denotes total accuracy) on the word analogy task.
",5.3 Impact of α−c,[0],[0]
"Corpus Text8 NewsIR
para.",5.3 Impact of α−c,[0],[0]
Sem.,5.3 Impact of α−c,[0],[0]
Syn.,5.3 Impact of α−c,[0],[0]
Tot.,5.3 Impact of α−c,[0],[0]
para.,5.3 Impact of α−c,[0],[0]
Sem.,5.3 Impact of α−c,[0],[0]
Syn.,5.3 Impact of α−c,[0],[0]
"Tot.
SG 1e-4 8 25 47.51 32.26 38.60 1e-5 10 10 70.81 47.48 58.10 SGA 6e-3 - - 48.10 33.78 39.74 6e-3 - - 71.74 48.71 59.20 GloVe 10 15 1 45.11 26.89 34.47 50 8 1 78.79 41.58 58.52 LexVec 1e-4 25 - 51.87 31.78 40.14 1e-5 10 - 76.11 39.09 55.95 AllVec 350 0.75 - 56.66 32.42 42.50 100 0.8 - 78.47 48.33 61.57
Wiki-sub Wiki-all
SG 1e-5 10 5 72.05 55.88 63.24 1e-5 10 5 73.91 61.91 67.37 SGA 6e-3 - - 73.93 56.10 63.81 6e-3 - - 75.11 61.94 67.92 GloVe 100 8 1 77.22 53.16 64.13 100 8 1 77.38 58.94 67.33 LexVec 1e-5 5 - 75.95 52.78 63.33 1e-5 5 - 76.31 56.83 65.48 AllVec 100 0.75 - 76.66 54.72 64.75 50 0.75 - 77.64 60.96 68.52
The parameter columns (para.)",5.3 Impact of α−c,[0],[0]
for each model are given from left to right as follows.,5.3 Impact of α−c,[0],[0]
"SG: subsampling of frequent words, window size and the number of negative samples; SGA: λ (Chen et al., 2018) that controls the distribution of the rank, the other parameters are the same with SG; GloVe:",5.3 Impact of α−c,[0],[0]
"xmax, window size and symmetric window; LexVec: subsampling of frequent words and the number of negative samples; AllVec: the negative weight α0 and δ.",5.3 Impact of α−c,[0],[0]
"Boldface denotes the highest total accuracy.
",5.3 Impact of α−c,[0],[0]
Figure 2(a) shows the impact of the overall weight α0 by setting δ as 0.75 (inspired by the setting of skip-gram).,5.3 Impact of α−c,[0],[0]
"Clearly, we observe that all results (including semantic, syntactic and total accuracy) have been greatly improved when α0 increases from 0 to a larger value.",5.3 Impact of α−c,[0],[0]
"As mentioned before, α0 = 0 means that no negative information is considered.",5.3 Impact of α−c,[0],[0]
This observation verifies that negative samples are very important for learning good embeddings.,5.3 Impact of α−c,[0],[0]
It also helps to explain why GloVe performs poorly on syntactic tasks.,5.3 Impact of α−c,[0],[0]
"In addition, we find that in all corpora the optimal results are usually obtained when α0 falls in the range of 50 to 400.",5.3 Impact of α−c,[0],[0]
"For example, in the NewIR corpus as shown, AllVec achieves the best performance when α0 = 100.",5.3 Impact of α−c,[0],[0]
Figure 2(b) shows the impact of δ with α0 = 100.,5.3 Impact of α−c,[0],[0]
"As mentioned before, δ = 0 denotes a uniform value for all negative words and δ = 1 denotes that no smoothing is applied to word frequency.",5.3 Impact of α−c,[0],[0]
We can see that the total accuracy is only around 55% when δ = 0.,5.3 Impact of α−c,[0],[0]
"By increasing its value, the performance is gradually improved, achieving the highest score when δ is around 0.8.",5.3 Impact of α−c,[0],[0]
Further increase of δ will degrade the total accuracy.,5.3 Impact of α−c,[0],[0]
This analysis demonstrates the effectiveness of the proposed negative weighting scheme.,5.3 Impact of α−c,[0],[0]
Figure 3(a) compares the convergence between AllVec and GloVe on NewsIR.,5.4 Convergence Rate and Runtime,[0],[0]
"Clearly, AllVec ex-
hibits a more stable convergence due to its full batch learning.",5.4 Convergence Rate and Runtime,[0],[0]
"In contrast, GloVe has a more dramatic fluctuation because of the one-sample learning scheme.",5.4 Convergence Rate and Runtime,[0],[0]
Figure 3(b) shows the relationship between the embedding size k and runtime on NewsIR.,5.4 Convergence Rate and Runtime,[0],[0]
"Although the analysis in Section 4.3 demonstrates that the time complexity of AllVec is O(4|S|k + 4|V |k2), the actual runtime shows a near linear relationship with k.",5.4 Convergence Rate and Runtime,[0],[0]
"This is because 4|V |k2/4|S|k = k/c, where c generally ranges from 1000 ∼ 6000 and k is set from 200 to 300 in practice.",5.4 Convergence Rate and Runtime,[0],[0]
"The above ratio explains the fact that 4|S|k dominates the complexity, which is linear
with k and |S|.",5.4 Convergence Rate and Runtime,[0],[0]
We also compare the overall runtime of AllVec and SG on NewsIR and show the results in Table 5.,5.4 Convergence Rate and Runtime,[0],[0]
"As can be seen, the runtime of AllVec falls in the range of SG-3 and SG-7 in a single iteration, which confirms the theoretical analysis in Section 4.3.",5.4 Convergence Rate and Runtime,[0],[0]
"In contrast with SG, AllVec needs more iterations to converge.",5.4 Convergence Rate and Runtime,[0],[0]
"The reason is that each parameter in SG is updated many times during each iteration, although only one training example is used in each update.",5.4 Convergence Rate and Runtime,[0],[0]
"Despite this, the total run time of AllVec is still in a feasible range.",5.4 Convergence Rate and Runtime,[0],[0]
"Assuming the convergence is measured by the number of parameter updates, our AllVec yields a much faster convergence rate than the one-sample SG method.
",5.4 Convergence Rate and Runtime,[0],[0]
"In practice, the runtime of our model in each iteration can be further reduced by increasing the number of parallel workers.",5.4 Convergence Rate and Runtime,[0],[0]
"Although baseline methods like SG and GloVe can also be parallelized, the stochastic gradient steps in these methods unnecessarily influence each other as there is no exact way to separate these updates for different workers.",5.4 Convergence Rate and Runtime,[0],[0]
"In other words, the parallelization of SGD is not well suited to a large number of work-
ers.",5.4 Convergence Rate and Runtime,[0],[0]
"In contrast, the parameter updates in AllVec are completely independent of each other, therefore AllVec does not have the update collision issue.",5.4 Convergence Rate and Runtime,[0],[0]
"This means we can achieve the embarrassing parallelization by simply separating the updates by words; that is, letting different workers update the model parameters for disjoint sets of words.",5.4 Convergence Rate and Runtime,[0],[0]
"As such, AllVec can provide a near linear scaling without any approximation since there is no potential conflicts between updates.",5.4 Convergence Rate and Runtime,[0],[0]
"In this paper, we presented AllVec, an efficient batch learning based word embedding model that is capable to leverage all positive and negative training examples without any sampling and approximation.",6 Conclusion,[0],[0]
"In contrast with models based on SGD and negative sampling, AllVec shows more stable convergence and better embedding quality by the all-sample optimization.",6 Conclusion,[0],[0]
"Besides, both theoretical analysis and experiments demonstrate that AllVec achieves the same time complexity with the classic SGD models.",6 Conclusion,[0],[0]
"In future, we will extend
our proposed all-sample learning scheme to deep learning methods, which are more expressive than the shallow embedding model.",6 Conclusion,[0],[0]
"Moreover, we will integrate prior knowledge, such as the words that are synonyms and antonyms, into the word embedding process.",6 Conclusion,[0],[0]
"Lastly, we are interested in exploring the recent adversarial learning techniques to enhance the robustness of word embeddings.
",6 Conclusion,[0],[0]
Acknowledgements.,6 Conclusion,[0],[0]
"This research is supported by the National Research Foundation, Prime Minister’s Office, Singapore under its IRC@SG Funding Initiative.",6 Conclusion,[0],[0]
Joemon M.Jose and Xiangnan,6 Conclusion,[0],[0]
He are corresponding authors.,6 Conclusion,[0],[0]
Stochastic Gradient Descent (SGD) with negative sampling is the most prevalent approach to learn word representations.,abstractText,[0],[0]
"However, it is known that sampling methods are biased especially when the sampling distribution deviates from the true data distribution.",abstractText,[0],[0]
"Besides, SGD suffers from dramatic fluctuation due to the onesample learning scheme.",abstractText,[0],[0]
"In this work, we propose AllVec that uses batch gradient learning to generate word representations from all training samples.",abstractText,[0],[0]
"Remarkably, the time complexity of AllVec remains at the same level as SGD, being determined by the number of positive samples rather than all samples.",abstractText,[0],[0]
We evaluate AllVec on several benchmark tasks.,abstractText,[0],[0]
"Experiments show that AllVec outperforms samplingbased SGD methods with comparable efficiency, especially for small training corpora.",abstractText,[0],[0]
Batch IS NOT Heavy: Learning Word Representations From All Samples,title,[0],[0]
Optimization is one of the fundamental pillars of modern machine learning.,1. Introduction,[0],[0]
"Considering that most modern machine learning methods involve the solution of some optimization problem, it is not surprising that many recent breakthroughs in this area have been on the back of more effective techniques for optimization.",1. Introduction,[0],[0]
"A case in point is deep learning, whose rise has been mirrored by the development of numerous techniques like batch normalization.
",1. Introduction,[0],[0]
"While modern algorithms have been shown to be very
*Equal contribution 1Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Massachusetts, USA 2DeepMind, London, UK.",1. Introduction,[0],[0]
"Correspondence to: Zi Wang <ziw@csail.mit.edu>, Chengtao Li <ctli@mit.edu>, Stefanie Jegelka <stefje@csail.mit.edu>, Pushmeet Kohli <pushmeet@google.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
effective for convex optimization problems defined over continuous domains, the same cannot be stated for nonconvex optimization, which has generally been dominated by stochastic techniques.",1. Introduction,[0],[0]
"During the last decade, Bayesian optimization has emerged as a popular approach for optimizing black-box functions.",1. Introduction,[0],[0]
"However, its applicability is limited to low-dimensional problems because of computational and statistical challenges that arise from optimization in high-dimensional settings.
",1. Introduction,[0],[0]
"In the past, these two problems have been addressed by assuming a simpler underlying structure of the black-box function.",1. Introduction,[0],[0]
"For instance, Djolonga et al. (2013) assume that the function being optimized has a low-dimensional effective subspace, and learn this subspace via low-rank matrix recovery.",1. Introduction,[0],[0]
"Similarly, Kandasamy et al. (2015) assume additive structure of the function where different constituent functions operate on disjoint low-dimensional subspaces.",1. Introduction,[0],[0]
The subspace decomposition can be partially optimized by searching possible decompositions and choosing the one with the highest GP marginal likelihood (treating the decomposition as a hyper-parameter of the GP).,1. Introduction,[0],[0]
"Fully optimizing the decomposition is, however, intractable.",1. Introduction,[0],[0]
"Li et al. (2016) extended (Kandasamy et al., 2015) to functions with a projected-additive structure, and approximate the projective matrix via projection pursuit with the assumption that the projected subspaces have the same and known dimensions.",1. Introduction,[0],[0]
The aforementioned approaches share the computational challenge of learning the groups of decomposed subspaces without assuming the dimensions of the subspaces are known.,1. Introduction,[0],[0]
"Both (Kandasamy et al., 2015) and subsequently (Li et al., 2016) adapt the decomposition by maximizing the GP marginal likelihood every certain number of iterations.",1. Introduction,[0],[0]
"However, such maximization is computationally intractable due to the combinatorial nature of the partitions of the feature space, which forces prior work to adopt randomized search heuristics.
",1. Introduction,[0],[0]
"In this paper, we develop a new formulation of Bayesian optimization specialized for high dimensions.",1. Introduction,[0],[0]
"One of the key contributions of this work is a new formulation that interprets prior work on high-dimensional Bayesian optimization (HDBO) through the lens of structured kernels, and places a prior on the kernel structure.",1. Introduction,[0],[0]
"Thereby, our
formulation enables simultaneous learning of the decomposition of the function domain.
",1. Introduction,[0],[0]
Prior work on latent decomposition of the feature space considers the setting where exploration/evaluation is performed once at a time.,1. Introduction,[0],[0]
"This approach makes Bayesian optimization time-consuming for problems where a large number of function evaluations need to be made, which is the case for high dimensional problems.",1. Introduction,[0],[0]
"To overcome this restriction, we extend our approach to a batched version that allows multiple function evaluations to be performed in parallel (Desautels et al., 2014; González et al., 2016; Kathuria et al., 2016).",1. Introduction,[0],[0]
"Our second contribution is an approach to select the batch of evaluations for structured kernel learning-based HDBO.
",1. Introduction,[0],[0]
Other Related Work.,1. Introduction,[0],[0]
"In the past half century, a series of different acquisition functions was developed for sequential BO in relatively low dimensions (Kushner, 1964; Moc̆kus, 1974; Srinivas et al., 2012; Hennig & Schuler, 2012; Hernández-Lobato et al., 2014; Kawaguchi et al., 2015; Wang et al., 2016a; Kawaguchi et al., 2016; Wang & Jegelka, 2017).",1. Introduction,[0],[0]
"More recent developments address high dimensional BO by making assumptions on the latent structure of the function to be optimized, such as lowdimensional structure (Wang et al., 2016b; Djolonga et al., 2013) or additive structure of the function (Li et al., 2016; Kandasamy et al., 2015).",1. Introduction,[0],[0]
"Duvenaud et al. (2013) explicitly search over kernel structures.
",1. Introduction,[0],[0]
"While the aforementioned methods are sequential in nature, the growth of computing power has motivated settings where at once a batch of points is selected for observation (Contal et al., 2013; Desautels et al., 2014; González et al., 2016; Snoek et al., 2012; Wang et al., 2017).",1. Introduction,[0],[0]
"For example, the UCB-PE algorithm (Contal et al., 2013) exploits that the posterior variance of a Gaussian Process is independent of the function mean.",1. Introduction,[0],[0]
"It greedily selects points with the highest posterior variance, and is able to update the variances without observations in between selections.",1. Introduction,[0],[0]
"Similarly, B-UCB (Desautels et al., 2014) greedily chooses points with the highest UCB score computed via the out-dated function mean but up-to-date function variances.",1. Introduction,[0],[0]
"However, these methods may be too greedy in their selection, resulting in points that lie far from an optimum.",1. Introduction,[0],[0]
"More recently, Kathuria et al. (2016) tries to resolve this issue by sampling the batch via a diversity-promoting distribution for better randomized exploration, while Wang et al. (2017) quantifies the goodness of the batch with a submodular surrogate function that trades off quality and diversity.",1. Introduction,[0],[0]
Let f : X → R be an unknown function and we aim to optimize it over a compact set X ⊆ RD.,2. Background,[0],[0]
"Within as few
function evaluations as possible, we want to find
f(x∗) = max x∈X f(x).
",2. Background,[0],[0]
"Following (Kandasamy et al., 2015), we assume a latent decomposition of the feature dimensions [D] = {1, . . .",2. Background,[0],[0]
", D} into disjoint subspaces, namely, ⋃M m=1Am = [D] and Ai ∩ Aj = ∅ for all i 6= j, i, j ∈",2. Background,[0],[0]
[D].,2. Background,[0],[0]
"Further, f can be decomposed into the following additive form:
f(x) = ∑
m∈[M ]
fm(x Am).
",2. Background,[0],[0]
"To make the problem tractable, we assume that each fm is drawn independently from GP(0, k(m)) for all m ∈",2. Background,[0],[0]
[M ].,2. Background,[0],[0]
"The resulting f will also be a sample from a GP: f ∼ GP(µ, k), where the priors are µ(x) = ∑ m∈[M ] µm(x",2. Background,[0],[0]
"Am)
and k(x, x′) = ∑ m∈[M ] k (m)(xAm , x′ Am).",2. Background,[0],[0]
"Let Dn = {(xt, yt)}nt=1 be the data we observed from f , where yt ∼ N (f(xt), σ).",2. Background,[0],[0]
"The log data likelihood for Dn is
log p(Dn|{k(m), Am}m∈[M ]) (2.1)
",2. Background,[0],[0]
= −1 2 (yT(Kn + σ,2. Background,[0],[0]
2I)−1y + log |Kn + σ2I|+ n,2. Background,[0],[0]
"log 2π)
",2. Background,[0],[0]
where Kn =,2. Background,[0],[0]
"[∑M m=1 k (m)(xAmi , x Am j ) ]",2. Background,[0],[0]
"i≤n,j≤n is the gram matrix associated with Dn, and y =",2. Background,[0],[0]
[yt]t≤n are the concatenated observed function values.,2. Background,[0],[0]
"Conditioned on the observations Dn, we can infer the posterior mean and covariance function of the function component f (m) to be
µ(m)n (x Am) =",2. Background,[0],[0]
"k(m)n (x Am)T(Kn + σ 2I)−1y,
k(m)n (x Am , x′ Am) = k(m)(xAm , x′ Am)
",2. Background,[0],[0]
− k(m)n (xAm)T(Kn + σ2I)−1k(m)n,2. Background,[0],[0]
"(x′ Am),
where k(m)n (xAm) =",2. Background,[0],[0]
"[k(m)(xAmt , x Am)]t≤n.
",2. Background,[0],[0]
"We use regret to evaluate the BO algorithms, both in the sequential and the batch selection case.",2. Background,[0],[0]
"For the sequential selection, let r̃t = maxx∈X f(x) − f(xt) denote the immediate regret at iteration t. We are interested in both the averaged cumulative regret RT = 1T ∑ t r̃t and the simple regret rT = mint≤T r̃t for a total number of T iterations.",2. Background,[0],[0]
"For batch evaluations, r̃t = maxx∈X ,b∈[B] f(x) − f(xt,b) denotes the immediate regret obtained by the batch at iteration t. The averaged cumulative regret of the batch setting is RT = 1T ∑ t r̃t, and the simple regret rT = mint≤T r̃t.",2. Background,[0],[0]
"We use the averaged cumulative regret in the bandit setting, where each evaluation of the function incurs a cost.",2. Background,[0],[0]
"If we simply want to optimize the function, we use the simple regret to capture the minimum gap between the best point found and the global optimum of the black-box function f .",2. Background,[0],[0]
Note that the averaged cumulative regret upper bounds the simple regret.,2. Background,[0],[0]
We take a Bayesian view on the task of learning the latent structure of the GP kernel.,3. Learning Additive Kernel Structure,[0],[0]
The decomposition of the input space X will be learned simultaneously with optimization as more and more data is observed.,3. Learning Additive Kernel Structure,[0],[0]
Our generative model draws mixing proportions θ ∼ DIR(α).,3. Learning Additive Kernel Structure,[0],[0]
Each dimension j is assigned to one out of M groups via the decomposition assignment variable zj ∼ MULTI(θ).,3. Learning Additive Kernel Structure,[0],[0]
"The objective function is then f(x) = ∑M m=1 fm(x
Am), where Am = {j : zj = m} is the set of support dimensions for function fm, and each fm is drawn from a Gaussian Process.",3. Learning Additive Kernel Structure,[0],[0]
"Finally, given an input x, we observe y ∼ N (f(x), σ).",3. Learning Additive Kernel Structure,[0],[0]
"Figure 1 illustrates the corresponding graphical model.
",3. Learning Additive Kernel Structure,[0],[0]
"Given the observed data Dn = {(xt, yt)}nt=1, we obtain a posterior distribution over possible decompositions z (and mixing proportions θ) that we will include later in the BO process:
p(z, θ | Dn;α) ∝",3. Learning Additive Kernel Structure,[0],[0]
p(Dn |,3. Learning Additive Kernel Structure,[0],[0]
z)p(z,3. Learning Additive Kernel Structure,[0],[0]
"| θ)p(θ;α).
",3. Learning Additive Kernel Structure,[0],[0]
"Marginalizing over θ yields the posterior distribution of the decomposition assignment
p(z | Dn;α) ∝",3. Learning Additive Kernel Structure,[0],[0]
p(Dn,3. Learning Additive Kernel Structure,[0],[0]
| z) ∫,3. Learning Additive Kernel Structure,[0],[0]
p(z | θ)p(θ;α),3. Learning Additive Kernel Structure,[0],[0]
"dθ
∝ p(Dn",3. Learning Additive Kernel Structure,[0],[0]
| z),3. Learning Additive Kernel Structure,[0],[0]
"Γ( ∑ m αm)
Γ(D + ∑ m αm)",3. Learning Additive Kernel Structure,[0],[0]
∏,3. Learning Additive Kernel Structure,[0],[0]
"m Γ(|Am|+ αm) Γ(αm)
where p(Dn|z) is the data likelihood (2.1) for the additive GP given a fixed structure defined by",3. Learning Additive Kernel Structure,[0],[0]
"z. We learn the posterior distribution for z via Gibbs sampling, choose the decomposition among the samples that achieves the highest data likelihood, and then proceed with BO.",3. Learning Additive Kernel Structure,[0],[0]
"The Gibbs sampler repeatedly draws coordinate assignments zj according to
p(zj = m | z¬j ,Dn; α) ∝ p(Dn",3. Learning Additive Kernel Structure,[0],[0]
| z)p(zj | z¬j) ∝,3. Learning Additive Kernel Structure,[0],[0]
"p(Dn | z)(|Am|+ αm) ∝ eφm ,
where
φm = − 1
2 yT(K(zj=m)n + σ",3. Learning Additive Kernel Structure,[0],[0]
"2I)−1y
− 1 2 log |K(zj=m)n + σ2I|+ log(|Am|+ αm)
and K(zj=m)n is the gram matrix associated with the observations Dn by setting zj = m. We can use the Gumbel trick to efficiently sample from this categorical distribution.",3. Learning Additive Kernel Structure,[0],[0]
"Namely, we sample a vector of i.i.d standard Gumbel variables ωi of length M , and then choose the sampled decomposition assignment zj = arg maxi≤M φi + ωi.
",3. Learning Additive Kernel Structure,[0],[0]
"With a Dirichlet process, we could make the model nonparametric and the number M of possible groups in the decomposition infinite.",3. Learning Additive Kernel Structure,[0],[0]
"Given that we have a fixed number of input dimension D, we set M = D in practice.",3. Learning Additive Kernel Structure,[0],[0]
"In real-world applications where function evaluations translate into time-intensive experiments, the typical sequential exploration strategy – observe one function value, update the model, then select the next observation – is undesirable.",4. Diverse Batch Sampling,[0],[0]
"Batched Bayesian Optimization (BBO) (Azimi et al., 2010; Contal et al., 2013; Kathuria et al., 2016) instead selects a batch of B observations to be made in parallel, then the model is updated with all simultaneously.
",4. Diverse Batch Sampling,[0],[0]
"Extending this scenario to high dimensions, two questions arise: (1) the acquisition function is expensive to optimize and (2), by itself, does not sufficiently account for exploration.",4. Diverse Batch Sampling,[0],[0]
The additive kernel structure improves efficiency for (1).,4. Diverse Batch Sampling,[0],[0]
"For batch selection (2), we need an efficient strategy that enourages observations that are both informative and non-redundant.",4. Diverse Batch Sampling,[0],[0]
"Recent work (Contal et al., 2013; Kathuria et al., 2016) selects a point that maximizes the acquisition function, and adds additional batch points via a diversity criterion.",4. Diverse Batch Sampling,[0],[0]
"In high dimensions, this diverse selection becomes expensive.",4. Diverse Batch Sampling,[0],[0]
"For example, if each dimension has a finite number of possible values1, the cost of sampling batch points via a Determinantal Point Process (DPP), as proposed in (Kathuria et al., 2016), grows exponentially with the number of dimensions.",4. Diverse Batch Sampling,[0],[0]
"The same obstacle arises with the approach by Contal et al. (2013), where points are selected greedily.",4. Diverse Batch Sampling,[0],[0]
"Thus, naı̈ve adoptions of these approaches in our setting would result in intractable algorithms.",4. Diverse Batch Sampling,[0],[0]
"Instead, we propose a general approach that explicitly takes advantage of the structured kernel to enable relevant, non-redundant high-dimensional batch selection.
",4. Diverse Batch Sampling,[0],[0]
We describe our approach for a single decomposition sampled from the posterior; it extends to a distribution of decompositions by sampling a set of decompositions from the posterior and then sampling points for each decomposition individually.,4. Diverse Batch Sampling,[0],[0]
"Given a decomposition z, we define a separate Determinantal Point Process (DPP) on each group of Am dimensions.",4. Diverse Batch Sampling,[0],[0]
"A set S of points in the subspace R|Am| is sampled with probability proportional to det(K(m)n (S)),
1While we use this discrete categorical domain to illustrate the batch setting, our proposed method is general and is applicable to continuous box-constrained domains.
where K(m)n is the posterior covariance matrix of the mth group given n observations, and K(S) is the submatrix of K with rows and columns indexed by S. Assuming the group sizes are upper-bounded by some constant, sampling from each such DPP individually implies an exponential speedup compared to using the full kernel.
",4. Diverse Batch Sampling,[0],[0]
Sampling vs. Greedy Maximization The determinant det(K (m) n,4. Diverse Batch Sampling,[0],[0]
"(S)) measures diversity, and hence the DPP assigns higher probability to diverse subsets S. An alternative to sampling is to directly maximize the determinant.",4. Diverse Batch Sampling,[0],[0]
"While this is NP-hard, a greedy strategy gives an approximate solution, and is used in (Kathuria et al., 2016), and in (Contal et al., 2013) as Pure Exploration (PE).",4. Diverse Batch Sampling,[0],[0]
We too test this strategy in the experiments.,4. Diverse Batch Sampling,[0],[0]
"In the beginning, if the GP is not approximating the function well, then greedy may perform no better than a stochastic combination of coordinates, as we observe in Fig. 6.
",4. Diverse Batch Sampling,[0],[0]
Sample Combination Now we have chosen a diverse subset Xm = {x(m)i }i∈[B−1],4. Diverse Batch Sampling,[0],[0]
⊂,4. Diverse Batch Sampling,[0],[0]
R|Am| of size (B − 1) for each group Am.,4. Diverse Batch Sampling,[0],[0]
We need to combine these subspace points to obtain B − 1 final batch query points in RD.,4. Diverse Batch Sampling,[0],[0]
"A simple way to combine samples from each group is to do it randomly without replacement, i.e., we sample one x (m) i from each Xm uniformly randomly without replacement, and combine the parts, one for each m ∈",4. Diverse Batch Sampling,[0],[0]
"[M ], to get one sample in RD.",4. Diverse Batch Sampling,[0],[0]
We repeat this procedure until we have (B − 1) points.,4. Diverse Batch Sampling,[0],[0]
"This retains diversity across the batch of samples, since the samples are diverse within each group of features.
",4. Diverse Batch Sampling,[0],[0]
"Besides this random combination, we can also combine samples greedily.",4. Diverse Batch Sampling,[0],[0]
We define a quality function ψ(m)t for each group m ∈,4. Diverse Batch Sampling,[0],[0]
"[M ] at time t, and combine samples to maximize this quality function.",4. Diverse Batch Sampling,[0],[0]
"Concretely, for the first point, we combine the maximizers x(m)∗ = arg maxx(m)∈Xm ψ",4. Diverse Batch Sampling,[0],[0]
"(m) t (x
(m)) from each group.",4. Diverse Batch Sampling,[0],[0]
"We remove those used parts, Xm ← Xm\{x(m)∗ }, and repeat the procedure until we have (B − 1) samples.",4. Diverse Batch Sampling,[0],[0]
"In each iteration, the sample achieving the highest quality score gets selected, while diversity is retained.
",4. Diverse Batch Sampling,[0],[0]
"Both selection strategies can be combined with a wide range of existing quality and acquisition functions.
",4. Diverse Batch Sampling,[0],[0]
"Add-UCB-DPP-BBO We illustrate the above framework with GP-UCB (Srinivas et al., 2012) as both the acquisition and quality functions.",4. Diverse Batch Sampling,[0],[0]
The Upper Confidence Bound (f (m)t ) + and Lower Confidence Bound (f (m)t ),4. Diverse Batch Sampling,[0],[0]
"− with parameter βt for group m at time t are
(f (m) t )",4. Diverse Batch Sampling,[0],[0]
+(x) = µ (m) t−1(x) +,4. Diverse Batch Sampling,[0],[0]
β 1/2 t σ,4. Diverse Batch Sampling,[0],[0]
"(m) t (x); (4.1)
(f (m) t ) −(x) = µ (m) t−1(x)− β 1/2 t σ",4. Diverse Batch Sampling,[0],[0]
"(m) t (x),
and combine the expected value µ(m)t−1(x) of f (m) t with the uncertainty β1/2t σ (m) t (x).",4. Diverse Batch Sampling,[0],[0]
"We set both the acquisition function and quality function ψ(m)t to be (f (m) t )
+ for group m at time t.
To ensure that we select points with high acquisition function values, we follow (Contal et al., 2013; Kathuria et al., 2016) and define a relevance region R(m)t for each group m as
R(m)t = {x ∈",4. Diverse Batch Sampling,[0],[0]
"Xm |
µ (m) t−1(x) + 2 √ β (m) t+1σ",4. Diverse Batch Sampling,[0],[0]
(m) t−1(x) ≥ (y (m) t ),4. Diverse Batch Sampling,[0],[0]
"• } ,
where (y(m)t ) • = maxx(m)∈Xm(f (m) t ) −(x(m)).",4. Diverse Batch Sampling,[0],[0]
We then use R(m)t as the ground set to sample with PE/DPP.,4. Diverse Batch Sampling,[0],[0]
The full algorithm is shown in the appendix.,4. Diverse Batch Sampling,[0],[0]
"We empirically evaluate our approach in two parts: First, we verify the effectiveness of using our Gibbs sampling algorithm to learn the additive structure of the unknown function, and then we test our batch BO for high dimensional problems with the Gibbs sampler.",5. Empirical Results,[0],[0]
Our code is available at https://github.com/zi-w/ Structural-Kernel-Learning-for-HDBBO.,5. Empirical Results,[0],[0]
We first probe the effectiveness of using the Gibbs sampling method described in Section 3 to learn the decomposition of the input space.,5.1. Effectiveness of Decomposition Learning,[0],[0]
"More details of the experiments including sensitivity analysis for α can be found in the appendix.
",5.1. Effectiveness of Decomposition Learning,[0],[0]
"Recovering Decompositions First, we sample test functions from a known additive Gaussian Process prior with zero-mean and isotropic Gaussian kernel with bandwidth = 0.1 and scale = 5 for each function component.",5.1. Effectiveness of Decomposition Learning,[0],[0]
"For D = 2, 5, 10, 20, 50, 100 input dimensions, we randomly sample decomposition settings that have at least two groups in the decomposition and at most 3 dimensions in each group.
",5.1. Effectiveness of Decomposition Learning,[0],[0]
"We set the burn-in period to be 50 iterations, and the total number of iterations for Gibbs sampling to be 100.",5.1. Effectiveness of Decomposition Learning,[0],[0]
"In Tables 1 and 2, we show two quantities that are closely related
to the learned empirical posterior of the decompositions with different numbers of randomly sampled observed data points (N ).",5.1. Effectiveness of Decomposition Learning,[0],[0]
"Table 1 shows the probability of two dimensions being correctly grouped together by Gibbs sampling in each iteration of Gibbs sampling after the burn-in period, namely, ( ∑ i<j≤D 1zgi≡z g j∧zi≡zj )/",5.1. Effectiveness of Decomposition Learning,[0],[0]
( ∑ i<j≤D 1zi≡zj ).,5.1. Effectiveness of Decomposition Learning,[0],[0]
"Table 2 reports the probability of two dimensions being correctly separated in each iteration of Gibbs sampling after the burn-in period, namely, ( ∑ i<j≤D 1zgi 6=z g j∧zi 6=zj )/",5.1. Effectiveness of Decomposition Learning,[0],[0]
( ∑ i<j≤D 1zi 6=zj ).,5.1. Effectiveness of Decomposition Learning,[0],[0]
"The results show that the more data we observe, the more accurate the learned decompositions are.",5.1. Effectiveness of Decomposition Learning,[0],[0]
They also suggest that the Gibbs sampling procedure can converge to the ground truth decomposition with enough data for relatively small numbers of dimensions.,5.1. Effectiveness of Decomposition Learning,[0],[0]
"The higher the dimension, the more data we need to recover the true decomposition.
",5.1. Effectiveness of Decomposition Learning,[0],[0]
"Effectiveness of Learning Decompositions for Bayesian Optimization To verify the effectiveness of the learned decomposition for Bayesian optimization, we tested on 2, 10, 20 and 50 dimensional functions sampled from a zero-mean Add-GP with randomly sampled decomposi-
tion settings (at least two groups, at most 3 dimensions in each group) and isotropic Gaussian kernel with bandwidth = 0.1 and scale = 5.",5.1. Effectiveness of Decomposition Learning,[0],[0]
Each experiment was repeated 50 times.,5.1. Effectiveness of Decomposition Learning,[0],[0]
An example of a 2-dimensional function component is shown in the appendix.,5.1. Effectiveness of Decomposition Learning,[0],[0]
"For Add-GPUCB, we used β(m)t = |Am| log 2t for lower dimensions (D = 2, 5, 10), and β(m)t = |Am| log 2t/5 for higher dimensions (D = 20, 30, 50).",5.1. Effectiveness of Decomposition Learning,[0],[0]
"We show parts of the results on averaged cumulative regret and simple regret in Fig. 2, and the rest in the appendix.",5.1. Effectiveness of Decomposition Learning,[0],[0]
"We compare Add-GP-UCB with known additive structure (Known), no partitions (NP), fully partitioned with one dimension for each group (FP)
and the following methods of learning the decomposition: Gibbs sampling (Gibbs), randomly sampling the same number of decompositions sampled by Gibbs and select the one with the highest data likelihood (PL-1), randomly sampling 5 decompositions and selecting the one with the highest data likelihood (PL-2).",5.1. Effectiveness of Decomposition Learning,[0],[0]
"For the latter two learning methods are referred to as “partial learning” in (Kandasamy et al., 2015).",5.1. Effectiveness of Decomposition Learning,[0],[0]
The learning of the decomposition is done every 50 iterations.,5.1. Effectiveness of Decomposition Learning,[0],[0]
"Fig. 3 shows the improvement of learning decompositions with Gibbs over optimizing without partitions (NP).
",5.1. Effectiveness of Decomposition Learning,[0],[0]
"Overall, the results show that Gibbs outperforms both of the partial learning methods, and for higher dimensions, Gibbs is sometimes even better than Known.",5.1. Effectiveness of Decomposition Learning,[0],[0]
"Interestingly, similar results can be found in Fig. 3 (c) of (Kandasamy et al., 2015), where different decompositions than the ground truth may give better simple regret.",5.1. Effectiveness of Decomposition Learning,[0],[0]
"We conjecture that this is because Gibbs is able to explore more than Known, for two reasons:
1.",5.1. Effectiveness of Decomposition Learning,[0],[0]
"Empirically, Gibbs changes the decompositions across iterations, especially in the beginning.",5.1. Effectiveness of Decomposition Learning,[0],[0]
"With fluctuating partitions, even exploitation leads to moving around, because the supposedly “good” points are influenced by the partition.",5.1. Effectiveness of Decomposition Learning,[0],[0]
"The result is an implicit “exploration” effect that is absent with a fixed partition.
",5.1. Effectiveness of Decomposition Learning,[0],[0]
2.,5.1. Effectiveness of Decomposition Learning,[0],[0]
Gibbs sometimes merges “true” parts into larger parts.,5.1. Effectiveness of Decomposition Learning,[0],[0]
"The parameter βt in UCB depends on the size of the part, |Am|(log 2t)/5 (as in (Kandasamy et al., 2015)).",5.1. Effectiveness of Decomposition Learning,[0],[0]
"Larger parts hence lead to larger βt and hence more exploration.
",5.1. Effectiveness of Decomposition Learning,[0],[0]
"Of course, more exploration is not always better, but Gibbs was able to find a good balance between exploration and exploitation, which leads to better performance.",5.1. Effectiveness of Decomposition Learning,[0],[0]
Our preliminary experiments indicate that one solution to ensure that the ground truth decomposition produces the best result is to tune βt.,5.1. Effectiveness of Decomposition Learning,[0],[0]
"Hyperparameter selection (such as choosing βt) for BO is, however, very challenging and an active topic of research (e.g. (Wang et al., 2016a)).
",5.1. Effectiveness of Decomposition Learning,[0],[0]
"Next, we test the decomposition learning algorithm on a real-world function, which returns the distance between a designated goal location and two objects being pushed by two robot hands, whose trajectory is determined by 14 parameters specifying the location, rotation, velocity, moving direction etc.",5.1. Effectiveness of Decomposition Learning,[0],[0]
"This function is implemented with a physics engine, the Box2D simulator (Catto, 2011).",5.1. Effectiveness of Decomposition Learning,[0],[0]
We use add-GP-UCB with different ways of setting the additive structure to tune the parameters for the robot hand so as to push the object closer to the goal.,5.1. Effectiveness of Decomposition Learning,[0],[0]
The regrets are shown in Fig. 4.,5.1. Effectiveness of Decomposition Learning,[0],[0]
"We observe that the performance of learning the decomposition with Gibbs dominates all existing
alternatives including partial learning.",5.1. Effectiveness of Decomposition Learning,[0],[0]
"Since the function we tested here is composed of the distance to two objects, there could be some underlying additive structure for this function in certain regions of the input space, e.g. when the two robots hands are relatively distant from each other so that one of the hands only impacts one of the objects.",5.1. Effectiveness of Decomposition Learning,[0],[0]
"Hence, it is possible for Gibbs to learn a good underlying additive structure and perform effective BO with the structures it learned.",5.1. Effectiveness of Decomposition Learning,[0],[0]
"Next, we probe the effectiveness of batch BO in high dimensions.",5.2. Diverse Batch Sampling,[0],[0]
"In particular, we compare variants of the AddUCB-DPP-BBO approach outlined in Section 4, and a baseline:
• Rand:",5.2. Diverse Batch Sampling,[0],[0]
"All batch points are chosen uniformly at random from X .
• Batch-UCB-*: *∈ {PE,DPP}.",5.2. Diverse Batch Sampling,[0],[0]
All acquisition functions are UCB (Eq. 4.1).,5.2. Diverse Batch Sampling,[0],[0]
Exploration is done via PE or DPP with posterior covariance kernels for each group.,5.2. Diverse Batch Sampling,[0],[0]
"Combination is via sampling without replacement.
",5.2. Diverse Batch Sampling,[0],[0]
"• *-Fnc: *∈ {Batch-UCB-PE,Batch-UCB-DPP}.",5.2. Diverse Batch Sampling,[0],[0]
"All quality functions are also UCB’s, and combination is done by maximizing the quality functions.
",5.2. Diverse Batch Sampling,[0],[0]
"A direct application of existing batch selection methods is very inefficient in the high-dimensional settings where they differ more, algorithmically, from our approach that ex-
ploits decompositions.",5.2. Diverse Batch Sampling,[0],[0]
"Hence, we only compare to uniform sampling as a baseline.
",5.2. Diverse Batch Sampling,[0],[0]
"Effectiveness We tested on 2, 10, 20 and 50-dimensional functions sampled the same way as in Section 5.1; we assume the ground-truth decomposition of the feature space is known.",5.2. Diverse Batch Sampling,[0],[0]
"Since Rand performs the worst, we show relative averaged cumulative regret and simple regret of all methods compared to Rand in Fig. 5.",5.2. Diverse Batch Sampling,[0],[0]
Results for absolute values of regrets are shown in the appendix.,5.2. Diverse Batch Sampling,[0],[0]
Each experiment was repeated for 20 times.,5.2. Diverse Batch Sampling,[0],[0]
"For all experiments, we set βmt = |Am| log 2t and B = 10.",5.2. Diverse Batch Sampling,[0],[0]
"All diverse batch sampling methods perform comparably well and far better than Rand, although there exist slight differences.",5.2. Diverse Batch Sampling,[0],[0]
"While in lower dimensions (D ∈ {2, 10}), Batch-UCB-PE-Fnc performs among the best, in higher dimensions (D ∈ {20, 50}), Batch-UCB-DPP-Fnc performs better than (or comparable to) all other variants.",5.2. Diverse Batch Sampling,[0],[0]
"We will see a larger performance gap in later real-world experiments, showing that biasing the combination towards higher quality functions while retaining diversity across the batch of samples provides a better exploration-exploitation trade-off.
",5.2. Diverse Batch Sampling,[0],[0]
"For a real-data experiment, we tested the diverse batch sampling algorithms for BBO on the Walker function which returns the walking speed of a three-link planar bipedal walker implemented in Matlab (Westervelt et al., 2007).",5.2. Diverse Batch Sampling,[0],[0]
"We tune 25 parameters that may influence the walking speed, including 3 sets of 8 parameters for the ODE solver and 1 parameter specifying the initial velocity of the stance leg.",5.2. Diverse Batch Sampling,[0],[0]
"We discretize each dimension into 40 points, resulting in a function domain of |X | = 4025.",5.2. Diverse Batch Sampling,[0],[0]
This size is very inefficient for existing batch sampling techniques.,5.2. Diverse Batch Sampling,[0],[0]
We learn the additive structure via Gibbs sampling and sample batches of size B = 5.,5.2. Diverse Batch Sampling,[0],[0]
"To further improve efficiency, we limit the maximum size of each group to 2.",5.2. Diverse Batch Sampling,[0],[0]
The regrets for all methods are shown in Fig. 6.,5.2. Diverse Batch Sampling,[0],[0]
"Again, all diverse batch sampling methods outperform Rand by a large gap.",5.2. Diverse Batch Sampling,[0],[0]
"Moreover, Batch-UCB-DPP-Fnc is a bit better than other variants, suggesting that a selection by quality functions is useful.
",5.2. Diverse Batch Sampling,[0],[0]
"Batch Sizes Finally, we show how the batch size B affects the performance of the proposed methods.",5.2. Diverse Batch Sampling,[0],[0]
"We test the algorithms on the 14-dimensional Robot dataset with B ∈ {5, 10}.",5.2. Diverse Batch Sampling,[0],[0]
The regrets are shown in Fig. 4.,5.2. Diverse Batch Sampling,[0],[0]
"With larger batches, the differences between the batch selection approaches become more pronounced.",5.2. Diverse Batch Sampling,[0],[0]
"In both settings, Batch-UCB-DPP-Fnc performs a bit better than other variants, in particular with larger batch sizes.",5.2. Diverse Batch Sampling,[0],[0]
"In this paper, we propose two novel solutions for high dimensional BO: inferring latent structure, and combining it with batch Bayesian Optimization.",6. Conclusion,[0],[0]
"The experimental results demonstrate that the proposed techniques are effective at optimizing high-dimensional black-box functions.
",6. Conclusion,[0],[0]
"Moreover, their gain over existing methods increases as the dimensionality of the input grows.",6. Conclusion,[0],[0]
We believe that these results have the potential to enable the increased use of Bayesian optimization for challenging black-box optimization problems in machine learning that typically involve a large number of parameters.,6. Conclusion,[0],[0]
"We gratefully acknowledge support from NSF CAREER award 1553284, NSF grants 1420927 and 1523767, from ONR grant N00014-14-1-0486, and from ARO grant W911NF1410433.",Acknowledgements,[0],[0]
We thank MIT Supercloud and the Lincoln Laboratory Supercomputing Center for providing computational resources.,Acknowledgements,[0],[0]
"Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of our sponsors.",Acknowledgements,[0],[0]
Optimization of high-dimensional black-box functions is an extremely challenging problem.,abstractText,[0],[0]
"While Bayesian optimization has emerged as a popular approach for optimizing black-box functions, its applicability has been limited to low-dimensional problems due to its computational and statistical challenges arising from high-dimensional settings.",abstractText,[0],[0]
"In this paper, we propose to tackle these challenges by (1) assuming a latent additive structure in the function and inferring it properly for more efficient and effective BO, and (2) performing multiple evaluations in parallel to reduce the number of iterations required by the method.",abstractText,[0],[0]
Our novel approach learns the latent structure with Gibbs sampling and constructs batched queries using determinantal point processes.,abstractText,[0],[0]
Experimental validations on both synthetic and real-world functions demonstrate that the proposed method outperforms the existing state-of-the-art approaches.,abstractText,[0],[0]
Batched High-dimensional Bayesian Optimization via Structural Kernel Learning,title,[0],[0]
"Boolean matrix factorisation aims to decompose a binary data matrix into an approximate Boolean product of two low rank, binary matrices: one containing meaningful patterns, the other quantifying how the observations can be expressed as a combination of these patterns. We introduce the OrMachine, a probabilistic generative model for Boolean matrix factorisation and derive a Metropolised Gibbs sampler that facilitates efficient parallel posterior inference. On real world and simulated data, our method outperforms all currently existing approaches for Boolean matrix factorisation and completion. This is the first method to provide full posterior inference for Boolean Matrix factorisation which is relevant in applications, e.g. for controlling false positive rates in collaborative filtering and, crucially, improves the interpretability of the inferred patterns. The proposed algorithm scales to large datasets as we demonstrate by analysing single cell gene expression data in 1.3 million mouse brain cells across 11 thousand genes on commodity hardware.",text,[0],[0]
"Boolean matrix factorisation (BooMF) can infer interpretable decompositions of a binary data matrix X ∈ {0, 1}N×D into a pair of low-rank, binary matrices Z ∈ {0, 1}N×L and U ∈ {0, 1}D×L.",1. Introduction,[0],[0]
"The data generating process is based on the Boolean product, a special case of matrix product between binary matrices where all values
1Department of Statistics, University of Oxford, UK 2Nuffield Department of Medicine, University of Oxford, UK 3Department of Informatics, Athens University of Economics and Business, Greece 4Centre for Computational Biology, Institute of Cancer and Genomic Sciences, University of Birmingham, UK.",1. Introduction,[0],[0]
"Correspondence to: Tammo Rukat <tammo.rukat@stats.ox.ac.uk>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
n=0 n=1 n=2,1. Introduction,[0],[0]
"n=3 n=4 n=5 n=6 n=7 n=8 n=9
N=10 observation of size D=17x10
' l=0",1. Introduction,[0],[0]
l=1,1. Introduction,[0],[0]
"l=2
l=3 l=4",1. Introduction,[0],[0]
"l=5
Codes u
⊗
0 1 2 3 4 5 6 7 8 9 n
5
4
3
2
1
0
l
0.0
0.2
0.4
0.6
0.8
1.0 Latent representations z
Figure 1.",1. Introduction,[0],[0]
The observed images are 10 digits from 0 to 9 as they are traditionally represented in calculators.,1. Introduction,[0],[0]
"The data is factorised into matrices of rank 6, which is not sufficient for full error-free reconstruction.",1. Introduction,[0],[0]
"Every digit, except 7, can be constructed by Boolean combination of the inferred codes.",1. Introduction,[0],[0]
The OrMachine infers a posterior mean probability of 50% for using code l = 5 in constructing a 7.,1. Introduction,[0],[0]
Note that there exist other equally valid solutions to this problem with 6 latent dimensions.,1. Introduction,[0],[0]
The pixels represent posterior means.,1. Introduction,[0],[0]
"Codes and observations are arranged to 10×17 images for interpretation.
larger than zero are set to one, i.e.
xnd",1. Introduction,[0],[0]
= L∨ l=1 znl ∧ uld .,1. Introduction,[0],[0]
"(1)
Here, ∨ and∧ encode the Boolean disjunction and conjunction, respectively.",1. Introduction,[0],[0]
BooMF provides a framework for learning from binary data where the inferred codes U provide a basis and the indicator variables Z encode the presence or absence of these codes.,1. Introduction,[0],[0]
This representation is illustrated in the calculator digits example in Fig. 1.,1. Introduction,[0],[0]
"We can think of BooMF as binary factor analysis or as clustering with joint assignments, where each observation is assigned to a subset of L cluster centroids or codes.",1. Introduction,[0],[0]
The L-dimensional indicators provide a compact representation of which codes are allocated to each observation.,1. Introduction,[0],[0]
As stated in Eq.,1. Introduction,[0],[0]
"(1), a feature xnd takes a value of one if it equals one in any of the assigned codes.
",1. Introduction,[0],[0]
"BooMF has many real-world applications ranging from topic modelling (Blei, 2012) to collaborating filtering (Su
& Khoshgoftaar, 2009) and computer vision (LázaroGredilla et al., 2016).",1. Introduction,[0],[0]
"In this paper, we introduce the OrMachine, a Bayesian approach to BooMF, and fit the model using a fast and scalable Metropolised Gibbs sampling algorithm.",1. Introduction,[0],[0]
"On simulated and real-world data, our method is shown to significantly outperform the current state-of-theart message passing approaches for learning BooMF models.",1. Introduction,[0],[0]
"Moreover, we consider a challenging application in the analysis of high-throughput single cell genomics data.",1. Introduction,[0],[0]
BooMF is used to identify latent gene signatures (codes) that correspond to key cellular pathways or biological processes from large gene expression datasets consisting of 1.3 million cells across 11 thousand genes.,1. Introduction,[0],[0]
"Genes are expressed if one or more relevant biological processes are active, a property which is naturally modelled by the Boolean OR operation.",1. Introduction,[0],[0]
We also introduce a multi-layered extensions of Bayesian BooMF that can capture hierarchical dependencies in the latent representations.,1. Introduction,[0],[0]
There has been a sustained interest in BooMF and related methods of which we will give a brief review.,2. Related Work,[0],[0]
"The Discrete Basis Problem (Miettinen et al., 2006) provides a greedy heuristic algorithm to solve BooMF without recourse to an underlying probabilistic model.",2. Related Work,[0],[0]
"It is based on association rule mining (Agrawal et al., 1994) and has more recently been extended to automatically select the optimal dimensionality of the latent space based on the minimum description length principle (Miettinen & Vreeken, 2014).",2. Related Work,[0],[0]
"In contrast, multi assignment clustering for Boolean data (Streich et al., 2009) leverages on a probabilistic model for BooMF, adding a further global noise source to the generative process.",2. Related Work,[0],[0]
Point estimates are inferred by deterministic annealing.,2. Related Work,[0],[0]
"Similarly, Wood et al. (2012) develop a probabilistic model to infer hidden causes.",2. Related Work,[0],[0]
"In contrast to the Boolean OR, the likelihood of an observation increases with the number of active hidden codes.",2. Related Work,[0],[0]
They use an Indian Buffet process prior over the latent space and a Gibbs sampler to infer the distribution over the unbounded number of hidden causes.,2. Related Work,[0],[0]
"A more expressive model for matrix factorisation with binary latent variables is introcued by (Meeds et al., 2007), who combine binary interactions with continous weights.",2. Related Work,[0],[0]
A similar approach to ours is the work by Ravanbakhsh et al. (2016).,2. Related Work,[0],[0]
The authors tackle BooMF using a probabilistic graphical model and derive a message passing algorithm to perform MAP inference.,2. Related Work,[0],[0]
Their method is shown to have state-of-the-art performance for BooMF and completion.,2. Related Work,[0],[0]
It therefore serves us as baseline benchmark in these tasks.,2. Related Work,[0],[0]
The message passing approach has recently been employed by Lázaro-Gredilla et al. (2016) in a hierarchical network combined with pooling layers to infer the building blocks of binary images.,2. Related Work,[0],[0]
The OrMachine is a probabilistic generative model for Boolean matrix factorisation.,3.1. Model Formulation,[0],[0]
"A matrix of N binary observations xn ∈ {0, 1}D is generated from a discrete mixture of L binary codes ul ∈ {0, 1}D. Binary latent variables znl denote whether or not code l is used in generating a particular observation xn.",3.1. Model Formulation,[0],[0]
"The probability for a data point xnd to be one is greater than 1/2 if the corresponding codes and latent variables in at least one latent dimension both equal one; conversely, if there exists no dimension where codes and latent variables both equal one, the probability for the data point to be one is less than 1/2.",3.1. Model Formulation,[0],[0]
"The exact magnitude of this probability is inferred from the data and, for later notational convenience, is parametrised as the logistic sigmoid of a global dispersion parameter σ(λ) =",3.1. Model Formulation,[0],[0]
"(1+e−λ)−1, with λ ∈ R+.",3.1. Model Formulation,[0],[0]
"Next, we give a full description of the likelihood and prior distributions used in the OrM.
The likelihood function is factorised across the N observations and D features with each factor given by
p(xnd|u, z, λ) = { σ(λ); if x=min(1,uTd zn) 1−σ(λ); if x 6=min(1,uTd zn) (2)
= σ",3.1. Model Formulation,[0],[0]
"[ λx̃nd ( 1− 2
∏ l (1− znluld)
)] .
(3)
",3.1. Model Formulation,[0],[0]
"Tilde denotes the {0, 1} → {−1, 1} mapping so that for any binary variable x ∈ {0, 1}, x̃ = 2x − 1.",3.1. Model Formulation,[0],[0]
The expression inside the parentheses of Eq.,3.1. Model Formulation,[0],[0]
"(3) encodes the OR operation and evaluates to 1 if znl = uld = 1 for at least one l, and to −1 otherwise.",3.1. Model Formulation,[0],[0]
"The dispersion parameter controls the noise in the generative process, i.e. as λ→∞, all probabilities tend to 0 or 1 and the model describes a deterministic Boolean matrix product.",3.1. Model Formulation,[0],[0]
Note that the likelihood can be computed efficiently from Eq. (3) as we describe in detail in the next section.,3.1. Model Formulation,[0],[0]
We further assume independent Bernoulli priors for all variables uld and znl.,3.1. Model Formulation,[0],[0]
Such priors allow us to promote denseness or sparsity in codes and latent variables.,3.1. Model Formulation,[0],[0]
Notice that the designation of U as codes and Z as latent variables is not necessary since these matrices appear in a symmetric manner.,3.1. Model Formulation,[0],[0]
"If we transpose the matrix of observations X , then codes and latent variables merely swap roles.
",3.1. Model Formulation,[0],[0]
"Finally, we do not place a prior on the dispersion parameter λ, but maximise it using an EM-type algorithm described below.",3.1. Model Formulation,[0],[0]
"The full joint distribution of all data and random variables is given by p(X,U ,Z|λ) = p(X|U ,Z, λ)p(U)p(Z).
",3.2. Fast Posterior Inference,[0],[0]
"The full conditional for znl (and analogous for uld) is
p(znl| ·)",3.2. Fast Posterior Inference,[0],[0]
"=
σ [ λz̃nl ∑ d x̃nduld ∏ l′ 6=l (1−znl′ul′d)+logit(p(znl)) ] .",3.2. Fast Posterior Inference,[0],[0]
"(4)
We give a detailed derivation in the supplement.",3.2. Fast Posterior Inference,[0],[0]
"Notice that the independent Bernoulli prior enters the expression as additive term inside the sigmoid function that vanishes for the uninformative Bernoulli prior p(z) = 1/2.
",3.2. Fast Posterior Inference,[0],[0]
The form of Eq.,3.2. Fast Posterior Inference,[0],[0]
(4) allows for computationally efficient evaluation of the conditionals.,3.2. Fast Posterior Inference,[0],[0]
"The underlying principle is that once certain conditions are met, the result of the full conditional is known without considering the remainder of a variable’s Markov blanket.",3.2. Fast Posterior Inference,[0],[0]
"For instance, when computing updates for znl, terms in the sum over d necessarily evaluate to zero if one of the following conditions is met: (i) uld = 0 or (ii) znl′ul′d",3.2. Fast Posterior Inference,[0],[0]
= 1 for some l′ 6=,3.2. Fast Posterior Inference,[0],[0]
"l. This leads to Algorithm 1 for fast evaluation of the conditionals.
",3.2. Fast Posterior Inference,[0],[0]
"Algorithm 1 Computation of the full conditional of znl accumulator = 0 for d in 1, . . .",3.2. Fast Posterior Inference,[0],[0]
", D do
if uld = 0 then continue (next iteration over d) end if for l′ in 1, . . .",3.2. Fast Posterior Inference,[0],[0]
", L do
if l′",3.2. Fast Posterior Inference,[0],[0]
6=,3.2. Fast Posterior Inference,[0],[0]
"l and znl′ = 1 and ul′d = 1 then continue (next iteration over d)
end if end for accumulator = accumulator +",3.2. Fast Posterior Inference,[0],[0]
"x̃nd
end for p(znl| ·)",3.2. Fast Posterior Inference,[0],[0]
= σ,3.2. Fast Posterior Inference,[0],[0]
"(λ · z̃nl · accumulator)
To infer the posterior distribution over all variables uld and znl we could iteratively sample from the above conditionals using standard Gibbs sampling.",3.2. Fast Posterior Inference,[0],[0]
In practice we use a modification of this procedure which is referred to as Metropolised Gibbs sampler and was proposed by Liu (1996).,3.2. Fast Posterior Inference,[0],[0]
"We always propose to flip the current state, leading to a Hastings acceptance probability of p(z| ·)/(1−p(z|",3.2. Fast Posterior Inference,[0],[0]
·)),3.2. Fast Posterior Inference,[0],[0]
.,3.2. Fast Posterior Inference,[0],[0]
"This is guaranteed to yield lower variance Monte Carlo estimates (Peskun, 1973).
",3.2. Fast Posterior Inference,[0],[0]
"After every sweep through all variables, the dispersion parameter λ is updated to maximise the likelihood akin to the M-step of a Monte Carlo EM algorithm.",3.2. Fast Posterior Inference,[0],[0]
"Specifically, given the current values of the codes U and latent variables Z we can compute how many observations xnd are correctly predicted by the model, as
P = ∑ n,d",3.2. Fast Posterior Inference,[0],[0]
I,3.2. Fast Posterior Inference,[0],[0]
[ xnd = 1− ∏,3.2. Fast Posterior Inference,[0],[0]
l (1− znluld) ] .,3.2. Fast Posterior Inference,[0],[0]
"This allows us to
Algorithm 2 Sampling from the OrMachine for i in 1, . . .",3.2. Fast Posterior Inference,[0],[0]
",max-iters do
for n in 1, . . .",3.2. Fast Posterior Inference,[0],[0]
",N (in parallel) do for l in 1, . . .",3.2. Fast Posterior Inference,[0],[0]
",L do
Compute p(znl| ·) following Algorithm 1 Flip znl with probability [p(znl| ·)−1−1]−1
end for end for for d in 1, . . .",3.2. Fast Posterior Inference,[0],[0]
", d (in parallel) do
for l in 1, . . .",3.2. Fast Posterior Inference,[0],[0]
",L do Compute p(uld| ·) following Algorithm 1 Flip uld with probability [p(uld| ·)−1−1]−1
end for end for Set λ to its MLE according to Eq.",3.2. Fast Posterior Inference,[0],[0]
"(5).
end for
rewrite the likelihood as σ(λ)Pσ(−λ)ND−P which can be subsequently maximised with respect to λ to yield the update
σ(λ̂) = P
ND .",3.2. Fast Posterior Inference,[0],[0]
"(5)
The alternation between sampling (U ,Z) and updating the dispersion parameter is carried out until convergence; see Algorithm 2 for all steps of this procedure.",3.2. Fast Posterior Inference,[0],[0]
"We can handle unobserved data, by marginalising the likelihood over the missing observations.",3.3. Dealing with Missing Data,[0],[0]
"More precisely, if X = (Xobs,Xmis) is the decomposition of the full matrix into the observed part Xobs and the missing part Xmis, after marginalisation, the initial likelihood p(X|U ,Z, λ) simplifies to p(Xobs|U ,Z, λ).",3.3. Dealing with Missing Data,[0],[0]
"Then, a naı̈ve implementation could be based on indexing the observed components inside matrix X and modifying the inference procedure so that the posterior conditionals of znl and uld involve only sums over observed elements.",3.3. Dealing with Missing Data,[0],[0]
"A simpler, equivalent implementation, which we follow in our experiments, is to represent the data as x̃nd ∈ {−1, 0, 1} where missing observations are encoded as zeros, each contributing the constant factor σ(0) = 1/2 to the full likelihood, so that p(X|U ,Z, λ) = C p(Xobs|U ,Z, λ), where C is a constant.",3.3. Dealing with Missing Data,[0],[0]
"Thus, the missing values do not contribute to the posterior over U and Z which is also clear from the form of the full conditionals in Eq.",3.3. Dealing with Missing Data,[0],[0]
(4) that depend on a sum weighted by xnds.,3.3. Dealing with Missing Data,[0],[0]
"For the update of the dispersion parameter in Eq. (5)), we need to subtract the number of all missing observations in the denominator.",3.3. Dealing with Missing Data,[0],[0]
The dispersion now indicates the fraction of correct prediction in the observed data.,3.3. Dealing with Missing Data,[0],[0]
"Following this inference procedure, we can impute missing data based on a Monte Carlo estimate of
the predictive distribution of some unobserved xnd as
1
S S∑ s=1",3.3. Dealing with Missing Data,[0],[0]
"p(xnd|U (s),Z(s), λ̂) , (6)
where each (U (s),Z(s)) is a posterior sample.",3.3. Dealing with Missing Data,[0],[0]
"A much faster approximation of the predictive distribution is obtained by p(xnd|Û , Ẑ, λ̂), where we simply plug the posterior mean estimates for (U ,Z) into the predictive distribution.",3.3. Dealing with Missing Data,[0],[0]
"For the simulated data in Section 4.2, we find both methods to perform equally well and therefore follow the second, faster approach for all remaining experiments.",3.3. Dealing with Missing Data,[0],[0]
BooMF learns patterns of correlation in the data.,3.4. Multi-Layer OrMachine,[0],[0]
"In analogy to multi-layer neural networks, we can build a hierarchy of correlations by applying another layer of factorisation to the factor matrix Z. This is reminiscent of the idea of deep exponential families, as introduced by Ranganath et al. (2015).",3.4. Multi-Layer OrMachine,[0],[0]
"The ability to learn features at different levels of abstraction is commonly cited as an explanation for the success that deep neural networks have across many domains of application (Lin & Tegmark, 2016; Bengio et al., 2013).",3.4. Multi-Layer OrMachine,[0],[0]
"In the present setting, with stochasticity at every step of the generative process and posterior inference, we are able to infer meaningful and interpretable hierarchies of abstraction.
",3.4. Multi-Layer OrMachine,[0],[0]
"To give an example, we determine the optimal multi-layer architecture for representing the calculator digit toy dataset as introduced in Fig. 1.",3.4. Multi-Layer OrMachine,[0],[0]
We observe 50 digits and consider 70% of the data points randomly as unobserved.,3.4. Multi-Layer OrMachine,[0],[0]
"We then
train multi-layer OrMachines with various depths and layer widths, iterating through the individual layers during 200 iterations of burn-in.",3.4. Multi-Layer OrMachine,[0],[0]
We then draw 200 samples from each consecutive layer with the remaining layers held fixed to their MAP estimate.,3.4. Multi-Layer OrMachine,[0],[0]
"In order to enforce distributed representations, we choose independent Bernoulli sparsity priors for the codes: p(uld) =",3.4. Multi-Layer OrMachine,[0],[0]
"[0.01, 0.05, 0.2] for each layer, respectively.",3.4. Multi-Layer OrMachine,[0],[0]
"Superior performance in reconstructing the unobserved data is achieved by a 3-hidden layer architecture with hidden layers of size L1 = 7, L2 = 4, L3 = 2.",3.4. Multi-Layer OrMachine,[0],[0]
This 3-layer model reduces the reconstruction error from 1.4% to 0.4% compared to the single-layer model with width L = 7.,3.4. Multi-Layer OrMachine,[0],[0]
Maximum likelihood estimates of the dispersion for the three layers are λ̂ =,3.4. Multi-Layer OrMachine,[0],[0]
"[1.0, 0.93, 0.8].",3.4. Multi-Layer OrMachine,[0],[0]
"The first layer infers the seven bars that compose all digits, the following layer infers dominant groupings of these bars, and so on.",3.4. Multi-Layer OrMachine,[0],[0]
"In Fig. 2, we plot the probabilities that each prototype induces in the observation layer.",3.4. Multi-Layer OrMachine,[0],[0]
"They are given by the means of the posterior predictive as described in the the previous section, conditioned on the one-hot activations of zn with znl ∈ {0, 1} and ∑ l znl = 1.",3.4. Multi-Layer OrMachine,[0],[0]
"Alongside, we depict the average posterior mean of the corresponding representations for each digit in the training data.",3.4. Multi-Layer OrMachine,[0],[0]
This example illustrates that the multi-layer OrMachine infers interpretable higherorder correlations and is able to exploit them to achieve significant improvements in missing data imputation.,3.4. Multi-Layer OrMachine,[0],[0]
The algorithm is implemented in Python with the core sampling routines in compiled Cython.,3.5. Practical Implementation and Speed,[0],[0]
Code is avaialble on GitHub1.,3.5. Practical Implementation and Speed,[0],[0]
"Binary data is represented as {−1, 1} with missing data encoded as 0.",3.5. Practical Implementation and Speed,[0],[0]
This economical representation of data and variables as integer types simplifies computations considerably.,3.5. Practical Implementation and Speed,[0],[0]
"Algorithm 1 is implemented in parallel across the observations [n] = {1, . . .",3.5. Practical Implementation and Speed,[0],[0]
", N} and conversely updates for uld are implemented in parallel across all features [d] = {1, . . .",3.5. Practical Implementation and Speed,[0],[0]
", D}.",3.5. Practical Implementation and Speed,[0],[0]
The computation time scales linearly in each dimension.,3.5. Practical Implementation and Speed,[0],[0]
A single sweep through high-resolution calculator digits toy dataset with ND = 1.7 × 106 data points and L = 7 latent dimensions takes approximately 1 second on a desktop computer.,3.5. Practical Implementation and Speed,[0],[0]
A single sweep through the approximately 1.4× 1010 data points presented in the biological example in Section 5.2 with L = 2 latent dimensions takes approximately 5 minutes executed on 24 computing cores.,3.5. Practical Implementation and Speed,[0],[0]
For all examples presented here 100 iterations suffice for the algorithm to converge to a (local) posterior mode.,3.5. Practical Implementation and Speed,[0],[0]
"In this section, we probe the performance of the OrMachine (OrM) at random matrix factorisation and completion
1https://github.com/TammoR/OrMachine/
tasks.",4. Experiments on Simulated Data,[0],[0]
"Message passing (MP) has been shown to compare favourably with other state-of-the-art methods for BooMF that we introduced in Section 2 (Ravanbakhsh et al., 2016).",4. Experiments on Simulated Data,[0],[0]
It therefore is the focus of our comparison.,4. Experiments on Simulated Data,[0],[0]
"The following settings for MP and the OrM are used throughout our experiments, unless mentioned otherwise.",4. Experiments on Simulated Data,[0],[0]
"For MP, we use the Python implementation provided by the authors.",4. Experiments on Simulated Data,[0],[0]
"We also proceed with their choice of hyper-parameters, as experimentation with different learning rates and maximum number of iterations did not lead to any improvements.",4. Experiments on Simulated Data,[0],[0]
"For both methods, we set the priors p(u) and p(z) to the factor matrices’ expected value based on the density of the product matrix in an Empirical-Bayes fashion.",4. Experiments on Simulated Data,[0],[0]
"The only exception is MP in the matrix completion task, where uniform priors, as used by Ravanbakhsh et al. (2016), lead to slightly better performance.",4. Experiments on Simulated Data,[0],[0]
"For the OrM, we initialise the parameters uniformly at random and draw 100 iterations after 100 samples of burn-in.",4. Experiments on Simulated Data,[0],[0]
Note that around 10–100 sampling steps are usually sufficient for convergence.,4. Experiments on Simulated Data,[0],[0]
"We generate a quadratic matrix X ∈ {0, 1}N×N of rank L by taking the Boolean product of two random N × L factor matrices.",4.1. Random Matrix Factorisation,[0],[0]
The Boolean product X of two rank L binary matrices that are sampled i.i.d. from a Bernoulli distribution with parameter p has an expected value of E(X),4.1. Random Matrix Factorisation,[0],[0]
"= 1− (1− p2)L. Since we generally prefer X to be neither sparse nor dense, we fix its expected density to 1/2, unless stated otherwise.",4.1. Random Matrix Factorisation,[0],[0]
This ensures that a simple bias toward zeroes or ones in either method is not met with reward.,4.1. Random Matrix Factorisation,[0],[0]
Bits in the data are flipped at random with probabilities ranging from 5% to 50%.,4.1. Random Matrix Factorisation,[0],[0]
Factor matrices of the correct underlying dimension are inferred and the data is reconstructed from the inferred factorisation.,4.1. Random Matrix Factorisation,[0],[0]
"An example of the task is shown in Fig. 3.
",4.1. Random Matrix Factorisation,[0],[0]
"Results for the reconstruction error, defined as the fraction of misclassified data points, are depicted in Fig. 4.",4.1. Random Matrix Factorisation,[0],[0]
All experiments were repeated 10 times with error bars denoting standard deviations.,4.1. Random Matrix Factorisation,[0],[0]
"The OrM outperforms MP under all conditions, except when both methods infer equally errorfree reconstructions.",4.1. Random Matrix Factorisation,[0],[0]
"Fig. 4 (top) reproduces the experi-
mental settings of Fig. 2 in Ravanbakhsh et al. (2016).",4.1. Random Matrix Factorisation,[0],[0]
We find that the OrMachine enables virtually perfect reconstruction of a 1000 × 1000 matrix of rank L = 5 for up to 35% bit flip probability.,4.1. Random Matrix Factorisation,[0],[0]
"Notably, MP performs worse for smaller noise levels.",4.1. Random Matrix Factorisation,[0],[0]
It was hypothesised by Ravanbakhsh et al. (2016) that symmetry breaking at higher noise levels helps message passage to converge to a better solution.,4.1. Random Matrix Factorisation,[0],[0]
Fig. 4 (middle) demonstrates the consistently improved performance of the OrMachine for a more challenging example of 100 × 100 matrices of rank 7.,4.1. Random Matrix Factorisation,[0],[0]
"The reconstruction performance of both methods is similar for lower noise levels, while the OrMachine consistently out-
performs MP for larger noise levels.",4.1. Random Matrix Factorisation,[0],[0]
"For biased data with E[xnd] = 0.7 in Fig. 4 (bottom), we observe a similar pattern with a larger performance gap for higher noise levels.",4.1. Random Matrix Factorisation,[0],[0]
"Even for a bit flip-probability of 50% the OrMachine retains a reconstruction error of approximately 30%, which is achieved by levering the bias in the data.
",4.1. Random Matrix Factorisation,[0],[0]
"Fig. 4 (middle) also shows the reconstruction error on the observed data, indicating that MP overfits the data more than the OrM for larger noise levels.",4.1. Random Matrix Factorisation,[0],[0]
This may contribute to the improved performance of the OrMachine.,4.1. Random Matrix Factorisation,[0],[0]
"We further investigate the problem of matrix completion or collaborative filtering, where bits of the data matrix are unobserved and reconstructed from the inferred factor matrices.",4.2. Random Matrix Completion,[0],[0]
"Following the procedure outlined in Section 4.1, we generate random matrices of rank 5 and size 250 × 250.",4.2. Random Matrix Completion,[0],[0]
"We only observe a random subset of the data, ranging from 0.5% and 3.5%.",4.2. Random Matrix Completion,[0],[0]
The missing data is reconstructed from the inferred factor matrices.,4.2. Random Matrix Completion,[0],[0]
"As shown in Fig. 5, the OrMachine outperforms message passing throughout.",4.2. Random Matrix Completion,[0],[0]
"The plot indicates means and standard deviations from 10 repetitions of each experiment.
",4.2. Random Matrix Completion,[0],[0]
"Notably, the OrMachine does not only provide a MAP estimate, but also an estimate of the posterior probability for each unobserved data point xnd.",4.2. Random Matrix Completion,[0],[0]
Fig. 5 (bottom) shows an estimate of the density of the posterior means for the correctly and incorrectly completed data points.,4.2. Random Matrix Completion,[0],[0]
"The distribution of incorrect predictions peaks around a probability of 1/2, indicating that the OrMachine’s uncertainty about
its reconstruction provides further useful information about the missing data.",4.2. Random Matrix Completion,[0],[0]
"For instance, this information can be used to control for false positives or false negatives, simply by setting a threshold for the posterior mean.",4.2. Random Matrix Completion,[0],[0]
We investigate the OrMachine’s performance for collaborative filtering on a real-world dataset.,5.1. MovieLens Matrix Completion,[0],[0]
"The MovieLens-1M dataset2 contains 106 integer film ratings from 1 to 5 from 6000 users for 4000 films, i.e. 1/24 of the possible ratings are available.",5.1. MovieLens Matrix Completion,[0],[0]
"Similarly, the MovieLens 100k dataset contains 943 users and 1682 films.",5.1. MovieLens Matrix Completion,[0],[0]
"Following Ravanbakhsh et al. (2016), we binarise the data taking the global mean as threshold.",5.1. MovieLens Matrix Completion,[0],[0]
"We observe only a fraction of the available data, varying from 1% to 95%, and reconstruct the remaining available data following the procedure in Section 4.2 with L = 2 latent dimensions.",5.1. MovieLens Matrix Completion,[0],[0]
Reconstruction accuracies are given as fractions of correctly reconstructed unobserved ratings in Table 1.,5.1. MovieLens Matrix Completion,[0],[0]
The given values are means from 10 randomly initialised runs of each algorithm.,5.1. MovieLens Matrix Completion,[0],[0]
The corresponding standard deviations are always smaller than 0.2%.,5.1. MovieLens Matrix Completion,[0],[0]
"The OrMachine is more accurate than message passing in all cases, except for the 1M dataset with 95% available ratings.",5.1. MovieLens Matrix Completion,[0],[0]
The OrMachine’s advantage is particularly significant if only little data is observed.,5.1. MovieLens Matrix Completion,[0],[0]
"Increasing the latent dimension L to values of 3 or 4 yields no consistent improvement, while a further increase is met with diminishing
2The MovieLens dataset is available online: https:// grouplens.org/datasets/movielens/.
returns.",5.1. MovieLens Matrix Completion,[0],[0]
We achieve the best within-sample performance for a two-layer OrMachine with different architectures performing best for different amounts of observed data.,5.1. MovieLens Matrix Completion,[0],[0]
An OrMachine with two hidden layers of sizes 4 and 2 respectively yields the best average performance.,5.1. MovieLens Matrix Completion,[0],[0]
"As indicated in Table 1, it provides better results throughout but exceeds the performance of the shallow OrMachine rarely by more than 1%.",5.1. MovieLens Matrix Completion,[0],[0]
"This indicates that there is not much higher order structure in the data, which is unsurprising given the sparsity of the observations and the low dimensionality of the first hidden layer.
",5.1. MovieLens Matrix Completion,[0],[0]
We illustrate a further advantage of full posterior inference for collaborative filtering.,5.1. MovieLens Matrix Completion,[0],[0]
We can choose a threshold for how likely we want a certain prediction to take a certain value and trade off false with true positives.,5.1. MovieLens Matrix Completion,[0],[0]
"A corresponding ROC curve for the MovieLens 100k dataset, where 10% of the available data was observed, is shown in Fig. 6.",5.1. MovieLens Matrix Completion,[0],[0]
"Single-cell RNA expression analysis is a revolutionary experimental technique that facilitates the measurement of gene expression on the level of a single cell (Blainey & Quake, 2014).",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"In recent years this has led to the discovery of new cell types and to a better understanding of tissue heterogeneity (Trapnell, 2015).",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"The latter is particularly relevant in cancer research where it helps to understand the cellular composition of a tumour and its relationship to disease progression and treatment (Patel et al., 2014).",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
Here we apply the OrMachine to binarised gene expression profiles of about 1.3 million cells for about 28 thousand genes per cell.,5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"Cell specimens were obtained from cortex, hip-
pocampus and subventricular zone of E18 (embryonic day 18) mice; the data is publicly available3.",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
Only 7% of the data points are non-zero.,5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"We set all non-zero expression levels to one, retaining the essential information of whether or not a particular gene is expressed.",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
We remove genes that are expressed in fewer than 1% of cells with roughly 11 thousand genes remaining.,5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
This leaves us with approximately 1.4×1010 data points.,5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"We apply the OrMachine for latent dimensions L = 2, . . .",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
", 10.",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"The algorithm converges to a posterior mode after 10–20 iteration, taking roughly an hour on a 4-core desktop computer and 10–30 minutes on a cluster with 24 cores.",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"We draw 125 samples and discard the first 25 as burn-in.
",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"Factorisations with different latent dimensionality form hierarchies of representations, where features that appear together in codes for lower dimensions are progressively split apart when moving to a higher dimensional latent space.",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
We illustrate this approach to analysing the inferred factorisations on calculator digits in Fig. 7.,5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
Each row corresponds to an independently trained OrMachine with the dimensionality L increasing from 3 to 7.,5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
We observe denser patterns dividing up consecutively until only the seven constituent bars remain.,5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"This is a form of hierarchical clustering that, in contrast to traditional methods, does not impose any hierarchical structure on the model.",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"We perform the same analysis on the single cell gene expression data with the results for both, gene patterns and specimen patterns shown in Fig. 8.",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
This Figure should be interpreted in analogy to Fig. 7.,5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"Furthermore, we run a gene set enrichment analysis for the genes that are unique to each inferred code, looking for associated biological states.",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"This
3https://support.10xgenomics.com
is done using the Enrichr analysis tool (Chen et al., 2013) and a mouse gene atlas (Su et al., 2004).",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"Biological states are denoted above each code, together with the logarithm to base 10 of their adjusted p-value.",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"Increasing the latent dimensionality leads to a more distributed representation with subtler, biologically plausible patterns.",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
The columns in Fig. 8 are ordered to emphasise the hierarchical structure.,5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"E.g., in the first column for L = 5 and second column for L = 6, a gene set with significant overlap to two biological processes (olfactory bulb and hippocampus) splits into two gene sets each corresponding to one of the two processes.",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"In the assignment of cells to these sets (Fig. 8B), this is associated with an increase in posterior uncertainty as to which cell expresses this property.",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
The significance levels of the associated biological processes drop from p-values on the order of 10−3 to p-values on the order of 1.,5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"In addition, typical genes for each of the biological states are annotated (Lopez-Bendito et al., 2007; Zheng et al., 2008; Demyanenko et al., 2010; Upadhya et al., 2011; Raman et al., 2013).",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
This examples illustrates the OrMachine’s ability to scale posterior inference to massive datasets.,5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"It enables the
discovery of readily interpretable patterns, representations and hierarchies, all of which are biologically plausible.",5.2. Explorative Analysis of Single Cell Gene Expression Profiles,[0],[0]
"We have developed the OrMachine, a probabilistic model for Boolean matrix factorisation.",6. Conclusion,[0],[0]
The extremely efficient Metropolised Gibbs sampler outperforms state-of-the-art methods in matrix factorisation and completion.,6. Conclusion,[0],[0]
"It is the first method that infers posterior distributions for Boolean matrix factorisation, a property which is highly relevant in practical applications where full uncertainty quantification matters.",6. Conclusion,[0],[0]
"Despite full posterior inference, the proposed method scales to very large datasets.",6. Conclusion,[0],[0]
We have shown that tens of billions of data points can be handled on commodity hardware.,6. Conclusion,[0],[0]
The OrMachine can readily accommodate missing data and prior knowledge.,6. Conclusion,[0],[0]
"Layers of OrMachines can be stacked, akin to deep belief networks, inferring representations at different levels of abstraction.",6. Conclusion,[0],[0]
This leads to improved reconstruction performance on simulated and real world data.,6. Conclusion,[0],[0]
"Boolean matrix factorisation aims to decompose a binary data matrix into an approximate Boolean product of two low rank, binary matrices: one containing meaningful patterns, the other quantifying how the observations can be expressed as a combination of these patterns.",abstractText,[0],[0]
"We introduce the OrMachine, a probabilistic generative model for Boolean matrix factorisation and derive a Metropolised Gibbs sampler that facilitates efficient parallel posterior inference.",abstractText,[0],[0]
"On real world and simulated data, our method outperforms all currently existing approaches for Boolean matrix factorisation and completion.",abstractText,[0],[0]
"This is the first method to provide full posterior inference for Boolean Matrix factorisation which is relevant in applications, e.g. for controlling false positive rates in collaborative filtering and, crucially, improves the interpretability of the inferred patterns.",abstractText,[0],[0]
The proposed algorithm scales to large datasets as we demonstrate by analysing single cell gene expression data in 1.3 million mouse brain cells across 11 thousand genes on commodity hardware.,abstractText,[0],[0]
Bayesian Boolean Matrix Factorisation,title,[0],[0]
"In statistical applications, random graphs serve as Bayesian models for network data, that is, data consisting of objects and the observed linkages between them.",1. Introduction,[0],[0]
"Here we will focus on models for random simple graphs (that is, graphs with edges that take binary values), which are appropriate for applications where we observe either the presence or
1Pohang University of Science and Technology, Pohang, South Korea 2University of Cambridge, Cambridge, UK 3Uber AI Labs, San Francisco, CA, USA 4Hong Kong University of Science and Technology, Hong Kong.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Juho Lee <stonecold@postech.ac.kr>, Seungjin Choi <seungjin@postech.ac.kr>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
absence of links between objects in the network.",1. Introduction,[0],[0]
"For example, in social networks, nodes may represent individuals and a link (i.e., a nonzero value of an edge) could represent friendship.",1. Introduction,[0],[0]
"In a protein-protein interaction network, nodes may represent proteins and links could represent an observed physical or chemical interaction between proteins.",1. Introduction,[0],[0]
"Many domains involving network data (including social and protein-protein interaction networks) have been shown to exhibit power law, i.e., heavy-tailed, degree distributions (Barabási & Albert, 1999).",1. Introduction,[0],[0]
"Models for random graphs with power law degree distributions, also called scale-free random graphs, have therefore become one of the most actively studied areas of graph theory and network science (Bollobás et al., 2001; Albert & Barabási, 2002; Dorogovtsev & Mendes, 2002).",1. Introduction,[0],[0]
"In this paper we present a model for simple, scale-free random graphs, which we apply as a probabilistic model for several network datasets.
",1. Introduction,[0],[0]
"The model we present in this paper is a special case of the generalized random graph defined by Britton et al. (2006), and studied further by van der Hofstad (2016, Ch. 6), which outlines a framework for defining scale-free random graphs, but does not provide practical constructions, much less algorithms for performing statistical inference on the model components given data.",1. Introduction,[0],[0]
"Here we provide one such practical construction, along with a variational inference routine (Jordan et al., 1999) for efficient posterior inference.",1. Introduction,[0],[0]
"What’s more, our construction readily generalizes to include the structure of latent factors/clusters, as captured by the popular stochastic blockmodels (Nowicki & Snijders, 2001; Airoldi et al., 2009), while maintaining power law behavior in the graph.
",1. Introduction,[0],[0]
"Applying Bayesian inference algorithms on network datasets is a challenge because likelihood computations, in general, scale with the number of edges in the graph, which is O(n2) in a network with n nodes.",1. Introduction,[0],[0]
"To help overcome these difficulties, we follow Hoffman et al. (2013) and develop a stochastic variational inference algorithm in which we approximate many likelihood computations on only subsets of the data, called minibatches.",1. Introduction,[0],[0]
"In the case of a network dataset, the minibatches are comprised of subsets of edges in the graph.
",1. Introduction,[0],[0]
"We apply this inference procedure to several network
datasets that are commonly observed to possess power law structure.",1. Introduction,[0],[0]
Our experiments show that accurately capturing this power law structure improves performance on tasks predicting missing edges in the networks.,1. Introduction,[0],[0]
"We represent a simple graph with n nodes by an adjacency matrix X := (Xi,j)i,j≤n, where Xi,j = 1 if there is a link between nodes i and j and Xi,j = 0 otherwise.",2. Bayesian models for simple graphs,[0],[0]
"Here we will only consider undirected graphs, in which case X represents a symmetric matrix.",2. Bayesian models for simple graphs,[0],[0]
"Furthermore, we do not allow self links, so the diagonal entries in X are meaningless.",2. Bayesian models for simple graphs,[0],[0]
"Most probabilistic models for simple graphs take the entries in X to be conditionally independent Bernoulli random variables; in particular, for every i, j ≤ n, let pi,j be the (random) probability of a link between nodes i and j, and let Xi,j | pi,j ∼ Bernoulli(pi,j).",2. Bayesian models for simple graphs,[0],[0]
"For every simple graph x := (xi,j)i,j≤n, we may then write the likelihood for the parameters p := (pi,j)i,j≥1 given X as
P (X = x | p) =",2. Bayesian models for simple graphs,[0],[0]
"∏
i<j≤n
p xi,j i,j (1− pi,j) 1−xi,j , (1)
where in our case it should be clear that the product is only over i, j ≤ n",2. Bayesian models for simple graphs,[0],[0]
such that i < j and i 6=,2. Bayesian models for simple graphs,[0],[0]
"j. Random simple graphs date back to the Erdös–Rényi model, which may be reviewed, along with the more general theory of random graphs, in the text by Bollobás (1998).",2. Bayesian models for simple graphs,[0],[0]
A random graph is called scale-free when the fraction of nodes in the network having k connections to other nodes behaves like k−τ for large values of k and some exponent τ > 1.,2. Bayesian models for simple graphs,[0],[0]
"More precisely, let Dn,i := ∑ j 6=iXi,j denote the (random) degree of node i, for every i ≤ n.",2. Bayesian models for simple graphs,[0],[0]
"Then X is (asymptotically) scale-free when, for every node i ≤ n,
P{Dn,i = k} ∼ ck−τ , as n→∞, (2)
for some constant c > 0, a power law exponent τ",2. Bayesian models for simple graphs,[0],[0]
"> 1, and k sufficiently large.",2. Bayesian models for simple graphs,[0],[0]
"Here the notation A ∼ B denotes that the ratio A/B → 1 in the specified limit.
",2. Bayesian models for simple graphs,[0],[0]
"In order to model scale-free random graphs, Britton et al. (2006) suggested reparameterizing the model in Eq.",2. Bayesian models for simple graphs,[0],[0]
"(1) by a sequence of odds ratios ri,j := pi,j/(1 − pi,j), for every i < j ≤ n, which factorize as ri,j = UiUj , for some U := (U1, . . .",2. Bayesian models for simple graphs,[0],[0]
", Un).",2. Bayesian models for simple graphs,[0],[0]
"The node-specific factors Ui are then modeled as Ui := Wi/ √ L for some sequence of nonnegative random variables W := (W1, . . .",2. Bayesian models for simple graphs,[0],[0]
",Wn) and where L := ∑n i=1Wi.",2. Bayesian models for simple graphs,[0],[0]
"In a series of results, (Britton et al., 2006, Thms.",2. Bayesian models for simple graphs,[0],[0]
"3.1 & 3.2) and (van der Hofstad, 2016, Cor. 6.11 & Thm.",2. Bayesian models for simple graphs,[0],[0]
"6.13) assert conditions on the random variablesW so that the limiting distribution of the degrees Dn,i is a mixed Poisson distribution.",2. Bayesian models for simple graphs,[0],[0]
"We will further detail these previous results in Section 4.
",2. Bayesian models for simple graphs,[0],[0]
"The distribution of Wi is interpreted here as a prior distribution for the degree Dn,i of node i, and if its distribution has heavy tails, then so will the distribution of Dn,i. Conversely, if the distribution of Wi does not have heavy tails, then neither will the distribution of the degrees Dn,i.",2. Bayesian models for simple graphs,[0],[0]
"We explore this alternative in Section 7.
",2. Bayesian models for simple graphs,[0],[0]
"Previous authors did not suggest any particular choices for the distribution of Wi, and so we elect to model them with BFRY random variables (Bertoin et al., 2006; Devroye & James, 2014), which have a heavy-tailed distribution and have recently played a role in the construction of several power law models in Bayesian statistics.",2. Bayesian models for simple graphs,[0],[0]
"Other heavy tailed distributions, such as those exhibited by log normal random variables, may also be used to model the Wi, and these options may be explored.",2. Bayesian models for simple graphs,[0],[0]
"One benefit of the BFRY distribution is that the thickness of its tails, and thus the power law behavior of the resulting graph, may be straightforwardly controlled by the discount parameter α.",2. Bayesian models for simple graphs,[0],[0]
"Consider the model from the previous section, parameterized by the odds ratios r := (ri,j : i < j ≤ n).",3. A generalized random graph,[0],[0]
"Define
G(r) := ∏
i<j≤n
(1 + ri,j) = ∏
i<j≤n
(1 + UiUj), (3)
and note that the conditional likelihood in Eq.",3. A generalized random graph,[0],[0]
"(1) may be rewritten in terms of the degrees Dn,i as
P (X = x | r) =",3. A generalized random graph,[0],[0]
"G(r)−1 ∏
i<j≤n
(UiUj) xi,j (4)
= G(r)−1 ∏ i≤n U Dn,i i .",3. A generalized random graph,[0],[0]
"(5)
The random simple graphX is called a generalized random graph, and we will henceforth write X | r ∼ GRG(n, r).
",3. A generalized random graph,[0],[0]
"Let α ∈ (0, 1), which we call the discount parameter, and let C1, C2, . . .",3. A generalized random graph,[0],[0]
"be a sequence of positive values satisfying
lim n→∞ Cn =∞ and lim n→∞ Cαn/n = 0.",3. A generalized random graph,[0],[0]
"(6)
Let the weights W1, . . .",3. A generalized random graph,[0],[0]
",Wn be i.i.d.",3. A generalized random graph,[0],[0]
"with density
fn(w) ∝ w−α−1(1− e−w)1{0≤w≤Cn}.",3. A generalized random graph,[0],[0]
"(7)
(These are truncated BFRY random variables and will be discussed, along with a method for simulation, in Section 3.1.)",3. A generalized random graph,[0],[0]
Then the corresponding generalized random graph has an (asymptotic) power law degree distribution with power law exponent τ,3. A generalized random graph,[0],[0]
= 1,3. A generalized random graph,[0],[0]
+ α.,3. A generalized random graph,[0],[0]
"We summarize this construction in the following theorem:
Theorem 3.1.",3. A generalized random graph,[0],[0]
"For every n, let W1, . . .",3. A generalized random graph,[0],[0]
",Wn be i.i.d.",3. A generalized random graph,[0],[0]
"with density fn and let (Dn,i)i≤n be the degrees of the generalized random graph X | r ∼ GRG(n, r), where
r := (ri,j)i<j≤n is the sequence of odds ratios
ri,j = WiWj/L, i < j ≤ n, (8)
and L := ∑ iWi.",3. A generalized random graph,[0],[0]
"Then the following hold:
1.",3. A generalized random graph,[0],[0]
"For y 1, P{Dn,i = y} ∼ cy−1−α, for every node i and for some constant c, as n→∞. 2.",3. A generalized random graph,[0],[0]
"For any m, the collection Dn,1, . . .",3. A generalized random graph,[0],[0]
", Dn,m are asymptotically independent, as n→∞.
This construction is closely related to the model described by van der Hofstad (2016, Thm. 6.13), and the proof of Theorem 3.1, which is provided in the supplementary material, follows analogously to the results by Britton et al. (2006, Thms.",3. A generalized random graph,[0],[0]
3.1 & 3.2),3. A generalized random graph,[0],[0]
.,3. A generalized random graph,[0],[0]
Note that the power law exponent τ,3. A generalized random graph,[0],[0]
= 1 + α of the graph (as described by Eq.,3. A generalized random graph,[0],[0]
"(2)) is determined by the parameter α ∈ (0, 1), and takes values in (1, 2).",3. A generalized random graph,[0],[0]
"While power law exponents in (2, 3) has often been suggested in the past, it has more recently been shown that exponents within the (1, 2) range of our model is more appropriate in many domains (van der Hofstad, 2016, Ch. 1); (Crane & Dempsey, 2015).",3. A generalized random graph,[0],[0]
"A random variable W with density function fn given by Eq. (7) is a ratio of gamma and beta random variables, upper truncated at Cn.",3.1. Truncated BFRY random variables,[0],[0]
"In particular let
g ∼ gamma(1− α, 1) and b ∼ beta(α, 1), (9)
be independent, then the ratio Z := g/b has density p(z) ∝ z−α−1(1",3.1. Truncated BFRY random variables,[0],[0]
"− e−z) on (0,∞)",3.1. Truncated BFRY random variables,[0],[0]
"(by construction), which is known as the Bertoin-Fujita-Roynette-Yor (BFRY) distribution (Bertoin et al., 2006; Devroye & James, 2014) and has been used in the construction of power law models in some recent applications in machine learning (James et al., 2015; Lee et al., 2016).",3.1. Truncated BFRY random variables,[0],[0]
"The random variable W is then
obtained by upper truncating the random variable Z at Cn.",3.1. Truncated BFRY random variables,[0],[0]
By our requirements on the sequence Cn (c.f. Eq.,3.1. Truncated BFRY random variables,[0],[0]
"(6)), the density function fn of W approaches the density function of the BFRY random variable Z as n→∞, that is,
lim n→∞
fn(w) = α
Γ(1− α) w−α−1(1− e−w), (10)
which is heavy-tailed with infinite moments.",3.1. Truncated BFRY random variables,[0],[0]
It is straightforward to simulate these truncated BFRY random variables by repeatedly simulating g and b as in Eq.,3.1. Truncated BFRY random variables,[0],[0]
"(9), accepting W := g/b as a sample when W < Cn.
",3.1. Truncated BFRY random variables,[0],[0]
"The truncation of W at Cn produces a random variable with finite mean (for n < ∞), which is essential when constructing the generalized random graph and motivates the construction by van der Hofstad (2016, Thm. 6.13) alluded to earlier; see Section 4.",3.1. Truncated BFRY random variables,[0],[0]
"For simplicity, one could take Cn = n, but the flexibility to set this parameter allows us to control other properties of the model.",3.1. Truncated BFRY random variables,[0],[0]
"For example, in the next section we show how to vary this truncation level to control the sparsity of the graph.",3.1. Truncated BFRY random variables,[0],[0]
"The discount parameter α ∈ (0, 1) controls the power law behavior of the graph, where decreasing α results in heavier tails in the degree distribution of the nodes in the graph.",3.2. Controlling power law and sparsity in the graph,[0],[0]
We can visualize this behavior by simulating graphs at different values of α.,3.2. Controlling power law and sparsity in the graph,[0],[0]
"In Section 3, we set Cn = n and show the number of nodes of varying degrees in two simulated graphs, one with α = 0.2 and one with α = 0.8.
",3.2. Controlling power law and sparsity in the graph,[0],[0]
"The degree distribution of the nodes in a graph of course affects the sparsity of the graph; to characterize this relationship, we can upper bound the expected number of links in the graph as follows:
Theorem 3.2.",3.2. Controlling power law and sparsity in the graph,[0],[0]
Let En be the number of positive edges in the graph.,3.2. Controlling power law and sparsity in the graph,[0],[0]
"Then E[En] = O(nC1−αn ).
",3.2. Controlling power law and sparsity in the graph,[0],[0]
The derivation of this result is provided in the supplementary material.,3.2. Controlling power law and sparsity in the graph,[0],[0]
"While varying α can thus control the sparsity of the graph in addition to the power law behavior, we often want to decouple these behaviors, in which case we could parameterize the truncation level as Cn = nβ , for some sparsity parameter β > 0.",3.2. Controlling power law and sparsity in the graph,[0],[0]
"Note the restriction α < min{1, 1/β} must be enforced in order to ensure that the conditions in Eq.",3.2. Controlling power law and sparsity in the graph,[0],[0]
(6) are satisfied.,3.2. Controlling power law and sparsity in the graph,[0],[0]
"In this case, the bound in Theorem 3.2 becomes E[En] = O(n1+β(1−α)).",3.2. Controlling power law and sparsity in the graph,[0],[0]
"The interpretation here is that increasing the upper bound Cn increases the likelihood that any particular node will link to others, but does not affect the (asymptotic) power law characterized by Theorem 3.1.",3.2. Controlling power law and sparsity in the graph,[0],[0]
"In Section 3.2, we display the average number of positive edges in graphs that were simulated with fixed α = 0.3 and varying values of the sparsity parameter β.",3.2. Controlling power law and sparsity in the graph,[0],[0]
"We note that in simulations, we encountered numerical issues in β > 1.4 regimes.",3.2. Controlling power law and sparsity in the graph,[0],[0]
"Referring to the construction for generalized random graphs in Section 2, Britton et al. (2006, Thm. 3.1) shows that when the weights Wi have finite first and second moments, then the limiting distribution of the degree Dn,i is a mixed Poisson distribution.",4. Related work,[0],[0]
"Most such distributions are light-tailed, however, in which case the degrees will not exhibit power law behavior.",4. Related work,[0],[0]
"Britton et al. (2006, Thm. 3.2) therefore provides an alternative construction in which Wi may have infinite moments (so that it may exhibit a heavy tail), which results in a graph with a power law exponent of τ = 2.",4. Related work,[0],[0]
"Finally, van der Hofstad (2016, Thm. 6.13) suggests yet another construction where the Wi are upper truncated to be of order o(n), where n is the number of nodes in the graph.",4. Related work,[0],[0]
"The resulting random variables therefore have finite moments, yet exhibit a heavy tail, and the resulting random graph has a heavy tailed degree distribution with an arbitrary power law exponent.",4. Related work,[0],[0]
"None of these results suggest a particular choice for the distribution of Wi, however, and so we have elected to use BFRY random variables (which are heavy tailed) that are upper truncated (so that they have finite moments).",4. Related work,[0],[0]
We note that the requirements on our truncation level (c.f. Eq.,4. Related work,[0],[0]
"(6)) is less strict than the o(n) criterion of the van der Hofstad (2016, Thm.",4. Related work,[0],[0]
"6.13) construction.
",4. Related work,[0],[0]
"The reader may consult the surveys by Bollobás & Riordan (2003); Albert & Barabási (2002); Dorogovtsev & Mendes (2002) for a background on scale-free random graphs, which is too large to review here.",4. Related work,[0],[0]
"While these models are numerous, the following recent pieces of work in the Bayesian statistics and machine learning communities may be of interest to the reader: Caron & Fox (2014); Veitch & Roy (2015); Crane & Dempsey (2016); Cai & Broderick (2015).",4. Related work,[0],[0]
"This collection of work discusses power law degree distributions, albeit in some cases in multi-graphs
(i.e., graphs with nonnegative integer-valued edges) and in some cases the power law behavior is not characterized, only numerically observed in simulations.",4. Related work,[0],[0]
"Many of these models can be seen to invoke their power law properties from the Pitman–Yor process (Pitman & Yor, 1997) (or related stochastic processes), where the extent of this behavior is controlled by the discount parameter α ∈ (0, 1) of the Pitman–Yor model, which, like the BFRY distribution, is related to a stable subordinator of index α.",4. Related work,[0],[0]
Latent factor models for relational data assume that a set of latent clusters underlie the network.,5. Incorporating latent factors,[0],[0]
"For example, in a social network, the latent factors could be the unobserved hobbies or interests of individuals, which determine the observed friendships in the network.",5. Incorporating latent factors,[0],[0]
"Bayesian models for latent factors in relational data are widespread, with some of the most popular based on stochastic blockmodels, where models for unsupervised learning, or clustering, are used to infer the latent factors (Nowicki & Snijders, 2001; Kemp et al., 2006; Airoldi et al., 2009; Miller et al., 2009).",5. Incorporating latent factors,[0],[0]
"In this section, we present extensions of the generalized random graph that incorporate latent factors by scaling the odds ratios, while maintaining their power law degree distribution.
",5. Incorporating latent factors,[0],[0]
"We will first provide a general result showing how to incorporate random scaling variables into the model, followed by specific examples that model these scaling variables with latent clusters.",5. Incorporating latent factors,[0],[0]
"Let the odds ratios in the generalized random graph be given by ri,j = Ai,jUiUj for some Ai,j ≥ 0.",5. Incorporating latent factors,[0],[0]
"Note that pi,j → 1 as Ai,j → ∞ and pi,j → 0 as Ai,j → 0, and so the edge-specific weight Ai,j simply scales the link probability.",5. Incorporating latent factors,[0],[0]
"The random graph X | r ∼ GRG(n, r) then has the likelihood
P (X = x | r) =",5. Incorporating latent factors,[0],[0]
"G(r)−1 ∏
i<j≤n
A xi,j i,j ∏ i≤n U Dn,i i , (11)
where the normalization term G(r) in Eq.",5. Incorporating latent factors,[0],[0]
"(3) is now
G(r) := ∏
i<j≤n
(1 +Ai,jUiUj) (12)
= ∑ x ∏",5. Incorporating latent factors,[0],[0]
"i<j≤n A xi,j i,j ∏ i≤n U Dn,i i , (13)
where the final equality follows simply because∑ x P (X = x | r) = 1.",5. Incorporating latent factors,[0],[0]
"So constructed, the odds ratios r will influence the link probabilities in the generalized random graph, but will not affect the power law behavior of the degree distributions (under some assumptions on the random variables Ai,j).",5. Incorporating latent factors,[0],[0]
"We summarize this construction in the following theorem, the proof for which is provided in the supplementary material:
Theorem 5.1.",5. Incorporating latent factors,[0],[0]
Let (Wi)i≤n be i.i.d. random variables with density function fn(w) (in Eq. (7)).,5. Incorporating latent factors,[0],[0]
"Let (Ai,j)i<j≤n be a collection of uniformly bounded random variables, where, for every i ≤ n, the collection (Ai,j)j>i is exchangeable.",5. Incorporating latent factors,[0],[0]
"Let (Dn,i)i≤n be the degrees of the random graph X | r ∼ GRG(n, r), where r := (ri,j)i<j≤n is the sequence of odds ratios
ri,j = Ai,jWiWj/L, i < j ≤ n, (14) and where L := ∑ iWi.",5. Incorporating latent factors,[0],[0]
"Then the degrees (Dn,i)i≤n satisfy statements (1) and (2) in Theorem 3.1.
",5. Incorporating latent factors,[0],[0]
"For example, we may construct stochastic blockmodels, such as those introduced by Nowicki & Snijders (2001), as follows: For every i ≤ n, let Zi be a random variable taking values in {1, . . .",5. Incorporating latent factors,[0],[0]
",K}, indicating which one (and only one) of K different factors to associate with node i.",5. Incorporating latent factors,[0],[0]
"We want the latent cluster assignments for two nodes i and j to influence their link probability, which we could capture with a set of parameters θk,`, for k, ` = 1, . . .",5. Incorporating latent factors,[0],[0]
",K. Then the parameter θZi,Zj could represent, or influence, the probability of a link between nodes i and j. Taking a Bayesian approach, the indicator variables Zi may be modeled with a Dirichlet-categorical conjugate distribution and their values may be inferred via probabilistic inference.",5. Incorporating latent factors,[0],[0]
"An example of such a model could be summarized as follows: Let
Zi ∼ categorical(π), i ≤ n, (15) π ∼ Dirichlet(c/K), where c > 0, (16)
θ`,k ∼ gamma(aθ, bθ), `, k ≤ K, (17) Ai,j = θZi,Zj , i < j ≤ n, (18)
and construct the random graph X as in Theorem 5.1.",5. Incorporating latent factors,[0],[0]
"Kemp et al. (2006) developed a nonparametric extension of a similar model that in a sense takes the limit K → ∞, allowing an appropriate number of clusters to be automatically inferred from the data.",5. Incorporating latent factors,[0],[0]
"In this case, the marginal law of the indicator variables Z1, . . .",5. Incorporating latent factors,[0],[0]
", Zn is given by a Chinese restaurant process (with concentration parameter c).
",5. Incorporating latent factors,[0],[0]
"Several generalizations of the stochastic blockmodel allow the clusters underlying the network to overlap, leading to mixed membership stochastic blockmodels (Airoldi et al., 2009) or the related latent feature relational models (Miller et al., 2009).",5. Incorporating latent factors,[0],[0]
"To capture this structure, we may generalize the indicators Zi to now represent a binary K-vector with entry Zi,k = 1 indicating node i is associated with cluster k, now called a feature, and Zi,k = 0 otherwise.",5. Incorporating latent factors,[0],[0]
"One example of such a model could be summarized as follows:
Zi,k ∼ Bernoulli(pk), i ≤ n, k ≤ K, (19) pk ∼ beta(c, cγ/K), k ≤ K, and c, γ > 0, (20) θ`,k ∼ gamma(aθ, bθ), `, k = 1, 2, . . .",5. Incorporating latent factors,[0],[0]
", (21)
Ai,j = ∑ k,` θk,`Zi,kZj,`, i < j ≤ n, (22)
and construct the random graph X as in Theorem 5.1.",5. Incorporating latent factors,[0],[0]
"Miller et al. (2009) derived a nonparametric extension of this model that in a sense takes the limit K →∞, in which case the marginal law of the vectors Z1, . . .",5. Incorporating latent factors,[0],[0]
", Zn is that of an Indian buffet process (with mass parameter γ and concentration parameter c)",5. Incorporating latent factors,[0],[0]
"(Ghahramani et al., 2007).",5. Incorporating latent factors,[0],[0]
"We derive a variational Bayesian inference algorithm (Jordan et al., 1999) that approximates the (optimal state of the) posterior distribution of the model components, given a network dataset.",6. Variational inference,[0],[0]
"We approximate the required gradients in this procedure with stochastic gradient ascent (Bottou, 2010; Hoffman et al., 2013), computed on minibatches (i.e., subsets) of edges in the graph.",6. Variational inference,[0],[0]
"In variational inference, we approximate the posterior distribution on the latent variables W := (W1, . . .",6.1. The variational lower bound,[0],[0]
",Wn) with a variational distribution q(W ; θ), the parameters θ of which are fit to maximize the following lower bound on the marginal likelihood
log p(X) ≥ Eq(W ;θ) [ log
p(X |W ;α)p(W ;α) q(W ; θ)
] , (23)
where p(X | W ) is the likelihood function computed as in Eq.",6.1. The variational lower bound,[0],[0]
"(5), and p(W ;α) is the prior on W represented by the density function in Eq.",6.1. The variational lower bound,[0],[0]
(7).,6.1. The variational lower bound,[0],[0]
"The (non random) discount parameter α is inferred by corresponding gradient ascent updates maximizing the likelihood of the model, which is described in Section 6.4.",6.1. The variational lower bound,[0],[0]
We specify a mean field variational distribution q(W ; θ) =∏n i=1 q(Wi; θi).,6.1. The variational lower bound,[0],[0]
"We considered several approximations for the marginals q(Wi; θi) including truncated BFRY and truncated gamma distributions, however, in our experiments we found that the following rectified gamma distribution performed well:
Wi =q min{W ′i , Cn}, (24) W ′i ∼ gamma(θi,shp, θi,rte), (25)
independently for every i ≤ n, where θi,shp and θi,rte denote the shape and rate parameters of the gamma distribution, respectively, and the notation =q emphasizes that this formula holds under the variational distribution q.",6.1. The variational lower bound,[0],[0]
We maximize the lower bound on the right hand side of Eq.,6.2. Stochastic gradient ascent,[0],[0]
"(23) by stochastic gradient ascent, where on the t-th step of the algorithm, we make the following updates to the
parameters in parallel
θ (t+1) i ← θ (t)",6.2. Stochastic gradient ascent,[0],[0]
"i + ρt∇θiEq(W ;θ(t))[L(X,W ; θ (t))",6.2. Stochastic gradient ascent,[0],[0]
"], (26)
",6.2. Stochastic gradient ascent,[0],[0]
"for i ≤ n and some sequence (ρt)t≥1 of positive numbers satisfying the Robbins–Monro criterion (Robbins & Monro, 1951) ∑ t ρt =∞ and ∑ t ρ",6.2. Stochastic gradient ascent,[0],[0]
2,6.2. Stochastic gradient ascent,[0],[0]
"t <∞, and where
L(X,W ; θ) := log p(X,W ;α)− log q(W ; θ) (27)
= ∑
(i,j)∈E log p(Xi,j |W ) + n∑ i=1",6.2. Stochastic gradient ascent,[0],[0]
"log p(Wi;α)
",6.2. Stochastic gradient ascent,[0],[0]
− n∑ i=1,6.2. Stochastic gradient ascent,[0],[0]
"log q(Wi; θi), (28)
where E denotes the observed edges (both links and nonlinks) in the dataset.",6.2. Stochastic gradient ascent,[0],[0]
"We cannot evaluate the expectation (with respect to the rectified gamma distributions q(W ; θ)) analytically, and so we elect to use a particular Monte Carlo approximation of this gradient detailed by Knowles (2015), which was developed for gamma variational distributions and easily applies to the rectified gamma case.
",6.2. Stochastic gradient ascent,[0],[0]
"Briefly, for every i ≤ n, create the collection of S Monte Carlo samples from the variational distribution as follows: Independently for s ≤ S, let z (s) i ∼ Uniform(0, 1), and set W (s) i = ψ(z (s) i ; θi), where ψ(z; θ) :",6.2. Stochastic gradient ascent,[0],[0]
"= min{F−1θ (z), Cn} and F −1 θ",6.2. Stochastic gradient ascent,[0],[0]
(x) is the inverse of the cumulative distribution function for a gamma random variable.,6.2. Stochastic gradient ascent,[0],[0]
"For convenience, we recall that
Fa,b(x) = ∫ x 0 ba Γ(a) ta−1e−btdt. (29)
",6.2. Stochastic gradient ascent,[0],[0]
"For every k ≤ n, the gradient with respect to the parameters θk is then approximated by
∇θkEq(W ;θ)[L(X,W ; θ)]
≈ 1 S ∑ s ∇WkL(X,W (s); θ)∇θkψ(z (s) k ; θk), (30)
where W (s) := (W (s)1 , . . .",6.2. Stochastic gradient ascent,[0],[0]
",W (s) n ).",6.2. Stochastic gradient ascent,[0],[0]
"This estimator is unbiased and has low enough variance that often a single sample suffices for the approximation (Salimans & Knowles, 2013; Kingma & Welling, 2014).",6.2. Stochastic gradient ascent,[0],[0]
The gradient of ψ is nonzero only when {F−1θk (z (s) k ),6.2. Stochastic gradient ascent,[0],[0]
"< Cn}, in which case we may immediately obtain the partial derivative with respect to the rate parameter; in particular, we have
∇θk,rteψ(z (s) k ; θk) =
{ z (s) k
θk,rte , if F−1θk (z (s) k )",6.2. Stochastic gradient ascent,[0],[0]
"< Cn,
0, otherwise.",6.2. Stochastic gradient ascent,[0],[0]
"(31)
",6.2. Stochastic gradient ascent,[0],[0]
"The partial derivative with respect to the shape parameter ∇θk,shpψ(z (s) k ; θk) does not have a closed form solution and must be approximated.",6.2. Stochastic gradient ascent,[0],[0]
"Different approximation routines are suggested by Knowles (2015) for different regimes of the shape parameter θk,shp, and we found these approximations to be accurate and efficient in our experiments.",6.2. Stochastic gradient ascent,[0],[0]
"Computing the n required gradients in Eq. (26) may be done in parallel, and this computation, whether performed analytically or with automatic differentiation methods, scales with the number of edges in the graph.",6.3. Minibatches of edges in the graph,[0],[0]
"This can be prohibitive for many network datasets, and we therefore introduce a further approximation where this gradient is evaluated on subsets (a.k.a. minibatches) of the dataset, a technique from stochastic gradient ascent (Bottou, 2010) adopted in the context of variational Bayesian inference by Hoffman et al. (2013).",6.3. Minibatches of edges in the graph,[0],[0]
"In the case of a network dataset, we may select minibatches that are subsets of the observed edges in the graph.",6.3. Minibatches of edges in the graph,[0],[0]
"In particular, write the gradient of Eq.",6.3. Minibatches of edges in the graph,[0],[0]
(28) with respect to the variable Wk (which is required by Eq. (30)),6.3. Minibatches of edges in the graph,[0],[0]
"as
∇WkL(W (s); θ) = ∑
(i,j)∈E
g(i,j)(X,W (s); k), (32)
where g(i,j)(X,W ; k) := ∇Wk",6.3. Minibatches of edges in the graph,[0],[0]
"[log p(Xi,j | W ) + |E|−1 log p(W ;α)− |E|−1 log q(W ; θ)] is the gradient that ignores all but one edge of the graph.",6.3. Minibatches of edges in the graph,[0],[0]
"We may therefore compute the unbiased estimate of this gradient
∇WkL(W",6.3. Minibatches of edges in the graph,[0],[0]
"(s); θ) ≈ |E| |B| ∑ (i,j)∈B g(i,j)(X,W (s); k), (33)
on a minibatch B ⊆ E of the observed edges.",6.3. Minibatches of edges in the graph,[0],[0]
"Without good prior knowledge of how to set the discount parameter α and the sparsity parameter β controlling the power law and sparsity behaviors of the graph, respectively, we infer their values from the data.",6.4. Inference on the parameters α and β,[0],[0]
"First consider the discount parameter, which we infer with gradient ascent.",6.4. Inference on the parameters α and β,[0],[0]
"After every update to the latent variables W , we fix them to their mean under the distribution q, i.e., Ŵ := (Ŵ1, . . .",6.4. Inference on the parameters α and β,[0],[0]
", Ŵn) where Ŵi = Eq(Wi;θi)[Wi], and take a step in the direction of the gradient
∇α log p(Ŵ ;α) = n∑ i=1",6.4. Inference on the parameters α and β,[0],[0]
"∇α log p(Ŵi;α) (34)
= n∑ i=1",6.4. Inference on the parameters α and β,[0],[0]
"[ −∇αZα,β Zα,β − log(Ŵi) ] , (35)
which is straightforward to derive from the density function in Eq. (7), and where the normalization term
Zα,β := ∫ Cn 0 w−α−1(1− e−w) dw (36)
is a function of α and β, if we let Cn = nβ as suggested in Section 3.2.",6.4. Inference on the parameters α and β,[0],[0]
"We do not have a closed form solution for
α = 0.3 BFRY -57323.19 ± 91.62 -57675.72",6.4. Inference on the parameters α and β,[0],[0]
"± 31.71 Gamma -71341.90 ± 116.82 -71841.66 ± 47.38
α = 0.5 BFRY -21077.62 ± 79.64 -21289.75 ±",6.4. Inference on the parameters α and β,[0],[0]
34.23 Gamma -24430.38 ± 73.06 -24701.06 ±,6.4. Inference on the parameters α and β,[0],[0]
"11.31
α = 0.7 BFRY -7894.67 ± 41.84 -8027.42 ± 51.08 Gamma -8511.48 ± 22.45 -8601.50 ± 15.42
this term when Cn < ∞, and, unfortunately, inference on model parameters where the likelihood is difficult to evaluate is a challenging problem; for example, see the approaches taken by Murray et al. (2006) on such problems, which those authors call doubly intractable distributions.",6.4. Inference on the parameters α and β,[0],[0]
"Accurate inference for α is important in our model, because it controls the power law behavior of the graph.",6.4. Inference on the parameters α and β,[0],[0]
"In our experiments, we approximate the gradient in Eq.",6.4. Inference on the parameters α and β,[0],[0]
"(35) for (fixed β) by approximating Zα,β (via Eq. (36)) and ∇αZα,β = ∫ Cn 0 −w−α−1(1 − e−w) logw dw, with line integrals.",6.4. Inference on the parameters α and β,[0],[0]
"In the Section 7, we demonstrate that this approximation works well in various regimes of α, with slight overestimation for moderate values.
",6.4. Inference on the parameters α and β,[0],[0]
"Similar approaches to infer β may be derived with finite difference approximations; we did not find these approaches successful in our experiments, however, and so we instead select β by cross validation.",6.4. Inference on the parameters α and β,[0],[0]
"We first demonstrate how the inference procedure in Section 6.4 can correctly differentiate between various regimes
of α.",7. Experiments,[0],[0]
"We ran an experiment where for each value α ∈ {0.1, 0.3, 0.5, 0.7}, we simulated 10 datasets from the model with n = 1, 000 nodes, while fixing β = 1.0.",7. Experiments,[0],[0]
"For each simulated dataset, we ran an instance of the inference routine with α randomly initialized.",7. Experiments,[0],[0]
"In Fig. 3, we show the trace plots of alpha during each instance of the inference routine.",7. Experiments,[0],[0]
"For comparison, the true values of α are also shown as horizontal dashed lines.",7. Experiments,[0],[0]
"We can see that the inference routine can correctly distinguish between these different regimes of α, with slight overestimation in the moderate α regime.",7. Experiments,[0],[0]
"Interestingly, despite random initializations of α ∈ (0, 1), the algorithm always immediately inflates α to around 0.9, and then slowly decreases this value during inference, regardless of what value of α generated the data.
",7. Experiments,[0],[0]
We next demonstrate that accurately capturing power law structures in datasets will improve predictive performance.,7. Experiments,[0],[0]
"While fixing β = 1.0, we simulate three network datasets with 5,000 nodes from our model with discount parameters α = 0.3, 0.5, and 0.7, respectively, which therefore exhibit increasingly lighter-tailed degree distributions.",7. Experiments,[0],[0]
"The generated graphs have 117,300, 32,925, and 9,460 links, respectively.",7. Experiments,[0],[0]
"To establish a baseline model that does not exhibit power law degree distributions but is otherwise comparable to our model, we implement the generalized random graph where the node-specific weights are constructed from the gamma random variables Wi ∼ gamma(θ, 1), for some positive parameter θ, i.i.d. for every node i ≤",7. Experiments,[0],[0]
n. Note that the parameter θ controls the sparsity of the generated graph; larger values of θ imply denser graphs.,7. Experiments,[0],[0]
"It follows analogously to Theorem 5.1 that
P{Dn,i = k} ∼ kθ−1
2k+θ , (37)
for k 1, as n → ∞.",7. Experiments,[0],[0]
"This model therefore does not exhibit power law behavior, as desired.",7. Experiments,[0],[0]
"We refer to this model as “Gamma” and the power law graph model as “BFRY”.
",7. Experiments,[0],[0]
"We ran an experiment holding out 20% of the edges in the
simulated graphs as test sets, training the two models on the remaining 80% of the edges.",7. Experiments,[0],[0]
"We used a mini-batch size of 5,000 edges (note that the training dataset corresponds to almost 10 million observed edges).",7. Experiments,[0],[0]
"We ran each inference procedure for 20,000 steps of stochastic gradient ascent updates, using Adam (Kingma & Ba, 2015) to adjust the learning rates at each step.",7. Experiments,[0],[0]
"We repeated each experiment 5 times, each time holding out a different test set and using a different random initialization.",7. Experiments,[0],[0]
"Again, for this experiment we fixed β = 1.",7. Experiments,[0],[0]
"In Table 1 we report a mean loglikelihood metric for the test datasets, where the metric for each run is obtained by averaging the test log-likelihoods across the states for the last 4,000 steps of the inference procedure; the displayed intervals are at ±1 standard deviation about the metric, from across the 5 repeats.",7. Experiments,[0],[0]
"We also report a max log-likelihood metric, which simply records the maximum test log-likelihood across the last 4,000 steps of the inference procedure, instead of the average.",7. Experiments,[0],[0]
"The best performing method is highlighted in bold (which in each case was the BFRY model).
",7. Experiments,[0],[0]
"In each case, we see that the BFRY model achieves higher test log-likelihood metrics than the Gamma model, as expected, implying that accurately capturing a power law degree distribution improves predictive performance (when power law behavior is truly present in the network).",7. Experiments,[0],[0]
"In Table 3, we report the inferred values for α, which were reasonably accurate, though we see slight overestimation for some regimes, as seen in the demonstration earlier.",7. Experiments,[0],[0]
"For the baseline Gamma model, we optimized the hyperparameter θ using gradient ascent maximizing the evidence lower bound of the model",7. Experiments,[0],[0]
"(c.f. Eq. (23)), and the inferred values are also reported in Table 3.
",7. Experiments,[0],[0]
"Next, we ran similar experiments on the following network datasets, each of which are expected to exhibit power law degree distributions:
• ‘USTop500Airports’: 500 nodes, 2,980 links • ‘openflights’: 7,976 nodes, 15,243 links • ‘polblogs’: 1,490 nodes, 9,517 links • ‘Facebook107’: 1,034 nodes, 26,749 links
Where appropriate, we saved only the upper triangular parts of the adjacency matrices.",7. Experiments,[0],[0]
"The ‘USTop500Airports’ dataset contains the (undirected, unweighted) flight connections between the 500 busiest US airports.",7. Experiments,[0],[0]
"The similar,
though much larger, ‘openflights’ dataset contains the flight connections between non-US airports.",7. Experiments,[0],[0]
"Scale-free networks have been proposed for such traffic networks, detailed for these datasets by Colizza et al. (2007).",7. Experiments,[0],[0]
"The ‘polblogs’ dataset contains the links between political blogs (judged by hyperlinks between the front webpages of the blogs) in the period leading up to the 2004 US presidential election, which is observed to exhibit power law degree distributions by Adamic & Glance (2005).",7. Experiments,[0],[0]
"The ‘Facebook107’ dataset contains “friendships” between users of a Facebook app, collected by Leskovec & McAuley (2012); social networks are widely studied for their power law degree distributions.
",7. Experiments,[0],[0]
"For both the Gamma and BFRY models, we ran our variational inference procedure for 20,000 steps on each dataset.",7. Experiments,[0],[0]
"As before, we repeated the experiment 5 times for each network, each time holding out a different 20% of the edges in the network as a testing set.",7. Experiments,[0],[0]
"We selected the value of β from among the grid {0.6, 0.9, 1.0, 1.2, 1.4} with 5-fold cross validation on the training set.",7. Experiments,[0],[0]
"We set the minibatch size to be equal to the number of nodes in the graph; for example, we used minibatches of 1,490 edges for the polblog dataset.",7. Experiments,[0],[0]
"The evaluation metrics on the test datasets are summarized in Table 2, and the inferred hyperparameter values are reported in Table 3.",7. Experiments,[0],[0]
"We see that the BFRY model once again outperforms the Gamma baseline model, according to the test log-likelihood metrics.
",7. Experiments,[0],[0]
Probabilistic inference on α by the BFRY model provides some of the most interesting analyses here.,7. Experiments,[0],[0]
"With α ≈ 0.00 (underflowing our machine’s precision), the Facebook107 social network has the degree distribution with the heaviest tails, followed by the USTop500Airports traffic network with α ≈ 0.23, the polblog citation network with α ≈ 0.64, and the openflights network has the lightest tailed degree distribution with α ≈ 0.67.",7. Experiments,[0],[0]
"Future work could focus on implementing the latent factor modeling generalizations presented in Section 5, which are natural assumptions in many domains where networks are expected to exhibit power law degree distributions.",8. Future work,[0],[0]
"Alternative approaches to inference on the sparsity parameter β should also be explored, since controlling the sparsity in the graph was important for good predictive performance.",8. Future work,[0],[0]
The authors thank Remco van der Hofstad for helpful advice and anonymous reviewers for helpful feedback.,Acknowledgements,[0],[0]
"J. Lee and S. Choi were partly supported by an Institute for Information & Communications Technology Promotion (IITP) grant, funded by the Korean government (MSIP) (No.20140-00147, Basic Software Research in Human-level Lifelong Machine Learning (Machine Learning Center)) and Naver, Inc.",Acknowledgements,[0],[0]
"C. Heaukulani undertook this work in part while a visiting researcher at the Hong Kong University of Science and Technology, who along with L. F. James was funded by grant rgc-hkust 601712 of the Hong Kong Special Administrative Region.",Acknowledgements,[0],[0]
"We present a model for random simple graphs with power law (i.e., heavy-tailed) degree distributions.",abstractText,[0],[0]
"To attain this behavior, the edge probabilities in the graph are constructed from Bertoin–Fujita–Roynette–Yor (BFRY) random variables, which have been recently utilized in Bayesian statistics for the construction of power law models in several applications.",abstractText,[0],[0]
"Our construction readily extends to capture the structure of latent factors, similarly to stochastic blockmodels, while maintaining its power law degree distribution.",abstractText,[0],[0]
"The BFRY random variables are well approximated by gamma random variables in a variational Bayesian inference routine, which we apply to several network datasets for which power law degree distributions are a natural assumption.",abstractText,[0],[0]
"By learning the parameters of the BFRY distribution via probabilistic inference, we are able to automatically select the appropriate power law behavior from the data.",abstractText,[0],[0]
"In order to further scale our inference procedure, we adopt stochastic gradient ascent routines where the gradients are computed on minibatches (i.e., subsets) of the edges in the graph.",abstractText,[0],[0]
Bayesian inference on random simple graphs with power law degree distributions,title,[0],[0]
A classical estimation problem in many scientific inquiries is the well-studied change point detection problem where one tries to estimate when some properties of a sequence of random variables changes.,1. Introduction,[0],[0]
"This local property is of prime importance in many learning tasks such as signal segmentation (Abou-Elailah et al., 2016; Kim et al., 2009), change point detection in comparative genomics for early cancer diagnosis (Lai et al., 2005), and modeling and forecasting of changes in financial data (Lavielle & TeyssiÃĺre, 2006; Spokoiny, 2009).
",1. Introduction,[0],[0]
"For other applications, one needs more than this local answer and is interested in a more general overview of the time series where for instance earlier data samples behave like new ones creating a clustering effect.",1. Introduction,[0],[0]
"Examples of this are found in: electricity market data, where prices
1KTH Royal Institute of Technology, Stockholm, Sweden.",1. Introduction,[0],[0]
"Correspondence to: Othmane Mazhar <othmane@kth.se>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
might have different behavior corresponding to different price regimes that might reappear depending on some triggering events; signal partitioning with some parts of the signal sharing similar properties; and speech segmentation with different alternating sources.",1. Introduction,[0],[0]
"Generally speaking, it is of interest in these situations to determine not only the changes but also the clusters for a more precise description of the inhomogeneous time series.
",1. Introduction,[0],[0]
Parametric models for solving the change point detection problem have been proposed in Cleynen & Lebarbier (2014) and Rigaill et al. (2012).,1. Introduction,[0],[0]
"However, in dealing with the change point and clustering problem we would naturally require that our solution does not assume any knowledge of the number of changes nor the actual number of clusters, as these numbers would evolve over time, so we expect new changes in the process to happen and new clusters to form as N , the number of samples, grows.",1. Introduction,[0],[0]
"Thus, any practical procedure should be able to estimate these numbers and also have adaptive guarantees with respect to how fast these numbers grow.",1. Introduction,[0],[0]
"Similar setups for change point detection have been the subject of study by Harchaoui & Cappé (2007), Arlot et al. (2016) and Garreau & Arlot (2017) who use characteristic kernels for detecting changes in the distribution, while from a computational standpoint a more effective implementation has been proposed by Celisse et al. (2017).",1. Introduction,[0],[0]
"In this study, we will restrict ourselves to an iid (independent and identically distributed) Gaussian sequence model of the data with known variance, noting that the same study can be done using kernels and that the algorithm we develop can be effectively implemented using the same procedure as in (Celisse et al., 2017), as explained later in the paper.
",1. Introduction,[0],[0]
"Two other related lines of research, but which we do not explore here, are on-line algorithms for segmentation and L1-regularized segmentation.",1. Introduction,[0],[0]
"We refer the reader to (Tartakovsky et al., 2014) for an extensive review of on-line algorithms.",1. Introduction,[0],[0]
Data segmentation using the L1-penalty was introduced by Rudin et al. (1992).,1. Introduction,[0],[0]
"The one-dimensional case, corresponding to the Fussed LASSO, has been studied in (Tibshirani et al., 2005) and (Rennie & Dobson, 1969) and an efficient algorithm has been proposed by Arnold & Tibshirani (2016).",1. Introduction,[0],[0]
"More recent results can be found in (Dalalyan et al., 2017) for the one-dimensional case and (Hütter & Rigollet, 2016) for two-dimensional case.
",1. Introduction,[0],[0]
"Main contribution: The generalized setting of change point detection while clustering the segments for sequences
of data points does not seem to have been previously studied.",1. Introduction,[0],[0]
"In this work, we propose a two-pass dynamic programming algorithm for selecting an adequate model from a collection of candidate models.",1. Introduction,[0],[0]
We motivate the choice of the algorithm computationally by showing that it runs in O(N2D,1. Introduction,[0],[0]
"+ D4) time (where D is an upper bound on the number of change points), statistically by showing that it can be seen as an approximation of a computationally hard MAP optimization problem for which we can derive an oracle inequality that guarantees low sample complexity, consistency and adaptivity, and practically by testing the model on simulation data.
",1. Introduction,[0],[0]
Structure of the paper: In Section 2 we formulate the problem as one of nonparametric model selection from a family of models over all partitions of the data set.,1. Introduction,[0],[0]
"After some preliminaries and notations are given in Section 3, we propose in Section 4 a two-pass dynamic programming algorithm as a computationally effective relaxation of the optimization criterion and analyze its computational cost.",1. Introduction,[0],[0]
"We then put the model selection problem in a Bayesian framework in Section 5, and use a Laplace-type approximation to derive as optimization criterion the maximum a-posteriori probability.",1. Introduction,[0],[0]
"In Section 6 we derive an oracle inequality for the criterion that our algorithm is approximating, and study its properties.",1. Introduction,[0],[0]
Experimental results showing that the clusters and segments can be effectively estimated are presented in Section 7 using simulation data.,1. Introduction,[0],[0]
"Let Y be a measurable space and Y1, Y2, . . .",2. Problem formulation,[0],[0]
", YN ∈ Y denote random variables with distributions PYi .",2. Problem formulation,[0],[0]
Our goal is on one hand to detect changes in the sequence of distribution measures (PYi) N i=1,2. Problem formulation,[0],[0]
and on the other hand to cluster the data points coming from the same process.,2. Problem formulation,[0],[0]
"Hence we put random variables between two consecutive changes in the same segment, and we think of random variables of the same segment or different segments as belonging to the same cluster if they are the realization of the same process.
",2. Problem formulation,[0],[0]
"One important case both in theory and in practice is the uniform constant design model were the Yis depend on deterministic variables uniformly spaced on a grid Xi = i for i ∈ J1, NK := {1, . . .",2. Problem formulation,[0],[0]
", N} through a regression function f∗ with an additive iid random noise ( i)Ni=1.",2. Problem formulation,[0],[0]
"Taking the distribution of the i’s as N (0, σ2) with known variance, we end up with the following Gaussian sequence model:
Yi = f ∗",2. Problem formulation,[0],[0]
"i + i, for i ∈ J1, NK.",2. Problem formulation,[0],[0]
"(1)
Here we are placed in a regression setting of the form Y = f∗+ , where Y =",2. Problem formulation,[0],[0]
"[Y1 · · · YN ]T , f∗ =",2. Problem formulation,[0],[0]
"[f∗1 · · · f∗N ]T and = [ 1 · · · N ]T ∼ N (0, σ2IN ), and we are interested in estimating f∗ as a piecewise constant function that takes limited number of values.
",2. Problem formulation,[0],[0]
"We emphasize that it is unlikely that the data correspond exactly to a piecewise constant function plus independent
random Gaussian noise and that we are in this low dimensional hidden structure exactly, yet there might exist a good sparse linear approximation.",2. Problem formulation,[0],[0]
"Hence our search is not for an exact model, rather we are trying to select the best model in a collection of candidates, as we explain in the next section.",2. Problem formulation,[0],[0]
"We would like to perform dimensionality reduction by exploiting the hidden structure on the data sequence Y1, Y2, . . .",3. Preliminaries and notation,[0],[0]
", YN .",3. Preliminaries and notation,[0],[0]
To do this we split it into different segments while also putting the segments sharing the same mean into the same cluster.,3. Preliminaries and notation,[0],[0]
Hence if we knew the clusters our problem reduces to fitting a constant to a set of observations over each cluster.,3. Preliminaries and notation,[0],[0]
"Observe that if f∗ is constant over parts of J1, NK, then it determines a clustering of Y1, Y2, . . .",3. Preliminaries and notation,[0],[0]
", YN over the values where it is constant.",3. Preliminaries and notation,[0],[0]
"Hence, we can think about the problem as, first determining the clustering of the Y1, Y2, . . .",3. Preliminaries and notation,[0],[0]
", YN which would result in a partition π of J1, NK, and then choosing the best value of f̂ over each part as our estimate.",3. Preliminaries and notation,[0],[0]
"So f∗ belong to the subspace Fπ: subspace of functions that are constant over the parts of the partition π.
To formalize this, let M be an index set over the collection of partitions ΠN of J1, NK; given m ∈ M, denote by Fm the subspace of functions that are constant over the parts of πm.",3. Preliminaries and notation,[0],[0]
"Our goal is two-fold: find m̂ as the index estimate of Fm̂, the subspace where the estimate of f∗ lives, and from Fm̂ compute f̂m̂ as our estimate.",3. Preliminaries and notation,[0],[0]
We represent a partition π as an unordered collection of its subsets π = {,3. Preliminaries and notation,[0],[0]
"[1], [2], . . .",3. Preliminaries and notation,[0],[0]
", [|π|]} with [k] being the kth-equivalent class, -part or -cluster, and |π| the cardinality of the partition.",3. Preliminaries and notation,[0],[0]
Every part [k] can be seen as the union of segments [k] = {,3. Preliminaries and notation,[0],[0]
"[k1], [k2], . . .",3. Preliminaries and notation,[0],[0]
",",3. Preliminaries and notation,[0],[0]
[k|[k]|]} where (ki) |[k]| i=1 is the collection of maximal intervals in [k] that we call segments of the kth-cluster.,3. Preliminaries and notation,[0],[0]
The last element in each segment [ki] is called a change point.,3. Preliminaries and notation,[0],[0]
We define d′m := |πm|−1 = dim(Fm)−1 as the clustering dimension.,3. Preliminaries and notation,[0],[0]
Even though this choice might create some confusion it will be consistent the notations used in the proofs of sections 5 and 6.,3. Preliminaries and notation,[0],[0]
Also we define d′′m = |πm|0 := |∪ d′m+1 k=1,3. Preliminaries and notation,[0],[0]
"[k]| as the change point dimension.
",3. Preliminaries and notation,[0],[0]
"To link partitions to subspaces let el := (0, . . .",3. Preliminaries and notation,[0],[0]
", 1, . . .",3. Preliminaries and notation,[0],[0]
", 0) be the lth-component of the standard orthonormal basis of RN , and define for a subset A of J1, NK the vector 1A :=∑ l∈A el.",3. Preliminaries and notation,[0],[0]
"For [k], the k
th cluster of πm, with a slight abuse of notation we define 1[k] := ∑|[k]| i=1 eki , and observe that Fm = span{1[k] : k ∈ πm}, which is consistent with the definition of the clustering dimension d′m := |πm| − 1 = dim(Fm)− 1.
We define 〈f∗〉 := span{f∗}, S1 ⊕ S2 as the direct sum of the two vector space S1 and S2, and S1 S2 as their direct difference.",3. Preliminaries and notation,[0],[0]
"PS denotes the (orthogonal) projection operator onto the subspace S. We also define the partitions inclusion as m1 ⊂ m2 if Fm1 ⊂ Fm2 , or equivalently if
πm2 is finer than πm1 .",3. Preliminaries and notation,[0],[0]
Example 1.,3. Preliminaries and notation,[0],[0]
"Consider the signal f∗ of Figure 1, whose partition is
π = {[1]; [2];",3. Preliminaries and notation,[0],[0]
"[3]; [4]; [5]},
where
[1] = J615, 678K ∪ J821, 926K ∪ J1019, 1211K ∪ J1753, 2000K [2] = J1, 100K ∪ J679, 820K ∪ J1212, 1280K [3] = J101, 214K ∪ J505, 614K ∪ J926, 1018K ∪ J1281, 1600K [4] = J215, 504K [5] = J1601, 1752K.
Hence, d′π = 4 and d ′′",3. Preliminaries and notation,[0],[0]
"π = 12 for this signal.
",3. Preliminaries and notation,[0],[0]
"We also denote by CNk the binomial coefficient that gives the number of ways, disregarding order, that k objects can be chosen from among N objects.",3. Preliminaries and notation,[0],[0]
"This is given by
CNk := N !
k!(N − k)! .",3. Preliminaries and notation,[0],[0]
"(2)
The Stirling numbers of the second kind, S(N, k), correspond to the number of ways to partition a set of N objects into k non-empty subsets, or, similarly, to the number of different equivalence relations with precisely k equivalence classes that can be defined on an set of N elements.
",3. Preliminaries and notation,[0],[0]
"We are precisely interested in the case where the element set is J1, NK and the distance between every two elements in each equivalence class is at least 2; we denote the number of such equivalent classes by S2(N, k).",3. Preliminaries and notation,[0],[0]
"S(N, k) and S2(N, k) satisfy the following recurrence relations:
S(N, k) = S(N − 1, k − 1) + kS(N",3. Preliminaries and notation,[0],[0]
"− 1, k), N > k, S2(N, k) = S(N",3. Preliminaries and notation,[0],[0]
"− 1, k − 1), N, k > 2. (3)
",3. Preliminaries and notation,[0],[0]
"For the proofs of these results, we refer the reader to (Graham et al., 1988) and (Mohr & Porter, 2009).",3. Preliminaries and notation,[0],[0]
"To solve the change point and clustering problem, a natural approach is to consider the minimization of a criterion of the form,
Crit(m) = ‖y",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
− f̂m‖22 + σ2K pen(m).,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"(4)
Uniqueness, continuity and stability properties of similar criterion have been studied in (O. et al.), we restrict to a penalty term pen(d′m, d ′′ m) := pen(m) depending only on d′ and d′′ and a multiplicative tuning parameter K. Indeed, as we shall see the penalty can be chosen such that the minimizer f̂m̂ of (4) behaves like an approximation to a maximum a-posteriori estimator (MAP), and also, the average expected risk 1NE[‖f̂m̂−",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"f
∗‖22]→ 0",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"for a large class of signals f∗, namely, those corresponding to models with d′ 6 d′′ = o(N/ lnN), i.e., f∗ is a consistent estimator for those signals.",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"The specific form of pen(m) will be derived in the next section, based on an oracle inequality that will guarantee consistency and adaptivity of our estimator.
",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"Although the estimator f̂m̂ enjoys good statistical properties, from a computational stand it would involve the exploration ofM.",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"The setM is identified with the collection of all the partitions of J1, NK, whose number asymptotically behaves like O(NeN/ lnN), rendering the minimization of the criterion (4) computationally challenging.",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"A way to bypass this issue for the change point only detection problem is via dynamic programming (Harchaoui & Cappé, 2007); this approach works in this simplified setup since there is a natural ordering for exploring the subproblems, which does not hold here.",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"To overcome this, we will relax the criterion in such a way to create a subproblem ordering and thus derive a computationally feasible approximation.",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"The proposed new method is outlined in Algorithm 1.
",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"Let ȳ[k] := ( ∑ i∈[k] Yi)/|[k]|, the average of the elements of Y in the [k]-th part.",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"Notice that, given πm := {[1]; [2]; . . .",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"; [d′m − 1]}, we have
PFm Y = d′m−1∑ k=1 〈Y,1[k]〉 ‖1[k]‖2 1[k] = d′m−1∑ k=1 ȳ[k]1[k].
",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"The minimization of criterion (4) can then be equivalently written as
min m∈M Crit(m)
=",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"min m∈M
{‖y",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"− PFm Y ‖22 + σ2K pen(d′m, d′′m)}
= min 06d′
6d′′6D  min|m|=d′ |m|0=d′′ ‖y",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"− PFm Y ‖22 + σ2K pen(d′, d′′)  ,
Algorithm 1 Two-Pass Dynamic Programming Algorithm input data points (yi)Ni=1, maximum number of changes D and
penalty strength K. 1:
ȳ[k,l] :=
∑l i=k Yi k",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"− l + 1
R[k,l] := l∑",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"i=k (yi − ȳ[k,l])2, 1 6 k 6 l 6 N.
2: for d = 1 to D do 3: use the dynamic programming recurrence in (9) and a
backtracking step to compute
Cd(N) := min |m̄|=d
‖Y − PFm̄ Y ‖ 2, (5)
m̃d ∈ arg min |m̄|=d ‖Y − PFm̄ Y ‖ 2.
4: end for 5: for d = 1 to D do 6: m̃d =: {0 6 i1 < i2 < · · ·",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"< id < N}
(αk) d k=0 := (ik+1 − ik)d0, (i0 = 0, id+1 = N).
7: sort (ȳ[1,i1], ȳ[i1+1,i2], . . .",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
", ȳ[id+1,N ]).",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
8: ( ȳ(k) ),4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"d k=0 := ordered sequence of (ȳ[ik+1,ik+1])",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"d k=0
(α(k))",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
d k=0 := corresponding permuted (αk) d k=0,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"according to permutation φd.
9: ȳ(k,l) := ∑l i=k α(i)ȳ(i)∑l−1 i=k α(i) and R̄[k,l] := ∑l i=k α(i)(ȳ(i) −
ȳ(k,l)) 2, 1 6 k 6 l 6 d.
10: for δ = 1 to d do 11: use the dynamic programming recurrence in (10) and a
backtracking step to compute
G(d,δ) := min m∈Mȳm̃,δ
‖PFm̄ Y − PFm PFm̄ Y ‖ 2, (6)
˜̃m(d,δ) ∈ arg min m∈Mȳm̃δ ‖PFm̄",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"Y − PFm PFm̄ Y ‖ 2.
12: end for 13: end for 14: B(d,δ) := Cd+G(d,δ) +σ2K pen((d, δ)), 1 6 δ 6 d 6 D. 15: (d̂, δ̂) := arg min
16δ6d6D B(d,δ).
16: reconstruct m(d̂,δ̂) from m̃d̂ and ˜̃m(d̂,δ̂).",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"output value of criterion Crit(m(d̂,δ̂))",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"= B(d̂,δ̂) and selected
model for change points and clusters m(d̂,δ̂).
",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
where D is a reasonable upper bound on the number of change points.,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"As we shall see later, from a statistical point of view there is no need to explore all possible values of d′ and d′′, since the statistical guarantees only hold in a regime where d′ 6 d′′ = o(N/ lnN).
",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"We define πm̄ to be the partition having as elements all the segments of πm and instead of computing the minimum exactly we will take a greedy step by defining
m̃ := arg min |m̄|=d′′
‖Y − PFm̄ Y ‖2
and defining Mm̃,d′ := {m ∈ M : m ⊂ m̃, |m| = d′}, which can be identified with the collection of all partitions of J1, d′′K into d′ sets.",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"We restrict further this collection to partitions π satisfying what we call the clustering property, which states that if I1, I2 and I are segments in some (possibly different) parts of π, then
{I1, I2 ∈",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
[k] ȳI1 6,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
ȳI 6 ȳI2 ⇒,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
I ∈,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"[k]. (7)
",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"This sub-collection will be denoted asMȳm̃,d′ .",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"Simply put, this property says that the partitions considered are those that respect the ordering of (ȳ[ik+1,ik+1])",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"d′′
k=0, since if two segments I1, I2 belong to [k], and the segment I satisfies ȳI1 6",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"ȳI 6 ȳI2 , then it should also be in cluster [k].
",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"This leads to the following upper bound, whose detailed derivation is given in appendix B:
min m∈M Crit(m) 6 min 06d′′6D { min |m|=d′′ ‖Y − PFm",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
Y,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"‖2
+ min 06d′6d′′
m∈Mȳm̃,d′′
‖PFm̃ Y",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
− PFm PFm̃,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"Y ‖2
+ σ2K pen(d′, d′′) } .
",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"Therefore, we can define the following relaxation for the minimization of the criterion in (4):
Critr(d ′′) := min |m|=d′′ ‖Y − PFm",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
Y,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"‖2
+ min 06d′6d′′
m∈Mȳm̃,d′′
{ ‖PFm̃ Y",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
− PFm PFm̃,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"Y ‖2
+ σ2K pen(d′m, d ′′ m)
} .",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"(8)
and our algorithm computes min 06d′′6D
Critr(d ′′) and returns
m(d̂,δ̂).",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"From this last definition we observe that
min m∈M Crit(m) 6 Crit(m(d̂,δ̂))",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"= min06d′′6D Critr(d
′′).
",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"Thus, obtainingm(d̂,δ̂) ensures making progress toward the minimization of Crit(m).",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"The Two-Pass Dynamic Programming Algorithm 1 is aimed at doing this by computing the value of the minimum in (8) and returning a solution m̂ = m(d̂,δ̂) in the following way:
Details of Main Steps in Algorithm 1
• Step 3: It computes Cd(n) defined in (9) for all d and n to obtain Cd(N) for all d ∈ J1, NK.",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"It does so by using a dynamic programming algorithm that computes recursively for all 2 6 d 6 D and d 6 n 6 N the following recurrence, similar to the one in Hawkins (1976):
C1(n)",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
":= R[1,n] (9)
Cd(n) := min i∈Jd,nK
{Cd−1(i− 1) +R[i,n]}, d > 2.
•",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"Step 7: For all values of d, it sorts the obtained segments according to their levels to yield ( ȳ(k) )",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"d 0 , and it keeps track
of the segments’ sizes as (αk)dk=0 = (ik+1 − ik)d0 .
",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
• Step 11: It runs a modified dynamic programming recurrence on ( ȳ(k) ),4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"d 0
that uses weights according to the sizes (α(k)) d 0 .",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"It does so using the following recurrence for all 1 6 δ 6 t 6 d:
G(t,1)",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
":= R̄[1,t], (10)
G(t,δ) := min i∈Jδ,tK
{G(i−1,δ−1) + R̄[i,t]}, δ > 2.
• Step 15: It computes the minimum in (8) and finds for which model it is attained by solving the minimization problem:
(d̂, δ̂) := arg min 16δ6d6D B(d,δ).
",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"• Step 16: It finally reconstructs m(d̂,δ̂) from m̃d̂ and ˜̃m(d̂,δ̂) using the permutation φ(d̂).
",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"This algorithm can be thought of as an efficient way to compute the relaxation in (8), based on solving the change point detection problem in (5) using the dynamic programming recurrence of (9), followed by a solving a clustering problem in (6) using the dynamic programming recurrence of (10).
",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
The next theorem shows that Algorithm 1 correctly solves the minimization problem in (8) and explicits its time and space complexity.,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
Theorem 4.1.,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"Let (yi)Ni=1 ⊂ R, D ∈ N and K > 0.",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"Then,
• for all 1 6 d 6 D,
m̃d ∈ arg min |m̄|=d ‖Y − PFm̄ Y ‖2,
• for all 1 6 δ 6 d 6 D,
˜̃m(d,δ) ∈ arg",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
min,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
m∈Mȳm̃δ ‖PFm̄,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
Y − PFm PFm̄,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"Y ‖2.
",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"Furthermore, Algorithm 1 correctly solves the minimization problem in (8), with time and space complexity O(N3 +D4) and O(N2 +D3), respectively.
",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
Proof.,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"See Appendix B.
The time and space complexity can be improved to O(N2D",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
+,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"D4) and O(DN +D3), respectively.",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
We refer the reader to the discussion after the proof in Appendix B for the derivation of this result.,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
In this way we obtain a computationally feasible algorithm that finds the minimum in (8) and returns an approximation to the criterion in (4).,4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"In the next section, we will motivate the use of Algorithm 1 from a statistical point of view by showing that the minimization of criterion (4) can be viewed as an approximate maximum a-posteriori estimator.",4. Two-pass dynamic programming for change point detection and clustering,[0],[0]
"In this part, we provide a derivation of the optimization criterion in (4).",5. Model selection criterion for change point detection and clustering,[0],[0]
"We start by proposing a Bayesian model selection scheme, which is later inverted to arrive at an integral form of the maximum a-posteriori probability (MAP) estimator.",5. Model selection criterion for change point detection and clustering,[0],[0]
"Then we use a Laplace approximation to derive turn the MAP into an optimization problem of the desired form.
",5. Model selection criterion for change point detection and clustering,[0],[0]
Here we show that the proposed selection criterion in (4) follows naturally from a Bayesian reasoning.,5. Model selection criterion for change point detection and clustering,[0],[0]
"For this, we model the data as being the outcome of the following sampling model.",5. Model selection criterion for change point detection and clustering,[0],[0]
The observation Y is generated from a multivariate Gaussian of mean F and variance σ2IN as described by (1).,5. Model selection criterion for change point detection and clustering,[0],[0]
"For the random variable F , given that it belongs to a subspace Fm, we choose an absolutely continuous measure Ld′m with respect to λd′m , the Lebesgue measure on Rd′m+1, such that dLd′m = lf/mdλd ′",5. Model selection criterion for change point detection and clustering,[0],[0]
"m+1 =
d′m+1∏ k=1 (lfk/mdλ) with lf1/m = · · · = lfd′m+1/m. Later we will see that the choice lf/m will not matter in comparison to the order of approximation, nevertheless we would like it to be a bounded continuous prior satisfying some additional conditions given in Lemma 2, even though we might be chosen as an improper prior.",5. Model selection criterion for change point detection and clustering,[0],[0]
"On the family of models M we impose a categorical distribution measure PM as prior, with a weight pm for model m. Thus, we obtain the following sampling model for the data1:
Y/F ∼ N (F, σ2IN ) F/m ∼ Ld ′ m (11)
m ∼ PM = Categorical((pm)m∈M).
",5. Model selection criterion for change point detection and clustering,[0],[0]
"Since Y , F and m are now random variables, it makes sense to compute µm/Y , the posterior distribution of m
1Here and in the sequel, the dependence of pm and PM on the number of samples N is omitted, for simplicity of notation.
",5. Model selection criterion for change point detection and clustering,[0],[0]
"given Y , and maximize it, to arrive at a MAP estimate of m given bellow.
pm/",5. Model selection criterion for change point detection and clustering,[0],[0]
"Y =
pm ∫ f∈Fm φN",5. Model selection criterion for change point detection and clustering,[0],[0]
"( Y − f σ ) lf/m(f)df
∑ m′∈M pm′ ∫ f ′∈Fm φN",5. Model selection criterion for change point detection and clustering,[0],[0]
"( Y − f ′ σ ) lf/m′(f ′)df ′ .
(12)
",5. Model selection criterion for change point detection and clustering,[0],[0]
"For the complete derivation of the formula in 12 we refer you to appendix B.
Starting from the a-posteriori distribution (12) we can derive an approximation for the MAP as follows:
pm/Y ∝ pm ∫",5. Model selection criterion for change point detection and clustering,[0],[0]
f∈Fm φN,5. Model selection criterion for change point detection and clustering,[0],[0]
"( Y − f σ ) lf/m(f)df
= pm",5. Model selection criterion for change point detection and clustering,[0],[0]
"d′m+1∏ k=1
1
(2πσ2) |[k]| 2
(13)
· ∫ R exp ( − ‖y[k]",5. Model selection criterion for change point detection and clustering,[0],[0]
− fk1[k]‖22 2σ2 ),5. Model selection criterion for change point detection and clustering,[0],[0]
"lfk/m(fk)dfk.
",5. Model selection criterion for change point detection and clustering,[0],[0]
In the last step of (13) we define y[k] as the vector obtained from the entries of y corresponding to cluster [k].,5. Model selection criterion for change point detection and clustering,[0],[0]
To obtain an approximation of the MAP estimate as a solution of a criterion of the form (4) we need the result of lemma 2 stated and proved in Appendix C using a Laplace approximation type of argument.,5. Model selection criterion for change point detection and clustering,[0],[0]
"We then obtain the following upper bound for the MAP for all K > 1 :
CritMAP(m) 6 ‖y",5. Model selection criterion for change point detection and clustering,[0],[0]
"− PFm y‖22
2σ2
+K ( ln 1
pm +
1 2 (d′m + 1) ln N
d′m
) +O(d′m).",5. Model selection criterion for change point detection and clustering,[0],[0]
"(14)
The complete derivation of (14) can be found in Appendix C.",5. Model selection criterion for change point detection and clustering,[0],[0]
"Now we define our approximate MAP criterion as:
Crit(m) = ‖y",5. Model selection criterion for change point detection and clustering,[0],[0]
"− PFm y‖22 + σ2K pen(m),
pen(m) =",5. Model selection criterion for change point detection and clustering,[0],[0]
"( 2 ln 1
pm + (d′m + 1)",5. Model selection criterion for change point detection and clustering,[0],[0]
"ln
N
d′m
) .",5. Model selection criterion for change point detection and clustering,[0],[0]
"(15)
In the next section, we finish the specification of the penalty term by providing the probabilities pm over the space of models.",5. Model selection criterion for change point detection and clustering,[0],[0]
"To do so we will exhibit an oracle inequality satisfied by the estimator that minimizes (4), and choose a probability mass function (pm) that gives a reasonable upper bound on the expected quadratic risk defined below.",5. Model selection criterion for change point detection and clustering,[0],[0]
The standard way of assessing the performance of a statistical algorithm is by comparing its performance to a reasonable oracle.,6. Oracle inequality and upper bound for the risk,[0],[0]
"For this we use as a measure of performance
of an estimator f̂ the expected quadratic risk:
Rn(f̂) =",6. Oracle inequality and upper bound for the risk,[0],[0]
E[‖f̂,6. Oracle inequality and upper bound for the risk,[0],[0]
"− f∗‖22].
In the case of the change point detection and clustering problem, the comparison should be non-asymptotic, reflecting our lack of knowledge about both the clustering dimension and the change point dimension.",6. Oracle inequality and upper bound for the risk,[0],[0]
"For this we state below a non-asymptotic oracle inequality for Crit(m) using an oracle with remainder of the form:
inf m∈M {Rn(PFm y) + om(1)}.
",6. Oracle inequality and upper bound for the risk,[0],[0]
"This type of oracle has access to f∗ and chooses the m that minimizes the risk criterion up to a remainder term.
",6. Oracle inequality and upper bound for the risk,[0],[0]
To derive this we finish the specification of Crit(m) by providing an appropriate prior pm.,6. Oracle inequality and upper bound for the risk,[0],[0]
The intuition behind our choice is the following.,6. Oracle inequality and upper bound for the risk,[0],[0]
Defining r̂m = ‖y,6. Oracle inequality and upper bound for the risk,[0],[0]
− f̂m‖22 and pen(m) = 2σ2,6. Oracle inequality and upper bound for the risk,[0],[0]
ln 1pm + σ 2(d′m + 1),6. Oracle inequality and upper bound for the risk,[0],[0]
"ln N d′m
we see that the criterion (15) is of the form:
Crit(m)",6. Oracle inequality and upper bound for the risk,[0],[0]
"= r̂m + pen(m).
",6. Oracle inequality and upper bound for the risk,[0],[0]
The number of models in the family M having the same values of d′m and d ′′ m grows exponentially with those dimensions.,6. Oracle inequality and upper bound for the risk,[0],[0]
"Thus for fix d′m and d ′′ m we might find a model with low r̂m just because of randomness since some of them will deviate largely from their means, which would correspond to an over-fitting case, this was the problem of case with the traditional AIC type of estimators.",6. Oracle inequality and upper bound for the risk,[0],[0]
"Therefore, we need to penalize models of high dimensions more by taking into account the number of models with same dimensions.",6. Oracle inequality and upper bound for the risk,[0],[0]
On the other hand we want this penalty to be as small as possible this way we give more importance to the fitting term r̂m.,6. Oracle inequality and upper bound for the risk,[0],[0]
"In particular we would prefer the term 2σ2 ln 1pm to stay close to σ
2(d′+1)",6. Oracle inequality and upper bound for the risk,[0],[0]
ln Nd′ at least for values of d′m close to d ′′ m.,6. Oracle inequality and upper bound for the risk,[0],[0]
"Our choice for pm, useful inequalities and a complete discussion of the role of pm as a prior and tuning parameter for the risk can be found in Appendix D. From Lemmas 3 and 4, the following oracle inequality can be derived for f̂m̂:
Theorem 6.1 (Oracle inequality for f̂m̂).",6. Oracle inequality and upper bound for the risk,[0],[0]
"With M restricted to models such that ed′m 6 N and for the choice of K = 3a, pm as in 3, pen(m) as in 15 and m̂ ∈ M corresponding to
m̂ ∈ arg min",6. Oracle inequality and upper bound for the risk,[0],[0]
m∈M ‖y,6. Oracle inequality and upper bound for the risk,[0],[0]
"− f̂m‖22 + σ2K pen(m), (16)
We obtain for all a > 1,
Ef∗",6. Oracle inequality and upper bound for the risk,[0],[0]
"[‖PFm̂ Y − f∗‖2] 6
arg min m∈M
{ a
a− 1 Ef∗",6. Oracle inequality and upper bound for the risk,[0],[0]
"[‖PFm Y − f∗‖2]
+ a2σ2
a− 1
( 7 + 3(d′m + 1) ln N
d′m + 6 ln
1
pm
)} .",6. Oracle inequality and upper bound for the risk,[0],[0]
"(17)
Proof.",6. Oracle inequality and upper bound for the risk,[0],[0]
"See Appendix D.
By investigating the oracle inequality, one notices that for an optimal choice of a one has to make a trade-off between the performance of the oracle part and the bias part of the inequality.",6. Oracle inequality and upper bound for the risk,[0],[0]
In general this trade-off is not possible to optimize since the value of the oracle part is not available to us and depends on the variance of the noise.,6. Oracle inequality and upper bound for the risk,[0],[0]
"In practice, one can use the SLOPE heuristic introduced in Lebarbier (2002) and described in Baudry et al. (2012) and in (Arlot & Massart, 2009).",6. Oracle inequality and upper bound for the risk,[0],[0]
"In our case, the value of the tuning parameter can be chosen independently of the variance of the noise and we can use the value of a for which we know that our estimator f̂m̂ will perform well.
",6. Oracle inequality and upper bound for the risk,[0],[0]
Corollary 6.1.,6. Oracle inequality and upper bound for the risk,[0],[0]
"For the set of models described in 6.1 with f∗ ∈ Fm∗ the following properties hold:
• Adaptation and Risk Upper bound: The following adaptive upper bound in terms of d′m∗ and d ′′ m∗ holds
for a = 2:
Ef∗",6. Oracle inequality and upper bound for the risk,[0],[0]
"[‖PFm̂ Y − f ∗‖2] 6 4σ2 ( 7 + 3(d′m∗ + 1) ln N
d′m∗
+ 6 ( d′m∗ ln[d ′′ m∗e 13 6 ] + d′′m∗ ln[d ′ m∗e 2] + d′′m∗ ln N
d′′m∗
)) .
",6. Oracle inequality and upper bound for the risk,[0],[0]
• Consistency:,6. Oracle inequality and upper bound for the risk,[0],[0]
"If d′′m∗ = o(N/ lnN), then limN→∞N −1Ef∗",6. Oracle inequality and upper bound for the risk,[0],[0]
"[‖f̂m̂ − f∗‖2] = 0.
",6. Oracle inequality and upper bound for the risk,[0],[0]
Proof.,6. Oracle inequality and upper bound for the risk,[0],[0]
"See Appendix D.
We notice that the consistency condition d′′m∗ = o(N/ lnN) is within the restriction on the models in theorem 6.1, hence there is no loss of generality of having only models with ed′m 6 N in M since for other models we cannot guarantee convergent mean square risk anyway.",6. Oracle inequality and upper bound for the risk,[0],[0]
"In the special case d′m∗ = d ′′ m∗ , i.e when the change point and clustering problem reduces to a change point only problem, Kernel methods have comparable accuracy (Celisse et al., 2017).",6. Oracle inequality and upper bound for the risk,[0],[0]
"The interesting case is when the numbers are different, we gain a logarithmic factor in accuracy with almost the same computational cost.",6. Oracle inequality and upper bound for the risk,[0],[0]
"In the next section, we validate these theoretical guarantees by a series of tests on simulated data to get a sense of how tight the oracle inequality is, which signals are difficult to estimate and how the algorithm behaves in practice.",6. Oracle inequality and upper bound for the risk,[0],[0]
Consider first an experiment based data generated randomly according to the setup of (1) with the same change points of Example 1.,7. Experimental results,[0],[0]
"This is considered to be an easy case since d′m∗ = 4 < d ′′ m∗ = 12 N = 2000, which is within the range of signals for which the consistency result of Corollary 6.1 holds.
",7. Experimental results,[0],[0]
"The experiments in Figure 2 show that the algorithm is quite robust to the level of noise as measured by the signalto-noise ratio S/N = magnitude of smallest jump in f ∗
σ2 .",7. Experimental results,[0],[0]
"We observe that the difference between the ground truth f∗ and f̂m̂ is quite small even for small S/N levels such as S/N = 0.5 and the change point locations do not vary appreciably; in fact, for this experiment, S/N = 0.3 seems to be the limiting case for which the algorithm performs well, and for lower values the risk upper-bound in Corollary 6.1 becomes loose when σ increases.",7. Experimental results,[0],[0]
"Also, we note that an S/N of 0.5 is quite low for this kind of problems.",7. Experimental results,[0],[0]
"In particular, algorithms relying on the L1-penalty such as Fussed LASSO do not achieve this kind of performance on the simpler task of change point only detection, while on the other hand, they are more computational efficient (Xin et al., 2014).
",7. Experimental results,[0],[0]
"Figure 3 illustrates a difficult case, where we reduced the number of observation by segment by scaling down the signal f∗ to a support of size N = 500.",7. Experimental results,[0],[0]
"Now we are outside of the useful regime of Corollary 6.1 and we notice that the second segment J15, 53K is wider than what it should since the first change point at 25 was detected at 14; also the segment J206, 237K belongs to cluster [4] while it is actually in cluster",7. Experimental results,[0],[0]
[3] in the original signal f∗.,7. Experimental results,[0],[0]
"Nevertheless we can observe an interesting property for segment J324, 346K, namely, that the end point 346 does not correspond to any real change point, yet this segment belongs to the optimal solution of the 1st dynamic programming pass.",7. Experimental results,[0],[0]
"On the other hand the 2nd dynamic programming pass puts it in the same cluster [3] as J347, 399K, turning them into one single segment of cluster [3].",7. Experimental results,[0],[0]
"This behavior actually is the norm for the algorithm, where false changes are often detected in difficult signals in the 1st
dynamic programming pass but are removed after the 2nd pass.",7. Experimental results,[0],[0]
"These kinds of false discoveries are actually one of the weaknesses of many change point only detection algorithms like Fussed LASSO, and they have been studied in (Levy-leduc & Harchaoui, 2008), (Rinaldo, 2009) and (Rojas & Wahlberg, 2014).",7. Experimental results,[0],[0]
"In the last experiment,
we run Algorithm 1 300 times with the parameter values d′m∗ = 4 < d ′′ m∗ = 12 N = 2000 and signal-to-noise ratio S/N = 1; Figure 4 summarizes the results.",7. Experimental results,[0],[0]
"In the top histogram we notice that the algorithm successfully detects the change points most of the time; in fact, the achieved accuracy was number of change points correctly detectednumber of change points detected ≈ 0.8528.",7. Experimental results,[0],[0]
"The
middle histogram shows the placement of estimated clusters and the true values of the clusters; we observe that the true values lie in a small neighborhood of the estimated values for every cluster.",7. Experimental results,[0],[0]
"In the bottom histogram we observe that the theoretical upper bound on the average mean square error –in this case 12.1575– found in Corollary 6.1 is very conservative and most of the 300 estimates –given by ‖f̂m̂−f
∗‖2 N – are significantly smaller.",7. Experimental results,[0],[0]
"In this work, we considered a novel problem related to change point detection where we have to address the simultaneous task of segmenting and clustering the observed signal.",8. Conclusions,[0],[0]
Our approach has been to view this problem as a non-parametric model selection problem on the set of all possible partitions.,8. Conclusions,[0],[0]
"We derived for this the computationally tractable Algorithm 1, that computes a relaxation of the penalized minimization of criterion (4), and we justified it from a statistical standpoint by showing that this minimization can be viewed as an approximate MAP.",8. Conclusions,[0],[0]
This approximate MAP estimate enjoys the properties of being adaptive and consistent in the sense of Corollary 6.1.,8. Conclusions,[0],[0]
"We finally justified the use of Algorithm 1 by simulation data that shows some useful properties of the resulting estimate and validates the theoretical guarantees.
",8. Conclusions,[0],[0]
"One extension of this work concerns developing a more complete analysis of Algorithm 1, to obtain consistency results on the number and locations of the change points and clusters.",8. Conclusions,[0],[0]
"Another possible extension relates to the use of Algorithm 1 in the non-scalar case; this was already explored for change point only detection in (Arlot et al., 2016) through the use of characteristic kernels (Sriperumbudur et al., 2011).",8. Conclusions,[0],[0]
"We believe that the same approach can be adopted here except that we cannot perform the sorting step; this can be overcome using a Kernel clustering algorithm (Filipponea et al., 2008) or a spectral version of it (Schölkopf et al., 1998) for the second stage.",8. Conclusions,[0],[0]
"Finally, the remark after Figure 3 hints to the possibility of using a combined algorithm starting with the sparse solution of Fussed LASSO and running the 2nd dynamic programming pass of our algorithm as a way to boost the performance of Fussed LASSO to get rid of false discoveries.",8. Conclusions,[0],[0]
"This would be still computationally attractive according to the comment after Theorem 4.1, since the solution of Fussed LASSO has a small number of changes.",8. Conclusions,[0],[0]
We address a generalization of change point detection with the purpose of detecting the change locations and the levels of clusters of a piecewise constant signal.,abstractText,[0],[0]
Our approach is to model it as a nonparametric penalized least square model selection on a family of models indexed over the collection of partitions of the design points and propose a computationally efficient algorithm to approximately solve it.,abstractText,[0],[0]
"Statistically, minimizing such a penalized criterion yields an approximation to the maximum a-posteriori probability (MAP) estimator.",abstractText,[0],[0]
The criterion is then analyzed and an oracle inequality is derived using a Gaussian concentration inequality.,abstractText,[0],[0]
"The oracle inequality is used to derive on one hand conditions for consistency and on the other hand an adaptive upper bound on the expected square risk of the estimator, which statistically motivates our approximation.",abstractText,[0],[0]
"Finally, we apply our algorithm to simulated data to experimentally validate the statistical guarantees and illustrate its behavior.",abstractText,[0],[0]
Bayesian Model Selection for Change Point Detection and Clustering,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1029–1039 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1095",text,[0],[0]
Dictionaries and gazetteers are useful in many natural language processing tasks.,1 Introduction,[0],[0]
These lexical resources may be derived from freely available sources (such as Wikidata and Wiktionary) or constructed for a particular domain.,1 Introduction,[0],[0]
"Lexical resources are typically used to complement existing annotations for a given task (Ando and Zhang, 2005; Collobert et al., 2011).",1 Introduction,[0],[0]
"In this paper, we focus instead on low-resource settings where task annotations are unavailable or scarce.",1 Introduction,[0],[0]
"Specifically, we use lexical resources to guide part-of-speech induction (§4) and to bootstrap named-entity recognizers in low-resource languages (§5).
",1 Introduction,[0],[0]
"Given their success, it is perhaps surprising that incorporating gazetteers or dictionaries into dis-
criminative models (e.g. conditional random fields) may sometimes hurt performance.",1 Introduction,[0],[0]
"This phenomena is called weight under-training, in which lexical features—which detect whether a name is listed in the dictionary or gazetteer—are given excessive weight at the expense of other useful features such as spelling features that would generalize to unlisted names (Smith et al., 2005; Sutton et al., 2006; Smith and Osborne, 2006).",1 Introduction,[0],[0]
"Furthermore, discriminative training with lexical features requires sufficient annotated training data, which poses challenges for the unsupervised and low-resource settings we consider here.
",1 Introduction,[0],[0]
Our observation is that Bayesian modeling provides a principled solution.,1 Introduction,[0],[0]
The lexicon is itself a dataset that was generated by some process.,1 Introduction,[0],[0]
"Practically, this means that lexicon entries (words or phrases) may be treated as additional observations.",1 Introduction,[0],[0]
"As a result, these entries provide information about how names are spelled.",1 Introduction,[0],[0]
"The presence of the lexicon therefore now improves training of the spelling features, rather than competing with the spelling features to help explain the labeled corpus.
",1 Introduction,[0],[0]
A downside is that generative models are typically less feature-rich than their globally normalized discriminative counterparts (e.g. conditional random fields).,1 Introduction,[0],[0]
In designing our approach—the hierarchical sequence memoizer (HSM)—we aim to be reasonably expressive while retaining practically useful inference algorithms.,1 Introduction,[0],[0]
We propose a Bayesian nonparametric model to serve as a generative distribution responsible for both lexicon and corpus data.,1 Introduction,[0],[0]
"The proposed model memoizes previously used lexical entries (words or phrases) but backs off to a character-level distribution when generating novel types (Teh, 2006; Mochihashi et al., 2009).",1 Introduction,[0],[0]
We propose an efficient inference algorithm for the proposed model using particle Gibbs sampling (§3).,1 Introduction,[0],[0]
"Our code is available at https://github.com/noa/bayesner.
1029",1 Introduction,[0],[0]
Our goal is to fit a model that can automatically annotate text.,2 Model,[0],[0]
We observe a supervised or unsupervised training corpus.,2 Model,[0],[0]
"For each label y in the annotation scheme, we also observe a lexicon of strings of type y.",2 Model,[0],[0]
"For example, in our tagging task (§4), a dictionary provides us with a list of words for each part-of-speech tag y.",2 Model,[0],[0]
(These lists need not be disjoint.),2 Model,[0],[0]
"For named-entity recognition (NER, §5), we use a list of words or phrases for each named-entity type y (PER, LOC, ORG, etc.).1",2 Model,[0],[0]
"We may treat the lexicon for type y, of size my, as having been produced by a set of my IID draws from an unknown distribution Py over the words or named entities of type y.",2.1 Modeling the lexicon,[0],[0]
It therefore provides some evidence about Py.,2.1 Modeling the lexicon,[0],[0]
We will later assume that Py is also used when generating mentions of these words or entities in text.,2.1 Modeling the lexicon,[0],[0]
"Thanks to this sharing of Py, if x = Washington is listed in the gazetteer of locations (y = LOC), we can draw the same conclusions as if we had seen a LOC-labeled instance of Washington in a supervised corpus.
",2.1 Modeling the lexicon,[0],[0]
"Generalizing this a bit, we may suppose that one observation of string x in the lexicon is equivalent to c labeled tokens of x in a corpus, where the constant c > 0 is known as a pseudocount.",2.1 Modeling the lexicon,[0],[0]
"In other words, observing a lexicon of my distinct types {x1, . .",2.1 Modeling the lexicon,[0],[0]
.,2.1 Modeling the lexicon,[0],[0]
", xmy} is equivalent to observing a labeled pseudocorpus of cmy tokens.",2.1 Modeling the lexicon,[0],[0]
"Notice that given such an observation, the prior probability of any candidate distribution Py is reweighted by the likelihood (cmy)!(c!)my · (Py(x1)Py(x2) · · ·Py(xmy))c.",2.1 Modeling the lexicon,[0],[0]
"Therefore, this choice of Py can have relatively high posterior probability only to the extent that it assigns high probability to all of the lexicon types.",2.1 Modeling the lexicon,[0],[0]
"We employ the above model because it has reasonable qualitative behavior and because computationally, it allows us to condition on observed lexicons as easily as we condition on observed corpora.",2.2 Discussion,[0],[0]
"However, we caution that as a generative model of the lexicon, it is deficient, in the sense that it
1Dictionaries and knowledge bases provide more information than we use in this paper.",2.2 Discussion,[0],[0]
"For instance, Wikidata also provides a wealth of attributes and other metadata for each entity s. In principle, this additional information could also be helpful in estimating Py(s); we leave this intriguing possibility for future work.
",2.2 Discussion,[0],[0]
allocates probability mass to events that cannot actually correspond to any lexicon.,2.2 Discussion,[0],[0]
"After all, drawing cmy IID tokens from Py is highly unlikely to result in exactly c tokens of each of my different types, and yet a run of our system will always assume that precisely this happened to produce each observed lexicon!",2.2 Discussion,[0],[0]
"To avoid the deficiency, one could assume that the lexicon was generated by rejection sampling: that is, the gazetteer author repeatedly drew samples of size cmy from Py until one was obtained that had this property, and then returned the set of distinct types in that sample as the lexicon for y.",2.2 Discussion,[0],[0]
But this is hardly a realistic description of how gazetteers are actually constructed.,2.2 Discussion,[0],[0]
"Rather, one imagines that the gazetteer author simply harvested a lexicon of frequent types from Py or from a corpus of tokens generated from Py.",2.2 Discussion,[0],[0]
"For example, a much better generative story is that the lexicon was constructed as the first my distinct types to appear ≥ c times in an unbounded sequence of IID draws from Py.",2.2 Discussion,[0],[0]
"When c = 1, this is equivalent to modeling the lexicon as my draws without replacement from Py.2 Unfortunately, draws without replacement are no longer IID or exchangeable: order matters.",2.2 Discussion,[0],[0]
"It would therefore become difficult to condition inference and learning on an observed lexicon, because we would need to explicitly sum or sample over the possibilities for the latent sequence of tokens (or stick segments).",2.2 Discussion,[0],[0]
"We therefore adopt the simpler deficient model.
",2.2 Discussion,[0],[0]
"A version of our lexicon model (with c = 1) was previously used by Dreyer and Eisner (2011, Appendix C), who observed a list of verb paradigm types rather than word or entity-name types.",2.2 Discussion,[0],[0]
"We assume a priori that Py was drawn from a Pitman-Yor process (PYP) (Pitman and Yor, 1997).",2.3 Prior distribution over Py,[0],[0]
Both the lexicon and the ordinary corpus are observations that provide information about Py.,2.3 Prior distribution over Py,[0],[0]
"The PYP is defined by three parameters: a concentration parameter α, a discount parameter d, and a base distribution Hy.",2.3 Prior distribution over Py,[0],[0]
"In our case, Hy is a distribution over X = Σ∗, the set of possible strings over a finite character alphabet",2.3 Prior distribution over Py,[0],[0]
"Σ.
For example, HLOC is used to choose new place names, so it describes what place names tend to
2If we assume that Py was drawn from a Pitman-Yor process prior (as in §2.3) using the stick-breaking method (Pitman, 1996), it is also equivalent to modeling the lexicon as the set of labels of the first my stick segments (which tend to have high probability).
look like in the language.",2.3 Prior distribution over Py,[0],[0]
"The draw PLOC ∼ PYP(d, α,HLOC) is an “adapted” version of HLOC.",2.3 Prior distribution over Py,[0],[0]
It is PLOC that determines how often each name is mentioned in text (and whether it is mentioned in the lexicon).,2.3 Prior distribution over Py,[0],[0]
"Some names such as Washington that are merely plausible under HLOC are far more frequent under PLOC, presumably because they were chosen as the names of actual, significant places.",2.3 Prior distribution over Py,[0],[0]
"These place names were randomly drawn from HLOC as part of the procedure for drawing Py.
",2.3 Prior distribution over Py,[0],[0]
"The expected value of Py is H (i.e., H is the mean of the PYP distribution), but if α and d are small, then a typical draw of Py will be rather different from H , with much of the probability mass falling on a subset of the strings.
",2.3 Prior distribution over Py,[0],[0]
"At training or test time, when deciding whether to label a corpus token of x = Washington as a place or person, we will be interested in the relative values of PLOC(x) and PPER(x).",2.3 Prior distribution over Py,[0],[0]
"In practice, we do not have to represent the unknown infinite object Py, but can integrate over its possible values.",2.3 Prior distribution over Py,[0],[0]
"When Py ∼ PYP(d, α,Hy), then a sequence of draws X1, X2, . . .",2.3 Prior distribution over Py,[0],[0]
"∼ Py is distributed according to a Chinese restaurant process, via
Py(Xi+1 = x",2.3 Prior distribution over Py,[0],[0]
"| X1, . . .",2.3 Prior distribution over Py,[0],[0]
", Xi) (1)
",2.3 Prior distribution over Py,[0],[0]
"= customers(x)− d · tables(x)
α+",2.3 Prior distribution over Py,[0],[0]
"i
+ α+ d ·∑x′ tables(x′)
α+ i Hy(x)
where customers(x) ≤ i is the number of times that x appeared among X1, . . .",2.3 Prior distribution over Py,[0],[0]
", Xi, and tables(x) ≤ customers(x) is the number of those times that x was drawn from Hy (where each Py(Xi | · · · ) defined by (1) is interpreted as a mixture distribution that sometimes uses Hy).",2.3 Prior distribution over Py,[0],[0]
"By fitting Hy on corpus and lexicon data, we learn what place names or noun strings tend to look like in the language.",2.4 Form of the base distribution Hy,[0],[0]
"By simultaneously fitting Py, we learn which ones are commonly mentioned.",2.4 Form of the base distribution Hy,[0],[0]
"Recall that under our model, tokens are drawn from Py but the underlying types are drawn fromHy, e.g.,Hy is responsible for (at least) the first token of each type.
",2.4 Form of the base distribution Hy,[0],[0]
"A simple choice for Hy is a Markov process that emits characters in Σ ∪ {$}, where $ is a distinguished stop symbol that indicates the end of the string.",2.4 Form of the base distribution Hy,[0],[0]
"Thus, the probability of producing $ controls the typical string length under Hy.
We use a more sophisticated model of strings—a sequence memoizer (SM), which is a (hierarchical) Bayesian treatment of variable-order Markov modeling (Wood et al., 2009).",2.4 Form of the base distribution Hy,[0],[0]
"The SM allows dependence on an unbounded history, and the probability of a given sequence (string) can be found efficiently much as in equation (1).
",2.4 Form of the base distribution Hy,[0],[0]
"Given a string x = a1 · · · aJ ∈ Σ∗, the SM assigns a probability to it via
Hy(a1:J) =",2.4 Form of the base distribution Hy,[0],[0]
"( J∏
j=1
Hy(aj |",2.4 Form of the base distribution Hy,[0],[0]
a1:,2.4 Form of the base distribution Hy,[0],[0]
j−1) ),2.4 Form of the base distribution Hy,[0],[0]
"Hy($ | a1:J)
=",2.4 Form of the base distribution Hy,[0],[0]
"( J∏
j=1
Hy,a1:j−1(aj) )",2.4 Form of the base distribution Hy,[0],[0]
"Hy,a1:J ($) (2)
where Hy,u(a) denotes the conditional probability of character a given the left context u ∈ Σ∗. Each Hy,u is a distribution over Σ, defined recursively as
Hy, ∼ PYP(d , α ,UΣ) (3) Hy,u ∼ PYP(d|u|, α|u|, Hy,σ(u))
where is the empty sequence, UΣ is the uniform distribution over Σ ∪ {$}, and σ(u) drops the first symbol from u.",2.4 Form of the base distribution Hy,[0],[0]
"The discount and concentration parameters (d|u|, α|u|) are associated with the lengths of the contexts |u|, and should generally be larger for longer (more specific) contexts, implying stronger backoff from those contexts.3
Our inference procedure is largely indifferent to the form of Hy, so the SM is not the only option.",2.4 Form of the base distribution Hy,[0],[0]
"It would be possible to inject more assumptions into Hy, for instance via structured priors for morphology or a grammar of name structure.",2.4 Form of the base distribution Hy,[0],[0]
"Another possibility is to use a parametric model such as a neural language model (e.g., Jozefowicz et al. (2016)), although this would require an inner-loop of gradient optimization.",2.4 Form of the base distribution Hy,[0],[0]
We now turn to modeling the corpus.,2.5 Modeling the sequence of tags y,[0],[0]
We assume that each sentence is generated via a sequence of latent labels y = y1:T ∈ Y∗.4,2.5 Modeling the sequence of tags y,[0],[0]
"The observations
3We fix these hyperparameters using the values suggested in (Wood et al., 2009; Gasthaus and Teh, 2010), which we find to be quite robust in practice.",2.5 Modeling the sequence of tags y,[0],[0]
"One could also resample their values (Blunsom and Cohn, 2010); we experimented with this but did not observe any consistent advantage to doing so in our setting.
",2.5 Modeling the sequence of tags y,[0],[0]
"4The label sequence is terminated by a distinguished endof-sequence label, again written as $.
",2.5 Modeling the sequence of tags y,[0],[0]
x1:T are then generated conditioned on the label sequence via the corresponding Py distribution (defined in §2.3).,2.5 Modeling the sequence of tags y,[0],[0]
"All observations with the same label y are drawn from the same Py, and thus this subsequence of observations is distributed according to the Chinese restaurant process (1).
",2.5 Modeling the sequence of tags y,[0],[0]
We model y using another sequence memoizer model.,2.5 Modeling the sequence of tags y,[0],[0]
"This is similar to other hierarchical Bayesian models of latent sequences (Goldwater and Griffiths, 2007; Blunsom and Cohn, 2010), but again, it does not limit the Markov order (the number of preceding labels that are conditioned on).",2.5 Modeling the sequence of tags y,[0],[0]
"Thus, the probability of a sequence of latent types is computed in the same way as the base distribution in §2.4, that is,
p(y1:T ) := ( T∏
t=1
Gy1:t−1(yt) ) Gy1",2.5 Modeling the sequence of tags y,[0],[0]
":T ($) (4)
where Gv(y) denotes the conditional probability of latent label y ∈ Y given the left context v ∈ Y∗.",2.5 Modeling the sequence of tags y,[0],[0]
"Each Gv is a distribution over Y , defined recursively as
G ∼ PYP(d , α ,UY) (5) Gv ∼ PYP(d|v|, α|v|, Gσ(v))
",2.5 Modeling the sequence of tags y,[0],[0]
The probability of transitioning to label yt depends on the assignments of all previous labels y1 . . .,2.5 Modeling the sequence of tags y,[0],[0]
"yt−1.
",2.5 Modeling the sequence of tags y,[0],[0]
"For part-of-speech induction, each label yt is the part-of-speech associated with the corresponding word xt.",2.5 Modeling the sequence of tags y,[0],[0]
"For named-entity recognition, we say that each word token is labeled with a named entity type (LOC, PER, . . . ),5 or with itself if it is not a named entity but rather a “context word.”",2.5 Modeling the sequence of tags y,[0],[0]
"For example, the word token xt",2.5 Modeling the sequence of tags y,[0],[0]
"= Washington could have been emitted from the label yt = LOC, or from yt = PER, or from yt = Washington itself (in which case p(xt | yt) = 1).",2.5 Modeling the sequence of tags y,[0],[0]
"This uses a much larger set of labels Y than in the traditional setup where all context words are emitted from the same latent label type O. Of course, most labels are impossible at most positions (e.g., yt cannot be Washington unless xt = Washington).",2.5 Modeling the sequence of tags y,[0],[0]
This scheme makes our generative model sensitive to specific contexts (which is accomplished in discriminative NER systems by contextual features).,2.5 Modeling the sequence of tags y,[0],[0]
"For example, the SM for y can learn that spoke to PER yesterday is a common 4-gram
5In §3.2, we will generalize this labeling scheme to allow multi-word named entities such as New York.
in the label sequence y, and thus we are more likely to label Washington as a person if x = . .",2.5 Modeling the sequence of tags y,[0],[0]
.spoke,2.5 Modeling the sequence of tags y,[0],[0]
to Washington,2.5 Modeling the sequence of tags y,[0],[0]
yesterday .,2.5 Modeling the sequence of tags y,[0],[0]
.,2.5 Modeling the sequence of tags y,[0],[0]
"..
",2.5 Modeling the sequence of tags y,[0],[0]
"We need one change to make this work, since now Y must include not only the standard NER labels Y ′ = {PER, LOC, ORG, GPE} but also words like Washington.",2.5 Modeling the sequence of tags y,[0],[0]
"Indeed, now Y = Y ′ ∪ Σ∗.",2.5 Modeling the sequence of tags y,[0],[0]
"But no uniform distribution exists over the infinite set Σ∗, so how should we replace the base distribution UY over labels in equation (5)?",2.5 Modeling the sequence of tags y,[0],[0]
"Answer: To draw from the new base distribution, sample y ∼ UY ′ ∪{CONTEXT}.",2.5 Modeling the sequence of tags y,[0],[0]
"If y = CONTEXT, however, then “expand” it by resampling y ∼ HCONTEXT.",2.5 Modeling the sequence of tags y,[0],[0]
"Here HCONTEXT is the base distribution over spellings of context words, and is learned just like the other Hy distributions in §2.4.",2.5 Modeling the sequence of tags y,[0],[0]
"Taking Y to be a random variable, we are interested in the posterior distribution p(Y = y | x) over label sequences y given the emitted word sequence",3.1 Sequential sampler,[0],[0]
x.,3.1 Sequential sampler,[0],[0]
"Our model does not admit an efficient dynamic programming algorithm, owing to the dependencies introduced among the Yt when we marginalize over the unknown G and P distributions that govern transitions and emissions, respectively.",3.1 Sequential sampler,[0],[0]
"In contrast to tagging with a hidden Markov model tagging, the distribution of each label Yt depends on all previous labels y1:t−1, for two reasons: ¬ The transition distribution p(Yt = y | y1:t−1) has unbounded dependence because of the PYP prior (4).  ",3.1 Sequential sampler,[0],[0]
"The emission distribution p(xt | Yt = y) depends on the emissions observed from any earlier tokens of y, because of the Chinese restaurant process (1).",3.1 Sequential sampler,[0],[0]
"When  is the only complication, block Metropolis-Hastings samplers have proven effective (Johnson et al., 2007).",3.1 Sequential sampler,[0],[0]
"However, this approach uses dynamic programming to sample from a proposal distribution efficiently, which ¬ precludes in our case.",3.1 Sequential sampler,[0],[0]
"Instead, we use sequential Monte Carlo (SMC)—sometimes called particle filtering—as a proposal distribution.",3.1 Sequential sampler,[0],[0]
"Particle filtering is typically used in online settings, including word segmentation (Borschinger and Johnson, 2011), to make decisions before all of x has been observed.",3.1 Sequential sampler,[0],[0]
"However, we are interested in the inference (or smoothing) problem that conditions on all of x (Dubbin and Blunsom, 2012; Tripuraneni et al., 2015).
",3.1 Sequential sampler,[0],[0]
"SMC employs a proposal distribution q(y | x)
whose definition decomposes as follows:
q(y1",3.1 Sequential sampler,[0],[0]
| x1),3.1 Sequential sampler,[0],[0]
"T∏
t=2
q(yt | y1:t−1,x1:t) (6)
for T = |x|.",3.1 Sequential sampler,[0],[0]
"To sample a sequence of latent labels, first sample an initial label y1 from q1, then proceed incrementally by sampling yt from qt(· | y1:t−1,x1:t) for t = 2, . . .",3.1 Sequential sampler,[0],[0]
", T .",3.1 Sequential sampler,[0],[0]
"The final sampled sequence y is called a particle, and is given an unnormalized importance weight of w̃ = w̃T · p($",3.1 Sequential sampler,[0],[0]
"| y1:T ) where w̃T was built up via
w̃t := w̃t−1 · p(y1:t,x1:t)
p(y1:t−1,x1:t−1) q(yt",3.1 Sequential sampler,[0],[0]
"| y1:t−1,x1:t) (7)
",3.1 Sequential sampler,[0],[0]
The SMC procedure consists of generating a system of M weighted particles whose unnormalized importance weights w̃(m) : 1 ≤ m ≤ M are normalized into w(m),3.1 Sequential sampler,[0],[0]
:= w̃(m)/ ∑M m=1,3.1 Sequential sampler,[0],[0]
"w̃
(m).",3.1 Sequential sampler,[0],[0]
"As M → ∞, SMC provides a consistent estimate of the marginal likelihood p(x) as 1M ∑M m=1 w̃
(m), and samples from the weighted particle system are distributed as samples from the desired posterior p(y | x) (Doucet and Johansen, 2009).",3.1 Sequential sampler,[0],[0]
Particle Gibbs.,3.1 Sequential sampler,[0],[0]
"We employ SMC as a kernel in an MCMC sampler (Andrieu et al., 2010).",3.1 Sequential sampler,[0],[0]
"In particular, we use a block Gibbs sampler in which we iteratively resample the hidden labeling y of a sentence x conditioned on the current labelings for all other sentences in the corpus.",3.1 Sequential sampler,[0],[0]
"In this context, the algorithm is called conditional SMC since one particle is always fixed to the previous sampler state for the sentence being resampled, which ensures that the MCMC procedure is ergodic.",3.1 Sequential sampler,[0],[0]
"At a high level, this procedure is analogous to other Gibbs samplers (e.g. for topic models), except that the conditional SMC (CSMC) kernel uses auxiliary variables (particles) in order to generate the new block variable assignments.",3.1 Sequential sampler,[0],[0]
The procedure is outlined in Algorithm 1.,3.1 Sequential sampler,[0],[0]
"Given a previous latent state assignment y′1:T and observations x1:T , the CSMC kernel produces a new latent state assignment via M auxiliary particles where one particle is fixed to the previous assignment.",3.1 Sequential sampler,[0],[0]
"For ergodicity, M ≥ 2, where larger values of M may improve mixing rate at the expense of increased computation per step.
",3.1 Sequential sampler,[0],[0]
Proposal distribution.,3.1 Sequential sampler,[0],[0]
The choice of proposal distribution q is crucial to the performance of SMC methods.,3.1 Sequential sampler,[0],[0]
"In the case of continuous latent variables,
it is common to propose yt from the transition probability p(Yt | y1:t−1) because this distribution usually has a simple form that permits efficient sampling.",3.1 Sequential sampler,[0],[0]
"However, it is possible to do better in the case of discrete latent variables.",3.1 Sequential sampler,[0],[0]
"The optimal proposal distribution is the one which minimizes the variance of the importance weights, and is given by
q(yt | y1:t−1,x1:t)",3.1 Sequential sampler,[0],[0]
":= p(yt | y1:t−1,x1:t) (8)
= p(yt | y1:t−1)p(xt | yt)
p(xt | y1:t−1)
where
p(xt | y1:t−1)= ∑
yt∈Y p(yt | y1:t−1)p(xt | yt) (9)
Substituting this expression in equation (7) and simplifying yields the incremental weight update:
w̃t := w̃t−1 · p(xt | y1:t−1) (10)
Resampling.",3.1 Sequential sampler,[0],[0]
"In filtering applications, it is common to use resampling operations to prevent weight degeneracy.",3.1 Sequential sampler,[0],[0]
We do not find resampling necessary here for three reasons.,3.1 Sequential sampler,[0],[0]
"First, note that we resample hidden label sequences that are only as long as the number of words in a given sentence.",3.1 Sequential sampler,[0],[0]
"Second, we use a proposal which minimizes the variance of the weights.",3.1 Sequential sampler,[0],[0]
"Finally, we use SMC as a kernel embedded in an MCMC sampler; asymptotically, this procedure yields samples from the desired posterior regardless of degeneracy (which only affects the mixing rate).",3.1 Sequential sampler,[0],[0]
"Practically speaking, one can diagnose the need for resampling via the effective sample size (ESS) of the particle system:
ESS := 1
∑M m=1(w̃",3.1 Sequential sampler,[0],[0]
"(m))2 =
( ∑M
m=1w (m))2
∑M m=1(w",3.1 Sequential sampler,[0],[0]
"(m))2
In our experiments, we find that ESS remains high (a significant fraction of M ) even for long sentences, suggesting that resampling is not necessary to enable mixing of the the Gibbs sampler.
",3.1 Sequential sampler,[0],[0]
Decoding.,3.1 Sequential sampler,[0],[0]
"In order to obtain a single latent variable assignment for evaluation purposes, we simply take the state of the Markov chain after a fixed number of iterations of particle Gibbs.",3.1 Sequential sampler,[0],[0]
"In principle, one could collect many samples during particle Gibbs and use them to perform minimum Bayes risk decoding under a given loss function.",3.1 Sequential sampler,[0],[0]
"However, this approach is somewhat slower and did not appear to improve performance in preliminary experiments
Algorithm 1 Conditional SMC 1: procedure CSMC(x1:T , y′1:T , M ) 2:",3.1 Sequential sampler,[0],[0]
Draw y(m)1 (eqn.,3.1 Sequential sampler,[0],[0]
8) for m ∈,3.1 Sequential sampler,[0],[0]
"[1,M − 1] 3: Set y(M)1 = y ′ 1
4: Set w̃(m)1 (eqn. 10) for m ∈",3.1 Sequential sampler,[0],[0]
"[1,M ] 5: for t",3.1 Sequential sampler,[0],[0]
= 2 to T do 6: Draw y(m)t (eqn.,3.1 Sequential sampler,[0],[0]
8) for m ∈,3.1 Sequential sampler,[0],[0]
"[1,M −1] 7: Set yMt = y ′ t 8: Set w̃(m)t (eqn. 10) for m ∈",3.1 Sequential sampler,[0],[0]
"[1,M ] 9: Set w̃(m) = w̃(m)T",3.1 Sequential sampler,[0],[0]
p($|y1:T ) for m ∈,3.1 Sequential sampler,[0],[0]
"[1,M ]
10: Draw index k where p(k = m) ∝",3.1 Sequential sampler,[0],[0]
w̃(m) 11: return y(k)1:T,3.1 Sequential sampler,[0],[0]
We now present an sampler for settings such as NER where each latent label emits a segment consisting of 1 or more words.,3.2 Segmental sampler,[0],[0]
"We make use of the same transition distribution p(yt | y1:t−1), which determines the probability of a label in a given context, and an emission distribution p(xt | yt) (namely Pyt); these are assumed to be drawn from hierarchical Pitman-Yor processes described in §2.5 and §2.1, respectively.",3.2 Segmental sampler,[0],[0]
"To allow the xt to be a multi-word string, we simply augment the character set with a distinguished space symbol ∈ Σ that separates words within a string.",3.2 Segmental sampler,[0],[0]
"For instance, New York would be generated as the 9-symbol sequence New York$.
Although the model emits New York all at once, we still formulate our inference procedure as a particle filter that proposes one tag for each word.",3.2 Segmental sampler,[0],[0]
"Thus, for a given segment label type y, we allow two tag types for its words:
• I-y corresponds to a non-final word in a segment of type y (in effect, a word with a following attached).",3.2 Segmental sampler,[0],[0]
•,3.2 Segmental sampler,[0],[0]
"E-y corresponds to the final word in a segment
of type y.
For instance, x1:2 = New York would be annotated as a location segment by defining y1:2 = I-LOC E-LOC.",3.2 Segmental sampler,[0],[0]
"This says that y1:2 has jointly emitted x1:2, an event with prior probability PLOC(New York).",3.2 Segmental sampler,[0],[0]
Each word that is not part of a named entity is considered to be a singleword segment.,3.2 Segmental sampler,[0],[0]
"For example, if the next word were x3 = hosted then it should be tagged with y3 = hosted as in §2.5, in which case x3 was emitted with probability 1.
",3.2 Segmental sampler,[0],[0]
"To adapt the sampler described in §3.1 for the segmental case, we need only to define the transition and emission probabilities used in equation (8) and its denominator (9).
",3.2 Segmental sampler,[0],[0]
"For the transition probabilities, we want to model the sequence of segment labels.",3.2 Segmental sampler,[0],[0]
"If yt−1 is an I- tag, we take p(yt | y1:t−1) = 1 , since then yt merely continues an existing segment.",3.2 Segmental sampler,[0],[0]
"Otherwise yt starts a new segment, and we take p(yt | y1:t−1) = 1 to be defined by the PYP’s probability Gy1:t−1(yt) as usual, but where we interpret the subscript y1:t−1 to refer to the possibly shorter sequence of segment labels implied by those t− 1 tags.
",3.2 Segmental sampler,[0],[0]
"For the emission probabilities, if yt has the form I-y or E-y, then its associated emission probability no longer has the form p(xt | yt), since the choice of xt also depends on any words emitted earlier in the segment.",3.2 Segmental sampler,[0],[0]
Let s ≤ t be the starting position of the segment that contains t.,3.2 Segmental sampler,[0],[0]
"If yt = E-y, then the emission probability is proportional to Py(xs xs+1 . . .",3.2 Segmental sampler,[0],[0]
xt).,3.2 Segmental sampler,[0],[0]
If yt = I-y then the emission probability is proportional to the prefix probability ∑ x Py(x) where x ranges over all strings in Σ∗,3.2 Segmental sampler,[0],[0]
that have xs xs+1 . . .,3.2 Segmental sampler,[0],[0]
xt,3.2 Segmental sampler,[0],[0]
as a proper prefix.,3.2 Segmental sampler,[0],[0]
"Prefix probabilities in Hy are easy to compute because Hy has the form of a language model, and prefix probabilities in Py are therefore also easy to compute (using a prefix tree for efficiency).
",3.2 Segmental sampler,[0],[0]
This concludes the description of the segmental sampler.,3.2 Segmental sampler,[0],[0]
Note that the particle Gibbs procedure is unchanged.,3.2 Segmental sampler,[0],[0]
"Automatically inducing parts-of-speech from raw text is a challenging problem (Goldwater et al., 2005).",4 Inducing parts-of-speech with type-level supervision,[0],[0]
"Our focus here is on the easier problem of type-supervised part-of-speech induction, in which (partial) dictionaries are used to guide inference (Garrette and Baldridge, 2012; Li et al., 2012).",4 Inducing parts-of-speech with type-level supervision,[0],[0]
"Conditioned on the unlabeled corpus and dictionary, we use the MCMC procedure described in §3.1 to impute the latent parts-of-speech.
",4 Inducing parts-of-speech with type-level supervision,[0],[0]
"Since dictionaries are freely available for hundreds of languages,6 we see this as a mild additional requirement in practice over the purely unsupervised setting.
",4 Inducing parts-of-speech with type-level supervision,[0],[0]
"In prior work, dictionaries have been used as constraints on possible parts-of-speech: words appearing in the dictionary take one of their known parts-
6https://www.wiktionary.org/
of-speech.",4 Inducing parts-of-speech with type-level supervision,[0],[0]
"In our setting, however, the dictionaries are not constraints but evidence.",4 Inducing parts-of-speech with type-level supervision,[0],[0]
"If monthly is listed in (only) the adjective lexicon, this tells us that PADJ sometimes generates monthly and therefore that HADJ may also tend to generate other words that end with -ly.",4 Inducing parts-of-speech with type-level supervision,[0],[0]
"However, for us, PADV(monthly) > 0",4 Inducing parts-of-speech with type-level supervision,[0],[0]
"as well, allowing us to still correctly treat monthly as a possible adverb if we later encounter it in a training or test corpus.",4 Inducing parts-of-speech with type-level supervision,[0],[0]
"We follow the experimental procedure described in Li et al. (2012), and use their released code and data to compare to their best model: a second-order maximum entropy Markov model parametrized with log-linear features (SHMM-ME).",4.1 Experiments,[0],[0]
"This model uses hand-crafted features designed to distinguish between different parts-of-speech, and it has special handling for rare words.",4.1 Experiments,[0],[0]
"This approach is surprisingly effective and outperforms alternate approaches such as cross-lingual transfer (Das and Petrov, 2011).",4.1 Experiments,[0],[0]
"However, it also has limitations, since words that do not appear in the dictionary will be unconstrained, and spurious or incorrect lexical entries may lead to propagation of errors.
",4.1 Experiments,[0],[0]
"The lexicons are taken from the Wiktionary project; their size and coverage are documented by (Li et al., 2012).",4.1 Experiments,[0],[0]
We evaluate our model on multi-lingual data released as part of the CoNLL 2007 and CoNLL-X shared tasks.,4.1 Experiments,[0],[0]
"In particular, we use the same set of languages as Li et al. (2012).7 For our method, we impute the parts-of-speech by running particle Gibbs for 100 epochs, where one epoch consists of resampling the states for a each sentence in the corpus.",4.1 Experiments,[0],[0]
"The final sampler state is then taken as a 1-best tagging of the unlabeled data.
",4.1 Experiments,[0],[0]
Results.,4.1 Experiments,[0],[0]
The results are reported in Table 1.,4.1 Experiments,[0],[0]
"We find that our hierarchical sequence memoizer (HSM) matches or exceeds the performance of the baseline (SHMM-ME) for nearly all the tested languages, particularly for morphologically rich languages such as German where the spelling distributions Hy may capture regularities.",4.1 Experiments,[0],[0]
"It is interesting to note that our model performs worse relative to the baseline for English; one possible explanation is that the baseline uses hand-engineered features whereas ours does not, and these features may have been tuned using English data for validation.
",4.1 Experiments,[0],[0]
7With the exception of Dutch.,4.1 Experiments,[0],[0]
"Unlike the other CoNLL languages, Dutch includes phrases, and the procedure by which these were split into tokens was not fully documented.
",4.1 Experiments,[0],[0]
Our generative model is supposed to exploit lexicons well.,4.1 Experiments,[0],[0]
"To see what is lost from using a generative model, we also compared with Li et al. (2012) on standard supervised tagging without any lexicons.",4.1 Experiments,[0],[0]
"Even here our generative model is very competive, losing only on English and Swedish.",4.1 Experiments,[0],[0]
Name lists and dictionaries are useful for NER particularly when in-domain annotations are scarce.,5 Boostrapping NER with type-level supervision,[0],[0]
"However, with little annotated data, discriminative training may be unable to reliably estimate lexical feature weights and may overfit.",5 Boostrapping NER with type-level supervision,[0],[0]
"In this section, we are interested in evaluating our proposed Bayesian model in the context of low-resource NER.",5 Boostrapping NER with type-level supervision,[0],[0]
"Most languages do not have corpora annotated for parts-of-speech, named-entities, syntactic parses, or other linguistic annotations.",5.1 Data,[0],[0]
"Therefore, rapidly deploying natural language technologies in a new language may be challenging.",5.1 Data,[0],[0]
"In the context of facilitating relief responses in emergencies such as natural disasters, the DARPA LORELEI (Low Resource Languages for Emergent Incidents) program has sponsored the development and release of representative “language packs” for Turkish and Uzbek with more languages planned (Strassel and Tracey, 2016).",5.1 Data,[0],[0]
"We use the named-entity annotations as part of these language packs which include persons, locations, organizations, and geo-political entities, in order to explore bootstrapping named-entity recognition from small amounts of data.",5.1 Data,[0],[0]
"We consider two types of data: ¬ in-context annotations, where sentences are fully annotated for named-entities, and  lexical resources.
",5.1 Data,[0],[0]
The LORELEI language packs lack adequate indomain lexical resources for our purposes.,5.1 Data,[0],[0]
"Therefore, we simulate in-domain lexical resources by holding out portions of the annotated development data and deriving dictionaries and name lists from them.",5.1 Data,[0],[0]
"For each label y ∈ {PER, LOC, ORG, GPE, CONTEXT}, our lexicon for y lists all distinct y-labeled strings that appear in the held-out data.",5.1 Data,[0],[0]
This setup ensures that the labels associated with lexicon entries correspond to the annotation guidelines used in the data we use for evaluation.,5.1 Data,[0],[0]
"It avoids possible problems that might arise when leveraging noisy out-of-domain knowledge bases, which we may explore in future.",5.1 Data,[0],[0]
In this section we report supervised NER experiments on two low-resource languages: Turkish and Uzbek.,5.2 Evaluation,[0],[0]
We vary both the amount of supervision as well as the size of the lexical resources.,5.2 Evaluation,[0],[0]
A challenge when evaluating the performance of a model with small amounts of training data is that there may be high-variance in the results.,5.2 Evaluation,[0],[0]
"In order to have more confidence in our results, we perform bootstrap resampling experiments in which the training set, evaluation set, and lexical resources are randomized across several replications of the same experiment (for each of the data conditions).",5.2 Evaluation,[0],[0]
"We use 10 replications for each of the data conditions reported in Figures 1–2, and report both the mean performance and 95% confidence intervals.
Baseline.",5.2 Evaluation,[0],[0]
"We use the Stanford NER system with a standard set of language-independent features (Finkel et al., 2005).8.",5.2 Evaluation,[0],[0]
This model is a conditional random field (CRF) with feature templates which include character n-grams as well as word shape features.,5.2 Evaluation,[0],[0]
"Crucially, we also incorporate lexical features.",5.2 Evaluation,[0],[0]
"The CRF parameters are regularized using an L1 penalty and optimized via Orthant-wise limited-memory quasi-Newton optimization (Andrew and Gao, 2007).",5.2 Evaluation,[0],[0]
"For both our proposed method and the discriminative baseline, we use a fixed set of hyperparameters (i.e. we do not use a separate validation set for tuning each data condition).",5.2 Evaluation,[0],[0]
"In order to make a fair comparison to the CRF, we use our sampler for forward inference only, without resampling on the test data.
Results.",5.2 Evaluation,[0],[0]
We show learning curves as a function of supervised training corpus size.,5.2 Evaluation,[0],[0]
Figure 1 shows that our generative model strongly beats the baseline in this low-data regime.,5.2 Evaluation,[0],[0]
"In particular, when there is little annotated training data, our proposed generative model can compensate by exploiting the lexicon, while the discriminative baseline scores terribly.",5.2 Evaluation,[0],[0]
"The performance gap decreases with larger
8We also experimented with neural models, but found that the CRF outperformed them in low-data conditions.
supervised corpora, which is consistent with prior results comparing generative and discriminative training (Ng and Jordan, 2002).
",5.2 Evaluation,[0],[0]
"In Figure 2, we show the effect of the lexicon’s size: as expected, larger lexicons are better.",5.2 Evaluation,[0],[0]
"The generative approach significantly outperforms the discriminative baseline at any lexicon size, although its advantage drops for smaller lexicons or larger training corpora.
",5.2 Evaluation,[0],[0]
"In Figure 1 we found that increasing the pseudocount c consistently decreases performance, so we used c = 1 in our other experiments.9",5.2 Evaluation,[0],[0]
This paper has described a generative model for low-resource sequence labeling and segmentation tasks using lexical resources.,6 Conclusion,[0],[0]
Experiments in semisupervised and low-resource settings have demonstrated its applicability to part-of-speech induction and low-resource named-entity recognition.,6 Conclusion,[0],[0]
There are many potential avenues for future work.,6 Conclusion,[0],[0]
Our model may be useful in the context of active learning where efficient re-estimation and performance in low-data conditions are important.,6 Conclusion,[0],[0]
"It would also be interesting to explore more expressive parameterizations, such recurrent neural networks for Hy.",6 Conclusion,[0],[0]
"In the space of neural methods, differentiable memory (Santoro et al., 2016) may be more flexible than the PYP prior, while retaining the ability of the model to cache strings observed in the gazetteer.",6 Conclusion,[0],[0]
"This work was supported by the JHU Human Language Technology Center of Excellence, DARPA LORELEI, and NSF grant IIS-1423276.",Acknowledgments,[0],[0]
"Thanks to Jay Feldman for early discussions.
",Acknowledgments,[0],[0]
9Why?,Acknowledgments,[0],[0]
"Even a pseudocount of c = 1 is enough to ensure that Py(s) Hy(s), since the prior probability Hy(s) is rather small for most strings in the lexicon.",Acknowledgments,[0],[0]
"Indeed, perhaps c < 1 would have increased performance, particularly if the lexicon reflects out-of-domain data.",Acknowledgments,[0],[0]
"This could be arranged, in effect, by using a hierarchical Bayesian model in which the lexicon and corpus emissions are not drawn from the identical distribution Py but only from similar (coupled) distributions.",Acknowledgments,[0],[0]
Lexical resources such as dictionaries and gazetteers are often used as auxiliary data for tasks such as part-of-speech induction and named-entity recognition.,abstractText,[0],[0]
"However, discriminative training with lexical features requires annotated data to reliably estimate the lexical feature weights and may result in overfitting the lexical features at the expense of features which generalize better.",abstractText,[0],[0]
"In this paper, we investigate a more robust approach: we stipulate that the lexicon is the result of an assumed generative process.",abstractText,[0],[0]
"Practically, this means that we may treat the lexical resources as observations under the proposed generative model.",abstractText,[0],[0]
The lexical resources provide training data for the generative model without requiring separate data to estimate lexical feature weights.,abstractText,[0],[0]
We evaluate the proposed approach in two settings: part-of-speech induction and lowresource named-entity recognition.,abstractText,[0],[0]
Bayesian Modeling of Lexical Resources for Low-Resource Settings,title,[0],[0]
"Flexible and computationally efficient models for streaming data are required in many machine learning applications, and in this paper we propose a new class of models for these situations.",1. Introduction,[0],[0]
"Specifically, we are interested in models suitable for domains that exhibit changes in the underlying generative process (Gama et al., 2014).",1. Introduction,[0],[0]
"We envision a situation, where one receives batches of data at discrete points in time.",1. Introduction,[0],[0]
"As each new batch arrives, we want to glean information from the new data, while also retaining relevant information from the historical observations.
",1. Introduction,[0],[0]
"Our modelling is inspired by previous works on Bayesian recursive estimation (Özkan et al., 2013; Kárnỳ, 2014), power priors (Ibrahim & Chen, 2000) and exponential for-
1Department of Mathematics, Unversity of Almerı́a, Almerı́a, Spain 2Department of Computer and Information Science, Norwegian University of Science and Technology, Trondheim, Norway 3Department of Computer Science, Aalborg University, Aalborg, Denmark 4Hugin",1. Introduction,[0],[0]
"Expert A/S, Aalborg, Denmark.",1. Introduction,[0],[0]
Correspondence to: Andrés,1. Introduction,[0],[0]
Masegosa,1. Introduction,[0],[0]
"<andresmasegosa@ual.es>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
getting approaches (Honkela & Valpola, 2003).",1. Introduction,[0],[0]
"However, all of these methods were developed for slowly changing processes, where the rate of change anticipated by the model is controlled by a quantity that must be set manually.",1. Introduction,[0],[0]
"Our solution, on the other hand, can accommodate both gradual and abrupt concept drift by continuously assessing the similarity between new and historic data using a fully Bayesian paradigm.
",1. Introduction,[0],[0]
"Building Bayesian models for data streams raises computational problems, as data may arrive with high velocity and is unbounded in size.",1. Introduction,[0],[0]
We therefore develop an approximate variational inference technique based on a novel lower-bound of the data likelihood function.,1. Introduction,[0],[0]
"The appropriateness of the approach is investigated through experiments using both synthetic and real-life data, giving encouraging results.",1. Introduction,[0],[0]
The proposed methods are released as part of an open-source toolbox for scalable probabilistic machine learning (http://www.amidsttoolbox.com),1. Introduction,[0],[0]
"(Masegosa et al., 2017; 2016b; Cabañas et al., 2016).",1. Introduction,[0],[0]
In this paper we focus on conjugate exponential Bayesian network models for performing Bayesian learning on streaming data.,2. Preliminaries,[0],[0]
"To simplify the presentation, we shall initially focus on the model structure shown in Figure 1 (a).",2. Preliminaries,[0],[0]
This model includes the observed data x = xi=1,2. Preliminaries,[0],[0]
":N , global hidden variables (or parameters) β = β1:M , a set of local hidden variables z = z1:N , and a vector of fixed (hyper) parameters denoted by α.",2. Preliminaries,[0],[0]
"Notice how the dynamics of the process is not included in the model of Figure 1 (a); the model will be set in the context of data streams in Section 4, where we extend it to incorporate explicit dynamics over the (global) parameters to capture concept drift.
",2. Preliminaries,[0],[0]
"With the conditional distributions in the model belonging to the exponential family, we have that all distributions are of the following form
ln p(Y |pa(Y ))",2. Preliminaries,[0],[0]
= lnhY + ηY,2. Preliminaries,[0],[0]
(pa(Y )),2. Preliminaries,[0],[0]
"T tY (Y )− aY (ηY (pa(Y ))),
where pa(Y ) denotes the parents of Y in the directed acyclic graph of the induced Bayesian network model.",2. Preliminaries,[0],[0]
"The scalar functions hY and aY (·) are the base measure and
the log-normalizer, respectively; the vector functions ηY (·) and tY (·) are the natural parameters and the sufficient statistics vectors, respectively.",2. Preliminaries,[0],[0]
"The subscript Y means that the associated functional forms may be different for the different factors of the model, but we may remove the subscript when clear from the context.",2. Preliminaries,[0],[0]
"By also requiring that the distributions are conjugate, we have that the posterior distribution for each variable in the model has the same functional form as its prior distribution.",2. Preliminaries,[0],[0]
"Consequently, learning (i.e. conditioning the model on observations) only changes the values of the parameters of the model, and not the functional form of the distributions.
",2. Preliminaries,[0],[0]
"Variational inference is a deterministic technique for finding tractable posterior distributions, denoted by q, which approximates the Bayesian posterior, p(β, z|x), that is often intractable to compute.",2. Preliminaries,[0],[0]
"More specifically, by letting Q be a set of possible approximations of this posterior, variational inference solves the following optimization problem for any model in the conjugate exponential family:
min q(β,z)∈Q
KL(q(β, z)|p(β, z|x)), (1)
where KL denotes the Kullback-Leibler divergence between two probability distributions.
",2. Preliminaries,[0],[0]
In the mean field variational approach the approximation familyQ is assumed to fully factorize.,2. Preliminaries,[0],[0]
"Extending the notation of Hoffman et al. (2013), we have that
q(β, z|λ,φ) = M∏ k=1 q(βk|λk) N∏ i=1",2. Preliminaries,[0],[0]
"J∏ j=1 q(zi,j |φi,j),
where J is the number of local hidden variables, which is assumed fixed for all i = 1, . . .",2. Preliminaries,[0],[0]
", N .",2. Preliminaries,[0],[0]
"The parameterizations of the variational distributions are made explicit, in that λ parameterize the variational distribution of β, while φ has the same role for the variational distribution of z.
To solve the minimization problem in Equation (1), the
variational approach exploits the transformation
lnP (x) = L(λ,φ|x,αu) + KL(q(β, z|λ,φ)|p(β, z|x)), (2) where L(·|·) is a lower bound of lnP (x) since KL is nonnegative.",2. Preliminaries,[0],[0]
x,2. Preliminaries,[0],[0]
"and αu are introduced in L’s notation to make explicit the function’s dependency on x, the data sample, and αu, the natural parameters of the prior over β.",2. Preliminaries,[0],[0]
"As lnP (x) is constant, minimizing the KL term is equivalent to maximizing the lower bound.",2. Preliminaries,[0],[0]
"Variational methods maximize this lower bound by applying a coordinate ascent that iteratively updates the individual variational distributions while holding the others fixed (Winn & Bishop, 2005).",2. Preliminaries,[0],[0]
"The key advantage of having a conjugate exponential model is that the gradients of theL function can be always computed in closed form (Winn & Bishop, 2005).",2. Preliminaries,[0],[0]
"Bayesian inference on streaming data has been widely studied (Ahmed et al., 2011; Doucet et al., 2000; Yao et al., 2009).",3. Related Work,[0],[0]
"In the context of variational inference, there are two main approaches.",3. Related Work,[0],[0]
Ghahramani & Attias (2000); Broderick et al. (2013) propose recursive Bayesian updating of the variational approximation.,3. Related Work,[0],[0]
"The streaming variational Bayes (SVB) algorithm (Broderick et al., 2013) is the most known approach of this category.",3. Related Work,[0],[0]
"Alternatively, one could cast the inference problem as a stochastic optimization problem.",3. Related Work,[0],[0]
"Stochastic variational inference (SVI) (Hoffman et al., 2013) and the closely related population variational Bayes (PVB) (McInerney et al., 2015) are prominent examples from this group.",3. Related Work,[0],[0]
"SVI assumes the existence of a fixed data set observed in a sequential manner, and in particular that this data set has a known finite size.",3. Related Work,[0],[0]
This is unrealistic when modeling data streams.,3. Related Work,[0],[0]
"PVB addresses this problem by using the frequentist notion of a population distribution, F, which is assumed to generate the data stream by repeatedly sampling M data points at the time.",3. Related Work,[0],[0]
"M parameterizes the size of the population, and helps control the variance of the population posterior.",3. Related Work,[0],[0]
"Unfortunately, M must be specified by the user.",3. Related Work,[0],[0]
"No clear rule exists regarding how to set it, and McInerney et al. (2015) show that its optimal value may differ from one data stream to another.
",3. Related Work,[0],[0]
"The problem of Bayesian modeling of non-stationary data streams (i.e., with concept drift (Gama et al., 2014)) is not addressed by SVB, as it assumes data exchangeability.",3. Related Work,[0],[0]
"An online variational inference method, which exponentially forgets the variational parameters associated with old data, was proposed by Honkela & Valpola (2003).",3. Related Work,[0],[0]
"The so-called power prior approach (Ibrahim & Chen, 2000) is also based on an exponential forgetting mechanisms, and has nice theoretical properties (Ibrahim et al., 2003).",3. Related Work,[0],[0]
"Nevertheless, both approaches rely on a hyper-parameter determining forgetting, which has to be set manually.",3. Related Work,[0],[0]
"PVB can
also adapt to concept drift, because the variance of the variational posterior never decreases below a given threshold indirectly controlled by M , but again, the hyper-parameter has to be set manually.
",3. Related Work,[0],[0]
A time series based modeling approach for concept drift using implicit transition models was pursued by Özkan et al. (2013); Kárnỳ (2014).,3. Related Work,[0],[0]
"Unfortunately, the implicit transition model depends on a hyper-parameter determining the forgetting-factor, which has to be manually set.",3. Related Work,[0],[0]
"In this paper we build on this approach, adapt it to variational settings, and place a hierarchical prior on its forgetting parameter.",3. Related Work,[0],[0]
This greatly improves the flexibility and accuracy of the resulting model when making inferences over drifting data streams.,3. Related Work,[0],[0]
In this section we extend the model in Figure 1 (a) to also account for the dynamics of the data stream being modeled.,4. Hierarchical Power Priors,[0],[0]
"We shall here assume that only the parameters β in Figure 1 (a) are time-varying, which we will indicate with the subscript t, i.e., βt.",4. Hierarchical Power Priors,[0],[0]
First we briefly describe the approach on which the proposed model is based.,4. Hierarchical Power Priors,[0],[0]
"Afterwards, we introduce the hierarchical power prior and detail a variational inference procedure for this model class.",4. Hierarchical Power Priors,[0],[0]
"In order to extend the model in Figure 1 (a) to data streams, we may introduce a transition model p(βt|βt−1) to explicitly model the evolution of the parameters over time, enabling the estimation of the predictive density at time t:
p(βt|x1:t−1) = ∫",4.1. Power Priors as Implicit Transition Models,[0],[0]
"p(βt|βt−1)p(βt−1|x1:t−1)dβt−1.
(3) However, this approach introduces two problems.",4.1. Power Priors as Implicit Transition Models,[0],[0]
"First of all, in non-stationary domains we may not have a single transition model or the transition model may be unknown.",4.1. Power Priors as Implicit Transition Models,[0],[0]
"Secondly, if we seek to position the model within the conjugate exponential family in order to be able to compute the gradients of L in closed-form, we need to ensure that the distribution family for βt is its own conjugate distribution, thereby severely limiting model expressivity (we can, e.g., not assign a Dirichlet distribution to βt).
",4.1. Power Priors as Implicit Transition Models,[0],[0]
"Rather than explicitly modeling the evolution of the βt parameters as in Equation (3), we instead follow the approach of Kárnỳ (2014) and Özkan et al. (2013) who define the time evolution model implicitly by constraining the maximum KL divergence over consecutive parameter distributions.",4.1. Power Priors as Implicit Transition Models,[0],[0]
"Specifically, by defining
pδ(βt|x1:t−1) = ∫",4.1. Power Priors as Implicit Transition Models,[0],[0]
"δ(βt − βt−1)p(βt−1|x1:t−1)dβt−1
(4)
one can restrict the space of possible distributions p(βt|x1:t−1), supported by an unknown transition model, by the constraint
KL(p(βt|x1:t−1), pδ(βt|x1:t−1)) ≤ κ.",4.1. Power Priors as Implicit Transition Models,[0],[0]
"(5)
Kárnỳ (2014) and Özkan et al. (2013) seek to approximate p(βt|x1:t−1) by the distribution p̂(βt|x1:t−1) having maximum entropy under the constraint in (5); for continuous distributions the maximum entropy can be formulated relative to an uninformative prior density pu(βt), which corresponds to the Kullbach-Leibler divergence between the two distributions.",4.1. Power Priors as Implicit Transition Models,[0],[0]
"This approach ensures that we will not underestimate the uncertainty in the parameter distribution and the particular solution being sought takes the form
p̂(βt|x1:t−1, ρt) ∝",4.1. Power Priors as Implicit Transition Models,[0],[0]
"pδ(βt|x1:t−1)ρtpu(βt)(1−ρt), (6)
",4.1. Power Priors as Implicit Transition Models,[0],[0]
"where 0 ≤ ρt ≤ 1 is indirectly defined by (5) which in turn depends on the user defined parameter κ.
",4.1. Power Priors as Implicit Transition Models,[0],[0]
"In our streaming data setting we follow assumed density filtering (Lauritzen, 1992) and the SVB approach (Broderick et al., 2013) and employ the approximation p(βt−1|x1:t−1)",4.1. Power Priors as Implicit Transition Models,[0],[0]
"≈ q(βt−1|λt−1), where q(βt−1|λt−1) is the variational distribution calculated in the previous time step.",4.1. Power Priors as Implicit Transition Models,[0],[0]
"Using this approximation in (3) and (4), we can express pδ in terms of λt−1 in which case (6) becomes
p̂(βt|λt−1, ρt) ∝ pδ(βt|λt−1)ρtpu(βt)(1−ρt), (7)
which we use as the prior density for time step t. Now, if pu(βt) belong to the same family as q(βt−1|λt−1), then p̂(βt|λt−1, ρt) will stay within the same family and have natural parameters ρtλt−1+(1−ρt)αu, where αu are the natural parameters of pu(βt).",4.1. Power Priors as Implicit Transition Models,[0],[0]
"Thus, under this approach, the transitioned posterior remains within the same exponential family, so we can enjoy the full flexibility of the conjugate exponential family (i.e. computing gradients of the L function in closed form), an option that would not be available if one were to explicitly specify a transition model as in Equation (3).
",4.1. Power Priors as Implicit Transition Models,[0],[0]
"So, at each time step, we simply have to solve the following variational problem, where only the prior changes with respect to the original SVB approach,
arg max λt,φt L(λt,φt|xt, ρtλt−1 + (1− ρt)αu).
",4.1. Power Priors as Implicit Transition Models,[0],[0]
"As stated in the following lemma, this approach coincides with the so-called power priors approach (Ibrahim & Chen, 2000), a term that we will also adopt in the following.
",4.1. Power Priors as Implicit Transition Models,[0],[0]
Lemma 1.,4.1. Power Priors as Implicit Transition Models,[0],[0]
"The Bayesian updating scheme described by Figure 1 (b) and Equation 6, but with ρt fixed to a constant value, is equivalent to the recursive application of
the Bayesian updating scheme of power priors (Ibrahim & Chen, 2000).",4.1. Power Priors as Implicit Transition Models,[0],[0]
"This scheme is expressed as follows:
p(β|x1,x0, ρ) ∝ p(x1|β)p(x0|β)ρp(β),
where x0 and x1 is the observation at time 0 (historical observation) and time 1 (current observation), respectively.
",4.1. Power Priors as Implicit Transition Models,[0],[0]
Proof sketch.,4.1. Power Priors as Implicit Transition Models,[0],[0]
"Translate the recursive Bayesian updating approach of power priors into an equivalent two time slice model, where β0 is given a prior distribution p and p(β1|β0) is a Dirac delta function.",4.1. Power Priors as Implicit Transition Models,[0],[0]
"The distribution p(β1|x0,x1, ρ) in this model is equivalent to p(β|x1,x0, ρ), which, in turn, is equivalent (up to proportionality) to p(x1|β1)p̂(β1|x0, ρt).",4.1. Power Priors as Implicit Transition Models,[0],[0]
"Note that the last p̂ term can alternatively be expressed as p̂(β1|x0, ρt) ∝ pδ(β1|x0)ρp(β1)1−ρ ∝ pδ(x0|β1)ρp(β1).
",4.1. Power Priors as Implicit Transition Models,[0],[0]
"The perspective provided by Lemma 1 introduces a well known result of power priors, which is also applicable in the current context (see the discussion after Theorem 1 in (Ibrahim et al., 2003)): “the power prior is an optimal prior to use and in fact minimizes the convex combination of KL divergences between two extremes: one in which no historical data is used and the other in which the historical data and current data are given equal weight.”",4.1. Power Priors as Implicit Transition Models,[0],[0]
"As noted in (Olesen et al., 1992; Özkan et al., 2013), this schema works as a moving window with exponential forgetting of past data, where the effective number of samples or, more technically, the so-called equivalent sample size of the posterior (Heckerman et al., 1995), converges to,
lim t→∞ ESSt = |xt| 1− ρ
(8)
if the size of the data batches is constant1.
",4.1. Power Priors as Implicit Transition Models,[0],[0]
For the experimental results reported in Section 5 we shall refer to the method outlined above as SVB with power priors (SVB-PP).,4.1. Power Priors as Implicit Transition Models,[0],[0]
"In the approach taken by Özkan et al. (2013) (and, by extension, SVB-PP), the forgetting factor ρt is user-defined.",4.2. The Hierarchical Power Prior Model,[0],[0]
"In this paper, we instead pursue a (hierarchical) Bayesian approach and introduce a prior distribution over ρt allowing the distribution over ρt (and thereby the forgetting mechanism) to adapt to the data stream.
",4.2. The Hierarchical Power Prior Model,[0],[0]
"As we shall see below, in order to support a variational updating scheme we need to restrict the prior distribution over ρt, effectively limiting the choice of prior distribution to
1For instance, the ESS of a Beta distribution is equal to the sum of the components of λt and, in turn, equal to the number of data samples seen so far plus the prior’s pseudo-samples.
",4.2. The Hierarchical Power Prior Model,[0],[0]
"either an exponential distribution or a normal distribution with fixed variance, both of which should be truncated to the interval",4.2. The Hierarchical Power Prior Model,[0],[0]
"[0, 1].",4.2. The Hierarchical Power Prior Model,[0],[0]
"Unless explicitly stated otherwise, we shall for now assume a truncated exponential distribution with natural parameter γ as prior distribution over ρt:
p(ρt|γ) = γ exp(−γρt) 1− exp(−γ) .",4.2. The Hierarchical Power Prior Model,[0],[0]
"(9)
The resulting model can be illustrated as in Figure 1 (b).",4.2. The Hierarchical Power Prior Model,[0],[0]
We shall refer to models of this type as hierarchical power prior (HPP) models.,4.2. The Hierarchical Power Prior Model,[0],[0]
"For updating the model distributions we pursue a variational approach, where we seek to maximize the evidence lower bound L in Equation (2) for time step t. However, since the model in Figure 1 (b) does not define a conjugate exponential distribution due to the introduction of p(ρt) we cannot maximize L directly.",4.3. Variational Updating,[0],[0]
"Instead we will derive a (double) lower bound L̂ (L̂ ≤ L) and use this lower bound as a proxy for the updating rules for the variational posteriors.
",4.3. Variational Updating,[0],[0]
"First of all, by instantiating the lower bound LHPP (λt,φt, ωt|xt,λt−1) in Equation (2) for the HPP model we obtain
LHPP (λt,φt, ωt|xt,λt−1) = Eq[ln p(xt,Zt|βt)]",4.3. Variational Updating,[0],[0]
"+ Eq[ln p̂(βt|λt−1, ρt)]",4.3. Variational Updating,[0],[0]
+ Eq[p(ρt|γ)]− Eq[ln q(Zt|φt)],4.3. Variational Updating,[0],[0]
"− Eq[q(βt|λt)]− Eq[q(ρt|ωt)], (10)
where ωt is the variational parameter for the variational distribution for ρt; as we shall see later, ωt is a scalar and is therefore not shown in boldface.",4.3. Variational Updating,[0],[0]
"For ease of presentation we shall sometimes drop from LHPP (λt,φt, ωt|xt,λt−1) the subscript as well as the explicit specification of the parameters when these is otherwise clear from the context.
",4.3. Variational Updating,[0],[0]
We now define L̂HPP,4.3. Variational Updating,[0],[0]
"(λt,φt, ωt|xt,λt−1)",4.3. Variational Updating,[0],[0]
"as
L̂HPP (λt,φt, ωt|xt,λt−1) = Eq[ln p(xt,Zt|βt)]",4.3. Variational Updating,[0],[0]
+ Eq[ρt]Eq[ln pδ(βt|λt−1)],4.3. Variational Updating,[0],[0]
+ (1− Eq[ρt])Eq[ln pu(βt)],4.3. Variational Updating,[0],[0]
+ Eq[p(ρt|γ)]− Eq[ln q(Zt|φt)],4.3. Variational Updating,[0],[0]
"− Eq[q(βt|λt)]− Eq[q(ρt|ωt)], (11)
which provide a lower bound for L. Theorem 1. L̂HPP gives a lower bound for LHPP :
L̂HPP",4.3. Variational Updating,[0],[0]
"(λt,φt, ωt|xt,λt−1) ≤ LHPP (λt,φt, ωt|xt,λt−1).
",4.3. Variational Updating,[0],[0]
Proof sketch.,4.3. Variational Updating,[0],[0]
The inequality derives by using Equation (12) and observing that ag(ρtλt−1,4.3. Variational Updating,[0],[0]
+ (1 − ρt)αu) ≤ ρtag(λt−1),4.3. Variational Updating,[0],[0]
"+ (1− ρt)ag(αu) because the log-normalizer
ag is always a convex function (Wainwright et al., 2008)",4.3. Variational Updating,[0],[0]
.,4.3. Variational Updating,[0],[0]
"Full details are given in the supplementary material.
",4.3. Variational Updating,[0],[0]
"Rather than seeking to maximize L we will instead maximize L̂. The gap between the two bounds is determined only by the log-normalizer of p̂(βt|λt−1, ρt):
L̂ − L = Eq[ρtag(λt−1)",4.3. Variational Updating,[0],[0]
+ (1− ρt)ag(αu) + ag(ρtλt−1,4.3. Variational Updating,[0],[0]
"+ (1− ρt)αu)]
(12)
",4.3. Variational Updating,[0],[0]
"Thus, maximizing L̂ wrt.",4.3. Variational Updating,[0],[0]
"the variational parameters λt and φ also maxmizes L. By the same observation, we also have that the (natural) gradients are consistent relative to the two bounds:
Corollary 1.
",4.3. Variational Updating,[0],[0]
"∇̂λtL = ∇̂λtL̂ ∇̂φtL = ∇̂φtL̂ .
",4.3. Variational Updating,[0],[0]
Proof.,4.3. Variational Updating,[0],[0]
"Follows immediately from Equation (12) because the difference does not depend of λt and φt.
",4.3. Variational Updating,[0],[0]
"Thus, updating the variational parametersλt andφt in HPP models can be done as for regular conjugate exponential models of the form in Figure 1.
",4.3. Variational Updating,[0],[0]
"In order to update ωt we rely on L̂, which we can maximize using the natural gradient wrt.",4.3. Variational Updating,[0],[0]
"ωt (Sato, 2001) and which can be calculated in closed form for a restricted distribution family for ρt.
",4.3. Variational Updating,[0],[0]
Lemma 2.,4.3. Variational Updating,[0],[0]
"Assuming that the sufficient statistics function for ρt is the identity function, t(ρt) = ρt, then we have
∇̂ωtL̂ =KL(q(βt|λt), pu(βt))",4.3. Variational Updating,[0],[0]
"− KL(q(βt|λt), pδ(βt|λt−1))",4.3. Variational Updating,[0],[0]
"+ γ − ωt (13)
",4.3. Variational Updating,[0],[0]
Proof sketch.,4.3. Variational Updating,[0],[0]
Based on a straightforward algebraic derivation of the gradient using standard properties of the exponential family.,4.3. Variational Updating,[0],[0]
"Full details are given in the supplementary material.
",4.3. Variational Updating,[0],[0]
"Note that the truncated exponential distribution (see Equation (9)) satisfies the restriction expressed in Lemma 2, and also note that the variational posterior q(ρt|ωt) will be a truncated exponential density too.
",4.3. Variational Updating,[0],[0]
"On the other hand, observe that the form of the natural gradient of ωt have an intuitive semantic interpretation, which also extends to the coordinate ascent variational message passing framework (Winn & Bishop, 2005) as shown by Masegosa et al. (2016a).",4.3. Variational Updating,[0],[0]
"Specifically, using the constant γ as a threshold, we see that if the uninformed prior pu(βt) provides a better fit to the variational posterior at time t than the variational parameters λt from the previous time step (KL(q(βt|λt), pu(βt))",4.3. Variational Updating,[0],[0]
"+
γ < KL(q(βt|λt), pδ(βt|λt−1))), then we will get a negative value for ωt when performing coordinate ascent using Equation (13).",4.3. Variational Updating,[0],[0]
This in turn implies that Eq[ρ] < 0.5 because Eq[ρ] = 1/(1 − e−ωt),4.3. Variational Updating,[0],[0]
"− 1/ωt (plotted in Figure 2), which means that we have a higher degree of forgetting for past data.",4.3. Variational Updating,[0],[0]
If ωt > 0 then Eq[ρ] > 0.5 and less past data is forgotten.,4.3. Variational Updating,[0],[0]
Figure 2 graphically illustrates this trade-off.,4.3. Variational Updating,[0],[0]
"The HPP model can immediately be extended to include multiple power priors ρ(i)t , one for each global parameter βi.",4.4. The Multiple Hierarchical Power Prior Model,[0],[0]
In this model the ρ (i) t ’s are pair-wise independent.,4.4. The Multiple Hierarchical Power Prior Model,[0],[0]
"The latter ensures that optimizing the L̂ can be performed as above, since the variational distribution for each ρ(i)t can be updated independently of the other variational distributions over ρ(j)t , for j 6= i.",4.4. The Multiple Hierarchical Power Prior Model,[0],[0]
"This extended model allows local model substructures to have different forgetting mechanisms, thereby extending the expressivity of the model.",4.4. The Multiple Hierarchical Power Prior Model,[0],[0]
We shall refer to this extended model as a multiple hierarchical power prior (MHPP) model.,4.4. The Multiple Hierarchical Power Prior Model,[0],[0]
"In this section we will evaluate the following methods:
• Streaming variational Bayes (SVB).",5.1. Experimental Set-up,[0],[0]
"• Four versions of Population Variational Bayes
(PVB)2: Population-size M equal to the average size of each data-batch, or M equal to a fixed value (M = 1000 in Section 5.2 and M = 10 000 in Section 5.3).",5.1. Experimental Set-up,[0],[0]
Learning-rate ν = 0.1 or ν = 0.01.,5.1. Experimental Set-up,[0],[0]
• Two versions of SVB-PP: ρ = 0.9 or ρ = 0.99.,5.1. Experimental Set-up,[0],[0]
"• Two versions of SVB-HPP: A single shared ρ (de-
noted SVB-HPP) or separate ρ(i) parameters (SVBMHPP).
",5.1. Experimental Set-up,[0],[0]
"The underlying variational engine is the VMP algorithm (Winn & Bishop, 2005) for all models; VMP was termi-
2We do not compare with SVI, because SVI is a special case of PVB when M is equal to the total size of the stream.
nated after 100 iterations or if the relative increase in the lower bound fell below 0.01%.",5.1. Experimental Set-up,[0],[0]
"All priors were uninformative, using either flat Gaussians, flat Gamma priors or uniform Dirichlet priors.",5.1. Experimental Set-up,[0],[0]
We set γ = 0.1 for the HPP priors.,5.1. Experimental Set-up,[0],[0]
Variational parameters were randomly initialized using the same seed for all methods.,5.1. Experimental Set-up,[0],[0]
"First, we illustrate the behavior of the different approaches in a controlled experimental setting: We produced an artificial data stream by generating 100 samples (i.e., |xt| = 100) from a Binomial distribution at each time step.",5.2. Evaluation using an Artificial Data Set,[0],[0]
"We artificially introduce concept drift by changing the parameter p of the Binomial distribution: p = 0.2 for the first 30 time steps, then p = 0.5 for the following 30 time steps, and finally p = 0.8 for the last 40 time steps.",5.2. Evaluation using an Artificial Data Set,[0],[0]
"The data stream was modelled using a Beta-Binomial model.
",5.2. Evaluation using an Artificial Data Set,[0],[0]
Parameter Estimation: Figure 3 shows the evolution of Eq[βt] for the different methods.,5.2. Evaluation using an Artificial Data Set,[0],[0]
"We recognize that SVB simply generates a running average of the data, as it is not able to adapt to the concept drift.",5.2. Evaluation using an Artificial Data Set,[0],[0]
"The results from PVB depend heavily on the learning rate ν, where the higher learning rate, which results in the more aggressive forgetting, works better in this example.",5.2. Evaluation using an Artificial Data Set,[0],[0]
"Recall, though, that ν needs to be hand-tuned to achieve an optimal performance.",5.2. Evaluation using an Artificial Data Set,[0],[0]
"As expected, the choice of M does not have an impact, because the present model has no local hidden variables (cf. Section 3).",5.2. Evaluation using an Artificial Data Set,[0],[0]
"SVB-PP produces results almost identical to PVB when ρ matches the learning rate of PVB (i.e., ρ = 1 − ν).",5.2. Evaluation using an Artificial Data Set,[0],[0]
"Finally, SVB-HPP provides the best results, almost mirroring the true model.
",5.2. Evaluation using an Artificial Data Set,[0],[0]
"Equivalent Sample Size (ESS): Figure 4 (left) gives the evolution of the equivalent sample size, ESSt, for the different methods 3.",5.2. Evaluation using an Artificial Data Set,[0],[0]
The ESS of PVB is always given by the constant M .,5.2. Evaluation using an Artificial Data Set,[0],[0]
"For SVB, the ESS monotonically increases as more data is seen, while SVB-PP exhibits convergence to the limiting value computed in Equation (8).",5.2. Evaluation using an Artificial Data Set,[0],[0]
A different behaviour is observed for SVB-HPP:,5.2. Evaluation using an Artificial Data Set,[0],[0]
"It is automatically ad-
3For this model, ESS is simply computed by summing up the components of the λt defining the Beta posterior.
justed.",5.2. Evaluation using an Artificial Data Set,[0],[0]
Notice that the values for this model is to be read off the alternative y-axis.,5.2. Evaluation using an Artificial Data Set,[0],[0]
"We can detect the the concept drift, by identifying where the ESS rapidly declines.
",5.2. Evaluation using an Artificial Data Set,[0],[0]
Evolution of Expected Forgetting factor: In Figure 4 (right) the series denoted “E[ρ]−100” shows the evolution of Eq[ρt] for the artificial data set.,5.2. Evaluation using an Artificial Data Set,[0],[0]
Notice how the model clearly identifies abrupt concept drift at time steps t = 30 and t = 60.,5.2. Evaluation using an Artificial Data Set,[0],[0]
The series denoted “E[ρ],5.2. Evaluation using an Artificial Data Set,[0],[0]
− 1000” illustrates the evolution of the parameter when we increase the batch size to 1000 samples.,5.2. Evaluation using an Artificial Data Set,[0],[0]
We recognize a more confident assessment about the absence of concept drift as more data is made available.,5.2. Evaluation using an Artificial Data Set,[0],[0]
"For this evaluation we consider three real data sets from different domains:
Electricity Market (Harries, 1999):",5.3.1. DATA AND MODELS,[0],[0]
The data set describes the electricity market of two Australian states.,5.3.1. DATA AND MODELS,[0],[0]
"It contains 45312 instances of 6 attributes, including a class label comparing the change of the electricity price related to a moving average of the last 24 hours.",5.3.1. DATA AND MODELS,[0],[0]
"Each instance in the data set represents 30 minutes of trading; during our analysis we created batches such that xt contains all information associated with month t.
The data is analyzed using a Bayesian linear regression model.",5.3.1. DATA AND MODELS,[0],[0]
The binary class label is assumed to follow a Gaussian distribution in order to fit within the conjugate model class.,5.3.1. DATA AND MODELS,[0],[0]
"Similarly, the marginal densities of the predictive attributes are also assumed to be Gaussian.",5.3.1. DATA AND MODELS,[0],[0]
"The regression coefficients are given Gaussian prior distributions, and the variance is given a Gamma prior.",5.3.1. DATA AND MODELS,[0],[0]
"Note that the overall distribution does not fall inside the conditional conjugate exponential family (Hoffman et al., 2013), hence PVB cannot be applied here, because lower-bound’s gradient cannot be computed in closed-form.
",5.3.1. DATA AND MODELS,[0],[0]
"GPS (Zheng et al., 2008; 2009; 2010):",5.3.1. DATA AND MODELS,[0],[0]
"This data set contains 17 621 GPS trajectories (time-stamped x and y coordinates), totalling more than 4.5 million observations.",5.3.1. DATA AND MODELS,[0],[0]
To reduce the data-size we kept only one out of every ten measurements.,5.3.1. DATA AND MODELS,[0],[0]
"We grouped the data so that xt contains all data collected during hour t of the day, giving a total of 24 batches of this stream.
",5.3.1. DATA AND MODELS,[0],[0]
"Here we employ a model with one independent Gaussian mixture model per day of the week, each mixture with 5 components.",5.3.1. DATA AND MODELS,[0],[0]
"This enables us to track changes in the users’ profiles across hours of the day, and also to monitor how the changes are affected by the day of the week.
",5.3.1. DATA AND MODELS,[0],[0]
"Finance (reference withheld): The data contains monthly aggregated information about the financial profile of
around 50 000 customers over 62 (non-consecutive) months.",5.3.1. DATA AND MODELS,[0],[0]
"Three attributes were extracted per customer, in addition to a class-label telling whether or not the customer will default within the next 24 months.
",5.3.1. DATA AND MODELS,[0],[0]
"We fit a naı̈ve Bayes model to this data set, where the distribution at the leaf-nodes is 5-component mixture of Gaussians distribution.",5.3.1. DATA AND MODELS,[0],[0]
"The distribution over the mixture node is shared by all the attributes, but not between the two classes of customers.
",5.3.1. DATA AND MODELS,[0],[0]
"A detailed description of all the models, including their structure and their variational families, is given at the supplementary material.",5.3.1. DATA AND MODELS,[0],[0]
"To evaluate the different methods discussed, we look at the test marginal log-likelihood (TMLL).",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"Specifically, each data batch is randomly split in a train data set, xt, and a test data set, x̃t, containing two thirds and one third of the data batch, respectively.",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"Then, TMLLt is computed as TMLLt = 1|x̃t| ∫ p(x̃t, zt|βt)p(βt|xt)dztdβt.",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
Figure 5 (left) shows for each method the difference between its TMLLt and that obtained by SVB (which is considered the baseline method).,5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"To improve readability, we only plot the results of the best performing method inside each group of methods.",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
The right-hand side of Figure 5 shows the development of Eq[ρt] over time for SVB-HPP and SVB-MHPP.,5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"For SVB-HPP we only have one ρt-parameter, and its value is given by the solid line.",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
SVB-MHPP utilizes one ρ(i) for each variational parameter.4,5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"In this case, we plot Eq[ρ(i)t ] at each point in time to indicate the variability between the different estimates throughout the series.",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"Finally, we compute each method’s aggregated test marginal log-likelihood measure ∑T t=1 TMLLt, and report these values in Table 1.
",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"For the electricity data set, we can see that the two proposed methods (SVB-HPP and SVB-MHPP) perform best.",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"All models are comparable during the first nine months, which is a period where our models detect no or very limited con-
4The numbers of variational parameters are 14, 78 and 33 for the Electricity, GPS and Financial model, respectively.
",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
cept drift (cf. top right plot or Figure 5).,5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"However, after this period, both SVB-HPP and SVB-MHPP detects substantial drift, and is able to adapt better than the other methods, which appear unable to adjust to the complex concept drift structure in the latter part of the data.",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"SVB-HPP and SVB-MHPP continue to behave at a similar level, mainly because when drift happens it typically includes a high proportion of the parameters of the model.
",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"For the GPS data set, we can observe how the SVB-MHPP is superior to the rest of the methods, particularly towards the end of the series.",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"When looking at Figure 5 (middle right panel), we can see that a significative proportion of the model parameters are drifting (i.e., Eq[ρ(i)t ]",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
≤ 0.05),5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"at all times, while another proportion of the parameters show a quite stable behavior (ρ-values above 0.9)",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
.,5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"This complex pattern is not captured well by SVB-HPP, which ends up assuming no concept drift after the initial time-step.
",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
The financial data set shows a different behavior.,5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"During the first months, SVB-MHPP slightly outperforms the rest of the approaches, but after month 30, SVB-PP with ρ = 0.9 is superior, with SVB-MHPP second.",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"Looking at the E[ρ(i)t ]-values of SVB-MHPP, we observe that there is significant concept drift in some of the parameters over the first few months.",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"However, only a few parameters exhibit noteworthy drift after the first third of the sequence.",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"Apparently, the simple SVB-PP approach has the upper hand when the drift is constant and fairly limited, at least when the optimal forgetting factor ρ has been identified.
",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"We conclude this section by highlighting that the performance of SVB-PP and PVB depend heavily on the hyperparameters of the model, cf. Table 1.",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"As an example, consider SVB-PP for the financial data set.",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"While it was the best overall with ρ = 0.9, it is inferior to SVB-MHPP if ρ = 0.99.",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"Similarly, PVB’s performance is sensitive both to ν (see in particular the results for the GPS data) and M (financial data).",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"These hyper-parameters are hard to fix, as their optimal values depend on data characteristics (see Broderick et al. (2013); McInerney et al. (2015) for similar conclusions).",5.3.2. EVALUATION AND DISCUSSION,[0],[0]
We therefore believe that the fully Bayesian formulation is an important strong point of our approach.,5.3.2. EVALUATION AND DISCUSSION,[0],[0]
"We have introduced a new class of Bayesian models for streaming data, able to capture changes in the underlying generative process.",6. Conclusions and Future Work,[0],[0]
"Unlike existing solutions to this problem, aimed at modeling slowly changing processes, our proposal is able to handle both abrupt and gradual concept drift following a Bayesian approach.",6. Conclusions and Future Work,[0],[0]
The new model accounts for the dynamics of the data stream by assuming that only the global parameters evolve over time.,6. Conclusions and Future Work,[0],[0]
"We intro-
duce the so-called hierarchical power priors, where a prior on the learning rate is given allowing it to adapt to the data stream.",6. Conclusions and Future Work,[0],[0]
"We have addressed the complexity of the underlying inference tasks by developing an approximate variational inference scheme that optimizes a novel lower bound of the likelihood function.
",6. Conclusions and Future Work,[0],[0]
As future work we aim to provide a sound approach to semantically characterize concept drift by inspecting the E[ρ(i)t ] values provided by SVB-MHPP.,6. Conclusions and Future Work,[0],[0]
This work was partly carried out as part of the AMIDST project.,Acknowledgements,[0],[0]
"AMIDST has received funding from the European Union’s Seventh Framework Programme for research, technological development and demonstration under grant agreement no 619209.",Acknowledgements,[0],[0]
"Furthermore, this research has been partly funded by the Spanish Ministry of Economy and Competitiveness, through projects TIN2015-74368JIN, TIN2013-46638-C3-1-P, TIN2016-77902-C3-3-P and by ERDF funds.",Acknowledgements,[0],[0]
Making inferences from data streams is a pervasive problem in many modern data analysis applications.,abstractText,[0],[0]
"But it requires to address the problem of continuous model updating, and adapt to changes or drifts in the underlying data generating distribution.",abstractText,[0],[0]
"In this paper, we approach these problems from a Bayesian perspective covering general conjugate exponential models.",abstractText,[0],[0]
Our proposal makes use of non-conjugate hierarchical priors to explicitly model temporal changes of the model parameters.,abstractText,[0],[0]
We also derive a novel variational inference scheme which overcomes the use of non-conjugate priors while maintaining the computational efficiency of variational methods over conjugate models.,abstractText,[0],[0]
The approach is validated on three real data sets over three latent variable models.,abstractText,[0],[0]
Bayesian Models of Data Streams with Hierarchical Power Priors,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2100–2105, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
NLP researchers and practitioners spend a considerable amount of time comparing machine-learned models of text that differ in relatively uninteresting ways.,1 Introduction,[0],[0]
"For example, in categorizing texts, should the “bag of words” include bigrams, and is tf-idf weighting a good idea?",1 Introduction,[0],[0]
"In learning word embeddings, distributional similarity approaches have been shown to perform competitively with neural network models when the hyperparameters (e.g., context window, subsampling rate, smoothing constant) are carefully tuned (Levy et al., 2015).",1 Introduction,[0],[0]
"These choices matter experimentally, often leading to big differences in performance, with little consistency across tasks and datasets in which combination of choices works best.",1 Introduction,[0],[0]
"Unfortunately, these differences tell us little about language or the problems that machine learners are supposed to solve.
",1 Introduction,[0],[0]
"We propose that these decisions can be automated in a similar way to hyperparameter selection (e.g., choosing the strength of a ridge or lasso regularizer).",1 Introduction,[0],[0]
"Given a particular text dataset and classification task, we show a technique for optimizing over the space of representational choices, along
with other “nuisances” that interact with these decisions, like hyperparameter selection.",1 Introduction,[0],[0]
"For example, using higher-order n-grams means more features and a need for stronger regularization and more training iterations.",1 Introduction,[0],[0]
"Generally, these decisions about instance representation are made by humans, heuristically; our work seeks to automate them, not unlike Daelemans et al. (2003), who proposed to use genetic algorithms to optimize representational choices.
",1 Introduction,[0],[0]
"Our technique instantiates sequential modelbased optimization (SMBO; Hutter et al., 2011).",1 Introduction,[0],[0]
"SMBO and other Bayesian optimization approaches have been shown to work well for hyperparameter tuning (Bergstra et al., 2011; Hoffman et al., 2011; Snoek et al., 2012).",1 Introduction,[0],[0]
"Though popular in computer vision (Bergstra et al., 2013), these techniques have received little attention in NLP.
",1 Introduction,[0],[0]
We apply it to logistic regression on a range of topic and sentiment classification tasks.,1 Introduction,[0],[0]
"Consistently, our method finds representational choices that perform better than linear baselines previously reported in the literature, and that, in some cases, are competitive with more sophisticated non-linear models trained using neural networks.",1 Introduction,[0],[0]
"Let the training data consist of a collection of pairs dtrain = 〈〈d.i1, d.o1〉, . . .",2 Problem Formulation and Notation,[0],[0]
", 〈d.in, d.on〉〉, where each input d.i ∈ I is a text document and each output",2 Problem Formulation and Notation,[0],[0]
d.o ∈,2 Problem Formulation and Notation,[0],[0]
"O, the output space.",2 Problem Formulation and Notation,[0],[0]
"The overall training goal is to maximize a performance function f (e.g., classification accuracy, log-likelihood, F1 score, etc.) of a machine-learned model, on a held-out dataset, ddev ∈ (I× O)n′ .
",2 Problem Formulation and Notation,[0],[0]
"Classification proceeds in three steps: first, x : I → RN maps each input to a vector representation.",2 Problem Formulation and Notation,[0],[0]
"Second, a predictive model (typically, its parameters) is learned from the inputs (now transformed into vectors) and outputs:",2 Problem Formulation and Notation,[0],[0]
L : (RN × O)n,2 Problem Formulation and Notation,[0],[0]
→ (RN → O).,2 Problem Formulation and Notation,[0],[0]
"Finally, the resulting classifier c :",2 Problem Formulation and Notation,[0],[0]
I → O is fixed as L(dtrain) ◦,2 Problem Formulation and Notation,[0],[0]
"x (i.e., the composition of the representation function with
2100
the learned mapping).",2 Problem Formulation and Notation,[0],[0]
Here we consider linear classifiers of the form c(d.i) = arg maxo∈O,2 Problem Formulation and Notation,[0],[0]
"w>o x(d.i), where the parameters wo ∈ RN , for each output o, are learned using logistic regression on the training data.",2 Problem Formulation and Notation,[0],[0]
We let w denote the concatenation of all wo.,2 Problem Formulation and Notation,[0],[0]
Hence the parameters can be understood as a function of the training data and the representation function x.,2 Problem Formulation and Notation,[0],[0]
"The performance function f , in turn, is a function of the held-out data ddev and x—also w and dtrain , through x.",2 Problem Formulation and Notation,[0],[0]
"For simplicity, we will write “f(x)” when the rest are clear from context.
",2 Problem Formulation and Notation,[0],[0]
"Typically, x is fixed by the model designer, perhaps after some experimentation, and learning focuses on selecting the parameters w. For logistic regression and many other linear models, this training step reduces to convex optimization in N |O| dimensions—a solvable problem that is costly for large datasets and/or large output spaces.",2 Problem Formulation and Notation,[0],[0]
"In seeking to maximize f with respect to x, we do not wish to carry out training any more times than necessary.
",2 Problem Formulation and Notation,[0],[0]
Choosing x can be understood as a problem of selecting hyperparameter values.,2 Problem Formulation and Notation,[0],[0]
"We therefore turn to Bayesian optimization, a family of techniques that can be used to select hyperparameter values intelligently when solving for parameters (w) is costly.",2 Problem Formulation and Notation,[0],[0]
"Our approach is based on sequential model-based optimization (SMBO; Hutter et al., 2011).",3 Bayesian Optimization,[0],[0]
It iteratively chooses representation functions x.,3 Bayesian Optimization,[0],[0]
"On each round, it makes this choice through a probabilistic model of f , then evaluates f—we call this a “trial.”",3 Bayesian Optimization,[0],[0]
"As in any iterative search algorithm, the goal is to balance exploration of options for x with exploitation of previously-explored options, so that a good choice is found in a small number of trials.
",3 Bayesian Optimization,[0],[0]
"More concretely, in the tth trial, xt is selected using an acquisition function A and a “surrogate” probabilistic model pt.",3 Bayesian Optimization,[0],[0]
"Second, f is evaluated given xt—an expensive operation which involves training to learn parameters w and assessing performance on the held-out data.",3 Bayesian Optimization,[0],[0]
"Third, the surrogate model is updated.",3 Bayesian Optimization,[0],[0]
"See Algorithm 1; details on A and pt follow.
",3 Bayesian Optimization,[0],[0]
Acquisition Function.,3 Bayesian Optimization,[0],[0]
"A good acquisition function returns high values for x when either the value f(x) is predicted to be high, or the uncertainty about f(x)’s value is high; balancing between these is the classic tradeoff between exploitation
Algorithm 1",3 Bayesian Optimization,[0],[0]
"SMBO algorithm Input: number of trials T , target function f p1 = initial surrogate model Initialize y∗
for t = 1 to T do xt ← arg maxx A(x; pt, y∗) yt",3 Bayesian Optimization,[0],[0]
← evaluate f(xt),3 Bayesian Optimization,[0],[0]
"Update y∗
Estimate pt given x1:t and y1:",3 Bayesian Optimization,[0],[0]
"t end for
and exploration.",3 Bayesian Optimization,[0],[0]
"We use a criterion called Expected Improvement (EI; Jones, 2001),1 which is the expectation (under the current surrogate model pt) that f(x) = y will exceed f(x∗) = y∗:
A(x; pt, y∗) = ∫ ∞ −∞ max(y",3 Bayesian Optimization,[0],[0]
"− y∗, 0)pt(y",3 Bayesian Optimization,[0],[0]
"| x)dy
where x∗ is chosen depending on the surrogate model, discussed below.",3 Bayesian Optimization,[0],[0]
"(For now, think of it as a strongly-performing “benchmark” discovered in earlier iterations.)",3 Bayesian Optimization,[0],[0]
"Other options for the acquisition function include maximum probability of improvement (Jones, 2001), minimum conditional entropy (Villemonteix et al., 2009), Gaussian process upper confidence bound (Srinivas et al., 2010), or a combination of them (Hoffman et al., 2011).
",3 Bayesian Optimization,[0],[0]
Surrogate Model.,3 Bayesian Optimization,[0],[0]
"As a surrogate model, we use a tree-structured Parzen estimator (TPE; Bergstra et al., 2011).2",3 Bayesian Optimization,[0],[0]
This is a nonparametric approach to density estimation.,3 Bayesian Optimization,[0],[0]
"We seek to estimate pt(y | x) where y = f(x), the performance function that is expensive to compute exactly.",3 Bayesian Optimization,[0],[0]
"The TPE approach
seeks pt(y | x) ∝",3 Bayesian Optimization,[0],[0]
"pt(y) · { p<t (x), if y<y ∗
p≥t (x), if y≥y∗ , where
p<t and p ≥ t are densities estimated using observations from previous trials that are less than and greater than y∗, respectively.",3 Bayesian Optimization,[0],[0]
"In TPE, y∗ is defined as some quantile of the observed y from previous trials; we use 15-quantiles.
",3 Bayesian Optimization,[0],[0]
"As shown by Bergstra et al. (2011), the Expected Improvement in TPE can be written as:
1EI is the most widely used acquisition function that has been shown to work well on a range of tasks.
",3 Bayesian Optimization,[0],[0]
"2Another common approach to the surrogate is the Gaussian process (Rasmussen and Williams, 2006; Hoffman et al., 2011; Snoek et al., 2012).",3 Bayesian Optimization,[0],[0]
"Like Bergstra et al. (2011), our preliminary experiments found the TPE to perform favorably.",3 Bayesian Optimization,[0],[0]
"Further TPE’s tree-structured configuration space is advantageous, because it allows nested definitions of hyperparameters, which we exploit in our experiments (e.g., only allows bigrams to be chosen if unigrams are also chosen).
",3 Bayesian Optimization,[0],[0]
"A(x; pt, y∗) ∝",3 Bayesian Optimization,[0],[0]
"( γ + p < t (x)
p≥t (x) (1− γ)
)−1 , where
γ = pt(y < y∗), fixed at 0.15 by definition of y∗ (above).",3 Bayesian Optimization,[0],[0]
"Here, we prefer x with high probability under p≥t (x) and low probability under p < t (x).",3 Bayesian Optimization,[0],[0]
"To maximize this quantity, we draw many candidates according to p≥t (x) and evaluate them according to p<t (x)/p ≥ t (x).",3 Bayesian Optimization,[0],[0]
Note that p(y) does not need to be given an explicit form.,3 Bayesian Optimization,[0],[0]
"To compute p<t (x) and p≥t (x), we associate each hyperparameter with a node in the graphical model and multiply individual probabilities at every node—see Bergstra et al. (2011) for details.",3 Bayesian Optimization,[0],[0]
We fix L to logistic regression.,4 Experiments,[0],[0]
"We optimize text representation based on the types of n-grams used, the type of weighting scheme, and the removal of stopwords; we also optimize the regularizer and training convergence criterion, which interact with the representation.",4 Experiments,[0],[0]
"See Table 1 for a complete list.
",4 Experiments,[0],[0]
"Note that even with this limited number of options, the number of possible combinations is huge,3 so exhaustive search is computationally expensive.",4 Experiments,[0],[0]
"In all our experiments for all datasets, we limit ourselves to 30 trials per dataset.",4 Experiments,[0],[0]
"The only preprocessing we applied was downcasing.
",4 Experiments,[0],[0]
We always use a development set to evaluate f(x) during learning and report the final result on an unseen test set.,4 Experiments,[0],[0]
"We summarize the hyperparameters selected by our method, and the accuracies achieved (on test data) in Table 5.",4 Experiments,[0],[0]
We discuss comparisons to baselines for each dataset in turn.,4 Experiments,[0],[0]
"For each of our datasets, we select supervised, nonensemble classification methods from previous literature as baselines.",4 Experiments,[0],[0]
"In each case, we emphasize comparisons with the best-published linear method
3It is actually infinite since the reg. strength and conv.",4 Experiments,[0],[0]
"tolerance are continuous values, but we could discretize them.
",4 Experiments,[0],[0]
(often an SVM with a linear kernel with representation selected by experts) and the best-published method overall.,4 Experiments,[0],[0]
"In the following, “SVM” always means “linear SVM.”",4 Experiments,[0],[0]
"All methods were trained and evaluated on the same training/testing splits as baselines; in cases where standard development sets were not available, we used a random 20% of the training data as a development set.
",4 Experiments,[0],[0]
"Stanford sentiment treebank (Socher et al., 2013)—Table 2.",4 Experiments,[0],[0]
A sentence-level sentiment analysis dataset of rottentomatoes.com movie reviews: http://nlp.stanford.edu/sentiment.,4 Experiments,[0],[0]
We use the binary classification task where the goal is to predict whether a review is positive or negative (no neutral).,4 Experiments,[0],[0]
"Our logistic regression model outperforms the baseline SVM reported by Socher et al. (2013), who used only unigrams but did not specify the weighting scheme for their SVM baseline.",4 Experiments,[0],[0]
"While our result is still below the state-of-the-art based on the the recursive neural tensor networks (Socher et al., 2013) and the paragraph vector (Le and Mikolov, 2014), we show that logistic regression is comparable with recursive and matrix-vector neural networks (Socher et al., 2011; Socher et al., 2012).
",4 Experiments,[0],[0]
"Amazon electronics (McAuley and Leskovec, 2013)—Table 3.",4 Experiments,[0],[0]
A binary sentiment analysis dataset of Amazon electronics product reviews: http://riejohnson.com/cnn data.html.,4 Experiments,[0],[0]
"The bestperforming methods on this dataset are based on convolutional neural networks (Johnson and Zhang, 2015).4 Our method is on par with the secondbest of these, outperforming all of the reported feed-forward neural networks and SVM variants Johnson and Zhang used as baselines.",4 Experiments,[0],[0]
"They varied
4These are convolutional neural networks with a rectifier activation function, trained under `2 regularization with stochastic gradient descent.",4 Experiments,[0],[0]
"The authors also consider an extension based on parallel CNN that we do not include here.
",4 Experiments,[0],[0]
"the representations, and used log term frequency and normalization to unit vectors as the weighting scheme, after finding that this outperformed term frequency.",4 Experiments,[0],[0]
"Our method achieved the best performance with binary weighting, which they did not consider.
",4 Experiments,[0],[0]
"IMDB movie reviews (Maas et al., 2011)— Table 3.",4 Experiments,[0],[0]
A binary sentiment analysis dataset of highly polar IMDB movie reviews: http://ai.stanford.edu/~amaas/data/sentiment.,4 Experiments,[0],[0]
"The results parallel those for Amazon electronics; our method comes close to convolutional neural networks (Johnson and Zhang, 2015), which are state-of-the-art.5 It outperforms SVMs and feed-forward neural networks, the restricted Boltzmann machine approach presented by Dahl et al. (2012), and compressive feature learning (Paskov et al., 2013).6
Congressional vote (Thomas et al., 2006)—Table 4.",4 Experiments,[0],[0]
A dataset of transcripts from the U.S. Congressional debates: http://www.cs.cornell.edu/~ainur/sle-data.html.,4 Experiments,[0],[0]
"Similar to previous work (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010), we consider the task to predict the vote (“yea” or “nay”) for the speaker of each speech segment (speaker-based speech-segment classification).",4 Experiments,[0],[0]
"Our method outperforms the best results of Yessenalina et al. (2010), which use a multi-level structured
5As noted, semi-supervised and ensemble methods are excluded for a fair comparison.
",4 Experiments,[0],[0]
"6This approach is based on minimum description length, using unlabeled data to select a set of higher-order n-grams to use as features.
model based on a latent-variable SVM.",4 Experiments,[0],[0]
We show comparisons to two weaker baselines as well.,4 Experiments,[0],[0]
20 Newsgroups is a benchmark topic classification dataset: http://qwone.com/~jason/20Newsgroups.,"20 Newsgroups (Lang, 1995) all topics—Table 6.",[0],[0]
There are 20 topics in this dataset.,"20 Newsgroups (Lang, 1995) all topics—Table 6.",[0],[0]
"Our method outperforms state-of-the-art methods including the distributed structured output model (Srikumar and Manning, 2014).7 The strong logistic regression baseline from Paskov et al. (2013) uses all 5-grams, heuristic normalization, and elastic net regularization; our method found that unigrams and bigrams, with binary weighting and `2 penalty, achieved far better results.
20 Newsgroups: talk.religion.misc vs. alt.atheism and comp.graphics vs. comp.windows.x.","20 Newsgroups (Lang, 1995) all topics—Table 6.",[0],[0]
We derived three additional topic classification tasks from the 20N dataset.,"20 Newsgroups (Lang, 1995) all topics—Table 6.",[0],[0]
The first and second tasks are talk.religion.misc vs. alt.atheism (test size = 686) and comp.graphics vs. comp.windows.x (test size = 942).,"20 Newsgroups (Lang, 1995) all topics—Table 6.",[0],[0]
"Wang and Manning (2012) report a bigram naı̈ve Bayes model achieving 85.1% and 91.2% on these tasks, respectively (best single model results).8 Our
7This method was designed for structured prediction, but Srikumar and Manning (2014) also applied it to classification.","20 Newsgroups (Lang, 1995) all topics—Table 6.",[0],[0]
It attempts to learn a distributed representation for features and for labels.,"20 Newsgroups (Lang, 1995) all topics—Table 6.",[0],[0]
"The authors used unigrams and did not discuss the weighting scheme.
","20 Newsgroups (Lang, 1995) all topics—Table 6.",[0],[0]
"8They also report a naı̈ve Bayes/SVM ensemble achieving 87.9% and 91.2%.
method achieves 86.3% and 92.1% using slightly different representations (see Table 5).","20 Newsgroups (Lang, 1995) all topics—Table 6.",[0],[0]
"The last task is to classify related science documents into four science topics (sci.crypt, sci.electronics, sci.space, sci.med; test size = 1, 899).","20 Newsgroups (Lang, 1995) all topics—Table 6.",[0],[0]
We were not able to find previous results that are comparable to ours on this task; we include our result (95.82%) to enable further comparisons in the future.,"20 Newsgroups (Lang, 1995) all topics—Table 6.",[0],[0]
Optimized representations.,5 Discussion,[0],[0]
"For each task, the chosen representation is different.",5 Discussion,[0],[0]
"Out of all possible choices in our experiments (Table 1), each of them is used by at least one of the datsets (Table 5).",5 Discussion,[0],[0]
"For example, on the Congress vote dataset, we only need to use bigrams, whereas on the Amazon electronics dataset we need to use {1, 2, 3}-grams.",5 Discussion,[0],[0]
"The binary weighting scheme works well for most of the datasets, except the sentence-level sentiment analysis task, where the tf-idf weighting scheme was selected.",5 Discussion,[0],[0]
`2 regularization was best in all cases but one.,5 Discussion,[0],[0]
"We do not believe that an NLP expert would be likely to make these particular choices, except through the same kind of trial-and-error process our method automates efficiently.
",5 Discussion,[0],[0]
Number of trials.,5 Discussion,[0],[0]
We ran 30 trials for each dataset in our experiments.,5 Discussion,[0],[0]
Figure 1 shows each trial accuracy and the best accuracy on development data as we increase the number of trials for two datasets.,5 Discussion,[0],[0]
"We can see that 30 trials are generally enough for the model to obtain good results, although the search space is large.
",5 Discussion,[0],[0]
Transfer learning and multitask setting.,5 Discussion,[0],[0]
We treat each dataset independently and create a separate model for each of them.,5 Discussion,[0],[0]
"It is also possible to learn from previous datasets (i.e., transfer learning) or to learn from all datasets simultaneously (i.e., multitask learning) to improve performance.",5 Discussion,[0],[0]
"This has the potential to reduce the number of trials
required even further.",5 Discussion,[0],[0]
"See Bardenet et al. (2013), Swersky et al. (2013), and Yogatama and Mann (2014) for more about how to perform Bayesian optimization in these settings.
",5 Discussion,[0],[0]
Beyond supervised learning.,5 Discussion,[0],[0]
Our framework could also be extended to unsupervised and semisupervised models.,5 Discussion,[0],[0]
"For example, in document clustering (e.g., k-means), we also need to construct representations for documents.",5 Discussion,[0],[0]
Log-likelihood might serve as a performance function.,5 Discussion,[0],[0]
A range of random initializations might be considered.,5 Discussion,[0],[0]
Investigation of this approach for nonconvex problems is an exciting area for future work.,5 Discussion,[0],[0]
We used Bayesian optimization to optimize choices about text representations for various categorization problems.,6 Conclusion,[0],[0]
Our technique identifies settings for a standard linear model (logistic regression) that are competitive with far more sophisticated methods on topic classification and sentiment analysis.,6 Conclusion,[0],[0]
We thank several reviewers for their helpful feedback.,Acknowledgments,[0],[0]
This work was supported by the Defense Advanced Research Projects Agency through grant FA87501420244 and computing resources provided by Amazon.,Acknowledgments,[0],[0]
This research was completed while NAS was at CMU.,Acknowledgments,[0],[0]
"When applying machine learning to problems in NLP, there are many choices to make about how to represent input texts.",abstractText,[0],[0]
"They can have a big effect on performance, but they are often uninteresting to researchers or practitioners who simply need a module that performs well.",abstractText,[0],[0]
"We apply sequential model-based optimization over this space of choices and show that it makes standard linear models competitive with more sophisticated, expensive state-ofthe-art methods based on latent variables or neural networks on various topic classification and sentiment analysis problems.",abstractText,[0],[0]
Our approach is a first step towards black-box NLP systems that work with raw text and do not require manual tuning.,abstractText,[0],[0]
Bayesian Optimization of Text Representations,title,[0],[0]
"In recent years, Bayesian optimization has gained a growing attention from machine learning experts in, both, academia and industry (Shahriari et al., 2016).",1. Introduction,[0],[0]
"It takes the widespread application of machine learning to the next level of sophistication as it enables to automatically fine-tune hyperparameters (Snoek et al., 2012), whether they are parametrizing data pre-processors, models or the learning algorithms.",1. Introduction,[0],[0]
"Finetuning is essential to obtain state-of-the-art performance
1Amazon, Berlin, Germany.",1. Introduction,[0],[0]
"2Amazon, Cambridge, United Kingdom.",1. Introduction,[0],[0]
"Correspondence to: Rodolphe Jenatton <jenatton@amazon.de>, Cedric Archambeau <cedrica@amazon.de>, Javier Gonzalez <gojav@amazon.co.uk>, Matthias Seeger <matthias@amazon.de>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
with complex machine learning models, such as deep neural networks.",1. Introduction,[0],[0]
"Historically, this vital step has been done, either manually, or via regular or random grid search, which can consume vast amounts of human expert time and are wasteful of computing resources.",1. Introduction,[0],[0]
"Hence, one of the main benefits of Bayesian optimization is that it removes this burden from the shoulders of the practitioners, who can then focus their attention on more rewarding value-adding tasks.
",1. Introduction,[0],[0]
"To set the stage, our goal is to solve a global optimization problem:
min x∈X f(x),
where X is the optimization domain and f is a black-box function, typically continuous and multimodal.",1. Introduction,[0],[0]
We further assume that querying f is costly.,1. Introduction,[0],[0]
"For example, f may be the outcome of a physical experiment or require a large amount of computation.",1. Introduction,[0],[0]
"The latter arises when f corresponds to a model selection score for a machine learning model trained on a possibly large dataset.
",1. Introduction,[0],[0]
"The protocol for sequential Bayesian optimization proceeds as follows (Mockus et al., 1978; Shahriari et al., 2016).",1. Introduction,[0],[0]
"Given n noisy evaluations yi ≈ f(xi), i ∈ {1, . . .",1. Introduction,[0],[0]
",",1. Introduction,[0],[0]
"n}, a surrogate probabilistic model of f is maintained.",1. Introduction,[0],[0]
Our goal is to find a global optimum of f by querying it as few times as possible.,1. Introduction,[0],[0]
The location xn+1 is chosen by maximizing an acquisition function which performs an explorationexploitation trade-off.,1. Introduction,[0],[0]
"A common choice for the surrogate model is a Gaussian process (GP) (Rasmussen & Williams, 2006).",1. Introduction,[0],[0]
"For a GP surrogate model, common acquisition functions can be tractably computed and optimized via gradientbased optimization algorithms.",1. Introduction,[0],[0]
"While existing Bayesian optimization approaches mitigate the high evaluation cost of f , they suffer from the curse of dimensionality when facing a high-dimensional space X .
",1. Introduction,[0],[0]
"In this paper, we introduce a novel methodology able to exploit a given tree-shaped dependency structure on X by transferring information between overlapping paths.",1. Introduction,[0],[0]
"By constructing a surrogate model tailored to the structure, we can reduce the number of evaluations of commonly used acquisition functions.",1. Introduction,[0],[0]
"The same structure also allows us to take acquisition decisions more efficiently, thus speeding up the search of candidates.
",1. Introduction,[0],[0]
Tree-based dependencies occur often in practice.,1. Introduction,[0],[0]
"For exam-
ple, faced with a classification problem, we may want to simultaneously search over many different machine learning models, each coming with their own hyperparameters.",1. Introduction,[0],[0]
"Some configurations may also share parameters (e.g., logistic regression with `2 and `1 penalty may share the learning rate).",1. Introduction,[0],[0]
"These choices could be encoded in a decision tree, where inner nodes select between different models and hyperparameters populate leaf nodes.",1. Introduction,[0],[0]
"Another example arises when having to decide on a deep neural network architecture: the size of a layer, choice of activation function, or dropout fraction may depend on the number of layers (Bengio, 2009).",1. Introduction,[0],[0]
"A baseline approach to Bayesian optimization in this setting is to ignore the structure of X and, as a result, choose a GP with covariance kernel K(x,x′) defined over the joint input space.",1.1. Baselines and Related Work,[0],[0]
"When comparing a pair of points, all coordinates are taken into account.",1.1. Baselines and Related Work,[0],[0]
"While easy to run in existing Bayesian optimization toolboxes, this approach can be highly inefficient.",1.1. Baselines and Related Work,[0],[0]
"Not only do we encounter a cost of O(n3) after n acquisitions due to the global nature of the GP, but we also suffer from the curse of dimensionality when searching over X .",1.1. Baselines and Related Work,[0],[0]
"Several authors attempted to design covariance functions that are aware of the structure: Duvenaud et al. (2011) consider kernels with an additive structure, while Swersky et al. (2014a); Hutter & Osborne (2013) introduce the Arc-kernel.",1.1. Baselines and Related Work,[0],[0]
"However, the cost remains O(n3).
",1.1. Baselines and Related Work,[0],[0]
"Another idea is to consider an independent GP for every valid subset of hyperparameters, as proposed by Bergstra et al. (2011).",1.1. Baselines and Related Work,[0],[0]
This approach corresponds to having an independent GP per leaf in the dependency tree.,1.1. Baselines and Related Work,[0],[0]
"It scales as O( ∑ p n 3 p), where np is the number of evaluations at leaf
node p and ∑ p np = n.",1.1. Baselines and Related Work,[0],[0]
"However, it lacks a mechanism for information sharing across the leaves.",1.1. Baselines and Related Work,[0],[0]
"As we will show, information sharing can be beneficial in order to cut down on the number of evaluations.",1.1. Baselines and Related Work,[0],[0]
"Moreover, the independent approach requires a sizable number of evaluations at each leaf, which can be problematic when there are many leafs.
",1.1. Baselines and Related Work,[0],[0]
"Tree-structured dependencies can also be dealt with by assigning default values to coordinates of x which do not fall into the leaf node under consideration, using a Random Forest model to make this choice (Hutter et al., 2011).",1.1. Baselines and Related Work,[0],[0]
"This strategy is implemented in the SMAC library.
",1.1. Baselines and Related Work,[0],[0]
"Finally, Zhang et al. (2016) proposed a dedicated approach to tune data analytic pipelines, via a two-layer Bayesian optimization framework.",1.1. Baselines and Related Work,[0],[0]
"Their method first uses a parametric model to select some promising algorithms, whose hyperparameters are then refined by a nonparametric model.",1.1. Baselines and Related Work,[0],[0]
"First, we introduce a novel Bayesian optimization methodology able to leverage conditional dependencies between hyperparameters.",1.2. Contributions,[0],[0]
"To this end, we build a tree-structured surrogate model, with separate GPs at the leaf nodes, and random linear (or constant) functions at the inner nodes.",1.2. Contributions,[0],[0]
"This allows us to transfer information between leafs that share nodes on their respective paths, which enables us in turn to efficiently search the space X .",1.2. Contributions,[0],[0]
"Yet, we also retain the beneficial scaling of the independent approach (Bergstra et al., 2011).",1.2. Contributions,[0],[0]
"To our knowledge, no prior published work satisfied these two aspects.",1.2. Contributions,[0],[0]
"The Arc-kernel allows for information sharing, but comes with O(n3) computations.",1.2. Contributions,[0],[0]
"Hutter et al. (2011) rely on Random Forests to represent correlations, but no particular sharing mechanism exists.
",1.2. Contributions,[0],[0]
"Second, we introduce a novel acquisition function which is also able to exploit the tree structure and relies on the expected improvement (Mockus et al., 1978).",1.2. Contributions,[0],[0]
The acquisition operates in two steps.,1.2. Contributions,[0],[0]
"We first select the most promising leaf node to score, effectively restricting our attention to a portion of X .",1.2. Contributions,[0],[0]
We then optimize over all possible anchor points in this portion of space.,1.2. Contributions,[0],[0]
This can result in a drastic reduction in the number of surrogate functions to optimize over.,1.2. Contributions,[0],[0]
"In comparison, the independent baseline requires to score every anchor point of every leaf in the tree at each iteration.
",1.2. Contributions,[0],[0]
The paper is organized as follows.,1.2. Contributions,[0],[0]
"In Section 2, we detail our surrogate GP model and inference computations.",1.2. Contributions,[0],[0]
"In Section 3, we show how the model structure gives rise to efficient acquisition optimization.",1.2. Contributions,[0],[0]
"For a range of experiments on simulated and real data, we report in Section 4 favorable comparisons with existing alternatives.",1.2. Contributions,[0],[0]
We conclude with possible extensions in Section 5.,1.2. Contributions,[0],[0]
"We assume that the hyperparameters exhibit conditional dependencies, which can be modeled with decision tree T .",2. Tree-structured semi-parametric Gaussian process regression model,[0],[0]
"The set of inner nodes V is indexed by v ∈ {1, . . .",2. Tree-structured semi-parametric Gaussian process regression model,[0],[0]
", V }; each v has a decision variable and a weight variable cv.",2. Tree-structured semi-parametric Gaussian process regression model,[0],[0]
"The set of leaf nodes P is indexed by p ∈ {1, . . .",2. Tree-structured semi-parametric Gaussian process regression model,[0],[0]
", P}.",2. Tree-structured semi-parametric Gaussian process regression model,[0],[0]
"Equivalently, p indexes (unique) paths from the root to a leaf.
",2. Tree-structured semi-parametric Gaussian process regression model,[0],[0]
"Further, let Dn = {(xi, yi)}ni=1 be the set of observations.",2. Tree-structured semi-parametric Gaussian process regression model,[0],[0]
We introduce a set of auxiliary variables {pi | pi ∈ P}ni=1 that indicate the leaf to which observation i is associated and let np= |{i | pi=p}|.,2. Tree-structured semi-parametric Gaussian process regression model,[0],[0]
Note that xi∈Xpi since the input domain may vary from one leaf to another.,2. Tree-structured semi-parametric Gaussian process regression model,[0],[0]
"We consider a surrogate model that associates with leaf p a latent function gp with GP prior, whose mean function and covariance kernel are bp and Kp(x,x′).",2.1. Model with Random Inner Node Parameters,[0],[0]
We impose a zero-mean Gaussian prior over the weight vector c =,2.1. Model with Random Inner Node Parameters,[0],[0]
"[c1, . .",2.1. Model with Random Inner Node Parameters,[0],[0]
.,2.1. Model with Random Inner Node Parameters,[0],[0]
", cV ] T .
",2.1. Model with Random Inner Node Parameters,[0],[0]
"The resulting generative model is given by
c ∼ N (0,Σc), gp(·) ∼ GP ( bp,Kp ) ,
yi|pi, {gp(xi)}Pp=1, c ∼ N (gpi(xi) + z>pic, σ 2).",2.1. Model with Random Inner Node Parameters,[0],[0]
"(1)
where Σc, {bp}Pp=1 and σ2 are the prior covariance, the scalar offsets and the noise variance.",2.1. Model with Random Inner Node Parameters,[0],[0]
"Vector zp∈ {0, 1}V is a binary mask that activates the weights of the inner node decision variables on the path to leaf node p.",2.1. Model with Random Inner Node Parameters,[0],[0]
"In other words, (zp)v = 1 iff v lies on the path from the root to p.",2.1. Model with Random Inner Node Parameters,[0],[0]
"The prior covariance Σc will be diagonal in our experiments.
",2.1. Model with Random Inner Node Parameters,[0],[0]
"When c=0, model (1) boils down to assuming P independent GPs.",2.1. Model with Random Inner Node Parameters,[0],[0]
"While inference only scales as O( ∑ p n 3 p) in this case, information is not transferred between overlapping paths.",2.1. Model with Random Inner Node Parameters,[0],[0]
"Introducing the weight vector c allows us to couple inference for such paths, while keeping the favorable scaling and better exploring the optimization space (see Section 4).
",2.1. Model with Random Inner Node Parameters,[0],[0]
"Next, we show how to perform efficient inference in this model and give an interpretation of the induced kernel when computing the marginal likelihood.",2.1. Model with Random Inner Node Parameters,[0],[0]
Posterior inference over the surrogate models {gp(·)} and the random weights c is needed to compute the acquisition functions (see Section 3).,2.1. Model with Random Inner Node Parameters,[0],[0]
"Before starting, we need some notation.",2.2. Posterior Inference,[0],[0]
Let y ∈,2.2. Posterior Inference,[0],[0]
Rn be the vector of all observations and g ∈,2.2. Posterior Inference,[0],[0]
Rn the vector of latent function values at {xi}ni=1.,2.2. Posterior Inference,[0],[0]
"Further, let Ip = {i | pi = p}, noting that np = |Ip|.",2.2. Posterior Inference,[0],[0]
"We partition the data accordingly, so that yp =",2.2. Posterior Inference,[0],[0]
"[yi]i∈Ip , and similarly gp = [gp(xi)]i∈Ip .",2.2. Posterior Inference,[0],[0]
"Also, we define the matrix Zp = zp1>np ∈ R
V×np , where 1np =",2.2. Posterior Inference,[0],[0]
"[1] ∈ Rnp , and the vector bp = bp1np ∈ Rnp .
",2.2. Posterior Inference,[0],[0]
"The joint distribution P (y,g, c) of our model is given by P (c) ∏ pN (gp; bp,Kp)N (yp; gp + Z > p c, σ 2Inp), (2)
where Kp = [Kp(xi,xj)]i,j∈Ip are kernel matrices, with the prior P (c) = N (c; 0,Σc).",2.2. Posterior Inference,[0],[0]
"Our goal is to obtain the posterior process P (gp(·)|c,yp) and the posterior distribution P (c|y).
",2.2. Posterior Inference,[0],[0]
"We can directly read off the posterior over the latent functions and parameters after rewriting the joint distribution into the following form (see Section 2 of the Appendix for details):
P (y)P (c|y)",2.2. Posterior Inference,[0],[0]
"∏ pP (gp|c,yp).
",2.2. Posterior Inference,[0],[0]
"First, we obtain the posterior GP over the latent functions:
gp(·)|c,yp ∼ GP ( mp(·), Sp(·, ·) ) ,
where mp(x) =",2.2. Posterior Inference,[0],[0]
kp(x)>M−1p (yp − Z>p c,2.2. Posterior Inference,[0],[0]
"− bp) + bp, Sp(x,x
′)",2.2. Posterior Inference,[0],[0]
"= Kp(x,x′)",2.2. Posterior Inference,[0],[0]
"− kp(x)>M−1p kp(x′) and Mp = Kp + σ 2Inp .
",2.2. Posterior Inference,[0],[0]
"Next, we obtain the posterior for the weights c:
c|y",2.2. Posterior Inference,[0],[0]
"∼ N (Λ−1c fc,Λ −1 c ),
where fc = ∑ p ZpM −1",2.2. Posterior Inference,[0],[0]
p (yp − bp) and Λc = Σ −1,2.2. Posterior Inference,[0],[0]
"c +∑
p ZpM −1",2.2. Posterior Inference,[0],[0]
"p Z > p .
",2.2. Posterior Inference,[0],[0]
"In the sequel, we compute expressions such as M−1p and log |Mp| by using the Cholesky decomposition Mp = LpL > p .",2.2. Posterior Inference,[0],[0]
"Similarly, the expressions depending on Λc are computed using its Cholesky decomposition.",2.2. Posterior Inference,[0],[0]
"As shown in Section 2 of the Appendix, we can derive the expression for the log-marginal likelihood logP (y) in closed form:
logP (y) = ∑ p logN (yp; Z>p c + bp,Mp)
+ logN (c;",2.3. Marginal Likelihood and Its Interpretation,[0],[0]
"0,Σc)− logP (c|y), (3)
",2.3. Marginal Likelihood and Its Interpretation,[0],[0]
"The p-dependent terms require computing the Cholesky decompositions of all Mp ∈ Rnp×np , whereas the final term needs the Cholesky decomposition of Λc ∈ RV×V .",2.3. Marginal Likelihood and Its Interpretation,[0],[0]
"Therefore, logP (y) can be computed in O(V 3 + ∑ p n 3 p).",2.3. Marginal Likelihood and Its Interpretation,[0],[0]
"Note that this computation is required for optimizing the hyperparameters of the GPs.
",2.3. Marginal Likelihood and Its Interpretation,[0],[0]
We can also obtain an interesting interpretation for the induced kernel of the marginal likelihood by computing it in a different way.,2.3. Marginal Likelihood and Its Interpretation,[0],[0]
Let Z =,2.3. Marginal Likelihood and Its Interpretation,[0],[0]
"[Zp] ∈ RV×n, b =",2.3. Marginal Likelihood and Its Interpretation,[0],[0]
[bp] ∈,2.3. Marginal Likelihood and Its Interpretation,[0],[0]
Rn and Kblock ∈ Rn×n the block-diagonal matrix with blocks Kp.,2.3. Marginal Likelihood and Its Interpretation,[0],[0]
"With these notations, it can be shown that P (yp|c) = N (yp|Z>p c + bp,Mp).",2.3. Marginal Likelihood and Its Interpretation,[0],[0]
"Integrating out c leads to
P (y) = N",2.3. Marginal Likelihood and Its Interpretation,[0],[0]
"( b,Z>ΣcZ + K block + σ2I ) .",2.3. Marginal Likelihood and Its Interpretation,[0],[0]
"(4)
If we further assume that Σc = σ2cIV , then
Z>ΣcZ = [ σ2cZ > p Zp′ ]",2.3. Marginal Likelihood and Its Interpretation,[0],[0]
"p,p′ = [ σ2c (z > p zp′)1np1 > np′",2.3. Marginal Likelihood and Its Interpretation,[0],[0]
],2.3. Marginal Likelihood and Its Interpretation,[0],[0]
"p,p′ .
",2.3. Marginal Likelihood and Its Interpretation,[0],[0]
"Hence, the diagonal blocks are proportional to z>p zp, which is the length of path p, and the off-diagonal blocks are proportional to z>p zp′ , which is the path overlap length between p and p′. The resulting kernel is thus the intersection kernel (see Shawe-Taylor & Cristianini (2004), Section 9.5).",2.3. Marginal Likelihood and Its Interpretation,[0],[0]
We have so far associated a random scalar cv with each inner node.,2.4. Model with Random Linear Inner Node Functions,[0],[0]
"More generally, we can use linear functions c>v rv, where cv is a weight vector and rv∈Rdv is a feature vector.",2.4. Model with Random Linear Inner Node Functions,[0],[0]
"The special case above is obtained with dv=1 and rv=[1].
",2.4. Model with Random Linear Inner Node Functions,[0],[0]
We collect the weight vectors in c =,2.4. Model with Random Linear Inner Node Functions,[0],[0]
"[cv] ∈ Rd, where d = ∑ v dv.",2.4. Model with Random Linear Inner Node Functions,[0],[0]
Let Vp ⊆ V be the set of inner nodes on the path from the root to leaf p.,2.4. Model with Random Linear Inner Node Functions,[0],[0]
"Concatenating the rv’s, we define the induced feature vector zp =",2.4. Model with Random Linear Inner Node Functions,[0],[0]
"[rv]v∈Vp such that
c>zp = ∑ v∈Vp c > v rv.
",2.4. Model with Random Linear Inner Node Functions,[0],[0]
"Hence, the dataset we collect during the optimization is now the extended set Dn = {(xi, yi, pi, zpi =[rv,i]v∈Vpi )} n i=1.",2.4. Model with Random Linear Inner Node Functions,[0],[0]
"It is easy to see that all our results above transfer to this more general case, if only we redefine
Zp =",2.4. Model with Random Linear Inner Node Functions,[0],[0]
[zpi ]i∈Ip =,2.4. Model with Random Linear Inner Node Functions,[0],[0]
"[rv,i]i∈Ip,v∈Vpi ∈ R d×np .
",2.4. Model with Random Linear Inner Node Functions,[0],[0]
"Except for an increased dimensionality d > V of the weight vector c, the extension with random linear inner node functions is not more difficult to implement or run.
",2.4. Model with Random Linear Inner Node Functions,[0],[0]
"In Section 4, we will use rv to encode both numerical (i.e., dv = 1) and categorical parameters (via one-hot representations, so that dv equals the number of categories).",2.4. Model with Random Linear Inner Node Functions,[0],[0]
"In our deep learning use case, parameters such as the learning rate, the number of units and the type of activation functions are encoded via the rv’s (see Figure 3, bottom).",2.4. Model with Random Linear Inner Node Functions,[0],[0]
We will refer to the parameter associated with rv as a shared parameter since it is shared across all the leaves whose paths contain v.,2.4. Model with Random Linear Inner Node Functions,[0],[0]
"Bayesian optimization generally proceeds by discretizing the search space X into a set of anchor points, for example by using quasi-random sequences (Sobol, 1967).",3. Acquisition Functions,[0],[0]
"We then maximize an acquisition function starting from the most promising anchor point(s), typically with a numerical solver like L-BFGS (Nocedal & Wright, 2006).",3. Acquisition Functions,[0],[0]
Acquisition functions are defined in terms of expectations over the surrogate model posterior.,3. Acquisition Functions,[0],[0]
"Frequently used choices include Thompson sampling (Thompson, 1933), probability of improvement (PI) (Kushner, 1964), expected improvement (EI) (Mockus et al., 1978), or GP-UCB (Srinivas et al., 2010).",3. Acquisition Functions,[0],[0]
We will focus on EI in the sequel as it has been shown to perform better than PI.,3. Acquisition Functions,[0],[0]
"Our initial experiments also showed that Thompson sampling was not performing well.
",3. Acquisition Functions,[0],[0]
The naive approach of globally optimizing EI over anchor points does not scale well with a high-dimensional X .,3. Acquisition Functions,[0],[0]
"In the previous section, we specified a tree-structured model for the (random) surrogate function, with which the evaluation of an acquisition function at some x ∈ X is sped up.",3. Acquisition Functions,[0],[0]
"In this section, we show how the model structure can also be exploited in order to speed up the optimization itself.",3. Acquisition Functions,[0],[0]
The acquisition function α(x|Dn) plays a critical role in Bayesian Optimization as it selects anchor points by performing an exploration-exploitation trade-off.,3.1. Acquisition Strategies,[0],[0]
The key question that concerns us is whether we can leverage the explicit structure in high-dimensional structured space in order to make the search more efficient.,3.1. Acquisition Strategies,[0],[0]
"The naive approach ignores structure in the search space, using a surrogate model based on a global kernel, like the one proposed by Swersky et al. (2014a).",3.1. Acquisition Strategies,[0],[0]
"While the design of a kernel that incorporate structure is non-trivial, it is not explicitly used to guide the search and the cost of evaluations still scales as O(n3).
",3.1. Acquisition Strategies,[0],[0]
"As noted above, we can speed up evaluations to O( ∑ p n 3 p) by adopting an independent model, which corresponds to our tree model with c = 0, so that the surrogate models {gp(·)}Pp=1 can be learnt and queried independently from each other.",3.1. Acquisition Strategies,[0],[0]
"With this approach, the search decouples across the leaf nodes and can be parallelized accordingly.",3.1. Acquisition Strategies,[0],[0]
"However, if acquisitions are done sequentially, then all leafs have to be searched in order to find the overall best candidate.",3.1. Acquisition Strategies,[0],[0]
"The independent model also fails to represent dependencies between the leaf nodes, so that a larger total number of evaluations may be required to reach a good solution.
",3.1. Acquisition Strategies,[0],[0]
"Given our tree-structured surrogate model, we can improve on both the naive and the independent approach.",3.1. Acquisition Strategies,[0],[0]
"The acquisition function becomes α(x, p|Dn), p being the leaf node where x is evaluated.",3.1. Acquisition Strategies,[0],[0]
"For our model, α(x, p|Dn) can be evaluated in O(V 3 + ∑ p n 3 p), which is often much cheaper than O(n3) required in the naive approach, and is comparable to O( ∑ p n 3 p) for independent.",3.1. Acquisition Strategies,[0],[0]
"We could maximize α(x, p|Dn) separately at each leaf p, and then pick the best candidate across leaf nodes:
(x?, p?) ∈ arg max p∈P,x∈Xp α(x, p|Dn).
",3.1. Acquisition Strategies,[0],[0]
"In practice, the set of leaf nodes P can become large, in which case the requirement to search in every leaf node can be costly.",3.1. Acquisition Strategies,[0],[0]
We propose to further exploit the tree structure of our surrogate model in order to speed up the optimization.,3.1. Acquisition Strategies,[0],[0]
"Namely, our model implies a path acquisition function α(p|Dn).",3.1. Acquisition Strategies,[0],[0]
"Based on this, we select p? and x?",3.1. Acquisition Strategies,[0],[0]
"in two steps:
p? = arg max p∈P α(p|Dn), x? ∈ arg max x∈Xp? α(x, p?|Dn).
",3.1. Acquisition Strategies,[0],[0]
This strategy can greatly speed up the optimization.,3.1. Acquisition Strategies,[0],[0]
"There are obvious intermediates, such as searching in a subset of top-ranked leafs p, which we defer for future work.",3.1. Acquisition Strategies,[0],[0]
"Given our surrogate model, the EI acquisition function is: α(x, p|Dn) = E {",3.2. Two-step Expected Improvement,[0],[0]
[ymin − gp(x)−,3.2. Two-step Expected Improvement,[0],[0]
"z>p c]+ } , (5)
Table 1.",3.2. Two-step Expected Improvement,[0],[0]
Comparison of different surrogate models and acquisition strategies (see text for details).,3.2. Two-step Expected Improvement,[0],[0]
"Here,M(X ) is the complexity of optimizing a surrogate function over the space X .",3.2. Two-step Expected Improvement,[0],[0]
"p? is the path selected by tree, and Xp? is the corresponding leaf domain.
",3.2. Two-step Expected Improvement,[0],[0]
sharing?,3.2. Two-step Expected Improvement,[0],[0]
"complexity independent × O
(∑
p n 3 p · M(Xp)
)
",3.2. Two-step Expected Improvement,[0],[0]
"naive X O ( ( ∑
p np) 3 · M
( P× ∏ pXp ))",3.2. Two-step Expected Improvement,[0],[0]
tree X O ( V 3 + n3p? ·,3.2. Two-step Expected Improvement,[0],[0]
"M(Xp?)
)",3.2. Two-step Expected Improvement,[0],[0]
"where [u]+ = max{u, 0}, and ymin is the best evaluation so far (across all leafs).",3.2. Two-step Expected Improvement,[0],[0]
The expectation is computed with respect to the posterior of gp(x) + z,3.2. Two-step Expected Improvement,[0],[0]
">p c, which is a GP with mean and covariance functions respectively given by
m̃p(x) = kp(x) >",3.2. Two-step Expected Improvement,[0],[0]
M−1p (yp − bp) + tp(x)>Λ −1,3.2. Two-step Expected Improvement,[0],[0]
"c fc + bp,
S̃p(x,x ′)",3.2. Two-step Expected Improvement,[0],[0]
=,3.2. Two-step Expected Improvement,[0],[0]
"Sp(x,x ′) + tp(x) >Λ−1c tp(x ′),
where tp(x) = zp",3.2. Two-step Expected Improvement,[0],[0]
− ZpM−1p kp(x).,3.2. Two-step Expected Improvement,[0],[0]
"We can analytically compute (5), leading to
α(x, p|Dn) = σ̃p(x) (ξΦ(ξ) +N (ξ; 0, 1)) ,
where σ̃p(x) = {S̃p(x,x)}1/2, ξ = m̃p(x)−yminσ̃p(x) and Φ(ξ) is the CDF of a standard Gaussian.
",3.2. Two-step Expected Improvement,[0],[0]
"As noted above, we could optimize α(x, p|Dn) at all leaves and pick the overall winner.",3.2. Two-step Expected Improvement,[0],[0]
"Instead, we propose a two-step approach, based on a path EI acquisition function:
α(p|Dn) = E {",3.2. Two-step Expected Improvement,[0],[0]
"[ymin − bp − z>p c]+ } , (6)
where the expectation is taken with respect to z",3.2. Two-step Expected Improvement,[0],[0]
>p c + bp ∼ N,3.2. Two-step Expected Improvement,[0],[0]
(z>p Λ −1,3.2. Two-step Expected Improvement,[0],[0]
"c fc + bp, z > p Λ −1 c zp).",3.2. Two-step Expected Improvement,[0],[0]
"We first select the path p? = arg maxp α(p|Dn), then find x? by maximizing α(x, p?|Dn) at leaf p?",3.2. Two-step Expected Improvement,[0],[0]
only.,3.2. Two-step Expected Improvement,[0],[0]
Our tree acquisition strategy is related to the naive and independent ones in Table 1.,3.2. Two-step Expected Improvement,[0],[0]
"Interestingly, tree can be faster than independent overall.",3.2. Two-step Expected Improvement,[0],[0]
"Finally, (6) is easily extended to the case where we have random linear functions at the inner nodes by considering the augmented induced variable zp =",3.2. Two-step Expected Improvement,[0],[0]
[rv]v∈Vp (see Section 2.4 for details).,3.2. Two-step Expected Improvement,[0],[0]
"In particular, the resulting optimization of (6) is carried out jointly over p and zp =",3.2. Two-step Expected Improvement,[0],[0]
[rv]v∈Vp .,3.2. Two-step Expected Improvement,[0],[0]
"In this section, we conduct two sets of experiments.",4. Experiments,[0],[0]
"First, we focus on optimizing synthetic functions designed to have tree-structured conditional relationships.",4. Experiments,[0],[0]
"We then consider the tuning of a multi-layer perceptron for binary classification, which we evaluate over a large number of datasets.
",4. Experiments,[0],[0]
"Throughout the experiments, we use the following acronyms to refer to the different competing methods: tree is our proposed approach, independent is a baseline that consider an independent GP for every leaf, arc corresponds
to (Swersky et al., 2014a), smac refers to (Hutter et al., 2011) and gp-baseline is a standard GP-based Bayesian optimization solver taken from (GPyOpt, 2016).",4. Experiments,[0],[0]
"For tree, independent and gp-baseline, we use 5/2 Matérn kernels.",4. Experiments,[0],[0]
"marginal is another baseline obtained by replacing the kernel of gp-baseline by that stemming from the marginal (4), where c is viewed as a nuisance variable and integrated out.",4. Experiments,[0],[0]
"Finally, random is standard random search (Bergstra & Bengio, 2012).
",4. Experiments,[0],[0]
"Unless otherwise specified, all the results displayed in this section correspond to the means and twice the standard errors computed over 25 random replications.",4. Experiments,[0],[0]
"Also, in order to minimize the initialization bias, all methods (except smac1) start from the same set of random candidates; there is one random candidate drawn per conditional path.",4. Experiments,[0],[0]
Our implementation is in Python and we ran the experiments on a fleet of Amazon AWS c4.8xlarge machines.,4. Experiments,[0],[0]
"The functions we consider are defined over binary trees: Each inner node, including the root, corresponds to a binary variable.",4.1. Synthetic Tree-structured Functions,[0],[0]
A path in this tree thus represents successive binary decisions.,4.1. Synthetic Tree-structured Functions,[0],[0]
The leaves contain univariate quadratic functions that are shifted by different constant terms.,4.1. Synthetic Tree-structured Functions,[0],[0]
We give an example of such a function in Figure 1.,4.1. Synthetic Tree-structured Functions,[0],[0]
"In the sequel, we study the two functions from Figure 1 (referred to as small balanced), in addition to a higher-dimensional version of those, with a depth of 4 and 8 leaves whose constant shifts are {a× 0.1}8a=1 (referred to as large balanced).",4.1. Synthetic Tree-structured Functions,[0],[0]
"In the supplementary material, we provide further results based
1We use https://github.com/sfalkner/pySMAC.",4.1. Synthetic Tree-structured Functions,[0],[0]
"To the best of our knowledge, we cannot specify the starting point.
on unbalanced binary trees of increasing sizes, for which similar conclusions hold.",4.1. Synthetic Tree-structured Functions,[0],[0]
"All the non-shared continuous variables xj’s are defined in [−1, 1], while the shared ones are in [0, 1].",4.1. Synthetic Tree-structured Functions,[0],[0]
"The best function value will thus always be 0.1.
",4.1. Synthetic Tree-structured Functions,[0],[0]
"Those functions encode conditional relationships since given a path p and its leaf `p, all the binary variables outside of the path p and all the continuous variables defined in the leaves `′ 6=",4.1. Synthetic Tree-structured Functions,[0],[0]
`p are irrelevant.,4.1. Synthetic Tree-structured Functions,[0],[0]
We report in Figure 2 the optimization results for the different competing methods.,4.1. Synthetic Tree-structured Functions,[0],[0]
"We make the following observations:
Approaches blind to structure perform poorly: The results show that, both gp-baseline and random, which cannot use the conditional structure, do not fare well.",4.1. Synthetic Tree-structured Functions,[0],[0]
"As expected, the performance gap widens as the trees get deeper.
",4.1. Synthetic Tree-structured Functions,[0],[0]
"Independent vs. tree vs. arc: independent, tree and arc represent 3 ways of increasingly incorporating conditional structure.",4.1. Synthetic Tree-structured Functions,[0],[0]
"Indeed, independent takes into account the tree structure but does not allow for any sharing of information across different paths, arc defines a joint kernel over the union of all the leaves, while tree makes intermediate modeling assumptions.",4.1. Synthetic Tree-structured Functions,[0],[0]
"We can observe that, thanks to its joint nature, arc tends to perform well initially, but it is quickly overtaken by tree and later also by independent that lags behind because of the absence of sharing, but catches up once sufficient observations were collected.",4.1. Synthetic Tree-structured Functions,[0],[0]
"Also, as the dimension of the optimization space gets larger, the performance of independent worsens, while that of tree is barely affected (we do observe the same scalability with respect to the dimension on unbalanced binary trees, as reported in the supplementary material).",4.1. Synthetic Tree-structured Functions,[0],[0]
"At this juncture, we would also like to emphasize that while independent catches up with tree in some cases, it is more wasteful of resources as it requires to score every leaf at each iteration unlike tree (see also Table 1).
",4.1. Synthetic Tree-structured Functions,[0],[0]
"Importance of exploiting the latent variables c: It is interesting to observe that marginal, which considers c to be a nuisance variable and integrates it out, performs significantly worse than tree.",4.1. Synthetic Tree-structured Functions,[0],[0]
"Note that marginal cannot de facto be applied in presence of shared variables, which explains why it does not appear in the right panels of Figure 1.
",4.1. Synthetic Tree-structured Functions,[0],[0]
"Approach not based on GPs: smac is known to be stateof-the-art for optimization tasks in presence of conditional relationships (Eggensperger et al., 2013).",4.1. Synthetic Tree-structured Functions,[0],[0]
"In particular, it is known to work better than GP-based approaches, especially when the dimension gets large.",4.1. Synthetic Tree-structured Functions,[0],[0]
"We observe in our experiments (e.g., for large balanced, 7 categorical and 10 continuous parameters, 2 of which being shared) that smac does not reach good solutions on these synthetic tasks.",4.1. Synthetic Tree-structured Functions,[0],[0]
We now focus our attention on the tuning of a multilayer perceptron (MLP) for binary classification.,4.2. Multi-layer Perceptron Tuning,[0],[0]
The setting we consider is reminiscent of that proposed by Swersky et al. (2014a).,4.2. Multi-layer Perceptron Tuning,[0],[0]
"We optimize for the number of hidden layers in {0, 1, 2, 3, 4}, the number of units per layer in {1, 2, . . .",4.2. Multi-layer Perceptron Tuning,[0],[0]
", 30} (provided the corresponding layer is activated), the choice of the activation function in {identity, logistic, tanh, relu}, which we constrain to be identical across all layers, the amount of `2 regularization in [10−6, 10−1], the learning rate in [10−5, 10−1] of the underlying Adam solver (Kingma & Ba, 2014), the tolerance in [10−5, 10−2] of the solver (based on relative decrease), and the type of data pre-processing, which can be unit `2-norm observation-wise normalization, `∞-norm feature-wise normalization, mean/standarddeviation feature-wise whitening or no normalization at all.
",4.2. Multi-layer Perceptron Tuning,[0],[0]
"The optimization task can be specified in various ways, resulting in different topologies for the trees of conditional relationships.",4.2. Multi-layer Perceptron Tuning,[0],[0]
We consider the two instantiations of conditional relationships illustrated in Figure 3.,4.2. Multi-layer Perceptron Tuning,[0],[0]
"The first one has all the variables duplicated (top tree), which is similar to how independent proceeds.",4.2. Multi-layer Perceptron Tuning,[0],[0]
The second one consists in having most of the variables shared (bottom tree).,4.2. Multi-layer Perceptron Tuning,[0],[0]
"Note that in the two settings, we have one regularization parameter λk per number k of hidden layer(s) of the network.",4.2. Multi-layer Perceptron Tuning,[0],[0]
"We do
so to account for the fact that the λk’s regularize matrices of different dimensions.",4.2. Multi-layer Perceptron Tuning,[0],[0]
"In between those two extreme settings, we could consider intermediate modeling assumptions (e.g., a learning rate ηlinear for the case with no hidden layers and a shared learning rate ηnon-linear otherwise).
",4.2. Multi-layer Perceptron Tuning,[0],[0]
"To provide a robust evaluation of the different competing methods, we consider a subset of the datasets from the Libsvm repository (Chang & Lin, 2011).",4.2. Multi-layer Perceptron Tuning,[0],[0]
"More specifically, we consider all the datasets whose number of features is smaller than 106, which results in 45 data sets.",4.2. Multi-layer Perceptron Tuning,[0],[0]
"In absence of pre-defined default train-test split, we took a random 80%−20% split.",4.2. Multi-layer Perceptron Tuning,[0],[0]
"To limit the overall computational burden, we cap the training and test set sizes to a maximum of respectively 103 and 104 instances (randomly selected when the subsampling applies).",4.2. Multi-layer Perceptron Tuning,[0],[0]
"Note that this subsampling step is not related to a computational limitation of our approach, but is a practical consideration only modifying the properties of the black-box function we optimize.",4.2. Multi-layer Perceptron Tuning,[0],[0]
"We use the MLP implementation of scikit-learn (Pedregosa et al., 2011)
and we add a CPU-time constraint of 5 minutes to each evaluation, beyond which the worst classification error 1.0 is returned.",4.2. Multi-layer Perceptron Tuning,[0],[0]
"Under this constraint, the total computational time of the experiment was roughly 100 CPU days.
",4.2. Multi-layer Perceptron Tuning,[0],[0]
We run all the methods for 85 iterations and initialize them with one random choice for each of the 5 conditional paths.,4.2. Multi-layer Perceptron Tuning,[0],[0]
We aggregate the average classification errors per dataset by displaying the average rank of each method as a function of the number of iterations.,4.2. Multi-layer Perceptron Tuning,[0],[0]
"We say that the rank of a method is equal to i if it performs the ith best (see, e.g., Bardenet et al. (2013); Feurer et al. (2015)).",4.2. Multi-layer Perceptron Tuning,[0],[0]
"We can draw the following conclusions:
Effect of z>p c without shared variables: The top panel in Figure 4 compares independent with tree-based method when it is defined on the independent topology shown in Figure 3(top).",4.2. Multi-layer Perceptron Tuning,[0],[0]
"Since there are no shared variables in the inner nodes, the sharing mechanism of tree only
happens via the term z>p c",4.2. Multi-layer Perceptron Tuning,[0],[0]
which contributes to the mean.,4.2. Multi-layer Perceptron Tuning,[0],[0]
"As expected, sharing results in tree makes faster progress towards the optimum.",4.2. Multi-layer Perceptron Tuning,[0],[0]
"However, when more observations are collected, independent outperforms tree because it better explores all the leafs (though, at a higher computational cost; see Table 1).",4.2. Multi-layer Perceptron Tuning,[0],[0]
"We next show how we can additionally benefit from sharing parameters at inner nodes.
",4.2. Multi-layer Perceptron Tuning,[0],[0]
Shared topology: The lower panel in Figure 4 compares all the methods using the shared topology shown in Figure 3(bottom).,4.2. Multi-layer Perceptron Tuning,[0],[0]
"We found that arc, gp-baseline, random and smac all benefitted from running with the shared topology.",4.2. Multi-layer Perceptron Tuning,[0],[0]
"The results show that tree not only greatly improves upon all other GP-based approaches, but also converges faster than smac that finally reaches the same level of performance after about 75 iterations.",4.2. Multi-layer Perceptron Tuning,[0],[0]
"We can observe that a standard GP-based technique that is blind to the conditional structure, like gp-baseline, performs poorly.",4.2. Multi-layer Perceptron Tuning,[0],[0]
"Of independent interest is the comparison of arc with smac, which was not reported by Swersky et al. (2014a).",4.2. Multi-layer Perceptron Tuning,[0],[0]
"Finally, it is worth emphasizing that tree obtains good results while only modeling shared variables at the inner nodes in a linear fashion.",4.2. Multi-layer Perceptron Tuning,[0],[0]
"This conclusion is in agreement with the recent observations from (Zhang et al., 2016) where linear models lead to good results in the context of the optimization of data analytic pipelines.",4.2. Multi-layer Perceptron Tuning,[0],[0]
"Next, we discuss an extension to model the shared variables non-linearly.",4.2. Multi-layer Perceptron Tuning,[0],[0]
The approach we have introduced in Section 2.4 can easily be extended to account for non-linearities through the use of basis expansions.,4.3. Nonlinear Extensions,[0],[0]
"More specifically, we focus on the use of random Fourier features (Rahimi et al., 2007) that proved successful for large-scale kernel methods (Lu et al., 2014).",4.3. Nonlinear Extensions,[0],[0]
"Combining basis expansion with linear models for Bayesian optimization is by no means new (see (Shahriari et al., 2016) and references therein).",4.3. Nonlinear Extensions,[0],[0]
"We also follow this methodology since it naturally fits our proposed semi-parametric model.
",4.3. Nonlinear Extensions,[0],[0]
"In the supplementary material, we report results on synthetic tree-structured functions where the objectives at the leaves depend now quadratically on the shared variables and on the MLP tuning task.",4.3. Nonlinear Extensions,[0],[0]
"In a nutshell, on the synthetic functions with linearly-dependent shared variables, tree-nonlinear converges slower than the linear version tree, which might be due to the fact that c is of higher dimensionality.",4.3. Nonlinear Extensions,[0],[0]
"Moreover, in presence of quadratically-dependent shared variables, we observe that tree fails to model adequately the non-linearities, while tree-nonlinear, as expected, can.",4.3. Nonlinear Extensions,[0],[0]
"As for the MLP task, we notice that the non-linear extension of tree tends to perform worse than its linear counterpart.",4.3. Nonlinear Extensions,[0],[0]
"The black-box functions typically encountered in machine learning rely on incremental learning procedures, such as the application of (stochastic) gradient descent over several epochs.",5. Concluding Remarks,[0],[0]
"A recent line of work has been focusing on leveraging this property to speed up Bayesian optimization (Swersky et al., 2013; 2014b; Domhan et al., 2014; Li et al., 2016; Klein et al., 2016).",5. Concluding Remarks,[0],[0]
"In particular, Li et al. (2016) and Klein et al. (2016) have reported state-of-the-art results with methods based respectively on bandits and GPs, exploiting a dynamic subsampling of the training sets.
",5. Concluding Remarks,[0],[0]
The goal of our work is orthogonal to this idea and consists instead in efficiently encoding conditional relationships with GPs.,5. Concluding Remarks,[0],[0]
"We next outline ways of combining our work with the aforementioned subsampling idea:
Combination with Klein et al. (2016): The proposal of Klein et al. (2016) uses some contextual variable (also referred to as environmental variable) to encode the subsampling rate of the training set.",5. Concluding Remarks,[0],[0]
Let us denote it by β ∈,5. Concluding Remarks,[0],[0]
"[0, 1].",5. Concluding Remarks,[0],[0]
"Klein et al. (2016) define the following joint kernel:
K((x, β), (x′, β′))",5. Concluding Remarks,[0],[0]
"= K0(x,x′) · Kcontext(β, β′).
",5. Concluding Remarks,[0],[0]
"The optimization is then driven by a cost-normalized acquisition function maxβ′,x α(x, β = 1|Dn)/cost(x, β′) where both x and β are sought to perform well on the final task of interest where no subsampling is applied (i.e., β = 1).
",5. Concluding Remarks,[0],[0]
"Looking at our case, we could easily replace our kernel Kp by K̃p((x, β), (x′, β′)) , Kp(x,x′) · Kcontext(β, β′).",5. Concluding Remarks,[0],[0]
"To apply the two-step procedure, we could normalize (6) by a cost following a separate model (1) where the contextual variable β would be a shared variables at the root.",5. Concluding Remarks,[0],[0]
"Formally, we could consider a joint path/subsampling selection criterion:
(p?, β?)",5. Concluding Remarks,[0],[0]
"∈ arg max p∈P,β′∈[0,1]
E {",5. Concluding Remarks,[0],[0]
"[ymin − bp − zp(β = 1)>c]+ }
E { zp(β′)>ccost } , where zp(β) refers to the feature representation of the path p with context variable β.
Combination with Li et al. (2016): The approach of Li et al. (2016) is based on successive halving procedures where a pool containing initially many models is progressively refined and trimmed.",5. Concluding Remarks,[0],[0]
"The output of their theoreticallyjustifed algorithm, named hyperband, can be seen as triplets Hn , {(xi, yi, βi)}ni=1",5. Concluding Remarks,[0],[0]
"representing all the tested configurations xi along with their corresponding evaluations yi and subsampling rates βi.
",5. Concluding Remarks,[0],[0]
A natural approach to leverage hyperband is therefore to useHn to warm-start our context-aware extension with kernel K̃p while fixing β = 1.,5. Concluding Remarks,[0],[0]
"In other words, our approach would be used to refine the smart and computationallyefficient initialization provided by hyperband.",5. Concluding Remarks,[0],[0]
Bayesian optimization has been successfully used to optimize complex black-box functions whose evaluations are expensive.,abstractText,[0],[0]
"In many applications, like in deep learning and predictive analytics, the optimization domain is itself complex and structured.",abstractText,[0],[0]
"In this work, we focus on use cases where this domain exhibits a known dependency structure.",abstractText,[0],[0]
The benefit of leveraging this structure is twofold: we explore the search space more efficiently and posterior inference scales more favorably with the number of observations than Gaussian Process-based approaches published in the literature.,abstractText,[0],[0]
We introduce a novel surrogate model for Bayesian optimization which combines independent Gaussian Processes with a linear model that encodes a tree-based dependency structure and can transfer information between overlapping decision sequences.,abstractText,[0],[0]
We also design a specialized two-step acquisition function that explores the search space more effectively.,abstractText,[0],[0]
Our experiments on synthetic tree-structured objectives and on the tuning of feedforward neural networks show that our method compares favorably with competing approaches.,abstractText,[0],[0]
Bayesian Optimization with Tree-structured Dependencies,title,[0],[0]
"Probabilistic numerics (Hennig et al., 2015) proposes approaching problems of numerical analysis from the point of view of statistics.",1. Introduction,[0],[0]
"In particular, Bayesian probabilistic numerical methods approach this problem from a Bayesian point of view, and can provide posterior distributions on the solutions of numerical problems (e.g. in the case of this paper, the solution of some integral).",1. Introduction,[0],[0]
These posterior distributions represent our epistemic uncertainty about these quantities of interest.,1. Introduction,[0],[0]
"In the case of quadrature rules, the uncertainty is due to the fact that we only have a finite number of
*Equal contribution 1Department of Mathematics, Imperial College London 2Department of Statistics, University of Warwick 3The Alan Turing Institute for Data Science and AI.",1. Introduction,[0],[0]
"Correspondence to: François-Xavier Briol <f-x.briol@warwick.ac.uk>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
function evaluations and therefore are uncertaint about the value of the integral.",1. Introduction,[0],[0]
"The notion of Bayesian probabilistic numerical method was independently introduced by several authors (Larkin, 1972; Kadane & Wasilkowski, 1985; Diaconis, 1988; O’Hagan, 1992), but only recently formalised by (Cockayne et al., 2017).
",1. Introduction,[0],[0]
"Apart from the uncertainty quantification property described above, these methods have several other advantages over “classical” (i.e. non-Bayesian) numerical methods (although some of the classical and Bayesian methods coincide (Diaconis, 1988)).",1. Introduction,[0],[0]
"First of all, they allow the user to formulate all of its prior knowledge in the form of a prior, making all of the assumptions of the numerical scheme explicit.",1. Introduction,[0],[0]
"Second of all, they can allow for coherent propagation of numerical uncertainties through chains of computation; see (Cockayne et al., 2017; Oates et al., 2017a).
",1. Introduction,[0],[0]
"However, one property which has not been studied so far is the possibility of jointly inferring several quantities of interest.",1. Introduction,[0],[0]
"In this paper, we study the problem of numerically integrating a sequence of functions f1, . . .",1. Introduction,[0],[0]
", fD (which are correlated to one another) with respect to some probability measure Π, and hence propose to build a model for joint inference of ∫ f1dΠ, . . .",1. Introduction,[0],[0]
", ∫ fDdΠ. Such a joint model allows for better finite-sample performance, and can also lead to more refined posterior distributions on each of the individual integrals.
",1. Introduction,[0],[0]
"To tackle this problem, we extend the well-known Bayesian quadrature (O’Hagan, 1991) algorithm and study the performance of the proposed methodology from a theoretical and experimental point of view.",1. Introduction,[0],[0]
"In particular, we provide asymptotic convergence results for the marginal posterior variance on each of the integrals, both in the case of a well specified and misspecified prior.",1. Introduction,[0],[0]
"We also demonstrate the performance of our algorithm on some toy problems from the engineering literature on multi-fidelity models, and on a challenging problem from the field of computer graphics.",1. Introduction,[0],[0]
"Bayesian Quadrature Let (X ,B,Π) be a probability space and consider some function f :",2. Methodology,[0],[0]
"X → R where X ⊆ Rp, p ∈",2. Methodology,[0],[0]
N+.,2. Methodology,[0],[0]
"The classical problem of numerical
integration is concerned with approximating the integral:
Π[f ]",2. Methodology,[0],[0]
:,2. Methodology,[0],[0]
"= ∫ X f(x)Π(dx),
where we assume ∫ X f
2(x)Π(dx)",2. Methodology,[0],[0]
<,2. Methodology,[0],[0]
"∞. Under fairly general conditions on f , one can show that an optimal algorithm (in terms of worst-case integration error in some function space) takes the form of a quadrature (or cubature) rule Π̂[f ] = ∑N i=1 wif(xi) for some weights {wi}Ni=1 ∈ R and samples {xi}Ni=1 ∈ X (see (Bakhvalov, 1971)).",2. Methodology,[0],[0]
"These are also sometimes denoted in vectorised form as Π[f ] = w>f(X) where w = (w1, . . .",2. Methodology,[0],[0]
", wN )>, X =",2. Methodology,[0],[0]
"(x1, . . .",2. Methodology,[0],[0]
",xN )
> and f(X) = (f(x1), . . .",2. Methodology,[0],[0]
", f(xN ))",2. Methodology,[0],[0]
>.,2. Methodology,[0],[0]
The notation Π̂[f ] is motivated by the fact that we can see this object as an exact integral with respect to a discrete measure Π̂ = ∑N i=1,2. Methodology,[0],[0]
"wiδxi , where δxi denotes the Dirac delta measure taking value 1 at xi and 0 otherwise.",2. Methodology,[0],[0]
"Many popular numerical integration methods take this form, including Newton–Cotes rules, Gaussian quadrature, Monte Carlo methods and sparse grids.
",2. Methodology,[0],[0]
"Let (Ω,F ,P) be another probability space.",2. Methodology,[0],[0]
"Bayesian quadrature (BQ), introduced by (O’Hagan, 1991), proposes to approach the problem of numerical integration by first formulating a prior stochastic model g : X × Ω → R for the integrand f (where ∀ω ∈ Ω, g(·, ω) represents a realisation of g).",2. Methodology,[0],[0]
"This prior model is then conditioned on the vector of observations f(X) to obtain a posterior model for f , which is then pushed forward through the integral operator to give a posterior on Π[f ].
",2. Methodology,[0],[0]
"A popular choice of prior is a Gaussian Process (GP) GP(m, k) with m : X → R denoting the mean function (i.e. m(x) =",2. Methodology,[0],[0]
"Eω[g(x, ω)]), and c :",2. Methodology,[0],[0]
"X × X → R denoting the covariance function/kernel (i.e. c(x,x′) = Eω[(g(x, ω)−m(x))(g(x′, ω)−m(x′))]).",2. Methodology,[0],[0]
Let us assume that m = 0,2. Methodology,[0],[0]
(this can be done without loss of generality since the domain can be re-parametrized to be centred at 0).,2. Methodology,[0],[0]
"After conditioning on X , we have a new Gaussian process gN with mean and covariance:
mN (x) = c(x,X)c(X,X) −1f(X),
cN (x,x ′)",2. Methodology,[0],[0]
"= c(x,x′)− c(x,X)c(X,X)−1c(X,x′),
for all x,x′ ∈ X .",2. Methodology,[0],[0]
"Here, c(X,X) is the Gram matrix with entries (c(X,X))ij = c(xi,xj) and c(x,X) = (c(x,x1), . . .",2. Methodology,[0],[0]
", c(x,xN ))",2. Methodology,[0],[0]
"whilst c(X,x) = c(x,X)> .",2. Methodology,[0],[0]
"The push-forward of this posterior through the integral operator is a Gaussian distribution with mean and variance:
E",2. Methodology,[0],[0]
"[Π[gN ]] = Π[c(·,X)]c(X,X)−1f(X), V [Π[gN",2. Methodology,[0],[0]
]] = ΠΠ̄,2. Methodology,[0],[0]
"[c]−Π[c(·,X)]c(X,X)−1Π̄[c(X, ·)],
where Π[c(·,X)]",2. Methodology,[0],[0]
"= (Π[c(·,x1)], . . .",2. Methodology,[0],[0]
",Π[c(·,xN )]).",2. Methodology,[0],[0]
"These expression can be obtained in closed-form if the kernel mean Π[c(·,x)]",2. Methodology,[0],[0]
"= ∫ X c(x ′,x)Π(dx′) (also called
the representer of integration) and initial error ΠΠ̄[c] =∫ X×X c(x,x
′)Π(dx)Π(dx′) can be obtained in closed form (here Π̄ indicates that the integral is taken with respect to the second argument).
",2. Methodology,[0],[0]
"The choice of covariance function c can be used to encode prior beliefs about the function f , such as smoothness or periodicity, and is very important to obtain good performance in practice.",2. Methodology,[0],[0]
"A popular example is the family of Matérn kernels
cα(x,x ′) = λ2
21−α
Γ(α)
(√ 2α ‖x− x′‖22
σ2 )α ×",2. Methodology,[0],[0]
"Jα (√ 2α ‖x− x′‖22
σ2
) , (1)
for σ, λ > 0 where Jα is the Bessel function of the second kind and α > 0 gives the smoothness of the kernel.",2. Methodology,[0],[0]
"On X = Rp, this will give an RKHS normequivalent to the Sobolev space Wα2 (Rd)1.",2. Methodology,[0],[0]
"Examples of infinitely smooth kernels include the squared-exponential kernel c(x,x′) = exp(−‖x− x′‖22/σ2) where σ > 0, the multi-quadric kernel c(x,x′) =",2. Methodology,[0],[0]
"(−1)dβe(σ2+‖x−x′‖22)β for β, σ > 0, β 6∈ N and the inverse multi-quadric kernel c(x,x′) = (σ2 + ‖x− x′‖22)−β for β, σ > 0.
",2. Methodology,[0],[0]
"In practice, numerical inversion can be challenging since the Gram matrix tends to be nearly singular, and so one may wish to regularise the matrix using a Tikhonov penalty.",2. Methodology,[0],[0]
"The inverses above can also potentially render the computation of the BQ estimator computationally expensive (up to O(N3) cost in the most general settings), although this can be alleviated in specific cases (Karvonen & Särkkä, 2017b).",2. Methodology,[0],[0]
"Even if this is not the case, the additional cost can be worthwhile regardless since the method has been shown to attain fast convergence rates (Briol et al., 2015a;b; Kanagawa et al., 2016; 2017; Bach, 2017) when the target integrand and the kernel used are smooth.
",2. Methodology,[0],[0]
"Recent research directions in BQ include efficient sampling algorithms (for the point set X) to improve the performance of the method (Rasmussen & Ghahramani, 2002; Huszar & Duvenaud, 2012; Gunter et al., 2014; Briol et al., 2015a; Karvonen & Särkkä, 2017a; Briol et al., 2017), asymptotic convergence results (Briol et al., 2015a;b; Kanagawa et al., 2016; Bach, 2017) and equivalence of BQ with known quadrature rules for certain choices of point sets and kernels (Sarkka et al., 2016; Karvonen & Särkkä, 2017a).",2. Methodology,[0],[0]
"Furthermore, there has also been a wide range of new applications, including to other numerical methods in optimization, linear algebra and functional approximation (Kersting & Hennig, 2016; Fitzsimons et al., 2017), inference in complex computer models
1We say that two norms ‖ · ‖1 and ‖ · ‖2 on a vector space are norm-equivalent if and only if ∃C1, C2 > 0",2. Methodology,[0],[0]
"such that C1‖ · ‖2 ≤ ‖ · ‖1 ≤ C2‖ · ‖2.
(Oates et al., 2017c), and problems in econometrics (Oettershagen, 2017) and computer graphics (Brouillat et al., 2009; Marques et al., 2013; Briol et al., 2015b).
",2. Methodology,[0],[0]
"Although other stochastic processes could of course be used (Cockayne et al., 2017), GPs are popular due to their conjugacy properties, and the terminology Bayesian quadrature usually refers to this case.",2. Methodology,[0],[0]
"Note that other names for BQ with GP priors include Gaussian-process quadrature (Sarkka et al., 2016) or kernel quadrature (Bach, 2017; Briol et al., 2017; Kanagawa et al., 2017).",2. Methodology,[0],[0]
"In fact, a well-known alternative view of the posterior mean provided by BQ is that of an optimally-weighted quadrature rule in a reproducing kernel Hilbert spaces (RKHS) in the classical worst-case setting (Ritter, 2000).",2. Methodology,[0],[0]
"Let Hk be an RKHS with inner product and norm denoted 〈·, ·〉k and ‖·‖k respectively; i.e. a Hilbert space with an associated symmetric and positive definite reproducing kernel k :",2. Methodology,[0],[0]
"X × X → R such that f(x) = 〈f, k(·,x)〉k (see (Berlinet & Thomas-Agnan, 2004) for a detailed study).",2. Methodology,[0],[0]
"Suppose that our integrand f ∈ Hk and that ∫ X k(x,x)Π(dx) <",2. Methodology,[0],[0]
"∞. In that case, using the Cauchy–Schwarz inequality, the integration error can be decomposed as:∣∣∣Π[f ]",2. Methodology,[0],[0]
− Π̂[f ]∣∣∣ ≤ ‖f‖k ∥∥∥Π,2. Methodology,[0],[0]
"[k(·,x)]− Π̂",2. Methodology,[0],[0]
"[k(·,x)]∥∥∥
k .
",2. Methodology,[0],[0]
"The corresponding worst-case error over the unit ball of the spaceHk is given by:
e ( Hk, Π̂,X )",2. Methodology,[0],[0]
= sup ‖f‖k≤1 ∣∣∣Π[f,2. Methodology,[0],[0]
]− Π̂[f ]∣∣∣ = ∥∥∥Π,2. Methodology,[0],[0]
"[k(·,x)]− Π̂",2. Methodology,[0],[0]
"[k(·,x)]∥∥∥
k = ( w>k(X,X)w − 2Π[k(·,X)]>w + ΠΠ̄[k] )",2. Methodology,[0],[0]
"1 2 .
",2. Methodology,[0],[0]
"This final expression can be minimised in closed form over w ∈ RN to show that the optimal quadrature rule has weights w = Π[k(·,X)]k(X,X)−1.",2. Methodology,[0],[0]
"This corresponds exactly to the weights for the BQ posterior mean if we take our prior on f to be a GP(0, k), whilst the worst-case error can be shown to correspond to the posterior variance squared.",2. Methodology,[0],[0]
"The BQ estimator with prior GP(0, c) is therefore optimal in the classical worst-case sense for the RKHSHc.
",2. Methodology,[0],[0]
Multi-output Bayesian Quadrature We now extend the set-up of our problem.,2. Methodology,[0],[0]
"Suppose we have a sequence of probability spaces (Xd,Bd,Πd) and functions fd :",2. Methodology,[0],[0]
"Xd → R for which we are interested in numerically computing integrals of the form Πd[fd] for d = 1, . . .",2. Methodology,[0],[0]
", D.",2. Methodology,[0],[0]
"In many applications where we are faced with this type of problem, we also have prior knowledge about correlations between the individual fd.",2. Methodology,[0],[0]
"However, this information is often ignored and the integrals are approximated individually.",2. Methodology,[0],[0]
"This is not a principled approach from a Bayesian point of
view since it means we are not conditioning on all available information.",2. Methodology,[0],[0]
"In this section, we extend the BQ algorithm to solve this problem by building a joint model of f1, . . .",2. Methodology,[0],[0]
", fD in order to obtain a joint posterior on the integrals Π1[f1], . . .",2. Methodology,[0],[0]
",ΠD[fD].
",2. Methodology,[0],[0]
"For notational convenience, we will restrict ourselves to the case where all of the input domains are identical and denoted X , all of the probability measures are identical and denoted Π, and the input sets X = {Xd}Dd=1 consist of N points",2. Methodology,[0],[0]
"Xd = (xd,1, . . .",2. Methodology,[0],[0]
",xd,N ) per output function fd (note the setup can be made more general if necessary).",2. Methodology,[0],[0]
We re-frame the integration problem as that of integrating some vector-valued function f : X,2. Methodology,[0],[0]
"→ RD such that f(x) = (f1(x), . . .",2. Methodology,[0],[0]
", fD(x))
",2. Methodology,[0],[0]
">; i.e. we want to estimate Π[f ] = (Π[f1], . . .",2. Methodology,[0],[0]
",Π[fD])
>.",2. Methodology,[0],[0]
"In this multiple-integral setting, we can have generalised quadrature rules of the form:
Π̂[fd] = D∑ d′=1 N∑ i=1",2. Methodology,[0],[0]
"(Wi)dd′fd′(xd′,i)
where Wi ∈ RD×D are weight matrices and (Wi)dd′ gives the influence of the value of fd′ at xd′,i on the estimate of Π[fd].",2. Methodology,[0],[0]
"The quadrature rule for f can be re-written in compact form as Π̂[f ] = W>f(X) for some weight matrix W ∈ RND×D (a concatenation of {Wi}Ni=1) and function-evaluations vector f(X) = (f1(x1,1), . . .",2. Methodology,[0],[0]
", f1(x1,N ), . . .",2. Methodology,[0],[0]
", fD(xD,1), . . .",2. Methodology,[0],[0]
", fD(xD,N ))",2. Methodology,[0],[0]
">.
",2. Methodology,[0],[0]
"These generalised quadrature rules encompass popular Monte Carlo methods such as control variates or functionals (Glasserman, 2004; Oates et al., 2017b), multilevel Monte Carlo (Giles, 2015) and multi-fidelity Monte Carlo (Peherstorfer et al., 2016b).",2. Methodology,[0],[0]
"However, it is important to point out that these methods can only deal with very specific relations between integrands, usually requiring ( ∫ X (fd(x)−fd′(x)) 2Π(dx))",2. Methodology,[0],[0]
"1 2 to be small for all pairs of integrands fd, fd′ .",2. Methodology,[0],[0]
"Our method will be able to make use of much more complex relations.
",2. Methodology,[0],[0]
"We propose to approach this problem using an extended version of BQ, where we impose a prior g : X × Ω→ RD which is a GP(0,C) on the extended space (this is often called a multi-output GP or co-kriging model (Alvarez et al., 2012))",2. Methodology,[0],[0]
"where now C is matrix-valued and (C(x,x′))dd′ = Eω∼P[gd(x, ω)gd′(x′, ω)",2. Methodology,[0],[0]
].,2. Methodology,[0],[0]
"In this case, after conditioning on X , we have a GP gN with vectorvalued mean mN :",2. Methodology,[0],[0]
"X → RD and matrix-valued covariance CN : X × X → RD×D:
mN (x) = C(x,X)C(X,X) −1f(X),
CN (x,x ′) = C(x,x′)−C(x,X)C(X,X)−1C(X,x′),
for C(x,X) = (C(x,x1), . . .",2. Methodology,[0],[0]
", C(x,xN ))",2. Methodology,[0],[0]
"∈ RD×ND and Gram matrix C(X,X) ∈",2. Methodology,[0],[0]
"RND×ND is:
C(X,X) =  (C(X1,X1))1,1 . . .",2. Methodology,[0],[0]
"(C(X1,XD))1,D (C(X2,X1))2,1 ... (C(X2,XD))2,D
... ... ...",2. Methodology,[0],[0]
"(C(XD,X1))D,1 . . .",2. Methodology,[0],[0]
"(C(XD,XD))D,D
 ,
where C(Xd,Xd′)d,d′ is an N × N matrix.",2. Methodology,[0],[0]
"The posterior on the value of the integral vector Π[f ] can also be obtained whenever the kernel mean Π[C(·,x)] and initial error ΠΠ̄",2. Methodology,[0],[0]
"[C] are available in closed form, which is potentially a restrictive condition.",2. Methodology,[0],[0]
"The authors of (Briol et al., 2015b) give a table of closed-form expressions of these quantities for popular kernels in the uni-output case, and we envision the same type of table being necessary for future extensions of multi-output BQ.",2. Methodology,[0],[0]
"Alternatively, (Oates et al., 2017b; 2016) proposed a kernel which is tailored to the target probability measure Π and which could also be extended to the multi-output case.
",2. Methodology,[0],[0]
Proposition 1.,2. Methodology,[0],[0]
"Consider multi-output Bayesian Quadrature with a GP(0,C) prior on f = (f1, . . .",2. Methodology,[0],[0]
", fD)>.",2. Methodology,[0],[0]
"The posterior distribution on Π[f ] is a D-dimensional Gaussian distribution with mean and covariance matrix:
E",2. Methodology,[0],[0]
"[Π[gN ]] = Π[C(·,X)]C(X,X)−1f(X), V [Π[gN ]] = ΠΠ̄",2. Methodology,[0],[0]
"[C]−Π[C(·,X)]C(X,X)−1Π̄[C(X, ·)].
",2. Methodology,[0],[0]
"All proofs can be found in Appendix B. In this case, we clearly end up with a generalised quadrature rule with weight matrix: W BQ = (Π [C(·,X)]C(X,X)−1)",2. Methodology,[0],[0]
>,2. Methodology,[0],[0]
∈,2. Methodology,[0],[0]
"RND×D. In general, the computational cost for computing the posterior mean and variance is now of orderO(N3D3).",2. Methodology,[0],[0]
"However, several choices of kernels can reduce this cost significantly, and it is also possible to obtain sparse GP approximations; see e.g. (Álvarez & Lawrence, 2011).
",2. Methodology,[0],[0]
The choice of kernel C is of course once again of great importance since it encodes prior knowledge about each of the integrand and their correlation structure and should be made based on the application considered.,2. Methodology,[0],[0]
"We also remark that matrix valued kernels C can be described in term of some scalar-valued kernel r on the extended space X×{1, . . .",2. Methodology,[0],[0]
", D} as (C(x,x′))dd′ =",2. Methodology,[0],[0]
"r((x, d), (x′, d′)).",2. Methodology,[0],[0]
"We now present two choices of covariance functions which are popular in the literature and will be used in this paper:
• The separable kernel is of the form
C(x,x′) = Bc(x,x′),
whereB ∈ RD×D is symmetric and positive definite, and c :",2. Methodology,[0],[0]
X × X → R is a scalar-valued reproducing kernel.,2. Methodology,[0],[0]
"This treats the kernel as the product of two scalar-valued reproducing kernels, one defined on X
and the other on {1, . . .",2. Methodology,[0],[0]
", D}.",2. Methodology,[0],[0]
A particular case of interest is the linear model of coregionalization (LMC) where the matrix is of the form (B)dd′ = ∑R i=1,2. Methodology,[0],[0]
a i da,2. Methodology,[0],[0]
i d′ for some aid ∈ R.,2. Methodology,[0],[0]
"This type of kernel can lead to a lower computational cost of orderO(N3 +D3) when evaluating all fd on the same input set and using tensor product formulations (see Appendix C).
",2. Methodology,[0],[0]
•,2. Methodology,[0],[0]
"The process convolution kernel (Ver Hoef & Barry, 1998; Higdon, 2002; Alvarez et al., 2012) models the individual functions f1, . . .",2. Methodology,[0],[0]
", fD",2. Methodology,[0],[0]
as blurred transformations of R ∈ N+ underlying functions.,2. Methodology,[0],[0]
"It is given by:
(C(x,x′))d,d′ = cd,d′(x,x ′) + cwd(x,x ′)δd,d′ ,
where δdd′ = 1 if d = d′ and 0 else.",2. Methodology,[0],[0]
"Here there are two parts of the kernel, first cd,d′ : X × X → R defined as:
cd,d′(x,x ′)",2. Methodology,[0],[0]
= R∑ i=1,2. Methodology,[0],[0]
∫,2. Methodology,[0],[0]
"X Gid(x− z)×∫
X Gid′(x ′",2. Methodology,[0],[0]
"− z′)ci(z, z′)dz′dz,
and cwd : X ×X → R representing covariance inherent to the dth function and Gid : X → R is a blurring kernel2 which is a continuous function either having compact support or being square integrable.",2. Methodology,[0],[0]
"Notice that taking Gid(x− z) = aidδ(x− z) (where δ(·) represents a Dirac function) gives back the LMC case.
",2. Methodology,[0],[0]
"Note that it is also common to combine kernels, by summing them (i.e. C(x,x′) = ∑Q q=1Cq(x,x
′))",2. Methodology,[0],[0]
in order to obtain more flexible models.,2. Methodology,[0],[0]
"The kernel means and initial error, as well as other details for implementation are provided in Appendix C.",2. Methodology,[0],[0]
"In this section, we begin by exploring properties of multioutput BQ with GP(0,C) prior as an optimally-weighted quadrature algorithm in vector-valued RKHSHC .
",3. Theoretical results,[0],[0]
"Let HK be a vector-valued RKHS with norm and inner product denoted ‖ · ‖K and 〈·, ·〉K respectively.",3. Theoretical results,[0],[0]
"These spaces were extensively studied in (Pedrick, 1957; Micchelli & Pontil, 2005; Carmeli et al., 2006; 2010; De Vito et al., 2013), and generalise the notion of RKHS to vectorvalued functions.",3. Theoretical results,[0],[0]
"In the multi-output case, there is also a one-to-one correspondance between the RKHS HK and the kernel K. Theorem 3.1 in (Micchelli & Pontil, 2005) shows that the minimizer of the variational problem:
min h∈HK
{ ‖h‖2K : h : X → RD,h(xi) = f(xi) ∀xi ∈X } 2Note that the term “blurring kernel” does not mean the func-
tion is a reproducing kernel.
takes the form of the multi-output posterior GP mean mN obtained after conditioning a GP(0,K) on some data set X .",3. Theoretical results,[0],[0]
"We can therefore extend a well-known result from the uni-output case to show that Π̂BQ[fd] is an optimally weighted quadrature rule for all fd in terms of their worstcase integration error, denoted:
e(HC , Π̂,X, d)",3. Theoretical results,[0],[0]
:= sup ‖f‖C≤1 ∣∣∣Π[fd]− Π̂[fd]∣∣∣ .,3. Theoretical results,[0],[0]
(2) Proposition 2 (Optimally weighted quadrature rule in HC).,3. Theoretical results,[0],[0]
"For a fixed point set X , denote by Π̂[f ] = W>f(X) any quadrature rule for the vector-valued function f = (f1, . . .",3. Theoretical results,[0],[0]
", fD) and by Π̂BQ[f ]",3. Theoretical results,[0],[0]
"= W>BQf(X) the BQ rule with GP(0,C) prior.",3. Theoretical results,[0],[0]
"Then, ∀d = 1, . . .",3. Theoretical results,[0],[0]
", D:
WBQ = arg min W∈RND×D
e(HC , Π̂,X, d).
",3. Theoretical results,[0],[0]
"In specific cases, it is also possible to characterise the rate of convergence of the worst-case error for each element fd.",3. Theoretical results,[0],[0]
"This is for example the case with the separable kernel introduced in Sec. 2, as will be demonstrated in the Theorem 1 below.",3. Theoretical results,[0],[0]
"First, we introduce some technical definitions which will be required for the statement of the theorem.
",3. Theoretical results,[0],[0]
We say that a domain X ⊂,3. Theoretical results,[0],[0]
"Rp satisfies an interior cone condition if there exists an angle θ ∈ (0, π2 ) and a radius r > 0",3. Theoretical results,[0],[0]
"such that ∀x ∈ X , a unit vector ξ(x) exists such that the cone {x + λy : y ∈ Rp, ‖y‖2 = 1,y>ξ(x) ≥ cos θ, λ ∈",3. Theoretical results,[0],[0]
"[0, r]} is a subset of X .
",3. Theoretical results,[0],[0]
"For a point setX , we call hX,X :=",3. Theoretical results,[0],[0]
supx∈X infxj∈X,3. Theoretical results,[0],[0]
"‖x− xj‖2 the fill distance, qX := 12 minj 6=k ‖xj−xk‖2",3. Theoretical results,[0],[0]
"the separation radius and ρX,X := hX,X /qX the mesh ratio.",3. Theoretical results,[0],[0]
"We will assume we evaluate all integrands on the same point setX which satisfies either of these assumptions:
(A1) X consists of independently and identically distributed (IID) samples from some probability measure Π′ which admits a density π′ > 0",3. Theoretical results,[0],[0]
"on X .
(A2) X is a quasi-uniform grid on X ⊂",3. Theoretical results,[0],[0]
"Rp (i.e. satisfies hX,X ≤ C1N− 1 p for some C1 > 0) and satisfies
hX,X ≤ C2qX,X for some C2 > 0.
",3. Theoretical results,[0],[0]
"Examples of point sets satisfying (A2) include uniform grid points in some hypercube.
",3. Theoretical results,[0],[0]
Theorem 1 (Convergence rate for BQ with separable kernel).,3. Theoretical results,[0],[0]
Suppose we want to approximate Π[f ] for some f : X,3. Theoretical results,[0],[0]
"→ RD and Π̂BQ[f ] is the multi-output BQ rule with the kernel C(x,x′) = Bc(x,x′) for some positive definite B ∈ RD×D and scalar-valued",3. Theoretical results,[0],[0]
kernel c :,3. Theoretical results,[0],[0]
"X × X → R. Then, ∀d = 1, . . .",3. Theoretical results,[0],[0]
", D, we have:
e(HC , Π̂BQ,X, d) =",3. Theoretical results,[0],[0]
"O ( e(Hc, Π̂BQ,X) ) .
",3. Theoretical results,[0],[0]
"In particular, assume that X ⊂",3. Theoretical results,[0],[0]
Rp satisfies an interior cone condition with Lipschitz boundary3 and X satisfies assumption (A1) or (A2).,3. Theoretical results,[0],[0]
"Then, the following rates hold:
• IfHc is norm-equivalent to an RKHS with Matérn kernel of smoothness α > p2 , we have ∀d = 1, . . .",3. Theoretical results,[0],[0]
", D:
e(HC , Π̂BQ,X, d) =",3. Theoretical results,[0],[0]
"O ( N− α p+ ) ,
for > 0 arbitrarily small.
",3. Theoretical results,[0],[0]
"• If Hc is norm-equivalent to the RKHS with squaredexponential, multiquadric or inverse multiquadric kernel, we have ∀d = 1, . . .",3. Theoretical results,[0],[0]
", D:
e(HC , Π̂BQ,X, d) =",3. Theoretical results,[0],[0]
"O ( exp ( −C1N 1 p− )) ,
for someC1 > 0 and for some > 0 arbitrarily small.
",3. Theoretical results,[0],[0]
Proposition 3 (Convergence rate for sum of kernels).,3. Theoretical results,[0],[0]
"Suppose that C(x,x′) = ∑Q q=1Cq(x,x ′).",3. Theoretical results,[0],[0]
"Then:
e(HC , Π̂BQ,X, d) = arg max q∈{1,...,Q}
O",3. Theoretical results,[0],[0]
"( e(HCq , Π̂BQ,X, d) ) .
",3. Theoretical results,[0],[0]
"We clarify that the notation with is common in the numerical integration literature, and is used to hide powers of log n terms since these do not have a significant influence on the asymptotic convergence rate.
",3. Theoretical results,[0],[0]
"It is interesting to note that the rate of convergence for multi-output BQ is the same as that of uni-output BQ (Briol et al., 2015b).",3. Theoretical results,[0],[0]
"This can be explained intuitively by the fact that, when adding a new integrand, we can only gain by a constant factor since we always evaluate the functions at the same input points.",3. Theoretical results,[0],[0]
"In fact the proof of Thm. 1 provides an expression for this improvement factor (in terms of WCE) for any integrand fd, and this depends explicitly on its correlation with the other functions: | ∑D i,j=1(B
−1)ijBidBjd|.",3. Theoretical results,[0],[0]
"From a practitioner’s viewpoint, this can clearly be used to balance the value of using several integrands with the additional computational cost incurred by using multi-output BQ.
",3. Theoretical results,[0],[0]
We now give a result in the misspecified setting when the function f is assumed to be smoother than it is.,3. Theoretical results,[0],[0]
"In this case, it is still possible to recover the optimal convergence rate:
Theorem 2 (Misspecified Convergence Result for Separable Kernel).",3. Theoretical results,[0],[0]
Let cα be a kernel norm-equivalent to a Matérn kernel of smoothness α on some domain X with Lipschitz boundary and satisfying an interior cone condition.,3. Theoretical results,[0],[0]
"Consider the BQ rule Π̂BQ[f ] corresponding to a separable kernel Cα(x, x′) = Bcα(x, x′) with X satisfying
3Formally defined in Appendix A for completeness.
",3. Theoretical results,[0],[0]
Figure 1.,3. Theoretical results,[0],[0]
"Multi-fidelity modelling: Plot of the Step function (top), Forrester function (bottom) for the low fidelity (left) and high fidelity (right).",3. Theoretical results,[0],[0]
"Each plot gives the true function (blue) and their unit-output (dashed, red), LMC-based multi-output (dashed, yellow) and PC-based multi-output (dotted purple) approximations.
",3. Theoretical results,[0],[0]
"(A2), and suppose that f ∈ HCβ where p 2 ≤ β ≤ α.",3. Theoretical results,[0],[0]
"Then, ∀d = 1, . . .",3. Theoretical results,[0],[0]
", D:∣∣∣Π[fd]−",3. Theoretical results,[0],[0]
"Π̂BQ[fd]∣∣∣ = O (N− βp+ ) , for some > 0.
",3. Theoretical results,[0],[0]
This last theorem demonstrate that the method is rate adaptive as long as we choose a kernel which is too smooth.,3. Theoretical results,[0],[0]
"However, it also demonstrates a drawback of the separable kernels: if one of the integrands is rough but all other are smooth, then the worst-case error could potentially converge slowly for all of them.
",3. Theoretical results,[0],[0]
"Finally, we note that studying the method in other information complexity settings than the worst-case would also be interesting.",3. Theoretical results,[0],[0]
"For example, it is trivial to show that the method above satisfies the definition of Bayesian probabilistic numerical method of (Cockayne et al., 2017) (Def. 2.5).",3. Theoretical results,[0],[0]
"Furthermore, optimality conditions for this method could also be obtained in a game-theoretic setting (in terms of a two-player mixed strategies game) by extending the theory on gamblets by (Owhadi & Scovel, 2017).",3. Theoretical results,[0],[0]
"Multi-fidelity modelling Consider some function f high : X → R representing some complex engineering model of interest, which we would like to use for some task such as statistical inference or optimization.",4. Applications,[0],[0]
"These models usually require the simulation of underlying physical systems, which can make each evaluation prohibitively expensive and will therefore limit N to the order of tens or hundreds.",4. Applications,[0],[0]
"To tackle this issue, multi-fidelity modelling proposes to build cheap, but less accurate, alternatives f low1 , . . .",4. Applications,[0],[0]
", f low",4. Applications,[0],[0]
"D−1 :
Model BQ LMC-BQ PC-BQ
Step (l) 0.02 (0.22) 0.02 (0.21) 0.02 (0.52)",4. Applications,[0],[0]
Step (h) 0.41 (0.03) 0.09 (0.09) 0.04 (0.15) For.,4. Applications,[0],[0]
(l) 0.08 (4.91) 0.08 (4.95) 0.07 (33.95) For.,4. Applications,[0],[0]
"(h) 3.96 (3.98) 2.86 (27.01) 1.06 (63.80)
",4. Applications,[0],[0]
"X → R to f high, and use the cheaper models in order to accelerate computation for the task of interest.",4. Applications,[0],[0]
"This can be done using surrogate models (e.g. support vector machines, GPs or neural networks), projection-based models (Krylov subspace or reduced basis methods) or models where the underlying physics is simplified; see (Peherstorfer et al., 2016a) for an overview.
",4. Applications,[0],[0]
"In this section, we consider the problem of numerical integration in such a multi-fidelity setup.",4. Applications,[0],[0]
"Note that two related methods for Monte Carlo estimation are the multi-fidelity Monte Carlo estimator (Peherstorfer et al., 2016a) and the multilevel Monte Carlo of (Giles, 2015), both of which are based on control variate identities.
",4. Applications,[0],[0]
"We approach this problem with multi-output BQ on the vector-valued function f = (f high, f low1 , . . .",4. Applications,[0],[0]
", f low D−1)
>.",4. Applications,[0],[0]
"Note that multi-output Gaussian processes were already proposed for multi-fidelity modelling in (Perdikaris et al., 2016; Parussini et al., 2017), and we extend their methodologies to the task of numerical integration.",4. Applications,[0],[0]
"We consider two toy problems from this literature (Raissi & Karniadakis, 2016) to highlight some of the advantages and disadvantages of our methodology
1.",4. Applications,[0],[0]
A step function on X =,4. Applications,[0],[0]
"[0, 2]:
f low1 (x) = { 0, x ≤ 1 1, x > 1 f high(x) = { −1, x ≤ 1 2, x > 1
2.",4. Applications,[0],[0]
The Forrester function with Jump on X =,4. Applications,[0],[0]
"[0, 1]:
f low1 (x) =
{ (3x−1)2 sin(12x−4)
4 + 10(x− 1), x ≤ 1 2
3 + (3x−1) 2 sin(12x−4)
4 + 10(x− 1), x > 1 2
f high(x) =
{ 2f low(x)− 20(x− 1), x ≤ 12
4 + 2f low(x)− 20(x− 1), x > 12
The functions and conditioned GPs are given in Fig. 1, whilst the uni-output and multi-output BQ estimates for integration of these functions against a uniform measure Π
are given in the table in Fig. 2.",4. Applications,[0],[0]
"In both cases, 20 equidistant points are used, with point number 4, 10, 11, 14 and 17 used to evaluate the high fidelity model and the others used for the low fidelity model.",4. Applications,[0],[0]
The choice of kernel hyperparameters is made by maximising the marginal likelihood (often called empirical Bayes).,4. Applications,[0],[0]
"Further details, and an additional test function can be found in Appendix D.2.
",4. Applications,[0],[0]
Note that both of these problems are challenging for several reasons.,4. Applications,[0],[0]
"Firstly, due to their discontinuity, the integrands are not in the RKHS HC corresponding to the kernel C used in multi-output BQ.",4. Applications,[0],[0]
"In particular, the problems are misspecified in the sense that the true function is not in the support of the prior.",4. Applications,[0],[0]
"It is therefore difficult to interpret the posterior distribution on Π[f ], and we end up with credible intervals which are too wide.",4. Applications,[0],[0]
This is for example illustrated in the values of the posterior variance for the high-fidelity Forrester function.,4. Applications,[0],[0]
"Secondly, in each case, the high and low-fidelity models are defined on different scales and so require tuning of several kernel hyper-parameters.",4. Applications,[0],[0]
"This can of course make it challenging for multi-output BQ since the number of function evaluations N is small and empirical Bayes will tend to be inefficient in those cases.
",4. Applications,[0],[0]
"However, despite these two issues, it is interesting to note that both of the multi-output BQ methods manage to significantly outperform uni-output BQ in terms of point estimate, as the sharing of information allows the multi-output models to better represent the main trends in the functions.",4. Applications,[0],[0]
"Furthermore, the multi-output BQ does not suffer from the issues of overconfident posterior credible intervals present in uni-output BQ; contrast for example the posterior variances for the high-fidelity step function.
",4. Applications,[0],[0]
"Global illumination In this section, we apply multioutput BQ to a challenging numerical integration problem from the field of computer graphics, known as global illumination.",4. Applications,[0],[0]
"BQ was previously applied to this problem in several papers (Brouillat et al., 2009; Marques et al., 2013; Briol et al., 2015b), but we propose to extend these results using multi-output BQ.
",4. Applications,[0],[0]
Global illumination is a problem which occurs when trying to obtain realistic representation of light interactions for the design of virtual environments (e.g. a video game).,4. Applications,[0],[0]
"One model of the amount of light coming from an object towards the camera (representing the current viewpoint on this environment) is given by the following equation:
L0(ω0) = Le(ω0) + ∫ S2 Li(ωi)ρ(ωi, ω0)[ωi · n]+dΠ(ωi).
",4. Applications,[0],[0]
"where [x]+ = max(0, x).",4. Applications,[0],[0]
"The function L0 : S2 → R evaluated at ω0 is called the outgoing radiance in direction ω0 (the angle of the outgoing light from the object normal n), Le(ω0) : S2 → R is the amount of light emitted by
the object, and Li : S2 → R evaluated at ωi is the amount of light reflected by the object (which originated from an angle ωi from the object’s normal n).",4. Applications,[0],[0]
"Here, S2 = {x = (x1, x2, x3) ∈ R3 : ‖x‖2 = 1} and ρ(ωi, ω0) :",4. Applications,[0],[0]
"S2 × S2 → R is called the bidirectional reflectance distribution and represents the proportion of light being reflected.
",4. Applications,[0],[0]
"We follow (Briol et al., 2015b) and consider the problem as Π[hω0 ]",4. Applications,[0],[0]
"= ∫ S2 h
ω0(ωi)Π(dωi) where Π is the uniform measure on S2, and hω0(ωi) = Li(ωi)ρ(ωi, ω0)[ωi · ω0]+ is a function which can be evaluated by making a call to an environment map (which we consider to be a black box).",4. Applications,[0],[0]
"One scenario which is common in these type of problems is to look at an object from different angles ω0, with the camera moving.",4. Applications,[0],[0]
"In this case, it is reasonable to assume that the different integrands hω0 will be very similar when the difference in the angle ω0 is small, and it is therefore natural to consider jointly estimating their integrals.",4. Applications,[0],[0]
"In the experiments we consider five integrands fi = hω i 0 for i = 1, . . .",4. Applications,[0],[0]
", 5 where ω10 , . .",4. Applications,[0],[0]
.,4. Applications,[0],[0]
", ω 5 0 are on a great circle of the sphere at intervals determined by an angle of 0.005π.
",4. Applications,[0],[0]
We therefore consider two-output and five-output BQ with independent and identically distributed (Monte Carlo) samples X from the uniform measure,4. Applications,[0],[0]
"Π. We propose to use a separable kernel with scalar-valued RKHS Hc being a Sobolev space of smoothness 32 over S
2 and has kernel c(x,x′) = 83",4. Applications,[0],[0]
− ‖x,4. Applications,[0],[0]
"− x
′‖22.",4. Applications,[0],[0]
"For the matrix B representing the covariance between outputs, we propose to make this covariance proportional to the difference in angle at which the camera looks at the object.",4. Applications,[0],[0]
In particular we choose (B)ij = exp(ω i 0 · ω,4. Applications,[0],[0]
j 0,4. Applications,[0],[0]
"− 1) for simplicity, but this could be generalised to include a lengthscale and amplitude hyperparameter to be learnt together with the hyperparameters of the scalar-valued kernel c.
The GP means for the one-output and five-output cases are given in Fig. 3, and we can clearly notice a significant improvement in approximation accuracy with the larger number of outputs.",4. Applications,[0],[0]
Results for integration error are given in Fig. 4.,4. Applications,[0],[0]
"As noticed, the integration error (for a fixed number of evaluationsN of each integrand) is significantly reduced by increasing the number of outputs D. The individual posterior variances for this problem (see Appendix D.3 Fig. 10) are also smaller, reflecting the fact that our uncertainty is reduced due to use of observations from other integrands.
",4. Applications,[0],[0]
"In fact, a small extension of Thm. 1 (combined with the rate for the scalar-valued kernel in (Briol et al., 2015b)) allows us to obtain an asymptotic convergence rate for the posterior variance on each integral Π[fd]: Corollary 1.",4. Applications,[0],[0]
LetX be the sphere S2 andX be IID uniform points onX .,4. Applications,[0],[0]
AssumeC is a separable kernel with c defined above.,4. Applications,[0],[0]
"Then e(HC , Π̂BQ,X, d) = OP ( N− 3 4 ) .
",4. Applications,[0],[0]
"The same rate with improved rate constant was observed
in (Briol et al., 2015b) when using QMC point sets, and similar gains could be obtained in this multi-output case.
",4. Applications,[0],[0]
We note that there a significant potential further gains for the use of multi-output BQ in this setting.,4. Applications,[0],[0]
"Similar integration problems need to be computed for three colors in every pixel of an image, and for every image in a video.",4. Applications,[0],[0]
This is challenging computationally and limits the use of Monte Carlo methods to a few dozen points.,4. Applications,[0],[0]
Designing specific matrix-valued kernels could provide enormous gains since we end up with thousands of correlated integrands.,4. Applications,[0],[0]
"Furthermore, the weights only depend on the choice of kernel and not on function values, so that all of the weights could be pre-computed off-line to be later used in real-time.",4. Applications,[0],[0]
We have proposed an extension of Bayesian Quadrature to the case where we are interested in numerically computing the integral of several functions which are related.,5. Conclusion,[0],[0]
In particular we have proposed a new algorithm based on jointly modelling the integrands with a Gaussian prior.,5. Conclusion,[0],[0]
"Then, we provided a theoretical study of the rate of convergence for the case where the kernel is separable and illustrated the potential of our methodology on applications in multi-fidelity
modelling and computer graphics.",5. Conclusion,[0],[0]
"Our main contribution however, has been to highlight the natural extension of Bayesian probabilistic numerical methods to the joint estimation of the solution of several numerical problems (in this case, numerical integration problems).
",5. Conclusion,[0],[0]
There are several possible extensions of multi-output BQ which we reserve for future work.,5. Conclusion,[0],[0]
One important question remaining is that of the choice of sampling distribution.,5. Conclusion,[0],[0]
"In the uni-output case, it is well known that obtaining an optimal sampling distribution with respect to the Vn[Π[f ]] is intractable in most cases.",5. Conclusion,[0],[0]
"(Briol et al., 2017) proposed an algorithm to approach such a distribution, and (Kanagawa et al., 2017) provided conditions on the point sets to guarantee fast convergence.",5. Conclusion,[0],[0]
"In the multi-output case, the problem is even more complex due to the interaction between the different integration problems.",5. Conclusion,[0],[0]
"However, the literature on the design of experiments for co-kriging/multi-output GPs may be of interest, and the use of more advanced sampling distributions will certainly provide significant gains.",5. Conclusion,[0],[0]
"The authors are grateful to Alessandro Barp, Aretha Teckentrup, Chris Oates and Motonobu Kanagawa for helpful discussions.",Acknowledgements,[0],[0]
"FXB was supported by the EPSRC grants [EP/L016710/1, EP/R018413/1, EP/N510129/1].",Acknowledgements,[0],[0]
"MG was supported by the EPSRC grants [EP/J016934/3, EP/K034154/1, EP/P020720/1, EP/R018413/1, EP/N510129/1], an EPSRC Established Career Fellowship, the EU grant [EU/259348] and the
Lloyds Register Foundation Programme on Data-Centric Engineering.",Acknowledgements,[0],[0]
The authors would like to thank the Isaac Newton Institute for Mathematical Sciences for support and hospitality during the programme on “Uncertainty Quantification for Complex Systems: Theory and Methodologies”.,Acknowledgements,[0],[0]
This work was supported by EPSRC grant no [EP/K032208/1].,Acknowledgements,[0],[0]
"Finally, this material was also based upon work partially supported by the National Science Foundation under Grant DMS-1127914 to the Statistical and Applied Mathematical Sciences Institute.",Acknowledgements,[0],[0]
Bayesian probabilistic numerical methods are a set of tools providing posterior distributions on the output of numerical methods.,abstractText,[0],[0]
The use of these methods is usually motivated by the fact that they can represent our uncertainty due to incomplete/finite information about the continuous mathematical problem being approximated.,abstractText,[0],[0]
"In this paper, we demonstrate that this paradigm can provide additional advantages, such as the possibility of transferring information between several numerical methods.",abstractText,[0],[0]
"This allows users to represent uncertainty in a more faithful manner and, as a by-product, provide increased numerical efficiency.",abstractText,[0],[0]
We propose the first such numerical method by extending the well-known Bayesian quadrature algorithm to the case where we are interested in computing the integral of several related functions.,abstractText,[0],[0]
"We then prove convergence rates for the method in the well-specified and misspecified cases, and demonstrate its efficiency in the context of multi-fidelity models for complex engineering systems and a problem of global illumination in computer graphics.",abstractText,[0],[0]
Bayesian Quadrature for Multiple Related Integrals,title,[0],[0]
Deep learning has dramatically advanced the state of the art in a number of domains.,1. Introduction,[0],[0]
"Despite their unprecedented discriminative power, deep networks are prone to make mistakes.",1. Introduction,[0],[0]
"Nevertheless, they can already be found in settings where errors carry serious repercussions such as autonomous vehicles (Chen et al., 2016) and high frequency trading.",1. Introduction,[0],[0]
"We can soon expect automated systems to screen for various types of cancer (Esteva et al., 2017; Shen, 2017) and diagnose biopsies (Djuric et al., 2017).",1. Introduction,[0],[0]
"As autonomous systems based on deep learning are increasingly deployed in settings with the potential to cause physical or economic harm, we need to develop a better understanding of when we can be confident in the estimates produced by deep networks, and when we should be less certain.
",1. Introduction,[0],[0]
Standard deep learning techniques used for supervised learning lack methods to account for uncertainty in the model.,1. Introduction,[0],[0]
"This can be problematic when the network encounters conditions it was not exposed to during training,
* Co-first authorship 1School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden 2Current address: Electronic Arts, SEED, Stockholm, Sweden.",1. Introduction,[0],[0]
This work was carried out at Budbee AB.,1. Introduction,[0],[0]
3Science for Life Laboratory.,1. Introduction,[0],[0]
"Correspondence to: Kevin Smith <ksmith@kth.se>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"or if the network is confronted with adversarial examples (Goodfellow et al., 2014).",1. Introduction,[0],[0]
"When exposed to data outside the distribution it was trained on, the network is forced to extrapolate, which can lead to unpredictable behavior.
",1. Introduction,[0],[0]
"If the network can provide information about its uncertainty in addition to its point estimate, disaster may be avoided.",1. Introduction,[0],[0]
"In this work, we focus on estimating such predictive uncertainties in deep networks (Figure 1).
",1. Introduction,[0],[0]
"The Bayesian approach provides a theoretical framework for modeling uncertainty (Ghahramani, 2015), which has prompted several attempts to extend neural networks (NN) into a Bayesian setting.",1. Introduction,[0],[0]
"Most notably, Bayesian neural networks (BNNs) have been studied since the 1990’s (Neal, 2012), but do not scale well and struggle to compete with modern deep learning architectures.",1. Introduction,[0],[0]
"Recently, (Gal & Ghahramani, 2015) developed a practical solution to obtain uncertainty estimates by casting dropout training in conventional deep networks as a Bayesian approximation of a Gaussian Process (its correspondence to a general approximate Bayesian model was shown in (Gal, 2016)).",1. Introduction,[0],[0]
"They showed that any network trained with dropout is an approximate Bayesian model, and uncertainty estimates can be obtained by computing the variance on multiple predictions with different dropout masks.
",1. Introduction,[0],[0]
"The inference in this technique, called Monte Carlo Dropout (MCDO), has an attractive quality: it can be applied to any pre-trained networks with dropout layers.",1. Introduction,[0],[0]
Uncertainty estimates come (nearly) for free.,1. Introduction,[0],[0]
"However, not all architectures use dropout, and most modern networks have adopted other regularization techniques.",1. Introduction,[0],[0]
"Batch normalization (BN), in particular, has become widespread thanks to its ability to stabilize learning with improved generalization (Ioffe & Szegedy, 2015).
",1. Introduction,[0],[0]
An interesting aspect of BN is that the mini-batch statistics used for training each iteration depend on randomly selected batch members.,1. Introduction,[0],[0]
"We exploit this stochasticity and show that training using batch normalization, like dropout, can be cast as an approximate Bayesian inference.",1. Introduction,[0],[0]
We demonstrate how this finding allows us to make meaningful estimates of the model uncertainty in a technique we call Monte Carlo Batch Normalization (MCBN) (Figure 1).,1. Introduction,[0],[0]
"The method we propose can be applied to any network using standard batch normalization.
",1. Introduction,[0],[0]
"We validate our approach by empirical experiments on a variety of datasets and tasks, including regression and image classification.",1. Introduction,[0],[0]
"We measure uncertainty quality relative to a baseline of fixed uncertainty, and show that MCBN outperforms the baseline on nearly all datasets with strong statistical significance.",1. Introduction,[0],[0]
We also show that the uncertainty quality of MCBN is on par with other recent approximate Bayesian networks.,1. Introduction,[0],[0]
"Bayesian models provide a natural framework for modeling uncertainty, and several approaches have been developed to adapt NNs to Bayesian reasoning.",2. Related Work,[0],[0]
A common approach is to place a prior distribution (often a Gaussian) over each parameter.,2. Related Work,[0],[0]
"The resulting model corresponds to a Gaussian process for infinite parameters (Neal, 1995), and a Bayesian NN (MacKay, 1992) for a finite number of parameters.",2. Related Work,[0],[0]
"Inference in BNNs is difficult however (Gal, 2016), so focus has thus shifted to techniques that approximate the posterior, approximate BNNs.",2. Related Work,[0],[0]
"Methods based on variational inference (VI) typically rely on a fully factorized approximate distribution (Kingma & Welling, 2014; Hinton & Van Camp, 1993), but often do not scale.",2. Related Work,[0],[0]
"To alleviate these difficulties, (Graves, 2011) proposed a model using sampling methods to estimate a factorized posterior.",2. Related Work,[0],[0]
"Probabilistic backpropagation (PBP), estimates a factorized posterior via expectation propagation (HernándezLobato & Adams, 2015).
",2. Related Work,[0],[0]
"Using several strategies to address scaling issues, Deep
Gaussian Processes show superior performance in terms of RMSE and uncertainty quality compared to state-of-the-art approximate BNNs (Bui et al., 2016)1.",2. Related Work,[0],[0]
"Another recent approach to Bayesian learning, Bayesian hypernetworks, use a NN to learn a distribution of parameters over another network (Krueger et al., 2017).",2. Related Work,[0],[0]
"Multiplicative Normalizing Flows for variational Bayesian networks (MNF) (Louizos & Welling, 2017) is a recent model that formulates a posterior dependent on auxiliary variables.",2. Related Work,[0],[0]
"MNF achieves a highly flexible posterior by the application of normalizing flows to the auxiliary variables.
",2. Related Work,[0],[0]
"Although these recent techniques address some of the difficulties with approximate BNNs, they all require modifications to the architecture or the way networks are trained, as well as specialized knowledge from practitioners.",2. Related Work,[0],[0]
"Recently, (Gal & Ghahramani, 2015) showed that a network trained with dropout implicitly performs the VI objective.",2. Related Work,[0],[0]
Therefore any network trained with dropout can be treated as an approximate Bayesian model by making multiple predictions through the network while sampling different dropout masks for each prediction.,2. Related Work,[0],[0]
The mean and variance of the predictions are used in the estimation of the mean and variance of the predictive distribution 2.,2. Related Work,[0],[0]
"In the following, we introduce Bayesian models and a variational approximation using Kullback-Leibler (KL) divergence following (Gal, 2016).",3. Method,[0],[0]
We continue by showing that a batch normalized deep network can be seen as an approximate Bayesian model.,3. Method,[0],[0]
"Employing theoretical insights and empirical analysis, we study the induced prior on the parameters when using batch normalization.",3. Method,[0],[0]
"Finally, we describe the procedure for estimating the uncertainty of a batch normalized network’s output.3",3. Method,[0],[0]
"We assume a finite training set D = {(xi,yi)}i=1:N where each (xi,yi) is a sample-label pair.",3.1. Bayesian Modeling,[0],[0]
"Using D, we are interested in learning an inference function fω(x,y) with parameters ω.",3.1. Bayesian Modeling,[0],[0]
"In deterministic models, the estimated label ŷ is obtained as follows:
ŷ = arg max y fω(x,y)
",3.1. Bayesian Modeling,[0],[0]
"In probabilistic models we let fω(x,y) = p(y|x,ω).",3.1. Bayesian Modeling,[0],[0]
"In Bayesian modeling, in contrast to finding a point estimate
1By uncertainty quality, we refer to predictive probability distributions as measured by PLL and CRPS.
2This technique is referred to as “MC Dropout” in the original work, though we refer to it here as MCDO.
",3.1. Bayesian Modeling,[0],[0]
"3While the method applies to FC or Conv layers, the induced prior from weight decay (Section 3.3) is studied for FC layers.
of the model parameters, the idea is to estimate an (approximate) posterior distribution of the model parameters p(ω|D) to be used for probabilistic prediction:
p(y|x,D) = ∫ fω(x,y)p(ω|D)dω
The predicted label, ŷ, can then be accordingly obtained by sampling p(y|x,D) or taking its maxima.
",3.1. Bayesian Modeling,[0],[0]
"Variational Approximation In approximate Bayesian modeling, a common approach is to learn a parameterized approximating distribution qθ(ω) that minimizes KL(qθ(ω)||p(ω|D)); the Kullback-Leibler divergence of the true posterior w.r.t.",3.1. Bayesian Modeling,[0],[0]
its approximation.,3.1. Bayesian Modeling,[0],[0]
"Minimizing this KL divergence is equivalent to the following minimization while being free of the data term p(D) 4:
LVA(θ) :",3.1. Bayesian Modeling,[0],[0]
"=− N∑ i=1 ∫ qθ(ω) ln fω(xi,yi)dω
+ KL(qθ(ω)||p(ω))
",3.1. Bayesian Modeling,[0],[0]
"During optimization, we want to take the derivative of the expected likelihood w.r.t.",3.1. Bayesian Modeling,[0],[0]
the learnable parameters θ.,3.1. Bayesian Modeling,[0],[0]
"We use the same MC estimate as in (Gal, 2016) (explained in Appendix Section 1.1), such that one realized ω̂i is taken for each sample i 5.",3.1. Bayesian Modeling,[0],[0]
"Optimizing over mini-batches of size M , the approximated objective becomes:
L̂VA(θ) :",3.1. Bayesian Modeling,[0],[0]
"= − N
M M∑ i=1",3.1. Bayesian Modeling,[0],[0]
"ln fω̂i(xi,yi) + KL(qθ(ω)||p(ω))",3.1. Bayesian Modeling,[0],[0]
"(1)
The first term is the data likelihood and the second term is the divergence of the prior w.r.t.",3.1. Bayesian Modeling,[0],[0]
the approximated posterior.,3.1. Bayesian Modeling,[0],[0]
"We now describe the optimization procedure of a deep network with batch normalization and draw the resemblance to the approximate Bayesian modeling in Eq (1).
",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"The inference function of a feed-forward deep network with L layers can be described as:
fω(x) = W La(WL−1...a(W2a(W1x))
4Achieved by constructing the Evidence Lower Bound, called ELBO, and assuming i.i.d. observation noise; details can be found in Appendix Section 1.1.
",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"5While a MC integration using a single sample is a weak approximation, in an iterative optimization for θ several samples will be taken over time.
",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"where a(.) is an element-wise nonlinearity function and Wl is the weight vector at layer l. Furthermore, we denote the input to layer l as xl with x1 = x",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
and we then set hl = Wlxl.,3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
Parenthesized super-index for matrices (e.g. W(j)) and vectors (e.g. x(j)) indicates jth row and element respectively.,3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"Super-index u refers to a specific unit at layer l, (e.g. Wu = Wl,(j), hu = hl,(j)).",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"6
Batch Normalization Each layer of a deep network is constructed by several linear units whose parameters are the rows of the weight matrix W. Batch normalization is a unit-wise operation proposed in (Ioffe & Szegedy, 2015) to standardize the distribution of each unit’s input.",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"For FC layers, it converts a unit’s input hu in the following way:
ĥu = hu",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"− E[hu]√
Var[hu]
where the expectations are computed over the training set during evaluation, and mini-batch during training (in deep networks, the weight matrices are often optimized using back-propagated errors calculated on mini-batches of data)7.",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"Therefore, during training, the estimated mean and variance on the mini-batch B is used, which we denote by µB and σB respectively.",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"This makes the inference at training time for a sample x a stochastic process, varying based on other samples in the mini-batch.
",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"Loss Function and Optimization Training deep networks with mini-batch optimization involves a (regularized) risk minimization with the following form:
LRR(ω) :",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"= 1
M M∑ i=1",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"l(ŷi,yi) + Ω(ω)
where the first term is the empirical loss on the training data and the second term is a regularization penalty acting as a prior on model parameters ω.",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
If the loss l is cross-entropy for classification or sum-of-squares for regression problems (assuming i.i.d.,3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"Gaussian noise on labels), the first term is equivalent to minimizing the negative log-likelihood:
LRR(ω) := − 1
Mτ M∑ i=1",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"ln fω(xi,yi) + Ω(ω)
6For a (softmax) classification network, fω(x) is a vector with fω(x,y) = fω(x)
(y), for regression networks with i.i.d.",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"Gaussian noise we have fω(x,y) = N (fω(x), τ−1I).
",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"7It also learns an affine transformation for each unit with parameters γ and β, omitted for brevity: x̂(j)affine = γ (j)x̂(j) + β(j).
with τ = 1 for classification.",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"In a network with batch normalization, the model parameters include {W1:L,γ1:L,β1:L,µ1:LB ,σ1:LB }.",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"If we decouple the learnable parameters θ = {W1:L,γ1:L,β1:L} from the stochastic parameters ω = {µ1:LB ,σ1:LB }, we get the following objective at each step of the mini-batch optimization:
LRR(θ) := − 1
Mτ M∑ i=1",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"ln f{θ,ω̂i}(xi,yi) + Ω(θ) (2)
where ω̂i is the means and variances for sample i’s minibatch at a certain training step.",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
Note that while ω̂i formally needs to be i.i.d.,3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"for each training example, a batch normalized network samples the stochastic parameters once per training step (mini-batch).",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"For a large number of epochs, however, the distribution of sampled batch members for a given training example converges to the i.i.d. case.
",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"In a batch normalized network, qθ(ω) corresponds to the joint distribution of the weights, induced by the randomness of the normalization parameters µ1:LB ,σ 1:L B , as implied by the repeated sampling from D during training.",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"This is an approximation of the true posterior, where we have restricted the posterior to lie within the domain of our parametric network and source of randomness.",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
"With that, we can estimate the uncertainty of predictions from a trained batch normalized network using the inherent stochasticity of BN (Section 3.4).",3.2. Batch Normalized Deep Nets as Bayesian Modeling,[0],[0]
Equivalence between the VA and BN training procedures requires ∂∂θ of Eq.,3.3. Prior p(ω),[0],[0]
(1) and Eq. (2) to be equivalent up to a scaling factor.,3.3. Prior p(ω),[0],[0]
This is the case if ∂∂θKL(qθ(ω)||p(ω)),3.3. Prior p(ω),[0],[0]
"= Nτ ∂∂θΩ(θ).
",3.3. Prior p(ω),[0],[0]
"To reconcile this condition, one option is to let the prior p(ω) imply the regularization term Ω(θ).",3.3. Prior p(ω),[0],[0]
Eq. (1) reveals that the contribution of KL(qθ(ω)||p(ω)) to the optimization objective is inversely scaled with N .,3.3. Prior p(ω),[0],[0]
"For BN, this corresponds to a model with a small Ω(θ) when N is large.",3.3. Prior p(ω),[0],[0]
"In the limit as N →∞, the optimization objectives of Eq.",3.3. Prior p(ω),[0],[0]
"(1) and Eq. (2) become identical with no regularization.8
Another option is to let some Ω(θ) imply p(ω).",3.3. Prior p(ω),[0],[0]
"In Appendix Section 1.4 we explore this with L2-regularization, also called weight decay (Ω(θ) = λ ∑ l=1:L ||W l||2).",3.3. Prior p(ω),[0],[0]
"We find that unlike in MCDO (Gal, 2016), some simplifying
8To prove the existence and find an expression of KL(qθ(ω)||p(ω)), in Appendix Section 1.3 we find that BN approximately induces Gaussian distributions over BN units’ means and standard deviations, centered around the population values given by D. We assume a factorized distribution and Gaussian priors, and find the corresponding KL(qθ(ω)||p(ω)) components in Appendix Section 1.4 Eq.",3.3. Prior p(ω),[0],[0]
(7).,3.3. Prior p(ω),[0],[0]
"These could be used to construct a custom Ω(θ) for any Gaussian choice of p(ω).
assumptions are necessary to reconcile the VA and BN objectives with weight decay: no scale and shift applied to BN layers, uncorrelated units in each layer, BN applied on all layers, and large N and M .",3.3. Prior p(ω),[0],[0]
"Given these conditions:
p(µuB) = N (µµ,p, σµ,p) p(σuB) = N (µσ,p, σσ,p)
where µµ,p = 0, σµ,p →∞, µσ,p = 0 and σσ,p → 12Nτλl .
",3.3. Prior p(ω),[0],[0]
This corresponds to a wide and narrow distribution on BN units’ means and std.,3.3. Prior p(ω),[0],[0]
"devs respectively, where N accounts for the narrowness of the prior.",3.3. Prior p(ω),[0],[0]
"Due to its popularity in deep learning, our experiments in Section 4 are performed with weight decay.",3.3. Prior p(ω),[0],[0]
"In the absence of the true posterior, we rely on the approximate posterior to express an approximate predictive distribution:
p∗(y|x,D) := ∫ fω(x,y)qθ(ω)dω
Following (Gal, 2016)",3.4. Predictive Uncertainty in Batch Normalized Deep Nets,[0],[0]
"we estimate the first (for regression and classification) and second (for regression) moments of the predictive distribution empirically (see Appendix Section 1.5 for details):
Ep∗",3.4. Predictive Uncertainty in Batch Normalized Deep Nets,[0],[0]
"[y] ≈ 1
T T∑ i=1",3.4. Predictive Uncertainty in Batch Normalized Deep Nets,[0],[0]
"fω̂i(x)
Covp∗",3.4. Predictive Uncertainty in Batch Normalized Deep Nets,[0],[0]
"[y] ≈ τ−1I + 1
T T∑ i=1",3.4. Predictive Uncertainty in Batch Normalized Deep Nets,[0],[0]
"fω̂i(x) ᵀfω̂i(x)
",3.4. Predictive Uncertainty in Batch Normalized Deep Nets,[0],[0]
− Ep∗,3.4. Predictive Uncertainty in Batch Normalized Deep Nets,[0],[0]
"[y]ᵀEp∗ [y]
where each ω̂i corresponds to sampling the net’s stochastic parameters ω = {µ1:LB ,σ1:LB } the same way as during training.",3.4. Predictive Uncertainty in Batch Normalized Deep Nets,[0],[0]
"Sampling ω̂i therefore involves sampling a batch B from the training set and updating the parameters in the BN units, just as if we were taking a training step with B. From a VA perspective, training the network amounted to minimizing KL(qθ(ω)||p(ω|D))",3.4. Predictive Uncertainty in Batch Normalized Deep Nets,[0],[0]
wrt θ.,3.4. Predictive Uncertainty in Batch Normalized Deep Nets,[0],[0]
"Sampling ω̂i from the training set, and keeping the size of B consistent with the mini-batch size used during training, ensures that qθ(ω) during inference remains identical to the approximate posterior optimized during training.
",3.4. Predictive Uncertainty in Batch Normalized Deep Nets,[0],[0]
"The network is trained just as a regular BN network, but instead of replacing ω = {µ1:LB ,σ1:LB } with population values from D for inference, we update these parameters stochastically, once for each forward pass.9 Pseudocode for estimating predictive mean and variance is given in Algorithm 1.
9As an alternative to using the training set D to sample ω̂i,
",3.4. Predictive Uncertainty in Batch Normalized Deep Nets,[0],[0]
"Algorithm 1 MCBN Algorithm Input: sample x, number of inferences T , batchsize b Output: mean prediction ŷ, predictive uncertainty σ2
1: y = {} 2: loop for T iterations 3: B ∼ D // mini batch 4: ω̂ = {µB ,σB} // mini batch mean and variance 5: y = y ∪ fω̂(x) 6: end loop 7: ŷ = E[y] 8: σ2 = Cov[y] + τ−1I // for regression",3.4. Predictive Uncertainty in Batch Normalized Deep Nets,[0],[0]
We assess the uncertainty quality of MCBN quantitatively and qualitatively.,4. Experiments and Results,[0],[0]
"Our quantitative analysis relies on CIFAR10 for image classification and eight standard regression datasets, listed in Appendix Table 1.",4. Experiments and Results,[0],[0]
"Publicly available from the UCI Machine Learning Repository (University of California, 2017) and Delve (Ghahramani, 1996), these datasets have been used to benchmark comparative models in recent related literature (see (Hernández-Lobato & Adams, 2015), (Gal & Ghahramani, 2015), (Bui et al., 2016) and (Li & Gal, 2017)).",4. Experiments and Results,[0],[0]
"We report results using standard metrics, and also propose useful upper and lower bounds to normalize these metrics for an easier interpretation in Section 4.2.
",4. Experiments and Results,[0],[0]
"Our qualitative results include the toy dataset in Figure 1 in the style of (Karpathy, 2015), a new visualization of uncertainty quality that plots test errors sorted by predicted variance (Figure 2 and Appendix), and image segmentation results (Figure 2 and Appendix).",4. Experiments and Results,[0],[0]
"We evaluate uncertainty quality based on two standard metrics, described below: Predictive Log Likelihood (PLL) and Continuous Ranked Probability Score (CRPS).",4.1. Metrics,[0],[0]
"To improve the interpretability of the metrics, we propose to normalize them by upper and lower bounds.
",4.1. Metrics,[0],[0]
Predictive Log Likelihood (PLL),4.1. Metrics,[0],[0]
"Predictive Log Likelihood is widely accepted as the main uncertainty quality metric for regression (Hernández-Lobato & Adams, 2015; Gal & Ghahramani, 2015; Bui et al., 2016; Li & Gal, 2017).",4.1. Metrics,[0],[0]
A key property of PLL is that it makes no assumptions about the form of the distribution.,4.1. Metrics,[0],[0]
"The measure is defined for a probabilistic model fω(x) and a single observation
we could sample from the implied qθ(ω) as modeled in the Appendix.",4.1. Metrics,[0],[0]
This would alleviate having to store D for use during prediction.,4.1. Metrics,[0],[0]
"In our experiments we used D to sample ω̂i however, and leave the evaluation of the modeled qθ(ω) for future research.
",4.1. Metrics,[0],[0]
"(yi,xi) as:
PLL(fω(x), (yi,xi))",4.1. Metrics,[0],[0]
"= log p(yi|fω(xi))
where p(yi|fω(xi))",4.1. Metrics,[0],[0]
"is the model’s predicted PDF evaluated at yi, given the input xi.",4.1. Metrics,[0],[0]
A more detailed description is given in the Appendix Section 1.5.,4.1. Metrics,[0],[0]
The metric is unbounded and maximized by a perfect prediction (mode at yi) with no variance.,4.1. Metrics,[0],[0]
"As the predictive mode moves away from yi, increasing the variance tends to increase PLL (by maximizing probability mass at yi).",4.1. Metrics,[0],[0]
"While PLL is an elegant measure, it has been criticized for allowing outliers to have an overly negative effect on the score (Selten, 1998).
",4.1. Metrics,[0],[0]
Continuous Ranked Probability Score (CRPS) Continuous Ranked Probability Score is a measure that takes the full predicted PDF into account with less sensitivity to outliers.,4.1. Metrics,[0],[0]
A prediction with low variance that is slightly offset from the true observation will receive a higher score form CRPS than PLL.,4.1. Metrics,[0],[0]
"In order for CRPS to be analytically tractable, we need to assume a Gaussian unimodal predictive distribution.",4.1. Metrics,[0],[0]
"CRPS is defined as
CRPS(fω(xi), (yi, xi))",4.1. Metrics,[0],[0]
= ∫ ∞ −∞,4.1. Metrics,[0],[0]
( F (y)− 1(y ≥ yi) ),4.1. Metrics,[0],[0]
"2 dy
where F (y) is the predictive CDF, and 1(y ≥ yi) = 1 if y ≥ yi and 0 otherwise (for univariate distributions) (Gneiting & Raftery, 2007).",4.1. Metrics,[0],[0]
CRPS is interpreted as the sum of the squared area between the CDF and 0 where y < yi and between the CDF and 1 where y ≥ yi.,4.1. Metrics,[0],[0]
A perfect prediction with no variance yields a CRPS of 0; for all other cases the value is larger.,4.1. Metrics,[0],[0]
CRPS has no upper bound.,4.1. Metrics,[0],[0]
It is difficult to interpret the quality of uncertainty from raw PLL and CRPS values.,4.2. Benchmark models and normalized metrics,[0],[0]
We propose to normalize the metrics between useful lower and upper bounds.,4.2. Benchmark models and normalized metrics,[0],[0]
The normalized measures estimate the performance of an uncertainty model between the trivial solution (constant uncertainty) and optimal uncertainty for each prediction.,4.2. Benchmark models and normalized metrics,[0],[0]
"For the lower bound, we define a baseline that predicts constant variance regardless of input.",4.2. Benchmark models and normalized metrics,[0],[0]
The variance is set to a fixed value that optimizes CRPS on validation data.,4.2. Benchmark models and normalized metrics,[0],[0]
We call this model Constant Uncertainty BN (CUBN).,4.2. Benchmark models and normalized metrics,[0],[0]
"It reflects our best guess of constant variance on test data – thus, any improvement in uncertainty quality over CUBN indicates a sensible estimate of uncertainty.",4.2. Benchmark models and normalized metrics,[0],[0]
"We similarly define a baseline for dropout, Constant Uncertainty Dropout (CUDO).",4.2. Benchmark models and normalized metrics,[0],[0]
"The modeling of variance (uncertainty) by MCBN and CUBN are visualized in Figure 1.
",4.2. Benchmark models and normalized metrics,[0],[0]
An upper bound on uncertainty performance can also be defined for a probabilistic model f with respect to CRPS or PLL.,4.2. Benchmark models and normalized metrics,[0],[0]
"For each observation (yi, xi), a value
for the predictive variance Ti can be chosen that maximizes PLL or minimizes CRPS10.",4.2. Benchmark models and normalized metrics,[0],[0]
"Using CUBN as a lower bound and the optimized CRPS score as the upper bound, uncertainty estimates can be normalized between these bounds (1 indicating optimal performance, and 0 indicating same performance as fixed uncertainty).",4.2. Benchmark models and normalized metrics,[0],[0]
"We call this normalized measure CRPS =
CRPS(f,(yi,xi))−CRPS(fCU ,(yi,xi)) minT CRPS(f,(yi,xi))−CRPS(fCU ,(yi,xi)) × 100, and the PLL analogue PLL = PLL(f,(yi,xi))−PLL(fCU ,(yi,xi))maxT PLL(f,(yi,xi))−PLL(fCU ,(yi,xi))×100.",4.2. Benchmark models and normalized metrics,[0],[0]
"Our evaluation compares MCBN to MCDO (Gal & Ghahramani, 2015) and MNF (Louizos & Welling, 2017) using the datasets and metrics described above.",4.3. Test setup,[0],[0]
"Our setup is similar to (Hernández-Lobato & Adams, 2015), which was also followed by (Gal & Ghahramani, 2015).",4.3. Test setup,[0],[0]
"However, our comparison implements a different hyperparameter selection, allows for a larger range of dropout rates, and uses larger networks with two hidden layers.
",4.3. Test setup,[0],[0]
"For the regression task, all models share a similar architecture: two hidden layers with 50 units each, and ReLU activations, with the exception of Protein Tertiary Structure dataset (100 units per hidden layer).",4.3. Test setup,[0],[0]
Inputs and outputs were normalized during training.,4.3. Test setup,[0],[0]
Results were averaged over five random splits of 20% test and 80% training and cross-validation (CV) data.,4.3. Test setup,[0],[0]
"For each split, 5-fold CV by grid search with a RMSE minimization objective was used to find training hyperparameters and optimal n.o. epochs, out of a maximum of 2000.",4.3. Test setup,[0],[0]
"For BN-based models, the hyperparameter grid consisted of a weight decay factor ranging from 0.1 to 1−15 by a log 10 scale, and a batch size range from 32 to 1024 by a log 2 scale.",4.3. Test setup,[0],[0]
"For DO-based models, the hyperparameter grid consisted of the same weight decay range, and dropout probabilities in {0.2, 0.1, 0.05, 0.01, 0.005, 0.001}.",4.3. Test setup,[0],[0]
DO-based models used a batch size of 32 in all evaluations.,4.3. Test setup,[0],[0]
"For MNF11, the n.o. epochs was optimized, the batch size was set to 100, and early stopping test performed each epoch (compared to every 20th for MCBN, MCDO).
",4.3. Test setup,[0],[0]
"For MCBN and MCDO, the model with optimal training hyperparameters was used to optimize τ numerically.",4.3. Test setup,[0],[0]
"This optimization was made in terms of average CV CRPS for MCBN, CUBN, MCDO, and CUDO respectively.
",4.3. Test setup,[0],[0]
Estimates for the predictive distribution were obtained by taking T = 500 stochastic forward passes through the network.,4.3. Test setup,[0],[0]
"For each split, test set evaluation was done 5 times with different seeds.",4.3. Test setup,[0],[0]
"Implementation was done in TensorFlow with the Adam optimizer and a learning rate of 0.001.
",4.3. Test setup,[0],[0]
"10Ti can be found analytically for PLL, but must be found numerically for CRPS.
",4.3. Test setup,[0],[0]
"11Where we used an adapted version of the authors’ code.
",4.3. Test setup,[0],[0]
"For the image classification test we use CIFAR10 (Krizhevsky & Hinton, 2009) which includes 10 object classes with 5,000 and 1,000 images in the training and test sets, respectively.",4.3. Test setup,[0],[0]
Images are 32x32 RGB format.,4.3. Test setup,[0],[0]
"We trained a ResNet32 architecture with a batch size of 32, learning rate of 0.1, weight decay of 0.0002, leaky ReLU slope of 0.1, and 5 residual units.",4.3. Test setup,[0],[0]
"SGD with momentum was used as the optimizer.
",4.3. Test setup,[0],[0]
Code for reproducing our experiments is available at https://github.com/icml-mcbn/mcbn.,4.3. Test setup,[0],[0]
The regression experiment comparing uncertainty quality is summarized in Table 1.,4.4. Test results,[0],[0]
"We report CRPS and PLL, expressed as a percentage, which reflects how close the model is to the upper bound, and check to see if the model significantly exceeds the lower bound using a one sample t-test (significance level is indicated by *’s).",4.4. Test results,[0],[0]
"Further details are provided in Appendix Section 1.7.
",4.4. Test results,[0],[0]
"In Figure 2 (left), we present a novel visualization of uncertainty quality for regression problems.",4.4. Test results,[0],[0]
Data are sorted by estimated uncertainty in the x-axis.,4.4. Test results,[0],[0]
"Grey dots show the errors in model predictions, and the shaded areas show the model uncertainty.",4.4. Test results,[0],[0]
A running mean of the errors appears as a gray line.,4.4. Test results,[0],[0]
"If uncertainty estimation is working well, a correlation should exist between the mean error (gray line) and uncertainty (shaded area).",4.4. Test results,[0],[0]
"This indicates that the uncertainty estimation recognizes samples with larger (or smaller) potential for predictive errors.
",4.4. Test results,[0],[0]
We applied MCBN on the image classification task of CIFAR10.,4.4. Test results,[0],[0]
The baseline in this case is the softmax distribution using the moving average for BN units.,4.4. Test results,[0],[0]
Log likelihood (PLL) is the metric used to compare with the baseline.,4.4. Test results,[0],[0]
"The baseline achieves a PLL of -0.32 on the test set, while MCBN obtains a PLL of -0.28.",4.4. Test results,[0],[0]
Table 2 shows the performance of MCBN when using different number of stochastic forward passes (the MCBN batchsize is fixed to the training batch size at 32).,4.4. Test results,[0],[0]
"PLL improves as the number of the stochastic passes increases, until it is significantly better than the softmax baseline.
",4.4. Test results,[0],[0]
"To demonstrate how model uncertainty can be obtained from an existing network with minimal effort, we applied MCBN to an image segmentation task using Bayesian SegNet with the main CamVid and PASCAL-VOC models in (Kendall et al., 2015).",4.4. Test results,[0],[0]
We simply ran multiple forward passes with different mini-batches randomly taken from the train set.,4.4. Test results,[0],[0]
The models obtained from the online model zoo have BN blocks after each layer.,4.4. Test results,[0],[0]
We recalculate mean and variance for the first 2 blocks only and use the training statistics for the rest of the blocks.,4.4. Test results,[0],[0]
"Mini-batches of size 10 and 36 were used for CamVid and VOC respectively
due to memory limits.",4.4. Test results,[0],[0]
"The results in Figure 2 (right) were obtained from 20 stochastic forward passes, showing high uncertainty near object boundaries.",4.4. Test results,[0],[0]
"The VOC results are more appealing because of larger mini-batches.
",4.4. Test results,[0],[0]
We provide additional experimental results in the Appendix.,4.4. Test results,[0],[0]
Appendix Tables 2 and 3 show the mean CRPS and PLL values from the regression experiment.,4.4. Test results,[0],[0]
Table 4 provides the raw CRPS and PLL scores.,4.4. Test results,[0],[0]
In Table 5 we provide RMSE results of the MCBN and MCDO networks in comparison with non-stochastic BN and DO networks.,4.4. Test results,[0],[0]
These results indicate that the procedure of multiple forward passes in MCBN and MCDO show slight improvements in the predictive accuracy compared to their nonBayesian counterparts.,4.4. Test results,[0],[0]
"In Tables 6 and 7, we investigate the effect of varying batch size while keeping other hyperparameters fixed.",4.4. Test results,[0],[0]
"We see that performance deteriorates with small batch sizes (≤16), a known issue of BN (Ioffe, 2017).",4.4. Test results,[0],[0]
"Similarly, results varying the number of stochastic forward passes T is reported in Tables 8 and 9.",4.4. Test results,[0],[0]
"While performance benefits from large T , in some cases T = 50 (i.e. 1/10 of T in the main evaluation) performs well.",4.4. Test results,[0],[0]
Uncertainty-error plots for all the datasets are provided in the Appendix.,4.4. Test results,[0],[0]
"The results presented in Tables 1-2 and Appendix Tables 2-9 indicate that MCBN generates meaningful uncertainty
estimates that correlate with actual errors in the model’s prediction.",5. Discussion,[0],[0]
"In Table 1, we show statistically significant improvements over CUBN in the majority of the datasets, both in terms of CRPS and PLL.",5. Discussion,[0],[0]
The visualizations in Figure 2 and in the Appendix Figures 2-3 show correlations between the estimated model uncertainty and errors of the network’s predictions.,5. Discussion,[0],[0]
"We perform the same experiments using MCDO and MNF, and find that MCBN generally performs on par with both methods.",5. Discussion,[0],[0]
"Looking closer, MCBN outperforms MCDO and MNF in more cases than not, measured by CRPS.",5. Discussion,[0],[0]
"However, care must be used.",5. Discussion,[0],[0]
"The learned parameters are different, leading to different predictive means and confounding direct comparison.
",5. Discussion,[0],[0]
The results on the Yacht Hydrodynamics dataset seem contradictory.,5. Discussion,[0],[0]
"The CRPS score for MCBN are extremely negative, while the PLL score is extremely positive.",5. Discussion,[0],[0]
The opposite trend is observed for MCDO.,5. Discussion,[0],[0]
"To add to the puzzle, the visualization in Figure 2 depicts an extremely promising uncertainty estimation that models the predictive errors with high fidelity.",5. Discussion,[0],[0]
"We hypothesize that this strange behavior is due to the small size of the data set, which only contains 60 test samples, or due to the Gaussian assumption of CRPS.",5. Discussion,[0],[0]
"There is also a large variability in the model’s accuracy on this dataset, which further confounds the measurements for such limited data.
",5. Discussion,[0],[0]
"One might criticize the overall quality of uncertainty estimates observed in all the models we tested, due to the magnitude of CRPS and PLL in Table 1.",5. Discussion,[0],[0]
The scores rarely exceed 10% improvement over the lower bound.,5. Discussion,[0],[0]
"However, we caution that these measures should be taken in context.",5. Discussion,[0],[0]
"The upper bound is very difficult to achieve in practice – it is optimized for each test sample individually – and the lower bound is a quite reasonable estimate.
",5. Discussion,[0],[0]
"The study of MCBN sensitivity to batch size revealed that a certain batch size is required for the best performance, dependent on the data.",5. Discussion,[0],[0]
"When doing inference on a GPU, large
batch sizes may cause memory issues for cases where the input is large and the network has a large number of parameters, as is common for state-of-the-art image classification networks.",5. Discussion,[0],[0]
"However, there are various workarounds to this problem.",5. Discussion,[0],[0]
"One can store BN statistics, instead of batches, to reduce memory issues.",5. Discussion,[0],[0]
"Furthermore, we can use the Gaussian estimate of the BN statistics as discussed previously, which makes memory and computation extremely efficient.",5. Discussion,[0],[0]
"In this work, we have shown that training a deep network using batch normalization is equivalent to approximate inference in Bayesian models.",6. Conclusion,[0],[0]
"We show evidence that the uncertainty estimates from MCBN correlate with actual errors in the model’s prediction, and are useful for practical
tasks such as regression, image classification, and image segmentation.",6. Conclusion,[0],[0]
"Our experiments show that MCBN yields a significant improvement over the optimized constant uncertainty baseline, on par with MCDO and MNF.",6. Conclusion,[0],[0]
"Our evaluation also suggests new normalized metrics based on useful upper and lower bounds, and a new visualization which provides an intuitive explanation of uncertainty quality.
",6. Conclusion,[0],[0]
"Finally, it should be noted that over the past few years, batch normalization has become an integral part of most – if not all – cutting edge deep networks.",6. Conclusion,[0],[0]
We have shown that it is possible to obtain meaningful uncertainty estimates from existing models without modifying the network or the training procedure.,6. Conclusion,[0],[0]
"With a few lines of code, robust uncertainty estimates can be obtained by computing the variance of multiple stochastic forward passes through an existing network.",6. Conclusion,[0],[0]
We show that training a deep network using batch normalization is equivalent to approximate inference in Bayesian models.,abstractText,[0],[0]
"We further demonstrate that this finding allows us to make meaningful estimates of the model uncertainty using conventional architectures, without modifications to the network or the training procedure.",abstractText,[0],[0]
Our approach is thoroughly validated by measuring the quality of uncertainty in a series of empirical experiments on different tasks.,abstractText,[0],[0]
"It outperforms baselines with strong statistical significance, and displays competitive performance with recent Bayesian approaches.",abstractText,[0],[0]
Bayesian Uncertainty Estimation for Batch Normalized Deep Networks,title,[0],[0]
"Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 64–74, Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics",text,[0],[0]
"For the majority of the state-of-the-art parsers that routinely reach ninety percent performance plateau in capturing tree structures, the question of what next crucially arises.",1 Introduction,[0],[0]
"Indeed, it has long been thought that the bottleneck preventing the advent of accurate syntax-to-semantic interfaces lies in the quality of the preceding phase of analysis: the better the parse, the better the output.",1 Introduction,[0],[0]
"The truth is that most of the structures used to train current parsing models are degraded versions of a more informative data set: the Wall Street journal section of the Penn treebank (PTB, (Marcus et al., 1993)) which is often stripped of its richer set of annotations (i.e. traces and functional labels are removed), while, for reasons of efficiency and availability, projective dependency trees are often given preference over richer graph structures (Nivre and Nilsson, 2005; Sagae
and Tsujii, 2008).",1 Introduction,[0],[0]
"This led to the emergence of surface syntax-based parsers (Charniak, 2000; Nivre, 2003; Petrov et al., 2006) whose output cannot by themselves be used to extract full-fledged predicateargument structures.",1 Introduction,[0],[0]
"For example, control verb constructions, it-cleft structures, argument sharing in ellipsis coordination, etc. are among the phenomena requiring a graph to be properly accounted for.",1 Introduction,[0],[0]
The dichotomy between what can usually be parsed with high accuracy and what lies in the deeper syntactic description has initiated a line of research devoted to closing the gap between surface syntax and richer structures.,1 Introduction,[0],[0]
"For most of the previous decade, the term deep syntax was used for rich parsing models built upon enriched versions of a constituency treebank, either with added HPSG or LFG annotation or CCG (almost) full rewrites (Miyao and Tsujii, 2005; Cahill et al., 2004; Hockenmaier, 2003).",1 Introduction,[0],[0]
"Its use now spreads by misnomer to models that provide more abstract structures, capable of generalizing classical functional labels to more semantic (in a logical view) arguments, potentially capable of neutralizing diathesis distinctions and of providing accurate predicate-argument structures.",1 Introduction,[0],[0]
"Although the building of syntax-to-semantic interface seems inextricably linked to an efficient parsing stage, inspirational works on semantic role labelling (Toutanova et al., 2005) and more recently on broad coverage semantic parsing (Du et al., 2014) that provide stateof-the-art results without relying on surface syntax, lead us to question the usefulness of syntactic parses for predicate-argument structure parsing.
",1 Introduction,[0],[0]
"In this study, we investigate the impact of syntactic features on a transition-based graph parser by testing on two treebanks.",1 Introduction,[0],[0]
"We take advantage of the recent release for the SemEval 2014 shared task on semantic dependency parsing, by Oepen et
64
al. (2014) of two semantic-based treebanks, derived from two HPSG resources, the DeepBank (DM, (Flickinger et al., 2012)) and the Enju’s predicate argument structure (PAS, (Miyao and Tsujii, 2005)), to investigate the impact of syntactic features on a transition-based graph parser.",1 Introduction,[0],[0]
Our results show that surface syntactic features significantly improve the parsing of predicate-argument structures.,1 Introduction,[0],[0]
"More specifically, we show that adding syntactic context improves the recognition of long distance dependencies and elliptical constructions.",1 Introduction,[0],[0]
"We finally discuss the usefulness of our approach, when applied on a second-order model based on dual decomposition (Martins and Almeida, 2014), showing that our use of syntactic features enhances this model accuracy and provides state-of-the-art performance.",1 Introduction,[0],[0]
"DeepBank Corpus Semantic dependency graphs in the DM Corpus are the result of a two-step simplification of the underspecified logical-form meaning representations, based on Minimal Recursion Semantic (MRS, (Copestake et al., 1995; Copestake et al., 2005)), derived from the manually annotated DeepBank treebank (Flickinger et al., 2012).",2 Deep Syntax and Underspecified Semantic Corpora,[0],[0]
"First, Oepen and Lønning (2006) define a conversion from original MRS formulae to variable-free Elementary Dependency Structures (EDS), which (a) maps each predication in the MRS logical-form meaning representation to a node in a dependency graph and (b) transforms argument relations represented by shared logical variables into directed dependency links between graph nodes.",2 Deep Syntax and Underspecified Semantic Corpora,[0],[0]
"Then, in a second conversion step, the EDS graphs are further reduced into strict bi-lexical form, i.e. a set of directed, binary dependency relations holding exclusively between lexical units (Ivanova et al., 2012).",2 Deep Syntax and Underspecified Semantic Corpora,[0],[0]
"Even though both conversion steps are, by design, lossy, DM semantic dependency graphs present a true subset of the information encoded in the full, original MRS data set.
",2 Deep Syntax and Underspecified Semantic Corpora,[0],[0]
"Predicate-Argument Structure Corpus Enju Predicate-Argument Structures (PAS Corpus) are derived from the automatic HPSG-style annotation of the Penn Treebank (Miyao and Tsujii, 2004) that was primarily used for the development of the Enju parsing system (Miyao and Tsujii, 2005).",2 Deep Syntax and Underspecified Semantic Corpora,[0],[0]
"The
PAS data set is an extraction of predicate-argument structures from the Enju HPSG treebank and contains word-to-word semantic dependencies.",2 Deep Syntax and Underspecified Semantic Corpora,[0],[0]
"Each dependency type is made of two elements: a coarse part-of-speech of the head predicate dependent (e.g. verb and adjective), and the argument (e.g. ARG1 and ARG2).
",2 Deep Syntax and Underspecified Semantic Corpora,[0],[0]
"Although both are derived from HSPG resources (a hand-crafted grammar for DM, a treebank-based one for PAS), they differ in their core linguistic choices (functional heads vs lexical heads, coordination scheme, etc.) leading to different views of the predicate argument structure for the same sentence (Ivanova et al., 2012).",2 Deep Syntax and Underspecified Semantic Corpora,[0],[0]
"Thus, even though both corpora may appear to contain a similar number of dependency labels, as shown in Table 1, their annotation schemes depict a deeply divergent linguistic reality exposed by two very different distributions.",2 Deep Syntax and Underspecified Semantic Corpora,[0],[0]
"In DM, 9 labels account for almost 95% of all dependencies whereas a label set twice as large covers the same percentage for PAS, as shown in Table 2.",2 Deep Syntax and Underspecified Semantic Corpora,[0],[0]
"Furthermore, semantically empty elements are widespread in the DeepBank (around 21.5%), compared to a low rate of 4.3% in PAS.",2 Deep Syntax and Underspecified Semantic Corpora,[0],[0]
"In other words, the latter is somewhat more dense and consequently more syntactic.",2 Deep Syntax and Underspecified Semantic Corpora,[0],[0]
"This is due to the fact that PAS integrates markers for infinitives, auxiliaries, and most punctuation marks into its graphs, whereas DM considers them as semantically void.",2 Deep Syntax and Underspecified Semantic Corpora,[0],[0]
DM corpus is clearly heading toward more semantic analysis while the PAS corpus aims at providing a more abstract deep syntax analysis than regular surface syntax trees.,2 Deep Syntax and Underspecified Semantic Corpora,[0],[0]
Both treebanks are used in their bilexical dependency formats.,2 Deep Syntax and Underspecified Semantic Corpora,[0],[0]
"Shift-reduce transition-based parsers essentially rely on configurations formed of a stack and a buffer, with stack transitions used to move from a configuration to the next one, until reaching a final configuration.",3 Transition-based Graphs Parsing,[0],[0]
"Following Kübler et al. (2009), we define a configuration by c = (σ, β,A) where σ denotes a stack of words wi, β a buffer of words, and A a set of dependency arcs of the form (wi, r, wj), with wi the head, wj the dependent, and r a label in some set R. As shown in Figure 1, besides the usual shift and reduce transitions (lR & rR) of the arc-standard strategy, we introduced the new left and right attach (lA & rA) transitions for adding new dependencies (while keeping the dependent on the stack) and a pop0 transition to remove a word from the stack after attachment of its dependents.",3 Transition-based Graphs Parsing,[0],[0]
"All the transitions that add an edge must also satisfy the condition that the newly created edge does not introduce a cycle or
multiple edges between the same pair of nodes.",3 Transition-based Graphs Parsing,[0],[0]
"It is to be noted that the pop0 action may also be used to remove words with no heads.
",3 Transition-based Graphs Parsing,[0],[0]
"We base our work on the the DAG parser of Sagae and Tsujii (2008) (henceforth S&T) which we extended with the set of actions displayed above (Figure 1) to cope with partially connected planar graphs, and we gave it the ability to take advantage of an extended set of features.",3 Transition-based Graphs Parsing,[0],[0]
"Finally, for efficiency reasons (memory consumption and speed), we replaced the original Maxent model with an averaged structured perceptron (Freund and Schapire, 1999; Collins, 2002).",3 Transition-based Graphs Parsing,[0],[0]
We define Wordβi (resp.,4.1 Baseline Features,[0],[0]
Lemmaβi and POSβi) as the word (resp.,4.1 Baseline Features,[0],[0]
lemma and part-of-speech) at position i in the queue.,4.1 Baseline Features,[0],[0]
"The same goes for σi, which is the position i in the stack.",4.1 Baseline Features,[0],[0]
"Let di,j be the distance between Wordσi and Wordσj .",4.1 Baseline Features,[0],[0]
"We also define d′i,j , the distance between Wordβi and Wordσj .",4.1 Baseline Features,[0],[0]
"In addition, we define leftPOSσi (resp. leftLabelσi)",4.1 Baseline Features,[0],[0]
the part-of-speech (resp.,4.1 Baseline Features,[0],[0]
"the label if any) of the word immediately to the left of σi, and the same goes for rightPOSσi (resp.",4.1 Baseline Features,[0],[0]
rightLabelσi).,4.1 Baseline Features,[0],[0]
"Finally, a is the previous action predicted by the parser.",4.1 Baseline Features,[0],[0]
Table 3 lists our baseline features.,4.1 Baseline Features,[0],[0]
"Xσi, σj , σk means that we use Xσi, Xσj , Xσk as unigram features as well as bigram and trigram features.",4.1 Baseline Features,[0],[0]
"We combined the previous features with different types of syntactic features (constituents and dependencies), our intuition being that syntax and semantic are interdependent, and that syntactic features should therefore help predicate-argument parsing.",4.2 Syntactic Features,[0],[0]
"In fact, we considered that the low density of syntactic information (compared to regular dependency treebanks) would be counterbalanced by
adding more context.",4.2 Syntactic Features,[0],[0]
"We considered the following pieces of information in particular.
",4.2 Syntactic Features,[0],[0]
Constituent Tree Fragments These consist of fragments of syntactic trees predicted by the Petrov et al. (2006) parser in a 10-way jackknife setting.,4.2 Syntactic Features,[0],[0]
"They can be used as enhanced POS or as features.
",4.2 Syntactic Features,[0],[0]
Spinal Elementary Trees,4.2 Syntactic Features,[0],[0]
"A full set of parses was reconstructed from the tree fragments using a slightly tweaked version of the CONLL 2009 shared task processing tools (Hajič et al., 2009).",4.2 Syntactic Features,[0],[0]
"We then extracted a spine grammar (Seddah, 2010) using the head percolation table of the Bikel (2002) parser, slightly modified to avoid certain determiners being marked as heads in certain configurations.",4.2 Syntactic Features,[0],[0]
"The resulting spines were assigned in a deterministic way (red part in Figure 2).
",4.2 Syntactic Features,[0],[0]
"Predicted MATE Dependency Labels These consist of the dependency labels predicted by the MATE parser (Bohnet, 2010), trained on a Stanford surface dependency version of the Penn Treebank.",4.2 Syntactic Features,[0],[0]
We combined the labels with a distance δ =,4.2 Syntactic Features,[0],[0]
t − h where t is the token position and h the head position (brown labels and δ in Figure 2).,4.2 Syntactic Features,[0],[0]
"In addition, we expanded these features with the part-of-speech of the head of a given token (HPOS).",4.2 Syntactic Features,[0],[0]
"The idea is to evaluate the informativeness of more abstract syntactic features since a <LABEL,HPOS> pair can be seen as generalizing many constituent subtrees.
",4.2 Syntactic Features,[0],[0]
Constituent Head Paths.,4.2 Syntactic Features,[0],[0]
"Inspired by Björkelund et al. (2013), we used MATE dependencies to extract the shortest path between a token and its lexical head and included the path length w (in terms of traversed nodes) as a feature (blue part in Figure 2).",4.2 Syntactic Features,[0],[0]
The global idea is to use the phrase-based features to provide different kinds of syntactic context and the dependency-based features to provide generalisations over the functional label governing a token.,4.2 Syntactic Features,[0],[0]
"The spines are seen as deterministic supertags, bringing a vertical context.
",4.2 Syntactic Features,[0],[0]
"We report, in Table 4, the counts for each syntactic feature on each set.",4.2 Syntactic Features,[0],[0]
Experimental Setup Both DM and PAS treebanks consist of texts from the PTB and which were either automatically derived from the original annotations or annotated with a hand-crafted grammar (see above).,5 Experiments,[0],[0]
"We use them in their bi-lexical dependency format, aligned at the token level as provided by Oepen et al. (2014)1.",5 Experiments,[0],[0]
"The following split is used: sections 00-19 for training, 20 for the dev. set and 21 for test2.",5 Experiments,[0],[0]
"All predicted parses are evaluated against the gold standard with labeled precision, recall and f-measure metrics.
",5 Experiments,[0],[0]
"Results Our experiments are based on the evaluation of the combinations of the 4 main types of syntactic features described in section 4: tree fragments (BKY), predicted mate dependencies (BN) and their extension with POS heads (BN(HPOS)), spinal elementary trees (SPINES) and head paths (PATHS).
",5 Experiments,[0],[0]
The results are shown in Tables 5 and 6.,5 Experiments,[0],[0]
All improvements from the baseline are significant with a p-value p < 0.05.,5 Experiments,[0],[0]
"There was no significant difference of the same p value between our two best mod-
1This alignment entailed the removal of all unparsed sentences.
",5 Experiments,[0],[0]
"2We used the same unusual split as in (Oepen et al., 2014) to be able to conduct meaningful comparisons with others.
",5 Experiments,[0],[0]
els for each of the treebanks.,5 Experiments,[0],[0]
"3
As expected from the rapid overview of our datasets exposed earlier in section 2, the use of each single feature alone increases the performance over the baseline by 0.5 points for the BN feature in DM to 1.44 for PATHS, and by 1.10 for the SPINES to 1.85 for the PATHS features in PAS.",5 Experiments,[0],[0]
"Looking at the conjunction of two classes in the DM table, it seems that dependency-based features benefit from the extra context brought by constituents features, reaching an increase of 2.21 points for BKY+BN(HPOS).",5 Experiments,[0],[0]
"Interestingly, the maximum gain is brought by the addition of topologically different phrase-based features such as SPINES (+2.80, inherently vertical) or BKY (+2.76, often wider) to the previous best.",5 Experiments,[0],[0]
"Regarding PAS, similar trends can be observed, although the gains are more distributed.",5 Experiments,[0],[0]
"As opposed to DM where the conjunction of more features led to inferior results, here using a four-features class provides the second best improvement (ALL(HPOS) = BKY+BN(HPOS)+SPINES+PATHS), +2.82) while removing the SPINES slightly increases the score (+2.92).",5 Experiments,[0],[0]
"In fact, adding too many features to the model slightly degrades our scores, at least with regard to DM which has a larger label set than PAS.
Results show that syntactic information improves our parser performances.",5 Experiments,[0],[0]
"As each feature represents one unique piece of information, they benefit from being combined in order to provide more structural information.",5 Experiments,[0],[0]
"Following Mcdonald and Nivre (2007), we conducted an error analysis based on the two best models and the baseline for each corpus.",6 Results Analysis,[0],[0]
"As shown in section 5, syntactic features greatly improve semantic parsing.",6 Results Analysis,[0],[0]
"However, it is interesting to explore more precisely what kind of syntactic information boosts or penalizes our predictions.",6 Results Analysis,[0],[0]
"We consider, among other factors, the impact in terms of distance between the head and the dependent (edge length) and the labels.",6 Results Analysis,[0],[0]
"We also explore several linguistic phenomena well known to be difficult to recover.
",6 Results Analysis,[0],[0]
"3We tested the statistical significance between our best models and the baseline with the paired bootstrap test (BergKirkpatrick et al., 2012).",6 Results Analysis,[0],[0]
"In Figures 3(a) and 4(a), we detail the scores for the five most frequent labels.
",6.1 Breakdown by Labels,[0],[0]
"As observed in the charts, the scores are higher for the most frequent labels on both corpora, especially when dealing with verbal arguments.",6.1 Breakdown by Labels,[0],[0]
"There are also two interesting cases for DM: the predictions of _and_c and ARG3 edges show an improvement by at least 5 points (Figures 3(b) & 4(b)), showing that the recovery of coordination structures and the disambiguation of less frequent or more distant arguments is achieved by adding non-local features.",6.1 Breakdown by Labels,[0],[0]
Longer sentences are notoriously difficult to parse for most parsing models.,6.2 Length Factor,[0],[0]
"Figures 3(c) and 4(c) show the F1-measure of our models with respect to sentence length (in bins of size 10: 1-10, 11-20, etc.)",6.2 Length Factor,[0],[0]
"for the DM and PAS corpora.
",6.2 Length Factor,[0],[0]
It is worth noting that we greatly improve the scores for longer sentences.,6.2 Length Factor,[0],[0]
"The use of paths and of the output of a graph-based parser (Bohnet, 2010) favors the capture of complex dependencies and enhances the learning of these constructions for our local transition-based parser.",6.2 Length Factor,[0],[0]
"However, we also observe that the features are not able to completely stop the loss of F1-score for longer sentences.",6.2 Length Factor,[0],[0]
"The slopes of the curves in the different charts show the same trend: the longer the sentence, the lower the score.",6.2 Length Factor,[0],[0]
"We now center our analysis on long-distance dependencies (LDDs), by focusing our attention on edges length, i.e. the distance between two words linked by an edge.",6.3 Linguistic Factors,[0],[0]
"We will then concentrate on subject ellipsis, in a treatment of LDDs more similar to the linguistic definition of Cahill et al. (2004).
",6.3 Linguistic Factors,[0],[0]
Long-distance Dependencies (LDDs),6.3 Linguistic Factors,[0],[0]
"For many systems, LDDs are difficult to recover because they are generally under-represented in the training corpus and the constructions involved in LDDs often require deep linguistic knowledge to be recovered.",6.3 Linguistic Factors,[0],[0]
"In
Figure 7, we report the distribution of long-distance dependencies by bins of size 5 up to 40.",6.3 Linguistic Factors,[0],[0]
They only account for 15% of all the dependencies in both corpora.,6.3 Linguistic Factors,[0],[0]
The longest dependencies consist of the first and second arguments of the verb as well as coordination links.,6.3 Linguistic Factors,[0],[0]
"In the case of elided coordination structures, we have long-distance dependencies when two coordinated verbs share the same first or second argument, which explains the distribution of lengths.
BINS 5-10 11-15 16-20 21-25 26-40
DM 2907 734 329 141 92 PAS 3705 1007 408 175 127
Table 7: Number of LDDs edges (dev. set).
",6.3 Linguistic Factors,[0],[0]
"As outlined in Figures 3(d) and 4(d), we can see that without structural information such as spines, surfacic dependencies or paths, the longest dependencies have low F1-scores.",6.3 Linguistic Factors,[0],[0]
"When using these features, our models tend to perform better, with a gain of up to 25 points for high-dependency lengths (bins between 16-20 and 21-25).
",6.3 Linguistic Factors,[0],[0]
"In Table 8, we show the global improvement when considering edge lengths between 5 and 40.",6.3 Linguistic Factors,[0],[0]
"For both corpora, the improvement is the same (around 9 points), showing that structural information is the key to better predictions.",6.3 Linguistic Factors,[0],[0]
"Looking into this improvement more closely, we found that PATHS combined with BN tend to be crucial, whereas SPINES
may sometimes penalize the models.",6.3 Linguistic Factors,[0],[0]
"Even though, BN+SPINES+PATHS is the best model for DM, a spine is only a partial projection which lacks attachment information.",6.3 Linguistic Factors,[0],[0]
"Spines alone only therefore provide a local context and are unable to cope well with LDDs.
",6.3 Linguistic Factors,[0],[0]
Coordination Structures We now focus on structures with subject ellipsis.,6.3 Linguistic Factors,[0],[0]
"We extracted them by using a simple graph pattern, i.e. two verbs with a shared ARG1 and a coordination dependency.
",6.3 Linguistic Factors,[0],[0]
Our best models’ scores are displayed in Tables 9.,6.3 Linguistic Factors,[0],[0]
"Once again, our models improve the F1 score, but not in the same proportion.",6.3 Linguistic Factors,[0],[0]
DM considers the conjunction as a semantically empty word and attaches an edge _and_c between the two verbs to mark the coordination.,6.3 Linguistic Factors,[0],[0]
"Consequently this edge is more difficult to predict, because it is less informative, our baseline model relying on tokens, lemmas and POS.
",6.3 Linguistic Factors,[0],[0]
We note that the difference in the number of evaluated dependencies in both corpora comes from an annotation scheme divergence between PAS and DM regarding subject ellipsis.,6.3 Linguistic Factors,[0],[0]
"DM opts for coordinate structures with a chain of dependencies rooted at the first conjunct, the coordinating conjunctions being therefore semantically empty.",6.3 Linguistic Factors,[0],[0]
"In PAS, the final coordinating conjunction and each coordinating conjunction is a two-place predicate, taking left and right conjuncts as its arguments.
",6.3 Linguistic Factors,[0],[0]
"The gain of 6.30 points for DM (Table 9(a), resp.",6.3 Linguistic Factors,[0],[0]
"+3 for PAS) indicates that, when an annotation scheme is designed to have many semantically empty words, using syntactic information tends to enhance the parser accuracy.",6.3 Linguistic Factors,[0],[0]
"This gives a clear insight into what type of information is required to
parse semantic graphs: the greater the distance between the head and the dependent, the larger the context needed to disambiguate the attachments.",6.3 Linguistic Factors,[0],[0]
"PAS DM
Overlap +2.87 +2.67",6.4 Ruling out the Structural Factor Bias,[0],[0]
"Rest +2.70 +2.74 It may argued that the improvement we noticed could stem from a potentially strong overlap between surface trees and predicate-argument structures, both in terms of edges and labels.",6.4 Ruling out the Structural Factor Bias,[0],[0]
"In fact, the conversion from surfacic parses into predicate-argument structures requires a large amount of edges relabeling (for instance, when nsubj is relabeled to ARG1).",6.4 Ruling out the Structural Factor Bias,[0],[0]
We tested this hypothesis by computing the number of common edges between MATE predictions and DM and PAS.,6.4 Ruling out the Structural Factor Bias,[0],[0]
The overlap corresponds to about 22% of all edges in PAS and 27% in DM.,6.4 Ruling out the Structural Factor Bias,[0],[0]
"Although important, it does not represent the majority of dependencies in our corpora, because most of edges are not present in surface predictions.",6.4 Ruling out the Structural Factor Bias,[0],[0]
We evaluated the improvement of the overlap as well as for the rest.,6.4 Ruling out the Structural Factor Bias,[0],[0]
Results show that our best models perform roughly the same on both sets.,6.4 Ruling out the Structural Factor Bias,[0],[0]
"Interestingly, as opposed to PAS’s model, DM’s model performs better on the non-overlap part.",6.4 Ruling out the Structural Factor Bias,[0],[0]
"This suggests that the use of PTB-based features is somehow not optimal when applied on a none PTB-based treebank, such as DM which comes from a handcrafted grammar.",6.4 Ruling out the Structural Factor Bias,[0],[0]
"Our point was to prove that providing more syntactic context, in the form of phrased-based tree fragments and surface dependencies, helps transition-
based parsers to predict predicate-argument structures, especially for LDDs.",7 Discussion,[0],[0]
"Yet, compared to stateof-the-art systems, our results built on the S&T parser score lower than the top performers (Table 10).",7 Discussion,[0],[0]
"However, we are currently extending a more advanced lattice-aware transition-based parser (DSR) with beams (Villemonte De La Clergerie, 2013) that takes advantage of cutting-edge techniques (dynamic programming, averaged perceptron with early updates, etc. following (Goldberg et al., 2013; Huang et al., 2012)) 4, which proves effective by reaching the state-of-the-art on PAS, outperforming Thomson et al. (2014) and second to the model of Martins and Almeida (2014).",7 Discussion,[0],[0]
"5
The point here is that using the same syntactic features as our base system exhibits the same improvement over a now much stronger baseline.",7 Discussion,[0],[0]
"We can conjecture that the ambiguities added by the relative scarcity of the deep annotations is efficiently handled by a more complete exploration of the search space, made possible by beam optimization.
",7 Discussion,[0],[0]
"We can also wonder whether the lower improvement brought to DM parsing by the PTB-based syntactic features does not come from the fact that the DM corpus and the PTB have divergent annotation
4It uses a different set of transitions, notably pop actions instead of left and right reduce, and a swap that allow limited amount of non-planarity.",7 Discussion,[0],[0]
"Such a set raises issues with beams (several paths leading to a same item, final items reached with paths of various lengths, . . . ), overcome by adding a ’noop’ action only applied on final items to balance path lengths.
",7 Discussion,[0],[0]
"5Leaving aside the multiple (19) ensemble models of Du et al. (2014), because of the impracticability of the approach.
schemes.",7 Discussion,[0],[0]
"In that aspect, PTB syntactic features may add some noise to the learning process, because they give more weight to conflicting decisions that led to correct structures in one but not in the other scheme.
",7 Discussion,[0],[0]
"By using features which, to a certain extent, (i) extend the domain of locality available at a given node and (ii) generalize some structural and functional contexts otherwise unavailable, we tried to overcome the main issue of transition-based parsers: they remain local in the sense that they lack a global view of the whole sentence.
",7 Discussion,[0],[0]
"Impact Beyond Transition-based Parser Of course, it can be argued that improving over a somewhat weak baseline is of limited interest.",7 Discussion,[0],[0]
Our point was to investigate how the direct parsing of relatively sparse graph structures would benefit from the inclusion of more context via the use of topologically different syntactic pieces of information.,7 Discussion,[0],[0]
"However in that work, we mostly focused on transition based-parsing, which raises the question of the impact of our feature-set on a much more powerful and state-of-the-art model such as the TURBOSEMANTICPARSER developed by Martins and Almeida (2014).
",7 Discussion,[0],[0]
"To this end, we extended the T.PARSER so that it could cope with our syntactic features and studied the interaction of our best feature set with second order features (i.e. grand-parents and co-parents).",7 Discussion,[0],[0]
Results in Table 11 show that the gain brought by adding syntactic features (+2.14 on DM over the baseline) is higher than the sole use of second order ones (+1.09).,7 Discussion,[0],[0]
"Furthermore, the gain brought by
the second-order features is reduced by half when used jointly with our feature set (+1.09 vs +0.57 with them).",7 Discussion,[0],[0]
"However, although we could assess that the need of second order models is thus alleviated, the conjunction of both types of features still improves the parser performance by an overall gain of 1.62 points on DM (1.18 on PAS), suggesting that both feature sets contribute to different types of “structures”.",7 Discussion,[0],[0]
"In short, the use of syntactic features is also relevant with a strong baseline, as they provide a global view to graph-based models, establishing a new state-of-the-art on these corpora.
",7 Discussion,[0],[0]
"Baseline = arc-factored + siblings
Related Work A growing interest for semantic parsing has emerged over the past few years, with the availability of resources such as PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004) built on top of the Penn Treebank.",7 Discussion,[0],[0]
"The shallow semantic annotations they provide were among the targets of successful shared tasks on semantic role labeling (Surdeanu et al., 2008; Carreras and Màrquez, 2005).",7 Discussion,[0],[0]
"Actually, the conjoint use of such annotations with surface syntax dependencies bears some resemblance with predicate-argument structure parsing like we presented here.",7 Discussion,[0],[0]
"However, they diverge in that Propbank/Nombank annotations
do not form connected graphs by themselves, as they only cover argument identification and nominal predicates.",7 Discussion,[0],[0]
"The range of phenomena they describe is also limited, compared to a full predicate-argument analysis as provided by DM and PAS (Oepen et al., 2014).",7 Discussion,[0],[0]
"More importantly, as pointed out by Yi et al. (2007), being verb-specific, Propbank’s roles do not generalize well beyond the ARG0 argument (i.e. the subject/agent role) leading to inconsistencies.",7 Discussion,[0],[0]
"However, the advent of such semantic-based resources have ignited a fruitful line of research, of which the use of heterogeneous sources of information to boost parsing performance has been investigated over the past decade (Chen and Rambow, 2003; Tsuruoka et al., 2004) with a strong regain of interest raised by the work of Moschitti et al. (2008), Henderson et al. (2008), Sagae (2009).",7 Discussion,[0],[0]
We described the use and combination of several kinds of syntactic features to improve predicateargument parsing.,8 Conclusion,[0],[0]
"To do so, we tested our approach of injecting surface-syntax features by thoroughly evaluating their impact on one transitionbased graph parser, then validating on two more efficient parsers, over two deep syntax and semantic treebanks.",8 Conclusion,[0],[0]
"Results of the syntax-enhanced semantic parsers exhibit a constant improvement, regardless of the annotation scheme and the parser used.",8 Conclusion,[0],[0]
The question is now to establish whether will this be verified in other semantic data sets?,8 Conclusion,[0],[0]
"From the parsing of deep syntax treebanks a la Meaning Text Theory (Ballesteros et al., 2014), to Framenet semantic parsing (Das et al., 2014) or data-driven approaches closer to ours (Flanigan et al., 2014), it is difficult to know which models will predominate from this bubbling field and what kind of semantic data sets will benefit the most from syntax.",8 Conclusion,[0],[0]
We would like to thank Kenji Sagae and André F. T. Martins for making their parsers available and for kindly answering our questions.,Acknowledgements,[0],[0]
We also thank our anonymous reviewers for their comments.,Acknowledgements,[0],[0]
"This work was partly funded by the Program ""Investissements d’avenir"" managed by Agence Nationale de la Recherche ANR-10-LABX-0083 (Labex EFL).",Acknowledgements,[0],[0]
Parsing full-fledged predicate-argument structures in a deep syntax framework requires graphs to be predicted.,abstractText,[0],[0]
"Using the DeepBank (Flickinger et al., 2012) and the PredicateArgument Structure treebank (Miyao and Tsujii, 2005) as a test field, we show how transition-based parsers, extended to handle connected graphs, benefit from the use of topologically different syntactic features such as dependencies, tree fragments, spines or syntactic paths, bringing a much needed context to the parsing models, improving notably over long distance dependencies and elided coordinate structures.",abstractText,[0],[0]
"By confirming this positive impact on an accurate 2nd-order graphbased parser (Martins and Almeida, 2014), we establish a new state-of-the-art on these data sets.",abstractText,[0],[0]
Because Syntax Does Matter: Improving Predicate-Argument Structures Parsing with Syntactic Features,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 386–396 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Human reasoning is lazy and biased but it perfectly serves its purpose in the argumentative context (Mercier and Sperber, 2017).",1 Introduction,[0],[0]
"When challenged by genuine back-and-forth argumentation, humans do better in both generating and evaluating arguments (Mercier and Sperber, 2011).",1 Introduction,[0],[0]
"The dialogical perspective on argumentation has been reflected in argumentation theory prominently by the pragma-dialectic model of argumentation (van Eemeren and Grootendorst, 1992).",1 Introduction,[0],[0]
"Not only sketches this theory an ideal normative model of argumentation but also distinguishes the wrong argumentative moves, fallacies (van Eemeren and Grootendorst, 1987).",1 Introduction,[0],[0]
"Among the plethora of prototypical fallacies, notwithstanding the controversy of most taxonomies (Boudry et al., 2015), ad hominem argument is perhaps the most famous one.",1 Introduction,[0],[0]
"Arguing against the person is considered faulty, yet is prevalent in online and offline discourse.1
1According to ‘Godwin’s law’ known from the internet pop-culture (https://en.wikipedia.org/wiki/
Although the ad hominem fallacy has been known since Aristotle, surprisingly there are very few empirical works investigating its properties.",1 Introduction,[0],[0]
"While Sahlane (2012) analyzed ad hominem and other fallacies in several hundred newspaper editorials, others usually only rely on few examples, as observed by de Wijze (2002).",1 Introduction,[0],[0]
"As Macagno (2013) concludes, ad hominem arguments should be considered as multifaceted and complex strategies, involving not a simple argument, but several combined tactics.",1 Introduction,[0],[0]
"However, such research, to the best of our knowledge, does not exist.",1 Introduction,[0],[0]
"Very little is known not only about the feasibility of ad hominem theories in practical applications (the NLP perspective) but also about the dynamics and triggers of ad hominem (the theoretical counterpart).
",1 Introduction,[0],[0]
"This paper investigates the research gap at three levels of increasing discourse complexity: ad hominem in isolation, direct ad hominem without dialogical exchange, and ad hominem in large inter-personal discourse context.",1 Introduction,[0],[0]
We asked the following research questions.,1 Introduction,[0],[0]
"First, what qualitative and quantative properties do ad hominem arguments have in Web debates and how does that reflect the common theoretical view (RQ1)?",1 Introduction,[0],[0]
"Second, how much of the debate context do we need for recognizing ad hominem by humans and machine learning systems (RQ2)?",1 Introduction,[0],[0]
"And finally, what are the actual triggers of ad hominem arguments and can we predict whether the discussion is going to end up with one (RQ3)?
",1 Introduction,[0],[0]
"We tackle these questions by leveraging Webbased argumentation data (Change my View on Reddit), performing several large-scale annotation studies, and creating a new dataset.",1 Introduction,[0],[0]
"We experiment with various neural architectures and ex-
Godwin’s_law), if a discussion goes on long enough, sooner or later someone will compare someone or something to Adolf Hitler.
386
trapolate the trained models to validate our working hypotheses.",1 Introduction,[0],[0]
"Furthermore, we propose a list of potential linguistic and rhetorical triggers of ad hominem based on interpreting parameters of trained neural models.2",1 Introduction,[0],[0]
This article thus presents the first NLP work on multi-faceted ad hominem fallacies in genuine dialogical argumentation.,1 Introduction,[0],[0]
We also release the data and the source code to the research community.3,1 Introduction,[0],[0]
"The prevalent view on argumentation emphasizes its pragmatic goals, such as persuasion and groupbased deliberation (van Eemeren et al., 2014), although numerous works have dealt with argument as product, that is, treating a single argument and its properties in isolation (Toulmin, 1958; Habernal and Gurevych, 2017).",2 Theoretical background and related work,[0],[0]
"Yet the social role of argumentation and its alleged responsibility for the very skill of human reasoning explained from the evolutionary perspective (Mercier and Sperber, 2017) provide convincing reasons to treat argumentation as an inherently dialogical tool.
",2 Theoretical background and related work,[0],[0]
"The observation that some arguments are in fact ‘deceptions in disguise’ was made already by Aristotle (Aristotle and Kennedy (translator), 1991), for which the term fallacy has been adopted.",2 Theoretical background and related work,[0],[0]
"Leaving the controversial typology of fallacies aside (Hamblin, 1970; van Eemeren and Grootendorst, 1987; Boudry et al., 2015), the ad hominem argument is addressed in most theories.",2 Theoretical background and related work,[0],[0]
"Ad hominem argumentation relies on the strategy of attacking the opponent and some feature of the opponent’s character instead of the counterarguments (Tindale, 2007).",2 Theoretical background and related work,[0],[0]
"With few exceptions, the following five sub-types of ad hominem are prevalent in the literature: abusive ad hominem (a pure attack on the character of the opponent), tu quoque ad hominem (essentially analogous to the “He did it first” defense of a three-year-old in a sandbox), circumstantial ad hominem (the “practice what you preach” attack and accusation of hypocrisy), bias ad hominem (the attacked opponent has a hidden agenda), and guilt by association (associating the opponent with somebody with a low credibility) (Schiappa and Nordin,
2An attempt to address the plea for thinking about problems, cognitive science, and the details of human language (Manning, 2015).
",2 Theoretical background and related work,[0],[0]
"3https://github.com/UKPLab/ naacl2018-before-name-calling-habernal-et-al
2013; Macagno, 2013; Walton, 2007; Hansen, 2017; Woods, 2008).",2 Theoretical background and related work,[0],[0]
"We omit examples here as these provided in theoretical works or textbooks are usually artificial, as already criticized by (de Wijze, 2002) or (Boudry et al., 2015).
",2 Theoretical background and related work,[0],[0]
"The topic of fallacies, which might be considered as sub-topic of argumentation quality, has recently been investigated also in the NLP field.",2 Theoretical background and related work,[0],[0]
"Existing works are, however, limited to the monological view (Wachsmuth et al., 2017; Habernal and Gurevych, 2016b,a; Stab and Gurevych, 2017) or they focus primarily on learning fallacy recognition by humans (Habernal et al., 2017, 2018a).",2 Theoretical background and related work,[0],[0]
Another related NLP sub-field includes abusive language and personal attacks in general.,2 Theoretical background and related work,[0],[0]
Wulczyn et al. (2017) investigated whether or not Wikipedia talk page comments are personal attacks and annotated 38k instances resulting in a highly skewed distribution (only 0.9% were actual attacks).,2 Theoretical background and related work,[0],[0]
"Regarding the participants’ perspective, Jain et al. (2014) examined principal roles in 80 discussions from the Wikipedia:",2 Theoretical background and related work,[0],[0]
"Article for Deletion pages (focusing on stubbornness or ignoredness, among others) and found several typical roles, including ‘rebels’, ‘voices’, or ‘idiots’.",2 Theoretical background and related work,[0],[0]
"In contrast to our data under investigation (Change My View debates), Wikipedia talk pages do not adhere to strict argumentation rules with manual moderation and have a different pragmatic purpose.
",2 Theoretical background and related work,[0],[0]
Reddit as a source platform has also been used in other relevant works.,2 Theoretical background and related work,[0],[0]
Saleem et al. (2016) detected hateful speech on Reddit by exploiting particular sub-communities to automatically obtain training data.,2 Theoretical background and related work,[0],[0]
Wang et al. (2016) experimented with an unsupervised neural model to cluster social roles on sub-reddits dedicated to computer games.,2 Theoretical background and related work,[0],[0]
Zhang et al. (2017) proposed a set of nine comment-level dialogue act categories and annotated 9k threads with 100k comments and built a CRF classifier for dialogue act labeling.,2 Theoretical background and related work,[0],[0]
"Unlike these works which were not related to argumentation, Tan et al. (2016) examined persuasion strategies on Change My View using word overlap features.",2 Theoretical background and related work,[0],[0]
"In contrast to our work, they focused solely on the successful strategies with delta-awarded posts.",2 Theoretical background and related work,[0],[0]
"Using the same dataset, Musi (2017) recently studied concession in argumentation.",2 Theoretical background and related work,[0],[0]
"Change My View (CMV) is an online ‘place to post an opinion you accept [...] in an effort to un-
derstand other perspectives on the issue’, in other words an online platform for ‘good-faith’ argumentation hosted on Reddit.4 A user posts a submission (also called original post(er); OP) and other participants provide arguments to change the OP’s view, forming a typical tree-form Web discussion.",3 Data,[0],[0]
A special feature of CMV is that the OP acknowledges convincing arguments by giving a delta point (∆).,3 Data,[0],[0]
"Unlike the vast majority of internet discussion forums, CMV enforces obeying strict rules (such as no ‘low effort’ posts, or accusing of being unwilling to change view) whose violation results into deleting the comment by moderators.",3 Data,[0],[0]
"These formal requirements of an ideal debate with the notion of violating rules correspond to incorrect moves in critical discussion in the normative pragma-dialectic theory (van Eemeren and Grootendorst, 1987).",3 Data,[0],[0]
"Thus, violating the rule of ‘not being rude or hostile’ is equivalent to committing ad hominem fallacy.",3 Data,[0],[0]
"For our experiments, we scraped, in cooperation with Reddit, the complete CMV including the content of the deleted comments so we could fully reconstruct the fallacious discussions, relying on the rule violation labels provided by the moderators.",3 Data,[0],[0]
"The dataset contains ≈ 2M posts in 32k submissions, forming 780k unique threads.
",3 Data,[0],[0]
We will set up the stage for further experiments by providing several quantitative statistics we performed on the dataset.,3 Data,[0],[0]
Only 0.2% posts in CMV are ad hominem arguments.,3 Data,[0],[0]
This contrasts with a typical online discussion: Coe et al. (2014) found 19.5% of comments under online news articles to be incivil.,3 Data,[0],[0]
"Most threads contain only a single ad hominem argument (3,396 threads; there are 3,866 ad hominem arguments in total in CMV); only 35 threads contain more than three ad hominem arguments.",3 Data,[0],[0]
"In 48.6% of threads containing a single ad hominem, the ad hominem argument is the very last comment.",3 Data,[0],[0]
"This corresponds to the popular belief that if one is out of arguments, they start attacking and the discussion is over.",3 Data,[0],[0]
This trend is also shown in Figure 1 which displays the relative position of the first ad hominem argument in a thread.,3 Data,[0],[0]
"Replying to ad hominem with another ad hominem happens only in 15% of the cases; this speaks for the attempts of CMV participants to keep up with the standards of a rather rational discussion.
",3 Data,[0],[0]
"Regarding ad hominem authors, about 66% of
4https://www.reddit.com/r/changemyview/
them start attacking ‘out of blue’, without any previous interaction in the thread.",3 Data,[0],[0]
"On the other hand, 11% ad hominem authors write at least one ‘normal’ argument in the thread (we found one outlier who committed ad hominem after writing 57 normal arguments in the thread).",3 Data,[0],[0]
"Only in 20% cases, the ad hominem thread is an interplay between the original poster and another participant.",3 Data,[0],[0]
It means that there are usually more people involved in an ad hominem thread.,3 Data,[0],[0]
"Unfortunately, sometimes the OP herself also commits ad hominem (12%).
",3 Data,[0],[0]
We also investigated the relation between the presence of ad hominem arguments and the submission topic.,3 Data,[0],[0]
"While most submissions are accompanied by only one or two ad hominem arguments (75% of submissions), there are also extremes with over 50 ad hominem arguments.",3 Data,[0],[0]
"Manual analysis revealed that these extremes deal with religion, sexuality/gender, U.S. politics (mostly Trump), racism in the U.S., and veganism.",3 Data,[0],[0]
We will elaborate on that later in Section 4.2.,3 Data,[0],[0]
The experimental part is divided into three parts according to the increasing level of discourse complexity.,4 Experiments,[0],[0]
"We first experiment with ad hominem in isolation in section 4.1, then with direct ad hominem replies to original posts without dialogical exchange in section 4.2, and finally with ad hominem in a larger inter-personal discourse context in section 4.3.",4 Experiments,[0],[0]
The first experimental set-up examines ad hominem arguments in Change my view regardless of its dialogical context.,4.1 Ad hominem without context in CMV,[0],[0]
Ad hominem arguments labeled by the CMV moderators come with no warranty.,4.1.1 Data verification,[0],[0]
"To verify their reliability, we conducted the following annotation studies.",4.1.1 Data verification,[0],[0]
"First, we needed to estimate parameters of crowdsourcing and its reliability.",4.1.1 Data verification,[0],[0]
"We sampled 100 random arguments from CMV without context: positive candidates were the reported ad hominem arguments, whereas negative candidates were sampled from comments that either violate other argumentation rules or have a delta label.",4.1.1 Data verification,[0],[0]
"To ensure the maximal content similarity of these two groups, for each positive instance the semantically closest negative instance was selected.5 We then experimented with different numbers of Amazon Mechanical Turk workers and various thresholds of the MACE gold label estimator (Hovy et al., 2013); comparing two groups of six workers each and 0.9 threshold yielded almost perfect interannotator agreement (0.79 Cohen’s κ).",4.1.1 Data verification,[0],[0]
"We then used this setting (six workers, 0.9 MACE threshold) to annotate another 452 random arguments sampled in the same way as above.
",4.1.1 Data verification,[0],[0]
Crowdsourced ‘gold’ labels were then compared to the original CMV labels (balanced binary task: positive instances (ad hominem) and negative instances) reaching accuracy of 0.878.,4.1.1 Data verification,[0],[0]
This means that the ad hominem labels from CMV moderators are quite reliable.,4.1.1 Data verification,[0],[0]
Manual error analysis of disagreements revealed 11 missing ad hominem labels.,4.1.1 Data verification,[0],[0]
These were not spotted by the moderators but were annotated as such by crowd workers.,4.1.1 Data verification,[0],[0]
"We sampled a larger balanced set of positive instances (ad hominem) and negative instances using the same methodology as in section 4.1.1, resulting in 7,242 instances, and casted the task of recognition of ad hominem arguments as a binary supervised task.",4.1.2 Recognizing ad hominem arguments,[0],[0]
"We trained two neural classifiers, namely a 2-stacked bi-directional LSTM network (Graves and Schmidhuber, 2005), and a convolutional network (Kim, 2014), and evaluated them using 10-fold cross validation.",4.1.2 Recognizing ad hominem arguments,[0],[0]
"Throughout the paper we use pre-trained word2vec word embeddings (Mikolov et al., 2013).",4.1.2 Recognizing ad hominem arguments,[0],[0]
"Detailed hyperpa-
5Similarity was computed using a cosine similarity of average embedding vectors multiplied by the argument length difference to minimize length-related artifacts.",4.1.2 Recognizing ad hominem arguments,[0],[0]
"The sample was balanced with roughly 50% positive and 50% negative instances.
",4.1.2 Recognizing ad hominem arguments,[0],[0]
rameters are described in the source codes (link provided in section 1).,4.1.2 Recognizing ad hominem arguments,[0],[0]
"As results in Table 1 show, the task of recognizing ad hominem arguments is feasible and almost achieves the human upper bound performance.",4.1.2 Recognizing ad hominem arguments,[0],[0]
"While binary classification of ad hominem as presented above might be sufficient for the purpose of red-flagging arguments, theories provide us with a much finer granularity (recall the typology in section 2).",4.1.3 Typology of ad hominem,[0],[0]
"To validate whether this typology is empirically relevant, we executed an annotation experiment to classify ad hominem arguments into the provided five types (plus ‘other’ if none applies).",4.1.3 Typology of ad hominem,[0],[0]
We sampled 200 ad hominem arguments from threads in which interlocution happens only between two persons and which end up with ad hominem.,4.1.3 Typology of ad hominem,[0],[0]
The Mechanical Turk workers were shown this last ad hominem argument as well as the preceding one.,4.1.3 Typology of ad hominem,[0],[0]
Each instance was annotated by 16 workers to achieve a stable distribution of labels as suggested by Aroyo and Welty (2015).,4.1.3 Typology of ad hominem,[0],[0]
"While 41% arguments were categorized as abusive, other categories (tu quoque, circumstantial, and guilt by association) were found to be rather ambiguous with very subtle differences.",4.1.3 Typology of ad hominem,[0],[0]
"In particular, we observed a very low percentage agreement on these categories and a label distribution spiked around two or more categories.",4.1.3 Typology of ad hominem,[0],[0]
After a manual inspection we concluded that (1) the theoretical typology does not account for longer ad hominem arguments that mix up different attacks and that (2) there are actual phenomena in ad hominem arguments not covered by theoretical categories.,4.1.3 Typology of ad hominem,[0],[0]
"These observations reflect those of Macagno (2013, p. 399) about ad hominem moves as multifaceted strategies.
",4.1.3 Typology of ad hominem,[0],[0]
We thus propose a list of phenomena typical to ad hominem arguments in CMV based on our empirical study.,4.1.3 Typology of ad hominem,[0],[0]
"For this purpose, we follow up with another annotation experiment on 400 arguments, with seven workers per instance.6 The goal was
6Here we decided on seven workers per item by relying on other span annotation experiments done in a similar setup (Habernal et al., 2018b).
to annotate a text span which made the argument an ad hominem; a single argument could contain several spans.",4.1.3 Typology of ad hominem,[0],[0]
We estimated the gold spans using MACE and performed a manual post-analysis by designing a typology of causes of ad hominem together with their frequency of occurrence.,4.1.3 Typology of ad hominem,[0],[0]
The results and examples are summarized in Table 2.,4.1.3 Typology of ad hominem,[0],[0]
The data verification annotation study (section 4.1.1) has two direct consequences.,4.1.4 Results and interpretation,[0],[0]
"First, the high κ score (0.79) answers RQ2: for recognizing ad hominem argument, no previous context is necessary.",4.1.4 Results and interpretation,[0],[0]
"Second, we still found 5% overlooked ad hominem arguments in CMV thus a moderationfacilitating tool might come handy; this can be served by the well-performing CNN model (0.810 accuracy; section 4.1.2).
",4.1.4 Results and interpretation,[0],[0]
"The existing theoretical typology of ad hominem arguments, as presented for example in most textbooks, provides only a very simplified view.",4.1.4 Results and interpretation,[0],[0]
"On the one hand, some of the categories which we found in the empirical labeling study (section 4.1.3) do map to their corresponding counterparts (such as the vulgar insults).",4.1.4 Results and interpretation,[0],[0]
"On the other hand, some ad hominem insults typical to online argumentation (illiteracy insults, condescension) are not present in studies on ad hominem.",4.1.4 Results and interpretation,[0],[0]
"Hence, we claim that any potential typology of ad hominem arguments should be multinomial rather than categorical, as we found multiple different spans in a single argument.",4.1.4 Results and interpretation,[0],[0]
"In the following section, we increase the complexity of the studied discourse by taking the original post into account.",4.2 Triggers of first level ad hominem,[0],[0]
We already showed that ad hominem arguments are usually preceded by a discussion between the interlocutors.,4.2.1 Annotation study,[0],[0]
"However, 897 submissions (original posts; OPs) have at least one intermediate ad hominem (in other words, the original post is directly attacked).",4.2.1 Annotation study,[0],[0]
We were thus interested in what triggers these first-level ad hominem arguments.,4.2.1 Annotation study,[0],[0]
"We hypothesize two causes: (1) the controversy of the OP, similarly to some related works on news comments (Coe et al., 2014) and (2) the reasonableness of the OP (whether the topic is reasonable to argue about).",4.2.1 Annotation study,[0],[0]
"We model both features on a three-point scale, namely controversy: 1 = ‘not re-
ally controversial’, 2 = ‘somehow controversial’, 3 = ‘very controversial’ and reasonableness: 1 = ‘quite stupid’, 2 = ‘neutral’, 3 = ‘quite reasonable’.7
We sampled two groups of OPs: those which had some ad hominem arguments in any of its threads but no delta (ad hominem group) and those without ad hominem but some deltas (Delta group).",4.2.1 Annotation study,[0],[0]
"In total, 1,800 balanced instances were annotated by five workers and the resulting value was averaged for each item.8
Statistical analysis of the annotated 1,800 OPs revealed that ad hominem arguments are associated with more controversial OPs (mean controversy 1.23) while delta-awarded arguments with less controversial OPs (mean controversy 1.06; K-S test;9 statistics 0.13, P-value: 7.97× 10−7).",4.2.1 Annotation study,[0],[0]
"On the other hand, reasonableness does not seem to play such a role.",4.2.1 Annotation study,[0],[0]
"The difference between ad hominem in reasonable OPs (mean 1.20) and delta in reasonable OPs (mean 1.11) is not that statistically strong; (K-S test statistics: 0.07, P-value: 0.02).",4.2.1 Annotation study,[0],[0]
We further built a regression model for predicting controversy and reasonableness of the OPs.,4.2.2 Regression model for predicting controversy and reasonableness,[0],[0]
Along with Bi-LSTM and CNN networks (same models as in 4.1.2) we also developed a neural model that integrates CNN with topic distribution (CNN+LDA).,4.2.2 Regression model for predicting controversy and reasonableness,[0],[0]
The motivation for a topicincorporating model was based on our earlier observations presented in section 3.,4.2.2 Regression model for predicting controversy and reasonableness,[0],[0]
"In particular, we trained an LDA topic model (k = 50) (Blei et al., 2003) on the heldout OPs and during training/testing, we merged the estimated topic distribution vector with the output layer after convolution and pooling.",4.2.2 Regression model for predicting controversy and reasonableness,[0],[0]
"We performed 10-fold cross validation on the 1,800 annotated OPs and got reasonable performance for controversy prediction (ρ
7Examples of not really controversial: ”I Don’t Think Monty",4.2.2 Regression model for predicting controversy and reasonableness,[0],[0]
"Python is Funny”, very controversial: ”Blacks are generally intellectual inferior to the other major races”, quite stupid: ”Burritos are better than sandwiches”, and quite reasonable: ”Nations whose leadership is based upon religion are fundamentally backwards”.
",4.2.2 Regression model for predicting controversy and reasonableness,[0],[0]
"8A pilot crowd sourcing annotation with 5 + 5 workers showed a fair reliability for controversy (Spearman’s ρ 0.804) and medium reliability for reasonableness (Spearman’s ρ 0.646).
",4.2.2 Regression model for predicting controversy and reasonableness,[0],[0]
"9Kolmogorov-Smirnov (K-S) test is a non-parametric test without any assumptions about the underlying probability distribution.
0.569) and medium performance for reasonableness prediction (ρ 0.385), respectively; both using the CNN+LDA model (see Table 3).
",4.2.2 Regression model for predicting controversy and reasonableness,[0],[0]
"We then used the trained model and extrapolated on all held-out OPs (1,267 ad hominem and 10,861 delta OPs, respectively).",4.2.2 Regression model for predicting controversy and reasonableness,[0],[0]
"The analysis again showed that ad hominem arguments tend to be found under more controversial OPs whereas delta arguments in the less controversial ones (KS test statistics: 0.14, P-value: 1 × 10−18).",4.2.2 Regression model for predicting controversy and reasonableness,[0],[0]
"For reasonableness, the rather low performance of the predictor does not allow us draw any conclusions on the extrapolated data.",4.2.2 Regression model for predicting controversy and reasonableness,[0],[0]
Controversy of the original post is immediately heating up the debate participants and correlates with a higher number of direct ad hominem responses.,4.2.3 Results and interpretation,[0],[0]
"This corresponds to observations made in comments in newswire where ‘weightier’ topics tended to stir incivility (Coe et al., 2014).",4.2.3 Results and interpretation,[0],[0]
"On the other hand, ‘stupidity’ (or ‘reasonableness’) does not seem to play any significant role.",4.2.3 Results and interpretation,[0],[0]
The CNN+LDA model for predicting controversy (ρ 0.569) might come handy for signaling potentially ‘heated’ discussions.,4.2.3 Results and interpretation,[0],[0]
"In this section, we focus on the dialogical aspect of CMV debates and dynamics of ad hominem fallacies.",4.3 Before calling names,[0],[0]
"Although ad hominem arguments appear in many forms (Section 4.1.3), we treat all ad hominem arguments equal in the following experiments.",4.3 Before calling names,[0],[0]
"So far we explored what makes an ad hominem argument and whether debated topic influences the
number of intermediate attacks.",4.3.1 Data sampling,[0],[0]
"However, possible causes of the argumentative dynamics that ends up with an ad hominem argument remain an open question, which has been addressed in neither argumentation theory nor in cognitive psychology, to the best of our knowledge.",4.3.1 Data sampling,[0],[0]
"We thus cast an explanation of triggers and dynamics of ad hominem discussions as a supervised machine learning problem and draw theoretical insights by a retrospective interpretation of the learned models.
",4.3.1 Data sampling,[0],[0]
We sample positive instances by taking three contextual arguments preceding the ad hominem argument from threads which are an interplay between two persons.,4.3.1 Data sampling,[0],[0]
Negative samples are drawn similarly from threads in which the argument is awarded with ∆ as shown in Figure 2.10 Each instance consists of the three concatenated arguments delimited by a special OOV token.,4.3.1 Data sampling,[0],[0]
"This resulted in 2,582 balanced training instances.",4.3.1 Data sampling,[0],[0]
"The alleged lack of interpretability of neural networks has motivated several lines of approaches, such as layer-wise relevance propagation (Arras et al., 2017) or representation erasure (Li et al., 2016), both on sentiment analysis.",4.3.2 Neural models,[0],[0]
"As our task at hand deals with multi-party discourse that presumably involves temporal relations important for the learned representation, we opted for a state-of-theart self-attentive LSTM model.",4.3.2 Neural models,[0],[0]
"In particular, we re-implemented the Structured Self-Attentive Embedding Neural Network (SSAE-NN) (Lin et al., 2017) which learns an embedding matrix representation of the input using attention weights.",4.3.2 Neural models,[0],[0]
"To make the attention even more interpretable, we replaced the final non-linear MLP layers with a single linear classifier (softmax).",4.3.2 Neural models,[0],[0]
"By summing over one dimension of the attention embedding matrix, each word from the input sequence gets associated
10To ensure as much content similarity as possible, we used the same similarity sampling as in section 4.1.1.
with a single attention weight that gives us insights into the classifier’s ‘features’ (still indirectly, as the true representation is a matrix; see the original paper).11 The learning objective is to recognize whether the thread ends up in an ad hominem argument or a delta point.",4.3.2 Neural models,[0],[0]
"We trained the model in 10-fold cross-validation and although our goal is not to achieve the best performance but rather to gain insight, we also tested a CNN model (accuracy 0.7095) which performed slightly worse than the SSAE-NN model (accuracy 0.7208).",4.3.2 Neural models,[0],[0]
"During testing the model, we projected attention weights to the original texts as heat maps and manually analyzed 191 true positives (ad hominem threads recognized correctly), as well as 77 false positives (ad hominem threads misclassified as delta) and 84 false negatives (delta as ad hominem), in total about 120k tokens.",4.3.3 Results and interpretation,[0],[0]
"The full output is available in the supplementary materials, we use IDs as a reference in the following text.
",4.3.3 Results and interpretation,[0],[0]
"In the following analysis, we solely relied on the weights of words or phrases learned by the attention model, see an example in Figure 3.",4.3.3 Results and interpretation,[0],[0]
"Based on our observations, we summarize several linguistic and argumentative phenomena with examples most likely responsible for ad hominem threads in Table 4.
",4.3.3 Results and interpretation,[0],[0]
The identified phenomena have few interesting properties in common.,4.3.3 Results and interpretation,[0],[0]
"First, they all are topic-independent rhetorical devices (except for the loaded keywords at the bottom).",4.3.3 Results and interpretation,[0],[0]
"Second, many of them deal with meta-level argumentation, i.e., arguing about argumentation (such as missing support or fallacy accusations).",4.3.3 Results and interpretation,[0],[0]
"Third, most of them do not contain profanity (in contrast to the actual ad hominem arguments of which a third are vulgar insults; cf. Table 2).",4.3.3 Results and interpretation,[0],[0]
"And finally, all of them should be easy to avoid.
",4.3.3 Results and interpretation,[0],[0]
"Misleading ‘features’ False positives revealed properties that misled the network to classify delta threads as ad hominem threads.
",4.3.3 Results and interpretation,[0],[0]
"• These include topic words (such as racism, blacks, slave, abortion) which reflects the implicit bias in the data.
",4.3.3 Results and interpretation,[0],[0]
"• Actual interest mixed with indifference in 11We also experimented with regularizing the attention matrix as the authors proposed, but it resulted in worse performance.
",4.3.3 Results and interpretation,[0],[0]
"sarcasm is also problematic (185(-2) “That’s a very interesting ...”).
",4.3.3 Results and interpretation,[0],[0]
"• Another problematic phenomena is also expressed disagreement (678(-2) “overheated rhetoric”, 203(-2)",4.3.3 Results and interpretation,[0],[0]
"“But I suppose this argument is ...”, 230(-2)",4.3.3 Results and interpretation,[0],[0]
"“But I don’t think it’s quite ...”, 938(-1)",4.3.3 Results and interpretation,[0],[0]
"“I disagree too, however ...”).
",4.3.3 Results and interpretation,[0],[0]
"False negatives were caused basically by presence of many ‘informative’ content words (980 unemployment, quarterly publication, inflation data, 474 actual publications, this experiment, biological ailments, medical doctorate, 1214 graduate degree, education, health insurance) and misinterpreted sarcasm (285(-1)",4.3.3 Results and interpretation,[0],[0]
“Also this is a cute analogy”).,4.3.3 Results and interpretation,[0],[0]
"In this article, we investigated ad hominem argumentation on three levels of discourse complexity.",5 Conclusion,[0],[0]
"We looked into qualitative and quantative properties of ad hominem arguments, crowdsourced labeled data, experimented with models for prediction (0.810 accuracy; 4.1.2), and proposed an updated typology of ad hominem properties (4.1.3).",5 Conclusion,[0],[0]
We then looked into the dynamics of argumentation to examine the relation between the quality of the original post and immediate ad hominem arguments (4.2).,5 Conclusion,[0],[0]
"Finally, we exploited the learned representation of Self-Attentive Embedding Neural Network to search for features triggering ad hominem in one-to-one discussions.",5 Conclusion,[0],[0]
"We found several categories of rhetorical devices as well as misleading features (4.3.3).
",5 Conclusion,[0],[0]
There are several points that deserve further investigation.,5 Conclusion,[0],[0]
"First, we have ignored metainformation of the debate participants, such as their overall activity (i.e., whether they are spammers or trolls).",5 Conclusion,[0],[0]
"Second, the proposed typology of ad hominem causes has not yet been post-verified empirically.",5 Conclusion,[0],[0]
"Third, we expect that personality traits of the participants (BIG5) may also play a significant role in the argumentative exchange.",5 Conclusion,[0],[0]
"We leave these points for future work.
",5 Conclusion,[0],[0]
"We believe that our findings will help gain better understanding of, and hopefully keep restraining from, ad hominem fallacies in good-faith discussions.",5 Conclusion,[0],[0]
"This work has been supported by the ArguAna Project GU 798/20-1 (DFG), and by the DFGfunded research training group “Adaptive Preparation of Information form Heterogeneous Sources” (AIPHES, GRK 1994/1).",Acknowledgments,[0],[0]
Arguing without committing a fallacy is one of the main requirements of an ideal debate.,abstractText,[0],[0]
"But even when debating rules are strictly enforced and fallacious arguments punished, arguers often lapse into attacking the opponent by an ad hominem argument.",abstractText,[0],[0]
"As existing research lacks solid empirical investigation of the typology of ad hominem arguments as well as their potential causes, this paper fills this gap by (1) performing several large-scale annotation studies, (2) experimenting with various neural architectures and validating our working hypotheses, such as controversy or reasonableness, and (3) providing linguistic insights into triggers of ad hominem using explainable neural network architectures.",abstractText,[0],[0]
Before Name-calling: Dynamics and Triggers of Ad Hominem Fallacies in Web Argumentation,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 1975–1985 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"The task of Natural Language Inference (NLI)1 has received a lot of attention and has elicited models which have achieved impressive results on the Stanford NLI (SNLI) dataset (Bowman et al., 2015).",1 Introduction,[0],[0]
"Such results are impressive due to the linguistic knowledge required to solve the task (LoBue and Yates, 2011; Maccartney, 2009).",1 Introduction,[0],[0]
"However, the ever-growing complexity of these models inhibits a full understanding of the phenomena that they capture.
",1 Introduction,[0],[0]
"1Also known as Recognizing Textual Entailment.
",1 Introduction,[0],[0]
"As a consequence, evaluating these models purely on test set performance may not yield enough insight into the complete repertoire of abilities learned and any possible abnormal behaviors (Kummerfeld et al., 2012; Sammons et al., 2010).",1 Introduction,[0],[0]
"A similar case can be observed in models from other domains; take as an example an image classifier that predicts based on the image’s background rather than on the target object (Zhao et al., 2017; Ribeiro et al., 2016), or a classifier used in social contexts that predicts a label based on racial attributes (Crawford and Calo, 2016).",1 Introduction,[0],[0]
"In both examples, the models exploit a bias (an undesired pattern hidden in the dataset) to enhance accuracy.",1 Introduction,[0],[0]
"In such cases, the models may appear to be robust to new and even challenging test instances; however, this behavior may be due to spurious factors, such as biases.",1 Introduction,[0],[0]
"Assessing to what extent the models are robust to these contingencies just by looking at test accuracy is, therefore, difficult.
",1 Introduction,[0],[0]
"In this work we aim to study how certain factors affect the robustness of three pre-trained NLI models (a conditional encoder, the DAM model (Parikh et al., 2016), and the ESIM model (Chen et al., 2017)).",1 Introduction,[0],[0]
"We call these target factors insensitivity (not recognizing a new instance), polarity (a word-pair bias), and unseen pairs (recognizing the semantic relation of new word pairs).",1 Introduction,[0],[0]
"We became aware of these factors based on an exploration of the models’ behavior, and we hypothesize that these factors systematically influence the behavior of the models.
",1 Introduction,[0],[0]
"In order to systematically test if the above factors affect robustness, we propose a set of challenging instances for the models: We sample a set of instances from SNLI data, we apply a transformation on this set that yields a new set of instances, and we test both how well the models
1975
classify these new instances and whether the target factors influence the models’ behavior.",1 Introduction,[0],[0]
"The transformation (swapping a pair of words between premise and hypothesis sentences) is intended to yield both easy and difficult instances to challenge the models, but easy for a human to annotate them.
",1 Introduction,[0],[0]
"We draw motivation to study the robustness of NLI models from previous work on evaluating complex models (Isabelle et al., 2017; White et al., 2017).",1 Introduction,[0],[0]
"Furthermore, we base our approach on the discipline of behavioral science which provides methodologies for analyzing how certain factors influence the behavior of subjects under study (Epling and Pierce, 1986).
",1 Introduction,[0],[0]
We aim to answer the research questions: How robust is the predictive behavior of the pre-trained models under our transformation to input data?,1 Introduction,[0],[0]
"Do the target factors (insensitivity, polarity, and unseen pairs) influence the prediction of the models?",1 Introduction,[0],[0]
"Are these factors common across models?
",1 Introduction,[0],[0]
"Our results show that the models are robust mainly where the semantics of the new instances do not change significantly with respect to the sampled instances and thus the class labels remain unaltered; i.e., the models are insensitive to our transformation to input data.",1 Introduction,[0],[0]
"However, when the class labels change, the models significantly drop accuracy.",1 Introduction,[0],[0]
"In addition, the models exploit a bias, polarity, to stay robust when facing new instances.",1 Introduction,[0],[0]
"We also find that the models are able to cope with unseen word pairs under a hypernym relation, but not with those under an antonym relation, suggesting their inability to learn a symmetric relation.",1 Introduction,[0],[0]
"Previous works in ML and NLP have analyzed different aspects of complex models using a variety of approaches; for example, understanding input-output relationships by approximating the local or global behavior of the model using an interpretable model (Ribeiro et al., 2016; Craven and Shavlik, 1996), or analyzing the output of the model under lesions of its internal mechanism (Li et al., 2016).",2.1 Analysis of Complex Models,[0],[0]
"Another line of work has analyzed the robustness of NLP models both via controlled experiments to complement the information from the test set accuracy and test abilities of the models (Isabelle et al., 2017; B. Hashemi and Hwa, 2016; White et al., 2017) and via adversarial instances to expose weaknesses (Jia and Liang, 2017).",2.1 Analysis of Complex Models,[0],[0]
"In addi-
tion, work has been done to uncover and diminish gender biases in datasets captured by structured prediction models (Zhao et al., 2017) and word embeddings (Bolukbasi et al., 2016).",2.1 Analysis of Complex Models,[0],[0]
"However, to the best of our knowledge, there is no previous work to study the robustness of NLI models while analyzing factors affecting their predictions.",2.1 Analysis of Complex Models,[0],[0]
"Previous work on behavioral science has focused on understanding how environmental factors influence behaviors in both human (Soman, 2001) and animal (Mench, 1998) subjects with the objective of predicting behavioral patterns or analyzing environmental conditions.",2.2 Behavior Analysis,[0],[0]
"This methodology also helps to identify and understand abnormal behaviour by collecting behavioral data without the need to reach any internal component of the subject (Birkett and Newton-Fisher, 2011).
",2.2 Behavior Analysis,[0],[0]
"We base our approach in the discipline of behavioral science since some of our research questions and objectives align to those from this discipline; in addition, its methodology to study how factors effect on the subjects’ behavior provides statistical guarantees.",2.2 Behavior Analysis,[0],[0]
"NLI, or RTE, is the task of inferring whether a natural language sentence (hypothesis) is entailed by another natural language sentence (premise) (Maccartney, 2009; Dagan et al., 2009; Dagan and Glickman, 2004).",3.1 Natural Language Inference,[0],[0]
"More formally, given a pair of natural language sentences i = (premise, hypothesis), a model classifies the type of relation such sentences fall in from three possible classes, entailment, where the hypothesis is necessarily true given the premise, neutral, where the hypothesis may be true given the premise, and contradiction, where the hypothesis is necessarily false given the premise.",3.1 Natural Language Inference,[0],[0]
"Solving this task is challenging since it requires linguistic and semantic knowledge, such as co-reference, hypernymy, and antonymy (LoBue and Yates, 2011), as well as pragmatic knowledge and informal reasoning (Maccartney, 2009).",3.1 Natural Language Inference,[0],[0]
Behavior analysis seeks to account for the role that factors (independent variables) play in the behavior (dependent variable) of subjects.,3.2 Behavior Analysis,[0],[0]
"Testing for
the influence of a factor on the subject’s behavior can be done via statistical tests: A null hypothesis states no association between a target factor and behavior, whereas the alternative hypothesis states an association (McDonald, 2014).",3.2 Behavior Analysis,[0],[0]
"The Stanford NLI dataset (Bowman et al., 2015) was created with the purpose of training deep neural models while providing human-annotated data.",4.1 SNLI Dataset,[0],[0]
"Each instance was created by providing a premise sentence, harvested from a pre-existing dataset, to a crowdsource worker who was instructed to produce three hypothesis sentences, one for each NLI class (entailment, neutral, contradiction).",4.1 SNLI Dataset,[0],[0]
This process yielded a balanced dataset containing around 570K instances.,4.1 SNLI Dataset,[0],[0]
"Conditional Encoder We use two bidirectional LSTMs; the first LSTM encodes the premise sentence into a fixed-size vector embedding by sequentially reading on a word basis, while the second LSTM encodes the hypothesis sentence conditioned on the representation of the premise sentence.",4.2 Models,[0],[0]
At the final layer we used a softmax over the class labels on top of a 3-layer MLP.,4.2 Models,[0],[0]
"All embeddings, of dimensionality d = 100, were randomly initialized and learned during training.",4.2 Models,[0],[0]
"Accuracy on SNLI’s dev set is 0.782.
",4.2 Models,[0],[0]
"Decomposable Attention Model DAM (Parikh et al., 2016) consists of 2-layer multilayerperceptrons (MLPs) factorized in a 3-step process.",4.2 Models,[0],[0]
"First, a soft-alignment matrix is created for all the words in both the premise and hypothesis.",4.2 Models,[0],[0]
"Then, each word of the premise is paired with the softalignment representation of the hypothesis sentence and fed into an MLP, and similarly for each word in the hypothesis with the soft-alignment of the premise.",4.2 Models,[0],[0]
"The resulting representations are then aggregated where the vector representations of the premise are summed up and the same for those of the hypothesis; the new representations are then fed to an MLP, followed by a linear layer and a softmax whose output is a class label.",4.2 Models,[0],[0]
We use d = 300 dimensional GloVe embeddings (not updated at training time).,4.2 Models,[0],[0]
All layers use the ReLU function.,4.2 Models,[0],[0]
"Accuracy on SNLI’s dev set is 0.854.
",4.2 Models,[0],[0]
"Enhanced Sequential Information Model ESIM (Chen et al., 2017) performs inference in three stages.",4.2 Models,[0],[0]
"First, Input Encoding uses BiLSTMs to produce representations of each word in its context within premise or hypothesis.",4.2 Models,[0],[0]
"Then, Local Inference Modelling constructs new word representations for each hypothesis (premise) by summing over the BiLSTM hidden states for the premise (hypothesis) words using weights from a soft attention matrix.",4.2 Models,[0],[0]
"Additionally, these representations are enhanced with element-wise products and differences of the original hidden states vectors and the new attention based vectors.",4.2 Models,[0],[0]
"Finally, Inference Composition uses a BiLSTM, average and max pooling and an MLP output layer to produce predicted labels.",4.2 Models,[0],[0]
Accuracy on SNLI’s dev set is 0.882.,4.2 Models,[0],[0]
"We test our main hypothesis (Section 1) by perturbing instances in a controlled, simple, and meaningful way.",5 Methods,[0],[0]
"This alteration, at the instance level, yields new sets of instances which range from easy (the semantics and the label of the new instance are the same to those of the original instance) to challenging (both semantics and label of the new instance change with respect to those of the original instance), but all of them remain easy to annotate for a human.
",5 Methods,[0],[0]
"To examine how the models generalize from seen instances to transformed instances, we sample our original instances from the SNLI training set, which we refer to as control instances from now on.",5 Methods,[0],[0]
"We then produce new instances which differ either minimally from the control instances, by changing only a single word in the premise and hypothesis, or more substantially, by copying the same sentence structure into the premise and hypothesis with a single word changed.",5 Methods,[0],[0]
"In this way, we produce instances that contain only words seen at training time, within sentence structures also seen at training time.",5 Methods,[0],[0]
"Thus, our evaluation sets are as in-domain as possible, and control for factors associated with novel sentential contexts and vocabulary.",5 Methods,[0],[0]
"We first sample an instance from the SNLI dataset according to a given criterion, namely we look for a specific word pair in the instance; then, we apply our transformation over the word pair.",5.1 Basic Procedure and Statistical Analyses,[0],[0]
"This pro-
cedure generates a new instance.",5.1 Basic Procedure and Statistical Analyses,[0],[0]
"After that, the models label the new instance, and we statistically analyze which target factors influenced the models to respond in such a way via chi-square (McNemar’s, independence, and homogeneity) tests (McDonald, 2014; Alpaydin, 2010).",5.1 Basic Procedure and Statistical Analyses,[0],[0]
When the sample size is too small we apply Yate’s correction or a Fisher test.,5.1 Basic Procedure and Statistical Analyses,[0],[0]
"We use the StatsModels (Seabold and Perktold, 2010) and SciPy (Oliphant, 2007) packages.",5.1 Basic Procedure and Statistical Analyses,[0],[0]
"The level of significance is p < 0.0001, unless otherwise stated.2 This procedure is applied in four experiments, where we study the effect of different word pairs (hypernym, hyponym, and antonyms) and the effect of two types of context words surrounding the word pairs which we refer to as in situ and ex situ (explained in Section 5.3).",5.1 Basic Procedure and Statistical Analyses,[0],[0]
"Given a set of word pairs of the form W = (w1, w2), where w1 and w2 hold under a semantic relation s ∈ {antonymy, hypernymy, hyponymy}, we look through the training set for instances ik = (pk, hk), where pk and hk are premise and hypothesis sentences, respectively, such that w1 ∈ pk and w2 ∈ hk.",5.2 Transformation and Word Pairs,[0],[0]
"For each instance ik we apply transformation T : we swap w1 with w2; this transformation yields an instance im = (pm, hm) where w2 ∈ pm, w1 ∈ hm and w1 /∈",5.2 Transformation and Word Pairs,[0],[0]
"pm, w2 /∈",5.2 Transformation and Word Pairs,[0],[0]
"hm.3
",5.2 Transformation and Word Pairs,[0],[0]
"An example of transformation T on a contradiction instance ik is the following:
(1) pk :",5.2 Transformation and Word Pairs,[0],[0]
A soccer game occurring at sunset.,5.2 Transformation and Word Pairs,[0],[0]
hk :,5.2 Transformation and Word Pairs,[0],[0]
"A basketball game is occurring at sunrise.
",5.2 Transformation and Word Pairs,[0],[0]
"Where the word pair (sunset , sunrise) are antonyms.",5.2 Transformation and Word Pairs,[0],[0]
"After applying transformation T , we obtain the new contradiction instance",5.2 Transformation and Word Pairs,[0],[0]
"im:
(2) pm :",5.2 Transformation and Word Pairs,[0],[0]
A soccer game occurring at sunrise.,5.2 Transformation and Word Pairs,[0],[0]
hm :,5.2 Transformation and Word Pairs,[0],[0]
"A basketball game is occurring at sunset.
",5.2 Transformation and Word Pairs,[0],[0]
"Consider now the following instance il (class label entailment):
(3) pl :",5.2 Transformation and Word Pairs,[0],[0]
A little girl hugs her brother on a footbridge in a forest.,5.2 Transformation and Word Pairs,[0],[0]
hl :,5.2 Transformation and Word Pairs,[0],[0]
"A pair of siblings are on a bridge.
",5.2 Transformation and Word Pairs,[0],[0]
2We apply a Bonferroni correction.,5.2 Transformation and Word Pairs,[0],[0]
"3If a word w1 or w2 appears more than once, we replace
all the appearances with its corresponding pair, w2 or w1.
",5.2 Transformation and Word Pairs,[0],[0]
"If we now apply transformation T on the hypernym word pair (footbridge, bridge) we derive the new instance in (class neutral):
(4) pn :",5.2 Transformation and Word Pairs,[0],[0]
A little girl hugs her brother on a bridge in a forest.,5.2 Transformation and Word Pairs,[0],[0]
"hn : A pair of siblings are on a footbridge.
",5.2 Transformation and Word Pairs,[0],[0]
"Since swapping word pairs under hypernymy or hyponymy relations may yield a different class label for the new instance, we manually annotate all the instances in the new sample, discarding those that are semantically incoherent.",5.2 Transformation and Word Pairs,[0],[0]
"We consider two types of sentential context for the word pairs, namely in situ and ex situ.",5.3 Experimental Conditions,[0],[0]
"Examples of instances under the in situ condition are Examples 1, 2, 3, and 4 in Section 5.2.",5.3 Experimental Conditions,[0],[0]
The name in situ refers to the fact that we analyze the effect of the transformation T within the original context of the premise and hypothesis sentences.,5.3 Experimental Conditions,[0],[0]
"This allows to control for confounding factors, such as sentence length and order of the context words.
",5.3 Experimental Conditions,[0],[0]
We also consider an ex situ condition in which we remove the word pair from the original premise and hypothesis and analyze the effect of the transformation T within a simplified sentential context which is the same in premise and hypothesis.,5.3 Experimental Conditions,[0],[0]
"Specifically, we randomly select either the premise or hypothesis context from the original instance and copy it into both positions.",5.3 Experimental Conditions,[0],[0]
"In this way, we obtain a sentence pair where the only difference between the premise and hypothesis is the word pair, which allows us to isolate the effect of this pair from its interaction with the surrounding context; this condition thus allows to control for context words.",5.3 Experimental Conditions,[0],[0]
"This process yields a new set of instances, which we refer to as E.
An example of an ex situ instance can be constructed from Example 1 (Section 5.2).",5.3 Experimental Conditions,[0],[0]
"If the premise sentence is selected, then after performing the procedure described above, the following sentence pair ek is generated:
(5) pk :",5.3 Experimental Conditions,[0],[0]
A soccer game occurring at sunset.,5.3 Experimental Conditions,[0],[0]
hk :,5.3 Experimental Conditions,[0],[0]
"A soccer game occurring at sunrise.
",5.3 Experimental Conditions,[0],[0]
"Given a sample E, we apply the transformation T in order to generate a transformed sample ET where the word pairs are swapped, similar to the procedure applied in Section 5.2 on SNLI control instances in order to generate their transformed instances counterpart.",5.3 Experimental Conditions,[0],[0]
"In the latter case, we say that
given a sample of control instances I we generate a transformed sample IT .
",5.3 Experimental Conditions,[0],[0]
"As an example of obtaining a transformed ex situ instance, we apply T to (sunset , sunrise) in Example 5 to obtain the new instance",5.3 Experimental Conditions,[0],[0]
"em:
(6) pm :",5.3 Experimental Conditions,[0],[0]
A soccer game occurring at sunrise.,5.3 Experimental Conditions,[0],[0]
hm :,5.3 Experimental Conditions,[0],[0]
"A soccer game occurring at sunset.
",5.3 Experimental Conditions,[0],[0]
"We note that for both conditions, in situ and ex situ, the same word pairs are swapped, so the differences are the surrounding context words and the factors being controlled.",5.3 Experimental Conditions,[0],[0]
In each experiment we use two sets of instances in order to measure the robustness of the models and analyze our target factors: 1),5.4 Test Sets,[0],[0]
The control instances where the target word pair is in its original position and 2) the transformed instances generated after applying transformation T .,5.4 Test Sets,[0],[0]
The name of each set corresponds with the experimental setting it is used in.,5.4 Test Sets,[0],[0]
"Samples used in in situ experiments are named as I , and E for ex situ.",5.4 Test Sets,[0],[0]
Subscripts distinguish both the type of word pairs (A for antonyms and H for hypernym/hyponym) and the type of set (control or transformed).,5.4 Test Sets,[0],[0]
"For example, IA refers to the control in situ set whose instances contain antonym word pairs, whereas ETH refers to the ex situ transformed test set containing hypernym/hyponym swapped word pairs.
",5.4 Test Sets,[0],[0]
"We clarify: a) the sets IA and IH are sampled from the SNLI dataset; b) transformed test sets are
generated from control sets containing control instances; c) we refer to the sets EA and EH as control test sets because the target word pairs are in their original position, and we apply T on them in order to obtain the transformed samples ETA and ETH , respectively.
",5.4 Test Sets,[0],[0]
"Details about the sets: In order to build set IA, we sample only contradiction instances (instances in EA are also contradictions).",5.4 Test Sets,[0],[0]
"We use the antonym word pairs from (Mohammad et al., 2013) to yield the sets ITA1 and ETA, which also only contain contradictions since the relation of antonymy is symmetric.4 We build two more sets, ITA2 and ITA3 (explained in Section 6.1).",5.4 Test Sets,[0],[0]
"Sets IH , EH , ITH , and ETH contain instances with any class label.",5.4 Test Sets,[0],[0]
"In order to generate sets ITH and ETH , we use the hypernym word pairs from (Baroni et al., 2012).",5.4 Test Sets,[0],[0]
We manually annotate these transformed sets and discard incoherent instances.,5.4 Test Sets,[0],[0]
"We describe the three target factors that we hypothesize that affect the models’ response.
",5.5 Factors Under Study,[0],[0]
Insensitivity is the name we give to the tendency of a model to predict the original label on a transformed instance that is similar to a control instance.,5.5 Factors Under Study,[0],[0]
"Thus a model would be insensitive if, for example, it incorrectly predicts the same class label for both the control instance in Example 3
4The word pair (sunset , sunrise) holds in an antonymy relation regardless of the position of the words in premise and hypothesis sentences.
and the transformed instance in Example 4 just because they closely resemble each other.",5.5 Factors Under Study,[0],[0]
A simple measure of the impact of this effect is to look at the accuracy on the subset of instances in which the gold label was changed by the transformation.,5.5 Factors Under Study,[0],[0]
"We show this effect by statistically correlating the rate of correct predictions with changes in the labels predicted.
",5.5 Factors Under Study,[0],[0]
Unseen Word Pairs are another factor we can use to evaluate robustness.,5.5 Factors Under Study,[0],[0]
"In this case, we are interested in the subset of transformed instances where the swapped word pair is now in an order within premise and hypothesis that was unseen in the training data.",5.5 Factors Under Study,[0],[0]
"An example is Example 2 which contains the unseen word pair (sunrise, sunset); i.e., no instance in the training set contains the word sunrise in the premise and the word sunset in the hypothesis.",5.5 Factors Under Study,[0],[0]
Poor performance on this subset reflects an inability to exploit the symmetry (antonym pairs) or anti-symmetry (hypernym pairs) of the word pairs involved.,5.5 Factors Under Study,[0],[0]
"We show models’ abilities to cope with unseen pairs by statistically associating proportions of instances containing unseen pairs with incorrect predictions rates.
",5.5 Factors Under Study,[0],[0]
Polarity is the name we give to the association between a word pair and the most frequent class it is found in across training instances.,5.5 Factors Under Study,[0],[0]
"For example, we associate the word pair (sunset , sunrise) with polarity contradiction because it mainly appears on training instances with label contradiction.",5.5 Factors Under Study,[0],[0]
"We define four main categories of polarity: neutral, contradiction, entailment, and none for unseen word pairs.5 Accuracy on the subset of instances where polarity and gold label disagree is an indicator of the extent to which a model is influenced by this factor.",5.5 Factors Under Study,[0],[0]
"For example, a model incorrectly predicting label entailment for the instance in Example 4 (class neutral) based on the polarity of class entailment of its word pair (bridge, footbridge) indicates that the model is influenced by this factor.",5.5 Factors Under Study,[0],[0]
We show this influence by statistically correlating labels predicted with polarities.,5.5 Factors Under Study,[0],[0]
Table 1 presents the performance of the models across the different test sets.,6 Experiments and Results,[0],[0]
"In general, DAM and ESIM seem to be more robust than CE, with
5We also define categories when a word pair appears the same number of times in two classes, such as entailmentneutral, though these cases are rare.
",6 Experiments and Results,[0],[0]
the latter’s accuracy degrading to essentially random performance on the most challenging subsets.,6 Experiments and Results,[0],[0]
"However, this general trend is reversed in a single row of the table.",6 Experiments and Results,[0],[0]
"On ETH , ESIM shows a comparable performance to CE.",6 Experiments and Results,[0],[0]
"And on Subset 3 of IH , DAM appears to rely on a bias (polarity) in the same way as CE.",6 Experiments and Results,[0],[0]
"Overall, all models are affected by the three target factors, dropping performance up to 0.25, 0.20, and 0.28 for ESIM, DAM, CE, respectively, just by virtue of our simple transformation of swapping words.",6 Experiments and Results,[0],[0]
"Situ Instances
In this experiment we use sets IA and ITA1 .",6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
"Swapping antonyms seems to have no effect on the overall performance of the DAM model on ITA1 when compared to IA, and little effect on ESIM.",6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
Thus these two models appear to be robust to this transformation.,6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
"Nonetheless, further analysis will not support the conclusion that both models have learned that antonymy is symmetric, and we will show that this seemingly robust behavior is due to confounding factors and not due to inference abilities.",6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
"Accuracy scores of CE model seem to reveal that it is much less robust to the antonym swap, with performance significantly dropping by roughly 10.5% according to a McNemar’s test.
",6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
"Insensitivity Because instances in ITA1 are contradiction, we perform a proxy experiment to understand the models’ sensitivity.",6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
"From IA, we substitute one of the antonyms in each word pair (in each instance) with a hyponym, hypernym, or synonym6 of the other.",6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
"Doing this on both the premise and hypothesis yields two new samples, ITA2 and ITA3 , which we manually annotate.
",6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
"Examples of control (Example 7) and transformed (Example 8) instances are given below, showing the replacement of young, in the hypothesis, with aged, a synonym of elderly from the premise.",6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
This transformation changes gold-label from contradiction to neutral.,6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
"Approximately, half the sample yields such changes in gold-label.
",6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
(7) pk : An elderly woman sitting on a bench.,6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
hk :,6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
"A young mother sits down.
",6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
(8) pm : An elderly woman sitting on a bench.,6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
hm :,6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
"An aged mother sits down.
",6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
"6We manually select these from WordNet such that it appears at least t = 10 times in the training set on either the premise sentences or the hypothesis sentences.
",6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
"This transformation leads to a considerable drop in overall performance for all models when accuracy scores on sets ITA2 and ITA3 are compared to the accuracy on the control instances in IA: up to 0.175 (CE), 0.201 (DAM), and 0.24 (ESIM) points (Table 1).",6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
"To test if insensitivity to the transformation is associated with these behaviors, we measure accuracy only on those instances that changed gold-label (Subset 1 from the sets ITA2 and ITA3 ), where we see a further reduction in performance for all models.",6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
"2-way tests of independence provide strong evidence for the insensitivity of the models (CE: χ2(1) = 73.33, DAM: χ2(1)",6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
"= 108.30, ESIM: χ2(1) = 175.34).
",6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
Table 2 shows the case for ESIM: most of its incorrect predictions are due to predicting the same label on both control and transformed instances when these two type of instances have different gold labels.,6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
"Paradoxically, this effect works in the models’ favour in the antonym swapping case (ITA1 ) because all the gold-labels remain as contradiction.",6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
"Thus ignoring the transformation will avoid any loss in performance.
",6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
Unseen Word Pairs The results in the column Subset 2 of ITA1 (Table 1) suggest that performance on unseen word pairs is weak.,6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
"However, only 40 instances within ITA1 contain unseen antonym pairs; thus the impact of this result may be limited.",6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
2-way tests of homogeneity show that the difference in accuracy of predictions in instances containing seen or unseen word pairs is nonetheless significant for all models (CE: χ2(1),6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
"= 19.46, DAM: χ2(1) = 74.16, ESIM: χ2(1) = 39.33).",6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
"In other words, the models struggle to recognize the reversed antonym pairs, even though they were all seen in their original order at training time.",6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
"This effect can be seen, for example, in the contingency table for DAM in Table 3.
",6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
Polarity Only 11% of the instances in the transformed sample ITA1 contain word pairs that have polarity other than contradiction.,6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
"Thus, a model
relying only on this factor could achieve an accuracy of 89%.",6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
We investigate if the predicted labels on instances in ITA1 are associated with the polarity of the transformed word pair.,6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
"For all models, independence tests are highly significant (CE: χ2(6)",6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
"= 30.69, DAM: χ2(6) = 101.26, ESIM: χ2(6) = 64.40).",6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
Table 4 shows that the predictions of DAM change according to the polarity of the word pairs.,6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
"For example, when the polarity is contradiction, around 98.5% of the predictions are contradictions; however, this figure changes when the polarity is neutral where the rate of correct predictions (contradictions) fall to 80.7%, and a more dramatic fall is observed when the word pairs are unseen (polarity none) where only 50% of the predictions are correct.",6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
"This is strong evidence that the models learned to rely on polarity.
",6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
"We note that a model with perfect accuracy on ITA1 , would lead to a statistic that does not reject the null hypothesis, showing in this case that the predictions are independent of polarity.",6.1 Experiment 1: Swapping Antonyms in In,[0],[0]
"Ex Situ Instances
",6.2 Experiment 2: Swapping Antonyms in,[0],[0]
"In this experiment, we use samples EA and ETA.",6.2 Experiment 2: Swapping Antonyms in,[0],[0]
"Swapping antonyms has little effect on the performance of all models, where the biggest drop comes from DAM (0.029 points).",6.2 Experiment 2: Swapping Antonyms in,[0],[0]
"However, the CE model performs quite poorly at both samples (0.508 and 0.48 accuracy points on EA and ETA);
this drop in performance, with respect to the in situ condition, suggests that the repeated sentence context is too different from the structure of the training instances for the CE model to generalize effectively.
",6.2 Experiment 2: Swapping Antonyms in,[0],[0]
"In this condition, we refrain from analyzing the effect of insensitivity, since doing so would require a transformation similar to that in the in situ condition, which might add an extra layer of change and the results may turn difficult to interpret.
",6.2 Experiment 2: Swapping Antonyms in,[0],[0]
Unseen Word Pairs Accuracy scores strongly suggest that the models are weak at dealing with unseen antonym pairs (Subset 2 of ETA in Table 1); drops in performance on this subset range from 0.315 up to 0.429 points across the three models.,6.2 Experiment 2: Swapping Antonyms in,[0],[0]
"Tests of homogeneity show strong evidence of this weakness for all models (CE: χ2(1) = 15.91, DAM: χ2(1)",6.2 Experiment 2: Swapping Antonyms in,[0],[0]
"= 59.17, ESIM: χ2(1) = 44.72).",6.2 Experiment 2: Swapping Antonyms in,[0],[0]
"Comparing results on this subset with those of Subset 2 in ITA1 , we notice that ESIM and DAM keep similar behavior, but CE seems to be strongly affected by this context type.
",6.2 Experiment 2: Swapping Antonyms in,[0],[0]
"Polarity All models perform poorly in the subset of instances where polarity disagrees with gold label of the instance (Subset 3 of ETA), showing that the models’ behavior rely on this bias.",6.2 Experiment 2: Swapping Antonyms in,[0],[0]
"These results are highly significant (CE: χ2(6) = 34.37, DAM: χ2(6) = 136.99, ESIM: χ2(6) = 103.47).",6.2 Experiment 2: Swapping Antonyms in,[0],[0]
This is further evidence that the models get confused with a simple reversal of an antonym pair.,6.2 Experiment 2: Swapping Antonyms in,[0],[0]
"Hyponyms in In Situ Instances
We now study the effect on the robustness of the systems when we swap hypernym and hyponym word pairs in in situ instances.",6.3 Experiment 3: Swapping Hypernyms and,[0],[0]
"Whole sample accuracy scores in Table 1 significantly drop, according to McNemar’s tests, by 0.25 (ESIM), 0.285 (CE), and 0.128 (DAM) points when we compare scores on control instances (IH ) with those on transformed instances (ITH ).",6.3 Experiment 3: Swapping Hypernyms and,[0],[0]
"We investigate the role of our target factors on these behaviors.
",6.3 Experiment 3: Swapping Hypernyms and,[0],[0]
Insensitivity Around 42% of the instances in ITH (Subset 1) have different gold label from those in IH .,6.3 Experiment 3: Swapping Hypernyms and,[0],[0]
"On these instances, the models’ results are severely impaired: CE and ESIM models’ performances drop to close-to-random (0.271 and 0.315), while DAM decreases by 0.18 points.",6.3 Experiment 3: Swapping Hypernyms and,[0],[0]
"All models’ errors on this subset are strongly as-
sociated with failure to change the predicted class (CE:χ2(1)",6.3 Experiment 3: Swapping Hypernyms and,[0],[0]
"= 90.73, DAM:χ2(1) = 101.52, ESIM:χ2(1) = 150.92).",6.3 Experiment 3: Swapping Hypernyms and,[0],[0]
"In contrast to the case in Experiment 1, insensitivity acts in detriment of the models’ robustness when gold labels change after the transformation.
",6.3 Experiment 3: Swapping Hypernyms and,[0],[0]
Unseen Word Pairs,6.3 Experiment 3: Swapping Hypernyms and,[0],[0]
"Whereas model performance was significantly worse on unseen antonym pairs, this effect is not obvious on the hyponymhypernym results (Subset 2 of ITH ).",6.3 Experiment 3: Swapping Hypernyms and,[0],[0]
"In fact, all models have a slightly higher accuracy on this subset than overall.",6.3 Experiment 3: Swapping Hypernyms and,[0],[0]
"Homogeneity tests find no evidence of an association between unseen word pairs and incorrect predictions for any model (CE:χ2(1) = 0.00036, p = 0.98, DAM:χ2(1) = 0.98, p = 0.32, ESIM:χ2(1) = 0.178, p = 0.67).",6.3 Experiment 3: Swapping Hypernyms and,[0],[0]
This effect may be explained by the models exploiting information from word embeddings.,6.3 Experiment 3: Swapping Hypernyms and,[0],[0]
"It has been shown that word embeddings are able to capture hypernymy (Sanchez and Riedel, 2017); thus the models may use this information to generalize to unseen hypernym pairs.
",6.3 Experiment 3: Swapping Hypernyms and,[0],[0]
"Polarity We find very strong evidence for an association between polarity and class label predicted on sample IH for all models (CE:χ2(10) = 168.40, DAM:χ2(10) = 182.76, ESIM:χ2(10) = 157.76).",6.3 Experiment 3: Swapping Hypernyms and,[0],[0]
"However, for sample ITH , only DAM keeps this strong correlation (χ2(14) = 47.71).",6.3 Experiment 3: Swapping Hypernyms and,[0],[0]
"In the case of CE, we find weak evidence in favour of this correlation on instances of ITH (χ2(14) = 25.27, p = 0.03).",6.3 Experiment 3: Swapping Hypernyms and,[0],[0]
"For ESIM we find no evidence of correlation (χ2(14) = 22.72, p = 0.06), thus we do not reject the null hypothesis.",6.3 Experiment 3: Swapping Hypernyms and,[0],[0]
"Polarity’s influence can be observed in Subset 3 of IH (Table 1), where we observe a drop in accuracy for instances whose gold labels do not match the polarity of the word pairs, compared to the accuracy of the whole sample; this means that when the models have polarity as a cue, they improve performance.",6.3 Experiment 3: Swapping Hypernyms and,[0],[0]
"Hyponyms in Ex Situ Instances
All models’ performance significantly drop (p < 0.01) after our transformation by 0.208 (CE), 0.061 (DAM) and 0.195 (ESIM) points, where performance of ESIM is comparable to that of CE on both samples, EH and ETH .",6.4 Experiment 4: Swapping Hypernyms and,[0],[0]
"Compared to the in situ condition, DAM’s performance improves, opposite to CE’s and ESIM’s behavior.
",6.4 Experiment 4: Swapping Hypernyms and,[0],[0]
"Insensitivity The drop in performance described above can be partially explained by insensitivity to changes in gold label, since around 93% of the instances in ETH changed gold-label with respect to EH .",6.4 Experiment 4: Swapping Hypernyms and,[0],[0]
We find strong statistical evidence for this hypothesis (CE:χ2(1) =,6.4 Experiment 4: Swapping Hypernyms and,[0],[0]
"175.19, DAM:χ2(1) = 158.62, ESIM:χ2(1) = 252.27).",6.4 Experiment 4: Swapping Hypernyms and,[0],[0]
"However, in the case of DAM, this factor seems to play a small role on its behavior as seen when we compare accuracy on Subset 1 with that of the whole transformed sample.
",6.4 Experiment 4: Swapping Hypernyms and,[0],[0]
"Insensitivity seems to have a bigger influence on the models when the transformed instances are closer to the training set: Accuracy scores on Subset 1 from ITH are smaller than those on Subset 1 from ETH .
",6.4 Experiment 4: Swapping Hypernyms and,[0],[0]
"Unseen Word Pairs Similar to the in situ condition, our homogeneity tests show no evidence for incorrect predictions being due to unseen word pairs (CE:χ2(1) = 0.35, p = 0.55, DAM:χ2(1) = 2.43, p = 0.11, ESIM:χ2(1) = 0.183, p = 0.66).",6.4 Experiment 4: Swapping Hypernyms and,[0],[0]
"We posit the same explanation as before: Models may use hypernymy information contained in the embeddings.
",6.4 Experiment 4: Swapping Hypernyms and,[0],[0]
"Polarity We find statistically high correlation of the models’ predictions with the polarity of the word pairs in the instances from both samples, EH (CE:χ2(10) = 261.77, DAM:χ2(10) = 312.67, ESIM:χ2(10) = 176.38) and ETH (CE:χ2(14) = 56.52, DAM:χ2(14) = 258.09, ESIM:χ2(10) = 105.70).",6.4 Experiment 4: Swapping Hypernyms and,[0],[0]
"This evidence indicates that all models use, to some extent, the polarity as a feature for predicting class labels.",6.4 Experiment 4: Swapping Hypernyms and,[0],[0]
"Although all three models achieve strong results on the original SNLI development set (CE: 0.782, DAM: 0.854, ESIM: 0.882), each model exhibits particular weaknesses on the transformed training instances.",7 Discussion and Conclusions,[0],[0]
"Notably, all perform poorly on ITH instances in which the gold label is changed, with ESIM and CE performing below the level of chance.",7 Discussion and Conclusions,[0],[0]
"Thus, on these instances, the models tend to predict the label of the original unaltered training instance and inference in this case is similar to nearest-neighbour prediction.
",7 Discussion and Conclusions,[0],[0]
"On the other hand, much better performance is obtained for the DAM and ESIM models on ITH instances containing unseen word pairs, indicating these models have learned to infer hyper-
nym/hyponym relations from information in the pre-trained word embeddings.",7 Discussion and Conclusions,[0],[0]
"In contrast, performance on the unseen word pairs in ITA1 and ETA suggests that inferring antonymy from the embeddings is more difficult.
",7 Discussion and Conclusions,[0],[0]
Weak performance is seen again on the EA and ETA instances where the polarity of the antonym pair is not consistent with the gold label.,7 Discussion and Conclusions,[0],[0]
"For these cases, the only difference between premise and hypothesis is the antonym pair, and the models tend to fall back on predicting the most frequent gold label seen for that word pair.
",7 Discussion and Conclusions,[0],[0]
One result that remains anomalous is the overall performance of the ESIM model on the whole ETH sample.,7 Discussion and Conclusions,[0],[0]
"While this sample contains unseen word pairs and instances in which the gold label changes or is inconsistent with polarity, these effects do not by themselves explain the poor performance overall.",7 Discussion and Conclusions,[0],[0]
"Neither is this weakness explained by the ex situ structure, in which premise and hypothesis differ by only one word, as performance on the control ex situ sample, EH , is much stronger.",7 Discussion and Conclusions,[0],[0]
"The effect, then, appears to be due to an interaction of the ex situ structure in combination with the transformation.
",7 Discussion and Conclusions,[0],[0]
"In the present work, we have limited ourselves to examining single influences independently.",7 Discussion and Conclusions,[0],[0]
"However, there are undoubtedly manifold interactions contributing to model performance.",7 Discussion and Conclusions,[0],[0]
"In fact, the complexities of these models (LSTMs, attention mechanisms and MLPs) are specifically intended to capture the interactions between the words in the premise and hypothesis.",7 Discussion and Conclusions,[0],[0]
Further work is required to understand what these interactions are and how they contribute to performance.,7 Discussion and Conclusions,[0],[0]
Fully uncovering these factors in current NLI datasets is a pre-requisite for the construction of more effective resources in the future.,7 Discussion and Conclusions,[0],[0]
"We thank Raul Ortiz Pulido and Erick Sanchez Carmona for insightful discussions, Pasquale Minervini for providing the implementations of DAM and ESIM, Pontus Stenetorp for providing valuable feedback on the manuscript, and Johannes Welbl for insightful comments.",Acknowledgments,[0],[0]
The first author was recipient of a scholarship from CONACYT.,Acknowledgments,[0],[0]
This work was supported by an Allen Distinguished Investigator Award and the EU H2020 SUMMA project (grant agreement number 688139).,Acknowledgments,[0],[0]
"Natural Language Inference is a challenging task that has received substantial attention, and state-of-the-art models now achieve impressive test set performance in the form of accuracy scores.",abstractText,[0],[0]
"Here, we go beyond this single evaluation metric to examine robustness to semantically-valid alterations to the input data.",abstractText,[0],[0]
"We identify three factors insensitivity, polarity and unseen pairs and compare their impact on three SNLI models under a variety of conditions.",abstractText,[0],[0]
Our results demonstrate a number of strengths and weaknesses in the models’ ability to generalise to new in-domain instances.,abstractText,[0],[0]
"In particular, while strong performance is possible on unseen hypernyms, unseen antonyms are more challenging for all the models.",abstractText,[0],[0]
"More generally, the models suffer from an insensitivity to certain small but semantically significant alterations, and are also often influenced by simple statistical correlations between words and training labels.",abstractText,[0],[0]
"Overall, we show that evaluations of NLI models can benefit from studying the influence of factors intrinsic to the models or found in the dataset used.",abstractText,[0],[0]
Behavior Analysis of NLI Models: Uncovering the Influence of Three Factors on Robustness,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 431–441 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Multimodal information processing tasks such as image captioning (Farhadi et al., 2010; Ordonez et al., 2011; Xu et al., 2015) and visual question answering (Visual QA) (Antol et al., 2015) have
∗Equal contributions
gained a lot of attention recently.",1 Introduction,[0],[0]
"A number of significant advances in learning algorithms have been made, along with the development of nearly two dozens of datasets in this very active research domain.",1 Introduction,[0],[0]
"Among those datasets, popular ones include MSCOCO (Lin et al., 2014; Chen et al., 2015), Visual Genome (Krishna et al., 2017), VQA (Antol et al., 2015), and several others.",1 Introduction,[0],[0]
"The overarching objective is that a learning machine needs to go beyond understanding different modalities of information separately (such as image recognition alone) and to learn how to correlate them in order to perform well on those tasks.
",1 Introduction,[0],[0]
"431
To evaluate the progress on those complex and more AI-like tasks is however a challenging topic.",1 Introduction,[0],[0]
"For tasks involving language generation, developing an automatic evaluation metric is itself an open problem (Anderson et al., 2016; Kilickaya et al., 2017; Liu et al., 2016; Kafle and Kanan, 2017b).",1 Introduction,[0],[0]
"Thus, many efforts have concentrated on tasks such as multiple-choice Visual QA (Antol et al., 2015; Zhu et al., 2016; Jabri et al., 2016) or selecting the best caption (Hodosh et al., 2013; Hodosh and Hockenmaier, 2016; Ding et al., 2016; Lin and Parikh, 2016), where the selection accuracy is a natural evaluation metric.
",1 Introduction,[0],[0]
"In this paper, we study how to design highquality multiple choices for the Visual QA task.",1 Introduction,[0],[0]
"In this task, the machine (or the human annotator) is presented with an image, a question and a list of candidate answers.",1 Introduction,[0],[0]
"The goal is to select the correct answer through a consistent understanding of the image, the question and each of the candidate answers.",1 Introduction,[0],[0]
"As in any multiple-choice based tests (such as GRE), designing what should be presented as negative answers — we refer them as decoys — is as important as deciding the questions to ask.",1 Introduction,[0],[0]
"We all have had the experience of exploiting the elimination strategy: This question is easy — none of the three answers could be right so the remaining one must be correct!
",1 Introduction,[0],[0]
"While a clever strategy for taking exams, such “shortcuts” prevent us from studying faithfully how different learning algorithms comprehend the meanings in images and languages (e.g., the quality of the embeddings of both images and languages in a semantic space).",1 Introduction,[0],[0]
"It has been noted that machines can achieve very high accuracies of selecting the correct answer without the visual input (i.e., the image), the question, or both (Jabri et al., 2016; Antol et al., 2015).",1 Introduction,[0],[0]
"Clearly, the learning algorithms have overfit on incidental statistics in the datasets.",1 Introduction,[0],[0]
"For instance, if the decoy answers have rarely been used as the correct answers (to any questions), then the machine can rule out a decoy answer with a binary classifier that determines whether the answers are in the set of the correct answers — note that this classifier does not need to examine the image and it just needs to memorize the list of the correct answers in the training dataset.",1 Introduction,[0],[0]
"See Fig. 1 for an example, and Sect.",1 Introduction,[0],[0]
"3 for more and detailed analysis.
",1 Introduction,[0],[0]
We focus on minimizing the impacts of exploiting such shortcuts.,1 Introduction,[0],[0]
"We suggest a set of principles
for creating decoy answers.",1 Introduction,[0],[0]
"In light of the amount of human efforts in curating existing datasets for the Visual QA task, we propose two procedures that revise those datasets such that the decoy answers are better designed.",1 Introduction,[0],[0]
"In contrast to some earlier works, the procedures are fully automatic and do not incur additional human annotator efforts.",1 Introduction,[0],[0]
"We apply the procedures to revise both Visual7W (Zhu et al., 2016) and VQA (Antol et al., 2015).",1 Introduction,[0],[0]
"Additionally, we create new multiplechoice based datasets from COCOQA (Ren et al., 2015) and the recently released VQA2 (Goyal et al., 2017) and Visual Genome datasets (Krishna et al., 2017).",1 Introduction,[0],[0]
"The one based on Visual Genome becomes the largest multiple-choice dataset for the Visual QA task, with more than one million image-question-candidate answers triplets.
",1 Introduction,[0],[0]
We conduct extensive empirical and human studies to demonstrate the effectiveness of our procedures in creating high-quality datasets for the Visual QA task.,1 Introduction,[0],[0]
"In particular, we show that machines need to use all three information (image, questions and answers) to perform well — any missing information induces a large drop in performance.",1 Introduction,[0],[0]
"Furthermore, we show that humans dominate machines in the task.",1 Introduction,[0],[0]
"However, given the revised datasets are likely reflecting the true gap between the human and the machine understanding of multimodal information, we expect that advances in learning algorithms likely focus more on the task itself instead of overfitting to the idiosyncrasies in the datasets.
",1 Introduction,[0],[0]
The rest of the paper is organized as follows.,1 Introduction,[0],[0]
In Sect.,1 Introduction,[0],[0]
"2, we describe related work.",1 Introduction,[0],[0]
In Sect.,1 Introduction,[0],[0]
"3, we analyze and discuss the design deficiencies in existing datasets.",1 Introduction,[0],[0]
In Sect.,1 Introduction,[0],[0]
"4, we describe our automatic procedures for remedying those deficiencies.",1 Introduction,[0],[0]
In Sect. 5 we conduct experiments and analysis.,1 Introduction,[0],[0]
We conclude the paper in Sect.,1 Introduction,[0],[0]
6.,1 Introduction,[0],[0]
Wu et al. (2017) and Kafle and Kanan (2017b) provide recent overviews of the status quo of the Visual QA task.,2 Related Work,[0],[0]
There are about two dozens of datasets for the task.,2 Related Work,[0],[0]
"Most of them use real-world images, while some are based on synthetic ones.",2 Related Work,[0],[0]
"Usually, for each image, multiple questions and their corresponding answers are generated.",2 Related Work,[0],[0]
"This can be achieved either by human annotators, or with an automatic procedure that uses captions or question templates and detailed image annota-
tions.",2 Related Work,[0],[0]
"We concentrate on 3 datasets: VQA (Antol et al., 2015), Visual7W (Zhu et al., 2016), and Visual Genome (Krishna et al., 2017).",2 Related Work,[0],[0]
"All of them use images from MSCOCO (Lin et al., 2014).
",2 Related Work,[0],[0]
"Besides the pairs of questions and correct answers, VQA, Visual7W, and visual Madlibs (Yu et al., 2015) provide decoy answers for each pair so that the task can be evaluated in multiple-choice selection accuracy.",2 Related Work,[0],[0]
"What decoy answers to use is the focus of our work.
",2 Related Work,[0],[0]
"In VQA, the decoys consist of human-generated plausible answers as well as high-frequency and random answers from the datasets.",2 Related Work,[0],[0]
"In Visual7W, the decoys are all human-generated plausible ones.",2 Related Work,[0],[0]
"Note that, humans generate those decoys by only looking at the questions and the correct answers but not the images.",2 Related Work,[0],[0]
"Thus, the decoys might be unrelated to the corresponding images.",2 Related Work,[0],[0]
"A learning algorithm can potentially examine the image alone and be able to identify the correct answer.
",2 Related Work,[0],[0]
"In visual Madlibs, the questions are generated with a limited set of question templates and the detailed annotations (e.g., objects) of the images.",2 Related Work,[0],[0]
"Thus, similarly, a learning model can examine the image alone and deduce the correct answer.
",2 Related Work,[0],[0]
"We propose automatic procedures to revise VQA and Visual7W (and to create new datasets based on COCOQA (Ren et al., 2015), VQA2 (Goyal et al., 2017), and Visual Genome) such that the decoy generation is carefully orchestrated to prevent learning algorithms from exploiting the shortcuts in the datasets by overfitting on incidental statistics.",2 Related Work,[0],[0]
"In particular, our design goal is that a learning machine needs to understand all the 3 components of an image-question-candidate answers triplet in order to make the right choice — ignoring either one or two components will result in drastic degradation in performance.
",2 Related Work,[0],[0]
"Our work is inspired by the experiments in (Jabri et al., 2016) where they observe that machines without looking at images or questions can still perform well on the Visual QA task.",2 Related Work,[0],[0]
"Others have also reported similar issues (Goyal et al., 2017; Zhang et al., 2016; Johnson et al., 2017; Agrawal et al., 2016; Kafle and Kanan, 2017a; Agrawal et al., 2018), though not in the multiplechoice setting.",2 Related Work,[0],[0]
"Our work extends theirs by providing more detailed analysis as well as automatic procedures to remedy those design deficiencies.
",2 Related Work,[0],[0]
"Besides Visual QA, VisDial (Das et al., 2017) and Ding et al. (2016) also propose automatic
ways to generate decoys for the tasks of multiplechoice visual captioning and dialog, respectively.
",2 Related Work,[0],[0]
"Recently, Lin and Parikh (2017) study active learning for Visual QA: i.e., how to select informative image-question pairs (for acquiring annotations) or image-question-answer triplets for machines to “learn” from.",2 Related Work,[0],[0]
"On the other hand, our work further focuses on designing better datasets for “evaluating” a machine.",2 Related Work,[0],[0]
"In this section, we examine in detail the dataset Visual7W (Zhu et al., 2016), a popular choice for the Visual QA task.",3 Analysis of Decoy Answers’ Effects,[0],[0]
"We demonstrate how the deficiencies in designing decoy questions impact the performance of learning algorithms.
",3 Analysis of Decoy Answers’ Effects,[0],[0]
"In multiple-choice Visual QA datasets, a training or test example is a triplet that consists of an image I, a question Q, and a candidate answer set A.",3 Analysis of Decoy Answers’ Effects,[0],[0]
"The set A contains a target T (the correct answer) and K decoys (incorrect answers) denoted by D. An IQA triplet is thus {I,Q,A = {T,D1, · · · ,DK}}.",3 Analysis of Decoy Answers’ Effects,[0],[0]
We use C to denote either the target or a decoy.,3 Analysis of Decoy Answers’ Effects,[0],[0]
We investigate how well a learning algorithm can perform when supplied with different modalities of information.,3.1 Visual QA models,[0],[0]
"We concentrate on the one hiddenlayer MLP model proposed in (Jabri et al., 2016), which has achieved state-of-the-art results on the dataset Visual7W.",3.1 Visual QA models,[0],[0]
"The model computes a scoring function f(c, i)
f(c, i) = σ(U",3.1 Visual QA models,[0],[0]
"max(0,W g(c, i))",3.1 Visual QA models,[0],[0]
"+ b) (1)
over a candidate answer c and the multimodal information i, where g is the joint feature of (c, i) and σ(x) = 1/(1 + exp(−x)).",3.1 Visual QA models,[0],[0]
"The information i can be null, the image (I) alone, the question (Q) alone, or the combination of both (I+Q).
",3.1 Visual QA models,[0],[0]
"Given an IQA triplet, we use the penultimate layer of ResNet-200 (He et al., 2016) as visual features to represent I and the average WORD2VEC embeddings (Mikolov et al., 2013) as text features to represent Q and C. To form the joint feature g(c, i), we just concatenate the features together.",3.1 Visual QA models,[0],[0]
The candidate c ∈,3.1 Visual QA models,[0],[0]
"A that has the highest f(c, i) score in prediction is selected as the model output.
",3.1 Visual QA models,[0],[0]
"We use the standard training, validation, and test splits of Visual7W, where each contains 69,817, 28,020, and 42,031 examples respectively.
",3.1 Visual QA models,[0],[0]
Each question has 4 candidate answers.,3.1 Visual QA models,[0],[0]
"The parameters of f(c, i) are learned by minimizing the binary logistic loss of predicting whether or not a candidate c is the target of an IQA triplet.",3.1 Visual QA models,[0],[0]
Details are in Sect.,3.1 Visual QA models,[0],[0]
5 and the Supplementary Material.,3.1 Visual QA models,[0],[0]
"Machines find shortcuts Table 1 summarizes the performance of the learning models, together with the human studies we performed on a subset of 1,000 triplets (c.f. Sect.",3.2 Analysis results,[0],[0]
5 for details).,3.2 Analysis results,[0],[0]
"There are a few interesting observations.
",3.2 Analysis results,[0],[0]
"First, in the row of “A” where only the candidate answers (and whether they are right or wrong) are used to train a learning model, the model performs significantly better than random guessing and humans (52.9% vs. 25%) — humans will deem each of the answers equally likely without looking at both the image and the question!",3.2 Analysis results,[0],[0]
"Note that in this case, the information i in eq. (1) contains nothing.",3.2 Analysis results,[0],[0]
The model learns the specific statistics of the candidate answers in the dataset and exploits those.,3.2 Analysis results,[0],[0]
"Adding the information about the image (i.e., the row of “I+A”), the machine improves significantly and gets close to the performance when all information is used (62.4% vs. 65.7%).",3.2 Analysis results,[0],[0]
There is a weaker correlation between the question and the answers as “Q+A” improves over “A” only modestly.,3.2 Analysis results,[0],[0]
This is expected.,3.2 Analysis results,[0],[0]
"In the Visual7W dataset, the decoys are generated by human annotators as plausible answers to the questions without being shown the images — thus, many decoy answers do not have visual groundings.",3.2 Analysis results,[0],[0]
"For instance, a question of “what animal is running?” elicits equally likely answers such as “dog”, “tiger”, “lion”, or “cat”, while an image of a dog running in the park will immediately rule out all 3 but the “dog”, see Fig. 1 for a similar example.",3.2 Analysis results,[0],[0]
"Thus, the performance of “I+A” implies that many IQA triplets can be solved by object, attribute or concept detection on the image, without understanding the questions.",3.2 Analysis results,[0],[0]
This is indeed the case also for humans — humans can achieve 75.3% by considering “I+A” and not “Q”.,3.2 Analysis results,[0],[0]
"Note that the difference between ma-
chine and human on “I+A” are likely due to their difference in understanding visual information.
",3.2 Analysis results,[0],[0]
"Note that human improves significantly from “I+A” to “I+Q+A” with “Q” added, while the machine does so only marginally.",3.2 Analysis results,[0],[0]
The difference can be attributed to the difference in understanding the question and correlating with the answers between the two.,3.2 Analysis results,[0],[0]
"Since each image corresponds to multiple questions or have multiple objects, solely relying on the image itself will not work well in principle.",3.2 Analysis results,[0],[0]
"Such difference clearly indicates that in the Visual QA model, the language component is weak as the model cannot fully exploit the information in “Q”, making a smaller relative improvement 5.3% (from 62.4% to 65.7%) where humans improved relatively 17.4%.
",3.2 Analysis results,[0],[0]
"Shortcuts are due to design deficiencies We probe deeper on how the decoy answers have impacted the performance of learning models.
",3.2 Analysis results,[0],[0]
"As explained above, the decoys are drawn from all plausible answers to a question, irrespective of whether they are visually grounded or not.",3.2 Analysis results,[0],[0]
"We have also discovered that the targets (i.e., correct answers) are infrequently used as decoys.
",3.2 Analysis results,[0],[0]
"Specifically, among the 69,817 training samples, there are 19,503 unique correct answers and each one of them is used about 3.6 times as correct answers to a question.",3.2 Analysis results,[0],[0]
"However, among all the 69, 817× 3 ≈ 210K decoys, each correct answer appears 7.2 times on average, far below a chance level of 10.7 times (210K÷19, 503 ≈ 10.7).",3.2 Analysis results,[0],[0]
This disparity exists in the test samples too.,3.2 Analysis results,[0],[0]
"Consequently, the following rule, computing each answer’s likelihood of being correct,
P (correct|C) = { 0.5, if C is never seen in training,
# times C as target # times C as target+(# times C as decoys)/K , otherwise,
(2)
should perform well.",3.2 Analysis results,[0],[0]
"Essentially, it measures how unbiased C is used as the target and the decoys.",3.2 Analysis results,[0],[0]
"Indeed, it attains an accuracy of 48.73% on the test data, far better than the random guess and is close to the learning model using the answers’ information only (the “A” row in Table 1).
",3.2 Analysis results,[0],[0]
"Good rules for designing decoys Based on our analysis, we summarize the following guidance rules to design decoys: (1) Question only Unresolvable (QoU).",3.2 Analysis results,[0],[0]
"The decoys need to be equally
plausible to the question.",3.2 Analysis results,[0],[0]
"Otherwise, machines can rely on the correlation between the question and candidate answers to tell the target from decoys, even without the images.",3.2 Analysis results,[0],[0]
Note that this is a principle that is being followed by most datasets.,3.2 Analysis results,[0],[0]
(2) Neutrality.,3.2 Analysis results,[0],[0]
The decoys answers should be equally likely used as the correct answers.,3.2 Analysis results,[0],[0]
(3) Image only Unresolvable (IoU).,3.2 Analysis results,[0],[0]
The decoys need to be plausible to the image.,3.2 Analysis results,[0],[0]
"That is, they should appear in the image, or there exist questions so that the decoys can be treated as targets to the image.",3.2 Analysis results,[0],[0]
"Otherwise, Visual QA can be resolved by objects, attributes, or concepts detection in images, even without the questions.
",3.2 Analysis results,[0],[0]
"Ideally, each decoy in an IQA triplet should meet the three principles.",3.2 Analysis results,[0],[0]
Neutrality is comparably easier to achieve by reusing terms in the whole set of targets as decoys.,3.2 Analysis results,[0],[0]
"On the contrary, a decoy may hardly meet QoU and IoU simultaneously1.",3.2 Analysis results,[0],[0]
"However, as long as all decoys of an IQA triplet meet Neutrality and some meet QoU and others meet IoU, the triplet as a whole still achieves the three principles — a machine ignoring either images or questions will likely perform poorly.",3.2 Analysis results,[0],[0]
"In this section, we describe our approaches of remedying design deficiencies in the existing datasets for the Visual QA task.",4 Creating Better Visual QA Datasets,[0],[0]
We introduce two automatic and widely-applicable procedures to create new decoys that can prevent learning models from exploiting incident statistics in the datasets.,4 Creating Better Visual QA Datasets,[0],[0]
"Main ideas Our procedures operate on a dataset that already contains image-question-target (IQT) triplets, i.e., we do not assume it has decoys already.",4.1 Methods,[0],[0]
"For instance, we have used our procedures to create a multiple-choice dataset from the Visual Genome dataset which has no decoy.",4.1 Methods,[0],[0]
"We assume that each image in the dataset is coupled with “multiple” QT pairs, which is the case in nearly all the existing datasets.",4.1 Methods,[0],[0]
"Given an IQT triplet (I, Q, T), we create two sets of decoy answers.
",4.1 Methods,[0],[0]
• QoU-decoys.,4.1 Methods,[0],[0]
"We search among all other triplets that have similar questions to Q. The targets of those triplets are then collected as the decoys for T. As the targets to similar questions are likely 1E.g., in Fig 1, for the question “What vehicle is pictured?”, the only answer that meets both principles is “train”, which is the correct answer instead of being a decoy.
plausible for the question Q, QoU-decoys likely follow the rules of Neutrality and Question only Unresolvable (QoU).",4.1 Methods,[0],[0]
"We compute the average WORD2VEC (Mikolov et al., 2013) to represent a question, and use the cosine similarity to measure the similarity between questions.
",4.1 Methods,[0],[0]
• IoU-decoys.,4.1 Methods,[0],[0]
"We collect the targets from other triplets of the same image to be the decoys for T. The resulting decoys thus definitely follow the rules of Neutrality and Image only Unresolvable (IoU).
",4.1 Methods,[0],[0]
"We then combine the triplet (I, Q, T) with QoUdecoys and IoU-decoys to form an IQA triplet as a training or test sample.
",4.1 Methods,[0],[0]
"Resolving ambiguous decoys One potential drawback of automatically selected decoys is that they may be semantically similar, ambiguous, or rephrased terms to the target (Zhu et al., 2016).",4.1 Methods,[0],[0]
We utilize two filtering steps to alleviate it.,4.1 Methods,[0],[0]
"First, we perform string matching between a decoy and the target, deleting those decoys that contain or are covered by the target (e.g., “daytime” vs. “during the daytime” and “ponytail” vs. “pony tail”).
",4.1 Methods,[0],[0]
"Secondly, we utilize the WordNet hierarchy and the Wu-Palmer (WUP) score (Wu and Palmer, 1994) to eliminate semantically similar decoys.",4.1 Methods,[0],[0]
"The WUP score measures how similar two word senses are (in the range of [0, 1]), based on the depth of them in the taxonomy and that of their least common subsumer.",4.1 Methods,[0],[0]
"We compute the similarity of two strings according to the WUP scores in a similar manner to (Malinowski and Fritz, 2014), in which the WUP score is used to evaluate Visual QA performance.",4.1 Methods,[0],[0]
We eliminate decoys that have higher WUP-based similarity to the target.,4.1 Methods,[0],[0]
"We use the NLTK toolkit (Bird et al., 2009) to compute the similarity.",4.1 Methods,[0],[0]
"See the Supplementary Material for more details.
",4.1 Methods,[0],[0]
"Other details For QoU-decoys, we sort and keep for each triplet the top N (e.g., 10,000) similar triplets from the entire dataset according to the question similarity.",4.1 Methods,[0],[0]
"Then for each triplet, we compute the WUP-based similarity of each potential decoy to the target successively, and accept those with similarity below 0.9 until we have K decoys.",4.1 Methods,[0],[0]
"We choose 0.9 according to (Malinowski and Fritz, 2014).",4.1 Methods,[0],[0]
We also perform such a check among selected decoys to ensure they are not very similar to each other.,4.1 Methods,[0],[0]
"For IoU-decoys, the potential decoys are sorted randomly.",4.1 Methods,[0],[0]
"The WUP-based
similarity with a threshold of 0.9 is then applied to remove ambiguous decoys.",4.1 Methods,[0],[0]
"Several authors have noticed the design deficiencies in the existing databases and have proposed “fixes” (Antol et al., 2015; Yu et al., 2015; Zhu et al., 2016; Das et al., 2017).",4.2 Comparison to other datasets,[0],[0]
No dataset has used a procedure to generate IoU-decoys.,4.2 Comparison to other datasets,[0],[0]
"We empirically show that how the IoU-decoys significantly remedy the design deficiencies in the datasets.
",4.2 Comparison to other datasets,[0],[0]
Several previous efforts have generated decoys that are similar in spirit to our QoU-decoys.,4.2 Comparison to other datasets,[0],[0]
"Yu et al. (2015), Das et al. (2017), and Ding et al. (2016) automatically find decoys from similar questions or captions based on question templates and annotated objects, tri-grams and GLOVE embeddings (Pennington et al., 2014), and paragraph vectors (Le and Mikolov, 2014) and linguistic surface similarity, respectively.",4.2 Comparison to other datasets,[0],[0]
"The later two are for different tasks from Visual QA, and only Ding et al. (2016) consider removing semantically ambiguous decoys like ours.",4.2 Comparison to other datasets,[0],[0]
"Antol et al. (2015) and Zhu et al. (2016) ask humans to create decoys, given the questions and targets.",4.2 Comparison to other datasets,[0],[0]
"As shown earlier, such decoys may disobey the rule of Neutrality.
",4.2 Comparison to other datasets,[0],[0]
"Goyal et al. (2017) augment the VQA dataset (Antol et al., 2015) (by human efforts) with additional IQT triplets to eliminate the shortcuts (language prior) in the open-ended setting.",4.2 Comparison to other datasets,[0],[0]
Their effort is complementary to ours on the multiplechoice setting.,4.2 Comparison to other datasets,[0],[0]
"Note that an extended task of Visual QA, visual dialog (Das et al., 2017), also adopts the latter setting.",4.2 Comparison to other datasets,[0],[0]
We examine our automatic procedures for creating decoys on five datasets.,5.1 Dataset,[0],[0]
"Table 2 summarizes the characteristics of the three datasets we focus on.
",5.1 Dataset,[0],[0]
"VQA Real (Antol et al., 2015)",5.1 Dataset,[0],[0]
"The dataset uses images from MSCOCO (Lin et al., 2014) under the same training/validation/testing splits to construct IQA triplets.",5.1 Dataset,[0],[0]
"Totally 614,163 IQA triplets are generated for 204,721 images.",5.1 Dataset,[0],[0]
"Each question has 18 candidate answers: in general 3 decoys are human-generated, 4 are randomly sampled, and 10 are randomly sampled frequent-occurring targets.",5.1 Dataset,[0],[0]
"As the test set does not indicate the targets, our studies focus on the training and validation sets.
",5.1 Dataset,[0],[0]
"Visual7W Telling (Visual7W) (Zhu et al., 2016)",5.1 Dataset,[0],[0]
"The dataset uses 47,300 images from MSCOCO (Lin et al., 2014) and contains 139,868 IQA triplets.",5.1 Dataset,[0],[0]
"Each has 3 decoys generated by humans.
",5.1 Dataset,[0],[0]
"Visual Genome (VG) (Krishna et al., 2017)",5.1 Dataset,[0],[0]
"The dataset uses 101,174 images from MSCOCO (Lin et al., 2014) and contains 1,445,322 IQT triplets.",5.1 Dataset,[0],[0]
No decoys are provided.,5.1 Dataset,[0],[0]
Human annotators are asked to write diverse pairs of questions and answers freely about an image or with respect to some regions of it.,5.1 Dataset,[0],[0]
On average an image is coupled with 14 question-answer pairs.,5.1 Dataset,[0],[0]
We divide the dataset into non-overlapping 50%/20%/30% for training/validation/testing.,5.1 Dataset,[0],[0]
"Additionally, we partition such that each portion is a “superset” of the corresponding one in Visual7W, respectively.
",5.1 Dataset,[0],[0]
"VQA2 (Goyal et al., 2017) and COCOQA (Ren et al., 2015)",5.1 Dataset,[0],[0]
"We describe the datasets and experimental results in the Supplementary Material.
",5.1 Dataset,[0],[0]
"Creating decoys We create 3 QoU-decoys and 3 IoU-decoys for every IQT triplet in each dataset, following the steps in Sect.",5.1 Dataset,[0],[0]
4.1.,5.1 Dataset,[0],[0]
"In the cases that we cannot find 3 decoys, we include random ones from the original set of decoys for VQA and Visual7W; for other datasets, we randomly include those from the top 10 frequently-occurring targets.",5.1 Dataset,[0],[0]
Visual QA models We utilize the MLP models mentioned in Sect.,5.2 Setup,[0],[0]
3 for all the experiments.,5.2 Setup,[0],[0]
"We denote MLP-A, MLP-QA, MLP-IA, MLPIQA",5.2 Setup,[0],[0]
"as the models using A (Answers only), Q+A (Question plus Answers), I+A (Image plus Answers), and I+Q+A (Image, Question and Answers) for multimodal information, respectively.",5.2 Setup,[0],[0]
"The hidden-layer has 8,192 neurons.",5.2 Setup,[0],[0]
"We use a 200-layer ResNet (He et al., 2016) to compute visual features which are 2,048-dimensional.",5.2 Setup,[0],[0]
"The ResNet is pre-trained on ImageNet (Russakovsky et al., 2015).",5.2 Setup,[0],[0]
"The WORD2VEC feature (Mikolov et al., 2013) for questions and answers are 300- dimensional, pre-trained on Google News2.",5.2 Setup,[0],[0]
"The
2We experiment on using different features in the Supplementary Material.
parameters of the MLP models are learned by minimizing the binary logistic loss of predicting whether or not a candidate answer is the target of the corresponding IQA triplet.",5.2 Setup,[0],[0]
"Please see the Supplementary Material for details on optimization.
",5.2 Setup,[0],[0]
"We further experiment with a variant of the spatial memory network (denoted as Attention) (Xu and Saenko, 2016) and the HieCoAtt model (Lu et al., 2016) adjusted for the multiple-choice setting.",5.2 Setup,[0],[0]
Both models utilize the attention mechanism.,5.2 Setup,[0],[0]
"Details are listed in the Supplementary Material.
",5.2 Setup,[0],[0]
"Evaluation metric For VQA and VQA2, we follow their protocols by comparing the picked answer to 10 human-generated targets.",5.2 Setup,[0],[0]
The accuracy is computed based on the number of exactly matched targets (divided by 3 and clipped at 1).,5.2 Setup,[0],[0]
"For others, we compute the accuracy of picking the target from multiple choices.
",5.2 Setup,[0],[0]
"Decoy sets to compare For each dataset, we derive several variants: (1) Orig: the original decoys from the datasets, (2)",5.2 Setup,[0],[0]
QoU:,5.2 Setup,[0],[0]
"Orig replaced with ones selected by our QoU-decoys generating procedure, (3) IoU:",5.2 Setup,[0],[0]
"Orig replaced with ones selected by our IoU-decoys generating procedure, (4) QoU +IoU:",5.2 Setup,[0],[0]
"Orig replaced with ones combining QoU and IoU, (5) All: combining Orig, QoU, and IoU.
User studies Automatic decoy generation may lead to ambiguous decoys as mentioned in Sect.",5.2 Setup,[0],[0]
"4 and (Zhu et al., 2016).",5.2 Setup,[0],[0]
We conduct a user study via Amazon Mechanic Turk (AMT) to test humans’ performance on the datasets after they are remedied by our automatic procedures.,5.2 Setup,[0],[0]
"We select 1,000 IQA triplets from each dataset.",5.2 Setup,[0],[0]
Each triplet is answered by three workers and in total 169 workers get involved.,5.2 Setup,[0],[0]
The total cost is $215 — the rate for every 20 triplets is $0.25.,5.2 Setup,[0],[0]
We report the average human performance and compare it to the learning models’.,5.2 Setup,[0],[0]
See the Supplementary Material for more details.,5.2 Setup,[0],[0]
"The performances of learning models and humans on the 3 datasets are reported in Table 3, 4, and 53.
",5.3 Results,[0],[0]
"3We note that in Table 3, the 4.3% drop of the human performance on IoU +QoU, compared to Orig, is likely due to that IoU",5.3 Results,[0],[0]
+QoU has more candidates (7 per question).,5.3 Results,[0],[0]
"Besides, the human performance on qaVG cannot be directly compared to that on the other datasets, since the questions on qaVG tend to focus on local image regions and are considered harder.
",5.3 Results,[0],[0]
"Effectiveness of new decoys A better set of decoys will force learning models to integrate all 3 pieces of information — images, questions and answers — to make the correct selection from multiple-choices.",5.3 Results,[0],[0]
"In particular, they should prevent learning algorithms from exploiting shortcuts such that partial information is sufficient for performing well on the Visual QA task.
",5.3 Results,[0],[0]
Table 3 clearly indicates that those goals have been achieved.,5.3 Results,[0],[0]
"With the Orig decoys, the relatively small gain from MLP-IA to MLP-IQA suggests that the question information can be ignored to attain good performance.",5.3 Results,[0],[0]
"However, with the IoU-decoys which require questions to help to resolve (as image itself is inadequate to resolve), the gain is substantial (from 27.3% to 84.1%).",5.3 Results,[0],[0]
"Likewise, with the QoU-decoys (question itself is not adequate to resolve), including images information improves from 40.7% (MLP-QA) substantially to 57.6% (MLP-IQA).",5.3 Results,[0],[0]
"Note that with the Orig decoys, this gain is smaller (58.2% vs. 65.7%).
",5.3 Results,[0],[0]
"It is expected that MLP-IA matches better QoUdecoys but not IoU-decoys, and MLP-QA is the other way around.",5.3 Results,[0],[0]
Thus it is natural to combine these two decoys.,5.3 Results,[0],[0]
What is particularly appealing is that MLP-IQA improves noticeably over models learned with partial information on the combined IoU,5.3 Results,[0],[0]
+QoU-decoys (and “All” decoys4).,5.3 Results,[0],[0]
"Furthermore, using answer information only (MLP-A) attains about the chance-level accuracy.
",5.3 Results,[0],[0]
"On the VQA dataset (Table 4), the same observations hold, though to a lesser degree.",5.3 Results,[0],[0]
"On any of the IoU or QoU columns, we observe substan-
4We note that the decoys in Orig are not trivial, which can be seen from the gap between All and IoU +QoU.",5.3 Results,[0],[0]
"Our main concern on Orig is that for those questions that machines can accurately answer, they mostly rely on only partial information.",5.3 Results,[0],[0]
This will thus hinder designing machines to fully comprehend and reason from multimodal information.,5.3 Results,[0],[0]
"We further experiment on random decoys, which can achieve Neutrality but not the other two principles, to demonstrate the effectiveness of our methods in the Supplementary Material.
tial gains when the complementary information is added to the model (such as MLP-IA to MLPIQA).",5.3 Results,[0],[0]
"All these improvements are much more visible than those observed on the original decoy sets.
",5.3 Results,[0],[0]
"Combining both Table 3 and 4, we notice that the improvements from MLP-QA to MLP-IQA tend to be lower when facing IoU-decoys.",5.3 Results,[0],[0]
This is also expected as it is difficult to have decoys that are simultaneously both IoU and QoU — such answers tend to be the target answers.,5.3 Results,[0],[0]
"Nonetheless, we deem this as a future direction to explore.
",5.3 Results,[0],[0]
"Differences across datasets Contrasting Visual7W to VQA (on the column IoU +QoU), we notice that Visual7W tends to have bigger improvements in general.",5.3 Results,[0],[0]
"This is due to the fact that VQA has many questions with “Yes” or “No” as the targets — the only valid decoy to the target “Yes” is “No”, and vice versa.",5.3 Results,[0],[0]
"As such decoys are already captured by Orig of VQA (‘Yes” and “No” are both top frequently-occurring targets), adding other decoy answers will not make any noticeable improvement.",5.3 Results,[0],[0]
"In Supplementary Material, however, we show that once we remove such questions/answers pairs, the degree of improvements increases substantially.
",5.3 Results,[0],[0]
"Comparison on Visual QA models As presented in Table 3 and 4, MLP-IQA is on par with or even outperforms Attention and HieCoAtt on the Orig decoys, showing how the shortcuts make
it difficult to compare different models.",5.3 Results,[0],[0]
"By eliminating the shortcuts (i.e., on the combined IoU +QoU-decoys), the advantage of using sophisticated models becomes obvious (Attention outperforms MLP-IQA by 3% in Table 4), indicating the importance to design advanced models for achieving human-level performance on Visual QA.
",5.3 Results,[0],[0]
"For completeness, we include the results on the Visual Genome dataset in Table 5.",5.3 Results,[0],[0]
"This dataset has no “Orig” decoys, and we have created a multiplechoice based dataset qaVG from it for the task — it has over 1 million triplets, the largest dataset on this task to our knowledge.",5.3 Results,[0],[0]
"On the combined IoU +QoU-decoys, we again clearly see that machines need to use all the information to succeed.
",5.3 Results,[0],[0]
"With qaVG, we also investigate whether it can help improve the multiple-choice performances on the other two datasets.",5.3 Results,[0],[0]
We use the MLP-IQA trained on qaVG with both IoU and QoU decoys to initialize the models for the Visual7W and VQA datasets.,5.3 Results,[0],[0]
"We report the accuracies before and after fine-tuning, together with the best results learned solely on those two datasets.",5.3 Results,[0],[0]
"As shown in Table 6, fine-tuning largely improves the performance, justifying the finding by Fukui et al. (2016).",5.3 Results,[0],[0]
"In Fig. 2, we present examples of image-questiontarget triplets from V7W, VQA, and VG, together with our IoU-decoys (A, B, C) and QoU-decoys (D, E, F).",5.4 Qualitative Results,[0],[0]
G is the target.,5.4 Qualitative Results,[0],[0]
The predictions by the corresponding MLP-IQA are also included.,5.4 Qualitative Results,[0],[0]
"Ignoring information from images or questions makes it extremely challenging to answer the triplet correctly, even for humans.
",5.4 Qualitative Results,[0],[0]
"Our automatic procedures do fail at some triplets, resulting in ambiguous decoys to the targets.",5.4 Qualitative Results,[0],[0]
See Fig. 3 for examples.,5.4 Qualitative Results,[0],[0]
"We categorized those failure cases into two situations.
",5.4 Qualitative Results,[0],[0]
• Our filtering steps in Sect.,5.4 Qualitative Results,[0],[0]
"4 fail, as observed in the top example.",5.4 Qualitative Results,[0],[0]
"The WUP-based similarity re-
lies on the WordNet hierarchy.",5.4 Qualitative Results,[0],[0]
"For some semantically similar words like “lady” and “woman”, the similarity is only 0.632, much lower than that of 0.857 between “cat” and “dog”.",5.4 Qualitative Results,[0],[0]
"This issue can be alleviated by considering alternative semantic measures by WORD2VEC or by those used in (Das et al., 2017; Ding et al., 2016) for searching similar questions.
",5.4 Qualitative Results,[0],[0]
• The question is ambiguous to answer.,5.4 Qualitative Results,[0],[0]
"In the bottom example in Fig. 3, both candidates D and F seem valid as a target.",5.4 Qualitative Results,[0],[0]
Another representative case is when asked about the background of a image.,5.4 Qualitative Results,[0],[0]
"In images that contain sky and mountains in the distance, both terms can be valid.",5.4 Qualitative Results,[0],[0]
We perform detailed analysis on existing datasets for multiple-choice Visual QA.,6 Conclusion,[0],[0]
We found that the design of decoys can inadvertently provide “shortcuts” for machines to exploit to perform well on the task.,6 Conclusion,[0],[0]
"We describe several principles of constructing good decoys and propose automatic pro-
cedures to remedy existing datasets and create new ones.",6 Conclusion,[0],[0]
We conduct extensive empirical studies to demonstrate the effectiveness of our methods in creating better Visual QA datasets.,6 Conclusion,[0],[0]
The remedied datasets and the newly created ones are released and available at http://www.teds. usc.edu/website_vqa/.,6 Conclusion,[0],[0]
"This work is partially supported by USC Graduate Fellowship, NSF IIS-1065243, 1451412, 1513966/1632803, 1208500, CCF-1139148, a Google Research Award, an Alfred.",Acknowledgments,[0],[0]
P. Sloan Research Fellowship and ARO# W911NF-12-1-0241 and W911NF-15-1-0484.,Acknowledgments,[0],[0]
"Visual question answering (Visual QA) has attracted a lot of attention lately, seen essentially as a form of (visual) Turing test that artificial intelligence should strive to achieve.",abstractText,[0],[0]
"In this paper, we study a crucial component of this task: how can we design good datasets for the task?",abstractText,[0],[0]
We focus on the design of multiplechoice based datasets where the learner has to select the right answer from a set of candidate ones including the target (i.e. the correct one) and the decoys (i.e. the incorrect ones).,abstractText,[0],[0]
"Through careful analysis of the results attained by state-of-the-art learning models and human annotators on existing datasets, we show that the design of the decoy answers has a significant impact on how and what the learning models learn from the datasets.",abstractText,[0],[0]
"In particular, the resulting learner can ignore the visual information, the question, or both while still doing well on the task.",abstractText,[0],[0]
"Inspired by this, we propose automatic procedures to remedy such design deficiencies.",abstractText,[0],[0]
"We apply the procedures to re-construct decoy answers for two popular Visual QA datasets as well as to create a new Visual QA dataset from the Visual Genome project, resulting in the largest dataset for this task.",abstractText,[0],[0]
Extensive empirical studies show that the design deficiencies have been alleviated in the remedied datasets and the performance on them is likely a more faithful indicator of the difference among learning models.,abstractText,[0],[0]
The datasets are released and publicly available via http://www.teds. usc.edu/website_vqa/.,abstractText,[0],[0]
Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets,title,[0],[0]
Robust statistics was founded in the seminal works of [Tuk60] and [Hub64].,1 Introduction,[0],[0]
"The overarching motto is that any model (especially a parametric one) is only approximately valid, and that any estimator designed for a particular distribution that is to be used in practice must also be stable in the presence of model misspecification.",1 Introduction,[0],[0]
"The standard setup is to assume that the samples we are given come from a nice distribution, but that an adversary has the power to arbitrarily corrupt a constant fraction of the observed data.",1 Introduction,[0],[0]
"After several decades of work, the robust statistics community has discovered a myriad of estimators that are provably robust.",1 Introduction,[0],[0]
"An important feature of this line of work is that it can tolerate a constant fraction of corruptions independent of the dimension and that there are estimators for both the location (e.g., the mean) and scale (e.g., the covariance).",1 Introduction,[0],[0]
"See [HR09] and [HRRS86] for further background.
",1 Introduction,[0],[0]
"It turns out that there are vast gaps in our understanding of robustness, when computational considerations are taken into account.",1 Introduction,[0],[0]
"In one dimension, robustness and computational efficiency are in perfect
∗A version of this paper appeared in ICML 2017",1 Introduction,[0],[0]
"[DKK+17]. †Supported by NSF CAREER Award CCF-1652862, a Sloan Research Fellowship, and a Google Faculty Research Award.",1 Introduction,[0],[0]
"‡Supported by NSF CCF-1551875, CCF-1617730, CCF-1650733, and ONR N00014-12-1-0999.",1 Introduction,[0],[0]
§Supported by NSF CAREER Award CCF-1553288 and a Sloan Research Fellowship.,1 Introduction,[0],[0]
"¶Supported by NSF CAREER Award CCF-1453261, a Google Faculty Research Award, and an NSF Fellowship.",1 Introduction,[0],[0]
"‖Supported by NSF CAREER Award CCF-1453261, a grant from the MIT NEC Corporation, and a Google Faculty Research
Award.",1 Introduction,[0],[0]
"∗∗Research supported by a USC startup grant.
",1 Introduction,[0],[0]
Authors are in alphabetical order.,1 Introduction,[0],[0]
"Code of our implementation is available at https://github.com/hoonose/robust-filter.
",1 Introduction,[0],[0]
"ar X
iv :1
70 3.
00 89
3v 4
[ cs
.L",1 Introduction,[0],[0]
"G
] 1
3 M
harmony.",1 Introduction,[0],[0]
"The empirical mean and empirical variance are not robust, because a single corruption can arbitrarily bias these estimates, but alternatives such as the median and the interquartile range are straightforward to compute and are provably robust.
",1 Introduction,[0],[0]
"But in high dimensions, there is a striking tension between robustness and computational efficiency.",1 Introduction,[0],[0]
Let us consider estimators for location.,1 Introduction,[0],[0]
The Tukey median [Tuk60] is a natural generalization of the onedimensional median to high-dimensions.,1 Introduction,[0],[0]
"It is known that it behaves well (i.e., it needs few samples) when estimating the mean for various symmetric distributions [DG92, CGR16].",1 Introduction,[0],[0]
"However, it is hard to compute in general [JP78, AK95] and the many heuristics for computing it degrade badly in the quality of their approximation as the dimension scales [CEM+93, Cha04, MS10].",1 Introduction,[0],[0]
The same issues plague estimators for scale.,1 Introduction,[0],[0]
"The minimum volume ellipsoid [Rou85] is a natural generalization of the one-dimensional interquartile range and is provably robust in high-dimensions, but is also hard to compute.",1 Introduction,[0],[0]
"And once again, heuristics for computing it [VAR09, RS98] work poorly in high dimensions.
",1 Introduction,[0],[0]
The fact that robustness in high dimensions seems to come at such a steep price has long been a point of consternation within robust statistics.,1 Introduction,[0],[0]
"In a 1997 retrospective on the development of robust statistics [Hub97], Huber laments:
“It is one thing to design a theoretical algorithm whose purpose is to prove [large fractions of corruptions can be tolerated] and quite another thing to design a practical version that can be used not merely on small, but also on medium sized regression problems, with a 2000 by 50 matrix or so.",1 Introduction,[0],[0]
"This last requirement would seem to exclude all of the recently proposed [techniques].”
",1 Introduction,[0],[0]
"The goal of this paper is to answer Huber’s call to action and design estimators for both the mean and covariance that are highly practical, provably robust, and work in high-dimensions.",1 Introduction,[0],[0]
"Such estimators make the promise of robust statistics – estimators that work in high-dimensions and guarantee that their output has not been heavily biased by some small set of noisy samples – much closer to a reality.
",1 Introduction,[0],[0]
"First, we make some remarks to dispel some common misconceptions.",1 Introduction,[0],[0]
"There has been a considerable amount of recent work on robust principal component analysis, much of it making use of semidefinite programming.",1 Introduction,[0],[0]
"Some of these works can tolerate a constant fraction of corruptions [CLMW11], however require that the locations of the corruptions are evenly spread throughout the dataset so that no individual sample is entirely corrupted.",1 Introduction,[0],[0]
"In contrast, the usual models in robust statistics are quite rigid in what they require and they do this for good reason.",1 Introduction,[0],[0]
"A common scenario that is used to motivate robust statistical methods is if two studies are mixed together, and one subpopulation does not fit the model.",1 Introduction,[0],[0]
"Then one wants estimators that work without assuming anything at all about these outliers.
",1 Introduction,[0],[0]
There have also been semidefinite programming methods proposed for robust principal component analysis with outliers [XCS10].,1 Introduction,[0],[0]
"These methods assume that the uncorrupted matrix is rank r and that the fraction of outliers is at most 1/r, which again degrades badly as the rank of the matrix increases.",1 Introduction,[0],[0]
"Moreover, any method that uses semidefinite programming will have difficulty scaling to the sizes of the problems we consider here.",1 Introduction,[0],[0]
For sake of comparison – even with state-of-the-art interior point methods – it is not currently feasible to solve the types of semidefinite programs that have been proposed when the matrices have dimension larger than a hundred.,1 Introduction,[0],[0]
Recent works in theoretical computer science have sought to circumvent the usual difficulties of designing efficient and robust algorithms by instead working in a generative model.,1.1 Robustness in a Generative Model,[0],[0]
"The starting point for our paper is the work of [DKK+16] who gave an efficient algorithm for the problem of agnostically learning a Gaussian:
Given a polynomial number of samples from a high-dimensional Gaussian N (µ,Σ), where an adversary has arbitrarily corrupted an ε-fraction, find a set of parameters N ′(µ̂, Σ̂) that satisfy dTV (N ,N ′) ≤",1.1 Robustness in a Generative Model,[0],[0]
"Õ(ε)∗.
Total variation distance is the natural metric to use to measure closeness of the parameters, since a (1− ε)-fraction of the observed samples came from a Gaussian.",1.1 Robustness in a Generative Model,[0],[0]
"[DKK+16] gave an algorithm for the above ∗We use the notation Õ(·) to hide factors which are polylogarithmic in the argument – in particular, we note that this bound does not depend on the dimension.
problem (note that the guarantees are dimension independent), whose running time and sample complexity are polynomial in the dimension d and 1/ε.",1.1 Robustness in a Generative Model,[0],[0]
"[LRV16] independently gave an algorithm for the unknown mean case that achieves dTV (N ,N ′) ≤ Õ(ε √
log d), and in the unknown covariance case achieves guarantees in a weaker metric that is not affine invariant.",1.1 Robustness in a Generative Model,[0],[0]
"A crucial feature is that both algorithms work even when the moments of the underlying distribution satisfy certain conditions, and thus are not necessarily brittle to the modeling assumption that the inliers come from a Gaussian distribution.
",1.1 Robustness in a Generative Model,[0],[0]
A more conceptual way to view such work is as a proof-of-concept that the Tukey median and minimum volume ellipsoid can be computed efficiently in a natural family of distributional models.,1.1 Robustness in a Generative Model,[0],[0]
"This follows because not only would these be good estimates for the mean and covariance in the above model, but in fact any estimates that are good must also be close to them.",1.1 Robustness in a Generative Model,[0],[0]
"Thus, these works fit into the emerging research direction of circumventing worst-case lower bounds by going beyond worst-case analysis.
",1.1 Robustness in a Generative Model,[0],[0]
"Since the dissemination of the aforementioned works [DKK+16, LRV16], there has been a flurry of research activity on computationally efficient robust estimation in a variety of high-dimensional settings [DKS16, DKS17, CSV17, DKK+17, Li17, DBS17, BDLS17, SCV18, DKK+18], including studying graphical distributional models [DKS16], understanding the computation-robustness tradeoff for statistical query algorithms [DKS17], tolerating much more noise by allowing the algorithm to output a list of candidate hypotheses [CSV17], and developing robust algorithms under sparsity assumptions",1.1 Robustness in a Generative Model,[0],[0]
"[Li17, DBS17, BDLS17], where the number of samples is sublinear in the dimension.",1.1 Robustness in a Generative Model,[0],[0]
Our goal in this work is to show that high-dimensional robust estimation can be highly practical.,1.2 Our Results,[0],[0]
"However, there are two major obstacles to achieving this.",1.2 Our Results,[0],[0]
"First, the sample complexity and running time of the algorithms in [DKK+16] is prohibitively large for high-dimensional applications.",1.2 Our Results,[0],[0]
"We just would not be able to store as many samples as we would need, in order to compute accurate estimates, in high-dimensional applications.
",1.2 Our Results,[0],[0]
Our first main contribution is to show essentially tight bounds on the sample complexity of the filtering based algorithm of [DKK+16].,1.2 Our Results,[0],[0]
"Roughly speaking, we accomplish this with a new definition of the good set which plugs into the existing analysis in a straightforward manner and shows that it is possible to estimate the mean with Õ(d/ε2) samples (when the covariance is known) and the covariance with Õ(d2/ε2) samples.",1.2 Our Results,[0],[0]
"Both of these bounds are information-theoretically optimal, up to logarithmic factors.
",1.2 Our Results,[0],[0]
Our second main contribution is to vastly improve the fraction of adversarial corruptions that can be tolerated in applications.,1.2 Our Results,[0],[0]
"The fraction of errors that the algorithms of [DKK+16] can tolerate is indeed a constant that is independent of the dimension, but it is very small both in theory and in practice.",1.2 Our Results,[0],[0]
This is due to the fact that many of the steps in the algorithm are overly conservative.,1.2 Our Results,[0],[0]
"In fact, we found that a naive implementation of the algorithm did not remove any outliers in many realistic scenarios.",1.2 Our Results,[0],[0]
We combat this by giving new ways to empirically tune the threshold for where to remove points from the sample set.,1.2 Our Results,[0],[0]
"These optimizations dramatically improve the empirical performance.
",1.2 Our Results,[0],[0]
"Finally, we show that the same bounds on the error guarantee continue to work even when the underlying distribution is sub-Gaussian.",1.2 Our Results,[0],[0]
This theoretically confirms that the robustness guarantees of such algorithms are in fact not overly brittle to the distributional assumptions.,1.2 Our Results,[0],[0]
"In fact, the filtering algorithm of [DKK+16] is easily shown to be robust under much weaker distributional assumptions, while retaining near-optimal sample and error guarantees.",1.2 Our Results,[0],[0]
"As an example, we show that it yields a near sample-optimal efficient estimator for robustly estimating the mean of a distribution, under the assumption that its covariance is bounded.",1.2 Our Results,[0],[0]
"Even in this regime, the filtering algorithm guarantees optimal error, up to a constant factor.",1.2 Our Results,[0],[0]
"Furthermore we empirically corroborate this finding by showing that the algorithm works well on real world data, as we describe below.
",1.2 Our Results,[0],[0]
Now we come to the task of testing out our algorithms.,1.2 Our Results,[0],[0]
"To the best of our knowledge, there have been no experimental evaluations of the performance of the myriad of approaches to robust estimation.",1.2 Our Results,[0],[0]
"It remains mostly a mystery which ones perform well in high-dimensions, and which do not.",1.2 Our Results,[0],[0]
"To test out our algorithms, we design a synthetic experiment where a (1 − ε)-fraction of the samples come from a Gaussian and the rest are noise and sampled from another distribution (in many cases, Bernoulli).",1.2 Our Results,[0],[0]
"This gives us a baseline to compare how well various algorithms recover µ and Σ, and how their performance degrades based on the dimension.",1.2 Our Results,[0],[0]
"Our plots show a predictable and yet striking phenomenon: All earlier approaches have error
rates that scale polynomially with the dimension and ours is a constant that is almost indistinguishable from the error that comes from sample noise alone.",1.2 Our Results,[0],[0]
"Moreover, our algorithms are able to scale to hundreds of dimensions.
",1.2 Our Results,[0],[0]
But are algorithms for agnostically learning a Gaussian unduly sensitive to the distributional assumptions they make?,1.2 Our Results,[0],[0]
We are able to give an intriguing visual demonstration of our techniques on real data.,1.2 Our Results,[0],[0]
The famous study of [NJB+08] showed that performing principal component analysis on a matrix of genetic data recovers a map of Europe.,1.2 Our Results,[0],[0]
"More precisely, the top two singular vectors define a projection into the plane and when the groups of individuals are color-coded with where they are from, we recover familiar country boundaries that corresponds to the map of Europe.",1.2 Our Results,[0],[0]
The conclusion from their study was that genes mirror geography.,1.2 Our Results,[0],[0]
"Given that one of the most important applications of robust estimation ought to be in exploratory data analysis, we ask: To what extent can we recover the map of Europe in the presence of noise?",1.2 Our Results,[0],[0]
"We show that when a small number of corrupted samples are added to the dataset, the picture becomes entirely distorted (and this continues to hold even for many other methods that have been proposed).",1.2 Our Results,[0],[0]
"In contrast, when we run our algorithm, we are able to once again recover the map of Europe.",1.2 Our Results,[0],[0]
"Thus, even when some fraction of the data has been corrupted (e.g., medical studies were pooled together even though the subpopulations studied were different), it is still possible to perform principal component analysis and recover qualitatively similar conclusions as if there were no noise at all!",1.2 Our Results,[0],[0]
Notation.,2 Formal Framework,[0],[0]
"For a vector v, we will let ‖v‖2 denote its Euclidean norm.",2 Formal Framework,[0],[0]
"If M is a matrix, we will let ‖M‖2 denote its spectral norm and ‖M‖F denote its Frobenius norm.",2 Formal Framework,[0],[0]
"We will write X ∈u S to denote that X is drawn from the empirical distribution defined by S.
Robust Estimation.",2 Formal Framework,[0],[0]
"We consider the following powerful model of robust estimation that generalizes many other existing models, including Huber’s contamination model:
Definition 2.1.",2 Formal Framework,[0],[0]
"Given ε > 0 and a distribution family D, the adversary operates as follows:",2 Formal Framework,[0],[0]
"The algorithm specifies some number of samples m. The adversary generates m samples X1, X2, . . .",2 Formal Framework,[0],[0]
",",2 Formal Framework,[0],[0]
Xm from some (unknown) D ∈ D.,2 Formal Framework,[0],[0]
It then draws m′ from an appropriate distribution.,2 Formal Framework,[0],[0]
"This distribution is allowed to depend on X1, X2, . . .",2 Formal Framework,[0],[0]
", Xm, but when marginalized over the m samples satisfies m
′",2 Formal Framework,[0],[0]
"∼ Bin(ε,m).",2 Formal Framework,[0],[0]
"The adversary is allowed to inspect the samples, removes m′ of them, and replaces them with arbitrary points.",2 Formal Framework,[0],[0]
"The set of m points is then given to the algorithm.
",2 Formal Framework,[0],[0]
"In summary, the adversary is allowed to inspect the samples before corrupting them, both by adding corrupted points and deleting uncorrupted points.",2 Formal Framework,[0],[0]
"In contrast, in Huber’s model the adversary is oblivious to the samples and is only allowed to add corrupted points.
",2 Formal Framework,[0],[0]
We remark that there are no computational restrictions on the adversary.,2 Formal Framework,[0],[0]
The goal is to return the parameters of a distribution D̂ in D that are close to the true parameters in an appropriate metric.,2 Formal Framework,[0],[0]
"For the case of the mean, our metric will be the Euclidean distance.",2 Formal Framework,[0],[0]
"For the covariance, we will use the Mahalanobis distance, i.e., ‖Σ−1/2Σ̂Σ−1/2",2 Formal Framework,[0],[0]
− I‖F .,2 Formal Framework,[0],[0]
"This is a strong affine invariant distance that implies corresponding bounds in total variation distance.
",2 Formal Framework,[0],[0]
"We will use the following terminology:
Definition 2.2.",2 Formal Framework,[0],[0]
We say that a set of samples is ε-corrupted if it is generated by the process described in Definition 2.1.,2 Formal Framework,[0],[0]
"In this section, we present near sample-optimal efficient robust estimators for the mean and the covariance of high-dimensional distributions under various structural assumptions of varying strength.",3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
"Our estimators rely on the filtering technique introduced in [DKK+16].
",3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
"We note that [DKK+16] gave two algorithmic techniques: the first one was a spectral technique to iteratively remove outliers from the dataset (filtering), and the second one was a soft-outlier removal method relying on convex programming.",3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
"The filtering technique seemed amenable to practical implementation (as
it only uses simple eigenvalue computations), but the corresponding sample complexity bounds given in [DKK+16] are polynomially worse than the information-theoretic minimum.",3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
"On the other hand, the convex programming technique of [DKK+16] achieved better sample complexity bounds (e.g., near sample-optimal for robust mean estimation), but relied on the ellipsoid method, which seemed to preclude a practically efficient implementation.
",3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
"In this work, we achieve the best of both worlds: we provide a more careful analysis of the filter technique that yields sample-optimal bounds (up to logarithmic factors) for both the mean and the covariance.",3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
"Moreover, we show that the filtering technique easily extends to much weaker distributional assumptions (e.g., under bounded second moments).",3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
"Roughly speaking, the filtering technique follows a general iterative recipe: (1) via spectral methods, find some univariate test which is violated by the corrupted points, (2) find some concrete tail bound violated by the corrupted set of points, and (3) throw away all points which violate this tail bound.
",3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
We start with sub-gaussian distributions.,3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
Recall that if P is sub-gaussian on Rd with mean vector µ and parameter ν,3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
"> 0,",3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
then for any unit vector v ∈,3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
Rd,3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
we have that PrX∼P,3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
"[|v · (X − µ)| ≥ t] ≤ exp(−t2/2ν).
",3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
Theorem 3.1.,3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
"Let G be a sub-gaussian distribution on Rd with parameter ν = Θ(1), mean µG, covariance matrix I, and ε > 0.",3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
Let S be an ε-corrupted set of samples from G of size Ω((d/ε2) poly log(d/ε)).,3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
"There exists an efficient algorithm that, on input S and ε > 0, returns a mean vector µ̂ so that with probability at least 9/10 we have ‖µ̂− µG‖2 = O(ε √ log(1/ε)).
",3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
[DKK+16] gave algorithms for robustly estimating the mean of a Gaussian distribution with known covariance and for robustly estimating the mean of a binary product distribution.,3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
The main motivation for considering these specific distribution families is that robustly estimating the mean within Euclidean distance immediately implies total variation distance bounds for these families.,3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
The above theorem establishes that these guarantees hold in a more general setting with near sample-optimal bounds.,3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
"Under a bounded second moment assumption, we show:
Theorem 3.2.",3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
Let P be a distribution on Rd with unknown mean vector µP and unknown covariance matrix ΣP σ2I. Let S be an ε-corrupted set of samples from P of size Θ((d/ε) log d).,3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
"There exists an efficient algorithm that, on input S and ε > 0, with probability 9/10 outputs µ̂ with ‖µ̂− µP ‖2 ≤ O( √ εσ).
",3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
A similar result on mean estimation under bounded second moments was concurrently shown in [SCV18].,3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
"The sample size above is optimal, up to a logarithmic factor, and the error guarantee is easily seen to be the best possible up to a constant factor.",3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
The main difference between the filtering algorithm establishing the above theorem and the filtering algorithm for the sub-gaussian case is how we choose the threshold for the filter.,3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
"Instead of looking for a violation of a concentration inequality, here we will choose a threshold at random.",3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
"In this case, randomly choosing a threshold weighted towards higher thresholds suffices to throw out more corrupted samples than uncorrupted samples in expectation.",3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
"Although it is possible to reject many good samples this way, we show that the algorithm still only rejects a total of O(ε) samples with high probability.
",3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
"Finally, for robustly estimating the covariance of a Gaussian distribution, we have:
Theorem 3.3.",3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
"Let G ∼ N (0,Σ) be a Gaussian in d dimensions, and let ε > 0.",3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
Let S be an ε-corrupted set of samples from G of size Ω((d2/ε2) poly log(d/ε)).,3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
"There exists an efficient algorithm that, given S and ε, returns the parameters of a Gaussian distribution G′ ∼ N (0, Σ̂) so that with probability at least 9/10, it holds ‖I − Σ−1/2Σ̂Σ−1/2‖F = O(ε log(1/ε)).
",3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
We now provide a high-level description of the main ingredient which yields these improved sample complexity bounds.,3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
"The initial analysis of [DKK+16] established sample complexity bounds which were suboptimal by polynomial factors because it insisted that the set of good samples (i.e., before the corruption) satisfied very tight tail bounds.",3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
"To some degree such bounds are necessary, as when we perform our filtering procedure, we need to ensure that not too many good samples are thrown away.",3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
"However, the old analysis required that fairly strong tail bounds hold uniformly.",3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
"The idea for the improvement is as follows: If the errors are sufficient to cause the variance of some polynomial p (linear in the unknown mean case or quadratic in the unknown covariance case) to increase by more than ε, it must be the case that for some T , roughly an ε/T 2 fraction of samples are error points with |p(x)| > T .",3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
"As long as we can ensure that less than an
ε/T 2 fraction of our good sample points have |p(x)| > T , this will suffice for our filtering procedure to work.",3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
"For small values of T , these are much weaker tail bounds than were needed previously and can be achieved with a smaller number of samples.",3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
"For large values of T , these tail bounds are comparable to those used in previous work [DKK+16] , but in such cases we can take advantage of the fact that |p(G)| > T only with very small probability, again allowing us to reduce the sample complexity.",3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
The details are deferred to Appendix A.,3 Nearly Sample-Optimal Efficient Robust Learning,[0],[0]
We now describe the filtering technique more rigorously.,4 Filtering,[0],[0]
We also describe some additional heuristics we found useful in practice.,4 Filtering,[0],[0]
We first consider mean estimation.,4.1 Robust Mean Estimation,[0],[0]
The algorithms which achieve Theorems 3.1 and 3.2 both follow the general recipe in Algorithm 1.,4.1 Robust Mean Estimation,[0],[0]
"We must specify three parameter functions:
• Thres(ε) is a threshold function—we terminate if the covariance has spectral norm bounded by Thres(ε).
",4.1 Robust Mean Estimation,[0],[0]
"• Tail(T, d, ε, δ, τ) is an univariate tail bound, which would only be violated by a τ fraction of points if they were uncorrupted, but is violated by many more of the current set of points.
",4.1 Robust Mean Estimation,[0],[0]
"• δ(ε, s) is a slack function, which we require for technical reasons.
",4.1 Robust Mean Estimation,[0],[0]
"Given these objects, our filter is fairly easy to state: first, we compute the empirical covariance.",4.1 Robust Mean Estimation,[0],[0]
"Then, we check if the spectral norm of the empirical covariance exceeds Thres(ε).",4.1 Robust Mean Estimation,[0],[0]
"If it does not, we output the empirical mean with the current set of data points.",4.1 Robust Mean Estimation,[0],[0]
"Otherwise, we project onto the top eigenvector of the empirical covariance, and throw away all points which violate Tail(T, d, ε, δ, τ), for some choice of slack function δ.
",4.1 Robust Mean Estimation,[0],[0]
"Algorithm 1 Filter-based algorithm template for robust mean estimation
1: Input: An ε-corrupted set of samples S, Thres(ε),Tail(T, d, ε, δ, τ), δ(ε, s) 2: Compute the sample mean µS ′",4.1 Robust Mean Estimation,[0],[0]
"= EX∈uS′ [X] 3: Compute the sample covariance matrix Σ 4: Compute approximations for the largest absolute eigenvalue of Σ, λ∗ := ‖Σ‖2, and the associated unit
eigenvector v∗. 5: if ‖Σ‖2 ≤ Thres(ε) then 6: return µS ′ .",4.1 Robust Mean Estimation,[0],[0]
"7: Let δ = δ(ε, ‖Σ‖2).",4.1 Robust Mean Estimation,[0],[0]
8: Find T > 0,4.1 Robust Mean Estimation,[0],[0]
"such that
Pr X∈uS′
[ |v∗ · (X − µS ′ )",4.1 Robust Mean Estimation,[0],[0]
| > T,4.1 Robust Mean Estimation,[0],[0]
+ δ,4.1 Robust Mean Estimation,[0],[0]
"] > Tail(T, d, ε, δ, τ).
9: return {x ∈ S′ : |v∗ · (x− µS′)| ≤ T + δ}.
",4.1 Robust Mean Estimation,[0],[0]
"Sub-gaussian case To concretely instantiate this algorithm for the subgaussian case, we take Thres(ε)",4.1 Robust Mean Estimation,[0],[0]
"= O(ε log 1/ε), δ(ε, s) = 3 √ ε(s− 1), and
Tail(T, d, ε, δ, τ) = 8 exp(−T 2/2ν)",4.1 Robust Mean Estimation,[0],[0]
"+ 8 ε T 2 log(d log(d/ετ)) ,
where ν is the subgaussian parameter.",4.1 Robust Mean Estimation,[0],[0]
"See Section A.1 for details.
",4.1 Robust Mean Estimation,[0],[0]
"Second moment case To concretely instantiate this algorithm for the second moment case, we take Thres(ε)",4.1 Robust Mean Estimation,[0],[0]
"= 9,",4.1 Robust Mean Estimation,[0],[0]
"δ = 0, and we take Tail to be a random rescaling of the largest deviation in the data set, in the direction v∗.",4.1 Robust Mean Estimation,[0],[0]
See Section A.2 for details.,4.1 Robust Mean Estimation,[0],[0]
"Our algorithm for robust covariance follows the exact recipe outlined above, with one key difference—we check for deviations in the empirical fourth moment tensor.",4.2 Robust Covariance Estimation,[0],[0]
"Intuitively, just as in the robust mean setting, we used degree-2 information to detect outliers for the mean (the degree-1 moment), here we use degree-4 information to detect outliers for the covariance (the degree-2 moment).
",4.2 Robust Covariance Estimation,[0],[0]
"More concretely, this corresponds to finding a normalized degree-2 polynomial whose empirical variance is too large.",4.2 Robust Covariance Estimation,[0],[0]
"By then filtering along this polynomial, with an appropriate choice of Thres(ε), δ(ε, s), and Tail, we achieve the desired bounds.",4.2 Robust Covariance Estimation,[0],[0]
See Section A.3 for the formal pseudocode and more details.,4.2 Robust Covariance Estimation,[0],[0]
"In the algorithms described above for robust mean estimation, after projecting onto one dimension, we center the points at the empirical mean along this direction.",4.3 Better Univariate Tests,[0],[0]
"This is theoretically sufficient, however, introduces additional constant factors since the empirical mean along this direction may be corrupted.",4.3 Better Univariate Tests,[0],[0]
"Instead, one can use a robust estimate for the mean in one direction.",4.3 Better Univariate Tests,[0],[0]
"Namely, it is well known that the median is a provably robust estimator for the mean for symmetric distributions [HR09, HRRS86], and under certain models it is in fact optimal in terms of its resilience to noise [DKW56, Mas90, Che98, DK14, DKK+17].",4.3 Better Univariate Tests,[0],[0]
"By centering the points at the median instead of the mean, we are able to achieve better error in practice.",4.3 Better Univariate Tests,[0],[0]
"In our empirical evaluation, we found that it was important to find an appropriate choice of Tail, to achieve good error rates, especially for robust covariance estimation.",4.4 Adaptive Tail Bounding,[0],[0]
"Concretely, in this setting, our tail bound is given by
Tail(T, d, ε, δ, τ) = C1 exp(−C2T ) +",4.4 Adaptive Tail Bounding,[0],[0]
"Tail2(T, d, ε, δ, τ) ,
for some function Tail2, and constants C1, C2.",4.4 Adaptive Tail Bounding,[0],[0]
"We found that for reasonable settings, the term that dominated was always the first term on the RHS, and that Tail2 is less significant.",4.4 Adaptive Tail Bounding,[0],[0]
"Thus, we focused on optimizing the first term.
",4.4 Adaptive Tail Bounding,[0],[0]
"We found that depending on the setting, it was useful to change the constant C2.",4.4 Adaptive Tail Bounding,[0],[0]
"In particular, in low dimensions, we could be more stringent, and enforce a stronger tail bound (which corresponds to a higher C2), but in higher dimensions, we must be more lax with the tail bound.",4.4 Adaptive Tail Bounding,[0],[0]
"To do this in a principled manner, we introduced a heuristic we call adaptive tail bounding.",4.4 Adaptive Tail Bounding,[0],[0]
Our goal is to find a choice of C2 which throws away roughly an ε-fraction of points.,4.4 Adaptive Tail Bounding,[0],[0]
The heuristic is fairly simple: we start with some initial guess for C2.,4.4 Adaptive Tail Bounding,[0],[0]
We then run our filter with this C2.,4.4 Adaptive Tail Bounding,[0],[0]
"If we throw away too many data points, we increase our C2, and retry.",4.4 Adaptive Tail Bounding,[0],[0]
"If we throw away too few, then we decrease our C2 and retry.",4.4 Adaptive Tail Bounding,[0],[0]
"Since increasing C2 strictly decreases the number of points thrown away, and vice versa, we binary search over our choice of C2 until we reach something close to our target accuracy.",4.4 Adaptive Tail Bounding,[0],[0]
"In our current implementation, we stop when the fraction of points we throw away is between ε/2 and 3ε/2, or if we’ve binary searched for too long.",4.4 Adaptive Tail Bounding,[0],[0]
"We found that this heuristic drastically improves our accuracy, and allows our algorithm to scale fairly smoothly from low to high dimension.",4.4 Adaptive Tail Bounding,[0],[0]
We performed an empirical evaluation of the above algorithms on synthetic and real data sets with and without synthetic noise.,5 Experiments,[0],[0]
All experiments were done on a laptop computer with a 2.7 GHz Intel Core i5 CPU and 8 GB of RAM.,5 Experiments,[0],[0]
"The focus of this evaluation was on statistical accuracy, not time efficiency.",5 Experiments,[0],[0]
"In this measure, our algorithm performs the best of all algorithms we tried.",5 Experiments,[0],[0]
"In all synthetic trials, our algorithm consistently had the smallest error.",5 Experiments,[0],[0]
"In fact, in some of the synthetic benchmarks, our error was orders of magnitude better than any other algorithms.",5 Experiments,[0],[0]
"In the semi-synthetic benchmark, our algorithm also (arguably) performs the best, though there is no way to tell for sure, since there is no ground truth.",5 Experiments,[0],[0]
"We also note that despite not optimizing our code for runtime, the runtime of our algorithm is always comparable, and in many cases, better than the alternatives which provided comparable error.",5 Experiments,[0],[0]
Code of our implementation is available at https://github.com/hoonose/robust-filter.,5 Experiments,[0],[0]
Experiments with synthetic data allow us to verify the error guarantees and the sample complexity rates proven in Section 3 for unknown mean and unknown covariance.,5.1 Synthetic Data,[0],[0]
"In both cases, the experiments validate the accuracy and usefulness of our algorithm, almost exactly matching the best rate without noise.
",5.1 Synthetic Data,[0],[0]
Unknown mean The results of our synthetic mean experiment are shown in Figure 1.,5.1 Synthetic Data,[0],[0]
"In the synthetic mean experiment, we set ε = 0.1, and for dimension d =",5.1 Synthetic Data,[0],[0]
"[100, 150, . .",5.1 Synthetic Data,[0],[0]
.,5.1 Synthetic Data,[0],[0]
", 400], we generate n = 10dε2 samples, where a (1 − ε)-fraction come from N (µ, I), and an ε fraction come from a noise distribution.",5.1 Synthetic Data,[0],[0]
Our goal is to produce an estimator which minimizes the `2 error the estimator has to the truth.,5.1 Synthetic Data,[0],[0]
"As a baseline, we compute the error that is achieved by only the uncorrupted sample points.",5.1 Synthetic Data,[0],[0]
"This error will be used as the gold standard for comparison, since in the presence of error, this is roughly the best one could do even if all the noise points were identified exactly.†
On this data, we compared the performance of our Filter algorithm to that of (1) the empirical mean of all the points, (2) a trivial pruning procedure, (3) the geometric median of the data, (4) a RANSAC-based mean estimation algorithm, and (5) a recently proposed robust estimator for the mean due to [LRV16], which we will call LRVMean.",5.1 Synthetic Data,[0],[0]
"For (5), we use the implementation available in their Github.‡ In Figure 1, the x-axis indicates the dimension of the experiment, and the y-axis measures the `2 error of our estimated mean minus the `2 error of the empirical mean of the true samples from the Gaussian, i.e., the excess error induced over the sampling error.
",5.1 Synthetic Data,[0],[0]
"We tried various noise distributions, and found that the same qualitative pattern arose for all of them.",5.1 Synthetic Data,[0],[0]
"In the reported experiment, our noise distribution was a mixture of two binary product distributions, where one had a couple of large coordinates (see Section B.1 for a detailed description).",5.1 Synthetic Data,[0],[0]
"For all (nontrivial) error distributions we tried, we observed that indeed the empirical mean, pruning, geometric median, and RANSAC all have error which diverges as d grows, as the theory predicts.",5.1 Synthetic Data,[0],[0]
"On the other hand, both our algorithm and LRVMean have markedly smaller error as a function of dimension.",5.1 Synthetic Data,[0],[0]
"Indeed, our algorithm’s error is almost identical to that of the empirical mean of the uncorrupted sample points.
",5.1 Synthetic Data,[0],[0]
Unknown covariance The results of our synthetic covariance experiment are shown in Figure 2.,5.1 Synthetic Data,[0],[0]
Our setup is similar to that for the synthetic mean.,5.1 Synthetic Data,[0],[0]
"Since both our algorithm and LRVCov require access to fourth moment objects, we ran into issues with limited memory on machines.",5.1 Synthetic Data,[0],[0]
"Thus, we could not perform experiments at as high a dimension as for the unknown mean setting, and we could not use as many samples.",5.1 Synthetic Data,[0],[0]
"We set ε = 0.05, and for dimension d =",5.1 Synthetic Data,[0],[0]
"[10, 20, . .",5.1 Synthetic Data,[0],[0]
.,5.1 Synthetic Data,[0],[0]
", 100], we generate n = 0.5dε2 samples, where a (1 − ε)-fraction come from N (0,Σ), and an ε fraction come from a noise distribution.",5.1 Synthetic Data,[0],[0]
"We measure distance in the natural affine invariant way, namely, the Mahalanobis distance induced by Σ to the identity: err(Σ̂) = ‖Σ−1/2Σ̂Σ−1/2−",5.1 Synthetic Data,[0],[0]
I‖F .,5.1 Synthetic Data,[0],[0]
"As explained above, this is the right affine-invariant metric for this problem.",5.1 Synthetic Data,[0],[0]
"As before, we use the empirical error of only the uncorrupted data points as a benchmark.
",5.1 Synthetic Data,[0],[0]
"On this corrupted data, we compared the performance of our Filter algorithm to that of (1) the empirical covariance of all the points, (2) a trivial pruning procedure, (3) a RANSAC-based minimal volume ellipsoid (MVE) algorithm, and (5) a recently proposed robust estimator for the covariance due to [LRV16], which we will call LRVCov.",5.1 Synthetic Data,[0],[0]
"For (5), we again obtained the implementation from their Github repository.
",5.1 Synthetic Data,[0],[0]
We tried various choices of Σ and noise distribution.,5.1 Synthetic Data,[0],[0]
Figure 2 shows two choices of Σ and noise.,5.1 Synthetic Data,[0],[0]
"Again, the x-axis indicates the dimension of the experiment and the y-axis indicates the estimator’s excess Mahalanobis error over the sampling error.",5.1 Synthetic Data,[0],[0]
"In the left figure, we set Σ = I, and our noise points are simply all located at the all-zeros vector.",5.1 Synthetic Data,[0],[0]
"In the right figure, we set Σ = I + 10e1e T 1 , where e1 is the first basis vector, and our noise distribution is a somewhat more complicated distribution, which is similarly spiked, but in a different, random, direction.",5.1 Synthetic Data,[0],[0]
We formally define this distribution in Section B.1.,5.1 Synthetic Data,[0],[0]
"For all choices of Σ and noise we tried, the qualitative behavior of our algorithm and LRVCov was unchanged.",5.1 Synthetic Data,[0],[0]
"Namely, we seem to match the empirical error without noise up to a very small slack, for all dimensions.",5.1 Synthetic Data,[0],[0]
"On the other hand, the performance of empirical mean, pruning, and RANSAC varies widely with the noise distribution.",5.1 Synthetic Data,[0],[0]
"The performance of all these algorithms degrades substantially with dimension, and their error gets worse as we increase the skew of the underlying data.",5.1 Synthetic Data,[0],[0]
"The performance of LRVCov is the most similar to ours, but again is worse by a large
†We note that it is possible that an estimator may achieve slightly better error than this baseline.",5.1 Synthetic Data,[0],[0]
"‡https://github.com/kal2000/AgnosticMean\AndCovarianceCode
constant factor.",5.1 Synthetic Data,[0],[0]
"In particular, our excess risk was on the order of 10−4 for large d, for both experiments, whereas the excess risk achieved by LRVCov was in all cases a constant between 0.1 and 2.
",5.1 Synthetic Data,[0],[0]
Discussion These experiments demonstrate that our statistical guarantees are in fact quite strong.,5.1 Synthetic Data,[0],[0]
"In particular, since our excess error is almost zero (and orders of magnitude smaller than other approaches), this suggests that our sample complexity is indeed close to optimal, since we match the rate without noise, and that the constants and logarithmic factors in the theoretical recovery guarantee are often small or non-existent.",5.1 Synthetic Data,[0],[0]
"To demonstrate the efficacy of our method on real data, we revisit the famous study of [NJB+08].",5.2 Semi-synthetic Data,[0],[0]
"In this study, the authors investigated data collected as part of the Population Reference Sample (POPRES) project.",5.2 Semi-synthetic Data,[0],[0]
This dataset consists of the genotyping of thousands of individuals using the Affymetrix 500K single nucleotide polymorphism (SNP) chip.,5.2 Semi-synthetic Data,[0],[0]
"The authors pruned the dataset to obtain the genetic data of over 1387 European individuals, annotated by their country of origin.",5.2 Semi-synthetic Data,[0],[0]
"Using principal components analysis, they produce a two-dimensional summary of the genetic variation, which bears a striking resemblance to the map of Europe.
",5.2 Semi-synthetic Data,[0],[0]
Our experimental setup is as follows.,5.2 Semi-synthetic Data,[0],[0]
"While the original dataset is very high dimensional, we use a 20 dimensional version of the dataset as found in the authors’ GitHub§.",5.2 Semi-synthetic Data,[0],[0]
"We first randomly rotate the data, as then 20 dimensional data was diagonalized, and the high dimensional data does not follow such structure.",5.2 Semi-synthetic Data,[0],[0]
We then add an additional ε1−ε fraction of points (so that they make up an ε-fraction of the final points).,5.2 Semi-synthetic Data,[0],[0]
"These added points were discrete points, following a simple product distribution (see Section B.1 for full details).",5.2 Semi-synthetic Data,[0],[0]
"We used a number of methods to obtain a covariance matrix for this dataset, and we projected the data onto the top two singular vectors of this matrix.",5.2 Semi-synthetic Data,[0],[0]
"In Figure 3, we show the results when we compare our techniques to pruning.",5.2 Semi-synthetic Data,[0],[0]
"In particular, our output was able to more or less reproduce the map of Europe, whereas pruning fails to.",5.2 Semi-synthetic Data,[0],[0]
"In Section B.2, we also compare our result with a number of other techniques, including those we tested against in the unknown covariance experiments, and other robust PCA techniques.",5.2 Semi-synthetic Data,[0],[0]
"The only alternative algorithm which was able to produce meaningful output was LRVCov, which produced output that was similar to ours, but which produced a map which was somewhat more skewed.",5.2 Semi-synthetic Data,[0],[0]
"We believe that our algorithm produces the best picture.
",5.2 Semi-synthetic Data,[0],[0]
"In Figure 3, we also display the actual points which were output by our algorithm’s Filter.",5.2 Semi-synthetic Data,[0],[0]
"While it manages to remove most of the noise points, it also seems to remove some of the true data points, particularly those from Eastern Europe and Turkey.",5.2 Semi-synthetic Data,[0],[0]
"We attribute this to a lack of samples from these regions, and thus one could consider them as outliers to a dataset consisting of Western European individuals.",5.2 Semi-synthetic Data,[0],[0]
"For instance, Turkey had 4 data points, so it seems quite reasonable that any robust algorithm would naturally consider these points outliers.
",5.2 Semi-synthetic Data,[0],[0]
"Discussion We view our experiments as a proof of concept demonstration that our techniques can be useful in real world exploratory data analysis tasks, particularly those in high-dimensions.",5.2 Semi-synthetic Data,[0],[0]
"Our experiments reveal that a minimal amount of noise can completely disrupt a data analyst’s ability to notice an interesting phenomenon, thus limiting us to only very well-curated data sets.",5.2 Semi-synthetic Data,[0],[0]
"But with robust methods, this noise does not interfere with scientific discovery, and we can still recover interesting patterns which otherwise would have been obscured by noise.",5.2 Semi-synthetic Data,[0],[0]
We would like to thank Simon Du and Lili Su for helpful comments on a previous version of this work.,Acknowledgments,[0],[0]
"A.1 Robust Mean Estimation for Sub-Gaussian Distributions
In this section, we use our filter technique to give a near sample-optimal computationally efficient algorithm to robustly estimate the mean of a sub-gaussian density with a known covariance matrix, thus proving Theorem 3.1.
",A Omitted Details from Section 3,[0],[0]
"We emphasize that the algorithm and its analysis is essentially identical to the filtering algorithm given in Section 8.1 of [DKK+16] for the case of a Gaussian N (µ, I).",A Omitted Details from Section 3,[0],[0]
The only difference is a weaker definition of the “good set of samples” (Definition A.4) and a simple concentration argument (Lemma A.5) showing that a random set of uncorrupted samples of the appropriate size is good with high probability.,A Omitted Details from Section 3,[0],[0]
"Given these, the analysis of this subsection follows straightforwardly from the analysis in Section 8.1 of [DKK+16] by plugging in the modified parameters.",A Omitted Details from Section 3,[0],[0]
"For the sake of completeness, we provide the details below.
",A Omitted Details from Section 3,[0],[0]
"We start by formally defining sub-gaussian distributions:
Definition A.1.",A Omitted Details from Section 3,[0],[0]
"A distribution P on R with mean µ, is sub-gaussian with parameter ν",A Omitted Details from Section 3,[0],[0]
> 0,A Omitted Details from Section 3,[0],[0]
"if
EX∼P [exp(λ(X − µ))] ≤ exp(νλ2/2)
for all λ ∈ R.",A Omitted Details from Section 3,[0],[0]
A distribution P on Rd with mean vector µ is sub-gaussian with parameter ν,A Omitted Details from Section 3,[0],[0]
"> 0, if for all unit vectors v, the one-dimensional random variable v ·X, X ∼ P , is sub-gaussian with parameter ν.
",A Omitted Details from Section 3,[0],[0]
"We will use the following simple fact about the concentration of sub-gaussian random variables:
Fact A.2.",A Omitted Details from Section 3,[0],[0]
If P is sub-gaussian on Rd with mean vector µ and parameter ν,A Omitted Details from Section 3,[0],[0]
"> 0,",A Omitted Details from Section 3,[0],[0]
then for any unit vector v ∈,A Omitted Details from Section 3,[0],[0]
Rd,A Omitted Details from Section 3,[0],[0]
we have that PrX∼P,A Omitted Details from Section 3,[0],[0]
[|v · (X − µ)| ≥,A Omitted Details from Section 3,[0],[0]
T ] ≤,A Omitted Details from Section 3,[0],[0]
"exp(−T 2/2ν).
",A Omitted Details from Section 3,[0],[0]
"The following theorem is a high probability version of Theorem 3.1:
Theorem A.3.",A Omitted Details from Section 3,[0],[0]
"Let G be a sub-gaussian distribution on Rd with parameter ν = Θ(1), mean µG, covariance matrix I, and ε, τ > 0.",A Omitted Details from Section 3,[0],[0]
Let S′ be an ε-corrupted set of samples from G of size Ω((d/ε2) poly log(d/ετ)).,A Omitted Details from Section 3,[0],[0]
"There exists an efficient algorithm that, on input S′ and ε > 0, returns a mean vector µ̂ so that with probability at least 1− τ",A Omitted Details from Section 3,[0],[0]
we have ‖µ̂− µG‖2 = O(ε √ log(1/ε)).,A Omitted Details from Section 3,[0],[0]
Notation.,A Omitted Details from Section 3,[0],[0]
We will denote µS =,A Omitted Details from Section 3,[0],[0]
"1|S| ∑ X∈S X and MS = 1 |S| ∑ X∈S(X−µG)(X−µG)T for the sample mean and modified sample covariance matrix of the set S.
We start by defining our modified notion of good sample, i.e, a set of conditions on the uncorrupted set of samples under which our algorithm will succeed.
",A Omitted Details from Section 3,[0],[0]
Definition A.4.,A Omitted Details from Section 3,[0],[0]
"Let G be an identity covariance sub-gaussian in d dimensions with mean µG and covariance matrix I and ε, τ > 0.",A Omitted Details from Section 3,[0],[0]
"We say that a multiset S of elements in Rd is (ε, τ)-good with respect to G if the following conditions are satisfied:
(i)",A Omitted Details from Section 3,[0],[0]
For all x ∈ S we have ‖x− µG‖2 ≤,A Omitted Details from Section 3,[0],[0]
"O( √ d log(|S|/τ)).
",A Omitted Details from Section 3,[0],[0]
(ii) For every affine function L : Rd → R such that L(x) = v · (x − µG),A Omitted Details from Section 3,[0],[0]
"− T , ‖v‖2 = 1, we have that |PrX∈uS",A Omitted Details from Section 3,[0],[0]
[L(X) ≥ 0]− PrX∼G[L(X) ≥ 0]| ≤,A Omitted Details from Section 3,[0],[0]
εT 2,A Omitted Details from Section 3,[0],[0]
"log(d log( dετ )) .
",A Omitted Details from Section 3,[0],[0]
"(iii) We have that ‖µS − µG‖2 ≤ ε.
(iv) We have that ‖MS − I‖2 ≤ ε.
",A Omitted Details from Section 3,[0],[0]
"We show in the following subsection that a sufficiently large set of independent samples from G is (ε, τ)good",A Omitted Details from Section 3,[0],[0]
(with respect to G) with high probability.,A Omitted Details from Section 3,[0],[0]
"Specifically, we prove:
Lemma A.5.",A Omitted Details from Section 3,[0],[0]
"Let G be sub-gaussian distribution with parameter ν = Θ(1) and with identity covariance, and ε, τ > 0.",A Omitted Details from Section 3,[0],[0]
If the multiset S is obtained by taking Ω((d/ε2) poly log(d/ετ)),A Omitted Details from Section 3,[0],[0]
"independent samples from G, it is (ε, τ)-good with respect to G with probability at least 1− τ.
",A Omitted Details from Section 3,[0],[0]
"We require the following definition that quantifies the extent to which a multiset has been corrupted:
Definition A.6.",A Omitted Details from Section 3,[0],[0]
"Given finite multisets S and S′ we let ∆(S, S′) be the size of the symmetric difference of S and S′ divided by the cardinality of S.
The starting point of our algorithm will be a simple NaivePrune routine (Section 4.3.1 of [DKK+16]) that removes obvious outliers, i.e., points which are far from the mean.",A Omitted Details from Section 3,[0],[0]
"Then, we iterate the algorithm whose performance guarantee is given by the following:
Proposition A.7.",A Omitted Details from Section 3,[0],[0]
"Let G be a sub-gaussian distribution on Rd with parameter ν = Θ(1), mean µG, covariance matrix I, ε > 0 be sufficiently small and τ > 0.",A Omitted Details from Section 3,[0],[0]
"Let S be an (ε, τ)-good set with respect to G. Let S′ be any multiset with ∆(S, S′) ≤ 2ε and for any x, y ∈ S′, ‖x",A Omitted Details from Section 3,[0],[0]
− y‖2 ≤,A Omitted Details from Section 3,[0],[0]
O( √ d log(d/ετ)).,A Omitted Details from Section 3,[0],[0]
"There exists a polynomial time algorithm Filter-Sub-Gaussian-Unknown-Mean that, given S′ and ε > 0, returns one of the following:
(i) A mean vector µ̂ such that ‖µ̂− µG‖2 = O(ε √ log(1/ε)).
",A Omitted Details from Section 3,[0],[0]
"(ii) A multiset S′′ ⊆ S′ such that ∆(S, S′′) ≤ ∆(S, S′)− ε/α, where α def= d log(d/ετ) log(d log( dετ )).
",A Omitted Details from Section 3,[0],[0]
"We start by showing how Theorem A.3 follows easily from Proposition A.7.
",A Omitted Details from Section 3,[0],[0]
Proof of Theorem A.3.,A Omitted Details from Section 3,[0],[0]
"By the definition of ∆(S, S′), since S′ has been obtained from S by corrupting an ε-fraction of the points in S, we have that ∆(S, S′) ≤ 2ε.",A Omitted Details from Section 3,[0],[0]
"By Lemma A.5, the set S of uncorrupted samples is (ε, τ)-good with respect to G with probability at least 1− τ.",A Omitted Details from Section 3,[0],[0]
"We henceforth condition on this event.
",A Omitted Details from Section 3,[0],[0]
"Since S is (ε, τ)-good, all x ∈ S have ‖x− µG‖2 ≤",A Omitted Details from Section 3,[0],[0]
O( √ d log |S|/τ).,A Omitted Details from Section 3,[0],[0]
"Thus, the NaivePrune procedure does not remove from S′ any member of S. Hence, its output, S′′, has ∆(S, S′′) ≤ ∆(S, S′) and for any x ∈ S′′, there is a y ∈ S with ‖x",A Omitted Details from Section 3,[0],[0]
− y‖2 ≤,A Omitted Details from Section 3,[0],[0]
O( √ d log |S|/τ).,A Omitted Details from Section 3,[0],[0]
"By the triangle inequality, for any x, z ∈ S′′,
‖x− z‖2 ≤ O( √ d log |S|/τ)",A Omitted Details from Section 3,[0],[0]
"= O( √ d log(d/ετ)).
",A Omitted Details from Section 3,[0],[0]
"Then, we iteratively apply the Filter-Sub-Gaussian-Unknown-Mean procedure of Proposition A.7 until it terminates returning a mean vector µ with ‖µ̂− µG‖2 = O(ε √ log(1/ε)).",A Omitted Details from Section 3,[0],[0]
We claim that we need at most O(α) iterations for this to happen.,A Omitted Details from Section 3,[0],[0]
"Indeed, the sequence of iterations results in a sequence of sets S′i, so that ∆(S, S′i) ≤ ∆(S, S′)− i · ε/α.",A Omitted Details from Section 3,[0],[0]
"Thus, if we do not output the empirical mean in the first 2α iterations, in the next iteration there are no outliers left and the algorithm terminates outputting the sample mean of the remaining set.
",A Omitted Details from Section 3,[0],[0]
"A.1.1 Algorithm Filter-Sub-Gaussian-Unknown-Mean: Proof of Proposition A.7
In this subsection, we describe the efficient algorithm establishing Proposition A.7 and prove its correctness.",A Omitted Details from Section 3,[0],[0]
Our algorithm calculates the empirical mean vector µS ′,A Omitted Details from Section 3,[0],[0]
and,A Omitted Details from Section 3,[0],[0]
"empirical covariance matrix Σ. If the matrix Σ has no large eigenvalues, it returns µS ′ .",A Omitted Details from Section 3,[0],[0]
"Otherwise, it uses the eigenvector v∗ corresponding to the maximum magnitude eigenvalue of Σ and the mean vector µS ′
to define a filter.",A Omitted Details from Section 3,[0],[0]
"Our efficient filtering procedure is presented in detailed pseudocode below.
",A Omitted Details from Section 3,[0],[0]
"A.1.2 Proof of Correctness of Filter-Sub-Gaussian-Unknown-Mean
By definition, there exist disjoint multisets L,E, of points in Rd, where L ⊂ S, such that S′ = (S \ L) ∪ E. With this notation, we can write ∆(S, S′) = |L|+|E||S| .",A Omitted Details from Section 3,[0],[0]
"Our assumption ∆(S, S
′) ≤",A Omitted Details from Section 3,[0],[0]
"2ε is equivalent to |L|+|E| ≤ 2ε · |S|, and the definition of S′ directly implies that (1− 2ε)|S| ≤ |S′| ≤ (1 + 2ε)|S|.",A Omitted Details from Section 3,[0],[0]
"Throughout the proof, we assume that ε is a sufficiently small constant.
",A Omitted Details from Section 3,[0],[0]
"We define µG, µS , µS ′ , µL, and µE to be the means of G,S, S′, L, and E, respectively.",A Omitted Details from Section 3,[0],[0]
"Our analysis will make essential use of the following matrices:
• MS′ denotes EX∈uS′",A Omitted Details from Section 3,[0],[0]
[(X − µG)(X,A Omitted Details from Section 3,[0],[0]
"− µG)T ],
• MS denotes EX∈uS",A Omitted Details from Section 3,[0],[0]
[(X − µG)(X,A Omitted Details from Section 3,[0],[0]
"− µG)T ],
• ML denotes EX∈uL[(X − µG)(X",A Omitted Details from Section 3,[0],[0]
"− µG)T ], and
Algorithm 2 Filter algorithm for a sub-gaussian with unknown mean and identity covariance
1: procedure Filter-Sub-Gaussian-Unknown-Mean(S′, ε, τ) input: A multiset S′ such that there exists an (ε, τ)-good S with ∆(S, S′) ≤",A Omitted Details from Section 3,[0],[0]
2ε output: Multiset S′′ or mean vector µ̂ satisfying Proposition A.7 2: Compute the sample mean µS ′,A Omitted Details from Section 3,[0],[0]
"= EX∈uS′ [X] and the sample covariance matrix Σ , i.e., Σ =
(Σi,j)1≤i,j≤d with Σi,j = EX∈uS′ [(Xi − µS ′",A Omitted Details from Section 3,[0],[0]
"i )(Xj − µS ′
j )].",A Omitted Details from Section 3,[0],[0]
3: Compute approximations for the largest absolute eigenvalue of Σ,A Omitted Details from Section 3,[0],[0]
"− I, λ∗ := ‖Σ − I‖2, and the
associated unit eigenvector v∗. 4: if ‖Σ− I‖2 ≤ O(ε log(1/ε)), then return µS ′ .
",A Omitted Details from Section 3,[0],[0]
5: Let δ := 3 √ ε‖Σ− I‖2.,A Omitted Details from Section 3,[0],[0]
Find T > 0,A Omitted Details from Section 3,[0],[0]
"such that
Pr X∈uS′
[ |v∗ · (X − µS ′ )",A Omitted Details from Section 3,[0],[0]
| > T,A Omitted Details from Section 3,[0],[0]
+ δ,A Omitted Details from Section 3,[0],[0]
],A Omitted Details from Section 3,[0],[0]
> 8 exp(−T 2/2ν),A Omitted Details from Section 3,[0],[0]
"+ 8 ε
T 2 log ( d log( dετ ) ) .",A Omitted Details from Section 3,[0],[0]
"6: return the multiset S′′ = {x ∈ S′ : |v∗ · (x− µS′)| ≤ T + δ}.
",A Omitted Details from Section 3,[0],[0]
• ME denotes EX∈uE,A Omitted Details from Section 3,[0],[0]
[(X − µG)(X,A Omitted Details from Section 3,[0],[0]
"− µG)T ].
",A Omitted Details from Section 3,[0],[0]
Our analysis will hinge on proving the important claim that Σ− I is approximately (|E|/|S′|)ME .,A Omitted Details from Section 3,[0],[0]
This means two things for us.,A Omitted Details from Section 3,[0],[0]
"First, it means that if the positive errors align in some direction (causing ME to have a large eigenvalue), there will be a large eigenvalue in Σ− I. Second, it says that any large eigenvalue of Σ",A Omitted Details from Section 3,[0],[0]
"− I will correspond to an eigenvalue of ME , which will give an explicit direction in which many error points are far from the empirical mean.
",A Omitted Details from Section 3,[0],[0]
Useful Structural Lemmas.,A Omitted Details from Section 3,[0],[0]
"We begin by noting that we have concentration bounds on G and therefore, on S due to its goodness.
",A Omitted Details from Section 3,[0],[0]
Fact A.8.,A Omitted Details from Section 3,[0],[0]
"Let w ∈ Rd be any unit vector, then for any T > 0, PrX∼G",A Omitted Details from Section 3,[0],[0]
"[ |w · (X − µG)| > T ] ≤ 2 exp(−T 2/2ν)
and PrX∈uS [ |w · (X − µG)|",A Omitted Details from Section 3,[0],[0]
>,A Omitted Details from Section 3,[0],[0]
T,A Omitted Details from Section 3,[0],[0]
],A Omitted Details from Section 3,[0],[0]
≤ 2 exp(−T 2/2ν),A Omitted Details from Section 3,[0],[0]
"+ ε
T 2 log(d log( dετ )) .
",A Omitted Details from Section 3,[0],[0]
Proof.,A Omitted Details from Section 3,[0],[0]
"The first line is Fact A.2, and the former follows from it using the goodness of S.
By using the above fact, we obtain the following simple claim:
Claim A.9.",A Omitted Details from Section 3,[0],[0]
"Let w ∈ Rd be any unit vector, then for any T > 0, we have that:
Pr X∼G
[|w · (X − µS ′ )",A Omitted Details from Section 3,[0],[0]
| > T + ‖µS ′,A Omitted Details from Section 3,[0],[0]
"− µG‖2] ≤ 2 exp(−T 2/2ν).
and Pr
X∈uS",A Omitted Details from Section 3,[0],[0]
"[|w · (X − µS
′ )",A Omitted Details from Section 3,[0],[0]
| > T + ‖µS ′,A Omitted Details from Section 3,[0],[0]
− µG‖2] ≤ 2 exp(−T 2/2ν),A Omitted Details from Section 3,[0],[0]
"+
ε T 2 log ( d log( dετ ) ) .",A Omitted Details from Section 3,[0],[0]
Proof.,A Omitted Details from Section 3,[0],[0]
"This follows from Fact A.8 upon noting that |w · (X−µS′)| > T +‖µS′−µG‖2 only if |w · (X−µG)| > T .
",A Omitted Details from Section 3,[0],[0]
"We can use the above facts to prove concentration bounds for L. In particular, we have the following lemma:
Lemma A.10.",A Omitted Details from Section 3,[0],[0]
We have that ‖ML‖2 =,A Omitted Details from Section 3,[0],[0]
"O (log(|S|/|L|) + ε|S|/|L|).
",A Omitted Details from Section 3,[0],[0]
Proof.,A Omitted Details from Section 3,[0],[0]
"Since L ⊆ S, for any x ∈ Rd, we have that
|S| ·",A Omitted Details from Section 3,[0],[0]
Pr X∈uS (X = x) ≥ |L| ·,A Omitted Details from Section 3,[0],[0]
Pr X∈uL (X = x) .,A Omitted Details from Section 3,[0],[0]
"(1)
Since ML is a symmetric matrix, we have ‖ML‖2 = max‖v‖2=1 |vTMLv|.",A Omitted Details from Section 3,[0],[0]
"So, to bound ‖ML‖2 it suffices to bound |vTMLv| for unit vectors v.",A Omitted Details from Section 3,[0],[0]
"By definition of ML, for any v ∈",A Omitted Details from Section 3,[0],[0]
Rd,A Omitted Details from Section 3,[0],[0]
"we have that
|vTMLv| = EX∈uL[|v · (X − µG)|2].
",A Omitted Details from Section 3,[0],[0]
"For unit vectors v, the RHS is bounded from above as follows:
",A Omitted Details from Section 3,[0],[0]
EX∈uL [ |v · (X − µG)|2 ] = 2 ∫ ∞ 0,A Omitted Details from Section 3,[0],[0]
Pr X∈uL,A Omitted Details from Section 3,[0],[0]
[ |v · (X − µG)| > T ],A Omitted Details from Section 3,[0],[0]
"TdT
= 2 ∫ O(√d",A Omitted Details from Section 3,[0],[0]
log(d/ετ)) 0,A Omitted Details from Section 3,[0],[0]
Pr X∈uL,A Omitted Details from Section 3,[0],[0]
"[|v · (X − µG)| > T ]TdT
≤ 2 ∫ O(√d log(d/ετ))
0
min { 1, |S| |L| ·",A Omitted Details from Section 3,[0],[0]
"Pr X∈uS [ |v · (X − µG)| > T ]} TdT
∫ 4√ν log(|S|/|L|)
0
TdT
+ (|S|/|L|) ∫ O(√d",A Omitted Details from Section 3,[0],[0]
"log(d/ετ))
",A Omitted Details from Section 3,[0],[0]
4 √ ν,A Omitted Details from Section 3,[0],[0]
"log(|S|/|L|)
( exp(−T 2/2ν)",A Omitted Details from Section 3,[0],[0]
"+ ε
T 2 log ( d log( dετ ) ))",A Omitted Details from Section 3,[0],[0]
"TdT log(|S|/|L|) + ε · |S|/|L| ,
where the second line follows from the fact that ‖v‖2 = 1, L ⊂ S, and S satisfies condition (i) of Definition A.4",A Omitted Details from Section 3,[0],[0]
"; the third line follows from (1); and the fourth line follows from Fact A.8.
",A Omitted Details from Section 3,[0],[0]
"As a corollary, we can relate the matrices MS′ and ME , in spectral norm:
Corollary A.11.",A Omitted Details from Section 3,[0],[0]
We have that MS′,A Omitted Details from Section 3,[0],[0]
"− I = (|E|/|S′|)ME + O(ε log(1/ε)), where the O(ε log(1/ε)) term denotes a matrix of spectral norm O(ε log(1/ε)).
",A Omitted Details from Section 3,[0],[0]
Proof.,A Omitted Details from Section 3,[0],[0]
"By definition, we have that |S′|MS′ =",A Omitted Details from Section 3,[0],[0]
|S|MS − |L|ML + |E|ME .,A Omitted Details from Section 3,[0],[0]
"Thus, we can write
MS′ =",A Omitted Details from Section 3,[0],[0]
(,A Omitted Details from Section 3,[0],[0]
|S|/|S′|)MS − (|L|/|S′|)ML + (|E|/|S′|)ME = I +O(ε) +O(ε log(1/ε)),A Omitted Details from Section 3,[0],[0]
"+ (|E|/|S′|)ME ,
where the second line uses the fact that 1 − 2ε ≤ |S|/|S′| ≤ 1 + 2ε, the goodness of S (condition (iv) in Definition A.4), and Lemma A.10.",A Omitted Details from Section 3,[0],[0]
"Specifically, Lemma A.10 implies that (|L|/|S′|)‖ML‖2 = O(ε log(1/ε)).",A Omitted Details from Section 3,[0],[0]
"Therefore, we have that
MS′ = I + (|E|/|S′|)ME +O(ε log(1/ε)) ,
as desired.
",A Omitted Details from Section 3,[0],[0]
"We now establish a similarly useful bound on the difference between the mean vectors:
Lemma A.12.",A Omitted Details from Section 3,[0],[0]
We have that µS ′,A Omitted Details from Section 3,[0],[0]
"− µG = (|E|/|S′|)(µE − µG) +O(ε √ log(1/ε)), where the O(ε √ log(1/ε))
term denotes a vector with `2-norm at most O(ε √ log(1/ε)).
",A Omitted Details from Section 3,[0],[0]
Proof.,A Omitted Details from Section 3,[0],[0]
"By definition, we have that
|S′|(µS ′",A Omitted Details from Section 3,[0],[0]
− µG) = |S|(µS − µG)− |L|(µL − µG) + |E|(µE,A Omitted Details from Section 3,[0],[0]
"− µG).
",A Omitted Details from Section 3,[0],[0]
"Since S is a good set, by condition (iii) of Definition A.4, we have ‖µS−µG‖2 = O(ε).",A Omitted Details from Section 3,[0],[0]
"Since 1−2ε ≤ |S|/|S′| ≤ 1 + 2ε, it follows that (|S|/|S′|)‖µS − µG‖2 = O(ε).",A Omitted Details from Section 3,[0],[0]
"Using the valid inequality ‖ML‖2 ≥ ‖µL − µG‖22 and Lemma A.10, we obtain that ‖µL − µG‖2 ≤",A Omitted Details from Section 3,[0],[0]
O (√ log(|S|/|L|) +,A Omitted Details from Section 3,[0],[0]
√ ε|S|/|L| ) .,A Omitted Details from Section 3,[0],[0]
"Therefore,
(|L|/|S′|)‖µL − µG‖2 ≤",A Omitted Details from Section 3,[0],[0]
O ( (|L|/|S|) √ log(|S|/|L|) + √ ε|L|/|S| ) =,A Omitted Details from Section 3,[0],[0]
O(ε √ log(1/ε)),A Omitted Details from Section 3,[0],[0]
".
",A Omitted Details from Section 3,[0],[0]
"In summary, µS ′",A Omitted Details from Section 3,[0],[0]
"− µG = (|E|/|S′|)(µE − µG) +O(ε √ log(1/ε)) ,
as desired.",A Omitted Details from Section 3,[0],[0]
"This completes the proof of the lemma.
",A Omitted Details from Section 3,[0],[0]
"By combining the above, we can conclude that Σ−I is approximately proportional to ME .",A Omitted Details from Section 3,[0],[0]
"More formally, we obtain the following corollary:
Corollary A.13.",A Omitted Details from Section 3,[0],[0]
We have Σ− I = (|E|/|S′|)ME +O(ε log(1/ε)),A Omitted Details from Section 3,[0],[0]
"+O(|E|/|S′|)2‖ME‖2, where the additive terms denote matrices of appropriately bounded spectral norm.
",A Omitted Details from Section 3,[0],[0]
Proof.,A Omitted Details from Section 3,[0],[0]
"By definition, we can write Σ",A Omitted Details from Section 3,[0],[0]
− I =,A Omitted Details from Section 3,[0],[0]
MS′,A Omitted Details from Section 3,[0],[0]
− I − (µS ′,A Omitted Details from Section 3,[0],[0]
− µG)(µS′ − µG)T .,A Omitted Details from Section 3,[0],[0]
"Using Corollary A.11 and Lemma A.12, we obtain:
Σ− I = (|E|/|S′|)ME +O(ε log(1/ε))",A Omitted Details from Section 3,[0],[0]
+O((|E|/|S′|)2‖µE − µG‖22),A Omitted Details from Section 3,[0],[0]
+O(ε2 log(1/ε)) = (|E|/|S′|)ME +O(ε log(1/ε)),A Omitted Details from Section 3,[0],[0]
"+O(|E|/|S′|)2‖ME‖2 ,
where the second line follows from the valid inequality ‖ME‖2 ≥ ‖µE −µG‖22.",A Omitted Details from Section 3,[0],[0]
"This completes the proof.
",A Omitted Details from Section 3,[0],[0]
Case of Small Spectral Norm.,A Omitted Details from Section 3,[0],[0]
We are now ready to analyze the case that the mean vector µS ′ is returned by the algorithm in Step 4.,A Omitted Details from Section 3,[0],[0]
"In this case, we have that λ∗ def = ‖Σ − I‖2 = O(ε log(1/ε)).",A Omitted Details from Section 3,[0],[0]
"Hence, Corollary A.13 yields that (|E|/|S′|)‖ME‖2 ≤",A Omitted Details from Section 3,[0],[0]
λ∗,A Omitted Details from Section 3,[0],[0]
+O(ε log(1/ε)),A Omitted Details from Section 3,[0],[0]
"+O(|E|/|S′|)2‖ME‖2 ,
which in turns implies that (|E|/|S′|)‖ME‖2 = O(ε log(1/ε)) .
",A Omitted Details from Section 3,[0],[0]
"On the other hand, since ‖ME‖2 ≥ ‖µE − µG‖22, Lemma A.12 gives that
‖µS ′",A Omitted Details from Section 3,[0],[0]
− µG‖2 ≤ (|E|/|S′|) √ ‖ME‖2 +O(ε √ log(1/ε)),A Omitted Details from Section 3,[0],[0]
"= O(ε √ log(1/ε)).
",A Omitted Details from Section 3,[0],[0]
"This proves part (i) of Proposition A.7.
",A Omitted Details from Section 3,[0],[0]
Case of Large Spectral Norm.,A Omitted Details from Section 3,[0],[0]
"We next show the correctness of the algorithm when it returns a filter in Step 5.
",A Omitted Details from Section 3,[0],[0]
We start by proving that if λ∗ def = ‖Σ,A Omitted Details from Section 3,[0],[0]
"− I‖2 > Cε log(1/ε), for a sufficiently large universal constant C, then a value T satisfying the condition in Step 5 exists.",A Omitted Details from Section 3,[0],[0]
We first note that that ‖ME‖2 is appropriately large.,A Omitted Details from Section 3,[0],[0]
"Indeed, by Corollary A.13 and the assumption that λ∗ >",A Omitted Details from Section 3,[0],[0]
Cε log(1/ε),A Omitted Details from Section 3,[0],[0]
"we deduce that
(|E|/|S′|)‖ME‖2 = Ω(λ∗) .",A Omitted Details from Section 3,[0],[0]
"(2)
Moreover, using the inequality ‖ME‖2 ≥ ‖µE − µG‖22 and Lemma A.12 as above, we get that
‖µS ′",A Omitted Details from Section 3,[0],[0]
"− µG‖2 ≤ (|E|/|S′|) √ ‖ME‖2 +O(ε √ log(1/ε)) ≤ δ/2 , (3)
where we used the fact that δ def = √ ελ∗ > C ′ε √ log(1/ε).
",A Omitted Details from Section 3,[0],[0]
Suppose for the sake of contradiction that for all T > 0,A Omitted Details from Section 3,[0],[0]
"we have that
Pr X∈uS′
[ |v∗ · (X − µS ′ )",A Omitted Details from Section 3,[0],[0]
| > T,A Omitted Details from Section 3,[0],[0]
+ δ,A Omitted Details from Section 3,[0],[0]
],A Omitted Details from Section 3,[0],[0]
≤ 8 exp(−T 2/2ν),A Omitted Details from Section 3,[0],[0]
"+ 8 ε
T 2 log ( d log( dετ ) ) .",A Omitted Details from Section 3,[0],[0]
"Using (3), we obtain that for all T > 0",A Omitted Details from Section 3,[0],[0]
"we have that
Pr X∈uS′
[ |v∗ · (X − µG)| > T + δ/2 ] ≤ 8 exp(−T 2/2ν)",A Omitted Details from Section 3,[0],[0]
"+ 8 ε
T 2 log ( d log( dετ ) ) .",A Omitted Details from Section 3,[0],[0]
"(4) Since E ⊆ S′, for all x ∈ Rd we have that |S′|PrX∈uS′ [X = x] ≥",A Omitted Details from Section 3,[0],[0]
|E|PrY ∈uE,A Omitted Details from Section 3,[0],[0]
[Y = x].,A Omitted Details from Section 3,[0],[0]
"This fact combined with (4) implies that for all T > 0
Pr X∈uE
[ |v∗ · (X − µG)| > T + δ/2 ] (|S′|/|E|) ( exp(−T 2/2ν)",A Omitted Details from Section 3,[0],[0]
+,A Omitted Details from Section 3,[0],[0]
"ε
T 2 log ( d log( dετ )
)) .",A Omitted Details from Section 3,[0],[0]
"(5)
We now have the following sequence of inequalities: ‖ME‖2 = EX∈uE [ |v∗ · (X − µG)|2 ] = 2 ∫ ∞ 0",A Omitted Details from Section 3,[0],[0]
Pr X∈uE [ |v∗ · (X − µG)| > T ],A Omitted Details from Section 3,[0],[0]
"TdT
= 2 ∫ O(√d",A Omitted Details from Section 3,[0],[0]
log(d/ετ)) 0,A Omitted Details from Section 3,[0],[0]
Pr X∈uE [ |v∗ · (X − µG)| > T ],A Omitted Details from Section 3,[0],[0]
"TdT
≤ 2 ∫ O(√d log(d/ετ))
0
min { 1, |S′| |E| · Pr X∈uS′ [ |v∗ · (X − µG)| > T ]} TdT
∫ 4√ν log(|S′|/|E|)+δ
0
",A Omitted Details from Section 3,[0],[0]
TdT + (|S′|/|E|) ∫ O(√d,A Omitted Details from Section 3,[0],[0]
"log(d/ετ))
4 √ ν",A Omitted Details from Section 3,[0],[0]
"log(|S′|/|E|)+δ
( exp(−T 2/2ν)",A Omitted Details from Section 3,[0],[0]
"+ ε
T 2 log ( d log( dετ ) ))",A Omitted Details from Section 3,[0],[0]
TdT log(|S′|/|E|),A Omitted Details from Section 3,[0],[0]
+,A Omitted Details from Section 3,[0],[0]
"δ2 +O(1) + ε · |S′|/|E| log(|S′|/|E|) + ελ∗ + ε · |S′|/|E| .
",A Omitted Details from Section 3,[0],[0]
"Rearranging the above, we get that
(|E|/|S′|)‖ME‖2 (|E|/|S′|) log(|S′|/|E|)",A Omitted Details from Section 3,[0],[0]
"+ (|E|/|S′|)ελ∗ + ε = O(ε log(1/ε) + ε2λ∗).
",A Omitted Details from Section 3,[0],[0]
"Combined with (2), we obtain λ∗ = O(ε log(1/ε)), which is a contradiction if C is sufficiently large.",A Omitted Details from Section 3,[0],[0]
"Therefore, it must be the case that for some value of T the condition in Step 5 is satisfied.
",A Omitted Details from Section 3,[0],[0]
"The following claim completes the proof:
Claim A.14.",A Omitted Details from Section 3,[0],[0]
Fix α def = d log(d/ετ) log(d log( dετ )).,A Omitted Details from Section 3,[0],[0]
"We have that ∆(S, S ′′) ≤ ∆(S, S′)− 2ε/α .",A Omitted Details from Section 3,[0],[0]
Proof.,A Omitted Details from Section 3,[0],[0]
"Recall that S′ = (S \L)∪E, with E and L disjoint multisets such that L ⊂ S.",A Omitted Details from Section 3,[0],[0]
"We can similarly write S′′ = (S \ L′) ∪ E′, with L′ ⊇ L and E′ ⊂ E.",A Omitted Details from Section 3,[0],[0]
"Since
∆(S, S′)−∆(S, S′′) = |E",A Omitted Details from Section 3,[0],[0]
"\ E ′| − |L′ \ L| |S| ,
it suffices to show that |E",A Omitted Details from Section 3,[0],[0]
\E′| ≥ |L′ \L|+ ε|S|/α.,A Omitted Details from Section 3,[0],[0]
"Note that |L′ \L| is the number of points rejected by the filter that lie in S ∩ S′. Note that the fraction of elements of S that are removed to produce S′′ (i.e., satisfy |v∗ · (x − µS′)| > T + δ) is at most 2 exp(−T 2/2ν)",A Omitted Details from Section 3,[0],[0]
+ ε/α.,A Omitted Details from Section 3,[0],[0]
"This follows from Claim A.9 and the fact that T = O( √ d log(d/ετ)).
",A Omitted Details from Section 3,[0],[0]
"Hence, it holds that |L′ \ L| ≤ (2 exp(−T 2/2ν) + ε/α)|S|.",A Omitted Details from Section 3,[0],[0]
"On the other hand, Step 5 of the algorithm ensures that the fraction of elements of S′ that are rejected by the filter is at least 8 exp(−T 2/2ν) + 8ε/α).",A Omitted Details from Section 3,[0],[0]
Note that |E,A Omitted Details from Section 3,[0],[0]
\ E′| is the number of points rejected by the filter that lie in S′ \,A Omitted Details from Section 3,[0],[0]
"S. Therefore, we can write:
|E",A Omitted Details from Section 3,[0],[0]
"\ E′| ≥ (8 exp(−T 2/2ν) + 8ε/α)|S′| − (2 exp(−T 2/2ν) + ε/α)|S| ≥ (8 exp(−T 2/2ν) + 8ε/α)|S|/2− (2 exp(−T 2/2ν) + ε/α)|S| ≥ (2 exp(−T 2/2ν) + 3ε/α)|S| ≥ |L′ \ L|+ 2ε|S|/α ,
where the second line uses the fact that |S′| ≥ |S|/2 and the last line uses the fact that |L′ \ L|/|S| ≤ 2 exp(−T 2/2ν)",A Omitted Details from Section 3,[0],[0]
+ ε/α.,A Omitted Details from Section 3,[0],[0]
"Noting that log(d/ετ) ≥ 1, this completes the proof of the claim.
",A Omitted Details from Section 3,[0],[0]
"A.1.3 Proof of Lemma A.5
Proof.",A Omitted Details from Section 3,[0],[0]
"Let N = Ω((d/ε2) poly log(d/ετ)) be the number of samples drawn from G. For (i), the probability that a coordinate of a sample is at least √ 2ν log(Nd/3τ) is at most τ/3dN by Fact A.2.",A Omitted Details from Section 3,[0],[0]
"By a union bound,
the probability that all coordinates of all samples are smaller than √
2ν log(Nd/3τ) is at least 1 − τ/3.",A Omitted Details from Section 3,[0],[0]
"In this case, ‖x‖2 ≤ √ 2νd log(Nd/3τ) =",A Omitted Details from Section 3,[0],[0]
"O( √ dν log(Nν/τ)).
",A Omitted Details from Section 3,[0],[0]
"After translating by µG, we note that (iii) follows immediately from Lemmas 4.3 of [DKK+16] and (iv) follows from Theorem 5.50 of [Ver10], as long as N = Ω(ν4d log(1/τ)/ε2), with probability at least 1− τ/3.",A Omitted Details from Section 3,[0],[0]
"It remains to show that, conditioned on (i), (ii) holds with probability at least 1− τ/3.
",A Omitted Details from Section 3,[0],[0]
"To simplify some expressions, let δ := ε/(log(d log d/ετ)) and R = C √ d log(|S|/τ).",A Omitted Details from Section 3,[0],[0]
We need to show that for all unit vectors v and all 0 ≤ T ≤,A Omitted Details from Section 3,[0],[0]
R that∣∣∣∣ PrX∈uS[|v · (X − µG)| > T ],A Omitted Details from Section 3,[0],[0]
− PrX∼G[|v · (X − µG) > T ≥ 0] ∣∣∣∣ ≤ δT 2 .,A Omitted Details from Section 3,[0],[0]
"(6)
Firstly, we show that for all unit vectors v and T > 0∣∣∣∣",A Omitted Details from Section 3,[0],[0]
PrX∈uS[|v · (X − µG)| > T,A Omitted Details from Section 3,[0],[0]
],A Omitted Details from Section 3,[0],[0]
− PrX∼G[|v · (X − µG)| > T ≥ 0],A Omitted Details from Section 3,[0],[0]
"∣∣∣∣ ≤ δ10ν ln(1/δ)
with probability at least 1 − τ/6.",A Omitted Details from Section 3,[0],[0]
"Since the VC-dimension of the set of all halfspaces is d + 1, this follows from the VC inequality [DL01], since we have more than Ω(d/(δ/(10ν log(1/δ))2) samples.",A Omitted Details from Section 3,[0],[0]
We thus only need to consider the case,A Omitted Details from Section 3,[0],[0]
"when T ≥ √ 10ν ln(1/δ).
",A Omitted Details from Section 3,[0],[0]
Lemma A.15.,A Omitted Details from Section 3,[0],[0]
"For any fixed unit vector v and T > √
10ν ln(1/δ), except with probability exp(−Nδ/(6Cν)), we have that
Pr X∈uS",A Omitted Details from Section 3,[0],[0]
"[|v · (X − µG)| > T ] ≤ δ CT 2 ,
where C = 8.
",A Omitted Details from Section 3,[0],[0]
Proof.,A Omitted Details from Section 3,[0],[0]
Let E be the event that |v · (X − µG)| > T .,A Omitted Details from Section 3,[0],[0]
"Since G is sub-gaussian, Fact A.2 yields that PrG[E] =",A Omitted Details from Section 3,[0],[0]
PrY∼G[|v · (X − µG)| > T,A Omitted Details from Section 3,[0],[0]
] ≤ exp(−T 2/(2ν)).,A Omitted Details from Section 3,[0],[0]
"Note that, thanks to our assumption on T , we have that T ≤",A Omitted Details from Section 3,[0],[0]
"exp(T 2/(4ν))/2C, and therefore T 2 PrG[E] ≤",A Omitted Details from Section 3,[0],[0]
"exp(−T 2/(4ν))/2C ≤ δ/2C.
Consider ES [exp(t2/(3ν) ·",A Omitted Details from Section 3,[0],[0]
N PrS,A Omitted Details from Section 3,[0],[0]
[E])].,A Omitted Details from Section 3,[0],[0]
Each individual sample Xi for 1 ≤,A Omitted Details from Section 3,[0],[0]
"i ≤ N , is an independent copy of Y ∼ G, and hence:
ES [ exp ( T 2
3ν ·N Pr S",A Omitted Details from Section 3,[0],[0]
"[E]
)]",A Omitted Details from Section 3,[0],[0]
"= ES [ exp ( T 2
3ν ) · n∑ i=1 1Xi∈E) ]
= N∏ i=1",A Omitted Details from Section 3,[0],[0]
"EXi
[ exp ( T 2
3ν ) · n∑ i=1 1Xi∈E) ]
= ( exp ( T 2
3ν )",A Omitted Details from Section 3,[0],[0]
Pr G,A Omitted Details from Section 3,[0],[0]
"[G] + 1 )N (a)
≤ ( exp ( T 2
6ν
)",A Omitted Details from Section 3,[0],[0]
"+ 1 )N (b) ≤ (1 + δ5/3)N
(c) ≤ exp(Nδ5/3) ,
where (a) follows from sub-gaussianity, (b) follows from our choice of T , and (c) comes from the fact that 1 + x ≤ ex for all x.
Hence, by Markov’s inequality, we have
Pr [ Pr S [E] ≥ δ CT 2 ]",A Omitted Details from Section 3,[0],[0]
≤ exp,A Omitted Details from Section 3,[0],[0]
( Nδ5/3 − δN 3C ) = exp(Nδ(δ2/3,A Omitted Details from Section 3,[0],[0]
"− 1/(3C))) .
",A Omitted Details from Section 3,[0],[0]
"Thus, if δ is a sufficiently small constant and C is sufficiently large, this yields the desired bound.
",A Omitted Details from Section 3,[0],[0]
Now let C be a 1/2-cover in Euclidean distance for the set of unit vectors of size 2O(d).,A Omitted Details from Section 3,[0],[0]
"By a union bound, for all v′ ∈ C and T ′ a power of 2 between √ 4ν ln(1/δ) and R, we have that
Pr X∈uS",A Omitted Details from Section 3,[0],[0]
"[|v′ · (X − µG)| > T ′] ≤ δ 8T 2
except with probability
2O(d) log(R) exp(−Nδ/6Cν) = exp (O(d) + log logR−Nδ/6Cν) ≤ τ/6 .
",A Omitted Details from Section 3,[0],[0]
"However, for any unit vector v and √
4ν ln(1/δ) ≤ T ≤ R, there is a v′ ∈ C and such a T ′ such that for all x ∈ Rd, we have |v · (X − µG)| ≥ |v′ · (X − µG)|/2, and so |v′ · (",A Omitted Details from Section 3,[0],[0]
X − µG)|,A Omitted Details from Section 3,[0],[0]
>,A Omitted Details from Section 3,[0],[0]
2T ′ implies |v′ · (X − µG)| >,A Omitted Details from Section 3,[0],[0]
"T.
Then, by a union bound, (6) holds simultaneously for all unit vectors v and all 0 ≤ T ≤ R, with probability a least 1− τ/3.",A Omitted Details from Section 3,[0],[0]
"This completes the proof.
",A Omitted Details from Section 3,[0],[0]
"A.2 Robust Mean Estimation Under Second Moment Assumptions
In this section, we use our filtering technique to give a near sample-optimal computationally efficient algorithm to robustly estimate the mean of a density with a second moment assumption.",A Omitted Details from Section 3,[0],[0]
"We show:
Theorem A.16.",A Omitted Details from Section 3,[0],[0]
Let P be a distribution on Rd with unknown mean vector µP and unknown covariance matrix ΣP I.,A Omitted Details from Section 3,[0],[0]
Let S be an ε-corrupted set of samples from P of size Θ((d/ε) log d).,A Omitted Details from Section 3,[0],[0]
"Then there exists an algorithm that given S, with probability 2/3, outputs µ̂ with ‖µ̂− µP ‖2 ≤ O( √ ε) in time poly(d/ε).
",A Omitted Details from Section 3,[0],[0]
"Note that Theorem 3.2 follows straightforwardly from the above (divide every sample by σ, run the algorithm of Theorem A.16, and multiply its output by σ).
",A Omitted Details from Section 3,[0],[0]
"As usual in our filtering framework, the algorithm will iteratively look at the top eigenvalue and eigenvector of the sample covariance matrix and return the sample mean if this eigenvalue is small (Algorithm 3).",A Omitted Details from Section 3,[0],[0]
The main difference between this and the filter algorithm for the sub-gaussian case is how we choose the threshold for the filter.,A Omitted Details from Section 3,[0],[0]
"Instead of looking for a violation of a concentration inequality, here we will choose a threshold at random (with a bias towards higher thresholds).",A Omitted Details from Section 3,[0],[0]
"The reason is that, in this setting, the variance in the direction we look for a filter in needs to be a constant multiple larger – instead of the typical Ω̃(ε) relative for the sub-gaussian case.",A Omitted Details from Section 3,[0],[0]
"Therefore, randomly choosing a threshold weighted towards higher thresholds suffices to throw out more corrupted samples than uncorrupted samples in expectation.",A Omitted Details from Section 3,[0],[0]
"Although it is possible to reject many good samples this way, the algorithm still only rejects a total of O(ε) samples with high probability.
",A Omitted Details from Section 3,[0],[0]
We would like our good set of samples to have mean close to that of P and bounded variance in all directions.,A Omitted Details from Section 3,[0],[0]
"This motivates the following definition:
Definition A.17.",A Omitted Details from Section 3,[0],[0]
"We call a set S ε-good for a distribution P with mean µP and covariance ΣP I if the mean µS and covariance ΣS of S satisfy ‖µS − µP ‖2 ≤ √ ε and ‖ΣS‖2 ≤ 2.
",A Omitted Details from Section 3,[0],[0]
"However, since we have no assumptions about higher moments, it may be be possible for outliers to affect our sample covariance too much.",A Omitted Details from Section 3,[0],[0]
"Fortunately, such outliers have small probability and do not contribute too much to the mean, so we will later reclassify them as errors.
",A Omitted Details from Section 3,[0],[0]
Lemma A.18.,A Omitted Details from Section 3,[0],[0]
Let S be N = Θ((d/ε) log d) samples drawn from P .,A Omitted Details from Section 3,[0],[0]
"Then, with probability at least 9/10, a random X ∈u S satisfies
(i) ‖ES",A Omitted Details from Section 3,[0],[0]
"[X]− µP ‖2 ≤ √ ε/3,
(ii)",A Omitted Details from Section 3,[0],[0]
"PrS [ ‖X − µP ‖2 ≥ 80 √ d/ε ] ≤ ε/160,
(iii) ∥∥∥ES",A Omitted Details from Section 3,[0],[0]
"[(X − µP ) · 1‖X−µP ‖2≤80√d/ε]∥∥∥2 ≤ √ε/3, and
(iv) ∥∥∥ES",A Omitted Details from Section 3,[0],[0]
"[(X − µP )(X − µP )T · 1‖X−µP ‖2≤80√d/ε]∥∥∥2 ≤ 3/2.
",A Omitted Details from Section 3,[0],[0]
Proof.,A Omitted Details from Section 3,[0],[0]
"For (i), note that ES [‖E[X]− µP ‖22] = ∑ i ES",A Omitted Details from Section 3,[0],[0]
[(E[X]i − µPi )2] ≤,A Omitted Details from Section 3,[0],[0]
"d/N ≤ ε/360 ,
and so by Markov’s inequality, with probability at least 39/40, we have ‖E[X]− µP ‖22 ≤ ε/9.
",A Omitted Details from Section 3,[0],[0]
"For (ii), similarly to (i), note that E[‖Y − µP ‖22] = ∑ i E",A Omitted Details from Section 3,[0],[0]
[ (Yi − µPi )2 ] ≤,A Omitted Details from Section 3,[0],[0]
"d ,
for Y ∼ P .",A Omitted Details from Section 3,[0],[0]
"By Markov’s inequality, Pr[‖Y − µP ‖2 ≥ 80 √ d/ε] ≤ ε/160 with probability at least 39/40.
",A Omitted Details from Section 3,[0],[0]
"For (iii), let ν = EX∼P",A Omitted Details from Section 3,[0],[0]
[X · 1‖X−µP ‖2≤80 √ d/ε ] be the true mean of the distribution when we condition on the event,A Omitted Details from Section 3,[0],[0]
that ‖X − µP ‖2 ≤ 80 √ d/ε.,A Omitted Details from Section 3,[0],[0]
"By the same argument as (i), we know that∥∥∥EX∈uS",A Omitted Details from Section 3,[0],[0]
[X · 1‖X−µP ‖2≤80√d/ε]− ν∥∥∥2 ≤,A Omitted Details from Section 3,[0],[0]
"√ε/9 ,
with probability at least 39/40.",A Omitted Details from Section 3,[0],[0]
Thus it suffices to show that ∥∥∥ν − µP ·,A Omitted Details from Section 3,[0],[0]
1‖X−µP ‖2≤80√d/ε∥∥∥2 ≤ √ε/10.,A Omitted Details from Section 3,[0],[0]
"To do so, it suffices to show that for all unit vectors v ∈ Rd, we have∣∣∣〈v, ν",A Omitted Details from Section 3,[0],[0]
− µP · 1‖X−µP ‖2≤80√d/ε〉∣∣∣ <,A Omitted Details from Section 3,[0],[0]
√ε/10 .,A Omitted Details from Section 3,[0],[0]
"Observe that for any such v, we have〈
v, µP · 1‖X−µP ‖2≤80 √ d/ε",A Omitted Details from Section 3,[0],[0]
"− ν 〉 = EX∼P [〈 v,X − µP 〉 · 1‖X−µP ‖2≤80 √ d/ε ]",A Omitted Details from Section 3,[0],[0]
"(a)
≤ √
EX∼P",A Omitted Details from Section 3,[0],[0]
"[〈v,X − µP 〉2]",A Omitted Details from Section 3,[0],[0]
"Pr X∼P
[",A Omitted Details from Section 3,[0],[0]
"‖X − µP ‖2 ≥ 80 √ d/ε]
(b) = √ vTΣP v ·",A Omitted Details from Section 3,[0],[0]
"Pr
X∼P
[ ‖X − µP ‖2 ≥ 80 √ d/ε ]
(c) ≤",A Omitted Details from Section 3,[0],[0]
"√ ε/10 ,
where (a) follows from Cauchy-Schwarz, and (b) follows from the definition of the covariance, and (c) follows from the assumption that ΣP I and from Markov’s inequality.
",A Omitted Details from Section 3,[0],[0]
"For (iv), we require the following Matrix Chernoff bound:
Lemma A.19 (Part of Theorem 5.1.1 of [T+15]).",A Omitted Details from Section 3,[0],[0]
Consider a sequence of d×d positive semi-definite random matrices Xk with ‖Xk‖2 ≤,A Omitted Details from Section 3,[0],[0]
L for all k. Let µmax = ‖ ∑,A Omitted Details from Section 3,[0],[0]
k E[Xk]‖2.,A Omitted Details from Section 3,[0],[0]
"Then, for θ > 0,
E [∥∥∥∥∥∑ k Xk ∥∥∥∥∥ 2 ] ≤ (eθ − 1)µmax/θ + L log(d)/θ ,
and for any δ > 0,
Pr [∥∥∥∥∥∑ k Xk ∥∥∥∥∥ 2 ≥ (1 + δ)µmax ] ≤ d(eδ/(1 + δ)1+δ)µ max/L .
",A Omitted Details from Section 3,[0],[0]
We apply this lemma with Xk = (xk − µP ),A Omitted Details from Section 3,[0],[0]
(xk − µP )T 1‖xk−µP,A Omitted Details from Section 3,[0],[0]
"‖2≤80 √ d/ε for {x1, . . .",A Omitted Details from Section 3,[0],[0]
", xN}",A Omitted Details from Section 3,[0],[0]
= S. Note that ‖Xk‖2 ≤ (80)2d/ε = L and that µmax ≤,A Omitted Details from Section 3,[0],[0]
N‖ΣP,A Omitted Details from Section 3,[0],[0]
"‖2 ≤ N .
",A Omitted Details from Section 3,[0],[0]
Suppose that µmax ≤ N/80.,A Omitted Details from Section 3,[0],[0]
"Then, taking θ = 1, we have
E[ ∥∥∥∥∥∑ k Xk ∥∥∥∥∥ 2 ] ≤ (e− 1)N/80 +O(d log(d)/ε) .
",A Omitted Details from Section 3,[0],[0]
"By Markov’s inequality, except with probability 39/40, we have ‖ ∑ kXk‖2 ≤",A Omitted Details from Section 3,[0],[0]
N +,A Omitted Details from Section 3,[0],[0]
"O(d log(d)/ε) ≤ 3N/2, for N a sufficiently high multiple of d log(d)/ε.",A Omitted Details from Section 3,[0],[0]
"Suppose that µmax ≥ N/80, then we take δ = 1/2 and obtain
Pr [∥∥∥∥∥∑ k Xk ∥∥∥∥∥ 2 ≥ 3µmax2 ] ≤",A Omitted Details from Section 3,[0],[0]
"d(e3/2/(5/2)3/2)Nε/20d .
",A Omitted Details from Section 3,[0],[0]
"For N a sufficiently high multiple of d log(d)/ε, we get that Pr[‖ ∑ kXk‖2 ≥ 3µ
max/2] ≤ 1/40.",A Omitted Details from Section 3,[0],[0]
"Since µmax ≤ N , we have with probability at least 39/40, ‖ ∑ kXk‖2 ≤ 3N/2.
",A Omitted Details from Section 3,[0],[0]
Noting that ‖ ∑ kXk‖2 /N,A Omitted Details from Section 3,[0],[0]
=,A Omitted Details from Section 3,[0],[0]
"‖E[1‖X−µP ‖2≤80 √ d/ε
(X − µP )(X − µP )",A Omitted Details from Section 3,[0],[0]
"T ]‖2, we obtain (iv).",A Omitted Details from Section 3,[0],[0]
"By a union bound, (i)-(iv) all hold simultaneously with probability at least 9/10.
",A Omitted Details from Section 3,[0],[0]
"Now we can get a 2ε-corrupted good set from an ε-corrupted set of samples satisfying Lemma A.18, by reclassifying outliers as errors:
Lemma A.20.",A Omitted Details from Section 3,[0],[0]
"Let S = R ∪E \L, where R is a set of N = Θ(d log d/ε) samples drawn from P and E and L are disjoint sets with |E|, |L| ≤ ε.",A Omitted Details from Section 3,[0],[0]
"Then, with probability 9/10, we can also write S = G ∪ E′ \",A Omitted Details from Section 3,[0],[0]
"L′, where G ⊆ R is ε-good, L′ ⊆ L and E′ ⊆ E′ has |E′| ≤ 2ε|S|.",A Omitted Details from Section 3,[0],[0]
Proof.,A Omitted Details from Section 3,[0],[0]
Let G = {x ∈ R : ‖x‖2 ≤ 80 √ d/ε}.,A Omitted Details from Section 3,[0],[0]
Condition on the event that R satisfies Lemma A.18.,A Omitted Details from Section 3,[0],[0]
"By Lemma A.18, this occurs with probability at least 9/10.",A Omitted Details from Section 3,[0],[0]
"Since R satisfies (ii) of Lemma A.18, |G|",A Omitted Details from Section 3,[0],[0]
− |R| ≤ ε|R|/160 ≤ ε|S|.,A Omitted Details from Section 3,[0],[0]
"Thus, E′ = E ∪ (R \ G) has |E′| ≤ 3ε/2.",A Omitted Details from Section 3,[0],[0]
"Note that (iv) of Lemma A.18 for R in terms of G is exactly |G|‖ΣG‖2/|R| ≤ 3/2, and so ‖ΣG‖2 ≤ 3|R|/(2|G|) ≤ 2.
",A Omitted Details from Section 3,[0],[0]
It remains to check that ‖µG,A Omitted Details from Section 3,[0],[0]
− µP ‖2 ≤ √ ε.,A Omitted Details from Section 3,[0],[0]
"We have∥∥|G| · µG − |G| · µP∥∥
2 = |R| ·",A Omitted Details from Section 3,[0],[0]
∥∥∥EX∼uR,A Omitted Details from Section 3,[0],[0]
[(X − µP ) · 1‖X−µP ‖2≤80√d/ε]∥∥∥2 ≤,A Omitted Details from Section 3,[0],[0]
"|R| · √ ε/3 ,
where the last line follows from (iii) of Lemma A.18.",A Omitted Details from Section 3,[0],[0]
"Since we argued above that |R|/|G| ≥ 2/3, dividing this expression by |G| yields the desired claim.
",A Omitted Details from Section 3,[0],[0]
"Algorithm 3 Filter under second moment assumptions
1: function FilterUnder2ndMoment(S) 2: Compute µS , ΣS , the mean and covariance matrix of S. 3: Find the eigenvector v∗ with highest eigenvalue λ∗ of ΣS .",A Omitted Details from Section 3,[0],[0]
"4: if λ∗ ≤ 9 then 5: return µS 6: else 7: Draw Z from the distribution on [0, 1] with probability density function 2x. 8: Let T = Z max{|v∗ · x− µS",A Omitted Details from Section 3,[0],[0]
| : x ∈ S}.,A Omitted Details from Section 3,[0],[0]
"9: Return the set S′ = {x ∈ S : |v∗ · (X − µS)| < T}.
",A Omitted Details from Section 3,[0],[0]
An iteration of FilterUnder2ndMoment may throw out more samples from G than corrupted samples.,A Omitted Details from Section 3,[0],[0]
"However, in expectation, we throw out many more corrupted samples than from the good set:
Proposition A.21.",A Omitted Details from Section 3,[0],[0]
"If we run FilterUnder2ndMoment on a set S = G ∪ E \ L for some ε-good set G and disjoint E,L with |E| ≤ 2ε|S|, |L| ≤ 9ε|S|, then either it returns µS with ‖µS − µP ‖2 ≤ O( √ ε), or else it returns a set S′ ⊂",A Omitted Details from Section 3,[0],[0]
S with S′ = G ∪ E′ \,A Omitted Details from Section 3,[0],[0]
L′ for disjoint E′ and L′.,A Omitted Details from Section 3,[0],[0]
In the latter case we have EZ [|E′|+ 2|L′|],A Omitted Details from Section 3,[0],[0]
"≤ |E|+ 2|L|.
",A Omitted Details from Section 3,[0],[0]
"For D ∈ {G,E,L, S}, let µD be the mean of D and MD be the matrix EX∈uD[(X − µS)(X",A Omitted Details from Section 3,[0],[0]
− µS)T ].,A Omitted Details from Section 3,[0],[0]
Lemma A.22.,A Omitted Details from Section 3,[0],[0]
"If G is an ε-good set with x ≤ 40 √ d/ε for x ∈ S ∪G, then ‖MG‖2 ≤ 2‖µG − µS‖22 + 2 .
",A Omitted Details from Section 3,[0],[0]
Proof.,A Omitted Details from Section 3,[0],[0]
"For any unit vector v, we have
vTMGv = EX∈uG[(v · (X − µS))2] = EX∈uG[(v · (X − µG) + v · (µP − µG))2] = vTΣGv +",A Omitted Details from Section 3,[0],[0]
"(v · (µG − µS))2
≤ 2 + 2‖µG − µS‖22 .
",A Omitted Details from Section 3,[0],[0]
Lemma A.23.,A Omitted Details from Section 3,[0],[0]
We have that |L|‖ML‖2 ≤ 2|G|(1 + ‖µG,A Omitted Details from Section 3,[0],[0]
"− µS‖22) .
",A Omitted Details from Section 3,[0],[0]
Proof.,A Omitted Details from Section 3,[0],[0]
"Since L ⊆ G, for any unit vector v, we have
|L|vTMLv = |L|EX∈uL[(v · (X − µS))2] ≤ |G|EX∈uG[(v · (X − µS))2] ≤ 2|G|(1 + ‖µG",A Omitted Details from Section 3,[0],[0]
"− µS‖22) .
",A Omitted Details from Section 3,[0],[0]
Lemma A.24.,A Omitted Details from Section 3,[0],[0]
‖µG,A Omitted Details from Section 3,[0],[0]
"− µS‖2 ≤ √ 2ε‖MS‖2 + 12 √ ε.
",A Omitted Details from Section 3,[0],[0]
Proof.,A Omitted Details from Section 3,[0],[0]
We have that |E|ME ≤,A Omitted Details from Section 3,[0],[0]
|S|MS +,A Omitted Details from Section 3,[0],[0]
"|L|ML and so
|E|‖ME‖2 ≤ |S|‖MS‖2 +",A Omitted Details from Section 3,[0],[0]
2|G|(1 + ‖µG,A Omitted Details from Section 3,[0],[0]
"− µS‖22) .
",A Omitted Details from Section 3,[0],[0]
"By Cauchy Schwarz, we have that ‖ME‖2 ≥ ‖µE − µS‖22, and so√ |E|‖µE",A Omitted Details from Section 3,[0],[0]
− µS‖2 ≤ √ |S|‖MS‖2 +,A Omitted Details from Section 3,[0],[0]
2|G|(1 + ‖µG,A Omitted Details from Section 3,[0],[0]
"− µS‖22) .
",A Omitted Details from Section 3,[0],[0]
"By Cauchy-Schwarz and Lemma A.23, we have that√ |L|‖µL − µS‖2 ≤ √ |L|‖ML‖2 ≤",A Omitted Details from Section 3,[0],[0]
√ 2|G|(1 + ‖µG,A Omitted Details from Section 3,[0],[0]
"− µS‖22) .
",A Omitted Details from Section 3,[0],[0]
"Since |S|µS = |G|µG + |E|µE − |L|µL and |S| = |G|+ |E| − |L|, we get
|G|(µG",A Omitted Details from Section 3,[0],[0]
− µS),A Omitted Details from Section 3,[0],[0]
= |E|(µE,A Omitted Details from Section 3,[0],[0]
− µS)− |L|(µE,A Omitted Details from Section 3,[0],[0]
"− µS) .
",A Omitted Details from Section 3,[0],[0]
"Substituting into this, we obtain |G|‖µG − µS‖2 ≤ √ |E||S|‖MS‖2 + 2|E||G|(1 +",A Omitted Details from Section 3,[0],[0]
‖µG,A Omitted Details from Section 3,[0],[0]
− µS‖22) + √ 2|L||G|(1 + ‖µG,A Omitted Details from Section 3,[0],[0]
− µS‖22) .,A Omitted Details from Section 3,[0],[0]
"Since for x, y > 0, √ x+ y ≤",A Omitted Details from Section 3,[0],[0]
"√ x+ √ y, we have
|G|‖µG − µS‖2 ≤ √ |E||S|‖MS‖2 +",A Omitted Details from Section 3,[0],[0]
(,A Omitted Details from Section 3,[0],[0]
"√ 2|E||G|+ √ 2|L||G|)(1 + ‖µG − µS‖2) .
",A Omitted Details from Section 3,[0],[0]
"Since ||G| − |S|| ≤ ε|S| and |E| ≤ 2ε|S|, |L| ≤ 9ε|S|, we have
‖µG",A Omitted Details from Section 3,[0],[0]
− µS‖2 ≤ √ 2ε‖MS‖2 + (6 √ ε)(1 + ‖µG,A Omitted Details from Section 3,[0],[0]
"− µS‖2) .
",A Omitted Details from Section 3,[0],[0]
Moving the ‖µG,A Omitted Details from Section 3,[0],[0]
"− µS‖2 terms to the LHS, using 6 √ ε ≤ 1/2, gives
‖µG",A Omitted Details from Section 3,[0],[0]
"− µS‖2 ≤ √ 2ε‖MS‖2 + 12 √ ε .
",A Omitted Details from Section 3,[0],[0]
"Since λ∗ = ‖MS‖2, the correctness if we return the empirical mean is immediate.
",A Omitted Details from Section 3,[0],[0]
Corollary A.25.,A Omitted Details from Section 3,[0],[0]
"If λ∗ ≤ 9, we have that ‖µG",A Omitted Details from Section 3,[0],[0]
"− µS‖2 = O( √ ε).
",A Omitted Details from Section 3,[0],[0]
"From now on, we assume λ∗ > 9.",A Omitted Details from Section 3,[0],[0]
In this case we have ‖µG − µS‖22 ≤ O(ελ∗).,A Omitted Details from Section 3,[0],[0]
"Using Lemma A.22, we have
‖MG‖2 ≤ 2 +O(ελ∗) ≤ 2 + λ∗/5
for sufficiently small ε.",A Omitted Details from Section 3,[0],[0]
"Thus, we have that
v∗TMSv ∗ ≥ 4v∗TMGv∗ .",A Omitted Details from Section 3,[0],[0]
"(7)
Now we can show that in expectation, we throw out many more corrupted points from E than from G\L:
Lemma A.26.",A Omitted Details from Section 3,[0],[0]
"Let S′ = G∪E′ \L′ for disjoint E′, L′ be the set of samples returned by the iteration.",A Omitted Details from Section 3,[0],[0]
"Then we have EZ [|E′|+ 2|L′|] ≤ |E|+ 2|L|.
Proof.",A Omitted Details from Section 3,[0],[0]
Let a = maxx∈S |v∗ · x− µS,A Omitted Details from Section 3,[0],[0]
|.,A Omitted Details from Section 3,[0],[0]
"Firstly, we look at the expected number of samples we reject:
EZ",A Omitted Details from Section 3,[0],[0]
"[|S′|]− |S| = EZ [ |S| Pr
X∈uS",A Omitted Details from Section 3,[0],[0]
"[|X − µS | ≥ aZ] ] = |S|
∫ 1 0",A Omitted Details from Section 3,[0],[0]
"Pr X∈uS [ |v∗ · (X − µS)| ≥ ax ] 2xdx
= |S| ∫ a
0
Pr X∈uS
[ |v∗ · (X − µS)| ≥ T ] (2T/a)dT
= |S|EX∈uS",A Omitted Details from Section 3,[0],[0]
"[ (v∗ · (X − µS))2 ] /a = (|S|/a) · v∗TMSv∗ .
",A Omitted Details from Section 3,[0],[0]
"Next, we look at the expected number of false positive samples we reject, i.e., those in L′ \",A Omitted Details from Section 3,[0],[0]
"L.
EZ",A Omitted Details from Section 3,[0],[0]
[|L′|]− |L| = EZ [ (|G| − |L|),A Omitted Details from Section 3,[0],[0]
"Pr
X∈uG\L
[ |X − µS",A Omitted Details from Section 3,[0],[0]
| ≥ T ]] ≤,A Omitted Details from Section 3,[0],[0]
"EZ [ |G| Pr
X∈uG [|v∗ · (X − µS)| ≥ aZ]",A Omitted Details from Section 3,[0],[0]
"] = |G|
∫ 1 0",A Omitted Details from Section 3,[0],[0]
"Pr X∈uG [|v∗ · (X − µS)| ≥ ax]2x dx
= |G| ∫ a
0
Pr X∈uG
[|v∗ · (X − µS)| ≥ T ](2T/a) dT
≤ |G| ∫ ∞
0
Pr X∈uG
[|v∗ · (X − µS)| ≥ T ](2T/a) dT
= |G|EX∈uG [ (v∗ · (X − µS))2 ] /a = (|G|/a) ·",A Omitted Details from Section 3,[0],[0]
"v∗TMGv∗ .
",A Omitted Details from Section 3,[0],[0]
"Using (7), we have |S|v∗TMSv∗ ≥ 4|G|v∗TMGv∗ and so",A Omitted Details from Section 3,[0],[0]
EZ [S′] − S ≥ 3(EZ,A Omitted Details from Section 3,[0],[0]
[L′],A Omitted Details from Section 3,[0],[0]
− L).,A Omitted Details from Section 3,[0],[0]
Now consider that |S′| = |G|+,A Omitted Details from Section 3,[0],[0]
|E′| − |L′| = |S| − |E|+ |E′|+ |L|,A Omitted Details from Section 3,[0],[0]
"− |L′|, and thus |S′| − |S| =",A Omitted Details from Section 3,[0],[0]
|E| − |E′|+ |L′|,A Omitted Details from Section 3,[0],[0]
− |L|.,A Omitted Details from Section 3,[0],[0]
This yields that |E| − EZ [|E′|] ≥ 2(EZ,A Omitted Details from Section 3,[0],[0]
"[L′]− L), which can be rearranged to EZ [|E′|+ 2|L′|]",A Omitted Details from Section 3,[0],[0]
"≤ |E|+ 2|L|.
",A Omitted Details from Section 3,[0],[0]
Proof of Proposition A.21.,A Omitted Details from Section 3,[0],[0]
"If λ∗ ≤ 9, then we return the mean in Step 5, and by Corollary A.25, ‖µS−µP ‖2 ≤ O( √ ε).
",A Omitted Details from Section 3,[0],[0]
"If λ∗ > 9, then we return S′. Since at least one element of S has |v∗ ·X| = maxx∈S |v∗ ·X|, whatever value of Z is drawn, we still remove at least one element, and so have S′ ⊂ S. By Lemma A.26, we have EZ [|E′|+ 2|L′|]",A Omitted Details from Section 3,[0],[0]
"≤ |E|+ 2|L|.
",A Omitted Details from Section 3,[0],[0]
Proof of Theorem A.16.,A Omitted Details from Section 3,[0],[0]
"Our input is a set S of N = Θ((d/ε) log d) ε-corrupted samples so that with probability 9/10, S is a 2ε-corrupted set of ε-good samples for P by Lemmas A.18 and A.20.",A Omitted Details from Section 3,[0],[0]
We have a set S = G ∪ E′ \,A Omitted Details from Section 3,[0],[0]
"L, where G′ is an ε-good set, |E| ≤ 2ε, and |L| ≤ ε.",A Omitted Details from Section 3,[0],[0]
"Then, we iteratively apply FilterUnder2ndMoment until it outputs an approximation to the mean.",A Omitted Details from Section 3,[0],[0]
"Since each iteration removes a sample, this must happen within N iterations.",A Omitted Details from Section 3,[0],[0]
"The algorithm takes at most poly(N, d) = poly(d, 1/ε) time.
",A Omitted Details from Section 3,[0],[0]
"As long as we can show that the conditions of Proposition A.21 hold in each iteration, it ensures that ‖µS − µP ‖2 ≤ O( √ ε).",A Omitted Details from Section 3,[0],[0]
"However, the condition that |L| ≤ 9ε|S| need not hold in general.",A Omitted Details from Section 3,[0],[0]
"Although in expectation we reject many more samples in E than G, it is possible that we are unlucky and reject many samples in G, which could make L large in the next iteration.",A Omitted Details from Section 3,[0],[0]
"Thus, we need a bound on the probability that we ever have |L| > 9ε.
",A Omitted Details from Section 3,[0],[0]
We analyze the following procedure: We iteratively run FilterUnder2ndMoment starting with a set Si ∪ Ei \ Li of samples with S0 = S and producing a set Si+1 = G ∪ Ei+1,A Omitted Details from Section 3,[0],[0]
\,A Omitted Details from Section 3,[0],[0]
Li+1.,A Omitted Details from Section 3,[0],[0]
We stop if we output an approximation to the mean or if |Li+1| ≥ 13ε|S|.,A Omitted Details from Section 3,[0],[0]
"Since we do now always satisfy the conditions of Proposition A.21, this gives that EZ",A Omitted Details from Section 3,[0],[0]
[|Ei+1|+ |Li+1|],A Omitted Details from Section 3,[0],[0]
= |Ei|+ 2|Li|.,A Omitted Details from Section 3,[0],[0]
"This expectation is conditioned on the
state of the algorithm after previous iterations, which is determined by Si.",A Omitted Details from Section 3,[0],[0]
"Thus, if we consider the random variables Xi = |Ei| + 2|Li|, then we have E[Xi+1|Si] ≤",A Omitted Details from Section 3,[0],[0]
"Xi, i.e., the sequence Xi is a sub-martingale with respect to Xi.",A Omitted Details from Section 3,[0],[0]
"Using the convention that Si+1 = Si, if we stop in less than i iterations, and recalling that we always stop in N iterations, the algorithm fails if and only if |LN",A Omitted Details from Section 3,[0],[0]
| > 9ε|S|.,A Omitted Details from Section 3,[0],[0]
"By a simple induction or standard results on sub-martingales, we have E[XN ] ≤ X0.",A Omitted Details from Section 3,[0],[0]
Now X0 = |E0|+ 2|L0| ≤ 3ε|S|.,A Omitted Details from Section 3,[0],[0]
"Thus, E[XN ] ≤ 3ε|S|.",A Omitted Details from Section 3,[0],[0]
"By Markov’s inequality, except with probability 1/6, we have XN ≤ 18ε|S|.",A Omitted Details from Section 3,[0],[0]
"In this case, |LN | ≤ XN/2 ≤ 9ε|S|.",A Omitted Details from Section 3,[0],[0]
"Therefore, the probability that we ever have |Li| > 9ε is at most 1/6.
",A Omitted Details from Section 3,[0],[0]
"By a union bound, the probability that the uncorrupted samples satisfy Lemma A.18 and Proposition A.21 applies to every iteration is at least 9/10−1/6 ≥ 2/3.",A Omitted Details from Section 3,[0],[0]
"Thus, with at least 2/3 probability, the algorithm outputs a vector µ̂ with ‖µ̂− µP",A Omitted Details from Section 3,[0],[0]
‖2 ≤,A Omitted Details from Section 3,[0],[0]
"O( √ ε).
",A Omitted Details from Section 3,[0],[0]
"A.3 Robust Covariance Estimation
In this subsection, we give a near sample-optimal efficient robust estimator for the covariance of a zero-mean Gaussian density, thus proving Theorem 3.3.",A Omitted Details from Section 3,[0],[0]
Our algorithm is essentially identical to the filtering algorithm given in Section 8.2 of [DKK+16].,A Omitted Details from Section 3,[0],[0]
As in Section A.1 the only difference is a weaker definition of the “good set of samples” (Definition A.27) and a concentration argument (Lemma A.28) showing that a random set of uncorrupted samples of the appropriate size is good with high probability.,A Omitted Details from Section 3,[0],[0]
"Given these, the analysis of this subsection follows straightforwardly from the analysis in Section 8.2 of [DKK+16] by plugging in the modified parameters.
",A Omitted Details from Section 3,[0],[0]
"The algorithm Filter-Gaussian-Unknown-Covariance to robustly estimate the covariance of a mean 0 Gaussian in [DKK+16] is as follows:
Algorithm 4 Filter algorithm for a Gaussian with unknown covariance matrix.
1: procedure Filter-Gaussian-Unknown-Covariance(S′, ε, τ) input: A multiset S′ such that there exists an (ε, τ)-good set S with ∆(S, S′)",A Omitted Details from Section 3,[0],[0]
"≤ 2ε output: Either a set S′′ with ∆(S, S′′) < ∆(S, S′) or the parameters of a Gaussian G′ with dTV (G,G
′) = O(ε log(1/ε)).
",A Omitted Details from Section 3,[0],[0]
Let C > 0 be a sufficiently large universal constant.,A Omitted Details from Section 3,[0],[0]
2: Let Σ′ be the matrix EX∈uS′ [XXT ] and let G′ be the mean,A Omitted Details from Section 3,[0],[0]
0 Gaussian with covariance matrix Σ′. 3: if there is any x ∈ S′,A Omitted Details from Section 3,[0],[0]
so that xT,A Omitted Details from Section 3,[0],[0]
(Σ′)−1x ≥ Cd log(|S′|/τ) then 4: return S′′ = S′ − {x : xT,A Omitted Details from Section 3,[0],[0]
(Σ′)−1x ≥ Cd log(|S′|/τ)}.,A Omitted Details from Section 3,[0],[0]
"5: Compute an approximate eigendecomposition of Σ′ and use it to compute Σ′−1/2 6: Let x(1), . . .",A Omitted Details from Section 3,[0],[0]
", x(|S′|) be the elements of S ′. 7:",A Omitted Details from Section 3,[0],[0]
"For i = 1, . . .",A Omitted Details from Section 3,[0],[0]
", |S′|, let y(i) = Σ′−1/2x(i) and z(i) = y⊗2(i) .",A Omitted Details from Section 3,[0],[0]
8: Let TS′ = −I[I[T + (1/|S′|) ∑|S′| i=1,A Omitted Details from Section 3,[0],[0]
z(i)z,A Omitted Details from Section 3,[0],[0]
"T (i).
",A Omitted Details from Section 3,[0],[0]
"9: Approximate the top eigenvalue λ∗ and corresponding unit eigenvector v∗ of TS′ .. 10: Let p∗(x) = 1√
2 ((Σ′−1/2x)T v∗](Σ′−1/2x)− tr(v∗]))
11: if λ∗ ≤ (1 + Cε log2(1/ε))QG′(p∗) then 12: return G′ 13: Let µ be the median value of p∗(X) over X ∈ S′. 14: Find a T ≥ C ′",A Omitted Details from Section 3,[0],[0]
"so that
Pr X∈uS′
(|p∗(X)− µ| ≥ T + 4/3) ≥ Tail(T, d, ε, τ)
15: return S′′ = {X ∈ S′ : |p∗(X)− µ| < T}.
",A Omitted Details from Section 3,[0],[0]
"In [DKK+16], we take Tail(T, d, ε, τ) = 12 exp(−T ) + 3ε/(d",A Omitted Details from Section 3,[0],[0]
"log(N/τ))2, where N = Θ((d log(d/ετ))6/ε2) is the number of samples we took there.
",A Omitted Details from Section 3,[0],[0]
"To get a near sample-optimal algorithms, we will need a weaker definition of a good set.",A Omitted Details from Section 3,[0],[0]
"To use this, we will need to weaken the tail bound in the algorithm to Tail(T, d, ε, τ) = ε/(T 2 log2(T )), when T ≥ 10 log(1/ε).",A Omitted Details from Section 3,[0],[0]
"For T ≤ 10 log(1/ε), we take Tail(T, d, ε, τ) = 1",A Omitted Details from Section 3,[0],[0]
so that we always choose T ≥ 10 log(1/ε).,A Omitted Details from Section 3,[0],[0]
"It is easy to show
that the integrals of this tail bound used in the proofs of Lemma 8.19 and Claim 8.22 of [DKK+16] have similar bounds.",A Omitted Details from Section 3,[0],[0]
"Thus, our analysis here will sketch that these tail bounds hold for a set of Ω(d2 log5(d/ετ)/ε2) samples from the Guassian.
",A Omitted Details from Section 3,[0],[0]
"Firstly, we state the new, weaker, definition of a good set:
Definition A.27.",A Omitted Details from Section 3,[0],[0]
Let G be a Gaussian in Rd with mean 0 and covariance Σ.,A Omitted Details from Section 3,[0],[0]
Let ε > 0 be sufficiently small.,A Omitted Details from Section 3,[0],[0]
"We say that a multiset S of points in Rd is ε-good with respect to G if the following hold:
1.",A Omitted Details from Section 3,[0],[0]
"For all x ∈ S, xTΣ−1x < d+O( √ d log(d/ε)).
2.",A Omitted Details from Section 3,[0],[0]
We have that ‖Σ−1/2Cov(S)Σ−1/2,A Omitted Details from Section 3,[0],[0]
"− I‖F = O(ε).
",A Omitted Details from Section 3,[0],[0]
3.,A Omitted Details from Section 3,[0],[0]
"For all even degree-2 polynomials p, we have that Var(p(S)) = Var(p(G))(1 +O(ε)).
4.",A Omitted Details from Section 3,[0],[0]
For p an even degree-2 polynomial with E[p(G)] = 0 and Var(p(G)),A Omitted Details from Section 3,[0],[0]
"= 1, and for any T > 10 log(1/ε) we have that
Pr x∈uS
(|p(x)| > T ) ≤ ε/(T 2 log2(T )).
",A Omitted Details from Section 3,[0],[0]
It is easy to see that the algorithm and analysis of [DKK+16] can be pushed through using the above weaker definition.,A Omitted Details from Section 3,[0],[0]
"That is, if S is a good set, then G can be recovered to Õ(ε) error from an ε-corrupted version of S. Our main task will be to show that random sets of the appropriate size are good with high probability.
",A Omitted Details from Section 3,[0],[0]
Proposition A.28.,A Omitted Details from Section 3,[0],[0]
Let N be a sufficiently large constant multiple of d2 log5(d/ε)/ε2.,A Omitted Details from Section 3,[0],[0]
"Then a set S of N independent samples from G is ε-good with respect to G with high probability.
",A Omitted Details from Section 3,[0],[0]
Proof.,A Omitted Details from Section 3,[0],[0]
"First, note that it suffices to prove this when G = N(0, I).",A Omitted Details from Section 3,[0],[0]
Condition 1 follows by standard concentration bounds on ‖x‖22.,A Omitted Details from Section 3,[0],[0]
Condition 2 follows by estimating the entry-wise error between Cov(S) and I. Condition 3 is slightly more involved.,A Omitted Details from Section 3,[0],[0]
"Let {pi} be an orthonormal basis for the set of even, degree-2, mean-0 polynomials with respect to G. Define the matrix Mi,j = Ex∈uS",A Omitted Details from Section 3,[0],[0]
"[pi(x)pj(x)]− δi,j .",A Omitted Details from Section 3,[0],[0]
This condition is equivalent to ‖M‖2 = O(ε).,A Omitted Details from Section 3,[0],[0]
"Thus, it suffices to show that for every v with ‖v‖2 = 1 that vTMv = O(ε).",A Omitted Details from Section 3,[0],[0]
It actually suffices to consider a cover of such v’s.,A Omitted Details from Section 3,[0],[0]
"Note that this cover will be of size 2O(d
2).",A Omitted Details from Section 3,[0],[0]
"For each v, let pv = ∑ i vipi.",A Omitted Details from Section 3,[0],[0]
We need to show that Var(pv(S)),A Omitted Details from Section 3,[0],[0]
= 1 + O(ε).,A Omitted Details from Section 3,[0],[0]
"We can show this happens with probability 1− 2−Ω(d2), and thus it holds for all v in our cover by a union bound.",A Omitted Details from Section 3,[0],[0]
Condition 4 is substantially the most difficult of these conditions to prove.,A Omitted Details from Section 3,[0],[0]
"Naively, we would want to find a cover of all possible p and all possible T , and bound the probability that the desired condition fails.",A Omitted Details from Section 3,[0],[0]
"Unfortunately, the best a priori bound on Pr(|p(G)| > T ) are on the order of exp(−T ).",A Omitted Details from Section 3,[0],[0]
"As our cover would need to be of size 2d 2
or so, to make this work with T = d, we would require on the order of d3 samples in order to make this argument work.
",A Omitted Details from Section 3,[0],[0]
"However, we will note that this argument is sufficient to cover the case of T < 10 log(1/ε) log2(d/ε).",A Omitted Details from Section 3,[0],[0]
"Fortunately, most such polynomials p satisfy much better tail bounds.",A Omitted Details from Section 3,[0],[0]
"Note that any even, mean zero
polynomial p can be written in the form p(x) = xTAx−tr(A) for some matrix A.",A Omitted Details from Section 3,[0],[0]
We call A the associated matrix to p.,A Omitted Details from Section 3,[0],[0]
"We note by the Hanson-Wright inequality that Pr(|p(G)| > T ) = exp(−Ω(min((T/‖A‖F )2, T/‖A‖2))).",A Omitted Details from Section 3,[0],[0]
"Therefore, the tail bounds above are only as bad as described when A has a single large eigenvalue.",A Omitted Details from Section 3,[0],[0]
"To take advantage of this, we will need to break p into parts based on the size of its eigenvalues.",A Omitted Details from Section 3,[0],[0]
"We begin with a definition:
Definition A.29.",A Omitted Details from Section 3,[0],[0]
"Let Pk be the set of even, mean-0, degree-2 polynomials, so that the associated matrix A satisfies:
1.",A Omitted Details from Section 3,[0],[0]
rank(A) ≤,A Omitted Details from Section 3,[0],[0]
k 2.,A Omitted Details from Section 3,[0],[0]
‖A‖2 ≤ 1/,A Omitted Details from Section 3,[0],[0]
"√ k.
Note that for p ∈",A Omitted Details from Section 3,[0],[0]
Pk,A Omitted Details from Section 3,[0],[0]
"that |p(x)| ≤ |x|2/ √ k + √ k. Importantly, any polynomial can be written in terms of these sets.
",A Omitted Details from Section 3,[0],[0]
Lemma A.30.,A Omitted Details from Section 3,[0],[0]
"Let p be an even, degree-2 polynomial with E[p(G)] = 0,Var(p(G))",A Omitted Details from Section 3,[0],[0]
= 1.,A Omitted Details from Section 3,[0],[0]
"Then if t = blog2(d)c, it is possible to write p = 2(p1 + p2 + . .",A Omitted Details from Section 3,[0],[0]
.+,A Omitted Details from Section 3,[0],[0]
"p2t + pd) where pk ∈ Pk.
",A Omitted Details from Section 3,[0],[0]
Proof.,A Omitted Details from Section 3,[0],[0]
Let A be the associated matrix to p.,A Omitted Details from Section 3,[0],[0]
Note that ‖A‖F = Var p = 1.,A Omitted Details from Section 3,[0],[0]
Let Ak be the matrix corresponding to the top k eigenvalues of A.,A Omitted Details from Section 3,[0],[0]
"We now let p1 be the polynomial associated to A1/2, p2 be associated to (A2 − A1)/2, p4 be associated to (A4 − A2)/2, and so on.",A Omitted Details from Section 3,[0],[0]
It is clear that p = 2(p1 + p2 + . . .,A Omitted Details from Section 3,[0],[0]
+,A Omitted Details from Section 3,[0],[0]
p2t + pd).,A Omitted Details from Section 3,[0],[0]
It is also clear that the matrix associated to pk has rank at most k.,A Omitted Details from Section 3,[0],[0]
"If the matrix associated to pk had an eigenvalue more than 1/ √ k, it would need to be the case that the k/2nd largest eigenvalue of A had size at least 2/ √ k.",A Omitted Details from Section 3,[0],[0]
"This is impossible since the sum of the squares of the eigenvalues of A is at most 1.
",A Omitted Details from Section 3,[0],[0]
"This completes our proof.
",A Omitted Details from Section 3,[0],[0]
"We will also need covers of each of these sets Pk.
Lemma A.31.",A Omitted Details from Section 3,[0],[0]
"For each k, there exists a set Ck ⊂ Pk so that
1.",A Omitted Details from Section 3,[0],[0]
For each p ∈,A Omitted Details from Section 3,[0],[0]
"Pk there exists a q ∈ Ck so that ‖p(G)− q(G)‖2 ≤ (ε/d)2.
2.",A Omitted Details from Section 3,[0],[0]
|Ck| = 2O(dk log(d/ε)).,A Omitted Details from Section 3,[0],[0]
Proof.,A Omitted Details from Section 3,[0],[0]
We note that any such p is associated to a matrix A of the form A = ∑k i=1,A Omitted Details from Section 3,[0],[0]
"λiviv T i , for λi ∈",A Omitted Details from Section 3,[0],[0]
"[0, 1/ √ k] and vi orthonormal.",A Omitted Details from Section 3,[0],[0]
It suffices to let q correspond to the matrix A ′ = ∑k i=1,A Omitted Details from Section 3,[0],[0]
µiwiw T i for with |λi−µi| < (ε/d)3 and |vi − wi| <,A Omitted Details from Section 3,[0],[0]
(ε/d)3 for all i.,A Omitted Details from Section 3,[0],[0]
It is easy to let µi and wi range over covers of the interval and the sphere with appropriate errors.,A Omitted Details from Section 3,[0],[0]
This gives a set of possible q’s of size 2O(dk log(d/ε)) as desired.,A Omitted Details from Section 3,[0],[0]
"Unfortunately, some of these q will not be in Pk as they will have eigenvalues that are too large.",A Omitted Details from Section 3,[0],[0]
"However, this is easily fixed by replacing each such q by the closest element of Pk.",A Omitted Details from Section 3,[0],[0]
"This completes our proof.
",A Omitted Details from Section 3,[0],[0]
"We next will show that these covers are sufficient to express any polynomial.
",A Omitted Details from Section 3,[0],[0]
Lemma A.32.,A Omitted Details from Section 3,[0],[0]
Let p be an even degree-2 polynomial with E[p(G)],A Omitted Details from Section 3,[0],[0]
= 0 and Var(p(G)),A Omitted Details from Section 3,[0],[0]
= 1.,A Omitted Details from Section 3,[0],[0]
"It is possible to write p as a sum of O(log(d)) elements of some Ck plus another polynomial of L2 norm at most ε/d.
Proof.",A Omitted Details from Section 3,[0],[0]
"Combining the above two lemmas we have that any such p can be written as
p = (q1 + p1) + (q2 + p2) + . . .",A Omitted Details from Section 3,[0],[0]
(q2t + p2t),A Omitted Details from Section 3,[0],[0]
+ (,A Omitted Details from Section 3,[0],[0]
qd + pd) = q1 + q2 + . .,A Omitted Details from Section 3,[0],[0]
.+ q 2t +,A Omitted Details from Section 3,[0],[0]
"qd + p′ ,
where qk above is in Ck and ‖pk(G)‖2 < (ε/d)2.",A Omitted Details from Section 3,[0],[0]
"Thus, p′ = p1 + p2 + . . .",A Omitted Details from Section 3,[0],[0]
+,A Omitted Details from Section 3,[0],[0]
p2t + pd has ‖p′(G)‖2 ≤ (ε/d).,A Omitted Details from Section 3,[0],[0]
"This completes the proof.
",A Omitted Details from Section 3,[0],[0]
"The key observation now is that if |p(x)| ≥ T for ‖x‖2 ≤ √ d/ε, then writing p = q1 +q2 +q4 + . .",A Omitted Details from Section 3,[0],[0]
.+qd+p ′,A Omitted Details from Section 3,[0],[0]
"as above, it must be the case that |qk(x)| >",A Omitted Details from Section 3,[0],[0]
(T−1)/(2 log(d)),A Omitted Details from Section 3,[0],[0]
"for some k. Therefore, to prove our main result, it suffices to show that, with high probability over the choice of S, for any T ≥ 10 log(1/ε) log2(d/ε) and any q ∈ Ck for some k, that Prx∈uS(|q(x)| >",A Omitted Details from Section 3,[0],[0]
T/(2 log(d))),A Omitted Details from Section 3,[0],[0]
"< ε/(2T 2 log
2(T ) log(d)).",A Omitted Details from Section 3,[0],[0]
"Equivalently, it suffices to show that for T ≥ 10 log(1/ε) log(d/ε) it holds Prx∈uS(|q(x)| >",A Omitted Details from Section 3,[0],[0]
T/(2 log(d))),A Omitted Details from Section 3,[0],[0]
"< ε/(2T 2 log
2(T ) log2(d)).",A Omitted Details from Section 3,[0],[0]
"Note that this holds automatically for T > (d/ε), as p(x) cannot possibly be that large for ‖x‖2 ≤ √ d/ε.",A Omitted Details from Section 3,[0],[0]
"Furthermore, note that losing a constant factor in the probability, it suffices to show this only for T a power of 2.
",A Omitted Details from Section 3,[0],[0]
"Therefore, it suffices to show for every k ≤ d, every q ∈ Ck and every d/ √ kε T log(1/ε) log(d/ε)
that with probability at least 1 − 2−Ω(dk log(d/ε)) over the choice of S we have that Prx∈uS(|q(x)| > T ) ε/(T 2 log4(d/ε)).",A Omitted Details from Section 3,[0],[0]
"However, by the Hanson-Wright inequality, we have that
Pr(|q(G)| > T ) = exp(−Ω(min(T 2, T √ k)))",A Omitted Details from Section 3,[0],[0]
"< (ε/(T 2 log4(d/ε)))2 .
",A Omitted Details from Section 3,[0],[0]
"Therefore, by Chernoff bounds, the probability that more than a ε/(T 2 log4(d/ε))-fraction of the elements of S satisfy this property is at most
exp(−Ω(min(T 2, T √ k))|S|ε/(T 2 log4(d/ε)))",A Omitted Details from Section 3,[0],[0]
"= exp(−Ω(|S|ε/(log4(d/ε)) min(1, √ k/T )))
",A Omitted Details from Section 3,[0],[0]
≤ exp(−Ω(|S|ε2/(log4(d/ε))k/d)),A Omitted Details from Section 3,[0],[0]
"≤ exp(−Ω(dk log(d/ε))) ,
as desired.",A Omitted Details from Section 3,[0],[0]
This completes our proof.,A Omitted Details from Section 3,[0],[0]
"B.1 Full description of the distributions for experiments
Here we formally describe the distributions we used in our experiments.",B Omitted Details from Section 5,[0],[0]
"In all settings, our goal was to find noise distributions so that noise points were not “obvious” outliers, in the sense that there is no obvious pointwise pruning process which could throw away the noise points, which still gave the algorithms we tested the most difficulty.",B Omitted Details from Section 5,[0],[0]
"We again remark that while other algorithms had varying performances depending on the noise distribution, it seemed that the performance of ours was more or less unaffected by it.
",B Omitted Details from Section 5,[0],[0]
"Distribution for the synthetic mean experiment Our uncorrupted points were generated by N (µ, I), where µ is the all-ones vector.",B Omitted Details from Section 5,[0],[0]
"Our noise distribution is given as
N = 1
2 Π1 +
1 2 Π2 ,
where Π1 is the product distribution over the hypercube where every coordinate is 0 or 1 with probability 1/2, and Π2 is a product distribution where the first coordinate is ether 0 or 12 with equal probability, the second coordinate is −2 or 0 with equal probability, and all remaining coordinates are zero.
",B Omitted Details from Section 5,[0],[0]
"Distribution for the synthetic covariance experiment For the isotropic synthetic covariance experiment, our uncorrupted points were generated by N (0, I), and the noise points were all zeros.",B Omitted Details from Section 5,[0],[0]
"For the skewed synthetic covariance experiment, our uncorrupted points were generated by N (0, I + 100e1eT1 ), where e1 is the first unit vector, and our noise points were generated as follows: we took a fixed random rotation of points of the form Yi ∼ Π, where Π is a product distribution whose first d/2 coordinates are each uniformly selected from {−0.5, 0, 0.5}, and whose next d/2−1 coordinates are each 0.8×Ai, where for each coordinate",B Omitted Details from Section 5,[0],[0]
"i, Ai is an independent random integer between −2 and 2, and whose last coordinate is a uniformly random integer between [−100, 100].
",B Omitted Details from Section 5,[0],[0]
"Setup for the semi-synthetic geographic experiment We took the 20 dimensional data from [NJB+08], which was diagonalized, and randomly rotated it.",B Omitted Details from Section 5,[0],[0]
"This was to simulate the higher dimensional case, since the singular vectors that [NJB+08] obtained did not seem to be sparse or analytically sparse.",B Omitted Details from Section 5,[0],[0]
"Our noise was distributed as Π, where Π is a product distribution whose first d/2 coordinates are each uniformly random integers between 0 and 2 and whose last d/2 coordinates are each uniformly randomly either 2 or 3, all scaled by a factor of 1/24.
",B Omitted Details from Section 5,[0],[0]
"B.2 Comparison with other robust PCA methods on semi-synthetic data
In addition to comparing our results with simple pruning techniques, as we did in Figure 3 in the main text, we also compared our algorithm with implementations of other robust PCA techniques from the literature with accessible implementations.",B Omitted Details from Section 5,[0],[0]
"In particular, we compared our technique with RANSAC-based techniques, LRVCov, two SDPs ([CLMW11, XCS10]) for variants of robust PCA, and an algorithm proposed by [CLMW11] to speed up their SDP based on alternating descent.",B Omitted Details from Section 5,[0],[0]
"For the SDPs, since black box methods were too slow to run on the full data set (as [CLMW11] mentions, black-box solvers for the SDPs are impractical above perhaps 100 data points), we subsample the data, and run the SDP on the subsampled data.",B Omitted Details from Section 5,[0],[0]
"For each of these methods, we ran the algorithm on the true data points plus noise, where the noise was generated as described above.",B Omitted Details from Section 5,[0],[0]
"We then take the estimate of the covariance it outputs, and project the data points onto the top two singular values of this matrix, and plot the results in Figure 4.
",B Omitted Details from Section 5,[0],[0]
Similar results occurred for most noise patterns we tried.,B Omitted Details from Section 5,[0],[0]
"We found that only our algorithm and LRVCov were able to reasonably reconstruct Europe, in the presence of this noise.",B Omitted Details from Section 5,[0],[0]
"It is hard to judge qualitatively which of the two maps generated is preferable, but it seems that ours stretches the picture somewhat less than LRVCov.",B Omitted Details from Section 5,[0],[0]
Robust estimation is much more challenging in high dimensions than it is in one dimension: Most techniques either lead to intractable optimization problems or estimators that can tolerate only a tiny fraction of errors.,abstractText,[0],[0]
"Recent work in theoretical computer science has shown that, in appropriate distributional models, it is possible to robustly estimate the mean and covariance with polynomial time algorithms that can tolerate a constant fraction of corruptions, independent of the dimension.",abstractText,[0],[0]
"However, the sample and time complexity of these algorithms is prohibitively large for high-dimensional applications.",abstractText,[0],[0]
"In this work, we address both of these issues by establishing sample complexity bounds that are optimal, up to logarithmic factors, as well as giving various refinements that allow the algorithms to tolerate a much larger fraction of corruptions.",abstractText,[0],[0]
"Finally, we show on both synthetic and real data that our algorithms have state-of-the-art performance and suddenly make high-dimensional robust estimation a realistic possibility.",abstractText,[0],[0]
Being Robust (in High Dimensions) Can Be Practical∗,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 729–740 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1068
Automatic political preference prediction from social media posts has to date proven successful only in distinguishing between publicly declared liberals and conservatives in the US. This study examines users’ political ideology using a sevenpoint scale which enables us to identify politically moderate and neutral users – groups which are of particular interest to political scientists and pollsters. Using a novel data set with political ideology labels self-reported through surveys, our goal is two-fold: a) to characterize the political groups of users through language use on Twitter; b) to build a fine-grained model that predicts political ideology of unseen users. Our results identify differences in both political leaning and engagement and the extent to which each group tweets using political keywords. Finally, we demonstrate how to improve ideology prediction accuracy by exploiting the relationships between the user groups.",text,[0],[0]
Social media is used by people to share their opinions and views.,1 Introduction,[0],[0]
"Unsurprisingly, an important part of the population shares opinions and news related to politics or causes they support, thus offering strong cues about their political preferences and ideologies.",1 Introduction,[0],[0]
"In addition, political membership is also predictable purely from one’s interests or demographics — it is much more likely for a religious person to be conservative or for a younger person to lean liberal (Ellis and Stimson, 2012).
",1 Introduction,[0],[0]
"∗ Work carried out during a research visit at the University of Pennsylvania
User trait prediction from text is based on the assumption that language use reflects a user’s demographics, psychological states or preferences.",1 Introduction,[0],[0]
"Applications include prediction of age (Rao et al., 2010; Flekova et al., 2016b), gender (Burger et al., 2011; Sap et al., 2014), personality (Schwartz et al., 2013; Preoţiuc-Pietro et al., 2016), socioeconomic status (Preoţiuc-Pietro et al., 2015a,b; Liu et al., 2016c), popularity (Lampos et al., 2014) or location (Cheng et al., 2010).
",1 Introduction,[0],[0]
"Research on predicting political orientation has focused on methodological improvements (Pennacchiotti and Popescu, 2011) and used data sets with publicly stated dichotomous political orientation labels due to their easy accessibility (Sylwester and Purver, 2015).",1 Introduction,[0],[0]
"However, these data sets are not representative samples of the entire population (Cohen and Ruths, 2013) and do not accurately reflect the variety of political attitudes and engagement (Kam et al., 2007).
",1 Introduction,[0],[0]
"For example, we expect users who state their political affiliation in their profile description, tweet with partisan hashtags or appear in public party lists to use social media as a means of popularizing and supporting their political beliefs (BarberASa, 2015).",1 Introduction,[0],[0]
Many users may choose not to publicly post about their political preference for various social goals or perhaps this preference may not be strong or representative enough to be disclosed online.,1 Introduction,[0],[0]
Dichotomous political preference also ignores users who do not have a political ideology.,1 Introduction,[0],[0]
"All of these types of users are very important for researchers aiming to understand group preferences, traits or moral values (Lewis and Reiley, 2014; Hersh, 2015).
",1 Introduction,[0],[0]
"The most common political ideology spectrum in the US is the conservative – liberal (Ellis and Stimson, 2012).",1 Introduction,[0],[0]
"We collect a novel data set of Twitter users mapped to this seven-point spectrum which allows us to:
729
1.",1 Introduction,[0],[0]
Uncover the differences in language use between ideological groups; 2.,1 Introduction,[0],[0]
Develop a user-level political ideology prediction algorithm that classifies all levels of engagement and leverages the structure in the political ideology spectrum.,1 Introduction,[0],[0]
"First, using a broad range of language features
including unigrams, word clusters and emotions, we study the linguistic differences between the two ideologically extreme groups, the two ideologically moderate groups and between both extremes and moderates in order to provide insight into the content they post on Twitter.",1 Introduction,[0],[0]
"In addition, we examine the extent to which the ideological groups in our data set post about politics and compare it to a data set obtained similarly to previous work.
",1 Introduction,[0],[0]
"In prediction experiments, we show how accurately we can distinguish between opposing ideological groups in various scenarios and that previous binary political orientation prediction has been oversimplified.",1 Introduction,[0],[0]
"Then, we measure the extent to which we can predict the two dimensions of political leaning and engagement.",1 Introduction,[0],[0]
"Finally, we build an ideology classifier in a multi-task learning setup that leverages the relationships between groups.1",1 Introduction,[0],[0]
"Automatically inferring user traits from their online footprints is a prolific topic of research, enabled by the increasing availability of user generated data and advances in machine learning.",2 Related Work,[0],[0]
"Beyond its research oriented goals, user profiling has important industry applications in online marketing, personalization or large-scale audience profiling.",2 Related Work,[0],[0]
"To this end, researchers have used a wide range of types of online footprints, including video (Subramanian et al., 2013), audio (Alam and Riccardi, 2014), text (Preoţiuc-Pietro et al., 2015a), profile images (Liu et al., 2016a), social data (Van Der Heide et al., 2012; Hall et al., 2014), social networks (Perozzi and Skiena, 2015; Rout et al., 2013), payment data (Wang et al., 2016) and endorsements (Kosinski et al., 2013).
",2 Related Work,[0],[0]
"Political orientation prediction has been studied in two related, albeit crucially different scenarios, as also identified in (Zafar et al., 2016).",2 Related Work,[0],[0]
"First, researchers aimed to identify and quantify orientation of words (Monroe et al., 2008), hashtags (Weber et al., 2013) or documents (Iyyer et al., 2014),
1Data is available at http://www.preotiuc.ro
or to detect bias (Yano et al., 2010) or impartiality (Zafar et al., 2016) at a document level.
",2 Related Work,[0],[0]
"Our study belongs to the second category, where political orientation is inferred at a user-level.",2 Related Work,[0],[0]
"All previous studies study labeling US conservatives vs. liberals using either text (Rao et al., 2010), social network connections (Zamal et al., 2012), platform-specific features (Conover et al., 2011) or a combination of these (Pennacchiotti and Popescu, 2011; Volkova et al., 2014), with very high reported accuracies of up to 94.9% (Conover et al., 2011).
",2 Related Work,[0],[0]
"However, all previous work on predicting userlevel political preferences are limited to a binary prediction between liberal/democrat and conservative/republican, disregarding any nuances in political ideology.",2 Related Work,[0],[0]
"In addition, as the focus of the studies is more on the methodological or interpretation aspects of the problem, another downside is that the user labels were obtained in simple, albeit biased ways.",2 Related Work,[0],[0]
"These include users who explicitly state their political orientation on user lists of party supporters (Zamal et al., 2012; Pennacchiotti and Popescu, 2011), supporting partisan causes (Rao et al., 2010), by following political figures (Volkova et al., 2014) or party accounts (Sylwester and Purver, 2015) or that retweet partisan hashtags (Conover et al., 2011).",2 Related Work,[0],[0]
"As also identified in (Cohen and Ruths, 2013) and further confirmed later in this study, these data sets are biased: most people do not clearly state their political preference online – fewer than 5% according to Priante et al. (2016) – and those that state their preference are very likely to be political activists.",2 Related Work,[0],[0]
Cohen and Ruths (2013) demonstrated that predictive accuracy of classifiers is significantly lower when confronted with users that do not explicitly mention their political orientation.,2 Related Work,[0],[0]
"Despite this, their study is limited because in their hardest classification task, they use crowdsourced political orientation labels, which may not correspond to reality and suffer from biases (Flekova et al., 2016a; Carpenter et al., 2016).",2 Related Work,[0],[0]
"Further, they still only look at predicting binary political orientation.",2 Related Work,[0],[0]
"To date, no other research on this topic has taken into account these findings.",2 Related Work,[0],[0]
"The main data set used in this study consists of 3,938 users recruited through the Qualtrics platform (D1).",3 Data Set,[0],[0]
"Each participant was compensated
with 3 USD for 15 minutes of their time.",3 Data Set,[0],[0]
"All participants first answered the same demographic questions (including political ideology), then were directed to one of four sets of psychological questionnaires unrelated to the political ideology question.",3 Data Set,[0],[0]
"They were asked to self-report their political ideology on a seven point scale: Very conservative (1), Conservative (2), Moderately conservative (3), Moderate (4), Moderately liberal (5), Liberal (6), Very liberal (7).",3 Data Set,[0],[0]
"In addition, participants had the option of choosing Apathetic and Other, which have ambiguous fits on the conservative – liberal spectrum and were removed from our analysis (399 users).",3 Data Set,[0],[0]
"We also asked participants to self-report their gender (2322 female, 1205 male, 12 other) and age.",3 Data Set,[0],[0]
Participants were all from the US in order to limit the impact of cultural and political factors.,3 Data Set,[0],[0]
"The political ideology distribution in our sample is presented in Figure 1.
",3 Data Set,[0],[0]
"We asked users their Twitter handle and downloaded their most recent 3,200 tweets, leading to a total of 4,833,133 tweets.",3 Data Set,[0],[0]
"Before adding users to our 3,938 user data set, we performed the following checks to ensure that the Twitter handle was the user’s own: 1) after compensation, users were if they were truthful in reporting their handle and if not, we removed their data from analysis; 2) we manually examined all handles marked as verified by Twitter or that had over 2000 followers and eliminated them if they were celebrities or corporate/news accounts, as these were unlikely the users who participated in the survey.",3 Data Set,[0],[0]
"This study received approval from the Institutional Review Board (IRB) of the University of Pennsylvania.
",3 Data Set,[0],[0]
"In addition, to facilitate comparison to previous work, we also use a data set of 13,651 users with overt political orientation (D2).",3 Data Set,[0],[0]
"We selected popular political figures unambiguously associated with US liberal politics (@SenSanders,
@JoeBiden, @CoryBooker, @JohnKerry) or US conservative politics (@marcorubio, @tedcruz, @RandPaul, @RealBenCarson).",3 Data Set,[0],[0]
Liberals in our set (Nl = 7417) had to follow on Twitter all of the liberal political figures and none of the conservative figures.,3 Data Set,[0],[0]
"Likewise, conservative users (Nc = 6234) had to follow all of the conservative figures and no liberal figures.",3 Data Set,[0],[0]
"We downloaded up to 3,200 of each user’s most recent tweets, leading to a total of 25,493,407 tweets.",3 Data Set,[0],[0]
All tweets were downloaded around 10 August 2016.,3 Data Set,[0],[0]
"In our analysis, we use a broad range of linguistic features described below.",4 Features,[0],[0]
"Unigrams We use the bag-of-words representation to reduce each user’s posting history to a normalised frequency distribution over the vocabulary consisting of all words used by at least 10% of the users (6,060 words).",4 Features,[0],[0]
LIWC Traditional psychological studies use a dictionary-based approach to representing text.,4 Features,[0],[0]
"The most popular method is based on Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2001), and automatically counts word frequencies for 64 different categories manually constructed based on psychological theory.",4 Features,[0],[0]
"These include different parts-of-speech, topical categories and emotions.",4 Features,[0],[0]
Each user is thereby represented as a frequency distribution over these categories.,4 Features,[0],[0]
Word2Vec Topics,4 Features,[0],[0]
"An alternative to LIWC is to use automatically generated word clusters i.e., groups of words that are semantically and/or syntactically similar.",4 Features,[0],[0]
"The clusters help reducing the feature space and provides additional interpretability.
",4 Features,[0],[0]
"To create these groups of words, we use an automatic method that leverages word co-occurrence patterns in large corpora by making use of the distributional hypothesis: similar words tend to cooccur in similar contexts (Harris, 1954).",4 Features,[0],[0]
"Based on co-occurrence statistics, each word is represented as a low dimensional vector of numbers with words closer in this space being more similar (Deerwester et al., 1990).",4 Features,[0],[0]
"We use the method from (Preoţiuc-Pietro et al., 2015a) to compute topics using word2vec similarity (Mikolov et al., 2013a,b) and spectral clustering (Shi and Malik, 2000; von Luxburg, 2007) of different sizes (from 30 to 2000).",4 Features,[0],[0]
"We have tried other alternatives to building clusters: using other word similarities to
generate clusters – such as NPMI (Lampos et al., 2014) or GloVe (Pennington et al., 2014) as proposed in (Preoţiuc-Pietro et al., 2015a) – or using standard topic modelling approached to create soft clusters of words e.g., Latent Dirichlet Allocation (Blei et al., 2003).",4 Features,[0],[0]
"For brevity, we present experiments with the best performing feature set containing 500 Word2Vec clusters.",4 Features,[0],[0]
We aggregate all the words posted in a users’ tweets and represent each user as a distribution of the fraction of words belonging to each cluster.,4 Features,[0],[0]
Sentiment & Emotions We hypothesise that different political ideologies differ in the type and amount of emotions the users express through their posts.,4 Features,[0],[0]
"The most studied model of discrete emotions is the Ekman model (Ekman, 1992; Strapparava and Mihalcea, 2008; Strapparava et al., 2004) which posits the existence of six basic emotions: anger, disgust, fear, joy, sadness and surprise.",4 Features,[0],[0]
"We automatically quantify these emotions from our Twitter data set using a publicly available crowd-sourcing derived lexicon of words associated with any of the six emotions, as well as general positive and negative sentiment (Mohammad and Turney, 2010, 2013).",4 Features,[0],[0]
"Using these lexicons, we assign a predicted emotion to each message and then average across all users’ posts to obtain user level emotion expression scores.",4 Features,[0],[0]
"Political Terms In order to select unigrams pertaining to politics, we assigned the most frequent 12,000 unigrams in our data set to three categories: • Political words: mentions of political terms
(234);",4 Features,[0],[0]
"• Political NEs: mentions of politician proper
names out of the political terms (39); • Media NEs: mentions of political media
sources and pundits out of the political terms (20).
",4 Features,[0],[0]
This coding was initially performed by a research assistant studying political science with good knowledge of US politics and were further filtered and checked by one of the authors.,4 Features,[0],[0]
"First, we explore the relationships between language use and political ideological groups within each feature set and pairs of opposing user groups.",5 Analysis,[0],[0]
"To illustrate differences between ideological groups we compare the two political extremes (Very Conservative – Very Liberal) and the political moderates (Moderate Conservative – Moderate
Liberal).",5 Analysis,[0],[0]
"We further compare outright moderates with a group combining the two political extremes to study if we can uncover differences in political engagement and extremity, regardless of the conservative–liberal leaning.
",5 Analysis,[0],[0]
We use univariate partial linear correlations with age and gender as co-variates to factor out the influence of basic demographics.,5 Analysis,[0],[0]
"For example, in D1, users who reported themselves as very conservative are older and more likely males (µage = 35.1, pctmale = 44%) than the data average (µage = 31.2, pctmale = 35%).",5 Analysis,[0],[0]
"Additionally, prior to combining the two ideologically extreme groups, we sub-sampled the larger class (Very Liberal) to match the smaller class (Very Conservative) in age and gender.",5 Analysis,[0],[0]
"In the later prediction experiments, we do not perform matching, as this represents useful signal for classification (Ellis and Stimson, 2012).",5 Analysis,[0],[0]
Results with unigrams are presented in Figure 2 and with the other features in Table 1.,5 Analysis,[0],[0]
These are selected using standard statistical significance tests.,5 Analysis,[0],[0]
The comparison between the extreme categories reveals the largest number of significant differences.,5.1 Very Conservatives vs. Very Liberals,[0],[0]
"The unigrams and Word2Vec clusters specific to conservatives are dominated by religion specific terms (‘praying’, ‘god’, W2V485, W2V-018, W2V-099, L-RELIG), confirming a well-documented relationship (Gelman, 2009) and words describing family relationships (‘uncle’, ‘son’, L-FAMILY), another conservative value (Lakoff, 1997).",5.1 Very Conservatives vs. Very Liberals,[0],[0]
"The emphasis on religious terms among conservatives is consistent with the claim that many Americans associate ‘conservative’ with ‘religious’ (Ellis and Stimson, 2012).",5.1 Very Conservatives vs. Very Liberals,[0],[0]
"Extreme liberals show a tendency to use more adjectives (W2V-075, W2V-110), adverbs (L-ADVERB), conjunctions (L-CONJ) and comparisons (L-COMPARE) which indicate more nuanced and complex posts.",5.1 Very Conservatives vs. Very Liberals,[0],[0]
"Extreme conservatives post tweets higher in all positive emotions than liberals (L-POSEMO, Emot-Joy, EmotPositive), confirming a previously hypothesised relationship (Napier and Jost, 2008).",5.1 Very Conservatives vs. Very Liberals,[0],[0]
"However, extreme liberals are not associated with posting negative emotions either, only using words that reflect more anxiety (L-ANX), which is related to neuroticism in which the liberals are higher (Gerber et al., 2010).
",5.1 Very Conservatives vs. Very Liberals,[0],[0]
"Political term analysis reveals the partisan terms
employed by both sides.",5.1 Very Conservatives vs. Very Liberals,[0],[0]
"For example, conservatives retweet or mention politicians such as Donald Trump or Ted Cruz, while liberals mention
Barack Obama.",5.1 Very Conservatives vs. Very Liberals,[0],[0]
"Extreme conservatives also reference known partisan conservative media sources (@foxnews, @yahoonews) and hashtags (#pjnet,
#tcot), while extreme liberals focus on issues (‘gay’, ‘racism’, ‘feminism’, ‘transgender’).",5.1 Very Conservatives vs. Very Liberals,[0],[0]
"This perhaps reflects the desire for conservatives on Twitter to identify like-minded individuals, as extreme conservatives are a minority on the platform.",5.1 Very Conservatives vs. Very Liberals,[0],[0]
"Liberals, by contrast, use the platform to discuss and popularize their causes.",5.1 Very Conservatives vs. Very Liberals,[0],[0]
Comparing the two sides of moderate users reveals a slightly more nuanced view of the two ideologies.,5.2 Moderate Conservatives vs. Moderate Liberals,[0],[0]
"While moderate conservatives still make heavy use of religious terms and express positive emotions (Emot-Joy, L-DRIVES), they also use affiliative language (L-AFFILIATION) and plural pronouns (L-WE).",5.2 Moderate Conservatives vs. Moderate Liberals,[0],[0]
Moderate liberals are identified by very different features compared to their more extreme counterparts.,5.2 Moderate Conservatives vs. Moderate Liberals,[0],[0]
"Most striking is the use of swear and sex words (L-SEXUAL, L-ANGER, W2V-316), also highlighted by Sylwester and Purver (2015).",5.2 Moderate Conservatives vs. Moderate Liberals,[0],[0]
"Two word clusters relating to British culture (W2V-458) and art (W2V373) reflect that liberals are more inclined towards arts (Dollinger, 2007).",5.2 Moderate Conservatives vs. Moderate Liberals,[0],[0]
"Statistically significant political terms are very few compared to the previous comparison, probably due to their lower overall usage, which we further investigate later.",5.2 Moderate Conservatives vs. Moderate Liberals,[0],[0]
"Our final comparison looks at outright moderates compared to the two extreme groups combined, as we hypothesise the existence of a difference in overall political engagement.",5.3 Moderates vs. Extremists,[0],[0]
"Moderates are not characterized by many features besides a topic of casual words (W2V-098), indicating the heterogeneity of this group of users.",5.3 Moderates vs. Extremists,[0],[0]
"However, regardless of their orientation, the ideological extremists stand out from moderates.",5.3 Moderates vs. Extremists,[0],[0]
"They use words and word clusters related to political actors (W2V-309), issues (W2V-237) and laws (W2V296, W2V-288).",5.3 Moderates vs. Extremists,[0],[0]
LIWC analysis uncovers differences in article use (L-ARTICLE) or power words (L-POWER) specific of political tweets.,5.3 Moderates vs. Extremists,[0],[0]
"The overall sentiment of these users is negative (Emot-Fear, Emot-Disgust, Emot-Sadness, L-DEATH) compared to moderates.",5.3 Moderates vs. Extremists,[0],[0]
"This reveals – combined with the finding from the first comparison – that while extreme conservatives are overall more positive than liberals, both groups share negative expression.",5.3 Moderates vs. Extremists,[0],[0]
"Political terms are almost all significantly correlated with the extreme ideological groups,
confirming the existence of a difference in political engagement which we study in detail next.",5.3 Moderates vs. Extremists,[0],[0]
Figure 3 presents the use of the three types of political terms across the 7 ideological groups in D1 and the two political groups from D2.,5.4 Political Terms,[0],[0]
"We notice the following: • D2 has a huge skew towards political words,
with an average of more than three times more political terms across all three categories than our extreme classes from D1; • Within the groups in D1, we observe an almost
perfectly symmetrical U-shape across all three types of political terms, confirming our hypothesis about political engagement; • The difference between 1–2/6–7 is larger than
2–3/5–6.",5.4 Political Terms,[0],[0]
"The extreme liberals and conservatives are disproportionately political, and have the potential to give Twitter’s political discussions an unrepresentative, extremist hue (Fiorina, 1999).",5.4 Political Terms,[0],[0]
"It is also possible, however, that characterizing one as an extreme liberal or conservative indicates as much about her level of political engagement as it does about her placement on a left-right scale (Converse, 1964; Broockman, 2016).",5.4 Political Terms,[0],[0]
In this section we build predictive models of political ideology and compare them to data sets obtained using previous work.,6 Prediction,[0],[0]
"First, we experiment with classifying between conservatives and liberals across various levels of political engagement in D1 and between the two polarized groups in D2.",6.1 Cross-Group Prediction,[0],[0]
"We use logistic regression classification to compare three setups in Table 2 with results measured with ROC AUC as the classes are slightly inbalanced: • 10-fold cross-validation where training is per-
formed on the same task as the testing (principal diagonal); • A train–test setup where training is performed
on one task (presented in rows) and testing is performed on another (presented in columns); • A domain adaptation setup (results in brack-
ets) where on each of the 10 folds, the 9 training folds (presented in rows) are supplemented with all the data from a different task (presented in columns) using the EasyAdapt algorithm (Daumé III, 2007) as a proof on concept on the effects of using additional distantly supervised data.",6.1 Cross-Group Prediction,[0],[0]
Data pooling lead to worse results than EasyAdapt.,6.1 Cross-Group Prediction,[0],[0]
"Each of the three tasks from D1 have a similar number of training samples, hence we do not expect that data set size has any effects in comparing the results across tasks.
",6.1 Cross-Group Prediction,[0],[0]
"The results with both sets of features show that: • Prediction performance is much higher for D2
than for D1, with the more extreme groups in D1 being easier to predict than the moderate groups.",6.1 Cross-Group Prediction,[0],[0]
"This confirms that the very high accuracies reported by previous research are an artifact of user label collection and that on regular users, the expected accuracy is much lower (Cohen and Ruths, 2013).",6.1 Cross-Group Prediction,[0],[0]
"We further show that, as the level of political engagement decreases, the classification problem becomes even harder; • The model trained on D2 and Word2Vec word
clusters performs significantly worse on D1 tasks even if the training data is over 10 times larger.",6.1 Cross-Group Prediction,[0],[0]
"When using political words, the D2 trained classifier performs relatively well on all tasks from D1; • Overall, using political words as features per-
forms better than Word2Vec clusters in the binary classification tasks; • Domain adaptation helps in the majority of
cases, leading to improvements of up to .03 in AUC (predicting 2v6 supplemented with 3v5 data).",6.1 Cross-Group Prediction,[0],[0]
"Political leaning (Conservative – Liberal, excluding the Moderate group) can be considered an ordinal variable and the prediction problem framed as one of regression.",6.2 Political Leaning and Engagement Prediction,[0],[0]
"In addition to the political leaning prediction, based on analysis and previous prediction results, we hypothesize the existence of a separate dimension of political engagement regardless of the partisan side.",6.2 Political Leaning and Engagement Prediction,[0],[0]
"Thus, we merge users from classes 3–5, 2–6, 1–7 and create a variable with four values, where the lowest value is represented by moderate users (4) and the highest value is represented by either very conservative (1) or very liberal (7) users.
",6.2 Political Leaning and Engagement Prediction,[0],[0]
"We use a linear regression algorithm with an Elastic Net regularizer (Zou and Hastie, 2005) as implemented in ScikitLearn (Pedregosa et al., 2011).",6.2 Political Leaning and Engagement Prediction,[0],[0]
"To evaluate our results, we split our data into 10 stratified folds and performed crossvalidation on one held-out fold at a time.",6.2 Political Leaning and Engagement Prediction,[0],[0]
For all our methods we tune the parameters of our models on a separate validation fold.,6.2 Political Leaning and Engagement Prediction,[0],[0]
The overall performance is assessed using Pearson correlation between the set of predicted values and the userreported score.,6.2 Political Leaning and Engagement Prediction,[0],[0]
"Results are presented in Table 3.
",6.2 Political Leaning and Engagement Prediction,[0],[0]
"The same patterns hold when evaluating the results with Root Mean Squared Error (RMSE).
",6.2 Political Leaning and Engagement Prediction,[0],[0]
"The results show that both dimensions can be predicted well above chance, with political leaning being easier to predict than engagement.",6.2 Political Leaning and Engagement Prediction,[0],[0]
"Word2Vec clusters obtain the highest predictive accuracy for political leaning, even though they did not perform as well in the previous classification tasks.",6.2 Political Leaning and Engagement Prediction,[0],[0]
"For political engagement, political terms and Word2Vec clusters obtain similar predictive accuracy.",6.2 Political Leaning and Engagement Prediction,[0],[0]
"This result is expected based on the results from Figure 3, which showed how political term usage varies across groups, and how it is especially dependent on political engagement.",6.2 Political Leaning and Engagement Prediction,[0],[0]
"While political terms are very effective at distinguishing between two opposing political groups, they can not discriminate as well between levels of engagement within the same ideological orientation.",6.2 Political Leaning and Engagement Prediction,[0],[0]
Combining all classifiers’ predictions in a linear ensemble obtains best results when compared to each individual category.,6.2 Political Leaning and Engagement Prediction,[0],[0]
"In our previous experiments, we uncovered that certain relationships exist between the seven groups.",6.3 Encoding Class Structure,[0],[0]
"For example, extreme conservatives and liberals both demonstrate strong political engagement.",6.3 Encoding Class Structure,[0],[0]
"Therefore, this class structure can be exploited to improve classification performance.",6.3 Encoding Class Structure,[0],[0]
"To this end, we deploy the sparse graph regularized approach (Argyriou et al., 2007; Zhou et al., 2011) to encode the structure of the seven classes as a graph regularizer in a logistic regression framework.
",6.3 Encoding Class Structure,[0],[0]
"In particular, we employed a multi-task learning paradigm, where each task is a one-vs-all classification.",6.3 Encoding Class Structure,[0],[0]
"Multi-task learning (MTL) is a learning paradigm that jointly learns multiple related
tasks and can achieve better generalization performance than learning each task individually, especially when presented with insufficient training samples (Liu et al., 2015, 2016b,d).",6.3 Encoding Class Structure,[0],[0]
The group structure is encoded into a matrix R which codes the groups which are considered similar.,6.3 Encoding Class Structure,[0],[0]
"The objective of the sparse graph regularized multi-task learning problem is:
min W,c
τ∑
t=1
N∑
i=1
log(1 + exp(−Yt,i(WTi,tXt,i + ct)))
",6.3 Encoding Class Structure,[0],[0]
+,6.3 Encoding Class Structure,[0],[0]
γ‖WR‖2F +,6.3 Encoding Class Structure,[0],[0]
"λ‖W‖1,
where τ is the number of tasks, |N | the number of samples, X the feature matrix, Y the outcome matrix, Wi,t and ct is the model for task t and R is the structure matrix.
",6.3 Encoding Class Structure,[0],[0]
"We define three R matrices: (1) codes that groups with similar political engagement are similar (i.e. 1–7, 2–6, 3–5); (2) codes that groups from each ideological side are similar (i.e. 1–2, 1–3, 2–3, 5–6, 5–7, 6–7); (3) learnt from the data.",6.3 Encoding Class Structure,[0],[0]
Results are presented in Table 4.,6.3 Encoding Class Structure,[0],[0]
"Regular logistic regression performs slightly better than the majority class baseline, which demonstrates that the 7- class classification is a very hard problem although most miss-classifications are within one ideology point.",6.3 Encoding Class Structure,[0],[0]
"The graph regularization (GR) improves the classification performance over logistic regression (LR) in all cases, with political leaning based matrix (GR–Leaning) obtaining 2% in accuracy higher than the political engagement one (GR– Engagement) and the learnt matrix (GR–Learnt) obtaining best results.",6.3 Encoding Class Structure,[0],[0]
This study analyzed user-level political ideology through Twitter posts.,7 Conclusions,[0],[0]
"In contrast to previous work, we made use of a novel data set where finegrained user political ideology labels are obtained through surveys as opposed to binary self-reports.",7 Conclusions,[0],[0]
"We showed that users in our data set are far less
likely to post about politics and real-world finegrained political ideology prediction is harder and more nuanced than previously reported.",7 Conclusions,[0],[0]
"We analyzed language differences between the ideological groups and uncovered a dimension of political engagement separate from political leaning.
",7 Conclusions,[0],[0]
"Our work has implications for pollsters or marketers, who are most interested to identify and persuade moderate users.",7 Conclusions,[0],[0]
"With respect to political conclusions, researchers commonly conceptualize ideology as a single, left-right dimension similar to what we observe in the U.S. Congress (Ansolabehere et al., 2008; Bafumi and Herron, 2010).",7 Conclusions,[0],[0]
"Our results suggest a different direction: self-reported political extremity is more an indication of political engagement than of ideological self-placement (Abramowitz, 2010).",7 Conclusions,[0],[0]
"In fact, only self-reported extremists appear to devote much of their Twitter activity to politics at all.
",7 Conclusions,[0],[0]
"While our study focused solely on text posted by the user, follow-up work can use other modalities such as images or social network analysis to improve prediction performance.",7 Conclusions,[0],[0]
"In addition, our work on user-level modeling can be integrated with work on message-level political bias to study how this is revealed across users with various levels of engagement.",7 Conclusions,[0],[0]
"Another direction of future study will look at political ideology prediction in other countries and cultures, where ideology has different or multiple dimensions.",7 Conclusions,[0],[0]
"The authors acknowledge the support of the Templeton Religion Trust, grant TRT-0048.",Acknowledgments,[0],[0]
We wish to thank Prof. David S. Rosenblum for supporting the research visit of Ye Liu.,Acknowledgments,[0],[0]
Automatic political preference prediction from social media posts has to date proven successful only in distinguishing between publicly declared liberals and conservatives in the US.,abstractText,[0],[0]
This study examines users’ political ideology using a sevenpoint scale which enables us to identify politically moderate and neutral users – groups which are of particular interest to political scientists and pollsters.,abstractText,[0],[0]
"Using a novel data set with political ideology labels self-reported through surveys, our goal is two-fold: a) to characterize the political groups of users through language use on Twitter; b) to build a fine-grained model that predicts political ideology of unseen users.",abstractText,[0],[0]
Our results identify differences in both political leaning and engagement and the extent to which each group tweets using political keywords.,abstractText,[0],[0]
"Finally, we demonstrate how to improve ideology prediction accuracy by exploiting the relationships between the user groups.",abstractText,[0],[0]
Beyond Binary Labels: Political Ideology Prediction of Twitter Users,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3602–3611 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3602",text,[0],[0]
"Neural machine translation (NMT) has attracted much research attention in recent years (Bahdanau et al., 2014; Shen et al., 2018; Song et al., 2018; Xia et al., 2018; He et al., 2016; Wu et al., 2017, 2018).",1 Introduction,[0],[0]
"The major approach to the task typically leverages an encoder-decoder framework (Cho et al., 2014; Sutskever et al., 2014) and the decoder usually generates the target tokens one by one from left to right autoregressively, in which the generation of a target token is conditioned on previously generated target tokens.
",1 Introduction,[0],[0]
"It has been observed that for an NMT model with left-to-right decoding, the right part words in
∗Authors contribute equally to this work.
",1 Introduction,[0],[0]
"its translation results are usually worse than the left part words in terms of accuracy (Zhang et al., 2018; Bengio et al., 2015; Ranzato et al., 2015; Hassan et al., 2018; Liu et al., 2016b,a).",1 Introduction,[0],[0]
This phenomenon is referred to as accuracy drop in this paper.,1 Introduction,[0],[0]
"A straightforward explanation to accuracy drop is error propagation: If a word is mistakenly predicted during inference, the error will be propagated and the future words conditioned on this one will be impacted.",1 Introduction,[0],[0]
"Different methods have been proposed to address the problem of accuracy drop (Liu et al., 2016a,b; Hassan et al., 2018).
",1 Introduction,[0],[0]
"Instead of solving the problem, in this paper, we aim to deeply understand the causes of the problem.",1 Introduction,[0],[0]
"In particular, we want to answer the following two questions:
• Is error propagation the main cause of accuracy drop?
",1 Introduction,[0],[0]
"• Are there any other causes leading to accuracy drop?
",1 Introduction,[0],[0]
"To answer these two questions, we conduct a series of experiments to analyze the problem.
",1 Introduction,[0],[0]
"First, we train NMT models separately using left-to-right and right-to-left decoding (Sennrich et al., 2016; Liu et al., 2016b; He et al., 2017; Gao et al., 2018) on several language pairs (i.e., German to English, English to German, and English to Chinese).",1 Introduction,[0],[0]
"If error propagation is the main cause of accuracy drop, then the right part words in the translation results generated by right-toleft NMT models should be more accurate than the left part words.",1 Introduction,[0],[0]
"However, we observe the opposite phenomenon that the accuracy of the right part words of the translated sentences in both leftto-right and right-to-left models is lower than that of the left part, which contradicts with error propagation.",1 Introduction,[0],[0]
"This shows that error propagation alone cannot well explain the accuracy drop and even
suggests that error propagation may not exist or matter.
",1 Introduction,[0],[0]
"Second, to further investigate the influence of error propagation on accuracy drop, we conduct a set of experiments with teacher forcing (Williams and Zipser, 1989) during inference, in which we feed the ground-truth preceding words to predict the next target word.",1 Introduction,[0],[0]
Teacher forcing eliminates exposure bias as well as error propagation in inference.,1 Introduction,[0],[0]
"The results verify the existence of error propagation, since the later part (the right part in left-to-right decoding and the left part in right-toleft decoding) of the translation results get more accuracy improvement with teacher forcing, regardless of the decoding direction.",1 Introduction,[0],[0]
"Meanwhile, the accuracy of the right part is still lower than that of the left part with teacher forcing, which demonstrates that there must be some other causes apart from error propagation leading to accuracy drop.
",1 Introduction,[0],[0]
"Third, inspired by linguistics, we find that the concept of branching (Berg et al., 2011; Payne, 2006) can help to explain the problem.",1 Introduction,[0],[0]
We conduct the third set of experiments to study the correlation between language branching and accuracy drop.,1 Introduction,[0],[0]
"We find that if a target language is right branching such as English, the accuracy of the left part words is usually higher than that of the right part words, no matter for left-to-right or right-toleft NMT models, while for a left-branching target language such as Japanese, the accuracy of the left part words is usually lower than that of the right part, no matter for which models.",1 Introduction,[0],[0]
"The intuitive explanation is that a right-branching language has a clearer structure pattern (easier to predict) in the left part of sentence than that in the right part, since the main subject of the sentence is usually put in the left part.",1 Introduction,[0],[0]
We calculate two statistics to verify this assumption: n-gram statistics (including n-gram frequency and conditional probabilities) and dependency parsing statistics.,1 Introduction,[0],[0]
"For rightbranching languages, we found higher n-gram frequency/conditional probabilities as well as more dependencies in the left part compared with that in the right part.",1 Introduction,[0],[0]
"The opposite results are also found in left-branching languages.
",1 Introduction,[0],[0]
"We summarize our findings as follows.
",1 Introduction,[0],[0]
"• Through empirical analyses, we find that the influence of error propagation is overstated in the literature, which may misguide the future research.",1 Introduction,[0],[0]
"Error propagation alone cannot fully explain the accuracy drop in the left or
right part of sentence.
",1 Introduction,[0],[0]
"• We find the branching in linguistics well correlates with accuracy drop in the left or right part of sentence and the corresponding analysis on n-gram and dependency parsing statistics well explain this phenomenon.
",1 Introduction,[0],[0]
Our studies show that linguistics can be very helpful to understand existing machine learning models and build better models for language related tasks.,1 Introduction,[0],[0]
We hope that our work can bring some insights to the research on neural machine translation.,1 Introduction,[0],[0]
We believe that our findings can help us to design better translation models.,1 Introduction,[0],[0]
"For example, the finding on language branching suggests us to use left-to-right NMT models for right-branching languages such as English and right-to-right NMT models for left-branching languages such as Japanese.",1 Introduction,[0],[0]
"Exposure bias and error propagation are two different concepts but often mentioned together in literature (Bengio et al., 2015; Shen et al., 2016; Ranzato et al., 2015; Liu et al., 2016b,a; Zhang et al., 2018; Hassan et al., 2018).",2.1 Exposure Bias and Error Propagation,[0],[0]
Exposure bias refers to the fact that the sequence generation model is usually trained with teacher-forcing while generates the sequence autoaggressviely during inference.,2.1 Exposure Bias and Error Propagation,[0],[0]
"This discrepancy between training and inference can yield errors that accumulate quickly along the generated sequence, which is known as error propagation (Bengio et al., 2015; Shen et al., 2016; Ranzato et al., 2015).
",2.1 Exposure Bias and Error Propagation,[0],[0]
"Bengio et al. (2015) propose the scheduled sampling method to eliminate the exposure bias and the resulting error propagation, which achieves promising performance on sequence generation tasks such as image captioning.",2.1 Exposure Bias and Error Propagation,[0],[0]
Shen et al. (2016); Ranzato et al. (2015) improve the basic maximum likelihood estimation (MLE) with reinforcement learning or minimum risk training and aim to address the limitation of MLE training and exposure bias problem.,2.1 Exposure Bias and Error Propagation,[0],[0]
"(Liu et al., 2016b,a; Zhang et al., 2018; Hassan et al., 2018) mainly ascribe accuracy drop (the accuracy of right part words is worse than that in the left part in most cases) to error propagation and
propose different methods to solve this problem.",2.2 Tackling Accuracy Drop,[0],[0]
"Liu et al. (2016b,a); Hassan et al. (2018) use agreement regularization between the left-to-right and right-to-left models to achieve better performance.",2.2 Tackling Accuracy Drop,[0],[0]
"Zhang et al. (2018) and (Hassan et al., 2018) propose to use two-pass decoding to refine the generated sequence to yield better quality.
",2.2 Tackling Accuracy Drop,[0],[0]
All these works focus on error propagation and accuracy drop.,2.2 Tackling Accuracy Drop,[0],[0]
"To our knowledge, there is no deep study about other causes of accuracy drop.",2.2 Tackling Accuracy Drop,[0],[0]
"In this paper, we aim to conduct such a study.",2.2 Tackling Accuracy Drop,[0],[0]
"Our study shows that accuracy drop is not only caused by error propagation, but also the characteristics of language itself.",2.2 Tackling Accuracy Drop,[0],[0]
"A left-to-right NMT model feeds target tokens one by one from left to right in training and generate target tokens one by one from left to right during inference, while a right-to-left NMT model trains and generates token in the reverse direction.",3.1 Error Propagation is Not the Only Cause,[0],[0]
"Intuitively, if error propagation is the root cause of accuracy drop, then a right-to-left NMT model will generate translations with better right half accuracy than the left half.",3.1 Error Propagation is Not the Only Cause,[0],[0]
"In this section, we study the results of both left-to-right and right-to-left NMT models to analyze the relationship between error propagation and accuracy drop.
",3.1 Error Propagation is Not the Only Cause,[0],[0]
"We conduct experiments on three translation tasks with different language pairs, which include: IWSLT 2014 German-English (De-En), WMT 2014 English-German (En-De) and WMT 2017 English-Chinese (En-Zh).",3.1 Error Propagation is Not the Only Cause,[0],[0]
"We choose the state-ofthe-art NMT model Transformer (Vaswani et al., 2017) as the basic model structure and train two separate models with left-to-right and right-toleft decoding on each language pair.",3.1 Error Propagation is Not the Only Cause,[0],[0]
More details about the datasets and model descriptions can be found in supplementary materials (section A.1 and A.2).,3.1 Error Propagation is Not the Only Cause,[0],[0]
We evenly split each generated sentence into the left half and the right half with same number of words1.,3.1 Error Propagation is Not the Only Cause,[0],[0]
"Then for both the left and right half, we compute their accuracy with respect to the reference target sentence, in terms of BLEU score (Pa-
11) For most of the sentences, the last word of the sentence is period which is easy to decode.",3.1 Error Propagation is Not the Only Cause,[0],[0]
"To make a fair comparison, we simply remove the last period before dividing the translation sentence.",3.1 Error Propagation is Not the Only Cause,[0],[0]
2),3.1 Error Propagation is Not the Only Cause,[0],[0]
"For sentence with an odd number of words, we simply remove the word in the middle position to make the left half and right half have the same number of words.
",3.1 Error Propagation is Not the Only Cause,[0],[0]
"pineni et al., 2002) 2.",3.1 Error Propagation is Not the Only Cause,[0],[0]
We first report the BLEU scores of the full translation results (without split) in Table 1.,3.1 Error Propagation is Not the Only Cause,[0],[0]
"As can be seen, the accuracy of the model is comparable to state-of-the-art results (Vaswani et al., 2017; Wang et al., 2017, 2018).",3.1 Error Propagation is Not the Only Cause,[0],[0]
Afterwards we report the BLEU scores of the left half and the right half in Table 2.,3.1 Error Propagation is Not the Only Cause,[0],[0]
"We have several observations.
",3.1 Error Propagation is Not the Only Cause,[0],[0]
"• When translating from left-to-right, the BLEU score of the left half is higher than the right half on all the three tasks, which is consistent with previous observation and is able to be explained via error propagation.
",3.1 Error Propagation is Not the Only Cause,[0],[0]
"• When translating from right-to-left, the accuracy of the left half (in this way it’s the later part of the generated sentence) is still higher than the right half.",3.1 Error Propagation is Not the Only Cause,[0],[0]
"Such an observation is contradictory to the previous analyses between error propagation and accuracy drop, which regard that accumulated error brought by exposure bias will deteriorate the quality in later part of translation (i.e., the left half).
",3.1 Error Propagation is Not the Only Cause,[0],[0]
"The inconsistent observation above suggests that error propagation is not the only cause of accuracy drop that there are other factors beyond er-
2We use the multi-bleu.perl script https: //github.com/moses-smt/mosesdecoder/ scripts/generic/multi-bleu.perl.",3.1 Error Propagation is Not the Only Cause,[0],[0]
"When computing BLEU score of the left or right half, the reference is the full reference sentence.
",3.1 Error Propagation is Not the Only Cause,[0],[0]
ror propagation for accuracy drop.,3.1 Error Propagation is Not the Only Cause,[0],[0]
It even challenges the existence of error propagation: does error propagation really exist?,3.1 Error Propagation is Not the Only Cause,[0],[0]
In the next section we try to answer this question through teacher forcing experiments.,3.1 Error Propagation is Not the Only Cause,[0],[0]
"Teacher forcing (Williams and Zipser, 1989) in sequence generation means that when training a sequence generation model, we feed the previous ground-truth tokens as inputs to predict the next target word.",3.2 The Influence of Error Propagation,[0],[0]
"Here we apply teacher forcing in the inference phase of NMT: to generate the next word ŷi, we input the preceding ground-truth words y<i rather than the previously generated words ŷ<i, which largely alleviates the effect of error propagation, since there will be no error propagated from the previously generated words.
",3.2 The Influence of Error Propagation,[0],[0]
"Same as last section, we evaluate the quality of the left and right half of the translation results generated by both the left-to-right and right-toleft models.",3.2 The Influence of Error Propagation,[0],[0]
The results are summarized in Table 3.,3.2 The Influence of Error Propagation,[0],[0]
"For comparison, we also include the BLEU scores of normal translation (without teacher forcing).",3.2 The Influence of Error Propagation,[0],[0]
"We have several findings from Table 3 as follows:
• Exposure bias exists.",3.2 The Influence of Error Propagation,[0],[0]
"The accuracy of both left and right half tokens in the normal translation is lower than that in teacher forcing, which feeds the ground-truth tokens as inputs.",3.2 The Influence of Error Propagation,[0],[0]
"This demonstrates that feeding the previously generated tokens (which might be in-
correct) in inference indeed hurts translation accuracy.
",3.2 The Influence of Error Propagation,[0],[0]
• Error propagation does exist.,3.2 The Influence of Error Propagation,[0],[0]
We find the error is accumulated along the sequential generation of the sentence.,3.2 The Influence of Error Propagation,[0],[0]
"Taking En-Zh and the left-to-right NMT model as an example, the BLEU score improvement of the right half (the second half of the generation) of teacher forcing over normal translation is 2.64, which is much larger than the accuracy improvement of the left half (the first half of the generation): 1.70.",3.2 The Influence of Error Propagation,[0],[0]
"Similarly, for En-Zh with the right-to-left NMT model, the BLEU score improvement of the left half (the second half of the generation) of teacher forcing over normal translation is 2.82, which is much larger than the accuracy improvement of the right half (the first half of the generation): 1.77.
",3.2 The Influence of Error Propagation,[0],[0]
• Other causes exist.,3.2 The Influence of Error Propagation,[0],[0]
"Taking En-De translation with the left-to-right model as an example, the accuracy of the left half (9.43) is higher than that of the right half (8.36) when there is no error propagation with teacher forcing.",3.2 The Influence of Error Propagation,[0],[0]
Similar results can be found in other language pairs and models.,3.2 The Influence of Error Propagation,[0],[0]
"This suggests that there must be some other causes leading to accuracy drop, which will be studied in the next section.",3.2 The Influence of Error Propagation,[0],[0]
Section 3.1 and 3.2 together show that error propagation has influence on but is not the only cause of accuracy drop.,4 Language Branching Matters,[0],[0]
"We hypothesize that the language itself, i.e., its characteristics, may explain the phenomenon of accuracy drop.
",4 Language Branching Matters,[0],[0]
Watanabe and Sumita (2002) finds that leftto-right decoding performs better for JapaneseEnglish translation while right-to-left decoding performs better for English-Japanese translation.,4 Language Branching Matters,[0],[0]
We conduct the same analysis settings as in Section 3.1 and 3.2 on English-Japanese (En-Jp) translation dataset.,4 Language Branching Matters,[0],[0]
"More details about this dataset and model descriptions can be found in supplementary materials (section A.1 and A.2).
",4 Language Branching Matters,[0],[0]
Table 4 shows the BLEU score on the En-Jp test set.,4 Language Branching Matters,[0],[0]
"It can be observed that regardless of decoding direction (i.e., from left-to-right or from right-toleft) and with or without teacher forcing, the accuracy of the right half is always higher than that in the left half.",4 Language Branching Matters,[0],[0]
"This observation on Japanese is
opposite to English, German and Chinese in Section 3.1 and 3.2, and motivates us to investigate the differences between these languages.
",4 Language Branching Matters,[0],[0]
"We find that a linguistics concept, the branching, can differentiate Japanese from other languages such as English/German.",4 Language Branching Matters,[0],[0]
"Branching refers to the shape of the parse trees that represent the structure of sentences (Berg et al., 2011; Payne, 2006).",4 Language Branching Matters,[0],[0]
"Usually, right-branching sentences are head-initial, which means the main subject of the sentence is described first, and is followed by a sequence of modifiers that provide additional information about the subject.",4 Language Branching Matters,[0],[0]
"On the contrary, left-branching sentences are head-final that putting such modifiers to the left of the sentence (Payne, 2006).
",4 Language Branching Matters,[0],[0]
"English is a typical right-branching language, while Japanese is almost fully leftbranching (Wikipedia, 2018).",4 Language Branching Matters,[0],[0]
The two languages demonstrate the opposite phenomenon of accuracy drop as shown in previous studies.,4 Language Branching Matters,[0],[0]
"When we say a language is typical left/right-branching, we mean most of the sentences in this language follows the left/right-branching structure.",4 Language Branching Matters,[0],[0]
"While being predominantly right-branching, German is less conclusively so than English.",4 Language Branching Matters,[0],[0]
"Chinese features a mixture of head-final and head-initial structures, with the noun phrases are head-final while the strict head/complement ordering sentences are headinitial as right-branching (Wikipedia, 2018), but less conclusively than German.
",4 Language Branching Matters,[0],[0]
We believe the language branching is a main cause of accuracy drop.,4 Language Branching Matters,[0],[0]
"Intuitively, the main subject of a right-branching sentence is described first (in the left part) and is followed by additional modifiers (in the right part) (Berg et al., 2011).",4 Language Branching Matters,[0],[0]
"Therefore, the left half of a right-branching sentence is more likely to possess a clearer structure pattern and thus lead to higher generation accuracy than in the right part, since the main subject is usually simpler and clearer than the modifiers that providing additional information about the subjec-
t. In next section, we will verify this intuition this assumption from a statistical perspective.",4 Language Branching Matters,[0],[0]
"As previous work (Arpit et al., 2017) shows, neural networks are easy to learn and memorize simple patterns but difficult to make a correct prediction on noise examples.",5 Correlation between Language Branching and Accuracy Drop,[0],[0]
"In this section, we study different branching languages from two aspects, including the n-gram statistics of a target language, which has been used as a kind of characterization of hardness of learning (Bengio et al., 2009), and the dependency statistics in parse trees.",5 Correlation between Language Branching and Accuracy Drop,[0],[0]
We show that these statistics well correlate with the accuracy drop between the left half and the right half of translation results.,5 Correlation between Language Branching and Accuracy Drop,[0],[0]
"Intuitively speaking, if a pattern occurs frequently and deterministically, it is easy to be learned by neural networks.",5.1 N-gram Statistics,[0],[0]
"By comparing the general statistics on the n-gram frequency and n-gram conditional probability of the left and right half tokens, we link the language branching to accuracy drop.
",5.1 N-gram Statistics,[0],[0]
"Denote a bilingual dataset D = {(xi, yi}, i = 1, · · · ,M , where each yi is a sequence of words yi = {y1i , · · · , y Ti i }, Ti is the length of yi.",5.1 N-gram Statistics,[0],[0]
"F li,n and P li,n denote the average n-gram frequency and n-gram conditional probability of the left half of yi 3, i.e.,
F li,n = 1
Ti/2− n + 1 Ti/2−n+1∑ j=1 F (yji , ..., y j+n−1 i ),
P li,n = 1
Ti/2− n + 1 Ti/2−n+1∑ j=1 P (yj+n−1i |y j i , ..., y j+n−2 i ),
(1)
where F (.)",5.1 N-gram Statistics,[0],[0]
and P (.) are the n-gram frequency and n-gram conditional probability calculated from the training dataset.,5.1 N-gram Statistics,[0],[0]
"Similarly, F ri,n and P r i,n denote the n-gram frequency and n-gram conditional probability of the right half.
",5.1 N-gram Statistics,[0],[0]
We calculate the average n-gram frequencies F ln and F rn of the left half and right half over all the target sentences in the training set.,5.1 N-gram Statistics,[0],[0]
We also calculate the average n-gram conditional probabilities P ln and P r n,5.1 N-gram Statistics,[0],[0]
"over all the training sentences to compare the uncertainty of phrases in the left half and
3Again, we assume Ti is an even number.",5.1 N-gram Statistics,[0],[0]
"If not, we simply remove the middle word of yi, as done in Section 3.1.
right half.
",5.1 N-gram Statistics,[0],[0]
"F ln = 1
M M∑ i=1",5.1 N-gram Statistics,[0],[0]
"F li,n, F r n = 1 M M∑ i=1",5.1 N-gram Statistics,[0],[0]
"F ri,n,
P ln = 1
M M∑ i=1",5.1 N-gram Statistics,[0],[0]
"P li,n, P r n = 1 M M∑ i=1",5.1 N-gram Statistics,[0],[0]
"P ri,n.
(2)
We also calculate the ratio of the sentences that the frequency/conditional probability of left half is bigger/smaller than that in the right half, denoted as RF",5.1 N-gram Statistics,[0],[0]
l>rn /RF,5.1 N-gram Statistics,[0],[0]
l<r n and RP l>r n,5.1 N-gram Statistics,[0],[0]
/RP,5.1 N-gram Statistics,[0],[0]
"l<r n :
RF l>rn = 1
M M∑ i=1",5.1 N-gram Statistics,[0],[0]
"1{F li,n > F ri,n},
RF l<rn = 1
M M∑ i=1",5.1 N-gram Statistics,[0],[0]
"1{F li,n < F ri,n},
RP l>rn = 1
M M∑ i=1",5.1 N-gram Statistics,[0],[0]
"1{P li,n > P ri,n},
RP l<rn = 1
M M∑ i=1",5.1 N-gram Statistics,[0],[0]
"1{P li,n < P ri,n}.
(3)
",5.1 N-gram Statistics,[0],[0]
We choose n = 2 and 3 to calculate the metrics in Equation 2 and 3 on different translation datasets.,5.1 N-gram Statistics,[0],[0]
"The numbers are listed in Table 5 and 6.
",5.1 N-gram Statistics,[0],[0]
"We can see the 2/3-gram frequency as well as the conditional probability of the left half is higher than that of the right half for right-branching languages including English, German and Chinese in De-En, En-De and En-Zh translation datasets.",5.1 N-gram Statistics,[0],[0]
"For left-branching language Japanese, the result is opposite.",5.1 N-gram Statistics,[0],[0]
"The n-gram frequency and conditional probability statistics are consistent with our observations on accuracy drop in left/rightbranching languages and verify our hypothesis: right-branching languages have clearer patterns in left part (with larger n-gram frequency as well as the conditional probability) and consequently leads to higher translation accuracy in the left part than the right part; left-branching languages are opposite.
",5.1 N-gram Statistics,[0],[0]
We further visualize how the accuracy drop (between the left half and right half of the translations) correlates with the gap of n-gram statistics in the left and right part.,5.1 N-gram Statistics,[0],[0]
"The accuracy drop (e.g., BLEU score) of left/right half is taken from the teacherforcing with left-to-right decoding in Table 3, and the n-gram gap is taken from the ∆ in the last row of Table 5 and 6.",5.1 N-gram Statistics,[0],[0]
Figure 1 shows strong correlation between accuracy drop and the gap of n-gram statistics:,5.1 N-gram Statistics,[0],[0]
"As the gap of n-gram statistics increases from negative values to positive values, the accuracy drop also increases from negative to positive.
",5.1 N-gram Statistics,[0],[0]
"n represent the ra-
tio that the n-gram frequency of left half of sentences are bigger/smaller than that of the right half. ∆",5.1 N-gram Statistics,[0],[0]
= RF l>rn − RF l<rn .,5.1 N-gram Statistics,[0],[0]
Note that the sum of RF,5.1 N-gram Statistics,[0],[0]
l>rn and RF,5.1 N-gram Statistics,[0],[0]
l<rn is less than 1 since sentence with less than 4 words does not contribute to the n-gram statistics.,5.1 N-gram Statistics,[0],[0]
"In this subsection, we study language branching from the perspective of dependency structure.",5.2 Dependency Statistics,[0],[0]
"We hypothesize that if the left/right half of sentence contains more dependencies between its intra words, this half should be easier to predict, leading to higher accuracy.",5.2 Dependency Statistics,[0],[0]
"Here we analyze the English sentence in De-En translation and Japanese sentence in En-Jp translation, since English is fully right-branching and Japanese is fully left-branching as introduced before.
",5.2 Dependency Statistics,[0],[0]
"For English parsing, we utilize the wellacknowledged Standford Parser4 to parse the sentences.",5.2 Dependency Statistics,[0],[0]
"After obtaining the parsing results, we split the sentence into left and right half, and separately count the numbers of dependencies in each half5.",5.2 Dependency Statistics,[0],[0]
"For Japanese, we leverage the open-source toolkit J.DepP6 to parse the sentence, and then count the number of dependencies of each half.
",5.2 Dependency Statistics,[0],[0]
We provide the results in Table 7.,5.2 Dependency Statistics,[0],[0]
"As can be observed, for English sentences, the left-half words depend more on each other than the right-half words, while for the Japanese sentences, the righthalf words have more dependencies.",5.2 Dependency Statistics,[0],[0]
"This observation is consistent with our observations on accu-
4https://nlp.stanford.edu/software/ lex-parser.shtml
5For simplicity, we just count the number of dependency, without considering dependency types.",5.2 Dependency Statistics,[0],[0]
"The detailed parsing formats can be found in the supplementary material (Section A.3).
",5.2 Dependency Statistics,[0],[0]
"6http://www.tkl.iis.u-tokyo.ac.jp/ ˜ynaga/jdepp/
racy drop, and can well explain the high accuracy of left part in English translation and right part in Japanese translation.",5.2 Dependency Statistics,[0],[0]
We have analyzed the accuracy drop problem from the view of error propagation and language itself in previous sections.,6 Extended Analyses and Discussions,[0],[0]
"In this section, we further provide extended analyses and several discussions to give a more clear understanding of the accuracy drop problem.",6 Extended Analyses and Discussions,[0],[0]
"The previous analyses are based on four languages, three right-branching (En, De, Zh) and one left-branching language (Jp).",6.1 More Languages on Left-Branching,[0],[0]
"To avoid the experimental bias and randomness, we provide one more translation task, English-Turkish (EnTr) translation7, as Turkish is a left-branching language.",6.1 More Languages on Left-Branching,[0],[0]
"We simply calculate the BLEU score of the left/right half in left-to-right and right-to-left
7The detailed dataset and model description can be found in supplementary material (section A.1 and section A.2).
decodings, as in Section 3.1 and 3.2.",6.1 More Languages on Left-Branching,[0],[0]
The result is provided in Table 8.,6.1 More Languages on Left-Branching,[0],[0]
"For the leftto-right decoding, the accuracy of the left half is higher than that of the right half in the normal translation.",6.1 More Languages on Left-Branching,[0],[0]
"However, the accuracy of the right half becomes higher with teacher forcing translation.",6.1 More Languages on Left-Branching,[0],[0]
This demonstrates that English-Turkish translation performs similar to English-Japanese translation as the accuracy of right half is higher than that of the left half.,6.1 More Languages on Left-Branching,[0],[0]
"But different from what we observed in Japanese, Turkish shows the opposite phenomenon: the influence of language branching is weaker than error propagation.",6.1 More Languages on Left-Branching,[0],[0]
One may wonder whether the results in the paper are biased towards a certain model structure as we use Transformer on all the above analyses.,6.2 Other Model Structures,[0],[0]
"To address such concerns, we conduct an additional experiment on De-En translation task with RNN (GRU)-based model8.",6.2 Other Model Structures,[0],[0]
The results are shown in Table 9 and the observations are consistent with what we observed on Transformer.,6.2 Other Model Structures,[0],[0]
"The accuracy of the left half of the De-En translation sentence is always higher than the right half, in both the leftto-right and right-to-left decodings.",6.2 Other Model Structures,[0],[0]
"We conduct experimental analysis on abstractive summarization, which is also a sequence generation task.",6.3 Other Sequence Generation Tasks,[0],[0]
The goal of the task is to recap a long news sentence into a short summary.,6.3 Other Sequence Generation Tasks,[0],[0]
"We use Gigaword dataset which contains 3.8M training pairs,
8The detailed setting for GRU based RNN model can be found in supplementary material (section A.2).
",6.3 Other Sequence Generation Tasks,[0],[0]
"190k validation and 2k test pairs of English sentence, and train an RNN-based model for sentence summarization.",6.3 Other Sequence Generation Tasks,[0],[0]
"The accuracy is measured by the commonly used metric ROUGE F1 score and are reported in Table 10.
",6.3 Other Sequence Generation Tasks,[0],[0]
We observe the same phenomenon as in translation tasks.,6.3 Other Sequence Generation Tasks,[0],[0]
"The accuracy of the left half is always better than the right half, no matter in left-to-right or right-to-left decoding, since the target language English is a right-branching language.",6.3 Other Sequence Generation Tasks,[0],[0]
"In this work, we studied the problem of accuracy drop between the left half and the right half of the results generated by neural machine translation models.",7 Conclusion,[0],[0]
We found the influence of error propagation is overstated in literature and error propagation alone cannot explain accuracy drop.,7 Conclusion,[0],[0]
We showed that language branching well correlates to the accuracy drop problem and the evidences on n-gram statistics as well as the dependency statistics well support this correlation.,7 Conclusion,[0],[0]
"Our discoveries suggest that left-to-right NMT models fit better for right-branching languages (e.g., English) and right-to-left NMT models fit better for leftbranching languages (e.g., Japanese).
",7 Conclusion,[0],[0]
"For future works, we will study more left/rightbranching languages as well as other languages that have no obvious branching characteristics.",7 Conclusion,[0],[0]
"We will also investigate how language branching influences other natural language tasks, especially for neural networks based models.",7 Conclusion,[0],[0]
Neural machine translation usually adopts autoregressive models and suffers from exposure bias as well as the consequent error propagation problem.,abstractText,[0],[0]
"Many previous works have discussed the relationship between error propagation and the accuracy drop (i.e., the left part of the translated sentence is often better than its right part in left-to-right decoding models) problem.",abstractText,[0],[0]
"In this paper, we conduct a series of analyses to deeply understand this problem and get several interesting findings.",abstractText,[0],[0]
"(1) The role of error propagation on accuracy drop is overstated in the literature, although it indeed contributes to the accuracy drop problem.",abstractText,[0],[0]
"(2) Characteristics of a language play a more important role in causing the accuracy drop: the left part of the translation result in a right-branching language (e.g., English) is more likely to be more accurate than its right part, while the right part is more accurate for a left-branching language (e.g., Japanese).",abstractText,[0],[0]
"Our discoveries are confirmed on different model structures including Transformer and RNN, and in other sequence generation tasks such as text summarization.",abstractText,[0],[0]
Beyond Error Propagation in Neural Machine Translation: Characteristics of Language Also Matter,title,[0],[0]
"Tremendous power of convolutional neural networks (CNNs) have been well demonstrated in a wide variety of computer vision applications, from image classification (Simonyan & Zisserman, 2015) and object detection (Ren et al., 2015) to image segmentation (Long et al., 2015).",1. Introduction,[0],[0]
"Meanwhile, there is a recent consensus that there are
1Key Laboratory of Machine Perception (MOE) and Cooperative Medianet Innovation Center, School of EECS, Peking University, Beijing 100871, P.R. China.",1. Introduction,[0],[0]
"2UBTech Sydney AI Institute, School of IT, FEIT, The University of Sydney, Darlington, NSW 2008, Australia.",1. Introduction,[0],[0]
"Correspondence to: Yunhe Wang <wangyunhe@pku.edu.cn>, Chang Xu <c.xu@sydney.edu.au>, Chao Xu <xuchao@cis.pku.edu.cn>, Dacheng Tao <dacheng.tao@sydney.edu.au>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
significant redundancy in most of existing convolutional neural networks.,1. Introduction,[0],[0]
"For instance, the ResNet-50 (He et al., 2015) with some 50 convolutional layers needs over 95MB memory for storage and over 3.8 × 109 times of floating number multiplications for calculating each image, and after discarding more than 75% of its weights, the network still works as usual (Wang et al., 2016).
",1. Introduction,[0],[0]
"Admittedly, a heavy neural network is extremely difficult to train and to use in mobile terminal apps due to the limited memory and computational resource.",1. Introduction,[0],[0]
"Lots of methods have been developed to reduce the amount of parameters in CNNs (Liu et al., 2015) to obtain a considerable compression ratio.",1. Introduction,[0],[0]
"(Han et al., 2015) discarded subtle weights in a pre-trained network and constructed a sparse neural network with less computational complexity.",1. Introduction,[0],[0]
"Subsequently, (Wang et al., 2016) further studied the redundancy on all weights and their underlying connections in the DCT frequency domain, which achieves higher compression and speed-up ratios.",1. Introduction,[0],[0]
"(Wen et al., 2016) excavated subtle connections in different aspects and (Figurnov et al., 2016) refined the conventional convolution neurons as locally connection on the input data in order to reduce the computational cost.",1. Introduction,[0],[0]
"In addition, there are a variety of techniques for compressing convolution filters, e.g., pruning (Han et al., 2016; Hu et al., 2016; Li et al., 2016), quantization and binarization (Arora et al., 2014; Rastegari et al., 2016; Courbariaux & Bengio, 2016), matrix approximation (Cheng et al., 2015), and matrix decomposition (Denton et al., 2014).
",1. Introduction,[0],[0]
"Although these methods obtained promising performance to reduce the storage of convolution filters, the memory usage introduced by filters is still huge in the stage of online inference.",1. Introduction,[0],[0]
"This is because except convolution filters, we have to store feature maps (output data) of different layers for the subsequent processes, e.g., over 97MB memory is required for storing feature maps of one single image when running a ResNet-50 (He et al., 2015) without batch normalization layers, and a batch consisting of 32 instances consumes some 3.2GB GPU memory.",1. Introduction,[0],[0]
"However, existing compression methods tend to directly compress the filters in one step and rarely consider the significant demand of feature maps on the storage and computational cost.
",1. Introduction,[0],[0]
"In a CNN, the size of a convolution filter is usually much
smaller than the number of filters in a convolutional layer.",1. Introduction,[0],[0]
"Given a convolutional layer with 1024 filters of size 3× 3, any 3×3 patch in the input data will be mapped into a 1024- dimensional space R9 → R1024.",1. Introduction,[0],[0]
"The size of the space to describe this small patch has broken up more than 100 folds, which leads to the redundancy of the feature map.",1. Introduction,[0],[0]
We are therefore motivated to discover the intrinsic representations of the redundant feature maps via dimensionality reduction.,1. Introduction,[0],[0]
Circulant matrix is employed to formulate the feature map transformation considering its low space complexity and high mapping speed.,1. Introduction,[0],[0]
"Based on the obtained compact feature maps, we re-formulate the convolution filters to establish its connections with the input data.",1. Introduction,[0],[0]
"In summary, the proposed approach makes the following contributions: • We propose to excavate the intrinsic information and
decrease the redundancy in feature maps derived from a large number of filters in each layer, and then the network architecture is upgraded to produce a new compact network with fewer filters but the similar discriminativeness.",1. Introduction,[0],[0]
• We devise to learn a circulant matrix for projection which is exactly a diagonal matrix in the Fourier domain and thus yields a high speed for training and low complexity for mapping.,1. Introduction,[0],[0]
"• Experiments demonstrate that, compared to the original heavy network, the learned portable counterpart network achieves a comparable accuracy, but has significantly lower memory usage and computational cost.",1. Introduction,[0],[0]
"Here we will first introduce some preliminaries of CNNs and then develop feature map dimensionality reduction method.
",2. Feature Map Dimensionality Reduction,[0],[0]
"For a convolutional layer L in a pre-trained convolution neural network N whose input data and output feature maps are X ∈ RH×W×c and Y ∈ RH′×W ′×d, respectively, where H , W , H ′, W ′ are widths and heights of X and Y , c is the channel number (i.e., the number of convolution filters in the previous layer) and d is the number of convolution filters in this layer.",2. Feature Map Dimensionality Reduction,[0],[0]
"These convolution filters can be denoted as a tensor, i.e., F ∈ Rs1×s2×c×d, where s1 and s2 are the width and the height of filters, respectively.",2. Feature Map Dimensionality Reduction,[0],[0]
"Taking the neural network as a powerful feature extractor, the convolutional layer then becomes a mapping from the patch x ∈ Rs1×s2 to feature map y ∈ Rd×1.
",2. Feature Map Dimensionality Reduction,[0],[0]
"Generally, d is much larger than s = s1× s2, e.g., d = 256 and s = 1 in the second layer in ResNet-50, and there are even some layers with d > 2000.",2. Feature Map Dimensionality Reduction,[0],[0]
"Admittedly, using such a long d-dimensional vector to represent a s1 × s2 area is heavy and redundant.",2. Feature Map Dimensionality Reduction,[0],[0]
"To decrease the storage and com-
putation cost of the feature map, we attempt to discover the compact representations of the feature maps.",2. Feature Map Dimensionality Reduction,[0],[0]
"Many sophisticated methods such as locally linear embedding (LLE, (Roweis & Saul, 2000)), principle component analysis (PCA, (Wold et al., 1987; Pan & Jiashi, 2017)), isometric feature mapping (Isomap, (Tenenbaum et al., 2000)), locality preserving projection (LLP, (He & Niyogi, 2004)), and other excellent dimensionality methods (Pan et al., 2016; Xu et al., 2014), can be applied for dimensionality reduction.",2. Feature Map Dimensionality Reduction,[0],[0]
"Low-dimensional representations produced by these methods can inherit intrinsic information of original high-dimensional data, so that the performance of the transformed features can be maintained, and enhanced in some cases.",2. Feature Map Dimensionality Reduction,[0],[0]
"We thus proceed to develop an exclusive feature map dimensionality reduction method for the deep network compression problem.
",2. Feature Map Dimensionality Reduction,[0],[0]
Dividing X into q = H ′,2. Feature Map Dimensionality Reduction,[0],[0]
"× W ′ patches and vectorizing them, we have X =",2. Feature Map Dimensionality Reduction,[0],[0]
"[vec(x1), ..., vec(xq)] ∈ Rsc×q .",2. Feature Map Dimensionality Reduction,[0],[0]
"Accordingly, we reformulate feature maps and convolution filters as Y =",2. Feature Map Dimensionality Reduction,[0],[0]
"[vec(Y1), ..., vec(Yd)] ∈ Rq×d and F = [vec(F1), ..., vec(Fd)]",2. Feature Map Dimensionality Reduction,[0],[0]
"∈ Rsc×d, respectively.",2. Feature Map Dimensionality Reduction,[0],[0]
"Thus, the conventional convolution operation of the given layer L can be rewritten as:
Y = XTF, (1)
where the i-th column in Y corresponds to the convolution responses of all patches through the i-th filter in F. Note that, when we consider the entire dataset, q should be additionally multiplied by the number of samples which is an extreme large number.",2. Feature Map Dimensionality Reduction,[0],[0]
The most compact representation of a CNN should have no correlation between feature maps derived from different convolution filters.,2. Feature Map Dimensionality Reduction,[0],[0]
"In other words, feature maps from different filters should be independent of each other as far as possible.",2. Feature Map Dimensionality Reduction,[0],[0]
"The independence (or redundancy) in Y can be measured by
Θ(Y) = ||YTY ◦ (1− I)||2F , (2)
where || · ||F is the Frobenius norm for matrices, 1 is a full one matrix, ◦ is the element-wise product, and I is an identity matrix.",2. Feature Map Dimensionality Reduction,[0],[0]
"Denote the reduced feature maps as Ỹ = φ(Y) ∈ Rq×d̃, where d̃ ≤ d and φ(·) can be either a linear (Wold et al., 1987) or non-linear transformation (Roweis & Saul, 2000).",2. Feature Map Dimensionality Reduction,[0],[0]
"However, since there are numerous training samples in real word image datasets (e.g., ImageNet (Russakovsky et al., 2015)), computational complexities of the nonlinear transformation will be great.",2. Feature Map Dimensionality Reduction,[0],[0]
"We thus use the linear transformation instead, i.e., Ỹ = φ(Y) =",2. Feature Map Dimensionality Reduction,[0],[0]
"YPT , where P ∈ Rd̃×d is the projection matrix.",2. Feature Map Dimensionality Reduction,[0],[0]
An optimal transformation should generate the new representations which occupy more information of the original input and have less internal correlation.,2. Feature Map Dimensionality Reduction,[0],[0]
"Based on the measurement in Fcn. 2, the optimal projection P can be solved
by minimizing the following function:
min P,c ||PYTYPT − C||2F , s.t. C = diag(c), (3)
where C = diag(c) is a diagonal matrix, whose functionality is equal to the identity matrix in Fcn. 2.
",2. Feature Map Dimensionality Reduction,[0],[0]
Deep neural network enjoys great popularity due to its excellent capability of learning effective features for examples.,2. Feature Map Dimensionality Reduction,[0],[0]
"An optimal projection should thus not only remove redundancy between feature maps, but also preserve the discriminability of these features.",2. Feature Map Dimensionality Reduction,[0],[0]
"If images from different categories are well separated from each other in the feature space, classifiers will more easily accomplish the classification task.",2. Feature Map Dimensionality Reduction,[0],[0]
"To maintain the accuracy of the original network and its representation capability, we propose to preserve the distances between feature maps and form the following objective function for seeking the compact feature maps:
min P,c ||PYTYPT − C||2F + λ||D(Ỹ)−D(Y)||
s.t. Ỹ = YPT , C = diag(c), (4)
where D(Y) = D ∈ Rq×q and Dij is the Euclidean distance between the i-th column and the j-th column of Y.",2. Feature Map Dimensionality Reduction,[0],[0]
The above section has proposed a feature map dimensionality reduction model.,3. Optimal Feature Map Learning,[0],[0]
"However, calculating the distance matrix D = D(·) is inefficient and memory consuming since the column length of Y corresponds to the number of training samples and is rather large in practice.",3. Optimal Feature Map Learning,[0],[0]
"For example, there are over 1.2×106 images in the ILSVRC-2012 dataset and there are up to 4096 filters of a network layer.",3. Optimal Feature Map Learning,[0],[0]
"The size ofD will be larger than 4×109, which is inconvenient for distance calculation.",3. Optimal Feature Map Learning,[0],[0]
"In this section, we will propose an alternative feature map dimensionality reduction approach, which consists of two steps: distance preservation and sparse approximation.
",3. Optimal Feature Map Learning,[0],[0]
"In fact, distances between feature maps can be easily preserved if P is orthogonal, i.e., PPT = I, where I is an identity matrix with size",3. Optimal Feature Map Learning,[0],[0]
d × d.,3. Optimal Feature Map Learning,[0],[0]
"For any two feature maps y1, y2,∈ Rd, we have ||y1PT ||2 = ||y1||2 and ||y1PT",3. Optimal Feature Map Learning,[0],[0]
− y2PT ||22 = ||y1 − y2||22.,3. Optimal Feature Map Learning,[0],[0]
"We thus reformulate Fcn. 4 as
min P ||PYTYPT − C||2F ,
s.t. C = diag(c), PPT = I. (5)
",3. Optimal Feature Map Learning,[0],[0]
"The orthogonal transformation P learned by Fcn. 5 is able to extract the intrinsic representation and preserve distances between feature maps, but the dimensionality has not been reduced since P is a square matrix.",3. Optimal Feature Map Learning,[0],[0]
"Hence at the second
stage, we propose to use a sparse matrix to approximate the representation generated by P ,
min Ỹ
1 2 ||Ỹ −YPT ||2F",3. Optimal Feature Map Learning,[0],[0]
"+ λ||Ỹ||2,1, (6)
",3. Optimal Feature Map Learning,[0],[0]
"where ||Ỹ||2,1 = ∑ i ||Ỹi|| and Ỹi is the i-th column in Ỹ. || · ||2,1 is `2,1-norm which is a widely used regularization (Nie et al., 2010; Liu et al., 2010) for removing useless columns in Ỹ and the closed form solution of Fcn. 6 is
Ỹi =
{ ||ui||−λ ||ui|| ui, if λ < ||ui||
0, otherwise (7)
where ui = YPTi and Pi is the i-th column in P .",3. Optimal Feature Map Learning,[0],[0]
"Zero columns in Ỹ can be discarded to achieve the lowdimensional representations.
",3. Optimal Feature Map Learning,[0],[0]
"By combining Fcn. 6 and Fcn. 5, we can obtain a unified model as follow
min P,c ||PYTYPT − C||2F + β||c||1
s.t. C = diag(c), PPT = I, (8)
where || · ||1 is the `1 norm to make c sparse, so that some small valued columns in Ỹ = YPT will be discarded.",3. Optimal Feature Map Learning,[0],[0]
"β is a weight parameter which controls the sparsity of Ỹ and implicitly influence the resulting dimensionality of the new feature map of this layer.
",3. Optimal Feature Map Learning,[0],[0]
"Considering there are d × d variables in P and d can be up to 4096, Fcn. 8 cannot be efficiently optimized w.r.t.",3. Optimal Feature Map Learning,[0],[0]
P .,3. Optimal Feature Map Learning,[0],[0]
"Since each frequency coefficient corresponds to a Fourier base with different textures, circulant matrices have complex internal structures and strong diversity thus can be utilized for approximating huge matrices (Cheng et al., 2015).",3. Optimal Feature Map Learning,[0],[0]
"We therefore propose using a circulant matrix (Gray, 2006; Henriques et al., 2015; 2014) to formulate P , which then only has d variables in the Fourier frequency domain.",3. Optimal Feature Map Learning,[0],[0]
"We propose the following model to learn the projection for generating the compact low-dimensional feature maps:
min p,c ||PYTYPT − C||2F + α||PPT",3. Optimal Feature Map Learning,[0],[0]
"− I||2F + β||c||1,
s.t. P = circ(p), C = diag(c), (9)
where α is the weight for relaxing the equality constrain in Fcn. 5, and P = circ(p) is a circulant matrix.",3. Optimal Feature Map Learning,[0],[0]
"For the d×1 vector p = (p1, ..., pd)T , we can refer to it as the base sample, and the cyclic shift operator can be defined as:
circ(p)",3. Optimal Feature Map Learning,[0],[0]
:=  p1 pd . . .,3. Optimal Feature Map Learning,[0],[0]
p3 p2 p2 p1 pd p3 ...,3. Optimal Feature Map Learning,[0],[0]
p2 p1 . . .,3. Optimal Feature Map Learning,[0],[0]
"...
",3. Optimal Feature Map Learning,[0],[0]
pd−1 . . . . . .,3. Optimal Feature Map Learning,[0],[0]
"pd
pd pd−1 . . .",3. Optimal Feature Map Learning,[0],[0]
"p2 p1
 .",3. Optimal Feature Map Learning,[0],[0]
"(10)
Given the fact that all circulant matrices are made diagonal by the discrete Fourier transform (DFT (Bracewell, 1965)), P and PT can be expressed as
P = 1
d SH diag(p̂) S, PT =
1 d S diag(p̂) SH , (11)
where S is the DFT transform matrix which is constant, the DFT is a linear transform, and SHS = dI. p̂ is the frequency representation of p, i.e.,
p̂ = F(p)",3. Optimal Feature Map Learning,[0],[0]
"= Sp, (12)
and its inverse discrete Fourier transform (IDFT) is p = F−1(p̂) = 1dS
H(p̂).",3. Optimal Feature Map Learning,[0],[0]
"In following illustrations, we will use a hat ˆ to denote the DFT frequency representations.
",3. Optimal Feature Map Learning,[0],[0]
Since any two bases in S are orthogonal thus it can hold the Euclidean distance between any two vectors.,3. Optimal Feature Map Learning,[0],[0]
"For any two d-dimensional samples in Y we have
||SyT1 ||22 = 1
d ||y1||22,
||SyT1 − SyT2 ||22 = 1
d ||y1 − y2||22.
(13)
",3. Optimal Feature Map Learning,[0],[0]
"In addition, the most elegant property of the circulant matrix is that the projection in the original domain is equivalent to vector element wise product in the Fourier domain (Oppenheim et al., 1999), which is beneficial for significantly decreasing the computational complexity, i.e.,
F(PyT ) = F(p) ◦ F(y)T , F(yPT ) = F(y) ◦ F(p)T ,
(14)
",3. Optimal Feature Map Learning,[0],[0]
where ◦ denotes the element wise product.,3. Optimal Feature Map Learning,[0],[0]
"Since DFT and IDFT can be efficiently computed in O(d log d) with the fast Fourier transform (FFT, (Bracewell, 1965)), the projection for generating low-dimensional feature maps is only O(d log d), compared with the O(d2) complexity of the original dense matrix multiplication.",3. Optimal Feature Map Learning,[0],[0]
"Considering the efficient computation over circulant matrix in the frequency domain, we propose to use the frequency optimizing approach to obtain the optimal feature maps representation Ỹ = YPT .",3. Optimal Feature Map Learning,[0],[0]
"We optimize Fcn. 9 by alternatively fixing p and c, and leave the optimization details in the supplementary materials for the limited page length.
",3. Optimal Feature Map Learning,[0],[0]
"Given the optimal p̂ and c, the transformation for reducing the dimensionality of feature maps can be written as
P = 1
d diag(c̄)SH P̂S, (15)
where c̄i = 0, if ci = 0, and c̃i = 1, otherwise, the transformation P in Fcn. 9 is a row sparse matrix, and the rows with all zeros can be discarded to reformulate a compact transformation matrix Pd̃ with d̃ rows according to c̄.
Therefore the feature map matrix Y can be transformed as Ỹ = YPT
d̃ .",3. Optimal Feature Map Learning,[0],[0]
"This projection is exactly a linear transform,
if we only take one convolutional layer into consideration, i.e., the input data matrix X is fixed, we can explicitly include the filter matrix F into the dimensionality reduction procedure, i.e.,
Ỹ = YPT d̃ = XFTPT d̃ = XF̃T .",3. Optimal Feature Map Learning,[0],[0]
"(16)
Hence, we can also directly reduce the number of convolution filters after obtaining the optimal projection matrix Pd̃. Fixing the filter size as s1×s2×c, we can reconstruct convolutional layers with smaller filters F̃ ∈",3. Optimal Feature Map Learning,[0],[0]
Rs1×s2×d̃.,3. Optimal Feature Map Learning,[0],[0]
"Based on the above analysis, we have the following proposition:
Proposition 1.",3. Optimal Feature Map Learning,[0],[0]
"Given a convolutional layer L with d filters, i.e., its feature map dimensionality is d. For the ddimensional feature of any sample through L, the proposed method for solving its low-dimensional embedding has space complexity O(d), and time complexity O(d log d).",3. Optimal Feature Map Learning,[0],[0]
Section 3 proposes an effective approach for learning compact feature maps of a given convolutional layer.,4. CNN Layer Reconstruction,[0],[0]
"In the online inference, it is impossible to first calculate the original high-dimensional feature maps, and then project them into the low-dimensional space.",4. CNN Layer Reconstruction,[0],[0]
"To conserve the online computation resource, we thus aim to establish the mapping directly from the input data to the compact feature map.
",4. CNN Layer Reconstruction,[0],[0]
"The dimensionality of the feature map for the i-th convolutional layer Li has been reduced by Fcn. 16, and the number of convolution filters of Li has also been reduced from d to d̃ (where d̃ d).",4. CNN Layer Reconstruction,[0],[0]
"For the following convolutional layer L(i+1), the size of the input data X̃ has becomesH×W×d̃ and we have X̃ ∈ Rsd̃×q , which leads original filters can no longer be used for calculating.",4. CNN Layer Reconstruction,[0],[0]
"Thus, we propose minimizing the following function for reconstructing convolution filters of this layer:
min F̃ ||Ỹ",4. CNN Layer Reconstruction,[0],[0]
− X̃T F̃||2F,4. CNN Layer Reconstruction,[0],[0]
"+ γ||F̃||2F , (17)
where Ỹ is the compact feature maps of L(i+1) after applying Fcn. 16 and γ is a weight parameter for balancing the two terms.",4. CNN Layer Reconstruction,[0],[0]
"Note that the second term can be regarded as a weight decay regularization in the training of neural networks (Krizhevsky et al., 2012).",4. CNN Layer Reconstruction,[0],[0]
"Fcn. 17 can be efficiently solved by the following closed form solution:
F̃ = (X̃X̃T + γI)−1X̃Ỹ, (18)
where I is an identity matrix.",4. CNN Layer Reconstruction,[0],[0]
"However, when the scale of the dataset is enormous, we cannot construct the two huge matrices X̃ and Ỹ through all instances in the dataset.",4. CNN Layer Reconstruction,[0],[0]
"The
Algorithm 1 CNN Layer Reconstruction Method.",4. CNN Layer Reconstruction,[0],[0]
"Input: A pre-trained convolutional neural network N learned
through a dataset X with k convolutional layers: L1, ...,Lk, weight parameter γ, learning rate η. 1: Calculate feature maps of each layer by using the original network: {Y1, ...Yk} ← N (X ); 2: for i = 1 to k",4. CNN Layer Reconstruction,[0],[0]
"− 1 do 3: Learn the projection Pi by solving Fcn. 9; 4: Calculate new feature maps: Ỹi ← YiPTi ; 5: end for 6: Keep feature maps of the k-th layer: Ỹk ← Yk; 7: Construct a new network Ñ according to {Ỹ1, ...Ỹk} and
initialize convolution filters {F̃1, ...F̃k} by random values from the standard normal distribution;
8: repeat 9: Randomly select a batch Xj from X ;
10: for i = 1 to k do 11: Generate input data X̃i of Li exploiting Ñ ; 12: Estimate the new filter matrix (Fcn. 20):",4. CNN Layer Reconstruction,[0],[0]
"F̃i ← F̃i − η∂L(F̃i)/∂F̃i; 13: Convert F̃i into filter data and fill it in Ñ ; 14: end for 15: until Convergence; Output: The new convolutional neural network Ñ .
mini-batch strategy is adopted for updating F̃ iteratively.",4. CNN Layer Reconstruction,[0],[0]
"The loss function of F̃ can be directly formulated as
L(F̃) = Tr(F̃T X̃T X̃F̃)
",4. CNN Layer Reconstruction,[0],[0]
− 2Tr(F̃T X̃Ỹ) +,4. CNN Layer Reconstruction,[0],[0]
"γTr(F̃T F̃), (19)
and the gradient of L(F̃) is
∂L(F̃)
",4. CNN Layer Reconstruction,[0],[0]
"∂F̃ = 2X̃X̃T F̃− 2X̃Ỹ + 2γF̃. (20)
",4. CNN Layer Reconstruction,[0],[0]
"By using stochastic gradient descent (SGD), F̃ can be updated as
F̃ = F̃− η ∂L(F̃) ∂F̃ , (21)
where η is the learning rate.
",4. CNN Layer Reconstruction,[0],[0]
It is worth mentioning that input data of the first layer of the original network N and feature map of the last layer (closely related to the number of classification labels) are kept unchanged.,4. CNN Layer Reconstruction,[0],[0]
"As for other intermediate convolutional layers and fully connected layers, we can generate compact feature maps Ỹ from the original feature maps",4. CNN Layer Reconstruction,[0],[0]
"Y. Then, calculate the input data X̃ using the compressed network Ñ and estimate the filter matrix F̃. The detailed filters updating procedure can be found in Alg. 1.
",4. CNN Layer Reconstruction,[0],[0]
"According to Proposition 1, for a d-dimensional feature, the complexity of the proposed feature map dimensionality reduction method with the help of circulant matrix is only O(d log d).",4. CNN Layer Reconstruction,[0],[0]
"Compared to O(d2) of other traditional linear projection methods, the proposed scheme is of great benefit
for conducting experiments on large scale datasets.",4. CNN Layer Reconstruction,[0],[0]
"Moreover, since we only need to store a d-dimensional vector for one layer, the proposed method also have an obvious advantage on the space complexity for learning a portable version of neural networks with a larger number of layers (e.g., ResNet (He et al., 2015)).
",4. CNN Layer Reconstruction,[0],[0]
Discussion.,4. CNN Layer Reconstruction,[0],[0]
There are some works investigating the intrinsic information of feature maps in the original neural network to learn a new thinner and deeper neural network.,4. CNN Layer Reconstruction,[0],[0]
"(Hinton et al., 2015) first built a thinner neural network and then made its feature map of the fully connected layer similar to that of the original pre-trained networks, thus enhanced the accuracy of the new network.",4. CNN Layer Reconstruction,[0],[0]
"(Romero et al., 2015) further extended this work to a general model which minimizes the difference between the feature map of an arbitrary layer in the smaller network and the feature map of a given layer in the original network, yields a thinner and deeper network with some accuracy decline.
",4. CNN Layer Reconstruction,[0],[0]
"The difference between these methods and the proposed method is two-fold: (i) These methods rebuild a new student network with less parameters while the proposed method outputs a compact CNN based on the original network itself, which inherits the well-designed architectures; (ii)",4. CNN Layer Reconstruction,[0],[0]
"The performance of the newly learned student network will be declined, since it is only influenced by the information from one or several layers of the teacher network.",4. CNN Layer Reconstruction,[0],[0]
"By contrast, the proposed method excavates redundancy in feature maps of every layer and preserves the distances between examples to guarantee the accuracy of the CNN.",4. CNN Layer Reconstruction,[0],[0]
A novel dimensionality reduction method for learning a portable neural network has been proposed in Alg. 1.,5. Analysis on Compression Performance,[0],[0]
"Compared with the original heavy networkN , the new network Ñ has the same depth but less convolution filters per layer.",5. Analysis on Compression Performance,[0],[0]
"In this section, we will further analyze the memory usage and computation cost of Ñ and calculate the compression ratio and speed-up ratio theoretically.
",5. Analysis on Compression Performance,[0],[0]
Speed-up ratio.,5. Analysis on Compression Performance,[0],[0]
Consider the i-th convolutional layerLi in the original networkN with its output feature map and convolution filters are,5. Analysis on Compression Performance,[0],[0]
Yi ∈ RH ′,5. Analysis on Compression Performance,[0],[0]
i×W ′,5. Analysis on Compression Performance,[0],[0]
"i×di and F ∈ Rs2i×ci×di , respectively.",5. Analysis on Compression Performance,[0],[0]
We only discuss square filters and the conclusion can be straightforwardly extended to non-square filters as well.,5. Analysis on Compression Performance,[0],[0]
"Wherein, ci = di−1 is the channel number of filters in Li and d0 = 3",5. Analysis on Compression Performance,[0],[0]
(RGB color images).,5. Analysis on Compression Performance,[0],[0]
The feature map and convolution filters in the corresponding layer L̃i in the learned compact network are Yi ∈ RH ′,5. Analysis on Compression Performance,[0],[0]
i×W ′,5. Analysis on Compression Performance,[0],[0]
"i×d̃i and F ∈ Rs2i×c̃i×d̃i , respectively.",5. Analysis on Compression Performance,[0],[0]
"Generally, weights and feature maps are stored in 32-bit floating values whose multiplications are much more expensive than additions.",5. Analysis on Compression Performance,[0],[0]
"Complexities of other auxiliary layers (e.g., pooling, Relu, etc..)
have been discarded since they only account for a subtle proportion of the overall complexity.",5. Analysis on Compression Performance,[0],[0]
"Hence, considering the major multiplications, the speed-up ratio of the compact network for this layer compared with the original network is
rs = s2i di−1diH ′ iW ′",5. Analysis on Compression Performance,[0],[0]
"i
s2i d̃i−1d̃iH ′",5. Analysis on Compression Performance,[0],[0]
iW ′,5. Analysis on Compression Performance,[0],[0]
"i
= di−1di
d̃i−1d̃i .",5. Analysis on Compression Performance,[0],[0]
"(22)
",5. Analysis on Compression Performance,[0],[0]
It is obvious that the speed-up ratio for one convolutional layer relates to numbers of filters in this layer and the previous layer.,5. Analysis on Compression Performance,[0],[0]
"Thus, if we only keep 1/2 filters per layer, the speed-up ratio will be increased to 4×.
Compression ratio.",5. Analysis on Compression Performance,[0],[0]
"The online memory can be divided into two major parts, feature maps of different layers and filters of convolutional layers.",5. Analysis on Compression Performance,[0],[0]
"Although we can remove feature maps of a layer after it has already been used for calculating the following layer, the the memory allocation and release will increase the time consumption.",5. Analysis on Compression Performance,[0],[0]
"Moreover, if a layer is connected with several other convolutional layers, we have to store feature maps of previous layers when doing online inference (e.g., the second convolutional layer and the fifth convolutional layer will be combined in ResNet-50 (He et al., 2015), and we have to preserve these feature maps before merging them).",5. Analysis on Compression Performance,[0],[0]
"For a given convolutional layer Li, the compression ratio of the proposed method is
rc = s2i di−1di + diH ′ iW ′",5. Analysis on Compression Performance,[0],[0]
"i
s2i d̃i−1d̃i + d̃iH ′",5. Analysis on Compression Performance,[0],[0]
iW ′,5. Analysis on Compression Performance,[0],[0]
"i
, (23)
which is simultaneously affected by the current layer and the pervious layer.",5. Analysis on Compression Performance,[0],[0]
"Meanwhile, the memory for storing feature maps of other layers, such as pooling layers and Relu layers, will be reduced.",5. Analysis on Compression Performance,[0],[0]
We will further illustrate the detailed compression ratio and speed-up ratio of the proposed method in the following section experimentally.,5. Analysis on Compression Performance,[0],[0]
Baselines and Models.,6. Experiments,[0],[0]
"Several effective approaches for compressing deep neural networks were selected for comparison: SVD (Denton et al., 2014), XNOR-Net (Rastegari
et al., 2016), Pruning (Han et al., 2016), Perforation (Figurnov et al., 2016), and CNNpack (Wang et al., 2016), and we denoted the proposed method as RedCNN.",6. Experiments,[0],[0]
"The evaluation was conducted on the MNIST and ILSVRC2012 datasets (Russakovsky et al., 2015).",6. Experiments,[0],[0]
"We first tested the performance of the proposed method and analyzed impacts of parameters on the MNIST dataset using LeNet (LeCun et al., 1998), then compared the proposed method with two benchmark CNNs (VGGNet-16 (Simonyan & Zisserman, 2015) and ResNet-50 (He et al., 2015)) on the ILSVRC 2012 dataset (Russakovsky et al., 2015) which has more than 1 million nature images.",6. Experiments,[0],[0]
"All methods were implemented using MatConvNet (Vedaldi & Lenc, 2015) and ran on NVIDIA K40 cards.",6. Experiments,[0],[0]
"Filters and data in CNNs were stored and calculated as 32 bit floating-point values.
",6. Experiments,[0],[0]
Impact of parameters.,6. Experiments,[0],[0]
The hyper-parameter γ in the proposed reconstruction method (Alg. 1) controls the weight decay regularization and makes weights in new convolution filters not too large.,6. Experiments,[0],[0]
We set γ as 0.0005 empirically.,6. Experiments,[0],[0]
"In addition, the proposed dimensionality method (Fcn. 9) has two hyper-parameters, i.e., α and β.",6. Experiments,[0],[0]
"We first tested their impact on the network accuracy by conducting an experiment using a LeNet for classifying the MNIST dataset (Vedaldi & Lenc, 2015), where the network has four convolutional layers of size 5×5×1×20, 5×5×20×50, 4×4×50×500, and 1×1×500×10, respectively, and its accuracy is 99.2%.",6. Experiments,[0],[0]
"α is used for enforcing the projection matrix P to be orthogonal and is set to be 1.5, experimentally.",6. Experiments,[0],[0]
"β is directly related to the sparsity of P , and it effects compression and speed-up ratios of the proposed method.",6. Experiments,[0],[0]
"Although a larger β will produce a smaller network, it also leads to a larger distortion on the Euclidean distances between samples.",6. Experiments,[0],[0]
"To have a clear illustration, we reported the compression performance by ranging different β, as shown in Fig. 1.
",6. Experiments,[0],[0]
"From Fig. 1 (a), we found that the compact network reconstructed by using Alg. 1 can also hold a considerable accuracy (e.g., 78% when β = 0.06), which demonstrates that the proposed method can preserve the intrinsic information of the original CNN.",6. Experiments,[0],[0]
"Moreover, the accuracy decline can be rebounded (98.9% when β = 0.06) after fine-tuning as
shown in Fig. 1 (b).",6. Experiments,[0],[0]
"However, a network that was directly trained with such a small architecture can only achieve a 92.8% accuracy.",6. Experiments,[0],[0]
"In addition, although the impact of β is sensitive but monotonous, a larger β enhances compression and speed-up ratios simultaneously, but decreases the accuracy of CNNs as well.",6. Experiments,[0],[0]
The value of β can be easily adjusted according to the demand and restrictions of devices.,6. Experiments,[0],[0]
"In our experiments, we set β = 0.06 which provides the best trade-off between compression performance and accuracy, i.e., rc = 11.3×, rs = 8.7×, with 99.16% accuracy.",6. Experiments,[0],[0]
"In this case, the layers in the compact convolution network is of the size 5×5×1×5, 5×5×5×20, 4×4×20×96, and 1×1×96×10, respectively.",6. Experiments,[0],[0]
The resulting compact network only occupies around 130KB memory.,6. Experiments,[0],[0]
"The MATLAB file of the compressed network and the demo code can be found in https://github.com/YunheWang/RedCNN.
",6. Experiments,[0],[0]
Deep Neural Networks Compression on ISLVRC-2012.,6. Experiments,[0],[0]
"We next employed the proposed RedCNN for CNN compression on the ImageNet ILSVRC-2012 dataset (Russakovsky et al., 2015), which contains over 1.2M training images and 50k validation images.",6. Experiments,[0],[0]
"We evaluated the compression performance on three widely used models: AlexNet (Krizhevsky et al., 2012), which has more than 61M parameters and a top-5 accuracy of 80.8%; VGGNet16 (Simonyan & Zisserman, 2015), which has over 138M parameters and a top-5 accuracy of 90.1%; and ResNet50 (He et al., 2015) which has more than 150 layers with 54 convolutional layers, and a top-5 accuracy of 92.9%.",6. Experiments,[0],[0]
"It is worth mentioning that there are considerable filters in the ResNet-50, and thus the network has less redundancy and it is hard for further compression.",6. Experiments,[0],[0]
"We first begin our experiment with the AlexNet dataset, and the detailed experimental results were shown in Tab. 1.
",6. Experiments,[0],[0]
"From Tab. 1, we found that the proposed method achieved a 5.1× compression ratio and a 4.3× speed-up ratio for AlexNet.",6. Experiments,[0],[0]
"Then, we reported the compression result of VGGNet-16 in Tab. 2.
",6. Experiments,[0],[0]
"It can be seen from Tab. 2, we obtained a 6.19× compression ratio and a 9.63× speed-up ratio on VGGNet-16.",6. Experiments,[0],[0]
"In addition, the compression ratio and the speed-up ratio on ResNet-50 are 4.14× and 5.82×, respectively.",6. Experiments,[0],[0]
"Note that the compression ratio we reported here is calculated by
Fcn. 23, which contains both convolution filters and feature maps.",6. Experiments,[0],[0]
"More compression results of these three CNN models can be found in the supplementary material.
",6. Experiments,[0],[0]
Comparison with state-of-the-art methods.,6. Experiments,[0],[0]
Tab. 3 details the compression results of the proposed method and several state-of-the-art methods on three benchmark deep CNN models.,6. Experiments,[0],[0]
"Since comparison methods do not change the number of filters of the original neural network, feature map compression ratios of these methods are both equal to 1.",6. Experiments,[0],[0]
"Thus, we reported the compression ratio of filters rc1 and feature maps rc2 separately for a fair comparison.",6. Experiments,[0],[0]
"For a convolutional neural network with layers, its compression ratios is calculated as
rc1 =
∑p i=1 s
2 i di−1di∑p
i=1 s 2 i d̃i−1d̃i
, rc2",6. Experiments,[0],[0]
"=
∑p i=1",6. Experiments,[0],[0]
"diH
′ iW ′",6. Experiments,[0],[0]
"i∑p
i=1",6. Experiments,[0],[0]
d̃iH ′,6. Experiments,[0],[0]
iW ′,6. Experiments,[0],[0]
"i
.",6. Experiments,[0],[0]
"(24)
Tab. 3 also shows the cost of various models for processing one image, i.e., storage of filters, memory usage of feature maps, and multiplications for calculating convolutions.",6. Experiments,[0],[0]
"It is obvious that feature maps accounting a considerable proportion of memory usage of the whole network, and the proposed RedCNN can provide significant compression ratios rc2 on every network.",6. Experiments,[0],[0]
"Although we can remove the feature map of a layer after calculation for saving memory, frequently allocating and releasing is also time consuming.
",6. Experiments,[0],[0]
"It can be seen from Tab. 3, RedCNN clearly achieves the best performance in terms of both the speed-up ratio (rs) and the feature map compression ratio (rc1 ).",6. Experiments,[0],[0]
"In addition, convolution filter compression ratios of the proposed method is lower than those of pruning (Han et al., 2016) and CNNpack (Wang et al., 2016).",6. Experiments,[0],[0]
"These two comparison methods employed quantization approaches (i.e., onedimensional k-means clustering), and thus 32-bit floating values can be converted into about 8-bit values without af-
fecting the accuracy of the original network.",6. Experiments,[0],[0]
"If we adopt this similar strategy, the convolution filter compression ratio rc1 of the proposed scheme can be further multiplied a factor of around 4×, e.g., we can obtain an almost 17.4× filter compression ratio on ResNet-50 model, which is superior to all the other comparison methods.",6. Experiments,[0],[0]
"However, 8-bit (or other unconventional format) value cannot be directly used in generic devices (e.g., GPU cards, mobile phones), and thus we did not try them in the experiments of this paper.",6. Experiments,[0],[0]
"In summary, the proposed RedCNN can achieve considerable compression and speed-up ratios, which can make existing deep models portable.
",6. Experiments,[0],[0]
Runtime.,6. Experiments,[0],[0]
"In fact, most of comparison methods cannot significantly accelerate the deep network for various additional operations.",6. Experiments,[0],[0]
"For example, (Han et al., 2016) needs to decode the CSR data before testing, which slows down the online inference and will not achieve the comparable compression and speed-up ratios with those of the proposed algorithm in practice.",6. Experiments,[0],[0]
"Since the proposed compression method directly re-configures the network into a more compact form, and does not require other additional support for realizing the network speed-up, the runtime of the compressed models for processing images will be reduced significantly.",6. Experiments,[0],[0]
"To explicitly demonstrate the superiority of the proposed method, we compared runtimes for recognizing images by benchmark CNN models before and after applying the proposed method, and showed the experimental results in Tab. 4.
",6. Experiments,[0],[0]
"We found that runtimes of these models after compression
were significantly reduced.",6. Experiments,[0],[0]
"The results are extremely encouraging, e.g., the compressed ResNet can recognize over 500 images per second.",6. Experiments,[0],[0]
"This efficiency can also be inherited into the fine-tuning process, therefore, the compressed networks can be quickly adjusted when applied them to a new dataset.",6. Experiments,[0],[0]
"In addition, the practical speed-up ratios of runtimes were slightly lower than the corresponding theoretical speed-up ratios rs due to the costs incurred by data transmission, pooling, padding, etc..",6. Experiments,[0],[0]
"Note that, the runtime reported here is a bit higher than that in (Vedaldi & Lenc, 2015), due to different configurations and hardware environments.",6. Experiments,[0],[0]
Compression methods for learning portable CNNs are urgently required so that neural networks can be used on mobile devices.,7. Conclusions and Discussions,[0],[0]
"Besides convolution filters, the storage of feature maps also accounts for a larger proportion of the online memory usage, we thus no longer search useless connections or weights of filters.",7. Conclusions and Discussions,[0],[0]
"In this paper, we present a feature map dimensionality reduction method by excavating and removing redundancy in feature maps generated by different filters.",7. Conclusions and Discussions,[0],[0]
"Although the portable network learned by our approach has significantly fewer parameters, its feature maps can also preserve intrinsic information of the original network.",7. Conclusions and Discussions,[0],[0]
Experiments conducted on benchmark datasets and models show that the proposed method can achieve considerable compression ratio and speed-up ratios simultaneously without affecting the classification accuracy of the original CNN.,7. Conclusions and Discussions,[0],[0]
"In addition, the compressed network generated by exploiting the proposed method is still a regular CNN with 32-bit float values which does not have any decoding or other procedures for online inference.",7. Conclusions and Discussions,[0],[0]
"We thank supports of NSFC 61375026 and 2015BAF15B00, and ARC Projects: FT-130101457, DP-140102164, LP-150100671.",Acknowledgements,[0],[0]
"Convolutional neural networks (CNNs) have shown extraordinary performance in a number of applications, but they are usually of heavy design for the accuracy reason.",abstractText,[0],[0]
"Beyond compressing the filters in CNNs, this paper focuses on the redundancy in the feature maps derived from the large number of filters in a layer.",abstractText,[0],[0]
We propose to extract intrinsic representation of the feature maps and preserve the discriminability of the features.,abstractText,[0],[0]
"Circulant matrix is employed to formulate the feature map transformation, which only requires O(d log d) computation complexity to embed a d-dimensional feature map.",abstractText,[0],[0]
"The filter is then reconfigured to establish the mapping from original input to the new compact feature map, and the resulting network can preserve intrinsic information of the original network with significantly fewer parameters, which not only decreases the online memory for launching CNN but also accelerates the computation speed.",abstractText,[0],[0]
Experiments on benchmark image datasets demonstrate the superiority of the proposed algorithm over state-ofthe-art methods.,abstractText,[0],[0]
Beyond Filters: Compact Feature Map for Portable Deep Model,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2204–2214, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics
We develop a novel bi-directional attention model for dependency parsing, which learns to agree on headword predictions from the forward and backward parsing directions. The parsing procedure for each direction is formulated as sequentially querying the memory component that stores continuous headword embeddings. The proposed parser makes use of soft headword embeddings, allowing the model to implicitly capture high-order parsing history without dramatically increasing the computational complexity. We conduct experiments on English, Chinese, and 12 other languages from the CoNLL 2006 shared task, showing that the proposed model achieves state-of-the-art unlabeled attachment scores on 6 languages.1",text,[0],[0]
"Recently, several neural network models have been developed for efficiently accessing long-term memory and discovering dependencies in sequential data.",1 Introduction,[0],[0]
"The memory network framework has been studied in the context of question answering and language modeling (Weston et al., 2015; Sukhbaatar et al., 2015), whereas the neural attention model under the encoder-decoder framework has been applied to machine translation (Bahdanau et al., 2015) and constituency parsing (Vinyals et al., 2015b).",1 Introduction,[0],[0]
"Both frameworks learn the latent alignment between the source and target sequences, and the mechanism of
1Our software and models are available at https:// github.com/hao-cheng/biattdp.
attention over the encoder can be viewed as a soft operation on the memory.",1 Introduction,[0],[0]
"Although already used in the encoder for capturing global context information (Bahdanau et al., 2015), the bi-directional recurrent neural network (RNN) has yet to be employed in the decoder.",1 Introduction,[0],[0]
"Bi-directional decoding is expected to be advantageous over the previously developed uni-directional counterpart, because the former exploits richer contextual information.",1 Introduction,[0],[0]
"Intuitively, we can use two separate uni-directional RNNs where each one constructs its respective attended encoder context vectors for computing RNN hidden states.",1 Introduction,[0],[0]
"However, the drawback of this approach is that the decoder would often produce different alignments resulting in discrepancies for the forward and backward directions.",1 Introduction,[0],[0]
"In this paper, we design a training objective function to enforce attention agreement between both directions, inspired by the alignmentby-agreement idea from Liang et al. (2006).",1 Introduction,[0],[0]
"Specifically, we develop a dependency parser (BiAtt-DP) using a bi-directional attention model based on the memory network.",1 Introduction,[0],[0]
"Given that the golden alignment is observed for dependency parsing in the training stage, we further derive a simple and interpretable approximation for the agreement objective, which makes a natural connection between the latent and observed alignment cases.
",1 Introduction,[0],[0]
The proposed BiAtt-DP parses a sentence in a linear order via sequentially querying the memory component that stores continuous embeddings for all headwords.,1 Introduction,[0],[0]
"In other words, we consider all possible arcs during the parsing.",1 Introduction,[0],[0]
"This formulation is adopted by graph-based parsers such as the MSTParser (McDonald et al., 2005).",1 Introduction,[0],[0]
"The consideration
2204
of all possible arcs makes the proposed BiAtt-DP different from many recently developed neural dependency parsers (Chen and Manning, 2014; Weiss et al., 2015; Alberti et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015), which use a transitionbased algorithm by modeling the parsing procedure as a sequence of actions on buffers.",1 Introduction,[0],[0]
"Moreover, unlike most graph-based parsers which may suffer from high computational complexity when utilizing high-order parsing history (McDonald and Pereira, 2006), the proposed BiAtt-DP can implicitly inject such information into the model while keeping the computational complexity in the order of O(n2) for a sentence with n words.",1 Introduction,[0],[0]
"This is achieved by feeding the RNN in the query component with a soft headword embedding, which is computed as the probability-weighted sum of all headword embeddings in the memory component.
",1 Introduction,[0],[0]
"To the best of our knowledge, this is the first attempt to apply memory network models to graphbased dependency parsing.",1 Introduction,[0],[0]
"Moreover, it is the first extension of neural attention models from unidirection to multi-direction by enforcing agreement on alignments.",1 Introduction,[0],[0]
"Experiments on English, Chinese, and 12 languages from the CoNLL 2006 shared task show the BiAtt-DP can achieve competitive parsing accuracy with several state-of-the-art parsers.",1 Introduction,[0],[0]
"Furthermore, our model achieves the highest unlabeled attachment score (UAS) on Chinese, Czech, Dutch, German, Spanish and Turkish.",1 Introduction,[0],[0]
"The proposed parser first encodes each word in a sentence by continuous embeddings using a bidirectional RNN, and then performs two types of operations, i.e. 1) headword predictions based on bidirectional parsing history and 2) the relation prediction conditioned on the current modifier and its predicted headword both in the embedding space.",2 A MemNet-based Dependency Parser,[0],[0]
"In the following, we first present how the token embeddings are constructed.",2 A MemNet-based Dependency Parser,[0],[0]
"Then, the key components of the proposed parser, i.e. the memory component and the query component, are discussed in detail.",2 A MemNet-based Dependency Parser,[0],[0]
"Lastly, we describe the parsing algorithm using a bidirectional attention model with agreement.",2 A MemNet-based Dependency Parser,[0],[0]
"In the proposed BiAtt-DP, the memory and query components share the same token embeddings.",2.1 Token Embeddings,[0],[0]
"We use the notion of additive token embedding as in (Botha and Blunsom, 2014) to utilize the available information about the token, e.g., its word form, lemma, part-of-speech (POS) tag, and morphological features.",2.1 Token Embeddings,[0],[0]
"Specifically, the token embedding is computed as
Eformeformi + E poseposi + E lemmaelemmai + · · · ,
where ei’s are one-hot encoding vectors for the ith word, and E’s are parameters to be learned that store the continuous embeddings for corresponding feature.",2.1 Token Embeddings,[0],[0]
"Note those one-hot encoding vectors have different dimensions, depending on individual vocabulary sizes, and all E’s have the same first dimension but different second dimension.",2.1 Token Embeddings,[0],[0]
The additive token embeddings allow us to easily integrate a variety of information.,2.1 Token Embeddings,[0],[0]
"Moreover, we only need to make a single decision on the dimensionality of the token embedding, rather than a combination of decisions on word embeddings and POS tag embeddings as in concatenated token embeddings used by Chen and Manning (2014), Dyer et al. (2015) and Weiss et al. (2015).",2.1 Token Embeddings,[0],[0]
"It reduces the number of model parameters to be tuned, especially when lots of different features are used.",2.1 Token Embeddings,[0],[0]
"In our experiments, the word form and fine-grained POS tag are always used, whereas other features are used depending on their availability in the dataset.",2.1 Token Embeddings,[0],[0]
"All singleton words, lemmas, and POS tags are replaced by special tokens.
",2.1 Token Embeddings,[0],[0]
"The additive token embeddings are transformed into another space before they are used by the memory and query components, i.e.
xi = LReL",2.1 Token Embeddings,[0],[0]
"[ P ( Eformeformi + · · · )] ,
where P is the projection matrix and is shared by the memory and query components as well.",2.1 Token Embeddings,[0],[0]
"The activation function of this projection layer is the leaky rectified linear (LReL) function (Mass et al., 2013) with 0.1 as the slope of the negative part.",2.1 Token Embeddings,[0],[0]
"In the remaining part of the paper, we refer to xi ∈",2.1 Token Embeddings,[0],[0]
"Rp as the token embedding for word at position i. Note the subscript i is substituted by j and t for the memory and query components, respectively.",2.1 Token Embeddings,[0],[0]
"As shown in Figure 1, the proposed BiAtt-DP has three components, i.e. a memory component, a leftto-right query component, and a right-to-left query component.",2.2 Components,[0],[0]
"Given a sentence of length n, the parser first uses a bi-directional RNN to construct n + 1 headword embeddings, m0,m1, . . .",2.2 Components,[0],[0]
",mn ∈ Re, with m0 reserved for the ROOT symbol.",2.2 Components,[0],[0]
Each query component is an uni-directional attention model.,2.2 Components,[0],[0]
"In a query component, a sequence of n modifier embeddings q1, . . .",2.2 Components,[0],[0]
",qn ∈ Rd are constructed recursively by conditioning on all headword embeddings.",2.2 Components,[0],[0]
"To address the vanishing gradient issue in RNNs, we use the gated recurrent unit (GRU) proposed by Cho et al. (2014), where an update gate and a reset gate are employed to control the information flow.",2.2 Components,[0],[0]
"We replace the hyperbolic tangent function in GRU with the LReL function, which is faster to compute and achieves better parsing accuracy in our preliminary studies.",2.2 Components,[0],[0]
"In the following, we refer to headword and modifier embeddings as memory and query vectors, respectively.
",2.2 Components,[0],[0]
Memory Component: The proposed BiAtt-DP uses a bi-directional RNN to obtain the memory vectors.,2.2 Components,[0],[0]
"At time step j, the current hidden state vector hlj ∈ Re/2 (or hrj ∈ Re/2) is computed as a non-linear transformation based on the current input vector xj and the previous hidden state vector hlj−1 (or h r j+1), i.e. h l j = GRU(h l j−1,xj) (or hrj = GRU(h r j+1,xj)).",2.2 Components,[0],[0]
"Ideally, the recursive nature of the RNN allows it to capture all context information from one-side, and a bi-directional RNN can thus capture context information from both sides.",2.2 Components,[0],[0]
We concatenate the hidden layers of the left-to-right RNN and the right-to-left RNN for the word at position j as the memory vector mj =,2.2 Components,[0],[0]
[ hlj ;h r j ] .,2.2 Components,[0],[0]
"These memory vectors are expected to encode the words and their context information in the headword space.
",2.2 Components,[0],[0]
Query Component:,2.2 Components,[0],[0]
"For each query component, we use a single-directional RNN with GRU to obtain the query vectors qj’s, which are the hidden state vectors of the RNN.",2.2 Components,[0],[0]
"Each qt is used to query the memory component, returning association scores st,j’s between the word at position t and the head-
word at position j for j ∈ {0, · · · , n}, i.e.
st,j = v",2.2 Components,[0],[0]
"Tφ (Cmj + Dqt) , (1)
",2.2 Components,[0],[0]
"where φ(·) is the element-wise hyperbolic tangent function, and C ∈ Rh×e, D ∈ Rh×d and v ∈",2.2 Components,[0],[0]
Rh are model parameters.,2.2 Components,[0],[0]
"Then, we can obtain probabilities (aka attention weights), at,0, · · · , at,n, over all headwords in the sentence by normalizing st,j’s, using a softmax function
at = softmax(st).",2.2 Components,[0],[0]
"(2)
The soft headword embedding is then defined as m̃t = ∑n j=1 at,jmj .",2.2 Components,[0],[0]
"At each time step t, the
RNN takes the soft headword embedding m̃lt−1 or m̃rt+1 as the input, in addition to the token embedding xt.",2.2 Components,[0],[0]
"Formally, for the forward case, the qt can be computed as qt = GRU (qt−1, [m̃t;xt]).",2.2 Components,[0],[0]
"Although the RNN is able to capture long-span context information to some extent, the local context may very easily dominate the hidden state.",2.2 Components,[0],[0]
"Therefore, this additional soft headword embedding allows the model to access long-span context information in a different channel.",2.2 Components,[0],[0]
"On the other hand, by recursively feeding both the query vector and the soft headword embedding into the RNN, the model implicitly captures high-order parsing history information, which can potentially improve the parsing accuracy (Yamada and Matsumoto, 2003; McDonald and Pereira, 2006).",2.2 Components,[0],[0]
"However, for a graph-based dependency parser, utilizing parsing history features is computationally expensive.",2.2 Components,[0],[0]
"For example, an k-th order MSTParser (McDonald and Pereira, 2006) has O(nk+1) complexity for a sentence of n words.",2.2 Components,[0],[0]
"In contrast, the BiAtt-DP implicitly captures high-order parsing history while keeping the complexity in the order of O(n2), i.e. for each direction.",2.2 Components,[0],[0]
"we compute n(n+1) pair-wise probabilities at,j for t = 1, · · · , n and j = 0, · · · , n.
In this paper, we choose to use soft headword embeddings rather than making hard decisions on headwords.",2.2 Components,[0],[0]
"In the latter case, beam search may potentially improve the parsing accuracy at the cost of higher computational complexity, i.e. O(Bn2) with a beam width of B. When using soft headword embeddings, there is no need to perform beam search.",2.2 Components,[0],[0]
"Moreover, it is straightforward to incorporate parsing history from both directions by using two query components at the cost of O(2n2), which cannot be easily achieved when using beam search.",2.2 Components,[0],[0]
The parsing decision can be made directly based on attention weights from the two query components or further rescored by the maximum spanning tree (MST) search algorithm.,2.2 Components,[0],[0]
"For the bi-directional attention model, the underlying probability distributions alt and a r t may not agree with each other.",2.3 Parsing by Attention with Agreement,[0],[0]
"In order to encourage the agreement, we use the mathematically convenient metric, i.e. the squared Hellinger distance H2 ( alt||art ) , for quantifying the distance between these two distri-
butions.",2.3 Parsing by Attention with Agreement,[0],[0]
"For dependency parsing, when the golden alignment is known during training, we can derive an upper bound on the latent agreement objective as
H2(alt,a r t ) ≤ 2 √ D(gt||alt)",2.3 Parsing by Attention with Agreement,[0],[0]
"+D(gt||art ),
where D(·||·) is the KL-divergence.",2.3 Parsing by Attention with Agreement,[0],[0]
"The complete derivation is provided in the Appendix A. During optimization, we can safely drop the constant scaler and the square root operation in the upper bound, leading to the following loss function
D(gt||alt)",2.3 Parsing by Attention with Agreement,[0],[0]
"+D(gt||art ) = 2D(gt||alt art ), (3)
where indicates element-wise multiplication.",2.3 Parsing by Attention with Agreement,[0],[0]
"The resulting loss function is equivalent to the crossentropy loss, which is widely adopted for training neural networks.
",2.3 Parsing by Attention with Agreement,[0],[0]
"As we can see, the loss function (3) tries to minimize the distance between the golden alignment and the intersection of the two directional attention alignments at every time step.",2.3 Parsing by Attention with Agreement,[0],[0]
"Therefore, during inference, the headword prediction for the word at time step t can be obtained as
argmax j
log alt,j + log a r t,j ,
seeking for agreement between both query components.",2.3 Parsing by Attention with Agreement,[0],[0]
"This parsing procedure is also similar to the exhaustive left-to-right modifier-first search algorithm described in (Covington, 2001), but it is enhanced by an additional right-to-left search with the agreement enforcement.",2.3 Parsing by Attention with Agreement,[0],[0]
"Alternatively, we can treat (log alt,j + log a r t,j) as a score of the corresponding arc and then search for the MST to form a dependency parse tree, as proposed in (McDonald et al., 2005).",2.3 Parsing by Attention with Agreement,[0],[0]
"The MST search is achieved via the ChuLiu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967), which can be implemented in O(n2) for dense graphs according to Tarjan (1977).",2.3 Parsing by Attention with Agreement,[0],[0]
"In practice, the MST search slows down the parsing speed by 6–10%.",2.3 Parsing by Attention with Agreement,[0],[0]
"However, it forces the parser to produce a valid tree, and we observe a slight improvement on parsing accuracy in most cases.
",2.3 Parsing by Attention with Agreement,[0],[0]
"After obtaining each modifier and its soft header embeddings, we use a single-layer perceptron to predict the head-modifier relation, i.e.
yt = softmax ( U [ m̃lt; m̃ r t ] +",2.3 Parsing by Attention with Agreement,[0],[0]
"W [ qlt; q r t ]) , (4)
",2.3 Parsing by Attention with Agreement,[0],[0]
"where yt,1, · · · , yt,m are the probabilities of m possible relations, and U ∈ Rm×2e and W ∈ Rm×2d are model parameters.",2.3 Parsing by Attention with Agreement,[0],[0]
"For the t-th word (modifier) wt in a sentence of length n, let H lt and H r t denote random variables representing the predicted headword from forward (left-to-right) and backward (right-to-left) parsing directions, respectively.",3 Model Learning,[0],[0]
Also let Rt denote the random variable representing the dependency relation for wt.,3 Model Learning,[0],[0]
"The joint probability of headword and relation predictions can be written as
P (R1:n, H l 1:n, H r 1:n|w1:n)
= n∏
t=1
P (Rt|w1:n)P (H lt |w1:n)P (Hrt |w1:n)
= n∏
t=1
ylt,Rt · alt,Hlt · a r t,Hrt
(5)
where at each time step we assume head-modifier relations and headwords from both directions are independent with each other when conditioned on the global knowledge of the whole sentence.",3 Model Learning,[0],[0]
"Note that the long-span context and high-order parsing history information are injected when we model P (H lt |w1:n), P (Hrt |w1:n) and P (Rt|w1:n), as discussed in Section 2.2.
",3 Model Learning,[0],[0]
"As discussed in Section 2.3, the model can be trained by encouraging attention agreement between two query components.",3 Model Learning,[0],[0]
"From (5), we observe that it is equivalent to maximizing the log-likelihood of the golden dependency tree (or minimizing the crossentropy) for each training sentence, i.e.
n∑
t=1
( log yt,relationt + log a l t,headt + log a r t,headt ) ,
where at,j and yt,r are defined in (2) and (4), respectively, and relationt and headt are golden relation and headword labels, respectively.",3 Model Learning,[0],[0]
"The gradients are computed via the back-propagation algorithm (Rumelhart et al., 1986).",3 Model Learning,[0],[0]
"Errors of yt come from the arc labels, whereas there are two source of errors for at, one from the headword labels and the other back-propagated from errors of yt.",3 Model Learning,[0],[0]
"We use stochastic gradient descent with the Adam algorithm proposed in (Kingma and Ba, 2015).",3 Model Learning,[0],[0]
"The
learning rate is halved at each iteration once the loglikelihood of the dev set decreases.",3 Model Learning,[0],[0]
The whole training procedure terminates when the log-likelihood decreases for the second time.,3 Model Learning,[0],[0]
"All learning parameters except bias terms are initialized randomly according to the Gaussian distribution N (0, 10−2).",3 Model Learning,[0],[0]
"In our experiments, we tune the initial learning rate with a step size of 0.0002, and choose the best one based on the log-likelihood on the dev set at the first epoch.",3 Model Learning,[0],[0]
"Empirically, the selected initial learning rates fall in the range of [0.0004, 0.0010] for hidden layer size [128, 320], and tend to be larger when using a smaller hidden layer size, i.e. [0.0016, 0.0034] for hidden layer size around 80.",3 Model Learning,[0],[0]
The training data are randomly shuffled at every epoch.,3 Model Learning,[0],[0]
"In this section, we present the parsing accuracy of the proposed BiAtt-DP on 14 languages.",4 Experiments,[0],[0]
"We report both UAS and labeled attachment score (LAS), obtained by the CoNLL-X eval.pl script2 which ignores punctuation symbols.",4 Experiments,[0],[0]
"The headword predictions are made through the MST search, which slightly improves both UAS and LAS (less than 0.3% absolutely).",4 Experiments,[0],[0]
"Overall, the proposed BiAtt-DP achieves competitive parsing accuracy on all languages as state-of-the-art parsers, and obtains better UAS in 6 languages.",4 Experiments,[0],[0]
We also show the impact of using POS tags and pre-trained word embeddings.,4 Experiments,[0],[0]
"Moreover, different variants of the full model are compared in this section.",4 Experiments,[0],[0]
"We work on the English Treebank-3 (PTB) dataset (Marcus et al., 1999), the Chinese Treebank-5.1 (CTB) dataset (Palmer et al., 2005), and 12 other languages from the CoNLL 2006 shared task (Buchholz and Marsi, 2006).",4.1 Data,[0],[0]
"For PTB and CTB datasets, we use exactly the same setup as in (Chen and Manning, 2014; Dyer et al., 2015).",4.1 Data,[0],[0]
"Specifically, we convert the English and Chinese data using the Stanford parser v3.3.0 (de Marneffe et al., 2006) and the Penn2Malt tool (Zhang and Clark, 2008), respectively.
",4.1 Data,[0],[0]
"For English, POS tags are obtained using the Stanford POS tagger v3.3.0 (Toutanova et al., 2003),
2http://ilk.uvt.nl/conll/software.html
whereas for Chinese, we use gold segmentation and POS tags.",4.1 Data,[0],[0]
"When constructing the token embeddings for English and Chinese, both the word form and the POS tag are used.",4.1 Data,[0],[0]
"We also initialize Eform by pretrained word embeddings3.
",4.1 Data,[0],[0]
"For the 12 other languages, we randomly hold out 5% of the training data as the dev set.",4.1 Data,[0],[0]
"In addition to the word form and find-grained POS tags, we use extra features such as lemmas, coarse-grained POS tags, and morphemes when they are available in the dataset.",4.1 Data,[0],[0]
No pre-trained word embeddings are used for these 12 languages.,4.1 Data,[0],[0]
The hidden layer size is kept the same across all RNNs in the proposed BiAtt-DP.,4.2 Model Configurations,[0],[0]
We also require the dimension of the token embeddings to be the same as the hidden layer size.,4.2 Model Configurations,[0],[0]
"Note that we concatenate the hidden layers of two RNNs for constructing mj , and thus we have e = 2d.",4.2 Model Configurations,[0],[0]
"The weight matrices C and D respectively project vectors mj and qt to the same dimension h, which is equivalent to d. For English and Chinese, since the dimension of pretrained word embeddings are 300, we use 300 × h as the dimension of embedding parameters E’s.",4.2 Model Configurations,[0],[0]
"For the 12 other languages, we use square matrices for the embedding parameters E’s.",4.2 Model Configurations,[0],[0]
"For all languages, We tune the hidden layer size and choose one according to UAS on the dev set.",4.2 Model Configurations,[0],[0]
"The selected hidden layer sizes for these languages are: 368 (English), 114 (Chinese), 128 (Arabic), 160 (Bulgarian), 224 (Czech), 176 (Danish), 220 (Dutch), 200 (German), 128 (Japanese), 168 (Portuguese), 128 (Slovene), 144 (Spanish), 176 (Swedish), and 128 (Turkish).",4.2 Model Configurations,[0],[0]
We first compare our parser with state-of-the-art neural transition-based dependency parsers on PTB and CTB.,4.3 Results,[0],[0]
"For English, we also compare with stateof-the-art graph-based dependency parsers.",4.3 Results,[0],[0]
"The results are shown in Table 1 and Table 2, respectively.",4.3 Results,[0],[0]
It can be seen that the BiAtt-DP outperforms all other graph-based parsers on PTB.,4.3 Results,[0],[0]
"Compared with
3For English, we use the dependency-based word embeddings at https://goo.gl/tWke3I (Levy and Goldberg, 2014).",4.3 Results,[0],[0]
"For Chinese, we pre-train 192-dimension skip-gram embeddings (Mikolov et al., 2013) on Chinese Gigawords (Graff et al., 2005).
",4.3 Results,[0],[0]
"the transition-based parsers, it achieves better accuracy than Chen and Manning (2014), which uses a feed-forward neural network, and Dyer et al. (2015), which uses three stack LSTM networks.",4.3 Results,[0],[0]
"Compared with the integrated parsing and tagging models, the BiAtt-DP outperforms Bohnet and Nivre (2012) but has a small gap to Alberti et al. (2015).",4.3 Results,[0],[0]
"On CTB, it achieves best UAS and similar LAS.",4.3 Results,[0],[0]
"This may be caused by that the relation vocabulary size is relatively smaller than the average sentence length, which biases the joint objective to be more sensitive to UAS.",4.3 Results,[0],[0]
"The parsing speed is around 50–60 sents/sec measured on a desktop with Intel Core i7 CPU @ 3.33GHz using single thread.
",4.3 Results,[0],[0]
"Next, in Table 3 we show the parsing accuracy of the proposed BiAtt-DP on 12 languages in the CoNLL 2006 shared task, including comparison with state-of-the-art parsers.",4.3 Results,[0],[0]
"Specifically, we show UAS of the 3rd-order RBGParser as reported in (Lei et al., 2014) since it also uses low-dimensional continuous embeddings.",4.3 Results,[0],[0]
"However, there are several major differences between the RBGParser and the BiAtt-DP.",4.3 Results,[0],[0]
"First, in (Lei et al., 2014), the lowdimensional continuous embeddings are derived
from low-rank tensors.",4.3 Results,[0],[0]
"Second, the RBGParser uses combined scoring of arcs by including traditional features from the MSTParser (McDonald and Pereira, 2006) /",4.3 Results,[0],[0]
"TurboParser (Martins et al., 2013).",4.3 Results,[0],[0]
"Third, the RBGParser employs a third-order parsing algorithm based on (Zhang et al., 2014), although it also implements a first-order parsing algorithm, which achieves lower UAS in general.",4.3 Results,[0],[0]
"In Table 3, we show that the proposed BiAtt-DP outperforms the RBGParser in most languages except Japanese, Slovene, and Swedish.
",4.3 Results,[0],[0]
It can be observed from Table 3 that the BiAttDP has highly competitive parsing accuracy as stateof-the-art parsers.,4.3 Results,[0],[0]
"Moreover, it achieves best UAS for 5 out of 12 languages.",4.3 Results,[0],[0]
"For the remaining seven languages, the UAS gaps between the BiAtt-DP and state-of-the-art parsers are within 1.0%, except Swedish.",4.3 Results,[0],[0]
"An arguably fair comparison for the BiAttDP is the MSTParser (McDonald and Pereira, 2006), since the BiAtt-DP replaces the scoring function for arcs but uses exactly the same search algorithm.",4.3 Results,[0],[0]
"Due to the space limit, we refer readers to (Lei et al., 2014) for results of the MSTParsers (also shown in Appendix B).",4.3 Results,[0],[0]
"The BiAtt-DP consistently outperforms both parser by up to 5% absolute UAS score.
",4.3 Results,[0],[0]
"Finally, following (Pitler and McDonald, 2015), we also analyze the performance of the BiAtt-DP on both crossed and uncrossed arcs.",4.3 Results,[0],[0]
"Since the BiAtt-
DP uses a graph-based non-projective parsing algorithm, it is interesting to evaluate the performance on crossed arcs, which result in the non-projectivity of the dependency tree.",4.3 Results,[0],[0]
"The last three columns of Table 3 show the recall of crossed arcs, that of uncrossed arcs, and the percentage of crossed arcs in the test set.",4.3 Results,[0],[0]
"Pitler and McDonald (2015) reported numbers on the same data for Dutch, German, Portuguese, and Slovene as in this paper.",4.3 Results,[0],[0]
"For these four languages, the BiAtt-DP achieves better UAS than that reported in (Pitler and McDonald, 2015).",4.3 Results,[0],[0]
"More importantly, we observe that the improvement on recall of crossed arcs (around 10–18% absolutely) is much more significant than that of uncrossed arcs (around 1–3% absolutely), which indicates the effectiveness of the BiAtt-DP in parsing languages with non-projective trees.",4.3 Results,[0],[0]
"Here we try to study the impact of using pre-trained word embeddings, POS tags, as well as the bidirectional query components on our model.",4.4 Ablative Study,[0],[0]
"First of all, we start from our best model (Model 1 in Table 4) on English, which uses 300 as the token embedding dimension and 368 as the hidden layer size.",4.4 Ablative Study,[0],[0]
"We keep those model parameter dimensions unchanged and analyze different factors by comparing the parsing accuracy on PTB dev set.
",4.4 Ablative Study,[0],[0]
The results are summarized in Table 4.,4.4 Ablative Study,[0],[0]
"Comparing Models 1–3, it can be observed that without using pre-trained word embeddings, both UAS and LAS drop by 0.6%, and without using POS tags in token embeddings, the numbers further drop by 1.6% in UAS and around 2.6% in LAS.",4.4 Ablative Study,[0],[0]
"In terms of query components, using single query component (Models 4–5) degrades UAS by 0.7–0.9% and LAS by around 1.0%, compared with Model 2.",4.4 Ablative Study,[0],[0]
"For Model 6, the soft headword embedding is only used for arc label predictions but not fed into the next hidden state, which is around 0.3% worse than Model 2.",4.4 Ablative Study,[0],[0]
This supports the hypothesis about the usefulness of the parsing history information.,4.4 Ablative Study,[0],[0]
We also implement a variant of Model 6 which produces one at instead two by using both qlt and q r t in (1).,4.4 Ablative Study,[0],[0]
"It gets 92.44% UAS and 89.26% LAS, indicating that naively applying a bi-directional RNN may not be enough.",4.4 Ablative Study,[0],[0]
"Neural Dependency Parsing: Recently developed neural dependency parsers are mostly transition-based models, which read words sequentially from a buffer into a stack and incrementally build a parse tree by predicting a sequence of transitions (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004).",5 Related Work,[0],[0]
"A feed-forward neural network is used in (Chen and Manning, 2014), where they represent the current state with 18 selected elements such as the top words on the stack and buffer.",5 Related Work,[0],[0]
"Each element is encoded by concatenated embeddings of words, POS tags, and arc labels.",5 Related Work,[0],[0]
"Their dependency parser achieves improvement
on both accuracy and parsing speed.",5 Related Work,[0],[0]
Weiss et al. (2015) improve the parser using semi-supervised structured learning and unlabeled data.,5 Related Work,[0],[0]
"The model is extended to integrate parsing and tagging in (Alberti et al., 2015).",5 Related Work,[0],[0]
"On the other hand, Dyer et al. (2015) develop the stack LSTM architecture, which uses three LSTMs to respectively model the sequences of buffer states, stack states, and actions.",5 Related Work,[0],[0]
"Unlike the transition-based formulation, the proposed BiAtt-DP directly predicts the headword and the dependency relation at each time step.",5 Related Work,[0],[0]
"Specifically, there is no explicit representation of actions or headwords in our model.",5 Related Work,[0],[0]
"The model learns to retrieve the most relevant information from the input memory to make decisions on headwords and head-modifier relations.
",5 Related Work,[0],[0]
Graph-based Dependency Parsing:,5 Related Work,[0],[0]
"In addition to the transition-based parsers, another line of research in dependency parsing uses graph-based models.",5 Related Work,[0],[0]
Graph-based parser usually build a dependency tree from a directed graph and learns to scoring the possible arcs.,5 Related Work,[0],[0]
"Due to this nature, nonprojective parsing can be done straightforwardly by most graph-based dependency parsers.",5 Related Work,[0],[0]
"The MSTParser (McDonald et al., 2005) and the TurboParser (Martins et al., 2010) are two examples of graphbased parsers.",5 Related Work,[0],[0]
"The MSTParser formulates the parsing as searching for the MST, whereas the TurboParser performs approximate variational inference over a factor graph.",5 Related Work,[0],[0]
"The RBGParser proposed in (Lei et al., 2014) can also be viewed as a graph-based parser, which scores arcs using low-dimensional continuous features derived from low-rank tensors as well as features used by MSTParser/TurboParser.",5 Related Work,[0],[0]
"It also employs a sampler-based algorithm for parsing (Zhang et al., 2014).
",5 Related Work,[0],[0]
"Neural Attention Model: The proposed BiAttDP is closely related to the memory network (Sukhbaatar et al., 2015) for question answering, as well as the neural attention models for machine translation (Bahdanau et al., 2015) and constituency parsing (Vinyals et al., 2015b).",5 Related Work,[0],[0]
The way we query the memory component and obtain the soft headword embeddings is essentially the attention mechanism.,5 Related Work,[0],[0]
"However, different from the above studies where the alignment information is latent, in dependency parsing, the arc between the modifier and
headword is known during training.",5 Related Work,[0],[0]
"Thus, we can utilize these labels for attention weights.",5 Related Work,[0],[0]
"The similar idea is employed by the pointer network in (Vinyals et al., 2015a), which is used to solve three different combinatorial optimization problems.",5 Related Work,[0],[0]
"In this paper, we develop a bi-directional attention model by encouraging agreement between the latent attention alignments.",6 Conclusion,[0],[0]
"Through a simple and interpretable approximation, we make the connection between latent and observed alignments for training the model.",6 Conclusion,[0],[0]
We apply the bi-directional attention model incorporating the agreement objective during training to the proposed memory-network-based dependency parser.,6 Conclusion,[0],[0]
"The resulting parser is able to implicitly capture the high-order parsing history without suffering from issue of high computational complexity for graph-based dependency parsing.
",6 Conclusion,[0],[0]
We have carried out empirical studies over 14 languages.,6 Conclusion,[0],[0]
The parsing accuracy of the proposed model is highly competitive with state-of-the-art dependency parsers.,6 Conclusion,[0],[0]
"For English, the proposed BiAttDP outperforms all graph-based parsers.",6 Conclusion,[0],[0]
"It also achieves state-of-the-art performance in 6 languages in terms of UAS, demonstrating the effectiveness of the proposed mechanism of bi-directional attention with agreement and its use in dependency parsing.
",6 Conclusion,[0],[0]
"A Upper Bound on H2(p,q)
",6 Conclusion,[0],[0]
"Here, we use the following definition of squared Hellinger distance for countable space
H2(p,q) = 1
2
∑
i
( √ pi − √ qi) 2
where p,q ∈ ∆k are two k-simplexes.",6 Conclusion,[0],[0]
"Introducing g ∈ ∆k, the squared Hellinger distance can be upper bounded as
H2(p,q) ≤ √ 2H(p,q) (6)
≤",6 Conclusion,[0],[0]
√ 2,6 Conclusion,[0],[0]
"[H(p,g)",6 Conclusion,[0],[0]
"+H(q,g)]",6 Conclusion,[0],[0]
"(7) ≤ 2 √ H2(p,g) +H2(q,g) (8)
where (6), (7) and (8) follow the inequalities between the `1-norm and the `2-norm, the triangle
inequality defined for a metric, and the CauchySchwarz’s inequality, respectively.",6 Conclusion,[0],[0]
"Using the relationship between the KL-divergence and the squared Hellinger distance, (8) can be further bounded by
2 √ D(g||p)",6 Conclusion,[0],[0]
+D(g||q).,6 Conclusion,[0],[0]
"We develop a novel bi-directional attention model for dependency parsing, which learns to agree on headword predictions from the forward and backward parsing directions.",abstractText,[0],[0]
The parsing procedure for each direction is formulated as sequentially querying the memory component that stores continuous headword embeddings.,abstractText,[0],[0]
"The proposed parser makes use of soft headword embeddings, allowing the model to implicitly capture high-order parsing history without dramatically increasing the computational complexity.",abstractText,[0],[0]
"We conduct experiments on English, Chinese, and 12 other languages from the CoNLL 2006 shared task, showing that the proposed model achieves state-of-the-art unlabeled attachment scores on 6 languages.1",abstractText,[0],[0]
Bi-directional Attention with Agreement for Dependency Parsing,title,[0],[0]
"Learning from time-series data is of paramount importance for prediction, anomaly detection, classification, and other critical tasks that appear in business and society.",1. Introduction,[0],[0]
Various models of time-series have been studied in the literature to better learn from time-series data.,1. Introduction,[0],[0]
"These include vector autoregressive (VAR) models (Lütkepohl, 2005), hidden Markov models (HMM) (Baum & Petrie, 1966), and recurrent neural networks (RNN) (Rumelhart et al., 1986), including long short term memory (LSTM) (Hochreiter & Schmidhuber, 1997) and echo state networks (ESN) (Jaeger & Haas, 2004).",1. Introduction,[0],[0]
"With these models of time-series, one seeks to learn the relation between past values and future values.
",1. Introduction,[0],[0]
"In some of these models of time-series, hidden units (or latent variables) play essential roles in taking into account long term dependency or non-linearity in time-series.",1. Introduction,[0],[0]
"Hid-
1IBM Research - Tokyo, Tokyo, Japan.",1. Introduction,[0],[0]
"Correspondence to: Takayuki Osogami <osogami@jp.ibm.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"den units, however, make it difficult to learn the parameters of those models.",1. Introduction,[0],[0]
"For example, the Baum-Welch algorithm (Baum & Petrie, 1966) learns the parameters of an HMM by iteration of expectation and maximization (i.e., an EM algorithm).",1. Introduction,[0],[0]
"An RNN, including LSTM, is trained via back propagation through time (Rumelhart et al., 1986).",1. Introduction,[0],[0]
"These algorithms are time-consuming and do not necessarily find optimal values of the parameters.
",1. Introduction,[0],[0]
This difficulty in learning a model with hidden units partly stems from the fact that the values of hidden units can only be reliably estimated after observing future values of target time-series.,1. Introduction,[0],[0]
It then requires iteration or back propagation to learn the relation between the hidden values and preceding values.,1. Introduction,[0],[0]
"An ESN, on the other hand, gives up learning the hard-to-learn parameters between hidden values and preceding (visible or hidden) values and set those parameters randomly.",1. Introduction,[0],[0]
"The ESN only learns the relation between visible values and preceding (hidden) values (Jaeger & Haas, 2004).
",1. Introduction,[0],[0]
We study a model of time-series whose parameters can be represented as a matrix M or a set of such matrices.,1. Introduction,[0],[0]
"An element Mi,j of the matrix may, for example, represent the weight between a past value of a unit i and a future value of a unit j. In a VAR model, each matrix corresponds to the coefficients for a particular lag.",1. Introduction,[0],[0]
"In an RNN, a matrix corresponds to the weight between hidden units.",1. Introduction,[0],[0]
"We consider a situation where it is hard to estimate an appropriate value of Mi,j for some of the units j ∈ H (in particular, H may denote the set of hidden units).
",1. Introduction,[0],[0]
"We propose a method of training a time-series model with hidden units in a bidirectional manner, one from the past and the other from the future.",1. Introduction,[0],[0]
"From a time-series model with parameter M, we construct a backward model, whose parameters are represented by the transposed matrix M> or a set of such transposed matrices.",1. Introduction,[0],[0]
"The (i, j)-th element of M> is Mj,i, which represents the weight between a preceding value of a unit j and a succeeding value of a unit i.",1. Introduction,[0],[0]
Our key idea is to let the preceding value represent a future value and the succeeding value represent a past value in the backward model.,1. Introduction,[0],[0]
"Then an intuitive meaning of Mj,i in the backward model matches that in the original (forward) model.",1. Introduction,[0],[0]
We use a common matrix M both in the forward model and in the backward model.,1. Introduction,[0],[0]
"Namely, the parameters are shared between the two models.
",1. Introduction,[0],[0]
"The two models have an identical structure and trained in an identical manner with a stochastic gradient method (Bottou, 2009) except that we train the forward model using timeseries in a standard (forward) manner and the backward model using the time-series from the future to the past.",1. Introduction,[0],[0]
We alternately train the forward model to learn M and the backward model to learn M>.,1. Introduction,[0],[0]
An advantage of our bidirectional training is that the elements of M that are hard to estimate differ between when we train the forward model and when we train the backward model.,1. Introduction,[0],[0]
"For the forward model, it is hard to estimate Mi,j for j ∈ H (i.e., M:,H).",1. Introduction,[0],[0]
"For the backward model, it is hard to estimate Mj,i for j ∈ H (i.e., MH,: = (M
>):,H).",1. Introduction,[0],[0]
"Although MH,H is hard to estimate for both models, the other elements of M can be reliably estimated in either of the two models.",1. Introduction,[0],[0]
"This idea of bidirectional training for learning time-series models with hidden units constitutes our first contribution.
",1. Introduction,[0],[0]
"Here, we extend a Dynamic Boltzmann Machine (DyBM) to incorporate hidden units and apply bidirectional training to learn its parameters.",1. Introduction,[0],[0]
The DyBM has been proposed by Osogami & Otsuka (2015a;b) and subsequently studied by Dasgupta et al. (2016); Dasgupta & Osogami (2017); Kajino (2017).,1. Introduction,[0],[0]
The prior work on the DyBM does not consider hidden units but instead uses various features of past values to capture the long term dependency in time-series.,1. Introduction,[0],[0]
"For example, Dasgupta & Osogami (2017) use an ESN to create nonlinear features of past values.",1. Introduction,[0],[0]
"In our context, these features are fed into visible units of a DyBM.",1. Introduction,[0],[0]
"In particular, what features of past values are used is determined randomly without learning.",1. Introduction,[0],[0]
Our analysis of the DyBM with hidden units illuminates the difficulty of learning some of its parameters.,1. Introduction,[0],[0]
"With bidirectional learning, we seek to learn the weight from the past visible values to the future hidden values, which corresponds to learning what features of past values are effective for prediction.",1. Introduction,[0],[0]
"The DyBM with hidden units and its analysis constitute our second contribution.
",1. Introduction,[0],[0]
We validate the effectiveness of bidirectional training and the hidden units in DyBMs through numerical experiments using synthetic and real time-series.,1. Introduction,[0],[0]
We will show that the DyBM with hidden units can be trained more effectively with bidirectional training and reduces the predictive error by up to 90 %.,1. Introduction,[0],[0]
"Our bidirectional training is related to but different from bidirectional recurrent neural networks (BRNN) (Schuster & Paliwal, 1997; Baldi et al., 1999), including bidirectional LSTM (Graves & Schmidhuber, 2005; Chen & Chaudhari, 2005).",1.1. Related Work,[0],[0]
"Similar to our bidirectional training, a BRNN trains both a forward model and a backward model.",1.1. Related Work,[0],[0]
"These two models, however, do not share parameters, contrary to our bidirectional training.",1.1. Related Work,[0],[0]
"In fact, motivation and purpose of the
BRNN are quite different from ours.",1.1. Related Work,[0],[0]
The BRNN uses both the sequence from the past and the sequence from the future to better estimate missing values.,1.1. Related Work,[0],[0]
The BRNN thus needs both of the two sequences for learning and for prediction.,1.1. Related Work,[0],[0]
"On the other hand, our bidirectional training uses both a forward sequence and a backward sequence at the time of learning, but the trained model uses only the sequence from the past to predict future unseen values.",1.1. Related Work,[0],[0]
"Namely, our bidirectional training is used to learn a model for predicting future values from past values, while the BRNN is used for estimating missing values from past and future values.
",1.1. Related Work,[0],[0]
Our bidirectional training is also related to but different from Forward Backward Lasso Granger (FBLG) by Cheng et al. (2014).,1.1. Related Work,[0],[0]
"In FBLG, forward and backward VAR models are estimated with Lasso, and an averaged model is used to infer the Granger causality.",1.1. Related Work,[0],[0]
"The VAR models in FBLG do not have hidden units, while our bidirectional training is motivated by the need for training time-series models with hidden units.",1.1. Related Work,[0],[0]
"Winkler et al. (2016) also study a backward VAR in the context of the Granger causality but do not consider hidden units.
",1.1. Related Work,[0],[0]
"Another related work is structure learning of a VAR model (Bahadori et al., 2013) or a simpler linear dynamical system (Jalali & Sanghavi, 2012) that takes into account the existence of unobserved (or latent) variables.",1.1. Related Work,[0],[0]
"However, their goal is to reliably estimate the structure of the relation between observable variables by taking into account the unobserved variables.",1.1. Related Work,[0],[0]
"This is in contrast to the purpose of our bidirectional training, which aims at learning the relation between visible units and hidden units.",1.1. Related Work,[0],[0]
We study a particularly structured Boltzmann machine for time-series (see Figure 1).,2. DyBM with Hidden Units,[0],[0]
"Corresponding to a segment of a time-series of length T + 1, the Boltzmann machine in Figure 1 has T + 1 layers in the horizontal (temporal) direction.",2. DyBM with Hidden Units,[0],[0]
Each layer corresponds to a time t−δ for 0 ≤ δ ≤ T .,2. DyBM with Hidden Units,[0],[0]
"Each layer has two parts, hidden and visible.",2. DyBM with Hidden Units,[0],[0]
The visible part x[t−δ] at the δ-th layer represents the values of the timeseries at time t − δ.,2. DyBM with Hidden Units,[0],[0]
The hidden part h[t−δ] represents the values of hidden units at time t,2. DyBM with Hidden Units,[0],[0]
− δ.,2. DyBM with Hidden Units,[0],[0]
"Here, units within each layer do not have connections to each other.",2. DyBM with Hidden Units,[0],[0]
We let x[<t] ≡,2. DyBM with Hidden Units,[0],[0]
"(x[s])t−T≤s<t and define h[<t] analogously.
",2. DyBM with Hidden Units,[0],[0]
"The Boltzmann machine in Figure 1 has bias parameter b and weight parameter (U, V, W, Z).",2. DyBM with Hidden Units,[0],[0]
"Let θ ≡ (V,W,b) be the parameters connected to visible units x[t] (from the units in the past, x[s] or h[s] for s < t) and φ ≡ (U,Z).",2. DyBM with Hidden Units,[0],[0]
"The energy of this Boltzmann machine is given as follows:
Eθ,φ(x",2. DyBM with Hidden Units,[0],[0]
"[t],h[t]|x[<t],h[<t])
= Eθ(x",2. DyBM with Hidden Units,[0],[0]
"[t]|x[<t],h[<t]) +",2. DyBM with Hidden Units,[0],[0]
"Eφ(h[t]|x[<t],h[<t]), (1)
where we define
Eθ(x",2. DyBM with Hidden Units,[0],[0]
"[t]|x[<t],h[<t]) (2)
= −b>x[t] − T∑ δ=1 (x[t−δ])>W[δ] x[t] − T∑ δ=1 (h[t−δ])>V[δ] x[t]
and define Eφ(h[t]|x[<t],h[<t]) from (2) by letting W ← U, V← Z, b← 0, and x[t] ← h[h].
Similar to the DyBM in Osogami & Otsuka (2015a), we study the case where W ≡ (W[δ])1≤δ≤T and other matrices have the following parametric forms for δ ≥ d:
W[δ] = λδ−dW[d], V[δ] = λδ−dV[d], (3)
Z[δ] = λδ−d Z[d], U[δ] = λδ−dU[d], (4)
where λ is a decay rate satisfying 0 ≤ λ < 1.",2. DyBM with Hidden Units,[0],[0]
"Then, in the limit of T → ∞, the energy in (2) can be represented as follows (and Eφ(h[t]|x[<t],h[<t]) has an analogous limit shown in (37) of the supplementary material):
",2. DyBM with Hidden Units,[0],[0]
Eθ(x,2. DyBM with Hidden Units,[0],[0]
"[t]|x[<t],h[<t])
= −b>x[t] − d−1∑ δ=1 (x[t−δ])>W[δ] x[t]",2. DyBM with Hidden Units,[0],[0]
"− d−1∑ δ=1 (h[t−δ])>V[δ] x[t]
− (α[t−1])>W[d] x[t] − (β[t−1])>V[d] x[t], (5)
where α[t−1] is referred to as an eligibility trace in Osogami & Otsuka (2015a) and defined as follows (here, we define an eligibility trace β[t−1] for the hidden part analogously): α[t−1] ≡ ∞∑ δ=d λδ−d x[t−δ], β[t−1] ≡ ∞∑ δ=d λδ−d h[t−δ].",2. DyBM with Hidden Units,[0],[0]
"(6)
The energy in (5) gives the conditional probability distribution over x[t] given x[<t] and h[<t].",2. DyBM with Hidden Units,[0],[0]
"For binary-valued time-series, we have
pθ(x",2. DyBM with Hidden Units,[0],[0]
"[t]|x[<t],h[<t]) = 1
Z exp(−Eθ(x[t]|x[<t],h[<t]))",2. DyBM with Hidden Units,[0],[0]
"(7)
for any binary vector x[t], where Z is the normalization factor for the probabilities to sum up to one.",2. DyBM with Hidden Units,[0],[0]
"Due to the structure in Figure 1, the values in x[t] = (x[t]i )i=1,2,... are conditionally independent of each other given x[<t] and h[<t], so that we can represent
pθ(x",2. DyBM with Hidden Units,[0],[0]
"[t]|x[<t],h[<t]) = ∏ i=1,2,... pθ,i(x [t] i |x",2. DyBM with Hidden Units,[0],[0]
"[<t],h[<t]), (8)
where the conditional probability pi(x",2. DyBM with Hidden Units,[0],[0]
"[t] i |x[<t],h[<t]) is defined with the energy associated with unit i (see Osogami & Otsuka (2015a)).",2. DyBM with Hidden Units,[0],[0]
"For real-valued time-series, one can define the conditional density pi(x [t] i |x[<t],h[<t]) with a Gaussian distribution whose mean is given from the energy associated with unit i (Dasgupta & Osogami, 2017; Osogami, 2016).",2. DyBM with Hidden Units,[0],[0]
Conditional distributions can be defined analogously for h[t] (see (40)–(42) and (51) in the supplementary material).,2. DyBM with Hidden Units,[0],[0]
"Here, we derive a learning rule for θ.",3. Training a DyBM with Hidden Units,[0],[0]
"We will also see that φ cannot be trained in an analogous manner.
",3. Training a DyBM with Hidden Units,[0],[0]
"Our DyBM with binary hidden units gives the probability of a time-series, x ≡ (x[t])t=`,...,u, by
pθ,φ(x) = ∑ h̃ pφ(h̃|x) u∏ t=` pθ(x",3. Training a DyBM with Hidden Units,[0],[0]
"[t]|x[<t], h̃[<t]) (9)
where ∑
h̃ denotes the summation over all of the possible values of hidden units from time t = ` to t = u, and
pφ(h̃|x) ≡",3. Training a DyBM with Hidden Units,[0],[0]
u∏ s=` pφ(h̃,3. Training a DyBM with Hidden Units,[0],[0]
"[s]|x[<s], h̃[<s]), (10)
",3. Training a DyBM with Hidden Units,[0],[0]
"where pφ(h̃[s]|x[<s], h̃[<s]) is defined analogously to (7)– (8) and provided in (36) of the supplementary material, and we arbitrarily define x[s] = 0 and h̃[s] = 0 for s < `.
We seek to maximize the log likelihood of a given x by maximizing a lower bound given by Jensen’s inequality:
log pθ,φ(x) = log (∑
h̃
pφ(h̃|x) u∏ t=` pθ(x [t]|x[<t], h̃[<t]) )",3. Training a DyBM with Hidden Units,[0],[0]
"(11)
≥ ∑ h̃ pφ(h̃|x) log ( u∏ t=` pθ(x [t]|x[<t], h̃[<t]) )",3. Training a DyBM with Hidden Units,[0],[0]
"(12)
= ∑ h̃ pφ(h̃|x) u∑ t=` log pθ(x",3. Training a DyBM with Hidden Units,[0],[0]
"[t]|x[<t], h̃[<t]) (13)
= u∑ t=` ∑ h̃[<t]",3. Training a DyBM with Hidden Units,[0],[0]
pφ(h̃,3. Training a DyBM with Hidden Units,[0],[0]
"[<t]|x[<t−1]) log pθ(x[t]|x[<t], h̃[<t])
≡",3. Training a DyBM with Hidden Units,[0],[0]
"Lθ,φ(x), (14)
where the summation with respect to h̃[<t] is over all of the possible values of h̃[s] for s ≤ t− 1, and
pφ(h̃",3. Training a DyBM with Hidden Units,[0],[0]
[<t]|x[<t−1]) ≡,3. Training a DyBM with Hidden Units,[0],[0]
t−1∏ s=` pφ(h̃,3. Training a DyBM with Hidden Units,[0],[0]
"[s]|x[<s], h̃[<s]).",3. Training a DyBM with Hidden Units,[0],[0]
"(15)
The gradient of the lower bound with respect to θ is:
∇θLθ,φ(x) (16)
= u∑ t=` ∑ h̃[<t] pφ(h̃",3. Training a DyBM with Hidden Units,[0],[0]
"[<t]|x[<t−1])∇θ log pθ(x[t]|x[<t], h̃[<t]).
",3. Training a DyBM with Hidden Units,[0],[0]
"The right-hand side of (16) is a summation of expected gradients, which suggests a method of stochastic gradient.",3. Training a DyBM with Hidden Units,[0],[0]
"Namely, at each step t, we sample h[t−1] according to pφ(h",3. Training a DyBM with Hidden Units,[0],[0]
"[t−1]|x[<t−1],h[<t−1]) and update θ on the basis of
∇θ log pθ(x[t]|x[<t],h[<t]).",3. Training a DyBM with Hidden Units,[0],[0]
"(17)
",3. Training a DyBM with Hidden Units,[0],[0]
"This learning rule is equivalent to the one for the model where all of the units are visible, except that the values for the hidden units are given by sampled values.
",3. Training a DyBM with Hidden Units,[0],[0]
"Therefore, the learning rule for θ follows directly from Osogami & Otsuka (2015a):
b← b+ η (x[t]",3. Training a DyBM with Hidden Units,[0],[0]
− 〈X[t]〉θ) (18) W[d] ←W[d] + ηα[t−1] (x[t] − 〈X[t]〉θ)> (19) V[d] ← V[d] + η β[t−1] (x[t] − 〈X[t]〉θ)> (20) W[δ] ←W[δ] + η x[t−δ] (x[t],3. Training a DyBM with Hidden Units,[0],[0]
− 〈X[t]〉θ)> (21) V[δ] ← V[δ] + η h[t−δ] (x[t] − 〈X[t]〉θ)>,3. Training a DyBM with Hidden Units,[0],[0]
"(22)
for 1 ≤ δ <",3. Training a DyBM with Hidden Units,[0],[0]
"d, where 〈X[t]〉θ denotes the expected values of x[t] with respect to pθ in (7).
",3. Training a DyBM with Hidden Units,[0],[0]
"Now we take the gradient of Lθ,φ(x) with respect to φ:
∇φLθ,φ(x) (23)
= u∑ t=` ∑ h̃[<t] ∇φpφ(h̃[<t]|x[<t−1]) log pθ(x[t]|x[<t], h̃[<t]),
where
∇φpφ(h̃[<t]|x[<t−1])
= ∇φ t−1∏ s=` pφ(h̃",3. Training a DyBM with Hidden Units,[0],[0]
"[s]|x[<s], h̃[<s]) (24)
= t−1∑ s=` ∇φ log pφ(h̃[s]|x[<s], h̃[<s])",3. Training a DyBM with Hidden Units,[0],[0]
t−1∏ s′=` pφ(h̃,3. Training a DyBM with Hidden Units,[0],[0]
"[s′]|x[<s ′], h̃[<s ′])
= pφ(h̃",3. Training a DyBM with Hidden Units,[0],[0]
"[<t]|x[<t−1]) t−1∑ s=` ∇φ log pφ(h̃[s]|x[<s], h̃[<s]).",3. Training a DyBM with Hidden Units,[0],[0]
"(25)
Plugging (25) into the right-hand side of (23), we obtain
∇φLθ,φ(x)
= u∑ t=` ∑ h̃[<t] pφ(h̃",3. Training a DyBM with Hidden Units,[0],[0]
"[<t]|x[<t−1]) log pθ(x[t]|x[<t], h̃[<t])
t−1∑ s=` ∇φ log pφ(h̃[s]|x[<s], h̃[<s]).",3. Training a DyBM with Hidden Units,[0],[0]
"(26)
Similar to (16), the expression of (26) suggests a method of stochastic gradient: at each time t, we sample h[t−1] according to pφ(h[t−1]|x[<t−1],h[<t−1]) and update φ on the basis of the following stochastic gradient:
log pθ(x",3. Training a DyBM with Hidden Units,[0],[0]
"[t]|x[<t],h[<t])Gt−1, (27)
where
Gt−1 ≡ t−1∑ s=` ∇φ log pφ(h[s]|x[<s],h[<s]).",3. Training a DyBM with Hidden Units,[0],[0]
"(28)
Computation of (26) involves mainly two interrelated inefficiencies.",3. Training a DyBM with Hidden Units,[0],[0]
"First, although (26) can be approximately computed using sampled hidden values h̃[<t] in the same way as (16), the samples cannot be reused after updating φ because it was sampled from the distribution with the previous parameter.",3. Training a DyBM with Hidden Units,[0],[0]
"Second, since each summand of Gt−1 is dependent on φ, Gt−1 also has to be recomputed after each update.",3. Training a DyBM with Hidden Units,[0],[0]
"Thus, the computational complexity of (27) grows linearly with respect to the length of the time-series (i.e., t− `), in contrast to (17), whose complexity is independent of that length.",3. Training a DyBM with Hidden Units,[0],[0]
"One could approximately compute (28) recursively:
",3. Training a DyBM with Hidden Units,[0],[0]
"Gt ← γ Gt−1 + (1− γ)∇φ log pφ(h[t]|x[<t],h[<t]), (29)
",3. Training a DyBM with Hidden Units,[0],[0]
where γ ∈,3. Training a DyBM with Hidden Units,[0],[0]
"[0, 1) is a discount factor.",3. Training a DyBM with Hidden Units,[0],[0]
"The recursive update rule with γ < 1 puts exponentially small weight γt−s on ∇φ log pφ(h[s]|x[<s],h[<s]) computed with an old value of φ (i.e., s t).",3. Training a DyBM with Hidden Units,[0],[0]
"This recursively computed Gt is related to the momentum in gradient descent (Qian, 1999).",3. Training a DyBM with Hidden Units,[0],[0]
"See the supplementary material for specific learning rules suggested by (27)–(29).
",3. Training a DyBM with Hidden Units,[0],[0]
"Observe in (26) that ∇φLθ,φ(x) consists of the products of log pθ(x[t]|x[<t],h[<t]) and ∇φ log pφ(h[s]|x[<s],h[<s]) for s < t.",3. Training a DyBM with Hidden Units,[0],[0]
"Without the dependency on log pθ(x
[t]|x[<t],h[<t]), the parameter φ is updated in a way that h[s] is more likely to be generated (i.e., the learning rule would be equivalent to that for visible units).",3. Training a DyBM with Hidden Units,[0],[0]
"Such an update rule is undesirable, because h[s] has been sampled and is not necessarily what we want to sample again.",3. Training a DyBM with Hidden Units,[0],[0]
"The dependency on log pθ(x[t]|x[<t],h[<t]) suggests that φ is updated by a large amount if the sampled h[s] happens to make the future values, x[t] for t > s, likely.",3. Training a DyBM with Hidden Units,[0],[0]
"Intuitively, weighting ∇φ log pφ(h[s]|x[<s],h[<s]) by log pθ(x[t]|x[<t],h[<t]) for t > s is inevitable, because whether the particular values of hidden units are good for the purpose of predicting future values will only be known after seeing future values.",3. Training a DyBM with Hidden Units,[0],[0]
"Because the stochastic gradient for φ requires approximations that are not needed for θ, we might not be able to learn appropriate values of φ as effectively as θ.",4. Learning with Reversed Time-series,[0],[0]
"This motivates us to consider a backward DyBM in Figure 2, which has a common set of parameters, (θ, φ), as the forward DyBM in Figure 1 but defines the conditional distribution for time-series from the future.",4. Learning with Reversed Time-series,[0],[0]
"Specifically, the energy of the backward DyBM is represented analogously to (1) with the superscript",4. Learning with Reversed Time-series,[0],[0]
"[< t] replaced by [> t], where we define
Eθ(x",4. Learning with Reversed Time-series,[0],[0]
"[t]|x[>t],h[>t]) = −b>x[t] − T∑ δ=1 (x[t+δ])>(W[δ])>x[t]
− T∑ δ=1 (x[t+δ])>(U[δ])>h[t] (30)
and Eφ(h[t]|x[>t],h[>t]) is defined from (30) by letting W ← V, U ← Z, b ← 0, and x[t] ← h[t].",4. Learning with Reversed Time-series,[0],[0]
"Similar to the forward DyBM, we assume that the weight has the parametric form of (4) and let T →∞.
Namely, the backward DyBM is obtained from the forward DyBM by the following changes: W ← W>, Z ← Z>, U← V>, and V← U>.",4. Learning with Reversed Time-series,[0],[0]
"The other difference between (2) and (30) is the sign of δ, but this is because the backward DyBM deals with time-series from the future.
",4. Learning with Reversed Time-series,[0],[0]
"Because the backward DyBM has the structure that is equivalent to that of the forward DyBM, it can be trained in
the same manner as the forward DyBM but using timeseries from the future.",4. Learning with Reversed Time-series,[0],[0]
"Specifically, θ′ ≡ (U>,W>,b) in the backward DyBM is optimized analogously to θ in the forward DyBM.",4. Learning with Reversed Time-series,[0],[0]
"Likewise, φ′ ≡",4. Learning with Reversed Time-series,[0],[0]
"(V>,Z>) is optimized analogously to φ.",4. Learning with Reversed Time-series,[0],[0]
"Recall that φ and φ′ are relatively hard to optimize, and θ and θ′ are relatively easy to optimize.
",4. Learning with Reversed Time-series,[0],[0]
"Our key observation is that the parameter U, which is in φ and is relatively hard to optimize in the forward DyBM, is in θ′ and is relatively easy to optimize in the backward DyBM.",4. Learning with Reversed Time-series,[0],[0]
"By training both the forward DyBM and the backward DyBM, we expect to effectively find appropriate values of θ and θ′.
Consider a stochastic process X(t) whose distribution is given by a forward DyBM.",4. Learning with Reversed Time-series,[0],[0]
We remark that the distribution of the stochastic process X(−t) that is defined by reversing X(t) is generally different from what the corresponding backward DyBM gives unless the DyBMs have no hidden units.,4. Learning with Reversed Time-series,[0],[0]
"The exact distribution of X(−t) needs to be given by marginalizing out the past values (i.e., succeeding values for the backward process).",4. Learning with Reversed Time-series,[0],[0]
"Despite this discrepancy, we expect that bidirectional training is effective because of intuitive correspondence between the forward DyBM and the backward DyBM.",4. Learning with Reversed Time-series,[0],[0]
"In particular, W [δ]i,j in both DyBMs represent the strength of the correlation between the past value of unit i and the future value of unit j, where the time is separated by δ.",4. Learning with Reversed Time-series,[0],[0]
"A recommendation is, however, to perform backward training more moderately than forward training.",4. Learning with Reversed Time-series,[0],[0]
We will show an example of a specific procedure in the next section.,4. Learning with Reversed Time-series,[0],[0]
We now demonstrate the effectiveness of bidirectional training through numerical experiments in two settings.,5. Numerical Experiments,[0],[0]
The purpose of the first setting is to study whether bidirectional learning of the DyBM with hidden units can indeed learn to predict what cannot be done without bidirectional learning or hidden units.,5. Numerical Experiments,[0],[0]
We use a synthetic dataset that is designed specifically for this purpose.,5. Numerical Experiments,[0],[0]
"In the second setting, we study the effectiveness of bidirectional learning and hidden units on real datasets.",5. Numerical Experiments,[0],[0]
"We use the two datasets that have been used in Dasgupta & Osogami (2017) as well as a 391 dimensional time-series, which is substantially larger than the eight or lower dimensional time-series that are used in the other experiments.",5. Numerical Experiments,[0],[0]
The experiments are carried out with a Python implementation on workstations having 48-64 GB memory and 2.6-4.0 GHz CPU.,5. Numerical Experiments,[0],[0]
"For each dataset, we train a DyBM with or without hidden units.",5.1. Specific Learning Algorithms to Evaluate,[0],[0]
"Because all of the datasets are real valued, visible units are Gaussian and give predictions by (5) with x[t] omit-
Algorithm 1 Specific steps of bidirectional learning evaluated in experiments T :",5.1. Specific Learning Algorithms to Evaluate,[0],[0]
"The total number of iterations T0: The number of iterations of bidirectional learning F : The relative frequency of forward learning for t = 1 to T do
if t < T0 and t mod F + 1 = 0",5.1. Specific Learning Algorithms to Evaluate,[0],[0]
"then Backward learning to update (U,W,b) else Forward learning to update (V,W,b)
end if end for
ted (see (52) in the supplementary material).",5.1. Specific Learning Algorithms to Evaluate,[0],[0]
"To reduce the variability in the experiments, we use the expected value for the output of a hidden unit instead of sampling a binary value.",5.1. Specific Learning Algorithms to Evaluate,[0],[0]
"By the law of large numbers, the use of expected value corresponds to having an infinitely many binary hidden units that are conditionally independent and identically distributed (i.i.d.)",5.1. Specific Learning Algorithms to Evaluate,[0],[0]
given the internal state of the DyBM.,5.1. Specific Learning Algorithms to Evaluate,[0],[0]
"See also (Sutskever et al., 2008; Sutskever & Hinton, 2007) for the related use of the expected values for hidden units.
",5.1. Specific Learning Algorithms to Evaluate,[0],[0]
A DyBM with hidden unit is trained bidirectionally or only with forward learning.,5.1. Specific Learning Algorithms to Evaluate,[0],[0]
A DyBM without hidden units is trained only with forward learning.,5.1. Specific Learning Algorithms to Evaluate,[0],[0]
"While bidirectional learning has several design choices, here we evaluate the specific algorithm shown in Algorithm 1.",5.1. Specific Learning Algorithms to Evaluate,[0],[0]
"In particular, we perform bidirectional learning for the first T0 iterations, where the backward training is apply every F +1 steps.",5.1. Specific Learning Algorithms to Evaluate,[0],[0]
"For the rest of T − T0 iterations, we perform forward learning only.",5.1. Specific Learning Algorithms to Evaluate,[0],[0]
Throughout we set F = 2.,5.1. Specific Learning Algorithms to Evaluate,[0],[0]
Note that Z is fixed with its initial values throughout learning.,5.1. Specific Learning Algorithms to Evaluate,[0],[0]
Here we do not update U in forward learning and V in backward learning.,5.1. Specific Learning Algorithms to Evaluate,[0],[0]
"This is partly because the learning rule of (27)-(28) has no effect when we use the expected values in hidden units.
",5.1. Specific Learning Algorithms to Evaluate,[0],[0]
"Before applying Algorithm 1, the bias b is initialized to zero, and the weight (U, V, W, Z) is initialized with i.i.d. normal random variable with mean 0 and standard deviation of 0.01 (Hinton, 2012) except the following two changes.",5.1. Specific Learning Algorithms to Evaluate,[0],[0]
"First, we set the mean of W[1] as an identity matrix for real datasets, because using the previous value for prediction is clearly beneficial for these datasets.",5.1. Specific Learning Algorithms to Evaluate,[0],[0]
"Second, we use the small standard deviation of 0.001 for the large dataset of 391 dimensions for faster convergence.",5.1. Specific Learning Algorithms to Evaluate,[0],[0]
"The learning rate is adjusted according to AdaGrad (Duchi et al., 2011), where the the initial learning rate is optimized as we discuss in the following.
",5.1. Specific Learning Algorithms to Evaluate,[0],[0]
"Throughout the experiments, we set the decay rate of the eligibility traces in (6) to zero: α[t−1] = x[t−d] and β[t−1] = h[t−d].",5.1. Specific Learning Algorithms to Evaluate,[0],[0]
The delay d and the number of hidden units are varied for each dataset.,5.1. Specific Learning Algorithms to Evaluate,[0],[0]
"We first demonstrate the effectiveness of our bidirectional training in a synthetic setting of learning a one-dimensional noisy sawtooth wave, which is generated according to
x[t] = t C −",5.2. Synthetic Data,[0],[0]
⌊ t C ⌋,5.2. Synthetic Data,[0],[0]
+,5.2. Synthetic Data,[0],[0]
"εt, for t = 0, 1, . . .",5.2. Synthetic Data,[0],[0]
"(31)
where C is the period of the noisy sawtooth wave, and εt is an i.i.d. normal random variable, whose mean is fixed at 0 and standard deviation at 0.01.",5.2. Synthetic Data,[0],[0]
"The noisy sawtooth wave has large discontinuity at the end of each period, which makes hidden units essential for learning.
",5.2. Synthetic Data,[0],[0]
Here we train a DyBM with one hidden unit or no hidden units.,5.2. Synthetic Data,[0],[0]
"Throughout, the delay is set d = 4, and the learning rate is initialized to η = 1.0.",5.2. Synthetic Data,[0],[0]
"Bidirectional learning is continued for T0 = T/2 iterations, where T is varied depending on C. In each iteration, we use one period of the noisy sawtooth wave and update the parameters with stochastic gradients.
",5.2. Synthetic Data,[0],[0]
Figure 3(a) and (c) show how learning progresses over iterations.,5.2. Synthetic Data,[0],[0]
The period of the noisy sawtooth wave is C = 6 in (a) and C = 8 in (c).,5.2. Synthetic Data,[0],[0]
"Training is continued for T = 1, 000 iterations for C = 6 and T = 30, 000 iterations for C = 8.",5.2. Synthetic Data,[0],[0]
"In every F + 1 iterations, we let the DyBM predict the one-step-ahead value for each of the C steps of one period during forward learning.",5.2. Synthetic Data,[0],[0]
We evaluate the root mean squared error (RMSE) of one-step-ahead predictions against true values.,5.2. Synthetic Data,[0],[0]
"For clarity, the RMSE curves are smoothed with a Gaussian filter with window size of 50.
",5.2. Synthetic Data,[0],[0]
"In the figure, the solid curves show the results with the bidirectionally trained DyBMs (Bidirectional).",5.2. Synthetic Data,[0],[0]
"As a baseline, we also train the DyBM only with forward training and show the results with dashed curves (Baseline).",5.2. Synthetic Data,[0],[0]
The dotted curves show the results with the DyBM with no hidden units (No hidden).,5.2. Synthetic Data,[0],[0]
The comparison suggests that Bidirectional can substantially (by a factor of 10) improve the predictive accuracy over Baseline.,5.2. Synthetic Data,[0],[0]
Notice also that the hidden unit can hurt the predictive accuracy without bidirectional training.,5.2. Synthetic Data,[0],[0]
"No hidden often exhibits lower RMSE than Baseline.
",5.2. Synthetic Data,[0],[0]
"In Figure 3(c), the reduction of the RMSE accelerates at T0 iterations, after which bidirectional learning is no longer performed.",5.2. Synthetic Data,[0],[0]
"This suggests that, after learning appropriate values of U (namely, what features should be used for prediction) via bidirectional learning, it is better to optimally learn (V,W,b) given the learned values of U. Namely, although bidirectional learning help learn appropriate values of U, it does not necessarily optimize all of the parameters of the DyBM.",5.2. Synthetic Data,[0],[0]
"Recall also the discussion at the end of Section 4 that the backward DyBM is not exactly the same as the time-reversed DyBM.
",5.2. Synthetic Data,[0],[0]
"Figure 3(b) and (d) show the values predicted by bidirection-
ally trained DyBMs (black curves) and the corresponding target values (red curves).",5.2. Synthetic Data,[0],[0]
"For clarity, we use a noiseless sawtooth wave as the target by letting εt = 0 in (31).",5.2. Synthetic Data,[0],[0]
"Observe that the bidirectionally trained DyBM well predicts the next value of the sawtooth wave, substantially better than the baseline (or the DyBM with no hidden unit).",5.2. Synthetic Data,[0],[0]
"In particular, the bidirectionally trained DyBM can well predict the sharp drop at the end of each period.",5.2. Synthetic Data,[0],[0]
"This is in contrast to the baseline, whose prediction is rather smoothed out over the period.",5.2. Synthetic Data,[0],[0]
"Next, we demonstrate the effectiveness of bidirectional training on three real datasets, including the two datasets used in Dasgupta & Osogami (2017).",5.3. Real Data,[0],[0]
"The first dataset is the monthly sunspot number1, which we will refer to as Sunspot.",5.3. Real Data,[0],[0]
"This time-series has one dimension and 2,820 steps (corresponding to January 1749 to December 1983).",5.3. Real Data,[0],[0]
"The second dataset is the weekly retail gasoline and diesel prices2, which we will refer to as Price.",5.3. Real Data,[0],[0]
"This time-series has eight dimensions (corresponding to eight locations in the US) and 1,223 steps (corresponding to April 5th, 1993 to September 5th, 2016).",5.3. Real Data,[0],[0]
"Following Dasgupta & Osogami (2017), the first 67 % of each time-series is used for training, and the remaining 33 % is used for test.",5.3. Real Data,[0],[0]
See Dasgupta & Osogami (2017) for further details about the first two datasets.,5.3. Real Data,[0],[0]
"The third dataset is the NOAA Global Surface Temperature3, which consists of a real valued time-series of 1,635 steps (t = 1, . . .",5.3. Real Data,[0],[0]
", 1635)",5.3. Real Data,[0],[0]
with 391 dimensions.,5.3. Real Data,[0],[0]
"We use the first 80 % of the time-series for training and the remaining 20 % for test.
",5.3. Real Data,[0],[0]
"We normalize the values of each dataset in a way that the
1https://datamarket.com/data/set/22t4/ 2https://www.eia.gov/dnav/pet/pet_pri_
gnd_a_epm0_pte_dpgal_w.htm 3V4.00 of Air Temperature (air.mon.anom.nc) from https://www.esrl.noaa.gov/psd/data/gridded/ data.noaaglobaltemp.html
values in a training data are in [0,1] for each dimension.",5.3. Real Data,[0],[0]
"Notice that this normalization differs from that in (Dasgupta & Osogami, 2017), where the values in training or test data are in [0,1] for each dimension.",5.3. Real Data,[0],[0]
"However, the use of test data for normalization is less appropriate.",5.3. Real Data,[0],[0]
"This difference in normalization has no effect on the Price dataset, but the results on the Sunspot dataset need to be renormalized to be compared against those in (Dasgupta & Osogami, 2017).
",5.3. Real Data,[0],[0]
Here we train a DyBM with four hidden units or no hidden units.,5.3. Real Data,[0],[0]
"In each iteration, we use the whole training data (except the first d steps for forward learning and the last d steps for backward learning, which are used only to update the internal state of the DyBM) once and update the parameters with stochastic gradients.
",5.3. Real Data,[0],[0]
We find that the speed of convergence is sensitive to the initial learning rate.,5.3. Real Data,[0],[0]
"Here, we choose the initial learning rate from {20, 2−1, 2−2, . . .}.",5.3. Real Data,[0],[0]
"Specifically, we choose 2−k with the smallest k such that the training RMSE after the initial T/100 iterations is smaller than that with 2−(k+1).",5.3. Real Data,[0],[0]
"Because the training RMSE tends to decrease with k up to a point and then increases with k, we usually choose the initial learning rate that minimizes the training RMSE after those initial iterations.
",5.3. Real Data,[0],[0]
Figure 4 shows the RMSE of one-step-ahead prediction with respect to the test data after every F + 1 iterations of training.,5.3. Real Data,[0],[0]
"We show the results where the delay is set d = 30 for Sunspot, d = 3 for Price, and d = 2 for Temperature.",5.3. Real Data,[0],[0]
"However, we have also run experiments with d ∈ {20, 40} for Sunspot and d ∈ {2, 4} for Price and have found that these do not improve the accuracy for any of the three methods.",5.3. Real Data,[0],[0]
"For the large dataset of Temperature, we have been able to perform limited experiments due to its relatively heavy computational requirements.",5.3. Real Data,[0],[0]
"Again, we compare Bidirectional against Baseline and No hidden.",5.3. Real Data,[0],[0]
"However, we now vary T0 ∈ {T/4, T/2, T}, so that each figure has three solid curves.",5.3. Real Data,[0],[0]
"The range of the vertical axis is chosen in a way that the upper limit corresponds to the RMSE with a naı̈ve
prediction of using the preceding values as prediction.
",5.3. Real Data,[0],[0]
"For the Sunspot dataset (a), we find that Bidirectional does not improve upon Baseline, although having hidden units (Bidirectional and Baseline) can make the RMSE lower than No hidden.",5.3. Real Data,[0],[0]
"This means that the randomly set values of U is effective, and bidirectional learning does not find better values of U. After 1,000 iterations, however, Bidirectional with T0 = T/4 or T0 = T/2 achieves the RMSE that is essentially indistinguishable from that with Baseline.",5.3. Real Data,[0],[0]
"The best RMSE achieved by the Baseline is 0.0698, which corresponds to 0.0657 when the dataset is normalized in the way of (Dasgupta & Osogami, 2017) and is lower than 0.0734 reported in (Dasgupta & Osogami, 2017).
",5.3. Real Data,[0],[0]
"For Price (b) and Temperature (c), Bidirectional improves upon Baseline particularly when the bidirectional learning is stopped after T0 < T iterations.",5.3. Real Data,[0],[0]
"Bidirectional reduces the RMSE more slowly than Baseline or No hidden but eventually outperforms the others, and the reduction of the RMSE can be accelerated by stopping the bidirectional training after T0 < T iterations.",5.3. Real Data,[0],[0]
"In particular, the best RMSE achieved by Bidirectional with T0 = T/4 is 0.0399, which is lower than 0.0564 reported in (Dasgupta & Osogami, 2017).",5.3. Real Data,[0],[0]
"In addition, while Baseline and No hidden (namely, forward learning only) starts overfitting to training data and increases the RMSE with respect to the test data after some iterations, bidirectional training appears to avoid such overfit.",5.3. Real Data,[0],[0]
We have proposed bidirectional training for time-series models with hidden units.,6. Conclusion,[0],[0]
"Namely, we consider two models that have a common set of parameters, where one model is trained with forward time-series and the other with backward time-series.",6. Conclusion,[0],[0]
Our key idea is that some of the parameters that are difficult to learn in one model can be effectively learned in the other model.,6. Conclusion,[0],[0]
"Numerical experiments suggest that bidirectional training has the additional effect of
avoiding overfit to training data.
",6. Conclusion,[0],[0]
"The DyBM with hidden units analyzed in Sections 2–4 is new, and its analysis has two highlights, which have led to proposing bidirectional training.",6. Conclusion,[0],[0]
The first highlight is that the learning rule for V (hidden-to-visible weight) in (20)–(22) becomes equivalent to those for W (visible-tovisible weight) in (20)–(22) when the lower bound (14) is maximized.,6. Conclusion,[0],[0]
"The second highlight is that we cannot learn the weight to hidden units (φ = (U,Z)) in the same way as the weight to visible units (θ = (V,W,b)) due to the form of the gradient in (27).
",6. Conclusion,[0],[0]
"Although we have demonstrated the effectiveness of bidirectional training in specific cases, its capabilities are not fully explored.",6. Conclusion,[0],[0]
Bidirectional training has many design choices that need further study.,6. Conclusion,[0],[0]
"For example, one might want to use the gradients in (27)-(28), possibly with the approximation in (29), to update (U,Z) in forward learning and (V,Z) in backward learning.",6. Conclusion,[0],[0]
It would also be interesting to apply bidirectional training to other time-series models having parameters that represent the dependency between hidden values at one time and visible values at another time.,6. Conclusion,[0],[0]
"In addition to DyBM and VAR with hidden units, these include Spiking Boltzmann Machine (Hinton & Brown, 1999) and Conditional Restricted Boltzmann Machine (Taylor et al., 2007).",6. Conclusion,[0],[0]
"Because bidirectional training is largely complementary to other techniques for learning time-series, it would be interesting to investigate how bidirectional training improves performance when it is combined with these other techniques.",6. Conclusion,[0],[0]
We expect that this work opens up a line of research on more effective methods of bidirectional training.,6. Conclusion,[0],[0]
"This work was supported by JST CREST Grant Number JPMJCR1304, Japan.",Acknowledgments,[0],[0]
Hidden units can play essential roles in modeling time-series having long-term dependency or nonlinearity but make it difficult to learn associated parameters.,abstractText,[0],[0]
"Here we propose a way to learn such a time-series model by training a backward model for the time-reversed time-series, where the backward model has a common set of parameters as the original (forward) model.",abstractText,[0],[0]
"Our key observation is that only a subset of the parameters is hard to learn, and that subset is complementary between the forward model and the backward model.",abstractText,[0],[0]
"By training both of the two models, we can effectively learn the values of the parameters that are hard to learn if only either of the two models is trained.",abstractText,[0],[0]
We apply bidirectional learning to a dynamic Boltzmann machine extended with hidden units.,abstractText,[0],[0]
Numerical experiments with synthetic and real datasets clearly demonstrate advantages of bidirectional learning.,abstractText,[0],[0]
Bidirectional Learning for Time-series Models with Hidden Units,title,[0],[0]
"While in standard supervised learning problems we seek the best hypothesis in a given space and with a given learning algorithm, in hyperparameter optimization (HO) and metalearning (ML) we seek a configuration so that the optimized learning algorithm will produce a model that generalizes well to new data.",1. Introduction,[0],[0]
"The search space in ML often incorporates choices associated with the hypothesis space and the features of the learning algorithm itself (e.g., how optimization of the training loss is performed).",1. Introduction,[0],[0]
"Under this common perspective, both HO and ML essentially boil down to nesting two search problems: at the inner level we seek a good
1Computational Statistics and Machine Learning, Istituto Italiano di Tecnologia, Genoa, Italy 2Department of Computer Science, University College London, London, UK 3Department of Information Engineering, Università degli Studi di Firenze, Florence, Italy.",1. Introduction,[0],[0]
"Correspondence to: Luca Franceschi <luca.franceschi@iit.it>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
hypothesis (as in standard supervised learning) while at the outer level we seek a good configuration (including a good hypothesis space) where the inner search takes place.",1. Introduction,[0],[0]
"Surprisingly, the literature on ML has little overlap with the literature on HO and in this paper we present a unified framework encompassing both of them.
",1. Introduction,[0],[0]
"Classic approaches to HO (see e.g. Hutter et al., 2015, for a survey) have been only able to manage a relatively small number of hyperparameters, from a few dozens using random search (Bergstra and Bengio, 2012) to a few hundreds using Bayesian or model-based approaches (Bergstra et al., 2013; Snoek et al., 2012).",1. Introduction,[0],[0]
"Recent gradient-based techniques for HO, however, have significantly increased the number of hyperparameters that can be optimized (Domke, 2012; Maclaurin et al., 2015; Pedregosa, 2016; Franceschi et al., 2017) and it is now possible to tune as hyperparameters entire weight vectors associated with a neural network layer.",1. Introduction,[0],[0]
"In this way, it becomes feasible to design models that possibly have more hyperparameters than parameters.",1. Introduction,[0],[0]
"Such an approach is well suited for ML, since parameters are learned from a small dataset, whereas hyperparameters leverage multiple available datasets.
",1. Introduction,[0],[0]
HO and ML only differ substantially in terms of the experimental settings in which they are evaluated.,1. Introduction,[0],[0]
"While in HO the available data is associated with a single task and split into a training set (used to tune the parameters) and a validation set (used to tune the hyperparameters), in ML we are often interested in the so-called few-shot learning setting where data comes in the form of short episodes (small datasets with few examples per class) sampled from a common probability distribution over supervised tasks.
",1. Introduction,[0],[0]
"Early work on ML dates back at least to the 1990’s (Schmidhuber, 1992; Baxter, 1995; Thrun and Pratt, 1998) but this research area has received considerable attention in the last few years, mainly driven by the need in real-life and industrial scenarios for learning quickly a vast multitude of tasks.",1. Introduction,[0],[0]
"These tasks, or episodes, may appear and evolve continuously over time and may only contain few examples (Lake et al., 2017).",1. Introduction,[0],[0]
Different strategies have emerged to tackle ML.,1. Introduction,[0],[0]
"Although they do overlap in some aspects, it is possible to identify at least four of them.",1. Introduction,[0],[0]
"The metric strategy attempts to use training episodes to construct embeddings such that examples of the same class are mapped into similar repre-
sentations.",1. Introduction,[0],[0]
"It has been instantiated in several variants that involve non-parametric (or instance-based) predictors (Koch et al., 2015; Vinyals et al., 2016; Snell et al., 2017).",1. Introduction,[0],[0]
"In the related memorization strategy, the meta-learner learns to store and retrieve data points representations in memory.",1. Introduction,[0],[0]
"It can be implemented either using recurrent networks (Santoro et al., 2016) or temporal convolutions (Mishra et al., 2018).",1. Introduction,[0],[0]
"The use of an attention mechanism (Vaswani et al., 2017) is crucial both in (Vinyals et al., 2016) and in (Mishra et al., 2018).",1. Introduction,[0],[0]
"The initialization strategy (Ravi and Larochelle, 2017; Finn et al., 2017) uses training episodes to infer a good initial value for the model’s parameters so that new tasks can be learned quickly by fine tuning.",1. Introduction,[0],[0]
"The optimization strategy (Andrychowicz et al., 2016; Ravi and Larochelle, 2017; Wichrowska et al., 2017) forges an optimization algorithm that will find it easier to learn on novel related tasks.
",1. Introduction,[0],[0]
"A main contribution of this paper is a unified view of HO and ML within the natural mathematical framework of bilevel programming, where an outer optimization problem is solved subject to the optimality of an inner optimization problem.",1. Introduction,[0],[0]
In HO the outer problem involves hyperparameters while the inner problem is usually the minimization of an empirical loss.,1. Introduction,[0],[0]
In ML the outer problem could involve a shared representation among tasks while the inner problem could concern classifiers for individual tasks.,1. Introduction,[0],[0]
"Bilevel programming (Bard, 2013) has been suggested before in machine learning in the context of kernel methods and support vector machines (Keerthi et al., 2007; Kunapuli et al., 2008), multitask learning (Flamary et al., 2014), and more recently HO (Pedregosa, 2016), but never in the context of ML.",1. Introduction,[0],[0]
"The resulting framework outlined in Sec. 2 encompasses some existing approaches to ML, in particular those based on the initialization and the optimization strategies.
",1. Introduction,[0],[0]
A technical difficulty arises when the solution to the inner problem cannot be written analytically (for example this happens when using the log-loss for training neural networks) and one needs to resort to iterative optimization approaches.,1. Introduction,[0],[0]
"As a second contribution, we provide in Sec.",1. Introduction,[0],[0]
3 sufficient conditions that guarantee good approximation properties.,1. Introduction,[0],[0]
"We observe that these conditions are reasonable and apply
to concrete problems relevant to applications.
",1. Introduction,[0],[0]
"In Sec. 4, by taking inspiration on early work on representation learning in the context of multi-task and meta-learning (Baxter, 1995; Caruana, 1998), we instantiate the framework for ML in a simple way treating the weights of the last layer of a neural network as the inner variables and the remaining weights, which parametrize the representation mapping, as the outer variables.",1. Introduction,[0],[0]
"As shown in Sec. 5, the resulting ML algorithm performs well in practice, outperforming most of the existing strategies on MiniImagenet.",1. Introduction,[0],[0]
"In this paper, we consider bilevel optimization problems (see e.g. Colson et al., 2007) of the form
min{f(λ) : λ ∈ Λ}, (1)
where function f : Λ→ R is defined at λ ∈ Λ as
f(λ) = inf{E(wλ, λ) : wλ ∈ arg min u∈Rd Lλ(u)}.",2. A bilevel optimization framework,[0],[0]
"(2)
We call E : Rd × Λ→ R the outer objective and, for every λ ∈ Λ, we call Lλ : Rd → R the inner objective.",2. A bilevel optimization framework,[0],[0]
Note that {Lλ : λ ∈ Λ} is a class of objective functions parameterized by λ.,2. A bilevel optimization framework,[0],[0]
"Specific instances of this problem include HO and ML, which we discuss next.",2. A bilevel optimization framework,[0],[0]
"Table 1 outlines the links among bilevel programming, HO and ML.",2. A bilevel optimization framework,[0],[0]
"In the context of hyperparameter optimization, we are interested in minimizing the validation error of a model gw : X",2.1. Hyperparameter Optimization,[0],[0]
"→ Y parameterized by a vector w, with respect to a vector of hyperparameters λ.",2.1. Hyperparameter Optimization,[0],[0]
"For example, we may consider representation or regularization hyperparameters that control the hypothesis space or penalties, respectively.",2.1. Hyperparameter Optimization,[0],[0]
"In this setting, a prototypical choice for the inner objective is the regularized empirical error
Lλ(w) = ∑
(x,y)∈Dtr
`(gw(x), y) + Ωλ(w),
where Dtr = {(xi, yi)}ni=1 is a set of input/output points, ` is a prescribed loss function, and Ωλ a regularizer parameterized by λ.",2.1. Hyperparameter Optimization,[0],[0]
"The outer objective represents a proxy for the generalization error of gw, and it may be given by the average loss on a validation set Dval
E(w, λ) = ∑
(x,y)∈Dval
`(gw(x), y).
or, in more generality, by a cross-validation error, as detailed in Appendix B. Note that in this setting, the outer objective E does not depend explicitly on the hyperparameters λ,
Error
since in HO λ is instrumental in finding a good model gw, which is our final goal.",2.1. Hyperparameter Optimization,[0],[0]
"As a more specific example, consider linear models, gw(x) = 〈w, x〉, let ` be the square loss and let Ωλ(w) = λ‖w‖2, in which case the inner objective is ridge regression (Tikhonov regularization) and the bilevel problem optimizes over the regularization parameter the validation error of ridge regression.",2.1. Hyperparameter Optimization,[0],[0]
"In meta-learning (ML) the inner and outer objectives are computed by averaging a training and a validation error over multiple tasks, respectively.",2.2. Meta-Learning,[0],[0]
The goal is to produce a learning algorithm that will work well on novel tasks1.,2.2. Meta-Learning,[0],[0]
"For this purpose, we have available a meta-training set D = {Dj = Djtr ∪ D j val}Nj=1, which is a collection of datasets, sampled from a meta-distribution P .",2.2. Meta-Learning,[0],[0]
"Each dataset Dj = {(xji , y j i )}",2.2. Meta-Learning,[0],[0]
nj i=1,2.2. Meta-Learning,[0],[0]
"with (x j i , y j i ) ∈",2.2. Meta-Learning,[0],[0]
X,2.2. Meta-Learning,[0],[0]
×,2.2. Meta-Learning,[0],[0]
Yj is linked to a specific task.,2.2. Meta-Learning,[0],[0]
Note that the output space is task dependent (e.g. a multi-class classification problem with variable number of classes).,2.2. Meta-Learning,[0],[0]
"The model for each task is a function gwj ,λ : X → Yj , identified by a parameter vectors wj and hyperparameters λ.",2.2. Meta-Learning,[0],[0]
A key point here is that λ is shared between the tasks.,2.2. Meta-Learning,[0],[0]
"With this notation the inner and outer objectives are
Lλ(w) =",2.2. Meta-Learning,[0],[0]
"N∑ j=1 Lj(wj , λ,Djtr), (3)
E(w, λ) = N∑ j=1 Lj(wj , λ,Djval) (4)
respectively.",2.2. Meta-Learning,[0],[0]
"The loss Lj(wj , λ, S) represents the empirical error of the pair (wj , λ) on a set of examples S. Note that the
1The ML problem is also related to multitask learning, however in ML the goal is to extrapolate from the given tasks.
inner and outer losses for task j use different train/validation splits of the corresponding dataset Dj .",2.2. Meta-Learning,[0],[0]
"Furthermore, unlike in HO, in ML the final goal is to find a good λ and the wj are now instrumental.
",2.2. Meta-Learning,[0],[0]
The cartoon in Figure 1 illustrates ML as a bilevel problem.,2.2. Meta-Learning,[0],[0]
The parameter λ indexes an hypothesis space within which the inner objective is minimized.,2.2. Meta-Learning,[0],[0]
"A particular example, detailed in Sec. 4, is to choose the model gw,λ = 〈w, hλ(x)〉, in which case λ parameterizes a feature mapping.",2.2. Meta-Learning,[0],[0]
"Yet another choice would be to consider gwj ,λ(x) =",2.2. Meta-Learning,[0],[0]
"〈w + λ, x〉, in which case λ represents a common model around which task specific models are to be found (see e.g. Evgeniou et al., 2005; Finn et al., 2017; Khosla et al., 2012; Kuzborskij et al., 2013, and reference therein).",2.2. Meta-Learning,[0],[0]
We now discuss a general approach to solve Problem (1)-(2) when the hyperparameter vector λ is real-valued.,2.3. Gradient-Based Approach,[0],[0]
To simplify our discussion let us assume that the inner objective has a unique minimizer wλ.,2.3. Gradient-Based Approach,[0],[0]
"Even in this simplified scenario, Problem (1)-(2) remains challenging to solve.",2.3. Gradient-Based Approach,[0],[0]
"Indeed, in general there is no closed form expression wλ, so it is not possible to directly optimize the outer objective function.",2.3. Gradient-Based Approach,[0],[0]
While a possible strategy (implicit differentiation) is to apply the implicit function theorem to∇Lλ = 0,2.3. Gradient-Based Approach,[0],[0]
"(Pedregosa, 2016; Koh and Liang, 2017; Beirami et al., 2017), another compelling approach is to replace the inner problem with a dynamical system.",2.3. Gradient-Based Approach,[0],[0]
"This point, discussed in (Domke, 2012; Maclaurin et al., 2015; Franceschi et al., 2017), is developed further in this paper.
",2.3. Gradient-Based Approach,[0],[0]
"Specifically, we let [T ] = {1, . . .",2.3. Gradient-Based Approach,[0],[0]
", T} where T is a prescribed positive integer and consider the following approximation of Problem (1)-(2)
min λ fT (λ)",2.3. Gradient-Based Approach,[0],[0]
"= E(wT,λ, λ), (5)
where E is a smooth scalar function, and2
w0,λ = Φ0(λ), wt,λ = Φt(wt−1,λ, λ), t ∈",2.3. Gradient-Based Approach,[0],[0]
"[T ], (6)
with Φ0 :",2.3. Gradient-Based Approach,[0],[0]
"Rm → Rd a smooth initialization mapping and, for every t ∈",2.3. Gradient-Based Approach,[0],[0]
"[T ], Φt : Rd × Rm → Rd a smooth mapping that represents the operation performed by the t-th step of an optimization algorithm.",2.3. Gradient-Based Approach,[0],[0]
"For example, the optimization dynamics could be gradient descent: Φt(wt, λ) =",2.3. Gradient-Based Approach,[0],[0]
"wt − ηt∇Lλ(·) where (ηt)t∈[T ] is a sequence of steps sizes.
",2.3. Gradient-Based Approach,[0],[0]
"The approximation of the bilevel problem (1)-(2) by the procedure (5)-(6) raises the issue of the quality of this approximation and we return to this issue in the next section.
",2.3. Gradient-Based Approach,[0],[0]
"2In general, the algorithm used to minimize the inner objective may involve auxiliary variables, e.g., velocities when using gradient descent with momentum, so w should be intended as a larger vector containing both model parameters and auxiliary variables.
",2.3. Gradient-Based Approach,[0],[0]
"However, it also suggests to consider the inner dynamics as a form of approximate empirical error minimization (e.g. early stopping) which is valid in its own right.",2.3. Gradient-Based Approach,[0],[0]
From this perspective – conversely to the implicit differentiation strategy – it is possible to include among the components of λ variables which are associated with the optimization algorithm itself.,2.3. Gradient-Based Approach,[0],[0]
"For example, λ may include the step sizes or momentum factors if the dynamics",2.3. Gradient-Based Approach,[0],[0]
Φt in Eq.,2.3. Gradient-Based Approach,[0],[0]
"(6) is gradient descent with momentum; in (Andrychowicz et al., 2016; Wichrowska et al., 2017)",2.3. Gradient-Based Approach,[0],[0]
"the mapping Φt is implemented as a recurrent neural network, while (Finn et al., 2017) focus on the initialization mapping by letting Φ0(λ) =",2.3. Gradient-Based Approach,[0],[0]
"λ.
",2.3. Gradient-Based Approach,[0],[0]
"A major advantage of this reformulation is that it makes it possible to compute efficiently the gradient of fT , which we call hypergradient, either in time or in memory (Maclaurin et al., 2015; Franceschi et al., 2017), by making use of reverse or forward mode algorithmic differentiation (Griewank and Walther, 2008; Baydin et al., 2017).",2.3. Gradient-Based Approach,[0],[0]
"This allows us to optimize a number of hyperparameters of the same order of that of parameters, a situation which arise in ML.",2.3. Gradient-Based Approach,[0],[0]
"In this section, we provide results about the existence of solutions of Problem (1)-(2) and the approximation properties of Procedure (5)-(6) with respect to the original bilevel problem.",3. Exact and Approximate Bilevel Programming,[0],[0]
"Proofs of these results are provided in the supplementary material.
",3. Exact and Approximate Bilevel Programming,[0],[0]
"Procedure (5)-(6), though related to the bilevel problem (1)-(2), may not be, in general, a good approximation of it.",3. Exact and Approximate Bilevel Programming,[0],[0]
"Indeed, making the assumptions (which sound perfectly reasonable) that, for every λ ∈ Λ, wT,λ → wλ for some wλ ∈ arg maxLλ, and that E(·, λ) is continuous, one can only assert that limT→∞ fT (λ) = E(wλ, λ) ≥ f(λ).",3. Exact and Approximate Bilevel Programming,[0],[0]
"This is because the optimization dynamics converge to some minimizer of the inner objective Lλ, but not necessarily to the one that also minimizes the function E. This is illustrated in Figure 2.",3. Exact and Approximate Bilevel Programming,[0],[0]
"The situation is, however, different if the inner problem admits a unique minimizer for every λ ∈",3. Exact and Approximate Bilevel Programming,[0],[0]
Λ.,3. Exact and Approximate Bilevel Programming,[0],[0]
"Indeed in this case, it is possible to show that the set of minimizers of the approximate problems converge, as T → +∞ and in an appropriate sense, to the set of minimizers of the bilevel problem.",3. Exact and Approximate Bilevel Programming,[0],[0]
"More precisely, we make the following assumptions:
(i) Λ is a compact subset of Rm;
(ii) E : Rd × Λ→ R is jointly continuous;
(iii) the map (w, λ) 7→ Lλ(w) is jointly continuous and such that arg minLλ is a singleton for every λ ∈ Λ;
(iv) wλ = arg minLλ remains bounded as λ varies in Λ.
Then, problem (1)-(2) becomes
min λ∈Λ f(λ) = E(wλ, λ), wλ = argminuLλ(u).",3. Exact and Approximate Bilevel Programming,[0],[0]
"(7)
Under the above assumptions, in the following we give results about the existence of solutions of problem (7) and the (variational) convergence of the approximate problems (5)-(6) towards problem (7) — relating the minima as well as the set of minimizers.",3. Exact and Approximate Bilevel Programming,[0],[0]
"In this respect we note that, since both f and fT are nonconvex, argmin fT and argmin f are in general nonsingleton, so an appropriate definition of set convergence is required.
",3. Exact and Approximate Bilevel Programming,[0],[0]
Theorem 3.1 (Existence).,3. Exact and Approximate Bilevel Programming,[0],[0]
Under Assumptions (i)-(iv) problem (7) admits solutions.,3. Exact and Approximate Bilevel Programming,[0],[0]
"Proof See Appendix A.
The result below follows from general facts on the stability of minimizers in optimization problems (Dontchev and Zolezzi, 1993).
",3. Exact and Approximate Bilevel Programming,[0],[0]
Theorem 3.2 (Convergence).,3. Exact and Approximate Bilevel Programming,[0],[0]
"In addition to Assumptions (i)-(iv), suppose that:
(v) E(·, λ) is uniformly Lipschitz continuous; (vi) The iterates (wT,λ)T∈N converge uniformly to wλ on
Λ as T → +∞. Then
(a) inf fT → inf f , (b) argmin fT → argmin f , meaning that, for every
(λT )T∈N such that λT ∈ argmin fT , we have that:
- (λT )T∈N admits a convergent subsequence; - for every subsequence (λKT )T∈N such that λKT → λ̄, we have λ̄ ∈ argmin f .
",3. Exact and Approximate Bilevel Programming,[0],[0]
"Proof See Appendix A.
We stress that assumptions (i)-(vi) are very natural and satisfied by many problems of practical interests.",3. Exact and Approximate Bilevel Programming,[0],[0]
"Thus, the above results provide full theoretical justification to the proposed approximate procedure (5)-(6).",3. Exact and Approximate Bilevel Programming,[0],[0]
"The following remark discusses assumption (vi), while the subsequent example will be relevant to the experiments in Sec. 5.
",3. Exact and Approximate Bilevel Programming,[0],[0]
Algorithm 1.,3. Exact and Approximate Bilevel Programming,[0],[0]
"Reverse-HG for Hyper-representation Input: λ, current values of the hyperparameter, T number of iteration of GD, η ground learning rate, B minibatch of episodes from D Output: Gradient of meta-training error w.r.t.",3. Exact and Approximate Bilevel Programming,[0],[0]
"λ on B for j = 1 to |B| do
wj0",3. Exact and Approximate Bilevel Programming,[0],[0]
"= 0 for t = 1 to T do
wjt ← wt−1",3. Exact and Approximate Bilevel Programming,[0],[0]
"− η∇wLj(w j t−1, λ,D j tr)
αjT",3. Exact and Approximate Bilevel Programming,[0],[0]
"← ∇wLj(w j T , λ,Dval)",3. Exact and Approximate Bilevel Programming,[0],[0]
"pj ← ∇λLj(wjT , λ,Dval) for t = T − 1 downto 0",3. Exact and Approximate Bilevel Programming,[0],[0]
"do
pj ← pj − αjt+1η∇λ∇wLj(w j t , λ,D j tr) αjt ← α j t+1",3. Exact and Approximate Bilevel Programming,[0],[0]
"[ I − η∇w∇wLj(wjt , λ,D j tr) ] return ∑ j p j
Remark 3.3.",3. Exact and Approximate Bilevel Programming,[0],[0]
"If Lλ is strongly convex, then many gradientbased algorithms (e.g., standard and accelerated gradient descent) yield linear convergence of the iterates wT,λ’s.",3. Exact and Approximate Bilevel Programming,[0],[0]
"Moreover, in such cases, the rate of linear convergence is of type (νλ − µλ)/(νλ + µλ), where νλ and µλ are the Lipschitz constant of the gradient and the modulus of strong convexity of Lλ respectively.",3. Exact and Approximate Bilevel Programming,[0],[0]
"So, this rate can be uniformly bounded from above by ρ ∈ ]0, 1[, provided that supλ∈Λ νλ <",3. Exact and Approximate Bilevel Programming,[0],[0]
+∞,3. Exact and Approximate Bilevel Programming,[0],[0]
and infλ∈Λ,3. Exact and Approximate Bilevel Programming,[0],[0]
µλ > 0.,3. Exact and Approximate Bilevel Programming,[0],[0]
"Thus, in these cases wT,λ converges uniformly to wλ on Λ (at a linear rate).
",3. Exact and Approximate Bilevel Programming,[0],[0]
Example 3.4.,3. Exact and Approximate Bilevel Programming,[0],[0]
"Let us consider the following form of the inner objective:
LH(w) = ‖y −XHw‖2",3. Exact and Approximate Bilevel Programming,[0],[0]
"+ ρ‖w‖2, (8)
where ρ > 0 is a fixed regularization parameter and H ∈ Rd×d is the hyperparameter, representing a linear feature map.",3. Exact and Approximate Bilevel Programming,[0],[0]
"LH is strongly convex, with modulus µ = ρ > 0",3. Exact and Approximate Bilevel Programming,[0],[0]
"(independent on the hyperparameter H), and Lipschitz smooth with constant νH = 2‖(XH)>XH+ρI‖, which is bounded from above, if H ranges in a bounded set of square matrices.",3. Exact and Approximate Bilevel Programming,[0],[0]
In this case assumptions (i)-(vi) are satisfied.,3. Exact and Approximate Bilevel Programming,[0],[0]
"In this section, we instantiate the bilevel programming approach for ML outlined in Sec. 2.2 in the case of deep learning where representation layers are shared across episodes.",4. Learning Hyper-Representations,[0],[0]
Finding good data representations is a centerpiece in machine learning.,4. Learning Hyper-Representations,[0],[0]
"Classical approaches (Baxter, 1995; Caruana, 1998) learn both the weights of the representation mapping and those of the ground classifiers jointly on the same data.",4. Learning Hyper-Representations,[0],[0]
"Here we follow the bilevel approach and split each dataset/episode in training and validation sets.
",4. Learning Hyper-Representations,[0],[0]
"Our method involves the learning of a cross-task intermedi-
ate representation hλ :",4. Learning Hyper-Representations,[0],[0]
X → Rk (parametrized by a vector λ) on top of which task specific models gj :,4. Learning Hyper-Representations,[0],[0]
Rk → Yj (parametrized by vectors wj) are trained.,4. Learning Hyper-Representations,[0],[0]
The final ground model for task j is thus given by gj ◦,4. Learning Hyper-Representations,[0],[0]
"h. To find λ, we solve Problem (1)-(2) with inner and outer objectives as in Eqs.",4. Learning Hyper-Representations,[0],[0]
"(3) and (4), respectively.",4. Learning Hyper-Representations,[0],[0]
"Since, in general, this problem cannot be solved exactly, we instantiate the approximation scheme in Eqs.",4. Learning Hyper-Representations,[0],[0]
"(5)-(6) as follows:
min λ fT (λ) = N∑ j=1 Lj(wjT , λ,D j val) (9)
wjt = w j",4. Learning Hyper-Representations,[0],[0]
"t−1−η∇wLj(w j t−1, λ,D j tr), t, j ∈",4. Learning Hyper-Representations,[0],[0]
"[T ], [N ].",4. Learning Hyper-Representations,[0],[0]
"(10)
Starting from an initial value, the weights of the task-specific models are learned by T iterations of gradient descent.",4. Learning Hyper-Representations,[0],[0]
"The gradient of fT can be computed efficiently in time by making use of an extended reverse-hypergradient procedure (Franceschi et al., 2017) which we present in Algorithm 1.",4. Learning Hyper-Representations,[0],[0]
"Since, in general, the number of episodes in a meta-training set is large, we compute a stochastic approximation of the gradient of fT by sampling a mini-batch of episodes.",4. Learning Hyper-Representations,[0],[0]
"At test time, given a new episode D̄, the representation h is kept fixed, and all the examples in D̄ are used to tune the weights w̄ of the episode-specific model ḡ.
Like other initialization and optimization strategies for ML, our method does not require lookups in a support set as the memorization and metric strategies do (Santoro et al., 2016; Vinyals et al., 2016; Mishra et al., 2018).",4. Learning Hyper-Representations,[0],[0]
"Unlike (Andrychowicz et al., 2016; Ravi and Larochelle, 2017) we do not tune the optimization algorithm, which in our case is plain empirical loss minimization by gradient descent, and rather focus on the hypothesis space.",4. Learning Hyper-Representations,[0],[0]
"Unlike (Finn et al., 2017), that aims at maximizing sensitivity of new task losses to the model parameters, we aim at maximizing the generalization to novel examples during training episodes, with respect to λ.",4. Learning Hyper-Representations,[0],[0]
"Our assumptions about the structure of the model are slightly stronger than in (Finn et al., 2017) but still mild, namely that some (hyper)parameters define the representation and the remaining parameters define the classification function.",4. Learning Hyper-Representations,[0],[0]
"In (Munkhdalai and Yu, 2017)",4. Learning Hyper-Representations,[0],[0]
"the meta-knowledge is distributed among fast and slow weights and an external memory; our approach is more direct, since the meta-knowledge is solely distilled by λ.",4. Learning Hyper-Representations,[0],[0]
"A further advantage of our method is that, if the episode-specific models are linear (e.g. logistic regressors) and each loss Lj is strongly convex in w, the theoretical guarantees of Theorem 3.2 apply (see Remark 3.3).",4. Learning Hyper-Representations,[0],[0]
These assumptions are satisfied in the experiments reported in the next section.,4. Learning Hyper-Representations,[0],[0]
The aim of the following experiments is threefold.,5. Experiments,[0],[0]
"First, we investigate the impact of the number of iterations of the optimization dynamics on the quality of the solution on a
simple multiclass classification problem.",5. Experiments,[0],[0]
"Second, we test our hyper-representation method in the context of few-shot learning on two benchmark datasets.",5. Experiments,[0],[0]
"Finally, we constrast the bilevel ML approach against classical approaches to learn shared representations 3.",5. Experiments,[0],[0]
"Motivated by the theoretical findings of Sec. 3, we empirically investigate how solving the inner problem approximately (i.e. using small T ) affects convergence, generalization performances, and running time.",5.1. The Effect of T,[0],[0]
"We focus in particular on the linear feature map described in Example 3.4, which allows us to compare the approximated solution against the closed-form analytical solution given by
wH =",5.1. The Effect of T,[0],[0]
"[(XH) TXH + ρI]−1(XH)TY.
",5.1. The Effect of T,[0],[0]
"In this setting, the bilevel problem reduces to a (non-convex) optimization problem in H .
",5.1. The Effect of T,[0],[0]
"We use a subset of 100 classes extracted from Omniglot dataset (Lake et al., 2017) to construct a HO problem aimed at tuning H .",5.1. The Effect of T,[0],[0]
"A training set Dtr and a validation set Dval, each consisting of three randomly drawn examples per class, were sampled to form the HO problem.",5.1. The Effect of T,[0],[0]
"A third set Dtest, consisting of fifteen examples per class, was used for testing.",5.1. The Effect of T,[0],[0]
"Instead of using raw images as input, we employ feature vectors x ∈ R256 computed by the convolutional network trained on one-shot five-ways ML setting as described in Sec.",5.1. The Effect of T,[0],[0]
"5.2.
",5.1. The Effect of T,[0],[0]
"For the approximate problems we compute the hypergradient using Algorithm 1, where it is intended that B = {(Dtr, Dval)}.",5.1. The Effect of T,[0],[0]
Figure 3 shows the values of functions f and fT (see Eqs.,5.1. The Effect of T,[0],[0]
"(1) and (5), respectively) during the optimization of H .",5.1. The Effect of T,[0],[0]
"As T increases, the solution of the approximate
3The code for reproducing the experiments, based on the package FAR-HO (https://bit.ly/far-ho), is available at https://bit.ly/hyper-repr
problem approaches the true bilevel solution.",5.1. The Effect of T,[0],[0]
"However, performing a small number of gradient descent steps for solving the inner problem acts as implicit regularizer.",5.1. The Effect of T,[0],[0]
"As it is evident from Figure 4, the generalization error is better when T is smaller than the value yielding the best approximation of the inner solution.",5.1. The Effect of T,[0],[0]
"This is to be expected since, in this setting, the dimensions of parameters and hyperparameters are of the same order, leading to a concrete possibility of overfitting the outer objective (validation error).",5.1. The Effect of T,[0],[0]
"An appropriate, problem dependent, choice of T may help avoiding this issue (see also Appendix C).",5.1. The Effect of T,[0],[0]
"As T increases, the number of hyperiterations required to reach the maximum test accuracy decreases, further suggesting that there is an interplay between the number of iterations used to solve the inner and the outer objective.",5.1. The Effect of T,[0],[0]
"Finally, the running time of Algorithm 1, is linear in T and the size of w and independent of the size of H (see also Table 2), making it even more appealing to reduce the number of iterations.",5.1. The Effect of T,[0],[0]
"We now turn our attention to learning-to-learn, precisely to few-shot supervised learning, implementing the ML strategy outlined in Sec. 4 on two different benchmark datasets:
• OMNIGLOT (Lake et al., 2015), a dataset that contains examples of 1623 different handwritten characters from 50 alphabets.",5.2. Few-shot Learning,[0],[0]
"We downsample the images to 28× 28.
• MINIIMAGENET (Vinyals et al., 2016), a subset of ImageNet (Deng et al., 2009), that contains 60000 downsampled images from 100 different classes.
",5.2. Few-shot Learning,[0],[0]
"Following the experimental protocol used in a number of recent works, we build a meta-training set D, from which we sample datasets to solve Problem (9)-(10), a meta-validation
set V for tuning ML hyperparameters, and finally a metatest set T which is used to estimate accuracy.",5.2. Few-shot Learning,[0],[0]
"Operationally, each meta-dataset consists of a pool of samples belonging to different (non-overlapping between separate meta-dataset) classes, which can be combined to form ground classification datasets Dj = Djtr ∪ D j val with 5 or 20 classes (for Omniglot).",5.2. Few-shot Learning,[0],[0]
The Djtr’s contain 1 or 5 examples per class which are used to fit wj (see Eq. 10).,5.2. Few-shot Learning,[0],[0]
"The Djval’s, containing 15 examples per class, is used either to compute fT (λ) (see Eq. (9)) and its (stochastic) gradient if Dj ∈ D or to provide a generalization score if Dj comes from either V or T .",5.2. Few-shot Learning,[0],[0]
"For MiniImagenet we use the same split and images proposed in (Ravi and Larochelle, 2017), while for Omniglot we use the protocol defined by (Santoro et al., 2016).
",5.2. Few-shot Learning,[0],[0]
As ground classifiers we use multinomial logistic regressors and as task losses `j we employ cross-entropy.,5.2. Few-shot Learning,[0],[0]
"The inner problems, being strongly convex, admit unique minimizers, yet require numerical computation of the solutions.",5.2. Few-shot Learning,[0],[0]
"We initialize ground models parameters wj to 0 and, according to the observation in Sec. 5.1, we perform T gradient descent steps, where T is treated as a ML hyperparameter that has to be validated.",5.2. Few-shot Learning,[0],[0]
Figure 6 shows an example of meta-validation of T for one-shot learning on MiniImagenet.,5.2. Few-shot Learning,[0],[0]
"We compute a stochastic approximation of∇fT (λ) with Algorithm 1 and use Adam with decaying learning rate to optimize λ.
",5.2. Few-shot Learning,[0],[0]
"Regarding the specific implementation of the representation mapping h, we employ for Omniglot a four-layers convolutional neural network with strided convolutions and 64 filters per layer as in (Vinyals et al., 2016) and other successive works.",5.2. Few-shot Learning,[0],[0]
"For MiniImagenet we tried two different architectures:
• C4L, a four-layers convolutional neural network with maxpooling and 32 filters per layer;
• RN: a residual network (He et al., 2016) built of four residual blocks followed by two convolutional layers.
",5.2. Few-shot Learning,[0],[0]
"The first network architecture has been proposed in (Ravi and Larochelle, 2017) and then used in (Finn et al., 2017), while a similar residual network architecture has been employed in a more recent work (Mishra et al., 2018).",5.2. Few-shot Learning,[0],[0]
"Further details on the architectures of h, as well as other ML hyperparameters, are specified in the supplementary material.",5.2. Few-shot Learning,[0],[0]
"We report our results, using RN for MiniImagenet, in Table 3, alongside scores from various recently proposed methods for comparison.
",5.2. Few-shot Learning,[0],[0]
"The proposed method achieves competitive results highlighting the relative importance of learning a task independent representation, on the top of which logistic classifiers trained with very few samples generalize well.",5.2. Few-shot Learning,[0],[0]
"Moreover, utilizing more expressive models such as residual network as representation mappings, is beneficial for our proposed strategy and, unlike other methods, does not result in overfitting
of the outer objective, as reported in (Mishra et al., 2018).",5.2. Few-shot Learning,[0],[0]
"Indeed, compared to C4L, RN achieves a relative improvement of 6.5% on one-shot and 4.2% on five-shot.",5.2. Few-shot Learning,[0],[0]
"Figure 5 provides a visual example of the goodness of the learned representation, showing that MiniImagenet examples (the first from meta-training, the second from the meta-testing sets) from similar classes (different dog breeds) are mapped near each other by h and, conversely, samples from dissimilar classes are mapped afar.",5.2. Few-shot Learning,[0],[0]
"In this section, we show the benefits of learning a representation within the proposed bilevel framework compared to other possible approaches that involve an explicit factorization of a classifier as gj ◦",5.3. On Variants of Representation Learning Methods,[0],[0]
h.,5.3. On Variants of Representation Learning Methods,[0],[0]
The representation mapping h is either pretrained or learned with different meta-learning algorithms.,5.3. On Variants of Representation Learning Methods,[0],[0]
We focus on the problem of one-shot learning on MiniImagenet and we use C4L as architecture for the representation mapping.,5.3. On Variants of Representation Learning Methods,[0],[0]
"In all the experiments the ground models gj are multinomial logistic regressor as in Sec. 5.2, tuned with 5 steps of gradient descent.",5.3. On Variants of Representation Learning Methods,[0],[0]
"We ran the following experiments:
• Multiclass: the mapping h : X → R64 is given by the
linear outputs before the softmax operation of a network4 pretrained on the totality of examples contained in the training meta-dataset (600 examples for each of the 64 classes).",5.3. On Variants of Representation Learning Methods,[0],[0]
"In this setting, we found that using the second last layer or the output after the softmax yields worst results;
• Bilevel-train: we use a bilevel approach but, unlike in Sec. 4, we optimize the parameter vector λ of the representation mapping by minimizing the loss on the training sets of each episode.",5.3. On Variants of Representation Learning Methods,[0],[0]
"The hypergradient is still computed with Algorithm 1, albeit we set Djval = D j tr for each training episodes;
• Approx and Approx-train: we consider an approximation of the hypergradient ∇fT",5.3. On Variants of Representation Learning Methods,[0],[0]
(λ) by disregarding the optimization dynamics of the inner objectives (i.e. we set ∇λwjT = 0).,5.3. On Variants of Representation Learning Methods,[0],[0]
"In Approx-train we just use the training sets;
• Classic: as in (Baxter, 1995), we learn h by jointly optimize f̂(λ,w1, . . .",5.3. On Variants of Representation Learning Methods,[0],[0]
", wN ) = ∑N j=1 L
j(wj , λ,Djtr) and treat the problem as standard multitask learning, with the exception that we evaluate f̂ on mini-batches of 4 episodes, randomly sampled every 5 gradient descent iterations.
",5.3. On Variants of Representation Learning Methods,[0],[0]
"In settings where we do not use the validation sets, we let the training sets of each episode contain 16 examples per class.",5.3. On Variants of Representation Learning Methods,[0],[0]
Using training episodes with just one example per class resulted in performances just above random chance.,5.3. On Variants of Representation Learning Methods,[0],[0]
"While the first experiment constitutes a standard baseline, the others have the specific aim of assessing (i) the importance of splitting episodes of meta-training set into training and validation and (ii) the importance of computing the hypergradient of the approximate bilevel problem with Algorithm 1.",5.3. On Variants of Representation Learning Methods,[0],[0]
The results reported in Table 4 suggest that both the training/validation splitting and the full computation of the hypergradient constitute key factors for learning a good representation in a meta-learning context.,5.3. On Variants of Representation Learning Methods,[0],[0]
"On the other side, using pretrained representations, especially in a low-dimensional space, turns out to be a rather effective baseline.",5.3. On Variants of Representation Learning Methods,[0],[0]
"One possible explanation is that, in this context,
4The network is similar to C4L but has 64 filters per layer.
",5.3. On Variants of Representation Learning Methods,[0],[0]
some classes in the training and testing meta-datasets are rather similar (e.g. various dog breeds) and thus ground classifiers can leverage on very specific representations.,5.3. On Variants of Representation Learning Methods,[0],[0]
We have shown that both HO and ML can be formulated in terms of bilevel programming and solved with an iterative approach.,6. Conclusions,[0],[0]
"When the inner problem has a unique solution (e.g. is strongly convex), our theoretical results show that the iterative approach has convergence guarantees, a result that is interesting in its own right.",6. Conclusions,[0],[0]
"In the case of ML, by adapting classical strategies (Baxter, 1995) to the bilevel framework with training/validation splitting, we present a method for learning hyper-representations which is experimentally effective and supported by our theoretical guarantees.
",6. Conclusions,[0],[0]
"Our framework encompasses recently proposed methods for meta-learning, such as learning to optimize, but also suggests different design patterns for the inner learning algorithm which could be interesting to explore in future work.",6. Conclusions,[0],[0]
"The resulting inner problems may not satisfy the assumptions of our convergence analysis, raising the need for further theoretical investigations.",6. Conclusions,[0],[0]
An additional future direction of research is the study of the statistical properties of bilevel strategies where outer objectives are based on the generalization ability of the inner model to new (validation) data.,6. Conclusions,[0],[0]
"Ideas from (Maurer et al., 2016; Denevi et al., 2018) may be useful in this direction.",6. Conclusions,[0],[0]
We introduce a framework based on bilevel programming that unifies gradient-based hyperparameter optimization and meta-learning.,abstractText,[0],[0]
We show that an approximate version of the bilevel problem can be solved by taking into explicit account the optimization dynamics for the inner objective.,abstractText,[0],[0]
"Depending on the specific setting, the outer variables take either the meaning of hyperparameters in a supervised learning problem or parameters of a meta-learner.",abstractText,[0],[0]
We provide sufficient conditions under which solutions of the approximate problem converge to those of the exact problem.,abstractText,[0],[0]
We instantiate our approach for meta-learning in the case of deep learning where representation layers are treated as hyperparameters shared across a set of training episodes.,abstractText,[0],[0]
"In experiments, we confirm our theoretical findings, present encouraging results for few-shot learning and contrast the bilevel approach against classical approaches for learning-to-learn.",abstractText,[0],[0]
Bilevel Programming for Hyperparameter Optimization and Meta-Learning,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2398–2408, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Many model components of competitive statistical machine translation (SMT) systems are based on rather simplistic definitions with little linguistic grounding, which includes the definitions of phrase pairs, lexicalized reordering, and n-gram language models.",1 Introduction,[0],[0]
"However, earlier work has also shown that statistical MT can benefit from additional linguistically motivated models.",1 Introduction,[0],[0]
"Most prominent among the linguistically motivated approaches are syntax-based MT systems which take into account the syntactic structure of sentences through CKY decoding and categorial labels (Zollmann and Venugopal, 2006; Shen et al., 2008).",1 Introduction,[0],[0]
"On the other hand, the commonly used phrase-based SMT approaches can also reap some of the benefits of using syntactic information by
integrating linguistic components addressing specific phenomena, such as Cherry (2008), Carpuat et al. (2010), Crego and Yvon (2010), Ge (2010), Xiang et al. (2011), Lerner and Petrov (2013), Garmash and Monz (2014).
",1 Introduction,[0],[0]
This paper is a contribution to the existing body of work on how syntactically motivated models help translation performance.,1 Introduction,[0],[0]
"We work with the phrase-based SMT (PBSMT) (Koehn et al., 2003) framework as the baseline system.",1 Introduction,[0],[0]
Our choice is motivated by the fact that PBSMT is a conceptually simple and therefore flexible framework.,1 Introduction,[0],[0]
It is typically quite straightforward to integrate an additional model into the system.,1 Introduction,[0],[0]
"Also, PBSMT is the most widely used framework in the SMT research community, which ensures comparability of our results to other people’s work on the topic.
",1 Introduction,[0],[0]
There is a variety of ways syntax can be used in a PBSMT model.,1 Introduction,[0],[0]
Typically a syntactic representation of a source sentence is used to define constraints on the order in which the decoder translates it.,1 Introduction,[0],[0]
"For example, Cherry (2008) defines soft constraints based on the notion of syntactic cohesion (Section 2).",1 Introduction,[0],[0]
Ge (2010) captures reordering patterns by defining soft constraints based on the currently translated word’s POS tag and the words structurally related to it.,1 Introduction,[0],[0]
"On the other hand, target syntax is more challenging to use in PBSMT, since a target-side syntactic model does not have access to the whole target sentence at decoding.",1 Introduction,[0],[0]
"Post and Gildea (2008) is one of the few targetside syntactic approaches applicable to PBSMT, but it has been shown not to improve translation.",1 Introduction,[0],[0]
Their approach uses a target side parser as a language model: one of the reasons why it fails is that a parser assumes its input to be grammatical and chooses the most likely parse for it.,1 Introduction,[0],[0]
"What we are interested in during translation is how gram-
2398
matical the target sentence actually is.",1 Introduction,[0],[0]
"In addition to reordering constraints, source syntax can be used for target-side language modeling.",1 Introduction,[0],[0]
A target side string can be encoded with source-syntactic building blocks and then scored as to how well-formed it is.,1 Introduction,[0],[0]
"Crego and Yvon (2010), Niehues et al. (2011), Garmash and Monz (2014) model target sequences as strings of tokens built from the target POS tag and the POS tags of the source words related to it through alignment and the source parse.",1 Introduction,[0],[0]
"In this paper, we define a target-side syntactic language model that takes structural constraints from the source sentence, but uses the words from the target side (as ‘building blocks’).",1 Introduction,[0],[0]
"We do it by adapting an existing monolingual model of Chelba and Jelinek (2000), structured language models, to the bilingual setting.",1 Introduction,[0],[0]
"Our contributions can be summarized as follows:
• we propose a novel method to adapt monolingual structured language models (Chelba and Jelinek, 2000)",1 Introduction,[0],[0]
"(Section 3) to a PBSMT system (Section 4), which does not require an external on-the-fly parser, but only uses the given source-side syntactic analysis to infer structural relations between target words;
• building on the existing literature, we propose a set of deterministic rules that incrementally build up a parse of a target translation hypothesis based on the source parse (Section 4);
• we evaluate our models in a series of rescoring experiments and achieve statistically significant improvements of up to 0.7 BLEU for Chinese-English (Section 5).
",1 Introduction,[0],[0]
"Before describing the models, we motivate our method with a common assumption about crosslingual correspondence (Section 2).",1 Introduction,[0],[0]
"Before we apply the syntactic model introduced in Section 3 to the bilingual setting (Section 4), we first explain two widely used assumptions about syntactic correspondence across languages.
",2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
We take a dependency tree to be a syntactic representation of a sentence and reason about other syntactic assumptions and models in its terms.,2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
"In this work, we choose a dependency structure over a constituency structure because the former
is more primitive.1 A dependency parse D is a dependency tree analysis of a sentence W , and we will think of it as a relation between words of W , such that D(w, v) if w is a parent (head) of v (v being a child/modifier).",2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
"D can be generalized to D∗ which is an relation between words that are connected by a continuous path in a dependency tree (i.e. D∗(w, v) if D(w, v) or if ∃u s.t. D(w, u) ∧ D∗(u, v)).",2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
We assume unlabeled dependency trees.,2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
"Finally, we make a projectivity assumption, which is supported by empirical data in many languages (Kuhlmann and Nivre, 2006; Havelka, 2007), and makes a model computationally less expensive.",2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
"A dependency parse D of a sentence W = w1, . . .",2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
", wn is projective, if for every word pair wi, wj ∈ W s.t",2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
". D(wi, wj) it holds that every wk ∈ W s.t.",2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
i <,2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
k < j or j < k,2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
"< i is a descendant of wi, i.e., D∗(wi, wk); see Figure 1.
",2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
"Most NLP models that address the interaction of two or more languages are based (explicitly or implicitly) on the direct correspondence assumption (DCA) (Hwa et al., 2002).",2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
It states that close translation equivalents in different languages have the same dependency structure.,2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
"This is grounded linguistically, as translation equivalence implies semantic equivalence and therefore thematic relations are preserved (Hwa et al., 2002).",2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
"Thus dependency relations are preserved, as they are defined based on thematic relations between words.",2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
"On the other hand, there is plenty empirical evidence supporting the violation of DCA under certain conditions (Hwa et al., 2002).",2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
"For instance, even semantically very close sentences in different languages may have a different number of
1A dependency parse (a dependency tree analysis of a sentence) is more primitive because every constituency parse can be formalized as a projective dependency parse with labeled relations, but not vice versa (Osborne, 2008).
words.",2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
"Syntactic divergence increases if the two languages are typologically different.
",2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
"Even though DCA only holds up to a certain level of precision, it is widely used in NLP.",2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
"There are models of cross-lingual transfer that define syntactic structure of one language by conditioning it on the structure of semantically equivalent sentences in another language (Naseem et al., 2012).",2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
DCA has also been used in SMT.,2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
"In particular, syntax-based SMT is built implicitly around this assumption (Wu, 1997; Yamada and Knight, 2001).",2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
"In Quirk and Menezes (2006) DCA is explicitly implemented by defining a translation model in terms of treelet pairs where target-side treelets are produced by projecting source dependencies via word alignments.
",2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
"Closely related to DCA is the notion of syntactic cohesion of translation (Fox, 2002; Cherry, 2008).",2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
This is a constraint that does not allow for non-projective reordering:,2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
"Given a source parse DS , a translation W is cohesive if all translated target words wi, wj do not have any word wk between them such that there is a source subtree sub in DS such that some parts of it are translated by wi andwj but not bywk (Figure 2).",2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
Cherry (2008) and Bach et al. (2009) define a set of soft constraints based on the syntactic cohesion assumption which are applicable to PBSMT decoding.,2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
"They only require phrase applications, and not necessarily individual target words, to conform to the cohesion principle.",2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
"For example, if we imagine a situation where a subtree as in Figure 2(b) is translated as a whole with one phrase application (and not word by word), then it does not violate the cohesion principle, although it is internally
uncohesive.",2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
"Both our approach and Cherry (2008) implement the idea of conforming the target translation to the source syntactic structure, but in different ways.",2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
Approaches like Cherry (2008) define principles that constrain the decoder in order to produce better translations.,2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
Our goal is to have a model that allows for a more direct way of evaluation of how well-formed the target translation is.,2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
In Section 5 we compare translation performance of the two approaches.,2 Direct correspondence assumption and syntactic cohesion in SMT,[0],[0]
"As discussed in Sections 1 and 2, we would like to test how much a PBSMT can benefit from an additional syntax-based LM.",3 Structured language models,[0],[0]
"In this section, we describe a syntactic language model, structured LM (SLM) (Chelba and Jelinek, 2000), that we extend to a bilingual setting and apply to SMT in Section 4.",3 Structured language models,[0],[0]
"SLMs have been applied in SMT before (Yamada and Knight, 2001; Yu et al., 2014), but as we show in Section 4, we provide a much simpler method to integrate it into the system.",3 Structured language models,[0],[0]
"While a SLM is not the only syntactically defined LM, it is one of the few that models sentence generation sequentially.",3 Structured language models,[0],[0]
"And due to the way the decoding procedure of PBSMT is defined, it is natural and straightforward to use models whose score can be computed sequentially.",3 Structured language models,[0],[0]
"Other syntactic language models define sentence generation hierarchically (Shen et al., 2008; Sennrich, 2015), which complicates their integration into a PBSMT system.
",3 Structured language models,[0],[0]
The linguistic intuition behind SLMs is that the structural children of a word do not essentially change its distributional properties but just provide additional specification.,3 Structured language models,[0],[0]
In Figure 3(a),3 Structured language models,[0],[0]
the word president has two modifiers: the and former and it follows yesterday (an adjunct) and precedes met (a predicate).,3 Structured language models,[0],[0]
This ordering is correct in English.,3 Structured language models,[0],[0]
"If instead its modifier was a or an entire relative clause, it would not make it incorrect.
",3 Structured language models,[0],[0]
"To capture this observation, (Chelba and Jelinek, 2000) propose a language model where each word wi of a sentence W is predicted by an ordered subset of the words preceding wi.",3 Structured language models,[0],[0]
This conditioning subset is selected based on the syntactic properties of the preceding sequence Wi−1: the strong predictors are kept and the weak ones are left out.,3 Structured language models,[0],[0]
The strong predictors are the set of exposed heads.,3 Structured language models,[0],[0]
"Given a subsequence Wi−1 and its associated parseDi−1, exposed heads are the roots of all the disconnected subtrees inDi−1.",3 Structured language models,[0],[0]
"Note that
a parseDi−1 is not necessarily fully connected and thus a word can have multiple conditioning words.
",3 Structured language models,[0],[0]
"For an example, consider again Figure 3(a).",3 Structured language models,[0],[0]
"In a left-to-right scenario, when met is generated, a regular n-gram LM conditions it on yesterday the former president, while a SLM conditions it on yesterday president, since these two words are the exposed heads with respect to met (Figure 3(b)).",3 Structured language models,[0],[0]
The words the and former are modifiers of president and they get filtered out.,3 Structured language models,[0],[0]
"Thus we obtain a less specific conditioning history, which may lead to the resulting model being less sparse.",3 Structured language models,[0],[0]
"Another potential benefit is that SLMs can capture longdistance reordering: If president had as its modifier a relative clause (Figure 3(c)) then a simple n-gram LM would be conditioned on days before (assuming n = 3), while an SLM would condition met on yesterday president.
",3 Structured language models,[0],[0]
"Summarizing the ideas of words being conditioned on a structurally defined subset of the preceding sentence, Chelba and Jelinek (2000) formalize the generation process of W as follows:2 Each new word wi is conditioned on a
2The original model by (Chelba and Jelinek, 2000) is defined in terms of a lexicalized constituency grammar, but as
sequence of exposed heads Expos(W,D).",3 Structured language models,[0],[0]
"Then a tag ti is predicted, and the parse Di−i of Wi−1 is extended to Di incorporating wi and ti (where Wi−1 is the prefix of W preceding wi): p(W,D) = |W |∏ i=1",3 Structured language models,[0],[0]
"p(wi|Expos(Wi−1, Di−1))
· p(ti|wi, Expos(Wi−1, Di−1)) ·",3 Structured language models,[0],[0]
"p(Di|wi, ti, Expos(Wi−1, Di−1)).
",3 Structured language models,[0],[0]
"(1)
They use a shift-reduce parser with reduce-left, reduce-right, and shift operations.",3 Structured language models,[0],[0]
"In this section, we combine the direct correspondence assumption (Section 2) and SLMs (Section 3), and define bilingual structured language models (BiSLMs) for PBSMT.",4 Bilingual structured language models,[0],[0]
Structured LMs have been successfully applied in SMT before.,4 Bilingual structured language models,[0],[0]
"Yamada and Knight (2001) use SLMs in a stringto-tree SMT system where a derivation of a targetside parse tree is part of the decoding algorithm, and target syntactic representations are obtained ‘for free’.",4 Bilingual structured language models,[0],[0]
"Yu et al. (2014) use an on-the-fly shiftreduce parser to build an incremental target parse.
",4 Bilingual structured language models,[0],[0]
The approaches sketched above rely on resources that a standard PBSMT system does not have access to by default.,4 Bilingual structured language models,[0],[0]
"Phrase-based decoders do not provide us with a parse of the target sentence, and inferring the parse of a target string with an external parser is computationally expensive and potentially unreliable (see Section 1).",4 Bilingual structured language models,[0],[0]
Our main insight is that in a bilingual setting one does not need an additional probabilistic target parsing model.,4 Bilingual structured language models,[0],[0]
"We assume that the source parse is given (precomputed) and that the DCA (Section 2) holds, and project the parse deterministically onto the target side via word alignments3.",4 Bilingual structured language models,[0],[0]
"We obtain the following equation:
p(T |S,DS) = |T |∏ i=1",4 Bilingual structured language models,[0],[0]
"p(ti| Expos(Ti−1,
ProjP(DS , S, Ti−1)))",4 Bilingual structured language models,[0],[0]
",
(2)
where T is a target sentence, Ti−1 is the sequence in T preceding the i-th target word ti, S is a
we discussed in Section 2, constituency parses can be transformed into dependency parses.
",4 Bilingual structured language models,[0],[0]
"3Phrase-internal word alignments are stored in the phrase table and are available at decoding time, see Section 4.4.
source sentence,DS is a source dependency parse, and ProjP is a function that returns a partial target parse DT i−1 by projecting DS onto Ti−1.",4 Bilingual structured language models,[0],[0]
"In words, at each time step iwe predict the next word ti conditioned on the exposed heads of the partial parse of Ti−1 projected from the source side.",4 Bilingual structured language models,[0],[0]
"We limit Expos to returning the four preceding exposed heads.4 Because the function ProjP is deterministic and because we do not have to predict tags for words, Equation 2 is simpler than Equation 1.
",4 Bilingual structured language models,[0],[0]
We first illustrate Equation 2 with an example in Figure 4.,4 Bilingual structured language models,[0],[0]
"Since word alignment is monotonic in Figure 4(a), it is straightforward to project the source dependencies onto the target side.",4 Bilingual structured language models,[0],[0]
"We aim to imitate a monolingual parser in the way we build up our projected parse: Reduce operations should be invoked whenever both of the subtrees involved in the operation are complete, i.e., are not expected to have any more modifiers (Section 4.2).",4 Bilingual structured language models,[0],[0]
"For example, when the target word likes is produced its exposed heads are said and he (Figure 4(b)), since Putin is a modifier of said.",4 Bilingual structured language models,[0],[0]
"Likewise, the exposed heads for women are said likes all Russian (Figure 4(c)).
",4 Bilingual structured language models,[0],[0]
"In what follows we discuss how to define ProjP. Compared to projection approaches like (Quirk
4As written above, we choose the dependency structures over the lexicalized constituency ones because the latter can be mapped to the former.",4 Bilingual structured language models,[0],[0]
"It is thus more likely that a projected dependency tree is still be a well-formed parse, than a projected constituency tree.",4 Bilingual structured language models,[0],[0]
"We decided to work with structural models that are more flexible, but one may also define BiSLM in terms of the more constraining constituency trees and see if the such model has better generalization power.
and Menezes, 2006), we would like our model to project a source parse incrementally, allowing it to be used in a PBSMT decoder.",4 Bilingual structured language models,[0],[0]
"We think of ProjP as a function that computes the output in two stages: first, it infers from the source parse the dependency relations between target words (Section 4.1), second, it decides how to parse the target sequence, i.e. in which order to assign these dependencies (Section 4.2).",4 Bilingual structured language models,[0],[0]
"Additionally, in Section 4.3 we propose to use additional labelings of target words, and in Section 4.4 we describe some important implementation details.",4 Bilingual structured language models,[0],[0]
Adoption of DCA (Section 2) allows to build up a target dependency tree from a source tree by projecting the latter through word alignments.,4.1 Dependency graph projection,[0],[0]
"The definition of DCA can be rephrased as requiring a one-to-one correspondence map between words of a sentence pair, allowing one to unambiguously map dependencies:",4.1 Dependency graph projection,[0],[0]
"Given a source parse, if t1 is the head of t2, then map(t1) is the head of map(t2).",4.1 Dependency graph projection,[0],[0]
"The correspondence relation that we have in PBSMT is the word alignment align: in the most general case, it is a many-to-many correspondence, and the straightforward projection described above can lead to incorrect dependency structures.",4.1 Dependency graph projection,[0],[0]
"To overcome these problems, we describe a simple ordered set of projection rules, based on the ones specified by (Quirk and Menezes, 2006) (and we point out if otherwise).
",4.1 Dependency graph projection,[0],[0]
The general idea behind this set of rules is to extract a one-to-one function align1−1 from source words to target words from align and use it to project source dependencies as described in the paragraph above (R1 below).,4.1 Dependency graph projection,[0],[0]
We then use additional rules (R2-R4 below) for the target words that are not in align1−1.,4.1 Dependency graph projection,[0],[0]
"Given a source sentence S with a parse DS , a target sentence T and word alignment align, align1−1 is extracted as follows:",4.1 Dependency graph projection,[0],[0]
"For all ti ∈ T with multiple aligned source words {si1 , si2 , ...} only align1−1(si1) = ti (only leftmost source word is kept, the links from the rest of the source words are removed5).",4.1 Dependency graph projection,[0],[0]
"For all si ∈ S with aligned target words {ti1 , ti2 , ...} keep the link only for the leftmost aligned target word: align1−1(si) = ti1 .",4.1 Dependency graph projection,[0],[0]
"For example, in Figure 5(b)",4.1 Dependency graph projection,[0],[0]
"the link between f0 and e1 is not in align1−1, and in Figure 5(c) the link between f1 and e0 is removed (and the arc from f2 to f1 is not projected).
",4.1 Dependency graph projection,[0],[0]
"5This is an ad-hoc solution, other heuristics could be used.
",4.1 Dependency graph projection,[0],[0]
The following rules should be applied in order (as else-if conditions).,4.1 Dependency graph projection,[0],[0]
"Given a source sentence S with a parse DS , a target sentence T and word alignment align between them, ti ∈ T is a head of tj ∈ T (i.e. DT (ti, tj)): (R1) if there are sk, sl ∈ S s.t.",4.1 Dependency graph projection,[0],[0]
"DS(sk, sl) and align1−1(sk) = ti and align1−1(sl) = tj ; see Figures 5(a)-5(c); (R2) if ∃s ∈ S s.t.",4.1 Dependency graph projection,[0],[0]
"align1−1(s) = ti and (s, tj) ∈ align.",4.1 Dependency graph projection,[0],[0]
"This rule deals with one-to-many alignments; see Figure 5(d); (R3a) if ∃sk s.t. align1−1(sk) = ti and ∃sl s.t. (sl, tj) ∈ align and and DS(sl, sk), and ti linearly precedes tj .",4.1 Dependency graph projection,[0],[0]
"In words: if two target words are in align1−1 but do not get connected via R1, find a source word aligned to the second target word that may get them connected; see Figure 5(e); (R3b) same as R3a, but in case tj precedes ti (i.e., find an additional source word aligned to the first target word; see Figure 5(f)).6 (R4)",4.1 Dependency graph projection,[0],[0]
"In case ¬∃s (s, tj) ∈ align (tj is unaligned), we consider two strategies: We simplify the rule of Quirk and Menezes (2006) (dealing with the same situation) by adjoining it to the immediately preceding head.",4.1 Dependency graph projection,[0],[0]
"We also consider a strategy whereby the word remains unconnected to any word in the sentence; see Figure 5(g).
6R3a and R3b differ from the rules proposed in Quirk and Menezes (2006) dealing with the same situation, since we had to adapt it to the left-to-right parsing scenario.",4.1 Dependency graph projection,[0],[0]
"Given an inference procedure for dependency relations between target words (Section 4.1), one can specify in which order the corresponding dependency arcs are assigned to the target sentence.",4.2 BiSLM parsing procedure,[0],[0]
"We define an incremental parsing procedure in terms of three operations: shift, left-reduce, and right-reduce.",4.2 BiSLM parsing procedure,[0],[0]
The operations are applied as soon as the sufficient conditions hold: We specify the conditions using the following structural properties.,4.2 BiSLM parsing procedure,[0],[0]
A target subtree is source-complete if all the descendants of align−11−1(root(sub)),4.2 BiSLM parsing procedure,[0],[0]
(source correspondent of the root of the current subtree) (Section 4.1) have been translated and reduced.,4.2 BiSLM parsing procedure,[0],[0]
A target subtree is complete if it is source-complete and all the target words that are its children through non-projected arcs (through R2 or R4 in Section 4.1) have been translated and reduced.,4.2 BiSLM parsing procedure,[0],[0]
The bilingual parsing operations and the sufficient conditions for them are defined as follows:,4.2 BiSLM parsing procedure,[0],[0]
Shift: after the word is produced it is shifted onto the stack as an elementary subtree.,4.2 BiSLM parsing procedure,[0],[0]
"Left-reduce: if a disconnected subtree subi and a disconnected subtree subi−1 immediately preceding it are both complete and DT (root(subi), root(subi−1)), adjoin subi−1 to subi so that root(subi−1) is a modifier of root(subi).",4.2 BiSLM parsing procedure,[0],[0]
"Right-reduce: analogous to left-reduce, but DT (root(subi−1), root(subi)).
",4.2 BiSLM parsing procedure,[0],[0]
In the case of non-cohesive translation the resulting target dependencies are non-projective.,4.2 BiSLM parsing procedure,[0],[0]
Our definition of left- and right-reduce only produces projective parses.,4.2 BiSLM parsing procedure,[0],[0]
"For a non-cohesive translation, certain subtrees will never be sourcecomplete and will never be reduced; see Figure 6(a).",4.2 BiSLM parsing procedure,[0],[0]
"Note that this is not a disadvantage
of our model.",4.2 BiSLM parsing procedure,[0],[0]
"Cherry (2008) simply assumes that non-cohesive reordering should be penalized, and our model is able to learn this pattern.",4.2 BiSLM parsing procedure,[0],[0]
"We also consider an alternative to incorporating noncohesive alignments by relaxing the definition of completeness for subtrees: A projected subtree sub is weakly source-complete if all descendants of all source word(s) which are aligned to the root of sub have been translated and, only if the definition of reduce applies, reduced; see Figure 6(b).",4.2 BiSLM parsing procedure,[0],[0]
"One of the problems with SLMs in general is that at time steps i and j the sets of exposed heads for ti and tj can differ in size, which may imply different predictive power.",4.3 Syntactic labeling of tokens,[0],[0]
"To this end, we add an additional detail to our model: Each time a reduction occurs, we label the root of the subtree to which another subtree has been adjoined, thus making the conditioning history more specific.",4.3 Syntactic labeling of tokens,[0],[0]
"We use the following labelings: Reduction labeling: if a subtree is adjoint to sub from the left, then label root(sub) with LR.",4.3 Syntactic labeling of tokens,[0],[0]
"If it is adjoint from the right, then label it with RR.",4.3 Syntactic labeling of tokens,[0],[0]
"Reduction POS-labeling: same as in simple reduction labeling, but add the POS tag of the root of the reduced subtree to the label.",4.3 Syntactic labeling of tokens,[0],[0]
"To use BiSLM during decoding, one needs access to phrase-internal alignments and target POS tags.",4.4 Implementation and training,[0],[0]
"We store phrase-internal alignments and targetside POS annotations of each phrase in the phrase table, based on the most frequent internal alignment during training and the most likely targetside POS labeling t̂ given the phrase pair: t̂ = arg maxt̄ p(t̄|ē, f̄).",4.4 Implementation and training,[0],[0]
"We train BiSLMs on the parallel training data (Section 5.1) and use the Stanford dependency parser (Chang et al., 2009) for Chinese and and the Stanford constituency parser (Green and Manning, 2010) for Arabic7.",4.4 Implementation and training,[0],[0]
"POStagging of the training data is produced with the Stanford POS-tagger (Toutanova et al., 2003).",4.4 Implementation and training,[0],[0]
"We learn a 5-gram model using SRILM (Stolcke et al., 2011) with modified Kneser-Ney smoothing.",4.4 Implementation and training,[0],[0]
"To evaluate the effectiveness of BiSLMs for PBSMT, we performed rescoring experiments for
7We extract dependency parses from its output based on Collins (1999)
Arabic-English and Chinese-English.",5 Experiments,[0],[0]
We compare the resulting 1-best translation lists with an output of the baseline system and the baseline augmented with soft cohesion constraints from Bach et al. (2009).,5 Experiments,[0],[0]
This section provides information about our baseline system.,5.1 Experimental setup,[0],[0]
Word-alignment is produced with GIZA++,5.1 Experimental setup,[0],[0]
"(Och and Ney, 2003).",5.1 Experimental setup,[0],[0]
"We use an inhouse implementation of a PBSMT system similar to Moses (Koehn et al., 2007).",5.1 Experimental setup,[0],[0]
"Our baseline has all standard PBSMT features including language model, lexical weighting, and lexicalized reordering.",5.1 Experimental setup,[0],[0]
The distortion limit is set to 5.,5.1 Experimental setup,[0],[0]
A 5-gram LM is trained on the English Gigaword corpus (1.6B tokens) using SRILM with modified Kneser-Ney smoothing and linear interpolation.,5.1 Experimental setup,[0],[0]
"Information about the training data for the Arabic-English and Chinese-English systems is in Table 3.8 Feature weights are tuned using pairwise ranking optimization (Hopkins and May, 2011) on the MT04 benchmark (for both language pairs).",5.1 Experimental setup,[0],[0]
"For testing, we use MT08 and MT09 for Arabic, and MT06 and MT08 for Chinese.",5.1 Experimental setup,[0],[0]
"We use case-insensitive BLEU (Papineni et al., 2002) as evaluation metric.",5.1 Experimental setup,[0],[0]
"Approximate randomization (Noreen, 1989; Riezler and Maxwell, 2005) is used to detect statistically significant differences.",5.1 Experimental setup,[0],[0]
"As a comparison model, we implemented six features from Cherry (2008) and Bach et al. (2009)9 and added them to the log-linear interpolation used 8The standard LDC corpora were used for training.",5.2 Baseline and comparison systems,[0],[0]
"9Exhaustive and non-exhaustive interruption check, exhaustive and non-exhaustive interruption count, verb- and noun-dominated subtree interruption count.
by the baseline system.",5.2 Baseline and comparison systems,[0],[0]
"Since these features are binary or count-based, we cannot use them directly in rescoring.",5.2 Baseline and comparison systems,[0],[0]
For that reason we integrated the features into the decoder and tuned the corresponding weights.,5.2 Baseline and comparison systems,[0],[0]
"The results for Chinese-English and Arabic-English translation experiments are presented in Table 1 and 2, respectively.",5.2 Baseline and comparison systems,[0],[0]
We see that adding the cohesion constraints does not improve performance.,5.2 Baseline and comparison systems,[0],[0]
"This finding is different from, for example, Feng et al. (2010), where they get improvement for Chinese-English: however, we note that their training set is smaller than ours, and their baseline is weaker as it does not contain lexicalized distortion models.",5.2 Baseline and comparison systems,[0],[0]
Rescoring with BiSLMs is performed as follows: For the test runs of the baseline system we compute the n = 1000 best translation hypotheses for each source sentence and extract their derivations (sequence of phrase pair applications).,5.3 Rescoring experiments,[0],[0]
Each phrase pair in our implementation is associated with a unique phrase-internal alignment and target POS-sequence.,5.3 Rescoring experiments,[0],[0]
We fully reconstruct wordalignment for each pair of a source sentence and its translation hypothesis.,5.3 Rescoring experiments,[0],[0]
We project a precomputed source parse onto the target side and compute representations of the target sentence to be computed by a BiSLM.,5.3 Rescoring experiments,[0],[0]
"For each hypothesis, we take its BiSLM score and its score assigned by the baseline system and compute the final score as a weighted sum of the original baseline score and a length-normalized BiSLM score10, where the weight λ is empirically set to 0.3:
λ · scoreBiSLM lengthHypothesis + (1− λ) · scoreBaseline (3)",5.3 Rescoring experiments,[0],[0]
"Our main focus here is Chinese-English, since it has more instances of longer-distance reordering, at which syntax-based models are typically good.
",5.3.1 Chinese-English,[0],[0]
"10Normalization is needed to ensure comparability of scores for translation hypotheses of different lengths, since longer translation hypotheses will have lower scores.
",5.3.1 Chinese-English,[0],[0]
SLMs by design are good at capturing longerdistance dependencies.,5.3.1 Chinese-English,[0],[0]
We try out several variations of BiSLM.,5.3.1 Chinese-English,[0],[0]
"First, we test whether to use a strong or weak definition of a complete subtree (Section 4.2).",5.3.1 Chinese-English,[0],[0]
"Second, we investigate whether to adjoin unaligned target words to a preceding head (Section 4.1; unalign-adjoin+/-).",5.3.1 Chinese-English,[0],[0]
"Third, we compare several target-side labeling methods (Section 4.3): plain (just target words), reduce (LR or RR) or reduce-POS (LR POS or RR POS, where POS is the tag of the root of the reduced subtree).",5.3.1 Chinese-English,[0],[0]
"The rescoring results are presented in Table 4.
",5.3.1 Chinese-English,[0],[0]
The results show statistically significant improvement over the baseline of up to 0.7 BLEU (for all of the employed BiSLM variants except one).,5.3.1 Chinese-English,[0],[0]
The rescoring experiments also demonstrate the tendency of the unalign-adjoin- feature value to produce higher scores than unalign-adjoin+.,5.3.1 Chinese-English,[0],[0]
But the other two distinguishing features do not have an effect on BLEU scores.,5.3.1 Chinese-English,[0],[0]
"As future work, we are interested in examining if these features produce the same distribution of scores when a BiSLM is fully integrated into the decoder.",5.3.1 Chinese-English,[0],[0]
We also rescore the n-best lists for the output of the Arabic-English baseline system and results are shown in Table 5.,5.3.2 Arabic-English,[0],[0]
"Arabic and English are typologically very different, but the range of reordering is much smaller than for Chinese-English.",5.3.2 Arabic-English,[0],[0]
"We expect reordering-related models to have lesser effect on Arabic as compared to Chinese (Carpuat et al., 2010).",5.3.2 Arabic-English,[0],[0]
Experimental results on ArabicEnglish could indicate what kind of translation aspect benefits from BiSLMs.,5.3.2 Arabic-English,[0],[0]
"We see that for Arabic-English, just as for the cohesion constraint, BiSLM have little effect on BLEU scores, or even decrease them.",5.3.2 Arabic-English,[0],[0]
This is a weak indication that BiSLMs are better at capturing reordering aspects.,5.3.2 Arabic-English,[0],[0]
"As for the varying features defining different BiSLM versions, we again see little effect of the labeling type or subtree completeness definition.",5.3.2 Arabic-English,[0],[0]
"On the other hand, we see the opposite pattern for the unalign-adjoin feature, where unalign-adjoin+ is preferred.
",5.3.2 Arabic-English,[0],[0]
"To gain further insight into the different effect of BiSLM on the two language pairs, we evaluated our experimental output against a reorderingsensitive metric LRscore (Birch et al., 2010).",5.3.2 Arabic-English,[0],[0]
We use the version of LRscore which is an average of the inverse Kendall’s Tau distance and the Hamming distance.,5.3.2 Arabic-English,[0],[0]
In order to compute alignments for test sets which are needed to compute the score we concatenated the parallel text with an additional 250K lines of parallel text from the training data to ensure better generalization of the alignment algorithm (GIZA++).,5.3.2 Arabic-English,[0],[0]
"The LRscores of the baseline are compared to the best performing BiSLM system with respect to BLEU, for each of the language pair.",5.3.2 Arabic-English,[0],[0]
"The results are provided in Tables 6 and 7.
",5.3.2 Arabic-English,[0],[0]
"As expected, the scores for Chinese-English are much lower than for Arabic-English, which is consistent with the observation reordering is more difficult for Chinese-English.",5.3.2 Arabic-English,[0],[0]
BiSLM yields larger improvements for Chinese-English suggesting that the proposed model helps addressing difficult reordering problems.,5.3.2 Arabic-English,[0],[0]
While there are also small improvements for Arabic-English the they may be too small to be detectable by BLEU.,5.3.2 Arabic-English,[0],[0]
In this paper we proposed a novel way to adapt structured language models to phrase-based SMT.,6 Conclusions,[0],[0]
Our method requires minimal changes to the PBSMT pipeline.,6 Conclusions,[0],[0]
"We tried a number of variations of our model and evaluated them in rescoring experiments, resulting in statistically significant improvement for Chinese-English.",6 Conclusions,[0],[0]
The model is based on the idea of syntactic transfer (DCA; Section 2) and the positive result indicates its ability to capture syntactic patterns across languages.,6 Conclusions,[0],[0]
"For Arabic-English, we did not observe any improvements, suggesting that our models indeed mainly improve reordering aspects.",6 Conclusions,[0],[0]
Improvements in rescoring are a positive indication that our model may be a strong feature during decoding.,6 Conclusions,[0],[0]
"As future work, we will fully integrate our model into a PBSMT decoder and evaluate it on other language pairs with different reordering distributions.",6 Conclusions,[0],[0]
We thank the reviewers for their useful comments.,Acknowledgments,[0],[0]
This research was funded in part by the Netherlands Organization for Scientific Research (NWO) under project numbers 639.022.213 and 612.001.218.,Acknowledgments,[0],[0]
"This paper describes a novel target-side syntactic language model for phrase-based statistical machine translation, bilingual structured language model.",abstractText,[0],[0]
"Our approach represents a new way to adapt structured language models (Chelba and Jelinek, 2000) to statistical machine translation, and a first attempt to adapt them to phrasebased statistical machine translation.",abstractText,[0],[0]
We propose a number of variations of the bilingual structured language model and evaluate them in a series of rescoring experiments.,abstractText,[0],[0]
"Rescoring of 1000-best translation lists produces statistically significant improvements of up to 0.7 BLEU over a strong baseline for Chinese-English, but does not yield improvements for ArabicEnglish.",abstractText,[0],[0]
Bilingual Structured Language Models for Statistical Machine Translation,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2306–2312, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"Discovering the discourse relation between two sentences is crucial to understanding the meaning of a coherent text, and also beneficial to many downstream NLP applications, such as question answering and machine translation.",1 Introduction,[0],[0]
Implicit discourse relation recognition (DRRimp) remains a challenging task due to the absence of strong surface clues like discourse connectives (e.g. but).,1 Introduction,[0],[0]
"Most work resorts to large amounts of manually designed features (Soricut and Marcu, 2003; Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Rutherford and Xue, 2014), or distributed features learned via neural network models (Braud and Denis, 2015; Zhang et al., 2015; Ji and Eisenstein, 2015).",1 Introduction,[0],[0]
"The above methods usually suffer from limited labeled data.
",1 Introduction,[0],[0]
"Marcu and Echihabi (2002) attempt to create labeled implicit data automatically by removing connectives from explicit instances, as additional training data.",1 Introduction,[0],[0]
"These data are usually called as syn-
∗Corresponding author.
",1 Introduction,[0],[0]
thetic implicit data (hereafter SynData).,1 Introduction,[0],[0]
"However, Sporleder and Lascarides (2008) argue that SynData has two drawbacks: 1) meaning shifts in some cases when removing connectives, and 2) a different word distribution with the real implicit data.",1 Introduction,[0],[0]
They also show that using SynData directly degrades the performance.,1 Introduction,[0],[0]
"Recent work seeks to derive valuable information from SynData while filtering noise, via domain adaptation (Braud and Denis, 2014; Ji et al., 2015), classifying connectives (Rutherford and Xue, 2015) or multi-task learning (Lan et al., 2013; Liu et al., 2016), and shows promising results.
",1 Introduction,[0],[0]
"Different from previous work, we propose to construct bilingually-constrained synthetic implicit data (called BiSynData) for DRRimp, which can alleviate the drawbacks of SynData.",1 Introduction,[0],[0]
Our method is inspired by the findings that a discourse instance expressed implicitly in one language may be expressed explicitly in another.,1 Introduction,[0],[0]
"For example, Zhou and Xue
2306
(2012) show that the connectives in Chinese omit much more frequently than those in English with about 82.0% vs. 54.5%.",1 Introduction,[0],[0]
Li et al. (2014a) further argue that there are about 23.3% implicit/explicit mismatchs between Chinese/English instances.,1 Introduction,[0],[0]
"As illustrated in Figure 1, a Chinese implicit instance where the connective ´ is absent, is translated into an English explicit one with the connective but.",1 Introduction,[0],[0]
"Intuitively, the Chinese instance is a real implicit one which can be signaled by but.",1 Introduction,[0],[0]
"Hence, it could potentially serve as additional training data for the Chinese DRRimp, avoiding the different word distribution problem of SynData.",1 Introduction,[0],[0]
"Meanwhile, for the English explicit instance, it is very likely that removing but would not lose any information since its Chinese counterpart ´ can be omitted.",1 Introduction,[0],[0]
"Therefore it could be used for the English DRRimp, alleviating the meaning shift problem of SynData.
We extract our BiSynData from a ChineseEnglish sentence-aligned corpus (Section 2).",1 Introduction,[0],[0]
Then we design a multi-task neural network model to incorporate the BiSynData (Section 3).,1 Introduction,[0],[0]
"Experimental results, on both the English PDTB (Prasad et al., 2008) and Chinese CDTB (Li et al., 2014b), show that BiSynData is more effective than SynData used in previous work (Section 4).",1 Introduction,[0],[0]
"Finally, we review the related work (Section 5) and draw conclusions (Section 6).",1 Introduction,[0],[0]
"Formally, given a Chinese-English sentence pair (Sch, Sen), we try to find an English explicit instance (Arg1en, Arg2en, Connen) in Sen1, and a Chinese implicit instance (Arg1ch, Arg2ch) in Sch, where (Arg1en, Arg2en, Connen) is the translation of (Arg1ch, Arg2ch).",2 BiSynData,[0],[0]
"In most cases, discourse relations should be preserved during translating, so the connective Connen is potentially a strong indicator of the discourse relation between not only Arg1en and Arg2en, but also Arg1ch and Arg2ch.",2 BiSynData,[0],[0]
"Therefore, we can construct two synthetic implicit instances labeled by Connen, denoted as 〈(Arg1en, Arg2en), Connen〉 and 〈(Arg1ch, Arg2ch), Connen〉, respectively.",2 BiSynData,[0],[0]
"We refer to these synthetic instances as BiSynData be-
1In our experiments, we use the pdtb-parser toolkit (Lin et al., 2014) to identify English explicit instances.
cause they are constructed according to the bilingual implicit/explicit mismatch.
",2 BiSynData,[0],[0]
"In our experiments, we extract our BiSynData from a combined corpus (FBIS and HongKong Law), with about 2.38 million Chinese-English sentence pairs.",2 BiSynData,[0],[0]
"We generate 30,032 synthetic English instances and the same number of Chinese instances, with 80 connectives, as our BiSynData.",2 BiSynData,[0],[0]
"Table 1 lists the top 10 most frequent connectives in our BiSynData, which are roughly consistent with the statistics of Chinese/English implicit/explicit mismatches in (Li et al., 2014a).",2 BiSynData,[0],[0]
"According to connectives and their related relations in the PDTB, in most cases, and and also indicate the Expansion relation, if and because the Contigency relation, before the Temporal relation, and but the Comparison relation.",2 BiSynData,[0],[0]
"Connectives as, when, while and since are ambiguous.",2 BiSynData,[0],[0]
"For example, while can indicate the Comparison or Temporal relation.",2 BiSynData,[0],[0]
"Overall, our constructed BiSynData covers all four main discourse relations defined in the PDTB.
",2 BiSynData,[0],[0]
"With our BiSynData, we define two connective classification tasks: 1) given (Arg1en, Arg2en) to predict the connective Connen, and 2) given (Arg1ch, Arg2ch) to predict Connen.",2 BiSynData,[0],[0]
"We incorporate the first task to help the English DRRimp, and the second for the Chinese DRRimp.",2 BiSynData,[0],[0]
It is worthy to note that we use English connectives themselves as classification labels rather than mapping them to relations in both tasks.,2 BiSynData,[0],[0]
"We design a Multi-task Neural Network Model (denoted as MTN ), which incorporates a connective classification task on BiSynData (auxiliary task) to benefit DRRimp (main task).",3 Multi-Task Neural Network Model,[0],[0]
"In general, the more related two tasks are, the more powerful a multi-task learning method will be.",3 Multi-Task Neural Network Model,[0],[0]
"In the current problem, the
2307
two tasks are essentially the same, just with different output labels.",3 Multi-Task Neural Network Model,[0],[0]
"Therefore, as illustrated in Figure 2, MTN shares parameters in all feature layers (L1-L3) and uses two separate classifiers in the classifier layer (L4).",3 Multi-Task Neural Network Model,[0],[0]
"For each task, given an instance (Arg1, Arg2), MTN simply averages embeddings of words to represent arguments, as vArg1 and vArg2 .",3 Multi-Task Neural Network Model,[0],[0]
These two vectors are then concatenated and transformed through two non-linear hidden layers.,3 Multi-Task Neural Network Model,[0],[0]
"Finally, the corresponding softmax layer is used to perform classification.
",3 Multi-Task Neural Network Model,[0],[0]
MTN ignores the word order in arguments and uses two hidden layers to capture the interactions between two arguments.,3 Multi-Task Neural Network Model,[0],[0]
"The idea behind MTN is borrowed from (Iyyer et al., 2015), where a deep averaging network achieves close to the state-ofthe-art performance on text classification.",3 Multi-Task Neural Network Model,[0],[0]
"Though MTN is simple, it is easy to train and efficient on both memory and computational cost.",3 Multi-Task Neural Network Model,[0],[0]
"In addition, the simplicity of MTN allows us to focus on measuring the quality of BiSynData.
",3 Multi-Task Neural Network Model,[0],[0]
"We use the cross-entropy loss function and minibatch AdaGrad (Duchi et al., 2011) to optimize parameters.",3 Multi-Task Neural Network Model,[0],[0]
Pre-trained word embeddings are fixed.,3 Multi-Task Neural Network Model,[0],[0]
We find that fine-tuning word embeddings during training leads to severe overfitting in our experiments.,3 Multi-Task Neural Network Model,[0],[0]
"Following Liu et al. (2016), we alternately use two tasks to train the model, one task per epoch.",3 Multi-Task Neural Network Model,[0],[0]
"For tasks on both the PDTB and CDTB, we use the same hyper-parameters.",3 Multi-Task Neural Network Model,[0],[0]
The dimension of word embedding is 100.,3 Multi-Task Neural Network Model,[0],[0]
"We set the size of L2 to 200, and L3 to 100.",3 Multi-Task Neural Network Model,[0],[0]
ReLU is used as the non-linear function.,3 Multi-Task Neural Network Model,[0],[0]
"Different learning rates 0.005 and 0.001 are used in the main and auxiliary tasks, respectively.",3 Multi-Task Neural Network Model,[0],[0]
"To avoid overfitting, we randomly drop out 20% words
in each argument following Iyyer et al. (2015).",3 Multi-Task Neural Network Model,[0],[0]
All hyper-parameters are tuned on the development set.,3 Multi-Task Neural Network Model,[0],[0]
We evaluate our method on both the English PDTB and Chinese CDTB data sets.,4 Experiments,[0],[0]
"We tokenize English data and segment Chinese data using the Stanford CoreNLP toolkit (Manning et al., 2014).",4 Experiments,[0],[0]
"The English/Chinese Gigaword corpus (3rd edition) is used to train the English/Chinese word embeddings via word2vec (Mikolov et al., 2013), respectively.",4 Experiments,[0],[0]
"Due to the skewed class distribution of test data (see Section 4.1), we use the macro-averaged F1 for performance evaluation.",4 Experiments,[0],[0]
"Following Rutherford and Xue (2015), we perform a 4-way classification on the top-level discourse relations: Temporal (Temp), Comparison (Comp), Contigency (Cont) and Expansion (Expa).",4.1 On the PDTB,[0],[0]
"Sections 2-20 are used as training set, sections 0-1 as development set and sections 21-22 as test set.",4.1 On the PDTB,[0],[0]
"The training/test set contains 582/55 instances for Temp, 1855/145 for Comp, 3235/273 for Cont and 6673/538 for Expa.",4.1 On the PDTB,[0],[0]
"The top 20 most frequent connectives in our BiSynData are considered in the auxiliary task, with 28,013 synthetic English instances in total.
",4.1 On the PDTB,[0],[0]
"Table 2 shows the results of MTN combining our BiSynData (denoted as MTNbi) on the PDTB.
2308
STN means we train MTN with only the main task.",4.1 On the PDTB,[0],[0]
"On the macro F1, MTNbi gains an improvement of 4.17% over STN .",4.1 On the PDTB,[0],[0]
The improvement is significant under one-tailed t-test (p<0.05).,4.1 On the PDTB,[0],[0]
"A closer look into the results shows that MTNbi performs better across all relations, on the precision, recall and F1 score, except a little drop on the recall of Cont.",4.1 On the PDTB,[0],[0]
The reason for the recall drop of Cont is not clear.,4.1 On the PDTB,[0],[0]
"The greatest improvement is observed on Comp, up to 6.36% F1 score.",4.1 On the PDTB,[0],[0]
"The possible reason is that only while is ambiguous about Comp and Temp, while as, when and since are all ambiguous about Temp and Cont, among top 10 connectives in our BiSynData.",4.1 On the PDTB,[0],[0]
Meanwhile the amount of labeled data for Comp is relatively small.,4.1 On the PDTB,[0],[0]
"Overall, using BiSynData under our multi-task model achieves significant improvements on the English DRRimp.",4.1 On the PDTB,[0],[0]
"We believe the reasons for the improvements are twofold: 1) the added synthetic English instances from our BiSynData can alleviate the meaning shift problem, and 2) a multi-task learning method is helpful for addressing the different word distribution problem between implicit and explicit data.
",4.1 On the PDTB,[0],[0]
"Considering some of the English connectives (e.g., while) are highly ambiguous, we compare our method with ones that uses only unambiguous connectives.",4.1 On the PDTB,[0],[0]
"Specifically, we first discard as, when, while and since in top 20 connectives, and get 22,999 synthetic instances.",4.1 On the PDTB,[0],[0]
"Then, we leverage these instances in two different ways: 1) using them in our multi-task model as above, and 2) using them as additional training data directly after mapping unambiguous connectives into relations.",4.1 On the PDTB,[0],[0]
Both methods using only unambiguous connectives do not achieve better performance.,4.1 On the PDTB,[0],[0]
"One possible reason is that these synthetic instances become more unbalanced after discarding ones with ambiguous connectives.
",4.1 On the PDTB,[0],[0]
We also compare MTNbi with recent systems using additional training data.,4.1 On the PDTB,[0],[0]
"Rutherford and Xue (2015) select explicit instances that are similar to the implicit ones via connective classification, to enrich the training data.",4.1 On the PDTB,[0],[0]
"Liu et al. (2016) use a multi-task model with three auxiliary tasks: 1) conn: connective classification on explicit instances, 2) exp: relation classification on the labeled explicit instances in the PDTB, and 3) rst: relation classification on the labeled RST corpus (William and Thompson,
1988), which defines different discourse relations with that in the PDTB.",4.1 On the PDTB,[0],[0]
The results are shown in Table 3.,4.1 On the PDTB,[0],[0]
"Although Liu et al. (2016) achieve the stateof-the-art performance (Line 5), they use two additional labeled corpora.",4.1 On the PDTB,[0],[0]
"We can find that MTNbi (Line 6) yields better results than those systems incorporating SynData (Line 1, 2 and 3), or even the labeled RST (Line 4).",4.1 On the PDTB,[0],[0]
These results confirm that BiSynData can indeed alleviate the disadvantages of SynData effectively.,4.1 On the PDTB,[0],[0]
"Four top-level relations are defined in the CDTB, including Transition (Tran), Causality (Caus), Explanation (Expl) and Coordination (Coor).",4.2 On the CDTB,[0],[0]
"We use instances in the first 50 documents as test set, second 50 documents as development set and remaining 400 documents as training set.",4.2 On the CDTB,[0],[0]
We conduct a 3-way classification because of only 39 instances for Tran.,4.2 On the CDTB,[0],[0]
"The training/test set contains 682/95 instances for Caus, 1143/126 for Expl and 2300/347 for Coor.",4.2 On the CDTB,[0],[0]
"The top 20 most frequent connectives (excluding and)2 in our BiSynData are considered in the auxiliary task, with 13,899 synthetic Chinese instances in total.",4.2 On the CDTB,[0],[0]
The results are shown in Table 4.,4.2 On the CDTB,[0],[0]
"Compared with STN , MTNbi raises the macro F1 from 55.44% to 58.28%.",4.2 On the CDTB,[0],[0]
The improvement is significant under one-tailed t-test (p<0.05).,4.2 On the CDTB,[0],[0]
"Therefore, BiSynData is also helpful for the Chinese DRRimp.
",4.2 On the CDTB,[0],[0]
"Because of no reported results on the CDTB, we use MTN with two different auxiliary tasks as baselines: 1) exp: relation classification on the labeled
2Including and degrades the performance slightly.",4.2 On the CDTB,[0],[0]
"A possible reason is that and can be related to both the Expl and Coor relations in the CDTB, and instances marked by and account for about half of our BiSynData.
2309
explicit instances in the CDTB, including 466 instances for Caus, 201 for Expl and 974 for Coor.",4.2 On the CDTB,[0],[0]
2) conn: connective classification on explicit instances from the Xinhua part of the Chinese Gigaword corpus.,4.2 On the CDTB,[0],[0]
"We collect explicit instances with the top 20 most frequent Chinese connectives and sample 20,000 instances for the experiment.",4.2 On the CDTB,[0],[0]
Both exp and conn can be considered as tasks on SynData.,4.2 On the CDTB,[0],[0]
"The results in Table 5 show that MTN incorporating BiSynData (Line 3) performs better than using SynData (Line 1 and 2), for the task on the CDTB.",4.2 On the CDTB,[0],[0]
One line of research related to DRRimp tries to take advantage of explicit discourse data.,5 Related Work,[0],[0]
Zhou et al. (2010) predict the absent connectives based on a language model.,5 Related Work,[0],[0]
Using these predicted connectives as features is proven to be helpful.,5 Related Work,[0],[0]
"Biran and McKeown (2013) aggregate word-pair features that are collected around the same connectives, which can effectively alleviate the feature sparsity problem.",5 Related Work,[0],[0]
"More recently, Braud and Denis (2014) and Ji et al. (2015) consider explicit data from a different domain, and use domain adaptation methods to explore the effect of them.",5 Related Work,[0],[0]
"Rutherford and Xue (2015) propose to gather weakly labeled data from explicit instances via connective classification, which are
used as additional training data directly.",5 Related Work,[0],[0]
Lan et al. (2013) and Liu et al. (2016) combine explicit and implicit data using multi-task learning models and gain improvements.,5 Related Work,[0],[0]
"Different from all the above work, we construct additional training data from a bilingual corpus.
",5 Related Work,[0],[0]
Multi-task neural networks have been successfully used for many NLP tasks.,5 Related Work,[0],[0]
"For example, Collobert et al. (2011) jointly train models for the Partof-Speech tagging, chunking, named entity recognition and semantic role labeling using convolutional network.",5 Related Work,[0],[0]
Liu et al. (2015) successfully combine the tasks of query classification and ranking for web search using a deep multi-task neural network.,5 Related Work,[0],[0]
"Luong et al. (2016) explore multi-task sequence to sequence learning for constituency parsing, image caption generation and machine translation.",5 Related Work,[0],[0]
"In this paper, we introduce bilingually-constrained synthetic implicit data (BiSynData), which are generated based on the bilingual implicit/explicit mismatch, into implicit discourse relation recognition for the first time.",6 Conclusion,[0],[0]
"On both the PDTB and CDTB, using BiSynData as the auxiliary task significantly improves the performance of the main task.",6 Conclusion,[0],[0]
We also show that BiSynData is more beneficial than the synthetic implicit data typically used in previous work.,6 Conclusion,[0],[0]
"Since the lack of labeled data is a major challenge for implicit discourse relation classification, our proposed BiSynData can enrich the training data and then benefit future work.",6 Conclusion,[0],[0]
We would like to thank all the reviewers for their constructive and helpful suggestions on this paper.,Acknowledgments,[0],[0]
"This work is partially supported by the Natural Science Foundation of China (Grant Nos. 61573294, 61303082, 61672440), the Ph.D. Programs Foundation of Ministry of Education of China (Grant No. 20130121110040), the Fund of Research Project of Tibet Autonomous Region of China (Grant No. Z2014A18G2-13), and the Natural Science Foundation of Fujian Province (Grant No. 2016J05161).
2310",Acknowledgments,[0],[0]
"To alleviate the shortage of labeled data, we propose to use bilingually-constrained synthetic implicit data for implicit discourse relation recognition.",abstractText,[0],[0]
These data are extracted from a bilingual sentence-aligned corpus according to the implicit/explicit mismatch between different languages.,abstractText,[0],[0]
"Incorporating these data via a multi-task neural network model achieves significant improvements over baselines, on both the English PDTB and Chinese CDTB data sets.",abstractText,[0],[0]
Bilingually-constrained Synthetic Data for Implicit Discourse Relation Recognition,title,[0],[0]
"Binary classification, with the goal of predicting a binary response given input features, is perhaps the classical problem in machine learning, with wide ranging applications.",1. Introduction,[0],[0]
"A key ingredient in binary classification is a performance measure, that quantifies how well a given classifier fits the data.",1. Introduction,[0],[0]
"While the performance measure of accuracy has been the predominant focus of both theory and practice, it has severe limitations in many practical settings, such as imbal-
1University of Texas at Austin, Austin, Texas, USA 2University of Illinois at Urbana-Champaign, Champaign, Illinois, USA 3Carnegie Mellon University, Pittsburgh, Pennsylvania, USA.",1. Introduction,[0],[0]
"Correspondence to: Bowei Yan <boweiy@utexas.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"anced classes, and where different types of errors made by the classifier have different costs (Gu et al., 2009; Wallace et al., 2011).",1. Introduction,[0],[0]
"Accordingly, practitioners in applied machine learning settings such as information retrieval and medical diagnosis have developed complex performance metrics that capture important trade-offs between different types of errors; we have collated a few instances in Table 1.",1. Introduction,[0],[0]
"A key complication with many complex classification performance metrics, such as the F-measure (Manning et al., 2008) and Harmonic Mean (Kennedy et al., 2009), is that they cannot be decomposed into the sum or average of individual losses on each sample.",1. Introduction,[0],[0]
"Even the simple performance measure of precision — the fraction of correct positive predictions, among the set of positive predictions — is not a sum of individual losses on each sample.",1. Introduction,[0],[0]
"Thus, the standard theoretical and practical treatments of supervised learning, such as standard empirical risk minimization that minimizes the empirical expectation of a loss evaluated on a single random example, are not applicable.
",1. Introduction,[0],[0]
This practical reality has motivated research into effective and efficient algorithms tailored to complex nondecomposable performance measures.,1. Introduction,[0],[0]
"One class of approaches extend standard empirical risk minimization to this non-decomposable setting, which often relies on strong assumptions on either the form of the classifiers, such as requiring linear classifiers (Narasimhan et al., 2015a), or restricted to specific performance measures such as F-measure (Parambath et al., 2015).",1. Introduction,[0],[0]
"An alternative approach is the plug-in estimator, where we first derive the form of the Bayes optimal classifier, estimate the statistical quantities associated with the Bayes optimal classifier, and finally “plug-in” the sample estimates of the population quantities to then obtain the overall estimate of the Bayes optimal classifier.",1. Introduction,[0],[0]
"In particular, for many complex performance metrics, the Bayes optimal classifier is simply a thresholding of the conditional probability of the positive class (Koyejo et al., 2014; Narasimhan et al., 2014), so that the plug-in estimator requires (a) an estimate of the conditional probability, and (b) the associated threshold.",1. Introduction,[0],[0]
"Plug-in methods have been of particular interest in non-parametric functional estimation as they typically require weaker assumptions on the function class and are often easy to implement.
",1. Introduction,[0],[0]
"In this paper, we seek to advance our understanding and practice of binary classification under complex non-
decomposable performance measures.",1. Introduction,[0],[0]
"We show that for a very broad class of performance measures, encompassing a large set of performance measures used in practice, the Bayes optimal classifier is simply a thresholding of the conditional probability of the response.",1. Introduction,[0],[0]
"Towards this general result, we identify two key properties that a performance measure could satisfy.",1. Introduction,[0],[0]
"The first is what we call a “Karmic” property that loosely has the performance measure be more sensitive to an increase in true positives and true negatives, and a decrease in false positives and false negatives.",1. Introduction,[0],[0]
"The second is a more technical property we call threshold-quasiconcavity, which in turn ensures the performance measure is well-behaved around an optimal threshold.",1. Introduction,[0],[0]
"As we show these properties are satisfied by performance metrics used in practice, and in particular, these conditions are milder than existing results that restrict either the structural form of the performance measures, or impose strong shape constraints such as particular monotonicities.
",1. Introduction,[0],[0]
"Our general result has two main consequences, which we investigate further: one algorithmic, and the other for the analysis of classification error for general performance measures.",1. Introduction,[0],[0]
"As the algorithmic consequence, we leverage the derived form of the Bayes optimal classifier, and some additional general assumptions on the performance measures, to provide a tractable algorithm to estimate the threshold, which coupled with an estimator of the conditional probability, provides a tractable “plug-in estimator” of the Bayes optimal classifier.",1. Introduction,[0],[0]
"Towards the statistical analysis consequence, we provide an analysis of the excess classification error, but with respect to general non-decomposable performance measures, of the general class of plugin-estimators for our class of Bayes optimal classifiers.",1. Introduction,[0],[0]
"Our analysis of classification error rates for such plug-in classifiers depend on three natural quantities: the rate of convergence for the conditional probability estimate, the rate of convergence for the threshold estimate, and a measurement of noise in the data.",1. Introduction,[0],[0]
"For the last part, we extend margin or low-noise assumptions for binary classification with the accuracy performance measure to complex performance measures.",1. Introduction,[0],[0]
"Low noise assumptions, proposed by Mammen et al. (1999) in the context of the accuracy performance measure, bounds the noise level in the neighborhood of the Bayes optimal threshold i.e. 12 for standard classification.",1. Introduction,[0],[0]
"Under such a low-noise assumption, Audibert et al. (2007) derive fast convergence rates for plug-in classification rules based on the smoothness of the conditional probability function.",1. Introduction,[0],[0]
Similar margin assumptions have also been introduced for density level set estimation by Polonik (1995).,1. Introduction,[0],[0]
"We provide a natural extension of such a low-noise assumption, under which we provide explicit rates of convergence of classification error with respect to complex performance measures.",1. Introduction,[0],[0]
"We provide corollaries for both parametric and non-parametric instances of our general class of plugin-classifiers.
",1. Introduction,[0],[0]
The rest of the paper is organized as below.,1. Introduction,[0],[0]
In Section 2 we introduce the problem and relevant notations.,1. Introduction,[0],[0]
The characterization and properties of Bayes optimal classifier are derived in Section 3.,1. Introduction,[0],[0]
"We discuss the algorithm for estimating the plug-in estimator in Section 4, and present the statistical convergence guarantee in Section 5.",1. Introduction,[0],[0]
"Applications of the derived rate for two special cases, Gaussian generative model and β-Hölder class conditional probability are presented in Section 6 where explicit convergence rates are provided.",1. Introduction,[0],[0]
We conclude the paper in Section 7.,1. Introduction,[0],[0]
Detailed proofs are deferred to the supplementary materials.,1. Introduction,[0],[0]
Binary classification entails predicting a binary label Y ∈ {±1} associated with a feature vector X ∈ X ⊂ Rd.,2. Problem Setup and Preliminaries,[0],[0]
Such a a function mapping f : X 7→ {±1} from the feature space X to the labels {±1} is called a binary classifier.,2. Problem Setup and Preliminaries,[0],[0]
Let Θ = {f : X → {±1}} denote a set of binary classifiers.,2. Problem Setup and Preliminaries,[0],[0]
"We assume (X,Y ) has distribution P ∈ P , and let η(x) := P(Y = 1|X = x) denote the conditional probability of the label Y given feature vector x.
A key quantity is the confusion matrix, that consists of four population quantities: true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).",2. Problem Setup and Preliminaries,[0],[0]
Definition 2.1 (Confusion Matrix).,2. Problem Setup and Preliminaries,[0],[0]
"For any classifier f : Rd 7→ {±1}, its confusion matrix is defined as C(f,P) :=",2. Problem Setup and Preliminaries,[0],[0]
"[TP(f,P),FP(f,P),FN(f,P),TN(f,P)] ∈",2. Problem Setup and Preliminaries,[0],[0]
"[0, 1]4, where:
TP(f,P) =",2. Problem Setup and Preliminaries,[0],[0]
"P(Y = +1, f(X) = +1), FP(f,P) = P(Y = −1, f(X) = +1), FN(f,P) =",2. Problem Setup and Preliminaries,[0],[0]
"P(Y = +1, f(X) = −1), TN(f,P) = P(Y = −1, f(X) = −1).
",2. Problem Setup and Preliminaries,[0],[0]
"(1)
Another key ingredient is the utility or performance measure U : Θ×P",2. Problem Setup and Preliminaries,[0],[0]
"→ R, that measures the performance of a classifier.",2. Problem Setup and Preliminaries,[0],[0]
"In this paper, we focus on complex binary classification performance measures that can be expressed as a function of the confusion matrix.",2. Problem Setup and Preliminaries,[0],[0]
"Formally, U(f,P) = G(C(f,P)).",2. Problem Setup and Preliminaries,[0],[0]
"When it is clear from context, we will drop the dependency of the distribution P in C and U .",2. Problem Setup and Preliminaries,[0],[0]
The confusion-matrix functions G corresponding to popular performance measures are listed in Table 1.,2. Problem Setup and Preliminaries,[0],[0]
"Given the performance measure U , we are interested in the corresponding Bayes optimal classifier:
f∗ = arg max f∈Θ U(f,P).",2. Problem Setup and Preliminaries,[0],[0]
"(2)
Given any candidate classifier f , we are then interested in the excess risk U(f∗,P)− U(f,P), which is the utility gap between a given classifier f and the corresponding Bayes optimal.",2. Problem Setup and Preliminaries,[0],[0]
Assumption 1 (Karmic Performance Measure).,2. Problem Setup and Preliminaries,[0],[0]
"The confusion-matrix function G corresponding to the performance measure U is Lipschitz continuous, and satisfies the
Table 1.",2. Problem Setup and Preliminaries,[0],[0]
Examples of evaluation metrics.,2. Problem Setup and Preliminaries,[0],[0]
"Notation: TPR = TPTP+FN ;TNR = TN TN+FP .
",2. Problem Setup and Preliminaries,[0],[0]
"METRIC DEFINITION REFERENCE G(C)
ACCURACY TP + TN (1, 0, 0, 1)C ARITHMETIC MEAN (AM) (TPR + TNR)/2 MENON ET AL.",2. Problem Setup and Preliminaries,[0],[0]
"(2013) ( C1
C1+C3 + C4 C2+C4 )/2
YOUDEN’S INDEX TPR + TNR − 1 YOUDEN (1950) C1 C1+C3 + C4 C2+C4 − 1 Fβ (1+β2)TP (1+β2)TP+β2FN+FP VAN RIJSBERGEN (1974) (1+β2,0,0,0)C (1+β2,1,β2)C LINEAR-FRACTIONAL a1TP+a2FP+a3FN+a4TN b1TP+b2FP+b3FN+b4TN KOYEJO ET AL.",2. Problem Setup and Preliminaries,[0],[0]
(2014) a TC bTC G-MEAN √ TPR · TNR DASKALAKI ET AL.,2. Problem Setup and Preliminaries,[0],[0]
"(2006) √
C1C4 (C1+C3)(C2+C4) Q-MEAN 1− √
(1−TPR)2+(1−TNR)2 2 KUBAT ET AL.",2. Problem Setup and Preliminaries,[0],[0]
"(1997) 1−
√ (
C3 C1+C3 )2+( C2 C2+C4 )2
2
H-MEAN 2 1/TPR+1/TNR KENNEDY ET AL.",2. Problem Setup and Preliminaries,[0],[0]
"(2009) 2
/( C1+C3
C1 + C2+C4 C4 ) condition that ∇G(C)T (1,−1,−1, 1)T ≥ CB , for some constant CB > 0.
",2. Problem Setup and Preliminaries,[0],[0]
"We term performance measures that satisfy the condition ∇G(C)T (1,−1,−1, 1)T ≥ CB as “Karmic measures”, since it guarantees a lower bound on the sensitivity of the performance measure in the direction of increasing true positives and true negatives, and decreasing false positives and false negatives.",2. Problem Setup and Preliminaries,[0],[0]
"While our Karmic assumption slightly weakens the existing monotonicity assumption used in literature, it is worth pointing out that the analysis in (Narasimhan et al., 2014) requires not only monotonicity but also additional assumptions (Assumption B in (Narasimhan et al., 2014)).",2. Problem Setup and Preliminaries,[0],[0]
"Assumption B assumes the existence and uniqueness of an optimal threshold, which turns out to be non-trivial to check.",2. Problem Setup and Preliminaries,[0],[0]
"Our analysis on the threshold-quasi-concavity closes this gap.
",2. Problem Setup and Preliminaries,[0],[0]
"Assumption 1 is satisfied if G is strictly monotonically increasing with respect to TP,TN and decreasing with respect to FP,FN.",2. Problem Setup and Preliminaries,[0],[0]
"Such an assumption is natural in that one would typically prefer a classifier with higher TP for fixed TN and vice versa (Narasimhan et al., 2015b).",2. Problem Setup and Preliminaries,[0],[0]
"This condition is satisfied by most metrics in common use e.g. for the F1 measure, ∇G(C)T (1,−1,−1, 1)T = 4(FP+FN)(2TP+FP+FN)2 is strictly positive as long as the data is not fully separable.",2. Problem Setup and Preliminaries,[0],[0]
Representation of the Bayes Optimal Classifier.,2.1. Related Work,[0],[0]
"The Bayes optimal classifier under the accuracy metric is classically known to be a thresholding of the conditional probability of the response, with the threshold of 1/2 (see e.g. Devroye et al. (2013)).",2.1. Related Work,[0],[0]
This property of Bayes optimal classifier having the thresholded form is called the probability thresholding principle for binary classification by Lewis (1995).,2.1. Related Work,[0],[0]
"Prior work has also shown that the thresholding principle, with a metric dependent threshold, for more complex specific measures such as F-measure (Jansche, 2007;
Zhao et al., 2013), Arithmetic Mean (AM) (Menon et al., 2013), linear-fractional performance metrics (Koyejo et al., 2014), and monotonic concave metrics (Narasimhan et al., 2015a).
",2.1. Related Work,[0],[0]
Plug-in Classifiers for Complex Metrics.,2.1. Related Work,[0],[0]
"For Bayes optimal classifiers that have thresholded form, a line of work has devised plug-in classifiers that then estimate the threshold, and the conditional probability of response.",2.1. Related Work,[0],[0]
"For the AM metric, Menon et al. (2013) show that the threshold is simply the proportion of the positive class.",2.1. Related Work,[0],[0]
"For linear fractional functions, Koyejo et al. (2014) provide an implicit characterization of the optimal threshold, but the solution of which in turn requires the knowledge of the optimal classifier, which is unknown in practice.",2.1. Related Work,[0],[0]
"As a practical estimator, Koyejo et al. (2014) propose an exhaustive search for the threshold over all data samples, and show that the resulting algorithm is consistent, but for which non-asymptotic convergence rates are not known.",2.1. Related Work,[0],[0]
"Narasimhan et al. (2014) also note the importance of estimating the optimal threshold, but do not provide practical algorithms.",2.1. Related Work,[0],[0]
"As we show in Section 3, the empirical risk as a function of the threshold is in general neither convex nor concave.",2.1. Related Work,[0],[0]
"Hence, care must be taken to construct an optimization algorithm that guarantees convergence to the true threshold.
",2.1. Related Work,[0],[0]
Estimators designed for specific Utility Functions.,2.1. Related Work,[0],[0]
"Perhaps the most studied non-decomposable performance metric is the F-measure (Nan et al., 2012; Joachims, 2005; Zhao et al., 2013), with wide use in information retrieval and related areas, and for which researchers have developed tailored estimators (Nan et al., 2012; Joachims, 2005) as well as risk bounds for these estimators (Zhao et al., 2013).",2.1. Related Work,[0],[0]
"For instance, Busa-Fekete et al. (2015) propose a scalable online F-measure estimator for large-scale datasets, with a root finding algorithm for the threshold update which exploits special properties of the F-measure.",2.1. Related Work,[0],[0]
"Similarly, for the
Arithmetic Mean (AM) measure, Menon et al. (2013) design a consistent optimization scheme, based on a balanced classification-calibrated surrogate to AM.",2.1. Related Work,[0],[0]
"Unfortunately, these techniques are not easily extended to general complex performance metrics.
",2.1. Related Work,[0],[0]
Algorithms for General Classification Measures.,2.1. Related Work,[0],[0]
"Joachims (2005) poses the classification problem as a structured prediction problem, and for linear classifiers, propose a structural SVM solver, but for which neither consistency nor explicit convergence rates are known.",2.1. Related Work,[0],[0]
"Kar et al. (2014) proposes an online gradient descent algorithm which requires function classes that satisfy a uniform convergence property, which is difficult to verify apriori.",2.1. Related Work,[0],[0]
"Along similar lines, Narasimhan et al. (2015a) propose a stochastic gradient method, that involves a linearization of classification metric.",2.1. Related Work,[0],[0]
"Their proposed approach depends strongly on the assumption of a linear (or kernelized) classifier, and it is not obvious that the procedure can be extended to more complex non-linear function classes.",2.1. Related Work,[0],[0]
"In this section, we characterize the Bayes-optimal classifier for the broad class of Karmic performance measures, that satisfy Assumption 1.",3. The Bayes Optimal Classifier Revisited,[0],[0]
"We then show that with one additional assumption, we call threshold-quasi-concavity, the optimal threshold can be guaranteed to be unique.",3. The Bayes Optimal Classifier Revisited,[0],[0]
"This result will be crucial for the design and analysis of our computationally efficient threshold finding procedure in Section 4.
",3. The Bayes Optimal Classifier Revisited,[0],[0]
Denote µ as the measure corresponding to the marginal distribution of X .,3. The Bayes Optimal Classifier Revisited,[0],[0]
"The utility is Frechét differentiable, whose Frechét derivative of U may be computed as:
[∇U(f)]x =∇G(C(f))T ·",3. The Bayes Optimal Classifier Revisited,[0],[0]
"[∇C(f)]x
= 1
2
( ∇G(C)T (1,−1,−1, 1)T η(x)
−∇G(C)T (0,−1, 0, 1)T ) dµ(x)
From the Karmic measure Assumption 1, we know that ∇G(C)T (1,−1,−1, 1)T > 0.",3. The Bayes Optimal Classifier Revisited,[0],[0]
"We define the “Bayes critical set” of G(f,P) for any f ∈ F as the set of instances where the utility has zero derivative:
A3(f)",3. The Bayes Optimal Classifier Revisited,[0],[0]
"=
{ x : η(x) = ∇G(C)T (0,−1, 0, 1)T
∇G(C)T (1,−1,−1, 1)T
} .
",3. The Bayes Optimal Classifier Revisited,[0],[0]
"For notational simplicity, we have omitted the dependency of gi on C(f).",3. The Bayes Optimal Classifier Revisited,[0],[0]
"Similarly, we will use A∗3 := A3(f
∗) to denote the Bayes critical set.
",3. The Bayes Optimal Classifier Revisited,[0],[0]
"In this paper we focus on distributions where the critical set of G(f, P ) satisfies P(A3(f))",3. The Bayes Optimal Classifier Revisited,[0],[0]
= 0.,3. The Bayes Optimal Classifier Revisited,[0],[0]
"For instance, this is true for any distribution that satisfies the following assumption.
",3. The Bayes Optimal Classifier Revisited,[0],[0]
Assumption 2 (η-continuity).,3. The Bayes Optimal Classifier Revisited,[0],[0]
Let ν denote the probability measure that is associated with random variable Z = η(X),3. The Bayes Optimal Classifier Revisited,[0],[0]
= P,3. The Bayes Optimal Classifier Revisited,[0],[0]
(,3. The Bayes Optimal Classifier Revisited,[0],[0]
"Y = 1|X), then ν is absolutely continuous with respect to µ.",3. The Bayes Optimal Classifier Revisited,[0],[0]
"Furthermore, the density of η(X), denoted by pη(·), has full support on [0, 1], and is bounded everywhere.
",3. The Bayes Optimal Classifier Revisited,[0],[0]
"Absolute-continuity guarantees the existence of the density of Z. Armed with the above assumption on the conditional probability of the response, we can then characterize the Bayes optimal classifier as follows.
",3. The Bayes Optimal Classifier Revisited,[0],[0]
Theorem 3.1 (Bayes Optimal Classifier as a Thresholding Function).,3. The Bayes Optimal Classifier Revisited,[0],[0]
"Suppose that U is a performance measure that satisfies Assumption 1, and that η(X) satisfies Assumption 2.",3. The Bayes Optimal Classifier Revisited,[0],[0]
Let f∗ be the Bayes classifier with respect to U and C∗ be its confusion matrix.,3. The Bayes Optimal Classifier Revisited,[0],[0]
"Then, for all x ∈ (A∗3)c,
f∗(x) = sign ( η(x)− ∇G(C ∗)T (0,−1, 0, 1)T
∇G(C∗)T (1,−1,−1, 1)T
) .
(3)
Threshold of Bayes optimal classifier For some performance measures, the optimal threshold reduces to an absolute constant; for instance it has the value of 1/2 for the accuracy measure U(f,P) = TP + TN",3. The Bayes Optimal Classifier Revisited,[0],[0]
"(see e.g. (Devroye et al., 2013)).",3. The Bayes Optimal Classifier Revisited,[0],[0]
"In the general case however, the optimal threshold δ∗ is a solution of the fixed point equation:
(∇G(C∗)T (0,−1, 0, 1)T )/(∇G(C∗)T",3. The Bayes Optimal Classifier Revisited,[0],[0]
"(1,−1,−1, 1)T ) = δ∗,
which is fixed point equation due to the dependency of C∗
on the threshold δ∗.",3. The Bayes Optimal Classifier Revisited,[0],[0]
"Theorem 3.1 guarantees the existence of a solution to the above fixed point equation, but not its uniqueness.",3. The Bayes Optimal Classifier Revisited,[0],[0]
"As we will show in Section 5, uniqueness can be achieved with some additional regularity assumptions.
",3. The Bayes Optimal Classifier Revisited,[0],[0]
"We note that Theorem 3.1 only imposes a weak Karmic assumption on the performance measure, which as as stated in Section 2, is more general than even a simple strictly monotonicity assumption.",3. The Bayes Optimal Classifier Revisited,[0],[0]
"In particular, it generalizes prior work such as (Koyejo et al., 2014; Menon et al., 2013), that impose more stringent assumptions (linear or linear fractional form of the measures, or strong monotonicity conditions).
",3. The Bayes Optimal Classifier Revisited,[0],[0]
We next briefly discuss why the critical set is crucial.,3. The Bayes Optimal Classifier Revisited,[0],[0]
"Consider for instance the example studied in Narasimhan et al. (2014): with domain X = {x1, x2, x3}, a corresponding probability mass function (0.25, 0.5, 0.25), and the conditional probability η = (0.49, 0.5, 0.51).",3. The Bayes Optimal Classifier Revisited,[0],[0]
"Narasimhan et al. (2014) show that for this setting, and for the case of the Hmean measure, there exist at least two deterministic Bayes optima: (−1, 1,−1) and (1,−1, 1)}, which can be seen to not have a thresholded form i.e. it cannot be expressed as a (signed) thresholding of the conditional probability.",3. The Bayes Optimal Classifier Revisited,[0],[0]
"Our analysis reveals why this is the case.
",3. The Bayes Optimal Classifier Revisited,[0],[0]
"From the threshold expression in (3) from Theorem 3.1, the optimal threshold can be computed explicitly as ∇G(C∗)T (0,−1,0,1)T ∇G(C∗)T (1,−1,−1,1)T = 1 2 .",3. The Bayes Optimal Classifier Revisited,[0],[0]
"Thus, the Bayes critical set A∗3 = {x : η(x) = 12} = {x2} has measure P (X ∈ A3) = P (X = x2) = 1 2 > 0.",3. The Bayes Optimal Classifier Revisited,[0],[0]
It is clear that the Bayes optimal classifier may not take a thresholded form on the Bayes critical set.,3. The Bayes Optimal Classifier Revisited,[0],[0]
"We are interested in characterizing mild conditions on the performance measure under which the fixed point equation characterizing the Bayes optimal threshold has a unique solution, under which case P (A∗3) = 0 (guaranteed by the η-continuity Assumption 2).
",3.1. Uniqueness of the Bayes Optimal Threshold.,[0],[0]
"The performance measure restricted to classifiers that are threshold functions of the conditional probability, can be rewritten as a function of the conditional probability η and the threshold δ.
",3.1. Uniqueness of the Bayes Optimal Threshold.,[0],[0]
Definition 3.1.,3.1. Uniqueness of the Bayes Optimal Threshold.,[0],[0]
"We define Vη(δ,P) :",3.1. Uniqueness of the Bayes Optimal Threshold.,[0],[0]
"= U(fη,δ,P) as the performance measure of any threshold classifier fη,δ(x) = sign (η(x)− δ).",3.1. Uniqueness of the Bayes Optimal Threshold.,[0],[0]
"Its arguments are the threshold δ, and distribution P, while the subscript η notes its dependence on the conditional probability η.
",3.1. Uniqueness of the Bayes Optimal Threshold.,[0],[0]
"We next introduce the definition of quasi-concavity, and the assumption of V being strictly quasi-concave.",3.1. Uniqueness of the Bayes Optimal Threshold.,[0],[0]
Definition 3.2.,3.1. Uniqueness of the Bayes Optimal Threshold.,[0],[0]
"A function f : X → R is said to be quasiconcave if ∀x, y ∈ X , such that f(x) ≤ f(y), it follows that 〈∇f(x), y − x〉 ≥ 0.",3.1. Uniqueness of the Bayes Optimal Threshold.,[0],[0]
"We further say that f is strictly quasiconcave if it is quasi-concave and its gradient only vanishes at the global optimum, i.e., f(y) < maxx∈X f(x)⇒ ‖∇f(y)‖ > 0.",3.1. Uniqueness of the Bayes Optimal Threshold.,[0],[0]
"Quasi-concave functions have super level sets are convex sets, and moreover by definition are unimodal i.e. have a unique maximal point.
",3.1. Uniqueness of the Bayes Optimal Threshold.,[0],[0]
Assumption 3.,3.1. Uniqueness of the Bayes Optimal Threshold.,[0],[0]
(Threshold-Quasi-Concavity),3.1. Uniqueness of the Bayes Optimal Threshold.,[0],[0]
"The threshold-classifier performance measure Vη(δ,P) is strictly quasi-concave for δ ∈",3.1. Uniqueness of the Bayes Optimal Threshold.,[0],[0]
"[0, 1].
Assumption 3 seems abstract, but it entails that the performance measure is well-behaved as a function of the threshold.",3.1. Uniqueness of the Bayes Optimal Threshold.,[0],[0]
"Moreover, it can be easily shown to hold for performance measures in practical use.",3.1. Uniqueness of the Bayes Optimal Threshold.,[0],[0]
"We provide a proposition that shows that the assumption is satisfied for two important classes of performance measures: linear-fractional functions and concave functions.
",3.1. Uniqueness of the Bayes Optimal Threshold.,[0],[0]
Proposition 3.1.,3.1. Uniqueness of the Bayes Optimal Threshold.,[0],[0]
"If Assumptions 1, 2 hold, and either: (a) G is twice continuously differentiable and concave, or (b) G is a linear fractional function G(C) = a
TC bTC with |bTC| > 0.",3.1. Uniqueness of the Bayes Optimal Threshold.,[0],[0]
"Then Vη(δ,P) is strictly quasi-concave.",3.1. Uniqueness of the Bayes Optimal Threshold.,[0],[0]
Theorem 3.2.,3.1. Uniqueness of the Bayes Optimal Threshold.,[0],[0]
"Under Assumption 3, the fixed-point equa-
tion:
δ = ∇G(C(fδ),P)T (0,−1, 0, 1)T
∇G(C(fδ),P)T (1,−1,−1, 1)T , (4)
where fδ(x) = sign (η(x)− δ), has a unique fixed point δ∗ ∈ (0, 1).",3.1. Uniqueness of the Bayes Optimal Threshold.,[0],[0]
"Hence the threshold in Theorem 3.1 is uniquely defined.
",3.1. Uniqueness of the Bayes Optimal Threshold.,[0],[0]
"Theorems 3.1 and 3.2 have two key consequences: first, we can use the representation to design plugin-estimators of the Bayes optimal classifier; second, it facilitates the statistical analysis for rates of convergence.",3.1. Uniqueness of the Bayes Optimal Threshold.,[0],[0]
We will discuss each of these two consequences in the following sections.,3.1. Uniqueness of the Bayes Optimal Threshold.,[0],[0]
"Theorem 3.1 shows that for Karmic performance measures, the Bayes optimal classifiers has the thresholded form as in Eq.",4. Algorithmic Consequence: Estimation of the Threshold,[0],[0]
"(3), and moreover under the threshold-quasi-concavity Assumption 3, this threshold is unique.",4. Algorithmic Consequence: Estimation of the Threshold,[0],[0]
"An immediate algorithmic consequence of this is to focus on plug-in classifiers that separately estimate the conditional probability, and the threshold.",4. Algorithmic Consequence: Estimation of the Threshold,[0],[0]
We present this plugin-classifier template in Algorithm 1.,4. Algorithmic Consequence: Estimation of the Threshold,[0],[0]
"The template needs: (a) an estimator for conditional probability density η(x), and (b) an estimator for the threshold.",4. Algorithmic Consequence: Estimation of the Threshold,[0],[0]
"For the convenience of analysis, we divide the set of samples into two independent subsets: the conditional probability estimator is estimated using one subset, and the threshold is estimated using the other.",4. Algorithmic Consequence: Estimation of the Threshold,[0],[0]
"In the coming subsec-
Algorithm 1 Two-step Plug-in Classifier for General Metrics
1: Input: Training sample {Xi, Yi}ni=1, utility measure U , conditional probability estimator η̂, stepsize α.",4. Algorithmic Consequence: Estimation of the Threshold,[0],[0]
"2: Randomly split the training sample into two subsets {X(1)i , Y (1) i } n1 i=1",4. Algorithmic Consequence: Estimation of the Threshold,[0],[0]
"and {X (2) i , Y (2) i } n2 i=1",4. Algorithmic Consequence: Estimation of the Threshold,[0],[0]
;,4. Algorithmic Consequence: Estimation of the Threshold,[0],[0]
"3: Estimate η̂ on {X(1)i , Y (1) i } n1 i=1; 4: Estimate δ̂ with {X(2)i , Y (2) i };
5: Output: f̂(x) = sign ( η̂ − δ̂ ) .
tions we discuss how to estimate the conditional probability and the threshold respectively.",4. Algorithmic Consequence: Estimation of the Threshold,[0],[0]
"The estimation of the conditional probability of the response plays a crucial role in the success of Algorithm 1, but we emphasize that it is not the focus of our paper.",4.1. Estimation of Conditional Probability Function,[0],[0]
"In particular, this is a well studied problem, and numerous methods have been proposed for both parametric and non-parametric model assumptions on the conditional probability function.
",4.1. Estimation of Conditional Probability Function,[0],[0]
"In this section we briefly discuss some common estimators, and defer additional details to Section 6.
Parametric methods.",4.1. Estimation of Conditional Probability Function,[0],[0]
"In a classical paper, Ng and Jordan (2002) compares two models of classification: one can either estimate P (Y ) and P (X|Y ) first, then get the conditional probability by Bayes rule (generative model approach); or directly estimate P (Y |X) (discriminative model approach).",4.1. Estimation of Conditional Probability Function,[0],[0]
The two approaches can also be related.,4.1. Estimation of Conditional Probability Function,[0],[0]
"In particular, if PθY (X|Y ) belongs to exponential family, we have
PθY (X|Y ) = h(x) exp (〈θY , φ(X)〉 −A(θY )) ,
where φ(X) is the set of sufficient statistics, θY is the vector of the true canonical parameters, and A(θ) is the logpartition function.",4.1. Estimation of Conditional Probability Function,[0],[0]
"Using Bayes rule, we then have:
P (Y = 1|X) = 1 1 + exp (−〈θ1 − θ0, φ(X)〉+ c∗)
where c∗ = A(θ0) − A(θ1).",4.1. Estimation of Conditional Probability Function,[0],[0]
"The conditional distribution can be seen to follow a logistic regression model, with the generative model sufficient statistics as the features, and the difference of the generative model parameters serving as the parameters of the discriminative model.",4.1. Estimation of Conditional Probability Function,[0],[0]
A natural class of estimators for either the generative or discriminative models is based on Maximum likelihood Estimation (MLE).,4.1. Estimation of Conditional Probability Function,[0],[0]
"In Section 6, we derive the rate of convergence for the special case where the generative distributions are Gaussians with same covariances for both classes.
",4.1. Estimation of Conditional Probability Function,[0],[0]
Non-parametric methods.,4.1. Estimation of Conditional Probability Function,[0],[0]
"One can also estimate η(x) = P (Y = 1|X) non-parametrically, where a common model assumption is some form of smoothness on η(x).",4.1. Estimation of Conditional Probability Function,[0],[0]
"One popular class of smooth functions is the following.
",4.1. Estimation of Conditional Probability Function,[0],[0]
Definition 4.1 (β-Hölder class).,4.1. Estimation of Conditional Probability Function,[0],[0]
"Let β > 0, denote bβc the maximal integer that is strictly less than β.",4.1. Estimation of Conditional Probability Function,[0],[0]
"For x ∈ X and any bβc-times continuously differentiable real-valued function η on X , we denote by ηx its Taylor polynomial of degree bβc at point x,
ηx(x ′) = ∑",4.1. Estimation of Conditional Probability Function,[0],[0]
s≤bβc (x′ − x)s s!,4.1. Estimation of Conditional Probability Function,[0],[0]
"Dsη(x).
",4.1. Estimation of Conditional Probability Function,[0],[0]
"β-Hölder class is defined as the functions that satisfy, for ∀x, x′ ∈ X ,
|ηx(x)− ηx(x′)| ≤ Cβ‖x− x′‖β .
",4.1. Estimation of Conditional Probability Function,[0],[0]
"In particular, when 0 ≤ β < 1, we have |η(x)− η(x′)| ≤ Cβ‖x− x′‖β",4.1. Estimation of Conditional Probability Function,[0],[0]
"where β > 0.
",4.1. Estimation of Conditional Probability Function,[0],[0]
"We can then estimate η(x) from this family of smooth functions via locally polynomial estimators (Audibert et al., 2007), or kernel (conditional) density estimators (Jiang, 2017) with a properly chosen bandwidth.",4.1. Estimation of Conditional Probability Function,[0],[0]
"When Vη is quasi-concave, a key consequence is that its gradient with respect to the threshold suffices to provide ascent direction information.",4.2. Estimation of the Threshold,[0],[0]
"We leverage this consequence, and summarize a simple binary search algorithm based on the sign of V ′η(δ,P) in Algorithm 2.
",4.2. Estimation of the Threshold,[0],[0]
Algorithm 2 Binary search for the optimal threshold 1:,4.2. Estimation of the Threshold,[0],[0]
"Input: Training sample {Xi, Yi}ni=1, utility measure U , conditional probability estimator η̂, tolerance 0.
2: δ` = 0; δr = 1; 3: while |δ` − δr| ≥ 0 do 4: Evaluate s = sign ( V ′η̂(δ,Pn) ) ; 5: if s ≥ 0",4.2. Estimation of the Threshold,[0],[0]
"then 6: δ` = δ`+δr 2 ; 7: else 8: δr = δ`+δr 2 ;
9: end if 10: end while 11: Output:",4.2. Estimation of the Threshold,[0],[0]
"δ`+δr2 .
",4.2. Estimation of the Threshold,[0],[0]
"In the next section, we then analyze the rates of convergence for the excess generalization error of the plug-in classifier learned from Algorithm 1, and with threshold estimated via Algorithm 2.",4.2. Estimation of the Threshold,[0],[0]
We next analyze the convergence rate of the excess utility.,5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
"As we will show, the rates of convergence depend on three quantities: the noise level of the data distribution, the convergence rate of the conditional probability function, and the convergence rate of the threshold.",5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
"We start by introducing some assumptions.
",5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
We assume that the estimator of the conditional probability of response satisfies the following condition.,5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
Assumption 4.,5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
"Let Sn denote a sample set of size n, and ηSn denote the conditional probability estimator learnt from Sn.",5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
"Then, for some absolute constants c1, c2 > 0, the conditional probability estimator satisfies the following condition:
sup Sn
P (|ηSn(x)− η(x)| ≥ )",5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
≤,5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
"c1 exp(−c2 an 2) a.e.
",5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
"The convergence rate also depends on the noise in the training labels, which is typically captured via the probability mass near the Bayes optimal threshold.",5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
"Here we generalize the classical margin assumption (sometimes also called low noise assumption) of Audibert et al. (2007), developed for the accuracy metric, to the case where the optimal threshold is not a fixed constant 12 :
Assumption 5.",5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
For some functionC0(δ∗) > 0,5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
"that depends on the threshold δ∗, there exists an α ≥ 0 such that
PX(0 < |η(X)−",5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
δ∗| ≤ t) ≤,5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
"C0(δ∗) tα.
",5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
The assumption characterizes the behavior of the regression function in the vicinity of the optimal threshold δ∗.,5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
"The case α = 0 bounds the probability by a constant potentially larger than one, and is trivially satisfied.",5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
"The other extreme case α =∞ is most advantageous for classification, since in this case the regression function η is bounded away from the threshold.
",5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
"In cases where the threshold is not an absolute constant (such as 1/2), it has to be estimated from data.",5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
"We make the following assumption on its convergence rate.
",5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
Assumption 6.,5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
"Given a conditional probability estimate η̂ learned from an independent data source, the estimator δ̂n of the threshold, from a sample set of size n, satisfies the following condition, for some absolute constant c3 > 0",5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
":
P (∣∣∣U(sign (η̂ − δ∗))− U(sign(η̂",5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
− δ̂))∣∣∣ ≥ b−1n ),5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
"≤ n−c3 .
",5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
"Note that Assumption 6 allows the rate bn to in turn depend on η̂, or more specifically, EX |η(X)− η̂(X)|.",5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
"Moreover, it does not necessarily require that δ̂ converge to δ∗, only that their corresponding utility function values be close.
",5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
"Armed with these largely notational assumptions, we can now provide the rate for the overall data-splitting two-step plug-in classifier described in Algorithm 1:
Theorem 5.1.",5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
"Suppose Assumption 1 and 2 hold, and further that Assumptions 4 and 6 hold for some η̂ and δ̂. Let U∗ = U(sign (η − δ∗) ,P) be the Bayes optimal utility.",5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
"If we split the data as n1 = n2 = n2 , then with probability greater than 1− n−c4 :
U∗ − U ( sign ( η̂ − δ̂ ) ,P )",5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
≤,5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
"c5 max { a − 1+α2 n , b −1 n } .
where c4, c5 > 0 are absolute constants.",5. Statistical Analysis Consequence: Rates of Convergence,[0],[0]
"We provide a detailed proof of the theorem in the Appendix, but provide brief vignettes via some key lemmas that also provide some additional insight into the statistical analysis.",5.1. Key Lemmas,[0],[0]
A key tool when analyzing traditional binary classification is to turn the excess risk into an expectation of the absolute deviation of conditional probability from the threshold 12 .,5.1. Key Lemmas,[0],[0]
"We show in the following lemma that a similar result holds with general optimal threshold:
Lemma 5.1.",5.1. Key Lemmas,[0],[0]
"Let Cn and C∗ be the vectorized confusion matrices associated with fn = sign (ηn − δ∗)
and f∗ = sign (η − δ∗) respectively, where δ∗ is the threshold for the Bayes optimal classifier.",5.1. Key Lemmas,[0],[0]
"Denote CG := ∇G(C∗)T (1,−1,−1, 1), and CH := maxf ‖∇2G(C(f))‖op, where ‖ · ‖op refers to the operator norm of a matrix.",5.1. Key Lemmas,[0],[0]
"If for some constant c6, an ≥ c6 ( CH
CG min{δ∗,1−δ∗}
)2 , then
G(C∗)− G(Cn)",5.1. Key Lemmas,[0],[0]
"≥ 1
2 CGE[|η − δ∗|1(fn 6= f∗)],
G(C∗)− G(Cn) ≤ 3
2 CGE[|η − δ∗|1(fn 6= f∗)].
",5.1. Key Lemmas,[0],[0]
This lemma thus helps us control the excess utility via the error of the conditional probability estimator η̂ − η.,5.1. Key Lemmas,[0],[0]
"Armed with this result, and additionally using Assumption 4 on the convergence rate of the conditional probability estimator, we can then show that the excess utility converges at the rate O(a− 1+α 2 n ):
Lemma 5.2.",5.1. Key Lemmas,[0],[0]
"Suppose that Assumptions 4 and 5 are satisfied, and that the Bayes optimal classifier is f∗ = sign (η − δ∗).",5.1. Key Lemmas,[0],[0]
"Then there exists a constant c7 > 0 which depend on G and C(f∗), such that U(sign (η − δ∗))− U(sign (ηn − δ∗)) ≤ c7a − 1+α2 n .
",5.1. Key Lemmas,[0],[0]
Lemma 5.2 describes the classification error rate when the optimal threshold is known.,5.1. Key Lemmas,[0],[0]
Stitching this together with Assumption 6 on the convergence rate of the threshold estimator can then be shown to yield the statement of Theorem 5.1.,5.1. Key Lemmas,[0],[0]
"Algorithm 2
",5.2. Risk Bound for the Plugin Classifier from,[0],[0]
"Prior work on threshold estimation for plug-in classifiers have ranged over brute-force search (Koyejo et al., 2014) with no rates of convergence, level-set based methods (Parambath et al., 2015) for the specific class of linear fractional metrics, and Frank-Wolfe based methods (Narasimhan et al., 2015b) for the specific class of concave performance metrics.",5.2. Risk Bound for the Plugin Classifier from,[0],[0]
"However these estimators, in addition to focus on specific performance metrics, are only able to achieve a convergence rate of O(max{E‖η̂(X)",5.2. Risk Bound for the Plugin Classifier from,[0],[0]
"− η(X)‖1, 1/ √ n}).",5.2. Risk Bound for the Plugin Classifier from,[0],[0]
"This entails that even if when the conditional probability estimator has a fast convergence rate, the final convergence rate for these estimators will still be bounded by O(1/ √ n).",5.2. Risk Bound for the Plugin Classifier from,[0],[0]
"In this section we show that our simple threshold search procedure in Algorithm 2 achieves a fast O(1/n) or O(1/an) rate of convergence by leveraging our analysis from Section 3.
",5.2. Risk Bound for the Plugin Classifier from,[0],[0]
Lemma 5.3.,5.2. Risk Bound for the Plugin Classifier from,[0],[0]
"Assume that Assumptions 1, 2, 5 hold, and that the confusion-matrix function G corresponding to the performance measure U satisfies the same conditions as in Proposition 3.1.",5.2. Risk Bound for the Plugin Classifier from,[0],[0]
"Let δ̂ denote the output of Algorithm 2 with sample size n and tolerance τ = lognn , and η̂ denote
a conditional probability estimator satisfying Assumption 4 obtained on an independent sample set of size n. Denoting ñ = min{n, an}, we then have that the rate bn in Assumption 6 satisfies: bn = log ññ .
",5.2. Risk Bound for the Plugin Classifier from,[0],[0]
"An immediate corollary then gives the excess risk for the plug-in classifier.
",5.2. Risk Bound for the Plugin Classifier from,[0],[0]
Corollary 5.1.,5.2. Risk Bound for the Plugin Classifier from,[0],[0]
Suppose Assumption 3 holds.,5.2. Risk Bound for the Plugin Classifier from,[0],[0]
"If τ = logn n , n1 = n2 = n 2 , then there exist constants c8, c9 > 0, such that with probability at least 1−min{n, an}−c8 ,
U(f∗,P)− U(f̂ ,P) ≤ c9 max { log n
n , log an an , a − 1+α2 n
} .",5.2. Risk Bound for the Plugin Classifier from,[0],[0]
"In this section, we analyze two special cases where we can achieve explicit rate of convergence for the conditional probability estimation.",6. Explicit Rates for Specific Conditional Probability Models,[0],[0]
"For the first example, we consider the Gaussian generative model.",6. Explicit Rates for Specific Conditional Probability Models,[0],[0]
We will show that the rate of convergence for the excess utility obtained in Theorem 5.1 is O( lognn ) in this case.,6. Explicit Rates for Specific Conditional Probability Models,[0],[0]
The second example is for nonparametric kernel estimators when the conditional probability function satisfies certain smoothness assumption.,6. Explicit Rates for Specific Conditional Probability Models,[0],[0]
"Consider two Gaussian distributions with the same variance, without loss of generality we assume the covariance matrix is identity Id for both classes.",6.1. Gaussian Generative Model,[0],[0]
"We define an asymmetric mixture of two Gaussians indexed by the centers and mixing weights.
",6.1. Gaussian Generative Model,[0],[0]
"Pµ,κ :",6.1. Gaussian Generative Model,[0],[0]
"P (Y = 1) = κ, P (Y = 0) = 1− κ, X|Y = 1 ∼ N (µ
2 , Id
) , X|Y = 0 ∼ N ( −µ
2 , Id
) .",6.1. Gaussian Generative Model,[0],[0]
"(5)
As stated in Section 4, we can compute the conditional probability and show that it can be fitted with a logistic regression model.",6.1. Gaussian Generative Model,[0],[0]
"Next we present results related to the key quantities in Theorem 5.1: an and α.
",6.1. Gaussian Generative Model,[0],[0]
Lemma 6.1.,6.1. Gaussian Generative Model,[0],[0]
Model defined in Eq.,6.1. Gaussian Generative Model,[0],[0]
"(5) with maximum likelihood estimator satisfies Assumption 4 with an = n.
The following lemma specifies the margin assumption parameter for the above model.
",6.1. Gaussian Generative Model,[0],[0]
Lemma 6.2.,6.1. Gaussian Generative Model,[0],[0]
The Gaussian generative model defined as in Eq.,6.1. Gaussian Generative Model,[0],[0]
"(5), satisfies Assumption 5 with α = 1.
",6.1. Gaussian Generative Model,[0],[0]
"Combining this result with Theorem 5.1 gives us the following corollary.
",6.1. Gaussian Generative Model,[0],[0]
Corollary 6.1.,6.1. Gaussian Generative Model,[0],[0]
"Assume Assumptions 1-5 hold, P is generated from Eq.",6.1. Gaussian Generative Model,[0],[0]
(5).,6.1. Gaussian Generative Model,[0],[0]
"Let f̂ be the output of Algorithm 1 with
η̂ estimated by MLE of logistic regression.",6.1. Gaussian Generative Model,[0],[0]
"We have with probability tending to 1, U(f∗,P)−U(f̂ ,P) =",6.1. Gaussian Generative Model,[0],[0]
"O ( logn n ) .
",6.1. Gaussian Generative Model,[0],[0]
"For Gaussian generative models, fast rates of O( 1n ) are only known for 0-1 loss (Li et al., 2015).",6.1. Gaussian Generative Model,[0],[0]
"The logarithm factor can be further removed under 0-1 loss, or other cases when the threshold is known, as one can apply Lemma 5.2 with α = 1 and get exactly the same rate as in Li et al. (2015).",6.1. Gaussian Generative Model,[0],[0]
"Corollary 6.1 generalizes this result for a much broader class of utility functions, when the threshold is unknown and estimated from data.",6.1. Gaussian Generative Model,[0],[0]
"When the conditional probability function belongs to the β-Hölder class as defined in Definition 4.1, we have the following lemma on the convergence rates of ηn.
Lemma 6.3.",6.2. β-Hölder Densities,[0],[0]
"For β-Hölder conditional probability functions, there exists estimators such that Assumption 4 holds with an = n 2β 2β+d .
",6.2. β-Hölder Densities,[0],[0]
"Examples of such estimators include locally polynomial estimators (Audibert et al., 2007), or kernel (conditional) density estimators (Jiang, 2017).",6.2. β-Hölder Densities,[0],[0]
"Combined with Theorem 5.1 we have the following corollary.
",6.2. β-Hölder Densities,[0],[0]
Corollary 6.2.,6.2. β-Hölder Densities,[0],[0]
Assume Assumptions 1-5 hold and P be a distribution where P (Y = 1|X) belongs to β-Hölder class.,6.2. β-Hölder Densities,[0],[0]
"With locally polynomial estimators (Audibert et al., 2007) or kernel (conditional) density estimators (Jiang, 2017), we have: U(f∗,P)− U(f̂) =",6.2. β-Hölder Densities,[0],[0]
"O ( n− (min{α,1}+1)β 2β+d ) .
",6.2. β-Hölder Densities,[0],[0]
"The convergence rate obtained in Corollary 6.2 is faster than O( 1√
n )",6.2. β-Hölder Densities,[0],[0]
"if β > max{ d2α , d 2}.",6.2. β-Hölder Densities,[0],[0]
"It is worth pointing out that the fast rate is obtained via a trade-off between the parameter α and β: to have a very smooth conditional probability function η, i.e., a large value of β, it cannot deviate from the critical level very abruptly, hence α has to be small.",6.2. β-Hölder Densities,[0],[0]
We study Bayes optimal classification for general performance metrics.,7. Conclusion,[0],[0]
"We derive the form of the Bayes optimal classifier, provide practical algorithms to estimate this Bayes optimal classifier, and provide novel analysis of classification error with respect to general performance metrics, and in particular show our estimators are not only consistent but have fast rates of convergence.",7. Conclusion,[0],[0]
"We also provide corollaries of our general results for some special cases, such as when the inputs are drawn from a Gaussian mixture generative models, or when the conditional probability function lies in a Hölder space, explicitly proving fast rates under mild regularity conditions.",7. Conclusion,[0],[0]
"P.R. acknowledges the support of NSF via IIS-1149803, IIS-1664720, DMS-1264033, and PNC via the PNC Center for Financial Services Innovation.",Acknowledgements,[0],[0]
"Complex performance measures, beyond the popular measure of accuracy, are increasingly being used in the context of binary classification.",abstractText,[0],[0]
"These complex performance measures are typically not even decomposable, that is, the loss evaluated on a batch of samples cannot typically be expressed as a sum or average of losses evaluated at individual samples, which in turn requires new theoretical and methodological developments beyond standard treatments of supervised learning.",abstractText,[0],[0]
"In this paper, we advance this understanding of binary classification for complex performance measures by identifying two key properties: a so-called Karmic property, and a more technical threshold-quasiconcavity property, which we show is milder than existing structural assumptions imposed on performance measures.",abstractText,[0],[0]
"Under these properties, we show that the Bayes optimal classifier is a threshold function of the conditional probability of positive class.",abstractText,[0],[0]
"We then leverage this result to come up with a computationally practical plug-in classifier, via a novel threshold estimator, and further, provide a novel statistical analysis of classification error with respect to complex performance measures.",abstractText,[0],[0]
"Binary Classification with Karmic, Threshold-Quasi-Concave Metrics",title,[0],[0]
Decision Trees as well as ensemble methods that use them (e.g. Random Forests and Gradient Boosted Trees) are among the most popular methods for classification tasks.,1. Introduction,[0],[0]
"It is widely known that decision trees, specially small ones, are easy to interpret while ensemble methods usually yield to more stable/accurate classifications.
",1. Introduction,[0],[0]
"When building a decision tree, in each node, one needs to address two problems: which attribute shall be used for branching, and how to split the chosen attribute, i.e., which values of the attribute go to each branch.",1. Introduction,[0],[0]
"For the first problem we refer the reader to (Hothorn et al., 2006; Nowozin, 2012).",1. Introduction,[0],[0]
"Here we consider the latter, which is a well-studied problem (Breiman et al., 1984; Nadas et al., 1991; Chou, 1991; Burshtein et al., 1992; Coppersmith et al., 1999; Elomaa & Rousu, 2004).",1. Introduction,[0],[0]
"More specifically, we focus on nominal attributes (i.e. finite set of possible values with no additional structure such as order).
",1. Introduction,[0],[0]
"*Equal contribution 1Departamento de Informática, PUCRIO, Brazil.",1. Introduction,[0],[0]
"Correspondence to: Eduardo Laber <eduardo.laber1@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
An important design choice is whether to use multiway splits or binary splits.,1. Introduction,[0],[0]
"One possibility is splitting a nominal attribute with n distinct values into n branches, one for each value.",1. Introduction,[0],[0]
"When n is large, this option may lead to a severe data fragmentation, which makes the classification task harder and increases the risk of data overfit since we may have only a few examples associated with each branch.",1. Introduction,[0],[0]
Note that any decision tree obtained via multiway splits can be simulated by a decision tree that only uses binary splits.,1. Introduction,[0],[0]
"Thus, we focus our study on binary splits.
",1. Introduction,[0],[0]
"The standard approach for deciding the split is to search for ‘pure’ partitions of the set of examples, that is, partitions in which each branch is very homogeneous with respect to the class distribution of its examples.",1. Introduction,[0],[0]
"To measure how impure each branch is, impurity measures are often employed.",1. Introduction,[0],[0]
"An impurity measure maps a vector u = (u1, . . .",1. Introduction,[0],[0]
", uk), counting how many examples of each class we have in a node (branch), into a non-negative scalar 1.",1. Introduction,[0],[0]
"Arguably, two of the most classical impurity measures are the Gini impurity
iGini(u) = k∑ i=1",1. Introduction,[0],[0]
ui ‖u‖1,1. Introduction,[0],[0]
"( 1− ui ‖u‖1 ) ,
which is used in the CART package (Breiman et al., 1984), and the Entropy impurity
iEntr(u) =",1. Introduction,[0],[0]
"− k∑
i=1
ui ‖u‖1 log ( ui ‖u‖1 ) ,
that, along with its variants, is used in the C4.5 decision tree inducer (Quinlan, 1992).",1. Introduction,[0],[0]
"Given an attribute and an impurity measure, the goal is then to find a binary split (L∗, R∗) for the attribute values that induces a binary partition of the set of examples with minimum weighted impurity, where the weights are given by the number of examples that lie into each of the two branches.
",1. Introduction,[0],[0]
"For classification tasks with only two classes, Breiman et al. (Breiman et al., 1984) proposed an algorithm that finds a partition with minimum weighted impurity in O(n log n) time for a family of impurity measures that include both Gini and Entropy.",1. Introduction,[0],[0]
"For nominal attributes with a small number of distinct values n, the best partition can be found in O(2n)
1In the original definition an impurity measure maps a vector of probabilities into a non-negative scalar.
time by an exhaustive search.",1. Introduction,[0],[0]
"However, when k > 2 and n is large (e.g. states of a country, letters of the alphabet, breed of an animal), these methods are not effective.",1. Introduction,[0],[0]
"Thus, heuristics are commonly used (Nadas et al., 1991; Chou, 1991; Mehta et al., 1996; Coppersmith et al., 1999; Loh, 2009).",1. Introduction,[0],[0]
"Despite the importance of this problem, little is known about its computational complexity and the quality (approximation guarantee) of its heuristics.",1. Introduction,[0],[0]
"Therefore, our goal here is contributing to fill this gap.",1. Introduction,[0],[0]
"Given an impurity measure i (e.g. iGini), define I as I(v) = ‖v‖1 · i(v) for all vectors v. This scaled impurity I is called frequency-weighted impurity measure in (Coppersmith et al., 1999) and will be used to formalize our problem.
",1.1. Problem Description,[0],[0]
"Consider a nominal attribute A that may take n possible values a1, . . .",1.1. Problem Description,[0],[0]
", an.",1.1. Problem Description,[0],[0]
The `-ary Partition with Minimum Weighted Impurity Problem (`-PMWIP) can be described abstractly as follows.,1.1. Problem Description,[0],[0]
We are given a collection of n vectors V ⊂,1.1. Problem Description,[0],[0]
"Rk, where the ith coordinate of the jth vector counts the number of examples in class i for which the attribute A has value aj .",1.1. Problem Description,[0],[0]
We are also given a scaled impurity measure I .,1.1. Problem Description,[0],[0]
"The goal is to partition V into ` disjoint groups of vectors V1, . . .",1.1. Problem Description,[0],[0]
", V` so as to minimize the sum of the weighted impurities ∑̀
m=1
I ( ∑ v∈Vm v ) .
",1.1. Problem Description,[0],[0]
We focus on binary partitions (2-PMWIP) and on a broad class of impurity measures that includes both Gini and Entropy.,1.1. Problem Description,[0],[0]
"These impurities have the form
I(v) = ‖v‖1
( k∑
i=1
f",1.1. Problem Description,[0],[0]
( vi ‖v‖1 )),1.1. Problem Description,[0],[0]
",
where f is a strictly concave function that satisfies a certain property related to its curvature.",1.1. Problem Description,[0],[0]
The formal definition of this class is postponed to Section 2.1.,1.1. Problem Description,[0],[0]
In this paper we propose new splitting procedures that provably achieve near-optimal impurity.,1.2. Our Results,[0],[0]
"Our starting point is one of the results presented in (Burshtein et al., 1992; Coppersmith et al., 1999) that states that for every instance of 2-PMWIP, where the impurity I satisfies certain conditions, there exists an optimal binary partition that is induced by a homogeneous hyperplane in Rk.",1.2. Our Results,[0],[0]
"Building upon this result we prove that an optimal binary partition can be obtained by a non-homogeneous hyperplane whose normal direction belongs to the box [0, 1]k.",1.2. Our Results,[0],[0]
"Then, motivated by this observation, we propose and analyze two methods that belong to a family of algorithms that search for binary partitions with
reduced impurity by considering hyperplanes in Rk whose normal lie in the hypercube {0, 1}k.
",1.2. Our Results,[0],[0]
"Our first algorithm, the Hypercube Cover (HcC for short), is closely related with the well established Twoing method proposed in (Breiman et al., 1984).",1.2. Our Results,[0],[0]
We prove that HcC has a 2-approximation for every impurity measure in our class.,1.2. Our Results,[0],[0]
"A drawback of this method, however, is its running time proportional to 2k.",1.2. Our Results,[0],[0]
"Given this limitation, we present LargestClassAlone (LCA for short), a simple algorithm that runs in O(nk + n log n) time and provides a (3 + √ 3)-approximation for every impurity measure in our class.",1.2. Our Results,[0],[0]
This material is covered in Section 3.,1.2. Our Results,[0],[0]
"Furthermore, in Section 4, we show that the approximation ratio of LCA for Gini and Entropy impurities is indeed much better, being at most 2 for the former and at most 3 for the latter.",1.2. Our Results,[0],[0]
"We also show that, unless P = NP , it is not possible to find the partition with minimum impurity in polynomial time, even for the Entropy impurity.
",1.2. Our Results,[0],[0]
"To complement our theoretical findings, in Section 5 we present a set of experiments where we compare the proposed methods with PCext and SLIQext, the two splitting methods that obtained the best results in the study reported in (Coppersmith et al., 1999).",1.2. Our Results,[0],[0]
"Our experiments provide evidence that both methods proposed in this paper are interesting candidates to be used in splitting nominal attributes with many values during decision tree/ random forest induction: HcC is preferable when the number of classes is small and LCA is a good alternative when speed is an issue.
",1.2. Our Results,[0],[0]
We believe that our set of results contributes to improving the current knowledge on a classical and still relevant problem for both the Machine Learning and Data Mining communities.,1.2. Our Results,[0],[0]
"There have been theoretical investigations on methods to compute the best split efficiently (Breiman et al., 1984; Chou, 1991; Burshtein et al., 1992; Coppersmith et al., 1999; Kurkoski & Yagi, 2014).",1.3. Related Work,[0],[0]
"As mentioned above, for the 2- class problem, Breiman et.",1.3. Related Work,[0],[0]
"al. (Breiman et al., 1984) presented a simple algorithm that finds the best binary partition in O(n log n) time for impurity measures in a certain class that includes both Gini and Entropy.",1.3. Related Work,[0],[0]
"The correctness of this algorithm relies on a theorem, also proved in (Breiman et al., 1984), which is generalized for k > 2 classes and multiway partitions in (Chou, 1991; Burshtein et al., 1992; Coppersmith et al., 1999).",1.3. Related Work,[0],[0]
"Basically, these theorems provide necessary conditions for partitions with minimum impurity and can be used to restrict the set of partitions that need to be considered, as in the family of algorithms we study here.",1.3. Related Work,[0],[0]
"However, despite their usefulness, these conditions do not yield a method that has running time polynomial on n and k.
Some heuristics for computing suboptimal partitions are available in the literature (Breiman et al., 1984; Nadas et al., 1991; Mehta et al., 1996; Coppersmith et al., 1999; Loh, 2009).",1.3. Related Work,[0],[0]
For none of them approximation guarantees are available.,1.3. Related Work,[0],[0]
"The conclusion of the experiments reported in (Coppersmith et al., 1999) is that PCext, one of the methods proposed in that paper, overcomes Flip Flop (Nadas et al., 1991) and SLIQ (Mehta et al., 1996) in terms of running time and the impurity of the partitions found.
",1.3. Related Work,[0],[0]
"Recently, motivated by applications on signal processing (e.g. construction of polar codes (Tal & Vardy, 2013)), the problem of computing the quantization of the output of a Discrete Memoryless Channel (DMC) that provides the maximum mutual information with the DMC’s input has attracted a considerable attention in the Information Theory community (Tal & Vardy, 2013; Kurkoski & Yagi, 2014; Kartowsky & Tal, 2017; Pereg & Tal, 2017; Nazer et al., 2017).",1.3. Related Work,[0],[0]
"Kurkoski and Yagi (Kurkoski & Yagi, 2014) observed that this problem is equivalent to `-PMWIP when the impurity measure is the Entropy, and proved that it can be solved in polynomial time when k = 2.",1.3. Related Work,[0],[0]
"In (Gülcü et al., 2016; Nazer et al., 2017; Pereg & Tal, 2017; Kartowsky & Tal, 2017), upper and lower bounds on the difference between the entropy impurity of the n-ary partition and the optimal `-ary partition are proved.",1.3. Related Work,[0],[0]
These bounds do not imply constant approximations for the problem we consider here.,1.3. Related Work,[0],[0]
We start defining some notations employed throughout the paper.,2. Preliminaries,[0],[0]
"An input for 2-PMWIP is a pair (V, I), where V is a collection of non-null vectors in Rk with non-negative integer coordinates and I is a scaled impurity measure.",2. Preliminaries,[0],[0]
We assume that the vector ∑ v∈V v has no zero coordinates for otherwise we would have an instance with less than k classes.,2. Preliminaries,[0],[0]
"For a set of vectors L, the impurity I(L) of L is given by I( ∑ v∈L v).",2. Preliminaries,[0],[0]
"The impurity of a binary partition (L,R) of the set V is then I(L) + I(R).",2. Preliminaries,[0],[0]
"We use optI(V ) to denote the minimum possible impurity for a binary partition of V and, whenever the context is clear, we omit I from optI(V ).",2. Preliminaries,[0],[0]
"We say that a partition (L
∗, R∗) is optimal for input (V, I) iff I(L∗)",2. Preliminaries,[0],[0]
"+ I(R∗) = optI(V ).
",2. Preliminaries,[0],[0]
We use bold face to denote vectors.,2. Preliminaries,[0],[0]
"Given two vectors u = (u1, . . .",2. Preliminaries,[0],[0]
", uk) and v = (v1, . . .",2. Preliminaries,[0],[0]
", vk) we use u · v to denote their inner product and u ◦ v = (u1v1, . . .",2. Preliminaries,[0],[0]
", ukvk) to denote their component-wise (Hadamard) product.",2. Preliminaries,[0],[0]
"We use 0 and 1 to denote the vectors in Rk with all coordinates equal to 0 and 1, respectively.",2. Preliminaries,[0],[0]
For a non-null vector v ∈ Rk+ we use π(v) = v/‖v‖1 to denote the vector obtained by normalizing v w.r.t.,2. Preliminaries,[0],[0]
to the `1 norm.,2. Preliminaries,[0],[0]
"We use [m] to denote the set of the first m positive integers.
",2. Preliminaries,[0],[0]
Due to space constraints we omit most of the proofs.,2. Preliminaries,[0],[0]
"However, all of the them can be found in the full version available in the supplementary material.",2. Preliminaries,[0],[0]
"We are interested in the class C of scaled impurity measures I that satisfy
I(u) =",2.1. Impurity Measures,[0],[0]
‖u‖1 dim(u)∑ i=1,2.1. Impurity Measures,[0],[0]
"f ( ui ‖u‖1 ) , (P0)
where dim(u) is the dimension of vector u and f : R→ R is a function satisfying the following conditions:
1.",2.1. Impurity Measures,[0],[0]
"f(0) = f(1) = 0 (P1)
2.",2.1. Impurity Measures,[0],[0]
"f is strictly concave in the interval [0,1] (P2)
3.",2.1. Impurity Measures,[0],[0]
"For all 0 < p ≤ q ≤ 1
f(p) ≤ p q · f(q) + q · f
( p
q
) .",2.1. Impurity Measures,[0],[0]
"(P3)
Impurity measures satisfying conditions (P0)-(P2) are called frequency-weighted impurity measures with concave functions (Coppersmith et al., 1999).",2.1. Impurity Measures,[0],[0]
"These impurities measures are superadditive.
",2.1. Impurity Measures,[0],[0]
"Lemma 2.1 (Lemma 1 in (Coppersmith et al., 1999)).",2.1. Impurity Measures,[0],[0]
"If I satisfies (P0)-(P2) then for every vectors uL and uR in Rk+, we have I(uL + uR) ≥ I(uL) + I(uR).
",2.1. Impurity Measures,[0],[0]
"Although property (P3) is not particularly intuitive it can be shown that if a simple constraint (xf ′′(x) is non-increasing in [0,1]) is imposed on the second derivative f ′′ of f then (P3) is also satisfied.
",2.1. Impurity Measures,[0],[0]
"From (Coppersmith et al., 1999)",2.1. Impurity Measures,[0],[0]
we know that both IGini and IEntr satisfy (P0)-(P2).,2.1. Impurity Measures,[0],[0]
"It is not difficult to verify that they also satisfy (P3).
",2.1. Impurity Measures,[0],[0]
Lemma 2.2.,2.1. Impurity Measures,[0],[0]
"The Gini measure IGini and the Entropy measure IEntr belong to C.
The last lemma of this subsection shows that the impurity measures of our class satisfy a subsystem property.",2.1. Impurity Measures,[0],[0]
"It will be used in our analysis to relate the impurity of partitions for instances with k classes with the impurity of partitions for instances with 2 classes.
",2.1. Impurity Measures,[0],[0]
Lemma 2.3 (Subsystem Property).,2.1. Impurity Measures,[0],[0]
"Let I be an impurity measure in C. Then, for every u ∈ Rk+ and every d ∈",2.1. Impurity Measures,[0],[0]
"[0, 1]k,
I(u) ≤",2.1. Impurity Measures,[0],[0]
"I ( (u · d,u · (1− d)) )",2.1. Impurity Measures,[0],[0]
"+
I(u ◦ d) +",2.1. Impurity Measures,[0],[0]
I(u ◦,2.1. Impurity Measures,[0],[0]
"(1− d))
",2.1. Impurity Measures,[0],[0]
Proof’s sketch.,2.1. Impurity Measures,[0],[0]
"Note that by the definition of I , the desired inequality is invariant to scaling u; thus, we assume without loss of generality that ‖u‖1 = 1.",2.1. Impurity Measures,[0],[0]
"The left-hand side of the inequality is then ∑ i f(ui).
",2.1. Impurity Measures,[0],[0]
The result is then obtained by applying the following steps: (i) The subadditivity of f is used to obtain f(ui) ≤ f(diui) + f((1,2.1. Impurity Measures,[0],[0]
"− di)ui); (ii) Property (P3) is used, with p = diui and q = d · u, to upper bound f(diui); (iii) The same property is used with p = (1−di)ui and q = (1−d)·u to upper bound f(ui(1− di)) and (iv) The upper bound on f(ui) derived in the previous steps is added over all i to get the right-hand side of the desired inequality.",2.1. Impurity Measures,[0],[0]
We end our section of preliminaries presenting two results that give necessary conditions for optimal partitions.,2.2. Necessary conditions for optimal partitions,[0],[0]
"The first one, proved in (Breiman et al., 1984), yields to an O(n log n) time algorithm for 2-PMWIP when the number of classes k is 2.",2.2. Necessary conditions for optimal partitions,[0],[0]
"The second one works for 2-PMWIP, with arbitrary k, and it is a reduced version of a more general theorem that also works for `-ary partitions (Burshtein et al., 1992; Coppersmith et al., 1999).",2.2. Necessary conditions for optimal partitions,[0],[0]
Both results are stated using our notation.,2.2. Necessary conditions for optimal partitions,[0],[0]
"Theorem 2.4 (Theorem 4.5 of (Breiman et al., 1984)).",2.2. Necessary conditions for optimal partitions,[0],[0]
Let I be an impurity measure satisfying properties (P0)-(P2) and let V2 ⊆ R2+.,2.2. Necessary conditions for optimal partitions,[0],[0]
"Moreover, for every v = (v1, v2) ∈ V2 let r(v) = v1/‖v‖1.",2.2. Necessary conditions for optimal partitions,[0],[0]
"Furthermore, let Pj be the set containing the first j vectors of V2 when those are sorted with respect to r().",2.2. Necessary conditions for optimal partitions,[0],[0]
"Then (Pj , V2 \Pj), for some j ∈",2.2. Necessary conditions for optimal partitions,[0],[0]
"[n− 1], is an optimal partition for the instance (V2, I).",2.2. Necessary conditions for optimal partitions,[0],[0]
"Lemma 2.5 (Hyperplanes Lemma (Burshtein et al., 1992; Coppersmith et al., 1999)).",2.2. Necessary conditions for optimal partitions,[0],[0]
Let I be an impurity measure satisfying properties (P0)-(P2).,2.2. Necessary conditions for optimal partitions,[0],[0]
"If (L∗, R∗) is an optimal partition for an instance (V, I), then there is a vector d∗ ∈",2.2. Necessary conditions for optimal partitions,[0],[0]
Rk such that d∗·π(v) < 0,2.2. Necessary conditions for optimal partitions,[0],[0]
for every v ∈ L∗ and d∗·π(v) > 0,2.2. Necessary conditions for optimal partitions,[0],[0]
for every v ∈ R∗.,2.2. Necessary conditions for optimal partitions,[0],[0]
In this section we present approximation algorithms for finding binary partitions with reduced impurity.,3. Constant Approximations for Impurity Measures in C,[0],[0]
"We first analyze a general hyperplane-based procedure, and later specialize it to obtain different approximation algorithms.",3. Constant Approximations for Impurity Measures in C,[0],[0]
A direct consequence of the Hyperplanes Lemma (Lemma 2.5) above is that the search of the optimal partition can be reduced to the search of a direction in Rk.,3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"In fact, it is easy to see that we can normalize these directions to be in [0, 1]k, at the expense of working with non-homogeneous hyperplanes, as it is shown in the next proposition.
",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
Proposition 3.1.,3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"Let (L∗, R∗) be an optimal partition for input (V, I) and let d∗ ∈",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
Rk be such that d∗ · π(v) < 0 for every v in L∗ and d∗ · π(v) > 0,3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"for every v in R∗.
Then, there is a direction d ∈",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"[0, 1]k and a constant C such that d · π(v) < C for every v in L∗ and d · π(v) > C for every v in R∗.
The previous observation motivates the definition of a family of algorithms indexed by a direction d ∈",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"[0, 1]k.",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"The algorithm Bd searches for a partition with reduced impurity by considering all the n − 1 partitions of the input set V induced by the hyperplanes with normal d (Algorithm 1).
",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"Algorithm 1 Bd (V : collection of vectors, I: impurity measure)
1: For each v in V let r(v)",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"= (d · v)/‖v‖1 2: Rank the vectors in V according to r(v) 3: for j = 1, . . .",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
", n− 1 do 4:",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"Pj ← subset of V containing the j vectors with the largest value of r(·) 5: Evaluate the impurity of partition (Pj , V \ Pj) 6: end for 7: Return the partition (Pj∗ , V \ Pj∗) with the smallest
impurity found in the loop
We present a general analysis of the quality of solution produced by algorithm Bd when d ∈ {0, 1}k.",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"More specifically, we prove the following theorem:
Theorem 3.2.",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"Let I(Bd) be the impurity of the partition returned by Bd for an instance (V, I).",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"Then, for every direction d ∈ {0, 1}k we have
I(Bd) opt(V ) ≤ 1 + I(u ◦ d) + I(u ◦ (1− d))",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"mind′∈{0,1}k {I(u ◦ d′) + I(u ◦",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"(1− d′))}
(1)
",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
The bound given by this theorem is the basis for the approximation algorithms obtained in the next subsections since it motivates the use of a direction d such that I(u ◦ d) +,3.1. Analysis of a general hyperplane-based procedure,[0],[0]
I(u ◦,3.1. Analysis of a general hyperplane-based procedure,[0],[0]
(1− d)) is minimized.,3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"The remainder of this section is dedicated to prove Theorem 3.2.
",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"One of the key ideas of the proof is to establish a relation between the impurity of the partition obtained by Bd for the k-class instance (V, I) and the optimal impurity for the 2-class instance obtained by collapsing all the classes corresponding to the coordinates of d with value 0 into one “super class”, and all classes corresponding to the coordinates of d with value 1 into another super class.",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"Recall that each vector v ∈ V ⊆ Rk+, which corresponds to an attribute value, counts in its coordinates the number of examples of each of the k classes with the given attribute value.",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"Then, in the collapsed 2-class instance, this vector count becomes
simply ( ∑ i:di=1 vi, ∑ i:di=0 vi) = (v · d,v · (1 − d)).",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"Thus, define the operation collapsed : Rk+ → R2+ that maps v 7→ (v · d,v · (1 − d)).",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"Moreover, for a set of vectors S, define collapsed(S) as the set obtained applying collapsed() to each vector of S. Therefore, from a k-class instance (V, I) and a direction d ∈ {0, 1}k, we obtain the collapsed 2-class instance (collapsed(V ), I).
",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"The main motivation for looking at 2-class instances is that we know from Theorem 2.4 that an optimal partition can be obtained by sweeping the vectors according to some order, which is very similar to what algorithm Bd is doing.",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"To make this connection precise, let Ad be the algorithm obtained by modifying Line 5 of Bd so that the impurity of the binary partition (collapsed(Pj), collapsed(V",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"\ Pj)) is evaluated, rather than the impurity of (Pj , V \ Pj).",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"The following proposition states that the impurity Bd is at most that of Ad and that essentially the latter solves optimally the collapsed 2-class instance.
",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
Proposition 3.3.,3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"Let (L,R) be the partition returned by Ad for the k-class instance (V, I) and let I(Ad) be the impurity of (L,R).",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"Then: (i) I(Bd) ≤ I(Ad) and (ii) (collapsed(L), collapsed(R)) is an optimal partition for the 2-class instance (collapsed(V ), I).
",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"Given the first item of the above proposition, to prove Theorem 3.2, it suffices to upper bound the impurity of the partition (L,R) returned by Ad.",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"To simplify the notation, let u = ∑ v∈V v, uL = ∑ v∈L v, and uR = ∑ v∈R v. Also, for a direction d in {0, 1}k let d̄",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"= 1 − d. The impurity of the partition (L,R) constructed by Ad is
I(Ad) = I(uL) + I(uR).
",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
From the Subsystem Property (Lemma 2.3) we get I(Ad) ≤,3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"I ( (d · uL, d̄ · uL) )",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"+ I(d ◦ uL) + I(d̄ ◦ uL)
+ I ( (d · uR, d̄ · uR) )",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"+ I(d ◦ uR) + I(d̄ ◦ uR)
= opt(collapsed(V ))",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
+ I(d ◦ uL) + I(d̄ ◦ uL)+ I(d ◦ uR),3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"+ I(d̄ ◦ uR), (2)
where the last identity follows from the item (ii) of Proposition 3.3 because (d · uL, d̄ · uL) = ∑ v∈collapsed(L) v and
(d · uR, d̄ · uR) = ∑
v∈collapsed(R)",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"v.
Now we need to upper bound the last four terms in the RHS of the equation (2).",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
Using the superadditivity of I (Lemma 2.1),3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"we have
I(Ad) ≤ opt(collapsed(V ))",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"+ I(d ◦ u) + I(d̄ ◦ u) (3)
Now we devise lower bounds on opt(V ).",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"The first lower bound captures the intuitive fact that the impurity in the multi-class problem is at least as large as that in the collapsed 2-class problem.
",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
Lemma 3.4.,3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"For any input V and d ∈ {0, 1}k, we have opt(V ) ≥ opt(collapsed(V )).
",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"For our second lower bound, we consider the relaxed problem where each example corresponds to a distinct attribute value and the full class distribution is equal to that of V .",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"Formally, from the original instance (V, I) we consider the instance (V ′, I), where V ′ contains ui copies of the standard basis vector ei for i = 1, . . .",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
", k.
Let opt(V ′) be the optimal solution to this relaxed problem.",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"It is clear that opt(V ′) ≤ opt(V ), since any partition for the collection V can be realized in the collection V ′.
It follows from Lemma 2.5 that in the optimal partition of V ′ all vectors associated with the same class (standard basis vectors) end up on the same side of the partition.",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"Thus, the optimal solution for instance (V ′, I) corresponds to a partition of the set of classes, and so
opt(V ) ≥ opt(V ′) =",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"min
d′∈{0,1}k {I(u ◦ d′) + I(u ◦",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"(1− d′))} .
",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"Thus, by using the upper bound given by equation (3), the lower bound given by Lemma 3.4 and the previous inequality we obtain that the RHS of inequality (1) is an upper bound on the approximation ratio of algorithm Ad.",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"This bound together with the first item of Proposition 3.3 establish Theorem 3.2.
3.2.",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"The Hypercube Cover procedure
The HcC method simply returns the best Bd among all possible directions d ∈ {0, 1}k, hence it equals Bd′ for d′ satisfying
I(Bd′) =",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"min d∈{0,1}k I(Bd).
",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"To find d′, the algorithm examines all the 2k binary vectors in {0, 1}k.
",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"Given the general analysis provided by Theorem 3.2, it follows directly that HcC has the following approximation guarantee.",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
Theorem 3.5.,3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"HcC is a 2-approximation algorithm for every impurity measure in C.
We shall mention that HcC is closely related with the Twoing method proposed in (Breiman et al., 1984).",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"In fact, Twoing considers all 2k possibilities of grouping the k classes into 2 super classes and, for each possibility, optimally solves the 2-class problem; the best partition w.r.t.",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
the 2-class problem is then returned.,3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"In our notation, Twoing executes algorithm Ad, rather than Bd, for all directions d ∈ {0, 1}k and returns the partition, among the 2k generated, with minimum impurity with respect to the collapsed problem.
",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"It is also interesting to note that in (Breiman et al., 1984)",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"it was proved that if Twoing considers the Gini impurity for solving its 2-class problems then it finds a partition of attribute values that optimizes a specific objective function for the k-class problem that is significantly different from IGini.
3.3.",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
LargestClassAlone: an O(nk + n log,3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"n)-time constant approximation
A limitation of HcC is its running time, which is exponential on the number of classes k.",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"To address this issue, we show that a simple algorithm with O(nk+n log n) running time has a constant approximation for our class of impurity measures.
",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"Recall that we are using u to denote ∑
v∈V v.",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"Given u, let i∗ be the index of the coordinate corresponding to the largest value in u, that is, ui∗ ≥ ui for all i. Moreover, let ei∗ be the direction where all coordinates are 0 but coordinated i∗ whose value is 1.",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
We use LargestClassAlone (LCA for short) to denote the algorithm Bei∗ .,3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"The next result, which also relies on Theorem 3.2, shows that LCA has a constant approximation for our class.",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
Theorem 3.6.,3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"LCA is an (3 + √
3)−approximation for every impurity measure in the class C.",3.1. Analysis of a general hyperplane-based procedure,[0],[0]
"Here we show that we can obtain better approximations, with polynomial running time on n and k, when we focus on specific impurity measures.",4. Improved Approximations for Gini and Entropy,[0],[0]
"We consider both Gini and Entropy.
",4. Improved Approximations for Gini and Entropy,[0],[0]
The key idea for the improvement is to characterize the direction that minimizes the denominator of the upper bound on the approximation ration given by Theorem 3.2.,4. Improved Approximations for Gini and Entropy,[0],[0]
"It will be interesting to observe how Gini and Entropy behave significantly different in this sense, with the latter favoring balanced partition.",4. Improved Approximations for Gini and Entropy,[0],[0]
We prove that LCA is a 2-approximation algorithm for IGini.,4.1. Gini,[0],[0]
"To achieve this goal we show that, when I = IGini, ei∗ is a direction that minimizes the expression I(u ◦ d′)",4.1. Gini,[0],[0]
+ I(u ◦,4.1. Gini,[0],[0]
"(1 − d′)) that appears on the denominator of the righthand side of inequality (1).
",4.1. Gini,[0],[0]
Lemma 4.1.,4.1. Gini,[0],[0]
"The direction ei∗ satisfies
IGini(u ◦ ei∗)",4.1. Gini,[0],[0]
+,4.1. Gini,[0],[0]
IGini(u ◦ (1− ei∗)),4.1. Gini,[0],[0]
=,4.1. Gini,[0],[0]
"min
d∈{0,1}k {IGini(u ◦ d) +",4.1. Gini,[0],[0]
"IGini(u ◦ (1− d))}.
",4.1. Gini,[0],[0]
"A direct consequence of the previous lemma and Theorem 3.2 is that LCA gives a 2-approximation for IGini.
Theorem 4.2.",4.1. Gini,[0],[0]
"LCA is a 2-approximation for the Gini impurity measure.
",4.1. Gini,[0],[0]
A natural question is whether the analysis is tight.,4.1. Gini,[0],[0]
"The instance V = {(x, 0, 0), (0, x, 0), (c, 0, c)}, x >> c > 0, shows that this is the case for algorithm Aei∗ .",4.1. Gini,[0],[0]
For LCA (which is Bei∗ ) we are not aware whether the approximation is tight or not.,4.1. Gini,[0],[0]
The worst example we know has impurity 4/3 larger than the optimal one.,4.1. Gini,[0],[0]
"In this section we show that, for the Entropy impurity, LCA achieves an approximation ratio better than the one given by Theorem 3.6.
",4.2. Entropy,[0],[0]
"Let us define the balance of a direction d in {0, 1}k with respect to u as min{u ·d,u · (1−d)}.",4.2. Entropy,[0],[0]
"The next lemma implies that the most balanced direction with respect to u is the one that minimizes the denominator on the approximation ratio given by inequality (1).
",4.2. Entropy,[0],[0]
Lemma 4.3.,4.2. Entropy,[0],[0]
"Let d and d′ be directions in {0, 1}k.",4.2. Entropy,[0],[0]
"Then,
IEnt(u ◦",4.2. Entropy,[0],[0]
d) +,4.2. Entropy,[0],[0]
IEnt(u ◦,4.2. Entropy,[0],[0]
(1− d)),4.2. Entropy,[0],[0]
<,4.2. Entropy,[0],[0]
IEnt(u ◦ d′),4.2. Entropy,[0],[0]
+ IEnt(u ◦,4.2. Entropy,[0],[0]
"(1− d′)) (4)
if and only if d is more balanced than d′ with respect to u, that is, min{d ·u, (1−d) ·u} > min{d′ ·u, (1−d′) ·u}.
",4.2. Entropy,[0],[0]
"Let d∗ be the most balanced direction in {0, 1}k with respect to u.",4.2. Entropy,[0],[0]
The previous result together with Theorem 3.2 guarantee that algorithm Bd∗,4.2. Entropy,[0],[0]
is a 2-approximation for the Entropy impurity.,4.2. Entropy,[0],[0]
"The direction d∗ can be constructed in O(k ∑ v∈V ‖v‖1) time using an algorithm for the subset sum problem (Cormen et al., 1998).
",4.2. Entropy,[0],[0]
Theorem 4.4.,4.2. Entropy,[0],[0]
"There exists a 2-approximation algorithm for the entropy impurity measure that runs inO(k ∑ v∈V ‖v‖1) time.
",4.2. Entropy,[0],[0]
"For LCA we manage to prove the following bound.
",4.2. Entropy,[0],[0]
Theorem 4.5.,4.2. Entropy,[0],[0]
"LCA is a 3-approximation for the Entropy impurity measure.
",4.2. Entropy,[0],[0]
"Given Lemma 4.3, a straightforward reduction from PARTITION problem shows that 2-PMWIP is NP-hard even when I is the Entropy measure.
",4.2. Entropy,[0],[0]
Theorem 4.6.,4.2. Entropy,[0],[0]
"The 2-PMWIP for the Entropy impurity measure is NP-Hard.
",4.2. Entropy,[0],[0]
The complexity of the problem for the case where I is the scaled Gini impurity measure remains open.,4.2. Entropy,[0],[0]
To complement our theoretical study we report a number of experiments with the methods proposed/analyzed in the previous sections.,5. Experiments,[0],[0]
"Our first set of experiments is very similar to the set presented in (Coppersmith et al., 1999) except for a few details.",5.1. Evaluation of Splits Impurity,[0],[0]
"All experiments are Monte Carlo simulations with 10,000 runs, each using a randomly-generated contingency table for the given number of values n and classes k.",5.1. Evaluation of Splits Impurity,[0],[0]
By a contingency table we mean a matrix where each row corresponds to a distinct vector of the input V .,5.1. Evaluation of Splits Impurity,[0],[0]
"Each table was created by uniformly picking a number in {0, . . .",5.1. Evaluation of Splits Impurity,[0],[0]
", 7} for each entry.",5.1. Evaluation of Splits Impurity,[0],[0]
"This guarantees a substantial probability of a row/column having some zero frequencies, which is common in practice.",5.1. Evaluation of Splits Impurity,[0],[0]
"Differing from (Coppersmith et al., 1999), if all the entries corresponding to a value or a class are zero, we re-generate the contingency table, since otherwise the number of actual values and classes would not match n and k.
We compared the following splitting methods: HcC, LCA, SLIQext and PCext.",5.1. Evaluation of Splits Impurity,[0],[0]
"SLIQext is a variant, presented in (Coppersmith et al., 1999), of the SLIQ method proposed in (Mehta et al., 1996).",5.1. Evaluation of Splits Impurity,[0],[0]
"It starts with the partition (V, ∅) and then it greedily moves, from the ’left’ to the ’right’ partition, the vector that yields to the partition with minimum impurity until the the partition (∅, V ) is reached; the best partition found in this process is returned.",5.1. Evaluation of Splits Impurity,[0],[0]
"PCext is a method proposed in (Coppersmith et al., 1999) that defines the partition for the vectors in V by using a hyperplane in Rk whose normal direction is the principal direction of a certain contingency table associated with the instance.",5.1. Evaluation of Splits Impurity,[0],[0]
"According to the experiments reported in (Coppersmith et al., 1999), PCext and SLIQext outperformed other available methods, such as the Flip Flop method (Nadas et al., 1991), in terms of the impurity of the partitions found.
",5.1. Evaluation of Splits Impurity,[0],[0]
"Table 1 and 2 show, for different values of n and k, the percentage of times each method is at least as good as the other competitors for Gini and Entropy, respectively.",5.1. Evaluation of Splits Impurity,[0],[0]
"We only show results for k ≤ 9 because, for larger values of k, HcC becomes non-practical due to its running time.",5.1. Evaluation of Splits Impurity,[0],[0]
"Furthermore, we do not present results for small values of n because, in this case, the optimal partition can be found reasonably quick through an exhaustive search, hence there is no motivation for heuristics.
",5.1. Evaluation of Splits Impurity,[0],[0]
"In general, we observe an advantage of HcC for both the impurity measures, being much more evident for Entropy impurity.",5.1. Evaluation of Splits Impurity,[0],[0]
We also observe that LCA presents the worst results.,5.1. Evaluation of Splits Impurity,[0],[0]
"Additional experiments where we set the maximum possible value in the contingency table to 2 and 15, rather than to 7, presented similar behavior.
",5.1. Evaluation of Splits Impurity,[0],[0]
It is also interesting to observe how far the impurity of the partition generated by a splitting method may be with respect to the best partition found by the other methods.,5.1. Evaluation of Splits Impurity,[0],[0]
"To measure this distance, let us define the relative excess (in percentage) of a partition P w.r.t.",5.1. Evaluation of Splits Impurity,[0],[0]
a partition Q as 100 × (I(P )/I(Q)− 1).,5.1. Evaluation of Splits Impurity,[0],[0]
"The maximum relative excess observed for the partitions generated by HcC, with respect to the partitions generated by the other methods, was 2% and 1.9% for Gini and Entropy, respectively.",5.1. Evaluation of Splits Impurity,[0],[0]
"For SLIQext, the maximum relative excess observed was 9.4% for Gini and 14% for Entropy.",5.1. Evaluation of Splits Impurity,[0],[0]
"For PCext, we observed 3.7% for Gini and 21.6% for Entropy.",5.1. Evaluation of Splits Impurity,[0],[0]
"Finally, for LCA, we had 22.3% for Gini and 43.6% for Entropy.",5.1. Evaluation of Splits Impurity,[0],[0]
"These numbers suggest that the risk of finding a ‘bad’ partition is smaller when HcC is used, specially for the Entropy impurity.
",5.1. Evaluation of Splits Impurity,[0],[0]
Table 3 presents a comparison between the running time of the 4 methods for each configuration of n and k.,5.1. Evaluation of Splits Impurity,[0],[0]
"The numbers are relative to the running time of LCA, which is the fastest of them.",5.1. Evaluation of Splits Impurity,[0],[0]
"Among the other three methods, PCext ob-
tained the best results.",5.1. Evaluation of Splits Impurity,[0],[0]
"As expected, HcC is very competitive for small values of k and it becomes less competitive when k grows.",5.1. Evaluation of Splits Impurity,[0],[0]
"For the slowest configuration, n = 50 and k = 9, HcC took, in average, 0.38 seconds.",5.1. Evaluation of Splits Impurity,[0],[0]
We can also observe that SLIQext becomes less competitive as n grows.,5.1. Evaluation of Splits Impurity,[0],[0]
All the experiments were executed on a PC Intel i7-6500U CPU with 2.5GHz and 8GB of RAM.,5.1. Evaluation of Splits Impurity,[0],[0]
The algorithms were implemented in Python 3 using numpy and and are available in https://github.com/felipeamp/icml-2018.,5.1. Evaluation of Splits Impurity,[0],[0]
We also carried out a set of experiments to evaluate how the methods behave when they are used in decision tree induction.,5.2. Decision tree induction,[0],[0]
"Here we just present a summary of these experiments – a more complete complete description can be found in the supplementary material.
",5.2. Decision tree induction,[0],[0]
We employed 11 datasets in total.,5.2. Decision tree induction,[0],[0]
"Eight of them are from the UCI repository: Mushroom, KDD98, Adult, Nursery, Covertype, Cars, Contraceptive and Poker (Lichman, 2013).",5.2. Decision tree induction,[0],[0]
Two others are available in Kaggle: San Francisco Crime and Shelter Animal Outcome (SF-OpenData; AustinAnimal-Center).,5.2. Decision tree induction,[0],[0]
"The last dataset was created by translating texts from the Reuters database (Lichman, 2013) into phonemes, using the CMU pronouncing dictionary (CMU).",5.2. Decision tree induction,[0],[0]
"We shall note that these datasets were also used in (Laber & de A. Mello Pereira, 2018) where methods for splitting nominal attributes that do not rely on impurity measures are proposed.
",5.2. Decision tree induction,[0],[0]
We chose these datasets because they have at least 1000 samples and they either contain multi-valued attributes or attributes that can be naturally aggregated to produce multivalued attributes.,5.2. Decision tree induction,[0],[0]
"For datasets Cars, CoverType, Nursery and Contraceptive we add new nominal attributes that were obtained by aggregating some of the original ones.",5.2. Decision tree induction,[0],[0]
"Additional details are given in the supplementary material.
",5.2. Decision tree induction,[0],[0]
In this second set of experiments we build decision trees with depth at most 16.,5.2. Decision tree induction,[0],[0]
"We employed a 95% one-tailed
paired t-student test to compare the accuracy attained by the methods over 20 3-fold stratified cross-validations.",5.2. Decision tree induction,[0],[0]
Table 4 shows how LCA and HcC compare with each of the other methods with regards to the number of datasets in which they had statistically better/worse accuracy.,5.2. Decision tree induction,[0],[0]
"As an example, the entry associated with Entropy/PCExt in the top Table 4 shows that out of the 11 datasets, for the Entropy impurity, LCA was statistically better in 4 datasets while PCExt was better in none.
",5.2. Decision tree induction,[0],[0]
"Given the results on the previous section, we were not expecting a strong performance from LCA.",5.2. Decision tree induction,[0],[0]
"However, to our surprise, LCA was quite competitive, performing better than some of the other methods in these datasets, specially for the Entropy impurity measure.",5.2. Decision tree induction,[0],[0]
"HcC, as expected, had a good performance.",5.2. Decision tree induction,[0],[0]
"These results suggest that LCA is also an interesting alternative, specially when speed is an issue.",5.2. Decision tree induction,[0],[0]
In this paper we proved that the 2-PMWIP is NP-Hard and we devised algorithms with constant approximation guarantee for it.,6. Final Remarks,[0],[0]
"Furthermore, we reported experiments that suggest that our methods proposed are good candidates to be used in splitting nominal attributes with many values during decision tree/random forest induction.",6. Final Remarks,[0],[0]
"HcC has the advantage of generating partitions with lower impurity than other available methods while LCA has the advantage of being very fast.
",6. Final Remarks,[0],[0]
Some interesting questions remain open.,6. Final Remarks,[0],[0]
"The main one concerns the existence of a FPTAS for 2-PMWIP, that is, an algorithm that for every > 0 obtains an approximation (1+ ) with running time polynomial on n and 1/ .",6. Final Remarks,[0],[0]
"Another interesting question regards the existence of algorithms with provably approximation for the L-PMWIP, the most general problem where the values of an attribute have to be partitioned into at most L groups.",6. Final Remarks,[0],[0]
The problem of splitting attributes is one of the main steps in the construction of decision trees.,abstractText,[0],[0]
"In order to decide the best split, impurity measures such as Entropy and Gini are widely used.",abstractText,[0],[0]
"In practice, decision-tree inducers use heuristics for finding splits with small impurity when they consider nominal attributes with a large number of distinct values.",abstractText,[0],[0]
"However, there are no known guarantees for the quality of the splits obtained by these heuristics.",abstractText,[0],[0]
"To fill this gap, we propose two new splitting procedures that provably achieve nearoptimal impurity.",abstractText,[0],[0]
We also report experiments that provide evidence that the proposed methods are interesting candidates to be employed in splitting nominal attributes with many values during decision tree/random forest induction.,abstractText,[0],[0]
Binary Partitions with Approximate Minimum Impurity,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 665–675, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.
Knowledge Base Population (KBP) tasks, such as slot filling, show the particular importance of entity-oriented automatic relevant document acquisition. Rich, diverse and reliable relevant documents satisfy the fundamental requirement that a KBP system explores the nature of an entity. Towards the bottleneck problem between comprehensiveness and definiteness of acquisition, we propose a collaborative archiving method. In particular we introduce topic modeling methodologies into entity biography profiling, so as to build a bridge between fuzzy and exact matching. On one side, we employ the topics in a small-scale high-quality relevant documents (i.e., exact matching results) to summarize the life slices of a target entity (i.e., biography), and on the other side, we use the biography as a reliable reference material to detect new truly relevant documents from a large-scale partially complete pseudo-feedback (i.e., fuzzy matching results). We leverage the archiving method to enhance slot filling systems. Experiments on KBP corpus show significant improvement over stateof-the-art.",text,[0],[0]
Entity archiving is an entity-oriented document retrieval task.,1 Introduction,[0],[0]
"Towards a target entity of a specific type, such as the ones discussed in this paper, a person or an organization, the goal of entity archiving is to search and collect all relevant documents from large-scale data sets under limited prior knowledge of the entity.",1 Introduction,[0],[0]
"We limit our study to the regular English entity archiving, in which the prior knowledge contains the com-
monly used full name (formatted by English entity naming criteria) along with a gold-standard reference document, such as a news story on President “George W. Bush”.
",1 Introduction,[0],[0]
Entity archiving plays a fundamental role in KBP tasks.,1 Introduction,[0],[0]
It narrows down the range of the data source for knowledge discovery to small-scale closely related documents.,1 Introduction,[0],[0]
"Such documents, on one hand, contain informative content on a target entity, which is extremely favorable for background knowledge extraction.",1 Introduction,[0],[0]
"On the other hand, the documents provide definitive evidence for verifying the claimed identity of the entity.
",1 Introduction,[0],[0]
"As for KBP slot filling and verification tasks (Surdeanu and Ji, 2014), the archived relevant documents to an entity provide sufficient contexts (provenances) of the concrete instances (fillers) of the entity attributes (slots).",1 Introduction,[0],[0]
"See Figure 1, in which both the provenances support filler extraction, meanwhile the provenance 1 additionally provides the evidence to verify the fillers (e.g., is episcopalism the true religion of Bush?).
",1 Introduction,[0],[0]
"The main challenges of entity archiving are as
665
follows: 1) it is difficult to retrieve all relevant documents through exact matching at the level of entity name, because an entity can be mentioned in various forms, such as alternate names and abbreviations; 2) in contrast, fuzzy matching introduces a large amount of noise into retrieval results (see the examples in Figure 1), although it is capable of recalling an overwhelming majority of relevant documents; 3) inadequate prior knowledge makes it difficult to generate a full profile of an entity; 4) although pseudo-feedback is helpful to enrich the prior knowledge, traditional entity profiling (e.g., bag-of-words) methods establish vague boundaries among different life slices of an entity.",1 Introduction,[0],[0]
"For example, they are incapable of distinguishing the slice of the “Church Scientologist in Sea Organization” of Mark Fisher 1 from the freelance career as the “Corporate liaison to Miscavige”.",1 Introduction,[0],[0]
"As a result it is difficult to enhance the independent effects of different slices on the entity-document relevance determination.
",1 Introduction,[0],[0]
"To solve these problems, we propose a collaborative entity archiving (CEA) method.",1 Introduction,[0],[0]
"It employs the exact-matching based document retrieval to obtain a few high-quality reference documents, and leverages fuzzy matching for high-speed acquisition of adequate candidate documents (section 3).",1 Introduction,[0],[0]
"In addition, CEA uses the reference documents as prior knowledge to model the topic-level biography of an entity, and identifies the truly relevant documents from the candidates based on biography-document relevance (section 4).",1 Introduction,[0],[0]
Experiments show that CEA has substantial advantages over traditional retrieval methods (section 5.1).,1 Introduction,[0],[0]
We apply CEA to state-of-the-art slot filling systems.,1 Introduction,[0],[0]
Experimental results show that CEA provides consistent gains (section 5.2).,1 Introduction,[0],[0]
 Entity Search One research topic similar to entity archiving is entity search.,2 Related Work,[0],[0]
"Entity search aims to seek, collect, and rank entities associated with specific information needs (Balog et al, 2011).",2 Related Work,[0],[0]
"The TREC Enterprise track featured an expert finding task (Balog et al, 2008a): given a topic, return a ranked list of experts on the topic.",2 Related Work,[0],[0]
"In response to this problem, there have been considerable efforts on content based retrieval models, as well as feature
1 Mark Fisher: a PERSON query name in Slot Filing evaluation of 2014, ID=SF14_ENG_031
selection, such as proximity (Petkova and Croft, 2007), document priors (Hu et al, 2006; Zhu et al, 2010), expert-document associations (Balog and De-Rijke, 2008) and external evidence (Serdyukov and Hiemstra, 2008).
",2 Related Work,[0],[0]
"Since INEX was launched in 2002, which is an entity ranking task specific to structured data and multimedia (Demartini et al, 2010), structured features have been widely used in entity search, such as the most recent studies on Wikipedia links and categories (Vercoustre et al, 2008; Zhu et al, 2008; Jiang et al, 2009; Weerkamp et al, 2009; Kaptein and Kamps, 2009; Balog et al, 2011) and web link structure (Balog et al, 2008b; You et al, 2011; Blanco et al, 2011; Neumayer et al, 2012; Bron et al, 2013).",2 Related Work,[0],[0]
"
 ",2 Related Work,[0],[0]
"Slot Filling The goal of slot filling is to seek and extract the concrete instances (fillers) specific to multiple entity attributes (slots) from a large-scale textual data set (Ji et al., 2010 and 2011; Surdeanu, 2013; Surdeanu and Ji, 2014).",2 Related Work,[0],[0]
"The quality of the fillers largely depends on the performance of entity archiving and information extraction.
",2 Related Work,[0],[0]
"Related studies on archiving mainly employed traditional retrieval techniques, including query expansion and string matching (Ji and Grishman, 2011).",2 Related Work,[0],[0]
"A few studies involved document ranking and prioritizing by using probability model (Byrne and Dunnion, 2010; Roth et al, 2014) and statistical language model (Chrupala et al, 2010).
",2 Related Work,[0],[0]
"For filler extraction, great efforts were made to generate effective patterns and structure perceptrons by supervised learning and reasoning (Chen et al, 2010; Grishman and Min, 2010; Gao et al, 2010; Surdeanu et al, 2011; Louis et al, 2011; Kisiel et al, 2013).",2 Related Work,[0],[0]
"And effective feature selection and distant-supervision based classifiers have been explored (Lehmann et al, 2010; Artiles et al, 2011; Sun et al, 2011; Roth and Klakow, 2013; Roth et al, 2014).",2 Related Work,[0],[0]
"Recently active learning (Angeli et al, 2014), truth-finding (Yu et al, 2014) as well, scanning (Yu et al., 2015) and ensemble learning (Viswanathan et al., 2015) were introduced to this field.
 ",2 Related Work,[0],[0]
"Brief Summary In all, entity search concentrates on the analysis of a single specific aspect of an entity, which is of interest or related to a domain.",2 Related Work,[0],[0]
"In the expert finding task, only academic careers of person entities (potential experts) are of concern in entity-document relevance determination.",2 Related Work,[0],[0]
"By contrast, for the sake of comprehensive understanding of an entity, entity archiving necessarily
takes multiple and diverse aspects into account, such as a person’s career, family, religion, sociality, academics, etc.",2 Related Work,[0],[0]
"Due to the difference in goals, entity search techniques cannot be used directly to solve entity archiving problems.
",2 Related Work,[0],[0]
The performance of conventional retrieval techniques was generally limited due to the lack of precise modeling of the characteristics of an entity.,2 Related Work,[0],[0]
Sparse prior knowledge and absence of effective profiling methods cause difficulties in characterizing the entity.,2 Related Work,[0],[0]
"The rest of the paper will be about knowledge acquisition and partition, as well as the collaborative method, along with a topic-level biographical profiling method.",2 Related Work,[0],[0]
We use string matching based retrieval methods to acquire relevant documents.,3 Prior Knowledge Acquisition,[0],[0]
It is worth considering that the acquired documents are not straightforwardly defined as the final entity archiving results.,3 Prior Knowledge Acquisition,[0],[0]
"As we will show in this section, some of them are reliable, while others are full of noise.",3 Prior Knowledge Acquisition,[0],[0]
"Instead, we regard them as the prerequisite knowledge for a coarse-to-fine processing.
",3 Prior Knowledge Acquisition,[0],[0]
"In the retrieval phase, a query Q is formulated as the full name of the target entity, while a document D is represented as a string of words.",3 Prior Knowledge Acquisition,[0],[0]
D is determined to be relevant only if it contains some words that match,3 Prior Knowledge Acquisition,[0],[0]
Q. Accordingly we name such words as entity mentions.,3 Prior Knowledge Acquisition,[0],[0]
Both Q and D are preprocessed by tokenization and stop-word filtering.,3 Prior Knowledge Acquisition,[0],[0]
Other commonly used preprocessing steps (stemming and lemmatization) are disabled because they may cause confusion between entity mentions and common words.,3 Prior Knowledge Acquisition,[0],[0]
"Table 1 shows the examples where the underlined words in <1> denotes an entity mention but <2> does not.
",3 Prior Knowledge Acquisition,[0],[0]
"We employ two matching methods for the rel-
evance determination: exact and fuzzy matching.
Exact matching (EM) requires that a sequence of successive words in D exactly matches Q. By EM, entity archiving regards a full entity name
as an indivisible word-order-fixed unit.",3 Prior Knowledge Acquisition,[0],[0]
"Accordingly it only acquires the documents which contain the entity mentions in the form of completely-preserved full name.
",3 Prior Knowledge Acquisition,[0],[0]
"Fuzzy matching (FM) relaxes the conditions to a large extent, allowing Q to be split into nonadjacent words.",3 Prior Knowledge Acquisition,[0],[0]
"In particular, it supports the change in word order as well as partial match.",3 Prior Knowledge Acquisition,[0],[0]
"By FM, entity archiving is able to retrieve documents that contain the entity mentions in the form of separated, pruned and/or reordered names.",3 Prior Knowledge Acquisition,[0],[0]
"Table 2 shows some examples of using these matching methods, where the mark “•” denotes the available methods for a certain form of entity mention.
",3 Prior Knowledge Acquisition,[0],[0]
Mark Fisher (PER) <,3 Prior Knowledge Acquisition,[0],[0]
"ID: SF14_ENG_031 >
Mark Fisher, Sea Org member.",3 Prior Knowledge Acquisition,[0],[0]
"(exact)
",3 Prior Knowledge Acquisition,[0],[0]
"Availability: EM (•) FM (•)
Mark, husband of Julie Fisher.",3 Prior Knowledge Acquisition,[0],[0]
(separation),3 Prior Knowledge Acquisition,[0],[0]
"Fisher had been Miscavige’s aide for 7 years.(pruning) Fisher’s first name, Mark, is impressive due to his inconceivable career change.",3 Prior Knowledge Acquisition,[0],[0]
"(reordering)
Availability: EM ( ) FM (•)
",3 Prior Knowledge Acquisition,[0],[0]
Table 2:,3 Prior Knowledge Acquisition,[0],[0]
"Examples of string matching results
EM and FM have substantially different advantages and disadvantages in entity archiving.",3 Prior Knowledge Acquisition,[0],[0]
"Table 3 shows the performance of EM and FM based entity archiving on KBP corpus (Surdeanu, 2013).",3 Prior Knowledge Acquisition,[0],[0]
We will introduce the corpus in details in Section 5.,3 Prior Knowledge Acquisition,[0],[0]
EM yields precise archiving results because the constraint conditions are helpful to reduce uncertainty in string matching.,3 Prior Knowledge Acquisition,[0],[0]
"In contrast FM-based archiving is able to match entity name mentions with various forms, and thus it achieves higher recall.
",3 Prior Knowledge Acquisition,[0],[0]
"However FM generally introduces much noise,
namely those mistakenly retrieved irrelevant documents.",3 Prior Knowledge Acquisition,[0],[0]
The documents are recalled because some pseudo entity mentions they contain can easily satisfy the constraints of fuzzy matching.,3 Prior Knowledge Acquisition,[0],[0]
See examples of the pseudo mentions in Table 4.,3 Prior Knowledge Acquisition,[0],[0]
"As a result, FM yields a very low precision score.
",3 Prior Knowledge Acquisition,[0],[0]
Undoubtedly it is helpful for global optimization of entity archiving to make full use of the advantages of EM and FM.,3 Prior Knowledge Acquisition,[0],[0]
"In view of the abovementioned investigation, we partition the string matching results into two parts, exact and fuzzy ones, which are used as reliable prior knowledge (named reference source) and unrefined prior knowledge (candidate source) respectively.",3 Prior Knowledge Acquisition,[0],[0]
"Most documents in the reference are truly related to the target entity but the scale is not big (see Recall of EM in Table 3), while the candidate is full of both true answers and noise (see Precision and Recall of FM in Table 3), respectively.",3 Prior Knowledge Acquisition,[0],[0]
"As we will show in the next section, the final archiving results are generated by synthesizing the sources in a collaborative coarse-to-fine way.",3 Prior Knowledge Acquisition,[0],[0]
We propose a Collaborative Entity Archiving approach (CEA for short).,4 Collaborative Entity Archiving (CEA),[0],[0]
CEA synthesizes the reference source and candidate source in a collaborative manner (section 4.1) through a biography-document relevance determination method (section 4.2 and 4.3).,4 Collaborative Entity Archiving (CEA),[0],[0]
"In addition, CEA involves mention disambiguation and query expansion in pre-processing to optimize the quality of both sources (section 4.4)",4 Collaborative Entity Archiving (CEA),[0],[0]
"CEA models the biography of an entity by using the topics in the reference source, in which, each topic serves as the description of a slice of life of the target entity (life slice for short), as shown in Figure 2.",4.1 Overall Framework of CEA,[0],[0]
"The Life slice means an episode in the whole story of the entity, which may represent an event, state or scenario at a certain moment, such as a person’s birth or an organization’s establishment.
",4.1 Overall Framework of CEA,[0],[0]
"CEA pulls out a document from the candidate source, one by one, and measures the biographydocument relevance at the topic level.",4.1 Overall Framework of CEA,[0],[0]
"By using a relevance threshold as the discrimination factor, CEA either preserves the document if it is rele-
vant, or filters otherwise.",4.1 Overall Framework of CEA,[0],[0]
"Meanwhile, CEA adds the newly found relevant documents to the reference source, and updates the biography by reshaping life slices (i.e., topics).",4.1 Overall Framework of CEA,[0],[0]
"CEA iteratively goes through the process of biography formulation, biography-document relevance measurement and determination until a condition is satisfied.",4.1 Overall Framework of CEA,[0],[0]
Finally CEA selects all the preserved documents in the reference source as the final output.,4.1 Overall Framework of CEA,[0],[0]
Figure 2 shows the framework.,4.1 Overall Framework of CEA,[0],[0]
"We design a generative approach to estimate the biography-document relevance r, which calculates the conditional probability that a candidate document D generates the biography B:
 ( | )r P B D (1)
",4.2 Biography-Document Relevance Models,[0],[0]
"In total we leverage three probabilistic models
for modeling B and D, including relevance model, topic model and context-level topic model.",4.2 Biography-Document Relevance Models,[0],[0]
"Then we introduce Hellinger Distance (Lindsay, 1994) into relevance measurement.
 ",4.2 Biography-Document Relevance Models,[0],[0]
"Relevance Model (RM) Generally, Relevance Model (RM) (Huang and Croft, 2009) refers to the probability distribution over all words conditioned on their occurrences in a set of previously-known relevant documents (or high-quality pseudo-relevant documents), i.e., , ( | )w V P w R  , where V is the vocabulary, R is the document set, and P(w|R) can be estimated by TF-IDF.",4.2 Biography-Document Relevance Models,[0],[0]
RM is often used in combination with Document Model (DM).,4.2 Biography-Document Relevance Models,[0],[0]
"Similar to RM, DM refers to the probability distribution over words in a particular document, i.e., , ( | )w V P w D  .",4.2 Biography-Document Relevance Models,[0],[0]
The relevance between R and D is normally determined by the agreement of RM and DM.,4.2 Biography-Document Relevance Models,[0],[0]
"The agreement can be estimated with Hellinger Distance between the models:
2 ( | ) ( ( | ) ( | ))
w V
H RM DM P w R P w D

  (2)
RM is a widely-used probabilistic model for information retrieval.",4.2 Biography-Document Relevance Models,[0],[0]
"It determines the relevance of a document to an object in accordance with homogeneousness in content between the document and the relevant documents of the object.
",4.2 Biography-Document Relevance Models,[0],[0]
"For an entity, in our case, we generate RM on the reference source, and regard it as the probabilistic model of a macro-level all-embracing biography B over the prior knowledge R. For a candidate document D, the biography-document relevance r is measured with Hellinger Distance between RM and DM: P(B|D)=H(RM|DM).",4.2 Biography-Document Relevance Models,[0],[0]
"We
will demonstrate the effect of RM heavily relies on the quality of reference source in experiments.
 ",4.2 Biography-Document Relevance Models,[0],[0]
"Topic Model (TM) Empirically, RM is coarse-grained.",4.2 Biography-Document Relevance Models,[0],[0]
"It mixes up different, separate and incoherent life slices of an entity.",4.2 Biography-Document Relevance Models,[0],[0]
"A more serious problem is that RM assigns uneven weights to life slices, giving excessive weights to the words about the popular slices, but low or even zeroth weights to the unpopular ones.",4.2 Biography-Document Relevance Models,[0],[0]
"A popular slice is defined as the slice of greater concern, which is normally frequently mentioned in the reference source, such as the slice of “the career of George W. Bush as the President” (high weight) versus “his childhood” (low weight).",4.2 Biography-Document Relevance Models,[0],[0]
"As a result, the RM based biography-document relevance is only helpful to identify and recall the documents relevant to the popular slices but not to the unpopular ones.
",4.2 Biography-Document Relevance Models,[0],[0]
"As a modification, we employ Topic Model (TM) for biography modeling.",4.2 Biography-Document Relevance Models,[0],[0]
We define a topic in the reference source as an independent finegrained representation for a microscopic life slice.,4.2 Biography-Document Relevance Models,[0],[0]
Accordingly we treat the biography as a bucket of topics.,4.2 Biography-Document Relevance Models,[0],[0]
"We leverage Latent Dirichlet Allocation (LDA) (Blei et al, 2003; Wei and Croft, 2006) for topic discovery and modeling in the reference source.",4.2 Biography-Document Relevance Models,[0],[0]
"A topic is modeled as a probability distribution over all words in lexicon conditioned on the association of the words with the topic, denoted as w V  , P(w|tR), in which tR refers to a topic in the reference source, representing a life slice s. Table 5 shows partial topic models (slices) in the reference source of Mark Fisher, where the highlighted probability values by a box reveal the words that well characterize a topic (slice).",4.2 Biography-Document Relevance Models,[0],[0]
"In the same way, we survey the topics tC in the candidate source, modeled as w V  , P(w|tC).",4.2 Biography-Document Relevance Models,[0],[0]
"It is worth noting that those topics (tC) may represent anything, namely the slices of the target entity or namesakes, related or unrelated events, etc.",4.2 Biography-Document Relevance Models,[0],[0]
"It means they are full of noise.
",4.2 Biography-Document Relevance Models,[0],[0]
"In practice, given a target entity, its reference source (exact matching results) is a subset of the candidate source (fuzzy matching results).",4.2 Biography-Document Relevance Models,[0],[0]
"We picked the reference source out of the candidate to parse topics independently, forming the set of tR. Meanwhile, we parse topics in the candidate source to form the set of tC. Benefitting from the separate treatment, some of the truly related topics (tR) to the entity (correct slices) can be collected along with less noise.",4.2 Biography-Document Relevance Models,[0],[0]
"Using the topics as references, we detect the relevant documents in the candidate source in terms of the topic-level biography-document relevance P(B|D).
",4.2 Biography-Document Relevance Models,[0],[0]
"Given a document D in the candidate source, we transform P(B|D) into the combination of topic-document relevance of all topics in the reference source.",4.2 Biography-Document Relevance Models,[0],[0]
"We measure the topic-document relevance with the conditional probability P(tR|D) that the topic tR occurs in the document D. Accordingly, P(B|D) is estimated by:
( | ) ( | )
log( ) log ( | )
R
R
R
t
R
t
r P B D P t D
r P t D
 


 (3)
where, we incorporate the log likelihood into the numerical calculation for the sake of nonzero joint probability.
",4.2 Biography-Document Relevance Models,[0],[0]
"Due to the separate topic modeling procedures for the reference and candidate sources, the probability P(tR|D) ̶ a topic tR in the reference source occurs in a candidate document D ̶ cannot be obtained directly.",4.2 Biography-Document Relevance Models,[0],[0]
"To solve the problem, we introduce the joint probability of topic-topic relevance between topics (tR) in reference and topics (tC) in candidate (see the mode in Figure 2) into the probability calculation:
, ( | ) ( | ) ( | )
C C
R R R R C C
t T
t T P t D P t t P t D

    (4)
where TR is the set of all topics in the reference source while TC is the candidate.",4.2 Biography-Document Relevance Models,[0],[0]
"The topic-topic relevance P(tR|tC) is approximated by Hellinger distance estimation between the topic models of tR and tC. As a whole, we measure the biographydocument relevance as:
2
log( ) log ( | ) ( | )
log ( | ) ( | )
",4.2 Biography-Document Relevance Models,[0],[0]
"log ( | ) ( | )
log( ( | ) ( | ))",4.2 Biography-Document Relevance Models,[0],[0]
"( | )
R R C C
R R C C
R R C C
R R C C
R C C
t T t T
R C C
t T t T
R C C
t T t T
R C C
t T t T w V
r P t t P t D
P t t P t D
H t t P t D
P w t P w t P t D
 
 
 
  



 
 
 
 
  
(5)
",4.2 Biography-Document Relevance Models,[0],[0]
"We employ the toolkit GibbsLDA++ 2 in topic modeling, which is an implementation of LDA using Gibbs sampling (Porteous et al, 2008).",4.2 Biography-Document Relevance Models,[0],[0]
GibbsLDA++ makes it easy to parse the topics in a document set as well as estimate topic models P(w|t).,4.2 Biography-Document Relevance Models,[0],[0]
"Besides, GibbsLDA++ offers the probability over topics in generating a specific document, facilitating the estimation of P(tC|D) in equation (5).",4.2 Biography-Document Relevance Models,[0],[0]
"Table 6 shows the operating parameters what we set in experiments, where the ones {α, β} were set as the default values while the iterative number num is an empirical value.
",4.2 Biography-Document Relevance Models,[0],[0]
The necessary precondition for GibbsLDA++ in topic partition is to define the number Nt of potential topics in a set of documents.,4.2 Biography-Document Relevance Models,[0],[0]
"We execute the Hierarchical Dirichlet Processes (Teh et al, 2005), abbr., HDP, to predict Nt.",4.2 Biography-Document Relevance Models,[0],[0]
"HDP is similar to current hierarchical information organization methods, such as the hierarchical clustering (Kummamuru et al, 2004), unsupervised and coarse-to-fine grained.",4.2 Biography-Document Relevance Models,[0],[0]
"Hence HDP is useful in exploring the basic rules of topic partition in an automatic way, such as number and granularity.",4.2 Biography-Document Relevance Models,[0],[0]
"We employ HDP to estimate the number (Nt) of all possible topics in reference source and candidate separately, acquiring two Nt for each target entity, one per source.
 ",4.2 Biography-Document Relevance Models,[0],[0]
Context-level Topic Model (CTM),4.2 Biography-Document Relevance Models,[0],[0]
"In consideration of the reliability of contexts in representing closely-related life slices to the entity, we use the contexts around entity mentions to improve the slice-oriented topic modeling.
",4.2 Biography-Document Relevance Models,[0],[0]
A context consists of the words co-occurring with an entity mention in a radius-fixed text span or syntactic or dependent structure (see instructions in Table 7).,4.2 Biography-Document Relevance Models,[0],[0]
"Given a target entity, the entity mention in the reference source is its full name.",4.2 Biography-Document Relevance Models,[0],[0]
"The union of all contexts in the source defines the vocabulary VR that most probably represent
2 http://gibbslda.sourceforge.net/
the slices of the entity.",4.2 Biography-Document Relevance Models,[0],[0]
"In the candidate source, on the contrary, the entity mention can be a reordered, separated or pruned entity name, as well as abbreviation or alias, such as GWB (abbr.) and Dubya (alias) of George W. Bush.",4.2 Biography-Document Relevance Models,[0],[0]
"Different from the cases in reference, the vocabulary VC obtained from the contexts in candidate are closely related to diverse entities or other objects with the same name (see Table 2&4).
",4.2 Biography-Document Relevance Models,[0],[0]
"CTM measures the biography-document relevance in the same way with TM, estimating the topic-level P(B|D) by equation (5).",4.2 Biography-Document Relevance Models,[0],[0]
The only difference lies in the available words in topic model P(w|t).,4.2 Biography-Document Relevance Models,[0],[0]
"For the ones not included in VR and VC, CTM assigns a weight zero in topic model no matter what GibbsLDA++ does.",4.2 Biography-Document Relevance Models,[0],[0]
"For each target entity, CEA measures the biography-document relevance for all documents in the candidate source.",4.3 Unsupervised Threshold Estimation,[0],[0]
"In the light of the relevance scores, CEA ranks the documents and sets a clear threshold θ to cut off the long tail in the ranking list, in other words, filtering the documents that have a relevance score lower than θ.",4.3 Unsupervised Threshold Estimation,[0],[0]
"The preserved documents will be added to the reference source for both biography reformulation and archiving result output.
",4.3 Unsupervised Threshold Estimation,[0],[0]
"We estimate the threshold by learning density distribution of documents over relevance scores (Arampatzis et al, 2009).",4.3 Unsupervised Threshold Estimation,[0],[0]
Density means the number of documents that have similar relevance scores.,4.3 Unsupervised Threshold Estimation,[0],[0]
The distribution is produced by densities within all interval ranges of relevance score.,4.3 Unsupervised Threshold Estimation,[0],[0]
"Our empirical findings show that the density distribution fits a mixture of two Gaussians, where the highly relevant documents and the irrelevant ones distribute in two separate Gaussian peaks respectively.",4.3 Unsupervised Threshold Estimation,[0],[0]
"Accordingly we define the threshold as the range of relevance score at the extreme point between the peaks, as shown in Figure 3.
",4.3 Unsupervised Threshold Estimation,[0],[0]
"In order to detect the extreme point, we firstly use a cubic polynomial function to approximate
the density distribution.",4.3 Unsupervised Threshold Estimation,[0],[0]
"Second, we go through the integral solution of the function in every finegrained interval range of relevance score (interval is set as max(r)/100).",4.3 Unsupervised Threshold Estimation,[0],[0]
We finally detect the extremum between peaks.,4.3 Unsupervised Threshold Estimation,[0],[0]
"The threshold is initialized during runtime exclusively for each target entity, without training.",4.3 Unsupervised Threshold Estimation,[0],[0]
It is re-estimated every time when the biography is reformulated.,4.3 Unsupervised Threshold Estimation,[0],[0]
CEA identifies relevant documents in candidate source and moves them to reference.,4.4 Termination Criterion for Iteration,[0],[0]
"Then CEA reshapes statistical models (RM, TM or CTM) over the updated sources.",4.4 Termination Criterion for Iteration,[0],[0]
"In terms of the reformed models, CEA starts a new round of relevance determination, data movement, and statistical modeling.",4.4 Termination Criterion for Iteration,[0],[0]
"CEA keeps it going until meeting any of the following termination criterions:
 T1: No more new topic occurs in the reference source (Nt doesn’t change).  ",4.4 Termination Criterion for Iteration,[0],[0]
T2:,4.4 Termination Criterion for Iteration,[0],[0]
"The number of the documents in Peak1 (Figure 3) begins to increase continuously.
",4.4 Termination Criterion for Iteration,[0],[0]
T2 is triggered if T1 loses its efficacy.,4.4 Termination Criterion for Iteration,[0],[0]
"The in-
validation happens when some general slices (i.e., general topics) are mistakenly introduced into the reference source, causing large-scale irrelevant document to be recalled and moved to reference.",4.4 Termination Criterion for Iteration,[0],[0]
"It will dramatically increase the number (Nt) of topics in a long term in the iterative procedure, driving CEA to capture more irrelevant documents.",4.4 Termination Criterion for Iteration,[0],[0]
Thus Peak1 will be enlarged continuously.,4.4 Termination Criterion for Iteration,[0],[0]
"However, if as expected, Peak1 should be narrowed with increasing the iteration times because:
 Fewer new related slices appear.  ",4.4 Termination Criterion for Iteration,[0],[0]
"The number of documents related to the
slices is less than that in previous iterations.",4.4 Termination Criterion for Iteration,[0],[0]
"In the preprocessing phase, we improve the precision of EM because higher-quality EM results can offer more reliable reference documents for biography modeling.",4.5 Optimization of EM and FM,[0],[0]
"In addition, we expand queries for FM to recall a larger number of relevant documents.",4.5 Optimization of EM and FM,[0],[0]
"It is helpful to minimize the loss of relevant documents before proceeding to CEA.
",4.5 Optimization of EM and FM,[0],[0]
"To improve EM, we focus on identifying the common words that completely match the full name of the target entity.",4.5 Optimization of EM and FM,[0],[0]
"The words normally are elusive and easily treated as a correct entity name, called deceptive name, see that in (1).
",4.5 Optimization of EM and FM,[0],[0]
"(1) Countrywide Financial <ORG> True: Countrywide Financial Corporation.
",4.5 Optimization of EM and FM,[0],[0]
"Deceptive: Bank of America purchased the failing countrywide financial for $4.1 billion.
",4.5 Optimization of EM and FM,[0],[0]
"To reduce EM errors caused by deceptive names, we use name tagging (Miller et al, 2004) to distinguish deceptive names and true names.",4.5 Optimization of EM and FM,[0],[0]
"Further, we filter the documents that are mistakenly retrieved based on the match between a deceptive name and the full-entity-name based query Q.
We leverage an Alternate Name Table (ANT) for query expansion.",4.5 Optimization of EM and FM,[0],[0]
ANT is a mapping table between entity name and alternate name.,4.5 Optimization of EM and FM,[0],[0]
"An alternate name is either generated according to the naming conventions (Burman et al, 2011), such as abbreviation, suffixation and revivification.",4.5 Optimization of EM and FM,[0],[0]
"Some alternative names were extracted from knowledge base through redirect links (Nia et al, 2014), such as nicknames in Wikipedia dumps.",4.5 Optimization of EM and FM,[0],[0]
"For an entity, we reformulate query Q by the combination of the pre-assigned full entity name and all possible alternate names in ANT, see (2).
",4.5 Optimization of EM and FM,[0],[0]
"(2) Initial Q: Countrywide Financial Corporation.
",4.5 Optimization of EM and FM,[0],[0]
"Expanded: Countrywide Financial+Corporation
+Corp. +Company +Co. +Ltd. +Co Ltd. +CFC.
",4.5 Optimization of EM and FM,[0],[0]
"We use the expanded query for FM to increase the number of relevant documents in the candidate source, regardless of whether or not it will introduce a larger scale of new noises.",4.5 Optimization of EM and FM,[0],[0]
We evaluate our methods on KBP 2013 corpus.,5 Experiments,[0],[0]
"The corpus contains 2.1M texts collected from web pages, newswires and discussion forums.",5 Experiments,[0],[0]
"From this corpus, a slot filling system is required to find fillers for 41 types of slots that represent the attributes of the target entities.",5 Experiments,[0],[0]
"There are 25 slot types of person and 16 slot types of organization, such as a Person’s birth date and an Organization’s founder (Ji et al., 2010 and 2011).
",5 Experiments,[0],[0]
"KBP 2013 includes 100 target entities and ground-truth fillers and provenances, where the ground-truth data was obtained by manual verification and annotation on the pool of system outputs.",5 Experiments,[0],[0]
"The provenances contain the IDs of documents relevant to target entities and fine-grained text spans which illustrate the eligibility of fillers.
",5 Experiments,[0],[0]
"In total there are 1,851 gold standard relevant documents available for the evaluation of entity archiving.",5 Experiments,[0],[0]
However the data is far from enough because it only covers a small portion of all relevant documents in the pool.,5 Experiments,[0],[0]
"Most are excluded since KBP annotators ignore relevant documents in which there isn’t any filler for the assigned slots or, although exists, the fillers were inexactly identified by Slot Filling systems.",5 Experiments,[0],[0]
"Therefore, we manually went over the pool and extracted 4,405 relevant documents as our ground-truth.",5 Experiments,[0],[0]
"We evaluate the entity archiving methods by micro and macro Precision (P), Recall (R) and F metrics.",5.1 Archiving Results and Analysis,[0],[0]
"Table 8 shows the main results.
 ",5.1 Archiving Results and Analysis,[0],[0]
"Overall Archiving Results Overall, the proposed CEA methods perform much better than the string matching based entity archiving methods (i.e., EM and FM).
",5.1 Archiving Results and Analysis,[0],[0]
"In addition the methods outperform a randomsampling based CEA (baseline), which randomly selects a certain number of documents (candidates) from the candidate source to combine with reference source straightforwardly (for final archiving results generation).",5.1 Archiving Results and Analysis,[0],[0]
"The sampling number is set to be the same as the number of candidates eventually archived by RM-based CEA.
cates after optimization)
",5.1 Archiving Results and Analysis,[0],[0]
Table 9 shows the number of candidates archived from the candidate source by all kinds of CEA methods.,5.1 Archiving Results and Analysis,[0],[0]
"It demonstrates that the biography-based CEAs yield higher precision (Table 8) after introducing the same or smaller number of candidates in the reference source, revealing the positive effect of biography modeling on entity-oriented document relevance determination.
 ",5.1 Archiving Results and Analysis,[0],[0]
Reliability versus Comprehensiveness CEA achieves higher precision by using the optimized EM results as reference source.,5.1 Archiving Results and Analysis,[0],[0]
It demonstrates the importance of reliable prior knowledge for entity understanding as well as detecting relevant documents.,5.1 Archiving Results and Analysis,[0],[0]
"However, the reference source causes lower recall scores of all CEA methods.",5.1 Archiving Results and Analysis,[0],[0]
The reason lies in reduction of prior knowledge.,5.1 Archiving Results and Analysis,[0],[0]
"As shown in Table 8, the re-
fined reference source (i.e., optimized EM results) covers only 24.6% of all relevant documents, which is far less than the coverage before optimization (nearly 40%).
",5.1 Archiving Results and Analysis,[0],[0]
"The reduced prior knowledge provides fewer available life slices of an entity for constructing an informative biography, inevitably resulting in missing some relevant documents.",5.1 Archiving Results and Analysis,[0],[0]
"In order to confirm this, we regard the 41 KBP slot types as some readily-made visible life slices, and use the manual annotations of the slot fillers to verify whether a life slice appears in a relevant document.",5.1 Archiving Results and Analysis,[0],[0]
"For example, the filler “Corporate liaison of the slot Title reveals the slice of freelance career of Mark Fisher.",5.1 Archiving Results and Analysis,[0],[0]
Then we figure out the coverage rate of life slices for both the original reference source and,5.1 Archiving Results and Analysis,[0],[0]
"the refined.
Figure 4 exhibits the coverage rates for 5 most frequently occurred life slices.",5.1 Archiving Results and Analysis,[0],[0]
"The coverage rate is calculated by the number of reference sources that contain a specific life slice versus 100, i.e., the number of reference sources for the 100 KBP entities (one per entity).",5.1 Archiving Results and Analysis,[0],[0]
"It can be found that the refined reference sources miss lots of life slices.
",5.1 Archiving Results and Analysis,[0],[0]
"Figure 4: Coverage rates of life slices (for top 5)
 ",5.1 Archiving Results and Analysis,[0],[0]
Comparison among Biography Models RM is biased towards the popular life slices in biography modeling.,5.1 Archiving Results and Analysis,[0],[0]
"The reasons are as following: 1) RM gives greater weights to the highfrequency words, and 2) popular slices are of much greater public interest and hence frequently mentioned in relevant documents.",5.1 Archiving Results and Analysis,[0],[0]
"However, some entities not only share similar names but similar popular slices, such as the religious vocation of different church scientologists.",5.1 Archiving Results and Analysis,[0],[0]
"Therefore
RM is extremely likely to acquire the documents related to the namesakes if they have similar popular background as the target entity, causing a greater loss of precision.
",5.1 Archiving Results and Analysis,[0],[0]
"Table 10 shows the top highly-weighted words in RM for the target Mark Fisher (a church scientologist), along with 2 namesakes who occur most frequently in the incorrect archiving results.
",5.1 Archiving Results and Analysis,[0],[0]
"By contrast, TM independently represents different life slices and combines the effects of the slices on biography-document relevance determination, evenly and exhaustively.",5.1 Archiving Results and Analysis,[0],[0]
Comprehensive and unbiased measurement of every known life slices is helpful in disambiguating entities that have similar backgrounds (definitely not the same in all).,5.1 Archiving Results and Analysis,[0],[0]
"As a result, TM improves the precision.",5.1 Archiving Results and Analysis,[0],[0]
And the context-based TM goes further.,5.1 Archiving Results and Analysis,[0],[0]
"We apply our entity archiving methods to two top-ranked slot filling systems in the evaluation of KBP 2013, including LSV (Roth and Klakow, 2013) and Blender (Yu et al 2013).
",5.2 Slot Filling Results and Analysis,[0],[0]
The LSV incorporates a string matching based entity archiving and a SVM classifier based filler extraction.,5.2 Slot Filling Results and Analysis,[0],[0]
"LSV’s archiving model expands queries by using suffixes and Wikipedia anchor texts, and uses mutual information based relevance measure in document ranking and filtering.
",5.2 Slot Filling Results and Analysis,[0],[0]
Blender employs a hybrid retrieval model for archiving relevant documents.,5.2 Slot Filling Results and Analysis,[0],[0]
It combines Boolean and VSM models and expands query by an alternate name table similar to ours.,5.2 Slot Filling Results and Analysis,[0],[0]
"For filler extraction, Blender implements truth finding over conflicting claims from multiple rule-based extraction systems.
",5.2 Slot Filling Results and Analysis,[0],[0]
"Table 11 shows entity archiving performances
of LSV and Blender (Macro-Average P, R and F).",5.2 Slot Filling Results and Analysis,[0],[0]
All CEA methods perform better than the both.,5.2 Slot Filling Results and Analysis,[0],[0]
"With the aim to optimize provenances of fillers, we modify the slot filling systems by substituting their archiving methods with ours.",5.2 Slot Filling Results and Analysis,[0],[0]
"Table 12 exhibits the performance gains after replacement.
",5.2 Slot Filling Results and Analysis,[0],[0]
Both LSV and Blender achieve significant gains.,5.2 Slot Filling Results and Analysis,[0],[0]
The most interesting finding is on the different performance gains.,5.2 Slot Filling Results and Analysis,[0],[0]
"It should reveal the fact that the well-supervised classification based filler extraction of LSV has a better capability of noise resistance, while by contrast, the truthfinding in Blender is capable of identifying valid fillers if the quality of archiving results is high, otherwise easily makes mistake.",5.2 Slot Filling Results and Analysis,[0],[0]
"We doubt that it is easy to maintain the stability of current entity-oriented knowledge acquisition methods, including ours, in dealing with ordinary entities.",6 Conclusion,[0],[0]
"Most target entities now in use for the evaluation are made to stand as “out of the ordinary”, such as well-known enterprises, celebrities or domain experts.",6 Conclusion,[0],[0]
"As a result, a corpus contains abundant relevant documents of the entities but less about the little-known namesakes.",6 Conclusion,[0],[0]
"It greatly reduces the interference of namesakes and thus the difficulty of the task.
",6 Conclusion,[0],[0]
"In future work, we will make the task critical for success by employing the little known namesakes as targets.",6 Conclusion,[0],[0]
"In addition to verifying the robustness of the CEA method, we will work on the relationship among entities (ACE entity relation types, Doddington et al, 2004) and related events (e.g., causal, temporal and sub-event relations), by which to build graph-based biography.",6 Conclusion,[0],[0]
"This work was supported by the U.S. DARPA DEFT Program No. FA8750-13-2-0041, ARL NS-CTA No.",7 Acknowledgment,[0],[0]
"W911NF-09-2-0053, NSF CAREER Award IIS-1523198, AFRL DREAM project, gift awards from IBM, Google, Disney and Bosch.",7 Acknowledgment,[0],[0]
It was also supported by Natural Science Foundation of China (NSFC),7 Acknowledgment,[0],[0]
No.,7 Acknowledgment,[0],[0]
"K111818713, K111818612.",7 Acknowledgment,[0],[0]
"The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. and CHN Governments.",7 Acknowledgment,[0],[0]
The U.S. and CHN Governments are authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.,7 Acknowledgment,[0],[0]
"Knowledge Base Population (KBP) tasks, such as slot filling, show the particular importance of entity-oriented automatic relevant document acquisition.",abstractText,[0],[0]
"Rich, diverse and reliable relevant documents satisfy the fundamental requirement that a KBP system explores the nature of an entity.",abstractText,[0],[0]
"Towards the bottleneck problem between comprehensiveness and definiteness of acquisition, we propose a collaborative archiving method.",abstractText,[0],[0]
"In particular we introduce topic modeling methodologies into entity biography profiling, so as to build a bridge between fuzzy and exact matching.",abstractText,[0],[0]
"On one side, we employ the topics in a small-scale high-quality relevant documents (i.e., exact matching results) to summarize the life slices of a target entity (i.e., biography), and on the other side, we use the biography as a reliable reference material to detect new truly relevant documents from a large-scale partially complete pseudo-feedback (i.e., fuzzy matching results).",abstractText,[0],[0]
We leverage the archiving method to enhance slot filling systems.,abstractText,[0],[0]
Experiments on KBP corpus show significant improvement over stateof-the-art.,abstractText,[0],[0]
Biography-Dependent Collaborative Entity Archiving for Slot Filling,title,[0],[0]
"A stochastic differential equation (SDE) defines a diffusion process, which evolves randomly over time, by describing its instantaneous behaviour.",1. Introduction,[0],[0]
"As such, SDEs are powerful modelling tools used extensively in fields such as econometrics (Black & Scholes, 1973; Eraker, 2001), biology (Gillespie, 2000; Golightly & Wilkinson, 2011), physics (van Kampen, 2007) and epidemiology (Fuchs, 2013).
",1. Introduction,[0],[0]
It is only possible to work with analytic solutions to SDEs in special cases.,1. Introduction,[0],[0]
"Therefore it is common to use a numerical approximation, such as the Euler-Maruyama scheme.",1. Introduction,[0],[0]
"Here the diffusion process is defined only on a grid of time points, and the transition density between successive diffusion states is approximated as Gaussian.",1. Introduction,[0],[0]
"The approximation
*Equal contribution 1School of Mathematics, Statistics and Physics, Newcastle University, Newcastle, UK 2School of Computing, Newcastle University, Newcastle, UK.",1. Introduction,[0],[0]
Correspondence to: Tom Ryder <,1. Introduction,[0],[0]
"t.ryder2@newcastle.ac.uk>, Dennis Prangle <dennis.prangle@newcastle.ac.uk>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
error involved converges to zero as the grid becomes finer.
",1. Introduction,[0],[0]
"Even under discretisation, statistical inference for SDEs observed at discrete times is challenging.",1. Introduction,[0],[0]
"The difficulty is that, along with unknown parameters θ in the description of the SDE, there is an unknown latent path of the diffusion process,",1. Introduction,[0],[0]
"x. An inference method must somehow deal with these high dimensional, highly structured latent variables.
",1. Introduction,[0],[0]
Our proposed method uses recent advances in variational inference to jointly infer θ and x. We introduce a flexible family of approximations to the posterior distribution and select the member closest to the true posterior.,1. Introduction,[0],[0]
"We use a standard mean-field approximation for the θ posterior, and introduce a novel recurrent neural network (RNN) approximation for the posterior of x conditional on θ.",1. Introduction,[0],[0]
"The RNN learns how to supply Gaussian state transitions between successive time points which closely match those for the intractable conditioned diffusion process.
",1. Introduction,[0],[0]
Our black-box variational inference method is a simple and fast way to produce approximate inference for any SDE system.,1. Introduction,[0],[0]
"We illustrate our method on Lotka-Volterra and epidemic examples, achieving accurate parameter estimates in just a few hours under default tuning choices.",1. Introduction,[0],[0]
"Although our parameter posteriors are over-concentrated, as in most variational methods, our approximation of the conditioned diffusion process is close to the true posterior.",1. Introduction,[0],[0]
"In comparison, existing Markov chain Monte Carlo (MCMC) methods (see Section 1.1) require more tuning choices and can take days to run (Whitaker et al., 2017a).",1. Introduction,[0],[0]
"Variational inference Several authors have looked at variational inference for SDEs (Archambeau et al., 2008) or related problems such as Markov jump processes (Ruttor et al., 2010) and state space models (Archer et al., 2016; Quiroz et al., 2018).",1.1. Related Work,[0],[0]
The novelty of our approach is to use: (1) stochastic optimisation rather than variational calculus; (2) a RNN-based variational approximation for the latent states instead of a mean-field or multivariate Gaussian approach.,1.1. Related Work,[0],[0]
"We expect (2) is especially relevant to sparsely observed SDEs, where the latent states between observations may have a particularly complex dependency structure.
",1.1. Related Work,[0],[0]
"Another approach (Moreno et al., 2016) is to perform vari-
ational inference for the parameters only, using latent variables drawn from their prior in the ELBO estimate.",1.1. Related Work,[0],[0]
Such latent states are typically a poor match to the observed data and so make a negligible contribution to the ELBO.,1.1. Related Work,[0],[0]
"To deal with the problem, close matches are upweighted.",1.1. Related Work,[0],[0]
"Our approach avoids this extra approximation by instead learning the posterior distribution of the latent variables.
",1.1. Related Work,[0],[0]
"Our method can also be related to recent work on normalising flows as variational approximations (Rezende & Mohamed, 2015).",1.1. Related Work,[0],[0]
"As in that work, our variational approximation can be viewed as transforming a N(0, I) sample vector by successive linear transformations to an approximate posterior sample (of the diffusion states in our case).",1.1. Related Work,[0],[0]
"Our work uses SDE theory to select simple and cheap transformations which produce a particularly good approximation.
Monte Carlo A popular approach in the Monte Carlo literature on SDEs is to introduce a bridge construct: an approximation to the discretised diffusion process conditional on the parameters and observations at a single time, derived using probability theory and various simplifying approximations.",1.1. Related Work,[0],[0]
The goal is to produce a path bridging between two observation times.,1.1. Related Work,[0],[0]
Combining successive bridges forms a complete diffusion path.,1.1. Related Work,[0],[0]
Bridge constructs can be used to produce proposals within Monte Carlo algorithms such as MCMC (see e.g. Roberts & Stramer 2001; Golightly & Wilkinson 2008; Fuchs 2013; van der Meulen et al. 2017).,1.1. Related Work,[0],[0]
"However, designing a bridge construct with desirable features for a particular problem is a challenging and time consuming tuning choice.",1.1. Related Work,[0],[0]
(Some particularly difficult regimes for bridge constructs are discussed in Section 5.),1.1. Related Work,[0],[0]
"From this point of view, our contribution is to use machine learning to effectively automate the design of a bridge construct.
",1.1. Related Work,[0],[0]
"Another Monte Carlo approach is to perform approximate inference based on low dimensional summary statistics of the observations (Picchini, 2014).",1.1. Related Work,[0],[0]
"This results in a loss of information, which our approach avoids.",1.1. Related Work,[0],[0]
"Consider an Itô process {Xt, t ≥ 0} satisfying the SDE
dXt = α(Xt, θ)dt+ √ β(Xt, θ)dWt, X0 = x0.",2. Stochastic Differential Equations,[0],[0]
"(1)
Here Xt is a p-dimensional vector of random variables, α is a p-dimensional drift vector, β is a p× p positive definite diffusion matrix (with √ β representing a matrix square root) and Wt is a p-vector of standard and uncorrelated Brownian motion processes.",2. Stochastic Differential Equations,[0],[0]
"The drift and diffusion depend on θ = (θ1, θ2, ..., θc)
′, a vector of unknown parameters (which may also include the initial condition x0).
",2. Stochastic Differential Equations,[0],[0]
"We assume that α(·) and β(·) are sufficiently regular that (1) has a weak non-explosive solution (Øksendal, 2003).",2. Stochastic Differential Equations,[0],[0]
"In
this case, (1) defines a diffusion process.",2. Stochastic Differential Equations,[0],[0]
"Such processes are always Markovian (i.e. memoryless).
",2. Stochastic Differential Equations,[0],[0]
We further assume partial noisy observations of the latent process.,2. Stochastic Differential Equations,[0],[0]
"Suppose that there are d + 1 observation times t0, t1, . . .",2. Stochastic Differential Equations,[0],[0]
", td = T .",2. Stochastic Differential Equations,[0],[0]
"In the simplest case, these times are equally spaced, separated by a time-step of ∆t.",2. Stochastic Differential Equations,[0],[0]
"Let ytj be a vector of p0 observations at time tj , for some p0 ≤ p.",2. Stochastic Differential Equations,[0],[0]
"Following Golightly & Wilkinson (2008), among others, we assume that
ytj = F ′Xtj + ωtj , ωtj indep∼ N(0,Σ), (2)
where F is a constant p×p0 matrix, and Σ is a p0×p0 matrix which may be assumed known or the object of inference.",2. Stochastic Differential Equations,[0],[0]
"For the latter case Σ should be a specified function of θ.
",2. Stochastic Differential Equations,[0],[0]
"Upon choosing a prior density p(θ), Bayesian inference proceeds via the parameter posterior p(θ|y), or alternatively the joint posterior p(θ, x|y).
",2. Stochastic Differential Equations,[0],[0]
Discretisation Few SDEs permit analytical solutions to (1) and instead it is common to use an approximation based on time discretisation.,2. Stochastic Differential Equations,[0],[0]
We therefore introduce intermediate time-points between observation times.,2. Stochastic Differential Equations,[0],[0]
"For concreteness, we present our methods for the case of equally spaced observations with t0 = 0.",2. Stochastic Differential Equations,[0],[0]
"(It is easy to adapt them to alternative specifications of time points, such as those required by irregularly-spaced observation times.)",2. Stochastic Differential Equations,[0],[0]
"We introduce k − 1 time-points between successive observations, giving a regular grid of times τi = i∆τ for i = 0, 1, 2, . . .",2. Stochastic Differential Equations,[0],[0]
",m = dk, with time-step ∆τ = ∆t/k. Note that i = 0, k, 2k, . . .",2. Stochastic Differential Equations,[0],[0]
", dk give the observation times.",2. Stochastic Differential Equations,[0],[0]
"The role of k is to ensure the discretisation can be made arbitrarily accurate, at the expense of increased computational cost.
",2. Stochastic Differential Equations,[0],[0]
"We work with the simplest discretisation, the EulerMaruyama scheme, in which transition densities between states at successive times are approximated as Gaussian
p ( xτi+1 |xτi , θ ) = ϕ",2. Stochastic Differential Equations,[0],[0]
( xτi+1,2. Stochastic Differential Equations,[0],[0]
"− xτi ; α(xτi , θ)∆τ, β(xτi , θ)∆τ ) ,
(3)
where ϕ(·;µ, S) is the Gaussian density with mean µ and variance matrix S. A generative expression of this is
xτi+1 = xτi + α(xτi , θ)∆τ + √ β(xτi , θ)∆τ zi+1, (4)
where zi+1 is an independent N(0, Ip) realisation.
",2. Stochastic Differential Equations,[0],[0]
Discretisation is not guaranteed to preserve properties of the underlying SDE.,2. Stochastic Differential Equations,[0],[0]
An issue which is particularly relevant later is positivity.,2. Stochastic Differential Equations,[0],[0]
"In many SDEs, such as population models, it is guaranteed that some components of Xt are always positive.",2. Stochastic Differential Equations,[0],[0]
"However, in (4) xτi+1 is sampled from a Gaussian, which has unbounded support.",2. Stochastic Differential Equations,[0],[0]
"Consequently, there is a non-zero probability of sampling negative values.",2. Stochastic Differential Equations,[0],[0]
"This is problematic
as the drift or diffusion function may be poorly behaved or undefined for such input.",2. Stochastic Differential Equations,[0],[0]
"A simple solution to this problem is the use of a reflecting boundary (Skorokhod, 1961), for example by projecting invalid xτi+1 values back to the valid region (Dangerfield et al., 2012).
",2. Stochastic Differential Equations,[0],[0]
Posterior,2. Stochastic Differential Equations,[0],[0]
"The joint posterior under the Euler-Maruyama discretisation is
p(θ, x|y) ∝",2. Stochastic Differential Equations,[0],[0]
"p(θ)p(x|θ)p(y|x, θ), (5)
p(x|θ) = m−1∏ i=0 ϕ",2. Stochastic Differential Equations,[0],[0]
( xτi+1,2. Stochastic Differential Equations,[0],[0]
"− xτi ; α(xτi , θ)∆τ,
β(xτi , θ)∆τ ) (6)
p(y|x, θ) = d∏ i=0 ϕ",2. Stochastic Differential Equations,[0],[0]
"(yti ;F ′xti ,Σ) .",2. Stochastic Differential Equations,[0],[0]
"(7)
In principle Monte Carlo algorithms can sample from (5).",2. Stochastic Differential Equations,[0],[0]
"However this is difficult in practice due to its high dimension and complex dependency structure.
",2. Stochastic Differential Equations,[0],[0]
"Conditioned processes Consider the process defined by conditioning (1) on an initial state, x0 and an exactly observed future state, xt1 .",2. Stochastic Differential Equations,[0],[0]
"This conditioned process itself satisfies an SDE (see e.g. Rogers & Williams, 2013) with drift and diffusion
α̂(xt, θ) = α(xt, θ) + β(xt, θ)∇xt log π (xt1 |xt, θ) , (8)
β̂(xt, θ) = β(xt, θ), (9)
where π (xt1 |xt, θ) is the transition density of the unconditioned process.",2. Stochastic Differential Equations,[0],[0]
"While this is intractable in most cases, the result motivates our choice of variational approximation later.
",2. Stochastic Differential Equations,[0],[0]
In some simple situations a discretised approximation of this conditioned process can be derived (see e.g. Papaspiliopoulos et al. 2013) in which the diffusion matrix is scaled down as the observation time is approached.,2. Stochastic Differential Equations,[0],[0]
"Intuitively this is appealing: conditioned paths converge towards the observation, so nearby random deviations are smaller in scale.",2. Stochastic Differential Equations,[0],[0]
"This motivates us to use a variational approximation in which the diffusion matrix is not constrained to follow (9), and instead is allowed to shrink.",2. Stochastic Differential Equations,[0],[0]
Suppose we have a likelihood p(y|θ) for parameters θ under observations y.,3. Approximate Bayesian Inference,[0],[0]
Given a prior density p(θ),3. Approximate Bayesian Inference,[0],[0]
we wish to infer the posterior density p(θ|y) = p(θ)p(y|θ)/p(y).,3. Approximate Bayesian Inference,[0],[0]
"It is typically possible to numerically evaluate the unnormalised posterior p(θ, y) = p(θ)p(y|θ).",3. Approximate Bayesian Inference,[0],[0]
"Estimating the normalising constant p(y) = ∫ p(θ, y)dθ, known as the evidence, is useful for Bayesian model selection.",3. Approximate Bayesian Inference,[0],[0]
"Variational inference (VI) (see e.g. Blei et al., 2017) introduces a family of approximations to the posterior indexed by φ, q(θ;φ).",3.1. Variational Inference,[0],[0]
Optimisation is then used to find φ minimising the Kullback-Leibler divergence KL(q(θ;φ)||p(θ|y)).,3.1. Variational Inference,[0],[0]
"This is equivalent to maximising the ELBO (evidence lower bound) (Jordan et al., 1999),
Eθ∼q(·;φ)[log p(θ, y)− log q(θ;φ)].",3.1. Variational Inference,[0],[0]
"(10)
The optimal q(θ;φ) is an approximation to the posterior distribution.",3.1. Variational Inference,[0],[0]
"This is typically overconcentrated, unless the approximating family is rich enough to allow particularly close matches to the posterior.
",3.1. Variational Inference,[0],[0]
"The optimisation required by VI can be performed efficiently using the reparameterisation trick (Kingma & Welling, 2014; Rezende et al., 2014; Titsias & Lázaro-Gredilla, 2014).",3.1. Variational Inference,[0],[0]
"This requires expressing θ ∼ q(·;φ) as a non-centred parameterisation (Papaspiliopoulos et al., 2003).",3.1. Variational Inference,[0],[0]
"That is, writing θ as the output of an invertible deterministic function g( , φ) for some random variable with a fixed distribution.",3.1. Variational Inference,[0],[0]
"Then the ELBO can be written as
L(φ) = E",3.1. Variational Inference,[0],[0]
"[log p(θ, y)− log q(θ;φ)], (11)
with an unbiased Monte-Carlo estimate
L̂(φ) = 1 n n∑ i=1",3.1. Variational Inference,[0],[0]
"[log p(θ(i), y)− log q(θ(i);φ)], (12)
where θ(i)",3.1. Variational Inference,[0],[0]
"= g( (i), φ) and (1), . . .",3.1. Variational Inference,[0],[0]
", (n) are independent samples.",3.1. Variational Inference,[0],[0]
"Assuming L̂ is differentiable with respect to φ, the gradient of (12) can be calculated using automatic differentiation, and the resulting unbiased estimator of∇L(φ) used in stochastic gradient descent or similar algorithms.",3.1. Variational Inference,[0],[0]
"When variational inference outputs a good match to the posterior distribution, importance sampling (IS) (see e.g. Robert, 2004) can correct remaining inaccuracies and provide near-exact posterior inference.",3.2. Importance Sampling,[0],[0]
"In more detail, select an importance density q(θ) which can easily be sampled from, and satisfies supp q(θ) ⊇ supp p(θ|y).",3.2. Importance Sampling,[0],[0]
"IS samples θ(1), θ(2), . . .",3.2. Importance Sampling,[0],[0]
", θ(N) from q and calculates weights wi = p(θ
(i), y)/q(θ(i)).",3.2. Importance Sampling,[0],[0]
"Then, for any function h, an estimate of Eθ∼p(·|y)[h(θ)] is
N∑ i=1",3.2. Importance Sampling,[0],[0]
h(θ(i))wi / N∑ i=1,3.2. Importance Sampling,[0],[0]
wi.,3.2. Importance Sampling,[0],[0]
"(13)
",3.2. Importance Sampling,[0],[0]
"This is consistent for large N , but in practice q should approximate the posterior for accurate estimation at a feasible cost.",3.2. Importance Sampling,[0],[0]
Also note that N−1 ∑N i=1,3.2. Importance Sampling,[0],[0]
"wi is an unbiased and consistent estimate of the evidence.
",3.2. Importance Sampling,[0],[0]
A diagnostic for the quality of IS results is the effective sample size (ESS).,3.2. Importance Sampling,[0],[0]
"This is defined as
Neff =",3.2. Importance Sampling,[0],[0]
( N∑ i=1,3.2. Importance Sampling,[0],[0]
wi )2/ N∑ i=1,3.2. Importance Sampling,[0],[0]
w2i .,3.2. Importance Sampling,[0],[0]
"(14)
",3.2. Importance Sampling,[0],[0]
"For most functions h, the variance of (13) approximately equals that of an idealised Monte Carlo estimate based on Neff independent samples from p(θ|y) (Liu, 1996).",3.2. Importance Sampling,[0],[0]
"In practice we will use a variational approximation as the importance density, and the ESS to assess whether this is sufficiently good to produce accurate estimates.",3.2. Importance Sampling,[0],[0]
"However, ESS values can be an unstable for poor importance densities (Vehtari et al., 2017) so later we also consider other problemspecific evidence for the quality of our results.",3.2. Importance Sampling,[0],[0]
"Our variational approximation to the posterior (5) is
q(θ, x;φ) = q(θ;φθ)q(x|θ;φx).",4. Variational Inference for SDEs,[0],[0]
"(15)
These factors represent approximations to p(θ|y) and p(x|θ, y) respectively, which are described below.",4. Variational Inference for SDEs,[0],[0]
Here φθ,4. Variational Inference for SDEs,[0],[0]
"and φx are the variational parameters for the two factors, and φ is the collection of all variational parameters.",4. Variational Inference for SDEs,[0],[0]
"Note our eventual choice of q(x|θ;φx) depends on several features of the data and model (see list in Section 4.5), but we suppress this in our notation for simplicity.",4. Variational Inference for SDEs,[0],[0]
"For q(θ;φθ) we use the mean-field Gaussian approximation
q(θ;φθ) = c∏ i=1 ϕ(θi;µi, s 2 i ), (16)
with φθ = (µ1, . . .",4.1. Approximate Parameter Posterior,[0],[0]
", µc, s1, . . .",4.1. Approximate Parameter Posterior,[0],[0]
", sc).",4.1. Approximate Parameter Posterior,[0],[0]
Hence the components of θ are independent Gaussians.,4.1. Approximate Parameter Posterior,[0],[0]
"To express θ using a noncentred parameterisation, we write
θ = gθ( 1, φθ) = S 1 + µ. (17)
where ∼ N(0, Ic), S = diag(s1, . . .",4.1. Approximate Parameter Posterior,[0],[0]
", sc) and µ = (µ1, . . .",4.1. Approximate Parameter Posterior,[0],[0]
", µc).
",4.1. Approximate Parameter Posterior,[0],[0]
It may be necessary to transform θ to an alternative parameterisation ϑ,4.1. Approximate Parameter Posterior,[0],[0]
"so that a Gaussian approximation is appropriate e.g. log-transforming parameters constrained to be positive.
",4.1. Approximate Parameter Posterior,[0],[0]
"Mean-field approximations are imperfect, often producing underdispersed estimates of the posterior (see e.g. Blei et al., 2017), and more sophisticated approximations (e.g. Rezende & Mohamed, 2015) could be used here instead.",4.1. Approximate Parameter Posterior,[0],[0]
However mean-field approximations suffice to give good parameter estimation in our examples.,4.1. Approximate Parameter Posterior,[0],[0]
"Motivated by the result that a diffusion process conditioned on an exact observation is itself an SDE (see Section 2), we base q(x|θ;φx) upon a discretised diffusion.",4.2. Approximate Conditioned Diffusion Process,[0],[0]
"A generative definition is
xτi+1",4.2. Approximate Conditioned Diffusion Process,[0],[0]
= h,4.2. Approximate Conditioned Diffusion Process,[0],[0]
"( xτi + α̃(xτi , y, θ, τi;φx)∆τ
+ √ β̃(xτi , y, θ, τi;φx)∆τzi+1 ) ,
(18)
where α̃ and β̃ are drift and diffusion functions.",4.2. Approximate Conditioned Diffusion Process,[0],[0]
Taking h as the identity function gives a discretised diffusion process.,4.2. Approximate Conditioned Diffusion Process,[0],[0]
"However often we use h to impose positivity constraints on some components of x – see Section 4.3.
",4.2. Approximate Conditioned Diffusion Process,[0],[0]
"The resulting variational density q(x|θ;φx) is
m−1∏ i=0 ϕ",4.2. Approximate Conditioned Diffusion Process,[0],[0]
( xτi+1,4.2. Approximate Conditioned Diffusion Process,[0],[0]
"− xτi ; α̃(xτi , y, θ, τi;φx)∆τ,
β̃(xτi , y, θ, τi;φx)∆τ ) |det Ji|.
(19)
where Ji is the Jacobian matrix associated with the transformation h in (18).",4.2. Approximate Conditioned Diffusion Process,[0],[0]
"To express x with a non-centred parameterisation, let 2 ∼ N(0, Ipm) be the flattened vector of (z1, z2, . . .",4.2. Approximate Conditioned Diffusion Process,[0],[0]
", zm) realisations.",4.2. Approximate Conditioned Diffusion Process,[0],[0]
Then apply (18) repeatedly.,4.2. Approximate Conditioned Diffusion Process,[0],[0]
"Let the outcome be represented by the function
x = gx( 2, θ, φx).",4.2. Approximate Conditioned Diffusion Process,[0],[0]
"(20)
We use a neural network, with parameters φx, to serve as our functions α̃ and β̃.",4.2. Approximate Conditioned Diffusion Process,[0],[0]
At time τi it acts as follows.,4.2. Approximate Conditioned Diffusion Process,[0],[0]
"The network’s input is several features (described in Section 4.5) computed from: the current diffusion state xτi , the observations y, the parameters θ and the current time τi.",4.2. Approximate Conditioned Diffusion Process,[0],[0]
"The network outputs a drift vector and diffusion matrix (see Section 4.5 for details of the latter), which are used to sample xτi+1 from (18).",4.2. Approximate Conditioned Diffusion Process,[0],[0]
This state forms part of the neural network input at time τi+1.,4.2. Approximate Conditioned Diffusion Process,[0],[0]
So the network just discussed forms a cell of an overall recurrent neural network (RNN) structure for q(x|θ;φx).,4.2. Approximate Conditioned Diffusion Process,[0],[0]
"Note that long-term memory features are not required as we wish to produce a diffusion process, which is memoryless.",4.2. Approximate Conditioned Diffusion Process,[0],[0]
"In practice, we take h in (18) to be a function which applies the identity function to unconstrained components of the diffusion state, and the function softplus(z) = log(1 + ez) to components with positivity constraints.",4.3. Ensuring Positivity,[0],[0]
This function produces strictly positive outputs while having little effect on positive inputs above 2.,4.3. Ensuring Positivity,[0],[0]
"The latter property means our variational approximation usually remains similar to a discretised
diffusion process.",4.3. Ensuring Positivity,[0],[0]
"Alternative transformations could be used if state values below 2 were believed to be common, potentially with tuning parameters so a suitable shape could be learned.",4.3. Ensuring Positivity,[0],[0]
"However, this was not necessary for our examples.
",4.3. Ensuring Positivity,[0],[0]
Preliminary work found this transformation approach to enforcing positivity was much easier to implement in the variational framework than reflection methods.,4.3. Ensuring Positivity,[0],[0]
It can be interpreted as constraining the variational approximation based on prior beliefs about positivity of diffusion paths.,4.3. Ensuring Positivity,[0],[0]
Our overall inference algorithm is given in Algorithm 1.,4.4. Algorithm,[0],[0]
"This aims to maximise the ELBO
L(φ) = Eθ,x∼q(·;φ) [ log
p(θ)p(x|θ)p(y|x, θ) q(θ;φθ)q(x|θ;φx)
] , (21)
by differentiating the Monte Carlo estimate
L̂(φ) = 1 n n∑ i=1",4.4. Algorithm,[0],[0]
"log p(θ(i))p(x(i)|θ(i))p(y|x(i), θ(i))",4.4. Algorithm,[0],[0]
"q ( θ(i);φθ ) q ( x(i)|θ(i);φx
) , (22)",4.4. Algorithm,[0],[0]
"where θ(i) = gθ( (i) 1 , φθ) and x (i) = gx( (i) 2 , θ (i), φx).
",4.4. Algorithm,[0],[0]
Algorithm 1 Black-box variational inference for SDEs Initialise φ0 and k = 0.,4.4. Algorithm,[0],[0]
"loop
Sample (i)1 , (i) 2 for 1 ≤",4.4. Algorithm,[0],[0]
i ≤ n. Calculate ∇L̂(φk) using automatic differentiation of (22).,4.4. Algorithm,[0],[0]
"Calculate φk+1 using stochastic gradient descent, or a similar algorithm, and increment k.
end loop",4.4. Algorithm,[0],[0]
"Our RNN cell input at time τi, with tj ≤ τi < tj+1, is:
• The parameters θ. •",4.5. Implementation Details,[0],[0]
"The most recent latent state, xτi−1 .",4.5. Implementation Details,[0],[0]
"• The time until the next observation, tj+1",4.5. Implementation Details,[0],[0]
− τi.,4.5. Implementation Details,[0],[0]
"• The next observation time, tj+1.",4.5. Implementation Details,[0],[0]
•,4.5. Implementation Details,[0],[0]
"The difference between the next observation and what
the mean observation would be at the most recent latent state, ytj+1 − F ′xτi−1 .
",4.5. Implementation Details,[0],[0]
"Exploratory work showed that the RNN produces a much better approximation of the conditioned process with these features as input rather than simply xτi , y, θ and τi.
",4.5. Implementation Details,[0],[0]
"Our RNN cell outputs a vector α̃ and the coefficients of a lower-triangular matrix, M .",4.5. Implementation Details,[0],[0]
"In order to return a Cholesky factor of β̃, the diagonal elements of M are transformed
using the softplus function to ensure positivity.",4.5. Implementation Details,[0],[0]
"We also regularise to avoid β̃ matrices with very small determinants.
",4.5. Implementation Details,[0],[0]
Algorithm 1 requires automatic differentiation of (22).,4.5. Implementation Details,[0],[0]
This can be achieved using the standard tool-kit of backpropagation after rolling-out the RNN i.e. stacking m copies of the RNN cell to form a deep feed-forward network.,4.5. Implementation Details,[0],[0]
"The canonical challenge in training such networks, known as the exploding-gradient problem (Bengio et al., 1994), often necessitates the use of gradient clipping to control for numerical instability.",4.5. Implementation Details,[0],[0]
"We follow Pascanu et al. (2013) and perform gradient clipping using the L1 norm.
",4.5. Implementation Details,[0],[0]
"To initialise φ0 in Algorithm 1, we select φθ",4.5. Implementation Details,[0],[0]
so the margins of the variational approximation are based on those of the parameter priors.,4.5. Implementation Details,[0],[0]
Standard choices from the neural network literature – random Gaussian weights and constant biases – are used for φx.,4.5. Implementation Details,[0],[0]
We implement our method for two examples: (1) analysing synthetic data from a Lotka-Volterra SDE; (2) analysing real data from an SDE model of a susceptible-infectiousremoved (SIR) epidemic.,5. Experiments,[0],[0]
Our experiments include challenging regimes such as: (A) low-variance observations; (B) conditioned diffusions with non-linear dynamics; (C) unobserved time series; (D) widely spaced observation times; (E) data which is highly unlikely under the unconditioned model.,5. Experiments,[0],[0]
"Many of these violate the assumptions used by existing diffusion bridge constructs (Whitaker et al., 2017b).
",5. Experiments,[0],[0]
In all our experiments below similar tuning choices worked well.,5. Experiments,[0],[0]
We use batch size n = 50 in (22).,5. Experiments,[0],[0]
Our RNN cell has four hidden layers each with 20 hidden units and rectifiedlinear activation.,5. Experiments,[0],[0]
"We implement our algorithms in Tensorflow using the Adam optimiser (Kingma & Ba, 2015) and report results using an 8-core CPU.",5. Experiments,[0],[0]
The code is available at https://github.com/Tom-Ryder/VIforSDEs.,5. Experiments,[0],[0]
"Lotka-Volterra models describe simple predator-prey population dynamics combining three types of event: prey reproduction, predation (in which prey are consumed and predators have the resources to reproduce) and predator death.",5.1. Lotka-Volterra,[0],[0]
"A SDE Lotka-Volterra model (for derivation see e.g. Golightly & Wilkinson, 2011) is defined by
α(Xt, θ) = ( θ1Ut − θ2UtVt θ2UtVt − θ3Vt ) , (23)
β(Xt, θ) = ( θ1Ut + θ2UtVt −θ2UtVt −θ2UtVt θ3Vt + θ2UtVt ) , (24)
where Xt = (Ut, Vt)′ represents the populations of prey and predators at time t. The parameters θ = (θ1, θ2, θ3) ′ control the rates of the three events described above.
",5.1. Lotka-Volterra,[0],[0]
"In the experiments below, we use discretisation time step ∆τ = 0.1 and observation variance Σ = I , which is small relative to the typical population sizes (see e.g. Figure 1.)
",5.1. Lotka-Volterra,[0],[0]
"A single observation time with known parameters We begin with the case of a single observation time and known parameter values, where we follow Boys et al. (2008) by taking θ = (0.5, 0.0025, 0.3)′ and x0 = (71, 79)′. This setting solely investigates our ability to learn x, without uncertainty in θ: essentially the same problem as creating a bridge construct (described in Section 1.1.)
",5.1. Lotka-Volterra,[0],[0]
"We consider four different observations at time t = 10, listed in Table 1.",5.1. Lotka-Volterra,[0],[0]
"For each example we train our variational approximation until convergence (assessed manually throughout this paper), which takes roughly 20 minutes for the first 3 examples, and 90 minutes for the last.",5.1. Lotka-Volterra,[0],[0]
"We then perform importance sampling using 500,000 samples from the fitted approximation.",5.1. Lotka-Volterra,[0],[0]
Table 1 shows the resulting ESS values.,5.1. Lotka-Volterra,[0],[0]
"The first 3 rows in the table are typical observations under the model given our θ, while the final row represents highly unlikely observations (double those in the previous row).",5.1. Lotka-Volterra,[0],[0]
"Figure 1 shows fitted diffusion paths for this case.
",5.1. Lotka-Volterra,[0],[0]
This example contains several challenging features: all those listed at the start of Section 5 except (C).,5.1. Lotka-Volterra,[0],[0]
"While these features make it hard to use existing bridge constructs, our variational method produces a close approximation to the true posterior, as illustrated by the high ESS values.
",5.1. Lotka-Volterra,[0],[0]
"The case of highly unlikely observations takes longer to train and receives a lower ESS, reflecting that a more complicated diffusion path must be learned here.",5.1. Lotka-Volterra,[0],[0]
"(To check this we found that a simpler RNN cell suffices for good performance in the other examples but not this one.)
",5.1. Lotka-Volterra,[0],[0]
"Multiple observation times with known parameters We now extend the previous example to multiple observation times, t = 0, 10, 20, 30, 40.",5.1. Lotka-Volterra,[0],[0]
"We analyse synthetic data, produced using the parameters specified previously (including observation noise with Σ = I).",5.1. Lotka-Volterra,[0],[0]
"Here convergence takes 6 hours, and importance sampling with 500,000 samples produces an ESS of 96,812.",5.1. Lotka-Volterra,[0],[0]
"The resulting diffusion
paths are not shown as they are very similar visually to the next example (see Figure 2).
",5.1. Lotka-Volterra,[0],[0]
"This example shows that our method learns the conditioned process well even when there are several observation times.
",5.1. Lotka-Volterra,[0],[0]
Multiple observation times with unknown parameters We now analyse the same synthetic data with unknown θ parameters.,5.1. Lotka-Volterra,[0],[0]
"As these must take positive values, we work with the log-transformed parameters ϑ and assume they have independent N(0, 32) priors.",5.1. Lotka-Volterra,[0],[0]
"Our results are shown after transforming back to the original parameterisation.
",5.1. Lotka-Volterra,[0],[0]
"Convergence takes 2 hours, and importance sampling with 500,000 iterations produces an ESS of 635.4.",5.1. Lotka-Volterra,[0],[0]
Figure 2 shows 50 diffusion paths sampled from the fitted variational approximation.,5.1. Lotka-Volterra,[0],[0]
"Figure 3 shows two estimates of the marginal parameter posteriors: the variational inference output, and a kernel density estimate based on importance sampling results.
",5.1. Lotka-Volterra,[0],[0]
The estimated posteriors give accurate estimates of the true parameter values.,5.1. Lotka-Volterra,[0],[0]
"However, the low ESS here shows that both estimates of the parameter posteriors are imperfect approximations (also illustrated by the variational posterior estimates appearing overconcentrated compared to the importance sampling results.)",5.1. Lotka-Volterra,[0],[0]
"Achieving good point estimates but imperfect posteriors is typical for variational inference (Blei et al., 2017).",5.1. Lotka-Volterra,[0],[0]
"An SIR epidemic model (Andersson & Britton, 2000) describes the spread of an infectious disease.",5.2. Epidemic Model,[0],[0]
"The population is
split into those susceptible (S), infectious (I) and removed (R).",5.2. Epidemic Model,[0],[0]
"Two types of event take place: susceptibles can be infected by the infectious, and the infectious eventually become removed.",5.2. Epidemic Model,[0],[0]
Constant population size is assumed.,5.2. Epidemic Model,[0],[0]
"Hence only the S and I population sizes need to be modelled.
",5.2. Epidemic Model,[0],[0]
"An SIR epidemic model using SDEs is defined by
α(Xt, θ) =
( −θ1StIt
θ1StIt",5.2. Epidemic Model,[0],[0]
"− θ2It,
) (25)
β(Xt, θ) =",5.2. Epidemic Model,[0],[0]
( θ1StIt −θ1StIt −θ1StIt,5.2. Epidemic Model,[0],[0]
"θ1StIt + θ2It ) , (26)
where Xt = (St, It)′ is the state of the system at time t, θ1 is an infection parameter and θ2 is a removal parameter.",5.2. Epidemic Model,[0],[0]
"For a detailed derivation see Fuchs (2013).
",5.2. Epidemic Model,[0],[0]
"Our data is taken from an outbreak of influenza at a boys boarding school in 1978 (Jackson et al., 2013).",5.2. Epidemic Model,[0],[0]
Influenza was introduced to the population by a student returning from holiday in Hong Kong.,5.2. Epidemic Model,[0],[0]
"Of the 763 boys at the school, 512 were infected within 14 days.",5.2. Epidemic Model,[0],[0]
Hence we assume x0 =,5.2. Epidemic Model,[0],[0]
"(762, 1)
′. Observations of the number infectious are provided daily by those students confined to bed.",5.2. Epidemic Model,[0],[0]
"We
assume Gaussian observation error with unknown variance σ2.",5.2. Epidemic Model,[0],[0]
"Our analyses use a discretisation time step of ∆τ = 0.1.
",5.2. Epidemic Model,[0],[0]
We also consider an alternative model with a time-varying infection parameter.,5.2. Epidemic Model,[0],[0]
"Here we let ϑ1 = log θ1 follow an Ornstein-Uhlenbeck process
dϑt,1 = θ3 (θ4 − ϑt,1)",5.2. Epidemic Model,[0],[0]
"dt+ θ5dWt, (27)
where θ3, θ4 and θ5 are the mean-reversion rate, process mean and volatility, respectively, and θt,1 is the infection parameter at time t. Previous related work has focused on ODE epidemic models with time-varying parameters following SDEs (Dureau et al., 2013; Del Moral & Murray, 2015).",5.2. Epidemic Model,[0],[0]
"In contrast, our approach can easily be applied to a full SDE system.
",5.2. Epidemic Model,[0],[0]
Time-invariant infection parameter,5.2. Epidemic Model,[0],[0]
"We infer the logtransformed parameters ϑ = (log θ1, log θ2, log σ2)′ under independent N(0, 32) priors.",5.2. Epidemic Model,[0],[0]
"Our results are shown after transforming back to the original parameterisation.
",5.2. Epidemic Model,[0],[0]
"Convergence takes 2.5 hours, and importance sampling with 500,000 iterations produces an ESS of 718.2.",5.2. Epidemic Model,[0],[0]
"Figure 4 shows two estimates of the marginal parameter posteriors: variational inference output, and a kernel density estimate based on importance sampling results.",5.2. Epidemic Model,[0],[0]
"Figure 6 shows 50 diffusion paths sampled from the variational approximation.
",5.2. Epidemic Model,[0],[0]
The small ESS indicates there is some approximation error.,5.2. Epidemic Model,[0],[0]
"However, the marginal posteriors for θ1 and θ2 are very similar to those from the MCMC analysis of Fuchs (2013, pg 293), despite some modelling differences (that analysis fixed σ2 = 0 and used exponential priors for θ1 and θ2).
",5.2. Epidemic Model,[0],[0]
"Time-variant infection parameter We infer the logtransformed parameters ϑ = (log θ0,1, log θ2, log θ3, log θ4, log θ5, log σ
2)′ under independent N(0, 32) priors.",5.2. Epidemic Model,[0],[0]
"Results are shown after transforming back to the original parameterisation.
",5.2. Epidemic Model,[0],[0]
"Convergence now takes 3 hours, and 500,000 iterations of importance sampling produces an ESS of 256.1.",5.2. Epidemic Model,[0],[0]
"Figure 5 shows estimates of the marginal parameter posteriors, using variational inference and importance sampling outputs as before.",5.2. Epidemic Model,[0],[0]
Figures 7 (SIR) and 8 (Ornstein-Uhlenbeck) show 50 diffusion paths sampled from the variational approximation.,5.2. Epidemic Model,[0],[0]
"Again the low ESS indicates some approximation error.
",5.2. Epidemic Model,[0],[0]
"Model comparison The two models produce visually similar diffusion paths, but close inspection shows some differences.",5.2. Epidemic Model,[0],[0]
The time-invariant model paths for It appear smooth but slightly miss some of the observation points.,5.2. Epidemic Model,[0],[0]
The time-variant model paths for It are less smooth and more accurately capture the shape of the data.,5.2. Epidemic Model,[0],[0]
"Correspondingly, the time-varying model infers a smaller σ2 value.",5.2. Epidemic Model,[0],[0]
"The most obvious difference in It paths occurs for the t = 7, 8, 9
observations, shown in the zoomed-in inset of Figures 6 and 7.",5.2. Epidemic Model,[0],[0]
"Figure 8 shows that shortly before this the time-variant θ1 values become constrained to smaller values.
",5.2. Epidemic Model,[0],[0]
"Although the time-varying model appears to fit the data better, this is at the cost of increased model complexity, and could simply reflect overfitting.",5.2. Epidemic Model,[0],[0]
A better estimate of the parameter posteriors would allow formal model comparison based on importance sampling evidence estimates.,5.2. Epidemic Model,[0],[0]
"We provide a black-box variational approach to inference for SDES which is simple, practical and fast (relative to existing methods).",6. Conclusion,[0],[0]
This performs inference for a broad class of SDEs with minimal tuning requirements.,6. Conclusion,[0],[0]
Empirical investigation shows we obtain close matches to the posterior of the conditioned diffusion paths.,6. Conclusion,[0],[0]
"Approximate parameter inference is also possible, with our results recovering known parameters for synthetic data (Section 5.1), and previous results for real data (Section 5.2), using only a few hours of computation for a desktop PC.",6. Conclusion,[0],[0]
"An interesting future direction is develop choices of q(x|θ;φx) more efficent than standard RNNs, to further reduce computing time and enable real-time applications of this methodology.",6. Conclusion,[0],[0]
"Tom Ryder is supported by the Engineering and Physical Sciences Research Council, Centre for Doctoral Training in Cloud Computing for Big Data (grant number EP/L015358/1).
",Acknowledgements,[0],[0]
We acknowledge with thanks an NVIDIA academic GPU grant for this project.,Acknowledgements,[0],[0]
Parameter inference for stochastic differential equations is challenging due to the presence of a latent diffusion process.,abstractText,[0],[0]
"Working with an EulerMaruyama discretisation for the diffusion, we use variational inference to jointly learn the parameters and the diffusion paths.",abstractText,[0],[0]
"We use a standard mean-field variational approximation of the parameter posterior, and introduce a recurrent neural network to approximate the posterior for the diffusion paths conditional on the parameters.",abstractText,[0],[0]
This neural network learns how to provide Gaussian state transitions which bridge between observations as the conditioned diffusion process does.,abstractText,[0],[0]
The resulting black-box inference method can be applied to any SDE system with light tuning requirements.,abstractText,[0],[0]
"We illustrate the method on a LotkaVolterra system and an epidemic model, producing accurate parameter estimates in a few hours.",abstractText,[0],[0]
Black-Box Variational Inference for Stochastic Differential Equations,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 383–389 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
383",text,[0],[0]
"Author profiling is the task of discovering latent user attributes disclosed through text, such as gender, age, personality, income, location and occupation (Rao et al., 2010; Burger et al., 2011; Feng et al., 2012; Jurgens, 2013; Bamman et al., 2014; Plank and Hovy, 2015; Flekova et al., 2016).",1 Introduction,[0],[0]
"It is of interest to several applications including personalized machine translation, forensics, and marketing (Mirkin et al., 2015; Rangel et al., 2015).
",1 Introduction,[0],[0]
"Early approaches to gender prediction (Koppel et al., 2002; Schler et al., 2006, e.g.) are inspired by pioneering work on authorship attribution (Mosteller and Wallace, 1964).",1 Introduction,[0],[0]
Such stylometric models typically rely on carefully handselected sets of content-independent features to capture style beyond topic.,1 Introduction,[0],[0]
"Recently, open vocabulary approaches (Schwartz et al., 2013), where the entire linguistic production of an author is used, yielded substantial performance gains in on-
line user-attribute prediction (Nguyen et al., 2014; Preoţiuc-Pietro et al., 2015; Emmery et al., 2017).",1 Introduction,[0],[0]
"Indeed, the best performing gender prediction models exploit chiefly lexical information (Rangel et al., 2017; Basile et al., 2017).
",1 Introduction,[0],[0]
"Relying heavily on the lexicon though has its limitations, as it results in models with limited portability.",1 Introduction,[0],[0]
"Moreover, performance might be overly optimistic due to topic bias (Sarawgi et al., 2011).",1 Introduction,[0],[0]
"Recent work on cross-lingual author profiling has proposed the use of solely language-independent features (Ljubešić et al., 2017), e.g., specific textual elements (percentage of emojis, URLs, etc) and users’ meta-data/network (number of followers, etc), but this information is not always available.
",1 Introduction,[0],[0]
"We propose a novel approach where the actual text is still used, but bleached out and transformed into more abstract, and potentially better transferable features.",1 Introduction,[0],[0]
One could view this as a method in between the open vocabulary strategy and the stylometric approach.,1 Introduction,[0],[0]
"It has the advantage of fading out content in favor of more shallow patterns still based on the original text, without introducing additional processing such as part-of-speech tagging.",1 Introduction,[0],[0]
"In particular, we investigate to what extent gender prediction can rely on generic non-lexical features (RQ1), and how predictive such models are when transferred to other languages (RQ2).",1 Introduction,[0],[0]
"We also glean insights from human judgments, and investigate how well people can perform cross-lingual gender prediction (RQ3).",1 Introduction,[0],[0]
"We focus on gender prediction for Twitter, motivated by data availability.
",1 Introduction,[0],[0]
Contributions In this work,1 Introduction,[0],[0]
i),1 Introduction,[0],[0]
"we are the first to study cross-lingual gender prediction without relying on users’ meta-data; ii) we propose a novel simple abstract feature representation which is surprisingly effective; and iii) we gauge human ability to perform cross-lingual gender detection, an angle of analysis which has not been studied thus far.",1 Introduction,[0],[0]
"Can we recover the gender of an author from bleached text, i.e., transformed text were the raw lexical strings are converted into abstract features?",2 Profiling with Abstract Features,[0],[0]
"We investigate this question by building a series of predictive models to infer the gender of a Twitter user, in absence of additional user-specific metadata.",2 Profiling with Abstract Features,[0],[0]
"Our approach can be seen as taking advantage of elements from a data-driven open-vocabulary approach, while trying to capture gender-specific style in text beyond topic.
",2 Profiling with Abstract Features,[0],[0]
"To represent utterances in a more language agnostic way, we propose to simply transform the text into alternative textual representations, which deviate from the lexical form to allow for abstraction.",2 Profiling with Abstract Features,[0],[0]
"We propose the following transformations, exemplified in Table 1.",2 Profiling with Abstract Features,[0],[0]
"They are mostly motivated by intuition and inspired by prior work, like the use of shape features from NER and parsing (Petrov and Klein, 2007; Schnabel and Schütze, 2014;",2 Profiling with Abstract Features,[0],[0]
"Plank et al., 2016; Limsopatham and Collier, 2016):
• Frequency",2 Profiling with Abstract Features,[0],[0]
"Each word is presented as its binned frequency in the training data; bins are sized by orders of magnitude.
",2 Profiling with Abstract Features,[0],[0]
"• Length Number of characters (prefixed by 0 to avoid collision with the next transformation).
",2 Profiling with Abstract Features,[0],[0]
"• PunctC Merges all consecutive alphanumeric characters to one ‘W’ and leaves all other characters as they are (C for conservative).
",2 Profiling with Abstract Features,[0],[0]
• PunctA,2 Profiling with Abstract Features,[0],[0]
"Generalization of PunctC (A for aggressive), converting different types of punctuation to classes: emoticons1 to ‘E’ and emojis2 to ‘J’, other punctuation to ‘P’.
",2 Profiling with Abstract Features,[0],[0]
•,2 Profiling with Abstract Features,[0],[0]
"Shape Transforms uppercase characters to ‘U’, lowercase characters to ‘L’, digits to ‘D’ and all other characters to ‘X’.",2 Profiling with Abstract Features,[0],[0]
"Repetitions
1Using the NLTK tokenizer http://www.nltk.org/ _modules/nltk/tokenize/casual.html
2https://pypi.python.org/pypi/emoji/
of transformed characters are condensed to a maximum of 2 for greater generalization.
",2 Profiling with Abstract Features,[0],[0]
• Vowel-Consonant,2 Profiling with Abstract Features,[0],[0]
"To approximate vowels, while being able to generalize over (IndoEuropean) languages, we convert any of the ‘aeiou’ characters to ‘V’, other alphabetic character to ‘C’, and all other characters to ‘O’.
",2 Profiling with Abstract Features,[0],[0]
•,2 Profiling with Abstract Features,[0],[0]
AllAbs A combination (concatenation) of all previously described features.,2 Profiling with Abstract Features,[0],[0]
"In order to test whether abstract features are effective and transfer across languages, we set up experiments for gender prediction comparing lexicalized and bleached models for both in- and cross-language experiments.",3 Experiments,[0],[0]
"We compare them to a model using multilingual embeddings (Ruder, 2017).",3 Experiments,[0],[0]
"Finally, we elicit human judgments both within language and across language.",3 Experiments,[0],[0]
"The latter is to check whether a person with no prior knowledge of (the lexicon of) a given language can predict the gender of a user, and how that compares to an in-language setup and the machine.",3 Experiments,[0],[0]
"If humans can predict gender cross-lingually, they are likely to rely on aspects beyond lexical information.
",3 Experiments,[0],[0]
"Data We obtain data from the TWISTY corpus (Verhoeven et al., 2016), a multi-lingual collection of Twitter users, for the languages with 500+ users, namely Dutch, French, Portuguese, and Spanish.",3 Experiments,[0],[0]
"We complement them with English, using data from a predecessor of TWISTY (Plank and Hovy, 2015).",3 Experiments,[0],[0]
All datasets contain manually annotated gender information.,3 Experiments,[0],[0]
"To simplify interpretation for the cross-language experiments, we balance gender in all datasets by downsampling to the minority class.",3 Experiments,[0],[0]
The datasets’ final sizes are given in Table 2.,3 Experiments,[0],[0]
"We use 200 tweets per user, as done by previous work (Verhoeven et al., 2016).",3 Experiments,[0],[0]
"We leave the data untokenized to exclude any languagedependent processing, because original tokenization could preserve some signal.",3 Experiments,[0],[0]
Apart from mapping usernames to ‘USER’ and urls to ‘URL’ we do not perform any further data pre-processing.,3 Experiments,[0],[0]
"We use the scikit-learn (Pedregosa et al., 2011) implementation of a linear SVM with default parameters (e.g., L2 regularization).",3.1 Lexical vs Bleached Models,[0],[0]
We use 10-fold cross validation for all in-language experiments.,3.1 Lexical vs Bleached Models,[0],[0]
"For the cross-lingual experiments, we train
on all available source language data and test on all target language data.
",3.1 Lexical vs Bleached Models,[0],[0]
"For the lexicalized experiments, we adopt the features from the best performing system at the latest PAN evaluation campaign3",3.1 Lexical vs Bleached Models,[0],[0]
"(Basile et al., 2017) (word 1-2 grams and character 3-6 grams).
",3.1 Lexical vs Bleached Models,[0],[0]
"For the multilingual embeddings model we use the mean embedding representation from the system of (Plank, 2017) and add max, std and coverage features.",3.1 Lexical vs Bleached Models,[0],[0]
"We create multilingual embeddings by projecting monolingual embeddings to a single multilingual space for all five languages using a recently proposed SVD-based projection method with a pseudo-dictionary (Smith et al., 2017).",3.1 Lexical vs Bleached Models,[0],[0]
"The monolingual embeddings are trained on large amounts of in-house Twitter data (as much data as we had access to, i.e., ranging from 30M tweets for French to 1,500M tweets in Dutch, with a word type coverage between 63 and 77%).",3.1 Lexical vs Bleached Models,[0],[0]
This results in an embedding space with a vocabulary size of 16M word types.,3.1 Lexical vs Bleached Models,[0],[0]
"All code is available at https:// github.com/bplank/bleaching-text.
",3.1 Lexical vs Bleached Models,[0],[0]
"For the bleached experiments, we ran models with each feature set separately.",3.1 Lexical vs Bleached Models,[0],[0]
"In this paper, we report results for the model where all features are combined, as it proved to be the most robust across languages.",3.1 Lexical vs Bleached Models,[0],[0]
"We tuned the n-gram size of this model through in-language cross-validation, finding that n = 5 performs best.
",3.1 Lexical vs Bleached Models,[0],[0]
"When testing across languages, we report accuracy for two setups: average accuracy over each single-language model (AVG), and accuracy obtained when training on the concatenation of all languages but the target one (ALL).",3.1 Lexical vs Bleached Models,[0],[0]
The latter setting is also used for the embeddings model.,3.1 Lexical vs Bleached Models,[0],[0]
"We report accuracy for all experiments.
3http://pan.webis.de
Results and Analysis Table 2 shows results for both the cross-language and in-language experiments in the lexical and abstract-feature setting.
",3.1 Lexical vs Bleached Models,[0],[0]
"Within language, the lexical features unsurprisingly work the best, achieving an average accuracy of 80.5% over all languages.",3.1 Lexical vs Bleached Models,[0],[0]
"The abstract features lose some information and score on average 11.8% lower, still beating the majority baseline (50%) by a large margin (68.7%).",3.1 Lexical vs Bleached Models,[0],[0]
"If we go across language, the lexical approaches break down (overall to 53.7% for LEX AVG/56.3% for ALL), except for Portuguese and Spanish, thanks to their similarities (see Table 3 for pair-wise results).",3.1 Lexical vs Bleached Models,[0],[0]
"The closelyrelated-language effect is also observed when training on all languages, as scores go up when the classifier has access to the related language.",3.1 Lexical vs Bleached Models,[0],[0]
The same holds for the multilingual embeddings model.,3.1 Lexical vs Bleached Models,[0],[0]
"On average it reaches an accuracy of 59.8%.
",3.1 Lexical vs Bleached Models,[0],[0]
"The closeness effect for Portuguese and Spanish can also be observed in language-to-language experiments, where scores for ES7→PT and PT 7→ES are the highest.",3.1 Lexical vs Bleached Models,[0],[0]
"Results for the lexical models are generally lower on English, which might be due to smaller amounts of data (see first column in Table 2 providing number of users per language).
",3.1 Lexical vs Bleached Models,[0],[0]
"The abstract features fare surprisingly well and
work a lot better across languages.",3.1 Lexical vs Bleached Models,[0],[0]
"The performance is on average 6% higher across all languages (57.9% for AVG, 63.9% for ALL) in comparison to their lexicalized counterparts, where ABS ALL results in the overall best model.",3.1 Lexical vs Bleached Models,[0],[0]
"For Spanish, the multilingual embedding model clearly outperforms ABS.",3.1 Lexical vs Bleached Models,[0],[0]
"However, the approach requires large Twitterspecific embeddings.4
For our ABS model, if we investigate predictive features over all languages, cf",3.1 Lexical vs Bleached Models,[0],[0]
". Table 4, we can see that the use of an emoji (like ) and shape-based features are predictive of female users.",3.1 Lexical vs Bleached Models,[0],[0]
"Quotes, question marks and length features, for example, appear to be more predictive of male users.",3.1 Lexical vs Bleached Models,[0],[0]
"We experimented with three different conditions, one within language and two across language.",3.2 Human Evaluation,[0],[0]
"For the latter, we set up an experiment where native speakers of Dutch were presented with tweets written in Portuguese and were asked to guess the poster’s gender.",3.2 Human Evaluation,[0],[0]
"In the other experiment, we asked speakers of French to identify the gender of the writer when reading Dutch tweets.",3.2 Human Evaluation,[0],[0]
"In both cases, the participants declared to have no prior knowledge of the target language.",3.2 Human Evaluation,[0],[0]
"For the in-language experiment, we asked Dutch speakers to identify the gender of a user writing Dutch tweets.",3.2 Human Evaluation,[0],[0]
"The
4We tested the approach with more generic (from Wikipedia) but smaller (in terms of vocabulary size) Polyglot embeddings resulting in inferior multilingual embeddings for our task.
",3.2 Human Evaluation,[0],[0]
Dutch speakers who participated in the two experiments are distinct individuals.,3.2 Human Evaluation,[0],[0]
Participants were informed of the experiment’s goal.,3.2 Human Evaluation,[0],[0]
"Their identity is anonymized in the data.
",3.2 Human Evaluation,[0],[0]
"We selected a random sample of 200 users from the Dutch and Portuguese data, preserving a 50/50 gender distribution.",3.2 Human Evaluation,[0],[0]
Each user was represented by twenty tweets.,3.2 Human Evaluation,[0],[0]
The answer key (F/M) order was randomized.,3.2 Human Evaluation,[0],[0]
"For each of the three experiments we had six judges, balanced for gender, and obtained three annotations per target user.
Results and Analysis Inter-annotator agreement for the tasks was measured via Fleiss kappa (n = 3, N = 200), and was higher for the in-language experiment (K = 0.40) than for the cross-language tasks (NL 7→PT: K = 0.25; FR 7→NL: K = 0.28).",3.2 Human Evaluation,[0],[0]
"Table 5 shows accuracy against the gold labels, comparing humans (average accuracy over three annotators) to lexical and bleached models on the exact same subset of 200 users.",3.2 Human Evaluation,[0],[0]
"Systems were tested under two different conditions regarding the number of tweets per user for the target language: machine and human saw the exact same twenty tweets, or the full set of tweets (200) per user, as done during training (Section 3.1).
",3.2 Human Evaluation,[0],[0]
"First of all, our results indicate that in-language performance of humans is 70.5%, which is quite in line with the findings of Flekova et al. (2016), who report an accuracy of 75% on English.",3.2 Human Evaluation,[0],[0]
"Within language, lexicalized models are superior to humans if exposed to enough information (200 tweets setup).",3.2 Human Evaluation,[0],[0]
"One explanation for this might lie in an observation by Flekova et al. (2016), according to which people tend to rely too much on stereotypical lexical indicators when assigning gender to the poster of a tweet, while machines model less evident patterns.",3.2 Human Evaluation,[0],[0]
"Lexicalized models are also superior to the bleached ones, as already seen on the full datasets (Table 2).
",3.2 Human Evaluation,[0],[0]
We can also observe that the amount of information available to represent a user influences system’s performance.,3.2 Human Evaluation,[0],[0]
"Training on 200 tweets per
user, but testing on 20 tweets only, decreases performance by 12 percentage points.",3.2 Human Evaluation,[0],[0]
"This is likely due to the fact that inputs are sparser, especially since the bleached model is trained on 5-grams.5 The bleached model, when given 200 tweets per user, yields a performance that is slightly higher than human accuracy.
",3.2 Human Evaluation,[0],[0]
"In the cross-language setting, the picture is very different.",3.2 Human Evaluation,[0],[0]
"Here, human performance is superior to the lexicalized models, independently of the amount of tweets per user at testing time.",3.2 Human Evaluation,[0],[0]
"This seems to indicate that if humans cannot rely on the lexicon, they might be exploiting some other signal when guessing the gender of a user who tweets in a language unknown to them.",3.2 Human Evaluation,[0],[0]
"Interestingly, the bleached models, which rely on non-lexical features, not only outperform the lexicalized ones in the cross-language experiments, but also neatly match the human scores.",3.2 Human Evaluation,[0],[0]
Most existing work on gender prediction exploits shallow lexical information based on the linguistic production of the users.,4 Related Work,[0],[0]
"Few studies investigate deeper syntactic information (Koppel et al., 2002; Feng et al., 2012) or non-linguistic input, e.g., language-independent clues such as visual (Alowibdi et al., 2013) or network information (Jurgens, 2013; Plank and Hovy, 2015; Ljubešić et al., 2017).",4 Related Work,[0],[0]
A related angle is cross-genre profiling.,4 Related Work,[0],[0]
"In both settings lexical models have limited portability due to their bias towards the language/genre they have been trained on (Rangel et al., 2016; Busger op Vollenbroek et al., 2016; Medvedeva et al., 2017).
",4 Related Work,[0],[0]
"Lexical bias has been shown to affect inlanguage human gender prediction, too.",4 Related Work,[0],[0]
"Flekova et al. (2016) found that people tend to rely too much on stereotypical lexical indicators, while Nguyen et al. (2014) show that more than 10% of the Twitter users do actually not employ words that the crowd associates with their biological sex.",4 Related Work,[0],[0]
Our features abstract away from such lexical cues while retaining predictive signal.,4 Related Work,[0],[0]
"Bleaching text into abstract features is surprisingly effective for predicting gender, though lexical infor-
5We experimented with training on 20 tweets rather than 200, and with different n-gram sizes (e.g., 1–4).",5 Conclusions,[0],[0]
"Despite slightly better results, we decided to use the trained models as they were to employ the same settings across all experiments (200 tweets per users, n = 5), with no further tuning.
mation is still more useful within language (RQ1).",5 Conclusions,[0],[0]
"However, models based on lexical clues fail when transferred to other languages, or require large amounts of unlabeled data from a similar domain as our experiments with the multilingual embedding model indicate.",5 Conclusions,[0],[0]
"Instead, our bleached models clearly capture some signal beyond the lexicon, and perform well in a cross-lingual setting (RQ2).",5 Conclusions,[0],[0]
We are well aware that we are testing our crosslanguage bleached models in the context of closely related languages.,5 Conclusions,[0],[0]
"While some features (such as PunctA, or Frequency) might carry over to genetically more distant languages, other features (such as Vowels and Shape) would probably be meaningless.",5 Conclusions,[0],[0]
"Future work on this will require a sensible setting from a language typology perspective for choosing and testing adequate features.
",5 Conclusions,[0],[0]
"In our novel study on human proficiency for cross-lingual gender prediction, we discovered that people are also abstracting away from the lexicon.",5 Conclusions,[0],[0]
"Indeed, we observe that they are able to detect gender by looking at tweets in a language they do not know (RQ3) with an accuracy of 60% on average.",5 Conclusions,[0],[0]
We would like to thank the three anonymous reviewers and our colleagues for their useful feedback on earlier versions of this paper.,Acknowledgments,[0],[0]
"Furthermore, we are grateful to Chloé Braud for helping with the French human evaluation part.",Acknowledgments,[0],[0]
We would like to thank all of our human participants.,Acknowledgments,[0],[0]
"Gender prediction has typically focused on lexical and social network features, yielding good performance, but making systems highly language-, topic-, and platformdependent.",abstractText,[0],[0]
"Cross-lingual embeddings circumvent some of these limitations, but capture gender-specific style less.",abstractText,[0],[0]
"We propose an alternative: bleaching text, i.e., transforming lexical strings into more abstract features.",abstractText,[0],[0]
This study provides evidence that such features allow for better transfer across languages.,abstractText,[0],[0]
"Moreover, we present a first study on the ability of humans to perform cross-lingual gender prediction.",abstractText,[0],[0]
"We find that human predictive power proves similar to that of our bleached models, and both perform better than lexical models.",abstractText,[0],[0]
Bleaching Text: Abstract Features for Cross-lingual Gender Prediction,title,[0],[0]
"Concerns are rising that machine learning systems which make or support important decisions affecting individuals— such as car insurance pricing, résumé filtering or recidivism prediction—might illegally or unfairly discriminate against certain subgroups of the population (Schreurs et al., 2008; Calders & Žliobaitė, 2012; Barocas & Selbst, 2016).",1. Introduction,[0],[0]
"The growing field of fair learning seeks to formalize relevant requirements, and through altering parts of the algorithmic decision-making pipeline, to detect and mitigate potential discrimination (Friedler et al., 2016).
",1. Introduction,[0],[0]
"Most legally-problematic discrimination centers on differences based on sensitive attributes, such as gender or race (Barocas & Selbst, 2016).",1. Introduction,[0],[0]
"The first type, disparate treatment (or direct discrimination), occurs if individuals are treated differently according to their sensitive attributes (with all others equal).",1. Introduction,[0],[0]
"To avoid disparate treatment, one should not inquire about individuals’ sensitive attributes.",1. Introduction,[0],[0]
"While
1Max Planck Institute for Intelligent Systems 2University of Cambridge 3The Alan Turing Institute 4University of Warwick 5University College London 6Max Planck Institute for Software Systems.",1. Introduction,[0],[0]
"Correspondence to: Niki Kilbertus <niki.kilbertus@tuebingen.mpg.de>.
Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"this has some intuitive appeal and justification (Grgić-Hlača et al., 2018), a significant concern is that sensitive attributes may often be accurately predicted (“reconstructed”) from non-sensitive features (Dwork et al., 2012).",1. Introduction,[0],[0]
"This motivates measures to deal with the second type of discrimination.
",1. Introduction,[0],[0]
Disparate impact (or indirect discrimination) occurs when the outcomes of decisions disproportionately benefit or hurt individuals from subgroups with particular sensitive attribute settings without appropriate justification.,1. Introduction,[0],[0]
"For example, firms deploying car insurance telematics devices (Handel et al., 2014) build up high dimensional pictures of driving behavior which might easily proxy for sensitive attributes even when they are omitted.",1. Introduction,[0],[0]
"Much recent work in fair learning has focused on approaches to avoiding various notions of disparate impact (Feldman et al., 2015; Hardt et al., 2016; Zafar et al., 2017c).
",1. Introduction,[0],[0]
"In order to check and enforce such requirements, the modeler must have access to the sensitive attributes for individuals in the training data—however, this may be undesirable for several reasons (Žliobaitė & Custers, 2016).",1. Introduction,[0],[0]
"First, individuals are unlikely to want to entrust sensitive attributes to modelers in all application domains.",1. Introduction,[0],[0]
"Where applications have clear discriminatory potential, it is understandable that individuals may be wary of providing sensitive attributes to modelers who might exploit them to negative effect, especially with no guarantee that a fair model will indeed be learned and deployed.",1. Introduction,[0],[0]
"Even if certain modelers themselves were trusted, the wide provision of sensitive data creates heightened privacy risks in the event of a data breach.
",1. Introduction,[0],[0]
"Further, legal barriers may limit collection and processing of sensitive personal data.",1. Introduction,[0],[0]
"A timely example is the EU’s General Data Protection Regulation (GDPR), which contains heightened prerequisites for the collection and processing of some sensitive attributes.",1. Introduction,[0],[0]
"Unlike other data, modelers cannot justify using sensitive characteristics in fair learning with their “legitimate interests”—and instead will often need explicit, freely given consent (Veale & Edwards, 2018).
",1. Introduction,[0],[0]
One way to address these concerns was recently proposed by Veale & Binns (2017).,1. Introduction,[0],[0]
"The idea is to involve a highly trusted third party, and may work well in some cases.",1. Introduction,[0],[0]
"However, there are significant potential difficulties: individuals must disclose their sensitive attributes to the third party (even if an individual trusts the party, she may have concerns that
the data may somehow be obtained or hacked by others, e.g., Graham, 2017); and the modeler must disclose their model to the third party, which may be incompatible with their intellectual property or other business concerns.
Contribution.",1. Introduction,[0],[0]
We propose an approach to detect and mitigate disparate impact without disclosing readable access to sensitive attributes.,1. Introduction,[0],[0]
"This reflects the notion that decisions should be blind to an individual’s status—depicted in courtrooms by a blindfolded Lady Justice holding balanced scales (Bennett Capers, 2012).",1. Introduction,[0],[0]
We assume the existence of a regulator with fairness aims (such as a data protection authority or anti-discrimination agency).,1. Introduction,[0],[0]
"With recent methods from secure multi-party computation (MPC), we enable auditable fair learning while ensuring that both individuals’ sensitive attributes and the modeler’s model remain private to all other parties—including the regulator.",1. Introduction,[0],[0]
"Desirable fairness and accountability applications we enable include:
1.",1. Introduction,[0],[0]
Fairness certification.,1. Introduction,[0],[0]
"Given a model and a dataset of individuals, check that the model satisfies a given fairness constraint (we consider several notions from the literature, see Section 2.2); if yes, generate a certificate.
2.",1. Introduction,[0],[0]
Fair model training.,1. Introduction,[0],[0]
"Given a dataset of individuals, learn a model guaranteed and certified to be fair.
3.",1. Introduction,[0],[0]
Decision verification.,1. Introduction,[0],[0]
"A malicious modeler might go through fair model training, but then use a different model in practice.",1. Introduction,[0],[0]
"To address such accountability concerns (Kroll et al., 2016), we efficiently provide for an individual to challenge a received outcome, verifying that it matches the outcome from the previously certified model.
",1. Introduction,[0],[0]
We rely on recent theoretical developments in MPC (see Section 3) which we extend to admit linear constraints in order to enforce fairness requirements.,1. Introduction,[0],[0]
These extensions may be of independent interest.,1. Introduction,[0],[0]
"We demonstrate the realworld efficacy of our methods, and shall make our code publicly available.",1. Introduction,[0],[0]
Here we formalize our setup and requirements.,2. Fairness and Privacy Requirements,[0],[0]
"We assume three categories of participants: a modeler M, a regulator REG, and users U1, . . .",2.1. Assumptions and Incentives,[0],[0]
",Un.",2.1. Assumptions and Incentives,[0],[0]
"For each user, we consider a vector of sensitive features (or attributes, we use the terms interchangeably) zi ∈ Z (e.g., ethnicity or gender) which might be a source of discrimination, and a vector of non-sensitive features xi ∈ X (discrete or real).",2.1. Assumptions and Incentives,[0],[0]
"Additionally, each user has a non-sensitive feature yi ∈ Y which the modeler M would like to predict—the label (e.g., loan default).",2.1. Assumptions and Incentives,[0],[0]
"In line with current work in fair learning, we
assume that all zi and yi attributes are binary, though our MPC approach could be extended to multi-label settings.",2.1. Assumptions and Incentives,[0],[0]
"The source of societal concern is that sensitive attributes zi are potentially correlated with xi or yi.
",2.1. Assumptions and Incentives,[0],[0]
"Modeler M wishes to train a model fθ : X → Y , which accurately maps features xi to labels yi, in a supervised fashion.",2.1. Assumptions and Incentives,[0],[0]
We assume M needs to keep the model private for intellectual property or other business reasons.,2.1. Assumptions and Incentives,[0],[0]
"The model fθ does not use sensitive information zi as input to prevent disparate treatment (direct discrimination).
",2.1. Assumptions and Incentives,[0],[0]
"For each user Ui, M observes or is provided xi, yi.",2.1. Assumptions and Incentives,[0],[0]
The sensitive information in zi is required to ensure fθ meets a given disparate impact fairness condition F (see Section 2.2).,2.1. Assumptions and Incentives,[0],[0]
"While each user Ui wants fθ to meet F, they also wish to keep zi private from all other parties.",2.1. Assumptions and Incentives,[0],[0]
"The regulator REG aims to ensure that M deploys only models that meet fairness condition F. It has no incentive to collude with M (if collusion were a concern, more sophisticated cryptographic protocols would be required).",2.1. Assumptions and Incentives,[0],[0]
"Further, the modeler M might be legally obliged to demonstrate to the regulator REG that their model meets fairness condition F before it can be publicly deployed.",2.1. Assumptions and Incentives,[0],[0]
"As part of this, REG also has a positive duty to enable the training of fair models.
",2.1. Assumptions and Incentives,[0],[0]
"In Section 2.3, we define and address three fundamental problems in our setup: certification, training, and verification.",2.1. Assumptions and Incentives,[0],[0]
"For each problem, we present its functional goal and its privacy requirements.",2.1. Assumptions and Incentives,[0],[0]
"We refer to D = {(xi, yi)}ni=1 and Z = {zi}ni=1 as the non-sensitive and sensitive data, respectively.",2.1. Assumptions and Incentives,[0],[0]
"In Section 2.2, we first provide necessary background on various notions of fairness that have been explored in the fair learning literature.",2.1. Assumptions and Incentives,[0],[0]
"In large part, works that formalize fairness in machine learning do so by balancing a certain condition between groups of people with different sensitive attributes, z versus z′. Several possible conditions have been proposed.",2.2. Fairness Criteria,[0],[0]
"Popular choices include (where y ∈ {0, 1} and ŷ is the prediction of a machine learning model):
P (ŷ = y | z) = P (ŷ = y | z′) (acc) (1) P (ŷ = y | z, y = 1) = P (ŷ = y | z′, y = 1) (TPR) (2) P (ŷ = y | z, y = 0) = P (ŷ = y | z′, y = 0) (TNR) (3) P (ŷ = y | z, ŷ",2.2. Fairness Criteria,[0],[0]
"= 1) = P (ŷ = y | z′, ŷ = 1) (PPV) (4) P (ŷ = y | z, ŷ = 0) = P (ŷ = y | z′, ŷ = 0) (NPV) (5)
P (ŷ = 1 | z) = P (ŷ = 1 | z′) (AR) (6) Respectively, these consider equality of: (1) accuracy, (2) true positive rate, (3) true negative rate, (4) positive predicted value, (5) negative predicted value, or (6) acceptance rate.",2.2. Fairness Criteria,[0],[0]
"Works which use these or related notions include (Hardt et al., 2016; Zafar et al., 2017c;a;b).
",2.2. Fairness Criteria,[0],[0]
In this work we focus on a variant of eq.,2.2. Fairness Criteria,[0],[0]
"(6), formulated as a constrained optimization problem by Zafar et al. (2017c) mimicking the p%-rule (Biddle, 2006): for any binary protected attribute z ∈ {0, 1}, it aims to achieve
min { P (ŷ = 1 | z = 1) P (ŷ = 1 | z = 0) , P (ŷ = 1 | z = 0) P (ŷ = 1 | z = 1) } ≥ p 100 .",2.2. Fairness Criteria,[0],[0]
"(7)
We believe that in future work, a similar MPC approach could also be used for conditions (1), (2) or (3)—i.e., all the other measures which, to our knowledge, have been addressed with efficient standard (non-private) methods.",2.2. Fairness Criteria,[0],[0]
Fairness certification.,"2.3. Certification, Training, and Verification",[0],[0]
"Given a notion of fairness F, the modeler M would like to work with the regulator REG to obtain a certificate that model fθ is fair.","2.3. Certification, Training, and Verification",[0],[0]
"To do so, we propose that users send their non-sensitive data D to REG; and send encrypted versions of their sensitive data Z to both M and REG.","2.3. Certification, Training, and Verification",[0],[0]
Neither M nor REG can read the sensitive data.,"2.3. Certification, Training, and Verification",[0],[0]
"However, we can design a secure protocol between M and REG (described in Section 3) to certify if the model is fair.","2.3. Certification, Training, and Verification",[0],[0]
"This setup is shown in Figure 1 (Left).
","2.3. Certification, Training, and Verification",[0],[0]
"While both REG and M learn the outcome of the certification, we require the following privacy constraints: (C1) privacy of sensitive user data: no one other than Ui ever learns zi in the clear, (C2) model secrecy: only M learns fθ in the clear, and (C3) minimal disclosure of D to REG: only REG learns D in the clear.
","2.3. Certification, Training, and Verification",[0],[0]
Fair model training.,"2.3. Certification, Training, and Verification",[0],[0]
How can a modeler M learn a fair model without access to users’ sensitive data Z?,"2.3. Certification, Training, and Verification",[0],[0]
We propose to solve this by having users send their non-sensitive data D to M and to distribute encryptions of their sensitive data to M and REG as in certification.,"2.3. Certification, Training, and Verification",[0],[0]
We shall describe a secure MPC protocol between M and REG to train a fair model fθ privately.,"2.3. Certification, Training, and Verification",[0],[0]
"This setup is shown in Figure 1 (Center).
","2.3. Certification, Training, and Verification",[0],[0]
"Privacy constraints: (C1) privacy of sensitive user data, (C2) model secrecy, and (C3) minimal disclosure of D to M.
Decision verification.","2.3. Certification, Training, and Verification",[0],[0]
Assume that a malicious M has had model fθ successfully certified by REG as above.,"2.3. Certification, Training, and Verification",[0],[0]
It then swaps fθ for another potentially unfair model fθ′ in the real world.,"2.3. Certification, Training, and Verification",[0],[0]
"When a user receives a decision ŷ, e.g., her mortgage is denied, she can then challenge that decision by asking REG for a verification V .","2.3. Certification, Training, and Verification",[0],[0]
"The verification involves M and REG, and consists of verifying that fθ′(x) = fθ(x), where x is the user’s non-sensitive data.","2.3. Certification, Training, and Verification",[0],[0]
"This ensures that the user would have been subject to the same result with the certified model fθ, even if fθ′ 6= fθ and fθ′ is not fair.","2.3. Certification, Training, and Verification",[0],[0]
"Hence, while there is no simple technical way to prevent a malicious M from deploying an unfair model, it will get
caught if a user challenges a decision that would differ under fθ.","2.3. Certification, Training, and Verification",[0],[0]
"This setup is shown in Figure 1 (Right).
","2.3. Certification, Training, and Verification",[0],[0]
"Privacy constraint: While REG and the user learn the outcome of the verification, we require (C1) privacy of sensitive user data, and (C2) model secrecy.","2.3. Certification, Training, and Verification",[0],[0]
We use a regulator for several reasons.,2.4. Design Choices,[0],[0]
"Given fair learning is of most benefit to vulnerable individuals, we do not wish to deter adoption with high individual burdens.",2.4. Design Choices,[0],[0]
"While MPC could be carried out without the involvement of a regulator, using all users as parties, this comes at a significantly greater computational cost.",2.4. Design Choices,[0],[0]
"With current methods, taking that approach would be unrealistic given the size of the user-base in many domains of concern, and would furthermore require all users to be online simultaneously.",2.4. Design Choices,[0],[0]
"Introducing a regulator removes these barriers and leaves users’ computational burden at a minimum level, with envisaged applications practical with only their web browsers.
",2.4. Design Choices,[0],[0]
"In cases where users are uncomfortable sharing D with either REG or M, it is trivial to extend all three tasks such that all of xi, yi, zi remain private throughout, with the computation cost increasing only by a factor of 2.",2.4. Design Choices,[0],[0]
"This extension would sometimes be desirable as it restricts the view of M to the final model, prohibiting inferences about Z when D is known.",2.4. Design Choices,[0],[0]
"However, this setup hinders exploratory data analysis by the modeler which might promote robust model-building, and, in the case of verification, validation by the regulator that user-provided data is correct.",2.4. Design Choices,[0],[0]
Our proposed solution to these three problems is to use Multi-Party Computation (MPC).,3. Our Solution,[0],[0]
"Before we describe how it can be applied to fair learning, we first present the basic principles of MPC, as well as its limitations particularly in the context of machine learning applications.",3. Our Solution,[0],[0]
"Multi-Party Computation protocols allow two parties P1 andP2 holding secret values x1 and x2 to evaluate an agreedupon function f , via y = f(x1, x2) in a way in which the parties (either both or one of them) learn only y.",3.1. MPC for Machine Learning,[0],[0]
"For example, if f(x1, x2)",3.1. MPC for Machine Learning,[0],[0]
"= I(x1 < x2), then the parties would learn which of their values is bigger, but nothing else.1 This corresponds to the well-known Yao’s millionaires problem: two millionaires want to conclude who is richer without disclosing their wealth to each other.",3.1. MPC for Machine Learning,[0],[0]
"The problem was introduced by Andrew Yao in 1982, and kicked off the area of multi-party computation in cryptography.
",3.1. MPC for Machine Learning,[0],[0]
"1The function I is 1 if its argument is true and 0 otherwise.
",3.1. MPC for Machine Learning,[0],[0]
"In our setting—instead of a simple comparison as in the millionaires problem—f will be either (i) a procedure to check the fairness of a model and certify it, (ii) a machine learning training procedure with fairness constraints, or (iii) a model evaluation to verify a decision.",3.1. MPC for Machine Learning,[0],[0]
The two parties involved in our computation are the modeler M and the regulator REG.,3.1. MPC for Machine Learning,[0],[0]
"The inputs depend on the case (see Figure 1).
",3.1. MPC for Machine Learning,[0],[0]
"As generic solutions do not yet scale to real-world data analysis tasks, one typically has to tailor custom protocols to the desired functionality.",3.1. MPC for Machine Learning,[0],[0]
"This approach has been followed successfully for a variety of machine learning tasks such as logistic and linear regression (Nikolaenko et al., 2013b; Gascón et al., 2017; Mohassel & Zhang, 2017), neural network training (Mohassel & Zhang, 2017) and evaluation (Juvekar et al., 2018; Liu et al., 2017), matrix factorization (Nikolaenko et al., 2013a), and principal component analysis (Al-Rubaie et al., 2017).",3.1. MPC for Machine Learning,[0],[0]
In the next section we review challenges beyond scalability issues that arise when implementing machine learning algorithms in MPC.,3.1. MPC for Machine Learning,[0],[0]
MPC protocols can be classified into two groups depending on whether the target function is represented as either a Boolean or arithmetic circuit.,3.2. Challenges in Multi-Party Machine Learning,[0],[0]
"All protocols proceed by having the parties jointly evaluate the circuit, processing it gate by gate while keeping intermediate values hidden from both parties by means of a secret sharing scheme.",3.2. Challenges in Multi-Party Machine Learning,[0],[0]
"While representing functions as circuits can be done without losing expressiveness, it means certain operations are impractical.",3.2. Challenges in Multi-Party Machine Learning,[0],[0]
"In particular, algorithms that execute different branches depending on the input data will explode in size when implemented as circuits, and in some cases lose their run time guarantees (e.g., consider binary search).
",3.2. Challenges in Multi-Party Machine Learning,[0],[0]
"Crucially, this applies to floating-point arithmetic.",3.2. Challenges in Multi-Party Machine Learning,[0],[0]
"While
this is work in progress, state-of-the-art MPC floating-point arithmetic implementations take more than 15 milliseconds to multiply two 64 bit numbers (Demmler et al., 2015a,
Table 4), which is prohibitive for our applications.",3.2. Challenges in Multi-Party Machine Learning,[0],[0]
"Hence, machine learning MPC protocols are limited to fixed-point arithmetic.",3.2. Challenges in Multi-Party Machine Learning,[0],[0]
Overcoming this limitation is a key challenge for the field.,3.2. Challenges in Multi-Party Machine Learning,[0],[0]
"Another necessity for the feasibility of MPC is to approximate non-linear functions such as the sigmoid, ideally by (piecewise) linear functions.",3.2. Challenges in Multi-Party Machine Learning,[0],[0]
Input sharing.,3.3. Our MPC Protocols,[0],[0]
"To implement the functionality from Figure 1, we first need a secure procedure for the users to secret share a sensitive value, for example her race, with the modeler M and the regulator REG.",3.3. Our MPC Protocols,[0],[0]
We use additive secret sharing.,3.3. Our MPC Protocols,[0],[0]
A value z is represented in a finite domain Zq—we use q = 264.,3.3. Our MPC Protocols,[0],[0]
"To share z, the user samples a value r from Zq uniformly at random, and sends z − r to M and r to REG.",3.3. Our MPC Protocols,[0],[0]
"While z can be reconstructed (and subsequently operated on) inside the MPC computation by means of a simple addition, each share on its own does not reveal anything z",3.3. Our MPC Protocols,[0],[0]
(other than that it is in Zq).,3.3. Our MPC Protocols,[0],[0]
"One can think of arithmetic sharing as a “distributed one-time pad”.
",3.3. Our MPC Protocols,[0],[0]
"In Figure 1, we now reinterpret the key held by REG and the encrypted z by M, as their corresponding shares of the sensitive attributes and denote them by 〈z〉1 and 〈z〉2 respectively.",3.3. Our MPC Protocols,[0],[0]
"The idea of privately outsourcing computation to two non-colluding parties in this way is recurrent in MPC, and often referred to as the two-server model (Mohassel & Zhang, 2017; Gascón et al., 2017; Nikolaenko et al., 2013b; Al-Rubaie et al., 2017).
",3.3. Our MPC Protocols,[0],[0]
Signing and checking a model.,3.3. Our MPC Protocols,[0],[0]
"Note that certification and verification partly correspond to sub-procedures of the fair training task: during training we check the fairness
constraint F, and repeatedly evaluate partial models on the training dataset (using gradient descent).",3.3. Our MPC Protocols,[0],[0]
"Hence, certification and verification do not add technical difficulties over training, which is described in detail in Section 4.",3.3. Our MPC Protocols,[0],[0]
"However, for verification, we still need to “sign” the model, i.e., REG should obtain a signature s(θ) as a result of model certification, see Figure 1 (Left).",3.3. Our MPC Protocols,[0],[0]
"This signature is used to check in the verification phase, whether a given model θ′ from M satisfies s(θ′) = s(θ) for a certified fair model θ (in which case θ = θ′ with high probability).",3.3. Our MPC Protocols,[0],[0]
"Moreover, we need to preserve the secrecy of the model, i.e., REG should not be able to recover θ from s(θ).",3.3. Our MPC Protocols,[0],[0]
"These properties, given that the space of models is large, calls for a cryptographic hash function, such as SHA-256.
",3.3. Our MPC Protocols,[0],[0]
"Additionally, in our functionality, the hash of θ should be computed inside MPC, to hide θ from REG.",3.3. Our MPC Protocols,[0],[0]
"Fortunately, cryptographic hashes such as SHA-256 are a common benchmark functionality in MPC, and their execution is highly optimized.",3.3. Our MPC Protocols,[0],[0]
"More concretely, the overhead of computing s(θ), which needs to be done both for certification and verification is of the order of fractions of a second (Keller et al., 2013, Figure 14).",3.3. Our MPC Protocols,[0],[0]
"While cryptographic hash functions have various applications in MPC, we believe the application to machine learning model certification is novel.
",3.3. Our MPC Protocols,[0],[0]
"Hence, certification is implemented in MPC as a check that θ satisfies the criterion F, followed by the computation of s(θ).",3.3. Our MPC Protocols,[0],[0]
"On the other hand, for verification, the MPC protocol first computes the signature of the model provided by M, and then proceeds with a prediction as long as the computed signature matches the one obtained by REG in the verification phase.",3.3. Our MPC Protocols,[0],[0]
"An alternative solution is possible based on symmetric encryption under a shared key, as highly efficient MPC implementations of block ciphers such as AES are available (Keller et al., 2017).
",3.3. Our MPC Protocols,[0],[0]
Fair training.,3.3. Our MPC Protocols,[0],[0]
"To realize the fair training functionality from the previous section, we follow closely the techniques recently introduced by Mohassel & Zhang (2017).",3.3. Our MPC Protocols,[0],[0]
"Specifically, we extend their custom MPC protocol for logistic regression to additionally handle linear constraints.",3.3. Our MPC Protocols,[0],[0]
"This extension may be of independent interest, and has applications for privacy-preserving machine learning beyond fairness.",3.3. Our MPC Protocols,[0],[0]
"The concrete technical difficulties in achieving this goal, and how to overcome them, are presented in the next section.",3.3. Our MPC Protocols,[0],[0]
"The formal privacy guarantees of our fair training protocol are stated in the following proposition.
",3.3. Our MPC Protocols,[0],[0]
Proposition 1.,3.3. Our MPC Protocols,[0],[0]
"For non-colluding M and REG, our protocol implements the fair model training functionality satisfying constraints (C1)-(C3) in Section 2.3 in the presence of a semi-honest adversary.
",3.3. Our MPC Protocols,[0],[0]
"The proof holds in the random oracle model, as a standard simulation argument combining several MPC primitives
(Mohassel & Zhang, 2017; Gascón et al., 2017).",3.3. Our MPC Protocols,[0],[0]
"It leverages security of arithmetic sharing, garbled circuits, and oblivious transfer protocols in the semi-honest model (Goldreich et al., 1987).",3.3. Our MPC Protocols,[0],[0]
"A general introduction to MPC, as well as a description of the relevant techniques from (Mohassel & Zhang, 2017) used in our protocol, can be found in Section A in the appendix.",3.3. Our MPC Protocols,[0],[0]
We now present our tailored approaches for learning and evaluating fair models with encrypted sensitive attributes.,4. Technical Challenges of Fair Training,[0],[0]
"We do this via the following contributions:
•",4. Technical Challenges of Fair Training,[0],[0]
"We argue that current optimization techniques for fair learning algorithms are unstable for fixed-point data, which is required by our MPC techniques.",4. Technical Challenges of Fair Training,[0],[0]
"• We describe optimization schemes that are well-suited
for learning over fixed-point number representations.",4. Technical Challenges of Fair Training,[0],[0]
"• We combine tricks to approximate non-linear functions
with specialized operations to make fixed-point arithmetic feasible and avoid over- and under-flows.
",4. Technical Challenges of Fair Training,[0],[0]
"The optimization problem at hand is to learn a classifier θ subject to a (often convex) fairness constraint F(θ):
min θ
n∑
i=1
`θ(xi, yi) subject to F(θ) ≤ 0 , (8)
where `θ is a loss term (the logistic loss in this work).",4. Technical Challenges of Fair Training,[0],[0]
"We collect user data from U1, . . .",4. Technical Challenges of Fair Training,[0],[0]
",Un into matrices X ∈ Rn×d,Z ∈ {0, 1}n×p and a label vector y ∈ {0, 1}n.",4. Technical Challenges of Fair Training,[0],[0]
"Zafar et al. (2017c) use a convex approximation of the p%rule, see eq. (7), for linear classifiers to derive the constraint:
F(θ) = 1
n |Ẑ>Xθ|",4. Technical Challenges of Fair Training,[0],[0]
"− c , (9)
where Ẑ is the matrix of all ẑi := zi",4. Technical Challenges of Fair Training,[0],[0]
− z̄ and c ∈,4. Technical Challenges of Fair Training,[0],[0]
Rd is a constant vector corresponding to the tightness of the fairness constraint.,4. Technical Challenges of Fair Training,[0],[0]
"Here, z̄ is the mean of all inputs zi.",4. Technical Challenges of Fair Training,[0],[0]
"With A := 1/nẐ>X, the p% constraint reads F(θ) = |Aθ|",4. Technical Challenges of Fair Training,[0],[0]
"− c, where the absolute value is taken element-wise.",4. Technical Challenges of Fair Training,[0],[0]
"To solve the optimization problem in eq. (8), with the fairness function F in eq. (9), Zafar et al. (2017c) use Sequential Least Squares Programming (SLSQP).",4.1. Current Techniques,[0],[0]
This technique works by reformulating eq.,4.1. Current Techniques,[0],[0]
(8) as a sequence of Quadratic Programs (QPs).,4.1. Current Techniques,[0],[0]
"After solving each QP, their algorithm uses the Han-Powell method, a quasi-Newton method that iteratively approximates the Hessian H of the objective function via the update
Ht+1 = Ht + l∆l > ∆
θ>∆l∆ − Htθ∆θ
> ∆Ht
θ>∆Htθ∆ ,
where l∆ = l(θt+1,λt+1)",4.1. Current Techniques,[0],[0]
"− l(θt,λt) and l(θt,λt) =∑n i=1",4.1. Current Techniques,[0],[0]
"`θt(xi, yi) + λ
>F(θt) is the Lagrangian of eq.",4.1. Current Techniques,[0],[0]
(8).,4.1. Current Techniques,[0],[0]
"Finally, θ∆ = θt+1 − θt.",4.1. Current Techniques,[0],[0]
There are two issues with this approach from an MPC perspective.,4.1. Current Techniques,[0],[0]
"First, solving a sequence of QPs is prohibitively time-consuming in MPC.",4.1. Current Techniques,[0],[0]
"Second, while the above HanPowell update performs well on floating-point data, the two divisions by non-constant, non-integer numbers easily underflow or overflow with fixed-point numbers.",4.1. Current Techniques,[0],[0]
"Instead, to solve the optimization problem in eq.",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
"(8), we perform stochastic gradient descent and experiment with the following techniques to incorporate the constraints.
",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
Lagrangian multipliers.,4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
"Here we minimize
L := 1 n
n∑
i=1
`BCEθ (xi, yi) + λ",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
">max{F(θ),0} ,
using stochastic gradient descent, i.e., alternating updates θ ← θ − ηθ∇θL and λ ← max{λ + ηλ∇λL,0}, where ηθ, ηλ are the learning rates.
",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
Projected gradient descent.,4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
"For this method, consider specifically the p%-rule based notion F(θ) = |Aθ|",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
"− c. We first define Â as the matrix consisting of the rows of A for which F(θ) > 0, i.e., where the constraint is active.",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
"In each step, we project the computed gradient of the binary-crossentropy loss LBCE—either of a single example or averaged over a minibatch—back into the constraint set, i.e.,
θ ← θ − ηθ(Idd − Â>(ÂÂ>)−1Â)∇θ`BCEθ .",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
"(10)
Interior point log barrier (Boyd & Vandenberghe, 2004).",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
We can approximate eq.,4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
"(8) for the p%-rule constraint F(θ) = |Aθ|−c by: minimize∑ni=1 `BCEθ (xi, yi)− 1 t ∑p j=1 ( log(a>j θ + cj) + log(−a>j θ + cj) ) , where aj is the jth row of A.",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
The parameter t trades off the approximation of the true objective (I−(u) = 0 for u ≤ 0 and I−(u) =∞ for u > 0) and the smoothness of the objective function.,4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
"Throughout training t is increased, allowing the solution to move closer to the boundary.",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
"As the gradient of the objective has a simple closed form representation, we can perform regular (stochastic) gradient descent.
",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
"After extensive experiments (see Section 5) we found the Lagrangian multipliers technique to work best, both in yielding high accuracies, reliably staying within the constraints and being robust to hyperparameter changes such as learning rates or the batch size.",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
"For a proof of concept, in Section 5 we focus on the p%-rule, i.e., eq. (9).",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
Note that the gradients for eq. (2) and eq.,4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
"(3) take a similarly simple form, i.e., balancing the true positive or true negative rates (corresponding to equal opportunity or equal odds) is simple to implement
for the Lagrangian multiplier technique, but harder for projected gradient descent.",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
"However, these fairness notions are more expensive as we have to compute Z>X for each update step, instead of pre-computing it once at the beginning of training, see Algorithm 1 in the appendix.",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
"We could speed up the computation again by evaluating the constraint only on the current minibatch for each update, in which case we risk violating the fairness constraint.
",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
MPC-friendliness.,4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
"For eq. (9), we can compute the gradient updates in all three methods with elementary linear algebra operations (matrix multiplications) and a single evaluation of the logistic function.",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
"While MPC is well suited for linear operations, most nonlinear functions are prohibitively expensive to evaluate in an MPC framework.",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
Hence we tried two piecewise linear approximations for σ(x).,4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
"The first was recently suggested for machine learning in an MPC context (Mohassel & Zhang, 2017) and is simply constant 0 and 1 for x < −0.5 and x > 0.5 respectively, and linear in between.",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
The second uses the optimal first order Chebychev polynomial on each interval,4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
"[x, x + 1] for x ∈ {−5,−4, . . .",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
", 4}, and is constant 0 or 1 outside of [−5, 5] (Faiedh et al., 2001).",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
"While it is more accurate, we only report results for the simpler first approximation, as it yielded equal or better results in all our experiments.
",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
"As the largest number that can be represented in fixed-point format with m integer and m fractional bits is roughly 2m + 1, overflow becomes a common problem.",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
"Since we whiten the features X column-wise, we need to be careful whenever we add roughly 2m numbers or more, because we cannot even represent numbers greater than 2m. In particular, the minibatch size has to be smaller than this limit.",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
"For large n, the multiplication Z>X in the fairness function F for the p%-rule is particularly problematic.
",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
"Hence, we split both factors into blocks of size b × b with b < 2m and normalize the result of each blocked matrix multiplication by b before adding up the blocks.",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
We then multiply the sum by b/n > 2−m.,4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
"As long as b, b/n (and thus also n/b) can be represented with sufficient precision, which is the case in all our experiments, this procedure avoids under- and overflow.",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
Note that we require the sample size n to be a multiple of b.,4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
"In practice, we have to either discard or duplicate part of the data.",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
"Since the latter may introduce bias, we recommend subsampling.",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
Once we have (an approximation of),4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
"A ∈ Rp×d, we resort to normal matrix multiplication, as typically p, d .",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
"100, see Table 1.
Division is prohibitively expensive in MPC.",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
"Hence, we set the minibatch size to a power of two, which allows us to use fast bit shifts for divisions when averaging over minibatches.",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
"To exploit the same trick when averaging over/across blocks in the blocked matrix multiplication, we choose n as the largest possible power of two, see Table 1.
",4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
Algorithm 1 in Section B in the appendix describes the computations M and REG have to run for fair model training using the Lagrangian multiplier technique and the p%-rule from eq.,4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
(9).,4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
We implicitly assume all computations are performed jointly on additively shared secrets.,4.2. Fixed-Point-Friendly Optimization Techniques,[0],[0]
The root cause for most technical difficulties pointed out in the previous section is the necessity to work with fixed-point numbers and the high computational cost of MPC.,5. Experiments,[0],[0]
"Hence, major concerns are loss of precision and infeasible running times.",5. Experiments,[0],[0]
"In this section, we show how to overcome both doubts and that fair training, certification and verification are feasible for realistic datasets.",5. Experiments,[0],[0]
We work with two separate code bases.,5.1. Experimental Setup and Datasets,[0],[0]
"Our Python code does not implement MPC, to be able to flexibly switch between floating and fixed-point numbers as well as exact non-linear functions and their approximations.",5.1. Experimental Setup and Datasets,[0],[0]
We use it mostly for validation and empirical guidance in our design choices.,5.1. Experimental Setup and Datasets,[0],[0]
"The full MPC protocol is implemented in C++ on top of the Obliv-C garbled circuits framework (Zahur & Evans, 2015a) and the Absentminded Crypto Kit (lib).",5.1. Experimental Setup and Datasets,[0],[0]
This is done as described in Section 3 for the Lagrangian multiplier technique (see Section A in the appendix for more details).,5.1. Experimental Setup and Datasets,[0],[0]
It accurately mirrors the computations performed by the first implementation on encrypted data.2,5.1. Experimental Setup and Datasets,[0],[0]
"Except for the timing results in Table 1, all comparisons with floatingpoint numbers or non-linearities were done with the versatile Python implementation.",5.1. Experimental Setup and Datasets,[0],[0]
"Details about parameters and the algorithm can be found in Section B in the appendix.
",5.1. Experimental Setup and Datasets,[0],[0]
"We consider 5 real world datasets, namely the adult (Adult), German credit (German), and bank market (Bank) datasets from the UCI machine learning repository (Lichman, 2013), the stop, question and frisk 2012 dataset (SQF),3 and the COMPAS dataset (Angwin et al., 2016) (COMPAS).",5.1. Experimental Setup and Datasets,[0],[0]
"For practical purposes (see Section 4), we subsample 2i examples from each dataset with the largest possible i, see Table 1.",5.1. Experimental Setup and Datasets,[0],[0]
"Moreover, we also run on synthetic data, generated
2Code is available at https://github.com/ nikikilbertus/blind-justice
3https://perma.cc/6CSM-N7AQ
as described by Zafar et al. (2017c, Section 4.1), as it allows us to control the correlation between the sensitive attributes and the class labels.",5.1. Experimental Setup and Datasets,[0],[0]
It is thus well suited to observe how different optimization techniques handle the fairness-accuracy trade off.,5.1. Experimental Setup and Datasets,[0],[0]
For comparison we use the SLSQP approach described in Section 4.1 as a baseline.,5.1. Experimental Setup and Datasets,[0],[0]
"We run all methods for a range of constraint values in [10−4, 100] and a corresponding range for SLSQP.
",5.1. Experimental Setup and Datasets,[0],[0]
"In the plots in this section, discontinuations of lines indicate failed experiments.",5.1. Experimental Setup and Datasets,[0],[0]
"The most common reasons are overflow and underflow for fixed-point numbers, and instability due to exploding gradients.",5.1. Experimental Setup and Datasets,[0],[0]
Plots and analyses for the remaining datasets can be found in Section C in the appendix.,5.1. Experimental Setup and Datasets,[0],[0]
First we evaluate which of the three optimization techniques works best in practice.,5.2. Comparing Optimization Techniques,[0],[0]
Figure 2 shows the test set accuracy over the constraint value.,5.2. Comparing Optimization Techniques,[0],[0]
"By design, the synthetic dataset exhibits a clear trade-off between accuracy and fairness.",5.2. Comparing Optimization Techniques,[0],[0]
"The Lagrange technique closely follows the (dotted) baseline from (Zafar et al., 2017c), whereas iplb performs slightly worse (and fails for small c).",5.2. Comparing Optimization Techniques,[0],[0]
"Even though the projected gradient method formally satisfies the proxy constraint for the p% rule, it does so by merely shrinking the parameter vector θ, which is why it also fails for small c.",5.2. Comparing Optimization Techniques,[0],[0]
"We analyze this behavior in more detail in Section C in the appendix.
",5.2. Comparing Optimization Techniques,[0],[0]
"The COMPAS dataset is the most challenging as it contains 7 sensitive attributes, one of which has only 10 positive instances in the training set.",5.2. Comparing Optimization Techniques,[0],[0]
"Since we enforce the fairness constraint individually for each sensitive attribute (we randomly picked one for visualization), the classifier tends to collapse to negative predictions.",5.2. Comparing Optimization Techniques,[0],[0]
"All three methods maintain close to optimal accuracy in the unconstrained region, but collapse more quickly than SLSQP.",5.2. Comparing Optimization Techniques,[0],[0]
This example shows that the p%-rule proxy itself needs careful interpretation when applied to multiple sensitive attributes simultaneously and that our SGD based approach seems particularly prone to collapse in such a scenario.,5.2. Comparing Optimization Techniques,[0],[0]
On the Bank dataset accuracy increases for iplb and Lagrange when the constraint becomes active as c decreases until they match the baseline.,5.2. Comparing Optimization Techniques,[0],[0]
Determining the cause of this—perhaps unintuitive—behavior requires further investigation.,5.2. Comparing Optimization Techniques,[0],[0]
We currently suspect the constraint to act as a regularizer.,5.2. Comparing Optimization Techniques,[0],[0]
"The projected gradient method is unreliable on the Bank dataset.
",5.2. Comparing Optimization Techniques,[0],[0]
"Empirically, the Lagrangian multiplier technique is most robust with maximal deviations of accuracy from SLSQP of < 4% across the 6 datasets and all constraint values.",5.2. Comparing Optimization Techniques,[0],[0]
We substantiate this claim in Section C of the appendix.,5.2. Comparing Optimization Techniques,[0],[0]
For the rest of this section we only report results for Lagrangian multipliers.,5.2. Comparing Optimization Techniques,[0],[0]
Figure 2 also shows that using a piecewise linear approximation as described in Section 4 for the logistic function does not spoil performance.,5.2. Comparing Optimization Techniques,[0],[0]
Figure 3 shows how the fractions of users with positive outcomes in the two groups (z = 0 is continuous and z = 1 is dashed) are gradually balanced as we decrease the fairness constraint c. These plots can be interpreted as the degree to which disparate impact is mitigated as the constraint is tightened.,"5.3. Fair Training, Certification and Verification",[0],[0]
The effect is most pronounced for the synthetic dataset by construction.,"5.3. Fair Training, Certification and Verification",[0],[0]
"As discussed above, the collapse for the COMPAS dataset occurs faster than for SLSQP due to the constraints from multiple sensitive attributes.","5.3. Fair Training, Certification and Verification",[0],[0]
"In the Bank dataset, for large c—before the constraint becomes active—the fractions of positive outcomes for z = 1 differ, which is related to the slightly suboptimal accuracy at large c that needs further investigation.","5.3. Fair Training, Certification and Verification",[0],[0]
"However, as the constraint becomes active, the fractions are balanced at a similar rate as the baseline.","5.3. Fair Training, Certification and Verification",[0],[0]
"Overall, our Lagrangian multiplier technique with fixed point numbers and piecewise linear approximations of non-linearities robustly manages to satisfy the p%-rule proxy at similar rates as the baseline with only minor losses in accuracy on all but the challenging COMPAS dataset.
","5.3. Fair Training, Certification and Verification",[0],[0]
In Table 1 we show the online running times of 10 training epochs on a laptop computer.,"5.3. Fair Training, Certification and Verification",[0],[0]
"While training takes several orders of magnitudes longer than a non-MPC implementation, our approach still remains feasible and realistic.","5.3. Fair Training, Certification and Verification",[0],[0]
"We use the one time offline precomputation of multiplication triples described and timed in Mohassel & Zhang (2017,
Table 2).","5.3. Fair Training, Certification and Verification",[0],[0]
"As pointed out in Section 3, certification of a trained model requires checking whether F(θ) > 0.","5.3. Fair Training, Certification and Verification",[0],[0]
"We already perform this check at least once for each gradient
update during training.","5.3. Fair Training, Certification and Verification",[0],[0]
"It only takes a negligible fraction of the computation time, see Table 1.","5.3. Fair Training, Certification and Verification",[0],[0]
"Similarly, the operations required for certification stay well below one second.
","5.3. Fair Training, Certification and Verification",[0],[0]
Discussion.,"5.3. Fair Training, Certification and Verification",[0],[0]
"In this section, we have demonstrated the practicability of private and fair model training, certification and verification using MPC as described in Figure 1.","5.3. Fair Training, Certification and Verification",[0],[0]
"Using the methods and tricks introduced in Section 4, we can overcome accuracy as well as over- and underflow concerns due to fixed-point numbers.","5.3. Fair Training, Certification and Verification",[0],[0]
Offline precomputation combined with a fast C++ implementation yield viable running times for reasonably large datasets on a laptop computer.,"5.3. Fair Training, Certification and Verification",[0],[0]
"Real world fair learning has suffered from a dilemma: in order to enforce fairness, sensitive attributes must be examined; yet in many situations, users may feel uncomfortable in revealing these attributes, or modelers may be legally restricted in collecting and utilizing them.",6. Conclusion,[0],[0]
"By introducing recent methods from MPC, and extending them to handle linear constraints as required for various notions of fairness, we have demonstrated that it is practical on real-world datasets to: (i) certify and sign a model as fair; (ii) learn a fair model; and (iii) verify that a fair-certified model has indeed been used; all while maintaining cryptographic privacy of all users’ sensitive attributes.",6. Conclusion,[0],[0]
"Connecting concerns in privacy, algorithmic fairness and accountability, our proposal empowers regulators to provide better oversight, modelers to develop fair and private models, and users to retain control over data they consider highly sensitive.",6. Conclusion,[0],[0]
"The authors would like to thank Chris Russell and Phillipp Schoppmann for useful discussions and help with the implementation, as well as the anonymous reviewers for helpful comments.",Acknowledgments,[0],[0]
AG and MK were supported by The Alan Turing Institute under the EPSRC grant EP/N510129/1.,Acknowledgments,[0],[0]
MV was supported by EPSRC grant EP/M507970/1.,Acknowledgments,[0],[0]
"AW acknowledges support from the David MacKay Newton research fellowship at Darwin College, The Alan Turing Institute under EPSRC grant EP/N510129/1 & TU/B/000074, and the Leverhulme Trust via the CFI.",Acknowledgments,[0],[0]
Recent work has explored how to train machine learning models which do not discriminate against any subgroup of the population as determined by sensitive attributes such as gender or race.,abstractText,[0],[0]
"To avoid disparate treatment, sensitive attributes should not be considered.",abstractText,[0],[0]
"On the other hand, in order to avoid disparate impact, sensitive attributes must be examined—e.g., in order to learn a fair model, or to check if a given model is fair.",abstractText,[0],[0]
We introduce methods from secure multi-party computation which allow us to avoid both.,abstractText,[0],[0]
"By encrypting sensitive attributes, we show how an outcomebased fair model may be learned, checked, or have its outputs verified and held to account, without users revealing their sensitive attributes.",abstractText,[0],[0]
Blind Justice: Fairness with Encrypted Sensitive Attributes,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 1516–1527 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"A core step in statistical data-to-text generation concerns learning correspondences between structured data representations (e.g., facts in a database) and paired texts (Barzilay and Lapata, 2005; Kim and Mooney, 2010; Liang et al., 2009).",1 Introduction,[1.0],"['A core step in statistical data-to-text generation concerns learning correspondences between structured data representations (e.g., facts in a database) and paired texts (Barzilay and Lapata, 2005; Kim and Mooney, 2010; Liang et al., 2009).']"
"These correspondences describe how data representations are expressed in natural language (content realisation) but also indicate which subset of the data is verbalised in the text (content selection).
",1 Introduction,[0.9999999692480644],['These correspondences describe how data representations are expressed in natural language (content realisation) but also indicate which subset of the data is verbalised in the text (content selection).']
"Although content selection is traditionally performed by domain experts, recent advances in generation using neural networks (Bahdanau et al., 2015; Ranzato et al., 2016) have led to the use of large scale datasets containing loosely related data and text pairs.",1 Introduction,[1.0],"['Although content selection is traditionally performed by domain experts, recent advances in generation using neural networks (Bahdanau et al., 2015; Ranzato et al., 2016) have led to the use of large scale datasets containing loosely related data and text pairs.']"
"A prime example are online data sources like DBPedia (Auer et al., 2007) and Wikipedia and their associated texts which
1Our code and data are available at https://github.com/EdinburghNLP/wikigen.
are often independently edited.",1 Introduction,[0],[0]
Another example are sports databases and related textual resources.,1 Introduction,[0],[0]
"Wiseman et al. (2017) recently define a generation task relating statistics of basketball games with commentaries and a blog written by fans.
",1 Introduction,[0],[0]
"In this paper, we focus on short text generation from such loosely aligned data-text resources.",1 Introduction,[0],[0]
We work with the biographical subset of the DBPedia and Wikipedia resources where the data corresponds to DBPedia facts and texts are Wikipedia abstracts about people.,1 Introduction,[0],[0]
"Figure 1 shows an example for the film-maker Robert Flaherty, the Wikipedia infobox, and the corresponding abstract.",1 Introduction,[0],[0]
We wish to bootstrap a data-to-text generator that learns to verbalise properties about an entity from a loosely related example text.,1 Introduction,[0],[0]
"Given the set of properties in Figure (1a) and the related text in Figure (1b), we want to learn verbalisations for those properties that are mentioned in the text and produce a short description like the one in Figure (1c).
",1 Introduction,[0.9999999512374759],"['Given the set of properties in Figure (1a) and the related text in Figure (1b), we want to learn verbalisations for those properties that are mentioned in the text and produce a short description like the one in Figure (1c).']"
"In common with previous work (Mei et al., 2016; Lebret et al., 2016; Wiseman et al., 2017)",1 Introduction,[0],[0]
"our model draws on insights from neural machine translation (Bahdanau et al., 2015; Sutskever et al., 2014) using an encoder-decoder architecture as its backbone.",1 Introduction,[0],[0]
"Lebret et al. (2016) introduce the task of generating biographies from Wikipedia data, however they focus on single sentence generation.",1 Introduction,[0],[0]
"We generalize the task to multi-sentence text, and highlight the limitations of the standard attention mechanism which is often used as a proxy for content selection.",1 Introduction,[0],[0]
"When exposed to sub-sequences that do not correspond to any facts in the input, the soft attention mechanism will still try to justify the sequence and somehow distribute the attention weights over the input representation (Ghader and Monz, 2017).",1 Introduction,[1.0],"['When exposed to sub-sequences that do not correspond to any facts in the input, the soft attention mechanism will still try to justify the sequence and somehow distribute the attention weights over the input representation (Ghader and Monz, 2017).']"
"The decoder will still memorise high frequency sub-sequences in spite of these not being supported by any facts in the input.
",1 Introduction,[0],[0]
"We propose to alleviate these shortcom-
1516
ings via a specific content selection mechanism based on multi-instance learning (MIL; Keeler and Rumelhart, 1992) which automatically discovers correspondences, namely alignments, between data and text pairs.",1 Introduction,[0],[0]
These alignments are then used to modify the generation function during training.,1 Introduction,[0],[0]
"We experiment with two frameworks that allow to incorporate alignment information, namely multi-task learning (MTL; Caruana, 1993) and reinforcement learning (RL; Williams, 1992).",1 Introduction,[0],[0]
In both cases we define novel objective functions using the learnt alignments.,1 Introduction,[0],[0]
"Experimental results using automatic and human-based evaluation show that models trained with content-specific objectives improve upon vanilla encoder-decoder architectures which rely solely on soft attention.
",1 Introduction,[0],[0]
The remainder of this paper is organised as follows.,1 Introduction,[0],[0]
We discuss related work in Section 2 and describe the MIL-based content selection approach in Section 3.,1 Introduction,[0],[0]
We explain how the generator is trained in Section 4 and present evaluation experiments in Section 5.,1 Introduction,[0],[0]
Section 7 concludes the paper.,1 Introduction,[0],[0]
Previous attempts to exploit loosely aligned data and text corpora have mostly focused on extracting verbalisation spans for data units.,2 Related Work,[0],[0]
"Most approaches work in two stages: initially, data units are aligned with sentences from related corpora using some heuristics and subsequently extra content is discarded in order to retain only text spans verbalising the data.",2 Related Work,[0],[0]
"Belz and Kow (2010) obtain verbalisation spans using a measure of strength of association between data units and words, Walter et al. (2013) extract textual patterns from paths in dependency trees while Mrabet et al. (2016) rely on crowd-sourcing.",2 Related Work,[0],[0]
"Perez-Beltrachini and Gardent (2016) learn shared representations for data units and sentences reduced to subject-
predicate-object triples with the aim of extracting verbalisations for knowledge base properties.",2 Related Work,[0],[0]
"Our work takes a step further, we not only induce datato-text alignments but also learn generators that produce short texts verbalising a set of facts.
",2 Related Work,[0],[0]
Our work is closest to recent neural network models which learn generators from independently edited data and text resources.,2 Related Work,[0],[0]
"Most previous work (Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2017; Liu et al., 2017) targets the generation of single sentence biographies from Wikipedia infoboxes, while Wiseman et al. (2017) generate game summary documents from a database of basketball games where the input is always the same set of table fields.",2 Related Work,[0],[0]
"In contrast, in our scenario, the input data varies from one entity (e.g., athlete) to another (e.g., scientist) and properties might be present or not due to data incompleteness.",2 Related Work,[0],[0]
"Moreover, our generator is enhanced with a content selection mechanism based on multi-instance learning.",2 Related Work,[0],[0]
"MIL-based techniques have been previously applied to a variety of problems including image retrieval (Maron and Ratan, 1998; Zhang et al., 2002), object detection (Carbonetto et al., 2008; Cour et al., 2011), text classification (Andrews and Hofmann, 2004), image captioning (Wu et al., 2015; Karpathy and Fei-Fei, 2015), paraphrase detection (Xu et al., 2014), and information extraction (Hoffmann et al., 2011).",2 Related Work,[0],[0]
"The application of MIL to content selection is novel to our knowledge.
",2 Related Work,[0],[0]
We show how to incorporate content selection into encoder-decoder architectures following training regimes based on multi-task learning and reinforcement learning.,2 Related Work,[0],[0]
Multi-task learning aims to improve a main task by incorporating joint learning of one or more related auxiliary tasks.,2 Related Work,[0],[0]
"It has been applied with success to a variety of sequence-prediction tasks focus-
ing mostly on morphosyntax.",2 Related Work,[0],[0]
"Examples include chunking, tagging (Collobert et al., 2011; Søgaard and Goldberg, 2016; Bjerva et al., 2016; Plank, 2016), name error detection (Cheng et al., 2015), and machine translation (Luong et al., 2016).",2 Related Work,[0],[0]
"Reinforcement learning (Williams, 1992) has also seen popularity as a means of training neural networks to directly optimize a taskspecific metric (Ranzato et al., 2016) or to inject task-specific knowledge (Zhang and Lapata, 2017).",2 Related Work,[0],[0]
We are not aware of any work that compares the two training methods directly.,2 Related Work,[0],[0]
"Furthermore, our reinforcement learning-based algorithm differs from previous text generation approaches (Ranzato et al., 2016; Zhang and Lapata, 2017) in that it is applied to documents rather than individual sentences.",2 Related Work,[0],[0]
"We consider loosely coupled data and text pairs where the data component is a set P of propertyvalues {p1 : v1, · · · , p|P | : v|P |} and the related text T is a sequence of sentences (s1, · · · ,s|T |).",3 Bidirectional Content Selection,[0],[0]
We define a mention span τ as a (possibly discontinuous) subsequence of T containing one or several words that verbalise one or more property-value from P .,3 Bidirectional Content Selection,[0],[0]
"For instance, in Figure 1, the mention span “married to Frances H. Flaherty” verbalises the property-value {Spouse(s) :",3 Bidirectional Content Selection,[0],[0]
"Frances Johnson Hubbard}.
",3 Bidirectional Content Selection,[0],[0]
"In traditional supervised data to text generation tasks, data units (e.g., pi : vi in our particular setting) are either covered by some mention span τ j or do not have any mention span at all in T .",3 Bidirectional Content Selection,[0],[0]
The latter is a case of content selection where the generator will learn which properties to ignore when generating text from such data.,3 Bidirectional Content Selection,[0],[0]
"In this work, we consider text components which are independently edited, and will unavoidably contain unaligned spans, i.e., text segments which do not correspond to any property-value in P .",3 Bidirectional Content Selection,[0],[0]
The phrase “from 1914” in the text in Figure (1b) is such an example.,3 Bidirectional Content Selection,[0],[0]
"Similarly, the last sentence, talks about Frances’ awards and nominations and this information is not supported by the properties either.
",3 Bidirectional Content Selection,[0],[0]
Our model checks content in both directions; it identifies which properties have a corresponding text span (data selection) and also foregrounds (un)aligned text spans (text selection).,3 Bidirectional Content Selection,[1.0],['Our model checks content in both directions; it identifies which properties have a corresponding text span (data selection) and also foregrounds (un)aligned text spans (text selection).']
"This knowledge is then used to discourage the generator from producing text not supported by facts in the prop-
erty set P .",3 Bidirectional Content Selection,[0],[0]
"We view a property set P and its loosely coupled text T as a coarse level, imperfect alignment.",3 Bidirectional Content Selection,[1.0],"['We view a property set P and its loosely coupled text T as a coarse level, imperfect alignment.']"
"From this alignment signal, we want to discover a set of finer grained alignments indicating which mention spans in T align to which properties in P .",3 Bidirectional Content Selection,[0],[0]
"For each pair (P ,T ), we learn an alignment set A(P ,T ) which contains property-value word pairs.",3 Bidirectional Content Selection,[0],[0]
"For example, for the properties spouse and died in Figure 1, we would like to derive the alignments in Table 1.
",3 Bidirectional Content Selection,[0],[0]
"We formulate the task of discovering finergrained word alignments as a multi-instance learning problem (Keeler and Rumelhart, 1992).",3 Bidirectional Content Selection,[0],[0]
We assume that words from the text are positive labels for some property-values but we do not know which ones.,3 Bidirectional Content Selection,[0],[0]
"For each data-text pair (P ,T ), we derive |T | pairs of the form (P ,s) where |T | is the number of sentences in T .",3 Bidirectional Content Selection,[0],[0]
We encode property sets P and sentences s into a common multimodal h-dimensional embedding space.,3 Bidirectional Content Selection,[1.0],['We encode property sets P and sentences s into a common multimodal h-dimensional embedding space.']
"While doing this, we discover finer grained alignments between words and property-values.",3 Bidirectional Content Selection,[0],[0]
"The intuition is that by learning a high similarity score for a property set P and sentence pair s, we will also learn the contribution of individual elements (i.e., words and property-values) to the overall similarity score.",3 Bidirectional Content Selection,[0],[0]
We will then use this individual contribution as a measure of word and property-value alignment.,3 Bidirectional Content Selection,[0],[0]
"More concretely, we assume the pair is aligned (or unaligned) if this individual score is above (or below) a given threshold.",3 Bidirectional Content Selection,[0],[0]
"Across examples like the one shown in Figure (1a-b), we expect the model to learn an alignment between the text span
“married to Frances H. Flaherty” and the propertyvalue {spouse : Frances Johnson Hubbard}.
",3 Bidirectional Content Selection,[0],[0]
"In what follows we describe how we encode (P ,s) pairs and define the similarity function.
",3 Bidirectional Content Selection,[0],[0]
"Property Set Encoder As there is no fixed order among the property-value pairs p : v in P , we individually encode each one of them.",3 Bidirectional Content Selection,[0],[0]
"Furthermore, both properties p and values v may consist of short phrases.",3 Bidirectional Content Selection,[0],[0]
"For instance, the property cause o f death and value cerebral thrombosis in Figure 1.",3 Bidirectional Content Selection,[0],[0]
"We
therefore consider property-value pairs as concatenated sequences pv and use a bidirectional Long Short-Term Memory Network (LSTM; Hochreiter and Schmidhuber, 1997) network for their encoding.",3 Bidirectional Content Selection,[0],[0]
Note that the same network is used for all pairs.,3 Bidirectional Content Selection,[0],[0]
"Each property-value pair is encoded into a vector representation:
pi = biLSTMdenc(pvi) (1)
which is the output of the recurrent network at the final time step.",3 Bidirectional Content Selection,[0],[0]
"We use addition to combine the forward and backward outputs and generate encoding {p1, · · · ,p|P |} for P .",3 Bidirectional Content Selection,[0],[0]
"Sentence Encoder We also use a biLSTM to obtain a representation for the sentence s = w1, · · · ,w|s|.",3 Bidirectional Content Selection,[0],[0]
"Each word wt is represented by the output of the forward and backward networks at time step t. A word at position t is represented by the concatenation of the forward and backward outputs of the networks at time step t :
wt = biLSTMsenc(wt)",3 Bidirectional Content Selection,[0],[0]
"(2)
and each sentence is encoded as a sequence of vectors (w1, · · · ,w|s|).",3 Bidirectional Content Selection,[0],[0]
Alignment Objective,3 Bidirectional Content Selection,[0],[0]
"Our learning objective seeks to maximise the similarity score between property set P and a sentence s (Karpathy and Fei-Fei, 2015).",3 Bidirectional Content Selection,[0.9981028252152279],"['Alignment Objective Our learning objective seeks to maximise the similarity score between property set P and a sentence s (Karpathy and Fei-Fei, 2015).']"
This similarity score is in turn defined on top of the similarity scores among property-values in P and words in s. Equation (3) defines this similarity function using the dot product.,3 Bidirectional Content Selection,[0],[0]
"The function seeks to align each word to the best scoring property-value:
SP s = |s| ∑ t=1",3 Bidirectional Content Selection,[0],[0]
"maxi∈{1,...,|P |} pi • wt (3)
",3 Bidirectional Content Selection,[0],[0]
Equation (4) defines our objective which encourages related properties P and sentences s to have higher similarity than other P ′ 6=,3 Bidirectional Content Selection,[0],[0]
"P and s′ 6= s:
LCA = max(0,SP s −SP s ′ +1) +max(0,SP s −SP ′s +1)
(4)",3 Bidirectional Content Selection,[0],[0]
In this section we describe the base generation architecture and explain two alternative ways of using the alignments to guide the training of the model.,4 Generator Training,[0],[0]
"One approach follows multi-task training where the generator learns to output a sequence of words but also to predict alignment labels for
each word.",4 Generator Training,[0],[0]
The second approach relies on reinforcement learning for adjusting the probability distribution of word sequences learnt by a standard word prediction training algorithm.,4 Generator Training,[0],[0]
"We follow a standard attention based encoderdecoder architecture for our generator (Bahdanau et al., 2015; Luong et al., 2015).",4.1 Encoder-Decoder Base Generator,[0],[0]
"Given a set of properties X as input, the model learns to predict an output word sequence Y which is a verbalisation of (part of) the input.",4.1 Encoder-Decoder Base Generator,[0],[0]
"More precisely, the generation of sequence Y is conditioned on input X :
P(Y |X) = |Y | ∏ t=1",4.1 Encoder-Decoder Base Generator,[0],[0]
"P(yt |y1:t−1,X) (5)
",4.1 Encoder-Decoder Base Generator,[0],[0]
The encoder module constitutes an intermediate representation of the input.,4.1 Encoder-Decoder Base Generator,[0],[0]
"For this, we use the property-set encoder described in Section 3 which outputs vector representations {p1, · · · ,p|X |} for a set of property-value pairs.",4.1 Encoder-Decoder Base Generator,[0],[0]
"The decoder uses an LSTM and a soft attention mechanism (Luong et al., 2015) to generate one word yt at a time conditioned on the previous output words and a context vector ct dynamically created: P(yt+1|y1:t ,X) =",4.1 Encoder-Decoder Base Generator,[0],[0]
"so f tmax(g(ht ,ct))",4.1 Encoder-Decoder Base Generator,[0],[0]
"(6) where g(·) is a neural network with one hidden layer parametrised by Wo ∈ R|V |×d , |V",4.1 Encoder-Decoder Base Generator,[0],[0]
"| is the output vocabulary size and d the hidden unit dimension, over ht and ct composed as follows:
g(ht ,ct) =",4.1 Encoder-Decoder Base Generator,[0],[0]
"Wo tanh(Wc[ct ;ht ]) (7)
where Wc ∈ Rd×2d .",4.1 Encoder-Decoder Base Generator,[0],[0]
ht is the hidden state of the LSTM decoder which summarises y1:,4.1 Encoder-Decoder Base Generator,[0],[0]
"t :
ht = LSTM(yt ,ht−1) (8)
The dynamic context vector ct is the weighted sum of the hidden states of the input property set (Equation (9)); and the weights αti are determined by a dot product attention mechanism:
ct = |X",4.1 Encoder-Decoder Base Generator,[0],[0]
| ∑ i=1,4.1 Encoder-Decoder Base Generator,[0],[0]
"αti pi (9)
αti = exp(ht • pi)
∑i ′",4.1 Encoder-Decoder Base Generator,[0],[0]
"exp(ht • pi ′) (10)
We initialise the decoder with the averaged sum of the encoded input representations (Vinyals et al., 2016).",4.1 Encoder-Decoder Base Generator,[0],[0]
"The model is trained to optimize negative log likelihood:
LwNLL =− |Y | ∑ t=1",4.1 Encoder-Decoder Base Generator,[0],[0]
"logP(yt |y1:t−1,X) (11)
We extend this architecture to multi-sentence texts in a way similar to Wiseman et al. (2017).",4.1 Encoder-Decoder Base Generator,[0],[0]
"We view the abstract as a single sequence, i.e., all sentences are concatenated.",4.1 Encoder-Decoder Base Generator,[0],[0]
"When training, we cut the abstracts in blocks of equal size and perform forward backward iterations for each block (this includes the back-propagation through the encoder).",4.1 Encoder-Decoder Base Generator,[0],[0]
"From one block iteration to the next, we initialise the decoder with the last state of the previous block.",4.1 Encoder-Decoder Base Generator,[0],[0]
The block size is a hyperparameter tuned experimentally on the development set.,4.1 Encoder-Decoder Base Generator,[0],[0]
The generation of the output sequence is conditioned on the previous words and the input.,4.2 Predicting Alignment Labels,[0],[0]
"However, when certain sequences are very common, the language modelling conditional probability will prevail over the input conditioning.",4.2 Predicting Alignment Labels,[0],[0]
"For instance, the phrase from 1914 in our running example is very common in contexts that talk about periods of marriage or club membership, and as a result, the language model will output this phrase often, even in cases where there are no supporting facts in the input.",4.2 Predicting Alignment Labels,[0],[0]
"The intuition behind multi-task training (Caruana, 1993) is that it will smooth the probabilities of frequent sequences when trying to simultaneously predict alignment labels.
",4.2 Predicting Alignment Labels,[0],[0]
"Using the set of alignments obtained by our content selection model, we associate each word in the training data with a binary label at indicating whether it aligns with some property in the input set.",4.2 Predicting Alignment Labels,[1.0],"['Using the set of alignments obtained by our content selection model, we associate each word in the training data with a binary label at indicating whether it aligns with some property in the input set.']"
"Our auxiliary task is to predict at given the sequence of previously predicted words and input X :
P(at+1|y1:t ,X) =",4.2 Predicting Alignment Labels,[0],[0]
"sigmoid(g′(ht ,ct))",4.2 Predicting Alignment Labels,[0],[0]
"(12)
g′(ht ,ct) = va • tanh(Wc[ct ;ht ]) (13)
where va ∈ Rd and the other operands are as defined in Equation (7).",4.2 Predicting Alignment Labels,[0],[0]
"We optimise the following auxiliary objective function:
Laln =− |Y | ∑ t=1 logP(at |y1:t−1,X) (14)
and the combined multi-task objective is the weighted sum of both word prediction and alignment prediction losses:
LMT L = λLwNLL +(1−λ)Laln (15)
where λ controls how much model training will focus on each task.",4.2 Predicting Alignment Labels,[0.9915710795604372],"['We optimise the following auxiliary objective function: Laln =− |Y | ∑ t=1 logP(at |y1:t−1,X) (14) and the combined multi-task objective is the weighted sum of both word prediction and alignment prediction losses: LMTL = λLwNLL+(1−λ)Laln (15) where λ controls how much model training will focus on each task.']"
"As we will explain in Section 5, we can anneal this value during training in favour of one objective or the other.",4.2 Predicting Alignment Labels,[0],[0]
"Although the multi-task approach aims to smooth the target distribution, the training process is still driven by the imperfect target text.",4.3 Reinforcement Learning Training,[0],[0]
"In other words, at each time step t the algorithm feeds the previous word wt−1 of the target text and evaluates the prediction against the target wt .
",4.3 Reinforcement Learning Training,[0],[0]
"Alternatively, we propose a training approach based on reinforcement learning (Williams 1992) which allows us to define an objective function that does not fully rely on the target text but rather on a revised version of it.",4.3 Reinforcement Learning Training,[0],[0]
"In our case, the set of alignments obtained by our content selection model provides a revision for the target text.",4.3 Reinforcement Learning Training,[0],[0]
"The advantages of reinforcement learning are twofold: (a) it allows to exploit additional task-specific knowledge (Zhang and Lapata, 2017) during training, and (b) enables the exploration of other word sequences through sampling.",4.3 Reinforcement Learning Training,[1.0],"['The advantages of reinforcement learning are twofold: (a) it allows to exploit additional task-specific knowledge (Zhang and Lapata, 2017) during training, and (b) enables the exploration of other word sequences through sampling.']"
"Our setting differs from previous applications of RL (Ranzato et al., 2016; Zhang and Lapata, 2017) in that the reward function is not computed on the target text but rather on its alignments with the input.
",4.3 Reinforcement Learning Training,[0],[0]
The encoder-decoder model is viewed as an agent whose action space is defined by the set of words in the target vocabulary.,4.3 Reinforcement Learning Training,[0],[0]
"At each time step, the encoder-decoder takes action ŷt with policy Pπ(ŷt |ŷ1:t−1,X) defined by the probability in Equation (6).",4.3 Reinforcement Learning Training,[0],[0]
"The agent terminates when it emits the End Of Sequence (EOS) token, at which point the sequence of all actions taken yields the output sequence Ŷ = (ŷ1, · · · , ŷ|Ŷ |).",4.3 Reinforcement Learning Training,[0],[0]
This sequence in our task is a short text describing the properties of a given entity.,4.3 Reinforcement Learning Training,[0],[0]
"After producing the sequence of actions Ŷ , the agent receives a reward r(Ŷ ) and the policy is updated according to this reward.
",4.3 Reinforcement Learning Training,[0],[0]
Reward Function,4.3 Reinforcement Learning Training,[0],[0]
"We define the reward function r(Ŷ ) on the alignment set A(X ,Y ).",4.3 Reinforcement Learning Training,[0],[0]
"If the output action sequence Ŷ is precise with respect to the set of alignments A(X ,Y ), the agent will receive a high reward.",4.3 Reinforcement Learning Training,[0],[0]
"Concretely, we define r(Ŷ ) as follows:
r(Ŷ ) = γpr rpr(Ŷ )",4.3 Reinforcement Learning Training,[0],[0]
"(16)
where γpr adjusts the reward value rpr which is the unigram precision of the predicted sequence Ŷ and the set of words in A(X ,Y ).
",4.3 Reinforcement Learning Training,[0],[0]
Training Algorithm,4.3 Reinforcement Learning Training,[0],[0]
"We use the REINFORCE algorithm (Williams, 1992) to learn an agent that maximises the reward function.",4.3 Reinforcement Learning Training,[0],[0]
"As this is a gradient descent method, the training loss of a sequence
is defined as the negative expected reward:
LRL =−E(ŷ1,··· ,ŷ|Ŷ |) ∼ Pπ(·|X)[r(ŷ1, · · · , ŷ|Ŷ |)]
where Pπ is the agent’s policy, i.e., the word distribution produced by the encoder-decoder model (Equation (6)) and r(·) is the reward function as defined in Equation (16).",4.3 Reinforcement Learning Training,[0],[0]
"The gradient of LRL is given by:
∇LRL ≈ |Ŷ | ∑ t=1 ∇ logPπ(ŷt",4.3 Reinforcement Learning Training,[0],[0]
"|ŷ1:t−1,X)[r(ŷ1:|Ŷ |)−bt ]
where bt is a baseline linear regression model used to reduce the variance of the gradients during training.",4.3 Reinforcement Learning Training,[0],[0]
bt predicts the future reward and is trained by minimizing mean squared error.,4.3 Reinforcement Learning Training,[0],[0]
"The input to this predictor is the agent hidden state ht , however we do not back-propagate the error to ht .",4.3 Reinforcement Learning Training,[0],[0]
"We refer the interested reader to Williams (1992) and Ranzato et al. (2016) for more details.
",4.3 Reinforcement Learning Training,[0],[0]
"Document Level Curriculum Learning Rather than starting from a state given by a random policy, we initialise the agent with a policy learnt by pretraining with the negative log-likelihood objective (Ranzato et al., 2016; Zhang and Lapata, 2017).",4.3 Reinforcement Learning Training,[0],[0]
The reinforcement learning objective is applied gradually in combination with the log-likelihood objective on each target block subsequence.,4.3 Reinforcement Learning Training,[0],[0]
Recall from Section 4.1 that our document is segmented into blocks of equal size during training which we denote as MAXBLOCK.,4.3 Reinforcement Learning Training,[0],[0]
"When training begins, only the last ℧ tokens are predicted by the agent while for the first (MAXBLOCK −℧) we still use the negative log-likelihood objective.",4.3 Reinforcement Learning Training,[0],[0]
The number of tokens ℧ predicted by the agent is incremented by ℧ units every 2 epochs.,4.3 Reinforcement Learning Training,[0],[0]
We set ℧ = 3 and the training ends when (MAXBLOCK−℧) = 0.,4.3 Reinforcement Learning Training,[0],[0]
"Since we evaluate the model’s predictions at the block level, the reward function is also evaluated at the block level.",4.3 Reinforcement Learning Training,[0],[0]
"Data We evaluated our model on a dataset collated from WIKIBIO (Lebret et al., 2016), a corpus of 728,321 biography articles (their first paragraph) and their infoboxes sampled from the English Wikipedia.",5 Experimental Setup,[0],[0]
We adapted the original dataset in three ways.,5 Experimental Setup,[0],[0]
"Firstly, we make use of the entire abstract rather than first sentence.",5 Experimental Setup,[0],[0]
"Secondly, we reduced the dataset to examples with a rich set of properties and multi-sentential text.",5 Experimental Setup,[0],[0]
"We eliminated examples with less than six property-value
pairs and abstracts consisting of one sentence.",5 Experimental Setup,[0],[0]
We also placed a minimum restriction of 23 words in the length of the abstract.,5 Experimental Setup,[0],[0]
We considered abstracts up to a maximum of 12 sentences and property sets with a maximum of 50 property-value pairs.,5 Experimental Setup,[1.0],['We considered abstracts up to a maximum of 12 sentences and property sets with a maximum of 50 property-value pairs.']
"Finally, we associated each abstract with the set of DBPedia properties p : v corresponding to the abstract’s main entity.",5 Experimental Setup,[1.0],"['Finally, we associated each abstract with the set of DBPedia properties p : v corresponding to the abstract’s main entity.']"
"As entity classification is available in DBPedia for most entities, we concatenate class information c (whenever available) with the property value, i.e., p : vc.",5 Experimental Setup,[0],[0]
"In Figure 1, the property value spouse : FrancesH.Flaherty is extended with class information from the DBPedia ontology to spouse : FrancesH.FlahertyPerson.
",5 Experimental Setup,[0],[0]
Pre-processing Numeric date formats were converted to a surface form with month names.,5 Experimental Setup,[0],[0]
Numerical expressions were delexicalised using different tokens created with the property name and position of the delexicalised token on the value sequence.,5 Experimental Setup,[0],[0]
"For instance, given the property-value for birth date in Figure (1a), the first sentence in the abstract (Figure (1b)) becomes “ Robert Joseph Flaherty, (February DLX birth date 2, DLX birth date 4 – July . . .",5 Experimental Setup,[0],[0]
”.,5 Experimental Setup,[0],[0]
Years and numbers in the text not found in the values of the property set were replaced with tokens YEAR and NUMERIC.2,5 Experimental Setup,[0],[0]
"In a second phase, when creating the input and output vocabularies, V I and V O respectively, we delexicalised words w which were absent from the output vocabulary but were attested in the input vocabulary.",5 Experimental Setup,[0],[0]
"Again, we created tokens based on the property name and the position of the word in the value sequence.",5 Experimental Setup,[0],[0]
Words not in V O or V I were replaced with the symbol UNK.,5 Experimental Setup,[0],[0]
Vocabulary sizes were limited to |V,5 Experimental Setup,[0],[0]
I | = 50k and |V O| = 50k for the alignment model and |V O| = 20k for the generator.,5 Experimental Setup,[0],[0]
We discarded examples where the text contained more than three UNKs (for the content aligner) and five UNKs (for the generator); or more than two UNKs in the property-value (for generation).,5 Experimental Setup,[0],[0]
"Finally, we added the empty relation to the property sets.
",5 Experimental Setup,[0],[0]
Table 2 summarises the dataset statistics for the generator.,5 Experimental Setup,[0],[0]
"We report the number of abstracts in the dataset (size), the average number of sentences and tokens in the abstracts, and the average number of properties and sentence length in tokens
2We exploit these tokens to further adjust the score of the reward function given by Equation (16).",5 Experimental Setup,[0],[0]
"Each time the predicted output contains some of these symbols we decrease the reward score by κ which we empirically set to 0.025 .
",5 Experimental Setup,[0],[0]
(sent.len).,5 Experimental Setup,[0],[0]
"For the content aligner (cf. Section 3), each sentence constitutes a training instance, and as a result the sizes of the train and development sets are 796,446 and 153,096, respectively.
",5 Experimental Setup,[0],[0]
Training Configuration,5 Experimental Setup,[0],[0]
We adjusted all models’ hyperparameters according to their performance on the development set.,5 Experimental Setup,[0],[0]
"The encoders for both content selection and generation models were initialised with GloVe (Pennington et al., 2014) pre-trained vectors.",5 Experimental Setup,[0],[0]
The input and hidden unit dimension was set to 200 for content selection and 100 for generation.,5 Experimental Setup,[0],[0]
"In all models, we used encoder biLSTMs and decoder LSTM (regularised with a dropout rate of 0.3 (Zaremba et al., 2014)) with one layer.",5 Experimental Setup,[0],[0]
"Content selection and generation models (base encoder-decoder and MTL) were trained for 20 epochs with the ADAM optimiser (Kingma and Ba, 2014) using a learning rate of 0.001.",5 Experimental Setup,[0],[0]
The reinforcement learning model was initialised with the base encoder-decoder model and trained for 35 additional epochs with stochastic gradient descent and a fixed learning rate of 0.001.,5 Experimental Setup,[0],[0]
"Block sizes were set to 40 (base), 60 (MTL) and 50 (RL).",5 Experimental Setup,[0],[0]
"Weights for the MTL objective were also tuned experimentally; we set λ = 0.1 for the first four epochs (training focuses on alignment prediction) and switched to λ = 0.9 for the remaining epochs.
",5 Experimental Setup,[0],[0]
Content Alignment We optimized content alignment on the development set against manual alignments.,5 Experimental Setup,[0],[0]
"Specifically, two annotators aligned 132 sentences to their infoboxes.",5 Experimental Setup,[0],[0]
"We used the Yawat annotation tool (Germann, 2008) and followed the alignment guidelines (and evaluation metrics) used in Cohn et al. (2008).",5 Experimental Setup,[0],[0]
"The inter-annotator agreement using macro-averaged f-score was 0.72 (we treated one annotator as the reference and the other one as hypothetical system output).
",5 Experimental Setup,[0],[0]
"Alignment sets were extracted from the model’s output (cf. Section 3) by optimizing the threshold avg(sim) + a ∗ std(sim) where sim denotes the similarity between the set of property values and words, and a is empirically set to 0.75; avg
and std are the mean and standard deviation of sim scores across the development set.",5 Experimental Setup,[0],[0]
Each word was aligned to a property-value if their similarity exceeded a threshold of 0.22.,5 Experimental Setup,[0],[0]
"Our best content alignment model (Content-Aligner) obtained an fscore of 0.36 on the development set.
",5 Experimental Setup,[0],[0]
We also compared our Content-Aligner against a baseline based on pre-trained word embeddings (EmbeddingsBL).,5 Experimental Setup,[0],[0]
"For each pair (P ,s) we computed the dot product between words in s and properties in P (properties were represented by the the averaged sum of their words’ vectors).",5 Experimental Setup,[0],[0]
Words were aligned to property-values if their similarity exceeded a threshold of 0.4.,5 Experimental Setup,[0],[0]
EmbeddingsBL,5 Experimental Setup,[0],[0]
obtained an f-score of 0.057 against the manual alignments.,5 Experimental Setup,[0],[0]
"Finally, we compared the performance of the Content-Aligner at the level of property set P and sentence s similarity by comparing the average ranking position of correct pairs among 14 distractors, namely rank@15.",5 Experimental Setup,[1.0],"['Finally, we compared the performance of the Content-Aligner at the level of property set P and sentence s similarity by comparing the average ranking position of correct pairs among 14 distractors, namely rank@15.']"
"The Content-Aligner obtained a rank of 1.31, while the EmbeddingsBL model had a rank of 7.99 (lower is better).",5 Experimental Setup,[0],[0]
"We compared the performance of an encoderdecoder model trained with the standard negative log-likelihood method (ED), against a model trained with multi-task learning (EDMTL) and reinforcement learning (EDRL).",6 Results,[0],[0]
"We also included a template baseline system (Templ) in our evaluation experiments.
",6 Results,[0.999999928855333],['We also included a template baseline system (Templ) in our evaluation experiments.']
The template generator used hand-written rules to realise property-value pairs.,6 Results,[0],[0]
"As an approximation for content selection, we obtained the 50 more frequent property names from the training set and manually defined content ordering rules with the following criteria.",6 Results,[0],[0]
"We ordered personal life properties (e.g., birth date or occupation) based on their most common order of mention in the Wikipedia abstracts.",6 Results,[0],[0]
"Profession dependent properties (e.g., position or genre), were assigned an equal ordering but posterior to the personal properties.",6 Results,[0],[0]
We manually lexicalised properties into single sentence templates to be concatenated to produce the final text.,6 Results,[1.0],['We manually lexicalised properties into single sentence templates to be concatenated to produce the final text.']
The template for the property position and example verbalisation for the property-value position : de f ender of the entity zanetti are “[NAME] played as [POSITION].”,6 Results,[0],[0]
and “ Zanetti played as defender.”,6 Results,[0],[0]
"respectively.
",6 Results,[0],[0]
"Automatic Evaluation Table 3 shows the results of automatic evaluation using BLEU-4
(Papineni et al., 2002) against the noisy Wikipedia abstracts.",6 Results,[0],[0]
"Considering these as a gold standard is, however, not entirely satisfactory for two reasons.",6 Results,[0],[0]
"Firstly, our models generate considerably shorter text and will be penalized for not generating text they were not supposed to generate in the first place.",6 Results,[1.0],"['Firstly, our models generate considerably shorter text and will be penalized for not generating text they were not supposed to generate in the first place.']"
"Secondly, the model might try to reproduce what is in the imperfect reference but not supported by the input properties and as a result will be rewarded when it should not.",6 Results,[1.0],"['Secondly, the model might try to reproduce what is in the imperfect reference but not supported by the input properties and as a result will be rewarded when it should not.']"
"To alleviate this, we crowd-sourced using AMT a revised version of 200 randomly selected abstracts from the test set.
",6 Results,[1.0000000703784357],"['To alleviate this, we crowd-sourced using AMT a revised version of 200 randomly selected abstracts from the test set.']"
Crowdworkers were shown a Wikipedia infobox with the accompanying abstract and were asked to adjust the text to the content present in the infobox.,6 Results,[0],[0]
Annotators were instructed to delete spans which did not have supporting facts and rewrite the remaining parts into a well-formed text.,6 Results,[0],[0]
We collected three revised versions for each abstract.,6 Results,[0],[0]
"Inter-annotator agreement was 81.64 measured as the mean pairwise BLEU-4 amongst AMT workers.
",6 Results,[0],[0]
Automatic evaluation results against the revised abstracts are also shown in Table 3.,6 Results,[0],[0]
"As can be seen, all encoder-decoder based models have a significant advantage over Templ when evaluating against both types of abstracts.",6 Results,[0],[0]
The model enabled with the multi-task learning content selection mechanism brings an improvement of 1.29 BLEU-4 over a vanilla encoder-decoder model.,6 Results,[1.0],['The model enabled with the multi-task learning content selection mechanism brings an improvement of 1.29 BLEU-4 over a vanilla encoder-decoder model.']
Performance of the RL trained model is inferior and close to the ED model.,6 Results,[0],[0]
"We discuss the reasons for this discrepancy shortly.
",6 Results,[0.9999999548335465],['We discuss the reasons for this discrepancy shortly.']
"To provide a rough comparison with the results reported in Lebret et al. (2016), we also computed BLEU-4 on the first sentence of the text generated by our system.3 Recall that their model generates the first sentence of the abstract, whereas we out-
3We post-processed system output with Stanford CoreNLP",6 Results,[0],[0]
"(Manning et al., 2014) to extract the first sentence.
put multi-sentence text.",6 Results,[0],[0]
"Using the first sentence in the Wikipedia abstract as reference, we obtained a score of 37.29% (ED), 38.42% (EDMTL) and 38.1% (EDRL) which compare favourably with their best performing model (34.7%±0.36).
",6 Results,[0],[0]
Human-Based Evaluation,6 Results,[0],[0]
We further examined differences among systems in a human-based evaluation study.,6 Results,[0],[0]
"Using AMT, we elicited 3 judgements for the same 200 infobox-abstract pairs we used in the abstract revision study.",6 Results,[0],[0]
"We compared the output of the templates, the three neural generators and also included one of the human edited abstracts as a gold standard (reference).",6 Results,[0],[0]
"For each test case, we showed crowdworkers the Wikipedia infobox and five short texts in random order.",6 Results,[0],[0]
The annotators were asked to rank each of the texts according to the following criteria: (1) Is the text faithful to the content of the table?,6 Results,[1.0],['The annotators were asked to rank each of the texts according to the following criteria: (1) Is the text faithful to the content of the table?']
and (2) Is the text overall comprehensible and fluent?,6 Results,[0],[0]
Ties were allowed only when texts were identical strings.,6 Results,[0],[0]
"Table 5 presents examples of the texts (and properties) crowdworkers saw.
",6 Results,[0],[0]
"Table 4 shows, proportionally, how often crowdworkers ranked each system, first, second, and so on.",6 Results,[0],[0]
"Unsurprisingly, the human authored gold text is considered best (and ranked first 47% of the time).",6 Results,[0],[0]
"EDMTL is mostly ranked second and third best, followed closely by EDRL.",6 Results,[0],[0]
The vanilla encoder-decoder system ED is mostly forth and Templ is fifth.,6 Results,[0],[0]
"As shown in the last column of the table (Rank), the ranking of EDMTL is overall slightly better than EDRL.",6 Results,[0],[0]
We further converted the ranks to ratings on a scale of 1 to 5 (assigning ratings 5. .,6 Results,[0],[0]
.1,6 Results,[0],[0]
to rank placements 1. .,6 Results,[0],[0]
.5),6 Results,[0],[0]
.,6 Results,[0],[0]
This allowed us to perform Analysis of Variance (ANOVA) which revealed a reliable effect of system type.,6 Results,[0],[0]
Post-hoc Tukey tests showed that all systems were significantly worse than RevAbs and significantly better than Templ (p < 0.05).,6 Results,[0],[0]
"EDMTL is not significantly better than EDRL but is significantly (p < 0.05) different from ED.
",6 Results,[0],[0]
Discussion,6 Results,[0],[0]
"The texts generated by EDRL are shorter compared to the other two neural systems
which might affect BLEU-4 scores and also the ratings provided by the annotators.",6 Results,[0],[0]
"As shown in Table 5 (entity dorsey burnette), EDRL drops information pertaining to dates or chooses to just verbalise birth place information.",6 Results,[0],[0]
"In some cases, this is preferable to hallucinating incorrect facts; however, in other cases outputs with more information are rated more favourably.",6 Results,[0],[0]
"Overall, EDMTL seems to be more detail oriented and faithful to the facts included in the infobox (see dorsey burnette, aaron moores, or kirill moryganov).",6 Results,[1.0],"['Overall, EDMTL seems to be more detail oriented and faithful to the facts included in the infobox (see dorsey burnette, aaron moores, or kirill moryganov).']"
"The template system manages in some specific configurations to verbalise appropriate facts (indrani bose), however, it often fails to verbalise infrequent properties (aaron moores) or focuses on properties which are very frequent in the knowledge base but are rarely found in the abstracts (kirill moryganov).",6 Results,[0],[0]
"In this paper we focused on the task of bootstrapping generators from large-scale datasets consisting of DBPedia facts and related Wikipedia biog-
raphy abstracts.",7 Conclusions,[0],[0]
"We proposed to equip standard encoder-decoder models with an additional content selection mechanism based on multi-instance learning and developed two training regimes, one based on multi-task learning and the other on reinforcement learning.",7 Conclusions,[0],[0]
"Overall, we find that the proposed content selection mechanism improves the accuracy and fluency of the generated texts.",7 Conclusions,[1.0],"['Overall, we find that the proposed content selection mechanism improves the accuracy and fluency of the generated texts.']"
"In the future, it would be interesting to investigate a more sophisticated representation of the input (Vinyals et al., 2016).",7 Conclusions,[1.0],"['In the future, it would be interesting to investigate a more sophisticated representation of the input (Vinyals et al., 2016).']"
"It would also make sense for the model to decode hierarchically, taking sequences of words and sentences into account (Zhang and Lapata, 2014; Lebret et al., 2015).",7 Conclusions,[1.0],"['It would also make sense for the model to decode hierarchically, taking sequences of words and sentences into account (Zhang and Lapata, 2014; Lebret et al., 2015).']"
We thank the NAACL reviewers for their constructive feedback.,Acknowledgments,[0],[0]
"We also thank Xingxing Zhang, Li Dong and Stefanos Angelidis for useful discussions about implementation details.",Acknowledgments,[0],[0]
We gratefully acknowledge the financial support of the European Research Council (award number 681760).,Acknowledgments,[0],[0]
"A core step in statistical data-to-text generation concerns learning correspondences between structured data representations (e.g., facts in a database) and associated texts.",abstractText,[0],[0]
"In this paper we aim to bootstrap generators from large scale datasets where the data (e.g., DBPedia facts) and related texts (e.g., Wikipedia abstracts) are loosely aligned.",abstractText,[0],[0]
We tackle this challenging task by introducing a special-purpose content selection mechanism.1,abstractText,[0],[0]
We use multi-instance learning to automatically discover correspondences between data and text pairs and show how these can be used to enhance the content signal while training an encoder-decoder architecture.,abstractText,[0],[0]
Experimental results demonstrate that models trained with content-specific objectives improve upon a vanilla encoder-decoder which solely relies on soft attention.,abstractText,[0],[0]
Bootstrapping Generators from Noisy Data,title,[0],[0]
