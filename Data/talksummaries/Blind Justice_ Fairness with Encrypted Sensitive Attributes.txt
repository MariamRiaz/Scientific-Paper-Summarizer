37	17	The source of societal concern is that sensitive attributes zi are potentially correlated with xi or yi.
41	8	For each user Ui, M observes or is provided xi, yi.
43	7	While each user Ui wants fθ to meet F, they also wish to keep zi private from all other parties.
49	6	We refer to D = {(xi, yi)}ni=1 and Z = {zi}ni=1 as the non-sensitive and sensitive data, respectively.
50	4	In Section 2.2, we first provide necessary background on various notions of fairness that have been explored in the fair learning literature.
54	15	Works which use these or related notions include (Hardt et al., 2016; Zafar et al., 2017c;a;b).
55	6	In this work we focus on a variant of eq.
59	5	To do so, we propose that users send their non-sensitive data D to REG; and send encrypted versions of their sensitive data Z to both M and REG.
65	16	We propose to solve this by having users send their non-sensitive data D to M and to distribute encryptions of their sensitive data to M and REG as in certification.
67	8	This setup is shown in Figure 1 (Center).
68	15	Privacy constraints: (C1) privacy of sensitive user data, (C2) model secrecy, and (C3) minimal disclosure of D to M. Decision verification.
81	4	Introducing a regulator removes these barriers and leaves users’ computational burden at a minimum level, with envisaged applications practical with only their web browsers.
84	6	However, this setup hinders exploratory data analysis by the modeler which might promote robust model-building, and, in the case of verification, validation by the regulator that user-provided data is correct.
91	6	The two parties involved in our computation are the modeler M and the regulator REG.
97	9	All protocols proceed by having the parties jointly evaluate the circuit, processing it gate by gate while keeping intermediate values hidden from both parties by means of a secret sharing scheme.
119	4	Additionally, in our functionality, the hash of θ should be computed inside MPC, to hide θ from REG.
125	6	An alternative solution is possible based on symmetric encryption under a shared key, as highly efficient MPC implementations of block ciphers such as AES are available (Keller et al., 2017).
126	4	To realize the fair training functionality from the previous section, we follow closely the techniques recently introduced by Mohassel & Zhang (2017).
130	4	The formal privacy guarantees of our fair training protocol are stated in the following proposition.
131	6	For non-colluding M and REG, our protocol implements the fair model training functionality satisfying constraints (C1)-(C3) in Section 2.3 in the presence of a semi-honest adversary.
146	4	(8), with the fairness function F in eq.
213	4	By design, the synthetic dataset exhibits a clear trade-off between accuracy and fairness.
229	4	The effect is most pronounced for the synthetic dataset by construction.
237	4	As pointed out in Section 3, certification of a trained model requires checking whether F(θ) > 0.
239	14	It only takes a negligible fraction of the computation time, see Table 1.
242	37	Using the methods and tricks introduced in Section 4, we can overcome accuracy as well as over- and underflow concerns due to fixed-point numbers.
243	13	Offline precomputation combined with a fast C++ implementation yield viable running times for reasonably large datasets on a laptop computer.
244	14	Real world fair learning has suffered from a dilemma: in order to enforce fairness, sensitive attributes must be examined; yet in many situations, users may feel uncomfortable in revealing these attributes, or modelers may be legally restricted in collecting and utilizing them.
245	143	By introducing recent methods from MPC, and extending them to handle linear constraints as required for various notions of fairness, we have demonstrated that it is practical on real-world datasets to: (i) certify and sign a model as fair; (ii) learn a fair model; and (iii) verify that a fair-certified model has indeed been used; all while maintaining cryptographic privacy of all users’ sensitive attributes.
246	136	Connecting concerns in privacy, algorithmic fairness and accountability, our proposal empowers regulators to provide better oversight, modelers to develop fair and private models, and users to retain control over data they consider highly sensitive.
