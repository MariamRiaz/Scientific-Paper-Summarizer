7	19	Similarly, in Children’s Books test (CBT) (Hill et al., 2016), cloze questions are obtained by removing a word in the last sentence of every consecutive 21 sentences, with the first 20 sentences being the context.
8	23	Different from CNN/Daily Mail datasets, CBT also provides each question with a candidate answer set, consisting of randomly sampled words with the same part-of-speech tag from the context as that of the correct answer.
19	14	While there have been works trying to incorporate human design into cloze question generation (Zweig and Burges, 2011; Paperno et al., 2016), due to the expensive labeling process, the MSR Sentence Completion Challenge created by this effort has 1, 040 questions and the LAMBADA (Paperno et al., 2016) dataset has 10, 022 questions, limiting the possibility of developing powerful neural models on it.
21	14	Motivated by the aforementioned drawbacks, we propose CLOTH, a large-scale cloze test dataset collected from English exams.
22	63	Questions in the dataset are designed by middle-school and highschool teachers to prepare Chinese students for entrance exams.
28	14	We find that the state-of-theart model lags behind human performance even if the model is trained on a large external corpus.
55	27	We collect the raw data from three free and public websites in China that gather exams created by English teachers to prepare students for college/high school entrance exams3.
77	42	Grammar questions are easily differentiated from other two categories.
92	15	The majority of questions are shortterm-reasoning questions while approximately 22.4% of the data needs long-term information, in which the long-term-reasoning questions constitute a large proportion.
100	14	Specifically, we adapt the Stanford Attentive Reader (Chen et al., 2016) and the positionaware attention model (Zhang et al., 2017) to the cloze test problem.
101	38	With the position-aware attention model, the attention scores are based on both the context match and the distance from a context to the blank.
104	19	Suppose xi is the missing word and x1, · · · , xi−1, xi+1, · · · , xn are the context, we choose xi that maximizes the joint probability p(x1, · · · , xn), which essentially maximizes the conditional likelihood p(xi | x1, · · · , xi−1, xi+1, · · · , xn).
151	12	However, to investigate the differences between training on sentence level and on paragraph level, a prohibitive amount of computational resource is required to train a large model on the 1 Billion Word Corpus.
155	29	Another reason is that the reliability of question type labels depends on whether turkers are careful enough.
156	15	For example, in the error analysis shown in Table 5, a careless turker would label the second example as short-term-reasoning without noticing that the meaning of “they” relies on a long context.
159	21	By limiting the context span manually, the ceiling performance with the access to only a short context is estimated accurately.
160	79	As shown in Table 6, The performance of 1BLM using one sentence as the context can almost match the human ceiling performance of only using short-term information.
166	20	However, research on cloze test design (Sachs et al., 1997) shows that tests created by deliberately deleting words are more reliable than tests created by randomly or periodically deleting words.
176	68	The model’s performance and human’s performance on the human-created data are 0.484 and 0.859 respectively, as shown in Tab.
177	32	In comparison, the performance gap on the automatically-generated data is at most 0.185 since the model’s performance reaches an accuracy of 0.815 when fully trained on generated data.
188	17	A possible avenue towards having large-scale human-created data is to automatically pick out a large number of generated questions which are representative of or similar to human-created questions.
189	17	In other words, we train a network to predict whether a question is a generated question or a human-created question.
193	49	Let x denote the passage and z denote whether a word is selected as a question by human, i.e., z is 1 if this word is selected to be filled in the original passage or 0 otherwise.
212	14	In this paper, we propose a large-scale cloze test dataset CLOTH that is designed by teachers.
216	34	We also show that, compared to automatically-generated questions, human-created questions are more difficult and lead to a larger margin between human performance and the model’s performance.
219	55	We hope our dataset provides a valuable testbed to the language modeling community and the machine comprehension community.
232	12	In addition, the vocabulary used in CLOTH are usually not difficult since they are constructed to test non-native speakers in middle school or high school.
233	45	To get a concrete idea of the nature of question types, please refer to examples shown in Tab.
236	78	From the comparison shown in Figure 2, we see that 1B-LM is indeed good at short-term questions.
242	17	The hidden dimension is set to 650 and we initialize the word embedding by 300-dimensional Glove word vector (Pennington et al., 2014).
