0,1,label2,summary_sentences
"Recently, deep learning has emerged as a powerful and popular class of machine learning algorithms.",1. Introduction,[0],[0]
"Well-known examples include the convolutional neural network (LeCun et al., 1998), long short term memory (Hochreiter & Schmidhuber, 1997), memory network (Weston et al., 2014), and deep Q-network (Mnih et al., 2015).",1. Introduction,[0],[0]
"These models have achieved remarkable performance on various difficult tasks such as image classification (He et al., 2016), speech recognition (Graves et al., 2013), natural language understanding (Bahdanau et al., 2015; Sukhbaatar et al., 2015), and game playing (Silver et al., 2016).
",1. Introduction,[0],[0]
"Deep network is a highly nonlinear model with typically millions of parameters (Hinton et al., 2006).",1. Introduction,[0],[0]
"Thus, it is imperative to design scalable and effective solvers.",1. Introduction,[0],[0]
"How-
",1. Introduction,[0],[0]
"1Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong.",1. Introduction,[0],[0]
"Correspondence to: Shuai Zheng <szhengac@cse.ust.hk>, James T. Kwok <jamesk@cse.ust.hk>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
ever, training deep networks is difficult as the optimization can suffer from pathological curvature and get stuck in local minima (Martens, 2010).",1. Introduction,[0],[0]
"Moreover, every critical point that is not a global minimum is a saddle point (Kawaguchi, 2016), which can significantly slow down training.",1. Introduction,[0],[0]
Second-order information is useful in that it reflects local curvature of the error surface.,1. Introduction,[0],[0]
"However, a direct computation of the Hessian is computationally infeasible.",1. Introduction,[0],[0]
"Martens (2010) introduced Hessian-free optimization, a variant of truncated-Newton methods that relies on using the linear conjugate gradient to avoid computing the Hessian.",1. Introduction,[0],[0]
Dauphin et al. (2014) proposed to use the absolute Hessian to escape from saddle points.,1. Introduction,[0],[0]
"However, these methods still require higher computational costs.
",1. Introduction,[0],[0]
"Recent advances in deep learning optimization focus mainly on stochastic gradient descent (SGD) (Bottou, 1998) and its variants (Sutskever et al., 2013).",1. Introduction,[0],[0]
"However, SGD requires careful stepsize tuning, which is difficult as different weights have vastly different gradients (in terms of both magnitude and direction).",1. Introduction,[0],[0]
"On the other hand, online learning (Zinkevich, 2003), which is closely related to stochastic optimization, has been extensively studied in the past decade.",1. Introduction,[0],[0]
"Well-known algorithms include follow the regularized leader (FTRL) (Kalai & Vempala, 2005), follow the proximally-regularized leader (FTPRL) (McMahan & Streeter, 2010) and their variants (Duchi & Singer, 2009; Duchi et al., 2011; Shalev-Shwartz, 2012; Xiao, 2010).",1. Introduction,[0],[0]
"In particular, adaptive gradient descent (Adagrad) (Duchi et al., 2011) uses an adaptive per-coordinate stepsize.",1. Introduction,[0],[0]
"On convex problems, it has been shown both theoretically and empirically that Adagrad is especially efficient on highdimensional data (Duchi et al., 2011; McMahan et al., 2013).",1. Introduction,[0],[0]
"When used on deep networks, Adagrad also demonstrates significantly better performance than SGD (Dean et al., 2012).",1. Introduction,[0],[0]
"However, in Adagrad, the variance estimate underlying the adaptive stepsize is based on accumulating all past (squared) gradients.",1. Introduction,[0],[0]
This becomes infinitesimally small as training proceeds.,1. Introduction,[0],[0]
"In more recent algorithms, such as RMSprop (Tieleman & Hinton, 2012) and Adam (Kingma & Ba, 2015), the variance is estimated by an exponentially decaying average of the squared gradients.
",1. Introduction,[0],[0]
"Another problem with the FTRL family of algorithms is that in each round, the learner has to solve an optimization problem that considers the sum of all previous gradients.
",1. Introduction,[0],[0]
"For highly nonconvex models such as the deep network, the parameter iterate may move from one local basin to another.",1. Introduction,[0],[0]
Gradients that are due to samples in the distant past are less informative than those from the recent ones.,1. Introduction,[0],[0]
"In applications where the data distribution is changing (as in deep reinforcement learning), this may impede parameter adaptation to the environment.
",1. Introduction,[0],[0]
"To alleviate this problem, we propose a FTPRL variant that reweighs the learning subproblems in each iteration.",1. Introduction,[0],[0]
"The proposed algorithm, which will be called follow the moving leader (FTML), shows strong connections with popular deep learning optimizers such as RMSprop and Adam.",1. Introduction,[0],[0]
"Experiments on various deep learning models demonstrate that FTML outperforms or at least has comparable convergence performance with state-of-the-art solvers.
",1. Introduction,[0],[0]
The rest of this paper is organized as follows.,1. Introduction,[0],[0]
Section 2 first gives a brief review on FTRL and other solvers for deep learning.,1. Introduction,[0],[0]
Section 3 presents the proposed FTML.,1. Introduction,[0],[0]
"Experimental results are shown in Section 4, and the last section gives some concluding remarks.
Notation.",1. Introduction,[0],[0]
"For a vector x ∈ Rd, ‖x‖ = √∑d
i=1",1. Introduction,[0],[0]
"x 2 i ,
diag(x) is a diagonal matrix with x on its diagonal, √ x is the element-wise square root of x, x2 denotes the Hadamard (elementwise) product x x, and ‖x‖2Q = xTQx, whereQ is a symmetric matrix.",1. Introduction,[0],[0]
"For any two vectors x and y, x/y, and 〈x, y〉 denote the elementwise division and dot product, respectively.",1. Introduction,[0],[0]
"For a matrix X , X2 = XX , and diag(X) is a vector with the diagonal of X as its elements.",1. Introduction,[0],[0]
"For t vectors {x1, . . .",1. Introduction,[0],[0]
", xt}, x1:",1. Introduction,[0],[0]
t = ∑t i=1,1. Introduction,[0],[0]
"xi, and
x21:",1. Introduction,[0],[0]
t = ∑t i=1,1. Introduction,[0],[0]
"x
2 i .",1. Introduction,[0],[0]
"For t matrices {X1, . . .",1. Introduction,[0],[0]
", Xt}, X1:t =∑t
i=1Xi.",1. Introduction,[0],[0]
"In online learning, the learner observes a sequence of functions fi’s, which can be deterministic, stochastic, or even adversarially chosen.",2.1. Follow the Regularized Leader and its Variants,[0],[0]
Let Θ ⊆ Rd be a convex compact set.,2.1. Follow the Regularized Leader and its Variants,[0],[0]
"At round t, the learner picks a predictor θt−1 ∈ Θ, and the adversary picks a loss ft.",2.1. Follow the Regularized Leader and its Variants,[0],[0]
The learner then suffers a loss ft(θt−1).,2.1. Follow the Regularized Leader and its Variants,[0],[0]
The goal of the learner is to minimize the cumulative loss suffered over the course of T rounds.,2.1. Follow the Regularized Leader and its Variants,[0],[0]
"In online convex learning, ft is assumed to be convex.
",2.1. Follow the Regularized Leader and its Variants,[0],[0]
"Two popular online learning algorithms are the follow the regularized leader (FTRL) (Kalai & Vempala, 2005; Shalev-Shwartz, 2012), and its variant follow the proximally-regularized leader (FTPRL) (McMahan & Streeter, 2010).",2.1. Follow the Regularized Leader and its Variants,[0],[0]
"Both achieve the optimal O( √ T ) regret, where T is the number of rounds (Shalev-Shwartz, 2012).",2.1. Follow the Regularized Leader and its Variants,[0],[0]
"Other FTRL-like algorithms include regularized dual aver-
aging (RDA) (Xiao, 2010) as well as its adaptive variant presented in (Duchi et al., 2011).",2.1. Follow the Regularized Leader and its Variants,[0],[0]
"Gradient descent style algorithms like online forward and backward splitting (FOBOS) (Duchi & Singer, 2009) and adaptive gradient descent (Adagrad) (Duchi et al., 2011) can also be expressed as special cases of the FTRL family (McMahan, 2011).
",2.1. Follow the Regularized Leader and its Variants,[0],[0]
"At round t, FTRL generates the next iterate θt by solving the optimization problem:
θt = arg min θ∈Θ t∑ i=1",2.1. Follow the Regularized Leader and its Variants,[0],[0]
"( 〈gi, θ〉+ αt 2 ‖θ‖2 ) ,
where gt is a subgradient of ft at θt−1 (usually, θ0 = 0), and αt is the regularization parameter at round t. Note that the regularization is centered at the origin.",2.1. Follow the Regularized Leader and its Variants,[0],[0]
"McMahan & Streeter (2010) generalizes this to FTPRL by centering regularization at each iterate θi−1 as in online gradient descent and online mirror descent (Cesa-Bianchi & Lugosi, 2006),
θt = arg min θ∈Θ t∑ i=1",2.1. Follow the Regularized Leader and its Variants,[0],[0]
"( 〈gi, θ〉+ 1 2 ‖θ − θi−1‖2Qi ) , (1)
where Qi is a full or diagonal positive semidefinite matrix, and ‖θ",2.1. Follow the Regularized Leader and its Variants,[0],[0]
− θi−1‖Qi is the corresponding Mahalanobis distance between θ and θi−1.,2.1. Follow the Regularized Leader and its Variants,[0],[0]
"When Qi is diagonal, each of its entries controls the learning rate in the corresponding dimension.",2.1. Follow the Regularized Leader and its Variants,[0],[0]
"When Θ = Rd, θt can be obtained in closedform (McMahan, 2011):
θt = θt−1 −Q−11:t gt.",2.1. Follow the Regularized Leader and its Variants,[0],[0]
"(2)
",2.1. Follow the Regularized Leader and its Variants,[0],[0]
"When
Qt = 1
η diag
(√ g21:t − √ g21:t−1 ) , (3)
where η > 0 is the stepsize, (2) becomes the update rule of Adagrad (Duchi et al., 2011)
θt = θt−1 − diag
( η√
g21:t + 1
) gt.",2.1. Follow the Regularized Leader and its Variants,[0],[0]
"(4)
Here, > 0 (usually a very small number) is used to avoid division by zero, and 1 is the vector of all 1’s.
",2.1. Follow the Regularized Leader and its Variants,[0],[0]
"In general, all these algorithms satisfy (McMahan & Streeter, 2010):
Q1:t = diag ( 1
η
(√ g21:t + 1 )) .",2.1. Follow the Regularized Leader and its Variants,[0],[0]
"(5)
It can be shown that this setting is optimal within a factor of √ 2 of the best possible regret bound for any nonincreasing per-coordinate learning rate schedule (McMahan & Streeter, 2010).",2.1. Follow the Regularized Leader and its Variants,[0],[0]
"In training deep networks, different weights may have vastly different gradients (in terms of both magnitude and direction).",2.2. Adaptive Learning Rate in Deep Learning,[0],[0]
"Hence, using a per-coordinate learning rate as in Adagrad can significantly improve performance over standard SGD (Dean et al., 2012).",2.2. Adaptive Learning Rate in Deep Learning,[0],[0]
"However, a caveat is that Adagrad suffers from diminishing stepsize.",2.2. Adaptive Learning Rate in Deep Learning,[0],[0]
"As optimization proceeds, the accumulated squared gradient g21:t in (5) becomes larger and larger, making training difficult.
",2.2. Adaptive Learning Rate in Deep Learning,[0],[0]
"To alleviate this problem, a number of algorithms have been proposed (Zeiler, 2012; Tieleman & Hinton, 2012; Kingma & Ba, 2015).",2.2. Adaptive Learning Rate in Deep Learning,[0],[0]
"Typically, they employ an average of the past squared gradients (i.e., vt = ∑t i=1",2.2. Adaptive Learning Rate in Deep Learning,[0],[0]
"αi,tg 2",2.2. Adaptive Learning Rate in Deep Learning,[0],[0]
"i , where αi,t ∈",2.2. Adaptive Learning Rate in Deep Learning,[0],[0]
"[0, 1]), which is exponentially decaying.",2.2. Adaptive Learning Rate in Deep Learning,[0],[0]
"For example, RMSprop (Tieleman & Hinton, 2012) uses
vi = βvi−1 + (1− β)g2i , (6)
where β is close to 1, and the corresponding αi,t is (1 − β)βt−i.",2.2. Adaptive Learning Rate in Deep Learning,[0],[0]
"This vt can then be used to replace g21:t, and the update in (4) becomes
θt = θt−1 − diag (
η √ vt + 1
) gt.",2.2. Adaptive Learning Rate in Deep Learning,[0],[0]
"(7)
Zeiler (2012) further argues that the parameter and update should have the same unit, and modifies (7) to the Adadelta update rule:
θt = θt−1 − diag (√
ut−1 + 1√ vt + 1
) gt,
where ut−1 = ∑t−1 i=0 αi,t−1(4θi)2, and 4θt = θt − θt−1 with4θ0 = 0.
",2.2. Adaptive Learning Rate in Deep Learning,[0],[0]
"As v0 in (6) is often initialized to 0, the bias has to be corrected.",2.2. Adaptive Learning Rate in Deep Learning,[0],[0]
"Adam (Kingma & Ba, 2015) uses the variance estimate vt/(1 − βt) (which corresponds to αi,t = (1− β)βt−i/(1− βt)).
",2.2. Adaptive Learning Rate in Deep Learning,[0],[0]
"Another recent proposal is the equilibrated stochastic gradient descent (Dauphin et al., 2015).",2.2. Adaptive Learning Rate in Deep Learning,[0],[0]
"It uses the variance estimate vt = vt−1 +(Htζt)2, whereHt is the Hessian and ζt ∼ N (0, 1).",2.2. Adaptive Learning Rate in Deep Learning,[0],[0]
"It is shown that (Htζt)2 is an unbiased estimator of √ diag(H2t ), which serves as the Jacobi preconditioner of the absolute Hessian.",2.2. Adaptive Learning Rate in Deep Learning,[0],[0]
"Computation of the Hessian can be avoided by using the R-operator (Schraudolph, 2002), though it still costs roughly twice that of standard backpropagation.",2.2. Adaptive Learning Rate in Deep Learning,[0],[0]
"Recall that at round t, FTRL generates the next iterate θt as
θt = arg min θ∈Θ t∑ i=1",3. Follow the Moving Leader,[0],[0]
"Pi(θ), (8)
where Pi(θ) = 〈gi, θ〉 + 12‖θ",3. Follow the Moving Leader,[0],[0]
− θi−1‖ 2 Qi .,3. Follow the Moving Leader,[0],[0]
Note that all Pi’s have the same weight.,3. Follow the Moving Leader,[0],[0]
"However, for highly nonconvex models such as the deep network, the parameter iterate may move from one local basin to another.",3. Follow the Moving Leader,[0],[0]
Pi’s that are due to samples in the distant past are less informative than those from the recent ones.,3. Follow the Moving Leader,[0],[0]
"To alleviate this problem, one may consider only Pi’s in a recent window.",3.1. Weighting the Components,[0],[0]
"However, a large memory is needed for its implementation.",3.1. Weighting the Components,[0],[0]
"A simpler alternative is by using an exponential moving average of the Pi’s: Si = β1Si−1 + (1 − β1)Pi, where β1 ∈",3.1. Weighting the Components,[0],[0]
"[0, 1) and S0 = 0.",3.1. Weighting the Components,[0],[0]
This can be easily rewritten as St = (1− β1) ∑t i=1,3.1. Weighting the Components,[0],[0]
β t−i 1 Pi.,3.1. Weighting the Components,[0],[0]
"Instead of minimizing (8), we have
θt = arg min θ∈Θ t∑ i=1",3.1. Weighting the Components,[0],[0]
"wi,tPi(θ), (9)
where the weights
wi,t = (1− β1)βt−i1
1− βt1 (10)
are normalized to sum to 1.",3.1. Weighting the Components,[0],[0]
The denominator 1− βt1 plays a similar role as bias correction in Adam.,3.1. Weighting the Components,[0],[0]
"When β1 = 0, wi,t = 0 for i < t, and wt,t = 1.",3.1. Weighting the Components,[0],[0]
"Thus, (9) reduces to minθ∈Θ Pt(θ).",3.1. Weighting the Components,[0],[0]
"When β1 → 1, the following Lemma shows that all Pi’s are weighted equally, and (8) is recovered.",3.1. Weighting the Components,[0],[0]
Lemma 1.,3.1. Weighting the Components,[0],[0]
"limβ1→1 wi,t = 1/t.
Note that the Hessian of the objective in (8) is Q1:t. This becomes ∑t i=1",3.1. Weighting the Components,[0],[0]
"wi,tQi in (9).",3.1. Weighting the Components,[0],[0]
"Recall that Q1:t depends on the accumulated past gradients in (5), which is then refined by an exponential moving average in (6).",3.1. Weighting the Components,[0],[0]
"As in Adam, we define vi = β2vi−1 + (1 − β2)g2i , where β2 ∈",3.1. Weighting the Components,[0],[0]
"[0, 1) and v0 = 0, and then correct its bias by dividing by 1 − βt2.",3.1. Weighting the Components,[0],[0]
"Thus, (5) is changed to
t∑ i=1",3.1. Weighting the Components,[0],[0]
"wi,tQi = diag ( 1 ηt (√ vt 1− βt2 + t1 )) , (11)
where ηt and t are the stepsize and value at time t, respectively.",3.1. Weighting the Components,[0],[0]
"When β2 = 0, (11) reduces to ∑t i=1",3.1. Weighting the Components,[0],[0]
"wi,tQi =
diag (
1 ηt ( √ g2t + t1) ) .",3.1. Weighting the Components,[0],[0]
"When β2 → 1, all g2i ’s are
weighted equally and (11) reduces to ∑t i=1",3.1. Weighting the Components,[0],[0]
"wi,tQi =
diag (
1 ηt (√ g21:",3.1. Weighting the Components,[0],[0]
t t + t1 )) .,3.1. Weighting the Components,[0],[0]
"Using ηt = η/ √ t and t =
/ √ t, this is further reduced to (5).",3.1. Weighting the Components,[0],[0]
"The following shows
that Qt in (11) has a closed-form expression.
",3.1. Weighting the Components,[0],[0]
Proposition 1.,3.1. Weighting the Components,[0],[0]
"Define dt = 1−βt1 ηt
(√ vt
1−βt2 + t1
) .",3.1. Weighting the Components,[0],[0]
"Then,
Qt = diag ( dt − β1dt−1
1− β1
) .",3.1. Weighting the Components,[0],[0]
"(12)
Algorithm 1 Follow the Moving Leader (FTML).",3.1. Weighting the Components,[0],[0]
"1: Input: ηt > 0, β1, β2 ∈",3.1. Weighting the Components,[0],[0]
"[0, 1), t > 0. 2: initialize θ0 ∈ Θ; d0 ← 0; v0 ← 0; z0 ← 0; 3: for t = 1, 2, . . .",3.1. Weighting the Components,[0],[0]
", T do 4: fetch function ft; 5: gt ← ∂θft(θt−1); 6: vt ← β2vt−1",3.1. Weighting the Components,[0],[0]
"+ (1− β2)g2t ;
7: dt ← 1−β t 1
ηt
(√ vt
1−βt2 + t1
) ;
8: σt ← dt − β1dt−1; 9: zt ← β1zt−1 + (1− β1)gt − σtθt−1;
10: θt ← Π diag(dt/(1−βt1))",3.1. Weighting the Components,[0],[0]
"Θ (−zt/dt); 11: end for 12: Output: θT .
",3.1. Weighting the Components,[0],[0]
"Substituting this back into (9), θt is then equal to
arg min θ∈Θ t∑ i=1",3.1. Weighting the Components,[0],[0]
"wi,t ( 〈gi, θ〉+ 1 2 ‖θ",3.1. Weighting the Components,[0],[0]
"− θi−1‖2diag ( σi 1−β1 )) , (13) where σi ≡",3.1. Weighting the Components,[0],[0]
di − β1di−1.,3.1. Weighting the Components,[0],[0]
"Note that some entries of σi may be negative, and ‖θ− θi−1‖2diag(σi/(1−β1)) is then not a regularizer in the usual sense.",3.1. Weighting the Components,[0],[0]
"Instead, the negative entries of σi encourage the corresponding entries of θ to move away from those of θi−1.",3.1. Weighting the Components,[0],[0]
"Nevertheless, from the definitions of dt, σt and (11), we have ∑t i=1",3.1. Weighting the Components,[0],[0]
"wi,tdiag(σi/(1 − β1))",3.1. Weighting the Components,[0],[0]
"=∑t
i=1",3.1. Weighting the Components,[0],[0]
"wi,tQi = diag(dt/(1−βt1)), and thus the following: Lemma 2. ∑t i=1",3.1. Weighting the Components,[0],[0]
"wi,tdiag(σi/(1− β1)) 0.
",3.1. Weighting the Components,[0],[0]
"Hence, the objective in (13) is still strongly convex.",3.1. Weighting the Components,[0],[0]
"Moreover, the following Proposition shows that θt in (13) has a simple closed-form solution.
",3.1. Weighting the Components,[0],[0]
Proposition 2.,3.1. Weighting the Components,[0],[0]
"In (13),
θt = Π diag(dt/(1−βt1))",3.1. Weighting the Components,[0],[0]
"Θ (−zt/dt),
where zt = β1zt−1 + (1 − β1)gt − σtθt−1, and ΠAΘ(x) ≡ arg minu∈Θ 1 2‖u−x‖ 2",3.1. Weighting the Components,[0],[0]
"A is the projection onto Θ for a given positive semidefinite matrix A.
The proposed procedure, which will be called follow the moving leader (FTML), is shown in Algorithm 1.",3.1. Weighting the Components,[0],[0]
"Note that though {P1, . . .",3.1. Weighting the Components,[0],[0]
", Pt} are considered in each round, the update depends only the current gradient gt and parameter θt−1.",3.1. Weighting the Components,[0],[0]
"It can be easily seen that FTML is easy to implement, memory-efficient and has low per-iteration complexity.",3.1. Weighting the Components,[0],[0]
"The following Propositions show that we can recover Adagrad in two extreme cases: (i) β1 = 0 with decreasing stepsize; and (ii) β1 → 1 with increasing stepsize.
",3.2.1. RELATIONSHIP WITH ADAGRAD,[0],[0]
Proposition 3.,3.2.1. RELATIONSHIP WITH ADAGRAD,[0],[0]
"With β1 = 0, β2 → 1, ηt = η/ √ t, and
t = / √ t, θt in (13) reduces to:
Π diag(( √ g21:t+ 1)/η)
",3.2.1. RELATIONSHIP WITH ADAGRAD,[0],[0]
"Θ
( θt−1 − diag ( η√
g21:t + 1
) gt ) ,
which recovers Adagrad in (4).",3.2.1. RELATIONSHIP WITH ADAGRAD,[0],[0]
Proposition 4.,3.2.1. RELATIONSHIP WITH ADAGRAD,[0],[0]
"With β1 → 1, β2 → 1, ηt = η √ t, and
t = / √ t, we recover (1) with Qi in (3).",3.2.1. RELATIONSHIP WITH ADAGRAD,[0],[0]
"If Θ = Rd, it
generates identical updates as Adagrad in (4).",3.2.1. RELATIONSHIP WITH ADAGRAD,[0],[0]
"When Θ = Rd, McMahan (2011) showed that (1) and (2) generate the same updates.",3.2.2. RELATIONSHIP WITH RMSPROP,[0],[0]
The following Theorem shows that FTML also has a similar gradient descent update.,3.2.2. RELATIONSHIP WITH RMSPROP,[0],[0]
Theorem 1.,3.2.2. RELATIONSHIP WITH RMSPROP,[0],[0]
"With Θ = Rd, FTML generates the same updates as:
θt = θt−1 − diag ( 1− β1 1− βt1 ηt√ vt/(1− βt2) + t1 ) gt.",3.2.2. RELATIONSHIP WITH RMSPROP,[0],[0]
"(14)
When β1 = 0 and bias correction for the variance is not used, (14) reduces to RMSprop in (7).",3.2.2. RELATIONSHIP WITH RMSPROP,[0],[0]
"However, recall from Section 3.1 that when β1 = 0, we have wi,t = 0 for i < t, and wt,t = 1.",3.2.2. RELATIONSHIP WITH RMSPROP,[0],[0]
"Hence, only the current loss component Pt is taken into account, and this may be sensitive to the noise in Pt.",3.2.2. RELATIONSHIP WITH RMSPROP,[0],[0]
"Moreover, as demonstrated in Adam, bias correction of the variance can be very important.",3.2.2. RELATIONSHIP WITH RMSPROP,[0],[0]
"When β2 → 1, the variance estimate of RMSprop,∑t i=1(1−β2)β t−i 2 g 2 i , becomes zero and blows up the stepsize, leading to divergence.",3.2.2. RELATIONSHIP WITH RMSPROP,[0],[0]
"In contrast, FTML’s Qi in (12) recovers that of Adagrad in this case (Proposition 4).",3.2.2. RELATIONSHIP WITH RMSPROP,[0],[0]
"In practice, a smaller β2 has to be used for RMSprop.",3.2.2. RELATIONSHIP WITH RMSPROP,[0],[0]
"However, a larger β2 enables the algorithm to be more robust to the gradient noise in general.",3.2.2. RELATIONSHIP WITH RMSPROP,[0],[0]
"At iteration t, instead of centering regularization at each θi−1 in (13), consider centering all the proximal regularization terms at the last iterate θt−1. θt then becomes:
arg min θ∈Θ t∑ i=1",3.2.3. RELATIONSHIP WITH ADAM,[0],[0]
"wi,t ( 〈gi, θ〉+ 1 2 ‖θ − θt−1‖2diag ( σi 1−β1 )) .",3.2.3. RELATIONSHIP WITH ADAM,[0],[0]
"(15) Compared with (13), the regularization in (15) is more aggressive as it encourages θt to be close only to the last iterate θt−1.",3.2.3. RELATIONSHIP WITH ADAM,[0],[0]
The following Proposition shows that (15) generates the same updates as Adam.,3.2.3. RELATIONSHIP WITH ADAM,[0],[0]
Proposition 5.,3.2.3. RELATIONSHIP WITH ADAM,[0],[0]
"In (15),
θt = Π At Θ ( θt−1 −A−1t t∑ i=1",3.2.3. RELATIONSHIP WITH ADAM,[0],[0]
"wi,tgi ) , (16)
where At = diag(( √ vt/(1− βt2) + t1)/ηt).
",3.2.3. RELATIONSHIP WITH ADAM,[0],[0]
"As in Adam, ∑t i=1",3.2.3. RELATIONSHIP WITH ADAM,[0],[0]
"wi,tgi in (16) can be obtained as mt/(1−βt1), wheremt is computed as an exponential moving average of gt’s: mt = β1mt−1 +",3.2.3. RELATIONSHIP WITH ADAM,[0],[0]
"(1− β1)gt.
Note that the θt updates of Adagrad (4), RMSprop (7), and FTML (14) depend only on the current gradient gt.",3.2.3. RELATIONSHIP WITH ADAM,[0],[0]
"On the other hand, the Adam update in (16) involves ∑t i=1",3.2.3. RELATIONSHIP WITH ADAM,[0],[0]
"wi,tgi, which contains all the past gradients (evaluated at past parameter estimates θi−1’s).",3.2.3. RELATIONSHIP WITH ADAM,[0],[0]
"This is similar to the use of momentum, which is sometimes helpful in escaping from local minimum.",3.2.3. RELATIONSHIP WITH ADAM,[0],[0]
"However, when the data distribution is changing (as in deep reinforcement learning), the past gradients may not be very informative, and can even impede parameter adaptation to the environment.",3.2.3. RELATIONSHIP WITH ADAM,[0],[0]
"Recently, it is also reported that the use of momentum can make training unstable when the loss is nonstationary (Arjovsky et al., 2017).",3.2.3. RELATIONSHIP WITH ADAM,[0],[0]
"Indeed, Theorem 4.1 in (Kingma & Ba, 2015) shows that Adam has low regret only when β1 is decreasing w.r.t.",3.2.3. RELATIONSHIP WITH ADAM,[0],[0]
t.,3.2.3. RELATIONSHIP WITH ADAM,[0],[0]
"When β1 → 0, ∑t i=1 wi,tgi → gt and so only the current gradient is used.
",3.2.3. RELATIONSHIP WITH ADAM,[0],[0]
Remark 1.,3.2.3. RELATIONSHIP WITH ADAM,[0],[0]
(Summary) RMSprop and Adam are improvements over Adagrad in training deep networks.,3.2.3. RELATIONSHIP WITH ADAM,[0],[0]
"However, RMSprop uses β1 = 0 (and thus relies only on the current sample), does not correct the bias of the variance estimate, but centers the regularization at the current iterates θi−1’s.",3.2.3. RELATIONSHIP WITH ADAM,[0],[0]
"On the other hand, Adam uses β1 > 0, bias-corrected variance, but centers all regularization terms at the last iterate θt−1.",3.2.3. RELATIONSHIP WITH ADAM,[0],[0]
The proposed FTML combines the nice properties of the two.,3.2.3. RELATIONSHIP WITH ADAM,[0],[0]
"In this section, experiments are performed on a number of deep learning models, including convolutional neural networks (Section 4.1), deep residual networks (Section 4.2), memory networks (Section 4.3), neural conversational model (Section 4.4), deep Q-network (Section 4.5), and long short-term memory (LSTM) (Section 4.6).",4. Experiments,[0],[0]
"A summary of the empirical performance of the various deep learning optimizers is presented in Section 4.7.
",4. Experiments,[0],[0]
"The following state-of-the-art optimizers for deep learning models will be compared: (i) Adam (Kingma & Ba, 2015); (ii) RMSprop (Tieleman & Hinton, 2012); (iii) Adadelta (Zeiler, 2012); and (iv)",4. Experiments,[0],[0]
"Nesterov accelerated gradient (NAG) (Sutskever et al., 2013).",4. Experiments,[0],[0]
"For FTML, we set β1 = 0.6, β2 = 0.999, and a constant t = = 10−8 for all t. For FTML, Adam, RMSprop, and NAG, η is selected by monitoring performance on the training set (note that Adadelta does not need to set η).",4. Experiments,[0],[0]
"The learning rate is chosen from {0.5, 0.25, 0.1, . . .",4. Experiments,[0],[0]
", 0.00005, 0.000025, 0.00001}.",4. Experiments,[0],[0]
Significantly underperforming learning rates are removed after running the model for 5− 20 epochs.,4. Experiments,[0],[0]
We then pick the rate that leads to the smallest final training loss.,4. Experiments,[0],[0]
"In the section, we perform experiments with the convolutional neural network (CNN) (LeCun et al., 1998).",4.1. Convolutional Neural Networks,[0],[0]
We use the example models on the MNIST and CIFAR-10 data sets from the Keras library1.,4.1. Convolutional Neural Networks,[0],[0]
"For MNIST, the CNN has two alternating stages of 3 × 3 convolution filters (using ReLU activation), followed by a 2 × 2 max-pooling layer and a dropout layer (with a dropout rate of 0.25).",4.1. Convolutional Neural Networks,[0],[0]
"Finally, there is a fully-connected layer with ReLU activation and a dropout rate of 0.5.",4.1. Convolutional Neural Networks,[0],[0]
"For CIFAR-10, the CNN has four alternating stages of 3× 3 convolution filters (using ReLU activation).",4.1. Convolutional Neural Networks,[0],[0]
Every two convolutional layers is followed by a 2×2 maxpooling layer and a dropout layer (with a dropout rate of 0.25).,4.1. Convolutional Neural Networks,[0],[0]
The last stage has a fully-connected layer with ReLU activation and a dropout rate of 0.5.,4.1. Convolutional Neural Networks,[0],[0]
"Features in both data sets are normalized to [0, 1].",4.1. Convolutional Neural Networks,[0],[0]
"Minibatches of sizes 128 and 32 are used for MNIST and CIFAR-10, respectively.
",4.1. Convolutional Neural Networks,[0],[0]
"As the iteration complexities of the various algorithms are comparable and the total cost is dominated by backpropagation, we report convergence of the training cross entropy loss versus the number of epochs.",4.1. Convolutional Neural Networks,[0],[0]
"This setup is also used in (Zeiler, 2012; Kingma & Ba, 2015).
",4.1. Convolutional Neural Networks,[0],[0]
Figure 1 shows the convergence results.,4.1. Convolutional Neural Networks,[0],[0]
"As can be seen, FTML performs best on both data sets.",4.1. Convolutional Neural Networks,[0],[0]
"Adam has comparable performance with FTML on MNIST, but does not perform as well on CIFAR-10.",4.1. Convolutional Neural Networks,[0],[0]
The other methods are much inferior.,4.1. Convolutional Neural Networks,[0],[0]
"In particular, RMSprop is slow on both MNIST and CIFAR-10, and Adadelta tends to diverge on CIFAR-10.",4.1. Convolutional Neural Networks,[0],[0]
"Recently, substantially deeper networks have been popularly used, particularly in computer vision.",4.2. Deep Residual Networks,[0],[0]
"For example, a 152-layer deep residual network (He et al., 2016) achieves state-of-the-art performance on ImageNet classification, and won the first place on the ILSVRC 2015 classification task.
",4.2. Deep Residual Networks,[0],[0]
"In this section, we perform experiments with a 110-layer deep residual network on the CIFAR-10 and CIFAR-100 data sets.",4.2. Deep Residual Networks,[0],[0]
The code is based on its Torch implementation2.,4.2. Deep Residual Networks,[0],[0]
"We leave the architecture and related settings intact, and use the same learning rate schedule.",4.2. Deep Residual Networks,[0],[0]
The default optimizer in the Torch code is NAG.,4.2. Deep Residual Networks,[0],[0]
"Here, we also experiment with Adadelta, RMSprop, Adam and the proposed FTML.",4.2. Deep Residual Networks,[0],[0]
"A minibatch size of 32 is used.
",4.2. Deep Residual Networks,[0],[0]
Convergence of the training cross entropy loss is shown in Figure 2.,4.2. Deep Residual Networks,[0],[0]
"As can been seen, all optimizers, except Adadelta, are very competitive and have comparable per-
1https://github.com/fchollet/keras.",4.2. Deep Residual Networks,[0],[0]
"2https://github.com/facebook/fb.resnet.
torch.
",4.2. Deep Residual Networks,[0],[0]
formance on these two data sets.,4.2. Deep Residual Networks,[0],[0]
"NAG shows slower initial convergence, while FTML converges slightly faster than the others on the CIFAR-10 data set.",4.2. Deep Residual Networks,[0],[0]
"Recently, there has been a lot of attention on combining inference, attention and memory for various machine learning tasks.",4.3. Memory Networks,[0],[0]
"In particular, the memory network (Weston et al., 2014; Sukhbaatar et al., 2015) has been popularly used for natural language understanding.
",4.3. Memory Networks,[0],[0]
"In this section, we use the example model of the end-toend memory network (with LSTM) from the Keras library.",4.3. Memory Networks,[0],[0]
"We consider the question answering task (Sukhbaatar et al., 2015; Weston et al., 2016), and perform experiments on the “single supporting fact” task in the bAbI data set (Weston et al., 2016).",4.3. Memory Networks,[0],[0]
This task consists of questions in which a previously given single sentence provides the answer.,4.3. Memory Networks,[0],[0]
"An
example is shown below.",4.3. Memory Networks,[0],[0]
"We use a single supporting memory, and a minibatch size of 32.
",4.3. Memory Networks,[0],[0]
Single Supporting Fact:,4.3. Memory Networks,[0],[0]
Mary moved to the bathroom.,4.3. Memory Networks,[0],[0]
John went to the hallway.,4.3. Memory Networks,[0],[0]
Where is Mary?,4.3. Memory Networks,[0],[0]
"A: bathroom
Convergence of the training cross entropy loss is shown in Figure 3.",4.3. Memory Networks,[0],[0]
"As can be seen, FTML and RMSprop perform best on this data set.",4.3. Memory Networks,[0],[0]
"Adam is slower, while NAG and Adadelta perform poorly.",4.3. Memory Networks,[0],[0]
"The neural conversational model (Vinyals & Le, 2015) is a sequence-to-sequence model (Sutskever et al., 2014) that is capable of predicting the next sequence given the last or previous sequences in a conversation.",4.4. Neural Conversational Model,[0],[0]
"A LSTM layer en-
codes the input sentence to a thought vector, and a second LSTM layer decodes the thought vector to the response.",4.4. Neural Conversational Model,[0],[0]
"It has been shown that this model can often produce fluent and natural conversations.
",4.4. Neural Conversational Model,[0],[0]
"In this experiment, we use the publicly available Torch implementation3 with a constant stepsize, and its default data set Cornell Movie-Dialogs Corpus (with 50, 000 samples) (Danescu-Niculescu-Mizil & Lee, 2011).",4.4. Neural Conversational Model,[0],[0]
"The number of hidden units is set to 1000, and the minibatch size is 10.
",4.4. Neural Conversational Model,[0],[0]
Convergence of the training cross entropy loss is shown in Figure 4.,4.4. Neural Conversational Model,[0],[0]
"Adadelta is not reported here, since it performs poorly (as in previous experiments).",4.4. Neural Conversational Model,[0],[0]
"As can be seen, FTML outperforms Adam and RMSprop.",4.4. Neural Conversational Model,[0],[0]
"In particular, RMSprop is much inferior.",4.4. Neural Conversational Model,[0],[0]
"NAG is slower than FTML and Adam in the first 21 epochs, but becomes faster towards the end of training.",4.4. Neural Conversational Model,[0],[0]
"In this section, we use the Deep Q-network (DQN) (Mnih et al., 2015) for deep reinforcement learning.",4.5. Deep Q-Network,[0],[0]
Experiments are performed on two computer games on the Atari 2600 platform: Breakout and Asterix.,4.5. Deep Q-Network,[0],[0]
"We use the publicly available Torch implementation with the default network setup4, and a minibatch size of 32.",4.5. Deep Q-Network,[0],[0]
"We only compare FTML with RMSprop and Adam for optimization, as NAG and Adadelta are rarely used in training the DQN.",4.5. Deep Q-Network,[0],[0]
"As in (Mnih et al., 2015), we use = 10−2 for all methods, and performance evaluation is based on the average score per episode.",4.5. Deep Q-Network,[0],[0]
"The higher the score, the better the performance.
",4.5. Deep Q-Network,[0],[0]
Convergence is shown in Figure 5.,4.5. Deep Q-Network,[0],[0]
"On Breakout, RM-
3https://github.com/macournoyer/ neuralconvo.
",4.5. Deep Q-Network,[0],[0]
"4https://github.com/Kaixhin/Atari.
",4.5. Deep Q-Network,[0],[0]
Sprop and FTML are comparable and yield higher scores than Adam.,4.5. Deep Q-Network,[0],[0]
"On Asterix, FTML outperforms all the others.",4.5. Deep Q-Network,[0],[0]
"In particular, the DQN trained with RMSprop fails to learn the task, and its score begins to drop after about 100 epochs.",4.5. Deep Q-Network,[0],[0]
"A similar problem has also been observed in (Hasselt et al., 2016).",4.5. Deep Q-Network,[0],[0]
"Experience replay (Mnih et al., 2015) has been commonly used in deep reinforcement learning to smooth over changes in the data distribution, and avoid oscillations or divergence of the parameters.",4.5. Deep Q-Network,[0],[0]
"However, results here show that Adam still has inferior performance because of its use of all past gradients, many of these are not informative when the data distribution has changed.",4.5. Deep Q-Network,[0],[0]
"To illustrate the problem of Adam in Section 4.5 more clearly, we perform the following timeseries prediction experiment with the LSTM.",4.6. Long Short-Term Memory (LSTM),[0],[0]
We construct a synthetic timeseries of length 1000.,4.6. Long Short-Term Memory (LSTM),[0],[0]
"This is divided into 20 segments, each of length 50.",4.6. Long Short-Term Memory (LSTM),[0],[0]
"At each time point, the sample is 10- dimensional.",4.6. Long Short-Term Memory (LSTM),[0],[0]
"In segment i, samples are generated from a normal distribution with mean ([i, i, . . .",4.6. Long Short-Term Memory (LSTM),[0],[0]
", i] + ζi) ∈ R10 and identity covariance matrix, where the components of ζi are independent standard normal random variables.",4.6. Long Short-Term Memory (LSTM),[0],[0]
Noise from the standard normal distribution is added to corrupt the data.,4.6. Long Short-Term Memory (LSTM),[0],[0]
"The task is to predict the data sample at the next time point t.
We use a one-layer LSTM implemented in (Léonard et al., 2015).",4.6. Long Short-Term Memory (LSTM),[0],[0]
100 hidden units are used.,4.6. Long Short-Term Memory (LSTM),[0],[0]
"We truncate backpropagation through time (BPTT) to 5 timesteps, and input 5 samples to the LSTM in each iteration.",4.6. Long Short-Term Memory (LSTM),[0],[0]
"Thus, the data distribution changes every 10 iterations, as a different normal distribution is then used for data generation.",4.6. Long Short-Term Memory (LSTM),[0],[0]
"Performance evaluation is based on the squared loss ft(θt−1) at time t.
Convergence of the loss is shown in Figure 6(a).",4.6. Long Short-Term Memory (LSTM),[0],[0]
"As can be
seen, Adam has difficulty in adapting to the data.",4.6. Long Short-Term Memory (LSTM),[0],[0]
"In contrast, FTML and RMSprop can adapt more quickly, yielding better and more stable performance.
",4.6. Long Short-Term Memory (LSTM),[0],[0]
"As a baseline, we consider the case where the data distribution does not change (the means of all the segments are fixed to the vector of ones)",4.6. Long Short-Term Memory (LSTM),[0],[0]
Figure 6(b) shows the results.,4.6. Long Short-Term Memory (LSTM),[0],[0]
"As can be seen, Adam now performs comparably to FTML and RMSprop.",4.6. Long Short-Term Memory (LSTM),[0],[0]
The main problem with RMSprop is that its performance is not stable.,4.7. Summary of Results,[0],[0]
"Sometimes, it performs well, but sometimes it can have significantly inferior performance (e.g., as can be seen from Figures 1, 4 and 5(b)).",4.7. Summary of Results,[0],[0]
"The performance of Adam is more stable, though it often lags behind the best optimizer (e.g., Figures 1(b), 3, and 4).",4.7. Summary of Results,[0],[0]
"It is particularly problematic when learning in a changing environment (Fig-
ures 5 and 6(a)).",4.7. Summary of Results,[0],[0]
"In contrast, the proposed FTML shows stable performance on various models and tasks.",4.7. Summary of Results,[0],[0]
"It converges quickly, and is always the best (or at least among the best) in all our experiments.",4.7. Summary of Results,[0],[0]
"In this paper, we proposed a FTPRL variant called FTML, in which the recent samples are weighted more heavily in each iteration.",5. Conclusion,[0],[0]
"Hence, it is able to adapt more quickly when the parameter moves to another local basin, or when the data distribution changes.",5. Conclusion,[0],[0]
FTML is closely related to RMSprop and Adam.,5. Conclusion,[0],[0]
"In particular, it enjoys their nice properties, but avoids their pitfalls.",5. Conclusion,[0],[0]
"Experimental results on a number of deep learning models and tasks demonstrate that FTML converges quickly, and is always the best (or among the best) of the various optimizers.",5. Conclusion,[0],[0]
This research was supported in part by ITF/391/15FX.,Acknowledgments,[0],[0]
Deep networks are highly nonlinear and difficult to optimize.,abstractText,[0],[0]
"During training, the parameter iterate may move from one local basin to another, or the data distribution may even change.",abstractText,[0],[0]
"Inspired by the close connection between stochastic optimization and online learning, we propose a variant of the follow the regularized leader (FTRL) algorithm called follow the moving leader (FTML).",abstractText,[0],[0]
"Unlike the FTRL family of algorithms, the recent samples are weighted more heavily in each iteration and so FTML can adapt more quickly to changes.",abstractText,[0],[0]
"We show that FTML enjoys the nice properties of RMSprop and Adam, while avoiding their pitfalls.",abstractText,[0],[0]
"Experimental results on a number of deep learning models and tasks demonstrate that FTML converges quickly, and outperforms other state-ofthe-art optimizers.",abstractText,[0],[0]
Follow the Moving Leader in Deep Learning,title,[0],[0]
NMT has witnessed promising improvements recently.,1 Introduction,[0],[0]
"Depending on the types of input and output, these efforts can be divided into three categories: string-to-string systems (Sutskever et al., 2014; Bahdanau et al., 2014); tree-to-string systems (Eriguchi et al., 2016, 2017); and string-totree systems (Aharoni and Goldberg, 2017; Nadejde et al., 2017).",1 Introduction,[0],[0]
"Compared with string-to-string systems, tree-to-string and string-to-tree systems (henceforth, tree-based systems) offer some attractive features.",1 Introduction,[0],[0]
"They can use more syntactic information (Li et al., 2017), and can conveniently incorporate prior knowledge (Zhang et al., 2017).
∗ Contribution during internship at National Institute of Information and Communications Technology.
†Corresponding author
Because of these advantages, tree-based methods become the focus of many researches of NMT nowadays.
",1 Introduction,[0],[0]
"Based on how to represent trees, there are two main categories of tree-based NMT methods: representing trees by a tree-structured neural network (Eriguchi et al., 2016; Zaremoodi and Haffari, 2017), representing trees by linearization (Vinyals et al., 2015; Dyer et al., 2016; Ma et al., 2017).",1 Introduction,[0],[0]
"Compared with the former, the latter method has a relatively simple model structure, so that a larger corpus can be used for training and the model can be trained within reasonable time, hence is preferred from the viewpoint of computation.",1 Introduction,[0],[0]
"Therefore we focus on this kind of methods in this paper.
",1 Introduction,[0],[0]
"In spite of impressive performance of tree-based NMT systems, they suffer from a major drawback: they only use the 1-best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors (Quirk and Corston-Oliver, 2006).",1 Introduction,[0],[0]
"For SMT, forest-based methods have employed a packed forest to address this problem (Huang, 2008), which represents exponentially many parse trees rather than just the 1-best one (Mi et al., 2008; Mi and Huang, 2008).",1 Introduction,[0],[0]
"But for NMT, (computationally efficient) forestbased methods are still being explored1.
",1 Introduction,[0],[0]
"Because of the structural complexity of forests, the inexistence of appropriate topological ordering, and the hyperedge-attachment nature of weights (see Section 3.1 for details), it is not trivial to linearize a forest.",1 Introduction,[0],[0]
"This hinders the development of forest-based NMT to some extent.
",1 Introduction,[0],[0]
"Inspired by the tree-based NMT methods based on linearization, we propose an efficient forestbased NMT approach (Section 3), which can en-
1Zaremoodi and Haffari (2017) have proposed a forestbased NMT method based on a forest-structured neural network recently, but it is computationally inefficient (see Section 5).
code the syntactic information of a packed forest on the basis of a novel weighted linearization method for a packed forest (Section 3.1), and can decode the linearized packed forest under the simple sequence-to-sequence framework (Section 3.2).",1 Introduction,[0],[0]
Experiments demonstrate the effectiveness of our method (Section 4).,1 Introduction,[0],[0]
"We first review the general sequence-to-sequence model (Section 2.1), then describe tree-based NMT systems based on linearization (Section 2.2), and finally introduce the packed forest, through which exponentially many trees can be represented in a compact manner (Section 2.3).",2 Preliminaries,[0],[0]
"Current NMT systems usually resort to a simple framework, i.e., the sequence-to-sequence model (Cho et al., 2014; Sutskever et al., 2014).",2.1 Sequence-to-sequence model,[0],[0]
"Given a source sequence (x0, . . .",2.1 Sequence-to-sequence model,[0],[0]
", xT ), in order to find a target sequence (y0, . . .",2.1 Sequence-to-sequence model,[0],[0]
", yT ′) that maximizes the conditional probability p(y0, . . .",2.1 Sequence-to-sequence model,[0],[0]
", yT ′",2.1 Sequence-to-sequence model,[0],[0]
"| x0, . . .",2.1 Sequence-to-sequence model,[0],[0]
", xT ), the sequence-to-sequence model uses one RNN to encode the source sequence into a fixed-length context vector c and a second RNN to decode this vector and generate the target sequence.",2.1 Sequence-to-sequence model,[0],[0]
"Formally, the probability of the target sequence can be calculated as follows:
p(y0, . . .",2.1 Sequence-to-sequence model,[0],[0]
",yT ′",2.1 Sequence-to-sequence model,[0],[0]
"| x0, . . .",2.1 Sequence-to-sequence model,[0],[0]
", xT )
",2.1 Sequence-to-sequence model,[0],[0]
"= T ′∏ t=0 p(yt | c, y0, . . .",2.1 Sequence-to-sequence model,[0],[0]
", yt−1), (1)
where
p(yt | c, y0, . . .",2.1 Sequence-to-sequence model,[0],[0]
", yt−1) = g(yt−1, st, c), (2) st = f(st−1, yt−1, c), (3)
c = q(h0, . . .",2.1 Sequence-to-sequence model,[0],[0]
", hT ), (4)
ht = f(et, ht−1).",2.1 Sequence-to-sequence model,[0],[0]
"(5)
Here, g, f , and q are nonlinear functions; ht and st are the hidden states of the source-side RNN and target-side RNN, respectively, c is the context vector, and et is the embedding of xt.
Bahdanau et al. (2014) introduced an attention mechanism to deal with the issues related to long sequences (Cho et al., 2014).",2.1 Sequence-to-sequence model,[0],[0]
"Instead of encoding the source sequence into a fixed vector c, the attention model uses different ci-s when calculating
the target-side output yi at time step i:
ci = T∑
j=0
αijhj , (6)
αij = exp(a(si−1, hj))∑T k=0",2.1 Sequence-to-sequence model,[0],[0]
"exp(a(si−1, hk))",2.1 Sequence-to-sequence model,[0],[0]
.,2.1 Sequence-to-sequence model,[0],[0]
"(7)
The function a(si−1, hj) can be regarded as representing the soft alignment between the target-side RNN hidden state si−1 and the source-side RNN hidden state hj .
",2.1 Sequence-to-sequence model,[0],[0]
"By changing the format of the source/target sequences, this framework can be regarded as a string-to-string NMT system (Sutskever et al., 2014), a tree-to-string NMT system (Li et al., 2017), or a string-to-tree NMT system (Aharoni and Goldberg, 2017).",2.1 Sequence-to-sequence model,[0],[0]
"Regarding the linearization adopted for tree-tostring NMT (i.e., linearization of the source side), Sennrich and Haddow (2016) encoded the sequence of dependency labels and the sequence of words simultaneously, partially utilizing the syntax information, while Li et al. (2017) traversed the constituent tree of the source sentence and combined this with the word sequence, utilizing the syntax information completely.
",2.2 Linear-structured tree-based NMT systems,[0],[0]
"Regarding the linearization used for string-totree NMT (i.e., linearization of the target side), Nadejde et al. (2017) used a CCG supertag sequence as the target sequence, while Aharoni and Goldberg (2017) applied a linearization method in a top-down manner, generating a sequence ensemble for the annotated tree in the Penn Treebank (Marcus et al., 1993).",2.2 Linear-structured tree-based NMT systems,[0],[0]
"Wu et al. (2017) used transition actions to linearize a dependency tree, and employed the sequence-to-sequence framework for NMT.
",2.2 Linear-structured tree-based NMT systems,[0],[0]
It can be seen all current tree-based NMT systems use only one tree for encoding or decoding.,2.2 Linear-structured tree-based NMT systems,[0],[0]
"In contrast, we hope to utilize multiple trees (i.e., a forest).",2.2 Linear-structured tree-based NMT systems,[0],[0]
"This is not trivial, on account of the lack of a fixed traversal order and the need for a compact representation.",2.2 Linear-structured tree-based NMT systems,[0],[0]
"The packed forest gives a representation of exponentially many parsing trees, and can compactly encode many more candidates than the n-best list
(Huang, 2008).",2.3 Packed forest,[0],[0]
"Figure 1a shows a packed forest, which can be unpacked into two constituent trees (Figure 1b and Figure 1c).
",2.3 Packed forest,[0],[0]
"Formally, a packed forest is a pair 〈V,E〉, where V is the set of nodes and E is the set of hyperedges.",2.3 Packed forest,[0],[0]
"Each v ∈ V can be represented as Xi,j , where X is a constituent label and i, j ∈",2.3 Packed forest,[0],[0]
"[0, n] are indices of words, showing that the node spans the words ranging from i (inclusive) to j (exclusive).",2.3 Packed forest,[0],[0]
"Here, n is the length of the input sentence.",2.3 Packed forest,[0],[0]
"Each e ∈ E is a three-tuple 〈head(e), tails(e), score(e)〉, where head(e) ∈ V is similar to the head node in a constituent tree, and tails(e) ∈ V ∗ is similar to the set of child nodes in a constituent tree.",2.3 Packed forest,[0],[0]
score(e) ∈ R is the logarithm of the probability that tails(e) represents the tails of head(e) calculated by the parser.,2.3 Packed forest,[0],[0]
"Based on score(e), the score of a constituent tree T can be calculated as follows:
score(T ) = −λn+",2.3 Packed forest,[0],[0]
"∑
e∈E(T )
score(e), (8)
where E(T ) is the set of hyperedges appearing in tree T , and λ is a regularization coefficient for the sentence length2.
2Following the configuration of Charniak and Johnson",2.3 Packed forest,[0],[0]
"We first propose a linearization method for the packed forest (Section 3.1), then describe how to encode the linearized forest (Section 3.2), which can then be translated by the conventional decoder (see Section 2.1).",3 Forest-based NMT,[0],[0]
"Recently, several studies have focused on the linearization methods of a syntax tree, both in the area of tree-based NMT (Section 2.2) and in the area of parsing (Vinyals et al., 2015; Dyer et al., 2016; Ma et al., 2017).",3.1 Forest linearization,[0],[0]
"Basically, these methods follow a fixed traversal order (e.g., depthfirst), which does not exist for the packed forest (a directed acyclic graph (DAG)).",3.1 Forest linearization,[0],[0]
"Furthermore, the weights are attached to edges of a packed forest instead of the nodes, which further increase the difficulty.
",3.1 Forest linearization,[0],[0]
"Topological ordering algorithms for DAG (Kahn, 1962; Tarjan, 1976) are not good solutions, because the outputted ordering is not always optimal for machine translation.",3.1 Forest linearization,[0],[0]
"In particular, a topo-
(2005), for all the experiments in this paper, we fixed λ to log2 600.
",3.1 Forest linearization,[0],[0]
"Algorithm 1 Linearization of a packed forest 1: function LINEARIZEFOREST(〈V,E〉,w) 2: v ← FINDROOT(V ) 3: r←",3.1 Forest linearization,[0],[0]
"[] 4: EXPANDSEQ(v, r, 〈V,E〉,w) 5: return r 6: function FINDROOT(V ) 7: for v ∈ V do 8: if v has no parent then 9: return v 10: procedure EXPANDSEQ(v, r, 〈V,E〉,w) 11: for e ∈ E do 12: if head(e) = v then 13: if tails(e) 6= ∅",3.1 Forest linearization,[0],[0]
then 14: for t ∈ SORT(tails(e)) do .,3.1 Forest linearization,[0],[0]
"Sort
tails(e) by word indices.",3.1 Forest linearization,[0],[0]
"15: EXPANDSEQ(t, r, 〈V,E〉,w) 16: l← LINEARIZEEDGE(head(e),w) 17: r.append(〈l, σ(0.0)〉) .",3.1 Forest linearization,[0],[0]
"σ is the sigmoid
function, i.e., σ(x) = 1 1+e−x , x ∈ R.
18:",3.1 Forest linearization,[0],[0]
"l ← c©LINEARIZEEDGES(tails(e),w) .",3.1 Forest linearization,[0],[0]
c© is a unary operator.,3.1 Forest linearization,[0],[0]
"19: r.append(〈l, σ(score(e))",3.1 Forest linearization,[0],[0]
"〉) 20: else 21: l← LINEARIZEEDGE(head(e),w) 22:",3.1 Forest linearization,[0],[0]
"r.append(〈l, σ(0.0)〉) 23: function LINEARIZEEDGE(Xi,j ,w) 24: return X ⊗",3.1 Forest linearization,[0],[0]
"( j−1k=iwk) 25: function LINEARIZEEDGES(v,w) 26: return ⊕v∈vLINEARIZEEDGE(v,w)
logical ordering could ignore “word sequential information” and “parent-child information” in the sentences.
",3.1 Forest linearization,[0],[0]
"For example, for the packed forest in Figure 1a, although “[10]→[1]→[2]→ · · · →[9]→[11]” is a valid topological ordering, the word sequential information of the words (e.g., “John” should be located ahead of the period), which is fairly crucial for translation of languages with fixed pragmatic word order such as Chinese or English, is lost.
",3.1 Forest linearization,[0],[0]
"As another example, for the packed forest in Figure 1a, nodes [2], [9], and [10] are all the children of node [11].",3.1 Forest linearization,[0],[0]
"However, in the topological order “[1]→[2]→ · · · →[9]→[10]→[11],” node [2] is quite far from node [11], while nodes [9] and [10] are both close to node [11].",3.1 Forest linearization,[0],[0]
"The parent-child information cannot be reflected in this topological order, which is not what we would expect.
",3.1 Forest linearization,[0],[0]
"To address the above two problems, we propose a novel linearization algorithm for a packed forest (Algorithm 1).",3.1 Forest linearization,[0],[0]
"The algorithm linearizes the packed forest from the root node (Line 2) to leaf nodes by calling the EXPANDSEQ procedure (Line 15) recursively, while preserving the word order in the sentence (Line 14).",3.1 Forest linearization,[0],[0]
"In this way, word sequential information is preserved.",3.1 Forest linearization,[0],[0]
"Within the
EXPANDSEQ procedure, once a hyperedge is linearized (Line 16), the tails are also linearized immediately (Line 18).",3.1 Forest linearization,[0],[0]
"In this way, parent-child information is preserved.",3.1 Forest linearization,[0],[0]
"Intuitively, different parts of constituent trees should be combined in different ways, therefore we define different operators ( c©, ⊗, ⊕, or ) to represent the relationships between different parts, so that the representations of these parts can be combined in different ways (see Section 3.2 for details).",3.1 Forest linearization,[0],[0]
"Words are concatenated by the operator “ ” with each other, a word and a constituent label is concatenated by the operator “⊗”, the linearization results of child nodes are concatenated by the operator “⊕” with each other, while the unary operator “ c©” is used to indicate that the node is the child node of the previous part.",3.1 Forest linearization,[0],[0]
"Furthermore, each token in the linearized sequence is related to a score, representing the confidence of the parser.
",3.1 Forest linearization,[0],[0]
The linearization result of the packed forest in Figure 1a is shown in Figure 2.,3.1 Forest linearization,[0],[0]
Tokens in the linearized sequence are separated by slashes.,3.1 Forest linearization,[0],[0]
Each token in the sequence is composed of different types of symbols and combined by different operators.,3.1 Forest linearization,[0],[0]
We can see that word sequential information is preserved.,3.1 Forest linearization,[0],[0]
"For example, “NNP⊗John” (linearization result of node [1]) is in front of “VBZ⊗has” (linearization result of node [3]), which is in front of “DT⊗a” (linearization result of node [4]).",3.1 Forest linearization,[0],[0]
"Moreover, parent-child information is also preserved.",3.1 Forest linearization,[0],[0]
"For example, “NP⊗John” (linearization result of node [2]) is followed by “ c©NNP⊗John” (linearization result of node [1], the child of node [2]).
",3.1 Forest linearization,[0],[0]
Note that our linearization method cannot fully recover packed forest.,3.1 Forest linearization,[0],[0]
What we want to do is not to propose a fully recoverable linearization method.,3.1 Forest linearization,[0],[0]
"What we actually want to do is to encode syntax information as much as possible, so that we can improve the performance of NMT.",3.1 Forest linearization,[0],[0]
"As will be shown in Section 4, this goal is achieved.
",3.1 Forest linearization,[0],[0]
"Also note that there is one more advantage of our linearization method: the linearized sequence
is a weighted sequence, while all the previous studies ignored the weights during linearization.",3.1 Forest linearization,[0],[0]
"As will be shown in Section 4, the weights are actually important not only for the linearization of a packed forest, but also for the linearization of a single tree.
",3.1 Forest linearization,[0],[0]
"By preserving only the nodes and hyperedges in the 1-best tree and removing all others, our linearization method can be regarded as a treelinearization method.",3.1 Forest linearization,[0],[0]
"Compared with other treelinearization methods, our method combines several different kinds of information within one symbol, retaining the parent-child information, and incorporating the confidence of the parser in the sequence.",3.1 Forest linearization,[0],[0]
"We examine whether the weights can be useful not only for linear structured tree-based NMT but also for our forest-based NMT.
",3.1 Forest linearization,[0],[0]
"Furthermore, although our method is nonreversible for packed forests, it is reversible for constituent trees, in that the linearization is processed exactly in the depth-first traversal order and all necessary information in the tree nodes has been encoded.",3.1 Forest linearization,[0],[0]
"As far as we know, there is no previous work on linearization of packed forests.",3.1 Forest linearization,[0],[0]
"The linearized packed forest forms the input of the encoder, which has two major differences from the input of a sequence-to-sequence NMT system.",3.2 Encoding the linearized forest,[0],[0]
"First, the input sequence of the encoder consists of two parts: the symbol sequence and the score sequence.",3.2 Encoding the linearized forest,[0],[0]
"Second, each symbol in the symbol sequence consists of several parts (words and constituent labels), which are combined by certain operators ( c©, ⊗, ⊕, or ).",3.2 Encoding the linearized forest,[0],[0]
"Based on these observa-
tions, we propose two new frameworks, which are illustrated in Figure 3.
",3.2 Encoding the linearized forest,[0],[0]
"Formally, the input layer receives the sequence (〈l0, ξ0〉, . . .",3.2 Encoding the linearized forest,[0],[0]
", 〈lT , ξT 〉), where li denotes the i-th symbol and ξi its score.",3.2 Encoding the linearized forest,[0],[0]
"Then, the sequence is fed into the score layer and the symbol layer.",3.2 Encoding the linearized forest,[0],[0]
"The score and symbol layers receive the sequence and output the score sequence ξ = (ξ0, . . .",3.2 Encoding the linearized forest,[0],[0]
", ξT ) and symbol sequence",3.2 Encoding the linearized forest,[0],[0]
"l = (l0, . . .",3.2 Encoding the linearized forest,[0],[0]
", lT ), respectively, from the input.",3.2 Encoding the linearized forest,[0],[0]
Any item l ∈,3.2 Encoding the linearized forest,[0],[0]
"l in the symbol layer has the form
l = o0x1o1 . . .",3.2 Encoding the linearized forest,[0],[0]
"xm−1om−1xm, (9)
where each xk (k = 1, . . .",3.2 Encoding the linearized forest,[0],[0]
",m) is a word or a constituent label, m is the total number of words and constituent labels in a symbol, o0 is “ c©” or empty, and each ok (k = 1, . . .",3.2 Encoding the linearized forest,[0],[0]
",m − 1) is either “⊗”, “⊕”, or “ ”.",3.2 Encoding the linearized forest,[0],[0]
"Then, in the node/operator layer, the x-s and o-s are separated and rearranged as x = (x1, . . .",3.2 Encoding the linearized forest,[0],[0]
", xm, o0, . . .",3.2 Encoding the linearized forest,[0],[0]
", om−1), which is fed to the pre-embedding layer.",3.2 Encoding the linearized forest,[0],[0]
"The pre-embedding layer generates a sequence p = (p1, . .",3.2 Encoding the linearized forest,[0],[0]
.,3.2 Encoding the linearized forest,[0],[0]
", pm, . . .",3.2 Encoding the linearized forest,[0],[0]
", p2m), which is calculated as follows:
p =Wemb[I(x)].",3.2 Encoding the linearized forest,[0],[0]
"(10)
Here, the function I(x) returns a list of the indices in the dictionary for all the elements in x, which consist of words, constituent labels, or operators.",3.2 Encoding the linearized forest,[0],[0]
"In addition, Wemb is the embedding matrix of size (|wword| + |wlabel| + 4) × dword, where |wword| and |wlabel| are the total number of words and constituent labels, respectively, dword is the dimension of the word embedding, and there are four possible operators: “",3.2 Encoding the linearized forest,[0],[0]
"c©,” “⊗,” “⊕,” and “ .”",3.2 Encoding the linearized forest,[0],[0]
"Note
that p is a list of 2m vectors, and the dimension of each vector is dword.
",3.2 Encoding the linearized forest,[0],[0]
"Because the length of the sequence of the input layer is T + 1, there are T + 1 different ps in the pre-embedding layer, which we denote by P = (p0, . . .",3.2 Encoding the linearized forest,[0],[0]
",pT ).",3.2 Encoding the linearized forest,[0],[0]
"Depending on where the score layer is incorporated, we propose two frameworks: Score-on-Embedding (SoE) and Score-onAttention (SoA).",3.2 Encoding the linearized forest,[0],[0]
"In SoE, the k-th element of the embedding layer is calculated as follows:
ek = ξk ∑ p∈pk p, (11)
while in SoA, the k-th element of the embedding layer is calculated as
ek = ∑ p∈pk p, (12)
where k = 0, . . .",3.2 Encoding the linearized forest,[0],[0]
", T .",3.2 Encoding the linearized forest,[0],[0]
Note that ek ∈ Rdword .,3.2 Encoding the linearized forest,[0],[0]
"In this manner, the proposed forest-to-string NMT framework is connected with the conventional sequence-to-sequence NMT framework.
",3.2 Encoding the linearized forest,[0],[0]
"After calculating the embedding vectors in the embedding layer, the hidden vectors are calculated using Equation 5.",3.2 Encoding the linearized forest,[0],[0]
"When calculating the context vector ci-s, SoE and SoA differ from each other.",3.2 Encoding the linearized forest,[0],[0]
"For SoE, the ci-s are calculated using Equation 6 and 7, while for SoA, the αij-s used to calculate the ci-s are determined as follows:
αij = exp(ξja(si−1, hj))∑T k=0",3.2 Encoding the linearized forest,[0],[0]
"exp(ξka(si−1, hk)) .",3.2 Encoding the linearized forest,[0],[0]
"(13)
Then, using the decoder of the sequence-tosequence framework, the sentence of the target language can be generated.",3.2 Encoding the linearized forest,[0],[0]
We evaluate the effectiveness of our forest-based NMT systems on English-to-Chinese and Englishto-Japanese translation tasks3.,4.1 Setup,[0],[0]
"The statistics of the corpora used in our experiments are summarized in Table 1.
",4.1 Setup,[0],[0]
The packed forests of English sentences are obtained by the constituent parser proposed by Huang (2008)4.,4.1 Setup,[0],[0]
"We filtered out the sentences for
3English is commonly chosen as the target language.",4.1 Setup,[0],[0]
"We chose English as the source language because a highperformance forest parser is not available for other languages.
4http://web.engr.oregonstate.edu/ ˜huanlian/software/forest-reranker/ forest-charniak-v0.8.tar.bz2
which the parser cannot generate the packed forest successfully and the sentences longer than 80 words.",4.1 Setup,[0],[0]
"For NIST datasets, we simply choose the first reference among the four English references of NIST corpora, because all of them are independent with each other, according to the documents of NIST datasets.",4.1 Setup,[0],[0]
"For Chinese sentences, we used Stanford segmenter5 for segmentation.",4.1 Setup,[0],[0]
"For Japanese sentences, we followed the preprocessing steps recommended in WAT 20176.
",4.1 Setup,[0],[0]
"We implemented our framework based on nematus8 (Sennrich et al., 2017).",4.1 Setup,[0],[0]
"For optimization, we used the Adadelta algorithm (Zeiler, 2012).",4.1 Setup,[0],[0]
"In order to avoid overfitting, we used dropout (Srivastava et al., 2014) on the embedding layer and hidden layer, with the dropout probability set to 0.2.",4.1 Setup,[0],[0]
"We used the gated recurrent unit (Cho et al., 2014) as the recurrent unit of RNNs, which are bi-directional, with one hidden layer.
",4.1 Setup,[0],[0]
"Based on the tuning result, we set the maximum length of the input sequence to 300, the hidden layer size as 512, the dimension of word embedding as 620, and the batch size for training as 40.",4.1 Setup,[0],[0]
"We pruned the packed forest using the algorithm of Huang (2008), with a threshold of 5.",4.1 Setup,[0],[0]
"If the linearization of the pruned forest is still longer than 300, then we linearize the 1-best parsing tree instead of the forest.",4.1 Setup,[0],[0]
"During decoding, we used beam search, and fixed the beam size to 12.",4.1 Setup,[0],[0]
"For the case of Forest (SoA), with 1 core of Tesla K80 GPU and LDC corpus as the training data, training spent about 10 days, and decoding speed is about 10 sentences per second.
",4.1 Setup,[0],[0]
"5https://nlp.stanford.edu/software/ stanford-segmenter-2017-06-09.zip
6http://lotus.kuee.kyoto-u.ac.jp/WAT/ WAT2017/baseline/dataPreparationJE.html
7LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08, and LDC2005T06
8https://github.com/EdinburghNLP/ nematus",4.1 Setup,[0],[0]
Table 2 and 3 summarize the experimental results.,4.2 Experimental results,[0],[0]
"To avoid the affect of segmentation errors, the performance were evaluated by character-level BLEU (Papineni et al., 2002).",4.2 Experimental results,[0],[0]
"We compare our proposed models (i.e., Forest (SoE) and Forest (SoA)) with three types of baseline: a string-to-string model (s2s), forest-based models that do not use score sequences (Forest (No score)), and tree-based models that use the 1-best parsing tree (1-best (No score, SoE, SoA)).",4.2 Experimental results,[0],[0]
"For the 1-best models, we preserve the nodes and hyperedges that are used in the 1-best constituent tree in the packed forest, and remove all other nodes and hyperedges, yielding a pruned forest that contains only the 1-best constituent tree.",4.2 Experimental results,[0],[0]
"For the “No score” configurations, we force the input score sequence to be a sequence of 1.0 with the same length as the input symbol sequence, so that neither the embedding layer nor the attention layer are affected by the score sequence.
",4.2 Experimental results,[0],[0]
"In addition, we also perform a comparison with some state-of-the-art tree-based systems that are
publicly available, including an SMT system (Mi et al., 2008) and the NMT systems (Eriguchi et al. (2016)9, Chen et al. (2017)10, and Li et al. (2017)).",4.2 Experimental results,[0],[0]
"For Mi et al. (2008), we use the implementation of cicada11.",4.2 Experimental results,[0],[0]
"For Li et al. (2017), we reimplemented the “Mixed RNN Encoder” model, because of its outstanding performance on the NIST MT corpus.
",4.2 Experimental results,[0],[0]
"We can see that for both English-Chinese and English-Japanese, compared with the s2s baseline system, both the 1-best and forest-based configurations yield better results.",4.2 Experimental results,[0],[0]
This indicates syntactic information contained in the constituent trees or forests is indeed useful for machine translation.,4.2 Experimental results,[0],[0]
"Specifically, we observe the following facts.
",4.2 Experimental results,[0],[0]
"First, among the three different frameworks SoE, SoA, and No-score, the SoA framework performs the best, while the No-score framework per-
9https://github.com/tempra28/tree2seq 10https://github.com/howardchenhd/
Syntax-awared-NMT 11https://github.com/tarowatanabe/ cicada
forms the worst.",4.2 Experimental results,[0],[0]
"This indicates that the scores of the edges in constituent trees or packed forests, which reflect the confidence of the correctness of the edges, are indeed useful.",4.2 Experimental results,[0],[0]
"In fact, for the 1-best constituent parsing tree, the score of the edge reflects the confidence of the parser.",4.2 Experimental results,[0],[0]
"By using this information, the NMT system succeed to learn a better attention, paying much attention to the confident structure and not paying attention to the unconfident structure, which improved the translation performance.",4.2 Experimental results,[0],[0]
This fact is ignored by previous studies on tree-based NMT.,4.2 Experimental results,[0],[0]
"Furthermore, it is better to use the scores to modify the values of attention instead of rescaling the word embeddings, because modifying word embeddings carelessly may change the semantic meanings of words.
",4.2 Experimental results,[0],[0]
"Second, compared with the cases that only using the 1-best constituent trees, using packed forests yields statistical significantly better results for the SoE and SoA frameworks.",4.2 Experimental results,[0],[0]
This shows the effectiveness of using more syntactic information.,4.2 Experimental results,[0],[0]
"Compared with one constituent tree, the packed forest, which contains multiple different trees, describes the syntactic structure of the sentence in different aspects, which together increase the accuracy of machine translation.",4.2 Experimental results,[0],[0]
"However, without using the scores, the 1-best constituent tree is preferred.",4.2 Experimental results,[0],[0]
"This is because without using the scores, all trees in the packed forest are treated equally, which makes it easy to import noise into the encoder.
",4.2 Experimental results,[0],[0]
"Compared with other types of state-of-the-art systems, our systems using only the 1-best tree (1-best(SoE, SoA)) are better than the other treebased systems.",4.2 Experimental results,[0],[0]
"Moreover, our NMT systems using the packed forests achieve the best performance.",4.2 Experimental results,[0],[0]
"These results also support the usefulness of the scores of the edges and packed forests in NMT.
",4.2 Experimental results,[0],[0]
"As for the efficiency, the training time of the SoA system was slightly longer than that of the SoE system, which was about twice of the s2s baseline.",4.2 Experimental results,[0],[0]
The training time of the tree-based system was about 1.5 times of the baseline.,4.2 Experimental results,[0],[0]
"For the
case of Forest (SoA), with 1 core of Tesla P100 GPU and LDC corpus as the training data, training spent about 10 days, and decoding speed was about 10 sentences per second.",4.2 Experimental results,[0],[0]
"The reason for the relatively low efficiency is that the linearized sequences of packed forests were much longer than word sequences, enlarging the scale of the inputs.",4.2 Experimental results,[0],[0]
"Despite this, the training process ended within reasonable time.",4.2 Experimental results,[0],[0]
"Figure 4 illustrates the translation results of an English sentence using several different configurations: the s2s baseline, using only the 1-best tree (SoE), and using the packed forest (SoE).",4.3 Qualitative analysis,[0],[0]
"This is a sentence from NIST MT 03, and the training corpus is the LDC corpus.
",4.3 Qualitative analysis,[0],[0]
"For the s2s case, no syntactic information is utilized, and therefore the output of the system is not a grammatical Chinese sentence.",4.3 Qualitative analysis,[0],[0]
The attributive phrase of “Czech border region” is a complete sentence.,4.3 Qualitative analysis,[0],[0]
"However, the attributive is not allowed to be a complete sentence in Chinese.
",4.3 Qualitative analysis,[0],[0]
"For the case of using 1-best constituent tree, the output is a grammatical Chinese sentence.",4.3 Qualitative analysis,[0],[0]
"However, the phrase “adjacent to neighboring Slovakia” is completely ignored in the translation result.",4.3 Qualitative analysis,[0],[0]
"After analyzing the constituent tree, we found that this phrase was incorrectly parsed as an “adverb phrase”, so that the NMT system paid little attention to it, because of the low confidence given by the parser.
",4.3 Qualitative analysis,[0],[0]
"In contrast, for the case of the packed forest, we can see this phrase was not ignored and was translated correctly.",4.3 Qualitative analysis,[0],[0]
"Actually, besides “adverb phrase”, this phrase was also correctly parsed as an “adjective phrase”, and covered by multiple different nodes in the forest, making it difficult for the encoder to ignore the phrase.
",4.3 Qualitative analysis,[0],[0]
We also noticed that our method performed better on learning attention.,4.3 Qualitative analysis,[0],[0]
"For the example in Figure 4, we observed that for s2s model, the decoder paid attention to the word “Czech” twice, which
causes the output sentence contains the Chinese translation of Czech twice.",4.3 Qualitative analysis,[0],[0]
"On the other hand, for our forest model, by using the syntax information, the decoder paid attention to the phrase “In the Czech Republic” only once, making the decoder generates the correct output.",4.3 Qualitative analysis,[0],[0]
Incorporating syntactic information into NMT systems is attracting widespread attention nowadays.,5 Related work,[0],[0]
"Compared with conventional string-to-string NMT systems, tree-based systems demonstrate a better performance with the help of constituent trees or dependency trees.
",5 Related work,[0],[0]
"The first noteworthy study is Eriguchi et al. (2016), which used Tree-structured LSTM (Tai et al., 2015) to encode the HPSG syntax tree of the sentence in the source-side in a bottom-up manner.",5 Related work,[0],[0]
"Then, Chen et al. (2017) enhanced the encoder with a top-down tree encoder.
",5 Related work,[0],[0]
"As a simple extension of Eriguchi et al. (2016), very recently, Zaremoodi and Haffari (2017) proposed a forest-based NMT method by representing the packed forest with a forest-structured neural network.",5 Related work,[0],[0]
"However, their method was evaluated in small-scale MT settings (each training dataset consists of under 10k parallel sentences).",5 Related work,[0],[0]
"In contrast, our proposed method is effective in a largescale MT setting, and we present qualitative analysis regarding the effectiveness of using forests in NMT.
",5 Related work,[0],[0]
"Although these methods obtained good results, the tree-structured network used by the encoder made the training and decoding relatively slow, therefore restricts the scope of application.
",5 Related work,[0],[0]
Other attempts at encoding syntactic trees have also been proposed.,5 Related work,[0],[0]
"Eriguchi et al. (2017) combined the Recurrent Neural Network Grammar (Dyer et al., 2016) with NMT systems, while Li et al. (2017) linearized the constituent tree and encoded it using RNNs.",5 Related work,[0],[0]
"The training of these methods is fast, because of the linear structures of RNNs.",5 Related work,[0],[0]
"However, all these syntax-based NMT systems used only the 1-best parsing tree, making the systems sensitive to parsing errors.
",5 Related work,[0],[0]
"Instead of using trees to represent syntactic information, some studies use other data structures to represent the latent syntax of the input sentence.",5 Related work,[0],[0]
"For example, Hashimoto and Tsuruoka (2017) proposed translating using a latent graph.",5 Related work,[0],[0]
"However, such systems do not enjoy the benefit of
handcrafted syntactic knowledge, because they do not use a parser trained from a large treebank with human annotations.
",5 Related work,[0],[0]
"Compared with these related studies, our framework utilizes a linearized packed forest, meaning the encoder can encode exponentially many trees in an efficient manner.",5 Related work,[0],[0]
The experimental results demonstrated these advantages.,5 Related work,[0],[0]
"We proposed a new NMT framework, which encodes a packed forest for the source sentence using linear-structured neural networks, such as RNN.",6 Conclusion and future work,[0],[0]
"Compared with conventional string-tostring NMT systems and tree-to-string NMT systems, our framework can utilize exponentially many linearized parsing trees during encoding, without significantly decreasing the efficiency.",6 Conclusion and future work,[0],[0]
This represents the first attempt at using a forest under the string-to-string NMT framework.,6 Conclusion and future work,[0],[0]
"The experimental results demonstrate the effectiveness of our framework.
",6 Conclusion and future work,[0],[0]
"As future work, we plan to design some more elaborate structures to incorporate the score layer in the encoder.",6 Conclusion and future work,[0],[0]
Further improvement in the translation performance is expected to be achieved for the forest-based NMT system.,6 Conclusion and future work,[0],[0]
We will also apply the proposed linearization method to other tasks.,6 Conclusion and future work,[0],[0]
We are grateful to the anonymous reviewers for their insightful comments and suggestions.,Acknowledgements,[0],[0]
We thank Lemao Liu from Tencent AI Lab for his suggestions about the experiments.,Acknowledgements,[0],[0]
We thank Atsushi Fujita whose suggestions greatly improve the readability and the logical soundness of this paper.,Acknowledgements,[0],[0]
This work was done during the internship of Chunpeng Ma at NICT.,Acknowledgements,[0],[0]
Akihiro Tamura is supported by JSPS KAKENHI Grant Number JP18K18110.,Acknowledgements,[0],[0]
Tiejun Zhao is supported by the National Natural Science Foundation of China (NSFC) via grant 91520204 and State High-Tech Development Plan of China (863 program) via grant 2015AA015405.,Acknowledgements,[0],[0]
"Tree-based neural machine translation (NMT) approaches, although achieved impressive performance, suffer from a major drawback: they only use the 1best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors.",abstractText,[0],[0]
"For statistical machine translation (SMT), forestbased methods have been proven to be effective for solving this problem, while for NMT this kind of approach has not been attempted.",abstractText,[0],[0]
"This paper proposes a forest-based NMT method that translates a linearized packed forest under a simple sequence-to-sequence framework (i.e., a forest-to-string NMT model).",abstractText,[0],[0]
"The BLEU score of the proposed method is higher than that of the string-to-string NMT, treebased NMT, and forest-based SMT systems.",abstractText,[0],[0]
Forest-Based Neural Machine Translation,title,[0],[0]
"Since its development by Breiman (2001), random forest has proven to be both accurate and efficient for classification and regression problems.",1. Introduction,[0],[0]
"In regression setting, random forest will predict the conditional mean of a response variable by averaging predictions of a large number of regression trees.",1. Introduction,[0],[0]
"Later then, many other machine learning algorithms were developed upon random forest.",1. Introduction,[0],[0]
"Among them, robust versions of random forest have also been proposed using various methodologies.",1. Introduction,[0],[0]
"Besides the sampling idea (Breiman, 2001) which adds extra randomness, the other variations are mainly based on two ideas: (1) use more robust criterion to construct regression trees (Galimberti et al., 2007; Brence & Brown, 2006; Roy & Larocque, 2012); (2) choose more robust aggregation method (Meinshausen, 2006; Roy & Larocque, 2012; Tsymbal et al., 2006).
",1. Introduction,[0],[0]
"Meinshausen (2006) generalized random forest to pre-
1University of California at San Diego, San Diego, California, USA 2Zillow, Seattle, Washington, USA.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Alexander Hanbo Li <alexanderhanboli@gmail.com>.
Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"dict quantiles by discovering that besides calculating the weighted mean of the observed response variables, one could also get information for the weighted distribution of observed response variables using the sets of local weights generated by random forest.",1. Introduction,[0],[0]
"This method is strongly connected to the adaptive nearest neighbors procedure (Lin & Jeon, 2006) which we will briefly review in section 1.2.",1. Introduction,[0],[0]
"Different from classical k-NN methods that rely on predefined distance metrics, the dissimilarities generated by random forest are data dependent and scale-invariant.
",1. Introduction,[0],[0]
"Another state-of-the-art algorithm AdaBoost (Freund & Schapire, 1995; Freund et al., 1996) has been generalized to be applicable to a large family of loss functions (Friedman, 2001; Mason et al., 1999; Li & Bradic, 2016).",1. Introduction,[0],[0]
"Recent development of more flexible boosting algorithms such as xgboost (Chen & Guestrin, 2016) have become the go-to forest estimators with tabular or matrix data.",1. Introduction,[0],[0]
"One way in which recent boosting algorithms have an advantage over the random forest is the ability to customize the loss function used to reduce the influence of outliers or optimize a metric more suited to the specific problem other than the mean squared error.
",1. Introduction,[0],[0]
"In this paper, we will propose a general framework for forest-type regression which can also be applied to a broad family of loss functions.",1. Introduction,[0],[0]
"It is claimed in (Meinshausen, 2006) that quantile random forest is another nonparametric approach which does not minimize an empirical loss.",1. Introduction,[0],[0]
"However, we will show in fact both random forest and quantile random forest estimators can be re-derived as regression methods using the squared error or quantile loss respectively in our framework.",1. Introduction,[0],[0]
"Inspired by the adaptive nearest neighbor viewpoint, we explore how random forest makes predictions using the local weights generated by ensemble of trees, and connect that with locally weighted regression (Fan & Gijbels, 1996; Tibshirani & Hastie, 1987; Staniswalis, 1989; Newey, 1994; Loader, 2006; Hastie & Loader, 1993).",1. Introduction,[0],[0]
"The intuition is that when predicting the target value (e.g. E[Y |X = x]) at point x, the observations closer to x should receive larger weights.",1. Introduction,[0],[0]
"Different from predefining a kernel, random forest assigns the weights data dependently and adaptively.",1. Introduction,[0],[0]
"After we illustrate the relation between random forest and local regression, we will use random forest weights to design other regression algo-
rithms.",1. Introduction,[0],[0]
"By plugging robust loss functions like Huber loss and Tukey’s redescending loss, we get forest-type regression methods that are more robust to outliers.",1. Introduction,[0],[0]
"Finally, motivated from the truncated squared error loss example, we will show that decreasing the number of nearest neighbors in random forest will also immediately improve its generalization performance.
",1. Introduction,[0],[0]
The layout of this paper is as follows.,1. Introduction,[0],[0]
In Section 1.1 and 1.2 we review random forest and adaptive nearest neighbors.,1. Introduction,[0],[0]
Section 2 introduces the general framework of forest-type regression.,1. Introduction,[0],[0]
In Section 3 we plug in robust regression loss functions to get robust forest algorithms.,1. Introduction,[0],[0]
In Section 4 we motivate from the truncated squared error loss and investigate the importance of choosing right number of nearest neighbors.,1. Introduction,[0],[0]
"Finally, we test our robust forests in Section 5 and show that they are always superior to the traditional formulation in the presence of outliers in both synthetic and real data set.",1. Introduction,[0],[0]
"Following the notation of Breiman (2001), let θ be the random parameter determining how a tree is grown, and data (X,Y ) ∈ X × Y .",1.1. Random forest,[0],[0]
"For each tree T (θ), let L be the total number of leaves, and Rl denotes the rectangular subspace in X corresponding to the l-th leaf.",1.1. Random forest,[0],[0]
"Then for every x ∈ X , there is exactly one leaf l",1.1. Random forest,[0],[0]
such that x ∈ Rl.,1.1. Random forest,[0],[0]
"Denote this leaf by l(x, θ).
",1.1. Random forest,[0],[0]
"For each tree T (θ), the prediction of a new data point X = x is the average of data values in leaf l(x, θ), that is, Ŷ (x, θ) = ∑n j=1 w(Xi, x, θ)Yi, where
w(Xi, x, θ) = 1I{Xi∈Rl(x,θ)}
#{j : Xj ∈ Rl(x,θ)} .",1.1. Random forest,[0],[0]
"(1)
Finally, the conditional mean E[Y |X = x] is approximated by the averaged prediction of m trees, Ŷ (x) = m−1 ∑m t=1",1.1. Random forest,[0],[0]
"Ŷ (x, θt).",1.1. Random forest,[0],[0]
"After rearranging the terms, we can write the prediction of random forest as
Ŷ (x) = n∑ i=1 w(Xi, x)Yi, (2)
where the averaged weight w(Xi, x) is defined as
w(Xi, x) = 1
m m∑ t=1 w(Xi, x, θt).",1.1. Random forest,[0],[0]
"(3)
From equation (2), the prediction of the conditional expectation E[Y |X = x] is the weighted average of the response values of all observations.",1.1. Random forest,[0],[0]
"Furthermore, it is easy to show that ∑n i=1 w(Xi, x) = 1.",1.1. Random forest,[0],[0]
Lin and Jeon (2006) studies the connection between random forest and adaptive nearest neighbor.,1.2. Adaptive nearest neighbors,[0],[0]
"They introduced the so-called potential nearest neighbors (PNN): A sample point xi is called a k-PNN to a target point x if there exists a monotone distance metric under which xi is among the k closest to x among all the sample points.
",1.2. Adaptive nearest neighbors,[0],[0]
"Therefore, any k-NN method can be viewed as choosing k points from the k-PNNs according to some monotone metric.",1.2. Adaptive nearest neighbors,[0],[0]
"For example, under Euclidean metric, the classical k-NN algorithm sorts the observations by their Euclidean distances to the target point and outputs the k closest ones.",1.2. Adaptive nearest neighbors,[0],[0]
"This is equivalent to weighting the k-PNNs using inverse L2 distance.
",1.2. Adaptive nearest neighbors,[0],[0]
"More interestingly, they prove that those observations with positive weights (3) all belong to the k-PNNs (Lin & Jeon, 2006).",1.2. Adaptive nearest neighbors,[0],[0]
"Therefore, random forests is another weighted kPNN method, but it assigns weights to the observations different from any k-NN method under a pre-defined monotonic distance metric.",1.2. Adaptive nearest neighbors,[0],[0]
"In fact, the random forest weights are adaptive to the data if the splitting scheme is adaptive.",1.2. Adaptive nearest neighbors,[0],[0]
"In this section, we generalize the classical random forest to a general forest-type regression (FTR) framework which is applicable to a broad family of loss functions.",2. General framework for forest-type regression,[0],[0]
"In Section 2.1, we motivate the framework by connecting random forest predictor with locally weighted regression.",2. General framework for forest-type regression,[0],[0]
"Then in Section 2.2, we formally propose the new forest-type regression framework.",2. General framework for forest-type regression,[0],[0]
"In Section 2.3, we rediscover the quantile random forest estimator by plugging the quantile loss function into our framework.",2. General framework for forest-type regression,[0],[0]
Classical random forest can be understood as an estimator of conditional mean E[Y |X].,2.1. Squared error and random forest,[0],[0]
"As shown in (2), the estimator Ŷ (x) is weighted average of all response",2.1. Squared error and random forest,[0],[0]
Yi’s.,2.1. Squared error and random forest,[0],[0]
"This special form reminds us of the classical least squares regression, where the estimator is the sample mean.",2.1. Squared error and random forest,[0],[0]
"To be more precise, we rewrite (2) as
n∑ i=1 w(Xi, x)(Yi",2.1. Squared error and random forest,[0],[0]
− Ŷ (x)),2.1. Squared error and random forest,[0],[0]
"= 0. (4)
Equation (4) is the estimating equation (first order condition) of the locally weighted least squares regression (Ruppert & Wand, 1994):
Ŷ (x) = argmin λ∈R n∑ i=1 w(Xi, x)(Yi",2.1. Squared error and random forest,[0],[0]
"− λ)2 (5)
In classical local regression, the weight w(Xi, x) serves as a local metric between the target point x and observation Xi.",2.1. Squared error and random forest,[0],[0]
"Intuitively, observations closer to target x should be given more weights when predicting the response at x. One common choice of such local metric is kernel Kh(Xi, x) = K((Xi − x)/h).",2.1. Squared error and random forest,[0],[0]
"For example, the tricube kernel K(u) =",2.1. Squared error and random forest,[0],[0]
(1 − |u|3)3 1I(|u| ≤ 1) will ignore the impact of observations outside a window centered at x and increase the weight of an observation when it is getting closer to x.,2.1. Squared error and random forest,[0],[0]
"The form of kernel-type local regression is as follows:
argmin λ∈R n∑ i=1 Kh(Xi",2.1. Squared error and random forest,[0],[0]
− x)(Yi,2.1. Squared error and random forest,[0],[0]
"− λ)2,
The random forest weight w(Xi, x) (3) defines a similar data dependent metric, which is constructed using the ensemble of regression trees.",2.1. Squared error and random forest,[0],[0]
"Using an adaptive splitting scheme, each tree chooses the most informative predictors from those at its disposal.",2.1. Squared error and random forest,[0],[0]
"The averaging process then assigns positive weights to these training responses, which are called voting points in (Lin & Jeon, 2006).",2.1. Squared error and random forest,[0],[0]
"Hence via the random forest voting mechanism, those observations close to the target point get assigned positive weights equivalent to a kernel functionality (Friedman et al., 2001).",2.1. Squared error and random forest,[0],[0]
"Note that the formation (5) is just a special case when using squared error loss φ(a, b) = (a−b)2.",2.2. Extension to general loss,[0],[0]
"In more general form, we have the following local regression problem:
Ŷ (x) = argmin s∈F n∑ i=1 w(Xi, x)φ(s(Xi), Yi) (6)
where w(Xi, x) is a local weight, F is a family of functions, and φ(·) is a general loss.",2.2. Extension to general loss,[0],[0]
"For example, when local weight is a kernel and F stands for polynomials of a certain degree, it reduces to local polynomial regression (Fan & Gijbels, 1996).",2.2. Extension to general loss,[0],[0]
"Random forest falls into this framework with squared error loss, a family of constant functions and local weights (3) constructed from ensemble of trees.
",2.2. Extension to general loss,[0],[0]
"Algorithm 1 Forest-type regression Step 1: Calculate local weights w(Xi, x) using ensemble or trees.",2.2. Extension to general loss,[0],[0]
"Step 2: Choose a loss φ(·, ·) and a family F of function.",2.2. Extension to general loss,[0],[0]
"Then do the locally weighted regression
Ŷ (x) = argmin s∈F n∑ i=1 w(Xi, x)φ(Yi, s(Xi)).
",2.2. Extension to general loss,[0],[0]
"In Algorithm 1, we summarize the forest-type regression as a general two-step method.",2.2. Extension to general loss,[0],[0]
"Note that here we only focus on local weights generated by random forest, which
uses ensemble of trees to recursively partition the covariate space X .",2.2. Extension to general loss,[0],[0]
"However, there are many other data dependent dissimilarity measures that can potentially be used, such as k-NN, mp-dissimilarity (Aryal et al., 2014), shared nearest neighbors (Jarvis & Patrick, 1973), information-based similarity (Lin et al., 1998), mass-based dissimilarity (Ting et al., 2016), etc.",2.2. Extension to general loss,[0],[0]
And there are many other domain specific dissimilarity measures.,2.2. Extension to general loss,[0],[0]
"To avoid distraction, we will only use random forest weights throughout the rest of this paper.",2.2. Extension to general loss,[0],[0]
Meinshausen (2006) proposed the quantile random forest which can extract the information of different quantiles rather than just predicting the average.,2.3. Quantile loss and quantile random forest,[0],[0]
"It has been shown that quantile random forest is more robust than the classical random forest (Meinshausen, 2006; Roy & Larocque, 2012).",2.3. Quantile loss and quantile random forest,[0],[0]
"In this section, we show quantile random forest estimator is also a special case of Algorithm 1.",2.3. Quantile loss and quantile random forest,[0],[0]
It is well known that the τ -th quantile of an (empirical) distribution is the constant that minimizes the (empirical) risk using τ - th quantile loss function,2.3. Quantile loss and quantile random forest,[0],[0]
"ρτ (z) = z(τ −1I{z<0}) (Koenker, 2005).",2.3. Quantile loss and quantile random forest,[0],[0]
"Now let the loss function in Algorithm 1 be the quantile loss ρτ (·), F be the family of constant functions, and w(Xi, x) be random forest weights (3).",2.3. Quantile loss and quantile random forest,[0],[0]
"Solving the optimization problem
Ŷτ (x) = argmin λ∈R n∑ i=1 w(Xi, x)ρτ (Yi − λ),
we get the corresponding first order condition
n∑ i=1 w(Xi, x)(τ",2.3. Quantile loss and quantile random forest,[0],[0]
"− 1I {Yi − Ŷτ (x) < 0}) = 0.
Recall that ∑n i=1 w(Xi, x) = 1, hence, we have
n∑ i=1 w(Xi, x) 1I {Yi < Ŷτ",2.3. Quantile loss and quantile random forest,[0],[0]
"(x)} = τ. (7)
The estimator Ŷτ (x) in (7) is exactly the same estimator proposed in (Meinshausen, 2006).",2.3. Quantile loss and quantile random forest,[0],[0]
"In particular, when τ = 0.5, the equation ∑n i=1 w(Xi, x) 1I {Yi < Ŷ0.5(x)} = 0.5 will give us the median estimator Ŷ0.5(x).",2.3. Quantile loss and quantile random forest,[0],[0]
"Therefore, we have rediscovered quantile random forest from a totally different point of view as a local regression estimator with quantile loss function and random forest weights.",2.3. Quantile loss and quantile random forest,[0],[0]
"From the framework 1, quantile random forest is insensitive to outliers because of the more robust loss function.",3. Robust forest,[0],[0]
"In this section, we test our framework on other robust losses and proposed fixed-point method to solve the estimating
equation.",3. Robust forest,[0],[0]
"In Section 3.1 we choose the famous robust loss – (pseudo) Huber loss, and in Section 3.2, we further investigate a non-convex loss – Tukey’s biweight.",3. Robust forest,[0],[0]
"The Huber loss (Huber et al., 1964)
Hδ(y) =
{ 1 2y
2 for |y| ≤ δ, δ(|y| − 12δ) elsewhere
is a well-known loss function used in robust regression.",3.1. Huber loss,[0],[0]
"The penalty acts like squared error loss when the error is within [−δ, δ] but becomes linear outside this range.",3.1. Huber loss,[0],[0]
"In this way, it will penalize the outliers more lightly but still preserves more efficiency than absolute deviation when data is concentrated in the center and has light tails (e.g. Normal).",3.1. Huber loss,[0],[0]
"By plugging Huber loss into the FTR framework 1, we get a robust counterpart of random forest.",3.1. Huber loss,[0],[0]
"The estimating equation is
n∑ i=1 wi(x) sign(Ŷ (x)− Yi) min(Ŷ",3.1. Huber loss,[0],[0]
"(x)− Yi, δ) = 0.",3.1. Huber loss,[0],[0]
"(8)
Direct optimization of (8) with local weights is hard, hence instead we will investigate the pseudo-Huber loss (see Figure 1),
Lδ(y) = δ 2 (√ 1 + (y δ )2 − 1 ) which is a smooth approximation of Huber loss (Charbonnier et al., 1997).",3.1. Huber loss,[0],[0]
"The estimating equation
n∑ i=1",3.1. Huber loss,[0],[0]
wpHi (x) ( ŶpH(x)− Yi ) = 0.,3.1. Huber loss,[0],[0]
"(9)
is very similar to that of square error loss if we define a new weight
wpHi (x) = wi(x)√
1 + ( ŶpH(x)−Yi
δ )2 .",3.1. Huber loss,[0],[0]
"(10) Then the (pseudo) Huber estimator can be expressed as
ŶpH(x) =
∑n i=1",3.1. Huber loss,[0],[0]
"w
pH i (x)Yi∑n
i=1",3.1. Huber loss,[0],[0]
w pH,3.1. Huber loss,[0],[0]
"i (x)
.",3.1. Huber loss,[0],[0]
"(11)
Informally, the estimator (11) can be viewed as a weighted average of all the responses Yi’s.",3.1. Huber loss,[0],[0]
"From (10), we know the new weight for pseudo-Huber loss has an extra scaling factor (√
1 + (δ−1u)2 )−1
(12)
and hence will shrink more to zero whenever δ−1|ŶpH(x)− Yi| is large.",3.1. Huber loss,[0],[0]
The tuning parameter δ acts like a control of the level of robustness.,3.1. Huber loss,[0],[0]
"A smaller δ will lead to more shrinkage on the weights of data that have responses far away from the estimator.
",3.1. Huber loss,[0],[0]
The estimating equation (9) can be solved by fix-point method which we propose in Algorithm 2.,3.1. Huber loss,[0],[0]
"For notation simplicity, we will use wi,j to denote w(Xi, xj), where Xi is the i-th training point and xj is the j-th testing point.",3.1. Huber loss,[0],[0]
"The convergence to the unique solution (if exists) is guaranteed by Lemma 1.
",3.1. Huber loss,[0],[0]
Lemma 1.,3.1. Huber loss,[0],[0]
"Define
Kδ(y) =
∑n i=1
wiYi√ 1+( y−Yiδ )
2∑n i=1
wi√ 1+( y−Yiδ ) 2 ,
Algorithm 2 pseudo-Huber loss (δ)
",3.1. Huber loss,[0],[0]
"Input: Test points {xj}mj=1, initial guess {Ŷ (0)(xj)}, local weights wi,j , training responses {Yi}ni=1, and error tolerance 0.",3.1. Huber loss,[0],[0]
"while > 0 do
(a) Update the weights
w (k) i,j = wi,j√ 1 + ( Ŷ (k−1)(xj)−Yi
δ )2 (b) Update the estimator
Ŷ (k)(xj) =
∑n i=1",3.1. Huber loss,[0],[0]
"w
(k) i,j Yi∑n
i=1",3.1. Huber loss,[0],[0]
w (k),3.1. Huber loss,[0],[0]
"i,j
(c) Calculate error
= 1
m",3.1. Huber loss,[0],[0]
m∑ j=1,3.1. Huber loss,[0],[0]
( Ŷ k(xj)− Ŷ (k−1)(xj) )2,3.1. Huber loss,[0],[0]
(d),3.1. Huber loss,[0],[0]
k,3.1. Huber loss,[0],[0]
"← k + 1
end while Output the pseudo-Huber estimator:
ŶpH(xj) =",3.1. Huber loss,[0],[0]
"Ŷ (k)(xj)
where",3.1. Huber loss,[0],[0]
∑n i=1 wi = 1.,3.1. Huber loss,[0],[0]
"Let K = maxi=1,··· ,n |Yi|.",3.1. Huber loss,[0],[0]
"Then Algorithm 2 can be written as Ŷ (k)(x) = Kδ(Ŷ (k−1)), and converges exponentially to a unique solution as long as δ > 2K.
From Lemma 1, we know it is important to standardize the responses Yi so that δ will be of the same scale for different problems.",3.1. Huber loss,[0],[0]
"In practice, we observe that one will not need to choose δ that satisfies the worst-case condition δ",3.1. Huber loss,[0],[0]
"> K in order for convergence, but making δ too small does lead to slow convergence rate.",3.1. Huber loss,[0],[0]
"For assigning the initial guess Ŷ (0), two simplest ways are to either take the random forest estimator we got or a constant vector equaling to the sample mean.",3.1. Huber loss,[0],[0]
"Throughout the rest of this paper, we will choose the weights to be random forest weights (3).",3.1. Huber loss,[0],[0]
"Non-convex function has played an important role in the context of robust regression (Huber, 2011; Hampel et al., 2011).",3.2. Tukey’s biweight,[0],[0]
"Unlike convex losses, the penalization on the errors can be bounded and hence the contribution of outliers in the estimating equation will eventually vanish.",3.2. Tukey’s biweight,[0],[0]
"Our forest regression framework 1 also incorporates the nonconvex losses which will show through the Tukey’s biweight function Tδ(·) (Huber, 2011), which is an example
of redescending loss whose derivative will vanish to zero as the input goes outside the interval",3.2. Tukey’s biweight,[0],[0]
"[−δ, δ].",3.2. Tukey’s biweight,[0],[0]
"It is defined in the following way:
d
dy Tδ(y) = y",3.2. Tukey’s biweight,[0],[0]
"( 1− y 2 δ2 )2 for |y| ≤ δ,
0 elsewhere.
",3.2. Tukey’s biweight,[0],[0]
"Similarly, by rearranging the estimating equation, we have
Ŷtukey(x) =
∑n i=1",3.2. Tukey’s biweight,[0],[0]
"w
tukey(Xi, x)Yi∑n i=1 w tukey(Xi, x)
where
wtukey(Xi, x) = w(Xi, x) max 1− ( Ŷtukey − Yi δ )2 , 0  with an extra scaling factor (see Figure 2)
",3.2. Tukey’s biweight,[0],[0]
"max { 1− (u δ )2 , 0 } .",3.2. Tukey’s biweight,[0],[0]
"(13)
In another word, the final estimator actually only depends on data with responses inside [−δ, δ], and the importance of any data (Xi, Yi) will be shrinking to zero when |Ŷtukey(x)− Yi| gets closer to the boundary value δ.",3.2. Tukey’s biweight,[0],[0]
"In this section, we will further use the framework 1 to investigate truncated squared error loss, and use this example to motivate the relation between random forest generalization performance and the number of adaptive nearest neighbors.",4. Truncated squared loss and nearest neighbors,[0],[0]
"For the truncated squared error loss
Sδ(y) =
{ 1 2y
2 for |y| ≤ δ, 1 2δ 2 elsewhere
the corresponding estimating equation is∑ |Ŷtrunc(x)−Yi|≤δ w(Xi, x)(Ŷtrunc(x)− Yi) = 0.
If we define a new weight
wtrunc(Xi, x) = w(Xi, x) 1I{|Ŷtrunc(x)− Yi| ≤ δ}, (14)
then the estimator for truncated squared loss is
Ŷtrunc(x) =
∑n i=1",4.1. Truncated squared error,[0],[0]
"w
trunc(Xi, x)Yi∑n i=1 w trunc(Xi, x) .",4.1. Truncated squared error,[0],[0]
"(15)
The estimator (15) is like a trimmed version of the random forest estimator (2).",4.1. Truncated squared error,[0],[0]
We first sort {Yi}ni=1 and trim off the responses where |Ŷtrunc(x),4.1. Truncated squared error,[0],[0]
− Yi| > δ.,4.1. Truncated squared error,[0],[0]
"Therefore, for any truncation level δ, the estimator Ŷtrunc(x) only depends on data satisfying |Ŷtrunc(x) − Yi| ≤ δ with the same local random forest weights (1).",4.1. Truncated squared error,[0],[0]
"In classical random forest, all the data with positive weights (3) are included when calculating the final estimator Ŷ (x).",4.2. Random Forest Nearest Neighbors,[0],[0]
"However, from section 4.1, we know in order to achieve robustness, some of the data should be dropped out of consideration.",4.2. Random Forest Nearest Neighbors,[0],[0]
"For example, using the truncated squared error loss, we will only consider the data satisfying |Yi − Ŷtrunc(x)| ≤ δ.",4.2. Random Forest Nearest Neighbors,[0],[0]
"In classical random forest, the criterion of tree split is to reduce the mean squared error, then in most cases, data points inside one terminal node will tend to have more similar responses.",4.2. Random Forest Nearest Neighbors,[0],[0]
"So informally larger |Ŷtrim(x)−Yi|will indicate smaller local weightw(Xi, x).",4.2. Random Forest Nearest Neighbors,[0],[0]
"Therefore, instead of solving for (15), we investigate a related estimator
Ŷwt(x) = ∑ w(Xi,x)≥ w(Xi, x)Yi∑ w(Xi,x)≥ w(Xi, x)
(16)
where > 0 is a constant in (0, 1).",4.2. Random Forest Nearest Neighbors,[0],[0]
"Recall that in (Lin & Jeon, 2006), they show all the observations with positive weights are considered voting points for random forest estimator.",4.2. Random Forest Nearest Neighbors,[0],[0]
"However, (16) implies that we should drop observations with weights smaller than a threshold in order for the robustness.",4.2. Random Forest Nearest Neighbors,[0],[0]
"More formally, let σ be a permutation such that w(Xσ(1), x) ≥ · · · ≥ w(Xσ(n0), x) > 0, then (2) is equivalent to
Ŷ (x) = n0∑ i=1",4.2. Random Forest Nearest Neighbors,[0],[0]
"w(Xσ(i), x)Yσ(i).
",4.2. Random Forest Nearest Neighbors,[0],[0]
"Then we can define the k random forest nearest neighbors (k-RFNN) of x to be {Xσ(1), · · · , Xσ(k)}, k ≤ n0, and get predictor
Ŷk(x) = k∑ i=1 w̃(Xσ(i), x)Yσ(i), (17)
where w̃(Xσ(i), x) = w(Xσ(i), x)/ ∑k j=1 w(Xσ(i), x).",4.2. Random Forest Nearest Neighbors,[0],[0]
"In the numerical experiments (Section 5.3), we will test the performance of the estimator (17) with different k, and show that by merely choosing the right number of nearest neighbors, one can largely improve the performance of classical random forest.
Shi and Horvath (2006) proposed a similar ensemble tree based nearest neighbor method.",4.2. Random Forest Nearest Neighbors,[0],[0]
"In their approach, if the observations Xi and Xj lie in the same leaf, then the similarity between them is increased by one.",4.2. Random Forest Nearest Neighbors,[0],[0]
"At the end, the similarities are normalized by dividing the total number of trees in the forest.",4.2. Random Forest Nearest Neighbors,[0],[0]
"Therefore, their weights (similarities) w(Xi, x) will be m−1 ∑m t=1 1I{Xi∈Rl(x,θ)} contrast to (3).",4.2. Random Forest Nearest Neighbors,[0],[0]
"So different from their approach, for random forest, the similarity between Xi and Xj will be increased by 1/#{p :",4.2. Random Forest Nearest Neighbors,[0],[0]
"Xp ∈ Rl(Xi,θ)} if they both lie in the same leaf l(Xi, θ).",4.2. Random Forest Nearest Neighbors,[0],[0]
This means the increment in the similarity also depends on the number of data points in the leaf.,4.2. Random Forest Nearest Neighbors,[0],[0]
"In this section, we plug in the quantile loss, Huber loss and Tukey’s biweight loss into the general forest framework and compare these algorithms with random forest.",5. Experiments,[0],[0]
"Unless otherwise stated, for both Huber and Tukey forest, the error tolerance is set to be 10−6, and every forest is an ensemble of 1000 trees with maximum terminal node size 10.",5. Experiments,[0],[0]
"The robust parameter δ are set to be 0.005 and 0.8 for Huber and Tukey forest, respectively.",5. Experiments,[0],[0]
"We generate 1000 training data points from a Uniform distribution on [−5, 5] and another 1000 testing points from the same distribution.",5.1. One dimensional toy example,[0],[0]
"The true underlying model is Y = X2 + , ∼ N (0, 1).",5.1. One dimensional toy example,[0],[0]
"But on the training samples, we choose 20% of the data and add noise 2T2 to the responses, where T2 follows t-distribution with degree of freedom 2.
",5.1. One dimensional toy example,[0],[0]
"In Figure 3, we plot the true squared curve and different forest predictions.",5.1. One dimensional toy example,[0],[0]
"It is clear that Huber and Tukey forest achieve competitive robustness as quantile random forest, and can almost recover the true underlying distribution, but random forest is largely impacted by the outliers.",5.1. One dimensional toy example,[0],[0]
"We also repeat the experiments for 20 times, and report the average mean squared error (MSE), mean absolute deviation (MAD) and median absolute percentage error (MAPE) in Table 1.",5.1. One dimensional toy example,[0],[0]
"We generate data from 10 dimensional Normal distribution, i.e. X ∼ N10(~0,Σ).",5.2. Multivariate example,[0],[0]
"Then we test out algorithms on following models.
",5.2. Multivariate example,[0],[0]
"(1) Y = ∑10 i=1X 2 i + and ∼ N (0, 1), Σ = I.
(2) Y = ∑10 i=1X 2 i + and ∼ N (0, 1), Σ = Toeplitz(ρ = 0.7).
",5.2. Multivariate example,[0],[0]
"Then for each model, we randomly choose η proportion of the training samples and add noise 15T2 where T2 follows t-distribution with degree of freedom 2.",5.2. Multivariate example,[0],[0]
"The noise level η ∈ {0, 0.05, 0.1, 0.15, 0.2}.",5.2. Multivariate example,[0],[0]
The results are summarized in Table 2 and 3.,5.2. Multivariate example,[0],[0]
"On the clean data, random forest still play the best, however, Huber forest’s performance is also competitive and lose less efficiency than QRF and Tukey forest.",5.2. Multivariate example,[0],[0]
"On the noisy data, all three robust methods outperform random forest.",5.2. Multivariate example,[0],[0]
"Among them, Huber forest is most robust and stable.",5.2. Multivariate example,[0],[0]
"In this section, we check how the number of adaptive nearest neighbors k in (17) will have impact on the performance of k-RFNN.",5.3. Nearest neighbors,[0],[0]
"We consider the same two models (1) and (2), and keep both training sample size and testing sample size to be 1000.",5.3. Nearest neighbors,[0],[0]
"The relations between MSE, MAD and the number of adaptive nearest neighbors are illustrated in Figure 4.",5.3. Nearest neighbors,[0],[0]
Recall that k-RFNN with all 1000 neighbors is equivalent to random forest.,5.3. Nearest neighbors,[0],[0]
"From the figures, we clearly observe a kink at k = 15, which is much less than 1000.",5.3. Nearest neighbors,[0],[0]
"We take two regression datasets from UCI machine learning repository (Lichman, 2013), and one real estate dataset from OpenIntro.",5.4. Real data,[0],[0]
"For each dataset, we randomly choose 2/3 observations for training and the rest for testing.",5.4. Real data,[0],[0]
MSE and MAD are reported by averaging over 20 trials.,5.4. Real data,[0],[0]
The results are presented in Table 4.,5.4. Real data,[0],[0]
"To further test the robustness, we then repeat the experiment but add extra T2 noise to 20%
of the standardized training data response variables everytime.",5.4. Real data,[0],[0]
The results are in Table 5.,5.4. Real data,[0],[0]
"Robust forests outperform random forest in most of the cases except for Ames data sets, on which quantile random forest behaves poorly.",5.4. Real data,[0],[0]
"The experimental results show that Huber forest, Tukey forest and quantile random forest are all much more robust than random forest in the presence of outliers.",6. Conclusion and discussion,[0],[0]
"However, without outliers, Huber forest preserves more efficiency than the other two robust methods.",6. Conclusion and discussion,[0],[0]
"We did not cross validate the parameter δ for different noise levels, so one would
expect even better performance after carefully tuning the parameter.
",6. Conclusion and discussion,[0],[0]
"Besides random forest weights, other data dependent similarities could also be used in Algorithm 1.",6. Conclusion and discussion,[0],[0]
We could also design loss functions which optimizes a metric for specific problems.,6. Conclusion and discussion,[0],[0]
The fixed-point method could be replaced by other more efficient algorithms.,6. Conclusion and discussion,[0],[0]
The framework could be easily extended to classification problems.,6. Conclusion and discussion,[0],[0]
All these will be potential future work.,6. Conclusion and discussion,[0],[0]
Proof.,7.1. Proof of Lemma 1,[0],[0]
"Because Ŷ (k)(x) = Kδ(Ŷ (k−1)) which is a fixedpoint method, we only need to show ∣∣∣K ′δ(y)∣∣∣ < 1 in order for the existence and uniqueness of the solution.",7.1. Proof of Lemma 1,[0],[0]
"Define the normalized weight
w̃i = wi√
1 + ( y−Yi δ
)2 / n∑
i=1 wi√",7.1. Proof of Lemma 1,[0],[0]
"1 + ( y−Yi δ )2 , we have ∑n i=1 w̃i = 1, and
∣∣∣K ′δ(y)∣∣∣ ≤
∣∣∣∣∣∣ n∑ i=1",7.1. Proof of Lemma 1,[0],[0]
w̃iYi  n∑ j=1 (1I(i = j)− w̃j),7.1. Proof of Lemma 1,[0],[0]
y,7.1. Proof of Lemma 1,[0],[0]
− Yj δ2 +,7.1. Proof of Lemma 1,[0],[0]
(y − Yj)2,7.1. Proof of Lemma 1,[0],[0]
"∣∣∣∣∣∣ ≤ 2
n∑ i=1",7.1. Proof of Lemma 1,[0],[0]
"w̃i |Yi| max i=1,··· ,n ( |y",7.1. Proof of Lemma 1,[0],[0]
− Yi| δ2 +,7.1. Proof of Lemma 1,[0],[0]
(,7.1. Proof of Lemma 1,[0],[0]
"y − Yi)2 )
",7.1. Proof of Lemma 1,[0],[0]
= 2 n∑ i=1,7.1. Proof of Lemma 1,[0],[0]
"w̃i |Yi| 1 mini=1,··· ,n (",7.1. Proof of Lemma 1,[0],[0]
"δ2 |y−Yi| + |y − Yi| )
≤ max i=1,··· ,n
|Yi| 1
δ .
",7.1. Proof of Lemma 1,[0],[0]
"Therefore, ∣∣∣K ′δ(y)∣∣∣ < 12 if δ > 2 maxi=1,··· ,n |Yi| = 2K.",7.1. Proof of Lemma 1,[0],[0]
"We would like to thank Stan Humphrys and Zillow for supporting this research, as well as three anonymous referees for their insightful comments.",Acknowledgements,[0],[0]
Part of the implementation in this paper is based on Zillow code library.,Acknowledgements,[0],[0]
This paper introduces a new general framework for forest-type regression which allows the development of robust forest regressors by selecting from a large family of robust loss functions.,abstractText,[0],[0]
"In particular, when plugged in the squared error and quantile losses, it will recover the classical random forest (Breiman, 2001) and quantile random forest (Meinshausen, 2006).",abstractText,[0],[0]
We then use robust loss functions to develop more robust foresttype regression algorithms.,abstractText,[0],[0]
"In the experiments, we show by simulation and real data that our robust forests are indeed much more insensitive to outliers, and choosing the right number of nearest neighbors can quickly improve the generalization performance of random forest.",abstractText,[0],[0]
Forest-type Regression with General Losses  and Robust Forest,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 47–57 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Due to the advent of computing technologies to indigenous communities all over the world, natural language processing (NLP) applications
∗*The first two authors contributed equally.
for languages with limited computer-readable textual data are getting increasingly important.",1 Introduction,[0],[0]
"This contrasts with current research, which focuses strongly on approaches which require large amounts of training data, e.g., deep neural networks.",1 Introduction,[0],[0]
"Those are not trivially applicable to minimal-resource settings with less than 1, 000 available training examples.",1 Introduction,[0],[0]
"We aim at closing this gap for morphological surface segmentation, the task of splitting a word into the surface forms of its smallest meaning-bearing units, its morphemes.
",1 Introduction,[0],[0]
Recovering morphemes provides information about unknown words and is thus especially important for polysynthetic languages with a high morpheme-to-word ratio and a consequently large overall number of words.,1 Introduction,[0],[0]
"To illustrate how segmentation helps understanding unknown multiplemorpheme words, consider an example in this paper’s language of writing: even if the word unconditionally did not appear in a given training corpus, its meaning could still be derived from a combination of its morphs un, condition, al and ly.
",1 Introduction,[0],[0]
"Due to its importance for down-stream tasks (Creutz et al., 2007; Dyer et al., 2008), segmentation has been tackled in many different ways, considering unsupervised (Creutz and Lagus, 2002), supervised (Ruokolainen et al., 2013) and semisupervised settings (Ruokolainen et al., 2014).",1 Introduction,[0],[0]
"Here, we add three new questions to this line of research: (i) Are data-hungry neural network models
47
applicable to segmentation of polysynthetic languages in minimal-resource settings?",1 Introduction,[0],[0]
(ii) How can the performance of neural networks for surface segmentation be improved if we have only unlabeled or no external data at hand?,1 Introduction,[0],[0]
(iii) Is crosslingual transfer for this task possible between related languages?,1 Introduction,[0],[0]
"The last two questions are crucial: While for many languages it is difficult to obtain the number of annotated examples used in earlier work on (semi-)supervised methods, a limited amount might still be obtainable.
",1 Introduction,[0],[0]
"We experiment on four polysynthetic Mexican languages: Mexicanero, Nahuatl, Wixarika and Yorem Nokki (details in §2).",1 Introduction,[0],[0]
"The datasets we use are, as far as we know, the first computer-readable datasets annotated for morphological segmentation in those languages.
",1 Introduction,[0],[0]
Our experiments show that neural seq2seq models perform on par with or better than other strong baselines for our polysynthetic languages in a minimal-resource setting.,1 Introduction,[0],[0]
"However, we further propose two novel multi-task approaches and two new data augmentation methods.",1 Introduction,[0],[0]
"Combining them with our neural model yields up to 5.05% absolute accuracy or 3.40% F1 improvements over our strongest baseline.
",1 Introduction,[0],[0]
"Finally, following earlier work on cross-lingual knowledge transfer for seq2seq tasks (Johnson et al., 2017; Kann et al., 2017), we investigate training one single model for all languages, while sharing parameters.",1 Introduction,[0],[0]
"The resulting model performs comparably to or better than the individual models, but requires only roughly as many parameters as one single model.
",1 Introduction,[0],[0]
Contributions.,1 Introduction,[0],[0]
"To sum up, we make the following contributions: (i) we confirm the applicability of neural seq2seq models to morphological segmentation of polysynthetic languages in minimalresource settings; (ii) we propose two novel multi-task training approaches and two novel data augmentation methods for neural segmentation models; (iii) we investigate the effectiveness of cross-lingual transfer between related languages; and (iv) we provide morphological segmentation datasets for Mexicanero, Nahuatl, Wixarika and Yorem Nokki.",1 Introduction,[0],[0]
"Polysynthetic languages are morphologically rich languages which are highly synthetic, i.e., single words can be composed of many individual
morphemes.",2 Polysynthetic Languages,[0],[0]
"In extreme cases, entire sentences consist of only one single token, whereupon “every argument of a predicate must be expressed by morphology on the word that contains that assigner” (Baker, 2006).",2 Polysynthetic Languages,[0],[0]
"This property makes surface segmentation of polysynthetic languages at the same time complex and particularly relevant for further linguistic analysis.
",2 Polysynthetic Languages,[0],[0]
"In this paper, we experiment on four polysynthetic languages of the Yuto-Aztecan family (Baker, 1997), with the goal of improving the performance of neural seq2seq models.",2 Polysynthetic Languages,[0],[0]
"The languages will be described in the rest of this section.
",2 Polysynthetic Languages,[0],[0]
"Mexicanero is a Western Peripheral Nahuatl variant, spoken in the Mexican state of Durango by approximately one thousand people.",2 Polysynthetic Languages,[0],[0]
"This dialect is isolated from the rest of the other branches and has a strong process of Spanish stem incorporation, while also having borrowed some suffixes from that language (Vanhove et al., 2012).",2 Polysynthetic Languages,[0],[0]
It is common to see Spanish words mixed with Nahuatl agglutinations.,2 Polysynthetic Languages,[0],[0]
"In the following example we can see an intrasentencial mixing of Spanish (in uppercases) and Mexicanero:
u|ni|ye MALO – I was sick
Nahuatl is a large subgroup of the YutoAztecan language family, and, including all of its variants, the most spoken native language in Mexico.",2 Polysynthetic Languages,[0],[0]
"Its almost two million native speakers live mainly in Puebla, Guerrero, Hidalgo, Veracruz, and San Luis Potosi, but also in Oaxaca, Durango, Modelos, Mexico City, Tlaxcala, Michoacan, Nayarit and the State of Mexico.",2 Polysynthetic Languages,[0],[0]
"Three dialectical groups are known: Central Nahuatl, Occidental Nahuatl and Oriental Nahuatl.",2 Polysynthetic Languages,[0],[0]
"The data collected for this work belongs to the Oriental branch spoken by 70 thousand people in Northern Puebla.
",2 Polysynthetic Languages,[0],[0]
"Like all languages of the Yuto-Aztecan family, Nahuatl is agglutinative and one word can consist of a combination of many different morphemes.",2 Polysynthetic Languages,[0],[0]
"Usually, the verb functions as the stem and gets extended by morphemes specifying, e.g., subject, patient, object or indirect object.",2 Polysynthetic Languages,[0],[0]
The most common syntax sequence for Nahuatl is SOV.,2 Polysynthetic Languages,[0],[0]
"An example word is:
o|ne|mo|kokowa|ya – I was sick
Wixarika is a language spoken in the states of Jalisco, Nayarit, Durango and Zacatecas in Central West Mexico by approximately fifty thousand people.",2 Polysynthetic Languages,[0],[0]
It belongs to the Coracholan group of languages within the Yuto-Aztecan family.,2 Polysynthetic Languages,[0],[0]
"Wixarika has five vowels {a,e,i,+1,u} with long and short variants.",2 Polysynthetic Languages,[0],[0]
"An example for a word in the language is:
ne|p+|ti|kuye|kai – I was sick
Like Nahuatl, it has an SOV syntax, with heavy agglutination on the verb.",2 Polysynthetic Languages,[0],[0]
"Wixarika is morphologically more complex than other languages from the same family, because it incorporates more information into the verb (Leza and López, 2006).",2 Polysynthetic Languages,[0],[0]
"This leads to a higher number of morphemes per word as can also be seen in Table 3.
",2 Polysynthetic Languages,[0],[0]
Yorem Nokki is part of Taracachita subgroup of the Yuto-Aztecan language family.,2 Polysynthetic Languages,[0],[0]
"Its Southern dialect is spoken by close to forty thousand people in the Mexican states of Sinaloa and Sonora, while its Northern dialect has about twenty thousand speakers.",2 Polysynthetic Languages,[0],[0]
"In this work, we consider the Southern dialect.",2 Polysynthetic Languages,[0],[0]
"The nominal morphology of Yorem
1While linguists often use a dashed i (i) to denote this vowel, in practice almost all native speakers use a plus symbol (+).",2 Polysynthetic Languages,[0],[0]
"In this work, we choose to use the latter.
",2 Polysynthetic Languages,[0],[0]
"Nokki is rather simple, but, like in the other YutoAztecan languages, the verb is highly complex.",2 Polysynthetic Languages,[0],[0]
Its alphabet consists of 28 characters and contains 8 different vowels.,2 Polysynthetic Languages,[0],[0]
"An example verb is:
ko’kore|ye|ne – I was sick",2 Polysynthetic Languages,[0],[0]
"To create our datasets, we make use of both segmentable (i.e., consisting of multiple morphemes) and non-segmentable (i.e., consisting of one single morpheme) words described in books of the collection Archive of Indigenous Languages in Mexicanero (Canger, 2001), Nahuatl (Lastra de Suárez, 1980), Wixarika (Gómez and López, 1999), and Yorem Nokki (Freeze, 1989).",3 Morphological Segmentation Datasets,[0],[0]
"Statistics about the data in the four languages are displayed in Tables 1, 2 and 3.",3 Morphological Segmentation Datasets,[0],[0]
We include segmentable as well as non-segmentable words into our datasets in order to ensure that our methods can correctly decide against splitting up single morphemes.,3 Morphological Segmentation Datasets,[0],[0]
"The phrases in all languages are mostly parallel, such that the corpora are roughly equivalent.",3 Morphological Segmentation Datasets,[0],[0]
"Therefore, we can compare the morphology of translated words (cf. Table 3), noticing that the language with most agglutination is Wixarika, with an average rate of 3.25 morphemes per word; the other languages have an average of close to 2.2 morphemes per word.",3 Morphological Segmentation Datasets,[0],[0]
This higher morphological complexity naturally produces data sparsity at the token level.,3 Morphological Segmentation Datasets,[0],[0]
"Also, we can notice that Wixarika has more unique words than the rest of our studied languages.",3 Morphological Segmentation Datasets,[0],[0]
"However, Nahuatl has with 810 the highest number of unique morphemes.
",3 Morphological Segmentation Datasets,[0],[0]
Final splits.,3 Morphological Segmentation Datasets,[0],[0]
"In order to make follow-up work on minimal-resource settings for morphological segmentation easily comparable, we provide predefined splits of our datasets2.",3 Morphological Segmentation Datasets,[0],[0]
40% of the data constitute the test sets.,3 Morphological Segmentation Datasets,[0],[0]
"Of the remaining data, we
2Our datasets can be found together with the code of our models at http://turing.iimas.unam.mx/wix/MexSeg .
use 20% for development and the rest for training.",3 Morphological Segmentation Datasets,[0],[0]
The final numbers of words per dataset and language are shown in Table 2.,3 Morphological Segmentation Datasets,[0],[0]
"In the beginning of this section, we will introduce our neural architecture for segmentation.",4 Neural Seq2seq Models for Segmentation,[0],[0]
"Subsequently, we will first describe our two proposed multi-task training approaches and second our data augmentation methods.",4 Neural Seq2seq Models for Segmentation,[0],[0]
"Finally, we will elaborate on expected differences between the two.",4 Neural Seq2seq Models for Segmentation,[0],[0]
"Following work on segmentation by Kann et al. (2016) for high-resource settings, our approach is based on the neural seq2seq model introduced by Bahdanau et al. (2015) for machine translation.
Encoder.",4.1 Character-Based Encoder-Decoder RNN,[0],[0]
"The first part of our model is a bidirectional recurrent neural network (RNN) which encodes the input sequence, i.e., the sequence of characters of a given word w = w1, w2, . . .",4.1 Character-Based Encoder-Decoder RNN,[0],[0]
", wTv , represented by the corresponding embedding vectors vw1 , ..., vwTv .",4.1 Character-Based Encoder-Decoder RNN,[0],[0]
"In particular, our encoder consists of one gated recurrent neural network (GRU) which processes the input in forward direction and a second GRU which processes the input from the opposite side.
",4.1 Character-Based Encoder-Decoder RNN,[0],[0]
Encoding with this bidirectional GRU yields the forward hidden state −→ h i = f,4.1 Character-Based Encoder-Decoder RNN,[0],[0]
"(−→ h i−1, vi ) and the backward hidden state ←−",4.1 Character-Based Encoder-Decoder RNN,[0],[0]
h,4.1 Character-Based Encoder-Decoder RNN,[0],[0]
"i = f (←− h i+1, vi ) , for a non-linear activation function f .",4.1 Character-Based Encoder-Decoder RNN,[0],[0]
Their concatenation hi =,4.1 Character-Based Encoder-Decoder RNN,[0],[0]
"[−→ hi ; ←− hi ] is passed on to the decoder.
",4.1 Character-Based Encoder-Decoder RNN,[0],[0]
Decoder.,4.1 Character-Based Encoder-Decoder RNN,[0],[0]
"The second part of our network, the decoder, is a single GRU, defining a probability distribution over strings in (Σ ∪ S)∗, for an alphabet Σ and a separation symbol S:
pED(c | w) = Tc∏
t=1
p(ct | c1, . . .",4.1 Character-Based Encoder-Decoder RNN,[0],[0]
", ct−1, w).",4.1 Character-Based Encoder-Decoder RNN,[0],[0]
"(1)
where p(ct | c1, . . .",4.1 Character-Based Encoder-Decoder RNN,[0],[0]
", ct−1, w) is computed using an attention mechanism and an output softmax layer over Σ ∪ S.
A more detailed description of the general attention-based encoder-decoder architecture can be found in the original paper by Bahdanau et al. (2015).",4.1 Character-Based Encoder-Decoder RNN,[0],[0]
"In order to leverage unlabeled data or even random strings during training, we define an autoencoding auxiliary task, which consists of encoding the input and decoding an output which is identical to the original string.
",5.1 Multi-Task Training,[0],[0]
"Then, our multi-task training objective is to maximize the joint log-likelihood of this auxiliary task and our segmentation main task:
L(θ)= ∑
(w,c)∈T log pθ (c | e(w))",5.1 Multi-Task Training,[0],[0]
"(2)
+ ∑
a∈A log pθ(a | e(a))
",5.1 Multi-Task Training,[0],[0]
T denotes the segmentation training data with examples consisting of a word w and its segmentation c. A denotes either a set of words in the language of the system or a set of random strings.,5.1 Multi-Task Training,[0],[0]
"The function e describes the encoder and depends on the model parameters θ, which are shared across the two tasks.",5.1 Multi-Task Training,[0],[0]
"For training, we use data from both sets at the same time and mark each example with an additional, task-specific input symbol.
",5.1 Multi-Task Training,[0],[0]
We treat the size of A as a hyperparameter which we optimize on the development set separately for each language.,5.1 Multi-Task Training,[0],[0]
"Values we experiment with are m times the amount of instances in the original training set, with m ∈ {1, 2, 4, 8}.3
3An exception is Yorem Nokki, for which we do not have enough unlabeled data available, such that we experiment only with m ∈ {1, 2}.
",5.1 Multi-Task Training,[0],[0]
There are multiple reasons why we expect multi-task training to improve the performance of the final model.,5.1 Multi-Task Training,[0],[0]
"First, multi-task training should act as a regularizer.",5.1 Multi-Task Training,[0],[0]
"Second, for our models, the segmentation task consists in large parts of learning to copy the input character sequence to the output.",5.1 Multi-Task Training,[0],[0]
"This, however, can be learned from any string and does not require annotated segmentation boundaries.",5.1 Multi-Task Training,[0],[0]
"Third, in the case of unlabeled data (i.e., not for random strings), we expect the character language model in the decoder to improve, since it is trained on additional data.
",5.1 Multi-Task Training,[0],[0]
We denote models trained with multi-task training using unlabeled corpus data as MTT-U and models trained with multi-task training using random strings as MTT-R.,5.1 Multi-Task Training,[0],[0]
A second option to make use of unlabeled data or random strings is to extend the available training data with new examples made from those.,5.2 Data Augmentation,[0],[0]
The main question to answer here is how to include the new data into the existing datasets.,5.2 Data Augmentation,[0],[0]
We do this by building new training examples in a fashion similar to the multi-task setup.,5.2 Data Augmentation,[0],[0]
"All newly created instances are of the form
w 7→ w (3)
where either w ∈ V with V being the observed vocabulary of the language, e.g., words in a given unlabeled corpus, or w ∈ R with R being a set of sequences of random characters from the alphabet Σ of the language.
",5.2 Data Augmentation,[0],[0]
"Again, we treat the amount of additional training examples as a hyperparameter which we optimize on the development set separately for each language.",5.2 Data Augmentation,[0],[0]
"We explore m times the amount of instances in the original training set, with m ∈ {1, 2, 4, 8}.
",5.2 Data Augmentation,[0],[0]
"The reasons why we expect our data augmentation methods to lead to better segmentation models are similar to those for multi-task training.
",5.2 Data Augmentation,[0],[0]
"We call models trained on datasets augmented with unlabeled corpus data or random strings DAU or DA-R, respectively.",5.2 Data Augmentation,[0],[0]
The difference between MTT-U (resp.,5.3 Differences Between Multi-task Training and Data Augmentation,[0],[0]
MTT-R) and DA-U (resp.,5.3 Differences Between Multi-task Training and Data Augmentation,[0],[0]
"MTT-U) is a single element in the input sequence (the one representing the task).
",5.3 Differences Between Multi-task Training and Data Augmentation,[0],[0]
"However, this information enables the model to handle each given instance correctly at inference time.",5.3 Differences Between Multi-task Training and Data Augmentation,[0],[0]
"As a result, it gets more robust against noisy data, which seems crucial for our way of using unlabeled corpora.",5.3 Differences Between Multi-task Training and Data Augmentation,[0],[0]
"Consider, for example, the Nahuatl word onemokokowaya.",5.3 Differences Between Multi-task Training and Data Augmentation,[0],[0]
"Training on
onemokokowaya 7→ onemokokowaya
will make the model learn not to segment words which consist of the morphemes o, ne,mo, kokowa, ya, which should ultimately hurt performance.",5.3 Differences Between Multi-task Training and Data Augmentation,[0],[0]
"The multi-task approach, in contrast, mitigates this problem.
",5.3 Differences Between Multi-task Training and Data Augmentation,[0],[0]
"As a conclusion, we expect the data augmentation approach with unlabeled data to not obtain outstanding performance, but rather consider it an important and informative baseline for the corresponding multi-task approach.",5.3 Differences Between Multi-task Training and Data Augmentation,[0],[0]
"Using random strings, the difference between the multi-task and the data augmentation approaches is less obvious: Real morphemes should appear rarely enough in the created random character sequences to avoid the negative effect which we expect for corpus words.",5.3 Differences Between Multi-task Training and Data Augmentation,[0],[0]
We thus assume that the performances of MTT-R and DA-R should be similar.,5.3 Differences Between Multi-task Training and Data Augmentation,[0],[0]
We apply our models to the datasets described in §3.,6.1 Data,[0],[0]
"For the multi-task training and data augmentation using unlabeled data, we use (unsegmented) words from a parallel corpus collected by Gutierrez-Vasques et al. (2016) for Nahuatl and the closely related Mexicanero.",6.1 Data,[0],[0]
For Wixarika we use data from Mager et al. (2018) and for Yorem Nokki we use text from Maldonado,6.1 Data,[0],[0]
Martı́nez,6.1 Data,[0],[0]
et al. (2010).,6.1 Data,[0],[0]
"Now, we will describe the baselines we use to evaluate the overall performance of our approaches.
",6.2 Baselines,[0],[0]
Supervised seq2seq RNN (S2S).,6.2 Baselines,[0],[0]
"As a first baseline, we employ a fully supervised neural model without data augmentation or multi-task training, i.e., an attention-based encoder-decoder RNN (Bahdanau et al., 2015) which has been trained only on the available annotated data.
",6.2 Baselines,[0],[0]
Semi-supervised MORFESSOR (MORF).,6.2 Baselines,[0],[0]
"We further compare to the semi-supervised version
of MORFESSOR (Kohonen et al., 2010), a wellknown morphological segmentation system.",6.2 Baselines,[0],[0]
"During training, we tune the hyperparameters for each language on the respective development set.",6.2 Baselines,[0],[0]
"The best performing model is applied to the test set.
",6.2 Baselines,[0],[0]
FlatCat (FC).,6.2 Baselines,[0],[0]
"Our next baseline is FlatCat (Grönroos et al., 2014), a variant of MORFESSOR.",6.2 Baselines,[0],[0]
It consists of a hidden Markov model for segmentation.,6.2 Baselines,[0],[0]
"The states of the model correspond either to a word boundary and one of the four morph categories stem, prefix, suffix, and nonmorpheme.",6.2 Baselines,[0],[0]
"It can work in an unsupervised way, but, similar to the previous baseline, can make effective use of small amounts of labeled data.
CRF.",6.2 Baselines,[0],[0]
"We further compare to a conditional random fields (CRF) (Lafferty et al., 2001) model, in particular a strong discriminative model for segmentation by Ruokolainen et al. (2014).",6.2 Baselines,[0],[0]
"It reduces the task to a classification problem with four classes: beginning of a morph, middle of a morph, end of a morph and single character morph.",6.2 Baselines,[0],[0]
"Training is again semi-supervised and the model was previously reported to obtain good results for small amounts of unlabeled data (Ruokolainen et al., 2014), which makes it very suitable for our minimal-resource setting.",6.2 Baselines,[0],[0]
Neural network parameters.,6.3 Hyperparameters,[0],[0]
All GRUs in both the encoder and the decoder have 100- dimensional hidden states.,6.3 Hyperparameters,[0],[0]
"All embeddings are 300-dimensional.
",6.3 Hyperparameters,[0],[0]
"For training, we use ADADELTA (Zeiler, 2012) with a minibatch size of 20.",6.3 Hyperparameters,[0],[0]
"We initialize all weights to the identity matrix and biases to zero (Le et al., 2015).",6.3 Hyperparameters,[0],[0]
"All models are trained for a maximum of 200 epochs, but we evaluate after every 5 epochs and apply the best performing model at test time.",6.3 Hyperparameters,[0],[0]
"Our final reported results are averaged accuracies over 5 single training runs.
",6.3 Hyperparameters,[0],[0]
Optimizing the amount of auxiliary task data.,6.3 Hyperparameters,[0],[0]
The performance of our neural segmentation model in dependence of the amount of auxiliary task training data can be seen in Figure 1.,6.3 Hyperparameters,[0],[0]
"As a general tendency across all languages, adding more data seems better, particularly for the autoencoding task with random strings.",6.3 Hyperparameters,[0],[0]
"The only exception is Wixarika.
",6.3 Hyperparameters,[0],[0]
The final configurations we choose for m (cf.,6.3 Hyperparameters,[0],[0]
"§5.1) in the case of multi-task training with the
auxiliary task of autoencoding corpus data are m = 4 for Mexicanero, Nahuatl and Wixarika and m = 1 for Yorem Nokki.",6.3 Hyperparameters,[0],[0]
"For multi-task training with autoencoding of random strings we select m = 8 for Mexicanero, Nahuatl and Yorem Nokki and m = 4 for Wixarika.
Optimizing the amount of artificial training data for data augmentation.",6.3 Hyperparameters,[0],[0]
Figure 2 shows the performance of the encoder-decoder depending on the amount of added artificial training data.,6.3 Hyperparameters,[0],[0]
"In the case of random strings, again, adding more training data seems to help more.",6.3 Hyperparameters,[0],[0]
"However, using corpus data seems to hurt performance and the more such examples we use, the worse accuracy we obtain.",6.3 Hyperparameters,[0],[0]
"Thus, we conclude that (as expected) data augmentation with corpus data is not a good way to improve the model’s performance.",6.3 Hyperparameters,[0],[0]
"We will discuss this in more detail in §6.5.
",6.3 Hyperparameters,[0],[0]
"Even though the final conclusion should be to not add much corpus data, we apply what gives best results on the development set.",6.3 Hyperparameters,[0],[0]
"The final configurations we thus choose for DA-U are m = 1 for Mexicanero, Wixarika and Yorem Nokki and m = 2 for Nahuatl.",6.3 Hyperparameters,[0],[0]
"For DA-R, we select m = 4 for Mexicanero, Wixarika and Yorem Nokki and m = 8 for Nahuatl.",6.3 Hyperparameters,[0],[0]
Accuracy.,6.4 Evaluation Metrics,[0],[0]
"First, we evaluate using accuracy on the token level.",6.4 Evaluation Metrics,[0],[0]
"Thus, an example counts as correct if and only if the output of the system matches the reference solution exactly, i.e., if all output symbols are predicted correctly.
F1.",6.4 Evaluation Metrics,[0],[0]
"Our second evaluation metric is border F1, which measures how many segment boundaries are predicted correctly by the model.",6.4 Evaluation Metrics,[0],[0]
"While we use this metric because it is common for segmentation tasks, it is not ideal for our models since those are not guaranteed to preserve the input character sequence.",6.4 Evaluation Metrics,[0],[0]
We handle this problem as follows:,6.4 Evaluation Metrics,[0],[0]
"In order to compare borders, we identify them by the position of their preceding letter, i.e., if in both the model’s guess and the gold solution a segment border appears after the second character, it counts as correct.",6.4 Evaluation Metrics,[0],[0]
Wrong characters are ignored.,6.4 Evaluation Metrics,[0],[0]
Note that this comes with the disadvantage of erroneously inserted characters leading to all subsequent segment borders being counted as incorrect.,6.4 Evaluation Metrics,[0],[0]
Table 4 shows that accuracy and F1 seem to be highly correlated for our task.,6.5 Test Results and Discussion,[0],[0]
"The test results also give an answer to our first research question: The neural model S2S performs on par with CRF, the strongest baseline, for all languages but Nahuatl.",6.5 Test Results and Discussion,[0],[0]
"Further, S2S and CRF both outperform MORF and FC by a wide margin.",6.5 Test Results and Discussion,[0],[0]
"We may thus conclude that neural models are indeed applicable to segmentation of polysynthetic languages in a low-resource setting.
",6.5 Test Results and Discussion,[0],[0]
"Second, we can see that all our proposed methods except for DA-U improve over S2S, the neural baseline: The accuracy of MTT-U is between 0.0141 (Wixarika) and 0.0547 (Mexicanero) higher than S2S’s.",6.5 Test Results and Discussion,[0],[0]
"MTT-R improves between 0.0380 (Wixarika) and 0.0532 (Yorem
Nokki).",6.5 Test Results and Discussion,[0],[0]
"Finally, DA-R outperforms S2S by 0.0367 to 0.0479 accuracy for Yorem Nokki and Mexicanero, respectively.",6.5 Test Results and Discussion,[0],[0]
The overall picture when considering F1 looks similar.,6.5 Test Results and Discussion,[0],[0]
"Comparing our approaches to each other, there is no clear winner.",6.5 Test Results and Discussion,[0],[0]
This might be due to differences in the unlabeled data we use: the corpus we use for Mexicanero and Nahuatl is from dialects different from both respective test sets.,6.5 Test Results and Discussion,[0],[0]
"Assuming that the effect of training a language model using unlabeled data and erroneously learning to not segment words are working against each other for MTT-U, this might explain why MTT-U is best for Mexicanero and the gap between MTT-U and MTT-R is smaller for Nahuatl than for Yorem Nokki and Wixarika.
",6.5 Test Results and Discussion,[0],[0]
"As mentioned before (cf. §5.3), a simple data augmentation method using unlabeled data should
hurt performance.",6.5 Test Results and Discussion,[0],[0]
"This is indeed the result of our experiments: DA-U performs worse than S2S for all languages except for Mexicanero, where the unlabeled corpus is from another language: the closely related Nahuatl.",6.5 Test Results and Discussion,[0],[0]
"We thus conclude that multi-task training (instead of simple data augmentation) is crucial for the use of unlabeled data.
",6.5 Test Results and Discussion,[0],[0]
"Finally, our methods compare favorably to all baselines, with the exception of CRF for Nahuatl.",6.5 Test Results and Discussion,[0],[0]
"While CRF is overall the strongest baseline for our considered languages, our methods outperform it by up to 0.0214 accuracy or 0.0147 F1 for Mexicanero, 0.0322 accuracy or 0.0229 F1 for Wixarika and 0.0505 accuracy or 0.0340 F1 for Yorem Nokki.",6.5 Test Results and Discussion,[0],[0]
This shows the effectiveness of our fortified neural models for minimal-resource morphological segmentation.,6.5 Test Results and Discussion,[0],[0]
We now want to investigate the performance of one single model trained on all languages at once.,7 Cross-Lingual Transfer Learning,[0],[0]
This is done in analogy to the multi-task training described in §5.1.,7 Cross-Lingual Transfer Learning,[0],[0]
"We treat segmentation in each language as a separate task and train an attentionbased encoder-decoder model on maximizing the joint log-likelihood:
L(θ)= ∑
Li∈L
∑
(w,c)∈TLi
log pθ (c | e(w))
(4)
TLi denotes the segmentation training data in language Li and L is the set of our languages.",7 Cross-Lingual Transfer Learning,[0],[0]
"As before, each training example consists of a word w and its segmentation c.",7 Cross-Lingual Transfer Learning,[0],[0]
We keep all model parameters and the training regime as described in §6.3.,7.1 Experimental Setup,[0],[0]
"However, our training data now consists of a combination of all available training data for all 4 languages.",7.1 Experimental Setup,[0],[0]
"In order to enable the model to differentiate between the tasks,
we prepend one language-specific input symbol to each instance.",7.1 Experimental Setup,[0],[0]
This corresponds to having one embedding in the input which marks the task.,7.1 Experimental Setup,[0],[0]
"An example training instance for Yorem Nokki is
L=YN ko′koreyene 7→ ko′kore|ye|ne,
where L=YN indicates the language.",7.1 Experimental Setup,[0],[0]
Due to the previous high correlation between accuracy and F1 we only use accuracy on the word level as the evaluation metric for this experiment.,7.1 Experimental Setup,[0],[0]
"In Table 5, we show the results of the multi-lingual model, which was trained on all languages, compared to all individual models, as well as each respective best multi-task approach and data augmentation method.",7.2 Results and Discussion,[0],[0]
"The results differ among languages: Most remarkably, for both Wixarika and Nahuatl, the accuracy of the multi-lingual model is higher than the one of the single-language model.",7.2 Results and Discussion,[0],[0]
"This might be related to them being the languages with most training data available (cf. Table 3).
",7.2 Results and Discussion,[0],[0]
"Note, however, that even for the remaining two languages—Mexicanero and Yorem Nokki— we hardly lose accuracy when comparing the multi-lingual to the individual models.",7.2 Results and Discussion,[0],[0]
"Since we only use one model (instead of four), without increasing its size significantly, we thus reduce the amount of parameters by nearly 75%.",7.2 Results and Discussion,[0],[0]
"Work on morphological segmentation was started more than 6 decades ago (Harris, 1951).",8 Related Work,[0],[0]
"Since then, many approaches have been developed: In the realm of unsupervised methods, two important systems are LINGUISTICS (Goldsmith, 2001) and MORFESSOR (Creutz and Lagus, 2002).",8 Related Work,[0],[0]
"The latter was later extended to a semi-supervised version (Kohonen et al., 2010) in order to make use of the abundance of unlabeled data which is available for many languages.
",8 Related Work,[0],[0]
Ruokolainen et al. (2013) focused explicitly on low-resource scenarios and applied CRFs to morphological segmentation in several languages.,8 Related Work,[0],[0]
"They reported better results than earlier work, including semi-supervised approaches.",8 Related Work,[0],[0]
"In the following year, they extended their approach to be able to use unlabeled data as well, further improving performance (Ruokolainen et al., 2014).
",8 Related Work,[0],[0]
"Cotterell et al. (2015) trained a semi-Markov CRF (semi-CRF) (Sarawagi and Cohen, 2005) jointly on morphological segmentation, stemming and tagging.",8 Related Work,[0],[0]
"For the similar problem of Chinese word segmentation, Zhang and Clark (2008) trained a model jointly on part-of-speech tagging.",8 Related Work,[0],[0]
"However, we are not aware of any prior work on multi-task training or data augmentation for neural segmentation models.
",8 Related Work,[0],[0]
"In fact, the two only neural seq2seq approaches for morphological segmentation we know of focused on canonical segmentation (Cotterell et al., 2016) which differs from the surface segmentation task considered here in that it restores changes to the surface form of morphemes which occurred during word formation.",8 Related Work,[0],[0]
Kann et al. (2016) also used an encoder-decoder RNN and combined it with a neural reranker.,8 Related Work,[0],[0]
"While our model architecture was inspired by them, their model was purely supervised.",8 Related Work,[0],[0]
"Additionally, they did not investigate the applicability of their neural seq2seq model in low-resource settings or for polysynthetic languages.",8 Related Work,[0],[0]
Ruzsics and Samardzic (2017) extended the standard encoder-decoder architecture for canonical segmentation to contain a language model over segments and improved results.,8 Related Work,[0],[0]
"However, a big difference to our work is that they still used more than ten times as much training data as we have available for the indigenous Mexican languages we are working on here.
",8 Related Work,[0],[0]
"Another neural approach—this time for surface segmentation—was presented by Wang et al.
(2016).",8 Related Work,[0],[0]
"The authors, instead of using seq2seq models, treat the task as a sequence labeling problem and use LSTMs to classify every character either as the beginning, middle or end of a morpheme, or as a single-character morpheme.
",8 Related Work,[0],[0]
"Cross-lingual knowledge transfer via language tags was proposed for neural seq2seq models before, both for tasks that handle sequences of words (Johnson et al., 2017) and tasks that work on sequences of characters (Kann et al., 2017).",8 Related Work,[0],[0]
"However, to the best of our knowledge, we are the first to try such an approach for a morphological segmentation task.",8 Related Work,[0],[0]
"In many other areas of NLP, cross-lingual transfer has been applied successfully, e.g., in entity recognition (Wang and Manning, 2014), language modeling (Tsvetkov et al., 2016), or parsing (Cohen et al., 2011; Søgaard, 2011; Ammar et al., 2016).",8 Related Work,[0],[0]
"We first investigated the applicability of neural seq2seq models to morphological surface segmentation for polysynthetic languages in minimalresource settings, i.e., for considerably less than 1, 000 training instances.",9 Conclusion and Future Work,[0],[0]
"Although they are generally thought to require large amounts of training data, neural networks obtained an accuracy comparable to or higher than several strong baselines.
",9 Conclusion and Future Work,[0],[0]
"Subsequently, we proposed two novel multitask training approaches and two novel data augmentation methods to further increase the performance of our neural models.",9 Conclusion and Future Work,[0],[0]
"Adding those, we improved over the neural baseline for all languages, and for Mexicanero, Wixarika and Yorem Nokki our final models outperformed all baselines by up to 5.05% absolute accuracy or 3.40% F1.",9 Conclusion and Future Work,[0],[0]
"Furthermore, we explored cross-lingual transfer between our languages and reduced the amount of necessary model parameters by about 75%, while improving performance for some of the languages.
",9 Conclusion and Future Work,[0],[0]
"We publically release our datasets for morphological surface segmentation of the polysynthetic minimal-resource languages Mexicanero, Nahuatl, Wixarika and Norem Yokki.",9 Conclusion and Future Work,[0],[0]
"We would like to thank Paulina Grnarova, Rodrigo Nogueira and Ximena Gutierrez-Vasques for their helpful feedback.",Acknowledgments,[0],[0]
"Morphological segmentation for polysynthetic languages is challenging, because a word may consist of many individual morphemes and training data can be extremely scarce.",abstractText,[0],[0]
"Since neural sequence-to-sequence (seq2seq) models define the state of the art for morphological segmentation in high-resource settings and for (mostly) European languages, we first show that they also obtain competitive performance for Mexican polysynthetic languages in minimal-resource settings.",abstractText,[0],[0]
"We then propose two novel multi-task training approaches— one with, one without need for external unlabeled resources—, and two corresponding data augmentation methods, improving over the neural baseline for all languages.",abstractText,[0],[0]
"Finally, we explore cross-lingual transfer as a third way to fortify our neural model and show that we can train one single multi-lingual model for related languages while maintaining comparable or even improved performance, thus reducing the amount of parameters by close to 75%.",abstractText,[0],[0]
"We provide our morphological segmentation datasets for Mexicanero, Nahuatl, Wixarika and Yorem Nokki for future research.",abstractText,[0],[0]
Fortification of Neural Morphological Segmentation Models for Polysynthetic Minimal-Resource Languages,title,[0],[0]
"The increasing complexity of machine learning algorithms has driven a large amount of research in the area of hyperparameter optimization (HO) — see, e.g., (Hutter et al., 2015) for a review.",1. Introduction,[0],[0]
"The core idea is relatively simple: given a measure of interest (e.g. the misclassification error) HO methods use a validation set to construct a response function of the hyperparameters (such as the average loss on the validation set) and explore the hyperparameter space to seek an optimum.
",1. Introduction,[0],[0]
"Early approaches based on grid search quickly become impractical as the number of hyperparameters grows and are even outperformed by random search (Bergstra & Bengio, 2012).",1. Introduction,[0],[0]
"Given the high computational cost of evaluating the
1Computational Statistics and Machine Learning, Istituto Italiano di Tecnologia, Genoa, Italy 2Department of Computer Science, University College London, UK 3Department of Information Engineering, Università degli Studi di Firenze, Italy.",1. Introduction,[0],[0]
"Correspondence to: Luca Franceschi <luca.franceschi@iit.it>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
response function, Bayesian optimization approaches provide a natural framework and have been extensively studied in this context (Snoek et al., 2012; Swersky et al., 2013; Snoek et al., 2015).",1. Introduction,[0],[0]
"Related and faster sequential modelbased optimization methods have been proposed using random forests (Hutter et al., 2011) and tree Parzen estimators (Bergstra et al., 2011), scaling up to a few hundreds of hyperparameters (Bergstra et al., 2013).
",1. Introduction,[0],[0]
"In this paper, we follow an alternative direction, where gradient-based algorithms are used to optimize the performance on a validation set with respect to the hyperparameters (Bengio, 2000; Larsen et al., 1996).",1. Introduction,[0],[0]
"In this setting, the validation error should be evaluated at a minimizer of the training objective.",1. Introduction,[0],[0]
"However, in many current learning systems such as deep learning, the minimizer is only approximate.",1. Introduction,[0],[0]
"Domke (2012) specifically considered running an iterative algorithm, like gradient descent or momentum, for a given number of steps, and subsequently computing the gradient of the validation error by a back-optimization algorithm.",1. Introduction,[0],[0]
Maclaurin et al. (2015) considered reverse-mode differentiation of the response function.,1. Introduction,[0],[0]
"They suggested the idea of reversing parameter updates to achieve space efficiency, proposing an approximation capable of addressing the associated loss of information due to finite precision arithmetics.",1. Introduction,[0],[0]
"Pedregosa (2016) proposed the use of inexact gradients, allowing hyperparameters to be updated before reaching the minimizer of the training objective.",1. Introduction,[0],[0]
"Both (Maclaurin et al., 2015) and (Pedregosa, 2016) managed to optimize a number of hyperparameters in the order of one thousand.
",1. Introduction,[0],[0]
"In this paper, we illustrate two alternative approaches to compute the hypergradient (i.e., the gradient of the response function), which have different trade-offs in terms of running time and space requirements.",1. Introduction,[0],[0]
One approach is based on a Lagrangian formulation associated with the parameter optimization dynamics.,1. Introduction,[0],[0]
"It encompasses the reverse-mode differentiation (RMD) approach used by Maclaurin et al. (2015), where the dynamics corresponds to stochastic gradient descent with momentum.",1. Introduction,[0],[0]
We do not assume reversible parameter optimization dynamics.,1. Introduction,[0],[0]
A well-known drawback of RMD is its space complexity: we need to store the whole trajectory of training iterates in order to compute the hypergradient.,1. Introduction,[0],[0]
"An alternative approach that we consider overcomes this problem by computing
the hypergradient in forward-mode and it is efficient when the number of hyperparameters is much smaller than the number of parameters.",1. Introduction,[0],[0]
"To the best of our knowledge, the forward-mode has not been studied before in this context.
",1. Introduction,[0],[0]
"As we shall see, these two approaches have a direct correspondence to two classic alternative ways of computing gradients for recurrent neural networks (RNN) (Pearlmutter, 1995): the Lagrangian (reverse) way corresponds to back-propagation through time (Werbos, 1990), while the forward way corresponds to real-time recurrent learning (RTRL) (Williams & Zipser, 1989).",1. Introduction,[0],[0]
"As RTRL allows one to update parameters after each time step, the forward approach is suitable for real-time hyperparameter updates, which may significantly speed up the overall hyperparameter optimization procedure in the presence of large datasets.",1. Introduction,[0],[0]
We give experimental evidence that the real-time approach is efficient enough to allow for the automatic tuning of crucial hyperparameters in a deep learning model.,1. Introduction,[0],[0]
"In our experiments, we also explore constrained hyperparameter optimization, showing that it can be used effectively to detect noisy examples and to discover the relationships between different learning tasks.
",1. Introduction,[0],[0]
The paper is organized in the following manner.,1. Introduction,[0],[0]
In Section 2 we introduce the problem under study.,1. Introduction,[0],[0]
In Section 3.1 we derive the reverse-mode computation.,1. Introduction,[0],[0]
"In Section 3.2 we present the forward-mode computation of the hypergradient, and in Section 3.3 we introduce the idea of real-time hyperparameter updates.",1. Introduction,[0],[0]
In Section 4 we discuss the time and space complexity of these methods.,1. Introduction,[0],[0]
In Section 5 we present empirical results with both algorithms.,1. Introduction,[0],[0]
Finally in Section 6 we discuss our findings and highlight directions of future research.,1. Introduction,[0],[0]
We focus on training procedures based on the optimization of an objective function J(w) with respect to w (e.g. the regularized average training loss for a neural network with weights w).,2. Hyperparameter Optimization,[0],[0]
"We see the training procedure by stochastic gradient descent (or one of its variants like momentum, RMSProp, Adam, etc.)",2. Hyperparameter Optimization,[0],[0]
as a dynamical system with a state st ∈ Rd that collects weights and possibly accessory variables such as velocities and accumulated squared gradients.,2. Hyperparameter Optimization,[0],[0]
"The dynamics are defined by the system of equations
st = Φt(st−1, λ) t = 1, . . .",2. Hyperparameter Optimization,[0],[0]
", T (1)
where T is the number of iterations, s0 contains initial weights and initial accessory variables, and, for every t ∈ {1, . . .",2. Hyperparameter Optimization,[0],[0]
", T},
Φt : (Rd × Rm)→",2. Hyperparameter Optimization,[0],[0]
"Rd
is a smooth mapping that represents the operation performed by the t-th step of the optimization algorithm (i.e.
on mini-batch t).",2. Hyperparameter Optimization,[0],[0]
"Finally, λ ∈ Rm is the vector of hyperparameters that we wish to tune.
",2. Hyperparameter Optimization,[0],[0]
"As simple example of these dynamics occurs when training a neural network by gradient descent with momentum (GDM), in which case st = (vt, wt) and
vt = µvt−1 +∇Jt(wt−1) wt = wt−1 − η(µvt−1 −∇Jt(wt−1))
(2)
where Jt is the objective associated with the t-th minibatch, µ is the rate and η is the momentum.",2. Hyperparameter Optimization,[0],[0]
"In this example, λ = (µ, η).
",2. Hyperparameter Optimization,[0],[0]
"Note that the iterates s1, . . .",2. Hyperparameter Optimization,[0],[0]
", sT implicitly depend on the vector of hyperparameters λ.",2. Hyperparameter Optimization,[0],[0]
Our goal is to optimize the hyperparameters according to a certain error function E evaluated at the last iterate sT .,2. Hyperparameter Optimization,[0],[0]
"Specifically, we wish to solve the problem
min λ∈Λ f(λ) (3)
where the set Λ ⊂ Rd incorporates constraints on the hyperparameters, and the response function f : Rm → R is defined at λ ∈",2. Hyperparameter Optimization,[0],[0]
"Rm as
f(λ) = E(sT (λ)).",2. Hyperparameter Optimization,[0],[0]
"(4)
We highlight the generality of the framework.",2. Hyperparameter Optimization,[0],[0]
"The vector of hyperparameters λ may include components associated with the training objective, and components associated with the iterative algorithm.",2. Hyperparameter Optimization,[0],[0]
"For example, the training objective may depend on hyperparameters used to design the loss function as well as multiple regularization parameters.",2. Hyperparameter Optimization,[0],[0]
"Yet other components of λ may be associated with the space of functions used to fit the training objective (e.g. number of layers and weights of a neural network, parameters associated with the kernel function used within a kernel based method, etc.).",2. Hyperparameter Optimization,[0],[0]
The validation error E can in turn be of different kinds.,2. Hyperparameter Optimization,[0],[0]
The simplest example is to choose E as the average of a loss function over a validation set.,2. Hyperparameter Optimization,[0],[0]
"We may however consider multiple validation objectives, in that the hyperparameters associated with the iterative algorithm (µ and γ in the case of momentum mentioned above) may be optimized using the training set, whereas the regularization parameters would typically require a validation set, which is distinct from the training set (in order to avoid over-fitting).",2. Hyperparameter Optimization,[0],[0]
"In this section, we review the reverse-mode computation of the gradient of the response function (or hypergradient) under a Lagrangian perspective and introduce a forwardmode strategy.",3. Hypergradient Computation,[0],[0]
"These procedures correspond to the reversemode and the forward-mode algorithmic differentiation schemes (Griewank & Walther, 2008).",3. Hypergradient Computation,[0],[0]
"We finally introduce a real-time version of the forward-mode procedure.
",3. Hypergradient Computation,[0],[0]
"Algorithm 1 REVERSE-HG Input: λ current values of the hyperparameters, s0 initial optimization state Output: Gradient of validation error w.r.t.",3. Hypergradient Computation,[0],[0]
λ for t,3. Hypergradient Computation,[0],[0]
"= 1 to T do st = Φt(st−1, λ)
end for αT =",3. Hypergradient Computation,[0],[0]
∇E(sT ) g = 0 for t = T − 1 downto 1 do g = g + αt+1Bt+1 αt = αt+1At+1 end for return g,3. Hypergradient Computation,[0],[0]
"The reverse-mode computation leads to an algorithm closely related to the one presented in (Maclaurin et al., 2015).",3.1. Reverse-Mode,[0],[0]
A major difference with respect to their work is that we do not require the mappings Φt defined in Eq.,3.1. Reverse-Mode,[0],[0]
(1) to be invertible.,3.1. Reverse-Mode,[0],[0]
"We also note that the reverse-mode calculation is structurally identical to back-propagation through time (Werbos, 1990).
",3.1. Reverse-Mode,[0],[0]
"We start by reformulating problem (3) as the constrained optimization problem
min λ,s1,...,sT
E(sT )
subject to st = Φt(st−1, λ), t ∈ {1, . . .",3.1. Reverse-Mode,[0],[0]
", T}.",3.1. Reverse-Mode,[0],[0]
"(5)
This formulation closely follows a classical Lagrangian approach used to derive the back-propagation algorithm (LeCun, 1988).",3.1. Reverse-Mode,[0],[0]
"Furthermore, the framework naturally allows one to incorporate constraints on the hyperparameters.
",3.1. Reverse-Mode,[0],[0]
"The Lagrangian of problem (5) is
L(s, λ, α) = E(sT ) + T∑ t=1 αt(Φt(st−1, λ)− st) (6)
where, for each t ∈ {1, . . .",3.1. Reverse-Mode,[0],[0]
", T}, αt ∈",3.1. Reverse-Mode,[0],[0]
"Rd is a row vector of Lagrange multipliers associated with the t-th step of the dynamics.
",3.1. Reverse-Mode,[0],[0]
"The partial derivatives of the Lagrangian are given by
∂L ∂αt",3.1. Reverse-Mode,[0],[0]
"= Φt(st−1, λ)− st, t ∈ {1, . . .",3.1. Reverse-Mode,[0],[0]
", T} (7) ∂L ∂st = αt+1At+1",3.1. Reverse-Mode,[0],[0]
"− αt, t ∈ {1, . . .",3.1. Reverse-Mode,[0],[0]
", T−1} (8) ∂L ∂sT = ∇E(sT )− αT (9)
",3.1. Reverse-Mode,[0],[0]
∂L ∂λ =,3.1. Reverse-Mode,[0],[0]
"T∑ t=1 αtBt, (10)
",3.1. Reverse-Mode,[0],[0]
"Algorithm 2 FORWARD-HG Input: λ current values of the hyperparameters, s0 initial optimization state Output: Gradient of validation error w.r.t.",3.1. Reverse-Mode,[0],[0]
λ,3.1. Reverse-Mode,[0],[0]
"Z0 = 0 for t = 1 to T do st = Φt(st−1, λ)",3.1. Reverse-Mode,[0],[0]
"Zt = AtZt−1 +Bt
end for return ∇E(s)ZT
where for every t ∈ {1, . . .",3.1. Reverse-Mode,[0],[0]
", T}, we define the matrices
At = ∂Φt(st−1, λ)
∂st−1 , Bt =
∂Φt(st−1, λ)
",3.1. Reverse-Mode,[0],[0]
∂λ .,3.1. Reverse-Mode,[0],[0]
"(11)
Note that At ∈ Rd×d and Bt ∈ Rd×m.
",3.1. Reverse-Mode,[0],[0]
The optimality conditions are then obtained by setting each derivative to zero.,3.1. Reverse-Mode,[0],[0]
"In particular, setting the right hand side of Equations (8) and (9) to zero gives
αt =  ∇E(sT ) if t = T,
∇E(sT )AT · · ·At+1 if t ∈ {1, . . .",3.1. Reverse-Mode,[0],[0]
", T−1}.
",3.1. Reverse-Mode,[0],[0]
Combining these equations with Eq.,3.1. Reverse-Mode,[0],[0]
"(10) we obtain that
∂L ∂λ =",3.1. Reverse-Mode,[0],[0]
"∇E(sT ) T∑ t=1
( T∏
s=t+1
As ) Bt.
",3.1. Reverse-Mode,[0],[0]
As we shall see this coincides with the expression for the gradient of f in Eq.,3.1. Reverse-Mode,[0],[0]
(15) derived in the next section.,3.1. Reverse-Mode,[0],[0]
Pseudo-code of REVERSE-HG is presented in Algorithm 1.,3.1. Reverse-Mode,[0],[0]
"The second approach to compute the hypergradient appeals to the chain rule for the derivative of composite functions, to obtain that the gradient of f at λ satisfies1
∇f(λ) = ∇E(sT ) dsT",3.2. Forward-Mode,[0],[0]
"dλ
(12)
where dsTdλ is the d×mmatrix formed by the total derivative of the components of sT (regarded as rows) with respect to the components of λ (regarded as columns).
",3.2. Forward-Mode,[0],[0]
"Recall that st = Φt(st−1, λ).",3.2. Forward-Mode,[0],[0]
The operators Φt depends on the hyperparameter λ both directly by its expression and indirectly through the state st−1.,3.2. Forward-Mode,[0],[0]
"Using again the chain
1Remember that the gradient of a scalar function is a row vector.
rule we have, for every t ∈ {1, . . .",3.2. Forward-Mode,[0],[0]
", T}, that
dst dλ = ∂Φt(st−1, λ)
",3.2. Forward-Mode,[0],[0]
"∂st−1
dst−1 dλ + ∂Φt(st−1, λ)",3.2. Forward-Mode,[0],[0]
∂λ .,3.2. Forward-Mode,[0],[0]
"(13)
Defining Zt = dstdλ for every t ∈ {1, . . .",3.2. Forward-Mode,[0],[0]
", T} and recalling Eq.",3.2. Forward-Mode,[0],[0]
"(11), we can rewrite Eq.",3.2. Forward-Mode,[0],[0]
"(13) as the recursion
Zt = AtZt−1 +Bt, t ∈ {1, . . .",3.2. Forward-Mode,[0],[0]
", T}.",3.2. Forward-Mode,[0],[0]
"(14)
Using Eq. (14), we obtain that
∇f(λ) =",3.2. Forward-Mode,[0],[0]
∇E(sT )ZT = ∇E(sT )(ATZT−1 +BT ) =,3.2. Forward-Mode,[0],[0]
"∇E(sT )(ATAT−1ZT−2 +ATBT−1 +BT ) ...
= ∇E(sT ) T∑ t=1
( T∏
s=t+1
As ) Bt. (15)
Note that the recurrence (14) on the Jacobian matrix is structurally identical to the recurrence in the RTRL procedure described in (Williams & Zipser, 1989, eq. (2.10)).
",3.2. Forward-Mode,[0],[0]
From the above derivation it is apparent that∇f(λ) can be computed by an iterative algorithm which runs in parallel to the training algorithm.,3.2. Forward-Mode,[0],[0]
Pseudo-code of FORWARD-HG is presented in Algorithm 2.,3.2. Forward-Mode,[0],[0]
"At first sight, the computation of the terms in the right hand side of Eq.",3.2. Forward-Mode,[0],[0]
(14) seems prohibitive.,3.2. Forward-Mode,[0],[0]
"However, in Section 4 we observe that if m is much smaller than d, the computation can be done efficiently.",3.2. Forward-Mode,[0],[0]
"For every t ∈ {1, . . .",3.3. Real-Time Forward-Mode,[0],[0]
", T} let ft : Rm → R be the response function at time t: ft(λ) = E(st(λ)).",3.3. Real-Time Forward-Mode,[0],[0]
Note that fT coincides with the definition of the response function in Eq.,3.3. Real-Time Forward-Mode,[0],[0]
(4).,3.3. Real-Time Forward-Mode,[0],[0]
"A major difference between REVERSE-HG and FORWARD-HG is that the partial hypergradients
∇ft(λ) = dE(st)
dλ = ∇E(st)Zt (16)
are available in the second procedure at each time step t and not only at the end.
",3.3. Real-Time Forward-Mode,[0],[0]
"The availability of partial hypergradients is significant since we are allowed to update hyperparameters several times in a single optimization epoch, without having to wait until time T .",3.3. Real-Time Forward-Mode,[0],[0]
This is reminiscent of the real-time updates suggested by Williams & Zipser (1989) for RTRL.,3.3. Real-Time Forward-Mode,[0],[0]
"The real-time approach may be suitable in the case of a data stream (i.e. T = ∞), where REVERSE-HG would be hardly applicable.",3.3. Real-Time Forward-Mode,[0],[0]
"Even in the case of finite (but large) datasets it is possible to perform one hyperparameter update after a hyper-batch of data (i.e. a set of minibatches)
has been processed.",3.3. Real-Time Forward-Mode,[0],[0]
Algorithm 2 can be easily modified to yield a partial hypergradient when t mod ∆ = 0,3.3. Real-Time Forward-Mode,[0],[0]
"(for some hyper-batch size ∆) and letting t run from 1 to ∞, reusing examples in a circular or random way.",3.3. Real-Time Forward-Mode,[0],[0]
We use this strategy in the phone recognition experiment reported in Section 5.3.,3.3. Real-Time Forward-Mode,[0],[0]
We discuss the time and space complexity of Algorithms 1 and 2.,4. Complexity Analysis,[0],[0]
"We begin by recalling some basic results from the algorithmic differentiation (AD) literature.
",4. Complexity Analysis,[0],[0]
Let F :,4. Complexity Analysis,[0],[0]
Rn 7→,4. Complexity Analysis,[0],[0]
"Rp be a differentiable function and suppose it can be evaluated in time c(n, p) and requires space s(n, p).",4. Complexity Analysis,[0],[0]
Denote by JF the p × n Jacobian matrix of F .,4. Complexity Analysis,[0],[0]
"Then the following facts hold true (Griewank & Walther, 2008) (see also Baydin et al. (2015) for a shorter account):
(i) For any vector r ∈ Rn, the product JF r can be evaluated in time O(c(n, p)) and requires space O(s(n, p)) using forward-mode AD.
(ii) For any vector q ∈",4. Complexity Analysis,[0],[0]
"Rp, the product JᵀF q has time and space complexities O(c(n, p)) using reverse-mode AD.
",4. Complexity Analysis,[0],[0]
"(iii) As a corollary of item (i), the whole JF can be computed in time O(nc(n, p)) and requires space O(s(n, p)) using forward-mode AD (just use unitary vectors r = ei for i = 1, . . .",4. Complexity Analysis,[0],[0]
",",4. Complexity Analysis,[0],[0]
"n).
",4. Complexity Analysis,[0],[0]
"(iv) Similarly, JF can be computed in time O(pc(n, p)) and requires space O(c(n, p)) using reverse-mode AD.
",4. Complexity Analysis,[0],[0]
"Let g(d,m) and h(d,m) denote time and space, respectively, required to evaluate the update map Φt defined by Eq.",4. Complexity Analysis,[0],[0]
(1).,4. Complexity Analysis,[0],[0]
Then the response function f :,4. Complexity Analysis,[0],[0]
Rm 7→ R defined in Eq.,4. Complexity Analysis,[0],[0]
"(3) can be evaluated in timeO(Tg(d,m))",4. Complexity Analysis,[0],[0]
"(assuming the time required to compute the validation errorE(λ) does not affect the bound2) and requires spaceO(h(d,m)) since variables st may be overwritten at each iteration.",4. Complexity Analysis,[0],[0]
"Then, a direct application of Fact (i) above shows that Algorithm 2 runs in time O(Tmg(d,m)) and space O(h(d,m)).",4. Complexity Analysis,[0],[0]
"The same results can also be obtained by noting that in Algorithm 2 the product AtZt−1 requires m Jacobian-vector products, each costing O(g(d,m)) (from Fact (i)), while computing the Jacobian Bt takes time O(mg(d,m))",4. Complexity Analysis,[0],[0]
"(from Fact (iii)).
",4. Complexity Analysis,[0],[0]
"Similarly, a direct application of Fact (ii) shows that Algorithm 1 has both time and space complexities O(Tg(d,m)).",4. Complexity Analysis,[0],[0]
"Again the same results can be obtained by
2This is indeed realistic since the number of validation examples is typically lower than the number of training iterations.
noting that αt+1At1 and αtBt are transposed-Jacobianvector products that in reverse-mode take both time O(g(d,m))",4. Complexity Analysis,[0],[0]
(from Fact (ii)).,4. Complexity Analysis,[0],[0]
"Unfortunately in this case variables st cannot be overwritten, explaining the much higher space requirement.
",4. Complexity Analysis,[0],[0]
"As an example, consider training a neural network with k weights3, using classic iterative optimization algorithms such as SGD (possibly with momentum) or Adam, where the hyperparameters are just learning rate and momentum terms.",4. Complexity Analysis,[0],[0]
"In this case, d = O(k) and m = O(1).",4. Complexity Analysis,[0],[0]
"Moreover, g(d,m) and h(d,m) are both O(k).",4. Complexity Analysis,[0],[0]
"As a result, Algorithm 1 runs in time and space O(Tk), while Algorithm 2 runs in timeO(Tk) and spaceO(k), which would typically make a dramatic difference in terms of memory requirements.",4. Complexity Analysis,[0],[0]
"In this section, we present numerical simulations with the proposed methods.",5. Experiments,[0],[0]
All algorithms were implemented in TensorFlow and the software package used to reproduce our experiments is available at https://github.,5. Experiments,[0],[0]
com/lucfra/RFHO.,5. Experiments,[0],[0]
"In all the experiments, hypergradients were used inside the Adam algorithm (Kingma & Ba, 2014) in order to minimize the response function.",5. Experiments,[0],[0]
The goal of this experiment is to highlight one potential advantage of constraints on the hyperparameters.,5.1. Data Hyper-cleaning,[0],[0]
Suppose we have a dataset with label noise and due to time or resource constraints we can only afford to cleanup (by checking and correcting the labels) a subset of the available data.,5.1. Data Hyper-cleaning,[0],[0]
"Then we may use the cleaned data as the validation set, the rest as the training set, and assign one hyperparameter to each training example.",5.1. Data Hyper-cleaning,[0],[0]
"By putting a sparsity constraint on the vector of hyperparameters λ, we hope to bring to zero the influence of noisy examples, in order to generate a better model.",5.1. Data Hyper-cleaning,[0],[0]
"While this is the same kind of data sparsity observed in support vector machines (SVM), our setting aims to get rid of erroneously labeled examples, in contrast to SVM which puts zero weight on redundant examples.",5.1. Data Hyper-cleaning,[0],[0]
"Although this experimental setup does not necessarily reflect a realistic scenario, it aims to test the ability of our HO method to effectively make use of constraints on the hyperparameters4
We instantiated the above setting with a balanced subset of N = 20000 examples from the MNIST dataset, split into three subsets:",5.1. Data Hyper-cleaning,[0],[0]
"Dtr of Ntr = 5000 training examples, V of
3This includes linear SVM and logistic regression as special cases.
",5.1. Data Hyper-cleaning,[0],[0]
"4We note that a related approach based on reinforcement learning is presented in (Fan et al., 2017).
",5.1. Data Hyper-cleaning,[0],[0]
Nval = 5000 validation examples and a test set containing the remaining samples.,5.1. Data Hyper-cleaning,[0],[0]
"Finally, we corrupted the labels of 2500 training examples, selecting a random subset Df ⊂ Dtr.
We considered a plain softmax regression model with parameters W (weights) and b (bias).",5.1. Data Hyper-cleaning,[0],[0]
"The error of a model (W, b) on an example x was evaluated by using the crossentropy `(W, b, x) both in the training objective function, Etr, and in the validation one, Eval.",5.1. Data Hyper-cleaning,[0],[0]
We added in Etr an hyperparameter vector λ ∈,5.1. Data Hyper-cleaning,[0],[0]
"[0, 1]Ntr that weights each example in the training phase, i.e. Etr(W, b) =
1 Ntr ∑ i∈Dtr λi`(W, b, xi).
",5.1. Data Hyper-cleaning,[0],[0]
"According to the general HO framework, we fit the parameters (W, b) to minimize the training loss and the hyperparameters λ to minimize the validation error.",5.1. Data Hyper-cleaning,[0],[0]
"The sparsity constraint was implemented by bounding the L1-norm of λ, resulting in the optimization problem
min λ∈Λ Eval(WT , bT ) (PHO)
where Λ = {λ : λ ∈",5.1. Data Hyper-cleaning,[0],[0]
"[0, 1]Ntr , ‖λ‖1 ≤ R} and (WT , bT ) are the parameters obtained after T iterations of gradient descent on the training objective.",5.1. Data Hyper-cleaning,[0],[0]
"Given the high dimensionality of λ, we solved (PHO) iteratively computing the hypergradients with REVERSE-HG method and projecting Adam updates on the set Λ.
We are interested in comparing the following three test set accuracies:
• Oracle: the accuracy of the minimizer of Etr trained on clean examples only, i.e. (Dtr\Df )∪V; this setting is effectively taking advantage of an oracle that tells which examples have a wrong label;
• Baseline: the accuracy of the minimizer ofEtr trained on all available data D ∪ V;
• DH-R: the accuracy of the data hyper-cleaner with a given value of the L1 radius, R.",5.1. Data Hyper-cleaning,[0],[0]
"In this case, we first optimized hyperparameters",5.1. Data Hyper-cleaning,[0],[0]
and then constructed a cleaned training set Dc ⊂,5.1. Data Hyper-cleaning,[0],[0]
"Dtr (keeping examples with λi > 0); we finally trained on Dc ∪ V .
",5.1. Data Hyper-cleaning,[0],[0]
We are also interested in evaluating the ability of the hypercleaner to detect noisy samples.,5.1. Data Hyper-cleaning,[0],[0]
Results are shown in Table 1.,5.1. Data Hyper-cleaning,[0],[0]
"The data hyper-cleaner is robust with respect to the choice of R and is able to identify corrupted examples, recovering a model that has almost the same accuracy as a model produced with the help of an oracle.
",5.1. Data Hyper-cleaning,[0],[0]
Figure 1 shows how the accuracy of DH-1000 improves with the number of hyper-iterations and the progression of the amount of discarded examples.,5.1. Data Hyper-cleaning,[0],[0]
"The data hyper-cleaner starts by discarding mainly corrupted examples, and while
0 100 200 300 400 500
Hyper-iterations
0
500
1000
1500
2000
2500
3000
3500
N u
m b
er of
d is
ca rd
ed ex
am p
le s
80
82
84
86
88
",5.1. Data Hyper-cleaning,[0],[0]
"90
92
A cc
u ra
cy
N u
m b
er of
d is
ca rd
ed ex
am p
le s
Accuracy and sparsity of λ
Validation
Test
TP
FP
Figure 1:",5.1. Data Hyper-cleaning,[0],[0]
Right vertical axis: accuracies of DH-1000 on validation and test sets.,5.1. Data Hyper-cleaning,[0],[0]
"Left vertical axis: number of discarded examples among noisy (True Positive, TP) and clean (False Positive, FP) ones.
",5.1. Data Hyper-cleaning,[0],[0]
"the optimization proceeds, it begins to remove also a portion of cleaned one.",5.1. Data Hyper-cleaning,[0],[0]
"Interestingly, the test set accuracy continues to improve even when some of the clean examples are discarded.",5.1. Data Hyper-cleaning,[0],[0]
"This second set of experiments is in the multitask learning (MTL) context, where the goal is to find simultaneously the model of multiple related tasks.",5.2. Learning Task Interactions,[0],[0]
Many MTL methods require that a task interaction matrix is given as input to the learning algorithm.,5.2. Learning Task Interactions,[0],[0]
"However, in real applications, this matrix is often unknown and it is interesting to learn it from data.",5.2. Learning Task Interactions,[0],[0]
"Below, we show that our framework can be naturally applied to learning the task relatedness matrix.
",5.2. Learning Task Interactions,[0],[0]
"We used CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009), two object recognition datasets with 10 and 100 classes, respectively.",5.2. Learning Task Interactions,[0],[0]
As features we employed the preactivation of the second last layer of Inception-V3 model trained on ImageNet5.,5.2. Learning Task Interactions,[0],[0]
"From CIFAR-10, we extracted 50
5Available at tinyurl.com/h2x8wws
examples as training set, different 50 examples as validation set and the remaining for testing.",5.2. Learning Task Interactions,[0],[0]
"From CIFAR-100, we selected 300 examples as training set, 300 as validation set and the remaining for testing.",5.2. Learning Task Interactions,[0],[0]
"Finally, we used a onehot encoder of the labels obtaining a set of labels in {0, 1}K (K = 10 or K = 100).
",5.2. Learning Task Interactions,[0],[0]
The choice of small training set sizes is due to the strong discriminative power of the selected features.,5.2. Learning Task Interactions,[0],[0]
"In fact, using larger sample sizes would not allow to appreciate the advantage of MTL.",5.2. Learning Task Interactions,[0],[0]
"In order to leverage information among the different classes, we employed a multitask learning (MTL) regularizer (Evgeniou et al., 2005)
ΩC,ρ(W ) = K∑ j,k=1 Cj,k‖wj − wk‖22 + ρ K∑ k=1 ‖wk‖2,
where wk are the weights for class k, K is the number of classes, and the symmetric non-negative matrix C models the interactions between the classes/tasks.",5.2. Learning Task Interactions,[0],[0]
"We used a regularized training error defined as Etr(W ) =∑ i∈Dtr `(Wxi + b, yi) + ΩC,ρ(W )",5.2. Learning Task Interactions,[0],[0]
"where `(·, ·) is the categorical cross-entropy and b = (b1, . . .",5.2. Learning Task Interactions,[0],[0]
", bK) is the vector of thresholds associated with each linear model.",5.2. Learning Task Interactions,[0],[0]
"We wish solve the following optimization problem:
min { Eval(WT , bT ) subject to ρ ≥ 0, C = Cᵀ, C ≥ 0 } ,
where (WT , bT ) is the T -th iteration obtained by running gradient descent with momentum (GDM) on the training objective.",5.2. Learning Task Interactions,[0],[0]
"We solve this problem using REVERSE-HG and optimizing the hyperparameters by projecting Adam updates on the set {(ρ, C) : ρ ≥ 0, C = Cᵀ, C ≥ 0}.",5.2. Learning Task Interactions,[0],[0]
"We compare the following methods:
• SLT: single task learning, i.e. C = 0, using a validation set to tune the optimal value of ρ for each task;
• NMTL: we considered the naive MTL scenario in which the tasks are equally related, that is Cj,k = a for every 1 ≤ j, k ≤",5.2. Learning Task Interactions,[0],[0]
K.,5.2. Learning Task Interactions,[0],[0]
"In this case we learn the two non-negative hyperparameters a and ρ;
• HMTL:",5.2. Learning Task Interactions,[0],[0]
"our hyperparameter optimization method REVERSE-HG to tune C and ρ;
• HMTL-S: Learning the matrix C with only few examples per class could bring the discovery of spurious relationships.",5.2. Learning Task Interactions,[0],[0]
"We try to remove this effect by imposing the constraint that ∑ j,k Cj,k ≤ R, where6 R = 10−3.
",5.2. Learning Task Interactions,[0],[0]
"In this case, Adam updates are projected onto the set {(ρ, C) : ρ ≥ 0, C = Cᵀ, C ≥ 0, ∑ j,k Cj,k ≤ R}.
",5.2. Learning Task Interactions,[0],[0]
Results of five repetitions with different splits are presented in Table 2.,5.2. Learning Task Interactions,[0],[0]
"Note that HMTL gives a visible improvement in
6We observed that R = 10−4 yielded very similar results.
performance, and adding the constraint that ∑ j,k Cj,k ≤ R further improves performance in both datasets.",5.2. Learning Task Interactions,[0],[0]
"The matrix C can been interpreted as an adjacency matrix of a graph, highlighting the relationships between the classes.",5.2. Learning Task Interactions,[0],[0]
"Figure 2 depicts the graph for CIFAR-10 extracted from the algorithm HMTL-S. Although this result is strongly influenced by the choice of the data representations, we can note that animals tends to be more related to themselves than to vehicles and vice versa.",5.2. Learning Task Interactions,[0],[0]
The aim of the third set of experiments is to assess the efficacy of the real-time FORWARD-HG algorithm (RTHO).,5.3. Phone Classification,[0],[0]
"We run experiments on phone recognition in the multitask framework proposed in (Badino, 2016, and references therein).",5.3. Phone Classification,[0],[0]
"Data for all experiments was obtained from the TIMIT phonetic recognition dataset (Garofolo et al., 1993).",5.3. Phone Classification,[0],[0]
The dataset contains 5040 sentences corresponding to around 1.5 million speech acoustic frames.,5.3. Phone Classification,[0],[0]
"Training, validation and test sets contain respectively 73%, 23% and 4% of the data.",5.3. Phone Classification,[0],[0]
The primary task is a frame-level phone state classification with 183 classes and it consists in learning a mapping fP from acoustic speech vectors to hidden Markov model monophone states.,5.3. Phone Classification,[0],[0]
"Each 25ms speech frame is represented by a 123-dimensional vector containing 40 Mel frequency scale cepstral coefficients and energy, augmented with their deltas and delta-deltas.",5.3. Phone Classification,[0],[0]
We used a window of eleven frames centered around the prediction target to create the 1353-dimensional input to fP .,5.3. Phone Classification,[0],[0]
"The secondary (or auxiliary) task consists in learning a mapping fS from acoustic vectors to 300-dimensional real vectors of contextdependent phonetic embeddings defined in (Badino, 2016).
",5.3. Phone Classification,[0],[0]
"As in previous work, we assume that the two mappings fP and fS share inputs and an intermediate representation, obtained by four layers of a feed-forward neural network with 2000 units on each layer.",5.3. Phone Classification,[0],[0]
We denote by W the parameter vector of these four shared layers.,5.3. Phone Classification,[0],[0]
The network has two different output layers with parameter vectorsWP andWS each relative to the primary and secondary task.,5.3. Phone Classification,[0],[0]
"The network is trained to jointly minimize Etot(W,WP ,WS) = EP (W,W P )+ρES(W,W S), where the primary error EP is the average cross-entropy loss on the primary task, the secondary error ES is given by mean squared error on the embedding vectors and ρ ≥ 0 is a design hyperparameter.",5.3. Phone Classification,[0],[0]
"Since we are ultimately interested in learning fP , we formulate the hyperparameter optimization problem as
min { Eval(WT ,W P T ) subject to ρ, η ≥ 0, 0 ≤ µ ≤ 1 } ,
where Eval is the cross entropy loss computed on a validation set after T iterations of stochastic GDM, and η and µ are defined in (2).",5.3. Phone Classification,[0],[0]
In all the experiments we fix a minibatch size of 500.,5.3. Phone Classification,[0],[0]
"We compare the following methods:
1.",5.3. Phone Classification,[0],[0]
"Vanilla: the secondary target is ignored (ρ = 0); η and µ are set to 0.075 and 0.5 respectively as in (Badino, 2016).
2.",5.3. Phone Classification,[0],[0]
"RS: random search with ρ ∼ U(0, 4), η ∼ E(0.1) (exponential distribution with scale parameter 0.1) and µ ∼ U(0, 1) (Bergstra & Bengio, 2012).
3.",5.3. Phone Classification,[0],[0]
"RTHO: real-time hyperparameter optimization with initial learning rate and momentum factor as in Vanilla and initial ρ set to 1.6 (best value obtained by gridsearch in Badino (2016)).
",5.3. Phone Classification,[0],[0]
4.,5.3. Phone Classification,[0],[0]
"RTHO-NT: RTHO with “null teacher,” i.e. when the initial values of ρ, η and µ are set to 0.",5.3. Phone Classification,[0],[0]
"We regard this experiment as particularly interesting: this initial setting, while clearly not optimal, does not require any background knowledge on the task at hand.
",5.3. Phone Classification,[0],[0]
"We also tried to run FORWARD-HG for a fixed number of epochs, not in real-time mode.",5.3. Phone Classification,[0],[0]
"Results are not reported
since the method could not make any appreciable progress after running 24 hours on a Titan X GPU.
Test accuracies and execution times are reported in Table 3.",5.3. Phone Classification,[0],[0]
Figure 3 shows learning curves and hyperparameter evolutions for RTHO-NT.,5.3. Phone Classification,[0],[0]
"In Experiments 1 and 2 we employ a standard early stopping procedure on the validation accuracy, while in Experiments 3 and 4 a natural stopping time is given by the decay to 0 of the learning rate (see Figure 3 left-bottom plot).",5.3. Phone Classification,[0],[0]
"In Experiments 3 and 4 we used a hyperbatch size of ∆ = 200 (see Eq. (16)) and a hyper-learning rate of 0.005.
",5.3. Phone Classification,[0],[0]
"The best results in Table 3 are very similar to those obtained in state-of-the-art recognizers using multitask learning (Badino, 2016; 2017).",5.3. Phone Classification,[0],[0]
"In spite of the small number of hyperparameters, random search yields results only slightly better than the vanilla network (the result reported in Table 3 are an average over 5 trials, with a minimum and maximum accuracy of 59.93 and 60.86, respectively).",5.3. Phone Classification,[0],[0]
"Within the same time budget of 300 minutes, RTHO-NT is able to find hyperparameters yielding a substantial improvement over the vanilla version, thus effectively exploiting the auxiliary task.",5.3. Phone Classification,[0],[0]
Note that the model trained has more that 15×106 parameters for a corresponding state of more than 30× 106 variables.,5.3. Phone Classification,[0],[0]
"To the best of our knowledge, reversemode (Maclaurin et al., 2015) or approximate (Pedregosa, 2016) methods have not been applied to models of this size.",5.3. Phone Classification,[0],[0]
"We studied two alternative strategies for computing the hypergradients of any iterative differentiable learning dynam-
ics.",6. Discussion,[0],[0]
"Previous work has mainly focused on the reverse-mode computation, attempting to deal with its space complexity, that becomes prohibitive for very large models such as deep networks.
",6. Discussion,[0],[0]
Our first contribution is the definition and the application of forward-mode computation to HO.,6. Discussion,[0],[0]
Our analysis suggests that for large models the forward-mode computation may be a preferable alternative to reverse-mode if the number of hyperparameters is small.,6. Discussion,[0],[0]
"Additionally, forward-mode is amenable to real-time hyperparameter updates, which we showed to be an effective strategy for large datasets (see Section 5.3).",6. Discussion,[0],[0]
"We showed experimentally that even starting from a far-from-optimal value of the hyperparameters (the null teacher), our RTHO algorithm finds good values at a reasonable cost, whereas other gradient-based algorithms could not be applied in this context.
",6. Discussion,[0],[0]
Our second contribution is the Lagrangian derivation of the reverse-mode computation.,6. Discussion,[0],[0]
"It provides a general framework to tackle hyperparameter optimization problems involving a wide class of response functions, including those that take into account the whole parameter optimization dynamics.",6. Discussion,[0],[0]
"We have also presented in Sections 5.1 and 5.2 two non-standard learning problems where we specifically take advantage of a constrained formulation of the HO problem.
",6. Discussion,[0],[0]
We close by highlighting some potential extensions of our framework and direction of future research.,6. Discussion,[0],[0]
"First, the relatively low cost of our RTHO algorithm could suggest to make it a standard tool for the optimization of real-valued critical hyperparameters (such as learning rates, regularization factors and error function design coefficient), in context where no previous or expert knowledge is available (e.g. novel domains).",6. Discussion,[0],[0]
"Yet, RTHO must be thoroughly validated on diverse datasets and with different models and settings to empirically asses its robustness and its ability to find good hyperparameter values.",6. Discussion,[0],[0]
"Second, in order to perform gradient-based hyperparameter optimization, it is necessary to set a descent procedure over the hyperparameters.",6. Discussion,[0],[0]
In our experiments we have always used Adam with a manually adjusted value for the hyperlearning rate.,6. Discussion,[0],[0]
Devising procedures which are adaptive in these hyper-hyperparameters is an important direction of future research.,6. Discussion,[0],[0]
"Third, extensions of gradient-based HO techniques to integer or nominal hyperparameters (such as the depth and the width of a neural network) require additional design efforts and may not arise naturally in our framework.",6. Discussion,[0],[0]
"Future research should instead focus on the integration of gradient-based algorithm with Bayesian optimization and/or with emerging reinforcement learning hyperparameter optimization approaches (Zoph & Le, 2016).",6. Discussion,[0],[0]
A final important problem is to study the converge properties of RTHO.,6. Discussion,[0],[0]
Results in Pedregosa (2016) may prove useful in this direction.,6. Discussion,[0],[0]
We study two procedures (reverse-mode and forward-mode) for computing the gradient of the validation error with respect to the hyperparameters of any iterative learning algorithm such as stochastic gradient descent.,abstractText,[0],[0]
These procedures mirror two methods of computing gradients for recurrent neural networks and have different trade-offs in terms of running time and space requirements.,abstractText,[0],[0]
Our formulation of the reverse-mode procedure is linked to previous work by Maclaurin et al. (2015) but does not require reversible dynamics.,abstractText,[0],[0]
"The forward-mode procedure is suitable for real-time hyperparameter updates, which may significantly speed up hyperparameter optimization on large datasets.",abstractText,[0],[0]
We present experiments on data cleaning and on learning task interactions.,abstractText,[0],[0]
We also present one large-scale experiment where the use of previous gradient-based methods would be prohibitive.,abstractText,[0],[0]
Forward and Reverse Gradient-Based Hyperparameter Optimization,title,[0],[0]
"Goal-oriented, information-retrieving dialogue systems have been designed traditionally to help users find items in a database given a set of constraints (Singh et al., 2002; Raux et al., 2003; El Asri et al., 2014; Laroche et al., 2011).",1 Introduction,[0],[0]
"For instance, the LET’S GO dialogue system finds a bus schedule given a bus number and a location (Raux et al., 2003).
",1 Introduction,[0],[0]
"Available resources for data-driven learning of such goal-oriented systems are often collected with an existing system (Henderson et al., 2014b; Bennett and Rudnicky, 2002) and have been proposed to study one component of dialogue.",1 Introduction,[0],[0]
"Examples are the first three Dialogue State Tracking Challenges (DSTC, Williams et al., 2016) during which a se-
ries of datasets and tasks of increasing complexity were released.",1 Introduction,[0],[0]
These shared tasks were essential to advance the state of the art on state tracking.,1 Introduction,[0],[0]
"Other resources have allowed to study and develop different approaches to spoken language understanding and entity extraction (Mesnil et al., 2013).",1 Introduction,[0],[0]
"As for dialogue management, simulators have been proposed (Schatzmann et al., 2006) but datasets are scarce.
",1 Introduction,[0],[0]
"In most datasets collected with an existing system, the dialogues consist of sequential slot-filling: the system requests constraints until it can query the database and return several results to the user.",1 Introduction,[0],[0]
"Then, the user can ask for more information about a given result or request other possibilities.",1 Introduction,[0],[0]
"As a consequence, the tasks and methods that were based on these datasets were defined according to this sequential slot-filling process
We propose the Frames dataset to study more complex dialogue flows and decision-making behaviour.",1 Introduction,[0],[0]
"Our motivation comes from user studies in e-commerce which show that several informationseeking behaviours are exhibited by users who may come with a very well defined item in mind, but may also visit an e-commerce website with the intent to compare items and explore different possibilities (Moe and Fader, 2001; Saha et al., 2017).",1 Introduction,[0],[0]
Supporting this kind of decision-making process in conversational systems implies adding memory.,1 Introduction,[0],[0]
Memory is necessary to track different items or preferences set by the user during the dialogue.,1 Introduction,[0],[0]
"For instance, consider product comparisons.",1 Introduction,[0],[0]
"If a user wants to compare different items using a dialogue system, then this system should be able to separately recall properties pertaining to each item.
",1 Introduction,[0],[0]
"We collected 1369 human-human dialogues in a Wizard-of-Oz (WOz) setting – i.e., users were paired up with humans, whom we refer to as wizards, who assumed the role of the dialogue system.",1 Introduction,[0],[0]
"Wizards were given access to a database of vaca-
tion packages containing round-trip flights and a hotel.",1 Introduction,[0],[0]
Users were tasked with finding packages based on a few constraints such as a destination and a budget.,1 Introduction,[0],[0]
"The dataset has been fully annotated by human experts and is publicly available1.
",1 Introduction,[0],[0]
"Along with this dataset, we formalize a new task called frame tracking.",1 Introduction,[0],[0]
"Frame tracking is an extension of state tracking (Henderson, 2015; Williams et al., 2016).",1 Introduction,[0],[0]
"In state tracking, the information summarizing the full dialogue history is compressed into a single semantic frame which contains properties and values corresponding to the user’s preferences (e.g., destination city).",1 Introduction,[0],[0]
"In frame tracking, the dialogue agent must simultaneously track multiple semantic frames (e.g., different destination cities; frames are defined formally in Section 4.2) throughout the conversation.",1 Introduction,[0],[0]
"We collected the Frames data over a period of 20 days with 12 participants, who worked either for one day, one week, or 20 days.",2 Data Collection,[0],[0]
The participants alternated between the user and wizard roles on a daily basis.,2 Data Collection,[0],[0]
"Due to this rotation, we can assume that we deal with returning users who know how to use the system, and focus on the decision making process, skipping the phase where the user learns about the system capabilities.",2 Data Collection,[0],[0]
"The domain for all dialogues is travel: specifically, finding a vacation package that fulfils certain a priori requirements through a conversational search-and-compare process.",2 Data Collection,[0],[0]
"Wizard-of-Oz (WOz) dialogues (Kelley, 1984; Rieser et al., 2005; Wen et al., 2016) have the considerable advantage of exhibiting realistic behaviours often beyond the capabilities of existing dialogue systems.",2.1 Wizard-Of-Oz Setting,[0],[0]
"Our setting is slightly different from the usual WOz setting because, in our case, users did not believe they were interacting with a dialogue system; they knew they were conversing with fellow humans.",2.1 Wizard-Of-Oz Setting,[0],[0]
"We chose not to give templated answers to wizards because, apart from studying decision-making, we also wanted to study information presentation and dialogue management.",2.1 Wizard-Of-Oz Setting,[0],[0]
"We work with text-based dialogues because this engenders a more controlled wizard behaviour, obviates handling time-sensitive turn taking, and speech recognition noise.
1datasets.maluuba.com/Frames",2.1 Wizard-Of-Oz Setting,[0],[0]
User-wizard dialogues took place on Slack.2 We deployed a Slack bot to pair up participants and record conversations.,2.2 Task Templates and Instructions,[0],[0]
"At the beginning of each dialogue, a user was paired with a wizard and given a new task.",2.2 Task Templates and Instructions,[0],[0]
"Tasks were built from templates like the following:
“Find a vacation between [START DATE] and",2.2 Task Templates and Instructions,[0],[0]
[END DATE] for [NUM ADULTS] adults and [NUM CHILDREN] kids.,2.2 Task Templates and Instructions,[0],[0]
You leave from [ORIGIN CITY].,2.2 Task Templates and Instructions,[0],[0]
"You are travelling on a budget and you would like to spend at most $[BUDGET].”
Tasks were generated by drawing values (e.g., for BUDGET) from a database.",2.2 Task Templates and Instructions,[0],[0]
We constructed our database of flight and hotel properties by hand to simulate what one would find on a standard travel booking site.,2.2 Task Templates and Instructions,[0],[0]
"Each template was assigned a probability of success, and then constraint values were drawn in order to comply with this probability.",2.2 Task Templates and Instructions,[0],[0]
"For example, if 20 tasks were generated at probability 0.5, about 10 tasks would be generated with successful database queries and the other 10 would be generated such that the database returned no results for the constraints.",2.2 Task Templates and Instructions,[0],[0]
This success mechanism allowed us to emulate cases when a user would find nothing meeting her constraints.,2.2 Task Templates and Instructions,[0],[0]
"If a task was unsuccessful, the user either ended the dialogue or got an alternative task such as: “If nothing matches your constraints, try increasing your budget by $200.”",2.2 Task Templates and Instructions,[0],[0]
We wrote 38 templates.,2.2 Task Templates and Instructions,[0],[0]
14 were generic like the one presented above and the other 24 included a background story to encourage role-playing from users and to keep them engaged.,2.2 Task Templates and Instructions,[0],[0]
These templates were meant to add variety to the dialogues.,2.2 Task Templates and Instructions,[0],[0]
The generic templates were also important for the users to create their own character and personality.,2.2 Task Templates and Instructions,[0],[0]
We found that the combination of the two types of templates prevented the task from becoming too repetitive.,2.2 Task Templates and Instructions,[0],[0]
"Notably, we distributed the role-playing templates throughout the data collection process to bring some novelty and surprise.",2.2 Task Templates and Instructions,[0],[0]
"We also asked the participants to write templates (13 of them) to keep them engaged in the task.
",2.2 Task Templates and Instructions,[0],[0]
"To control data collection, we gave a set of instructions to the participants.",2.2 Task Templates and Instructions,[0],[0]
The user instructions encouraged a variety of behaviours.,2.2 Task Templates and Instructions,[0],[0]
"As for the wizards, they were asked only to talk about the
2www.slack.com
database results and the task at hand.",2.2 Task Templates and Instructions,[0],[0]
"We also asked the wizards to perform untimely actions occasionally, for instance, to ask for information that the user has already provided.",2.2 Task Templates and Instructions,[0],[0]
It is interesting from a dialogue management point of view to have examples of bad behaviour and of how it impacts user satisfaction.,2.2 Task Templates and Instructions,[0],[0]
"At the end of each dialogue, the user provided a wizard cooperativity rating on a scale of 1 to 5.",2.2 Task Templates and Instructions,[0],[0]
"The wizard, on the other hand, was shown the user’s task and was asked whether she thought the user had accomplished it.",2.2 Task Templates and Instructions,[0],[0]
Wizards received a link to a search interface every time a user was connected to them.,2.3 Search Interface And Suggestions,[0],[0]
The search interface was a simple GUI with all the searchable fields in the database (see Appendix A).,2.3 Search Interface And Suggestions,[0],[0]
"For every database search, up to 10 results were displayed, sorted by increasing price.
",2.3 Search Interface And Suggestions,[0],[0]
Another important property of human dialogue that we want to study with Frames is how to provide users with database information.,2.3 Search Interface And Suggestions,[0],[0]
"When a set of user constraints leads to no results, users would benefit from knowing that relaxing a given constraint (e.g., increasing the budget by a reasonable amount) leads to results.",2.3 Search Interface And Suggestions,[0],[0]
We modelled this by displaying suggestions to the wizards when a database query returned no results.,2.3 Search Interface And Suggestions,[0],[0]
Suggestions were packages obtained by randomly relaxing one or more constraints.,2.3 Search Interface And Suggestions,[0],[0]
It was up to the wizard to decide whether or not to use suggestions.,2.3 Search Interface And Suggestions,[0],[0]
"Using the data collection process described above, we collected 1369 dialogues.",3 Statistics of the Corpus,[0],[0]
Figure 1a shows the distribution of dialogue lengths in the corpus.,3 Statistics of the Corpus,[0],[0]
"The average number of turns is 15, for a total of 19986 turns in the dataset.",3 Statistics of the Corpus,[0],[0]
A turn is defined as a Slack message sent by either a user or a wizard.,3 Statistics of the Corpus,[0],[0]
"Turns always alternate between user and wizard.
",3 Statistics of the Corpus,[0],[0]
Figure 1b shows the number of acts per dialogue turn.,3 Statistics of the Corpus,[0],[0]
About 25% of the dialogue turns have more than one dialogue act.,3 Statistics of the Corpus,[0],[0]
"The turns without dialogue acts are turns where the user asked for something that the wizard could not provide, e.g., because it was not part of the database.",3 Statistics of the Corpus,[0],[0]
"We left such (rarely occurring) user turns unannotated, as they are usually followed up by the wizard saying she cannot provide the required information.",3 Statistics of the Corpus,[0],[0]
"This rarely occurs, since our users are familiar with the capabilities of the “system” after only few dialogues.
",3 Statistics of the Corpus,[0],[0]
Figure 1c shows the distribution of user ratings.,3 Statistics of the Corpus,[0],[0]
More than 70% of the dialogues have the maximum rating of 5.,3 Statistics of the Corpus,[0],[0]
Figure 2 shows the occurrences of dialogue acts in the corpus.,3 Statistics of the Corpus,[0],[0]
The dialogue acts are described in Table 9.,3 Statistics of the Corpus,[0],[0]
We present the annotation scheme in the following section.,3 Statistics of the Corpus,[0],[0]
"We manually annotated the Frames dataset with dialogue acts, slot types and values, references to other frames, and the ID of the currently active frame for each utterance.",4 Annotation,[0],[0]
We also computed frame descriptions based on the labels of earlier turns.,4 Annotation,[0],[0]
"Most of the dialogue acts used for annotation are typical of the goal-oriented setting, such as inform and offer (Henderson et al., 2014b).","4.1 Dialogue Acts, Slot Types, Slot Values",[0],[0]
"We also introduced dialogue acts specifically for frame tracking, such as switch frame and request compare.","4.1 Dialogue Acts, Slot Types, Slot Values",[0],[0]
"The dialogue acts are listed in Table 9.
","4.1 Dialogue Acts, Slot Types, Slot Values",[0],[0]
Our annotation uses three sets of slot types.,"4.1 Dialogue Acts, Slot Types, Slot Values",[0],[0]
"The first set, listed in Tables 7 and 8, corresponds to the fields of the database.","4.1 Dialogue Acts, Slot Types, Slot Values",[0],[0]
"The second set is listed in Table 10 and contains the slot types which we defined to describe specific aspects of the dialogue, such as intent, action, and count.","4.1 Dialogue Acts, Slot Types, Slot Values",[0],[0]
The remaining slot types in Table 10 were introduced to describe frames and cross-references between them.,"4.1 Dialogue Acts, Slot Types, Slot Values",[0],[0]
Semantic frames form the core of our dataset.,4.2 Frame Definition,[0],[0]
"A semantic frame is defined by the following four components: • User requests: slots whose values the user
wants to know for this frame.",4.2 Frame Definition,[0],[0]
•,4.2 Frame Definition,[0],[0]
"User binary questions: user questions with
slot types and slot values.",4.2 Frame Definition,[0],[0]
•,4.2 Frame Definition,[0],[0]
"Constraints: slots which have been set to a
particular value by the user or the wizard.",4.2 Frame Definition,[0],[0]
•,4.2 Frame Definition,[0],[0]
"User comparison requests: slots whose values
the user wants to know for this frame and one or more other frames.
",4.2 Frame Definition,[0],[0]
"In DSTC, a semantic frame contains the constraints set by the user, the user requests, and the user’s search method (e.g., by constraints or alternatives).",4.2 Frame Definition,[0],[0]
"In our case, constraints can also be set by the wizard when she suggests or offers a package.",4.2 Frame Definition,[0],[0]
Any field in the database (see Tables 7 and 8 in Appendix A) can be constrained by the user or,4.2 Frame Definition,[0],[0]
"User I’d like to book a trip to Atlantis from Caprica on Saturday, 1 August 13, 2016 for 8 adults.",Author Utterance Frame,[0],[0]
I have a tight budget of 1700.,Author Utterance Frame,[0],[0]
"Wizard Hi...I checked a few options for you, and unfortunately, we do not currently have any 1 trips that meet this criteria.",Author Utterance Frame,[0],[0]
Would you like to book an alternate travel option?,Author Utterance Frame,[0],[0]
User,Author Utterance Frame,[0],[0]
"Yes, how about going to Neverland from Caprica on August 13, 2 2016 for 5 adults.",Author Utterance Frame,[0],[0]
"For this trip, my budget would be 1900.",Author Utterance Frame,[0],[0]
Wizard I checked the availability for those dates and there were no trips available.,Author Utterance Frame,[0],[0]
"2 Would you like to select some alternate dates?
the wizard.",Author Utterance Frame,[0],[0]
The comparison requests and the binary questions were added after analysing the dialogues.,Author Utterance Frame,[0],[0]
The comparison requests correspond to the request compare dialogue act.,Author Utterance Frame,[0],[0]
"This dialogue act is used to annotate turns when a user asks to compare different results, for instance: “Could you tell me which of these resorts offers free wifi?”.",Author Utterance Frame,[0],[0]
These questions possibly relate to several frames.,Author Utterance Frame,[0],[0]
"Binary questions are questions with slot types and slot values, e.g., “In which part of the town is the hotel located?”",Author Utterance Frame,[0],[0]
"(request act), or “Is the trip to Marseille cheaper than to Naples?”",Author Utterance Frame,[0],[0]
"(request compare act), as well as all confirm acts.",Author Utterance Frame,[0],[0]
Binary questions may concern one or several frames.,Author Utterance Frame,[0],[0]
Each dialogue starts in frame 1.,4.3 Frame Creation and Switching,[0],[0]
"New frames are introduced when the wizard offers or suggests some-
thing, or when the user modifies pre-established slots.",4.3 Frame Creation and Switching,[0],[0]
"Thus, all values discussed during the dialogue are recorded and the user can return to a previous set of constraints at any point.",4.3 Frame Creation and Switching,[0],[0]
"An example is given in Table 1: the frame number changes when the user modifies several slot values, namely, the destination city, the number of adults for the trip, and the budget.",4.3 Frame Creation and Switching,[0],[0]
"While modifying pre-established slots is supported by most dialogue systems, these rules allow us to clearly distinguishing creating frames from extending frames and thus define how the items in the dialogue memory, which the user can reference, are structured.",4.3 Frame Creation and Switching,[0],[0]
"Though frames are created for each offer or suggestion made by the wizard, the active frame can only be changed by the user so that the user has control over the dialogue.",4.3 Frame Creation and Switching,[0],[0]
"When creating frames, the annotator can explicitly mark which frame the new frame is derived from, which heuristically copies some of its content to the new frame.",4.3 Frame Creation and Switching,[0],[0]
"If not annotated, we assume it is derived from the currently active frame.",4.3 Frame Creation and Switching,[0],[0]
"If the user asks for more information about a specific offer or suggestion, the active frame is changed to the frame introduced with that offer or suggestion.",4.3 Frame Creation and Switching,[0],[0]
This change of frame is indicated by a switch frame act (see Appendix A).,4.3 Frame Creation and Switching,[0],[0]
"The rules for creating and switching frames are summarized in Table 2.
",4.3 Frame Creation and Switching,[0],[0]
We introduced specific slot types for recording the creation and modification of frames.,4.3 Frame Creation and Switching,[0],[0]
"These slot types are id, ref, read, and write (see Table 10 in Appendix A).",4.3 Frame Creation and Switching,[0],[0]
The frame id is defined when the frame is created and is used to switch to,4.3 Frame Creation and Switching,[0],[0]
"Switching User Changing the value of a slot (it causes the dialogue to switch to that frame)
50% 2092
Considering a wizard offer or suggestion 39% 1635 Switching to an earlier frame by mentioning its slot values 11% 458
this frame when the user decides to do so.",Rule Type Author Rule Description Relative Frequency Absolute Frequency,[0],[0]
The other slot types are used to annotate cross-references between frames.,Rule Type Author Rule Description Relative Frequency Absolute Frequency,[0],[0]
A reference has two parts: the id of the frame it refers to and the slots and values that are used to refer to that frame (if any).,Rule Type Author Rule Description Relative Frequency Absolute Frequency,[0],[0]
"For instance, ref[1{name=Tropic}] means that frame 1 is being referred to by the hotel name Tropic.",Rule Type Author Rule Description Relative Frequency Absolute Frequency,[0],[0]
"If anaphora are used to refer to a frame, we annotated this with the slot ref anaphora (e.g., “This is too long” – inform(duration=toolong, ref anaphora=this)).",Rule Type Author Rule Description Relative Frequency Absolute Frequency,[0],[0]
"Inside an offer dialogue act, a ref means that the frame corresponding to the offer is derived from another frame.",Rule Type Author Rule Description Relative Frequency Absolute Frequency,[0],[0]
This happens for instance when a wizard proposes a package with business or economy options.,Rule Type Author Rule Description Relative Frequency Absolute Frequency,[0],[0]
"In this case, the business and economy offers are derived from the hotel offer.
",Rule Type Author Rule Description Relative Frequency Absolute Frequency,[0],[0]
"The slot types read and write only occur inside a wizard’s inform act and are used by wizards to provide relations between offers or suggestions: read is used to indicate which frame the values come from (and which slots are used to refer to this frame, if any), while write indicates the frame where the slot values are to be written (and which slot values are used to refer to this frame, if any).",Rule Type Author Rule Description Relative Frequency Absolute Frequency,[0],[0]
"If there is a read without a write, the current frame is assumed as the storage for the slot values.",Rule Type Author Rule Description Relative Frequency Absolute Frequency,[0],[0]
"A slot type without a value indicates that the value is the same as in the referenced frame, but was not mentioned explicitly e.g., “for the same price”.
",Rule Type Author Rule Description Relative Frequency Absolute Frequency,[0],[0]
"Table 3 gives an example of how these slot types are used in practice: inform( read=[7{dst city=Punta Cana, category=2.5}]) means that the values 2.5 and Punta Cana are to be read from frame 7, and to be written in the current frame.",Rule Type Author Rule Description Relative Frequency Absolute Frequency,[0],[0]
"At this turn of the dialogue, the wizard repeats information from frame 7.",Rule Type Author Rule Description Relative Frequency Absolute Frequency,[0],[0]
"The annotation inform(breakfast=False,write= [7{name=El Mar}]) means that the value
False for breakfast is written in frame 7 and that frame 7 was identified in this utterance by the name of the hotel El Mar.
The average number of frames created per dialogue is 6.71 and the average number of frame switches is 3.58.",Rule Type Author Rule Description Relative Frequency Absolute Frequency,[0],[0]
Figure 3 shows boxplots for the number of frame creations and the number of frame changes in the corpus.,Rule Type Author Rule Description Relative Frequency Absolute Frequency,[0],[0]
Five trained experts annotated the dataset according to the above rules.,4.4 Annotation Reproducibility,[0],[0]
"To measure inter-annotator agreement, the experts annotated the same randomly chosen 10 dialogues.",4.4 Annotation Reproducibility,[0],[0]
"On this subset, we compute the inter-annotator agreement rate as the F1-score.",4.4 Annotation Reproducibility,[0],[0]
"Note that the commonly used κ statistic cannot be directly applied here, since the annotation is not a multi-class classification problem.",4.4 Annotation Reproducibility,[0],[0]
"The provided F1 score also captures how much the annotators failed to annotate words or acts, or disagreed about the correct value.",4.4 Annotation Reproducibility,[0],[0]
We report the mean and standard deviation over all possible pairing of annotators.,4.4 Annotation Reproducibility,[0],[0]
"On dialogue acts only, this score is 81.2 ± 3.1, on slot values, it is 95.2 ± 1.1, and on dialogue acts, slot values, and content of referenced frames, it is 62.3 ± 4.9.
",4.4 Annotation Reproducibility,[0],[0]
Table 3:,4.4 Annotation Reproducibility,[0],[0]
Annotation example with the write and read slot types,4.4 Annotation Reproducibility,[0],[0]
"Wizard I am only able to find hotels with a 6 inform(read=[7{dst city=Punta Cana, 2.5 star rating in Punta Cana for that time.",Author Utterance Frame Annotation,[0],[0]
category=2.5}]),Author Utterance Frame Annotation,[0],[0]
User 2.5 stars will do.,Author Utterance Frame Annotation,[0],[0]
11 inform(category=2.5) Can you offer any additional activities?,Author Utterance Frame Annotation,[0],[0]
"Wizard Unfortunately I am not able to provide 11 sorry, canthelp this information.",Author Utterance Frame Annotation,[0],[0]
User How about breakfast?,Author Utterance Frame Annotation,[0],[0]
11 request(breakfast) Wizard El Mar does not provide breakfast.,Author Utterance Frame Annotation,[0],[0]
"11 inform(breakfast=False, write=[7{name=El Mar}])
id=0 (current)",Author Utterance Frame Annotation,[0],[0]
id=1 dst city=Mannheim or city=Melbourne price=8000USD id=2,Author Utterance Frame Annotation,[0],[0]
dst city=New York or city=Melbourne id=3,Author Utterance Frame Annotation,[0],[0]
"(new)
inform(dst city=Mannheim, budget=cheaper, flex=T) Is there a cheaper package to Mannheim?",Author Utterance Frame Annotation,[0],[0]
"I’m flexible with the dates.
",Author Utterance Frame Annotation,[0],[0]
Figure 4: Illustration of the frame tracking task.,Author Utterance Frame Annotation,[0],[0]
"The model must choose, for each slot, which frame it is referring to, given the set of available frames, the previous active frame (bold), and the potential new frame (marked “(new)”).",Author Utterance Frame Annotation,[0],[0]
"Frames can be used to study many aspects of goaloriented dialogue, from Natural Language Understanding (NLU) to Natural Language Generation (NLG).",5 Research Topics,[0],[0]
"In this section, we propose three topics that we believe are new and representative of Frames.",5 Research Topics,[0],[0]
"We propose Frame tracking as an extension of state tracking (Henderson, 2015) to a setting where several semantic frames are tracked simultaneously.",5.1.1 Definition,[0],[0]
"In state tracking, the dialogue history is compressed into one semantic frame.",5.1.1 Definition,[0],[0]
"The state tracker updates a probability distribution, for each slot, over the different possible values.",5.1.1 Definition,[0],[0]
"Every time the user sets a new value, the probability distribution is updated.",5.1.1 Definition,[0],[0]
This architecture prevents the user from comparing options or returning to an item discussed earlier since the values for each slot are tracked separately.,5.1.1 Definition,[0],[0]
"In frame tracking, a new value creates a new semantic frame.",5.1.1 Definition,[0],[0]
"The frame tracking task is significantly harder as it requires, for each user utterance, identifying the active frame as well as all the frames modified by the utterance.",5.1.1 Definition,[0],[0]
"An example is provided in Fig. 4.
",5.1.1 Definition,[0],[0]
Definition 1 (Frame Tracking).,5.1.1 Definition,[0],[0]
"At each user turn t, we assume access to the full dialogue history H = {f1, ..., fnt−1}, where fi is a frame and nt−1 is the number of frames created so far in the dialogue.",5.1.1 Definition,[0],[0]
"For a user utterance ut at time t, we provide the following NLU labels: dialogue acts, slot types, and slot values.",5.1.1 Definition,[0],[0]
"The goal of frame tracking is to predict if a new frame is created and to predict for each dialogue act the ref labels (possibly none) and the ids of the frames referenced.
",5.1.1 Definition,[0],[0]
"Predicting the frame that is referenced by a dialogue act requires detecting if a new frame is created and recognizing a previous frame from the values mentioned by the user (potentially synonyms, e.g., NYC for New York), or by using the user utterance directly.",5.1.1 Definition,[0],[0]
It is necessary in many cases to use the user utterance directly because users do not always use slot values to refer to previous frames.,5.1.1 Definition,[0],[0]
An example in the corpus is a user asking: “Which package has the soonest departure?”.,5.1.1 Definition,[0],[0]
"In this case, the user refers to several frames (the packages) without ever explicitly describing which ones.",5.1.1 Definition,[0],[0]
This phenomenon is quite common for dialogue acts such as switch frame (979 occurrences in the corpus) and request compare (455 occurrences in the corpus).,5.1.1 Definition,[0],[0]
"These cases can only be resolved by working on the text directly and solving anaphora.
",5.1.1 Definition,[0],[0]
"Note that when talking with real users, a system would need to generate the frames dynamically during the dialogue.",5.1.1 Definition,[0],[0]
We propose the frame tracking task as a first step and we show in Section 6.2 that this simplified task entails many challenges.,5.1.1 Definition,[0],[0]
We define two metrics: frame identification and frame creation.,5.1.2 Evaluation Metrics,[0],[0]
"For frame identification, for each dialogue act, we compare the ground truth pair (key-value, frame) to that predicted by the frame tracker.",5.1.2 Evaluation Metrics,[0],[0]
"We compute performance as the number of correct predictions over the number of
pairs.",5.1.2 Evaluation Metrics,[0],[0]
The frame is the id of the referenced frame.,5.1.2 Evaluation Metrics,[0],[0]
"The key and value are respectively the type and the value of the slot used to refer to the frame (these can be null).
",5.1.2 Evaluation Metrics,[0],[0]
"For frame creation, we compute the number of times the frame tracker correctly predicts that a frame is created or correctly predicts that a frame has not been created over the number of dialogue turns.",5.1.2 Evaluation Metrics,[0],[0]
"In previous work, some limitations of sequential slot filling dialogue systems were addressed using goal-modeling (Crook and Lemon, 2010; Crook et al., 2012; Misu et al., 2011), task tracking (Lee and Stent, 2016) and memory-augmentation of classical state tracking (Weston et al., 2015).
",5.1.3 Related Work,[0],[0]
Crook and Lemon (2010); Crook et al. (2012) model the user goal as a subset of all possible slot value combinations and propose techniques to automatically compress this huge space into a summary space.,5.1.3 Related Work,[0],[0]
"Rewards, transitions, and observations of a POMDP system can then be projected to the reduced space, which facilitates policy learning.",5.1.3 Related Work,[0],[0]
Misu et al. (2011) propose a method for decision support in spoken dialogue systems that aids a user who is assumed to have an (unknown) weighted preference over the possible slot values and limited knowledge about alternatives.,5.1.3 Related Work,[0],[0]
The authors employ a user simulator that outputs dialogue acts to learn a policy that optimizes the sum of the weights of the final user selection.,5.1.3 Related Work,[0],[0]
The Frames dataset allows learning and evaluating these techniques on a large and more realistic text-based dataset.,5.1.3 Related Work,[0],[0]
"Additionally, the memorized frames would allow a dialogue system to compare disjunct goals or return to earlier states.
",5.1.3 Related Work,[0],[0]
Recent approaches to state tracking have been suggested to go beyond the sequential slot-filling approach.,5.1.3 Related Work,[0],[0]
An important contribution is the Task Lineage-based Dialog State Tracking (TL-DST) proposed by Lee and Stent (2016).,5.1.3 Related Work,[0],[0]
TL-DST is a framework that allows keeping track of tasks across different domains.,5.1.3 Related Work,[0],[0]
"Similarly to frame tracking, Lee and Stent propose building a dynamic structure of the dialogue containing different frames corresponding to different tasks.",5.1.3 Related Work,[0],[0]
They defined different sub-tasks among which task frame parsing which is closely related to frame tracking except that they impose constraints on how a dialogue act can be assigned to a frame and a dialogue act can only relate to one frame.,5.1.3 Related Work,[0],[0]
"Because of the lack of data, Lee
and Stent (2016) trained their tracking model on datasets released for DSTC (DSTC2 and DSTC3, Henderson et al., 2014b,a).",5.1.3 Related Work,[0],[0]
"As a result, they could artificially mix different tasks, e.g., looking for a restaurant and looking for a pub, but they could not study how human beings switch between topics.",5.1.3 Related Work,[0],[0]
"In addition, this framework can switch between different tasks but does not handle comparisons between disjunct frames, which is an important aspect of frame tracking.
",5.1.3 Related Work,[0],[0]
Another related approach was proposed by Perez and Liu (2016) who re-interpreted the state tracking task as a question-answering task.,5.1.3 Related Work,[0],[0]
"Their state tracker is based on a memory network (Weston et al., 2015) and can answer questions about the user goal at the end of the dialogue.",5.1.3 Related Work,[0],[0]
"They also propose adding functionalities such as keeping a list of the constraints expressed by the user during the dialogue.
",5.1.3 Related Work,[0],[0]
The Frames dataset may be used to test and validate these approaches on real data.,5.1.3 Related Work,[0],[0]
"In addition, we propose the frame tracking task as benchmark and as a first step towards modelling complex decisionmaking behaviour.",5.1.3 Related Work,[0],[0]
"Most of the time, the wizard would speak about the current frame to ask or answer questions.",5.2 Dialogue Management,[0],[0]
"However, sometimes, the wizard would talk about previous frames.",5.2 Dialogue Management,[0],[0]
"An example is given in Table 11 in Appendix A. In the bold utterance in this dialogue, the wizard mentions a frame which is not the currently active frame.",5.2 Dialogue Management,[0],[0]
"In order to reproduce this kind of behaviour, a dialogue manager would need to be able to identify potentially relevant frames for the current turn and to output actions for these frames.
",5.2 Dialogue Management,[0],[0]
Table 11 also illustrates another novelty.,5.2 Dialogue Management,[0],[0]
"In the utterance in italics, the wizard actually performs two actions.",5.2 Dialogue Management,[0],[0]
"The first action consists of informing the user about the price of the regal resort and the second action consists of proposing another option, Hotel Globetrotter.",5.2 Dialogue Management,[0],[0]
"Performing more than one action per turn is a challenge when using reinforcement learning (Pietquin et al., 2011; Gašić et al., 2012; Fatemi et al., 2016) and, to our knowledge, has only been tackled in a simulated setting (Laroche et al., 2009).",5.2 Dialogue Management,[0],[0]
An interesting behaviour observed in Frames is that wizards often tend to summarize database results.,5.3 Natural Language Generation,[0],[0]
"An example is a wizard saying: “The cheapest
available flight is 1947.14USD.”",5.3 Natural Language Generation,[0],[0]
"In this case, the wizard informs the user that the database has no cheaper result than the one she is proposing.",5.3 Natural Language Generation,[0],[0]
"To imitate this behaviour, a natural language generator (Oh and Rudnicky, 2000; Wen et al., 2015; Sharma et al., 2017) would need to reason over the database and decide how to tailor the results to the user and present them in a concise but sufficient way.",5.3 Natural Language Generation,[0],[0]
"Various strategies and their combinations can be employed, e.g. summarization, comparison or recommendation (Rieser and Lemon, 2009).",5.3 Natural Language Generation,[0],[0]
A decision-theoretical foundation of such an approach was presented by Walker et al. (2004).,5.3 Natural Language Generation,[0],[0]
A data-driven approach to attribute selection for NLG as planning under uncertainty was proposed by Rieser et al. (2014).,5.3 Natural Language Generation,[0],[0]
"The Frames dataset contains a larger set of dialogues as well as wizard-generated text with detailed annotations, which we believe will provide insight into when humans use which strategy and how they present the information.",5.3 Natural Language Generation,[0],[0]
We developed baseline models for natural language understanding and frame tracking.,6 Baselines,[0],[0]
"We define the NLU task as dialogue act prediction and IOB (Inside, Outside, Beginning) tagging.",6.1 Natural Language Understanding,[0],[0]
The NLU model that we propose as baseline is illustrated in Fig. 5.,6.1 Natural Language Understanding,[0],[0]
"We predict, for each word of the utterance, a pair of tags – one for the act and one for the slot.",6.1 Natural Language Understanding,[0],[0]
"This model operates on character trigrams and is based on the robust named entity recognition model (Arnold et al., 2016) except that it has two heads instead of one: one head for the slot type (either a slot type or an O tag) as in the original model and one head for dialogue act prediction.",6.1 Natural Language Understanding,[0],[0]
"These two parts share an embedding matrix for the input character trigrams.
",6.1 Natural Language Understanding,[0],[0]
We generated the IOB tags by matching the slot values in the manual annotations with the corresponding textual utterances.,6.1 Natural Language Understanding,[0],[0]
Note that the model only predicts IOB tags for slots whose values can be found in the text.,6.1 Natural Language Understanding,[0],[0]
"Therefore, the prediction for slots such as intent or vicinities and amenities is not evaluated for this simple baseline.",6.1 Natural Language Understanding,[0],[0]
"The act tags were also generated at the word level: for a given dialogue act with slot values, each word between the slot value that occurred first in the text and the one that occurred last in the text was tagged with the corresponding act.",6.1 Natural Language Understanding,[0],[0]
"For example, for the utterance I am only able to find hotels with a 2.5 star rating in Punta Cana for that time.",6.1 Natural Language Understanding,[0],[0]
", the words 2.5 star rating in Punta Cana are tagged with the inform dialogue act.",6.1 Natural Language Understanding,[0],[0]
"The other words are tagged with O.
The two parts of the model are trained simultaneously, using a modified categorical crossentropy loss for both sets of outputs.",6.1 Natural Language Understanding,[0],[0]
We modify the loss to ignore O labels that are already predicted correctly by the model.,6.1 Natural Language Understanding,[0],[0]
"We introduce this modification because O labels are far more frequent than other labels, and not limiting their contribution to the loss causes the model to degenerate to predicting O labels for every word.",6.1 Natural Language Understanding,[0],[0]
"The losses for both parts of the model are added together and the combined objective is optimized using ADAM (Kingma and Ba, 2015).
",6.1 Natural Language Understanding,[0],[0]
We provide F1 scores for acts and slots for this model in Table 4.,6.1 Natural Language Understanding,[0],[0]
"We report average and stan-
dard deviation over ten leave-one-user-out splits of the Frames dataset.",6.1 Natural Language Understanding,[0],[0]
We had a total of 11 participants who played the user role at least once during data collection.,6.1 Natural Language Understanding,[0],[0]
Two participants performed significantly fewer dialogues than the others.,6.1 Natural Language Understanding,[0],[0]
We merged the dialogues generated by these two participants (ids U21E41CQP and U23KPC9QV).,6.1 Natural Language Understanding,[0],[0]
"For each of the resulting 10 users, we randomly split the combined dialogues of the nine others into training (80%) and validation (20%), and then tested on the dialogues from the held-out user.",6.1 Natural Language Understanding,[0],[0]
"We propose a rule-based frame tracking baseline which takes as input the dialogue acts with slot types and slot values but without the referenced frames (i.e., the ref slots) as well as all the frames created so far during the dialogue.",6.2 Frame Tracking,[0],[0]
"Based on this input, the tracker predicts the ref tags (for frame identification, see Section 5.1.2) for each dialogue act, and it predicts if a frame is created.",6.2 Frame Tracking,[0],[0]
We write f,6.2 Frame Tracking,[0],[0]
[k] to denote the value of slot k in frame f .,6.2 Frame Tracking,[0],[0]
"For an act a(k=v) in frame f , the following rules are used:
• Create and switch to a new frame if f",6.2 Frame Tracking,[0],[0]
"[k] is set and a is inform, but v does not match f",6.2 Frame Tracking,[0],[0]
[k].,6.2 Frame Tracking,[0],[0]
• Switch to frame g if a is switch frame and g[k] matches v.,6.2 Frame Tracking,[0],[0]
"If no match is found, switch to the most recently created frame.3 • Assign ref to frame g if a can have a ref tag, and g[k] matches v. The most recently created frame is used in ambiguous cases.",6.2 Frame Tracking,[0],[0]
"If no match is found, assign ref to the current frame.
",6.2 Frame Tracking,[0],[0]
We compare this baseline to random performance.,6.2 Frame Tracking,[0],[0]
"For random performance, for each (dialogue act, slot type) combination, we compute priors on the corpus for each time the user would refer to the current frame vs a previous one.",6.2 Frame Tracking,[0],[0]
"We sampled whether each slot referred to the current frame or another one based on that prior, and if it referred to another frame, the frame number for that other frame was sampled uniformly from the list of frames created so far.
",6.2 Frame Tracking,[0],[0]
Table 5 presents results for these baselines.,6.2 Frame Tracking,[0],[0]
We report results over 10 runs following the same evaluation method as for the NLU model.,6.2 Frame Tracking,[0],[0]
"Table 5 shows that the rule-based model performs only slightly better than random on frame identification
3a reasonable assumption since this case often happens when a wizard makes an offer and the user talks about it.
and performs similarly on frame creation.",6.2 Frame Tracking,[0],[0]
Table 6 presents an analysis of the performance of the rulebased model.,6.2 Frame Tracking,[0],[0]
We report the accuracy of the frame tracking baseline on the most crucial sub-tasks of frame tracking for one fold.,6.2 Frame Tracking,[0],[0]
The top table shows that the most difficult tasks consist of assigning the correct frame to a switch frame act when the act is not directly preceded by an offer and when the act has no slots.,6.2 Frame Tracking,[0],[0]
"As discussed previously, when the act has no slots, it is important to consider the text and solve anaphora.",6.2 Frame Tracking,[0],[0]
"When the act is directly preceded by an offer, the baseline assigns the previous frame, which is the frame of the offer and which most of the time is the frame that the user switched to, e.g., to ask for more information about the offer.",6.2 Frame Tracking,[0],[0]
"In terms of frame creation, the baseline has very poor performance in correctly predicting that a frame is created because the user changes the value of a previously set slot.",6.2 Frame Tracking,[0],[0]
These results demonstrate that frame tracking cannot be solved with simple rules and necessitates tackling many complex sub-tasks.,6.2 Frame Tracking,[0],[0]
We introduced the Frames dataset: a corpus of human-human dialogues in a travel domain.,7 Conclusion,[0],[0]
This dataset contains complex user behaviour such as comparing between offers.,7 Conclusion,[0],[0]
"We formalized the frame tracking task, which requires tracking simultaneously several semantic frames during a dialogue.",7 Conclusion,[0],[0]
We proposed a rule-based model for this task and analysed its performance.,7 Conclusion,[0],[0]
We release Frames in the hope of driving further research on complex decision-making in the dialogue community.,7 Conclusion,[0],[0]
"Wizard A 5 star hotel called the Regal Resort, Wizard it has free wifi and a spa.",Author Utterance,[0],[0]
User dates?,Author Utterance,[0],[0]
Wizard Starts on august 27th until the 30th User ok that could work.,Author Utterance,[0],[0]
"I would like to see my options in Santos as well Wizard regal resort goes for $2800 or there is the Hotel
Globetrotter in Santos it has 3 stars and comes with breakfast and wifi, it leaves on the 25th and returns on the 30th!",Author Utterance,[0],[0]
"all for $2000
User",Author Utterance,[0],[0]
ahh,Author Utterance,[0],[0]
I can’t leave until august 26 though Wizard then i guess you might have to choose the Regal resort,Author Utterance,[0],[0]
User,Author Utterance,[0],[0]
yeah.,Author Utterance,[0],[0]
I will book it Wizard Thank you!,Author Utterance,[0],[0]
"This paper proposes a new dataset, Frames, composed of 1369 human-human dialogues with an average of 15 turns per dialogue.",abstractText,[0],[0]
This corpus contains goal-oriented dialogues between users who are given some constraints to book a trip and assistants who search a database to find appropriate trips.,abstractText,[0],[0]
"The users exhibit complex decision-making behaviour which involve comparing trips, exploring different options, and selecting among the trips that were discussed during the dialogue.",abstractText,[0],[0]
"To drive research on dialogue systems towards handling such behaviour, we have annotated and released the dataset and we propose in this paper a task called frame tracking.",abstractText,[0],[0]
This task consists of keeping track of different semantic frames throughout each dialogue.,abstractText,[0],[0]
We propose a rule-based baseline and analyse the frame tracking task through this baseline.,abstractText,[0],[0]
Frames: A Corpus for Adding Memory to Goal-Oriented Dialogue Systems,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 773–783 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1072",text,[0],[0]
"Ideas exist in the mind, but are made manifest in language, where they compete with each other for the scarce resource of human attention.",1 Introduction,[0],[0]
Milton (1644) used the “marketplace of ideas” metaphor to argue that the truth will win out when ideas freely compete; Dawkins (1976) similarly likened the evolution of ideas to natural selection of genes.,1 Introduction,[0],[0]
"We propose a framework to quantitatively characterize competition and cooperation between ideas in texts, independent of how they might be represented.
",1 Introduction,[0],[0]
"By “ideas”, we mean any discrete conceptual
units that can be identified as being present or absent in a document.",1 Introduction,[0],[0]
"In this work, we consider representing ideas using keywords and topics obtained in an unsupervised fashion, but our way of characterizing the relations between ideas could be applied to many other types of textual representations, such as frames (Card et al., 2015) and hashtags.
",1 Introduction,[0],[0]
"What does it mean for two ideas to compete in texts, quantitatively?",1 Introduction,[0],[0]
"Consider, for example, the issue of immigration.",1 Introduction,[0],[0]
There are two strongly competing narratives about the roughly 11 million people1 who are residing in the United States without permission.,1 Introduction,[0],[0]
"One is “illegal aliens”, who “steal” jobs and deny opportunities to legal immigrants; the other is “undocumented immigrants”, who are already part of the fabric of society and deserve a path to citizenship (Merolla et al., 2013).
",1 Introduction,[0],[0]
"Although prior knowledge suggests that these two narratives compete, it is not immediately obvious what measures might reveal this competition in a corpus of writing about immigration.",1 Introduction,[0],[0]
One question is whether or not these two ideas cooccur in the same documents.,1 Introduction,[0],[0]
"In the example above, these narratives are used by distinct groups of people with different ideologies.",1 Introduction,[0],[0]
"The fact that they don’t cooccur is one clue that they may be in competition with each other.
",1 Introduction,[0],[0]
"However, cooccurrence is insufficient to express the selection process of ideas, i.e., some ideas fade out over time, while others rise in popularity, analogous to the populations of species in nature.",1 Introduction,[0],[0]
"Of the two narratives on immigration, we may expect one to win out at the expense of another as public opinion shifts.",1 Introduction,[0],[0]
"Alternatively, we might expect to see these narratives reinforcing each other, as both sides intensify their messaging in response to growing opposition, much like the U.S.S.R. and
1As of 2014, according to the most recent numbers from the Center for Migration Studies (Warren, 2016).
",1 Introduction,[0],[0]
"773
the U.S. during the cold war.",1 Introduction,[0],[0]
"To capture these possibilities, we use prevalence correlation over time.
",1 Introduction,[0],[0]
"Building on these insights, we propose a framework that combines cooccurrence within documents and prevalence correlation over time.",1 Introduction,[0],[0]
This framework gives rise to four possible types of relation that correspond to the four quadrants in Fig. 1.,1 Introduction,[0],[0]
We explain each type using examples from news articles in U.S. newspapers on immigration from 1980 to 2016.,1 Introduction,[0],[0]
"Here, we have used LDA to identify ideas in the form of topics, and we denote each idea with a pair of words most strongly associated with the corresponding topic.
",1 Introduction,[0],[0]
"Friendship (correlated over time, likely to cooccur).",1 Introduction,[0],[0]
"The “immigrant, undocumented” topic tends to cooccur with “obama, president” and both topics have been rising during the period of our dataset, likely because the “undocumented immigrants” narrative was an important part of Obama’s framing of the immigration issue (Haynes et al., 2016).
",1 Introduction,[0],[0]
"Head-to-head (anti-correlated over time, unlikely to cooccur).",1 Introduction,[0],[0]
"“immigrant, undocumented” and “illegal, alien” are in a head-to-head competition: these two topics rarely cooccur, and “immigrant, undocu-
mented” has been growing in prevalence, while the usage of “illegal, alien” in newspapers has been declining.",1 Introduction,[0],[0]
"This observation agrees with a report from Pew Research Center (Guskin, 2013).
",1 Introduction,[0],[0]
"Tryst (anti-correlated over time, likely to cooccur).",1 Introduction,[0],[0]
The two off-diagonal examples use topics related to law enforcement.,1 Introduction,[0],[0]
"Overall, “immigration, deportation” and “detention, jail” often cooccur but “detention, jail” has been declining, while “immigration, deportation” has been rising.",1 Introduction,[0],[0]
"This possibly relates to the promises to overhaul the immigration detention system (Kalhan, 2010).2
Arms-race (correlated over time, unlikely to cooccur).",1 Introduction,[0],[0]
"One of the above law enforcement topics (“immigration, deportation”) and a topic on the Republican party (“republican, party”) hold an armsrace relation: they are both growing in prevalence over time, but rarely cooccur, perhaps suggesting an underlying common cause.
",1 Introduction,[0],[0]
"2The tryst relation is the least intuitive, yet is nevertheless observed.",1 Introduction,[0],[0]
"The pattern of being anti-correlated yet likely to cooccur is typically found when two ideas exhibit a friendship pattern (cooccurring and correlated), but only briefly, and then diverge.
",1 Introduction,[0],[0]
"Note that our terminology describes the relations between ideas in texts, not necessarily between the entities to which the ideas refer.",1 Introduction,[0],[0]
"For example, we find that the relation between “Israel” and “Palestine” is “friendship” in news articles on terrorism, based on their prevalence correlation and cooccurrence in that corpus.
We introduce the formal definition of our framework in §2 and apply it to news articles on five issues and research papers from ACL Anthology and NIPS as testbeds.",1 Introduction,[0],[0]
"We operationalize ideas using topics (Blei et al., 2003) and keywords (Monroe et al., 2008).
",1 Introduction,[0],[0]
"To explore whether the four relation types exist and how strong these relations are, we first examine the marginal and joint distributions of cooccurrence and prevalence correlation (§3).",1 Introduction,[0],[0]
We find that cooccurrence shows a unimodal normal-shaped distribution but prevalence correlation demonstrates more diverse distributions across corpora.,1 Introduction,[0],[0]
"As we would expect, there are, in general, more and stronger friendship and head-to-head relations than arms-race and tryst relations.
",1 Introduction,[0],[0]
"Second, we demonstrate the effectiveness of our framework through in-depth case studies (§4).",1 Introduction,[0],[0]
"We not only validate existing knowledge about some news issues and research areas, but also identify hypotheses that require further investigation.",1 Introduction,[0],[0]
"For example, using keywords to represent ideas, a top pair with the tryst relation in news articles on terrorism is “arab” and “islam”; they are likely to cooccur, but “islam” is rising in relative prevalence while “arab” is declining.",1 Introduction,[0],[0]
This suggests a conjecture that the news media have increasingly linked terrorism to a religious group rather than an ethnic group.,1 Introduction,[0],[0]
"We also show relations between topics in ACL that center around machine translation.
",1 Introduction,[0],[0]
"Our work is a first step towards understanding relations between ideas from text corpora, a complex and important research question.",1 Introduction,[0],[0]
We provide some concluding thoughts in §6.,1 Introduction,[0],[0]
The aim of our computational framework is to explore relations between ideas.,2 Computational Framework,[0],[0]
"We thus assume that the set of relevant ideas has been identified, and those expressed in each document have been tabulated.",2 Computational Framework,[0],[0]
Our open-source implementation is at https://github.com/Noahs-ARK/ idea_relations/.,2 Computational Framework,[0],[0]
"In the following, we introduce our formal definitions and datasets.
∀x, y ∈",2 Computational Framework,[0],[0]
"I, P̂MI(x, y) = log P̂ (x, y) P̂ (x)P̂ (y)
",2 Computational Framework,[0],[0]
"= C + log 1+ ∑ t ∑ k 1{x∈dtk}·1{y∈dtk}
(1+ ∑
t ∑ k 1{x∈dtk})·(1+ ∑ t ∑ k 1{y∈dtk})
(1)
r̂(x, y) =
∑ t ( P̂ (x|t)−P̂ (x|t) )",2 Computational Framework,[0],[0]
"( P̂ (y|t)−P̂ (y|t) )
",2 Computational Framework,[0],[0]
"√ ∑
t
( P̂ (x|t)−P̂ (x|t) )",2 Computational Framework,[0],[0]
"2√∑ t ( P̂ (y|t)−P̂ (y|t) )2
",2 Computational Framework,[0],[0]
"(2)
Figure 2: Eq. 1 is the empirical pointwise mutual information for two ideas, our measure of cooccurrence of ideas; note that we use add-one smoothing in estimating PMI.",2 Computational Framework,[0],[0]
Eq. 2 is the Pearson correlation between two ideas’ prevalence over time.,2 Computational Framework,[0],[0]
"As discussed in the introduction, we focus on two dimensions to quantify relations between ideas:
1.",2.1 Cooccurrence and Prevalence Correlation,[0],[0]
"cooccurrence reveals to what extent two ideas tend to occur in the same contexts;
2.",2.1 Cooccurrence and Prevalence Correlation,[0],[0]
"similarity between the relative prevalence of ideas over time reveals how two ideas relate in terms of popularity or coverage.
",2.1 Cooccurrence and Prevalence Correlation,[0],[0]
"Our input is a collection of documents, each represented by a set of ideas and indexed by time.",2.1 Cooccurrence and Prevalence Correlation,[0],[0]
"We denote a static set of ideas as I and a text corpus that consists of these ideas as C = {D1, . . .",2.1 Cooccurrence and Prevalence Correlation,[0],[0]
", DT }, where Dt = {dt1 , . . .",2.1 Cooccurrence and Prevalence Correlation,[0],[0]
", dtNt} gives the collection of documents at timestep t, and each document, dtk , is represented as a subset of ideas in I.",2.1 Cooccurrence and Prevalence Correlation,[0],[0]
"Here T is the total number of timesteps, and Nt is the number of documents at timestep t.",2.1 Cooccurrence and Prevalence Correlation,[0],[0]
"It follows that the total number of documents N = ∑T t=1Nt.
",2.1 Cooccurrence and Prevalence Correlation,[0],[0]
"In order to formally capture the two dimensions above, we employ two commonly-used statistics.",2.1 Cooccurrence and Prevalence Correlation,[0],[0]
"First, we use empirical pointwise mutual information (PMI) to capture the cooccurrence of ideas within the same document (Church and Hanks, 1990); see Eq. 1 in Fig. 2.",2.1 Cooccurrence and Prevalence Correlation,[0],[0]
Positive,2.1 Cooccurrence and Prevalence Correlation,[0],[0]
"P̂MI indicates that ideas occur together more frequently than would be expected if they were independent, while negative P̂MI indicates the opposite.
",2.1 Cooccurrence and Prevalence Correlation,[0],[0]
"Second, we compute the correlation between normalized document frequency of ideas to capture the relation between the relative prevalence of ideas across documents over time; see Eq. 2 in Fig. 2.",2.1 Cooccurrence and Prevalence Correlation,[0],[0]
"Positive r̂ indicates that two ideas have similar prevalence over time, while negative r̂ sug-
gests two anti-correlated ideas (i.e., when one goes up, the other goes down).
",2.1 Cooccurrence and Prevalence Correlation,[0],[0]
"The four types of relations in the introduction can now be obtained using P̂MI and r̂, which capture cooccurrence and prevalence correlation respectively.",2.1 Cooccurrence and Prevalence Correlation,[0],[0]
"We further define the strength of the relation between two ideas as the absolute value of the product of their P̂MI and r̂ scores:
∀x, y ∈",2.1 Cooccurrence and Prevalence Correlation,[0],[0]
"I, strength(x, y) = |P̂MI(x, y)×r̂(x, y)|.",2.1 Cooccurrence and Prevalence Correlation,[0],[0]
(3),2.1 Cooccurrence and Prevalence Correlation,[0],[0]
We use two types of datasets to validate our framework: news articles and research papers.,2.2 Datasets and Representation of Ideas,[0],[0]
"We choose these two domains because competition between ideas has received significant interest in history of science (Kuhn, 1996) and research on framing (Chong and Druckman, 2007; Entman, 1993; Gitlin, 1980; Lakoff, 2014).",2.2 Datasets and Representation of Ideas,[0],[0]
"Furthermore, interesting differences may exist in these two domains as news evolves with external events and scientific research progresses through innovations.",2.2 Datasets and Representation of Ideas,[0],[0]
• News articles.,2.2 Datasets and Representation of Ideas,[0],[0]
"We follow the strategy in Card
et al. (2015) to obtain news articles from LexisNexis on five issues: abortion, immigration, same-sex marriage, smoking, and terrorism.",2.2 Datasets and Representation of Ideas,[0],[0]
We search for relevant articles using LexisNexis subject terms in U.S. newspapers from 1980 to 2016.,2.2 Datasets and Representation of Ideas,[0],[0]
"Each of these corpora contains more than 25,000 articles.",2.2 Datasets and Representation of Ideas,[0],[0]
"Please refer to the supplementary material for details.
",2.2 Datasets and Representation of Ideas,[0],[0]
• Research papers.,2.2 Datasets and Representation of Ideas,[0],[0]
"We consider full texts of papers from two communities: our own ACL community captured by papers from ACL, NAACL, EMNLP, and TACL from 1980 to 2014 (Radev et al., 2009); and the NIPS community from 1987 to 2016.3 There are 4.8K papers from the ACL community and 6.6K papers from the NIPS community.",2.2 Datasets and Representation of Ideas,[0],[0]
"The processed datasets are available at https://chenhaot.com/ pages/idea-relations.html.
",2.2 Datasets and Representation of Ideas,[0],[0]
"In order to operationalize ideas in a text corpus, we consider two ways to represent ideas.",2.2 Datasets and Representation of Ideas,[0],[0]
•,2.2 Datasets and Representation of Ideas,[0],[0]
Topics.,2.2 Datasets and Representation of Ideas,[0],[0]
"We extract topics from each document
by running LDA (Blei et al., 2003) on each corpus C. In all datasets, we set the number of topics to 50.4 Formally, I is the 50 topics learned 3 http://papers.nips.cc/. 4We chose 50 topics based on past experience, though this could be tuned for particular applications.",2.2 Datasets and Representation of Ideas,[0],[0]
"Recall that
from the corpus, and each document is represented as the set of topics that are present with greater than 0.01 probability in the topic distribution for that document.
",2.2 Datasets and Representation of Ideas,[0],[0]
•,2.2 Datasets and Representation of Ideas,[0],[0]
Keywords.,2.2 Datasets and Representation of Ideas,[0],[0]
We identify a list of distinguishing keywords for each corpus by comparing its word frequencies to the background frequencies found in other corpora using the informative Dirichlet prior model in Monroe et al. (2008).,2.2 Datasets and Representation of Ideas,[0],[0]
We set the number of keywords to 100 for all corpora.,2.2 Datasets and Representation of Ideas,[0],[0]
"For news articles, the background corpus for each issue is comprised of all articles from the other four issues.",2.2 Datasets and Representation of Ideas,[0],[0]
"For research papers, we use NIPS as the background corpus for ACL and vice versa to identify what are the core concepts for each of these research areas.",2.2 Datasets and Representation of Ideas,[0],[0]
"Formally, I is the 100 top distinguishing keywords in the corpus and each document is represented as the set of keywords within I that are present in the document.",2.2 Datasets and Representation of Ideas,[0],[0]
"Refer to the supplementary material for a list of example keywords in each corpus.
",2.2 Datasets and Representation of Ideas,[0],[0]
"In both procedures, we lemmatize all words and add common bigram phrases to the vocabulary following Mikolov et al. (2013).",2.2 Datasets and Representation of Ideas,[0],[0]
"Note that in our analysis, ideas are only present or absent in a document, and a document can in principle be mapped to any subset of ideas in I.",2.2 Datasets and Representation of Ideas,[0],[0]
"In our experiments 90% of documents are marked as containing between 7 and 14 ideas using topics, 8 and 33 ideas using keywords.",2.2 Datasets and Representation of Ideas,[0],[0]
"To provide an overview of the four relation types in Fig. 1, we first examine the empirical distributions of the two statistics of interest across pairs of ideas.",3 Characterizing the Space of Relations,[0],[0]
"In most exploratory studies, however, we are most interested in pairs that exemplify each type of relation, i.e., the most extreme points in each quadrant.",3 Characterizing the Space of Relations,[0],[0]
We thus look at these pairs in each corpus to observe how the four types differ in salience across datasets.,3 Characterizing the Space of Relations,[0],[0]
"To the best of our knowledge, the distributions of pairwise cooccurrence and prevalence correlation have not been examined in previous literature.",3.1 Empirical Distribution Properties,[0],[0]
"We thus first investigate the marginal distributions of cooccurrence and prevalence correlation and then
our framework is to analyze relations between ideas, so this choice is not essential in this work.
",3.1 Empirical Distribution Properties,[0],[0]
their joint distribution.,3.1 Empirical Distribution Properties,[0],[0]
Fig. 3 shows three examples: two from news articles and one from research papers.,3.1 Empirical Distribution Properties,[0],[0]
We will also focus our case studies on these three corpora in §4.,3.1 Empirical Distribution Properties,[0],[0]
The corresponding plots for keywords have been relegated to supplementary material due to space limitations.,3.1 Empirical Distribution Properties,[0],[0]
Cooccurrence tends to be unimodal but not normal.,3.1 Empirical Distribution Properties,[0],[0]
"In all of our datasets, pairwise cooccurrence (P̂MI) presents a unimodal distribution that somewhat resembles a normal distribution, but it is rarely precisely normal.",3.1 Empirical Distribution Properties,[0],[0]
"We cannot reject the hypothesis that it is unimodal for any dataset (using topics or keywords) using the dip test (Hartigan and Hartigan, 1985), though D’Agostino’s K2 test (D’Agostino et al., 1990) rejects normality in almost all cases.",3.1 Empirical Distribution Properties,[0],[0]
Prevalence correlation exhibits diverse distributions.,3.1 Empirical Distribution Properties,[0],[0]
"Pairwise prevalence correlation follows different distributions in news articles compared to research papers: they are unimodal in news articles, but not in ACL or NIPS.",3.1 Empirical Distribution Properties,[0],[0]
The dip test only rejects the unimodality hypothesis in NIPS.,3.1 Empirical Distribution Properties,[0],[0]
None follow normal distributions based on D’Agostino’s K2 test.,3.1 Empirical Distribution Properties,[0],[0]
Cooccurrence is positively correlated with prevalence correlation.,3.1 Empirical Distribution Properties,[0],[0]
"In all of our datasets, cooccurrence is positively correlated with prevalence correlation whether we use topics or keywords to represent ideas, although the Pearson correlation coefficients vary.",3.1 Empirical Distribution Properties,[0],[0]
This suggests that there are more friendship and head-to-head relations than tryst and arms-race relations.,3.1 Empirical Distribution Properties,[0],[0]
"Based on the results of kernel density estimation, we also observe that this correlation is often loose, e.g., in
ACL topics, cooccurrence spreads widely at each mode of prevalence correlation.",3.1 Empirical Distribution Properties,[0],[0]
We are interested in how our framework can identify intriguing relations between ideas.,3.2 Relative Strength of Extreme Pairs,[0],[0]
"These potentially interesting pairs likely correspond to the extreme points in each quadrant instead of the ones around the origin, where PMI and prevalence correlation are both close to zero.",3.2 Relative Strength of Extreme Pairs,[0],[0]
Here we compare the relative strength of extreme pairs in each dataset.,3.2 Relative Strength of Extreme Pairs,[0],[0]
"We will discuss how these extreme pairs confirm existing knowledge and suggest new hypotheses via case studies in §4.
",3.2 Relative Strength of Extreme Pairs,[0],[0]
"For each relation type, we average the strengths of the 25 pairs with the strongest relations in that type, with strength defined in Eq. 3.",3.2 Relative Strength of Extreme Pairs,[0],[0]
"This heuristic (henceforth collective strength) allows us to collectively compare the strengths of the most prominent friendship, tryst, arms-race, and head-to-head relations.",3.2 Relative Strength of Extreme Pairs,[0],[0]
"The results are not sensitive to the choice of 25.
",3.2 Relative Strength of Extreme Pairs,[0],[0]
Fig. 4 shows the collective strength of the four types in all of our datasets.,3.2 Relative Strength of Extreme Pairs,[0],[0]
"The most common ordering is:
friendship > head-to-head > arms-race > tryst.
",3.2 Relative Strength of Extreme Pairs,[0],[0]
The fact that friendship and head-to-head relations are strong is consistent with the positive correlation between cooccurrence and prevalence correlation.,3.2 Relative Strength of Extreme Pairs,[0],[0]
"In news, friendship is the strongest relation type, but head-to-head is the strongest in ACL topics and NIPS topics.",3.2 Relative Strength of Extreme Pairs,[0],[0]
"This suggests, unsurprisingly, that there are stronger head-to-head competitions
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 co lle ct iv e st re ng th
friends tryst head-to-head arms-race
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 co lle ct iv e st re ng th
friends tryst head-to-head arms-race
(i.e., one idea takes over another) between ideas in scientific research than in news.",3.2 Relative Strength of Extreme Pairs,[0],[0]
"We also see that topics show greater strength in our scientific article collections, while keywords dominate in news, especially in friendship.",3.2 Relative Strength of Extreme Pairs,[0],[0]
"We conjecture that terms in scientific literature are often overloaded (e.g., a tree could be a parse tree or a decision tree), necessitating some abstraction when representing ideas.",3.2 Relative Strength of Extreme Pairs,[0],[0]
"In contrast, news stories are more self-contained and seek to employ consistent usage.",3.2 Relative Strength of Extreme Pairs,[0],[0]
We present case studies based on strongly related pairs of ideas in the four types of relation.,4 Exploratory Studies,[0],[0]
"Throughout this section, “rank” refers to the rank of the relation strength between a pair of ideas in its corresponding relation type.",4 Exploratory Studies,[0],[0]
"Following a decade of declining violence in the 90s, the events of September 11, 2001 precipitated a dramatic increase in concern about terrorism, and a major shift in how it was framed (Kern et al., 2003).",4.1 International Relations in Terrorism,[0],[0]
"As a showcase, we consider a topic which encompasses much of the U.S. government’s response to terrorism: “federal, state”.5 We observe two topics engaging in an “arms race” with this one: “afghanistan, taliban” and “pakistan, india”.",4.1 International Relations in Terrorism,[0],[0]
"These correspond to two geopolitical regions closely linked to the U.S. government’s concern with terrorism, and both were sites of U.S. military action during the period of our dataset.",4.1 International Relations in Terrorism,[0],[0]
"Events abroad and the U.S. government’s responses follow the arms-race pattern, each holding increasing
5As in §1, we summarize each topic using a pair of strongly associated words, instead of assigning a name.
attention with the other, likely because they share the same underlying cause.
",4.1 International Relations in Terrorism,[0],[0]
"We also observe two head-to-head rivals to the “federal, state” topic: “iran, libya” and “israel, palestinian”.",4.1 International Relations in Terrorism,[0],[0]
"While these topics correspond to regions that are hotly debated in the U.S., their coverage in news tends not to correlate temporally with the U.S. government’s responses to terrorism, at least during the time period of our corpus.",4.1 International Relations in Terrorism,[0],[0]
"Discussion of these regions was more prevalent in the 80s and 90s, with declining media coverage since then (Kern et al., 2003).
",4.1 International Relations in Terrorism,[0],[0]
"The relations between these topics are consistent with structural balance theory (Cartwright and Harary, 1956; Heider, 1946), which suggests that the enemy of an enemy is a friend.",4.1 International Relations in Terrorism,[0],[0]
"The “afghanistan, taliban” topic has the strongest friendship relation with the “pakistan, india” topic, i.e., they are likely to cooccur and are positively correlated in prevalence.",4.1 International Relations in Terrorism,[0],[0]
"Similarly, the “iran, libya” topic is a close “friend” with the “israel, palestinian” topic (ranked 8th in friendship).
",4.1 International Relations in Terrorism,[0],[0]
"When using keywords to represent ideas, we observe similar relations between the term homeland security and terms related to the above foreign countries.",4.1 International Relations in Terrorism,[0],[0]
"In addition, we highlight an interesting but unexpected tryst relation between arab and islam (Fig. 6).",4.1 International Relations in Terrorism,[0],[0]
"It is not surprising that these two words tend to cooccur in the same news articles, but the usage of islam in the news is increasing while arab is declining.",4.1 International Relations in Terrorism,[0],[0]
"The increasing prevalence of islam and decreasing prevalence of arab over this time period can also be seen, for example, using Google’s n-gram viewer, but it of course provides no information about cooccurrence.
",4.1 International Relations in Terrorism,[0],[0]
"This trend has not been previously noted to the best of our knowledge, although an article in the Huffington Post called for news editors to distinguish Muslim from Arab.6 Our observation suggests a conjecture that the news media have increasingly linked terrorism to a religious group rather than an ethnic group, perhaps in part due to the tie between the events of 9/11 and Afghanistan, which is not an Arab or Arabic-speaking country.",4.1 International Relations in Terrorism,[0],[0]
"We leave it to further investigation to confirm or reject this hypothesis.
",4.1 International Relations in Terrorism,[0],[0]
"To further demonstrate the effectiveness of our approach, we compare a pair’s rank using only cooccurrence or prevalence correlation with its rank in our framework.",4.1 International Relations in Terrorism,[0],[0]
Table 1 shows the results for three pairs above.,4.1 International Relations in Terrorism,[0],[0]
"If we had looked at only cooccurrence or prevalence correlation, we would probably have missed these interesting pairs.
",4.1 International Relations in Terrorism,[0],[0]
6http://www.huffingtonpost.com/ haroon-moghul/even-the-new-york-times-d_ b_766658.html,4.1 International Relations in Terrorism,[0],[0]
"In addition to results on topics in §1, we observe unexpected patterns about ethnicity keywords in immigration news.",4.2 Ethnicity Keywords in Immigration,[0],[0]
Our observation starts with a top tryst relation between latino and asian.,4.2 Ethnicity Keywords in Immigration,[0],[0]
"Although these words are likely to cooccur, their prevalence trajectories differ, with the discussion of Asian immigrants in the 1990s giving way to a focus on the word latino from 2000 onward.",4.2 Ethnicity Keywords in Immigration,[0],[0]
"Possible theories to explain this observation include that undocumented immigrants are generally perceived as a Latino issue, or that Latino voters are increasingly influential in U.S. elections.
",4.2 Ethnicity Keywords in Immigration,[0],[0]
"Furthermore, latino holds head-to-head relations with two subgroups of Latin American immigrants: haitian and cuban.",4.2 Ethnicity Keywords in Immigration,[0],[0]
"In particular, the strength of the relation with haitian is ranked #18 in headto-head relations.",4.2 Ethnicity Keywords in Immigration,[0],[0]
"Meanwhile, haitian and cuban have a friendship relation, which is again consistent with structural balance theory.",4.2 Ethnicity Keywords in Immigration,[0],[0]
"The decreasing prevalence of haitian and cuban perhaps speaks to the shifting geographical focus of recent immigration to the U.S., and issues of the Latino panethnicity.",4.2 Ethnicity Keywords in Immigration,[0],[0]
"In fact, a majority of Latinos prefer to identify with their national origin relative to the
pan-ethnic terms (Taylor et al., 2012).",4.2 Ethnicity Keywords in Immigration,[0],[0]
"However, we should also note that much of this coverage relates to a set of specific refugee crises, temporarily elevating the political importance of these nations in the U.S.",4.2 Ethnicity Keywords in Immigration,[0],[0]
"Nevertheless, the underlying social and political reasons behind these head-to-head relations are worth further investigation.",4.2 Ethnicity Keywords in Immigration,[0],[0]
"Finally, we analyze relations between topics in the ACL Anthology.",4.3 Relations between Topics in ACL,[0],[0]
It turns out that “machine translation” is at a central position among top ranked relations in all the four types (Fig.,4.3 Relations between Topics in ACL,[0],[0]
8).7,4.3 Relations between Topics in ACL,[0],[0]
"It is part of the strongest relation in all four types except tryst (ranked #5).
",4.3 Relations between Topics in ACL,[0],[0]
The full relation graph presents further patterns.,4.3 Relations between Topics in ACL,[0],[0]
Friendship demonstrates transitivity: both “machine translation” and “word alignment” have similar relations with other topics.,4.3 Relations between Topics in ACL,[0],[0]
"But such transitivity does not hold for tryst: although the prevalence of “rule, forest methods” is anti-correlated with both “machine translation” and “sentiment analysis”, “sentiment analysis” seldom cooccurs with “rule, for-
7In the ranking, we filtered a topic where the top words are ion, ing, system, process, language, one, input, natural language, processing, grammar.",4.3 Relations between Topics in ACL,[0],[0]
"For the purposes of this corpus, this is effectively a stopword topic.
",4.3 Relations between Topics in ACL,[0],[0]
est methods” because “sentiment analysis” is seldom built on parsing algorithms.,4.3 Relations between Topics in ACL,[0],[0]
"Similarly, “rule, forest methods” and “discourse (coherence)” hold an armsrace relation: they do not tend to cooccur and both decline in relative prevalence as “machine translation” rises.
",4.3 Relations between Topics in ACL,[0],[0]
"The prevalence of each of these ideas in comparison to machine translation is shown in in Fig. 9, which reveals additional detail.",4.3 Relations between Topics in ACL,[0],[0]
We present two strands of related studies in addition to what we have discussed.,5 Related Work,[0],[0]
Trends in ideas.,5 Related Work,[0],[0]
"Most studies have so far examined the trends of ideas individually (Michel et al., 2011; Hall et al., 2008; Rule et al., 2015).",5 Related Work,[0],[0]
"For instance, Hall et al. (2008) present various trends in our own computational linguistics community, including the rise of statistical machine translation.",5 Related Work,[0],[0]
"More recently, rhetorical framing has been used to predict these sorts of patterns (Prabhakaran et al., 2016).",5 Related Work,[0],[0]
An exception is that Shi et al. (2010) use prevalence correlation to analyze lag relations between topics in publications and research grants.,5 Related Work,[0],[0]
"Anecdotally, Grudin (2009) observes a “head-tohead” relation between artificial intelligence and human-computer interaction in research funding.",5 Related Work,[0],[0]
"However, to our knowledge, our work is the first study to systematically characterize relations between ideas.",5 Related Work,[0],[0]
Representation of ideas.,5 Related Work,[0],[0]
"In addition to topics and keywords, studies have also sought to operationalize the “memes” metaphor using quotes and text reuse in the media (Leskovec et al., 2009; Niculae et al., 2015; Smith et al., 2013; Wei et al., 2013).",5 Related Work,[0],[0]
"In topic modeling literature, Blei and Lafferty (2006) also point out that topics do not cooccur independently and explicitly model the cooccurrence within documents.",5 Related Work,[0],[0]
We proposed a method to characterize relations between ideas in texts through the lens of cooccurrence within documents and prevalence correlation over time.,6 Concluding Discussion,[0],[0]
"For the first time, we observe that the distribution of pairwise cooccurrence is unimodal, while the distribution of pairwise prevalence correlation is not always unimodal, and show that they are positively correlated.",6 Concluding Discussion,[0],[0]
"This combination suggests four types of relations between ideas, and these four types are all found to varying extents in our experiments.
",6 Concluding Discussion,[0],[0]
We illustrate our computational method by exploratory studies on news corpora and scientific research papers.,6 Concluding Discussion,[0],[0]
"We not only confirm existing knowledge but also suggest hypotheses around the usage of arab and islam in terrorism and latino and asian in immigration.
",6 Concluding Discussion,[0],[0]
It is important to note that the relations found using our approach depend on the nature of the representation of ideas and the source of texts.,6 Concluding Discussion,[0],[0]
"For instance, we cannot expect relations found in news articles to reflect shifts in public opinion if news articles do not effectively track public opinion.
",6 Concluding Discussion,[0],[0]
Our method is entirely observational.,6 Concluding Discussion,[0],[0]
"It remains as a further stage of analysis to understand the underlying reasons that lead to these relations be-
tween ideas.",6 Concluding Discussion,[0],[0]
"In scientific research, for example, it could simply be the progress of science, i.e., newer ideas overtake older ones deemed less valuable at a given time; on the other hand, history suggests that it is not always the correct ideas that are most expressed, and many other factors may be important.",6 Concluding Discussion,[0],[0]
"Similarly, in news coverage, underlying sociological and political situations have significant impact on which ideas are presented, and how.
",6 Concluding Discussion,[0],[0]
There are many potential directions to improve our method to account for complex relations between ideas.,6 Concluding Discussion,[0],[0]
"For instance, we assume that both ideas and relations are statically grounded in keywords or topics.",6 Concluding Discussion,[0],[0]
"In reality, ideas and relations both evolve over time: a tryst relation might appear as friendship if we focus on a narrower time period.",6 Concluding Discussion,[0],[0]
"Similarly, new ideas show up and even the same idea may change over time and be represented by different words.",6 Concluding Discussion,[0],[0]
Acknowledgments.,6 Concluding Discussion,[0],[0]
"We thank Amber Boydstun, Justin Gross, Lillian Lee, anonymous reviewers, and all members of Noah’s ARK for helpful comments and discussions.",6 Concluding Discussion,[0],[0]
This research was made possible by a Natural Sciences and Engineering Research Council of Canada Postgraduate Scholarship (to D.C.) and a University of Washington Innovation Award.,6 Concluding Discussion,[0],[0]
"Understanding how ideas relate to each other is a fundamental question in many domains, ranging from intellectual history to public communication.",abstractText,[0],[0]
"Because ideas are naturally embedded in texts, we propose the first framework to systematically characterize the relations between ideas based on their occurrence in a corpus of documents, independent of how these ideas are represented.",abstractText,[0],[0]
Combining two statistics—cooccurrence within documents and prevalence correlation over time—our approach reveals a number of different ways in which ideas can cooperate and compete.,abstractText,[0],[0]
"For instance, two ideas can closely track each other’s prevalence over time, and yet rarely cooccur, almost like a “cold war” scenario.",abstractText,[0],[0]
We observe that pairwise cooccurrence and prevalence correlation exhibit different distributions.,abstractText,[0],[0]
We further demonstrate that our approach is able to uncover intriguing relations between ideas through in-depth case studies on news articles and research papers.,abstractText,[0],[0]
"Friendships, Rivalries, and Trysts: Characterizing Relations between Ideas in Texts",title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1051–1062 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1097",text,[0],[0]
"We are interested in learning a semantic parser that maps natural language utterances into executable programs (e.g., logical forms).",1 Introduction,[0],[0]
"For example, in Figure 1, a program corresponding to the utterance transforms an initial world state into a new world state.",1 Introduction,[0],[0]
"We would like to learn from indirect supervision, where each training example is only labeled with the correct output (e.g. a target world state), but not the program that produced that out-
z*
z'0.1
0.1
0.1 0.1 0.1
0.1 0.1 0.1 0.1 0.1
p(z')",1 Introduction,[0],[0]
"= 10-4
p(z*) = 10-6
red
yellow hasHat blue hasShirt leftOf move
move1hasShirt
put (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Liang et al., 2017).
",1 Introduction,[0],[0]
"The process of constructing a program can be formulated as a sequential decision-making process, where feedback is only received at the end of the sequence when the completed program is executed.",1 Introduction,[0],[0]
"In the natural language processing literature, there are two common approaches for handling this situation: 1) reinforcement learning (RL), particularly the REINFORCE algorithm (Williams, 1992; Sutton et al., 1999), which maximizes the expected reward of a sequence of actions; and 2) maximum marginal likelihood (MML), which treats the sequence of actions as a latent variable, and then maximizes the marginal likelihood of observing the correct program output (Dempster et al., 1977).
",1 Introduction,[0],[0]
"While the two approaches have enjoyed success on many tasks, we found them to work poorly out of the box for our task.",1 Introduction,[0],[0]
"This is because in addition to the sparsity of correct programs, our task also requires weeding out spurious programs (Pasupat and Liang, 2016): incorrect interpretations
1051
of the utterances that accidentally produce the correct output, as illustrated in Figure 1.
",1 Introduction,[0],[0]
We show that MML and RL optimize closely related objectives.,1 Introduction,[0],[0]
"Furthermore, both MML and RL methods have a mechanism for exploring program space in search of programs that generate the correct output.",1 Introduction,[0],[0]
"We explain why this exploration tends to quickly concentrate around short spurious programs, causing the model to sometimes overlook the correct program.",1 Introduction,[0],[0]
"To address this problem, we propose RANDOMER, a new learning algorithm with two parts:
First, we propose randomized beam search, an exploration strategy which combines the systematic beam search traditionally employed in MML with the randomized off-policy exploration of RL.",1 Introduction,[0],[0]
"This increases the chance of finding correct programs even when the beam size is small or the parameters are not pre-trained.
",1 Introduction,[0],[0]
"Second, we observe that even with good exploration, the gradients of both the RL and MML objectives may still upweight entrenched spurious programs more strongly than correct programs with low probability under the current model.",1 Introduction,[0],[0]
"We propose a meritocratic parameter update rule, a modification to the MML gradient update, which more equally upweights all programs that produce the correct output.",1 Introduction,[0],[0]
"This makes the model less likely to overfit spurious programs.
",1 Introduction,[0],[0]
"We apply RANDOMER to train a new neural semantic parser, which outputs programs in a stackbased programming language.",1 Introduction,[0],[0]
"We evaluate our resulting system on SCONE, the context-dependent semantic parsing dataset of Long et al. (2016).",1 Introduction,[0],[0]
"Our approach outperforms standard RL and MML methods in a direct comparison, and achieves new state-of-the-art results, improving over Long et al. (2016) in all three domains of SCONE, and by over 30% accuracy on the most challenging one.",1 Introduction,[0],[0]
"We consider the semantic parsing task in the SCONE dataset1 (Long et al., 2016).",2 Task,[0],[0]
"As illustrated in Figure 1, each example consists of a world containing several objects (e.g., people), each with certain properties (e.g., shirt color and hat color).",2 Task,[0],[0]
"Given the initial world state w0 and a sequence of M natural language utterances u = (u1, . . .",2 Task,[0],[0]
", uM ), the task is to generate a program that manipulates the world state according to the utterances.",2 Task,[0],[0]
"Each
1 https://nlp.stanford.edu/projects/scone
utterance um describes a single action that transforms the world state wm−1 into a new world state wm.",2 Task,[0],[0]
"For training, the system receives weakly supervised examples with input x = (u, w0) and the target final world state y = wM .
",2 Task,[0],[0]
"The dataset includes 3 domains: ALCHEMY, TANGRAMS, and SCENE.",2 Task,[0],[0]
"The description of each domain can be found in Appendix B. The domains highlight different linguistic phenomena: ALCHEMY features ellipsis (e.g., “throw the rest out”, “mix”); TANGRAMS features anaphora on actions (e.g., “repeat step 3”, “bring it back”); and SCENE features anaphora on entities (e.g., “he moves back”, “. . .",2 Task,[0],[0]
to his left”).,2 Task,[0],[0]
"Each domain contains roughly 3,700 training and 900 test examples.",2 Task,[0],[0]
"Each example contains 5 utterances and is labeled with the target world state after each utterance, but not the target program.
",2 Task,[0],[0]
Spurious programs.,2 Task,[0],[0]
"Given a training example (u, w0, wM ), our goal is to find the true underlying program z∗ which reflects the meaning of u. The constraint that z∗ must transformw0 intowM , i.e. z(w0) = wM , is not enough to uniquely identify the true z∗, as there are often many z satisfying z(w0) = wM",2 Task,[0],[0]
": in our experiments, we found at least 1600 on average for each example.",2 Task,[0],[0]
Almost all do not capture the meaning of u (see Figure 1).,2 Task,[0],[0]
We refer to these incorrect z’s as spurious programs.,2 Task,[0],[0]
"Such programs encourage the model to learn an incorrect mapping from language to program operations: e.g., the spurious program in Figure 1 would cause the model to learn that “man in the yellow hat” maps to hasShirt(red).
",2 Task,[0],[0]
Spurious programs in SCONE.,2 Task,[0],[0]
"In this dataset, utterances often reference objects in different ways (e.g. a person can be referenced by shirt color, hat color, or position).",2 Task,[0],[0]
"Hence, any target programming language must also support these different reference strategies.",2 Task,[0],[0]
"As a result, even a single action such as moving a person to a target destination can be achieved by many different programs, each selecting the person and destination in a different way.",2 Task,[0],[0]
"Across multiple actions, the number of programs grows combinatorially.2",2 Task,[0],[0]
Only a few programs actually implement the correct reference strategy as defined by the utterance.,2 Task,[0],[0]
"This problem would be more severe in any more general-purpose language (e.g. Python).
2The number of well-formed programs in SCENE exceeds 1015",2 Task,[0],[0]
We formulate program generation as a sequence prediction problem.,3 Model,[0],[0]
"We represent a program as a sequence of program tokens in postfix notation; for example, move(hasHat(yellow), leftOf(hasShirt(blue))) is linearized as yellow hasHat blue hasShirt leftOf move.",3 Model,[0],[0]
"This representation also allows us to incrementally execute programs from left to right using a stack: constants (e.g., yellow) are pushed onto the stack, while functions (e.g., hasHat) pop appropriate arguments from the stack and push back the computed result (e.g., the list of people with yellow hats).",3 Model,[0],[0]
"Appendix B lists the full set of program tokens, Z , and how they are executed.",3 Model,[0],[0]
"Note that each action always ends with an action token (e.g., move).
",3 Model,[0],[0]
"Given an input x = (u, w0), the model generates program tokens z1, z2, . . .",3 Model,[0],[0]
"from left to right using a neural encoder-decoder model with attention (Bahdanau et al., 2015).",3 Model,[0],[0]
"Throughout the generation process, the model maintains an utterance pointer, m, initialized to 1.",3 Model,[0],[0]
"To generate zt, the model’s encoder first encodes the utterance um into a vector em.",3 Model,[0],[0]
"Then, based on em and previously generated tokens z1:t−1, the model’s decoder defines a distribution p(zt | x, z1:t−1) over the possible values of zt ∈ Z .",3 Model,[0],[0]
The next token zt is sampled from this distribution.,3 Model,[0],[0]
"If an action token (e.g., move) is generated, the model increments the utterance pointer m. The process terminates when all M utterances are processed.",3 Model,[0],[0]
"The final probability of generating a particular program z = (z1, . . .",3 Model,[0],[0]
", zT ) is",3 Model,[0],[0]
p(z | x) = ∏T t=1,3 Model,[0],[0]
p(zt,3 Model,[0],[0]
"| x, z1:t−1).
",3 Model,[0],[0]
Encoder.,3 Model,[0],[0]
"The utterance um under the pointer is encoded using a bidirectional LSTM:
hFi = LSTM(h F i−1,Φu(um,i))",3 Model,[0],[0]
"hBi = LSTM(h B i+1,Φu(um,i))
",3 Model,[0],[0]
hi =,3 Model,[0],[0]
"[h F i ;h B i ],
where Φu(um,i) is the fixed GloVe word embedding (Pennington et al., 2014) of the ith word in um.",3 Model,[0],[0]
The final utterance embedding is the concatenation em =,3 Model,[0],[0]
"[hF|um|;h B 1 ].
Decoder.",3 Model,[0],[0]
"Unlike Bahdanau et al. (2015), which used a recurrent network for the decoder, we opt for a feed-forward network for simplicity.",3 Model,[0],[0]
"We use em and an embedding f(z1:t−1) of the previous execution history (described later) as inputs to
compute an attention vector ct:
qt = ReLU(Wq[em; f(z1:t−1)])",3 Model,[0],[0]
αi ∝,3 Model,[0],[0]
"exp(q>t Wahi) (i = 1, . . .",3 Model,[0],[0]
", |um|) ct = ∑
i
αihi.
",3 Model,[0],[0]
"Finally, after concatenating qt with ct, the distribution over the set Z of possible program tokens is computed via a softmax:
p(zt | x, z1:t−1) ∝",3 Model,[0],[0]
"exp(Φz(zt)>Ws[qt; ct]),
where Φz(zt) is the embedding for token zt.
",3 Model,[0],[0]
Execution history embedding.,3 Model,[0],[0]
"We compare two options for f(z1:t−1), our embedding of the execution history.",3 Model,[0],[0]
A standard approach is to simply take the k most recent tokens zt−k:t−1 and concatenate their embeddings.,3 Model,[0],[0]
"We will refer to this as TOKENS and use k = 4 in our experiments.
",3 Model,[0],[0]
We also consider a new approach which leverages our ability to incrementally execute programs using a stack.,3 Model,[0],[0]
"We summarize the execution history by embedding the state of the stack at time t − 1, achieved by concatenating the embeddings of all values on the stack.",3 Model,[0],[0]
(We limit the maximum stack size to 3.),3 Model,[0],[0]
We refer to this as STACK.,3 Model,[0],[0]
"Having formulated our task as a sequence prediction problem, we must still choose a learning algorithm.",4 Reinforcement learning versus maximum marginal likelihood,[0],[0]
We first compare two standard paradigms: reinforcement learning (RL) and maximum marginal likelihood (MML).,4 Reinforcement learning versus maximum marginal likelihood,[0],[0]
"In the next section, we propose a better alternative.",4 Reinforcement learning versus maximum marginal likelihood,[0],[0]
Reinforcement learning.,4.1 Comparing objective functions,[0],[0]
"From an RL perspective, given a training example (x, y), a policy makes a sequence of decisions z = (z1, . . .",4.1 Comparing objective functions,[0],[0]
", zT ), and then receives a reward at the end of the episode: R(z)",4.1 Comparing objective functions,[0],[0]
"= 1 if z executes to y and 0 otherwise (dependence on x and y has been omitted from the notation).
",4.1 Comparing objective functions,[0],[0]
"We focus on policy gradient methods, in which a stochastic policy function is trained to maximize the expected reward.",4.1 Comparing objective functions,[0],[0]
"In our setup, pθ(z | x) is the policy (with parameters θ), and its expected reward on a given example (x, y) is
G(x, y) = ∑
z
R(z) pθ(z | x), (1)
where the sum is over all possible programs.",4.1 Comparing objective functions,[0],[0]
"The overall RL objective, JRL, is the expected reward across examples:
JRL = ∑
(x,y)
G(x, y).",4.1 Comparing objective functions,[0],[0]
"(2)
Maximum marginal likelihood.",4.1 Comparing objective functions,[0],[0]
"The MML perspective assumes that y is generated by a partially-observed random process: conditioned on x, a latent program z is generated, and conditioned on z, the observation y is generated.",4.1 Comparing objective functions,[0],[0]
"This implies the marginal likelihood:
pθ(y",4.1 Comparing objective functions,[0],[0]
"| x) = ∑
z
p(y | z) pθ(z | x).",4.1 Comparing objective functions,[0],[0]
"(3)
Note that since the execution of z is deterministic, pθ(y | z) = 1 if z executes to y and 0 otherwise.",4.1 Comparing objective functions,[0],[0]
"The log marginal likelihood of the data is then
JMML = logLMML, (4)",4.1 Comparing objective functions,[0],[0]
"where LMML = ∏
(x,y)
pθ(y | x).",4.1 Comparing objective functions,[0],[0]
"(5)
To estimate our model parameters θ, we maximize JMML with respect to θ.
",4.1 Comparing objective functions,[0],[0]
"With our choice of reward, the RL expected reward (1) is equal to the MML marginal probability (3).",4.1 Comparing objective functions,[0],[0]
"Hence the only difference between the two formulations is that in RL we optimize the sum of expected rewards (2), whereas in MML we optimize the product (5).3",4.1 Comparing objective functions,[0],[0]
"In both policy gradient and MML, the objectives are typically optimized via (stochastic) gradient ascent.",4.2 Comparing gradients,[0],[0]
The gradients of JRL and JMML are closely related.,4.2 Comparing gradients,[0],[0]
"They both have the form:
∇θJ = ∑
(x,y)
Ez∼q",4.2 Comparing gradients,[0],[0]
[R(z)∇ log pθ(z | x)],4.2 Comparing gradients,[0],[0]
"(6)
= ∑
(x,y)
∑
z
q(z)R(z)∇ log pθ(z | x),
where q(z) equals
qRL(z) = pθ(z | x) for JRL, (7)
qMML(z) = R(z)pθ(z | x)∑ z̃R(z̃)pθ(z̃",4.2 Comparing gradients,[0],[0]
"| x)
",4.2 Comparing gradients,[0],[0]
"(8)
= pθ(z | x,R(z) 6= 0) for JMML. 3",4.2 Comparing gradients,[0],[0]
"Note that the log of the product in (5) does not equal the
sum in (2).
",4.2 Comparing gradients,[0],[0]
"Taking a step in the direction of∇ log pθ(z | x) upweights the probability of z, so we can heuristically think of the gradient as attempting to upweight each reward-earning program z by a gradient weight q(z).",4.2 Comparing gradients,[0],[0]
"In Subsection 5.2, we argue why qMML is better at guarding against spurious programs, and propose an even better alternative.",4.2 Comparing gradients,[0],[0]
It is often intractable to compute the gradient (6) because it involves taking an expectation over all possible programs.,4.3 Comparing gradient approximation strategies,[0],[0]
"So in practice, the expectation is approximated.
",4.3 Comparing gradient approximation strategies,[0],[0]
"In the policy gradient literature, Monte Carlo integration (MC) is the typical approximation strategy.",4.3 Comparing gradient approximation strategies,[0],[0]
"For example, the popular REINFORCE algorithm (Williams, 1992) uses Monte Carlo sampling to compute an unbiased estimate of the gradient:
∆MC = 1
B
∑ z∈S",4.3 Comparing gradient approximation strategies,[0],[0]
"[R(z)− c]∇ log pθ(z | x), (9)
where S is a collection of B samples z(b) ∼ q(z), and c is a baseline (Williams, 1992) used to reduce the variance of the estimate without altering its expectation.
",4.3 Comparing gradient approximation strategies,[0],[0]
"In the MML literature for latent sequences, the expectation is typically approximated via numerical integration (NUM) instead:
∆NUM = ∑
z∈S q(z)R(z)∇ log pθ(z | x).",4.3 Comparing gradient approximation strategies,[0],[0]
"(10)
where the programs in S come from beam search.
",4.3 Comparing gradient approximation strategies,[0],[0]
Beam search.,4.3 Comparing gradient approximation strategies,[0],[0]
Beam search generates a set of programs via the following process.,4.3 Comparing gradient approximation strategies,[0],[0]
"At step t of beam search, we maintain a beam Bt of at most B search states.",4.3 Comparing gradient approximation strategies,[0],[0]
"Each state s ∈ Bt represents a partially constructed program, s = (z1, . . .",4.3 Comparing gradient approximation strategies,[0],[0]
", zt) (the first t tokens of the program).",4.3 Comparing gradient approximation strategies,[0],[0]
"For each state s in the beam, we generate all possible continuations,
cont(s) = cont((z1, . . .",4.3 Comparing gradient approximation strategies,[0],[0]
", zt))
",4.3 Comparing gradient approximation strategies,[0],[0]
"= {(z1, . . .",4.3 Comparing gradient approximation strategies,[0],[0]
", zt, zt+1)",4.3 Comparing gradient approximation strategies,[0],[0]
"| zt+1 ∈ Z} .
",4.3 Comparing gradient approximation strategies,[0],[0]
"We then take the union of these continuations, cont(Bt) =",4.3 Comparing gradient approximation strategies,[0],[0]
⋃ s∈Bt cont(s).,4.3 Comparing gradient approximation strategies,[0],[0]
"The new beam Bt+1 is simply the highest scoringB continuations in cont(Bt), as scored by the policy, pθ(s | x).",4.3 Comparing gradient approximation strategies,[0],[0]
"Search is halted after a fixed number of iterations
or when there are no continuations possible.",4.3 Comparing gradient approximation strategies,[0],[0]
S is then the set of all complete programs discovered during beam search.,4.3 Comparing gradient approximation strategies,[0],[0]
"We will refer to this as beam search MML (BS-MML).
",4.3 Comparing gradient approximation strategies,[0],[0]
"In both policy gradient and MML, we think of the procedure used to produce the set of programs S as an exploration strategy which searches for programs that produce reward.",4.3 Comparing gradient approximation strategies,[0],[0]
One advantage of numerical integration is that it allows us to decouple the exploration strategy from the gradient weights assigned to each program.,4.3 Comparing gradient approximation strategies,[0],[0]
"In this section, we illustrate why spurious programs are problematic for the most commonly used methods in RL (REINFORCE) and MML (beam search MML).",5 Tackling spurious programs,[0],[0]
"We describe two key problems and propose a solution to each, based on insights gained from our comparison of RL and MML in Section 4.",5 Tackling spurious programs,[0],[0]
"As mentioned in Section 4, REINFORCE and BSMML both employ an exploration strategy to approximate their respective gradients.",5.1 Spurious programs bias exploration,[0],[0]
"In both methods, exploration is guided by the current model policy, whereby programs with high probability under the current policy are more likely to be explored.",5.1 Spurious programs bias exploration,[0],[0]
"A troubling implication is that programs with low probability under the current policy are likely to be overlooked by exploration.
",5.1 Spurious programs bias exploration,[0],[0]
"If the current policy incorrectly assigns low probability to the correct program z∗, it will likely fail to discover z∗ during exploration, and will consequently fail to upweight the probability of z∗.",5.1 Spurious programs bias exploration,[0],[0]
"This repeats on every gradient step, keeping the probability of z∗ perpetually low.",5.1 Spurious programs bias exploration,[0],[0]
The same feedback loop can also cause already highprobability spurious programs to gain even more probability.,5.1 Spurious programs bias exploration,[0],[0]
"From this, we see that exploration is sensitive to initial conditions: the rich get richer, and the poor get poorer.
",5.1 Spurious programs bias exploration,[0],[0]
"Since there are often thousands of spurious programs and only a few correct programs, spurious programs are usually found first.",5.1 Spurious programs bias exploration,[0],[0]
"Once spurious programs get a head start, exploration increasingly biases towards them.
",5.1 Spurious programs bias exploration,[0],[0]
"As a remedy, one could try initializing parameters such that the model puts a uniform distribution over all possible programs.",5.1 Spurious programs bias exploration,[0],[0]
"A seemingly reasonable tactic is to initialize parameters such that the
""The man in the yellow hat moves to the left of the woman in blue.”
",5.1 Spurious programs bias exploration,[0],[0]
"Spurious: move(hasShirt(red), 1) Correct: move(hasHat(yellow), leftOf(hasShirt(blue)))
1 2 3 1 2 3
BEFORE AFTER
model policy puts near-uniform probability over the decisions at each time step.",5.1 Spurious programs bias exploration,[0],[0]
"However, this causes shorter programs to have orders of magnitude higher probability than longer programs, as illustrated in Figure 2 and as we empirically observe.",5.1 Spurious programs bias exploration,[0],[0]
A more sophisticated approach might involve approximating the total number of programs reachable from each point in the programgenerating decision tree.,5.1 Spurious programs bias exploration,[0],[0]
"However, we instead propose to reduce sensitivity to the initial distribution over programs.
",5.1 Spurious programs bias exploration,[0],[0]
Solution: randomized beam search One solution to biased exploration is to simply rely less on the untrustworthy current policy.,5.1 Spurious programs bias exploration,[0],[0]
"We can do this by injecting random noise into exploration.
",5.1 Spurious programs bias exploration,[0],[0]
"In REINFORCE, a common solution is to sample from an -greedy variant of the current policy.",5.1 Spurious programs bias exploration,[0],[0]
"On the other hand, MML exploration with beam search is deterministic.",5.1 Spurious programs bias exploration,[0],[0]
"However, it has a key advantage over REINFORCE-style sampling: even if one program occupies almost all probability under the current policy (a peaky distribution), beam search will still use its remaining beam capacity to explore at least B− 1 other programs.",5.1 Spurious programs bias exploration,[0],[0]
"In contrast, sampling methods will repeatedly visit the mode of the distribution.
",5.1 Spurious programs bias exploration,[0],[0]
"To get the best of both worlds, we propose a simple -greedy randomized beam search.",5.1 Spurious programs bias exploration,[0],[0]
"Like regular beam search, at iteration t we compute the set of all continuations cont(Bt) and sort them by their model probability pθ(s | x).",5.1 Spurious programs bias exploration,[0],[0]
"But instead of selecting the B highest-scoring continuations, we choose B continuations one by one without replacement from cont(Bt).",5.1 Spurious programs bias exploration,[0],[0]
"When choosing a continuation from the remaining pool, we either uniformly sample a random continuation with probability , or pick the highest-scoring continuation in the pool with probability 1− .",5.1 Spurious programs bias exploration,[0],[0]
"Empirically, we
find that this performs much better than both classic beam search and -greedy sampling (Table 3).",5.1 Spurious programs bias exploration,[0],[0]
"In both RL and MML, even if exploration is perfect and the gradient is exactly computed, spurious programs can still be problematic.
",5.2 Spurious programs dominate gradients,[0],[0]
"Even if perfect exploration visits every program, we see from the gradient weights q(z) in (7) and (8) that programs are weighted proportional to their current policy probability.",5.2 Spurious programs dominate gradients,[0],[0]
"If a spurious program z′ has 100 times higher probability than z∗ as in Figure 2, the gradient will spend roughly 99% of its magnitude upweighting towards z′ and only 1% towards z∗ even though the two programs get the same reward.
",5.2 Spurious programs dominate gradients,[0],[0]
This implies that it would take many updates for z∗ to catch up.,5.2 Spurious programs dominate gradients,[0],[0]
"In fact, z∗ may never catch up, depending on the gradient updates for other training examples.",5.2 Spurious programs dominate gradients,[0],[0]
"Simply increasing the learning rate is inadequate, as it would cause the model to take overly large steps towards z′, potentially causing optimization to diverge.
",5.2 Spurious programs dominate gradients,[0],[0]
"Solution: the meritocratic update rule To solve this problem, we want the upweighting to be more “meritocratic”: any program that obtains reward should be upweighted roughly equally.
",5.2 Spurious programs dominate gradients,[0],[0]
We first observe that JMML already improves over JRL in this regard.,5.2 Spurious programs dominate gradients,[0],[0]
"From (6), we see that the gradient weight qMML(z) is the policy distribution restricted to and renormalized over only rewardearning programs.",5.2 Spurious programs dominate gradients,[0],[0]
"This renormalization makes the gradient weight uniform across examples: even if all reward-earning programs for a particular example have very low model probability, their combined gradient weight ∑ z qMML(z) is always 1.",5.2 Spurious programs dominate gradients,[0],[0]
"In our experiments, JMML performs significantly better than JRL (Table 4).
",5.2 Spurious programs dominate gradients,[0],[0]
"However, while JMML assigns uniform weight across examples, it is still not uniform over the programs within each example.",5.2 Spurious programs dominate gradients,[0],[0]
Hence we propose a new update rule which goes one step further in pursuing uniform updates.,5.2 Spurious programs dominate gradients,[0],[0]
"Extending qMML(z), we define a β-smoothed version:
qβ(z) = qMML(z)
β
∑ z̃ qMML(z̃) β .",5.2 Spurious programs dominate gradients,[0],[0]
"(11)
When β = 0, our weighting is completely uniform across all reward-earning programs within an example while β = 1 recovers the original MML weighting.",5.2 Spurious programs dominate gradients,[0],[0]
"Our new update rule is to simply take
a modified gradient step where q = qβ .4 We will refer to this as the β-meritocratic update rule.",5.2 Spurious programs dominate gradients,[0],[0]
We described two problems5 and their solutions: we reduce exploration bias using -greedy randomized beam search and perform more balanced optimization using the β-meritocratic parameter update rule.,5.3 Summary of the proposed approach,[0],[0]
We call our resulting approach RANDOMER.,5.3 Summary of the proposed approach,[0],[0]
Table 1 summarizes how RANDOMER combines desirable qualities from both REINFORCE and BS-MML.,5.3 Summary of the proposed approach,[0],[0]
Evaluation.,6 Experiments,[0],[0]
We evaluate our proposed methods on all three domains of the SCONE dataset.,6 Experiments,[0],[0]
Accuracy is defined as the percentage of test examples where the model produces the correct final world state wM .,6 Experiments,[0],[0]
"All test examples have M = 5 (5utts), but we also report accuracy after processing the first 3 utterances (3utts).",6 Experiments,[0],[0]
"To control for the effects of randomness, we train 5 instances of each model with different random seeds.",6 Experiments,[0],[0]
"We report the median accuracy of the instances unless otherwise noted.
Training.",6 Experiments,[0],[0]
"Following Long et al. (2016), we decompose each training example into smaller examples.",6 Experiments,[0],[0]
"Given an example with 5 utterances, u = [u1, . . .",6 Experiments,[0],[0]
", u5], we consider all length-1 and length-2 substrings of u:",6 Experiments,[0],[0]
"[u1], [u2], . . .",6 Experiments,[0],[0]
",",6 Experiments,[0],[0]
"[u3, u4], [u4, u5] (9 total).",6 Experiments,[0],[0]
"We form a new training example from each substring, e.g., (u′, w′0, w ′ M )",6 Experiments,[0],[0]
"where u
′ =",6 Experiments,[0],[0]
"[u4, u5], w′0 = w3 and w ′",6 Experiments,[0],[0]
"M = w5.
",6 Experiments,[0],[0]
"All models are implemented in TensorFlow (Abadi et al., 2015).",6 Experiments,[0],[0]
"Model parameters are randomly initialized (Glorot and Bengio, 2010), with no pre-training.",6 Experiments,[0],[0]
"We use the Adam optimizer (Kingma and Ba, 2014) (which is applied to the gradient in (6)), a learning rate of 0.001, a minibatch size of 8 examples (different from the beam size), and train until accuracy on the validation set converges (on average about 13,000 steps).",6 Experiments,[0],[0]
"We
4 Also, note that if exploration were exhaustive, β = 0 would be equivalent to supervised learning using the set of all reward-earning programs as targets.
",6 Experiments,[0],[0]
5,6 Experiments,[0],[0]
These problems concern the gradient w.r.t.,6 Experiments,[0],[0]
a single example.,6 Experiments,[0],[0]
"The full gradient averages over multiple examples, which helps separate correct from spurious.",6 Experiments,[0],[0]
"E.g., if multiple examples all mention “yellow hat”, we will find a correct program parsing this as hasHat(yellow) for each example, whereas the spurious programs we find will follow no consistent pattern.",6 Experiments,[0],[0]
"Consequently, spurious gradient contributions may cancel out while correct program gradients will all “vote” in the same direction.
",6 Experiments,[0],[0]
"use fixed GloVe vectors (Pennington et al., 2014) to embed the words in each utterance.
Hyperparameters.",6 Experiments,[0],[0]
"For all models, we performed a grid search over hyperparameters to maximize accuracy on the validation set.",6 Experiments,[0],[0]
"Hyperparameters include the learning rate, the baseline in REINFORCE, -greediness and βmeritocraticness.",6 Experiments,[0],[0]
"For REINFORCE, we also experimented with a regression-estimated baseline (Ranzato et al., 2015), but found it to perform worse than a constant baseline.",6 Experiments,[0],[0]
Comparison to prior work.,6.1 Main results,[0],[0]
"Table 2 compares RANDOMER to results from Long et al. (2016) as well as two baselines, REINFORCE and BSMML",6.1 Main results,[0],[0]
(using the same neural model but different learning algorithms).,6.1 Main results,[0],[0]
"Our approach achieves new state-of-the-art results by a significant margin, especially on the SCENE domain, which features the most complex program syntax.",6.1 Main results,[0],[0]
"We report the results for REINFORCE, BS-MML, and RANDOMER on the seed and hyperparameters that achieve the best validation accuracy.
",6.1 Main results,[0],[0]
We note that REINFORCE performs very well on TANGRAMS but worse on ALCHEMY and very poorly on SCENE.,6.1 Main results,[0],[0]
"This might be because the program syntax for TANGRAMS is simpler than the other two: there is no other way to refer to objects except by index.
",6.1 Main results,[0],[0]
We also found that REINFORCE required - greedy exploration to make any progress.,6.1 Main results,[0],[0]
"Using -greedy greatly skews the Monte Carlo approximation of ∇JRL, making it more uniformly weighted over programs in a similar spirit to using β-meritocratic gradient weights qβ .",6.1 Main results,[0],[0]
"However, qβ increases uniformity over reward-earning programs only, rather than over all programs.
",6.1 Main results,[0],[0]
Effect of randomized beam search.,6.1 Main results,[0],[0]
Table 3 shows that -greedy randomized beam search consistently outperforms classic beam search.,6.1 Main results,[0],[0]
"Even when we increase the beam size of classic beam
search to 128, it still does not surpass randomized beam search with a beam of 32, and further increases yield no additional improvement.
",6.1 Main results,[0],[0]
Effect of β-meritocratic updates.,6.1 Main results,[0],[0]
Table 4 evaluates the impact of β-meritocratic parameter updates (gradient weight qβ).,6.1 Main results,[0],[0]
"More uniform upweighting across reward-earning programs leads to higher accuracy and fewer spurious programs, especially in SCENE.",6.1 Main results,[0],[0]
"However, no single value of β performs best over all domains.
",6.1 Main results,[0],[0]
Choosing the right value of β in RANDOMER significantly accelerates training.,6.1 Main results,[0],[0]
"Figure 3 illustrates that while β = 0 and β = 1 ultimately achieve similar accuracy on ALCHEMY, β = 0 reaches good performance in half the time.
",6.1 Main results,[0],[0]
"Since lowering β reduces trust in the model policy, β < 1 helps in early training when the current policy is untrustworthy.",6.1 Main results,[0],[0]
"However, as it grows more trustworthy, β < 1 begins to pay a price for ignoring it.",6.1 Main results,[0],[0]
"Hence, it may be worthwhile to anneal β towards 1 over time.
",6.1 Main results,[0],[0]
Effect of execution history embedding.,6.1 Main results,[0],[0]
Table 5 compares our two proposals for embedding the execution history: TOKENS and STACK.,6.1 Main results,[0],[0]
STACK performs better in the two domains where an object can be referenced in multiple ways (SCENE and ALCHEMY).,6.1 Main results,[0],[0]
"STACK directly embeds objects on the stack, invariant to the way in which they were pushed onto the stack, unlike TOKENS.",6.1 Main results,[0],[0]
"We hypothesize that this invariance increases robustness to spurious behavior: if a program accidentally pushes the right object onto the stack via spurious means, the model can still learn the remaining steps of the program without conditioning on a spurious history.
",6.1 Main results,[0],[0]
Fitting vs overfitting the training data.,6.1 Main results,[0],[0]
Table 6 reveals that BS-MML and RANDOMER use different strategies to fit the training data.,6.1 Main results,[0],[0]
"On the depicted training example, BS-MML actually achieves higher expected reward / marginal probability than RANDOMER, but it does so by putting most of its probability on a spurious program— a form of overfitting.",6.1 Main results,[0],[0]
"In contrast, RANDOMER spreads probability mass over multiple rewardearning programs, including the correct ones.
",6.1 Main results,[0],[0]
"As a consequence of overfitting, we observed at test time that BS-MML only references people by positional indices instead of by shirt or hat color, whereas RANDOMER successfully learns to use multiple reference strategies.",6.1 Main results,[0],[0]
Semantic parsing from indirect supervision.,7 Related work and discussion,[0],[0]
"Our work is motivated by the classic problem of learning semantic parsers from indirect supervision (Clarke et al., 2010; Liang et al., 2011; Artzi
and Zettlemoyer, 2011, 2013; Reddy et al., 2014; Pasupat and Liang, 2015).",7 Related work and discussion,[0],[0]
"We are interested in the initial stages of training from scratch, where getting any training signal is difficult due to the combinatorially large search space.",7 Related work and discussion,[0],[0]
"We also highlighted the problem of spurious programs which capture reward but give incorrect generalizations.
",7 Related work and discussion,[0],[0]
"Maximum marginal likelihood with beam search (BS-MML) is traditionally used to learn semantic parsers from indirect supervision.
",7 Related work and discussion,[0],[0]
Reinforcement learning.,7 Related work and discussion,[0],[0]
"Concurrently, there has been a recent surge of interest in reinforcement learning, along with the wide application of the classic REINFORCE algorithm (Williams, 1992)—to troubleshooting (Branavan et al., 2009), dialog generation (Li et al., 2016), game playing (Narasimhan et al., 2015), coreference resolution (Clark and Manning, 2016), machine translation (Norouzi et al., 2016), and even semantic parsing (Liang et al., 2017).",7 Related work and discussion,[0],[0]
"Indeed, the challenge of training semantic parsers from indirect supervision is perhaps better captured by the notion of sparse rewards in reinforcement learning.
",7 Related work and discussion,[0],[0]
"The RL answer would be better exploration, which can take many forms including simple action-dithering such as -greedy, entropy regularization (Williams and Peng, 1991), Monte Carlo tree search (Coulom, 2006), randomized value functions (Osband et al., 2014, 2016), and methods which prioritize learning environment dynamics (Duff, 2002) or under-explored states (Kearns and Singh, 2002; Bellemare et al., 2016; Nachum et al., 2016).",7 Related work and discussion,[0],[0]
The majority of these methods employ Monte Carlo sampling for exploration.,7 Related work and discussion,[0],[0]
"In
contrast, we find randomized beam search to be more suitable in our setting, because it explores low-probability states even when the policy distribution is peaky.",7 Related work and discussion,[0],[0]
"Our β-meritocratic update also depends on the fact that beam search returns an entire set of reward-earning programs rather than one, since it renormalizes over the reward-earning set.",7 Related work and discussion,[0],[0]
"While similar to entropy regularization, βmeritocratic update is more targeted as it only increases uniformity of the gradient among rewardearning programs, rather than across all programs.
",7 Related work and discussion,[0],[0]
"Our strategy of using randomized beam search and meritocratic updates lies closer to MML than RL, but this does not imply that RL has nothing to offer in our setting.",7 Related work and discussion,[0],[0]
"With the simple connection between RL and MML we established, much of the literature on exploration and variance reduction in RL can be directly applied to MML problems.",7 Related work and discussion,[0],[0]
"Of special interest are methods which incorporate a value function such as actor-critic.
",7 Related work and discussion,[0],[0]
Maximum likelihood and RL.,7 Related work and discussion,[0],[0]
"It is tempting to group our approach with sequence learning methods which interpolate between supervised learning and reinforcement learning (Ranzato et al., 2015; Venkatraman et al., 2015; Ross et al., 2011; Norouzi et al., 2016; Bengio et al., 2015; Levine,
2014).",7 Related work and discussion,[0],[0]
These methods generally seek to make RL training easier by pre-training or “warm-starting” with fully supervised learning.,7 Related work and discussion,[0],[0]
This requires each training example to be labeled with a reasonably correct output sequence.,7 Related work and discussion,[0],[0]
"In our setting, this would amount to labeling each example with the correct program, which is not known.",7 Related work and discussion,[0],[0]
"Hence, these methods cannot be directly applied.
",7 Related work and discussion,[0],[0]
"Without access to correct output sequences, we cannot directly maximize likelihood, and instead resort to maximizing the marginal likelihood (MML).",7 Related work and discussion,[0],[0]
"Rather than proposing MML as a form of pre-training, we argue that MML is a superior substitute for the standard RL objective, and that the β-meritocratic update is even better.
",7 Related work and discussion,[0],[0]
Simulated annealing.,7 Related work and discussion,[0],[0]
"Our β-meritocratic update employs exponential smoothing, which bears resemblance to the simulated annealing strategy of Och (2003); Smith and Eisner (2006); Shen et al. (2015).",7 Related work and discussion,[0],[0]
"However, a key difference is that these methods smooth the objective function whereas we smooth an expectation in the gradient.",7 Related work and discussion,[0],[0]
"To underscore the difference, we note that fixing β = 0 in our method (total smoothing) is quite effective, whereas total smoothing in the simulated annealing methods would correspond to a completely flat objective function, and an uninformative gradient of zero everywhere.
",7 Related work and discussion,[0],[0]
Neural semantic parsing.,7 Related work and discussion,[0],[0]
"There has been recent interest in using recurrent neural networks for semantic parsing, both for modeling logical forms (Dong and Lapata, 2016; Jia and Liang, 2016; Liang et al., 2017) and for end-to-end execution (Yin et al., 2015; Neelakantan et al., 2016).",7 Related work and discussion,[0],[0]
"We develop a neural model for the context-dependent setting, which is made possible by a new stackbased language similar to Riedel et al. (2016).
Acknowledgments.",7 Related work and discussion,[0],[0]
This work was supported by the NSF Graduate Research Fellowship under No.,7 Related work and discussion,[0],[0]
DGE-114747 and the NSF CAREER Award under No.,7 Related work and discussion,[0],[0]
"IIS-1552635.
Reproducibility.",7 Related work and discussion,[0],[0]
Our code is made available at https://github.com/kelvinguu/lang2program.,7 Related work and discussion,[0],[0]
Reproducible experiments are available at https://worksheets.codalab.org/worksheets/ 0x88c914ee1d4b4a4587a07f36f090f3e5/.,7 Related work and discussion,[0],[0]
"System ALCHEMY TANGRAMS SCENE
REINFORCE
Sample size 32 Baseline 10−2
= 0.15 embed TOKENS
Sample size 32 Baseline 10−2
= 0.15 embed TOKENS
Sample size 32 Baseline 10−4
= 0.15 embed TOKENS
BS-MML Beam size 128 embed TOKENS Beam size 128 embed TOKENS Beam size 128 embed TOKENS
RANDOMER β = 1 = 0.05 embed TOKENS β",A Hyperparameters in Table 2,[0],[0]
= 1 = 0.15 embed TOKENS β,A Hyperparameters in Table 2,[0],[0]
"= 0 = 0.15 embed STACK
B SCONE domains and program tokens token type semantics Shared across ALCHEMY, TANGRAMS, SCENE 1, 2, 3, . . .",A Hyperparameters in Table 2,[0],[0]
"constant push: number -1, -2, -3, . . .",A Hyperparameters in Table 2,[0],[0]
"red, yellow, green, constant push: color orange, purple, brown allObjects constant push: the list of all objects index function pop: a list L and a number i
push: the object L[i] (the index starts from 1; negative indices are allowed) prevArgj (j = 1, 2) function pop: a number i push: the j argument from the ith action prevAction action pop: a number i perform: fetch the ith action and execute it using the arguments on the stack Additional tokens for the ALCHEMY domain An ALCHEMY world contains 7 beakers.",A Hyperparameters in Table 2,[0],[0]
Each beaker may contain up to 4 units of colored chemical.,A Hyperparameters in Table 2,[0],[0]
1/1 constant push: fraction (used in the drain action) hasColor function pop: a color c push: list of beakers with chemical color c drain action pop: a beaker b and a number or fraction a perform: remove a units of chemical (or all chemical if a = 1/1) from b pour action pop: two beakers b1 and b2 perform: transfer all chemical from b1 to b2 mix action pop: a beaker b perform: turn the color of the chemical in b to brown Additional tokens for the TANGRAMS domain A TANGRAMS world contains a row of tangram pieces with different shapes.,A Hyperparameters in Table 2,[0],[0]
"The shapes are anonymized; a tangram can be referred to by an index or a history reference, but not by shape.",A Hyperparameters in Table 2,[0],[0]
swap action pop: two tangrams t1 and t2 perform: exchange the positions of t1 and t2 remove action pop: a tangram t perform: remove t from the stage add action pop: a number i and a previously removed tangram t perform: insert t to position i Additional tokens for the SCENE domain A SCENE world is a linear stage with 10 positions.,A Hyperparameters in Table 2,[0],[0]
Each position may be occupied by a person with a colored shirt and optionally a colored hat.,A Hyperparameters in Table 2,[0],[0]
There are usually 1-5 people on the stage.,A Hyperparameters in Table 2,[0],[0]
noHat constant push: pseudo-color (indicating that the person is not wearing a hat),A Hyperparameters in Table 2,[0],[0]
"hasShirt, hasHat function pop: a color c push: the list of all people with shirt or hat color c hasShirtHat function pop: two colors c1 and c2 push: the list of all people with shirt color c1 and hat color c2 leftOf, rightOf function pop: a person p push: the location index left or right of p create action pop: a number i and two colors c1, c2 perform: add a new person at position i with shirt color c1 and hat color c2 move action pop: a person p and a number i perform: move p to position i swapHats action pop: two people p1 and p2 perform: have p1 and p2 exchange their hats leave action pop: a person p
perform: remove p from the stage",A Hyperparameters in Table 2,[0],[0]
"Our goal is to learn a semantic parser that maps natural language utterances into executable programs when only indirect supervision is available: examples are labeled with the correct execution result, but not the program itself.",abstractText,[0],[0]
"Consequently, we must search the space of programs for those that output the correct result, while not being misled by spurious programs: incorrect programs that coincidentally output the correct result.",abstractText,[0],[0]
"We connect two common learning paradigms, reinforcement learning (RL) and maximum marginal likelihood (MML), and then present a new learning algorithm that combines the strengths of both.",abstractText,[0],[0]
"The new algorithm guards against spurious programs by combining the systematic search traditionally employed in MML with the randomized exploration of RL, and by updating parameters such that probability is spread more evenly across consistent programs.",abstractText,[0],[0]
We apply our learning algorithm to a new neural semantic parser and show significant gains over existing state-of-theart results on a recent context-dependent semantic parsing task.,abstractText,[0],[0]
From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood,title,[0],[0]
"ar X
iv :1
50 6.
03 48
7v 2
[ cs
.C L
] 2
6 A
ug 2
01 5",text,[0],[0]
Paraphrase detection3 is the task of analyzing two segments of text and determining if they have the same meaning despite differences in structure and wording.,1 Introduction,[0],[0]
"It is useful for a variety of NLP tasks like question answering (Rinaldi et al., 2003; Fader et al., 2013), semantic parsing (Berant and Liang, 2014), textual entail-
1We release our datasets, code, and trained models on the authors’ websites.
",1 Introduction,[0],[0]
"2This version differs from the previous one with the inclusion of Appendix A, which contains details about new higher dimensional embeddings we have released.",1 Introduction,[0],[0]
"These embeddings achieve human-level performance on SL999 and WS353.
3See Androutsopoulos and Malakasiotis (2010) for a survey on approaches for detecting paraphrases.
ment (Bosma and Callison-Burch, 2007), and machine translation (Marton et al., 2009).
",1 Introduction,[0],[0]
"One component of many such systems is a paraphrase table containing pairs of text snippets, usually automatically generated, that have the same meaning.",1 Introduction,[0],[0]
"The most recent work in this area is the Paraphrase Database (PPDB; Ganitkevitch et al., 2013), a collection of confidence-rated paraphrases created using the pivoting technique of Bannard and Callison-Burch (2005) over large parallel corpora.",1 Introduction,[0],[0]
"The PPDB is a massive resource, containing 220 million paraphrase pairs.",1 Introduction,[0],[0]
It captures many short paraphrases that would be difficult to obtain using any other resource.,1 Introduction,[0],[0]
"For example, the pair {we must do our utmost, we must make every effort} has little lexical overlap but is present in PPDB.",1 Introduction,[0],[0]
"The PPDB has recently been used for monolingual alignment (Yao et al., 2013), for predicting sentence similarity (Bjerva et al., 2014), and to improve the coverage of FrameNet (Rastogi and Van Durme, 2014).
",1 Introduction,[0],[0]
"Though already effective for multiple NLP tasks, we note some drawbacks of PPDB.",1 Introduction,[0],[0]
"The first is lack of coverage: to use the PPDB to compare two phrases, both must be in the database.",1 Introduction,[0],[0]
The second is that PPDB is a nonparametric paraphrase model; the number of parameters (phrase pairs) grows with the size of the dataset used to build it.,1 Introduction,[0],[0]
"In practice, it can become unwieldy to work with as the size of the database increases.",1 Introduction,[0],[0]
"A third concern is that the confidence estimates in PPDB are a heuristic combination of features, and their quality is unclear.
",1 Introduction,[0],[0]
We address these issues in this work by introducing ways to use PPDB to construct parametric paraphrase models.,1 Introduction,[0],[0]
"First we show that initial skip-gram word vectors (Mikolov et al., 2013a) can be fine-tuned for the paraphrase task by training on word pairs from PPDB.",1 Introduction,[0],[0]
"We call them PARA-
GRAM word vectors.",1 Introduction,[0],[0]
We find additive composition of PARAGRAM vectors to be a simple but effective way to embed phrases for short-phrase paraphrase tasks.,1 Introduction,[0],[0]
"We find improved performance by training a recursive neural network (RNN; Socher et al., 2010) directly on phrase pairs from PPDB.
",1 Introduction,[0],[0]
"We show that our resulting word and phrase representations are effective on a wide variety of tasks, including two new datasets that we introduce.",1 Introduction,[0],[0]
"The first, Annotated-PPDB, contains pairs from PPDB that were scored by human annotators.",1 Introduction,[0],[0]
It can be used to evaluate paraphrase models for short phrases.,1 Introduction,[0],[0]
We use it to show that the phrase embeddings produced by our methods are significantly more indicative of paraphrasability than the original heuristic scoring used by Ganitkevitch et al. (2013).,1 Introduction,[0],[0]
"Thus we use the power of PPDB to improve its contents.
",1 Introduction,[0],[0]
"Our second dataset, ML-Paraphrase, is a reannotation of the bigram similarity corpus from Mitchell and Lapata (2010).",1 Introduction,[0],[0]
"The task was originally developed to measure semantic similarity of bigrams, but some annotations are not congruent with the functional similarity central to paraphrase relationships.",1 Introduction,[0],[0]
Our re-annotation can be used to assess paraphrasing capability of bigram compositional models.,1 Introduction,[0],[0]
"In summary, we make the following contributions:
Provide new PARAGRAM word vectors, learned using PPDB, that achieve state-of-the-art performance on the SimLex-999 lexical similarity task (Hill et al., 2014b) and lead to improved performance in sentiment analysis.
",1 Introduction,[0],[0]
Provide ways to use PPDB to embed phrases.,1 Introduction,[0],[0]
We compare additive and RNN composition of PARAGRAM vectors.,1 Introduction,[0],[0]
Both can improve PPDB by reranking the paraphrases in PPDB to improve correlations with human judgments.,1 Introduction,[0],[0]
"They can be used as concise parameterizations of PPDB, thereby vastly increasing its coverage.",1 Introduction,[0],[0]
"We also perform a qualitative analysis of the differences between additive and RNN composition.
",1 Introduction,[0],[0]
Introduce two new datasets.,1 Introduction,[0],[0]
The first contains PPDB phrase pairs and evaluates how well models can measure the quality of short paraphrases.,1 Introduction,[0],[0]
"The second is a new annotation of the bigram similarity task in Mitchell and Lapata (2010) that makes it suitable for evaluating bigram paraphrases.
",1 Introduction,[0],[0]
"We release the new datasets, complete with annotation instructions and raw annotations, as well as our code and the trained models.4",1 Introduction,[0],[0]
There is a vast literature on representing words as vectors.,2 Related Work,[0],[0]
"The intuition of most methods to create these vectors (or embeddings) is that similar words have similar contexts (Firth, 1957).",2 Related Work,[0],[0]
"Earlier models made use of latent semantic analysis (LSA) (Deerwester et al., 1990).",2 Related Work,[0],[0]
"Recently, more sophisticated neural models, work originating with (Bengio et al., 2003), have been gaining popularity (Mikolov et al., 2013a; Pennington et al., 2014).",2 Related Work,[0],[0]
"These embeddings are now being used in new ways as they are being tailored to specific downstream tasks (Bansal et al., 2014).
",2 Related Work,[0],[0]
Phrase representations can be created from word vectors using compositional models.,2 Related Work,[0],[0]
Simple but effective compositional models were studied by Mitchell and Lapata (2008; 2010) and Blacoe and Lapata (2012).,2 Related Work,[0],[0]
They compared a variety of binary operations on word vectors and found that simple point-wise multiplication of explicit vector representations performed very well.,2 Related Work,[0],[0]
"Other works like Zanzotto et al. (2010) and Baroni and Zamparelli (2010) also explored composition using models based on operations of vectors and matrices.
",2 Related Work,[0],[0]
"More recent work has shown that the extremely efficient neural embeddings of Mikolov et al. (2013a) also do well on compositional tasks simply by adding the word vectors (Mikolov et al., 2013b).",2 Related Work,[0],[0]
"Hashimoto et al. (2014) introduced an alternative word embedding and compositional model based on predicate-argument structures that does well on two simple composition tasks, including the one introduced by Mitchell and Lapata (2010).
",2 Related Work,[0],[0]
"An alternative approach to composition, used by Socher et al. (2011), is to train a recursive neural network (RNN) whose structure is defined by a binarized parse tree.",2 Related Work,[0],[0]
"In particular, they trained their RNN as an unsupervised autoencoder.",2 Related Work,[0],[0]
The RNN captures the latent structure of composition.,2 Related Work,[0],[0]
"Recent work has shown that this model struggles in tasks in-
4available on the authors’ websites
volving compositionality (Blacoe and Lapata, 2012; Hashimoto et al., 2014).5",2 Related Work,[0],[0]
"However, we found success using RNNs in a supervised setting, similar to Socher et al. (2014), who used RNNs to learn representations for image descriptions.",2 Related Work,[0],[0]
"The objective function we used in this work was motivated by their multimodal objective function for learning joint image-sentence representations.
",2 Related Work,[0],[0]
"Lastly, the PPDB has been used along with other resources to learn word embeddings for several tasks, including semantic similarity, language modeling, predicting human judgments, and classification (Yu and Dredze, 2014; Faruqui et al., 2015).",2 Related Work,[0],[0]
"Concurrently with our work, it has also been used to construct paraphrase models for short phrases (Yu and Dredze, 2015).",2 Related Work,[0],[0]
"We created two novel datasets: (1) AnnotatedPPDB, a subset of phrase pairs from PPDB which are annotated according to how strongly they represent a paraphrase relationship, and (2) MLParaphrase, a re-annotation of the bigram similarity dataset from Mitchell and Lapata (2010), again annotated for strength of paraphrase relationship.",3 New Paraphrase Datasets,[0],[0]
Our motivation for creating Annotated-PPDB was to establish a way to evaluate compositional paraphrase models on short phrases.,3.1 Annotated-PPDB,[0],[0]
"Most existing paraphrase tasks focus on words, like SimLex-999 (Hill et al., 2014b), or entire sentences, such as the Microsoft Research Paraphrase Corpus (Dolan et al., 2004; Quirk et al., 2004).",3.1 Annotated-PPDB,[0],[0]
"To our knowledge, there are no datasets that focus on the paraphrasability of short phrases.",3.1 Annotated-PPDB,[0],[0]
"Thus, we created Annotated-PPDB so that researchers can focus on local compositional phenomena and measure the performance of models directly—avoiding the need to do so indirectly in a sentence-level task.",3.1 Annotated-PPDB,[0],[0]
"Models that have strong performance on Annotated-PPDB can be used to provide more accurate confidence scores for the paraphrases in the PPDB as well as reduce the need for large paraphrase tables altogether.
",3.1 Annotated-PPDB,[0],[0]
"5We also replicated this approach and found training to be time-consuming even using low-dimensional word vectors.
",3.1 Annotated-PPDB,[0],[0]
Annotated-PPDB was created in a multi-step process (outlined below) involving various automatic filtering steps followed by crowdsourced human annotation.,3.1 Annotated-PPDB,[0],[0]
One of the aims for our dataset was to collect a variety of paraphrase types—we wanted to include pairs that were non-trivial to recognize as well as those with a range of similarity and length.,3.1 Annotated-PPDB,[0],[0]
"We focused on phrase pairs with limited lexical overlap to avoid including those with only trivial differences.
",3.1 Annotated-PPDB,[0],[0]
We started with candidate phrases extracted from the first 10M pairs in the XXL version of the PPDB and then executed the following steps.6 Filter phrases for quality: Only those phrases whose tokens were in our vocabulary were retained.7,3.1 Annotated-PPDB,[0],[0]
"Next, all duplicate paraphrase pairs were removed; in PPDB, these are distinct pairs that contain the same two phrases with the order swapped.",3.1 Annotated-PPDB,[0],[0]
"Filter by lexical overlap: Next, we calculated the word overlap score in each phrase pair and then retained only those pairs that had a score of less than 0.5.",3.1 Annotated-PPDB,[0],[0]
"By word overlap score, we mean the fraction of tokens in the smaller of the phrases with Levenshtein distance ≤ 1 to a token in the larger of the phrases.",3.1 Annotated-PPDB,[0],[0]
"This was done to exclude less interesting phrase pairs like 〈my dad had, my father had〉 or 〈ballistic missiles, of ballistic missiles〉 that only differ in a synonym or the addition of a single word.",3.1 Annotated-PPDB,[0],[0]
"Select range of paraphrasabilities: To balance our dataset with both clear paraphrases and erroneous pairs in PPDB, we sampled 5,000 examples from ten chunks of the first 10M initial phrase pairs where a chunk is defined as 1M phrase pairs.",3.1 Annotated-PPDB,[0],[0]
"Select range of phrase lengths: We then selected 1,500 phrases from each 5000-example sample that encompassed a wide range of phrase lengths.",3.1 Annotated-PPDB,[0],[0]
"To do this, we first binned the phrase pairs by their effective size.",3.1 Annotated-PPDB,[0],[0]
Let n1 be the number of tokens of length greater than one character in the first phrase and n2 the same for the second phrase.,3.1 Annotated-PPDB,[0],[0]
"Then the effective size is defined as max(n1, n2).",3.1 Annotated-PPDB,[0],[0]
"The bins contained pairs of effective size of 3, 4, and 5 or more, and 500
6Note that the confidence scores for phrase pairs in PPDB are based on a weighted combination of features with weights determined heuristically.",3.1 Annotated-PPDB,[0],[0]
"The confidence scores were used to place the phrase pairs into their respective sets (S, M, L, XL, XXL, etc.), where each larger set subsumes all smaller ones.
",3.1 Annotated-PPDB,[0],[0]
"7Throughout, our vocabulary is defined as the most common 100K word types in English Wikipedia, following tokenization and lowercasing (see §5).
",3.1 Annotated-PPDB,[0],[0]
pairs were selected from each bin.,3.1 Annotated-PPDB,[0],[0]
"This gave us a total of 15,000 phrase pairs.
",3.1 Annotated-PPDB,[0],[0]
"Prune to 3,000: 3,000 phrase pairs were then selected randomly from the 15,000 remaining pairs to form an initial dataset, Annotated-PPDB-3K.",3.1 Annotated-PPDB,[0],[0]
"The phrases were selected so that every phrase in the dataset was unique.
",3.1 Annotated-PPDB,[0],[0]
Annotate with Mechanical Turk:,3.1 Annotated-PPDB,[0],[0]
"The dataset was then rated on a scale from 1-5 using Amazon Mechanical Turk, where a score of 5 denoted phrases that are equivalent in a large number of contexts, 3 meant that the phrases had some overlap in meaning, and 1 indicated that the phrases were dissimilar or contradictory in some way (e.g., can not adopt and is able to accept).
",3.1 Annotated-PPDB,[0],[0]
"We only permitted workers whose location was in the United States and who had done at least 1,000 HITS with a 99% acceptance rate.",3.1 Annotated-PPDB,[0],[0]
Each example was labeled by 5 annotators and their scores were averaged to produce the final rating.,3.1 Annotated-PPDB,[0],[0]
Table 1 shows some statistics of the data.,3.1 Annotated-PPDB,[0],[0]
"Overall, the annotated data had a mean deviation (MD)8 of 0.80.",3.1 Annotated-PPDB,[0],[0]
"Table 1 shows that overall, workers found the phrases to be of high quality, as more than two-thirds of the pairs had an average score of at least 3.",3.1 Annotated-PPDB,[0],[0]
"Also from the Table, we can see that workers had stronger agreement on very low and very high quality pairs and were less certain in the middle of the range.
",3.1 Annotated-PPDB,[0],[0]
"Prune to 1,260: To create our final dataset, Annotated-PPDB, we selected 1,260 phrase pairs from the 3,000 annotations.",3.1 Annotated-PPDB,[0],[0]
We did this by first binning the phrases into 3 categories: those with scores in the interval,3.1 Annotated-PPDB,[0],[0]
"[1, 2.5), those with scores in the interval [2.5, 3.5], and those with scores in the interval (3.5, 5].",3.1 Annotated-PPDB,[0],[0]
"We took the 420 phrase pairs with the lowest MD in each bin, as these have the most agreement about their label, to form Annotated-PPDB.
",3.1 Annotated-PPDB,[0],[0]
"These 1,260 examples were then randomly split into a development set of 260 examples and a test set of 1,000 examples.",3.1 Annotated-PPDB,[0],[0]
"The development set had an MD of 0.61 and the test set had an MD of 0.60, indicating the final dataset had pairs of higher agreement than the initial 3,000.
8MD is similar to standard deviation, but uses absolute value instead of squared value and thus is both more intuitive and less sensitive to outliers.",3.1 Annotated-PPDB,[0],[0]
"Our second newly-annotated dataset, ML-Paraphrase, is based on the bigram similarity task originally introduced by Mitchell and Lapata (2010); we refer to the original annotations as the ML dataset.
",3.2 ML-Paraphrase,[0],[0]
"The ML dataset consists of human similarity ratings for three types of bigrams: adjective-noun (JN), noun-noun (NN), and verb-noun (VN).",3.2 ML-Paraphrase,[0],[0]
"Through manual inspection, we found that the annotations were not consistent with the notion of similarity central to paraphrase tasks.",3.2 ML-Paraphrase,[0],[0]
"For instance, television set and television programme were the highest rated phrases in the NN section (based on average annotator score).",3.2 ML-Paraphrase,[0],[0]
"Similarly, one of the highest ranked JN pairs was older man and elderly woman.",3.2 ML-Paraphrase,[0],[0]
"This indicates that the annotations reflect topical similarity in addition to capturing functional or definitional similarity.
",3.2 ML-Paraphrase,[0],[0]
"Therefore, we had the data re-annotated by two authors of this paper who are native English speakers.9",3.2 ML-Paraphrase,[0],[0]
"The bigrams were labeled on a scale from 1- 5 where 5 denotes phrases that are equivalent in a large number of contexts, 3 indicates the phrases are roughly equivalent in a narrow set of contexts, and 1 means the phrases are not at all equivalent in any context.",3.2 ML-Paraphrase,[0],[0]
"Following annotation, we collapsed the rating scale by merging 4s and 5s together and 1s and 2s together.
",3.2 ML-Paraphrase,[0],[0]
Statistics for the data are shown in Table 2.,3.2 ML-Paraphrase,[0],[0]
"We show inter-annotator Spearman ρ and Cohen’s κ in columns 2 and 3, indicating substantial agreement on the JN and VN portions but only moderate agreement on NN.",3.2 ML-Paraphrase,[0],[0]
"In fact, when evaluating our NN anno-
9We tried using Mechanical Turk here, but due to such short phrases, with few having the paraphrase relationship, workers did not perform well on the task.
tations against those from the original ML data (column 4), we find ρ to be 0.38, well below the average human correlation of 0.49 (final column) reported by Mitchell and Lapata and also surpassed by pointwise multiplication (Mitchell and Lapata, 2010).",3.2 ML-Paraphrase,[0],[0]
"This suggests that the original NN portion, more so than the others, favored a notion of similarity more related to association than paraphrase.",3.2 ML-Paraphrase,[0],[0]
We now present parametric paraphrase models and discuss training.,4 Paraphrase Models,[0],[0]
"Our goal is to embed phrases into a low-dimensional space such that cosine similarity in the space corresponds to the strength of the paraphrase relationship between phrases.
",4 Paraphrase Models,[0],[0]
We use a recursive neural network (RNN) similar to that used by Socher et al. (2014).,4 Paraphrase Models,[0],[0]
We first use a constituent parser to obtain a binarized parse of a phrase.,4 Paraphrase Models,[0],[0]
"For phrase p, we compute its vector g(p) through recursive computation on the parse.",4 Paraphrase Models,[0],[0]
"That is, if phrase p is the yield of a parent node in a parse tree, and phrases c1 and c2 are the yields of its two child nodes, we define g(p) recursively as follows:
g(p) = f(W [g(c1); g(c2)]",4 Paraphrase Models,[0],[0]
"+ b)
where f is an element-wise activation function (tanh), [g(c1); g(c2)] ∈ R2n is the concatenation of the child vectors, W ∈ Rn×2n is the composition matrix, b ∈",4 Paraphrase Models,[0],[0]
"Rn is the offset, and n is the dimensionality of the word embeddings.",4 Paraphrase Models,[0],[0]
"If node p has no children (i.e., it is a single token), we define g(p) = W (p) w , where Ww is the word embedding matrix in which particular word vectors are indexed using superscripts.",4 Paraphrase Models,[0],[0]
"The trainable parameters of the model are W , b, and Ww.",4 Paraphrase Models,[0],[0]
We now present objective functions for training on pairs extracted from PPDB.,4.1 Objective Functions,[0],[0]
The training data consists of (possibly noisy) pairs taken directly from the original PPDB.,4.1 Objective Functions,[0],[0]
"In subsequent sections, we discuss how we extract training pairs for particular tasks.
",4.1 Objective Functions,[0],[0]
"We assume our training data consists of a set X of phrase pairs 〈x1, x2〉, where x1 and x2 are assumed to be paraphrases.",4.1 Objective Functions,[0],[0]
"To learn the model parameters (W, b,Ww), we minimize our objective function over the data using AdaGrad (Duchi et al., 2011) with mini-batches.",4.1 Objective Functions,[0],[0]
"The objective function follows:
min W,b,Ww
1
|X|
(
∑
〈x1,x2〉∈X
max(0, δ − g(x1) · g(x2) + g(x1) · g(t1))
+ max(0, δ − g(x1) · g(x2) + g(x2) · g(t2))
)
+ λW (‖W‖ 2 + ‖b‖2) +",4.1 Objective Functions,[0],[0]
"λWw ‖Wwinitial −Ww‖ 2
(1)
where λW and λWw are regularization parameters, Wwinitial is the initial word embedding matrix, δ is the margin (set to 1 in all of our experiments), and t1 and t2 are carefully-selected negative examples taken from a mini-batch during optimization.
",4.1 Objective Functions,[0],[0]
"The intuition for this objective is that we want the two phrases to be more similar to each other (g(x1) · g(x2)) than either is to their respective negative examples t1 and t2, by a margin of at least δ.
",4.1 Objective Functions,[0],[0]
"Selecting Negative Examples To select t1 and t2 in Eq. 1, we simply chose the most similar phrase in the mini-batch (other than those in the given phrase pair).",4.1 Objective Functions,[0],[0]
"E.g., for choosing t1 for a given 〈x1, x2〉:
t1 = argmax t:〈t,·〉∈Xb\{〈x1,x2〉} g(x1) · g(t)
where Xb ⊆ X is the current mini-batch.",4.1 Objective Functions,[0],[0]
"That is, we want to choose a negative example ti that is similar to xi according to the current model parameters.",4.1 Objective Functions,[0],[0]
The downside of this approach is that we may occasionally choose a phrase ti that is actually a true paraphrase of xi.,4.1 Objective Functions,[0],[0]
"We also tried a strategy in which we selected the least similar phrase that would trigger an update (i.e., g(ti) ·g(xi) > g(x1) ·g(x2)−δ), but we found the simpler strategy above to work better and used it for all experiments reported below.
",4.1 Objective Functions,[0],[0]
"Discussion The objective in Eq. 1 is similar to one used by Socher et al. (2014), but with several differences.",4.1 Objective Functions,[0],[0]
Their objective compared text and projected images.,4.1 Objective Functions,[0],[0]
"They also did not update the underlying word embeddings; we do so here, and in a way such that they are penalized from deviating from their initialization.",4.1 Objective Functions,[0],[0]
"Also for a given 〈x1, x2〉, they do not select a single t1 and t2 as we do, but use the entire training set, which can be very expensive with a large training dataset.
",4.1 Objective Functions,[0],[0]
"We also experimented with a simpler objective that sought to directly minimize the squared L2norm between g(x1) and g(x2) in each pair, along with the same regularization terms as in Eq. 1.",4.1 Objective Functions,[0],[0]
One problem with this objective function is that the global minimum is 0 and is achieved simply by driving the parameters to 0.,4.1 Objective Functions,[0],[0]
"We obtained much better results using the objective in Eq. 1.
",4.1 Objective Functions,[0],[0]
"Training Word Paraphrase Models To train just word vectors on word paraphrase pairs (again from PPDB), we used the same objective function as above, but simply dropped the composition terms.",4.1 Objective Functions,[0],[0]
"This gave us an objective that bears some similarity to the skip-gram objective with negative sampling in word2vec (Mikolov et al., 2013a).",4.1 Objective Functions,[0],[0]
Both seek to maximize the dot products of certain word pairs while minimizing the dot products of others.,4.1 Objective Functions,[0],[0]
"This objective function is:
min Ww
1
|X|
(
∑
〈x1,x2〉∈X
max(0, δ −W",4.1 Objective Functions,[0],[0]
(x1)w ·W (x2),4.1 Objective Functions,[0],[0]
"w
+W (x1)w ·W (t1) w )",4.1 Objective Functions,[0],[0]
"+ max(0, δ −W (x1) w ·W (x2)",4.1 Objective Functions,[0],[0]
w,4.1 Objective Functions,[0],[0]
"+
W (x2)w ·W (t2) w )
)
",4.1 Objective Functions,[0],[0]
"+ λWw ‖Wwinitial −Ww‖ 2 (2)
",4.1 Objective Functions,[0],[0]
"It is like Eq. 1 except with word vectors replacing the RNN composition function and with the regularization terms on the W and b removed.
",4.1 Objective Functions,[0],[0]
We further found we could improve this model by incorporating constraints.,4.1 Objective Functions,[0],[0]
"From our training pairs, for a given word w, we assembled all other words that were paired with it in PPDB and all of their lemmas.",4.1 Objective Functions,[0],[0]
These were then used as constraints during the pairing process: a word t could only be paired with w if it was not in its list of assembled words.,4.1 Objective Functions,[0],[0]
We first present experiments on learning lexical paraphrasability.,5 Experiments – Word Paraphrasing,[0],[0]
"We train on word pairs from PPDB and evaluate on the SimLex-999 dataset (Hill et al., 2014b), achieving the best results reported to date.",5 Experiments – Word Paraphrasing,[0],[0]
"To learn word vectors that reflect paraphrasability, we optimized Eq. 2.",5.1 Training Procedure,[0],[0]
"There are many tunable hyperparameters with this objective, so to make training tractable we fixed the initial learning rates for the word embeddings to 0.5 and the margin δ to 1.",5.1 Training Procedure,[0],[0]
Then we did a coarse grid search over a parameter space for λWw and the mini-batch size.,5.1 Training Procedure,[0],[0]
"We considered λWw values in {10
−2, 10−3, ..., 10−7, 0} and minibatch sizes in {100, 250, 500, 1000}.",5.1 Training Procedure,[0],[0]
"We trained for 20 epochs for each set of hyperparameters using AdaGrad (Duchi et al., 2011).
",5.1 Training Procedure,[0],[0]
"For all experiments, we initialized our word vectors with skip-gram vectors trained using word2vec (Mikolov et al., 2013a).",5.1 Training Procedure,[0],[0]
"The vectors were trained on English Wikipedia (tokenized and lowercased, yielding 1.8B tokens).10",5.1 Training Procedure,[0],[0]
"We used a window size of 5 and a minimum count cut-off of 60, producing vectors for approximately 270K word types.",5.1 Training Procedure,[0],[0]
"We retained vectors for only the 100K most frequent words, averaging the rest to obtain a single vector for unknown words.",5.1 Training Procedure,[0],[0]
We will refer to this set of the 100K most frequent words as our vocabulary.,5.1 Training Procedure,[0],[0]
"For training, we extracted word pairs from the lexical XL section of PPDB.",5.2 Extracting Training Data,[0],[0]
"We used the XL data for all experiments, including those for phrases.",5.2 Extracting Training Data,[0],[0]
We used XL instead of XXL because XL has better quality overall while still being large enough so that we could be selective in choosing training pairs.,5.2 Extracting Training Data,[0],[0]
"There are a total of 548,085 pairs.",5.2 Extracting Training Data,[0],[0]
"We removed 174,766 that either contained numerical digits or words not in our vocabulary.",5.2 Extracting Training Data,[0],[0]
"We then removed 260,425 redundant pairs, leaving us with a final training set of 112,894 word pairs.
",5.2 Extracting Training Data,[0],[0]
"10We used the December 2, 2013 snapshot.",5.2 Extracting Training Data,[0],[0]
"Hyperparameters were tuned using the wordsim-353 (WS353) dataset (Finkelstein et al., 2001), specifically its similarity (WS-S) and relatedness (WSR) partitions (Agirre et al., 2009).",5.3 Tuning and Evaluation,[0],[0]
"In particular, we tuned to maximize 2×WS-S correlation minus the WS-R correlation.",5.3 Tuning and Evaluation,[0],[0]
"The idea was to reward vectors with high similarity and relatively low relatedness, in order to target the paraphrase relationship.
",5.3 Tuning and Evaluation,[0],[0]
"After tuning, we evaluated the best hyperparameters on the SimLex-999 (SL999) dataset (Hill et al., 2014b).",5.3 Tuning and Evaluation,[0],[0]
We chose SL999 as our primary test set as it most closely evaluates the paraphrase relationship.,5.3 Tuning and Evaluation,[0],[0]
"Even though WS-S is a close approximation to this relationship, it does not include pairs that are merely associated and assigned low scores, which SL999 does (see discussion in Hill et al., 2014b).
",5.3 Tuning and Evaluation,[0],[0]
"Note that for all experiments we used cosine similarity as our similarity metric and evaluated the statistical significance of dependent correlations using the one-tailed method of (Steiger, 1980).",5.3 Tuning and Evaluation,[0],[0]
"Table 3 shows results on SL999 when improving the initial word vectors by training on word pairs from PPDB, both with and without constraints.",5.4 Results,[0],[0]
"The “PARAGRAM WS” rows show results when tuning to maximize 2×WS-S − WS-R. We also show results for strong skip-gram baselines and the best results from the literature, including the state-of-the-art results from Hill et al. (2014a) as well as the inter-
annotator agreement from Hill et al.",5.4 Results,[0],[0]
"(2014b).11
The table illustrates that, by training on PPDB, we can surpass the previous best correlations on SL999 by 4-6% absolute, achieving the best results reported to date.",5.4 Results,[0],[0]
We also find that we can train low-dimensional word vectors that exceed the performance of much larger vectors.,5.4 Results,[0],[0]
"This is very useful as using large vectors can increase both time and memory consumption in NLP applications.
",5.4 Results,[0],[0]
"To generate word vectors to use for downstream applications, we chose hyperparameters so as to maximize performance on SL999.12 These word vectors, which we refer to as PARAGRAM vectors, had a ρ of 0.57 on SL999.",5.4 Results,[0],[0]
We use them as initial word vectors for the remainder of the paper.,5.4 Results,[0],[0]
"As an extrinsic evaluation of our PARAGRAM word vectors, we used them in a convolutional neural network (CNN) for sentiment analysis.",5.5 Sentiment Analysis,[0],[0]
We used the simple CNN from Kim (2014) and the binary sentence-level sentiment analysis task from Socher et al. (2013).,5.5 Sentiment Analysis,[0],[0]
"We used the standard data splits, removing examples with a neutral rating.",5.5 Sentiment Analysis,[0],[0]
"We trained on all constituents in the training set while only using full sentences from development and test, giving us train/development/test sizes of 67,349/872/1,821.
",5.5 Sentiment Analysis,[0],[0]
"The CNN uses m-gram filters, each of which is an m×n vector.",5.5 Sentiment Analysis,[0],[0]
"The CNN computes the inner product between an m-gram filter and each m-gram in an example, retaining the maximum match (so-called “max-pooling”).",5.5 Sentiment Analysis,[0],[0]
"The score of the match is a single dimension in a feature vector for the example, which is then associated with a weight in a linear classifier used to predict positive or negative sentiment.
",5.5 Sentiment Analysis,[0],[0]
"While Kim (2014) used m-gram filters of several lengths, we only used unigram filters.",5.5 Sentiment Analysis,[0],[0]
We also fixed the word vectors during learning (called “static” by Kim).,5.5 Sentiment Analysis,[0],[0]
"After learning, the unigram filters correspond to locations in the fixed word vector space.",5.5 Sentiment Analysis,[0],[0]
The learned classifier weights represent how strongly each location corresponds to positive or negative sentiment.,5.5 Sentiment Analysis,[0],[0]
"We expect this static CNN to
11Hill et al. (2014a) did not report the dimensionality of the vectors that led to their state-of-the-art results.
",5.5 Sentiment Analysis,[0],[0]
"12We did not use constraints during training.
be more effective if the word vector space separates positive and negative sentiment.
",5.5 Sentiment Analysis,[0],[0]
"In our experiments, we compared baseline skipgram embeddings to our PARAGRAM vectors.",5.5 Sentiment Analysis,[0],[0]
"We used AdaGrad learning rate of 0.1, mini-batches of size 10, and a dropout rate of 0.5.",5.5 Sentiment Analysis,[0],[0]
We used 200 unigram filters and rectified linear units as the activation (applied to the filter output + filter bias).,5.5 Sentiment Analysis,[0],[0]
"We trained for 30 epochs, predicting labels on the development set after each set of 3,000 examples.",5.5 Sentiment Analysis,[0],[0]
"We recorded the highest development accuracy and used those parameters to predict labels on the test set.
",5.5 Sentiment Analysis,[0],[0]
Results are shown in Table 4.,5.5 Sentiment Analysis,[0],[0]
"We see improvements over the baselines when using PARAGRAM vectors, even exceeding the performance of higherdimensional skip-gram vectors.",5.5 Sentiment Analysis,[0],[0]
"In this section, we describe experiments on a variety of compositional phrase-based paraphrasing tasks.",6 Experiments – Compositional Paraphrasing,[0],[0]
"We start with the simplest case of bigrams, and then proceed to short phrases.",6 Experiments – Compositional Paraphrasing,[0],[0]
"For all tasks, we again train on appropriate data from PPDB and test on various evaluation datasets, including our two novel datasets (Annotated-PPDB and ML-Paraphrase).",6 Experiments – Compositional Paraphrasing,[0],[0]
"We trained our models by optimizing Eq. 1 using AdaGrad (Duchi et al., 2011).",6.1 Training Procedure,[0],[0]
"We fixed the initial learning rates to 0.5 for the word embeddings and 0.05 for the composition parameters, and the margin to 1.",6.1 Training Procedure,[0],[0]
"Then we did a coarse grid search over a parameter space for λWw , λW , and mini-batch size.
",6.1 Training Procedure,[0],[0]
"For λWw , our search space again consisted of {10−2, 10−3, ..., 10−7, 0}, for λW it was {10−1, 10−2, 10−3, 0}, and we explored batch sizes of {100, 250, 500, 1000, 2000}.",6.1 Training Procedure,[0],[0]
"When initializing with PARAGRAM vectors, the search space for λWw was shifted upwards to be
{10, 1, 10−1 , 10−3, ..., 10−6} to reflect our increased confidence in the initial vectors.",6.1 Training Procedure,[0],[0]
We trained only for 5 epochs for each set of parameters.,6.1 Training Procedure,[0],[0]
"For baselines, we used the same initial skip-gram vectors as in Section 5.",6.1 Training Procedure,[0],[0]
"For all experiments, we again used cosine similarity as our similarity metric and evaluated the statistical significance using the method of (Steiger, 1980).
",6.2 Evaluation and Baselines,[0],[0]
A baseline used in all compositional experiments is vector addition of skip-gram (or PARAGRAM) word vectors.,6.2 Evaluation and Baselines,[0],[0]
"Unlike explicit word vectors, where point-wise multiplication acts as a conjunction of features and performs well on composition tasks (Mitchell and Lapata, 2008), using addition with skip-gram vectors (Mikolov et al., 2013b) gives better performance than multiplication.",6.2 Evaluation and Baselines,[0],[0]
"To evaluate our ability to paraphrase bigrams, we consider the original bigram similarity task from Mitchell and Lapata (2010) as well as our newlyannotated version of it: ML-Paraphrase.
",6.3 Bigram Paraphrasability,[0],[0]
Extracting Training Data Training data for these tasks was extracted from the XL portion of PPDB.,6.3 Bigram Paraphrasability,[0],[0]
"The bigram similarity task from Mitchell and Lapata (2010) contains three types of bigrams: adjective-noun (JN), noun-noun (NN), and verb-noun (VN).",6.3 Bigram Paraphrasability,[0],[0]
"We aimed to collect pairs from PPDB that mirrored these three types of bigrams.
",6.3 Bigram Paraphrasability,[0],[0]
"We found parsing to be unreliable on such short segments of text, so we used a POS tagger (Manning et al., 2014) to tag the tokens in each phrase.",6.3 Bigram Paraphrasability,[0],[0]
We then used the word alignments in PPDB to extract bigrams for training.,6.3 Bigram Paraphrasability,[0],[0]
"For JN and NN, we extracted pairs containing aligned, adjacent tokens in the two phrases with the appropriate partof-speech tag.",6.3 Bigram Paraphrasability,[0],[0]
"Thus we extracted pairs like 〈easy job, simple task〉 for the JN section and 〈town meeting, town council〉 for the NN section.",6.3 Bigram Paraphrasability,[0],[0]
We used a different strategy for extracting training data for the VN subset: we took aligned VN tokens and took the closest noun after the verb.,6.3 Bigram Paraphrasability,[0],[0]
This was done to approximate the direct object that would have been ideally extracted with a dependency parse.,6.3 Bigram Paraphrasability,[0],[0]
"An example from this section is 〈achieve goal, achieve aim〉.
",6.3 Bigram Paraphrasability,[0],[0]
"We removed phrase pairs that (1) contained words not in our vocabulary, (2) were redundant with others, (3) contained brackets, or (4) had Levenshtein distance ≤ 1.",6.3 Bigram Paraphrasability,[0],[0]
The final criterion helps to ensure that we train on phrase pairs with non-trivial differences.,6.3 Bigram Paraphrasability,[0],[0]
"The final training data consisted of 133,997 JN pairs, 62,640 VN pairs and 35,601 NN pairs.
",6.3 Bigram Paraphrasability,[0],[0]
Baselines,6.3 Bigram Paraphrasability,[0],[0]
"In addition to RNN models, we report baselines that use vector addition as the composition function, both with our skip-gram embeddings and PARAGRAM embeddings from Section 5.
",6.3 Bigram Paraphrasability,[0],[0]
We also compare to several results from prior work.,6.3 Bigram Paraphrasability,[0],[0]
"When doing so, we took their best correlations for each data subset.",6.3 Bigram Paraphrasability,[0],[0]
"That is, the JN and NN results from Mitchell and Lapata (2010) use their multiplicative model and the VN results use their dilation model.",6.3 Bigram Paraphrasability,[0],[0]
From Hashimoto et al. (2014) we used their PAS-CLBLM Addl and PAS-CLBLM Addnl models.,6.3 Bigram Paraphrasability,[0],[0]
"We note that their vector dimensionalities are larger than ours, using n = 2000 and 50 respectively.
",6.3 Bigram Paraphrasability,[0],[0]
Results Results are shown in Table 5.,6.3 Bigram Paraphrasability,[0],[0]
We report results on the test portion of the original Mitchell and Lapata (2010) dataset (ML) as well as the entirety of our newly-annotated dataset (MLParaphrase).,6.3 Bigram Paraphrasability,[0],[0]
"RNN results on ML were tuned on the respective development sections and RNN results on ML-Paraphrase were tuned on the entire ML dataset.
",6.3 Bigram Paraphrasability,[0],[0]
Our RNN model outperforms results from the literature on most sections in both datasets and its average correlations are among the highest.13,6.3 Bigram Paraphrasability,[0],[0]
"The one
13The results obtained here differ from those reported in Hashimoto et al. (2014) as we scored their vectors with a newer Python implementation of Spearman ρ that handles ties (Hashimoto, P.C.).
subset of the data that posed difficulty was the NN section of the ML dataset.",6.3 Bigram Paraphrasability,[0],[0]
"We suspect this is due to the reasons discussed in Section 3.2; for our MLParaphrase dataset, by contrast, we do see gains on the NN section.
",6.3 Bigram Paraphrasability,[0],[0]
"We also outperform the strong baseline of adding 1000-dimensional skip-gram embeddings, a model with 40 times the number of parameters, on our MLParaphrase dataset.",6.3 Bigram Paraphrasability,[0],[0]
"This baseline had correlations of 0.45, 0.43, and 0.47 on the JN, NN, and VN partitions, with an average of 0.45—below the average ρ of the RNN (0.52) and even the {PARAGRAM, +} model (0.46).
",6.3 Bigram Paraphrasability,[0],[0]
"Interestingly, the type of vectors used to initialize the RNN has a significant effect on performance.",6.3 Bigram Paraphrasability,[0],[0]
"If we initialize using the 25-dimensional skip-gram vectors, the average ρ on ML-Paraphrase drops to 0.43, below even the {PARAGRAM, +} model.",6.3 Bigram Paraphrasability,[0],[0]
"In this section we show that by training a model based on filtered phrase pairs in PPDB, we can actually distinguish between quality paraphrases and poor paraphrases in PPDB better than the original heuristic scoring scheme from Ganitkevitch et al. (2013).
",6.4 Phrase Paraphrasability,[0],[0]
"Extracting Training Data As before, training data was extracted from the XL section of PPDB.",6.4 Phrase Paraphrasability,[0],[0]
"Similar to the procedure to create our AnnotatedPPDB dataset, phrases were filtered such that only those with a word overlap score of less than 0.5 were kept.",6.4 Phrase Paraphrasability,[0],[0]
We also removed redundant phrases and phrases that contained tokens not in our vocabulary.,6.4 Phrase Paraphrasability,[0],[0]
"The phrases were then binned according to their effective size and 20,000 examples were selected from
bins of effective sizes of 3, 4, and more than 5, creating a training set of 60,000 examples.",6.4 Phrase Paraphrasability,[0],[0]
"Care was taken to ensure that none of our training pairs was also present in our development and test sets.
",6.4 Phrase Paraphrasability,[0],[0]
Baselines We compare our models with strong lexical baselines.,6.4 Phrase Paraphrasability,[0],[0]
"The first, strict word overlap, is the percentage of words in the smaller phrase that are also in the larger phrase.",6.4 Phrase Paraphrasability,[0],[0]
"We also include a version where the words are lemmatized prior to the calculation.
",6.4 Phrase Paraphrasability,[0],[0]
"We also train a support vector regression model (epsilon-SVR) (Chang and Lin, 2011) on the 33 features that are included for each phrase pair in PPDB.",6.4 Phrase Paraphrasability,[0],[0]
We scaled the features such that each lies in the interval,6.4 Phrase Paraphrasability,[0],[0]
"[−1, 1] and tuned the parameters using 5-fold cross validation on our dev set.14",6.4 Phrase Paraphrasability,[0],[0]
"We then trained on the entire dev set after finding the best performing C and ǫ combination and evaluated on the test set of Annotated-PPDB.
",6.4 Phrase Paraphrasability,[0],[0]
Results We evaluated on our Annotated-PPDB dataset described in §3.1.,6.4 Phrase Paraphrasability,[0],[0]
Table 6 shows the Spearman correlations on the 1000-example test set.,6.4 Phrase Paraphrasability,[0],[0]
RNN models were tuned on the development set of 260 examples.,6.4 Phrase Paraphrasability,[0],[0]
"All other methods had no hyperparameters and therefore required no tuning.
",6.4 Phrase Paraphrasability,[0],[0]
"We note that the confidence estimates from Ganitkevitch et al. (2013) reach a ρ of 0.25 on the test set, similar to the results of strict overlap.",6.4 Phrase Paraphrasability,[0],[0]
"While 25-dimensional skip-gram embeddings only reach 0.20, we can improve this to 0.32 by fine-tuning them using PPDB (thereby obtaining our PARA-
14We tuned both parameters over {2−10, 2−9, ..., 210}.
GRAM vectors).",6.4 Phrase Paraphrasability,[0],[0]
"By using the PARAGRAM vectors to initialize the RNN, we reach a correlation of 0.40, which is better than the PPDB confidence estimates by 15% absolute.
",6.4 Phrase Paraphrasability,[0],[0]
"We again consider addition of 1000-dimensional skip-gram embeddings as a baseline, and they continue to perform strongly (ρ = 0.37).",6.4 Phrase Paraphrasability,[0],[0]
"The RNN initialized with PARAGRAM vectors does reach a higher ρ (0.40), but the difference is not statistically significant (p = 0.16).",6.4 Phrase Paraphrasability,[0],[0]
"Thus we can achieve similarlystrong results with far fewer parameters.
",6.4 Phrase Paraphrasability,[0],[0]
This task also illustrates the importance of initializing our RNN model with appropriate word embeddings.,6.4 Phrase Paraphrasability,[0],[0]
"An RNN initialized with skip-gram vectors has a modest ρ of 0.22, well below the ρ of the RNN initialized with PARAGRAM vectors.",6.4 Phrase Paraphrasability,[0],[0]
"Clearly, initialization is important when optimizing non-convex objectives like ours, but it is noteworthy that our best results came from first improving the word vectors and then learning the composition model, rather than jointly learning both from scratch.",6.4 Phrase Paraphrasability,[0],[0]
We performed a qualitative analysis to uncover sources of error and determine differences between adding PARAGRAM vectors and using an RNN initialized with them.,7 Qualitative Analysis,[0],[0]
"To do so, we took the output of both systems on Annotated-PPDB and mapped their cosine similarities to the interval",7 Qualitative Analysis,[0],[0]
"[1, 5].",7 Qualitative Analysis,[0],[0]
"We then computed their absolute error as compared to the gold ratings.
",7 Qualitative Analysis,[0],[0]
Table 7 shows how the average of these absolute errors changes with the magnitude of the gold ratings.,7 Qualitative Analysis,[0],[0]
The RNN performs better (has lower average absolute error) for less similar pairs.,7 Qualitative Analysis,[0],[0]
Vector addition only does better on the most similar pairs.,7 Qualitative Analysis,[0],[0]
"This is presumably because the most positive pairs have high word overlap and so can be represented effectively with a simpler model.
",7 Qualitative Analysis,[0],[0]
"To further investigate the differences between these models, we removed those pairs with gold scores in [2, 4], in order to focus on pairs with extreme scores.",7 Qualitative Analysis,[0],[0]
We identified two factors that distinguished the performance between the two models: length ratio and the amount of lexical overlap.,7 Qualitative Analysis,[0],[0]
"We did not find evidence that non-compositional phrases, such as idioms, were a source of error as these were not found in ML-Paraphrase and only appear rarely in Annotated-PPDB.
",7 Qualitative Analysis,[0],[0]
We define length ratio as simply the number of tokens in the smaller phrase divided by the number of tokens in the larger phrase.,7 Qualitative Analysis,[0],[0]
Overlap ratio is the number of equivalent tokens in the phrase pair divided by the number of tokens in the smaller of the two phrases.,7 Qualitative Analysis,[0],[0]
"Equivalent tokens are defined as tokens that are either exact matches or are paired up in the lexical portion of PPDB used to train the PARAGRAM vectors.
",7 Qualitative Analysis,[0],[0]
Table 9 shows how the performance of the models changes under different values of length ratio and overlap ratio.15,7 Qualitative Analysis,[0],[0]
The values in this table are the percentage changes in absolute error when using the RNN over the PARAGRAM vector addition model.,7 Qualitative Analysis,[0],[0]
"So negative values indicate superior performance by the RNN.
",7 Qualitative Analysis,[0],[0]
A few trends emerge from this table.,7 Qualitative Analysis,[0],[0]
"One is that as the length ratio increases (i.e., the phrase pairs are closer in length), addition surpasses the RNN for positive examples.",7 Qualitative Analysis,[0],[0]
"For negative examples, the trend is reversed.",7 Qualitative Analysis,[0],[0]
"The same trend appears for over-
15The bin delimiters were chosen to be uniform over the range of output values of the length ratio ([0.4,1] with one outlier data point removed) and overlap ratio ([0,1]).
lap ratio.",7 Qualitative Analysis,[0],[0]
"Examples from Annotated-PPDB illustrating these trends on positive examples are shown in Table 8.
",7 Qualitative Analysis,[0],[0]
"When considering both positive and negative examples (“Both”), we see that the RNN excels on the most difficult examples (large differences in phrase length and less lexical overlap).",7 Qualitative Analysis,[0],[0]
"For easier examples, the two fare similarly overall (-2.0 to 0.0% change), but the RNN does much better on negative examples.",7 Qualitative Analysis,[0],[0]
This aligns with the intuition that addition should perform well when two paraphrastic phrases have high lexical overlap and similar length.,7 Qualitative Analysis,[0],[0]
"But when they are not paraphrases, simple addition is misled and the RNN’s learned composition function better captures the relationship.",7 Qualitative Analysis,[0],[0]
This may suggest new architectures for modeling compositionality differently depending on differences in length and amount of overlap.,7 Qualitative Analysis,[0],[0]
We have shown how to leverage PPDB to learn state-of-the-art word embeddings and compositional models for paraphrase tasks.,8 Conclusion,[0],[0]
"Since PPDB was created automatically from parallel corpora, our models are also built automatically.",8 Conclusion,[0],[0]
"Only small amounts of annotated data are used to tune hyperparameters.
",8 Conclusion,[0],[0]
"We also introduced two new datasets to evaluate compositional models of short paraphrases, filling a gap in the NLP community, as currently there are no datasets created for this purpose.",8 Conclusion,[0],[0]
"Successful models on these datasets can then be used to extend the coverage of, or provide an alternative to, PPDB.
",8 Conclusion,[0],[0]
"There remains a great deal of work to be done in developing new composition models, whether with new network architectures or distance functions.",8 Conclusion,[0],[0]
"In this work, we based our composition function on constituent parse trees, but this may not be the best approach—especially for short phrases.",8 Conclusion,[0],[0]
"Dependency syntax may be a better alternative (Socher et al., 2014).",8 Conclusion,[0],[0]
"Besides improving composition, another direction to explore is how to use models for short phrases in sentence-level paraphrase recognition and other downstream tasks.",8 Conclusion,[0],[0]
Increasing the dimension of word embeddings or training them on more data can have a significant positive impact on many tasks—both at the word level and on downstream tasks.,Appendix A,[0],[0]
"We scaled
up our original 25-dimensional PARAGRAM embeddings and modified our training procedure slightly in order to produce two sets of 300-dimensional PARAGRAM vectors.16 The vectors outperform our original 25-dimensional PARAGRAM vectors on all tasks and achieve human-level performance on SL999 and WS353.",Appendix A,[0],[0]
"Moreover, when simply using vector addition as a compositional model, they are both on par with the RNN models we trained specifically for each task.",Appendix A,[0],[0]
"These results can be seen in Tables 10, 11, and 12.
",Appendix A,[0],[0]
"The main modification was to use higherdimensional initial embeddings, in our case the pretrained 300-dimensional GloVe embeddings.17 Since PPDB only contains lowercased words, we extracted only one GloVe vector per word type (regardless of case) by taking the first occurrence of each word in the vocabulary.",Appendix A,[0],[0]
"This is the vector for the most common casing of the word, and was used as
16Both PARAGRAM300,WS353 and PARAGRAM300,SL999 vectors can be found on the authors’ websites.
",Appendix A,[0],[0]
"17We used the GloVe vectors trained on 840 billion tokens of Common Crawl data, available at http://nlp.stanford.edu/projects/glove/
the word’s single initial vector in our experiments.",Appendix A,[0],[0]
"This reduced the vocabulary from the original 2.2 million types to 1.7 million.
",Appendix A,[0],[0]
Smaller changes included replacing dot product with cosine similarity in Equation 2 and a change to the negative sampling procedure.,Appendix A,[0],[0]
"We experimented with three approaches: MAX sampling discussed in Section 4.1, RAND sampling which is random sampling from the batch, and a 50/50 mixture of MAX sampling and RAND sampling.
",Appendix A,[0],[0]
"For training data, we selected all word pairs in the lexical portion of PPDB XL that were in our vocabulary, removing redundancies.",Appendix A,[0],[0]
"This resulted in 169,591 pairs for training.",Appendix A,[0],[0]
"We trained our models for 10 epochs and tuned hyperparameters (batch size, λWw , δ, and sampling method) in two ways: maximum correlation on WS353 (PARAGRAM300,WS353) and maximum correlation on SL999 (PARAGRAM300,SL999).18",Appendix A,[0],[0]
"We report results for both sets of embeddings in Tables 10, 11, and 12, and make both available to the community in the hope that they may be useful for other downstream tasks.",Appendix A,[0],[0]
"We thank the editor and the anonymous reviewers as well as Juri Ganitkevitch, Weiran Wang, and Kazuma Hashimoto for their valuable comments and technical assistance.",Acknowledgements,[0],[0]
"We also thank Chris Callison-Burch, Dipanjan Das, Kuzman Ganchev, Ellie Pavlick, Slav Petrov, Owen Rambow, David Sontag, Oscar Täckström, Kapil Thadani, Lyle Ungar, Benjamin Van Durme, and Mo Yu for helpful conversations.",Acknowledgements,[0],[0]
"This research was supported by a Google Faculty Research Award to Mohit Bansal, Karen Livescu, and Kevin Gimpel, the Multimodal Information Access & Synthesis Center at UIUC, part of CCICADA, a DHS Science and Technology Center of Excellence, and by DARPA under agreement number FA8750-13-2-0008.",Acknowledgements,[0],[0]
"The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA or the U.S. Government.
",Acknowledgements,[0],[0]
"18Note that if we use the approach in Section 5.3 in which we tune to maximize 2×WS-S correlation minus the WS-R correlation, the SL999 ρ is 0.640, still higher than any other reported result to the best of our knowledge.",Acknowledgements,[0],[0]
"The Paraphrase Database (PPDB; Ganitkevitch et al., 2013) is an extensive semantic resource, consisting of a list of phrase pairs with (heuristic) confidence estimates.",abstractText,[0],[0]
"However, it is still unclear how it can best be used, due to the heuristic nature of the confidences and its necessarily incomplete coverage.",abstractText,[0],[0]
We propose models to leverage the phrase pairs from the PPDB to build parametric paraphrase models that score paraphrase pairs more accurately than the PPDB’s internal scores while simultaneously improving its coverage.,abstractText,[0],[0]
They allow for learning phrase embeddings as well as improved word embeddings.,abstractText,[0],[0]
"Moreover, we introduce two new, manually annotated datasets to evaluate short-phrase paraphrasing models.",abstractText,[0],[0]
"Using our paraphrase model trained using PPDB, we achieve state-of-the-art results on standard word and bigram similarity tasks and beat strong baselines on our new short phrase paraphrase tasks.1,2",abstractText,[0],[0]
From Paraphrase Database to Compositional Paraphrase Model and Back,title,[0],[0]
"Models of the statistical structure of natural images play a key role in computer vision and image processing (Srivastava et al., 2003).",1. Introduction,[0],[0]
"Due to the high dimensionality of the images captured by modern cameras, a rich research literature instead models the statistics of small image patches.",1. Introduction,[0],[0]
"For example, the K-SVD method (Elad & Aharon, 2006) generalizes K-means clustering to learn a dictionary for sparse coding of image patches.",1. Introduction,[0],[0]
"The state-of-the-art learned simultaneous sparse coding (LSSC, Mairal et al. (2009)) and block matching and 3D filtering (BM3D, Dabov et al. (2008)) methods integrate clustering, dictionary learning,
1Brown University, Providence, RI, USA.",1. Introduction,[0],[0]
"2Harvard University, Cambridge, MA, USA. 3University of California, Irvine, CA, USA.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Geng Ji <gji@cs.brown.edu>.
Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
and denoising to extract information directly from a single corrupted image.,1. Introduction,[0],[0]
"Alternatively, the accurate expected patch log-likelihood (EPLL, Zoran & Weiss (2011)) method maximizes the log-likelihood of overlapping image patches under a finite Gaussian mixture model learned from uncorrupted natural images.
",1. Introduction,[0],[0]
"We show that with minor modifications, the objective function underlying EPLL is equivalent to a variational loglikelihood bound for a novel generative model of whole images.",1. Introduction,[0],[0]
Our model coherently captures overlapping image patches via a randomly positioned spatial grid.,1. Introduction,[0],[0]
"By deriving a rigorous variational bound, we then develop improved nonparametric models of natural image statistics using the hierarchical Dirichlet process (HDP, Teh et al. (2006)).",1. Introduction,[0],[0]
"In particular, DP mixtures allow an appropriate model complexity to be inferred from data, while the hierarchical DP captures the patch self-similarities and repetitions that are ubiquitous in natural images (Jégou et al., 2009).",1. Introduction,[0],[0]
"Unlike previous whole-image generative models such as fields of experts (FoE, Roth & Black (2005)), which uses a single set of Markov random field parameters to model all images, our HDP model learns image-specific clusters to accurately model distinctive textures.",1. Introduction,[0],[0]
"Coupled with a scalable structured variational inference algorithm, we improve on the excellent denoising accuracy of the LSSC and BM3D algorithms, while providing a Bayesian nonparametric model with a broader range of potential applications.",1. Introduction,[0],[0]
"Our approach is derived from models of small (8× 8 pixel) patches of a large natural image x. Let Pi be a binary indicator matrix that extracts the G = 82 pixels Pix ∈ RG in patch i. To reduce sensitivity to lighting variations, a contrast normalizing transform is applied to remove the mean (or “DC component”) of the pixel intensities in each patch:
vi = Pix− 1G1TPix = BPix, (1)
for a “zero-centering” matrix B. Zoran & Weiss (2012) show that a finite mixture of K zero-mean Gaussians,
p(vi) = ∑K k=1 πkNorm(vi | 0,Λ−1k ), (2)
is superior to many classic image models in terms of predictive likelihood and patch denoising performance.
",2. Expected Patch Log-likelihood,[0],[0]
"The widely-used EPLL image restoration framework measures the quality of a reconstruction by the expected patch log-likelihood, “assuming a patch location in the image is chosen uniformly at random” (Zoran & Weiss, 2011).",2. Expected Patch Log-likelihood,[0],[0]
"Given a corrupted image y, EPLL estimates a clean image x by minimizing the objective:
min x
λ 2 ‖x− y‖2 −∑i log p(BPix).",2. Expected Patch Log-likelihood,[0],[0]
"(3)
Here, the sum ranges over all overlapping, completely visible (uncropped) image patches.",2. Expected Patch Log-likelihood,[0],[0]
"The constant λ is determined by the noise level of the corrupted image y.
Direct optimization of Eq.",2. Expected Patch Log-likelihood,[0],[0]
"(3) is challenging, so inspired by half quadratic splitting (Geman & Yang, 1995), the EPLL objective can be reformulated as follows:
min x,v̄
λ",2. Expected Patch Log-likelihood,[0],[0]
2 ‖x− y‖2 + ∑,2. Expected Patch Log-likelihood,[0],[0]
i κ 2 ‖Pix− v̄i‖2 − log p(Bv̄i).,2. Expected Patch Log-likelihood,[0],[0]
"(4)
Each patch i is allocated an auxiliary variable v̄i, which (unlike the vi variable in Eq.",2. Expected Patch Log-likelihood,[0],[0]
(1)) includes an estimate of the mean patch intensity.,2. Expected Patch Log-likelihood,[0],[0]
This augmented objective leads to closed-form coordinate descent updates.,2. Expected Patch Log-likelihood,[0],[0]
Gating.,2. Expected Patch Log-likelihood,[0],[0]
"Assign each patch i to some cluster zi:
zi = arg max k
πk Norm ( BPix | 0,Λ−1k + κI ) .",2. Expected Patch Log-likelihood,[0],[0]
"(5)
Filtering.",2. Expected Patch Log-likelihood,[0],[0]
"Given an approximate clean image x and cluster assignments z, denoise patches via least squares:
v̄i =",2. Expected Patch Log-likelihood,[0],[0]
( I + κ−1BTΛziB )−1,2. Expected Patch Log-likelihood,[0],[0]
Pix.,2. Expected Patch Log-likelihood,[0],[0]
"(6)
Mixing.",2. Expected Patch Log-likelihood,[0],[0]
"Given a fixed set of auxiliary patches v̄ and the noisy image y, a denoised image x is estimated as
x = ( λI + κ ∑ i PTi Pi )−1",2. Expected Patch Log-likelihood,[0],[0]
( λy + κ ∑ i PTi v̄i ) .,2. Expected Patch Log-likelihood,[0],[0]
"(7)
Annealing.",2. Expected Patch Log-likelihood,[0],[0]
Optimal solutions of Eq.,2. Expected Patch Log-likelihood,[0],[0]
(4) approach those of the EPLL objective in Eq.,2. Expected Patch Log-likelihood,[0],[0]
(3) as κ→∞. EPLL denoising algorithms slowly increase κ via an annealing schedule that must be tuned for best performance.,2. Expected Patch Log-likelihood,[0],[0]
Justification?,2. Expected Patch Log-likelihood,[0],[0]
"Empirically, the intuitive EPLL objective is much more effective than baselines which use only a subset of non-overlapping patches, or average independently denoised patches (Zoran & Weiss, 2011).",2. Expected Patch Log-likelihood,[0],[0]
"But why should we optimize the expected log-likelihood, instead of the expected likelihood or another function of patch-specific likelihoods?",2. Expected Patch Log-likelihood,[0],[0]
And how can the EPLL heuristic be generalized to capture more complex statistics of natural images?,2. Expected Patch Log-likelihood,[0],[0]
"This paper answers these questions by linking EPLL to a rigorous, nonparametric generative model of whole images.",2. Expected Patch Log-likelihood,[0],[0]
"We now develop the HDP-Grid generative model summarized in Fig. 1, which uses randomly placed patch grids to formalize the EPLL objective, and hierarchical DP mixtures to capture image patch self-similarity.",3. Mixture Models for Grids of Image Patches,[0],[0]
"The hierarchical Dirichlet process (HDP, Teh et al. (2006)) is a Bayesian nonparametric prior used to cluster groups of related data; we model natural images as groups of patches.",3.1. Hierarchical Dirichlet Process Mixtures,[0],[0]
"The HDP shares visual structure, such as patches of grass or bricks, by sharing a common set of clusters (called topics in applications to text data) across images.",3.1. Hierarchical Dirichlet Process Mixtures,[0],[0]
"In addition, the HDP models image-specific variability by allowing each image to use this shared set of clusters with unique frequencies; grass might be abundant in one image but absent in another.",3.1. Hierarchical Dirichlet Process Mixtures,[0],[0]
"Via the HDP, we can learn the proper number of hidden clusters from data, and discover new clusters as we collect new images with novel visual textures.
",3.1. Hierarchical Dirichlet Process Mixtures,[0],[0]
The HDP uses a stick-breaking construction to generate a corpus-wide vector π0 =,3.1. Hierarchical Dirichlet Process Mixtures,[0],[0]
"[π01, π02, . . .",3.1. Hierarchical Dirichlet Process Mixtures,[0],[0]
", π0k, . . .",3.1. Hierarchical Dirichlet Process Mixtures,[0],[0]
],3.1. Hierarchical Dirichlet Process Mixtures,[0],[0]
"of frequencies for a countably infinite set of visual clusters:
βk ∼ Beta(1, γ), π0k(β) , βk ∏k−1 `=1 (1− β`).",3.1. Hierarchical Dirichlet Process Mixtures,[0],[0]
"(8) The HDP allocates each image m its own cluster frequencies πm, where the vector π0 determines the mean of a DP prior on the frequencies of shared clusters:
πm ∼ DP(απ0), E[πmk] = π0k.",3.1. Hierarchical Dirichlet Process Mixtures,[0],[0]
"(9) When the concentration parameter α < 1, we capture the “burstiness” and self-similarity of natural image regions (Jégou et al., 2009) by placing most probability mass in πm on a sparse subset of global clusters.",3.1. Hierarchical Dirichlet Process Mixtures,[0],[0]
We sample pixels in imagem via a randomly placed grid of patches.,3.2. Image Generation via Random Grids,[0],[0]
"When each patch has G pixels, Fig. 2 shows there are exactlyG grid alignments for an image of arbitrary size.",3.2. Image Generation via Random Grids,[0],[0]
"The alignment wm ∈ {1, . . .",3.2. Image Generation via Random Grids,[0],[0]
", G} has a uniform prior: wm ∼ Cat(1/G, . . .",3.2. Image Generation via Random Grids,[0],[0]
", 1/G).",3.2. Image Generation via Random Grids,[0],[0]
(10) Modeling multiple overlapping grids is crucial to capture real image statistics.,3.2. Image Generation via Random Grids,[0],[0]
"As the true grid alignment for each image is uncertain, posterior inference will favor images
that are likely under all possible wm.",3.2. Image Generation via Random Grids,[0],[0]
"Models based on a single, fixed grid produce severe artifacts at patch boundaries, as shown in Fig. 2 of Zoran & Weiss (2011).",3.2. Image Generation via Random Grids,[0],[0]
"Gaussian mixtures provide excellent density models for natural image patches (Zoran & Weiss, 2012).",3.3. Patch Generation via Gaussian Mixtures,[0],[0]
"We associate clusters with zero-mean, full-covariance Gaussian distributions on patches with G pixels.",3.3. Patch Generation via Gaussian Mixtures,[0],[0]
We parameterize cluster k by a precision (inverse covariance) matrix,3.3. Patch Generation via Gaussian Mixtures,[0],[0]
"Λk ∼ Wish(ν,W ), whose conjugate Wishart prior has ν degrees of freedom and scale matrix W .",3.3. Patch Generation via Gaussian Mixtures,[0],[0]
"Given that wm = g, each of the Nmg patches vmgn in grid g is sampled from an infinite mixture with image-specific cluster frequencies:
p(vmgn|wm = g) = ∞∑ k=1 πmkNorm(vmgn|0,Λ−1k ).",3.3. Patch Generation via Gaussian Mixtures,[0],[0]
(11) Let zmgn |,3.3. Patch Generation via Gaussian Mixtures,[0],[0]
wm = g ∼ Cat(πm) denote the cluster that generates patch n. To account for the contrast normalization of Eq.,3.3. Patch Generation via Gaussian Mixtures,[0],[0]
"(1), the intensities in patch n are shifted by an independent, scalar “DC offset” umgn: p(umgn | wm = g) =",3.3. Patch Generation via Gaussian Mixtures,[0],[0]
"Norm(umgn | r, s2).",3.3. Patch Generation via Gaussian Mixtures,[0],[0]
"(12) Finally, if wm 6=",3.3. Patch Generation via Gaussian Mixtures,[0],[0]
"g so that grid g is unobserved, we sample (zmgn, vmgn, umgn) from some reference distribution
independent of the HDP mixture model parameters.",3.3. Patch Generation via Gaussian Mixtures,[0],[0]
"Given patches vmg with offsets umg generated via grid wm = g, we sample a whole “clean image” xm as
Norm ( xm | ∑Nmg n=1 PTmgnv̄mgn, δ 2I ) , (13)
where v̄mgn , Cmgnvmgn+umgn.",3.4. From Patches to Corrupted Images,[0],[0]
"Binary indicator matrices Pmgn, as in Sec. 2, stitch together patches in the chosen grid g. Image xm is then generated by adding independent Gaussian noise with small variance δ2.",3.4. From Patches to Corrupted Images,[0],[0]
"Most patches in the chosen grid will be fully observed in xm, but as illustrated in Fig. 2, some may be clipped by the image boundary.",3.4. From Patches to Corrupted Images,[0],[0]
"Indicator matrices Cmgn are defined so Cmgnvmgn + umgn is a vector containing the observed pixels from patch n.
For image restoration tasks, the observed image",3.4. From Patches to Corrupted Images,[0],[0]
ym is a corrupted version of some clean image xm,3.4. From Patches to Corrupted Images,[0],[0]
that we would like to estimate.,3.4. From Patches to Corrupted Images,[0],[0]
"Models of natural image statistics are commonly validated on the problem of image denoising, where xm is polluted by additive white Gaussian noise:
p(ym | xm) = Norm(ym",3.4. From Patches to Corrupted Images,[0],[0]
"| xm, σ2I).",3.4. From Patches to Corrupted Images,[0],[0]
(14) The variance σ2 δ2 indicates the noise level.,3.4. From Patches to Corrupted Images,[0],[0]
"We also validate our model on image inpainting problems (Bertalmio et al., 2000), where some pixels are observed without noise but others are completely missing.",3.4. From Patches to Corrupted Images,[0],[0]
"By replacing Eq. (14) with other linear likelihood models, our novel generative model for natural images may be easily applied to other tasks including image deblurring (Zoran & Weiss, 2011), image super resolution (Yang & Huang, 2010), and color image demosaicing (Mairal et al., 2009).",3.4. From Patches to Corrupted Images,[0],[0]
"We now develop scalable learning algorithms for our nonparametric, grid-based image model.",4. Variational Inference,[0],[0]
We first examine a baseline DP Grid model in which the same cluster frequencies π0 are shared by all images.,4. Variational Inference,[0],[0]
"Our full HDP Grid model then learns image-specific cluster frequencies πm, and instantiates new clusters to model unique visual textures.",4. Variational Inference,[0],[0]
Our goal is to infer the DP Grid model parameters that best explain observed images which may be clean (xm) or corrupted by noise (ym).,4.1. DP Grid: Variational Inference,[0],[0]
"The DP Grid model uses the same cluster probabilities π0, generated from stickbreaking weights β as in Eq.",4.1. DP Grid: Variational Inference,[0],[0]
"(8), for all images.
",4.1. DP Grid: Variational Inference,[0],[0]
Learning from clean images.,4.1. DP Grid: Variational Inference,[0],[0]
"Given a training set D of uncorrupted images x1, . . .",4.1. DP Grid: Variational Inference,[0],[0]
"xM , we estimate the posterior distribution p(β,Λ, w,Ψpatch | x) for our global mixture model parameters β and Λ, grid assignment indicators wm, and patch-level latent variables Ψpatchm = {um, vm, zm}.
",4.1. DP Grid: Variational Inference,[0],[0]
"Exact posterior inference is intractable, so we instead find an approximate posterior q(·) = q(β,Λ, w,Ψpatch) minimizing the KL divergence (Wainwright & Jordan, 2008) from the true posterior p(·|x).",4.1. DP Grid: Variational Inference,[0],[0]
"Equivalently, our variational method maximizes the following objective L:
max q∈Q L(q, x) = max q∈Q Eq [ log p(x, ·) q(·) ] ≤ log p(x).",4.1. DP Grid: Variational Inference,[0],[0]
"(15)
We constrain the solution of our optimization to come from a tractable family of structured mean-field distributions Q, parameterized by free parameters.",4.1. DP Grid: Variational Inference,[0],[0]
"Unlike naı̈ve mean-field methods which assume complete posterior independence, our structured mean-field approximation is more accurate and includes dependencies between some latent variables:
q(·) = ∞∏ k=1 q(Λk)q(βk) · M∏",4.1. DP Grid: Variational Inference,[0],[0]
"m=1 q(wm)q(Ψ patch m |wm).
",4.1. DP Grid: Variational Inference,[0],[0]
"As in Hughes & Sudderth (2013), this approximate posterior family contains infinitely many clusters, just like the true posterior.",4.1. DP Grid: Variational Inference,[0],[0]
"Rather than applying a fixed truncation to the stick-breaking prior (Blei & Jordan, 2006), we dynamically truncate the patch assignment distributions q(z) to only use the first K clusters to explain the M observed images.",4.1. DP Grid: Variational Inference,[0],[0]
"Clusters with indices k > K then have factors q(Λk) set to the prior, and need not be explicitly represented.
",4.1. DP Grid: Variational Inference,[0],[0]
Global mixture model.,4.1. DP Grid: Variational Inference,[0],[0]
"The global cluster weights β and precision matrices Λ have standard exponential family forms (free parameters are marked by hats):
q(Λk) = Wish ( ν̂k, Ŵk ) , q(βk) = Beta ( ρ̂kω̂k, (1− ρ̂k)ω̂k ) .
",4.1. DP Grid: Variational Inference,[0],[0]
"Here ρ̂k = Eq[βk], and ω̂k controls the variance of q(βk).
",4.1. DP Grid: Variational Inference,[0],[0]
Image-specific alignment.,4.1. DP Grid: Variational Inference,[0],[0]
"For natural images, all grid alignments are typically of similar quality, so we fix a uniform alignment posterior q(wm) =",4.1. DP Grid: Variational Inference,[0],[0]
"Cat ( 1 G , . . .",4.1. DP Grid: Variational Inference,[0],[0]
", 1 G ) .",4.1. DP Grid: Variational Inference,[0],[0]
"This simplifies many updates while still avoiding artifacts that would arise from a single, non-overlapping patch grid.
",4.1. DP Grid: Variational Inference,[0],[0]
Patch-specific factors.,4.1. DP Grid: Variational Inference,[0],[0]
"The patch-specific variables Ψpatch have structured posteriors, conditioned on the value of the grid indicator wm for the current image:
q(zmgn | wm = g)",4.1. DP Grid: Variational Inference,[0],[0]
"= Categorical ( r̂mgn1, ..., r̂mgnK ) ,
q(umgn | wm = g) =",4.1. DP Grid: Variational Inference,[0],[0]
"Norm ( ûmgn, φ̂ u mgn ) ,
q(vmgn | wm = g,zmgn = k) =",4.1. DP Grid: Variational Inference,[0],[0]
"Norm ( v̂mgnk, φ̂ v mgnk ) .
",4.1. DP Grid: Variational Inference,[0],[0]
"Below, we let",4.1. DP Grid: Variational Inference,[0],[0]
"Eq[·] denote the conditional expectation with respect to the variational distribution q, given wm.
Learning.",4.1. DP Grid: Variational Inference,[0],[0]
"Given clean images x, we perform coodinate ascent on the objective L, alternatively updating one factor among q(β)q(Λ)q(w)q(Ψpatch).",4.1. DP Grid: Variational Inference,[0],[0]
Most updates have closed forms due to the exponential families defining Q (see supplement).,4.1. DP Grid: Variational Inference,[0],[0]
"As one intuitive example, consider the update for
the cluster precision matrix posterior q(Λk|ν̂k, Ŵk):
ν̂k = ν + 1
G Nk, Nk = M∑ m=1",4.1. DP Grid: Variational Inference,[0],[0]
"G∑ g=1 Nmg∑ n=1 r̂mgnk, (16)
",4.1. DP Grid: Variational Inference,[0],[0]
"Ŵk = W + 1
G M∑ m=1",4.1. DP Grid: Variational Inference,[0],[0]
G∑ g=1 Nmg∑ n=1,4.1. DP Grid: Variational Inference,[0],[0]
Eq [ 1k(zmgn)vmgnv T mgn ] .︸,4.1. DP Grid: Variational Inference,[0],[0]
"︷︷ ︸
Sk
Statistic Nk(r̂) counts patches assigned to cluster k, while Sk(r̂, v̂, φ̂
v) aggregates second moments.",4.1. DP Grid: Variational Inference,[0],[0]
"These updates follow the standard form of prior parameter plus expected sufficient statistic, except the statistics are averaged (not simply added) across the G grid alignments.",4.1. DP Grid: Variational Inference,[0],[0]
"Given a corrupted image ym, we seek to compute the posterior p(xm | ym,D), where we condition on the training set D. Our variational posterior family Q now includes an additional factor for the unobserved, “clean” image xm:
q(xm) =",4.2. Image Denoising and Connections to EPLL,[0],[0]
"Norm ( xm | x̂m, φ̂xm ) .",4.2. Image Denoising and Connections to EPLL,[0],[0]
"(17)
",4.2. Image Denoising and Connections to EPLL,[0],[0]
"The variational inference objective becomes
max q∈Q
Eq [ log
p(D, ym, xm, ·) q(xm, ·)
] ≤ log p(ym,D), (18)
and the coordinate ascent update for q(xm) equals
x̂m = φ̂ x m (ym σ2 + hm δ2 ) , φ̂xm = δ2σ2 δ2 + σ2 I. (19)
",4.2. Image Denoising and Connections to EPLL,[0],[0]
"The updated covariance is diagonal, improving computational efficiency.",4.2. Image Denoising and Connections to EPLL,[0],[0]
"The mean depends on the average image vector across all patches in all grids, denoted by hm:
hm , 1
G G∑ g=1 Nmg∑ n=1 PTmgn(CmgnEq[vmgn] + ûmgn).",4.2. Image Denoising and Connections to EPLL,[0],[0]
"(20)
Note that the update for x̂m in Eq.",4.2. Image Denoising and Connections to EPLL,[0],[0]
(19) is similar to the EPLL update in Eq.,4.2. Image Denoising and Connections to EPLL,[0],[0]
"(7), except that some terms involving projection matrices become constants because we account for partially observed patches.",4.2. Image Denoising and Connections to EPLL,[0],[0]
Modeling partial patches is necessary to produce a valid likelihood bound in Eq.,4.2. Image Denoising and Connections to EPLL,[0],[0]
"(18).
",4.2. Image Denoising and Connections to EPLL,[0],[0]
"In fact, as we show below all three terms in the EPLL objective in Eq. (4) are very similar to our proposed minimization objective function −L, up to a scale factor of G. Of course, a key difference is that our objective seeks full posteriors rather than point estimates, and enables the HDP model of multiple images detailed in Sec. 4.3.
",4.2. Image Denoising and Connections to EPLL,[0],[0]
EPLL Term 1.,4.2. Image Denoising and Connections to EPLL,[0],[0]
"When we set λ , Gσ2 , the first term of the EPLL objective in Eq. (4) becomes
G · 12σ2 (x− y)T (x− y).",4.2. Image Denoising and Connections to EPLL,[0],[0]
"(21) Similarly, suppressing the subscript m denoting the image for simplicity, Eq[− log p(y|x)] in our −L simplifies as
1 2σ2Eq[(x− y)T (x− y)].",4.2. Image Denoising and Connections to EPLL,[0],[0]
"(22)
EPLL Term 2.",4.2. Image Denoising and Connections to EPLL,[0],[0]
Taking the second term in Eq.,4.2. Image Denoising and Connections to EPLL,[0],[0]
"(4) and substituting κ = 1/δ2, we have:
1 2δ2 ∑ i(Pix− v̄i)T (Pix− v̄i).",4.2. Image Denoising and Connections to EPLL,[0],[0]
"(23)
",4.2. Image Denoising and Connections to EPLL,[0],[0]
"The corresponding term Eq[− log p(x|w, u, v)] in our objective −L can be written similarly up to a scaling by G:
1
G
1
2δ2 G∑",4.2. Image Denoising and Connections to EPLL,[0],[0]
g=1 Ng∑ n=1,4.2. Image Denoising and Connections to EPLL,[0],[0]
Eq [ (Pgnx− v̄gn)T (Pgnx− v̄gn) ] .,4.2. Image Denoising and Connections to EPLL,[0],[0]
"(24)
EPLL Term 3.",4.2. Image Denoising and Connections to EPLL,[0],[0]
"The third EPLL term assumes zerocentered patches Bv̄i are drawn from Gaussian mixtures:
−∑i log p(Bv̄i | π0,Λ).",4.2. Image Denoising and Connections to EPLL,[0],[0]
"(25) Similarly, in our minimization objective −L we draw vgn from a DP mixture model.",4.2. Image Denoising and Connections to EPLL,[0],[0]
"Explicitly including the cluster assignment zgn, Eq[− log p(v, z|w)] equals
− 1 G G∑ g=1",4.2. Image Denoising and Connections to EPLL,[0],[0]
Ng∑ n=1,4.2. Image Denoising and Connections to EPLL,[0],[0]
Eq[log,4.2. Image Denoising and Connections to EPLL,[0],[0]
"p(vgn, zgn | π0,Λ)].",4.2. Image Denoising and Connections to EPLL,[0],[0]
"(26)
EPLL is similar, but maximizes assignments (Eq. (5)) rather than computing posterior assignment probabilities.",4.2. Image Denoising and Connections to EPLL,[0],[0]
Image-specific frequencies.,4.3. HDP Grid: Variational Inference,[0],[0]
"The DP model above, and the parametric EPLL objective it generalizes, assume the same cluster frequency vector π0 for each image m. Our HDP Grid model allows image-specific frequencies πm to be learned from data, via the hierarchical regularization of the HDP prior (Teh et al., 2006).",4.3. HDP Grid: Variational Inference,[0],[0]
"Our approximate posterior family Q now has the following HDP-specific factors:
q(β)",4.3. HDP Grid: Variational Inference,[0],[0]
"= ∏∞ k=1 Beta (βk | ρ̂kω̂k, (1− ρ̂k)ω̂k) , (27)
q([πm1 . .",4.3. HDP Grid: Variational Inference,[0],[0]
.πmK,4.3. HDP Grid: Variational Inference,[0],[0]
πm>K ]),4.3. HDP Grid: Variational Inference,[0],[0]
= Dir(θ̂m1 . . .,4.3. HDP Grid: Variational Inference,[0],[0]
"θ̂mK , θ̂m>K).",4.3. HDP Grid: Variational Inference,[0],[0]
"This approximate posterior represents infinitely many clusters via a finite partition of πm into K + 1 terms: one for each of theK active clusters, and a remainder term at index >K that aggregates the mass of all inactive clusters.",4.3. HDP Grid: Variational Inference,[0],[0]
The free parameter θ̂m is also a vector of size K + 1 whose last entry represents all inactive clusters.,4.3. HDP Grid: Variational Inference,[0],[0]
"We follow Hughes et al. (2015) to obtain a closed-form update for θ̂m, and gradient-based updates for ρ̂, ω̂; see the supplement for details.",4.3. HDP Grid: Variational Inference,[0],[0]
We highlight that the θ̂m update naturally includes a 1G rescaling of count sufficient statistics as in Eq.,4.3. HDP Grid: Variational Inference,[0],[0]
(16).,4.3. HDP Grid: Variational Inference,[0],[0]
"Other factors remain unchanged from the DP Grid model.
",4.3. HDP Grid: Variational Inference,[0],[0]
Image-specific clusters.,4.3. HDP Grid: Variational Inference,[0],[0]
"Due to the heavy-tailed distribution of natural images (Ruderman, 1997), even with large training sets, test images may still contain unique textural patterns like the striped scarf in the Barbara image in Fig. 3.",4.3. HDP Grid: Variational Inference,[0],[0]
"Fortunately, our Bayesian nonparametric HDP Grid model provides a coherent way to capture such patterns by appending K ′ novel, image-specific clusters to the original K clusters learned from training images.",4.3. HDP Grid: Variational Inference,[0],[0]
"These novel clusters lead to more accurate posterior approximations q ∈ Q that better optimize our objective L.
We initialize inference by creating K ′ = 100 imagespecific clusters with the k-means++ algorithm (Arthur & Vassilvitskii, 2007), which minimizes the cost function
J (z′,Λ′) = ∑i∑K′k=1 1k(z′i)D(ṽiṽTi ,Λ′k), (28) where the first sum is over the set of fully-observed patches within the image.",4.3. HDP Grid: Variational Inference,[0],[0]
"The function D is the Bregman divergence associated with our zero-mean Gaussian likelihood (Banerjee et al., 2005), and ṽi = BPiy is a zerocentered patch.",4.3. HDP Grid: Variational Inference,[0],[0]
We initialize the algorithm by sampling K ′,4.3. HDP Grid: Variational Inference,[0],[0]
"diverse patches in a distance-biased fashion, and refine with 50 iterations of coordinate descent updates of z′ and Λ′.
Then we expand the variational posterior q(Λ) intoK+K ′ clusters.",4.3. HDP Grid: Variational Inference,[0],[0]
"The first K indices are kept the same as training, and the last K ′ indices are set via Eq.",4.3. HDP Grid: Variational Inference,[0],[0]
"(16) using sufficient statistics N ′, S′ derived from hard assignments z′:
N ′k′ ← ∑ i 1k′(z ′",4.3. HDP Grid: Variational Inference,[0],[0]
"i), S ′ k′",4.3. HDP Grid: Variational Inference,[0],[0]
←,4.3. HDP Grid: Variational Inference,[0],[0]
[∑ i 1k′(z ′,4.3. HDP Grid: Variational Inference,[0],[0]
i)ṽiṽ T i −Nk′σ2I ] + .,4.3. HDP Grid: Variational Inference,[0],[0]
"Here, following Portilla et al. (2003) and Kivinen et al. (2007), S′k′ estimates the clean data statistic Sk′ by subtracting the expected noise covariance.",4.3. HDP Grid: Variational Inference,[0],[0]
"The [·]+ operator thresholds any negative eigenvalues to zero.
",4.3. HDP Grid: Variational Inference,[0],[0]
"Similarly, the other global variational factor q(β) is also expanded to K + K ′ clusters via sufficient statistics N ′ and counts of cluster usage from training data.",4.3. HDP Grid: Variational Inference,[0],[0]
"Given {β,Λ}K+K′k=1 , each factor in q may then be updated in turn to maximize the variational objective L (see supplement).",4.3. HDP Grid: Variational Inference,[0],[0]
"Finally, while we initialize K ′ to a large number to avoid local optima, this may lead to extraneous clusters.",4.3. HDP Grid: Variational Inference,[0],[0]
We thus delete new clusters that our sparsity-biased variational updates do not assign to any patch.,4.3. HDP Grid: Variational Inference,[0],[0]
"In the Barbara image in Fig. 3, this leaves 9 image-specific clusters.",4.3. HDP Grid: Variational Inference,[0],[0]
"Deletion improves model interpretability and algorithm speed, because costs scale linearly with the number of instantiated clusters.",4.3. HDP Grid: Variational Inference,[0],[0]
"Following EPLL, we train our HDP-Grid model using 400 clean training and validation images from the Berkeley segmentation dataset (BSDS, Martin et al. (2001)).",5. Experiments,[0],[0]
We fix δ = 0.5/255 to account for the quantization of image intensities to 8-bit integers.,5. Experiments,[0],[0]
Observed DC offsets u provide maximum likelihood estimates of the mean r and variance s2 in Eq.,5. Experiments,[0],[0]
(12).,5. Experiments,[0],[0]
"Similarly, we compute empirical covariance matrices for patches in the same image segments to estimate hyperparameters W and ν in Eq.",5. Experiments,[0],[0]
(16).,5. Experiments,[0],[0]
"Using variational learning algorithms that adapt the number of clusters to the observed data (Hughes & Sudderth, 2013), we discover K = 449 clusters for the DP-Grid model, which we use to initialize our HDP model.",5. Experiments,[0],[0]
"We set our annealing schedule for κ to match that used by the public EPLL code.
",5. Experiments,[0],[0]
"Image denoising methods are often divided into two types (Zontak & Irani, 2011): external methods (like
EPLL) that learn all parameters from a training database of clean images, and internal methods that denoise patches using other patches of the single noisy image.",5. Experiments,[0],[0]
"For example, the K-SVD (Elad & Aharon, 2006) has an external variant that uses a dictionary learned from clean images, and an internal variant that learns its dictionary from the noisy image.",5. Experiments,[0],[0]
"A major contribution of our paper is to show that the hierarchical DP leads to a principled hybrid of internal and external methods, in which cues from clean and noisy images are automatically combined in an adaptive way.",5. Experiments,[0],[0]
"We test our algorithm on 12 “classic” images used in many previous denoising papers (Mairal et al., 2009; Zoran & Weiss, 2011), as well as the 68 BSDS test images used by (Roth & Black, 2005; Zoran & Weiss, 2011).",5.1. Image Denoising,[0],[0]
"We evaluate
the denoising performance by the peak signal-to-noise ratio (PSNR), a logarithmic transform of the mean squared error (MSE) between images with normalized intensities,
PSNR , −20 log10 MSE.",5.1. Image Denoising,[0],[0]
"(29) We also evaluate the structural similarity index (SSIM, Wang et al. (2004)), which quantifies image quality degradation via changes in structure, luminance, and contrast.
",5.1. Image Denoising,[0],[0]
Internal vs. external clusters.,5.1. Image Denoising,[0],[0]
"In result figures, we use eDP to refer to our DP-Grid model trained solely on external clean images and HDP to refer to the HDP-Grid model that also learns novel image-specific clusters.",5.1. Image Denoising,[0],[0]
"We also train an internal DP-Grid model, referred to as iDP, using only information from the noisy test image.",5.1. Image Denoising,[0],[0]
"The first four columns of Table 1 compare their average denoising performance, where EPLL can be viewed as a simplification of eDP.",5.1. Image Denoising,[0],[0]
"For all noise levels and datasets, the HDP model has superior performance.",5.1. Image Denoising,[0],[0]
"As shown in Fig. 6, HDP is more accurate than EPLL and eDP for every single classic-12 image.",5.1. Image Denoising,[0],[0]
"Also, the consistent gain in performance from EPLL to eDP demonstrates the benefits of Bayesian nonparametric learning of an appropriate model complexity (for EPLL, the number of clusters was arbitrarily fixed at K = 200).
",5.1. Image Denoising,[0],[0]
"Fig. 3 further illustrates the complementary role of internal
Table 1.",5.1. Image Denoising,[0],[0]
Average PSNR and SSIM values on benchmark datasets (larger values indicate better denoising).,5.1. Image Denoising,[0],[0]
"Methods are highlighted if they are indistinguishable with 95% confidence, according to a Wilcoxon signed-rank test on the fraction of images where one method outperforms another.",5.1. Image Denoising,[0],[0]
"For all noise levels the patch size of BM3D is fixed to 8× 8 and LSSC is fixed to 9× 9.
",5.1. Image Denoising,[0],[0]
metric dataset σ iDP,5.1. Image Denoising,[0],[0]
"EPLL eDP HDP FoE eKSVD iKSVD BM3D LSSC
PSNR
classic-12 10 33.66 33.68 33.77 33.99 33.11 33.45 33.62 33.98 34.05 25 29.02 29.39 29.47 29.68 28.32 28.89 29.11 29.73 29.74 50 25.44 26.22 26.28 26.42 24.69 25.44 25.64 26.55 26.43 BSDS-68 10 33.10 33.37 33.42 33.47 32.69 33.06 33.08 33.26 33.45 25 28.33 28.72 28.76 28.82 27.76 28.28 28.28 28.55 28.70 50 25.10 25.72 25.75 25.83 24.48 25.17 25.17 25.59 25.50
SSIM
classic-12 10 0.9118 0.9136 0.9143 0.9169 0.8962 0.9084 0.9111 0.9168 0.9185 25 0.8189 0.8286 0.8299 0.8337 0.8018 0.8082 0.8131 0.8357 0.8359 50 0.6962 0.7301 0.7316 0.7366 0.6885 0.6926 0.6975 0.7425 0.7390 BSDS-68 10 0.9119 0.9219 0.9224 0.9230 0.8971 0.9128 0.9135 0.9157 0.9206 25 0.7964 0.8090 0.8103 0.8131 0.7804 0.7859 0.7879 0.8010 0.8109 50 0.6636 0.6870 0.6880 0.6962 0.6585 0.6544 0.6539 0.6840 0.6885
1 1.5 2
ELBO/pixel
26
28
30
32
P S
N R
eDP HDP
Figure 6.",5.1. Image Denoising,[0],[0]
Clean-image evidence lower bound (ELBO) versus output PSNR (σ = 25) for 12 “classic” images.,5.1. Image Denoising,[0],[0]
The horizontal axis plots log p(xtest|xtrain),5.1. Image Denoising,[0],[0]
"≈ L(xtest, xtrain)−L(xtrain), divided by the number of pixels.",5.1. Image Denoising,[0],[0]
"Our HDP is uniformly superior to the eDP.
and external clusters for a single test image (“Barbara”).",5.1. Image Denoising,[0],[0]
"The internal iDP perfectly captures some unique textures like the striped clothing, but produces artifacts in smooth background regions.",5.1. Image Denoising,[0],[0]
"The external EPLL and eDP better represent smooth surfaces and contours, which are common in training data, but poorly recover striped textures.
",5.1. Image Denoising,[0],[0]
"As shown in Fig. 5, while the relative accuracy of the eDP and iDP models varies depending on image statistics, the HDP model adaptively combines external and internal clusters for superior performance at all noise levels.",5.1. Image Denoising,[0],[0]
"By capturing the expected self-similarity of image patches, the HDP model also reduces artifacts in large regions with regular textures, such as the smoothly shaded areas of Fig. 4.
",5.1. Image Denoising,[0],[0]
Computational speed.,5.1. Image Denoising,[0],[0]
"To denoise a 512× 512 pixel image on a modern laptop, our Python code for eDP inference with K = 449 clusters takes about 12 min.",5.1. Image Denoising,[0],[0]
"The public EPLL Matlab code (Zoran & Weiss, 2011) with K = 200 clusters takes about 5 min.",5.1. Image Denoising,[0],[0]
"With equal numbers of clusters, the two methods have comparable runtimes.",5.1. Image Denoising,[0],[0]
"Our open-source Python code is available online at
Original FoE
EPLL HDP
Figure 7.",5.1. Image Denoising,[0],[0]
A qualitative comparison of image inpainting algorithms.,5.1. Image Denoising,[0],[0]
"As illustrated in the three close-up views, the HDP exploits patch self-similarity to better recover fine details.
",5.1. Image Denoising,[0],[0]
"github.com/bnpy/hdp-grid-image-restoration.
",5.1. Image Denoising,[0],[0]
Learning image-specific clusters for the HDP model is more expensive: our non-optimized Python denoising code currently requires about 30 min. per image.,5.1. Image Denoising,[0],[0]
Nearly all of the extra time is spent on the k-means++ initialization of Eq.,5.1. Image Denoising,[0],[0]
(28).,5.1. Image Denoising,[0],[0]
"We expect this can be sped up significantly by coding core routines in C, parallelizing some sub-steps (possibly via GPUs), using fewer internal clusters (100 is often too many), or using faster initialization heuristics.
Performance.",5.1. Image Denoising,[0],[0]
We compare our HDP model to other patch-based denoising methods in Table 1.,5.1. Image Denoising,[0],[0]
"On classic-12, where many top methods have been hand-tuned to perform well, our model is statistically indistinguishable from the best baselines.",5.1. Image Denoising,[0],[0]
"On the larger BSDS-68, our performance is superior to the state-of-the-art, showing the value of nonparametric learning from large image collections.",5.1. Image Denoising,[0],[0]
See Fig. 8 for examples.,5.1. Image Denoising,[0],[0]
"At higher noise levels (σ = 50), LSSC has modestly improved performance (0.2 dB in PSNR) when modeling 12× 12 patches (Mairal et al., 2009).",5.1. Image Denoising,[0],[0]
HDP models of larger patches are a promising research area.,5.1. Image Denoising,[0],[0]
"While many image processing systems are designed for just one problem, our generative model is useful for many tasks.",5.2. Image Inpainting,[0],[0]
"For example, we can “inpaint” occluded image regions (like the red pixels in Fig. 7) by modifying Eq. (14) to
let σ2 →∞ for only those regions and setting σ2 = 0 elsewhere.",5.2. Image Inpainting,[0],[0]
"To process color images, we follow the approach of FoE and EPLL and convert to the YCbCr color space before independently inpainting each channel.",5.2. Image Inpainting,[0],[0]
"While ground truth is unavailable for the classic image in Fig. 7, our gridbased HDP produces fewer visual artifacts than baselines.",5.2. Image Inpainting,[0],[0]
"We have developed a coherent Bayesian nonparametric model that, via randomly positioned grids of image patches, provides a novel statistical foundation for the popular EPLL method.",6. Conclusion,[0],[0]
We show that HDP mixture models of visual textures can grow in complexity as additional images are observed and capture the self-similarity of natural images.,6. Conclusion,[0],[0]
"Our HDP-grid image denoising and inpainting algorithms are competitive with the state-of-the-art, and our model is applicable to many other computer vision tasks.",6. Conclusion,[0],[0]
This research supported in part by NSF CAREER Award No. IIS-1349774.,Acknowledgements,[0],[0]
MCH supported in part by Oracle Labs.,Acknowledgements,[0],[0]
We propose a hierarchical generative model that captures the self-similar structure of image regions as well as how this structure is shared across image collections.,abstractText,[0],[0]
"Our model is based on a novel, variational interpretation of the popular expected patch log-likelihood (EPLL) method as a model for randomly positioned grids of image patches.",abstractText,[0],[0]
"While previous EPLL methods modeled image patches with finite Gaussian mixtures, we use nonparametric Dirichlet process (DP) mixtures to create models whose complexity grows as additional images are observed.",abstractText,[0],[0]
An extension based on the hierarchical DP then captures repetitive and self-similar structure via imagespecific variations in cluster frequencies.,abstractText,[0],[0]
We derive a structured variational inference algorithm that adaptively creates new patch clusters to more accurately model novel image textures.,abstractText,[0],[0]
"Our denoising performance on standard benchmarks is superior to EPLL and comparable to the state-ofthe-art, and we provide novel statistical justifications for common image processing heuristics.",abstractText,[0],[0]
We also show accurate image inpainting results.,abstractText,[0],[0]
From Patches to Images: A Nonparametric Generative Model,title,[0],[0]
Nearly all previous work in machine translation has been at the level of words.,1 Introduction,[0],[0]
"Aside from our intu-
∗The majority of this work was completed while the author was visiting New York University.
",1 Introduction,[0],[0]
"itive understanding of word as a basic unit of meaning (Jackendoff, 1992), one reason behind this is that sequences are significantly longer when represented in characters, compounding the problem of data sparsity and modeling long-range dependencies.",1 Introduction,[0],[0]
"This has driven NMT research to be almost exclusively word-level (Bahdanau et al., 2015; Sutskever et al., 2015).
",1 Introduction,[0],[0]
"Despite their remarkable success, word-level NMT models suffer from several major weaknesses.",1 Introduction,[0],[0]
"For one, they are unable to model rare, out-ofvocabulary words, making them limited in translating languages with rich morphology such as Czech, Finnish and Turkish.",1 Introduction,[0],[0]
"If one uses a large vocabulary to combat this (Jean et al., 2015), the complexity of training and decoding grows linearly with respect to the target vocabulary size, leading to a vicious cycle.
",1 Introduction,[0],[0]
"To address this, we present a fully character-level NMT model that maps a character sequence in a source language to a character sequence in a target language.",1 Introduction,[0],[0]
"We show that our model outperforms a baseline with a subword-level encoder on DE-EN and CS-EN, and achieves a comparable result on FI-EN and RU-EN.",1 Introduction,[0],[0]
"A purely character-level NMT model with a basic encoder was proposed as a baseline by Luong and Manning (2016), but training it was prohibitively slow.",1 Introduction,[0],[0]
"We were able to train our model at a reasonable speed by drastically reducing the length of source sentence representation using a stack of convolutional, pooling and highway layers.
",1 Introduction,[0],[0]
One advantage of character-level models is that they are better suited for multilingual translation than their word-level counterparts which require a separate word vocabulary for each language.,1 Introduction,[0],[0]
"We
ar X
iv :1
61 0.
03 01
7v 3
[ cs
.C",1 Introduction,[0],[0]
"L
] 1
3 Ju
n 20
verify this by training a single model to translate four languages (German, Czech, Finnish and Russian) to English.",1 Introduction,[0],[0]
"Our multilingual character-level model outperforms the subword-level baseline by a considerable margin in all four language pairs, strongly indicating that a character-level model is more flexible in assigning its capacity to different language pairs.",1 Introduction,[0],[0]
"Furthermore, we observe that our multilingual character-level translation even exceeds the quality of bilingual translation in three out of four language pairs, both in BLEU score metric and human evaluation.",1 Introduction,[0],[0]
This demonstrates excellent parameter efficiency of character-level translation in a multilingual setting.,1 Introduction,[0],[0]
"We also showcase our model’s ability to handle intra-sentence codeswitching while performing language identification on the fly.
",1 Introduction,[0],[0]
The contributions of this work are twofold: we empirically show that (1) we can train character-tocharacter NMT model without any explicit segmentation; and (2) we can share a single character-level encoder across multiple languages to build a multilingual translation system without increasing the model size.,1 Introduction,[0],[0]
Neural machine translation (NMT) is a recently proposed approach to machine translation that builds a single neural network which takes as an input a source sentence X =,2 Background: Attentional Neural Machine Translation,[0],[0]
"(x1, . . .",2 Background: Attentional Neural Machine Translation,[0],[0]
", xTX ) and generates its translation Y =",2 Background: Attentional Neural Machine Translation,[0],[0]
"(y1, . . .",2 Background: Attentional Neural Machine Translation,[0],[0]
", yTY ), where xt and yt′ are source and target symbols (Bahdanau et al., 2015; Sutskever et al., 2015; Luong et al., 2015; Cho et al., 2014a).",2 Background: Attentional Neural Machine Translation,[0],[0]
"Attentional NMT models have three components: an encoder, a decoder and an attention mechanism.
",2 Background: Attentional Neural Machine Translation,[0],[0]
Encoder,2 Background: Attentional Neural Machine Translation,[0],[0]
"Given a source sentence X , the encoder constructs a continuous representation that summarizes its meaning with a recurrent neural network (RNN).",2 Background: Attentional Neural Machine Translation,[0],[0]
"A bidirectional RNN is often implemented as proposed in (Bahdanau et al., 2015).",2 Background: Attentional Neural Machine Translation,[0],[0]
"A forward encoder reads the input sentence from left to right: −→ h t = −→ fenc ( Ex(xt), −→ h t−1 ) .",2 Background: Attentional Neural Machine Translation,[0],[0]
"Similarly, a backward encoder reads it from right to left: ←− h t = ←− fenc ( Ex(xt), ←− h t+1 ) , where Ex is
the source embedding lookup table, and −→ fenc and←− fenc are recurrent activation functions such as long short-term memory units (LSTMs, (Hochreiter and Schmidhuber, 1997)) or gated recurrent units (GRUs, (Cho et al., 2014b)).",2 Background: Attentional Neural Machine Translation,[0],[0]
"The encoder constructs a set of continuous source sentence representations C by concatenating the forward and backward hidden states at each timestep: C = { h1, . . .",2 Background: Attentional Neural Machine Translation,[0],[0]
",hTX } ,
",2 Background: Attentional Neural Machine Translation,[0],[0]
where ht =,2 Background: Attentional Neural Machine Translation,[0],[0]
"[−→ h t; ←− h t ] .
",2 Background: Attentional Neural Machine Translation,[0],[0]
"Attention First introduced in (Bahdanau et al., 2015), the attention mechanism lets the decoder attend more to different source symbols for each target symbol.",2 Background: Attentional Neural Machine Translation,[0],[0]
"More concretely, it computes the context vector ct′ at each decoding time step t′ as a weighted sum of the source hidden states: ct′ = ∑TX t=1",2 Background: Attentional Neural Machine Translation,[0],[0]
αt′tht.,2 Background: Attentional Neural Machine Translation,[0],[0]
"Similarly to (Chung et al., 2016; Firat et al., 2016a), each attentional weight αt′t represents how relevant the t-th source token xt is to the t′-th target token yt′ , and is computed as:
αt′t = 1 Z exp ( score ( Ey(yt′−1), st′−1,ht ))",2 Background: Attentional Neural Machine Translation,[0],[0]
", (1)
where Z = ∑TX k=1 exp ( score(Ey(yt′−1), st′−1,hk) ) is the normalization constant.",2 Background: Attentional Neural Machine Translation,[0],[0]
score() is a feedforward neural network with a single hidden layer that scores how well the source symbol xt and the target symbol yt′ match.,2 Background: Attentional Neural Machine Translation,[0],[0]
"Ey is the target embedding lookup table and st′ is the target hidden state at time t′.
Decoder Given a source context vector ct′ , the decoder computes its hidden state at time t′ as: st′ = fdec ( Ey(yt′−1), st′−1, ct′ ) .",2 Background: Attentional Neural Machine Translation,[0],[0]
"Then, a parametric function outk() returns the conditional probability of the next target symbol being k:
p(yt′ =k|y<t′ , X) = 1 Z exp ( outk ( Ey(yt′−1), st′ , ct′ ))",2 Background: Attentional Neural Machine Translation,[0],[0]
"(2) where Z is again the normalization constant:
Z = ∑ j exp ( outj(Ey(yt′−1), st′ , ct′) ) .
",2 Background: Attentional Neural Machine Translation,[0],[0]
"Training The entire model can be trained end-toend by minimizing the negative conditional log-
likelihood, which is defined as:
L = − 1 N N∑ n=1 T (n) Y∑ t=1",2 Background: Attentional Neural Machine Translation,[0],[0]
log p(yt = y (n) t |y (n) <,2 Background: Attentional Neural Machine Translation,[0],[0]
"t , X (n)),
where N is the number of sentence pairs, and X(n) and y(n)t are the source sentence and the t-th target symbol in the n-th pair, respectively.",2 Background: Attentional Neural Machine Translation,[0],[0]
The benefits of character-level translation over word-level translation are well known.,3.1 Why Character-Level?,[0],[0]
"Chung et al. (2016) present three main arguments: character level models (1) do not suffer from out-of-vocabulary issues, (2) are able to model different, rare morphological variants of a word, and (3) do not require segmentation.",3.1 Why Character-Level?,[0],[0]
"Particularly, text segmentation is highly non-trivial for many languages and problematic even for English as word tokenizers are either manually designed or trained on a corpus using an objective function that is unrelated to the translation task at hand, which makes the overall system sub-optimal.
",3.1 Why Character-Level?,[0],[0]
Here we present two additional arguments for character-level translation.,3.1 Why Character-Level?,[0],[0]
"First, a character-level translation system can easily be applied to a multilingual translation setting.",3.1 Why Character-Level?,[0],[0]
"Between European languages where the majority of alphabets overlaps, for instance, a character-level model may easily identify morphemes that are shared across different languages.",3.1 Why Character-Level?,[0],[0]
"A word-level model, however, will need a separate word vocabulary for each language, allowing no cross-lingual parameter sharing.
",3.1 Why Character-Level?,[0],[0]
"Also, by not segmenting source sentences into words, we no longer inject our knowledge of words and word boundaries into the system; instead, we encourage the model to discover an internal structure of a sentence by itself and learn how a sequence of symbols can be mapped to a continuous meaning representation.",3.1 Why Character-Level?,[0],[0]
"To address these limitations associated with wordlevel translation, a recent line of research has investigated using sub-word information.
",3.2 Related Work,[0],[0]
"Costa-Jussá and Fonollosa (2016) replaced the word-lookup table with convolutional and highway
layers on top of character embeddings, while still segmenting source sentences into words.",3.2 Related Work,[0],[0]
"Target sentences were also segmented into words, and prediction was made at word-level.
",3.2 Related Work,[0],[0]
"Similarly, Ling et al. (2015) employed a bidirectional LSTM to compose character embeddings into word embeddings.",3.2 Related Work,[0],[0]
"At the target side, another LSTM takes the hidden state of the decoder and generates the target word, character by character.",3.2 Related Work,[0],[0]
"While this system is completely open-vocabulary, it also requires offline segmentation.",3.2 Related Work,[0],[0]
"Also, characterto-word and word-to-character LSTMs significantly slow down training.
",3.2 Related Work,[0],[0]
"Most recently, Luong and Manning (2016) proposed a hybrid scheme that consults character-level information whenever the model encounters an outof-vocabulary word.",3.2 Related Work,[0],[0]
"As a baseline, they also implemented a purely character-level NMT model with 4 layers of unidirectional LSTMs with 512 cells, with attention over each character.",3.2 Related Work,[0],[0]
"Despite being extremely slow (approximately 3 months to train), the character-level model gave comparable performance to the word-level baseline.",3.2 Related Work,[0],[0]
"This shows the possibility of fully character-level translation.
",3.2 Related Work,[0],[0]
Having a word-level decoder restricts the model to only being able to generate previously seen words.,3.2 Related Work,[0],[0]
Sennrich et al. (2015) introduced a subword-level NMT model that is capable of open-vocabulary translation using subword-level segmentation based on the byte pair encoding (BPE) algorithm.,3.2 Related Work,[0],[0]
"Starting from a character vocabulary, the algorithm identifies frequent character n-grams in the training data and iteratively adds them to the vocabulary, ultimately giving a subword vocabulary which consists of words, subwords and characters.",3.2 Related Work,[0],[0]
"Once the segmentation rules have been learned, their model performs subword-to-subword translation (bpe2bpe) in the same way as word-to-word translation.
",3.2 Related Work,[0],[0]
"Perhaps the work that is closest to our end goal is (Chung et al., 2016), which used a subword-level encoder from (Sennrich et al., 2015) and a fully character-level decoder (bpe2char).",3.2 Related Work,[0],[0]
Their results show that character-level decoding performs better than subword-level decoding.,3.2 Related Work,[0],[0]
"Motivated by this work, we aim for fully character-level translation at both sides (char2char).
",3.2 Related Work,[0],[0]
"Outside NMT, our work is based on a few existing approaches that applied convolutional networks
to text, most notably in text classification (Zhang et al., 2015; Xiao and Cho, 2016).",3.2 Related Work,[0],[0]
"Also, we drew inspiration for our multilingual models from previous work that showed the possibility of training a single recurrent model for multiple languages in domains other than translation (Tsvetkov et al., 2016; Gillick et al., 2015).",3.2 Related Work,[0],[0]
"Sentences are on average 6 (DE, CS and RU) to 8 (FI) times longer when represented in characters.",3.3 Challenges,[0],[0]
"This poses three major challenges to achieving fully character-level translation.
",3.3 Challenges,[0],[0]
"(1) Training/decoding latency For the decoder, although the sequence to be generated is much longer, each character-level softmax operation costs considerably less compared to a word- or subwordlevel softmax.",3.3 Challenges,[0],[0]
"Chung et al. (2016) report that character-level decoding is only 14% slower than subword-level decoding.
",3.3 Challenges,[0],[0]
"On the other hand, computational complexity of the attention mechanism grows quadratically with respect to the sentence length, as it needs to attend to every source token for every target token.",3.3 Challenges,[0],[0]
"This makes a naive character-level approach, such as in (Luong and Manning, 2016), computationally prohibitive.",3.3 Challenges,[0],[0]
"Consequently, reducing the length of the source sequence is key to ensuring reasonable speed in both training and decoding.
",3.3 Challenges,[0],[0]
"(2) Mapping character sequence to continuous representation The arbitrary relationship between the orthography of a word and its meaning is a well-known problem in linguistics (de Saussure, 1916).",3.3 Challenges,[0],[0]
"Building a character-level encoder is arguably a more difficult problem, as the encoder needs to learn a highly non-linear function from a long sequence of character symbols to a meaning representation.
",3.3 Challenges,[0],[0]
(3) Long range dependencies in characters A character-level encoder needs to model dependencies over longer timespans than a word-level encoder does.,3.3 Challenges,[0],[0]
We design an encoder that addresses all the challenges discussed above by using convolutional and pooling layers aggressively to both (1) drastically shorten the input sentence and (2) efficiently capture local regularities.,4.1 Encoder,[0],[0]
"Inspired by the character-level language model from (Kim et al., 2015), our encoder first reduces the source sentence length with a series of convolutional, pooling and highway layers.",4.1 Encoder,[0],[0]
"The shorter representation, instead of the full character sequence, is passed through a bidirectional GRU to (3) help it resolve long term dependencies.",4.1 Encoder,[0],[0]
"We illustrate the proposed encoder in Figure 1 and discuss each layer in detail below.
",4.1 Encoder,[0],[0]
"Embedding We map the sequence of source characters (x1, . . .",4.1 Encoder,[0],[0]
", xTx) to a sequence of character embeddings of dimensionality dc: X = (C(x1), . . .",4.1 Encoder,[0],[0]
",C(xTx)) ∈ Rdc×Tx where Tx is the number of source characters and C is the character embedding lookup table: C ∈ Rdc×|C|.
",4.1 Encoder,[0],[0]
Convolution One-dimensional convolution operation is then used along consecutive character embeddings.,4.1 Encoder,[0],[0]
"Assuming we have a single filter f ∈ Rdc×w of width w, we first apply padding to the beginning and the end of X , such that the padded sentence X ′ ∈ Rdc×(Tx+w−1) is w − 1 symbols longer.",4.1 Encoder,[0],[0]
"We then apply narrow convolution between X ′ and f such that the k-th element of the output Yk is given as:
Yk = (X ′ ∗ f)k = ∑ i,j (X ′[:,k−w+1:k] ⊗ f)ij , (3)
where ⊗ denotes elementwise matrix multiplication and ∗ is the convolution operation.",4.1 Encoder,[0],[0]
"X ′[:,k−w+1:k] is the sliced subset of X ′",4.1 Encoder,[0],[0]
that contains all the rows but only w adjacent columns.,4.1 Encoder,[0],[0]
"The padding scheme employed above, commonly known as half convolution, ensures the length of the output is identical to the input’s: Y ∈ R1×Tx .
",4.1 Encoder,[0],[0]
We just illustrated how a single convolutional filter of fixed width might be applied to a sentence.,4.1 Encoder,[0],[0]
"In order to extract informative character patterns of different lengths, we employ a set of filters of varying widths.",4.1 Encoder,[0],[0]
"More concretely, we use a filter
bank F = {f1, . . .",4.1 Encoder,[0],[0]
", fm} where fi = Rdc×i×ni is a collection of ni filters of width i.",4.1 Encoder,[0],[0]
"Our model uses m = 8, hence extracts character n-grams up to 8 characters long.",4.1 Encoder,[0],[0]
"Outputs from all the filters are stacked upon each other, giving a single representation Y ∈ RN×Tx , where the dimensionality of each column is given by the total number of filters N = ∑m i=1 ni.",4.1 Encoder,[0],[0]
"Finally, rectified linear activation (ReLU) is applied elementwise to this representation.
",4.1 Encoder,[0],[0]
"Max pooling with stride The output from the convolutional layer is first split into segments of width s, and max-pooling over time is applied to each segment with no overlap.",4.1 Encoder,[0],[0]
This procedure selects the most salient features to give a segment embedding.,4.1 Encoder,[0],[0]
Each segment embedding is a summary of meaningful character n-grams occurring in a particular (overlapping) subsequence in the source sentence.,4.1 Encoder,[0],[0]
Note that the rightmost segment (above ‘on’) in Figure 1 may capture ‘son’ (the filter in green) although ‘s’ occurs in the previous segment.,4.1 Encoder,[0],[0]
"In other words, our segments are overlapping as opposed to in word- or subword-level models with hard segmentation.
",4.1 Encoder,[0],[0]
"Segments act as our internal linguistic unit from this layer and above: the attention mechanism, for instance, attends to each source segment instead of source character.",4.1 Encoder,[0],[0]
This shortens the source representation s-fold: Y ′ ∈ RN×(Tx/s).,4.1 Encoder,[0],[0]
"Empirically, we found using smaller s leads to better performance
at increased training time.",4.1 Encoder,[0],[0]
"We chose s = 5 in our experiments as it gives a reasonable balance between the two.
Highway network A sequence of segment embeddings from the max pooling layer is fed into a highway network (Srivastava et al., 2015).",4.1 Encoder,[0],[0]
"Highway networks are shown to significantly improve the quality of a character-level language model when used with convolutional layers (Kim et al., 2015).",4.1 Encoder,[0],[0]
"A highway network transforms input x with a gating mechanism that adaptively regulates information flow:
y = g ReLU(W1x+ b1) +",4.1 Encoder,[0],[0]
(1− g),4.1 Encoder,[0],[0]
"x,
where g = σ((W2x + b2)).",4.1 Encoder,[0],[0]
"We apply this to each segment embedding individually.
",4.1 Encoder,[0],[0]
"Recurrent layer Finally, the output from the highway layer is given to a bidirectional GRU from §2, using each segment embedding as input.
",4.1 Encoder,[0],[0]
"Subword-level encoder Unlike a subword-level encoder, our model does not commit to a specific choice of segmentation; it is instead trained to consider every possible character pattern and extract only the most meaningful ones.",4.1 Encoder,[0],[0]
"Therefore, the definition of segmentation in our model is dynamic unlike subword-level encoders.",4.1 Encoder,[0],[0]
"During training, the model finds the most salient character patterns in a sentence via max-pooling, and the character
sequences extracted by the model change over the course of training.",4.1 Encoder,[0],[0]
This is in contrast to how BPE segmentation rules are learned: the segmentation is learned and fixed before training begins.,4.1 Encoder,[0],[0]
"Similarly to the attention model in (Chung et al., 2016; Firat et al., 2016a), a single-layer feedforward network computes the attention score of next target character to be generated with every source segment representation.",4.2 Attention and Decoder,[0],[0]
A standard two-layer character-level decoder then takes the source context vector from the attention mechanism and predicts each target character.,4.2 Attention and Decoder,[0],[0]
This decoder was described as base decoder by Chung et al. (2016).,4.2 Attention and Decoder,[0],[0]
"We evaluate the proposed character-to-character (char2char) translation model against subwordlevel baselines (bpe2bpe and bpe2char) on the WMT’15 DE→EN, CS→EN, FI→EN and RU→EN translation tasks.1 We do not consider word-level models, as it has already been shown that subword-level models outperform them by mitigating issues inherent to closed-vocabulary translation (Sennrich et al., 2015; Sennrich et al., 2016).",5.1 Task and Models,[0],[0]
"Indeed, subword-level NMT models have been the de-facto state-of-the-art and are now used in a very large-scale industry NMT system to serve millions of users per day (Wu et al., 2016).
",5.1 Task and Models,[0],[0]
"1http://www.statmt.org/wmt15/translation -task.html
",5.1 Task and Models,[0],[0]
We experiment in two different scenarios: 1) a bilingual setting where we train a model on data from a single language pair; and 2) a multilingual setting where the task is many-to-one translation: we train a single model on data from all four language pairs.,5.1 Task and Models,[0],[0]
"Hence, our baselines and models are:
(a) bilingual bpe2bpe: from (Firat et al., 2016a).",5.1 Task and Models,[0],[0]
"(b) bilingual bpe2char: from (Chung et al., 2016).",5.1 Task and Models,[0],[0]
"(c) bilingual char2char (d) multilingual bpe2char (e) multilingual char2char
We train all the models ourselves other than (a), for which we report the results from (Firat et al., 2016a).",5.1 Task and Models,[0],[0]
We detail the configuration of our models in Table 1 and Table 2.,5.1 Task and Models,[0],[0]
"We use all available parallel data on the four language pairs from WMT’15: DE-EN, CS-EN, FI-EN and RU-EN.
",5.2 Datasets and Preprocessing,[0],[0]
"For the bpe2char baselines, we only use sentence pairs where the source is no longer than 50 subword symbols.",5.2 Datasets and Preprocessing,[0],[0]
"For our char2char models, we only use pairs where the source sentence is no longer than 450 characters.",5.2 Datasets and Preprocessing,[0],[0]
"For all the language pairs apart from FI-EN, we use newstest-2013 as a development set and newstest-2014 and newstest-2015 as test sets.",5.2 Datasets and Preprocessing,[0],[0]
"For FI-EN, we use newsdev-2015 and newstest-2015 as development and test sets respectively.",5.2 Datasets and Preprocessing,[0],[0]
"We tokenize2 each corpus using the script from Moses.3
When training bilingual bpe2char models, we extract 20,000 BPE operations from each of the source and target corpus using a script from (Sennrich et al., 2015).",5.2 Datasets and Preprocessing,[0],[0]
This gives a source BPE vocabulary of size 20k−24k for each language.,5.2 Datasets and Preprocessing,[0],[0]
"Each model is trained using stochastic gradient descent and Adam (Kingma and Ba, 2014) with learning rate 0.0001 and minibatch size 64.",5.3 Training Details,[0],[0]
"Training continues until the BLEU score on the validation set
2This is unnecessary for char2char models, yet was carried out for comparison.
",5.3 Training Details,[0],[0]
"3https://github.com/moses-smt/mosesdecod er
stops improving.",5.3 Training Details,[0],[0]
"The norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2013).",5.3 Training Details,[0],[0]
All weights are initialized from a uniform distribution,5.3 Training Details,[0],[0]
"[−0.01, 0.01].
",5.3 Training Details,[0],[0]
Each model is trained on a single pre-2016 GTX Titan X GPU with 12GB RAM.,5.3 Training Details,[0],[0]
"As from (Chung et al., 2016), a two-layer unidirectional character-level decoder with 1024 GRU units is used for all our experiments.",5.4 Decoding Details,[0],[0]
"For decoding, we use beam search with length-normalization to penalize shorter hypotheses.",5.4 Decoding Details,[0],[0]
The beam width is 20 for all models.,5.4 Decoding Details,[0],[0]
"Task description We train a model on a many-toone translation task to translate a sentence in any of the four languages (German, Czech, Finnish and Russian) to English.",5.5 Training Multilingual Models,[0],[0]
"We do not provide a language identifier to the encoder, but merely the sentence itself, encouraging the model to perform language identification on the fly.",5.5 Training Multilingual Models,[0],[0]
"In addition, by not providing the language identifier, we expect the model to handle intra-sentence code-switching seamlessly.
",5.5 Training Multilingual Models,[0],[0]
"Model architecture The multilingual char2char model uses slightly more convolutional filters than the bilingual char2char model, namely (200-250- 300-300-400-400-400-400).",5.5 Training Multilingual Models,[0],[0]
"Otherwise, the architecture remains the same as shown in Table 1.",5.5 Training Multilingual Models,[0],[0]
"By not changing the size of the encoder and the decoder, we fix the capacity of the core translation module, and only allow the multilingual model to detect more character patterns.
",5.5 Training Multilingual Models,[0],[0]
"Similarly, the multilingual bpe2char model has the same encoder and decoder as the bilingual bpe2char model, but a larger vocabulary.",5.5 Training Multilingual Models,[0],[0]
"We learn 50,000 multilingual BPE operations on the multilingual corpus, resulting in 54,544 subwords.",5.5 Training Multilingual Models,[0],[0]
"See Table 2 for the exact configuration of our multilingual models.
",5.5 Training Multilingual Models,[0],[0]
"Data scheduling For the multilingual models, an appropriate scheduling of data from different languages is crucial to avoid overfitting to one language too soon.",5.5 Training Multilingual Models,[0],[0]
"Following (Firat et al., 2016a; Firat et al., 2016b), each minibatch is balanced, in that the proportion of each language pair in a single minibatch corresponds to that of the full corpus.",5.5 Training Multilingual Models,[0],[0]
"With this minibatch scheme, roughly the same number of updates is required to make one full pass over the entire training corpus of each language pair.",5.5 Training Multilingual Models,[0],[0]
Minibatches from all language pairs are combined and presented to the model as a single minibatch.,5.5 Training Multilingual Models,[0],[0]
"See Table 3 for the minibatch size for each language pair.
",5.5 Training Multilingual Models,[0],[0]
"Treatment of Cyrillic To facilitate cross-lingual parameter sharing, we convert every Cyrillic character in the Russian source corpus to Latin alphabet according to ISO-9.",5.5 Training Multilingual Models,[0],[0]
"Table 4 shows an example of how this conversion may help the multilingual models identify lexemes that are shared across multiple languages.
",5.5 Training Multilingual Models,[0],[0]
"Multilingual BPE For the multilingual bpe2char model, multilingual BPE segmentation rules are extracted from a large dataset containing training source corpora of all the language pairs.",5.5 Training Multilingual Models,[0],[0]
"To ensure the BPE rules are not biased towards one language,
larger datasets such as Czech and German corpora are trimmed such that every corpus contains an approximately equal number of characters.",5.5 Training Multilingual Models,[0],[0]
"In this section, we first establish our main hypotheses for introducing character-level and multilingual models, and investigate whether our observations support or disagree with our hypotheses.",6.1 Evaluation with BLEU Score,[0],[0]
"From our empirical results, we want to verify: (1) if fully character-level translation outperforms subwordlevel translation, (2) in which setting and to what extent is multilingual translation beneficial and (3) if multilingual, character-level translation achieves superior performance to other models.",6.1 Evaluation with BLEU Score,[0],[0]
"We outline our results with respect to each hypothesis below.
",6.1 Evaluation with BLEU Score,[0],[0]
(1) Character-,6.1 Evaluation with BLEU Score,[0],[0]
"vs. subword-level In a bilingual setting, the char2char model outperforms both subword-level baselines on DE-EN (Table 5 (a-c)) and CS-EN (Table 5 (f-h)).",6.1 Evaluation with BLEU Score,[0],[0]
"On the other two language pairs, it exceeds the bpe2bpe model and achieves similar performance with the bpe2char baseline (Table 5 (k-m) and (p-r)).",6.1 Evaluation with BLEU Score,[0],[0]
"We conclude that
the proposed character-level model is comparable to or better than both subword-level baselines.
",6.1 Evaluation with BLEU Score,[0],[0]
"Meanwhile, in a multilingual setting, the character-level encoder significantly surpasses the subword-level encoder consistently in all the language pairs (Table 5 (d-e), (i-j), (n-o) and (s-t)).",6.1 Evaluation with BLEU Score,[0],[0]
"From this, we conclude that translating at the level of characters allows the model to discover shared constructs between languages more effectively.",6.1 Evaluation with BLEU Score,[0],[0]
"This also demonstrates that the character-level model is more flexible in assigning model capacity to different language pairs.
(2) Multilingual vs. bilingual At the level of characters, we note that multilingual translation is indeed strongly beneficial.",6.1 Evaluation with BLEU Score,[0],[0]
"On the test sets, the multilingual character-level model outperforms the singlepair character-level model by 2.64 BLEU in FI-EN (Table 5 (m, o)) and 0.78 BLEU in CS-EN (Table 5 (h, j)), while achieving comparable results on DE-EN and RU-EN.
",6.1 Evaluation with BLEU Score,[0],[0]
"At the level of subwords, on the other hand, we do not observe the same degree of performance benefit from multilingual translation.",6.1 Evaluation with BLEU Score,[0],[0]
"Also, the multilingual bpe2char model requires much more updates to reach the performance of the bilingual
bpe2char model (see Figure 2).",6.1 Evaluation with BLEU Score,[0],[0]
"This suggests that learning useful subword segmentation across languages is difficult.
(3) Multilingual char2char vs. others The multilingual char2char model is the best performer in CS-EN, FI-EN and RU-EN (Table 5 (j, o, t)), and is the runner-up in DE-EN (Table 5 (e)).",6.1 Evaluation with BLEU Score,[0],[0]
"The fact that the multilingual char2char model outperforms the single-pair models goes to show the parameter efficiency of character-level translation: instead of training N separate models for N language pairs, it is possible to get better performance with a single multilingual character-level model.",6.1 Evaluation with BLEU Score,[0],[0]
"It is well known that automatic evaluation metrics such as BLEU encourage reference-like translations and do not fully capture true translation quality (Callison-Burch, 2009; Graham et al., 2015).",6.2 Human Evaluation,[0],[0]
"Therefore, we also carry out a recently proposed evaluation from (Graham et al., 2016) where we have human assessors rate both (1) adequacy and (2) fluency of each system translation on a scale from 0 to 100 via Amazon Mechanical Turk.",6.2 Human Evaluation,[0],[0]
Adequacy is the degree to which assessors agree that the system translation expresses the meaning of the reference translation.,6.2 Human Evaluation,[0],[0]
"Fluency is evaluated using system translation alone without any reference translation.
",6.2 Human Evaluation,[0],[0]
Approximately 1k turkers assessed a single test set (3k sentences in newstest-2014) for each system and language pair.,6.2 Human Evaluation,[0],[0]
"Each turker conducted a minimum of 100 assessments for quality control, and the set of scores generated by each turker was standardized to remove any bias in the individual’s scoring strategy.
",6.2 Human Evaluation,[0],[0]
"We consider three models (bilingual bpe2char, bilingual char2char and multilingual char2char) for the human evaluation.",6.2 Human Evaluation,[0],[0]
"We leave out the multilingual bpe2char model to minimize the number of similar systems to improve the interpretability of the evaluation overall.
",6.2 Human Evaluation,[0],[0]
"For DE-EN, we observe that the multilingual char2char and bilingual char2char models are tied with respect to both adequacy and fluency (Table 6 (b-c)).",6.2 Human Evaluation,[0],[0]
"For CS-EN, the multilingual char2char and bilingual bpe2char models ared tied for adequacy.",6.2 Human Evaluation,[0],[0]
"However, the multilingual char2char model yields significantly better fluency (Table 6 (d, f)).",6.2 Human Evaluation,[0],[0]
"For FI-EN and RU-EN, the multilingual char2char model is tied with the bilingual char2char model with respect to adequacy, but significantly outperforms all other models in fluency (Table 6 (g-i, j-l)).
",6.2 Human Evaluation,[0],[0]
"Overall, the improvement in translation quality yielded by the multilingual character-level model mainly comes from fluency.",6.2 Human Evaluation,[0],[0]
"We conjecture that because the English decoder of the multilingual model is tuned on all the training sentence pairs, it becomes
a better language model than a bilingual model’s decoder.",6.2 Human Evaluation,[0],[0]
We leave it for future work to confirm if this is indeed the case.,6.2 Human Evaluation,[0],[0]
"In Table 7, we demonstrate our character-level model’s robustness in four translation scenarios that conventional NMT systems are known to suffer in.",7 Qualitative Analysis,[0],[0]
"We also showcase our model’s ability to seamlessly handle intra-sentence code-switching, or mixed utterances from two or more languages.",7 Qualitative Analysis,[0],[0]
"We compare
sample translations from the character-level model with those from the subword-level model, which already sidesteps some of the issues associated with word-level translation.
",7 Qualitative Analysis,[0],[0]
"With real-world text containing typos and spelling mistakes, the quality of word-based translation would severely drop, as every non-canonical form of a word cannot be represented.",7 Qualitative Analysis,[0],[0]
"On the other hand, a character-level model has a much better chance recovering the original word or sentence.",7 Qualitative Analysis,[0],[0]
"Indeed, our char2char model is robust against a few spelling
mistakes (Table 7 (a)).",7 Qualitative Analysis,[0],[0]
"Given a long, rare word such as “Siebentausendzweihundertvierundfünfzig” (seven thousand two hundred fifty four) in Table 7 (b), the subword-level model segments “Siebentausend” as (Sieb, ent, aus, end), which results in an inaccurate translation.",7 Qualitative Analysis,[0],[0]
"The character-level model performs better on these long, concatenative words with ambiguous segmentation.
",7 Qualitative Analysis,[0],[0]
"Also, we expect a character-level model to handle novel and unseen morphological inflections well.",7 Qualitative Analysis,[0],[0]
"We observe that this is indeed the case, as our char2char model correctly understands “gesperrt”, a past participle form of “sperren” (to block) (Table 7 (c)).
",7 Qualitative Analysis,[0],[0]
Nonce words are terms coined for a single use.,7 Qualitative Analysis,[0],[0]
"They are not actual words but are constructed in a way that humans can intuitively guess what they mean, such as workoliday and friyay.",7 Qualitative Analysis,[0],[0]
"We construct a few DE-EN sentence pairs that contain German nonce words (one example shown in Table 7 (d)), and observe that the character-level model can indeed detect salient character patterns and arrive at a correct translation.
",7 Qualitative Analysis,[0],[0]
"Finally, we evaluate our multilingual models’ capacity to perform intra-sentence code-switching, by giving them as input mixed sentences from multiple languages.",7 Qualitative Analysis,[0],[0]
"The newstest-2013 development datasets for DE-EN, CS-EN and FI-EN contain intersecting examples with the same English sentences.",7 Qualitative Analysis,[0],[0]
"We compile a list of these sentences in DE/CS/FI and their translation in EN, and choose a few samples uniformly at random from the English side.",7 Qualitative Analysis,[0],[0]
"Words or clauses from different languages are manually intermixed to create multilingual sentences.
",7 Qualitative Analysis,[0],[0]
"We discover that when given sentences with high degree of language intermixing, as in Table 7 (e), the multilingual bpe2char model fails to seamlessly handle alternation of languages.",7 Qualitative Analysis,[0],[0]
"Overall, however, both multilingual models generate reasonable translations.",7 Qualitative Analysis,[0],[0]
"This is possible because we did not provide a language identifier when training our multilingual models; as a result, they learned to understand a multilingual sentence and translate it into a coherent English sentence.",7 Qualitative Analysis,[0],[0]
"We show supplementary sample translations in each scenario on a webpage.4
4https://sites.google.com/site/dl4mtc2c
Training and decoding speed On a single Titan X GPU, we observe that our char2char models are approximately 35% slower to train than our bpe2char baselines when the same batch size was used.",7 Qualitative Analysis,[0],[0]
"Our bilingual character-level models can be trained in roughly two weeks.
",7 Qualitative Analysis,[0],[0]
"We further note that the bilingual bpe2char model can translate 3,000 sentences in 66.63 minutes while the bilingual char2char model requires 71.71 minutes (online, not in batch).",7 Qualitative Analysis,[0],[0]
"See Table 8 for the exact details.
",7 Qualitative Analysis,[0],[0]
Further observations We also note that the multilingual models are less prone to overfitting than the bilingual models.,7 Qualitative Analysis,[0],[0]
This is particularly visible for low-resource language pairs such as FI-EN.,7 Qualitative Analysis,[0],[0]
Figure 2 shows the evolution of the FI-EN validation BLEU scores where the bilingual models overfit rapidly but the multilingual models seem to regularize learning by training simultaneously on other language pairs.,7 Qualitative Analysis,[0],[0]
We propose a fully character-level NMT model that accepts a sequence of characters in the source language and outputs a sequence of characters in the target language.,8 Conclusion,[0],[0]
"What is remarkable about this model is the absence of explicitly hard-coded knowledge of words and their boundaries, and that the model learns these concepts from a translation task alone.
",8 Conclusion,[0],[0]
"Our empirical results show that the fully character-level model performs as well as, or better than, subword-level translation models.",8 Conclusion,[0],[0]
"The performance gain is distinctly pronounced in the multilingual many-to-one translation task, where results show that character-level model can assign model capacities to different languages more efficiently than the subword-level models.",8 Conclusion,[0],[0]
"We observe a particularly large improvement in FI-EN translation when the model is trained to translate multiple languages, indicating positive cross-lingual transfer to a lowresource language pair.
",8 Conclusion,[0],[0]
We discover two main benefits of the multilingual character-level model: (1) it is much more parameter efficient than the bilingual models and (2) it can naturally handle intra-sentence code-switching as a result of the many-to-one translation task.,8 Conclusion,[0],[0]
"Ultimately, we present a case for fully character-level translation: that translation at the level of character is strongly beneficial and should be encouraged more.
",8 Conclusion,[0],[0]
"The repository https://github.com/nyu-dl /dl4mt-c2c contains the source code and pretrained models for reproducing the experimental results.
",8 Conclusion,[0],[0]
"In the next stage of this research, we will investigate extending our multilingual many-to-one translation models to perform many-to-many translation, which will allow the decoder, similarly with the encoder, to learn from multiple target languages.",8 Conclusion,[0],[0]
"Furthermore, a more thorough investigation into model architectures and hyperparameters is needed.",8 Conclusion,[0],[0]
"KC thanks the support by eBay, Facebook, Google (Google Faculty Award 2016) and NVidia (NVIDIA AI Lab 2016-2019).",Acknowledgements,[0],[0]
"This work was partly supported by Samsung Advanced Institute of Technol-
ogy (Deep Learning).",Acknowledgements,[0],[0]
"JL was supported by Qualcomm Innovation Fellowship, and thanks David Yenicelik and Kevin Wallimann for their contribution in designing the qualitative analysis.",Acknowledgements,[0],[0]
"The authors would like to thank Prof. Zheng Zhang (NYU Shanghai) for fruitful discussion and comments, as well as Yvette Graham for her help with the human evaluation.",Acknowledgements,[0],[0]
"Most existing machine translation systems operate at the level of words, relying on explicit segmentation to extract tokens.",abstractText,[0],[0]
We introduce a neural machine translation (NMT) model that maps a source character sequence to a target character sequence without any segmentation.,abstractText,[0],[0]
"We employ a character-level convolutional network with max-pooling at the encoder to reduce the length of source representation, allowing the model to be trained at a speed comparable to subword-level models while capturing local regularities.",abstractText,[0],[0]
"Our character-to-character model outperforms a recently proposed baseline with a subwordlevel encoder on WMT’15 DE-EN and CSEN, and gives comparable performance on FIEN and RU-EN.",abstractText,[0],[0]
We then demonstrate that it is possible to share a single characterlevel encoder across multiple languages by training a model on a many-to-one translation task.,abstractText,[0],[0]
"In this multilingual setting, the character-level encoder significantly outperforms the subword-level encoder on all the language pairs.",abstractText,[0],[0]
"We observe that on CS-EN, FI-EN and RU-EN, the quality of the multilingual character-level translation even surpasses the models specifically trained on that language pair alone, both in terms of BLEU score and human judgment.",abstractText,[0],[0]
Fully Character-Level Neural Machine Translation without Explicit Segmentation,title,[0],[0]
"Deep neural networks have achieved great success in classification tasks; in particular, residual network (ResNet) (He et al., 2016) and its variants such as wide-ResNet (Zagoruyko & Komodakis, 2016), ResNeXt (Xie et al., 2017), and DenseNet (Huang et al., 2017b) have become the most prominent architectures in computer vision.",1. Introduction,[0],[0]
"Thus, to reveal a factor in their success, several studies have explored the behavior of ResNets and some promising perceptions have been advocated.",1. Introduction,[0],[0]
"Concerning the behavior of ResNets, there are mainly two types of thoughts.",1. Introduction,[0],[0]
"One is the ensemble views, which were pointed out in Veit et al. (2016); Littwin
1Graduate School of Information Science and Technology, The University of Tokyo 2Center for Advanced Intelligence Project, RIKEN.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Atsushi Nitanda <atsushi nitanda@mist.i.u-tokyo.ac.jp>, Taiji Suzuki <taiji@mist.i.u-tokyo.ac.jp>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
& Wolf (2016).",1. Introduction,[0],[0]
They presented that ResNets are ensemble of shallower models using an unraveled view of ResNets.,1. Introduction,[0],[0]
"Moreover, Veit et al. (2016) enhanced their claim by showing that dropping or shuffling residual blocks does not affect the performance of ResNets experimentally.",1. Introduction,[0],[0]
The other is the optimization or ordinary differential equation views.,1. Introduction,[0],[0]
"In Jastrzebski et al. (2017), it was observed experimentally that ResNet layers iteratively move data representations along the negative gradient of the loss function with respect to hidden representations.",1. Introduction,[0],[0]
"Moreover, several studies (Weinan, 2017; Haber et al., 2017; Chang et al., 2017a;b; Lu et al., 2017) have pointed out that ResNet layers can be regarded as discretization steps of ordinary differential equations.",1. Introduction,[0],[0]
"Since optimization methods are constructed based on the discretization of gradient flows, these studies are closely related to each other.
",1. Introduction,[0],[0]
"On the other hand, gradient boosting (Mason et al., 1999; Friedman, 2001) is known to be a state-of-the-art method in data analysis; in particular, XGBoost (Chen & Guestrin, 2016) and LightGBM (Ke et al., 2017) are notable because of their superior performance.",1. Introduction,[0],[0]
"Although ResNets and gradient boosting are prominent methods in different domains, we notice an interesting similarity by recalling that gradient boosting is an ensemble method based on iterative refinement by functional gradients for optimizing predictors.",1. Introduction,[0],[0]
"However, there is a key difference between ResNets and gradient boosting methods.",1. Introduction,[0],[0]
"While gradient boosting directly updates the predictor, ResNets iteratively optimize the feature extraction by stacking ResNet layers rather than the predictor, according to the existing work.
",1. Introduction,[0],[0]
"In this paper, leveraging this observation, we propose a new gradient boosting method called ResFGB for classification tasks based on ResNet perception, that is, the feature extraction gradually grows by functional gradient methods in the space of feature extractions and the resulting predictor naturally forms a ResNet-type architecture.",1. Introduction,[0],[0]
The expected benefit of the proposed method over usual gradient boosting methods is that functional gradients with respect to feature extraction can learn a deep model rather than a shallow model like usual gradient boosting.,1. Introduction,[0],[0]
"As a result, more efficient optimization is expected.
",1. Introduction,[0],[0]
"In the theoretical analysis of the proposed method, we first formalize the gradient boosting perspective of ResNet math-
ematically using the notion of functional gradients in the space of feature extractions.",1. Introduction,[0],[0]
"That is, we explain that optimization in that space is achieved by stacking ResNet layers.",1. Introduction,[0],[0]
"We next show a good consistency property of the functional gradient, which motivates us to find feature extraction with small functional gradient norms for estimating the correct label of data.",1. Introduction,[0],[0]
This fact is very helpful from the optimization perspective because minimizing the gradient norm is much easier than minimizing the objective function without strong convexity.,1. Introduction,[0],[0]
"Moreover, we show the margin maximization property of the proposed method and derive the margin bound by utilizing this formalization and the standard complexity analysis techniques developed in Koltchinskii & Panchenko (2002); Bartlett & Mendelson (2002), which guarantee the generalization ability of the method.",1. Introduction,[0],[0]
"As for another generalization guarantee, we also provide convergence analysis of the sample-splitting variant of the method for the expected risk minimization.",1. Introduction,[0],[0]
"We finally show superior performance, empirically, of the proposed method over state-of-the-art methods including LightGBM.
",1. Introduction,[0],[0]
Related work.,1. Introduction,[0],[0]
Several studies have attempted to grow neural networks sequentially based on the boosting theory.,1. Introduction,[0],[0]
"Bengio et al. (2006) introduced convex neural networks consisting of a single hidden layer, and proposed a gradient boosting-based method in which linear classifiers are incrementally added with their weights.",1. Introduction,[0],[0]
"However, the expressive power of the convex neural network is somewhat limited because the method cannot learn deep architectures.",1. Introduction,[0],[0]
"Moghimi et al. (2016) proposed boosted convolutional neural networks and showed superior empirical performance on fine-grained classification tasks, where convolutional neural networks are iteratively added, while our method constructs a deeper network by iteratively adding layers.",1. Introduction,[0],[0]
"Cortes et al. (2017) proposed AdaNet to adaptively learn both the structure of the network and its weight, and provided data-dependent generalization guarantees for an adaptively learned network; however, the learning strategy quite differs from our method and the convergence rate is unclear.",1. Introduction,[0],[0]
"The most related work is BoostResNet (Huang et al., 2017a), which constructs ResNet iteratively like our method; however, this method is based on an different theory rather than functional gradient boosting with a constant learning rate.",1. Introduction,[0],[0]
This distinction makes the different optimizationgeneralization tradeoff.,1. Introduction,[0],[0]
"Indeed, our method exhibits a tradeoff with respect to the learning rate, which recalls perception of usual functional gradient boosting methods, namely a smaller learning rate leads to a good generalization performance.",1. Introduction,[0],[0]
"In this section, we provide several notations and describe a problem setting of the classification.",2. Preliminary,[0],[0]
"An important notion in
this paper is the functional gradient, which is also introduced in this section.
",2. Preliminary,[0],[0]
Problem setting.,2. Preliminary,[0],[0]
"Let X = Rd and Y be a feature space and a finite label set of cardinal c, respectively.",2. Preliminary,[0],[0]
"We denote by ν a true Borel probability measure on X × Y and by νn an empirical probability measure of samples (xi, yi)
n i=1",2. Preliminary,[0],[0]
"independently drawn from ν, i.e., dνn(X,Y )",2. Preliminary,[0],[0]
"=∑n
i=1 δ(xi,yi)(X,Y )dXdY/n, where δ denotes the Dirac delta function.",2. Preliminary,[0],[0]
We denote by νX the marginal distribution on X and by ν(·|X) the conditional distribution on Y .,2. Preliminary,[0],[0]
"We also denote empirical variants of these distributions by νn,X and νn(·|X).",2. Preliminary,[0],[0]
"In general, for a probability measure µ, we denote by Eµ the expectation with respect to a random variable according to µ, by L2(µ) the space of square-integrable functions with respect to µ, and by Lq2(µ) the product space of L2(µ) equipped with 〈·, ·〉Lq2(µ)-inner product: for ∀ξ,∀ζ ∈ Lq2(µ),
〈ξ, ζ〉Lq2(µ) def = Eµ[ξ(X)>ζ(X)]",2. Preliminary,[0],[0]
= Eµ  q∑ j=1 ξj(X)ζj(X)  .,2. Preliminary,[0],[0]
"We also use the following norm: for ∀p ∈ (0, 2] and ∀ξ ∈ Lq2(µ), ‖ξ‖ p Lqp(µ) def = Eµ[‖ξ(X)‖p2] =
Eµ [ ( ∑q j=1 ξ 2 j (X)) p/2 ] .
",2. Preliminary,[0],[0]
The ultimate goal in classification problems is to find a predictor f ∈ Lc2(νX) such that arg maxy∈Y fy(x) correctly classifies its label.,2. Preliminary,[0],[0]
"The quality of the predictor is measured by a loss function l(ζ, y) ≥ 0.",2. Preliminary,[0],[0]
"A typical choice of l in multiclass classification problems is l(ζ, y) =",2. Preliminary,[0],[0]
"− log(exp(ζy)/ ∑ y∈Y exp(ζy)), which is used for the multiclass logistic regression.",2. Preliminary,[0],[0]
"The goal of classification is achieved by solving the expected risk minimization problem:
min f∈Lc2(νX)
{ L(f) def=",2. Preliminary,[0],[0]
"Eν [l(f(X), Y )] } .",2. Preliminary,[0],[0]
"(1)
However, the true probability measure ν is unknown, so we approximate L using the observed data probability measure νn and solve the empirical risk minimization problems:
min f∈Lc2(νX)
{ Ln(f) def = Eνn",2. Preliminary,[0],[0]
"[l(f(X), Y )] } .",2. Preliminary,[0],[0]
"(2)
In general, some regularization is needed for the problem (2) to guarantee generalization.",2. Preliminary,[0],[0]
"In this paper, we rely on early stopping (Zhang et al., 2005) and some restriction on optimization methods for solving the problem.
",2. Preliminary,[0],[0]
"Similar to neural networks, we split the predictor f into the feature extraction and linear predictor, that is, f(x) = w>φ(x), where w ∈ Rd×c is a weight for the last layer and φ ∈ Ld2(νX) is a feature extraction from X to X .",2. Preliminary,[0],[0]
"For
simplicity, we also denote l(z, y, w) = l(w>z, y).",2. Preliminary,[0],[0]
"Usually, φ is parameterized by a neural network and optimized using the stochastic gradient method.",2. Preliminary,[0],[0]
"In this paper, we propose a way to optimize φ in Ld2(νX) via the following problem:
min w∈Rd×c φ∈Ld2(νX)
{ R(φ,w) def=",2. Preliminary,[0],[0]
"Eν [l(φ(X), Y, w)]",2. Preliminary,[0],[0]
"+ λ
2 ‖w‖22
} (3)
where λ > 0 is a regularization parameter to stabilize the optimization procedure and ‖ · ‖2 for w is a Euclidean norm.",2. Preliminary,[0],[0]
"When we focus on the problem with respect to φ, we use the notation R(φ) def= minw∈Rd×c",2. Preliminary,[0],[0]
"R(φ,w).",2. Preliminary,[0],[0]
"We also denote byRn(φ,w) andRn(φ) empirical variants ofR(φ,w) and R(φ), respectively, which are defined by replacing Eν by Eνn .",2. Preliminary,[0],[0]
"In this paper, we denote by ∂ the partial derivative and its subscript indicates the direction.
",2. Preliminary,[0],[0]
Functional gradient.,2. Preliminary,[0],[0]
The key notion used for solving the problem is the functional gradient in function spaces.,2. Preliminary,[0],[0]
"Since they are taken in some function spaces, we first introduce Fréchet differential in general Hilbert spaces.
",2. Preliminary,[0],[0]
Definition 1.,2. Preliminary,[0],[0]
"LetH be a Hilbert space and h be a function onH. For ξ ∈ H, we call that h is Fréchet differentiable at ξ inH when there exists an element ∇ξh(ξ) ∈ H such that
h(ζ) = h(ξ) +",2. Preliminary,[0],[0]
"〈∇ξh(ξ), ζ",2. Preliminary,[0],[0]
"− ξ〉H + o(‖ξ − ζ‖H).
",2. Preliminary,[0],[0]
"Moreover, for simplicity, we call∇ξh(ξ) Fréchet differential or functional gradient.
",2. Preliminary,[0],[0]
"We here make an assumption to guarantee Fréchet differentiability of R,Rn, which is valid for multiclass logistic loss: l(z, y, w) =",2. Preliminary,[0],[0]
− log(exp(w>y z)/ ∑ y∈Y exp(w > y z)).,2. Preliminary,[0],[0]
Assumption 1.,2. Preliminary,[0],[0]
"The loss function l(z, y, w) :",2. Preliminary,[0],[0]
"X × Y × Rd×c → R is a non-negative C2-function with respect to z and w, convex with respect to w, and satisfies the following smoothness: There exists a real number Ar > 0 depending on r > 0 such that
‖∂2z l(z, y, w)‖ ≤ Ar for z ∈ X , y ∈ Y, w ∈ Br(0),
where ‖ · ‖ used for a matrix ∂2z l is the spectral norm and Br(0)",2. Preliminary,[0],[0]
"⊂ Rd×c is a closed ball of center 0 and radius r.
For φ ∈ Ld2(νX), we set wφ def = arg minw∈Rd×c R(φ,w) and wn,φ def = arg minw∈Rd×c Rn(φ,w).",2. Preliminary,[0],[0]
"Moreover, we define the following notations:
∇φR(φ)(x) def = Eν(Y |x)[∂zl(φ(x), Y, wφ)],
∇φRn(φ)(x) def =
{ ∂zl(φ(xi), yi, wn,φ) (x = xi),
0 (otherwise).
",2. Preliminary,[0],[0]
"We also similarly define functional gradients ∂φR(φ,w) and ∂φRn(φ,w) for fixed w by replacing wφ, wn,φ by",2. Preliminary,[0],[0]
"w. It follows that
∇φR(φ) = ∂φR(φ,wφ),∇φRn(φ) =",2. Preliminary,[0],[0]
"∂φRn(φ,wn,φ).
",2. Preliminary,[0],[0]
"The next proposition means that the above maps are functional gradients in Ld2(νX) and L d 2(νn,X).",2. Preliminary,[0],[0]
"We set l0 = maxy∈Y l(0, y).",2. Preliminary,[0],[0]
Proposition 1.,2. Preliminary,[0],[0]
Let Assumption 1 hold.,2. Preliminary,[0],[0]
"Then, for ∀φ, ψ ∈ Ld2(νX), it follows that
R(ψ) = R(φ) + 〈∇φR(φ), ψ",2. Preliminary,[0],[0]
"− φ〉Ld2(νX) +Hφ(ψ), (4)
where Hφ(ψ) ≤",2. Preliminary,[0],[0]
"Acλ 2 ‖φ− ψ‖ 2 Ld2(νX)
",2. Preliminary,[0],[0]
"(cλ = √
2l0/λ).",2. Preliminary,[0],[0]
"Furthermore, the corresponding statements hold for R(·, w) (∀w ∈ Rd) by replacingR(·) byR(·, w) and for empirical variants by replacing νX by νn,X .
",2. Preliminary,[0],[0]
We can also show differentiability of L(f) and Ln(f).,2. Preliminary,[0],[0]
"Their functional gradients have the form ∇fL(f)(x) = Eν(Y |x)[∂ζ l(f(x), Y )] and∇fLn(f)(xi) = ∂ζ l(f(xi), yi).",2. Preliminary,[0],[0]
"In this paper, we derive functional gradient methods using ∇φRn(φ) rather than ∇fLn(f) like usual gradient boosting (Mason et al., 1999; Friedman, 2001), and provide convergence analyses for problems (1) and (2).",2. Preliminary,[0],[0]
"However, we cannot apply ∇φRn(φ) or ∂φRn(φ,w) directly to the expected risk minimization problem because these functional gradients are zero outside the training data.",2. Preliminary,[0],[0]
"Thus, we need a smoothing technique to propagate these to unseen data.",2. Preliminary,[0],[0]
The expected benefit of functional gradient methods using ∇φRn(φ) over usual gradient boosting is that the former can learn a deep model that is known to have high representational power.,2. Preliminary,[0],[0]
"Before providing a concrete algorithm description, we first explain the basic property of functional gradients and functional gradient methods.",2. Preliminary,[0],[0]
"In this section, we explain the motivation for using functional gradients for solving classification problems.",3. Basic Property of Functional Gradient,[0],[0]
"We first show the consistency of functional gradient norms, namely predicted probabilities by predictors with small norms converge to empirical/expected conditional probabilities.",3. Basic Property of Functional Gradient,[0],[0]
"We next explain the superior performance of functional gradient methods intuitively, which motivate us to use it for finding predictors with small norms.",3. Basic Property of Functional Gradient,[0],[0]
"Moreover, we explain that the optimization procedure of functional gradient methods can be realized by stacking ResNet layers iteratively on the top of feature extractions.
",3. Basic Property of Functional Gradient,[0],[0]
Consistency of functional gradient norm.,3. Basic Property of Functional Gradient,[0],[0]
"We here provide upper bounds on the gaps between true empirical/expected conditional probabilities and predicted probabilities.
",3. Basic Property of Functional Gradient,[0],[0]
Proposition 2.,3. Basic Property of Functional Gradient,[0],[0]
"Let l(ζ, y) be the loss function for the multiclass logistic regression.",3. Basic Property of Functional Gradient,[0],[0]
"Then,
‖∇fL(f)‖Lc1(νX) ≥ 1√ c ∑ y∈Y ‖ν(y|·)− pf (y|·)‖L1(νX),
‖∇fLn(f)‖Lc1(νn,X) ≥ 1√ c ∑ y∈Y ‖νn(y|·)− pf (y|·)‖L1(νn,X),
where we denote by pf (y|x) the softmax function defined by the predictor f , i.e., exp(fy(·))/ ∑ y∈Y exp(fy(·)).
",3. Basic Property of Functional Gradient,[0],[0]
"Many studies (Zhang, 2004; Steinwart, 2005; Bartlett et al., 2006) have exploited the consistency of convex loss functions for classification problems in terms of the classification error or conditional probability.",3. Basic Property of Functional Gradient,[0],[0]
"Basically, these studies used the excess empirical/expected risk to estimate the excess classification error or the gap between the true conditional probability and the predicted probability.",3. Basic Property of Functional Gradient,[0],[0]
"On the other hand, Proposition 2 argues that functional gradient norms give sufficient bounds on such gaps.",3. Basic Property of Functional Gradient,[0],[0]
"This fact is very helpful from the optimization perspective for non-strongly convex smooth problems since the excess risk always bounds the functional gradient norm by the reasonable order, but the inverse relationship does not always hold.",3. Basic Property of Functional Gradient,[0],[0]
"This means that finding a predictor with a small functional gradient is much easier than finding a small excess risk.
",3. Basic Property of Functional Gradient,[0],[0]
"Note that the latter inequality in Proposition 2 provides the lower bound on empirical classification accuracy, which is confirmed by Markov inequality as follows.
",3. Basic Property of Functional Gradient,[0],[0]
Pνn [1− pf (Y |X) ≥ 1/2] ≤ 2Eνn,3. Basic Property of Functional Gradient,[0],[0]
"[1− pf (Y |X)] ≤ 2 √ c‖∇fLn(f)‖Lc1(νn,X).
",3. Basic Property of Functional Gradient,[0],[0]
"Generally, we can derive a bound on the empirical margin distribution (Koltchinskii & Panchenko, 2002) by using the functional gradient norm in a similar way, and can obtain a generalization bound using it, as shown later.
",3. Basic Property of Functional Gradient,[0],[0]
Powerful optimization ability and connection to residual networks.,3. Basic Property of Functional Gradient,[0],[0]
"In the above discussion, we have seen that the classification problem can be reinterpreted as finding a predictor with small functional gradient norms, which may lead to reasonable convergence compared to minimizing the excess risk.",3. Basic Property of Functional Gradient,[0],[0]
"However, finding such a good predictor is still difficult because a function space is quite comprehensive, and thus, a superior optimization method is required to achieve this goal.",3. Basic Property of Functional Gradient,[0],[0]
We explain that functional gradient methods exhibit an excellent performance by using the simplified problem.,3. Basic Property of Functional Gradient,[0],[0]
"Namely, we apply the functional gradient method to the following problem:
min φ∈Ld2(νX)
{ R′(φ) def=",3. Basic Property of Functional Gradient,[0],[0]
EνX [r(φ(X))],3. Basic Property of Functional Gradient,[0],[0]
"} , (5)
where r is a sufficiently smooth function.",3. Basic Property of Functional Gradient,[0],[0]
"Note that the main problem (3) is not interpreted as this simplified problem correctly, but is useful in explaining a property and an advantage of the method and leads to a deeper understanding.
",3. Basic Property of Functional Gradient,[0],[0]
"If R′ is Fréchet differentiable, the functional gradient is represented as∇φR′(φ)(·) = ∇zr(φ(·)), where z indicates the input to r. Therefore, the negative functional gradient indicates the direction of decreasing the objective r at each point φ(x).",3. Basic Property of Functional Gradient,[0],[0]
"An iteration of the functional gradient method with a learning rate η is described as
φt+1 ← φt",3. Basic Property of Functional Gradient,[0],[0]
− η∇zr ◦ φt = (id− η∇zr),3. Basic Property of Functional Gradient,[0],[0]
"◦ φt.
",3. Basic Property of Functional Gradient,[0],[0]
"We can immediately notice that this iterate makes φt one level deeper by stacking a residual network-type layer id − η∇zr (He et al., 2016), and data representation is refined through this layer.",3. Basic Property of Functional Gradient,[0],[0]
"Starting from a simple feature extraction φ0 and running the functional gradient method for T -iterations, we finally obtain a residual network:
φT = (id− η∇zr) ◦ · · · ◦ (id− η∇zr) ◦ φ0.
",3. Basic Property of Functional Gradient,[0],[0]
"Therefore, feature extraction φ gradually grows by using the functional gradient method to optimizeR′.",3. Basic Property of Functional Gradient,[0],[0]
"Indeed, we can show the convergence of φT to a stationary point of R′ in Ld2(νX) under smoothness and boundedness assumptions by analogy with a finite-dimensional optimization method.",3. Basic Property of Functional Gradient,[0],[0]
This is a huge advantage of the functional gradient method because stationary points in Ld2(νX) are potentially significant better than those of finite-dimensional spaces.,3. Basic Property of Functional Gradient,[0],[0]
"Note that this formulation explains the optimization view (Jastrzebski et al., 2017) of ResNet mathematically.
",3. Basic Property of Functional Gradient,[0],[0]
"We now briefly explain how powerful the functional gradient method is compared to the gradient method in a finitedimensional space, for optimizingR′. Let us consider any parameterization of φt ∈ Ld2(νX).",3. Basic Property of Functional Gradient,[0],[0]
"That is, we assume that φt is contained in a family of parameterized feature extractionsM = {gθ : X → X | θ ∈",3. Basic Property of Functional Gradient,[0],[0]
Θ,3. Basic Property of Functional Gradient,[0],[0]
⊂,3. Basic Property of Functional Gradient,[0],[0]
"Rm} ⊂ Ld2(νX), i.e., ∃θ′ ∈ Θ s.t. φt = gθ′ .",3. Basic Property of Functional Gradient,[0],[0]
"Typically, the familyM is given by neural networks.",3. Basic Property of Functional Gradient,[0],[0]
"IfR′(gθ) is differentiable at θ′, we get ∇θR′(gθ)|θ=θ′ = 〈∇φR′(φt),∇θg|θ=θ′〉Ld2(νX) according to the chain rule of derivatives.",3. Basic Property of Functional Gradient,[0],[0]
Note that ∇φR′(φt) dominates the norm of gradients.,3. Basic Property of Functional Gradient,[0],[0]
"Namely, if φt is a stationary point inLd2(νX), φt is also a stationary point inM",3. Basic Property of Functional Gradient,[0],[0]
and there is no room for improvement using gradient-based methods.,3. Basic Property of Functional Gradient,[0],[0]
"This result holds for any familyM, but the inverse relation does not always hold.",3. Basic Property of Functional Gradient,[0],[0]
"This means that gradient-based methods may fail to optimizeR′ in the function space, while the functional gradient method exceeds such a limit by making a feature extraction φt deeper.",3. Basic Property of Functional Gradient,[0],[0]
"For detailed descriptions and related work in this line, we refer to Ambrosio et al. (2008); Daneri & Savaré (2010); Sonoda & Murata (2017); Nitanda & Suzuki (2017; 2018).",3. Basic Property of Functional Gradient,[0],[0]
"In this section, we provide concrete description of the proposed method.",4. Algorithm Description,[0],[0]
Let φt ∈ Ld2(νX) and wt denote t-th iterates of φ and w.,4. Algorithm Description,[0],[0]
"As mentioned above, since functional gradients ∂φRn(φt, wt+1) for the empirical risk vanish outside the training data, we need a smoothing technique to propagate these to unseen data.",4. Algorithm Description,[0],[0]
"Hence, we use the convolution Tkt,n∂φRn(φt, wt+1) of the functional gradient by using an adaptively chosen kernel function kt on X .",4. Algorithm Description,[0],[0]
"The convolution is applied element-wise as follows.
",4. Algorithm Description,[0],[0]
"Tkt,n∂φRn(φt, wt+1) def = Eνn,X",4. Algorithm Description,[0],[0]
"[∂φRn(φt, wt+1)(X)kt(X, ·)]
= 1
n n∑ i=1 ∂zl(φt(xi), yi, wt+1)kt(xi, ·).
",4. Algorithm Description,[0],[0]
"Namely, this quantity is a weighted sum of ∂φRn(φt, wt+1)(xi) by kt(xi, ·), which we also call a functional gradient.",4. Algorithm Description,[0],[0]
"In particular, we restrict the form of a kernel kt to the inner-product of a non-linear feature embedding to a finite-dimensional space by ιt : Rd → RD, that is, kt(x, x′) = ιt(φt(x))>ιt(φt(x)).",4. Algorithm Description,[0],[0]
"The requirements on the choice of ιt to guarantee the convergence are the uniform boundedness and sufficiently preserving the magnitude of the functional gradient ∂φRn(φt, wt+1).",4. Algorithm Description,[0],[0]
Let F be a given restricted class of bounded embeddings.,4. Algorithm Description,[0],[0]
"We pick up ιt from this class F by approximately solving the following problem to acquire magnitude preservation:
max ιt∈F
‖Tkt,n∂φRn(φt, wt+1)‖2kt .",4. Algorithm Description,[0],[0]
"(6)
where we define ‖Tkt,nξ‖2kt = 〈ξ, Tkt,nξ〉Ld2(νn,X) for a vector function ξ.",4. Algorithm Description,[0],[0]
Detailed conditions on ιt and an alternative problem to guarantee the convergence will be discussed later.,4. Algorithm Description,[0],[0]
"Note that due to the restriction on the form of kt, the computation of the functional gradient is compressed to the matrix-vector product.",4. Algorithm Description,[0],[0]
"Namely,
At def =
1
n n∑ i=1 ∂φRn(φt, wt+1)(xi)ιt(φt(xi))",4. Algorithm Description,[0],[0]
">,
Tkt,n∂φRn(φt, wt+1)",4. Algorithm Description,[0],[0]
"= Atιt(φt(·)).
",4. Algorithm Description,[0],[0]
"Therefore, the functional gradient method φt+1 ← φt",4. Algorithm Description,[0],[0]
"− ηTkt,n∂φRn(φt, wt+1) can be recognized as the procedure of successively stacking layers id − ηAtιt(φt(·))",4. Algorithm Description,[0],[0]
"(t ∈ {0, . . .",4. Algorithm Description,[0],[0]
", T − 1}) and obtaining a residual network.",4. Algorithm Description,[0],[0]
The entire algorithm is described in Algorithm 1.,4. Algorithm Description,[0],[0]
"Note that because a loss function l is chosen typically to be convex with respect to w, a procedure in Algorithm 1 to obtain wn,φt is easily achieved by running an efficient method for convex minimization problems.",4. Algorithm Description,[0],[0]
The notation T0 is the stopping time of iterates with respect to w.,4. Algorithm Description,[0],[0]
"That is, functional gradients ∂φRn(φt, wt+1) are computed at wt+1 = wn,φt
and correspond to ∇φRn(φt) when t < T0 and computed at an older point of w when t ≥ T0, rather than∇φRn(φt).
",4. Algorithm Description,[0],[0]
"Algorithm 1 ResFGB Input: S = (xi, yi)ni=1, initial points φ0, w0, the number of iterations T of φ, the number of iterations T0 of w, embedding class F , and learning rate η for t = 0 to T",4. Algorithm Description,[0],[0]
"− 1 do
if t < T0 then",4. Algorithm Description,[0],[0]
"wt+1 ← wn,φt = arg minw∈Rd×c Rn(φt, w) else wt+1",4. Algorithm Description,[0],[0]
← wt end if Get ιt by approximately solving (6) on S At ← 1n,4. Algorithm Description,[0],[0]
"∑n i=1 ∂zl(φt(xi), yi, wt+1)ιt(φt(xi))",4. Algorithm Description,[0],[0]
">
φt+1 ← φt",4. Algorithm Description,[0],[0]
"− ηAtιt(φt(·)) end for Return φT−1 and wT
Choice of embedding.",4. Algorithm Description,[0],[0]
We here provide policies for the choice of ιt.,4. Algorithm Description,[0],[0]
"A sufficient condition for ιt to achieve good convergence is to maintain the functional gradient norm, which is summarized below.",4. Algorithm Description,[0],[0]
Assumption 2.,4. Algorithm Description,[0],[0]
"For positive values γ, , p ≤ 2, q, and K, a function kt(x, x′) = ιt(φt(x))>ιt(φt(x)) satisfies ‖ιt(x)‖2 ≤ √ K onX , and γ‖∂φRn(φt, wt+1)‖qLdp(νn,X)− γ ≤ ‖Tkt,n∂φRn(φt, wt+1)‖2kt .
",4. Algorithm Description,[0],[0]
This assumption is a counterpart of that imposed in Mason et al. (1999).,4. Algorithm Description,[0],[0]
"The existence of ιt, not necessarily included in F , satisfying this assumption is confirmed as follows.",4. Algorithm Description,[0],[0]
We here assume that φt is a bijection that is a realistic assumption when learning rates are sufficiently small because of the inverse mapping theorem.,4. Algorithm Description,[0],[0]
"Then, since νn(·|X) = νn(·|φt(X)), functional gradients ∂φRn(φt, wt+1)(x) become the map of φt(x), so we can choose ιt such that
ιt(φt(·))",4. Algorithm Description,[0],[0]
"= ∂φRn(φt, wt+1)(·)/‖∂φRn(φt, wt+1)(·)‖2.
",4. Algorithm Description,[0],[0]
"By simple computation, we find that kt(x, x′) ≤ 1 and ‖Tkt,n∂φRn(φt, wt+1)‖2kt are lower-bounded by 1 d‖∂φRn(φt, wt+1)‖ 2 Ld1(νn,X)
.",4. Algorithm Description,[0],[0]
A detailed derivation is provided in Appendix.,4. Algorithm Description,[0],[0]
"Thus, Assumption 2 may be satisfied if an embedding class F is sufficiently large, but we note that too large F leads to overfitting.",4. Algorithm Description,[0],[0]
"Therefore, one way of choosing ιt is to approximate ∂φRn(φt, wt+1)(·)/‖∂φRn(φt, wt+1)(·)‖2 rather than maximizing (6) directly, and indeed, this procedure has been adopted in experiments.",4. Algorithm Description,[0],[0]
"In this section, we provide a convergence analysis for the proposed method.",5. Convergence Analysis,[0],[0]
All proofs are included in Appendix.,5. Convergence Analysis,[0],[0]
"For
the empirical risk minimization problem, we first show the global convergence rate, which also provides the generalization bound by combining the standard complexity analyses.",5. Convergence Analysis,[0],[0]
"Next, for the expected risk minimization problem, we describe how the size of F and the learning rate control the tradeoff between optimization speed and generalization by using the sample-splitting variant of Algorithm 1, whose detailed description will be provided later.
",5. Convergence Analysis,[0],[0]
Empirical risk minimization.,5. Convergence Analysis,[0],[0]
"Using Proposition 1, Assumption 2, and an additional assumption on wt, we can show the global convergence of Algorithm 1.",5. Convergence Analysis,[0],[0]
"The following inequality shows how functional gradients decrease the objective function, which is a direct consequence of Proposition 1.",5. Convergence Analysis,[0],[0]
"When η ≤ 1AcλK , we have
Rn(φt+1, wt+2) ≤ Rn(φt, wt+1)
",5. Convergence Analysis,[0],[0]
"− η 2 ‖Tkt,n∂φRn(φt, wt+1)‖2kt .
",5. Convergence Analysis,[0],[0]
"Therefore, Algorithm 1 provides a certain decrease in the objective function; moreover, we can conclude a stronger result.",5. Convergence Analysis,[0],[0]
Theorem 1.,5. Convergence Analysis,[0],[0]
Let Assumptions 1 and 2 hold.,5. Convergence Analysis,[0],[0]
Consider running Algorithm 1 with a learning rate η ≤ 1AcλK .,5. Convergence Analysis,[0],[0]
"If p ≥ 1 and the minimum eigenvalues of (wt>wt)T0t=0 have a uniform lower bound σ2 > 0, then we get
min t∈[T ]
‖∇fLn(ft)‖Lc1(νn,X) ≤ ( 2Rn(φ0, w1)",5. Convergence Analysis,[0],[0]
"ηγσqT + σq ) 1 q
(7) where we denote ft = w>t+1φt and [T ] = {0, . . .",5. Convergence Analysis,[0],[0]
", T − 1}.
Remark.",5. Convergence Analysis,[0],[0]
(i),5. Convergence Analysis,[0],[0]
"This theorem states the convergence of the minimum functional gradient norm obtained by running Algorithm 1, but returning the last iterate or the best iterate on the validation set is practically better.",5. Convergence Analysis,[0],[0]
(ii),5. Convergence Analysis,[0],[0]
"Although a larger value of T0 may affect the bound in Theorem 1 because of dependency on the minimum eigenvalue of (w>t wt) T0 t=0, optimizing w at each iteration facilitates the convergence speed empirically.
",5. Convergence Analysis,[0],[0]
Generalization bound.,5. Convergence Analysis,[0],[0]
"Here, we derive a generalization bound using the margin bound developed by Koltchinskii & Panchenko (2002), which is composed of the sum of the empirical margin distribution and Rademacher complexity of predictors.",5. Convergence Analysis,[0],[0]
"The margin and the empirical margin distribution for multiclass classification are defined as mf (x, y) def = fy(x)",5. Convergence Analysis,[0],[0]
"− maxy′ 6=y fy′(x) and Pνn [mf (x, y) ≤ δ] (δ > 0), respectively.",5. Convergence Analysis,[0],[0]
"When l is the multiclass logistic loss, using Markov inequality and Proposition 2, we can obtain an upper bound on the margin distribution:
Pνn [mf (x, y) ≤ δ] ≤ ( 1 + 1
exp(−δ)
)",5. Convergence Analysis,[0],[0]
"√ c‖∇fLn(f)‖Lc1(νn,X).
",5. Convergence Analysis,[0],[0]
"Since the convergence of functional gradient norms has been shown in Theorem 1, the resulting problem to derive a generalization bound is to estimate Rademacher complexity, which can be achieved using standard techniques developed by Bartlett & Mendelson (2002); Koltchinskii & Panchenko (2002).",5. Convergence Analysis,[0],[0]
"Thus, we specify here the architecture of predictors.",5. Convergence Analysis,[0],[0]
"In the theoretical analysis, we suppose F is the set of shallow neural networks Bσ(Cx) for simplicity, where B,C are weight matrices and σ is an element-wise activation function.",5. Convergence Analysis,[0],[0]
"Then, the t-th layer is represented as
φt+1(x) = φt(x)− ηDtσ(Ctφt(x)),
where Dt = AtBt, and a predictor is fT−1(x) =",5. Convergence Analysis,[0],[0]
w>T φT−1(x).,5. Convergence Analysis,[0],[0]
"Bounding norms of these weights by controlling the size of F and λ, we can restrict the Rademacher complexity of a set of predictors and obtain a generalization bound.",5. Convergence Analysis,[0],[0]
"We denote by GT−1 the set of predictors under constraints on weight matrices where L1-norms of each row of w>T , Ct, and Dt are bounded by Λw,Λ, and Λ ′. Theorem 2.",5. Convergence Analysis,[0],[0]
Let l be the multiclass logistic regression loss.,5. Convergence Analysis,[0],[0]
Fix δ > 0.,5. Convergence Analysis,[0],[0]
Suppose σ is Lσ-Lipschitz continuous and ‖x‖2 ≤ Λ∞ on X .,5. Convergence Analysis,[0],[0]
"Then, for ∀ρ > 0, with probability at least 1−ρ over the random choice of S from νn, we have ∀f ∈ GT−1,
",5. Convergence Analysis,[0],[0]
"Pν [mf (X,Y ) ≤ 0] ≤ 2c3Λ∞Λw δ √ n",5. Convergence Analysis,[0],[0]
"(1 + ηΛΛ′Lσ) T−1
+
√ log(1/ρ)
2n",5. Convergence Analysis,[0],[0]
"+
( 1 +
1
exp(−δ)
)",5. Convergence Analysis,[0],[0]
"√ c‖∇fLn(f)‖Lc1(νn,X).
",5. Convergence Analysis,[0],[0]
"Combining Theorems 1 and 2, we observe that the learning rate η, the number of iterations T , and the size of F have an impact on the optimization-generalization tradeoff, that is, larger values of these quantities facilitate the convergence on training data while the generalization bound becomes gradually loose.",5. Convergence Analysis,[0],[0]
"Especially, this bound has an exponential dependence on depth T , which is known to be unavoidable (Neyshabur et al., 2015) in the worst case for some networks with L1 or the group norm constraints, but this bound is useful when an initial objective is small and required T is also small sufficiently.
",5. Convergence Analysis,[0],[0]
"We note another type of bound can be derived by utilizing VC-dimension or pseudo-dimension (Vapnik & Chervonenkis, 1971).",5. Convergence Analysis,[0],[0]
"When the activation function is piece-wise linear, such as Relu function σ(x) = max{0, x}, reasonable bounds on these quantities are given by Bartlett et al. (1998; 2017).",5. Convergence Analysis,[0],[0]
"Thus, for that case, we can obtain better bounds with respect to T by combining our analysis and the VC bound, but we omit the precise description for simplicity.",5. Convergence Analysis,[0],[0]
"We next show the other generalization guarantee from the optimization perspective by using the modified algorithm, which may slow down the optimization speed but alleviates the exponential dependence on T in the generalization bound.
",5. Convergence Analysis,[0],[0]
Sample-splitting technique.,5. Convergence Analysis,[0],[0]
"To remedy the exponential dependence on T of the generalization bound, we introduce the sample-splitting technique which has been used recently to provide statistical guarantee of expectation-maximization algorithms (Balakrishnan et al., 2017; Wang et al., 2015).",5. Convergence Analysis,[0],[0]
"That is, instead of Algorithm 1, we analyze its samplesplitting variant.",5. Convergence Analysis,[0],[0]
"Although Algorithm 1 exhibits good empirical performance, the sample-splitting variant is useful for analyzing the behavior of the expected risk.",5. Convergence Analysis,[0],[0]
"In this variant, the entire dataset is split into T pieces, where T is the number of iterations, and each iteration uses a fresh batch of samples.",5. Convergence Analysis,[0],[0]
The key benefit of the sample-splitting method is that it allows us to use concentration inequalities independently at each iterate φt rather than using the complexity measure of the entire model.,5. Convergence Analysis,[0],[0]
"As a result, sample-splitting alleviates the exponential dependence on T presented in Theorem 2.",5. Convergence Analysis,[0],[0]
We now present the details in Algorithm 2.,5. Convergence Analysis,[0],[0]
"For simplicity, we assume T0 = 0, namely the weight vector wt is fixed to the initial weight w0.
",5. Convergence Analysis,[0],[0]
Algorithm 2,5. Convergence Analysis,[0],[0]
"Sample-splitting ResFGB Input: S = (xi, yi)ni=1, initial points φ0, w0, the number of iterations T , embedding class F , and learning rates η Split S into T disjoint subsets S1, . . .",5. Convergence Analysis,[0],[0]
", ST of size bn/T c for t = 0 to T",5. Convergence Analysis,[0],[0]
"− 1 do
DefineRbn/Tc(φt, w) using St Get ιt by approximately solving (6) on St At ← ⌊ T n ⌋∑bn/Tc i=1 ∂zl(φt(xi), yi, w0)ιt(φt(xi))",5. Convergence Analysis,[0],[0]
">
φt+1 ← φt",5. Convergence Analysis,[0],[0]
"− ηAtιt(φt(·)) end for Return φT−1 and w0
Our proof mainly relies on bounding a statistical error of the functional gradient at each iteration in Algorithm 2.",5. Convergence Analysis,[0],[0]
"Because the population version of Algorithm 1 strictly decreases the value of R due to its smoothness, we can show that Algorithm 2 also decreases it with high probability when the norm of a functional gradient is larger than a statistical error bound.",5. Convergence Analysis,[0],[0]
"Thus, we make here an additional assumption on the loss function to bound the statistical error, which is satisfied for a multiclass logistic loss function.
",5. Convergence Analysis,[0],[0]
Assumption 3.,5. Convergence Analysis,[0],[0]
"For the differentiable loss function l(z, y, w) with respect to z, w, there exists a positive real number βr depending on r > 0 such that ‖∂zl(z, y, w)‖2 ≤ βr for z ∈ X , y ∈ Y, w ∈ Br(0).
",5. Convergence Analysis,[0],[0]
We here introduce the notation required to describe the statement.,5. Convergence Analysis,[0],[0]
We let F j be a collection of j-th elements of functions in F .,5. Convergence Analysis,[0],[0]
"For a positive value M , we set
(m, ρ) def = β‖w0‖2
√ KdD
m
( 2M + √ 2K log 2dD
ρ
) .
",5. Convergence Analysis,[0],[0]
"The following proposition is a key result to bound a statistical error as mentioned above.
",5. Convergence Analysis,[0],[0]
Proposition 3.,5. Convergence Analysis,[0],[0]
Let Assumption 3 hold and each F j be the VC-class (for the definition see van der Vaart & Wellner (1996)).,5. Convergence Analysis,[0],[0]
"For ι ∈ F , we assume ‖ι(x)‖2 ≤",5. Convergence Analysis,[0],[0]
√ K on X .,5. Convergence Analysis,[0],[0]
"We set µ to be νX or νm,X and k(x, x′) to be ι(φ(x))>ι(φ(x′)).",5. Convergence Analysis,[0],[0]
"Then, there exists a positive value M depending on F and it follows that with probability at least 1−ρ over the choice of the sample of size m, (m, ρ) upper-bounds the following.
sup ι∈F ‖Tk∂φR(φ,w0)− Tk,m∂φRm(φ,w0)‖Ld2(µ) .
",5. Convergence Analysis,[0],[0]
"Since each iterate in Algorithm 2 is computed on a fresh batch not depending on previous batches, Proposition 3 can be applied to all iterates with m ← bn/T c and ρ ← δ/T for δ ∈ (0, 1).",5. Convergence Analysis,[0],[0]
"Thus, when bn/T c is large and η is small sufficiently, functional gradients used in Algorithm 2 become good approximation to the population variant, and we find that the expected risk function is likely to decrease from Proposition 1.",5. Convergence Analysis,[0],[0]
"Moreover, we note that statistical errors are accumulated additively rather than the exponential growth.",5. Convergence Analysis,[0],[0]
"Concretely, we obtain the following generalization guarantee.
",5. Convergence Analysis,[0],[0]
Theorem 3.,5. Convergence Analysis,[0],[0]
"Let Assumptions 1, 2, and 3 and the same assumption in Proposition 3 hold.",5. Convergence Analysis,[0],[0]
Consider running Algorithm 2.,5. Convergence Analysis,[0],[0]
"If p ≥ 1, ‖∂ζ l(ζ, y)‖2 ≤ B, and the minimum eigenvalue of w0>w0 is lower-bounded by σ2 > 0, then we get with probability at least 1− ρ, ‖∇fL(w>0 φt∗)‖Lc1(νX)",5. Convergence Analysis,[0],[0]
"≤ B ( 2T
n log
T
ρ
) 1 4
+
√ B
γ 1 q σ
· { R0 ηT + β‖w0‖2",5. Convergence Analysis,[0],[0]
"( n T , ρ T ) + η 2 A‖w0‖2K 2β2‖w0‖2 + γ } 1 2q
whereR0 = R(w0, φ0) and t∗ is the index giving the minimum value of ‖∇fLbn/Tc(w>0 φt)‖Lcp(νbnT c,X).",5. Convergence Analysis,[0],[0]
"In this section, we present experimental results of the binary and multiclass classification tasks.",6. Experiments,[0],[0]
"We run Algorithm 1 and compare it with support vector machine, random forest, multilayer perceptron, and gradient boosting methods.",6. Experiments,[0],[0]
We here introduce settings used for Algorithm 1.,6. Experiments,[0],[0]
"As for the loss function, we test both multiclass logistic loss and smooth hinge loss, and as for the embedding class F , we use two or three hidden-layer neural networks.",6. Experiments,[0],[0]
The number of hidden units in each layer is set to 100 or 1000.,6. Experiments,[0],[0]
Linear classifiers and embeddings are trained by Nesterov’s momentum method.,6. Experiments,[0],[0]
"The learning rate is chosen from {10−3, 10−2, 10−1, 1}.",6. Experiments,[0],[0]
"These parameters and the number of iterations T are tuned based on the performance on the validation set.
0 5 10 15 20 0.8
0.9
1.0
ac cu
ra cy
letter
0 1 0.90
0.95
1.00 usps
0 5 10 15 20 25 0.90
0.95
1.00 ijcnn1
0 1 2 3 4 5 6 7 8
iterations
0.90
0.95
1.00
ac cu
ra cy
mnist
0 10 20 30 40 50
iterations
0.8
0.9
1.0 covtype
0 1
iterations
0.78
0.80
0.82 susy
train test
Figure 1.",6. Experiments,[0],[0]
"Learning curves for Algorithm 1 with multiclass logistic loss on libsvm datasets showing classification accuracy on training and test sets versus the number of iterations.
",6. Experiments,[0],[0]
"We use the following benchmark datasets: letter, usps, ijcnn1, mnist, covtype, and susy.",6. Experiments,[0],[0]
We now explain the experimental procedure.,6. Experiments,[0],[0]
"For datasets not providing a fixed test set, we first divide each dataset randomly into two parts: 80% for training and the rest for test.",6. Experiments,[0],[0]
We next divide each training set randomly and use 80% for training and the rest for validation.,6. Experiments,[0],[0]
We perform each method on the training dataset with several hyperparameter settings and choose the best setting on the validation dataset.,6. Experiments,[0],[0]
"Finally, we train each model on the entire training dataset using this setting and evaluate it on the test dataset.",6. Experiments,[0],[0]
"This procedure is run 5 times.
",6. Experiments,[0],[0]
The mean classification accuracy and the standard deviation are listed in Table 1.,6. Experiments,[0],[0]
"The support vector machine is performed using a random Fourier feature (Rahimi & Recht, 2007) with an embedding dimension of 103 or 104.",6. Experiments,[0],[0]
"For multilayer perceptron, we use three, four, or five hidden layers and rectified linear unit as the activation function.",6. Experiments,[0],[0]
"The number of hidden units in each layer is set to 100 or
1000.",6. Experiments,[0],[0]
"As for random forest, the number of trees is set to 100, 500, or 1000 and the maximum depth is set to 10, 20, or 30.",6. Experiments,[0],[0]
"Gradient boosting in Table 1 indicates LightGBM (Ke et al., 2017) with the hyperparameter settings: the maximum number of estimators is 1000, the learning rate is chosen from {10−3, 10−2, 10−1, 1}, and number of leaves in one tree is chosen from {16, 32, . . .",6. Experiments,[0],[0]
", 1024}.
",6. Experiments,[0],[0]
"As seen in Table 1, our method shows superior performance over the competitors except for covtype.",6. Experiments,[0],[0]
"However, the method that achieves higher accuracy than our method is only LightGBM on covtype.",6. Experiments,[0],[0]
"We plot learning curves for one run of Algorithm 1 with logistic loss, which depicts classification accuracies on training and test sets.",6. Experiments,[0],[0]
Note that the number of iterations are determined by classification results on validation sets.,6. Experiments,[0],[0]
This figure shows the efficiency of the proposed method.,6. Experiments,[0],[0]
We have formalized the gradient boosting perspective of ResNet and have proposed new gradient boosting method by leveraging this viewpoint.,7. Conclusion,[0],[0]
We have shown two types of generalization bounds: one is by the margin bound and the other is by the sample-splitting technique.,7. Conclusion,[0],[0]
These bounds clarify the optimization-generalization tradeoff of the proposed method.,7. Conclusion,[0],[0]
Impressive empirical performance of the method has been confirmed on several benchmark datasets.,7. Conclusion,[0],[0]
"We note that our method can take in convolutional neural networks as feature extractions, but additional efforts will be required to achieve high performance on image datasets.",7. Conclusion,[0],[0]
This is one of important topics left for future work.,7. Conclusion,[0],[0]
"This work was partially supported by MEXT KAKENHI (25730013, 25120012, 26280009, and 15H05707), JSTPRESTO (JPMJPR14E4), and JST-CREST (JPMJCR14D7, JPMJCR1304).",Acknowledgements,[0],[0]
Residual Networks (ResNets) have become stateof-the-art models in deep learning and several theoretical studies have been devoted to understanding why ResNet works so well.,abstractText,[0],[0]
One attractive viewpoint on ResNet is that it is optimizing the risk in a functional space by combining an ensemble of effective features.,abstractText,[0],[0]
"In this paper, we adopt this viewpoint to construct a new gradient boosting method, which is known to be very powerful in data analysis.",abstractText,[0],[0]
"To do so, we formalize the gradient boosting perspective of ResNet mathematically using the notion of functional gradients and propose a new method called ResFGB for classification tasks by leveraging ResNet perception.",abstractText,[0],[0]
Two types of generalization guarantees are provided from the optimization perspective: one is the margin bound and the other is the expected risk bound by the sample-splitting technique.,abstractText,[0],[0]
Experimental results show superior performance of the proposed method over state-of-the-art methods such as LightGBM.,abstractText,[0],[0]
Functional Gradient Boosting based on Residual Network Perception,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 125–136 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
125",text,[0],[0]
"Dialogue systems or conversational agents which are able to hold natural, relevant, and coherent interactions with humans have been a long-standing goal of artificial intelligence and machine learning.",1 Introduction,[0],[0]
"There has been a lot of important previous work in this field for decades (Weizenbaum, 1966; Isbell et al., 2000; Rambow et al., 2001; Rieser et al., 2005; Georgila et al., 2006; Rieser and Lemon, 2008; Ritter et al., 2011), includ-
We release all data, code, and models at: https:// github.com/ramakanth-pasunuru/video-dialogue
ing recent work on introduction of large textualdialogue datasets (e.g., Lowe et al. (2015); Serban et al. (2016)) and end-to-end neural network based models (Sordoni et al., 2015; Vinyals and Le, 2015; Su et al., 2016; Luan et al., 2016; Li et al., 2016; Serban et al., 2017a,b).
",1 Introduction,[0],[0]
Current dialogue tasks are usually focused on the textual or verbal context (conversation history).,1 Introduction,[0],[0]
"In terms of multimodal dialogue, speechbased spoken dialogue systems have been widely explored (Eckert et al., 1997; Singh et al., 2000; Young, 2000; Janin et al., 2003; Celikyilmaz et al., 2017; Wen et al., 2015; Su et al., 2016; Mrkšić et al., 2016), as well as work on gesture and haptics based dialogue (Johnston et al., 2002; Cassell, 1999; Foster et al., 2008).",1 Introduction,[0],[0]
"In order to address the additional advantage of using visually-grounded context knowledge in dialogue, recent work introduced the visual dialogue task (Das et al., 2017; de Vries et al., 2017; Mostafazadeh et al., 2017).",1 Introduction,[0],[0]
"However, the visual context in these tasks is lim-
ited to one static image.",1 Introduction,[0],[0]
"Moreover, the interactions are between two speakers with fixed roles (one asks questions and the other answers).
",1 Introduction,[0],[0]
"Several situations of real-world dialogue among humans involve more ‘dynamic’ visual context, i.e., video-style information of the world moving around us (both spatially and temporally).",1 Introduction,[0],[0]
"Further, several human conversations involve more than two speakers, with changing roles.",1 Introduction,[0],[0]
"In order to develop such dynamically-visual multimodal dialogue models, we introduce a new ‘manyspeaker, video-context chat’ testbed, along with a new dataset and models for the same.",1 Introduction,[0],[0]
"Our dataset is based on live-broadcast soccer (FIFA18) game videos from the ‘Twitch.tv’ live video streaming platform, along with the spontaneous, many-speaker live chats about the game.",1 Introduction,[0],[0]
"This challenging testbed allows us to develop dialogue models where the generated response is required to be relevant to the temporal and spatial events in the live video, as well as be relevant to the chat history (with potential impact towards videogrounded applications such as personal assistants, intelligent tutors, and human-robot collaboration).
",1 Introduction,[0],[0]
We also present several strong discriminative and generative baselines that learn to retrieve and generate bimodal-relevant responses.,1 Introduction,[0],[0]
"We first present a triple-encoder discriminative model to encode the video, chat history, and response, and then classify the relevance label of the response.",1 Introduction,[0],[0]
We then improve over this model via tridirectional attention flow (TriDAF).,1 Introduction,[0],[0]
"For the generative models, we model bidirectional attention flow between the video and textual chat context encoders, which then decodes the response.",1 Introduction,[0],[0]
"We evaluate these models via retrieval ranking-recall, phrasematching metrics, as well as human evaluation studies.",1 Introduction,[0],[0]
We also present dataset analysis as well as model ablations and attention visualizations to understand the contribution of the video vs. chat modalities and the model components.,1 Introduction,[0],[0]
"Early dialogue systems had components of natural language (NL) understanding unit, dialogue manager, and NL generation unit (Bates, 1995).",2 Related Work,[0],[0]
"Statistical learning methods were used for automatic feature extraction (Dowding et al., 1993; Mikolov et al., 2013), dialogue managers incorporated reward-driven reinforcement learning (Young et al., 2013; Shah et al., 2016), and the
generation units have been extended with seq2seq neural network models (Vinyals and Le, 2015; Serban et al., 2016; Luan et al., 2016).
",2 Related Work,[0],[0]
"In addition to the focus on textual dialogue context, using multimodal context brings more potential for having real-world grounded conversations.",2 Related Work,[0],[0]
"For example, spoken dialogue systems have been widely explored (Singh et al., 2000; Gurevych and Strube, 2004; Georgila et al., 2006; Eckert et al., 1997; Young, 2000; Janin et al., 2003; De Mori, 2007; Wen et al., 2015; Su et al., 2016; Mrkšić et al., 2016; Hori et al., 2016; Celikyilmaz et al., 2015, 2017), as well as gesture and haptics based dialogue (Johnston et al., 2002; Cassell, 1999; Foster et al., 2008).",2 Related Work,[0],[0]
"Additionally, dialogue systems for digital personal assistants are also well explored (Myers et al., 2007; Sarikaya et al., 2016; Damacharla et al., 2018).",2 Related Work,[0],[0]
"In the visual modality direction, some important recent attempts have been made to use static image based context in dialogue systems (Das et al., 2017; de Vries et al., 2017; Mostafazadeh et al., 2017), who proposed the ‘visual dialog’ task, where the human can ask questions on a static image, and an agent interacts by answering these questions based on the previous chat context and the image’s visual features.",2 Related Work,[0],[0]
"Also, Celikyilmaz et al. (2014) used visual display information for on-screen item resolution in utterances for improving personal digital assistants.
",2 Related Work,[0],[0]
"In contrast, we propose to employ dynamic video-based information as visual context knowledge in dialogue models, so as to move towards video-grounded intelligent assistant applications.",2 Related Work,[0],[0]
"In the video+language direction, previous work has looked at video captioning (Venugopalan et al., 2015) as well as Q&A and fill-inthe-blank tasks on videos (Tapaswi et al., 2016; Jang et al., 2017; Maharaj et al., 2017) and interactive 3D environments (Das et al., 2018; Yan et al., 2018; Gordon et al., 2017; Anderson et al., 2017).",2 Related Work,[0],[0]
"There has also been early related work on generating sportscast commentaries from simulation (RoboCup) soccer videos represented as non-visual state information (Chen and Mooney, 2008).",2 Related Work,[0],[0]
"Also, Liu et al. (2016a) presented some initial ideas on robots learning grounded task representations by watching and interacting with humans performing the task (i.e., by converting human demonstration videos to Causal And-Or graphs).",2 Related Work,[0],[0]
"On the other hand, we propose a new video-chat dataset where the
dialogue models need to generate the next response in the sequence of chats, conditioned both on the raw video features as well as the previous textual chat history.",2 Related Work,[0],[0]
"Moreover, our new dataset presents a many-speaker conversation setting, similar to previous work on meeting understanding and Computer Supported Cooperative Work (CSCW) (Janin et al., 2003; Waibel et al., 2001; Schmidt and Bannon, 1992).",2 Related Work,[0],[0]
"In the live video stream direction, Fu et al. (2017) and Ping and Chen (2017) used real-time comments to predict the frame highlights in a video, and Barbieri et al. (2017) presented emotes and troll prediction.",2 Related Work,[0],[0]
"For our new video-context dialogue task, we used the publicly accessible Twitch.tv live broadcast platform, and collected videos of soccer (FIFA18) games along with the users’ live chat conversations about the game.",3.1 Dataset Collection and Processing,[0],[0]
This dataset has videos involving various realistic human actions and events in a complex sports environment and hence serves as a good testbed and first step towards multimodal video-based dialogue data.,3.1 Dataset Collection and Processing,[0],[0]
"An example is shown in Fig. 1 (and an original screenshot example in Fig. 2), where the users perform a complex ‘manyspeaker’, ‘multimodal’ dialogue.",3.1 Dataset Collection and Processing,[0],[0]
"Overall, we collected 49 FIFA-18 game videos along with their users’ chat, and divided them into 33 videos for training, 8 videos for validation, and 8 videos for testing.",3.1 Dataset Collection and Processing,[0],[0]
"Each such video is several hours long, providing a good amount of data (Table 2).
",3.1 Dataset Collection and Processing,[0],[0]
"To extract triples (instances) of video context, chat context, and response from this data, we divide these videos based on the fixed time frames instead of fixed number of utterances in order to maintain conversation topic clusters (because of the sparse nature of chat utterances count over the time).",3.1 Dataset Collection and Processing,[0],[0]
"First, we use 20-sec context windows to extract the video clips and users utterances in
this time frame, and use it as our video and chat contexts, resp.",3.1 Dataset Collection and Processing,[0],[0]
"Next, the chat utterances in the immediately-following 10-sec window (response window) that do not overlap with the next instance’s context window are considered as potential responses.1 Hence, there are only two instances (triples) in a 60-sec long video, i.e., 20-sec video+chat context window and 10-sec response window, and there is no overlap between the instances.",3.1 Dataset Collection and Processing,[0],[0]
"Now, out of these potential responses, to only allow the response that has at least some good coherence and relevance with the chat context’s topic, we choose the first (earliest) response that has high similarity with some other utterance in this response window (using 0.5 BLEU-4 threshold, based on manual inspection).2 Human Quality Evaluation of Data Filtering Process: To evaluate the quality of the responses that result from our filtering process described above, we performed an anonymous (randomly shuffled w/o identity) human comparison between the response selected by our filtering process vs. the first response from the response window without any filtering, based on relevance w.r.t.",3.1 Dataset Collection and Processing,[0],[0]
video and chat context.,3.1 Dataset Collection and Processing,[0],[0]
"Table 1 presents the results on 100 sample size, showing that humans in a blindtest found 90% (34+56) of our filtered responses as valid responses, verifying that our response selection procedure is reasonable.",3.1 Dataset Collection and Processing,[0],[0]
"Furthermore, out of these 90% valid responses, we found that 55% are chat-only relevant, 11% are video-only relevant, and 24% are both video+chat relevant.
",3.1 Dataset Collection and Processing,[0],[0]
"In order to make the above procedure safe and to make the dataset more challenging, we also discourage frequent responses (top-20 most-frequent
1We use non-overlapping windows because: (1) the utterances are non-uniformly distributed in time and hence if we have a shifting window, sometimes a particular data instance/chunk becomes very sparse and contains almost zero utterances; (2) we do not want overlap between response of one window with the context of the next window, so as to avoid the encoder already having seen the response (as part of context) that the decoder needs to generate for the other window.
",3.1 Dataset Collection and Processing,[0],[0]
"2Based on intuition that if multiple speakers are saying the same response in that 10-second window, then this response should be more meaningful/relevant w.r.t.",3.1 Dataset Collection and Processing,[0],[0]
"chat context.
generic utterances) unless no other response satisfies the similarity condition, hence suppressing the frequent",3.1 Dataset Collection and Processing,[0],[0]
responses.3,3.1 Dataset Collection and Processing,[0],[0]
"If we couldn’t find any utterance based on the multi-response matching procedure described above, then we just consider the first utterance in the 10-second window as the response.4 We also make sure that the chat context window has at least 4 utterances, otherwise we exclude that context window and also the corresponding response window from the dataset.",3.1 Dataset Collection and Processing,[0],[0]
"After all this processing, our final resulting dataset contains 10, 510 samples in training, 2, 153 samples in validation, and 2, 780 samples in test.5",3.1 Dataset Collection and Processing,[0],[0]
"Dataset Statistics Table 2 presents the full statistics on train, validation, and test sets of our Twitch-FIFA dataset, after the filtering process described in Sec. 3.1.",3.2 Dataset Analysis,[0],[0]
"As shown, the average chat context length in the dataset is around 68 words, and the average response length is 6.3 words.",3.2 Dataset Analysis,[0],[0]
Chat Context Size Fig. 3 presents the study of number of utterances in the chat context vs. the number of such training samples.,3.2 Dataset Analysis,[0],[0]
"As we limit the minimum number of utterances to 4, chat context with less than 4 utterances is not present in the dataset.",3.2 Dataset Analysis,[0],[0]
"From the Fig. 3, it is clear that as the number of utterances in the chat context increases, the number of such training samples decrease.",3.2 Dataset Analysis,[0],[0]
Frequent Words Fig. 4 presents the top-20 frequent words (excluding stop words) and their corresponding frequency in our Twitch-FIFA dataset.,3.2 Dataset Analysis,[0],[0]
Most of these frequent words are related to soccer vocabulary.,3.2 Dataset Analysis,[0],[0]
"Also, some of these frequent words are twitch emotes (e.g. ‘kappa’, ‘inceptionlove’).
",3.2 Dataset Analysis,[0],[0]
"3Note that this filtering suppresses the performance of simple frequent-response baseline described in Sec. 4.1.
",3.2 Dataset Analysis,[0],[0]
"4Other preprocessing steps include: omit the utterances in the response window which refer to a speaker name out of the current chat context; remove non-representative utterances, e.g., those with hyperlinks; replace (anonymize) all the user identities mentioned in the utterances with a common tag (i.e., anonymizing due to similar intuitions from the Q&A community (Hermann et al., 2015)).
5Note that this is substantially larger than or comparable to most current video captioning datasets.",3.2 Dataset Analysis,[0],[0]
We plan to further extend our dataset based on diverse games and video types.,3.2 Dataset Analysis,[0],[0]
"Let v = {v1, v2, .., vm} be the video context frames, u = {u1, u2, .., un} be the textual chat (utterance) context tokens, and r = {r1, r2, .., rk} be response tokens generated (or retrieved).",4 Models,[0],[0]
"Our simple non-trained baselines are MostFrequent-Response (re-rank the candidate responses based on their frequency in the training set), Chat-Response-Cosine (re-rank the candidate responses based on their similarity score w.r.t.",4.1 Baselines,[0],[0]
"the chat context), and Nearest-Neighbor (find the Kbest similar chat contexts in the training set, take their corresponding responses, and then re-rank the candidate responses based on mean similarity score w.r.t.",4.1 Baselines,[0],[0]
this K-best response set).,4.1 Baselines,[0],[0]
"For trained baselines, we use logistic regression and Naive Bayes methods.",4.1 Baselines,[0],[0]
We use the final state of a Twitch-trained RNN Language Model to represent the chat context and response.,4.1 Baselines,[0],[0]
Please see supplementary for full details.,4.1 Baselines,[0],[0]
"For our simpler discriminative model, we use a ‘triple encoder’ to encode the video context, chat context, and response (see Fig. 5), as an extension of the dual encoder model in Lowe et al. (2015).",4.2.1 Triple Encoder,[0],[0]
"The task here is to predict the given train-
ing triple (v, u, r) as positive or negative.",4.2.1 Triple Encoder,[0],[0]
"Let hvf , huf , and h r f be the final state information of the video, chat, and response LSTM-RNN (bidirectional) encoders respectively; then the probability of a positive training triple is defined as follows:
p(v, u, r; θ) = σ([hvf ;h u",4.2.1 Triple Encoder,[0],[0]
"f ] TWhrf + b) (1)
where W and b are trainable parameters.",4.2.1 Triple Encoder,[0],[0]
"Here, W can be viewed as a similarity matrix which will bring the context [hvf ;h u f ] into the same space as the response hrf , and get a suitable similarity score.",4.2.1 Triple Encoder,[0],[0]
"For optimizing our discriminative model, we use max-margin loss function similar to Mao et al. (2016) and Yu et al. (2017).",4.2.1 Triple Encoder,[0],[0]
"Given a positive training triple (v, u, r), let the corresponding negative training triples be (v′, u, r), (v, u′, r), and (v, u, r′), i.e., one modality is wrong at a time in each of these three (see Sec. 5 for the negative example selection).",4.2.1 Triple Encoder,[0],[0]
"The max-margin loss is:
L(θ) = ∑",4.2.1 Triple Encoder,[0],[0]
"[max(0,M + log p(v′, u, r)− log p(v, u, r))
",4.2.1 Triple Encoder,[0],[0]
"+ max(0,M + log p(v, u′, r)− log p(v, u, r))",4.2.1 Triple Encoder,[0],[0]
"+ max(0,M + log p(v, u, r′)− log p(v, u, r))",4.2.1 Triple Encoder,[0],[0]
"]
(2)
where the summation is over all the training triples in the dataset.",4.2.1 Triple Encoder,[0],[0]
M is a tunable margin hyperparameter between positive and negative training triples.,4.2.1 Triple Encoder,[0],[0]
Our tridirectional attention flow model learns stronger joint spaces between the three modalities in a mutual-information way.,4.2.2 Tridirectional Attention Flow (TriDAF),[0],[0]
"We use bidirectional attention flow mechanisms (Seo et al., 2017) between the video and chat contexts, between the video context and the response, as well as between the chat context and the response, hence enabling attention flow across all three modalities, as shown in Fig. 6.",4.2.2 Tridirectional Attention Flow (TriDAF),[0],[0]
We name this model Tridirectional Attention Flow or TriDAF.,4.2.2 Tridirectional Attention Flow (TriDAF),[0],[0]
"We will next discuss the bidirectional attention flow mechanism between video and chat contexts, but the same formulation holds true for bidirectional attention between video context and response, and between chat context and response.",4.2.2 Tridirectional Attention Flow (TriDAF),[0],[0]
"Given the video context hidden
state hvi and chat context hidden state h u",4.2.2 Tridirectional Attention Flow (TriDAF),[0],[0]
j,4.2.2 Tridirectional Attention Flow (TriDAF),[0],[0]
"at time steps i and j respectively, the bidirectional attention mechanism is based on the similarity score:
S (v,u) i,j = w T S(v,u)",4.2.2 Tridirectional Attention Flow (TriDAF),[0],[0]
[hvi ;h u j ;h v,4.2.2 Tridirectional Attention Flow (TriDAF),[0],[0]
i huj ],4.2.2 Tridirectional Attention Flow (TriDAF),[0],[0]
"(3)
where S(v,u)i,j is a scalar, wS(v,u) is a trainable parameter, and denote element-wise multiplication.",4.2.2 Tridirectional Attention Flow (TriDAF),[0],[0]
"The attention distribution from chat context to video context is defined as αi: = softmax(Si:), hence the chat-to-video context vector cv←ui = ∑ j αi,jh u j .",4.2.2 Tridirectional Attention Flow (TriDAF),[0],[0]
"Similarly, the attention distribution from video context to chat context is defined as βj: = softmax(S:j), hence the videoto-chat context vector cu←vj = ∑ i βj,ih v i .
",4.2.2 Tridirectional Attention Flow (TriDAF),[0],[0]
"We then compute similar bidirectional attention flow mechanisms between the video context and response, and between the chat context and response.",4.2.2 Tridirectional Attention Flow (TriDAF),[0],[0]
"Then, we concatenate each hidden state and its corresponding context vector from other two modalities, e.g., ĥvi =",4.2.2 Tridirectional Attention Flow (TriDAF),[0],[0]
[h v i ; c v←u,4.2.2 Tridirectional Attention Flow (TriDAF),[0],[0]
i ; c v←r,4.2.2 Tridirectional Attention Flow (TriDAF),[0],[0]
i ] for the ith timestep of the video context.,4.2.2 Tridirectional Attention Flow (TriDAF),[0],[0]
"Finally, we add self-attention mechanism (Lin et al., 2017) across the concatenated hidden states of each of the three modules.6 If ĥvi is the final concatenated vector of the video context at time step i, then the selfattention weights αs for this video context are the softmax of es:
esi = V v a tanh(W v a ĥ v",4.2.2 Tridirectional Attention Flow (TriDAF),[0],[0]
"i + b v a) (4)
where V va , W v a , and b v a are trainable self-attention parameters.",4.2.2 Tridirectional Attention Flow (TriDAF),[0],[0]
"The final representation vector of the full video context after self-attention is ĉv =∑
i α s",4.2.2 Tridirectional Attention Flow (TriDAF),[0],[0]
i ĥ v i .,4.2.2 Tridirectional Attention Flow (TriDAF),[0],[0]
"Similarly, the final representation vectors of the chat context and the response are ĉu and ĉr, respectively.",4.2.2 Tridirectional Attention Flow (TriDAF),[0],[0]
"Finally, the probability that 6In our preliminary experiments, we found that adding self-attention is 0.92% better in recall@1 and faster than passing the hidden states through another layer of RNN, as done in Seo et al. (2017).
",4.2.2 Tridirectional Attention Flow (TriDAF),[0],[0]
"the given training triple (v, u, r) is positive is:
p(v, u, r; θ) = σ([ĉv; ĉu]TWĉr + b) (5)
",4.2.2 Tridirectional Attention Flow (TriDAF),[0],[0]
"Again, here also we use max-margin loss (Eqn. 2).",4.2.2 Tridirectional Attention Flow (TriDAF),[0],[0]
Our simpler generative model is a sequence-tosequence model with bilinear attention mechanism (similar to Luong et al. (2015)).,4.3.1 Seq2seq with Attention,[0],[0]
"We have two encoders, one for encoding the video context and another for encoding the chat context, as shown in Fig. 7.",4.3.1 Seq2seq with Attention,[0],[0]
We combine the final state information from both encoders and give it as initial state to the response generation decoder.,4.3.1 Seq2seq with Attention,[0],[0]
The two encoders and the decoder are all two-layer LSTMRNNs.,4.3.1 Seq2seq with Attention,[0],[0]
Let hvi and h u j be the hidden states of video and chat encoders at time step i and j respectively.,4.3.1 Seq2seq with Attention,[0],[0]
"At each time step t of the decoder with hidden state hrt , the decoder attends to parts of video and chat encoders and uses the combined information to generate the next token.",4.3.1 Seq2seq with Attention,[0],[0]
"Let αt and βt be the attention weight distributions for video and chat encoders respectively with video context vector cvt = ∑ i αt,ih v i and chat context vector
cut = ∑ j βt,jh u j .",4.3.1 Seq2seq with Attention,[0],[0]
"The attention distribution for video encoder is defined as (and the same holds for chat encoder):
et,i = h r t",4.3.1 Seq2seq with Attention,[0],[0]
"TW va h v i ; αt = softmax(et) (6)
where W va is a trainable parameter.",4.3.1 Seq2seq with Attention,[0],[0]
"Next, we concatenate the attention-based context information (cvt and c u t ) and decoder hidden state (h r t ), and do a non-linear transformation to get the final hidden state ĥrt as follows:
ĥrt = tanh(Wc[c v t ; c u t ;h r t ]) (7)
where Wc is again a trainable parameter.",4.3.1 Seq2seq with Attention,[0],[0]
"Finally, we project the final hidden state information to vocabulary size and give it as input to a softmax layer to get the vocabulary distribution p(rt|r1:t−1, v, u; θ).",4.3.1 Seq2seq with Attention,[0],[0]
"During training, we minimize the cross-entropy loss defined as follows:
LXE(θ) =",4.3.1 Seq2seq with Attention,[0],[0]
"− ∑∑
t
log p(rt|r1:t−1, v, u; θ) (8)
where the final summation is over all the training triples in the dataset.
",4.3.1 Seq2seq with Attention,[0],[0]
"Further, to train a stronger generative model with negative training examples (which teaches
the model to give higher generative decoder probability to the positive response as compared to all the negative ones), we use a max-margin loss (similar to Eqn. 2 in Sec. 4.2.1): LMM(θ) = ∑",4.3.1 Seq2seq with Attention,[0],[0]
"[max(0,M + log p(r|v′, u)− log p(r|v, u))
+ max(0,M + log p(r|v, u′)− log p(r|v, u))",4.3.1 Seq2seq with Attention,[0],[0]
+,4.3.1 Seq2seq with Attention,[0],[0]
"max(0,M + log p(r′|v, u)− log p(r|v, u))",4.3.1 Seq2seq with Attention,[0],[0]
"]
(9)
where the summation is over all the training triples in the dataset.",4.3.1 Seq2seq with Attention,[0],[0]
"Overall, the final joint loss function is a weighted combination of cross-entropy loss and max-margin loss: L(θ) = LXE(θ) + λLMM(θ), where λ is a tunable hyperparameter.",4.3.1 Seq2seq with Attention,[0],[0]
"The stronger version of our generative model extends the two-encoder-attention-decoder model above to add bidirectional attention flow (BiDAF) mechanism (Seo et al., 2017) between video and chat encoders, as shown in Fig. 7.",4.3.2 Bidirectional Attention Flow (BiDAF),[0],[0]
"Given the hidden states hvi and h u j of video and chat encoders at time step i and j, the final hidden states after the BiDAF are ĥvi =",4.3.2 Bidirectional Attention Flow (BiDAF),[0],[0]
[h v i ; c v←u,4.3.2 Bidirectional Attention Flow (BiDAF),[0],[0]
i ] and ĥ u j =,4.3.2 Bidirectional Attention Flow (BiDAF),[0],[0]
"[h u i ; c u←v j ] (similar to as described in Sec. 4.2.2), respectively.",4.3.2 Bidirectional Attention Flow (BiDAF),[0],[0]
"Now, the decoder attends over these final hidden states, and the rest of the decoder process is similar to Sec 4.3.1 above, including the weighted joint cross-entropy and max-margin loss.",4.3.2 Bidirectional Attention Flow (BiDAF),[0],[0]
"Evaluation We first evaluate both our discriminative and generative models using retrieval-based recall@k scores, which is a concrete metric for such dialogue generation tasks (Lowe et al., 2015).",5 Experimental Setup,[0],[0]
"For our discriminative models, we simply rerank the given responses (in a candidate list of size 10, based on 9 negative examples; more details below)
in the order of the probability score each response gets from the model.",5 Experimental Setup,[0],[0]
"If the positive response is within the top-k list, then the recall@k score is 1, otherwise 0, following previous Ubuntu-dialogue work (Lowe et al., 2015).",5 Experimental Setup,[0],[0]
"For the generative models, we follow a similar approach, but the reranking score for a candidate response is based on the log probability score given by the generative models’ decoder for that response, following the setup of previous visual-dialog work (Das et al., 2017).",5 Experimental Setup,[0],[0]
"In our experiments, we use recall@1, recall@2, and recall@5 scores.",5 Experimental Setup,[0],[0]
"For completeness, we also report the phrase-matching metric scores: METEOR (Denkowski and Lavie, 2014) and ROUGE (Lin, 2004) for our generative models.",5 Experimental Setup,[0],[0]
"We also present human evaluation.
",5 Experimental Setup,[0],[0]
"Training Details For negative samples, during training, for every positive triple (video, chat, response) in the training set, we sample 3 random negative triples.",5 Experimental Setup,[0],[0]
"For validation/test, we sample 9 random negative responses elsewhere from the validation/test set.",5 Experimental Setup,[0],[0]
"Also, the negative samples don’t come from the video corresponding to the positive response.",5 Experimental Setup,[0],[0]
"More details of negative samples and other training details (e.g., dimension/vocab sizes, visual feature details, validationbased hyperparamater tuning and model selection), are discussed in the supplementary.",5 Experimental Setup,[0],[0]
"First, the overall human quality evaluation of our dataset (shown in Table 1) demonstrates that it
contains 90% responses relevant to video and/or chat context.",6.1 Human Evaluation of Dataset,[0],[0]
"Next, we also do a blind human study on the recall-based setup (on a set of 100 samples from the validation set), where we anonymize the positive response by randomly mixing it with 9 tricky negative responses in the retrieval list, and ask the user to select the most relevant response for the given video and/or chat context.",6.1 Human Evaluation of Dataset,[0],[0]
"We found that human performance on this task is around 55% recall@1, demonstrating that this 10-way-discriminative recall-based task setup is reasonably challenging for humans,7 but also that there is a lot of scope for future model improvements because the chance baseline is only 10% and the best-performing model so far (see Sec. 6.3) achieves only 22% recall@1 (on dev set), and hence there is a large 33% gap.",6.1 Human Evaluation of Dataset,[0],[0]
Table 3 displays all our primary results.,6.2 Baseline Results,[0],[0]
We first discuss results of our simple non-trained and trained baselines (see Sec. 4.1).,6.2 Baseline Results,[0],[0]
"The ‘MostFrequent-Response’ baseline, which just ranks the 10-sized response retrieval list based on their frequency in the training data, gets only around 10% recall@1.8",6.2 Baseline Results,[0],[0]
"Our other non-trained baselines: ‘Chat-Response-Cosine’ and ‘Nearest Neighbor’, which ranks the candidate responses based on (Twitch-trained RNN encoder’s vector) cosine similarity with chat-context and K-best training contexts’ response vectors, respectively, achieves slightly better scores.",6.2 Baseline Results,[0],[0]
"We also show that our simple trained baselines (logistic regression and nearest neighbor) also achieve relatively low scores, indicating that a simple, shallow model will not work on this challenging dataset.",6.2 Baseline Results,[0],[0]
"Next, we present the recall@k retrieval performance of our various discriminative models in Ta-
7This relatively low human recall@1 performance is because this is a challenging, 10-way-discriminative evaluation, i.e., the choice comes w.r.t.",6.3 Discriminative Model Results,[0],[0]
9 tricky negative examples along with just 1 positive example (hence chance-baseline is only 10%).,6.3 Discriminative Model Results,[0],[0]
"Note that these negative examples are an artifact of specifically recall-based evaluation only, and will not affect the more important real-world task of response generation (for which our dataset’s response quality is 90%, as shown in Table 1).",6.3 Discriminative Model Results,[0],[0]
"Moreover, our dataset filtering (see Sec. 3.1) also ‘suppresses’ simple baselines and makes the task even harder.
8Note that the performance of this baseline is worse than the random choice baseline (recall@1:10%, recall@2:20%, recall@5:50%) because our dataset filtering process already suppresses frequent responses (see Sec. 3.1), in order to provide a challenging dataset for the community.
",6.3 Discriminative Model Results,[0],[0]
"ble 3: dual encoder (chat context only), dual encoder (video context only), triple encoder, and TriDAF model with self-attention.",6.3 Discriminative Model Results,[0],[0]
"Our dual encoder models are significantly better than random choice and all our simple baselines above, and further show that they have complementary information because using both of them together (in ‘Triple Encoder’) improves the overall performance of the model.",6.3 Discriminative Model Results,[0],[0]
"Finally, we show that our novel TriDAF model with self-attention performs significantly better than the triple encoder model.9",6.3 Discriminative Model Results,[0],[0]
"Next, we evaluate the performance of our generative models with both retrieval-based recall@k scores and phrase matching-based metrics as discussed in Sec. 5 (as well as human evaluation).",6.4 Generative Model Results,[0],[0]
We first discuss the retrieval-based recall@k results in Table 3.,6.4 Generative Model Results,[0],[0]
"Starting with a simple sequenceto-sequence attention model with video only, chat only, and both video and chat encoders, the recall@k scores are better than all the simple baselines.",6.4 Generative Model Results,[0],[0]
"Moreover, using both video+chat context is again better than using only one context modality.",6.4 Generative Model Results,[0],[0]
"Finally, we show that the addition of the bidirectional attention flow mechanism improves the performance in all recall@k scores.10 Note that generative model scores are lower than the discriminative models on retrieval recall@k metric, which is expected (see discussion in previous visual dialogue work (Das et al., 2017)), because discriminative models can tune to the biases in the response candidate options, but generative models are more useful for real-world tasks such as
9Statistical significance of p < 0.01 for recall@1, based on the bootstrap test (Noreen, 1989; Efron and Tibshirani, 1994) with 100K samples.
10Stat. signif.",6.4 Generative Model Results,[0],[0]
p < 0.05 for recall@1 w.r.t.,6.4 Generative Model Results,[0],[0]
Seq2seq+Atten (video+chat); p < 0.01 w.r.t. chat-,6.4 Generative Model Results,[0],[0]
"and video-only models.
generation of novel responses word-by-word from scratch in Siri/Alexa/Cortana style applications (whereas discriminative models can only rank the pre-given list of responses).
",6.4 Generative Model Results,[0],[0]
"We also evaluate our generative models with phrase-level matching metrics: METEOR and ROUGE-L, as shown in Table 4.",6.4 Generative Model Results,[0],[0]
"Again, our BiDAF model is stat.",6.4 Generative Model Results,[0],[0]
significantly better than nonBiDAF model on both METEOR (p < 0.01) and ROUGE-L (p < 0.02) metrics.,6.4 Generative Model Results,[0],[0]
"Since dialogue systems can have several diverse, non-overlapping valid responses, we consider a multi-reference setup where all the utterances in the 10-sec response window are treated as valid responses.11",6.4 Generative Model Results,[0],[0]
"Finally, we also perform human evaluation to compare our top two generative models, i.e., the video+chat seq2seq with attention and its extension with BiDAF (Sec. 4.3), based on a 100-sized sample.",6.5 Human Evaluation of Models,[0],[0]
"We take the generated response from both these models, and randomly shuffle these pairs to anonymize model identity.",6.5 Human Evaluation of Models,[0],[0]
We then ask two annotators (for 50 task instances each) to score the responses of these two models based on relevance.,6.5 Human Evaluation of Models,[0],[0]
Note that the human evaluators were familiar with Twitch FIFA-18 video games and also the Twitch’s unique set of chat mannerisms and emotes.,6.5 Human Evaluation of Models,[0],[0]
"As shown in Table 5, our BiDAF based generative model performs better than the non-BiDAF one, which is already quite a strong video+chat encoder model with attention.",6.5 Human Evaluation of Models,[0],[0]
We also compare the effect of different negative training triples that we discussed in Sec. 5.,7.1 Negative Training Pairs,[0],[0]
"Table 6 shows the comparison between one negative
11Liu et al. (2016b) discussed that BLEU and most phrase matching metrics are not good for evaluating dialogue systems.",7.1 Negative Training Pairs,[0],[0]
"Also, generative models have very low phrasematching metric scores because the generated response can be valid but still very different from the ground truth reference (Lowe et al., 2015; Liu et al., 2016b; Li et al., 2016).",7.1 Negative Training Pairs,[0],[0]
"We present results for the relatively better metrics like paraphrase-enabled METEOR for completeness, but still focus on retrieval recall@k and human evaluation.
",7.1 Negative Training Pairs,[0],[0]
"training triple (with just a negative response) vs. three negative training triples (one with negative video context, one with negative chat context, and another with negative response), showing that using the 3-negative examples setup is substantially better.",7.1 Negative Training Pairs,[0],[0]
Table 7 shows the performance comparison between the classification loss and max-margin loss on our TriDAF with self-attention discriminative model (Sec. 4.2.2).,7.2 Discriminative Loss Functions,[0],[0]
"We observe that max-margin loss performs better than the classification loss, which is intuitive because max-margin loss tries to differentiate between positive and negative training example triples.",7.2 Discriminative Loss Functions,[0],[0]
"For our best generative model (BiDAF), Table 8 shows that using a joint loss of cross-entropy and max-margin is better than just using only cross-entropy loss optimization (Sec. 4.3.1).",7.3 Generative Loss Functions,[0],[0]
"Maxmargin loss provides knowledge about the negative samples for the generative model, hence improves the retrieval-based recall@k scores.",7.3 Generative Loss Functions,[0],[0]
"Finally, we show some interesting output examples from both our discriminative and generative models as shown in Fig. 8.",7.4 Attention Visualization and Examples,[0],[0]
"Additionally, Fig. 9
visualizes that our models can learn some correct attention alignments from the generated output response word to the appropriate (goal-related) video frames as well as chat context words.",7.4 Attention Visualization and Examples,[0],[0]
"We presented a new game-chat based videocontext, many-speaker dialogue task and dataset.",8 Conclusion,[0],[0]
We also presented several baselines and state-ofthe-art discriminative and generative models on this task.,8 Conclusion,[0],[0]
We hope that this testbed will be a good starting point to encourage future work on the challenging video-context dialogue paradigm.,8 Conclusion,[0],[0]
"In future work, we plan to investigate the effects of multiple users, i.e., the multi-party aspect of this dataset.",8 Conclusion,[0],[0]
"We also plan to explore advanced video features such as activity recognition, person identification, etc.",8 Conclusion,[0],[0]
We thank the reviewers for their helpful comments.,Acknowledgments,[0],[0]
"This work was supported by DARPA YFA17-D17AP00022, ARO-YIP Award W911NF-18-1-0336, Google Faculty Research Award, Bloomberg Data Science Research Grant, and NVidia GPU awards.",Acknowledgments,[0],[0]
"The views, opinions, and/or findings contained in this article are those of the authors and should not be interpreted as representing the official views or policies, either expressed or implied, of the funding agency.",Acknowledgments,[0],[0]
Current dialogue systems focus more on textual and speech context knowledge and are usually based on two speakers.,abstractText,[0],[0]
Some recent work has investigated static image-based dialogue.,abstractText,[0],[0]
"However, several real-world human interactions also involve dynamic visual context (similar to videos) as well as dialogue exchanges among multiple speakers.",abstractText,[0],[0]
"To move closer towards such multimodal conversational skills and visually-situated applications, we introduce a new video-context, many-speaker dialogue dataset based on livebroadcast soccer game videos and chats from Twitch.tv.",abstractText,[0],[0]
"This challenging testbed allows us to develop visually-grounded dialogue models that should generate relevant temporal and spatial event language from the live video, while also being relevant to the chat history.",abstractText,[0],[0]
"For strong baselines, we also present several discriminative and generative models, e.g., based on tridirectional attention flow (TriDAF).",abstractText,[0],[0]
"We evaluate these models via retrieval ranking-recall, automatic phrasematching metrics, as well as human evaluation studies.",abstractText,[0],[0]
"We also present dataset analyses, model ablations, and visualizations to understand the contribution of different modalities and model components.",abstractText,[0],[0]
Game-Based Video-Context Dialogue,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1181–1189 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1181",text,[0],[0]
"In constituency parsing, refining coarse syntactic categories of treebank grammars (Charniak, 1996) into fine-grained subtypes has been proven effective in improving parsing results.",1 Introduction,[0],[0]
"Previous approaches to refining syntactic categories use tree annotations (Johnson, 1998), lexicalization (Charniak, 2000; Collins, 2003), or linguistically motivated category splitting (Klein and Manning, 2003).",1 Introduction,[0],[0]
"Matsuzaki et al. (2005) introduce latent variable grammars, in which each syntactic category (represented by a nonterminal) is split into
a fixed number of subtypes and a discrete latent variable is used to indicate the subtype of the nonterminal when it appears in a specific parse tree.",1 Introduction,[0],[0]
"Since the latent variables are not observable in treebanks, the grammar is learned using expectation-maximization.",1 Introduction,[0],[0]
"Petrov et al. (2006) present a split-merge approach to learning latent variable grammars, which hierarchically splits each nonterminal and merges ineffective splits.",1 Introduction,[0],[0]
"Petrov and Klein (2008b) further allow a nonterminal to have different splits in different production rules, which results in a more compact grammar.
",1 Introduction,[0],[0]
"Recently, neural approaches become very popular in natural language processing (NLP).",1 Introduction,[0],[0]
An important technique in neural approaches to NLP is to represent discrete symbols such as words and syntactic categories with continuous vectors or embeddings.,1 Introduction,[0],[0]
"Since the distances between such vector representations often reflect the similarity between the corresponding symbols, this technique facilitates more informed smoothing in learning functions of symbols (e.g., the probability of a production rule).",1 Introduction,[0],[0]
"In addition, what a symbol represents may subtly depend on its context, and a continuous vector representation has the potential of representing each instance of the symbol in a more precise manner.",1 Introduction,[0],[0]
"For constituency parsing, recursive neural networks (Socher et al., 2011) and their extensions such as compositional vector grammars (Socher et al., 2013) can be seen as representing nonterminals in a context-free grammar with continuous vectors.",1 Introduction,[0],[0]
"However, exact inference in these models is intractable.
",1 Introduction,[0],[0]
"In this paper, we introduce latent vector grammars (LVeGs), a novel framework of grammars with fine-grained nonterminal subtypes.",1 Introduction,[0],[0]
A LVeG associates each nonterminal with a continuous vector space that represents the set of (infinitely many) subtypes of the nonterminal.,1 Introduction,[0],[0]
"For each in-
stance of a nonterminal that appears in a parse tree, its subtype is represented by a latent vector.",1 Introduction,[0],[0]
"For each production rule over nonterminals, a nonnegative continuous function specifies the weight of any fine-grained production rule over subtypes of the nonterminals.",1 Introduction,[0],[0]
"Compared with latent variable grammars which assume a small fixed number of subtypes for each nonterminal, LVeGs assume an unlimited number of subtypes and are potentially more expressive.",1 Introduction,[0],[0]
"By having weight functions of varying smoothness for different production rules, LVeGs can also control the level of subtype granularity for different productions, which has been shown to improve the parsing accuracy (Petrov and Klein, 2008b).",1 Introduction,[0],[0]
"In addition, similarity between subtypes of a nonterminal can be naturally modeled by the distance between the corresponding vectors, so by using continuous and smooth weight functions we can ensure that similar subtypes will have similar syntactic behaviors.
",1 Introduction,[0],[0]
"We further present Gaussian Mixture LVeGs (GM-LVeGs), a special case of LVeGs that uses mixtures of Gaussian distributions as the weight functions of fine-grained production rules.",1 Introduction,[0],[0]
A major advantage of GM-LVeGs is that the partition function and the expectations of fine-grained production rules can be computed using an extension of the inside-outside algorithm.,1 Introduction,[0],[0]
This makes it possible to efficiently compute the gradients during discriminative learning of GM-LVeGs.,1 Introduction,[0],[0]
"We evaluate GM-LVeGs on part-of-speech tagging and constituency parsing on a variety of languages and corpora and show that GM-LVeGs achieve competitive results.
",1 Introduction,[0],[0]
"It shall be noted that many modern state-ofthe-art constituency parsers predict how likely a constituent is based on not only local information (such as the production rules used in composing the constituent), but also contextual information of the constituent.",1 Introduction,[0],[0]
"For example, the neural CRF parser (Durrett and Klein, 2015) looks at the words before and after the constituent; and RNNG (Dyer et al., 2016) looks at the constituents that are already predicted (in the stack) and the words that are not processed (in the buffer).",1 Introduction,[0],[0]
"In this paper, however, we choose to focus on the basic framework and algorithms of LVeGs and leave the incorporation of contextual information for future work.",1 Introduction,[0],[0]
"We believe that by laying a solid foundation for LVeGs, our work can pave the way for many interesting extensions of LVeGs in the future.",1 Introduction,[0],[0]
A latent vector grammar (LVeG) considers subtypes of nonterminals as continuous vectors and associates each nonterminal with a latent vector space representing the set of its subtypes.,2 Latent Vector Grammars,[0],[0]
"For each production rule, the LVeG defines a weight function over the subtypes of the nonterminal involved in the production rule.",2 Latent Vector Grammars,[0],[0]
"In this way, it models the space of refinements of the production rule.",2 Latent Vector Grammars,[0],[0]
"A latent vector grammar is defined as a 5-tuple G = (N,S,Σ, R,W ), where N is a finite set of nonterminal symbols, S ∈ N is the start symbol, Σ is a finite set of terminal symbols such that N∩Σ = ∅,R is a set production rules of the form X γ",2.1 Model Definition,[0],[0]
where X ∈ N,2.1 Model Definition,[0],[0]
"and γ ∈ (N ∪ Σ)∗, W is a set of rule weight functions indexed by production rules in R (to be defined below).",2.1 Model Definition,[0],[0]
"In the following discussion, we consider R in the Chomsky normal form (CNF) for clarity of presentation.",2.1 Model Definition,[0],[0]
"However, it is straightforward to extend our formulation to the general case.
",2.1 Model Definition,[0],[0]
"Unless otherwise specified, we always use capital letters A,B,C, . . .",2.1 Model Definition,[0],[0]
"for nonterminal symbols and use bold lowercase letters a,b, c, . . .",2.1 Model Definition,[0],[0]
for their subtypes.,2.1 Model Definition,[0],[0]
Note that subtypes are represented by continuous vectors.,2.1 Model Definition,[0],[0]
"For a production rule of the form A BC, its weight function is WA BC(a,b, c).",2.1 Model Definition,[0],[0]
"For a production rule of the form A w where w ∈ Σ, its weight function is WA w(a).",2.1 Model Definition,[0],[0]
"The weight functions should be non-negative, continuous and smooth, and hence fine-grained production rules of similar subtypes of a nonterminal would have similar weight assignments.",2.1 Model Definition,[0],[0]
"Rule weights can be normalized such that ∑ B,C ∫ b,cWA BC(a,b, c)dbdc = 1, which leads to a probabilistic context-free grammar (PCFG).",2.1 Model Definition,[0],[0]
Whether the weights are normalized or not leads to different model classes and accordingly different estimation methods.,2.1 Model Definition,[0],[0]
"However, the two model classes are proven equivalent by Smith and Johnson (2007).",2.1 Model Definition,[0],[0]
"Latent variable grammars (LVGs) (Matsuzaki et al., 2005; Petrov et al., 2006) associate each nonterminal with a discrete latent variable, which is used to indicate the subtype of the nonterminal when it appears in a parse tree.",2.2 Relation to Other Models,[0],[0]
"Through nonterminal-splitting and the
expectation-maximization algorithm, fine-grained production rules can be automatically induced from a treebank.
",2.2 Relation to Other Models,[0],[0]
We show that LVGs can be seen as a special case of LVeGs.,2.2 Relation to Other Models,[0],[0]
"Specifically, we can use one-hot vectors in LVeGs to represent latent variables in LVGs and define weight functions in LVeGs accordingly.",2.2 Relation to Other Models,[0],[0]
Consider a production rule r : A BC.,2.2 Relation to Other Models,[0],[0]
"In a LVG, each nonterminal is split into a number of subtypes.",2.2 Relation to Other Models,[0],[0]
"Suppose A, B, and C are split into nA, nB , and nC subtypes respectively.",2.2 Relation to Other Models,[0],[0]
"ax is the x-th subtype of A, by is the y-th subtype of B, and cz is the z-th subtype of C. ax bycz is a finegrained production rule of A BC, where x = 1, . . .",2.2 Relation to Other Models,[0],[0]
", nA, y = 1, . . .",2.2 Relation to Other Models,[0],[0]
", nB , and z = 1, . .",2.2 Relation to Other Models,[0],[0]
.,2.2 Relation to Other Models,[0],[0]
", nC .",2.2 Relation to Other Models,[0],[0]
The probabilities of all the fine-grained production rules can be represented by a rank-3 tensor ΘA BC ∈ RnA×nB×nC .,2.2 Relation to Other Models,[0],[0]
"To cast the LVG as a LVeG, we require that the latent vectors in the LVeG must be one-hot vectors.",2.2 Relation to Other Models,[0],[0]
We achieve this by defining weight functions that output zero if any of the input vectors is not one-hot.,2.2 Relation to Other Models,[0],[0]
"Specifically, we define the weight function of the production rule A BC as:
Wr(a,b, c) = ∑ x,y,z ΘA BCcba× (δ(a− ax)
× δ(b− by)× δ(c− cz)) , (1)
where δ(·) is the Dirac delta function, ax ∈ RnA , by ∈ RnB , cz ∈",2.2 Relation to Other Models,[0],[0]
"RnC are one-hot vectors (which are zero everywhere with the exception of a single 1 at the x-th index of ax, the y-th index of by, and the z-th index of cz) and ΘA BC is multiplied sequentially by c, b, and a.
",2.2 Relation to Other Models,[0],[0]
"Compared with LVGs, LVeGs have the following advantages.",2.2 Relation to Other Models,[0],[0]
"While a LVG contains a finite, typically small number of subtypes for each nonterminal, a LVeG uses a continuous space to represent an infinite number of subtypes.",2.2 Relation to Other Models,[0],[0]
"When equipped with weight functions of sufficient complexity, LVeGs can represent more fine-grained syntactic categories and production rules than LVGs.",2.2 Relation to Other Models,[0],[0]
"By controlling the complexity and smoothness of the weight functions, a LVeG is also capable of representing any level of subtype granularity.",2.2 Relation to Other Models,[0],[0]
"Importantly, this allows us to change the level of subtype granularity for the same nonterminal in different production rules, which is similar to multi-scale grammars (Petrov and Klein, 2008b).",2.2 Relation to Other Models,[0],[0]
"In addition, with a continuous space of subtypes in a LVeG, similarity between subtypes
can be naturally modeled by their distance in the space and can be automatically learned from data.",2.2 Relation to Other Models,[0],[0]
"Consequently, with continuous and smooth weight functions, fine-grained production rules over similar subtypes would have similar weights in LVeGs, eliminating the need for the extra smoothing steps that are necessary in training LVGs.
",2.2 Relation to Other Models,[0],[0]
"Compositional vector grammars (CVGs) (Socher et al., 2013), an extension of recursive neural networks (RNNs) (Socher et al., 2011), can also be seen as a special case of LVeGs.",2.2 Relation to Other Models,[0],[0]
"For a production rule r : A BC, a CVG can be interpreted as specifying its weight function Wr(a,b, c) in the following way.",2.2 Relation to Other Models,[0],[0]
"First, a neural network f indexed byB and C is used to compute a parent vector p = fBC(b, c).",2.2 Relation to Other Models,[0],[0]
"Next, the score of the parent vector is computed using a base PCFG and a vector vBC :
s(p) =",2.2 Relation to Other Models,[0],[0]
"vTBCp + logP (A BC) , (2)
where P (A BC) is the rule probability from the base PCFG.",2.2 Relation to Other Models,[0],[0]
"Then, the weight function of the production rule A BC is defined as:
Wr(a,b, c) = exp (s(p))× δ(a− p) .",2.2 Relation to Other Models,[0],[0]
"(3)
This form of weight functions in CVGs leads to point estimation of latent vectors in a parse tree, i.e., for each nonterminal in a given parse tree, only one subtype in the whole subtype space would lead to a non-zero weight of the parse.",2.2 Relation to Other Models,[0],[0]
"In addition, different parse trees of the same substring typically lead to different point estimations of the subtype vector at the root nonterminal.",2.2 Relation to Other Models,[0],[0]
"Consequently, CVGs cannot use dynamic programming for inference and hence have to resort to greedy search or beam search.",2.2 Relation to Other Models,[0],[0]
A major challenge in applying LVeGs to parsing is that it is impossible to enumerate the infinite number of subtypes.,3 Gaussian Mixture LVeGs,[0],[0]
Previous work such as CVGs resorts to point estimation and greedy search.,3 Gaussian Mixture LVeGs,[0],[0]
"In this section we present Gaussian Mixture LVeGs (GMLVeGs), which use mixtures of Gaussian distributions as the weight functions in LVeGs.",3 Gaussian Mixture LVeGs,[0],[0]
"Because Gaussian mixtures have the nice property of being closed under product, summation, and marginalization, we can compute the partition function and the expectations of fine-grained production rules using dynamic programming.",3 Gaussian Mixture LVeGs,[0],[0]
This in turn makes efficient learning and parsing possible.,3 Gaussian Mixture LVeGs,[0],[0]
"In a GM-LVeG, the weight function of a production rule r is defined as a Gaussian mixture containing Kr mixture components:
Wr(r) =",3.1 Representation,[0],[0]
"Kr∑ k=1 ρr,kN (r|µr,k,Σr,k) , (4)
where r is the concatenation of the latent vectors of the nonterminals in r, which denotes a finegrained production rule of r. ρr,k > 0 is the k-th mixture weight (the mixture weights do not necessarily sum up to 1), N (r|µr,k,Σr,k) is the k-th Gaussian distribution parameterized by mean µr,k and covariance matrix Σr,k, and Kr is the number of mixture components, which can be different for different production rules.",3.1 Representation,[0],[0]
"Below we write N (r|µr,k,Σr,k) as Nr,k(r) for brevity.",3.1 Representation,[0],[0]
"Given a production rule of the form A BC, the GMLVeG expects r =",3.1 Representation,[0],[0]
"[a; b; c] and a,b, c ∈ Rd, where d is the dimension of the vectors a,b, c. We use the same dimension for all the subtype vectors.
",3.1 Representation,[0],[0]
"For the sake of computational efficiency, we use diagonal or spherical Gaussian distributions, whose covariance matrices are diagonal, so that the inverse of covariance matrices in Equation 15– 16 can be computed in linear time.",3.1 Representation,[0],[0]
"A spherical Gaussian has a diagonal covariance matrix where all the diagonal elements are equal, so it has fewer free parameters than a diagonal Gaussian and results in faster learning and parsing.",3.1 Representation,[0],[0]
We empirically find that spherical Gaussians lead to slightly better balance between the efficiency and the parsing accuracy than diagonal Gaussians.,3.1 Representation,[0],[0]
The goal of parsing is to find the most probable parse tree T ∗ with unrefined nonterminals for a sentence w of n words w1:n = w1 . . .,3.2 Parsing,[0],[0]
wn.,3.2 Parsing,[0],[0]
"This is formally defined as:
T ∗ = argmax T∈G(w) P (T |w) , (5)
where G(w) denotes the set of parse trees with unrefined nonterminals for w.",3.2 Parsing,[0],[0]
"In a PCFG, T ∗ can be found using dynamic programming such as the CYK algorithm.",3.2 Parsing,[0],[0]
"However, parsing becomes intractable with LVeGs, and even with LVGs, the special case of LVeGs.
",3.2 Parsing,[0],[0]
"A common practice in parsing with LVGs is to use max-rule parsing (Petrov et al., 2006; Petrov
and Klein, 2007).",3.2 Parsing,[0],[0]
The basic idea of max-rule parsing is to decompose the posteriors over parses into the posteriors over production rules approximately.,3.2 Parsing,[0],[0]
This requires calculating the expected counts of unrefined production rules in parsing the input sentence.,3.2 Parsing,[0],[0]
"Since Gaussian mixtures are closed under product, summation, and marginalization, in GM-LVeGs the expected counts can be calculated using the inside-outside algorithm in the following way.",3.2 Parsing,[0],[0]
"Given a sentence w1:n, we first calculate the inside score sAI",3.2 Parsing,[0],[0]
"(a, i, j) and outside score sAO(a, i, j) for a nonterminal A over a span wi:j using Equation 6 and Equation 7 in Table 1 respectively.",3.2 Parsing,[0],[0]
"Note that both sAI (a, i, j) and sAO(a, i, j) are mixtures of Gaussian distributions of the subtype vector a. Next, using Equation 8 in Table 1, we calculate the score s(A BC, i, k, j) (1 ≤",3.2 Parsing,[0],[0]
i ≤,3.2 Parsing,[0],[0]
k,3.2 Parsing,[0],[0]
"< j ≤ n), where 〈A BC, i, k, j〉 represents a production ruleA BC with nonterminalsA,B, andC spanning wordswi:j ,wi,k, and wk+1:j respectively in the sentence w1:",3.2 Parsing,[0],[0]
"n. Then the expected count (or posterior) of 〈A BC, i, k, j〉 is calculated as:
q(A BC, i, k, j) = s(A BC, i, k, j) sI(S, 1, n) , (9)
where sI(S, 1, n) is the inside score for the start symbol S spanning the whole sentence w1:n. After calculating all the expected counts, we can use the MAX-RULE-PRODUCT algorithm (Petrov and Klein, 2007) for parsing, which returns a parse with the highest probability that all the production rules are correct.",3.2 Parsing,[0],[0]
"Its objective function is given by
T ∗q = argmax T∈G(w) ∏ e∈T q(e) , (10)
where e ranges over all the 4-tuples 〈A BC, i, k, j〉 in the parse tree T .",3.2 Parsing,[0],[0]
"This objective function can be efficiently solved by dynamic programming such as the CYK algorithm.
",3.2 Parsing,[0],[0]
"Although the time complexity of the insideoutside algorithm with GM-LVeGs is polynomial in the sentence length and the nonterminal number, in practice the algorithm is still slow because the number of Gaussian components in the inside and outside scores increases dramatically with the recursion depth.",3.2 Parsing,[0],[0]
"To speed up the computation, we prune Gaussian components in the inside and outside scores using the following technique.",3.2 Parsing,[0],[0]
"Suppose we have a minimum pruning threshold kmin
and a maximum pruning threshold kmax.",3.2 Parsing,[0],[0]
"Given an inside or outside score with kc Gaussian components, if kc ≤ kmin, then we do not prune any Gaussian component; otherwise, we compute kallow = min{kmin + floor(kϑc ), kmax} (0 ≤ ϑ ≤ 1 is a constant) and keep only kallow components with the largest mixture weights.
",3.2 Parsing,[0],[0]
"In addition to component pruning, we also employ two constituent pruning techniques to reduce the search space during parsing.",3.2 Parsing,[0],[0]
The first technique is used by Petrov et al. (2006).,3.2 Parsing,[0],[0]
"Before parsing a sentence with a GM-LVeG, we run the inside-outside algorithm with the treebank grammar and calculate the posterior probability of every nonterminal spanning every substring.",3.2 Parsing,[0],[0]
Then a nonterminal would be pruned from a span if its posterior probability is below a pre-specified threshold pmin.,3.2 Parsing,[0],[0]
"When parsing with GM-LVeGs, we only consider the unpruned nonterminals for each span.
",3.2 Parsing,[0],[0]
The second constituent pruning technique is similar to the one used by Socher et al. (2013).,3.2 Parsing,[0],[0]
"Note that for a strong constituency parser such as the Berkeley parser (Petrov and Klein, 2007), the constituents in the top 200 best parses of a sentence can cover almost all the constituents in the gold parse tree.",3.2 Parsing,[0],[0]
So we first use an existing constituency parser to run k-best parsing with k = 200 on the input sentence.,3.2 Parsing,[0],[0]
Then we parse with a GM-LVeG and only consider the constituents that appear in the top 200 parses.,3.2 Parsing,[0],[0]
Note that this method is different from the re-ranking technique because it may produce a parse different from the top 200 parses.,3.2 Parsing,[0],[0]
"Given a training dataset D = {(Ti,wi) | i = 1, . . .",3.3 Learning,[0],[0]
",m} containing m samples, where Ti is the gold parse tree with unrefined nonterminals for the sentence wi, the objective of discriminative learning is to minimize the negative log conditional likelihood:
L(Θ) =",3.3 Learning,[0],[0]
− log m∏ i=1,3.3 Learning,[0],[0]
"P (Ti|wi; Θ) , (11)
where Θ represents the set of parameters of the GM-LVeG.
We optimize the objective function using the Adam (Kingma and Ba, 2014) optimization algorithm.",3.3 Learning,[0],[0]
"The derivative with respect to Θr, the parameters of the weight function Wr(r) of an unrefined production rule r, is calculated as follows (the derivation is in the supplementary material):
∂L(Θ) ∂Θr = m∑ i=1 ∫",3.3 Learning,[0],[0]
"( ∂Wr(r) ∂Θr (12)
× EP (t|wi)[fr(t)]− EP (t|Ti)[fr(t)]
Wr(r)
)",3.3 Learning,[0],[0]
"dr ,
where t indicates a parse tree with nonterminal subtypes, and fr(t) is the number of occurrences of the unrefined rule r in the unrefined parse tree that is obtained by replacing all the subtypes in t with the corresponding nonterminals.",3.3 Learning,[0],[0]
The two expectations in Equation 12 can be efficiently computed using the inside-outside algorithm.,3.3 Learning,[0],[0]
"Because the second expectation is conditioned on the parse tree Ti, in Equation 6 and Equation 7 we can skip all the summations and assign the values of B, C, and k according to Ti.
",3.3 Learning,[0],[0]
"In GM-LVeGs, Θr is the set of parameters in a Gaussian mixture:
",3.3 Learning,[0],[0]
"Θr = {(ρr,k,µr,k,Σr,k)|k = 1, . . .",3.3 Learning,[0],[0]
",Kr} .",3.3 Learning,[0],[0]
"(13)
",3.3 Learning,[0],[0]
"According to Equation 12, we need to take the derivatives ofWr(r) respect to ρr,k, µr,k, and Σr,k respectively:
∂Wr(r)/∂ρr,k = Nr,k(r) , (14) ∂Wr(r)/∂µr,k = ρr,kNr,k(r)Σ−1r,k(r− µr,k) ,(15) ∂Wr(r)/∂Σr,k = ρr,kNr,k(r)Σ−1r,k 1
2
( − I (16)
+ (r− µr,k)(r− µr,k)TΣ−1r,k ) .
",3.3 Learning,[0],[0]
"Substituting Equation 14–16 into Equation 12, we have the full gradient formulations of all the parameters.",3.3 Learning,[0],[0]
"In spite of the integral in Equation 12, we can derive a closed-form solution for the gradient of each parameter, which is shown in the supplementary material.
",3.3 Learning,[0],[0]
"In order to keep each mixture weight ρr,k positive, we do not directly optimize ρr,k; instead, we set ρr,k = exp(θρr,k) and optimize θρr,k by gradient descent.",3.3 Learning,[0],[0]
"We use a similar trick to keep each covariance matrix Σr,k positive definite.
",3.3 Learning,[0],[0]
"Since we use the inside-outside algorithm described in Section 3.2 to calculate the two expectations in Equation 12, we face the same efficiency problem that we encounter in parsing.",3.3 Learning,[0],[0]
"To speed up the computation,we again use both component pruning and constituent pruning introduced in Section 3.2.
",3.3 Learning,[0],[0]
"Because gradient descent is often sensitive to the initial values of the parameters, we employ the following informed initialization method.",3.3 Learning,[0],[0]
Mixture weights are initialized using the treebank grammar.,3.3 Learning,[0],[0]
"Suppose in the treebank grammar P (r) is the probability of a production rule r. We initialize the mixture weights in the weight function Wr by ρr,k = α · P (r) where α > 1 is a constant.",3.3 Learning,[0],[0]
"We initialize all the covariance matrices to identity matrices and initialize each mean with a value uniformly sampled from [−0.05, 0.05].",3.3 Learning,[0],[0]
We evaluate the GM-LVeG on part-of-speech (POS) tagging and constituency parsing and compare it against its special cases such as LVGs and CVGs.,4 Experiment,[0],[0]
"It shall be noted that in this paper we focus on the basic framework of LVeGs and aim to show
its potential advantage over previous special cases.",4 Experiment,[0],[0]
It is therefore not our goal to compete with the latest state-of-the-art approaches to tagging and parsing.,4 Experiment,[0],[0]
"In particular, we currently do not incorporate contextual information of words and constituents during tagging and parsing, while such information is critical in achieving state-of-the-art accuracy.",4 Experiment,[0],[0]
We will discuss future improvements of LVeGs in Section 5.,4 Experiment,[0],[0]
Parsing.,4.1 Datasets,[0],[0]
"We use the Wall Street Journal corpus from the Penn English Treebank (WSJ) (Marcus et al., 1994).",4.1 Datasets,[0],[0]
"Following the standard data splitting, we use sections 2 to 21 for training, section 23 for testing, and section 22 for development.",4.1 Datasets,[0],[0]
"We preprocess the treebank using a right-branching binarization procedure to obtain an unannotated X-bar grammar, so that there are only binary and unary production rules.",4.1 Datasets,[0],[0]
"To deal with the problem of unknown words in testing, we adopt the unknown word features used in the Berkeley parser and set the unknown word threshold to 1.",4.1 Datasets,[0],[0]
"Specifically, any word occurring less than two times is replaced by one of the 60 unknown word categories.",4.1 Datasets,[0],[0]
Tagging.,4.1 Datasets,[0],[0]
"(1) We use Wall Street Journal corpus from the Penn English Treebank (WSJ) (Marcus et al., 1994).",4.1 Datasets,[0],[0]
"Following the standard data splitting, we use sections 0 to 18 for training, sections 22 to 24 for testing, and sections 19 to 21 for development.",4.1 Datasets,[0],[0]
"(2) The Universal Dependencies treebank 1.4 (UD) (Nivre et al., 2016), in which English, French, German, Russian, Spanish, Indonesian, Finnish, and Italian treebanks are used.",4.1 Datasets,[0],[0]
We use the original data splitting of these corpora for training and testing.,4.1 Datasets,[0],[0]
"For both WSJ and UD English treebanks, we deal with unknown words in the same way as we do in parsing.",4.1 Datasets,[0],[0]
"For the rest of the data, we use only one unknown word category and the unknown word threshold is also set to 1.",4.1 Datasets,[0],[0]
POS tagging is the task of labeling each word in a sentence with the most probable part-of-speech tag.,4.2 POS Tagging,[0],[0]
Here we focus on POS tagging with Hidden Markov Models (HMMs).,4.2 POS Tagging,[0],[0]
"Because HMMs are equivalent to probabilistic regular grammars, we can extend HMMs with both LVGs and LVeGs.",4.2 POS Tagging,[0],[0]
"Specifically, the hidden states in HMMs can be seen as nonterminals in regular grammars and therefore can be associated with latent variables or latent vectors.
",4.2 POS Tagging,[0],[0]
We implement two training methods for LVGs.,4.2 POS Tagging,[0],[0]
The first (LVG-G) is generative training using expectation-maximization that maximizes the joint probability of the sentence and the tags.,4.2 POS Tagging,[0],[0]
The second (LVG-D) is discriminative training using gradient descent that maximizes the conditional probability of the tags given the sentence.,4.2 POS Tagging,[0],[0]
"In both cases, each nonterminal is split into a fixed number of subtypes.",4.2 POS Tagging,[0],[0]
"In our experiments we test 1, 2, 4, 8, and 16 subtypes of each nonterminal.",4.2 POS Tagging,[0],[0]
"Due to the limited space, we only report experimental results of LVG with 16 subtypes for each nonterminal.",4.2 POS Tagging,[0],[0]
"Full experimental results can be found in the supplementary material.
",4.2 POS Tagging,[0],[0]
We experiment with two different GM-LVeGs: GM-LVeG-D with diagonal Gaussians and GMLVeG-S with spherical Gaussians.,4.2 POS Tagging,[0],[0]
"In both cases, we fix the number of Gaussian components Kr to 4 and the dimension of the latent vectors d to 3.",4.2 POS Tagging,[0],[0]
"We do not use any pruning techniques in learning and inference because we find that our algorithm is fast enough with the current setting of Kr and d. We train the GM-LVeGs for 20 epoches and select the models with the best token accuracy on the development data for the final testing.
",4.2 POS Tagging,[0],[0]
We report both token accuracy and sentence accuracy of POS tagging in Table 2.,4.2 POS Tagging,[0],[0]
"It can be seen that, on all the testing data, GM-LVeGs consistently surpass LVGs in terms of both token accuracy and sentence accuracy.",4.2 POS Tagging,[0],[0]
"GM-LVeG-D is slightly better than GM-LVeG-S in sentence accuracy, producing the best sentence accuracy on 5 of the 9 testing datasets.",4.2 POS Tagging,[0],[0]
GM-LVeG-S performs slightly better than GM-LVeG-D in token accuracy on 5 of the 9 datasets.,4.2 POS Tagging,[0],[0]
"Overall, there is not significant difference between GM-LVeG-D and GMLVeG-S. However, GM-LVeG-S admits more efficient learning than GM-LVeG-D in practice since it has fewer parameters.",4.2 POS Tagging,[0],[0]
"For efficiency, we train GM-LVeGs only on sentences with no more than 50 words (totally 39115 sentences).",4.3 Parsing,[0],[0]
"Since we have found that spherical Gaussians are better than diagonal Gaussians considering both model performance and learning efficiency, here we use spherical Gaussians in the weight functions.",4.3 Parsing,[0],[0]
"The dimension of latent vectors d is set to 3, and all the Gaussian mixtures have Kr = 4 components.",4.3 Parsing,[0],[0]
We use α = 8 in initializing mixture weights.,4.3 Parsing,[0],[0]
"We train the GM-LVeG for 15
epoches and select the model with the highest F1 score on the development data for the final testing.",4.3 Parsing,[0],[0]
"We use component pruning in both learning and parsing, with kmax = 50 and ϑ = 0.35 in both learning and parsing, kmin = 40 in learning and kmin = 20 in parsing.",4.3 Parsing,[0],[0]
During learning we use the first constituent pruning technique with the pruning threshold pmin = 1e,4.3 Parsing,[0],[0]
"− 5, and during parsing we use the second constituent pruning technique based on the Berkeley parser which produced 133 parses on average for each testing sentence.",4.3 Parsing,[0],[0]
"As can be seen, we use weaker pruning during training than during testing.",4.3 Parsing,[0],[0]
"This is because in training stronger pruning (even if accurate) results in worse estimation of the first expectation in Equation 12, which makes gradient computation less accurate.
",4.3 Parsing,[0],[0]
"We compare LVeGs with CVGs and several variants of LVGs: (1) LVG-G-16 and LVG-D-16, which are LVGs with 16 subtypes for each nonterminal with discriminative and generative training respectively (accuracies obtained from Petrov and Klein (2008a)); (2) Multi-scale grammars (Petrov and Klein, 2008b), trained without using the span features in order for a fair comparison; (3) Berkeley parser (Petrov and Klein, 2007) (accuracies obtained from Petrov and Klein (2008b) because Petrov and Klein (2007) do not report exact match scores).",4.3 Parsing,[0],[0]
The experimental results are shown in Table 3.,4.3 Parsing,[0],[0]
It can be seen that GM-LVeG-S produces the best F1 scores on both the development data and the testing data.,4.3 Parsing,[0],[0]
It surpasses the Berkeley parser by 0.92% in F1 score on the testing data.,4.3 Parsing,[0],[0]
"Its exact match score on the testing data is only slightly lower than that of LVG-D-16.
",4.3 Parsing,[0],[0]
We further investigate the influence of the latent vector dimension and the Gaussian component number on the efficiency and the parsing accuracy .,4.3 Parsing,[0],[0]
We experiment on a small dataset (statistics of this dataset are in the supplemental material).,4.3 Parsing,[0],[0]
"We first fix the component number to 4 and experiment with the dimension 2, 3, 4, 5, 6, 7, 8, 9.",4.3 Parsing,[0],[0]
"Then we fix the dimension to 3 and experiment with the component number 2, 3, 4, 5, 6, 7, 8, 9.",4.3 Parsing,[0],[0]
F1 scores on the development data are shown in the first row in Figure 1.,4.3 Parsing,[0],[0]
Average time consumed per epoch in learning is shown in the second row in Figure 1.,4.3 Parsing,[0],[0]
"When Kr = 4, the best dimension is 5;",4.3 Parsing,[0],[0]
"when d = 3, the best Gaussian component number is 3.",4.3 Parsing,[0],[0]
A higher dimension or a larger Gaussian component number hurts the model performance and requires much more time for learning.,4.3 Parsing,[0],[0]
"Thus
our choice ofKr = 4 and d = 3 in GM-LVeGs for parsing is a good balance between the efficiency and the parsing accuracy.",4.3 Parsing,[0],[0]
"It shall be noted that in this paper we choose to focus on the basic framework and algorithms of LVeGs, and therefore we leave a few important extensions for future work.",5 Discussion,[0],[0]
One extension is to incorporate contextual information of words and constituents.,5 Discussion,[0],[0]
which is a crucial technique that can be found in most state-of-the-art approaches to parsing or POS tagging.,5 Discussion,[0],[0]
"One possible way to uti-
lize contextual information in LVeGs is to allow the words in the context of an anchored production rule to influence the rule’s weight function.",5 Discussion,[0],[0]
"For example, we may learn neural networks to predict the parameters of the Gaussian mixture weight functions in a GM-LVeG from the pre-trained embeddings of the words in the context.
",5 Discussion,[0],[0]
"In GM-LVeGs, we currently use the same number of Gaussian components for all the weight functions.",5 Discussion,[0],[0]
"A more desirable way would be automatically determining the number of Gaussian components for each production rule based on the ideal refinement granularity of the rule, e.g., we may need more Gaussian components for NP DT NN than for NP DT JJ, since the latter is rarely used.",5 Discussion,[0],[0]
"There are a few possible ways to learn the component numbers such as greedy addition and removal, the split-merge method, and sparsity priors over mixture weights.
",5 Discussion,[0],[0]
An interesting extension beyond LVeGs is to have a single continuous space for subtypes of all the nonterminals.,5 Discussion,[0],[0]
"Ideally, subtypes of the same nonterminal or similar nonterminals are close to each other.",5 Discussion,[0],[0]
The benefit is that similarity between nonterminals can now be modeled.,5 Discussion,[0],[0]
We present Latent Vector Grammars (LVeGs) that associate each nonterminal with a latent continuous vector space representing the set of subtypes of the nonterminal.,6 Conclusion,[0],[0]
"For each production rule, a LVeG defines a continuous weight function over the subtypes of the nonterminals involved in the rule.",6 Conclusion,[0],[0]
We show that LVeGs can subsume latent variable grammars and compositional vector grammars as special cases.,6 Conclusion,[0],[0]
We then propose Gaussian mixture LVeGs (GM-LVeGs).,6 Conclusion,[0],[0]
which formulate weight functions of production rules by mixtures of Gaussian distributions.,6 Conclusion,[0],[0]
"The partition function and the expectations of fine-grained production rules in GM-LVeGs can be efficiently computed using dynamic programming, which makes learning and inference with GM-LVeGs feasible.
",6 Conclusion,[0],[0]
We empirically show that GM-LVeGs can achieve competitive accuracies on POS tagging and constituency parsing.,6 Conclusion,[0],[0]
"This work was supported by the National Natural Science Foundation of China (61503248), Major Program of Science and Technology Commission Shanghai Municipal (17JC1404102), and Program of Shanghai Subject Chief Scientist (A type) (No.15XD1502900).",Acknowledgments,[0],[0]
We would like to thank the anonymous reviewers for their careful reading and useful comments.,Acknowledgments,[0],[0]
"We introduce Latent Vector Grammars (LVeGs), a new framework that extends latent variable grammars such that each nonterminal symbol is associated with a continuous vector space representing the set of (infinitely many) subtypes of the nonterminal.",abstractText,[0],[0]
We show that previous models such as latent variable grammars and compositional vector grammars can be interpreted as special cases of LVeGs.,abstractText,[0],[0]
"We then present Gaussian Mixture LVeGs (GMLVeGs), a new special case of LVeGs that uses Gaussian mixtures to formulate the weights of production rules over subtypes of nonterminals.",abstractText,[0],[0]
"A major advantage of using Gaussian mixtures is that the partition function and the expectations of subtype rules can be computed using an extension of the inside-outside algorithm, which enables efficient inference and learning.",abstractText,[0],[0]
We apply GM-LVeGs to part-of-speech tagging and constituency parsing and show that GM-LVeGs can achieve competitive accuracies.,abstractText,[0],[0]
Our code is available at https://github.com/zhaoyanpeng/lveg.,abstractText,[0],[0]
Gaussian Mixture Latent Vector Grammars,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1828–1836, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Word alignment is a natural language processing task that aims to specify the correspondence between words in two languages (Brown et al., 1993).",1 Introduction,[0],[0]
"It plays an important role in statistical machine translation (SMT) as word-aligned bilingual corpora serve as the input of translation rule extraction (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006; Liu et al., 2006).
",1 Introduction,[0],[0]
"Although state-of-the-art generative alignment models (Brown et al., 1993; Vogel et al., 1996) have been widely used in practical SMT systems, they fail to model the symmetry of word alignment.",1 Introduction,[0],[0]
"While word alignments in real-world bilingual data usually exhibit complicated mappings (i.e., mixed with one-to-one, one-to-many, manyto-one, and many-to-many links), these models assume that each target word is aligned to exactly
∗Corresponding author: Yang Liu.
one source word.",1 Introduction,[0],[0]
"To alleviate this problem, heuristic methods (e.g., grow-diag-final) have been proposed to combine two asymmetric alignments (source-to-target and target-to-source) to generate symmetric bidirectional alignments (Och and Ney, 2003; Koehn and Hoang, 2007).
",1 Introduction,[0],[0]
"Instead of using heuristic symmetrization, Liang et al. (2006) introduce a principled approach that encourages the agreement between asymmetric alignments in two directions.",1 Introduction,[0],[0]
The basic idea is to favor links on which both unidirectional models agree.,1 Introduction,[0],[0]
"They associate two models via the agreement constraint and show that agreement-based joint training improves alignment accuracy significantly.
",1 Introduction,[0],[0]
"However, enforcing agreement in joint training faces a major problem: the two models are restricted to one-to-one alignments (Liang et al., 2006).",1 Introduction,[0],[0]
"This significantly limits the translation accuracy, especially for distantly-related language pairs such as Chinese-English (see Section 5).",1 Introduction,[0],[0]
"Although posterior decoding can potentially address this problem, Liang et al. (2006) find that many-to-many alignments occur infrequently because posteriors are sharply peaked around the Viterbi alignments.",1 Introduction,[0],[0]
"We believe that this happens because their model imposes a hard constraint on agreement: the two models must share the same alignment when estimating the parameters by calculating the products of alignment posteriors (see Section 2).
",1 Introduction,[0],[0]
"In this work, we propose a general framework for imposing agreement constraints in joint training of unidirectional models.",1 Introduction,[0],[0]
"The central idea is to use the expectation of a loss function, which measures the disagreement between two models, to replace the original probability of agreement.",1 Introduction,[0],[0]
This allows for many possible ways to quantify agreement.,1 Introduction,[0],[0]
"Experiments on Chinese-English translation show that our approach outperforms two state-ofthe-art baselines significantly.
1828",1 Introduction,[0],[0]
"Given a source-language sentence e ≡ eI1 = e1, . . .",2.1 Asymmetric Alignment Models,[0],[0]
", eI and a target-language sentence f ≡ fJ1 = f1, . . .",2.1 Asymmetric Alignment Models,[0],[0]
", fJ , a source-to-target translation model (Brown et al., 1993; Vogel et al., 1996) can be defined as
P (f |e; θ1) = ∑ a1 P (f ,a1|e; θ1) (1)
where a1 denotes the source-to-target alignment and θ1 is the set of source-to-target translation model parameters.
",2.1 Asymmetric Alignment Models,[0],[0]
"Likewise, the target-to-source translation model is given by
P (e|f ; θ2) = ∑ a2 P (e,a2|f ; θ2) (2)
where a2 denotes the target-to-source alignment and θ2 is the set of target-to-source translation model parameters.
",2.1 Asymmetric Alignment Models,[0],[0]
"Given a training set D = {〈f (s), e(s)〉}Ss=1, the two models are trained independently to maximize the log-likelihood of the training data for each direction, respectively:
L(θ1) = S∑
s=1
logP (f (s)|e(s); θ1) (3)
L(θ2) = S∑
s=1
logP (e(s)|f (s); θ2) (4)
",2.1 Asymmetric Alignment Models,[0],[0]
One key limitation of these generative models is that they are asymmetric: each target word is restricted to be aligned to exactly one source word (including the empty cept) in the sourceto-target direction and vice versa.,2.1 Asymmetric Alignment Models,[0],[0]
"This is undesirable because most real-world word alignments are symmetric, in which one-to-one, oneto-many, many-to-one, and many-to-many links are usually mixed.",2.1 Asymmetric Alignment Models,[0],[0]
See Figure 1(a) for example.,2.1 Asymmetric Alignment Models,[0],[0]
"Therefore, a number of heuristic symmetrization methods such as intersection, union, and growdiag-final have been proposed to combine asym-
metric alignments (Och and Ney, 2003; Koehn and Hoang, 2007).",2.1 Asymmetric Alignment Models,[0],[0]
"Rather than using heuristic symmetrization methods, Liang et al. (2006) propose a principled approach to jointly training of the two models via enforcing agreement:
J(θ1,θ2)
= S∑
s=1
logP (f (s)|e(s); θ1)",2.2 Alignment by Agreement,[0],[0]
"+
logP (e(s)|f (s); θ2) + log ∑ a P (a|f (s), e(s); θ1)×
P (a|e(s), f (s); θ2) (5) Note that the last term in Eq.",2.2 Alignment by Agreement,[0],[0]
(5) encourages the two models to agree on asymmetric alignments.,2.2 Alignment by Agreement,[0],[0]
"While this strategy significantly improves alignment accuracy, the joint model is prone to generate one-to-one alignments because it imposes a hard constraint on agreement: the two models must share the same alignment when estimating the parameters by calculating the products of alignment posteriors.",2.2 Alignment by Agreement,[0],[0]
"In Figure 1(b), the two oneto-one alignments are almost identical except for one link.",2.2 Alignment by Agreement,[0],[0]
"This makes the posteriors to be sharply peaked around the Viterbi alignments (Liang et al., 2006).",2.2 Alignment by Agreement,[0],[0]
"As a result, the lack of many-to-many alignments limits the benefits of joint training to end-to-end machine translation.",2.2 Alignment by Agreement,[0],[0]
"Our intuition is that the agreement between two alignments can be defined as a loss function, which enables us to consider various ways of quantification (Section 3.1) and even to incorporate the dependency between alignments and other latent structures such as phrase segmentations (Section 3.2).",3 Generalized Agreement for Bidirectional Alignment,[0],[0]
The key idea of generalizing agreement is to leverage loss functions that measure the difference between two unidirectional alignments.,3.1 Agreement between Word Alignments,[0],[0]
"For example, the last term in Eq.",3.1 Agreement between Word Alignments,[0],[0]
"(5) can be re-written as
∑ a P (a|f (s), e(s); θ1)P (a|e(s), f (s); θ2)
= ∑",3.1 Agreement between Word Alignments,[0],[0]
"a1 ∑ a2 P (a1|f (s), e(s); θ1)×
P (a2|e(s), f (s); θ2)× δ(a1,a2) (6)
Note that the last term in Eq.",3.1 Agreement between Word Alignments,[0],[0]
"(6) is actually the expected value of agreement:
Ea1|f (s),e(s);θ1 [ Ea2|e(s),f (s);θ2 [ δ(a1,a2) ]",3.1 Agreement between Word Alignments,[0],[0]
"] (7)
Our idea is to replace δ(a1,a2) in Eq.",3.1 Agreement between Word Alignments,[0],[0]
"(6) with an arbitrary loss function ∆(a1,a2) that measures the difference between a1 and a2.",3.1 Agreement between Word Alignments,[0],[0]
"This gives the new joint training objective with generalized agreement:
J(θ1,θ2)
= S∑
s=1
logP (f (s)|e(s); θ1) +
logP (e(s)|f (s); θ2)− log ∑ a1 ∑ a2 P (a1|f (s), e(s); θ1)×
P (a2|e(s), f (s); θ2)× ∆(a1,a2) (8)
Obviously, Liang et al. (2006)’s training objective is a special case of our framework.",3.1 Agreement between Word Alignments,[0],[0]
"We refer to its loss function as hard matching:
∆HM(a1,a2) = 1− δ(a1,a2) (9)
We are interested in developing a soft version of the hard matching loss function because this will help to produce many-to-many symmetric alignments.",3.1 Agreement between Word Alignments,[0],[0]
"For example, in Figure 1(c), the two alignments share most links but still allow for disagreed links to capture one-to-many and many-toone links.",3.1 Agreement between Word Alignments,[0],[0]
"Note that the union of the two asymmetric alignments is almost the same with the goldstandard alignment in this example.
",3.1 Agreement between Word Alignments,[0],[0]
"While there are many possible ways to define a soft matching loss function, we choose the difference between disagreed and agreed link counts because it is easy and efficient to calculate during search:
∆SM(a1,a2) = |a1 ∪ a2| − 2|a1 ∩ a2| (10)
CE EC",3.1 Agreement between Word Alignments,[0],[0]
"Our framework is very general and can be extended to include the agreement between word alignment and other latent structures such as phrase segmentations.
",3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
The words in a Chinese sentence often constitute phrases that are translated as units in English and vice versa.,3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
"Inspired by the alignment consistency constraint widely used in translation rule extraction (Koehn et al., 2003), we make the following assumption to impose a structural agreement constraint between word alignment and phrase segmentation: source words in one source phrase should be aligned to target words belonging to the same target phrase and vice versa.
",3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
"For example, consider the C→ E alignment in Figure 2.",3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
"We segment Chinese and English sentences into phrases, which are sequences of consecutive words.",3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
"Since “2002” and “APEC” belong to the same English phrase, their counterparts on the Chinese side should also belong to one phrase.
",3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
"While this assumption can potentially improve the correlation between word alignment and phrase-based translation, a question naturally arises: how to segment sentences into phrases?",3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
"Instead of leveraging chunking, we treat phrase segmentation as a latent variable and train the
joint alignment and segmentation model from unlabeled data in an unsupervised way.
",3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
"Formally, given a target-language sentence f ≡ fJ1 = f1, . . .",3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
", fJ , we introduce a latent variable b ≡ bJ1 = b1, . . .",3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
", bJ to denote a phrase segmentation.",3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
Each label bj ∈,3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
"{B, I,E, S}, where B denotes the beginning word of a phrase, I denotes the internal word, E denotes the ending word, and S denotes the one-word phrase.",3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
"Figure 2 shows the label sequences for the sentence pair.
",3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
"We use a first-order HMM to model phrase segmentation of a target sentence:
P (f ; λ1) = ∑ b1 P (f ,b1; λ1) (11)
",3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
"Similarly, the hidden Markov model for the phrase segmentation of the source sentence can be defined as
P (e; λ2) = ∑ b2 P (e,b2; λ2) (12)
",3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
"Then, we can combine word alignment and phrase segmentation and define the joint training objective as
J(θ1,θ2,λ1,λ2)
= S∑
s=1
logP (f (s)|e(s); θ1) +
1: procedure VITERBIEM(D) 2: Initialize Θ(0) 3: for all k = 1, . . .",3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
",K do 4: Ĥ(k) ← SEARCH(D,Θ(k−1)) 5: Θ(k) ← UPDATE(D, Ĥ(k)) 6: end for 7: return Ĥ(K),Θ(K) 8: end procedure
Algorithm 1: A Viterbi EM algorithm for learning the joint word alignment and phrase segmentation model from bilingual corpus.",3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
"D is a bilingual corpus, Θ(k) is the set of model parameters at the k-th iteration, H(k) is the set of Viterbi latent variables at the k-th iteration.
",3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
"logP (e(s)|f (s); θ2) + logP (f (s); λ1) + logP (e(s); λ2)− log E(f (s), e(s),θ1,θ2,λ1,λ2) (13)
where the expected loss is given by
E(f (s), e(s),θ1,θ2,λ1,λ2) = ∑ a1 ∑ a2 ∑ b1 ∑ b2 P (a1|f (s), e(s); θ1)×
P (a2|e(s), f (s); θ2)× P (b1|f (s); λ1)× P (b2|e(s); λ2)× ∆(a1,a2,b1,b2) (14)
We define a new loss function segmentation violation to measure the degree that an alignment violates phrase segmentations.
∆SV(a1,a2,b1,b2)
",3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
"= J−1∑ j=1 β(a1, j,b1,b2) + I−1∑ i=1",3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
"β(a2, i,b2,b1)
(15)
where β(a1, j,b1,b2) evaluates whether two links l1 = (j, aj) and l2 = (j + 1, aj+1) violate the phrase segmentation:
1.",3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
"fj and fj+1 belong to one phrase but eaj and eaj+1 belong to two phrases, or
2.",3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
"fj and fj+1 belong to two phrases but eaj and eaj+1 belong to one phrase.
",3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
"The β function returns 1 if there is violation and 0 otherwise.
1: procedure SEARCH(D, Θ) 2: Ĥ← ∅ 3: for all s ∈ {1, . . .",3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
", S} do 4: â1 ← ALIGN(f (s), e(s),θ1) 5: â2 ← ALIGN(e(s), f (s),θ2) 6:",3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
"b̂1 ← SEGMENT(f (s),λ1) 7: b̂2 ← SEGMENT(e(s),λ2) 8: h0 ← 〈â1, â2, b̂1, b̂2〉 9: ĥ←HILLCLIMB(f (s), e(s),h0,Θ)
10: Ĥ← Ĥ ∪ {ĥ} 11: end for 12: return Ĥ 13: end procedure
Algorithm 2: A search algorithm for finding the Viterbi latent variables.",3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
"â1 and â2 denote Viterbi alignments, b̂1 and b̂2 denote Viterbi segmentations.",3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
"They form a starting point h0 for the hill climbing algorithm, which keeps changing alignments and segmentations until the model score does not increase.",3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
"ĥ is the final set of Viterbi latent variables for one sentence.
",3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
"In Figure 2, we use “+” to label words that do not violate the phrase segmentations and “-” to label violations.
",3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
"In practice, we combine the two loss functions to enable word alignment and phrase segmentation to benefit each other in a joint search space:
∆SM+SV(a1,a2,b1,b2) = ∆SM(a1,a2) + ∆SV(a1,a2,b1,b2)",3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
(16),3.2 Agreement between Word Alignments and Phrase Segmentations,[0],[0]
Liang et al. (2006) indicate that it is intractable to train the joint model.,4 Training,[0],[0]
"For simplicity and efficiency, they exploit a simple heuristic procedure that leverages the product of posterior marginal probabilities.",4 Training,[0],[0]
"The intuition behind the heuristic is that links on which two models disagree should be discounted because the products of the marginals are small (Liang et al., 2006).
",4 Training,[0],[0]
"Unfortunately, it is hard to develop a similar heuristic for our model that allows for arbitrary loss functions.",4 Training,[0],[0]
"Alternatively, we resort to a Viterbi EM algorithm, as shown in Algorithm 1.",4 Training,[0],[0]
"The algorithm takes the training data D = {〈f (s), e(s)〉}Ss=1 as input (line 1).",4 Training,[0],[0]
"We use Θ(k) = 〈θ(k)1 ,θ(k)2 ,λ(k)1 ,λ(k)2 〉 to denote the set of model parameters at the k-th iteration.",4 Training,[0],[0]
"After initializing the model parameters (line 2), the algorithm alternates between searching for the Viterbi alignments
and segmentations Ĥ(k) using the SEARCH procedure (line 4) and updating model parameters using the UPDATE procedure (line 5).",4 Training,[0],[0]
"The algorithm terminates after running for K iterations.
",4 Training,[0],[0]
It is challenging to search for the Viterbi alignments and segmentations because of complicated structural dependencies.,4 Training,[0],[0]
"As shown in Algorithm 2, our strategy is first to find Viterbi alignments and segmentations independently using the ALIGN and SEGMENT procedures (lines 4-7), which then serve as a starting point for the HILLCLIMB procedure (lines 8-9).
",4 Training,[0],[0]
Figure 3 shows three operators we use in the HILLCLIMB procedure.,4 Training,[0],[0]
"The MOVE operator moves a link in an alignment, the MERGE operator merges two phrases into one phrase, and the SPLIT operator splits one phrase into two smaller phrases.",4 Training,[0],[0]
Note that each operator can be further divided into two variants: one for the source side and another for the target side.,4 Training,[0],[0]
"We evaluate our approach on Chinese-English alignment and translation tasks.
",5.1 Setup,[0],[0]
The training corpus consists of 1.2M sentence pairs with 32M Chinese words and 35.4M English words.,5.1 Setup,[0],[0]
"We used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of the English GIGAWORD corpus, which contains 398.6M words.",5.1 Setup,[0],[0]
"For alignment evaluation, we used the Tsinghua Chinese-English
word alignment evaluation data set.1",5.1 Setup,[0],[0]
"The evaluation metric is alignment error rate (AER) (Och and Ney, 2003).",5.1 Setup,[0],[0]
"For translation evaluation, we used the NIST 2006 dataset as the development set and the NIST 2002, 2003, 2004, 2005, and 2008 datasets as the test sets.",5.1 Setup,[0],[0]
"The evaluation metric is case-insensitive BLEU (Papineni et al., 2002).
",5.1 Setup,[0],[0]
"We used both phrase-based (Koehn et al., 2003) and hierarchical phrase-based (Chiang, 2007) translation systems to evaluate whether our approach improves translation performance.",5.1 Setup,[0],[0]
"For the phrase-based model, we used the open-source toolkit Moses (Koehn and Hoang, 2007).",5.1 Setup,[0],[0]
"For the hierarchical phrase-based model, we used an inhouse re-implementation on par with state-of-theart open-source decoders.
",5.1 Setup,[0],[0]
"We compared our approach with two state-ofthe-art generative alignment models:
1. GIZA++",5.1 Setup,[0],[0]
"(Och and Ney, 2003): unsupervised training of IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) using EM,
2.",5.1 Setup,[0],[0]
"BERKELEY (Liang et al., 2006): unsupervised training of joint HMMs using EM.
",5.1 Setup,[0],[0]
"For GIZA++, we trained IBM Model 4 in two directions with the default setting and used the grow-diag-final heuristic to generate symmetric alignments.",5.1 Setup,[0],[0]
"For BERKELEY, we trained joint HMMs using the default setting.",5.1 Setup,[0],[0]
"The hyperparameter of posterior decoding was optimized on the development set.
",5.1 Setup,[0],[0]
We used first-order HMMs for both word alignment and phrase segmentation.,5.1 Setup,[0],[0]
Our joint alignment and segmentation model were trained using the Viterbi EM algorithm for five iterations.,5.1 Setup,[0],[0]
Note that the Chinese-to-English and English-toChinese alignments are generally non-identical but share many links (see Figure 1(c)).,5.1 Setup,[0],[0]
"Then, we used the grow-diag-final heuristic to generate symmetric alignments.",5.1 Setup,[0],[0]
"BERKELEY
Table 1 shows the comparison of our approach with GIZA++ and BERKELEY in terms of AER and BLEU.",5.2 Comparison with GIZA++ and,[0],[0]
"GIZA++ trains two asymmetric models independently and uses the grow-diagfinal (i.e., GDF) for symmetrization.",5.2 Comparison with GIZA++ and,[0],[0]
"BERKELEY
1http://nlp.csai.tsinghua.edu.cn/˜ly/systems/TsinghuaAlig ner/TsinghuaAligner.html
trains two models jointly with the hard-matching (i.e., HM) loss function and uses posterior decoding for symmetrization.
",5.2 Comparison with GIZA++ and,[0],[0]
"For our approach, we distinguish between two variants:
1.",5.2 Comparison with GIZA++ and,[0],[0]
"Imposing agreement between word alignments (i.e., word-word) that uses the soft matching loss function (i.e., SM) (see Section 3.1);
2.",5.2 Comparison with GIZA++ and,[0],[0]
"Imposing agreement between word alignments and phrase segmentations (i.e., wordword, word-phrase) that uses both the soft matching and segmentation violation loss functions (i.e., SM+SV) (see Section 3.2).
",5.2 Comparison with GIZA++ and,[0],[0]
"We used the grow-diag-final heuristic for symmetrization.
",5.2 Comparison with GIZA++ and,[0],[0]
"For the alignment evaluation, we find that our approach achieves higher AER scores than the two baseline systems.",5.2 Comparison with GIZA++ and,[0],[0]
One possible reason is that links in the intersection of two symmetric alignments or two symmetric models agree usually correspond to sure links in the gold-standard annotation.,5.2 Comparison with GIZA++ and,[0],[0]
"Our approach loosens the hard constraint on agreement
and makes the posteriors less peaked around the Viterbi alignments.
",5.2 Comparison with GIZA++ and,[0],[0]
"For the translation evaluation, we used the phrase-based system Moses to report BLEU scores on the NIST 2008 test set.",5.2 Comparison with GIZA++ and,[0],[0]
We find that both the two variants of our approach significantly outperforms the two baselines (p < 0.01).,5.2 Comparison with GIZA++ and,[0],[0]
Table 2 shows the results on phrase-based and hierarchical phrase-based translation systems.,5.3 Results on (Hierarchical) Phrase-based Translation,[0],[0]
"We find that our approach systematically outperforms GIZA++ and BERKELEY on all NIST datasets.
",5.3 Results on (Hierarchical) Phrase-based Translation,[0],[0]
"In particular, generalizing the agreement to model the discrepancy between word alignment and phrase segmentation is consistently beneficial for improving translation quality, suggesting that it is important to introduce structural constraints into word alignment to increase the correlation between alignment and translation.
",5.3 Results on (Hierarchical) Phrase-based Translation,[0],[0]
"While “SM+SV” improves over “SM” significantly on phrase-based translation, the margins on the hierarchical phrase-based system are relatively smaller.",5.3 Results on (Hierarchical) Phrase-based Translation,[0],[0]
"One possible reason is that the “SV”
loss function can better account for phrase-based rather than hierarchical phrase-based translation.",5.3 Results on (Hierarchical) Phrase-based Translation,[0],[0]
"It is possible to design new loss functions tailored to hierarchical phrase-based translation.
",5.3 Results on (Hierarchical) Phrase-based Translation,[0],[0]
We also find that the BLEU scores of BERKELEY on hierarchical phrase-based translation are much lower than those on phrase-based translation.,5.3 Results on (Hierarchical) Phrase-based Translation,[0],[0]
"This might result from the fact that BERKELEY is prone to produce one-to-one alignments, which are not optimal for hierarchical phrasebased translation.",5.3 Results on (Hierarchical) Phrase-based Translation,[0],[0]
"Table 3 compares how well two asymmetric models agree with each other among GIZA++, BERKELEY and our approach.",5.4 Agreement Evaluation,[0],[0]
"We use F1 score to measure the degree of agreement:
2|AC→E ∩ AE→C | |AC→E |+ |AE→C",5.4 Agreement Evaluation,[0],[0]
"| (17)
where AC→E is the set of Chinese-to-English alignments on the training data and AE→C is the set of English-to-Chinese alignments.
",5.4 Agreement Evaluation,[0],[0]
It is clear that independent training leads to low agreement and joint training results in high agreement.,5.4 Agreement Evaluation,[0],[0]
BERKELEY achieves the highest value of agreement because of the hard constraint.,5.4 Agreement Evaluation,[0],[0]
This work is inspired by two lines of research: (1) agreement-based learning and (2) joint modeling of multiple NLP tasks.,6 Related Work,[0],[0]
"The key idea of agreement-based learning is to train a set of models jointly by encouraging them to agree on the hidden variables (Liang et al., 2006; Liang et al., 2008).",6.1 Agreement-based Learning,[0],[0]
"This can also be seen as a particular form of posterior constraint or posterior regularization (Graça et al., 2007; Ganchev et al., 2010).",6.1 Agreement-based Learning,[0],[0]
"The agreement is prior knowledge and
indirect supervision, which helps to train a more reasonable model with biased guidance.
",6.1 Agreement-based Learning,[0],[0]
"While agreement-based learning provides a principled approach to training a generative model, it constrains that the sub-models must share the same output space.",6.1 Agreement-based Learning,[0],[0]
"Our work extends (Liang et al., 2006) to introduce arbitrary loss functions that can encode prior knowledge.",6.1 Agreement-based Learning,[0],[0]
"As a result, Liang et al. (2006)’s model is a special case of our framework.",6.1 Agreement-based Learning,[0],[0]
Another difference is that our framework allows for including the agreement between word alignment and other structures such as phrase segmentations and parse trees.,6.1 Agreement-based Learning,[0],[0]
It is well accepted that different NLP tasks can help each other by providing additional information for resolving ambiguities.,6.2 Joint Modeling of Multiple NLP Tasks,[0],[0]
"As a result, joint modeling of multiple NLP tasks has received intensive attention in recent years, including phrase segmentation and alignment (Zhang et al., 2003), alignment and parsing (Burkett et al., 2010), tokenization and translation (Xiao et al., 2010), parsing and translation (Liu and Liu, 2010), alignment and named entity recognition (Chen et al., 2010; Wang et al., 2013).
",6.2 Joint Modeling of Multiple NLP Tasks,[0],[0]
"Among them, Zhang et al. (2003)’s integrated search algorithm for phrase segmentation and alignment is most close to our work.",6.2 Joint Modeling of Multiple NLP Tasks,[0],[0]
They use Point-wise Mutual Information to identify possible phrase pairs.,6.2 Joint Modeling of Multiple NLP Tasks,[0],[0]
The major difference is we train models jointly instead of integrated decoding.,6.2 Joint Modeling of Multiple NLP Tasks,[0],[0]
We have presented generalized agreement for bidirectional word alignment.,7 Conclusion,[0],[0]
The loss functions can be defined both between asymmetric alignments and between alignments and other latent structures such as phrase segmentations.,7 Conclusion,[0],[0]
We develop a Viterbi EM algorithm to train the joint model.,7 Conclusion,[0],[0]
"Experiments on Chinese-English translation show that joint training with generalized agreement achieves
significant improvements over two baselines for (hierarchical) phrase-based MT systems.",7 Conclusion,[0],[0]
"In the future, we plan to investigate more loss functions to account for syntactic constraints.",7 Conclusion,[0],[0]
"Yang Liu and Maosong Sun are supported by the 863 Program (2015AA011808), the National Natural Science Foundation of China (No. 61331013 and No. 61432013), and Samsung R&D Institute of China.",Acknowledgments,[0],[0]
Huanbo Luan is supported by the National Natural Science Foundation of China (No. 61303075).,Acknowledgments,[0],[0]
This research is also supported by the Singapore National Research Foundation under its International Research Centre@Singapore Funding Initiative and administered by the IDM Programme.,Acknowledgments,[0],[0]
We sincerely thank the reviewers for their valuable suggestions.,Acknowledgments,[0],[0]
"We also thank Yue Zhang, Meng Zhang and Shiqi Shen for their insightful discussions.",Acknowledgments,[0],[0]
"While agreement-based joint training has proven to deliver state-of-the-art alignment accuracy, the produced word alignments are usually restricted to one-toone mappings because of the hard constraint on agreement.",abstractText,[0],[0]
We propose a general framework to allow for arbitrary loss functions that measure the disagreement between asymmetric alignments.,abstractText,[0],[0]
The loss functions can not only be defined between asymmetric alignments but also between alignments and other latent structures such as phrase segmentations.,abstractText,[0],[0]
We use a Viterbi EM algorithm to train the joint model since the inference is intractable.,abstractText,[0],[0]
Experiments on ChineseEnglish translation show that joint training with generalized agreement achieves significant improvements over two state-ofthe-art alignment methods.,abstractText,[0],[0]
Generalized Agreement for Bidirectional Word Alignment,title,[0],[0]
We consider the task of labeling unsegmented sequence data and predicting future labels.,1. Introduction,[0],[0]
"This is a ubiquitous problem with important applications in many perceptual tasks, e.g., recognition and anticipation of human activities or speech.",1. Introduction,[0],[0]
"A generic modeling approach is key for intelligent agents to perform future-aware tasks (e.g., assistive activities).
",1. Introduction,[0],[0]
"Such tasks often exhibit non-Markovian and compositional properties, which should be captured by a top-down prediction algorithm.",1. Introduction,[0],[0]
"Consider the video sequence in Figure 1, human observers recognize the first two actions and predict the most likely future action based on the entire history.",1. Introduction,[0],[0]
"Context-free grammars are natural choices to model such reasoning processes, and it is one step closer to Turing machines than Markov models (e.g., Hidden Markov Models) in the Chomsky hierarchy.
",1. Introduction,[0],[0]
"However, it is not clear how to directly use symbolic gram-
1University of California, Los Angeles, USA 2Peking University, Beijing, China.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Siyuan Qi <syqi@cs.ucla.edu>.
Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"...
...",1. Introduction,[0],[0]
"open microwave take food put into microwave
Parse Tree Future prediction
...Sequenceinput data
Classi er",1. Introduction,[0],[0]
"raw output
Final output
Grammar
Generalized Earley Parser microwave food
Figure 1.",1. Introduction,[0],[0]
"The input of the generalized Earley parser is a matrix of probabilities of each label for each frame, given by an arbitrary classifier.",1. Introduction,[0],[0]
The parser segments and labels the sequence data into a label sentence in the language of a given grammar.,1. Introduction,[0],[0]
"Future predictions are then made based on the grammar.
",1. Introduction,[0],[0]
mars to parse and label sequence data.,1. Introduction,[0],[0]
Traditional grammar parsers take symbolic sentences as inputs instead of noisy sequence data like videos or audios.,1. Introduction,[0],[0]
The data has to be i) segmented and ii) labeled to apply existing grammar parsers to.,1. Introduction,[0],[0]
One naive solution is to first segment and label the data using a classifier and thus generating a label sentence.,1. Introduction,[0],[0]
Then grammar parsers can be applied on top of it for prediction.,1. Introduction,[0],[0]
"But this is apparently non-optimal, since the grammar rules are not considered in the classification process.",1. Introduction,[0],[0]
"It may not even be possible to parse this label sentence, because they are very often grammatically incorrect.
",1. Introduction,[0],[0]
"In this paper, we design a grammar-based parsing algorithm that directly operates on sequence input data, which goes beyond the scope of symbolic string inputs.",1. Introduction,[0],[0]
"Specifically, we propose a generalized Earley parser based on the Earley parser (Earley, 1970).",1. Introduction,[0],[0]
The algorithm finds the optimal segmentation and label sentence according to both a symbolic grammar and a classifier output of probabilities of labels for each frame as shown in Figure 1.,1. Introduction,[0],[0]
"Optimality here means maximizing the probability of the label sentence according to the classifier output while being grammatically correct.
",1. Introduction,[0],[0]
The difficulty of achieving this optimality lies in the joint optimization of both the grammar structure and the parsing likelihood of the output label sentence.,1. Introduction,[0],[0]
"For example, an expectation-maximization-type algorithm will not work well
since i)",1. Introduction,[0],[0]
"there is no guarantee for optimality, and ii) any grammatically incorrect sentence has a grammar prior of probability 0.",1. Introduction,[0],[0]
"The algorithm can easily get stuck in local minimums and fail to find a grammatically correct solution.
",1. Introduction,[0],[0]
The core idea of our algorithm is to directly and efficiently search for the optimal label sentence in the language defined by the grammar.,1. Introduction,[0],[0]
The constraint of the search space ensures that the sentence is grammatically correct.,1. Introduction,[0],[0]
"Specifically, a heuristic search is performed on the prefix tree expanded according to the grammar, where the path from the root to a node represents a partial sentence (prefix).",1. Introduction,[0],[0]
"By carefully defining the heuristic as a prefix probability computed based on the classifier output, we can efficiently search through the tree to find the optimal label sentence.
",1. Introduction,[0],[0]
The generalized Earley parser has four major advantages.,1. Introduction,[0],[0]
i),1. Introduction,[0],[0]
The inference process highly integrates a high-level grammar with an underlying classifier; the grammar gives guidance for segmenting and labeling the sequence data.,1. Introduction,[0],[0]
ii),1. Introduction,[0],[0]
It can be applied to any classifier outputs.,1. Introduction,[0],[0]
iii),1. Introduction,[0],[0]
It generates a grammar parse tree for data sequence that is highly explainable.,1. Introduction,[0],[0]
iv),1. Introduction,[0],[0]
"It is principled and generic, as it applies to most sequence data parsing and prediction problems.
",1. Introduction,[0],[0]
We evaluate the proposed approach on two datasets of human activities in the computer vision domain.,1. Introduction,[0],[0]
"The first dataset CAD-120 (Koppula et al., 2013) consists of daily activities and most activity prediction methods are based on this dataset.",1. Introduction,[0],[0]
Comparisons show that our method significantly outperforms state-of-the-art methods on future activity prediction.,1. Introduction,[0],[0]
"The second dataset Watch-n-Patch (Wu et al., 2015) is designed for “action patching”, which includes daily activities that have action forgotten by people.",1. Introduction,[0],[0]
Experiments on the second dataset show the robustness of our method on noisy data.,1. Introduction,[0],[0]
"Both experiments show that the generalized Earley parser performs particularly well on prediction tasks, primarily due to its non-Markovian property.
",1. Introduction,[0],[0]
This paper makes three major contributions.,1. Introduction,[0],[0]
•We design a parsing algorithm for symbolic context-free grammars.,1. Introduction,[0],[0]
It directly operates on sequence data to obtain the optimal segmentation and labels.,1. Introduction,[0],[0]
•We propose a prediction algorithm that naturally integrates with this parsing algorithm.,1. Introduction,[0],[0]
•We formulate an objective for future prediction for both grammar induction and classifier training.,1. Introduction,[0],[0]
The generalized Earley parser serves as a concrete example for combining symbolic reasoning methods and connectionist approaches.,1. Introduction,[0],[0]
"In formal language theory, a context-free grammar is a type of formal grammar, which contains a set of production rules that describe all possible sentences in a given formal language.",2. Background: Context-Free Grammars,[0],[0]
"A context-free grammar G in Chomsky Normal
Form is defined by a 4-tuple G = (V,Σ, R, γ) where 1.",2. Background: Context-Free Grammars,[0],[0]
V is a finite set of non-terminal symbols that can be replaced by/expanded to a sequence of symbols.,2. Background: Context-Free Grammars,[0],[0]
2.,2. Background: Context-Free Grammars,[0],[0]
"Σ is a finite set of terminal symbols that represent actual words in a language, which cannot be further expanded.",2. Background: Context-Free Grammars,[0],[0]
3.,2. Background: Context-Free Grammars,[0],[0]
"R is a finite set of production rules describing the replacement of symbols, typically of the form A→ BC or A→ α for A,B,C ∈ V and α ∈ Σ. 4.",2. Background: Context-Free Grammars,[0],[0]
"γ ∈ V is the start symbol (root of the grammar).
",2. Background: Context-Free Grammars,[0],[0]
"Given a formal grammar, parsing is the process of analyzing a string of symbols, conforming to the production rules and generating a parse tree.",2. Background: Context-Free Grammars,[0],[0]
A parse tree represents the syntactic structure of a string according to some contextfree grammar.,2. Background: Context-Free Grammars,[0],[0]
The root node of the tree is the grammar root.,2. Background: Context-Free Grammars,[0],[0]
"Other non-leaf nodes correspond to non-terminals in the grammar, expanded according to grammar production rules.",2. Background: Context-Free Grammars,[0],[0]
The leaf nodes are terminal symbols.,2. Background: Context-Free Grammars,[0],[0]
"All the leaf nodes together form a sentence.
",2. Background: Context-Free Grammars,[0],[0]
"The above definition can be augmented by assigning a probability to each production rule, thus becoming a probabilistic context-free grammar.",2. Background: Context-Free Grammars,[0],[0]
The probability of a parse tree is the product of the production rules that derive the parse tree.,2. Background: Context-Free Grammars,[0],[0]
"In this section, we review the original Earley parser (Earley, 1970) and introduce the basic concepts.",3. Earley Parser,[0],[0]
Earley parser is an algorithm for parsing sentences of a given context-free language.,3. Earley Parser,[0],[0]
"In the following descriptions, α, β, and γ represent any string of terminals/nonterminals (including the empty string ), A and B represent single nonterminals, and a represents a terminal symbol.",3. Earley Parser,[0],[0]
"We adopt Earley’s dot notation: for production of form A → αβ, the notation A → α · β means α has been parsed and β is expected.
",3. Earley Parser,[0],[0]
"Input position n is defined as the position after accepting the nth token, and input position 0 is the position prior to input.",3. Earley Parser,[0],[0]
"At each input position m, the parser generates a state set S(m).",3. Earley Parser,[0],[0]
"Each state is a tuple (A→ α ·β, i), consisting of
• The production currently being matched (A→ αβ).",3. Earley Parser,[0],[0]
• The dot: the current position in that production.,3. Earley Parser,[0],[0]
•,3. Earley Parser,[0],[0]
"The position i in the input at which the matching of this
production began: the position of origin.
",3. Earley Parser,[0],[0]
"Seeded with S(0) containing only the top-level rule, the parser then repeatedly executes three operations: prediction, scanning and completion:
• Prediction: for every state in S(m) of the form",3. Earley Parser,[0],[0]
"(A→ α · Bβ, i), where i is the origin position as above, add (B → ·γ,m) to S(m) for every production in the grammar with B on the left-hand side (i.e., B → γ).",3. Earley Parser,[0],[0]
•,3. Earley Parser,[0],[0]
"Scanning: if a is the next symbol in the input stream, for every state in S(m) of the form (A → α · aβ, i), add (A→ αa · β, i) to S(m+ 1).",3. Earley Parser,[0],[0]
"• Completion: for every state in S(m) of the form (A → γ·, j), find states in S(j) of the form (B → α ·",3. Earley Parser,[0],[0]
"Aβ, i) and add (B → αA · β, i) to S(m).
",3. Earley Parser,[0],[0]
"In this process, duplicate states are not added to the state set.",3. Earley Parser,[0],[0]
These three operations are repeated until no new states can be added to the set.,3. Earley Parser,[0],[0]
"The Earley parser executes in O(n2) for unambiguous grammars regarding the string length n, and O(n) for almost all LR(k) grammars.",3. Earley Parser,[0],[0]
A simple example is demonstrated in Figure 2.,3. Earley Parser,[0],[0]
"In this section, we introduce the proposed algorithm.",4. Generalized Earley Parser,[0],[0]
"Instead of taking symbolic sentences as input, we aim to design an algorithm that can parse raw sequence data x of length T (e.g., videos or audios) into a sentence l of labels (e.g., actions or words) of length |l| ≤",4. Generalized Earley Parser,[0],[0]
"T , where each label k ∈ {0, 1, · · · ,K} corresponds to a segment of a sequence.",4. Generalized Earley Parser,[0],[0]
"To achieve that, a classifier (e.g., a neural network) is first applied to each sequence x to get a T × K probability matrix y (e.g., softmax activations of the neural network), with ykt representing the probability of frame t being labeled as k. The proposed generalized Earley parser takes y as input and outputs the sentence l∗ that best explains the data according to a grammar G of Chomsky normal form.
",4. Generalized Earley Parser,[0],[0]
The core idea is to use the original Earley parser to help construct a prefix tree according to the grammar as illustrated in Figure 3.,4. Generalized Earley Parser,[0],[0]
A prefix tree is composed of terminal symbols and terminations that represent ends of sentences.,4. Generalized Earley Parser,[0],[0]
The root node of the tree is the “empty” symbol.,4. Generalized Earley Parser,[0],[0]
The path from the root to any node in the tree represents a partial sentence (prefix).,4. Generalized Earley Parser,[0],[0]
"For each prefix, we can compute the probability that the best label sentence starts with this prefix.",4. Generalized Earley Parser,[0],[0]
"This probability
is used as a heuristic to search for the best label sentence in the prefix tree: the prefix probabilities prioritize the nodes to be expanded in the prefix tree.",4. Generalized Earley Parser,[0],[0]
The parser finds the best solution when it expands a termination node in the tree.,4. Generalized Earley Parser,[0],[0]
"It then returns the current prefix string as the best solution.
",4. Generalized Earley Parser,[0],[0]
This heuristic search is realized by generalizing the Earley parser.,4. Generalized Earley Parser,[0],[0]
"Specifically, the scan operation in the Earley parser essentially expands a new node in the grammar prefix tree.",4. Generalized Earley Parser,[0],[0]
"For each prefix l, we can compute p(l|x0:T ) and p(l···|x0:T ) based on y, where p(l|x0:T ) is the probability of l being the best label, and p(l···|x0:T ) is the probability of l being the prefix of the best label of x0:T .",4. Generalized Earley Parser,[0],[0]
"The formulations for p(l|x0:T ) and p(l···|x0:T ) are derived in Section 4.1.
",4. Generalized Earley Parser,[0],[0]
We now describe the details.,4. Generalized Earley Parser,[0],[0]
"Each scan operation will create a new set S(m,n) ∈ S(m), where m is the length of the scanned string, n is the total number of the terminals that have been scanned at position m. This can be thought of as creating a new leaf node in the prefix tree, and S(m) is the set of all created nodes at level m. A priority queue q is kept for state sets for prefix search.",4. Generalized Earley Parser,[0],[0]
"Scan operations will push the newly created set into the queue with priority p(l···), where l is the parsed string of the state being scanned.
",4. Generalized Earley Parser,[0],[0]
"Each state is a tuple (A→ α · β, i, j, l, p(l···))",4. Generalized Earley Parser,[0],[0]
"augmented from the original Earley parser by adding j, l, p(l···).",4. Generalized Earley Parser,[0],[0]
"Here l is the parsed string of the state, and i, j are the indices of the set that this rule originated.",4. Generalized Earley Parser,[0],[0]
"The parser then repeatedly executes three operations: prediction, scanning, and completion modified from Earley parser:
• Prediction: for every state in S(m,n) of the form (A→ α · Bβ, i, j, l, p(l···)), add (B → ·γ,m, n, l, p(l···)) to S(m,n) for every production in the grammar with B on the left-hand side.",4. Generalized Earley Parser,[0],[0]
•,4. Generalized Earley Parser,[0],[0]
"Scanning: for every state in S(m,n) of the form (A → α · aβ, i, j, l, p(l···)), append the new terminal a to l and compute the probability p((l + a)···).",4. Generalized Earley Parser,[0],[0]
"Create a new set
S(m + 1, n′) where n′ is the current size of S(m + 1).",4. Generalized Earley Parser,[0],[0]
"Add (A→ αa·β, i, j, l+a, p((l+a)···)) to S(m+1, n′).",4. Generalized Earley Parser,[0],[0]
"Push S(m+ 1, n′) into q with priority p((l + a)···).",4. Generalized Earley Parser,[0],[0]
"• Completion: for every state in S(m,n) of the form (A → γ·, i, j, l, p(l···)), find states in S(i, j) of the form (B → α · Aβ, i′, j′, l′, p(l′···)) and add (B → αA · β, i′, j′, l, p(l···)) to S(m,n).
",4. Generalized Earley Parser,[0],[0]
This parsing process is efficient since we do not need to search through the entire tree.,4. Generalized Earley Parser,[0],[0]
"As shown in Figure 3 and Algorithm 1, the best label sentence l is returned when the probability of termination is larger than any other prefix probabilities.",4. Generalized Earley Parser,[0],[0]
"As long as the prefix probability is computed correctly, it is guaranteed to return the best solution.
",4. Generalized Earley Parser,[0],[0]
The original Earley parser is a special case of the generalized Earley parser.,4. Generalized Earley Parser,[0],[0]
"Intuitively, for any input sentence to Earley parser, we can always convert it to one-hot vectors and apply the proposed algorithm.",4. Generalized Earley Parser,[0],[0]
"On the other hand, the original Earley parser cannot be applied to segmented one-hot vectors since the labels are often grammatically incorrect.",4. Generalized Earley Parser,[0],[0]
"Hence we have the following proposition.
",4. Generalized Earley Parser,[0],[0]
Proposition 1.,4. Generalized Earley Parser,[0],[0]
Earley parser is a special case of the generalized Earley parser.,4. Generalized Earley Parser,[0],[0]
Proof.,4. Generalized Earley Parser,[0],[0]
"Let L(G) denote the language of grammar G, h(·) denote a one-to-one mapping from a label to a one-hot vector. L(G) is the input space for Earley parser.",4. Generalized Earley Parser,[0],[0]
∀,4. Generalized Earley Parser,[0],[0]
"l ∈ L(G), the generalized Earley parser accepts h(l) as input.",4. Generalized Earley Parser,[0],[0]
"Therefore the proposition follows.
",4. Generalized Earley Parser,[0],[0]
Here we emphasize two important distinctions of our method to traditional probabilistic parsers with prefix probabilities.,4. Generalized Earley Parser,[0],[0]
i),4. Generalized Earley Parser,[0],[0]
"In traditional parsers, the prefix probability is the probability of a string being a prefix of some strings generated by a grammar (top-down grammar prior).",4. Generalized Earley Parser,[0],[0]
"In our case, the parser computes the bottom-up data likelihood.",4. Generalized Earley Parser,[0],[0]
It is straightforward to extend this to a posterior.,4. Generalized Earley Parser,[0],[0]
"ii) Traditional parsers only maintain a parse tree, while our algorithm maintains both a parse tree and a prefix tree.",4. Generalized Earley Parser,[0],[0]
The introduction of the prefix tree into the parser enables us to efficiently search in the grammar according to a desired heuristic.,4. Generalized Earley Parser,[0],[0]
The parsing probability p(l|x0:T ) is computed in a dynamic programming fashion.,4.1. Parsing Probability Formulation,[0],[0]
"Let k be the last label in l. For t = 0, the probability is initialized by:
p(l|x0) =",4.1. Parsing Probability Formulation,[0],[0]
"{ yk0 l contains only k 0 otherwise
",4.1. Parsing Probability Formulation,[0],[0]
"(1)
Let l− be the label sentence obtained by removing the last label k from the label sentence l. For t > 0, the last frame t must be classified as k. The previous frames can be labeled as either l or l−. Then we have:
p(l|x0:t) = ykt (p(l|x0:t−1) +",4.1. Parsing Probability Formulation,[0],[0]
"p(l−|x0:t−1)) (2)
Algorithm 1 Generalized Earley Parser Initialize: S(0, 0) =",4.1. Parsing Probability Formulation,[0],[0]
"{(γ → ·R, 0, 0, , 1.0)}.",4.1. Parsing Probability Formulation,[0],[0]
"q = priorityQueue() q.push(1.0, (0, 0, , S(0, 0)))",4.1. Parsing Probability Formulation,[0],[0]
"{Push to queue with prob 1.0 and index (0, 0)} while (m,n, l−, currentSet) = q.pop() do
for s = (r, i, j, l, p(l···))",4.1. Parsing Probability Formulation,[0],[0]
∈ currentSet do if p(l) > p(l∗): l∗,4.1. Parsing Probability Formulation,[0],[0]
"= l if r is (A→ α ·Bβ) then {prediction}
for each (B → γ) in grammar g do r′ =",4.1. Parsing Probability Formulation,[0],[0]
"(B → ·γ) s′ = (r′,m, n, l, p(l···))",4.1. Parsing Probability Formulation,[0],[0]
"S(m,n).add(s′)
end for else if r is (A→ α · aβ) then {scanning} r′ =",4.1. Parsing Probability Formulation,[0],[0]
(A→ αa · β) m′,4.1. Parsing Probability Formulation,[0],[0]
"= m+ 1, n′ = |S(m+ 1)| − 1 s′ = (r′, i, j, l + a, p((l + a)···)) S(m′, n′).add(s′) q.push(p((l + a)···), (m
′, n′, S(m′, n′))) else if r is (B → γ·) then {completion}
for each ((A→ α ·Bβ), i′, j′) in S(i, j) do r′ =",4.1. Parsing Probability Formulation,[0],[0]
(A→ αB · β) s′ =,4.1. Parsing Probability Formulation,[0],[0]
"(r′, i′, j′, l, p(l···))",4.1. Parsing Probability Formulation,[0],[0]
"S(m,n).add(s′)
end for end if
end for if p(l−) >",4.1. Parsing Probability Formulation,[0],[0]
"p(l) for all un-expanded l: return l−
end while return l∗
It is worth mentioning that when ykt is wrongly given as 0, the dynamic programming process will have trouble correcting the mistake.",4.1. Parsing Probability Formulation,[0],[0]
"Even if p(l−|x0:t−1) is high, the probability p(l|x0:t) will be 0.",4.1. Parsing Probability Formulation,[0],[0]
"Fortunately, since the softmax function is usually adopted to compute y, ykt will not be 0 and the solution will be kept for consideration.
",4.1. Parsing Probability Formulation,[0],[0]
Then we compute the prefix probability p(l···|x0:T ) based on p(l−|x0:t).,4.1. Parsing Probability Formulation,[0],[0]
"For l to be the prefix, the transition from l− to l can happen at any frame t ∈ {0, · · · , T}.",4.1. Parsing Probability Formulation,[0],[0]
"Once the label k is observed (the transition happens), l becomes the prefix and the rest frames can be labeled arbitrarily.",4.1. Parsing Probability Formulation,[0],[0]
"Hence the probability of l being the prefix is:
p(l···|x0:T ) = p(l|x0) + T∑ t=1 ykt p(l −|x0:t−1) (3)
",4.1. Parsing Probability Formulation,[0],[0]
"In practice, the probability p(l|x0:t) decreases exponentially as t increases and will soon lead to numeric underflow.",4.1. Parsing Probability Formulation,[0],[0]
"To avoid this, the probabilities need to be computed in log space.",4.1. Parsing Probability Formulation,[0],[0]
The time complexity of computing the probabilities is O(T ) for each sentence,4.1. Parsing Probability Formulation,[0],[0]
l because p(l−|x0:t) are cached.,4.1. Parsing Probability Formulation,[0],[0]
The worst case complexity of the entire parsing is O(T |G|).,4.1. Parsing Probability Formulation,[0],[0]
"The generalized Earley parser gives us the best grammatically correct label sentence l to explain the sequence data, which takes all possible segmentations into consideration.",4.2. Segmentation and Labeling,[0],[0]
Therefore the probability p(l|x0:T ) is the summation of probabilities of all possible segmentations.,4.2. Segmentation and Labeling,[0],[0]
Let p(l|y0:e) be the probability of the best segmentation based on the classifier output y for sentence l.,4.2. Segmentation and Labeling,[0],[0]
"We perform a maximization over different segmentations by dynamic programming to find the best segmentation:
p(l|y0:e) = max b<e p(l−|y0:b) e∏ t=b ykt (4)
where e is the time frame that l ends and b is the time frame that l− ends.",4.2. Segmentation and Labeling,[0],[0]
The best segmentation can be obtained by backtracing the above probability.,4.2. Segmentation and Labeling,[0],[0]
"Similar to the previous probabilities, this probability needs to be computed in log space as well.",4.2. Segmentation and Labeling,[0],[0]
The time complexity of the segmentation and labeling is O(T 2).,4.2. Segmentation and Labeling,[0],[0]
"Given the parsing result l, we make grammar-based predictions for the next label z to be observed.",4.3. Future Label Prediction,[0],[0]
"The predictions are naturally obtained by the predict operation in the generalized Earley parser.
",4.3. Future Label Prediction,[0],[0]
"To predict the next possible symbols at current position (m,n), we search through the states S(m,n) of the form (X → α · zβ, i, j, l, p(l···)), where the first symbol z after the current position is a terminal node.",4.3. Future Label Prediction,[0],[0]
"The predictions Σ are then given by the set of all possible z:
Σ =",4.3. Future Label Prediction,[0],[0]
{z :,4.3. Future Label Prediction,[0],[0]
"∃s ∈ S(m,n), s = (X → α ·",4.3. Future Label Prediction,[0],[0]
"zβ, i, j, l, p(l···))} (5)
",4.3. Future Label Prediction,[0],[0]
The probability of each prediction is then given by the parsing likelihood of the sentence constructed by appending the predicted label z to the current sentence l.,4.3. Future Label Prediction,[0],[0]
"Assuming that the best prediction corresponds to the best parsing result, the goal is to find the best prediction z∗ that maximizes the following conditional probability as parsing likelihood:
z∗ = argmax z∈Σ p(z, l|G) (6)
",4.3. Future Label Prediction,[0],[0]
"For a grammatically complete sentence u, the parsing likelihood is simply the Viterbi likelihood (Viterbi, 1967) given by the probabilistic context-free grammar.",4.3. Future Label Prediction,[0],[0]
"For an incomplete sentence l of length |l|, the parsing likelihood is given by the sum of all the grammatically possible sentences:
p(l|G) = ∑
u1:|l|=l
p(u|G) (7)
where u1:|l| denotes the first |l|words of a complete sentence u, and p(u|G) is the Viterbi likelihood of u.",4.3. Future Label Prediction,[0],[0]
We are interested in finding the best grammar and classifier that give us the most accurate predictions based on the generalized Earley parser.,4.4. Maximum Likelihood Estimation for Prediction,[0],[0]
"Let G be the grammar, f be the classifier, and D be the set of training examples.",4.4. Maximum Likelihood Estimation for Prediction,[0],[0]
"The training set consists of pairs of complete or partial data sequence x and the corresponding label sequence y for all the frames in x. By merging consecutive labels in y that are the same, we can obtain partial label sentences l and predicted labels z.",4.4. Maximum Likelihood Estimation for Prediction,[0],[0]
"Hence we have D = {(x,y, l, z)}.",4.4. Maximum Likelihood Estimation for Prediction,[0],[0]
"The best grammar G∗ and the best classifier f∗ together minimizes the prediction loss:
G∗, f∗ = argmin G,f Lpred(G, f) (8)
where the prediction loss is given by the negative log likelihood of the predictions over the entire training set:
Lpred(G, f) =",4.4. Maximum Likelihood Estimation for Prediction,[0],[0]
"− ∑
(x,z)∈D
log(p(z|x))
=",4.4. Maximum Likelihood Estimation for Prediction,[0],[0]
"− ∑
(x,l,z)∈D (log(p(z|l, G))︸ ︷︷ ︸ grammar + log(p(l|x))︸ ︷︷ ︸ classifier )
(9) Given the intermediate variable l, the loss is decomposed into two parts that correspond to the induced grammar and the trained classifier, respectively.",4.4. Maximum Likelihood Estimation for Prediction,[0],[0]
Let u ∈,4.4. Maximum Likelihood Estimation for Prediction,[0],[0]
{,4.4. Maximum Likelihood Estimation for Prediction,[0],[0]
"l} be the complete label sentences in the training set (i.e., the label sentence for a complete sequence x).",4.4. Maximum Likelihood Estimation for Prediction,[0],[0]
"The best grammar maximizes the following probability:∏
(z,l)∈D
p(z|l, G) = ∏
(z,l)∈D
p(z, l|G) p(l|G) = ∏ u∈D p(u|G)
(10) where denominators p(l|G) are canceled by the previous numerator p(z, l−|G), and only the likelihood of the complete sentences remain.",4.4. Maximum Likelihood Estimation for Prediction,[0],[0]
Therefore inducing the best grammar that gives us the most accurate future prediction is equivalent to the maximum likelihood estimation (MLE) of the grammar for complete sentences in the dataset.,4.4. Maximum Likelihood Estimation for Prediction,[0],[0]
"This finding lets us to turn the problem (induce the grammar that gives the best future prediction) into a standard grammar induction problem, which can be solved by existing algorithms, e.g., (Solan et al., 2005) and (Tu et al., 2013).
",4.4. Maximum Likelihood Estimation for Prediction,[0],[0]
The best classifier minimizes the second term of Eq.,4.4. Maximum Likelihood Estimation for Prediction,[0],[0]
"9:
f∗ = argmin f",4.4. Maximum Likelihood Estimation for Prediction,[0],[0]
"− ∑ (x,l,z)∈D log(p(l|x)
",4.4. Maximum Likelihood Estimation for Prediction,[0],[0]
≈ argmin f,4.4. Maximum Likelihood Estimation for Prediction,[0],[0]
"− ∑ (x,y)∈D ∑ k yk log(ŷk) (11)
where p(l|x) can be maximized by the CTC loss (Graves et al., 2006).",4.4. Maximum Likelihood Estimation for Prediction,[0],[0]
"In practice, it can be substituted by the commonly adopted cross entropy loss for efficiency.",4.4. Maximum Likelihood Estimation for Prediction,[0],[0]
Therefore we can directly apply generalized Earley parser to outputs of general detectors/classifiers for parsing and prediction.,4.4. Maximum Likelihood Estimation for Prediction,[0],[0]
Future activity prediction.,5. Related Work,[0],[0]
This is a relatively new but important domain in computer vision.,5. Related Work,[0],[0]
"(Ziebart et al., 2009; Gupta et al., 2009; Yuen & Torralba, 2010; Ryoo, 2011; Kitani et al., 2012; Kuderer et al., 2012; Wang et al., 2012; Pei et al., 2013; Walker et al., 2014; Vu et al., 2014; Li & Fu, 2014; Wei et al., 2016; Holtzen et al., 2016; Alahi et al., 2016; Xie et al., 2018; Rhinehart & Kitani, 2017; Ma et al., 2017) predict human trajectories/actions in various settings including complex indoor/outdoor scenes.",5. Related Work,[0],[0]
"Koppula, Gupta and Saxena (KGS) (Koppula et al., 2013) proposed a model incorporating object affordances that detects and predicts human activities.",5. Related Work,[0],[0]
"Koppula et al. (Koppula & Saxena, 2016) later proposed an anticipatory temporal conditional random field to model the spatial-temporal relations.",5. Related Work,[0],[0]
"Qi et al. (Qi et al., 2017) proposed a spatial-temporal And-Or graph (STAOG) for activity prediction.
",5. Related Work,[0],[0]
Hierarchical/grammar models.,5. Related Work,[0],[0]
"Pei et al. (Pei et al., 2013) unsupervisedly learned a temporal grammar for video parsing.",5. Related Work,[0],[0]
"Pirsiavash et al. (Pirsiavash & Ramanan, 2014) proposed a segmental grammar to parse videos.",5. Related Work,[0],[0]
"Holtzen et al. (Holtzen et al., 2016) inferred human intents by a hierarchical model.",5. Related Work,[0],[0]
"The ST-AOG (Qi et al., 2017) is also a type of grammar model.",5. Related Work,[0],[0]
"Grammar-based methods show effectiveness on tasks that have compositional structures.
",5. Related Work,[0],[0]
"However, previous grammar-based algorithms take symbolic inputs like the traditional language parsers.",5. Related Work,[0],[0]
This seriously limits the applicability of these algorithms.,5. Related Work,[0],[0]
"Additionally, the parser does not provide guidance for either training the classifiers or segmenting the sequences.",5. Related Work,[0],[0]
They also lack a good approach to handle grammatically incorrect label sentences.,5. Related Work,[0],[0]
"For example, (Qi et al., 2017) finds in the training corpus the closest sentence to the recognized sentence and applies the language parser afterward.",5. Related Work,[0],[0]
"In our case, the proposed parsing algorithm takes sequence data of raw signals as input and generates the label sentence as well as the parse tree.",5. Related Work,[0],[0]
"All parsed label sentences are grammatically correct, and a learning objective is formulated for the classifier.",5. Related Work,[0],[0]
We evaluate our method on the task of human activity detection and prediction.,6. Human Activity Detection and Prediction,[0],[0]
"We present and discuss our experiment results on two datasets, CAD-120 (Koppula et al., 2013) and Watch-n-Patch (Wu et al., 2015), for comparisons with state-of-the-art methods and evaluation of the robustness of our approach.",6. Human Activity Detection and Prediction,[0],[0]
CAD-120 is the dataset that most existing prediction algorithms are evaluated on.,6. Human Activity Detection and Prediction,[0],[0]
It contains videos of daily activities that are long sequences of sub-activities.,6. Human Activity Detection and Prediction,[0],[0]
Watch-n-Patch is a daily activity dataset that features forgotten actions.,6. Human Activity Detection and Prediction,[0],[0]
Results show that our method significantly outperforms the other methods for activity prediction.,6. Human Activity Detection and Prediction,[0],[0]
"In both experiments, we used a modified version of the ADIOS (automatic distillation of structure) (Solan et al., 2005) grammar induction algorithm to learn the event grammar.",6.1. Grammar Induction,[0],[0]
The algorithm learns the production rules by generating significant patterns and equivalent classes.,6.1. Grammar Induction,[0],[0]
The significant patterns are selected according to a context-sensitive criterion defined regarding local flow quantities in the graph: two probabilities are defined over a search path.,6.1. Grammar Induction,[0],[0]
One is the right-moving ratio of fan-through (through-going flux of path) to fan-in (incoming flux of paths).,6.1. Grammar Induction,[0],[0]
"The other one, similarly, is the left-going ratio of fan-through to fan-in.",6.1. Grammar Induction,[0],[0]
"The criterion is described in detail in (Solan et al., 2005).
",6.1. Grammar Induction,[0],[0]
"The algorithm starts by loading the corpus of activity onto a graph whose vertices are sub-activities, augmented by two special symbols, begin and end.",6.1. Grammar Induction,[0],[0]
Each event sample is represented by a separate path over the graph.,6.1. Grammar Induction,[0],[0]
Then it generates candidate patterns by traversing a different search path.,6.1. Grammar Induction,[0],[0]
"At each iteration, it tests the statistical significance of each subpath to find significant patterns.",6.1. Grammar Induction,[0],[0]
The algorithm then finds the equivalent classes that are interchangeable.,6.1. Grammar Induction,[0],[0]
"At the end of the iteration, the significant pattern is added to the graph as a new node, replacing the subpaths it subsumes.",6.1. Grammar Induction,[0],[0]
We favor shorter patterns in our implementation.,6.1. Grammar Induction,[0],[0]
Dataset The CAD-120 dataset is a standard dataset for human activity prediction.,6.2. Experiment on CAD-120 Dataset,[0],[0]
"It contains 120 RGB-D videos of four different subjects performing 10 high-level activities, where each high-level activity was performed three times with different objects.",6.2. Experiment on CAD-120 Dataset,[0],[0]
"It includes a total of 61,585 total video frames.",6.2. Experiment on CAD-120 Dataset,[0],[0]
Each video is a sequence of sub-activities involving 10 different sub-activity labels.,6.2. Experiment on CAD-120 Dataset,[0],[0]
"The videos vary from subject to subject regarding the lengths and orders of the sub-activities as well as the way they executed the task.
",6.2. Experiment on CAD-120 Dataset,[0],[0]
Evaluation metrics We use the following metrics to evaluate and compare the algorithms.,6.2. Experiment on CAD-120 Dataset,[0],[0]
1) Frame-wise detection accuracy of sub-activity labels for all frames.,6.2. Experiment on CAD-120 Dataset,[0],[0]
2),6.2. Experiment on CAD-120 Dataset,[0],[0]
Future 3s online prediction accuracy.,6.2. Experiment on CAD-120 Dataset,[0],[0]
"We compute the frame-wise accuracy of prediction of the sub-activity labels of the future 3s (using the frame rate of 14Hz as reported in (Koppula et al., 2013)).",6.2. Experiment on CAD-120 Dataset,[0],[0]
"The predictions are made online at each frame t, i.e., the algorithms only sees frame 0 to t and predicts the labels of frame t + 1 to t + δt. 3)",6.2. Experiment on CAD-120 Dataset,[0],[0]
Future segment online prediction accuracy.,6.2. Experiment on CAD-120 Dataset,[0],[0]
"At each frame t, the algorithm predicts the sub-activity label of the next video segment.
",6.2. Experiment on CAD-120 Dataset,[0],[0]
"We consider the overall micro accuracy (P/R), macro precision, macro recall and macro F1 score for all evaluation metrics.",6.2. Experiment on CAD-120 Dataset,[0],[0]
Micro accuracy is the percentage of correctly classified labels.,6.2. Experiment on CAD-120 Dataset,[0],[0]
"Macro precision and recall are the average of precision and recall respectively for all classes.
",6.2. Experiment on CAD-120 Dataset,[0],[0]
Comparative methods We compare our method with four state-of-the-art methods for activity prediction: 1.,6.2. Experiment on CAD-120 Dataset,[0],[0]
"KGS (Koppula et al., 2013): a Markov random field model where the nodes represent objects and sub-activities, and the edges represent the spatial-temporal relationships.",6.2. Experiment on CAD-120 Dataset,[0],[0]
Future frames are predicted based on the transition probabilities given the inferred label of the last frame.,6.2. Experiment on CAD-120 Dataset,[0],[0]
2.,6.2. Experiment on CAD-120 Dataset,[0],[0]
"Anticipatory temporal CRF (ATCRF) (Koppula & Saxena, 2016): an anticipatory temporal conditional random field that models the spatial-temporal relations through object affordances.",6.2. Experiment on CAD-120 Dataset,[0],[0]
Future frames are predicted by sampling a spatial-temporal graph.,6.2. Experiment on CAD-120 Dataset,[0],[0]
3.,6.2. Experiment on CAD-120 Dataset,[0],[0]
"ST-AOG + Earley (Qi et al., 2017): a spatial-temporal And-Or graph (ST-AOG) that uses a symbolic context-free grammar to model activities.",6.2. Experiment on CAD-120 Dataset,[0],[0]
This sets up a comparison between our proposed method and methods that use traditional probabilistic parsers.,6.2. Experiment on CAD-120 Dataset,[0],[0]
"Since traditional parsers operate on symbolic data, extra efforts need to be done first to extract symbols from sequence data.",6.2. Experiment on CAD-120 Dataset,[0],[0]
"In this comparative method, the videos are first segmented and labeled by classifiers; the predictions are then made by the original Earley parser.",6.2. Experiment on CAD-120 Dataset,[0],[0]
4.,6.2. Experiment on CAD-120 Dataset,[0],[0]
Bidirectional LSTM (Bi-LSTM): a two-layer Bi-LSTM with a hidden size of 256.,6.2. Experiment on CAD-120 Dataset,[0],[0]
"For the detection task, the output for each frame input is the sub-activity label.",6.2. Experiment on CAD-120 Dataset,[0],[0]
"For the future 3s prediction, the LSTM is trained to output the label for frame t+3s for an input frame at time t. For future segment prediction, it outputs the label of the next segment for an input frame.",6.2. Experiment on CAD-120 Dataset,[0],[0]
All three tasks use the same training schemes.,6.2. Experiment on CAD-120 Dataset,[0],[0]
5.,6.2. Experiment on CAD-120 Dataset,[0],[0]
Bi-LSTM + generalized Earley parser (our method): the proposed generalized Earley parser applied to the classifier output of the above detection Bi-LSTM.,6.2. Experiment on CAD-120 Dataset,[0],[0]
The predictions for the next segments are made according to Section 4.3.,6.2. Experiment on CAD-120 Dataset,[0],[0]
"The lengths of unobserved segments are sampled from a log-normal distribution for the future 3s prediction.
",6.2. Experiment on CAD-120 Dataset,[0],[0]
"Feature extraction All methods in the experiment use the same publicly available features from KGS (Koppula et al., 2013).",6.2. Experiment on CAD-120 Dataset,[0],[0]
These features include the human skeleton features and human-object interaction features for each frame.,6.2. Experiment on CAD-120 Dataset,[0],[0]
The human skeleton features are location and distance features (relative to the subjects head location) for all upper-skeleton joints of a subject.,6.2. Experiment on CAD-120 Dataset,[0],[0]
"The human-object features are spatialtemporal, containing the distances between object centroids and skeleton joint locations as well as the temporal changes.
",6.2. Experiment on CAD-120 Dataset,[0],[0]
"Experiment results We follow the convention in KGS (Koppula et al., 2013) to train on three subjects and test on a new subject with a 4-fold validation.",6.2. Experiment on CAD-120 Dataset,[0],[0]
"The results for the three evaluation metrics are summarized in Table 1, Table 2 and Table 3, respectively.",6.2. Experiment on CAD-120 Dataset,[0],[0]
Our method outperforms the comparative methods on all three tasks.,6.2. Experiment on CAD-120 Dataset,[0],[0]
"Specifically, the generalized Earley parser on top of a Bi-LSTM performs better than ST-AOG, while ST-AOG outperforms the Bi-LSTM.",6.2. Experiment on CAD-120 Dataset,[0],[0]
More discussions are highlighted in Section 6.4.,6.2. Experiment on CAD-120 Dataset,[0],[0]
Dataset Watch-n-Patch is an RGB-D dataset that features forgotten actions.,6.3. Experiment on Watch-n-Patch Dataset,[0],[0]
"For example, a subject might fetch milk from a fridge, pour milk, and leave.",6.3. Experiment on Watch-n-Patch Dataset,[0],[0]
The typical action “putting the milk back into the fridge” is forgotten.,6.3. Experiment on Watch-n-Patch Dataset,[0],[0]
"The dataset contains 458 videos with a total length of about 230 minutes, in which people forgot actions in 222 videos.",6.3. Experiment on Watch-n-Patch Dataset,[0],[0]
Each video in the dataset contains 2-7 actions interacted with different objects.,6.3. Experiment on Watch-n-Patch Dataset,[0],[0]
7 subjects are asked to perform daily activities in 8 offices and 5 kitchens with complex backgrounds.,6.3. Experiment on Watch-n-Patch Dataset,[0],[0]
"It consists of 21 types of fully annotated actions (10 in the office, 11 in the kitchen) interacted with 23 types of objects.
",6.3. Experiment on Watch-n-Patch Dataset,[0],[0]
"Feature extraction We extract the same features as described in (Wu et al., 2015) for all methods.",6.3. Experiment on Watch-n-Patch Dataset,[0],[0]
"Similar to the previous experiment, the features are composed of skeleton features and human-object interaction features extracted from RGB-D images.",6.3. Experiment on Watch-n-Patch Dataset,[0],[0]
"The skeleton features include angles between connected parts, the change of joint positions and angles from previous frames.",6.3. Experiment on Watch-n-Patch Dataset,[0],[0]
"Each frame is segmented into super-pixels, and foreground masks are detected.",6.3. Experiment on Watch-n-Patch Dataset,[0],[0]
We extract features from the image segments with more than 50% in the foreground mask and within a distance to the human hand joints in both 3D points and 2D pixels.,6.3. Experiment on Watch-n-Patch Dataset,[0],[0]
"Six kernel descriptors (Wu et al., 2014) are extracted from these image segments: gradient, color, local binary pattern, depth gradient, spin, surface normals, and KPCA/self-similarity.
",6.3. Experiment on Watch-n-Patch Dataset,[0],[0]
"Experiment results We use the same evaluation metrics as the previous experiment and compare our method to STAOG (Qi et al., 2017) and Bi-LSTM.",6.3. Experiment on Watch-n-Patch Dataset,[0],[0]
"We use the train/test split in (Wu et al., 2015).",6.3. Experiment on Watch-n-Patch Dataset,[0],[0]
"The results for the three evaluation metrics are summarized in Table 4, Table 5 and Table 6, respectively.",6.3. Experiment on Watch-n-Patch Dataset,[0],[0]
"Our method slightly improves the detection re-
sults over the Bi-LSTM outputs, and outperforms the other methods on both prediction tasks.",6.3. Experiment on Watch-n-Patch Dataset,[0],[0]
"In general, the algorithms make better predictions on CAD-120, since Watch-n-Patch features forgotten actions and the behaviors are more unpredictable.",6.3. Experiment on Watch-n-Patch Dataset,[0],[0]
More details are discussed in Section 6.4.,6.3. Experiment on Watch-n-Patch Dataset,[0],[0]
How different are the classifier outputs and the final outputs for detection?,6.4. Discussion,[0],[0]
Figure 4 shows some qualitative examples of the ground truth segmentations and results given by different methods.,6.4. Discussion,[0],[0]
"The segmentation results show that the refined outputs are similar with the classifier outputs since the confidence given by the classifiers are often very high.
",6.4. Discussion,[0],[0]
How does the generalized Earley parser refine the classifier detection outputs?,6.4. Discussion,[0],[0]
"When the classifier outputs violate the grammar, two types of refinements occur: i) correction and deletion of wrong labels as shown in 4a; ii) insertion of new labels as shown in 4b.",6.4. Discussion,[0],[0]
The inserted segments are usually very short to accommodate both the grammar and the classifier outputs.,6.4. Discussion,[0],[0]
"Most boundaries of the refined results are well aligned with the classifier outputs.
",6.4. Discussion,[0],[0]
Why do we use two metrics for future prediction?,6.4. Discussion,[0],[0]
The future 3s prediction is a standard evaluation criterion set up by KGS and ATCRF.,6.4. Discussion,[0],[0]
"However, this criterion does not tell how well the algorithm predicts the next segment label.",6.4. Discussion,[0],[0]
i),6.4. Discussion,[0],[0]
"At any time frame, part of the future 3s involves the current sub-activity for most of the times.",6.4. Discussion,[0],[0]
ii),6.4. Discussion,[0],[0]
"If the predicted length of the current sub-activity is inaccurate, the framewise inaccuracy drops proportionally, even when the future segment label prediction is accurate.",6.4. Discussion,[0],[0]
"Therefore we also compare against the future segment label prediction because it is invariant to variations in activity lengths.
",6.4. Discussion,[0],[0]
How well does the generalized Earley parser perform for activity detection and prediction?,6.4. Discussion,[0],[0]
"From the results we can see that it slightly improves over the classifier outputs
for detection, but significantly outperforms the classifier for predictions.",6.4. Discussion,[0],[0]
"The modifications on classifier outputs (corrections and insertions in Figure 4) are minor but important to make the sentences grammatically correct, thus high-quality predictions can be made.
",6.4. Discussion,[0],[0]
How useful is the grammar for activity modeling?,6.4. Discussion,[0],[0]
"From Table 2, Table 3, Table 5 and Table 6 we can see that both STAOG and generalized Earley parser outperforms Bi-LSTM for prediction.",6.4. Discussion,[0],[0]
Prediction algorithms need to give different outputs for similar inputs based on the observation history.,6.4. Discussion,[0],[0]
"Hence the non-Markovian property of grammars is useful for activity modeling, especially for future prediction.
",6.4. Discussion,[0],[0]
How robust is the generalized Earley parser?,6.4. Discussion,[0],[0]
Comparing Table 3 and Table 6 we can see that there is a performance drop when the action sequences are more unpredictable (in the Watch-n-Patch dataset).,6.4. Discussion,[0],[0]
But it is capable of improving over the noisy classifier inputs and significantly outperforms the other methods.,6.4. Discussion,[0],[0]
It is also robust in the sense that it can always find the best sentence in a given language that best explains the classifier outputs.,6.4. Discussion,[0],[0]
We proposed a generalized Earley parser for parsing sequence data according to symbolic grammars.,7. Conclusions,[0],[0]
Detections and predictions are made by the parser given the probabilistic outputs from any classifier.,7. Conclusions,[0],[0]
We are optimistic about and interested in further applications of the generalized Earley parser.,7. Conclusions,[0],[0]
"In general, we believe this is a step towards the goal of integrating the connectionist and symbolic approaches.",7. Conclusions,[0],[0]
The authors thank Professor Ying Nian Wu from UCLA Statistics Department for helpful comments on this work.,Acknowledgment,[0],[0]
"The work reported herein is supported by DARPA XAI N66001-17-2-4029, ONR MURI N00014-16-1-2007, and N66001-17-2-3602.",Acknowledgment,[0],[0]
"Future predictions on sequence data (e.g., videos or audios) require the algorithms to capture nonMarkovian and compositional properties of highlevel semantics.",abstractText,[0],[0]
"Context-free grammars are natural choices to capture such properties, but traditional grammar parsers (e.g., Earley parser) only take symbolic sentences as inputs.",abstractText,[0],[0]
"In this paper, we generalize the Earley parser to parse sequence data which is neither segmented nor labeled.",abstractText,[0],[0]
"This generalized Earley parser integrates a grammar parser with a classifier to find the optimal segmentation and labels, and makes top-down future predictions.",abstractText,[0],[0]
Experiments show that our method significantly outperforms other approaches for future human activity prediction.,abstractText,[0],[0]
Generalized Earley Parser: Bridging Symbolic Grammars and Sequence Data for Future Prediction,title,[0],[0]
"Gaussian process (GP) (Rasmussen & Williams, 2006) is a well-known statistical learning model extensively used in various scenarios, e.g., regression, classification, optimization (Shahriari et al., 2016), visualization (Lawrence, 2005), active learning (Fu et al., 2013; Liu et al., 2017) and multi-task learning (Alvarez et al., 2012; Liu et al., 2018).",1. Introduction,[0],[0]
Given the training setX = {xi ∈ Rd}ni=1,1. Introduction,[0],[0]
"and the observation set y = {y(xi) ∈ R}ni=1, as an approximation of the underlying function η",1. Introduction,[0],[0]
:,1. Introduction,[0],[0]
"Rd → R, GP provides informative
1Rolls-Royce@NTU Corporate Lab, Nanyang Technological University, Singapore 637460 2School of Computer Science and Engineering, Nanyang Technological University, Singapore 639798 3Applied Technology Group, Rolls-Royce Singapore, 6 Seletar Aerospace Rise, Singapore 797575 4Data Science and Artificial Intelligence Research Center, Nanyang Technological University, Singapore 639798.",1. Introduction,[0],[0]
"Correspondence to: Haitao Liu <htliu@ntu.edu.sg>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
predictive distributions at test points.
",1. Introduction,[0],[0]
"However, the most prominent weakness of the full GP is that it scales poorly with the training size.",1. Introduction,[0],[0]
"Given n data points, the time complexity of a standard GP paradigm scales as O(n3) in the training process due to the inversion of an n×n covariance matrix; it scales asO(n2) in the prediction process due to the matrix-vector operation.",1. Introduction,[0],[0]
"This weakness confines the full GP to training data of size O(104).
",1. Introduction,[0],[0]
"To cope with large-scale regression, various computationally efficient approximations have been presented.",1. Introduction,[0],[0]
"The sparse approximations reviewed in (Quiñonero-Candela & Rasmussen, 2005) employ m (m n) inducing points to summarize the whole training data (Seeger et al., 2003; Snelson & Ghahramani, 2006; 2007; Titsias, 2009; Bauer et al., 2016), thus reducing the training complexity of full GP to O(nm2) and the predicting complexity to O(nm).",1. Introduction,[0],[0]
"The complexity can be further reduced through distributed inference, stochastic variational inference or Kronecker structure (Hensman et al., 2013; Gal et al., 2014; Wilson & Nickisch, 2015; Hoang et al., 2016; Peng et al., 2017).",1. Introduction,[0],[0]
"A main drawback of sparse approximations, however, is that the representational capability is limited by the number of inducing points (Moore & Russell, 2015).",1. Introduction,[0],[0]
"For example, for a quick-varying function, the sparse approximations need many inducing points to capture the local structures.",1. Introduction,[0],[0]
"That is, this kind of scheme has not reduced the scaling of the complexity (Bui & Turner, 2014).
",1. Introduction,[0],[0]
"The method exploited in this article belongs to the aggregation models (Hinton, 2002; Tresp, 2000; Cao & Fleet, 2014; Deisenroth & Ng, 2015; Rullière",1. Introduction,[0],[0]
"et al., 2017), also known as consensus statistical methods (Genest & Zidek, 1986; Ranjan & Gneiting, 2010).",1. Introduction,[0],[0]
"This kind of scheme produces the final predictions by the aggregation of M submodels (GP experts) respectively trained on the subsets {Di = {Xi,yi}}Mi=1 of D = {X,y}, thus distributing the computations to “local” experts.",1. Introduction,[0],[0]
"Particularly, due to the product of experts, the aggregation scheme derives a factorized marginal likelihood for efficient training; and then it combines the experts’ posterior distributions according to a certain aggregation criterion.",1. Introduction,[0],[0]
"In comparison to sparse approximations, the aggregation models (i) operate directly on the full training data, (ii) require no additional
inducing or variational parameters and (iii) distribute the computations on individual experts for straightforward parallelization (Tavassolipour et al., 2017), thus scaling them to arbitrarily large training data.",1. Introduction,[0],[0]
"In comparison to typical local GPs (Snelson & Ghahramani, 2007; Park et al., 2011), the aggregations smooth out the ugly discontinuity by the product of posterior distributions from GP experts.",1. Introduction,[0],[0]
"Note that the aggregation methods are different from the mixture-of-experts (Rasmussen & Ghahramani, 2002; Yuan & Neubauer, 2009), which suffers from intractable inference and is mainly developed for non-stationary regression.
",1. Introduction,[0],[0]
"However, it has been pointed out (Rullière et al., 2017) that there exists a particular type of training data such that typical aggregations, e.g., product-of-experts (PoE) (Hinton, 2002; Cao & Fleet, 2014) and Bayesian committee machine (BCM) (Tresp, 2000; Deisenroth & Ng, 2015), cannot offer consistent predictions, where “consistent” means the aggregated predictive distribution can converge to the true underlying predictive distribution when the training size n approaches infinity.
",1. Introduction,[0],[0]
The major contributions of this paper are three-fold.,1. Introduction,[0],[0]
"We first prove the inconsistency of typical aggregation models, e.g., the overconfident or conservative prediction variances illustrated in Fig. 3, using conventional disjoint or random data partition.",1. Introduction,[0],[0]
"Thereafter, we present a consistent yet efficient aggregation model for large-scale GP regression.",1. Introduction,[0],[0]
"Particularly, the proposed generalized robust Bayesian committee machine (GRBCM) selects a global subset to communicate with the remaining subsets, leading to the consistent aggregated predictive distribution derived under the Bayes rule.",1. Introduction,[0],[0]
"Finally, theoretical and empirical analyses reveal that GRBCM outperforms existing aggregations due to the consistent yet efficient predictions.",1. Introduction,[0],[0]
We release the demo codes in https://github.com/LiuHaiTao01/GRBCM.,1. Introduction,[0],[0]
"A GP usually places a probability distribution over the latent function space as f(x) ∼ GP(0, k(x,x′)), which is defined by the zero mean and the covariance k(x,x′).",2.1. Factorized training,[0],[0]
"The wellknown squared exponential (SE) covariance function is
k(x,x′) = σ2f exp
( −1
2 d∑ i=1",2.1. Factorized training,[0],[0]
"(xi − x′i)2 l2i
) , (1)
where σ2f is an output scale amplitude, and li is an input length-scale along the ith dimension.",2.1. Factorized training,[0],[0]
"Given the noisy observation y(x) = f(x) + where the i.i.d. noise follows ∼ N (0, σ2 ) and the training dataD, we have the marginal likelihood p(y|X,θ) = N (0, k(X,X) + σ2 I) where θ represents the hyperparameters to be inferred.
",2.1. Factorized training,[0],[0]
"In order to train the GP on large-scale datasets, the aggregation models introduce a factorized training process.",2.1. Factorized training,[0],[0]
"It first partitions the training set D into M subsets Di = {Xi,yi}, 1 ≤ i ≤M , and then trains GP on Di as an expertMi.",2.1. Factorized training,[0],[0]
"In data partition, we can assign the data points randomly to the experts (random partition), or assign disjoint subsets obtained by clustering techniques to the experts (disjoint partition).",2.1. Factorized training,[0],[0]
"Ignoring the correlation between the experts {Mi}Mi=1 leads to the factorized approximation as
p(y|X,θ) ≈ M∏ i=1",2.1. Factorized training,[0],[0]
"pi(yi|Xi,θi), (2)
where pi(yi|Xi,θi) ∼ N (0,Ki + σ2 ,iIi) with Ki = k(Xi,Xi) ∈ Rni×ni and ni being the training size of Mi.",2.1. Factorized training,[0],[0]
"Note that for simplicity all the M GP experts in (2) share the same hyperparameters as θi = θ (Deisenroth & Ng, 2015).",2.1. Factorized training,[0],[0]
"The factorization (2) degenerates the full covariance matrix K = k(X,X) into a diagonal block matrix diag[K1, · · · ,KM ], leading to K−1 ≈ diag[K−11 , · · · ,K −1 M ].",2.1. Factorized training,[0],[0]
"Hence, compared to the full GP, the complexity of the factorized training process is reduced to O(nm20) given ni = m0 = n/M , 1 ≤ i ≤M .
",2.1. Factorized training,[0],[0]
"Conditioned on the related subset Di, the predictive distribution pi(y∗|Di,x∗) ∼ N (µi(x∗), σ2i (x∗)) ofMi has1
µi(x∗)",2.1. Factorized training,[0],[0]
"= k T i∗[Ki + σ 2 I] −1yi, (3a) σ2i (x∗) = k(x∗,x∗)− kTi∗[Ki + σ2 I]−1ki∗ + σ2 , (3b)
",2.1. Factorized training,[0],[0]
"where ki∗ = k(Xi,x∗)",2.1. Factorized training,[0],[0]
.,2.1. Factorized training,[0],[0]
"Thereafter, the experts’ predictions {µi, σ2i }Mi=1 are combined by the following aggregation methods to perform the final predicting.",2.1. Factorized training,[0],[0]
"The state-of-the-art aggregation methods include PoE (Hinton, 2002; Cao & Fleet, 2014), BCM (Tresp, 2000; Deisenroth & Ng, 2015), and nested pointwise aggregation of experts (NPAE)",2.2. Prediction aggregation,[0],[0]
"(Rullière et al., 2017).
",2.2. Prediction aggregation,[0],[0]
"For the PoE and BCM family, the aggregated prediction mean and precision are generally formulated as
µA(x∗) = σ 2 A(x∗) M∑ i=1",2.2. Prediction aggregation,[0],[0]
βiσ −2,2.2. Prediction aggregation,[0],[0]
"i (x∗)µi(x∗), (4a)
σ−2A (x∗) = M∑ i=1",2.2. Prediction aggregation,[0],[0]
βiσ −2,2.2. Prediction aggregation,[0],[0]
i (x∗) + (1− M∑ i=1 βi)σ −2,2.2. Prediction aggregation,[0],[0]
"∗∗ , (4b)
where the prior variance σ2∗∗ = k(x∗,x∗) + σ 2 , which is a correction term to σ−2A , is only available for the BCM family; and βi is the weight of the expertMi at x∗.
1Instead of using pi(f∗|Di,x∗) in (Deisenroth & Ng, 2015), we here consider the aggregations in a general scenario where each expert has all its belongings at hand.
",2.2. Prediction aggregation,[0],[0]
"The predictions of the PoE family, which omit the prior precision σ−2∗∗ in (4b), are derived from the product of M experts as
pA(y∗|D,x∗) = M∏ i=1",2.2. Prediction aggregation,[0],[0]
"pβii (y∗|Di,x∗).",2.2. Prediction aggregation,[0],[0]
"(5)
The original PoE (Hinton, 2002) employs the constant weight βi = 1, resulting in the aggregated prediction variances that vanish with increasing M .",2.2. Prediction aggregation,[0],[0]
"On the contrary, the generalized PoE (GPoE) (Cao & Fleet, 2014) considers a varying βi = 0.5(log σ2∗∗",2.2. Prediction aggregation,[0],[0]
"− log σ2i (x∗)), which represents the difference in the differential entropy between the prior p(y∗|x∗) and the posterior p(y∗|Di,x∗), to weigh the contribution ofMi at x∗.",2.2. Prediction aggregation,[0],[0]
This varying βi brings the flexibility of increasing or reducing the importance of experts based on the predictive uncertainty.,2.2. Prediction aggregation,[0],[0]
"However, the varying βi may produce undesirable errors for GPoE. For instance, when x∗ is far away from the training data such that σ2i (x∗)→ σ2∗∗, we have βi → 0 and σ2GPoE →∞.
The BCM family, which is opposite to the PoE family, explicitly incorporates the GP prior p(y∗|x∗) when combining predictions.",2.2. Prediction aggregation,[0],[0]
"For two expertsMi andMj , BCM introduces a conditional independence assumption Di ⊥ Dj |y∗, leading to the aggregated predictive distribution as
pA(y∗|D,x∗) = ∏M i=1",2.2. Prediction aggregation,[0],[0]
p βi,2.2. Prediction aggregation,[0],[0]
"i (y∗|Di,x∗)
p ∑ i βi−1(y∗|x∗) .",2.2. Prediction aggregation,[0],[0]
"(6)
The original BCM (Tresp, 2000) employs βi = 1 but its predictions suffer from weak experts when leaving the data.",2.2. Prediction aggregation,[0],[0]
"Hence, inspired by GPoE, the robust BCM (RBCM) (Deisenroth & Ng, 2015) uses a varying βi to produce robust predictions by reducing the weights of weak experts.",2.2. Prediction aggregation,[0],[0]
"When x∗ is far away from the training data X , the correction term brought by the GP prior in (4b) helps the (R)BCM’s prediction variance recover σ2∗∗. However, given M = 1, the predictions of RBCM as well as GPoE cannot recover the full GP predictions because usually β1 = 0.5(log σ2∗∗",2.2. Prediction aggregation,[0],[0]
− log σ21(x∗)),2.2. Prediction aggregation,[0],[0]
= 0.5(log σ 2 ∗∗ − log σ2full(x∗)),2.2. Prediction aggregation,[0],[0]
6=,2.2. Prediction aggregation,[0],[0]
"1.
To achieve computation gains, the above aggregations introduce additional independence assumption for the experts’ predictions, which however is often violated in practice and yields poor results.",2.2. Prediction aggregation,[0],[0]
"Hence, in the aggregation process, NPAE (Rullière et al., 2017) regards the prediction mean µi(x∗) in (3a) as a random variable by assuming that yi has not yet been observed, thus allowing for considering the covariances between the experts’ predictions.",2.2. Prediction aggregation,[0],[0]
"Thereafter, for the random vector [µ1, · · · , µM , y∗]T, the covariances are derived as
cov[µi, y∗] = k T i∗K −1",2.2. Prediction aggregation,[0],[0]
"i, ki∗, (7a)
cov[µi, µj ] =
{ kTi∗K −1",2.2. Prediction aggregation,[0],[0]
"i, KijK −1 j, kj∗, i 6= j,
kTi∗K −1",2.2. Prediction aggregation,[0],[0]
"i, Kij, K −1 j, kj∗, i = j,
(7b)
where Kij = k(Xi,Xj) ∈ Rni×nj , Ki, = Ki + σ2",2.2. Prediction aggregation,[0],[0]
"I , Kj, = Kj + σ 2 I , and Kij, = Kij + σ 2",2.2. Prediction aggregation,[0],[0]
I .,2.2. Prediction aggregation,[0],[0]
"With these covariances, a nested GP training process is performed to derive the aggregated prediction mean and variance as
µNPAE(x∗)",2.2. Prediction aggregation,[0],[0]
= k T A∗K −1,2.2. Prediction aggregation,[0],[0]
"A µ, (8a) σ2NPAE(x∗) = k(x∗,x∗)− kTA∗K−1A kA∗",2.2. Prediction aggregation,[0],[0]
"+ σ 2 , (8b)
where kA∗ ∈ RM×1 has the ith element as cov[µi, y∗], KA ∈ RM×M has KijA = cov[µi, µj ], and µ = [µ1(x∗), · · · , µM (x∗)]T. The NPAE is capable of providing consistent predictions at the cost of implementing a much more time-consuming aggregation because of the inversion ofKA at each test point.",2.2. Prediction aggregation,[0],[0]
"Though showcasing promising results (Deisenroth & Ng, 2015), given that n→∞ and the experts are noise-free GPs, (G)PoE and (R)BCM have been proved to be inconsistent, since there exists particular triangular array of data points that are dense in the input domain Ω such that the prediction variances do not go to zero (Rullière et al., 2017).
",2.3. Discussions of existing aggregations,[0],[0]
"Particularly, we further show below the inconsistency of (G)PoE and (R)BCM using two typical data partitions (random and disjoint partition) in the scenario where the observations are blurred with noise.",2.3. Discussions of existing aggregations,[0],[0]
"Note that since GPoE using a varying βi may produce undesirable errors, we adopt βi = 1/M as suggested in (Deisenroth & Ng, 2015).",2.3. Discussions of existing aggregations,[0],[0]
"Now the GPoE’s prediction mean is the same as that of PoE; but the prediction variance blows up as M times that of PoE.
Definition 1.",2.3. Discussions of existing aggregations,[0],[0]
"When n → ∞, let X ∈ Rn×d be dense in Ω ∈",2.3. Discussions of existing aggregations,[0],[0]
"[0, 1]d such that for any x ∈ Ω we have limn→∞min1≤i≤n ‖xi − x‖ = 0.",2.3. Discussions of existing aggregations,[0],[0]
"Besides, the underlying function to be approximated has true continuous response µη(x) and true noise variance σ2η .
",2.3. Discussions of existing aggregations,[0],[0]
"Firstly, for the disjoint partition that uses clustering techniques to partition the data D into disjoint local subsets {Di}Mi=1, The proposition below reveals that when n→∞, PoE and (R)BCM produce overconfident prediction variance that shrinks to zero; on the contrary, GPoE provides conservative prediction variance.
",2.3. Discussions of existing aggregations,[0],[0]
Proposition 1.,2.3. Discussions of existing aggregations,[0],[0]
Let {Di}Mni=1 be a disjoint partition of the training data D. Let the expertMi trained on Di be GP with zero mean and stationary covariance function k(.),2.3. Discussions of existing aggregations,[0],[0]
>,2.3. Discussions of existing aggregations,[0],[0]
0.,2.3. Discussions of existing aggregations,[0],[0]
"We further assume that (i) limn→∞Mn = ∞ and (ii) limn→∞ n/M 2 n > 0, where the second condition implies that the subset size m0 = n/Mn and the number of experts Mn are comparable such that too weak experts are not preferred.",2.3. Discussions of existing aggregations,[0],[0]
"Besides, from the second condition we have m0 →n→∞ ∞, which implies that the experts become more informative with increasing n. Then, PoE and (R)BCM
produce overconfident prediction variance at x∗ ∈ Ω as
lim n→∞
σ2A,n(x∗) = 0, (9)
whereas GPoE yields conservative prediction variance
σ2η < lim n→∞ σ2A,n(x∗)",2.3. Discussions of existing aggregations,[0],[0]
"< σ 2 bn(x∗) < σ 2 ∗∗, (10)
where σ2bn(x∗) is offered by the farthest expertMbn (1 ≤ bn ≤Mn) whose prediction variance is closet to σ2∗∗.
The detailed proof is given in",2.3. Discussions of existing aggregations,[0],[0]
"Appendix A. Moreover, we have the following findings.",2.3. Discussions of existing aggregations,[0],[0]
Remark 1.,2.3. Discussions of existing aggregations,[0],[0]
For the averaging σ−2GPoE,2.3. Discussions of existing aggregations,[0],[0]
= 1 M ∑M i=1,2.3. Discussions of existing aggregations,[0],[0]
σ −2,2.3. Discussions of existing aggregations,[0],[0]
"i and
µ(G)PoE = ∑M i=1",2.3. Discussions of existing aggregations,[0],[0]
"σ−2i∑ σ−2i
µi using disjoint partition, more and more experts become relatively far away from x∗ when n→∞, i.e., the prediction variances at x∗ approach σ2∗∗ and the prediction means approach the prior mean µ∗∗.",2.3. Discussions of existing aggregations,[0],[0]
"Hence, empirically, when n→∞, the conservative σ2GPoE approaches σ2bn , and the µ(G)PoE approaches µ∗∗.",2.3. Discussions of existing aggregations,[0],[0]
Remark 2.,2.3. Discussions of existing aggregations,[0],[0]
"The BCM’s prediction variance is always larger than that of PoE since
a∗ = σ−2PoE(x∗)
σ−2BCM(x∗) =
∑M i=1",2.3. Discussions of existing aggregations,[0],[0]
"σ
−2",2.3. Discussions of existing aggregations,[0],[0]
"i (x∗)∑M
i=1 σ −2",2.3. Discussions of existing aggregations,[0],[0]
"i (x∗)− (M − 1)σ −2 ∗∗
> 1
for M > 1.",2.3. Discussions of existing aggregations,[0],[0]
This means σ2PoE deteriorates faster to zero when n→∞.,2.3. Discussions of existing aggregations,[0],[0]
"Besides, it is observed that µBCM is a∗ times that of PoE, which alleviates the deterioration of prediction mean when n → ∞. However, when x∗ is leaving X , a∗ →M since σ−2i",2.3. Discussions of existing aggregations,[0],[0]
(x∗)→ σ−2∗∗ .,2.3. Discussions of existing aggregations,[0],[0]
"That is why BCM suffers from undesirable prediction mean when leavingX .
",2.3. Discussions of existing aggregations,[0],[0]
"Secondly, for the random partition that assigns the data points randomly to the experts without replacement, The proposition below implies that when n→∞, the prediction variances of PoE and (R)BCM will shrink to zero; the PoE’s prediction mean will recover µη(x), but the (R)BCM’s prediction mean cannot; interestingly, the simple GPoE can converge to the underlying true predictive distribution.
",2.3. Discussions of existing aggregations,[0],[0]
Proposition 2.,2.3. Discussions of existing aggregations,[0],[0]
Let {Di}Mni=1 be a random partition of the training data D with (i) limn→∞Mn = ∞ and (ii) limn→∞ n/M 2 n > 0.,2.3. Discussions of existing aggregations,[0],[0]
Let the experts {Mi} Mn i=1 be GPs with zero mean and stationary covariance function,2.3. Discussions of existing aggregations,[0],[0]
k(.),2.3. Discussions of existing aggregations,[0],[0]
> 0.,2.3. Discussions of existing aggregations,[0],[0]
"Then, for the aggregated predictions at x∗ ∈ Ω we have lim n→∞ µPoE(x∗) = µη(x∗), lim n→∞ σ2PoE(x∗) = 0, lim n→∞ µGPoE(x∗) = µη(x∗), lim n→∞ σ2GPoE(x∗) = σ",2.3. Discussions of existing aggregations,[0],[0]
"2 η,
lim n→∞ µ(R)BCM(x∗) = aµη(x∗), lim n→∞
σ2(R)BCM(x∗) = 0,
(11)
",2.3. Discussions of existing aggregations,[0],[0]
where a = σ−2η /(σ −2,2.3. Discussions of existing aggregations,[0],[0]
η,2.3. Discussions of existing aggregations,[0],[0]
− σ−2∗∗ ),2.3. Discussions of existing aggregations,[0],[0]
"≥ 1 and the equality holds when σ2η = 0.
",2.3. Discussions of existing aggregations,[0],[0]
"The detailed proof is provided in Appendix B. Propositions 1 and 2 imply that no matter what kind of data partition has been used, the prediction variances of PoE and (R)BCM will shrink to zero when n→∞, which strictly limits their usability since no benefits can be gained from such useless uncertainty information.
",2.3. Discussions of existing aggregations,[0],[0]
"As for data partition, intuitively, the random partition provides overlapping and coarse global information about the target function, which limits the ability to describe quickvarying characteristics.",2.3. Discussions of existing aggregations,[0],[0]
"On the contrary, the disjoint partition provides separate and refined local information, which enables the model to capture the variability of target function.",2.3. Discussions of existing aggregations,[0],[0]
"The superiority of disjoint partition has been empirically confirmed in (Rullière et al., 2017).",2.3. Discussions of existing aggregations,[0],[0]
"Therefore, unless otherwise indicated, we employ disjoint partition for the aggregation models throughout the article.
",2.3. Discussions of existing aggregations,[0],[0]
"As for time complexity, the five aggregation models have the same training process, and they only differ in how to combine the experts’ predictions.",2.3. Discussions of existing aggregations,[0],[0]
"For (G)PoE and (R)BCM, their time complexity in prediction scales as O(nm20) + O(n′nm0) where n′ is the number of test points.2 For the complicated NPAE, it however needs to invert an M ×M matrixKA at each test point, leading to a greatly increased time complexity in prediction as O(n′n2).3
The inconsistency of (G)PoE and (R)BCM and the extremely time-consuming process of NPAE impose the demand of developing a consistent yet efficient aggregation model for large-scale GP regression.",2.3. Discussions of existing aggregations,[0],[0]
Our proposed GRBCM divides M experts into two groups.,3.1. GRBCM,[0],[0]
"The first group has a global communication expert Mc trained on the subset Dc = D1, and the second group contains the remainingM−1 global or local experts4 {Mi}Mi=2 trained on {Di}Mi=2, respectively.",3.1. GRBCM,[0],[0]
The training process of GRBCM is identical to that of typical aggregations in section 2.1.,3.1. GRBCM,[0],[0]
"The prediction process of GRBCM, however, is different.",3.1. GRBCM,[0],[0]
"Particularly, GRBCM assigns the global communication expert with the following properties:
• (Random selection)",3.1. GRBCM,[0],[0]
"The communication subset Dc is a random subset wherein the points are randomly se-
2O(nm20) is induced by the update of M GP experts after optimizing hyperparameters.
",3.1. GRBCM,[0],[0]
"3The predicting complexity of NPAE can be reduced by employing various hierarchical computing structure (Rullière et al., 2017), which however cannot provide identical predictions.
",3.1. GRBCM,[0],[0]
"4“Global” means the expert is trained on a random subset, whereas “local” means it is trained on a disjoint subset.
",3.1. GRBCM,[0],[0]
"lected without replacement from D. It indicates that the points inXc spread over the entire domain, which enablesMc to capture the main features of the target function.",3.1. GRBCM,[0],[0]
"Note that there is no limit to the partition type for the remaining M − 1 subsets.
",3.1. GRBCM,[0],[0]
• (Expert communication),3.1. GRBCM,[0],[0]
"The expertMc with predictive distribution pc(y∗|Dc,x∗) ∼ N (µc, σ2c ) is allowed to communicate with each of the remaining experts {Mi}Mi=2.",3.1. GRBCM,[0],[0]
"It means we can utilize the augmented dataD+i = {Dc,Di} to improve over the base expertMc, leading to a new expertM+i with the improved predictive distribution as p+i(y∗|D+i,x∗) ∼ N",3.1. GRBCM,[0],[0]
"(µ+i, σ2+i) for 2 ≤ i ≤M .
• (Conditional independence)",3.1. GRBCM,[0],[0]
"Given the communication subset Dc and y∗, the independence assumption Di ⊥ Dj |Dc, y∗ holds for 2 ≤",3.1. GRBCM,[0],[0]
"i 6= j ≤M .
",3.1. GRBCM,[0],[0]
"Given the conditional independence assumption and the weights {βi}Mi=2, we approximate the exact predictive distribution p(y∗|D,x∗) using the Bayes rule as
p(y∗|D,x∗)
∝ p(y∗|x∗)p(Dc|y∗,x∗) M∏ i=2",3.1. GRBCM,[0],[0]
"p(Di|{Dj}i−1j=1, y∗,x∗)
≈ p(y∗|x∗)p(Dc|y∗,x∗) M∏ i=2",3.1. GRBCM,[0],[0]
"pβi(Di|Dc, y∗,x∗)
= p(y∗|x∗)
",3.1. GRBCM,[0],[0]
∏M i=2,3.1. GRBCM,[0],[0]
"p
βi(D+i|y∗,x∗) p ∑M i=2 βi−1(Dc|y∗,x∗) .
",3.1. GRBCM,[0],[0]
"(12) Note that p(D2|Dc, y∗,x∗) is exact with no approximation in (12).",3.1. GRBCM,[0],[0]
"Hence, we set β2 = 1.
",3.1. GRBCM,[0],[0]
"With (12), GRBCM’s predictive distribution is
pA(y∗|D,x∗) =",3.1. GRBCM,[0],[0]
"∏M i=2 p βi +i(y∗|D+i,x∗)
",3.1. GRBCM,[0],[0]
p ∑M i=2,3.1. GRBCM,[0],[0]
"βi−1 c (y∗|Dc,x∗) .",3.1. GRBCM,[0],[0]
"(13)
with
µA(x∗) = σ 2 A(x∗) [ M∑ i=2 βiσ −2",3.1. GRBCM,[0],[0]
"+i (x∗)µ+i(x∗)
− ( M∑ i=2",3.1. GRBCM,[0],[0]
βi,3.1. GRBCM,[0],[0]
"− 1 ) σ−2c (x∗)µc(x∗) ] , (14a)
",3.1. GRBCM,[0],[0]
σ−2A (x∗) = M∑ i=2 βiσ −2,3.1. GRBCM,[0],[0]
+i (x∗)− ( M∑ i=2,3.1. GRBCM,[0],[0]
βi,3.1. GRBCM,[0],[0]
"− 1 ) σ−2c (x∗).
(14b)
Different from (R)BCM, GRBCM employs the informative σ−2c rather than the prior σ −2",3.1. GRBCM,[0],[0]
"∗∗ to correct the prediction
precision in (14b), leading to consistent predictions when n → ∞, which will be proved below.",3.1. GRBCM,[0],[0]
"Also, the prediction mean of GRBCM in (14a) now is corrected by µc(x∗).",3.1. GRBCM,[0],[0]
"Fig. 1 depicts the structure of the GRBCM aggregation model.
",3.1. GRBCM,[0],[0]
"In (14a) and (14b), the parameter βi (i > 2) akin to that of RBCM is defined as the difference in the differential entropy between the base predictive distribution pc(y∗|Dc,x∗) and the enhanced predictive distribution p+i(y∗|D+i,x∗) as
βi =
{ 1, i = 2,
0.5(log σ2c (x∗)− log σ2+i(x∗)), 3 ≤",3.1. GRBCM,[0],[0]
"i ≤M. (15)
It is found that after adding a subset Di (i ≥ 2) into the communication subset",3.1. GRBCM,[0],[0]
"Dc, if there is little improvement of p+i(y∗|D+i,x∗) over pc(y∗|Dc,x∗), we weak the vote of M+i by assigning a small βi that approaches zero.
",3.1. GRBCM,[0],[0]
"As for the size of Xc, more data points bring more informativeMc and better GRBCM predictions at the cost of higher computing complexity.",3.1. GRBCM,[0],[0]
"In this article, we assign all the experts with the same training size as nc = ni = m0 and n+i = 2m0 for 2 ≤ i ≤M .
",3.1. GRBCM,[0],[0]
"Next, we show that the GRBCM’s predictive distribution will converge to the underlying true predictive distribution when n→∞. Proposition 3.",3.1. GRBCM,[0],[0]
Let {Di}Mni=1 be a partition of the training data D with (i) limn→∞Mn = ∞ and (ii) limn→∞ n/M 2 n > 0.,3.1. GRBCM,[0],[0]
"Besides, among the M subsets, there is a global communication subset",3.1. GRBCM,[0],[0]
"Dc, the points in which are randomly selected from D without replacement.",3.1. GRBCM,[0],[0]
Let the global expertMc and the enhanced experts {M+i}Mni=2 be GPs with zero mean and stationary covariance function k(.),3.1. GRBCM,[0],[0]
> 0.,3.1. GRBCM,[0],[0]
"Then, GRBCM yields consistent predictions as limn→∞µGRBCM(x∗) = µη(x∗),lim
n→∞ σ2GRBCM(x∗) = σ 2 η.
(16)
",3.1. GRBCM,[0],[0]
"The detailed proof is provided in Appendix C. It is found in Proposition 3 that apart from the requirement that the communication subset Dc should be a random subset, the consistency of GRBCM holds for any partition of the remaining data D\Dc.",3.1. GRBCM,[0],[0]
"Besides, according to Propositions 2
and 3, both GPoE and GRBCM produce consistent predictions using random partition.",3.1. GRBCM,[0],[0]
"It is known that the GP modelM provides more confident predictions, i.e., lower uncertainty U(M) = ∫ σ2(x)dx, with more data points.",3.1. GRBCM,[0],[0]
"Since GRBCM trains experts on more informative subsets {D+i}Mi=2, we have the following finding.",3.1. GRBCM,[0],[0]
Remark 3.,3.1. GRBCM,[0],[0]
"When using random subsets, the GRBCM’s prediction uncertainty is always lower than that of GPoE, since the discrepancy δU−1",3.1. GRBCM,[0],[0]
= U −1 GRBCM,3.1. GRBCM,[0],[0]
− U −1,3.1. GRBCM,[0],[0]
"GPoE satisfies
δU−1 =",3.1. GRBCM,[0],[0]
"[ U−1(M+2)− 1
Mn Mn∑ i=1",3.1. GRBCM,[0],[0]
"U−1(Mi)
]
+ ∫ Mn∑ i=3 βi",3.1. GRBCM,[0],[0]
( σ−2+i (x∗)− σ −2,3.1. GRBCM,[0],[0]
c (x∗) ),3.1. GRBCM,[0],[0]
"dx∗ > 0
for a large enough n. It means compared to GPoE, GRBCM converges faster to the underlying function when n→∞.
Finally, similar to RBCM, GRBCM can be executed in multi-layer computing architectures with identical predictions (Deisenroth & Ng, 2015; Ionescu, 2015), which allow to run optimally and efficiently with the available computing infrastructure for distributed computing.",3.1. GRBCM,[0],[0]
Assuming that the experts {Mi}Mi=1 have the same training size ni = m0 = n/M for 1 ≤,3.2. Complexity,[0],[0]
i ≤ M .,3.2. Complexity,[0],[0]
"Compared to (G)PoE and (R)BCM, the proposed GRBCM has a higher time complexity in prediction due to the construction of new experts {M+i}Mi=2.",3.2. Complexity,[0],[0]
"In prediction, it first needs to calculate the inverse of k(Xc,Xc) and M − 1 augmented covariance matrices {k({Xi,Xc}, {Xi,Xc})}Mi=2, which scales as O(8nm20 − 7m30), in order to obtain the predictions µc, {µ+i}Mi=2 and σ2c , {σ2+i}Mi=2.",3.2. Complexity,[0],[0]
"Then, it combines the predictions of Mc and {M+i}Mi=2 at n′ test points.",3.2. Complexity,[0],[0]
"Therefore, the time complexity of the GRBCM prediction process is O(αnm20) + O(βn′nm0), where α = (8M − 7)/M and β = (4M − 3)/M .",3.2. Complexity,[0],[0]
"We employ a 1D toy example
f(x) = 5x2 sin(12x) + (x3 − 0.5) sin(3x− 0.5) + 4 cos(2x) + , (17)
where ∼ N (0, 0.25), to illustrate the characteristics of existing aggregation models.
",4.1. Toy example,[0],[0]
"We generate n = 104, 5× 104, 105, 5× 105 and 106 training points, respectively, in [0, 1], and select n′ = 0.1n test points randomly in [−0.2, 1.2].",4.1. Toy example,[0],[0]
We pre-normalize each column of X and y to zero mean and unit variance.,4.1. Toy example,[0],[0]
"Due to
the global expertMc in GRBCM, we slightly modify the disjoint partition: we first generate a random subset and then use the k-means technique to generateM−1 disjoint subsets.",4.1. Toy example,[0],[0]
Each expert is assigned with m0 = 500 data points.,4.1. Toy example,[0],[0]
"We implement the aggregations by the GPML toolbox5 using the SE kernel in (1) and the conjugate gradients algorithm with the maximum number of evaluations as 500, and execute the code on a workstation with four 3.70 GHz cores and 16 GB RAM (multi-core computing in Matalb is employed).",4.1. Toy example,[0],[0]
"Finally, we use the Standardized Mean Square Error (SMSE) to evaluate the accuracy of prediction mean, and the Mean Standardized Log Loss (MSLL) to quantify the quality of predictive distribution (Rasmussen & Williams, 2006).
",4.1. Toy example,[0],[0]
Fig. 2 depicts the comparative results of six aggregation models on the toy example.,4.1. Toy example,[0],[0]
Note that NPAE using n > 5,4.1. Toy example,[0],[0]
× 104 is unavailable due to the time-consuming prediction process.,4.1. Toy example,[0],[0]
"Fig. 2(a) shows that these models require the same training time, but they differ in the predicting time.",4.1. Toy example,[0],[0]
"Due to the communication expert, the GRBCM’s predicting time slightly offsets the curves of (G)PoE and (R)BCM.",4.1. Toy example,[0],[0]
The NPAE however exhibits significantly larger predicting time with increasing M and n′.,4.1. Toy example,[0],[0]
"Besides, Fig. 2(b) and (c) reveal that GRBCM and NPAE yield better predictions with increasing n, which confirm their consistency when n→∞.6 As for NPAE, though performing slightly better than GRBCM using n = 5× 104, it requires several orders of magnitude larger predicting time, rendering it unsuitable for cases with many test points and subsets.
",4.1. Toy example,[0],[0]
"Fig. 3 illustrates the six aggregation models using n = 104 and n = 5× 105, respectively, in comparison to the full GP (ground truth) using n = 104.7",4.1. Toy example,[0],[0]
"It is observed that in terms
5http://www.gaussianprocess.org/gpml/ code/matlab/doc/
6Further discussions of GRBCM is shown in Appendix D. 7The full GP is intractable using our computer for n = 5×105.
of prediction mean, as discussed in remark 1, PoE and GPoE provide poorer results in the entire domain with increasing n.",4.1. Toy example,[0],[0]
"On the contrary, BCM and RBCM provide good predictions in the range [0, 1].",4.1. Toy example,[0],[0]
"As discussed in remark 2, BCM however yields unreliable predictions when leaving the training data.",4.1. Toy example,[0],[0]
RBCM alleviates the issue by using a varying βi.,4.1. Toy example,[0],[0]
"In terms of prediction variance, with increasing n, PoE and (R)BCM tend to shrink to zero (overconfident), while GPoE tends to approach σ2∗∗ (too conservative).",4.1. Toy example,[0],[0]
"Particularly, PoE always has the largest MSLL value in Fig. 2(b), since as discussed in remark 2, its prediction variance approaches zero faster.",4.1. Toy example,[0],[0]
"We use two realistic datasets, kin40k (8D, 104 training points, 3× 104 test points) (Seeger et al., 2003) and sarcos (21D, 44484 training points, 4449 test points) (Rasmussen & Williams, 2006), to assess the performance of our approach.
",4.2. Medium-scale datasets,[0],[0]
The comparison includes all the aggregations except the expensive NPAE.8,4.2. Medium-scale datasets,[0],[0]
"Besides, we employ the fully independent training conditional (FITC) (Snelson & Ghahramani, 2006), the GP using stochastic variational inference (SVI)9 (Hensman et al., 2013), and the subset-of-data (SOD) (Chalupka et al., 2013) for comparison.",4.2. Medium-scale datasets,[0],[0]
"We select the inducing size m for FITC and SVI, the batch size mb for SVI, and the
8The comparison of NPAE and GRBCM are separately provided in Appendix E.
9https://github.com/SheffieldML/GPy
subset size msod for SOD, such that the computing time is similar to or a bit larger than that of GRBCM.",4.2. Medium-scale datasets,[0],[0]
"Particularly, we choose m = 200, mb = 0.1n and msod = 2500 for kin40k, and m = 300, mb = 0.1n and msod = 3000 for sarcos.",4.2. Medium-scale datasets,[0],[0]
"Differently, SVI employs the stochastic gradients algorithm with tsg = 1200 iterations.",4.2. Medium-scale datasets,[0],[0]
"Finally, we adopt the disjoint partition used before to divide the kin40k dataset into 16 subsets, and the sarcos dataset into 72 subsets for the aggregations.",4.2. Medium-scale datasets,[0],[0]
"Each experiment is repeated ten times.
",4.2. Medium-scale datasets,[0],[0]
Fig. 4 depicts the comparative results of different approximation models over 10 runs on the kin40k and sarcos datasets.,4.2. Medium-scale datasets,[0],[0]
The horizontal axis represents the sum of training and predicting time.,4.2. Medium-scale datasets,[0],[0]
It is first observed that GRBCM provides the best performance on the two datasets in terms of both SMSE and MSLL at the cost of requiring a bit more computing time than (G)PoE and (R)BCM.,4.2. Medium-scale datasets,[0],[0]
"As for (R)BCM, the small SMSE values reveal that they provide better prediction mean than FITC and SOD; but the large MSLL values again confirm that they provide overconfident prediction variance.",4.2. Medium-scale datasets,[0],[0]
"As for (G)PoE, they suffer from poor prediction mean, as indicated by the large SMSE; but GPoE performs well in terms of MSLL.",4.2. Medium-scale datasets,[0],[0]
"Finally, the simple SOD outperforms FITC and SVI on the kin40k dataset, and performs similarly on the sarcos dataset, which are consistent with the findings in (Chalupka et al., 2013).
",4.2. Medium-scale datasets,[0],[0]
"Next, we explore the impact of the number M of experts on the performance of aggregations.",4.2. Medium-scale datasets,[0],[0]
"To this end, we run them on the kin40k dataset with M respectively being 8, 16 and 64, and we run on the sarcos dataset with M respectively being 36, 72 and 288.",4.2. Medium-scale datasets,[0],[0]
"The results in Fig. 5 turn out that all the aggregations perform worse with increasing M , since the experts become weaker; but GRBCM still yields the best performance with different M .",4.2. Medium-scale datasets,[0],[0]
"Besides, with increasing M , the poor prediction mean and the vanishing prediction variance of PoE result in the sharp increase of MSLL values.
",4.2. Medium-scale datasets,[0],[0]
"Finally, we investigate the impact of data partition (disjoint or random) on the performance of aggregations.",4.2. Medium-scale datasets,[0],[0]
The average results in Fig. 6 turn out that the disjoint partition is more beneficial for the aggregations.,4.2. Medium-scale datasets,[0],[0]
"The results are expectable since the disjoint subsets provide separate and refined local information, whereas the random subsets provide overlapping and coarse global information.",4.2. Medium-scale datasets,[0],[0]
"But we observe that GPoE performs well on the sarcos dataset using random partition, which confirms the conclusions in Proposition 2.",4.2. Medium-scale datasets,[0],[0]
"Besides, as revealed in remark 3, even using random partition, GRBCM outperforms GPoE.",4.2. Medium-scale datasets,[0],[0]
This section explores the performance of aggregations and SVI on two large-scale datasets.,4.3. Large-scale datasets,[0],[0]
"We first assess them on the 90D song dataset, which is a subset of the million song dataset (Bertin-Mahieux et al., 2011).",4.3. Large-scale datasets,[0],[0]
"The song dataset is partitioned into 450000 training points and 65345 test
points.",4.3. Large-scale datasets,[0],[0]
We then assess the models on the 11D electric dataset that is partitioned into 1.8 million training points and 249280 test points.,4.3. Large-scale datasets,[0],[0]
"We follow the normalization and data pre-processing in (Wilson et al., 2016) to generate the two datasets.10 For the song dataset, we use the foregoing disjoint partition to divide it into M = 720 subsets, and use m = 800, mb = 5000 and tsg = 1300 for SVI; for the electric dataset, we divide it into M = 2880 subsets, and use m = 1000, mb = 5000 and tsg = 1500 for SVI.",4.3. Large-scale datasets,[0],[0]
"As a result, each expert is assigned with m0 = 625 data points for the aggregations.
",4.3. Large-scale datasets,[0],[0]
Table 1 reveals that the (G)PoE’s SMSE value is smaller than that of (R)BCM on the song dataset.,4.3. Large-scale datasets,[0],[0]
The poor prediction mean of BCM is caused by the fact that the song dataset is highly clustered such that BCM suffers from weak experts in regions with scarce points.,4.3. Large-scale datasets,[0],[0]
"On the contrary, due to the almost uniform distribution of the electric data points, the (R)BCM’s SMSE is much smaller than that of (G)PoE. Besides, unlike the vanishing prediction variances of PoE and (R)BCM when n → ∞, GPoE provides conservative prediction variance, resulting in small MSLL values on the two datasets.",4.3. Large-scale datasets,[0],[0]
The proposed GRBCM always outperforms the other aggregations in terms of both SMSE and MSLL on the two datasets due to the consistency.,4.3. Large-scale datasets,[0],[0]
"Finally, GRBCM performs similarly to SVI on the song dataset; but GRBCM outperforms SVI on the electric dataset.",4.3. Large-scale datasets,[0],[0]
"To scale the standard GP to large-scale regression, we present the GRBCM aggregation model, which introduces a global communication expert to yield consistent yet efficient predictions when n → ∞. Through theoretical and empirical analyses, we demonstrated the superiority of GRBCM over existing aggregations on datasets with up to 1.8M training points.
",5. Conclusions,[0],[0]
The superiority of local experts is the capability of capturing local patterns.,5. Conclusions,[0],[0]
"Hence, further works will consider the experts with individual hyperparameters in order to capture non-stationary and heteroscedastic features.
",5. Conclusions,[0],[0]
10The datasets and the pre-processing scripts are available in https://people.orie.cornell.edu/andrew/.,5. Conclusions,[0],[0]
This work was conducted within the Rolls-Royce@NTU Corporate Lab with support from the National Research Foundation (NRF) Singapore under the Corp Lab@University Scheme.,Acknowledgements,[0],[0]
It is also partially supported by the Data Science and Artificial Intelligence Research Center (DSAIR) and the School of Computer Science and Engineering at Nanyang Technological University.,Acknowledgements,[0],[0]
"In order to scale standard Gaussian process (GP) regression to large-scale datasets, aggregation models employ factorized training process and then combine predictions from distributed experts.",abstractText,[0],[0]
"The state-of-the-art aggregation models, however, either provide inconsistent predictions or require time-consuming aggregation process.",abstractText,[0],[0]
"We first prove the inconsistency of typical aggregations using disjoint or random data partition, and then present a consistent yet efficient aggregation model for large-scale GP.",abstractText,[0],[0]
"The proposed model inherits the advantages of aggregations, e.g., closed-form inference and aggregation, parallelization and distributed computing.",abstractText,[0],[0]
"Furthermore, theoretical and empirical analyses reveal that the new aggregation model performs better due to the consistent predictions that converge to the true underlying function when the training size approaches infinity.",abstractText,[0],[0]
Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1163–1172, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics
Language models (LMs) are statistical models that calculate probabilities over sequences of words or other discrete symbols. Currently two major paradigms for language modeling exist: count-based n-gram models, which have advantages of scalability and test-time speed, and neural LMs, which often achieve superior modeling performance. We demonstrate how both varieties of models can be unified in a single modeling framework that defines a set of probability distributions over the vocabulary of words, and then dynamically calculates mixture weights over these distributions. This formulation allows us to create novel hybrid models that combine the desirable features of count-based and neural LMs, and experiments demonstrate the advantages of these approaches.1",text,[0],[0]
"Language models (LMs) are statistical models that, given a sentence wI1",1 Introduction,[0],[0]
":= w1, . . .",1 Introduction,[0],[0]
", wI , calculate its probability P (wI1).",1 Introduction,[0],[0]
"LMs are widely used in applications such as machine translation and speech recognition, and because of their broad applicability they have also been widely studied in the literature.",1 Introduction,[0],[0]
"The most traditional and broadly used language modeling paradigm is that of count-based LMs, usually smoothed n-grams (Witten and Bell, 1991; Chen
1Work was performed while GN was at the Nara Institute of Science and Technology and CD was at Carnegie Mellon University.",1 Introduction,[0],[0]
"Code and data to reproduce experiments is available at http://github.com/neubig/modlm
and Goodman, 1996).",1 Introduction,[0],[0]
"Recently, there has been a focus on LMs based on neural networks (Nakamura et al., 1990; Bengio et al., 2006; Mikolov et al., 2010), which have shown impressive improvements in performance over count-based LMs.",1 Introduction,[0],[0]
"On the other hand, these neural LMs also come at the cost of increased computational complexity at both training and test time, and even the largest reported neural LMs (Chen et al., 2015; Williams et al., 2015) are trained on a fraction of the data of their count-based counterparts (Brants et al., 2007).
",1 Introduction,[0],[0]
"In this paper we focus on a class of LMs, which we will call mixture of distributions LMs (MODLMs; §2).",1 Introduction,[0],[0]
"Specifically, we define MODLMs as all LMs that take the following form, calculating the probabilities of the next word in a sentence wi given preceding context c according to a mixture of several component probability distributions Pk(wi|c):
P (wi|c) = K∑
k=1
λk(c)Pk(wi|c).",1 Introduction,[0],[0]
"(1)
Here, λk(c) is a function that defines the mixture weights, with the constraint that ∑K k=1 λk(c)",1 Introduction,[0],[0]
= 1 for all c.,1 Introduction,[0],[0]
"This form is not new in itself, and widely used both in the calculation of smoothing coefficients for n-gram LMs (Chen and Goodman, 1996), and interpolation of LMs of various varieties (Jelinek and Mercer, 1980).
",1 Introduction,[0],[0]
"The main contribution of this paper is to demonstrate that depending on our definition of c, λk(c), and Pk(wi|c), Eq. 1 can be used to describe not only n-gram models, but also feed-forward (Nakamura et al., 1990; Bengio et al., 2006; Schwenk, 2007) and
1163
recurrent (Mikolov et al., 2010; Sundermeyer et al., 2012) neural network LMs (§3).",1 Introduction,[0],[0]
"This observation is useful theoretically, as it provides a single mathematical framework that encompasses several widely used classes of LMs.",1 Introduction,[0],[0]
"It is also useful practically, in that this new view of these traditional models allows us to create new models that combine the desirable features of n-gram and neural models, such as:
neurally interpolated n-gram LMs (§4.1), which learn the interpolation weights of n-gram models using neural networks, and
neural/n-gram hybrid LMs (§4.2), which add a count-based n-gram component to neural models, allowing for flexibility to add large-scale external data sources to neural LMs.
",1 Introduction,[0],[0]
"We discuss learning methods for these models (§5) including a novel method of randomly dropping out more easy-to-learn distributions to prevent the parameters from falling into sub-optimal local minima.
",1 Introduction,[0],[0]
Experiments on language modeling benchmarks (§6) find that these models outperform baselines in terms of performance and convergence speed.,1 Introduction,[0],[0]
"As mentioned above, MODLMs are LMs that take the form of Eq. 1.",2 Mixture of Distributions LMs,[0],[0]
"This can be re-framed as the following matrix-vector multiplication:
pᵀc = Dcλ ᵀ c,
where pc is a vector with length equal to vocabulary size, in which the jth element pc,j corresponds to P (wi = j|c), λc is a size K vector that contains the mixture weights for the distributions, and Dc is a Jby-K matrix, where element dc,j,k is equivalent to the probability Pk(wi = j|c).2 An example of this formulation is shown in Fig. 1.
",2 Mixture of Distributions LMs,[0],[0]
"Note that all columns in D represent probability distributions, and thus must sum to one over the J words in the vocabulary, and that all λ must sum to 1 over the K distributions.",2 Mixture of Distributions LMs,[0],[0]
"Under this condition, the vector pwill represent a well-formed probability distribution as well.",2 Mixture of Distributions LMs,[0],[0]
"This conveniently allows us to
2We omit the subscript c when appropriate.
calculate the probability of a single word wi = j by calculating the product of the jth row of Dc and λ ᵀ c
Pk(wi = j|c) = dc,jλᵀc.
",2 Mixture of Distributions LMs,[0],[0]
In the sequel we show how this formulation can be used to describe several existing LMs (§3) as well as several novel model structures that are more powerful and general than these existing models (§4).,2 Mixture of Distributions LMs,[0],[0]
"3.1 n-gram LMs as Mixtures of Distributions
First, we discuss how count-based interpolated ngram LMs fit within the MODLM framework.
",3 Existing LMs as Linear Mixtures,[0],[0]
Maximum likelihood estimation: n-gram models predict the next word based on the previous N -1 words.,3 Existing LMs as Linear Mixtures,[0],[0]
"In other words, we set c = wi−1i−N+1 and calculate P (wi|wi−1i−N+1).",3 Existing LMs as Linear Mixtures,[0],[0]
"The maximum-likelihood (ML) estimate for this probability is
PML(wi|wi−1i−N+1) = c(wii−N+1)/c(wi−1i−N+1),
where c(·) counts frequency in the training corpus.",3 Existing LMs as Linear Mixtures,[0],[0]
Interpolation:,3 Existing LMs as Linear Mixtures,[0],[0]
"Because ML estimation assigns zero probability to word sequences where c(wii−N+1) = 0, n-gram models often interpolate the ML distributions for sequences of length 1 to N .",3 Existing LMs as Linear Mixtures,[0],[0]
"The simplest form is static interpolation
P (wi|wi−1i−n+1) = N∑
n=1
λS,nPML(wi|wi−1i−n+1).",3 Existing LMs as Linear Mixtures,[0],[0]
"(2)
λS is a vector where λS,n represents the weight put on the distribution PML(wi|wi−1i−n+1).",3 Existing LMs as Linear Mixtures,[0],[0]
"This can be expressed as linear equations (Fig. 2a) by setting the nth column of D to the ML distribution PML(wi|wi−1i−n+1), and λ(c) equal to λS .
",3 Existing LMs as Linear Mixtures,[0],[0]
"Static interpolation can be improved by calculating λ(c) dynamically, using heuristics based on the frequency counts of the context (Good, 1953; Katz, 1987; Witten and Bell, 1991).",3 Existing LMs as Linear Mixtures,[0],[0]
"These methods define a context-sensitive fallback probability α(wi−1i−n+1) for order n models, and recursively calculate the probability of the higher order models from the lower order models:
P (wi|wi−1i−n+1) = α(wi−1i−n+1)P (wi|wi−1i−n+2)+ (1− α(wi−1i−n+1))PML(wi|wi−1i−n+1).",3 Existing LMs as Linear Mixtures,[0],[0]
"(3)
To express this as a linear mixture, we convert α(wi−1i−n+1) into the appropriate value for λn(w i−1 i−N+1).",3 Existing LMs as Linear Mixtures,[0],[0]
"Specifically, the probability assigned to each PML(wi|wi−1i−n+1) is set to the product of the fallbacks α for all higher orders and the probability of not falling back (1− α) at the current level:
λn(w i−1 i−N+1) = (1−α(wi−1i−n+1))
N∏
ñ=n+1
α(wi−1i−ñ+1).
",3 Existing LMs as Linear Mixtures,[0],[0]
"Discounting: The widely used technique of discounting (Ney et al., 1994) defines a fixed discount d and subtracts it from the count of each word before calculating probabilities:
PD(wi|wi−1i−n+1) =",3 Existing LMs as Linear Mixtures,[0],[0]
"(c(wii−n+1)− d)/c(wi−1i−n+1).
",3 Existing LMs as Linear Mixtures,[0],[0]
"Discounted LMs then assign the remaining probability mass after discounting as the fallback probability
βD(w i−1 i−n+1)",3 Existing LMs as Linear Mixtures,[0],[0]
=,3 Existing LMs as Linear Mixtures,[0],[0]
"1−
J∑
j=1
PD(wi = j|wi−1i−n+1),
P (wi|wi−1i−n+1) =βD(wi−1i−n+1)P (wi|wi−1i−n+2)+ PD(wi|wi−1i−n+1).",3 Existing LMs as Linear Mixtures,[0],[0]
"(4)
In this case, PD(·) does not add to one, and thus violates the conditions for MODLMs stated in §2, but it is easy to turn discounted LMs into interpolated LMs by normalizing the discounted distribution:
PND(wi|wi−1i−n+1) =",3 Existing LMs as Linear Mixtures,[0],[0]
"PD(wi|wi−1i−n+1)∑J
j=1 PD(wi = j|wi−1i−n+1) ,
which allows us to replace β(·) for α(·) and PND(·) for PML(·) in Eq. 3, and proceed as normal.
",3 Existing LMs as Linear Mixtures,[0],[0]
"Kneser–Ney (KN; Kneser and Ney (1995)) and Modified KN (Chen and Goodman, 1996) smoothing further improve discounted LMs by adjusting the counts of lower-order distributions to more closely match their expectations as fallbacks for higher order distributions.",3 Existing LMs as Linear Mixtures,[0],[0]
"Modified KN is currently the defacto standard in n-gram LMs despite occasional improvements (Teh, 2006; Durrett and Klein, 2011), and we will express it as PKN (·).",3 Existing LMs as Linear Mixtures,[0],[0]
"In this section we demonstrate how neural network LMs can also be viewed as an instantiation of the MODLM framework.
",3.2 Neural LMs as Mixtures of Distributions,[0],[0]
Feed-forward neural network LMs:,3.2 Neural LMs as Mixtures of Distributions,[0],[0]
"Feedforward LMs (Bengio et al., 2006; Schwenk, 2007) are LMs that, like n-grams, calculate the probability of the next word based on the previous words.",3.2 Neural LMs as Mixtures of Distributions,[0],[0]
"Given context wi−1i−N+1, these words are converted into real-valued word representation vectors ri−1i−N+1, which are concatenated into an overall representation vector q = ⊕(ri−1i−N+1), where ⊕(·) is the vector concatenation function.",3.2 Neural LMs as Mixtures of Distributions,[0],[0]
q is then run through a series of affine transforms and nonlinearities defined as function NN(q) to obtain a vector h.,3.2 Neural LMs as Mixtures of Distributions,[0],[0]
"For example, for a one-layer neural net-
work with a tanh non-linearity we can define
NN(q)",3.2 Neural LMs as Mixtures of Distributions,[0],[0]
":= tanh(qWq + bq), (5)
where Wq and bq are weight matrix and bias vector parameters respectively.",3.2 Neural LMs as Mixtures of Distributions,[0],[0]
"Finally, the probability vector p is calculated using the softmax function p = softmax(hWs + bs), similarly parameterized.
",3.2 Neural LMs as Mixtures of Distributions,[0],[0]
"As these models are directly predicting p with no concept of mixture weights λ, they cannot be interpreted as MODLMs as-is.",3.2 Neural LMs as Mixtures of Distributions,[0],[0]
"However, we can perform a trick shown in Fig.",3.2 Neural LMs as Mixtures of Distributions,[0],[0]
"2b, not calculating p directly, but instead calculating mixture weights λ = softmax(hWs + bs), and defining the MODLM’s distribution matrix D as a J-by-J identity matrix.",3.2 Neural LMs as Mixtures of Distributions,[0],[0]
"This is equivalent to defining a linear mixture of J Kronecker δj distributions, the jth of which assigns a probability of 1 to word j and zero to everything else, and estimating the mixture weights with a neural network.",3.2 Neural LMs as Mixtures of Distributions,[0],[0]
"While it may not be clear why it is useful to define neural LMs in this somewhat roundabout way, we describe in §4 how this opens up possibilities for novel expansions to standard models.
",3.2 Neural LMs as Mixtures of Distributions,[0],[0]
"Recurrent neural network LMs: LMs using recurrent neural networks (RNNs) (Mikolov et al., 2010) consider not the previous few words, but also maintain a hidden state summarizing the sentence up until this point by re-defining the net in Eq. 5 as
RNN(qi) := tanh(qiWq + hi−1Wh + bq),
where qi is the current input vector and hi−1 is the hidden vector at the previous time step.",3.2 Neural LMs as Mixtures of Distributions,[0],[0]
"This allows for consideration of long-distance dependencies beyond the scope of standard n-grams, and LMs using RNNs or long short-term memory (LSTM) networks (Sundermeyer et al., 2012) have posted large improvements over standard n-grams and feed-forward
models.",3.2 Neural LMs as Mixtures of Distributions,[0],[0]
"Like feed-forward LMs, LMs using RNNs can be expressed as MODLMs by predicting λ instead of predicting p directly.",3.2 Neural LMs as Mixtures of Distributions,[0],[0]
"This section describes how we can use this framework of MODLMs to design new varieties of LMs that combine the advantages of both n-gram and neural network LMs.
4.1 Neurally Interpolated n-gram Models
The first novel instantiation of MODLMs that we propose is neurally interpolated n-gram models, shown in Fig. 3a.",4 Novel Applications of MODLMs,[0],[0]
"In these models, we setD to be the same matrix used in n-gram LMs, but calculateλ(c) using a neural network model.",4 Novel Applications of MODLMs,[0],[0]
"As λ(c) is learned from data, this framework has the potential to allow us to learn more intelligent interpolation functions than the heuristics described in §3.1.",4 Novel Applications of MODLMs,[0],[0]
"In addition, because the neural network only has to calculate a softmax over N distributions instead of J vocabulary words, training and test efficiency of these models can be expected to be much greater than that of standard neural network LMs.
",4 Novel Applications of MODLMs,[0],[0]
"Within this framework, there are several design decisions.",4 Novel Applications of MODLMs,[0],[0]
"First, how we decide D: do we use the maximum likelihood estimate PML or KN estimated distributions PKN?",4 Novel Applications of MODLMs,[0],[0]
"Second, what do we provide as input to the neural network to calculate the mixture weights?",4 Novel Applications of MODLMs,[0],[0]
"To provide the neural net with the same information used by interpolation heuristics used in traditional LMs, we first calculate three features for each of the N contexts wi−1i−n+1: a binary feature indicating whether the context has been observed in the training corpus (c(wi−1i−n+1) > 0), the log frequency of the context counts (log(c(wi−1i−n+1)) or
zero for unobserved contexts), and the log frequency of the number of unique words following the context (log(u(wi−1i−n+1)) or likewise zero).",4 Novel Applications of MODLMs,[0],[0]
"When using discounted distributions, we also use the log of the sum of the discounted counts as a feature.",4 Novel Applications of MODLMs,[0],[0]
"We can also optionally use the word representation vector q used in neural LMs, allowing for richer representation of the input, but this may or may not be necessary in the face of the already informative count-based features.
4.2 Neural/n-gram Hybrid Models
Our second novel model enabled by MODLMs is neural/n-gram hybrid models, shown in Fig.",4 Novel Applications of MODLMs,[0],[0]
3b.,4 Novel Applications of MODLMs,[0],[0]
"These models are similar to neurally interpolated n-grams, but D is augmented with J additional columns representing the Kronecker δj distributions used in the standard neural LMs.",4 Novel Applications of MODLMs,[0],[0]
"In this construction, λ is still a stochastic vector, but its contents are both the mixture coefficients for the count-based models and direct predictions of the probabilities of words.",4 Novel Applications of MODLMs,[0],[0]
"Thus, the learned LM can use count-based models when they are deemed accurate, and deviate from them when deemed necessary.
",4 Novel Applications of MODLMs,[0],[0]
This model is attractive conceptually for several reasons.,4 Novel Applications of MODLMs,[0],[0]
"First, it has access to all information used by both neural and n-gram LMs, and should be able to perform as well or better than both models.",4 Novel Applications of MODLMs,[0],[0]
"Second, the efficiently calculated n-gram counts are likely sufficient to capture many phenomena necessary for language modeling, allowing the neural component to focus on learning only the phenomena that are not well modeled by n-grams, requiring fewer parameters and less training time.",4 Novel Applications of MODLMs,[0],[0]
"Third, it is possible to train n-grams from much larger amounts of data, and use these massive models to bootstrap learning of neural nets on smaller datasets.",4 Novel Applications of MODLMs,[0],[0]
"While the MODLM formulations of standard heuristic n-gram LMs do not require learning, the remaining models are parameterized.",5 Learning Mixtures of Distributions,[0],[0]
This section discusses the details of learning these parameters.,5 Learning Mixtures of Distributions,[0],[0]
The first step in learning parameters is defining our training objective.,5.1 Learning MODLMs,[0],[0]
"Like most previous work on LMs (Bengio et al., 2006), we use a negative log-
likelihood loss summed over words wi in every sentence w in corpusW
L(W) =",5.1 Learning MODLMs,[0],[0]
"− ∑
w∈W
∑
wi∈w logP (wi|c),
where c represents all words preceding wi inw that are used in the probability calculation.",5.1 Learning MODLMs,[0],[0]
"As noted in Eq. 2, P (wi = j|c) can be calculated efficiently from the distribution matrix Dc and mixture function output λc.
",5.1 Learning MODLMs,[0],[0]
"Given that we can calculate the log likelihood, the remaining parts of training are similar to training for standard neural network LMs.",5.1 Learning MODLMs,[0],[0]
"As usual, we perform forward propagation to calculate the probabilities of all the words in the sentence, back-propagate the gradients through the computation graph, and perform some variant of stochastic gradient descent (SGD) to update the parameters.",5.1 Learning MODLMs,[0],[0]
"While the training method described in the previous section is similar to that of other neural network models, we make one important modification to the training process specifically tailored to the hybrid models of §4.2.
",5.2 Block Dropout for Hybrid Models,[0],[0]
"This is motivated by our observation (detailed in §6.3) that the hybrid models, despite being strictly more expressive than the corresponding neural network LMs, were falling into poor local minima with higher training error than neural network LMs.",5.2 Block Dropout for Hybrid Models,[0],[0]
"This is because at the very beginning of training, the count-based elements of the distribution matrix in Fig.",5.2 Block Dropout for Hybrid Models,[0],[0]
"3b are already good approximations of the target distribution, while the weights of the single-word δj distributions are not yet able to provide accurate probabilities.",5.2 Block Dropout for Hybrid Models,[0],[0]
"Thus, the model learns to set the mixture proportions of the δ elements to near zero and rely mainly on the count-based n-gram distributions.
",5.2 Block Dropout for Hybrid Models,[0],[0]
"To encourage the model to use the δ mixture components, we adopt a method called block dropout (Ammar et al., 2016).",5.2 Block Dropout for Hybrid Models,[0],[0]
"In contrast to standard dropout (Srivastava et al., 2014), which drops out single nodes or connections, block dropout randomly drops out entire subsets of network nodes.",5.2 Block Dropout for Hybrid Models,[0],[0]
"In our case, we want to prevent the network from overusing the count-based n-gram distributions, so for a randomly selected portion of the training examples (here, 50%) we disable all n-gram distributions and
force the model to rely on only the δ distributions.",5.2 Block Dropout for Hybrid Models,[0],[0]
"To do so, we zero out all elements in λ(c) that correspond to n-gram distributions, and re-normalize over the rest of the elements so they sum to one.",5.2 Block Dropout for Hybrid Models,[0],[0]
"Finally, we note design details that were determined based on preliminary experiments.
",5.3 Network and Training Details,[0],[0]
"Network structures: We used both feed-forward networks with tanh non-linearities and LSTM (Hochreiter and Schmidhuber, 1997) networks.",5.3 Network and Training Details,[0],[0]
"Most experiments used single-layer 200-node networks, and 400-node networks were used for experiments with larger training data.",5.3 Network and Training Details,[0],[0]
Word representations were the same size as the hidden layer.,5.3 Network and Training Details,[0],[0]
"Larger and multi-layer networks did not yield improvements.
",5.3 Network and Training Details,[0],[0]
"Training: We used ADAM (Kingma and Ba, 2015) with a learning rate of 0.001, and minibatch sizes of 512 words.",5.3 Network and Training Details,[0],[0]
"This led to faster convergence than standard SGD, and more stable optimization than other update rules.",5.3 Network and Training Details,[0],[0]
"Models were evaluated every 500k-3M words, and the model with the best development likelihood was used.",5.3 Network and Training Details,[0],[0]
"In addition to the block dropout of §5.2, we used standard dropout with a rate of 0.5 for both feed-forward (Srivastava et al., 2014) and LSTM (Pham et al., 2014) nets in the neural LMs and neural/n-gram hybrids, but not in the neurally interpolated n-grams, where it resulted in slightly worse perplexities.
",5.3 Network and Training Details,[0],[0]
"Features: If parameters are learned on the data used to train count-based models, they will heavily over-fit and learn to trust the count-based distributions too much.",5.3 Network and Training Details,[0],[0]
"To prevent this, we performed 10-fold cross validation, calculating count-based elements of D for each fold with counts trained on the other 9/10.",5.3 Network and Training Details,[0],[0]
"In addition, the count-based contextual features in §4.1 were normalized by subtracting the training set mean, which improved performance.",5.3 Network and Training Details,[0],[0]
"In this section, we perform experiments to evaluate the neurally interpolated n-grams (§6.2) and neural/n-gram hybrids (§6.3), the ability of our models to take advantage of information from large data sets (§6.4), and the relative performance compared
to post-facto static interpolation of already-trained models (§6.5).",6.1 Experimental Setup,[0],[0]
"For the main experiments, we evaluate on two corpora: the Penn Treebank (PTB) data set prepared by Mikolov et al. (2010),3 and the first 100k sentences in the English side of the ASPEC corpus (Nakazawa et al., 2015)4 (details in Tab. 1).",6.1 Experimental Setup,[0],[0]
"The PTB corpus uses the standard vocabulary of 10k words, and for the ASPEC corpus we use a vocabulary of the 20k most frequent words.",6.1 Experimental Setup,[0],[0]
"Our implementation is included as supplementary material.
",6.1 Experimental Setup,[0],[0]
"6.2 Results for Neurally Interpolated n-grams
First",6.1 Experimental Setup,[0],[0]
", we investigate the utility of neurally interpolated n-grams.",6.1 Experimental Setup,[0],[0]
"In all cases, we use a history of N = 5 and test several different settings for the models:
Estimation type: λ(c) is calculated with heuristics (HEUR) or by the proposed method using feedforward (FF), or LSTM nets.
Distributions: We compare PML(·) and PKN (·).",6.1 Experimental Setup,[0],[0]
"For heuristics, we use Witten-Bell for ML and the appropriate discounted probabilities for KN.
Input features: As input features for the neural network, we either use only the count-based features (C) or count-based features together with the word representation for the single previous word (CR).
",6.1 Experimental Setup,[0],[0]
"From the results shown in Tab. 2, we can first see that when comparing models using the same set of
3http://rnnlm.org/simple-examples.tgz 4http://lotus.kuee.kyoto-u.ac.jp/ASPEC/
input distributions, the neurally interpolated model outperforms corresponding heuristic methods.",6.1 Experimental Setup,[0],[0]
"We can also see that LSTMs have a slight advantage over FF nets, and models using word representations have a slight advantage over those that use only the count-based features.",6.1 Experimental Setup,[0],[0]
"Overall, the best model achieves a relative perplexity reduction of 4- 5% over KN models.",6.1 Experimental Setup,[0],[0]
"Interestingly, even when using simple ML distributions, the best neurally interpolated n-gram model nearly matches the heuristic KN method, demonstrating that the proposed model can automatically learn interpolation functions that are nearly as effective as carefully designed heuristics.5
6.3 Results for Neural/n-gram Hybrids
In experiments with hybrid models, we test a neural/n-gram hybrid LM using LSTM networks with both Kronecker δ and KN smoothed 5-gram distributions, trained either with or without block dropout.",6.1 Experimental Setup,[0],[0]
"As our main baseline, we compare to LSTMs with only δ distributions, which have reported competitive numbers on the PTB data set (Zaremba et al., 2014).6 We also report results for heuristically smoothed KN 5-gram models, and the best neurally interpolated n-grams from the previous section for reference.
",6.1 Experimental Setup,[0],[0]
"The results, shown in Tab. 3, demonstrate that similarly to previous research, LSTM LMs (2) achieve a large improvement in perplexity over ngram models, and that the proposed neural/n-gram hybrid method (5) further reduces perplexity by 10- 11% relative over this strong baseline.
",6.1 Experimental Setup,[0],[0]
"Comparing models without (4) and with (5) the proposed block dropout, we can see that this method contributes significantly to these gains.",6.1 Experimental Setup,[0],[0]
"To examine this more closely, we show the test perplexity for the
5Neurally interpolated n-grams are also more efficient than standard neural LMs, as mentioned in §4.1.",6.1 Experimental Setup,[0],[0]
"While a standard LSTM LM calculated 1.4kw/s on the PTB data, the neurally interpolated models using LSTMs and FF nets calculated 11kw/s and 58kw/s respectively, only slightly inferior to 140kw/s of heuristic KN.
6Note that unlike this work, we opt to condition only on insentence context, not inter-sentential dependencies, as training through gradient calculations over sentences is more straightforward and because examining the effect of cross-boundary information is not central to the proposed method.",6.1 Experimental Setup,[0],[0]
"Thus our baseline numbers are not directly comparable (i.e. have higher perplexity) to previous reported results on this data, but we still feel that the comparison is appropriate.
",6.1 Experimental Setup,[0],[0]
"three models using δ distributions in Fig. 5, and the amount of the probability mass in λ(c) assigned to the non-δ distributions in the hybrid models.",6.1 Experimental Setup,[0],[0]
"From this, we can see that the model with block dropout quickly converges to a better result than the LSTM LM, but the model without converges to a worse result, assigning too much probability mass to the dense count-based distributions, demonstrating the learning problems mentioned in §5.2.
",6.1 Experimental Setup,[0],[0]
It is also of interest to examine exactly why the proposed model is doing better than the more standard methods.,6.1 Experimental Setup,[0],[0]
One reason can be found in the behavior with regards to low-frequency words.,6.1 Experimental Setup,[0],[0]
"In Fig. 4, we show perplexities for words that appear n times or less in the training corpus, for n = 10, n = 100, n = 1000 and n = ∞ (all words).",6.1 Experimental Setup,[0],[0]
"From the results, we can first see that if we compare the baselines, LSTM language models achieve better perplexities overall but n-gram language models tend to perform better on low-frequency words, corroborating the observations of Chen et al. (2015).
",6.1 Experimental Setup,[0],[0]
"The neurally interpolated n-gram models consistently outperform standard KN-smoothed n-grams, demonstrating their superiority within this model class.",6.1 Experimental Setup,[0],[0]
"In contrast, the neural/n-gram hybrid models tend to follow a pattern more similar to that of LSTM language models, similarly with consistently higher performance.",6.1 Experimental Setup,[0],[0]
"To examine the ability of the hybrid models to use counts trained over larger amounts of data, we perform experiments using two larger data sets:
WSJ: The PTB uses data from the 1989 Wall Street Journal, so we add the remaining years between 1987 and 1994 (1.81M sents., 38.6M words).
",6.4 Results for Larger Data Sets,[0],[0]
"GW: News data from the English Gigaword 5th Edition (LDC2011T07, 59M sents., 1.76G words).
",6.4 Results for Larger Data Sets,[0],[0]
"We incorporate this data either by training net parameters over the whole large data, or by separately training count-based n-grams on each of PTB, WSJ, and GW, and learning net parameters on only PTB data.",6.4 Results for Larger Data Sets,[0],[0]
The former has the advantage of training the net on much larger data.,6.4 Results for Larger Data Sets,[0],[0]
"The latter has two main advantages: 1) when the smaller data is of a particular domain the mixture weights can be learned to match this in-domain data; 2) distributions can be trained on data such as Google n-grams (LDC2006T13), which contain n-gram counts but not full sentences.
",6.4 Results for Larger Data Sets,[0],[0]
"In the results of Fig. 6, we can first see that the neural/n-gram hybrids significantly outperform the traditional neural LMs in the scenario with larger data as well.",6.4 Results for Larger Data Sets,[0],[0]
"Comparing the two methods for incorporating larger data, we can see that the results are mixed depending on the type and size of the data
being used.",6.4 Results for Larger Data Sets,[0],[0]
"For the WSJ data, training on all data slightly outperforms the method of adding distributions, but when the GW data is added this trend reverses.",6.4 Results for Larger Data Sets,[0],[0]
"This can be explained by the fact that the GW data differs from the PTB test data, and thus the effect of choosing domain-specific interpolation coefficients was more prominent.",6.4 Results for Larger Data Sets,[0],[0]
"Finally, because the proposed neural/n-gram hybrid models combine the advantages of neural and ngram models, we compare with the more standard method of training models independently and combining them with static interpolation weights tuned on the validation set using the EM algorithm.",6.5 Comparison with Static Interpolation,[0],[0]
"Tab. 4 shows perplexities for combinations of a standard neural model (or δ distributions) trained on PTB, and count based distributions trained on PTB, WSJ, and GW are added one-by-one using the standard static and proposed LSTM interpolation methods.",6.5 Comparison with Static Interpolation,[0],[0]
"From the results, we can see that when only PTB data is used, the methods have similar results, but with the more diverse data sets the proposed method edges out its static counterpart.7
7In addition to better perplexities, neural/n-gram hybrids are trained in a single pass instead of performing post-facto interpolation, which may give advantages when training for other objectives (Auli and Gao, 2014; Li et al., 2015).",6.5 Comparison with Static Interpolation,[0],[0]
"A number of alternative methods focus on interpolating LMs of multiple varieties such as in-domain and out-of-domain LMs (Bulyko et al., 2003; Bacchiani et al., 2006; Gülçehre et al., 2015).",7 Related Work,[0],[0]
Perhaps most relevant is Hsu (2007)’s work on learning to interpolate multiple LMs using log-linear models.,7 Related Work,[0],[0]
"This differs from our work in that it learns functions to estimate the fallback probabilities αn(c) in Eq. 3 instead of λ(c), and does not cover interpolation of n-gram components, non-linearities, or the connection with neural network LMs.",7 Related Work,[0],[0]
"Also conceptually similar is work on adaptation of n-gram LMs, which start with n-gram probabilities (Della Pietra et al., 1992; Kneser and Steinbiss, 1993; Rosenfeld, 1996; Iyer and Ostendorf, 1999) and adapt them based on the distribution of the current document, albeit in a linear model.",7 Related Work,[0],[0]
"There has also been work incorporating binary n-gram features into neural language models, which allows for more direct learning of ngram weights (Mikolov et al., 2011), but does not afford many of the advantages of the proposed model such as the incorporation of count-based probability estimates.",7 Related Work,[0],[0]
"Finally, recent works have compared ngram and neural models, finding that neural models often perform better in perplexity, but n-grams have their own advantages such as effectiveness in extrinsic tasks (Baltescu and Blunsom, 2015) and better modeling of rare words (Chen et al., 2015).",7 Related Work,[0],[0]
"In this paper, we proposed a framework for language modeling that generalizes both neural network and count-based n-gram LMs.",8 Conclusion and Future Work,[0],[0]
"This allowed us to learn more effective interpolation functions for count-based n-grams, and to create neural LMs that incorporate information from count-based models.
",8 Conclusion and Future Work,[0],[0]
"As the framework discussed here is general, it is also possible that they could be used in other tasks that perform sequential prediction of words such as
neural machine translation (Sutskever et al., 2014) or dialog response generation (Sordoni et al., 2015).",8 Conclusion and Future Work,[0],[0]
"In addition, given the positive results using block dropout for hybrid models, we plan to develop more effective learning methods for mixtures of sparse and dense distributions.",8 Conclusion and Future Work,[0],[0]
"We thank Kevin Duh, Austin Matthews, Shinji Watanabe, and anonymous reviewers for valuable comments on earlier drafts.",Acknowledgements,[0],[0]
"This work was supported in part by JSPS KAKENHI Grant Number 16H05873, and the Program for Advancing Strategic International Networks to Accelerate the Circulation of Talented Researchers.",Acknowledgements,[0],[0]
Language models (LMs) are statistical models that calculate probabilities over sequences of words or other discrete symbols.,abstractText,[0],[0]
"Currently two major paradigms for language modeling exist: count-based n-gram models, which have advantages of scalability and test-time speed, and neural LMs, which often achieve superior modeling performance.",abstractText,[0],[0]
"We demonstrate how both varieties of models can be unified in a single modeling framework that defines a set of probability distributions over the vocabulary of words, and then dynamically calculates mixture weights over these distributions.",abstractText,[0],[0]
"This formulation allows us to create novel hybrid models that combine the desirable features of count-based and neural LMs, and experiments demonstrate the advantages of these approaches.1",abstractText,[0],[0]
Generalizing and Hybridizing Count-based and Neural Language Models,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 601–606 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
601",text,[0],[0]
"Word embeddings have been an essential part of neural-network based approaches for natural language processing tasks (Goldberg, 2016).",1 Introduction,[0],[0]
"However, many popular word embeddings techniques have a fixed vocabulary (Mikolov et al., 2013; Pennington et al., 2014), i.e., they can only provide vectors over a finite set of common words that appear frequently in a given corpus.",1 Introduction,[0],[0]
"Such methods fail to generate vectors for rare words and words not present in the training corpus, but appearing in the test corpus or downstream task texts, raising difficulty for any methods relying on word vectors to efficiently extract useful features from text.",1 Introduction,[0],[0]
This is often referred to as the out-ofvocabulary (OOV) word problem.,1 Introduction,[0],[0]
"We aim to address this problem by inferring vectors for OOV words with only access to pre-trained vectors over a fixed vocabulary of common words and the OOV word itself without context.
",1 Introduction,[0],[0]
The motivations come from both linguistics and natural language processing applications.,1 Introduction,[0],[0]
"First, from a linguistic view a word can be decomposed
into multiple morphemes: stems, affixes, modifiers and etc.",1 Introduction,[0],[0]
This is more often the case for rare words.,1 Introduction,[0],[0]
"In some field such as chemistry and agglutinative languages such as Turkish, there exists a systematic way of composing words from morphemes.",1 Introduction,[0],[0]
"Some can even be arbitrarily long.
",1 Introduction,[0],[0]
"Apart from the explicit and systematic way of making words, we can also observe the ability of a language speaker to infer the meaning of an unseen word.",1 Introduction,[0],[0]
"For instance, one can guess that “preEMNLP” means “before EMNLP”, even without the presence of any context, suggesting that it is part of our implicit linguistic knowledge to infer meaning of an unseen word solely from its lexical form.",1 Introduction,[0],[0]
"This observation, together with the morpheme decomposition of many rare words, implies the feasibility of inferring their vectors from those for common words, and also raises the algorithmic question of how to compute them efficiently.
",1 Introduction,[0],[0]
"Second, there are many NLP applications where estimating word embeddings of OOV is critical.",1 Introduction,[0],[0]
"For instance, in the case of analyzing Twitter data, while there exists pre-trained word embeddings with giant vocabularies trained on massive number of tweets, such as GloVe vectors (Pennington et al., 2014), this would still not cover new words coined by users everyday.",1 Introduction,[0],[0]
"In such cases, it is more prudent to extend the available pre-trained vectors trained on very large corpora, so that we can estimate embeddings for OOV words, instead of retraining a new word / subword level embedding model on the new extended data corpus.
OOV words have always been a problem for methods that assume fixed vocabularies.",1 Introduction,[0],[0]
A common workaround is to view all OOV words as a special UNK token and use the same vector for all of them.,1 Introduction,[0],[0]
This would restrict any downstream models from accessing distinct features of those words.,1 Introduction,[0],[0]
"Thus, we would like a method to provide vectors that capture semantic and grammatical fea-
tures even for OOV words.",1 Introduction,[0],[0]
"We also would like such method to maximally rely on the word itself, instead of its context, as contextual information is already used later with sentence level models stacking over word vectors.
",1 Introduction,[0],[0]
"To achieve this, we aim to build a word embedding model that generalizes pre-trained word embeddings to OOV words.",1 Introduction,[0],[0]
"First, given word embeddings for a fixed vocabulary, our model learns the relationship between the subwords present in each word and its corresponding pre-trained word vector.",1 Introduction,[0],[0]
"Then, using the learned subword information, our model can generate word embeddings for any word, regardless if it is OOV or not.
",1 Introduction,[0],[0]
Contribution We propose a simple yet effective subword-level word embedding method that can be efficiently trained given pre-trained word vectors for a limited number of words.,1 Introduction,[0],[0]
"Once trained, our embedding model takes the characters n-grams in a word as input and gives its word vector as output.1
Our experiments on word similarity tasks in English and POS tagging in a variety of languages suggests that the proposed word embedder is able to mimic and generalize consistently the word vectors from in-vocabulary words to out-of-vocabulary words, and achieves state-ofthe-art scores for the tasks compared to previous subword-level word embedders trained under the same setting.",1 Introduction,[0],[0]
"This gives evidence that such a simple model is capable of capturing language speaker’s morphological knowledge, and also provides an easy way to generate word vectors for rare or unseen (OOV) words with potential application to various natural language processing tasks.
",1 Introduction,[0],[0]
"Related work There exist a large body of works that try to incorporate morphological information into word representations, e.g., (Alexandrescu and Kirchhoff, 2006; Luong et al., 2013a; Qiu et al., 2014; Botha and Blunsom, 2014; Cotterell and Schütze, 2015; Soricut and Och, 2015).",1 Introduction,[0],[0]
These approaches typically rely on the morphological decomposition of words.,1 Introduction,[0],[0]
"Some other approaches using subword information do not rely on morphological decomposition but requires context information from large text corpus (Schütze, 1993; Santos and Zadrozny, 2014; Ling et al., 2015; Wieting et al., 2016).
",1 Introduction,[0],[0]
"1The code is available at https://github.com/ jmzhao/bag-of-substring-embedder.
",1 Introduction,[0],[0]
"In particular, Bojanowski et al. (2017) introduced fastText, a word embedding method enhanced with subword (character n-gram) embeddings.",1 Introduction,[0],[0]
"They are able to generate vectors for OOV words, which has been shown useful for text classification (Joulin et al., 2016), but the model is to be trained over large text corpus.
",1 Introduction,[0],[0]
"Pinter et al. (2017) use a character-level bidirectional LSTM model called MIMICK, mapping from word strings to word vectors.",1 Introduction,[0],[0]
"The idea of using character-level recurrent neural networks (RNNs) for word vectors is not new (Ling et al., 2015; Plank et al., 2016), but as per authors’ knowledge, they are by far the only attempt to the exact task of generalizing word vectors from only pre-trained vectors with a fixed vocabulary, i.e. with no access to contextual information.",1 Introduction,[0],[0]
"Our Bag-of-Substring (BoS) word vector generation model views a word as a bag of its substrings, or character n-grams.",2 Bag-of-Substring Model,[0],[0]
"Specifically, we maintain a vector lookup table for each possible substrings (or character n-grams) of length between lmin and lmax.",2 Bag-of-Substring Model,[0],[0]
A word vector is then formed as the average of vectors of all its substrings with lengths in the range.,2 Bag-of-Substring Model,[0],[0]
"Let Σ be the finite set of characters in the language, subsba(s) = {t is substring of s : a ≤ |t| ≤ b} for string s ∈ Σ∗ be the set of substrings of s whose length is between a and b inclusive, and <s> be the concatenation of character <, string s and character > where <,> 6∈",2 Bag-of-Substring Model,[0],[0]
"Σ. The BoS embedding for a string/word s can be expressed as
BoS(s;V )",2 Bag-of-Substring Model,[0],[0]
"= 1 |S<s>| ∑
t∈S<s>
vt, (1)
where V ∈ Rd×(|Σ| lmin+···+|Σ|lmax ) are the parameters which stores the embeddings of dimension d for each possible substring of length between lmin and lmax, vt is the vector in V indexed by t, S<s> is a shorthand for subslmaxlmin (<s>).",2 Bag-of-Substring Model,[0],[0]
"Special characters <,> 6∈ Σ are used to mark the start and the end of the word and thus help the model to distinguish homographic morphemes that occur at different word parts, e.g. prefixes or suffixes.",2 Bag-of-Substring Model,[0],[0]
An example BoS representation for word infix is subs43(<infix>),2 Bag-of-Substring Model,[0],[0]
"= {<in, <inf, inf, infi, nfi, nfix, fix, fix>, ix>}.
",2 Bag-of-Substring Model,[0],[0]
"fastText (Bojanowski et al., 2017) uses the same idea for their word vector generation part.",2 Bag-of-Substring Model,[0],[0]
"How-
ever, unlike them, we train the model directly towards pre-trained vectors, instead of via context prediction over text corpora.
",2 Bag-of-Substring Model,[0],[0]
"Training Given pre-trained vectors for a set of common words, our model views them as targets and is trained to fit these targets.",2 Bag-of-Substring Model,[0],[0]
"Once the parameters (the vectors vt for the substrings) are learned, the model can then be used to infer vectors for rare words.",2 Bag-of-Substring Model,[0],[0]
Let,2 Bag-of-Substring Model,[0],[0]
U ∈,2 Bag-of-Substring Model,[0],[0]
Rd×|W | be the target vectors of the same dimension d over finite vocabulary W ⊂,2 Bag-of-Substring Model,[0],[0]
"Σ∗. Our model is trained by minimizing the overall loss between the generated and the given vectors for each word:
minimize V
1 |W",2 Bag-of-Substring Model,[0],[0]
"| ∑ w∈W l(BoS(w;V ), uw) (2)
where the loss function l(v, u) = 12‖v",2 Bag-of-Substring Model,[0],[0]
"− u‖ 2 2, namely the mean squared loss.",2 Bag-of-Substring Model,[0],[0]
"After training, one can use the learned V and Eqn (1) to compute the vector for any given word, even if it is OOV.
Hyperparameters We set the following hyperparameters for all the experiments.",2 Bag-of-Substring Model,[0],[0]
"For BoS model, lmin = 3 and lmax = 6 following Bojanowski et al. (2017).",2 Bag-of-Substring Model,[0],[0]
"Note that under this setting, S<s> can never be empty for non-empty string s. For optimization, stochastic gradient descent with learning rate 1 for 100 epochs.",2 Bag-of-Substring Model,[0],[0]
The dimension of the word vectors is not a hyperparameters here as it needs to agree with the target vector.,2 Bag-of-Substring Model,[0],[0]
"We run experiments to quantitatively evaluate the our model’s generalizability towards OOV words.
",3 Word Similarity,[0],[0]
The word similarity task asks to predict word similarity between a pair of two words.,3 Word Similarity,[0],[0]
"Given a set of pairs of words and gold labels for their similarities, the performance of word embeddings is measured by the correlation between the gold similarities and the similarities induced by the generated embeddings.",3 Word Similarity,[0],[0]
And we can thus imply how good our model is at generating word vectors.,3 Word Similarity,[0],[0]
"The word similarity here is computed using the cosine distance between the two word vectors, and the correlation is computed using Spearman’s ρ.
",3 Word Similarity,[0],[0]
Datasets We evaluate over Stanford RareWord (RW) introduced by Luong et al. (2013b) and WordSim353 (WS) introduced by Finkelstein et al. (2001).,3 Word Similarity,[0],[0]
"RW consists of less common words
so we use it to access our model’s ability to generalize word embeddings to OOV words.",3 Word Similarity,[0],[0]
"WS is composed of mostly common words and we use it to test if our subword-level models successfully mimic the target vectors.
",3 Word Similarity,[0],[0]
"Target vectors We train our BoS model over the English Polyglot vectors 2 to establish a direct comparison with results from MIMICK (Pinter et al., 2017), and as well as the Google word2vec vectors 3 which are popularly used in NLP tasks.",3 Word Similarity,[0],[0]
"Polyglot (Al-Rfou et al., 2013) is a multilingual NLP dataset, which also provides pre-trained word vectors over each language’s corpus with a vocabulary of 100,000 most frequent words.",3 Word Similarity,[0],[0]
"For Google vectors, most of their vocabulary consists of nonwords such as URLs and phrases, so we normalize tokens into ASCII characters by taking off all the diacritics and take only tokens consisting of a single word with all lower letters.",3 Word Similarity,[0],[0]
"Statistics of the processed vectors are summarized in Table 1, along with their word similarity task scores (for in-vocabulary words only) and OOV rate over the aforementioned evaluation sets.
",3 Word Similarity,[0],[0]
"Baselines We compare the scores with other subword-level models (fastText and MIMICK) and word similarity induced by non-parametric edit distance (EditDist).
",3 Word Similarity,[0],[0]
"fastText (Bojanowski et al., 2017) uses the same subword-level character n-gram model but is to be trained via context prediction over large text corpora (here English Wikipedia dump 4).",3 Word Similarity,[0],[0]
"MIM-
2http://polyglot.readthedocs.io/en/ latest/Download.html
3https://code.google.com/archive/p/ word2vec/
4https://fasttext.cc/docs/en/ pretrained-vectors.html
ICK (Pinter et al., 2017) is a character-level bidirectional LSTM word embedder trained against pre-trained word vectors (here Polyglot vectors 5).
",3 Word Similarity,[0],[0]
"Edit distance is defined between two strings as the smallest number of modifications: adding, deleting and changing one character, needed to turn one string into the other.",3 Word Similarity,[0],[0]
It can be computed using dynamic programming in O(|s1| × |s2|) time.,3 Word Similarity,[0],[0]
"The word similarity betweenw1 andw2 here is the edit distance normalized by the length of the longer word:
sEditDist(w1, w2) =",3 Word Similarity,[0],[0]
"− dedit(w1, w2)
max(|w1|, |w2|) (3)
where dedit is edit distance.
",3 Word Similarity,[0],[0]
Results Results are summarized in Table 2.,3 Word Similarity,[0],[0]
"When trained over Polyglot vectors, our BoS model works better than EditDist and MIMICK.",3 Word Similarity,[0],[0]
"When trained on Google vectors, the correlation scores are almost as good as those of fastText, the state-of-the-art subword level word embedder.",3 Word Similarity,[0],[0]
"However, unlike fastText, our model does not have access to word contexts in a large text corpus for training.",3 Word Similarity,[0],[0]
"In both cases, the significant differences of scores compared to those of EditDist, suggest that our model indeed learns to capture semantic similarities between words, rather than superficial similarities in spelling.
",3 Word Similarity,[0],[0]
"Comparing to MIMICK, our model is able to fill up 81% (14 to 36 against 41) and 73% (12 to 36 against 45) of the gaps in scores over RW and WS respectively.",3 Word Similarity,[0],[0]
"This improvement is more significant on RW with most (58%) of its words are OOV for the PloyGlot vectors, suggesting our model’s power in generating consistent word vectors for OOV words.",3 Word Similarity,[0],[0]
Surprisingly MIMICK performs no better than the edit distance baseline when evaluated on RW.,3 Word Similarity,[0],[0]
"Combined with the fact that it does no better for WS which has a near-zero OOV rate, it suggests MIMICK’s limited power of generalizing word vectors towards OOV words, or even reproduce consistent word vector for in-vocabulary words.",3 Word Similarity,[0],[0]
"As a sanity check, we see that all of the embedder models scores obviously better than EditDist when evaluated over common words (WS), showing that all of them are able to at least remember or mimic the word vectors for in-vocabulary words.
",3 Word Similarity,[0],[0]
Also note that our model is fast to train.,3 Word Similarity,[0],[0]
"With a naive single-thread CPU-only Python implemen-
5https://github.com/yuvalpinter/Mimick
tation, it can finish 100 epochs of training over English PolyGlot vectors within 352 seconds on a machine with an Intel Core i7-6700 (3.4 GHz) CPU, 32GB memory and 1TB SSD.",3 Word Similarity,[0],[0]
"Compared to fastText which, with a fast multithread C++ implementation, takes hours to be trained over giga bytes of text corpus, our method provides a cheap way to generalize reasonably good word vectors for OOV words.",3 Word Similarity,[0],[0]
"Besides word similarity, we try to access our embedders’ ability of capturing words’ syntactic and semantic features by evaluating with the task of predicting part-of-speech (POS) tags and morphosyntactic attributes for words in a sentence.",4 Joint Prediction of Part-of-Speech Tags and Morphosyntactic Attributes,[0],[0]
"For each word in a given sentence, the task asks for a POS tag and a label for each applicable morphosyntactic category, such as gender, case or tense.
",4 Joint Prediction of Part-of-Speech Tags and Morphosyntactic Attributes,[0],[0]
"Dataset We use Universal Dependencies (UD) dataset (Petrov et al., 2012) for this task.",4 Joint Prediction of Part-of-Speech Tags and Morphosyntactic Attributes,[0],[0]
UD is an open-community effort to build consistent annotated treebank cross many languages.,4 Joint Prediction of Part-of-Speech Tags and Morphosyntactic Attributes,[0],[0]
We pick the specific version 1.4 to enable a direct comparison with Pinter et al. (2017).,4 Joint Prediction of Part-of-Speech Tags and Morphosyntactic Attributes,[0],[0]
"Since we use PolyGlot vectors to train our word embedders, we conduct experiments on the 23 languages that appear in both Polyglot and UD 1.4.
",4 Joint Prediction of Part-of-Speech Tags and Morphosyntactic Attributes,[0],[0]
Model We adopt the same sentence-level bidirectional LSTM model from Pinter et al. (2017) for the joint prediction of both labels.,4 Joint Prediction of Part-of-Speech Tags and Morphosyntactic Attributes,[0],[0]
"Given a sentence as a sequence of words, we first embed each word using the word embedder we choose and then fed the embeddings into the LSTM.",4 Joint Prediction of Part-of-Speech Tags and Morphosyntactic Attributes,[0],[0]
"The output of LSTM is then used to predict POS and morphosyntactic tags.
",4 Joint Prediction of Part-of-Speech Tags and Morphosyntactic Attributes,[0],[0]
"We emphasize the difference in the setting that we fix the word embeddings during the training, as to better evaluate the ability and consistency of the embeddings in capturing words’ semantics and syntactics, rather than LSTM’s ability to memorize words and infer the role of words from their context.
",4 Joint Prediction of Part-of-Speech Tags and Morphosyntactic Attributes,[0],[0]
We use the same set of hyperparameters for the LSTM model as Sec. 5.3 in Pinter et al. (2017) and train the model for 20 epochs for each language.,4 Joint Prediction of Part-of-Speech Tags and Morphosyntactic Attributes,[0],[0]
"The BoS and MIMICK word embedders
are trained beforehand with PolyGlot dataset using the same way described earlier.
",4 Joint Prediction of Part-of-Speech Tags and Morphosyntactic Attributes,[0],[0]
Results The POS tagging accuracies and micro F1 scores for morphosyntactic attributes are reported in Table 3 with word vectors generated by different models.,4 Joint Prediction of Part-of-Speech Tags and Morphosyntactic Attributes,[0],[0]
The BoS and MIMICK model here are trained against Polyglot vectors.,4 Joint Prediction of Part-of-Speech Tags and Morphosyntactic Attributes,[0],[0]
"As a comparison, we include the results using random word vectors of the same dimension (64).
",4 Joint Prediction of Part-of-Speech Tags and Morphosyntactic Attributes,[0],[0]
Our BoS model shows steady and significant gain compared to MIMICK embeddings for both tasks in all languages.,4 Joint Prediction of Part-of-Speech Tags and Morphosyntactic Attributes,[0],[0]
"We especially observe the greatest margins for agglutinative languages such as Turkish and Indonesian, and in Germanic languages English, Swedish and Danish, suggesting that our model learns stable representations for morphemes to consistent word type signal.",4 Joint Prediction of Part-of-Speech Tags and Morphosyntactic Attributes,[0],[0]
We proposed a subword-level word embedding model and a word vector generalization method that enables extending pre-trained word embeddings with fixed size vocabularies to estimate word embeddings for out-of-vocabulary words.,5 Conclusion,[0],[0]
"Intrinsic evaluation on word similarity tasks and extrinsic evaluation on POS tagging task demonstrate that our model captures morphological knowledge and generates good estimates of word vectors for
OOV words.",5 Conclusion,[0],[0]
We would like to thank the anonymous reviewers for helpful comments.,Acknowledgements,[0],[0]
This work was supported in part by FA9550-18-1-0166.,Acknowledgements,[0],[0]
Y. L. would also like to acknowledge that support for this research was provided by the Office of the Vice Chancellor for Research and Graduate Education at the University of Wisconsin-Madison with funding from the Wisconsin Alumni Research Foundation.,Acknowledgements,[0],[0]
We approach the problem of generalizing pretrained word embeddings beyond fixed-size vocabularies without using additional contextual information.,abstractText,[0],[0]
We propose a subwordlevel word vector generation model that views words as bags of character n-grams.,abstractText,[0],[0]
"The model is simple, fast to train and provides good vectors for rare or unseen words.",abstractText,[0],[0]
"Experiments show that our model achieves stateof-the-art performances in English word similarity task and in joint prediction of part-ofspeech tag and morphosyntactic attributes in 23 languages, suggesting our model’s ability in capturing the relationship between words’ textual representations and their embeddings.",abstractText,[0],[0]
Generalizing Word Embeddings using Bag of Subwords,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 102–111 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1010",text,[0],[0]
"Previous works on zero pronoun (ZP) resolution mainly focused on the supervised learning approaches (Han, 2006; Zhao and Ng, 2007; Iida et al., 2007; Kong and Zhou, 2010; Iida and Poesio, 2011; Chen and Ng, 2013).",1 Introduction,[0],[0]
"However, a major obstacle for training the supervised learning models for ZP resolution is the lack of annotated data.",1 Introduction,[0],[0]
"An important step is to organize the shared task on anaphora and coreference resolution, such as the ACE evaluations, SemEval-2010 shared task on Coreference Resolution in Multiple Languages (Marta Recasens, 2010) and CoNLL2012 shared task on Modeling Multilingual Unre-
stricted Coreference in OntoNotes (Sameer Pradhan, 2012).",1 Introduction,[0],[0]
"Following these shared tasks, the annotated evaluation data can be released for the following researches.",1 Introduction,[0],[0]
"Despite the success and contributions of these shared tasks, it still faces the challenge of spending manpower on labeling the extended data for better training performance and domain adaptation.
",1 Introduction,[0],[0]
"To address the problem above, in this paper, we propose a simple but novel approach to automatically generate large-scale pseudo training data for zero pronoun resolution.",1 Introduction,[0],[0]
"Inspired by data generation on cloze-style reading comprehension, we can treat the zero pronoun resolution task as a special case of reading comprehension problem.",1 Introduction,[0],[0]
So we can adopt similar data generation methods of reading comprehension to the zero pronoun resolution task.,1 Introduction,[0],[0]
"For the noun or pronoun in the document, which has the frequency equal to or greater than 2, we randomly choose one position where the noun or pronoun is located on, and replace it with a specific symbol 〈blank〉.",1 Introduction,[0],[0]
"Let query Q and answer A denote the sentence that contains a 〈blank〉, and the noun or pronoun which is replaced by the 〈blank〉, respectively.",1 Introduction,[0],[0]
"Thus, a pseudo training sample can be represented as a triple:
〈D,Q,A〉 (1)
For the zero pronoun resolution task, a 〈blank〉 represents a zero pronoun (ZP) in query Q, and A indicates the corresponding antecedent of the ZP.",1 Introduction,[0],[0]
"In this way, tremendous pseudo training samples can be generated from the various documents, such as news corpus.
Towards the shortcomings of the previous approaches that are based on feature engineering, we propose a neural network architecture, which is an attention-based neural network model, for zero pronoun resolution.",1 Introduction,[0],[0]
"Also we propose a two-step
102
training method, which benefit from both largescale pseudo training data and task-specific data, showing promising performance.
",1 Introduction,[0],[0]
"To sum up, the contributions of this paper are listed as follows.
",1 Introduction,[0],[0]
"• To our knowledge, this is the first time that utilizing reading comprehension neural network model into zero pronoun resolution task.
",1 Introduction,[0],[0]
"• We propose a two-step training approach, namely pre-training-then-adaptation, which benefits from both the large-scale automatically generated pseudo training data and taskspecific data.
",1 Introduction,[0],[0]
"• Towards the shortcomings of the feature engineering approaches, we first propose an attention-based neural network model for zero pronoun resolution.",1 Introduction,[0],[0]
"In this section, we will describe our approach in detail.",2 The Proposed Approach,[0],[0]
"First, we will describe our method of generating large-scale pseudo training data for zero pronoun resolution.",2 The Proposed Approach,[0],[0]
Then we will introduce twostep training approach to alleviate the gaps between pseudo and real training data.,2 The Proposed Approach,[0],[0]
"Finally, the attention-based neural network model as well as associated unknown words processing techniques will be described.",2 The Proposed Approach,[0],[0]
"In order to get large quantities of training data for neural network model, we propose an approach, which is inspired by (Hermann et al., 2015), to automatically generate large-scale pseudo training data for zero pronoun resolution.",2.1 Generating Pseudo Training Data,[0],[0]
"However, our approach is much more simple and general than that of (Hermann et al., 2015).",2.1 Generating Pseudo Training Data,[0],[0]
"We will introduce the details of generating the pseudo training data for zero pronoun resolution as follows.
",2.1 Generating Pseudo Training Data,[0],[0]
"First, we collect a large number of documents that are relevant (or homogenous in some sense) to the released OntoNote 5.0 data for zero pronoun resolution task in terms of its domain.",2.1 Generating Pseudo Training Data,[0],[0]
"In our experiments, we used large-scale news data for training.
",2.1 Generating Pseudo Training Data,[0],[0]
"Given a certain document D, which is composed by a set of sentences D = {s1, s2, ..., sn},
we randomly choose an answer wordA in the document.",2.1 Generating Pseudo Training Data,[0],[0]
"Note that, we restrictA to be either a noun or pronoun, where the part-of-speech is identified using LTP Toolkit (Che et al., 2010), as well as the answer word should appear at least twice in the document.",2.1 Generating Pseudo Training Data,[0],[0]
"Second, after the answer word A is chosen, the sentence that contains A is defined as a queryQ, in which the answer wordA is replaced by a specific symbol 〈blank〉.",2.1 Generating Pseudo Training Data,[0],[0]
"In this way, given the queryQ and documentD, the target of the prediction is to recover the answer A. That is quite similar to the zero pronoun resolution task.",2.1 Generating Pseudo Training Data,[0],[0]
"Therefore, the automatically generated training samples is called pseudo training data.",2.1 Generating Pseudo Training Data,[0],[0]
"Figure 1 shows an example of a pseudo training sample.
",2.1 Generating Pseudo Training Data,[0],[0]
"In this way, we can generate tremendous triples of 〈D,Q,A〉 for training neural network, without making any assumptions on the nature of the original corpus.",2.1 Generating Pseudo Training Data,[0],[0]
"It should be noted that, though we have generated large-scale pseudo training data for neural network training, there is still a gap between pseudo training data and the real zero pronoun resolution task in terms of the query style.",2.2 Two-step Training,[0],[0]
"So we should do some adaptations to our model to deal with the zero pronoun resolution problems ideally.
",2.2 Two-step Training,[0],[0]
"In this paper, we used an effective approach to deal with the mismatch between pseudo training data and zero pronoun resolution task-specific data.",2.2 Two-step Training,[0],[0]
"Generally speaking, in the first stage, we use a large amount of the pseudo training data to train a fundamental model, and choose the best model according to the validation accuracy.",2.2 Two-step Training,[0],[0]
"Then we continue to train from the previous best model using the zero pronoun resolution task-specific training data, which is exactly the same domain and query type as the standard zero pronoun resolution task data.
",2.2 Two-step Training,[0],[0]
"The using of the combination of proposed pseudo training data and task-specific data, i.e. zero pronoun resolution task data, is far more effective than using either of them alone.",2.2 Two-step Training,[0],[0]
"Though there is a gap between these two data, they share many similar characteristics to each other as illustrated in the previous part, so it is promising to utilize these two types of data together, which will compensate to each other.
",2.2 Two-step Training,[0],[0]
"The two-step training procedure can be concluded as,
• Pre-training stage: by using large-scale training data to train the neural network model, we can learn richer word embeddings, as well as relatively reasonable weights in neural networks than just training with a small amount of zero pronoun resolution task training data;
• Adaptation stage: after getting the best model that is produced in the previous step, we continue to train the model with task-specific data, which can force the previous model to adapt to the new data, without losing much knowledge that has learned in the previous stage (such as word embeddings).
",2.2 Two-step Training,[0],[0]
As we will see in the experiment section that the proposed two-step training approach is effective and brings significant improvements.,2.2 Two-step Training,[0],[0]
"Our model is primarily an attention-based neural network model, which is similar to Attentive Reader proposed by (Hermann et al., 2015).",2.3 Attention-based Neural Network Model,[0],[0]
"Formally, when given a set of training triple 〈D,Q,A〉, we will construct our network in the following way.
",2.3 Attention-based Neural Network Model,[0],[0]
"Firstly, we project one-hot representation of document D and query Q into a continuous space with the shared embedding matrix We.",2.3 Attention-based Neural Network Model,[0],[0]
Then we input these embeddings into different bidirectional RNN to get their contextual representations respectively.,2.3 Attention-based Neural Network Model,[0],[0]
"In our model, we used the bidirectional Gated Recurrent Unit (GRU) as RNN implementation (Cho et al., 2014).
e(x) =We · x, where x ∈ D,Q (2)
−→ hs = −−−→ GRU(e(x)); ←− hs = ←−−− GRU(e(x))",2.3 Attention-based Neural Network Model,[0],[0]
"(3)
hs = [ −→ hs; ←− hs] (4)
",2.3 Attention-based Neural Network Model,[0],[0]
"For the query representation, instead of concatenating the final forward and backward states as its representations, we directly get an averaged representations on all bi-directional RNN slices, which can be illustrated as
hquery = 1
n
n∑
t=1
hquery(t) (5)
",2.3 Attention-based Neural Network Model,[0],[0]
"For the document, we place a soft attention over all words in document (Bahdanau et al., 2014), which indicate the degree to which part of document is attended when filling the blank in the query sentence.",2.3 Attention-based Neural Network Model,[0],[0]
"Then we calculate a weighted sum of all document tokens to get the attended representation of document.
m(t) =",2.3 Attention-based Neural Network Model,[0],[0]
"tanh(W · hdoc(t) + U · hquery) (6)
α(t) = exp(Ws ·m(t))
",2.3 Attention-based Neural Network Model,[0],[0]
n∑ j=1 exp(Ws ·m(j)),2.3 Attention-based Neural Network Model,[0],[0]
"(7)
hdoc att = hdoc · α (8) where variable α(t) is the normalized attention weight at tth word in document, hdoc is a matrix that concatenate all hdoc(t) in sequence.
",2.3 Attention-based Neural Network Model,[0],[0]
"hdoc = concat[hdoc(1), hdoc(2), ..., hdoc(t)] (9)
Then we use attended document representation and query representation to estimate the final answer, which can be illustrated as follows, where V
is the vocabulary,
r = concat[hdoc att, hquery] (10)
P (A|D,Q) ∝",2.3 Attention-based Neural Network Model,[0],[0]
"softmax(Wr · r) , s.t.",2.3 Attention-based Neural Network Model,[0],[0]
"A ∈ V (11)
",2.3 Attention-based Neural Network Model,[0],[0]
"Figure 2 shows the proposed neural network architecture.
",2.3 Attention-based Neural Network Model,[0],[0]
"Note that, for zero pronoun resolution task, antecedents of zero pronouns are always noun phrases (NPs), while our model generates only one word (a noun or a pronoun) as the result.",2.3 Attention-based Neural Network Model,[0],[0]
"To better adapt our model to zero pronoun resolution task, we further process the output result in the following procedure.",2.3 Attention-based Neural Network Model,[0],[0]
"First, for a given zero pronoun, we extract a set of NPs as its candidates utilizing the same strategy as (Chen and Ng, 2015).",2.3 Attention-based Neural Network Model,[0],[0]
"Then, we use our model to generate an answer (one word) for the zero pronoun.",2.3 Attention-based Neural Network Model,[0],[0]
"After that, we go through all the candidates from the nearest to the far-most.",2.3 Attention-based Neural Network Model,[0],[0]
"For an NP candidate, if the produced answer is its head word, we then regard this NP as the antecedent of the given zero pronoun.",2.3 Attention-based Neural Network Model,[0],[0]
"By doing so, for a given zero pronoun, we generate an NP as the prediction of its antecedent.",2.3 Attention-based Neural Network Model,[0],[0]
"Because of the restriction on both memory occupation and training time, it is usually suggested to use a shortlist of vocabulary in neural network training.",2.4 Unknown Words Processing,[0],[0]
"However, we often replace the out-ofvocabularies to a unique special token, such as 〈unk〉.",2.4 Unknown Words Processing,[0],[0]
"But this may place an obstacle in real
world test.",2.4 Unknown Words Processing,[0],[0]
"When the model predicts the answer as 〈unk〉, we do not know what is the exact word it represents in the document, as there may have many 〈unk〉s in the document.
",2.4 Unknown Words Processing,[0],[0]
"In this paper, we propose to use a simple but effective way to handle unknown words issue.",2.4 Unknown Words Processing,[0],[0]
"The idea is straightforward, which can be illustrated as follows.
",2.4 Unknown Words Processing,[0],[0]
"• Identify all unknown words inside of each 〈D,Q,A〉;
• Instead of replacing all these unknown words into one unique token 〈unk〉, we make a hash table to project these unique unknown words to numbered tokens, such as 〈unk1〉, 〈unk2〉, ..., 〈unkN〉 in terms of its occurrence order in the document.",2.4 Unknown Words Processing,[0],[0]
"Note that, the same words are projected to the same unknown word tokens, and all these projections are only valid inside of current sample.",2.4 Unknown Words Processing,[0],[0]
"For example, 〈unk1〉 indicate the first unknown word, say “apple”, in the current sample, but in another sample the 〈unk1〉 may indicate the unknown word “orange”.",2.4 Unknown Words Processing,[0],[0]
"That is, the unknown word labels are indicating position features rather than the exact word;
• Insert these unknown marks in the vocabulary.",2.4 Unknown Words Processing,[0],[0]
"These marks may only take up dozens of slots, which is negligible to the size of shortlists (usually 30K ∼ 100K).
",2.4 Unknown Words Processing,[0],[0]
We take one sentence “The weather of today is not as pleasant as the weather of yesterday.”,2.4 Unknown Words Processing,[0],[0]
"as an example to show our unknown word processing method, which is shown in Figure 3.
",2.4 Unknown Words Processing,[0],[0]
"If we do not discriminate the unknown words and assign different unknown words with the same token 〈unk〉, it would be impossible for us to know what is the exact word that 〈unk〉 represents for in the real test.",2.4 Unknown Words Processing,[0],[0]
"However, when using our proposed unknown word processing method, if the model predicts a 〈unkX〉 as the answer,
we can simply scan through the original document and identify its position according to its unknown word number X and replace the 〈unkX〉 with the real word.",2.4 Unknown Words Processing,[0],[0]
"For example, in Figure 3, if we adopt original unknown words processing method, we could not know whether the 〈unk〉 is the word “weather” or “pleasant”.",2.4 Unknown Words Processing,[0],[0]
"However, when using our approach, if the model predicts an answer as 〈unk1〉, from the original text, we can know that 〈unk1〉 represents the word “weather”.",2.4 Unknown Words Processing,[0],[0]
"In our experiments, we choose a selection of public news data to generate large-scale pseudo training data for pre-training our neural network model (pre-training step)1.",3.1 Data,[0],[0]
"In the adaptation step, we used the official dataset OntoNotes Release 5.02 which is provided by CoNLL-2012 shared task, to carry out our experiments.",3.1 Data,[0],[0]
"The CoNLL2012 shared task dataset consists of three parts: a training set, a development set and a test set.",3.1 Data,[0],[0]
"The datasets are made up of 6 different domains, namely Broadcast News (BN), Newswires (NW), Broadcast Conversations (BC), Telephone Conversations (TC), Web Blogs (WB), and Magazines (MZ).",3.1 Data,[0],[0]
"We closely follow the experimental settings as (Kong and Zhou, 2010; Chen and Ng, 2014, 2015, 2016), where we treat the training set for training and the development set for testing, because only the training and development set are annotated with ZPs.",3.1 Data,[0],[0]
The statistics of training and testing data is shown in Table 1 and 2 respectively.,3.1 Data,[0],[0]
"Training details of our neural network models are listed as follows.
",3.2 Neural Network Setups,[0],[0]
"1The news data is available at http://www.sogou. com/labs/dl/cs.html
2http://catalog.ldc.upenn.edu/ LDC2013T19
Docs Sentences Words AZPs
Test 172 6,083 110K 1,713
All models are trained on Tesla K40 GPU.",3.2 Neural Network Setups,[0],[0]
"Our model is implemented with Theano (Theano Development Team, 2016) and Keras (Chollet, 2015).",3.2 Neural Network Setups,[0],[0]
"Same to the previous researches that are related to zero pronoun resolution, we evaluate our system performance in terms of F-score (F).",3.3 Experimental results,[0],[0]
"We focus on AZP resolution process, where we assume that gold AZPs and gold parse trees are given3.",3.3 Experimental results,[0],[0]
"The same experimental setting is utilized in (Chen and Ng, 2014, 2015, 2016).",3.3 Experimental results,[0],[0]
"The overall results are shown in Table 3, where the performances of each domain are listed in detail and overall performance is also shown in the last column.
",3.3 Experimental results,[0],[0]
• Overall Performance We employ four Chinese ZP resolution baseline systems on OntoNotes 5.0 dataset.,3.3 Experimental results,[0],[0]
"As we can
3All gold information are provided by the CoNLL-2012 shared task dataset
see that our model significantly outperforms the previous state-of-the-art system (Chen and Ng, 2016) by 3.1% in overall F-score, and substantially outperform the other systems by a large margin.",3.3 Experimental results,[0],[0]
"When observing the performances of different domains, our approach also gives relatively consistent improvements among various domains, except for BN and TC with a slight drop.",3.3 Experimental results,[0],[0]
"All these results approve that our proposed approach is effective and achieves significant improvements in AZP resolution.
",3.3 Experimental results,[0],[0]
"In our quantitative analysis, we investigated the reasons of the declines in the BN and TC domain.",3.3 Experimental results,[0],[0]
A primary observation is that the word distributions in these domains are fairly different from others.,3.3 Experimental results,[0],[0]
"The average document length of BN and TC are quite longer than other domains, which suggest that there is a bigger chance to have unknown words than other domains, and add difficulties to the model training.",3.3 Experimental results,[0],[0]
"Also, we have found that in the BN and TC domains, the texts are often in oral form, which means that there are many irregular expressions in the context.",3.3 Experimental results,[0],[0]
"Such expressions add noise to the model, and it is difficult for the model to extract useful information in these contexts.",3.3 Experimental results,[0],[0]
"These phenomena indicate that further improvements can be obtained by filtering stop words in contexts, or increasing the size of task-specific data, while we leave this in the future work.
",3.3 Experimental results,[0],[0]
"• Effect of UNK processing As we have mentioned in the previous section, traditional unknown word replacing methods are vulnerable to the real word test.",3.3 Experimental results,[0],[0]
"To alleviate this issue, we proposed the UNK processing mechanism to recover the UNK tokens to the real words.",3.3 Experimental results,[0],[0]
"In Table 4, we compared the performance that with and without the proposed UNK processing,
to show whether the proposed UNK processing method is effective.",3.3 Experimental results,[0],[0]
"As we can see that, by applying our UNK processing mechanism, the model do learned the positional features in these lowfrequency words, and brings over 3% improvements in F-score, which indicated the effectiveness of our UNK processing approach.
",3.3 Experimental results,[0],[0]
• Effect of Domain Adaptation We also tested out whether our domain adaptation method is effective.,3.3 Experimental results,[0],[0]
"In this experiments, we used three different types of training data: only pseudo training data, only task-specific data, and our adaptation method, i.e. using pseudo training data in the pre-training step and task-specific data for domain adaptation step.",3.3 Experimental results,[0],[0]
The results are given in Table 5.,3.3 Experimental results,[0],[0]
"As we can see that, using either pseudo training data or task-specific data alone can not bring inspiring result.",3.3 Experimental results,[0],[0]
"By adopting our domain adaptation method, the model could give significant improvements over the other models, which demonstrate the effectiveness of our proposed two-step training approach.",3.3 Experimental results,[0],[0]
"An intuition behind this phenomenon is that though pseudo training data is fairly big enough to train a reliable model parameters, there is still a gap to the real zero pronoun resolution tasks.",3.3 Experimental results,[0],[0]
"On the contrary, though task-specific training data is exactly the same type as the real test, the quantity is not enough to train a reasonable model (such as word embedding).",3.3 Experimental results,[0],[0]
"So it is better to make use of both to
take the full advantage.",3.3 Experimental results,[0],[0]
"However, as the original task-specific data is fairly small compared to pseudo training data, we also wondered if the large-scale pseudo training data is only providing rich word embedding information.",3.3 Experimental results,[0],[0]
"So we use the large-scale pseudo training data for embedding training using GloVe toolkit (Pennington et al., 2014), and initialize the word embeddings in the “only task-specific data” system.",3.3 Experimental results,[0],[0]
"From the result we can see that the pseudo training data provide more information than word embeddings, because though we used GloVe embeddings in “only task-specific data”, it still can not outperform the system that uses domain adaptation which supports our claim.",3.3 Experimental results,[0],[0]
"To better evaluate our proposed approach, we performed a qualitative analysis of errors, where two major errors are revealed by our analysis, as discussed below.",4 Error Analysis,[0],[0]
"Our approach does not do well when there are lots of 〈unk〉s in the context of ZPs, especially when the 〈unk〉s appears near the ZP.",4.1 Effect of Unknown Words,[0],[0]
"An example is given below, where words with # are regarded as 〈unk〉s in our model.
",4.1 Effect of Unknown Words,[0],[0]
"φ 登上# 太平山# 顶 , 将香港岛# 和维多 利亚港# 的美景尽收眼底",4.1 Effect of Unknown Words,[0],[0]
"。 φ Successfully climbed# the peak of [Taiping Mountain]#, to have a panoramic view of the beauty of [Hong Kong Island]# and [Victoria Harbour]#.
",4.1 Effect of Unknown Words,[0],[0]
"In this case, the words “登上/climbed” and “太 平山/Taiping Mountain” that appears immediately after the ZP “φ” are all regarded as 〈unk〉s in our model.",4.1 Effect of Unknown Words,[0],[0]
"As we model the sequence of words by RNN, the 〈unk〉s make the model more difficult to capture the semantic information of the sentence, which in turn influence the overall performance.",4.1 Effect of Unknown Words,[0],[0]
"Especially for the words that are near
the ZP, which play important roles when modeling context information for the ZP.",4.1 Effect of Unknown Words,[0],[0]
"By looking at the word “顶/peak”, it is hard to comprehend the context information, due to the several surrounding 〈unk〉s.",4.1 Effect of Unknown Words,[0],[0]
"Though our proposed unknown words processing method is effective in empirical evaluation, we think that more advanced method for unknown words processing would be of a great help in improving comprehension of the context.",4.1 Effect of Unknown Words,[0],[0]
"Also, our model makes incorrect decisions when the correct antecedents of ZPs are in long distance.",4.2 Long Distance Antecedents,[0],[0]
"As our model chooses answer from words in the context, if there are lots of words between the ZP and its antecedent, more noise information are introduced, and adds more difficulty in choosing the right answer.",4.2 Long Distance Antecedents,[0],[0]
"For example:
我帮不了那个人 ... ...",4.2 Long Distance Antecedents,[0],[0]
那天结束后 φ 回到,4.2 Long Distance Antecedents,[0],[0]
家中,4.2 Long Distance Antecedents,[0],[0]
。,4.2 Long Distance Antecedents,[0],[0]
I can’t help that guy ... ...,4.2 Long Distance Antecedents,[0],[0]
"After that day, φ return home.
",4.2 Long Distance Antecedents,[0],[0]
"In this case, the correct antecedent of ZP “φ” is the NP candidate “我/I”.",4.2 Long Distance Antecedents,[0],[0]
"By seeing the contexts, we observe that there are over 30 words between the ZP and its antecedent.",4.2 Long Distance Antecedents,[0],[0]
"Although our model does not intend to fill the ZP gap only with the words near the ZP, as most of the antecedents appear just a few words before the ZPs, our model prefers the nearer words as correct antecedents.",4.2 Long Distance Antecedents,[0],[0]
"Hence, once there are lots of words between ZP and its nearest antecedent, our model can sometimes make wrong decisions.",4.2 Long Distance Antecedents,[0],[0]
"To correctly handle such cases, our model should learn how to filter the useless words and enhance the learning of longterm dependency.",4.2 Long Distance Antecedents,[0],[0]
"For Chinese zero pronoun (ZP) resolution, early studies employed heuristic rules to Chinese ZP resolution.",5.1 Zero pronoun resolution,[0],[0]
"Converse (2006) proposes a rule-based method to resolve the zero pronouns, by utilizing Hobbs algorithm (Hobbs, 1978) in the CTB documents.",5.1 Zero pronoun resolution,[0],[0]
"Then, supervised approaches to this task have been vastly explored.",5.1 Zero pronoun resolution,[0],[0]
Zhao and Ng (2007) first present a supervised machine learning approach to the identification and resolution of Chinese ZPs.,5.1 Zero pronoun resolution,[0],[0]
Kong and Zhou (2010) develop a tree-kernel based approach for Chinese ZP resolution.,5.1 Zero pronoun resolution,[0],[0]
"More recently, unsupervised approaches
have been proposed.",5.1 Zero pronoun resolution,[0],[0]
"Chen and Ng (2014) develop an unsupervised language-independent approach, utilizing the integer linear programming to using ten overt pronouns.",5.1 Zero pronoun resolution,[0],[0]
"Chen and Ng (2015) propose an end-to-end unsupervised probabilistic model for Chinese ZP resolution, using a salience model to capture discourse information.",5.1 Zero pronoun resolution,[0],[0]
"Also, there have been many works on ZP resolution for other languages.",5.1 Zero pronoun resolution,[0],[0]
These studies can be divided into rule-based and supervised machine learning approaches.,5.1 Zero pronoun resolution,[0],[0]
Ferrández and Peral (2000) proposed a set of hand-crafted rules for Spanish ZP resolution.,5.1 Zero pronoun resolution,[0],[0]
"Recently, supervised approaches have been exploited for ZP resolution in Korean (Han, 2006) and Japanese (Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano and Kurohashi, 2011).",5.1 Zero pronoun resolution,[0],[0]
"Iida and Poesio (2011) developed a cross-lingual approach for Japanese and Italian ZPs where an ILPbased model was employed to zero anaphora detection and resolution.
",5.1 Zero pronoun resolution,[0],[0]
"In sum, most recent researches on ZP resolution are supervised approaches, which means that their performance highly relies on large-scale annotated data.",5.1 Zero pronoun resolution,[0],[0]
"Even for the unsupervised approach (Chen and Ng, 2014), they also utilize a supervised pronoun resolver to resolve ZPs.",5.1 Zero pronoun resolution,[0],[0]
"Therefore, the advantage of our proposed approach is obvious.",5.1 Zero pronoun resolution,[0],[0]
"We are able to generate large-scale pseudo training data for ZP resolution, and also we can benefit from the task-specific data for fine-tuning via the proposed two-step training approach.",5.1 Zero pronoun resolution,[0],[0]
"Our neural network model is mainly motivated by the recent researches on cloze-style reading comprehension tasks, which aims to predict one-word answer given the document and query.",5.2 Cloze-style Reading Comprehension,[0],[0]
"These models can be seen as a general model of mining the relations between the document and query, so it is promising to combine these models to the specific domain.
",5.2 Cloze-style Reading Comprehension,[0],[0]
A representative work of cloze-style reading comprehension is done by Hermann et al. (2015).,5.2 Cloze-style Reading Comprehension,[0],[0]
"They proposed a methodology for obtaining large quantities of 〈D,Q,A〉 triples.",5.2 Cloze-style Reading Comprehension,[0],[0]
"By using this method, a large number of training data can be obtained without much human intervention, and make it possible to train a reliable neural network.",5.2 Cloze-style Reading Comprehension,[0],[0]
They used attention-based neural networks for this task.,5.2 Cloze-style Reading Comprehension,[0],[0]
"Evaluation on CNN/DailyMail datasets showed that their approach is much effective than
traditional baseline systems.",5.2 Cloze-style Reading Comprehension,[0],[0]
"While our work is similar to Hermann et al. (2015), there are several differences which can be illustrated as follows.",5.2 Cloze-style Reading Comprehension,[0],[0]
"Firstly, though we both utilize the large-scale corpus, they require that the document should accompany with a brief summary of it, while this is not always available in most of the document, and it may place an obstacle in generating limitless training data.",5.2 Cloze-style Reading Comprehension,[0],[0]
"In our work, we do not assume any prerequisite of the training data, and directly extract queries from the document, which makes it easy to generate large-scale training data.",5.2 Cloze-style Reading Comprehension,[0],[0]
"Secondly, their work mainly focuses on reading comprehension in the general domain.",5.2 Cloze-style Reading Comprehension,[0],[0]
"We are able to exploit large-scale training data for solving problems in the specific domain, and we proposed two-step training method which can be easily adapted to other domains as well.",5.2 Cloze-style Reading Comprehension,[0],[0]
"In this study, we propose an effective way to generate and exploit large-scale pseudo training data for zero pronoun resolution task.",6 Conclusion,[0],[0]
The main idea behind our approach is to automatically generate large-scale pseudo training data and then utilize an attention-based neural network model to resolve zero pronouns.,6 Conclusion,[0],[0]
"For training purpose, two-step training approach is employed, i.e. a pre-training and adaptation step, and this can be also easily applied to other tasks as well.",6 Conclusion,[0],[0]
"The experimental results on OntoNotes 5.0 corpus are encouraging, showing that the proposed model and accompanying approaches significantly outperforms the stateof-the-art systems.
",6 Conclusion,[0],[0]
"The future work will be carried out on two main aspects: First, as experimental results show that the unknown words processing is a critical part in comprehending context, we will explore more effective way to handle the UNK issue.",6 Conclusion,[0],[0]
"Second, we will develop other neural network architecture to make it more appropriate for zero pronoun resolution task.",6 Conclusion,[0],[0]
We would like to thank the anonymous reviewers for their thorough reviewing and proposing thoughtful comments to improve our paper.,Acknowledgements,[0],[0]
"This work was supported by the National 863 Leading Technology Research Project via grant 2015AA015407, Key Projects of National Natural Science Foundation of China via grant 61632011,
and National Natural Science Youth Foundation of China via grant 61502120.",Acknowledgements,[0],[0]
"Most existing approaches for zero pronoun resolution are heavily relying on annotated data, which is often released by shared task organizers.",abstractText,[0],[0]
"Therefore, the lack of annotated data becomes a major obstacle in the progress of zero pronoun resolution task.",abstractText,[0],[0]
"Also, it is expensive to spend manpower on labeling the data for better performance.",abstractText,[0],[0]
"To alleviate the problem above, in this paper, we propose a simple but novel approach to automatically generate large-scale pseudo training data for zero pronoun resolution.",abstractText,[0],[0]
"Furthermore, we successfully transfer the cloze-style reading comprehension neural network model into zero pronoun resolution task and propose a two-step training mechanism to overcome the gap between the pseudo training data and the real one.",abstractText,[0],[0]
Experimental results show that the proposed approach significantly outperforms the state-of-the-art systems with an absolute improvements of 3.1% F-score on OntoNotes 5.0 data.,abstractText,[0],[0]
Generating and Exploiting Large-scale Pseudo Training Data for Zero Pronoun Resolution,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 678–687 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1063",text,[0],[0]
"Interactive natural language generation (NLG) systems face the task of detecting when they have been misunderstood, and reacting appropriately to fix the problem.",1 Introduction,[0],[0]
"For instance, even when the system generated a semantically correct referring expression (RE), the user may still misunderstand it, i.e. resolve it to a different object from the one the system intended.",1 Introduction,[0],[0]
"In an interactive setting, such as a dialogue system or a pedestrian navigation system, the system can try to detect such misunderstandings – e.g. by predicting what the hearer understood from their behavior (Engonopoulos et al., 2013) – and to produce further utterances which resolve the misunderstanding and get the hearer to identify the intended object after all.
",1 Introduction,[0],[0]
"When humans correct their own REs, they routinely employ contrastive focus (Rooth, 1992; Krifka, 2008) to clarify the relationship to the original RE.",1 Introduction,[0],[0]
"Say that we originally described an object b as “the blue button”, but the hearer approaches a button b′ which is green, thus providing evidence that they misunderstood the RE to mean b′.",1 Introduction,[0],[0]
"In this
case, we would like to say “no, the BLUE button”, with the contrastive focus realized by an appropriate pitch accent on “BLUE”.",1 Introduction,[0],[0]
"This utterance alerts the hearer to the fact that they misunderstood the original RE; it reiterates the information from the original RE; and it marks the attribute “blue” as a salient difference between b′ and the object the original RE was intended to describe.
",1 Introduction,[0],[0]
"In this paper, we describe an algorithm for generating REs with contrastive focus.",1 Introduction,[0],[0]
"We start from the modeling assumption that misunderstandings arise because the RE rs the system uttered was corrupted by a noisy channel into an RE ru which the user “heard” and then resolved correctly; in the example above, we assume the user literally heard “the green button”.",1 Introduction,[0],[0]
We compute this (hypothetical) RE ru as the RE which refers to b′ and has the lowest edit distance from rs.,1 Introduction,[0],[0]
"Based on this, we mark the contrastive words in rs, i.e. we transform “the blue button” into “the BLUE button”.",1 Introduction,[0],[0]
"We evaluate our system empirically on REs from the GIVE Challenge (Koller et al., 2010) and the TUNA Challenge (van der Sluis et al., 2007), and show that the contrastive REs generated by our system are preferred over a number of baselines.
",1 Introduction,[0],[0]
The paper is structured as follows.,1 Introduction,[0],[0]
We first review related work in Section 2 and define the problem of generating contrastive REs in Section 3.,1 Introduction,[0],[0]
Section 4 sketches the general architecture for RE generation on which our system is based.,1 Introduction,[0],[0]
"In Section 5, we present the corruption model and show how to use it to reconstruct ru. Section 6 describes how we use this information to generate contrastive markup in rs, and in Section 7 we evaluate our approach.",1 Introduction,[0],[0]
"The notion of focus has been extensively studied in the literature on theoretical semantics and prag-
678
matics, see e.g. Krifka (2008) and Rooth (1997) for overview papers.",2 Related Work,[0],[0]
Krifka follows Rooth (1992) in taking focus as “indicat(ing) the presence of alternatives that are relevant for the interpretation of linguistic expressions”; focus then establishes a contrast between an object and these alternatives.,2 Related Work,[0],[0]
"Bornkessel and Schlesewsky (2006) find that corrective focus can even override syntactic requirements, on the basis of “its extraordinarily high communicative saliency”.",2 Related Work,[0],[0]
"This literature is purely theoretical; we offer an algorithm for automatically generating contrastive focus.
",2 Related Work,[0],[0]
"In speech, focus is typically marked through intonation and pitch accents (Levelt, 1993; Pierrehumbert and Hirschberg, 1990; Steube, 2001), while concepts that can be taken for granted are deaccented and/or deleted.",2 Related Work,[0],[0]
Developing systems which realize precise pitch contours for focus in text-to-speech settings is an ongoing research effort.,2 Related Work,[0],[0]
"We therefore realize focus in written language in this paper, by capitalizing the focused word.",2 Related Work,[0],[0]
"We also experiment with deletion of background words.
",2 Related Work,[0],[0]
There is substantial previous work on interactive systems that detect and respond to misunderstandings.,2 Related Work,[0],[0]
"Misu et al. (2014) present an error analysis of an in-car dialogue system which shows that more than half the errors can only be resolved through further clarification dialogues, as opposed to better sensors and/or databases; that is, by improved handling of misunderstandings.",2 Related Work,[0],[0]
Engonopoulos et al. (2013) detect misunderstandings of REs in interactive NLG through the use of a statistical model.,2 Related Work,[0],[0]
Their model also predicts the object to which a misunderstood RE was incorrectly resolved.,2 Related Work,[0],[0]
"Moving from misunderstanding detection to error correction, Zarrieß and Schlangen (2016) present an interactive NLG algorithm which is capable of referring in installments, in that it can generate multiple REs that are designed to correct misunderstandings of earlier REs to the same object.",2 Related Work,[0],[0]
The interactive NLG system developed by Akkersdijk et al. (2011) generates both reflective and anticipative feedback based on what a user does and sees.,2 Related Work,[0],[0]
"Their error detection and correction strategy distinguishes a fixed set of possible situations where feedback is necessary, and defines custom, hard-coded RE generation sub-strategies for each one.",2 Related Work,[0],[0]
"None of these systems generate REs marked for focus.
",2 Related Work,[0],[0]
"We are aware of two items of previous work that
address the generation of contrastive REs directly.",2 Related Work,[0],[0]
Milosavljevic and Dale (1996) outline strategies for generating clarificatory comparisons in encyclopedic descriptions.,2 Related Work,[0],[0]
"Their surface realizer can generate contrastive REs, but the attributes that receive contrastive focus have to be specified by hand.",2 Related Work,[0],[0]
"Krahmer and Theune (2002) extend the Incremental Algorithm (Dale and Reiter, 1995) so it can mark attributes as contrastive.",2 Related Work,[0],[0]
"This is a fully automatic algorithm for contrastive REs, but it inherits all the limitations of the Incremental Algorithm, such as its reliance on a fixed attribute order.",2 Related Work,[0],[0]
"Neither of these two approaches evaluates the quality of the contrastive REs it generates.
",2 Related Work,[0],[0]
"Finally, some work has addressed the issue of generating texts that realize the discourse relation contrast.",2 Related Work,[0],[0]
"For instance, Howcroft et al. (2013) show how to choose contrastive discourse connectives (but, while, . . . )",2 Related Work,[0],[0]
"when generating restaurant descriptions, thus increasing human ratings for naturalness.",2 Related Work,[0],[0]
"Unlike their work, the research presented in this paper is not about discourse relations, but about assigning focus in contrastive REs.",2 Related Work,[0],[0]
We start by introducing the problem of generating corrective REs in an interactive NLG setting.,3 Interactive NLG,[0],[0]
"We use examples from the GIVE Challenge (Koller et al., 2010) throughout the paper; however, the algorithm itself is domain-independent.
",3 Interactive NLG,[0],[0]
"GIVE is a shared task in which an NLG system (the instruction giver, IG) must guide a human user (the instruction follower, IF) through a virtual 3D environment.",3 Interactive NLG,[0],[0]
The IF needs to open a safe and steal a trophy by clicking on a number of buttons in the right order without triggering alarms.,3 Interactive NLG,[0],[0]
"The job of the NLG system is to generate natural-language instructions which guide the IF to complete this task successfully.
",3 Interactive NLG,[0],[0]
The generation of REs has a central place in the GIVE Challenge because the system frequently needs to identify buttons in the virtual environment to the IF.,3 Interactive NLG,[0],[0]
"Figure 1 shows a screenshot of a GIVE game in progress; here b1 and b4 are blue buttons, b2 and b3 are yellow buttons, and w1 is a window.",3 Interactive NLG,[0],[0]
"If the next button the IF needs to press is b4 – the intended object, os – then one good RE for b4 would be “the blue button below the window”, and the system should utter:
(1) Press the blue button below the window.
",3 Interactive NLG,[0],[0]
"After uttering this sentence, the system can
track the IF’s behavior to see whether the IF has understood the RE correctly.",3 Interactive NLG,[0],[0]
"If the wrong button is pressed, or if a model of IF’s behavior suggests that they are about to press the wrong button (Engonopoulos et al., 2013), the original RE has been misunderstood.",3 Interactive NLG,[0],[0]
"However, the system still gets a second chance, since it can utter a corrective RE, with the goal of identifying b4 to the IF after all.",3 Interactive NLG,[0],[0]
"Examples include simply repeating the original RE, or generating a completely new RE from scratch.",3 Interactive NLG,[0],[0]
The system can also explicitly take into account which part of the original RE the IF misunderstood.,3 Interactive NLG,[0],[0]
"If it has reason to believe that the IF resolved the RE to b3, it could say:
(2) No, the BLUE button below the window.
",3 Interactive NLG,[0],[0]
"This use of contrastive focus distinguishes the attributes the IF misunderstood (blue) from those that they understood correctly (below the window), and thus makes it easier for the IF to resolve the misunderstanding.",3 Interactive NLG,[0],[0]
"In speech, contrastive focus would be realized with a pitch accent; we approximate this accent in written language by capitalizing the focused word.",3 Interactive NLG,[0],[0]
"We call an RE that uses contrastive focus to highlight the difference between the misunderstood and the intended object, a contrastive RE.",3 Interactive NLG,[0],[0]
The aim of this paper is to present an algorithm for computing contrastive REs.,3 Interactive NLG,[0],[0]
"While we make no assumptions on how the original RE rs was generated, our algorithm for reconstructing the corrupted RE ru requires an RE generation algorithm that can represent all semantically correct REs for a given object compactly in a chart.",4 Generating Referring Expressions,[0],[0]
"Here we sketch the RE generation of Engonopoulos and Koller (2014), which satisfies this requirement.
",4 Generating Referring Expressions,[0],[0]
This algorithm assumes a synchronous grammar which relates strings with the sets of objects they refer to.,4 Generating Referring Expressions,[0],[0]
Strings and their referent sets are constructed in parallel from lexicon entries and grammar rules; each grammar rule specifies how the referent set of the parent is determined from those of the children.,4 Generating Referring Expressions,[0],[0]
"For the scene in Figure 1, we assume lexicon entries which express, among other things, that the word “blue” denotes the set {b1, b4} and the word “below” denotes the relation {(w1, b1), (w1, b2), (b3, w1), (b4, w1)}.",4 Generating Referring Expressions,[0],[0]
"We combine these lexicons entries using rules such as
“N→ button() |button |{b1, b2, b3, b4}” which generates the string “button” and asso-
ciates it with the set of all buttons or
“N→ N1(N,PP) |w1 • w2 |R1 ∩R2” which states that a phrase of type noun can be combined with a prepositional phrase and their denotations will be intersected.",4 Generating Referring Expressions,[0],[0]
"Using these rules we can determine that “the window” denotes {w1}, that “below the window” can refer to {b3, b4} and that “blue button below the window” uniquely refers to {b4}.",4 Generating Referring Expressions,[0],[0]
"The syntax tree in Fig. 2 represents a complete derivation of an RE for {b4}.
",4 Generating Referring Expressions,[0],[0]
"The algorithm of Engonopoulos and Koller computes a chart which represents the set of all possible REs for a given set of input objects, such as {b4}, according to the grammar.",4 Generating Referring Expressions,[0],[0]
This is done by building a chart containing all derivations of the grammar which correspond to the desired set.,4 Generating Referring Expressions,[0],[0]
"They represent this chart as a finite tree automaton (Comon et al., 2007).",4 Generating Referring Expressions,[0],[0]
Here we simply write the chart as a Context-Free Grammar.,4 Generating Referring Expressions,[0],[0]
The strings produced by this Context-Free Grammar are then exactly the REs for the intended object.,4 Generating Referring Expressions,[0],[0]
"For example, the syntax tree in Fig. 2 is generated by the parse chart for the set {b4}.",4 Generating Referring Expressions,[0],[0]
"Its nonterminal symbols consist of three parts: a syntactic category
(given by the synchronous grammar), the referent for which an RE is currently being constructed, and the set of objects to which the entire subtree refers.",4 Generating Referring Expressions,[0],[0]
The grammar may include recursion and therefore allow for an infinite set of possible REs.,4 Generating Referring Expressions,[0],[0]
"If it is weighted, one can use the Viterbi algorithm to compute the best RE from the chart.",4 Generating Referring Expressions,[0],[0]
"Now let us say that the system has generated and uttered an RE rs with the intention of referring to the object os, but it has then found that the IF has misunderstood the RE and resolved it to another object, ou (see Fig. 3).",5.1 Corruption model,[0],[0]
"We assume for the purposes of this paper that such a misunderstanding arises because rs was corrupted by a noisy channel when it was transmitted to the IF, and the IF “heard” a different RE, ru.",5.1 Corruption model,[0],[0]
"We further assume that the IF then resolved ru correctly, i.e. the corruption in the transmission is the only source of misunderstandings.
",5.1 Corruption model,[0],[0]
"In reality, there are of course many other reasons why the IF might misunderstand rs, such as lack of attention, discrepancies in the lexicon or the world model of the IG and IF, and so on.",5.1 Corruption model,[0],[0]
"We make a simplifying assumption in order to make the misunderstanding explicit at the level of the RE strings, while still permitting meaningful corrections for a large class of misunderstandings.
",5.1 Corruption model,[0],[0]
"An NLG system that builds upon this idea in order to generate a corrective RE has access to the values of os, rs and ou; but it needs to infer the most likely corrupted RE ru.",5.1 Corruption model,[0],[0]
"To do this, we model the corruption using the edit operations used for the familiar Levenshtein edit distance (Mohri, 2003) over the alphabet Σ: Sa, substitution of a word with a symbol a ∈ Σ; D, deletion of a word; Ia, insertion of the symbol a ∈ Σ; or K, keeping the word.",5.1 Corruption model,[0],[0]
"The noisy channel passes
over each word in rs and applies either D, K or one of the S operations to it.",5.1 Corruption model,[0],[0]
It may also apply I operations before or after a word.,5.1 Corruption model,[0],[0]
"We call any sequence s of edit operations that could apply to rs an edit sequence for rs.
",5.1 Corruption model,[0],[0]
An example for an edit sequence which corrupts rs = “the blue button below the window” into ru = “the yellow button above the window” is shown in Figure 4.,5.1 Corruption model,[0],[0]
"The same ru could also have been generated by the edit operation sequence K Syellow K Sabove KK, and there is generally a large number of edit sequences that could transform between any two REs.",5.1 Corruption model,[0],[0]
"If an edit sequence s maps x to y, we write apply(s, x) =",5.1 Corruption model,[0],[0]
"y.
We can now define a probability distribution P (s | rs) over edit sequences s that the noisy channel might apply to the string rs, as follows:
P (s | rs) = 1
Z
∏ si∈s exp(−c(si)),
where c(si) is a cost for using the edit operation si.",5.1 Corruption model,[0],[0]
"We set c(K) = 0, and for any a in our alphabet we set c(Sa) = c(Ia) = c(D) = C, for some fixed C > 0.",5.1 Corruption model,[0],[0]
Z is a normalizing constant which is independent of s and ensures that the probabilities sum to 1.,5.1 Corruption model,[0],[0]
"It is finite for sufficiently high values of C, because no sequence for rs can ever contain more K, S and D operations than there are words in rs, and the total weight of sequences generated by adding more and more",5.1 Corruption model,[0],[0]
"I operations will converge.
",5.1 Corruption model,[0],[0]
"Finally, let L be the set of referring expressions that the IF would resolve to ou, i.e. the set of candidates for ru.",5.1 Corruption model,[0],[0]
"Then the most probable edit sequence for rs which generates an ru ∈ L is given by
s∗ = arg max s : apply(s,rs)∈L P (s | rs)
",5.1 Corruption model,[0],[0]
"= arg mins ∑ si∈s c(si),
i.e. s∗ is the edit sequence that maps rs to an RE in L with minimal cost.",5.1 Corruption model,[0],[0]
"We will assume that s∗ is the edit sequence that corrupted rs, i.e. that ru = apply(s∗, rs).",5.1 Corruption model,[0],[0]
It remains to compute s∗; we will then show in Section 6 how it can be used to generate a corrective RE.,5.2 Finding the most likely corruption,[0],[0]
"Attempting to find s∗ by enumeration is impractical, as the set of edit sequences for a given rs and ru may be large and the set of possible ru for a given ou may be infinite.",5.2 Finding the most likely corruption,[0],[0]
"Instead
we will use the algorithm from Section 4 to compute a chart for all the possible REs for ou, represented as a context-free grammar G whose language L = L(G) consists of these REs.",5.2 Finding the most likely corruption,[0],[0]
"We will then intersect it with a finite-state automaton which keeps track of the edit costs, obtaining a second context-free grammar G′. These operations can be performed efficiently, and s∗ can be read off of the minimum-cost syntax tree of G′.
Edit automaton.",5.2 Finding the most likely corruption,[0],[0]
"The possible edit sequences for a given rs can be represented compactly in the form of a weighted finite-state automaton F (rs) (Mohri, 2003).",5.2 Finding the most likely corruption,[0],[0]
"Each run of the automaton on a string w corresponds to a specific edit sequence that transforms rs intow, and the sum of transition weights of the run is the cost of that edit sequence.",5.2 Finding the most likely corruption,[0],[0]
We call F (rs) the edit automaton.,5.2 Finding the most likely corruption,[0],[0]
It has a state qi for every position i in rs; the start state is q0 and the final state is q|rs|.,5.2 Finding the most likely corruption,[0],[0]
"For each i, it has a “keep” transition from qi to qi+1 that reads the word at position i with cost 0.",5.2 Finding the most likely corruption,[0],[0]
"In addition, there are transitions from qi to qi+1 with cost C that read any symbol in Σ (for substitution) and ones that read the empty string (for deletion).",5.2 Finding the most likely corruption,[0],[0]
"Finally, there is a loop with cost C from each qi to itself and for any symbol in Σ, implementing insertion.
",5.2 Finding the most likely corruption,[0],[0]
An example automaton for rs = “the blue button below the window” is shown in Figure 5.,5.2 Finding the most likely corruption,[0],[0]
The transitions are written in the form 〈word in w : associated cost〉.,5.2 Finding the most likely corruption,[0],[0]
"Note that every path through the edit transducer corresponds to a specific edit sequence s, and the sum of the costs along the path corresponds to − logP (s | rs)− logZ.
Combining G and F (rs).",5.2 Finding the most likely corruption,[0],[0]
"Now we can combine G with F (rs) to obtain G′, by intersecting them using the Bar-Hillel construction (Bar-Hillel et al., 1961; Hopcroft and Ullman, 1979).",5.2 Finding the most likely corruption,[0],[0]
"For the purposes of our presentation we assume that G is in Chomsky Normal Form, i.e. all rules have the form A → a, where a is a word, or A → B C, where both symbols on the right hand side are nonterminals.",5.2 Finding the most likely corruption,[0],[0]
"The resulting grammar G′ uses nonterminal symbols of the form Nb,A,〈qi,qk〉, where
b, A are as in Section 4, and qi, qk indicate that the string derived by this nonterminal was generated by editing the substring of rs from position i to k.
Let Nb,A → a be a production rule of G with a word a on the right-hand side; as explained above, b is the object to which the subtree should refer, and A is the set of objects to which the subtree actually might refer.",5.2 Finding the most likely corruption,[0],[0]
"Let t = qi → 〈a:c〉qk be a transition in F (rs), where q, q′ are states of F (rs) and c is the edit cost.",5.2 Finding the most likely corruption,[0],[0]
"From these two, we create a context-free rule Nb,A,〈qi,qk〉 → a with weight c and add it to G′. If k = i + 1, these rules represent K and S operations; if k = i, they represent insertions.
",5.2 Finding the most likely corruption,[0],[0]
"Now let Nb,A → Xb1,A1 Yb2,A2 be a binary rule in G, and let qi, qj , qk be states of F (rs) with i ≤ j",5.2 Finding the most likely corruption,[0],[0]
≤,5.2 Finding the most likely corruption,[0],[0]
k.,5.2 Finding the most likely corruption,[0],[0]
"We then add a rule Nb,A,〈qi,qk〉 → Xb1,A1,〈qi,qj〉 Yb2,A2,〈qj ,qk〉 to G
′. These rules are assigned weight 0, as they only combine words according to the grammar structure of G and do not encode any edit operations.
",5.2 Finding the most likely corruption,[0],[0]
"Finally, we deal with deletion.",5.2 Finding the most likely corruption,[0],[0]
"Let Nb,A be a nonterminal symbol in G and let qh, qi, qj , qk be states of F (rs) with h ≤",5.2 Finding the most likely corruption,[0],[0]
i ≤ j,5.2 Finding the most likely corruption,[0],[0]
≤ k.,5.2 Finding the most likely corruption,[0],[0]
"We then add a rule Nb,A,〈qh,qk〉 → Nb,A,〈qi,qj〉 to G′.",5.2 Finding the most likely corruption,[0],[0]
This rule deletes the substrings from positions h to i and j to k from rs; thus we assign it the cost ((i− h) +,5.2 Finding the most likely corruption,[0],[0]
"(k − j))C, i.e. the cost of the corresponding transitions.
",5.2 Finding the most likely corruption,[0],[0]
"If the start symbol of G is Sb,A, then the start symbol of G′ is Sb,A,〈q0,q|rs|〉.",5.2 Finding the most likely corruption,[0],[0]
"This construction intersects the languages of G and F (rs), but because F (rs) accepts all strings over the alphabet, the languages of G′ and G will be the same (namely, all REs for ou).",5.2 Finding the most likely corruption,[0],[0]
"However, the weights in G′ are inherited from F (rs); thus the weight of each RE in L(G′) is the edit cost from rs.
Example.",5.2 Finding the most likely corruption,[0],[0]
Fig. 6 shows an example tree for the G′ we obtain from the automaton in Fig. 5.,5.2 Finding the most likely corruption,[0],[0]
"We can read the string w = “the yellow button above the window” off of the leaves; by construction, this is an RE for ou.",5.2 Finding the most likely corruption,[0],[0]
"Furthermore, we can reconstruct the edit sequence that maps from rs to w from the rules of G′ that
were used to derive w. We can see that “yellow” was created by an insertion because the two states of F (rs) in the preterminal symbol just above it are the same.",5.2 Finding the most likely corruption,[0],[0]
"If the two states are different, then the word was either substituted (“above”, if the rule had weight C) or kept (“the”, if the rule had weight 0).",5.2 Finding the most likely corruption,[0],[0]
"By contrast, unary rules indicate deletions, in that they make “progress” in rs without adding new words to w.
We can compute the minimal-cost tree ofG′ using the Viterbi algorithm.",5.2 Finding the most likely corruption,[0],[0]
"Thus, to summarize, we can calculate s∗ from the intersection of a contextfree grammar G representing the REs to ou with the automaton F (rs) representing the edit distance to rs.",5.2 Finding the most likely corruption,[0],[0]
"From this, we obtain ru = apply(s∗, rs).",5.2 Finding the most likely corruption,[0],[0]
This is efficient in practice.,5.2 Finding the most likely corruption,[0],[0]
"We are now ready to generate a contrastive RE from rs and s∗. We assign focus to the words in rs which were changed by the corruption – that is, the ones to which s∗ applied Substitute or Delete operations.",6.1 Contrastive focus,[0],[0]
"For instance, the edit sequence in Fig. 6 deleted “blue” and substituted “below” with “above”.",6.1 Contrastive focus,[0],[0]
"Thus, we mark these words with focus, and obtain the contrastive RE “the BLUE button BELOW the window”.",6.1 Contrastive focus,[0],[0]
"We call this strategy Emphasis, and write rsE for the RE obtained
by applying the Emphasis strategy to the RE rs.",6.1 Contrastive focus,[0],[0]
"We also investigate a second strategy, which generates more succinct contrastive REs than the Emphasis strategy.",6.2 Shortening,[0],[0]
"Most research on RE generation (e.g. Dale and Reiter (1995)) has assumed that hearers should prefer succinct REs, which in particular do not violate the Maxim of Quantity (Grice, 1975).",6.2 Shortening,[0],[0]
"When we utter a contrastive RE, the user has previously heard the RE rs, so some of the information in rsE is redundant.",6.2 Shortening,[0],[0]
"Thus we might obtain a more succinct, and possibly better, RE by dropping such redundant information from the RE.
",6.2 Shortening,[0],[0]
"For the grammars we consider here, rsE often combines an NP and a PP, e.g. “[blue button]NP",6.2 Shortening,[0],[0]
[below the window]PP ”.,6.2 Shortening,[0],[0]
"If errors occur only in one of these constituents, then it might be sufficient to generate a contrastive RE using only that constituent.",6.2 Shortening,[0],[0]
"We call this strategy Shortening and define it as follows.
",6.2 Shortening,[0],[0]
"If all the words that are emphasized in rsE are in the NP, the Shortening RE is “the” plus the NP, with emphasis as in rsE .",6.2 Shortening,[0],[0]
So if rs is “the [blue button],6.2 Shortening,[0],[0]
"[above the window]” and s∗ = K SyellowKKKK, corresponding to a rsE of “the [BLUE button]",6.2 Shortening,[0],[0]
"[above the window]”, then the RE would be “the [BLUE button]”.
",6.2 Shortening,[0],[0]
"If all the emphasis in rsE is in the PP, we use
“the one” plus the PP and again capitalize as in rs
E .",6.2 Shortening,[0],[0]
"So if we have s∗ = KKK SbelowKK, where rsE is “the [blue button]",6.2 Shortening,[0],[0]
"[ABOVE the window]”, we obtain “the one [ABOVE the window].”",6.2 Shortening,[0],[0]
"If there is no PP or if rsE emphasizes words in both the NP and the PP, then we just use rsE .",6.2 Shortening,[0],[0]
"To test whether our algorithm for contrastive REs assigns contrastive focus correctly, we evaluated it against several baselines in crowdsourced pairwise comparison overhearer experiments.",7 Evaluation,[0],[0]
"Like Buß et al. (2010), we opted for an overhearer experiment to focus our evaluation on the effects of contrastive feedback, as opposed to the challenges presented by the navigational and timing aspects of a fully interactive system.",7 Evaluation,[0],[0]
We created the stimuli for our experiments from two different domains.,7.1 Domains and stimuli,[0],[0]
"We performed a first experiment with scenes from the GIVE Challenge, while a second experiment replaced these scenes with stimuli from the “People” domain of the TUNA Reference Corpus (van der Sluis et al., 2007).",7.1 Domains and stimuli,[0],[0]
"This corpus consists of photographs of men annotated with nine attributes, such as whether the
We wanted our player to select the person circled in green:
So we told them: the light haired old man in a suit looking straight.
",7.1 Domains and stimuli,[0],[0]
But they selected the person circled in red instead.,7.1 Domains and stimuli,[0],[0]
"Which correction is better for this scene?
person has a beard, a tie, or is looking straight.",7.1 Domains and stimuli,[0],[0]
Six of these attributes were included in the corpus to better reflect human RE generation strategies.,7.1 Domains and stimuli,[0],[0]
"Many human-generated REs in the corpus are overspecific, in that they contain attributes that are not necessary to make the RE semantically unique.
",7.1 Domains and stimuli,[0],[0]
"We chose the GIVE environment in order to test REs referring both to attributes of an object, i.e. color, and to its spatial relation to other visible objects in the scene.",7.1 Domains and stimuli,[0],[0]
"The TUNA Corpus was chosen as a more challenging domain, due to the greater number of available properties for each object on a scene.
",7.1 Domains and stimuli,[0],[0]
Each experimental subject was presented with screenshots containing a marked object and an RE.,7.1 Domains and stimuli,[0],[0]
"Subjects were told that we had previously referred to the marked object with the given RE, but an (imaginary) player misunderstood this RE and selected a different object, shown in a second screenshot.",7.1 Domains and stimuli,[0],[0]
"They were then asked to select which one of two corrections they considered better, where “better” was intentionally left unspecific.",7.1 Domains and stimuli,[0],[0]
Figs. 7 and 8 show examples for each domain.,7.1 Domains and stimuli,[0],[0]
"The full set of stimuli is available as supplementary material.
",7.1 Domains and stimuli,[0],[0]
"To maintain annotation quality in our crowdsourcing setting, we designed test items with a
clearly incorrect answer, such as REs referring to the wrong target or a nonexistent one.",7.1 Domains and stimuli,[0],[0]
"These test items were randomly interspersed with the real stimuli, and only subjects with a perfect score on the test items were taken into account.",7.1 Domains and stimuli,[0],[0]
"Experimental subjects were asked to rate up to 12 comparisons, shown in groups of 3 scenes at a time, and were automatically disqualified if they evaluated any individual scene in less than 10 seconds.",7.1 Domains and stimuli,[0],[0]
"The order in which the pairs of strategies were shown was randomized, to avoid effects related to the order in which they were presented on screen.",7.1 Domains and stimuli,[0],[0]
Our first experiment tested four strategies against each other.,7.2 Experiment 1,[0],[0]
Each experimental subject was presented with two screenshots of 3D scenes with a marked object and an RE (see Fig. 7 for an example).,7.2 Experiment 1,[0],[0]
"Each subject was shown a total of 12 scenes, selected at random from 16 test scenes.",7.2 Experiment 1,[0],[0]
"We collected 10 judgments for each possible combination of GIVE scene and pair of strategies, yielding a total of 943 judgements from 142 subjects after removing fake answers.
",7.2 Experiment 1,[0],[0]
We compared the Emphasis and Shortening strategies from Section 6 against two baselines.,7.2 Experiment 1,[0],[0]
"The Repeat strategy simply presented rs as a “contrastive” RE, without any capitalization.",7.2 Experiment 1,[0],[0]
Comparisons to Repeat test the hypothesis that subjects prefer explicit contrastive focus.,7.2 Experiment 1,[0],[0]
"The Random strategy randomly capitalized adjectives, adverbs, and/or prepositions that were not capitalized by the Emphasis strategy.",7.2 Experiment 1,[0],[0]
"Comparisons to Random verify that any preference for Emphasis is not only due to the presence of contrastive focus, but also because our method identifies precisely where that focus should be.
",7.2 Experiment 1,[0],[0]
Table 1a shows the results of all pairwise comparisons.,7.2 Experiment 1,[0],[0]
"For each row strategy StratR and each column strategy StratC , the table value corresponds to (#StratR pref.",7.2 Experiment 1,[0],[0]
over StratC)−(#StratC pref.,7.2 Experiment 1,[0],[0]
"over StratR)
",7.2 Experiment 1,[0],[0]
"(# tests between StratR and StratC)
",7.2 Experiment 1,[0],[0]
Significance levels are taken from a two-tailed binomial test over the counts of preferences for each strategy.,7.2 Experiment 1,[0],[0]
"We find a significant preference for the Emphasis strategy over all others, providing evidence that our algorithm assigns contrastive focus to the right words in the corrective RE.
",7.2 Experiment 1,[0],[0]
"While the Shortening strategy is numerically preferred over both baselines, the difference is not significant, and it is significantly worse than
the Emphasis strategy.",7.2 Experiment 1,[0],[0]
"This is surprising, given our initial assumption that listeners prefer succinct REs.",7.2 Experiment 1,[0],[0]
It is possible that a different strategy for shortening contrastive REs would work better; this bears further study.,7.2 Experiment 1,[0],[0]
"In our second experiment, we paired the Emphasis, Repeat, and Random strategies against each other, this time evaluating each strategy in the TUNA people domain.",7.3 Experiment 2,[0],[0]
"Due to its poor performance in Experiment 1, which was confirmed in pilot experiments for Experiment 2, the Shortening strategy was not included.
",7.3 Experiment 2,[0],[0]
"The experimental setup for the TUNA domain used 3x4 grids of pictures of people chosen at random from the TUNA Challenge, as shown in Fig. 8.",7.3 Experiment 2,[0],[0]
"We generated 8 such grids, along with REs ranging from two to five attributes and requiring one or two attributes to establish the correct contrast.",7.3 Experiment 2,[0],[0]
"The larger visual size of objects in the the TUNA scenes allowed us to mark both os and ou in a single picture without excessive clutter.
",7.3 Experiment 2,[0],[0]
"The REs for Experiment 2 were designed to only include attributes from the referred objects, but no information about its position in relation to other objects.",7.3 Experiment 2,[0],[0]
"The benefit is twofold: we avoid taxing our subjects’ memory with extremely long REs, and we ensure that the overall length of the second set of REs is comparable to those in the previous experiment.
",7.3 Experiment 2,[0],[0]
We obtained 240 judgements from 65 subjects (after removing fake answers).,7.3 Experiment 2,[0],[0]
Table 1b shows the results of all pairwise comparisons.,7.3 Experiment 2,[0],[0]
"We find that even in the presence of a larger number of attributes, our algorithm assigns contrastive focus to the correct words of the RE.",7.3 Experiment 2,[0],[0]
Our experiments confirm that the strategy for computing contrastive REs presented in this paper works in practice.,7.4 Discussion,[0],[0]
"This validates the corruption model, which approximates semantic mismatches between what the speaker said and what the listener understood as differences at the level of words in strings.",7.4 Discussion,[0],[0]
"Obviously, this model is still an approximation, and we will test its limits in future work.
",7.4 Discussion,[0],[0]
We find that users generally prefer REs with an emphasis over simple repetitions.,7.4 Discussion,[0],[0]
"In the more challenging scenes of the TUNA corpus, users even have a significant preference of Random over
Repeat, although this makes no semantic sense.",7.4 Discussion,[0],[0]
This preference may be due to the fact that emphasizing anything at least publically acknowledges the presence of a misunderstanding that requires correction.,7.4 Discussion,[0],[0]
"It will be interesting to explore whether this preference holds up in an interactive setting, rather than an overhearer experiment, where listeners will have to act upon the corrective REs.
",7.4 Discussion,[0],[0]
The poor performance of the Shortening strategy is a surprising negative result.,7.4 Discussion,[0],[0]
"We would expect a shorter RE to always be preferred, following the Gricean Maxim of Quantity (Grice, 1975).",7.4 Discussion,[0],[0]
"This may because our particular Shortening strategy can be improved, or it may be because listeners interpret the shortened REs not with respect to the original instructions, but rather with respect to a “refreshed” context (as observed, for instance, in Gotzner et al. (2016)).",7.4 Discussion,[0],[0]
"In this case the shortened REs would not be unique with respect to the refreshed, wider context.",7.4 Discussion,[0],[0]
"In this paper, we have presented an algorithm for generating contrastive feedback for a hearer who has misunderstood a referring expression.",8 Conclusion,[0],[0]
Our technique is based on modeling likely user misunderstandings and then attempting to give feedback that contrasts with the most probable incorrect understanding.,8 Conclusion,[0],[0]
"Our experiments show that this technique accurately predicts which words to mark as focused in a contrastive RE.
",8 Conclusion,[0],[0]
"In future work, we will complement the overhearer experiment presented here with an end-toend evaluation in an interactive NLG setting.",8 Conclusion,[0],[0]
This will allow us to further investigate the quality of the correction strategies and refine the Shortening strategy.,8 Conclusion,[0],[0]
It will also give us the opportunity to investigate empirically the limits of the corruption model.,8 Conclusion,[0],[0]
"Furthermore, we could use this data to refine the costs c(D), c(Ia) etc. for the edit operations, possibly assigning different costs to different edit operations.
",8 Conclusion,[0],[0]
"Finally, it would be interesting to combine our algorithm with a speech synthesis system.",8 Conclusion,[0],[0]
"In this way, we will be able to express focus with actual pitch accents, in contrast to the typographic approximation we made here.",8 Conclusion,[0],[0]
"The referring expressions (REs) produced by a natural language generation (NLG) system can be misunderstood by the hearer, even when they are semantically correct.",abstractText,[0],[0]
"In an interactive setting, the NLG system can try to recognize such misunderstandings and correct them.",abstractText,[0],[0]
"We present an algorithm for generating corrective REs that use contrastive focus (“no, the BLUE button”) to emphasize the information the hearer most likely misunderstood.",abstractText,[0],[0]
We show empirically that these contrastive REs are preferred over REs without contrast marking.,abstractText,[0],[0]
Generating Contrastive Referring Expressions,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 877–888 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
877",text,[0],[0]
"Broad-coverage knowledge graphs such as Freebase, Wikidata, and NELL are increasingly being used in many NLP and AI tasks.",1 Introduction,[0],[0]
"For instance, DBpedia and YAGO were vital for IBM’s Watson!",1 Introduction,[0],[0]
"Jeopardy system (Welty et al., 2012).",1 Introduction,[0],[0]
"Google’s Knowledge Graph is tightly integrated into its search engine, yielding improved responses for entity queries as well as for question answering.",1 Introduction,[0],[0]
"In a similar effort, Apple Inc. is building an inhouse knowledge graph to power Siri and its next generation of intelligent products and services.
",1 Introduction,[0],[0]
"Despite being rich sources of factual knowledge, cross-domain knowledge graphs often lack a succinct textual description for many of the existing entities.",1 Introduction,[0],[0]
Fig. 1 depicts an example of a concise entity description presented to a user.,1 Introduction,[0],[0]
"Descriptions of this sort can be beneficial both to humans and in downstream AI and natural language processing tasks, including question answering (e.g., Who
is Roger Federer?), named entity disambiguation (e.g., Philadelphia as a city vs. the film or even the brand of cream cheese), and information retrieval, to name but a few.
",1 Introduction,[0],[0]
"Additionally, descriptions of this sort can also be useful to determine the ontological type of an entity – another challenging task that often needs to be addressed in cross-domain knowledge graphs.",1 Introduction,[0],[0]
"Many knowledge graphs already provide ontological type information, and there has been substantial previous research on how to predict such types automatically for entities in knowledge graphs (Neelakantan and Chang, 2015; Miao et al., 2016; Kejriwal and Szekely, 2017), in semistructured resources such as Wikipedia (Ponzetto and Strube, 2007; de Melo and Weikum, 2010), or even in unstructured text (Snow et al., 2006; Bansal et al., 2014; Tandon et al., 2015).",1 Introduction,[0],[0]
"However, most such work has targeted a fixed inventory of types from a given target ontology, many
of which are more abstract in nature (e.g., human or artifact).",1 Introduction,[0],[0]
"In this work, we consider the task of generating more detailed open vocabulary descriptions (e.g., Swiss tennis player) that can readily be presented to end users, generated from facts in the knowledge graph.
",1 Introduction,[0],[0]
"Apart from type descriptions, certain knowledge graphs, such as Freebase and DBpedia, also provide a paragraph-length textual abstract for every entity.",1 Introduction,[0],[0]
"In the latter case, these are sourced from Wikipedia.",1 Introduction,[0],[0]
"There has also been research on generating such abstracts automatically (Biran and McKeown, 2017).",1 Introduction,[0],[0]
"While abstracts of this sort provide considerably more detail than ontological types, they are not sufficiently concise to be grasped at a single glance, and thus the onus is put on the reader to comprehend and summarize them.
",1 Introduction,[0],[0]
"Typically, a short description of an entity will hence need to be synthesized just by drawing on certain most relevant facts about it.",1 Introduction,[0],[0]
"While in many circumstances, humans tend to categorize entities at a level of abstraction commonly referred to as basic level categories (Rosch et al., 1976), in an information seeking setting, however, such as in Fig. 1, humans naturally expect more detail from their interlocutor.",1 Introduction,[0],[0]
"For example, occupation and nationality are often the two most relevant properties used in describing a person in Wikidata, while terms such as person or human being are likely to be perceived as overly unspecific.",1 Introduction,[0],[0]
"However, choosing such most relevant and distinctive attributes from the set of available facts about the entity is non-trivial, especially given the diversity of different kinds of entities in broad-coverage knowledge graphs.",1 Introduction,[0],[0]
"Moreover, the generated text should be coherent, succinct, and non-redundant.
",1 Introduction,[0],[0]
"To address this problem, we propose a dynamic memory-based generative network that can generate short textual descriptions from the available factual information about the entities.",1 Introduction,[0],[0]
"To the best of our knowledge, we are the first to present neural methods to tackle this problem.",1 Introduction,[0],[0]
Previous work has suggested generating short descriptions using predefined templates (cf. Section 4).,1 Introduction,[0],[0]
"However, this approach severely restricts the expressivity of the model and hence such templates are typically only applied to very narrow classes of entities.",1 Introduction,[0],[0]
"In contrast, our goal is to design a broad-coverage open domain description generation architecture.
",1 Introduction,[0],[0]
"In our experiments, we induce a new benchmark dataset for this task by relying on Wikidata, which
has recently emerged as the most popular crowdsourced knowledge base, following Google’s designation of Wikidata as the successor to Freebase (Tanon et al., 2016).",1 Introduction,[0],[0]
"With a broad base of 19,000 casual Web users as contributors, Wikidata is a crucial source of machine-readable knowledge in many applications.",1 Introduction,[0],[0]
"Unlike DBpedia and Freebase, Wikidata usually contains a very concise description for many of its entities.",1 Introduction,[0],[0]
"However, because Wikidata is based on user contributions, many new entries are created that still lack such descriptions.",1 Introduction,[0],[0]
This can be a problem for downstream tools and applications using Wikidata for background knowledge.,1 Introduction,[0],[0]
"Hence, even for Wikidata, there is a need for tools to generate fine-grained type descriptions.",1 Introduction,[0],[0]
"Fortunately, we can rely on the entities for which users have already contributed short descriptions to induce a new benchmark dataset for the task of automatically inducing type descriptions from structured data.",1 Introduction,[0],[0]
"Our proposed dynamic memory-based generative network consists of three key components: an input module, a dynamic memory module, and an output module.",2 A Dynamic Memory-based Generative Network Architecture,[0],[0]
A schematic diagram of these are given in Fig. 2.,2 A Dynamic Memory-based Generative Network Architecture,[0],[0]
"The input to the input module is a set of N facts F = {f1, f2, . . .",2.1 Input Module,[0],[0]
", fN} pertaining to an entity.",2.1 Input Module,[0],[0]
"Each of these input facts are essentially (s, p, o) triples, for subjects s, predicates p, and objects o.",2.1 Input Module,[0],[0]
"Upon being encoded into a distributed vector representation, we refer to them as fact embeddings.
",2.1 Input Module,[0],[0]
"Although many different encoding schemes can be adopted to obtain such fact embeddings, we opt for a positional encoding as described by Sukhbaatar et al. (2015), motivated in part by the considerations given by Xiong et al. (2016).",2.1 Input Module,[0],[0]
"For completeness, we describe the positional encoding scheme here.",2.1 Input Module,[0],[0]
We encode each fact fi as a vector fi =∑J j=1,2.1 Input Module,[0],[0]
"lj◦wij, where ◦ is an element-wise multiplication, and lj is a column vector with the structure lkj =",2.1 Input Module,[0],[0]
(1 − jJ ),2.1 Input Module,[0],[0]
"− (k/d)(1 − 2 j J ), with J being the number of words in the factual phrase, wij as the embedding of the j-th word, and d as the dimensionality of the embedding.",2.1 Input Module,[0],[0]
"Details about how these factual phrases are formed for our data are
given in Section 3.3.",2.1 Input Module,[0],[0]
"Thus, the output of this module is a concatenation of N fact embeddings F = [f1; f2; . . .",2.1 Input Module,[0],[0]
; fN].,2.1 Input Module,[0],[0]
The dynamic memory module is responsible for memorizing specific facts about an entity that will be useful for generating the next word in the output description sequence.,2.2 Dynamic Memory Module,[0],[0]
"Intuitively, such a memory should be able to update itself dynamically by accounting not only for the factual embeddings but also for the current context of the generated sequence of words.
",2.2 Dynamic Memory Module,[0],[0]
"To begin with, the memory is initialized as m(0) = max(0,WmF+ bm).",2.2 Dynamic Memory Module,[0],[0]
"At each time step t, the memory module attempts to gather pertinent contextual information by attending to and summing over the fact embeddings in a weighted manner.",2.2 Dynamic Memory Module,[0],[0]
"These attention weights are scalar values informed by two factors: (1) how much information from a particular fact is used by the previous memory state m(t−1), and (2) how much information of a particular fact is invoked in the current context of the output sequence h(t−1).",2.2 Dynamic Memory Module,[0],[0]
"Formally,
xi (t) =",2.2 Dynamic Memory Module,[0],[0]
"[|fi − h(t−1)|; |fi −m(t−1)|], (1)
zi (t) = W2 tanh(W1xi (t) + b1) + b2, (2)
a (t) i = exp(zi (t))∑N
k=1 exp(zk (t))
, (3)
where |.| is the element-wise absolute difference
and [; ] denotes the concatenation of vectors.",2.2 Dynamic Memory Module,[0],[0]
"Having obtained the attention weights, we apply a soft attention mechanism to extract the current context vector at time t as
c(t) =",2.2 Dynamic Memory Module,[0],[0]
N∑ i=1,2.2 Dynamic Memory Module,[0],[0]
a (t) i fi.,2.2 Dynamic Memory Module,[0],[0]
"(4)
This newly obtained context information is then used along with the previous memory state to update the memory state as follows:
C(t) =",2.2 Dynamic Memory Module,[0],[0]
"[m(t−1); c(t);h(t−1)] (5)
m(t) = max(0,WmC (t) + bm) (6)
",2.2 Dynamic Memory Module,[0],[0]
Such updated memory states serve as the input to the decoder sequence of the output module at each time step.,2.2 Dynamic Memory Module,[0],[0]
The output module governs the process of repeatedly decoding the current memory state so as to emit the next word in an ordered sequence of output words.,2.3 Output Module,[0],[0]
"We rely on GRUs for this.
",2.3 Output Module,[0],[0]
"At each time step, the decoder GRU is presented as input a glimpse of the current memory state m(t) as well as the previous context of the output sequence, i.e., the previous hidden state of the decoder h(t−1).",2.3 Output Module,[0],[0]
"At each step, the resulting output of the GRU is concatenated with the context vector ci(t) and is passed through a fully connected
layer and finally through a softmax layer.",2.3 Output Module,[0],[0]
"During training, we deploy teacher forcing at each step by providing the vector embedding of the previous correct word in the sequence as an additional input.",2.3 Output Module,[0],[0]
"During testing, when such a signal is not available, we use the embedding of the predicted word in the previous step as an additional input to the current step.",2.3 Output Module,[0],[0]
"Formally,
h(t) = GRU([m(t);w(t−1)],h(t−1)), (7)
h̃(t) = tanh(Wd[h (t); c(t)] + bd), (8)
ŷ(t) = Softmax(Woh̃(t) + bo), (9)
where [; ] is the concatenation operator, w(t−1) is vector embedding of the previous word in the sequence, and ŷ(t) is the probability distribution for the predicted word over the vocabulary at the current step.",2.3 Output Module,[0],[0]
"Training this model amounts to picking suitable values for the model parameters θ, which include the matrices W1, W2, Wm, Wd, Wo and the corresponding bias terms b1, b2, bm, bd, and bo as well as the various transition and output matrices of the GRU.
",2.4 Loss Function and Training,[0],[0]
"To this end, if each of the training instances has a description with a maximum of M words, we can rely on the categorical cross-entropy over the entire output sequence as the loss function:
L(θ) = − M∑ t=1 |V|∑",2.4 Loss Function and Training,[0],[0]
j=1 y (t) j log(ŷ (t) j ).,2.4 Loss Function and Training,[0],[0]
"(10)
where y(t)j ∈ {0, 1} and |V| is the vocabulary size.",2.4 Loss Function and Training,[0],[0]
We train our model end-to-end using Adam as the optimization technique.,2.4 Loss Function and Training,[0],[0]
"In this section, we describe the process of creating our benchmark dataset as well as the baseline methods and the experimental results.",3 Evaluation,[0],[0]
"For the evaluation of our method, we introduce a novel benchmark dataset that we have extracted from Wikidata and transformed to a suitable format.",3.1 Benchmark Dataset Creation,[0],[0]
"We rely on the official RDF exports of Wikidata, which are generated regularly (Erxleben et al., 2014), specifically, the RDF dump dated
2016-08-01, which consists of 19,768,780 entities with 2,570 distinct properties.",3.1 Benchmark Dataset Creation,[0],[0]
A pair of a property and its corresponding value represents a fact about an entity.,3.1 Benchmark Dataset Creation,[0],[0]
"In Wikidata parlance, such facts are called statements.",3.1 Benchmark Dataset Creation,[0],[0]
"We sample a dataset of 10K entities from Wikidata, and henceforth refer to the resulting dataset as WikiFacts10K. Our sampling method ensures that each entity in WikiFacts10K has an English description and at least 5 associated statements.",3.1 Benchmark Dataset Creation,[0],[0]
We then transform each extracted statement into a phrasal form by concatenating the words of the property name and its value.,3.1 Benchmark Dataset Creation,[0],[0]
"For example, the (subject, predicate, object) triple (Roger Federer, occupation, tennis player) is transformed to ’occupation tennis player’.",3.1 Benchmark Dataset Creation,[0],[0]
"We refer to these phrases as the factual phrases, which are embedded as described earlier.",3.1 Benchmark Dataset Creation,[0],[0]
"We randomly divide this dataset into training, validation, and test sets with a 8:1:1 ratio.",3.1 Benchmark Dataset Creation,[0],[0]
We have made our code and data available1 for reproducibility and to facilitate further research in this area.,3.1 Benchmark Dataset Creation,[0],[0]
We compare our model against an array of baselines of varying complexity.,3.2 Baselines,[0],[0]
"We experiment with some variants of our model as well as several other state-of-the-art models that, although not specifically designed for this setting, can straightforwardly be applied to the task of generating descriptions from factual data.
1.",3.2 Baselines,[0],[0]
Facts-to-sequence Encoder-Decoder Model.,3.2 Baselines,[0],[0]
This model is a variant of the standard sequence-to-sequence encoderdecoder architecture described by Sutskever et al. (2014).,3.2 Baselines,[0],[0]
"However, instead of an input sequence, it here operates on a set of fact embeddings {f1, f2, . . .",3.2 Baselines,[0],[0]
", fN}, which are emitted by the positional encoder described in Section 2.1.",3.2 Baselines,[0],[0]
"We initialize the hidden state of the decoder with a linear transformation of the fact embeddings as h(0) = WF + b, where F = [f1; f2; . . .",3.2 Baselines,[0],[0]
"; fN] is the concatenation of N fact embeddings.
",3.2 Baselines,[0],[0]
"As an alternative, we also experimented with a sequence encoder that takes a separate fact embedding as input at each step and initializes the decoder hidden state with the final hidden state of the encoder.",3.2 Baselines,[0],[0]
"However, this approach did not yield us better results.
",3.2 Baselines,[0],[0]
"1https://github.com/kingsaint/Open-vocabulary-entitytype-description
Table 1: Automatic evaluation results of different models.",3.2 Baselines,[0],[0]
"For a detailed explanation of the baseline models, please refer to Section 3.2.",3.2 Baselines,[0],[0]
"The best performing model for each column is highlighted in boldface.
",3.2 Baselines,[0],[0]
Model B-1 B-2 B-3 B-4 ROUGE-L METEOR CIDEr Facts-to-seq 0.404 0.324 0.274 0.242 0.433 0.214 1.627 Facts-to-seq w. Attention 0.491 0.414 0.366 0.335 0.512 0.257 2.207 Static Memory 0.374 0.298 0.255 0.223 0.383 0.185 1.328 DMN+ 0.281 0.234 0.236 0.234 0.275 0.139 0.912,3.2 Baselines,[0],[0]
"Our Model 0.611 0.535 0.485 0.461 0.641 0.353 3.295
2.",3.2 Baselines,[0],[0]
Facts-to-sequence Model with Attention Decoder.,3.2 Baselines,[0],[0]
The encoder of this model is identical to the one described above.,3.2 Baselines,[0],[0]
"The difference is in the decoder module that uses an attention mechanism.
",3.2 Baselines,[0],[0]
"At each time step t, the decoder GRU receives a context vector c(t) as input, which is an attention weighted sum of the fact embeddings.",3.2 Baselines,[0],[0]
"The attention weights and the context vectors are computed as follows:
x(t) =",3.2 Baselines,[0],[0]
"[w(t−1);h(t−1)] (11)
z(t) = Wx(t) + b",3.2 Baselines,[0],[0]
"(12)
a(t) = softmax(z(t))",3.2 Baselines,[0],[0]
"(13) c(t) = max(0, N∑ i=1",3.2 Baselines,[0],[0]
"a (t) i fi) (14)
",3.2 Baselines,[0],[0]
"After obtaining the context vector, it is fed to the GRU as input:
h(t) = GRU([w(t−1); c(t)],h(t−1)) (15)
3.",3.2 Baselines,[0],[0]
Static Memory Model.,3.2 Baselines,[0],[0]
This is a variant of our model in which we do not upgrade the memory dynamically at each time step.,3.2 Baselines,[0],[0]
"Rather, we use the initial memory state as the input to all of the decoder GRU steps.
4.",3.2 Baselines,[0],[0]
Dynamic Memory Network (DMN+).,3.2 Baselines,[0],[0]
"We consider the approach proposed by Xiong et al. (2016), which supersedes Kumar et al. (2016).",3.2 Baselines,[0],[0]
"However, some minor modifications are needed to adapt it to our task.",3.2 Baselines,[0],[0]
"Unlike the bAbI dataset, our task does not involve any question.",3.2 Baselines,[0],[0]
"The presence of a question is imperative in DMN+, as it helps to determine the initial state of the episodic memory module.",3.2 Baselines,[0],[0]
"Thus, we prepend an interrogative phrase such as ”Who is” or ”What is” to every entity name.",3.2 Baselines,[0],[0]
"The question module of the DMN+ is hence presented with a question such as
”Who is Roger Federer?”",3.2 Baselines,[0],[0]
or ”What is Star Wars?”.,3.2 Baselines,[0],[0]
Another difference is in the output module.,3.2 Baselines,[0],[0]
"In DMN+, the final memory state is passed through a softmax layer to generate the answer.",3.2 Baselines,[0],[0]
"Since most answers in the bAbI dataset are unigrams, such an approach suffices.",3.2 Baselines,[0],[0]
"However, as our task is to generate a sequence of words as descriptions, we use a GRU-based decoder sequence model, which at each time step receives the final memory state m(T ) as input to the GRU.",3.2 Baselines,[0],[0]
"We restrict the number of memory update episodes to 3, which is also the preferred number of episodes in the original paper.",3.2 Baselines,[0],[0]
"For each entity in the WikiFacts10K dataset, there is a corresponding set of facts expressed as factual phrases as defined earlier.",3.3 Experimental Setup,[0],[0]
Each factual phrase in turn is encoded as a vector by means of the positional encoding scheme described in Section 2.1.,3.3 Experimental Setup,[0],[0]
"Although other variants could be considered, such as LSTMs and GRUs, we apply this standard fact encoding mechanism for our model as well as all our baselines for the sake of uniformity and fair comparison.",3.3 Experimental Setup,[0],[0]
"Another factor that makes the use of a sequence encoder such as LSTMs or GRUs less suitable is that the set of input facts is essentially unordered without any temporal correlation between facts.
",3.3 Experimental Setup,[0],[0]
We fixed the dimensionality of the fact embeddings and all hidden states to be 100.,3.3 Experimental Setup,[0],[0]
"The vocabulary size is 29K. Our models and all other baselines are trained for a maximum of 25 epochs with an early stopping criterion and a fixed learning rate of 0.001.
",3.3 Experimental Setup,[0],[0]
"To evaluate the quality of the generated descriptions, we rely on the standard BLEU (B-1, B-2, B-3, B-4), ROUGE-L, METEOR and CIDEr metrics, as implemented by Sharma et al. (2017).",3.3 Experimental Setup,[0],[0]
"Of course, we would be remiss not to point out that these metrics are imperfect.",3.3 Experimental Setup,[0],[0]
"In general, they tend
to be conservative in that they only reward generated descriptions that overlap substantially with the ground truth descriptions given in Wikidata.",3.3 Experimental Setup,[0],[0]
"In reality, it may of course be the case that alternative descriptions are equally appropriate.",3.3 Experimental Setup,[0],[0]
"In fact, inspecting the generated descriptions, we found that our method often indeed generates correct alternative descriptions.",3.3 Experimental Setup,[0],[0]
"For instance, Darius Kaiser is described as a cyclist, but one could also describe him as a German bicycle racer.",3.3 Experimental Setup,[0],[0]
"Despite their shortcomings, the aforementioned metrics have generally been found suitable for comparing supervised systems, in that systems with significantly higher scores tend to fare better at learning to reproduce ground truth captions.",3.3 Experimental Setup,[0],[0]
The results of the experiments are reported in Table 1.,3.4 Results,[0],[0]
"Across all metrics, we observe that our model obtains significantly better scores than the alternatives.
",3.4 Results,[0],[0]
A facts-to-seq model exploiting our positional fact encoding performs adequately.,3.4 Results,[0],[0]
"With an additional attention mechanism (Facts-to-seq w. Attention), the results are even better.",3.4 Results,[0],[0]
This is on account of the attention mechanism’s ability to reconsider the attention distribution at each time step using the current context of the output sequence.,3.4 Results,[0],[0]
The results suggest that this enables the model to more flexibly focus on the most pertinent parts of the input.,3.4 Results,[0],[0]
"In this regard, such a model thus resembles our approach.",3.4 Results,[0],[0]
"However, there are important differences between this baseline and our model.",3.4 Results,[0],[0]
"Our model not only uses the current context of the output sequence, but also memorizes how information of a particular fact has been used thus far, via the dynamic memory module.",3.4 Results,[0],[0]
"We conjecture that the dynamic memory module thereby facilitates generating longer description sequences more accurately by better tracking which parts have been attended to, as is empirically corroborated by the comparably higher BLEU scores for longer n-grams.
",3.4 Results,[0],[0]
"The analysis of the Static Memory approach amounts to an ablation study, as it only differs from our full model in lacking memory updates.",3.4 Results,[0],[0]
The divergence of scores between the two variants suggests that the dynamic memory indeed is vital for more dynamically attending to the facts by taking into account the current context of the output sequence at each step.,3.4 Results,[0],[0]
"Our model needs to dynam-
ically achieve different objectives at different time points.",3.4 Results,[0],[0]
"For instance, it may start off looking at several properties to infer a type of the appropriate granularity for the entity (e.g., village), while in the following steps it considers a salient property and emits the corresponding named entity for it as well as a suitable preposition (e.g., in China).
",3.4 Results,[0],[0]
"Finally, the poor results of the DMN+ approach show that a naı̈ve application of a state-of-theart dynamic memory architecture does not suffice to obtain strong results on this task.",3.4 Results,[0],[0]
"Indeed, the DMN+ is even outperformed by our Facts-to-seq baseline.",3.4 Results,[0],[0]
"This appears to stem from the inability of the model to properly memorize all pertinent facts in its encoder.
",3.4 Results,[0],[0]
Analysis.,3.4 Results,[0],[0]
"In Figure 3, we visualize the attention distribution over facts.",3.4 Results,[0],[0]
"We observe how the model shifts its focus to different sorts of properties while generating successive words.
",3.4 Results,[0],[0]
Table 2 provides a representative sample of the generated descriptions and their ground truth counterparts.,3.4 Results,[0],[0]
A manual inspection reveals five distinct patterns.,3.4 Results,[0],[0]
The first case is that of exact matches with the reference descriptions.,3.4 Results,[0],[0]
"The second involves examples on which there is a high overlap of words between the ground truth and generated descriptions, but the latter as a whole is incorrect because of semantic drift or other challenges.",3.4 Results,[0],[0]
"In some cases, the model may have never seen a word or named entity during training (e.g., Hypocrisy), or their frequency is very limited in the training set.",3.4 Results,[0],[0]
"While it has been shown that GRUs with an attention mechanism are capable of learning to copy random strings from the input (Gu et al., 2016), we conjecture that a dedicated copy mechanism may help to mitigate this problem, which we will explore in future research.",3.4 Results,[0],[0]
"In other cases, the model conflates semantically related concepts, as is evident from examples such as a film being described as a filmmaker and a polo player as a water polo player.",3.4 Results,[0],[0]
"Next, the third group involves generated descriptions that are more specific than the ground truth, but correct, while, in the fourth group, the generated outputs generalize the descriptions to a certain extent.",3.4 Results,[0],[0]
"For example, American musician and pianist is generalized as American musician, since musician is a hypernym of pianist.",3.4 Results,[0],[0]
"Finally, the last group consists of cases in which our model generated descriptions that are factually accurate and may be deemed appropriate despite diverging from the
reference descriptions to an extent that almost no overlapping words are shared with them.",3.4 Results,[0],[0]
Note that such outputs are heavily penalized by the metrics considered in our evaluation.,3.4 Results,[0],[0]
Type Prediction.,4 Related Work,[0],[0]
"There has been extensive work on predicting the ontological types of entities in large knowledge graphs (Neelakantan and Chang, 2015; Miao et al., 2016; Kejriwal and Szekely, 2017; Shimaoka et al., 2017), in semistructured resources such as Wikipedia (Ponzetto and Strube, 2007; de Melo and Weikum, 2010), as well as in text (Del Corro et al., 2015; Yaghoobzadeh and Schütze, 2015; Ren et al.,
2016).",4 Related Work,[0],[0]
"However, the major shortcoming of these sorts of methods, including those aiming at more fine-grained typing, is that they assume that the set of candidate types is given as input, and the main remaining challenge is to pick the correct one(s).",4 Related Work,[0],[0]
"In contrast, our work yields descriptions that often indicate the type of entity, but typically are more natural-sounding and descriptive (e.g. French Impressionist artist) than the oftentimes abstract ontological types (such as human or artifact) chosen by type prediction methods.
",4 Related Work,[0],[0]
"A separate, long-running series of work has obtained open vocabulary type predictions for named entities and concepts mentioned in text (Hearst, 1992; Snow et al., 2006), possibly also induc-
ing taxonomies from them (Poon and Domingos, 2010; Velardi et al., 2013; Bansal et al., 2014).",4 Related Work,[0],[0]
"However, these methods typically just need to select existing spans of text from the input as the output description.
",4 Related Work,[0],[0]
Text Generation from Structured Data.,4 Related Work,[0],[0]
Research on methods to generate descriptions for entities has remained scant.,4 Related Work,[0],[0]
"Lebret et al. (2016) take Wikipedia infobox data as input and train a custom form of neural language model that, conditioned on occurrences of words in the input table, generates biographical sentences as output.",4 Related Work,[0],[0]
"However, their system is limited to a single kind of description (biographical sentences) that tend to share a common structure.",4 Related Work,[0],[0]
Wang et al. (2016) focus on the problem of temporal ordering of extracted facts.,4 Related Work,[0],[0]
Biran and McKeown (2017) introduced a template-based description generation framework for creating hybrid concept-to-text and text-to-text generation systems that produce descriptions of RDF entities.,4 Related Work,[0],[0]
"Their framework can be tuned for new domains, but does not yield a broad-coverage multi-domain model.",4 Related Work,[0],[0]
"Voskarides et al. (2017) first create sentence templates for specific entity relationships, and then, given a new relationship instance, generate a description by selecting the best template and filling the template slots with the appropriate entities from the knowledge graph.",4 Related Work,[0],[0]
Kutlak et al. (2013) generates referring expressions by converting property-value pairs to text using a hand-crafted mapping scheme.,4 Related Work,[0],[0]
Wiseman et al. (2017) considered the related task of mapping tables with numeric basketball statistics to natural language.,4 Related Work,[0],[0]
They investigated an extensive array of current state-of-the-art neural pointer methods but found that template-based models outperform all neural models on this task by a significant margin.,4 Related Work,[0],[0]
"However, their method requires specific templates for each domain (for example, basketball games in their case).",4 Related Work,[0],[0]
"Applying template-based methods to cross-domain knowledge bases is highly challenging, as this would require too many different templates for different types of entities.",4 Related Work,[0],[0]
"Our dataset contains items of from a large number of diverse domains such as humans, books, films, paintings, music albums, genes, proteins, cities, scientific articles, etc., to name but a few.
",4 Related Work,[0],[0]
"Chen and Mooney (2008) studied the task of taking representations of observations from a sports simulation (Robocup simulator) as input, e.g. pass(arg1=purple6, arg2=purple3), and gen-
erating game commentary.",4 Related Work,[0],[0]
"Liang et al. (2009) learned alignments between formal descriptions such as rainChance(time=26-30,mode=Def) and natural language weather reports.",4 Related Work,[0],[0]
"Mei et al. (2016) used LSTMs for these sorts of generation tasks, via a custom coarse-to-fine architecture that first determines which input parts to focus on.
",4 Related Work,[0],[0]
Much of the aforementioned work essentially involves aligning small snippets in the input to the relevant parts in the training output and then learning to expand such input snippets into full sentences.,4 Related Work,[0],[0]
"In contrast, in our task, alignments between parts of the input and the output do not suffice.",4 Related Work,[0],[0]
"Instead, describing an entity often also involves considering all available evidence about that entity to infer information about it that is often not immediately given.",4 Related Work,[0],[0]
"Rather than verbalizing facts, our method needs a complex attention mechanism to predict an object’s general type and consider the information that is most likely to appear salient to humans from across the entire input.
",4 Related Work,[0],[0]
"The WebNLG Challenge (Gardent et al., 2017) is another task for generating text from structured data.",4 Related Work,[0],[0]
"However, this task requires a textual verbalization of every triple.",4 Related Work,[0],[0]
"On the contrary, the task we consider in this work is quite complementary in that a verbalization of all facts one-by-one is not the sought result.",4 Related Work,[0],[0]
"Rather, our task requires synthesizing a short description by carefully selecting the most relevant and distinctive facts from the set of all available facts about the entity.",4 Related Work,[0],[0]
"Due to these differences, the WebNLG dataset was not suitable for the research question considered by our paper.
",4 Related Work,[0],[0]
Neural Text Summarization.,4 Related Work,[0],[0]
Generating entity descriptions is related to the task of text summarization.,4 Related Work,[0],[0]
"Most traditional work in this area was extractive in nature, i.e. it selects the most salient sentences from a given input text and concatenates them to form a shorter summary or presents them differently to the user (Yang et al., 2017).",4 Related Work,[0],[0]
"Abstractive summarization goes beyond this in generating new text not necessarily encountered in the input, as is typically necessary in our setting.",4 Related Work,[0],[0]
"The surge of sequence-to-sequence modeling of text via LSTMs naturally extends to the task of abstractive summarization by training a model to accept a longer sequence as input and learning to generate a shorter compressed sequence as a summary.
",4 Related Work,[0],[0]
Rush et al. (2015) employed this idea to generate a short headline from the first sentence of a text.,4 Related Work,[0],[0]
"Subsequent work investigated the use of
architectures such as pointer-generator networks to better cope with long input texts (See et al., 2017).",4 Related Work,[0],[0]
"Recently, Liu et al. (2018) presented a model that generates an entire Wikipedia article via a neural decoder component that performs abstractive summarization of multiple source documents.",4 Related Work,[0],[0]
Our work differs from such previous work in that we do not consider a text sequence as input.,4 Related Work,[0],[0]
"Rather, our input are a series of entity relationships or properties, as reflected by our facts-to-sequence baselines in the experiments.",4 Related Work,[0],[0]
Note that our task is in certain respects also more difficult than text summarization.,4 Related Work,[0],[0]
"While regular neural summarizers are often able to identify salient spans of text that can be copied to the output, our input is of a substantially different form than the desired output.
",4 Related Work,[0],[0]
"Additionally, our goal is to make our method applicable to any entity with factual information that may not have a corresponding Wikipedia-like article available.",4 Related Work,[0],[0]
"Indeed, Wikidata currently has 46 million items, whereas the English Wikipedia has only 5.6 million articles.",4 Related Work,[0],[0]
"Hence, for the vast majority of items in Wikidata, no corresponding Wikipedia article is available.",4 Related Work,[0],[0]
"In such cases, a summarization baseline will not be effective.
",4 Related Work,[0],[0]
Episodic Memory Architectures.,4 Related Work,[0],[0]
A number of neural models have been put forth that possess the ability to interact with a memory component.,4 Related Work,[0],[0]
Recent advances in neural architectures that combine memory components with an attention mechanism exhibit the ability to extract and reason over factual information.,4 Related Work,[0],[0]
"A well-known example is the End-To-End Memory Network model by Sukhbaatar et al. (2015), which may make multiple passes over the memory input to facilitate multi-hop reasoning.",4 Related Work,[0],[0]
"These have been particularly successful on the bAbI test suite of artificial comprehension tests (Weston et al., 2015), due to their ability to extract and reason over the input.
",4 Related Work,[0],[0]
"At the core of the Dynamic Memory Networks (DMN) architecture (Kumar et al., 2016) is an episodic memory module, which is updated at each episode with new information that is required to answer a predefined question.",4 Related Work,[0],[0]
"Our approach shares several commonalities with DMNs, as it is also endowed with a dynamic memory of this sort.",4 Related Work,[0],[0]
"However, there are also a number of significant differences.",4 Related Work,[0],[0]
"First of all, DMN and its improved version DMN+ (Xiong et al., 2016) assume sequential correlations between the sentences and
rely on them for reasoning purposes.",4 Related Work,[0],[0]
"To this end, DMN+ needs an additional layer of GRUs, which is used to capture sequential correlations among sentences.",4 Related Work,[0],[0]
"Our model does not need any such layer, as facts in a knowledge graph do not necessarily possess any sequential interconnections.",4 Related Work,[0],[0]
"Additionally, DMNs assume a predefined number of memory episodes, with the final memory state being passed to the answer module.",4 Related Work,[0],[0]
"Unlike DMNs, our model uses the dynamic context of the output sequence to update the memory state.",4 Related Work,[0],[0]
The number of memory updates in our model flexibly depends on the length of the generated sequence.,4 Related Work,[0],[0]
"DMNs also have an additional question module as input, which guides the memory updates and also the output, while our model does not leverage any such guiding factor.",4 Related Work,[0],[0]
"Finally, in DMNs, the output is typically a unigram, whereas our model emits a sequence of words.",4 Related Work,[0],[0]
Short textual descriptions of entities facilitate instantaneous grasping of key information about entities and their types.,5 Conclusion,[0],[0]
"Generating them from facts in a knowledge graph requires not only mapping the structured fact information to natural language, but also identifying the type of entity and then discerning the most crucial pieces of information for that particular type from the long list of input facts and compressing them down to a highly succinct form.",5 Conclusion,[0],[0]
"This is very challenging in light of the very heterogeneous kinds of entities in our data.
",5 Conclusion,[0],[0]
"To this end, we have introduced a novel dynamic memory-based neural architecture that updates its memory at each step to continually reassess the relevance of potential input signals.",5 Conclusion,[0],[0]
We have shown that our approach outperforms several competitive baselines.,5 Conclusion,[0],[0]
"In future work, we hope to explore the potential of this architecture on further kinds of data, including multimodal data (Long et al., 2018), from which one can extract structured signals.",5 Conclusion,[0],[0]
Our code and data is freely available.2,5 Conclusion,[0],[0]
This research is funded in part by ARO grant no.,Acknowledgments,[0],[0]
"W911NF-17-C-0098 as part of the DARPA SocialSim program.
",Acknowledgments,[0],[0]
2https://github.com/kingsaint/ Open-vocabulary-entity-type-description,Acknowledgments,[0],[0]
"While large-scale knowledge graphs provide vast amounts of structured facts about entities, a short textual description can often be useful to succinctly characterize an entity and its type.",abstractText,[0],[0]
"Unfortunately, many knowledge graph entities lack such textual descriptions.",abstractText,[0],[0]
"In this paper, we introduce a dynamic memory-based network that generates a short open vocabulary description of an entity by jointly leveraging induced fact embeddings as well as the dynamic context of the generated sequence of words.",abstractText,[0],[0]
We demonstrate the ability of our architecture to discern relevant information for more accurate generation of type description by pitting the system against several strong baselines.,abstractText,[0],[0]
Generating Fine-Grained Open Vocabulary Entity Type Descriptions,title,[0],[0]
"Recently there has been an explosion in applications for natural language and dialogue interaction ranging from direction-giving and tourist information to interactive story systems (Dethlefs et al., 2014; Walker et al., 2011; Hu et al., 2015).",1 Introduction,[0],[0]
"While this is due in part to progress in statistical natural language understanding, many applications require the system to actually respond in a meaningful way.",1 Introduction,[0],[0]
Yet the natural language generation (NLG) component of many interactive dialogue systems remains largely handcrafted.,1 Introduction,[0],[0]
"This
limitation greatly restricts the range of applications; it also means that it is impossible to take advantage of recent work in expressive and statistical language generation that can dynamically and automatically produce a large number of variations of given content (Rieser and Lemon, 2011; Paiva and Evans, 2004; Langkilde, 1998; Rowe et al., 2008; Mairesse and Walker, 2011).",1 Introduction,[0],[0]
"Such variations are important for expressive purposes, we well as for user adaptation and personalization (Zukerman and Litman, 2001; Wang et al., 2005; McQuiggan et al., 2008).",1 Introduction,[0],[0]
"We propose that a solution to this problem lies in new methods for developing language generation resources.
",1 Introduction,[0],[0]
"First we describe the ES-TRANSLATOR (or EST), a computational language generator that has previously been applied only to fables, e.g. the fable in Table 3 (Rishes et al., 2013).",1 Introduction,[0],[0]
"We quantitatively evaluate the domain independence of the EST by applying it to social media narratives, such as the Startled Squirrel story in Table 1.",1 Introduction,[0],[0]
"We then present a parameterized general-purpose framework built on the EST pipeline, EST 2.0, that can generate many different tellings of the same story, by utilizing sentence planning and point of view parameters.",1 Introduction,[0],[0]
"Automatically generated story variations are shown in Table 2 and Table 4.
",1 Introduction,[0],[0]
"We hypothesize many potential uses for our ap-
ar X
iv :1
70 8.
08 58
0v 1
[ cs
.C",1 Introduction,[0],[0]
"L
] 2
9 A
ug 2
01 7
proach to repurposing and retelling existing stories.",1 Introduction,[0],[0]
"First, such stories are created daily in the thousands and cover any topic imaginable.",1 Introduction,[0],[0]
"They are natural and personal, and may be funny, sad, heart-warming or serious.",1 Introduction,[0],[0]
"There are many potential applications: virtual companions, educational storytelling, or to share troubles in therapeutic settings (Bickmore, 2003; Pennebaker and Seagal, 1999; Gratch et al., 2012).
",1 Introduction,[0],[0]
"Previous research on NLG of linguistic style shows that dialogue systems are more effective if they can generate stylistic linguistic variations based on the user’s emotional state, personality, style, confidence, or other factors (André et al., 2000; Piwek, 2003; McQuiggan et al., 2008; Porayska-Pomsta and Mellish, 2004; Forbes-Riley and Litman, 2011; Wang et al., 2005; Dethlefs et al., 2014).",1 Introduction,[0],[0]
"Other work focuses on variation in journalistic writing or instruction manuals, where stylistic variations as well as journalistic slant or connotations have been explored (Hovy, 1988; Green and DiMarco, 1993; Paris and Scott, 1994; Power et al., 2003; Inkpen and Hirst, 2004).",1 Introduction,[0],[0]
"Previous iterations of the EST simply presented a sequence of events (Rishes et al., 2013).",1 Introduction,[0],[0]
"This work implements parameterized variation of linguistic style in the context of weblogs in order to introduce discourse structure into our generated stories.
",1 Introduction,[0],[0]
"Our approach differs from previous work on NLG for narrative because we emphasize (1) domain-independent methods; and (2) generating a large range of variation, both narratological and stylistic.",1 Introduction,[0],[0]
"(Lukin and Walker, 2015)’s",1 Introduction,[0],[0]
"work on the EST is the first to generate dialogue within stories, to have the ability to vary direct vs. indirect speech, and to generate dialogue utterances using different stylistic models for character voices.",1 Introduction,[0],[0]
"Previous work can generate narratological variations, but is domain dependent (Callaway and Lester, 2002; Montfort, 2007).
",1 Introduction,[0],[0]
"Sec. 2 describes our corpus of stories and the ar-
chitecture of our story generation framework, EST 2.0.1 Sec. 3 describes experiments testing the coverage and correctness of EST 2.0.",1 Introduction,[0],[0]
Sec. 4 describes experiments testing user perceptions of different linguistic variations in storytelling.,1 Introduction,[0],[0]
"Our contributions are:
• We produce SIG representations of 100 personal narratives from a weblog corpus, using the story annotation tool Scheherezade (Elson and McKeown, 2009; Elson, 2012); • We compare EST 2.0 to EST and show how we have not only made improvements to the translation algorithm, but can extend and compare to personal narratives.",1 Introduction,[0],[0]
• We implement a parameterized variation of linguistic style in order to introduce discourse structure into our generated narratives.,1 Introduction,[0],[0]
"• We carry out experiments to gather user perceptions of different sentence planning choices that can be made with complex sentences in stories.
",1 Introduction,[0],[0]
We sum up and discuss future work in Sec. 5.,1 Introduction,[0],[0]
"Fig. 1 illustrates our overall architecture, which uses NLG modules to separate the process of planning What to say (content planning and selection,
1The corpus is available from https://nlds.soe.",2 Story Generation Framework,[0],[0]
"ucsc.edu/personabank.
fabula) from decisions about How to say it (sentence planning and realization, discourse).",2 Story Generation Framework,[0],[0]
"We build on three existing tools from previous work: the SCHEHEREZADE story annotation tool, the PERSONAGE generator, and the ES-TRANSLATOR (EST) (Elson, 2012; Mairesse and Walker, 2011; Rishes et al., 2013).",2 Story Generation Framework,[0],[0]
The EST uses the STORY INTENTION GRAPH (SIG) representation produced by SCHEHEREZADE and its theoretical grounding as a basis for the content for generation.,2 Story Generation Framework,[0],[0]
The EST bridges the narrative representation of the SIG to the representation required by PERSONAGE by generating the text plans and the deep syntactic structures that PERSONAGE requires.,2 Story Generation Framework,[0],[0]
Thus any story or content represented as a SIG can be retold using PERSONAGE.,2 Story Generation Framework,[0],[0]
"See Fig. 1.
",2 Story Generation Framework,[0],[0]
"There are several advantages to using the SIG as the representation for a content pool:
• Elson’s DRAMABANK provides stories encoded as SIGs including 36 Aesop’s Fables, such as The Fox and the Crow in Table 3.
",2 Story Generation Framework,[0],[0]
"• The SIG framework includes an annotation tool called SCHEHERAZADE that supports representing any narrative as a SIG.
• SCHEHEREZADE comes with a realizer that regenerates stories from the SIG: this realizer provides alternative story realizations that we can compare to the EST 2.0 output.
",2 Story Generation Framework,[0],[0]
"We currently have 100 personal narratives annotated with the SIG representation on topics such as travel, storms, gardening, funerals, going to the doctor, camping, and snorkeling, selected from a corpus of a million stories (Gordon and Swanson, 2009).",2 Story Generation Framework,[0],[0]
"We use the stories in Tables 1 and 3 in this paper to explain our framework.
",2 Story Generation Framework,[0],[0]
Fig. 2 shows the SIG for The Startled Squirrel story in Table 1.,2 Story Generation Framework,[0],[0]
"To create a SIG, SCHEHERAZADE annotators: (1) identify key entities; (2) model events and statives as propositions and arrange them in a timeline; and (3) model the annotator’s understanding of the overarching goals, plans and beliefs of the story’s agents.",2 Story Generation Framework,[0],[0]
"SCHEHERAZADE allows users to annotate a story along several dimensions, starting with the surface form of the story (first column in Table 2) and then proceeding to deeper representations.",2 Story Generation Framework,[0],[0]
"The first dimension (second column in Table 2) is called the “timeline layer”, in which the story is encoded as predicate-argument structures (propositions) that are temporally ordered on a timeline.",2 Story Generation Framework,[0],[0]
"SCHEHERAZADE adapts information about predicate-argument structures from the VerbNet lexical database (Kipper et al., 2006) and uses
WordNet (Fellbaum, 1998) as its noun and adjectives taxonomy.",2 Story Generation Framework,[0],[0]
"The arcs of the story graph are labeled with discourse relations, such as attempts to cause, or temporal order (see Chapter 4 of (Elson, 2012).)
",2 Story Generation Framework,[0],[0]
"The EST applies a model of syntax to the SIG which translates from the semantic representation of the SIG to the syntactic formalism of Deep Syntactic Structures (DSYNTS) required by the PERSONAGE generator (Lavoie and Rambow, 1997; Melčuk, 1988; Mairesse and Walker, 2011).",2 Story Generation Framework,[0],[0]
Fig. 1 provides a high level view of the architecture of EST.,2 Story Generation Framework,[0],[0]
"The full translation methodology is described in (Rishes et al., 2013).
DSYNTS are a flexible dependency tree representation of an utterance that gives us access to the underlying linguistic structure of a sentence that goes beyond surface string manipulation.",2 Story Generation Framework,[0],[0]
The nodes of the DSYNTS syntactic trees are labeled with lexemes and the arcs of the tree are labeled with syntactic relations.,2 Story Generation Framework,[0],[0]
"The DSYNTS formalism distinguishes between arguments and modifiers and between different types of arguments
(subject, direct and indirect object etc).",2 Story Generation Framework,[0],[0]
Lexicalized nodes also contain a range of grammatical features used in generation.,2 Story Generation Framework,[0],[0]
"RealPro handles morphology, agreement and function words to produce an output string.
",2 Story Generation Framework,[0],[0]
"This paper utilizes the ability of the EST 2.0 and the flexibility of DSYNTS to produce direct speech that varies the character voice as illustrated in Table 4 (Lukin and Walker, 2015).",2 Story Generation Framework,[0],[0]
"By simply modifying the person parameter in the DSYNTS, we can change the sentence to be realized in the first person.",2 Story Generation Framework,[0],[0]
"For example, to produce the variations in Table 4, we use both first person, and direct speech, as well as linguistic styles from PERSONAGE: a neutral voice for the narrator, a shy voice for the crow, and a laid-back voice for the fox (Lukin and Walker, 2015).",2 Story Generation Framework,[0],[0]
"We fully utilize this variation when we retell personal narratives in EST 2.0.
",2 Story Generation Framework,[0],[0]
"This paper and introduces support for new discourse relations, such as aggregating clauses related by the contingency discourse relation (one of many listed in the Penn Discourse Tree Bank (PDTB) (Prasad et al., 2008)).",2 Story Generation Framework,[0],[0]
"In SIG encoding, contingency clauses are always expressed with the “in order to” relation (Table 6, 1).",2 Story Generation Framework,[0],[0]
"To support linguistic variation, we introduce “de-aggregation” onto these aggregating clauses in order to have the flexibility to rephrase, restructure, or ignore clauses as indicated by our parameterized sentence planner.",2 Story Generation Framework,[0],[0]
"We identify candidate story points in the SIG that contain a contingency relation (annotated in the Timeline layer) and deliberately break apart
this hard relationship to create nucleus and satellite DSYNTS that represents the entire sentence (Table 6, 2) (Mann and Thompson, 1988).",2 Story Generation Framework,[0],[0]
"We create a text plan (Table 6, 3) to allow the sentence planner to reconstruct this content in various ways.",2 Story Generation Framework,[0],[0]
"Table 5 shows sentence planning variations for the contingency relation for both fables and personal narratives (soSN, becauseNS, becauseSN, NS, N), the output of EST 1.0, the original sentence (original), and the SCHEHERAZADE realization (Sch) which provides an additional baseline.",2 Story Generation Framework,[0],[0]
The Sch variant is the original “in order to” contingency relationship produced by the SIG annotation.,2 Story Generation Framework,[0],[0]
"The becauseNS operation presents the nucleus first, followed by a because, and then the satellite.",2 Story Generation Framework,[0],[0]
We can also treat the nucleus and satellite as two different sentences (NS) or completely leave off the satellite (N).,2 Story Generation Framework,[0],[0]
"We believe the N variant is useful if the satellite can be easily inferred from the prior context.
",2 Story Generation Framework,[0],[0]
The richness of the discourse information present in the SIG and our ability to de-aggregate and aggregate will enable us to implement other discourse relations in future work.,2 Story Generation Framework,[0],[0]
"After annotating our 100 stories with the SCHEHERAZADE annotation tool, we ran them through the EST, and examined the output.",3 Personal Narrative Evaluation,[0],[0]
"We discovered several bugs arising from variation in the blogs that are not present in the Fables, and fixed them.",3 Personal Narrative Evaluation,[0],[0]
"In previous work on the EST, the machine translation metrics Levenshtein’s distance and BLEU score were used to compare
Table 6: 1: original unbroken DSYNTS; 2) deaggregated DSYNTS; 3) contingency text plan
1: ORIGINAL <dsynts id=""5_6""> <dsyntnode class=""verb"" lexeme=""organize""
mode="""" mood=""ind"" rel=""II"" tense=""past""> <dsyntnode article=""def"" class=""common_noun""
lexeme=""bird"" number=""pl"" person="""" rel=""I""/> <dsyntnode article=""def"" class=""common_noun""
lexeme=""bird"" number=""pl"" person="""" rel=""II""/> <dsyntnode class=""preposition"" lexeme=""on""
rel=""ATTR""> <dsyntnode article=""def"" class=""common_noun""
lexeme=""railing"" number=""sg"" person="""" rel=""II""> <dsyntnode article=""no-art"" class=""common_noun"" lexeme=""deck"" number=""sg"" person="""" rel=""I""/>
</dsyntnode> </dsyntnode>",3 Personal Narrative Evaluation,[0],[0]
<,3 Personal Narrative Evaluation,[0],[0]
"dsyntnode class=""preposition"" lexeme=""in_order""
rel=""ATTR""> <dsyntnode class=""verb"" extrapo=""+"" lexeme=""wait""
mode=""inf-to"" mood=""inf-to"" rel=""II"" tense=""inf-to""> <dsyntnode article=""def"" class=""common_noun"" lexeme=""bird"" number=""pl"" person="""" rel=""I""/>
</dsyntnode> </dsyntnode>
</dsyntnode> </dsynts>
2: DEAGGREGATION <dsynts id=""5"">
<dsyntnode class=""verb"" lexeme=""organize"" mood=""ind"" rel=""II""",3 Personal Narrative Evaluation,[0],[0]
"tense=""past"">
<dsyntnode article=""def"" class=""common_noun"" lexeme=""bird"" number=""pl"" person="""" rel=""I""/> <dsyntnode article=""def"" class=""common_noun"" lexeme=""bird"" number=""pl"" person="""" rel=""II""/> <dsyntnode class=""preposition"" lexeme=""on"" rel=""ATTR"">
<dsyntnode article=""def"" class=""common_noun"" l lexeme=""railing"" number=""sg"" person="""" rel=""II""> <dsyntnode article=""no-art"" class=""common_noun"" lexeme=""deck"" number=""sg"" person="""" rel=""I""/> </dsyntnode>
</dsyntnode> </dsyntnode>
</dsynts>
<dsynts id=""6""> <dsyntnode class=""verb"" lexeme=""want""
mood=""ind"" rel=""II""",3 Personal Narrative Evaluation,[0],[0]
"tense=""past""> <dsyntnode article=""def"" class=""common_noun""
lexeme=""bird"" number=""pl"" person="""" r <dsyntnode class=""verb"" extrapo=""+"" lexeme=""wait"" mode=""inf-to"" mood=""inf-to"" rel=""II"" tense=""inf-to""/>
</dsyntnode> </dsynts>
3:",3 Personal Narrative Evaluation,[0],[0]
AGGREGATION,3 Personal Narrative Evaluation,[0],[0]
"TEXT PLAN <speechplan voice=""Narrator""> <",3 Personal Narrative Evaluation,[0],[0]
"rstplan> <relation name=""contingency_cause"">
<proposition id=""1"" ns=""nucleus""/> <proposition id=""2"" ns=""satellite""/>
</relation>",3 Personal Narrative Evaluation,[0],[0]
"</rstplan> <proposition dialogue_act=""5"" id=""1""/> <proposition dialogue_act=""6"" id=""2""/>
</speechplan>
the original Aesop’s Fables to their generated EST and SCHEHERAZADE reproductions (denoted EST and Sch) (Rishes et al., 2013).",3 Personal Narrative Evaluation,[0],[0]
"These metrics are not ideal for evaluating story quality, especially when generating stylistic variations of the original story.",3 Personal Narrative Evaluation,[0],[0]
"However they allow us to automatically test some aspects of system coverage, so we repeat this evaluation on the blog dataset.
",3 Personal Narrative Evaluation,[0],[0]
"Table 7 presents BLEU and Levenshtein scores for the original 36 Fables and all 100 blog stories, compared to both Sch and EST 1.0.",3 Personal Narrative Evaluation,[0],[0]
"Levenshtein
distance computes the minimum edit distance between two strings, so we compare the entire original story to a generated version.",3 Personal Narrative Evaluation,[0],[0]
A lower score indicates a closer comparison.,3 Personal Narrative Evaluation,[0],[0]
BLEU score computes the overlap between two strings taking word order into consideration: a higher BLEU score indicates a closer match between candidate strings.,3 Personal Narrative Evaluation,[0],[0]
Thus Table 7 provides quantitative evidence that the style of the original blogs is very different from Aesop’s Fables.,3 Personal Narrative Evaluation,[0],[0]
"Neither the EST output nor the Sch output comes close to representing the original textual style (Blogs Original-Sch and OriginalEST).
",3 Personal Narrative Evaluation,[0],[0]
"However we find that EST compares favorably to Sch on the blogs with a relatively low Levenshtein score, and higher BLEU score (Blogs Sch-EST) than the original Fables evaluation (Fables Sch-EST).",3 Personal Narrative Evaluation,[0],[0]
"This indicates that even though the blogs have a diversity of language and style, our translation comes close to the Sch baseline.",3 Personal Narrative Evaluation,[0],[0]
We conduct two experiments on Mechanical Turk to test variations generated with the deaggregation and point of view parameters.,4 Experimental Design and Results,[0],[0]
We compare the variations amongst themselves and to the original sentence in a story.,4 Experimental Design and Results,[0],[0]
"We are also interested in identifying differences among individual stories.
",4 Experimental Design and Results,[0],[0]
"In the first experiment, we show an excerpt from the original story telling and indicate to the participants that “any of the following sentences could come next in the story”.",4 Experimental Design and Results,[0],[0]
"We then list all variations of the following sentence with the “in order to” contingency relationship (examples from the Startled Squirrel labeled EST 2.0 in Table 5).
",4 Experimental Design and Results,[0],[0]
"Our aim is to elicit rating of the variations in terms of correctness and goodness of fit within the story context (1 is best, 5 is worst), and to rank the sentences by personal preference (in experiment 1 we showed 7 variations where 1 is best, 7 is worst; in experiment 2 we showed 3 variations where 1 is best, 3 is worst).",4 Experimental Design and Results,[0],[0]
"We also show
the original blog sentence and the EST 1.0 output before de-aggregation and sentence planning.",4 Experimental Design and Results,[0],[0]
"We emphasize that the readers should read each variation in the context of the entire story and encourage them to reread the story with each new sentence to understand this context.
",4 Experimental Design and Results,[0],[0]
"In the second experiment, we compare the original sentence with our best realization, and the realization produced by SCHEHEREZADE (Sch).",4 Experimental Design and Results,[0],[0]
"We expect that SCHEHEREZADE will score more poorly in this instance because it cannot change point of view from third person to first person, even though its output is more fluent than EST 2.0 for many cases.",4 Experimental Design and Results,[0],[0]
We had 7 participants analyze each of the 16 story segments.,4.1 Results Experiment 1,[0],[0]
All participants were native English speakers.,4.1 Results Experiment 1,[0],[0]
Table 8 shows the means and standard deviations for correctness and preference rankings in the first experiment.,4.1 Results Experiment 1,[0],[0]
"We find that averaged across all stories, there is a clear order for correctness and preference: original, soSN, becauseNS, becauseSN, NS, EST, N.
We performed an ANOVA on preference and found that story has no significant effect on the results (F(1, 15) = 0.18, p = 1.00), indicating that all stories are well-formed and there are no outliers in the story selection.",4.1 Results Experiment 1,[0],[0]
"On the other hand, realization does have a significant effect on preference (F(1, 6) = 33.74, p = 0.00).",4.1 Results Experiment 1,[0],[0]
"This supports our hypothesis that the realizations are distinct from each other and there are preferences amongst them.
",4.1 Results Experiment 1,[0],[0]
Fig. 3 shows the average correctness and preference for all stories.,4.1 Results Experiment 1,[0],[0]
"Paired t-tests show that there is a significant difference in reported correctness between orig and soSN (p < 0.05), but no difference between soSN and becauseNS (p = 0.133), or becauseSN (p = 0.08).",4.1 Results Experiment 1,[0],[0]
"There is a difference between soSN and NS (p < 0.005), as well as between the two different because operations and NS (p < 0.05).",4.1 Results Experiment 1,[0],[0]
"There are no other significant differences.
",4.1 Results Experiment 1,[0],[0]
The are larger differences on the preference metric.,4.1 Results Experiment 1,[0],[0]
Paired t-tests show that there is a significant difference between orig and soSN (p < 0.0001) and soSN and becauseNS (p < 0.05).,4.1 Results Experiment 1,[0],[0]
There is no difference in preference between becauseNS and becauseSN (p = 0.31).,4.1 Results Experiment 1,[0],[0]
However there is a significant difference between soSN and becauseSN (p < 0.005) and becauseNS and NS (p < 0.0001).,4.1 Results Experiment 1,[0],[0]
"Finally, there is significant difference between becauseSN and NS (p < 0.005) and NS and EST (p < 0.005).",4.1 Results Experiment 1,[0],[0]
"There is no difference between EST and N (p = 0.375), but there is a difference between NS and N (p < 0.05).
",4.1 Results Experiment 1,[0],[0]
"These results indicate that the original sentence, as expected, is the most correct and preferred.",4.1 Results Experiment 1,[0],[0]
Qualitative feedback on the original sentence included: “The one I ranked first makes a more interesting story.,4.1 Results Experiment 1,[0],[0]
"Most of the others would be sufficient, but boring.”; “The sentence I ranked first makes more sense in the context of the story.",4.1 Results Experiment 1,[0],[0]
"The others tell you similar info, but do not really fit.”.",4.1 Results Experiment 1,[0],[0]
"Some participants ranked soSN as their preferred variant (although the difference was never statistically significant): “The one I rated the best sounded really natural.”
",4.1 Results Experiment 1,[0],[0]
"Although we observe an overall ranking trend, there are some differences by story for NS and N. Most of the time, these two are ranked the lowest.",4.1 Results Experiment 1,[0],[0]
Some subjects observe: “#1 [orig] & #2 [soSN] had a lot of detail.,4.1 Results Experiment 1,[0],[0]
"#7 [N] did not explain what the person wanted to see” (a044 in Table 10); “The sentence I rated the worst [N] didn’t explain why the person wanted to cook them, but it would have been an okay sentence.”",4.1 Results Experiment 1,[0],[0]
(a060 in Table 10); “I ranked the lower number [N] because they either did not contain the full thought of the subject or they added details that are to be assumed.”,4.1 Results Experiment 1,[0],[0]
(a044 in Table 10); “They were all fairly good sentences.,4.1 Results Experiment 1,[0],[0]
The one I ranked worst [N] just left out why they decided to use facebook.”,4.1 Results Experiment 1,[0],[0]
"(a042 in Table 10).
",4.1 Results Experiment 1,[0],[0]
"However, there is some support for NS and N. We also find that there is a significant interaction between story and realization (F(2, 89) = 1.70, p = 0.00), thus subjects’ preference of the realization are based on the story they are reading.",4.1 Results Experiment 1,[0],[0]
One subject commented: “#1 [orig] was the most descriptive about what family the person is looking for.,4.1 Results Experiment 1,[0],[0]
I did like the way #3 [NS] was two sentences.,4.1 Results Experiment 1,[0],[0]
It seemed to put a different emphasis on finding family” (a042 in Table 10).,4.1 Results Experiment 1,[0],[0]
"Another thought that the explanatory utterance altered the tone of the story: “The parent and the children in the story
were having a good time.",4.1 Results Experiment 1,[0],[0]
It doesn’t make sense that parent would want to do something to annoy them [the satellite utterance]” (a060 in Table 10).,4.1 Results Experiment 1,[0],[0]
"This person preferred leaving off the satellite and ranked N as the highest preference.
",4.1 Results Experiment 1,[0],[0]
We examined these interactions between story and preference ranking for NS and N.,4.1 Results Experiment 1,[0],[0]
This may be depend on either context or on the SIG annotations.,4.1 Results Experiment 1,[0],[0]
"For example, in one story (protest in Table 10) our best realization soSN, produces: “The protesters wanted to block the street, so the person said for the protesters to protest in the street in order to block it.”",4.1 Results Experiment 1,[0],[0]
and N produces “The person said for the protesters to protest in the street in order to block it.”.,4.1 Results Experiment 1,[0],[0]
"One subject, who ranked N second only to original, observed: “Since the police were coming there with tear gas, it appears the protesters had already shut things down.",4.1 Results Experiment 1,[0],[0]
There is no need to tell them to block the street.”,4.1 Results Experiment 1,[0],[0]
Another subject who ranked N as second preference similarly observed “Frankly using the word protesters and protest too many times made it seem like a word puzzle or riddle.,4.1 Results Experiment 1,[0],[0]
The meaning was lost in too many variations of the word ‘protest.’,4.1 Results Experiment 1,[0],[0]
"If the wording was awkward, I tried to assign it toward the ‘worst’ end of the scale.",4.1 Results Experiment 1,[0],[0]
"If it seemed to flow more naturally, as a story would, I tried to assign it toward the ‘best’ end.”
",4.1 Results Experiment 1,[0],[0]
"Although the means in this story seem very distinct (Table 8), there is only a significant difference between orig and N (p < 0.005) and N and EST (p < 0.05).",4.1 Results Experiment 1,[0],[0]
Table 8 also includes the means for story a042 (Table 10) where NS is ranked highest for preference.,4.1 Results Experiment 1,[0],[0]
"Despite this, the only significant difference between NS is with EST 1.0 (p < 0.05).",4.1 Results Experiment 1,[0],[0]
"Experiment 2 compares our best realization to the SCHEHERAZADE realizer, exploiting the ability of EST 2.0 to change the point of view.",4.2 Results Experiment 2,[0],[0]
Seven participants analyzed each of the 16 story segments.,4.2 Results Experiment 2,[0],[0]
"All participants were native English speakers.
",4.2 Results Experiment 2,[0],[0]
Table 9 shows the means for correctness and preference rankings.,4.2 Results Experiment 2,[0],[0]
Figure 4 shows a histogram of average correctness and preference by realization for all stories.,4.2 Results Experiment 2,[0],[0]
"There is a clear order for correctness and preference: original, soSN, Sch, with significant differences between all pairs of realizations (p < 0.0001).
",4.2 Results Experiment 2,[0],[0]
"However, in six of the 19 stories, there is no significant difference between Sch and soSN.",4.2 Results Experiment 2,[0],[0]
Three of them do not contain “I” or “the narrator” in the realization sentence.,4.2 Results Experiment 2,[0],[0]
Many of the subjects comment that the realization with “the narrator” does not follow the style of the story: “The second [Sch] uses that awful ‘narrator.”’,4.2 Results Experiment 2,[0],[0]
(a001 in Table 10); “Forget the narrator sentence.,4.2 Results Experiment 2,[0],[0]
From here on out it’s always the worst!”,4.2 Results Experiment 2,[0],[0]
(a001 in Table 10).,4.2 Results Experiment 2,[0],[0]
"We hypothesize that in the three sentences without “the narrator”, Sch can be properly evaluated without the “narrator” bias.",4.2 Results Experiment 2,[0],[0]
"In fact, in these situations, Sch was rated higher than soSN: “I chose
the sentences in order of best explanatory detail” (Startled Squirrel in Table 5).
",4.2 Results Experiment 2,[0],[0]
"Compare the soSN realization in the protest story in Table 10 “The leaders wanted to talk, so they met near the workplace.”",4.2 Results Experiment 2,[0],[0]
with Sch “The group of leaders was meeting in order to talk about running a group of countries and near a workplace.”,4.2 Results Experiment 2,[0],[0]
Sch has so much more detail than soSN.,4.2 Results Experiment 2,[0],[0]
"While the EST has massively improved and overall is preferred to Sch, some semantic components are lost in the translation process.",4.2 Results Experiment 2,[0],[0]
"To our knowledge, this is the first time that sentence planning variations for story telling have been implemented in a framework where the discourse (telling) is completely independent of the fabula (content) of the story (Lonneker, 2005).",5 Discussion and Conclusions,[0],[0]
"We also show for the first time that the SCHEHEREZADE annotation tool can be applied to informal narratives such as personal narratives from weblogs, and the resulting SIG representations work with existing tools for translating from the SIG to a retelling of a story.
",5 Discussion and Conclusions,[0],[0]
"We present a parameterized sentence planner for story generation, that provides aggregation operations and variations in point of view.",5 Discussion and Conclusions,[0],[0]
"The technical aspects of de-aggregation and aggregation builds on previous work in NLG and our earlier work on SPaRKy (Cahill et al., 2001; Scott and de Souza, 1990; Paris and Scott, 1994; Nakatsu and White, 2010; Howcroft et al., 2013; Walker et al., 2007; Stent and Molina, 2009).",5 Discussion and Conclusions,[0],[0]
"However we are not aware of previous NLG applications needing to first de-aggregate the content, before applying aggregation operations.
",5 Discussion and Conclusions,[0],[0]
"Our experiments show that, as expected, readers almost always prefer the original sentence over automatically produced variations, but that the soSN variant is preferred.",5 Discussion and Conclusions,[0],[0]
We examine two specific stories where preferences vary from the overall trend: these stories suggest future possible experiments where we might vary more aspects of the story context and audience.,5 Discussion and Conclusions,[0],[0]
We also compare our best variation to what SCHEHERAZADE produces.,5 Discussion and Conclusions,[0],[0]
"Despite the fact that the SCHEHERAZADE realizer was targeted at the SIG, our best variant is most often ranked as a preferred choice.
",5 Discussion and Conclusions,[0],[0]
"In future work, we aim to explore interactions between a number of our novel narratological parameters.",5 Discussion and Conclusions,[0],[0]
"We expect to do this both with a rule-based approach, as well as by building on recent work on statistical models for expressive generation (Rieser and Lemon, 2011; Paiva and
Evans, 2004; Langkilde, 1998; Rowe et al., 2008; Mairesse and Walker, 2011).",5 Discussion and Conclusions,[0],[0]
"This should allow us to train a narrative generator to achieve particular narrative effects, such as engagement or empathy with particular characters.",5 Discussion and Conclusions,[0],[0]
"We will also expand the discourse relations that EST 2.0 can handle.
Acknowledgements.",5 Discussion and Conclusions,[0],[0]
"This research was supported by Nuance Foundation Grant SC-14-74, NSF Grants IIS-HCC-1115742 and IIS-1002921.
Appendix.",5 Discussion and Conclusions,[0],[0]
"Table 10 provides additional examples of the output of the EST 2.0 system, illustrating particular user preferences and system strengths and weaknesses.",5 Discussion and Conclusions,[0],[0]
There has been a recent explosion in applications for dialogue interaction ranging from direction-giving and tourist information to interactive story systems.,abstractText,[0],[0]
Yet the natural language generation (NLG) component for many of these systems remains largely handcrafted.,abstractText,[0],[0]
This limitation greatly restricts the range of applications; it also means that it is impossible to take advantage of recent work in expressive and statistical language generation that can dynamically and automatically produce a large number of variations of given content.,abstractText,[0],[0]
We propose that a solution to this problem lies in new methods for developing language generation resources.,abstractText,[0],[0]
"We describe the ES-TRANSLATOR, a computational language generator that has previously been applied only to fables, and quantitatively evaluate the domain independence of the EST by applying it to personal narratives from weblogs.",abstractText,[0],[0]
"We then take advantage of recent work on language generation to create a parameterized sentence planner for story generation that provides aggregation operations, variations in discourse and in point of view.",abstractText,[0],[0]
"Finally, we present a user evaluation of different personal narrative retellings.",abstractText,[0],[0]
Generating Sentence Planning Variations for Story Telling,title,[0],[0]
"The ability to generate sentences is core to many NLP tasks, including machine translation, summarization, speech recognition, and dialogue.",1 Introduction,[0],[0]
"Most neural models for these tasks are based on recurrent neural language models (NLMs), which generate sentences from scratch, often in a left-to-right manner (Bengio et al., 2003).",1 Introduction,[0],[0]
"It is often observed that such NLMs suffer from the problem of favoring generic utterances such as “I don’t know” (Li et al., 2016).",1 Introduction,[0],[0]
"At the same time, naive strategies to increase diversity have been shown to compromise grammaticality (Shao et al., 2017), suggesting that current NLMs may lack the inductive bias to faithfully represent the full diversity of complex utterances.
",1 Introduction,[0],[0]
"Indeed, it is difficult even for humans to write complex text from scratch in a single pass; we often create an initial draft and incrementally revise it (Hayes and Flower, 1986).",1 Introduction,[0],[0]
"Inspired by this process,
we propose a new unconditional generative model of text which we call the prototype-then-edit model, illustrated in Figure 1.",1 Introduction,[0],[0]
"It first samples a random prototype sentence from the training corpus, and then invokes a neural editor, which draws a random “edit vector” and generates a new sentence by attending to the prototype while conditioning on the edit vector.",1 Introduction,[0],[0]
"The motivation is that sentences from the corpus provide a high quality starting point: they are grammatical, naturally diverse, and exhibit no bias towards shortness or vagueness.",1 Introduction,[0],[0]
"The attention mechanism (Bahdanau et al., 2015) of the neural editor strongly biases the generation towards the prototype, and therefore it needs to solve a much easier problem than generating from scratch.
",1 Introduction,[0],[0]
We train the neural editor by maximizing an approximation to the generative model’s loglikelihood.,1 Introduction,[0],[0]
"This objective is a sum over lexically-
ar X
iv :1
70 9.
",1 Introduction,[0],[0]
"08 87
8v 2
[ cs
.C",1 Introduction,[0],[0]
"L
] 7
S ep
similar sentence pairs in the training set, which we can scalably approximate using locality sensitive hashing.",1 Introduction,[0],[0]
"We also show empirically that most lexically similar sentences are also semantically similar, thereby endowing the neural editor with additional semantic structure.",1 Introduction,[0],[0]
"For example, we can use the neural editor to perform a random walk from a seed sentence to traverse semantic space.
",1 Introduction,[0],[0]
We compare our prototype-then-edit model to approaches that generate from scratch on both language generation quality and semantic properties.,1 Introduction,[0],[0]
"For the former, our model generates higher quality generations according to human evaluations, and improves perplexity by 13 points on the Yelp corpus and 7 points on the One Billion Word Benchmark.",1 Introduction,[0],[0]
"For the latter, we show that latent edit vectors outperform standard sentence variational autoencoders (Bowman et al., 2016) on semantic similarity, locally-controlled text generation, and a sentence analogy task.",1 Introduction,[0],[0]
Our primary goal is to learn a generative model of sentences for use as a language model.1,2 Problem statement,[0],[0]
"In particular, we model sentence generation as a prototypethen-edit process:
1.",2 Problem statement,[0],[0]
Select prototype:,2 Problem statement,[0],[0]
"Given a training corpus of sentencesX , randomly sample a prototype sentence x′ from a prototype distribution p(x′) (in our case, uniform over X ).
2.",2 Problem statement,[0],[0]
Edit: Sample an edit vector z,2 Problem statement,[0],[0]
(encoding the type of edit to be performed) from an edit prior p(z).,2 Problem statement,[0],[0]
"Then, feed the edit vector z and the previously selected prototype x′ into a neural editor pedit(x | x′, z), which generates a new sentence x.
Under this model, the likelihood of a sentence is: p(x) = ∑ x′∈X p(x | x′)p(x′)",2 Problem statement,[0],[0]
"(1)
p(x | x′) = Ez∼p(z) [ pedit(x | x′, z) ] , (2)
1",2 Problem statement,[0],[0]
"For many applications such as machine translation or dialogue generation, there is a context (e.g. foreign sentence, dialogue history), which can be supplied to both the prototype selector and the neural editor.",2 Problem statement,[0],[0]
"This paper focuses on the unconditional case, proposing an alternative to LSTM based language models.
where both prototype x′ and edit vector z are latent variables.
",2 Problem statement,[0],[0]
Our formulation stems from the observation that many sentences in a large corpus can be represented as minor transformations of other sentences.,2 Problem statement,[0],[0]
"For example, in the Yelp restaurant review corpus (Yelp, 2017)",2 Problem statement,[0],[0]
"we find that 70% of the test set is within wordtoken Jaccard distance 0.5 of a training set sentence, even though almost no sentences are repeated verbatim.",2 Problem statement,[0],[0]
"This implies that a neural editor which models lexically similar sentences should be an effective generative model for large parts of the test set.
",2 Problem statement,[0],[0]
"A secondary goal for the neural editor is to capture certain semantic properties; we focus on the following two in particular:
1.",2 Problem statement,[0],[0]
"Semantic smoothness: an edit should be able to alter the semantics of a sentence by a small and well-controlled amount, while multiple edits should make it possible to accumulate a larger change.
",2 Problem statement,[0],[0]
2.,2 Problem statement,[0],[0]
Consistent edit behavior: the edit vector z should model/control the variation in the type of edit that is performed.,2 Problem statement,[0],[0]
"When we apply the same edit vector on different sentences, the neural editor should perform semantically analogous edits across the sentences.
",2 Problem statement,[0],[0]
"In Section 4, we show that the neural editor can successfully capture both properties, as reported by human evaluations.",2 Problem statement,[0],[0]
"We would like to train our neural editor pedit(x | x′, z) by maximizing the marginal likelihood (Equation 1) via gradient ascent, but the objective cannot be computed exactly because it involves a sum over all prototypes x′ (expensive) and an expectation over the edit prior p(z) (no closed form).
",3 Approach,[0],[0]
"We therefore propose two approximations to overcome these challenges:
1.",3 Approach,[0],[0]
"We lower bound the sum over latent prototypes x′ (in Equation 1) by only summing over x′ that are lexically similar to x.
2.",3 Approach,[0],[0]
We lower bound the expectation over the edit prior (in Equation 2) using the evidence lower bound (ELBO),3 Approach,[0],[0]
"(Jordan et al., 1999; Doersch, 2016) which can be effectively approximated.
",3 Approach,[0],[0]
"We describe and motivate these approximations in Sections 3.1 and 3.2, respectively.",3 Approach,[0],[0]
"In Section 3.3, we combine the two approximations to give the final objective.",3 Approach,[0],[0]
Sections 3.4 and 3.5 drill down further into our specific model architecture.,3 Approach,[0],[0]
Equation 1 defines the probability of generating a sentence x as the total probability of reaching x via edits from every prototype x′ ∈ X .,"3.1 Approximate sum on prototypes, x′",[0],[0]
"However, most prototypes are unrelated and should have very small probability of transforming into x. Therefore, we approximate the summation over prototypes by only considering prototypes x′ that have high lexical overlap with x. To that end, define a lexical similarity neighborhood as:
N (x) def= {","3.1 Approximate sum on prototypes, x′",[0],[0]
"x′ ∈ X : dJ(x, x′) < 0.5},
where dJ(x, x′) is the Jaccard distance between x and x′ (treating each as a set of word tokens).
","3.1 Approximate sum on prototypes, x′",[0],[0]
"We will now lower bound log p(x) in two ways: (i) we will sum over only prototypes in the neighborhood N (x) rather than over the entire training set X as discussed above; (ii) we will push the log inside the summation using Jensen’s inequality, as is standard with variational lower bounds.","3.1 Approximate sum on prototypes, x′",[0],[0]
"Recall that the distribution over prototypes is uniform (p(x′) = 1/|X |), and define R(x) = log(|N (x)|/|X |).","3.1 Approximate sum on prototypes, x′",[0],[0]
"The derivation is as follows:
log p(x) = log [∑ x′∈X p(x | x′)p(x′) ]","3.1 Approximate sum on prototypes, x′",[0],[0]
"(i)
≥ log  ∑ x′∈N (x) p(x | x′)p(x′)  ","3.1 Approximate sum on prototypes, x′",[0],[0]
"(3) = log
|N (x)|−1 ∑ x′∈N (x) p(x | x′)","3.1 Approximate sum on prototypes, x′",[0],[0]
"+R(x) (ii)
≥ |N (x)|−1 ∑
x′∈N (x)
log p(x | x′)
︸ ︷︷ ︸ def = LEX(x)
+R(x).
","3.1 Approximate sum on prototypes, x′",[0],[0]
"Assuming the neighborhood size |N (x)| is constant across x, then LEX(x) is a lower bound of log p(x) up to constants.","3.1 Approximate sum on prototypes, x′",[0],[0]
"For each x, the neighborhood N (x) can be efficiently precomputed with locality sensitive hashing (LSH) and minhashing.","3.1 Approximate sum on prototypes, x′",[0],[0]
"The full procedure is described in Appendix 6.
","3.1 Approximate sum on prototypes, x′",[0],[0]
Note that LEX(x) is still intractable to compute because each log p(x|x′) term involves an expectation over the edit prior p(z) (Equation 2).,"3.1 Approximate sum on prototypes, x′",[0],[0]
"We address this in Section 3.2, but first, an interlude.
","3.1 Approximate sum on prototypes, x′",[0],[0]
Interlude: lexical similarity semantics.,"3.1 Approximate sum on prototypes, x′",[0],[0]
"So far, we have motivated lexical similarity neighborhoods via computational considerations, but we found that lexical similarity training also captures semantic similarity.","3.1 Approximate sum on prototypes, x′",[0],[0]
"One can certainly construct sentences with small lexical distance that differ semantically (e.g., insertion of the word “not”).","3.1 Approximate sum on prototypes, x′",[0],[0]
"However, since we mine sentences from a corpus grounded in real world events, most lexically similar sentences are also semantically similar.","3.1 Approximate sum on prototypes, x′",[0],[0]
"For example, given “my son enjoyed the delicious pizza”, we are far more likely to see “my son enjoyed the delicious macaroni”, versus “my son hated the delicious pizza”.
","3.1 Approximate sum on prototypes, x′",[0],[0]
Human evaluations of 250 edit pairs sampled from lexical similarity neighborhoods on the Yelp corpus support this conclusion.,"3.1 Approximate sum on prototypes, x′",[0],[0]
"35.2% of the sentence pairs were judged to be exact paraphrases, while 84% of the pairs were judged to be at least roughly equivalent.","3.1 Approximate sum on prototypes, x′",[0],[0]
Sentence pairs were negated or change in topic only 7.2% of the time.,"3.1 Approximate sum on prototypes, x′",[0],[0]
"Thus, a neural editor trained on this distribution should preferentially generate semantically similar edits.
","3.1 Approximate sum on prototypes, x′",[0],[0]
Note that semantic similarity is not needed if we are only interested in modeling the distribution p(x).,"3.1 Approximate sum on prototypes, x′",[0],[0]
"But it does enable us to learn an edit model p(x|x′) that prefers semantically meaningful edits, which we explore in Section 4.3.","3.1 Approximate sum on prototypes, x′",[0],[0]
"In Section 3.1, we approximated the marginal likelihood log p(x) by LEX(x), which is a summation over terms of the form:
log p(x | x′) = logEz∼p(z)","3.2 Approximate expectation on edit vectors, z",[0],[0]
"[ pedit(x | x′, z) ] .","3.2 Approximate expectation on edit vectors, z",[0],[0]
"(4)
Unfortunately the expectation over p(z) has no closed form, and naively approximating it by Monte Carlo sampling z ∼ p(z) will have unacceptably high variance, because pedit(x | x′, z) will be almost zero for nearly all z sampled from p(z), while being large for a few important but rare values.
","3.2 Approximate expectation on edit vectors, z",[0],[0]
"To address this, we introduce an inverse neural editor q(z | x′, x): given a prototype x′ and a revised sentence x, it generates edit vectors that are likely to
map x′ to x, concentrating probability on the few rare but important values of z.
We can then use the evidence lower bound (ELBO) to lower bound Equation 4:
log p(x|x′) ≥ Ez∼q(z|x′,x) [ log pedit(x | x′, z) ]︸ ︷︷ ︸","3.2 Approximate expectation on edit vectors, z",[0],[0]
"Lgen
− KL(q(z | x′, x) ‖","3.2 Approximate expectation on edit vectors, z",[0],[0]
"p(z))︸ ︷︷ ︸ LKL
def = ELBO(x, x′).
","3.2 Approximate expectation on edit vectors, z",[0],[0]
"Since Lgen is an expectation over q(z | x′, x) instead of p(x), it can be effectively Monte Carlo estimated by sampling z ∼ q(z | x′, x).","3.2 Approximate expectation on edit vectors, z",[0],[0]
"The second term, LKL, penalizes the difference between q(z | x′, x) and p(x), which is necessary for the lower bound to hold.","3.2 Approximate expectation on edit vectors, z",[0],[0]
"A thorough introduction to the ELBO is provided in Doersch (2016).
","3.2 Approximate expectation on edit vectors, z",[0],[0]
"Note that q(z | x′, x) and pedit(x | x′, z) combine to form a variational autoencoder (VAE) (Kingma and Welling, 2014), where q(z | x′, x) is the variational encoder and pedit(x | x′, z) is the variational decoder.","3.2 Approximate expectation on edit vectors, z",[0],[0]
"Combining the lower bounds LEX(x) and ELBO(x, x′), our final approximation of the log-likelihood is∑
x′∈N (x)
ELBO(x, x′).
",3.3 Final objective,[0],[0]
We optimize this objective using stochastic gradient ascent with respect to Θ =,3.3 Final objective,[0],[0]
"(Θp,Θq), where Θp are the parameters for the neural editor and Θq are the parameters for the inverse neural editor.",3.3 Final objective,[0],[0]
"To recap, our model features three components: the neural editor pedit(x | x′, z), the edit prior p(z), and the inverse neural editor q(z | x′, x).",3.4 Model architecture,[0],[0]
"We detail each of these components below.
",3.4 Model architecture,[0],[0]
"Neural editor pedit(x | x′, z).",3.4 Model architecture,[0],[0]
"We implement our neural editor as a left-to-right sequence-to-sequence model with attention, where the prototype x′ is the
input sequence and the revised sentence x is the output sequence.",3.4 Model architecture,[0],[0]
"We employ an encoder-decoder architecture similar to Wu (2016), extending it to condition on an edit vector z by concatenating z to the input of the decoder at each time step.
",3.4 Model architecture,[0],[0]
The prototype encoder is a 3-layer bidirectional LSTM.,3.4 Model architecture,[0],[0]
"The inputs to each layer are the concatenation of the forward and backward hidden states of the previous layer, with the exception of the first layer, which takes word vectors initialized using GloVe (Pennington et al., 2014).
",3.4 Model architecture,[0],[0]
The decoder is a 3-layer LSTM with attention.,3.4 Model architecture,[0],[0]
"At each time step, the hidden state of the top layer is used to compute attention over the top-layer hidden states of the prototype encoder.",3.4 Model architecture,[0],[0]
"The resulting attention context vector is then concatenated with the decoder’s top-layer hidden state and used to compute a softmax distribution over output tokens.
",3.4 Model architecture,[0],[0]
Edit prior p(z).,3.4 Model architecture,[0],[0]
"We sample the edit vector z from the prior by first sampling its scalar length znorm ∼ Unif(0, 10) and then sampling its direction zdir (a unit vector) from the uniform distribution on the unit sphere.",3.4 Model architecture,[0],[0]
The resulting z = znorm ·zdir.,3.4 Model architecture,[0],[0]
"As we will see later, this particular choice of the prior enables us to easily compute LKL.
",3.4 Model architecture,[0],[0]
"Inverse neural editor q(z | x′, x).",3.4 Model architecture,[0],[0]
"Given an edit pair (x′, x), the inverse neural editor must infer what vectors z are likely to map x′ to x.
Suppose that x′ and x only differed by a single word w.",3.4 Model architecture,[0],[0]
"Then one might propose that the edit vector z should be equal to the word vector for w. Generalizing this intuition to multi-word edits, we would like multi-word insertions to be represented as the sum of the inserted word vectors, and similarly for deletions.
",3.4 Model architecture,[0],[0]
"Formally, define I = x\x′ to be the set of words added to x′, and D = x′\x to be the words deleted.",3.4 Model architecture,[0],[0]
"We represent the difference between x′ and x using the following vector:
",3.4 Model architecture,[0],[0]
"f ( x, x′ ) =",3.4 Model architecture,[0],[0]
∑ w∈I,3.4 Model architecture,[0],[0]
Φ (w)⊕ ∑ w∈D,3.4 Model architecture,[0],[0]
"Φ (w)
where Φ(w) is the word vector for word w and ⊕ denotes concatenation.",3.4 Model architecture,[0],[0]
The word embeddings Φ are parameters of q.,3.4 Model architecture,[0],[0]
"In our work, we initialize Φ(w) to be 300-dimensional GloVe vectors.
",3.4 Model architecture,[0],[0]
"Since we construct our edit vectors as the sum of word vectors, and similarities between word vectors have traditionally been measured with cosine similarity, we design q to add noise to perturb the direction of the vector f .",3.4 Model architecture,[0],[0]
"In particular, a sample from q is simply a perturbed version of f : obtained by adding von-Mises Fisher (vMF) noise, and we perturb the magnitude of f by adding uniform noise.",3.4 Model architecture,[0],[0]
"We visualize this perturbation process in Figure 2.
",3.4 Model architecture,[0],[0]
"Formally, let fnorm = ‖f‖ and fdir = f/fnorm.",3.4 Model architecture,[0],[0]
"Let vMF (v;µ, κ) denote a vMF distribution over points v on the unit sphere (i.e., directions) with mean vector µ and concentration parameter κ (in such a distribution, the log-likelihood of a point decays linearly with its cosine similarity to µ, and the rate of decay is controlled by κ).",3.4 Model architecture,[0],[0]
"Finally, define:
q(zdir | x′, x) = vMF (zdir; fdir, κ) q(znorm | x′, x) = Unif(znorm; [f̃norm, f̃norm + ])
where f̃norm = min(fnorm, 10 − ) is the truncated norm.",3.4 Model architecture,[0],[0]
"The resulting edit vector is z = zdir · znorm.
",3.4 Model architecture,[0],[0]
The inverse neural editor q is parameterized by the word vectors Φ and has hyperparameters κ and .,3.4 Model architecture,[0],[0]
Further details are provided in Section 3.5.,3.4 Model architecture,[0],[0]
Differentiating w.r.t.,3.5 Details of the inverse neural editor,[0],[0]
Θq.,3.5 Details of the inverse neural editor,[0],[0]
"To maximize our training objective, we must be able to compute ∇ΘqELBO(x, x′) = ∇ΘqLgen −∇ΘqLKL.
",3.5 Details of the inverse neural editor,[0],[0]
"To compute ∇ΘqLgen, we use a reparameterization trick.",3.5 Details of the inverse neural editor,[0],[0]
"Specifically, we can rewrite z ∼ q(z | x′, x) as z = h(α) where h is a deterministic function differentiable with respect to Θq and α ∼ p(α) is an auxiliary random variable not depending on Θq (the details of h and α are given in Appendix 6).",3.5 Details of the inverse neural editor,[0],[0]
"We
can then write:
∇ΘqLgen = ∇ΘqEz∼q(z|x′,x) [ log pedit(x | x′, z) ] = Eα∼p(α)",3.5 Details of the inverse neural editor,[0],[0]
"[ ∇Θq log pedit(x | x′, h(α)) ] .
",3.5 Details of the inverse neural editor,[0],[0]
This moves the derivative inside the expectation.,3.5 Details of the inverse neural editor,[0],[0]
"The inner derivative can now be computed via standard backpropagation.
",3.5 Details of the inverse neural editor,[0],[0]
"Next, we turn to∇ΘqLKL.",3.5 Details of the inverse neural editor,[0],[0]
"First, note that:
LKL = KL(q(znorm|x′, x)‖p(znorm))",3.5 Details of the inverse neural editor,[0],[0]
"+ KL(q(zdir|x′, x)‖p(zdir))",3.5 Details of the inverse neural editor,[0],[0]
.,3.5 Details of the inverse neural editor,[0],[0]
"(5)
It is easy to verify that the first KL term does not depend on Θq.",3.5 Details of the inverse neural editor,[0],[0]
"The second term has the closed form
KL(vMF(µ, κ)‖vMF(µ, 0))",3.5 Details of the inverse neural editor,[0],[0]
=,3.5 Details of the inverse neural editor,[0],[0]
κ Id/2(κ),3.5 Details of the inverse neural editor,[0],[0]
"+ Id/2−1(κ)
",3.5 Details of the inverse neural editor,[0],[0]
"d−2 2κ
Id/2−1(κ)− d−22κ",3.5 Details of the inverse neural editor,[0],[0]
− log(Id/2−1(κ))− log(Γ(d/2)),3.5 Details of the inverse neural editor,[0],[0]
"+ log(κ)(d/2− 1)− (d− 2) log(2)/2, (6)
where In(κ) is the modified Bessel function of the first kind, Γ is the gamma function, and d is the dimensionality of f .",3.5 Details of the inverse neural editor,[0],[0]
"We can see that this too is constant with respect to Θq via the following intuition: both the KL divergence and the prior do not change under rotations, and thus we can see KL(vMF(µ, κ)‖vMF(µ, 0)))",3.5 Details of the inverse neural editor,[0],[0]
"= KL(vMF(e1, κ)‖vMF(e1, 0)))",3.5 Details of the inverse neural editor,[0],[0]
by rotating µ to the first canonical basis vector.,3.5 Details of the inverse neural editor,[0],[0]
"Hence∇ΘqLKL = 0.
",3.5 Details of the inverse neural editor,[0],[0]
Comparison with existing VAE encoders.,3.5 Details of the inverse neural editor,[0],[0]
"Our design of q differs from the typical choice of a standard normal distribution (Bowman et al., 2016; Kingma and Welling, 2014) for two reasons:
First, by construction, edit vectors are sums of word vectors and since cosine distances are traditionally used to measure distances between word vectors, it would be natural to encode distances between edit vectors by the cosine distance.",3.5 Details of the inverse neural editor,[0],[0]
"The vonMises Fisher distribution captures this idea, as the log likelihood decays with cosine similarity.
",3.5 Details of the inverse neural editor,[0],[0]
"Second, our design of q allows us to explicitly control the tradeoff between the two terms in our objective, Lgen and LKL.",3.5 Details of the inverse neural editor,[0],[0]
"Note from equations 5 and 6 that LKL is purely a function of the hyperparameters and κ, and can thus be controlled exactly.",3.5 Details of the inverse neural editor,[0],[0]
"By taking κ→ 0 and to the maximum norm, we can drive
LKL arbitrarily close to 0.",3.5 Details of the inverse neural editor,[0],[0]
"As a tradeoff, smaller values of κ produce a noisier edit vector, leading to a smaller Lgen.",3.5 Details of the inverse neural editor,[0],[0]
"We find a good balance by tuning κ.
",3.5 Details of the inverse neural editor,[0],[0]
"In contrast, when using a Gaussian variational encoder, the KL term takes a different value per example and cannot be explicitly controlled.",3.5 Details of the inverse neural editor,[0],[0]
"Consequently, Bowman et al. (2016) and others have observed that training tends to aggressively drive these KL terms to zero, leading to uninformative values of z — even when multiplying LKL by a carefully tuned and annealed importance weight.",3.5 Details of the inverse neural editor,[0],[0]
We divide our experimental results into two parts.,4 Experiments,[0],[0]
"In Section 4.2, we evaluate the merits of the prototypethen-edit model as a generative modeling strategy, measuring its improvements on language modeling (perplexity) and generation quality (human evaluations of diversity and plausibility).",4 Experiments,[0],[0]
"In Section 4.3, we focus on the semantics learned by the model and its latent edit vector space.",4 Experiments,[0],[0]
"We demonstrate that it possesses interpretable semantics, enabling us to smoothly control the magnitude of edits, incrementally optimize sentences for target properties, and perform analogy-style sentence transformations.",4 Experiments,[0],[0]
"We evaluate perplexity on the Yelp review corpus (YELP, Yelp (2017)) and the One Billion Word Language Model Benchmark (BILLIONWORD, Chelba (2013)).",4.1 Datasets,[0],[0]
"For qualitative evaluations of generation quality and semantics, we focus on YELP as our primary test case, as we found that human judgments of semantic similarity were much better calibrated in this focused setting.
",4.1 Datasets,[0],[0]
"For both corpora, we used the named-entity recognizer (NER) in spaCy2 to replace named entities with their NER categories.",4.1 Datasets,[0],[0]
"We replaced tokens outside the top 10,000 most frequent tokens with an “out-of-vocabulary” token.",4.1 Datasets,[0],[0]
"We compare NEURALEDITOR as a language model against the following baseline language models:
1.",4.2 Generative modeling,[0],[0]
NLM: a standard left-to-right neural language model generating from scratch.,4.2 Generative modeling,[0],[0]
"For fair com-
2honnibal.github.io/spaCy
parison, we use the exact same architecture as the decoder of NEURALEDITOR.
2.",4.2 Generative modeling,[0],[0]
"KN5: a standard 5-gram Kneser-Ney language model in KenLM (Heafield et al., 2013).
3.",4.2 Generative modeling,[0],[0]
"MEMORIZATION: generates by sampling a sentence from the training set.
Perplexity.",4.2 Generative modeling,[0],[0]
"We start by evaluating NEURALEDITOR’s value as a language model, measured in terms of perplexity.",4.2 Generative modeling,[0],[0]
"We use the likelihood lower bound in Equation 3, where we sum over training set instances within Jaccard distance < 0.5, and for the VAE term in NEURALEDITOR, we use the onesample approximation to the lower bound used in Kingma (2014) and Bowman (2016).
",4.2 Generative modeling,[0],[0]
"To evaluate NEURALEDITOR’s perplexity, we use linear smoothing with NLM to account for rare sentences not within our Jaccard distance threshold.",4.2 Generative modeling,[0],[0]
"This smoothing corresponds to occasionally sampling a special prototype sentence that can be edited into any other sentence and we use a smoothing weight of 0.1 (for full details, see Appendix 6).",4.2 Generative modeling,[0],[0]
We find NEURALEDITOR improves perplexity over NLM and KN5.,4.2 Generative modeling,[0],[0]
"Table 1 shows that this is the case for both YELP and the more general BILLIONWORD, which contains substantially fewer test-set sentences close to the training set.",4.2 Generative modeling,[0],[0]
"On YELP, we surpass even the best ensemble of NLM and KN5, while on BILLIONWORD we nearly match their performance.
",4.2 Generative modeling,[0],[0]
"Comparing each model at a per-sentence level, we see that NEURALEDITOR drastically improves loglikelihood for a significant number of sentences in the test set (Figure 3).",4.2 Generative modeling,[0],[0]
"Proximity to a prototype seems to be the chief determiner of NEURALEDITOR’s performance.
",4.2 Generative modeling,[0],[0]
"Since NEURALEDITOR draws its strength from sentences in the training set, we also compared against a simpler alternative, in which we ensemble NLM and MEMORIZATION (retrieval without edits).",4.2 Generative modeling,[0],[0]
NEURALEDITOR performs dramatically better than this alternative.,4.2 Generative modeling,[0],[0]
"Table 2 also qualitatively demonstrates that sentences generated by NEURALEDITOR are substantially different from the original prototypes.
",4.2 Generative modeling,[0],[0]
Human evaluation.,4.2 Generative modeling,[0],[0]
"We now turn to human evaluation of generation quality, focusing on grammaticality and plausibility.",4.2 Generative modeling,[0],[0]
"We evaluated plausibility by asking human raters, “How plausible is it for this sentence to appear in the corpus?” on a scale of 1– 3.",4.2 Generative modeling,[0],[0]
"We evaluate generations from NEURALEDITOR against an NLM with a temperature parameter on the per-token softmax3 as well as a baseline which generates sentences by randomly sampling from the training set and replacing synonyms, where the probability of substitution follows exp(sij/τ), where sij is the cosine similarity between the original word and its synonym according to GloVe word vectors.
",4.2 Generative modeling,[0],[0]
"Decreasing the temperature parameter below 1 is 3 If si is the softmax logit for tokenwi and τ is a temperature parameter, the temperature-adjusted distribution is p(wi) ∝ exp(si/τ).
",4.2 Generative modeling,[0],[0]
a popular technique for suppressing incoherent and ungrammatical sentences.,4.2 Generative modeling,[0],[0]
"Many NLM systems have noted an undesirable tradeoff between grammaticality and diversity, where a temperature low enough to enforce grammaticality results in short and generic utterances (Li et al., 2016).
",4.2 Generative modeling,[0],[0]
"Figure 4 illustrates that both the grammaticality and plausibility of NEURALEDITOR without any temperature annealing is on par with the best tuned temperature for NLM, with a far higher diversity, as measured by the discrete entropy over unigram frequencies.",4.2 Generative modeling,[0],[0]
"We also find that decreasing the temperature of NEURALEDITOR can be used to slightly improve the grammaticality, without substantially reducing the diversity of the generations.
",4.2 Generative modeling,[0],[0]
"Comparing with the synonym substitution model, we find both models have high plausibility, since synonym substitution maintains most of the words, but low grammaticality compared to both NEURALEDITOR and the NLM.",4.2 Generative modeling,[0],[0]
"Additionally, applying synonym substitutions to training examples has extremely low coverage – none of the sentences in the test set can be generated via synonym substitution, and thus this baseline has higher perplexity than all other baselines in Table 1.
",4.2 Generative modeling,[0],[0]
"A key advantage of edit-based models thus emerges: Prototypes sampled from the training set organically inject diversity into the generation process, even if the temperature of the decoder in NEURALEDITOR is zero.",4.2 Generative modeling,[0],[0]
"Hence, we can keep the decoder at a very low temperature to maximize grammaticality and plausibility, without sacrificing diversity.",4.2 Generative modeling,[0],[0]
"In contrast, a zero temperature NLM would collapse to outputting one generic sentence.
",4.2 Generative modeling,[0],[0]
"This also suggests that the temperature parameter for NEURALEDITOR captures a more natural notion of diversity — a temperature of 1.0 encourages more aggressive extrapolation from the training set
while lower temperatures favor more conservative mimicking.",4.2 Generative modeling,[0],[0]
"This is likely to be more useful than the tradeoff for generation-from-scratch, where low temperature also affects the diversity of generations.
",4.2 Generative modeling,[0],[0]
Categorizing edits.,4.2 Generative modeling,[0],[0]
"To better understand the behavior of NEURALEDITOR, we measured the frequency with which random edits from NEURALEDITOR matched known syntactic transformations.
",4.2 Generative modeling,[0],[0]
"We use the rule-based transformations defined in He (2015) as our set of transformations to test, and search the corpus for sentences where these rules can be applied.",4.2 Generative modeling,[0],[0]
"We then apply the rule-based transformation, and measure the log-likelihood that NEURALEDITOR generates the transformed outputs.",4.2 Generative modeling,[0],[0]
"We find that the edit model assigns relatively high probability to the identity map (no edits), followed by simple reordering transformations such as reordering to/that Clauses (It is ADJP to/that SBAR/S → To S/BARS is ADJP).",4.2 Generative modeling,[0],[0]
"Of the rules, active / passive receives the lowest probability, partially due to the rarity of passive voice sentences in the Yelp corpus (Table 3).
",4.2 Generative modeling,[0],[0]
"In all cases, the model assigns substantially higher probability to these rule-based transformations over editing to random sentences or shuffling the tokens randomly to match the Levenstein distance of each rule-based transform.",4.2 Generative modeling,[0],[0]
"In this section, we investigate the learned semantics of NEURALEDITOR, focusing on the two desiderata discussed in Section 2: semantic smoothness, and consistent edit behavior.
",4.3 Semantics of NEURALEDITOR,[0],[0]
"In order to establish a baseline for these properties, we consider existing sentence generation techniques which can sample semantically similar sentences.",4.3 Semantics of NEURALEDITOR,[0],[0]
"The most similar language modeling approach which can capture semantics is the sentence
variational autoencoder (SVAE) which imposes semantic structure onto a latent vector space, but uses the latent vector to represent the entire sentence, rather than just an edit.
",4.3 Semantics of NEURALEDITOR,[0],[0]
"To use the SVAE to “edit” a target sentence into a semantically similar sentence, we perturb its underlying latent sentence vector and then decode the result back into a sentence — the same method used in Bowman et al. (2016).
",4.3 Semantics of NEURALEDITOR,[0],[0]
Semantic smoothness.,4.3 Semantics of NEURALEDITOR,[0],[0]
"A good editing system should have fine-grained control over the semantics of a sentence: i.e., each edit should only alter the semantics of a sentence by a small and well-controlled amount.",4.3 Semantics of NEURALEDITOR,[0],[0]
"We call this property semantic smoothness.
",4.3 Semantics of NEURALEDITOR,[0],[0]
"To study smoothness, we first generate an “edit sequence” by randomly selecting a prototype sentence, and then repeatedly editing via NEURALEDITOR (with edits drawn from the edit prior p(z)) to produce a sequence of revisions.",4.3 Semantics of NEURALEDITOR,[0],[0]
We then ask human annotators to rate the size of the semantic changes between revisions.,4.3 Semantics of NEURALEDITOR,[0],[0]
"An example is given in Table 4.
",4.3 Semantics of NEURALEDITOR,[0],[0]
"We compare to two baselines, one based upon the sentence variational autoencoder (SVAE) and another baseline which simply samples similar sentences from the training set according to average word vector similarity (COSINE).
",4.3 Semantics of NEURALEDITOR,[0],[0]
"For SVAE, we generate a similar sequence of sentences by first encoding the prototype sentence, and then decoding after the addition of a random Gaussian with variance 0.4.4 This process is repeated to produce a sequence of sentences which we can view as the SVAE equivalent of the edit sequence.
",4.3 Semantics of NEURALEDITOR,[0],[0]
"For COSINE, we generate sentences from the training set using exponentiated cosine similarity be-
4The variance was selected so that SVAE and NEURALEDITOR have the same average human similarity judgement between two successive sentences.",4.3 Semantics of NEURALEDITOR,[0],[0]
"This avoids situations where SVAE produces completely unrelated sentence due to the perturbation size.
",4.3 Semantics of NEURALEDITOR,[0],[0]
tween averaged word vectors.,4.3 Semantics of NEURALEDITOR,[0],[0]
"The temperature parameter for the exponential was selected as before to match the average human similarity judgement.
",4.3 Semantics of NEURALEDITOR,[0],[0]
"Figure 5 shows that NEURALEDITOR frequently generates paraphrases despite being trained on lexical similarity, and only 1% of edits are unrelated from the prototype.",4.3 Semantics of NEURALEDITOR,[0],[0]
"In contrast, SVAE often repeats sentences exactly, and when it makes an edit it is equally likely to generate unrelated sentences.",4.3 Semantics of NEURALEDITOR,[0],[0]
"COSINE performs even worse likely due to the difficulty of retrieving similar sentences for rare and long sentences.
",4.3 Semantics of NEURALEDITOR,[0],[0]
"Less Similar By Turk EvaluationDegenerate
Figure 5: Compared with baselines, NEURALEDITOR frequently generates paraphrases and similar sentences while avoiding unrelated and degenerate ones.6
Qualitatively (Table 4), NEURALEDITOR seems
to generate long, diverse sentences which smoothly change over time, while the SVAE biases towards short sentences with several semantic jumps, presumably due to the difficulty of training a sufficiently informative SVAE encoder.
Smoothly controlling sentences.",4.3 Semantics of NEURALEDITOR,[0],[0]
We now show that we can selectively choose edits sampled from NEURALEDITOR to incrementally optimize a sentence towards desired attributes.,4.3 Semantics of NEURALEDITOR,[0],[0]
"This task serves as a useful measure of semantic coverage: if an edit model has high coverage over sentences that are semantically similar to a prototype, it should be able to satisfy the target attribute while deviating minimally from the prototype’s original meaning.
",4.3 Semantics of NEURALEDITOR,[0],[0]
"We focus on controlling two simple attributes: compressing a sentence to below a desired length (e.g., 7 words), and inserting a target keyword into the sentence (e.g., “service” or “pizza”).
",4.3 Semantics of NEURALEDITOR,[0],[0]
"Given a prototype sentence, we try to discover a semantically similar sentence satisfying the target
6 545 similarity assessments pairs were collected through Amazon Mechanical Turk following Agirre (2014), with the same scale and prompt.",4.3 Semantics of NEURALEDITOR,[0],[0]
"Similarity judgements were converted to descriptions by defining Paraphrase (5), Roughly Equivalent (4-3), Same Topic (2-1), Unrelated (0).
attribute using the following procedure: First, we generate 1,000 edit sequences using the procedure described earlier.",4.3 Semantics of NEURALEDITOR,[0],[0]
"Then, we select the sequence with highest likelihood whose endpoint possesses the target attribute.",4.3 Semantics of NEURALEDITOR,[0],[0]
"We repeat this process for a large number of prototypes.
",4.3 Semantics of NEURALEDITOR,[0],[0]
"We use almost the same procedure for the SVAE, but instead of selecting by highest likelihood, we select the sequence whose endpoint has shortest latent vector distance from the prototype (as this is the SVAE’s metric of semantic similarity).
",4.3 Semantics of NEURALEDITOR,[0],[0]
"In Figure 6, we then aggregate the sentences from the collected edit sequences, and plot their semantic similarity to the prototype against their success in satisfying the target attribute.",4.3 Semantics of NEURALEDITOR,[0],[0]
"Not surprisingly, as target attribute satisfaction rises, semantic similarity drops.",4.3 Semantics of NEURALEDITOR,[0],[0]
"However, we also see that NEURALEDITOR sacrifices less semantic similarity to achieve the same level of attribute satisfaction as SVAE.",4.3 Semantics of NEURALEDITOR,[0],[0]
"SVAE is reasonable on tasks involving common words (such as the word service), but fails when the model is asked to generate rarer words such as pizza.",4.3 Semantics of NEURALEDITOR,[0],[0]
"Examples from these word inclusion problems show that SVAE often becomes stuck generating short, generic sentences (Table 5).
",4.3 Semantics of NEURALEDITOR,[0],[0]
Consistent edit behavior: sentence analogies.,4.3 Semantics of NEURALEDITOR,[0],[0]
"In the previous results, we showed that edit models learn to generate semantically similar sentences.",4.3 Semantics of NEURALEDITOR,[0],[0]
"We now assess whether the edit vector possesses glob-
ally consistent semantics.",4.3 Semantics of NEURALEDITOR,[0],[0]
"Specifically, applying the same edit vector to different sentences should result in semantically analogous edits.
",4.3 Semantics of NEURALEDITOR,[0],[0]
"For example, if we have an edit vector which edits the sentence",4.3 Semantics of NEURALEDITOR,[0],[0]
x1 =,4.3 Semantics of NEURALEDITOR,[0],[0]
“this was a good restaurant” into x2 =,4.3 Semantics of NEURALEDITOR,[0],[0]
“this was the best restaurant”.,4.3 Semantics of NEURALEDITOR,[0],[0]
Given a new sentence y1,4.3 Semantics of NEURALEDITOR,[0],[0]
=,4.3 Semantics of NEURALEDITOR,[0],[0]
"“The cake was great”, we expect applying the same edit vector to result in y2 =",4.3 Semantics of NEURALEDITOR,[0],[0]
"“The cake was the greatest”.
",4.3 Semantics of NEURALEDITOR,[0],[0]
"Formally, suppose we have two sentences, x1 and x2, which are related by some underlying semantic relation r.",4.3 Semantics of NEURALEDITOR,[0],[0]
"Given a new sentence y1, we would like to find a y2 such that the same relation r holds between y1 and y2.
",4.3 Semantics of NEURALEDITOR,[0],[0]
"Our approach is to estimate the edit vector between x1 and x2 as ẑ = f(x1, x2) — the mode of the inverse neural editor q.",4.3 Semantics of NEURALEDITOR,[0],[0]
"We then apply this edit vector to y1 using the neural editor to yield ŷ2 = argmaxxpedit(x | y1, ẑ).
",4.3 Semantics of NEURALEDITOR,[0],[0]
"Since it is difficult to output ŷ2 exactly matching y2, we take the top k candidate outputs of pedit (using beam search) and evaluate whether the gold y2 appears among the top k elements.
",4.3 Semantics of NEURALEDITOR,[0],[0]
"We generate the semantic relations r using prior evaluations for word analogies (Mikolov et al., 2013a; Mikolov et al., 2013b).",4.3 Semantics of NEURALEDITOR,[0],[0]
"We leverage these to generate a new dataset of sentence analogies, using a simple strategy: given an analogous word pair (w1, w2), we mine the Yelp corpus for sentence pairs
(x1, x2) such that x1 is transformed into x2 by insertingw1 and removingw2 (allowing for reordering and inclusion/exclusion of stop words).
",4.3 Semantics of NEURALEDITOR,[0],[0]
"For this task, we initially compared against the SVAE, but it had a top-k accuracy close to zero.",4.3 Semantics of NEURALEDITOR,[0],[0]
"Hence, we instead compare to SAMPLING which is a baseline which randomly samples an edit vector ẑ ∼ p(z), instead using ẑ derived from f(x1, x2).
",4.3 Semantics of NEURALEDITOR,[0],[0]
"We also compare our accuracies to the simpler task of solving the word, rather than sentence-level analogies in (Mikolov et al., 2013a) using GloVe.",4.3 Semantics of NEURALEDITOR,[0],[0]
"This task is substantially simpler, since the goal is to identify a single word (such as “good:better::bad:?”) instead of an entire sentence.",4.3 Semantics of NEURALEDITOR,[0],[0]
"Despite this, the top10 performance of our model in Table 6 is nearly as good as the performance of GloVe vectors on the simpler lexical analogy task.",4.3 Semantics of NEURALEDITOR,[0],[0]
"In some categories, NEURALEDITOR at top-10 actually performs better than word vectors, since NEURALEDITOR has an understanding of which words are likely to appear in the context of a Yelp review.",4.3 Semantics of NEURALEDITOR,[0],[0]
Examples in Table 7 show the model is accurate and captures lexical analogies requiring word reorderings.,4.3 Semantics of NEURALEDITOR,[0],[0]
"Our work connects with a broad literature on attention-based neural models, retrieval-augmented text generation, semantically meaningful representations, and nonparametric statistics.
",5 Related work and discussion,[0],[0]
"Based upon recurrent neural networks and sequence-to-sequence architectures (Sutskever et al., 2014), neural language models (Bengio et al., 2003) have been widely used due to their flexibility and performance across a wide range of NLP tasks
(Kalchbrenner and Blunsom, 2013; Hahn and Mani, 2000; Ritter et al., 2011).",5 Related work and discussion,[0],[0]
"Our work is motivated by an emerging consensus that attention-based mechanisms (Bahdanau et al., 2015) can substantially improve performance on various sequence to sequence tasks by capturing more information from the input sequence (Vaswani et al., 2017).",5 Related work and discussion,[0],[0]
"Our work extends the applicability of attention mechanisms beyond sequence-to-sequence models by allowing models to attend to randomly sampled sentences.
",5 Related work and discussion,[0],[0]
There is a growing literature on applying retrieval mechanisms to augment text generation models.,5 Related work and discussion,[0],[0]
"For example, in the image captioning literature, Hodosh (2013), Kuznetsova (2013) and Mason (2014) proposed to generate image captions by first retrieving a prototype caption based on an image context, and then applying sentence compression to tailor the prototype to a particular image.",5 Related work and discussion,[0],[0]
"More recently, Song (2016) ensembled a retrieval system and an NLM for dialogue, using the NLM to transform the retrieved utterance, and Gu (2017) used an off-the-shelf search engine system to retrieve and condition on training set examples.",5 Related work and discussion,[0],[0]
"Although these approaches also edit text from the training set, these papers solve a fundamentally different problem since they solve conditional generation problems, and retrieve prototypes based on a context, where as our task is unconditional and thus there is no context which we can use to retrieve.
",5 Related work and discussion,[0],[0]
"Our work treats the prototype x′ as a latent variable rather than being given by a retrieval mechanism, and marginalizes over all possible prototypes — a challenge which motivates our new lexical similarity training method in Section 3.1.",5 Related work and discussion,[0],[0]
"Practically,
marginalization over x′ makes our model attend to training examples based on similarity of output sequences, while prior retrieval models attend to examples based on similarity of the input sequences.
",5 Related work and discussion,[0],[0]
"In terms of generation techniques that capture semantics, the sentence variational autoencoder (SVAE) (Bowman et al., 2016) is closest to our work in that it attempts to impose semantic structure on a latent vector space.",5 Related work and discussion,[0],[0]
"However, the SVAE’s latent vector is meant to represent the entire sentence, whereas the neural editor’s latent vector represents an edit.",5 Related work and discussion,[0],[0]
"Our results from Section 4.3 suggest that local variation over edits is easier to model than global variation over sentences.
",5 Related work and discussion,[0],[0]
"Our use of lexical similarity neighborhoods is comparable to context windows in word vector training (Mikolov et al., 2013a).",5 Related work and discussion,[0],[0]
"More generally, results in manifold learning demonstrate that a weak metric such as lexical similarity can be used to extract semantic similarity through distributional statistics (Tenenbaum et al., 2000; Hashimoto et al., 2016).
",5 Related work and discussion,[0],[0]
"From a generative modeling perspective, editing randomly sampled training sentences closely resembles nonparametric kernel density estimation (Parzen, 1962) where one samples points from a training set, and adds noise to smooth the density.",5 Related work and discussion,[0],[0]
"Our edit model is the text equivalent of Gaussian noise, and our training mechanism is a type of learned smoothing kernel.
",5 Related work and discussion,[0],[0]
Prototype-then-edit is a semi-parametric approach that remembers the entire training set and uses a neural editor to generalize meaningfully beyond the training set.,5 Related work and discussion,[0],[0]
The training set provides a strong inductive bias — that the corpus can be characterized by prototypes surrounded by semantically similar sentences reachable by edits.,5 Related work and discussion,[0],[0]
"Beyond improvements on generation quality as measured by perplexity, the approach also reveals new semantic structures via the edit vector.
Reproducibility.",5 Related work and discussion,[0],[0]
"All code, data and experiments are available on the CodaLab platform at https: //bit.ly/2rHsWAX.
",5 Related work and discussion,[0],[0]
Acknowledgements.,5 Related work and discussion,[0],[0]
We thank the reviewers and editor for their insightful comments.,5 Related work and discussion,[0],[0]
This work was funded by DARPA CwC program under ARO prime contract no.,5 Related work and discussion,[0],[0]
W911NF-15-1-0462.,5 Related work and discussion,[0],[0]
Construction of the LSH.,6 Appendix,[0],[0]
"The LSH maps a sentence to lexically similar sentences in the corpus, representing a graph over sentences.",6 Appendix,[0],[0]
"We apply breadth-first search (BFS) over the LSH sentence graph started at randomly selected seed sentences and uniformly sample this set to form the training set.
",6 Appendix,[0],[0]
"Reparameterization trick for q. First, note that we can write znorm ∼ q(znorm|x′, x) as znorm = hnorm(αnorm) def = f̃norm + αnorm where αnorm ∼ Unif(0, ).",6 Appendix,[0],[0]
"Furthermore, Wood (1994) present a function hdir and auxiliary random variable αdir, such that zdir = hdir(αdir) is distributed according to a vMF with mean f and concentration κ.",6 Appendix,[0],[0]
We can then define z = h(α) def=,6 Appendix,[0],[0]
hdir(αdir),6 Appendix,[0],[0]
"· hnorm(αnorm).
",6 Appendix,[0],[0]
Smoothing for language models.,6 Appendix,[0],[0]
"As a language model, NEURALEDITOR does not place probability on any test sentence which is sufficiently dissimilar from all training set sentences.",6 Appendix,[0],[0]
"In order to avoid this problem, we can consider a special prototype sentence ‘∅’ which can be edited into any sentence, and draw this special prototype with probability p∅. Concretely, we write:
p(x) = ∑
x′∈X∪{∅}
pedit(x|x′)pprior(x′)
= (1− p∅) ∑ x′∈X 1 |X | pedit(x|x′) + p∅ pNLM(x).
",6 Appendix,[0],[0]
"This linearly smoothes between our edit model (pedit) and the NLM (pNLM) since our decoder is identical to the NLM, and thus conditioning on the special ∅ token reduces to using a NLM.
",6 Appendix,[0],[0]
"Empirically, we observe that even small values of p∅ produces low perplexity (Figure 7) corresponding to the observation that smoothing of NEURALEDITOR is only necessary to avoid degenerate loglikelihoods on a very small subset of the test set.",6 Appendix,[0],[0]
We propose a new generative language model for sentences that first samples a prototype sentence from the training corpus and then edits it into a new sentence.,abstractText,[0],[0]
"Compared to traditional language models that generate from scratch either left-to-right or by first sampling a latent sentence vector, our prototype-thenedit model improves perplexity on language modeling and generates higher quality outputs according to human evaluation.",abstractText,[0],[0]
"Furthermore, the model gives rise to a latent edit vector that captures interpretable semantics such as sentence similarity and sentence-level analogies.",abstractText,[0],[0]
Generating Sentences by Editing Prototypes,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 937–943 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
937",text,[0],[0]
"The ability to automatically generate paraphrases (alternative phrasings of the same content) has been shown to be useful in many areas of Natural Language Processing such as question answering (Riezler et al., 2007), semantic parsing (Berant and Liang, 2014)), machine translation (Kauchak and Barzilay, 2006; Zhou et al., 2006), sentence compression (Napoles et al., 2011) and sentence representation (Wieting et al., 2015).",1 Introduction,[0],[0]
"From a linguistic standpoint, the automatic generation of paraphrases is an important task in its own right as it demonstrates the capacity of NLP techniques to handle a key feature of natural language.
",1 Introduction,[0],[0]
"In this paper, we focus on the automatic generation of syntactic paraphrases that is, texts which share the same meaning but differ in their syntax.",1 Introduction,[0],[0]
Our work makes the following contributions.,1 Introduction,[1.0],['Our work makes the following contributions.']
We show that conditioning text generation on syntactic information permits generating distinct syntactic paraphrases for the same input.,1 Introduction,[1.0],['We show that conditioning text generation on syntactic information permits generating distinct syntactic paraphrases for the same input.']
"We provide a systematic exploration of how different types of generation tasks impact paraphrasing and show that exploiting different types of input permits increasing the number of paraphrases produced for a
given input.",1 Introduction,[0],[0]
"We make available four training corpora for syntactically constrained, data- and textto-text generation, text expansion and text reduction.",1 Introduction,[0],[0]
Previous work on paraphrase generation falls into three main groups.,2 Related Work,[0],[0]
"Based mainly on monolingual data, earlier approaches use data-driven, (Lin and Pantel, 2001), grammar- or thesaurus-based methods (Madnani et al., 2007; McKeown, 1983; Hassan et al., 2007; Kozlowski et al., 2003; Quirk et al., 2004; Zhao et al., 2008).",2 Related Work,[0],[0]
"In contrast, the pivot-based approach exploits bilingual data and machine translation methods to extract and generate paraphrases (Callison-Burch, 2008; Ganitkevitch and Callison-Burch, 2014; Ganitkevitch et al., 2011).",2 Related Work,[0],[0]
"Finally, neural approaches build upon the encoder-decoder architecture to learn paraphrase generation models (Mallinson et al., 2017; Prakash et al., 2016).
",2 Related Work,[0],[0]
"(Prakash et al., 2016) uses a stacked residual LSTM network with residual connections between LSTM layers and show that their model outperforms sequence to sequence, attention-based, and bi- directional LSTM model on three datasets (PPDB, WikiAnswers, and MSCOCO).
",2 Related Work,[0],[0]
"(Mallinson et al., 2017) introduces a neural model for multi-lingual, multi-pivot backtranslation and show that it outperforms a paraphrase model trained with a commonly used Statistical Machine Translation system (SMT) on three tasks, namely, correlation with human judgments of paraphrase quality; paraphrase and similarity detection; and sentence-level paraphrase generation.
",2 Related Work,[0],[0]
"(Iyyer et al., 2018) also use backtranslation as a mean to provide training data.",2 Related Work,[0],[0]
"In addition, it uses syntax to control paraphrase generation.",2 Related Work,[0],[0]
"Given
a syntactic template T and an input sentence S, the model first generates a full syntactic parse PT .",2 Related Work,[0],[0]
"Next this syntactic parse is used together with the input sentence to predict a syntactic paraphrase of S which realises the input syntactic template T .
",2 Related Work,[0],[0]
"Our approach is closest to (Iyyer et al., 2018) but differs from it in that instead of restricting paraphrase generation to a text rewriting problem, we explore how various sources of input impacts the number and the type of generated paraphrases.",2 Related Work,[0],[0]
"It also differs from the former two approaches (Prakash et al., 2016; Mallinson et al., 2017) in that we focus on syntactic paraphrases and condition generation on syntax.",2 Related Work,[0],[0]
"In that sense, our approach also shares similarities with recent models for controllable text generation (Hu et al., 2017; Semeniuta et al., 2017), which use variational autoencoders to model holistic properties of sentences such as style, topic and various other syntactic features.",2 Related Work,[0],[0]
"Our work is arguably conceptually simpler, focuses on syntactic paraphrases and introduces a new text production mode based on hybrid “data and text” input.",2 Related Work,[0],[0]
"In order to generate syntactically distinct paraphrases, we formulate the generation task as a structured prediction task conditioned on both some input I and some syntactic constraint k.",3 Generating Syntactic Paraphrases,[0],[0]
"In this way, the same input I can be mapped to several output Ti each satisfying a different syntactic constraint ki.",3 Generating Syntactic Paraphrases,[0],[0]
"Table 1 shows some examples.
",3 Generating Syntactic Paraphrases,[0],[0]
"In addition, we consider different, semantically equivalent, sources of information.",3 Generating Syntactic Paraphrases,[0],[0]
"That is, we compare the paraphrases obtained when generating text from data, from text or from text and data.",3 Generating Syntactic Paraphrases,[0],[0]
"For the later, we consider two subtasks namely text expansion and text reduction.",3 Generating Syntactic Paraphrases,[1.0],"['For the later, we consider two subtasks namely text expansion and text reduction.']"
"For each of these two tasks, the input is a text and a data unit.",3 Generating Syntactic Paraphrases,[0],[0]
"For text expansion, the output is a text verbalising both the input text and the input data.",3 Generating Syntactic Paraphrases,[0.9910589433268949],"['Conversely, for text reduction, the output is a text verbalising the input text minus the text verbalising the input data.']"
"Conversely, for text reduction, the output is a text verbalising the input text minus the text verbalising the input data.",3 Generating Syntactic Paraphrases,[0],[0]
Table 2 shows some example input and output for text expansion and text reduction.,3 Generating Syntactic Paraphrases,[0],[0]
Training data.,4 Training and Test Data,[0],[0]
"The WEBNLG dataset (Gardent et al., 2017) associates sets of RDF triples with one or more texts verbalising these sets of triples.
",4 Training and Test Data,[1.0000000144676453],"['The WEBNLG dataset (Gardent et al., 2017) associates sets of RDF triples with one or more texts verbalising these sets of triples.']"
"We derive training corpora for syntactically constrained generation from this dataset as follows.
",4 Training and Test Data,[1.0000000569003238],['We derive training corpora for syntactically constrained generation from this dataset as follows.']
"We enrich the WEBNLG texts with labels indicating syntactic structures that are realised by these texts by first, parsing1 these texts and then using syntactic templates to identify the target structures occurring in those texts.",4 Training and Test Data,[0],[0]
"We use the following list of syntactic labels: subject relative, object relative, sentence coordination, VP coordination, passive voice, apposition, possessive relative, pied piping, transitive clause, prepositional object, ditransitive clause, predicative clause.
",4 Training and Test Data,[1.0000000458248939],"['We use the following list of syntactic labels: subject relative, object relative, sentence coordination, VP coordination, passive voice, apposition, possessive relative, pied piping, transitive clause, prepositional object, ditransitive clause, predicative clause.']"
"Based on the resulting, syntactically enriched, WEBNLG corpus, we then build four training corpora (T2Tsyn, TXsyn, D2Tsyn, TRsyn) using the sets of RDF triples as pivots to relate paraphrases.",4 Training and Test Data,[1.0],"['Based on the resulting, syntactically enriched, WEBNLG corpus, we then build four training corpora (T2Tsyn, TXsyn, D2Tsyn, TRsyn) using the sets of RDF triples as pivots to relate paraphrases.']"
"For data-to-text generation (D2Tsyn), the input is a linearised and delexicalised version of the set of RDF triples representing the meaning of the output text, for text-to-text generation (T2Tsyn), the input is a text and for hybrid data-and-text-to-text generation (TXsyn and TRsyn), the input is a text and a linearised RDF triple.
",4 Training and Test Data,[0.9999999494976385],"['For data-to-text generation (D2Tsyn), the input is a linearised and delexicalised version of the set of RDF triples representing the meaning of the output text, for text-to-text generation (T2Tsyn), the input is a text and for hybrid data-and-text-to-text generation (TXsyn and TRsyn), the input is a text and a linearised RDF triple.']"
"For the text-to-text datasets, we additionally require that, for any corpus instance 〈k, Ti, To〉, To differs from Ti on exactly one syntactic label2.
Test data.",4 Training and Test Data,[0.9945810553814043],"['For the text-to-text datasets, we additionally require that, for any corpus instance 〈k, Ti, To〉, To differs from Ti on exactly one syntactic label2.']"
"For any input 〈k, I〉 occuring in the test data, we ensure that 〈k, I〉 does not occur in the training data.",4 Training and Test Data,[0],[0]
"(where I is either a set of RDF triples, a text or a text and an RDF triple).",4 Training and Test Data,[0],[0]
"Models and Baselines D2T5best and T2T5best For each generation task, we aim to learn a model that maximises the likelihood P (T |I; k; θ) of a text given some input I , some model parameters θ and some syntactic constraint k.",5 Experimental Setup,[0],[0]
We use a simple encoder-decoder model where both encoder and decoder are bidirectional LSTMs and the encoder receives as input a sequence including both the input I and the syntactic,5 Experimental Setup,[0],[0]
"constraint k.
We compare our models with the output produced by beam search when no syntactic constraint applies.",5 Experimental Setup,[0],[0]
"For D2T5best, we take the 5 best output generated from data.",5 Experimental Setup,[0],[0]
"For T2T5best, there may be several input sentences associated with the same meaning: we take the 5 best output for each
1We used the Stanford CoreNLP dependency parser version 3.8, 2018-06-09
2K(Ti) =",5 Experimental Setup,[0],[0]
(K(Ti) ∩ K(To)),5 Experimental Setup,[0],[0]
∪ {k} and K(To) = (K(Ti) ∩K(To)),5 Experimental Setup,[0],[0]
"∪ {k′} for some k 6= k′.
of these sentences hence T2T5best may (and does) in fact yield more than 5 output per input meaning.",5 Experimental Setup,[0],[0]
"Finally, ALLsyn groups together all output generated by the four syntactically constrained models for a given meaning.
",5 Experimental Setup,[0],[0]
Implementation Details,5 Experimental Setup,[0],[0]
"We use the OpenNMTpy sequence-to-sequence model (Klein et al., 2017) with attention and a bidirectional LSTM encoder.",5 Experimental Setup,[0],[0]
The encoder and decoder have two layers.,5 Experimental Setup,[1.0],['The encoder and decoder have two layers.']
"Models were trained for 13 epochs, with a mini-batch size of 64, a dropout rate of 0.3, and a word embedding size of 500.",5 Experimental Setup,[0],[0]
"They were optimised with SGD with a starting learning rate of 1.0.
",5 Experimental Setup,[0],[0]
Evaluation.,5 Experimental Setup,[0],[0]
"We assess both the linguistic/syntactic adequacy of the generated texts and the diversity of the paraphrases being generated.
Syntactic and Linguistic Adequacy (BLEU, Synt, BLEUsyn).",5 Experimental Setup,[0],[0]
"For the syntactically constrained models, given an input syntactic constraint k, the BLEU score3 is computed with respect to those references which satisfy k.",5 Experimental Setup,[0],[0]
"In that way, the BLEU score indicates how close to the syntactic target the generated sentence is and therefore how well the model succeeds in generating the required syntactic constructs – as the number of references varies across inputs, we use BLEU at the sentence level (Papineni et al., 2002).",5 Experimental Setup,[0],[0]
"In addition, we compute the proportion of output satisfying the input syntactic constraint (Synt) and the BLEU score for these output which satisfy the input syntactic constraint (BLEUsyn).",5 Experimental Setup,[0],[0]
"The number of output satisfying the input syntactic constraint is computed by first parsing the generated output and then applying the templates used for the automatic annotation of the training data.
",5 Experimental Setup,[0],[0]
"Diversity (Sim, #Txt/Mg).",5 Experimental Setup,[0],[0]
"To measure the level of paraphrasing obtained, we group together inputs which share the same meaning (i.e., inputs that are linked in the WEBNLG dataset to the same set of RDF triples) and we compute the number of distinct texts generated per meaning (# Txt/Mg).",5 Experimental Setup,[0],[0]
We further analyse these sets by computing the average pairwise similarity (Sim) of the texts present in these sets.,5 Experimental Setup,[0],[0]
"We use the Ratcliff/Obershelp algorithm (Black, 2004) to compute similarity4.",5 Experimental Setup,[0],[0]
"A low similarity indicates more
3We use the sacrebleu script with BLEU-4.",5 Experimental Setup,[0],[0]
4The Ratcliff/Obershelp similarity score varies between O and 1 where 1 is a complete match.,5 Experimental Setup,[0],[0]
"It is expressed by the
diversity across the set of outputs sharing the same meaning.
",5 Experimental Setup,[0],[0]
Human Evaluation (% SPar).,5 Experimental Setup,[0],[0]
"For each model, we manually examined for 50 meanings, a maximum of 10 randomly chosen output and recorded the average number (# SPar) of syntactically correct paraphrases per input.",5 Experimental Setup,[0],[0]
"Table 3 summarises the results.
",6 Results,[0],[0]
Diversity.,6 Results,[0],[0]
"The results for ALLsyn (aggregating all output texts generated for a given meaning) shows that combining different generation models increases diversity (# Txt/Mg:13.25, Sim:0.61)) while maintaining a good level of linguistic (BLEU:62.87) and syntactic adequacy (Synt:0.91).
",6 Results,[0],[0]
"The human evaluation further shows that the distinct outputs generated by the ALLsyn model are indeed syntactic, not purely lexical, variants.",6 Results,[1.0],"['The human evaluation further shows that the distinct outputs generated by the ALLsyn model are indeed syntactic, not purely lexical, variants.']"
"Table 1 shows some example output for ALLsyn.
",6 Results,[0],[0]
"Expansion, Reduction and Generation.",6 Results,[0],[0]
"Interestingly, the text expansion and reduction models markedly improve on traditional T2T and D2T models both in terms of linguistic adequacy (higher BLEU score) and in terms of diversity (higher number of distinct output per meaning, lower similarity between texts generated from the same meaning).",6 Results,[1.0],"['Interestingly, the text expansion and reduction models markedly improve on traditional T2T and D2T models both in terms of linguistic adequacy (higher BLEU score) and in terms of diversity (higher number of distinct output per meaning, lower similarity between texts generated from the same meaning).']"
The comparison with T2T generation is particularly striking as the training data is 3 to 5 times larger for the T2Tsyn model than for the TXsyn and the TRsyn model respectively.,6 Results,[1.0],['The comparison with T2T generation is particularly striking as the training data is 3 to 5 times larger for the T2Tsyn model than for the TXsyn and the TRsyn model respectively.']
"Similarly, it is noticable that although the T2Tsyn training corpus is 3 times larger than the D2Tsyn corpus, the T2Tsyn and the D2Tsyn models show similar results.",6 Results,[0],[0]
"This is in line with results from (Aharoni and Goldberg, 2018) which shows that rephrasing is a difficult task.
",6 Results,[0],[0]
Linguistic Adequacy.,6 Results,[0],[0]
Overall the linguistic adequacy of the syntactically constrained models is high with a BLEU score with respect to a single reference ranging from 46.20 (D2Tsyn) to 83.87 (TXsyn).,6 Results,[0],[0]
"Moreover, the generated sentences show close similarity with the reference sentence realising the input constraint (BLEUsyn: from 48.16 to 89.32).
formula sim(S1, S2) = 2∗match(S1,S2) len(S1)+len(S2)
where a match is defined as the sum of the length of the matching segments (match(S1, S2) = ∑ m∈overlap(S1,S2) len(m).
",6 Results,[0],[0]
"While the baseline models underperform in terms of BLEU scores, the manual evaluation (# SPar/Mg) indicates that they, in fact, produce acceptable output.",6 Results,[0],[0]
The low BLEU scores for these models are probably due to the fact that each output is evaluated against a single reference while the dataset is constructed to maximise the number of paraphrases available for a given input.,6 Results,[0],[0]
"Table 1 shows some example outputs illustrating the main differences between the D2T5best, T2T5best and the ALLsyn model.",7 Some examples,[1.0],"['Table 1 shows some example outputs illustrating the main differences between the D2T5best, T2T5best and the ALLsyn model.']"
"As these examples show, syntactically constrained generation (ALLsyn) outputs a much larger number of paraphrases.",7 Some examples,[1.0],"['As these examples show, syntactically constrained generation (ALLsyn) outputs a much larger number of paraphrases.']"
"The difference is due both to the fact that ALLsyn groups together the output of 4 (syntactically driven) generation models and to the input syntactic constraint, which ensures greater diversity.",7 Some examples,[0],[0]
"Thus in the example shown, ALLsyn yields 15 paraphrases each with strong syntactic differences as summarised below.
",7 Some examples,[1.0000000447599253],"['Thus in the example shown, ALLsyn yields 15 paraphrases each with strong syntactic differences as summarised below.']"
Sentence Segmentation.,7 Some examples,[0],[0]
"The number of verb phrases, clauses and sentences used to verbalise the same input varies.",7 Some examples,[0],[0]
"One output text is made of 2 sentences and one VP coordination, another of 3 coordinated clauses and a third of two coordinated clauses and a VP coordination.
",7 Some examples,[1.0000000539948608],"['One output text is made of 2 sentences and one VP coordination, another of 3 coordinated clauses and a third of two coordinated clauses and a VP coordination.']"
Syntax.,7 Some examples,[0],[0]
The same input property is realised by different syntactic structures.,7 Some examples,[1.0],['The same input property is realised by different syntactic structures.']
"For instance, the property operatingOrganisation is alternatively realised by an active verb (operates), a passive verb (is operated by), a participial apposition (,operated by ..,), a subject relative (which is operated
by), a nominal predicative construction (is the operation organization) and a preposed participial (Operated by .., ).
",7 Some examples,[0],[0]
Word Order.,7 Some examples,[0],[0]
The same content is verbalised using varying word order and clause ordering.,7 Some examples,[1.0],['The same content is verbalised using varying word order and clause ordering.']
"Thus the ALLsyn output shows four different ways of ordering the realisation of the three properties operatinOrganization (oO), runwayLength (rL), runwayName (rN) contained in the input namely, rLoO-rN (once), oO-rN-rL (6 times), oO-rL-rN (6 times) and rN-rL-oO (once).
",7 Some examples,[1.0000000469967218],"['Thus the ALLsyn output shows four different ways of ordering the realisation of the three properties operatinOrganization (oO), runwayLength (rL), runwayName (rN) contained in the input namely, rLoO-rN (once), oO-rN-rL (6 times), oO-rL-rN (6 times) and rN-rL-oO (once).']"
"By constrast, the baseline models output a much smaller range of syntactic paraphrases.",7 Some examples,[1.0],"['By constrast, the baseline models output a much smaller range of syntactic paraphrases.']"
"The D2T5best model is particularly weak as among the five best outputs it produces, only three are distinct and all have almost identical syntax.",7 Some examples,[0],[0]
The T2T5best model produces more outputs (8 against 3 for the D2T5best model and 15 for the ALLsyn model).,7 Some examples,[0],[0]
"One reason for this is that, contrary to the D2T5best model which has a single input (namely a set of RDF triples), this model can have several inputs for the same set of RDF triples.",7 Some examples,[1.0],"['One reason for this is that, contrary to the D2T5best model which has a single input (namely a set of RDF triples), this model can have several inputs for the same set of RDF triples.']"
We have proposed new syntactically constrained models for text generation and shown that their use effectively supports the generation of syntactic paraphrases.,8 Conclusion,[0],[0]
"In future work, we plan to investigate to what extent these methods can be used to support the automatic generation of grammar exercises.",8 Conclusion,[0],[0]
"We study the automatic generation of syntactic paraphrases using four different models for generation: data-to-text generation, textto-text generation, text reduction and text expansion, We derive training data for each of these tasks from the WebNLG dataset and we show (i) that conditioning generation on syntactic constraints effectively permits the generation of syntactically distinct paraphrases for the same input and (ii) that exploiting different types of input (data, text or data+text) further increases the number of distinct paraphrases that can be generated for a given input.",abstractText,[0],[0]
Generating Syntactic Paraphrases,title,[0],[0]
